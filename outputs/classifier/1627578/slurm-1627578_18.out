ROOT_PATH: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language
Building Bayesian Optimizer for 
 data:IMDB 
 choices:[{'type': 'continuous', 'domain': [5e-05, 0.005], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'l2'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'batch_size': 32, 'l2': 0.31239159386110527, 'attn_type': None, 'lr': 0.004052780290644904, 'num_layers': 1, 'wordvec_source': 'google', 'tune_wordvecs': True, 'data': 'IMDB', 'dropout': 0.9388793698545737, 'rnn_dropout': 0.5382800153940416}
Using CUDA!
here
None
Building RNN Classifier...

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/imdb/aclImdb/train...
Got 25000 examples in 63.31498146057129 seconds
Retrieving Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/imdb/aclImdb/test...
Got 25000 examples in 60.866124868392944 seconds
Loading Vectors From Memory...
Using these vectors: ['googlenews']
Getting google news vectors
Building Vocab...
[<torchtext.vocab.Vectors object at 0x7fbf75967e80>]
Getting Batches...
Created Iterator with 782 batches
Getting Batches...
Created Iterator with 782 batches
Building model...
Traceback (most recent call last):
  File "tune_models.py", line 186, in <module>
    batch_size = args.batch_size, seq_len = args.seq_len)
  File "tune_models.py", line 67, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 119, in getError
    optimizer = trainer.start_train()
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/trainer.py", line 588, in start_train
    self.get_model(pretrained_weights, pretrained_args)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/trainer.py", line 387, in get_model
    self.model = VanillaRNN(**args)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/model.py", line 70, in __init__
    self.init_embedding(vectors)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/model.py", line 73, in init_embedding
    self.embed.weight.data.copy_(pretrained_embeddings)# this provides the values
RuntimeError: inconsistent tensor size, expected tensor [101925 x 300] and src [101925 x 100] to have the same number of elements, but got 30577500 and 10192500 elements respectively at /opt/conda/conda-bld/pytorch_1513366702650/work/torch/lib/TH/generic/THTensorCopy.c:86
