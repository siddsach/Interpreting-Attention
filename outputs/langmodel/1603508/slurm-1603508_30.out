Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.388774761071636, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.653007880104995, 'data': 'ptb', 'lr': 21.23500549346587}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4344732761383057 and batch: 50, loss is 6.7164343738555905 and perplexity is 825.8675205015076
At time: 2.366009473800659 and batch: 100, loss is 5.825381803512573 and perplexity is 338.79045977523094
At time: 3.2791526317596436 and batch: 150, loss is 5.74709849357605 and perplexity is 313.28035533299897
At time: 4.202671051025391 and batch: 200, loss is 5.751506032943726 and perplexity is 314.6641982630885
At time: 5.122575044631958 and batch: 250, loss is 5.809445495605469 and perplexity is 333.43418368359374
At time: 6.04064416885376 and batch: 300, loss is 5.866055002212525 and perplexity is 352.8542217928284
At time: 6.957472562789917 and batch: 350, loss is 5.940219793319702 and perplexity is 380.0184458754062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.910218863651671 and perplexity of 368.78686061631925
Finished 1 epochs...
Completing Train Step...
At time: 8.737811088562012 and batch: 50, loss is 5.755442380905151 and perplexity is 315.90526707528676
At time: 9.64747142791748 and batch: 100, loss is 5.767315225601196 and perplexity is 319.6783153359163
At time: 10.559850931167603 and batch: 150, loss is 5.799076271057129 and perplexity is 329.9945935211674
At time: 11.47181248664856 and batch: 200, loss is 5.670797576904297 and perplexity is 290.26595151202173
At time: 12.391072273254395 and batch: 250, loss is 5.571229257583618 and perplexity is 262.7568967069394
At time: 13.304367303848267 and batch: 300, loss is 5.635229959487915 and perplexity is 280.12332692112784
At time: 14.213694334030151 and batch: 350, loss is 5.740628852844238 and perplexity is 311.2600862400028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.004752849710399 and perplexity of 405.35079378823053
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.987507820129395 and batch: 50, loss is 5.600835647583008 and perplexity is 270.65248304651675
At time: 16.900169372558594 and batch: 100, loss is 5.432064313888549 and perplexity is 228.62070353133194
At time: 17.81173300743103 and batch: 150, loss is 5.3684297370910645 and perplexity is 214.52574127051088
At time: 18.722394943237305 and batch: 200, loss is 5.287875528335571 and perplexity is 197.92249770508073
At time: 19.62929654121399 and batch: 250, loss is 5.249687309265137 and perplexity is 190.50668946749025
At time: 20.53768825531006 and batch: 300, loss is 5.239636993408203 and perplexity is 188.60162634420044
At time: 21.45879578590393 and batch: 350, loss is 5.325828733444214 and perplexity is 205.57865979945183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.487878733667834 and perplexity of 241.74385946708574
Finished 3 epochs...
Completing Train Step...
At time: 23.232844352722168 and batch: 50, loss is 5.304152173995972 and perplexity is 201.17037261858226
At time: 24.158504724502563 and batch: 100, loss is 5.261711435317993 and perplexity is 192.81119297058086
At time: 25.069742441177368 and batch: 150, loss is 5.250519590377808 and perplexity is 190.6653105864959
At time: 25.980644702911377 and batch: 200, loss is 5.225151119232177 and perplexity is 185.88925984570028
At time: 26.893423557281494 and batch: 250, loss is 5.231375474929809 and perplexity is 187.04990911909616
At time: 27.806307315826416 and batch: 300, loss is 5.229770412445069 and perplexity is 186.7499231397183
At time: 28.719114780426025 and batch: 350, loss is 5.290248079299927 and perplexity is 198.3926364113649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.454617204337285 and perplexity of 233.8353427800372
Finished 4 epochs...
Completing Train Step...
At time: 30.482280254364014 and batch: 50, loss is 5.259913930892944 and perplexity is 192.46492530008544
At time: 31.411678791046143 and batch: 100, loss is 5.228799962997437 and perplexity is 186.56877968945906
At time: 32.32691407203674 and batch: 150, loss is 5.226888227462768 and perplexity is 186.21245023603012
At time: 33.25932335853577 and batch: 200, loss is 5.204825267791748 and perplexity is 182.1490426835322
At time: 34.176279067993164 and batch: 250, loss is 5.19893367767334 and perplexity is 181.07905025700944
At time: 35.09273982048035 and batch: 300, loss is 5.195695390701294 and perplexity is 180.49361274658247
At time: 36.01370429992676 and batch: 350, loss is 5.265609540939331 and perplexity is 193.5642581760288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.436640246161099 and perplexity of 229.66925360420743
Finished 5 epochs...
Completing Train Step...
At time: 37.7943696975708 and batch: 50, loss is 5.233962087631226 and perplexity is 187.5343610644953
At time: 38.70989680290222 and batch: 100, loss is 5.195324125289917 and perplexity is 180.42661414909574
At time: 39.6274528503418 and batch: 150, loss is 5.189467105865479 and perplexity is 179.37294067051326
At time: 40.5440137386322 and batch: 200, loss is 5.171641263961792 and perplexity is 176.20379723191732
At time: 41.46597599983215 and batch: 250, loss is 5.172346773147583 and perplexity is 176.3281544918722
At time: 42.38010096549988 and batch: 300, loss is 5.166847333908081 and perplexity is 175.3611100583422
At time: 43.29487872123718 and batch: 350, loss is 5.237343339920044 and perplexity is 188.16953528907194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.419482000942888 and perplexity of 225.76214762860857
Finished 6 epochs...
Completing Train Step...
At time: 45.07044982910156 and batch: 50, loss is 5.2092485427856445 and perplexity is 182.95652252534006
At time: 45.99732971191406 and batch: 100, loss is 5.16963942527771 and perplexity is 175.85141847472576
At time: 46.91098475456238 and batch: 150, loss is 5.169275894165039 and perplexity is 175.78750263130556
At time: 47.825822830200195 and batch: 200, loss is 5.156422262191772 and perplexity is 173.54245418991175
At time: 48.74138569831848 and batch: 250, loss is 5.149853210449219 and perplexity is 172.40618103346483
At time: 49.66663074493408 and batch: 300, loss is 5.1467546939849855 and perplexity is 171.872804407862
At time: 50.58684420585632 and batch: 350, loss is 5.2127123546600345 and perplexity is 183.59134832441143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.403674947804418 and perplexity of 222.2215701339689
Finished 7 epochs...
Completing Train Step...
At time: 52.34493923187256 and batch: 50, loss is 5.1855180549621585 and perplexity is 178.66598461859311
At time: 53.259116649627686 and batch: 100, loss is 5.148582353591919 and perplexity is 172.18721662173988
At time: 54.17312812805176 and batch: 150, loss is 5.147234630584717 and perplexity is 171.95531225487986
At time: 55.08872699737549 and batch: 200, loss is 5.135364532470703 and perplexity is 169.92625222238132
At time: 56.01693606376648 and batch: 250, loss is 5.133905076980591 and perplexity is 169.67843330498906
At time: 56.93128299713135 and batch: 300, loss is 5.130018272399902 and perplexity is 169.0202064214744
At time: 57.847044467926025 and batch: 350, loss is 5.200144357681275 and perplexity is 181.29841180453747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.394028762291217 and perplexity of 220.08828521608189
Finished 8 epochs...
Completing Train Step...
At time: 59.61975288391113 and batch: 50, loss is 5.167963724136353 and perplexity is 175.55699080736298
At time: 60.53543043136597 and batch: 100, loss is 5.13338996887207 and perplexity is 169.591053075217
At time: 61.46124815940857 and batch: 150, loss is 5.130840177536011 and perplexity is 169.15918210182755
At time: 62.38088345527649 and batch: 200, loss is 5.116323051452636 and perplexity is 166.72121587466
At time: 63.29736804962158 and batch: 250, loss is 5.11678466796875 and perplexity is 166.79819490752377
At time: 64.2126681804657 and batch: 300, loss is 5.115563821792603 and perplexity is 166.59468422198097
At time: 65.13681149482727 and batch: 350, loss is 5.1837114810943605 and perplexity is 178.3435027011265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.383347609947467 and perplexity of 217.7499987402847
Finished 9 epochs...
Completing Train Step...
At time: 66.90230178833008 and batch: 50, loss is 5.147672901153564 and perplexity is 172.03069172449304
At time: 67.83160281181335 and batch: 100, loss is 5.113365297317505 and perplexity is 166.2288240549293
At time: 68.75610184669495 and batch: 150, loss is 5.110848379135132 and perplexity is 165.81096578336172
At time: 69.67066788673401 and batch: 200, loss is 5.099461078643799 and perplexity is 163.93353620584676
At time: 70.58683681488037 and batch: 250, loss is 5.098012056350708 and perplexity is 163.69616487700623
At time: 71.50220727920532 and batch: 300, loss is 5.096523447036743 and perplexity is 163.45266652323386
At time: 72.42692518234253 and batch: 350, loss is 5.161231060028076 and perplexity is 174.3789945327723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.372641727842134 and perplexity of 215.4312273193649
Finished 10 epochs...
Completing Train Step...
At time: 74.19610667228699 and batch: 50, loss is 5.124817781448364 and perplexity is 168.14349999515497
At time: 75.12386918067932 and batch: 100, loss is 5.090334997177124 and perplexity is 162.44427131183585
At time: 76.03983879089355 and batch: 150, loss is 5.086452779769897 and perplexity is 161.81484989995826
At time: 76.95435094833374 and batch: 200, loss is 5.074822340011597 and perplexity is 159.943773846072
At time: 77.86892485618591 and batch: 250, loss is 5.076404943466186 and perplexity is 160.19710182109057
At time: 78.79683351516724 and batch: 300, loss is 5.071390829086304 and perplexity is 159.3958656528092
At time: 79.71230387687683 and batch: 350, loss is 5.134677886962891 and perplexity is 169.80961317405544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.354095985149515 and perplexity of 211.472715439337
Finished 11 epochs...
Completing Train Step...
At time: 81.48095083236694 and batch: 50, loss is 5.101151838302612 and perplexity is 164.21094286349864
At time: 82.3966794013977 and batch: 100, loss is 5.06530421257019 and perplexity is 158.42863072445772
At time: 83.31263375282288 and batch: 150, loss is 5.056066656112671 and perplexity is 156.9718760892688
At time: 84.22759342193604 and batch: 200, loss is 5.049084672927856 and perplexity is 155.879718240506
At time: 85.1443178653717 and batch: 250, loss is 5.0521328735351565 and perplexity is 156.3555958089901
At time: 86.05967998504639 and batch: 300, loss is 5.047635803222656 and perplexity is 155.6540323732806
At time: 86.98360466957092 and batch: 350, loss is 5.109273948669434 and perplexity is 165.55011334818013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.336592049434267 and perplexity of 207.80331875232872
Finished 12 epochs...
Completing Train Step...
At time: 88.76362490653992 and batch: 50, loss is 5.077487020492554 and perplexity is 160.37054124514677
At time: 89.68545150756836 and batch: 100, loss is 5.042909116744995 and perplexity is 154.9200406042962
At time: 90.61130952835083 and batch: 150, loss is 5.034095916748047 and perplexity is 153.56069817985568
At time: 91.53584551811218 and batch: 200, loss is 5.0282133769989015 and perplexity is 152.66002299496031
At time: 92.45416522026062 and batch: 250, loss is 5.03275086402893 and perplexity is 153.3542897914204
At time: 93.37322902679443 and batch: 300, loss is 5.024149599075318 and perplexity is 152.04090539394298
At time: 94.29695320129395 and batch: 350, loss is 5.085985460281372 and perplexity is 161.73924833348585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.326044016870959 and perplexity of 205.6229222421133
Finished 13 epochs...
Completing Train Step...
At time: 96.05243968963623 and batch: 50, loss is 5.055532732009888 and perplexity is 156.8880873915587
At time: 96.99158024787903 and batch: 100, loss is 5.018226404190063 and perplexity is 151.14299934222478
At time: 97.90899109840393 and batch: 150, loss is 5.011085805892944 and perplexity is 150.0675919938553
At time: 98.83684015274048 and batch: 200, loss is 5.007795543670654 and perplexity is 149.5746416777453
At time: 99.76027774810791 and batch: 250, loss is 5.017227573394775 and perplexity is 150.99210842977953
At time: 100.67967343330383 and batch: 300, loss is 5.007270259857178 and perplexity is 149.496093171501
At time: 101.60599160194397 and batch: 350, loss is 5.070372409820557 and perplexity is 159.23361646522068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.309891273235452 and perplexity of 202.32822869850867
Finished 14 epochs...
Completing Train Step...
At time: 103.45165371894836 and batch: 50, loss is 5.035181436538696 and perplexity is 153.72748186391325
At time: 104.38158345222473 and batch: 100, loss is 4.9979478454589845 and perplexity is 148.10890465952278
At time: 105.30645847320557 and batch: 150, loss is 4.993901968002319 and perplexity is 147.51088475441077
At time: 106.23564314842224 and batch: 200, loss is 4.988151807785034 and perplexity is 146.66510754094315
At time: 107.15732645988464 and batch: 250, loss is 4.995011081695557 and perplexity is 147.6745818591629
At time: 108.07622289657593 and batch: 300, loss is 4.985708131790161 and perplexity is 146.3071430910601
At time: 109.00289559364319 and batch: 350, loss is 5.05023422241211 and perplexity is 156.05901272441145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.300272448309537 and perplexity of 200.3913988359911
Finished 15 epochs...
Completing Train Step...
At time: 110.77666068077087 and batch: 50, loss is 5.017048978805542 and perplexity is 150.96514446407795
At time: 111.70597720146179 and batch: 100, loss is 4.980863075256348 and perplexity is 145.5999911902921
At time: 112.6211907863617 and batch: 150, loss is 4.9761609363555905 and perplexity is 144.91696690377822
At time: 113.53784132003784 and batch: 200, loss is 4.969912595748902 and perplexity is 144.0142993588218
At time: 114.4533998966217 and batch: 250, loss is 4.97918662071228 and perplexity is 145.35610391353197
At time: 115.37458562850952 and batch: 300, loss is 4.971363477706909 and perplexity is 144.22339876005037
At time: 116.29629349708557 and batch: 350, loss is 5.035290336608886 and perplexity is 153.74422370905586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.288185908876616 and perplexity of 197.9839385315286
Finished 16 epochs...
Completing Train Step...
At time: 118.07441091537476 and batch: 50, loss is 5.001904373168945 and perplexity is 148.6960624319459
At time: 119.0056037902832 and batch: 100, loss is 4.963972864151001 and perplexity is 143.16142849385153
At time: 119.92049551010132 and batch: 150, loss is 4.960937795639038 and perplexity is 142.72758245935748
At time: 120.83560967445374 and batch: 200, loss is 4.957220258712769 and perplexity is 142.1979724335192
At time: 121.75169253349304 and batch: 250, loss is 4.964781923294067 and perplexity is 143.2773014242152
At time: 122.66672730445862 and batch: 300, loss is 4.9587282466888425 and perplexity is 142.4125670285393
At time: 123.58149075508118 and batch: 350, loss is 5.022578334808349 and perplexity is 151.80219653863898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.282308644261853 and perplexity of 196.82374725051878
Finished 17 epochs...
Completing Train Step...
At time: 125.3458960056305 and batch: 50, loss is 4.987483549118042 and perplexity is 146.5671300524827
At time: 126.27667093276978 and batch: 100, loss is 4.9520555591583255 and perplexity is 141.46545587090026
At time: 127.19995760917664 and batch: 150, loss is 4.9478187751770015 and perplexity is 140.86736517874826
At time: 128.11556839942932 and batch: 200, loss is 4.942724533081055 and perplexity is 140.15157746303413
At time: 129.03133702278137 and batch: 250, loss is 4.954200944900513 and perplexity is 141.76927963599093
At time: 129.94618821144104 and batch: 300, loss is 4.947512989044189 and perplexity is 140.82429647715225
At time: 130.8626208305359 and batch: 350, loss is 5.013416423797607 and perplexity is 150.41775009453687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.275159112338362 and perplexity of 195.42156802161824
Finished 18 epochs...
Completing Train Step...
At time: 132.64399981498718 and batch: 50, loss is 4.9740917778015135 and perplexity is 144.61742073321244
At time: 133.56812500953674 and batch: 100, loss is 4.938415431976319 and perplexity is 139.54894947071517
At time: 134.48425698280334 and batch: 150, loss is 4.934679079055786 and perplexity is 139.02851820868523
At time: 135.3992199897766 and batch: 200, loss is 4.933744716644287 and perplexity is 138.89867585649768
At time: 136.31564116477966 and batch: 250, loss is 4.944073257446289 and perplexity is 140.34073083959493
At time: 137.23213720321655 and batch: 300, loss is 4.935782175064087 and perplexity is 139.18196462966878
At time: 138.1473262310028 and batch: 350, loss is 5.001936197280884 and perplexity is 148.70079462738013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.273423819706358 and perplexity of 195.08274847504927
Finished 19 epochs...
Completing Train Step...
At time: 139.91496205329895 and batch: 50, loss is 4.9638975143432615 and perplexity is 143.15064171413488
At time: 140.82972764968872 and batch: 100, loss is 4.930914182662963 and perplexity is 138.5060743326266
At time: 141.75305438041687 and batch: 150, loss is 4.923484115600586 and perplexity is 137.48077863653862
At time: 142.67137932777405 and batch: 200, loss is 4.922029714584351 and perplexity is 137.28097178723982
At time: 143.59798312187195 and batch: 250, loss is 4.933335876464843 and perplexity is 138.84190010383134
At time: 144.52151536941528 and batch: 300, loss is 4.928010759353637 and perplexity is 138.10451579722226
At time: 145.44586968421936 and batch: 350, loss is 4.992320375442505 and perplexity is 147.27776703381187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.263781843514278 and perplexity of 193.21080438146527
Finished 20 epochs...
Completing Train Step...
At time: 147.19171833992004 and batch: 50, loss is 4.94978964805603 and perplexity is 141.1452706165078
At time: 148.12612891197205 and batch: 100, loss is 4.917046489715577 and perplexity is 136.59857152374505
At time: 149.04319047927856 and batch: 150, loss is 4.912440176010132 and perplexity is 135.97080261186863
At time: 149.98512625694275 and batch: 200, loss is 4.913361215591431 and perplexity is 136.09609479364724
At time: 150.90073823928833 and batch: 250, loss is 4.925312213897705 and perplexity is 137.73233688039144
At time: 151.8176233768463 and batch: 300, loss is 4.914782657623291 and perplexity is 136.2896850592334
At time: 152.7385447025299 and batch: 350, loss is 4.980134029388427 and perplexity is 145.49388080271518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.264132795662715 and perplexity of 193.27862402839358
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 154.54321098327637 and batch: 50, loss is 4.916055727005005 and perplexity is 136.46330177393912
At time: 155.46056461334229 and batch: 100, loss is 4.848410110473633 and perplexity is 127.53745810039383
At time: 156.37660789489746 and batch: 150, loss is 4.8018535614013675 and perplexity is 121.73585340359502
At time: 157.2909379005432 and batch: 200, loss is 4.782786073684693 and perplexity is 119.43664622608328
At time: 158.20700550079346 and batch: 250, loss is 4.783573112487793 and perplexity is 119.53068450220732
At time: 159.12021851539612 and batch: 300, loss is 4.77309024810791 and perplexity is 118.28420529821868
At time: 160.04075002670288 and batch: 350, loss is 4.8575355815887455 and perplexity is 128.7066239716402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.181508031384698 and perplexity of 177.95096438991422
Finished 22 epochs...
Completing Train Step...
At time: 161.85021567344666 and batch: 50, loss is 4.844740381240845 and perplexity is 127.07028788051186
At time: 162.76852917671204 and batch: 100, loss is 4.801616439819336 and perplexity is 121.70699062757475
At time: 163.6905608177185 and batch: 150, loss is 4.764613561630249 and perplexity is 117.28578480179304
At time: 164.60629725456238 and batch: 200, loss is 4.76342438697815 and perplexity is 117.1463944156386
At time: 165.5258390903473 and batch: 250, loss is 4.786461935043335 and perplexity is 119.87648667975675
At time: 166.4538631439209 and batch: 300, loss is 4.783013505935669 and perplexity is 119.46381306057357
At time: 167.37444472312927 and batch: 350, loss is 4.855312051773072 and perplexity is 128.42075888831098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.172945746060075 and perplexity of 176.4338019169369
Finished 23 epochs...
Completing Train Step...
At time: 169.13361763954163 and batch: 50, loss is 4.827442016601562 and perplexity is 124.89108239217154
At time: 170.0610592365265 and batch: 100, loss is 4.788816900253296 and perplexity is 120.15912430564545
At time: 170.97653222084045 and batch: 150, loss is 4.7587157821655275 and perplexity is 116.59609492724343
At time: 171.891743183136 and batch: 200, loss is 4.761987962722778 and perplexity is 116.97824329036052
At time: 172.80760788917542 and batch: 250, loss is 4.786877536773682 and perplexity is 119.9263179093041
At time: 173.7453818321228 and batch: 300, loss is 4.780730819702148 and perplexity is 119.19142566485192
At time: 174.66123580932617 and batch: 350, loss is 4.851001749038696 and perplexity is 127.86841777398031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.169635641163793 and perplexity of 175.85075303418478
Finished 24 epochs...
Completing Train Step...
At time: 176.4184513092041 and batch: 50, loss is 4.8197274494171145 and perplexity is 123.9313086259564
At time: 177.34601545333862 and batch: 100, loss is 4.782225742340088 and perplexity is 119.36974087584125
At time: 178.26098561286926 and batch: 150, loss is 4.753388357162476 and perplexity is 115.97658962509252
At time: 179.17592692375183 and batch: 200, loss is 4.758543739318847 and perplexity is 116.57603712861022
At time: 180.09131598472595 and batch: 250, loss is 4.785118560791016 and perplexity is 119.7155558134858
At time: 181.00757575035095 and batch: 300, loss is 4.778357334136963 and perplexity is 118.90886199958325
At time: 181.924946308136 and batch: 350, loss is 4.846429920196533 and perplexity is 127.28515954790939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.167609379209321 and perplexity of 175.49479409846037
Finished 25 epochs...
Completing Train Step...
At time: 183.69554948806763 and batch: 50, loss is 4.813672208786011 and perplexity is 123.18314218044591
At time: 184.61032128334045 and batch: 100, loss is 4.776726789474488 and perplexity is 118.715133773513
At time: 185.53404211997986 and batch: 150, loss is 4.748557672500611 and perplexity is 115.41769430282764
At time: 186.45031094551086 and batch: 200, loss is 4.754730310440063 and perplexity is 116.13232926395091
At time: 187.36768794059753 and batch: 250, loss is 4.782212963104248 and perplexity is 119.36821543151754
At time: 188.28561115264893 and batch: 300, loss is 4.774786901473999 and perplexity is 118.48506293800547
At time: 189.20332217216492 and batch: 350, loss is 4.842621631622315 and perplexity is 126.8013427708229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.165885136045259 and perplexity of 175.19245912381805
Finished 26 epochs...
Completing Train Step...
At time: 190.97104215621948 and batch: 50, loss is 4.808782300949097 and perplexity is 122.58225829884354
At time: 191.8836190700531 and batch: 100, loss is 4.771527252197266 and perplexity is 118.09947197536577
At time: 192.81439352035522 and batch: 150, loss is 4.743696832656861 and perplexity is 114.85802870008732
At time: 193.73587584495544 and batch: 200, loss is 4.750932493209839 and perplexity is 115.69211635608377
At time: 194.65003728866577 and batch: 250, loss is 4.779373703002929 and perplexity is 119.02977870238418
At time: 195.56479954719543 and batch: 300, loss is 4.771841955184937 and perplexity is 118.13664408081834
At time: 196.48129391670227 and batch: 350, loss is 4.838771762847901 and perplexity is 126.31411272838834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.163426366345636 and perplexity of 174.76223034720925
Finished 27 epochs...
Completing Train Step...
At time: 198.25535035133362 and batch: 50, loss is 4.803842639923095 and perplexity is 121.97823655456973
At time: 199.18325686454773 and batch: 100, loss is 4.766596632003784 and perplexity is 117.51860153655763
At time: 200.0993218421936 and batch: 150, loss is 4.739133729934692 and perplexity is 114.33511368147579
At time: 201.0167887210846 and batch: 200, loss is 4.746311769485474 and perplexity is 115.1587682254587
At time: 201.9356827735901 and batch: 250, loss is 4.775501594543457 and perplexity is 118.56977365879834
At time: 202.85176873207092 and batch: 300, loss is 4.7679681396484375 and perplexity is 117.67988977569807
At time: 203.76819324493408 and batch: 350, loss is 4.83493974685669 and perplexity is 125.83100126510513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.161715803475215 and perplexity of 174.46354409845924
Finished 28 epochs...
Completing Train Step...
At time: 205.53595304489136 and batch: 50, loss is 4.799488086700439 and perplexity is 121.4482306384754
At time: 206.4536008834839 and batch: 100, loss is 4.760920000076294 and perplexity is 116.85338158176297
At time: 207.36861419677734 and batch: 150, loss is 4.734178800582885 and perplexity is 113.76999249473671
At time: 208.28225827217102 and batch: 200, loss is 4.742014980316162 and perplexity is 114.6650168099162
At time: 209.20634531974792 and batch: 250, loss is 4.771487846374511 and perplexity is 118.0948182601981
At time: 210.12188482284546 and batch: 300, loss is 4.763664636611939 and perplexity is 117.17454217510571
At time: 211.03939628601074 and batch: 350, loss is 4.83125415802002 and perplexity is 125.36809349937704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.159215992894666 and perplexity of 174.02796294701804
Finished 29 epochs...
Completing Train Step...
At time: 212.8267116546631 and batch: 50, loss is 4.79479341506958 and perplexity is 120.87940733951686
At time: 213.74461317062378 and batch: 100, loss is 4.756414489746094 and perplexity is 116.32808172450963
At time: 214.66156578063965 and batch: 150, loss is 4.7278399085998535 and perplexity is 113.05109770679691
At time: 215.5783953666687 and batch: 200, loss is 4.7368188571929934 and perplexity is 114.07074854727007
At time: 216.5014078617096 and batch: 250, loss is 4.768149271011352 and perplexity is 117.70120722509185
At time: 217.4239444732666 and batch: 300, loss is 4.760365171432495 and perplexity is 116.78856596098083
At time: 218.34268403053284 and batch: 350, loss is 4.827475709915161 and perplexity is 124.8952904574677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.157606453731142 and perplexity of 173.7480834240549
Finished 30 epochs...
Completing Train Step...
At time: 220.10662007331848 and batch: 50, loss is 4.790755586624146 and perplexity is 120.39230111759072
At time: 221.0476450920105 and batch: 100, loss is 4.751276607513428 and perplexity is 115.73193451874111
At time: 221.96197271347046 and batch: 150, loss is 4.722729692459106 and perplexity is 112.47485577698457
At time: 222.87694358825684 and batch: 200, loss is 4.731840467453003 and perplexity is 113.50427114569261
At time: 223.79350471496582 and batch: 250, loss is 4.763673229217529 and perplexity is 117.17554901405757
At time: 224.7086968421936 and batch: 300, loss is 4.755986137390137 and perplexity is 116.27826298738742
At time: 225.62312150001526 and batch: 350, loss is 4.82297716140747 and perplexity is 124.3347047912466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.153794913456358 and perplexity of 173.08709609655384
Finished 31 epochs...
Completing Train Step...
At time: 227.39144110679626 and batch: 50, loss is 4.786520881652832 and perplexity is 119.88355320047657
At time: 228.31840658187866 and batch: 100, loss is 4.747187213897705 and perplexity is 115.25962746750947
At time: 229.23375868797302 and batch: 150, loss is 4.716925439834594 and perplexity is 111.82391424218729
At time: 230.14922666549683 and batch: 200, loss is 4.725668859481812 and perplexity is 112.80592445871
At time: 231.06453585624695 and batch: 250, loss is 4.757833337783813 and perplexity is 116.49325074215551
At time: 231.9887354373932 and batch: 300, loss is 4.751627769470215 and perplexity is 115.77258230789032
At time: 232.9053876399994 and batch: 350, loss is 4.818532266616821 and perplexity is 123.78327653779318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.152196423760776 and perplexity of 172.81063917264348
Finished 32 epochs...
Completing Train Step...
At time: 234.6788465976715 and batch: 50, loss is 4.780754709243775 and perplexity is 119.19427312738904
At time: 235.5942723751068 and batch: 100, loss is 4.740699348449707 and perplexity is 114.51425905243379
At time: 236.52156972885132 and batch: 150, loss is 4.710547189712525 and perplexity is 111.11294313361512
At time: 237.43561625480652 and batch: 200, loss is 4.7194515991210935 and perplexity is 112.1067563633157
At time: 238.3516080379486 and batch: 250, loss is 4.752559032440185 and perplexity is 115.88044724424654
At time: 239.26739811897278 and batch: 300, loss is 4.746696176528931 and perplexity is 115.20304457660582
At time: 240.18275904655457 and batch: 350, loss is 4.813461122512817 and perplexity is 123.1571426542108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.148627445615571 and perplexity of 172.19498106684046
Finished 33 epochs...
Completing Train Step...
At time: 241.94776940345764 and batch: 50, loss is 4.776610956192017 and perplexity is 118.70138340627953
At time: 242.8717498779297 and batch: 100, loss is 4.735544834136963 and perplexity is 113.92551232039784
At time: 243.7854573726654 and batch: 150, loss is 4.705364379882813 and perplexity is 110.53855563606092
At time: 244.71369743347168 and batch: 200, loss is 4.714477338790894 and perplexity is 111.55049281921148
At time: 245.62992000579834 and batch: 250, loss is 4.747968273162842 and perplexity is 115.34968723386821
At time: 246.55916786193848 and batch: 300, loss is 4.7426135540008545 and perplexity is 114.73367281732324
At time: 247.48562240600586 and batch: 350, loss is 4.809364967346191 and perplexity is 122.6537036740226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.145344701306573 and perplexity of 171.63063578014564
Finished 34 epochs...
Completing Train Step...
At time: 249.24091696739197 and batch: 50, loss is 4.771412954330445 and perplexity is 118.08597422904282
At time: 250.17157006263733 and batch: 100, loss is 4.730513038635254 and perplexity is 113.35370226208326
At time: 251.08743405342102 and batch: 150, loss is 4.699767208099365 and perplexity is 109.92158061978013
At time: 252.0075991153717 and batch: 200, loss is 4.709264678955078 and perplexity is 110.97053093086606
At time: 252.93674182891846 and batch: 250, loss is 4.743343114852905 and perplexity is 114.81740855486667
At time: 253.85313153266907 and batch: 300, loss is 4.738202962875366 and perplexity is 114.22874383439095
At time: 254.76763486862183 and batch: 350, loss is 4.804378499984741 and perplexity is 122.0436173358396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.144044547245421 and perplexity of 171.40763451143388
Finished 35 epochs...
Completing Train Step...
At time: 256.53652691841125 and batch: 50, loss is 4.766818027496338 and perplexity is 117.54462250558541
At time: 257.45104813575745 and batch: 100, loss is 4.725653467178344 and perplexity is 112.8041881290509
At time: 258.37575912475586 and batch: 150, loss is 4.694592933654786 and perplexity is 109.35428513123432
At time: 259.29310870170593 and batch: 200, loss is 4.70425630569458 and perplexity is 110.41613855188724
At time: 260.2106809616089 and batch: 250, loss is 4.738665075302124 and perplexity is 114.28154255494258
At time: 261.12661051750183 and batch: 300, loss is 4.734605417251587 and perplexity is 113.81853902457618
At time: 262.0424780845642 and batch: 350, loss is 4.80123459815979 and perplexity is 121.66052669979142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.142199812264278 and perplexity of 171.09172432688797
Finished 36 epochs...
Completing Train Step...
At time: 263.8123548030853 and batch: 50, loss is 4.762993984222412 and perplexity is 117.09598513358249
At time: 264.7326183319092 and batch: 100, loss is 4.720797700881958 and perplexity is 112.25776507920816
At time: 265.6537754535675 and batch: 150, loss is 4.6892321681976314 and perplexity is 108.76963095400905
At time: 266.5689957141876 and batch: 200, loss is 4.6990015411376955 and perplexity is 109.83744950944103
At time: 267.50826382637024 and batch: 250, loss is 4.733949680328369 and perplexity is 113.74392847111186
At time: 268.42579317092896 and batch: 300, loss is 4.7309317207336425 and perplexity is 113.40117136454472
At time: 269.3452339172363 and batch: 350, loss is 4.798266220092773 and perplexity is 121.2999277225394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.14019407075027 and perplexity of 170.7489024736448
Finished 37 epochs...
Completing Train Step...
At time: 271.1117351055145 and batch: 50, loss is 4.758796234130859 and perplexity is 116.60547568957561
At time: 272.03915905952454 and batch: 100, loss is 4.716280069351196 and perplexity is 111.75176967108972
At time: 272.9518573284149 and batch: 150, loss is 4.683978118896484 and perplexity is 108.19964861935053
At time: 273.867066860199 and batch: 200, loss is 4.693561201095581 and perplexity is 109.24151893705597
At time: 274.78428173065186 and batch: 250, loss is 4.729339246749878 and perplexity is 113.22072666429715
At time: 275.70121908187866 and batch: 300, loss is 4.726774120330811 and perplexity is 112.93067335788703
At time: 276.6271917819977 and batch: 350, loss is 4.79440203666687 and perplexity is 120.83210700691991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.137834746262123 and perplexity of 170.34652526256806
Finished 38 epochs...
Completing Train Step...
At time: 278.3870484828949 and batch: 50, loss is 4.755266342163086 and perplexity is 116.19459656363794
At time: 279.3147509098053 and batch: 100, loss is 4.712021837234497 and perplexity is 111.27691643156038
At time: 280.2281632423401 and batch: 150, loss is 4.679739789962769 and perplexity is 107.74203336500393
At time: 281.1512107849121 and batch: 200, loss is 4.688768653869629 and perplexity is 108.71922635413989
At time: 282.0776150226593 and batch: 250, loss is 4.725534524917602 and perplexity is 112.79077174179722
At time: 282.99388313293457 and batch: 300, loss is 4.72273473739624 and perplexity is 112.47542320699243
At time: 283.9077491760254 and batch: 350, loss is 4.7908744049072265 and perplexity is 120.4066067739756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.134985035863416 and perplexity of 169.861778020818
Finished 39 epochs...
Completing Train Step...
At time: 285.6770169734955 and batch: 50, loss is 4.751354026794433 and perplexity is 115.74089474874374
At time: 286.5909628868103 and batch: 100, loss is 4.708253116607666 and perplexity is 110.85833407672781
At time: 287.5147178173065 and batch: 150, loss is 4.67646861076355 and perplexity is 107.3901656914527
At time: 288.4306583404541 and batch: 200, loss is 4.685683393478394 and perplexity is 108.38431613957633
At time: 289.3462793827057 and batch: 250, loss is 4.722036628723145 and perplexity is 112.39693053993521
At time: 290.26200675964355 and batch: 300, loss is 4.719221792221069 and perplexity is 112.08099641718414
At time: 291.1994502544403 and batch: 350, loss is 4.787657594680786 and perplexity is 120.01990387835116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.133162399818158 and perplexity of 169.5524637907467
Finished 40 epochs...
Completing Train Step...
At time: 292.97032952308655 and batch: 50, loss is 4.748300085067749 and perplexity is 115.38796798397279
At time: 293.8851969242096 and batch: 100, loss is 4.7040582942962645 and perplexity is 110.39427706237949
At time: 294.7983138561249 and batch: 150, loss is 4.672689218521118 and perplexity is 106.9850621371418
At time: 295.71562790870667 and batch: 200, loss is 4.682796955108643 and perplexity is 108.07192256009185
At time: 296.64438223838806 and batch: 250, loss is 4.71926329612732 and perplexity is 112.08564831288714
At time: 297.55935764312744 and batch: 300, loss is 4.715293016433716 and perplexity is 111.64151918128725
At time: 298.4722092151642 and batch: 350, loss is 4.784133405685425 and perplexity is 119.59767549718902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.131356601057382 and perplexity of 169.2465624430106
Finished 41 epochs...
Completing Train Step...
At time: 300.2228949069977 and batch: 50, loss is 4.744704818725586 and perplexity is 114.9738623624496
At time: 301.14798951148987 and batch: 100, loss is 4.7007067775726314 and perplexity is 110.02490811546265
At time: 302.06157088279724 and batch: 150, loss is 4.669082593917847 and perplexity is 106.59990216108345
At time: 302.97686195373535 and batch: 200, loss is 4.679502849578857 and perplexity is 107.71650795037523
At time: 303.89490127563477 and batch: 250, loss is 4.716231632232666 and perplexity is 111.74635686846753
At time: 304.8119492530823 and batch: 300, loss is 4.711715755462646 and perplexity is 111.24286180782858
At time: 305.72889590263367 and batch: 350, loss is 4.779737815856934 and perplexity is 119.07312686615194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.129631831728179 and perplexity of 168.95490275817843
Finished 42 epochs...
Completing Train Step...
At time: 307.50344824790955 and batch: 50, loss is 4.741440286636353 and perplexity is 114.59913848120307
At time: 308.4176709651947 and batch: 100, loss is 4.697592849731445 and perplexity is 109.68283136842474
At time: 309.33527970314026 and batch: 150, loss is 4.665918626785278 and perplexity is 106.26315658120535
At time: 310.24971437454224 and batch: 200, loss is 4.676681337356567 and perplexity is 107.41301286553839
At time: 311.16377902030945 and batch: 250, loss is 4.71215784072876 and perplexity is 111.29205151021434
At time: 312.0814733505249 and batch: 300, loss is 4.707866373062134 and perplexity is 110.8154686210587
At time: 313.0026252269745 and batch: 350, loss is 4.77621208190918 and perplexity is 118.65404591858244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.128816275761046 and perplexity of 168.8171667524027
Finished 43 epochs...
Completing Train Step...
At time: 314.78794384002686 and batch: 50, loss is 4.738640575408936 and perplexity is 114.27874270365479
At time: 315.7063875198364 and batch: 100, loss is 4.694266138076782 and perplexity is 109.3185544730462
At time: 316.62355160713196 and batch: 150, loss is 4.662084760665894 and perplexity is 105.85653782444537
At time: 317.54298400878906 and batch: 200, loss is 4.671979122161865 and perplexity is 106.90911940054525
At time: 318.4678223133087 and batch: 250, loss is 4.707854862213135 and perplexity is 110.81419304827409
At time: 319.38559222221375 and batch: 300, loss is 4.703875331878662 and perplexity is 110.37408090618383
At time: 320.3019824028015 and batch: 350, loss is 4.772345924377442 and perplexity is 118.19619631492623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.124904764109645 and perplexity of 168.1581262003655
Finished 44 epochs...
Completing Train Step...
At time: 322.0590269565582 and batch: 50, loss is 4.734512853622436 and perplexity is 113.80800405512444
At time: 322.9912805557251 and batch: 100, loss is 4.689809331893921 and perplexity is 108.83242695629767
At time: 323.9115951061249 and batch: 150, loss is 4.656541900634766 and perplexity is 105.27141298253397
At time: 324.82650351524353 and batch: 200, loss is 4.667080888748169 and perplexity is 106.38673400691579
At time: 325.7415888309479 and batch: 250, loss is 4.703084545135498 and perplexity is 110.28683304798577
At time: 326.65568923950195 and batch: 300, loss is 4.69771487236023 and perplexity is 109.6962159724365
At time: 327.57060146331787 and batch: 350, loss is 4.7665400791168215 and perplexity is 117.51195570829107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.120080355940194 and perplexity of 167.34881655232093
Finished 45 epochs...
Completing Train Step...
At time: 329.3280050754547 and batch: 50, loss is 4.728077983856201 and perplexity is 113.07801557997645
At time: 330.25499415397644 and batch: 100, loss is 4.682080411911011 and perplexity is 107.99451209640915
At time: 331.1693642139435 and batch: 150, loss is 4.648279647827149 and perplexity is 104.40521724668497
At time: 332.0858187675476 and batch: 200, loss is 4.658039865493774 and perplexity is 105.42922402803089
At time: 333.00153160095215 and batch: 250, loss is 4.693352375030518 and perplexity is 109.21870884226888
At time: 333.91743445396423 and batch: 300, loss is 4.685980033874512 and perplexity is 108.41647207518788
At time: 334.8326983451843 and batch: 350, loss is 4.754435482025147 and perplexity is 116.09809520022881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.112543040308459 and perplexity of 166.0921974180475
Finished 46 epochs...
Completing Train Step...
At time: 336.609210729599 and batch: 50, loss is 4.718612003326416 and perplexity is 112.01267150427098
At time: 337.5287833213806 and batch: 100, loss is 4.672350444793701 and perplexity is 106.94882454738196
At time: 338.462651014328 and batch: 150, loss is 4.638021831512451 and perplexity is 103.33972187661792
At time: 339.3799328804016 and batch: 200, loss is 4.650218029022216 and perplexity is 104.60779062524892
At time: 340.2962577342987 and batch: 250, loss is 4.684439449310303 and perplexity is 108.2495759236296
At time: 341.22101163864136 and batch: 300, loss is 4.678574132919311 and perplexity is 107.61651627408843
At time: 342.1369774341583 and batch: 350, loss is 4.747934017181397 and perplexity is 115.34573588480167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.107872272359914 and perplexity of 165.3182282281666
Finished 47 epochs...
Completing Train Step...
At time: 343.9063482284546 and batch: 50, loss is 4.7114106559753415 and perplexity is 111.20892684475785
At time: 344.8224377632141 and batch: 100, loss is 4.666180658340454 and perplexity is 106.29100452974347
At time: 345.7474808692932 and batch: 150, loss is 4.632547969818115 and perplexity is 102.77559990293798
At time: 346.66237235069275 and batch: 200, loss is 4.645022563934326 and perplexity is 104.06571389085778
At time: 347.58236837387085 and batch: 250, loss is 4.677899942398072 and perplexity is 107.54398669101356
At time: 348.4977157115936 and batch: 300, loss is 4.672714595794678 and perplexity is 106.98777716078027
At time: 349.4145727157593 and batch: 350, loss is 4.741270475387573 and perplexity is 114.57967991057734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.103877100451239 and perplexity of 164.65907108656094
Finished 48 epochs...
Completing Train Step...
At time: 351.1653311252594 and batch: 50, loss is 4.706458683013916 and perplexity is 110.6595845326737
At time: 352.1006133556366 and batch: 100, loss is 4.662191495895386 and perplexity is 105.86783704930525
At time: 353.0200400352478 and batch: 150, loss is 4.628570909500122 and perplexity is 102.3676668677393
At time: 353.93264961242676 and batch: 200, loss is 4.640604496002197 and perplexity is 103.60695864960832
At time: 354.85543155670166 and batch: 250, loss is 4.672693071365356 and perplexity is 106.9854743347161
At time: 355.7712731361389 and batch: 300, loss is 4.66748477935791 and perplexity is 106.42971128825815
At time: 356.6873199939728 and batch: 350, loss is 4.736426916122436 and perplexity is 114.02604829646951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.102521699050377 and perplexity of 164.43604313129103
Finished 49 epochs...
Completing Train Step...
At time: 358.515132188797 and batch: 50, loss is 4.7021173572540285 and perplexity is 110.18021652699085
At time: 359.4293987751007 and batch: 100, loss is 4.657410936355591 and perplexity is 105.36293736401583
At time: 360.3535485267639 and batch: 150, loss is 4.623959970474243 and perplexity is 101.89674233409666
At time: 361.26634979248047 and batch: 200, loss is 4.635646390914917 and perplexity is 103.09453583366745
At time: 362.1935193538666 and batch: 250, loss is 4.6688022041320805 and perplexity is 106.57001682732074
At time: 363.107403755188 and batch: 300, loss is 4.663425760269165 and perplexity is 105.99858662207528
At time: 364.02251410484314 and batch: 350, loss is 4.731905403137207 and perplexity is 113.51164186250831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.101941338900862 and perplexity of 164.34063869185889
Finished Training.
Improved accuracyfrom -10000000 to -164.34063869185889
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73cc4be8d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.0847376931910826, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 2.2344801255126594, 'data': 'ptb', 'lr': 16.129247394125382}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1454882621765137 and batch: 50, loss is 6.667200174331665 and perplexity is 786.1913214564779
At time: 2.0970585346221924 and batch: 100, loss is 5.73686445236206 and perplexity is 310.09058124492145
At time: 3.0294759273529053 and batch: 150, loss is 5.43470232963562 and perplexity is 229.22460474759427
At time: 3.956989049911499 and batch: 200, loss is 5.36115385055542 and perplexity is 212.97054090956314
At time: 4.880793809890747 and batch: 250, loss is 5.348414144515991 and perplexity is 210.27456823594977
At time: 5.8053505420684814 and batch: 300, loss is 5.350110340118408 and perplexity is 210.6315376933178
At time: 6.737400054931641 and batch: 350, loss is 5.396511726379394 and perplexity is 220.63543552067696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.475759571996228 and perplexity of 238.83180800430878
Finished 1 epochs...
Completing Train Step...
At time: 8.52553415298462 and batch: 50, loss is 5.306204996109009 and perplexity is 201.5837637720603
At time: 9.47316598892212 and batch: 100, loss is 5.2614788246154784 and perplexity is 192.76634823941566
At time: 10.410129308700562 and batch: 150, loss is 5.208831396102905 and perplexity is 182.88021873492607
At time: 11.342812538146973 and batch: 200, loss is 5.206050596237183 and perplexity is 182.37237188470962
At time: 12.278401136398315 and batch: 250, loss is 5.22644700050354 and perplexity is 186.13030640621076
At time: 13.21002721786499 and batch: 300, loss is 5.207251977920532 and perplexity is 182.59160237522056
At time: 14.150844097137451 and batch: 350, loss is 5.2768580436706545 and perplexity is 195.753858035568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.468673179889548 and perplexity of 237.14533472815546
Finished 2 epochs...
Completing Train Step...
At time: 16.024632930755615 and batch: 50, loss is 5.250039215087891 and perplexity is 190.57374167812713
At time: 16.9495747089386 and batch: 100, loss is 5.196752119064331 and perplexity is 180.68444627836334
At time: 17.88759970664978 and batch: 150, loss is 5.168908815383912 and perplexity is 175.72298661105776
At time: 18.82254981994629 and batch: 200, loss is 5.164432954788208 and perplexity is 174.93823255450803
At time: 19.759835243225098 and batch: 250, loss is 5.182286787033081 and perplexity is 178.0895986826809
At time: 20.694870710372925 and batch: 300, loss is 5.183995742797851 and perplexity is 178.39420613519167
At time: 21.62750244140625 and batch: 350, loss is 5.252067642211914 and perplexity is 190.96069894950773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.436758633317618 and perplexity of 229.69644510361184
Finished 3 epochs...
Completing Train Step...
At time: 23.468470811843872 and batch: 50, loss is 5.2274551105499265 and perplexity is 186.31804085062632
At time: 24.39322590827942 and batch: 100, loss is 5.1774275016784665 and perplexity is 177.22630968649236
At time: 25.317999601364136 and batch: 150, loss is 5.157378883361816 and perplexity is 173.70854800725576
At time: 26.252222299575806 and batch: 200, loss is 5.145440607070923 and perplexity is 171.64709693680325
At time: 27.17817783355713 and batch: 250, loss is 5.169340791702271 and perplexity is 175.7989111774918
At time: 28.109337091445923 and batch: 300, loss is 5.163790340423584 and perplexity is 174.82585084626396
At time: 29.037433385849 and batch: 350, loss is 5.2312296199798585 and perplexity is 187.02262895348528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.474420481714709 and perplexity of 238.51220468789847
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 30.817803144454956 and batch: 50, loss is 5.100753269195557 and perplexity is 164.14550649595637
At time: 31.74836015701294 and batch: 100, loss is 4.939408464431763 and perplexity is 139.68759493499226
At time: 32.66351389884949 and batch: 150, loss is 4.888995800018311 and perplexity is 132.8201290885835
At time: 33.578460454940796 and batch: 200, loss is 4.858034410476685 and perplexity is 128.7708425694606
At time: 34.50114846229553 and batch: 250, loss is 4.846763687133789 and perplexity is 127.32765021636554
At time: 35.42395806312561 and batch: 300, loss is 4.844567461013794 and perplexity is 127.04831675716068
At time: 36.343942403793335 and batch: 350, loss is 4.924586534500122 and perplexity is 137.63242361801906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.181694820009429 and perplexity of 177.98420671036902
Finished 5 epochs...
Completing Train Step...
At time: 38.172438621520996 and batch: 50, loss is 4.8926526927948 and perplexity is 133.3067272355769
At time: 39.09934973716736 and batch: 100, loss is 4.842366809844971 and perplexity is 126.76903514380673
At time: 40.02599787712097 and batch: 150, loss is 4.8170293426513675 and perplexity is 123.59737941457466
At time: 40.94038963317871 and batch: 200, loss is 4.824914474487304 and perplexity is 124.57581351724983
At time: 41.85524106025696 and batch: 250, loss is 4.823332576751709 and perplexity is 124.37890310706537
At time: 42.77018690109253 and batch: 300, loss is 4.810298862457276 and perplexity is 122.76830287172636
At time: 43.69689440727234 and batch: 350, loss is 4.88122218132019 and perplexity is 131.79163877460905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.163992520036368 and perplexity of 174.86120064247632
Finished 6 epochs...
Completing Train Step...
At time: 45.4945764541626 and batch: 50, loss is 4.867481651306153 and perplexity is 129.9931362917747
At time: 46.412630558013916 and batch: 100, loss is 4.819207696914673 and perplexity is 123.86691175483438
At time: 47.351901054382324 and batch: 150, loss is 4.800095624923706 and perplexity is 121.52203749871063
At time: 48.273584604263306 and batch: 200, loss is 4.795545148849487 and perplexity is 120.97031063656787
At time: 49.19343900680542 and batch: 250, loss is 4.792009773254395 and perplexity is 120.54339025922299
At time: 50.119279861450195 and batch: 300, loss is 4.782887115478515 and perplexity is 119.44871492877753
At time: 51.0412814617157 and batch: 350, loss is 4.854896841049194 and perplexity is 128.367448280386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.149681880556304 and perplexity of 172.37664523118553
Finished 7 epochs...
Completing Train Step...
At time: 52.797526597976685 and batch: 50, loss is 4.8327805709838865 and perplexity is 125.55960310673967
At time: 53.728468894958496 and batch: 100, loss is 4.79094295501709 and perplexity is 120.41486094300707
At time: 54.656270027160645 and batch: 150, loss is 4.770552110671997 and perplexity is 117.98436440833589
At time: 55.57670783996582 and batch: 200, loss is 4.770228214263916 and perplexity is 117.94615588463014
At time: 56.494866371154785 and batch: 250, loss is 4.768167266845703 and perplexity is 117.70332537557884
At time: 57.421584367752075 and batch: 300, loss is 4.760529451370239 and perplexity is 116.80775355535135
At time: 58.34713053703308 and batch: 350, loss is 4.824815330505371 and perplexity is 124.56346318728568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.148476962385507 and perplexity of 172.16907055948593
Finished 8 epochs...
Completing Train Step...
At time: 60.174622535705566 and batch: 50, loss is 4.8101699256896975 and perplexity is 122.75247454404163
At time: 61.10433483123779 and batch: 100, loss is 4.768973140716553 and perplexity is 117.79821764047408
At time: 62.03434896469116 and batch: 150, loss is 4.748566637039184 and perplexity is 115.41872897383797
At time: 62.95186400413513 and batch: 200, loss is 4.747667551040649 and perplexity is 115.31500424636118
At time: 63.86907243728638 and batch: 250, loss is 4.746252346038818 and perplexity is 115.1519252978553
At time: 64.78521275520325 and batch: 300, loss is 4.735999088287354 and perplexity is 113.9772752130691
At time: 65.70204138755798 and batch: 350, loss is 4.801868000030518 and perplexity is 121.73761111512601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.142057747676454 and perplexity of 171.06741997802598
Finished 9 epochs...
Completing Train Step...
At time: 67.46920204162598 and batch: 50, loss is 4.783854904174805 and perplexity is 119.56437200164865
At time: 68.38407492637634 and batch: 100, loss is 4.745493869781495 and perplexity is 115.06461841081777
At time: 69.31842231750488 and batch: 150, loss is 4.722573518753052 and perplexity is 112.45729153349214
At time: 70.25627017021179 and batch: 200, loss is 4.728056955337524 and perplexity is 113.07563774181521
At time: 71.1916663646698 and batch: 250, loss is 4.729251098632813 and perplexity is 113.21074691028366
At time: 72.10757398605347 and batch: 300, loss is 4.713427581787109 and perplexity is 111.43345335034805
At time: 73.02810406684875 and batch: 350, loss is 4.780017642974854 and perplexity is 119.10645141840385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.13836406839305 and perplexity of 170.43671731653524
Finished 10 epochs...
Completing Train Step...
At time: 74.81709909439087 and batch: 50, loss is 4.763134298324585 and perplexity is 117.1124165043541
At time: 75.73936438560486 and batch: 100, loss is 4.7263545894622805 and perplexity is 112.88330539126632
At time: 76.66236591339111 and batch: 150, loss is 4.703441143035889 and perplexity is 110.32616811407556
At time: 77.58521342277527 and batch: 200, loss is 4.702539014816284 and perplexity is 110.22668464462852
At time: 78.50822353363037 and batch: 250, loss is 4.704305276870728 and perplexity is 110.42154589245868
At time: 79.43194508552551 and batch: 300, loss is 4.687190132141113 and perplexity is 108.54774607131557
At time: 80.35509729385376 and batch: 350, loss is 4.75519549369812 and perplexity is 116.18636464644688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.1337432861328125 and perplexity of 169.65098310807008
Finished 11 epochs...
Completing Train Step...
At time: 82.12216329574585 and batch: 50, loss is 4.74087984085083 and perplexity is 114.53492987141982
At time: 83.05840063095093 and batch: 100, loss is 4.709270286560058 and perplexity is 110.97115321151273
At time: 83.97889637947083 and batch: 150, loss is 4.679034337997437 and perplexity is 107.66605333909808
At time: 84.89986276626587 and batch: 200, loss is 4.687102031707764 and perplexity is 108.53818338909198
At time: 85.82759404182434 and batch: 250, loss is 4.689682912826538 and perplexity is 108.81866933201222
At time: 86.74917435646057 and batch: 300, loss is 4.671192531585693 and perplexity is 106.82505875970612
At time: 87.68470573425293 and batch: 350, loss is 4.735021419525147 and perplexity is 113.86589764554765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.127033102101293 and perplexity of 168.51640466254858
Finished 12 epochs...
Completing Train Step...
At time: 89.48298788070679 and batch: 50, loss is 4.721319074630737 and perplexity is 112.31630859121611
At time: 90.41351175308228 and batch: 100, loss is 4.684948329925537 and perplexity is 108.30467605292924
At time: 91.351731300354 and batch: 150, loss is 4.660691776275635 and perplexity is 105.70918397428278
At time: 92.29102230072021 and batch: 200, loss is 4.669592676162719 and perplexity is 106.65429074862811
At time: 93.21573805809021 and batch: 250, loss is 4.672451772689819 and perplexity is 106.9596619958243
At time: 94.15199112892151 and batch: 300, loss is 4.658201866149902 and perplexity is 105.44630501502668
At time: 95.10888862609863 and batch: 350, loss is 4.720965766906739 and perplexity is 112.27663338105162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.120093510068696 and perplexity of 167.35101789463684
Finished 13 epochs...
Completing Train Step...
At time: 96.90220165252686 and batch: 50, loss is 4.703632946014404 and perplexity is 110.34733103121846
At time: 97.83366346359253 and batch: 100, loss is 4.671142921447754 and perplexity is 106.81975928526063
At time: 98.75565266609192 and batch: 150, loss is 4.646972484588623 and perplexity is 104.26883174326788
At time: 99.68560147285461 and batch: 200, loss is 4.655753164291382 and perplexity is 105.18841432952516
At time: 100.6127679347992 and batch: 250, loss is 4.656393060684204 and perplexity is 105.25574555662845
At time: 101.53567814826965 and batch: 300, loss is 4.636394786834717 and perplexity is 103.1717202422847
At time: 102.45918822288513 and batch: 350, loss is 4.704121141433716 and perplexity is 110.40121524470406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.118480813914332 and perplexity of 167.0813490570439
Finished 14 epochs...
Completing Train Step...
At time: 104.21124529838562 and batch: 50, loss is 4.689162845611572 and perplexity is 108.76209102325339
At time: 105.14043688774109 and batch: 100, loss is 4.657871360778809 and perplexity is 105.41146020337317
At time: 106.05669951438904 and batch: 150, loss is 4.628319540023804 and perplexity is 102.34193799478895
At time: 106.97171354293823 and batch: 200, loss is 4.638410205841065 and perplexity is 103.37986416633369
At time: 107.88664102554321 and batch: 250, loss is 4.642416143417359 and perplexity is 103.79482805361539
At time: 108.80037117004395 and batch: 300, loss is 4.624639949798584 and perplexity is 101.9660535745361
At time: 109.71556448936462 and batch: 350, loss is 4.693899545669556 and perplexity is 109.27848646576982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.126581126245959 and perplexity of 168.44025652626934
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 111.45913457870483 and batch: 50, loss is 4.634776067733765 and perplexity is 103.00484930307579
At time: 112.38780641555786 and batch: 100, loss is 4.542315969467163 and perplexity is 93.90803661134636
At time: 113.30567502975464 and batch: 150, loss is 4.483008213043213 and perplexity is 88.50050100251374
At time: 114.23008871078491 and batch: 200, loss is 4.480163221359253 and perplexity is 88.24907563426208
At time: 115.15373253822327 and batch: 250, loss is 4.480137701034546 and perplexity is 88.24682351793432
At time: 116.07367324829102 and batch: 300, loss is 4.47484489440918 and perplexity is 87.78098403240284
At time: 116.98809719085693 and batch: 350, loss is 4.561234292984008 and perplexity is 95.70153068829333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.01837473902209 and perplexity of 151.16542077654304
Finished 16 epochs...
Completing Train Step...
At time: 118.74783492088318 and batch: 50, loss is 4.542178907394409 and perplexity is 93.89516626323862
At time: 119.66152834892273 and batch: 100, loss is 4.492776155471802 and perplexity is 89.36920461755507
At time: 120.57482528686523 and batch: 150, loss is 4.461141757965088 and perplexity is 86.58631329322658
At time: 121.48902249336243 and batch: 200, loss is 4.470209693908691 and perplexity is 87.375043106879
At time: 122.40770053863525 and batch: 250, loss is 4.482571611404419 and perplexity is 88.46186997254107
At time: 123.32256507873535 and batch: 300, loss is 4.46518871307373 and perplexity is 86.93743422224897
At time: 124.24500632286072 and batch: 350, loss is 4.545448875427246 and perplexity is 94.20270299897179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.014724205280173 and perplexity of 150.61459232801883
Finished 17 epochs...
Completing Train Step...
At time: 126.00948166847229 and batch: 50, loss is 4.527635145187378 and perplexity is 92.53945972608398
At time: 126.92485451698303 and batch: 100, loss is 4.4867104816436765 and perplexity is 88.82876090671957
At time: 127.83933210372925 and batch: 150, loss is 4.453634376525879 and perplexity is 85.93871075317638
At time: 128.75435781478882 and batch: 200, loss is 4.463272886276245 and perplexity is 86.77103660146723
At time: 129.6713626384735 and batch: 250, loss is 4.475583915710449 and perplexity is 87.8458800262565
At time: 130.58636331558228 and batch: 300, loss is 4.45869722366333 and perplexity is 86.37490857863189
At time: 131.50696873664856 and batch: 350, loss is 4.537393817901611 and perplexity is 93.4469427398146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.015321402714171 and perplexity of 150.7045658393808
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 133.26135659217834 and batch: 50, loss is 4.500868425369263 and perplexity is 90.09533841461736
At time: 134.1860785484314 and batch: 100, loss is 4.436056842803955 and perplexity is 84.4413189482255
At time: 135.0989966392517 and batch: 150, loss is 4.378839035034179 and perplexity is 79.74539802897911
At time: 136.02320528030396 and batch: 200, loss is 4.388163976669311 and perplexity is 80.49249712537437
At time: 136.94623804092407 and batch: 250, loss is 4.399100837707519 and perplexity is 81.3776640315299
At time: 137.8602933883667 and batch: 300, loss is 4.3869784069061275 and perplexity is 80.39712420141795
At time: 138.78614902496338 and batch: 350, loss is 4.476112747192383 and perplexity is 87.89234797894778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.972115352235991 and perplexity of 144.3318774361421
Finished 19 epochs...
Completing Train Step...
At time: 140.63715529441833 and batch: 50, loss is 4.466919040679931 and perplexity is 87.08799468659468
At time: 141.5739562511444 and batch: 100, loss is 4.418890781402588 and perplexity is 83.00416451377065
At time: 142.54192638397217 and batch: 150, loss is 4.3666750431060795 and perplexity is 78.78125147461849
At time: 143.46486067771912 and batch: 200, loss is 4.384035596847534 and perplexity is 80.16087851921417
At time: 144.38923025131226 and batch: 250, loss is 4.400114192962646 and perplexity is 81.46017031205228
At time: 145.31160831451416 and batch: 300, loss is 4.385685930252075 and perplexity is 80.29327991794663
At time: 146.2324879169464 and batch: 350, loss is 4.468977975845337 and perplexity is 87.2674879404165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.969144097689925 and perplexity of 143.90366716505753
Finished 20 epochs...
Completing Train Step...
At time: 148.03158378601074 and batch: 50, loss is 4.456820917129517 and perplexity is 86.21299472082474
At time: 148.94612789154053 and batch: 100, loss is 4.413586177825928 and perplexity is 82.56502608319386
At time: 149.86739373207092 and batch: 150, loss is 4.362521505355835 and perplexity is 78.45470919490548
At time: 150.7841193675995 and batch: 200, loss is 4.380852012634278 and perplexity is 79.90608540470653
At time: 151.69938921928406 and batch: 250, loss is 4.396842660903931 and perplexity is 81.19410620929418
At time: 152.61670923233032 and batch: 300, loss is 4.381484289169311 and perplexity is 79.9566241230535
At time: 153.5406732559204 and batch: 350, loss is 4.46301347732544 and perplexity is 86.74853033719155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.969139888368804 and perplexity of 143.9030614295868
Finished 21 epochs...
Completing Train Step...
At time: 155.30695223808289 and batch: 50, loss is 4.450982666015625 and perplexity is 85.71112804571841
At time: 156.23546433448792 and batch: 100, loss is 4.4086230373382564 and perplexity is 82.15625948149741
At time: 157.150723695755 and batch: 150, loss is 4.358212442398071 and perplexity is 78.1173702430188
At time: 158.0662546157837 and batch: 200, loss is 4.376442813873291 and perplexity is 79.55453918006887
At time: 158.98455357551575 and batch: 250, loss is 4.393275518417358 and perplexity is 80.90499122711425
At time: 159.90243315696716 and batch: 300, loss is 4.377589254379273 and perplexity is 79.64579602649708
At time: 160.81965041160583 and batch: 350, loss is 4.457997179031372 and perplexity is 86.31446344715889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.96954345703125 and perplexity of 143.96114791576431
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 162.6148064136505 and batch: 50, loss is 4.440310974121093 and perplexity is 84.80130858630699
At time: 163.54486107826233 and batch: 100, loss is 4.384470348358154 and perplexity is 80.19573615889976
At time: 164.46239948272705 and batch: 150, loss is 4.326105298995972 and perplexity is 75.64908152089133
At time: 165.39776134490967 and batch: 200, loss is 4.339024066925049 and perplexity is 76.63271443154679
At time: 166.3184516429901 and batch: 250, loss is 4.356192960739135 and perplexity is 77.95977283266946
At time: 167.24230694770813 and batch: 300, loss is 4.339289999008178 and perplexity is 76.65309623889956
At time: 168.1707398891449 and batch: 350, loss is 4.4311721229553225 and perplexity is 84.0298525287563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.947246683054957 and perplexity of 140.78679911667845
Finished 23 epochs...
Completing Train Step...
At time: 169.9599883556366 and batch: 50, loss is 4.425406608581543 and perplexity is 83.54677115067093
At time: 170.87766242027283 and batch: 100, loss is 4.372267055511474 and perplexity is 79.22303127802151
At time: 171.79351139068604 and batch: 150, loss is 4.31976845741272 and perplexity is 75.171220937844
At time: 172.7091407775879 and batch: 200, loss is 4.337560863494873 and perplexity is 76.52066717488276
At time: 173.62357258796692 and batch: 250, loss is 4.358709440231324 and perplexity is 78.1562040561301
At time: 174.5383062362671 and batch: 300, loss is 4.34012698173523 and perplexity is 76.71728041320219
At time: 175.4527223110199 and batch: 350, loss is 4.428730421066284 and perplexity is 83.82492696448217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9454019480738145 and perplexity of 140.52732418828714
Finished 24 epochs...
Completing Train Step...
At time: 177.20942878723145 and batch: 50, loss is 4.417966785430909 and perplexity is 82.92750442238928
At time: 178.12571597099304 and batch: 100, loss is 4.367090482711792 and perplexity is 78.81398712604084
At time: 179.0546634197235 and batch: 150, loss is 4.316723318099975 and perplexity is 74.94266227088644
At time: 179.9852089881897 and batch: 200, loss is 4.336217575073242 and perplexity is 76.41794685560495
At time: 180.90977907180786 and batch: 250, loss is 4.358677034378052 and perplexity is 78.15367137868617
At time: 181.8327193260193 and batch: 300, loss is 4.339394569396973 and perplexity is 76.66111230208968
At time: 182.74710631370544 and batch: 350, loss is 4.4264430332183835 and perplexity is 83.6334059700955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.944680049501616 and perplexity of 140.42591432182368
Finished 25 epochs...
Completing Train Step...
At time: 184.50280857086182 and batch: 50, loss is 4.412512102127075 and perplexity is 82.4763926031572
At time: 185.42972350120544 and batch: 100, loss is 4.362438364028931 and perplexity is 78.44818663743195
At time: 186.34559798240662 and batch: 150, loss is 4.313531150817871 and perplexity is 74.70381418070143
At time: 187.26148462295532 and batch: 200, loss is 4.334670724868775 and perplexity is 76.29983111611604
At time: 188.177401304245 and batch: 250, loss is 4.357716026306153 and perplexity is 78.07860114696955
At time: 189.11266326904297 and batch: 300, loss is 4.337677993774414 and perplexity is 76.52963058695288
At time: 190.0277807712555 and batch: 350, loss is 4.424508190155029 and perplexity is 83.47174489952315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.944471688106142 and perplexity of 140.39665803040157
Finished 26 epochs...
Completing Train Step...
At time: 191.78606462478638 and batch: 50, loss is 4.40836088180542 and perplexity is 82.13472458638645
At time: 192.70091223716736 and batch: 100, loss is 4.358948307037354 and perplexity is 78.17487520883495
At time: 193.61656832695007 and batch: 150, loss is 4.311133146286011 and perplexity is 74.52488871353087
At time: 194.5326030254364 and batch: 200, loss is 4.332815656661987 and perplexity is 76.15842092855344
At time: 195.4578354358673 and batch: 250, loss is 4.356565036773682 and perplexity is 77.98878519286643
At time: 196.37372660636902 and batch: 300, loss is 4.335943946838379 and perplexity is 76.39703960823203
At time: 197.28935146331787 and batch: 350, loss is 4.4224434661865235 and perplexity is 83.29957658831951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.943609829606681 and perplexity of 140.2757081057292
Finished 27 epochs...
Completing Train Step...
At time: 199.0477545261383 and batch: 50, loss is 4.40442274093628 and perplexity is 81.81190254726933
At time: 199.96309876441956 and batch: 100, loss is 4.355444688796997 and perplexity is 77.9014595418638
At time: 200.87763166427612 and batch: 150, loss is 4.30822681427002 and perplexity is 74.30860908591623
At time: 201.79286932945251 and batch: 200, loss is 4.330661678314209 and perplexity is 75.99455388526074
At time: 202.71661114692688 and batch: 250, loss is 4.355314273834228 and perplexity is 77.89130068836567
At time: 203.63229417800903 and batch: 300, loss is 4.334369649887085 and perplexity is 76.27686260365557
At time: 204.5477409362793 and batch: 350, loss is 4.420522527694702 and perplexity is 83.13971681485161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.943064196356412 and perplexity of 140.19918989252076
Finished 28 epochs...
Completing Train Step...
At time: 206.3046534061432 and batch: 50, loss is 4.401041688919068 and perplexity is 81.53575933946598
At time: 207.2331304550171 and batch: 100, loss is 4.352172079086304 and perplexity is 77.64693517550886
At time: 208.14945888519287 and batch: 150, loss is 4.305306568145752 and perplexity is 74.09192619590456
At time: 209.0653531551361 and batch: 200, loss is 4.328462610244751 and perplexity is 75.82762030478503
At time: 209.99178814888 and batch: 250, loss is 4.353858480453491 and perplexity is 77.77798954722319
At time: 210.92491722106934 and batch: 300, loss is 4.332898712158203 and perplexity is 76.16474656668053
At time: 211.8425476551056 and batch: 350, loss is 4.418359794616699 and perplexity is 82.96010209855548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.942674307987608 and perplexity of 140.14453851372676
Finished 29 epochs...
Completing Train Step...
At time: 213.59040427207947 and batch: 50, loss is 4.397861948013306 and perplexity is 81.27690850759923
At time: 214.52852320671082 and batch: 100, loss is 4.34920859336853 and perplexity is 77.41717021283436
At time: 215.45134949684143 and batch: 150, loss is 4.302281904220581 and perplexity is 73.86816159654332
At time: 216.36511993408203 and batch: 200, loss is 4.326070537567139 and perplexity is 75.64645189643277
At time: 217.28106331825256 and batch: 250, loss is 4.3519457244873045 and perplexity is 77.62936142365847
At time: 218.20505785942078 and batch: 300, loss is 4.331008958816528 and perplexity is 76.02094989525202
At time: 219.12238669395447 and batch: 350, loss is 4.416075658798218 and perplexity is 82.77082620606215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.941901897561961 and perplexity of 140.0363312066872
Finished 30 epochs...
Completing Train Step...
At time: 220.8945188522339 and batch: 50, loss is 4.394463129043579 and perplexity is 81.00113193197139
At time: 221.80788826942444 and batch: 100, loss is 4.346243019104004 and perplexity is 77.1879239366585
At time: 222.72234916687012 and batch: 150, loss is 4.299465484619141 and perplexity is 73.66041055268377
At time: 223.636981010437 and batch: 200, loss is 4.323693780899048 and perplexity is 75.4668721806507
At time: 224.5519609451294 and batch: 250, loss is 4.350117597579956 and perplexity is 77.48757474075282
At time: 225.46605610847473 and batch: 300, loss is 4.328957920074463 and perplexity is 75.86518777348589
At time: 226.38066148757935 and batch: 350, loss is 4.414157199859619 and perplexity is 82.61218599569278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.940940067685884 and perplexity of 139.90170483380888
Finished 31 epochs...
Completing Train Step...
At time: 228.13730788230896 and batch: 50, loss is 4.391326732635498 and perplexity is 80.74747825989465
At time: 229.0515193939209 and batch: 100, loss is 4.34236180305481 and perplexity is 76.88892154924747
At time: 229.97431707382202 and batch: 150, loss is 4.296796655654907 and perplexity is 73.46408561085842
At time: 230.89124631881714 and batch: 200, loss is 4.3210022926330565 and perplexity is 75.26402707976247
At time: 231.81012511253357 and batch: 250, loss is 4.3480020523071286 and perplexity is 77.32381954514642
At time: 232.72556066513062 and batch: 300, loss is 4.327064418792725 and perplexity is 75.72167285891408
At time: 233.65018391609192 and batch: 350, loss is 4.41199912071228 and perplexity is 82.43409459656561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.939143213732489 and perplexity of 139.65054761638672
Finished 32 epochs...
Completing Train Step...
At time: 235.44492101669312 and batch: 50, loss is 4.388172817230225 and perplexity is 80.49320872734376
At time: 236.38107919692993 and batch: 100, loss is 4.339280300140381 and perplexity is 76.65235279425815
At time: 237.2946639060974 and batch: 150, loss is 4.2946176338195805 and perplexity is 73.30418004632953
At time: 238.21120691299438 and batch: 200, loss is 4.319122114181519 and perplexity is 75.12265022637588
At time: 239.13130807876587 and batch: 250, loss is 4.345745086669922 and perplexity is 77.14949913308287
At time: 240.05725693702698 and batch: 300, loss is 4.325036067962646 and perplexity is 75.56823840297386
At time: 240.97194409370422 and batch: 350, loss is 4.4096753025054936 and perplexity is 82.24275515182967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.939186885439116 and perplexity of 139.65664652730646
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 242.75437450408936 and batch: 50, loss is 4.382501554489136 and perplexity is 80.03800260860199
At time: 243.67417764663696 and batch: 100, loss is 4.328923425674438 and perplexity is 75.86257089448499
At time: 244.5967767238617 and batch: 150, loss is 4.277848501205444 and perplexity is 72.08518186647298
At time: 245.51360750198364 and batch: 200, loss is 4.2999151420593265 and perplexity is 73.6935399522182
At time: 246.4294731616974 and batch: 250, loss is 4.326653671264649 and perplexity is 75.69057675572643
At time: 247.34443378448486 and batch: 300, loss is 4.3067260837554935 and perplexity is 74.19717552555532
At time: 248.2606964111328 and batch: 350, loss is 4.394867486953736 and perplexity is 81.03389200335032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.931390696558459 and perplexity of 138.57209012912972
Finished 34 epochs...
Completing Train Step...
At time: 250.03175806999207 and batch: 50, loss is 4.376163892745971 and perplexity is 79.53235283258171
At time: 250.94750499725342 and batch: 100, loss is 4.32481276512146 and perplexity is 75.55136568456815
At time: 251.86472082138062 and batch: 150, loss is 4.275676193237305 and perplexity is 71.9287606106016
At time: 252.7813880443573 and batch: 200, loss is 4.299871139526367 and perplexity is 73.69029732114006
At time: 253.69569087028503 and batch: 250, loss is 4.326943998336792 and perplexity is 75.71255496954561
At time: 254.61017656326294 and batch: 300, loss is 4.306541700363159 and perplexity is 74.18349605980218
At time: 255.5357837677002 and batch: 350, loss is 4.394009962081909 and perplexity is 80.96443321107387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.930095804148707 and perplexity of 138.39277030641287
Finished 35 epochs...
Completing Train Step...
At time: 257.2814071178436 and batch: 50, loss is 4.373112573623657 and perplexity is 79.29004411215928
At time: 258.2127215862274 and batch: 100, loss is 4.322783451080323 and perplexity is 75.39820369680508
At time: 259.1383352279663 and batch: 150, loss is 4.27441367149353 and perplexity is 71.83800628803563
At time: 260.07651591300964 and batch: 200, loss is 4.299520835876465 and perplexity is 73.66448786186395
At time: 260.9937436580658 and batch: 250, loss is 4.32649266242981 and perplexity is 75.67839088519743
At time: 261.9100561141968 and batch: 300, loss is 4.305842866897583 and perplexity is 74.13167226037396
At time: 262.8260054588318 and batch: 350, loss is 4.393163700103759 and perplexity is 80.8959450732058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.930116324589171 and perplexity of 138.3956102161545
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 264.64603781700134 and batch: 50, loss is 4.3701584815979 and perplexity is 79.05615965331887
At time: 265.5742623806 and batch: 100, loss is 4.318839044570923 and perplexity is 75.10138829647444
At time: 266.4888472557068 and batch: 150, loss is 4.267058515548706 and perplexity is 71.31156495138248
At time: 267.4048047065735 and batch: 200, loss is 4.290464887619018 and perplexity is 73.00039759343852
At time: 268.3300371170044 and batch: 250, loss is 4.316585006713868 and perplexity is 74.93229756418414
At time: 269.24605560302734 and batch: 300, loss is 4.297277383804321 and perplexity is 73.49941035490164
At time: 270.1651999950409 and batch: 350, loss is 4.386598043441772 and perplexity is 80.36654988777704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.928475741682382 and perplexity of 138.16874688859176
Finished 37 epochs...
Completing Train Step...
At time: 271.92152070999146 and batch: 50, loss is 4.367074108123779 and perplexity is 78.81269659003804
At time: 272.83576011657715 and batch: 100, loss is 4.316693086624145 and perplexity is 74.94039667784962
At time: 273.74876976013184 and batch: 150, loss is 4.265901823043823 and perplexity is 71.22912708551388
At time: 274.66433095932007 and batch: 200, loss is 4.290242433547974 and perplexity is 72.9841601639141
At time: 275.5789260864258 and batch: 250, loss is 4.316777210235596 and perplexity is 74.9467011998376
At time: 276.4935965538025 and batch: 300, loss is 4.297146053314209 and perplexity is 73.48975827513684
At time: 277.4078142642975 and batch: 350, loss is 4.385740032196045 and perplexity is 80.29762405799009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927987460432382 and perplexity of 138.10129814846772
Finished 38 epochs...
Completing Train Step...
At time: 279.1671371459961 and batch: 50, loss is 4.365769300460816 and perplexity is 78.70992824063393
At time: 280.0837025642395 and batch: 100, loss is 4.316144161224365 and perplexity is 74.8992712790642
At time: 280.9976580142975 and batch: 150, loss is 4.265287170410156 and perplexity is 71.18535936730656
At time: 281.913188457489 and batch: 200, loss is 4.290098762512207 and perplexity is 72.9736752072391
At time: 282.8281624317169 and batch: 250, loss is 4.3166752052307125 and perplexity is 74.93905665111345
At time: 283.75551772117615 and batch: 300, loss is 4.296908407211304 and perplexity is 73.47229579550662
At time: 284.6701934337616 and batch: 350, loss is 4.385007123947144 and perplexity is 80.23879482779823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927861180798761 and perplexity of 138.08385986820792
Finished 39 epochs...
Completing Train Step...
At time: 286.4150552749634 and batch: 50, loss is 4.36483962059021 and perplexity is 78.6367872088723
At time: 287.34576082229614 and batch: 100, loss is 4.315711622238159 and perplexity is 74.8668814296389
At time: 288.2641155719757 and batch: 150, loss is 4.264718656539917 and perplexity is 71.14490100480812
At time: 289.1807985305786 and batch: 200, loss is 4.289892845153808 and perplexity is 72.95865020781498
At time: 290.10507130622864 and batch: 250, loss is 4.316427068710327 and perplexity is 74.92046384122806
At time: 291.0245249271393 and batch: 300, loss is 4.296590147018432 and perplexity is 73.44891620906921
At time: 291.94406175613403 and batch: 350, loss is 4.3843199729919435 and perplexity is 80.18367760238398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927763314082704 and perplexity of 138.07034671555695
Finished 40 epochs...
Completing Train Step...
At time: 293.75659441947937 and batch: 50, loss is 4.364045944213867 and perplexity is 78.57439980953055
At time: 294.6723527908325 and batch: 100, loss is 4.315319900512695 and perplexity is 74.83756018892362
At time: 295.5956976413727 and batch: 150, loss is 4.264147710800171 and perplexity is 71.1042927203414
At time: 296.5133571624756 and batch: 200, loss is 4.289629049301148 and perplexity is 72.93940655678371
At time: 297.4282133579254 and batch: 250, loss is 4.316127910614013 and perplexity is 74.89805413008075
At time: 298.3440203666687 and batch: 300, loss is 4.296226148605347 and perplexity is 73.42218578533729
At time: 299.26014471054077 and batch: 350, loss is 4.383678503036499 and perplexity is 80.13225867589644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927662816540948 and perplexity of 138.05647168233767
Finished 41 epochs...
Completing Train Step...
At time: 301.03380250930786 and batch: 50, loss is 4.363337726593017 and perplexity is 78.5187717357539
At time: 301.9580125808716 and batch: 100, loss is 4.314962902069092 and perplexity is 74.81084806479016
At time: 302.87223649024963 and batch: 150, loss is 4.263569002151489 and perplexity is 71.06315595543462
At time: 303.7873570919037 and batch: 200, loss is 4.289315156936645 and perplexity is 72.91651502692037
At time: 304.70302653312683 and batch: 250, loss is 4.31577356338501 and perplexity is 74.87151891375984
At time: 305.6194357872009 and batch: 300, loss is 4.295824308395385 and perplexity is 73.39268772593563
At time: 306.551992893219 and batch: 350, loss is 4.383074674606323 and perplexity is 80.08388714545609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927562845164332 and perplexity of 138.0426706766771
Finished 42 epochs...
Completing Train Step...
At time: 308.2964105606079 and batch: 50, loss is 4.362672615051269 and perplexity is 78.46656535788537
At time: 309.23463225364685 and batch: 100, loss is 4.314618291854859 and perplexity is 74.78507192402739
At time: 310.16072702407837 and batch: 150, loss is 4.262992744445801 and perplexity is 71.02221706103434
At time: 311.07901525497437 and batch: 200, loss is 4.288961505889892 and perplexity is 72.89073258431813
At time: 312.00452637672424 and batch: 250, loss is 4.315396642684936 and perplexity is 74.84330360623606
At time: 312.9224441051483 and batch: 300, loss is 4.295406293869019 and perplexity is 73.36201492762136
At time: 313.83965039253235 and batch: 350, loss is 4.382513484954834 and perplexity is 80.03895750494281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927449719659213 and perplexity of 138.02705541308777
Finished 43 epochs...
Completing Train Step...
At time: 315.58443808555603 and batch: 50, loss is 4.3620526981353756 and perplexity is 78.41793768080503
At time: 316.5221664905548 and batch: 100, loss is 4.314265775680542 and perplexity is 74.75871362271343
At time: 317.4497411251068 and batch: 150, loss is 4.262403440475464 and perplexity is 70.9803757163843
At time: 318.37546253204346 and batch: 200, loss is 4.288569593429566 and perplexity is 72.86217139508456
At time: 319.2894265651703 and batch: 250, loss is 4.314971227645874 and perplexity is 74.81147091084262
At time: 320.21444153785706 and batch: 300, loss is 4.294957647323608 and perplexity is 73.32910869524542
At time: 321.1288812160492 and batch: 350, loss is 4.381978378295899 and perplexity is 79.99613958290699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927316073713632 and perplexity of 138.00860988936344
Finished 44 epochs...
Completing Train Step...
At time: 322.90179204940796 and batch: 50, loss is 4.361467342376709 and perplexity is 78.37204872139445
At time: 323.8277349472046 and batch: 100, loss is 4.313914222717285 and perplexity is 74.73243659456429
At time: 324.7417850494385 and batch: 150, loss is 4.261812171936035 and perplexity is 70.93841965817731
At time: 325.6569998264313 and batch: 200, loss is 4.2881536865234375 and perplexity is 72.83187381572883
At time: 326.5770490169525 and batch: 250, loss is 4.314537696838379 and perplexity is 74.77904486280201
At time: 327.4970827102661 and batch: 300, loss is 4.294509687423706 and perplexity is 73.29626755136134
At time: 328.4191429615021 and batch: 350, loss is 4.381453161239624 and perplexity is 79.95413527761627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.92718558475889 and perplexity of 137.99060246502412
Finished 45 epochs...
Completing Train Step...
At time: 330.17744874954224 and batch: 50, loss is 4.360895395278931 and perplexity is 78.32723687180652
At time: 331.10213804244995 and batch: 100, loss is 4.313547477722168 and perplexity is 74.70503387268305
At time: 332.0281343460083 and batch: 150, loss is 4.261218881607055 and perplexity is 70.89634506230058
At time: 332.94441413879395 and batch: 200, loss is 4.287717428207397 and perplexity is 72.80010723482529
At time: 333.8711144924164 and batch: 250, loss is 4.314078149795532 and perplexity is 74.74468826870864
At time: 334.7861340045929 and batch: 300, loss is 4.294037733078003 and perplexity is 73.26168322112109
At time: 335.69860100746155 and batch: 350, loss is 4.380936050415039 and perplexity is 79.91280081696353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.927041941675647 and perplexity of 137.9707824929625
Finished 46 epochs...
Completing Train Step...
At time: 337.4550259113312 and batch: 50, loss is 4.360332651138306 and perplexity is 78.28317107825202
At time: 338.3865211009979 and batch: 100, loss is 4.313185510635376 and perplexity is 74.67799800254616
At time: 339.2976896762848 and batch: 150, loss is 4.260634126663208 and perplexity is 70.85490019275187
At time: 340.210834980011 and batch: 200, loss is 4.287283067703247 and perplexity is 72.7684926101138
At time: 341.1245176792145 and batch: 250, loss is 4.313636684417725 and perplexity is 74.71169835915028
At time: 342.04079246520996 and batch: 300, loss is 4.293572626113892 and perplexity is 73.22761662497209
At time: 342.9716124534607 and batch: 350, loss is 4.3804285430908205 and perplexity is 79.87225477482701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.926936182482489 and perplexity of 137.9561915859016
Finished 47 epochs...
Completing Train Step...
At time: 344.732057094574 and batch: 50, loss is 4.359787425994873 and perplexity is 78.24050075859516
At time: 345.6475262641907 and batch: 100, loss is 4.312821397781372 and perplexity is 74.65081173329757
At time: 346.5681047439575 and batch: 150, loss is 4.260053367614746 and perplexity is 70.81376251503255
At time: 347.4924988746643 and batch: 200, loss is 4.28685284614563 and perplexity is 72.73719276929351
At time: 348.414165019989 and batch: 250, loss is 4.313195514678955 and perplexity is 74.67874508822953
At time: 349.3364701271057 and batch: 300, loss is 4.29310884475708 and perplexity is 73.19366289573962
At time: 350.25382471084595 and batch: 350, loss is 4.379925146102905 and perplexity is 79.83205744081263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.926823583142511 and perplexity of 137.94065868429675
Finished 48 epochs...
Completing Train Step...
At time: 352.0216956138611 and batch: 50, loss is 4.359249801635742 and perplexity is 78.1984480648059
At time: 352.9353415966034 and batch: 100, loss is 4.312461557388306 and perplexity is 74.62395418836536
At time: 353.8633213043213 and batch: 150, loss is 4.259486713409424 and perplexity is 70.77364696560454
At time: 354.7795240879059 and batch: 200, loss is 4.28642951965332 and perplexity is 72.70640770514441
At time: 355.6993854045868 and batch: 250, loss is 4.312761745452881 and perplexity is 74.64635877137003
At time: 356.61765122413635 and batch: 300, loss is 4.292645282745362 and perplexity is 73.15974095719109
At time: 357.53051257133484 and batch: 350, loss is 4.37943549156189 and perplexity is 79.792976880136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.9267383443898165 and perplexity of 137.92890129570435
Finished 49 epochs...
Completing Train Step...
At time: 359.3012435436249 and batch: 50, loss is 4.358733625411987 and perplexity is 78.15809430090303
At time: 360.23582315444946 and batch: 100, loss is 4.312102642059326 and perplexity is 74.5971753132599
At time: 361.14990639686584 and batch: 150, loss is 4.258927412033081 and perplexity is 70.73407423502083
At time: 362.07129526138306 and batch: 200, loss is 4.286008882522583 and perplexity is 72.6758311216951
At time: 362.98382806777954 and batch: 250, loss is 4.3123237419128415 and perplexity is 74.6136705612756
At time: 363.89755940437317 and batch: 300, loss is 4.29218132019043 and perplexity is 73.12580544986955
At time: 364.81579327583313 and batch: 350, loss is 4.378947191238403 and perplexity is 79.75402345497251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.926646265490302 and perplexity of 137.91620153896054
Finished Training.
Improved accuracyfrom -164.34063869185889 to -137.91620153896054
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73cc4becf8>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.8468608750403953, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 5.643681528926425, 'data': 'ptb', 'lr': 25.36693148619431}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1468701362609863 and batch: 50, loss is 6.587388477325439 and perplexity is 725.882733247211
At time: 2.080430030822754 and batch: 100, loss is 6.128363580703735 and perplexity is 458.684945300505
At time: 3.0089473724365234 and batch: 150, loss is 6.116029176712036 and perplexity is 453.0620884612908
At time: 3.9312856197357178 and batch: 200, loss is 6.071585597991944 and perplexity is 433.3672833877117
At time: 4.8559956550598145 and batch: 250, loss is 6.111638326644897 and perplexity is 451.0771217943703
At time: 5.779058933258057 and batch: 300, loss is 6.118418674468995 and perplexity is 454.1459737604559
At time: 6.70255184173584 and batch: 350, loss is 6.143705062866211 and perplexity is 465.77610759553954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.199302410257274 and perplexity of 492.4054242821674
Finished 1 epochs...
Completing Train Step...
At time: 8.48713755607605 and batch: 50, loss is 6.062178287506104 and perplexity is 429.3095787667719
At time: 9.416136264801025 and batch: 100, loss is 6.200995931625366 and perplexity is 493.2400299017073
At time: 10.331547021865845 and batch: 150, loss is 6.1275913047790525 and perplexity is 458.3308507071986
At time: 11.244950294494629 and batch: 200, loss is 6.029163856506347 and perplexity is 415.3675772451194
At time: 12.156587362289429 and batch: 250, loss is 6.125702743530273 and perplexity is 457.4660816651123
At time: 13.068936824798584 and batch: 300, loss is 6.225323801040649 and perplexity is 505.3866606921842
At time: 13.982691287994385 and batch: 350, loss is 6.318181400299072 and perplexity is 554.5635457414419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.477267561287715 and perplexity of 650.1919072826887
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.857447385787964 and batch: 50, loss is 6.201963138580322 and perplexity is 493.71732587387436
At time: 16.774339199066162 and batch: 100, loss is 6.057483034133911 and perplexity is 427.2985862734228
At time: 17.690364837646484 and batch: 150, loss is 6.009290790557861 and perplexity is 407.1944317090067
At time: 18.60589027404785 and batch: 200, loss is 5.978994979858398 and perplexity is 395.0431422639715
At time: 19.527724742889404 and batch: 250, loss is 5.908817348480224 and perplexity is 368.27036226082015
At time: 20.442697525024414 and batch: 300, loss is 5.872547245025634 and perplexity is 355.1524894617068
At time: 21.358842611312866 and batch: 350, loss is 5.89976167678833 and perplexity is 364.95048132947426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.036450090079472 and perplexity of 418.40509502537464
Finished 3 epochs...
Completing Train Step...
At time: 23.126121520996094 and batch: 50, loss is 5.905178060531616 and perplexity is 366.9325561769324
At time: 24.048689126968384 and batch: 100, loss is 5.899792594909668 and perplexity is 364.9617650871737
At time: 24.96324920654297 and batch: 150, loss is 5.906601009368896 and perplexity is 367.45505408683897
At time: 25.884511470794678 and batch: 200, loss is 5.8905744457244875 and perplexity is 361.6129517338042
At time: 26.81206464767456 and batch: 250, loss is 5.871848669052124 and perplexity is 354.9044751041581
At time: 27.737030029296875 and batch: 300, loss is 5.861546363830566 and perplexity is 351.2669106999562
At time: 28.64948010444641 and batch: 350, loss is 5.88230372428894 and perplexity is 358.6344857521839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.015069895777209 and perplexity of 409.55446404380683
Finished 4 epochs...
Completing Train Step...
At time: 30.445514678955078 and batch: 50, loss is 5.872137823104858 and perplexity is 355.0071120096916
At time: 31.373915195465088 and batch: 100, loss is 5.867636270523072 and perplexity is 353.41262036462314
At time: 32.2905068397522 and batch: 150, loss is 5.878460588455201 and perplexity is 357.25884977986755
At time: 33.208096742630005 and batch: 200, loss is 5.854961614608765 and perplexity is 348.96150477300887
At time: 34.120264530181885 and batch: 250, loss is 5.8488899612426755 and perplexity is 346.84915070807176
At time: 35.03530931472778 and batch: 300, loss is 5.834028425216675 and perplexity is 341.7325539754286
At time: 35.9513156414032 and batch: 350, loss is 5.86306999206543 and perplexity is 351.8025188132789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.990660042598329 and perplexity of 399.678327606934
Finished 5 epochs...
Completing Train Step...
At time: 37.72799205780029 and batch: 50, loss is 5.839125165939331 and perplexity is 343.47872230316233
At time: 38.64355826377869 and batch: 100, loss is 5.8274680519104 and perplexity is 339.4979986234041
At time: 39.5698938369751 and batch: 150, loss is 5.838389682769775 and perplexity is 343.22619236095255
At time: 40.48433995246887 and batch: 200, loss is 5.803858413696289 and perplexity is 331.57645406429856
At time: 41.39725041389465 and batch: 250, loss is 5.80343635559082 and perplexity is 331.4365390624855
At time: 42.311116456985474 and batch: 300, loss is 5.771121168136597 and perplexity is 320.8973108759376
At time: 43.22435426712036 and batch: 350, loss is 5.770610342025757 and perplexity is 320.73343001151966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.891424771012931 and perplexity of 361.92057114103716
Finished 6 epochs...
Completing Train Step...
At time: 44.9837532043457 and batch: 50, loss is 5.739340667724609 and perplexity is 310.85938377346264
At time: 45.90068459510803 and batch: 100, loss is 5.717945756912232 and perplexity is 304.2792170225786
At time: 46.81720185279846 and batch: 150, loss is 5.720415287017822 and perplexity is 305.03157230917833
At time: 47.73740315437317 and batch: 200, loss is 5.695608501434326 and perplexity is 297.5578028581561
At time: 48.65928530693054 and batch: 250, loss is 5.6871913814544675 and perplexity is 295.0637342926771
At time: 49.57374691963196 and batch: 300, loss is 5.664640417098999 and perplexity is 288.48422846812844
At time: 50.500500440597534 and batch: 350, loss is 5.69885220527649 and perplexity is 298.5245593347485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.851389391668912 and perplexity of 347.7171603416276
Finished 7 epochs...
Completing Train Step...
At time: 52.25450277328491 and batch: 50, loss is 5.68074408531189 and perplexity is 293.1674904082236
At time: 53.18337655067444 and batch: 100, loss is 5.675573863983154 and perplexity is 291.65566120995413
At time: 54.09939098358154 and batch: 150, loss is 5.688380842208862 and perplexity is 295.4149098380239
At time: 55.01512908935547 and batch: 200, loss is 5.668263463973999 and perplexity is 289.5313160288441
At time: 55.9286322593689 and batch: 250, loss is 5.668320951461792 and perplexity is 289.54796093527216
At time: 56.842992544174194 and batch: 300, loss is 5.659627695083618 and perplexity is 287.04175560320186
At time: 57.75670027732849 and batch: 350, loss is 5.693613290786743 and perplexity is 296.9647042367182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.845486344962285 and perplexity of 345.6706160694614
Finished 8 epochs...
Completing Train Step...
At time: 59.54657340049744 and batch: 50, loss is 5.671916561126709 and perplexity is 290.5909363245958
At time: 60.494017362594604 and batch: 100, loss is 5.667225818634034 and perplexity is 289.2310410244146
At time: 61.409961223602295 and batch: 150, loss is 5.681742572784424 and perplexity is 293.46036066405924
At time: 62.33926701545715 and batch: 200, loss is 5.655382947921753 and perplexity is 285.82591821249224
At time: 63.25566554069519 and batch: 250, loss is 5.657455778121948 and perplexity is 286.4190012756492
At time: 64.17364501953125 and batch: 300, loss is 5.650942792892456 and perplexity is 284.5596201844356
At time: 65.10147523880005 and batch: 350, loss is 5.6898697471618656 and perplexity is 295.85508216653693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.8412759715113145 and perplexity of 344.21827328303925
Finished 9 epochs...
Completing Train Step...
At time: 66.88640785217285 and batch: 50, loss is 5.668056564331055 and perplexity is 289.47141829955956
At time: 67.80809783935547 and batch: 100, loss is 5.656923713684082 and perplexity is 286.2666484450972
At time: 68.72484135627747 and batch: 150, loss is 5.674583311080933 and perplexity is 291.36690388660617
At time: 69.6421902179718 and batch: 200, loss is 5.651527528762817 and perplexity is 284.72606105884614
At time: 70.55816268920898 and batch: 250, loss is 5.653252582550049 and perplexity is 285.217652717919
At time: 71.47410750389099 and batch: 300, loss is 5.641806125640869 and perplexity is 281.9715348629121
At time: 72.39197659492493 and batch: 350, loss is 5.678109455108642 and perplexity is 292.39611906869015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.837320788153287 and perplexity of 342.85951573594707
Finished 10 epochs...
Completing Train Step...
At time: 74.173907995224 and batch: 50, loss is 5.659605255126953 and perplexity is 287.0353144709149
At time: 75.09213018417358 and batch: 100, loss is 5.65192590713501 and perplexity is 284.8395123603436
At time: 76.01088953018188 and batch: 150, loss is 5.661230611801147 and perplexity is 287.50222858321337
At time: 76.93575429916382 and batch: 200, loss is 5.641475839614868 and perplexity is 281.8784189835002
At time: 77.85546469688416 and batch: 250, loss is 5.637192811965942 and perplexity is 280.6737076689272
At time: 78.77728247642517 and batch: 300, loss is 5.618244762420654 and perplexity is 275.40555656118266
At time: 79.70338034629822 and batch: 350, loss is 5.632197418212891 and perplexity is 279.2751281191053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.781167392073007 and perplexity of 324.1373650257709
Finished 11 epochs...
Completing Train Step...
At time: 81.46765398979187 and batch: 50, loss is 5.597337961196899 and perplexity is 269.70747916821483
At time: 82.3994140625 and batch: 100, loss is 5.586848649978638 and perplexity is 266.8932191094812
At time: 83.31755518913269 and batch: 150, loss is 5.592874183654785 and perplexity is 268.506247988719
At time: 84.23517298698425 and batch: 200, loss is 5.571524868011474 and perplexity is 262.83458186730314
At time: 85.16313219070435 and batch: 250, loss is 5.565580062866211 and perplexity is 261.27671668314855
At time: 86.10997033119202 and batch: 300, loss is 5.5590544605255126 and perplexity is 259.5772796848763
At time: 87.02618908882141 and batch: 350, loss is 5.594921398162842 and perplexity is 269.05650092582357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.760546980233028 and perplexity of 317.5219596627166
Finished 12 epochs...
Completing Train Step...
At time: 88.78781533241272 and batch: 50, loss is 5.57724160194397 and perplexity is 264.3414402906304
At time: 89.7026195526123 and batch: 100, loss is 5.564849109649658 and perplexity is 261.0858054087839
At time: 90.61771774291992 and batch: 150, loss is 5.56463960647583 and perplexity is 261.0311128332435
At time: 91.5327217578888 and batch: 200, loss is 5.552385339736938 and perplexity is 257.851887272933
At time: 92.45219731330872 and batch: 250, loss is 5.558299398422241 and perplexity is 259.3813566944368
At time: 93.36666464805603 and batch: 300, loss is 5.552260637283325 and perplexity is 257.8197345147269
At time: 94.28287029266357 and batch: 350, loss is 5.583869218826294 and perplexity is 266.09921256939435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.7514017039331895 and perplexity of 314.6313713710608
Finished 13 epochs...
Completing Train Step...
At time: 96.04472231864929 and batch: 50, loss is 5.564603672027588 and perplexity is 261.0217329927606
At time: 96.9582290649414 and batch: 100, loss is 5.551560478210449 and perplexity is 257.6392828682405
At time: 97.87287855148315 and batch: 150, loss is 5.55396222114563 and perplexity is 258.2588098701894
At time: 98.7901566028595 and batch: 200, loss is 5.539189691543579 and perplexity is 254.47171531011116
At time: 99.70803928375244 and batch: 250, loss is 5.542694416046142 and perplexity is 255.3651332433341
At time: 100.62552499771118 and batch: 300, loss is 5.534419937133789 and perplexity is 253.26083781366037
At time: 101.54143834114075 and batch: 350, loss is 5.57464521408081 and perplexity is 263.6559976059238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.734781199488147 and perplexity of 309.4452565709995
Finished 14 epochs...
Completing Train Step...
At time: 103.30540990829468 and batch: 50, loss is 5.5544951629638675 and perplexity is 258.39648347264887
At time: 104.23281455039978 and batch: 100, loss is 5.54165470123291 and perplexity is 255.09976430942925
At time: 105.14573550224304 and batch: 150, loss is 5.54263596534729 and perplexity is 255.35020740905043
At time: 106.05770182609558 and batch: 200, loss is 5.530637874603271 and perplexity is 252.30479852829552
At time: 106.97065353393555 and batch: 250, loss is 5.532626256942749 and perplexity is 252.80697602883805
At time: 107.88465857505798 and batch: 300, loss is 5.529106473922729 and perplexity is 251.9187144882594
At time: 108.8019232749939 and batch: 350, loss is 5.568162155151367 and perplexity is 261.95222901986915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.728169408337823 and perplexity of 307.4060181004337
Finished 15 epochs...
Completing Train Step...
At time: 110.5577883720398 and batch: 50, loss is 5.54826789855957 and perplexity is 256.7923800208539
At time: 111.48443245887756 and batch: 100, loss is 5.533994226455689 and perplexity is 253.15304491658006
At time: 112.39789605140686 and batch: 150, loss is 5.535171947479248 and perplexity is 253.45136421372897
At time: 113.310307264328 and batch: 200, loss is 5.5231796169281 and perplexity is 250.43004421890225
At time: 114.22487616539001 and batch: 250, loss is 5.528255987167358 and perplexity is 251.70455204222813
At time: 115.13822984695435 and batch: 300, loss is 5.521254005432129 and perplexity is 249.94827724369586
At time: 116.05207204818726 and batch: 350, loss is 5.563140497207642 and perplexity is 260.6400918375385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.7318267822265625 and perplexity of 308.53237534351194
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 117.85302257537842 and batch: 50, loss is 5.509007253646851 and perplexity is 246.90589043657715
At time: 118.77607011795044 and batch: 100, loss is 5.456652507781983 and perplexity is 234.3117533142309
At time: 119.69487309455872 and batch: 150, loss is 5.428737831115723 and perplexity is 227.86146419791766
At time: 120.61134195327759 and batch: 200, loss is 5.395977478027344 and perplexity is 220.51759288426624
At time: 121.52774405479431 and batch: 250, loss is 5.393422031402588 and perplexity is 219.95479135673503
At time: 122.44258379936218 and batch: 300, loss is 5.379710102081299 and perplexity is 216.9593702374721
At time: 123.36039185523987 and batch: 350, loss is 5.440540580749512 and perplexity is 230.56678974574146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.635218127020474 and perplexity of 280.12001239059225
Finished 17 epochs...
Completing Train Step...
At time: 125.1230058670044 and batch: 50, loss is 5.438800544738769 and perplexity is 230.1659440727929
At time: 126.03797101974487 and batch: 100, loss is 5.413944320678711 and perplexity is 224.51540425840622
At time: 126.95529127120972 and batch: 150, loss is 5.393380947113037 and perplexity is 219.9457548560292
At time: 127.87164258956909 and batch: 200, loss is 5.374426050186157 and perplexity is 215.81596922157897
At time: 128.78873252868652 and batch: 250, loss is 5.391598949432373 and perplexity is 219.55416104426192
At time: 129.71064162254333 and batch: 300, loss is 5.38734540939331 and perplexity is 218.6222619695702
At time: 130.62500977516174 and batch: 350, loss is 5.437063331604004 and perplexity is 229.76644388053597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.6245706492456895 and perplexity of 277.15326301144086
Finished 18 epochs...
Completing Train Step...
At time: 132.44434762001038 and batch: 50, loss is 5.420639352798462 and perplexity is 226.02358512749
At time: 133.38164353370667 and batch: 100, loss is 5.398252267837524 and perplexity is 221.0197950431408
At time: 134.30331444740295 and batch: 150, loss is 5.384783000946045 and perplexity is 218.06277955609744
At time: 135.21888208389282 and batch: 200, loss is 5.369211959838867 and perplexity is 214.69361383363093
At time: 136.14603972434998 and batch: 250, loss is 5.385882110595703 and perplexity is 218.30258622410886
At time: 137.06162285804749 and batch: 300, loss is 5.378937845230102 and perplexity is 216.7918865559088
At time: 137.98022150993347 and batch: 350, loss is 5.426165323257447 and perplexity is 227.27604211497493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.612553037446121 and perplexity of 273.84247640770985
Finished 19 epochs...
Completing Train Step...
At time: 139.80760717391968 and batch: 50, loss is 5.40538685798645 and perplexity is 222.60231931376083
At time: 140.7226061820984 and batch: 100, loss is 5.383899345397949 and perplexity is 217.87017228288536
At time: 141.63717555999756 and batch: 150, loss is 5.3731320858001705 and perplexity is 215.53689164064372
At time: 142.55200719833374 and batch: 200, loss is 5.356430072784423 and perplexity is 211.9668877859936
At time: 143.4784231185913 and batch: 250, loss is 5.370182762145996 and perplexity is 214.90214009177546
At time: 144.4013156890869 and batch: 300, loss is 5.358958215713501 and perplexity is 212.50344833964735
At time: 145.32107758522034 and batch: 350, loss is 5.40212121963501 and perplexity is 221.87656631108612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.58856253788389 and perplexity of 267.3510363813828
Finished 20 epochs...
Completing Train Step...
At time: 147.0921666622162 and batch: 50, loss is 5.380742864608765 and perplexity is 217.18355348914054
At time: 148.00952744483948 and batch: 100, loss is 5.359876613616944 and perplexity is 212.69870070703544
At time: 148.92548060417175 and batch: 150, loss is 5.353269348144531 and perplexity is 211.2979764999975
At time: 149.84175419807434 and batch: 200, loss is 5.33769492149353 and perplexity is 208.0326256512461
At time: 150.75889682769775 and batch: 250, loss is 5.354562034606934 and perplexity is 211.57129515335322
At time: 151.6747808456421 and batch: 300, loss is 5.345997591018676 and perplexity is 209.76704197185177
At time: 152.59277868270874 and batch: 350, loss is 5.392159557342529 and perplexity is 219.67727935097486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.58221435546875 and perplexity of 265.6592188981569
Finished 21 epochs...
Completing Train Step...
At time: 154.34331941604614 and batch: 50, loss is 5.371817121505737 and perplexity is 215.25365458812138
At time: 155.27182126045227 and batch: 100, loss is 5.349268560409546 and perplexity is 210.4543069439308
At time: 156.1862165927887 and batch: 150, loss is 5.3422524356842045 and perplexity is 208.98290109684712
At time: 157.114022731781 and batch: 200, loss is 5.3261448001861575 and perplexity is 205.64364664621715
At time: 158.0309820175171 and batch: 250, loss is 5.343048372268677 and perplexity is 209.14930444783172
At time: 158.94713020324707 and batch: 300, loss is 5.335074882507325 and perplexity is 207.48828546927632
At time: 159.8634614944458 and batch: 350, loss is 5.382031888961792 and perplexity is 217.46368889055324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.571088067416487 and perplexity of 262.7198006356404
Finished 22 epochs...
Completing Train Step...
At time: 161.62350988388062 and batch: 50, loss is 5.362243118286133 and perplexity is 213.2026492384751
At time: 162.5501902103424 and batch: 100, loss is 5.339536323547363 and perplexity is 208.41605026647971
At time: 163.46557998657227 and batch: 150, loss is 5.333926277160645 and perplexity is 207.25010013187523
At time: 164.37926292419434 and batch: 200, loss is 5.318797750473022 and perplexity is 204.13830921697632
At time: 165.2950723171234 and batch: 250, loss is 5.3361747932434085 and perplexity is 207.71662961814508
At time: 166.21056413650513 and batch: 300, loss is 5.328315353393554 and perplexity is 206.0904918982882
At time: 167.12626433372498 and batch: 350, loss is 5.377676477432251 and perplexity is 216.5186046421084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.568388840247845 and perplexity of 262.01161641704175
Finished 23 epochs...
Completing Train Step...
At time: 168.88983345031738 and batch: 50, loss is 5.358796291351318 and perplexity is 212.46904164002996
At time: 169.80415654182434 and batch: 100, loss is 5.335696210861206 and perplexity is 207.61724388274735
At time: 170.73283314704895 and batch: 150, loss is 5.329007024765015 and perplexity is 206.23308810063494
At time: 171.65126705169678 and batch: 200, loss is 5.3145359516143795 and perplexity is 203.27016404780028
At time: 172.5809006690979 and batch: 250, loss is 5.332453622817993 and perplexity is 206.94511699439028
At time: 173.51178789138794 and batch: 300, loss is 5.324458074569702 and perplexity is 205.29707460769308
At time: 174.44017219543457 and batch: 350, loss is 5.37392258644104 and perplexity is 215.70734105292544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.564238449622845 and perplexity of 260.9264194141335
Finished 24 epochs...
Completing Train Step...
At time: 176.22469830513 and batch: 50, loss is 5.353903551101684 and perplexity is 211.43202480395664
At time: 177.14314603805542 and batch: 100, loss is 5.331196203231811 and perplexity is 206.68506368334454
At time: 178.0651125907898 and batch: 150, loss is 5.324326114654541 and perplexity is 205.2699854105284
At time: 178.98668789863586 and batch: 200, loss is 5.309831504821777 and perplexity is 202.3161362226153
At time: 179.90306186676025 and batch: 250, loss is 5.3282827949523925 and perplexity is 206.08378202236588
At time: 180.84394216537476 and batch: 300, loss is 5.319570188522339 and perplexity is 204.29605433065922
At time: 181.7730677127838 and batch: 350, loss is 5.369427080154419 and perplexity is 214.73980375960343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.560461636247306 and perplexity of 259.94280765150165
Finished 25 epochs...
Completing Train Step...
At time: 183.53593611717224 and batch: 50, loss is 5.347388362884521 and perplexity is 210.05898303686038
At time: 184.46510362625122 and batch: 100, loss is 5.323454675674438 and perplexity is 205.09118306277955
At time: 185.38108825683594 and batch: 150, loss is 5.315746068954468 and perplexity is 203.5162936908759
At time: 186.29571104049683 and batch: 200, loss is 5.2999780654907225 and perplexity is 200.3324157333676
At time: 187.21203422546387 and batch: 250, loss is 5.314940862655639 and perplexity is 203.35248704720286
At time: 188.12898921966553 and batch: 300, loss is 5.302972030639649 and perplexity is 200.93310277360092
At time: 189.04826474189758 and batch: 350, loss is 5.3532339954376225 and perplexity is 211.29050667660394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.542705667429957 and perplexity of 255.36800647062503
Finished 26 epochs...
Completing Train Step...
At time: 190.81526279449463 and batch: 50, loss is 5.328862400054931 and perplexity is 206.20326385677208
At time: 191.73002576828003 and batch: 100, loss is 5.306383838653565 and perplexity is 201.6198187493003
At time: 192.64440488815308 and batch: 150, loss is 5.296758460998535 and perplexity is 199.68846178251906
At time: 193.55803394317627 and batch: 200, loss is 5.280615816116333 and perplexity is 196.49084032807042
At time: 194.4746961593628 and batch: 250, loss is 5.299662990570068 and perplexity is 200.26930595605208
At time: 195.38939690589905 and batch: 300, loss is 5.289408521652222 and perplexity is 198.226144255878
At time: 196.30512642860413 and batch: 350, loss is 5.339697732925415 and perplexity is 208.44969328660565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.531150028623384 and perplexity of 252.43405054079918
Finished 27 epochs...
Completing Train Step...
At time: 198.07209062576294 and batch: 50, loss is 5.316722679138183 and perplexity is 203.71514686105704
At time: 198.99920845031738 and batch: 100, loss is 5.294243860244751 and perplexity is 199.18695583387785
At time: 199.92092299461365 and batch: 150, loss is 5.283571758270264 and perplexity is 197.0725151609001
At time: 200.8487527370453 and batch: 200, loss is 5.270631799697876 and perplexity is 194.53883320249443
At time: 201.764981508255 and batch: 250, loss is 5.287759284973145 and perplexity is 197.89949186561162
At time: 202.67855668067932 and batch: 300, loss is 5.278906269073486 and perplexity is 196.15521695675181
At time: 203.60665583610535 and batch: 350, loss is 5.330405855178833 and perplexity is 206.5217750815969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.521013457199623 and perplexity of 249.88815985824218
Finished 28 epochs...
Completing Train Step...
At time: 205.36416792869568 and batch: 50, loss is 5.307516613006592 and perplexity is 201.8483379149306
At time: 206.3028063774109 and batch: 100, loss is 5.285958375930786 and perplexity is 197.54341360974624
At time: 207.21577668190002 and batch: 150, loss is 5.274210138320923 and perplexity is 195.2362059969018
At time: 208.13143396377563 and batch: 200, loss is 5.260393028259277 and perplexity is 192.55715683108582
At time: 209.058753490448 and batch: 250, loss is 5.280091228485108 and perplexity is 196.3877906952299
At time: 209.97535037994385 and batch: 300, loss is 5.273434333801269 and perplexity is 195.08479960436514
At time: 210.9003632068634 and batch: 350, loss is 5.325009899139404 and perplexity is 205.41039384084252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.51989640860722 and perplexity of 249.60917848789728
Finished 29 epochs...
Completing Train Step...
At time: 212.7007269859314 and batch: 50, loss is 5.303586435317993 and perplexity is 201.05659494517852
At time: 213.63064289093018 and batch: 100, loss is 5.279742984771729 and perplexity is 196.3194117886891
At time: 214.5521240234375 and batch: 150, loss is 5.26803599357605 and perplexity is 194.03450296328597
At time: 215.46770024299622 and batch: 200, loss is 5.2542492771148686 and perplexity is 191.377760247753
At time: 216.384583234787 and batch: 250, loss is 5.274508018493652 and perplexity is 195.29437165443508
At time: 217.30947947502136 and batch: 300, loss is 5.26880443572998 and perplexity is 194.18366455836852
At time: 218.23329854011536 and batch: 350, loss is 5.320255613327026 and perplexity is 204.4361319146384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.51471052498653 and perplexity of 248.31808496147735
Finished 30 epochs...
Completing Train Step...
At time: 220.01784443855286 and batch: 50, loss is 5.299009456634521 and perplexity is 200.1384659272177
At time: 220.93318915367126 and batch: 100, loss is 5.275846643447876 and perplexity is 195.55597262748256
At time: 221.8515100479126 and batch: 150, loss is 5.264211416244507 and perplexity is 193.29382030362473
At time: 222.76679730415344 and batch: 200, loss is 5.250519437789917 and perplexity is 190.66528149328053
At time: 223.68265652656555 and batch: 250, loss is 5.272210321426392 and perplexity is 194.84615947452923
At time: 224.59893155097961 and batch: 300, loss is 5.266145429611206 and perplexity is 193.66801486780003
At time: 225.5149974822998 and batch: 350, loss is 5.317984218597412 and perplexity is 203.97230372992846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.51025758940598 and perplexity of 247.21479877985962
Finished 31 epochs...
Completing Train Step...
At time: 227.3038833141327 and batch: 50, loss is 5.2950063323974605 and perplexity is 199.33888825564387
At time: 228.21936798095703 and batch: 100, loss is 5.272228803634643 and perplexity is 194.84976069510486
At time: 229.13351774215698 and batch: 150, loss is 5.260536394119263 and perplexity is 192.58476493245357
At time: 230.0492947101593 and batch: 200, loss is 5.24790584564209 and perplexity is 190.16761084804511
At time: 230.96687054634094 and batch: 250, loss is 5.269190235137939 and perplexity is 194.25859495431226
At time: 231.88139462471008 and batch: 300, loss is 5.2629898738861085 and perplexity is 193.05784786901185
At time: 232.79812622070312 and batch: 350, loss is 5.314892568588257 and perplexity is 203.3426665656284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.506601794012662 and perplexity of 246.31268203915278
Finished 32 epochs...
Completing Train Step...
At time: 234.55763125419617 and batch: 50, loss is 5.293466529846191 and perplexity is 199.03218192113872
At time: 235.50398230552673 and batch: 100, loss is 5.269118909835815 and perplexity is 194.24473989545095
At time: 236.42312669754028 and batch: 150, loss is 5.256830177307129 and perplexity is 191.872325082683
At time: 237.33987402915955 and batch: 200, loss is 5.2440919589996335 and perplexity is 189.44371444408338
At time: 238.26879382133484 and batch: 250, loss is 5.266795082092285 and perplexity is 193.79387265164883
At time: 239.18709754943848 and batch: 300, loss is 5.261020956039428 and perplexity is 192.67810678907213
At time: 240.10175776481628 and batch: 350, loss is 5.313487253189087 and perplexity is 203.05710668284206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.503742086476293 and perplexity of 245.60930601023398
Finished 33 epochs...
Completing Train Step...
At time: 241.87857151031494 and batch: 50, loss is 5.290917110443115 and perplexity is 198.52541167414378
At time: 242.79642176628113 and batch: 100, loss is 5.26666256904602 and perplexity is 193.768194136643
At time: 243.7192347049713 and batch: 150, loss is 5.254772777557373 and perplexity is 191.47797281830213
At time: 244.63439989089966 and batch: 200, loss is 5.2418860912323 and perplexity is 189.02628722455322
At time: 245.55072331428528 and batch: 250, loss is 5.2647444438934325 and perplexity is 193.39687871826519
At time: 246.4661145210266 and batch: 300, loss is 5.258370876312256 and perplexity is 192.16817042884256
At time: 247.38137459754944 and batch: 350, loss is 5.311157770156861 and perplexity is 202.5846391144967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.502374583277209 and perplexity of 245.27366404657587
Finished 34 epochs...
Completing Train Step...
At time: 249.16131162643433 and batch: 50, loss is 5.287837905883789 and perplexity is 197.91505151552698
At time: 250.07780003547668 and batch: 100, loss is 5.2636799621582036 and perplexity is 193.19112080541842
At time: 251.00590753555298 and batch: 150, loss is 5.2519573307037355 and perplexity is 190.93963494862598
At time: 251.9219937324524 and batch: 200, loss is 5.240104141235352 and perplexity is 188.68975176634106
At time: 252.83770847320557 and batch: 250, loss is 5.262951831817627 and perplexity is 193.05050368883718
At time: 253.75185084342957 and batch: 300, loss is 5.257381610870361 and perplexity is 191.97815910012184
At time: 254.66623210906982 and batch: 350, loss is 5.310060930252075 and perplexity is 202.36255801419057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.500518798828125 and perplexity of 244.81891108731497
Finished 35 epochs...
Completing Train Step...
At time: 256.4213101863861 and batch: 50, loss is 5.2856893062591555 and perplexity is 197.49026781859442
At time: 257.3485884666443 and batch: 100, loss is 5.261346998214722 and perplexity is 192.74093822043278
At time: 258.2634961605072 and batch: 150, loss is 5.249573669433594 and perplexity is 190.4850415494475
At time: 259.18674659729004 and batch: 200, loss is 5.2381799602508545 and perplexity is 188.3270276193905
At time: 260.1088070869446 and batch: 250, loss is 5.261349058151245 and perplexity is 192.7413352549398
At time: 261.0292155742645 and batch: 300, loss is 5.254994707107544 and perplexity is 191.52047215443216
At time: 261.94371128082275 and batch: 350, loss is 5.308279981613159 and perplexity is 202.002481426056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.499182339372306 and perplexity of 244.4919390797123
Finished 36 epochs...
Completing Train Step...
At time: 263.7081353664398 and batch: 50, loss is 5.283423490524292 and perplexity is 197.04329782933175
At time: 264.6373243331909 and batch: 100, loss is 5.258885459899902 and perplexity is 192.26708246248523
At time: 265.55086374282837 and batch: 150, loss is 5.246923341751098 and perplexity is 189.9808621861124
At time: 266.4760949611664 and batch: 200, loss is 5.235901794433594 and perplexity is 187.8984757640456
At time: 267.39100551605225 and batch: 250, loss is 5.258934669494629 and perplexity is 192.2765440804917
At time: 268.3058753013611 and batch: 300, loss is 5.253252849578858 and perplexity is 191.18716115252545
At time: 269.2200026512146 and batch: 350, loss is 5.307111644744873 and perplexity is 201.76661229365274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.496999806371228 and perplexity of 243.958909243366
Finished 37 epochs...
Completing Train Step...
At time: 270.98496675491333 and batch: 50, loss is 5.280952434539795 and perplexity is 196.5569938985971
At time: 271.9002380371094 and batch: 100, loss is 5.2569467735290525 and perplexity is 191.89469797515136
At time: 272.8171010017395 and batch: 150, loss is 5.244743890762329 and perplexity is 189.56725908573486
At time: 273.73118448257446 and batch: 200, loss is 5.2338700866699215 and perplexity is 187.51710851663742
At time: 274.65993642807007 and batch: 250, loss is 5.25777229309082 and perplexity is 192.05317620656942
At time: 275.57530450820923 and batch: 300, loss is 5.251737852096557 and perplexity is 190.897732382019
At time: 276.4917757511139 and batch: 350, loss is 5.3048203086853025 and perplexity is 201.30482643462193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4969198292699355 and perplexity of 243.93939889717106
Finished 38 epochs...
Completing Train Step...
At time: 278.25321531295776 and batch: 50, loss is 5.279517784118652 and perplexity is 196.27520550677124
At time: 279.16779923439026 and batch: 100, loss is 5.254815120697021 and perplexity is 191.4860807685015
At time: 280.08237862586975 and batch: 150, loss is 5.2428138542175295 and perplexity is 189.2017401938758
At time: 280.99701952934265 and batch: 200, loss is 5.232024364471435 and perplexity is 187.17132323680104
At time: 281.9133222103119 and batch: 250, loss is 5.256006546020508 and perplexity is 191.7143580949149
At time: 282.82975220680237 and batch: 300, loss is 5.250432224273681 and perplexity is 190.64865362875528
At time: 283.7466459274292 and batch: 350, loss is 5.304134922027588 and perplexity is 201.16690206361116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.495875391466864 and perplexity of 243.6847523716816
Finished 39 epochs...
Completing Train Step...
At time: 285.5034866333008 and batch: 50, loss is 5.277397193908691 and perplexity is 195.8594272309957
At time: 286.43805146217346 and batch: 100, loss is 5.252623558044434 and perplexity is 191.06688653838899
At time: 287.35674262046814 and batch: 150, loss is 5.240981140136719 and perplexity is 188.85530505575483
At time: 288.27220010757446 and batch: 200, loss is 5.2306571960449215 and perplexity is 186.91560335922577
At time: 289.1875231266022 and batch: 250, loss is 5.254409732818604 and perplexity is 191.40847036469367
At time: 290.10756945610046 and batch: 300, loss is 5.249198045730591 and perplexity is 190.41350428916044
At time: 291.0425193309784 and batch: 350, loss is 5.302819757461548 and perplexity is 200.9025083808678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.49523557465652 and perplexity of 243.52888863812515
Finished 40 epochs...
Completing Train Step...
At time: 292.8687183856964 and batch: 50, loss is 5.275722665786743 and perplexity is 195.53172955820622
At time: 293.7937662601471 and batch: 100, loss is 5.251182155609131 and perplexity is 190.79168065169273
At time: 294.72004890441895 and batch: 150, loss is 5.239328413009644 and perplexity is 188.54343655774977
At time: 295.6389937400818 and batch: 200, loss is 5.2292725944519045 and perplexity is 186.6569788043581
At time: 296.56429719924927 and batch: 250, loss is 5.252995195388794 and perplexity is 191.1379073248682
At time: 297.48375177383423 and batch: 300, loss is 5.24819935798645 and perplexity is 190.22343558154986
At time: 298.43016362190247 and batch: 350, loss is 5.301358680725098 and perplexity is 200.6091887330152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.494193767679149 and perplexity of 243.27531065534174
Finished 41 epochs...
Completing Train Step...
At time: 300.19758319854736 and batch: 50, loss is 5.273526668548584 and perplexity is 195.10281354168492
At time: 301.120240688324 and batch: 100, loss is 5.24979413986206 and perplexity is 190.5270424979878
At time: 302.03886318206787 and batch: 150, loss is 5.237042999267578 and perplexity is 188.11302881408946
At time: 302.95471572875977 and batch: 200, loss is 5.227420215606689 and perplexity is 186.31153940660118
At time: 303.8781452178955 and batch: 250, loss is 5.251212177276611 and perplexity is 190.7974086220684
At time: 304.80007672309875 and batch: 300, loss is 5.24652982711792 and perplexity is 189.90611664451504
At time: 305.7191870212555 and batch: 350, loss is 5.299989776611328 and perplexity is 200.33476186418739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4921764505320585 and perplexity of 242.78504187976202
Finished 42 epochs...
Completing Train Step...
At time: 307.47219944000244 and batch: 50, loss is 5.2735056304931645 and perplexity is 195.09870900105707
At time: 308.3996868133545 and batch: 100, loss is 5.249505081176758 and perplexity is 190.47197696053811
At time: 309.32124376296997 and batch: 150, loss is 5.23600567817688 and perplexity is 187.9179963749852
At time: 310.237233877182 and batch: 200, loss is 5.226331815719605 and perplexity is 186.10886826178134
At time: 311.15370655059814 and batch: 250, loss is 5.250098390579224 and perplexity is 190.5850193066024
At time: 312.0684289932251 and batch: 300, loss is 5.245298137664795 and perplexity is 189.67235527382005
At time: 312.9821174144745 and batch: 350, loss is 5.298976345062256 and perplexity is 200.13183913765238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.49090470938847 and perplexity of 242.47647840097872
Finished 43 epochs...
Completing Train Step...
At time: 314.73617458343506 and batch: 50, loss is 5.272061433792114 and perplexity is 194.81715145031868
At time: 315.66420888900757 and batch: 100, loss is 5.24817741394043 and perplexity is 190.21926135552522
At time: 316.57778573036194 and batch: 150, loss is 5.234853916168213 and perplexity is 187.7016841600238
At time: 317.49436020851135 and batch: 200, loss is 5.225755128860474 and perplexity is 186.00157266404125
At time: 318.411288022995 and batch: 250, loss is 5.249264469146729 and perplexity is 190.4261526246623
At time: 319.33850622177124 and batch: 300, loss is 5.244169836044311 and perplexity is 189.45846833518434
At time: 320.25133323669434 and batch: 350, loss is 5.29729118347168 and perplexity is 199.79486865388773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.491802347117457 and perplexity of 242.6942321537541
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 322.0158267021179 and batch: 50, loss is 5.2661004066467285 and perplexity is 193.65929555593232
At time: 322.9321219921112 and batch: 100, loss is 5.235577068328857 and perplexity is 187.83747012952196
At time: 323.849143743515 and batch: 150, loss is 5.2134965133666995 and perplexity is 183.73536953903314
At time: 324.7684030532837 and batch: 200, loss is 5.202910013198853 and perplexity is 181.800514759334
At time: 325.6868681907654 and batch: 250, loss is 5.2211534214019775 and perplexity is 185.14761418031398
At time: 326.6025581359863 and batch: 300, loss is 5.211098690032959 and perplexity is 183.2953323592625
At time: 327.51876068115234 and batch: 350, loss is 5.272598161697387 and perplexity is 194.92174331810406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.475719583445582 and perplexity of 238.82225765741208
Finished 45 epochs...
Completing Train Step...
At time: 329.28904914855957 and batch: 50, loss is 5.254390134811401 and perplexity is 191.4047191768708
At time: 330.20520067214966 and batch: 100, loss is 5.228588380813599 and perplexity is 186.52930923538602
At time: 331.13109159469604 and batch: 150, loss is 5.208959140777588 and perplexity is 182.90358220122172
At time: 332.05151534080505 and batch: 200, loss is 5.20066237449646 and perplexity is 181.39235175955608
At time: 332.9668445587158 and batch: 250, loss is 5.22048734664917 and perplexity is 185.02433309073447
At time: 333.880437374115 and batch: 300, loss is 5.2127966213226316 and perplexity is 183.60681960646394
At time: 334.79576539993286 and batch: 350, loss is 5.273283319473267 and perplexity is 195.05534122882375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.474459944100215 and perplexity of 238.52161713418533
Finished 46 epochs...
Completing Train Step...
At time: 336.55618476867676 and batch: 50, loss is 5.251497859954834 and perplexity is 190.85192392342947
At time: 337.49297642707825 and batch: 100, loss is 5.2271044921875 and perplexity is 186.2527257752543
At time: 338.4199147224426 and batch: 150, loss is 5.208352956771851 and perplexity is 182.79274257309828
At time: 339.33708810806274 and batch: 200, loss is 5.2005735111236575 and perplexity is 181.37623333955736
At time: 340.25472617149353 and batch: 250, loss is 5.220772771835327 and perplexity is 185.07715123290598
At time: 341.1722671985626 and batch: 300, loss is 5.213177614212036 and perplexity is 183.67678582665
At time: 342.08870458602905 and batch: 350, loss is 5.272926979064941 and perplexity is 194.98584751132938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.473650702114763 and perplexity of 238.32867350673388
Finished 47 epochs...
Completing Train Step...
At time: 343.9092688560486 and batch: 50, loss is 5.2496496772766115 and perplexity is 190.4995204568311
At time: 344.8275182247162 and batch: 100, loss is 5.22622820854187 and perplexity is 186.0895870460427
At time: 345.7565915584564 and batch: 150, loss is 5.208156118392944 and perplexity is 182.75676548692613
At time: 346.6792175769806 and batch: 200, loss is 5.200353059768677 and perplexity is 181.33625311016615
At time: 347.74119114875793 and batch: 250, loss is 5.220800056457519 and perplexity is 185.08220106194472
At time: 348.6582546234131 and batch: 300, loss is 5.2132315254211425 and perplexity is 183.68668833118434
At time: 349.57296085357666 and batch: 350, loss is 5.272530364990234 and perplexity is 194.90852871371294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.473403404498923 and perplexity of 238.26974268101597
Finished 48 epochs...
Completing Train Step...
At time: 351.3589916229248 and batch: 50, loss is 5.248550796508789 and perplexity is 190.29029917319775
At time: 352.2794659137726 and batch: 100, loss is 5.225846290588379 and perplexity is 186.01852966170122
At time: 353.19794964790344 and batch: 150, loss is 5.208162956237793 and perplexity is 182.75801515360604
At time: 354.113653421402 and batch: 200, loss is 5.200322551727295 and perplexity is 181.33072098063985
At time: 355.0314574241638 and batch: 250, loss is 5.220800447463989 and perplexity is 185.08227343029694
At time: 355.96300292015076 and batch: 300, loss is 5.2133051300048825 and perplexity is 183.70020901100344
At time: 356.8810896873474 and batch: 350, loss is 5.2722389602661135 and perplexity is 194.8517397223664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.473190833782327 and perplexity of 238.21909889395508
Finished 49 epochs...
Completing Train Step...
At time: 358.6600544452667 and batch: 50, loss is 5.247688627243042 and perplexity is 190.12630743015941
At time: 359.57670617103577 and batch: 100, loss is 5.225556325912476 and perplexity is 185.9645986784538
At time: 360.4921214580536 and batch: 150, loss is 5.208174085617065 and perplexity is 182.7600491481903
At time: 361.40831422805786 and batch: 200, loss is 5.200302762985229 and perplexity is 181.32713270927763
At time: 362.32390451431274 and batch: 250, loss is 5.220762166976929 and perplexity is 185.07518852633146
At time: 363.2532422542572 and batch: 300, loss is 5.213329877853393 and perplexity is 183.70475525220206
At time: 364.1799826622009 and batch: 350, loss is 5.271975927352905 and perplexity is 194.8004940415693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4730377197265625 and perplexity of 238.18262699381427
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73cc4becf8>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.8766070316297858, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 6.598249028711501, 'data': 'ptb', 'lr': 27.971676517488476}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1694304943084717 and batch: 50, loss is 6.6659673404693605 and perplexity is 785.2226753856603
At time: 2.0884270668029785 and batch: 100, loss is 6.1962471771240235 and perplexity is 490.9033067429994
At time: 3.007476568222046 and batch: 150, loss is 6.170802040100098 and perplexity is 478.56978436376744
At time: 3.934227705001831 and batch: 200, loss is 6.132423276901245 and perplexity is 460.5508517728148
At time: 4.8672919273376465 and batch: 250, loss is 6.153065223693847 and perplexity is 470.156314613309
At time: 5.787351846694946 and batch: 300, loss is 6.166346473693848 and perplexity is 476.4422281631761
At time: 6.732832670211792 and batch: 350, loss is 6.179861288070679 and perplexity is 482.9249642530826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.173969137257543 and perplexity of 480.08786405147174
Finished 1 epochs...
Completing Train Step...
At time: 8.51764702796936 and batch: 50, loss is 6.051234693527221 and perplexity is 424.6370030642536
At time: 9.433578729629517 and batch: 100, loss is 6.233732461929321 and perplexity is 509.6542027517171
At time: 10.349770784378052 and batch: 150, loss is 6.397559881210327 and perplexity is 600.3782547769852
At time: 11.271971940994263 and batch: 200, loss is 6.466471767425537 and perplexity is 643.2103231603346
At time: 12.190625667572021 and batch: 250, loss is 6.8078701877593994 and perplexity is 904.9413977712651
At time: 13.10728907585144 and batch: 300, loss is 6.840214595794678 and perplexity is 934.6896936864802
At time: 14.031383991241455 and batch: 350, loss is 6.9254212474823 and perplexity is 1017.8229346791943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 8.760572236159753 and perplexity of 6377.760118936586
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 15.80618166923523 and batch: 50, loss is 6.417829608917236 and perplexity is 612.6719323569395
At time: 16.730390071868896 and batch: 100, loss is 6.0704654026031495 and perplexity is 432.8820991565114
At time: 17.641441822052002 and batch: 150, loss is 6.031850967407227 and perplexity is 416.48521692819537
At time: 18.553786516189575 and batch: 200, loss is 5.960711555480957 and perplexity is 387.8860284951114
At time: 19.465340852737427 and batch: 250, loss is 5.973568305969239 and perplexity is 392.90517822227844
At time: 20.379669666290283 and batch: 300, loss is 5.964110641479492 and perplexity is 389.20672978062913
At time: 21.29276967048645 and batch: 350, loss is 5.990688591003418 and perplexity is 399.68973794860847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 6.118441877693965 and perplexity of 454.15651153390934
Finished 3 epochs...
Completing Train Step...
At time: 23.0429470539093 and batch: 50, loss is 5.968497915267944 and perplexity is 390.9180375072902
At time: 23.971315383911133 and batch: 100, loss is 5.93261227607727 and perplexity is 377.1384178174799
At time: 24.888643264770508 and batch: 150, loss is 5.924118318557739 and perplexity is 373.9485864417791
At time: 25.833646535873413 and batch: 200, loss is 5.8589105224609375 and perplexity is 350.3422460150209
At time: 26.753682851791382 and batch: 250, loss is 5.83481915473938 and perplexity is 342.0028788575736
At time: 27.679798364639282 and batch: 300, loss is 5.8023217582702635 and perplexity is 331.06732658395504
At time: 28.597524881362915 and batch: 350, loss is 5.821382846832275 and perplexity is 337.4383567055564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.954835562870421 and perplexity of 385.61349628786655
Finished 4 epochs...
Completing Train Step...
At time: 30.37565279006958 and batch: 50, loss is 5.779905500411988 and perplexity is 323.72859675270024
At time: 31.2907874584198 and batch: 100, loss is 5.753339614868164 and perplexity is 315.24169012681784
At time: 32.20576786994934 and batch: 150, loss is 5.761159543991089 and perplexity is 317.71652169233465
At time: 33.13136267662048 and batch: 200, loss is 5.7380933094024655 and perplexity is 310.471872467067
At time: 34.060564041137695 and batch: 250, loss is 5.75072434425354 and perplexity is 314.418324928822
At time: 34.990333557128906 and batch: 300, loss is 5.736204261779785 and perplexity is 309.88592992540134
At time: 35.90755295753479 and batch: 350, loss is 5.766024808883667 and perplexity is 319.2660631393445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.922569011819774 and perplexity of 373.3696739494642
Finished 5 epochs...
Completing Train Step...
At time: 37.68169379234314 and batch: 50, loss is 5.7382737827301025 and perplexity is 310.52790941546397
At time: 38.606194257736206 and batch: 100, loss is 5.725744800567627 and perplexity is 306.6615819279009
At time: 39.53240251541138 and batch: 150, loss is 5.7361187076568605 and perplexity is 309.8594190405338
At time: 40.45189642906189 and batch: 200, loss is 5.71013840675354 and perplexity is 301.9128521688582
At time: 41.37073087692261 and batch: 250, loss is 5.714921369552612 and perplexity is 303.3603490116201
At time: 42.29094862937927 and batch: 300, loss is 5.704422636032104 and perplexity is 300.1921098968783
At time: 43.208173513412476 and batch: 350, loss is 5.742845420837402 and perplexity is 311.95078058741194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.90728812382139 and perplexity of 367.7076245276069
Finished 6 epochs...
Completing Train Step...
At time: 44.98062610626221 and batch: 50, loss is 5.7198723030090335 and perplexity is 304.86599000157963
At time: 45.91707682609558 and batch: 100, loss is 5.706056747436524 and perplexity is 300.6830582700962
At time: 46.84690713882446 and batch: 150, loss is 5.7196626949310305 and perplexity is 304.80209432412596
At time: 47.777748823165894 and batch: 200, loss is 5.691749305725097 and perplexity is 296.41168203686476
At time: 48.70333003997803 and batch: 250, loss is 5.7027957725524905 and perplexity is 299.70413535802317
At time: 49.63322925567627 and batch: 300, loss is 5.6912659168243405 and perplexity is 296.26843454462664
At time: 50.55552816390991 and batch: 350, loss is 5.7252532005310055 and perplexity is 306.5108641324669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.8694305419921875 and perplexity of 354.0473077766499
Finished 7 epochs...
Completing Train Step...
At time: 52.32300806045532 and batch: 50, loss is 5.6782668018341065 and perplexity is 292.44213026032423
At time: 53.2597131729126 and batch: 100, loss is 5.658718452453614 and perplexity is 286.78088361835336
At time: 54.18334245681763 and batch: 150, loss is 5.6660919094085695 and perplexity is 288.90326514787137
At time: 55.106101274490356 and batch: 200, loss is 5.641712236404419 and perplexity is 281.94506201358035
At time: 56.03071212768555 and batch: 250, loss is 5.654029121398926 and perplexity is 285.4392213228205
At time: 56.943211793899536 and batch: 300, loss is 5.651626758575439 and perplexity is 284.7543157743654
At time: 57.857094287872314 and batch: 350, loss is 5.690093269348145 and perplexity is 295.9212197326559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.838735120049838 and perplexity of 343.34477596370755
Finished 8 epochs...
Completing Train Step...
At time: 59.62423324584961 and batch: 50, loss is 5.641234111785889 and perplexity is 281.81028935998796
At time: 60.54104733467102 and batch: 100, loss is 5.627803707122803 and perplexity is 278.0507656087345
At time: 61.457587480545044 and batch: 150, loss is 5.64230863571167 and perplexity is 282.1132640060383
At time: 62.38448476791382 and batch: 200, loss is 5.61353967666626 and perplexity is 274.1127934658509
At time: 63.29839086532593 and batch: 250, loss is 5.622965917587281 and perplexity is 276.7088630627553
At time: 64.21416234970093 and batch: 300, loss is 5.617632007598877 and perplexity is 275.2368521709483
At time: 65.13904047012329 and batch: 350, loss is 5.662055921554566 and perplexity is 287.7396049176208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.824614426185345 and perplexity of 338.53057938363145
Finished 9 epochs...
Completing Train Step...
At time: 66.92168092727661 and batch: 50, loss is 5.62504810333252 and perplexity is 277.2856225648345
At time: 67.84242796897888 and batch: 100, loss is 5.613876752853393 and perplexity is 274.2052059352632
At time: 68.7564697265625 and batch: 150, loss is 5.6257991600036625 and perplexity is 277.4939580075425
At time: 69.668781042099 and batch: 200, loss is 5.599692811965943 and perplexity is 270.34334842771653
At time: 70.59254956245422 and batch: 250, loss is 5.609107971191406 and perplexity is 272.900694115787
At time: 71.51343059539795 and batch: 300, loss is 5.60715009689331 and perplexity is 272.3669115709189
At time: 72.4321768283844 and batch: 350, loss is 5.650949935913086 and perplexity is 284.56165280693256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.815903762291217 and perplexity of 335.59455919560867
Finished 10 epochs...
Completing Train Step...
At time: 74.22376656532288 and batch: 50, loss is 5.610150823593139 and perplexity is 273.18543770704343
At time: 75.15426135063171 and batch: 100, loss is 5.593769512176514 and perplexity is 268.74675694200226
At time: 76.07059597969055 and batch: 150, loss is 5.605078010559082 and perplexity is 271.8031281212667
At time: 76.99255609512329 and batch: 200, loss is 5.579374208450317 and perplexity is 264.90577810745674
At time: 77.90998363494873 and batch: 250, loss is 5.580830888748169 and perplexity is 265.2919423258333
At time: 78.82770276069641 and batch: 300, loss is 5.560730934143066 and perplexity is 260.01281912916903
At time: 79.74798798561096 and batch: 350, loss is 5.586980361938476 and perplexity is 266.9283744535765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.743368740739493 and perplexity of 312.11407336281906
Finished 11 epochs...
Completing Train Step...
At time: 81.59093117713928 and batch: 50, loss is 5.5324834537506105 and perplexity is 252.7708769632584
At time: 82.51485466957092 and batch: 100, loss is 5.516373891830444 and perplexity is 248.73147274319837
At time: 83.43904542922974 and batch: 150, loss is 5.5217975234985355 and perplexity is 250.0841655735808
At time: 84.35487246513367 and batch: 200, loss is 5.498966512680053 and perplexity is 244.43917688716792
At time: 85.27295660972595 and batch: 250, loss is 5.501682777404785 and perplexity is 245.1040409653243
At time: 86.18844628334045 and batch: 300, loss is 5.497979583740235 and perplexity is 244.1980517957099
At time: 87.10699081420898 and batch: 350, loss is 5.541124448776245 and perplexity is 254.9645328892745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.7147500926050645 and perplexity of 303.3083948264386
Finished 12 epochs...
Completing Train Step...
At time: 88.88480257987976 and batch: 50, loss is 5.497611436843872 and perplexity is 244.10816758715492
At time: 89.80037689208984 and batch: 100, loss is 5.485336389541626 and perplexity is 241.13004398163665
At time: 90.72392272949219 and batch: 150, loss is 5.4944088172912595 and perplexity is 243.3276325422334
At time: 91.64354729652405 and batch: 200, loss is 5.468180980682373 and perplexity is 237.02864070312623
At time: 92.56308627128601 and batch: 250, loss is 5.464824714660645 and perplexity is 236.2344430449864
At time: 93.48299503326416 and batch: 300, loss is 5.458986463546753 and perplexity is 234.85926526743117
At time: 94.4102110862732 and batch: 350, loss is 5.5046814441680905 and perplexity is 245.84012939690058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.6800863331761855 and perplexity of 292.97472226906604
Finished 13 epochs...
Completing Train Step...
At time: 96.23764705657959 and batch: 50, loss is 5.4620387077331545 and perplexity is 235.57720820567127
At time: 97.17232847213745 and batch: 100, loss is 5.447043714523315 and perplexity is 232.07108243006664
At time: 98.0880970954895 and batch: 150, loss is 5.449818449020386 and perplexity is 232.715912270339
At time: 99.00787997245789 and batch: 200, loss is 5.41967001914978 and perplexity is 225.8045990134815
At time: 99.93295621871948 and batch: 250, loss is 5.4195819664001466 and perplexity is 225.78471717299658
At time: 100.85632371902466 and batch: 300, loss is 5.420055770874024 and perplexity is 225.8917203294116
At time: 101.78162288665771 and batch: 350, loss is 5.467650852203369 and perplexity is 236.90301837127984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.645341018150592 and perplexity of 282.9700376898065
Finished 14 epochs...
Completing Train Step...
At time: 103.53268456459045 and batch: 50, loss is 5.426323184967041 and perplexity is 227.31192313157777
At time: 104.46140456199646 and batch: 100, loss is 5.415159387588501 and perplexity is 224.78837129988474
At time: 105.38476467132568 and batch: 150, loss is 5.418529996871948 and perplexity is 225.54732341798837
At time: 106.30458855628967 and batch: 200, loss is 5.400222873687744 and perplexity is 221.4557673679903
At time: 107.23311519622803 and batch: 250, loss is 5.405859041213989 and perplexity is 222.70745321463107
At time: 108.16137504577637 and batch: 300, loss is 5.407474431991577 and perplexity is 223.06750351332457
At time: 109.07890439033508 and batch: 350, loss is 5.4526906585693355 and perplexity is 233.3852819605939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.632163212217134 and perplexity of 279.265575398639
Finished 15 epochs...
Completing Train Step...
At time: 110.867027759552 and batch: 50, loss is 5.412649993896484 and perplexity is 224.22499593982735
At time: 111.79748630523682 and batch: 100, loss is 5.401449089050293 and perplexity is 221.7274863910357
At time: 112.72287058830261 and batch: 150, loss is 5.402640161514282 and perplexity is 221.99173723429703
At time: 113.63881587982178 and batch: 200, loss is 5.3871697998046875 and perplexity is 218.58387317490082
At time: 114.56104755401611 and batch: 250, loss is 5.3892213821411135 and perplexity is 219.0327763115546
At time: 115.48085618019104 and batch: 300, loss is 5.389451971054077 and perplexity is 219.08328866491826
At time: 116.39890241622925 and batch: 350, loss is 5.436358375549316 and perplexity is 229.60452571405986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.623691953461746 and perplexity of 276.909836572282
Finished 16 epochs...
Completing Train Step...
At time: 118.16111040115356 and batch: 50, loss is 5.3945639705657955 and perplexity is 220.20610981500485
At time: 119.07610893249512 and batch: 100, loss is 5.377403154373169 and perplexity is 216.45943320156726
At time: 119.99055695533752 and batch: 150, loss is 5.375253171920776 and perplexity is 215.99454914388656
At time: 120.9198215007782 and batch: 200, loss is 5.356548118591308 and perplexity is 211.99191106521275
At time: 121.83453011512756 and batch: 250, loss is 5.361621026992798 and perplexity is 213.07005897256877
At time: 122.75106835365295 and batch: 300, loss is 5.3605615234375 and perplexity is 212.8444300359967
At time: 123.66583514213562 and batch: 350, loss is 5.404117641448974 and perplexity is 222.3199679891962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.593038624730603 and perplexity of 268.5504050755222
Finished 17 epochs...
Completing Train Step...
At time: 125.42714190483093 and batch: 50, loss is 5.3617151260375975 and perplexity is 213.0901096049514
At time: 126.35811424255371 and batch: 100, loss is 5.346010360717774 and perplexity is 209.7697206509612
At time: 127.27339291572571 and batch: 150, loss is 5.349362630844116 and perplexity is 210.47410540325237
At time: 128.19709181785583 and batch: 200, loss is 5.328214778900146 and perplexity is 206.0697654937606
At time: 129.11377787590027 and batch: 250, loss is 5.327868967056275 and perplexity is 205.99851644828095
At time: 130.03084993362427 and batch: 300, loss is 5.3212485599517825 and perplexity is 204.63922689635294
At time: 130.94740176200867 and batch: 350, loss is 5.356786136627197 and perplexity is 212.04237496893018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.544598809603987 and perplexity of 255.85191232033262
Finished 18 epochs...
Completing Train Step...
At time: 132.73932194709778 and batch: 50, loss is 5.313183221817017 and perplexity is 202.99538033593714
At time: 133.655348777771 and batch: 100, loss is 5.292057409286499 and perplexity is 198.75191908998653
At time: 134.57338571548462 and batch: 150, loss is 5.2945208168029785 and perplexity is 199.242129607626
At time: 135.49490571022034 and batch: 200, loss is 5.281763858795166 and perplexity is 196.71654973599308
At time: 136.41391801834106 and batch: 250, loss is 5.281493978500366 and perplexity is 196.66346697887556
At time: 137.32700490951538 and batch: 300, loss is 5.280079908370972 and perplexity is 196.38556757560735
At time: 138.2413845062256 and batch: 350, loss is 5.327782163619995 and perplexity is 205.98063584524482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.519664895945582 and perplexity of 249.55139749139053
Finished 19 epochs...
Completing Train Step...
At time: 140.00946044921875 and batch: 50, loss is 5.284918355941772 and perplexity is 197.33807130945144
At time: 140.93093276023865 and batch: 100, loss is 5.266181812286377 and perplexity is 193.67506115645668
At time: 141.84867334365845 and batch: 150, loss is 5.267872743606567 and perplexity is 194.00282942202122
At time: 142.76797342300415 and batch: 200, loss is 5.257069225311279 and perplexity is 191.91819726165372
At time: 143.69845294952393 and batch: 250, loss is 5.259352588653565 and perplexity is 192.35691692558308
At time: 144.6156919002533 and batch: 300, loss is 5.25784008026123 and perplexity is 192.06619538921456
At time: 145.5347409248352 and batch: 350, loss is 5.30895339012146 and perplexity is 202.13855742797261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.507497853246228 and perplexity of 246.53349170673482
Finished 20 epochs...
Completing Train Step...
At time: 147.30835556983948 and batch: 50, loss is 5.267554569244385 and perplexity is 193.9411125143978
At time: 148.2455017566681 and batch: 100, loss is 5.2480250835418705 and perplexity is 190.19028738649345
At time: 149.1702582836151 and batch: 150, loss is 5.246632671356201 and perplexity is 189.92564839877323
At time: 150.10359907150269 and batch: 200, loss is 5.2350914096832275 and perplexity is 187.74626738667322
At time: 151.01873922348022 and batch: 250, loss is 5.2394881916046145 and perplexity is 188.57356416994358
At time: 151.93668913841248 and batch: 300, loss is 5.236626243591308 and perplexity is 188.0346479755151
At time: 152.8601474761963 and batch: 350, loss is 5.282316741943359 and perplexity is 196.82534107298991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.4770702493601835 and perplexity of 239.145044680121
Finished 21 epochs...
Completing Train Step...
At time: 154.6225287914276 and batch: 50, loss is 5.2352142333984375 and perplexity is 187.76932849694708
At time: 155.56161546707153 and batch: 100, loss is 5.214708890914917 and perplexity is 183.958261263035
At time: 156.48474407196045 and batch: 150, loss is 5.217626600265503 and perplexity is 184.49578178503913
At time: 157.39917969703674 and batch: 200, loss is 5.20762936592102 and perplexity is 182.66052325908615
At time: 158.31558108329773 and batch: 250, loss is 5.210798616409302 and perplexity is 183.24033851619564
At time: 159.24270486831665 and batch: 300, loss is 5.207603693008423 and perplexity is 182.65583389163262
At time: 160.1654851436615 and batch: 350, loss is 5.261423044204712 and perplexity is 192.7555959532151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.461649927599677 and perplexity of 235.48563826866027
Finished 22 epochs...
Completing Train Step...
At time: 161.97052907943726 and batch: 50, loss is 5.219560928344727 and perplexity is 184.85300253592655
At time: 162.89834856987 and batch: 100, loss is 5.200479478836059 and perplexity is 181.35917891926613
At time: 163.81797790527344 and batch: 150, loss is 5.198093748092651 and perplexity is 180.92702046235422
At time: 164.75058317184448 and batch: 200, loss is 5.191244630813599 and perplexity is 179.6920640884662
At time: 165.67997813224792 and batch: 250, loss is 5.194135694503784 and perplexity is 180.21231697010788
At time: 166.59747052192688 and batch: 300, loss is 5.193893270492554 and perplexity is 180.1686344724119
At time: 167.53052806854248 and batch: 350, loss is 5.24687385559082 and perplexity is 189.97146099533285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.455207561624461 and perplexity of 233.9734299350154
Finished 23 epochs...
Completing Train Step...
At time: 169.31904458999634 and batch: 50, loss is 5.204952449798584 and perplexity is 182.1722102375406
At time: 170.2374131679535 and batch: 100, loss is 5.187598810195923 and perplexity is 179.038131840555
At time: 171.15221858024597 and batch: 150, loss is 5.188251390457153 and perplexity is 179.15500672237434
At time: 172.06825304031372 and batch: 200, loss is 5.181269874572754 and perplexity is 177.90858920172286
At time: 172.98355078697205 and batch: 250, loss is 5.1850810146331785 and perplexity is 178.58791743839353
At time: 173.8996183872223 and batch: 300, loss is 5.181471881866455 and perplexity is 177.94453166455116
At time: 174.81732034683228 and batch: 350, loss is 5.2373353958129885 and perplexity is 188.16804045607654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.443921845534752 and perplexity of 231.3477166288146
Finished 24 epochs...
Completing Train Step...
At time: 176.578617811203 and batch: 50, loss is 5.193682241439819 and perplexity is 180.13061766761294
At time: 177.50968956947327 and batch: 100, loss is 5.173169631958007 and perplexity is 176.47330737929755
At time: 178.42468404769897 and batch: 150, loss is 5.1722214412689205 and perplexity is 176.30605633783907
At time: 179.35459542274475 and batch: 200, loss is 5.169378023147583 and perplexity is 175.80545654688532
At time: 180.28297328948975 and batch: 250, loss is 5.175322971343994 and perplexity is 176.85372373842222
At time: 181.2100694179535 and batch: 300, loss is 5.170891752243042 and perplexity is 176.07177990145215
At time: 182.1247193813324 and batch: 350, loss is 5.225989246368409 and perplexity is 186.0451239865699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.44171458277209 and perplexity of 230.83763457832464
Finished 25 epochs...
Completing Train Step...
At time: 183.89599561691284 and batch: 50, loss is 5.183608303070068 and perplexity is 178.3251025201359
At time: 184.82058763504028 and batch: 100, loss is 5.168730993270874 and perplexity is 175.69174195634682
At time: 185.73537921905518 and batch: 150, loss is 5.163727693557739 and perplexity is 174.81489889769594
At time: 186.66343307495117 and batch: 200, loss is 5.162068004608154 and perplexity is 174.52500117833858
At time: 187.5920910835266 and batch: 250, loss is 5.1668723487854 and perplexity is 175.36549674986287
At time: 188.52391839027405 and batch: 300, loss is 5.161142559051513 and perplexity is 174.3635625043491
At time: 189.43909645080566 and batch: 350, loss is 5.217488641738892 and perplexity is 184.47033077445036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.437175882273707 and perplexity of 229.79230570302548
Finished 26 epochs...
Completing Train Step...
At time: 191.22640228271484 and batch: 50, loss is 5.1781172180175785 and perplexity is 177.34858773173912
At time: 192.1428301334381 and batch: 100, loss is 5.15847749710083 and perplexity is 173.89949147203265
At time: 193.059006690979 and batch: 150, loss is 5.155237979888916 and perplexity is 173.33705258337096
At time: 193.97595930099487 and batch: 200, loss is 5.152211971282959 and perplexity is 172.81332597077602
At time: 194.89232921600342 and batch: 250, loss is 5.158122596740722 and perplexity is 173.8377854302825
At time: 195.80974888801575 and batch: 300, loss is 5.151833543777466 and perplexity is 172.74794102742365
At time: 196.72724390029907 and batch: 350, loss is 5.209400081634522 and perplexity is 182.98424964697313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.432694007610452 and perplexity of 228.7647098882337
Finished 27 epochs...
Completing Train Step...
At time: 198.48551392555237 and batch: 50, loss is 5.165925846099854 and perplexity is 175.19959136356954
At time: 199.41307258605957 and batch: 100, loss is 5.149673433303833 and perplexity is 172.37518912829324
At time: 200.326917886734 and batch: 150, loss is 5.143868179321289 and perplexity is 171.37740636847053
At time: 201.25198316574097 and batch: 200, loss is 5.141089715957642 and perplexity is 170.90190141596628
At time: 202.17390727996826 and batch: 250, loss is 5.146468000411987 and perplexity is 171.82353664217845
At time: 203.0963191986084 and batch: 300, loss is 5.142495765686035 and perplexity is 171.14236700170363
At time: 204.0119993686676 and batch: 350, loss is 5.199158220291138 and perplexity is 181.1197147862716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.422680032664332 and perplexity of 226.4852978505717
Finished 28 epochs...
Completing Train Step...
At time: 205.75974106788635 and batch: 50, loss is 5.156982011795044 and perplexity is 173.63962170199653
At time: 206.6872079372406 and batch: 100, loss is 5.13794243812561 and perplexity is 170.36487118514773
At time: 207.60262799263 and batch: 150, loss is 5.132844104766845 and perplexity is 169.49850466850066
At time: 208.5299370288849 and batch: 200, loss is 5.129664134979248 and perplexity is 168.96036063897483
At time: 209.4456307888031 and batch: 250, loss is 5.136017446517944 and perplexity is 170.03723568684117
At time: 210.36062026023865 and batch: 300, loss is 5.128361473083496 and perplexity is 168.74040570983433
At time: 211.27724313735962 and batch: 350, loss is 5.184900512695313 and perplexity is 178.55568488232342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.412154099036908 and perplexity of 224.11383148217183
Finished 29 epochs...
Completing Train Step...
At time: 213.05869221687317 and batch: 50, loss is 5.1394690322875975 and perplexity is 170.62514782075525
At time: 213.9766948223114 and batch: 100, loss is 5.121322298049927 and perplexity is 167.55678320915447
At time: 214.9193799495697 and batch: 150, loss is 5.116456718444824 and perplexity is 166.7435024875789
At time: 215.842059135437 and batch: 200, loss is 5.114707937240601 and perplexity is 166.4521594064041
At time: 216.77280259132385 and batch: 250, loss is 5.123325471878052 and perplexity is 167.89276497459375
At time: 217.70288014411926 and batch: 300, loss is 5.113670272827148 and perplexity is 166.2795275065254
At time: 218.61903166770935 and batch: 350, loss is 5.171880331039429 and perplexity is 176.24592679448506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.40624684300916 and perplexity of 222.79383631346337
Finished 30 epochs...
Completing Train Step...
At time: 220.45741868019104 and batch: 50, loss is 5.131098546981812 and perplexity is 169.2028933125368
At time: 221.3739070892334 and batch: 100, loss is 5.113107137680053 and perplexity is 166.1859160207769
At time: 222.28744196891785 and batch: 150, loss is 5.109199142456054 and perplexity is 165.5377296342712
At time: 223.20161151885986 and batch: 200, loss is 5.107474851608276 and perplexity is 165.2525403873649
At time: 224.1152217388153 and batch: 250, loss is 5.115948743820191 and perplexity is 166.65882252894872
At time: 225.02948260307312 and batch: 300, loss is 5.106912822723388 and perplexity is 165.15968978117155
At time: 225.9435486793518 and batch: 350, loss is 5.16237512588501 and perplexity is 174.57860975128898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.399933913658405 and perplexity of 221.3917847476095
Finished 31 epochs...
Completing Train Step...
At time: 227.6945013999939 and batch: 50, loss is 5.1191020011901855 and perplexity is 167.18517010792993
At time: 228.6232554912567 and batch: 100, loss is 5.10316759109497 and perplexity is 164.5422853702089
At time: 229.53987646102905 and batch: 150, loss is 5.0988084506988525 and perplexity is 163.82658350296242
At time: 230.4569923877716 and batch: 200, loss is 5.095887870788574 and perplexity is 163.34881289763666
At time: 231.3725447654724 and batch: 250, loss is 5.106578540802002 and perplexity is 165.10448910954324
At time: 232.28792929649353 and batch: 300, loss is 5.094975337982178 and perplexity is 163.19981973789393
At time: 233.2038073539734 and batch: 350, loss is 5.151663541793823 and perplexity is 172.7185760309034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.39239238870555 and perplexity of 219.72843306621436
Finished 32 epochs...
Completing Train Step...
At time: 234.96757292747498 and batch: 50, loss is 5.1091595649719235 and perplexity is 165.5311781970491
At time: 235.88195824623108 and batch: 100, loss is 5.091778411865234 and perplexity is 162.67891506242572
At time: 236.79783844947815 and batch: 150, loss is 5.088231029510498 and perplexity is 162.1028531097419
At time: 237.71235632896423 and batch: 200, loss is 5.084825134277343 and perplexity is 161.55168691501174
At time: 238.6481807231903 and batch: 250, loss is 5.091503038406372 and perplexity is 162.63412377435
At time: 239.56237936019897 and batch: 300, loss is 5.085398283004761 and perplexity is 161.64430659867065
At time: 240.47700333595276 and batch: 350, loss is 5.143038234710693 and perplexity is 171.23523162042105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.388725543844289 and perplexity of 218.924198393569
Finished 33 epochs...
Completing Train Step...
At time: 242.2384853363037 and batch: 50, loss is 5.10062578201294 and perplexity is 164.12458138166423
At time: 243.15492272377014 and batch: 100, loss is 5.082672290802002 and perplexity is 161.20426552602538
At time: 244.07176733016968 and batch: 150, loss is 5.079740800857544 and perplexity is 160.73238883145262
At time: 244.98823881149292 and batch: 200, loss is 5.079012641906738 and perplexity is 160.6153927048984
At time: 245.9048900604248 and batch: 250, loss is 5.08582184791565 and perplexity is 161.71278795711726
At time: 246.83228278160095 and batch: 300, loss is 5.08060040473938 and perplexity is 160.87061441794285
At time: 247.75157284736633 and batch: 350, loss is 5.137264385223388 and perplexity is 170.2493939440675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.381342920763739 and perplexity of 217.31391492535042
Finished 34 epochs...
Completing Train Step...
At time: 249.502947807312 and batch: 50, loss is 5.0935145854949955 and perplexity is 162.96159922826456
At time: 250.43135023117065 and batch: 100, loss is 5.073298149108886 and perplexity is 159.70017469399633
At time: 251.34582805633545 and batch: 150, loss is 5.072396936416626 and perplexity is 159.55631570311684
At time: 252.2721242904663 and batch: 200, loss is 5.0719899082183835 and perplexity is 159.4913849986282
At time: 253.18236756324768 and batch: 250, loss is 5.078531942367554 and perplexity is 160.53820351352994
At time: 254.09365701675415 and batch: 300, loss is 5.072475957870483 and perplexity is 159.5689245733348
At time: 255.00434708595276 and batch: 350, loss is 5.129443550109864 and perplexity is 168.92309465019986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.379048314587823 and perplexity of 216.81583673923208
Finished 35 epochs...
Completing Train Step...
At time: 256.7306430339813 and batch: 50, loss is 5.086509504318237 and perplexity is 161.82402903457228
At time: 257.65432810783386 and batch: 100, loss is 5.067584505081177 and perplexity is 158.79030655124603
At time: 258.5663859844208 and batch: 150, loss is 5.066992530822754 and perplexity is 158.6963345945145
At time: 259.47979187965393 and batch: 200, loss is 5.0680832672119145 and perplexity is 158.86952489679518
At time: 260.3931555747986 and batch: 250, loss is 5.072444305419922 and perplexity is 159.5638739057721
At time: 261.31795620918274 and batch: 300, loss is 5.066386623382568 and perplexity is 158.6002084293737
At time: 262.23733735084534 and batch: 350, loss is 5.121864109039307 and perplexity is 167.64759191399793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.37585764917834 and perplexity of 216.1251524059759
Finished 36 epochs...
Completing Train Step...
At time: 263.9868040084839 and batch: 50, loss is 5.080246210098267 and perplexity is 160.81364499813054
At time: 264.89902329444885 and batch: 100, loss is 5.064111652374268 and perplexity is 158.23980765937569
At time: 265.8250789642334 and batch: 150, loss is 5.0599191379547115 and perplexity is 157.5777737471013
At time: 266.73761463165283 and batch: 200, loss is 5.059718465805053 and perplexity is 157.54615544906716
At time: 267.6502923965454 and batch: 250, loss is 5.065447120666504 and perplexity is 158.45127307632535
At time: 268.5619513988495 and batch: 300, loss is 5.061670360565185 and perplexity is 157.85396927673762
At time: 269.4727990627289 and batch: 350, loss is 5.1164666938781735 and perplexity is 166.74516583457074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.373762985755658 and perplexity of 215.6729167605948
Finished 37 epochs...
Completing Train Step...
At time: 271.2131817340851 and batch: 50, loss is 5.0736766529083255 and perplexity is 159.76063325806538
At time: 272.1247477531433 and batch: 100, loss is 5.056662130355835 and perplexity is 157.06537663420127
At time: 273.036021232605 and batch: 150, loss is 5.053872203826904 and perplexity is 156.62778647918293
At time: 273.9483118057251 and batch: 200, loss is 5.054957551956177 and perplexity is 156.79787443985853
At time: 274.86262011528015 and batch: 250, loss is 5.0596346759796145 and perplexity is 157.53295523723295
At time: 275.7739837169647 and batch: 300, loss is 5.048415670394897 and perplexity is 155.77546918949395
At time: 276.6852457523346 and batch: 350, loss is 5.103873033523559 and perplexity is 164.65840143138868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.367860465214171 and perplexity of 214.40365255326947
Finished 38 epochs...
Completing Train Step...
At time: 278.41680669784546 and batch: 50, loss is 5.064107532501221 and perplexity is 158.23915573280004
At time: 279.3555338382721 and batch: 100, loss is 5.045563716888427 and perplexity is 155.33183770279882
At time: 280.27718448638916 and batch: 150, loss is 5.040451564788818 and perplexity is 154.53978399702459
At time: 281.2006080150604 and batch: 200, loss is 5.043206415176392 and perplexity is 154.9661049364473
At time: 282.1352696418762 and batch: 250, loss is 5.0483996868133545 and perplexity is 155.77297935947806
At time: 283.06864523887634 and batch: 300, loss is 5.0421281337738035 and perplexity is 154.79909792392215
At time: 284.00193190574646 and batch: 350, loss is 5.0957667255401615 and perplexity is 163.3290251637395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.35809641870959 and perplexity of 212.3203923944097
Finished 39 epochs...
Completing Train Step...
At time: 285.76843452453613 and batch: 50, loss is 5.05335072517395 and perplexity is 156.5461297250522
At time: 286.69014620780945 and batch: 100, loss is 5.033027038574219 and perplexity is 153.39664819155834
At time: 287.63255739212036 and batch: 150, loss is 5.031913270950318 and perplexity is 153.22589507848338
At time: 288.55560970306396 and batch: 200, loss is 5.0348313045501705 and perplexity is 153.67366637680868
At time: 289.47885370254517 and batch: 250, loss is 5.040859394073486 and perplexity is 154.60282270022566
At time: 290.4069514274597 and batch: 300, loss is 5.031661872863769 and perplexity is 153.18737922326002
At time: 291.34256076812744 and batch: 350, loss is 5.084709625244141 and perplexity is 161.533027313541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.351754024110991 and perplexity of 210.97803406741858
Finished 40 epochs...
Completing Train Step...
At time: 293.1051275730133 and batch: 50, loss is 5.046329288482666 and perplexity is 155.45080087702814
At time: 294.0271623134613 and batch: 100, loss is 5.025362548828125 and perplexity is 152.2254352626623
At time: 294.9457354545593 and batch: 150, loss is 5.023643093109131 and perplexity is 151.96391526788156
At time: 295.87065505981445 and batch: 200, loss is 5.025132169723511 and perplexity is 152.19036974252373
At time: 296.79792857170105 and batch: 250, loss is 5.032057008743286 and perplexity is 153.2479210134093
At time: 297.7249312400818 and batch: 300, loss is 5.023119907379151 and perplexity is 151.8844307103477
At time: 298.651104927063 and batch: 350, loss is 5.078446998596191 and perplexity is 160.52456737223656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.347686767578125 and perplexity of 210.12167497665158
Finished 41 epochs...
Completing Train Step...
At time: 300.39887166023254 and batch: 50, loss is 5.037127380371094 and perplexity is 154.02691815773133
At time: 301.35076117515564 and batch: 100, loss is 5.018462162017823 and perplexity is 151.17863668815735
At time: 302.27536702156067 and batch: 150, loss is 5.01406530380249 and perplexity is 150.51538483814346
At time: 303.20009779930115 and batch: 200, loss is 5.018753337860107 and perplexity is 151.22266266436975
At time: 304.1240220069885 and batch: 250, loss is 5.024903707504272 and perplexity is 152.15560396439022
At time: 305.04963779449463 and batch: 300, loss is 5.018184251785279 and perplexity is 151.13662843561164
At time: 305.97420597076416 and batch: 350, loss is 5.072130146026612 and perplexity is 159.51375328929507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.345840980266702 and perplexity of 209.73419277000545
Finished 42 epochs...
Completing Train Step...
At time: 307.7276167869568 and batch: 50, loss is 5.0316835880279545 and perplexity is 153.19070574846884
At time: 308.6581108570099 and batch: 100, loss is 5.011642255783081 and perplexity is 150.1511203264439
At time: 309.5844576358795 and batch: 150, loss is 5.010193014144898 and perplexity is 149.93367267600033
At time: 310.5110819339752 and batch: 200, loss is 5.011157360076904 and perplexity is 150.0783303421216
At time: 311.43841457366943 and batch: 250, loss is 5.018022184371948 and perplexity is 151.11213609794012
At time: 312.3659439086914 and batch: 300, loss is 5.009641771316528 and perplexity is 149.85104559009315
At time: 313.2921333312988 and batch: 350, loss is 5.06283166885376 and perplexity is 158.03739288470695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.342184658708243 and perplexity of 208.9687373477766
Finished 43 epochs...
Completing Train Step...
At time: 315.0760633945465 and batch: 50, loss is 5.024086933135987 and perplexity is 152.03137790631732
At time: 315.9912602901459 and batch: 100, loss is 5.006661405563355 and perplexity is 149.4050995370073
At time: 316.9159996509552 and batch: 150, loss is 5.001518220901489 and perplexity is 148.63865419514977
At time: 317.8436748981476 and batch: 200, loss is 5.007978076934815 and perplexity is 149.6019465172713
At time: 318.76913022994995 and batch: 250, loss is 5.013342800140381 and perplexity is 150.40667619731863
At time: 319.6959402561188 and batch: 300, loss is 5.006038084030151 and perplexity is 149.31200113942768
At time: 320.62183380126953 and batch: 350, loss is 5.059151458740234 and perplexity is 157.45685098641565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.341381730704472 and perplexity of 208.80101783900324
Finished 44 epochs...
Completing Train Step...
At time: 322.3829028606415 and batch: 50, loss is 5.019780864715576 and perplexity is 151.37812787007144
At time: 323.3030045032501 and batch: 100, loss is 5.003961524963379 and perplexity is 149.00226765092572
At time: 324.2284140586853 and batch: 150, loss is 4.993027124404907 and perplexity is 147.38189223370227
At time: 325.1531238555908 and batch: 200, loss is 5.001634569168091 and perplexity is 148.6559490510091
At time: 326.0778000354767 and batch: 250, loss is 5.00643551826477 and perplexity is 149.37135463409413
At time: 327.0033938884735 and batch: 300, loss is 4.99996247291565 and perplexity is 148.40758969393877
At time: 327.9273040294647 and batch: 350, loss is 5.05389295578003 and perplexity is 156.63103684539175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.334244300579202 and perplexity of 207.31602099884975
Finished 45 epochs...
Completing Train Step...
At time: 329.676007270813 and batch: 50, loss is 5.013181734085083 and perplexity is 150.38245273813388
At time: 330.6078326702118 and batch: 100, loss is 4.995879287719727 and perplexity is 147.80284949405933
At time: 331.52726554870605 and batch: 150, loss is 4.991950941085816 and perplexity is 147.2233676158188
At time: 332.47475814819336 and batch: 200, loss is 5.000210962295532 and perplexity is 148.44447198611084
At time: 333.4120376110077 and batch: 250, loss is 5.000705518722534 and perplexity is 148.51790431050176
At time: 334.35600566864014 and batch: 300, loss is 4.995007524490356 and perplexity is 147.67405655130665
At time: 335.29598593711853 and batch: 350, loss is 5.05012300491333 and perplexity is 156.04165719649134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.337442858465787 and perplexity of 207.98019492595628
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 337.0802307128906 and batch: 50, loss is 4.982837285995483 and perplexity is 145.88772018189837
At time: 338.0087785720825 and batch: 100, loss is 4.9322050762176515 and perplexity is 138.68498638463012
At time: 338.9409191608429 and batch: 150, loss is 4.887239694595337 and perplexity is 132.58708762219229
At time: 339.8753876686096 and batch: 200, loss is 4.8749009799957275 and perplexity is 130.96118479168513
At time: 340.8078899383545 and batch: 250, loss is 4.8699518585205075 and perplexity is 130.3146432057242
At time: 341.7409636974335 and batch: 300, loss is 4.852099046707154 and perplexity is 128.00880449960022
At time: 342.6749384403229 and batch: 350, loss is 4.92370831489563 and perplexity is 137.51160518570717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.246348413927802 and perplexity of 189.87166829485858
Finished 47 epochs...
Completing Train Step...
At time: 344.4536020755768 and batch: 50, loss is 4.919495677947998 and perplexity is 136.93353716720108
At time: 345.3728563785553 and batch: 100, loss is 4.88915524482727 and perplexity is 132.8413082571014
At time: 346.2990674972534 and batch: 150, loss is 4.857095394134522 and perplexity is 128.6499813980821
At time: 347.2279472351074 and batch: 200, loss is 4.852868566513061 and perplexity is 128.1073477206106
At time: 348.16642785072327 and batch: 250, loss is 4.862986841201782 and perplexity is 129.41015301035966
At time: 349.10359168052673 and batch: 300, loss is 4.85662425994873 and perplexity is 128.5893842696543
At time: 350.032399892807 and batch: 350, loss is 4.922711267471313 and perplexity is 137.37456792162112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.240360654633621 and perplexity of 188.73815942412614
Finished 48 epochs...
Completing Train Step...
At time: 351.7966969013214 and batch: 50, loss is 4.903024816513062 and perplexity is 134.69659659112835
At time: 352.72941303253174 and batch: 100, loss is 4.875029077529907 and perplexity is 130.97796167104573
At time: 353.6516580581665 and batch: 150, loss is 4.848206491470337 and perplexity is 127.51149169400905
At time: 354.57904958724976 and batch: 200, loss is 4.849431800842285 and perplexity is 127.66782848071769
At time: 355.5071671009064 and batch: 250, loss is 4.863975315093994 and perplexity is 129.53813481091115
At time: 356.4478540420532 and batch: 300, loss is 4.85766432762146 and perplexity is 128.7231955055974
At time: 357.37551259994507 and batch: 350, loss is 4.9204932308197025 and perplexity is 137.07020376518034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.237251544820851 and perplexity of 188.152263040681
Finished 49 epochs...
Completing Train Step...
At time: 359.13421058654785 and batch: 50, loss is 4.8947797203063965 and perplexity is 133.59057608202377
At time: 360.0633180141449 and batch: 100, loss is 4.867927799224853 and perplexity is 130.0511453983355
At time: 360.98681473731995 and batch: 150, loss is 4.8446139717102055 and perplexity is 127.05422600027141
At time: 361.91350054740906 and batch: 200, loss is 4.847682275772095 and perplexity is 127.4446656855085
At time: 362.8400011062622 and batch: 250, loss is 4.863693933486939 and perplexity is 129.50169029001248
At time: 363.7666552066803 and batch: 300, loss is 4.8567117404937745 and perplexity is 128.60063383112885
At time: 364.70005536079407 and batch: 350, loss is 4.917918481826782 and perplexity is 136.71773634835844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.235665156923491 and perplexity of 187.8540171970785
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73cc4becf8>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.1407018580502576, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.874140291698923, 'data': 'ptb', 'lr': 0.9007984133907654}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.146679401397705 and batch: 50, loss is 7.310040636062622 and perplexity is 1495.2379485398446
At time: 2.078580856323242 and batch: 100, loss is 6.056477937698364 and perplexity is 426.8693257476946
At time: 2.997831344604492 and batch: 150, loss is 5.857630996704102 and perplexity is 349.89426075299735
At time: 3.9167051315307617 and batch: 200, loss is 5.716050672531128 and perplexity is 303.7031282717126
At time: 4.836035966873169 and batch: 250, loss is 5.649847497940064 and perplexity is 284.24811409545833
At time: 5.755237579345703 and batch: 300, loss is 5.588623905181885 and perplexity is 267.36744349552674
At time: 6.674217462539673 and batch: 350, loss is 5.571180095672608 and perplexity is 262.7439793932886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.642285051016972 and perplexity of 282.106610529297
Finished 1 epochs...
Completing Train Step...
At time: 8.412303686141968 and batch: 50, loss is 5.438758039474488 and perplexity is 230.15616101642865
At time: 9.3374924659729 and batch: 100, loss is 5.366731357574463 and perplexity is 214.1617043696663
At time: 10.25404143333435 and batch: 150, loss is 5.321360635757446 and perplexity is 204.662163287861
At time: 11.165029287338257 and batch: 200, loss is 5.273190212249756 and perplexity is 195.03718101300777
At time: 12.08385419845581 and batch: 250, loss is 5.247655563354492 and perplexity is 190.12002121904405
At time: 12.998707294464111 and batch: 300, loss is 5.224665870666504 and perplexity is 185.79907923076266
At time: 13.908492803573608 and batch: 350, loss is 5.240255441665649 and perplexity is 188.71830276681087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.353344621329472 and perplexity of 211.313882170288
Finished 2 epochs...
Completing Train Step...
At time: 15.65927767753601 and batch: 50, loss is 5.174124412536621 and perplexity is 176.64188112849183
At time: 16.573644399642944 and batch: 100, loss is 5.112346000671387 and perplexity is 166.05947389576906
At time: 17.49970817565918 and batch: 150, loss is 5.0834622478485105 and perplexity is 161.3316602830731
At time: 18.413958311080933 and batch: 200, loss is 5.062846288681031 and perplexity is 158.03970338098267
At time: 19.327833652496338 and batch: 250, loss is 5.048978614807129 and perplexity is 155.86318680719177
At time: 20.24315857887268 and batch: 300, loss is 5.036465749740601 and perplexity is 153.9250429363576
At time: 21.157546758651733 and batch: 350, loss is 5.066017923355102 and perplexity is 158.54174330687812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.187147995521283 and perplexity of 178.9574370139737
Finished 3 epochs...
Completing Train Step...
At time: 22.907289505004883 and batch: 50, loss is 5.008537006378174 and perplexity is 149.68558682230082
At time: 23.817758798599243 and batch: 100, loss is 4.947398815155029 and perplexity is 140.80821893737036
At time: 24.729156494140625 and batch: 150, loss is 4.926109495162964 and perplexity is 137.84219207913333
At time: 25.638598918914795 and batch: 200, loss is 4.9204163742065425 and perplexity is 137.0596694183761
At time: 26.54874062538147 and batch: 250, loss is 4.91330397605896 and perplexity is 136.08830493975603
At time: 27.458595037460327 and batch: 300, loss is 4.904041604995728 and perplexity is 134.83362419143378
At time: 28.37459444999695 and batch: 350, loss is 4.941809225082397 and perplexity is 140.02335429395586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.069111791150323 and perplexity of 159.0330100660282
Finished 4 epochs...
Completing Train Step...
At time: 30.114242792129517 and batch: 50, loss is 4.890097799301148 and perplexity is 132.96657745376157
At time: 31.041247129440308 and batch: 100, loss is 4.829605140686035 and perplexity is 125.16152970051877
At time: 31.956393718719482 and batch: 150, loss is 4.809356956481934 and perplexity is 122.65272111578739
At time: 32.87159085273743 and batch: 200, loss is 4.814588575363159 and perplexity is 123.29607483075857
At time: 33.7855863571167 and batch: 250, loss is 4.813003358840942 and perplexity is 123.10077869004361
At time: 34.703890800476074 and batch: 300, loss is 4.803997783660889 and perplexity is 121.99716218217662
At time: 35.62113881111145 and batch: 350, loss is 4.847645301818847 and perplexity is 127.43995363950981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.988043423356681 and perplexity of 146.64921218852288
Finished 5 epochs...
Completing Train Step...
At time: 37.3766450881958 and batch: 50, loss is 4.798486127853393 and perplexity is 121.3266054512205
At time: 38.28956341743469 and batch: 100, loss is 4.739006643295288 and perplexity is 114.32058413938702
At time: 39.20606350898743 and batch: 150, loss is 4.716844358444214 and perplexity is 111.81484777130875
At time: 40.11494994163513 and batch: 200, loss is 4.731004028320313 and perplexity is 113.40937142603143
At time: 41.044427156448364 and batch: 250, loss is 4.734232254028321 and perplexity is 113.77607405536145
At time: 41.95336103439331 and batch: 300, loss is 4.723836660385132 and perplexity is 112.59943077235522
At time: 42.863407373428345 and batch: 350, loss is 4.773495006561279 and perplexity is 118.3320915207361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.918600674333243 and perplexity of 136.8110359841334
Finished 6 epochs...
Completing Train Step...
At time: 44.617064476013184 and batch: 50, loss is 4.724512996673584 and perplexity is 112.67561161247504
At time: 45.53851866722107 and batch: 100, loss is 4.66674991607666 and perplexity is 106.35152873166182
At time: 46.46819472312927 and batch: 150, loss is 4.640573644638062 and perplexity is 103.60376228290644
At time: 47.39729619026184 and batch: 200, loss is 4.662615003585816 and perplexity is 105.91268238796742
At time: 48.32587432861328 and batch: 250, loss is 4.669680147171021 and perplexity is 106.66362031500691
At time: 49.25423979759216 and batch: 300, loss is 4.65802903175354 and perplexity is 105.4280818413918
At time: 50.18337416648865 and batch: 350, loss is 4.712266540527343 and perplexity is 111.30414959131502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.868388208849677 and perplexity of 130.11103598343917
Finished 7 epochs...
Completing Train Step...
At time: 51.93890380859375 and batch: 50, loss is 4.663000764846802 and perplexity is 105.95354727941911
At time: 52.86757493019104 and batch: 100, loss is 4.605972957611084 and perplexity is 100.08030939303728
At time: 53.78421998023987 and batch: 150, loss is 4.5759485244750975 and perplexity is 97.12011626862848
At time: 54.705594539642334 and batch: 200, loss is 4.604839105606079 and perplexity is 99.96689744190489
At time: 55.628098011016846 and batch: 250, loss is 4.615583381652832 and perplexity is 101.04676016695193
At time: 56.5575635433197 and batch: 300, loss is 4.601654663085937 and perplexity is 99.64906493135099
At time: 57.49542808532715 and batch: 350, loss is 4.659081249237061 and perplexity is 105.53907349579852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.826512303845636 and perplexity of 124.77502351883734
Finished 8 epochs...
Completing Train Step...
At time: 59.25077152252197 and batch: 50, loss is 4.609097146987915 and perplexity is 100.39346816140467
At time: 60.180041551589966 and batch: 100, loss is 4.553145990371704 and perplexity is 94.93058975236819
At time: 61.10661840438843 and batch: 150, loss is 4.519746866226196 and perplexity is 91.81235422868416
At time: 62.030606269836426 and batch: 200, loss is 4.555011253356934 and perplexity is 95.10782551186358
At time: 62.95135855674744 and batch: 250, loss is 4.568155565261841 and perplexity is 96.36620458070843
At time: 63.88530349731445 and batch: 300, loss is 4.552215347290039 and perplexity is 94.84228435255484
At time: 64.82538414001465 and batch: 350, loss is 4.612195682525635 and perplexity is 100.7050233233603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.790350552262931 and perplexity of 120.34354797282828
Finished 9 epochs...
Completing Train Step...
At time: 66.60882687568665 and batch: 50, loss is 4.561528749465943 and perplexity is 95.7297147736252
At time: 67.53162670135498 and batch: 100, loss is 4.506111555099487 and perplexity is 90.56896050804818
At time: 68.45569562911987 and batch: 150, loss is 4.470112724304199 and perplexity is 87.3665707942915
At time: 69.39326906204224 and batch: 200, loss is 4.510916881561279 and perplexity is 91.00522127882715
At time: 70.33355665206909 and batch: 250, loss is 4.525873832702636 and perplexity is 92.37661227506057
At time: 71.2676215171814 and batch: 300, loss is 4.507894468307495 and perplexity is 90.73058113893927
At time: 72.20203733444214 and batch: 350, loss is 4.570310831069946 and perplexity is 96.57412334611625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.760019762762662 and perplexity of 116.74823314381386
Finished 10 epochs...
Completing Train Step...
At time: 73.96979665756226 and batch: 50, loss is 4.518992490768433 and perplexity is 91.74311935976091
At time: 74.88501620292664 and batch: 100, loss is 4.463677968978882 and perplexity is 86.806193167662
At time: 75.8066976070404 and batch: 150, loss is 4.425753955841064 and perplexity is 83.57579593321948
At time: 76.728431224823 and batch: 200, loss is 4.471188249588013 and perplexity is 87.4605862991044
At time: 77.65001583099365 and batch: 250, loss is 4.487663383483887 and perplexity is 88.91344633849505
At time: 78.57242608070374 and batch: 300, loss is 4.467697076797485 and perplexity is 87.15577865763566
At time: 79.49488377571106 and batch: 350, loss is 4.53271336555481 and perplexity is 93.01059073632935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.733391071188039 and perplexity of 113.68040781627181
Finished 11 epochs...
Completing Train Step...
At time: 81.24002408981323 and batch: 50, loss is 4.480709142684937 and perplexity is 88.29726583945624
At time: 82.18135166168213 and batch: 100, loss is 4.425396423339844 and perplexity is 83.54592021094712
At time: 83.10400247573853 and batch: 150, loss is 4.38564775466919 and perplexity is 80.29021473369198
At time: 84.02969479560852 and batch: 200, loss is 4.434962348937988 and perplexity is 84.3489490009938
At time: 84.95420026779175 and batch: 250, loss is 4.452971687316895 and perplexity is 85.88177896305122
At time: 85.88045454025269 and batch: 300, loss is 4.43097041130066 and perplexity is 84.01290443753287
At time: 86.80716753005981 and batch: 350, loss is 4.498579893112183 and perplexity is 89.8893880783236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.709712587553879 and perplexity of 111.02024671911686
Finished 12 epochs...
Completing Train Step...
At time: 88.56028771400452 and batch: 50, loss is 4.445823297500611 and perplexity is 85.27005156676265
At time: 89.4710443019867 and batch: 100, loss is 4.390679578781128 and perplexity is 80.69523912331057
At time: 90.38682866096497 and batch: 150, loss is 4.349011650085449 and perplexity is 77.40192492244387
At time: 91.30910968780518 and batch: 200, loss is 4.401712436676025 and perplexity is 81.59046761282944
At time: 92.2312421798706 and batch: 250, loss is 4.421094026565552 and perplexity is 83.18724464889219
At time: 93.15399169921875 and batch: 300, loss is 4.397349281311035 and perplexity is 81.23525122200788
At time: 94.08813118934631 and batch: 350, loss is 4.467145147323609 and perplexity is 87.10768808709832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.6885354929956895 and perplexity of 108.6938802392851
Finished 13 epochs...
Completing Train Step...
At time: 95.87799525260925 and batch: 50, loss is 4.413765506744385 and perplexity is 82.5798337077017
At time: 96.80515718460083 and batch: 100, loss is 4.3589548492431645 and perplexity is 78.17538664663077
At time: 97.73148989677429 and batch: 150, loss is 4.315360822677612 and perplexity is 74.84062276656688
At time: 98.65741968154907 and batch: 200, loss is 4.371027984619141 and perplexity is 79.12492911628033
At time: 99.58320236206055 and batch: 250, loss is 4.391638593673706 and perplexity is 80.7726641793468
At time: 100.5089156627655 and batch: 300, loss is 4.366522731781006 and perplexity is 78.76925311158195
At time: 101.432213306427 and batch: 350, loss is 4.43801441192627 and perplexity is 84.60678056528384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.669467268318965 and perplexity of 106.64091630264596
Finished 14 epochs...
Completing Train Step...
At time: 103.19726490974426 and batch: 50, loss is 4.384070463180542 and perplexity is 80.16367348382366
At time: 104.13801431655884 and batch: 100, loss is 4.329745969772339 and perplexity is 75.92499687496702
At time: 105.06842875480652 and batch: 150, loss is 4.284249515533447 and perplexity is 72.54808007695014
At time: 105.99551796913147 and batch: 200, loss is 4.342576875686645 and perplexity is 76.90546003038764
At time: 106.9217917919159 and batch: 250, loss is 4.36438720703125 and perplexity is 78.60121890650372
At time: 107.8483612537384 and batch: 300, loss is 4.33802939414978 and perplexity is 76.55652785344574
At time: 108.77376699447632 and batch: 350, loss is 4.410880861282348 and perplexity is 82.34196341581682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.652754684974408 and perplexity of 104.87348144060981
Finished 15 epochs...
Completing Train Step...
At time: 110.52849674224854 and batch: 50, loss is 4.356378335952758 and perplexity is 77.97422598179872
At time: 111.4547791481018 and batch: 100, loss is 4.302625560760498 and perplexity is 73.89355123577556
At time: 112.37424111366272 and batch: 150, loss is 4.255359907150268 and perplexity is 70.48217966451827
At time: 113.29912614822388 and batch: 200, loss is 4.316126928329468 and perplexity is 74.89798055891585
At time: 114.2227349281311 and batch: 250, loss is 4.339093799591065 and perplexity is 76.63805842135129
At time: 115.14817523956299 and batch: 300, loss is 4.31148627281189 and perplexity is 74.55121007568587
At time: 116.07374215126038 and batch: 350, loss is 4.385537071228027 and perplexity is 80.28132842822608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.637945767106681 and perplexity of 103.33186170102447
Finished 16 epochs...
Completing Train Step...
At time: 117.86546444892883 and batch: 50, loss is 4.330416927337646 and perplexity is 75.97595641995058
At time: 118.78438830375671 and batch: 100, loss is 4.277362194061279 and perplexity is 72.05013485004085
At time: 119.71276664733887 and batch: 150, loss is 4.228348903656006 and perplexity is 68.60386701718656
At time: 120.64026045799255 and batch: 200, loss is 4.291424989700317 and perplexity is 73.0705190836115
At time: 121.56818413734436 and batch: 250, loss is 4.315510845184326 and perplexity is 74.85185138665013
At time: 122.49659371376038 and batch: 300, loss is 4.2865798377990725 and perplexity is 72.71733761899657
At time: 123.43300342559814 and batch: 350, loss is 4.361814317703247 and perplexity is 78.39924660681568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.624499090786638 and perplexity of 101.95169174849761
Finished 17 epochs...
Completing Train Step...
At time: 125.20943546295166 and batch: 50, loss is 4.306020832061767 and perplexity is 74.14486628960299
At time: 126.1264898777008 and batch: 100, loss is 4.253682584762573 and perplexity is 70.36405741887084
At time: 127.04340672492981 and batch: 150, loss is 4.202937173843384 and perplexity is 66.88248832116557
At time: 127.96036028862 and batch: 200, loss is 4.2683212852478025 and perplexity is 71.40167191499626
At time: 128.8803038597107 and batch: 250, loss is 4.293380537033081 and perplexity is 73.21355175030233
At time: 129.80427885055542 and batch: 300, loss is 4.2632104015350345 and perplexity is 71.0376772325177
At time: 130.72901320457458 and batch: 350, loss is 4.339486989974976 and perplexity is 76.66819769381195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.61266984610722 and perplexity of 100.75278530050313
Finished 18 epochs...
Completing Train Step...
At time: 132.48994851112366 and batch: 50, loss is 4.282948846817017 and perplexity is 72.45378039837212
At time: 133.4243927001953 and batch: 100, loss is 4.2315054035186765 and perplexity is 68.82075724090375
At time: 134.34564423561096 and batch: 150, loss is 4.179072427749634 and perplexity is 65.30524977382699
At time: 135.27898621559143 and batch: 200, loss is 4.246533718109131 and perplexity is 69.86282790134342
At time: 136.20538997650146 and batch: 250, loss is 4.272480278015137 and perplexity is 71.6992493342952
At time: 137.14515566825867 and batch: 300, loss is 4.24119381904602 and perplexity is 69.49076173408277
At time: 138.07319355010986 and batch: 350, loss is 4.318404111862183 and perplexity is 75.06873134853475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.60235332620555 and perplexity of 99.71871038444415
Finished 19 epochs...
Completing Train Step...
At time: 139.85918354988098 and batch: 50, loss is 4.261213779449463 and perplexity is 70.89598333889813
At time: 140.78600907325745 and batch: 100, loss is 4.210625486373901 and perplexity is 67.39868358676597
At time: 141.7128164768219 and batch: 150, loss is 4.156409978866577 and perplexity is 63.8419168710631
At time: 142.63965582847595 and batch: 200, loss is 4.225953302383423 and perplexity is 68.43971620450779
At time: 143.56531953811646 and batch: 250, loss is 4.252576327323913 and perplexity is 70.28625969702529
At time: 144.49112915992737 and batch: 300, loss is 4.220465850830078 and perplexity is 68.06518512790667
At time: 145.4174313545227 and batch: 350, loss is 4.298387985229493 and perplexity is 73.58108424996824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.592938653353987 and perplexity of 98.78429684974822
Finished 20 epochs...
Completing Train Step...
At time: 147.1664559841156 and batch: 50, loss is 4.240536060333252 and perplexity is 69.44506860926815
At time: 148.08142256736755 and batch: 100, loss is 4.190878992080688 and perplexity is 66.08084998378177
At time: 149.00690412521362 and batch: 150, loss is 4.13527322769165 and perplexity is 62.50666728992765
At time: 149.9290211200714 and batch: 200, loss is 4.206443619728089 and perplexity is 67.11741979363555
At time: 150.850980758667 and batch: 250, loss is 4.233667068481445 and perplexity is 68.96968556912451
At time: 151.77318334579468 and batch: 300, loss is 4.200689115524292 and perplexity is 66.7323014645506
At time: 152.69702434539795 and batch: 350, loss is 4.279383192062378 and perplexity is 72.19589526969766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.584502647662985 and perplexity of 97.95445714694166
Finished 21 epochs...
Completing Train Step...
At time: 154.45481085777283 and batch: 50, loss is 4.220997982025146 and perplexity is 68.1014143747126
At time: 155.38275122642517 and batch: 100, loss is 4.172189693450928 and perplexity is 64.85731436980701
At time: 156.30095887184143 and batch: 150, loss is 4.115734066963196 and perplexity is 61.297193996713396
At time: 157.2197024822235 and batch: 200, loss is 4.187881636619568 and perplexity is 65.88307873064724
At time: 158.13826084136963 and batch: 250, loss is 4.215647716522216 and perplexity is 67.73802670378375
At time: 159.07678604125977 and batch: 300, loss is 4.181858367919922 and perplexity is 65.48743996014346
At time: 160.01187562942505 and batch: 350, loss is 4.261367082595825 and perplexity is 70.90685274934455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.57665647309402 and perplexity of 97.18889666443086
Finished 22 epochs...
Completing Train Step...
At time: 161.773268699646 and batch: 50, loss is 4.202357015609741 and perplexity is 66.8436971484767
At time: 162.7002444267273 and batch: 100, loss is 4.154349265098571 and perplexity is 63.71049241459275
At time: 163.61469793319702 and batch: 150, loss is 4.096536293029785 and perplexity is 60.131648064320586
At time: 164.5362229347229 and batch: 200, loss is 4.17031035900116 and perplexity is 64.73554024758504
At time: 165.45885515213013 and batch: 250, loss is 4.198487091064453 and perplexity is 66.58551697529423
At time: 166.3821837902069 and batch: 300, loss is 4.163905553817749 and perplexity is 64.32224666911014
At time: 167.3064670562744 and batch: 350, loss is 4.244253196716309 and perplexity is 69.70368576025189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.569696886786099 and perplexity of 96.51485041212167
Finished 23 epochs...
Completing Train Step...
At time: 169.07806301116943 and batch: 50, loss is 4.18474422454834 and perplexity is 65.67670028055635
At time: 169.9954686164856 and batch: 100, loss is 4.137288842201233 and perplexity is 62.63278369379188
At time: 170.91628122329712 and batch: 150, loss is 4.078111968040466 and perplexity is 59.03390666311555
At time: 171.8368480205536 and batch: 200, loss is 4.153649258613586 and perplexity is 63.665910262459214
At time: 172.76526427268982 and batch: 250, loss is 4.182087798118591 and perplexity is 65.50246648020665
At time: 173.69391536712646 and batch: 300, loss is 4.146783819198609 and perplexity is 63.23031280768268
At time: 174.6220235824585 and batch: 350, loss is 4.227912263870239 and perplexity is 68.5739183782489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.563117717874461 and perplexity of 95.88194718036956
Finished 24 epochs...
Completing Train Step...
At time: 176.4098880290985 and batch: 50, loss is 4.1679456996917725 and perplexity is 64.5826435951815
At time: 177.3266565799713 and batch: 100, loss is 4.120979876518249 and perplexity is 61.61959228335906
At time: 178.2543089389801 and batch: 150, loss is 4.060976085662841 and perplexity is 58.03092659912033
At time: 179.17207407951355 and batch: 200, loss is 4.137591695785522 and perplexity is 62.65175512946619
At time: 180.09574627876282 and batch: 250, loss is 4.166391925811768 and perplexity is 64.48237468823848
At time: 181.0201711654663 and batch: 300, loss is 4.1302611446380615 and perplexity is 62.194162486284625
At time: 181.94443130493164 and batch: 350, loss is 4.212364654541016 and perplexity is 67.5160032214811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.557307802397629 and perplexity of 95.32649629528608
Finished 25 epochs...
Completing Train Step...
At time: 183.71846508979797 and batch: 50, loss is 4.15185001373291 and perplexity is 63.55146268982542
At time: 184.66343784332275 and batch: 100, loss is 4.105309934616089 and perplexity is 60.661542746311696
At time: 185.58380913734436 and batch: 150, loss is 4.044377164840698 and perplexity is 57.07562625458345
At time: 186.51492285728455 and batch: 200, loss is 4.1224598741531375 and perplexity is 61.710856653068134
At time: 187.44185423851013 and batch: 250, loss is 4.151319313049316 and perplexity is 63.51774483296873
At time: 188.3810760974884 and batch: 300, loss is 4.114487247467041 and perplexity is 61.2208150854312
At time: 189.3158826828003 and batch: 350, loss is 4.197455835342407 and perplexity is 66.51688567420146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.55169572501347 and perplexity of 94.79301499068218
Finished 26 epochs...
Completing Train Step...
At time: 191.10592365264893 and batch: 50, loss is 4.136352019309998 and perplexity is 62.57413534413332
At time: 192.02393460273743 and batch: 100, loss is 4.090267963409424 and perplexity is 59.75590195418967
At time: 192.93904781341553 and batch: 150, loss is 4.028545141220093 and perplexity is 56.17911907888829
At time: 193.8555679321289 and batch: 200, loss is 4.107811636924744 and perplexity is 60.81348985181213
At time: 194.78230834007263 and batch: 250, loss is 4.136949167251587 and perplexity is 62.61151251899165
At time: 195.71375036239624 and batch: 300, loss is 4.099320020675659 and perplexity is 60.299271396035806
At time: 196.64346146583557 and batch: 350, loss is 4.183199167251587 and perplexity is 65.5753043669803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546460381869612 and perplexity of 94.29803784772196
Finished 27 epochs...
Completing Train Step...
At time: 198.42217636108398 and batch: 50, loss is 4.121467461585999 and perplexity is 61.64964440232409
At time: 199.3422257900238 and batch: 100, loss is 4.075815286636352 and perplexity is 58.89848016276103
At time: 200.26827716827393 and batch: 150, loss is 4.013411302566528 and perplexity is 55.335314461386055
At time: 201.1841335296631 and batch: 200, loss is 4.093634376525879 and perplexity is 59.95740398555709
At time: 202.11105370521545 and batch: 250, loss is 4.1230863571167 and perplexity is 61.74952956612383
At time: 203.0374093055725 and batch: 300, loss is 4.084759273529053 and perplexity is 59.42763022551193
At time: 203.96347165107727 and batch: 350, loss is 4.169483432769775 and perplexity is 64.68203085845936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.541557575094289 and perplexity of 93.83684428415508
Finished 28 epochs...
Completing Train Step...
At time: 205.7314441204071 and batch: 50, loss is 4.10718138217926 and perplexity is 60.77517393690554
At time: 206.66571927070618 and batch: 100, loss is 4.061899290084839 and perplexity is 58.08452574484653
At time: 207.58147954940796 and batch: 150, loss is 3.9987953758239745 and perplexity is 54.53241937996359
At time: 208.50656580924988 and batch: 200, loss is 4.079976167678833 and perplexity is 59.14406029284579
At time: 209.4333164691925 and batch: 250, loss is 4.109692444801331 and perplexity is 60.927975971977475
At time: 210.3599977493286 and batch: 300, loss is 4.070668792724609 and perplexity is 58.59613816115667
At time: 211.28596138954163 and batch: 350, loss is 4.156260600090027 and perplexity is 63.83238095587769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5369415283203125 and perplexity of 93.4046872177957
Finished 29 epochs...
Completing Train Step...
At time: 213.06577801704407 and batch: 50, loss is 4.093474955558777 and perplexity is 59.94784628009835
At time: 214.0210907459259 and batch: 100, loss is 4.048591375350952 and perplexity is 57.31666249061957
At time: 214.9386384487152 and batch: 150, loss is 3.9845924472808836 and perplexity is 53.76337360315508
At time: 215.8562877178192 and batch: 200, loss is 4.0668064308166505 and perplexity is 58.37025517090525
At time: 216.77375745773315 and batch: 250, loss is 4.096694831848144 and perplexity is 60.14118202048182
At time: 217.70327138900757 and batch: 300, loss is 4.057116904258728 and perplexity is 57.80740630641654
At time: 218.62741899490356 and batch: 350, loss is 4.143530049324036 and perplexity is 63.024910268223415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.532725367052802 and perplexity of 93.01170700944576
Finished 30 epochs...
Completing Train Step...
At time: 220.42325687408447 and batch: 50, loss is 4.080233707427978 and perplexity is 59.15929420088128
At time: 221.34242343902588 and batch: 100, loss is 4.0358141040802 and perplexity is 56.58897080282824
At time: 222.26443195343018 and batch: 150, loss is 3.9709230422973634 and perplexity is 53.03346038129136
At time: 223.18745493888855 and batch: 200, loss is 4.054092168807983 and perplexity is 57.632818369516016
At time: 224.11441493034363 and batch: 250, loss is 4.084068040847779 and perplexity is 59.386566099397434
At time: 225.044020652771 and batch: 300, loss is 4.0440524435043335 and perplexity is 57.05709558976615
At time: 225.975421667099 and batch: 350, loss is 4.131217622756958 and perplexity is 62.25367830007534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.529080621127425 and perplexity of 92.6733200112264
Finished 31 epochs...
Completing Train Step...
At time: 227.77080845832825 and batch: 50, loss is 4.0674438524246215 and perplexity is 58.40747349343529
At time: 228.68626832962036 and batch: 100, loss is 4.023484106063843 and perplexity is 55.89551285806897
At time: 229.60256338119507 and batch: 150, loss is 3.9576420402526855 and perplexity is 52.333779402194544
At time: 230.53540539741516 and batch: 200, loss is 4.0417616128921505 and perplexity is 56.926537049393914
At time: 231.46052765846252 and batch: 250, loss is 4.071776294708252 and perplexity is 58.66106944953248
At time: 232.38742542266846 and batch: 300, loss is 4.031432456970215 and perplexity is 56.34156033097505
At time: 233.31346201896667 and batch: 350, loss is 4.119273080825805 and perplexity is 61.51450993124271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.525848388671875 and perplexity of 92.37426187159821
Finished 32 epochs...
Completing Train Step...
At time: 235.09542417526245 and batch: 50, loss is 4.055076227188111 and perplexity is 57.68956034153048
At time: 236.02826738357544 and batch: 100, loss is 4.011551947593689 and perplexity is 55.23252206266202
At time: 236.94762635231018 and batch: 150, loss is 3.94474769115448 and perplexity is 51.66330135961041
At time: 237.86663842201233 and batch: 200, loss is 4.029769172668457 and perplexity is 56.247926189825215
At time: 238.788578748703 and batch: 250, loss is 4.059786195755005 and perplexity is 57.96191725012377
At time: 239.7217206954956 and batch: 300, loss is 4.019261322021484 and perplexity is 55.65997583932059
At time: 240.65507435798645 and batch: 350, loss is 4.10766966342926 and perplexity is 60.80485656094906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.523064975080819 and perplexity of 92.11750359364851
Finished 33 epochs...
Completing Train Step...
At time: 242.45391488075256 and batch: 50, loss is 4.043097324371338 and perplexity is 57.002625283052176
At time: 243.36996388435364 and batch: 100, loss is 4.000037269592285 and perplexity is 54.600184921855
At time: 244.28803968429565 and batch: 150, loss is 3.932246136665344 and perplexity is 51.021450210624906
At time: 245.20670676231384 and batch: 200, loss is 4.018130383491516 and perplexity is 55.5970634098176
At time: 246.135436296463 and batch: 250, loss is 4.048101325035095 and perplexity is 57.288581323216604
At time: 247.0663092136383 and batch: 300, loss is 4.007503228187561 and perplexity is 55.009353157726714
At time: 247.99266505241394 and batch: 350, loss is 4.096354327201843 and perplexity is 60.1207071546611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.520430992389548 and perplexity of 91.87518695263546
Finished 34 epochs...
Completing Train Step...
At time: 249.78475689888 and batch: 50, loss is 4.031432013511658 and perplexity is 56.34153534583354
At time: 250.7002592086792 and batch: 100, loss is 3.9889046335220337 and perplexity is 53.99571186598814
At time: 251.6164689064026 and batch: 150, loss is 3.920176091194153 and perplexity is 50.40932063402939
At time: 252.5463047027588 and batch: 200, loss is 4.006918663978577 and perplexity is 54.977206055649404
At time: 253.4855546951294 and batch: 250, loss is 4.036687002182007 and perplexity is 56.63838877331374
At time: 254.40140056610107 and batch: 300, loss is 3.9960906219482424 and perplexity is 54.385121898807704
At time: 255.32354736328125 and batch: 350, loss is 4.085313458442688 and perplexity is 59.46057324905618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5184610301050645 and perplexity of 91.69437445483356
Finished 35 epochs...
Completing Train Step...
At time: 257.0825123786926 and batch: 50, loss is 4.020200977325439 and perplexity is 55.7123016110804
At time: 258.0218644142151 and batch: 100, loss is 3.9780741834640505 and perplexity is 53.41406941551237
At time: 258.9435079097748 and batch: 150, loss is 3.9084257555007933 and perplexity is 49.8204606212473
At time: 259.8688554763794 and batch: 200, loss is 3.9962219285964964 and perplexity is 54.39226349573844
At time: 260.79474115371704 and batch: 250, loss is 4.02552218914032 and perplexity is 56.00954872468759
At time: 261.7216293811798 and batch: 300, loss is 3.98497848033905 and perplexity is 53.784132049149875
At time: 262.6479067802429 and batch: 350, loss is 4.074537954330444 and perplexity is 58.82329525955564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.516160109947467 and perplexity of 91.48363555997284
Finished 36 epochs...
Completing Train Step...
At time: 264.4287815093994 and batch: 50, loss is 4.009231033325196 and perplexity is 55.104480758043415
At time: 265.3530764579773 and batch: 100, loss is 3.967567458152771 and perplexity is 52.85580038573859
At time: 266.26426696777344 and batch: 150, loss is 3.8970285558700564 and perplexity is 49.25587037124165
At time: 267.175856590271 and batch: 200, loss is 3.9859994983673097 and perplexity is 53.83907466153219
At time: 268.09285855293274 and batch: 250, loss is 4.014651741981506 and perplexity is 55.40399715603818
At time: 269.0258593559265 and batch: 300, loss is 3.9741781997680663 and perplexity is 53.20637392377019
At time: 269.97001028060913 and batch: 350, loss is 4.063964905738831 and perplexity is 58.20463005244395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.515210381869612 and perplexity of 91.39679222792027
Finished 37 epochs...
Completing Train Step...
At time: 271.7538661956787 and batch: 50, loss is 3.9988843250274657 and perplexity is 54.53727021096747
At time: 272.671763420105 and batch: 100, loss is 3.957353720664978 and perplexity is 52.31869272349112
At time: 273.59302616119385 and batch: 150, loss is 3.886119747161865 and perplexity is 48.72146765170358
At time: 274.52079939842224 and batch: 200, loss is 3.9760160207748414 and perplexity is 53.3042476250857
At time: 275.44811940193176 and batch: 250, loss is 4.004003925323486 and perplexity is 54.817195176241555
At time: 276.37546825408936 and batch: 300, loss is 3.9637176275253294 and perplexity is 52.65270569749328
At time: 277.326682806015 and batch: 350, loss is 4.053610043525696 and perplexity is 57.60503882783837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.511977097083783 and perplexity of 91.10175759266237
Finished 38 epochs...
Completing Train Step...
At time: 279.0947253704071 and batch: 50, loss is 3.9883854150772096 and perplexity is 53.96768357347881
At time: 280.00560903549194 and batch: 100, loss is 3.9474941730499267 and perplexity is 51.80538871230881
At time: 280.91853952407837 and batch: 150, loss is 3.8753573513031006 and perplexity is 48.199919517950704
At time: 281.8328425884247 and batch: 200, loss is 3.9664740228652953 and perplexity is 52.798037574152396
At time: 282.7590346336365 and batch: 250, loss is 3.99371666431427 and perplexity is 54.2561670507368
At time: 283.6893961429596 and batch: 300, loss is 3.953616142272949 and perplexity is 52.12351248600172
At time: 284.6137914657593 and batch: 350, loss is 4.043621015548706 and perplexity is 57.03248487291919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.510940025592673 and perplexity of 91.0073275308989
Finished 39 epochs...
Completing Train Step...
At time: 286.3837420940399 and batch: 50, loss is 3.978240661621094 and perplexity is 53.42296243157478
At time: 287.3165273666382 and batch: 100, loss is 3.937768588066101 and perplexity is 51.303993136527694
At time: 288.23730993270874 and batch: 150, loss is 3.865023126602173 and perplexity is 47.704375660049514
At time: 289.1653411388397 and batch: 200, loss is 3.957165551185608 and perplexity is 52.30884886850561
At time: 290.0928039550781 and batch: 250, loss is 3.9837690496444704 and perplexity is 53.71912318874806
At time: 291.0302493572235 and batch: 300, loss is 3.9437639904022217 and perplexity is 51.612505119444805
At time: 291.96038007736206 and batch: 350, loss is 4.0338420486450195 and perplexity is 56.477484180416084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.509297338025323 and perplexity of 90.85795364632725
Finished 40 epochs...
Completing Train Step...
At time: 293.7570023536682 and batch: 50, loss is 3.968487229347229 and perplexity is 52.90443799269946
At time: 294.670264005661 and batch: 100, loss is 3.9283778619766236 and perplexity is 50.8244664654424
At time: 295.5844237804413 and batch: 150, loss is 3.8549483728408815 and perplexity is 47.226178724892584
At time: 296.5011315345764 and batch: 200, loss is 3.948100543022156 and perplexity is 51.83681147036928
At time: 297.4176435470581 and batch: 250, loss is 3.973974165916443 and perplexity is 53.19551912977798
At time: 298.33423376083374 and batch: 300, loss is 3.9341925525665284 and perplexity is 51.120855883624166
At time: 299.2569305896759 and batch: 350, loss is 4.024381222724915 and perplexity is 55.94568015352678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.50775093867861 and perplexity of 90.71755954681437
Finished 41 epochs...
Completing Train Step...
At time: 301.02456879615784 and batch: 50, loss is 3.9589270973205566 and perplexity is 52.401074525079615
At time: 301.935284614563 and batch: 100, loss is 3.9192516374588013 and perplexity is 50.36274108291004
At time: 302.8478915691376 and batch: 150, loss is 3.8452212762832643 and perplexity is 46.76903208360737
At time: 303.7708435058594 and batch: 200, loss is 3.9392360162734987 and perplexity is 51.37933332784987
At time: 304.69642758369446 and batch: 250, loss is 3.964389276504517 and perplexity is 52.6880817123287
At time: 305.62091064453125 and batch: 300, loss is 3.9248743629455567 and perplexity is 50.646714555067135
At time: 306.5430724620819 and batch: 350, loss is 4.015192170143127 and perplexity is 55.43394712854302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.506560226966595 and perplexity of 90.60960537008259
Finished 42 epochs...
Completing Train Step...
At time: 308.2958378791809 and batch: 50, loss is 3.949511013031006 and perplexity is 51.909977325294825
At time: 309.2211244106293 and batch: 100, loss is 3.91033745765686 and perplexity is 49.9157935983284
At time: 310.1346275806427 and batch: 150, loss is 3.8357519817352297 and perplexity is 46.32825257222282
At time: 311.05809688568115 and batch: 200, loss is 3.930594277381897 and perplexity is 50.93723952568878
At time: 311.9827718734741 and batch: 250, loss is 3.955139646530151 and perplexity is 52.202983400941655
At time: 312.9071433544159 and batch: 300, loss is 3.9157840728759767 and perplexity is 50.188407456872916
At time: 313.83054637908936 and batch: 350, loss is 4.0062058162689205 and perplexity is 54.938029645295494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.505334788355334 and perplexity of 90.49863686754159
Finished 43 epochs...
Completing Train Step...
At time: 315.58324551582336 and batch: 50, loss is 3.940626301765442 and perplexity is 51.450814947977136
At time: 316.50944542884827 and batch: 100, loss is 3.9019053745269776 and perplexity is 49.4966690072214
At time: 317.42299461364746 and batch: 150, loss is 3.8262060022354127 and perplexity is 45.88810816999442
At time: 318.338063955307 and batch: 200, loss is 3.922165265083313 and perplexity is 50.509693334686254
At time: 319.25374007225037 and batch: 250, loss is 3.9461764001846316 and perplexity is 51.73716593772078
At time: 320.170538187027 and batch: 300, loss is 3.9068604278564454 and perplexity is 49.74253628145552
At time: 321.0878806114197 and batch: 350, loss is 3.9974156522750857 and perplexity is 54.45723159787147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.504074622844827 and perplexity of 90.38466543313102
Finished 44 epochs...
Completing Train Step...
At time: 322.87420868873596 and batch: 50, loss is 3.9315904855728148 and perplexity is 50.98800890516409
At time: 323.7923262119293 and batch: 100, loss is 3.8933632946014405 and perplexity is 49.07566518859791
At time: 324.725305557251 and batch: 150, loss is 3.8169454956054687 and perplexity is 45.46512259324577
At time: 325.6444003582001 and batch: 200, loss is 3.9139272356033326 and perplexity is 50.095302218641685
At time: 326.56368112564087 and batch: 250, loss is 3.9375227069854737 and perplexity is 51.29138000598342
At time: 327.48875522613525 and batch: 300, loss is 3.89811484336853 and perplexity is 49.309405479447385
At time: 328.41673588752747 and batch: 350, loss is 3.98889799118042 and perplexity is 53.995353209215416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.502409836341595 and perplexity of 90.2343194437296
Finished 45 epochs...
Completing Train Step...
At time: 330.1873118877411 and batch: 50, loss is 3.9230946636199953 and perplexity is 50.55665879119239
At time: 331.098929643631 and batch: 100, loss is 3.8856102275848388 and perplexity is 48.69664943333456
At time: 332.0135190486908 and batch: 150, loss is 3.808619108200073 and perplexity is 45.08813402354892
At time: 332.9399137496948 and batch: 200, loss is 3.9056701374053957 and perplexity is 49.68336343892972
At time: 333.8641571998596 and batch: 250, loss is 3.928928418159485 and perplexity is 50.852455893865006
At time: 334.78846883773804 and batch: 300, loss is 3.8894079065322877 and perplexity is 48.88193527908838
At time: 335.7135441303253 and batch: 350, loss is 3.9805400657653807 and perplexity is 53.54594475163161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.501748446760507 and perplexity of 90.17465913652052
Finished 46 epochs...
Completing Train Step...
At time: 337.4844591617584 and batch: 50, loss is 3.9146973657608033 and perplexity is 50.133896981215614
At time: 338.4200940132141 and batch: 100, loss is 3.8776791715621948 and perplexity is 48.31196108743387
At time: 339.3501317501068 and batch: 150, loss is 3.799248194694519 and perplexity is 44.667590535277725
At time: 340.2810730934143 and batch: 200, loss is 3.8978046703338625 and perplexity is 49.29411340322961
At time: 341.2114839553833 and batch: 250, loss is 3.9208025455474855 and perplexity is 50.44090966589931
At time: 342.1475257873535 and batch: 300, loss is 3.8811120128631593 and perplexity is 48.47809337252285
At time: 343.0852863788605 and batch: 350, loss is 3.972327151298523 and perplexity is 53.107977443171364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.500054195009429 and perplexity of 90.02200991199838
Finished 47 epochs...
Completing Train Step...
At time: 344.8716597557068 and batch: 50, loss is 3.906134386062622 and perplexity is 49.706434228569925
At time: 345.79944586753845 and batch: 100, loss is 3.8696017360687254 and perplexity is 47.92329615932994
At time: 346.71872663497925 and batch: 150, loss is 3.790673313140869 and perplexity is 44.28620872594115
At time: 347.6374628543854 and batch: 200, loss is 3.8901032495498655 and perplexity is 48.91593681146446
At time: 348.5694987773895 and batch: 250, loss is 3.912956485748291 and perplexity is 50.04669580742503
At time: 349.48816084861755 and batch: 300, loss is 3.8730056428909303 and perplexity is 48.08670054294248
At time: 350.40801763534546 and batch: 350, loss is 3.964406213760376 and perplexity is 52.68897411140677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.498866640288254 and perplexity of 89.91516730239705
Finished 48 epochs...
Completing Train Step...
At time: 352.1868031024933 and batch: 50, loss is 3.89781934261322 and perplexity is 49.29483666553809
At time: 353.1023654937744 and batch: 100, loss is 3.8617655181884767 and perplexity is 47.549226329615166
At time: 354.01748275756836 and batch: 150, loss is 3.7823011541366576 and perplexity is 43.9169853000723
At time: 354.9352185726166 and batch: 200, loss is 3.8825565719604493 and perplexity is 48.548173448542435
At time: 355.854688167572 and batch: 250, loss is 3.9053170776367185 and perplexity is 49.66582533830792
At time: 356.77911376953125 and batch: 300, loss is 3.8650599622726443 and perplexity is 47.70613291507599
At time: 357.7050681114197 and batch: 350, loss is 3.9566915321350096 and perplexity is 52.28405935344243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.498205776872306 and perplexity of 89.85576528825894
Finished 49 epochs...
Completing Train Step...
At time: 359.47338485717773 and batch: 50, loss is 3.8897204208374023 and perplexity is 48.89721397040523
At time: 360.4023892879486 and batch: 100, loss is 3.8541015625 and perplexity is 47.18620403626665
At time: 361.31969022750854 and batch: 150, loss is 3.773974533081055 and perplexity is 43.552823428549885
At time: 362.24305868148804 and batch: 200, loss is 3.875060410499573 and perplexity is 48.18560911989478
At time: 363.1692645549774 and batch: 250, loss is 3.8978090143203734 and perplexity is 49.2943275366584
At time: 364.09555673599243 and batch: 300, loss is 3.857297077178955 and perplexity is 47.337229417305785
At time: 365.02160358428955 and batch: 350, loss is 3.9492230796813965 and perplexity is 51.89503286325317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4981689453125 and perplexity of 89.85245582121264
Finished Training.
Improved accuracyfrom -137.91620153896054 to -89.85245582121264
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73c491f240>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.1425630954123083, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.701291397671644, 'data': 'ptb', 'lr': 2.288409930916093}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 373 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.1526546478271484 and batch: 50, loss is 6.744803066253662 and perplexity is 849.631789029485
At time: 2.088010549545288 and batch: 100, loss is 5.997460155487061 and perplexity is 402.4054472055647
At time: 3.0094847679138184 and batch: 150, loss is 5.800556535720825 and perplexity is 330.483434574975
At time: 3.930771589279175 and batch: 200, loss is 5.622980804443359 and perplexity is 276.71298241843743
At time: 4.850343942642212 and batch: 250, loss is 5.486258249282837 and perplexity is 241.3524345522874
At time: 5.782491683959961 and batch: 300, loss is 5.382790937423706 and perplexity is 217.6288170313301
At time: 6.704874753952026 and batch: 350, loss is 5.331772403717041 and perplexity is 206.8041900343634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 5.333266159583783 and perplexity of 207.1133358431935
Finished 1 epochs...
Completing Train Step...
At time: 8.463932037353516 and batch: 50, loss is 5.13324390411377 and perplexity is 169.56628360805686
At time: 9.396494150161743 and batch: 100, loss is 5.015541248321533 and perplexity is 150.73770121839033
At time: 10.311593532562256 and batch: 150, loss is 4.947894058227539 and perplexity is 140.87797050291567
At time: 11.228572845458984 and batch: 200, loss is 4.906530170440674 and perplexity is 135.1695843454463
At time: 12.145739555358887 and batch: 250, loss is 4.869840250015259 and perplexity is 130.3000997947833
At time: 13.062503337860107 and batch: 300, loss is 4.825781307220459 and perplexity is 124.68384672676243
At time: 13.979368448257446 and batch: 350, loss is 4.850053043365478 and perplexity is 127.74716580592207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.95037789180361 and perplexity of 141.22832286453635
Finished 2 epochs...
Completing Train Step...
At time: 15.747515678405762 and batch: 50, loss is 4.76103175163269 and perplexity is 116.86644085870857
At time: 16.664530754089355 and batch: 100, loss is 4.680893316268921 and perplexity is 107.86638834434963
At time: 17.58394193649292 and batch: 150, loss is 4.631795511245728 and perplexity is 102.69829460992145
At time: 18.496971368789673 and batch: 200, loss is 4.642959184646607 and perplexity is 103.85120823162627
At time: 19.41040062904358 and batch: 250, loss is 4.635377616882324 and perplexity is 103.06683042294742
At time: 20.323500394821167 and batch: 300, loss is 4.6005518341064455 and perplexity is 99.53922963067383
At time: 21.236595630645752 and batch: 350, loss is 4.650443277359009 and perplexity is 104.631356010035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.791650706324084 and perplexity of 120.5001148839564
Finished 3 epochs...
Completing Train Step...
At time: 22.997796773910522 and batch: 50, loss is 4.57484920501709 and perplexity is 97.01340889854089
At time: 23.913490533828735 and batch: 100, loss is 4.504280462265014 and perplexity is 90.40327207518207
At time: 24.830036640167236 and batch: 150, loss is 4.454214172363281 and perplexity is 85.98855210745143
At time: 25.747323513031006 and batch: 200, loss is 4.489596900939941 and perplexity is 89.08552834724179
At time: 26.6665358543396 and batch: 250, loss is 4.492079381942749 and perplexity is 89.30695621050462
At time: 27.58430004119873 and batch: 300, loss is 4.458993625640869 and perplexity is 86.40051406691555
At time: 28.500946044921875 and batch: 350, loss is 4.522693548202515 and perplexity is 92.08329503023475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.702540956694504 and perplexity of 110.22689869163452
Finished 4 epochs...
Completing Train Step...
At time: 30.275614738464355 and batch: 50, loss is 4.450281963348389 and perplexity is 85.65109106617727
At time: 31.20120668411255 and batch: 100, loss is 4.384185914993286 and perplexity is 80.17292905951977
At time: 32.11351156234741 and batch: 150, loss is 4.331229209899902 and perplexity is 76.03769543586962
At time: 33.026402950286865 and batch: 200, loss is 4.380500078201294 and perplexity is 79.877968649765
At time: 33.93991827964783 and batch: 250, loss is 4.388684949874878 and perplexity is 80.53444248488132
At time: 34.853028297424316 and batch: 300, loss is 4.354676179885864 and perplexity is 77.84161457465632
At time: 35.76563882827759 and batch: 350, loss is 4.426937761306763 and perplexity is 83.67479200172778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.64160261483028 and perplexity of 103.71042233169186
Finished 5 epochs...
Completing Train Step...
At time: 37.52063751220703 and batch: 50, loss is 4.3552721118927 and perplexity is 77.88801670913045
At time: 38.43366885185242 and batch: 100, loss is 4.2922086334228515 and perplexity is 73.12780277926645
At time: 39.347243785858154 and batch: 150, loss is 4.235584325790406 and perplexity is 69.10204504592103
At time: 40.26056933403015 and batch: 200, loss is 4.2948604011535645 and perplexity is 73.32197806699087
At time: 41.17398023605347 and batch: 250, loss is 4.307182865142822 and perplexity is 74.23107515609848
At time: 42.097200870513916 and batch: 300, loss is 4.273364820480347 and perplexity is 71.7626984226246
At time: 43.01654291152954 and batch: 350, loss is 4.350731143951416 and perplexity is 77.53513154872985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5976978170460665 and perplexity of 99.2555479798252
Finished 6 epochs...
Completing Train Step...
At time: 44.77704048156738 and batch: 50, loss is 4.277756910324097 and perplexity is 72.07857982348187
At time: 45.697136878967285 and batch: 100, loss is 4.217298135757447 and perplexity is 67.84991515203822
At time: 46.623093128204346 and batch: 150, loss is 4.158135347366333 and perplexity is 63.952162783459464
At time: 47.54002928733826 and batch: 200, loss is 4.224051475524902 and perplexity is 68.30967940697228
At time: 48.45547676086426 and batch: 250, loss is 4.240553398132324 and perplexity is 69.44627264435186
At time: 49.36984610557556 and batch: 300, loss is 4.205894317626953 and perplexity is 67.08056217782978
At time: 50.28918433189392 and batch: 350, loss is 4.285932502746582 and perplexity is 72.67028036997863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.567431219692888 and perplexity of 96.29642742186236
Finished 7 epochs...
Completing Train Step...
At time: 52.022112131118774 and batch: 50, loss is 4.212327480316162 and perplexity is 67.51349341304649
At time: 52.949434995651245 and batch: 100, loss is 4.1542991065979 and perplexity is 63.7072968719587
At time: 53.86303377151489 and batch: 150, loss is 4.0932880306243895 and perplexity is 59.936641580117325
At time: 54.77658939361572 and batch: 200, loss is 4.163337874412536 and perplexity is 64.28574261665881
At time: 55.68924570083618 and batch: 250, loss is 4.1837780570983885 and perplexity is 65.61327623458317
At time: 56.603150367736816 and batch: 300, loss is 4.148158597946167 and perplexity is 63.31730027849458
At time: 57.51667642593384 and batch: 350, loss is 4.230323057174683 and perplexity is 68.73943535497996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.546945506128772 and perplexity of 94.34379521157928
Finished 8 epochs...
Completing Train Step...
At time: 59.262057065963745 and batch: 50, loss is 4.1557123613357545 and perplexity is 63.79739516202051
At time: 60.19172406196594 and batch: 100, loss is 4.100391421318054 and perplexity is 60.363910695255555
At time: 61.107763051986694 and batch: 150, loss is 4.038157124519348 and perplexity is 56.721715368937595
At time: 62.02351760864258 and batch: 200, loss is 4.110410895347595 and perplexity is 60.97176543799664
At time: 62.93990612030029 and batch: 250, loss is 4.134207615852356 and perplexity is 62.44009492168012
At time: 63.85591411590576 and batch: 300, loss is 4.097507824897766 and perplexity is 60.19009626423643
At time: 64.7718734741211 and batch: 350, loss is 4.181156625747681 and perplexity is 65.4415007823857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5322765481883085 and perplexity of 92.96997096738332
Finished 9 epochs...
Completing Train Step...
At time: 66.55943393707275 and batch: 50, loss is 4.107190389633178 and perplexity is 60.775721368949604
At time: 67.47209143638611 and batch: 100, loss is 4.05351215839386 and perplexity is 57.59940042698062
At time: 68.3859293460846 and batch: 150, loss is 3.9902651929855346 and perplexity is 54.069226241755366
At time: 69.310706615448 and batch: 200, loss is 4.063762636184692 and perplexity is 58.19285821845637
At time: 70.22370147705078 and batch: 250, loss is 4.090679574012756 and perplexity is 59.780503179761
At time: 71.13687515258789 and batch: 300, loss is 4.052689433097839 and perplexity is 57.55203143172095
At time: 72.0584032535553 and batch: 350, loss is 4.1370882892608645 and perplexity is 62.62022376436611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.52050676016972 and perplexity of 91.882148395327
Finished 10 epochs...
Completing Train Step...
At time: 73.81887459754944 and batch: 50, loss is 4.063378958702088 and perplexity is 58.170535211800896
At time: 74.7348210811615 and batch: 100, loss is 4.012151856422424 and perplexity is 55.26566648110131
At time: 75.65120005607605 and batch: 150, loss is 3.948374309539795 and perplexity is 51.85100459644374
At time: 76.5921688079834 and batch: 200, loss is 4.02246693611145 and perplexity is 55.83868652783352
At time: 77.51157331466675 and batch: 250, loss is 4.052357664108277 and perplexity is 57.53294061945042
At time: 78.43885946273804 and batch: 300, loss is 4.012402963638306 and perplexity is 55.2795458312748
At time: 79.36077237129211 and batch: 350, loss is 4.09785150051117 and perplexity is 60.21078568751233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.5114288330078125 and perplexity of 91.05182346151253
Finished 11 epochs...
Completing Train Step...
At time: 81.11685109138489 and batch: 50, loss is 4.023961362838745 and perplexity is 55.92219573705301
At time: 82.05534076690674 and batch: 100, loss is 3.975405044555664 and perplexity is 53.27168994440274
At time: 82.96903562545776 and batch: 150, loss is 3.910927481651306 and perplexity is 49.945253804512824
At time: 83.89241743087769 and batch: 200, loss is 3.9854960680007934 and perplexity is 53.81197725784267
At time: 84.80571913719177 and batch: 250, loss is 4.017913103103638 and perplexity is 55.584984570609976
At time: 85.71890926361084 and batch: 300, loss is 3.975991578102112 and perplexity is 53.30294474272894
At time: 86.63466358184814 and batch: 350, loss is 4.062331008911133 and perplexity is 58.10960734183615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.504711282664332 and perplexity of 90.44222803986675
Finished 12 epochs...
Completing Train Step...
At time: 88.4376072883606 and batch: 50, loss is 3.9881041860580444 and perplexity is 53.95250842870638
At time: 89.36264491081238 and batch: 100, loss is 3.9425062942504883 and perplexity is 51.547633073587185
At time: 90.2770082950592 and batch: 150, loss is 3.8772393608093263 and perplexity is 48.29071763934634
At time: 91.1932463645935 and batch: 200, loss is 3.9517965650558473 and perplexity is 52.02875596475429
At time: 92.120840549469 and batch: 250, loss is 3.985981559753418 and perplexity is 53.838108871822044
At time: 93.03891634941101 and batch: 300, loss is 3.942592239379883 and perplexity is 51.552063531967114
At time: 93.95676255226135 and batch: 350, loss is 4.0299687623977665 and perplexity is 56.25915381861007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.497955848430765 and perplexity of 89.83331058302862
Finished 13 epochs...
Completing Train Step...
At time: 95.72134113311768 and batch: 50, loss is 3.9561820650100707 and perplexity is 52.25742912823164
At time: 96.63418793678284 and batch: 100, loss is 3.9127742576599123 and perplexity is 50.03757672462013
At time: 97.55160188674927 and batch: 150, loss is 3.846044883728027 and perplexity is 46.807567273372086
At time: 98.4709324836731 and batch: 200, loss is 3.921075143814087 and perplexity is 50.45466164473767
At time: 99.40830326080322 and batch: 250, loss is 3.9560290813446044 and perplexity is 52.24943520666106
At time: 100.33584332466125 and batch: 300, loss is 3.91147500038147 and perplexity is 49.97260725403323
At time: 101.26224517822266 and batch: 350, loss is 4.000447626113892 and perplexity is 54.62259506157763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.495214001885776 and perplexity of 89.58733879332812
Finished 14 epochs...
Completing Train Step...
At time: 103.01606559753418 and batch: 50, loss is 3.926285676956177 and perplexity is 50.71824343589378
At time: 103.94303178787231 and batch: 100, loss is 3.8854009771347044 and perplexity is 48.68646070355592
At time: 104.86610269546509 and batch: 150, loss is 3.8176942825317384 and perplexity is 45.499179031561496
At time: 105.78368496894836 and batch: 200, loss is 3.8934909820556642 and perplexity is 49.081931935434135
At time: 106.70153021812439 and batch: 250, loss is 3.9271299982070924 and perplexity is 50.76108400969423
At time: 107.62229418754578 and batch: 300, loss is 3.8827894926071167 and perplexity is 48.55948263751727
At time: 108.54758620262146 and batch: 350, loss is 3.973429284095764 and perplexity is 53.16654175380222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.49584908320986 and perplexity of 89.64425210944935
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 110.30659055709839 and batch: 50, loss is 3.9000649785995485 and perplexity is 49.40565931178354
At time: 111.23687744140625 and batch: 100, loss is 3.847544617652893 and perplexity is 46.877818836110855
At time: 112.15556192398071 and batch: 150, loss is 3.767254419326782 and perplexity is 43.261124722789525
At time: 113.07634806632996 and batch: 200, loss is 3.834742193222046 and perplexity is 46.28149444682164
At time: 113.99664068222046 and batch: 250, loss is 3.853989782333374 and perplexity is 47.18092984929728
At time: 114.92034125328064 and batch: 300, loss is 3.7904185676574706 and perplexity is 44.2749284511519
At time: 115.84935808181763 and batch: 350, loss is 3.867997074127197 and perplexity is 47.84645713668609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.450620980098329 and perplexity of 85.68013314329653
Finished 16 epochs...
Completing Train Step...
At time: 117.61747574806213 and batch: 50, loss is 3.865959496498108 and perplexity is 47.749065521176966
At time: 118.53231382369995 and batch: 100, loss is 3.8198276138305665 and perplexity is 45.59634746363726
At time: 119.44750881195068 and batch: 150, loss is 3.743723363876343 and perplexity is 42.255028474804654
At time: 120.36373257637024 and batch: 200, loss is 3.8171283769607545 and perplexity is 45.47343807683414
At time: 121.29055643081665 and batch: 250, loss is 3.8404329776763917 and perplexity is 46.54562329329228
At time: 122.21394515037537 and batch: 300, loss is 3.7824522972106935 and perplexity is 43.92362354988305
At time: 123.15189528465271 and batch: 350, loss is 3.8668506240844724 and perplexity is 47.7916349952853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.447884921369882 and perplexity of 85.44602767635148
Finished 17 epochs...
Completing Train Step...
At time: 124.92090845108032 and batch: 50, loss is 3.8511641788482667 and perplexity is 47.04780341931059
At time: 125.8382318019867 and batch: 100, loss is 3.8063563394546507 and perplexity is 44.98622534446005
At time: 126.75551080703735 and batch: 150, loss is 3.7313174867630003 and perplexity is 41.73405602592133
At time: 127.67392015457153 and batch: 200, loss is 3.807198209762573 and perplexity is 45.02411385821266
At time: 128.59575819969177 and batch: 250, loss is 3.8320161151885985 and perplexity is 46.155499295789575
At time: 129.5173375606537 and batch: 300, loss is 3.776485662460327 and perplexity is 43.66232763508431
At time: 130.43879866600037 and batch: 350, loss is 3.8638134813308715 and perplexity is 47.64670517505883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.446886786099138 and perplexity of 85.36078353205349
Finished 18 epochs...
Completing Train Step...
At time: 132.1885735988617 and batch: 50, loss is 3.8396088743209837 and perplexity is 46.50728069026065
At time: 133.11548399925232 and batch: 100, loss is 3.795951256752014 and perplexity is 44.520566758552214
At time: 134.02993273735046 and batch: 150, loss is 3.721634211540222 and perplexity is 41.331883989013924
At time: 134.94884061813354 and batch: 200, loss is 3.7992718744277956 and perplexity is 44.66864826443102
At time: 135.86482620239258 and batch: 250, loss is 3.8248871469497683 and perplexity is 45.82762828685942
At time: 136.7978127002716 and batch: 300, loss is 3.7708147287368776 and perplexity is 43.415422222664745
At time: 137.717449426651 and batch: 350, loss is 3.859928135871887 and perplexity is 47.46194043532518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4463611471241915 and perplexity of 85.31592636766685
Finished 19 epochs...
Completing Train Step...
At time: 139.48317670822144 and batch: 50, loss is 3.8298100996017457 and perplexity is 46.05379177036396
At time: 140.39610290527344 and batch: 100, loss is 3.7870494508743286 and perplexity is 44.1260120458767
At time: 141.31028580665588 and batch: 150, loss is 3.7131678915023802 and perplexity is 40.98343216510052
At time: 142.2311360836029 and batch: 200, loss is 3.7922558975219727 and perplexity is 44.35635087646537
At time: 143.15077090263367 and batch: 250, loss is 3.818329944610596 and perplexity is 45.52811032858103
At time: 144.06843090057373 and batch: 300, loss is 3.7652314615249636 and perplexity is 43.17369775336626
At time: 144.99563121795654 and batch: 350, loss is 3.8556125354766846 and perplexity is 47.25755500655701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.44610648319639 and perplexity of 85.29420224504663
Finished 20 epochs...
Completing Train Step...
At time: 146.78067064285278 and batch: 50, loss is 3.8209711742401122 and perplexity is 45.64851946664955
At time: 147.69738674163818 and batch: 100, loss is 3.779039511680603 and perplexity is 43.77397714379311
At time: 148.62288093566895 and batch: 150, loss is 3.705472512245178 and perplexity is 40.66925950064166
At time: 149.54263472557068 and batch: 200, loss is 3.7857630681991576 and perplexity is 44.06928560223614
At time: 150.46348810195923 and batch: 250, loss is 3.812127766609192 and perplexity is 45.24661074189766
At time: 151.38532209396362 and batch: 300, loss is 3.7597507667541503 and perplexity is 42.937723136959605
At time: 152.3149380683899 and batch: 350, loss is 3.8510858964920045 and perplexity is 47.044120550555675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.446097538389009 and perplexity of 85.293439308249
Finished 21 epochs...
Completing Train Step...
At time: 154.07896041870117 and batch: 50, loss is 3.8128442001342773 and perplexity is 45.27903854552544
At time: 155.01337265968323 and batch: 100, loss is 3.7716441679000856 and perplexity is 43.45144761251267
At time: 155.9389250278473 and batch: 150, loss is 3.6982346105575563 and perplexity is 40.37596210793295
At time: 156.8539695739746 and batch: 200, loss is 3.7796341609954833 and perplexity is 43.80001505025562
At time: 157.7731294631958 and batch: 250, loss is 3.806162700653076 and perplexity is 44.97751510904398
At time: 158.69260668754578 and batch: 300, loss is 3.754335103034973 and perplexity is 42.70581540154669
At time: 159.6189968585968 and batch: 350, loss is 3.846430249214172 and perplexity is 46.8256087703515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.446171201508621 and perplexity of 85.29972252048856
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 161.3896038532257 and batch: 50, loss is 3.811645007133484 and perplexity is 45.22477278348315
At time: 162.33204102516174 and batch: 100, loss is 3.7692299699783325 and perplexity is 43.346673741308685
At time: 163.25089406967163 and batch: 150, loss is 3.6895720100402833 and perplexity is 40.02771183194297
At time: 164.16936492919922 and batch: 200, loss is 3.7684562969207764 and perplexity is 43.31315055736464
At time: 165.0935139656067 and batch: 250, loss is 3.7908010625839235 and perplexity is 44.29186662582996
At time: 166.01851630210876 and batch: 300, loss is 3.732382836341858 and perplexity is 41.77854107679023
At time: 166.94512152671814 and batch: 350, loss is 3.8190818786621095 and perplexity is 45.56235733917125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.43676389496902 and perplexity of 84.50104447765302
Finished 23 epochs...
Completing Train Step...
At time: 168.72858595848083 and batch: 50, loss is 3.8036587953567507 and perplexity is 44.86503654734581
At time: 169.64474511146545 and batch: 100, loss is 3.761963038444519 and perplexity is 43.03281819557333
At time: 170.57337522506714 and batch: 150, loss is 3.6833877038955687 and perplexity is 39.78093207500579
At time: 171.49977278709412 and batch: 200, loss is 3.764148302078247 and perplexity is 43.12695907208671
At time: 172.422691822052 and batch: 250, loss is 3.7879353857040403 and perplexity is 44.165122138783104
At time: 173.34346556663513 and batch: 300, loss is 3.7316156244277954 and perplexity is 41.746500374899725
At time: 174.26143956184387 and batch: 350, loss is 3.820426163673401 and perplexity is 45.62364731959264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.436324020911908 and perplexity of 84.46388283420906
Finished 24 epochs...
Completing Train Step...
At time: 176.0248703956604 and batch: 50, loss is 3.8003412818908693 and perplexity is 44.71644280161285
At time: 176.94320964813232 and batch: 100, loss is 3.758661193847656 and perplexity is 42.89096483507831
At time: 177.86565399169922 and batch: 150, loss is 3.6802866792678834 and perplexity is 39.65776150112473
At time: 178.79035592079163 and batch: 200, loss is 3.761895971298218 and perplexity is 43.02993220403835
At time: 179.71206879615784 and batch: 250, loss is 3.7862837982177733 and perplexity is 44.09223977809369
At time: 180.63392567634583 and batch: 300, loss is 3.7309188652038574 and perplexity is 41.71742324675135
At time: 181.55527710914612 and batch: 350, loss is 3.820663990974426 and perplexity is 45.63449915887827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.436175116177263 and perplexity of 84.45130669849488
Finished 25 epochs...
Completing Train Step...
At time: 183.3031632900238 and batch: 50, loss is 3.797706112861633 and perplexity is 44.59876253823591
At time: 184.2297065258026 and batch: 100, loss is 3.7561751413345337 and perplexity is 42.784468077275875
At time: 185.14346575737 and batch: 150, loss is 3.6778969764709473 and perplexity is 39.56310438378067
At time: 186.0579161643982 and batch: 200, loss is 3.7601133155822755 and perplexity is 42.953292980408975
At time: 186.97191047668457 and batch: 250, loss is 3.7848686265945433 and perplexity is 44.0298858227356
At time: 187.88543391227722 and batch: 300, loss is 3.7300991296768187 and perplexity is 41.68324000534235
At time: 188.79577326774597 and batch: 350, loss is 3.8204305839538573 and perplexity is 45.623848989354954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.436120395002694 and perplexity of 84.44668555023696
Finished 26 epochs...
Completing Train Step...
At time: 190.5462920665741 and batch: 50, loss is 3.7953926277160646 and perplexity is 44.49570322265709
At time: 191.45972347259521 and batch: 100, loss is 3.7540396213531495 and perplexity is 42.69319847951465
At time: 192.37358236312866 and batch: 150, loss is 3.675816464424133 and perplexity is 39.48087843419997
At time: 193.2871060371399 and batch: 200, loss is 3.758519859313965 and perplexity is 42.884903288926765
At time: 194.21319770812988 and batch: 250, loss is 3.7835338592529295 and perplexity is 43.971155373546736
At time: 195.12677121162415 and batch: 300, loss is 3.729194374084473 and perplexity is 41.64554391628556
At time: 196.04590892791748 and batch: 350, loss is 3.8199600172042847 and perplexity is 45.60238497355519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.436109871699892 and perplexity of 84.44579689687005
Finished 27 epochs...
Completing Train Step...
At time: 197.8242301940918 and batch: 50, loss is 3.7932735443115235 and perplexity is 44.40151295015314
At time: 198.7417459487915 and batch: 100, loss is 3.75210045337677 and perplexity is 42.61048941551616
At time: 199.65670037269592 and batch: 150, loss is 3.6739072799682617 and perplexity is 39.405574062630684
At time: 200.57202315330505 and batch: 200, loss is 3.7570270681381226 and perplexity is 42.82093284285835
At time: 201.48706245422363 and batch: 250, loss is 3.7822369813919066 and perplexity is 43.91416711701064
At time: 202.40392661094666 and batch: 300, loss is 3.7282353115081786 and perplexity is 41.60562238033158
At time: 203.326890707016 and batch: 350, loss is 3.819353656768799 and perplexity is 45.57474187323158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4361246043238145 and perplexity of 84.44704101420211
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 205.08811569213867 and batch: 50, loss is 3.7950287818908692 and perplexity is 44.47951659169802
At time: 206.01514172554016 and batch: 100, loss is 3.7535295820236207 and perplexity is 42.67142882134958
At time: 206.92877221107483 and batch: 150, loss is 3.6740042781829834 and perplexity is 39.409396518347535
At time: 207.85533094406128 and batch: 200, loss is 3.755337662696838 and perplexity is 42.74865199892961
At time: 208.77874422073364 and batch: 250, loss is 3.777981004714966 and perplexity is 43.72766659841421
At time: 209.6937279701233 and batch: 300, loss is 3.722351951599121 and perplexity is 41.361560186486486
At time: 210.6113715171814 and batch: 350, loss is 3.8119960403442383 and perplexity is 45.24065096740086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433833155138739 and perplexity of 84.25375644634474
Finished 29 epochs...
Completing Train Step...
At time: 212.36399507522583 and batch: 50, loss is 3.792361536026001 and perplexity is 44.361036862522035
At time: 213.29452228546143 and batch: 100, loss is 3.750920934677124 and perplexity is 42.560259176026506
At time: 214.21254754066467 and batch: 150, loss is 3.6718347454071045 and perplexity is 39.3239892213885
At time: 215.13056445121765 and batch: 200, loss is 3.753804907798767 and perplexity is 42.68317898305338
At time: 216.05564212799072 and batch: 250, loss is 3.77749710559845 and perplexity is 43.70651193795348
At time: 216.98836255073547 and batch: 300, loss is 3.722597894668579 and perplexity is 41.37173402659778
At time: 217.9099304676056 and batch: 350, loss is 3.8128437995910645 and perplexity is 45.2790204093175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433640578697467 and perplexity of 84.2375327199684
Finished 30 epochs...
Completing Train Step...
At time: 219.67176818847656 and batch: 50, loss is 3.7913242530822755 and perplexity is 44.31504577262882
At time: 220.58457779884338 and batch: 100, loss is 3.749870433807373 and perplexity is 42.51557306225542
At time: 221.4986116886139 and batch: 150, loss is 3.6709045696258547 and perplexity is 39.28742800580741
At time: 222.4132091999054 and batch: 200, loss is 3.753098282814026 and perplexity is 42.653028636104565
At time: 223.3393430709839 and batch: 250, loss is 3.7772182559967042 and perplexity is 43.69432609359378
At time: 224.25702834129333 and batch: 300, loss is 3.722698063850403 and perplexity is 41.37587840691198
At time: 225.17747807502747 and batch: 350, loss is 3.8132075691223144 and perplexity is 45.295494533557815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433586909853179 and perplexity of 84.233011910256
Finished 31 epochs...
Completing Train Step...
At time: 226.9465012550354 and batch: 50, loss is 3.7905679416656493 and perplexity is 44.28154246864532
At time: 227.86709237098694 and batch: 100, loss is 3.7491088056564332 and perplexity is 42.48320433299027
At time: 228.79063844680786 and batch: 150, loss is 3.6702378177642823 and perplexity is 39.261241770878705
At time: 229.71205973625183 and batch: 200, loss is 3.752591495513916 and perplexity is 42.63141809931587
At time: 230.63383150100708 and batch: 250, loss is 3.776957459449768 and perplexity is 43.682932250029964
At time: 231.55550527572632 and batch: 300, loss is 3.722694573402405 and perplexity is 41.37573398681207
At time: 232.48026180267334 and batch: 350, loss is 3.8133684730529787 and perplexity is 45.30278334305297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4335653370824355 and perplexity of 84.23119479040128
Finished 32 epochs...
Completing Train Step...
At time: 234.23374390602112 and batch: 50, loss is 3.7899273347854616 and perplexity is 44.25318449200185
At time: 235.157395362854 and batch: 100, loss is 3.7484715747833253 and perplexity is 42.45614134720154
At time: 236.07152199745178 and batch: 150, loss is 3.6696788263320923 and perplexity is 39.23930120597686
At time: 236.98576164245605 and batch: 200, loss is 3.752163887023926 and perplexity is 42.61319243999733
At time: 237.9002046585083 and batch: 250, loss is 3.77669930934906 and perplexity is 43.67165695209267
At time: 238.81300139427185 and batch: 300, loss is 3.7226308965682984 and perplexity is 41.373099394945086
At time: 239.72854900360107 and batch: 350, loss is 3.813427753448486 and perplexity is 45.30546898956944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433558496935614 and perplexity of 84.23061863863245
Finished 33 epochs...
Completing Train Step...
At time: 241.48662304878235 and batch: 50, loss is 3.789350595474243 and perplexity is 44.22766929937327
At time: 242.40063095092773 and batch: 100, loss is 3.7479038524627684 and perplexity is 42.432044888809855
At time: 243.3147416114807 and batch: 150, loss is 3.6691763257980345 and perplexity is 39.21958838943003
At time: 244.22794723510742 and batch: 200, loss is 3.7517760181427002 and perplexity is 42.59666731371847
At time: 245.14586329460144 and batch: 250, loss is 3.7764407920837404 and perplexity is 43.6603685339538
At time: 246.06289625167847 and batch: 300, loss is 3.7225295543670653 and perplexity is 41.368906766429255
At time: 246.97972917556763 and batch: 350, loss is 3.8134281826019287 and perplexity is 45.3054884325716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433555339944774 and perplexity of 84.23035272376069
Finished 34 epochs...
Completing Train Step...
At time: 248.74032711982727 and batch: 50, loss is 3.788815026283264 and perplexity is 44.20398866418307
At time: 249.65509939193726 and batch: 100, loss is 3.747381067276001 and perplexity is 42.409867841718686
At time: 250.56995105743408 and batch: 150, loss is 3.668708291053772 and perplexity is 39.20123655439131
At time: 251.48623132705688 and batch: 200, loss is 3.7514116859436033 and perplexity is 42.58115080299574
At time: 252.40409779548645 and batch: 250, loss is 3.776181335449219 and perplexity is 43.649042031103704
At time: 253.3226888179779 and batch: 300, loss is 3.7224028444290163 and perplexity is 41.363665246899046
At time: 254.24189567565918 and batch: 350, loss is 3.8133902597427367 and perplexity is 45.30377035149063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433557970770474 and perplexity of 84.23057431942887
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 255.99769115447998 and batch: 50, loss is 3.7893857097625734 and perplexity is 44.2292223497722
At time: 256.9243721961975 and batch: 100, loss is 3.7475623512268066 and perplexity is 42.417556767032494
At time: 257.8375868797302 and batch: 150, loss is 3.6689735794067384 and perplexity is 39.211637565443624
At time: 258.7519180774689 and batch: 200, loss is 3.7514201259613036 and perplexity is 42.581510190178825
At time: 259.66534996032715 and batch: 250, loss is 3.7746182441711427 and perplexity is 43.5808678892923
At time: 260.58036494255066 and batch: 300, loss is 3.7201675748825074 and perplexity is 41.271309564017415
At time: 261.4978950023651 and batch: 350, loss is 3.810992169380188 and perplexity is 45.19525797966321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433341716897899 and perplexity of 84.21236110095354
Finished 36 epochs...
Completing Train Step...
At time: 263.2471477985382 and batch: 50, loss is 3.788863458633423 and perplexity is 44.20612961908579
At time: 264.1857147216797 and batch: 100, loss is 3.7470363426208495 and perplexity is 42.39525063425258
At time: 265.1024754047394 and batch: 150, loss is 3.6684736680984495 and perplexity is 39.192040123307734
At time: 266.019305229187 and batch: 200, loss is 3.7510453557968138 and perplexity is 42.56555490057063
At time: 266.9367034435272 and batch: 250, loss is 3.7745262384414673 and perplexity is 43.576858384193805
At time: 267.85664415359497 and batch: 300, loss is 3.7202789878845213 and perplexity is 41.27590798066993
At time: 268.77706956863403 and batch: 350, loss is 3.811276226043701 and perplexity is 45.208097817385934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.4332775247508085 and perplexity of 84.20695550218328
Finished 37 epochs...
Completing Train Step...
At time: 270.54193234443665 and batch: 50, loss is 3.7885429525375365 and perplexity is 44.191963555343484
At time: 271.45642161369324 and batch: 100, loss is 3.7467094898223876 and perplexity is 42.38139589229522
At time: 272.3705151081085 and batch: 150, loss is 3.668176918029785 and perplexity is 39.180411608176705
At time: 273.28451204299927 and batch: 200, loss is 3.750822343826294 and perplexity is 42.55606333070217
At time: 274.19859623908997 and batch: 250, loss is 3.7744874238967894 and perplexity is 43.57516700110247
At time: 275.11162877082825 and batch: 300, loss is 3.720369176864624 and perplexity is 41.27963078058877
At time: 276.03595900535583 and batch: 350, loss is 3.811466307640076 and perplexity is 45.21669186154707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433250690328664 and perplexity of 84.20469588750967
Finished 38 epochs...
Completing Train Step...
At time: 277.80345702171326 and batch: 50, loss is 3.7883032751083374 and perplexity is 44.18137300833552
At time: 278.72136425971985 and batch: 100, loss is 3.7464631843566893 and perplexity is 42.37095840830047
At time: 279.6369891166687 and batch: 150, loss is 3.6679625368118285 and perplexity is 39.17201296410405
At time: 280.55160117149353 and batch: 200, loss is 3.7506599378585816 and perplexity is 42.549152533247494
At time: 281.47153186798096 and batch: 250, loss is 3.7744558000564576 and perplexity is 43.573789008767626
At time: 282.39224004745483 and batch: 300, loss is 3.720429639816284 and perplexity is 41.28212674436511
At time: 283.31761598587036 and batch: 350, loss is 3.8115945434570313 and perplexity is 45.222490632765094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433236483869882 and perplexity of 84.20349964546551
Finished 39 epochs...
Completing Train Step...
At time: 285.0797109603882 and batch: 50, loss is 3.788105697631836 and perplexity is 44.172644626442406
At time: 286.0064241886139 and batch: 100, loss is 3.7462602519989012 and perplexity is 42.362360842200616
At time: 286.91994619369507 and batch: 150, loss is 3.667789783477783 and perplexity is 39.165246452748754
At time: 287.8469340801239 and batch: 200, loss is 3.7505288457870485 and perplexity is 42.543575042290335
At time: 288.7633955478668 and batch: 250, loss is 3.7744223833084107 and perplexity is 43.572332938767566
At time: 289.6810665130615 and batch: 300, loss is 3.72046772480011 and perplexity is 41.283699003433995
At time: 290.5982012748718 and batch: 350, loss is 3.8116833353042603 and perplexity is 45.226506199516834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433229117557921 and perplexity of 84.20287937850343
Finished 40 epochs...
Completing Train Step...
At time: 292.35886120796204 and batch: 50, loss is 3.787933359146118 and perplexity is 44.16503263569563
At time: 293.27251505851746 and batch: 100, loss is 3.7460839557647705 and perplexity is 42.35489317579532
At time: 294.1875720024109 and batch: 150, loss is 3.6676409339904783 and perplexity is 39.1594171597485
At time: 295.10141825675964 and batch: 200, loss is 3.7504158067703246 and perplexity is 42.53876623019645
At time: 296.0142924785614 and batch: 250, loss is 3.774385814666748 and perplexity is 43.57073958687145
At time: 296.9441440105438 and batch: 300, loss is 3.720489673614502 and perplexity is 41.28460514162514
At time: 297.87167286872864 and batch: 350, loss is 3.8117460203170777 and perplexity is 45.22934131249625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433225960567079 and perplexity of 84.20261355120404
Finished 41 epochs...
Completing Train Step...
At time: 299.6319992542267 and batch: 50, loss is 3.787777557373047 and perplexity is 44.15815218131067
At time: 300.5471742153168 and batch: 100, loss is 3.7459250545501708 and perplexity is 42.3481634665191
At time: 301.4628915786743 and batch: 150, loss is 3.667506790161133 and perplexity is 39.154164517888375
At time: 302.3776113986969 and batch: 200, loss is 3.7503143072128298 and perplexity is 42.53444878336089
At time: 303.2928547859192 and batch: 250, loss is 3.774345664978027 and perplexity is 43.5689902703572
At time: 304.2102074623108 and batch: 300, loss is 3.720499668121338 and perplexity is 41.28501776295542
At time: 305.1296558380127 and batch: 350, loss is 3.8117904043197632 and perplexity is 45.231348816252726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433223329741379 and perplexity of 84.20239202909566
Finished 42 epochs...
Completing Train Step...
At time: 306.8746931552887 and batch: 50, loss is 3.787633376121521 and perplexity is 44.15178586262702
At time: 307.8017547130585 and batch: 100, loss is 3.7457783031463623 and perplexity is 42.34194927006385
At time: 308.7163670063019 and batch: 150, loss is 3.6673826360702515 and perplexity is 39.14930366994176
At time: 309.63755440711975 and batch: 200, loss is 3.750220470428467 and perplexity is 42.530457674721745
At time: 310.561949968338 and batch: 250, loss is 3.774302682876587 and perplexity is 43.567117623843174
At time: 311.50312066078186 and batch: 300, loss is 3.7205011796951295 and perplexity is 41.28508016835342
At time: 312.4199206829071 and batch: 350, loss is 3.8118218612670898 and perplexity is 45.23277167878929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433222803576239 and perplexity of 84.20234772474393
Finished 43 epochs...
Completing Train Step...
At time: 314.329665184021 and batch: 50, loss is 3.7874977874755857 and perplexity is 44.145799787597355
At time: 315.2473113536835 and batch: 100, loss is 3.745640501976013 and perplexity is 42.33611490190018
At time: 316.1679139137268 and batch: 150, loss is 3.6672656631469724 and perplexity is 39.14472452927011
At time: 317.0930268764496 and batch: 200, loss is 3.75013213634491 and perplexity is 42.526700951645566
At time: 318.01730704307556 and batch: 250, loss is 3.774257307052612 and perplexity is 43.56514077483371
At time: 318.93820118904114 and batch: 300, loss is 3.7204960680007932 and perplexity is 41.28486913218232
At time: 319.8549213409424 and batch: 350, loss is 3.811843729019165 and perplexity is 45.233760828641245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433223329741379 and perplexity of 84.20239202909566
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 321.62970519065857 and batch: 50, loss is 3.787616090774536 and perplexity is 44.15102269028425
At time: 322.5571024417877 and batch: 100, loss is 3.7456112241744997 and perplexity is 42.334875411676094
At time: 323.471910238266 and batch: 150, loss is 3.6671930313110352 and perplexity is 39.14188147930951
At time: 324.3864312171936 and batch: 200, loss is 3.750099415779114 and perplexity is 42.52530947669404
At time: 325.30059027671814 and batch: 250, loss is 3.773798065185547 and perplexity is 43.545138431553596
At time: 326.21136689186096 and batch: 300, loss is 3.7198559141159055 and perplexity is 41.25844892022318
At time: 327.1236822605133 and batch: 350, loss is 3.8111673736572267 and perplexity is 45.20317707587263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433222803576239 and perplexity of 84.20234772474393
Finished 45 epochs...
Completing Train Step...
At time: 328.8966519832611 and batch: 50, loss is 3.787544436454773 and perplexity is 44.14785919212703
At time: 329.8150713443756 and batch: 100, loss is 3.7455641269683837 and perplexity is 42.33288160427469
At time: 330.7329988479614 and batch: 150, loss is 3.6671463775634767 and perplexity is 39.14005540644891
At time: 331.65142703056335 and batch: 200, loss is 3.750061311721802 and perplexity is 42.523689120735796
At time: 332.5702278614044 and batch: 250, loss is 3.7737972259521486 and perplexity is 43.54510188703442
At time: 333.4934046268463 and batch: 300, loss is 3.719869403839111 and perplexity is 41.25900548903298
At time: 334.4296827316284 and batch: 350, loss is 3.8112091636657714 and perplexity is 45.205066156500955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433221751245959 and perplexity of 84.20225911611041
Finished 46 epochs...
Completing Train Step...
At time: 336.19989943504333 and batch: 50, loss is 3.787482204437256 and perplexity is 44.14511186726712
At time: 337.1140286922455 and batch: 100, loss is 3.7455200004577636 and perplexity is 42.331013643138604
At time: 338.028856754303 and batch: 150, loss is 3.6671039485931396 and perplexity is 39.138394769428885
At time: 338.9428286552429 and batch: 200, loss is 3.7500276231765746 and perplexity is 42.522256583641806
At time: 339.8569025993347 and batch: 250, loss is 3.7737951612472536 and perplexity is 43.54501197934221
At time: 340.7712209224701 and batch: 300, loss is 3.7198814249038694 and perplexity is 41.25950146919092
At time: 341.68928384780884 and batch: 350, loss is 3.811244778633118 and perplexity is 45.20667616212599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433221225080819 and perplexity of 84.20221481182863
Finished 47 epochs...
Completing Train Step...
At time: 343.5053894519806 and batch: 50, loss is 3.7874267148971557 and perplexity is 44.14266234327404
At time: 344.4281735420227 and batch: 100, loss is 3.7454780912399293 and perplexity is 42.32923962064089
At time: 345.3486695289612 and batch: 150, loss is 3.6670645570755003 and perplexity is 39.13685307902591
At time: 346.270544052124 and batch: 200, loss is 3.749996991157532 and perplexity is 42.520954061017946
At time: 347.18807339668274 and batch: 250, loss is 3.7737918519973754 and perplexity is 43.54486787825506
At time: 348.1023778915405 and batch: 300, loss is 3.7198918199539186 and perplexity is 41.2599303660029
At time: 349.0162978172302 and batch: 350, loss is 3.811275267601013 and perplexity is 45.208054488035906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433219646585399 and perplexity of 84.20208189912304
Finished 48 epochs...
Completing Train Step...
At time: 350.795672416687 and batch: 50, loss is 3.78737633228302 and perplexity is 44.140438376575375
At time: 351.71199107170105 and batch: 100, loss is 3.7454381561279297 and perplexity is 42.32754923146895
At time: 352.6406707763672 and batch: 150, loss is 3.667027678489685 and perplexity is 39.13540979384442
At time: 353.5619468688965 and batch: 200, loss is 3.749968843460083 and perplexity is 42.51975721091217
At time: 354.4816372394562 and batch: 250, loss is 3.7737874603271484 and perplexity is 43.54467664397518
At time: 355.39652824401855 and batch: 300, loss is 3.719900984764099 and perplexity is 41.26030850716556
At time: 356.31164479255676 and batch: 350, loss is 3.811301670074463 and perplexity is 45.20924810825144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433219120420259 and perplexity of 84.2020375949345
Finished 49 epochs...
Completing Train Step...
At time: 358.0783004760742 and batch: 50, loss is 3.7873302555084227 and perplexity is 44.13840457440156
At time: 358.99739933013916 and batch: 100, loss is 3.745399384498596 and perplexity is 42.32590815523335
At time: 359.9112961292267 and batch: 150, loss is 3.666992769241333 and perplexity is 39.134043629950575
At time: 360.82577872276306 and batch: 200, loss is 3.7499424266815184 and perplexity is 42.5186339907373
At time: 361.73875284194946 and batch: 250, loss is 3.7737820768356323 and perplexity is 43.5444422222089
At time: 362.651917219162 and batch: 300, loss is 3.7199088525772095 and perplexity is 41.26063313683883
At time: 363.5667724609375 and batch: 350, loss is 3.8113246536254883 and perplexity is 45.210287189252995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 30 batches
Done Evaluating: Achieved loss of 4.433217541924838 and perplexity of 84.20190468250864
Finished Training.
Improved accuracyfrom -89.85245582121264 to -84.20190468250864
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f73c4036cf8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.388774761071636, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.653007880104995, 'data': 'ptb', 'lr': 21.23500549346587}, 'best_accuracy': -164.34063869185889}, {'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.0847376931910826, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 2.2344801255126594, 'data': 'ptb', 'lr': 16.129247394125382}, 'best_accuracy': -137.91620153896054}, {'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.8468608750403953, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 5.643681528926425, 'data': 'ptb', 'lr': 25.36693148619431}, 'best_accuracy': -238.18262699381427}, {'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.8766070316297858, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 6.598249028711501, 'data': 'ptb', 'lr': 27.971676517488476}, 'best_accuracy': -187.8540171970785}, {'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.1407018580502576, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.874140291698923, 'data': 'ptb', 'lr': 0.9007984133907654}, 'best_accuracy': -89.85245582121264}, {'params': {'tune_wordvecs': True, 'num_layers': 1, 'wordvec_source': 'glove', 'dropout': 0.1425630954123083, 'seq_len': 35, 'batch_size': 80, 'wordvec_dim': 200, 'anneal': 4.701291397671644, 'data': 'ptb', 'lr': 2.288409930916093}, 'best_accuracy': -84.20190468250864}]
