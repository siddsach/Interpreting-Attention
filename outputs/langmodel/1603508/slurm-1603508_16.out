Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 22.06859021267849, 'num_layers': 1, 'dropout': 0.6841578975066428, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 4.726216532739262}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9258410930633545 and batch: 50, loss is 6.666465148925782 and perplexity is 785.6136631840827
At time: 1.3570702075958252 and batch: 100, loss is 6.101133098602295 and perplexity is 446.3632572279254
At time: 1.7986271381378174 and batch: 150, loss is 6.053147401809692 and perplexity is 425.44998702978506
At time: 2.229025363922119 and batch: 200, loss is 6.057349786758423 and perplexity is 427.2416536513976
At time: 2.654121160507202 and batch: 250, loss is 6.079207859039307 and perplexity is 436.68314307117356
At time: 3.0783498287200928 and batch: 300, loss is 6.015418872833252 and perplexity is 409.69741409665164
At time: 3.502936601638794 and batch: 350, loss is 6.0839893245697025 and perplexity is 438.77612824815697
At time: 3.914360523223877 and batch: 400, loss is 6.056530323028564 and perplexity is 426.8916880239986
At time: 4.340373516082764 and batch: 450, loss is 6.021050634384156 and perplexity is 412.01124158763014
At time: 4.767162084579468 and batch: 500, loss is 6.039189548492431 and perplexity is 419.5528698058039
At time: 5.19953179359436 and batch: 550, loss is 6.1445331859588626 and perplexity is 466.1619873021403
At time: 5.6309003829956055 and batch: 600, loss is 6.1536480331420895 and perplexity is 470.4304060193887
At time: 6.05673885345459 and batch: 650, loss is 6.143987483978272 and perplexity is 465.9076711791105
At time: 6.472399950027466 and batch: 700, loss is 6.184503927230835 and perplexity is 485.1722231732049
At time: 6.883429288864136 and batch: 750, loss is 6.095607023239136 and perplexity is 443.90342310327964
At time: 7.314460039138794 and batch: 800, loss is 6.150097827911377 and perplexity is 468.7632426683482
At time: 7.734368801116943 and batch: 850, loss is 6.1240356826782225 and perplexity is 456.7040931862784
At time: 8.1677987575531 and batch: 900, loss is 6.156411151885987 and perplexity is 471.7320585740887
At time: 8.585994482040405 and batch: 950, loss is 6.242304401397705 and perplexity is 514.0417055663529
At time: 9.006086587905884 and batch: 1000, loss is 6.147257843017578 and perplexity is 467.433850760389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.191599403939596 and perplexity of 488.6269935092788
Finished 1 epochs...
Completing Train Step...
At time: 10.326438188552856 and batch: 50, loss is 6.241456565856933 and perplexity is 513.6060674397737
At time: 10.759945154190063 and batch: 100, loss is 6.489025239944458 and perplexity is 657.8817736938367
At time: 11.182368516921997 and batch: 150, loss is 6.554465999603272 and perplexity is 702.3739816770307
At time: 11.594751596450806 and batch: 200, loss is 6.595944299697876 and perplexity is 732.1199009779318
At time: 12.019739389419556 and batch: 250, loss is 6.617016658782959 and perplexity is 747.7110890542592
At time: 12.444804668426514 and batch: 300, loss is 6.53264723777771 and perplexity is 687.2150272406877
At time: 12.870079278945923 and batch: 350, loss is 6.614584445953369 and perplexity is 745.894706360705
At time: 13.27293872833252 and batch: 400, loss is 6.541971645355225 and perplexity is 693.6528681297117
At time: 13.698181390762329 and batch: 450, loss is 6.440705823898315 and perplexity is 626.8490887715601
At time: 14.115159034729004 and batch: 500, loss is 6.4996764183044435 and perplexity is 664.9264402050657
At time: 14.54192852973938 and batch: 550, loss is 6.589102296829224 and perplexity is 727.1278318653021
At time: 14.956595659255981 and batch: 600, loss is 6.648629207611084 and perplexity is 771.7257243903939
At time: 15.394641876220703 and batch: 650, loss is 6.59775068283081 and perplexity is 733.4435851988351
At time: 15.79779052734375 and batch: 700, loss is 6.45713394165039 and perplexity is 637.2320925032978
At time: 16.21804404258728 and batch: 750, loss is 6.493736591339111 and perplexity is 660.9885988315148
At time: 16.635916471481323 and batch: 800, loss is 6.498295516967773 and perplexity is 664.0088760735746
At time: 17.055254220962524 and batch: 850, loss is 6.502231760025024 and perplexity is 666.6277272379548
At time: 17.474718809127808 and batch: 900, loss is 6.414928388595581 and perplexity is 610.8970120586675
At time: 17.882816076278687 and batch: 950, loss is 6.6007466316223145 and perplexity is 735.6442394993082
At time: 18.315001487731934 and batch: 1000, loss is 6.583675537109375 and perplexity is 723.192571350468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.721800269150153 and perplexity of 830.3109499707673
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 19.62937831878662 and batch: 50, loss is 6.2676436233520505 and perplexity is 527.2335521619518
At time: 20.045021533966064 and batch: 100, loss is 6.171054172515869 and perplexity is 478.6904625324141
At time: 20.469273805618286 and batch: 150, loss is 6.127456579208374 and perplexity is 458.2691059811677
At time: 20.904606103897095 and batch: 200, loss is 6.10830361366272 and perplexity is 449.5754143336628
At time: 21.309380769729614 and batch: 250, loss is 6.061758785247803 and perplexity is 429.1295201990976
At time: 21.71680188179016 and batch: 300, loss is 5.960590705871582 and perplexity is 387.83915545243735
At time: 22.123886108398438 and batch: 350, loss is 6.00528338432312 and perplexity is 405.56590347115
At time: 22.5304217338562 and batch: 400, loss is 5.952518043518066 and perplexity is 384.72086429317085
At time: 22.944469690322876 and batch: 450, loss is 5.915117988586426 and perplexity is 370.5980264621074
At time: 23.36193561553955 and batch: 500, loss is 5.888516645431519 and perplexity is 360.8695896035064
At time: 23.78376007080078 and batch: 550, loss is 5.951239900588989 and perplexity is 384.2294501564582
At time: 24.208521842956543 and batch: 600, loss is 5.963825492858887 and perplexity is 389.09576384014616
At time: 24.64036226272583 and batch: 650, loss is 5.932961082458496 and perplexity is 377.26998904932856
At time: 25.06554412841797 and batch: 700, loss is 5.909539699554443 and perplexity is 368.536478855835
At time: 25.49412727355957 and batch: 750, loss is 5.824994411468506 and perplexity is 338.65924046480757
At time: 25.935604095458984 and batch: 800, loss is 5.90644528388977 and perplexity is 367.39783642772335
At time: 26.348862886428833 and batch: 850, loss is 5.863614931106567 and perplexity is 351.99428198542466
At time: 26.769280672073364 and batch: 900, loss is 5.887506027221679 and perplexity is 360.5050724497374
At time: 27.19099497795105 and batch: 950, loss is 5.892462701797485 and perplexity is 362.2964146594256
At time: 27.607107877731323 and batch: 1000, loss is 5.831931715011597 and perplexity is 341.01679047863433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.918395623928163 and perplexity of 371.81470447542563
Finished 3 epochs...
Completing Train Step...
At time: 28.925247192382812 and batch: 50, loss is 5.934862546920776 and perplexity is 377.9880369811572
At time: 29.337905168533325 and batch: 100, loss is 5.9272669982910156 and perplexity is 375.1278864218528
At time: 29.745935916900635 and batch: 150, loss is 5.923160028457642 and perplexity is 373.59040686083006
At time: 30.158812046051025 and batch: 200, loss is 5.923129472732544 and perplexity is 373.5789917094589
At time: 30.56656289100647 and batch: 250, loss is 5.907450323104858 and perplexity is 367.7672712780305
At time: 30.97581934928894 and batch: 300, loss is 5.826546783447266 and perplexity is 339.1853738517579
At time: 31.402737617492676 and batch: 350, loss is 5.872014198303223 and perplexity is 354.96322703858146
At time: 31.821785926818848 and batch: 400, loss is 5.840501356124878 and perplexity is 343.9517397560743
At time: 32.249778270721436 and batch: 450, loss is 5.799564332962036 and perplexity is 330.1556906205711
At time: 32.67267370223999 and batch: 500, loss is 5.790175037384033 and perplexity is 327.07026889012144
At time: 33.09052109718323 and batch: 550, loss is 5.859432477951049 and perplexity is 350.52515680523953
At time: 33.507521867752075 and batch: 600, loss is 5.894852228164673 and perplexity is 363.16316664593717
At time: 33.93101191520691 and batch: 650, loss is 5.860571451187134 and perplexity is 350.92462302489724
At time: 34.346412897109985 and batch: 700, loss is 5.8426697731018065 and perplexity is 344.6983797686846
At time: 34.76622819900513 and batch: 750, loss is 5.767925777435303 and perplexity is 319.87355511360585
At time: 35.18254733085632 and batch: 800, loss is 5.831447629928589 and perplexity is 340.8517492875208
At time: 35.59281659126282 and batch: 850, loss is 5.799679479598999 and perplexity is 330.19370912682984
At time: 36.0259268283844 and batch: 900, loss is 5.840254545211792 and perplexity is 343.8668591882838
At time: 36.46342921257019 and batch: 950, loss is 5.866099538803101 and perplexity is 352.8699370667873
At time: 36.8796010017395 and batch: 1000, loss is 5.795089550018311 and perplexity is 328.68161610884897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.892432980421113 and perplexity of 362.2856468713446
Finished 4 epochs...
Completing Train Step...
At time: 38.16280674934387 and batch: 50, loss is 5.886621437072754 and perplexity is 360.18631421999135
At time: 38.60655879974365 and batch: 100, loss is 5.885335006713867 and perplexity is 359.7232575194815
At time: 39.01700568199158 and batch: 150, loss is 5.887798414230347 and perplexity is 360.61049486080117
At time: 39.43603157997131 and batch: 200, loss is 5.880274076461792 and perplexity is 357.907322239985
At time: 39.86212944984436 and batch: 250, loss is 5.873415822982788 and perplexity is 355.46110109270944
At time: 40.28293490409851 and batch: 300, loss is 5.788087091445923 and perplexity is 326.3880762891866
At time: 40.696765661239624 and batch: 350, loss is 5.835141115188598 and perplexity is 342.11300798574507
At time: 41.10107660293579 and batch: 400, loss is 5.808288745880127 and perplexity is 333.04870677633426
At time: 41.51142454147339 and batch: 450, loss is 5.77547251701355 and perplexity is 322.29668941333534
At time: 41.93536186218262 and batch: 500, loss is 5.769087343215943 and perplexity is 320.24532516524215
At time: 42.357720613479614 and batch: 550, loss is 5.839384307861328 and perplexity is 343.56774357349855
At time: 42.76250123977661 and batch: 600, loss is 5.873132200241089 and perplexity is 355.3602985362744
At time: 43.18466234207153 and batch: 650, loss is 5.8427743721008305 and perplexity is 344.73443675990336
At time: 43.61103415489197 and batch: 700, loss is 5.82181902885437 and perplexity is 337.58557335461035
At time: 44.034733057022095 and batch: 750, loss is 5.751721925735474 and perplexity is 314.73213932904395
At time: 44.44337511062622 and batch: 800, loss is 5.805835008621216 and perplexity is 332.232494550183
At time: 44.850512742996216 and batch: 850, loss is 5.786664047241211 and perplexity is 325.9239419489026
At time: 45.267258405685425 and batch: 900, loss is 5.825835227966309 and perplexity is 338.9441104862477
At time: 45.67293334007263 and batch: 950, loss is 5.85425371170044 and perplexity is 348.7145613251915
At time: 46.095627307891846 and batch: 1000, loss is 5.782026786804199 and perplexity is 324.4160467010863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.8867950439453125 and perplexity of 360.2488504677456
Finished 5 epochs...
Completing Train Step...
At time: 47.39496421813965 and batch: 50, loss is 5.864497470855713 and perplexity is 352.305068051105
At time: 47.81437039375305 and batch: 100, loss is 5.8610066795349125 and perplexity is 351.07738861031635
At time: 48.235848903656006 and batch: 150, loss is 5.867937059402466 and perplexity is 353.51893893958726
At time: 48.64828634262085 and batch: 200, loss is 5.859440584182739 and perplexity is 350.5279982548903
At time: 49.056495904922485 and batch: 250, loss is 5.860298776626587 and perplexity is 350.8289478522145
At time: 49.46544361114502 and batch: 300, loss is 5.766347608566284 and perplexity is 319.36913875868976
At time: 49.88197660446167 and batch: 350, loss is 5.8190467834472654 and perplexity is 336.6509993321874
At time: 50.29094862937927 and batch: 400, loss is 5.79206582069397 and perplexity is 327.6892729125377
At time: 50.716980934143066 and batch: 450, loss is 5.758982877731324 and perplexity is 317.02571096434673
At time: 51.137126445770264 and batch: 500, loss is 5.750887823104859 and perplexity is 314.46972987711126
At time: 51.56235647201538 and batch: 550, loss is 5.821606750488281 and perplexity is 337.51391884630357
At time: 51.98499059677124 and batch: 600, loss is 5.8602066993713375 and perplexity is 350.79664597279117
At time: 52.40864706039429 and batch: 650, loss is 5.829193906784058 and perplexity is 340.08442880005396
At time: 52.83181643486023 and batch: 700, loss is 5.806713237762451 and perplexity is 332.524398969284
At time: 53.25912070274353 and batch: 750, loss is 5.734599752426147 and perplexity is 309.38911373197567
At time: 53.673638582229614 and batch: 800, loss is 5.787006301879883 and perplexity is 326.03551002114483
At time: 54.10061740875244 and batch: 850, loss is 5.768890047073365 and perplexity is 320.1821482303961
At time: 54.52789282798767 and batch: 900, loss is 5.80953950881958 and perplexity is 333.465532376469
At time: 54.94048833847046 and batch: 950, loss is 5.83297173500061 and perplexity is 341.37163925024424
At time: 55.371870279312134 and batch: 1000, loss is 5.761164016723633 and perplexity is 317.7179427565391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.863562607183689 and perplexity of 351.9758647455959
Finished 6 epochs...
Completing Train Step...
At time: 56.68447732925415 and batch: 50, loss is 5.845313396453857 and perplexity is 345.61083802141616
At time: 57.109761476516724 and batch: 100, loss is 5.845047111511231 and perplexity is 345.5188193113847
At time: 57.530933141708374 and batch: 150, loss is 5.853350048065185 and perplexity is 348.39958299570384
At time: 57.963762521743774 and batch: 200, loss is 5.845316209793091 and perplexity is 345.61181034331406
At time: 58.377610206604004 and batch: 250, loss is 5.8492135906219485 and perplexity is 346.96141944916843
At time: 58.803664445877075 and batch: 300, loss is 5.75454607963562 and perplexity is 315.62224763764334
At time: 59.226285219192505 and batch: 350, loss is 5.806248550415039 and perplexity is 332.36991498453364
At time: 59.64227104187012 and batch: 400, loss is 5.777910509109497 and perplexity is 323.08340480795835
At time: 60.063623666763306 and batch: 450, loss is 5.736822576522827 and perplexity is 310.07759621347486
At time: 60.47974419593811 and batch: 500, loss is 5.711077404022217 and perplexity is 302.1964806547701
At time: 60.89372897148132 and batch: 550, loss is 5.759645738601685 and perplexity is 317.23592456653193
At time: 61.302489280700684 and batch: 600, loss is 5.79170223236084 and perplexity is 327.57015057307365
At time: 61.734405279159546 and batch: 650, loss is 5.759872255325317 and perplexity is 317.30779194805785
At time: 62.150728702545166 and batch: 700, loss is 5.740232648849488 and perplexity is 311.13678817763355
At time: 62.558143615722656 and batch: 750, loss is 5.666206159591675 and perplexity is 288.9362742844283
At time: 62.9841206073761 and batch: 800, loss is 5.718014783859253 and perplexity is 304.30022121289096
At time: 63.40288972854614 and batch: 850, loss is 5.696564426422119 and perplexity is 297.84238179360824
At time: 63.82260346412659 and batch: 900, loss is 5.739038877487182 and perplexity is 310.76558360092736
At time: 64.22552871704102 and batch: 950, loss is 5.757265796661377 and perplexity is 316.481819204119
At time: 64.6404161453247 and batch: 1000, loss is 5.690800638198852 and perplexity is 296.13061923837057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.792331974680831 and perplexity of 327.77650032642214
Finished 7 epochs...
Completing Train Step...
At time: 65.92690443992615 and batch: 50, loss is 5.767219018936157 and perplexity is 319.647561630692
At time: 66.36261558532715 and batch: 100, loss is 5.767070589065551 and perplexity is 319.60011990545127
At time: 66.7789957523346 and batch: 150, loss is 5.774437465667725 and perplexity is 321.96326837491733
At time: 67.19945287704468 and batch: 200, loss is 5.768512067794799 and perplexity is 320.0611488820623
At time: 67.61473059654236 and batch: 250, loss is 5.771498947143555 and perplexity is 321.01856204500683
At time: 68.02673363685608 and batch: 300, loss is 5.677668476104737 and perplexity is 292.2672069452163
At time: 68.44496011734009 and batch: 350, loss is 5.732309007644654 and perplexity is 308.6811933762418
At time: 68.8767294883728 and batch: 400, loss is 5.703643083572388 and perplexity is 299.9581855892017
At time: 69.29119277000427 and batch: 450, loss is 5.667216320037841 and perplexity is 289.22829374859725
At time: 69.71100544929504 and batch: 500, loss is 5.657188816070557 and perplexity is 286.3425484769632
At time: 70.12995839118958 and batch: 550, loss is 5.723088397979736 and perplexity is 305.8480463254917
At time: 70.53613138198853 and batch: 600, loss is 5.763166122436523 and perplexity is 318.3546848646312
At time: 70.94726514816284 and batch: 650, loss is 5.734715070724487 and perplexity is 309.4247940153513
At time: 71.36631321907043 and batch: 700, loss is 5.712790422439575 and perplexity is 302.7145924326043
At time: 71.79146957397461 and batch: 750, loss is 5.641516304016113 and perplexity is 281.8898252557206
At time: 72.20233750343323 and batch: 800, loss is 5.693600902557373 and perplexity is 296.96102539263444
At time: 72.6089358329773 and batch: 850, loss is 5.673573684692383 and perplexity is 291.0728806234908
At time: 73.03005003929138 and batch: 900, loss is 5.7201761054992675 and perplexity is 304.9586231189032
At time: 73.44586253166199 and batch: 950, loss is 5.737294292449951 and perplexity is 310.2238992582783
At time: 73.85964488983154 and batch: 1000, loss is 5.672678899765015 and perplexity is 290.8125494847021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.784132422470465 and perplexity of 325.09986838673956
Finished 8 epochs...
Completing Train Step...
At time: 75.16895318031311 and batch: 50, loss is 5.750319385528565 and perplexity is 314.2910242623474
At time: 75.589182138443 and batch: 100, loss is 5.74716700553894 and perplexity is 313.30181952034627
At time: 76.01145839691162 and batch: 150, loss is 5.756574792861938 and perplexity is 316.2632046050537
At time: 76.42703199386597 and batch: 200, loss is 5.752607297897339 and perplexity is 315.01091779678666
At time: 76.84802722930908 and batch: 250, loss is 5.756237440109253 and perplexity is 316.1565303368477
At time: 77.25544929504395 and batch: 300, loss is 5.663927869796753 and perplexity is 288.27874302713536
At time: 77.66497921943665 and batch: 350, loss is 5.7185546875 and perplexity is 304.4645583693265
At time: 78.07839274406433 and batch: 400, loss is 5.69094409942627 and perplexity is 296.17310554797865
At time: 78.50491523742676 and batch: 450, loss is 5.657913484573364 and perplexity is 286.55012710662146
At time: 78.92672872543335 and batch: 500, loss is 5.647524843215942 and perplexity is 283.5886699973667
At time: 79.36226511001587 and batch: 550, loss is 5.715122814178467 and perplexity is 303.42146547921124
At time: 79.78084349632263 and batch: 600, loss is 5.758293752670288 and perplexity is 316.8073158613407
At time: 80.19100189208984 and batch: 650, loss is 5.730645055770874 and perplexity is 308.1679898175449
At time: 80.59564685821533 and batch: 700, loss is 5.7055972290039065 and perplexity is 300.5449206032782
At time: 81.0017900466919 and batch: 750, loss is 5.633084754943848 and perplexity is 279.52304917679163
At time: 81.42197585105896 and batch: 800, loss is 5.686708908081055 and perplexity is 294.9214082344489
At time: 81.83690524101257 and batch: 850, loss is 5.664191045761108 and perplexity is 288.35462104753236
At time: 82.24771809577942 and batch: 900, loss is 5.713750743865967 and perplexity is 303.0054353707999
At time: 82.6546003818512 and batch: 950, loss is 5.7297436714172365 and perplexity is 307.8903371679077
At time: 83.06866359710693 and batch: 1000, loss is 5.664810638427735 and perplexity is 288.5333388165133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.777712193931022 and perplexity of 323.0193388177105
Finished 9 epochs...
Completing Train Step...
At time: 84.42508339881897 and batch: 50, loss is 5.743899335861206 and perplexity is 312.27972351025716
At time: 84.86118602752686 and batch: 100, loss is 5.7431251907348635 and perplexity is 312.0380672348287
At time: 85.2807948589325 and batch: 150, loss is 5.7529323387146 and perplexity is 315.11332584544965
At time: 85.70545768737793 and batch: 200, loss is 5.746394853591919 and perplexity is 313.05999628455464
At time: 86.11627721786499 and batch: 250, loss is 5.749473886489868 and perplexity is 314.02540381020475
At time: 86.5273003578186 and batch: 300, loss is 5.6554810905456545 and perplexity is 285.85397129466367
At time: 86.95877957344055 and batch: 350, loss is 5.712405061721801 and perplexity is 302.59796059409183
At time: 87.38019132614136 and batch: 400, loss is 5.68303936958313 and perplexity is 293.84116598050707
At time: 87.80689120292664 and batch: 450, loss is 5.651845197677613 and perplexity is 284.81652404555535
At time: 88.22614932060242 and batch: 500, loss is 5.639683218002319 and perplexity is 281.373570274067
At time: 88.64332389831543 and batch: 550, loss is 5.706710872650146 and perplexity is 300.87980698195094
At time: 89.05429768562317 and batch: 600, loss is 5.753487873077392 and perplexity is 315.28843076002585
At time: 89.48976993560791 and batch: 650, loss is 5.723873662948608 and perplexity is 306.08831240599073
At time: 89.90706634521484 and batch: 700, loss is 5.699040832519532 and perplexity is 298.5808745104773
At time: 90.33713865280151 and batch: 750, loss is 5.627702407836914 and perplexity is 278.02260069130443
At time: 90.74525165557861 and batch: 800, loss is 5.676276235580445 and perplexity is 291.8605838200629
At time: 91.15488052368164 and batch: 850, loss is 5.653512945175171 and perplexity is 285.29192240281725
At time: 91.57199692726135 and batch: 900, loss is 5.703546762466431 and perplexity is 299.92929467644956
At time: 91.98740887641907 and batch: 950, loss is 5.720019207000733 and perplexity is 304.9107793222287
At time: 92.41418623924255 and batch: 1000, loss is 5.6571902084350585 and perplexity is 286.34294717044065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.77192241389577 and perplexity of 321.15453152012094
Finished 10 epochs...
Completing Train Step...
At time: 93.7343921661377 and batch: 50, loss is 5.733168992996216 and perplexity is 308.9467688599933
At time: 94.17771768569946 and batch: 100, loss is 5.7348956203460695 and perplexity is 309.4806655884629
At time: 94.61065578460693 and batch: 150, loss is 5.745976161956787 and perplexity is 312.9289481191156
At time: 95.03924894332886 and batch: 200, loss is 5.736786098480224 and perplexity is 310.06628539600956
At time: 95.45472693443298 and batch: 250, loss is 5.73677020072937 and perplexity is 310.0613560786386
At time: 95.88509750366211 and batch: 300, loss is 5.646034746170044 and perplexity is 283.1664100402426
At time: 96.31068229675293 and batch: 350, loss is 5.701855773925781 and perplexity is 299.4225462497894
At time: 96.7289879322052 and batch: 400, loss is 5.672370986938477 and perplexity is 290.7230183551965
At time: 97.14733505249023 and batch: 450, loss is 5.641511011123657 and perplexity is 281.88833324713966
At time: 97.5700695514679 and batch: 500, loss is 5.63284013748169 and perplexity is 279.454681320214
At time: 97.99489331245422 and batch: 550, loss is 5.6991155815124515 and perplexity is 298.60319396432004
At time: 98.41995286941528 and batch: 600, loss is 5.742664337158203 and perplexity is 311.8942965066611
At time: 98.83744406700134 and batch: 650, loss is 5.7132909393310545 and perplexity is 302.86614412334086
At time: 99.25973582267761 and batch: 700, loss is 5.6865911960601805 and perplexity is 294.8866944826395
At time: 99.6868884563446 and batch: 750, loss is 5.614959869384766 and perplexity is 274.502363025628
At time: 100.1042263507843 and batch: 800, loss is 5.665289468765259 and perplexity is 288.67153041505037
At time: 100.52697491645813 and batch: 850, loss is 5.641033658981323 and perplexity is 281.75380535850763
At time: 100.97767400741577 and batch: 900, loss is 5.6947531986236575 and perplexity is 297.3034096401606
At time: 101.39316272735596 and batch: 950, loss is 5.7083728313446045 and perplexity is 301.3802725545114
At time: 101.80235362052917 and batch: 1000, loss is 5.648300008773804 and perplexity is 283.8085833905807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.765595226753049 and perplexity of 319.1289415982974
Finished 11 epochs...
Completing Train Step...
At time: 103.05818033218384 and batch: 50, loss is 5.722838459014892 and perplexity is 305.7716125336829
At time: 103.4900918006897 and batch: 100, loss is 5.7224909114837645 and perplexity is 305.66536082948613
At time: 103.90858888626099 and batch: 150, loss is 5.736372365951538 and perplexity is 309.9380274217698
At time: 104.33832359313965 and batch: 200, loss is 5.727797374725342 and perplexity is 307.2916740002645
At time: 104.75557231903076 and batch: 250, loss is 5.726011266708374 and perplexity is 306.7433077442505
At time: 105.1804711818695 and batch: 300, loss is 5.6283588790893555 and perplexity is 278.2051744568449
At time: 105.60345792770386 and batch: 350, loss is 5.68746732711792 and perplexity is 295.14516708559364
At time: 106.02186465263367 and batch: 400, loss is 5.659210147857666 and perplexity is 286.9219271331811
At time: 106.43027472496033 and batch: 450, loss is 5.632007961273193 and perplexity is 279.222222519823
At time: 106.83725428581238 and batch: 500, loss is 5.620928897857666 and perplexity is 276.14577535443146
At time: 107.26975274085999 and batch: 550, loss is 5.6864272975921635 and perplexity is 294.83836696568636
At time: 107.6847608089447 and batch: 600, loss is 5.734250059127808 and perplexity is 309.28094134700575
At time: 108.10071182250977 and batch: 650, loss is 5.7032074546813964 and perplexity is 299.8275435952492
At time: 108.51516509056091 and batch: 700, loss is 5.674562730789185 and perplexity is 291.360907532422
At time: 108.92775678634644 and batch: 750, loss is 5.6025251770019535 and perplexity is 271.1101448866393
At time: 109.34231901168823 and batch: 800, loss is 5.655438528060913 and perplexity is 285.84180489829015
At time: 109.7622287273407 and batch: 850, loss is 5.629223213195801 and perplexity is 278.44574062751394
At time: 110.19066786766052 and batch: 900, loss is 5.684917554855347 and perplexity is 294.3935727295446
At time: 110.61569213867188 and batch: 950, loss is 5.696422052383423 and perplexity is 297.79997978936126
At time: 111.04133820533752 and batch: 1000, loss is 5.6296106052398684 and perplexity is 278.5536291883733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.736147624690358 and perplexity of 309.86837938528635
Finished 12 epochs...
Completing Train Step...
At time: 112.35297179222107 and batch: 50, loss is 5.701406106948853 and perplexity is 299.287936085734
At time: 112.79170036315918 and batch: 100, loss is 5.697793073654175 and perplexity is 298.2085499112468
At time: 113.20971059799194 and batch: 150, loss is 5.707906751632691 and perplexity is 301.2398380532809
At time: 113.63801789283752 and batch: 200, loss is 5.6991236114501955 and perplexity is 298.60559173900475
At time: 114.05188989639282 and batch: 250, loss is 5.698919696807861 and perplexity is 298.54470789433094
At time: 114.4786467552185 and batch: 300, loss is 5.6045895862579345 and perplexity is 271.6704052835324
At time: 114.88827657699585 and batch: 350, loss is 5.655406541824341 and perplexity is 285.83266204092
At time: 115.29859852790833 and batch: 400, loss is 5.623756055831909 and perplexity is 276.92758771806695
At time: 115.71590876579285 and batch: 450, loss is 5.59046558380127 and perplexity is 267.8603021040071
At time: 116.14900660514832 and batch: 500, loss is 5.56018895149231 and perplexity is 259.87193487408643
At time: 116.57461166381836 and batch: 550, loss is 5.614648065567017 and perplexity is 274.416785483251
At time: 116.9975974559784 and batch: 600, loss is 5.648684701919556 and perplexity is 283.9177836102579
At time: 117.40977501869202 and batch: 650, loss is 5.619837589263916 and perplexity is 275.84457947537777
At time: 117.81923222541809 and batch: 700, loss is 5.583581972122192 and perplexity is 266.0227874245807
At time: 118.23949217796326 and batch: 750, loss is 5.514228639602661 and perplexity is 248.19845293256216
At time: 118.65368700027466 and batch: 800, loss is 5.567615756988525 and perplexity is 261.809137899103
At time: 119.06796264648438 and batch: 850, loss is 5.531623706817627 and perplexity is 252.55365136994104
At time: 119.49017381668091 and batch: 900, loss is 5.583074617385864 and perplexity is 265.8878537359271
At time: 119.91625237464905 and batch: 950, loss is 5.5861573410034175 and perplexity is 266.7087771922073
At time: 120.32840919494629 and batch: 1000, loss is 5.523479690551758 and perplexity is 250.50520294575554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.626394132288491 and perplexity of 277.65910834682984
Finished 13 epochs...
Completing Train Step...
At time: 121.63203024864197 and batch: 50, loss is 5.59132194519043 and perplexity is 268.08978557078353
At time: 122.06650233268738 and batch: 100, loss is 5.591231498718262 and perplexity is 268.06553889198426
At time: 122.48377108573914 and batch: 150, loss is 5.603226737976074 and perplexity is 271.3004119181381
At time: 122.91298317909241 and batch: 200, loss is 5.591785230636597 and perplexity is 268.21401644165974
At time: 123.3294186592102 and batch: 250, loss is 5.587230043411255 and perplexity is 266.9950298441983
At time: 123.74452066421509 and batch: 300, loss is 5.497174263000488 and perplexity is 244.00147320502526
At time: 124.16184663772583 and batch: 350, loss is 5.550901136398315 and perplexity is 257.46946650626325
At time: 124.57758736610413 and batch: 400, loss is 5.519879341125488 and perplexity is 249.60491832415855
At time: 124.99532794952393 and batch: 450, loss is 5.493954381942749 and perplexity is 243.21708098590685
At time: 125.40751433372498 and batch: 500, loss is 5.481560192108154 and perplexity is 240.2212063834996
At time: 125.81385397911072 and batch: 550, loss is 5.541219844818115 and perplexity is 254.98885665670636
At time: 126.21888518333435 and batch: 600, loss is 5.589746398925781 and perplexity is 267.6677302816615
At time: 126.6422119140625 and batch: 650, loss is 5.564729919433594 and perplexity is 261.054688389685
At time: 127.06272625923157 and batch: 700, loss is 5.53094898223877 and perplexity is 252.38330468889043
At time: 127.48743271827698 and batch: 750, loss is 5.46733304977417 and perplexity is 236.82774197870054
At time: 127.91429114341736 and batch: 800, loss is 5.516938219070434 and perplexity is 248.87187830232492
At time: 128.33804726600647 and batch: 850, loss is 5.489256286621094 and perplexity is 242.07710391286378
At time: 128.74739837646484 and batch: 900, loss is 5.5484865951538085 and perplexity is 256.8485457811973
At time: 129.16606044769287 and batch: 950, loss is 5.554140014648437 and perplexity is 258.30473069071786
At time: 129.5871720314026 and batch: 1000, loss is 5.4933138275146485 and perplexity is 243.06133709423665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.611201588700458 and perplexity of 273.4726422986698
Finished 14 epochs...
Completing Train Step...
At time: 130.90490055084229 and batch: 50, loss is 5.568605632781982 and perplexity is 262.0684247369254
At time: 131.3273205757141 and batch: 100, loss is 5.566513385772705 and perplexity is 261.5206860611571
At time: 131.74291586875916 and batch: 150, loss is 5.5775559139251705 and perplexity is 264.42453903122225
At time: 132.1607804298401 and batch: 200, loss is 5.56802978515625 and perplexity is 261.917556699431
At time: 132.5840609073639 and batch: 250, loss is 5.564578609466553 and perplexity is 261.0151912016233
At time: 132.99649119377136 and batch: 300, loss is 5.480603294372559 and perplexity is 239.99144919966517
At time: 133.41553330421448 and batch: 350, loss is 5.532112188339234 and perplexity is 252.67704929819965
At time: 133.83625197410583 and batch: 400, loss is 5.50081953048706 and perplexity is 244.89254695634858
At time: 134.25034952163696 and batch: 450, loss is 5.475739316940308 and perplexity is 238.82697050167414
At time: 134.66997694969177 and batch: 500, loss is 5.46438455581665 and perplexity is 236.13048524627476
At time: 135.1010205745697 and batch: 550, loss is 5.524583606719971 and perplexity is 250.7818923823816
At time: 135.523672580719 and batch: 600, loss is 5.571406621932983 and perplexity is 262.80350454612733
At time: 135.93877482414246 and batch: 650, loss is 5.545065879821777 and perplexity is 255.97144104072188
At time: 136.35527110099792 and batch: 700, loss is 5.514677410125732 and perplexity is 248.3098620788109
At time: 136.78222584724426 and batch: 750, loss is 5.455965156555176 and perplexity is 234.1507541809497
At time: 137.188214302063 and batch: 800, loss is 5.502364330291748 and perplexity is 245.2711492721563
At time: 137.59977197647095 and batch: 850, loss is 5.472891788482666 and perplexity is 238.14787124281932
At time: 138.02520442008972 and batch: 900, loss is 5.534166831970214 and perplexity is 253.19674429942037
At time: 138.45795726776123 and batch: 950, loss is 5.5368925476074216 and perplexity is 253.88782804566546
At time: 138.87856340408325 and batch: 1000, loss is 5.47928451538086 and perplexity is 239.67516212049088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.598898166563453 and perplexity of 270.1286066618734
Finished 15 epochs...
Completing Train Step...
At time: 140.16639637947083 and batch: 50, loss is 5.554515686035156 and perplexity is 258.4017866165181
At time: 140.60373616218567 and batch: 100, loss is 5.554156932830811 and perplexity is 258.3091007742264
At time: 141.01073789596558 and batch: 150, loss is 5.5611973476409915 and perplexity is 260.1341209038379
At time: 141.43466138839722 and batch: 200, loss is 5.550504465103149 and perplexity is 257.36735601300813
At time: 141.84290027618408 and batch: 250, loss is 5.549097967147827 and perplexity is 257.0056238004506
At time: 142.2689425945282 and batch: 300, loss is 5.463349170684815 and perplexity is 235.8861257775879
At time: 142.6979353427887 and batch: 350, loss is 5.5174347686767575 and perplexity is 248.99548622171295
At time: 143.1281087398529 and batch: 400, loss is 5.486904020309448 and perplexity is 241.50834329698029
At time: 143.5562481880188 and batch: 450, loss is 5.462452211380005 and perplexity is 235.67464038327122
At time: 143.98696947097778 and batch: 500, loss is 5.451570520401001 and perplexity is 233.12400455905617
At time: 144.43395256996155 and batch: 550, loss is 5.509901628494263 and perplexity is 247.12681563490523
At time: 144.8627290725708 and batch: 600, loss is 5.559855432510376 and perplexity is 259.78527710274307
At time: 145.28193593025208 and batch: 650, loss is 5.535010948181152 and perplexity is 253.41056200664056
At time: 145.70329451560974 and batch: 700, loss is 5.503053913116455 and perplexity is 245.44034237371437
At time: 146.1225414276123 and batch: 750, loss is 5.444269151687622 and perplexity is 231.42807906862927
At time: 146.52868175506592 and batch: 800, loss is 5.491230354309082 and perplexity is 242.55545249259592
At time: 146.94612860679626 and batch: 850, loss is 5.461758995056153 and perplexity is 235.51132348894825
At time: 147.37349605560303 and batch: 900, loss is 5.524167566299439 and perplexity is 250.67757867927608
At time: 147.80328249931335 and batch: 950, loss is 5.523634262084961 and perplexity is 250.54392691178452
At time: 148.23072052001953 and batch: 1000, loss is 5.466818246841431 and perplexity is 236.7058537394794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.597591865353468 and perplexity of 269.7759677126285
Finished 16 epochs...
Completing Train Step...
At time: 149.69874835014343 and batch: 50, loss is 5.547107706069946 and perplexity is 256.4946241905995
At time: 150.1411612033844 and batch: 100, loss is 5.547975988388061 and perplexity is 256.71743065293356
At time: 150.56906509399414 and batch: 150, loss is 5.5543272495269775 and perplexity is 258.3530988735592
At time: 150.98752331733704 and batch: 200, loss is 5.543258552551269 and perplexity is 255.50923467975196
At time: 151.40598464012146 and batch: 250, loss is 5.541331224441528 and perplexity is 255.01725880121603
At time: 151.83220553398132 and batch: 300, loss is 5.453014059066772 and perplexity is 233.46077108282628
At time: 152.2547595500946 and batch: 350, loss is 5.51050877571106 and perplexity is 247.27690355133356
At time: 152.68565917015076 and batch: 400, loss is 5.478941020965576 and perplexity is 239.5928491786475
At time: 153.12261128425598 and batch: 450, loss is 5.453561782836914 and perplexity is 233.58867812220916
At time: 153.54462957382202 and batch: 500, loss is 5.4423832607269285 and perplexity is 230.99204223445864
At time: 153.96756768226624 and batch: 550, loss is 5.503238735198974 and perplexity is 245.48570936120757
At time: 154.3806083202362 and batch: 600, loss is 5.551337337493896 and perplexity is 257.5817994677312
At time: 154.7909390926361 and batch: 650, loss is 5.5277997016906735 and perplexity is 251.58972910872643
At time: 155.22643947601318 and batch: 700, loss is 5.493148136138916 and perplexity is 243.02106726318007
At time: 155.6305856704712 and batch: 750, loss is 5.437896575927734 and perplexity is 229.95797525081502
At time: 156.03818011283875 and batch: 800, loss is 5.486464347839355 and perplexity is 241.40218206693012
At time: 156.44234657287598 and batch: 850, loss is 5.4522899150848385 and perplexity is 233.29177306726197
At time: 156.86354517936707 and batch: 900, loss is 5.514224061965942 and perplexity is 248.197316772811
At time: 157.27921295166016 and batch: 950, loss is 5.517623233795166 and perplexity is 249.0424176078326
At time: 157.69658994674683 and batch: 1000, loss is 5.460144519805908 and perplexity is 235.13140305461502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.5759455983231705 and perplexity of 263.999074728311
Finished 17 epochs...
Completing Train Step...
At time: 159.11445689201355 and batch: 50, loss is 5.517947750091553 and perplexity is 249.12324904568797
At time: 159.53111815452576 and batch: 100, loss is 5.517913398742675 and perplexity is 249.11469147302947
At time: 159.96328496932983 and batch: 150, loss is 5.519955215454101 and perplexity is 249.62385764825004
At time: 160.38918948173523 and batch: 200, loss is 5.5119343090057376 and perplexity is 247.6296563810656
At time: 160.8010814189911 and batch: 250, loss is 5.512396383285522 and perplexity is 247.74410611629432
At time: 161.21083116531372 and batch: 300, loss is 5.418413171768188 and perplexity is 225.52097536761414
At time: 161.62241625785828 and batch: 350, loss is 5.4725221824645995 and perplexity is 238.05986662093892
At time: 162.02923607826233 and batch: 400, loss is 5.440771732330322 and perplexity is 230.62009178386143
At time: 162.45281171798706 and batch: 450, loss is 5.414744758605957 and perplexity is 224.6951868460277
At time: 162.869863986969 and batch: 500, loss is 5.401155233383179 and perplexity is 221.66234008487916
At time: 163.28813695907593 and batch: 550, loss is 5.4640359210968015 and perplexity is 236.0481763094134
At time: 163.69755673408508 and batch: 600, loss is 5.5085282802581785 and perplexity is 246.78765740305036
At time: 164.11456942558289 and batch: 650, loss is 5.486043758392334 and perplexity is 241.30067220514962
At time: 164.52680897712708 and batch: 700, loss is 5.458462419509888 and perplexity is 234.7362209131112
At time: 164.9354898929596 and batch: 750, loss is 5.39388750076294 and perplexity is 220.0571974043639
At time: 165.3582935333252 and batch: 800, loss is 5.445742626190185 and perplexity is 231.7693337956414
At time: 165.773122549057 and batch: 850, loss is 5.4069473171234135 and perplexity is 222.9499522998365
At time: 166.20959496498108 and batch: 900, loss is 5.483311424255371 and perplexity is 240.6422580545653
At time: 166.61943793296814 and batch: 950, loss is 5.478194332122802 and perplexity is 239.41401464654143
At time: 167.0331916809082 and batch: 1000, loss is 5.424872379302979 and perplexity is 226.98237681762257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.553803234565549 and perplexity of 258.2177534490287
Finished 18 epochs...
Completing Train Step...
At time: 168.3541693687439 and batch: 50, loss is 5.502797193527222 and perplexity is 245.3773411170134
At time: 168.7820429801941 and batch: 100, loss is 5.494217462539673 and perplexity is 243.2810750981903
At time: 169.19830322265625 and batch: 150, loss is 5.503673076629639 and perplexity is 245.5923571345158
At time: 169.6246678829193 and batch: 200, loss is 5.50463755607605 and perplexity is 245.82934017943515
At time: 170.033522605896 and batch: 250, loss is 5.495936336517334 and perplexity is 243.69960420378197
At time: 170.44289135932922 and batch: 300, loss is 5.406892156600952 and perplexity is 222.93765460316166
At time: 170.86770462989807 and batch: 350, loss is 5.463855285644531 and perplexity is 236.00554149112386
At time: 171.27609658241272 and batch: 400, loss is 5.431234817504883 and perplexity is 228.4311421156451
At time: 171.70360136032104 and batch: 450, loss is 5.403147964477539 and perplexity is 222.1044939229506
At time: 172.13084530830383 and batch: 500, loss is 5.390867223739624 and perplexity is 219.3935663864601
At time: 172.56160688400269 and batch: 550, loss is 5.45259352684021 and perplexity is 233.36261394551414
At time: 172.98133969306946 and batch: 600, loss is 5.4999307441711425 and perplexity is 244.67498650843964
At time: 173.40003371238708 and batch: 650, loss is 5.473779592514038 and perplexity is 238.35939376435152
At time: 173.81833052635193 and batch: 700, loss is 5.451187238693238 and perplexity is 233.0346695138062
At time: 174.23318433761597 and batch: 750, loss is 5.3864569568634035 and perplexity is 218.42811272679333
At time: 174.6683144569397 and batch: 800, loss is 5.441320075988769 and perplexity is 230.74658552655285
At time: 175.08793783187866 and batch: 850, loss is 5.403944721221924 and perplexity is 222.2815276935018
At time: 175.49699711799622 and batch: 900, loss is 5.478420457839966 and perplexity is 239.468158433723
At time: 175.9030179977417 and batch: 950, loss is 5.4704685974121094 and perplexity is 237.57149206822066
At time: 176.31941771507263 and batch: 1000, loss is 5.418565082550049 and perplexity is 225.5552370376008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.544717183927211 and perplexity of 255.88220040992968
Finished 19 epochs...
Completing Train Step...
At time: 177.67284607887268 and batch: 50, loss is 5.49202564239502 and perplexity is 242.74843068059803
At time: 178.10026907920837 and batch: 100, loss is 5.485711612701416 and perplexity is 241.22053853542425
At time: 178.50946855545044 and batch: 150, loss is 5.495073089599609 and perplexity is 243.48932204737233
At time: 178.92886304855347 and batch: 200, loss is 5.497253828048706 and perplexity is 244.02088796636394
At time: 179.34136939048767 and batch: 250, loss is 5.493701591491699 and perplexity is 243.1556058007986
At time: 179.7484278678894 and batch: 300, loss is 5.405010604858399 and perplexity is 222.51858024933793
At time: 180.16840744018555 and batch: 350, loss is 5.457245006561279 and perplexity is 234.45062387830163
At time: 180.59122467041016 and batch: 400, loss is 5.421595783233642 and perplexity is 226.23986437491493
At time: 181.01110172271729 and batch: 450, loss is 5.395518379211426 and perplexity is 220.4163767543644
At time: 181.43157148361206 and batch: 500, loss is 5.385800123214722 and perplexity is 218.28468890049032
At time: 181.84657645225525 and batch: 550, loss is 5.45322585105896 and perplexity is 233.51022144104167
At time: 182.26196765899658 and batch: 600, loss is 5.497506217956543 and perplexity is 244.08248414858855
At time: 182.67047357559204 and batch: 650, loss is 5.471334571838379 and perplexity is 237.77731200930293
At time: 183.10066890716553 and batch: 700, loss is 5.447819929122925 and perplexity is 232.25128932295877
At time: 183.52296352386475 and batch: 750, loss is 5.381195878982544 and perplexity is 217.28196304940917
At time: 183.94781613349915 and batch: 800, loss is 5.434416265487671 and perplexity is 229.15904118448768
At time: 184.36111617088318 and batch: 850, loss is 5.398055934906006 and perplexity is 220.97640583836076
At time: 184.79465198516846 and batch: 900, loss is 5.4754995918273925 and perplexity is 238.7697245411248
At time: 185.2148814201355 and batch: 950, loss is 5.4639113330841065 and perplexity is 236.01876936814097
At time: 185.63778162002563 and batch: 1000, loss is 5.412063436508179 and perplexity is 224.09351367653227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.544018629120617 and perplexity of 255.7035150869235
Finished 20 epochs...
Completing Train Step...
At time: 187.01496624946594 and batch: 50, loss is 5.485940952301025 and perplexity is 241.27586630133075
At time: 187.42658853530884 and batch: 100, loss is 5.483297719955444 and perplexity is 240.63896024348298
At time: 187.8634762763977 and batch: 150, loss is 5.492344694137573 and perplexity is 242.82589234689152
At time: 188.2874538898468 and batch: 200, loss is 5.493836212158203 and perplexity is 243.18834177393552
At time: 188.70855832099915 and batch: 250, loss is 5.492407751083374 and perplexity is 242.84120468879397
At time: 189.13390183448792 and batch: 300, loss is 5.398480854034424 and perplexity is 221.0703228922937
At time: 189.5526418685913 and batch: 350, loss is 5.4562249183654785 and perplexity is 234.2115855052249
At time: 189.96097111701965 and batch: 400, loss is 5.4215800571441655 and perplexity is 226.23630653454006
At time: 190.376629114151 and batch: 450, loss is 5.396489343643188 and perplexity is 220.63049715119337
At time: 190.79692840576172 and batch: 500, loss is 5.385471448898316 and perplexity is 218.21295611859134
At time: 191.22019410133362 and batch: 550, loss is 5.451700935363769 and perplexity is 233.15440940001122
At time: 191.64069271087646 and batch: 600, loss is 5.496027402877807 and perplexity is 243.72179805032658
At time: 192.06500959396362 and batch: 650, loss is 5.468137502670288 and perplexity is 237.0183353930501
At time: 192.489887714386 and batch: 700, loss is 5.4409997940063475 and perplexity is 230.67269338649555
At time: 192.91675996780396 and batch: 750, loss is 5.377130184173584 and perplexity is 216.40035429064218
At time: 193.3284900188446 and batch: 800, loss is 5.432035045623779 and perplexity is 228.61401229796994
At time: 193.74803757667542 and batch: 850, loss is 5.395096368789673 and perplexity is 220.32337837077282
At time: 194.17215847969055 and batch: 900, loss is 5.471747379302979 and perplexity is 237.87548852121716
At time: 194.5836639404297 and batch: 950, loss is 5.462394227981568 and perplexity is 235.66097556286653
At time: 195.007798910141 and batch: 1000, loss is 5.40603497505188 and perplexity is 222.7466384385044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.555124050233422 and perplexity of 258.55903684011673
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 196.3487696647644 and batch: 50, loss is 5.460395965576172 and perplexity is 235.1905332850809
At time: 196.80744552612305 and batch: 100, loss is 5.4302651309967045 and perplexity is 228.2097428803617
At time: 197.2325234413147 and batch: 150, loss is 5.420469760894775 and perplexity is 225.98525660760535
At time: 197.65522694587708 and batch: 200, loss is 5.413149585723877 and perplexity is 224.3370449023385
At time: 198.0643608570099 and batch: 250, loss is 5.407739953994751 and perplexity is 223.12674070774162
At time: 198.49468731880188 and batch: 300, loss is 5.300613098144531 and perplexity is 200.4596737611996
At time: 198.91430687904358 and batch: 350, loss is 5.362100658416748 and perplexity is 213.17227858026328
At time: 199.33018493652344 and batch: 400, loss is 5.315983695983887 and perplexity is 203.56466040957625
At time: 199.74967432022095 and batch: 450, loss is 5.299529409408569 and perplexity is 200.24255553626566
At time: 200.1755621433258 and batch: 500, loss is 5.282231845855713 and perplexity is 196.80863208085728
At time: 200.5957486629486 and batch: 550, loss is 5.3523342990875244 and perplexity is 211.10049486821902
At time: 201.00324416160583 and batch: 600, loss is 5.381722621917724 and perplexity is 217.3964449370034
At time: 201.42629384994507 and batch: 650, loss is 5.352475051879883 and perplexity is 211.1302099435308
At time: 201.8492546081543 and batch: 700, loss is 5.329316864013672 and perplexity is 206.29699710594804
At time: 202.26791143417358 and batch: 750, loss is 5.260374259948731 and perplexity is 192.5535428924823
At time: 202.6745994091034 and batch: 800, loss is 5.30319764137268 and perplexity is 200.97844055236044
At time: 203.08600759506226 and batch: 850, loss is 5.267452068328858 and perplexity is 193.92123439158706
At time: 203.50499606132507 and batch: 900, loss is 5.342687282562256 and perplexity is 209.07379642029673
At time: 203.91118717193604 and batch: 950, loss is 5.322224063873291 and perplexity is 204.8389506645061
At time: 204.33376288414001 and batch: 1000, loss is 5.277664070129394 and perplexity is 195.91170443018453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.448312898961509 and perplexity of 232.3658104289736
Finished 22 epochs...
Completing Train Step...
At time: 205.60869240760803 and batch: 50, loss is 5.387425889968872 and perplexity is 218.63985752308483
At time: 206.04339480400085 and batch: 100, loss is 5.378060207366944 and perplexity is 216.60170525520414
At time: 206.4568042755127 and batch: 150, loss is 5.385842046737671 and perplexity is 218.29384035548415
At time: 206.8705973625183 and batch: 200, loss is 5.381909866333007 and perplexity is 217.437155018469
At time: 207.28344655036926 and batch: 250, loss is 5.382123527526855 and perplexity is 217.48361786407386
At time: 207.69429540634155 and batch: 300, loss is 5.2790078449249265 and perplexity is 196.17514260189367
At time: 208.1104793548584 and batch: 350, loss is 5.341473064422607 and perplexity is 208.8200892832326
At time: 208.5187749862671 and batch: 400, loss is 5.296090469360352 and perplexity is 199.55511610166496
At time: 208.93839645385742 and batch: 450, loss is 5.285007200241089 and perplexity is 197.35560445097371
At time: 209.35944771766663 and batch: 500, loss is 5.268727655410767 and perplexity is 194.16875564698046
At time: 209.77389812469482 and batch: 550, loss is 5.33590425491333 and perplexity is 207.66044190884583
At time: 210.18638443946838 and batch: 600, loss is 5.366939287185669 and perplexity is 214.20623955952306
At time: 210.60696601867676 and batch: 650, loss is 5.340554456710816 and perplexity is 208.62835361725357
At time: 211.02622723579407 and batch: 700, loss is 5.319438552856445 and perplexity is 204.26916345344603
At time: 211.4485845565796 and batch: 750, loss is 5.254452495574951 and perplexity is 191.41665569348712
At time: 211.85722756385803 and batch: 800, loss is 5.297844514846802 and perplexity is 199.9054520151025
At time: 212.27169561386108 and batch: 850, loss is 5.267083168029785 and perplexity is 193.84970998372233
At time: 212.6907410621643 and batch: 900, loss is 5.343799905776978 and perplexity is 209.306546237198
At time: 213.1028664112091 and batch: 950, loss is 5.326781044006347 and perplexity is 205.7745277772974
At time: 213.52985620498657 and batch: 1000, loss is 5.28087477684021 and perplexity is 196.5417303272881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.441376476753049 and perplexity of 230.75960017734607
Finished 23 epochs...
Completing Train Step...
At time: 214.91436457633972 and batch: 50, loss is 5.374609670639038 and perplexity is 215.85560108608547
At time: 215.34837317466736 and batch: 100, loss is 5.366927108764648 and perplexity is 214.2036308816372
At time: 215.77036786079407 and batch: 150, loss is 5.376862907409668 and perplexity is 216.3425232330304
At time: 216.2008535861969 and batch: 200, loss is 5.373488359451294 and perplexity is 215.61369543675227
At time: 216.61596655845642 and batch: 250, loss is 5.374204902648926 and perplexity is 215.76824732847453
At time: 217.0270938873291 and batch: 300, loss is 5.26919885635376 and perplexity is 194.2602697068035
At time: 217.4423005580902 and batch: 350, loss is 5.3332936668396 and perplexity is 207.1190330410625
At time: 217.8672637939453 and batch: 400, loss is 5.289467697143555 and perplexity is 198.2378747324343
At time: 218.29225611686707 and batch: 450, loss is 5.2786108493804935 and perplexity is 196.0972774014437
At time: 218.70738983154297 and batch: 500, loss is 5.264259281158448 and perplexity is 193.3030725171253
At time: 219.1311707496643 and batch: 550, loss is 5.329011306762696 and perplexity is 206.2339711921306
At time: 219.5450143814087 and batch: 600, loss is 5.363385190963745 and perplexity is 213.44628125521834
At time: 220.00040316581726 and batch: 650, loss is 5.338074836730957 and perplexity is 208.11167543077
At time: 220.41925764083862 and batch: 700, loss is 5.315932874679565 and perplexity is 203.55431525089998
At time: 220.83697962760925 and batch: 750, loss is 5.250971622467041 and perplexity is 190.75151690772853
At time: 221.26070427894592 and batch: 800, loss is 5.295316600799561 and perplexity is 199.4007464097985
At time: 221.68428254127502 and batch: 850, loss is 5.26713080406189 and perplexity is 193.85894443467515
At time: 222.10255241394043 and batch: 900, loss is 5.344814739227295 and perplexity is 209.51906533919745
At time: 222.5371162891388 and batch: 950, loss is 5.3271110725402835 and perplexity is 205.8424504506148
At time: 222.95834946632385 and batch: 1000, loss is 5.280708065032959 and perplexity is 196.5089672312984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.441070928806212 and perplexity of 230.68910282600805
Finished 24 epochs...
Completing Train Step...
At time: 224.24493312835693 and batch: 50, loss is 5.369854316711426 and perplexity is 214.8315680551491
At time: 224.69424891471863 and batch: 100, loss is 5.362891969680786 and perplexity is 213.34103096451005
At time: 225.10015058517456 and batch: 150, loss is 5.372645854949951 and perplexity is 215.43211642910197
At time: 225.51429796218872 and batch: 200, loss is 5.370007095336914 and perplexity is 214.8643922341805
At time: 225.92243337631226 and batch: 250, loss is 5.3708225440979005 and perplexity is 215.03967459378663
At time: 226.337584733963 and batch: 300, loss is 5.265363130569458 and perplexity is 193.51656781151894
At time: 226.76458859443665 and batch: 350, loss is 5.329476404190063 and perplexity is 206.32991239084075
At time: 227.18832182884216 and batch: 400, loss is 5.2860210990905765 and perplexity is 197.55580454543892
At time: 227.60109758377075 and batch: 450, loss is 5.277700204849243 and perplexity is 195.9187837726434
At time: 228.01188397407532 and batch: 500, loss is 5.263485412597657 and perplexity is 193.1535392136238
At time: 228.4405837059021 and batch: 550, loss is 5.328075485229492 and perplexity is 206.04106327877903
At time: 228.85729336738586 and batch: 600, loss is 5.363059701919556 and perplexity is 213.37681813450249
At time: 229.28572058677673 and batch: 650, loss is 5.334316997528076 and perplexity is 207.331092788859
At time: 229.71166276931763 and batch: 700, loss is 5.314320411682129 and perplexity is 203.22635593178157
At time: 230.1381893157959 and batch: 750, loss is 5.250699729919433 and perplexity is 190.69966004190354
At time: 230.55378985404968 and batch: 800, loss is 5.294995222091675 and perplexity is 199.33667355194345
At time: 230.99707126617432 and batch: 850, loss is 5.268022041320801 and perplexity is 194.03179576325934
At time: 231.4049744606018 and batch: 900, loss is 5.345177154541016 and perplexity is 209.59501201828144
At time: 231.81940484046936 and batch: 950, loss is 5.3274524974823 and perplexity is 205.91274219631916
At time: 232.24288606643677 and batch: 1000, loss is 5.280386562347412 and perplexity is 196.44579922548536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.438577047208461 and perplexity of 230.1145083008296
Finished 25 epochs...
Completing Train Step...
At time: 233.5057406425476 and batch: 50, loss is 5.366266508102417 and perplexity is 214.0621745494419
At time: 233.92693042755127 and batch: 100, loss is 5.35909026145935 and perplexity is 212.53151036867396
At time: 234.33295369148254 and batch: 150, loss is 5.369272232055664 and perplexity is 214.70655428363975
At time: 234.75438570976257 and batch: 200, loss is 5.366376581192017 and perplexity is 214.0857383312065
At time: 235.18019676208496 and batch: 250, loss is 5.367981472015381 and perplexity is 214.42959842321446
At time: 235.59213161468506 and batch: 300, loss is 5.262424058914185 and perplexity is 192.9486437458176
At time: 236.0100769996643 and batch: 350, loss is 5.326038484573364 and perplexity is 205.62178467806174
At time: 236.4212760925293 and batch: 400, loss is 5.282973299026489 and perplexity is 196.95461057657133
At time: 236.83497619628906 and batch: 450, loss is 5.275764570236206 and perplexity is 195.53992337936353
At time: 237.2541539669037 and batch: 500, loss is 5.26206114768982 and perplexity is 192.87863322184705
At time: 237.66394639015198 and batch: 550, loss is 5.325461483001709 and perplexity is 205.5031748074675
At time: 238.06527972221375 and batch: 600, loss is 5.3605101108551025 and perplexity is 212.8334874354957
At time: 238.4757261276245 and batch: 650, loss is 5.334714326858521 and perplexity is 207.4134878810465
At time: 238.87989354133606 and batch: 700, loss is 5.313227643966675 and perplexity is 203.0043980273935
At time: 239.2840552330017 and batch: 750, loss is 5.249464874267578 and perplexity is 190.46431882501517
At time: 239.70077633857727 and batch: 800, loss is 5.294291696548462 and perplexity is 199.19648442949696
At time: 240.10675573349 and batch: 850, loss is 5.266682224273682 and perplexity is 193.77200273204159
At time: 240.5285189151764 and batch: 900, loss is 5.344079322814942 and perplexity is 209.36503822382238
At time: 240.94661355018616 and batch: 950, loss is 5.326795873641967 and perplexity is 205.7775793611911
At time: 241.38978672027588 and batch: 1000, loss is 5.279177207946777 and perplexity is 196.20837023054304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.438976380883194 and perplexity of 230.20641912336063
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 242.68689846992493 and batch: 50, loss is 5.3615483474731445 and perplexity is 213.05457370576784
At time: 243.10800004005432 and batch: 100, loss is 5.348139772415161 and perplexity is 210.21688267492786
At time: 243.53854537010193 and batch: 150, loss is 5.356810092926025 and perplexity is 212.04745478027567
At time: 243.97755527496338 and batch: 200, loss is 5.3527203464508055 and perplexity is 211.18200539009894
At time: 244.40063762664795 and batch: 250, loss is 5.348228950500488 and perplexity is 210.2356302499522
At time: 244.82048559188843 and batch: 300, loss is 5.244339027404785 and perplexity is 189.49052578304037
At time: 245.2363314628601 and batch: 350, loss is 5.308183603286743 and perplexity is 201.9830137031125
At time: 245.64635515213013 and batch: 400, loss is 5.262462520599366 and perplexity is 192.9560650185258
At time: 246.06247925758362 and batch: 450, loss is 5.2523422527313235 and perplexity is 191.01314596715483
At time: 246.46870827674866 and batch: 500, loss is 5.232825832366943 and perplexity is 187.32139517422183
At time: 246.89137864112854 and batch: 550, loss is 5.298162622451782 and perplexity is 199.96905357519913
At time: 247.3154227733612 and batch: 600, loss is 5.33605224609375 and perplexity is 207.69117609690886
At time: 247.74775218963623 and batch: 650, loss is 5.304956102371216 and perplexity is 201.33216421510267
At time: 248.16970705986023 and batch: 700, loss is 5.285105028152466 and perplexity is 197.3749122819627
At time: 248.5954749584198 and batch: 750, loss is 5.218719148635865 and perplexity is 184.69746250367564
At time: 249.02478218078613 and batch: 800, loss is 5.259262628555298 and perplexity is 192.33961325676572
At time: 249.43976640701294 and batch: 850, loss is 5.231068153381347 and perplexity is 186.99243348358902
At time: 249.86099791526794 and batch: 900, loss is 5.303976955413819 and perplexity is 201.13512691906178
At time: 250.28676676750183 and batch: 950, loss is 5.289434032440186 and perplexity is 198.23120122551637
At time: 250.73116207122803 and batch: 1000, loss is 5.242710981369019 and perplexity is 189.1822774730288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.413606969321647 and perplexity of 224.4396764562544
Finished 27 epochs...
Completing Train Step...
At time: 252.00392508506775 and batch: 50, loss is 5.345109977722168 and perplexity is 209.58093256503935
At time: 252.44709491729736 and batch: 100, loss is 5.337899522781372 and perplexity is 208.07519374896276
At time: 252.85905265808105 and batch: 150, loss is 5.348504304885864 and perplexity is 210.29352752347216
At time: 253.27700996398926 and batch: 200, loss is 5.344457368850708 and perplexity is 209.4442028095371
At time: 253.69577479362488 and batch: 250, loss is 5.341596269607544 and perplexity is 208.84581858591045
At time: 254.11078572273254 and batch: 300, loss is 5.238564167022705 and perplexity is 188.39939804043703
At time: 254.51871514320374 and batch: 350, loss is 5.302126445770264 and perplexity is 200.76326859686156
At time: 254.9289116859436 and batch: 400, loss is 5.258099174499511 and perplexity is 192.11596508105023
At time: 255.35338234901428 and batch: 450, loss is 5.248262872695923 and perplexity is 190.23551795149586
At time: 255.78595113754272 and batch: 500, loss is 5.2291033077239994 and perplexity is 186.62538292963222
At time: 256.22716522216797 and batch: 550, loss is 5.295339727401734 and perplexity is 199.4053579248578
At time: 256.65195965766907 and batch: 600, loss is 5.334788866043091 and perplexity is 207.4289488895204
At time: 257.09219193458557 and batch: 650, loss is 5.304124956130981 and perplexity is 201.16489726505432
At time: 257.505170583725 and batch: 700, loss is 5.28451826095581 and perplexity is 197.2591331290192
At time: 257.9293329715729 and batch: 750, loss is 5.218862686157227 and perplexity is 184.7239754223991
At time: 258.35098218917847 and batch: 800, loss is 5.259723768234253 and perplexity is 192.42832913790767
At time: 258.7817449569702 and batch: 850, loss is 5.232190170288086 and perplexity is 187.20235990386277
At time: 259.1925621032715 and batch: 900, loss is 5.3063345146179195 and perplexity is 201.60987429142594
At time: 259.61258149147034 and batch: 950, loss is 5.29176923751831 and perplexity is 198.6946526497135
At time: 260.0314860343933 and batch: 1000, loss is 5.2431903648376466 and perplexity is 189.27299007072938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.411782985780297 and perplexity of 224.03067529945426
Finished 28 epochs...
Completing Train Step...
At time: 261.308974981308 and batch: 50, loss is 5.34233320236206 and perplexity is 208.99978063314026
At time: 261.7433395385742 and batch: 100, loss is 5.334732866287231 and perplexity is 207.41733324426397
At time: 262.17835664749146 and batch: 150, loss is 5.3454358196258545 and perplexity is 209.6492339422038
At time: 262.60198616981506 and batch: 200, loss is 5.341135530471802 and perplexity is 208.74961730750053
At time: 263.0247757434845 and batch: 250, loss is 5.33839864730835 and perplexity is 208.17907510432806
At time: 263.4858875274658 and batch: 300, loss is 5.235350894927978 and perplexity is 187.79499109408528
At time: 263.9057755470276 and batch: 350, loss is 5.298974657058716 and perplexity is 200.13150131468467
At time: 264.34227323532104 and batch: 400, loss is 5.255822725296021 and perplexity is 191.67912026153655
At time: 264.75906014442444 and batch: 450, loss is 5.245949754714966 and perplexity is 189.79598929110463
At time: 265.176931142807 and batch: 500, loss is 5.226650667190552 and perplexity is 186.16821880968482
At time: 265.6142544746399 and batch: 550, loss is 5.293684167861938 and perplexity is 199.07550360433342
At time: 266.04377007484436 and batch: 600, loss is 5.333855419158936 and perplexity is 207.23541532420015
At time: 266.4526481628418 and batch: 650, loss is 5.3040594959259035 and perplexity is 201.15172940061515
At time: 266.86143255233765 and batch: 700, loss is 5.284468421936035 and perplexity is 197.249302172167
At time: 267.30026054382324 and batch: 750, loss is 5.218878879547119 and perplexity is 184.72696675397535
At time: 267.72066020965576 and batch: 800, loss is 5.259764022827149 and perplexity is 192.4360754178693
At time: 268.1413187980652 and batch: 850, loss is 5.232066745758057 and perplexity is 187.17925596639694
At time: 268.5607855319977 and batch: 900, loss is 5.307223577499389 and perplexity is 201.7891978503443
At time: 268.9888746738434 and batch: 950, loss is 5.292247133255005 and perplexity is 198.7896306701067
At time: 269.40218353271484 and batch: 1000, loss is 5.2431049633026126 and perplexity is 189.2568265570411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.410855921303353 and perplexity of 223.82308066031428
Finished 29 epochs...
Completing Train Step...
At time: 270.7104845046997 and batch: 50, loss is 5.340748748779297 and perplexity is 208.66889238967508
At time: 271.12915992736816 and batch: 100, loss is 5.333103113174438 and perplexity is 207.0795695102714
At time: 271.5421793460846 and batch: 150, loss is 5.3435806465148925 and perplexity is 209.26065886911954
At time: 271.96840596199036 and batch: 200, loss is 5.339187068939209 and perplexity is 208.3432727101991
At time: 272.3826382160187 and batch: 250, loss is 5.336627597808838 and perplexity is 207.81070595384156
At time: 272.8029360771179 and batch: 300, loss is 5.2338690662384035 and perplexity is 187.51691716836737
At time: 273.21581983566284 and batch: 350, loss is 5.2973506641387935 and perplexity is 199.80675293940024
At time: 273.6231338977814 and batch: 400, loss is 5.254554929733277 and perplexity is 191.43626430178102
At time: 274.05788373947144 and batch: 450, loss is 5.244632711410523 and perplexity is 189.54618429230857
At time: 274.47812938690186 and batch: 500, loss is 5.225255393981934 and perplexity is 185.90864441239597
At time: 274.8878970146179 and batch: 550, loss is 5.292850522994995 and perplexity is 198.90961448853173
At time: 275.31086707115173 and batch: 600, loss is 5.333826875686645 and perplexity is 207.22950019028482
At time: 275.72201704978943 and batch: 650, loss is 5.304204607009888 and perplexity is 201.18092086406506
At time: 276.1436972618103 and batch: 700, loss is 5.284655561447144 and perplexity is 197.28621876431083
At time: 276.5558795928955 and batch: 750, loss is 5.219253015518189 and perplexity is 184.79609268750744
At time: 276.9710690975189 and batch: 800, loss is 5.260173072814942 and perplexity is 192.51480749374943
At time: 277.38995027542114 and batch: 850, loss is 5.232369899749756 and perplexity is 187.2360087069817
At time: 277.81092715263367 and batch: 900, loss is 5.307980661392212 and perplexity is 201.94202704674402
At time: 278.2340290546417 and batch: 950, loss is 5.292595558166504 and perplexity is 198.85890599750388
At time: 278.65329217910767 and batch: 1000, loss is 5.242956876754761 and perplexity is 189.22880224200216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.410279808974847 and perplexity of 223.69417056105587
Finished 30 epochs...
Completing Train Step...
At time: 279.9564964771271 and batch: 50, loss is 5.3397822761535645 and perplexity is 208.467317041557
At time: 280.37590408325195 and batch: 100, loss is 5.331924381256104 and perplexity is 206.83562201464972
At time: 280.7866117954254 and batch: 150, loss is 5.342363424301148 and perplexity is 209.0060971072274
At time: 281.1957790851593 and batch: 200, loss is 5.337775287628173 and perplexity is 208.04934510107913
At time: 281.61349153518677 and batch: 250, loss is 5.33539155960083 and perplexity is 207.55400266147066
At time: 282.0364294052124 and batch: 300, loss is 5.2327930355072025 and perplexity is 187.31525172144137
At time: 282.4507055282593 and batch: 350, loss is 5.296217517852783 and perplexity is 199.5804708889323
At time: 282.8835060596466 and batch: 400, loss is 5.253592271804809 and perplexity is 191.25206533867572
At time: 283.29753589630127 and batch: 450, loss is 5.243710966110229 and perplexity is 189.37155148359815
At time: 283.72992992401123 and batch: 500, loss is 5.2242225170135494 and perplexity is 185.71672278813395
At time: 284.1503553390503 and batch: 550, loss is 5.2923188972473145 and perplexity is 198.80389711953592
At time: 284.5703058242798 and batch: 600, loss is 5.333878965377807 and perplexity is 207.24029499209595
At time: 285.01487612724304 and batch: 650, loss is 5.304326791763305 and perplexity is 201.2055036070607
At time: 285.4383735656738 and batch: 700, loss is 5.284769268035888 and perplexity is 197.30865278267652
At time: 285.862428188324 and batch: 750, loss is 5.219496479034424 and perplexity is 184.8410892713107
At time: 286.2867147922516 and batch: 800, loss is 5.260362691879273 and perplexity is 192.55131543260748
At time: 286.69175267219543 and batch: 850, loss is 5.23252197265625 and perplexity is 187.26448439616166
At time: 287.1027657985687 and batch: 900, loss is 5.308447790145874 and perplexity is 202.03638201039294
At time: 287.5104410648346 and batch: 950, loss is 5.292767429351807 and perplexity is 198.89308705067072
At time: 287.91857647895813 and batch: 1000, loss is 5.242652416229248 and perplexity is 189.17119831093595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.40988456912157 and perplexity of 223.6057751797439
Finished 31 epochs...
Completing Train Step...
At time: 289.2269215583801 and batch: 50, loss is 5.339059476852417 and perplexity is 208.31669145308
At time: 289.65492844581604 and batch: 100, loss is 5.330892753601074 and perplexity is 206.62235469208235
At time: 290.08076310157776 and batch: 150, loss is 5.341361684799194 and perplexity is 208.79683227552954
At time: 290.50322365760803 and batch: 200, loss is 5.336663980484008 and perplexity is 207.81826680079428
At time: 290.9301929473877 and batch: 250, loss is 5.334364376068115 and perplexity is 207.34091606604443
At time: 291.34649181365967 and batch: 300, loss is 5.231907949447632 and perplexity is 187.1495349510177
At time: 291.7645447254181 and batch: 350, loss is 5.295357904434204 and perplexity is 199.408982555466
At time: 292.1892354488373 and batch: 400, loss is 5.252726497650147 and perplexity is 191.08655590071493
At time: 292.60656547546387 and batch: 450, loss is 5.242956771850586 and perplexity is 189.2287823911118
At time: 293.0273931026459 and batch: 500, loss is 5.223253650665283 and perplexity is 185.53687524329982
At time: 293.4399571418762 and batch: 550, loss is 5.291881504058838 and perplexity is 198.7169606631862
At time: 293.86717081069946 and batch: 600, loss is 5.3339532852172855 and perplexity is 207.25569762990685
At time: 294.27605509757996 and batch: 650, loss is 5.304295787811279 and perplexity is 201.19926553798237
At time: 294.6897644996643 and batch: 700, loss is 5.28487756729126 and perplexity is 197.33002231998287
At time: 295.0967094898224 and batch: 750, loss is 5.219628601074219 and perplexity is 184.8655124664488
At time: 295.5391254425049 and batch: 800, loss is 5.260529661178589 and perplexity is 192.58346827502183
At time: 295.95148849487305 and batch: 850, loss is 5.2325863265991215 and perplexity is 187.27653599187204
At time: 296.387864112854 and batch: 900, loss is 5.308793468475342 and perplexity is 202.10623368182695
At time: 296.8058681488037 and batch: 950, loss is 5.292879686355591 and perplexity is 198.91541544593233
At time: 297.22540807724 and batch: 1000, loss is 5.24238697052002 and perplexity is 189.1209902920811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4095983737852515 and perplexity of 223.54178940636524
Finished 32 epochs...
Completing Train Step...
At time: 298.56721353530884 and batch: 50, loss is 5.338510608673095 and perplexity is 208.2023844225354
At time: 298.9900805950165 and batch: 100, loss is 5.330156211853027 and perplexity is 206.47022473368202
At time: 299.40377950668335 and batch: 150, loss is 5.340594425201416 and perplexity is 208.63669234428602
At time: 299.8150246143341 and batch: 200, loss is 5.335862627029419 and perplexity is 207.6517976240001
At time: 300.24429988861084 and batch: 250, loss is 5.3335662460327145 and perplexity is 207.1754970750783
At time: 300.66293239593506 and batch: 300, loss is 5.231224451065064 and perplexity is 187.0216622519499
At time: 301.0832853317261 and batch: 350, loss is 5.294657258987427 and perplexity is 199.26931649370073
At time: 301.50319623947144 and batch: 400, loss is 5.252009124755859 and perplexity is 190.949524742145
At time: 301.9324493408203 and batch: 450, loss is 5.242318477630615 and perplexity is 189.1080372926082
At time: 302.35423040390015 and batch: 500, loss is 5.222548418045044 and perplexity is 185.40607471444196
At time: 302.7874982357025 and batch: 550, loss is 5.291524229049682 and perplexity is 198.64597674039172
At time: 303.2217609882355 and batch: 600, loss is 5.334030599594116 and perplexity is 207.27172209446644
At time: 303.64743518829346 and batch: 650, loss is 5.304380464553833 and perplexity is 201.21630315772722
At time: 304.0786256790161 and batch: 700, loss is 5.2850589847564695 and perplexity is 197.3658246799304
At time: 304.502641916275 and batch: 750, loss is 5.219755191802978 and perplexity is 184.88891620771142
At time: 304.92346358299255 and batch: 800, loss is 5.260652561187744 and perplexity is 192.60713823952545
At time: 305.33322834968567 and batch: 850, loss is 5.232650642395019 and perplexity is 187.2885812186824
At time: 305.76634407043457 and batch: 900, loss is 5.309039011001587 and perplexity is 202.15586545012053
At time: 306.17937660217285 and batch: 950, loss is 5.292897453308106 and perplexity is 198.91894959806842
At time: 306.62247490882874 and batch: 1000, loss is 5.242144250869751 and perplexity is 189.0750924818346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.409375074433117 and perplexity of 223.49187824238854
Finished 33 epochs...
Completing Train Step...
At time: 307.91362738609314 and batch: 50, loss is 5.338056564331055 and perplexity is 208.10787276575417
At time: 308.3458261489868 and batch: 100, loss is 5.329535980224609 and perplexity is 206.3422050750002
At time: 308.7529923915863 and batch: 150, loss is 5.339935312271118 and perplexity is 208.4992225116762
At time: 309.177316904068 and batch: 200, loss is 5.33520094871521 and perplexity is 207.5144443794487
At time: 309.58525252342224 and batch: 250, loss is 5.332897415161133 and perplexity is 207.03697803486816
At time: 310.0066065788269 and batch: 300, loss is 5.2306501579284665 and perplexity is 186.91428783007152
At time: 310.4259567260742 and batch: 350, loss is 5.2940766143798825 and perplexity is 199.15364542477067
At time: 310.83920645713806 and batch: 400, loss is 5.251381549835205 and perplexity is 190.8297272042031
At time: 311.25623321533203 and batch: 450, loss is 5.241746072769165 and perplexity is 188.99982190718367
At time: 311.6779189109802 and batch: 500, loss is 5.221926412582397 and perplexity is 185.29078698167854
At time: 312.1105041503906 and batch: 550, loss is 5.291180286407471 and perplexity is 198.577665666506
At time: 312.53557109832764 and batch: 600, loss is 5.334098863601684 and perplexity is 207.2858717758237
At time: 312.9605357646942 and batch: 650, loss is 5.304453678131104 and perplexity is 201.2310354623824
At time: 313.3884394168854 and batch: 700, loss is 5.285210304260254 and perplexity is 197.39569223829997
At time: 313.81407833099365 and batch: 750, loss is 5.219842500686646 and perplexity is 184.90505935729797
At time: 314.23221492767334 and batch: 800, loss is 5.26070930480957 and perplexity is 192.61806777622655
At time: 314.6579611301422 and batch: 850, loss is 5.232672204971314 and perplexity is 187.2926196865437
At time: 315.08324432373047 and batch: 900, loss is 5.30921275138855 and perplexity is 202.19099113969787
At time: 315.50280117988586 and batch: 950, loss is 5.292864389419556 and perplexity is 198.91237267281846
At time: 315.93787717819214 and batch: 1000, loss is 5.241901960372925 and perplexity is 189.02928693308826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.409177082340892 and perplexity of 223.44763299806877
Finished 34 epochs...
Completing Train Step...
At time: 317.21793508529663 and batch: 50, loss is 5.3376673889160156 and perplexity is 208.02689805570284
At time: 317.6601610183716 and batch: 100, loss is 5.328981647491455 and perplexity is 206.22785453354822
At time: 318.0810582637787 and batch: 150, loss is 5.339347667694092 and perplexity is 208.37673506731917
At time: 318.49184703826904 and batch: 200, loss is 5.334622278213501 and perplexity is 207.39439662920358
At time: 318.91433095932007 and batch: 250, loss is 5.332322788238526 and perplexity is 206.91804318816918
At time: 319.3468267917633 and batch: 300, loss is 5.230155839920044 and perplexity is 186.82191556408378
At time: 319.7548944950104 and batch: 350, loss is 5.293573932647705 and perplexity is 199.05355968306364
At time: 320.16024947166443 and batch: 400, loss is 5.250829963684082 and perplexity is 190.72449719383087
At time: 320.5707380771637 and batch: 450, loss is 5.241228075027466 and perplexity is 188.90194577824957
At time: 320.98060393333435 and batch: 500, loss is 5.221368398666382 and perplexity is 185.1874209865473
At time: 321.40225172042847 and batch: 550, loss is 5.2908604717254635 and perplexity is 198.51416776782844
At time: 321.8185329437256 and batch: 600, loss is 5.334145803451538 and perplexity is 207.29560197188675
At time: 322.23309803009033 and batch: 650, loss is 5.304510402679443 and perplexity is 201.242450525735
At time: 322.6478774547577 and batch: 700, loss is 5.2853328895568845 and perplexity is 197.4198915309951
At time: 323.0551836490631 and batch: 750, loss is 5.219908123016357 and perplexity is 184.91719365620457
At time: 323.4699101448059 and batch: 800, loss is 5.260740461349488 and perplexity is 192.62406918223516
At time: 323.8756585121155 and batch: 850, loss is 5.232660503387451 and perplexity is 187.2904280790702
At time: 324.3070561885834 and batch: 900, loss is 5.309333190917969 and perplexity is 202.2153443940413
At time: 324.7358410358429 and batch: 950, loss is 5.292790393829346 and perplexity is 198.89765457894623
At time: 325.1744246482849 and batch: 1000, loss is 5.241665420532226 and perplexity is 188.98457926345074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408989138719512 and perplexity of 223.40564138688575
Finished 35 epochs...
Completing Train Step...
At time: 326.5087492465973 and batch: 50, loss is 5.337313394546509 and perplexity is 207.95327073768215
At time: 326.9505467414856 and batch: 100, loss is 5.328485479354859 and perplexity is 206.12555622392958
At time: 327.37172532081604 and batch: 150, loss is 5.338823328018188 and perplexity is 208.26750351730846
At time: 327.788369178772 and batch: 200, loss is 5.334102878570556 and perplexity is 207.2867040238172
At time: 328.21488761901855 and batch: 250, loss is 5.331814622879028 and perplexity is 206.81292131827448
At time: 328.64008808135986 and batch: 300, loss is 5.229714403152466 and perplexity is 186.7394637015452
At time: 329.0665006637573 and batch: 350, loss is 5.293144683837891 and perplexity is 198.96813451511792
At time: 329.4961588382721 and batch: 400, loss is 5.250334053039551 and perplexity is 190.6299383338101
At time: 329.904123544693 and batch: 450, loss is 5.240765991210938 and perplexity is 188.8146774103979
At time: 330.3127689361572 and batch: 500, loss is 5.220868110656738 and perplexity is 185.09479711152986
At time: 330.7398045063019 and batch: 550, loss is 5.290565900802612 and perplexity is 198.45569987812289
At time: 331.16101932525635 and batch: 600, loss is 5.3341782283782955 and perplexity is 207.3023236255719
At time: 331.58523440361023 and batch: 650, loss is 5.304549608230591 and perplexity is 201.2503405015866
At time: 332.0034701824188 and batch: 700, loss is 5.285438270568847 and perplexity is 197.44069693517426
At time: 332.42199993133545 and batch: 750, loss is 5.21995364189148 and perplexity is 184.92561107042476
At time: 332.83063101768494 and batch: 800, loss is 5.260736532211304 and perplexity is 192.62331233713672
At time: 333.2435350418091 and batch: 850, loss is 5.232618179321289 and perplexity is 187.2825013543475
At time: 333.66903352737427 and batch: 900, loss is 5.309406833648682 and perplexity is 202.2302366325403
At time: 334.0952513217926 and batch: 950, loss is 5.292691307067871 and perplexity is 198.87794743086388
At time: 334.5193247795105 and batch: 1000, loss is 5.241440391540527 and perplexity is 188.9420570386785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408810499237805 and perplexity of 223.36573588335318
Finished 36 epochs...
Completing Train Step...
At time: 335.8967592716217 and batch: 50, loss is 5.336993064880371 and perplexity is 207.88666780391165
At time: 336.3239824771881 and batch: 100, loss is 5.328030042648315 and perplexity is 206.03170045377223
At time: 336.7279794216156 and batch: 150, loss is 5.3383469390869145 and perplexity is 208.16831081291795
At time: 337.15061044692993 and batch: 200, loss is 5.333634557723999 and perplexity is 207.18965006707822
At time: 337.56656289100647 and batch: 250, loss is 5.331364984512329 and perplexity is 206.7199511971538
At time: 338.00278973579407 and batch: 300, loss is 5.2293085193634035 and perplexity is 186.66368456025342
At time: 338.4281919002533 and batch: 350, loss is 5.292766084671021 and perplexity is 198.89281960313784
At time: 338.86087012290955 and batch: 400, loss is 5.249889717102051 and perplexity is 190.5452534171142
At time: 339.30775237083435 and batch: 450, loss is 5.240345945358277 and perplexity is 188.73538324298903
At time: 339.73630261421204 and batch: 500, loss is 5.220413742065429 and perplexity is 185.01071495290023
At time: 340.1525604724884 and batch: 550, loss is 5.290285367965698 and perplexity is 198.40003434600416
At time: 340.5774335861206 and batch: 600, loss is 5.334193925857544 and perplexity is 207.30557777503614
At time: 340.99333453178406 and batch: 650, loss is 5.30457501411438 and perplexity is 201.2554535092998
At time: 341.4071514606476 and batch: 700, loss is 5.2855332088470455 and perplexity is 197.4594425048095
At time: 341.83529448509216 and batch: 750, loss is 5.219984865188598 and perplexity is 184.93138514786642
At time: 342.2567386627197 and batch: 800, loss is 5.260738687515259 and perplexity is 192.623727499371
At time: 342.67636370658875 and batch: 850, loss is 5.232570705413818 and perplexity is 187.27361053324992
At time: 343.0825572013855 and batch: 900, loss is 5.309441995620728 and perplexity is 202.2373475714842
At time: 343.5145649909973 and batch: 950, loss is 5.292568092346191 and perplexity is 198.85344424953027
At time: 343.9321541786194 and batch: 1000, loss is 5.24122917175293 and perplexity is 188.90215295193735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408651212366616 and perplexity of 223.3301594876551
Finished 37 epochs...
Completing Train Step...
At time: 345.22840309143066 and batch: 50, loss is 5.33670015335083 and perplexity is 207.8257843192467
At time: 345.6627025604248 and batch: 100, loss is 5.327615022659302 and perplexity is 205.94621092087309
At time: 346.08189845085144 and batch: 150, loss is 5.337912731170654 and perplexity is 208.07794210527234
At time: 346.50354266166687 and batch: 200, loss is 5.333209085464477 and perplexity is 207.10151536927867
At time: 346.9320275783539 and batch: 250, loss is 5.330947561264038 and perplexity is 206.6336794907991
At time: 347.35322999954224 and batch: 300, loss is 5.228940105438232 and perplexity is 186.59492772580293
At time: 347.7720055580139 and batch: 350, loss is 5.292431030273438 and perplexity is 198.82619085203348
At time: 348.18653559684753 and batch: 400, loss is 5.249471435546875 and perplexity is 190.46556851870685
At time: 348.62039613723755 and batch: 450, loss is 5.239962701797485 and perplexity is 188.66306548121108
At time: 349.05344891548157 and batch: 500, loss is 5.219992275238037 and perplexity is 184.93275550365038
At time: 349.47582507133484 and batch: 550, loss is 5.290013418197632 and perplexity is 198.34608683851812
At time: 349.9141800403595 and batch: 600, loss is 5.334185762405395 and perplexity is 207.30388545277938
At time: 350.3339443206787 and batch: 650, loss is 5.3045901107788085 and perplexity is 201.25849181828
At time: 350.76190853118896 and batch: 700, loss is 5.285605268478394 and perplexity is 197.4736718721179
At time: 351.1732349395752 and batch: 750, loss is 5.2200045394897465 and perplexity is 184.93502357942128
At time: 351.59112787246704 and batch: 800, loss is 5.260704860687256 and perplexity is 192.61721175987566
At time: 352.00929856300354 and batch: 850, loss is 5.232519445419311 and perplexity is 187.26401113503738
At time: 352.4361605644226 and batch: 900, loss is 5.309441785812378 and perplexity is 202.2373051404046
At time: 352.8512420654297 and batch: 950, loss is 5.292436170578003 and perplexity is 198.82721288183674
At time: 353.28999519348145 and batch: 1000, loss is 5.241027059555054 and perplexity is 188.86397738062504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408504579125381 and perplexity of 223.29741426333186
Finished 38 epochs...
Completing Train Step...
At time: 354.63046956062317 and batch: 50, loss is 5.336427345275879 and perplexity is 207.76909550003666
At time: 355.0601441860199 and batch: 100, loss is 5.327226705551148 and perplexity is 205.8662540091361
At time: 355.48752665519714 and batch: 150, loss is 5.337493772506714 and perplexity is 207.9907843076846
At time: 355.9186828136444 and batch: 200, loss is 5.332816371917724 and perplexity is 207.02019976655453
At time: 356.35167145729065 and batch: 250, loss is 5.330553598403931 and perplexity is 206.55228952879625
At time: 356.76726365089417 and batch: 300, loss is 5.228602561950684 and perplexity is 186.5319544518467
At time: 357.1929693222046 and batch: 350, loss is 5.292116746902466 and perplexity is 198.76371290493924
At time: 357.60981583595276 and batch: 400, loss is 5.249056415557861 and perplexity is 190.38653790133014
At time: 358.02738213539124 and batch: 450, loss is 5.2396041202545165 and perplexity is 188.59542651585664
At time: 358.43373823165894 and batch: 500, loss is 5.21959846496582 and perplexity is 184.85994142327107
At time: 358.84239530563354 and batch: 550, loss is 5.289735193252564 and perplexity is 198.290909685589
At time: 359.2643418312073 and batch: 600, loss is 5.334144201278686 and perplexity is 207.29526984876713
At time: 359.7010178565979 and batch: 650, loss is 5.304595317840576 and perplexity is 201.25953978640655
At time: 360.11780309677124 and batch: 700, loss is 5.28564640045166 and perplexity is 197.48179452095934
At time: 360.5492994785309 and batch: 750, loss is 5.220013761520386 and perplexity is 184.93672906373905
At time: 360.9684634208679 and batch: 800, loss is 5.26068157196045 and perplexity is 192.61272600248674
At time: 361.3811066150665 and batch: 850, loss is 5.232448320388794 and perplexity is 187.25069245018224
At time: 361.79803252220154 and batch: 900, loss is 5.309408807754517 and perplexity is 202.23063585682456
At time: 362.211816072464 and batch: 950, loss is 5.292302646636963 and perplexity is 198.8006664611176
At time: 362.639107465744 and batch: 1000, loss is 5.240832204818726 and perplexity is 188.82717992530627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408351246903583 and perplexity of 223.26317819949392
Finished 39 epochs...
Completing Train Step...
At time: 363.96722745895386 and batch: 50, loss is 5.33616527557373 and perplexity is 207.7146526492858
At time: 364.41613125801086 and batch: 100, loss is 5.326860208511352 and perplexity is 205.7908184607449
At time: 364.8357849121094 and batch: 150, loss is 5.337064342498779 and perplexity is 207.90148599858813
At time: 365.2569580078125 and batch: 200, loss is 5.332454776763916 and perplexity is 206.94535579800208
At time: 365.67846870422363 and batch: 250, loss is 5.330180625915528 and perplexity is 206.4752655721866
At time: 366.0961151123047 and batch: 300, loss is 5.228293542861938 and perplexity is 186.47432142257182
At time: 366.5285255908966 and batch: 350, loss is 5.291821250915527 and perplexity is 198.7049877023849
At time: 366.94853687286377 and batch: 400, loss is 5.248655166625976 and perplexity is 190.31016083048573
At time: 367.3712236881256 and batch: 450, loss is 5.2392590045928955 and perplexity is 188.53035051047618
At time: 367.792551279068 and batch: 500, loss is 5.219216566085816 and perplexity is 184.78935709757923
At time: 368.21828961372375 and batch: 550, loss is 5.289476375579834 and perplexity is 198.2395951346636
At time: 368.6462678909302 and batch: 600, loss is 5.334097814559937 and perplexity is 207.28565432440456
At time: 369.07002425193787 and batch: 650, loss is 5.304585628509521 and perplexity is 201.25758972554502
At time: 369.4988567829132 and batch: 700, loss is 5.285651178359985 and perplexity is 197.4827380731235
At time: 369.9232506752014 and batch: 750, loss is 5.220013055801392 and perplexity is 184.93659855042264
At time: 370.3399965763092 and batch: 800, loss is 5.260636663436889 and perplexity is 192.60407624356844
At time: 370.7522795200348 and batch: 850, loss is 5.232371797561646 and perplexity is 187.2363640460424
At time: 371.15781354904175 and batch: 900, loss is 5.309358930587768 and perplexity is 202.22054941722183
At time: 371.5839216709137 and batch: 950, loss is 5.292173671722412 and perplexity is 198.7750278155547
At time: 371.9997069835663 and batch: 1000, loss is 5.240635757446289 and perplexity is 188.7900889652954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408216150795541 and perplexity of 223.23301825034187
Finished 40 epochs...
Completing Train Step...
At time: 373.28890585899353 and batch: 50, loss is 5.335933380126953 and perplexity is 207.66649015165524
At time: 373.72553157806396 and batch: 100, loss is 5.326525363922119 and perplexity is 205.72192205410775
At time: 374.15497493743896 and batch: 150, loss is 5.336693649291992 and perplexity is 207.8244326125132
At time: 374.58224415779114 and batch: 200, loss is 5.332110500335693 and perplexity is 206.8741216528928
At time: 375.00617480278015 and batch: 250, loss is 5.329850912094116 and perplexity is 206.40719904520182
At time: 375.4281072616577 and batch: 300, loss is 5.228004541397095 and perplexity is 186.42043785711468
At time: 375.84823417663574 and batch: 350, loss is 5.291535911560058 and perplexity is 198.64829743763187
At time: 376.2814404964447 and batch: 400, loss is 5.2482854270935055 and perplexity is 190.2398086473889
At time: 376.69284677505493 and batch: 450, loss is 5.238928680419922 and perplexity is 188.46808466288684
At time: 377.1093363761902 and batch: 500, loss is 5.2188434982299805 and perplexity is 184.72043098620335
At time: 377.5248055458069 and batch: 550, loss is 5.289272956848144 and perplexity is 198.19927358886844
At time: 377.93653678894043 and batch: 600, loss is 5.334067258834839 and perplexity is 207.2793206576997
At time: 378.3474826812744 and batch: 650, loss is 5.304542331695557 and perplexity is 201.2488761017612
At time: 378.7772352695465 and batch: 700, loss is 5.285632362365723 and perplexity is 197.47902227401536
At time: 379.2007873058319 and batch: 750, loss is 5.219980802536011 and perplexity is 184.93063383742216
At time: 379.6203570365906 and batch: 800, loss is 5.260575885772705 and perplexity is 192.59237057342725
At time: 380.0442740917206 and batch: 850, loss is 5.232298145294189 and perplexity is 187.22257417111413
At time: 380.4634597301483 and batch: 900, loss is 5.309304466247559 and perplexity is 202.20953590834552
At time: 380.894734621048 and batch: 950, loss is 5.292043790817261 and perplexity is 198.74921241152092
At time: 381.31323742866516 and batch: 1000, loss is 5.240427103042602 and perplexity is 188.75070119121958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.408097057807736 and perplexity of 223.20643434623148
Finished 41 epochs...
Completing Train Step...
At time: 382.63080883026123 and batch: 50, loss is 5.335724830627441 and perplexity is 207.6231859247632
At time: 383.07303047180176 and batch: 100, loss is 5.326210355758667 and perplexity is 205.65712817509575
At time: 383.49406147003174 and batch: 150, loss is 5.336374549865723 and perplexity is 207.7581265349801
At time: 383.9252452850342 and batch: 200, loss is 5.331769485473632 and perplexity is 206.80358653027952
At time: 384.34019136428833 and batch: 250, loss is 5.329557790756225 and perplexity is 206.34670555726655
At time: 384.7572042942047 and batch: 300, loss is 5.227719011306763 and perplexity is 186.3672168111203
At time: 385.1780433654785 and batch: 350, loss is 5.291235408782959 and perplexity is 198.58861204084872
At time: 385.59858083724976 and batch: 400, loss is 5.247948703765869 and perplexity is 190.17576124970378
At time: 386.0087945461273 and batch: 450, loss is 5.238621301651001 and perplexity is 188.41016247752341
At time: 386.4283232688904 and batch: 500, loss is 5.218505229949951 and perplexity is 184.6579564908931
At time: 386.854065656662 and batch: 550, loss is 5.289110336303711 and perplexity is 198.16704493568275
At time: 387.27909445762634 and batch: 600, loss is 5.33403247833252 and perplexity is 207.2721115041766
At time: 387.69662070274353 and batch: 650, loss is 5.304475908279419 and perplexity is 201.23550890786882
At time: 388.11160016059875 and batch: 700, loss is 5.285606527328492 and perplexity is 197.47392046202555
At time: 388.53655433654785 and batch: 750, loss is 5.219928874969482 and perplexity is 184.92103108895625
At time: 388.9519829750061 and batch: 800, loss is 5.260521354675293 and perplexity is 192.58186858645178
At time: 389.39388966560364 and batch: 850, loss is 5.2322282886505125 and perplexity is 187.20949588726984
At time: 389.8142454624176 and batch: 900, loss is 5.309249801635742 and perplexity is 202.1984825046772
At time: 390.2526607513428 and batch: 950, loss is 5.291902265548706 and perplexity is 198.7210863661793
At time: 390.68006896972656 and batch: 1000, loss is 5.240214500427246 and perplexity is 188.71057656394748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407974987495236 and perplexity of 223.17918912998869
Finished 42 epochs...
Completing Train Step...
At time: 391.96173071861267 and batch: 50, loss is 5.335519399642944 and perplexity is 207.5805380700196
At time: 392.3828570842743 and batch: 100, loss is 5.325915079116822 and perplexity is 205.59641139348156
At time: 392.7977685928345 and batch: 150, loss is 5.336078786849976 and perplexity is 207.69668845093454
At time: 393.2172634601593 and batch: 200, loss is 5.33144697189331 and perplexity is 206.73690031934657
At time: 393.62569069862366 and batch: 250, loss is 5.329299697875976 and perplexity is 206.29345581368483
At time: 394.04739212989807 and batch: 300, loss is 5.227436809539795 and perplexity is 186.31463107347435
At time: 394.4543933868408 and batch: 350, loss is 5.290967388153076 and perplexity is 198.535393328137
At time: 394.8879771232605 and batch: 400, loss is 5.2476358890533445 and perplexity is 190.11628077728784
At time: 395.30338287353516 and batch: 450, loss is 5.238354682922363 and perplexity is 188.35993549556574
At time: 395.7201364040375 and batch: 500, loss is 5.218207607269287 and perplexity is 184.6030062724976
At time: 396.1342170238495 and batch: 550, loss is 5.288911409378052 and perplexity is 198.12762809533217
At time: 396.54521441459656 and batch: 600, loss is 5.334006071090698 and perplexity is 207.2666380916744
At time: 396.9518811702728 and batch: 650, loss is 5.304386739730835 and perplexity is 201.21756582960697
At time: 397.35599994659424 and batch: 700, loss is 5.285584115982056 and perplexity is 197.4694948551742
At time: 397.76165437698364 and batch: 750, loss is 5.21988543510437 and perplexity is 184.91299831878186
At time: 398.1651523113251 and batch: 800, loss is 5.260465545654297 and perplexity is 192.57112108081097
At time: 398.5865113735199 and batch: 850, loss is 5.232152690887451 and perplexity is 187.19534380309665
At time: 399.00311732292175 and batch: 900, loss is 5.3091858291625975 and perplexity is 202.18554778142283
At time: 399.4204432964325 and batch: 950, loss is 5.291756296157837 and perplexity is 198.692081287228
At time: 399.8364453315735 and batch: 1000, loss is 5.240001831054688 and perplexity is 188.67044787125766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407858127500953 and perplexity of 223.1531099350598
Finished 43 epochs...
Completing Train Step...
At time: 401.0996298789978 and batch: 50, loss is 5.335322666168213 and perplexity is 207.53970404631983
At time: 401.5338909626007 and batch: 100, loss is 5.32563159942627 and perplexity is 205.53813724656
At time: 401.9456968307495 and batch: 150, loss is 5.3357878684997555 and perplexity is 207.63627446117937
At time: 402.3672525882721 and batch: 200, loss is 5.331147718429565 and perplexity is 206.6750428418359
At time: 402.7866494655609 and batch: 250, loss is 5.329055881500244 and perplexity is 206.24316422215693
At time: 403.19931292533875 and batch: 300, loss is 5.227167816162109 and perplexity is 186.2645204115698
At time: 403.62130856513977 and batch: 350, loss is 5.2907014751434325 and perplexity is 198.48260720274612
At time: 404.0708107948303 and batch: 400, loss is 5.247328805923462 and perplexity is 190.05790823781484
At time: 404.4908142089844 and batch: 450, loss is 5.238103790283203 and perplexity is 188.31268330209883
At time: 404.9200015068054 and batch: 500, loss is 5.21793152809143 and perplexity is 184.55204826084267
At time: 405.342001914978 and batch: 550, loss is 5.288696212768555 and perplexity is 198.08499628879292
At time: 405.75569677352905 and batch: 600, loss is 5.333972434997559 and perplexity is 207.25966656897893
At time: 406.1738724708557 and batch: 650, loss is 5.304334583282471 and perplexity is 201.20707130970573
At time: 406.58986616134644 and batch: 700, loss is 5.2855691146850585 and perplexity is 197.46653257885296
At time: 406.9982132911682 and batch: 750, loss is 5.219848690032959 and perplexity is 184.906203802287
At time: 407.4070920944214 and batch: 800, loss is 5.260404319763183 and perplexity is 192.5593311032497
At time: 407.84217405319214 and batch: 850, loss is 5.2320638847351075 and perplexity is 187.17872044301606
At time: 408.267609834671 and batch: 900, loss is 5.30909441947937 and perplexity is 202.1670669092252
At time: 408.69345808029175 and batch: 950, loss is 5.2916007995605465 and perplexity is 198.66118774666168
At time: 409.1180181503296 and batch: 1000, loss is 5.239813432693482 and perplexity is 188.6349060161895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407743500500191 and perplexity of 223.12753202934448
Finished 44 epochs...
Completing Train Step...
At time: 410.52193665504456 and batch: 50, loss is 5.335129022598267 and perplexity is 207.4995192080161
At time: 410.94417238235474 and batch: 100, loss is 5.32536208152771 and perplexity is 205.4827485041994
At time: 411.3773548603058 and batch: 150, loss is 5.335503616333008 and perplexity is 207.57726178790583
At time: 411.7888295650482 and batch: 200, loss is 5.3308602142333985 and perplexity is 206.61563144069873
At time: 412.20657563209534 and batch: 250, loss is 5.328804969787598 and perplexity is 206.19142188825066
At time: 412.62815117836 and batch: 300, loss is 5.226913833618164 and perplexity is 186.2172184820155
At time: 413.0486669540405 and batch: 350, loss is 5.29043872833252 and perplexity is 198.4304633812933
At time: 413.46777296066284 and batch: 400, loss is 5.247024173736572 and perplexity is 190.00001929945736
At time: 413.8806052207947 and batch: 450, loss is 5.237855033874512 and perplexity is 188.26584514117746
At time: 414.31350350379944 and batch: 500, loss is 5.217655439376831 and perplexity is 184.50110255615226
At time: 414.7640314102173 and batch: 550, loss is 5.28849313735962 and perplexity is 198.04477418136636
At time: 415.1877410411835 and batch: 600, loss is 5.333933668136597 and perplexity is 207.25163191804208
At time: 415.6081430912018 and batch: 650, loss is 5.304328107833863 and perplexity is 201.20576840787425
At time: 416.0215198993683 and batch: 700, loss is 5.285552864074707 and perplexity is 197.46332365324812
At time: 416.43744134902954 and batch: 750, loss is 5.219806060791016 and perplexity is 184.89832155899663
At time: 416.87119722366333 and batch: 800, loss is 5.260301761627197 and perplexity is 192.53958358983635
At time: 417.284964799881 and batch: 850, loss is 5.23197455406189 and perplexity is 187.16200038872483
At time: 417.7055342197418 and batch: 900, loss is 5.308987836837769 and perplexity is 202.14552055744318
At time: 418.12705874443054 and batch: 950, loss is 5.291455173492432 and perplexity is 198.63225960540004
At time: 418.54763746261597 and batch: 1000, loss is 5.239615354537964 and perplexity is 188.5975452622363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4076359446455795 and perplexity of 223.10353464750239
Finished 45 epochs...
Completing Train Step...
At time: 419.87927889823914 and batch: 50, loss is 5.334930620193481 and perplexity is 207.4583548880985
At time: 420.3266062736511 and batch: 100, loss is 5.3251145362854 and perplexity is 205.4318885227634
At time: 420.74971866607666 and batch: 150, loss is 5.335210733413696 and perplexity is 207.51647485565226
At time: 421.16376662254333 and batch: 200, loss is 5.330592393875122 and perplexity is 206.56030297763587
At time: 421.5804114341736 and batch: 250, loss is 5.328571119308472 and perplexity is 206.14320956290783
At time: 421.98923921585083 and batch: 300, loss is 5.2266551113128665 and perplexity is 186.16904616585873
At time: 422.41102600097656 and batch: 350, loss is 5.290200700759888 and perplexity is 198.3832370805623
At time: 422.8288698196411 and batch: 400, loss is 5.246728143692017 and perplexity is 189.94378190966773
At time: 423.23864793777466 and batch: 450, loss is 5.237632007598877 and perplexity is 188.2238615927976
At time: 423.6459364891052 and batch: 500, loss is 5.217386150360108 and perplexity is 184.45142512475417
At time: 424.0664939880371 and batch: 550, loss is 5.28829873085022 and perplexity is 198.00627673031164
At time: 424.4873342514038 and batch: 600, loss is 5.333890438079834 and perplexity is 207.24267261188714
At time: 424.9123945236206 and batch: 650, loss is 5.304289169311524 and perplexity is 201.19793390509935
At time: 425.33291506767273 and batch: 700, loss is 5.285544176101684 and perplexity is 197.46160810467157
At time: 425.77610182762146 and batch: 750, loss is 5.219755249023438 and perplexity is 184.88892678714043
At time: 426.21340584754944 and batch: 800, loss is 5.260229711532593 and perplexity is 192.52571159436891
At time: 426.63732719421387 and batch: 850, loss is 5.2319017505645755 and perplexity is 187.14837483653213
At time: 427.08272910118103 and batch: 900, loss is 5.308879356384278 and perplexity is 202.123592909084
At time: 427.52088379859924 and batch: 950, loss is 5.291300868988037 and perplexity is 198.60161211760823
At time: 427.9399266242981 and batch: 1000, loss is 5.239423427581787 and perplexity is 188.561351782795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.40753173828125 and perplexity of 223.0802870505825
Finished 46 epochs...
Completing Train Step...
At time: 429.240567445755 and batch: 50, loss is 5.3347343158721925 and perplexity is 207.4176339135289
At time: 429.6796908378601 and batch: 100, loss is 5.324868459701538 and perplexity is 205.38134276473806
At time: 430.0876977443695 and batch: 150, loss is 5.33491886138916 and perplexity is 207.45591544024109
At time: 430.50896739959717 and batch: 200, loss is 5.330346717834472 and perplexity is 206.50956229338533
At time: 430.9292049407959 and batch: 250, loss is 5.328338651657105 and perplexity is 206.09529350481802
At time: 431.3515536785126 and batch: 300, loss is 5.2264010524749756 and perplexity is 186.12175428205347
At time: 431.7765402793884 and batch: 350, loss is 5.289975109100342 and perplexity is 198.33848852452354
At time: 432.1890981197357 and batch: 400, loss is 5.2464342212677 and perplexity is 189.88796137666003
At time: 432.61143922805786 and batch: 450, loss is 5.237412424087524 and perplexity is 188.18253527380386
At time: 433.0243012905121 and batch: 500, loss is 5.21711841583252 and perplexity is 184.4020477198985
At time: 433.4476945400238 and batch: 550, loss is 5.288121633529663 and perplexity is 197.97121345414706
At time: 433.862694978714 and batch: 600, loss is 5.3338369846344 and perplexity is 207.23159507306394
At time: 434.2844421863556 and batch: 650, loss is 5.304227952957153 and perplexity is 201.18561767805988
At time: 434.70764207839966 and batch: 700, loss is 5.285538263320923 and perplexity is 197.46044056092583
At time: 435.133873462677 and batch: 750, loss is 5.219701852798462 and perplexity is 184.87905467997916
At time: 435.54472637176514 and batch: 800, loss is 5.260150814056397 and perplexity is 192.51052240082373
At time: 435.96561670303345 and batch: 850, loss is 5.231839714050293 and perplexity is 187.1367651638191
At time: 436.4242489337921 and batch: 900, loss is 5.3087759208679195 and perplexity is 202.10268723209666
At time: 436.84275817871094 and batch: 950, loss is 5.291134605407715 and perplexity is 198.56859464739728
At time: 437.255802154541 and batch: 1000, loss is 5.239259395599365 and perplexity is 188.5304242270774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407437952553353 and perplexity of 223.05936628453205
Finished 47 epochs...
Completing Train Step...
At time: 438.59456634521484 and batch: 50, loss is 5.3345419120788575 and perplexity is 207.37772981293264
At time: 439.0096824169159 and batch: 100, loss is 5.324627666473389 and perplexity is 205.33189428188103
At time: 439.4268922805786 and batch: 150, loss is 5.334626989364624 and perplexity is 207.39537369784983
At time: 439.8451461791992 and batch: 200, loss is 5.330113954544068 and perplexity is 206.4615000419468
At time: 440.2706663608551 and batch: 250, loss is 5.328101139068604 and perplexity is 206.04634909086718
At time: 440.7012004852295 and batch: 300, loss is 5.226156063079834 and perplexity is 186.07616201108877
At time: 441.1286926269531 and batch: 350, loss is 5.28976372718811 and perplexity is 198.29656778634904
At time: 441.5490257740021 and batch: 400, loss is 5.246154413223267 and perplexity is 189.83483663023816
At time: 441.9714472293854 and batch: 450, loss is 5.237204046249389 and perplexity is 188.1433262892124
At time: 442.3814027309418 and batch: 500, loss is 5.2168683910369875 and perplexity is 184.3559483988484
At time: 442.8034517765045 and batch: 550, loss is 5.2879543876647945 and perplexity is 197.9381063559237
At time: 443.21790957450867 and batch: 600, loss is 5.333793678283691 and perplexity is 207.2226208232523
At time: 443.6494052410126 and batch: 650, loss is 5.304154691696167 and perplexity is 201.1708791059062
At time: 444.0762975215912 and batch: 700, loss is 5.2855221557617185 and perplexity is 197.45725998080468
At time: 444.5066087245941 and batch: 750, loss is 5.21965895652771 and perplexity is 184.87112422808795
At time: 444.9247319698334 and batch: 800, loss is 5.260076370239258 and perplexity is 192.4961917161188
At time: 445.342889547348 and batch: 850, loss is 5.231776332855224 and perplexity is 187.1249045878745
At time: 445.7679908275604 and batch: 900, loss is 5.308677301406861 and perplexity is 202.08275695677594
At time: 446.1947867870331 and batch: 950, loss is 5.2909903049469 and perplexity is 198.53994317494633
At time: 446.62729930877686 and batch: 1000, loss is 5.239071779251098 and perplexity is 188.49505615526468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407354215296303 and perplexity of 223.04068868705636
Finished 48 epochs...
Completing Train Step...
At time: 447.93294763565063 and batch: 50, loss is 5.334352302551269 and perplexity is 207.3384127471134
At time: 448.37224984169006 and batch: 100, loss is 5.324391088485718 and perplexity is 205.2833230211992
At time: 448.7864978313446 and batch: 150, loss is 5.334348983764649 and perplexity is 207.3377246363051
At time: 449.2218770980835 and batch: 200, loss is 5.329900312423706 and perplexity is 206.41739588072576
At time: 449.6411118507385 and batch: 250, loss is 5.327874460220337 and perplexity is 205.9996480350364
At time: 450.0626244544983 and batch: 300, loss is 5.225928707122803 and perplexity is 186.03386129603595
At time: 450.4896237850189 and batch: 350, loss is 5.289557933807373 and perplexity is 198.25576386400826
At time: 450.90952610969543 and batch: 400, loss is 5.245894775390625 and perplexity is 189.7855547226963
At time: 451.3190987110138 and batch: 450, loss is 5.236989936828613 and perplexity is 188.10304734280254
At time: 451.7378532886505 and batch: 500, loss is 5.216633234024048 and perplexity is 184.31260090163903
At time: 452.16030287742615 and batch: 550, loss is 5.2877756786346435 and perplexity is 197.9027361894851
At time: 452.60504269599915 and batch: 600, loss is 5.333760662078857 and perplexity is 207.21577923169926
At time: 453.0326633453369 and batch: 650, loss is 5.304085607528687 and perplexity is 201.1569818632472
At time: 453.4733955860138 and batch: 700, loss is 5.285499057769775 and perplexity is 197.45269916727756
At time: 453.894464969635 and batch: 750, loss is 5.219621019363403 and perplexity is 184.86411087490671
At time: 454.30675506591797 and batch: 800, loss is 5.260007009506226 and perplexity is 192.4828405021859
At time: 454.7335557937622 and batch: 850, loss is 5.23170802116394 and perplexity is 187.11212220575877
At time: 455.16433358192444 and batch: 900, loss is 5.308578672409058 and perplexity is 202.0628267198499
At time: 455.5925750732422 and batch: 950, loss is 5.290852632522583 and perplexity is 198.51261158109227
At time: 456.0160608291626 and batch: 1000, loss is 5.238875904083252 and perplexity is 188.45813827026853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407282015172447 and perplexity of 223.02458570303384
Finished 49 epochs...
Completing Train Step...
At time: 457.400826215744 and batch: 50, loss is 5.334160223007202 and perplexity is 207.29859110390925
At time: 457.8226480484009 and batch: 100, loss is 5.324165697097778 and perplexity is 205.2370591420385
At time: 458.2389175891876 and batch: 150, loss is 5.334087677001953 and perplexity is 207.28355296471608
At time: 458.66871094703674 and batch: 200, loss is 5.329698076248169 and perplexity is 206.3756550369152
At time: 459.0839033126831 and batch: 250, loss is 5.327660970687866 and perplexity is 205.9556739606575
At time: 459.5017960071564 and batch: 300, loss is 5.225709524154663 and perplexity is 185.99309031045863
At time: 459.9178831577301 and batch: 350, loss is 5.289369068145752 and perplexity is 198.21832369368843
At time: 460.32506704330444 and batch: 400, loss is 5.245648431777954 and perplexity is 189.7388080216251
At time: 460.73323607444763 and batch: 450, loss is 5.236785669326782 and perplexity is 188.06462792728755
At time: 461.1434028148651 and batch: 500, loss is 5.216408805847168 and perplexity is 184.27124060202462
At time: 461.55134630203247 and batch: 550, loss is 5.287594785690308 and perplexity is 197.8669402185606
At time: 461.96139764785767 and batch: 600, loss is 5.333729543685913 and perplexity is 207.20933110998502
At time: 462.36546635627747 and batch: 650, loss is 5.304031133651733 and perplexity is 201.14602436102044
At time: 462.7692337036133 and batch: 700, loss is 5.285473833084106 and perplexity is 197.44771854782408
At time: 463.1724967956543 and batch: 750, loss is 5.2195842075347905 and perplexity is 184.85730581419463
At time: 463.5824782848358 and batch: 800, loss is 5.259926090240478 and perplexity is 192.4672655622283
At time: 464.00453996658325 and batch: 850, loss is 5.231632642745971 and perplexity is 187.0980185215675
At time: 464.4154779911041 and batch: 900, loss is 5.308478689193725 and perplexity is 202.04262483867657
At time: 464.8326590061188 and batch: 950, loss is 5.2907257175445555 and perplexity is 198.48741895604985
At time: 465.23889231681824 and batch: 1000, loss is 5.238680877685547 and perplexity is 188.421387542241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.407212792373285 and perplexity of 223.00914785126133
Finished Training.
Improved accuracyfrom -10000000 to -223.00914785126133
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1cb57bc8d0>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 1.9081563806455393, 'num_layers': 1, 'dropout': 0.5358826467218077, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 3.593027008332874}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6538209915161133 and batch: 50, loss is 6.988718252182007 and perplexity is 1084.330746525625
At time: 1.089879035949707 and batch: 100, loss is 6.148124551773071 and perplexity is 467.8391553866193
At time: 1.5050606727600098 and batch: 150, loss is 5.993564357757569 and perplexity is 400.8408067183446
At time: 1.9276368618011475 and batch: 200, loss is 5.884491748809815 and perplexity is 359.4200459000714
At time: 2.355541944503784 and batch: 250, loss is 5.8358245468139645 and perplexity is 342.3468987499204
At time: 2.782956838607788 and batch: 300, loss is 5.677627935409546 and perplexity is 292.25535846963953
At time: 3.2031538486480713 and batch: 350, loss is 5.684131879806518 and perplexity is 294.16236588353377
At time: 3.6245815753936768 and batch: 400, loss is 5.595707235336303 and perplexity is 269.26801862436275
At time: 4.038653135299683 and batch: 450, loss is 5.552187852859497 and perplexity is 257.8009699367915
At time: 4.47752833366394 and batch: 500, loss is 5.539310312271118 and perplexity is 254.5024117248246
At time: 4.904638290405273 and batch: 550, loss is 5.5743779373168945 and perplexity is 263.5855379006372
At time: 5.338216066360474 and batch: 600, loss is 5.572859935760498 and perplexity is 263.18571818411266
At time: 5.754753589630127 and batch: 650, loss is 5.520326004028321 and perplexity is 249.7164324843037
At time: 6.179245233535767 and batch: 700, loss is 5.490313301086426 and perplexity is 242.33311819498834
At time: 6.598939657211304 and batch: 750, loss is 5.3877413368225096 and perplexity is 218.70883765743136
At time: 7.029088973999023 and batch: 800, loss is 5.430603580474854 and perplexity is 228.28699342071323
At time: 7.45302939414978 and batch: 850, loss is 5.381609659194947 and perplexity is 217.37188862966093
At time: 7.886744737625122 and batch: 900, loss is 5.474573583602905 and perplexity is 238.54872415232208
At time: 8.310537576675415 and batch: 950, loss is 5.423103294372559 and perplexity is 226.58118069496487
At time: 8.736232280731201 and batch: 1000, loss is 5.364968786239624 and perplexity is 213.78456155671742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.231238016268102 and perplexity of 187.02419925597835
Finished 1 epochs...
Completing Train Step...
At time: 10.01027512550354 and batch: 50, loss is 5.196689262390136 and perplexity is 180.67308941192277
At time: 10.422542572021484 and batch: 100, loss is 5.11080041885376 and perplexity is 165.80301363348337
At time: 10.834460496902466 and batch: 150, loss is 5.09299467086792 and perplexity is 162.87689513053195
At time: 11.239665269851685 and batch: 200, loss is 5.053328313827515 and perplexity is 156.5426213548196
At time: 11.652682542800903 and batch: 250, loss is 5.020402593612671 and perplexity is 151.47227328998773
At time: 12.071480512619019 and batch: 300, loss is 4.872194738388061 and perplexity is 130.60725131517268
At time: 12.492867469787598 and batch: 350, loss is 4.922098112106323 and perplexity is 137.2903617866467
At time: 12.902617931365967 and batch: 400, loss is 4.8324569606781 and perplexity is 125.51897729900348
At time: 13.313061237335205 and batch: 450, loss is 4.840000686645507 and perplexity is 126.46943856975679
At time: 13.751705169677734 and batch: 500, loss is 4.814004201889038 and perplexity is 123.22404492339756
At time: 14.179615020751953 and batch: 550, loss is 4.846648168563843 and perplexity is 127.3129423578297
At time: 14.599787473678589 and batch: 600, loss is 4.856699914932251 and perplexity is 128.59911306541343
At time: 15.012189626693726 and batch: 650, loss is 4.810074586868286 and perplexity is 122.74077202565425
At time: 15.459384679794312 and batch: 700, loss is 4.785179615020752 and perplexity is 119.72286517766493
At time: 15.875413656234741 and batch: 750, loss is 4.6999802398681645 and perplexity is 109.9449999029659
At time: 16.292248249053955 and batch: 800, loss is 4.732191209793091 and perplexity is 113.54408888183903
At time: 16.701974391937256 and batch: 850, loss is 4.69132363319397 and perplexity is 108.99735688721265
At time: 17.119938611984253 and batch: 900, loss is 4.826092033386231 and perplexity is 124.72259528017246
At time: 17.54073405265808 and batch: 950, loss is 4.7401273059844975 and perplexity is 114.44877076621
At time: 17.957922220230103 and batch: 1000, loss is 4.695857248306274 and perplexity is 109.49263079392428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.79300708305545 and perplexity of 120.66366933156036
Finished 2 epochs...
Completing Train Step...
At time: 19.349666357040405 and batch: 50, loss is 4.735626449584961 and perplexity is 113.93481078156816
At time: 19.77788233757019 and batch: 100, loss is 4.653695192337036 and perplexity is 104.97216211972776
At time: 20.196381092071533 and batch: 150, loss is 4.681277351379395 and perplexity is 107.90782077995966
At time: 20.603938579559326 and batch: 200, loss is 4.678285408020019 and perplexity is 107.58544919140101
At time: 21.02441692352295 and batch: 250, loss is 4.660899333953857 and perplexity is 105.73112700421866
At time: 21.44346570968628 and batch: 300, loss is 4.535357770919799 and perplexity is 93.25687393425926
At time: 21.860957384109497 and batch: 350, loss is 4.607606601715088 and perplexity is 100.24393861997397
At time: 22.290708541870117 and batch: 400, loss is 4.528925647735596 and perplexity is 92.65895922529083
At time: 22.71992802619934 and batch: 450, loss is 4.566198282241821 and perplexity is 96.17777311179219
At time: 23.147023677825928 and batch: 500, loss is 4.529040241241455 and perplexity is 92.66957794868449
At time: 23.568475484848022 and batch: 550, loss is 4.575648784637451 and perplexity is 97.09100986313847
At time: 23.989150285720825 and batch: 600, loss is 4.592902889251709 and perplexity is 98.78076398122755
At time: 24.416227340698242 and batch: 650, loss is 4.554442987442017 and perplexity is 95.05379432987685
At time: 24.837252855300903 and batch: 700, loss is 4.534690418243408 and perplexity is 93.19465947165041
At time: 25.25917339324951 and batch: 750, loss is 4.468384971618653 and perplexity is 87.21575329216432
At time: 25.679670810699463 and batch: 800, loss is 4.4947695064544675 and perplexity is 89.54752647938274
At time: 26.130557537078857 and batch: 850, loss is 4.465763025283813 and perplexity is 86.98737779246443
At time: 26.556119680404663 and batch: 900, loss is 4.61858564376831 and perplexity is 101.35058487939264
At time: 26.97320818901062 and batch: 950, loss is 4.535449371337891 and perplexity is 93.26541669415577
At time: 27.3999080657959 and batch: 1000, loss is 4.489131469726562 and perplexity is 89.04407480932218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.652062020650724 and perplexity of 104.80086447400976
Finished 3 epochs...
Completing Train Step...
At time: 28.675474166870117 and batch: 50, loss is 4.545317125320435 and perplexity is 94.19029260034341
At time: 29.108115911483765 and batch: 100, loss is 4.45107738494873 and perplexity is 85.71924689682058
At time: 29.525130033493042 and batch: 150, loss is 4.488094291687012 and perplexity is 88.95176812788414
At time: 29.942222595214844 and batch: 200, loss is 4.50486496925354 and perplexity is 90.45612886557242
At time: 30.35926604270935 and batch: 250, loss is 4.4824769496917725 and perplexity is 88.45349641675939
At time: 30.779560089111328 and batch: 300, loss is 4.369933271408081 and perplexity is 79.03835740529608
At time: 31.18711495399475 and batch: 350, loss is 4.447652826309204 and perplexity is 85.42619837660352
At time: 31.593542098999023 and batch: 400, loss is 4.369117789268493 and perplexity is 78.97392931004062
At time: 32.016056060791016 and batch: 450, loss is 4.416849308013916 and perplexity is 82.83488656776825
At time: 32.432589530944824 and batch: 500, loss is 4.3700518226623535 and perplexity is 79.04772805714236
At time: 32.866533279418945 and batch: 550, loss is 4.423390474319458 and perplexity is 83.37849932915904
At time: 33.28764605522156 and batch: 600, loss is 4.440504398345947 and perplexity is 84.81771280012191
At time: 33.72116947174072 and batch: 650, loss is 4.4072141408920285 and perplexity is 82.04059132084213
At time: 34.150519132614136 and batch: 700, loss is 4.385118942260743 and perplexity is 80.24776749616603
At time: 34.573867082595825 and batch: 750, loss is 4.329528684616089 and perplexity is 75.9085012923453
At time: 34.99264907836914 and batch: 800, loss is 4.349767427444458 and perplexity is 77.46044565638125
At time: 35.42314529418945 and batch: 850, loss is 4.327421617507935 and perplexity is 75.74872537444708
At time: 35.82924270629883 and batch: 900, loss is 4.489098110198975 and perplexity is 89.04110439059825
At time: 36.24232268333435 and batch: 950, loss is 4.409354810714722 and perplexity is 82.21640124728366
At time: 36.687421798706055 and batch: 1000, loss is 4.358007440567016 and perplexity is 78.1013576804402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.573406312523819 and perplexity of 96.87352991852794
Finished 4 epochs...
Completing Train Step...
At time: 37.95320796966553 and batch: 50, loss is 4.421052522659302 and perplexity is 83.18379212493623
At time: 38.38673281669617 and batch: 100, loss is 4.317649512290955 and perplexity is 75.0121058835093
At time: 38.80816292762756 and batch: 150, loss is 4.35957501411438 and perplexity is 78.22388331163239
At time: 39.234909534454346 and batch: 200, loss is 4.387882709503174 and perplexity is 80.46986041244386
At time: 39.64488768577576 and batch: 250, loss is 4.363776206970215 and perplexity is 78.5532082257125
At time: 40.058879137039185 and batch: 300, loss is 4.2593635559082035 and perplexity is 70.76493119680917
At time: 40.47793102264404 and batch: 350, loss is 4.340145435333252 and perplexity is 76.71869613611882
At time: 40.8895308971405 and batch: 400, loss is 4.258120589256286 and perplexity is 70.67702738927437
At time: 41.308764696121216 and batch: 450, loss is 4.312427682876587 and perplexity is 74.62142638116903
At time: 41.71857023239136 and batch: 500, loss is 4.257895455360413 and perplexity is 70.66111738575721
At time: 42.130335569381714 and batch: 550, loss is 4.316263680458069 and perplexity is 74.90822371755608
At time: 42.55140924453735 and batch: 600, loss is 4.332299966812133 and perplexity is 76.1191569287929
At time: 42.963458776474 and batch: 650, loss is 4.302047686576843 and perplexity is 73.85086239575506
At time: 43.39013075828552 and batch: 700, loss is 4.278701763153077 and perplexity is 72.14671565763574
At time: 43.820828676223755 and batch: 750, loss is 4.229014277458191 and perplexity is 68.64952942262792
At time: 44.24112248420715 and batch: 800, loss is 4.244896883964539 and perplexity is 69.74856757730738
At time: 44.65520882606506 and batch: 850, loss is 4.225876779556274 and perplexity is 68.43447920431215
At time: 45.079373836517334 and batch: 900, loss is 4.3938448524475096 and perplexity is 80.95106630663976
At time: 45.510993003845215 and batch: 950, loss is 4.3163794422149655 and perplexity is 74.91689572707348
At time: 45.93498873710632 and batch: 1000, loss is 4.261220970153809 and perplexity is 70.8964931327865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5226261790205795 and perplexity of 92.0770916629389
Finished 5 epochs...
Completing Train Step...
At time: 47.228275775909424 and batch: 50, loss is 4.328009371757507 and perplexity is 75.7932600961395
At time: 47.67740035057068 and batch: 100, loss is 4.219010257720948 and perplexity is 67.96618198508928
At time: 48.10365629196167 and batch: 150, loss is 4.263213877677917 and perplexity is 71.03792417006302
At time: 48.5351026058197 and batch: 200, loss is 4.298532953262329 and perplexity is 73.59175192822345
At time: 48.96095323562622 and batch: 250, loss is 4.273907203674316 and perplexity is 71.80163186166996
At time: 49.384236574172974 and batch: 300, loss is 4.175512576103211 and perplexity is 65.07318607414102
At time: 49.81116795539856 and batch: 350, loss is 4.258639554977417 and perplexity is 70.71371586296752
At time: 50.243279695510864 and batch: 400, loss is 4.172387228012085 and perplexity is 64.87012719638525
At time: 50.665345430374146 and batch: 450, loss is 4.231416883468628 and perplexity is 68.81466549365328
At time: 51.080718755722046 and batch: 500, loss is 4.171688027381897 and perplexity is 64.82478581577733
At time: 51.49115180969238 and batch: 550, loss is 4.2328646898269655 and perplexity is 68.91436796141934
At time: 51.920706033706665 and batch: 600, loss is 4.248509879112244 and perplexity is 70.0010246019005
At time: 52.343109369277954 and batch: 650, loss is 4.219539575576782 and perplexity is 68.00216722178085
At time: 52.753742933273315 and batch: 700, loss is 4.195637812614441 and perplexity is 66.39606632369257
At time: 53.180891275405884 and batch: 750, loss is 4.149983797073364 and perplexity is 63.43297248999706
At time: 53.60036325454712 and batch: 800, loss is 4.161894659996033 and perplexity is 64.19303142329633
At time: 54.023117542266846 and batch: 850, loss is 4.145612678527832 and perplexity is 63.15630456221975
At time: 54.435149908065796 and batch: 900, loss is 4.31883713722229 and perplexity is 75.10124505208077
At time: 54.85407519340515 and batch: 950, loss is 4.242375545501709 and perplexity is 69.57292934590811
At time: 55.27220273017883 and batch: 1000, loss is 4.184419407844543 and perplexity is 65.65537085551925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4868860012147485 and perplexity of 88.84435346109196
Finished 6 epochs...
Completing Train Step...
At time: 56.55935597419739 and batch: 50, loss is 4.253205933570862 and perplexity is 70.33052629900747
At time: 56.99783372879028 and batch: 100, loss is 4.140380759239196 and perplexity is 62.826738756795024
At time: 57.42544150352478 and batch: 150, loss is 4.185474123954773 and perplexity is 65.72465516409908
At time: 57.837295055389404 and batch: 200, loss is 4.225332851409912 and perplexity is 68.3972658864943
At time: 58.247965574264526 and batch: 250, loss is 4.200357189178467 and perplexity is 66.71015493128925
At time: 58.67418694496155 and batch: 300, loss is 4.1077623462677 and perplexity is 60.81049238881427
At time: 59.08181118965149 and batch: 350, loss is 4.191802611351013 and perplexity is 66.14191172478576
At time: 59.492666244506836 and batch: 400, loss is 4.102722301483154 and perplexity is 60.50477584349625
At time: 59.89975571632385 and batch: 450, loss is 4.164873638153076 and perplexity is 64.38454617924798
At time: 60.31205773353577 and batch: 500, loss is 4.101711292266845 and perplexity is 60.44363586923431
At time: 60.7301549911499 and batch: 550, loss is 4.164606266021728 and perplexity is 64.36733384706167
At time: 61.149314403533936 and batch: 600, loss is 4.179937953948975 and perplexity is 65.36179764676336
At time: 61.564552307128906 and batch: 650, loss is 4.151851453781128 and perplexity is 63.55155420706188
At time: 61.97701716423035 and batch: 700, loss is 4.126904516220093 and perplexity is 61.985749770754566
At time: 62.39217686653137 and batch: 750, loss is 4.084790501594544 and perplexity is 59.42948606441756
At time: 62.799436807632446 and batch: 800, loss is 4.093332033157349 and perplexity is 59.93927900219014
At time: 63.22473621368408 and batch: 850, loss is 4.079261102676392 and perplexity is 59.10178356233472
At time: 63.63627481460571 and batch: 900, loss is 4.25552951335907 and perplexity is 70.49413489361115
At time: 64.07674264907837 and batch: 950, loss is 4.180969500541687 and perplexity is 65.42925617374446
At time: 64.50073385238647 and batch: 1000, loss is 4.120363821983338 and perplexity is 61.5816429447248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.463010090153392 and perplexity of 86.74823650549205
Finished 7 epochs...
Completing Train Step...
At time: 65.82449913024902 and batch: 50, loss is 4.190687346458435 and perplexity is 66.06818709160028
At time: 66.2337543964386 and batch: 100, loss is 4.075092058181763 and perplexity is 58.85589850596424
At time: 66.66497731208801 and batch: 150, loss is 4.120219302177429 and perplexity is 61.57274382070424
At time: 67.08944344520569 and batch: 200, loss is 4.162879333496094 and perplexity is 64.2562717306451
At time: 67.51643180847168 and batch: 250, loss is 4.13841543674469 and perplexity is 62.703385208315986
At time: 67.9355800151825 and batch: 300, loss is 4.0502081298828125 and perplexity is 57.40940441471396
At time: 68.34866404533386 and batch: 350, loss is 4.135121512413025 and perplexity is 62.49718479282184
At time: 68.76997470855713 and batch: 400, loss is 4.044066157341003 and perplexity is 57.057878066821296
At time: 69.20973324775696 and batch: 450, loss is 4.1088523149490355 and perplexity is 60.87681005653946
At time: 69.62424683570862 and batch: 500, loss is 4.04276074886322 and perplexity is 56.983442823837514
At time: 70.03760409355164 and batch: 550, loss is 4.107031354904175 and perplexity is 60.766056687102086
At time: 70.46056294441223 and batch: 600, loss is 4.122020840644836 and perplexity is 61.68376946569801
At time: 70.87881374359131 and batch: 650, loss is 4.094722499847412 and perplexity is 60.02268054316404
At time: 71.29599714279175 and batch: 700, loss is 4.068630690574646 and perplexity is 58.47683486344211
At time: 71.71615028381348 and batch: 750, loss is 4.029041953086853 and perplexity is 56.207036466181265
At time: 72.12999558448792 and batch: 800, loss is 4.035205912590027 and perplexity is 56.55456433626898
At time: 72.54376792907715 and batch: 850, loss is 4.022497434616088 and perplexity is 55.840389550243266
At time: 72.95041918754578 and batch: 900, loss is 4.2003693151474 and perplexity is 66.71096386145997
At time: 73.38109087944031 and batch: 950, loss is 4.128143329620361 and perplexity is 62.06258613132631
At time: 73.7935357093811 and batch: 1000, loss is 4.065607900619507 and perplexity is 58.30033856441948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.446811955149581 and perplexity of 85.35439614255705
Finished 8 epochs...
Completing Train Step...
At time: 75.07575225830078 and batch: 50, loss is 4.136573138237 and perplexity is 62.58797319964821
At time: 75.4997341632843 and batch: 100, loss is 4.0194629955291745 and perplexity is 55.67120211386909
At time: 75.9131588935852 and batch: 150, loss is 4.06446590423584 and perplexity is 58.23379779050558
At time: 76.32223343849182 and batch: 200, loss is 4.109147233963013 and perplexity is 60.894766433043685
At time: 76.74810838699341 and batch: 250, loss is 4.085191330909729 and perplexity is 59.45331191935014
At time: 77.15889835357666 and batch: 300, loss is 4.0013012075424195 and perplexity is 54.669239799008864
At time: 77.57170724868774 and batch: 350, loss is 4.086053252220154 and perplexity is 59.50457808639587
At time: 78.00786924362183 and batch: 400, loss is 3.993839316368103 and perplexity is 54.262822089177426
At time: 78.42724394798279 and batch: 450, loss is 4.060188369750977 and perplexity is 57.985232714120826
At time: 78.8436233997345 and batch: 500, loss is 3.99188636302948 and perplexity is 54.15695274221159
At time: 79.26044631004333 and batch: 550, loss is 4.05710666179657 and perplexity is 57.806814219277186
At time: 79.67797780036926 and batch: 600, loss is 4.07182578086853 and perplexity is 58.663972432445476
At time: 80.09749126434326 and batch: 650, loss is 4.045003638267517 and perplexity is 57.11139382029593
At time: 80.50435209274292 and batch: 700, loss is 4.018362951278687 and perplexity is 55.60999499950532
At time: 80.93040013313293 and batch: 750, loss is 3.9806444311141966 and perplexity is 53.55153338445803
At time: 81.34459567070007 and batch: 800, loss is 3.9841129684448244 and perplexity is 53.73760138246817
At time: 81.76628875732422 and batch: 850, loss is 3.9727368927001954 and perplexity is 53.129742438994356
At time: 82.17352819442749 and batch: 900, loss is 4.1518922662734985 and perplexity is 63.55414795731144
At time: 82.5904188156128 and batch: 950, loss is 4.081759881973267 and perplexity is 59.24965054204361
At time: 83.00613713264465 and batch: 1000, loss is 4.018108229637146 and perplexity is 55.59583173421467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.435026587509528 and perplexity of 84.35436763102913
Finished 9 epochs...
Completing Train Step...
At time: 84.32081198692322 and batch: 50, loss is 4.089027805328369 and perplexity is 59.68184112255195
At time: 84.75004577636719 and batch: 100, loss is 3.9708861923217773 and perplexity is 53.03150613557824
At time: 85.16223907470703 and batch: 150, loss is 4.015953035354614 and perplexity is 55.476140940314735
At time: 85.58192086219788 and batch: 200, loss is 4.062113599777222 and perplexity is 58.09697515565815
At time: 85.99863862991333 and batch: 250, loss is 4.038636918067932 and perplexity is 56.748936611802414
At time: 86.419029712677 and batch: 300, loss is 3.9585304641723633 and perplexity is 52.38029464318943
At time: 86.82956027984619 and batch: 350, loss is 4.042902550697327 and perplexity is 56.99152375347544
At time: 87.25366687774658 and batch: 400, loss is 3.9499374532699587 and perplexity is 51.932118549048305
At time: 87.66578316688538 and batch: 450, loss is 4.017329425811767 and perplexity is 55.55255034382878
At time: 88.08210897445679 and batch: 500, loss is 3.9468632173538207 and perplexity is 51.77271211703781
At time: 88.50214982032776 and batch: 550, loss is 4.013005146980285 and perplexity is 55.31284427780526
At time: 88.91931223869324 and batch: 600, loss is 4.027509660720825 and perplexity is 56.120976804406865
At time: 89.3357343673706 and batch: 650, loss is 4.001226534843445 and perplexity is 54.665157651736564
At time: 89.74707078933716 and batch: 700, loss is 3.974056057929993 and perplexity is 53.19987559632882
At time: 90.16596460342407 and batch: 750, loss is 3.9381621170043943 and perplexity is 51.32418671559518
At time: 90.61505270004272 and batch: 800, loss is 3.9384947299957274 and perplexity is 51.34126064621455
At time: 91.03922009468079 and batch: 850, loss is 3.9288815116882323 and perplexity is 50.850070640546846
At time: 91.46145367622375 and batch: 900, loss is 4.109015016555786 and perplexity is 60.886715617152994
At time: 91.88773703575134 and batch: 950, loss is 4.040599966049195 and perplexity is 56.86044691148207
At time: 92.29467821121216 and batch: 1000, loss is 3.9761121034622193 and perplexity is 53.30936948650334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.426511811047066 and perplexity of 83.63915829197724
Finished 10 epochs...
Completing Train Step...
At time: 93.64438319206238 and batch: 50, loss is 4.046826014518738 and perplexity is 57.2155671607485
At time: 94.06261730194092 and batch: 100, loss is 3.9276900815963747 and perplexity is 50.789522412865225
At time: 94.48140597343445 and batch: 150, loss is 3.9728189945220946 and perplexity is 53.13410466671661
At time: 94.9063971042633 and batch: 200, loss is 4.020383372306823 and perplexity is 55.72246418206842
At time: 95.32420206069946 and batch: 250, loss is 3.997368321418762 and perplexity is 54.454654151463785
At time: 95.73980069160461 and batch: 300, loss is 3.920672821998596 and perplexity is 50.43436671648472
At time: 96.1677360534668 and batch: 350, loss is 4.004161472320557 and perplexity is 54.82583214107547
At time: 96.59344291687012 and batch: 400, loss is 3.9107816886901854 and perplexity is 49.93797266884881
At time: 97.01480054855347 and batch: 450, loss is 3.9790937328338623 and perplexity is 53.46855546721411
At time: 97.4452314376831 and batch: 500, loss is 3.906738634109497 and perplexity is 49.73647832049746
At time: 97.86226439476013 and batch: 550, loss is 3.973710732460022 and perplexity is 53.18150749595511
At time: 98.2914822101593 and batch: 600, loss is 3.9876261949539185 and perplexity is 53.92672577206333
At time: 98.71007180213928 and batch: 650, loss is 3.9623530435562135 and perplexity is 52.580905659100104
At time: 99.12324094772339 and batch: 700, loss is 3.934995675086975 and perplexity is 51.16192868528712
At time: 99.53316402435303 and batch: 750, loss is 3.90023992061615 and perplexity is 49.41430319352207
At time: 99.96122622489929 and batch: 800, loss is 3.8975063800811767 and perplexity is 49.27941164249161
At time: 100.38983607292175 and batch: 850, loss is 3.8891073942184446 and perplexity is 48.86724786259782
At time: 100.80732083320618 and batch: 900, loss is 4.070631203651428 and perplexity is 58.59393562802704
At time: 101.21642136573792 and batch: 950, loss is 4.003346147537232 and perplexity is 54.78114949927231
At time: 101.66059184074402 and batch: 1000, loss is 3.938278570175171 and perplexity is 51.33016392790166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.421061446027058 and perplexity of 83.18453440781651
Finished 11 epochs...
Completing Train Step...
At time: 102.94121742248535 and batch: 50, loss is 4.009198079109192 and perplexity is 55.10266486300259
At time: 103.36460781097412 and batch: 100, loss is 3.8893181324005126 and perplexity is 48.87754714276273
At time: 103.77855014801025 and batch: 150, loss is 3.934182353019714 and perplexity is 51.12033447672046
At time: 104.19945788383484 and batch: 200, loss is 3.982811827659607 and perplexity is 53.66772666588945
At time: 104.63063430786133 and batch: 250, loss is 3.960209412574768 and perplexity is 52.46831232309378
At time: 105.04636883735657 and batch: 300, loss is 3.8864114236831666 and perplexity is 48.735680632596576
At time: 105.4758243560791 and batch: 350, loss is 3.968924469947815 and perplexity is 52.92757501879603
At time: 105.88745856285095 and batch: 400, loss is 3.8754824018478393 and perplexity is 48.205947321024965
At time: 106.31533527374268 and batch: 450, loss is 3.94439640045166 and perplexity is 51.64515570955149
At time: 106.7369875907898 and batch: 500, loss is 3.8705692148208617 and perplexity is 47.96968336579648
At time: 107.15452146530151 and batch: 550, loss is 3.9383385610580444 and perplexity is 51.33324336212179
At time: 107.56593322753906 and batch: 600, loss is 3.9513493967056275 and perplexity is 52.005495552833146
At time: 107.9776406288147 and batch: 650, loss is 3.9274187660217286 and perplexity is 50.77574429359947
At time: 108.43116569519043 and batch: 700, loss is 3.899510021209717 and perplexity is 49.37824888256257
At time: 108.85133218765259 and batch: 750, loss is 3.8658263444900514 and perplexity is 47.742708060483714
At time: 109.2652633190155 and batch: 800, loss is 3.860520601272583 and perplexity is 47.49006832446078
At time: 109.6811511516571 and batch: 850, loss is 3.85315055847168 and perplexity is 47.14135109719035
At time: 110.10508346557617 and batch: 900, loss is 4.0358356809616085 and perplexity is 56.59019182951322
At time: 110.521311044693 and batch: 950, loss is 3.969038805961609 and perplexity is 52.93362689271044
At time: 110.93745946884155 and batch: 1000, loss is 3.903676166534424 and perplexity is 49.58439496238117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.417372912895389 and perplexity of 82.8782706758056
Finished 12 epochs...
Completing Train Step...
At time: 112.35098147392273 and batch: 50, loss is 3.975039629936218 and perplexity is 53.25222724628825
At time: 112.81477427482605 and batch: 100, loss is 3.8551043939590453 and perplexity is 47.233547580937476
At time: 113.2311441898346 and batch: 150, loss is 3.8990164947509767 and perplexity is 49.35388542275331
At time: 113.65691709518433 and batch: 200, loss is 3.948597650527954 and perplexity is 51.862586344338716
At time: 114.07234525680542 and batch: 250, loss is 3.92660719871521 and perplexity is 50.73455307655002
At time: 114.48452138900757 and batch: 300, loss is 3.8551077747344973 and perplexity is 47.23370726722558
At time: 114.90909194946289 and batch: 350, loss is 3.9369628381729127 and perplexity is 51.262671599185566
At time: 115.33619976043701 and batch: 400, loss is 3.843320870399475 and perplexity is 46.68023634055697
At time: 115.76873874664307 and batch: 450, loss is 3.9126671361923218 and perplexity is 50.032216913047286
At time: 116.18350076675415 and batch: 500, loss is 3.8375074243545533 and perplexity is 46.40965058513452
At time: 116.61105370521545 and batch: 550, loss is 3.9060310077667237 and perplexity is 49.70129592770316
At time: 117.02976250648499 and batch: 600, loss is 3.918302283287048 and perplexity is 50.31495169267983
At time: 117.45983719825745 and batch: 650, loss is 3.8955224084854128 and perplexity is 49.18173961084278
At time: 117.87840151786804 and batch: 700, loss is 3.8669818305969237 and perplexity is 47.79790598042542
At time: 118.30584573745728 and batch: 750, loss is 3.8345833826065063 and perplexity is 46.274145037797986
At time: 118.71932339668274 and batch: 800, loss is 3.82699143409729 and perplexity is 45.92416431019948
At time: 119.12848901748657 and batch: 850, loss is 3.8206732797622682 and perplexity is 45.63492305002794
At time: 119.5511519908905 and batch: 900, loss is 4.00400297164917 and perplexity is 54.81714289851534
At time: 119.96991872787476 and batch: 950, loss is 3.9378637742996214 and perplexity is 51.30887680282414
At time: 120.39539790153503 and batch: 1000, loss is 3.871795110702515 and perplexity is 48.0285252627293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.414898383908156 and perplexity of 82.67343952743373
Finished 13 epochs...
Completing Train Step...
At time: 121.74663591384888 and batch: 50, loss is 3.943569884300232 and perplexity is 51.60248778950602
At time: 122.16302800178528 and batch: 100, loss is 3.824391498565674 and perplexity is 45.804919525197576
At time: 122.59921503067017 and batch: 150, loss is 3.866879439353943 and perplexity is 47.79301214396749
At time: 123.02230334281921 and batch: 200, loss is 3.9175084733963015 and perplexity is 50.27502703476439
At time: 123.45360088348389 and batch: 250, loss is 3.895851230621338 and perplexity is 49.19791431466473
At time: 123.86806011199951 and batch: 300, loss is 3.8262893772125244 and perplexity is 45.89193424946021
At time: 124.27957081794739 and batch: 350, loss is 3.9074769735336305 and perplexity is 49.77321428339502
At time: 124.6999032497406 and batch: 400, loss is 3.813569221496582 and perplexity is 45.31187871921075
At time: 125.10779595375061 and batch: 450, loss is 3.8832230138778687 and perplexity is 48.58053876994821
At time: 125.51781487464905 and batch: 500, loss is 3.806870446205139 and perplexity is 45.00935901266659
At time: 125.92924046516418 and batch: 550, loss is 3.8765346384048462 and perplexity is 48.256698077279204
At time: 126.3386480808258 and batch: 600, loss is 3.8880330514907837 and perplexity is 48.81477588172835
At time: 126.7674548625946 and batch: 650, loss is 3.866188259124756 and perplexity is 47.759989972331006
At time: 127.18972945213318 and batch: 700, loss is 3.8369422435760496 and perplexity is 46.38342815359189
At time: 127.63045024871826 and batch: 750, loss is 3.805737671852112 and perplexity is 44.95840243173372
At time: 128.0504105091095 and batch: 800, loss is 3.796433606147766 and perplexity is 44.542046406958
At time: 128.46240043640137 and batch: 850, loss is 3.790839185714722 and perplexity is 44.29355520264135
At time: 128.87214970588684 and batch: 900, loss is 3.9742784214019777 and perplexity is 53.21170662072067
At time: 129.28992438316345 and batch: 950, loss is 3.9093109703063966 and perplexity is 49.86458195615952
At time: 129.71236538887024 and batch: 1000, loss is 3.842564277648926 and perplexity is 46.644931769423195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.413319657488567 and perplexity of 82.54302375675195
Finished 14 epochs...
Completing Train Step...
At time: 130.961745262146 and batch: 50, loss is 3.9145493698120117 and perplexity is 50.12647791657456
At time: 131.40534114837646 and batch: 100, loss is 3.7961709833145143 and perplexity is 44.53035018444655
At time: 131.8083255290985 and batch: 150, loss is 3.837915644645691 and perplexity is 46.42859981367463
At time: 132.23664498329163 and batch: 200, loss is 3.889024119377136 and perplexity is 48.863178619722014
At time: 132.6560173034668 and batch: 250, loss is 3.867811055183411 and perplexity is 47.837557617030896
At time: 133.08902406692505 and batch: 300, loss is 3.799754328727722 and perplexity is 44.690204045279565
At time: 133.51370334625244 and batch: 350, loss is 3.879989924430847 and perplexity is 48.423727172275484
At time: 133.9339940547943 and batch: 400, loss is 3.7860326957702637 and perplexity is 44.08116949871452
At time: 134.37054824829102 and batch: 450, loss is 3.855803370475769 and perplexity is 47.26657426259091
At time: 134.79313254356384 and batch: 500, loss is 3.7787619113922117 and perplexity is 43.76182716161127
At time: 135.20179057121277 and batch: 550, loss is 3.8494451332092283 and perplexity is 46.96699557410297
At time: 135.61730217933655 and batch: 600, loss is 3.859981279373169 and perplexity is 47.46446279604049
At time: 136.0386643409729 and batch: 650, loss is 3.839046540260315 and perplexity is 46.481135414138045
At time: 136.45627212524414 and batch: 700, loss is 3.809369797706604 and perplexity is 45.1219939201739
At time: 136.88756942749023 and batch: 750, loss is 3.778872499465942 and perplexity is 43.76666696538736
At time: 137.30485320091248 and batch: 800, loss is 3.7679339694976806 and perplexity is 43.2905328184947
At time: 137.71729636192322 and batch: 850, loss is 3.7634731912612915 and perplexity is 43.097853421390035
At time: 138.12059211730957 and batch: 900, loss is 3.946600332260132 and perplexity is 51.75910363157477
At time: 138.55135083198547 and batch: 950, loss is 3.882808961868286 and perplexity is 48.56042806397034
At time: 138.96184659004211 and batch: 1000, loss is 3.815588173866272 and perplexity is 45.403453655737316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.413566775438262 and perplexity of 82.563424140091
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 140.2439112663269 and batch: 50, loss is 3.897060265541077 and perplexity is 49.25743228345068
At time: 140.68427276611328 and batch: 100, loss is 3.7780650424957276 and perplexity is 43.73134152888737
At time: 141.1058497428894 and batch: 150, loss is 3.8196912145614625 and perplexity is 45.59012857930469
At time: 141.52048325538635 and batch: 200, loss is 3.861689081192017 and perplexity is 47.5455919484729
At time: 141.93615055084229 and batch: 250, loss is 3.8324364948272707 and perplexity is 46.17490620675559
At time: 142.37065935134888 and batch: 300, loss is 3.7600168132781984 and perplexity is 42.94914808866769
At time: 142.791512966156 and batch: 350, loss is 3.8390164804458617 and perplexity is 46.47973822083171
At time: 143.22564673423767 and batch: 400, loss is 3.7411991643905638 and perplexity is 42.148502856174254
At time: 143.64356422424316 and batch: 450, loss is 3.804275779724121 and perplexity is 44.89272611467373
At time: 144.05609774589539 and batch: 500, loss is 3.723506474494934 and perplexity is 41.40934063122974
At time: 144.46355390548706 and batch: 550, loss is 3.7906724405288696 and perplexity is 44.28617008128086
At time: 144.90959095954895 and batch: 600, loss is 3.7986092233657835 and perplexity is 44.63905834219941
At time: 145.32778692245483 and batch: 650, loss is 3.7712280321121217 and perplexity is 43.43336967182219
At time: 145.75428318977356 and batch: 700, loss is 3.7367053556442262 and perplexity is 41.959520488732714
At time: 146.19124841690063 and batch: 750, loss is 3.701093282699585 and perplexity is 40.491548879646
At time: 146.63080668449402 and batch: 800, loss is 3.6840631628036498 and perplexity is 39.80781153691165
At time: 147.06173849105835 and batch: 850, loss is 3.676600036621094 and perplexity is 39.511826676360165
At time: 147.48604106903076 and batch: 900, loss is 3.851912202835083 and perplexity is 47.08300947063909
At time: 147.92385172843933 and batch: 950, loss is 3.784209589958191 and perplexity is 44.000878074518326
At time: 148.3350567817688 and batch: 1000, loss is 3.712830591201782 and perplexity is 40.96961077222271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.368898345202934 and perplexity of 78.95660085130291
Finished 16 epochs...
Completing Train Step...
At time: 149.75370812416077 and batch: 50, loss is 3.8584807777404784 and perplexity is 47.39329569864593
At time: 150.18252229690552 and batch: 100, loss is 3.7398886346817015 and perplexity is 42.09330216996732
At time: 150.60583114624023 and batch: 150, loss is 3.784700970649719 and perplexity is 44.02250456940029
At time: 151.02511882781982 and batch: 200, loss is 3.829381742477417 and perplexity is 46.03406852515466
At time: 151.45185589790344 and batch: 250, loss is 3.803374004364014 and perplexity is 44.85226120828541
At time: 151.86162042617798 and batch: 300, loss is 3.734225130081177 and perplexity is 41.85558036416272
At time: 152.28125667572021 and batch: 350, loss is 3.8141798639297484 and perplexity is 45.33955652484442
At time: 152.70511436462402 and batch: 400, loss is 3.7184960556030275 and perplexity is 41.20238139781977
At time: 153.13689494132996 and batch: 450, loss is 3.7834954690933227 and perplexity is 43.96946734627587
At time: 153.55374360084534 and batch: 500, loss is 3.7037117004394533 and perplexity is 40.59771159806454
At time: 153.97581315040588 and batch: 550, loss is 3.7729834365844725 and perplexity is 43.50967976113397
At time: 154.3996934890747 and batch: 600, loss is 3.782236614227295 and perplexity is 43.91415099328549
At time: 154.81209301948547 and batch: 650, loss is 3.756419620513916 and perplexity is 42.79492926764123
At time: 155.23944997787476 and batch: 700, loss is 3.7248601293563843 and perplexity is 41.465432542463674
At time: 155.6926383972168 and batch: 750, loss is 3.690464596748352 and perplexity is 40.06345598548359
At time: 156.13030219078064 and batch: 800, loss is 3.6754432773590087 and perplexity is 39.46614742992966
At time: 156.5480751991272 and batch: 850, loss is 3.668941297531128 and perplexity is 39.210371760668636
At time: 156.9625973701477 and batch: 900, loss is 3.8468307781219484 and perplexity is 46.84436753675158
At time: 157.37040615081787 and batch: 950, loss is 3.78161750793457 and perplexity is 43.88697188031394
At time: 157.80592560768127 and batch: 1000, loss is 3.711943573951721 and perplexity is 40.93328613341176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.36687153141673 and perplexity of 78.79673259051101
Finished 17 epochs...
Completing Train Step...
At time: 159.11142802238464 and batch: 50, loss is 3.8425952434539794 and perplexity is 46.6463761896508
At time: 159.53343987464905 and batch: 100, loss is 3.723863306045532 and perplexity is 41.42411942706993
At time: 159.93782138824463 and batch: 150, loss is 3.7685408401489258 and perplexity is 43.31681254573005
At time: 160.35222554206848 and batch: 200, loss is 3.813753809928894 and perplexity is 45.320243539869445
At time: 160.77072858810425 and batch: 250, loss is 3.7887687349319457 and perplexity is 44.20194244917553
At time: 161.18235850334167 and batch: 300, loss is 3.7204363632202146 and perplexity is 41.28240430171138
At time: 161.59881162643433 and batch: 350, loss is 3.801022834777832 and perplexity is 44.74692981037327
At time: 162.0054361820221 and batch: 400, loss is 3.7057319688796997 and perplexity is 40.679812778839995
At time: 162.42162013053894 and batch: 450, loss is 3.7711968517303465 and perplexity is 43.43201542388714
At time: 162.84958839416504 and batch: 500, loss is 3.691808476448059 and perplexity is 40.11733264444824
At time: 163.27228331565857 and batch: 550, loss is 3.762048625946045 and perplexity is 43.03650142458324
At time: 163.70231223106384 and batch: 600, loss is 3.7717546129226687 and perplexity is 43.45624687364788
At time: 164.1232714653015 and batch: 650, loss is 3.7465140628814697 and perplexity is 42.37311423499999
At time: 164.53878688812256 and batch: 700, loss is 3.7162730646133424 and perplexity is 41.11089060450002
At time: 164.96946358680725 and batch: 750, loss is 3.6823157835006715 and perplexity is 39.7383129288334
At time: 165.38220596313477 and batch: 800, loss is 3.6679686641693117 and perplexity is 39.172252985766164
At time: 165.79024505615234 and batch: 850, loss is 3.661734690666199 and perplexity is 38.92881378374914
At time: 166.2010190486908 and batch: 900, loss is 3.840791554450989 and perplexity is 46.562316465477465
At time: 166.6338028907776 and batch: 950, loss is 3.7767369747161865 and perplexity is 43.673301892063236
At time: 167.0589418411255 and batch: 1000, loss is 3.7076328229904174 and perplexity is 40.757212707865094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.366250759217797 and perplexity of 78.74783294889141
Finished 18 epochs...
Completing Train Step...
At time: 168.3764190673828 and batch: 50, loss is 3.830024700164795 and perplexity is 46.06367600055118
At time: 168.82997226715088 and batch: 100, loss is 3.7113689613342284 and perplexity is 40.90977210709916
At time: 169.26525950431824 and batch: 150, loss is 3.756004128456116 and perplexity is 42.77715200782728
At time: 169.69104719161987 and batch: 200, loss is 3.8016082525253294 and perplexity is 44.773133126425066
At time: 170.12294340133667 and batch: 250, loss is 3.777363314628601 and perplexity is 43.70066479248895
At time: 170.5459840297699 and batch: 300, loss is 3.709468765258789 and perplexity is 40.83210932931528
At time: 170.97176265716553 and batch: 350, loss is 3.790605697631836 and perplexity is 44.283214392627826
At time: 171.39284539222717 and batch: 400, loss is 3.6953712368011473 and perplexity is 40.26051599920002
At time: 171.80947542190552 and batch: 450, loss is 3.761108422279358 and perplexity is 42.99605736394884
At time: 172.24542117118835 and batch: 500, loss is 3.6819430017471313 and perplexity is 39.72350197165588
At time: 172.6671199798584 and batch: 550, loss is 3.7528366899490355 and perplexity is 42.64187236740689
At time: 173.08934497833252 and batch: 600, loss is 3.7628506565093995 and perplexity is 43.0710318594467
At time: 173.52024793624878 and batch: 650, loss is 3.737926378250122 and perplexity is 42.01078530316145
At time: 173.9386682510376 and batch: 700, loss is 3.708523464202881 and perplexity is 40.79352893116981
At time: 174.3575155735016 and batch: 750, loss is 3.674750609397888 and perplexity is 39.43881995958029
At time: 174.78083300590515 and batch: 800, loss is 3.6606777477264405 and perplexity is 38.887689985449754
At time: 175.19958329200745 and batch: 850, loss is 3.6546047353744506 and perplexity is 38.65224023210209
At time: 175.60701608657837 and batch: 900, loss is 3.8342541360855105 and perplexity is 46.25891194439135
At time: 176.02509474754333 and batch: 950, loss is 3.7710673999786377 and perplexity is 43.42639343730613
At time: 176.4436047077179 and batch: 1000, loss is 3.702193326950073 and perplexity is 40.5361158835284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3661692549542686 and perplexity of 78.7414149263141
Finished 19 epochs...
Completing Train Step...
At time: 177.77285838127136 and batch: 50, loss is 3.8191691732406614 and perplexity is 45.56633485955847
At time: 178.20034384727478 and batch: 100, loss is 3.700590353012085 and perplexity is 40.47118959769436
At time: 178.6202118396759 and batch: 150, loss is 3.7451998472213743 and perplexity is 42.31746340131374
At time: 179.03362607955933 and batch: 200, loss is 3.7911261177062987 and perplexity is 44.306266264167554
At time: 179.4358777999878 and batch: 250, loss is 3.7674578762054445 and perplexity is 43.26992739164445
At time: 179.84839034080505 and batch: 300, loss is 3.699877634048462 and perplexity is 40.44235528998723
At time: 180.26441502571106 and batch: 350, loss is 3.7814503145217895 and perplexity is 43.87963488107473
At time: 180.6766996383667 and batch: 400, loss is 3.686195344924927 and perplexity is 39.89277959245213
At time: 181.07958102226257 and batch: 450, loss is 3.7521222114562987 and perplexity is 42.611416548019896
At time: 181.5011763572693 and batch: 500, loss is 3.6730404138565063 and perplexity is 39.37142950737863
At time: 181.91746711730957 and batch: 550, loss is 3.7444331645965576 and perplexity is 42.28503177136984
At time: 182.324800491333 and batch: 600, loss is 3.754700255393982 and perplexity is 42.721412378247074
At time: 182.7498688697815 and batch: 650, loss is 3.729982852935791 and perplexity is 41.67839349581268
At time: 183.1601243019104 and batch: 700, loss is 3.7011509466171266 and perplexity is 40.49388384830281
At time: 183.57574772834778 and batch: 750, loss is 3.667482304573059 and perplexity is 39.15320581688184
At time: 183.98842525482178 and batch: 800, loss is 3.6534885692596437 and perplexity is 38.609121979336464
At time: 184.4050998687744 and batch: 850, loss is 3.647544431686401 and perplexity is 38.380304780890945
At time: 184.83422827720642 and batch: 900, loss is 3.8275473308563233 and perplexity is 45.94970050138618
At time: 185.25560021400452 and batch: 950, loss is 3.765115747451782 and perplexity is 43.168702237976206
At time: 185.66576170921326 and batch: 1000, loss is 3.6963061046600343 and perplexity is 40.29817186047454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.366417117235137 and perplexity of 78.76093437198382
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 186.92745184898376 and batch: 50, loss is 3.819974675178528 and perplexity is 45.603053417038566
At time: 187.3629970550537 and batch: 100, loss is 3.704040193557739 and perplexity is 40.611049857585954
At time: 187.77475237846375 and batch: 150, loss is 3.7476983213424684 and perplexity is 42.42332467925516
At time: 188.2133674621582 and batch: 200, loss is 3.792163028717041 and perplexity is 44.35223174644065
At time: 188.6353280544281 and batch: 250, loss is 3.76676682472229 and perplexity is 43.24003597358842
At time: 189.05849266052246 and batch: 300, loss is 3.6968585681915282 and perplexity is 40.32044128176848
At time: 189.4839243888855 and batch: 350, loss is 3.7786327695846555 and perplexity is 43.75617604505522
At time: 189.90905809402466 and batch: 400, loss is 3.679394717216492 and perplexity is 39.62240405391735
At time: 190.3172948360443 and batch: 450, loss is 3.745607266426086 and perplexity is 42.334707861221666
At time: 190.73713660240173 and batch: 500, loss is 3.664116325378418 and perplexity is 39.02163849135836
At time: 191.16442322731018 and batch: 550, loss is 3.7335121726989744 and perplexity is 41.82574975439604
At time: 191.59090781211853 and batch: 600, loss is 3.740597047805786 and perplexity is 42.123132182399225
At time: 192.0004801750183 and batch: 650, loss is 3.713347592353821 and perplexity is 40.99079758452179
At time: 192.42075634002686 and batch: 700, loss is 3.685351486206055 and perplexity is 39.85912992235356
At time: 192.84930753707886 and batch: 750, loss is 3.6462016344070434 and perplexity is 38.32880239841846
At time: 193.271253824234 and batch: 800, loss is 3.6300792026519777 and perplexity is 37.715803690561884
At time: 193.69880890846252 and batch: 850, loss is 3.62162257194519 and perplexity is 37.398199888746184
At time: 194.10454082489014 and batch: 900, loss is 3.8008107709884644 and perplexity is 44.73744161296223
At time: 194.52601718902588 and batch: 950, loss is 3.735321888923645 and perplexity is 41.901511024858785
At time: 194.95352458953857 and batch: 1000, loss is 3.6689618015289307 and perplexity is 39.21117573828741
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.348390532702934 and perplexity of 77.3538641686643
Finished 21 epochs...
Completing Train Step...
At time: 196.2672758102417 and batch: 50, loss is 3.8095149755477906 and perplexity is 45.12854510937351
At time: 196.68569779396057 and batch: 100, loss is 3.6894605684280397 and perplexity is 40.023251327749506
At time: 197.1102330684662 and batch: 150, loss is 3.734300765991211 and perplexity is 41.858746268800104
At time: 197.5307638645172 and batch: 200, loss is 3.779755725860596 and perplexity is 43.80533991682891
At time: 197.94699954986572 and batch: 250, loss is 3.7558005237579346 and perplexity is 42.76844326530421
At time: 198.37097573280334 and batch: 300, loss is 3.68706006526947 and perplexity is 39.927290609604974
At time: 198.78665733337402 and batch: 350, loss is 3.769208941459656 and perplexity is 43.3457622345542
At time: 199.2223596572876 and batch: 400, loss is 3.670800666809082 and perplexity is 39.283346143436034
At time: 199.63481044769287 and batch: 450, loss is 3.7384435892105103 and perplexity is 42.03251936183694
At time: 200.04998302459717 and batch: 500, loss is 3.657252345085144 and perplexity is 38.75471187127502
At time: 200.47618293762207 and batch: 550, loss is 3.7274719047546387 and perplexity is 41.57387248780436
At time: 200.89810848236084 and batch: 600, loss is 3.73545446395874 and perplexity is 41.907066487403135
At time: 201.31689023971558 and batch: 650, loss is 3.7091682147979737 and perplexity is 40.819839064049596
At time: 201.74003624916077 and batch: 700, loss is 3.681879324913025 and perplexity is 39.72097258534321
At time: 202.16054391860962 and batch: 750, loss is 3.6438725757598878 and perplexity is 38.23963624666261
At time: 202.57379484176636 and batch: 800, loss is 3.62858003616333 and perplexity is 37.65930378354215
At time: 202.99108624458313 and batch: 850, loss is 3.6209801054000854 and perplexity is 37.37418051311935
At time: 203.41148138046265 and batch: 900, loss is 3.8010295534133913 and perplexity is 44.74723044969701
At time: 203.8273949623108 and batch: 950, loss is 3.736729869842529 and perplexity is 41.960549105346466
At time: 204.2339391708374 and batch: 1000, loss is 3.6709582376480103 and perplexity is 39.28953654094402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.347734776938834 and perplexity of 77.30315555441044
Finished 22 epochs...
Completing Train Step...
At time: 205.60379076004028 and batch: 50, loss is 3.8045599603652955 and perplexity is 44.90548557127448
At time: 206.01956605911255 and batch: 100, loss is 3.684285306930542 and perplexity is 39.81665559073994
At time: 206.44054508209229 and batch: 150, loss is 3.728719515800476 and perplexity is 41.62577287935721
At time: 206.84990119934082 and batch: 200, loss is 3.7743113899230956 and perplexity is 43.56749696641405
At time: 207.27204751968384 and batch: 250, loss is 3.750665965080261 and perplexity is 42.54940898719494
At time: 207.6922037601471 and batch: 300, loss is 3.682282576560974 and perplexity is 39.736993362981785
At time: 208.10949873924255 and batch: 350, loss is 3.764533448219299 and perplexity is 43.14357245303306
At time: 208.52682209014893 and batch: 400, loss is 3.666431760787964 and perplexity is 39.11209525784646
At time: 208.95474123954773 and batch: 450, loss is 3.7344807243347167 and perplexity is 41.86627977727838
At time: 209.3763711452484 and batch: 500, loss is 3.6533823251724242 and perplexity is 38.60502020631185
At time: 209.82426953315735 and batch: 550, loss is 3.7240249872207642 and perplexity is 41.4308174688409
At time: 210.25943231582642 and batch: 600, loss is 3.732261061668396 and perplexity is 41.773453818348806
At time: 210.69025921821594 and batch: 650, loss is 3.7064518404006956 and perplexity is 40.70910756051337
At time: 211.106835603714 and batch: 700, loss is 3.6795519161224366 and perplexity is 39.62863314207562
At time: 211.51632857322693 and batch: 750, loss is 3.6419528484344483 and perplexity is 38.166296990255724
At time: 211.93899607658386 and batch: 800, loss is 3.6270277070999146 and perplexity is 37.600889502594185
At time: 212.37276339530945 and batch: 850, loss is 3.6196937704086305 and perplexity is 37.32613570443247
At time: 212.81360578536987 and batch: 900, loss is 3.800087738037109 and perplexity is 44.70510665954379
At time: 213.2332639694214 and batch: 950, loss is 3.7363753795623778 and perplexity is 41.94567713467907
At time: 213.65478467941284 and batch: 1000, loss is 3.670786862373352 and perplexity is 39.282803862751884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.347505150771722 and perplexity of 77.28540676496605
Finished 23 epochs...
Completing Train Step...
At time: 214.93134450912476 and batch: 50, loss is 3.8003031396865845 and perplexity is 44.71473725044357
At time: 215.35381531715393 and batch: 100, loss is 3.680038809776306 and perplexity is 39.64793277011596
At time: 215.7759611606598 and batch: 150, loss is 3.7243225717544557 and perplexity is 41.44314847400503
At time: 216.19170880317688 and batch: 200, loss is 3.770071949958801 and perplexity is 43.383186142016925
At time: 216.60580778121948 and batch: 250, loss is 3.746673164367676 and perplexity is 42.379856396779715
At time: 217.01244497299194 and batch: 300, loss is 3.6785487031936643 and perplexity is 39.58889712013604
At time: 217.42933773994446 and batch: 350, loss is 3.7609050846099854 and perplexity is 42.98731553465397
At time: 217.86850833892822 and batch: 400, loss is 3.662977685928345 and perplexity is 38.97723220054062
At time: 218.29626750946045 and batch: 450, loss is 3.731259722709656 and perplexity is 41.73164536730815
At time: 218.72446179389954 and batch: 500, loss is 3.6502116250991823 and perplexity is 38.48280911565901
At time: 219.1637408733368 and batch: 550, loss is 3.721159381866455 and perplexity is 41.31226304269566
At time: 219.58286786079407 and batch: 600, loss is 3.729513530731201 and perplexity is 41.65883748968637
At time: 220.00251078605652 and batch: 650, loss is 3.7040155601501463 and perplexity is 40.610049481363426
At time: 220.43098211288452 and batch: 700, loss is 3.6773884201049807 and perplexity is 39.54298943041621
At time: 220.86360335350037 and batch: 750, loss is 3.6400089025497437 and perplexity is 38.09217584137979
At time: 221.27897667884827 and batch: 800, loss is 3.625302858352661 and perplexity is 37.53608955655842
At time: 221.7035059928894 and batch: 850, loss is 3.618106870651245 and perplexity is 37.2669498421746
At time: 222.13383150100708 and batch: 900, loss is 3.7987138557434084 and perplexity is 44.64372927736996
At time: 222.5382204055786 and batch: 950, loss is 3.735408263206482 and perplexity is 41.90513039413131
At time: 222.94463992118835 and batch: 1000, loss is 3.66991886138916 and perplexity is 39.248721144393635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.347440393959603 and perplexity of 77.2804021704432
Finished 24 epochs...
Completing Train Step...
At time: 224.22795343399048 and batch: 50, loss is 3.796489453315735 and perplexity is 44.54453402356789
At time: 224.6508071422577 and batch: 100, loss is 3.676266841888428 and perplexity is 39.498663736866426
At time: 225.0664415359497 and batch: 150, loss is 3.7204779148101808 and perplexity is 41.28411968688599
At time: 225.4739625453949 and batch: 200, loss is 3.7663738775253295 and perplexity is 43.22304826051183
At time: 225.90650987625122 and batch: 250, loss is 3.743183946609497 and perplexity is 42.23224152922277
At time: 226.32661652565002 and batch: 300, loss is 3.6752683687210084 and perplexity is 39.45924506349493
At time: 226.75133800506592 and batch: 350, loss is 3.757728500366211 and perplexity is 42.85097936175756
At time: 227.17077946662903 and batch: 400, loss is 3.659920301437378 and perplexity is 38.858245801581695
At time: 227.58660507202148 and batch: 450, loss is 3.728363580703735 and perplexity is 41.610959442327946
At time: 227.99548053741455 and batch: 500, loss is 3.6473402833938597 and perplexity is 38.37247030692716
At time: 228.42276525497437 and batch: 550, loss is 3.718532190322876 and perplexity is 41.20387026122833
At time: 228.84456968307495 and batch: 600, loss is 3.7269568109512328 and perplexity is 41.552463557979195
At time: 229.26043796539307 and batch: 650, loss is 3.7016905450820925 and perplexity is 40.51574018215963
At time: 229.67710709571838 and batch: 700, loss is 3.675276656150818 and perplexity is 39.45957208057379
At time: 230.08773612976074 and batch: 750, loss is 3.638046426773071 and perplexity is 38.01749417346098
At time: 230.50691771507263 and batch: 800, loss is 3.6234803438186645 and perplexity is 37.4677417891104
At time: 230.93276739120483 and batch: 850, loss is 3.616379113197327 and perplexity is 37.202617183418866
At time: 231.360116481781 and batch: 900, loss is 3.797142424583435 and perplexity is 44.573629822745005
At time: 231.76992011070251 and batch: 950, loss is 3.734161524772644 and perplexity is 41.85291821172424
At time: 232.1968765258789 and batch: 1000, loss is 3.668733630180359 and perplexity is 39.202229892071834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.347465329053925 and perplexity of 77.28232918858563
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 233.59519934654236 and batch: 50, loss is 3.7983673810958862 and perplexity is 44.62826403631619
At time: 234.03232073783875 and batch: 100, loss is 3.680534119606018 and perplexity is 39.66757564519773
At time: 234.4568178653717 and batch: 150, loss is 3.723625364303589 and perplexity is 41.4142640724816
At time: 234.901775598526 and batch: 200, loss is 3.7694091606140137 and perplexity is 43.35444175528795
At time: 235.32874584197998 and batch: 250, loss is 3.7463468313217163 and perplexity is 42.36602870549345
At time: 235.76313710212708 and batch: 300, loss is 3.678591513633728 and perplexity is 39.59059197452186
At time: 236.18284606933594 and batch: 350, loss is 3.7608798503875733 and perplexity is 42.98623079685919
At time: 236.61123156547546 and batch: 400, loss is 3.6603100442886354 and perplexity is 38.87339347675258
At time: 237.03572702407837 and batch: 450, loss is 3.7283307552337646 and perplexity is 41.60959356544622
At time: 237.46273851394653 and batch: 500, loss is 3.645324831008911 and perplexity is 38.29521030321288
At time: 237.88733077049255 and batch: 550, loss is 3.718289465904236 and perplexity is 41.19387028944123
At time: 238.30042505264282 and batch: 600, loss is 3.722833399772644 and perplexity is 41.38147842850821
At time: 238.72997546195984 and batch: 650, loss is 3.696501541137695 and perplexity is 40.306048362892
At time: 239.14179348945618 and batch: 700, loss is 3.671390380859375 and perplexity is 39.30651891658273
At time: 239.55694103240967 and batch: 750, loss is 3.6329247093200685 and perplexity is 37.82327709708639
At time: 239.96852850914001 and batch: 800, loss is 3.616878981590271 and perplexity is 37.2212182445376
At time: 240.392502784729 and batch: 850, loss is 3.607677516937256 and perplexity is 36.88029940223337
At time: 240.81900477409363 and batch: 900, loss is 3.7889456367492675 and perplexity is 44.209762544798124
At time: 241.2558777332306 and batch: 950, loss is 3.724689426422119 and perplexity is 41.45835487556454
At time: 241.68424129486084 and batch: 1000, loss is 3.6581823110580443 and perplexity is 38.790769198049695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3413539979516 and perplexity of 76.81147153554471
Finished 26 epochs...
Completing Train Step...
At time: 243.03427267074585 and batch: 50, loss is 3.79515917301178 and perplexity is 44.48531670385732
At time: 243.46449255943298 and batch: 100, loss is 3.6741781997680665 and perplexity is 39.41625125913359
At time: 243.88131499290466 and batch: 150, loss is 3.718479161262512 and perplexity is 41.20168531663832
At time: 244.28957772254944 and batch: 200, loss is 3.7647014236450196 and perplexity is 43.15082012168103
At time: 244.6993293762207 and batch: 250, loss is 3.7424392986297605 and perplexity is 42.20080508188441
At time: 245.1144711971283 and batch: 300, loss is 3.6745247650146484 and perplexity is 39.42991392933713
At time: 245.5263156890869 and batch: 350, loss is 3.756891303062439 and perplexity is 42.8151196502933
At time: 245.95799589157104 and batch: 400, loss is 3.656925687789917 and perplexity is 38.74205442935325
At time: 246.3831331729889 and batch: 450, loss is 3.7258922052383423 and perplexity is 41.50825010701285
At time: 246.81324577331543 and batch: 500, loss is 3.643337345123291 and perplexity is 38.21917469812338
At time: 247.23859643936157 and batch: 550, loss is 3.7160291481018066 and perplexity is 41.10086420232987
At time: 247.66985273361206 and batch: 600, loss is 3.721302890777588 and perplexity is 41.31819214601075
At time: 248.09046387672424 and batch: 650, loss is 3.6953482007980347 and perplexity is 40.25958856851033
At time: 248.50868272781372 and batch: 700, loss is 3.6703870582580564 and perplexity is 39.26710157523595
At time: 248.9271764755249 and batch: 750, loss is 3.6323067903518678 and perplexity is 37.79991259615616
At time: 249.35209822654724 and batch: 800, loss is 3.6167744255065917 and perplexity is 37.217326743171796
At time: 249.760516166687 and batch: 850, loss is 3.6080437517166137 and perplexity is 36.893808724188375
At time: 250.16653490066528 and batch: 900, loss is 3.7897845315933227 and perplexity is 44.24686544719471
At time: 250.58844327926636 and batch: 950, loss is 3.7258585691452026 and perplexity is 41.506853955126864
At time: 251.00327515602112 and batch: 1000, loss is 3.659608941078186 and perplexity is 38.84614876757751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.341122510956555 and perplexity of 76.79369273667368
Finished 27 epochs...
Completing Train Step...
At time: 252.28776240348816 and batch: 50, loss is 3.7936514234542846 and perplexity is 44.41829452630815
At time: 252.72234225273132 and batch: 100, loss is 3.6723321151733397 and perplexity is 39.343552649440504
At time: 253.13639569282532 and batch: 150, loss is 3.716433410644531 and perplexity is 41.1174831011733
At time: 253.5759220123291 and batch: 200, loss is 3.7626639366149903 and perplexity is 43.062990391700176
At time: 254.01473236083984 and batch: 250, loss is 3.7404538869857786 and perplexity is 42.11710223189134
At time: 254.4183325767517 and batch: 300, loss is 3.672700214385986 and perplexity is 39.358037645987835
At time: 254.83409762382507 and batch: 350, loss is 3.7551372766494753 and perplexity is 42.74008662374495
At time: 255.26099300384521 and batch: 400, loss is 3.6553522396087645 and perplexity is 38.68114374674525
At time: 255.67661237716675 and batch: 450, loss is 3.7245315504074097 and perplexity is 41.451810112364534
At time: 256.0988140106201 and batch: 500, loss is 3.6421168756484987 and perplexity is 38.172557815080445
At time: 256.5209541320801 and batch: 550, loss is 3.714903531074524 and perplexity is 41.05462639765982
At time: 256.9515223503113 and batch: 600, loss is 3.720377779006958 and perplexity is 41.27998587537552
At time: 257.36534571647644 and batch: 650, loss is 3.6946214485168456 and perplexity is 40.23034045003417
At time: 257.7811908721924 and batch: 700, loss is 3.6696909952163694 and perplexity is 39.239778707397676
At time: 258.19416642189026 and batch: 750, loss is 3.6318727779388427 and perplexity is 37.783510524486985
At time: 258.6161539554596 and batch: 800, loss is 3.616556386947632 and perplexity is 37.20921281548707
At time: 259.03154730796814 and batch: 850, loss is 3.6080070161819457 and perplexity is 36.89245343529274
At time: 259.4387717247009 and batch: 900, loss is 3.7899671983718872 and perplexity is 44.25494861780841
At time: 259.86314487457275 and batch: 950, loss is 3.7261930227279665 and perplexity is 41.52073839286196
At time: 260.289297580719 and batch: 1000, loss is 3.6599958610534666 and perplexity is 38.86118202664497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.341052543826219 and perplexity of 76.78831989032852
Finished 28 epochs...
Completing Train Step...
At time: 261.60216546058655 and batch: 50, loss is 3.7922987842559817 and perplexity is 44.358253216279294
At time: 262.0243377685547 and batch: 100, loss is 3.6708763599395753 and perplexity is 39.286319735420705
At time: 262.45247650146484 and batch: 150, loss is 3.7148459005355834 and perplexity is 41.05226046559014
At time: 262.8611567020416 and batch: 200, loss is 3.7611372423172 and perplexity is 42.997296529805446
At time: 263.2839150428772 and batch: 250, loss is 3.7389693689346313 and perplexity is 42.05462501911506
At time: 263.70063495635986 and batch: 300, loss is 3.671332001686096 and perplexity is 39.304224301483444
At time: 264.133504152298 and batch: 350, loss is 3.753820581436157 and perplexity is 42.683847988966285
At time: 264.5385887622833 and batch: 400, loss is 3.6541437530517578 and perplexity is 38.63442633888301
At time: 264.956218957901 and batch: 450, loss is 3.723444032669067 and perplexity is 41.40675503711823
At time: 265.3905234336853 and batch: 500, loss is 3.6410911750793455 and perplexity is 38.133424273882135
At time: 265.8081045150757 and batch: 550, loss is 3.713991813659668 and perplexity is 41.017213237519066
At time: 266.22664999961853 and batch: 600, loss is 3.719556040763855 and perplexity is 41.24607846572259
At time: 266.6340982913971 and batch: 650, loss is 3.6939466381073 and perplexity is 40.20320175528562
At time: 267.045734167099 and batch: 700, loss is 3.669046764373779 and perplexity is 39.21450737285856
At time: 267.4723505973816 and batch: 750, loss is 3.631418590545654 and perplexity is 37.7663536268546
At time: 267.8914427757263 and batch: 800, loss is 3.616222095489502 and perplexity is 37.19677617232673
At time: 268.3018217086792 and batch: 850, loss is 3.6077734565734865 and perplexity is 36.883837854478294
At time: 268.72987627983093 and batch: 900, loss is 3.789875907897949 and perplexity is 44.25090874697865
At time: 269.1489489078522 and batch: 950, loss is 3.726210060119629 and perplexity is 41.52144580397027
At time: 269.57155084609985 and batch: 1000, loss is 3.6600300216674806 and perplexity is 38.862509571159045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3410227705792686 and perplexity of 76.78603368675154
Finished 29 epochs...
Completing Train Step...
At time: 270.91278290748596 and batch: 50, loss is 3.791045069694519 and perplexity is 44.302675474892595
At time: 271.34754180908203 and batch: 100, loss is 3.669585704803467 and perplexity is 39.23564735239522
At time: 271.78043699264526 and batch: 150, loss is 3.7134690284729004 and perplexity is 40.995775650150804
At time: 272.1977741718292 and batch: 200, loss is 3.7598295831680297 and perplexity is 42.94110746768606
At time: 272.6201329231262 and batch: 250, loss is 3.737707667350769 and perplexity is 42.00159809123358
At time: 273.02854561805725 and batch: 300, loss is 3.670158233642578 and perplexity is 39.25811732376519
At time: 273.4675831794739 and batch: 350, loss is 3.752687487602234 and perplexity is 42.63551057458667
At time: 273.881085395813 and batch: 400, loss is 3.653086290359497 and perplexity is 38.59359346781667
At time: 274.29484963417053 and batch: 450, loss is 3.7224782609939577 and perplexity is 41.366784870079506
At time: 274.7245454788208 and batch: 500, loss is 3.6401587390899657 and perplexity is 38.09788386884225
At time: 275.1715724468231 and batch: 550, loss is 3.713165183067322 and perplexity is 40.983321164286345
At time: 275.5941183567047 and batch: 600, loss is 3.7187767219543457 and perplexity is 41.21394714285412
At time: 276.0091109275818 and batch: 650, loss is 3.693285493850708 and perplexity is 40.176630424057905
At time: 276.4321050643921 and batch: 700, loss is 3.668413987159729 and perplexity is 39.18970117535881
At time: 276.8464415073395 and batch: 750, loss is 3.6309357500076294 and perplexity is 37.748122901970866
At time: 277.2686040401459 and batch: 800, loss is 3.615818200111389 and perplexity is 37.1817555999241
At time: 277.6865508556366 and batch: 850, loss is 3.60743736743927 and perplexity is 36.87144368023751
At time: 278.11127829551697 and batch: 900, loss is 3.789648404121399 and perplexity is 44.24084264320467
At time: 278.52813601493835 and batch: 950, loss is 3.7260713005065917 and perplexity is 41.51568470393102
At time: 278.95125436782837 and batch: 1000, loss is 3.6598984813690185 and perplexity is 38.85739792125242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.341013466439596 and perplexity of 76.78531926209274
Finished 30 epochs...
Completing Train Step...
At time: 280.23648047447205 and batch: 50, loss is 3.7898608446121216 and perplexity is 44.25024218791236
At time: 280.6682868003845 and batch: 100, loss is 3.6683904552459716 and perplexity is 39.18877897754115
At time: 281.10179114341736 and batch: 150, loss is 3.7122157526016237 and perplexity is 40.94442881629904
At time: 281.5242040157318 and batch: 200, loss is 3.7586441326141355 and perplexity is 42.89023306855377
At time: 281.9418125152588 and batch: 250, loss is 3.7365688419342042 and perplexity is 41.953792829880925
At time: 282.36758279800415 and batch: 300, loss is 3.669091248512268 and perplexity is 39.21625183523547
At time: 282.81665205955505 and batch: 350, loss is 3.751655087471008 and perplexity is 42.5915163815875
At time: 283.2596230506897 and batch: 400, loss is 3.6521122312545775 and perplexity is 38.55601932939443
At time: 283.7027311325073 and batch: 450, loss is 3.7215813255310057 and perplexity is 41.32969816841652
At time: 284.1257584095001 and batch: 500, loss is 3.63928138256073 and perplexity is 38.06447310040096
At time: 284.5510540008545 and batch: 550, loss is 3.712384333610535 and perplexity is 40.95133185126213
At time: 284.97669553756714 and batch: 600, loss is 3.7180228090286254 and perplexity is 41.18288712512876
At time: 285.4086072444916 and batch: 650, loss is 3.692630705833435 and perplexity is 40.150331858814155
At time: 285.85186743736267 and batch: 700, loss is 3.6677846908569336 and perplexity is 39.16504699950596
At time: 286.27302289009094 and batch: 750, loss is 3.6304313707351685 and perplexity is 37.72908833192963
At time: 286.68992853164673 and batch: 800, loss is 3.6153722906112673 and perplexity is 37.165179597843945
At time: 287.10670709609985 and batch: 850, loss is 3.607042098045349 and perplexity is 36.856872407019914
At time: 287.53828477859497 and batch: 900, loss is 3.789343523979187 and perplexity is 44.2273565447343
At time: 287.95951986312866 and batch: 950, loss is 3.725843224525452 and perplexity is 41.50621705312239
At time: 288.38077211380005 and batch: 1000, loss is 3.659674677848816 and perplexity is 38.84870247188412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3410145829363564 and perplexity of 76.78540499270083
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 289.667849779129 and batch: 50, loss is 3.790763511657715 and perplexity is 44.290203456441624
At time: 290.0972692966461 and batch: 100, loss is 3.6704515266418456 and perplexity is 39.26963314341277
At time: 290.519903421402 and batch: 150, loss is 3.7141917943954468 and perplexity is 41.02541671024281
At time: 290.9375286102295 and batch: 200, loss is 3.7602731561660767 and perplexity is 42.96015920857149
At time: 291.347291469574 and batch: 250, loss is 3.7381007862091065 and perplexity is 42.01811295746354
At time: 291.77118277549744 and batch: 300, loss is 3.671387848854065 and perplexity is 39.306419392394126
At time: 292.2131288051605 and batch: 350, loss is 3.753375382423401 and perplexity is 42.664849411368756
At time: 292.64798283576965 and batch: 400, loss is 3.6537148094177248 and perplexity is 38.617857901367366
At time: 293.0715696811676 and batch: 450, loss is 3.7224426221847535 and perplexity is 41.365310633396305
At time: 293.50127482414246 and batch: 500, loss is 3.6381898593902586 and perplexity is 38.02294751323324
At time: 293.9152889251709 and batch: 550, loss is 3.7130410623550416 and perplexity is 40.97823460095229
At time: 294.33780169487 and batch: 600, loss is 3.7171779537200926 and perplexity is 41.14810823795282
At time: 294.76907539367676 and batch: 650, loss is 3.6909905910491942 and perplexity is 40.08453467815412
At time: 295.1758472919464 and batch: 700, loss is 3.665222587585449 and perplexity is 39.064830541739354
At time: 295.59436440467834 and batch: 750, loss is 3.6281859064102173 and perplexity is 37.644464056020716
At time: 296.0147216320038 and batch: 800, loss is 3.6132405900955202 and perplexity is 37.08603894737899
At time: 296.44568157196045 and batch: 850, loss is 3.603913731575012 and perplexity is 36.7417507684056
At time: 296.8900775909424 and batch: 900, loss is 3.786351056098938 and perplexity is 44.09520542844754
At time: 297.3280653953552 and batch: 950, loss is 3.7221289587020876 and perplexity is 41.35233788064724
At time: 297.746267080307 and batch: 1000, loss is 3.6544568872451784 and perplexity is 38.64652599312177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339510289634147 and perplexity of 76.66998405749821
Finished 32 epochs...
Completing Train Step...
At time: 299.0399100780487 and batch: 50, loss is 3.790108289718628 and perplexity is 44.26119304861605
At time: 299.4637613296509 and batch: 100, loss is 3.66814377784729 and perplexity is 39.179113183700906
At time: 299.88529443740845 and batch: 150, loss is 3.7126859521865843 and perplexity is 40.96368539659816
At time: 300.29689621925354 and batch: 200, loss is 3.7585639715194703 and perplexity is 42.88679507831891
At time: 300.7307481765747 and batch: 250, loss is 3.7364535093307496 and perplexity is 41.948954468744816
At time: 301.1627206802368 and batch: 300, loss is 3.669590811729431 and perplexity is 39.23584772645306
At time: 301.58849811553955 and batch: 350, loss is 3.7517826080322267 and perplexity is 42.59694802197519
At time: 302.0179491043091 and batch: 400, loss is 3.652247681617737 and perplexity is 38.56124210992036
At time: 302.43781328201294 and batch: 450, loss is 3.7213248205184937 and perplexity is 41.319098253194525
At time: 302.877747297287 and batch: 500, loss is 3.6374693536758422 and perplexity is 37.99556162929867
At time: 303.28709506988525 and batch: 550, loss is 3.7118691539764406 and perplexity is 40.930239992617835
At time: 303.70575284957886 and batch: 600, loss is 3.7165146541595457 and perplexity is 41.12082376573082
At time: 304.12522411346436 and batch: 650, loss is 3.6905558061599733 and perplexity is 40.0671103163837
At time: 304.56020045280457 and batch: 700, loss is 3.66496178150177 and perplexity is 39.05464352475191
At time: 304.9814386367798 and batch: 750, loss is 3.628156991004944 and perplexity is 37.643375566823366
At time: 305.39862728118896 and batch: 800, loss is 3.6133599042892457 and perplexity is 37.09046410220111
At time: 305.80569672584534 and batch: 850, loss is 3.6041843271255494 and perplexity is 36.75169426795518
At time: 306.21042680740356 and batch: 900, loss is 3.7868914127349855 and perplexity is 44.11903900405412
At time: 306.62835025787354 and batch: 950, loss is 3.7228623819351196 and perplexity is 41.38267777061919
At time: 307.0428194999695 and batch: 1000, loss is 3.6552159881591795 and perplexity is 38.67587374386906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339426924542683 and perplexity of 76.66359272367549
Finished 33 epochs...
Completing Train Step...
At time: 308.3636202812195 and batch: 50, loss is 3.789626021385193 and perplexity is 44.23985242317621
At time: 308.7975113391876 and batch: 100, loss is 3.667424273490906 and perplexity is 39.150933779903546
At time: 309.21536922454834 and batch: 150, loss is 3.712016644477844 and perplexity is 40.936277259445724
At time: 309.64245343208313 and batch: 200, loss is 3.7578419256210327 and perplexity is 42.855840020666555
At time: 310.06375074386597 and batch: 250, loss is 3.7357383823394774 and perplexity is 41.91896636307847
At time: 310.491215467453 and batch: 300, loss is 3.6688374662399292 and perplexity is 39.206300708495306
At time: 310.92376494407654 and batch: 350, loss is 3.7510677003860473 and perplexity is 42.566506021036545
At time: 311.3447422981262 and batch: 400, loss is 3.651624321937561 and perplexity is 38.53721207682653
At time: 311.7573425769806 and batch: 450, loss is 3.720831108093262 and perplexity is 41.29870353596448
At time: 312.18407106399536 and batch: 500, loss is 3.637092685699463 and perplexity is 37.98125261303165
At time: 312.5973262786865 and batch: 550, loss is 3.7114107275009154 and perplexity is 40.91148078714237
At time: 313.0127589702606 and batch: 600, loss is 3.7162206602096557 and perplexity is 41.10873626924168
At time: 313.42093801498413 and batch: 650, loss is 3.690335669517517 and perplexity is 40.05829104800341
At time: 313.8300404548645 and batch: 700, loss is 3.664812250137329 and perplexity is 39.04880406721981
At time: 314.2367649078369 and batch: 750, loss is 3.628091893196106 and perplexity is 37.640925145316096
At time: 314.65429878234863 and batch: 800, loss is 3.6133927297592163 and perplexity is 37.091681634099615
At time: 315.08531641960144 and batch: 850, loss is 3.6042927885055542 and perplexity is 36.75568062361191
At time: 315.5188491344452 and batch: 900, loss is 3.787133274078369 and perplexity is 44.1297109846142
At time: 315.9391107559204 and batch: 950, loss is 3.723161849975586 and perplexity is 41.395072415847835
At time: 316.36042380332947 and batch: 1000, loss is 3.655506954193115 and perplexity is 38.68712874679369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339397523461319 and perplexity of 76.66133876428275
Finished 34 epochs...
Completing Train Step...
At time: 317.6871461868286 and batch: 50, loss is 3.789194345474243 and perplexity is 44.22075926590567
At time: 318.10406708717346 and batch: 100, loss is 3.6669207191467286 and perplexity is 39.13122411997899
At time: 318.54218769073486 and batch: 150, loss is 3.7114938163757323 and perplexity is 40.91488021727352
At time: 318.9717767238617 and batch: 200, loss is 3.757305088043213 and perplexity is 42.83283956962003
At time: 319.4102747440338 and batch: 250, loss is 3.735210738182068 and perplexity is 41.89685389966143
At time: 319.8458640575409 and batch: 300, loss is 3.668318362236023 and perplexity is 39.185953842345924
At time: 320.2882282733917 and batch: 350, loss is 3.750569453239441 and perplexity is 42.54530266356539
At time: 320.7072162628174 and batch: 400, loss is 3.6511878395080566 and perplexity is 38.52039493133417
At time: 321.1369535923004 and batch: 450, loss is 3.7204666900634766 and perplexity is 41.28365628570038
At time: 321.5580232143402 and batch: 500, loss is 3.6367904233932493 and perplexity is 37.96977404688028
At time: 321.9906301498413 and batch: 550, loss is 3.711098289489746 and perplexity is 40.89870048208176
At time: 322.4021577835083 and batch: 600, loss is 3.7159908056259154 and perplexity is 41.099288323646824
At time: 322.8262691497803 and batch: 650, loss is 3.6901496410369874 and perplexity is 40.05083975808469
At time: 323.2607626914978 and batch: 700, loss is 3.6646585607528688 and perplexity is 39.042803141709896
At time: 323.7147581577301 and batch: 750, loss is 3.6280014371871947 and perplexity is 37.63752045144556
At time: 324.13824343681335 and batch: 800, loss is 3.6133678483963014 and perplexity is 37.09075875398906
At time: 324.5464458465576 and batch: 850, loss is 3.604312400817871 and perplexity is 36.756401494568664
At time: 324.9657633304596 and batch: 900, loss is 3.78724169254303 and perplexity is 44.13449571949719
At time: 325.37431502342224 and batch: 950, loss is 3.723298764228821 and perplexity is 41.400740379278886
At time: 325.7950406074524 and batch: 1000, loss is 3.655636034011841 and perplexity is 38.69212279666793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339379287347561 and perplexity of 76.65994077213523
Finished 35 epochs...
Completing Train Step...
At time: 327.0529022216797 and batch: 50, loss is 3.788796720504761 and perplexity is 44.20317948316399
At time: 327.4819104671478 and batch: 100, loss is 3.666488003730774 and perplexity is 39.11429509904543
At time: 327.90812158584595 and batch: 150, loss is 3.7110368251800536 and perplexity is 40.89618674894253
At time: 328.32508420944214 and batch: 200, loss is 3.756846947669983 and perplexity is 42.813220610974774
At time: 328.73954153060913 and batch: 250, loss is 3.734761667251587 and perplexity is 41.87804346442243
At time: 329.183602809906 and batch: 300, loss is 3.6678923606872558 and perplexity is 39.169264120495235
At time: 329.59982657432556 and batch: 350, loss is 3.7501622295379637 and perplexity is 42.52798073512354
At time: 330.01073694229126 and batch: 400, loss is 3.6508262586593627 and perplexity is 38.50646921203129
At time: 330.42971777915955 and batch: 450, loss is 3.7201528263092043 and perplexity is 41.27070087557165
At time: 330.854772567749 and batch: 500, loss is 3.6365151262283324 and perplexity is 37.95932251443727
At time: 331.2774987220764 and batch: 550, loss is 3.710834183692932 and perplexity is 40.88790032445718
At time: 331.68162202835083 and batch: 600, loss is 3.7157782554626464 and perplexity is 41.090553591520646
At time: 332.09995675086975 and batch: 650, loss is 3.6899698686599733 and perplexity is 40.043640370564866
At time: 332.5274932384491 and batch: 700, loss is 3.6644977951049804 and perplexity is 39.03652690468262
At time: 332.9510736465454 and batch: 750, loss is 3.6278960847854616 and perplexity is 37.63355545713522
At time: 333.3694324493408 and batch: 800, loss is 3.613311047554016 and perplexity is 37.08865202748332
At time: 333.794287443161 and batch: 850, loss is 3.6042868661880494 and perplexity is 36.75546294544573
At time: 334.2165849208832 and batch: 900, loss is 3.7872804307937624 and perplexity is 44.13620544577401
At time: 334.6196668148041 and batch: 950, loss is 3.7233579349517822 and perplexity is 41.4031901634953
At time: 335.0368447303772 and batch: 1000, loss is 3.6556892919540407 and perplexity is 38.694183514381734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339368866711128 and perplexity of 76.6591419309257
Finished 36 epochs...
Completing Train Step...
At time: 336.31788396835327 and batch: 50, loss is 3.7884217882156372 and perplexity is 44.18660939041934
At time: 336.7467579841614 and batch: 100, loss is 3.6660913848876953 and perplexity is 39.09878470863527
At time: 337.15789461135864 and batch: 150, loss is 3.710617117881775 and perplexity is 40.87902592240622
At time: 337.5781497955322 and batch: 200, loss is 3.756433143615723 and perplexity is 42.7955079917396
At time: 337.99527192115784 and batch: 250, loss is 3.734356527328491 and perplexity is 41.861080433546384
At time: 338.4293763637543 and batch: 300, loss is 3.6675142192840577 and perplexity is 39.1544554000701
At time: 338.84515714645386 and batch: 350, loss is 3.7498020792007445 and perplexity is 42.512667026304754
At time: 339.26147174835205 and batch: 400, loss is 3.650502920150757 and perplexity is 38.49402060037082
At time: 339.6789503097534 and batch: 450, loss is 3.71986487865448 and perplexity is 41.258818784837885
At time: 340.1148204803467 and batch: 500, loss is 3.6362530851364134 and perplexity is 37.949376915251825
At time: 340.5446364879608 and batch: 550, loss is 3.710591950416565 and perplexity is 40.877997113889805
At time: 340.95981001853943 and batch: 600, loss is 3.715571069717407 and perplexity is 41.082041096416766
At time: 341.3822648525238 and batch: 650, loss is 3.689790210723877 and perplexity is 40.03644685898717
At time: 341.81083250045776 and batch: 700, loss is 3.6643315744400025 and perplexity is 39.03003876646835
At time: 342.2351770401001 and batch: 750, loss is 3.627780809402466 and perplexity is 37.62921748465194
At time: 342.6580171585083 and batch: 800, loss is 3.6132342195510865 and perplexity is 37.08580268987255
At time: 343.0787374973297 and batch: 850, loss is 3.604234323501587 and perplexity is 36.75353176541554
At time: 343.48582100868225 and batch: 900, loss is 3.787278246879578 and perplexity is 44.13610905619414
At time: 343.91913533210754 and batch: 950, loss is 3.7233729457855222 and perplexity is 41.40381166456377
At time: 344.3428723812103 and batch: 1000, loss is 3.6556997060775758 and perplexity is 38.69458648248722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339362539896151 and perplexity of 76.6586569242527
Finished 37 epochs...
Completing Train Step...
At time: 345.63761019706726 and batch: 50, loss is 3.788062777519226 and perplexity is 44.17074877224617
At time: 346.0670382976532 and batch: 100, loss is 3.6657177305221555 and perplexity is 39.08417800614062
At time: 346.4885802268982 and batch: 150, loss is 3.7102221488952636 and perplexity is 40.86288316312257
At time: 346.9268319606781 and batch: 200, loss is 3.756048393249512 and perplexity is 42.779045571531775
At time: 347.33829498291016 and batch: 250, loss is 3.7339805030822752 and perplexity is 41.845342611417365
At time: 347.749365568161 and batch: 300, loss is 3.6671652126312257 and perplexity is 39.14079261898689
At time: 348.166442155838 and batch: 350, loss is 3.749469795227051 and perplexity is 42.49854309508089
At time: 348.61416840553284 and batch: 400, loss is 3.650202097892761 and perplexity is 38.48244248373939
At time: 349.0380699634552 and batch: 450, loss is 3.719592361450195 and perplexity is 41.24757657880739
At time: 349.4641194343567 and batch: 500, loss is 3.6359989547729494 and perplexity is 37.93973405162724
At time: 349.87143111228943 and batch: 550, loss is 3.7103612756729127 and perplexity is 40.86856867987715
At time: 350.2788643836975 and batch: 600, loss is 3.71536572933197 and perplexity is 41.073606160309645
At time: 350.7464916706085 and batch: 650, loss is 3.6896094989776613 and perplexity is 40.02921245645346
At time: 351.1758210659027 and batch: 700, loss is 3.6641620922088625 and perplexity is 39.0234244289389
At time: 351.6065447330475 and batch: 750, loss is 3.627658224105835 and perplexity is 37.62460497858308
At time: 352.0206153392792 and batch: 800, loss is 3.6131429862976074 and perplexity is 37.08241938577254
At time: 352.4472951889038 and batch: 850, loss is 3.6041637659072876 and perplexity is 36.750938616116436
At time: 352.8677439689636 and batch: 900, loss is 3.787249445915222 and perplexity is 44.13483791199559
At time: 353.2981457710266 and batch: 950, loss is 3.7233598232269287 and perplexity is 41.403268344184085
At time: 353.71818351745605 and batch: 1000, loss is 3.655683045387268 and perplexity is 38.69394180933561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339359934737042 and perplexity of 76.65845721651448
Finished 38 epochs...
Completing Train Step...
At time: 354.9993431568146 and batch: 50, loss is 3.7877156162261962 and perplexity is 44.15541705943092
At time: 355.4390139579773 and batch: 100, loss is 3.66536057472229 and perplexity is 39.070221357780056
At time: 355.8521041870117 and batch: 150, loss is 3.7098452615737916 and perplexity is 40.8474853623398
At time: 356.27089643478394 and batch: 200, loss is 3.755684108734131 and perplexity is 42.76346466576096
At time: 356.685485124588 and batch: 250, loss is 3.7336252880096437 and perplexity is 41.83048115466534
At time: 357.10914158821106 and batch: 300, loss is 3.6668360328674314 and perplexity is 39.12791038251996
At time: 357.52845883369446 and batch: 350, loss is 3.7491559553146363 and perplexity is 42.48520744877668
At time: 357.9454324245453 and batch: 400, loss is 3.6499160051345827 and perplexity is 38.47143451035359
At time: 358.37574529647827 and batch: 450, loss is 3.719330310821533 and perplexity is 41.23676904155695
At time: 358.79495096206665 and batch: 500, loss is 3.635750398635864 and perplexity is 37.93030506975352
At time: 359.22912979125977 and batch: 550, loss is 3.710137715339661 and perplexity is 40.85943311025709
At time: 359.6509110927582 and batch: 600, loss is 3.7151614952087404 and perplexity is 41.06521838493174
At time: 360.0713403224945 and batch: 650, loss is 3.689427623748779 and perplexity is 40.02193279629096
At time: 360.4901702404022 and batch: 700, loss is 3.6639897203445435 and perplexity is 39.016698468217825
At time: 360.91388869285583 and batch: 750, loss is 3.6275302457809446 and perplexity is 37.619790152766505
At time: 361.3339726924896 and batch: 800, loss is 3.6130412340164186 and perplexity is 37.078646356968434
At time: 361.77584195137024 and batch: 850, loss is 3.6040801906585695 and perplexity is 36.74786727562678
At time: 362.2034869194031 and batch: 900, loss is 3.787202224731445 and perplexity is 44.13275386190963
At time: 362.6341028213501 and batch: 950, loss is 3.7233274602890014 and perplexity is 41.401928434462505
At time: 363.05822563171387 and batch: 1000, loss is 3.6556476354599 and perplexity is 38.69257168392472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339359562571456 and perplexity of 76.6584286868801
Finished 39 epochs...
Completing Train Step...
At time: 364.4065406322479 and batch: 50, loss is 3.7873779726028443 and perplexity is 44.14051078107089
At time: 364.85693097114563 and batch: 100, loss is 3.6650158405303954 and perplexity is 39.056754837911434
At time: 365.28426241874695 and batch: 150, loss is 3.709482579231262 and perplexity is 40.8326733868455
At time: 365.6966016292572 and batch: 200, loss is 3.755335364341736 and perplexity is 42.74855374746008
At time: 366.11612129211426 and batch: 250, loss is 3.7332857704162596 and perplexity is 41.8162813810465
At time: 366.5422294139862 and batch: 300, loss is 3.666521167755127 and perplexity is 39.11559230799086
At time: 366.979505777359 and batch: 350, loss is 3.748855276107788 and perplexity is 42.47243495060673
At time: 367.40035939216614 and batch: 400, loss is 3.649640231132507 and perplexity is 38.46082655165992
At time: 367.8277041912079 and batch: 450, loss is 3.719075779914856 and perplexity is 41.22627434501333
At time: 368.24363827705383 and batch: 500, loss is 3.6355061769485473 and perplexity is 37.92104279771883
At time: 368.65572023391724 and batch: 550, loss is 3.709918808937073 and perplexity is 40.850489697663996
At time: 369.07753586769104 and batch: 600, loss is 3.7149575901031495 and perplexity is 41.056845830873115
At time: 369.50498247146606 and batch: 650, loss is 3.689244661331177 and perplexity is 40.014610956540615
At time: 369.94331526756287 and batch: 700, loss is 3.6638158416748046 and perplexity is 39.00991488636775
At time: 370.3638825416565 and batch: 750, loss is 3.627398042678833 and perplexity is 37.61481702854598
At time: 370.7842574119568 and batch: 800, loss is 3.612931456565857 and perplexity is 37.07457618111194
At time: 371.2149498462677 and batch: 850, loss is 3.6039867544174196 and perplexity is 36.744433853443795
At time: 371.63278460502625 and batch: 900, loss is 3.7871413707733153 and perplexity is 44.13006829086859
At time: 372.0451810359955 and batch: 950, loss is 3.7232809829711915 and perplexity is 41.40000422859302
At time: 372.4917223453522 and batch: 1000, loss is 3.6555986309051516 and perplexity is 38.69067561813559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339362167730564 and perplexity of 76.65862839454394
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 373.9791328907013 and batch: 50, loss is 3.787766485214233 and perplexity is 44.15766325794352
At time: 374.4131224155426 and batch: 100, loss is 3.665780630111694 and perplexity is 39.086636462211786
At time: 374.8413393497467 and batch: 150, loss is 3.7104348707199097 and perplexity is 40.87157651478936
At time: 375.27348589897156 and batch: 200, loss is 3.7560280084609987 and perplexity is 42.778173538623136
At time: 375.6945004463196 and batch: 250, loss is 3.7337260818481446 and perplexity is 41.834697621920654
At time: 376.11214089393616 and batch: 300, loss is 3.6668696784973145 and perplexity is 39.12922688785798
At time: 376.521226644516 and batch: 350, loss is 3.749335389137268 and perplexity is 42.49283141593285
At time: 376.93237709999084 and batch: 400, loss is 3.6501278018951417 and perplexity is 38.479583498491124
At time: 377.3411135673523 and batch: 450, loss is 3.719234347343445 and perplexity is 41.232812007643
At time: 377.7514398097992 and batch: 500, loss is 3.635112080574036 and perplexity is 37.90610119664335
At time: 378.1783449649811 and batch: 550, loss is 3.709783983230591 and perplexity is 40.8449823728032
At time: 378.5952568054199 and batch: 600, loss is 3.7144953632354736 and perplexity is 41.03787263892541
At time: 379.0082106590271 and batch: 650, loss is 3.6887083721160887 and perplexity is 39.99315730543371
At time: 379.4162905216217 and batch: 700, loss is 3.662997899055481 and perplexity is 38.97802006025303
At time: 379.83145213127136 and batch: 750, loss is 3.6263242626190184 and perplexity is 37.57444866531764
At time: 380.2572841644287 and batch: 800, loss is 3.6116794776916503 and perplexity is 37.0281886391335
At time: 380.67874479293823 and batch: 850, loss is 3.60255913734436 and perplexity is 36.69201429876331
At time: 381.096519947052 and batch: 900, loss is 3.7856959009170534 and perplexity is 44.06632568750379
At time: 381.51919651031494 and batch: 950, loss is 3.7214094972610474 and perplexity is 41.32259716797613
At time: 381.9393792152405 and batch: 1000, loss is 3.653518533706665 and perplexity is 38.61027889765968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339118027105564 and perplexity of 76.6399151935153
Finished 41 epochs...
Completing Train Step...
At time: 383.2413001060486 and batch: 50, loss is 3.7875888299942018 and perplexity is 44.149819115358426
At time: 383.6999969482422 and batch: 100, loss is 3.665138940811157 and perplexity is 39.06156303133653
At time: 384.120242357254 and batch: 150, loss is 3.709948935508728 and perplexity is 40.85172040140737
At time: 384.55364060401917 and batch: 200, loss is 3.7556281518936157 and perplexity is 42.761071824337314
At time: 384.9864103794098 and batch: 250, loss is 3.733297462463379 and perplexity is 41.816770301837
At time: 385.40125465393066 and batch: 300, loss is 3.6664472150802614 and perplexity is 39.11269971226964
At time: 385.8213882446289 and batch: 350, loss is 3.7489597845077514 and perplexity is 42.476873908776135
At time: 386.2491190433502 and batch: 400, loss is 3.6497442531585693 and perplexity is 38.46482753285333
At time: 386.6769759654999 and batch: 450, loss is 3.7189932250976563 and perplexity is 41.222871057951764
At time: 387.10469484329224 and batch: 500, loss is 3.6349318838119506 and perplexity is 37.89927125532958
At time: 387.52770590782166 and batch: 550, loss is 3.7095132875442505 and perplexity is 40.83392730861283
At time: 387.9555687904358 and batch: 600, loss is 3.7143328952789307 and perplexity is 41.031205841202066
At time: 388.3846015930176 and batch: 650, loss is 3.6885972309112547 and perplexity is 39.988712664741605
At time: 388.8052520751953 and batch: 700, loss is 3.66287691116333 and perplexity is 38.97330447703571
At time: 389.2260546684265 and batch: 750, loss is 3.6263578033447263 and perplexity is 37.57570896072944
At time: 389.6363251209259 and batch: 800, loss is 3.6118423986434935 and perplexity is 37.03422179832229
At time: 390.05752062797546 and batch: 850, loss is 3.6027130317687988 and perplexity is 36.69766142970515
At time: 390.4749505519867 and batch: 900, loss is 3.7859556436538697 and perplexity is 44.07777308216375
At time: 390.88744354248047 and batch: 950, loss is 3.721634259223938 and perplexity is 41.33188595987179
At time: 391.30985164642334 and batch: 1000, loss is 3.6537453031539915 and perplexity is 38.61903552209633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339059597108422 and perplexity of 76.63543725431384
Finished 42 epochs...
Completing Train Step...
At time: 392.62234354019165 and batch: 50, loss is 3.787433285713196 and perplexity is 44.14295239754077
At time: 393.04440236091614 and batch: 100, loss is 3.664827995300293 and perplexity is 39.04941890184372
At time: 393.44883036613464 and batch: 150, loss is 3.709704933166504 and perplexity is 40.84175370194399
At time: 393.8575654029846 and batch: 200, loss is 3.7554032468795775 and perplexity is 42.75145572627324
At time: 394.2609040737152 and batch: 250, loss is 3.7330424642562865 and perplexity is 41.80610845981671
At time: 394.6846158504486 and batch: 300, loss is 3.666172161102295 and perplexity is 39.101943088018736
At time: 395.1057975292206 and batch: 350, loss is 3.7487368535995484 and perplexity is 42.467405556131474
At time: 395.5523293018341 and batch: 400, loss is 3.6495351648330687 and perplexity is 38.45678582721649
At time: 395.96964383125305 and batch: 450, loss is 3.7188316917419435 and perplexity is 41.2162127270434
At time: 396.38232803344727 and batch: 500, loss is 3.6348104286193847 and perplexity is 37.89466847156281
At time: 396.7944037914276 and batch: 550, loss is 3.709351887702942 and perplexity is 40.82733725105665
At time: 397.2077143192291 and batch: 600, loss is 3.7142402744293213 and perplexity is 41.02740567204671
At time: 397.62569642066956 and batch: 650, loss is 3.688530831336975 and perplexity is 39.98605751939591
At time: 398.0318100452423 and batch: 700, loss is 3.6628127813339235 and perplexity is 38.97080520580797
At time: 398.4567382335663 and batch: 750, loss is 3.62636438369751 and perplexity is 37.57595622296403
At time: 398.8699507713318 and batch: 800, loss is 3.6119168472290037 and perplexity is 37.03697904638598
At time: 399.2821629047394 and batch: 850, loss is 3.602804102897644 and perplexity is 36.70100367934645
At time: 399.6861140727997 and batch: 900, loss is 3.786094694137573 and perplexity is 44.08390254397383
At time: 400.10244846343994 and batch: 950, loss is 3.7217799186706544 and perplexity is 41.33790677799632
At time: 400.51251769065857 and batch: 1000, loss is 3.65387526512146 and perplexity is 38.62405485408859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339042105325839 and perplexity of 76.63409677563092
Finished 43 epochs...
Completing Train Step...
At time: 401.79813027381897 and batch: 50, loss is 3.7872952222824097 and perplexity is 44.13685829078406
At time: 402.2118330001831 and batch: 100, loss is 3.66462185382843 and perplexity is 39.04137002678788
At time: 402.6609995365143 and batch: 150, loss is 3.709528079032898 and perplexity is 40.834531307652064
At time: 403.0841865539551 and batch: 200, loss is 3.755229434967041 and perplexity is 42.74402565972548
At time: 403.5005760192871 and batch: 250, loss is 3.7328497982025146 and perplexity is 41.79805461775209
At time: 403.93009185791016 and batch: 300, loss is 3.6659694385528563 and perplexity is 39.09401704584881
At time: 404.3389256000519 and batch: 350, loss is 3.7485675907135008 and perplexity is 42.46021800881375
At time: 404.7665550708771 and batch: 400, loss is 3.649381856918335 and perplexity is 38.450890549481876
At time: 405.1927192211151 and batch: 450, loss is 3.718701596260071 and perplexity is 41.210851032761376
At time: 405.6155815124512 and batch: 500, loss is 3.6347135400772093 and perplexity is 37.890997090238635
At time: 406.02479434013367 and batch: 550, loss is 3.7092340421676635 and perplexity is 40.822526215129415
At time: 406.4366683959961 and batch: 600, loss is 3.714166326522827 and perplexity is 41.02437189346055
At time: 406.8632593154907 and batch: 650, loss is 3.68847505569458 and perplexity is 39.98382733354653
At time: 407.28570103645325 and batch: 700, loss is 3.662763767242432 and perplexity is 38.9688951340067
At time: 407.69899702072144 and batch: 750, loss is 3.6263570308685305 and perplexity is 37.575679934399936
At time: 408.1253740787506 and batch: 800, loss is 3.6119511413574217 and perplexity is 37.03824921908122
At time: 408.56305050849915 and batch: 850, loss is 3.60285786151886 and perplexity is 36.7029767277352
At time: 408.97534465789795 and batch: 900, loss is 3.7861791038513184 and perplexity is 44.08762381062166
At time: 409.4082763195038 and batch: 950, loss is 3.7218749427795412 and perplexity is 41.34183506238905
At time: 409.8195731639862 and batch: 1000, loss is 3.653955383300781 and perplexity is 38.62714946700722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339038383669969 and perplexity of 76.63381157042556
Finished 44 epochs...
Completing Train Step...
At time: 411.1277220249176 and batch: 50, loss is 3.787170162200928 and perplexity is 44.13133887682624
At time: 411.5594618320465 and batch: 100, loss is 3.66446072101593 and perplexity is 39.03507968783523
At time: 411.97980785369873 and batch: 150, loss is 3.709379110336304 and perplexity is 40.8284486938179
At time: 412.4055244922638 and batch: 200, loss is 3.7550801420211792 and perplexity is 42.73764475454067
At time: 412.82058668136597 and batch: 250, loss is 3.732689414024353 and perplexity is 41.791351408672185
At time: 413.24176597595215 and batch: 300, loss is 3.6658052444458007 and perplexity is 39.08759856558149
At time: 413.648118019104 and batch: 350, loss is 3.7484251976013185 and perplexity is 42.45417239666452
At time: 414.07057094573975 and batch: 400, loss is 3.6492549753189087 and perplexity is 38.4460121484858
At time: 414.4876515865326 and batch: 450, loss is 3.7185899209976196 and perplexity is 41.20624905712465
At time: 414.9231822490692 and batch: 500, loss is 3.634628028869629 and perplexity is 37.88775712384973
At time: 415.33818435668945 and batch: 550, loss is 3.709138126373291 and perplexity is 40.81861087787355
At time: 415.7622649669647 and batch: 600, loss is 3.7141002464294433 and perplexity is 41.02166108870092
At time: 416.18937492370605 and batch: 650, loss is 3.688422989845276 and perplexity is 39.98174559581218
At time: 416.6262843608856 and batch: 700, loss is 3.6627196979522707 and perplexity is 38.96717784030002
At time: 417.04654598236084 and batch: 750, loss is 3.626341977119446 and perplexity is 37.57511428380011
At time: 417.45997953414917 and batch: 800, loss is 3.611964421272278 and perplexity is 37.03874108714325
At time: 417.88395977020264 and batch: 850, loss is 3.602887716293335 and perplexity is 36.704072503184946
At time: 418.3060235977173 and batch: 900, loss is 3.786233468055725 and perplexity is 44.090020664365234
At time: 418.7296714782715 and batch: 950, loss is 3.7219380235671995 and perplexity is 41.34444302016317
At time: 419.1363263130188 and batch: 1000, loss is 3.6540063619613647 and perplexity is 38.62911867754264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339038383669969 and perplexity of 76.63381157042556
Finished 45 epochs...
Completing Train Step...
At time: 420.4526023864746 and batch: 50, loss is 3.7870535039901734 and perplexity is 44.12619089407773
At time: 420.87355732917786 and batch: 100, loss is 3.6643225622177122 and perplexity is 39.029687020668
At time: 421.2904853820801 and batch: 150, loss is 3.709245038032532 and perplexity is 40.82297509657918
At time: 421.6982452869415 and batch: 200, loss is 3.7549456691741945 and perplexity is 42.73189808817106
At time: 422.1245357990265 and batch: 250, loss is 3.7325484418869017 and perplexity is 41.78546040778037
At time: 422.53298139572144 and batch: 300, loss is 3.665664119720459 and perplexity is 39.08208272818932
At time: 422.9563066959381 and batch: 350, loss is 3.748298807144165 and perplexity is 42.44880693348604
At time: 423.3837606906891 and batch: 400, loss is 3.6491429567337037 and perplexity is 38.44170572180256
At time: 423.8003497123718 and batch: 450, loss is 3.718489990234375 and perplexity is 41.20213149094511
At time: 424.23726892471313 and batch: 500, loss is 3.6345487785339357 and perplexity is 37.88475462535507
At time: 424.64735221862793 and batch: 550, loss is 3.709054179191589 and perplexity is 40.81518441435235
At time: 425.0604763031006 and batch: 600, loss is 3.7140383434295656 and perplexity is 41.01912180341506
At time: 425.4662039279938 and batch: 650, loss is 3.6883725786209105 and perplexity is 39.979730117866204
At time: 425.8756375312805 and batch: 700, loss is 3.662676815986633 and perplexity is 38.965506886946024
At time: 426.2859139442444 and batch: 750, loss is 3.6263216972351073 and perplexity is 37.57435227255521
At time: 426.713342666626 and batch: 800, loss is 3.6119649314880373 and perplexity is 37.03875998489748
At time: 427.11701250076294 and batch: 850, loss is 3.602901940345764 and perplexity is 36.704594587549664
At time: 427.5181493759155 and batch: 900, loss is 3.7862689304351806 and perplexity is 44.091584229131946
At time: 427.9402232170105 and batch: 950, loss is 3.7219803619384764 and perplexity is 41.34619351359825
At time: 428.35365176200867 and batch: 1000, loss is 3.6540391397476197 and perplexity is 38.63038487528934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.33903950016673 and perplexity of 76.63389713187571
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 429.6334857940674 and batch: 50, loss is 3.787157850265503 and perplexity is 44.13079553797655
At time: 430.06088495254517 and batch: 100, loss is 3.664513125419617 and perplexity is 39.03712535150955
At time: 430.48026728630066 and batch: 150, loss is 3.7095190620422365 and perplexity is 40.83416310472463
At time: 430.9065523147583 and batch: 200, loss is 3.7551593923568727 and perplexity is 42.74103186144712
At time: 431.3364417552948 and batch: 250, loss is 3.732672863006592 and perplexity is 41.79065972499681
At time: 431.76311802864075 and batch: 300, loss is 3.665664291381836 and perplexity is 39.08208943707404
At time: 432.18212628364563 and batch: 350, loss is 3.7483924531936648 and perplexity is 42.45278228269631
At time: 432.6056685447693 and batch: 400, loss is 3.6491676473617556 and perplexity is 38.44265488337787
At time: 433.03862023353577 and batch: 450, loss is 3.71838755607605 and perplexity is 41.19791120143924
At time: 433.4609820842743 and batch: 500, loss is 3.6343964099884034 and perplexity is 37.878982620142146
At time: 433.88166642189026 and batch: 550, loss is 3.7089242219924925 and perplexity is 40.80988053195156
At time: 434.3070342540741 and batch: 600, loss is 3.713889899253845 and perplexity is 41.0130332056098
At time: 434.7431881427765 and batch: 650, loss is 3.6882125091552735 and perplexity is 39.9733310959876
At time: 435.1557285785675 and batch: 700, loss is 3.662444386482239 and perplexity is 38.95645120593631
At time: 435.5829339027405 and batch: 750, loss is 3.6259041452407836 and perplexity is 37.55866630191045
At time: 436.00131034851074 and batch: 800, loss is 3.6114268827438356 and perplexity is 37.018836686934186
At time: 436.4268958568573 and batch: 850, loss is 3.6023223066329955 and perplexity is 36.68332553183957
At time: 436.8465111255646 and batch: 900, loss is 3.7856663131713866 and perplexity is 44.06502188355528
At time: 437.25854992866516 and batch: 950, loss is 3.7213721799850465 and perplexity is 41.32105514998467
At time: 437.68508768081665 and batch: 1000, loss is 3.6533684062957765 and perplexity is 38.60448287153717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339017914562691 and perplexity of 76.63224296076945
Finished 47 epochs...
Completing Train Step...
At time: 438.9518496990204 and batch: 50, loss is 3.7870984411239625 and perplexity is 44.12817384317523
At time: 439.3732295036316 and batch: 100, loss is 3.664393787384033 and perplexity is 39.032467015619126
At time: 439.8088266849518 and batch: 150, loss is 3.7093958950042722 and perplexity is 40.829133991524124
At time: 440.2191753387451 and batch: 200, loss is 3.755066547393799 and perplexity is 42.73706375613436
At time: 440.6518316268921 and batch: 250, loss is 3.7325884103775024 and perplexity is 41.787130542938094
At time: 441.0765097141266 and batch: 300, loss is 3.6656028509140013 and perplexity is 39.07968828897965
At time: 441.49597668647766 and batch: 350, loss is 3.7483187103271485 and perplexity is 42.449651808265685
At time: 441.91402864456177 and batch: 400, loss is 3.6490979051589965 and perplexity is 38.439973901436275
At time: 442.32544445991516 and batch: 450, loss is 3.7183617973327636 and perplexity is 41.196850008688216
At time: 442.7337489128113 and batch: 500, loss is 3.634369044303894 and perplexity is 37.87794605003751
At time: 443.1431908607483 and batch: 550, loss is 3.708878769874573 and perplexity is 40.808025678603165
At time: 443.56425428390503 and batch: 600, loss is 3.7138477182388305 and perplexity is 41.01130327072582
At time: 443.9782118797302 and batch: 650, loss is 3.6881857824325563 and perplexity is 39.97226275412801
At time: 444.4027225971222 and batch: 700, loss is 3.662424006462097 and perplexity is 38.95565728076622
At time: 444.81182527542114 and batch: 750, loss is 3.6259253358840944 and perplexity is 37.55946220264407
At time: 445.23436188697815 and batch: 800, loss is 3.61147198677063 and perplexity is 37.02050642319164
At time: 445.6713750362396 and batch: 850, loss is 3.6023694229125978 and perplexity is 36.68505395438017
At time: 446.0820791721344 and batch: 900, loss is 3.7857382822036745 and perplexity is 44.068193314659034
At time: 446.4947807788849 and batch: 950, loss is 3.7214054965972903 and perplexity is 41.32243185048998
At time: 446.92860555648804 and batch: 1000, loss is 3.6534076166152953 and perplexity is 38.605996595322026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339005260932736 and perplexity of 76.63127329085937
Finished 48 epochs...
Completing Train Step...
At time: 448.2140197753906 and batch: 50, loss is 3.7870460844039915 and perplexity is 44.12586349721609
At time: 448.65290427207947 and batch: 100, loss is 3.664300003051758 and perplexity is 39.02880655341269
At time: 449.085741519928 and batch: 150, loss is 3.7093035316467287 and perplexity is 40.82536304977417
At time: 449.5134770870209 and batch: 200, loss is 3.75499388217926 and perplexity is 42.73395837105586
At time: 449.94604086875916 and batch: 250, loss is 3.732519311904907 and perplexity is 41.784243215799535
At time: 450.3698425292969 and batch: 300, loss is 3.6655442333221435 and perplexity is 39.07739759889962
At time: 450.7878131866455 and batch: 350, loss is 3.748257007598877 and perplexity is 42.447032629740995
At time: 451.19524240493774 and batch: 400, loss is 3.649041748046875 and perplexity is 38.437815284123374
At time: 451.630224943161 and batch: 450, loss is 3.718335132598877 and perplexity is 41.19575152029128
At time: 452.0542356967926 and batch: 500, loss is 3.6343416118621827 and perplexity is 37.876906979742536
At time: 452.4823868274689 and batch: 550, loss is 3.7088408613204957 and perplexity is 40.80647873467634
At time: 452.9102454185486 and batch: 600, loss is 3.713816704750061 and perplexity is 41.01003138685529
At time: 453.3481903076172 and batch: 650, loss is 3.6881647396087645 and perplexity is 39.97142163369612
At time: 453.7694523334503 and batch: 700, loss is 3.6624071264266966 and perplexity is 38.954999713442156
At time: 454.18977665901184 and batch: 750, loss is 3.6259380769729614 and perplexity is 37.559940754138424
At time: 454.61432003974915 and batch: 800, loss is 3.611505355834961 and perplexity is 37.021741783463334
At time: 455.03473138809204 and batch: 850, loss is 3.602404794692993 and perplexity is 36.6863515930022
At time: 455.4443838596344 and batch: 900, loss is 3.7857924556732176 and perplexity is 44.070580706253445
At time: 455.8507409095764 and batch: 950, loss is 3.721437134742737 and perplexity is 41.32373923628059
At time: 456.3004286289215 and batch: 1000, loss is 3.653440098762512 and perplexity is 38.60725062135351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3389952124618905 and perplexity of 76.63050326761262
Finished 49 epochs...
Completing Train Step...
At time: 457.57626581192017 and batch: 50, loss is 3.7869982147216796 and perplexity is 44.12375125670529
At time: 458.0016713142395 and batch: 100, loss is 3.664222812652588 and perplexity is 39.02579402052652
At time: 458.4314954280853 and batch: 150, loss is 3.7092299842834473 and perplexity is 40.82236056238072
At time: 458.86791038513184 and batch: 200, loss is 3.7549331855773924 and perplexity is 42.7313646437144
At time: 459.2997233867645 and batch: 250, loss is 3.7324595499038695 and perplexity is 41.781746180427774
At time: 459.7251305580139 and batch: 300, loss is 3.6654887580871582 and perplexity is 39.07522983121448
At time: 460.1446952819824 and batch: 350, loss is 3.748203043937683 and perplexity is 42.44474209425688
At time: 460.56626653671265 and batch: 400, loss is 3.6489940071105957 and perplexity is 38.43598027063616
At time: 460.9946799278259 and batch: 450, loss is 3.718307523727417 and perplexity is 41.194614167783435
At time: 461.4039099216461 and batch: 500, loss is 3.6343143367767334 and perplexity is 37.87587389795687
At time: 461.82424569129944 and batch: 550, loss is 3.708807282447815 and perplexity is 40.80510852212758
At time: 462.23752665519714 and batch: 600, loss is 3.7137915468215943 and perplexity is 41.008999672397195
At time: 462.6505596637726 and batch: 650, loss is 3.6881467485427857 and perplexity is 39.97070251168115
At time: 463.0506308078766 and batch: 700, loss is 3.6623917293548582 and perplexity is 38.95439992513061
At time: 463.45381903648376 and batch: 750, loss is 3.6259449911117554 and perplexity is 37.56020044967968
At time: 463.87565207481384 and batch: 800, loss is 3.611530203819275 and perplexity is 37.02266171055157
At time: 464.29742884635925 and batch: 850, loss is 3.60243155002594 and perplexity is 36.68733316168471
At time: 464.7235128879547 and batch: 900, loss is 3.78583402633667 and perplexity is 44.07241278761229
At time: 465.12699723243713 and batch: 950, loss is 3.7214655971527097 and perplexity is 41.324915426226866
At time: 465.54922699928284 and batch: 1000, loss is 3.653466863632202 and perplexity is 38.608283953213906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.338991490806022 and perplexity of 76.6302180757811
Finished Training.
Improved accuracyfrom -223.00914785126133 to -76.6302180757811
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1cac512908>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 22.905441939296505, 'num_layers': 1, 'dropout': 0.8954330128718082, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 2.9022484911763584}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6598312854766846 and batch: 50, loss is 7.0319154262542725 and perplexity is 1132.1971747701944
At time: 1.0895581245422363 and batch: 100, loss is 6.539671020507813 and perplexity is 692.0588674077183
At time: 1.5105409622192383 and batch: 150, loss is 6.414834337234497 and perplexity is 610.8395590650097
At time: 1.9361202716827393 and batch: 200, loss is 6.316800422668457 and perplexity is 553.7982344505322
At time: 2.3540854454040527 and batch: 250, loss is 6.279903564453125 and perplexity is 533.7371901058376
At time: 2.791909694671631 and batch: 300, loss is 6.167150459289551 and perplexity is 476.8254348775014
At time: 3.2193186283111572 and batch: 350, loss is 6.2072382831573485 and perplexity is 496.3286376160854
At time: 3.6417396068573 and batch: 400, loss is 6.155321989059448 and perplexity is 471.21854525237785
At time: 4.083085775375366 and batch: 450, loss is 6.116662540435791 and perplexity is 453.34913244475825
At time: 4.506579399108887 and batch: 500, loss is 6.110529308319092 and perplexity is 450.57714629219026
At time: 4.93733024597168 and batch: 550, loss is 6.176094083786011 and perplexity is 481.10910975352476
At time: 5.361914157867432 and batch: 600, loss is 6.209418907165527 and perplexity is 497.41212466881484
At time: 5.790081977844238 and batch: 650, loss is 6.173773517608643 and perplexity is 479.9939586172384
At time: 6.219513416290283 and batch: 700, loss is 6.173323793411255 and perplexity is 479.7781422520064
At time: 6.667723178863525 and batch: 750, loss is 6.087859706878662 and perplexity is 440.47765025882046
At time: 7.09575080871582 and batch: 800, loss is 6.155564918518066 and perplexity is 471.33303202400833
At time: 7.52112078666687 and batch: 850, loss is 6.136959924697876 and perplexity is 462.64495529321607
At time: 7.958671569824219 and batch: 900, loss is 6.166263875961303 and perplexity is 476.40287674063296
At time: 8.405583143234253 and batch: 950, loss is 6.20822603225708 and perplexity is 496.81912798189165
At time: 8.820656061172485 and batch: 1000, loss is 6.142369766235351 and perplexity is 465.15457338695023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.030629599966654 and perplexity of 415.976845961829
Finished 1 epochs...
Completing Train Step...
At time: 10.1236252784729 and batch: 50, loss is 6.113798561096192 and perplexity is 452.0526073925082
At time: 10.538515090942383 and batch: 100, loss is 6.53613676071167 and perplexity is 689.6172687395791
At time: 10.942935466766357 and batch: 150, loss is 6.563323135375977 and perplexity is 708.6226351320616
At time: 11.348020076751709 and batch: 200, loss is 6.68003701210022 and perplexity is 796.3485860041303
At time: 11.758012056350708 and batch: 250, loss is 6.490258016586304 and perplexity is 658.6932950870766
At time: 12.161785364151001 and batch: 300, loss is 6.390584106445313 and perplexity is 596.2047250237632
At time: 12.55854058265686 and batch: 350, loss is 6.462304706573486 and perplexity is 640.5356033336054
At time: 12.963407754898071 and batch: 400, loss is 6.343278436660767 and perplexity is 568.6575665244777
At time: 13.390275239944458 and batch: 450, loss is 6.293314876556397 and perplexity is 540.9435213093678
At time: 13.807488441467285 and batch: 500, loss is 6.264506578445435 and perplexity is 525.5821883879659
At time: 14.224170684814453 and batch: 550, loss is 6.23925048828125 and perplexity is 512.4742614965365
At time: 14.634367942810059 and batch: 600, loss is 6.2838758373260495 and perplexity is 535.8615563552229
At time: 15.052952766418457 and batch: 650, loss is 6.281101312637329 and perplexity is 534.376855858904
At time: 15.467078685760498 and batch: 700, loss is 6.369486198425293 and perplexity is 583.757816139203
At time: 15.88091516494751 and batch: 750, loss is 6.35512261390686 and perplexity is 575.43289240913
At time: 16.28521990776062 and batch: 800, loss is 6.473331556320191 and perplexity is 647.6377785349074
At time: 16.69985270500183 and batch: 850, loss is 6.343259334564209 and perplexity is 568.6467040764818
At time: 17.101115465164185 and batch: 900, loss is 6.391558494567871 and perplexity is 596.7859429463741
At time: 17.50223398208618 and batch: 950, loss is 6.396352663040161 and perplexity is 599.6539045511936
At time: 17.918081045150757 and batch: 1000, loss is 6.297205181121826 and perplexity is 543.0520551200175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.276417802019817 and perplexity of 531.8799478924168
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 19.162419080734253 and batch: 50, loss is 6.04758131980896 and perplexity is 423.0884757942395
At time: 19.599935293197632 and batch: 100, loss is 5.958229389190674 and perplexity is 386.9244247945693
At time: 20.01181936264038 and batch: 150, loss is 5.947194061279297 and perplexity is 382.67805999692075
At time: 20.424497604370117 and batch: 200, loss is 5.919958877563476 and perplexity is 372.3963997139977
At time: 20.839987993240356 and batch: 250, loss is 5.918959341049194 and perplexity is 372.0243618783715
At time: 21.24693512916565 and batch: 300, loss is 5.82802448272705 and perplexity is 339.6869583386582
At time: 21.65311884880066 and batch: 350, loss is 5.888687725067139 and perplexity is 360.9313323227124
At time: 22.07058620452881 and batch: 400, loss is 5.855800457000733 and perplexity is 349.25435128523975
At time: 22.47403120994568 and batch: 450, loss is 5.845239944458008 and perplexity is 345.5854531478726
At time: 22.875325202941895 and batch: 500, loss is 5.811880979537964 and perplexity is 334.247246979901
At time: 23.291077136993408 and batch: 550, loss is 5.878312740325928 and perplexity is 357.20603363174206
At time: 23.709530115127563 and batch: 600, loss is 5.900688285827637 and perplexity is 365.288804466805
At time: 24.14490818977356 and batch: 650, loss is 5.846170730590821 and perplexity is 345.9072690430165
At time: 24.546313285827637 and batch: 700, loss is 5.823152341842651 and perplexity is 338.0359807845336
At time: 24.961350202560425 and batch: 750, loss is 5.719254560470581 and perplexity is 304.677719468374
At time: 25.36196994781494 and batch: 800, loss is 5.76485276222229 and perplexity is 318.8920876171427
At time: 25.77330207824707 and batch: 850, loss is 5.732987976074218 and perplexity is 308.8908493281766
At time: 26.193477630615234 and batch: 900, loss is 5.817169198989868 and perplexity is 336.01950167888276
At time: 26.606101751327515 and batch: 950, loss is 5.813679838180542 and perplexity is 334.84905164737114
At time: 27.028363704681396 and batch: 1000, loss is 5.739436645507812 and perplexity is 310.8892207998279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.822605040015244 and perplexity of 337.8510236928147
Finished 3 epochs...
Completing Train Step...
At time: 28.317579746246338 and batch: 50, loss is 5.80938422203064 and perplexity is 333.41375360510756
At time: 28.75484538078308 and batch: 100, loss is 5.783263521194458 and perplexity is 324.8175113842131
At time: 29.165542125701904 and batch: 150, loss is 5.801483650207519 and perplexity is 330.78997263076104
At time: 29.58482837677002 and batch: 200, loss is 5.77297945022583 and perplexity is 321.4941830078571
At time: 30.00995135307312 and batch: 250, loss is 5.771532154083252 and perplexity is 321.02922226603386
At time: 30.438560962677002 and batch: 300, loss is 5.6574451351165775 and perplexity is 286.4159529329022
At time: 30.845816373825073 and batch: 350, loss is 5.713682365417481 and perplexity is 302.98471703759805
At time: 31.25269913673401 and batch: 400, loss is 5.668902034759522 and perplexity is 289.716261312797
At time: 31.672579288482666 and batch: 450, loss is 5.663177518844605 and perplexity is 288.06251393181986
At time: 32.09518313407898 and batch: 500, loss is 5.632739458084107 and perplexity is 279.42654740752323
At time: 32.508004665374756 and batch: 550, loss is 5.688969278335572 and perplexity is 295.58879379812583
At time: 32.91364026069641 and batch: 600, loss is 5.718186416625977 and perplexity is 304.3524535840372
At time: 33.32253575325012 and batch: 650, loss is 5.668721351623535 and perplexity is 289.6639191989678
At time: 33.729931592941284 and batch: 700, loss is 5.649541959762574 and perplexity is 284.16127871116066
At time: 34.144821882247925 and batch: 750, loss is 5.578236103057861 and perplexity is 264.60445891192506
At time: 34.581838846206665 and batch: 800, loss is 5.621027593612671 and perplexity is 276.1730311152134
At time: 35.015671491622925 and batch: 850, loss is 5.598062753677368 and perplexity is 269.9030319800852
At time: 35.44248676300049 and batch: 900, loss is 5.684977722167969 and perplexity is 294.41128613254745
At time: 35.86792469024658 and batch: 950, loss is 5.671385746002198 and perplexity is 290.43672719243955
At time: 36.29114818572998 and batch: 1000, loss is 5.603778152465821 and perplexity is 271.45005214944814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.717948727491425 and perplexity of 304.2801209094322
Finished 4 epochs...
Completing Train Step...
At time: 37.59040117263794 and batch: 50, loss is 5.686395788192749 and perplexity is 294.82907693218147
At time: 38.018078088760376 and batch: 100, loss is 5.661721248626709 and perplexity is 287.64332237406165
At time: 38.44637155532837 and batch: 150, loss is 5.684016962051391 and perplexity is 294.12856334709664
At time: 38.88417720794678 and batch: 200, loss is 5.651638412475586 and perplexity is 284.7576342920647
At time: 39.302562952041626 and batch: 250, loss is 5.66653844833374 and perplexity is 289.0323005088803
At time: 39.72290778160095 and batch: 300, loss is 5.552652406692505 and perplexity is 257.9207601878846
At time: 40.136138677597046 and batch: 350, loss is 5.614875116348267 and perplexity is 274.47909910269334
At time: 40.57392168045044 and batch: 400, loss is 5.567583751678467 and perplexity is 261.80075875055786
At time: 40.99349570274353 and batch: 450, loss is 5.562351884841919 and perplexity is 260.4346288640814
At time: 41.429808616638184 and batch: 500, loss is 5.52521541595459 and perplexity is 250.9403887623299
At time: 41.852617263793945 and batch: 550, loss is 5.572401714324951 and perplexity is 263.06514847243386
At time: 42.2818443775177 and batch: 600, loss is 5.608411846160888 and perplexity is 272.71078721890837
At time: 42.70304822921753 and batch: 650, loss is 5.563644170761108 and perplexity is 260.7714024248292
At time: 43.11257743835449 and batch: 700, loss is 5.553919591903687 and perplexity is 258.24780072755675
At time: 43.54065775871277 and batch: 750, loss is 5.475043783187866 and perplexity is 238.6609160376334
At time: 43.96294045448303 and batch: 800, loss is 5.522386474609375 and perplexity is 250.23149630173808
At time: 44.38638615608215 and batch: 850, loss is 5.49894907951355 and perplexity is 244.43491557544152
At time: 44.81349182128906 and batch: 900, loss is 5.585545558929443 and perplexity is 266.54565944469397
At time: 45.2479772567749 and batch: 950, loss is 5.5664628314971925 and perplexity is 261.5074654065249
At time: 45.699753761291504 and batch: 1000, loss is 5.511039304733276 and perplexity is 247.4081259307539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.636038059141578 and perplexity of 280.34978597301284
Finished 5 epochs...
Completing Train Step...
At time: 47.04461193084717 and batch: 50, loss is 5.583226366043091 and perplexity is 265.9282049222461
At time: 47.48198056221008 and batch: 100, loss is 5.559588956832886 and perplexity is 259.7160598677895
At time: 47.89958167076111 and batch: 150, loss is 5.587128925323486 and perplexity is 266.96803318228507
At time: 48.313663482666016 and batch: 200, loss is 5.558400249481201 and perplexity is 259.40751689805387
At time: 48.747010707855225 and batch: 250, loss is 5.562532291412354 and perplexity is 260.48161722067385
At time: 49.154120206832886 and batch: 300, loss is 5.467376308441162 and perplexity is 236.83798705271792
At time: 49.56263780593872 and batch: 350, loss is 5.529515142440796 and perplexity is 252.0216867753155
At time: 49.97228240966797 and batch: 400, loss is 5.4729913711547855 and perplexity is 238.17158782505786
At time: 50.37953019142151 and batch: 450, loss is 5.484987621307373 and perplexity is 241.04596014570967
At time: 50.80119585990906 and batch: 500, loss is 5.4606252288818355 and perplexity is 235.24446002567782
At time: 51.20736360549927 and batch: 550, loss is 5.509560918807983 and perplexity is 247.04263147707226
At time: 51.6274688243866 and batch: 600, loss is 5.547285995483398 and perplexity is 256.5403585435545
At time: 52.04962134361267 and batch: 650, loss is 5.495352382659912 and perplexity is 243.55733642280745
At time: 52.463299036026 and batch: 700, loss is 5.475876188278198 and perplexity is 238.85966130581699
At time: 52.88367581367493 and batch: 750, loss is 5.409704151153565 and perplexity is 223.56543631918873
At time: 53.32112717628479 and batch: 800, loss is 5.454484052658081 and perplexity is 233.80420928427355
At time: 53.730536222457886 and batch: 850, loss is 5.426018114089966 and perplexity is 227.2425874605032
At time: 54.139424324035645 and batch: 900, loss is 5.5029018688201905 and perplexity is 245.40302740641914
At time: 54.570871114730835 and batch: 950, loss is 5.482163057327271 and perplexity is 240.36607105627127
At time: 54.98369574546814 and batch: 1000, loss is 5.436763076782227 and perplexity is 229.697465753904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.576727890386814 and perplexity of 264.20567991154127
Finished 6 epochs...
Completing Train Step...
At time: 56.3594183921814 and batch: 50, loss is 5.498390312194824 and perplexity is 244.29837148480286
At time: 56.77749586105347 and batch: 100, loss is 5.475125036239624 and perplexity is 238.68030875324482
At time: 57.18858027458191 and batch: 150, loss is 5.503809156417847 and perplexity is 245.62577956446728
At time: 57.608582735061646 and batch: 200, loss is 5.46734585762024 and perplexity is 236.8307752513897
At time: 58.02971625328064 and batch: 250, loss is 5.4697533702850345 and perplexity is 237.40163524281928
At time: 58.4461133480072 and batch: 300, loss is 5.370020570755005 and perplexity is 214.86728764120724
At time: 58.866666316986084 and batch: 350, loss is 5.427724380493164 and perplexity is 227.6306548318719
At time: 59.28249216079712 and batch: 400, loss is 5.37098952293396 and perplexity is 215.07558466638451
At time: 59.70092582702637 and batch: 450, loss is 5.37724422454834 and perplexity is 216.425034075362
At time: 60.110127210617065 and batch: 500, loss is 5.35083236694336 and perplexity is 210.7836742304343
At time: 60.51812505722046 and batch: 550, loss is 5.398400020599365 and perplexity is 221.052453740927
At time: 60.93802261352539 and batch: 600, loss is 5.4428387546539305 and perplexity is 231.0972816730253
At time: 61.350046157836914 and batch: 650, loss is 5.397984046936035 and perplexity is 220.96052086410955
At time: 61.770158529281616 and batch: 700, loss is 5.385288982391358 and perplexity is 218.1731431950884
At time: 62.189124584198 and batch: 750, loss is 5.319178810119629 and perplexity is 204.21611291192824
At time: 62.60892033576965 and batch: 800, loss is 5.368374843597412 and perplexity is 214.5139655263029
At time: 63.01828145980835 and batch: 850, loss is 5.344397792816162 and perplexity is 209.43172532615813
At time: 63.44468975067139 and batch: 900, loss is 5.428579206466675 and perplexity is 227.8253226196761
At time: 63.86260628700256 and batch: 950, loss is 5.40135853767395 and perplexity is 221.7074095709755
At time: 64.27052688598633 and batch: 1000, loss is 5.371203479766845 and perplexity is 215.12160648047595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.523296449242569 and perplexity of 250.459304249806
Finished 7 epochs...
Completing Train Step...
At time: 65.58681464195251 and batch: 50, loss is 5.441394329071045 and perplexity is 230.7637198078815
At time: 65.9971992969513 and batch: 100, loss is 5.413515987396241 and perplexity is 224.4192574312481
At time: 66.4255428314209 and batch: 150, loss is 5.446945905685425 and perplexity is 232.048384937212
At time: 66.84241223335266 and batch: 200, loss is 5.407603349685669 and perplexity is 223.09626271524942
At time: 67.27605772018433 and batch: 250, loss is 5.413237609863281 and perplexity is 224.35679284678767
At time: 67.68685483932495 and batch: 300, loss is 5.315080041885376 and perplexity is 203.3807914593626
At time: 68.10875010490417 and batch: 350, loss is 5.385150938034058 and perplexity is 218.1430277024407
At time: 68.5308997631073 and batch: 400, loss is 5.329761209487915 and perplexity is 206.38868461191774
At time: 68.95814776420593 and batch: 450, loss is 5.33788803100586 and perplexity is 208.07280260928573
At time: 69.36788415908813 and batch: 500, loss is 5.314828252792358 and perplexity is 203.32958884074472
At time: 69.79106378555298 and batch: 550, loss is 5.357326469421387 and perplexity is 212.15697937735553
At time: 70.20491695404053 and batch: 600, loss is 5.40237943649292 and perplexity is 221.93386597843448
At time: 70.61760401725769 and batch: 650, loss is 5.36888126373291 and perplexity is 214.62262722970604
At time: 71.04338955879211 and batch: 700, loss is 5.356964378356934 and perplexity is 212.08017313712747
At time: 71.47941255569458 and batch: 750, loss is 5.291310329437255 and perplexity is 198.60349098696187
At time: 71.90123176574707 and batch: 800, loss is 5.3358075809478756 and perplexity is 207.6403675208095
At time: 72.32723903656006 and batch: 850, loss is 5.306420497894287 and perplexity is 201.62721011425026
At time: 72.75782990455627 and batch: 900, loss is 5.3980645751953125 and perplexity is 220.97831514668553
At time: 73.18464732170105 and batch: 950, loss is 5.375355653762817 and perplexity is 216.01668579743676
At time: 73.6190619468689 and batch: 1000, loss is 5.338615255355835 and perplexity is 208.22417325144815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.496951963843369 and perplexity of 243.94723791164924
Finished 8 epochs...
Completing Train Step...
At time: 74.91926860809326 and batch: 50, loss is 5.406188707351685 and perplexity is 222.78088442379493
At time: 75.34240341186523 and batch: 100, loss is 5.384054336547852 and perplexity is 217.90394284841878
At time: 75.75855779647827 and batch: 150, loss is 5.4111710548400875 and perplexity is 223.89362593433253
At time: 76.16656541824341 and batch: 200, loss is 5.379747629165649 and perplexity is 216.96751224283145
At time: 76.57858967781067 and batch: 250, loss is 5.382002429962158 and perplexity is 217.45728272218204
At time: 76.98844504356384 and batch: 300, loss is 5.281064224243164 and perplexity is 196.57896817486616
At time: 77.41320943832397 and batch: 350, loss is 5.352393083572387 and perplexity is 211.1129046668124
At time: 77.84294724464417 and batch: 400, loss is 5.293095302581787 and perplexity is 198.9583094612997
At time: 78.2765326499939 and batch: 450, loss is 5.302881517410278 and perplexity is 200.9149164926434
At time: 78.67779469490051 and batch: 500, loss is 5.280636119842529 and perplexity is 196.49482986479362
At time: 79.10324001312256 and batch: 550, loss is 5.320959939956665 and perplexity is 204.5801724462427
At time: 79.50705933570862 and batch: 600, loss is 5.37008264541626 and perplexity is 214.880625869281
At time: 79.92161226272583 and batch: 650, loss is 5.327335453033447 and perplexity is 205.88864266328278
At time: 80.32350540161133 and batch: 700, loss is 5.3220970344543455 and perplexity is 204.8129317442449
At time: 80.72540521621704 and batch: 750, loss is 5.259947652816773 and perplexity is 192.47141569706977
At time: 81.14194202423096 and batch: 800, loss is 5.303985376358032 and perplexity is 201.13682067387634
At time: 81.552490234375 and batch: 850, loss is 5.280608520507813 and perplexity is 196.4894068130508
At time: 81.96618008613586 and batch: 900, loss is 5.377485332489013 and perplexity is 216.47722216086615
At time: 82.37191247940063 and batch: 950, loss is 5.346127099990845 and perplexity is 209.79421044509496
At time: 82.79425644874573 and batch: 1000, loss is 5.313559226989746 and perplexity is 203.0717220005145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.471710205078125 and perplexity of 237.8666458486804
Finished 9 epochs...
Completing Train Step...
At time: 84.1888747215271 and batch: 50, loss is 5.383379983901977 and perplexity is 217.75704828294866
At time: 84.62389659881592 and batch: 100, loss is 5.354652366638184 and perplexity is 211.5904076814223
At time: 85.04039907455444 and batch: 150, loss is 5.381844320297241 and perplexity is 217.42290334200482
At time: 85.45581865310669 and batch: 200, loss is 5.352996444702148 and perplexity is 211.24032042247094
At time: 85.8706271648407 and batch: 250, loss is 5.357486534118652 and perplexity is 212.19094093798327
At time: 86.29778933525085 and batch: 300, loss is 5.25737813949585 and perplexity is 191.9774926731903
At time: 86.72352361679077 and batch: 350, loss is 5.328762521743775 and perplexity is 206.1826696514974
At time: 87.14887499809265 and batch: 400, loss is 5.266954345703125 and perplexity is 193.82473942147732
At time: 87.57566332817078 and batch: 450, loss is 5.289052295684814 and perplexity is 198.15554353152726
At time: 88.00930976867676 and batch: 500, loss is 5.261071367263794 and perplexity is 192.68782017317355
At time: 88.4314489364624 and batch: 550, loss is 5.30192349433899 and perplexity is 200.72252753852047
At time: 88.87032771110535 and batch: 600, loss is 5.347769746780395 and perplexity is 210.1391114290423
At time: 89.28011202812195 and batch: 650, loss is 5.316639757156372 and perplexity is 203.69825509771317
At time: 89.69376945495605 and batch: 700, loss is 5.307917470932007 and perplexity is 201.92926664029247
At time: 90.12031364440918 and batch: 750, loss is 5.243361005783081 and perplexity is 189.30529054851388
At time: 90.54832792282104 and batch: 800, loss is 5.282774076461792 and perplexity is 196.9153766821917
At time: 90.9625313282013 and batch: 850, loss is 5.250513277053833 and perplexity is 190.6641068584192
At time: 91.37410712242126 and batch: 900, loss is 5.347621746063233 and perplexity is 210.10801299119922
At time: 91.78373670578003 and batch: 950, loss is 5.320160074234009 and perplexity is 204.4166012050033
At time: 92.20616865158081 and batch: 1000, loss is 5.284588871002197 and perplexity is 197.2730620973164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4707794189453125 and perplexity of 237.64534588071368
Finished 10 epochs...
Completing Train Step...
At time: 93.5236177444458 and batch: 50, loss is 5.360011291503906 and perplexity is 212.72734844767166
At time: 93.95049524307251 and batch: 100, loss is 5.3287333965301515 and perplexity is 206.17666462464769
At time: 94.36469626426697 and batch: 150, loss is 5.361692533493042 and perplexity is 213.08529541153854
At time: 94.78679943084717 and batch: 200, loss is 5.330255804061889 and perplexity is 206.4907885834101
At time: 95.20110726356506 and batch: 250, loss is 5.328697528839111 and perplexity is 206.16926967636184
At time: 95.618417263031 and batch: 300, loss is 5.227482681274414 and perplexity is 186.32317784481265
At time: 96.05321335792542 and batch: 350, loss is 5.305931758880615 and perplexity is 201.5286911074519
At time: 96.48835158348083 and batch: 400, loss is 5.239360132217407 and perplexity is 188.54941710103492
At time: 96.91300749778748 and batch: 450, loss is 5.254787149429322 and perplexity is 191.48072473498348
At time: 97.33434820175171 and batch: 500, loss is 5.231507425308227 and perplexity is 187.074592053813
At time: 97.76762223243713 and batch: 550, loss is 5.270146131515503 and perplexity is 194.44437482054408
At time: 98.18588995933533 and batch: 600, loss is 5.316871452331543 and perplexity is 203.74545646856387
At time: 98.62446308135986 and batch: 650, loss is 5.284968795776368 and perplexity is 197.34802526016335
At time: 99.05418992042542 and batch: 700, loss is 5.273313293457031 and perplexity is 195.0611879020785
At time: 99.4928331375122 and batch: 750, loss is 5.211203956604004 and perplexity is 183.31462824597673
At time: 99.90425729751587 and batch: 800, loss is 5.2459025382995605 and perplexity is 189.78702801639335
At time: 100.32756614685059 and batch: 850, loss is 5.216094312667846 and perplexity is 184.21329766551685
At time: 100.75396060943604 and batch: 900, loss is 5.323834676742553 and perplexity is 205.1691327409469
At time: 101.17641687393188 and batch: 950, loss is 5.294437341690063 and perplexity is 199.2254985425092
At time: 101.589595079422 and batch: 1000, loss is 5.252224645614624 and perplexity is 190.99068278274697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.458363602801067 and perplexity of 234.71302619835106
Finished 11 epochs...
Completing Train Step...
At time: 102.94750380516052 and batch: 50, loss is 5.328695621490478 and perplexity is 206.16887644006223
At time: 103.39692759513855 and batch: 100, loss is 5.29623423576355 and perplexity is 199.5838074853258
At time: 103.84056997299194 and batch: 150, loss is 5.329053297042846 and perplexity is 206.24263119617427
At time: 104.27329111099243 and batch: 200, loss is 5.300671367645264 and perplexity is 200.47135478662724
At time: 104.70550155639648 and batch: 250, loss is 5.295915756225586 and perplexity is 199.5202542472729
At time: 105.13486003875732 and batch: 300, loss is 5.198319597244263 and perplexity is 180.96788729112487
At time: 105.55763244628906 and batch: 350, loss is 5.275031900405883 and perplexity is 195.39670964747663
At time: 105.97072911262512 and batch: 400, loss is 5.210979022979736 and perplexity is 183.27339925933057
At time: 106.38285446166992 and batch: 450, loss is 5.2290756893157955 and perplexity is 186.62022870480132
At time: 106.81225371360779 and batch: 500, loss is 5.208436222076416 and perplexity is 182.80796350015967
At time: 107.24565291404724 and batch: 550, loss is 5.244303302764893 and perplexity is 189.4837564231609
At time: 107.67334342002869 and batch: 600, loss is 5.295167026519775 and perplexity is 199.37092341719742
At time: 108.09251928329468 and batch: 650, loss is 5.263078651428223 and perplexity is 193.0749878310421
At time: 108.51455068588257 and batch: 700, loss is 5.250849113464356 and perplexity is 190.7281495610158
At time: 108.94045925140381 and batch: 750, loss is 5.188154792785644 and perplexity is 179.13770160171646
At time: 109.37499380111694 and batch: 800, loss is 5.225431928634643 and perplexity is 185.94146662741693
At time: 109.80109429359436 and batch: 850, loss is 5.192483654022217 and perplexity is 179.91484471295772
At time: 110.22192478179932 and batch: 900, loss is 5.299807901382446 and perplexity is 200.2983292467161
At time: 110.65823292732239 and batch: 950, loss is 5.265372791290283 and perplexity is 193.51843733008602
At time: 111.08293271064758 and batch: 1000, loss is 5.234452219009399 and perplexity is 187.62630006859357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.453749121689215 and perplexity of 233.63244245633123
Finished 12 epochs...
Completing Train Step...
At time: 112.45536994934082 and batch: 50, loss is 5.312082214355469 and perplexity is 202.7720038996541
At time: 112.88004946708679 and batch: 100, loss is 5.279159746170044 and perplexity is 196.20494411370206
At time: 113.30234575271606 and batch: 150, loss is 5.320456895828247 and perplexity is 204.47728547221672
At time: 113.71976327896118 and batch: 200, loss is 5.291308393478394 and perplexity is 198.60310649914572
At time: 114.14376878738403 and batch: 250, loss is 5.282996482849121 and perplexity is 196.9591767902605
At time: 114.5600757598877 and batch: 300, loss is 5.186921272277832 and perplexity is 178.91686780257967
At time: 114.99023199081421 and batch: 350, loss is 5.262391357421875 and perplexity is 192.9423341403953
At time: 115.4111487865448 and batch: 400, loss is 5.196343297958374 and perplexity is 180.61059376046927
At time: 115.82340908050537 and batch: 450, loss is 5.214028997421265 and perplexity is 183.83323174629246
At time: 116.23188185691833 and batch: 500, loss is 5.188163461685181 and perplexity is 179.139254535186
At time: 116.6465528011322 and batch: 550, loss is 5.225776786804199 and perplexity is 186.00560111925873
At time: 117.0775101184845 and batch: 600, loss is 5.271065177917481 and perplexity is 194.6231603671302
At time: 117.49511504173279 and batch: 650, loss is 5.245520849227905 and perplexity is 189.71460220480716
At time: 117.92898893356323 and batch: 700, loss is 5.23391019821167 and perplexity is 187.52463026781777
At time: 118.3646137714386 and batch: 750, loss is 5.1716428565979005 and perplexity is 176.2040778606707
At time: 118.79857420921326 and batch: 800, loss is 5.199592084884643 and perplexity is 181.1983132670192
At time: 119.22234416007996 and batch: 850, loss is 5.171135511398315 and perplexity is 176.1147042411762
At time: 119.65531539916992 and batch: 900, loss is 5.28602972984314 and perplexity is 197.55750960806336
At time: 120.07549047470093 and batch: 950, loss is 5.255779256820679 and perplexity is 191.67078844351093
At time: 120.49289917945862 and batch: 1000, loss is 5.195307826995849 and perplexity is 180.42367352704426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.434661865234375 and perplexity of 229.21532949887253
Finished 13 epochs...
Completing Train Step...
At time: 121.84120512008667 and batch: 50, loss is 5.289010429382325 and perplexity is 198.1472476652616
At time: 122.26493120193481 and batch: 100, loss is 5.260435791015625 and perplexity is 192.565391281929
At time: 122.68441462516785 and batch: 150, loss is 5.303684711456299 and perplexity is 201.07635498186474
At time: 123.10218811035156 and batch: 200, loss is 5.2701591110229495 and perplexity is 194.44689862913387
At time: 123.519775390625 and batch: 250, loss is 5.260878953933716 and perplexity is 192.65074803472973
At time: 123.92984986305237 and batch: 300, loss is 5.162670230865478 and perplexity is 174.63013637101693
At time: 124.3572347164154 and batch: 350, loss is 5.234895210266114 and perplexity is 187.7094352917826
At time: 124.77301788330078 and batch: 400, loss is 5.171425628662109 and perplexity is 176.16580556961492
At time: 125.18961691856384 and batch: 450, loss is 5.198922538757325 and perplexity is 181.0770332439102
At time: 125.61514115333557 and batch: 500, loss is 5.178134031295777 and perplexity is 177.35156956794984
At time: 126.031742811203 and batch: 550, loss is 5.217171020507813 and perplexity is 184.4117483848901
At time: 126.45430660247803 and batch: 600, loss is 5.268217887878418 and perplexity is 194.06979994390025
At time: 126.87693381309509 and batch: 650, loss is 5.234481296539307 and perplexity is 187.63175585726526
At time: 127.30358648300171 and batch: 700, loss is 5.22191312789917 and perplexity is 185.2883254686188
At time: 127.72429180145264 and batch: 750, loss is 5.158547420501709 and perplexity is 173.9116515410191
At time: 128.15002727508545 and batch: 800, loss is 5.1918869304656985 and perplexity is 179.80751731252892
At time: 128.57710647583008 and batch: 850, loss is 5.164183139801025 and perplexity is 174.89453582046144
At time: 129.00328707695007 and batch: 900, loss is 5.275273780822754 and perplexity is 195.44397800147547
At time: 129.41275429725647 and batch: 950, loss is 5.24438250541687 and perplexity is 189.49876463351345
At time: 129.84331059455872 and batch: 1000, loss is 5.203056974411011 and perplexity is 181.82723434667733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.434485830911776 and perplexity of 229.17498328487838
Finished 14 epochs...
Completing Train Step...
At time: 131.1742262840271 and batch: 50, loss is 5.27434024810791 and perplexity is 195.26160979068146
At time: 131.62203240394592 and batch: 100, loss is 5.246710395812988 and perplexity is 189.94041084031892
At time: 132.03967428207397 and batch: 150, loss is 5.288674945831299 and perplexity is 198.08078367240043
At time: 132.4829020500183 and batch: 200, loss is 5.256669273376465 and perplexity is 191.84145455505282
At time: 132.9028935432434 and batch: 250, loss is 5.250039129257202 and perplexity is 190.5737253210524
At time: 133.32424020767212 and batch: 300, loss is 5.153742513656616 and perplexity is 173.0780266050023
At time: 133.7306249141693 and batch: 350, loss is 5.240611810684204 and perplexity is 188.78556810808107
At time: 134.1362566947937 and batch: 400, loss is 5.166572761535645 and perplexity is 175.3129673519514
At time: 134.56983160972595 and batch: 450, loss is 5.18818434715271 and perplexity is 179.14299598134053
At time: 134.976797580719 and batch: 500, loss is 5.16804386138916 and perplexity is 175.57106002604408
At time: 135.39894795417786 and batch: 550, loss is 5.209633197784424 and perplexity is 183.0269112030992
At time: 135.81881284713745 and batch: 600, loss is 5.252460374832153 and perplexity is 191.03571017388202
At time: 136.25893998146057 and batch: 650, loss is 5.219788999557495 and perplexity is 184.89516699246533
At time: 136.67719197273254 and batch: 700, loss is 5.203411293029785 and perplexity is 181.89167053600335
At time: 137.10290956497192 and batch: 750, loss is 5.146440782546997 and perplexity is 171.81886003599988
At time: 137.52569007873535 and batch: 800, loss is 5.18254337310791 and perplexity is 178.13529985666642
At time: 137.9435031414032 and batch: 850, loss is 5.153786373138428 and perplexity is 173.08561788403577
At time: 138.35203313827515 and batch: 900, loss is 5.260238428115844 and perplexity is 192.5273897680761
At time: 138.75956773757935 and batch: 950, loss is 5.228961868286133 and perplexity is 186.59898860702208
At time: 139.17694687843323 and batch: 1000, loss is 5.191109342575073 and perplexity is 179.66775551000808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4223108059022485 and perplexity of 226.4016888536762
Finished 15 epochs...
Completing Train Step...
At time: 140.45920276641846 and batch: 50, loss is 5.265902509689331 and perplexity is 193.6209747624794
At time: 140.89962792396545 and batch: 100, loss is 5.232726669311523 and perplexity is 187.30282073329514
At time: 141.3227298259735 and batch: 150, loss is 5.273539066314697 and perplexity is 195.10523239572942
At time: 141.76630687713623 and batch: 200, loss is 5.243257236480713 and perplexity is 189.28564748977
At time: 142.19438099861145 and batch: 250, loss is 5.23419454574585 and perplexity is 187.57796001576426
At time: 142.62040281295776 and batch: 300, loss is 5.141734619140625 and perplexity is 171.01215214286853
At time: 143.04491257667542 and batch: 350, loss is 5.2296794891357425 and perplexity is 186.73294399060197
At time: 143.502516746521 and batch: 400, loss is 5.155135087966919 and perplexity is 173.31921851838362
At time: 143.92708110809326 and batch: 450, loss is 5.174418230056762 and perplexity is 176.69378923334
At time: 144.37066888809204 and batch: 500, loss is 5.148044643402099 and perplexity is 172.09465468878324
At time: 144.79867434501648 and batch: 550, loss is 5.186049947738647 and perplexity is 178.7610410428877
At time: 145.2204041481018 and batch: 600, loss is 5.243673086166382 and perplexity is 189.36437823572362
At time: 145.64578890800476 and batch: 650, loss is 5.204754486083984 and perplexity is 182.1361503195006
At time: 146.07718586921692 and batch: 700, loss is 5.189555368423462 and perplexity is 179.388773283793
At time: 146.50415706634521 and batch: 750, loss is 5.1353926086425785 and perplexity is 169.9310231680195
At time: 146.92878127098083 and batch: 800, loss is 5.158753662109375 and perplexity is 173.94752305859748
At time: 147.35786986351013 and batch: 850, loss is 5.132080669403076 and perplexity is 169.36915289807123
At time: 147.77717471122742 and batch: 900, loss is 5.241775074005127 and perplexity is 189.00530321509754
At time: 148.2160587310791 and batch: 950, loss is 5.212183055877685 and perplexity is 183.49419935997582
At time: 148.63595819473267 and batch: 1000, loss is 5.180320463180542 and perplexity is 177.73976091679242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.420034641172828 and perplexity of 225.88694735528438
Finished 16 epochs...
Completing Train Step...
At time: 149.9404067993164 and batch: 50, loss is 5.24747314453125 and perplexity is 190.08534291158279
At time: 150.36393475532532 and batch: 100, loss is 5.212434730529785 and perplexity is 183.54038601052292
At time: 150.78690266609192 and batch: 150, loss is 5.2585695934295655 and perplexity is 192.2063613281759
At time: 151.20297145843506 and batch: 200, loss is 5.229566993713379 and perplexity is 186.71193857072757
At time: 151.64459204673767 and batch: 250, loss is 5.218174457550049 and perplexity is 184.59688683609852
At time: 152.07039594650269 and batch: 300, loss is 5.127626123428345 and perplexity is 168.61636812177375
At time: 152.48732376098633 and batch: 350, loss is 5.215721874237061 and perplexity is 184.1447023285672
At time: 152.9082682132721 and batch: 400, loss is 5.1386016082763675 and perplexity is 170.47720764332306
At time: 153.33587050437927 and batch: 450, loss is 5.165325355529785 and perplexity is 175.09441724227028
At time: 153.76382422447205 and batch: 500, loss is 5.132857027053833 and perplexity is 169.50069499097398
At time: 154.2134838104248 and batch: 550, loss is 5.174829435348511 and perplexity is 176.76646159509772
At time: 154.64440178871155 and batch: 600, loss is 5.21875825881958 and perplexity is 184.70468619662503
At time: 155.06235098838806 and batch: 650, loss is 5.183045873641968 and perplexity is 178.2248354339326
At time: 155.4842917919159 and batch: 700, loss is 5.172360925674439 and perplexity is 176.3306499984728
At time: 155.90158557891846 and batch: 750, loss is 5.122055864334106 and perplexity is 167.67974230981358
At time: 156.31383991241455 and batch: 800, loss is 5.152316408157349 and perplexity is 172.8313749968688
At time: 156.72419714927673 and batch: 850, loss is 5.1232343101501465 and perplexity is 167.87746027764783
At time: 157.1366844177246 and batch: 900, loss is 5.233916940689087 and perplexity is 187.52589465266507
At time: 157.56957054138184 and batch: 950, loss is 5.201588850021363 and perplexity is 181.5604852076016
At time: 157.98141241073608 and batch: 1000, loss is 5.169402904510498 and perplexity is 175.80983088067157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.391550575814596 and perplexity of 219.54354067206916
Finished 17 epochs...
Completing Train Step...
At time: 159.27401971817017 and batch: 50, loss is 5.235428771972656 and perplexity is 187.80961658248452
At time: 159.71231842041016 and batch: 100, loss is 5.203529672622681 and perplexity is 181.91320407245294
At time: 160.13052916526794 and batch: 150, loss is 5.238269100189209 and perplexity is 188.3438158272617
At time: 160.55441403388977 and batch: 200, loss is 5.21605525970459 and perplexity is 184.20610373084497
At time: 160.98328566551208 and batch: 250, loss is 5.201681861877441 and perplexity is 181.57737327070413
At time: 161.40794396400452 and batch: 300, loss is 5.115144052505493 and perplexity is 166.52476756560802
At time: 161.8223376274109 and batch: 350, loss is 5.194178476333618 and perplexity is 180.22002694770867
At time: 162.24198532104492 and batch: 400, loss is 5.122524642944336 and perplexity is 167.75836541335212
At time: 162.6819543838501 and batch: 450, loss is 5.144966402053833 and perplexity is 171.56572031839215
At time: 163.11250972747803 and batch: 500, loss is 5.116624898910523 and perplexity is 166.7715478457541
At time: 163.5375542640686 and batch: 550, loss is 5.161060009002686 and perplexity is 174.34916937783538
At time: 163.96035075187683 and batch: 600, loss is 5.209752721786499 and perplexity is 183.04878861942578
At time: 164.37404799461365 and batch: 650, loss is 5.178610258102417 and perplexity is 177.43604925372668
At time: 164.80035495758057 and batch: 700, loss is 5.164552640914917 and perplexity is 174.95917148700067
At time: 165.23352527618408 and batch: 750, loss is 5.115223045349121 and perplexity is 166.53792235009257
At time: 165.6624608039856 and batch: 800, loss is 5.138213834762573 and perplexity is 170.41111391302059
At time: 166.08189702033997 and batch: 850, loss is 5.108075332641602 and perplexity is 165.35180120271224
At time: 166.50482368469238 and batch: 900, loss is 5.21931830406189 and perplexity is 184.8081581491446
At time: 166.92484211921692 and batch: 950, loss is 5.181811265945434 and perplexity is 178.0049334546628
At time: 167.3388991355896 and batch: 1000, loss is 5.146516351699829 and perplexity is 171.83184473230847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3871958662823936 and perplexity of 218.58957096081812
Finished 18 epochs...
Completing Train Step...
At time: 168.61756992340088 and batch: 50, loss is 5.216461524963379 and perplexity is 184.28095547504978
At time: 169.04177117347717 and batch: 100, loss is 5.184968528747558 and perplexity is 178.56782994814014
At time: 169.4673731327057 and batch: 150, loss is 5.2255674076080325 and perplexity is 185.96665949293933
At time: 169.8975305557251 and batch: 200, loss is 5.1947073554992675 and perplexity is 180.31536677459587
At time: 170.31181621551514 and batch: 250, loss is 5.184818696975708 and perplexity is 178.5410768180681
At time: 170.72463941574097 and batch: 300, loss is 5.096601305007934 and perplexity is 163.46539311166188
At time: 171.13964295387268 and batch: 350, loss is 5.179993982315064 and perplexity is 177.68174175740643
At time: 171.5466024875641 and batch: 400, loss is 5.112074995040894 and perplexity is 166.01447694083603
At time: 171.96068596839905 and batch: 450, loss is 5.134427547454834 and perplexity is 169.7671084395593
At time: 172.37200164794922 and batch: 500, loss is 5.101488399505615 and perplexity is 164.26621919738798
At time: 172.7828710079193 and batch: 550, loss is 5.142511911392212 and perplexity is 171.14513023838288
At time: 173.19772720336914 and batch: 600, loss is 5.196980962753296 and perplexity is 180.72579950512025
At time: 173.60437631607056 and batch: 650, loss is 5.161691570281983 and perplexity is 174.4593163409093
At time: 174.02023148536682 and batch: 700, loss is 5.143294868469238 and perplexity is 171.2791820008415
At time: 174.43968176841736 and batch: 750, loss is 5.091068687438965 and perplexity is 162.5634988244733
At time: 174.86254358291626 and batch: 800, loss is 5.121777191162109 and perplexity is 167.63302097443818
At time: 175.27458930015564 and batch: 850, loss is 5.089373788833618 and perplexity is 162.2882035417337
At time: 175.73328232765198 and batch: 900, loss is 5.2041362762451175 and perplexity is 182.02358675689572
At time: 176.16241097450256 and batch: 950, loss is 5.166461067199707 and perplexity is 175.29338698001044
At time: 176.58920621871948 and batch: 1000, loss is 5.129025964736939 and perplexity is 168.85256956287202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.381253498356517 and perplexity of 217.29448306078993
Finished 19 epochs...
Completing Train Step...
At time: 177.9881763458252 and batch: 50, loss is 5.19865683555603 and perplexity is 181.02892688778584
At time: 178.4190936088562 and batch: 100, loss is 5.164524221420288 and perplexity is 174.95419930642012
At time: 178.83298659324646 and batch: 150, loss is 5.209923238754272 and perplexity is 183.0800042051336
At time: 179.2622847557068 and batch: 200, loss is 5.181096076965332 and perplexity is 177.87767180134267
At time: 179.68937230110168 and batch: 250, loss is 5.171820077896118 and perplexity is 176.23530774331888
At time: 180.10738062858582 and batch: 300, loss is 5.080436382293701 and perplexity is 160.8442301901901
At time: 180.51828336715698 and batch: 350, loss is 5.163099308013916 and perplexity is 174.70508224959434
At time: 180.93901944160461 and batch: 400, loss is 5.094602832794189 and perplexity is 163.1390382797698
At time: 181.37329530715942 and batch: 450, loss is 5.122072725296021 and perplexity is 167.68256957539768
At time: 181.79145407676697 and batch: 500, loss is 5.093655910491943 and perplexity is 162.98463140325077
At time: 182.21601152420044 and batch: 550, loss is 5.13105504989624 and perplexity is 169.19553363987075
At time: 182.6247158050537 and batch: 600, loss is 5.183529663085937 and perplexity is 178.3110795882924
At time: 183.05839347839355 and batch: 650, loss is 5.151529197692871 and perplexity is 172.69537386766055
At time: 183.48478531837463 and batch: 700, loss is 5.137051162719726 and perplexity is 170.21309681186398
At time: 183.91013288497925 and batch: 750, loss is 5.084906406402588 and perplexity is 161.5648170974959
At time: 184.32296085357666 and batch: 800, loss is 5.10867561340332 and perplexity is 165.45108850503703
At time: 184.73546385765076 and batch: 850, loss is 5.0780793571472165 and perplexity is 160.46556273463165
At time: 185.1562376022339 and batch: 900, loss is 5.188800153732299 and perplexity is 179.25334739102803
At time: 185.58766770362854 and batch: 950, loss is 5.155475645065308 and perplexity is 173.37825366038507
At time: 186.01121997833252 and batch: 1000, loss is 5.118926134109497 and perplexity is 167.15577032543135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.378199507550495 and perplexity of 216.63188001412973
Finished 20 epochs...
Completing Train Step...
At time: 187.37885999679565 and batch: 50, loss is 5.188496379852295 and perplexity is 179.1989031759712
At time: 187.8256540298462 and batch: 100, loss is 5.1523050117492675 and perplexity is 172.82940535121355
At time: 188.24659872055054 and batch: 150, loss is 5.204512262344361 and perplexity is 182.09203796279527
At time: 188.66955471038818 and batch: 200, loss is 5.171188945770264 and perplexity is 176.1241150712169
At time: 189.0911750793457 and batch: 250, loss is 5.15709620475769 and perplexity is 173.65945125700765
At time: 189.51398086547852 and batch: 300, loss is 5.070159587860108 and perplexity is 159.19973166063826
At time: 189.9288637638092 and batch: 350, loss is 5.15266432762146 and perplexity is 172.89151685789474
At time: 190.3610155582428 and batch: 400, loss is 5.089734983444214 and perplexity is 162.34683175367599
At time: 190.78355813026428 and batch: 450, loss is 5.113026733398438 and perplexity is 166.17255449875356
At time: 191.21152687072754 and batch: 500, loss is 5.081872472763061 and perplexity is 161.07538299466088
At time: 191.62572312355042 and batch: 550, loss is 5.122085208892822 and perplexity is 167.6846628700528
At time: 192.0399878025055 and batch: 600, loss is 5.175866498947143 and perplexity is 176.94987474699028
At time: 192.47076272964478 and batch: 650, loss is 5.140593681335449 and perplexity is 170.8171491776255
At time: 192.893235206604 and batch: 700, loss is 5.128030138015747 and perplexity is 168.68450535746146
At time: 193.31538796424866 and batch: 750, loss is 5.076375303268432 and perplexity is 160.19235361768216
At time: 193.73873662948608 and batch: 800, loss is 5.095702581405639 and perplexity is 163.31854890077702
At time: 194.1613531112671 and batch: 850, loss is 5.069623966217041 and perplexity is 159.11448367116256
At time: 194.56903886795044 and batch: 900, loss is 5.184276456832886 and perplexity is 178.4442909220451
At time: 194.97919631004333 and batch: 950, loss is 5.147782678604126 and perplexity is 172.04957785186545
At time: 195.3888454437256 and batch: 1000, loss is 5.113868722915649 and perplexity is 166.3125289679381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.377515839367378 and perplexity of 216.48382630588677
Finished 21 epochs...
Completing Train Step...
At time: 196.7163643836975 and batch: 50, loss is 5.18179536819458 and perplexity is 178.0021035990741
At time: 197.17236518859863 and batch: 100, loss is 5.149054002761841 and perplexity is 172.26844773431833
At time: 197.59677505493164 and batch: 150, loss is 5.193170747756958 and perplexity is 180.03850555397642
At time: 198.03991889953613 and batch: 200, loss is 5.16797119140625 and perplexity is 175.55830174369012
At time: 198.4844880104065 and batch: 250, loss is 5.153251848220825 and perplexity is 172.99312403073498
At time: 198.90825366973877 and batch: 300, loss is 5.062863483428955 and perplexity is 158.04242085720742
At time: 199.33174180984497 and batch: 350, loss is 5.14295823097229 and perplexity is 171.22153270973445
At time: 199.75274467468262 and batch: 400, loss is 5.074166612625122 and perplexity is 159.83892871193467
At time: 200.17139291763306 and batch: 450, loss is 5.103256435394287 and perplexity is 164.5569046636714
At time: 200.59888648986816 and batch: 500, loss is 5.072123374938965 and perplexity is 159.51267321134728
At time: 201.0319254398346 and batch: 550, loss is 5.11403829574585 and perplexity is 166.340733445462
At time: 201.46472334861755 and batch: 600, loss is 5.165792121887207 and perplexity is 175.17616450256253
At time: 201.8758246898651 and batch: 650, loss is 5.133303918838501 and perplexity is 169.57646038726654
At time: 202.29841494560242 and batch: 700, loss is 5.1184773349761965 and perplexity is 167.08076779237797
At time: 202.7253496646881 and batch: 750, loss is 5.070295543670654 and perplexity is 159.22137726058418
At time: 203.16044330596924 and batch: 800, loss is 5.089290046691895 and perplexity is 162.27461374901984
At time: 203.58266186714172 and batch: 850, loss is 5.058713359832764 and perplexity is 157.38788442021644
At time: 203.99527144432068 and batch: 900, loss is 5.179685020446778 and perplexity is 177.6268533541612
At time: 204.41669297218323 and batch: 950, loss is 5.140855236053467 and perplexity is 170.86183305229557
At time: 204.82958316802979 and batch: 1000, loss is 5.100796136856079 and perplexity is 164.15254318062716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.375893197408536 and perplexity of 216.13283540920256
Finished 22 epochs...
Completing Train Step...
At time: 206.15396070480347 and batch: 50, loss is 5.17426474571228 and perplexity is 176.66667158404604
At time: 206.57169795036316 and batch: 100, loss is 5.1375391387939455 and perplexity is 170.29617699955028
At time: 206.98757076263428 and batch: 150, loss is 5.188418502807617 and perplexity is 179.18494823837403
At time: 207.3969578742981 and batch: 200, loss is 5.16312271118164 and perplexity is 174.70917094978066
At time: 207.81341338157654 and batch: 250, loss is 5.146769323348999 and perplexity is 171.87531881607376
At time: 208.2340931892395 and batch: 300, loss is 5.055143690109253 and perplexity is 156.82706322310182
At time: 208.6701488494873 and batch: 350, loss is 5.137436017990113 and perplexity is 170.27861682631357
At time: 209.0780713558197 and batch: 400, loss is 5.066235847473145 and perplexity is 158.57629714138142
At time: 209.49172472953796 and batch: 450, loss is 5.098523664474487 and perplexity is 163.7799345915949
At time: 209.9171748161316 and batch: 500, loss is 5.065668172836304 and perplexity is 158.48630294561806
At time: 210.33298015594482 and batch: 550, loss is 5.105065069198608 and perplexity is 164.8547971521886
At time: 210.7433648109436 and batch: 600, loss is 5.160961046218872 and perplexity is 174.3319161524058
At time: 211.1528868675232 and batch: 650, loss is 5.122049350738525 and perplexity is 167.6786501153421
At time: 211.56581449508667 and batch: 700, loss is 5.107326211929322 and perplexity is 165.22797912824936
At time: 211.99220275878906 and batch: 750, loss is 5.060182638168335 and perplexity is 157.6193009951251
At time: 212.42196488380432 and batch: 800, loss is 5.079788627624512 and perplexity is 160.74007632579003
At time: 212.8345603942871 and batch: 850, loss is 5.048645505905151 and perplexity is 155.81127603862527
At time: 213.2706048488617 and batch: 900, loss is 5.164357919692993 and perplexity is 174.9251065400335
At time: 213.6959309577942 and batch: 950, loss is 5.130156097412109 and perplexity is 169.0435032388928
At time: 214.12195253372192 and batch: 1000, loss is 5.093378562927246 and perplexity is 162.93943428059364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.36930102836795 and perplexity of 214.71273712964788
Finished 23 epochs...
Completing Train Step...
At time: 215.4042775630951 and batch: 50, loss is 5.165683145523071 and perplexity is 175.15707548121685
At time: 215.831876039505 and batch: 100, loss is 5.127615928649902 and perplexity is 168.61464912402133
At time: 216.24378752708435 and batch: 150, loss is 5.178395252227784 and perplexity is 177.39790356168632
At time: 216.6633267402649 and batch: 200, loss is 5.148891134262085 and perplexity is 172.2403929153657
At time: 217.08263301849365 and batch: 250, loss is 5.140469169616699 and perplexity is 170.79588176483787
At time: 217.51126885414124 and batch: 300, loss is 5.046871156692505 and perplexity is 155.535057550162
At time: 217.94877529144287 and batch: 350, loss is 5.126732578277588 and perplexity is 168.4657690772434
At time: 218.37987995147705 and batch: 400, loss is 5.058788194656372 and perplexity is 157.3996629555019
At time: 218.79850602149963 and batch: 450, loss is 5.087742385864257 and perplexity is 162.02366193024136
At time: 219.20978355407715 and batch: 500, loss is 5.0581217384338375 and perplexity is 157.29479791856005
At time: 219.6467056274414 and batch: 550, loss is 5.094482574462891 and perplexity is 163.11942063087315
At time: 220.0637481212616 and batch: 600, loss is 5.150985336303711 and perplexity is 172.60147705746658
At time: 220.47991800308228 and batch: 650, loss is 5.115767822265625 and perplexity is 166.62867308311928
At time: 220.90060091018677 and batch: 700, loss is 5.101228303909302 and perplexity is 164.2234998329519
At time: 221.3222029209137 and batch: 750, loss is 5.0534717464447025 and perplexity is 156.56507628304803
At time: 221.76868867874146 and batch: 800, loss is 5.0716228103637695 and perplexity is 159.43284679864698
At time: 222.19039511680603 and batch: 850, loss is 5.040819444656372 and perplexity is 154.59664653094262
At time: 222.61451864242554 and batch: 900, loss is 5.160221204757691 and perplexity is 174.20298587269338
At time: 223.03522729873657 and batch: 950, loss is 5.121995429992676 and perplexity is 167.66960900121896
At time: 223.4634644985199 and batch: 1000, loss is 5.0828917121887205 and perplexity is 161.23964107044566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.364525399556974 and perplexity of 213.6897933402021
Finished 24 epochs...
Completing Train Step...
At time: 224.7651207447052 and batch: 50, loss is 5.1557017421722415 and perplexity is 173.41745841381655
At time: 225.21407270431519 and batch: 100, loss is 5.117732915878296 and perplexity is 166.9564359611779
At time: 225.62796568870544 and batch: 150, loss is 5.171659832000732 and perplexity is 176.207069021261
At time: 226.04371881484985 and batch: 200, loss is 5.138836030960083 and perplexity is 170.51717605241893
At time: 226.46907305717468 and batch: 250, loss is 5.130163946151733 and perplexity is 169.0448300225417
At time: 226.89725923538208 and batch: 300, loss is 5.038106946945191 and perplexity is 154.17787170072165
At time: 227.32575964927673 and batch: 350, loss is 5.118872346878052 and perplexity is 167.14677972111755
At time: 227.74422311782837 and batch: 400, loss is 5.0499459075927735 and perplexity is 156.01402508396538
At time: 228.1757538318634 and batch: 450, loss is 5.080397243499756 and perplexity is 160.8379350642
At time: 228.60211324691772 and batch: 500, loss is 5.05081711769104 and perplexity is 156.15000530315902
At time: 229.02374911308289 and batch: 550, loss is 5.088585634231567 and perplexity is 162.16034573972985
At time: 229.43520140647888 and batch: 600, loss is 5.143145399093628 and perplexity is 171.25358292163972
At time: 229.86676955223083 and batch: 650, loss is 5.105213842391968 and perplexity is 164.8793249512958
At time: 230.324383020401 and batch: 700, loss is 5.096563701629639 and perplexity is 163.45924637621619
At time: 230.73466897010803 and batch: 750, loss is 5.048069753646851 and perplexity is 155.72159316461338
At time: 231.1433732509613 and batch: 800, loss is 5.063701267242432 and perplexity is 158.17488171827932
At time: 231.56441950798035 and batch: 850, loss is 5.026940145492554 and perplexity is 152.4657751314149
At time: 231.99485278129578 and batch: 900, loss is 5.150924158096314 and perplexity is 172.59091793150364
At time: 232.41557908058167 and batch: 950, loss is 5.107389669418335 and perplexity is 165.23846441360106
At time: 232.83165574073792 and batch: 1000, loss is 5.075416793823242 and perplexity is 160.03888129777664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.369530654535061 and perplexity of 214.76204645364376
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 234.14108991622925 and batch: 50, loss is 5.101179208755493 and perplexity is 164.21543745288204
At time: 234.57761001586914 and batch: 100, loss is 5.009685363769531 and perplexity is 149.85757810713866
At time: 235.00011205673218 and batch: 150, loss is 5.023355188369751 and perplexity is 151.92017043393594
At time: 235.4198648929596 and batch: 200, loss is 5.019498853683472 and perplexity is 151.335443586995
At time: 235.8515100479126 and batch: 250, loss is 5.002358016967773 and perplexity is 148.76353278117395
At time: 236.27609968185425 and batch: 300, loss is 4.89593770980835 and perplexity is 133.74536216972237
At time: 236.6932933330536 and batch: 350, loss is 4.968001909255982 and perplexity is 143.73939589305206
At time: 237.1095254421234 and batch: 400, loss is 4.8946958255767825 and perplexity is 133.57936900687824
At time: 237.51831817626953 and batch: 450, loss is 4.9247005176544185 and perplexity is 137.64811228990166
At time: 237.9244384765625 and batch: 500, loss is 4.884460048675537 and perplexity is 132.21905420332607
At time: 238.35224318504333 and batch: 550, loss is 4.93546690940857 and perplexity is 139.13809225244384
At time: 238.77189803123474 and batch: 600, loss is 4.975325441360473 and perplexity is 144.79594006892017
At time: 239.1838734149933 and batch: 650, loss is 4.937426109313964 and perplexity is 139.4109588023987
At time: 239.5923626422882 and batch: 700, loss is 4.919005527496338 and perplexity is 136.8664355783916
At time: 240.0058138370514 and batch: 750, loss is 4.871002588272095 and perplexity is 130.45164063944355
At time: 240.41886186599731 and batch: 800, loss is 4.874236631393432 and perplexity is 130.87420980566802
At time: 240.8592438697815 and batch: 850, loss is 4.8554394245147705 and perplexity is 128.4371172342432
At time: 241.31253671646118 and batch: 900, loss is 4.96209225654602 and perplexity is 142.89245102249927
At time: 241.73261761665344 and batch: 950, loss is 4.923993568420411 and perplexity is 137.55083645094695
At time: 242.15660166740417 and batch: 1000, loss is 4.887300472259522 and perplexity is 132.59514620056734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.213950552591464 and perplexity of 183.81881154531888
Finished 26 epochs...
Completing Train Step...
At time: 243.43193674087524 and batch: 50, loss is 4.9961834907531735 and perplexity is 147.84781840872083
At time: 243.86242508888245 and batch: 100, loss is 4.952096652984619 and perplexity is 141.47126934721857
At time: 244.27084922790527 and batch: 150, loss is 4.989506549835205 and perplexity is 146.8639355793645
At time: 244.69611430168152 and batch: 200, loss is 4.989110689163208 and perplexity is 146.8058094288215
At time: 245.11789846420288 and batch: 250, loss is 4.971205081939697 and perplexity is 144.2005561932846
At time: 245.55063104629517 and batch: 300, loss is 4.872445888519287 and perplexity is 130.64005746294126
At time: 245.98073172569275 and batch: 350, loss is 4.9406796741485595 and perplexity is 139.86528007660098
At time: 246.40302419662476 and batch: 400, loss is 4.873345994949341 and perplexity is 130.75770035630586
At time: 246.83556461334229 and batch: 450, loss is 4.9084100341796875 and perplexity is 135.4239237324698
At time: 247.2609052658081 and batch: 500, loss is 4.868641691207886 and perplexity is 130.14402101605458
At time: 247.69553446769714 and batch: 550, loss is 4.919187774658203 and perplexity is 136.89138137091135
At time: 248.11801290512085 and batch: 600, loss is 4.962233638763427 and perplexity is 142.91265490227659
At time: 248.53456711769104 and batch: 650, loss is 4.928489570617676 and perplexity is 138.1706576284638
At time: 248.95828080177307 and batch: 700, loss is 4.912924709320069 and perplexity is 136.0367009585957
At time: 249.3873963356018 and batch: 750, loss is 4.864530143737793 and perplexity is 129.61002622043083
At time: 249.79707074165344 and batch: 800, loss is 4.869296398162842 and perplexity is 130.22925511039233
At time: 250.207172870636 and batch: 850, loss is 4.8546857738494875 and perplexity is 128.34035698168682
At time: 250.62420105934143 and batch: 900, loss is 4.960912284851074 and perplexity is 142.72394141270794
At time: 251.05239295959473 and batch: 950, loss is 4.924025220870972 and perplexity is 137.55519034090264
At time: 251.4617795944214 and batch: 1000, loss is 4.884391078948974 and perplexity is 132.20993540577535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2105958519912345 and perplexity of 183.20318766215289
Finished 27 epochs...
Completing Train Step...
At time: 252.80226588249207 and batch: 50, loss is 4.9831107711791995 and perplexity is 145.92762376812536
At time: 253.24280524253845 and batch: 100, loss is 4.9393312454223635 and perplexity is 139.67680881373818
At time: 253.6580867767334 and batch: 150, loss is 4.976906805038452 and perplexity is 145.02509625114112
At time: 254.07660222053528 and batch: 200, loss is 4.980965633392334 and perplexity is 145.61492441973706
At time: 254.4978301525116 and batch: 250, loss is 4.963513135910034 and perplexity is 143.09562826842566
At time: 254.91447591781616 and batch: 300, loss is 4.860597505569458 and perplexity is 129.1013178229727
At time: 255.343998670578 and batch: 350, loss is 4.929848957061767 and perplexity is 138.35861267023913
At time: 255.77007246017456 and batch: 400, loss is 4.859976005554199 and perplexity is 129.02110628029524
At time: 256.1942255496979 and batch: 450, loss is 4.896084251403809 and perplexity is 133.76496286460346
At time: 256.6306953430176 and batch: 500, loss is 4.858555383682251 and perplexity is 128.83794620617817
At time: 257.0435049533844 and batch: 550, loss is 4.907138786315918 and perplexity is 135.25187573971337
At time: 257.46686267852783 and batch: 600, loss is 4.952386960983277 and perplexity is 141.51234555037695
At time: 257.8736574649811 and batch: 650, loss is 4.916721687316895 and perplexity is 136.5542111846199
At time: 258.3011095523834 and batch: 700, loss is 4.90027045249939 and perplexity is 134.32610360321962
At time: 258.72753620147705 and batch: 750, loss is 4.851380310058594 and perplexity is 127.91683293609391
At time: 259.1533420085907 and batch: 800, loss is 4.857107429504395 and perplexity is 128.65152975750993
At time: 259.57955050468445 and batch: 850, loss is 4.844799480438232 and perplexity is 127.07779785445192
At time: 259.9946439266205 and batch: 900, loss is 4.9526467037200925 and perplexity is 141.54910712837318
At time: 260.4193775653839 and batch: 950, loss is 4.912009363174438 and perplexity is 135.9122372610821
At time: 260.84877038002014 and batch: 1000, loss is 4.875780506134033 and perplexity is 131.0764192452414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.198407801186166 and perplexity of 180.98385007612177
Finished 28 epochs...
Completing Train Step...
At time: 262.1909885406494 and batch: 50, loss is 4.969023389816284 and perplexity is 143.88629790770912
At time: 262.6023597717285 and batch: 100, loss is 4.926039876937867 and perplexity is 137.83259608440912
At time: 263.0321228504181 and batch: 150, loss is 4.96557071685791 and perplexity is 143.39036222237624
At time: 263.4448127746582 and batch: 200, loss is 4.965242538452149 and perplexity is 143.3433123227033
At time: 263.86518359184265 and batch: 250, loss is 4.950880374908447 and perplexity is 141.29930554300506
At time: 264.27487874031067 and batch: 300, loss is 4.8464391326904295 and perplexity is 127.28633216706622
At time: 264.698073387146 and batch: 350, loss is 4.9147156238555905 and perplexity is 136.28054935434898
At time: 265.1182191371918 and batch: 400, loss is 4.846698884963989 and perplexity is 127.31939937569507
At time: 265.5347945690155 and batch: 450, loss is 4.8857324600219725 and perplexity is 132.38739830687018
At time: 265.9528124332428 and batch: 500, loss is 4.851879949569702 and perplexity is 127.98076120918029
At time: 266.36484956741333 and batch: 550, loss is 4.896683855056763 and perplexity is 133.84519287567443
At time: 266.7809703350067 and batch: 600, loss is 4.941154947280884 and perplexity is 139.93177008557225
At time: 267.21658158302307 and batch: 650, loss is 4.910023183822632 and perplexity is 135.64255908504336
At time: 267.6355788707733 and batch: 700, loss is 4.892940511703491 and perplexity is 133.34510095440706
At time: 268.04805731773376 and batch: 750, loss is 4.84266731262207 and perplexity is 126.80713531523459
At time: 268.46554374694824 and batch: 800, loss is 4.851466846466065 and perplexity is 127.92790287824086
At time: 268.8940324783325 and batch: 850, loss is 4.841080684661865 and perplexity is 126.60609909580712
At time: 269.33529019355774 and batch: 900, loss is 4.947720289230347 and perplexity is 140.85349240608363
At time: 269.7581753730774 and batch: 950, loss is 4.906531457901001 and perplexity is 135.16975837103556
At time: 270.1852705478668 and batch: 1000, loss is 4.868524188995361 and perplexity is 130.12872970403748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.193367562642911 and perplexity of 180.0739432991377
Finished 29 epochs...
Completing Train Step...
At time: 271.5224163532257 and batch: 50, loss is 4.958463850021363 and perplexity is 142.37491859768886
At time: 271.9579608440399 and batch: 100, loss is 4.914549102783203 and perplexity is 136.25785766050004
At time: 272.3821904659271 and batch: 150, loss is 4.955540046691895 and perplexity is 141.95925029895415
At time: 272.80367732048035 and batch: 200, loss is 4.957522287368774 and perplexity is 142.24092678241556
At time: 273.23033380508423 and batch: 250, loss is 4.942298564910889 and perplexity is 140.0918900654046
At time: 273.6664779186249 and batch: 300, loss is 4.837863531112671 and perplexity is 126.1994423241336
At time: 274.0845191478729 and batch: 350, loss is 4.906968488693237 and perplexity is 135.22884462793922
At time: 274.4939560890198 and batch: 400, loss is 4.839052724838257 and perplexity is 126.34960717897279
At time: 274.91294026374817 and batch: 450, loss is 4.879175939559937 and perplexity is 131.52223694433502
At time: 275.3361258506775 and batch: 500, loss is 4.842664966583252 and perplexity is 126.80683782112166
At time: 275.76067757606506 and batch: 550, loss is 4.888167791366577 and perplexity is 132.71019839063896
At time: 276.1816041469574 and batch: 600, loss is 4.9337899875640865 and perplexity is 138.90496406964814
At time: 276.6038568019867 and batch: 650, loss is 4.90170955657959 and perplexity is 134.51955200980169
At time: 277.0247333049774 and batch: 700, loss is 4.882483577728271 and perplexity is 131.95798516668873
At time: 277.4445655345917 and batch: 750, loss is 4.834134044647217 and perplexity is 125.72965978037698
At time: 277.8530251979828 and batch: 800, loss is 4.8457197570800785 and perplexity is 127.19479841169716
At time: 278.2579221725464 and batch: 850, loss is 4.832139043807984 and perplexity is 125.47907904110103
At time: 278.6743288040161 and batch: 900, loss is 4.940007190704346 and perplexity is 139.77125461015743
At time: 279.0952866077423 and batch: 950, loss is 4.898535804748535 and perplexity is 134.09329710673228
At time: 279.5292263031006 and batch: 1000, loss is 4.861441946029663 and perplexity is 129.2103822419313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.186930958817645 and perplexity of 178.91860089633667
Finished 30 epochs...
Completing Train Step...
At time: 280.82823967933655 and batch: 50, loss is 4.949583673477173 and perplexity is 141.1162012727106
At time: 281.25509095191956 and batch: 100, loss is 4.90653136253357 and perplexity is 135.16974548024353
At time: 281.6714656352997 and batch: 150, loss is 4.947488145828247 and perplexity is 140.82079799220196
At time: 282.0988657474518 and batch: 200, loss is 4.949593486785889 and perplexity is 141.11758609635334
At time: 282.5141656398773 and batch: 250, loss is 4.934405708312989 and perplexity is 138.99051707382839
At time: 282.92482805252075 and batch: 300, loss is 4.83160249710083 and perplexity is 125.41177171280225
At time: 283.3405089378357 and batch: 350, loss is 4.899597816467285 and perplexity is 134.23578140627134
At time: 283.7505793571472 and batch: 400, loss is 4.832787942886353 and perplexity is 125.56052872329924
At time: 284.1760425567627 and batch: 450, loss is 4.871754961013794 and perplexity is 130.54982582926925
At time: 284.6010375022888 and batch: 500, loss is 4.836054592132569 and perplexity is 125.97136158790846
At time: 285.01618933677673 and batch: 550, loss is 4.882044248580932 and perplexity is 131.90002491032817
At time: 285.4279794692993 and batch: 600, loss is 4.928225803375244 and perplexity is 138.13421754117812
At time: 285.8547098636627 and batch: 650, loss is 4.896910228729248 and perplexity is 133.8754953332401
At time: 286.27035760879517 and batch: 700, loss is 4.878163652420044 and perplexity is 131.38916603961258
At time: 286.68898582458496 and batch: 750, loss is 4.827876863479614 and perplexity is 124.94540269909824
At time: 287.1083650588989 and batch: 800, loss is 4.838616285324097 and perplexity is 126.29447524955269
At time: 287.5153160095215 and batch: 850, loss is 4.824043064117432 and perplexity is 124.4673041464775
At time: 287.9292275905609 and batch: 900, loss is 4.933701686859131 and perplexity is 138.892699204905
At time: 288.3528780937195 and batch: 950, loss is 4.8936498260498045 and perplexity is 133.43971810020702
At time: 288.783962726593 and batch: 1000, loss is 4.855545606613159 and perplexity is 128.45075568092872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.184862462485709 and perplexity of 178.54889093034421
Finished 31 epochs...
Completing Train Step...
At time: 290.077232837677 and batch: 50, loss is 4.943490657806397 and perplexity is 140.2589921931106
At time: 290.498685836792 and batch: 100, loss is 4.899982681274414 and perplexity is 134.28745397723137
At time: 290.91620993614197 and batch: 150, loss is 4.94134521484375 and perplexity is 139.95839709548179
At time: 291.3474712371826 and batch: 200, loss is 4.9446385478973385 and perplexity is 140.42008654202914
At time: 291.77586579322815 and batch: 250, loss is 4.927185401916504 and perplexity is 137.99057723449548
At time: 292.2133345603943 and batch: 300, loss is 4.825946855545044 and perplexity is 124.70448963734331
At time: 292.63478350639343 and batch: 350, loss is 4.894840497970581 and perplexity is 133.5986956519368
At time: 293.053923368454 and batch: 400, loss is 4.824159069061279 and perplexity is 124.48174380662564
At time: 293.4732599258423 and batch: 450, loss is 4.864769840240479 and perplexity is 129.64109701406048
At time: 293.8849618434906 and batch: 500, loss is 4.829610023498535 and perplexity is 125.16214084229257
At time: 294.30953764915466 and batch: 550, loss is 4.876771421432495 and perplexity is 131.2063692484289
At time: 294.738361120224 and batch: 600, loss is 4.923063726425171 and perplexity is 137.42299535195679
At time: 295.1792531013489 and batch: 650, loss is 4.889412355422974 and perplexity is 132.8754675561721
At time: 295.6097984313965 and batch: 700, loss is 4.871292190551758 and perplexity is 130.4894252029423
At time: 296.02557492256165 and batch: 750, loss is 4.821470489501953 and perplexity is 124.14751423744222
At time: 296.4353942871094 and batch: 800, loss is 4.831775646209717 and perplexity is 125.43348852938774
At time: 296.8604862689972 and batch: 850, loss is 4.818672513961792 and perplexity is 123.80063803110255
At time: 297.28325486183167 and batch: 900, loss is 4.929082117080688 and perplexity is 138.25255442436583
At time: 297.71664118766785 and batch: 950, loss is 4.887446460723877 and perplexity is 132.61450497538658
At time: 298.13653564453125 and batch: 1000, loss is 4.850424661636352 and perplexity is 127.79464780883139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1821958960556405 and perplexity of 178.07341268057658
Finished 32 epochs...
Completing Train Step...
At time: 299.4154860973358 and batch: 50, loss is 4.939348602294922 and perplexity is 139.67923318734788
At time: 299.84600710868835 and batch: 100, loss is 4.894337015151978 and perplexity is 133.531447934544
At time: 300.2570357322693 and batch: 150, loss is 4.935737762451172 and perplexity is 139.1757833322137
At time: 300.6779954433441 and batch: 200, loss is 4.941415214538575 and perplexity is 139.9681944834697
At time: 301.0870273113251 and batch: 250, loss is 4.9234042072296145 and perplexity is 137.46979321039746
At time: 301.5066683292389 and batch: 300, loss is 4.821359405517578 and perplexity is 124.13372420285059
At time: 301.9246623516083 and batch: 350, loss is 4.889793701171875 and perplexity is 132.92614871376577
At time: 302.3547787666321 and batch: 400, loss is 4.819461135864258 and perplexity is 123.8983084332404
At time: 302.77551770210266 and batch: 450, loss is 4.861704406738281 and perplexity is 129.24429934117362
At time: 303.19166374206543 and batch: 500, loss is 4.826615715026856 and perplexity is 124.78792731859326
At time: 303.6128354072571 and batch: 550, loss is 4.872158651351929 and perplexity is 130.60253817161748
At time: 304.02624702453613 and batch: 600, loss is 4.918731422424316 and perplexity is 136.8289249353737
At time: 304.45567440986633 and batch: 650, loss is 4.885820550918579 and perplexity is 132.39906094516505
At time: 304.87480211257935 and batch: 700, loss is 4.86810411453247 and perplexity is 130.07407742761686
At time: 305.30846881866455 and batch: 750, loss is 4.818420248031616 and perplexity is 123.76941128688077
At time: 305.7170066833496 and batch: 800, loss is 4.827879581451416 and perplexity is 124.9457422976411
At time: 306.1611576080322 and batch: 850, loss is 4.814092979431153 and perplexity is 123.23498493684188
At time: 306.58098101615906 and batch: 900, loss is 4.925431318283081 and perplexity is 137.7487423826863
At time: 306.99347138404846 and batch: 950, loss is 4.882143745422363 and perplexity is 131.91314919909317
At time: 307.4112010002136 and batch: 1000, loss is 4.842768030166626 and perplexity is 126.8199076617246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.183080161490092 and perplexity of 178.23094648478875
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 308.68295192718506 and batch: 50, loss is 4.923519639968872 and perplexity is 137.48566264110124
At time: 309.1218960285187 and batch: 100, loss is 4.861582946777344 and perplexity is 129.2286022869254
At time: 309.5408453941345 and batch: 150, loss is 4.8997032260894775 and perplexity is 134.24993189505986
At time: 309.9682047367096 and batch: 200, loss is 4.895920639038086 and perplexity is 133.74307905285823
At time: 310.3879761695862 and batch: 250, loss is 4.870495700836182 and perplexity is 130.3855330977658
At time: 310.8155109882355 and batch: 300, loss is 4.77292368888855 and perplexity is 118.26450561394917
At time: 311.2265884876251 and batch: 350, loss is 4.842621994018555 and perplexity is 126.80138872316108
At time: 311.6530635356903 and batch: 400, loss is 4.764933786392212 and perplexity is 117.32334862842558
At time: 312.06736993789673 and batch: 450, loss is 4.806252679824829 and perplexity is 122.27256349918831
At time: 312.49374318122864 and batch: 500, loss is 4.770932950973511 and perplexity is 118.02930616652422
At time: 312.9107315540314 and batch: 550, loss is 4.811612501144409 and perplexity is 122.92968203768324
At time: 313.32194685935974 and batch: 600, loss is 4.857891998291016 and perplexity is 128.75250533807343
At time: 313.74781107902527 and batch: 650, loss is 4.8229688453674315 and perplexity is 124.3336708231626
At time: 314.174254655838 and batch: 700, loss is 4.801281938552856 and perplexity is 121.66628629327565
At time: 314.5872292518616 and batch: 750, loss is 4.750618047714234 and perplexity is 115.65574321020421
At time: 314.9966814517975 and batch: 800, loss is 4.753095264434815 and perplexity is 115.94260271099643
At time: 315.41432547569275 and batch: 850, loss is 4.736328983306885 and perplexity is 114.01488195129839
At time: 315.83511567115784 and batch: 900, loss is 4.850542526245118 and perplexity is 127.80971116269839
At time: 316.2463843822479 and batch: 950, loss is 4.798349170684815 and perplexity is 121.30999004069041
At time: 316.6689283847809 and batch: 1000, loss is 4.77273250579834 and perplexity is 118.24189760150738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.122349436690167 and perplexity of 167.7289756732509
Finished 34 epochs...
Completing Train Step...
At time: 318.04495644569397 and batch: 50, loss is 4.889199075698852 and perplexity is 132.84713093502998
At time: 318.4697816371918 and batch: 100, loss is 4.835501546859741 and perplexity is 125.90171298306171
At time: 318.89859771728516 and batch: 150, loss is 4.879160041809082 and perplexity is 131.52014605320053
At time: 319.30686593055725 and batch: 200, loss is 4.875233955383301 and perplexity is 131.0047989037582
At time: 319.73245191574097 and batch: 250, loss is 4.85543272972107 and perplexity is 128.43625737711807
At time: 320.14971470832825 and batch: 300, loss is 4.759093389511109 and perplexity is 116.64013078276204
At time: 320.57988142967224 and batch: 350, loss is 4.829673881530762 and perplexity is 125.17013370551757
At time: 321.00828671455383 and batch: 400, loss is 4.7550535488128665 and perplexity is 116.1698737566755
At time: 321.4307019710541 and batch: 450, loss is 4.797279319763184 and perplexity is 121.18027583584548
At time: 321.84639739990234 and batch: 500, loss is 4.763470888137817 and perplexity is 117.15184198548799
At time: 322.26798701286316 and batch: 550, loss is 4.803287687301636 and perplexity is 121.9105631919246
At time: 322.6940279006958 and batch: 600, loss is 4.849441566467285 and perplexity is 127.66907524294291
At time: 323.1175112724304 and batch: 650, loss is 4.818580837249756 and perplexity is 123.78928891589263
At time: 323.54741406440735 and batch: 700, loss is 4.798744602203369 and perplexity is 121.35796931990244
At time: 323.9731032848358 and batch: 750, loss is 4.749121942520142 and perplexity is 115.48283942543739
At time: 324.40130376815796 and batch: 800, loss is 4.751961536407471 and perplexity is 115.81122981737714
At time: 324.82962346076965 and batch: 850, loss is 4.736288185119629 and perplexity is 114.0102304456817
At time: 325.2755432128906 and batch: 900, loss is 4.853645601272583 and perplexity is 128.2069302672792
At time: 325.6862635612488 and batch: 950, loss is 4.8027189826965335 and perplexity is 121.84125180394754
At time: 326.09770035743713 and batch: 1000, loss is 4.775967922210693 and perplexity is 118.62507891893758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.119377694478849 and perplexity of 167.23126829148063
Finished 35 epochs...
Completing Train Step...
At time: 327.40877199172974 and batch: 50, loss is 4.880624885559082 and perplexity is 131.71294369187876
At time: 327.8563597202301 and batch: 100, loss is 4.828362379074097 and perplexity is 125.00608036935323
At time: 328.28709268569946 and batch: 150, loss is 4.871421689987183 and perplexity is 130.50632460404285
At time: 328.7173070907593 and batch: 200, loss is 4.868176565170288 and perplexity is 130.08350171888392
At time: 329.1542184352875 and batch: 250, loss is 4.848290309906006 and perplexity is 127.52217995570317
At time: 329.5833582878113 and batch: 300, loss is 4.753042736053467 and perplexity is 115.93651259370017
At time: 330.0090856552124 and batch: 350, loss is 4.824460401535034 and perplexity is 124.51925985054146
At time: 330.43019461631775 and batch: 400, loss is 4.749863214492798 and perplexity is 115.56847535346162
At time: 330.85141801834106 and batch: 450, loss is 4.7930411434173585 and perplexity is 120.66777924979927
At time: 331.2694761753082 and batch: 500, loss is 4.760381116867065 and perplexity is 116.79042822026513
At time: 331.6848871707916 and batch: 550, loss is 4.798692407608033 and perplexity is 121.35163525510634
At time: 332.09601950645447 and batch: 600, loss is 4.846603183746338 and perplexity is 127.30721533716718
At time: 332.5238456726074 and batch: 650, loss is 4.817111015319824 and perplexity is 123.60747435460044
At time: 332.95603227615356 and batch: 700, loss is 4.798057041168213 and perplexity is 121.27455698771315
At time: 333.3698847293854 and batch: 750, loss is 4.748213367462158 and perplexity is 115.37796224951819
At time: 333.7929584980011 and batch: 800, loss is 4.750837564468384 and perplexity is 115.6811343703431
At time: 334.20568227767944 and batch: 850, loss is 4.734841985702515 and perplexity is 113.84546808519754
At time: 334.6192116737366 and batch: 900, loss is 4.853731088638305 and perplexity is 128.21789080850235
At time: 335.02732157707214 and batch: 950, loss is 4.8036441898345945 and perplexity is 121.95403236447157
At time: 335.44557642936707 and batch: 1000, loss is 4.776263475418091 and perplexity is 118.66014412305138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.117312919802782 and perplexity of 166.8863296365135
Finished 36 epochs...
Completing Train Step...
At time: 336.7668228149414 and batch: 50, loss is 4.876028900146484 and perplexity is 131.10898188697067
At time: 337.1935896873474 and batch: 100, loss is 4.823925180435181 and perplexity is 124.45263234714469
At time: 337.60139870643616 and batch: 150, loss is 4.865353012084961 and perplexity is 129.71672210079427
At time: 338.0183627605438 and batch: 200, loss is 4.862248249053955 and perplexity is 129.3146069766158
At time: 338.44317865371704 and batch: 250, loss is 4.843251276016235 and perplexity is 126.88120766604335
At time: 338.87744426727295 and batch: 300, loss is 4.749421043395996 and perplexity is 115.51738560999519
At time: 339.31225419044495 and batch: 350, loss is 4.820784301757812 and perplexity is 124.06235495567608
At time: 339.74574398994446 and batch: 400, loss is 4.74608452796936 and perplexity is 115.1326023454731
At time: 340.1720631122589 and batch: 450, loss is 4.789879474639893 and perplexity is 120.28687017117147
At time: 340.5961730480194 and batch: 500, loss is 4.756902370452881 and perplexity is 116.38484979813849
At time: 341.0208592414856 and batch: 550, loss is 4.794799165725708 and perplexity is 120.8801024774202
At time: 341.4415898323059 and batch: 600, loss is 4.844033212661743 and perplexity is 126.98045953122188
At time: 341.85281252861023 and batch: 650, loss is 4.815011396408081 and perplexity is 123.34821802879712
At time: 342.2625524997711 and batch: 700, loss is 4.796689834594726 and perplexity is 121.10886291102867
At time: 342.69742822647095 and batch: 750, loss is 4.74687385559082 and perplexity is 115.22351556407204
At time: 343.1202929019928 and batch: 800, loss is 4.749413671493531 and perplexity is 115.5165340302343
At time: 343.5562148094177 and batch: 850, loss is 4.734091806411743 and perplexity is 113.76009559903113
At time: 343.9779667854309 and batch: 900, loss is 4.853504180908203 and perplexity is 128.18880047847568
At time: 344.39235258102417 and batch: 950, loss is 4.803340196609497 and perplexity is 121.91696479928926
At time: 344.8112642765045 and batch: 1000, loss is 4.7756216335296635 and perplexity is 118.58400750851439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.11736576731612 and perplexity of 166.89514939709417
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 346.10668992996216 and batch: 50, loss is 4.871909656524658 and perplexity is 130.57002286342413
At time: 346.5287802219391 and batch: 100, loss is 4.813461275100708 and perplexity is 123.15716144650087
At time: 346.9537115097046 and batch: 150, loss is 4.852583885192871 and perplexity is 128.07088314237382
At time: 347.39286708831787 and batch: 200, loss is 4.84923770904541 and perplexity is 127.64305160705653
At time: 347.8285655975342 and batch: 250, loss is 4.827890310287476 and perplexity is 124.94708282721771
At time: 348.2551248073578 and batch: 300, loss is 4.732760801315307 and perplexity is 113.60878105457148
At time: 348.6642782688141 and batch: 350, loss is 4.804964036941528 and perplexity is 122.11509930975608
At time: 349.0771589279175 and batch: 400, loss is 4.725768938064575 and perplexity is 112.8172144806924
At time: 349.5078823566437 and batch: 450, loss is 4.771336040496826 and perplexity is 118.07689213334216
At time: 349.93075680732727 and batch: 500, loss is 4.732308464050293 and perplexity is 113.55740319020501
At time: 350.35630321502686 and batch: 550, loss is 4.769935159683228 and perplexity is 117.91159628754501
At time: 350.7626807689667 and batch: 600, loss is 4.820187406539917 and perplexity is 123.98832482559553
At time: 351.18677401542664 and batch: 650, loss is 4.790693340301513 and perplexity is 120.38480737280435
At time: 351.6125020980835 and batch: 700, loss is 4.770746088027954 and perplexity is 118.00725292324307
At time: 352.045316696167 and batch: 750, loss is 4.719526996612549 and perplexity is 112.11520925017994
At time: 352.46249294281006 and batch: 800, loss is 4.721547813415527 and perplexity is 112.34200262565442
At time: 352.8779249191284 and batch: 850, loss is 4.702575578689575 and perplexity is 110.23071503284199
At time: 353.2983293533325 and batch: 900, loss is 4.820786952972412 and perplexity is 124.06268387203886
At time: 353.7249348163605 and batch: 950, loss is 4.772455062866211 and perplexity is 118.20909677312574
At time: 354.14177227020264 and batch: 1000, loss is 4.744437990188598 and perplexity is 114.94318814752357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.098175420993712 and perplexity of 163.72290922702365
Finished 38 epochs...
Completing Train Step...
At time: 355.3897557258606 and batch: 50, loss is 4.858753795623779 and perplexity is 128.8635117293961
At time: 355.8290741443634 and batch: 100, loss is 4.803117370605468 and perplexity is 121.88980155565064
At time: 356.25332498550415 and batch: 150, loss is 4.844847946166992 and perplexity is 127.08395692178479
At time: 356.6822335720062 and batch: 200, loss is 4.842587537765503 and perplexity is 126.7970196976945
At time: 357.11275243759155 and batch: 250, loss is 4.822509260177612 and perplexity is 124.27654203823367
At time: 357.5394902229309 and batch: 300, loss is 4.727515735626221 and perplexity is 113.01445553580251
At time: 357.9678592681885 and batch: 350, loss is 4.799680442810058 and perplexity is 121.47159419463051
At time: 358.393905878067 and batch: 400, loss is 4.72209056854248 and perplexity is 112.402993373575
At time: 358.8259472846985 and batch: 450, loss is 4.767624177932739 and perplexity is 117.63941935944356
At time: 359.25895404815674 and batch: 500, loss is 4.728216829299927 and perplexity is 113.09371703723802
At time: 359.67786145210266 and batch: 550, loss is 4.766913719177246 and perplexity is 117.55587108628389
At time: 360.1028962135315 and batch: 600, loss is 4.818190755844117 and perplexity is 123.74101043295069
At time: 360.55965089797974 and batch: 650, loss is 4.789533586502075 and perplexity is 120.2452715642911
At time: 361.01379561424255 and batch: 700, loss is 4.770149459838867 and perplexity is 117.93686746869346
At time: 361.435786485672 and batch: 750, loss is 4.720709800720215 and perplexity is 112.24789803716443
At time: 361.85013031959534 and batch: 800, loss is 4.722481679916382 and perplexity is 112.44696406090146
At time: 362.2725901603699 and batch: 850, loss is 4.703479356765747 and perplexity is 110.33038416901529
At time: 362.68281984329224 and batch: 900, loss is 4.822399425506592 and perplexity is 124.26289291471056
At time: 363.114444732666 and batch: 950, loss is 4.774437532424927 and perplexity is 118.44367515446383
At time: 363.53347420692444 and batch: 1000, loss is 4.74479474067688 and perplexity is 114.98420150135183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.097192903844322 and perplexity of 163.56212765921794
Finished 39 epochs...
Completing Train Step...
At time: 364.84361362457275 and batch: 50, loss is 4.855302095413208 and perplexity is 128.4194802913866
At time: 365.2825071811676 and batch: 100, loss is 4.799834089279175 and perplexity is 121.4902593100543
At time: 365.70120310783386 and batch: 150, loss is 4.8417636489868165 and perplexity is 126.69259607862547
At time: 366.13515520095825 and batch: 200, loss is 4.839942169189453 and perplexity is 126.46203811647305
At time: 366.57756996154785 and batch: 250, loss is 4.819955825805664 and perplexity is 123.95961484276137
At time: 367.00372076034546 and batch: 300, loss is 4.725187873840332 and perplexity is 112.75167947534632
At time: 367.4274015426636 and batch: 350, loss is 4.797378921508789 and perplexity is 121.19234620395662
At time: 367.8486568927765 and batch: 400, loss is 4.7197589015960695 and perplexity is 112.14121234094013
At time: 368.26384449005127 and batch: 450, loss is 4.7645477771759035 and perplexity is 117.27806947421449
At time: 368.6850826740265 and batch: 500, loss is 4.725416393280029 and perplexity is 112.77744837019954
At time: 369.10344409942627 and batch: 550, loss is 4.764936656951904 and perplexity is 117.32368541258448
At time: 369.5219111442566 and batch: 600, loss is 4.8169816207885745 and perplexity is 123.59148125812929
At time: 369.9285261631012 and batch: 650, loss is 4.788342924118042 and perplexity is 120.10218524323591
At time: 370.36063861846924 and batch: 700, loss is 4.7693690586090085 and perplexity is 117.84486529624876
At time: 370.78394627571106 and batch: 750, loss is 4.721122512817383 and perplexity is 112.29423366354509
At time: 371.2243824005127 and batch: 800, loss is 4.722593336105347 and perplexity is 112.4595201613393
At time: 371.64274311065674 and batch: 850, loss is 4.703996305465698 and perplexity is 110.38743406234015
At time: 372.0592694282532 and batch: 900, loss is 4.822620878219604 and perplexity is 124.29041431670542
At time: 372.48840618133545 and batch: 950, loss is 4.774402809143067 and perplexity is 118.43956247275017
At time: 372.91280150413513 and batch: 1000, loss is 4.744158983230591 and perplexity is 114.9111226717083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.096076034918064 and perplexity of 163.37955217674295
Finished 40 epochs...
Completing Train Step...
At time: 374.28310990333557 and batch: 50, loss is 4.852160587310791 and perplexity is 128.01668248110536
At time: 374.7002704143524 and batch: 100, loss is 4.797293977737427 and perplexity is 121.18205210622571
At time: 375.1274645328522 and batch: 150, loss is 4.839408712387085 and perplexity is 126.39459407283981
At time: 375.54367876052856 and batch: 200, loss is 4.837739248275756 and perplexity is 126.18375887403657
At time: 375.96530318260193 and batch: 250, loss is 4.817768630981445 and perplexity is 123.68878729903192
At time: 376.3711769580841 and batch: 300, loss is 4.723224983215332 and perplexity is 112.53057733140508
At time: 376.80261182785034 and batch: 350, loss is 4.795201911926269 and perplexity is 120.9287962843811
At time: 377.2180349826813 and batch: 400, loss is 4.717878036499023 and perplexity is 111.93048808278118
At time: 377.64780354499817 and batch: 450, loss is 4.762296171188354 and perplexity is 117.0143025318271
At time: 378.0664875507355 and batch: 500, loss is 4.723423595428467 and perplexity is 112.55292949804736
At time: 378.4937813282013 and batch: 550, loss is 4.763114538192749 and perplexity is 117.11010237042825
At time: 378.9207487106323 and batch: 600, loss is 4.815787992477417 and perplexity is 123.44404697545335
At time: 379.34548783302307 and batch: 650, loss is 4.78751205444336 and perplexity is 120.00243742411162
At time: 379.7627384662628 and batch: 700, loss is 4.7687725257873534 and perplexity is 117.77458792969793
At time: 380.17287945747375 and batch: 750, loss is 4.7213213253021244 and perplexity is 112.31656137860263
At time: 380.60460925102234 and batch: 800, loss is 4.722607097625732 and perplexity is 112.46106778596734
At time: 381.0314271450043 and batch: 850, loss is 4.704062623977661 and perplexity is 110.39475503546191
At time: 381.4647674560547 and batch: 900, loss is 4.822595243453979 and perplexity is 124.28722820190282
At time: 381.8867287635803 and batch: 950, loss is 4.774347467422485 and perplexity is 118.43300800494747
At time: 382.32969903945923 and batch: 1000, loss is 4.743398084640503 and perplexity is 114.82372021690118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0955114597227515 and perplexity of 163.28733816748323
Finished 41 epochs...
Completing Train Step...
At time: 383.637992143631 and batch: 50, loss is 4.850145292282105 and perplexity is 127.75895088714188
At time: 384.0951681137085 and batch: 100, loss is 4.79538857460022 and perplexity is 120.95137128374249
At time: 384.51834201812744 and batch: 150, loss is 4.837544479370117 and perplexity is 126.15918459464167
At time: 384.9528880119324 and batch: 200, loss is 4.836294116973877 and perplexity is 126.00153847221469
At time: 385.3701581954956 and batch: 250, loss is 4.816340017318725 and perplexity is 123.51220996799707
At time: 385.8062262535095 and batch: 300, loss is 4.721937732696533 and perplexity is 112.38581547972818
At time: 386.22144985198975 and batch: 350, loss is 4.793863000869751 and perplexity is 120.76699172711191
At time: 386.6353290081024 and batch: 400, loss is 4.716443080902099 and perplexity is 111.76998798523745
At time: 387.0613670349121 and batch: 450, loss is 4.760824966430664 and perplexity is 116.84227710656478
At time: 387.4902877807617 and batch: 500, loss is 4.7217793273925786 and perplexity is 112.36801438039842
At time: 387.92303228378296 and batch: 550, loss is 4.761778650283813 and perplexity is 116.95376085128045
At time: 388.33686542510986 and batch: 600, loss is 4.814981470108032 and perplexity is 123.34452672824767
At time: 388.7553515434265 and batch: 650, loss is 4.786828851699829 and perplexity is 119.9204794297843
At time: 389.16385197639465 and batch: 700, loss is 4.768263244628907 and perplexity is 117.71462282196735
At time: 389.5829417705536 and batch: 750, loss is 4.721509284973145 and perplexity is 112.33767434666059
At time: 390.0063738822937 and batch: 800, loss is 4.722507333755493 and perplexity is 112.44984879422806
At time: 390.4335107803345 and batch: 850, loss is 4.703837804794311 and perplexity is 110.36993896645744
At time: 390.850465297699 and batch: 900, loss is 4.822354774475098 and perplexity is 124.25734457223608
At time: 391.2622218132019 and batch: 950, loss is 4.773980827331543 and perplexity is 118.38959367532514
At time: 391.6817526817322 and batch: 1000, loss is 4.74252254486084 and perplexity is 114.72323147956523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.094879522556212 and perplexity of 163.18418342676006
Finished 42 epochs...
Completing Train Step...
At time: 393.01825308799744 and batch: 50, loss is 4.848451013565064 and perplexity is 127.54267488339386
At time: 393.46667647361755 and batch: 100, loss is 4.793811683654785 and perplexity is 120.76079446045121
At time: 393.88183784484863 and batch: 150, loss is 4.835915117263794 and perplexity is 125.9537929740002
At time: 394.3142898082733 and batch: 200, loss is 4.835033826828003 and perplexity is 125.84283999897957
At time: 394.73977160453796 and batch: 250, loss is 4.815010595321655 and perplexity is 123.34811921625361
At time: 395.1784293651581 and batch: 300, loss is 4.720813980102539 and perplexity is 112.25959256300283
At time: 395.61318039894104 and batch: 350, loss is 4.792719097137451 and perplexity is 120.62892489717284
At time: 396.036390542984 and batch: 400, loss is 4.715105953216553 and perplexity is 111.62063711273464
At time: 396.4500501155853 and batch: 450, loss is 4.75949577331543 and perplexity is 116.68707432635034
At time: 396.8851549625397 and batch: 500, loss is 4.72031138420105 and perplexity is 112.2031855280375
At time: 397.31302881240845 and batch: 550, loss is 4.760710620880127 and perplexity is 116.82891747588481
At time: 397.7413353919983 and batch: 600, loss is 4.814079856872558 and perplexity is 123.23336778914175
At time: 398.1609683036804 and batch: 650, loss is 4.786359481811523 and perplexity is 119.86420557540829
At time: 398.58612751960754 and batch: 700, loss is 4.768837375640869 and perplexity is 117.78222584212911
At time: 399.03064608573914 and batch: 750, loss is 4.721499652862549 and perplexity is 112.33659230296836
At time: 399.4522759914398 and batch: 800, loss is 4.722563190460205 and perplexity is 112.45613004765052
At time: 399.8768787384033 and batch: 850, loss is 4.704028453826904 and perplexity is 110.39098289448725
At time: 400.28370571136475 and batch: 900, loss is 4.822220649719238 and perplexity is 124.2406797038422
At time: 400.70386362075806 and batch: 950, loss is 4.773871765136719 and perplexity is 118.37668255046523
At time: 401.11486625671387 and batch: 1000, loss is 4.7418838596344 and perplexity is 114.64998284039153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.094612307664825 and perplexity of 163.1405840083759
Finished 43 epochs...
Completing Train Step...
At time: 402.4065020084381 and batch: 50, loss is 4.847046422958374 and perplexity is 127.36365539427888
At time: 402.8141179084778 and batch: 100, loss is 4.792392482757569 and perplexity is 120.58953218913432
At time: 403.22552514076233 and batch: 150, loss is 4.8344675731658935 and perplexity is 125.77160120149843
At time: 403.63217067718506 and batch: 200, loss is 4.834028329849243 and perplexity is 125.71636899732253
At time: 404.04920959472656 and batch: 250, loss is 4.813924293518067 and perplexity is 123.214198684107
At time: 404.46501660346985 and batch: 300, loss is 4.719799613952636 and perplexity is 112.14577796690087
At time: 404.8769574165344 and batch: 350, loss is 4.791758890151978 and perplexity is 120.51315175282261
At time: 405.3056218624115 and batch: 400, loss is 4.713909120559692 and perplexity is 111.48712580034474
At time: 405.72660541534424 and batch: 450, loss is 4.758478708267212 and perplexity is 116.56845631281716
At time: 406.15031242370605 and batch: 500, loss is 4.719030666351318 and perplexity is 112.05957688623977
At time: 406.56826305389404 and batch: 550, loss is 4.759673709869385 and perplexity is 116.70783906959574
At time: 406.98622012138367 and batch: 600, loss is 4.8132653903961184 and perplexity is 123.13303920497906
At time: 407.4168601036072 and batch: 650, loss is 4.785518951416016 and perplexity is 119.76349839694016
At time: 407.8404197692871 and batch: 700, loss is 4.768039932250977 and perplexity is 117.6883386245304
At time: 408.26256585121155 and batch: 750, loss is 4.721334238052368 and perplexity is 112.31801170367171
At time: 408.68392038345337 and batch: 800, loss is 4.722254753112793 and perplexity is 112.42144972582658
At time: 409.10524582862854 and batch: 850, loss is 4.703397064208985 and perplexity is 110.32130517318542
At time: 409.51815962791443 and batch: 900, loss is 4.821920471191406 and perplexity is 124.20339091643854
At time: 409.9549837112427 and batch: 950, loss is 4.773408288955689 and perplexity is 118.32183049000733
At time: 410.3781521320343 and batch: 1000, loss is 4.740991621017456 and perplexity is 114.54773332053784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.094264705006669 and perplexity of 163.08388576252287
Finished 44 epochs...
Completing Train Step...
At time: 411.71778297424316 and batch: 50, loss is 4.845766620635986 and perplexity is 127.20075935191807
At time: 412.14650440216064 and batch: 100, loss is 4.79115665435791 and perplexity is 120.44059626912875
At time: 412.57091450691223 and batch: 150, loss is 4.833092250823975 and perplexity is 125.59874360309249
At time: 412.99172616004944 and batch: 200, loss is 4.8329340362548825 and perplexity is 125.57887362389634
At time: 413.40935587882996 and batch: 250, loss is 4.812768650054932 and perplexity is 123.07188924615615
At time: 413.8182008266449 and batch: 300, loss is 4.718751783370972 and perplexity is 112.02832973481327
At time: 414.23174357414246 and batch: 350, loss is 4.790774507522583 and perplexity is 120.39457906964319
At time: 414.65126729011536 and batch: 400, loss is 4.7128019046783445 and perplexity is 111.36375379641758
At time: 415.0895631313324 and batch: 450, loss is 4.757405261993409 and perplexity is 116.44339347387339
At time: 415.52369260787964 and batch: 500, loss is 4.717803831100464 and perplexity is 111.92218254446378
At time: 415.93272852897644 and batch: 550, loss is 4.758629684448242 and perplexity is 116.58605670176615
At time: 416.3627851009369 and batch: 600, loss is 4.812303028106689 and perplexity is 123.01459761246761
At time: 416.7823705673218 and batch: 650, loss is 4.784836854934692 and perplexity is 119.68183599007895
At time: 417.2044942378998 and batch: 700, loss is 4.767345285415649 and perplexity is 117.60661518030204
At time: 417.6189274787903 and batch: 750, loss is 4.720947217941284 and perplexity is 112.27455078497277
At time: 418.0344195365906 and batch: 800, loss is 4.721924839019775 and perplexity is 112.38436642269303
At time: 418.4541697502136 and batch: 850, loss is 4.703029594421387 and perplexity is 110.28077287425965
At time: 418.8816113471985 and batch: 900, loss is 4.821534461975098 and perplexity is 124.1554565150036
At time: 419.30166888237 and batch: 950, loss is 4.7730317878723145 and perplexity is 118.27729057782977
At time: 419.7305209636688 and batch: 1000, loss is 4.740037326812744 and perplexity is 114.43847322389736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.093927522984947 and perplexity of 163.02890607781265
Finished 45 epochs...
Completing Train Step...
At time: 421.041672706604 and batch: 50, loss is 4.8445350933074955 and perplexity is 127.04420456110972
At time: 421.4774796962738 and batch: 100, loss is 4.7899548530578615 and perplexity is 120.29593754688531
At time: 421.90821051597595 and batch: 150, loss is 4.831785955429077 and perplexity is 125.43478165740164
At time: 422.33060336112976 and batch: 200, loss is 4.831952953338623 and perplexity is 125.4557307529017
At time: 422.7519679069519 and batch: 250, loss is 4.811815729141236 and perplexity is 122.9546673294837
At time: 423.1680018901825 and batch: 300, loss is 4.7178678226470945 and perplexity is 111.92934484718802
At time: 423.59058237075806 and batch: 350, loss is 4.789770450592041 and perplexity is 120.27375672453549
At time: 424.0187427997589 and batch: 400, loss is 4.711613521575928 and perplexity is 111.23148959901877
At time: 424.45305609703064 and batch: 450, loss is 4.7565093517303465 and perplexity is 116.33911736058921
At time: 424.8727352619171 and batch: 500, loss is 4.71695384979248 and perplexity is 111.82709120000993
At time: 425.2870273590088 and batch: 550, loss is 4.7575927734375 and perplexity is 116.46522998997288
At time: 425.71583318710327 and batch: 600, loss is 4.811471080780029 and perplexity is 122.91229850647798
At time: 426.1287648677826 and batch: 650, loss is 4.78400990486145 and perplexity is 119.58290599776112
At time: 426.5669083595276 and batch: 700, loss is 4.766863737106323 and perplexity is 117.54999554723487
At time: 426.9804518222809 and batch: 750, loss is 4.720808877944946 and perplexity is 112.25901979833137
At time: 427.4078369140625 and batch: 800, loss is 4.721633081436157 and perplexity is 112.3515822142624
At time: 427.8262345790863 and batch: 850, loss is 4.702705345153809 and perplexity is 110.24502021112788
At time: 428.24232268333435 and batch: 900, loss is 4.821201648712158 and perplexity is 124.11414280766526
At time: 428.6622591018677 and batch: 950, loss is 4.772674131393432 and perplexity is 118.23499550255471
At time: 429.09273290634155 and batch: 1000, loss is 4.739147367477417 and perplexity is 114.33667294210585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0935170243426064 and perplexity of 162.96199666725616
Finished 46 epochs...
Completing Train Step...
At time: 430.40622901916504 and batch: 50, loss is 4.843423929214477 and perplexity is 126.90311600356158
At time: 430.820680141449 and batch: 100, loss is 4.7888611221313475 and perplexity is 120.1644380852791
At time: 431.23394203186035 and batch: 150, loss is 4.83060474395752 and perplexity is 125.28670412703161
At time: 431.6577911376953 and batch: 200, loss is 4.831018676757813 and perplexity is 125.3385751381262
At time: 432.0794641971588 and batch: 250, loss is 4.8108864021301265 and perplexity is 122.84045531436773
At time: 432.49203276634216 and batch: 300, loss is 4.716914377212524 and perplexity is 111.82267718332821
At time: 432.92411613464355 and batch: 350, loss is 4.788820657730103 and perplexity is 120.15957580161643
At time: 433.3535943031311 and batch: 400, loss is 4.710681505203247 and perplexity is 111.12786832541752
At time: 433.80210876464844 and batch: 450, loss is 4.755617341995239 and perplexity is 116.2353880060048
At time: 434.2187807559967 and batch: 500, loss is 4.715788259506225 and perplexity is 111.69682256344434
At time: 434.6502797603607 and batch: 550, loss is 4.755914306640625 and perplexity is 116.26991093257585
At time: 435.05768299102783 and batch: 600, loss is 4.8100027656555175 and perplexity is 122.73195695110972
At time: 435.47325801849365 and batch: 650, loss is 4.782825393676758 and perplexity is 119.44134256639458
At time: 435.88848876953125 and batch: 700, loss is 4.765993385314942 and perplexity is 117.44773020789967
At time: 436.3325307369232 and batch: 750, loss is 4.720400362014771 and perplexity is 112.21316956635063
At time: 436.7597026824951 and batch: 800, loss is 4.72099702835083 and perplexity is 112.28014336561212
At time: 437.1926064491272 and batch: 850, loss is 4.7020954990386965 and perplexity is 110.17780821041352
At time: 437.61936712265015 and batch: 900, loss is 4.8206508827209475 and perplexity is 124.04580377991225
At time: 438.05956530570984 and batch: 950, loss is 4.77201431274414 and perplexity is 118.15700757929565
At time: 438.47311544418335 and batch: 1000, loss is 4.738028516769409 and perplexity is 114.20881881281244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.093056283346036 and perplexity of 162.8869306888234
Finished 47 epochs...
Completing Train Step...
At time: 439.7522485256195 and batch: 50, loss is 4.842261667251587 and perplexity is 126.75570701938001
At time: 440.1971800327301 and batch: 100, loss is 4.787549123764038 and perplexity is 120.00688591539749
At time: 440.6195192337036 and batch: 150, loss is 4.829106330871582 and perplexity is 125.0991134693207
At time: 441.0354814529419 and batch: 200, loss is 4.829945373535156 and perplexity is 125.20412100943945
At time: 441.44408559799194 and batch: 250, loss is 4.8097985744476315 and perplexity is 122.70689872299566
At time: 441.85366773605347 and batch: 300, loss is 4.71595103263855 and perplexity is 111.71500528491285
At time: 442.2635381221771 and batch: 350, loss is 4.787864427566529 and perplexity is 120.04473050881022
At time: 442.681813955307 and batch: 400, loss is 4.710028610229492 and perplexity is 111.05533717893073
At time: 443.106876373291 and batch: 450, loss is 4.756013679504394 and perplexity is 116.2814655806952
At time: 443.5230944156647 and batch: 500, loss is 4.716703414916992 and perplexity is 111.79908930282124
At time: 443.9619390964508 and batch: 550, loss is 4.755765600204468 and perplexity is 116.25262213399846
At time: 444.38639855384827 and batch: 600, loss is 4.809692640304565 and perplexity is 122.6939005613178
At time: 444.81049633026123 and batch: 650, loss is 4.782334671020508 and perplexity is 119.382744372411
At time: 445.2345402240753 and batch: 700, loss is 4.765544204711914 and perplexity is 117.39498681216264
At time: 445.6576569080353 and batch: 750, loss is 4.720045022964477 and perplexity is 112.17330292875255
At time: 446.07623052597046 and batch: 800, loss is 4.7206829643249515 and perplexity is 112.24488574862488
At time: 446.49271035194397 and batch: 850, loss is 4.7019314861297605 and perplexity is 110.15973910941216
At time: 446.9008412361145 and batch: 900, loss is 4.82044132232666 and perplexity is 124.0198114159426
At time: 447.3376624584198 and batch: 950, loss is 4.7717491626739506 and perplexity is 118.12568239356371
At time: 447.7754580974579 and batch: 1000, loss is 4.73718445777893 and perplexity is 114.11246050427056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0926930497332314 and perplexity of 162.82777542475117
Finished 48 epochs...
Completing Train Step...
At time: 449.0756485462189 and batch: 50, loss is 4.84131350517273 and perplexity is 126.63557902411007
At time: 449.5201394557953 and batch: 100, loss is 4.7866561985015865 and perplexity is 119.89977656273554
At time: 449.94336438179016 and batch: 150, loss is 4.828066358566284 and perplexity is 124.96908148244742
At time: 450.37252974510193 and batch: 200, loss is 4.829054183959961 and perplexity is 125.09259010699485
At time: 450.78498935699463 and batch: 250, loss is 4.808675079345703 and perplexity is 122.56911553716776
At time: 451.2001049518585 and batch: 300, loss is 4.715170831680298 and perplexity is 111.6278791231154
At time: 451.6112916469574 and batch: 350, loss is 4.787164936065674 and perplexity is 119.9607896014951
At time: 452.029926776886 and batch: 400, loss is 4.708992424011231 and perplexity is 110.94032276754628
At time: 452.4506423473358 and batch: 450, loss is 4.754678316116333 and perplexity is 116.12629119900154
At time: 452.8824200630188 and batch: 500, loss is 4.715031585693359 and perplexity is 111.61233647106816
At time: 453.30678486824036 and batch: 550, loss is 4.7550718879699705 and perplexity is 116.17200423377659
At time: 453.7262330055237 and batch: 600, loss is 4.808978509902954 and perplexity is 122.60631239524325
At time: 454.1502151489258 and batch: 650, loss is 4.7817750167846675 and perplexity is 119.31595000645503
At time: 454.5742275714874 and batch: 700, loss is 4.765091009140015 and perplexity is 117.34179597781187
At time: 454.9917106628418 and batch: 750, loss is 4.719674310684204 and perplexity is 112.13172661473929
At time: 455.41602873802185 and batch: 800, loss is 4.7203530597686765 and perplexity is 112.20786175692542
At time: 455.8496992588043 and batch: 850, loss is 4.7015626335144045 and perplexity is 110.11911389435372
At time: 456.2603647708893 and batch: 900, loss is 4.820091714859009 and perplexity is 123.9764607420361
At time: 456.682781457901 and batch: 950, loss is 4.77126630783081 and perplexity is 118.06865860393516
At time: 457.1136565208435 and batch: 1000, loss is 4.736427087783813 and perplexity is 114.02606787033962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092313440834603 and perplexity of 162.76597628275036
Finished 49 epochs...
Completing Train Step...
At time: 458.46070075035095 and batch: 50, loss is 4.840305452346802 and perplexity is 126.5079879908671
At time: 458.8761537075043 and batch: 100, loss is 4.785730047225952 and perplexity is 119.78878263823987
At time: 459.29840445518494 and batch: 150, loss is 4.8269678974151615 and perplexity is 124.83188316865184
At time: 459.7242183685303 and batch: 200, loss is 4.828313837051391 and perplexity is 125.00001246863691
At time: 460.1511723995209 and batch: 250, loss is 4.807747535705566 and perplexity is 122.4554800426612
At time: 460.56176805496216 and batch: 300, loss is 4.713777303695679 and perplexity is 111.47243088558395
At time: 460.9891097545624 and batch: 350, loss is 4.786381206512451 and perplexity is 119.8668096177123
At time: 461.40952253341675 and batch: 400, loss is 4.707946872711181 and perplexity is 110.82438958645416
At time: 461.8192069530487 and batch: 450, loss is 4.753626766204834 and perplexity is 116.00424278901222
At time: 462.2328062057495 and batch: 500, loss is 4.71360523223877 and perplexity is 111.45325131217218
At time: 462.6456296443939 and batch: 550, loss is 4.754314270019531 and perplexity is 116.08402357010391
At time: 463.0750801563263 and batch: 600, loss is 4.8084492683410645 and perplexity is 122.54144120675238
At time: 463.4958665370941 and batch: 650, loss is 4.78128680229187 and perplexity is 119.25771244780681
At time: 463.9116778373718 and batch: 700, loss is 4.76450288772583 and perplexity is 117.27280504432967
At time: 464.3302104473114 and batch: 750, loss is 4.719300937652588 and perplexity is 112.08986746705753
At time: 464.7457354068756 and batch: 800, loss is 4.719898252487183 and perplexity is 112.1568404076776
At time: 465.1696038246155 and batch: 850, loss is 4.70109694480896 and perplexity is 110.06784460545046
At time: 465.59665751457214 and batch: 900, loss is 4.819575843811035 and perplexity is 123.91252136896263
At time: 466.0161130428314 and batch: 950, loss is 4.770755052566528 and perplexity is 118.00831080855569
At time: 466.42618012428284 and batch: 1000, loss is 4.735450410842896 and perplexity is 113.91475560609645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092087164157775 and perplexity of 162.72915030512337
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1cac512908>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 12.094282084128249, 'num_layers': 1, 'dropout': 0.046855977202634125, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 5.14395712120558}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6731104850769043 and batch: 50, loss is 6.766019868850708 and perplexity is 867.8508506529427
At time: 1.097898006439209 and batch: 100, loss is 5.860652866363526 and perplexity is 350.9531947780524
At time: 1.511587142944336 and batch: 150, loss is 5.574940824508667 and perplexity is 263.7339485891773
At time: 1.9209423065185547 and batch: 200, loss is 5.4304036617279055 and perplexity is 228.2413591327708
At time: 2.3395578861236572 and batch: 250, loss is 5.369978914260864 and perplexity is 214.85833720972158
At time: 2.7551515102386475 and batch: 300, loss is 5.200605192184448 and perplexity is 181.38197962205547
At time: 3.1962645053863525 and batch: 350, loss is 5.244867944717408 and perplexity is 189.59077711269975
At time: 3.6388485431671143 and batch: 400, loss is 5.17031681060791 and perplexity is 175.9705779997962
At time: 4.068748712539673 and batch: 450, loss is 5.169580774307251 and perplexity is 175.84110492082857
At time: 4.495800495147705 and batch: 500, loss is 5.159985036849975 and perplexity is 174.16184957572986
At time: 4.913225412368774 and batch: 550, loss is 5.186168165206909 and perplexity is 178.78217496975867
At time: 5.341963291168213 and batch: 600, loss is 5.206588039398193 and perplexity is 182.47041301214318
At time: 5.761337041854858 and batch: 650, loss is 5.165567760467529 and perplexity is 175.13686613828435
At time: 6.190342426300049 and batch: 700, loss is 5.133847932815552 and perplexity is 169.66873744962612
At time: 6.615421772003174 and batch: 750, loss is 5.059609880447388 and perplexity is 157.5290491721913
At time: 7.041132211685181 and batch: 800, loss is 5.0918678855896 and perplexity is 162.69347120201837
At time: 7.469712734222412 and batch: 850, loss is 5.0732363510131835 and perplexity is 159.6903058322584
At time: 7.897663116455078 and batch: 900, loss is 5.180689668655395 and perplexity is 177.80539552520483
At time: 8.306584358215332 and batch: 950, loss is 5.107727842330933 and perplexity is 165.29435303586138
At time: 8.720932722091675 and batch: 1000, loss is 5.079906978607178 and perplexity is 160.75910119756088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.147360080625953 and perplexity of 171.97688540904122
Finished 1 epochs...
Completing Train Step...
At time: 9.986754894256592 and batch: 50, loss is 5.110145750045777 and perplexity is 165.69450309529583
At time: 10.395431756973267 and batch: 100, loss is 5.041752338409424 and perplexity is 154.74093606967426
At time: 10.818618535995483 and batch: 150, loss is 5.039608345031739 and perplexity is 154.40952792286893
At time: 11.2289400100708 and batch: 200, loss is 5.038902797698975 and perplexity is 154.30062311555548
At time: 11.656444072723389 and batch: 250, loss is 5.016448812484741 and perplexity is 150.87456745211057
At time: 12.077385187149048 and batch: 300, loss is 4.9057944393157955 and perplexity is 135.07017244980366
At time: 12.493845224380493 and batch: 350, loss is 4.962075481414795 and perplexity is 142.89005400298745
At time: 12.896592140197754 and batch: 400, loss is 4.8975562000274655 and perplexity is 133.9620029984863
At time: 13.307835817337036 and batch: 450, loss is 4.92581337928772 and perplexity is 137.80138086050331
At time: 13.713452339172363 and batch: 500, loss is 4.903533143997192 and perplexity is 134.76508397873042
At time: 14.14174485206604 and batch: 550, loss is 4.933258113861084 and perplexity is 138.83110381594747
At time: 14.546659231185913 and batch: 600, loss is 4.945021257400513 and perplexity is 140.4738369283211
At time: 14.950728416442871 and batch: 650, loss is 4.943798980712891 and perplexity is 140.30224392065202
At time: 15.370178461074829 and batch: 700, loss is 4.910220937728882 and perplexity is 135.66938558339123
At time: 15.77174687385559 and batch: 750, loss is 4.843793411254882 and perplexity is 126.95001308907823
At time: 16.18265438079834 and batch: 800, loss is 4.8804693603515625 and perplexity is 131.69246060183778
At time: 16.588321685791016 and batch: 850, loss is 4.868709478378296 and perplexity is 130.15284341000427
At time: 17.00476336479187 and batch: 900, loss is 4.975058374404907 and perplexity is 144.7572750213357
At time: 17.413270235061646 and batch: 950, loss is 4.924608211517334 and perplexity is 137.6354071107713
At time: 17.824142932891846 and batch: 1000, loss is 4.881150588989258 and perplexity is 131.78220384173025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.081794180521151 and perplexity of 161.06277253546673
Finished 2 epochs...
Completing Train Step...
At time: 19.120617389678955 and batch: 50, loss is 4.958509502410888 and perplexity is 142.3814185012983
At time: 19.55612826347351 and batch: 100, loss is 4.8800496578216555 and perplexity is 131.63720054015852
At time: 19.971170663833618 and batch: 150, loss is 4.899271755218506 and perplexity is 134.19201945468123
At time: 20.391342401504517 and batch: 200, loss is 4.893912220001221 and perplexity is 133.47473646921844
At time: 20.81049633026123 and batch: 250, loss is 4.87985161781311 and perplexity is 131.6111336890636
At time: 21.219687700271606 and batch: 300, loss is 4.772943906784057 and perplexity is 118.26689669753713
At time: 21.644597053527832 and batch: 350, loss is 4.842462701797485 and perplexity is 126.7811918569612
At time: 22.067606925964355 and batch: 400, loss is 4.766704778671265 and perplexity is 117.53131146893683
At time: 22.50382399559021 and batch: 450, loss is 4.812918348312378 and perplexity is 123.09031427257965
At time: 22.92769455909729 and batch: 500, loss is 4.790133867263794 and perplexity is 120.31747415624419
At time: 23.34676718711853 and batch: 550, loss is 4.8222982120513915 and perplexity is 124.25031647442873
At time: 23.770516633987427 and batch: 600, loss is 4.842681522369385 and perplexity is 126.80893722538744
At time: 24.19631791114807 and batch: 650, loss is 4.830031862258911 and perplexity is 125.2149502223701
At time: 24.621049642562866 and batch: 700, loss is 4.79062032699585 and perplexity is 120.37601800094083
At time: 25.04646396636963 and batch: 750, loss is 4.735964164733887 and perplexity is 113.97329479110984
At time: 25.475189447402954 and batch: 800, loss is 4.767921876907349 and perplexity is 117.67444570735577
At time: 25.90429449081421 and batch: 850, loss is 4.740584335327148 and perplexity is 114.50108916729204
At time: 26.33146071434021 and batch: 900, loss is 4.872627210617066 and perplexity is 130.66374753991664
At time: 26.742340326309204 and batch: 950, loss is 4.816804008483887 and perplexity is 123.56953183960438
At time: 27.160258769989014 and batch: 1000, loss is 4.783890552520752 and perplexity is 119.56863434971709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.056001988852897 and perplexity of 156.96172547639011
Finished 3 epochs...
Completing Train Step...
At time: 28.464328050613403 and batch: 50, loss is 4.873010721206665 and perplexity is 130.71386808103975
At time: 28.893941402435303 and batch: 100, loss is 4.7653682422637935 and perplexity is 117.37433152021724
At time: 29.303346633911133 and batch: 150, loss is 4.8063935279846195 and perplexity is 122.28978657764101
At time: 29.719836235046387 and batch: 200, loss is 4.80695818901062 and perplexity is 122.35885835323394
At time: 30.152482509613037 and batch: 250, loss is 4.797421207427979 and perplexity is 121.19747104206805
At time: 30.576025009155273 and batch: 300, loss is 4.696106653213501 and perplexity is 109.51994219900713
At time: 31.016697645187378 and batch: 350, loss is 4.763310108184815 and perplexity is 117.1330078319571
At time: 31.438025951385498 and batch: 400, loss is 4.685521240234375 and perplexity is 108.36674269594748
At time: 31.87329626083374 and batch: 450, loss is 4.73451639175415 and perplexity is 113.80840672354523
At time: 32.29066348075867 and batch: 500, loss is 4.702943935394287 and perplexity is 110.27132673512673
At time: 32.70332455635071 and batch: 550, loss is 4.7360778903961185 and perplexity is 113.9862572166029
At time: 33.11385917663574 and batch: 600, loss is 4.764318466186523 and perplexity is 117.25117940728254
At time: 33.52899098396301 and batch: 650, loss is 4.74231336593628 and perplexity is 114.69923630711192
At time: 33.94768404960632 and batch: 700, loss is 4.717428665161133 and perplexity is 111.88020102922724
At time: 34.36511492729187 and batch: 750, loss is 4.655725784301758 and perplexity is 105.18553431125987
At time: 34.79344868659973 and batch: 800, loss is 4.696883287429809 and perplexity is 109.60503217110195
At time: 35.2189416885376 and batch: 850, loss is 4.678698043823243 and perplexity is 107.62985196009927
At time: 35.65567421913147 and batch: 900, loss is 4.802654218673706 and perplexity is 121.83336112985306
At time: 36.057862997055054 and batch: 950, loss is 4.739455947875976 and perplexity is 114.37196044244767
At time: 36.471983432769775 and batch: 1000, loss is 4.710476484298706 and perplexity is 111.10508712472468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.027329979873285 and perplexity of 152.5252231191278
Finished 4 epochs...
Completing Train Step...
At time: 37.77684020996094 and batch: 50, loss is 4.8089923000335695 and perplexity is 122.60800316396336
At time: 38.1971218585968 and batch: 100, loss is 4.69661434173584 and perplexity is 109.57555833327069
At time: 38.61778521537781 and batch: 150, loss is 4.738942441940307 and perplexity is 114.31324483858214
At time: 39.04159617424011 and batch: 200, loss is 4.735040731430054 and perplexity is 113.86809663416842
At time: 39.46290397644043 and batch: 250, loss is 4.729545478820801 and perplexity is 113.24407881712716
At time: 39.87645244598389 and batch: 300, loss is 4.641393051147461 and perplexity is 103.68869067080433
At time: 40.30008602142334 and batch: 350, loss is 4.726302070617676 and perplexity is 112.87737704616828
At time: 40.72489261627197 and batch: 400, loss is 4.635302152633667 and perplexity is 103.0590528554959
At time: 41.14302468299866 and batch: 450, loss is 4.686507720947265 and perplexity is 108.47369714311189
At time: 41.56178903579712 and batch: 500, loss is 4.638897438049316 and perplexity is 103.43024643877641
At time: 41.98441505432129 and batch: 550, loss is 4.67113959312439 and perplexity is 106.81940375515167
At time: 42.397610902786255 and batch: 600, loss is 4.715582189559936 and perplexity is 111.67380757664802
At time: 42.81798028945923 and batch: 650, loss is 4.68715931892395 and perplexity is 108.5444014175733
At time: 43.22894215583801 and batch: 700, loss is 4.652629413604736 and perplexity is 104.86034461879855
At time: 43.65392541885376 and batch: 750, loss is 4.6012405586242675 and perplexity is 99.60780835181811
At time: 44.08577752113342 and batch: 800, loss is 4.644619674682617 and perplexity is 104.02379537808586
At time: 44.51134467124939 and batch: 850, loss is 4.621127862930297 and perplexity is 101.60856806430692
At time: 44.921172857284546 and batch: 900, loss is 4.752855091094971 and perplexity is 115.91475973257756
At time: 45.32796597480774 and batch: 950, loss is 4.691227769851684 and perplexity is 108.98690853709603
At time: 45.748126745224 and batch: 1000, loss is 4.673910455703735 and perplexity is 107.11579608533934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0276920969893295 and perplexity of 152.58046511448015
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 47.03185486793518 and batch: 50, loss is 4.706936540603638 and perplexity is 110.7124766914714
At time: 47.464245319366455 and batch: 100, loss is 4.519479084014892 and perplexity is 91.78777180495858
At time: 47.88753652572632 and batch: 150, loss is 4.523251523971558 and perplexity is 92.13468961472825
At time: 48.306215047836304 and batch: 200, loss is 4.5239303684234615 and perplexity is 92.19725597161049
At time: 48.71906924247742 and batch: 250, loss is 4.48529616355896 and perplexity is 88.70321758374926
At time: 49.17051339149475 and batch: 300, loss is 4.381746912002564 and perplexity is 79.97762531579369
At time: 49.59390926361084 and batch: 350, loss is 4.458226280212402 and perplexity is 86.33424045805644
At time: 50.020843505859375 and batch: 400, loss is 4.357322902679443 and perplexity is 78.0479126367027
At time: 50.441049337387085 and batch: 450, loss is 4.403700675964355 and perplexity is 81.75285036047696
At time: 50.847994804382324 and batch: 500, loss is 4.338508291244507 and perplexity is 76.59319933244863
At time: 51.25450015068054 and batch: 550, loss is 4.378043050765991 and perplexity is 79.68194720296748
At time: 51.6671826839447 and batch: 600, loss is 4.406789646148682 and perplexity is 82.00577291172405
At time: 52.09311509132385 and batch: 650, loss is 4.3578529548645015 and perplexity is 78.08929306926055
At time: 52.52438402175903 and batch: 700, loss is 4.312438678741455 and perplexity is 74.62224691280102
At time: 52.95363807678223 and batch: 750, loss is 4.274653759002685 and perplexity is 71.85525576663733
At time: 53.37951898574829 and batch: 800, loss is 4.275200939178466 and perplexity is 71.89458429703332
At time: 53.80153203010559 and batch: 850, loss is 4.252210454940796 and perplexity is 70.26054859946662
At time: 54.22118926048279 and batch: 900, loss is 4.377653388977051 and perplexity is 79.65090424139488
At time: 54.63925123214722 and batch: 950, loss is 4.28936884880066 and perplexity is 72.92043015563392
At time: 55.06333255767822 and batch: 1000, loss is 4.25795696258545 and perplexity is 70.66546368866874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.732181083865282 and perplexity of 113.54293914841293
Finished 6 epochs...
Completing Train Step...
At time: 56.436548709869385 and batch: 50, loss is 4.492221984863281 and perplexity is 89.31969255138222
At time: 56.86365866661072 and batch: 100, loss is 4.367082672119141 and perplexity is 78.81337154449625
At time: 57.29632759094238 and batch: 150, loss is 4.410125637054444 and perplexity is 82.27980024659234
At time: 57.735469818115234 and batch: 200, loss is 4.421839628219605 and perplexity is 83.24929232464888
At time: 58.144251585006714 and batch: 250, loss is 4.393816690444947 and perplexity is 80.94878659460375
At time: 58.56812381744385 and batch: 300, loss is 4.304690408706665 and perplexity is 74.04628781792141
At time: 58.998003244400024 and batch: 350, loss is 4.381450891494751 and perplexity is 79.95395380233361
At time: 59.413313150405884 and batch: 400, loss is 4.286391544342041 and perplexity is 72.70364670910497
At time: 59.84294676780701 and batch: 450, loss is 4.339124250411987 and perplexity is 76.6403921486759
At time: 60.27550482749939 and batch: 500, loss is 4.277286005020142 and perplexity is 72.04464562846472
At time: 60.70093369483948 and batch: 550, loss is 4.318538331985474 and perplexity is 75.07880775912507
At time: 61.13372778892517 and batch: 600, loss is 4.354492034912109 and perplexity is 77.82728175228254
At time: 61.56071472167969 and batch: 650, loss is 4.3110294008255 and perplexity is 74.5171574956781
At time: 61.979445457458496 and batch: 700, loss is 4.268218731880188 and perplexity is 71.3943498085478
At time: 62.3975248336792 and batch: 750, loss is 4.238206663131714 and perplexity is 69.28349172222427
At time: 62.820085763931274 and batch: 800, loss is 4.242598209381104 and perplexity is 69.58842244907036
At time: 63.249303340911865 and batch: 850, loss is 4.228432545661926 and perplexity is 68.6096054222203
At time: 63.67052888870239 and batch: 900, loss is 4.358386297225952 and perplexity is 78.13095250561365
At time: 64.09131455421448 and batch: 950, loss is 4.278113508224488 and perplexity is 72.1042874770853
At time: 64.49993824958801 and batch: 1000, loss is 4.249361572265625 and perplexity is 70.06066939120748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.71813481028487 and perplexity of 111.95923258821773
Finished 7 epochs...
Completing Train Step...
At time: 65.83694505691528 and batch: 50, loss is 4.442313833236694 and perplexity is 84.97132386171214
At time: 66.24536752700806 and batch: 100, loss is 4.322185344696045 and perplexity is 75.35312103326936
At time: 66.66132378578186 and batch: 150, loss is 4.3683791351318355 and perplexity is 78.91561642964413
At time: 67.08478212356567 and batch: 200, loss is 4.382316102981568 and perplexity is 80.02316081661435
At time: 67.51090431213379 and batch: 250, loss is 4.355847396850586 and perplexity is 77.93283740464264
At time: 67.9262228012085 and batch: 300, loss is 4.268931932449341 and perplexity is 71.44528646133138
At time: 68.35655999183655 and batch: 350, loss is 4.345765190124512 and perplexity is 77.15105012012538
At time: 68.78337812423706 and batch: 400, loss is 4.254021034240723 and perplexity is 70.38787612787222
At time: 69.20617818832397 and batch: 450, loss is 4.304782609939576 and perplexity is 74.0531152916966
At time: 69.64186882972717 and batch: 500, loss is 4.245718383789063 and perplexity is 69.8058895551066
At time: 70.06597781181335 and batch: 550, loss is 4.288433532714844 and perplexity is 72.85225839036892
At time: 70.49343585968018 and batch: 600, loss is 4.32603832244873 and perplexity is 75.64401497628079
At time: 70.92245864868164 and batch: 650, loss is 4.283183917999268 and perplexity is 72.47081419618799
At time: 71.36808919906616 and batch: 700, loss is 4.2421307945251465 and perplexity is 69.55590338715342
At time: 71.79763579368591 and batch: 750, loss is 4.217146821022034 and perplexity is 67.83964923679093
At time: 72.21743845939636 and batch: 800, loss is 4.222359848022461 and perplexity is 68.19422255712841
At time: 72.629962682724 and batch: 850, loss is 4.211848196983337 and perplexity is 67.48114307402123
At time: 73.04528594017029 and batch: 900, loss is 4.34485315322876 and perplexity is 77.08071759367465
At time: 73.45250916481018 and batch: 950, loss is 4.264705991744995 and perplexity is 71.14399997493283
At time: 73.86703658103943 and batch: 1000, loss is 4.236079206466675 and perplexity is 69.13625077588424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.712648345202934 and perplexity of 111.34665414974491
Finished 8 epochs...
Completing Train Step...
At time: 75.23223090171814 and batch: 50, loss is 4.407469844818115 and perplexity is 82.06157210446136
At time: 75.65796518325806 and batch: 100, loss is 4.291474061012268 and perplexity is 73.07410483782597
At time: 76.07738161087036 and batch: 150, loss is 4.338440780639648 and perplexity is 76.58802865377345
At time: 76.49233269691467 and batch: 200, loss is 4.354431533813477 and perplexity is 77.82257325866892
At time: 76.92628335952759 and batch: 250, loss is 4.328717346191406 and perplexity is 75.84693878587733
At time: 77.34247326850891 and batch: 300, loss is 4.2415146636962895 and perplexity is 69.51306105034514
At time: 77.75940537452698 and batch: 350, loss is 4.319738578796387 and perplexity is 75.16897495932784
At time: 78.17113280296326 and batch: 400, loss is 4.228850193023682 and perplexity is 68.63826602751249
At time: 78.5972638130188 and batch: 450, loss is 4.27925030708313 and perplexity is 72.18630215705736
At time: 79.01888227462769 and batch: 500, loss is 4.221876792907715 and perplexity is 68.16128894414153
At time: 79.45555305480957 and batch: 550, loss is 4.265200657844543 and perplexity is 71.17920120561666
At time: 79.88293099403381 and batch: 600, loss is 4.304235811233521 and perplexity is 74.01263421258521
At time: 80.3027617931366 and batch: 650, loss is 4.263607020378113 and perplexity is 71.06585770195981
At time: 80.72052097320557 and batch: 700, loss is 4.223240032196045 and perplexity is 68.25427245616157
At time: 81.12884593009949 and batch: 750, loss is 4.2001156949996945 and perplexity is 66.69404676230113
At time: 81.54286122322083 and batch: 800, loss is 4.206021947860718 and perplexity is 67.08912423203847
At time: 81.95592999458313 and batch: 850, loss is 4.197051272392273 and perplexity is 66.48998084941397
At time: 82.37780237197876 and batch: 900, loss is 4.329168720245361 and perplexity is 75.88118185375197
At time: 82.80236530303955 and batch: 950, loss is 4.249734692573547 and perplexity is 70.08681532723051
At time: 83.2252926826477 and batch: 1000, loss is 4.220156788825989 and perplexity is 68.04415201581763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.706749148485137 and perplexity of 110.69173198967724
Finished 9 epochs...
Completing Train Step...
At time: 84.58544993400574 and batch: 50, loss is 4.379975452423095 and perplexity is 79.83607359887394
At time: 85.03404998779297 and batch: 100, loss is 4.265949144363403 and perplexity is 71.23249782155332
At time: 85.45461249351501 and batch: 150, loss is 4.313476715087891 and perplexity is 74.69974773472515
At time: 85.86947441101074 and batch: 200, loss is 4.3308595943450925 and perplexity is 76.00959591421469
At time: 86.29870343208313 and batch: 250, loss is 4.305067777633667 and perplexity is 74.07423585913325
At time: 86.72299599647522 and batch: 300, loss is 4.21811429977417 and perplexity is 67.90531441569058
At time: 87.13254880905151 and batch: 350, loss is 4.297115182876587 and perplexity is 73.48748964915505
At time: 87.54327178001404 and batch: 400, loss is 4.208912744522094 and perplexity is 67.28334584071771
At time: 87.95961117744446 and batch: 450, loss is 4.257929706573487 and perplexity is 70.66353765619317
At time: 88.3674750328064 and batch: 500, loss is 4.202219176292419 and perplexity is 66.83448409386965
At time: 88.79807114601135 and batch: 550, loss is 4.245770874023438 and perplexity is 69.80955377877724
At time: 89.22370958328247 and batch: 600, loss is 4.284489917755127 and perplexity is 72.56552289314111
At time: 89.65222382545471 and batch: 650, loss is 4.244659514427185 and perplexity is 69.73201335690202
At time: 90.09872460365295 and batch: 700, loss is 4.205178813934326 and perplexity is 67.03258295459065
At time: 90.52886605262756 and batch: 750, loss is 4.186346697807312 and perplexity is 65.78202980782903
At time: 90.93989253044128 and batch: 800, loss is 4.191381816864014 and perplexity is 66.11408542795908
At time: 91.36162376403809 and batch: 850, loss is 4.183521432876587 and perplexity is 65.59644043895848
At time: 91.78918981552124 and batch: 900, loss is 4.314274129867553 and perplexity is 74.75933817359659
At time: 92.20270848274231 and batch: 950, loss is 4.23553231716156 and perplexity is 69.09845123676313
At time: 92.61297869682312 and batch: 1000, loss is 4.205332059860229 and perplexity is 67.0428562119785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.703645659656059 and perplexity of 110.3487339565609
Finished 10 epochs...
Completing Train Step...
At time: 93.88167548179626 and batch: 50, loss is 4.356814165115356 and perplexity is 78.00821682997598
At time: 94.29953527450562 and batch: 100, loss is 4.2449397325515745 and perplexity is 69.751556268906
At time: 94.72218775749207 and batch: 150, loss is 4.292844648361206 and perplexity is 73.17432794802595
At time: 95.14310836791992 and batch: 200, loss is 4.311624183654785 and perplexity is 74.56149220489851
At time: 95.57439184188843 and batch: 250, loss is 4.285920219421387 and perplexity is 72.66938774277504
At time: 96.006511926651 and batch: 300, loss is 4.19827645778656 and perplexity is 66.5714933265689
At time: 96.44955587387085 and batch: 350, loss is 4.278321657180786 and perplexity is 72.11929747137142
At time: 96.88005757331848 and batch: 400, loss is 4.188972225189209 and perplexity is 65.95496925761171
At time: 97.303218126297 and batch: 450, loss is 4.239280290603638 and perplexity is 69.35791632728942
At time: 97.71610617637634 and batch: 500, loss is 4.184751563072204 and perplexity is 65.67718225235716
At time: 98.13897442817688 and batch: 550, loss is 4.229063229560852 and perplexity is 68.65289004369393
At time: 98.54815554618835 and batch: 600, loss is 4.2676578140258785 and perplexity is 71.35431467230444
At time: 98.98113584518433 and batch: 650, loss is 4.228415517807007 and perplexity is 68.60843715775967
At time: 99.40151762962341 and batch: 700, loss is 4.189804444313049 and perplexity is 66.00988109050962
At time: 99.82564210891724 and batch: 750, loss is 4.171425137519837 and perplexity is 64.80774627663529
At time: 100.24955654144287 and batch: 800, loss is 4.177051091194153 and perplexity is 65.17337920749227
At time: 100.6729326248169 and batch: 850, loss is 4.1695583534240725 and perplexity is 64.68687706007059
At time: 101.12510704994202 and batch: 900, loss is 4.302233266830444 and perplexity is 73.86456892931875
At time: 101.53814196586609 and batch: 950, loss is 4.221733918190003 and perplexity is 68.15155111488613
At time: 101.9477231502533 and batch: 1000, loss is 4.189482679367066 and perplexity is 65.98864484140965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.702309957364711 and perplexity of 110.20143929257682
Finished 11 epochs...
Completing Train Step...
At time: 103.31392645835876 and batch: 50, loss is 4.335258846282959 and perplexity is 76.34471787883342
At time: 103.75045680999756 and batch: 100, loss is 4.226132140159607 and perplexity is 68.45195690567377
At time: 104.16916060447693 and batch: 150, loss is 4.273366055488586 and perplexity is 71.76278705020317
At time: 104.58660006523132 and batch: 200, loss is 4.293354558944702 and perplexity is 73.21164982688867
At time: 104.99497771263123 and batch: 250, loss is 4.267434740066529 and perplexity is 71.33839915804818
At time: 105.40910243988037 and batch: 300, loss is 4.179698004722595 and perplexity is 65.34611601545612
At time: 105.8219826221466 and batch: 350, loss is 4.261635179519653 and perplexity is 70.92586520692181
At time: 106.2577691078186 and batch: 400, loss is 4.171878843307495 and perplexity is 64.83715659751802
At time: 106.68733334541321 and batch: 450, loss is 4.222932834625244 and perplexity is 68.2333081297262
At time: 107.12099027633667 and batch: 500, loss is 4.168441963195801 and perplexity is 64.61470155813205
At time: 107.52707958221436 and batch: 550, loss is 4.214040794372559 and perplexity is 67.62926437799834
At time: 107.9369056224823 and batch: 600, loss is 4.251692109107971 and perplexity is 70.22413877414392
At time: 108.35303068161011 and batch: 650, loss is 4.213115558624268 and perplexity is 67.56672030343383
At time: 108.77081489562988 and batch: 700, loss is 4.176310319900512 and perplexity is 65.12511851634
At time: 109.20315146446228 and batch: 750, loss is 4.157571845054626 and perplexity is 63.91613574350582
At time: 109.6281042098999 and batch: 800, loss is 4.162442903518677 and perplexity is 64.22823448601685
At time: 110.05919599533081 and batch: 850, loss is 4.157105884552002 and perplexity is 63.886360286401185
At time: 110.4827606678009 and batch: 900, loss is 4.288328065872192 and perplexity is 72.8445752978593
At time: 110.91197419166565 and batch: 950, loss is 4.208586187362671 and perplexity is 67.26137756956679
At time: 111.34232330322266 and batch: 1000, loss is 4.175668826103211 and perplexity is 65.08335455385688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.700260069312119 and perplexity of 109.97577005615095
Finished 12 epochs...
Completing Train Step...
At time: 112.74515676498413 and batch: 50, loss is 4.31591760635376 and perplexity is 74.88230440638755
At time: 113.18473410606384 and batch: 100, loss is 4.208729386329651 and perplexity is 67.27101001901435
At time: 113.62823939323425 and batch: 150, loss is 4.255506644248962 and perplexity is 70.49252277391224
At time: 114.04566240310669 and batch: 200, loss is 4.276390514373779 and perplexity is 71.98015919999001
At time: 114.46987676620483 and batch: 250, loss is 4.250572023391723 and perplexity is 70.14552575424649
At time: 114.89377522468567 and batch: 300, loss is 4.16331374168396 and perplexity is 64.2841912450004
At time: 115.32334232330322 and batch: 350, loss is 4.245066051483154 and perplexity is 69.76036776748774
At time: 115.74964261054993 and batch: 400, loss is 4.154566345214843 and perplexity is 63.72432419694199
At time: 116.16804194450378 and batch: 450, loss is 4.206770715713501 and perplexity is 67.139377223099
At time: 116.58770799636841 and batch: 500, loss is 4.151822710037232 and perplexity is 63.54972752371654
At time: 117.00391364097595 and batch: 550, loss is 4.198632020950317 and perplexity is 66.5951679060069
At time: 117.41736102104187 and batch: 600, loss is 4.236690940856934 and perplexity is 69.1785567367837
At time: 117.82543897628784 and batch: 650, loss is 4.198066802024841 and perplexity is 66.55753769241744
At time: 118.23989415168762 and batch: 700, loss is 4.160573391914368 and perplexity is 64.10827122761282
At time: 118.6507716178894 and batch: 750, loss is 4.144969935417175 and perplexity is 63.11572432529874
At time: 119.07019758224487 and batch: 800, loss is 4.149115109443665 and perplexity is 63.37789297838879
At time: 119.48941850662231 and batch: 850, loss is 4.1447269201278685 and perplexity is 63.10038810283576
At time: 119.9046425819397 and batch: 900, loss is 4.275279779434204 and perplexity is 71.90025270789197
At time: 120.31544733047485 and batch: 950, loss is 4.196170492172241 and perplexity is 66.43144357247364
At time: 120.73773717880249 and batch: 1000, loss is 4.160938911437988 and perplexity is 64.13170833547134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.698642637671494 and perplexity of 109.79803554142129
Finished 13 epochs...
Completing Train Step...
At time: 122.03966808319092 and batch: 50, loss is 4.298259286880493 and perplexity is 73.57161509525307
At time: 122.46239686012268 and batch: 100, loss is 4.1920978593826295 and perplexity is 66.16144287715508
At time: 122.9017322063446 and batch: 150, loss is 4.239765954017639 and perplexity is 69.3916091107348
At time: 123.31693506240845 and batch: 200, loss is 4.261423377990723 and perplexity is 70.91084459098113
At time: 123.74095106124878 and batch: 250, loss is 4.234863901138306 and perplexity is 69.05228015723814
At time: 124.15143418312073 and batch: 300, loss is 4.147232270240783 and perplexity is 63.25867486638038
At time: 124.57771301269531 and batch: 350, loss is 4.232905330657959 and perplexity is 68.91716875551363
At time: 124.98964381217957 and batch: 400, loss is 4.139860229492188 and perplexity is 62.794044080378605
At time: 125.40876913070679 and batch: 450, loss is 4.191532015800476 and perplexity is 66.12401643907053
At time: 125.83188509941101 and batch: 500, loss is 4.1370975255966185 and perplexity is 62.62080214844886
At time: 126.25317764282227 and batch: 550, loss is 4.184320497512817 and perplexity is 65.64887718215932
At time: 126.68167328834534 and batch: 600, loss is 4.22295271396637 and perplexity is 68.23466457641726
At time: 127.10210251808167 and batch: 650, loss is 4.1840589427948 and perplexity is 65.63170865395244
At time: 127.53001809120178 and batch: 700, loss is 4.148065304756164 and perplexity is 63.31139348110551
At time: 127.94175601005554 and batch: 750, loss is 4.132673954963684 and perplexity is 62.34440638585209
At time: 128.36246347427368 and batch: 800, loss is 4.136926240921021 and perplexity is 62.61007708321308
At time: 128.78023600578308 and batch: 850, loss is 4.132434992790222 and perplexity is 62.32951021088103
At time: 129.2084379196167 and batch: 900, loss is 4.263136034011841 and perplexity is 71.03239453284156
At time: 129.61941719055176 and batch: 950, loss is 4.185104398727417 and perplexity is 65.70035959264527
At time: 130.0343017578125 and batch: 1000, loss is 4.146948494911194 and perplexity is 63.240726161891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.697832061023247 and perplexity of 109.70907187859287
Finished 14 epochs...
Completing Train Step...
At time: 131.38799715042114 and batch: 50, loss is 4.282540135383606 and perplexity is 72.42417376062727
At time: 131.82331085205078 and batch: 100, loss is 4.176038942337036 and perplexity is 65.10744741823416
At time: 132.2437801361084 and batch: 150, loss is 4.224485521316528 and perplexity is 68.33933537140388
At time: 132.6582272052765 and batch: 200, loss is 4.248031330108643 and perplexity is 69.9675336954853
At time: 133.08354687690735 and batch: 250, loss is 4.218896641731262 and perplexity is 67.95846037871516
At time: 133.51990008354187 and batch: 300, loss is 4.1324781227111815 and perplexity is 62.33219853570305
At time: 133.94504404067993 and batch: 350, loss is 4.217476568222046 and perplexity is 67.8620228597936
At time: 134.36227226257324 and batch: 400, loss is 4.126446146965026 and perplexity is 61.95734391948919
At time: 134.80083203315735 and batch: 450, loss is 4.17759533405304 and perplexity is 65.20885900765363
At time: 135.23913192749023 and batch: 500, loss is 4.1224614524841305 and perplexity is 61.710954053302665
At time: 135.65135288238525 and batch: 550, loss is 4.170860409736633 and perplexity is 64.77115787396224
At time: 136.05822229385376 and batch: 600, loss is 4.209881601333618 and perplexity is 67.34856535772252
At time: 136.49387955665588 and batch: 650, loss is 4.170949492454529 and perplexity is 64.77692812175776
At time: 136.92742538452148 and batch: 700, loss is 4.13429099559784 and perplexity is 62.44530137795617
At time: 137.3620252609253 and batch: 750, loss is 4.119828143119812 and perplexity is 61.548663794123705
At time: 137.79335713386536 and batch: 800, loss is 4.124427886009216 and perplexity is 61.83242393429897
At time: 138.2139914035797 and batch: 850, loss is 4.121359663009644 and perplexity is 61.64299901661271
At time: 138.62617373466492 and batch: 900, loss is 4.2517801284790036 and perplexity is 70.23032013070569
At time: 139.03609085083008 and batch: 950, loss is 4.172765130996704 and perplexity is 64.89464644372195
At time: 139.460102558136 and batch: 1000, loss is 4.134008712768555 and perplexity is 62.42767662930665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6970103194073936 and perplexity of 109.61895639948423
Finished 15 epochs...
Completing Train Step...
At time: 140.73142385482788 and batch: 50, loss is 4.266643543243408 and perplexity is 71.28197876602849
At time: 141.17042207717896 and batch: 100, loss is 4.159921498298645 and perplexity is 64.06649307381987
At time: 141.59671783447266 and batch: 150, loss is 4.2094833326339725 and perplexity is 67.32174787281143
At time: 142.0184440612793 and batch: 200, loss is 4.2346240234375 and perplexity is 69.03571804155915
At time: 142.42064547538757 and batch: 250, loss is 4.205763807296753 and perplexity is 67.0718080427749
At time: 142.83685517311096 and batch: 300, loss is 4.118787364959717 and perplexity is 61.484638612848364
At time: 143.2459146976471 and batch: 350, loss is 4.202552161216736 and perplexity is 66.85674267517771
At time: 143.66126370429993 and batch: 400, loss is 4.111987624168396 and perplexity is 61.067977207820576
At time: 144.08468317985535 and batch: 450, loss is 4.163900442123413 and perplexity is 64.32191787428656
At time: 144.51146912574768 and batch: 500, loss is 4.108531646728515 and perplexity is 60.85729192777675
At time: 144.9393036365509 and batch: 550, loss is 4.158131594657898 and perplexity is 63.95192279008908
At time: 145.35246086120605 and batch: 600, loss is 4.197710919380188 and perplexity is 66.53385523421974
At time: 145.77374386787415 and batch: 650, loss is 4.158696904182434 and perplexity is 63.98808564179388
At time: 146.19065713882446 and batch: 700, loss is 4.122231183052063 and perplexity is 61.69674554291603
At time: 146.6172149181366 and batch: 750, loss is 4.1074901294708255 and perplexity is 60.79394100424447
At time: 147.03574180603027 and batch: 800, loss is 4.1115546226501465 and perplexity is 61.04154040497409
At time: 147.45665097236633 and batch: 850, loss is 4.10966236114502 and perplexity is 60.92614306325905
At time: 147.87600231170654 and batch: 900, loss is 4.240451965332031 and perplexity is 69.43922887168851
At time: 148.29470300674438 and batch: 950, loss is 4.16037220954895 and perplexity is 64.09537507124753
At time: 148.71535515785217 and batch: 1000, loss is 4.120923867225647 and perplexity is 61.61614111023463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.695634795398247 and perplexity of 109.46827654873336
Finished 16 epochs...
Completing Train Step...
At time: 150.01309442520142 and batch: 50, loss is 4.251840591430664 and perplexity is 70.23456659153234
At time: 150.45961904525757 and batch: 100, loss is 4.144883885383606 and perplexity is 63.11029344876868
At time: 150.87996339797974 and batch: 150, loss is 4.195777115821838 and perplexity is 66.40531615294047
At time: 151.30362844467163 and batch: 200, loss is 4.221292848587036 and perplexity is 68.12149816550279
At time: 151.71324348449707 and batch: 250, loss is 4.191170868873596 and perplexity is 66.10014026540347
At time: 152.12964129447937 and batch: 300, loss is 4.105360336303711 and perplexity is 60.66460026749137
At time: 152.54059720039368 and batch: 350, loss is 4.189147019386292 and perplexity is 65.96649881112688
At time: 152.95898818969727 and batch: 400, loss is 4.09778386592865 and perplexity is 60.2067134938713
At time: 153.3874204158783 and batch: 450, loss is 4.150665197372437 and perplexity is 63.476210465934685
At time: 153.81732511520386 and batch: 500, loss is 4.095961318016053 and perplexity is 60.09708380689349
At time: 154.2465841770172 and batch: 550, loss is 4.1452632331848145 and perplexity is 63.134238741332005
At time: 154.68786811828613 and batch: 600, loss is 4.185496606826782 and perplexity is 65.72613285972037
At time: 155.1170961856842 and batch: 650, loss is 4.14635790348053 and perplexity is 63.20338775790681
At time: 155.52951455116272 and batch: 700, loss is 4.110154700279236 and perplexity is 60.95614677318016
At time: 155.95112133026123 and batch: 750, loss is 4.096270508766175 and perplexity is 60.11566814222012
At time: 156.36171984672546 and batch: 800, loss is 4.100033349990845 and perplexity is 60.34229997895719
At time: 156.7839493751526 and batch: 850, loss is 4.098962502479553 and perplexity is 60.277717162544725
At time: 157.2118842601776 and batch: 900, loss is 4.229679713249206 and perplexity is 68.69522647908433
At time: 157.6382405757904 and batch: 950, loss is 4.149754252433777 and perplexity is 63.41841346222619
At time: 158.0488681793213 and batch: 1000, loss is 4.10921320438385 and perplexity is 60.89878381892492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.695938854682736 and perplexity of 109.50156645537115
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 159.38195824623108 and batch: 50, loss is 4.240332412719726 and perplexity is 69.43092772670212
At time: 159.8204050064087 and batch: 100, loss is 4.127713007926941 and perplexity is 62.035884999614396
At time: 160.23681497573853 and batch: 150, loss is 4.171246404647827 and perplexity is 64.79616403710911
At time: 160.65979886054993 and batch: 200, loss is 4.191081328392029 and perplexity is 66.09422189198341
At time: 161.0703160762787 and batch: 250, loss is 4.152599415779114 and perplexity is 63.59910613582262
At time: 161.4804093837738 and batch: 300, loss is 4.066133961677552 and perplexity is 58.33101617064855
At time: 161.8880434036255 and batch: 350, loss is 4.146118083000183 and perplexity is 63.18823210848539
At time: 162.30506014823914 and batch: 400, loss is 4.046797103881836 and perplexity is 57.213913046172074
At time: 162.7129669189453 and batch: 450, loss is 4.09931631565094 and perplexity is 60.29904798615861
At time: 163.1269929409027 and batch: 500, loss is 4.040075263977051 and perplexity is 56.83061994298498
At time: 163.5358657836914 and batch: 550, loss is 4.084630045890808 and perplexity is 59.419951029404295
At time: 163.95106649398804 and batch: 600, loss is 4.119789896011352 and perplexity is 61.5463097807214
At time: 164.36005854606628 and batch: 650, loss is 4.074214253425598 and perplexity is 58.80425718714097
At time: 164.77446579933167 and batch: 700, loss is 4.034238772392273 and perplexity is 56.499894584651436
At time: 165.18825674057007 and batch: 750, loss is 4.014393677711487 and perplexity is 55.389701208671916
At time: 165.6019651889801 and batch: 800, loss is 4.011420340538025 and perplexity is 55.22525355136063
At time: 166.0386393070221 and batch: 850, loss is 4.00371723651886 and perplexity is 54.80148195259377
At time: 166.45563769340515 and batch: 900, loss is 4.1274238872528075 and perplexity is 62.01795173528651
At time: 166.8930060863495 and batch: 950, loss is 4.042029185295105 and perplexity is 56.941771057713304
At time: 167.31107997894287 and batch: 1000, loss is 4.003531246185303 and perplexity is 54.79129035448475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.650604620212462 and perplexity of 104.64823889350404
Finished 18 epochs...
Completing Train Step...
At time: 168.6458330154419 and batch: 50, loss is 4.202791743278503 and perplexity is 66.87276227035808
At time: 169.08075594902039 and batch: 100, loss is 4.0942470788955685 and perplexity is 59.994151285488776
At time: 169.5039885044098 and batch: 150, loss is 4.141188225746155 and perplexity is 62.877489731181086
At time: 169.9287793636322 and batch: 200, loss is 4.163876423835754 and perplexity is 64.32037299051306
At time: 170.36548709869385 and batch: 250, loss is 4.129638648033142 and perplexity is 62.15545887897297
At time: 170.78086495399475 and batch: 300, loss is 4.047377800941467 and perplexity is 57.24714664564081
At time: 171.20217537879944 and batch: 350, loss is 4.127449612617493 and perplexity is 62.019547190233695
At time: 171.62178564071655 and batch: 400, loss is 4.029813809394836 and perplexity is 56.25043696915201
At time: 172.03110098838806 and batch: 450, loss is 4.084193305969238 and perplexity is 59.39400563076012
At time: 172.44257736206055 and batch: 500, loss is 4.025605082511902 and perplexity is 56.014191737457
At time: 172.85115242004395 and batch: 550, loss is 4.072296657562256 and perplexity is 58.691602434476756
At time: 173.2876374721527 and batch: 600, loss is 4.108218359947204 and perplexity is 60.838229128886766
At time: 173.69531178474426 and batch: 650, loss is 4.063939247131348 and perplexity is 58.20313662184749
At time: 174.12461376190186 and batch: 700, loss is 4.027426853179931 and perplexity is 56.116329756733094
At time: 174.54024529457092 and batch: 750, loss is 4.0097636795043945 and perplexity is 55.13383976746389
At time: 174.95755529403687 and batch: 800, loss is 4.007319498062134 and perplexity is 54.99924721078324
At time: 175.3894693851471 and batch: 850, loss is 4.001481761932373 and perplexity is 54.679111461409185
At time: 175.8121542930603 and batch: 900, loss is 4.128300542831421 and perplexity is 62.07234395678815
At time: 176.2275152206421 and batch: 950, loss is 4.047775173187256 and perplexity is 57.269899593263965
At time: 176.67358350753784 and batch: 1000, loss is 4.010638179779053 and perplexity is 55.18207541345234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.647417021960747 and perplexity of 104.3151934397153
Finished 19 epochs...
Completing Train Step...
At time: 178.04007840156555 and batch: 50, loss is 4.191297588348388 and perplexity is 66.10851697119627
At time: 178.45147252082825 and batch: 100, loss is 4.0825514793396 and perplexity is 59.29657097796545
At time: 178.87663078308105 and batch: 150, loss is 4.129473433494568 and perplexity is 62.14519074176059
At time: 179.2880527973175 and batch: 200, loss is 4.1528059768676755 and perplexity is 63.61224459331792
At time: 179.69973492622375 and batch: 250, loss is 4.1200790166854855 and perplexity is 61.56410666389508
At time: 180.10577034950256 and batch: 300, loss is 4.038091115951538 and perplexity is 56.71797137331163
At time: 180.53176021575928 and batch: 350, loss is 4.1181555318832395 and perplexity is 61.445802854627786
At time: 180.9597852230072 and batch: 400, loss is 4.021802759170532 and perplexity is 55.80161207319239
At time: 181.38011240959167 and batch: 450, loss is 4.076628098487854 and perplexity is 58.946373006771594
At time: 181.7948703765869 and batch: 500, loss is 4.0180087184906 and perplexity is 55.59029960451447
At time: 182.2089638710022 and batch: 550, loss is 4.065517387390137 and perplexity is 58.295061851312404
At time: 182.63612008094788 and batch: 600, loss is 4.10229458808899 and perplexity is 60.47890267400208
At time: 183.05615162849426 and batch: 650, loss is 4.058363533020019 and perplexity is 57.87951561916672
At time: 183.47192883491516 and batch: 700, loss is 4.023603892326355 and perplexity is 55.90220877367676
At time: 183.87927746772766 and batch: 750, loss is 4.006420640945435 and perplexity is 54.94983295751835
At time: 184.2926127910614 and batch: 800, loss is 4.003412013053894 and perplexity is 54.784757806817645
At time: 184.69716358184814 and batch: 850, loss is 3.998881735801697 and perplexity is 54.53712900184489
At time: 185.10535430908203 and batch: 900, loss is 4.128510599136352 and perplexity is 62.08538401352327
At time: 185.52261972427368 and batch: 950, loss is 4.049505414962769 and perplexity is 57.36907614098028
At time: 185.9393219947815 and batch: 1000, loss is 4.012531704902649 and perplexity is 55.28666304802736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.645281163657584 and perplexity of 104.09262873560634
Finished 20 epochs...
Completing Train Step...
At time: 187.21814107894897 and batch: 50, loss is 4.1831297063827515 and perplexity is 65.57074960755543
At time: 187.66128778457642 and batch: 100, loss is 4.074388279914856 and perplexity is 58.81449157607419
At time: 188.1023542881012 and batch: 150, loss is 4.121010432243347 and perplexity is 61.621475143447434
At time: 188.5258584022522 and batch: 200, loss is 4.145038928985596 and perplexity is 63.1200790545658
At time: 188.95805883407593 and batch: 250, loss is 4.113080735206604 and perplexity is 61.134767785895924
At time: 189.39222049713135 and batch: 300, loss is 4.03113938331604 and perplexity is 56.3250505234199
At time: 189.8174924850464 and batch: 350, loss is 4.111787362098694 and perplexity is 61.055748832793114
At time: 190.23423385620117 and batch: 400, loss is 4.016090087890625 and perplexity is 55.4837446071577
At time: 190.65677213668823 and batch: 450, loss is 4.071134939193725 and perplexity is 58.62345891130523
At time: 191.06601548194885 and batch: 500, loss is 4.01271418094635 and perplexity is 55.296752460079794
At time: 191.4802746772766 and batch: 550, loss is 4.060026526451111 and perplexity is 57.97584895208488
At time: 191.90382862091064 and batch: 600, loss is 4.097881164550781 and perplexity is 60.21257180913565
At time: 192.31793189048767 and batch: 650, loss is 4.053886742591858 and perplexity is 57.620980293680894
At time: 192.72912740707397 and batch: 700, loss is 4.022012667655945 and perplexity is 55.81332653450568
At time: 193.15417528152466 and batch: 750, loss is 4.004576044082642 and perplexity is 54.84856609501603
At time: 193.58063912391663 and batch: 800, loss is 4.00121835231781 and perplexity is 54.66471035451276
At time: 194.0069715976715 and batch: 850, loss is 3.997276396751404 and perplexity is 54.44964865556261
At time: 194.43860459327698 and batch: 900, loss is 4.126180229187011 and perplexity is 61.94087055064029
At time: 194.8670153617859 and batch: 950, loss is 4.048625607490539 and perplexity is 57.318624596193956
At time: 195.307288646698 and batch: 1000, loss is 4.01224301815033 and perplexity is 55.270704824400156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643917548947218 and perplexity of 103.95078322911095
Finished 21 epochs...
Completing Train Step...
At time: 196.608464717865 and batch: 50, loss is 4.175967469215393 and perplexity is 65.10279415201866
At time: 197.0447769165039 and batch: 100, loss is 4.067258424758911 and perplexity is 58.396644136027646
At time: 197.46536421775818 and batch: 150, loss is 4.113674302101135 and perplexity is 61.17106613184097
At time: 197.88533091545105 and batch: 200, loss is 4.1381327915191655 and perplexity is 62.68566490026081
At time: 198.3052899837494 and batch: 250, loss is 4.106395721435547 and perplexity is 60.72744402076681
At time: 198.74403023719788 and batch: 300, loss is 4.024967885017395 and perplexity is 55.97851100386933
At time: 199.16738057136536 and batch: 350, loss is 4.106214394569397 and perplexity is 60.71643350193485
At time: 199.58257341384888 and batch: 400, loss is 4.010520005226136 and perplexity is 55.17555468166116
At time: 200.00993871688843 and batch: 450, loss is 4.066212677955628 and perplexity is 58.33560795185951
At time: 200.4187934398651 and batch: 500, loss is 4.007727417945862 and perplexity is 55.0216870738326
At time: 200.85003995895386 and batch: 550, loss is 4.055223402976989 and perplexity is 57.698051472912965
At time: 201.28114867210388 and batch: 600, loss is 4.094438352584839 and perplexity is 60.005627685671506
At time: 201.70369029045105 and batch: 650, loss is 4.051045904159546 and perplexity is 57.457520689449545
At time: 202.13346314430237 and batch: 700, loss is 4.018799443244934 and perplexity is 55.63427361389023
At time: 202.56064295768738 and batch: 750, loss is 4.001578402519226 and perplexity is 54.68439593817277
At time: 202.9857907295227 and batch: 800, loss is 3.998004651069641 and perplexity is 54.489316289632264
At time: 203.4150230884552 and batch: 850, loss is 3.994311184883118 and perplexity is 54.288433048484606
At time: 203.84139704704285 and batch: 900, loss is 4.123958711624145 and perplexity is 61.80342054917689
At time: 204.2535524368286 and batch: 950, loss is 4.047550497055053 and perplexity is 57.25703385909752
At time: 204.67742657661438 and batch: 1000, loss is 4.011161122322083 and perplexity is 55.21094001490494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.642462753668064 and perplexity of 103.79966606932257
Finished 22 epochs...
Completing Train Step...
At time: 205.9829671382904 and batch: 50, loss is 4.169915933609008 and perplexity is 64.71001194157374
At time: 206.409512758255 and batch: 100, loss is 4.061007041931152 and perplexity is 58.03272304785997
At time: 206.8349254131317 and batch: 150, loss is 4.107172102928161 and perplexity is 60.77460999142249
At time: 207.25944685935974 and batch: 200, loss is 4.1320921993255615 and perplexity is 62.308147723794896
At time: 207.6742866039276 and batch: 250, loss is 4.1005009841918945 and perplexity is 60.37052470110568
At time: 208.0959713459015 and batch: 300, loss is 4.020428028106689 and perplexity is 55.724952568837004
At time: 208.519357919693 and batch: 350, loss is 4.101114420890808 and perplexity is 60.40756955766561
At time: 208.9501621723175 and batch: 400, loss is 4.00660249710083 and perplexity is 54.95982683157582
At time: 209.3996181488037 and batch: 450, loss is 4.0619608545303345 and perplexity is 58.08810179654356
At time: 209.8235743045807 and batch: 500, loss is 4.003563508987427 and perplexity is 54.7930581035597
At time: 210.2373445034027 and batch: 550, loss is 4.051562857627869 and perplexity is 57.48723123287378
At time: 210.64887952804565 and batch: 600, loss is 4.091450519561768 and perplexity is 59.82660846265285
At time: 211.0899555683136 and batch: 650, loss is 4.047235321998596 and perplexity is 57.23899071374246
At time: 211.50880217552185 and batch: 700, loss is 4.01570641040802 and perplexity is 55.462460827016045
At time: 211.9338414669037 and batch: 750, loss is 3.9990357303619386 and perplexity is 54.54552806973116
At time: 212.3447244167328 and batch: 800, loss is 3.995620985031128 and perplexity is 54.359586634443005
At time: 212.7572271823883 and batch: 850, loss is 3.9916865730285647 and perplexity is 54.14613380536735
At time: 213.16325068473816 and batch: 900, loss is 4.122623572349548 and perplexity is 61.7209594358823
At time: 213.58654689788818 and batch: 950, loss is 4.046214156150818 and perplexity is 57.18057004493681
At time: 213.9954695701599 and batch: 1000, loss is 4.0101467847824095 and perplexity is 55.154965878980065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641826722680069 and perplexity of 103.73366725602902
Finished 23 epochs...
Completing Train Step...
At time: 215.34624576568604 and batch: 50, loss is 4.164575610160828 and perplexity is 64.36536064127404
At time: 215.78169131278992 and batch: 100, loss is 4.0554869270324705 and perplexity is 57.71325830102497
At time: 216.21175122261047 and batch: 150, loss is 4.101518568992614 and perplexity is 60.4319880962577
At time: 216.63708806037903 and batch: 200, loss is 4.126301488876343 and perplexity is 61.94838193676515
At time: 217.06027150154114 and batch: 250, loss is 4.095961146354675 and perplexity is 60.09707349054618
At time: 217.49046969413757 and batch: 300, loss is 4.015961632728577 and perplexity is 55.47661789149468
At time: 217.90116572380066 and batch: 350, loss is 4.097042212486267 and perplexity is 60.162077531807434
At time: 218.32349157333374 and batch: 400, loss is 4.0030390071868895 and perplexity is 54.76432658145364
At time: 218.73754453659058 and batch: 450, loss is 4.057890682220459 and perplexity is 57.852153713487354
At time: 219.16711473464966 and batch: 500, loss is 3.9991251754760744 and perplexity is 54.55040711891525
At time: 219.5743203163147 and batch: 550, loss is 4.047868323326111 and perplexity is 57.27523454083497
At time: 219.99701166152954 and batch: 600, loss is 4.088153438568115 and perplexity is 59.62968011173383
At time: 220.4340124130249 and batch: 650, loss is 4.04387791633606 and perplexity is 57.047138445364325
At time: 220.86981773376465 and batch: 700, loss is 4.012893896102906 and perplexity is 55.306691017633135
At time: 221.2829999923706 and batch: 750, loss is 3.9968489360809327 and perplexity is 54.426378546124
At time: 221.69825148582458 and batch: 800, loss is 3.993539056777954 and perplexity is 54.246531602266444
At time: 222.11714243888855 and batch: 850, loss is 3.9895262479782105 and perplexity is 54.0292868153114
At time: 222.5474374294281 and batch: 900, loss is 4.120041542053222 and perplexity is 61.56179961486542
At time: 222.96325516700745 and batch: 950, loss is 4.04467001914978 and perplexity is 57.092343545421514
At time: 223.37407517433167 and batch: 1000, loss is 4.008487839698791 and perplexity is 55.063542673503754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641271079458842 and perplexity of 103.67604435737427
Finished 24 epochs...
Completing Train Step...
At time: 224.70414566993713 and batch: 50, loss is 4.159596800804138 and perplexity is 64.04569422088743
At time: 225.14281916618347 and batch: 100, loss is 4.050312080383301 and perplexity is 57.41537246122094
At time: 225.56716537475586 and batch: 150, loss is 4.096429705619812 and perplexity is 60.12523912925797
At time: 225.9845495223999 and batch: 200, loss is 4.121164350509644 and perplexity is 61.63096054403765
At time: 226.40464663505554 and batch: 250, loss is 4.091503667831421 and perplexity is 59.829788227870594
At time: 226.82106471061707 and batch: 300, loss is 4.012210001945496 and perplexity is 55.26888002561248
At time: 227.2477195262909 and batch: 350, loss is 4.093080859184266 and perplexity is 59.924225705916655
At time: 227.65952134132385 and batch: 400, loss is 3.9987901210784913 and perplexity is 54.53213282673205
At time: 228.07295656204224 and batch: 450, loss is 4.055250062942505 and perplexity is 57.699589721480265
At time: 228.49547290802002 and batch: 500, loss is 3.996164608001709 and perplexity is 54.3891457881983
At time: 228.9108383655548 and batch: 550, loss is 4.044220547676087 and perplexity is 57.06668793179669
At time: 229.3338441848755 and batch: 600, loss is 4.084844813346863 and perplexity is 59.4327138715983
At time: 229.76135277748108 and batch: 650, loss is 4.040433502197265 and perplexity is 56.85098249023062
At time: 230.1798861026764 and batch: 700, loss is 4.010299587249756 and perplexity is 55.16339433778105
At time: 230.58562421798706 and batch: 750, loss is 3.99460958480835 and perplexity is 54.30463513007698
At time: 231.02722215652466 and batch: 800, loss is 3.990797395706177 and perplexity is 54.098009689699516
At time: 231.45192170143127 and batch: 850, loss is 3.986010355949402 and perplexity is 53.83965922687857
At time: 231.87408256530762 and batch: 900, loss is 4.1178153085708615 and perplexity is 61.42490111587988
At time: 232.30171608924866 and batch: 950, loss is 4.042582025527954 and perplexity is 56.97325946291035
At time: 232.7255117893219 and batch: 1000, loss is 4.006693720817566 and perplexity is 54.96484069993894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.640939107755336 and perplexity of 103.64163255650428
Finished 25 epochs...
Completing Train Step...
At time: 234.0410315990448 and batch: 50, loss is 4.154902868270874 and perplexity is 63.74577250998773
At time: 234.45024251937866 and batch: 100, loss is 4.04543375492096 and perplexity is 57.13596366545033
At time: 234.86580657958984 and batch: 150, loss is 4.091421251296997 and perplexity is 59.82485746726044
At time: 235.27767133712769 and batch: 200, loss is 4.116534523963928 and perplexity is 61.3462794075328
At time: 235.71643257141113 and batch: 250, loss is 4.087515940666199 and perplexity is 59.591678430053825
At time: 236.1418011188507 and batch: 300, loss is 4.007748174667358 and perplexity is 55.02282915552035
At time: 236.5619809627533 and batch: 350, loss is 4.089710268974304 and perplexity is 59.72258571120942
At time: 236.98273348808289 and batch: 400, loss is 3.9954233503341676 and perplexity is 54.3488443555701
At time: 237.4212167263031 and batch: 450, loss is 4.051131281852722 and perplexity is 57.462426489441626
At time: 237.8337516784668 and batch: 500, loss is 3.992475094795227 and perplexity is 54.188846048006916
At time: 238.25365161895752 and batch: 550, loss is 4.0410345268249515 and perplexity is 56.8851616010302
At time: 238.6802954673767 and batch: 600, loss is 4.082460870742798 and perplexity is 59.29119844227663
At time: 239.10631227493286 and batch: 650, loss is 4.037868461608887 and perplexity is 56.705344276470136
At time: 239.51779103279114 and batch: 700, loss is 4.00774338722229 and perplexity is 55.022565737378834
At time: 239.93874502182007 and batch: 750, loss is 3.9925281381607056 and perplexity is 54.19172048300688
At time: 240.35475659370422 and batch: 800, loss is 3.9885559988021853 and perplexity is 53.97689036721334
At time: 240.77322673797607 and batch: 850, loss is 3.9831659603118896 and perplexity is 53.68673552590597
At time: 241.2120168209076 and batch: 900, loss is 4.1153204011917115 and perplexity is 61.271842689514465
At time: 241.6261625289917 and batch: 950, loss is 4.040755982398987 and perplexity is 56.8693187629157
At time: 242.06341814994812 and batch: 1000, loss is 4.004589281082153 and perplexity is 54.8492921302639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.640600809236852 and perplexity of 103.60657667576774
Finished 26 epochs...
Completing Train Step...
At time: 243.36302280426025 and batch: 50, loss is 4.1503599262237545 and perplexity is 63.45683596763989
At time: 243.80251574516296 and batch: 100, loss is 4.04095076084137 and perplexity is 56.88039675908612
At time: 244.22080492973328 and batch: 150, loss is 4.086696372032166 and perplexity is 59.54285896774583
At time: 244.64929175376892 and batch: 200, loss is 4.112240743637085 and perplexity is 61.08343665822673
At time: 245.06578493118286 and batch: 250, loss is 4.083325400352478 and perplexity is 59.3424796027702
At time: 245.47614431381226 and batch: 300, loss is 4.004366636276245 and perplexity is 54.83708157962164
At time: 245.90208649635315 and batch: 350, loss is 4.085964922904968 and perplexity is 59.49932231988563
At time: 246.31850004196167 and batch: 400, loss is 3.9914125633239745 and perplexity is 54.13129927173455
At time: 246.75068712234497 and batch: 450, loss is 4.047251625061035 and perplexity is 57.239923892188806
At time: 247.17714595794678 and batch: 500, loss is 3.988663935661316 and perplexity is 53.98271677766178
At time: 247.60118961334229 and batch: 550, loss is 4.038074288368225 and perplexity is 56.71701695495332
At time: 248.02797508239746 and batch: 600, loss is 4.07946560382843 and perplexity is 59.113871181084576
At time: 248.45550227165222 and batch: 650, loss is 4.034697828292846 and perplexity is 56.52583714873027
At time: 248.86388492584229 and batch: 700, loss is 4.00526222705841 and perplexity is 54.88621516291324
At time: 249.27708339691162 and batch: 750, loss is 3.9909329128265383 and perplexity is 54.105341392964384
At time: 249.70020937919617 and batch: 800, loss is 3.98721480846405 and perplexity is 53.90454560826055
At time: 250.11869287490845 and batch: 850, loss is 3.98120521068573 and perplexity is 53.58157241225368
At time: 250.54448699951172 and batch: 900, loss is 4.112615761756897 and perplexity is 61.10634834967512
At time: 250.9715301990509 and batch: 950, loss is 4.038873891830445 and perplexity is 56.76238621436842
At time: 251.4048683643341 and batch: 1000, loss is 4.002620205879212 and perplexity is 54.74139601187865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.640601181402439 and perplexity of 103.60661523457738
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 252.67578721046448 and batch: 50, loss is 4.148762912750244 and perplexity is 63.35557542436087
At time: 253.11610651016235 and batch: 100, loss is 4.038606796264649 and perplexity is 56.74722725724179
At time: 253.52279329299927 and batch: 150, loss is 4.083852233886719 and perplexity is 59.373751447835
At time: 253.94173502922058 and batch: 200, loss is 4.10860077381134 and perplexity is 60.86149896024461
At time: 254.368510723114 and batch: 250, loss is 4.079794278144837 and perplexity is 59.13330358557638
At time: 254.79187035560608 and batch: 300, loss is 4.000038228034973 and perplexity is 54.60023725302808
At time: 255.21455359458923 and batch: 350, loss is 4.078647961616516 and perplexity is 59.06555693927074
At time: 255.63396191596985 and batch: 400, loss is 3.9803083324432373 and perplexity is 53.53353780957348
At time: 256.04309248924255 and batch: 450, loss is 4.037210359573364 and perplexity is 56.66803865077729
At time: 256.45710372924805 and batch: 500, loss is 3.9767594814300535 and perplexity is 53.34389197113031
At time: 256.8888421058655 and batch: 550, loss is 4.023945193290711 and perplexity is 55.92129150773339
At time: 257.30900979042053 and batch: 600, loss is 4.06583261013031 and perplexity is 58.31344067700747
At time: 257.7452712059021 and batch: 650, loss is 4.018717045783997 and perplexity is 55.62968967985823
At time: 258.1632971763611 and batch: 700, loss is 3.988066473007202 and perplexity is 53.95047375338009
At time: 258.58796095848083 and batch: 750, loss is 3.9729532861709593 and perplexity is 53.14124061238148
At time: 259.00494170188904 and batch: 800, loss is 3.9657378339767457 and perplexity is 52.75918254962661
At time: 259.42734837532043 and batch: 850, loss is 3.956753206253052 and perplexity is 52.28728402612913
At time: 259.8497829437256 and batch: 900, loss is 4.086773529052734 and perplexity is 59.547453294579896
At time: 260.272257566452 and batch: 950, loss is 4.011787519454956 and perplexity is 55.24553482334678
At time: 260.6885106563568 and batch: 1000, loss is 3.9753763341903685 and perplexity is 53.27016051667985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.632946479611281 and perplexity of 102.8165651479813
Finished 28 epochs...
Completing Train Step...
At time: 261.9415338039398 and batch: 50, loss is 4.142632246017456 and perplexity is 62.968351688426786
At time: 262.3722438812256 and batch: 100, loss is 4.031763696670533 and perplexity is 56.36022598376645
At time: 262.79452419281006 and batch: 150, loss is 4.077324151992798 and perplexity is 58.987417119098694
At time: 263.24214005470276 and batch: 200, loss is 4.102564358711243 and perplexity is 60.49522030611957
At time: 263.6907889842987 and batch: 250, loss is 4.0740821838378904 and perplexity is 58.796491445959205
At time: 264.13701248168945 and batch: 300, loss is 3.994686508178711 and perplexity is 54.308812586307354
At time: 264.5595405101776 and batch: 350, loss is 4.074278092384338 and perplexity is 58.80801130951762
At time: 264.98481369018555 and batch: 400, loss is 3.97664587020874 and perplexity is 53.337831850669225
At time: 265.41633129119873 and batch: 450, loss is 4.033846893310547 and perplexity is 56.47775779559956
At time: 265.848920583725 and batch: 500, loss is 3.973417649269104 and perplexity is 53.16592317390332
At time: 266.2752254009247 and batch: 550, loss is 4.021097159385681 and perplexity is 55.76225235545639
At time: 266.6987702846527 and batch: 600, loss is 4.063319849967956 and perplexity is 58.16709692671813
At time: 267.1223850250244 and batch: 650, loss is 4.016648716926575 and perplexity is 55.514748096837025
At time: 267.5390684604645 and batch: 700, loss is 3.986268801689148 and perplexity is 53.85357565567854
At time: 267.9522786140442 and batch: 750, loss is 3.9718126392364503 and perplexity is 53.08065977642173
At time: 268.35785388946533 and batch: 800, loss is 3.965530986785889 and perplexity is 52.74827058951737
At time: 268.78052520751953 and batch: 850, loss is 3.957121934890747 and perplexity is 52.306567400084475
At time: 269.1950933933258 and batch: 900, loss is 4.08824535369873 and perplexity is 59.63516123346499
At time: 269.6183922290802 and batch: 950, loss is 4.013775668144226 and perplexity is 55.35548041887415
At time: 270.04067492485046 and batch: 1000, loss is 3.977551531791687 and perplexity is 53.38615975695482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6322877465224845 and perplexity of 102.74885877710365
Finished 29 epochs...
Completing Train Step...
At time: 271.32923674583435 and batch: 50, loss is 4.14051323890686 and perplexity is 62.835062573621975
At time: 271.7684404850006 and batch: 100, loss is 4.029405798912048 and perplexity is 56.22749088264742
At time: 272.1981062889099 and batch: 150, loss is 4.074690990447998 and perplexity is 58.832298037141356
At time: 272.622109413147 and batch: 200, loss is 4.1004440259933475 and perplexity is 60.36708620269964
At time: 273.06324005126953 and batch: 250, loss is 4.072112922668457 and perplexity is 58.68081972974645
At time: 273.4869170188904 and batch: 300, loss is 3.9928695487976076 and perplexity is 54.21022527149787
At time: 273.8949499130249 and batch: 350, loss is 4.072337808609009 and perplexity is 58.69401770504768
At time: 274.3323163986206 and batch: 400, loss is 3.9749168872833254 and perplexity is 53.24569132776979
At time: 274.76057839393616 and batch: 450, loss is 4.032329769134521 and perplexity is 56.39213898744417
At time: 275.1953537464142 and batch: 500, loss is 3.9718385410308836 and perplexity is 53.08203467856578
At time: 275.6099343299866 and batch: 550, loss is 4.019712662696838 and perplexity is 55.68510312046155
At time: 276.0285098552704 and batch: 600, loss is 4.062213263511658 and perplexity is 58.10276560570574
At time: 276.43516850471497 and batch: 650, loss is 4.015753664970398 and perplexity is 55.46508174325546
At time: 276.84837222099304 and batch: 700, loss is 3.9855200242996216 and perplexity is 53.81326640909198
At time: 277.2716908454895 and batch: 750, loss is 3.9712662172317503 and perplexity is 53.05166325878502
At time: 277.69828271865845 and batch: 800, loss is 3.965453929901123 and perplexity is 52.744206128708306
At time: 278.1211726665497 and batch: 850, loss is 3.95718976020813 and perplexity is 52.31011522993458
At time: 278.5668714046478 and batch: 900, loss is 4.088903722763061 and perplexity is 59.67443610602939
At time: 278.9776792526245 and batch: 950, loss is 4.014567432403564 and perplexity is 55.399326265324824
At time: 279.3964378833771 and batch: 1000, loss is 3.978404188156128 and perplexity is 53.431699217841185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.631879853039253 and perplexity of 102.7069567335652
Finished 30 epochs...
Completing Train Step...
At time: 280.6745777130127 and batch: 50, loss is 4.138902258872986 and perplexity is 62.73391803517931
At time: 281.09883642196655 and batch: 100, loss is 4.02757652759552 and perplexity is 56.12472956419687
At time: 281.5133786201477 and batch: 150, loss is 4.072695083618164 and perplexity is 58.71499135721839
At time: 281.92193055152893 and batch: 200, loss is 4.098979921340942 and perplexity is 60.27876714088951
At time: 282.3484683036804 and batch: 250, loss is 4.070747318267823 and perplexity is 58.60073963540026
At time: 282.76836705207825 and batch: 300, loss is 3.9916533470153808 and perplexity is 54.14433477509914
At time: 283.1883246898651 and batch: 350, loss is 4.07090353012085 and perplexity is 58.60989448055651
At time: 283.5999310016632 and batch: 400, loss is 3.9735466623306275 and perplexity is 53.1727827148964
At time: 284.0131125450134 and batch: 450, loss is 4.031169500350952 and perplexity is 56.326746892477615
At time: 284.44693636894226 and batch: 500, loss is 3.970642490386963 and perplexity is 53.01858382958631
At time: 284.867609500885 and batch: 550, loss is 4.0187475299835205 and perplexity is 55.631385532266094
At time: 285.31464099884033 and batch: 600, loss is 4.061408433914185 and perplexity is 58.0560215932568
At time: 285.7389931678772 and batch: 650, loss is 4.015053443908691 and perplexity is 55.42625751918556
At time: 286.1785683631897 and batch: 700, loss is 3.9849735641479493 and perplexity is 53.78386763672847
At time: 286.5903732776642 and batch: 750, loss is 3.9708183670043944 and perplexity is 53.02790937882026
At time: 287.01435470581055 and batch: 800, loss is 3.965356068611145 and perplexity is 52.73904476521066
At time: 287.440532207489 and batch: 850, loss is 3.9570726633071898 and perplexity is 52.303990236169234
At time: 287.86466336250305 and batch: 900, loss is 4.089149990081787 and perplexity is 59.689133779109696
At time: 288.274126291275 and batch: 950, loss is 4.014894442558289 and perplexity is 55.41744536998263
At time: 288.6879026889801 and batch: 1000, loss is 3.9787584018707274 and perplexity is 53.45062881086131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.631589191715892 and perplexity of 102.67710813172984
Finished 31 epochs...
Completing Train Step...
At time: 290.0123293399811 and batch: 50, loss is 4.137521419525147 and perplexity is 62.64735235311687
At time: 290.4212603569031 and batch: 100, loss is 4.025993165969848 and perplexity is 56.035934137340526
At time: 290.85234665870667 and batch: 150, loss is 4.071020293235779 and perplexity is 58.61673835394993
At time: 291.2867314815521 and batch: 200, loss is 4.09779586315155 and perplexity is 60.20743581156603
At time: 291.70386815071106 and batch: 250, loss is 4.069607105255127 and perplexity is 58.53396038803268
At time: 292.111718416214 and batch: 300, loss is 3.990544943809509 and perplexity is 54.08435426828844
At time: 292.5576374530792 and batch: 350, loss is 4.069706201553345 and perplexity is 58.539761174240645
At time: 292.9750165939331 and batch: 400, loss is 3.9723566818237304 and perplexity is 53.10954577279465
At time: 293.3975713253021 and batch: 450, loss is 4.030291538238526 and perplexity is 56.277315845258315
At time: 293.8222997188568 and batch: 500, loss is 3.96966739654541 and perplexity is 52.96691093206257
At time: 294.24849462509155 and batch: 550, loss is 4.017829141616821 and perplexity is 55.58031776857931
At time: 294.67743945121765 and batch: 600, loss is 4.060697741508484 and perplexity is 58.014776277707576
At time: 295.1011040210724 and batch: 650, loss is 4.014382076263428 and perplexity is 55.389058611657866
At time: 295.5170316696167 and batch: 700, loss is 3.984447741508484 and perplexity is 53.755594295519955
At time: 295.95259261131287 and batch: 750, loss is 3.9704093313217164 and perplexity is 53.00622350715653
At time: 296.3844244480133 and batch: 800, loss is 3.9652266693115235 and perplexity is 52.73222081127228
At time: 296.80896759033203 and batch: 850, loss is 3.956822910308838 and perplexity is 52.29092878891768
At time: 297.423232793808 and batch: 900, loss is 4.089189043045044 and perplexity is 59.691464862175565
At time: 297.8393123149872 and batch: 950, loss is 4.014994831085205 and perplexity is 55.42300892494266
At time: 298.2662672996521 and batch: 1000, loss is 3.9788886880874634 and perplexity is 53.45759314473975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.631367381026105 and perplexity of 102.65433577721899
Finished 32 epochs...
Completing Train Step...
At time: 299.6154692173004 and batch: 50, loss is 4.13631649017334 and perplexity is 62.571912178621254
At time: 300.03552770614624 and batch: 100, loss is 4.024579505920411 and perplexity is 55.956774341620985
At time: 300.44745111465454 and batch: 150, loss is 4.069553265571594 and perplexity is 58.53080902296448
At time: 300.8739449977875 and batch: 200, loss is 4.0967584896087645 and perplexity is 60.14501059530825
At time: 301.312091588974 and batch: 250, loss is 4.068608622550965 and perplexity is 58.475544409504465
At time: 301.7350513935089 and batch: 300, loss is 3.9896082735061644 and perplexity is 54.03371877785195
At time: 302.15316438674927 and batch: 350, loss is 4.068631706237793 and perplexity is 58.47689425623836
At time: 302.5720548629761 and batch: 400, loss is 3.9712998819351197 and perplexity is 53.05344925735427
At time: 302.98944330215454 and batch: 450, loss is 4.029433951377869 and perplexity is 56.229073847444766
At time: 303.400865316391 and batch: 500, loss is 3.968690619468689 and perplexity is 52.91519932710602
At time: 303.826229095459 and batch: 550, loss is 4.017018437385559 and perplexity is 55.535276829696194
At time: 304.24337339401245 and batch: 600, loss is 4.060003571510315 and perplexity is 57.974518135179046
At time: 304.6557376384735 and batch: 650, loss is 4.013740363121033 and perplexity is 55.35352612685247
At time: 305.06564569473267 and batch: 700, loss is 3.9839521074295043 and perplexity is 53.72895779257588
At time: 305.4938530921936 and batch: 750, loss is 3.970023341178894 and perplexity is 52.98576757552248
At time: 305.9112060070038 and batch: 800, loss is 3.964848279953003 and perplexity is 52.71227127465035
At time: 306.3307452201843 and batch: 850, loss is 3.956460452079773 and perplexity is 52.27197894594342
At time: 306.7417628765106 and batch: 900, loss is 4.089081678390503 and perplexity is 59.68505645269608
At time: 307.1652801036835 and batch: 950, loss is 4.01499270439148 and perplexity is 55.42289105730268
At time: 307.5888876914978 and batch: 1000, loss is 3.9788959646224975 and perplexity is 53.45798213220434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.631212560141959 and perplexity of 102.63844397241596
Finished 33 epochs...
Completing Train Step...
At time: 308.9031386375427 and batch: 50, loss is 4.135250058174133 and perplexity is 62.50521905738243
At time: 309.3279013633728 and batch: 100, loss is 4.0233155679702755 and perplexity is 55.88609312870681
At time: 309.75025391578674 and batch: 150, loss is 4.068231558799743 and perplexity is 58.45349955779313
At time: 310.1639349460602 and batch: 200, loss is 4.095830883979797 and perplexity is 60.089245612880134
At time: 310.5743250846863 and batch: 250, loss is 4.0677483892440796 and perplexity is 58.425263428353176
At time: 311.0051486492157 and batch: 300, loss is 3.9888056993484495 and perplexity is 53.9903701091031
At time: 311.4183909893036 and batch: 350, loss is 4.06765576839447 and perplexity is 58.41985228141217
At time: 311.84653878211975 and batch: 400, loss is 3.9703235149383547 and perplexity is 53.001674899934784
At time: 312.2797155380249 and batch: 450, loss is 4.028652710914612 and perplexity is 56.18516257460862
At time: 312.71043038368225 and batch: 500, loss is 3.9677960681915283 and perplexity is 52.86788513360802
At time: 313.1429657936096 and batch: 550, loss is 4.016259231567383 and perplexity is 55.49313012544903
At time: 313.57240748405457 and batch: 600, loss is 4.059342961311341 and perplexity is 57.93623222464219
At time: 314.00000834465027 and batch: 650, loss is 4.01311824798584 and perplexity is 55.31910056990298
At time: 314.440630197525 and batch: 700, loss is 3.9834552431106567 and perplexity is 53.70226842160702
At time: 314.85858821868896 and batch: 750, loss is 3.969526586532593 and perplexity is 52.95945318573033
At time: 315.2993006706238 and batch: 800, loss is 3.9645750617980955 and perplexity is 52.69787129240957
At time: 315.73405385017395 and batch: 850, loss is 3.9560675954818727 and perplexity is 52.251447587333104
At time: 316.15510606765747 and batch: 900, loss is 4.088877243995666 and perplexity is 59.67285602143576
At time: 316.5935695171356 and batch: 950, loss is 4.014863181114197 and perplexity is 55.41571296769131
At time: 317.0225384235382 and batch: 1000, loss is 3.978808717727661 and perplexity is 53.45331829271481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.631048062952553 and perplexity of 102.62156162544487
Finished 34 epochs...
Completing Train Step...
At time: 318.4055299758911 and batch: 50, loss is 4.134265818595886 and perplexity is 62.443729212272686
At time: 318.83170890808105 and batch: 100, loss is 4.022111458778381 and perplexity is 55.818840668050164
At time: 319.25122594833374 and batch: 150, loss is 4.066977252960205 and perplexity is 58.38022695468988
At time: 319.6600890159607 and batch: 200, loss is 4.094945993423462 and perplexity is 60.036096725842185
At time: 320.08304476737976 and batch: 250, loss is 4.066926651000976 and perplexity is 58.37727287556746
At time: 320.4946060180664 and batch: 300, loss is 3.9880028533935548 and perplexity is 53.947041554262555
At time: 320.90970611572266 and batch: 350, loss is 4.066716227531433 and perplexity is 58.36499021959139
At time: 321.3200213909149 and batch: 400, loss is 3.9694013881683348 and perplexity is 52.95282316386208
At time: 321.7422180175781 and batch: 450, loss is 4.027898144721985 and perplexity is 56.14278314145714
At time: 322.1705198287964 and batch: 500, loss is 3.9669371318817137 and perplexity is 52.82249448407283
At time: 322.59193205833435 and batch: 550, loss is 4.015517873764038 and perplexity is 55.45200510645893
At time: 323.00785875320435 and batch: 600, loss is 4.058685450553894 and perplexity is 57.89815104947167
At time: 323.4226701259613 and batch: 650, loss is 4.012511019706726 and perplexity is 55.28551944439814
At time: 323.8506329059601 and batch: 700, loss is 3.9829769229888914 and perplexity is 53.676587688331885
At time: 324.2643060684204 and batch: 750, loss is 3.969112153053284 and perplexity is 52.93750956268436
At time: 324.68749594688416 and batch: 800, loss is 3.9643059825897216 and perplexity is 52.683693298506455
At time: 325.1162893772125 and batch: 850, loss is 3.9556336545944215 and perplexity is 52.22877846668134
At time: 325.5445098876953 and batch: 900, loss is 4.0885959911346434 and perplexity is 59.65607521988916
At time: 325.9930772781372 and batch: 950, loss is 4.014671974182129 and perplexity is 55.40511811216359
At time: 326.4202239513397 and batch: 1000, loss is 3.9786481428146363 and perplexity is 53.44473571987045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630908873023056 and perplexity of 102.60727873155793
Finished 35 epochs...
Completing Train Step...
At time: 327.7119734287262 and batch: 50, loss is 4.133369550704956 and perplexity is 62.38778797570164
At time: 328.14537477493286 and batch: 100, loss is 4.020966153144837 and perplexity is 55.75494763088701
At time: 328.57290625572205 and batch: 150, loss is 4.065795102119446 and perplexity is 58.31125349685967
At time: 329.0005922317505 and batch: 200, loss is 4.094114098548889 and perplexity is 59.986173772890055
At time: 329.43296217918396 and batch: 250, loss is 4.066148724555969 and perplexity is 58.33187731070469
At time: 329.8539969921112 and batch: 300, loss is 3.987243642807007 and perplexity is 53.906099932824404
At time: 330.27767848968506 and batch: 350, loss is 4.065824165344238 and perplexity is 58.31294823455511
At time: 330.68596959114075 and batch: 400, loss is 3.968526005744934 and perplexity is 52.906489476001745
At time: 331.11086797714233 and batch: 450, loss is 4.027170281410218 and perplexity is 56.101933737582605
At time: 331.52805256843567 and batch: 500, loss is 3.9661137580871584 and perplexity is 52.77901972679565
At time: 331.9409968852997 and batch: 550, loss is 4.0148078012466435 and perplexity is 55.412644137823285
At time: 332.35081362724304 and batch: 600, loss is 4.058042454719543 and perplexity is 57.860934745776596
At time: 332.78939056396484 and batch: 650, loss is 4.011910419464112 and perplexity is 55.25232491732512
At time: 333.20901250839233 and batch: 700, loss is 3.98250159740448 and perplexity is 53.65107989565322
At time: 333.62149834632874 and batch: 750, loss is 3.9686911869049073 and perplexity is 52.91522935311515
At time: 334.04123759269714 and batch: 800, loss is 3.9639963340759277 and perplexity is 52.66738239662899
At time: 334.453307390213 and batch: 850, loss is 3.955172266960144 and perplexity is 52.2046863124818
At time: 334.8661732673645 and batch: 900, loss is 4.088275771141053 and perplexity is 59.63697521013031
At time: 335.27147817611694 and batch: 950, loss is 4.014446163177491 and perplexity is 55.392608439244654
At time: 335.6996295452118 and batch: 1000, loss is 3.9784585666656493 and perplexity is 53.43460483300665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6307830810546875 and perplexity of 102.59437237177254
Finished 36 epochs...
Completing Train Step...
At time: 337.0343747138977 and batch: 50, loss is 4.132503147125244 and perplexity is 62.33375838196569
At time: 337.46004843711853 and batch: 100, loss is 4.019888949394226 and perplexity is 55.69492052869819
At time: 337.87708806991577 and batch: 150, loss is 4.064665927886963 and perplexity is 58.245447092387465
At time: 338.3023326396942 and batch: 200, loss is 4.093307409286499 and perplexity is 59.93780308329664
At time: 338.7102255821228 and batch: 250, loss is 4.065397763252259 and perplexity is 58.28808877187852
At time: 339.13549733161926 and batch: 300, loss is 3.9865081834793092 and perplexity is 53.86646876415171
At time: 339.55068707466125 and batch: 350, loss is 4.064959387779236 and perplexity is 58.26254230327332
At time: 339.95657300949097 and batch: 400, loss is 3.9676756191253664 and perplexity is 52.861517629701375
At time: 340.37200236320496 and batch: 450, loss is 4.026460957527161 and perplexity is 56.0621534063343
At time: 340.78914761543274 and batch: 500, loss is 3.9653115177154543 and perplexity is 52.73669524586548
At time: 341.22157526016235 and batch: 550, loss is 4.014118247032165 and perplexity is 55.374447286436094
At time: 341.6450164318085 and batch: 600, loss is 4.057402892112732 and perplexity is 57.823940886723506
At time: 342.06800508499146 and batch: 650, loss is 4.011308040618896 and perplexity is 55.21905210807028
At time: 342.49860548973083 and batch: 700, loss is 3.982024602890015 and perplexity is 53.62549472732542
At time: 342.9295403957367 and batch: 750, loss is 3.968259181976318 and perplexity is 52.89237465026476
At time: 343.3414444923401 and batch: 800, loss is 3.963535718917847 and perplexity is 52.64312858822538
At time: 343.77669954299927 and batch: 850, loss is 3.954692325592041 and perplexity is 52.179637135460574
At time: 344.19311475753784 and batch: 900, loss is 4.08793493270874 and perplexity is 59.61665210063697
At time: 344.60256600379944 and batch: 950, loss is 4.014215383529663 and perplexity is 55.37982642754762
At time: 345.0298750400543 and batch: 1000, loss is 3.9782705879211426 and perplexity is 53.424561207100595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630685573670922 and perplexity of 102.58436915063538
Finished 37 epochs...
Completing Train Step...
At time: 346.36221385002136 and batch: 50, loss is 4.131695499420166 and perplexity is 62.28343498958126
At time: 346.80461502075195 and batch: 100, loss is 4.018872599601746 and perplexity is 55.63834376353857
At time: 347.225928068161 and batch: 150, loss is 4.063588132858277 and perplexity is 58.1827042570994
At time: 347.6643624305725 and batch: 200, loss is 4.092537159919739 and perplexity is 59.89165380393497
At time: 348.0750229358673 and batch: 250, loss is 4.064681496620178 and perplexity is 58.246353907273225
At time: 348.50384187698364 and batch: 300, loss is 3.985804891586304 and perplexity is 53.82859823194515
At time: 348.9307060241699 and batch: 350, loss is 4.064122009277344 and perplexity is 58.21377492411005
At time: 349.35760855674744 and batch: 400, loss is 3.9668464612960816 and perplexity is 52.81770525468781
At time: 349.7886254787445 and batch: 450, loss is 4.025769371986389 and perplexity is 56.023395035563155
At time: 350.2188048362732 and batch: 500, loss is 3.964541163444519 and perplexity is 52.696084951612946
At time: 350.64852380752563 and batch: 550, loss is 4.013454971313476 and perplexity is 55.33773093799244
At time: 351.07134222984314 and batch: 600, loss is 4.056768989562988 and perplexity is 57.78729775847242
At time: 351.5074200630188 and batch: 650, loss is 4.010697083473206 and perplexity is 55.185325937278186
At time: 351.9361481666565 and batch: 700, loss is 3.9815359210968015 and perplexity is 53.5992953265066
At time: 352.3707058429718 and batch: 750, loss is 3.9677434539794922 and perplexity is 52.86510360466433
At time: 352.78775668144226 and batch: 800, loss is 3.9632175016403197 and perplexity is 52.62637930026327
At time: 353.2221305370331 and batch: 850, loss is 3.9542034912109374 and perplexity is 52.154136168217825
At time: 353.6480257511139 and batch: 900, loss is 4.0875574541091915 and perplexity is 59.59415233714901
At time: 354.08061623573303 and batch: 950, loss is 4.013955750465393 and perplexity is 55.3654498599104
At time: 354.510306596756 and batch: 1000, loss is 3.978009648323059 and perplexity is 53.410622442238356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630586577624809 and perplexity of 102.57421420635447
Finished 38 epochs...
Completing Train Step...
At time: 355.89521861076355 and batch: 50, loss is 4.130908989906311 and perplexity is 62.23446773453663
At time: 356.34283232688904 and batch: 100, loss is 4.01788179397583 and perplexity is 55.58324428046749
At time: 356.7705626487732 and batch: 150, loss is 4.0625364112854 and perplexity is 58.12154441906195
At time: 357.2012197971344 and batch: 200, loss is 4.091775054931641 and perplexity is 59.84602746406448
At time: 357.62992906570435 and batch: 250, loss is 4.063961758613586 and perplexity is 58.20444687547159
At time: 358.0612895488739 and batch: 300, loss is 3.985077528953552 and perplexity is 53.78945955674829
At time: 358.49572491645813 and batch: 350, loss is 4.063304624557495 and perplexity is 58.16621131553399
At time: 358.95197677612305 and batch: 400, loss is 3.966039056777954 and perplexity is 52.77507721218088
At time: 359.383757352829 and batch: 450, loss is 4.025104899406433 and perplexity is 55.98618139081847
At time: 359.81000685691833 and batch: 500, loss is 3.9637811899185182 and perplexity is 52.656052535840686
At time: 360.22361421585083 and batch: 550, loss is 4.01278247833252 and perplexity is 55.30052921270619
At time: 360.6540484428406 and batch: 600, loss is 4.056127700805664 and perplexity is 57.75025129411395
At time: 361.0656282901764 and batch: 650, loss is 4.0101059579849245 and perplexity is 55.15271412432411
At time: 361.48122239112854 and batch: 700, loss is 3.9810683631896975 and perplexity is 53.574240409930105
At time: 361.8927798271179 and batch: 750, loss is 3.96731201171875 and perplexity is 52.842300284362516
At time: 362.31178522109985 and batch: 800, loss is 3.9627497816085815 and perplexity is 52.601770643896074
At time: 362.7249641418457 and batch: 850, loss is 3.953710842132568 and perplexity is 52.12844880904785
At time: 363.158056974411 and batch: 900, loss is 4.087160258293152 and perplexity is 59.57048648947926
At time: 363.58733105659485 and batch: 950, loss is 4.013662066459656 and perplexity is 55.34919230022556
At time: 364.0201063156128 and batch: 1000, loss is 3.977723021507263 and perplexity is 53.395315719361484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630501351705411 and perplexity of 102.56547259715332
Finished 39 epochs...
Completing Train Step...
At time: 365.40972566604614 and batch: 50, loss is 4.13019742488861 and perplexity is 62.19019961609157
At time: 365.8460717201233 and batch: 100, loss is 4.016933531761169 and perplexity is 55.530561772511575
At time: 366.2709381580353 and batch: 150, loss is 4.061532015800476 and perplexity is 58.06319670926393
At time: 366.6986565589905 and batch: 200, loss is 4.0910505676269535 and perplexity is 59.802685479185484
At time: 367.11927223205566 and batch: 250, loss is 4.063267459869385 and perplexity is 58.1640496266014
At time: 367.5285716056824 and batch: 300, loss is 3.9843982648849487 and perplexity is 53.75293471601214
At time: 367.9501836299896 and batch: 350, loss is 4.062528109550476 and perplexity is 58.12106191140962
At time: 368.3782866001129 and batch: 400, loss is 3.9652561235427854 and perplexity is 52.7337740211732
At time: 368.82031178474426 and batch: 450, loss is 4.0244449567794796 and perplexity is 55.94924591218726
At time: 369.2453420162201 and batch: 500, loss is 3.96305100440979 and perplexity is 52.61761788325304
At time: 369.67944979667664 and batch: 550, loss is 4.012136564254761 and perplexity is 55.2648213557254
At time: 370.09844160079956 and batch: 600, loss is 4.055507373809815 and perplexity is 57.71443836323145
At time: 370.5293815135956 and batch: 650, loss is 4.009526453018188 and perplexity is 55.12076211162931
At time: 370.9591763019562 and batch: 700, loss is 3.9805907344818117 and perplexity is 53.54865792465818
At time: 371.4047517776489 and batch: 750, loss is 3.9668111419677734 and perplexity is 52.8158398017589
At time: 371.82627964019775 and batch: 800, loss is 3.962396025657654 and perplexity is 52.583165745492245
At time: 372.2398910522461 and batch: 850, loss is 3.953199391365051 and perplexity is 52.10179449066182
At time: 372.6709713935852 and batch: 900, loss is 4.086736779212952 and perplexity is 59.545264975422334
At time: 373.0907173156738 and batch: 950, loss is 4.013344297409057 and perplexity is 55.33160683414335
At time: 373.51908016204834 and batch: 1000, loss is 3.977413492202759 and perplexity is 53.37879086201882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630389702029344 and perplexity of 102.55402183461104
Finished 40 epochs...
Completing Train Step...
At time: 374.8010699748993 and batch: 50, loss is 4.129475245475769 and perplexity is 62.145303347779944
At time: 375.24899888038635 and batch: 100, loss is 4.0159978103637695 and perplexity is 55.4786249406434
At time: 375.6630654335022 and batch: 150, loss is 4.060535397529602 and perplexity is 58.0053586925575
At time: 376.08616638183594 and batch: 200, loss is 4.09033387184143 and perplexity is 59.759840501780914
At time: 376.50735545158386 and batch: 250, loss is 4.062566084861755 and perplexity is 58.12326911873703
At time: 376.9278771877289 and batch: 300, loss is 3.983717670440674 and perplexity is 53.71636321387324
At time: 377.3392610549927 and batch: 350, loss is 4.06175657749176 and perplexity is 58.07623694303199
At time: 377.7557508945465 and batch: 400, loss is 3.964470176696777 and perplexity is 52.69234436069127
At time: 378.1739311218262 and batch: 450, loss is 4.023794288635254 and perplexity is 55.912853361199524
At time: 378.5883300304413 and batch: 500, loss is 3.962311797142029 and perplexity is 52.57873693001354
At time: 379.0283341407776 and batch: 550, loss is 4.011474080085755 and perplexity is 55.228221411254765
At time: 379.4557135105133 and batch: 600, loss is 4.054872155189514 and perplexity is 57.67778871882357
At time: 379.8837854862213 and batch: 650, loss is 4.008936338424682 and perplexity is 55.088244141110195
At time: 380.30822706222534 and batch: 700, loss is 3.980115532875061 and perplexity is 53.523217561502506
At time: 380.7756724357605 and batch: 750, loss is 3.9663484144210814 and perplexity is 52.79140611128775
At time: 381.200891494751 and batch: 800, loss is 3.962001852989197 and perplexity is 52.562442983176005
At time: 381.63024616241455 and batch: 850, loss is 3.9527202224731446 and perplexity is 52.076834911933794
At time: 382.0568675994873 and batch: 900, loss is 4.086314344406128 and perplexity is 59.520116295128105
At time: 382.4939775466919 and batch: 950, loss is 4.013037633895874 and perplexity is 55.314641250697186
At time: 382.9155180454254 and batch: 1000, loss is 3.9770871782302857 and perplexity is 53.36137545832613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630300010122904 and perplexity of 102.54482398137229
Finished 41 epochs...
Completing Train Step...
At time: 384.3034715652466 and batch: 50, loss is 4.12876871585846 and perplexity is 62.101411357709395
At time: 384.7563805580139 and batch: 100, loss is 4.015083556175232 and perplexity is 55.427926554554425
At time: 385.1849994659424 and batch: 150, loss is 4.05956476688385 and perplexity is 57.949084229070195
At time: 385.6125056743622 and batch: 200, loss is 4.089640216827393 and perplexity is 59.71840216239623
At time: 386.0449333190918 and batch: 250, loss is 4.061889729499817 and perplexity is 58.08397042545426
At time: 386.48144364356995 and batch: 300, loss is 3.983055601119995 and perplexity is 53.680811028075276
At time: 386.91105580329895 and batch: 350, loss is 4.060997385978698 and perplexity is 58.03216268935087
At time: 387.3391373157501 and batch: 400, loss is 3.9636952018737794 and perplexity is 52.65152493950183
At time: 387.77218866348267 and batch: 450, loss is 4.0231453323364255 and perplexity is 55.87658013396796
At time: 388.204119682312 and batch: 500, loss is 3.9616107034683226 and perplexity is 52.54188722923588
At time: 388.62537002563477 and batch: 550, loss is 4.010821199417114 and perplexity is 55.19217574117292
At time: 389.0580823421478 and batch: 600, loss is 4.054199271202087 and perplexity is 57.63899131290494
At time: 389.48927092552185 and batch: 650, loss is 4.008322916030884 and perplexity is 55.05446214089603
At time: 389.91709303855896 and batch: 700, loss is 3.9796245908737182 and perplexity is 53.49694721509042
At time: 390.34595465660095 and batch: 750, loss is 3.9658138513565064 and perplexity is 52.763193316884426
At time: 390.77035665512085 and batch: 800, loss is 3.9616289472579957 and perplexity is 52.5428458011195
At time: 391.1973886489868 and batch: 850, loss is 3.9523570251464846 and perplexity is 52.05792417908455
At time: 391.6627285480499 and batch: 900, loss is 4.085918936729431 and perplexity is 59.49658623651659
At time: 392.08930563926697 and batch: 950, loss is 4.012715249061585 and perplexity is 55.29681152341506
At time: 392.52862310409546 and batch: 1000, loss is 3.9766619300842283 and perplexity is 53.338688456486025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630205480063834 and perplexity of 102.53513087125634
Finished 42 epochs...
Completing Train Step...
At time: 393.82645297050476 and batch: 50, loss is 4.128046474456787 and perplexity is 62.05657534048751
At time: 394.2540464401245 and batch: 100, loss is 4.014146370887756 and perplexity is 55.37600465129446
At time: 394.6859722137451 and batch: 150, loss is 4.058545446395874 and perplexity is 57.89004563499215
At time: 395.1035144329071 and batch: 200, loss is 4.088835444450378 and perplexity is 59.670361775317566
At time: 395.51565384864807 and batch: 250, loss is 4.061246228218079 and perplexity is 58.04660533956676
At time: 395.9276351928711 and batch: 300, loss is 3.9822377252578733 and perplexity is 53.636924737691466
At time: 396.34766840934753 and batch: 350, loss is 4.06012158870697 and perplexity is 57.98136052903893
At time: 396.7736346721649 and batch: 400, loss is 3.9627167701721193 and perplexity is 52.60003421254787
At time: 397.197758436203 and batch: 450, loss is 4.022500824928284 and perplexity is 55.84057886691787
At time: 397.6054003238678 and batch: 500, loss is 3.9608360528945923 and perplexity is 52.50120138683904
At time: 398.01962661743164 and batch: 550, loss is 4.010165767669678 and perplexity is 55.15601288941724
At time: 398.42987751960754 and batch: 600, loss is 4.053354630470276 and perplexity is 57.590327627660024
At time: 398.8516435623169 and batch: 650, loss is 4.007698245048523 and perplexity is 55.02008195521729
At time: 399.286883354187 and batch: 700, loss is 3.9791591358184815 and perplexity is 53.47205258468466
At time: 399.71021032333374 and batch: 750, loss is 3.9652266693115235 and perplexity is 52.73222081127228
At time: 400.13061571121216 and batch: 800, loss is 3.961185255050659 and perplexity is 52.519538120986645
At time: 400.54129910469055 and batch: 850, loss is 3.9518384408950804 and perplexity is 52.03093475819365
At time: 400.9613561630249 and batch: 900, loss is 4.085536890029907 and perplexity is 59.47386010361042
At time: 401.372318983078 and batch: 950, loss is 4.012318811416626 and perplexity is 55.27489413040787
At time: 401.81042194366455 and batch: 1000, loss is 3.976130986213684 and perplexity is 53.3103761235821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630329783369855 and perplexity of 102.54787711919087
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 403.1097810268402 and batch: 50, loss is 4.128031840324402 and perplexity is 62.05566720299354
At time: 403.55214500427246 and batch: 100, loss is 4.014238319396973 and perplexity is 55.381096626464675
At time: 403.96957635879517 and batch: 150, loss is 4.058333849906921 and perplexity is 57.877797600456724
At time: 404.40792179107666 and batch: 200, loss is 4.088730888366699 and perplexity is 59.6641232021247
At time: 404.8262286186218 and batch: 250, loss is 4.060779891014099 and perplexity is 58.01954235866007
At time: 405.23892307281494 and batch: 300, loss is 3.981598386764526 and perplexity is 53.60264354685206
At time: 405.66352796554565 and batch: 350, loss is 4.058562588691712 and perplexity is 57.89103801178628
At time: 406.0747377872467 and batch: 400, loss is 3.9609952068328855 and perplexity is 52.50955782476725
At time: 406.51263308525085 and batch: 450, loss is 4.020005021095276 and perplexity is 55.7013855080571
At time: 406.9322190284729 and batch: 500, loss is 3.9582858324050902 and perplexity is 52.367482326354335
At time: 407.35437774658203 and batch: 550, loss is 4.007250113487244 and perplexity is 54.9954312437824
At time: 407.78314685821533 and batch: 600, loss is 4.049316687583923 and perplexity is 57.35825004723568
At time: 408.20494651794434 and batch: 650, loss is 4.003699369430542 and perplexity is 54.80050281842296
At time: 408.63493514060974 and batch: 700, loss is 3.9753484392166136 and perplexity is 53.268674567675674
At time: 409.06352496147156 and batch: 750, loss is 3.9608505964279175 and perplexity is 52.501964945363426
At time: 409.4912865161896 and batch: 800, loss is 3.9556549119949342 and perplexity is 52.229888726544075
At time: 409.90523195266724 and batch: 850, loss is 3.9454810571670533 and perplexity is 51.70120336524754
At time: 410.3271415233612 and batch: 900, loss is 4.078736486434937 and perplexity is 59.07078593841838
At time: 410.73836755752563 and batch: 950, loss is 4.005675048828125 and perplexity is 54.90887806493752
At time: 411.15809297561646 and batch: 1000, loss is 3.969328441619873 and perplexity is 52.9489605790638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.628838515863186 and perplexity of 102.39506477252843
Finished 44 epochs...
Completing Train Step...
At time: 412.4528651237488 and batch: 50, loss is 4.126393971443176 and perplexity is 61.95411134706902
At time: 412.8852496147156 and batch: 100, loss is 4.013011302947998 and perplexity is 55.313184782936794
At time: 413.3074080944061 and batch: 150, loss is 4.05717511177063 and perplexity is 57.81077122963809
At time: 413.74501943588257 and batch: 200, loss is 4.087492480278015 and perplexity is 59.590280402544515
At time: 414.1545042991638 and batch: 250, loss is 4.0594723558425905 and perplexity is 57.94372934128576
At time: 414.5679771900177 and batch: 300, loss is 3.980663733482361 and perplexity is 53.5525670658474
At time: 414.97394466400146 and batch: 350, loss is 4.057763409614563 and perplexity is 57.8447911876614
At time: 415.3782708644867 and batch: 400, loss is 3.9602060890197754 and perplexity is 52.46813794206219
At time: 415.79030776023865 and batch: 450, loss is 4.019399347305298 and perplexity is 55.667658853495794
At time: 416.19932675361633 and batch: 500, loss is 3.9577964973449706 and perplexity is 52.341863349884754
At time: 416.629860162735 and batch: 550, loss is 4.006730723381042 and perplexity is 54.96687457757502
At time: 417.04983949661255 and batch: 600, loss is 4.049087357521057 and perplexity is 57.34509758433168
At time: 417.46413397789 and batch: 650, loss is 4.0033929681777956 and perplexity is 54.78371444782849
At time: 417.8687472343445 and batch: 700, loss is 3.974931869506836 and perplexity is 53.24648907259422
At time: 418.27616333961487 and batch: 750, loss is 3.9606436920166015 and perplexity is 52.49110318092571
At time: 418.699533700943 and batch: 800, loss is 3.955796537399292 and perplexity is 52.23728632948642
At time: 419.1288287639618 and batch: 850, loss is 3.9457180309295654 and perplexity is 51.71345664573105
At time: 419.5528519153595 and batch: 900, loss is 4.079192090034485 and perplexity is 59.09770493284978
At time: 419.97236728668213 and batch: 950, loss is 4.006260857582093 and perplexity is 54.94105358981044
At time: 420.4007737636566 and batch: 1000, loss is 3.96988329410553 and perplexity is 52.9783475934291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.628702675423971 and perplexity of 102.38115632664233
Finished 45 epochs...
Completing Train Step...
At time: 421.7362153530121 and batch: 50, loss is 4.125832800865173 and perplexity is 61.91935427582975
At time: 422.160649061203 and batch: 100, loss is 4.012467532157898 and perplexity is 55.28311526494815
At time: 422.5780749320984 and batch: 150, loss is 4.0565913438797 and perplexity is 57.7770330062482
At time: 422.99813175201416 and batch: 200, loss is 4.087029085159302 and perplexity is 59.562672954555595
At time: 423.41033363342285 and batch: 250, loss is 4.058846530914306 and perplexity is 57.9074780556885
At time: 423.83085346221924 and batch: 300, loss is 3.9801287269592285 and perplexity is 53.52392375599871
At time: 424.2860622406006 and batch: 350, loss is 4.057338519096374 and perplexity is 57.820218705036275
At time: 424.7092173099518 and batch: 400, loss is 3.9598416328430175 and perplexity is 52.44901908930983
At time: 425.135311126709 and batch: 450, loss is 4.019064364433288 and perplexity is 55.64901426423835
At time: 425.5605239868164 and batch: 500, loss is 3.9575475740432737 and perplexity is 52.32883586193265
At time: 425.9735417366028 and batch: 550, loss is 4.006489968299865 and perplexity is 54.95364261611895
At time: 426.38343930244446 and batch: 600, loss is 4.048993535041809 and perplexity is 57.33971757749034
At time: 426.8041214942932 and batch: 650, loss is 4.003224639892578 and perplexity is 54.774493575206264
At time: 427.21430134773254 and batch: 700, loss is 3.9747440433502197 and perplexity is 53.23648892837213
At time: 427.63598465919495 and batch: 750, loss is 3.9605555582046508 and perplexity is 52.48647714376702
At time: 428.0515809059143 and batch: 800, loss is 3.9558645153045653 and perplexity is 52.24083743148513
At time: 428.47866129875183 and batch: 850, loss is 3.945835657119751 and perplexity is 51.71953986038327
At time: 428.903639793396 and batch: 900, loss is 4.079457039833069 and perplexity is 59.11336493233378
At time: 429.32991456985474 and batch: 950, loss is 4.006589522361756 and perplexity is 54.959113746789164
At time: 429.7401258945465 and batch: 1000, loss is 3.970200662612915 and perplexity is 52.99516392087377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.628652433069741 and perplexity of 102.37601258553761
Finished 46 epochs...
Completing Train Step...
At time: 431.047593832016 and batch: 50, loss is 4.125428586006165 and perplexity is 61.894330610580624
At time: 431.47702836990356 and batch: 100, loss is 4.012065978050232 and perplexity is 55.26092055941473
At time: 431.8880341053009 and batch: 150, loss is 4.056150803565979 and perplexity is 57.75158549973957
At time: 432.32666516304016 and batch: 200, loss is 4.0867041397094725 and perplexity is 59.54332147925653
At time: 432.74922275543213 and batch: 250, loss is 4.058403248786926 and perplexity is 57.881814394166256
At time: 433.18327498435974 and batch: 300, loss is 3.97974338054657 and perplexity is 53.503302477410955
At time: 433.6031379699707 and batch: 350, loss is 4.057031326293945 and perplexity is 57.80245947790822
At time: 434.02077865600586 and batch: 400, loss is 3.959589982032776 and perplexity is 52.43582191176948
At time: 434.4303617477417 and batch: 450, loss is 4.018797273635864 and perplexity is 55.634152909396576
At time: 434.8619019985199 and batch: 500, loss is 3.957368335723877 and perplexity is 52.319457369854454
At time: 435.31945538520813 and batch: 550, loss is 4.006321377754212 and perplexity is 54.944378732448186
At time: 435.7457022666931 and batch: 600, loss is 4.048917779922485 and perplexity is 57.33537396487076
At time: 436.16777324676514 and batch: 650, loss is 4.003084063529968 and perplexity is 54.76679411732922
At time: 436.57893800735474 and batch: 700, loss is 3.9746177864074705 and perplexity is 53.229767876336005
At time: 437.0058307647705 and batch: 750, loss is 3.9604803276062013 and perplexity is 52.48252870320462
At time: 437.427818775177 and batch: 800, loss is 3.9558859920501708 and perplexity is 52.241959406709015
At time: 437.84950065612793 and batch: 850, loss is 3.9458843898773193 and perplexity is 51.72206035759573
At time: 438.2607719898224 and batch: 900, loss is 4.079619431495667 and perplexity is 59.1229652294299
At time: 438.6831338405609 and batch: 950, loss is 4.006795048713684 and perplexity is 54.97041045378849
At time: 439.0960760116577 and batch: 1000, loss is 3.970398349761963 and perplexity is 53.00564141934189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6286256371474845 and perplexity of 102.37326936261722
Finished 47 epochs...
Completing Train Step...
At time: 440.44227719306946 and batch: 50, loss is 4.125089745521546 and perplexity is 61.873361858333084
At time: 440.9016270637512 and batch: 100, loss is 4.011729192733765 and perplexity is 55.24231262641106
At time: 441.3423342704773 and batch: 150, loss is 4.055777244567871 and perplexity is 57.730015904330195
At time: 441.7630934715271 and batch: 200, loss is 4.086439557075501 and perplexity is 59.52756943437477
At time: 442.19149589538574 and batch: 250, loss is 4.058044271469116 and perplexity is 57.86103986470058
At time: 442.615460395813 and batch: 300, loss is 3.97942928314209 and perplexity is 53.48649986793785
At time: 443.0295693874359 and batch: 350, loss is 4.056781182289123 and perplexity is 57.78800234746351
At time: 443.44809126853943 and batch: 400, loss is 3.9593901348114016 and perplexity is 52.42534380550502
At time: 443.855836391449 and batch: 450, loss is 4.018561415672302 and perplexity is 55.62103269870075
At time: 444.2906424999237 and batch: 500, loss is 3.957220516204834 and perplexity is 52.311724104407325
At time: 444.7162802219391 and batch: 550, loss is 4.006182508468628 and perplexity is 54.936749175594585
At time: 445.1363708972931 and batch: 600, loss is 4.048844509124756 and perplexity is 57.3311731101841
At time: 445.54487347602844 and batch: 650, loss is 4.002954874038696 and perplexity is 54.75971928006577
At time: 445.9854145050049 and batch: 700, loss is 3.9745141792297365 and perplexity is 53.2242531760011
At time: 446.4103407859802 and batch: 750, loss is 3.9604023933410644 and perplexity is 52.478438675276344
At time: 446.8319594860077 and batch: 800, loss is 3.9558803272247314 and perplexity is 52.24166346596659
At time: 447.2533423900604 and batch: 850, loss is 3.9458894157409667 and perplexity is 51.72232030627189
At time: 447.6839437484741 and batch: 900, loss is 4.079718918800354 and perplexity is 59.12884750648677
At time: 448.1088364124298 and batch: 950, loss is 4.006931462287903 and perplexity is 54.977909675440955
At time: 448.5237193107605 and batch: 1000, loss is 3.9705274629592897 and perplexity is 53.01248558900871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6286096340272485 and perplexity of 102.37163108398748
Finished 48 epochs...
Completing Train Step...
At time: 449.82927227020264 and batch: 50, loss is 4.124787840843201 and perplexity is 61.85468482040754
At time: 450.24546098709106 and batch: 100, loss is 4.011428818702698 and perplexity is 55.22572176213915
At time: 450.6682913303375 and batch: 150, loss is 4.055441722869873 and perplexity is 57.71064948047794
At time: 451.0855300426483 and batch: 200, loss is 4.086208515167236 and perplexity is 59.51381765981758
At time: 451.51584458351135 and batch: 250, loss is 4.057732095718384 and perplexity is 57.842979870235496
At time: 451.93500328063965 and batch: 300, loss is 3.979155640602112 and perplexity is 53.47186568861777
At time: 452.36270332336426 and batch: 350, loss is 4.056561965942382 and perplexity is 57.775335661126526
At time: 452.78349137306213 and batch: 400, loss is 3.9592191410064697 and perplexity is 52.416380162878376
At time: 453.22677087783813 and batch: 450, loss is 4.01834421634674 and perplexity is 55.608953159792904
At time: 453.65166878700256 and batch: 500, loss is 3.957089314460754 and perplexity is 52.30486116519365
At time: 454.0840411186218 and batch: 550, loss is 4.006060266494751 and perplexity is 54.930034009382936
At time: 454.51711916923523 and batch: 600, loss is 4.048772668838501 and perplexity is 57.327054570236854
At time: 454.9451620578766 and batch: 650, loss is 4.002831029891968 and perplexity is 54.75293802927419
At time: 455.38728523254395 and batch: 700, loss is 3.9744133281707765 and perplexity is 53.21888572436709
At time: 455.81088066101074 and batch: 750, loss is 3.960316653251648 and perplexity is 52.473939362140406
At time: 456.2403576374054 and batch: 800, loss is 3.955849738121033 and perplexity is 52.24006546474628
At time: 456.6640655994415 and batch: 850, loss is 3.9458646821975707 and perplexity is 51.72104104583843
At time: 457.1146216392517 and batch: 900, loss is 4.07977560043335 and perplexity is 59.13219912110721
At time: 457.5351240634918 and batch: 950, loss is 4.007026805877685 and perplexity is 54.983151716601675
At time: 457.96370124816895 and batch: 1000, loss is 3.9706149005889895 and perplexity is 53.01712107774823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.628591397913491 and perplexity of 102.3697642402995
Finished 49 epochs...
Completing Train Step...
At time: 459.2287902832031 and batch: 50, loss is 4.124509286880493 and perplexity is 61.83745735233994
At time: 459.6627161502838 and batch: 100, loss is 4.011150989532471 and perplexity is 55.21038057689984
At time: 460.0752947330475 and batch: 150, loss is 4.0551289510726924 and perplexity is 57.69260203943586
At time: 460.5122947692871 and batch: 200, loss is 4.085997285842896 and perplexity is 59.50124792391954
At time: 460.93118238449097 and batch: 250, loss is 4.057448554039001 and perplexity is 57.8265812995316
At time: 461.35902762413025 and batch: 300, loss is 3.978906650543213 and perplexity is 53.45855338301518
At time: 461.77236342430115 and batch: 350, loss is 4.05636323928833 and perplexity is 57.763855302748205
At time: 462.1818153858185 and batch: 400, loss is 3.9590632009506224 and perplexity is 52.408206986907786
At time: 462.6070554256439 and batch: 450, loss is 4.018140978813172 and perplexity is 55.59765248170823
At time: 463.01953625679016 and batch: 500, loss is 3.9569686889648437 and perplexity is 52.29855224589298
At time: 463.44478702545166 and batch: 550, loss is 4.0059570217132565 and perplexity is 54.924363062777
At time: 463.86264300346375 and batch: 600, loss is 4.0486961841583256 and perplexity is 57.32267009647734
At time: 464.2712540626526 and batch: 650, loss is 4.002704296112061 and perplexity is 54.745999422163976
At time: 464.67457723617554 and batch: 700, loss is 3.974311981201172 and perplexity is 53.21349242487509
At time: 465.0911304950714 and batch: 750, loss is 3.9602272510528564 and perplexity is 52.46924828628155
At time: 465.5005056858063 and batch: 800, loss is 3.955806169509888 and perplexity is 52.23778948722881
At time: 465.9074594974518 and batch: 850, loss is 3.945820918083191 and perplexity is 51.71877756981215
At time: 466.31929326057434 and batch: 900, loss is 4.079809083938598 and perplexity is 59.13417910755509
At time: 466.7360439300537 and batch: 950, loss is 4.007093534469605 and perplexity is 54.986820787309625
At time: 467.16098284721375 and batch: 1000, loss is 3.9706737422943115 and perplexity is 53.020240787347326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.628573161799733 and perplexity of 102.36789743065519
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1cac512908>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 18.154545353046444, 'num_layers': 1, 'dropout': 0.035335201470844235, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 7.735717459826611}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6545491218566895 and batch: 50, loss is 6.633513870239258 and perplexity is 760.1485466040253
At time: 1.0939462184906006 and batch: 100, loss is 5.838670377731323 and perplexity is 343.3225477464225
At time: 1.5165214538574219 and batch: 150, loss is 5.685672130584717 and perplexity is 294.61579880706665
At time: 1.9480564594268799 and batch: 200, loss is 5.6249417877197265 and perplexity is 277.2561443409782
At time: 2.3657703399658203 and batch: 250, loss is 5.634777164459228 and perplexity is 279.9965171828549
At time: 2.782905340194702 and batch: 300, loss is 5.506706304550171 and perplexity is 246.33842565516423
At time: 3.218280792236328 and batch: 350, loss is 5.5541536998748775 and perplexity is 258.3082656736364
At time: 3.6561737060546875 and batch: 400, loss is 5.522307977676392 and perplexity is 250.21185466765675
At time: 4.080792665481567 and batch: 450, loss is 5.5284072780609135 and perplexity is 251.7426355295883
At time: 4.507436037063599 and batch: 500, loss is 5.549147539138794 and perplexity is 257.01836439669773
At time: 4.923523664474487 and batch: 550, loss is 5.611747741699219 and perplexity is 273.6220409959
At time: 5.3374786376953125 and batch: 600, loss is 5.625623512268066 and perplexity is 277.44522110256565
At time: 5.768969535827637 and batch: 650, loss is 5.63241153717041 and perplexity is 279.33493262080947
At time: 6.183804035186768 and batch: 700, loss is 5.648469285964966 and perplexity is 283.85662977686866
At time: 6.610840559005737 and batch: 750, loss is 5.543402757644653 and perplexity is 255.54608306959827
At time: 7.052295684814453 and batch: 800, loss is 5.596088895797729 and perplexity is 269.370807194517
At time: 7.480195760726929 and batch: 850, loss is 5.558081092834473 and perplexity is 259.32473847516934
At time: 7.9133970737457275 and batch: 900, loss is 5.668150148391724 and perplexity is 289.49850947796307
At time: 8.34398603439331 and batch: 950, loss is 5.664783010482788 and perplexity is 288.5253673434314
At time: 8.767348527908325 and batch: 1000, loss is 5.640330839157104 and perplexity is 281.55585276920294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.735066855826029 and perplexity of 309.5336641962648
Finished 1 epochs...
Completing Train Step...
At time: 10.065375566482544 and batch: 50, loss is 5.668418140411377 and perplexity is 289.5761031649847
At time: 10.47672414779663 and batch: 100, loss is 5.665025157928467 and perplexity is 288.59524148372805
At time: 10.885990619659424 and batch: 150, loss is 5.662216348648071 and perplexity is 287.78576984907806
At time: 11.308634281158447 and batch: 200, loss is 5.646363019943237 and perplexity is 283.2593814053036
At time: 11.712125062942505 and batch: 250, loss is 5.644974737167359 and perplexity is 282.8664101257374
At time: 12.128504276275635 and batch: 300, loss is 5.54203914642334 and perplexity is 255.19785504094463
At time: 12.557528257369995 and batch: 350, loss is 5.598185644149781 and perplexity is 269.9362025293213
At time: 12.977318525314331 and batch: 400, loss is 5.520014705657959 and perplexity is 249.6387082641581
At time: 13.401659488677979 and batch: 450, loss is 5.520674114227295 and perplexity is 249.80337645346614
At time: 13.828051805496216 and batch: 500, loss is 5.5370879745483395 and perplexity is 253.93744941575508
At time: 14.247233867645264 and batch: 550, loss is 5.5918652439117436 and perplexity is 268.23547798214815
At time: 14.658621311187744 and batch: 600, loss is 5.596362590789795 and perplexity is 269.4445427255173
At time: 15.065890550613403 and batch: 650, loss is 5.595203466415406 and perplexity is 269.1324039272909
At time: 15.47123670578003 and batch: 700, loss is 5.603122673034668 and perplexity is 271.27218052564285
At time: 15.891143321990967 and batch: 750, loss is 5.508110733032226 and perplexity is 246.68463341149024
At time: 16.296072959899902 and batch: 800, loss is 5.586898145675659 and perplexity is 266.90642950231734
At time: 16.716325998306274 and batch: 850, loss is 5.517962999343872 and perplexity is 249.12704801793703
At time: 17.12232208251953 and batch: 900, loss is 5.604895353317261 and perplexity is 271.7534858454642
At time: 17.54703688621521 and batch: 950, loss is 5.56874647140503 and perplexity is 262.10533669226305
At time: 17.96270251274109 and batch: 1000, loss is 5.5506015872955325 and perplexity is 257.3923533087485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.706358374618903 and perplexity of 300.7737661330422
Finished 2 epochs...
Completing Train Step...
At time: 19.28382158279419 and batch: 50, loss is 5.640563430786133 and perplexity is 281.6213479201778
At time: 19.725918531417847 and batch: 100, loss is 5.615994243621826 and perplexity is 274.7864480977646
At time: 20.154545783996582 and batch: 150, loss is 5.625166130065918 and perplexity is 277.31835161249745
At time: 20.57979464530945 and batch: 200, loss is 5.60099455833435 and perplexity is 270.6954960534745
At time: 20.9929678440094 and batch: 250, loss is 5.640434589385986 and perplexity is 281.58506576877227
At time: 21.419302225112915 and batch: 300, loss is 5.54911093711853 and perplexity is 257.00895717747864
At time: 21.836779832839966 and batch: 350, loss is 5.584638795852661 and perplexity is 266.30407522880944
At time: 22.25986623764038 and batch: 400, loss is 5.525410165786743 and perplexity is 250.98926412000122
At time: 22.667820930480957 and batch: 450, loss is 5.505682592391968 and perplexity is 246.08637504943525
At time: 23.098953247070312 and batch: 500, loss is 5.518584871292115 and perplexity is 249.28202132242885
At time: 23.54961109161377 and batch: 550, loss is 5.562961320877076 and perplexity is 260.59339548584995
At time: 23.972100496292114 and batch: 600, loss is 5.592345895767212 and perplexity is 268.3644368520201
At time: 24.3966064453125 and batch: 650, loss is 5.57063027381897 and perplexity is 262.59955671851606
At time: 24.815950393676758 and batch: 700, loss is 5.574218425750733 and perplexity is 263.543496311818
At time: 25.24015712738037 and batch: 750, loss is 5.486235036849975 and perplexity is 241.34683224012616
At time: 25.656389236450195 and batch: 800, loss is 5.543485879898071 and perplexity is 255.56732551872307
At time: 26.08029818534851 and batch: 850, loss is 5.506976490020752 and perplexity is 246.40499171080756
At time: 26.50231432914734 and batch: 900, loss is 5.604455528259277 and perplexity is 271.6339881337653
At time: 26.91972851753235 and batch: 950, loss is 5.575828657150269 and perplexity is 263.968204171922
At time: 27.33018946647644 and batch: 1000, loss is 5.56153284072876 and perplexity is 260.2214087447088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6852476538681405 and perplexity of 294.4907677983909
Finished 3 epochs...
Completing Train Step...
At time: 28.65374207496643 and batch: 50, loss is 5.602513151168823 and perplexity is 271.106884580881
At time: 29.097191095352173 and batch: 100, loss is 5.602833232879639 and perplexity is 271.1936748255555
At time: 29.524867296218872 and batch: 150, loss is 5.606863985061645 and perplexity is 272.28899532187637
At time: 29.947941064834595 and batch: 200, loss is 5.594732446670532 and perplexity is 269.00566710117283
At time: 30.392126083374023 and batch: 250, loss is 5.616463603973389 and perplexity is 274.9154522339935
At time: 30.810803651809692 and batch: 300, loss is 5.514689531326294 and perplexity is 248.3128719106919
At time: 31.223443746566772 and batch: 350, loss is 5.571618003845215 and perplexity is 262.8590623252074
At time: 31.629328966140747 and batch: 400, loss is 5.488401317596436 and perplexity is 241.8702239380264
At time: 32.0451385974884 and batch: 450, loss is 5.494625387191772 and perplexity is 243.38033569015678
At time: 32.46936845779419 and batch: 500, loss is 5.5026744365692135 and perplexity is 245.34722118980682
At time: 32.88524508476257 and batch: 550, loss is 5.55862904548645 and perplexity is 259.46687509185165
At time: 33.30523443222046 and batch: 600, loss is 5.596946668624878 and perplexity is 269.6019652797444
At time: 33.7287871837616 and batch: 650, loss is 5.563823223114014 and perplexity is 260.8180983383856
At time: 34.193978786468506 and batch: 700, loss is 5.5415784740448 and perplexity is 255.0803195128289
At time: 34.613935708999634 and batch: 750, loss is 5.4711103343963625 and perplexity is 237.72399941065217
At time: 35.04057002067566 and batch: 800, loss is 5.534912586212158 and perplexity is 253.3856372706409
At time: 35.45916414260864 and batch: 850, loss is 5.48510064125061 and perplexity is 241.07320468600207
At time: 35.88165521621704 and batch: 900, loss is 5.58267484664917 and perplexity is 265.7815807965821
At time: 36.30301547050476 and batch: 950, loss is 5.577358169555664 and perplexity is 264.37225573700334
At time: 36.72054052352905 and batch: 1000, loss is 5.535458250045776 and perplexity is 253.5239383783825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.718881002286586 and perplexity of 304.563925868374
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 38.124706506729126 and batch: 50, loss is 5.487531147003174 and perplexity is 241.6598471269018
At time: 38.55112266540527 and batch: 100, loss is 5.3185328769683835 and perplexity is 204.0842455479158
At time: 38.96661376953125 and batch: 150, loss is 5.259131841659546 and perplexity is 192.31445940075085
At time: 39.37561893463135 and batch: 200, loss is 5.240213642120361 and perplexity is 188.71041459242986
At time: 39.79908490180969 and batch: 250, loss is 5.196765718460083 and perplexity is 180.68690349436275
At time: 40.21821713447571 and batch: 300, loss is 5.089523849487304 and perplexity is 162.31255844295325
At time: 40.64666438102722 and batch: 350, loss is 5.1559898853302 and perplexity is 173.46743466774112
At time: 41.06936025619507 and batch: 400, loss is 5.07201491355896 and perplexity is 159.49537318489192
At time: 41.49984288215637 and batch: 450, loss is 5.073201847076416 and perplexity is 159.6847959830999
At time: 41.91866111755371 and batch: 500, loss is 5.044505767822265 and perplexity is 155.16759142757215
At time: 42.330806255340576 and batch: 550, loss is 5.097940826416016 and perplexity is 163.68450522513578
At time: 42.73988914489746 and batch: 600, loss is 5.125265283584595 and perplexity is 168.21876140916189
At time: 43.15724062919617 and batch: 650, loss is 5.088002576828003 and perplexity is 162.06582450791151
At time: 43.57446551322937 and batch: 700, loss is 5.058365354537964 and perplexity is 157.33312213243659
At time: 44.005894899368286 and batch: 750, loss is 4.996219911575317 and perplexity is 147.85320324587897
At time: 44.42964768409729 and batch: 800, loss is 5.023898878097534 and perplexity is 152.00279032780247
At time: 44.844136238098145 and batch: 850, loss is 4.982439441680908 and perplexity is 145.8296911258881
At time: 45.32003331184387 and batch: 900, loss is 5.072268857955932 and perplexity is 159.53588128443997
At time: 45.7399525642395 and batch: 950, loss is 4.995287246704102 and perplexity is 147.7153700431993
At time: 46.16205620765686 and batch: 1000, loss is 4.960712404251098 and perplexity is 142.69541651654916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.205275000595465 and perplexity of 182.2309795066502
Finished 5 epochs...
Completing Train Step...
At time: 47.43987274169922 and batch: 50, loss is 5.144081888198852 and perplexity is 171.4140351554343
At time: 47.8844735622406 and batch: 100, loss is 5.099640998840332 and perplexity is 163.96303381342742
At time: 48.304746866226196 and batch: 150, loss is 5.115642147064209 and perplexity is 166.6077333069007
At time: 48.73612427711487 and batch: 200, loss is 5.115456743240356 and perplexity is 166.57684645942047
At time: 49.167797803878784 and batch: 250, loss is 5.096015844345093 and perplexity is 163.36971856383403
At time: 49.60479187965393 and batch: 300, loss is 4.995815286636352 and perplexity is 147.79339025426992
At time: 50.02145600318909 and batch: 350, loss is 5.066963138580323 and perplexity is 158.69167022192372
At time: 50.453447341918945 and batch: 400, loss is 4.99283724784851 and perplexity is 147.35391052414906
At time: 50.883456230163574 and batch: 450, loss is 5.009393672943116 and perplexity is 149.81387240092263
At time: 51.320515871047974 and batch: 500, loss is 4.980847940444947 and perplexity is 145.59778757856103
At time: 51.753868103027344 and batch: 550, loss is 5.031567392349243 and perplexity is 153.17290668454925
At time: 52.168681144714355 and batch: 600, loss is 5.071713027954101 and perplexity is 159.4472310947534
At time: 52.587984561920166 and batch: 650, loss is 5.035262517929077 and perplexity is 153.7399468072131
At time: 53.00717616081238 and batch: 700, loss is 5.004599599838257 and perplexity is 149.09737259302392
At time: 53.43813180923462 and batch: 750, loss is 4.955907821655273 and perplexity is 142.01146895880336
At time: 53.85848045349121 and batch: 800, loss is 4.983574552536011 and perplexity is 145.99531797591516
At time: 54.290026903152466 and batch: 850, loss is 4.951834716796875 and perplexity is 141.43421775503865
At time: 54.725810289382935 and batch: 900, loss is 5.0480649948120115 and perplexity is 155.7208521130339
At time: 55.15809893608093 and batch: 950, loss is 4.989757270812988 and perplexity is 146.90076206528732
At time: 55.58155059814453 and batch: 1000, loss is 4.962246246337891 and perplexity is 142.91445669557316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.193149473608994 and perplexity of 180.0346754289115
Finished 6 epochs...
Completing Train Step...
At time: 56.947309494018555 and batch: 50, loss is 5.112315940856933 and perplexity is 166.05448225381977
At time: 57.383482217788696 and batch: 100, loss is 5.0676462268829345 and perplexity is 158.8001076775365
At time: 57.812955141067505 and batch: 150, loss is 5.08549651145935 and perplexity is 161.66018544896576
At time: 58.24125814437866 and batch: 200, loss is 5.085274782180786 and perplexity is 161.62434462630029
At time: 58.66285276412964 and batch: 250, loss is 5.068864889144898 and perplexity is 158.993749343889
At time: 59.08313012123108 and batch: 300, loss is 4.96780689239502 and perplexity is 143.7113670204086
At time: 59.49437618255615 and batch: 350, loss is 5.041385469436645 and perplexity is 154.68417683364947
At time: 59.93391036987305 and batch: 400, loss is 4.968863906860352 and perplexity is 143.86335232536396
At time: 60.36386156082153 and batch: 450, loss is 4.988169279098511 and perplexity is 146.66766999539777
At time: 60.78645157814026 and batch: 500, loss is 4.960691556930542 and perplexity is 142.69244173046744
At time: 61.20489501953125 and batch: 550, loss is 5.006819133758545 and perplexity is 149.42866679227092
At time: 61.63456988334656 and batch: 600, loss is 5.0506518268585205 and perplexity is 156.12419727175873
At time: 62.04657554626465 and batch: 650, loss is 5.016382474899292 and perplexity is 150.86455912956768
At time: 62.47092604637146 and batch: 700, loss is 4.985755043029785 and perplexity is 146.3140067014974
At time: 62.88622236251831 and batch: 750, loss is 4.939540910720825 and perplexity is 139.7060972638248
At time: 63.299567461013794 and batch: 800, loss is 4.965738792419433 and perplexity is 143.41446466348262
At time: 63.72469997406006 and batch: 850, loss is 4.9376123332977295 and perplexity is 139.4369228840197
At time: 64.15488743782043 and batch: 900, loss is 5.0366599750518795 and perplexity is 153.95494197921695
At time: 64.57936072349548 and batch: 950, loss is 4.982106533050537 and perplexity is 145.78115124326914
At time: 64.98783278465271 and batch: 1000, loss is 4.953936910629272 and perplexity is 141.73185262878167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.183425531154725 and perplexity of 178.29251267793927
Finished 7 epochs...
Completing Train Step...
At time: 66.35889887809753 and batch: 50, loss is 5.087712640762329 and perplexity is 162.01884259157868
At time: 66.78085350990295 and batch: 100, loss is 5.045222225189209 and perplexity is 155.27880222570204
At time: 67.21026945114136 and batch: 150, loss is 5.065406942367554 and perplexity is 158.44490690159856
At time: 67.63086104393005 and batch: 200, loss is 5.064651269912719 and perplexity is 158.32521967770882
At time: 68.06779623031616 and batch: 250, loss is 5.048163185119629 and perplexity is 155.73614314210843
At time: 68.4915177822113 and batch: 300, loss is 4.947609930038452 and perplexity is 140.83794878619267
At time: 68.91203689575195 and batch: 350, loss is 5.017518663406372 and perplexity is 151.03606712203114
At time: 69.33036375045776 and batch: 400, loss is 4.948362455368042 and perplexity is 140.94397279789553
At time: 69.7585620880127 and batch: 450, loss is 4.968141946792603 and perplexity is 143.75952621343563
At time: 70.17672109603882 and batch: 500, loss is 4.940685234069824 and perplexity is 139.86605771870768
At time: 70.59478640556335 and batch: 550, loss is 4.985453577041626 and perplexity is 146.2699046528534
At time: 71.00617909431458 and batch: 600, loss is 5.029795961380005 and perplexity is 152.90181163797124
At time: 71.41768407821655 and batch: 650, loss is 4.997372484207153 and perplexity is 148.0237130450576
At time: 71.83452200889587 and batch: 700, loss is 4.967341384887695 and perplexity is 143.64448386868517
At time: 72.24700450897217 and batch: 750, loss is 4.922338237762451 and perplexity is 137.32333268327145
At time: 72.671541929245 and batch: 800, loss is 4.945415802001953 and perplexity is 140.52927105722384
At time: 73.0944709777832 and batch: 850, loss is 4.921998863220215 and perplexity is 137.27673654732203
At time: 73.5130398273468 and batch: 900, loss is 5.022642211914063 and perplexity is 151.81189353329947
At time: 73.92930555343628 and batch: 950, loss is 4.968025188446045 and perplexity is 143.74274206871667
At time: 74.33955907821655 and batch: 1000, loss is 4.939855527877808 and perplexity is 139.75005811401533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.173245685856517 and perplexity of 176.48672937269762
Finished 8 epochs...
Completing Train Step...
At time: 75.63116407394409 and batch: 50, loss is 5.064955234527588 and perplexity is 158.373352257062
At time: 76.0611219406128 and batch: 100, loss is 5.024554796218872 and perplexity is 152.10252441759516
At time: 76.487295627594 and batch: 150, loss is 5.044776191711426 and perplexity is 155.20955812526074
At time: 76.91442537307739 and batch: 200, loss is 5.044122877120972 and perplexity is 155.1081905724157
At time: 77.34184694290161 and batch: 250, loss is 5.0289950275421145 and perplexity is 152.77939643293968
At time: 77.78864526748657 and batch: 300, loss is 4.930274868011475 and perplexity is 138.4175536692792
At time: 78.20599055290222 and batch: 350, loss is 4.998429269790649 and perplexity is 148.18022505627175
At time: 78.61872720718384 and batch: 400, loss is 4.930942859649658 and perplexity is 138.51004632642952
At time: 79.0496654510498 and batch: 450, loss is 4.9509497737884525 and perplexity is 141.30911189682632
At time: 79.4736487865448 and batch: 500, loss is 4.924654312133789 and perplexity is 137.64175233414335
At time: 79.90639781951904 and batch: 550, loss is 4.9703398609161376 and perplexity is 144.07584479969697
At time: 80.32756924629211 and batch: 600, loss is 5.016227188110352 and perplexity is 150.8411336754945
At time: 80.74660134315491 and batch: 650, loss is 4.983493118286133 and perplexity is 145.98342944078368
At time: 81.16700291633606 and batch: 700, loss is 4.951226205825805 and perplexity is 141.34817966206668
At time: 81.59704971313477 and batch: 750, loss is 4.908839616775513 and perplexity is 135.4821119906003
At time: 82.02314043045044 and batch: 800, loss is 4.931712741851807 and perplexity is 138.616723805192
At time: 82.44125938415527 and batch: 850, loss is 4.908747367858886 and perplexity is 135.46961448899626
At time: 82.86964774131775 and batch: 900, loss is 5.01250002861023 and perplexity is 150.27997113188528
At time: 83.2917971611023 and batch: 950, loss is 4.9565019607543945 and perplexity is 142.0958685951146
At time: 83.712726354599 and batch: 1000, loss is 4.927442865371704 and perplexity is 138.02610933920826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1646788062118905 and perplexity of 174.98124665536034
Finished 9 epochs...
Completing Train Step...
At time: 84.97233295440674 and batch: 50, loss is 5.048502645492554 and perplexity is 155.7890183653511
At time: 85.41224145889282 and batch: 100, loss is 5.008479022979737 and perplexity is 149.67690779490195
At time: 85.84095478057861 and batch: 150, loss is 5.028346080780029 and perplexity is 152.68028290149212
At time: 86.26624727249146 and batch: 200, loss is 5.028356847763061 and perplexity is 152.68192681635747
At time: 86.69855332374573 and batch: 250, loss is 5.012346286773681 and perplexity is 150.25686858908617
At time: 87.11685943603516 and batch: 300, loss is 4.915341682434082 and perplexity is 136.36589567446416
At time: 87.52696561813354 and batch: 350, loss is 4.983012580871582 and perplexity is 145.91329579330338
At time: 87.93526411056519 and batch: 400, loss is 4.915808773040771 and perplexity is 136.42960578144434
At time: 88.36645412445068 and batch: 450, loss is 4.935532178878784 and perplexity is 139.1471740183896
At time: 88.79730033874512 and batch: 500, loss is 4.909010705947876 and perplexity is 135.50529349600646
At time: 89.22784113883972 and batch: 550, loss is 4.95752251625061 and perplexity is 142.24095933878377
At time: 89.64415049552917 and batch: 600, loss is 5.002597932815552 and perplexity is 148.79922779198787
At time: 90.07339072227478 and batch: 650, loss is 4.97122594833374 and perplexity is 144.20356517030444
At time: 90.49555778503418 and batch: 700, loss is 4.9368883228302005 and perplexity is 139.33600562927438
At time: 90.91193056106567 and batch: 750, loss is 4.896506452560425 and perplexity is 133.8214505103752
At time: 91.3412356376648 and batch: 800, loss is 4.920427503585816 and perplexity is 137.06119481590846
At time: 91.76546955108643 and batch: 850, loss is 4.89833083152771 and perplexity is 134.06581438844015
At time: 92.19366383552551 and batch: 900, loss is 4.999981718063355 and perplexity is 148.4104458474064
At time: 92.61798691749573 and batch: 950, loss is 4.944607610702515 and perplexity is 140.41574240565265
At time: 93.03927230834961 and batch: 1000, loss is 4.916475114822387 and perplexity is 136.52054482290256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.157031919898056 and perplexity of 173.64828794237943
Finished 10 epochs...
Completing Train Step...
At time: 94.30894041061401 and batch: 50, loss is 5.032634363174439 and perplexity is 153.336424926276
At time: 94.72251653671265 and batch: 100, loss is 4.992552633285523 and perplexity is 147.3119774229694
At time: 95.14225363731384 and batch: 150, loss is 5.013214979171753 and perplexity is 150.3874522989138
At time: 95.56379628181458 and batch: 200, loss is 5.013642730712891 and perplexity is 150.4517945236643
At time: 95.98457431793213 and batch: 250, loss is 4.996427278518677 and perplexity is 147.8838662918432
At time: 96.40436458587646 and batch: 300, loss is 4.899284601211548 and perplexity is 134.1937432955016
At time: 96.82070183753967 and batch: 350, loss is 4.968088798522949 and perplexity is 143.7518858464091
At time: 97.26316928863525 and batch: 400, loss is 4.9011436939239506 and perplexity is 134.44345395142128
At time: 97.68280744552612 and batch: 450, loss is 4.920984220504761 and perplexity is 137.13752034587895
At time: 98.09479570388794 and batch: 500, loss is 4.894310398101807 and perplexity is 133.52789376859582
At time: 98.50730395317078 and batch: 550, loss is 4.942466583251953 and perplexity is 140.11543004988914
At time: 98.91324853897095 and batch: 600, loss is 4.987808542251587 and perplexity is 146.61477110444122
At time: 99.34111738204956 and batch: 650, loss is 4.956883020401001 and perplexity is 142.15002591451488
At time: 99.76607704162598 and batch: 700, loss is 4.923968505859375 and perplexity is 137.54738911791262
At time: 100.1923189163208 and batch: 750, loss is 4.8841859722137455 and perplexity is 132.18282103832422
At time: 100.61005520820618 and batch: 800, loss is 4.906086511611939 and perplexity is 135.10962846693582
At time: 101.03759908676147 and batch: 850, loss is 4.886019868850708 and perplexity is 132.42545308233417
At time: 101.44999861717224 and batch: 900, loss is 4.98721284866333 and perplexity is 146.5274596333742
At time: 101.86912107467651 and batch: 950, loss is 4.931153917312622 and perplexity is 138.53928301830965
At time: 102.30013346672058 and batch: 1000, loss is 4.903161163330078 and perplexity is 134.71496329542887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.146352628382241 and perplexity of 171.80371415550005
Finished 11 epochs...
Completing Train Step...
At time: 103.61993265151978 and batch: 50, loss is 5.01543704032898 and perplexity is 150.72199396356922
At time: 104.06714987754822 and batch: 100, loss is 4.9759871196746825 and perplexity is 144.8917801065899
At time: 104.49340486526489 and batch: 150, loss is 4.999445390701294 and perplexity is 148.3308706055801
At time: 104.9127995967865 and batch: 200, loss is 5.000038356781006 and perplexity is 148.41885186279592
At time: 105.32312083244324 and batch: 250, loss is 4.98185284614563 and perplexity is 145.7441731648419
At time: 105.74794912338257 and batch: 300, loss is 4.885200004577637 and perplexity is 132.3169266790462
At time: 106.16223359107971 and batch: 350, loss is 4.955649967193604 and perplexity is 141.97485538861315
At time: 106.58397936820984 and batch: 400, loss is 4.889385795593261 and perplexity is 132.87193845324725
At time: 107.0090446472168 and batch: 450, loss is 4.9105802917480466 and perplexity is 135.71814768328176
At time: 107.44234132766724 and batch: 500, loss is 4.883136444091797 and perplexity is 132.0441642252459
At time: 107.8558759689331 and batch: 550, loss is 4.931458730697631 and perplexity is 138.58151808270264
At time: 108.27841448783875 and batch: 600, loss is 4.977036056518554 and perplexity is 145.04384217092635
At time: 108.70504474639893 and batch: 650, loss is 4.94714807510376 and perplexity is 140.7729171033116
At time: 109.12354946136475 and batch: 700, loss is 4.91447961807251 and perplexity is 136.2483901516081
At time: 109.56031203269958 and batch: 750, loss is 4.87584698677063 and perplexity is 131.08513357869964
At time: 109.99287962913513 and batch: 800, loss is 4.897559986114502 and perplexity is 133.9625101912494
At time: 110.46420383453369 and batch: 850, loss is 4.876945447921753 and perplexity is 131.2292046191596
At time: 110.8843641281128 and batch: 900, loss is 4.978553314208984 and perplexity is 145.26407809098149
At time: 111.31923007965088 and batch: 950, loss is 4.921833181381226 and perplexity is 137.25399416921204
At time: 111.72687411308289 and batch: 1000, loss is 4.892309398651123 and perplexity is 133.26097167106215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.140118482636242 and perplexity of 170.73599637391453
Finished 12 epochs...
Completing Train Step...
At time: 113.06794476509094 and batch: 50, loss is 5.004852380752563 and perplexity is 149.1350663271162
At time: 113.49291753768921 and batch: 100, loss is 4.966104469299316 and perplexity is 143.4669176072808
At time: 113.91911745071411 and batch: 150, loss is 4.989979829788208 and perplexity is 146.93345978680233
At time: 114.3522367477417 and batch: 200, loss is 4.990583639144898 and perplexity is 147.02220637494628
At time: 114.78193259239197 and batch: 250, loss is 4.971397838592529 and perplexity is 144.22835448890007
At time: 115.22175407409668 and batch: 300, loss is 4.875960578918457 and perplexity is 131.10002466631042
At time: 115.65369606018066 and batch: 350, loss is 4.945102872848511 and perplexity is 140.4853022313206
At time: 116.07997512817383 and batch: 400, loss is 4.8791115283966064 and perplexity is 131.5137657168734
At time: 116.49498414993286 and batch: 450, loss is 4.900078945159912 and perplexity is 134.3003816315479
At time: 116.91743755340576 and batch: 500, loss is 4.873104658126831 and perplexity is 130.7261475159674
At time: 117.34727144241333 and batch: 550, loss is 4.922433528900147 and perplexity is 137.3364190033702
At time: 117.783935546875 and batch: 600, loss is 4.96519907951355 and perplexity is 143.33708290985732
At time: 118.19805026054382 and batch: 650, loss is 4.936028547286988 and perplexity is 139.21625942415113
At time: 118.62567067146301 and batch: 700, loss is 4.904173622131347 and perplexity is 134.85142571531202
At time: 119.03801894187927 and batch: 750, loss is 4.867865562438965 and perplexity is 130.04305168491078
At time: 119.45152282714844 and batch: 800, loss is 4.887553577423096 and perplexity is 132.62871096426383
At time: 119.88234186172485 and batch: 850, loss is 4.867231655120849 and perplexity is 129.96064256540922
At time: 120.30556750297546 and batch: 900, loss is 4.969872169494629 and perplexity is 144.00847751781538
At time: 120.72931909561157 and batch: 950, loss is 4.912028045654297 and perplexity is 135.91477646243646
At time: 121.18238258361816 and batch: 1000, loss is 4.8819787979125975 and perplexity is 131.89139224805425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.135161237018864 and perplexity of 169.89171049936303
Finished 13 epochs...
Completing Train Step...
At time: 122.48485255241394 and batch: 50, loss is 4.994160575866699 and perplexity is 147.54903716233363
At time: 122.89310908317566 and batch: 100, loss is 4.95482310295105 and perplexity is 141.85750997839818
At time: 123.32174587249756 and batch: 150, loss is 4.980578746795654 and perplexity is 145.55859885370842
At time: 123.73946380615234 and batch: 200, loss is 4.980524749755859 and perplexity is 145.55073933245097
At time: 124.16187357902527 and batch: 250, loss is 4.960181140899659 and perplexity is 142.6196278050005
At time: 124.57497262954712 and batch: 300, loss is 4.865082340240479 and perplexity is 129.68161618766845
At time: 124.98901176452637 and batch: 350, loss is 4.932845907211304 and perplexity is 138.77388850483425
At time: 125.39956212043762 and batch: 400, loss is 4.868418960571289 and perplexity is 130.11503718331804
At time: 125.8418915271759 and batch: 450, loss is 4.889062719345093 and perplexity is 132.8290176196092
At time: 126.26578044891357 and batch: 500, loss is 4.863325691223144 and perplexity is 129.4540110737049
At time: 126.69744181632996 and batch: 550, loss is 4.911927423477173 and perplexity is 135.90110110975894
At time: 127.13755917549133 and batch: 600, loss is 4.956165742874146 and perplexity is 142.0481014539169
At time: 127.55793690681458 and batch: 650, loss is 4.926684379577637 and perplexity is 137.92145818928648
At time: 127.97222995758057 and batch: 700, loss is 4.89368691444397 and perplexity is 133.44466725684683
At time: 128.3825399875641 and batch: 750, loss is 4.8573895359039305 and perplexity is 128.68782829714903
At time: 128.81515049934387 and batch: 800, loss is 4.879069871902466 and perplexity is 131.50828742856638
At time: 129.23667860031128 and batch: 850, loss is 4.858101596832276 and perplexity is 128.77949450372193
At time: 129.66409373283386 and batch: 900, loss is 4.960070104598999 and perplexity is 142.603792728278
At time: 130.07820200920105 and batch: 950, loss is 4.903647985458374 and perplexity is 134.7805614866036
At time: 130.49142909049988 and batch: 1000, loss is 4.873043994903565 and perplexity is 130.71821748702692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.130600719917111 and perplexity of 169.11868049626898
Finished 14 epochs...
Completing Train Step...
At time: 131.77071237564087 and batch: 50, loss is 4.983804006576538 and perplexity is 146.0288210350819
At time: 132.1954984664917 and batch: 100, loss is 4.944335775375366 and perplexity is 140.37757763388063
At time: 132.60446310043335 and batch: 150, loss is 4.970888576507568 and perplexity is 144.15492315586647
At time: 133.0135509967804 and batch: 200, loss is 4.970337142944336 and perplexity is 144.0754532061457
At time: 133.43781208992004 and batch: 250, loss is 4.949271926879883 and perplexity is 141.07221563368415
At time: 133.86673212051392 and batch: 300, loss is 4.855350494384766 and perplexity is 128.4256958125719
At time: 134.29760336875916 and batch: 350, loss is 4.9240916728973385 and perplexity is 137.56433146575785
At time: 134.73529171943665 and batch: 400, loss is 4.857168216705322 and perplexity is 128.65935036159277
At time: 135.1693675518036 and batch: 450, loss is 4.880399932861328 and perplexity is 131.68331784219853
At time: 135.59653425216675 and batch: 500, loss is 4.852876243591308 and perplexity is 128.10833121451822
At time: 136.0399613380432 and batch: 550, loss is 4.903948001861572 and perplexity is 134.82100393227702
At time: 136.46152019500732 and batch: 600, loss is 4.9467218971252445 and perplexity is 140.7129355683833
At time: 136.8865463733673 and batch: 650, loss is 4.9157414054870605 and perplexity is 136.42041516222739
At time: 137.29370975494385 and batch: 700, loss is 4.884763278961182 and perplexity is 132.2591531041931
At time: 137.70089864730835 and batch: 750, loss is 4.850586366653443 and perplexity is 127.8153145154494
At time: 138.11030983924866 and batch: 800, loss is 4.871064376831055 and perplexity is 130.4597013073586
At time: 138.5282051563263 and batch: 850, loss is 4.850582389831543 and perplexity is 127.81480621771827
At time: 138.94290781021118 and batch: 900, loss is 4.9513269138336184 and perplexity is 141.36241527245642
At time: 139.35619354248047 and batch: 950, loss is 4.893600349426269 and perplexity is 133.43311611683322
At time: 139.7818250656128 and batch: 1000, loss is 4.864892358779907 and perplexity is 129.65698142496407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.124795308927211 and perplexity of 168.13972142925164
Finished 15 epochs...
Completing Train Step...
At time: 141.0868501663208 and batch: 50, loss is 4.9734908294677735 and perplexity is 144.530539243451
At time: 141.51915740966797 and batch: 100, loss is 4.933150033950806 and perplexity is 138.81609977353568
At time: 141.92979431152344 and batch: 150, loss is 4.961255083084106 and perplexity is 142.77287531438398
At time: 142.3545994758606 and batch: 200, loss is 4.958885841369629 and perplexity is 142.4350122601582
At time: 142.7800998687744 and batch: 250, loss is 4.939455537796021 and perplexity is 139.6941706548004
At time: 143.22074794769287 and batch: 300, loss is 4.845783767700195 and perplexity is 127.20294049020605
At time: 143.64168572425842 and batch: 350, loss is 4.914418306350708 and perplexity is 136.24003678429747
At time: 144.0610249042511 and batch: 400, loss is 4.848174962997437 and perplexity is 127.50747151477414
At time: 144.4825930595398 and batch: 450, loss is 4.869518022537232 and perplexity is 130.2581202860786
At time: 144.89123582839966 and batch: 500, loss is 4.843287677764892 and perplexity is 126.88582644793948
At time: 145.320152759552 and batch: 550, loss is 4.894456748962402 and perplexity is 133.54743712082072
At time: 145.74038648605347 and batch: 600, loss is 4.936400623321533 and perplexity is 139.26806809568674
At time: 146.152729511261 and batch: 650, loss is 4.905703620910645 and perplexity is 135.0579061491697
At time: 146.5635166168213 and batch: 700, loss is 4.874775047302246 and perplexity is 130.94469353535877
At time: 146.97953128814697 and batch: 750, loss is 4.840870780944824 and perplexity is 126.57952679391694
At time: 147.39448308944702 and batch: 800, loss is 4.861936054229736 and perplexity is 129.27424192682943
At time: 147.81633067131042 and batch: 850, loss is 4.84254599571228 and perplexity is 126.79175239856164
At time: 148.2433636188507 and batch: 900, loss is 4.942087297439575 and perplexity is 140.06229633224558
At time: 148.6796817779541 and batch: 950, loss is 4.884305543899536 and perplexity is 132.19862730604058
At time: 149.11843156814575 and batch: 1000, loss is 4.8552162647247314 and perplexity is 128.40845843199307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.11985741592035 and perplexity of 167.31151196233537
Finished 16 epochs...
Completing Train Step...
At time: 150.4245946407318 and batch: 50, loss is 4.963870124816895 and perplexity is 143.14672093955352
At time: 150.85484886169434 and batch: 100, loss is 4.922884464263916 and perplexity is 137.39836281671063
At time: 151.27302360534668 and batch: 150, loss is 4.9517279148101805 and perplexity is 141.41911310621342
At time: 151.68906140327454 and batch: 200, loss is 4.947183656692505 and perplexity is 140.77792611646817
At time: 152.09757685661316 and batch: 250, loss is 4.928357038497925 and perplexity is 138.15234679173477
At time: 152.51889061927795 and batch: 300, loss is 4.835219116210937 and perplexity is 125.86615950151112
At time: 152.93897914886475 and batch: 350, loss is 4.905364322662353 and perplexity is 135.01208901147223
At time: 153.36394476890564 and batch: 400, loss is 4.837975177764893 and perplexity is 126.21353285594708
At time: 153.83855056762695 and batch: 450, loss is 4.861052865982056 and perplexity is 129.16011883913498
At time: 154.27609372138977 and batch: 500, loss is 4.833020143508911 and perplexity is 125.58968734143116
At time: 154.7053520679474 and batch: 550, loss is 4.886838312149048 and perplexity is 132.53388017159884
At time: 155.1406135559082 and batch: 600, loss is 4.927009973526001 and perplexity is 137.96637189284078
At time: 155.57554864883423 and batch: 650, loss is 4.895625238418579 and perplexity is 133.70357709918932
At time: 155.99452900886536 and batch: 700, loss is 4.864454526901245 and perplexity is 129.6002258908028
At time: 156.4225947856903 and batch: 750, loss is 4.832721853256226 and perplexity is 125.55223074860547
At time: 156.84168577194214 and batch: 800, loss is 4.853892374038696 and perplexity is 128.23857215011304
At time: 157.27928042411804 and batch: 850, loss is 4.834554080963135 and perplexity is 125.78248189629956
At time: 157.69798684120178 and batch: 900, loss is 4.934547805786133 and perplexity is 139.01026867838897
At time: 158.11137056350708 and batch: 950, loss is 4.875327491760254 and perplexity is 131.01705319111304
At time: 158.52062797546387 and batch: 1000, loss is 4.84693169593811 and perplexity is 127.3490441797722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.113660486733041 and perplexity of 166.27790028738315
Finished 17 epochs...
Completing Train Step...
At time: 159.86711621284485 and batch: 50, loss is 4.953829984664917 and perplexity is 141.71669862395228
At time: 160.3042209148407 and batch: 100, loss is 4.912831344604492 and perplexity is 136.0240005235982
At time: 160.73254895210266 and batch: 150, loss is 4.940492219924927 and perplexity is 139.8390641963266
At time: 161.16489815711975 and batch: 200, loss is 4.9374086380004885 and perplexity is 139.40852313111273
At time: 161.5963010787964 and batch: 250, loss is 4.91886715888977 and perplexity is 136.84749887056793
At time: 162.00535416603088 and batch: 300, loss is 4.827499370574952 and perplexity is 124.8982455974048
At time: 162.4211916923523 and batch: 350, loss is 4.89526858329773 and perplexity is 133.6558995364699
At time: 162.83594846725464 and batch: 400, loss is 4.8269116497039795 and perplexity is 124.82486185840922
At time: 163.24999380111694 and batch: 450, loss is 4.84954044342041 and perplexity is 127.68169939621981
At time: 163.67499899864197 and batch: 500, loss is 4.820547866821289 and perplexity is 124.03302574801852
At time: 164.09479546546936 and batch: 550, loss is 4.874556369781494 and perplexity is 130.91606200507098
At time: 164.52043890953064 and batch: 600, loss is 4.9159166049957275 and perplexity is 136.44431804576155
At time: 164.96392464637756 and batch: 650, loss is 4.883752756118774 and perplexity is 132.12556971476448
At time: 165.38394117355347 and batch: 700, loss is 4.851720781326294 and perplexity is 127.96039235730872
At time: 165.79045796394348 and batch: 750, loss is 4.820899953842163 and perplexity is 124.0767038553403
At time: 166.2020480632782 and batch: 800, loss is 4.842463779449463 and perplexity is 126.78132848303693
At time: 166.6219606399536 and batch: 850, loss is 4.822598829269409 and perplexity is 124.28767387376244
At time: 167.03920078277588 and batch: 900, loss is 4.922543907165528 and perplexity is 137.35157879571383
At time: 167.4479842185974 and batch: 950, loss is 4.862449951171875 and perplexity is 129.34069263739903
At time: 167.87136840820312 and batch: 1000, loss is 4.835886964797973 and perplexity is 125.95024711406109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.105289366187119 and perplexity of 164.89177773389287
Finished 18 epochs...
Completing Train Step...
At time: 169.18092155456543 and batch: 50, loss is 4.941201076507569 and perplexity is 139.938225178798
At time: 169.63409972190857 and batch: 100, loss is 4.897345895767212 and perplexity is 133.93383318076377
At time: 170.0756938457489 and batch: 150, loss is 4.926486291885376 and perplexity is 137.89414035167357
At time: 170.49496269226074 and batch: 200, loss is 4.9248802185058596 and perplexity is 137.67284999550623
At time: 170.93438816070557 and batch: 250, loss is 4.906170272827149 and perplexity is 135.12094588757662
At time: 171.36651730537415 and batch: 300, loss is 4.812283477783203 and perplexity is 123.01219266079961
At time: 171.80408334732056 and batch: 350, loss is 4.882145109176636 and perplexity is 131.91332909633675
At time: 172.2243766784668 and batch: 400, loss is 4.814644632339477 and perplexity is 123.3029866296309
At time: 172.63702940940857 and batch: 450, loss is 4.837095594406128 and perplexity is 126.10256634209978
At time: 173.061683177948 and batch: 500, loss is 4.809377632141113 and perplexity is 122.65525706786276
At time: 173.4952163696289 and batch: 550, loss is 4.863203077316284 and perplexity is 129.43813918472551
At time: 173.9233899116516 and batch: 600, loss is 4.903676748275757 and perplexity is 134.7844382110329
At time: 174.36345839500427 and batch: 650, loss is 4.8736069393157955 and perplexity is 130.79182529374583
At time: 174.78419065475464 and batch: 700, loss is 4.840632886886596 and perplexity is 126.54941785810985
At time: 175.19758343696594 and batch: 750, loss is 4.8091044521331785 and perplexity is 122.62175468006335
At time: 175.6476709842682 and batch: 800, loss is 4.831489124298096 and perplexity is 125.3975542347002
At time: 176.07708287239075 and batch: 850, loss is 4.8107015323638915 and perplexity is 122.81774792712892
At time: 176.50009489059448 and batch: 900, loss is 4.914151592254639 and perplexity is 136.2037044914208
At time: 176.9197223186493 and batch: 950, loss is 4.850260066986084 and perplexity is 127.77361522444203
At time: 177.3515269756317 and batch: 1000, loss is 4.822976865768433 and perplexity is 124.33466803305954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0996246337890625 and perplexity of 163.96035057192847
Finished 19 epochs...
Completing Train Step...
At time: 178.6883029937744 and batch: 50, loss is 4.928977642059326 and perplexity is 138.2381112402781
At time: 179.12503361701965 and batch: 100, loss is 4.883201847076416 and perplexity is 132.05280059010673
At time: 179.54416155815125 and batch: 150, loss is 4.913843517303467 and perplexity is 136.16175000470523
At time: 179.96870756149292 and batch: 200, loss is 4.913706130981446 and perplexity is 136.14304452763992
At time: 180.38815593719482 and batch: 250, loss is 4.893905935287475 and perplexity is 133.47389762134338
At time: 180.81585955619812 and batch: 300, loss is 4.797520895004272 and perplexity is 121.2095535264359
At time: 181.2428228855133 and batch: 350, loss is 4.8694604396820065 and perplexity is 130.25061986754605
At time: 181.66033172607422 and batch: 400, loss is 4.801931247711182 and perplexity is 121.7453109801754
At time: 182.07111620903015 and batch: 450, loss is 4.827048168182373 and perplexity is 124.84190392186768
At time: 182.50057077407837 and batch: 500, loss is 4.795427837371826 and perplexity is 120.9561202630371
At time: 182.92660689353943 and batch: 550, loss is 4.850617084503174 and perplexity is 127.81924078737707
At time: 183.34597182273865 and batch: 600, loss is 4.88900580406189 and perplexity is 132.82145783358953
At time: 183.77665185928345 and batch: 650, loss is 4.859658861160279 and perplexity is 128.98019444756312
At time: 184.1851236820221 and batch: 700, loss is 4.8279282760620115 and perplexity is 124.95182663004373
At time: 184.59678649902344 and batch: 750, loss is 4.796155395507813 and perplexity is 121.0441548937044
At time: 185.00983834266663 and batch: 800, loss is 4.819415035247803 and perplexity is 123.89259677650035
At time: 185.43271899223328 and batch: 850, loss is 4.797396221160889 and perplexity is 121.19444280751826
At time: 185.8633532524109 and batch: 900, loss is 4.9017275524139405 and perplexity is 134.5219728231588
At time: 186.30360436439514 and batch: 950, loss is 4.8354230403900145 and perplexity is 125.89182927201556
At time: 186.75091242790222 and batch: 1000, loss is 4.8130714702606205 and perplexity is 123.10916354439266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.092013103205983 and perplexity of 162.7170988756432
Finished 20 epochs...
Completing Train Step...
At time: 188.02247738838196 and batch: 50, loss is 4.917453050613403 and perplexity is 136.65411845246507
At time: 188.46529173851013 and batch: 100, loss is 4.870443878173828 and perplexity is 130.3787763473861
At time: 188.88264775276184 and batch: 150, loss is 4.903344945907593 and perplexity is 134.73972383382116
At time: 189.30421328544617 and batch: 200, loss is 4.903321971893311 and perplexity is 134.73662835703934
At time: 189.7189176082611 and batch: 250, loss is 4.882323808670044 and perplexity is 131.93690404777323
At time: 190.12854647636414 and batch: 300, loss is 4.786076946258545 and perplexity is 119.83034445951354
At time: 190.53633332252502 and batch: 350, loss is 4.857560920715332 and perplexity is 128.70988532639728
At time: 190.9525740146637 and batch: 400, loss is 4.7912103462219235 and perplexity is 120.4470631228525
At time: 191.37484502792358 and batch: 450, loss is 4.817147417068481 and perplexity is 123.61197396471039
At time: 191.80234742164612 and batch: 500, loss is 4.784777660369873 and perplexity is 119.67475168555893
At time: 192.2273986339569 and batch: 550, loss is 4.840690689086914 and perplexity is 126.55673290432182
At time: 192.6443374156952 and batch: 600, loss is 4.8782447242736815 and perplexity is 131.39981843465017
At time: 193.06019067764282 and batch: 650, loss is 4.848182315826416 and perplexity is 127.50840905885258
At time: 193.4706711769104 and batch: 700, loss is 4.8164535331726075 and perplexity is 123.5262313577864
At time: 193.8962528705597 and batch: 750, loss is 4.784925088882447 and perplexity is 119.69239645683183
At time: 194.3205485343933 and batch: 800, loss is 4.810069389343262 and perplexity is 122.74013407907809
At time: 194.75660681724548 and batch: 850, loss is 4.788134021759033 and perplexity is 120.07709823387545
At time: 195.18196868896484 and batch: 900, loss is 4.892841300964355 and perplexity is 133.33187234460215
At time: 195.60896253585815 and batch: 950, loss is 4.825044651031494 and perplexity is 124.59203142171557
At time: 196.02778482437134 and batch: 1000, loss is 4.800463151931763 and perplexity is 121.56670833793258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.086362513100228 and perplexity of 161.80024407157453
Finished 21 epochs...
Completing Train Step...
At time: 197.34509921073914 and batch: 50, loss is 4.907478713989258 and perplexity is 135.29785941026677
At time: 197.784823179245 and batch: 100, loss is 4.859619274139404 and perplexity is 128.97508860697627
At time: 198.22227787971497 and batch: 150, loss is 4.89341685295105 and perplexity is 133.4086338566211
At time: 198.641685962677 and batch: 200, loss is 4.893942041397095 and perplexity is 133.47871693152499
At time: 199.06529784202576 and batch: 250, loss is 4.8730957889556885 and perplexity is 130.72498808853388
At time: 199.4914515018463 and batch: 300, loss is 4.777470445632934 and perplexity is 118.80344984817081
At time: 199.90744423866272 and batch: 350, loss is 4.8474793624877925 and perplexity is 127.41880809334064
At time: 200.3390989303589 and batch: 400, loss is 4.780766410827637 and perplexity is 119.19566789733238
At time: 200.7545027732849 and batch: 450, loss is 4.807653913497925 and perplexity is 122.4440160269334
At time: 201.17589378356934 and batch: 500, loss is 4.775898599624634 and perplexity is 118.61685580672246
At time: 201.58991122245789 and batch: 550, loss is 4.830948114395142 and perplexity is 125.32973126415662
At time: 202.0069887638092 and batch: 600, loss is 4.868859758377075 and perplexity is 130.17240424891767
At time: 202.42634844779968 and batch: 650, loss is 4.8380796909332275 and perplexity is 126.22672452149196
At time: 202.83681631088257 and batch: 700, loss is 4.808075742721558 and perplexity is 122.4956773865221
At time: 203.2455050945282 and batch: 750, loss is 4.777095651626587 and perplexity is 118.75893137038474
At time: 203.66467571258545 and batch: 800, loss is 4.803822050094604 and perplexity is 121.97572506945504
At time: 204.07934093475342 and batch: 850, loss is 4.780575761795044 and perplexity is 119.1729455246265
At time: 204.4945411682129 and batch: 900, loss is 4.88436372756958 and perplexity is 132.20631933112492
At time: 204.9274604320526 and batch: 950, loss is 4.815958385467529 and perplexity is 123.46508276785393
At time: 205.3376202583313 and batch: 1000, loss is 4.793983831405639 and perplexity is 120.78158494907554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.082349823742378 and perplexity of 161.15229084103473
Finished 22 epochs...
Completing Train Step...
At time: 206.65563917160034 and batch: 50, loss is 4.89892168045044 and perplexity is 134.14505043640804
At time: 207.08295321464539 and batch: 100, loss is 4.850310363769531 and perplexity is 127.7800419879187
At time: 207.50609683990479 and batch: 150, loss is 4.885399608612061 and perplexity is 132.34334030747956
At time: 207.9359588623047 and batch: 200, loss is 4.8856190586090085 and perplexity is 132.37238624005442
At time: 208.3714497089386 and batch: 250, loss is 4.866057653427124 and perplexity is 129.80815807695743
At time: 208.78166723251343 and batch: 300, loss is 4.769895439147949 and perplexity is 117.90691286883947
At time: 209.19344663619995 and batch: 350, loss is 4.838987741470337 and perplexity is 126.34139682273417
At time: 209.61810994148254 and batch: 400, loss is 4.772690095901489 and perplexity is 118.2368830811601
At time: 210.0502803325653 and batch: 450, loss is 4.800842142105102 and perplexity is 121.61278965742993
At time: 210.4792275428772 and batch: 500, loss is 4.768636827468872 and perplexity is 117.7586072004596
At time: 210.9015076160431 and batch: 550, loss is 4.824164505004883 and perplexity is 124.48242048420386
At time: 211.32624340057373 and batch: 600, loss is 4.8619410419464115 and perplexity is 129.27488671172958
At time: 211.75371313095093 and batch: 650, loss is 4.830931539535523 and perplexity is 125.3276539586704
At time: 212.17928099632263 and batch: 700, loss is 4.801027355194091 and perplexity is 121.63531602388754
At time: 212.59777927398682 and batch: 750, loss is 4.771237068176269 and perplexity is 118.06520636761678
At time: 213.02731227874756 and batch: 800, loss is 4.797653713226318 and perplexity is 121.22565343298713
At time: 213.45695185661316 and batch: 850, loss is 4.775793619155884 and perplexity is 118.6044040072075
At time: 213.87956714630127 and batch: 900, loss is 4.878388748168946 and perplexity is 131.41874451120904
At time: 214.30512881278992 and batch: 950, loss is 4.810273189544677 and perplexity is 122.76515109227472
At time: 214.72431325912476 and batch: 1000, loss is 4.7891238498687745 and perplexity is 120.19601276380673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0799143721417686 and perplexity of 160.76028977993028
Finished 23 epochs...
Completing Train Step...
At time: 216.05871272087097 and batch: 50, loss is 4.889071836471557 and perplexity is 132.8302286440815
At time: 216.50863933563232 and batch: 100, loss is 4.842383613586426 and perplexity is 126.77116535579552
At time: 216.9339292049408 and batch: 150, loss is 4.878694839477539 and perplexity is 131.45897680375364
At time: 217.34515357017517 and batch: 200, loss is 4.878923435211181 and perplexity is 131.4890312000224
At time: 217.7638156414032 and batch: 250, loss is 4.858679857254028 and perplexity is 128.85398412365058
At time: 218.18518161773682 and batch: 300, loss is 4.763898429870605 and perplexity is 117.20193999574167
At time: 218.60024976730347 and batch: 350, loss is 4.832196474075317 and perplexity is 125.48628554508889
At time: 219.01133561134338 and batch: 400, loss is 4.766248569488526 and perplexity is 117.47770483423456
At time: 219.44903588294983 and batch: 450, loss is 4.793948869705201 and perplexity is 120.77736229330009
At time: 219.8639702796936 and batch: 500, loss is 4.7607363414764405 and perplexity is 116.83192242395327
At time: 220.28453850746155 and batch: 550, loss is 4.81812801361084 and perplexity is 123.73324688916183
At time: 220.699223279953 and batch: 600, loss is 4.853983163833618 and perplexity is 128.25021543231725
At time: 221.12863993644714 and batch: 650, loss is 4.824149913787842 and perplexity is 124.48060414744012
At time: 221.541889667511 and batch: 700, loss is 4.79417667388916 and perplexity is 120.80487901585131
At time: 221.95345783233643 and batch: 750, loss is 4.763267250061035 and perplexity is 117.12798783858328
At time: 222.3790261745453 and batch: 800, loss is 4.791706857681274 and perplexity is 120.50688131891913
At time: 222.79508519172668 and batch: 850, loss is 4.768152647018432 and perplexity is 117.70160458587154
At time: 223.22760581970215 and batch: 900, loss is 4.871149644851685 and perplexity is 130.47082582213702
At time: 223.64562010765076 and batch: 950, loss is 4.8025086307525635 and perplexity is 121.81562495519806
At time: 224.0947711467743 and batch: 1000, loss is 4.781747732162476 and perplexity is 119.31269456024965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.076712259432164 and perplexity of 160.24634051371427
Finished 24 epochs...
Completing Train Step...
At time: 225.40994787216187 and batch: 50, loss is 4.8817595195770265 and perplexity is 131.86247449372033
At time: 225.8568651676178 and batch: 100, loss is 4.835209398269654 and perplexity is 125.86493634750677
At time: 226.28058314323425 and batch: 150, loss is 4.872943725585937 and perplexity is 130.70511111765234
At time: 226.70273566246033 and batch: 200, loss is 4.871738739013672 and perplexity is 130.54770806715595
At time: 227.12560892105103 and batch: 250, loss is 4.851013698577881 and perplexity is 127.86994575177829
At time: 227.55424165725708 and batch: 300, loss is 4.7556538772583 and perplexity is 116.23963477406053
At time: 227.96523261070251 and batch: 350, loss is 4.823673038482666 and perplexity is 124.42125657316981
At time: 228.37644720077515 and batch: 400, loss is 4.757784242630005 and perplexity is 116.48753162848423
At time: 228.78944396972656 and batch: 450, loss is 4.785941200256348 and perplexity is 119.8140790733168
At time: 229.20156931877136 and batch: 500, loss is 4.753870248794556 and perplexity is 116.03249124132876
At time: 229.63171076774597 and batch: 550, loss is 4.809862051010132 and perplexity is 122.71468798233678
At time: 230.08885860443115 and batch: 600, loss is 4.847792472839355 and perplexity is 127.45871048773884
At time: 230.52733159065247 and batch: 650, loss is 4.818319406509399 and perplexity is 123.7569308203279
At time: 230.94238686561584 and batch: 700, loss is 4.7884756565094 and perplexity is 120.11812775151078
At time: 231.3562150001526 and batch: 750, loss is 4.757008075714111 and perplexity is 116.39715293933158
At time: 231.76470708847046 and batch: 800, loss is 4.783417167663575 and perplexity is 119.51204576396908
At time: 232.1991319656372 and batch: 850, loss is 4.7600123310089115 and perplexity is 116.74736550291843
At time: 232.62870526313782 and batch: 900, loss is 4.864102210998535 and perplexity is 129.55457371269307
At time: 233.06420803070068 and batch: 950, loss is 4.795533781051636 and perplexity is 120.9689354783468
At time: 233.48863863945007 and batch: 1000, loss is 4.772240285873413 and perplexity is 118.18371090504725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.072587920398247 and perplexity of 159.58679131366662
Finished 25 epochs...
Completing Train Step...
At time: 234.793226480484 and batch: 50, loss is 4.873761100769043 and perplexity is 130.811989905869
At time: 235.22212862968445 and batch: 100, loss is 4.829154825210571 and perplexity is 125.10518021523671
At time: 235.63088726997375 and batch: 150, loss is 4.866044054031372 and perplexity is 129.80639277644747
At time: 236.04995584487915 and batch: 200, loss is 4.862781343460083 and perplexity is 129.38356224842738
At time: 236.48159551620483 and batch: 250, loss is 4.84453742980957 and perplexity is 127.04450140050402
At time: 236.9047405719757 and batch: 300, loss is 4.747461776733399 and perplexity is 115.29127782247474
At time: 237.31962299346924 and batch: 350, loss is 4.816743154525756 and perplexity is 123.56201237328457
At time: 237.733722448349 and batch: 400, loss is 4.751027717590332 and perplexity is 115.70313359073802
At time: 238.14618372917175 and batch: 450, loss is 4.777968263626098 and perplexity is 118.8626070666974
At time: 238.55895113945007 and batch: 500, loss is 4.747316045761108 and perplexity is 115.27447753665177
At time: 238.98502373695374 and batch: 550, loss is 4.800865163803101 and perplexity is 121.61558942257373
At time: 239.40676736831665 and batch: 600, loss is 4.840274887084961 and perplexity is 126.50412130018272
At time: 239.83638834953308 and batch: 650, loss is 4.809239473342895 and perplexity is 122.63831233551004
At time: 240.2491044998169 and batch: 700, loss is 4.782110843658447 and perplexity is 119.35602623789887
At time: 240.7101902961731 and batch: 750, loss is 4.750666208267212 and perplexity is 115.66131338888272
At time: 241.12451362609863 and batch: 800, loss is 4.774210691452026 and perplexity is 118.41681032309529
At time: 241.54555988311768 and batch: 850, loss is 4.752860269546509 and perplexity is 115.91535999309757
At time: 241.97272968292236 and batch: 900, loss is 4.8575029468536375 and perplexity is 128.70242373359744
At time: 242.39685559272766 and batch: 950, loss is 4.787620639801025 and perplexity is 120.0154686391868
At time: 242.80403327941895 and batch: 1000, loss is 4.765097818374634 and perplexity is 117.3425949883516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.067829783369855 and perplexity of 158.8292591428128
Finished 26 epochs...
Completing Train Step...
At time: 244.11624360084534 and batch: 50, loss is 4.86538423538208 and perplexity is 129.72077234778035
At time: 244.546124458313 and batch: 100, loss is 4.822018232345581 and perplexity is 124.21553377682727
At time: 244.95884656906128 and batch: 150, loss is 4.856993703842163 and perplexity is 128.6368996090186
At time: 245.37807703018188 and batch: 200, loss is 4.855823154449463 and perplexity is 128.4864118581581
At time: 245.80671048164368 and batch: 250, loss is 4.8374413394927975 and perplexity is 126.14617322282702
At time: 246.24451518058777 and batch: 300, loss is 4.740596933364868 and perplexity is 114.50253166541862
At time: 246.6685824394226 and batch: 350, loss is 4.808472862243653 and perplexity is 122.5443324716872
At time: 247.08911776542664 and batch: 400, loss is 4.743142700195312 and perplexity is 114.79439976897142
At time: 247.51163387298584 and batch: 450, loss is 4.769020624160767 and perplexity is 117.8038112383705
At time: 247.94115686416626 and batch: 500, loss is 4.7385446739196775 and perplexity is 114.2677837275384
At time: 248.35152220726013 and batch: 550, loss is 4.793548679351806 and perplexity is 120.72903802809921
At time: 248.78267455101013 and batch: 600, loss is 4.8316850662231445 and perplexity is 125.42212728023966
At time: 249.19955611228943 and batch: 650, loss is 4.800018692016602 and perplexity is 121.51268881470394
At time: 249.61493134498596 and batch: 700, loss is 4.774809322357178 and perplexity is 118.4877195075412
At time: 250.03608226776123 and batch: 750, loss is 4.74128758430481 and perplexity is 114.58164026160763
At time: 250.486022233963 and batch: 800, loss is 4.76581844329834 and perplexity is 117.42718546225379
At time: 250.9102463722229 and batch: 850, loss is 4.74588788986206 and perplexity is 115.10996511420345
At time: 251.3392140865326 and batch: 900, loss is 4.849760589599609 and perplexity is 127.70981112872725
At time: 251.79882955551147 and batch: 950, loss is 4.780766849517822 and perplexity is 119.19572018731351
At time: 252.21191477775574 and batch: 1000, loss is 4.7580454063415525 and perplexity is 116.5179579175417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.064683867663872 and perplexity of 158.33038080794122
Finished 27 epochs...
Completing Train Step...
At time: 253.51454067230225 and batch: 50, loss is 4.857035083770752 and perplexity is 128.64222270487218
At time: 253.97192454338074 and batch: 100, loss is 4.811663789749145 and perplexity is 122.93598709124298
At time: 254.39146995544434 and batch: 150, loss is 4.849108753204345 and perplexity is 127.62659235126183
At time: 254.7994408607483 and batch: 200, loss is 4.847559070587158 and perplexity is 127.42896480913804
At time: 255.23366904258728 and batch: 250, loss is 4.830584163665772 and perplexity is 125.28412571664083
At time: 255.64619040489197 and batch: 300, loss is 4.732449493408203 and perplexity is 113.57341924720342
At time: 256.05308294296265 and batch: 350, loss is 4.7998899269104 and perplexity is 121.4970432277483
At time: 256.4792003631592 and batch: 400, loss is 4.734679746627807 and perplexity is 113.8269994000075
At time: 256.8943922519684 and batch: 450, loss is 4.761519861221314 and perplexity is 116.92349841311686
At time: 257.32575821876526 and batch: 500, loss is 4.730293645858764 and perplexity is 113.32883600645674
At time: 257.7429599761963 and batch: 550, loss is 4.785831708908081 and perplexity is 119.80096118641835
At time: 258.1584894657135 and batch: 600, loss is 4.825194835662842 and perplexity is 124.61074463520691
At time: 258.563937664032 and batch: 650, loss is 4.791868877410889 and perplexity is 120.52640739300905
At time: 258.9900412559509 and batch: 700, loss is 4.7673350429534915 and perplexity is 117.6054106051655
At time: 259.4053874015808 and batch: 750, loss is 4.73258192062378 and perplexity is 113.58846045478847
At time: 259.8355612754822 and batch: 800, loss is 4.759507999420166 and perplexity is 116.68850096346355
At time: 260.25927114486694 and batch: 850, loss is 4.739195613861084 and perplexity is 114.34218940616945
At time: 260.6787815093994 and batch: 900, loss is 4.843335409164428 and perplexity is 126.89188303056055
At time: 261.1038761138916 and batch: 950, loss is 4.772596845626831 and perplexity is 118.22585797339318
At time: 261.51010751724243 and batch: 1000, loss is 4.750085315704346 and perplexity is 115.59414610250103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.063645153510861 and perplexity of 158.16600618444735
Finished 28 epochs...
Completing Train Step...
At time: 262.92624139785767 and batch: 50, loss is 4.849340791702271 and perplexity is 127.65621007013895
At time: 263.33818459510803 and batch: 100, loss is 4.803687343597412 and perplexity is 121.95929525341477
At time: 263.7561972141266 and batch: 150, loss is 4.842767581939698 and perplexity is 126.81985081763966
At time: 264.16845321655273 and batch: 200, loss is 4.841000833511353 and perplexity is 126.59598985675503
At time: 264.5873076915741 and batch: 250, loss is 4.823881425857544 and perplexity is 124.44718709390882
At time: 265.0050437450409 and batch: 300, loss is 4.726161451339721 and perplexity is 112.86150542686465
At time: 265.4264452457428 and batch: 350, loss is 4.794804725646973 and perplexity is 120.8807745631408
At time: 265.84926772117615 and batch: 400, loss is 4.727398662567139 and perplexity is 113.00122536223603
At time: 266.2686643600464 and batch: 450, loss is 4.754786148071289 and perplexity is 116.13881399917014
At time: 266.6815354824066 and batch: 500, loss is 4.724417333602905 and perplexity is 112.6648332330322
At time: 267.1145396232605 and batch: 550, loss is 4.776178197860718 and perplexity is 118.6500255072546
At time: 267.5413761138916 and batch: 600, loss is 4.816548566818238 and perplexity is 123.53797106370799
At time: 267.97369742393494 and batch: 650, loss is 4.782832241058349 and perplexity is 119.44216042964504
At time: 268.3973526954651 and batch: 700, loss is 4.759819040298462 and perplexity is 116.72480150247331
At time: 268.80828833580017 and batch: 750, loss is 4.725727252960205 and perplexity is 112.81251178134896
At time: 269.2295022010803 and batch: 800, loss is 4.752553100585938 and perplexity is 115.87975986036206
At time: 269.6391091346741 and batch: 850, loss is 4.731558456420898 and perplexity is 113.47226620212426
At time: 270.06352138519287 and batch: 900, loss is 4.837533674240112 and perplexity is 126.15782143561538
At time: 270.4818682670593 and batch: 950, loss is 4.763762140274048 and perplexity is 117.18596767907898
At time: 270.89702129364014 and batch: 1000, loss is 4.742042169570923 and perplexity is 114.66813450865423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0614735673113564 and perplexity of 157.82290773694484
Finished 29 epochs...
Completing Train Step...
At time: 272.26952171325684 and batch: 50, loss is 4.841400260925293 and perplexity is 126.64656586564854
At time: 272.70597219467163 and batch: 100, loss is 4.797463970184326 and perplexity is 121.20265389080781
At time: 273.13775873184204 and batch: 150, loss is 4.836768770217896 and perplexity is 126.06135970725036
At time: 273.5845522880554 and batch: 200, loss is 4.835384893417358 and perplexity is 125.88702697144386
At time: 274.01009488105774 and batch: 250, loss is 4.815635929107666 and perplexity is 123.4252770848371
At time: 274.4287052154541 and batch: 300, loss is 4.718812942504883 and perplexity is 112.03518149995521
At time: 274.84109926223755 and batch: 350, loss is 4.787955493927002 and perplexity is 120.05566304329712
At time: 275.2533366680145 and batch: 400, loss is 4.7190642929077145 and perplexity is 112.06334512727781
At time: 275.68414068222046 and batch: 450, loss is 4.749010925292969 and perplexity is 115.47001955244487
At time: 276.10845708847046 and batch: 500, loss is 4.717651453018188 and perplexity is 111.9051293562226
At time: 276.54044485092163 and batch: 550, loss is 4.769414701461792 and perplexity is 117.85024419483997
At time: 276.9675486087799 and batch: 600, loss is 4.8106901073455814 and perplexity is 122.81634474012579
At time: 277.3908154964447 and batch: 650, loss is 4.7766407775878905 and perplexity is 118.70492330000685
At time: 277.8123037815094 and batch: 700, loss is 4.753451871871948 and perplexity is 115.98395607842473
At time: 278.23453855514526 and batch: 750, loss is 4.7201479721069335 and perplexity is 112.18485166855177
At time: 278.65390849113464 and batch: 800, loss is 4.746211929321289 and perplexity is 115.1472713290673
At time: 279.07810163497925 and batch: 850, loss is 4.724969034194946 and perplexity is 112.72700763747991
At time: 279.49502062797546 and batch: 900, loss is 4.830354337692261 and perplexity is 125.25533547898146
At time: 279.9237856864929 and batch: 950, loss is 4.759182748794555 and perplexity is 116.65055412697426
At time: 280.35964918136597 and batch: 1000, loss is 4.7376907920837406 and perplexity is 114.17025418785774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060235000238186 and perplexity of 157.62755448405662
Finished 30 epochs...
Completing Train Step...
At time: 281.6765124797821 and batch: 50, loss is 4.837462463378906 and perplexity is 126.1488379483678
At time: 282.12681889533997 and batch: 100, loss is 4.790634546279907 and perplexity is 120.37772967390389
At time: 282.5465655326843 and batch: 150, loss is 4.829662523269653 and perplexity is 125.16871199853001
At time: 282.9759199619293 and batch: 200, loss is 4.826907396316528 and perplexity is 124.82433093103732
At time: 283.38982248306274 and batch: 250, loss is 4.808978576660156 and perplexity is 122.60632058009783
At time: 283.81517577171326 and batch: 300, loss is 4.712481632232666 and perplexity is 111.32809276555685
At time: 284.23356342315674 and batch: 350, loss is 4.781440401077271 and perplexity is 119.27603169445948
At time: 284.68225502967834 and batch: 400, loss is 4.712806091308594 and perplexity is 111.36422003625393
At time: 285.1095209121704 and batch: 450, loss is 4.741681337356567 and perplexity is 114.62676601575473
At time: 285.5365061759949 and batch: 500, loss is 4.710320501327515 and perplexity is 111.08775797468181
At time: 285.946982383728 and batch: 550, loss is 4.7607482814788815 and perplexity is 116.83331740572022
At time: 286.3688168525696 and batch: 600, loss is 4.803398027420044 and perplexity is 121.92401556005662
At time: 286.79710721969604 and batch: 650, loss is 4.771345252990723 and perplexity is 118.07797992100087
At time: 287.2116644382477 and batch: 700, loss is 4.746715831756592 and perplexity is 115.20530894092738
At time: 287.653103351593 and batch: 750, loss is 4.713240365982056 and perplexity is 111.4125931994051
At time: 288.06636023521423 and batch: 800, loss is 4.738743991851806 and perplexity is 114.29056161584532
At time: 288.48971343040466 and batch: 850, loss is 4.718398237228394 and perplexity is 111.98872955163456
At time: 288.904004573822 and batch: 900, loss is 4.823376226425171 and perplexity is 124.3843323240624
At time: 289.33842611312866 and batch: 950, loss is 4.752529850006104 and perplexity is 115.87706562007573
At time: 289.7678413391113 and batch: 1000, loss is 4.7306201362609865 and perplexity is 113.3658428245646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0564727783203125 and perplexity of 157.035638800974
Finished 31 epochs...
Completing Train Step...
At time: 291.1420798301697 and batch: 50, loss is 4.829437141418457 and perplexity is 125.1405044213653
At time: 291.56006026268005 and batch: 100, loss is 4.784348497390747 and perplexity is 119.6234027319225
At time: 291.971556186676 and batch: 150, loss is 4.823116111755371 and perplexity is 124.3519823420665
At time: 292.3898775577545 and batch: 200, loss is 4.820649003982544 and perplexity is 124.04557073051576
At time: 292.8280282020569 and batch: 250, loss is 4.801787376403809 and perplexity is 121.72779658305792
At time: 293.2370069026947 and batch: 300, loss is 4.707645263671875 and perplexity is 110.79096898900976
At time: 293.6464858055115 and batch: 350, loss is 4.776083145141602 and perplexity is 118.6387480356926
At time: 294.0783829689026 and batch: 400, loss is 4.706286315917969 and perplexity is 110.64051210522574
At time: 294.49775195121765 and batch: 450, loss is 4.734555625915528 and perplexity is 113.81287198853563
At time: 294.923956155777 and batch: 500, loss is 4.705049734115601 and perplexity is 110.50378061860927
At time: 295.37005066871643 and batch: 550, loss is 4.7543619346618655 and perplexity is 116.08955680543691
At time: 295.78830885887146 and batch: 600, loss is 4.795376882553101 and perplexity is 120.94995712287756
At time: 296.1985743045807 and batch: 650, loss is 4.7636428070068355 and perplexity is 117.17198432904037
At time: 296.6230580806732 and batch: 700, loss is 4.738282289505005 and perplexity is 114.23780557506109
At time: 297.0437819957733 and batch: 750, loss is 4.705942783355713 and perplexity is 110.6025100144645
At time: 297.46051383018494 and batch: 800, loss is 4.730329437255859 and perplexity is 113.33289227641791
At time: 297.8729758262634 and batch: 850, loss is 4.710429239273071 and perplexity is 111.09983808603177
At time: 298.2884576320648 and batch: 900, loss is 4.815975484848022 and perplexity is 123.46719396233178
At time: 298.7114899158478 and batch: 950, loss is 4.745053787231445 and perplexity is 115.01399162092753
At time: 299.1259870529175 and batch: 1000, loss is 4.722515010833741 and perplexity is 112.45071208382997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0525639231612045 and perplexity of 156.42300735791179
Finished 32 epochs...
Completing Train Step...
At time: 300.4248242378235 and batch: 50, loss is 4.821634654998779 and perplexity is 124.16789664879526
At time: 300.87332940101624 and batch: 100, loss is 4.776195878982544 and perplexity is 118.6521233913567
At time: 301.3085091114044 and batch: 150, loss is 4.814810943603516 and perplexity is 123.32349501053712
At time: 301.7283937931061 and batch: 200, loss is 4.812195310592651 and perplexity is 123.0013474994698
At time: 302.1473686695099 and batch: 250, loss is 4.792274875640869 and perplexity is 120.57535083587979
At time: 302.55666160583496 and batch: 300, loss is 4.698202896118164 and perplexity is 109.74976339713146
At time: 302.97783064842224 and batch: 350, loss is 4.767546005249024 and perplexity is 117.63022352975752
At time: 303.4099907875061 and batch: 400, loss is 4.697137832641602 and perplexity is 109.63293515836467
At time: 303.8369200229645 and batch: 450, loss is 4.7273169612884525 and perplexity is 112.9919933947677
At time: 304.2602252960205 and batch: 500, loss is 4.695793228149414 and perplexity is 109.48562128290321
At time: 304.6727063655853 and batch: 550, loss is 4.747790508270263 and perplexity is 115.32918393154578
At time: 305.1040880680084 and batch: 600, loss is 4.7849930381774906 and perplexity is 119.70052974711572
At time: 305.52612233161926 and batch: 650, loss is 4.754581127166748 and perplexity is 116.11500555516952
At time: 305.9568569660187 and batch: 700, loss is 4.729211835861206 and perplexity is 113.20630202984407
At time: 306.388311624527 and batch: 750, loss is 4.696868267059326 and perplexity is 109.60338587527592
At time: 306.8077902793884 and batch: 800, loss is 4.720044765472412 and perplexity is 112.17327404502086
At time: 307.2299551963806 and batch: 850, loss is 4.703229360580444 and perplexity is 110.30280544128199
At time: 307.6596074104309 and batch: 900, loss is 4.806939506530762 and perplexity is 122.35657240768091
At time: 308.0779812335968 and batch: 950, loss is 4.736406784057618 and perplexity is 114.02375273978141
At time: 308.4955189228058 and batch: 1000, loss is 4.7132639503479 and perplexity is 111.4152208257481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.046538934475038 and perplexity of 155.48339393082512
Finished 33 epochs...
Completing Train Step...
At time: 309.7674448490143 and batch: 50, loss is 4.811095094680786 and perplexity is 122.86609387750694
At time: 310.1908531188965 and batch: 100, loss is 4.765632762908935 and perplexity is 117.4053835608859
At time: 310.620502948761 and batch: 150, loss is 4.8067680358886715 and perplexity is 122.33559364631842
At time: 311.04377031326294 and batch: 200, loss is 4.802536506652832 and perplexity is 121.81902072274023
At time: 311.456583738327 and batch: 250, loss is 4.783283338546753 and perplexity is 119.4960526426315
At time: 311.87625789642334 and batch: 300, loss is 4.687016277313233 and perplexity is 108.52887616196546
At time: 312.3009204864502 and batch: 350, loss is 4.756895627975464 and perplexity is 116.3840650785625
At time: 312.7361078262329 and batch: 400, loss is 4.688007364273071 and perplexity is 108.63649103493539
At time: 313.1694164276123 and batch: 450, loss is 4.717708311080933 and perplexity is 111.91149224597802
At time: 313.5802948474884 and batch: 500, loss is 4.685625057220459 and perplexity is 108.37799358857272
At time: 314.01227283477783 and batch: 550, loss is 4.737067289352417 and perplexity is 114.0990909100902
At time: 314.4347641468048 and batch: 600, loss is 4.7737488937377925 and perplexity is 118.36213833543229
At time: 314.8651909828186 and batch: 650, loss is 4.7441772365570065 and perplexity is 114.91322020108261
At time: 315.29285073280334 and batch: 700, loss is 4.720316047668457 and perplexity is 112.20370878515625
At time: 315.72217988967896 and batch: 750, loss is 4.688210363388062 and perplexity is 108.65854638500369
At time: 316.1582889556885 and batch: 800, loss is 4.710222082138062 and perplexity is 111.07682534558283
At time: 316.57490491867065 and batch: 850, loss is 4.694321727752685 and perplexity is 109.32463162497147
At time: 317.0429127216339 and batch: 900, loss is 4.797701101303101 and perplexity is 121.23139821967615
At time: 317.4753952026367 and batch: 950, loss is 4.728423748016358 and perplexity is 113.11712066524454
At time: 317.9010663032532 and batch: 1000, loss is 4.703388061523437 and perplexity is 110.32031198963645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0398753096417686 and perplexity of 154.45075530515695
Finished 34 epochs...
Completing Train Step...
At time: 319.2080898284912 and batch: 50, loss is 4.800470609664917 and perplexity is 121.56761495338438
At time: 319.64315009117126 and batch: 100, loss is 4.755684394836425 and perplexity is 116.24318218032482
At time: 320.0651569366455 and batch: 150, loss is 4.794873275756836 and perplexity is 120.88906123754043
At time: 320.4982166290283 and batch: 200, loss is 4.791798372268676 and perplexity is 120.51790996107529
At time: 320.9201228618622 and batch: 250, loss is 4.772629928588867 and perplexity is 118.22976929966302
At time: 321.33832359313965 and batch: 300, loss is 4.675956182479858 and perplexity is 107.33515003015636
At time: 321.75619745254517 and batch: 350, loss is 4.746681852340698 and perplexity is 115.20139439832906
At time: 322.1754505634308 and batch: 400, loss is 4.675986928939819 and perplexity is 107.33845025678406
At time: 322.5859248638153 and batch: 450, loss is 4.704811134338379 and perplexity is 110.47741758640335
At time: 322.9955475330353 and batch: 500, loss is 4.673226470947266 and perplexity is 107.04255556419524
At time: 323.4249150753021 and batch: 550, loss is 4.724721250534057 and perplexity is 112.69907918709617
At time: 323.8369035720825 and batch: 600, loss is 4.762793979644775 and perplexity is 117.07256774240369
At time: 324.25908041000366 and batch: 650, loss is 4.73221495628357 and perplexity is 113.54678518747838
At time: 324.6786594390869 and batch: 700, loss is 4.7072200679779055 and perplexity is 110.74387115967163
At time: 325.09850549697876 and batch: 750, loss is 4.678093557357788 and perplexity is 107.56481083153199
At time: 325.5184578895569 and batch: 800, loss is 4.697936382293701 and perplexity is 109.72051746535028
At time: 325.94220519065857 and batch: 850, loss is 4.681055393218994 and perplexity is 107.88387241643218
At time: 326.35946226119995 and batch: 900, loss is 4.787298269271851 and perplexity is 119.9767854245517
At time: 326.77631521224976 and batch: 950, loss is 4.716394004821777 and perplexity is 111.76450288692413
At time: 327.19975066185 and batch: 1000, loss is 4.690734586715698 and perplexity is 108.93317128400498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.031973117735328 and perplexity of 153.23506543008946
Finished 35 epochs...
Completing Train Step...
At time: 328.5101125240326 and batch: 50, loss is 4.786114225387573 and perplexity is 119.8348117136533
At time: 328.9588289260864 and batch: 100, loss is 4.742780532836914 and perplexity is 114.75283251205434
At time: 329.3778827190399 and batch: 150, loss is 4.783614082336426 and perplexity is 119.53558175657754
At time: 329.807981967926 and batch: 200, loss is 4.778265008926391 and perplexity is 118.89788422063097
At time: 330.2218408584595 and batch: 250, loss is 4.75901873588562 and perplexity is 116.63142349914085
At time: 330.6576020717621 and batch: 300, loss is 4.663296308517456 and perplexity is 105.98486580746926
At time: 331.08504033088684 and batch: 350, loss is 4.734658269882202 and perplexity is 113.8245547927496
At time: 331.5123975276947 and batch: 400, loss is 4.663468246459961 and perplexity is 106.00309019391972
At time: 331.9547736644745 and batch: 450, loss is 4.6942715549469 and perplexity is 109.3191466390611
At time: 332.38852882385254 and batch: 500, loss is 4.663096866607666 and perplexity is 105.96373009116766
At time: 332.81431126594543 and batch: 550, loss is 4.713245315551758 and perplexity is 111.41314464516549
At time: 333.2387101650238 and batch: 600, loss is 4.750945701599121 and perplexity is 115.6936444726854
At time: 333.65951323509216 and batch: 650, loss is 4.719687623977661 and perplexity is 112.13321946725893
At time: 334.0728621482849 and batch: 700, loss is 4.696535539627075 and perplexity is 109.56692388841567
At time: 334.496666431427 and batch: 750, loss is 4.665537719726562 and perplexity is 106.22268790267408
At time: 334.91133403778076 and batch: 800, loss is 4.6878932857513425 and perplexity is 108.62409865149817
At time: 335.34782791137695 and batch: 850, loss is 4.670999202728272 and perplexity is 106.80440838937268
At time: 335.7592179775238 and batch: 900, loss is 4.776132173538208 and perplexity is 118.64456484587747
At time: 336.17933440208435 and batch: 950, loss is 4.705976867675782 and perplexity is 110.60627989006268
At time: 336.6019699573517 and batch: 1000, loss is 4.681146812438965 and perplexity is 107.893735526728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.025225755645008 and perplexity of 152.20461328500616
Finished 36 epochs...
Completing Train Step...
At time: 337.8965563774109 and batch: 50, loss is 4.776317453384399 and perplexity is 118.66654932918144
At time: 338.33317613601685 and batch: 100, loss is 4.734211711883545 and perplexity is 113.77373687478165
At time: 338.7601544857025 and batch: 150, loss is 4.7746786403656 and perplexity is 118.4722363080882
At time: 339.1998393535614 and batch: 200, loss is 4.7657755088806155 and perplexity is 117.42214390264996
At time: 339.6321151256561 and batch: 250, loss is 4.750133647918701 and perplexity is 115.59973315856496
At time: 340.06810235977173 and batch: 300, loss is 4.65435827255249 and perplexity is 105.04179016554146
At time: 340.48433351516724 and batch: 350, loss is 4.724507560729981 and perplexity is 112.67499911586955
At time: 340.91034626960754 and batch: 400, loss is 4.653898010253906 and perplexity is 104.99345451414477
At time: 341.3267047405243 and batch: 450, loss is 4.6842381954193115 and perplexity is 108.227792467353
At time: 341.7485280036926 and batch: 500, loss is 4.654226264953613 and perplexity is 105.02792476622915
At time: 342.1749942302704 and batch: 550, loss is 4.703295803070068 and perplexity is 110.31013447776503
At time: 342.60416078567505 and batch: 600, loss is 4.741291160583496 and perplexity is 114.5820500382183
At time: 343.02678203582764 and batch: 650, loss is 4.70969952583313 and perplexity is 111.01879661312714
At time: 343.4559864997864 and batch: 700, loss is 4.686832971572876 and perplexity is 108.50898401919822
At time: 343.8803176879883 and batch: 750, loss is 4.656103429794311 and perplexity is 105.22526465569486
At time: 344.3031539916992 and batch: 800, loss is 4.677531318664551 and perplexity is 107.50435073094842
At time: 344.7301228046417 and batch: 850, loss is 4.661938962936401 and perplexity is 105.84110530661894
At time: 345.16598200798035 and batch: 900, loss is 4.766358423233032 and perplexity is 117.49061090888394
At time: 345.5842332839966 and batch: 950, loss is 4.697171983718872 and perplexity is 109.63667930513758
At time: 345.9996645450592 and batch: 1000, loss is 4.6721773433685305 and perplexity is 106.93031315565284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.023389490639291 and perplexity of 151.92538172994978
Finished 37 epochs...
Completing Train Step...
At time: 347.34259486198425 and batch: 50, loss is 4.767334651947022 and perplexity is 117.60536462069805
At time: 347.7505283355713 and batch: 100, loss is 4.726102342605591 and perplexity is 112.85483452330308
At time: 348.16359066963196 and batch: 150, loss is 4.76516224861145 and perplexity is 117.35015564310008
At time: 348.57107877731323 and batch: 200, loss is 4.757946090698242 and perplexity is 116.50638643621798
At time: 348.9820303916931 and batch: 250, loss is 4.741768503189087 and perplexity is 114.63675798871654
At time: 349.39052510261536 and batch: 300, loss is 4.646600999832153 and perplexity is 104.23010465540709
At time: 349.8280518054962 and batch: 350, loss is 4.71616774559021 and perplexity is 111.73921799696218
At time: 350.2378067970276 and batch: 400, loss is 4.646285047531128 and perplexity is 104.19717811588698
At time: 350.6554672718048 and batch: 450, loss is 4.675371608734131 and perplexity is 107.27242305551258
At time: 351.0665457248688 and batch: 500, loss is 4.644656496047974 and perplexity is 104.02762574678056
At time: 351.47294092178345 and batch: 550, loss is 4.69471920967102 and perplexity is 109.36809482661718
At time: 351.89181995391846 and batch: 600, loss is 4.732251405715942 and perplexity is 113.55092397877405
At time: 352.30865693092346 and batch: 650, loss is 4.702639331817627 and perplexity is 110.23774280975178
At time: 352.7337341308594 and batch: 700, loss is 4.676971426010132 and perplexity is 107.44417668173212
At time: 353.1412732601166 and batch: 750, loss is 4.648724346160889 and perplexity is 104.45165639776958
At time: 353.55510425567627 and batch: 800, loss is 4.66935399055481 and perplexity is 106.6288369422541
At time: 353.9613082408905 and batch: 850, loss is 4.652097072601318 and perplexity is 104.80453801311555
At time: 354.3741080760956 and batch: 900, loss is 4.75713059425354 and perplexity is 116.41141462214577
At time: 354.79608058929443 and batch: 950, loss is 4.6881450843811034 and perplexity is 108.6514534945091
At time: 355.2039523124695 and batch: 1000, loss is 4.663999300003052 and perplexity is 106.05939846057476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0212182766053735 and perplexity of 151.59587705106605
Finished 38 epochs...
Completing Train Step...
At time: 356.50502347946167 and batch: 50, loss is 4.7582954406738285 and perplexity is 116.54709504983768
At time: 356.9312815666199 and batch: 100, loss is 4.716667404174805 and perplexity is 111.79506340712777
At time: 357.34282994270325 and batch: 150, loss is 4.758067541122436 and perplexity is 116.52053704555333
At time: 357.7498641014099 and batch: 200, loss is 4.750034694671631 and perplexity is 115.58829475555137
At time: 358.15799856185913 and batch: 250, loss is 4.733769817352295 and perplexity is 113.7234719893637
At time: 358.58005833625793 and batch: 300, loss is 4.639283752441406 and perplexity is 103.47021075045048
At time: 358.998477935791 and batch: 350, loss is 4.707069025039673 and perplexity is 110.72714534317048
At time: 359.4152481555939 and batch: 400, loss is 4.63839695930481 and perplexity is 103.37849475028504
At time: 359.82579708099365 and batch: 450, loss is 4.667296400070191 and perplexity is 106.40966402335754
At time: 360.2377769947052 and batch: 500, loss is 4.637623338699341 and perplexity is 103.29854994404575
At time: 360.671523809433 and batch: 550, loss is 4.687816047668457 and perplexity is 108.61570905836543
At time: 361.10696291923523 and batch: 600, loss is 4.72564172744751 and perplexity is 112.80286384601864
At time: 361.53199768066406 and batch: 650, loss is 4.695611886978149 and perplexity is 109.46576883219082
At time: 361.96378350257874 and batch: 700, loss is 4.667821617126465 and perplexity is 106.465566873135
At time: 362.3856027126312 and batch: 750, loss is 4.639427375793457 and perplexity is 103.48507255618154
At time: 362.8028621673584 and batch: 800, loss is 4.661098031997681 and perplexity is 105.75213765964462
At time: 363.211795091629 and batch: 850, loss is 4.646148777008056 and perplexity is 104.18298007933143
At time: 363.6287953853607 and batch: 900, loss is 4.7475859069824216 and perplexity is 115.30558984576011
At time: 364.05194568634033 and batch: 950, loss is 4.679349021911621 and perplexity is 107.69993944561429
At time: 364.479704618454 and batch: 1000, loss is 4.6569728088378906 and perplexity is 105.31678507284357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.015511861661586 and perplexity of 150.73327160590947
Finished 39 epochs...
Completing Train Step...
At time: 365.8378474712372 and batch: 50, loss is 4.74875638961792 and perplexity is 115.44063205331237
At time: 366.27263832092285 and batch: 100, loss is 4.708733396530151 and perplexity is 110.9115898966519
At time: 366.6943154335022 and batch: 150, loss is 4.750779809951783 and perplexity is 115.67445345527602
At time: 367.10868310928345 and batch: 200, loss is 4.741830635070801 and perplexity is 114.64388080747867
At time: 367.53480529785156 and batch: 250, loss is 4.726447677612304 and perplexity is 112.89381397843864
At time: 367.9528555870056 and batch: 300, loss is 4.633264636993408 and perplexity is 102.84928220149813
At time: 368.38693952560425 and batch: 350, loss is 4.700756206512451 and perplexity is 110.03034666443428
At time: 368.8142099380493 and batch: 400, loss is 4.6289066696167 and perplexity is 102.40204361834851
At time: 369.2368149757385 and batch: 450, loss is 4.659595375061035 and perplexity is 105.59334780963869
At time: 369.6545283794403 and batch: 500, loss is 4.6289473915100094 and perplexity is 102.40621370834985
At time: 370.0898177623749 and batch: 550, loss is 4.679150276184082 and perplexity is 107.67853666971881
At time: 370.51730966567993 and batch: 600, loss is 4.7176776790618895 and perplexity is 111.9080642235203
At time: 370.94802713394165 and batch: 650, loss is 4.687791175842285 and perplexity is 108.61300762092516
At time: 371.3707768917084 and batch: 700, loss is 4.660090227127075 and perplexity is 105.64561382686907
At time: 371.78601121902466 and batch: 750, loss is 4.6306185626983645 and perplexity is 102.57749510261029
At time: 372.20277190208435 and batch: 800, loss is 4.653600034713745 and perplexity is 104.96217369351369
At time: 372.62476778030396 and batch: 850, loss is 4.638267030715943 and perplexity is 103.36506380089395
At time: 373.0501184463501 and batch: 900, loss is 4.73916916847229 and perplexity is 114.33916562249775
At time: 373.46282410621643 and batch: 950, loss is 4.672191762924195 and perplexity is 106.93185505437229
At time: 373.89036083221436 and batch: 1000, loss is 4.649507808685303 and perplexity is 104.53352242146342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0132624928544205 and perplexity of 150.39459793035576
Finished 40 epochs...
Completing Train Step...
At time: 375.20708775520325 and batch: 50, loss is 4.743162040710449 and perplexity is 114.79661997326765
At time: 375.6358690261841 and batch: 100, loss is 4.699840517044067 and perplexity is 109.92963915023294
At time: 376.05575823783875 and batch: 150, loss is 4.742333946228027 and perplexity is 114.70159687514881
At time: 376.4793367385864 and batch: 200, loss is 4.734112615585327 and perplexity is 113.76246287723788
At time: 376.9057357311249 and batch: 250, loss is 4.717984704971314 and perplexity is 111.94242817375202
At time: 377.3280861377716 and batch: 300, loss is 4.6260892772674564 and perplexity is 102.1139429210329
At time: 377.7442569732666 and batch: 350, loss is 4.69117169380188 and perplexity is 108.98079715313759
At time: 378.16580605506897 and batch: 400, loss is 4.6220815563201905 and perplexity is 101.70551770679472
At time: 378.5958640575409 and batch: 450, loss is 4.651804361343384 and perplexity is 104.77386503433861
At time: 379.01596665382385 and batch: 500, loss is 4.621342554092407 and perplexity is 101.63038486771858
At time: 379.443284034729 and batch: 550, loss is 4.6707314682006835 and perplexity is 106.77581698917153
At time: 379.85798621177673 and batch: 600, loss is 4.708970699310303 and perplexity is 110.93791264839295
At time: 380.2765488624573 and batch: 650, loss is 4.681236152648926 and perplexity is 107.90337520631248
At time: 380.68828892707825 and batch: 700, loss is 4.6537657070159915 and perplexity is 104.97956445902261
At time: 381.1229817867279 and batch: 750, loss is 4.622529544830322 and perplexity is 101.75109081749717
At time: 381.55437660217285 and batch: 800, loss is 4.645650463104248 and perplexity is 104.13107718485874
At time: 381.9732186794281 and batch: 850, loss is 4.631959419250489 and perplexity is 102.71512906209634
At time: 382.4111409187317 and batch: 900, loss is 4.73306001663208 and perplexity is 113.64277962817619
At time: 382.83723187446594 and batch: 950, loss is 4.664275712966919 and perplexity is 106.0887187053104
At time: 383.2474992275238 and batch: 1000, loss is 4.64139157295227 and perplexity is 103.68853739879368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.009509575076219 and perplexity of 149.83123715458893
Finished 41 epochs...
Completing Train Step...
At time: 384.5279612541199 and batch: 50, loss is 4.734432601928711 and perplexity is 113.79887113651054
At time: 384.9651598930359 and batch: 100, loss is 4.691235446929932 and perplexity is 108.98774524133256
At time: 385.38642597198486 and batch: 150, loss is 4.734876079559326 and perplexity is 113.8493495824523
At time: 385.8074588775635 and batch: 200, loss is 4.72632568359375 and perplexity is 112.88004244844068
At time: 386.2221531867981 and batch: 250, loss is 4.710786333084107 and perplexity is 111.1395182349655
At time: 386.656165599823 and batch: 300, loss is 4.618864870071411 and perplexity is 101.37888857991054
At time: 387.0821385383606 and batch: 350, loss is 4.684278554916382 and perplexity is 108.23216057477275
At time: 387.5053114891052 and batch: 400, loss is 4.615050220489502 and perplexity is 100.99290031801632
At time: 387.9315776824951 and batch: 450, loss is 4.644791517257691 and perplexity is 104.04167263094516
At time: 388.35120248794556 and batch: 500, loss is 4.6133055496215825 and perplexity is 100.81685456257034
At time: 388.7724995613098 and batch: 550, loss is 4.66433165550232 and perplexity is 106.09465374322114
At time: 389.19994258880615 and batch: 600, loss is 4.701436967849731 and perplexity is 110.1052765721693
At time: 389.6228437423706 and batch: 650, loss is 4.67385573387146 and perplexity is 107.109934673087
At time: 390.0359661579132 and batch: 700, loss is 4.647094974517822 and perplexity is 104.28160440733113
At time: 390.4625792503357 and batch: 750, loss is 4.616061697006225 and perplexity is 101.09510394461428
At time: 390.86831307411194 and batch: 800, loss is 4.637854175567627 and perplexity is 103.32239781019078
At time: 391.27789545059204 and batch: 850, loss is 4.622708368301391 and perplexity is 101.76928792772894
At time: 391.6856849193573 and batch: 900, loss is 4.725752649307251 and perplexity is 112.81537684343017
At time: 392.1028187274933 and batch: 950, loss is 4.6571485042572025 and perplexity is 105.33529037515835
At time: 392.52478313446045 and batch: 1000, loss is 4.634960308074951 and perplexity is 103.0238286999868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.008409081435785 and perplexity of 149.6664395269645
Finished 42 epochs...
Completing Train Step...
At time: 393.9170575141907 and batch: 50, loss is 4.727964000701904 and perplexity is 113.06512732561495
At time: 394.35935854911804 and batch: 100, loss is 4.684583292007447 and perplexity is 108.26514795452972
At time: 394.78186774253845 and batch: 150, loss is 4.727135457992554 and perplexity is 112.97148683661663
At time: 395.20232486724854 and batch: 200, loss is 4.720729885101318 and perplexity is 112.2501524893662
At time: 395.6169400215149 and batch: 250, loss is 4.704042406082153 and perplexity is 110.39252310840249
At time: 396.035875082016 and batch: 300, loss is 4.610697383880615 and perplexity is 100.55425010322983
At time: 396.44572925567627 and batch: 350, loss is 4.675122709274292 and perplexity is 107.24572632989654
At time: 396.86817812919617 and batch: 400, loss is 4.606095304489136 and perplexity is 100.09255465551551
At time: 397.27705001831055 and batch: 450, loss is 4.637415943145752 and perplexity is 103.27712850552743
At time: 397.7122468948364 and batch: 500, loss is 4.605432558059692 and perplexity is 100.02624064941632
At time: 398.13092494010925 and batch: 550, loss is 4.6573270893096925 and perplexity is 105.35410336332829
At time: 398.55690717697144 and batch: 600, loss is 4.694924945831299 and perplexity is 109.39059811329435
At time: 398.96870613098145 and batch: 650, loss is 4.667077693939209 and perplexity is 106.38639412216773
At time: 399.3823835849762 and batch: 700, loss is 4.6403326988220215 and perplexity is 103.57880239696914
At time: 399.7936792373657 and batch: 750, loss is 4.60766206741333 and perplexity is 100.24949887422436
At time: 400.2171609401703 and batch: 800, loss is 4.63107855796814 and perplexity is 102.62469111928289
At time: 400.6382386684418 and batch: 850, loss is 4.616394834518433 and perplexity is 101.12878812645948
At time: 401.05103731155396 and batch: 900, loss is 4.720736789703369 and perplexity is 112.25092753467499
At time: 401.4764230251312 and batch: 950, loss is 4.651928453445435 and perplexity is 104.78686745022259
At time: 401.89715480804443 and batch: 1000, loss is 4.629722280502319 and perplexity is 102.48559790909603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.007284769197789 and perplexity of 149.49826227697307
Finished 43 epochs...
Completing Train Step...
At time: 403.2882487773895 and batch: 50, loss is 4.7219313049316405 and perplexity is 112.38509309245066
At time: 403.69750452041626 and batch: 100, loss is 4.678836345672607 and perplexity is 107.64473839705936
At time: 404.12958121299744 and batch: 150, loss is 4.720981760025024 and perplexity is 112.27842904888912
At time: 404.55542039871216 and batch: 200, loss is 4.711620082855225 and perplexity is 111.23221942228291
At time: 404.9880213737488 and batch: 250, loss is 4.695822410583496 and perplexity is 109.48881638644941
At time: 405.4152750968933 and batch: 300, loss is 4.60364372253418 and perplexity is 99.84747009988531
At time: 405.8414990901947 and batch: 350, loss is 4.669160804748535 and perplexity is 106.60823975402354
At time: 406.2677626609802 and batch: 400, loss is 4.600135850906372 and perplexity is 99.49783159443996
At time: 406.7249336242676 and batch: 450, loss is 4.629747591018677 and perplexity is 102.48819190532586
At time: 407.1553337574005 and batch: 500, loss is 4.600319166183471 and perplexity is 99.51607273889863
At time: 407.58898401260376 and batch: 550, loss is 4.651716947555542 and perplexity is 104.76470675421493
At time: 408.01643681526184 and batch: 600, loss is 4.689632501602173 and perplexity is 108.81318378792503
At time: 408.4289813041687 and batch: 650, loss is 4.660355844497681 and perplexity is 105.67367886414551
At time: 408.8501167297363 and batch: 700, loss is 4.634721593856812 and perplexity is 102.99923838241496
At time: 409.26035737991333 and batch: 750, loss is 4.600105829238892 and perplexity is 99.49484454846313
At time: 409.69233441352844 and batch: 800, loss is 4.625198707580567 and perplexity is 102.023043820858
At time: 410.10882568359375 and batch: 850, loss is 4.611162605285645 and perplexity is 100.60104097595841
At time: 410.53096747398376 and batch: 900, loss is 4.713885154724121 and perplexity is 111.4844539502362
At time: 410.94352316856384 and batch: 950, loss is 4.645445871353149 and perplexity is 104.10977500463316
At time: 411.37820410728455 and batch: 1000, loss is 4.624736766815186 and perplexity is 101.97592610154446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.004961339438834 and perplexity of 149.15131677331766
Finished 44 epochs...
Completing Train Step...
At time: 412.72815895080566 and batch: 50, loss is 4.715734157562256 and perplexity is 111.69077971167496
At time: 413.1831841468811 and batch: 100, loss is 4.672767763137817 and perplexity is 106.99346556785773
At time: 413.6070532798767 and batch: 150, loss is 4.715311832427979 and perplexity is 111.64361984723469
At time: 414.0294408798218 and batch: 200, loss is 4.705444765090943 and perplexity is 110.54744165800967
At time: 414.44314312934875 and batch: 250, loss is 4.690497350692749 and perplexity is 108.90733147687007
At time: 414.8826642036438 and batch: 300, loss is 4.5978235912323 and perplexity is 99.26803255070338
At time: 415.31125807762146 and batch: 350, loss is 4.663308563232422 and perplexity is 105.98616462974881
At time: 415.7245137691498 and batch: 400, loss is 4.59482894897461 and perplexity is 98.97120497361831
At time: 416.14219331741333 and batch: 450, loss is 4.624353113174439 and perplexity is 101.93681017019158
At time: 416.55017495155334 and batch: 500, loss is 4.594381284713745 and perplexity is 98.92690901789483
At time: 416.9685139656067 and batch: 550, loss is 4.646349010467529 and perplexity is 104.20384308651722
At time: 417.39409947395325 and batch: 600, loss is 4.6838772678375244 and perplexity is 108.18873712043553
At time: 417.8158619403839 and batch: 650, loss is 4.653788452148437 and perplexity is 104.98195226027568
At time: 418.2263629436493 and batch: 700, loss is 4.625785408020019 and perplexity is 102.08291834799019
At time: 418.6410348415375 and batch: 750, loss is 4.594689416885376 and perplexity is 98.95739627801481
At time: 419.0635094642639 and batch: 800, loss is 4.619256114959716 and perplexity is 101.41856031202468
At time: 419.4800992012024 and batch: 850, loss is 4.604017095565796 and perplexity is 99.8847574131011
At time: 419.89292073249817 and batch: 900, loss is 4.707811288833618 and perplexity is 110.80936460458109
At time: 420.30433773994446 and batch: 950, loss is 4.637948198318481 and perplexity is 103.33211292297142
At time: 420.72500824928284 and batch: 1000, loss is 4.617862701416016 and perplexity is 101.27734072799312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.002599576624428 and perplexity of 148.79947238968037
Finished 45 epochs...
Completing Train Step...
At time: 422.02671456336975 and batch: 50, loss is 4.710122604370117 and perplexity is 111.06577622050732
At time: 422.4722216129303 and batch: 100, loss is 4.666203403472901 and perplexity is 106.29342216021396
At time: 422.8905212879181 and batch: 150, loss is 4.70710940361023 and perplexity is 110.73161643728882
At time: 423.31792426109314 and batch: 200, loss is 4.698480281829834 and perplexity is 109.7802106359763
At time: 423.7392928600311 and batch: 250, loss is 4.684381856918335 and perplexity is 108.24334175114478
At time: 424.16458201408386 and batch: 300, loss is 4.590204830169678 and perplexity is 98.51460685911633
At time: 424.58497738838196 and batch: 350, loss is 4.656645545959472 and perplexity is 105.28232443776577
At time: 425.00749611854553 and batch: 400, loss is 4.587191200256347 and perplexity is 98.21816719705676
At time: 425.4160809516907 and batch: 450, loss is 4.61810486793518 and perplexity is 101.30186967899296
At time: 425.84054374694824 and batch: 500, loss is 4.5870676612854 and perplexity is 98.20603417521886
At time: 426.2510721683502 and batch: 550, loss is 4.639815464019775 and perplexity is 103.52524168851974
At time: 426.66062927246094 and batch: 600, loss is 4.676580266952515 and perplexity is 107.40215713753324
At time: 427.0996398925781 and batch: 650, loss is 4.648385601043701 and perplexity is 104.41627990132868
At time: 427.51710629463196 and batch: 700, loss is 4.620296430587769 and perplexity is 101.52412252478123
At time: 427.9360861778259 and batch: 750, loss is 4.586766958236694 and perplexity is 98.17650776090497
At time: 428.3487706184387 and batch: 800, loss is 4.6104514598846436 and perplexity is 100.52952444067398
At time: 428.7845788002014 and batch: 850, loss is 4.597887601852417 and perplexity is 99.27438696239753
At time: 429.2025499343872 and batch: 900, loss is 4.701859340667725 and perplexity is 110.15179187081843
At time: 429.6311249732971 and batch: 950, loss is 4.631561031341553 and perplexity is 102.67421674664024
At time: 430.0439627170563 and batch: 1000, loss is 4.613343915939331 and perplexity is 100.82072260824776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.000708231111852 and perplexity of 148.51830714942523
Finished 46 epochs...
Completing Train Step...
At time: 431.3554103374481 and batch: 50, loss is 4.704076709747315 and perplexity is 110.39631004150402
At time: 431.7784366607666 and batch: 100, loss is 4.6593070983886715 and perplexity is 105.56291209787203
At time: 432.192551612854 and batch: 150, loss is 4.700005292892456 and perplexity is 109.9477543922232
At time: 432.60421895980835 and batch: 200, loss is 4.691122274398804 and perplexity is 108.97541152027406
At time: 433.0147292613983 and batch: 250, loss is 4.6773309230804445 and perplexity is 107.48280949224636
At time: 433.4483976364136 and batch: 300, loss is 4.583650178909302 and perplexity is 97.87098961472616
At time: 433.87944769859314 and batch: 350, loss is 4.651187171936035 and perplexity is 104.70921966594283
At time: 434.301557302475 and batch: 400, loss is 4.582652015686035 and perplexity is 97.77334713194269
At time: 434.72337436676025 and batch: 450, loss is 4.612042751312256 and perplexity is 100.68962355953236
At time: 435.14066672325134 and batch: 500, loss is 4.581002063751221 and perplexity is 97.61215882171871
At time: 435.56629967689514 and batch: 550, loss is 4.634950923919678 and perplexity is 103.02286191291768
At time: 436.00174283981323 and batch: 600, loss is 4.671778402328491 and perplexity is 106.8876627733704
At time: 436.43328642845154 and batch: 650, loss is 4.642688608169555 and perplexity is 103.82311233878069
At time: 436.8629734516144 and batch: 700, loss is 4.6151478099823 and perplexity is 101.0027566448637
At time: 437.27519607543945 and batch: 750, loss is 4.580074834823608 and perplexity is 97.52169195260309
At time: 437.68415236473083 and batch: 800, loss is 4.604251194000244 and perplexity is 99.9081430155964
At time: 438.108136177063 and batch: 850, loss is 4.593330669403076 and perplexity is 98.82302947091982
At time: 438.51345348358154 and batch: 900, loss is 4.696390810012818 and perplexity is 109.55106745726117
At time: 438.9320857524872 and batch: 950, loss is 4.626174201965332 and perplexity is 102.1226152850281
At time: 439.3516569137573 and batch: 1000, loss is 4.6068221282958985 and perplexity is 100.16533075161463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.999576475562119 and perplexity of 148.35031581156764
Finished 47 epochs...
Completing Train Step...
At time: 440.6314764022827 and batch: 50, loss is 4.698176250457764 and perplexity is 109.74683908116735
At time: 441.06584429740906 and batch: 100, loss is 4.653866033554078 and perplexity is 104.99009722364364
At time: 441.4785623550415 and batch: 150, loss is 4.693301115036011 and perplexity is 109.21311043534196
At time: 441.8975749015808 and batch: 200, loss is 4.684526090621948 and perplexity is 108.25895521518386
At time: 442.3152017593384 and batch: 250, loss is 4.671538343429566 and perplexity is 106.86200651836566
At time: 442.7437207698822 and batch: 300, loss is 4.577657585144043 and perplexity is 97.28624235885373
At time: 443.16213750839233 and batch: 350, loss is 4.645172996520996 and perplexity is 104.08136994294273
At time: 443.57205271720886 and batch: 400, loss is 4.576253061294556 and perplexity is 97.14969742398918
At time: 443.9817099571228 and batch: 450, loss is 4.606956119537354 and perplexity is 100.17875292783974
At time: 444.3947424888611 and batch: 500, loss is 4.5751555633544925 and perplexity is 97.04313431827939
At time: 444.80348467826843 and batch: 550, loss is 4.628960466384887 and perplexity is 102.40755266553414
At time: 445.21883821487427 and batch: 600, loss is 4.664932041168213 and perplexity is 106.15837057797967
At time: 445.64485692977905 and batch: 650, loss is 4.637170858383179 and perplexity is 103.25181995650503
At time: 446.07021856307983 and batch: 700, loss is 4.609955368041992 and perplexity is 100.47966493212361
At time: 446.4837980270386 and batch: 750, loss is 4.573907709121704 and perplexity is 96.92211415582202
At time: 446.89640498161316 and batch: 800, loss is 4.599034175872803 and perplexity is 99.38827767496744
At time: 447.3472218513489 and batch: 850, loss is 4.587634840011597 and perplexity is 98.26175034760742
At time: 447.7671501636505 and batch: 900, loss is 4.690467290878296 and perplexity is 108.90405779189668
At time: 448.1825156211853 and batch: 950, loss is 4.621546058654785 and perplexity is 101.65106921932401
At time: 448.5939111709595 and batch: 1000, loss is 4.600897760391235 and perplexity is 99.57366882293827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.999182352205602 and perplexity of 148.29185900751222
Finished 48 epochs...
Completing Train Step...
At time: 449.8662054538727 and batch: 50, loss is 4.692521858215332 and perplexity is 109.12803852486583
At time: 450.2913167476654 and batch: 100, loss is 4.647535419464111 and perplexity is 104.32754482935393
At time: 450.70944237709045 and batch: 150, loss is 4.6875653648376465 and perplexity is 108.58848437747075
At time: 451.12235975265503 and batch: 200, loss is 4.679247579574585 and perplexity is 107.68901466618522
At time: 451.5309398174286 and batch: 250, loss is 4.666507244110107 and perplexity is 106.32572332828926
At time: 451.951956987381 and batch: 300, loss is 4.571917762756348 and perplexity is 96.72943612009539
At time: 452.37710213661194 and batch: 350, loss is 4.639281425476074 and perplexity is 103.46996997913735
At time: 452.80482029914856 and batch: 400, loss is 4.570197715759277 and perplexity is 96.56319995196456
At time: 453.2175450325012 and batch: 450, loss is 4.601309585571289 and perplexity is 99.61468421203521
At time: 453.6397407054901 and batch: 500, loss is 4.569593725204467 and perplexity is 96.5048943010551
At time: 454.0487804412842 and batch: 550, loss is 4.623291397094727 and perplexity is 101.82863965306922
At time: 454.4692130088806 and batch: 600, loss is 4.659679937362671 and perplexity is 105.60227740371508
At time: 454.889036655426 and batch: 650, loss is 4.633168487548828 and perplexity is 102.83939377553
At time: 455.30348443984985 and batch: 700, loss is 4.6047076225280765 and perplexity is 99.9537543505969
At time: 455.72694087028503 and batch: 750, loss is 4.567474250793457 and perplexity is 96.30057125227134
At time: 456.1617257595062 and batch: 800, loss is 4.59244556427002 and perplexity is 98.73559939838174
At time: 456.5843200683594 and batch: 850, loss is 4.580916271209717 and perplexity is 97.60378478575176
At time: 457.0006763935089 and batch: 900, loss is 4.684698753356933 and perplexity is 108.27764911630155
At time: 457.42255997657776 and batch: 950, loss is 4.6136300754547115 and perplexity is 100.84957754573021
At time: 457.8583323955536 and batch: 1000, loss is 4.5926027011871335 and perplexity is 98.75111562513467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.995613656392911 and perplexity of 147.7635936410641
Finished 49 epochs...
Completing Train Step...
At time: 459.24928402900696 and batch: 50, loss is 4.6855098915100095 and perplexity is 108.36551287863267
At time: 459.66063928604126 and batch: 100, loss is 4.638710298538208 and perplexity is 103.4108923640366
At time: 460.08760595321655 and batch: 150, loss is 4.677335557937622 and perplexity is 107.48330766087183
At time: 460.4999449253082 and batch: 200, loss is 4.67117974281311 and perplexity is 106.82369260705919
At time: 460.9290289878845 and batch: 250, loss is 4.6596478080749515 and perplexity is 105.59888453226613
At time: 461.3556034564972 and batch: 300, loss is 4.563154582977295 and perplexity is 95.88548194336656
At time: 461.7784583568573 and batch: 350, loss is 4.630578393936157 and perplexity is 102.57337477435651
At time: 462.2010338306427 and batch: 400, loss is 4.558976001739502 and perplexity is 95.48565260898832
At time: 462.6238102912903 and batch: 450, loss is 4.5900036716461186 and perplexity is 98.49479179930243
At time: 463.0342495441437 and batch: 500, loss is 4.560646877288819 and perplexity is 95.6453306151319
At time: 463.4618880748749 and batch: 550, loss is 4.616044187545777 and perplexity is 101.09333383938706
At time: 463.89663577079773 and batch: 600, loss is 4.64757399559021 and perplexity is 104.33156945950566
At time: 464.3099510669708 and batch: 650, loss is 4.620155878067017 and perplexity is 101.50985405620135
At time: 464.740602016449 and batch: 700, loss is 4.593594827651978 and perplexity is 98.8491378375544
At time: 465.1580481529236 and batch: 750, loss is 4.554382085800171 and perplexity is 95.04800557401262
At time: 465.5875885486603 and batch: 800, loss is 4.579425840377808 and perplexity is 97.45842144950412
At time: 466.01214575767517 and batch: 850, loss is 4.5715602779388425 and perplexity is 96.69486299532838
At time: 466.43549847602844 and batch: 900, loss is 4.675944833755493 and perplexity is 107.33393192003604
At time: 466.8561682701111 and batch: 950, loss is 4.601050138473511 and perplexity is 99.58884282370187
At time: 467.2875442504883 and batch: 1000, loss is 4.579720315933227 and perplexity is 97.48712479830061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.990960470060023 and perplexity of 147.07761932751936
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1cac512908>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 2.9354176601516984, 'num_layers': 1, 'dropout': 0.48795091158860243, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 3.748729590401724}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6674096584320068 and batch: 50, loss is 6.801206216812134 and perplexity is 898.9309435765323
At time: 1.1311755180358887 and batch: 100, loss is 6.083433132171631 and perplexity is 438.5321521562835
At time: 1.5591614246368408 and batch: 150, loss is 5.9089498043060305 and perplexity is 368.3191450464841
At time: 1.9871478080749512 and batch: 200, loss is 5.792889556884766 and perplexity is 327.9593136318586
At time: 2.4129092693328857 and batch: 250, loss is 5.7195906066894535 and perplexity is 304.78012246908287
At time: 2.8411269187927246 and batch: 300, loss is 5.557987174987793 and perplexity is 259.30038439780003
At time: 3.272392988204956 and batch: 350, loss is 5.5599293041229245 and perplexity is 259.80446856892263
At time: 3.6957383155822754 and batch: 400, loss is 5.461214466094971 and perplexity is 235.38311566222748
At time: 4.113753795623779 and batch: 450, loss is 5.422062377929688 and perplexity is 226.3454513269066
At time: 4.542754650115967 and batch: 500, loss is 5.402450037002564 and perplexity is 221.94953517559992
At time: 4.965720891952515 and batch: 550, loss is 5.4266103172302245 and perplexity is 227.37720108977064
At time: 5.398802995681763 and batch: 600, loss is 5.418255014419556 and perplexity is 225.48531038850254
At time: 5.824846506118774 and batch: 650, loss is 5.368211765289306 and perplexity is 214.47898580403435
At time: 6.251654386520386 and batch: 700, loss is 5.344006996154786 and perplexity is 209.3498960974505
At time: 6.683871269226074 and batch: 750, loss is 5.236167840957641 and perplexity is 187.94847215078403
At time: 7.119108200073242 and batch: 800, loss is 5.2708557605743405 and perplexity is 194.58240716933466
At time: 7.536135911941528 and batch: 850, loss is 5.22592643737793 and perplexity is 186.03343904711227
At time: 7.983682632446289 and batch: 900, loss is 5.333568735122681 and perplexity is 207.1760127541711
At time: 8.416362047195435 and batch: 950, loss is 5.266342525482178 and perplexity is 193.7061897958065
At time: 8.849998712539673 and batch: 1000, loss is 5.219714117050171 and perplexity is 184.88132209714558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.081327112709603 and perplexity of 160.9875628641523
Finished 1 epochs...
Completing Train Step...
At time: 10.217980861663818 and batch: 50, loss is 5.037575798034668 and perplexity is 154.09600203655734
At time: 10.628511428833008 and batch: 100, loss is 4.9360337066650395 and perplexity is 139.21697769531738
At time: 11.048614501953125 and batch: 150, loss is 4.927358827590942 and perplexity is 138.01451041867278
At time: 11.463728189468384 and batch: 200, loss is 4.889510679244995 and perplexity is 132.88853302230785
At time: 11.903277397155762 and batch: 250, loss is 4.8612313556671145 and perplexity is 129.18317464561935
At time: 12.313431739807129 and batch: 300, loss is 4.713315658569336 and perplexity is 111.42098205760806
At time: 12.730027437210083 and batch: 350, loss is 4.764205904006958 and perplexity is 117.23798210174448
At time: 13.15551495552063 and batch: 400, loss is 4.678708791732788 and perplexity is 107.63100876222911
At time: 13.582530736923218 and batch: 450, loss is 4.699523286819458 and perplexity is 109.89477167691473
At time: 14.01125717163086 and batch: 500, loss is 4.655205097198486 and perplexity is 105.13077981630998
At time: 14.43509578704834 and batch: 550, loss is 4.688005094528198 and perplexity is 108.63624445809668
At time: 14.862218856811523 and batch: 600, loss is 4.705969142913818 and perplexity is 110.6054254861789
At time: 15.288298845291138 and batch: 650, loss is 4.659382209777832 and perplexity is 105.57084137262926
At time: 15.698173761367798 and batch: 700, loss is 4.627038793563843 and perplexity is 102.21094782050298
At time: 16.120707511901855 and batch: 750, loss is 4.550613803863525 and perplexity is 94.69051188303554
At time: 16.543266773223877 and batch: 800, loss is 4.5795090675354 and perplexity is 97.46653297444969
At time: 16.959783792495728 and batch: 850, loss is 4.538633813858032 and perplexity is 93.56288844219655
At time: 17.370848178863525 and batch: 900, loss is 4.68556489944458 and perplexity is 108.37147400562799
At time: 17.779335021972656 and batch: 950, loss is 4.594849262237549 and perplexity is 98.97321542214767
At time: 18.21662974357605 and batch: 1000, loss is 4.545174856185913 and perplexity is 94.17689318211919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.671687800709794 and perplexity of 106.87797901679299
Finished 2 epochs...
Completing Train Step...
At time: 19.59712505340576 and batch: 50, loss is 4.581948461532593 and perplexity is 97.70458248013675
At time: 20.035670280456543 and batch: 100, loss is 4.469584474563598 and perplexity is 87.32043161351031
At time: 20.4538836479187 and batch: 150, loss is 4.507322654724121 and perplexity is 90.67871499051395
At time: 20.8962721824646 and batch: 200, loss is 4.5108307838439945 and perplexity is 90.99738627430692
At time: 21.315176963806152 and batch: 250, loss is 4.493036508560181 and perplexity is 89.39247519513515
At time: 21.726691007614136 and batch: 300, loss is 4.3806053733825685 and perplexity is 79.88637985777588
At time: 22.13418436050415 and batch: 350, loss is 4.449221935272217 and perplexity is 85.56034660922555
At time: 22.545992136001587 and batch: 400, loss is 4.364944181442261 and perplexity is 78.64500996822724
At time: 22.973740339279175 and batch: 450, loss is 4.412328290939331 and perplexity is 82.46123391268307
At time: 23.383321285247803 and batch: 500, loss is 4.361181144714355 and perplexity is 78.34962203364238
At time: 23.809600591659546 and batch: 550, loss is 4.408093957901001 and perplexity is 82.11280379073385
At time: 24.217523336410522 and batch: 600, loss is 4.427683010101318 and perplexity is 83.73717378168766
At time: 24.6437087059021 and batch: 650, loss is 4.3920756483078005 and perplexity is 80.80797396212458
At time: 25.0566885471344 and batch: 700, loss is 4.360717182159424 and perplexity is 78.31327917434058
At time: 25.475784301757812 and batch: 750, loss is 4.306123313903808 and perplexity is 74.15246518144579
At time: 25.892472982406616 and batch: 800, loss is 4.32620171546936 and perplexity is 75.65637569018003
At time: 26.31910252571106 and batch: 850, loss is 4.298583192825317 and perplexity is 73.59544923855468
At time: 26.73055124282837 and batch: 900, loss is 4.462941703796386 and perplexity is 86.74230431246362
At time: 27.156325578689575 and batch: 950, loss is 4.376625814437866 and perplexity is 79.56909903784378
At time: 27.579282522201538 and batch: 1000, loss is 4.326026868820191 and perplexity is 75.64314858279367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.543433584818026 and perplexity of 94.01304834506493
Finished 3 epochs...
Completing Train Step...
At time: 28.855082035064697 and batch: 50, loss is 4.382749156951904 and perplexity is 80.05782266880955
At time: 29.295585870742798 and batch: 100, loss is 4.259826645851136 and perplexity is 70.79770931378107
At time: 29.704648971557617 and batch: 150, loss is 4.306408753395081 and perplexity is 74.17363424448335
At time: 30.122018575668335 and batch: 200, loss is 4.324675993919373 and perplexity is 75.54103314007749
At time: 30.530747652053833 and batch: 250, loss is 4.309204816818237 and perplexity is 74.38131864422792
At time: 30.960965633392334 and batch: 300, loss is 4.211610040664673 and perplexity is 67.46507392696789
At time: 31.382347583770752 and batch: 350, loss is 4.2852363014221195 and perplexity is 72.6197048319571
At time: 31.809028387069702 and batch: 400, loss is 4.199591951370239 and perplexity is 66.65912532592498
At time: 32.221203327178955 and batch: 450, loss is 4.254030179977417 and perplexity is 70.38851987979754
At time: 32.636218786239624 and batch: 500, loss is 4.197108092308045 and perplexity is 66.49375891185906
At time: 33.046706676483154 and batch: 550, loss is 4.2512367153167725 and perplexity is 70.19216641791272
At time: 33.489208459854126 and batch: 600, loss is 4.266517877578735 and perplexity is 71.27302163160161
At time: 33.9153847694397 and batch: 650, loss is 4.237815384864807 and perplexity is 69.25638790057192
At time: 34.328141927719116 and batch: 700, loss is 4.207670745849609 and perplexity is 67.19983188734822
At time: 34.758357524871826 and batch: 750, loss is 4.158740839958191 and perplexity is 63.99089706973645
At time: 35.16984438896179 and batch: 800, loss is 4.172597675323487 and perplexity is 64.88378037683117
At time: 35.59167242050171 and batch: 850, loss is 4.154268479347229 and perplexity is 63.70534572248711
At time: 36.01374316215515 and batch: 900, loss is 4.324195499420166 and perplexity is 75.50474480805944
At time: 36.44675922393799 and batch: 950, loss is 4.242969813346863 and perplexity is 69.61428658813408
At time: 36.867605447769165 and batch: 1000, loss is 4.187814517021179 and perplexity is 65.87865683326189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.47990864079173 and perplexity of 88.22661199402822
Finished 4 epochs...
Completing Train Step...
At time: 38.24625563621521 and batch: 50, loss is 4.2522048330307 and perplexity is 70.26015360208945
At time: 38.66968274116516 and batch: 100, loss is 4.1269706344604495 and perplexity is 61.9898482949487
At time: 39.10060167312622 and batch: 150, loss is 4.174309225082397 and perplexity is 64.99492728509152
At time: 39.520711183547974 and batch: 200, loss is 4.199821825027466 and perplexity is 66.67445026418349
At time: 39.940431118011475 and batch: 250, loss is 4.185694117546081 and perplexity is 65.73911575758676
At time: 40.35998773574829 and batch: 300, loss is 4.0964223527908326 and perplexity is 60.12479704028261
At time: 40.79014730453491 and batch: 350, loss is 4.174271416664124 and perplexity is 64.99246997614885
At time: 41.20947217941284 and batch: 400, loss is 4.085667672157288 and perplexity is 59.48163873019991
At time: 41.6276752948761 and batch: 450, loss is 4.143954381942749 and perplexity is 63.05165946831942
At time: 42.05879282951355 and batch: 500, loss is 4.080223121643066 and perplexity is 59.15866795663199
At time: 42.47783422470093 and batch: 550, loss is 4.141904683113098 and perplexity is 62.92255491362715
At time: 42.91170573234558 and batch: 600, loss is 4.153975839614868 and perplexity is 63.6867057346985
At time: 43.32862687110901 and batch: 650, loss is 4.127264204025269 and perplexity is 62.0080492992356
At time: 43.73940634727478 and batch: 700, loss is 4.099338026046753 and perplexity is 60.300357116568335
At time: 44.16139817237854 and batch: 750, loss is 4.053999891281128 and perplexity is 57.62750040093945
At time: 44.57834076881409 and batch: 800, loss is 4.0619196033477785 and perplexity is 58.08570564307444
At time: 44.997342109680176 and batch: 850, loss is 4.050485653877258 and perplexity is 57.42533911297416
At time: 45.41422915458679 and batch: 900, loss is 4.2218920564651485 and perplexity is 68.16232933583012
At time: 45.83861708641052 and batch: 950, loss is 4.145643382072449 and perplexity is 63.15824371440399
At time: 46.25723695755005 and batch: 1000, loss is 4.087729334831238 and perplexity is 59.6043963034274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.449730477681974 and perplexity of 85.60386873955642
Finished 5 epochs...
Completing Train Step...
At time: 47.54279279708862 and batch: 50, loss is 4.155994892120361 and perplexity is 63.815422436641605
At time: 47.972885608673096 and batch: 100, loss is 4.028553738594055 and perplexity is 56.17960207386011
At time: 48.40117597579956 and batch: 150, loss is 4.076802115440369 and perplexity is 58.95663156751991
At time: 48.83753252029419 and batch: 200, loss is 4.108203988075257 and perplexity is 60.83735477593126
At time: 49.2627592086792 and batch: 250, loss is 4.093424115180969 and perplexity is 59.944798586418344
At time: 49.68565058708191 and batch: 300, loss is 4.0086501026153565 and perplexity is 55.072478169464596
At time: 50.10707092285156 and batch: 350, loss is 4.088118348121643 and perplexity is 59.62758771634748
At time: 50.52784442901611 and batch: 400, loss is 3.9989096117019653 and perplexity is 54.53864929460354
At time: 50.94635844230652 and batch: 450, loss is 4.058994555473328 and perplexity is 57.91605041903625
At time: 51.353689432144165 and batch: 500, loss is 3.990561075210571 and perplexity is 54.08522673173533
At time: 51.759612798690796 and batch: 550, loss is 4.056914386749267 and perplexity is 57.7957004798204
At time: 52.17669463157654 and batch: 600, loss is 4.066337461471558 and perplexity is 58.34288772831225
At time: 52.58773851394653 and batch: 650, loss is 4.04149733543396 and perplexity is 56.91149463665178
At time: 53.00840735435486 and batch: 700, loss is 4.016367373466491 and perplexity is 55.4991315824269
At time: 53.42599034309387 and batch: 750, loss is 3.973898892402649 and perplexity is 53.191515066836814
At time: 53.86003851890564 and batch: 800, loss is 3.976403760910034 and perplexity is 53.32491982872872
At time: 54.284072399139404 and batch: 850, loss is 3.9691720390319825 and perplexity is 52.94067987218199
At time: 54.71867084503174 and batch: 900, loss is 4.141340250968933 and perplexity is 62.88704942220389
At time: 55.14812469482422 and batch: 950, loss is 4.067320199012756 and perplexity is 58.40025165657098
At time: 55.569644927978516 and batch: 1000, loss is 4.008571581840515 and perplexity is 55.06815400557687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.432518935785061 and perplexity of 84.14310125752495
Finished 6 epochs...
Completing Train Step...
At time: 56.8782274723053 and batch: 50, loss is 4.080494179725647 and perplexity is 59.17470556519983
At time: 57.32449984550476 and batch: 100, loss is 3.9502359724044798 and perplexity is 51.947623594292615
At time: 57.73573040962219 and batch: 150, loss is 3.998870611190796 and perplexity is 54.53652230087975
At time: 58.144214391708374 and batch: 200, loss is 4.035344796180725 and perplexity is 56.56241938269027
At time: 58.567034006118774 and batch: 250, loss is 4.019143056869507 and perplexity is 55.653393593051746
At time: 58.99650502204895 and batch: 300, loss is 3.937558341026306 and perplexity is 51.2932077576778
At time: 59.432509899139404 and batch: 350, loss is 4.019015817642212 and perplexity is 55.64631274874476
At time: 59.85308480262756 and batch: 400, loss is 3.929034638404846 and perplexity is 50.85785774109496
At time: 60.28372001647949 and batch: 450, loss is 3.9907569074630738 and perplexity is 54.09581940067258
At time: 60.71295881271362 and batch: 500, loss is 3.9192022848129273 and perplexity is 50.36025560971698
At time: 61.140743017196655 and batch: 550, loss is 3.9877655124664306 and perplexity is 53.93423923272199
At time: 61.55530500411987 and batch: 600, loss is 3.9964726781845092 and perplexity is 54.405904043510475
At time: 61.97161293029785 and batch: 650, loss is 3.971288104057312 and perplexity is 53.05282440399137
At time: 62.38540863990784 and batch: 700, loss is 3.947812623977661 and perplexity is 51.82188881350271
At time: 62.81989049911499 and batch: 750, loss is 3.9085373497009277 and perplexity is 49.82602060592591
At time: 63.246419191360474 and batch: 800, loss is 3.9079930782318115 and perplexity is 49.7989091031685
At time: 63.68657660484314 and batch: 850, loss is 3.9024402570724486 and perplexity is 49.523150993277
At time: 64.10639572143555 and batch: 900, loss is 4.074417953491211 and perplexity is 58.816236838274726
At time: 64.5283145904541 and batch: 950, loss is 4.002183399200439 and perplexity is 54.71748982606626
At time: 64.95942044258118 and batch: 1000, loss is 3.9424835777282716 and perplexity is 51.54646210393548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420370706697789 and perplexity of 83.12709541828247
Finished 7 epochs...
Completing Train Step...
At time: 66.3193769454956 and batch: 50, loss is 4.0166473388671875 and perplexity is 55.51467159426998
At time: 66.73620963096619 and batch: 100, loss is 3.8858284330368043 and perplexity is 48.70727646712955
At time: 67.14663600921631 and batch: 150, loss is 3.9347325420379637 and perplexity is 51.14846806204395
At time: 67.56470227241516 and batch: 200, loss is 3.9741023492813112 and perplexity is 53.20233834746175
At time: 67.98295783996582 and batch: 250, loss is 3.958252625465393 and perplexity is 52.36574339139914
At time: 68.4030385017395 and batch: 300, loss is 3.8790918827056884 and perplexity is 48.380260165301316
At time: 68.83074259757996 and batch: 350, loss is 3.9606190490722657 and perplexity is 52.48980966153003
At time: 69.25462532043457 and batch: 400, loss is 3.871940007209778 and perplexity is 48.035484932492615
At time: 69.68291759490967 and batch: 450, loss is 3.9335587215423584 and perplexity is 51.0884641657053
At time: 70.10641670227051 and batch: 500, loss is 3.85908851146698 and perplexity is 47.42210695675193
At time: 70.52880096435547 and batch: 550, loss is 3.929104042053223 and perplexity is 50.86138758446141
At time: 70.94613981246948 and batch: 600, loss is 3.9377263402938842 and perplexity is 51.3018257028968
At time: 71.38460278511047 and batch: 650, loss is 3.912425284385681 and perplexity is 50.0201179941283
At time: 71.79859352111816 and batch: 700, loss is 3.8899730587005616 and perplexity is 48.909568818642676
At time: 72.21972012519836 and batch: 750, loss is 3.853501486778259 and perplexity is 47.15789723478497
At time: 72.63150310516357 and batch: 800, loss is 3.850612173080444 and perplexity is 47.02183992711775
At time: 73.05644106864929 and batch: 850, loss is 3.8454097366333007 and perplexity is 46.777847022371844
At time: 73.47323775291443 and batch: 900, loss is 4.01710503578186 and perplexity is 55.54008630385115
At time: 73.90296483039856 and batch: 950, loss is 3.9469872093200684 and perplexity is 51.77913191540467
At time: 74.3157787322998 and batch: 1000, loss is 3.887245192527771 and perplexity is 48.77633186924097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.415517295279154 and perplexity of 82.72462289658758
Finished 8 epochs...
Completing Train Step...
At time: 75.57997941970825 and batch: 50, loss is 3.961923904418945 and perplexity is 52.55834597557654
At time: 76.01444792747498 and batch: 100, loss is 3.831137294769287 and perplexity is 46.114954718855444
At time: 76.42917060852051 and batch: 150, loss is 3.8800206851959227 and perplexity is 48.42521674608121
At time: 76.86771059036255 and batch: 200, loss is 3.921513476371765 and perplexity is 50.47678241339557
At time: 77.29008960723877 and batch: 250, loss is 3.906466155052185 and perplexity is 49.72292801794133
At time: 77.71076250076294 and batch: 300, loss is 3.8296635246276853 and perplexity is 46.047041931720635
At time: 78.1404926776886 and batch: 350, loss is 3.911474709510803 and perplexity is 49.972592718469755
At time: 78.57711029052734 and batch: 400, loss is 3.822270002365112 and perplexity is 45.70784756776029
At time: 79.00999879837036 and batch: 450, loss is 3.8848711347579954 and perplexity is 48.660671386243116
At time: 79.44358539581299 and batch: 500, loss is 3.8066750526428224 and perplexity is 45.000565332813714
At time: 79.85189580917358 and batch: 550, loss is 3.8789178228378294 and perplexity is 48.371839836451876
At time: 80.28093481063843 and batch: 600, loss is 3.887542371749878 and perplexity is 48.790829335669294
At time: 80.7007384300232 and batch: 650, loss is 3.862745280265808 and perplexity is 47.59583608788765
At time: 81.11252236366272 and batch: 700, loss is 3.8398016357421874 and perplexity is 46.51624636387305
At time: 81.55303263664246 and batch: 750, loss is 3.8051796436309813 and perplexity is 44.93332137301999
At time: 81.97148537635803 and batch: 800, loss is 3.800726222991943 and perplexity is 44.73365931179961
At time: 82.3963508605957 and batch: 850, loss is 3.797914776802063 and perplexity is 44.608069662762304
At time: 82.82415509223938 and batch: 900, loss is 3.9675233030319212 and perplexity is 52.85346658300999
At time: 83.25217270851135 and batch: 950, loss is 3.9005657625198364 and perplexity is 49.43040706766001
At time: 83.67512893676758 and batch: 1000, loss is 3.838414740562439 and perplexity is 46.45177792183041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.415078512052211 and perplexity of 82.68833268195631
Finished 9 epochs...
Completing Train Step...
At time: 85.0072410106659 and batch: 50, loss is 3.915237832069397 and perplexity is 50.16099998692344
At time: 85.43613600730896 and batch: 100, loss is 3.7820348834991453 and perplexity is 43.90529305311894
At time: 85.85867404937744 and batch: 150, loss is 3.8324037647247313 and perplexity is 46.17339492207308
At time: 86.27613306045532 and batch: 200, loss is 3.8758369493484497 and perplexity is 48.22304164935881
At time: 86.68900060653687 and batch: 250, loss is 3.861840581893921 and perplexity is 47.55279568469731
At time: 87.09685158729553 and batch: 300, loss is 3.7869655466079712 and perplexity is 44.12230984052629
At time: 87.505446434021 and batch: 350, loss is 3.868017520904541 and perplexity is 47.84743545254352
At time: 87.95538783073425 and batch: 400, loss is 3.7783680820465086 and perplexity is 43.74459586317166
At time: 88.36897015571594 and batch: 450, loss is 3.8422676277160646 and perplexity is 46.63109660574712
At time: 88.80466771125793 and batch: 500, loss is 3.7614825344085694 and perplexity is 43.01214571975385
At time: 89.2248203754425 and batch: 550, loss is 3.8356859064102173 and perplexity is 46.325191519008015
At time: 89.64055347442627 and batch: 600, loss is 3.844931387901306 and perplexity is 46.75547624950524
At time: 90.06209540367126 and batch: 650, loss is 3.8190562915802 and perplexity is 45.56119154631672
At time: 90.48744177818298 and batch: 700, loss is 3.796190710067749 and perplexity is 44.53122863234053
At time: 90.89603972434998 and batch: 750, loss is 3.7633582496643068 and perplexity is 43.09289996997543
At time: 91.31040978431702 and batch: 800, loss is 3.75645721912384 and perplexity is 42.79653832774252
At time: 91.73093056678772 and batch: 850, loss is 3.7562214803695677 and perplexity is 42.78645071417741
At time: 92.14238452911377 and batch: 900, loss is 3.9235272645950316 and perplexity is 50.57853438243993
At time: 92.56470036506653 and batch: 950, loss is 3.858504910469055 and perplexity is 47.39443944198815
At time: 92.97178363800049 and batch: 1000, loss is 3.795712780952454 and perplexity is 44.50995094665502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4171641280011436 and perplexity of 82.86096875108042
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 94.31549286842346 and batch: 50, loss is 3.8880511569976806 and perplexity is 48.81565970599076
At time: 94.74416613578796 and batch: 100, loss is 3.748007140159607 and perplexity is 42.43642782334628
At time: 95.17171597480774 and batch: 150, loss is 3.796744050979614 and perplexity is 44.5558764016791
At time: 95.58983707427979 and batch: 200, loss is 3.8289098739624023 and perplexity is 46.01235162176405
At time: 96.01570010185242 and batch: 250, loss is 3.8074091863632202 and perplexity is 45.033613894809086
At time: 96.44435811042786 and batch: 300, loss is 3.7287459993362426 and perplexity is 41.626875291599895
At time: 96.87106037139893 and batch: 350, loss is 3.80936146736145 and perplexity is 45.12161803995611
At time: 97.30181932449341 and batch: 400, loss is 3.708352565765381 and perplexity is 40.786557976494954
At time: 97.73099613189697 and batch: 450, loss is 3.7707495164871214 and perplexity is 43.41259109762051
At time: 98.1447389125824 and batch: 500, loss is 3.6798075246810913 and perplexity is 39.63876385456778
At time: 98.58200860023499 and batch: 550, loss is 3.750596718788147 and perplexity is 42.54646270040182
At time: 98.99307060241699 and batch: 600, loss is 3.751494469642639 and perplexity is 42.58467597407975
At time: 99.40227341651917 and batch: 650, loss is 3.7188193941116334 and perplexity is 41.21570586841309
At time: 99.83218288421631 and batch: 700, loss is 3.686859860420227 and perplexity is 39.91929777253687
At time: 100.25306582450867 and batch: 750, loss is 3.651997218132019 and perplexity is 38.551585136218016
At time: 100.68609857559204 and batch: 800, loss is 3.63714524269104 and perplexity is 37.98324884586285
At time: 101.111492395401 and batch: 850, loss is 3.62776020526886 and perplexity is 37.628442175214715
At time: 101.53194284439087 and batch: 900, loss is 3.7869222450256346 and perplexity is 44.12039931605852
At time: 101.94808840751648 and batch: 950, loss is 3.716335096359253 and perplexity is 41.11344086391786
At time: 102.36950612068176 and batch: 1000, loss is 3.646024146080017 and perplexity is 38.322000087087126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3526391750428735 and perplexity of 77.68321221673939
Finished 11 epochs...
Completing Train Step...
At time: 103.75123023986816 and batch: 50, loss is 3.8308320379257204 and perplexity is 46.10087996165401
At time: 104.19380116462708 and batch: 100, loss is 3.688338351249695 and perplexity is 39.97836174022048
At time: 104.61822319030762 and batch: 150, loss is 3.744459900856018 and perplexity is 42.286162330063966
At time: 105.0410704612732 and batch: 200, loss is 3.7805721950531006 and perplexity is 43.841120232113106
At time: 105.46392297744751 and batch: 250, loss is 3.763656105995178 and perplexity is 43.105737374803326
At time: 105.88003373146057 and batch: 300, loss is 3.6880748224258424 and perplexity is 39.96782767764705
At time: 106.31382298469543 and batch: 350, loss is 3.770778093338013 and perplexity is 43.41383171048944
At time: 106.73395466804504 and batch: 400, loss is 3.672775068283081 and perplexity is 39.360983858754025
At time: 107.14999628067017 and batch: 450, loss is 3.737434120178223 and perplexity is 41.99011024413898
At time: 107.5595452785492 and batch: 500, loss is 3.647744183540344 and perplexity is 38.38797208367931
At time: 107.98923683166504 and batch: 550, loss is 3.7225585889816286 and perplexity is 41.37010791412947
At time: 108.41194796562195 and batch: 600, loss is 3.72488742351532 and perplexity is 41.46656432201524
At time: 108.84578967094421 and batch: 650, loss is 3.6943208360671997 and perplexity is 40.21824852642385
At time: 109.27827143669128 and batch: 700, loss is 3.6665868091583254 and perplexity is 39.11815999462924
At time: 109.7383759021759 and batch: 750, loss is 3.634016962051392 and perplexity is 37.864612244907875
At time: 110.16288352012634 and batch: 800, loss is 3.6218297195434572 and perplexity is 37.40594763846882
At time: 110.58618092536926 and batch: 850, loss is 3.6151385831832887 and perplexity is 37.15649483419619
At time: 111.01492166519165 and batch: 900, loss is 3.778103718757629 and perplexity is 43.733032926413905
At time: 111.4390139579773 and batch: 950, loss is 3.710899987220764 and perplexity is 40.890590981070545
At time: 111.86933946609497 and batch: 1000, loss is 3.6427446508407595 and perplexity is 38.196529123410976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.350684561380526 and perplexity of 77.53151984708988
Finished 12 epochs...
Completing Train Step...
At time: 113.26813769340515 and batch: 50, loss is 3.8065735054016114 and perplexity is 44.99599588156274
At time: 113.71041369438171 and batch: 100, loss is 3.663092842102051 and perplexity is 38.98172092790986
At time: 114.13459134101868 and batch: 150, loss is 3.720345220565796 and perplexity is 41.278641885263454
At time: 114.56377744674683 and batch: 200, loss is 3.7565826606750488 and perplexity is 42.801907128625025
At time: 114.98242807388306 and batch: 250, loss is 3.741459321975708 and perplexity is 42.1594695353653
At time: 115.40770030021667 and batch: 300, loss is 3.6664929246902465 and perplexity is 39.11448757937999
At time: 115.83275437355042 and batch: 350, loss is 3.7501115369796754 and perplexity is 42.52582493762315
At time: 116.2543785572052 and batch: 400, loss is 3.65335506439209 and perplexity is 38.60396781768073
At time: 116.67634010314941 and batch: 450, loss is 3.718363494873047 and perplexity is 41.19691994206002
At time: 117.08707189559937 and batch: 500, loss is 3.629082999229431 and perplexity is 37.67824978661085
At time: 117.5145046710968 and batch: 550, loss is 3.705473852157593 and perplexity is 40.66931399392387
At time: 117.9314444065094 and batch: 600, loss is 3.7082430601119993 and perplexity is 40.78209186235139
At time: 118.35116505622864 and batch: 650, loss is 3.6786272954940795 and perplexity is 39.59200862490017
At time: 118.76613330841064 and batch: 700, loss is 3.6525267648696897 and perplexity is 38.572005408626076
At time: 119.18144130706787 and batch: 750, loss is 3.620828251838684 and perplexity is 37.3685055415972
At time: 119.59147572517395 and batch: 800, loss is 3.6097560024261472 and perplexity is 36.957034287914865
At time: 120.0153534412384 and batch: 850, loss is 3.603946399688721 and perplexity is 36.74295107170328
At time: 120.45992350578308 and batch: 900, loss is 3.7687073707580567 and perplexity is 43.32402672158278
At time: 120.87900638580322 and batch: 950, loss is 3.702981028556824 and perplexity is 40.5680588262433
At time: 121.28569483757019 and batch: 1000, loss is 3.6352708959579467 and perplexity is 37.91212174672279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.350939866973133 and perplexity of 77.55131660471406
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 122.55058169364929 and batch: 50, loss is 3.802810940742493 and perplexity is 44.827013640316636
At time: 122.97235941886902 and batch: 100, loss is 3.6588777446746827 and perplexity is 38.817754985289504
At time: 123.38635301589966 and batch: 150, loss is 3.71895339012146 and perplexity is 41.221228978570736
At time: 123.81766128540039 and batch: 200, loss is 3.75194908618927 and perplexity is 42.60404007369703
At time: 124.22960877418518 and batch: 250, loss is 3.7332932853698733 and perplexity is 41.81659562964216
At time: 124.64189863204956 and batch: 300, loss is 3.6572951745986937 and perplexity is 38.75637175227791
At time: 125.06176280975342 and batch: 350, loss is 3.742153296470642 and perplexity is 42.1887372863045
At time: 125.4807243347168 and batch: 400, loss is 3.6403395652771 and perplexity is 38.10477358682208
At time: 125.90139865875244 and batch: 450, loss is 3.7027417993545533 and perplexity is 40.55835492266747
At time: 126.31361365318298 and batch: 500, loss is 3.609937252998352 and perplexity is 36.96373337861521
At time: 126.74189758300781 and batch: 550, loss is 3.6895904779434203 and perplexity is 40.02845106667392
At time: 127.15943598747253 and batch: 600, loss is 3.68576443195343 and perplexity is 39.87559297948985
At time: 127.58582854270935 and batch: 650, loss is 3.6539993476867676 and perplexity is 38.62884772324707
At time: 128.00946688652039 and batch: 700, loss is 3.6253290224075316 and perplexity is 37.537071665713135
At time: 128.4369878768921 and batch: 750, loss is 3.591367411613464 and perplexity is 36.28365671364799
At time: 128.84908509254456 and batch: 800, loss is 3.5755837965011597 and perplexity is 35.7154652942731
At time: 129.27654910087585 and batch: 850, loss is 3.5675633335113526 and perplexity is 35.430156410826875
At time: 129.70115566253662 and batch: 900, loss is 3.7321317291259763 and perplexity is 41.76805150071609
At time: 130.13300442695618 and batch: 950, loss is 3.6639208889007566 and perplexity is 39.01401298495437
At time: 130.55438923835754 and batch: 1000, loss is 3.593027458190918 and perplexity is 36.34393929589649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.333804246855945 and perplexity of 76.23374762415224
Finished 14 epochs...
Completing Train Step...
At time: 131.8591275215149 and batch: 50, loss is 3.7835359382629394 and perplexity is 43.97124679011393
At time: 132.30939102172852 and batch: 100, loss is 3.6380289363861085 and perplexity is 38.01682923859154
At time: 132.746808052063 and batch: 150, loss is 3.6999442100524904 and perplexity is 40.445047870025554
At time: 133.16607999801636 and batch: 200, loss is 3.7342364406585693 and perplexity is 41.85605377762102
At time: 133.58217191696167 and batch: 250, loss is 3.7176048135757447 and perplexity is 41.165676462813835
At time: 133.99750518798828 and batch: 300, loss is 3.6430634164810183 and perplexity is 38.20870680528283
At time: 134.40943360328674 and batch: 350, loss is 3.7278677940368654 and perplexity is 41.59033439667458
At time: 134.83062386512756 and batch: 400, loss is 3.6280851697921754 and perplexity is 37.64067207102279
At time: 135.25858807563782 and batch: 450, loss is 3.691580991744995 and perplexity is 40.10820760288693
At time: 135.6980016231537 and batch: 500, loss is 3.5997549200057986 and perplexity is 36.58926604762486
At time: 136.12348532676697 and batch: 550, loss is 3.6803083515167234 and perplexity is 39.658620983313845
At time: 136.553377866745 and batch: 600, loss is 3.6775238132476806 and perplexity is 39.54834364248057
At time: 136.98291635513306 and batch: 650, loss is 3.6470680332183836 and perplexity is 38.36202481711038
At time: 137.40719723701477 and batch: 700, loss is 3.619809937477112 and perplexity is 37.330472024058885
At time: 137.81923294067383 and batch: 750, loss is 3.5869792079925538 and perplexity is 36.12478547449044
At time: 138.2321698665619 and batch: 800, loss is 3.572791805267334 and perplexity is 35.61588710366323
At time: 138.65215134620667 and batch: 850, loss is 3.565981764793396 and perplexity is 35.37416547220483
At time: 139.07895803451538 and batch: 900, loss is 3.7323400592803955 and perplexity is 41.77675395179504
At time: 139.5070822238922 and batch: 950, loss is 3.6651690435409545 and perplexity is 39.0627389087124
At time: 139.92866444587708 and batch: 1000, loss is 3.5953481817245483 and perplexity is 36.42838147670467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.332915887600038 and perplexity of 76.16605474108927
Finished 15 epochs...
Completing Train Step...
At time: 141.30554342269897 and batch: 50, loss is 3.7753306341171267 and perplexity is 43.61192552267329
At time: 141.740651845932 and batch: 100, loss is 3.6292617225646975 and perplexity is 37.68498437087546
At time: 142.16305661201477 and batch: 150, loss is 3.6911043453216554 and perplexity is 40.089094724590844
At time: 142.5938160419464 and batch: 200, loss is 3.725613555908203 and perplexity is 41.49668547223897
At time: 143.01662802696228 and batch: 250, loss is 3.7095905256271364 and perplexity is 40.837081364679904
At time: 143.4269835948944 and batch: 300, loss is 3.635341486930847 and perplexity is 37.9147980947435
At time: 143.85878157615662 and batch: 350, loss is 3.720252523422241 and perplexity is 41.274815650414126
At time: 144.27348399162292 and batch: 400, loss is 3.621179323196411 and perplexity is 37.381626856698105
At time: 144.6892466545105 and batch: 450, loss is 3.6850179958343507 and perplexity is 39.845839502536656
At time: 145.10158705711365 and batch: 500, loss is 3.5935723400115966 and perplexity is 36.36374784387944
At time: 145.54129838943481 and batch: 550, loss is 3.674733672142029 and perplexity is 39.43815197985274
At time: 145.96318101882935 and batch: 600, loss is 3.672378807067871 and perplexity is 39.34538971733905
At time: 146.38444423675537 and batch: 650, loss is 3.642333064079285 and perplexity is 38.18081117256163
At time: 146.80813312530518 and batch: 700, loss is 3.6158713197708128 and perplexity is 37.18373073457712
At time: 147.2252185344696 and batch: 750, loss is 3.583528218269348 and perplexity is 36.00033407482665
At time: 147.65122866630554 and batch: 800, loss is 3.5699801301956176 and perplexity is 35.51588745088131
At time: 148.0612428188324 and batch: 850, loss is 3.563731508255005 and perplexity is 35.294654019195924
At time: 148.48715925216675 and batch: 900, loss is 3.731003370285034 and perplexity is 41.72094872994782
At time: 148.90033531188965 and batch: 950, loss is 3.6642627000808714 and perplexity is 39.02735069013228
At time: 149.32590889930725 and batch: 1000, loss is 3.594864091873169 and perplexity is 36.410751134608766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.332793445121951 and perplexity of 76.15672935152426
Finished 16 epochs...
Completing Train Step...
At time: 150.66550517082214 and batch: 50, loss is 3.7684245538711547 and perplexity is 43.31177568769861
At time: 151.0892391204834 and batch: 100, loss is 3.622131175994873 and perplexity is 37.41722560252785
At time: 151.51475143432617 and batch: 150, loss is 3.684107708930969 and perplexity is 39.80958486024973
At time: 151.9351360797882 and batch: 200, loss is 3.7188083124160767 and perplexity is 41.215249131039215
At time: 152.35513758659363 and batch: 250, loss is 3.7032314205169676 and perplexity is 40.5782180138484
At time: 152.76248455047607 and batch: 300, loss is 3.6291972017288208 and perplexity is 37.6825529826223
At time: 153.21376085281372 and batch: 350, loss is 3.7142378997802736 and perplexity is 41.027308246472586
At time: 153.631041765213 and batch: 400, loss is 3.6155882549285887 and perplexity is 37.1732068172492
At time: 154.0618028640747 and batch: 450, loss is 3.6796192264556886 and perplexity is 39.6313006483531
At time: 154.4881203174591 and batch: 500, loss is 3.5884351348876953 and perplexity is 36.17741882710345
At time: 154.91423296928406 and batch: 550, loss is 3.6700581026077272 and perplexity is 39.25418656465004
At time: 155.3323245048523 and batch: 600, loss is 3.6680015087127686 and perplexity is 39.1735396016607
At time: 155.75413632392883 and batch: 650, loss is 3.6381237173080443 and perplexity is 38.02043267948188
At time: 156.17267036437988 and batch: 700, loss is 3.612253770828247 and perplexity is 37.04945978108035
At time: 156.5951578617096 and batch: 750, loss is 3.580223650932312 and perplexity is 35.881564895178016
At time: 157.01708126068115 and batch: 800, loss is 3.567020115852356 and perplexity is 35.41091535071846
At time: 157.44126963615417 and batch: 850, loss is 3.561113700866699 and perplexity is 35.20238024331825
At time: 157.86153149604797 and batch: 900, loss is 3.7290602827072146 and perplexity is 41.63995998233222
At time: 158.273446559906 and batch: 950, loss is 3.6626007747650147 and perplexity is 38.96254401485299
At time: 158.7025866508484 and batch: 1000, loss is 3.5934286975860594 and perplexity is 36.358524842068825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.332944544350228 and perplexity of 76.16823744396743
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 160.04510235786438 and batch: 50, loss is 3.7687074279785158 and perplexity is 43.32402920060355
At time: 160.47635221481323 and batch: 100, loss is 3.6237009525299073 and perplexity is 37.476008411150566
At time: 160.88436794281006 and batch: 150, loss is 3.68767915725708 and perplexity is 39.95201692845154
At time: 161.29502892494202 and batch: 200, loss is 3.7220871114730834 and perplexity is 41.3506074361015
At time: 161.70260310173035 and batch: 250, loss is 3.705488181114197 and perplexity is 40.66989674693431
At time: 162.1162347793579 and batch: 300, loss is 3.6282776737213136 and perplexity is 37.64791874577617
At time: 162.53747463226318 and batch: 350, loss is 3.7165549182891846 and perplexity is 41.12247949324277
At time: 162.95862698554993 and batch: 400, loss is 3.6141713857650757 and perplexity is 37.12057454212978
At time: 163.38182163238525 and batch: 450, loss is 3.6775455379486086 and perplexity is 39.54920282775114
At time: 163.79633903503418 and batch: 500, loss is 3.5854668188095093 and perplexity is 36.07019203336999
At time: 164.23259973526 and batch: 550, loss is 3.668096175193787 and perplexity is 39.17724819834095
At time: 164.66436886787415 and batch: 600, loss is 3.665814552307129 and perplexity is 39.0879623892218
At time: 165.10306239128113 and batch: 650, loss is 3.6312260389328004 and perplexity is 37.75908235462221
At time: 165.52605724334717 and batch: 700, loss is 3.604026961326599 and perplexity is 36.74591126325943
At time: 165.949289560318 and batch: 750, loss is 3.5715728902816775 and perplexity is 35.572500812617065
At time: 166.357990026474 and batch: 800, loss is 3.5555029630661013 and perplexity is 35.00542197510947
At time: 166.77398419380188 and batch: 850, loss is 3.5495844793319704 and perplexity is 34.79885483963591
At time: 167.1924605369568 and batch: 900, loss is 3.717036695480347 and perplexity is 41.142296139127346
At time: 167.60492396354675 and batch: 950, loss is 3.6497993326187133 and perplexity is 38.466946213135714
At time: 168.0332534313202 and batch: 1000, loss is 3.578714861869812 and perplexity is 35.82746800318065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.326606192239901 and perplexity of 75.68698312627119
Finished 18 epochs...
Completing Train Step...
At time: 169.3009970188141 and batch: 50, loss is 3.763612108230591 and perplexity is 43.103840860439455
At time: 169.7369475364685 and batch: 100, loss is 3.616678485870361 and perplexity is 37.21375629765894
At time: 170.14662194252014 and batch: 150, loss is 3.6797678756713865 and perplexity is 39.63719224799155
At time: 170.57625532150269 and batch: 200, loss is 3.7155000352859497 and perplexity is 41.07912296062966
At time: 171.00007939338684 and batch: 250, loss is 3.699882583618164 and perplexity is 40.442555462739044
At time: 171.43263459205627 and batch: 300, loss is 3.6230759763717653 and perplexity is 37.45259411684124
At time: 171.8553431034088 and batch: 350, loss is 3.711335597038269 and perplexity is 40.908407204124586
At time: 172.29152154922485 and batch: 400, loss is 3.609582004547119 and perplexity is 36.9506044017431
At time: 172.7127435207367 and batch: 450, loss is 3.6739349460601805 and perplexity is 39.406664275945715
At time: 173.1285433769226 and batch: 500, loss is 3.5819514417648315 and perplexity is 35.94361432284035
At time: 173.5526990890503 and batch: 550, loss is 3.664623737335205 and perplexity is 39.04144356154208
At time: 173.9737901687622 and batch: 600, loss is 3.6630374717712404 and perplexity is 38.97956255688193
At time: 174.40937328338623 and batch: 650, loss is 3.6292120695114134 and perplexity is 37.68311324279249
At time: 174.85624599456787 and batch: 700, loss is 3.6025132226943968 and perplexity is 36.69032963644597
At time: 175.2751009464264 and batch: 750, loss is 3.570568265914917 and perplexity is 35.53678175663528
At time: 175.69825434684753 and batch: 800, loss is 3.5552323198318483 and perplexity is 34.99594927640847
At time: 176.13246941566467 and batch: 850, loss is 3.5497259426116945 and perplexity is 34.803777947983484
At time: 176.5526785850525 and batch: 900, loss is 3.718010149002075 and perplexity is 41.18236575198464
At time: 176.97215914726257 and batch: 950, loss is 3.651236515045166 and perplexity is 38.522269977881194
At time: 177.4028434753418 and batch: 1000, loss is 3.580559420585632 and perplexity is 35.89361485867627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.32633227836795 and perplexity of 75.66625425075964
Finished 19 epochs...
Completing Train Step...
At time: 178.74243807792664 and batch: 50, loss is 3.7611792421340944 and perplexity is 42.999102446310474
At time: 179.16422486305237 and batch: 100, loss is 3.6138524866104125 and perplexity is 37.108738709606484
At time: 179.60197019577026 and batch: 150, loss is 3.676799077987671 and perplexity is 39.519691947068935
At time: 180.0225260257721 and batch: 200, loss is 3.7125483989715575 and perplexity is 40.958051097489374
At time: 180.4382872581482 and batch: 250, loss is 3.6971035194396973 and perplexity is 40.33031903392165
At time: 180.85783290863037 and batch: 300, loss is 3.6203353643417358 and perplexity is 37.35009161080702
At time: 181.26822209358215 and batch: 350, loss is 3.7086416482925415 and perplexity is 40.79835036215319
At time: 181.6931290626526 and batch: 400, loss is 3.607131576538086 and perplexity is 36.86017045195974
At time: 182.11673593521118 and batch: 450, loss is 3.6717919540405273 and perplexity is 39.322306530153035
At time: 182.5303635597229 and batch: 500, loss is 3.5799145221710207 and perplexity is 35.8704745857239
At time: 182.94627499580383 and batch: 550, loss is 3.6627616405487062 and perplexity is 38.96881225919005
At time: 183.3706591129303 and batch: 600, loss is 3.6614858722686767 and perplexity is 38.9191287836391
At time: 183.78059911727905 and batch: 650, loss is 3.6279203271865845 and perplexity is 37.63446779594089
At time: 184.20187187194824 and batch: 700, loss is 3.601453285217285 and perplexity is 36.65146078393245
At time: 184.61124277114868 and batch: 750, loss is 3.56982027053833 and perplexity is 35.51021034706717
At time: 185.02323603630066 and batch: 800, loss is 3.554745578765869 and perplexity is 34.97891945564549
At time: 185.4333963394165 and batch: 850, loss is 3.549462308883667 and perplexity is 34.79460370762635
At time: 185.87068915367126 and batch: 900, loss is 3.718203315734863 and perplexity is 41.190321583401676
At time: 186.29041957855225 and batch: 950, loss is 3.6515700435638427 and perplexity is 38.53512039639438
At time: 186.6986517906189 and batch: 1000, loss is 3.5810613203048707 and perplexity is 35.91163437551325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.326280175185785 and perplexity of 75.66231190083603
Finished 20 epochs...
Completing Train Step...
At time: 188.0419216156006 and batch: 50, loss is 3.7590106678009034 and perplexity is 42.90595672960355
At time: 188.4714629650116 and batch: 100, loss is 3.611533818244934 and perplexity is 37.02279552645186
At time: 188.89211463928223 and batch: 150, loss is 3.674445276260376 and perplexity is 39.42677981916272
At time: 189.30627536773682 and batch: 200, loss is 3.710238103866577 and perplexity is 40.8635351344523
At time: 189.72960567474365 and batch: 250, loss is 3.6949499654769897 and perplexity is 40.24355897031955
At time: 190.1628077030182 and batch: 300, loss is 3.6182006931304933 and perplexity is 37.27044648383204
At time: 190.6002972126007 and batch: 350, loss is 3.706575665473938 and perplexity is 40.714148680840786
At time: 191.02791094779968 and batch: 400, loss is 3.605233459472656 and perplexity is 36.79027189220094
At time: 191.45482921600342 and batch: 450, loss is 3.670067539215088 and perplexity is 39.25455699274371
At time: 191.86727905273438 and batch: 500, loss is 3.578277506828308 and perplexity is 35.811802105454525
At time: 192.27821326255798 and batch: 550, loss is 3.6612694168090822 and perplexity is 38.91070543740375
At time: 192.70999813079834 and batch: 600, loss is 3.66020085811615 and perplexity is 38.869149271415814
At time: 193.13174390792847 and batch: 650, loss is 3.626766018867493 and perplexity is 37.59105107963866
At time: 193.55160212516785 and batch: 700, loss is 3.6004599809646605 and perplexity is 36.61507280722872
At time: 193.973895072937 and batch: 750, loss is 3.569052495956421 and perplexity is 35.482956973726886
At time: 194.40491890907288 and batch: 800, loss is 3.554113435745239 and perplexity is 34.95681476324203
At time: 194.82814383506775 and batch: 850, loss is 3.548988380432129 and perplexity is 34.77811746192834
At time: 195.24435544013977 and batch: 900, loss is 3.7180604410171507 and perplexity is 41.184436948225766
At time: 195.6656391620636 and batch: 950, loss is 3.651496376991272 and perplexity is 38.5322817507091
At time: 196.09599900245667 and batch: 1000, loss is 3.5810894918441774 and perplexity is 35.912646075783144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3263051102801064 and perplexity of 75.66419857124193
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 197.46621775627136 and batch: 50, loss is 3.759105544090271 and perplexity is 42.91002768068513
At time: 197.91591048240662 and batch: 100, loss is 3.612619938850403 and perplexity is 37.06302859257116
At time: 198.32750058174133 and batch: 150, loss is 3.675608081817627 and perplexity is 39.47265216298038
At time: 198.7387158870697 and batch: 200, loss is 3.712037615776062 and perplexity is 40.93713575532592
At time: 199.1520562171936 and batch: 250, loss is 3.6967093420028685 and perplexity is 40.31442486490557
At time: 199.56329202651978 and batch: 300, loss is 3.6177490758895874 and perplexity is 37.253618307856904
At time: 199.982727766037 and batch: 350, loss is 3.708460111618042 and perplexity is 40.79094463752902
At time: 200.39581513404846 and batch: 400, loss is 3.605292444229126 and perplexity is 36.7924420214306
At time: 200.80952191352844 and batch: 450, loss is 3.6694869327545168 and perplexity is 39.23177215849791
At time: 201.21998500823975 and batch: 500, loss is 3.577578125 and perplexity is 35.78676473818341
At time: 201.6358940601349 and batch: 550, loss is 3.659916386604309 and perplexity is 38.8580936783338
At time: 202.06118369102478 and batch: 600, loss is 3.6591615438461305 and perplexity is 38.8287729953689
At time: 202.48072457313538 and batch: 650, loss is 3.6242241954803465 and perplexity is 37.49562259940766
At time: 202.90072393417358 and batch: 700, loss is 3.597090845108032 and perplexity is 36.49191922960174
At time: 203.31399631500244 and batch: 750, loss is 3.5664773035049437 and perplexity is 35.39169908451804
At time: 203.7449107170105 and batch: 800, loss is 3.5498893213272096 and perplexity is 34.809464609046735
At time: 204.1596643924713 and batch: 850, loss is 3.543988437652588 and perplexity is 34.60466285754227
At time: 204.5854136943817 and batch: 900, loss is 3.7133366870880127 and perplexity is 40.99035057141585
At time: 205.00783228874207 and batch: 950, loss is 3.6464412117004397 and perplexity is 38.33798620922853
At time: 205.44601559638977 and batch: 1000, loss is 3.5751781558990476 and perplexity is 35.700980589417235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3248536644912345 and perplexity of 75.5544557510007
Finished 22 epochs...
Completing Train Step...
At time: 206.7960262298584 and batch: 50, loss is 3.758084244728088 and perplexity is 42.8662260678705
At time: 207.21308398246765 and batch: 100, loss is 3.610981402397156 and perplexity is 37.00234919543259
At time: 207.63733434677124 and batch: 150, loss is 3.6735334682464598 and perplexity is 39.39084654997167
At time: 208.05923628807068 and batch: 200, loss is 3.7093568420410157 and perplexity is 40.827539523989095
At time: 208.46858835220337 and batch: 250, loss is 3.6946291017532347 and perplexity is 40.230648343517835
At time: 208.88769340515137 and batch: 300, loss is 3.6161057472229006 and perplexity is 37.192448643651026
At time: 209.31413674354553 and batch: 350, loss is 3.706289873123169 and perplexity is 40.70251455113155
At time: 209.7272973060608 and batch: 400, loss is 3.6035091733932494 and perplexity is 36.726889598826524
At time: 210.14655661582947 and batch: 450, loss is 3.6684216022491456 and perplexity is 39.18999960957376
At time: 210.55870151519775 and batch: 500, loss is 3.576585416793823 and perplexity is 35.751256550705854
At time: 210.9782464504242 and batch: 550, loss is 3.6588847827911377 and perplexity is 38.81802819013103
At time: 211.3986701965332 and batch: 600, loss is 3.658514723777771 and perplexity is 38.803665886532
At time: 211.8136124610901 and batch: 650, loss is 3.6237793588638305 and perplexity is 37.47894688277605
At time: 212.21893787384033 and batch: 700, loss is 3.5966741609573365 and perplexity is 36.47671679275751
At time: 212.62648487091064 and batch: 750, loss is 3.566281294822693 and perplexity is 35.38476268403743
At time: 213.04245805740356 and batch: 800, loss is 3.5498534297943114 and perplexity is 34.8082152664231
At time: 213.46085023880005 and batch: 850, loss is 3.5440891790390014 and perplexity is 34.60814915485917
At time: 213.8800721168518 and batch: 900, loss is 3.713843960762024 and perplexity is 41.01114917199448
At time: 214.30236220359802 and batch: 950, loss is 3.647314329147339 and perplexity is 38.371474391297376
At time: 214.7158272266388 and batch: 1000, loss is 3.576219038963318 and perplexity is 35.738160482094074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324741642649581 and perplexity of 75.54599247576704
Finished 23 epochs...
Completing Train Step...
At time: 216.00916171073914 and batch: 50, loss is 3.7573579931259156 and perplexity is 42.83510570448435
At time: 216.45748567581177 and batch: 100, loss is 3.610109405517578 and perplexity is 36.97009732620543
At time: 216.87974429130554 and batch: 150, loss is 3.6725686836242675 and perplexity is 39.35286119375532
At time: 217.31642127037048 and batch: 200, loss is 3.7082929706573484 and perplexity is 40.78412736959294
At time: 217.73991012573242 and batch: 250, loss is 3.693618221282959 and perplexity is 40.190000515310125
At time: 218.1834216117859 and batch: 300, loss is 3.615197343826294 and perplexity is 37.15867823787294
At time: 218.5976734161377 and batch: 350, loss is 3.7053290462493895 and perplexity is 40.663425263346745
At time: 219.02499723434448 and batch: 400, loss is 3.6026846981048584 and perplexity is 36.696621665229046
At time: 219.4446120262146 and batch: 450, loss is 3.667829170227051 and perplexity is 39.16678907487004
At time: 219.8773341178894 and batch: 500, loss is 3.5759698820114134 and perplexity is 35.72925718016752
At time: 220.2976939678192 and batch: 550, loss is 3.6582654094696045 and perplexity is 38.79399278328881
At time: 220.70944929122925 and batch: 600, loss is 3.6580671310424804 and perplexity is 38.78630153394748
At time: 221.14996123313904 and batch: 650, loss is 3.6234300756454467 and perplexity is 37.46585840151369
At time: 221.5687415599823 and batch: 700, loss is 3.596436786651611 and perplexity is 36.468059185021154
At time: 221.99404573440552 and batch: 750, loss is 3.566154570579529 and perplexity is 35.38027886087731
At time: 222.40172243118286 and batch: 800, loss is 3.549844117164612 and perplexity is 34.80789111191319
At time: 222.81806421279907 and batch: 850, loss is 3.544138946533203 and perplexity is 34.60987155858107
At time: 223.22461009025574 and batch: 900, loss is 3.7140803289413453 and perplexity is 41.02084404839115
At time: 223.6421217918396 and batch: 950, loss is 3.647704019546509 and perplexity is 38.386430300367486
At time: 224.06416654586792 and batch: 1000, loss is 3.5766609382629393 and perplexity is 35.75395664007939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324707775581174 and perplexity of 75.54343399779627
Finished 24 epochs...
Completing Train Step...
At time: 225.42664313316345 and batch: 50, loss is 3.7566963720321653 and perplexity is 42.80677446830246
At time: 225.86069107055664 and batch: 100, loss is 3.609358253479004 and perplexity is 36.942337589429236
At time: 226.27179670333862 and batch: 150, loss is 3.6717711687088013 and perplexity is 39.321489211461724
At time: 226.686420917511 and batch: 200, loss is 3.7074890089035035 and perplexity is 40.751351667994506
At time: 227.09394598007202 and batch: 250, loss is 3.6928467845916746 and perplexity is 40.15900843004272
At time: 227.513090133667 and batch: 300, loss is 3.6144711494445803 and perplexity is 37.1317036101017
At time: 227.92771172523499 and batch: 350, loss is 3.704607005119324 and perplexity is 40.634075195070615
At time: 228.35875368118286 and batch: 400, loss is 3.602036089897156 and perplexity is 36.672827652552144
At time: 228.7767198085785 and batch: 450, loss is 3.6673148679733276 and perplexity is 39.146650686031
At time: 229.21336698532104 and batch: 500, loss is 3.5754556846618653 and perplexity is 35.71089001340308
At time: 229.6242651939392 and batch: 550, loss is 3.657776355743408 and perplexity is 38.77502507505635
At time: 230.21656012535095 and batch: 600, loss is 3.6576771211624144 and perplexity is 38.77117744260225
At time: 230.64841842651367 and batch: 650, loss is 3.6231099224090575 and perplexity is 37.453865505577006
At time: 231.0816206932068 and batch: 700, loss is 3.596197566986084 and perplexity is 36.45933635147874
At time: 231.4989950656891 and batch: 750, loss is 3.5660061645507812 and perplexity is 35.37502860379011
At time: 231.91365909576416 and batch: 800, loss is 3.5497724294662474 and perplexity is 34.805395903753386
At time: 232.32834887504578 and batch: 850, loss is 3.5441174936294555 and perplexity is 34.60912908430195
At time: 232.75532174110413 and batch: 900, loss is 3.714193377494812 and perplexity is 41.025481657605376
At time: 233.18499040603638 and batch: 950, loss is 3.6479009866714476 and perplexity is 38.3939919098502
At time: 233.62459683418274 and batch: 1000, loss is 3.576892681121826 and perplexity is 35.76224332436054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324703681759718 and perplexity of 75.5431247370984
Finished 25 epochs...
Completing Train Step...
At time: 235.04634475708008 and batch: 50, loss is 3.7560789728164674 and perplexity is 42.780353756221885
At time: 235.4766547679901 and batch: 100, loss is 3.608676943778992 and perplexity is 36.917176988542415
At time: 235.90181970596313 and batch: 150, loss is 3.6710658645629883 and perplexity is 39.293765380117215
At time: 236.31998777389526 and batch: 200, loss is 3.7067932319641113 and perplexity is 40.72300767894529
At time: 236.74908781051636 and batch: 250, loss is 3.692182755470276 and perplexity is 40.13235053074894
At time: 237.16734743118286 and batch: 300, loss is 3.6138324308395386 and perplexity is 37.107994472708654
At time: 237.59631633758545 and batch: 350, loss is 3.7039861154556273 and perplexity is 40.608853748466125
At time: 238.03274083137512 and batch: 400, loss is 3.6014632177352905 and perplexity is 36.65182482703454
At time: 238.46153092384338 and batch: 450, loss is 3.66683575630188 and perplexity is 39.12789956108934
At time: 238.8895959854126 and batch: 500, loss is 3.57498797416687 and perplexity is 35.694191560683386
At time: 239.3203718662262 and batch: 550, loss is 3.6573446130752565 and perplexity is 38.75828785561867
At time: 239.73889207839966 and batch: 600, loss is 3.6573183155059814 and perplexity is 38.75726862026057
At time: 240.18299221992493 and batch: 650, loss is 3.62280366897583 and perplexity is 37.442396886919845
At time: 240.6029555797577 and batch: 700, loss is 3.5959502553939817 and perplexity is 36.45032064984838
At time: 241.02681422233582 and batch: 750, loss is 3.5658388900756837 and perplexity is 35.369111759331176
At time: 241.45534110069275 and batch: 800, loss is 3.5496583080291746 and perplexity is 34.801424088593905
At time: 241.88923740386963 and batch: 850, loss is 3.544049563407898 and perplexity is 34.6067781583457
At time: 242.32823061943054 and batch: 900, loss is 3.71423752784729 and perplexity is 41.02729298706626
At time: 242.74535417556763 and batch: 950, loss is 3.6479964208602906 and perplexity is 38.39765618417031
At time: 243.16303730010986 and batch: 1000, loss is 3.577017779350281 and perplexity is 35.766717397489465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324709636409108 and perplexity of 75.5435745712593
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 244.4586477279663 and batch: 50, loss is 3.7561008310317994 and perplexity is 42.78128886862617
At time: 244.88077878952026 and batch: 100, loss is 3.609118504524231 and perplexity is 36.93348176423513
At time: 245.2973325252533 and batch: 150, loss is 3.671383810043335 and perplexity is 39.306260641526315
At time: 245.72201585769653 and batch: 200, loss is 3.707200360298157 and perplexity is 40.73959054466713
At time: 246.15587830543518 and batch: 250, loss is 3.6925459909439087 and perplexity is 40.1469306719539
At time: 246.57972955703735 and batch: 300, loss is 3.613525652885437 and perplexity is 37.09661230407197
At time: 247.0086748600006 and batch: 350, loss is 3.7045429611206053 and perplexity is 40.631472909742165
At time: 247.43526196479797 and batch: 400, loss is 3.6014503192901612 and perplexity is 36.651352078531986
At time: 247.86076760292053 and batch: 450, loss is 3.66665590763092 and perplexity is 39.120863093124505
At time: 248.29401302337646 and batch: 500, loss is 3.574533529281616 and perplexity is 35.677974203122055
At time: 248.7397072315216 and batch: 550, loss is 3.6564304304122923 and perplexity is 38.72287189161155
At time: 249.17193722724915 and batch: 600, loss is 3.656239380836487 and perplexity is 38.71547461000895
At time: 249.62465572357178 and batch: 650, loss is 3.6217447328567505 and perplexity is 37.40276876599873
At time: 250.05026578903198 and batch: 700, loss is 3.594884948730469 and perplexity is 36.41151055636891
At time: 250.47730112075806 and batch: 750, loss is 3.5647796154022218 and perplexity is 35.331665991199955
At time: 250.90230011940002 and batch: 800, loss is 3.5483873081207276 and perplexity is 34.75721957967768
At time: 251.33035254478455 and batch: 850, loss is 3.5422882509231566 and perplexity is 34.545878455366015
At time: 251.7374918460846 and batch: 900, loss is 3.7121231746673584 and perplexity is 40.940638441114814
At time: 252.157696723938 and batch: 950, loss is 3.6457858943939208 and perplexity is 38.31287089351786
At time: 252.57699012756348 and batch: 1000, loss is 3.574525785446167 and perplexity is 35.67769791983043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324675025009528 and perplexity of 75.54095994766226
Finished 27 epochs...
Completing Train Step...
At time: 253.852876663208 and batch: 50, loss is 3.7559592151641845 and perplexity is 42.77523078825564
At time: 254.30223608016968 and batch: 100, loss is 3.6086982345581053 and perplexity is 36.91796299237046
At time: 254.7312788963318 and batch: 150, loss is 3.670877537727356 and perplexity is 39.28636600639528
At time: 255.1521134376526 and batch: 200, loss is 3.7066363382339476 and perplexity is 40.71661899555234
At time: 255.56401705741882 and batch: 250, loss is 3.692092733383179 and perplexity is 40.12873789540497
At time: 255.9923300743103 and batch: 300, loss is 3.6131825160980227 and perplexity is 37.08388527538274
At time: 256.41309928894043 and batch: 350, loss is 3.703967056274414 and perplexity is 40.60807978433925
At time: 256.83958983421326 and batch: 400, loss is 3.600993480682373 and perplexity is 36.63461214988885
At time: 257.25707507133484 and batch: 450, loss is 3.66635817527771 and perplexity is 39.10921728024984
At time: 257.6760587692261 and batch: 500, loss is 3.5743364763259886 and perplexity is 35.670944445494534
At time: 258.08701491355896 and batch: 550, loss is 3.656285490989685 and perplexity is 38.717259827632354
At time: 258.5122940540314 and batch: 600, loss is 3.656190423965454 and perplexity is 38.713579267906866
At time: 258.94372820854187 and batch: 650, loss is 3.62170184135437 and perplexity is 37.401164539457255
At time: 259.392281293869 and batch: 700, loss is 3.5947136163711546 and perplexity is 36.40527262075447
At time: 259.80841851234436 and batch: 750, loss is 3.564803113937378 and perplexity is 35.3324962433502
At time: 260.2338089942932 and batch: 800, loss is 3.5483825111389162 and perplexity is 34.75705285032744
At time: 260.6605544090271 and batch: 850, loss is 3.5423563861846925 and perplexity is 34.548232328019516
At time: 261.07316303253174 and batch: 900, loss is 3.7123147583007814 and perplexity is 40.9484827487785
At time: 261.50017786026 and batch: 950, loss is 3.6460779237747194 and perplexity is 38.32406101132357
At time: 261.9168722629547 and batch: 1000, loss is 3.5748426342010498 and perplexity is 35.68900414508051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324662371379573 and perplexity of 75.54000408635625
Finished 28 epochs...
Completing Train Step...
At time: 263.2676339149475 and batch: 50, loss is 3.7557741260528563 and perplexity is 42.76731429145346
At time: 263.69436383247375 and batch: 100, loss is 3.60841326713562 and perplexity is 36.90744407445876
At time: 264.12281799316406 and batch: 150, loss is 3.670580039024353 and perplexity is 39.27468010181959
At time: 264.5357880592346 and batch: 200, loss is 3.7062850475311278 and perplexity is 40.70231813787518
At time: 264.96817088127136 and batch: 250, loss is 3.6917712593078615 and perplexity is 40.115839619838084
At time: 265.39353227615356 and batch: 300, loss is 3.6129238986968994 and perplexity is 37.074295977382384
At time: 265.81796050071716 and batch: 350, loss is 3.703628511428833 and perplexity is 40.5943344550758
At time: 266.24391865730286 and batch: 400, loss is 3.600720591545105 and perplexity is 36.62461632612301
At time: 266.67774295806885 and batch: 450, loss is 3.666158089637756 and perplexity is 39.101392870284364
At time: 267.09991788864136 and batch: 500, loss is 3.5741661739349366 and perplexity is 35.66487011561552
At time: 267.5227267742157 and batch: 550, loss is 3.6561563348770143 and perplexity is 38.712259579773004
At time: 267.9467136859894 and batch: 600, loss is 3.6561249494552612 and perplexity is 38.71104459824553
At time: 268.3768825531006 and batch: 650, loss is 3.6216364336013793 and perplexity is 37.398718293328095
At time: 268.825749874115 and batch: 700, loss is 3.594629054069519 and perplexity is 36.40219423726941
At time: 269.2473292350769 and batch: 750, loss is 3.564816241264343 and perplexity is 35.33296006762525
At time: 269.6860373020172 and batch: 800, loss is 3.5483898544311523 and perplexity is 34.75730808246091
At time: 270.1235213279724 and batch: 850, loss is 3.5423881578445435 and perplexity is 34.54933000014284
At time: 270.58280849456787 and batch: 900, loss is 3.7124475765228273 and perplexity is 40.95392181464821
At time: 270.9933531284332 and batch: 950, loss is 3.6462719011306763 and perplexity is 38.33149573240854
At time: 271.41474413871765 and batch: 1000, loss is 3.5750422716140746 and perplexity is 35.69612971678315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324656416730183 and perplexity of 75.53955427345619
Finished 29 epochs...
Completing Train Step...
At time: 272.7114360332489 and batch: 50, loss is 3.755578031539917 and perplexity is 42.75892867800103
At time: 273.13955640792847 and batch: 100, loss is 3.6081734466552735 and perplexity is 36.898593974752465
At time: 273.55882596969604 and batch: 150, loss is 3.6703452968597414 and perplexity is 39.265461760407106
At time: 273.97533917427063 and batch: 200, loss is 3.7060136699676516 and perplexity is 40.69127394059255
At time: 274.39086961746216 and batch: 250, loss is 3.6915068674087523 and perplexity is 40.10523471880337
At time: 274.80191469192505 and batch: 300, loss is 3.6127014541625977 and perplexity is 37.06604992005832
At time: 275.2435956001282 and batch: 350, loss is 3.703375906944275 and perplexity is 40.58408143917821
At time: 275.6942505836487 and batch: 400, loss is 3.6005084705352783 and perplexity is 36.616848299433364
At time: 276.1293478012085 and batch: 450, loss is 3.665995774269104 and perplexity is 39.09504662834605
At time: 276.5402715206146 and batch: 500, loss is 3.574015040397644 and perplexity is 35.65948036493417
At time: 276.9623568058014 and batch: 550, loss is 3.6560316276550293 and perplexity is 38.70743218243596
At time: 277.38090658187866 and batch: 600, loss is 3.6560466480255127 and perplexity is 38.70801358677424
At time: 277.79796147346497 and batch: 650, loss is 3.6215622663497924 and perplexity is 37.395944636037946
At time: 278.2144100666046 and batch: 700, loss is 3.5945679187774657 and perplexity is 36.39996884651899
At time: 278.6247525215149 and batch: 750, loss is 3.564813394546509 and perplexity is 35.332859484800856
At time: 279.0456819534302 and batch: 800, loss is 3.5483916711807253 and perplexity is 34.75737122784288
At time: 279.46319246292114 and batch: 850, loss is 3.542404336929321 and perplexity is 34.54988898120381
At time: 279.8688247203827 and batch: 900, loss is 3.712537980079651 and perplexity is 40.957624362205344
At time: 280.2705886363983 and batch: 950, loss is 3.646405243873596 and perplexity is 38.33660729997736
At time: 280.67917799949646 and batch: 1000, loss is 3.5751784944534304 and perplexity is 35.700992676142725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324653067239901 and perplexity of 75.53930125487702
Finished 30 epochs...
Completing Train Step...
At time: 281.9730520248413 and batch: 50, loss is 3.7553866147994994 and perplexity is 42.7507446865512
At time: 282.4079079627991 and batch: 100, loss is 3.607957487106323 and perplexity is 36.890626231426864
At time: 282.83074474334717 and batch: 150, loss is 3.6701346015930176 and perplexity is 39.257189584953174
At time: 283.2484619617462 and batch: 200, loss is 3.705780177116394 and perplexity is 40.681773928154506
At time: 283.65311789512634 and batch: 250, loss is 3.691275944709778 and perplexity is 40.095974578988496
At time: 284.060818195343 and batch: 300, loss is 3.6124997901916505 and perplexity is 37.05857578690113
At time: 284.47404289245605 and batch: 350, loss is 3.703162603378296 and perplexity is 40.57542563307523
At time: 284.89326214790344 and batch: 400, loss is 3.600323920249939 and perplexity is 36.610091273156215
At time: 285.31424498558044 and batch: 450, loss is 3.665851716995239 and perplexity is 39.08941510814761
At time: 285.7228796482086 and batch: 500, loss is 3.5738767099380495 and perplexity is 35.65454791378837
At time: 286.150004863739 and batch: 550, loss is 3.6559109687805176 and perplexity is 38.70276206898457
At time: 286.55311346054077 and batch: 600, loss is 3.655962471961975 and perplexity is 38.70475543569423
At time: 286.9632205963135 and batch: 650, loss is 3.6214858150482176 and perplexity is 37.39308577668005
At time: 287.38307547569275 and batch: 700, loss is 3.5945124435424805 and perplexity is 36.39794960570322
At time: 287.79894638061523 and batch: 750, loss is 3.5647987699508668 and perplexity is 35.33234275979647
At time: 288.2185220718384 and batch: 800, loss is 3.5483862781524658 and perplexity is 34.75718378086308
At time: 288.636479139328 and batch: 850, loss is 3.5424108600616453 and perplexity is 34.55011435543649
At time: 289.0604581832886 and batch: 900, loss is 3.712600116729736 and perplexity is 40.96016941084824
At time: 289.4781382083893 and batch: 950, loss is 3.646499905586243 and perplexity is 38.34023648065096
At time: 289.91344928741455 and batch: 1000, loss is 3.5752769756317138 and perplexity is 35.704508725096844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324650089915206 and perplexity of 75.53907635018474
Finished 31 epochs...
Completing Train Step...
At time: 291.2325484752655 and batch: 50, loss is 3.755202932357788 and perplexity is 42.74289284652684
At time: 291.67474269866943 and batch: 100, loss is 3.607756633758545 and perplexity is 36.8832173697189
At time: 292.112802028656 and batch: 150, loss is 3.669936022758484 and perplexity is 39.249394711972336
At time: 292.5382854938507 and batch: 200, loss is 3.7055682706832886 and perplexity is 40.67315411187852
At time: 292.95241928100586 and batch: 250, loss is 3.6910668230056762 and perplexity is 40.08759051713212
At time: 293.3710517883301 and batch: 300, loss is 3.6123118114471438 and perplexity is 37.05161021706155
At time: 293.80358242988586 and batch: 350, loss is 3.7029707431793213 and perplexity is 40.567641570589544
At time: 294.24717259407043 and batch: 400, loss is 3.6001549339294434 and perplexity is 36.6039051912353
At time: 294.6741361618042 and batch: 450, loss is 3.6657174730300905 and perplexity is 39.08416794227621
At time: 295.1199355125427 and batch: 500, loss is 3.573746509552002 and perplexity is 35.64990598008297
At time: 295.5426263809204 and batch: 550, loss is 3.6557940530776976 and perplexity is 38.698237372865364
At time: 295.9537694454193 and batch: 600, loss is 3.6558759021759033 and perplexity is 38.70140491832509
At time: 296.39101672172546 and batch: 650, loss is 3.621409049034119 and perplexity is 37.39021536870642
At time: 296.81629395484924 and batch: 700, loss is 3.594457063674927 and perplexity is 36.39593394788878
At time: 297.2491681575775 and batch: 750, loss is 3.5647757148742674 and perplexity is 35.33152817931785
At time: 297.67516589164734 and batch: 800, loss is 3.5483741760253906 and perplexity is 34.75676314755347
At time: 298.1076624393463 and batch: 850, loss is 3.5424099826812743 and perplexity is 34.55008404185764
At time: 298.527455329895 and batch: 900, loss is 3.7126430320739745 and perplexity is 40.961927268337824
At time: 298.9627501964569 and batch: 950, loss is 3.6465689659118654 and perplexity is 38.34288436129746
At time: 299.3870916366577 and batch: 1000, loss is 3.5753509044647216 and perplexity is 35.707148415333414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324649717749619 and perplexity of 75.53904823714527
Finished 32 epochs...
Completing Train Step...
At time: 300.73717522621155 and batch: 50, loss is 3.7550264024734497 and perplexity is 42.735348114551286
At time: 301.17066073417664 and batch: 100, loss is 3.6075658130645754 and perplexity is 36.87617996004763
At time: 301.5921628475189 and batch: 150, loss is 3.6697453355789182 and perplexity is 39.2419110691351
At time: 302.02371096611023 and batch: 200, loss is 3.7053703689575195 and perplexity is 40.665105620918695
At time: 302.4632956981659 and batch: 250, loss is 3.6908730030059815 and perplexity is 40.07982149327075
At time: 302.9064793586731 and batch: 300, loss is 3.612133469581604 and perplexity is 37.04500295296759
At time: 303.331889629364 and batch: 350, loss is 3.702792720794678 and perplexity is 40.56042026509387
At time: 303.75916600227356 and batch: 400, loss is 3.59999605178833 and perplexity is 36.598089946386914
At time: 304.183456659317 and batch: 450, loss is 3.665589199066162 and perplexity is 39.0791547826632
At time: 304.6131932735443 and batch: 500, loss is 3.5736219930648803 and perplexity is 35.645467255377056
At time: 305.03079557418823 and batch: 550, loss is 3.6556804180145264 and perplexity is 38.69384014606119
At time: 305.4536304473877 and batch: 600, loss is 3.655788140296936 and perplexity is 38.69800855934839
At time: 305.8811192512512 and batch: 650, loss is 3.6213323974609377 and perplexity is 37.387349459716454
At time: 306.318398475647 and batch: 700, loss is 3.5944003438949585 and perplexity is 36.393869637067695
At time: 306.7332589626312 and batch: 750, loss is 3.5647465324401857 and perplexity is 35.33049713437003
At time: 307.15448570251465 and batch: 800, loss is 3.5483563566207885 and perplexity is 34.756143808246435
At time: 307.58192014694214 and batch: 850, loss is 3.542402882575989 and perplexity is 34.54983873349418
At time: 308.0105664730072 and batch: 900, loss is 3.7126725912094116 and perplexity is 40.963138085388984
At time: 308.44158911705017 and batch: 950, loss is 3.646620178222656 and perplexity is 38.3448480392898
At time: 308.8907194137573 and batch: 1000, loss is 3.5754078102111815 and perplexity is 35.70918041508362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324651578577553 and perplexity of 75.53918880244714
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 310.1798095703125 and batch: 50, loss is 3.7550271224975584 and perplexity is 42.7353788850433
At time: 310.6117584705353 and batch: 100, loss is 3.607688798904419 and perplexity is 36.88071548690731
At time: 311.03564262390137 and batch: 150, loss is 3.669808850288391 and perplexity is 39.244403586870746
At time: 311.4549241065979 and batch: 200, loss is 3.7054419183731078 and perplexity is 40.66801528955201
At time: 311.87948513031006 and batch: 250, loss is 3.6909385919570923 and perplexity is 40.082450372934986
At time: 312.28869557380676 and batch: 300, loss is 3.612008261680603 and perplexity is 37.04036491627076
At time: 312.71435022354126 and batch: 350, loss is 3.7028874206542968 and perplexity is 40.56426151307901
At time: 313.1331408023834 and batch: 400, loss is 3.599929986000061 and perplexity is 36.59567214459334
At time: 313.5622069835663 and batch: 450, loss is 3.6655239868164062 and perplexity is 39.07660642615421
At time: 314.0146453380585 and batch: 500, loss is 3.5734458684921266 and perplexity is 35.63918976551244
At time: 314.44859981536865 and batch: 550, loss is 3.6553904485702513 and perplexity is 38.68262174131307
At time: 314.8731462955475 and batch: 600, loss is 3.655371694564819 and perplexity is 38.68189629401734
At time: 315.30601835250854 and batch: 650, loss is 3.6209455871582032 and perplexity is 37.37289044438183
At time: 315.7176296710968 and batch: 700, loss is 3.5941014862060547 and perplexity is 36.38299467441174
At time: 316.15157103538513 and batch: 750, loss is 3.5643233585357668 and perplexity is 35.31554935293119
At time: 316.57869243621826 and batch: 800, loss is 3.5479430532455445 and perplexity is 34.74178194480778
At time: 316.99969577789307 and batch: 850, loss is 3.5418444442749024 and perplexity is 34.53055016648759
At time: 317.4231491088867 and batch: 900, loss is 3.7120037937164305 and perplexity is 40.935751200493634
At time: 317.8405861854553 and batch: 950, loss is 3.6459023904800416 and perplexity is 38.317334453013565
At time: 318.2652735710144 and batch: 1000, loss is 3.5746363019943237 and perplexity is 35.681641113740774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324655672399009 and perplexity of 75.53949804703204
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 319.5500166416168 and batch: 50, loss is 3.7550281429290773 and perplexity is 42.735422493593134
At time: 319.9740147590637 and batch: 100, loss is 3.6076917362213137 and perplexity is 36.8808238174151
At time: 320.4029583930969 and batch: 150, loss is 3.6697798776626587 and perplexity is 39.24326658992451
At time: 320.8359730243683 and batch: 200, loss is 3.705415906906128 and perplexity is 40.66695746857297
At time: 321.2535903453827 and batch: 250, loss is 3.690934910774231 and perplexity is 40.08230282237722
At time: 321.6665258407593 and batch: 300, loss is 3.611967887878418 and perplexity is 37.03886948609307
At time: 322.07492327690125 and batch: 350, loss is 3.702869329452515 and perplexity is 40.563527663476975
At time: 322.48890376091003 and batch: 400, loss is 3.599883780479431 and perplexity is 36.593981261573454
At time: 322.9111008644104 and batch: 450, loss is 3.6654951667785642 and perplexity is 39.07548025310652
At time: 323.3230655193329 and batch: 500, loss is 3.573402428627014 and perplexity is 35.63764163754178
At time: 323.735830783844 and batch: 550, loss is 3.655315361022949 and perplexity is 38.679717267169586
At time: 324.14472222328186 and batch: 600, loss is 3.6552662372589113 and perplexity is 38.6778172205346
At time: 324.59212136268616 and batch: 650, loss is 3.620846586227417 and perplexity is 37.369190676584914
At time: 325.01315808296204 and batch: 700, loss is 3.5940149784088136 and perplexity is 36.37984739781936
At time: 325.4550709724426 and batch: 750, loss is 3.5642083787918093 and perplexity is 35.311489013541745
At time: 325.875506401062 and batch: 800, loss is 3.547831177711487 and perplexity is 34.7378954068069
At time: 326.3097012042999 and batch: 850, loss is 3.5417001342773435 and perplexity is 34.525567422415904
At time: 326.7280213832855 and batch: 900, loss is 3.7118294525146482 and perplexity is 40.92861503451543
At time: 327.1536500453949 and batch: 950, loss is 3.6457149362564087 and perplexity is 38.310152380007985
At time: 327.5736520290375 and batch: 1000, loss is 3.5744339990615845 and perplexity is 35.67442334321121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3246586497237045 and perplexity of 75.53972295297984
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 328.8545970916748 and batch: 50, loss is 3.7550273036956785 and perplexity is 42.735386628614314
At time: 329.29837369918823 and batch: 100, loss is 3.607690396308899 and perplexity is 36.880774400374506
At time: 329.72319412231445 and batch: 150, loss is 3.6697694730758665 and perplexity is 39.2428582820754
At time: 330.1431498527527 and batch: 200, loss is 3.7054061031341554 and perplexity is 40.66655878094945
At time: 330.55528926849365 and batch: 250, loss is 3.6909320640563963 and perplexity is 40.08218871953333
At time: 330.986688375473 and batch: 300, loss is 3.6119559049606322 and perplexity is 37.03842565502435
At time: 331.4010865688324 and batch: 350, loss is 3.7028616857528687 and perplexity is 40.563217609239906
At time: 331.8378677368164 and batch: 400, loss is 3.599869785308838 and perplexity is 36.593469126146736
At time: 332.2663412094116 and batch: 450, loss is 3.66548602104187 and perplexity is 39.07512288068715
At time: 332.6896457672119 and batch: 500, loss is 3.5733900260925293 and perplexity is 35.637199643203346
At time: 333.1082863807678 and batch: 550, loss is 3.6552951192855834 and perplexity is 38.6789343304153
At time: 333.53097343444824 and batch: 600, loss is 3.6552380752563476 and perplexity is 38.67672799108439
At time: 333.95478534698486 and batch: 650, loss is 3.620819959640503 and perplexity is 37.368195675828254
At time: 334.3787066936493 and batch: 700, loss is 3.593991103172302 and perplexity is 36.378978830727156
At time: 334.7976396083832 and batch: 750, loss is 3.5641771173477172 and perplexity is 35.31038514265653
At time: 335.22591185569763 and batch: 800, loss is 3.5478009939193726 and perplexity is 34.73684690121747
At time: 335.6782088279724 and batch: 850, loss is 3.541661510467529 and perplexity is 34.524233939218306
At time: 336.1014609336853 and batch: 900, loss is 3.7117828798294066 and perplexity is 40.92670892339675
At time: 336.5342900753021 and batch: 950, loss is 3.6456648445129396 and perplexity is 38.30823340574549
At time: 336.955992937088 and batch: 1000, loss is 3.574380292892456 and perplexity is 35.672507458045466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324659021889291 and perplexity of 75.53975106627036
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 338.36608600616455 and batch: 50, loss is 3.755027103424072 and perplexity is 42.73537806993064
At time: 338.8096749782562 and batch: 100, loss is 3.6076902389526366 and perplexity is 36.880768596954155
At time: 339.2349455356598 and batch: 150, loss is 3.6697666883468627 and perplexity is 39.24274900150191
At time: 339.65450406074524 and batch: 200, loss is 3.705403599739075 and perplexity is 40.66645697661369
At time: 340.0807464122772 and batch: 250, loss is 3.690931577682495 and perplexity is 40.082169224607576
At time: 340.5068485736847 and batch: 300, loss is 3.611952724456787 and perplexity is 37.03830785435647
At time: 340.9440438747406 and batch: 350, loss is 3.702859654426575 and perplexity is 40.5631352121931
At time: 341.36587023735046 and batch: 400, loss is 3.599866199493408 and perplexity is 36.59333790895577
At time: 341.7823967933655 and batch: 450, loss is 3.665483832359314 and perplexity is 39.07503735774091
At time: 342.21354031562805 and batch: 500, loss is 3.5733870840072632 and perplexity is 35.63709479567759
At time: 342.6376314163208 and batch: 550, loss is 3.655289740562439 and perplexity is 38.67872628769553
At time: 343.05674409866333 and batch: 600, loss is 3.655230460166931 and perplexity is 38.67643346546383
At time: 343.46541595458984 and batch: 650, loss is 3.620812759399414 and perplexity is 37.36792661677897
At time: 343.8849012851715 and batch: 700, loss is 3.5939847469329833 and perplexity is 36.37874759796642
At time: 344.3010106086731 and batch: 750, loss is 3.5641685247421266 and perplexity is 35.310081735747275
At time: 344.7247517108917 and batch: 800, loss is 3.547792682647705 and perplexity is 34.73655819504575
At time: 345.1563866138458 and batch: 850, loss is 3.5416510677337647 and perplexity is 34.523873413717304
At time: 345.5790503025055 and batch: 900, loss is 3.711770143508911 and perplexity is 40.926187671034505
At time: 346.0054500102997 and batch: 950, loss is 3.6456509256362915 and perplexity is 38.30770020188091
At time: 346.4597239494324 and batch: 1000, loss is 3.5743655347824097 and perplexity is 35.671981003139514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.324659394054878 and perplexity of 75.53977917957141
Annealing...
Model not improving. Stopping early with 75.53904823714527loss at 36 epochs.
Finished Training.
Improved accuracyfrom -76.6302180757811 to -75.53904823714527
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1caca42a58>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -223.00914785126133, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 22.06859021267849, 'num_layers': 1, 'dropout': 0.6841578975066428, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 4.726216532739262}}, {'best_accuracy': -76.6302180757811, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 1.9081563806455393, 'num_layers': 1, 'dropout': 0.5358826467218077, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 3.593027008332874}}, {'best_accuracy': -162.72915030512337, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 22.905441939296505, 'num_layers': 1, 'dropout': 0.8954330128718082, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 2.9022484911763584}}, {'best_accuracy': -102.36789743065519, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 12.094282084128249, 'num_layers': 1, 'dropout': 0.046855977202634125, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 5.14395712120558}}, {'best_accuracy': -147.07761932751936, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 18.154545353046444, 'num_layers': 1, 'dropout': 0.035335201470844235, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 7.735717459826611}}, {'best_accuracy': -75.53904823714527, 'params': {'batch_size': 50, 'wordvec_dim': 200, 'wordvec_source': 'glove', 'lr': 2.9354176601516984, 'num_layers': 1, 'dropout': 0.48795091158860243, 'tune_wordvecs': True, 'seq_len': 20, 'data': 'ptb', 'anneal': 3.748729590401724}}]
