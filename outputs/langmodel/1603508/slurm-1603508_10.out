Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'lr': 4.567992145082377, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.372063828199916, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 7.916763378916041, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.3085155487060547 and batch: 50, loss is 6.722806606292725 and perplexity is 831.1469432946216
At time: 1.8111317157745361 and batch: 100, loss is 6.053697605133056 and perplexity is 425.6841354352826
At time: 2.2984769344329834 and batch: 150, loss is 5.721162157058716 and perplexity is 305.25947634881845
At time: 2.7868471145629883 and batch: 200, loss is 5.592994976043701 and perplexity is 268.53868345879204
At time: 3.274388074874878 and batch: 250, loss is 5.5765504550933835 and perplexity is 264.1588046579853
At time: 3.761531114578247 and batch: 300, loss is 5.422965040206909 and perplexity is 226.54985706823692
At time: 4.248296499252319 and batch: 350, loss is 5.362120952606201 and perplexity is 213.17660478276923
At time: 4.735072374343872 and batch: 400, loss is 5.326602640151978 and perplexity is 205.73782008290533
At time: 5.222358703613281 and batch: 450, loss is 5.319511814117432 and perplexity is 204.2841290181327
At time: 5.709806442260742 and batch: 500, loss is 5.3090057468414305 and perplexity is 202.14914101687762
At time: 6.19799542427063 and batch: 550, loss is 5.215937633514404 and perplexity is 184.1844375429346
At time: 6.686816453933716 and batch: 600, loss is 5.106666860580444 and perplexity is 165.11907174539925
At time: 7.175879716873169 and batch: 650, loss is 5.077870540618896 and perplexity is 160.43205837116062
At time: 7.6642820835113525 and batch: 700, loss is 5.114265575408935 and perplexity is 166.37854360789458
At time: 8.156970500946045 and batch: 750, loss is 5.050115327835083 and perplexity is 156.04045925707763
At time: 8.649579286575317 and batch: 800, loss is 5.123625841140747 and perplexity is 167.94320237516425
At time: 9.141191959381104 and batch: 850, loss is 5.020064058303833 and perplexity is 151.42100325601723
At time: 9.630058765411377 and batch: 900, loss is 5.051109981536865 and perplexity is 156.19574269132448
At time: 10.11964201927185 and batch: 950, loss is 5.0028520202636715 and perplexity is 148.83704061173773
At time: 10.612218141555786 and batch: 1000, loss is 4.90336051940918 and perplexity is 134.7418222194637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8898747141768295 and perplexity of 132.9369178967261
Finished 1 epochs...
Completing Train Step...
At time: 12.042840242385864 and batch: 50, loss is 4.844188823699951 and perplexity is 127.0002206298443
At time: 12.543353796005249 and batch: 100, loss is 4.718236379623413 and perplexity is 111.9706047909392
At time: 13.032225131988525 and batch: 150, loss is 4.7232510471344 and perplexity is 112.53351035748818
At time: 13.519533157348633 and batch: 200, loss is 4.72802414894104 and perplexity is 113.07192819845949
At time: 14.00621771812439 and batch: 250, loss is 4.741621866226196 and perplexity is 114.61994923511178
At time: 14.49429702758789 and batch: 300, loss is 4.610665559768677 and perplexity is 100.55105010443748
At time: 14.99305510520935 and batch: 350, loss is 4.597148084640503 and perplexity is 99.20099898371792
At time: 15.479818344116211 and batch: 400, loss is 4.607098789215088 and perplexity is 100.19304641783532
At time: 15.966086864471436 and batch: 450, loss is 4.6267919921875 and perplexity is 102.18572513052891
At time: 16.452985286712646 and batch: 500, loss is 4.636879358291626 and perplexity is 103.22172642788098
At time: 16.938931703567505 and batch: 550, loss is 4.529361381530761 and perplexity is 92.69934266282544
At time: 17.42551565170288 and batch: 600, loss is 4.458433003425598 and perplexity is 86.35208959450458
At time: 17.925673961639404 and batch: 650, loss is 4.436432847976684 and perplexity is 84.4730752908425
At time: 18.412412643432617 and batch: 700, loss is 4.489325275421143 and perplexity is 89.06133373047348
At time: 18.899584770202637 and batch: 750, loss is 4.45694450378418 and perplexity is 86.22365015485191
At time: 19.385784149169922 and batch: 800, loss is 4.54756983757019 and perplexity is 94.40271540018188
At time: 19.871222019195557 and batch: 850, loss is 4.426041917800903 and perplexity is 83.599866048684
At time: 20.357969999313354 and batch: 900, loss is 4.409358787536621 and perplexity is 82.21672820791875
At time: 20.844338417053223 and batch: 950, loss is 4.418957810401917 and perplexity is 83.00972838632646
At time: 21.3305025100708 and batch: 1000, loss is 4.312711338996888 and perplexity is 74.64259620780112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.564487736399581 and perplexity of 96.0133972481848
Finished 2 epochs...
Completing Train Step...
At time: 22.7695050239563 and batch: 50, loss is 4.412914257049561 and perplexity is 82.50956756072055
At time: 23.257420301437378 and batch: 100, loss is 4.294970302581787 and perplexity is 73.33003669992007
At time: 23.74548029899597 and batch: 150, loss is 4.36730525970459 and perplexity is 78.83091637512779
At time: 24.23460030555725 and batch: 200, loss is 4.401878900527954 and perplexity is 81.60405060685639
At time: 24.72302770614624 and batch: 250, loss is 4.4015040731430055 and perplexity is 81.57346890575376
At time: 25.21156334877014 and batch: 300, loss is 4.272450232505799 and perplexity is 71.69709512619208
At time: 25.700817584991455 and batch: 350, loss is 4.286800785064697 and perplexity is 72.73340609098409
At time: 26.18923330307007 and batch: 400, loss is 4.302968606948853 and perplexity is 73.91890448529014
At time: 26.677457094192505 and batch: 450, loss is 4.338841032981873 and perplexity is 76.61868932722233
At time: 27.166433334350586 and batch: 500, loss is 4.357647004127503 and perplexity is 78.07321217779379
At time: 27.655436992645264 and batch: 550, loss is 4.254330191612244 and perplexity is 70.40964042276559
At time: 28.144323348999023 and batch: 600, loss is 4.197626514434814 and perplexity is 66.52823968479662
At time: 28.632990837097168 and batch: 650, loss is 4.175042624473572 and perplexity is 65.04261200902823
At time: 29.122722864151 and batch: 700, loss is 4.249784803390503 and perplexity is 70.09032752280312
At time: 29.61847949028015 and batch: 750, loss is 4.213792123794556 and perplexity is 67.61244906055956
At time: 30.12730383872986 and batch: 800, loss is 4.320504264831543 and perplexity is 75.22655283420552
At time: 30.61590003967285 and batch: 850, loss is 4.20199230670929 and perplexity is 66.81932310217356
At time: 31.10562038421631 and batch: 900, loss is 4.163226799964905 and perplexity is 64.27860250985613
At time: 31.595974922180176 and batch: 950, loss is 4.198161420822143 and perplexity is 66.56383558453008
At time: 32.087226152420044 and batch: 1000, loss is 4.101436815261841 and perplexity is 60.42704775772107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.470188419993331 and perplexity of 87.37318431737931
Finished 3 epochs...
Completing Train Step...
At time: 33.520917654037476 and batch: 50, loss is 4.215038642883301 and perplexity is 67.69678181922735
At time: 34.01053810119629 and batch: 100, loss is 4.096863660812378 and perplexity is 60.15133645110701
At time: 34.50020933151245 and batch: 150, loss is 4.188947696685791 and perplexity is 65.95335150076346
At time: 35.009668588638306 and batch: 200, loss is 4.232966833114624 and perplexity is 68.9214074610425
At time: 35.50519561767578 and batch: 250, loss is 4.22432222366333 and perplexity is 68.32817662943962
At time: 35.99380135536194 and batch: 300, loss is 4.091372060775757 and perplexity is 59.82191472371664
At time: 36.48258376121521 and batch: 350, loss is 4.115135736465454 and perplexity is 61.26052898611006
At time: 36.97198843955994 and batch: 400, loss is 4.131370029449463 and perplexity is 62.26316690032602
At time: 37.46725535392761 and batch: 450, loss is 4.176180787086487 and perplexity is 65.1166832228103
At time: 37.97097373008728 and batch: 500, loss is 4.196833214759827 and perplexity is 66.47548378226404
At time: 38.48442625999451 and batch: 550, loss is 4.097381863594055 and perplexity is 60.182515118716005
At time: 39.0097599029541 and batch: 600, loss is 4.044412903785705 and perplexity is 57.07766611370239
At time: 39.54646182060242 and batch: 650, loss is 4.021101222038269 and perplexity is 55.762478898575445
At time: 40.0863733291626 and batch: 700, loss is 4.10699499130249 and perplexity is 60.76384705459613
At time: 40.62342548370361 and batch: 750, loss is 4.069733047485352 and perplexity is 58.541332749783955
At time: 41.16629385948181 and batch: 800, loss is 4.181905326843261 and perplexity is 65.49051525202185
At time: 41.703840017318726 and batch: 850, loss is 4.062357964515686 and perplexity is 58.11117374254398
At time: 42.23953652381897 and batch: 900, loss is 4.014459977149963 and perplexity is 55.39337363649801
At time: 42.79930567741394 and batch: 950, loss is 4.058052845001221 and perplexity is 57.86153594031003
At time: 43.33581900596619 and batch: 1000, loss is 3.967449541091919 and perplexity is 52.84956815255865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.428746293230755 and perplexity of 83.82625745806857
Finished 4 epochs...
Completing Train Step...
At time: 44.87224197387695 and batch: 50, loss is 4.085141906738281 and perplexity is 59.45037356128225
At time: 45.41745138168335 and batch: 100, loss is 3.9671450519561766 and perplexity is 52.83347848291663
At time: 45.95151448249817 and batch: 150, loss is 4.069860811233521 and perplexity is 58.54881268770114
At time: 46.48565101623535 and batch: 200, loss is 4.118565106391907 and perplexity is 61.47097464365133
At time: 47.017720222473145 and batch: 250, loss is 4.10667501449585 and perplexity is 60.74440714318267
At time: 47.55149459838867 and batch: 300, loss is 3.968350896835327 and perplexity is 52.89722588941041
At time: 48.08691596984863 and batch: 350, loss is 3.9968906116485594 and perplexity is 54.42864684360974
At time: 48.62459874153137 and batch: 400, loss is 4.015945863723755 and perplexity is 55.475743087337065
At time: 49.163084506988525 and batch: 450, loss is 4.062122406959534 and perplexity is 58.09748682856333
At time: 49.70093894004822 and batch: 500, loss is 4.085583992004395 and perplexity is 59.47666150582699
At time: 50.23665165901184 and batch: 550, loss is 3.9908349561691283 and perplexity is 54.10004167414911
At time: 50.769967555999756 and batch: 600, loss is 3.937695083618164 and perplexity is 51.30022220342712
At time: 51.30166220664978 and batch: 650, loss is 3.914888868331909 and perplexity is 50.14349867073184
At time: 51.833131313323975 and batch: 700, loss is 4.003923511505127 and perplexity is 54.81278729349538
At time: 52.364017724990845 and batch: 750, loss is 3.965526194572449 and perplexity is 52.748017809151804
At time: 52.896979570388794 and batch: 800, loss is 4.081119437217712 and perplexity is 59.21171656269119
At time: 53.431694984436035 and batch: 850, loss is 3.96198166847229 and perplexity is 52.56138204636424
At time: 53.967024087905884 and batch: 900, loss is 3.9098134803771973 and perplexity is 49.889645707635104
At time: 54.50200581550598 and batch: 950, loss is 3.958536162376404 and perplexity is 52.3805931176464
At time: 55.059489488601685 and batch: 1000, loss is 3.8687222528457643 and perplexity is 47.881166953049416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4104997588367 and perplexity of 82.31058867105529
Finished 5 epochs...
Completing Train Step...
At time: 56.61274862289429 and batch: 50, loss is 3.990690703392029 and perplexity is 54.092238155749556
At time: 57.14699840545654 and batch: 100, loss is 3.8739534997940064 and perplexity is 48.13230146215718
At time: 57.66565489768982 and batch: 150, loss is 3.978632206916809 and perplexity is 53.443884036808804
At time: 58.186314821243286 and batch: 200, loss is 4.032432179450989 and perplexity is 56.39791441997107
At time: 58.70861101150513 and batch: 250, loss is 4.018125057220459 and perplexity is 55.59676728557652
At time: 59.23469257354736 and batch: 300, loss is 3.876636471748352 and perplexity is 48.261612468411144
At time: 59.755577087402344 and batch: 350, loss is 3.907484283447266 and perplexity is 49.773578122622595
At time: 60.27530002593994 and batch: 400, loss is 3.9246769523620606 and perplexity is 50.63671734440472
At time: 60.79376435279846 and batch: 450, loss is 3.9745970964431763 and perplexity is 53.228666565732325
At time: 61.31248211860657 and batch: 500, loss is 3.999722833633423 and perplexity is 54.58301935923154
At time: 61.83160138130188 and batch: 550, loss is 3.9063319826126097 and perplexity is 49.71625701892846
At time: 62.35525035858154 and batch: 600, loss is 3.855492601394653 and perplexity is 47.25188755494535
At time: 62.878214836120605 and batch: 650, loss is 3.8315681505203245 and perplexity is 46.134827893233194
At time: 63.40196180343628 and batch: 700, loss is 3.9244332551956176 and perplexity is 50.624378823362285
At time: 63.93878197669983 and batch: 750, loss is 3.8872067737579346 and perplexity is 48.774457978569934
At time: 64.45863151550293 and batch: 800, loss is 4.004657912254333 and perplexity is 54.85305663065561
At time: 64.97744250297546 and batch: 850, loss is 3.8849563884735105 and perplexity is 48.66482006612093
At time: 65.4984974861145 and batch: 900, loss is 3.830706477165222 and perplexity is 46.095091863492996
At time: 66.02254986763 and batch: 950, loss is 3.8807959842681883 and perplexity is 48.46277532938477
At time: 66.54620432853699 and batch: 1000, loss is 3.7934388971328734 and perplexity is 44.408855472628325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4031286472227515 and perplexity of 81.70609875326907
Finished 6 epochs...
Completing Train Step...
At time: 68.05648422241211 and batch: 50, loss is 3.91516092300415 and perplexity is 50.157142299650076
At time: 68.5813376903534 and batch: 100, loss is 3.802089800834656 and perplexity is 44.7946987450122
At time: 69.09199905395508 and batch: 150, loss is 3.9055246782302855 and perplexity is 49.676137063451236
At time: 69.61532640457153 and batch: 200, loss is 3.960686330795288 and perplexity is 52.49334138517406
At time: 70.12455773353577 and batch: 250, loss is 3.9477491903305055 and perplexity is 51.81860166635175
At time: 70.63443303108215 and batch: 300, loss is 3.803700633049011 and perplexity is 44.86691363620439
At time: 71.14487147331238 and batch: 350, loss is 3.8369039583206175 and perplexity is 46.38165238619028
At time: 71.65639543533325 and batch: 400, loss is 3.8522010135650633 and perplexity is 47.096609512798494
At time: 72.16878938674927 and batch: 450, loss is 3.9037253046035767 and perplexity is 49.586831503672705
At time: 72.67948508262634 and batch: 500, loss is 3.933375849723816 and perplexity is 51.079122379557525
At time: 73.18971800804138 and batch: 550, loss is 3.8391301965713502 and perplexity is 46.48502401711035
At time: 73.69932174682617 and batch: 600, loss is 3.79192852973938 and perplexity is 44.34183241280332
At time: 74.21186470985413 and batch: 650, loss is 3.7664364767074585 and perplexity is 43.22575407267199
At time: 74.72534227371216 and batch: 700, loss is 3.860231919288635 and perplexity is 47.4763607759754
At time: 75.23801493644714 and batch: 750, loss is 3.8229577207565306 and perplexity is 45.739292506556815
At time: 75.75037860870361 and batch: 800, loss is 3.94217568397522 and perplexity is 51.530593713276296
At time: 76.26258969306946 and batch: 850, loss is 3.8212743759155274 and perplexity is 45.66236227270701
At time: 76.77433681488037 and batch: 900, loss is 3.7662447834014894 and perplexity is 43.21746877911382
At time: 77.29820561408997 and batch: 950, loss is 3.8163251876831055 and perplexity is 45.43692896278017
At time: 77.83229351043701 and batch: 1000, loss is 3.7322109746932983 and perplexity is 41.771361564805424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.406967535251525 and perplexity of 82.02036214268938
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 79.30818009376526 and batch: 50, loss is 3.8910000991821287 and perplexity is 48.959826729792326
At time: 79.82385611534119 and batch: 100, loss is 3.764167380332947 and perplexity is 43.12778186704505
At time: 80.32768559455872 and batch: 150, loss is 3.85876886844635 and perplexity is 47.40695123357923
At time: 80.83137464523315 and batch: 200, loss is 3.909162073135376 and perplexity is 49.85715781370273
At time: 81.33563280105591 and batch: 250, loss is 3.8827110815048216 and perplexity is 48.55567518423204
At time: 81.8386561870575 and batch: 300, loss is 3.7242037057876587 and perplexity is 41.438222586860526
At time: 82.34189748764038 and batch: 350, loss is 3.753767628669739 and perplexity is 42.68158782097549
At time: 82.86004495620728 and batch: 400, loss is 3.7590412950515746 and perplexity is 42.9072708412193
At time: 83.36393594741821 and batch: 450, loss is 3.8062558794021606 and perplexity is 44.981706252898505
At time: 83.87026739120483 and batch: 500, loss is 3.8308086729049684 and perplexity is 46.09980282622072
At time: 84.3805239200592 and batch: 550, loss is 3.7200747776031493 and perplexity is 41.267479876469366
At time: 84.89038181304932 and batch: 600, loss is 3.6603489542007446 and perplexity is 38.87490606650333
At time: 85.40015697479248 and batch: 650, loss is 3.6255874586105348 and perplexity is 37.546773857631095
At time: 85.91072416305542 and batch: 700, loss is 3.7122696018218995 and perplexity is 40.94663370123057
At time: 86.42010021209717 and batch: 750, loss is 3.660033164024353 and perplexity is 38.862631691224955
At time: 86.92946171760559 and batch: 800, loss is 3.7607190895080564 and perplexity is 42.97932084802878
At time: 87.43843960762024 and batch: 850, loss is 3.6279155445098876 and perplexity is 37.63428780287919
At time: 87.94868659973145 and batch: 900, loss is 3.5511052322387697 and perplexity is 34.85181555916385
At time: 88.45906472206116 and batch: 950, loss is 3.574384913444519 and perplexity is 35.67267228510418
At time: 88.97091889381409 and batch: 1000, loss is 3.47023401260376 and perplexity is 32.14426372753217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.321250357278964 and perplexity of 75.28269973990723
Finished 8 epochs...
Completing Train Step...
At time: 90.45716762542725 and batch: 50, loss is 3.803086795806885 and perplexity is 44.83938110478631
At time: 90.95653414726257 and batch: 100, loss is 3.67769588470459 and perplexity is 39.55514936910834
At time: 91.45876002311707 and batch: 150, loss is 3.7870151948928834 and perplexity is 44.124500491916834
At time: 91.96578454971313 and batch: 200, loss is 3.8449145793914794 and perplexity is 46.75469036622803
At time: 92.49287939071655 and batch: 250, loss is 3.8193291807174683 and perplexity is 45.57362639716139
At time: 93.00699162483215 and batch: 300, loss is 3.662859706878662 and perplexity is 38.972633974978805
At time: 93.51718473434448 and batch: 350, loss is 3.697923641204834 and perplexity is 40.36340837314526
At time: 94.02667284011841 and batch: 400, loss is 3.705281310081482 and perplexity is 40.66148419358067
At time: 94.54352116584778 and batch: 450, loss is 3.756882081031799 and perplexity is 42.81472480976864
At time: 95.06092882156372 and batch: 500, loss is 3.7824569606781004 and perplexity is 43.92382838674749
At time: 95.58165287971497 and batch: 550, loss is 3.6763851833343506 and perplexity is 39.50333834243939
At time: 96.09270429611206 and batch: 600, loss is 3.621206078529358 and perplexity is 37.382627027950655
At time: 96.60604548454285 and batch: 650, loss is 3.589276838302612 and perplexity is 36.20788230288037
At time: 97.11988353729248 and batch: 700, loss is 3.680171084403992 and perplexity is 39.65317753252848
At time: 97.6332995891571 and batch: 750, loss is 3.63246527671814 and perplexity is 37.805903841701756
At time: 98.16482138633728 and batch: 800, loss is 3.738571300506592 and perplexity is 42.03788773215568
At time: 98.68152356147766 and batch: 850, loss is 3.6097048044204714 and perplexity is 36.95514220989935
At time: 99.19649529457092 and batch: 900, loss is 3.5381230449676515 and perplexity is 34.40228700825747
At time: 99.71049523353577 and batch: 950, loss is 3.568053274154663 and perplexity is 35.447519337505796
At time: 100.2255654335022 and batch: 1000, loss is 3.4735931730270386 and perplexity is 32.25242302659414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314232058641387 and perplexity of 74.7561930227329
Finished 9 epochs...
Completing Train Step...
At time: 101.71106910705566 and batch: 50, loss is 3.7734162282943724 and perplexity is 43.5285144652929
At time: 102.23721742630005 and batch: 100, loss is 3.6470999336242675 and perplexity is 38.36324860079207
At time: 102.74963426589966 and batch: 150, loss is 3.7573646450042726 and perplexity is 42.835390639344574
At time: 103.26134181022644 and batch: 200, loss is 3.816009817123413 and perplexity is 45.42260175237141
At time: 103.77537798881531 and batch: 250, loss is 3.790217332839966 and perplexity is 44.26601969041148
At time: 104.29108309745789 and batch: 300, loss is 3.633549771308899 and perplexity is 37.846926380256775
At time: 104.80831813812256 and batch: 350, loss is 3.670195426940918 and perplexity is 39.2595774897891
At time: 105.32393622398376 and batch: 400, loss is 3.6781038570404054 and perplexity is 39.571290068045435
At time: 105.83856081962585 and batch: 450, loss is 3.731201205253601 and perplexity is 41.729203409033566
At time: 106.35125422477722 and batch: 500, loss is 3.757103133201599 and perplexity is 42.824190143714986
At time: 106.86414861679077 and batch: 550, loss is 3.6530346727371215 and perplexity is 38.591601409696025
At time: 107.37825059890747 and batch: 600, loss is 3.6003106355667116 and perplexity is 36.60960492292124
At time: 107.89439868927002 and batch: 650, loss is 3.5692089128494264 and perplexity is 35.488507541701345
At time: 108.41092491149902 and batch: 700, loss is 3.6626357364654543 and perplexity is 38.963906235457834
At time: 108.94952750205994 and batch: 750, loss is 3.617195072174072 and perplexity is 37.23298538078482
At time: 109.46510982513428 and batch: 800, loss is 3.725590410232544 and perplexity is 41.49572501453137
At time: 109.97853136062622 and batch: 850, loss is 3.59848078250885 and perplexity is 36.54267597914734
At time: 110.48925518989563 and batch: 900, loss is 3.529111375808716 and perplexity is 34.09365770068527
At time: 111.00158333778381 and batch: 950, loss is 3.5619315338134765 and perplexity is 35.23118168546505
At time: 111.51644515991211 and batch: 1000, loss is 3.4714340353012085 and perplexity is 32.182860727604236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.312038142506669 and perplexity of 74.59236398398868
Finished 10 epochs...
Completing Train Step...
At time: 113.00023293495178 and batch: 50, loss is 3.7513558769226076 and perplexity is 42.57877445696794
At time: 113.51678228378296 and batch: 100, loss is 3.6250260305404662 and perplexity is 37.52569996113897
At time: 114.0209150314331 and batch: 150, loss is 3.735716323852539 and perplexity is 41.9180417043048
At time: 114.52548456192017 and batch: 200, loss is 3.7948816204071045 and perplexity is 44.472971401651
At time: 115.02951574325562 and batch: 250, loss is 3.769163498878479 and perplexity is 43.3437925359896
At time: 115.53215646743774 and batch: 300, loss is 3.6122842502593993 and perplexity is 37.05058904474855
At time: 116.03611707687378 and batch: 350, loss is 3.6500306272506715 and perplexity is 38.47584444031906
At time: 116.53881812095642 and batch: 400, loss is 3.65831693649292 and perplexity is 38.795991773760015
At time: 117.04724168777466 and batch: 450, loss is 3.7123231220245363 and perplexity is 40.948825232008616
At time: 117.55863928794861 and batch: 500, loss is 3.738742971420288 and perplexity is 42.04510503423521
At time: 118.07236433029175 and batch: 550, loss is 3.635798053741455 and perplexity is 37.93211268551744
At time: 118.59006357192993 and batch: 600, loss is 3.5847835779190063 and perplexity is 36.04555582043691
At time: 119.10016512870789 and batch: 650, loss is 3.5541964244842528 and perplexity is 34.95971590559858
At time: 119.61595630645752 and batch: 700, loss is 3.6492771673202515 and perplexity is 38.44686535190613
At time: 120.12859678268433 and batch: 750, loss is 3.6054047632217405 and perplexity is 36.79657474354153
At time: 120.64184474945068 and batch: 800, loss is 3.7150985527038576 and perplexity is 41.06263371856657
At time: 121.1513261795044 and batch: 850, loss is 3.588926296234131 and perplexity is 36.19519214127029
At time: 121.6723906993866 and batch: 900, loss is 3.520618886947632 and perplexity is 33.80534367706769
At time: 122.1819748878479 and batch: 950, loss is 3.5551115465164185 and perplexity is 34.99172295480631
At time: 122.69633054733276 and batch: 1000, loss is 3.466876230239868 and perplexity is 32.0365112915419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.311477288967225 and perplexity of 74.55054032220349
Finished 11 epochs...
Completing Train Step...
At time: 124.18846011161804 and batch: 50, loss is 3.733129620552063 and perplexity is 41.80975228415971
At time: 124.69783329963684 and batch: 100, loss is 3.607006616592407 and perplexity is 36.85556469483592
At time: 125.20554161071777 and batch: 150, loss is 3.717913465499878 and perplexity is 41.17838428910896
At time: 125.71631145477295 and batch: 200, loss is 3.777591710090637 and perplexity is 43.710646965913675
At time: 126.22847604751587 and batch: 250, loss is 3.7519604921340943 and perplexity is 42.604526015798704
At time: 126.74010634422302 and batch: 300, loss is 3.5948768424987794 and perplexity is 36.41121539742449
At time: 127.2523443698883 and batch: 350, loss is 3.633527703285217 and perplexity is 37.846091182604745
At time: 127.76570343971252 and batch: 400, loss is 3.6421314096450805 and perplexity is 38.173112618937
At time: 128.27867579460144 and batch: 450, loss is 3.6967887687683105 and perplexity is 40.31762703644062
At time: 128.7894401550293 and batch: 500, loss is 3.7237147665023804 and perplexity is 41.41796676426295
At time: 129.3003568649292 and batch: 550, loss is 3.6215458869934083 and perplexity is 37.39533211954976
At time: 129.81114292144775 and batch: 600, loss is 3.571784076690674 and perplexity is 35.580014034639966
At time: 130.32385396957397 and batch: 650, loss is 3.541544852256775 and perplexity is 34.5202066387727
At time: 130.83477568626404 and batch: 700, loss is 3.6378642559051513 and perplexity is 38.01056912434152
At time: 131.34489488601685 and batch: 750, loss is 3.595117211341858 and perplexity is 36.41996857109763
At time: 131.85678458213806 and batch: 800, loss is 3.705713086128235 and perplexity is 40.67904463929798
At time: 132.36815571784973 and batch: 850, loss is 3.5800291442871095 and perplexity is 35.87458637107229
At time: 132.87797474861145 and batch: 900, loss is 3.512306833267212 and perplexity is 33.52551642641225
At time: 133.38788390159607 and batch: 950, loss is 3.547924032211304 and perplexity is 34.74112112646857
At time: 133.9005184173584 and batch: 1000, loss is 3.4612376070022584 and perplexity is 31.856377804219708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.311674536728278 and perplexity of 74.56524669971964
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 135.36993527412415 and batch: 50, loss is 3.7317154598236084 and perplexity is 41.75066836134128
At time: 135.8830370903015 and batch: 100, loss is 3.6074295902252196 and perplexity is 36.87115692424275
At time: 136.38634967803955 and batch: 150, loss is 3.7200333976745608 and perplexity is 41.265772266429686
At time: 136.89054322242737 and batch: 200, loss is 3.7821511220932007 and perplexity is 43.91039683927774
At time: 137.39536595344543 and batch: 250, loss is 3.7507599210739135 and perplexity is 42.55340694700885
At time: 137.90163564682007 and batch: 300, loss is 3.59008141040802 and perplexity is 36.2370258774586
At time: 138.40856075286865 and batch: 350, loss is 3.6288556671142578 and perplexity is 37.66968528392097
At time: 138.91553926467896 and batch: 400, loss is 3.6362258768081666 and perplexity is 37.948344390194656
At time: 139.4245593547821 and batch: 450, loss is 3.687538843154907 and perplexity is 39.946411490336445
At time: 139.93706250190735 and batch: 500, loss is 3.71337251663208 and perplexity is 40.9918192632991
At time: 140.45019793510437 and batch: 550, loss is 3.6114231395721434 and perplexity is 37.018698119331965
At time: 140.9624981880188 and batch: 600, loss is 3.554968810081482 and perplexity is 34.98672871745776
At time: 141.47626972198486 and batch: 650, loss is 3.5200131940841675 and perplexity is 33.78487422138293
At time: 141.98795557022095 and batch: 700, loss is 3.6143742036819457 and perplexity is 37.128104023262466
At time: 142.49789452552795 and batch: 750, loss is 3.5711392164230347 and perplexity is 35.55707729355832
At time: 143.0056710243225 and batch: 800, loss is 3.6748590326309203 and perplexity is 39.44309627576911
At time: 143.51445055007935 and batch: 850, loss is 3.5455918169021605 and perplexity is 34.66019176095599
At time: 144.02589964866638 and batch: 900, loss is 3.4747329902648927 and perplexity is 32.28920585319001
At time: 144.5390281677246 and batch: 950, loss is 3.5055080699920653 and perplexity is 33.29835745160004
At time: 145.05535793304443 and batch: 1000, loss is 3.4164499282836913 and perplexity is 30.461083810957263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3019907881573936 and perplexity of 73.84666051795098
Finished 13 epochs...
Completing Train Step...
At time: 146.54091691970825 and batch: 50, loss is 3.722535419464111 and perplexity is 41.369149399793656
At time: 147.05608320236206 and batch: 100, loss is 3.5930980682373046 and perplexity is 36.3465056337396
At time: 147.55874013900757 and batch: 150, loss is 3.7064510440826415 and perplexity is 40.709075143128956
At time: 148.0742290019989 and batch: 200, loss is 3.7696208143234253 and perplexity is 43.36361885485425
At time: 148.57702827453613 and batch: 250, loss is 3.7401788854599 and perplexity is 42.105521556936594
At time: 149.08095693588257 and batch: 300, loss is 3.5786571502685547 and perplexity is 35.82540040229604
At time: 149.58833003044128 and batch: 350, loss is 3.619531445503235 and perplexity is 37.32007723471903
At time: 150.09508562088013 and batch: 400, loss is 3.627204909324646 and perplexity is 37.60755305424658
At time: 150.60428619384766 and batch: 450, loss is 3.6791433191299436 and perplexity is 39.612444309336574
At time: 151.11391592025757 and batch: 500, loss is 3.706107110977173 and perplexity is 40.69507635195608
At time: 151.62635493278503 and batch: 550, loss is 3.604439697265625 and perplexity is 36.76108075173123
At time: 152.13973212242126 and batch: 600, loss is 3.5493826389312746 and perplexity is 34.79183173362843
At time: 152.65416479110718 and batch: 650, loss is 3.514969334602356 and perplexity is 33.61489709393129
At time: 153.16672778129578 and batch: 700, loss is 3.6100204038619994 and perplexity is 36.96680707275766
At time: 153.67730045318604 and batch: 750, loss is 3.568495469093323 and perplexity is 35.46319751729529
At time: 154.18696904182434 and batch: 800, loss is 3.6730939865112306 and perplexity is 39.37353879587722
At time: 154.69691133499146 and batch: 850, loss is 3.544683198928833 and perplexity is 34.62871319092595
At time: 155.20690250396729 and batch: 900, loss is 3.47500593662262 and perplexity is 32.298020277199285
At time: 155.7205729484558 and batch: 950, loss is 3.507070393562317 and perplexity is 33.350420919699076
At time: 156.2349555492401 and batch: 1000, loss is 3.420148615837097 and perplexity is 30.5739584578581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.301169046541539 and perplexity of 73.78600256980499
Finished 14 epochs...
Completing Train Step...
At time: 157.72552466392517 and batch: 50, loss is 3.7187547874450684 and perplexity is 41.21304314506257
At time: 158.23722076416016 and batch: 100, loss is 3.5884443712234497 and perplexity is 36.17775297543361
At time: 158.74934339523315 and batch: 150, loss is 3.7016858577728273 and perplexity is 40.515550272800375
At time: 159.26094484329224 and batch: 200, loss is 3.7647887897491454 and perplexity is 43.15459020541126
At time: 159.77250814437866 and batch: 250, loss is 3.735346517562866 and perplexity is 41.902543014764845
At time: 160.2844455242157 and batch: 300, loss is 3.5736716318130495 and perplexity is 35.64723669566556
At time: 160.80696511268616 and batch: 350, loss is 3.6149506187438964 and perplexity is 37.14951139081497
At time: 161.31604599952698 and batch: 400, loss is 3.622666540145874 and perplexity is 37.43726280686723
At time: 161.82336497306824 and batch: 450, loss is 3.6748857259750367 and perplexity is 39.44414915796343
At time: 162.3308093547821 and batch: 500, loss is 3.7022273349761963 and perplexity is 40.537494460257506
At time: 162.83979201316833 and batch: 550, loss is 3.6008854579925536 and perplexity is 36.63065499427908
At time: 163.35065603256226 and batch: 600, loss is 3.546329965591431 and perplexity is 34.685785580934436
At time: 163.86302018165588 and batch: 650, loss is 3.5123003578186034 and perplexity is 33.52529933435645
At time: 164.37365579605103 and batch: 700, loss is 3.6077825927734377 and perplexity is 36.88417483413525
At time: 164.88303351402283 and batch: 750, loss is 3.567047996520996 and perplexity is 35.41190264447874
At time: 165.3905487060547 and batch: 800, loss is 3.6721213054656983 and perplexity is 39.33525952077563
At time: 165.89875316619873 and batch: 850, loss is 3.544233350753784 and perplexity is 34.613139030759115
At time: 166.4092481136322 and batch: 900, loss is 3.4751806020736695 and perplexity is 32.30366211818202
At time: 166.92374992370605 and batch: 950, loss is 3.5078468418121336 and perplexity is 33.376325851270764
At time: 167.43716192245483 and batch: 1000, loss is 3.4217645597457884 and perplexity is 30.623404199805712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3008806182117 and perplexity of 73.76472366518524
Finished 15 epochs...
Completing Train Step...
At time: 168.9111363887787 and batch: 50, loss is 3.7153889608383177 and perplexity is 41.07456037313573
At time: 169.4248492717743 and batch: 100, loss is 3.5847414112091065 and perplexity is 36.044035929986066
At time: 169.92534041404724 and batch: 150, loss is 3.69798321723938 and perplexity is 40.36581313658932
At time: 170.42593955993652 and batch: 200, loss is 3.7611103105545043 and perplexity is 42.99613855241201
At time: 170.92553758621216 and batch: 250, loss is 3.731652064323425 and perplexity is 41.74802164073351
At time: 171.42477536201477 and batch: 300, loss is 3.569921646118164 and perplexity is 35.51381039770667
At time: 171.9228551387787 and batch: 350, loss is 3.61142041683197 and perplexity is 37.01859732717265
At time: 172.42054414749146 and batch: 400, loss is 3.619202013015747 and perplexity is 37.307784813714946
At time: 172.9186761379242 and batch: 450, loss is 3.671668119430542 and perplexity is 39.3174373691517
At time: 173.41517281532288 and batch: 500, loss is 3.6992708826065064 and perplexity is 40.417824275465755
At time: 173.92612171173096 and batch: 550, loss is 3.5981747198104856 and perplexity is 36.53149334051328
At time: 174.42674446105957 and batch: 600, loss is 3.543970136642456 and perplexity is 34.604029563051675
At time: 174.92846369743347 and batch: 650, loss is 3.5102254438400267 and perplexity is 33.455809339943094
At time: 175.429603099823 and batch: 700, loss is 3.6060139513015748 and perplexity is 36.81899760743128
At time: 175.93415474891663 and batch: 750, loss is 3.5657803630828857 and perplexity is 35.36704177215762
At time: 176.4367983341217 and batch: 800, loss is 3.67120231628418 and perplexity is 39.299127447858794
At time: 176.93860363960266 and batch: 850, loss is 3.5436712217330935 and perplexity is 34.593687448474384
At time: 177.4404375553131 and batch: 900, loss is 3.475020937919617 and perplexity is 32.29850479302805
At time: 177.94362998008728 and batch: 950, loss is 3.5080701160430907 and perplexity is 33.38377875674729
At time: 178.4512221813202 and batch: 1000, loss is 3.422517890930176 and perplexity is 30.646482456855463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.300777528344131 and perplexity of 73.75711966154596
Finished 16 epochs...
Completing Train Step...
At time: 179.9166111946106 and batch: 50, loss is 3.7124457216262816 and perplexity is 40.95384584943056
At time: 180.4338150024414 and batch: 100, loss is 3.5814945316314697 and perplexity is 35.927195072573156
At time: 180.93995094299316 and batch: 150, loss is 3.6947545766830445 and perplexity is 40.23569659800301
At time: 181.44441509246826 and batch: 200, loss is 3.7579392290115354 and perplexity is 42.860010242088464
At time: 181.9501028060913 and batch: 250, loss is 3.7284738397598267 and perplexity is 41.615547680381866
At time: 182.45615100860596 and batch: 300, loss is 3.5667408514022827 and perplexity is 35.401027721612365
At time: 182.96155858039856 and batch: 350, loss is 3.6083940839767457 and perplexity is 36.906736079886244
At time: 183.46716976165771 and batch: 400, loss is 3.616232056617737 and perplexity is 37.19714669602951
At time: 183.97229027748108 and batch: 450, loss is 3.668917851448059 and perplexity is 39.20945244181243
At time: 184.482519865036 and batch: 500, loss is 3.6967217254638673 and perplexity is 40.3149241001047
At time: 184.99519991874695 and batch: 550, loss is 3.5958269214630127 and perplexity is 36.44582536573379
At time: 185.50738763809204 and batch: 600, loss is 3.541906099319458 and perplexity is 34.53267921472924
At time: 186.01906728744507 and batch: 650, loss is 3.508385066986084 and perplexity is 33.39429466525812
At time: 186.55335450172424 and batch: 700, loss is 3.6044172143936155 and perplexity is 36.760254266348674
At time: 187.06593346595764 and batch: 750, loss is 3.5645422315597535 and perplexity is 35.323279819976314
At time: 187.57815742492676 and batch: 800, loss is 3.6702383852005003 and perplexity is 39.26126404913557
At time: 188.08883929252625 and batch: 850, loss is 3.5429690647125245 and perplexity is 34.56940577373689
At time: 188.59893131256104 and batch: 900, loss is 3.47460898399353 and perplexity is 32.285202037421165
At time: 189.10930967330933 and batch: 950, loss is 3.507948470115662 and perplexity is 33.379718003011426
At time: 189.6200590133667 and batch: 1000, loss is 3.422810354232788 and perplexity is 30.655446739126145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.300785343821456 and perplexity of 73.75769611089483
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 191.098304271698 and batch: 50, loss is 3.712621250152588 and perplexity is 40.96103504857539
At time: 191.59657883644104 and batch: 100, loss is 3.582411346435547 and perplexity is 35.960148760791
At time: 192.09787273406982 and batch: 150, loss is 3.6958106327056885 and perplexity is 40.278210192138054
At time: 192.59872841835022 and batch: 200, loss is 3.7595844364166258 and perplexity is 42.9305818848976
At time: 193.09654569625854 and batch: 250, loss is 3.728430080413818 and perplexity is 41.6137266510754
At time: 193.59578847885132 and batch: 300, loss is 3.5644676065444947 and perplexity is 35.320643918034094
At time: 194.09545302391052 and batch: 350, loss is 3.6084099578857423 and perplexity is 36.90732193870606
At time: 194.5969295501709 and batch: 400, loss is 3.6173881816864015 and perplexity is 37.240176118711865
At time: 195.1015431880951 and batch: 450, loss is 3.66757218837738 and perplexity is 39.15672521413807
At time: 195.604594707489 and batch: 500, loss is 3.6936229991912843 and perplexity is 40.19019253990692
At time: 196.1069643497467 and batch: 550, loss is 3.595577530860901 and perplexity is 36.43673725269346
At time: 196.61084866523743 and batch: 600, loss is 3.53910346031189 and perplexity is 34.43603207770617
At time: 197.11370134353638 and batch: 650, loss is 3.5036775970458987 and perplexity is 33.23746146036867
At time: 197.6165680885315 and batch: 700, loss is 3.6000333070755004 and perplexity is 36.599453444136344
At time: 198.11863470077515 and batch: 750, loss is 3.5602481889724733 and perplexity is 35.17192534595173
At time: 198.62079763412476 and batch: 800, loss is 3.6639411878585815 and perplexity is 39.014804936796395
At time: 199.1229019165039 and batch: 850, loss is 3.535696210861206 and perplexity is 34.318899589448115
At time: 199.64433431625366 and batch: 900, loss is 3.4670131683349608 and perplexity is 32.04089861076029
At time: 200.15416765213013 and batch: 950, loss is 3.4997399282455444 and perplexity is 33.10684068482492
At time: 200.66420078277588 and batch: 1000, loss is 3.4121581649780275 and perplexity is 30.33063218319608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.299186520460175 and perplexity of 73.63986480410185
Finished 18 epochs...
Completing Train Step...
At time: 202.1285526752472 and batch: 50, loss is 3.7108017349243165 and perplexity is 40.88657358396603
At time: 202.64199924468994 and batch: 100, loss is 3.5806975412368773 and perplexity is 35.898572850528744
At time: 203.1419599056244 and batch: 150, loss is 3.69406964302063 and perplexity is 40.20814725078701
At time: 203.64232277870178 and batch: 200, loss is 3.7577210998535158 and perplexity is 42.85066224371421
At time: 204.1428439617157 and batch: 250, loss is 3.726939706802368 and perplexity is 41.551752844534896
At time: 204.64284896850586 and batch: 300, loss is 3.563167381286621 and perplexity is 35.27474896803987
At time: 205.14480662345886 and batch: 350, loss is 3.606925163269043 and perplexity is 36.85256280886552
At time: 205.64620089530945 and batch: 400, loss is 3.615678462982178 and perplexity is 37.176560291134756
At time: 206.1470808982849 and batch: 450, loss is 3.6663421392440796 and perplexity is 39.1085901285548
At time: 206.64942288398743 and batch: 500, loss is 3.6928948545455933 and perplexity is 40.160938918126234
At time: 207.15092420578003 and batch: 550, loss is 3.594382810592651 and perplexity is 36.39323153794322
At time: 207.6527009010315 and batch: 600, loss is 3.5380655670166017 and perplexity is 34.400309692115385
At time: 208.1556589603424 and batch: 650, loss is 3.502983260154724 and perplexity is 33.21439147480561
At time: 208.6563618183136 and batch: 700, loss is 3.5996327447891234 and perplexity is 36.58479601918642
At time: 209.1596975326538 and batch: 750, loss is 3.5598035049438477 and perplexity is 35.15628842949555
At time: 209.6618194580078 and batch: 800, loss is 3.663698844909668 and perplexity is 39.005351119495984
At time: 210.16433954238892 and batch: 850, loss is 3.535751724243164 and perplexity is 34.32080480051127
At time: 210.66598415374756 and batch: 900, loss is 3.467370505332947 and perplexity is 32.052350055173164
At time: 211.16811299324036 and batch: 950, loss is 3.5003180074691773 and perplexity is 33.12598459440001
At time: 211.67081212997437 and batch: 1000, loss is 3.4132966232299804 and perplexity is 30.36518200472507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298808400223895 and perplexity of 73.61202534468568
Finished 19 epochs...
Completing Train Step...
At time: 213.1628019809723 and batch: 50, loss is 3.7099113035202027 and perplexity is 40.850183098860036
At time: 213.6798198223114 and batch: 100, loss is 3.579824767112732 and perplexity is 35.867255173668035
At time: 214.18540596961975 and batch: 150, loss is 3.6930838012695313 and perplexity is 40.168527912900366
At time: 214.69268655776978 and batch: 200, loss is 3.7566695499420164 and perplexity is 42.80562631653667
At time: 215.2001485824585 and batch: 250, loss is 3.7260931539535522 and perplexity is 41.51659197465819
At time: 215.70709896087646 and batch: 300, loss is 3.56233717918396 and perplexity is 35.245475950217575
At time: 216.21577835083008 and batch: 350, loss is 3.6060170888900758 and perplexity is 36.81911313047602
At time: 216.72341299057007 and batch: 400, loss is 3.614700927734375 and perplexity is 37.140236649770486
At time: 217.23067140579224 and batch: 450, loss is 3.665604815483093 and perplexity is 39.079765063802796
At time: 217.7378249168396 and batch: 500, loss is 3.692409882545471 and perplexity is 40.14146670937199
At time: 218.24587774276733 and batch: 550, loss is 3.5936348056793213 and perplexity is 36.36601940061589
At time: 218.75475358963013 and batch: 600, loss is 3.537500433921814 and perplexity is 34.38087443090935
At time: 219.260324716568 and batch: 650, loss is 3.5026096391677854 and perplexity is 33.20198419903587
At time: 219.7673065662384 and batch: 700, loss is 3.5994008064270018 and perplexity is 36.57631158549004
At time: 220.2759711742401 and batch: 750, loss is 3.559641661643982 and perplexity is 35.15059908016904
At time: 220.78331208229065 and batch: 800, loss is 3.6636673974990845 and perplexity is 39.00412452149114
At time: 221.29139113426208 and batch: 850, loss is 3.535849723815918 and perplexity is 34.32416838953075
At time: 221.79955220222473 and batch: 900, loss is 3.4676641702651976 and perplexity is 32.06176408859868
At time: 222.30699157714844 and batch: 950, loss is 3.500738973617554 and perplexity is 33.13993244812705
At time: 222.81392216682434 and batch: 1000, loss is 3.4139820289611817 and perplexity is 30.38600160862294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298671071122333 and perplexity of 73.6019169654841
Finished 20 epochs...
Completing Train Step...
At time: 224.29983258247375 and batch: 50, loss is 3.7092585515975953 and perplexity is 40.82352676423672
At time: 224.8033390045166 and batch: 100, loss is 3.5791608715057373 and perplexity is 35.843450963152996
At time: 225.3198206424713 and batch: 150, loss is 3.6923459434509276 and perplexity is 40.138900182388525
At time: 225.8229455947876 and batch: 200, loss is 3.7558931922912597 and perplexity is 42.77240673785596
At time: 226.32563996315002 and batch: 250, loss is 3.7254380893707277 and perplexity is 41.489404831295566
At time: 226.82864499092102 and batch: 300, loss is 3.561670150756836 and perplexity is 35.221974054918775
At time: 227.33096432685852 and batch: 350, loss is 3.6053334426879884 and perplexity is 36.793950485773415
At time: 227.8336741924286 and batch: 400, loss is 3.6139965867996215 and perplexity is 37.11408647117332
At time: 228.33863735198975 and batch: 450, loss is 3.6650448560714723 and perplexity is 39.057888107226894
At time: 228.84274101257324 and batch: 500, loss is 3.6919952917098997 and perplexity is 40.12482787454044
At time: 229.34756088256836 and batch: 550, loss is 3.593079056739807 and perplexity is 36.34581463880715
At time: 229.85197019577026 and batch: 600, loss is 3.537091989517212 and perplexity is 34.36683462255848
At time: 230.35977482795715 and batch: 650, loss is 3.5023348760604858 and perplexity is 33.19286277186205
At time: 230.86763072013855 and batch: 700, loss is 3.5992079067230223 and perplexity is 36.56925670627646
At time: 231.37392592430115 and batch: 750, loss is 3.5595484733581544 and perplexity is 35.147323608715105
At time: 231.87837505340576 and batch: 800, loss is 3.6636614656448363 and perplexity is 39.003893155395616
At time: 232.38290882110596 and batch: 850, loss is 3.535917110443115 and perplexity is 34.32648145740391
At time: 232.8868157863617 and batch: 900, loss is 3.4678707933425903 and perplexity is 32.06838947341644
At time: 233.3909397125244 and batch: 950, loss is 3.501028652191162 and perplexity is 33.149533767069144
At time: 233.89596676826477 and batch: 1000, loss is 3.4144219255447386 and perplexity is 30.399371247332127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2986018483231705 and perplexity of 73.59682221110667
Finished 21 epochs...
Completing Train Step...
At time: 235.3878514766693 and batch: 50, loss is 3.7087029600143433 and perplexity is 40.800851855945226
At time: 235.90485763549805 and batch: 100, loss is 3.5785828971862794 and perplexity is 35.82274035465201
At time: 236.40668439865112 and batch: 150, loss is 3.691725378036499 and perplexity is 40.11399909633764
At time: 236.91007924079895 and batch: 200, loss is 3.7552525711059572 and perplexity is 42.745014602881106
At time: 237.41438746452332 and batch: 250, loss is 3.7248691749572753 and perplexity is 41.46580762391364
At time: 237.93188977241516 and batch: 300, loss is 3.561087555885315 and perplexity is 35.201459889773474
At time: 238.44354581832886 and batch: 350, loss is 3.604761338233948 and perplexity is 36.77290652306513
At time: 238.95393896102905 and batch: 400, loss is 3.6134239339828493 and perplexity is 37.09283906928658
At time: 239.45828700065613 and batch: 450, loss is 3.664568848609924 and perplexity is 39.03930068528227
At time: 239.96090292930603 and batch: 500, loss is 3.6916145706176757 and perplexity is 40.109554413895545
At time: 240.46292757987976 and batch: 550, loss is 3.592625393867493 and perplexity is 36.32932963174206
At time: 240.96450757980347 and batch: 600, loss is 3.5367537212371825 and perplexity is 34.35521137851887
At time: 241.46782159805298 and batch: 650, loss is 3.502098488807678 and perplexity is 33.1850173295364
At time: 241.97132658958435 and batch: 700, loss is 3.5990274858474733 and perplexity is 36.56265944412315
At time: 242.47890090942383 and batch: 750, loss is 3.559470558166504 and perplexity is 35.14458520494314
At time: 242.98555445671082 and batch: 800, loss is 3.663645029067993 and perplexity is 39.00325207017722
At time: 243.4911539554596 and batch: 850, loss is 3.535953235626221 and perplexity is 34.327721530230725
At time: 243.99543118476868 and batch: 900, loss is 3.468012194633484 and perplexity is 32.072924305692865
At time: 244.49867916107178 and batch: 950, loss is 3.501228718757629 and perplexity is 33.15616654394623
At time: 245.00245881080627 and batch: 1000, loss is 3.4147229194641113 and perplexity is 30.40852265041958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298560537943026 and perplexity of 73.59378196120109
Finished 22 epochs...
Completing Train Step...
At time: 246.47241640090942 and batch: 50, loss is 3.70819908618927 and perplexity is 40.7802985532248
At time: 246.98783135414124 and batch: 100, loss is 3.578053741455078 and perplexity is 35.80378956068882
At time: 247.489901304245 and batch: 150, loss is 3.691172652244568 and perplexity is 40.09183318082053
At time: 247.99205994606018 and batch: 200, loss is 3.754691686630249 and perplexity is 42.72104631012643
At time: 248.49599432945251 and batch: 250, loss is 3.724351234436035 and perplexity is 41.444336362797344
At time: 249.00000834465027 and batch: 300, loss is 3.56055805683136 and perplexity is 35.18282568389657
At time: 249.50254344940186 and batch: 350, loss is 3.604254713058472 and perplexity is 36.754281161282094
At time: 250.00688195228577 and batch: 400, loss is 3.612925901412964 and perplexity is 37.07437022674487
At time: 250.51131200790405 and batch: 450, loss is 3.664139552116394 and perplexity is 39.0225448472568
At time: 251.02747750282288 and batch: 500, loss is 3.6912557077407837 and perplexity is 40.095163166204436
At time: 251.53028416633606 and batch: 550, loss is 3.5922318983078 and perplexity is 36.315037014070576
At time: 252.0339686870575 and batch: 600, loss is 3.536452865600586 and perplexity is 34.34487697418903
At time: 252.53544902801514 and batch: 650, loss is 3.501878843307495 and perplexity is 33.17772919023927
At time: 253.03879928588867 and batch: 700, loss is 3.598851776123047 and perplexity is 36.55623559369095
At time: 253.54219579696655 and batch: 750, loss is 3.559390969276428 and perplexity is 35.14178819772133
At time: 254.0455355644226 and batch: 800, loss is 3.6636123609542848 and perplexity is 39.00197792831562
At time: 254.54566311836243 and batch: 850, loss is 3.5359646320343017 and perplexity is 34.328112745182985
At time: 255.04457569122314 and batch: 900, loss is 3.468106942176819 and perplexity is 32.07596328044382
At time: 255.54368996620178 and batch: 950, loss is 3.501368160247803 and perplexity is 33.16079021157562
At time: 256.042405128479 and batch: 1000, loss is 3.4149397087097166 and perplexity is 30.41511560572002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298537463676639 and perplexity of 73.59208385826298
Finished 23 epochs...
Completing Train Step...
At time: 257.5030708312988 and batch: 50, loss is 3.7077270221710203 and perplexity is 40.761052184740784
At time: 258.0007574558258 and batch: 100, loss is 3.577557182312012 and perplexity is 35.7860152749833
At time: 258.4969401359558 and batch: 150, loss is 3.6906639528274536 and perplexity is 40.07144367515491
At time: 258.9935784339905 and batch: 200, loss is 3.754182391166687 and perplexity is 42.69929421463403
At time: 259.49193596839905 and batch: 250, loss is 3.7238683557510375 and perplexity is 41.42432860720201
At time: 259.9905228614807 and batch: 300, loss is 3.560065755844116 and perplexity is 35.16550940683823
At time: 260.49198508262634 and batch: 350, loss is 3.603790493011475 and perplexity is 36.73722304681974
At time: 260.9920291900635 and batch: 400, loss is 3.6124747657775877 and perplexity is 37.0576484293594
At time: 261.4936566352844 and batch: 450, loss is 3.66374041557312 and perplexity is 39.00697263152363
At time: 261.99561834335327 and batch: 500, loss is 3.6909126615524293 and perplexity is 40.081411032252255
At time: 262.4982342720032 and batch: 550, loss is 3.5918761920928954 and perplexity is 36.302121826852535
At time: 263.0008318424225 and batch: 600, loss is 3.5361741399765014 and perplexity is 34.33530551088829
At time: 263.51602125167847 and batch: 650, loss is 3.5016668319702147 and perplexity is 33.1706958811027
At time: 264.0177915096283 and batch: 700, loss is 3.5986775588989257 and perplexity is 36.54986742254015
At time: 264.52268648147583 and batch: 750, loss is 3.5593045568466186 and perplexity is 35.1387516416153
At time: 265.02323818206787 and batch: 800, loss is 3.6635643815994263 and perplexity is 39.00010668346733
At time: 265.5253405570984 and batch: 850, loss is 3.5359566020965576 and perplexity is 34.32783709368151
At time: 266.0288965702057 and batch: 900, loss is 3.468168034553528 and perplexity is 32.07792293713529
At time: 266.5333421230316 and batch: 950, loss is 3.5014655065536497 and perplexity is 33.16401844912764
At time: 267.0370316505432 and batch: 1000, loss is 3.415101718902588 and perplexity is 30.420043563644438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298520344059642 and perplexity of 73.5908240007575
Finished 24 epochs...
Completing Train Step...
At time: 268.49120020866394 and batch: 50, loss is 3.7072767925262453 and perplexity is 40.742704481344916
At time: 269.0051763057709 and batch: 100, loss is 3.5770849800109863 and perplexity is 35.769121025291625
At time: 269.50566029548645 and batch: 150, loss is 3.6901864910125735 and perplexity is 40.05231565774524
At time: 270.0067949295044 and batch: 200, loss is 3.7537090969085694 and perplexity is 42.67908966558225
At time: 270.50845742225647 and batch: 250, loss is 3.7234119749069214 and perplexity is 41.405427650490665
At time: 271.0103416442871 and batch: 300, loss is 3.559600977897644 and perplexity is 35.149169051202094
At time: 271.51124715805054 and batch: 350, loss is 3.603356261253357 and perplexity is 36.7212740409008
At time: 272.0118217468262 and batch: 400, loss is 3.612055630683899 and perplexity is 37.04211952299523
At time: 272.51255083084106 and batch: 450, loss is 3.6633621072769165 and perplexity is 38.99221876109909
At time: 273.01297187805176 and batch: 500, loss is 3.6905820989608764 and perplexity is 40.06816380678755
At time: 273.51364374160767 and batch: 550, loss is 3.591545968055725 and perplexity is 36.290135972742505
At time: 274.01536536216736 and batch: 600, loss is 3.53590950012207 and perplexity is 34.32622022285371
At time: 274.51890683174133 and batch: 650, loss is 3.501458458900452 and perplexity is 33.163784721450575
At time: 275.02203154563904 and batch: 700, loss is 3.5985037422180177 and perplexity is 36.54351499799181
At time: 275.52396631240845 and batch: 750, loss is 3.5592100858688354 and perplexity is 35.13543220618726
At time: 276.0258526802063 and batch: 800, loss is 3.663503465652466 and perplexity is 38.99773102739555
At time: 276.5389406681061 and batch: 850, loss is 3.535932855606079 and perplexity is 34.3270219377034
At time: 277.0394039154053 and batch: 900, loss is 3.468203763961792 and perplexity is 32.07906908281561
At time: 277.5404243469238 and batch: 950, loss is 3.5015323972702026 and perplexity is 33.166236888281134
At time: 278.04115080833435 and batch: 1000, loss is 3.4152256631851197 and perplexity is 30.423814187787336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2985102955887955 and perplexity of 73.59008452922323
Finished 25 epochs...
Completing Train Step...
At time: 279.485901594162 and batch: 50, loss is 3.706843123435974 and perplexity is 40.72503946042092
At time: 279.9954950809479 and batch: 100, loss is 3.576631784439087 and perplexity is 35.75291429071979
At time: 280.4926788806915 and batch: 150, loss is 3.6897323608398436 and perplexity is 40.03413082217115
At time: 280.98756980895996 and batch: 200, loss is 3.753261947631836 and perplexity is 42.66001000755219
At time: 281.48205280303955 and batch: 250, loss is 3.7229764366149904 and perplexity is 41.38739792785741
At time: 281.97795391082764 and batch: 300, loss is 3.5591579389572146 and perplexity is 35.13360004968031
At time: 282.4765770435333 and batch: 350, loss is 3.602944025993347 and perplexity is 36.706139356688226
At time: 282.9772746562958 and batch: 400, loss is 3.6116593742370604 and perplexity is 37.027444252107045
At time: 283.47801876068115 and batch: 450, loss is 3.662999234199524 and perplexity is 38.97807210155906
At time: 283.97867608070374 and batch: 500, loss is 3.6902614879608153 and perplexity is 40.05531957183038
At time: 284.47956132888794 and batch: 550, loss is 3.5912333297729493 and perplexity is 36.27879206031334
At time: 284.9811203479767 and batch: 600, loss is 3.5356544065475464 and perplexity is 34.31746494139267
At time: 285.4808351993561 and batch: 650, loss is 3.5012522077560426 and perplexity is 33.15694535823632
At time: 285.9836173057556 and batch: 700, loss is 3.5983297348022463 and perplexity is 36.537156708594594
At time: 286.48521304130554 and batch: 750, loss is 3.5591080379486084 and perplexity is 35.131846891344416
At time: 286.98804450035095 and batch: 800, loss is 3.6634316062927246 and perplexity is 38.99492877609776
At time: 287.48895359039307 and batch: 850, loss is 3.5358967113494875 and perplexity is 34.32578123543672
At time: 287.9897675514221 and batch: 900, loss is 3.468220019340515 and perplexity is 32.079590544470896
At time: 288.4926323890686 and batch: 950, loss is 3.501576566696167 and perplexity is 33.167701854279
At time: 289.00961208343506 and batch: 1000, loss is 3.4153220510482787 and perplexity is 30.426746815558637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298506946098514 and perplexity of 73.58983804036309
Finished 26 epochs...
Completing Train Step...
At time: 290.4799118041992 and batch: 50, loss is 3.7064224243164063 and perplexity is 40.70791007558677
At time: 290.9827792644501 and batch: 100, loss is 3.57619421005249 and perplexity is 35.73727315351006
At time: 291.4856023788452 and batch: 150, loss is 3.6892965984344483 and perplexity is 40.01668925349213
At time: 291.9906601905823 and batch: 200, loss is 3.7528351354599 and perplexity is 42.64180608113109
At time: 292.49518632888794 and batch: 250, loss is 3.7225575160980227 and perplexity is 41.37006352884272
At time: 292.9990437030792 and batch: 300, loss is 3.5587319278717042 and perplexity is 35.1186359342511
At time: 293.50273728370667 and batch: 350, loss is 3.602548875808716 and perplexity is 36.691637784282534
At time: 294.00605273246765 and batch: 400, loss is 3.6112803506851194 and perplexity is 37.01341263799141
At time: 294.5084898471832 and batch: 450, loss is 3.6626486110687257 and perplexity is 38.96440788352177
At time: 295.01313996315 and batch: 500, loss is 3.6899492073059084 and perplexity is 40.0428130232802
At time: 295.51638412475586 and batch: 550, loss is 3.5909336900711057 and perplexity is 36.26792312234113
At time: 296.02279806137085 and batch: 600, loss is 3.535406398773193 and perplexity is 34.30895499860072
At time: 296.5284240245819 and batch: 650, loss is 3.501047053337097 and perplexity is 33.15014376208996
At time: 297.0309011936188 and batch: 700, loss is 3.598155312538147 and perplexity is 36.53078437075269
At time: 297.5339159965515 and batch: 750, loss is 3.558999147415161 and perplexity is 35.12802157406962
At time: 298.0380210876465 and batch: 800, loss is 3.663350944519043 and perplexity is 38.99178350283145
At time: 298.54391074180603 and batch: 850, loss is 3.5358500623703004 and perplexity is 34.32418001013034
At time: 299.04881739616394 and batch: 900, loss is 3.468221068382263 and perplexity is 32.079624197318296
At time: 299.55303025245667 and batch: 950, loss is 3.501603226661682 and perplexity is 33.16858611585379
At time: 300.0589487552643 and batch: 1000, loss is 3.4153971910476684 and perplexity is 30.42903316719294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2985050852705795 and perplexity of 73.58970110246418
Finished 27 epochs...
Completing Train Step...
At time: 301.52317357063293 and batch: 50, loss is 3.7060124254226685 and perplexity is 40.691223298503225
At time: 302.0336318016052 and batch: 100, loss is 3.5757694482803344 and perplexity is 35.72209654948215
At time: 302.53247904777527 and batch: 150, loss is 3.688875503540039 and perplexity is 39.99984197735625
At time: 303.0285441875458 and batch: 200, loss is 3.752424283027649 and perplexity is 42.62429018985674
At time: 303.5245282649994 and batch: 250, loss is 3.7221521949768066 and perplexity is 41.35329876609416
At time: 304.0224735736847 and batch: 300, loss is 3.558320107460022 and perplexity is 35.104176340725175
At time: 304.5187795162201 and batch: 350, loss is 3.6021671056747437 and perplexity is 36.67763268634413
At time: 305.0130488872528 and batch: 400, loss is 3.610914559364319 and perplexity is 36.99987592885152
At time: 305.5105276107788 and batch: 450, loss is 3.6623074531555178 and perplexity is 38.951117134689824
At time: 306.01127433776855 and batch: 500, loss is 3.6896439456939696 and perplexity is 40.030591355128124
At time: 306.5130479335785 and batch: 550, loss is 3.5906437826156616 and perplexity is 36.25741030498019
At time: 307.01569175720215 and batch: 600, loss is 3.5351635313034055 and perplexity is 34.30062348127737
At time: 307.51824474334717 and batch: 650, loss is 3.5008427619934084 and perplexity is 33.143372166390066
At time: 308.0209228992462 and batch: 700, loss is 3.5979803943634034 and perplexity is 36.52439503145068
At time: 308.52225160598755 and batch: 750, loss is 3.5588843488693236 and perplexity is 35.12398915973686
At time: 309.0239682197571 and batch: 800, loss is 3.663263158798218 and perplexity is 38.98836073124784
At time: 309.52652645111084 and batch: 850, loss is 3.5357948303222657 and perplexity is 34.32228426772462
At time: 310.02811551094055 and batch: 900, loss is 3.4682095670700073 and perplexity is 32.07925524166509
At time: 310.5295934677124 and batch: 950, loss is 3.5016160440444946 and perplexity is 33.169011253043955
At time: 311.0302357673645 and batch: 1000, loss is 3.4154558849334715 and perplexity is 30.43081921780537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298508806926448 and perplexity of 73.5899749785168
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 312.4761164188385 and batch: 50, loss is 3.7059948444366455 and perplexity is 40.69050791296377
At time: 312.9840884208679 and batch: 100, loss is 3.575914011001587 and perplexity is 35.72726100625316
At time: 313.479505777359 and batch: 150, loss is 3.68907000541687 and perplexity is 40.00762277835946
At time: 313.9746267795563 and batch: 200, loss is 3.7526055192947387 and perplexity is 42.63201595717157
At time: 314.47085547447205 and batch: 250, loss is 3.722165040969849 and perplexity is 41.353829993694454
At time: 314.9821925163269 and batch: 300, loss is 3.5577336883544923 and perplexity is 35.0835966157968
At time: 315.47971200942993 and batch: 350, loss is 3.601867413520813 and perplexity is 36.666642334546644
At time: 315.97852420806885 and batch: 400, loss is 3.6106988477706907 and perplexity is 36.991895487418674
At time: 316.4770336151123 and batch: 450, loss is 3.66180477142334 and perplexity is 38.93154204009173
At time: 316.9756860733032 and batch: 500, loss is 3.689014959335327 and perplexity is 40.005420576105514
At time: 317.4717960357666 and batch: 550, loss is 3.590291895866394 and perplexity is 36.244654047241774
At time: 317.9726004600525 and batch: 600, loss is 3.5344993257522583 and perplexity is 34.27784838126453
At time: 318.47485303878784 and batch: 650, loss is 3.4998598432540895 and perplexity is 33.11081092994977
At time: 318.97586250305176 and batch: 700, loss is 3.596939220428467 and perplexity is 36.48638657349604
At time: 319.4771966934204 and batch: 750, loss is 3.5578225231170655 and perplexity is 35.08671339720959
At time: 319.9789128303528 and batch: 800, loss is 3.6621530628204346 and perplexity is 38.94510392286637
At time: 320.4807708263397 and batch: 850, loss is 3.5345512437820434 and perplexity is 34.27962806581622
At time: 320.98197770118713 and batch: 900, loss is 3.4667768907546996 and perplexity is 32.03332895907185
At time: 321.4837849140167 and batch: 950, loss is 3.5000941562652588 and perplexity is 33.11857013276661
At time: 321.9863085746765 and batch: 1000, loss is 3.413749408721924 and perplexity is 30.378934031721244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298475684189215 and perplexity of 73.58753751748053
Finished 29 epochs...
Completing Train Step...
At time: 323.44661951065063 and batch: 50, loss is 3.7058844709396364 and perplexity is 40.68601700715338
At time: 323.9416847229004 and batch: 100, loss is 3.5758013486862184 and perplexity is 35.723236117037345
At time: 324.4376313686371 and batch: 150, loss is 3.688961343765259 and perplexity is 40.00327572017486
At time: 324.9341518878937 and batch: 200, loss is 3.7525175476074217 and perplexity is 42.62826571175422
At time: 325.42876958847046 and batch: 250, loss is 3.7220816898345945 and perplexity is 41.350383248664414
At time: 325.9256603717804 and batch: 300, loss is 3.5576777553558347 and perplexity is 35.08163433991288
At time: 326.42655301094055 and batch: 350, loss is 3.6017950582504272 and perplexity is 36.663989405704235
At time: 326.9267861843109 and batch: 400, loss is 3.6106215715408325 and perplexity is 36.989037003647944
At time: 327.4419412612915 and batch: 450, loss is 3.661744418144226 and perplexity is 38.929192464771646
At time: 327.94355154037476 and batch: 500, loss is 3.688978123664856 and perplexity is 40.00394697675679
At time: 328.44383811950684 and batch: 550, loss is 3.5902489757537843 and perplexity is 36.24309845599188
At time: 328.94402146339417 and batch: 600, loss is 3.534461030960083 and perplexity is 34.276535743318306
At time: 329.444744348526 and batch: 650, loss is 3.499839644432068 and perplexity is 33.11014213732724
At time: 329.9450168609619 and batch: 700, loss is 3.5969330883026123 and perplexity is 36.48616283506759
At time: 330.446408033371 and batch: 750, loss is 3.5578333854675295 and perplexity is 35.087094523457104
At time: 330.94746136665344 and batch: 800, loss is 3.662153306007385 and perplexity is 38.94511339380859
At time: 331.4479100704193 and batch: 850, loss is 3.5345564460754395 and perplexity is 34.27980639896279
At time: 331.94965720176697 and batch: 900, loss is 3.4668164587020875 and perplexity is 32.03459647722315
At time: 332.450430393219 and batch: 950, loss is 3.5001165580749514 and perplexity is 33.1193120569822
At time: 332.9502227306366 and batch: 1000, loss is 3.4137953615188597 and perplexity is 30.3803300607834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298447399604611 and perplexity of 73.58545615398519
Finished 30 epochs...
Completing Train Step...
At time: 334.398845911026 and batch: 50, loss is 3.7057853031158445 and perplexity is 40.681982463439795
At time: 334.92325162887573 and batch: 100, loss is 3.5757018089294434 and perplexity is 35.719680411773
At time: 335.4247443675995 and batch: 150, loss is 3.6888640308380127 and perplexity is 39.99938307372058
At time: 335.92220091819763 and batch: 200, loss is 3.752432761192322 and perplexity is 42.62465156713994
At time: 336.42040395736694 and batch: 250, loss is 3.7220063066482543 and perplexity is 41.34726624250514
At time: 336.9201600551605 and batch: 300, loss is 3.5576211500167845 and perplexity is 35.07964858830923
At time: 337.4192247390747 and batch: 350, loss is 3.601725163459778 and perplexity is 36.66142687339523
At time: 337.9182724952698 and batch: 400, loss is 3.610548105239868 and perplexity is 36.98631965574102
At time: 338.41954708099365 and batch: 450, loss is 3.6616886043548584 and perplexity is 38.927019739657744
At time: 338.92200922966003 and batch: 500, loss is 3.6889414739608766 and perplexity is 40.00248087080843
At time: 339.4256820678711 and batch: 550, loss is 3.590206713676453 and perplexity is 36.24156677972834
At time: 339.9293932914734 and batch: 600, loss is 3.534423794746399 and perplexity is 34.27525943867154
At time: 340.44620871543884 and batch: 650, loss is 3.4998189067840575 and perplexity is 33.10945551797348
At time: 340.9494597911835 and batch: 700, loss is 3.5969259548187256 and perplexity is 36.48590256254124
At time: 341.45167541503906 and batch: 750, loss is 3.5578396463394166 and perplexity is 35.08731419994849
At time: 341.9531283378601 and batch: 800, loss is 3.6621536302566526 and perplexity is 38.94512602173513
At time: 342.4555435180664 and batch: 850, loss is 3.5345613956451416 and perplexity is 34.279976069673836
At time: 342.95873975753784 and batch: 900, loss is 3.466849913597107 and perplexity is 32.035668209212524
At time: 343.4614338874817 and batch: 950, loss is 3.5001388597488403 and perplexity is 33.12005068131538
At time: 343.96571135520935 and batch: 1000, loss is 3.41383768081665 and perplexity is 30.381615762223014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298424325338224 and perplexity of 73.58375824315675
Finished 31 epochs...
Completing Train Step...
At time: 345.41608452796936 and batch: 50, loss is 3.7056940269470213 and perplexity is 40.67826933740293
At time: 345.9255428314209 and batch: 100, loss is 3.5756111001968383 and perplexity is 35.71644047178143
At time: 346.4229667186737 and batch: 150, loss is 3.6887743425369264 and perplexity is 39.995795757880735
At time: 346.9184024333954 and batch: 200, loss is 3.752351093292236 and perplexity is 42.62117064349636
At time: 347.4140079021454 and batch: 250, loss is 3.721936068534851 and perplexity is 41.34436219051865
At time: 347.91253423690796 and batch: 300, loss is 3.5575639247894286 and perplexity is 35.07764120488024
At time: 348.4108033180237 and batch: 350, loss is 3.601657385826111 and perplexity is 36.658942132840764
At time: 348.9132673740387 and batch: 400, loss is 3.6104777479171752 and perplexity is 36.9837174888556
At time: 349.41528153419495 and batch: 450, loss is 3.66163583278656 and perplexity is 38.92496555397868
At time: 349.91705203056335 and batch: 500, loss is 3.6889047527313235 and perplexity is 40.001011957496
At time: 350.4175601005554 and batch: 550, loss is 3.5901644611358643 and perplexity is 36.24003551380715
At time: 350.91879987716675 and batch: 600, loss is 3.5343873071670533 and perplexity is 34.27400884023898
At time: 351.42065477371216 and batch: 650, loss is 3.4997975969314576 and perplexity is 33.10874996787434
At time: 351.9235808849335 and batch: 700, loss is 3.5969175481796265 and perplexity is 36.48559584001545
At time: 352.42524909973145 and batch: 750, loss is 3.557842564582825 and perplexity is 35.08741659342127
At time: 352.9393470287323 and batch: 800, loss is 3.662153639793396 and perplexity is 38.945126393144804
At time: 353.4411745071411 and batch: 850, loss is 3.5345657587051393 and perplexity is 34.280125635592434
At time: 353.94270634651184 and batch: 900, loss is 3.466878209114075 and perplexity is 32.03657468783049
At time: 354.4452311992645 and batch: 950, loss is 3.5001602745056153 and perplexity is 33.12075994673944
At time: 354.9468355178833 and batch: 1000, loss is 3.4138763475418092 and perplexity is 30.38279054252189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298403484065358 and perplexity of 73.58222467995348
Finished 32 epochs...
Completing Train Step...
At time: 356.4038791656494 and batch: 50, loss is 3.70560884475708 and perplexity is 40.674804420914434
At time: 356.89921617507935 and batch: 100, loss is 3.5755270195007323 and perplexity is 35.713437534850414
At time: 357.39779448509216 and batch: 150, loss is 3.6886903047561646 and perplexity is 39.992434741193605
At time: 357.89378690719604 and batch: 200, loss is 3.7522720861434937 and perplexity is 42.617803399347686
At time: 358.39074635505676 and batch: 250, loss is 3.7218695211410524 and perplexity is 41.34161092251248
At time: 358.9028446674347 and batch: 300, loss is 3.5575066137313844 and perplexity is 35.075630925755256
At time: 359.41423773765564 and batch: 350, loss is 3.6015913820266725 and perplexity is 36.656522583227215
At time: 359.91319513320923 and batch: 400, loss is 3.6104100131988526 and perplexity is 36.98121249200756
At time: 360.4097201824188 and batch: 450, loss is 3.661585221290588 and perplexity is 38.922995553094104
At time: 360.9062852859497 and batch: 500, loss is 3.6888681411743165 and perplexity is 39.99954748497485
At time: 361.4026243686676 and batch: 550, loss is 3.5901224994659424 and perplexity is 36.23851485330391
At time: 361.89810037612915 and batch: 600, loss is 3.5343512392044065 and perplexity is 34.2727726688616
At time: 362.394348859787 and batch: 650, loss is 3.4997755670547486 and perplexity is 33.10802059422859
At time: 362.89161586761475 and batch: 700, loss is 3.596908006668091 and perplexity is 36.485247713942684
At time: 363.38949704170227 and batch: 750, loss is 3.557842788696289 and perplexity is 35.087424456984635
At time: 363.8846061229706 and batch: 800, loss is 3.662152862548828 and perplexity is 38.945096123268634
At time: 364.3816592693329 and batch: 850, loss is 3.53456955909729 and perplexity is 34.28025591376038
At time: 364.87932562828064 and batch: 900, loss is 3.466902732849121 and perplexity is 32.03736035393362
At time: 365.3788642883301 and batch: 950, loss is 3.500180358886719 and perplexity is 33.121425163384856
At time: 365.896550655365 and batch: 1000, loss is 3.413911962509155 and perplexity is 30.383872643884327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298384875786014 and perplexity of 73.58085545410138
Finished 33 epochs...
Completing Train Step...
At time: 367.342524766922 and batch: 50, loss is 3.7055283403396606 and perplexity is 40.671530051283256
At time: 367.8499436378479 and batch: 100, loss is 3.575447793006897 and perplexity is 35.71060819649249
At time: 368.3450417518616 and batch: 150, loss is 3.6886103630065916 and perplexity is 39.9892378037768
At time: 368.8422019481659 and batch: 200, loss is 3.7521956729888917 and perplexity is 42.61454696296661
At time: 369.33938241004944 and batch: 250, loss is 3.7218055057525636 and perplexity is 41.33896450793505
At time: 369.8361859321594 and batch: 300, loss is 3.5574492263793944 and perplexity is 35.073618085933376
At time: 370.33313941955566 and batch: 350, loss is 3.6015274238586428 and perplexity is 36.65417817416931
At time: 370.8303141593933 and batch: 400, loss is 3.6103447675704956 and perplexity is 36.97879970827376
At time: 371.3275468349457 and batch: 450, loss is 3.6615361070632932 and perplexity is 38.92108392718791
At time: 371.82607316970825 and batch: 500, loss is 3.688831300735474 and perplexity is 39.998073911235714
At time: 372.3246548175812 and batch: 550, loss is 3.590080614089966 and perplexity is 36.23699702127215
At time: 372.82641649246216 and batch: 600, loss is 3.534315781593323 and perplexity is 34.27155745976188
At time: 373.32734298706055 and batch: 650, loss is 3.4997532510757448 and perplexity is 33.10728176458004
At time: 373.82758593559265 and batch: 700, loss is 3.596897339820862 and perplexity is 36.484858533454876
At time: 374.32787013053894 and batch: 750, loss is 3.5578411436080932 and perplexity is 35.08736673512431
At time: 374.8305175304413 and batch: 800, loss is 3.6621513319015504 and perplexity is 38.94503651210889
At time: 375.33368706703186 and batch: 850, loss is 3.5345727682113646 and perplexity is 34.280365923188626
At time: 375.8371081352234 and batch: 900, loss is 3.4669238758087157 and perplexity is 32.0380377257099
At time: 376.3397958278656 and batch: 950, loss is 3.500199136734009 and perplexity is 33.12204711828807
At time: 376.8425290584564 and batch: 1000, loss is 3.4139445781707765 and perplexity is 30.384863650154298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298371105659299 and perplexity of 73.579842243374
Finished 34 epochs...
Completing Train Step...
At time: 378.294634103775 and batch: 50, loss is 3.7054515647888184 and perplexity is 40.66840759202577
At time: 378.80242443084717 and batch: 100, loss is 3.575372405052185 and perplexity is 35.707916148254306
At time: 379.29680490493774 and batch: 150, loss is 3.6885336875915526 and perplexity is 39.98617172991885
At time: 379.7901883125305 and batch: 200, loss is 3.7521218490600585 and perplexity is 42.611401105805555
At time: 380.2853581905365 and batch: 250, loss is 3.7217433643341065 and perplexity is 41.33639572585769
At time: 380.78131127357483 and batch: 300, loss is 3.557392020225525 and perplexity is 35.071611716529276
At time: 381.2797245979309 and batch: 350, loss is 3.601465034484863 and perplexity is 36.65189141428211
At time: 381.77653074264526 and batch: 400, loss is 3.6102815294265747 and perplexity is 36.9764613115545
At time: 382.2739088535309 and batch: 450, loss is 3.661487827301025 and perplexity is 38.919204871869226
At time: 382.77020502090454 and batch: 500, loss is 3.6887943983078 and perplexity is 39.99659791244026
At time: 383.2645833492279 and batch: 550, loss is 3.590038981437683 and perplexity is 36.23548841037935
At time: 383.758526802063 and batch: 600, loss is 3.5342806911468507 and perplexity is 34.27035487660904
At time: 384.25382566452026 and batch: 650, loss is 3.4997304010391237 and perplexity is 33.106525270622285
At time: 384.74936842918396 and batch: 700, loss is 3.5968856954574586 and perplexity is 36.4844336929769
At time: 385.24542689323425 and batch: 750, loss is 3.5578379678726195 and perplexity is 35.08725530710603
At time: 385.7405071258545 and batch: 800, loss is 3.6621490716934204 and perplexity is 38.944948488320215
At time: 386.23525309562683 and batch: 850, loss is 3.534575333595276 and perplexity is 34.28045386560065
At time: 386.7308690547943 and batch: 900, loss is 3.4669427108764648 and perplexity is 32.03864117000395
At time: 387.2262558937073 and batch: 950, loss is 3.5002164697647093 and perplexity is 33.12262122872316
At time: 387.7280352115631 and batch: 1000, loss is 3.413974471092224 and perplexity is 30.385771956072478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298359568526105 and perplexity of 73.57899334783059
Finished 35 epochs...
Completing Train Step...
At time: 389.193728685379 and batch: 50, loss is 3.7053779649734495 and perplexity is 40.66541451488197
At time: 389.69092750549316 and batch: 100, loss is 3.575299997329712 and perplexity is 35.70533071297562
At time: 390.1869070529938 and batch: 150, loss is 3.688459792137146 and perplexity is 39.98321704275921
At time: 390.6825249195099 and batch: 200, loss is 3.7520501041412353 and perplexity is 42.6083440639572
At time: 391.19090700149536 and batch: 250, loss is 3.721682596206665 and perplexity is 41.333883866815505
At time: 391.68627190589905 and batch: 300, loss is 3.557335057258606 and perplexity is 35.069613990370016
At time: 392.1831142902374 and batch: 350, loss is 3.6014041566848753 and perplexity is 36.64966019568395
At time: 392.6828804016113 and batch: 400, loss is 3.610220069885254 and perplexity is 36.974188825036336
At time: 393.18419575691223 and batch: 450, loss is 3.6614406538009643 and perplexity is 38.91736896005936
At time: 393.68612146377563 and batch: 500, loss is 3.6887575340270997 and perplexity is 39.995123493804606
At time: 394.19033312797546 and batch: 550, loss is 3.58999764919281 and perplexity is 36.23399074725038
At time: 394.6931972503662 and batch: 600, loss is 3.5342459869384766 and perplexity is 34.26916557170941
At time: 395.19371128082275 and batch: 650, loss is 3.4997073793411255 and perplexity is 33.10576311096887
At time: 395.69396114349365 and batch: 700, loss is 3.596873230934143 and perplexity is 36.48397893473665
At time: 396.19465827941895 and batch: 750, loss is 3.5578336763381957 and perplexity is 35.08710472926515
At time: 396.694207906723 and batch: 800, loss is 3.662146029472351 and perplexity is 38.9448300093576
At time: 397.19406747817993 and batch: 850, loss is 3.5345771884918213 and perplexity is 34.28051745235506
At time: 397.6936950683594 and batch: 900, loss is 3.4669591188430786 and perplexity is 32.03916686327138
At time: 398.19347286224365 and batch: 950, loss is 3.5002325439453124 and perplexity is 33.12315365199796
At time: 398.6931426525116 and batch: 1000, loss is 3.4140019941329958 and perplexity is 30.386608276421892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298349520055259 and perplexity of 73.57825399517577
Finished 36 epochs...
Completing Train Step...
At time: 400.13850474357605 and batch: 50, loss is 3.7053069734573363 and perplexity is 40.662527717922444
At time: 400.6440751552582 and batch: 100, loss is 3.5752298927307127 and perplexity is 35.702827692821494
At time: 401.1381528377533 and batch: 150, loss is 3.688388147354126 and perplexity is 39.980352556463714
At time: 401.63101744651794 and batch: 200, loss is 3.7519802331924437 and perplexity is 42.60536708253447
At time: 402.1231620311737 and batch: 250, loss is 3.721623067855835 and perplexity is 41.33142340210994
At time: 402.6168866157532 and batch: 300, loss is 3.5572782707214357 and perplexity is 35.06762256497522
At time: 403.10990810394287 and batch: 350, loss is 3.6013445043563843 and perplexity is 36.647474023320655
At time: 403.6023418903351 and batch: 400, loss is 3.6101603364944457 and perplexity is 36.97198029732751
At time: 404.1062242984772 and batch: 450, loss is 3.6613940954208375 and perplexity is 38.915557072581386
At time: 404.5987112522125 and batch: 500, loss is 3.6887204790115358 and perplexity is 39.99364150133886
At time: 405.092857837677 and batch: 550, loss is 3.5899565029144287 and perplexity is 36.232499884052174
At time: 405.5861644744873 and batch: 600, loss is 3.534211745262146 and perplexity is 34.267992158123725
At time: 406.07900190353394 and batch: 650, loss is 3.4996840953826904 and perplexity is 33.10499228673059
At time: 406.57159638404846 and batch: 700, loss is 3.5968599414825437 and perplexity is 36.48349408588614
At time: 407.06632590293884 and batch: 750, loss is 3.5578281927108764 and perplexity is 35.08691232518663
At time: 407.5618665218353 and batch: 800, loss is 3.6621423244476317 and perplexity is 38.94468571806703
At time: 408.05722308158875 and batch: 850, loss is 3.534578466415405 and perplexity is 34.280561260264776
At time: 408.5527448654175 and batch: 900, loss is 3.4669736051559448 and perplexity is 32.039630996028315
At time: 409.046612739563 and batch: 950, loss is 3.500247387886047 and perplexity is 33.123645333776956
At time: 409.5398938655853 and batch: 1000, loss is 3.4140273427963255 and perplexity is 30.387378546087437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298341332412347 and perplexity of 73.5776515651722
Finished 37 epochs...
Completing Train Step...
At time: 410.9815697669983 and batch: 50, loss is 3.705238118171692 and perplexity is 40.65972798435074
At time: 411.48968958854675 and batch: 100, loss is 3.575162048339844 and perplexity is 35.70040553839011
At time: 411.9857165813446 and batch: 150, loss is 3.6883184480667115 and perplexity is 39.97756605148978
At time: 412.4829330444336 and batch: 200, loss is 3.7519121170043945 and perplexity is 42.60246506617665
At time: 412.9793975353241 and batch: 250, loss is 3.7215646362304686 and perplexity is 41.32900841041849
At time: 413.47660970687866 and batch: 300, loss is 3.557221908569336 and perplexity is 35.06564613399691
At time: 413.9745864868164 and batch: 350, loss is 3.6012861633300783 and perplexity is 36.645336034441456
At time: 414.47165751457214 and batch: 400, loss is 3.610102038383484 and perplexity is 36.96982496354421
At time: 414.97020292282104 and batch: 450, loss is 3.6613479328155516 and perplexity is 38.913760670544384
At time: 415.46795296669006 and batch: 500, loss is 3.688683352470398 and perplexity is 39.992156703325286
At time: 415.9695041179657 and batch: 550, loss is 3.589915728569031 and perplexity is 36.231022557705984
At time: 416.483469247818 and batch: 600, loss is 3.534177846908569 and perplexity is 34.2668305492976
At time: 416.9856231212616 and batch: 650, loss is 3.4996605587005614 and perplexity is 33.10421311421985
At time: 417.48798537254333 and batch: 700, loss is 3.596845850944519 and perplexity is 36.48298001744721
At time: 417.98921489715576 and batch: 750, loss is 3.557821946144104 and perplexity is 35.0866931531305
At time: 418.49131655693054 and batch: 800, loss is 3.6621380376815797 and perplexity is 38.944518771668214
At time: 418.99975180625916 and batch: 850, loss is 3.534578981399536 and perplexity is 34.28057891421437
At time: 419.50870871543884 and batch: 900, loss is 3.4669863319396974 and perplexity is 32.040038760078275
At time: 420.0106737613678 and batch: 950, loss is 3.500260925292969 and perplexity is 33.12409374507772
At time: 420.51302218437195 and batch: 1000, loss is 3.4140505838394164 and perplexity is 30.38808478866852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298333516935023 and perplexity of 73.57707652295196
Finished 38 epochs...
Completing Train Step...
At time: 421.97300839424133 and batch: 50, loss is 3.7051711654663086 and perplexity is 40.65700579669197
At time: 422.46985507011414 and batch: 100, loss is 3.5750957345962524 and perplexity is 35.69803818934588
At time: 422.96518754959106 and batch: 150, loss is 3.688250365257263 and perplexity is 39.97484435912937
At time: 423.460173368454 and batch: 200, loss is 3.7518457412719726 and perplexity is 42.59963739020047
At time: 423.9568076133728 and batch: 250, loss is 3.7215068197250365 and perplexity is 41.326618980654125
At time: 424.4535286426544 and batch: 300, loss is 3.5571657180786134 and perplexity is 35.06367583348973
At time: 424.95051074028015 and batch: 350, loss is 3.601228804588318 and perplexity is 36.64323416435602
At time: 425.4443154335022 and batch: 400, loss is 3.6100449562072754 and perplexity is 36.967714705710875
At time: 425.93839836120605 and batch: 450, loss is 3.6613023233413697 and perplexity is 38.911985874855816
At time: 426.43374609947205 and batch: 500, loss is 3.688646159172058 and perplexity is 39.990669290770825
At time: 426.9302854537964 and batch: 550, loss is 3.5898753023147583 and perplexity is 36.22955790278097
At time: 427.42822337150574 and batch: 600, loss is 3.534144206047058 and perplexity is 34.26567780298646
At time: 427.9262707233429 and batch: 650, loss is 3.4996368312835693 and perplexity is 33.103427646069704
At time: 428.4231641292572 and batch: 700, loss is 3.596831045150757 and perplexity is 36.48243986196797
At time: 428.93412733078003 and batch: 750, loss is 3.557814974784851 and perplexity is 35.08644855204013
At time: 429.43216919898987 and batch: 800, loss is 3.6621330642700194 and perplexity is 38.94432508502999
At time: 429.92736864089966 and batch: 850, loss is 3.5345789766311646 and perplexity is 34.280578750751836
At time: 430.4231925010681 and batch: 900, loss is 3.4669975805282593 and perplexity is 32.040399167318824
At time: 430.91940546035767 and batch: 950, loss is 3.5002734375 and perplexity is 33.12450820318928
At time: 431.41568064689636 and batch: 1000, loss is 3.414072184562683 and perplexity is 30.3887412003681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298327562285633 and perplexity of 73.57663839856254
Finished 39 epochs...
Completing Train Step...
At time: 432.856737613678 and batch: 50, loss is 3.7051058912277224 and perplexity is 40.65435202820769
At time: 433.364054441452 and batch: 100, loss is 3.5750307750701906 and perplexity is 35.69571933702035
At time: 433.8584954738617 and batch: 150, loss is 3.6881838750839235 and perplexity is 39.972186513160004
At time: 434.3528778553009 and batch: 200, loss is 3.751780652999878 and perplexity is 42.59686474364524
At time: 434.8481640815735 and batch: 250, loss is 3.7214496517181397 and perplexity is 41.324256487745366
At time: 435.3436987400055 and batch: 300, loss is 3.557110171318054 and perplexity is 35.06172821397636
At time: 435.83667945861816 and batch: 350, loss is 3.6011723613739015 and perplexity is 36.64116596080174
At time: 436.3310248851776 and batch: 400, loss is 3.609989037513733 and perplexity is 36.965647577197394
At time: 436.82492780685425 and batch: 450, loss is 3.661257057189941 and perplexity is 38.91022451887603
At time: 437.32132053375244 and batch: 500, loss is 3.6886090564727785 and perplexity is 39.98918555651958
At time: 437.81745505332947 and batch: 550, loss is 3.5898351001739504 and perplexity is 36.22810142626968
At time: 438.31450605392456 and batch: 600, loss is 3.5341111469268798 and perplexity is 34.26454502855035
At time: 438.81226658821106 and batch: 650, loss is 3.4996130514144896 and perplexity is 33.10264046025382
At time: 439.30817794799805 and batch: 700, loss is 3.5968155813217164 and perplexity is 36.48187570811697
At time: 439.8038794994354 and batch: 750, loss is 3.5578073120117186 and perplexity is 35.086179693574955
At time: 440.2999563217163 and batch: 800, loss is 3.6621276187896727 and perplexity is 38.94411301505053
At time: 440.7962200641632 and batch: 850, loss is 3.5345783329010008 and perplexity is 34.28055668331636
At time: 441.29285764694214 and batch: 900, loss is 3.467007431983948 and perplexity is 32.04071481344625
At time: 441.8129036426544 and batch: 950, loss is 3.50028489112854 and perplexity is 33.124887601174535
At time: 442.3089370727539 and batch: 1000, loss is 3.4140922355651857 and perplexity is 30.38935053120279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29832421279535 and perplexity of 73.57639195473993
Finished 40 epochs...
Completing Train Step...
At time: 443.76866698265076 and batch: 50, loss is 3.7050419425964356 and perplexity is 40.65175232116437
At time: 444.2765393257141 and batch: 100, loss is 3.5749673652648926 and perplexity is 35.693455950168435
At time: 444.77423095703125 and batch: 150, loss is 3.6881187295913698 and perplexity is 39.969582590198996
At time: 445.2714431285858 and batch: 200, loss is 3.751717014312744 and perplexity is 42.59415402135126
At time: 445.76852893829346 and batch: 250, loss is 3.7213933753967283 and perplexity is 41.321930976041415
At time: 446.26544165611267 and batch: 300, loss is 3.5570546102523806 and perplexity is 35.05978020110977
At time: 446.7624523639679 and batch: 350, loss is 3.6011167097091676 and perplexity is 36.63912687565801
At time: 447.2637982368469 and batch: 400, loss is 3.609934158325195 and perplexity is 36.963618988118746
At time: 447.76538014411926 and batch: 450, loss is 3.6612122201919557 and perplexity is 38.90847994032878
At time: 448.2655756473541 and batch: 500, loss is 3.6885717916488647 and perplexity is 39.987695394326856
At time: 448.7638289928436 and batch: 550, loss is 3.589795250892639 and perplexity is 36.226657791228675
At time: 449.26390957832336 and batch: 600, loss is 3.534078106880188 and perplexity is 34.26341294508488
At time: 449.76307010650635 and batch: 650, loss is 3.4995890951156614 and perplexity is 33.101847453005746
At time: 450.2624216079712 and batch: 700, loss is 3.5967995357513427 and perplexity is 36.48129034030922
At time: 450.7619650363922 and batch: 750, loss is 3.557799015045166 and perplexity is 35.085888585923236
At time: 451.26210379600525 and batch: 800, loss is 3.6621216249465944 and perplexity is 38.94387959084786
At time: 451.7600381374359 and batch: 850, loss is 3.5345771169662474 and perplexity is 34.28051500042147
At time: 452.2572662830353 and batch: 900, loss is 3.4670161056518554 and perplexity is 32.040992725171314
At time: 452.75658917427063 and batch: 950, loss is 3.5002952909469602 and perplexity is 33.12523209578212
At time: 453.2554931640625 and batch: 1000, loss is 3.414110565185547 and perplexity is 30.38990756156611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298318630311547 and perplexity of 73.57598121687
Finished 41 epochs...
Completing Train Step...
At time: 454.73162508010864 and batch: 50, loss is 3.7049794149398805 and perplexity is 40.64921054182346
At time: 455.22423791885376 and batch: 100, loss is 3.5749047946929933 and perplexity is 35.69122266008642
At time: 455.72084283828735 and batch: 150, loss is 3.6880548620224 and perplexity is 39.96702991164377
At time: 456.2183258533478 and batch: 200, loss is 3.7516545152664182 and perplexity is 42.59149201053331
At time: 456.7172224521637 and batch: 250, loss is 3.7213375186920166 and perplexity is 41.31962293360519
At time: 457.215487241745 and batch: 300, loss is 3.556999487876892 and perplexity is 35.05784767600415
At time: 457.71300888061523 and batch: 350, loss is 3.6010619592666626 and perplexity is 36.63712092216249
At time: 458.20852994918823 and batch: 400, loss is 3.609880218505859 and perplexity is 36.96162523096046
At time: 458.709440946579 and batch: 450, loss is 3.661167573928833 and perplexity is 38.90674286087301
At time: 459.210088968277 and batch: 500, loss is 3.688534445762634 and perplexity is 39.98620204628941
At time: 459.71165895462036 and batch: 550, loss is 3.5897556209564208 and perplexity is 36.22522215953819
At time: 460.21266317367554 and batch: 600, loss is 3.5340454149246217 and perplexity is 34.262292825420886
At time: 460.71434926986694 and batch: 650, loss is 3.499564905166626 and perplexity is 33.101046730687635
At time: 461.2162435054779 and batch: 700, loss is 3.596782946586609 and perplexity is 36.48068515119387
At time: 461.71806240081787 and batch: 750, loss is 3.5577901649475097 and perplexity is 35.08557807375692
At time: 462.2195019721985 and batch: 800, loss is 3.662115068435669 and perplexity is 38.9436242557129
At time: 462.72096729278564 and batch: 850, loss is 3.534575424194336 and perplexity is 34.28045697137768
At time: 463.2221965789795 and batch: 900, loss is 3.4670236682891846 and perplexity is 32.04123504049524
At time: 463.723735332489 and batch: 950, loss is 3.5003048276901243 and perplexity is 33.12554800411923
At time: 464.2252435684204 and batch: 1000, loss is 3.414127779006958 and perplexity is 30.39043069251011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298316025152439 and perplexity of 73.57578953998208
Finished 42 epochs...
Completing Train Step...
At time: 465.6755402088165 and batch: 50, loss is 3.7049179220199586 and perplexity is 40.64671098002818
At time: 466.18191623687744 and batch: 100, loss is 3.574843306541443 and perplexity is 35.68902814024765
At time: 466.6760594844818 and batch: 150, loss is 3.687991909980774 and perplexity is 39.96451398470532
At time: 467.1828234195709 and batch: 200, loss is 3.751593117713928 and perplexity is 42.58887707744302
At time: 467.67791652679443 and batch: 250, loss is 3.7212821197509767 and perplexity is 41.31733393365517
At time: 468.1727285385132 and batch: 300, loss is 3.5569446849823 and perplexity is 35.055926457118
At time: 468.66832733154297 and batch: 350, loss is 3.601007785797119 and perplexity is 36.63513621596774
At time: 469.16647124290466 and batch: 400, loss is 3.6098270177841187 and perplexity is 36.959658898127095
At time: 469.66349482536316 and batch: 450, loss is 3.6611231899261476 and perplexity is 38.90501606221479
At time: 470.160879611969 and batch: 500, loss is 3.6884971332550047 and perplexity is 39.984710088655
At time: 470.65793108940125 and batch: 550, loss is 3.5897163009643553 and perplexity is 36.22379781209316
At time: 471.1553535461426 and batch: 600, loss is 3.5340129947662353 and perplexity is 34.26118205446659
At time: 471.65220618247986 and batch: 650, loss is 3.499540677070618 and perplexity is 33.10024476506456
At time: 472.14957785606384 and batch: 700, loss is 3.596765875816345 and perplexity is 36.480062403114005
At time: 472.6471872329712 and batch: 750, loss is 3.5577808809280396 and perplexity is 35.08525234007903
At time: 473.14355850219727 and batch: 800, loss is 3.662108135223389 and perplexity is 38.94335425223497
At time: 473.63997197151184 and batch: 850, loss is 3.534573125839233 and perplexity is 34.280378182805016
At time: 474.13461446762085 and batch: 900, loss is 3.4670301198959352 and perplexity is 32.041441758610354
At time: 474.62938499450684 and batch: 950, loss is 3.5003135585784912 and perplexity is 33.12583722084351
At time: 475.1245138645172 and batch: 1000, loss is 3.414143762588501 and perplexity is 30.39091644431922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298312675662157 and perplexity of 73.57554309900273
Finished 43 epochs...
Completing Train Step...
At time: 476.57763743400574 and batch: 50, loss is 3.704857530593872 and perplexity is 40.64425634130668
At time: 477.08559942245483 and batch: 100, loss is 3.574782581329346 and perplexity is 35.686860982245534
At time: 477.58241534233093 and batch: 150, loss is 3.6879300546646117 and perplexity is 39.962042043509655
At time: 478.0849242210388 and batch: 200, loss is 3.7515326356887817 and perplexity is 42.58630129380376
At time: 478.58556365966797 and batch: 250, loss is 3.7212271547317504 and perplexity is 41.31506298801298
At time: 479.0916037559509 and batch: 300, loss is 3.556890106201172 and perplexity is 35.05401319959277
At time: 479.5976550579071 and batch: 350, loss is 3.600954303741455 and perplexity is 36.63317694596674
At time: 480.1068127155304 and batch: 400, loss is 3.6097746515274047 and perplexity is 36.95772350981614
At time: 480.6010830402374 and batch: 450, loss is 3.6610790300369263 and perplexity is 38.90329805894902
At time: 481.09592604637146 and batch: 500, loss is 3.6884597730636597 and perplexity is 39.98321628013987
At time: 481.59234499931335 and batch: 550, loss is 3.589677195549011 and perplexity is 36.222381293131335
At time: 482.0887942314148 and batch: 600, loss is 3.533980703353882 and perplexity is 34.26007573037163
At time: 482.58674597740173 and batch: 650, loss is 3.499516348838806 and perplexity is 33.09943950443221
At time: 483.0834000110626 and batch: 700, loss is 3.5967483711242676 and perplexity is 36.47942383644364
At time: 483.5791611671448 and batch: 750, loss is 3.557771029472351 and perplexity is 35.0849067009728
At time: 484.0712432861328 and batch: 800, loss is 3.6621006870269777 and perplexity is 38.943064195563785
At time: 484.56415128707886 and batch: 850, loss is 3.534570450782776 and perplexity is 34.28028648098066
At time: 485.0624928474426 and batch: 900, loss is 3.467035846710205 and perplexity is 32.04162525452167
At time: 485.56179761886597 and batch: 950, loss is 3.5003215312957763 and perplexity is 33.12610132483131
At time: 486.0605285167694 and batch: 1000, loss is 3.414158616065979 and perplexity is 30.391367858464694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298310814834222 and perplexity of 73.57540618770422
Finished 44 epochs...
Completing Train Step...
At time: 487.5174140930176 and batch: 50, loss is 3.7047977876663207 and perplexity is 40.64182820697736
At time: 488.01307129859924 and batch: 100, loss is 3.5747226524353026 and perplexity is 35.6847223722179
At time: 488.51112151145935 and batch: 150, loss is 3.6878690624237063 and perplexity is 39.95960474334322
At time: 489.00645756721497 and batch: 200, loss is 3.751473140716553 and perplexity is 42.5837676983598
At time: 489.50256633758545 and batch: 250, loss is 3.721172742843628 and perplexity is 41.312815018586605
At time: 489.999618768692 and batch: 300, loss is 3.556835927963257 and perplexity is 35.052114086371525
At time: 490.49626779556274 and batch: 350, loss is 3.6009014558792116 and perplexity is 36.6312410120334
At time: 490.99231457710266 and batch: 400, loss is 3.6097229528427124 and perplexity is 36.95581289351006
At time: 491.4886360168457 and batch: 450, loss is 3.66103524684906 and perplexity is 38.901594785829126
At time: 491.98521614074707 and batch: 500, loss is 3.6884224367141725 and perplexity is 39.98172348067123
At time: 492.50293612480164 and batch: 550, loss is 3.58963849067688 and perplexity is 36.220979337626524
At time: 492.99817538261414 and batch: 600, loss is 3.5339487266540526 and perplexity is 34.2589802237293
At time: 493.4952235221863 and batch: 650, loss is 3.4994919729232787 and perplexity is 33.09863268512437
At time: 493.99101996421814 and batch: 700, loss is 3.5967303848266603 and perplexity is 36.47876771257061
At time: 494.4872145652771 and batch: 750, loss is 3.557760968208313 and perplexity is 35.08455370423854
At time: 494.9827313423157 and batch: 800, loss is 3.6620929193496705 and perplexity is 38.94276169958261
At time: 495.47948026657104 and batch: 850, loss is 3.534567141532898 and perplexity is 34.2801730391345
At time: 495.9763045310974 and batch: 900, loss is 3.4670407724380494 and perplexity is 32.04178308323607
At time: 496.47256994247437 and batch: 950, loss is 3.500328722000122 and perplexity is 33.12633952568848
At time: 496.96935987472534 and batch: 1000, loss is 3.414172477722168 and perplexity is 30.39178913607685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298309326171875 and perplexity of 73.57529665884888
Finished 45 epochs...
Completing Train Step...
At time: 498.4076931476593 and batch: 50, loss is 3.7047390079498292 and perplexity is 40.639439362046154
At time: 498.9115149974823 and batch: 100, loss is 3.5746635961532593 and perplexity is 35.68261502741543
At time: 499.40445470809937 and batch: 150, loss is 3.687808995246887 and perplexity is 39.95720455478648
At time: 499.8963301181793 and batch: 200, loss is 3.7514143657684325 and perplexity is 42.58126491317381
At time: 500.38849353790283 and batch: 250, loss is 3.7211186265945435 and perplexity is 41.310579384491284
At time: 500.88187289237976 and batch: 300, loss is 3.5567819404602052 and perplexity is 35.050221761336736
At time: 501.3735649585724 and batch: 350, loss is 3.6008489894866944 and perplexity is 36.629319153381005
At time: 501.86358070373535 and batch: 400, loss is 3.6096718549728393 and perplexity is 36.95392457843662
At time: 502.3545892238617 and batch: 450, loss is 3.6609915256500245 and perplexity is 38.8998939986412
At time: 502.84633922576904 and batch: 500, loss is 3.6883849620819094 and perplexity is 39.980225208360324
At time: 503.3377809524536 and batch: 550, loss is 3.589599914550781 and perplexity is 36.21958209951036
At time: 503.83032631874084 and batch: 600, loss is 3.5339167642593385 and perplexity is 34.2578852421801
At time: 504.3254704475403 and batch: 650, loss is 3.499467439651489 and perplexity is 33.09782067733349
At time: 504.83386421203613 and batch: 700, loss is 3.596712226867676 and perplexity is 36.47810533861638
At time: 505.34302830696106 and batch: 750, loss is 3.5577502965927126 and perplexity is 35.08417929736566
At time: 505.8357901573181 and batch: 800, loss is 3.6620847749710084 and perplexity is 38.94244453627673
At time: 506.32867908477783 and batch: 850, loss is 3.5345634508132933 and perplexity is 34.28004652086129
At time: 506.82179403305054 and batch: 900, loss is 3.4670448112487793 and perplexity is 32.04191249419472
At time: 507.31474256515503 and batch: 950, loss is 3.5003353118896485 and perplexity is 33.126557825325655
At time: 507.80761313438416 and batch: 1000, loss is 3.4141855382919313 and perplexity is 30.3921860727512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298307837509528 and perplexity of 73.57518713015659
Finished 46 epochs...
Completing Train Step...
At time: 509.24382042884827 and batch: 50, loss is 3.704680986404419 and perplexity is 40.63708146737476
At time: 509.75621008872986 and batch: 100, loss is 3.57460503578186 and perplexity is 35.68052550140924
At time: 510.2614414691925 and batch: 150, loss is 3.6877495527267454 and perplexity is 39.95482946844118
At time: 510.75570797920227 and batch: 200, loss is 3.7513565492630003 and perplexity is 42.578803084407504
At time: 511.24846959114075 and batch: 250, loss is 3.721064944267273 and perplexity is 41.30836179597222
At time: 511.7527949810028 and batch: 300, loss is 3.556728358268738 and perplexity is 35.048343743957936
At time: 512.2468631267548 and batch: 350, loss is 3.6007970905303956 and perplexity is 36.627418179276724
At time: 512.738264799118 and batch: 400, loss is 3.6096213388442995 and perplexity is 36.95205785638275
At time: 513.2299153804779 and batch: 450, loss is 3.6609481954574585 and perplexity is 38.89820849526029
At time: 513.718341588974 and batch: 500, loss is 3.688347644805908 and perplexity is 39.97873328309909
At time: 514.2093658447266 and batch: 550, loss is 3.5895614862442016 and perplexity is 36.21819026904827
At time: 514.7004721164703 and batch: 600, loss is 3.533885111808777 and perplexity is 34.25680091332206
At time: 515.1885049343109 and batch: 650, loss is 3.4994428062438967 and perplexity is 33.097005375268196
At time: 515.6790227890015 and batch: 700, loss is 3.5966934728622437 and perplexity is 36.47742123444557
At time: 516.1864743232727 and batch: 750, loss is 3.5577393102645876 and perplexity is 35.08379385317721
At time: 516.6816444396973 and batch: 800, loss is 3.6620762300491334 and perplexity is 38.94211177755225
At time: 517.1715838909149 and batch: 850, loss is 3.5345594978332517 and perplexity is 34.2799110127894
At time: 517.6845498085022 and batch: 900, loss is 3.4670482778549196 and perplexity is 32.042023571077856
At time: 518.1779751777649 and batch: 950, loss is 3.5003411293029787 and perplexity is 33.12675053676527
At time: 518.6703915596008 and batch: 1000, loss is 3.414197745323181 and perplexity is 30.392557073380743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298305976681593 and perplexity of 73.57505021952048
Finished 47 epochs...
Completing Train Step...
At time: 520.1312003135681 and batch: 50, loss is 3.7046236038208007 and perplexity is 40.634749673552285
At time: 520.626571893692 and batch: 100, loss is 3.574547061920166 and perplexity is 35.67845702351804
At time: 521.1233048439026 and batch: 150, loss is 3.6876909017562864 and perplexity is 39.95248614763803
At time: 521.6190495491028 and batch: 200, loss is 3.7512992811203003 and perplexity is 42.57636474525672
At time: 522.1175272464752 and batch: 250, loss is 3.7210115242004393 and perplexity is 41.30615515946415
At time: 522.6155209541321 and batch: 300, loss is 3.5566749382019043 and perplexity is 35.04647150910063
At time: 523.1115338802338 and batch: 350, loss is 3.600745687484741 and perplexity is 36.625535466816835
At time: 523.6080958843231 and batch: 400, loss is 3.609571309089661 and perplexity is 36.950209200239094
At time: 524.1038296222687 and batch: 450, loss is 3.6609046983718874 and perplexity is 38.896516573353914
At time: 524.5993716716766 and batch: 500, loss is 3.688310251235962 and perplexity is 39.97723836349007
At time: 525.0955440998077 and batch: 550, loss is 3.589523448944092 and perplexity is 36.21681265307615
At time: 525.592794418335 and batch: 600, loss is 3.533853545188904 and perplexity is 34.255719558977
At time: 526.0882480144501 and batch: 650, loss is 3.499418115615845 and perplexity is 33.096188199507175
At time: 526.5851426124573 and batch: 700, loss is 3.5966745519638064 and perplexity is 36.47673105539256
At time: 527.0818195343018 and batch: 750, loss is 3.55772807598114 and perplexity is 35.08339971410659
At time: 527.57741355896 and batch: 800, loss is 3.662067379951477 and perplexity is 38.94176713758512
At time: 528.0721926689148 and batch: 850, loss is 3.534554986953735 and perplexity is 34.279756380589745
At time: 528.567907333374 and batch: 900, loss is 3.467051091194153 and perplexity is 32.04211371628668
At time: 529.064651966095 and batch: 950, loss is 3.500346345901489 and perplexity is 33.12692334617352
At time: 529.5607392787933 and batch: 1000, loss is 3.4142090272903443 and perplexity is 30.392899963145883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29830634884718 and perplexity of 73.57507760162729
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 530.9930863380432 and batch: 50, loss is 3.7046142578125 and perplexity is 40.63436990261921
At time: 531.4912226200104 and batch: 100, loss is 3.5745589923858643 and perplexity is 35.67888268666491
At time: 531.9797375202179 and batch: 150, loss is 3.6877095222473146 and perplexity is 39.953230089474154
At time: 532.4723417758942 and batch: 200, loss is 3.7513176012039184 and perplexity is 42.5771447549639
At time: 532.9667749404907 and batch: 250, loss is 3.7210163068771362 and perplexity is 41.30635271392229
At time: 533.4603147506714 and batch: 300, loss is 3.5565943050384523 and perplexity is 35.0436457151629
At time: 533.9562151432037 and batch: 350, loss is 3.6006923961639403 and perplexity is 36.62358369566347
At time: 534.4507207870483 and batch: 400, loss is 3.609520468711853 and perplexity is 36.94833068539589
At time: 534.9449005126953 and batch: 450, loss is 3.660835094451904 and perplexity is 38.89380931754562
At time: 535.4468185901642 and batch: 500, loss is 3.6882218170166015 and perplexity is 39.97370316394183
At time: 535.9438104629517 and batch: 550, loss is 3.589460310935974 and perplexity is 36.214526067850834
At time: 536.438333272934 and batch: 600, loss is 3.53376042842865 and perplexity is 34.25252992585795
At time: 536.9347543716431 and batch: 650, loss is 3.499275817871094 and perplexity is 33.09147902162718
At time: 537.4289014339447 and batch: 700, loss is 3.5965306997299193 and perplexity is 36.47148417354223
At time: 537.9236354827881 and batch: 750, loss is 3.557568211555481 and perplexity is 35.077791574844056
At time: 538.4288249015808 and batch: 800, loss is 3.661914196014404 and perplexity is 38.93580234124557
At time: 538.9291775226593 and batch: 850, loss is 3.5343934202194216 and perplexity is 34.274218359690295
At time: 539.4349315166473 and batch: 900, loss is 3.466857361793518 and perplexity is 32.0359068180501
At time: 539.9394166469574 and batch: 950, loss is 3.5001498651504517 and perplexity is 33.12041518278025
At time: 540.4341678619385 and batch: 1000, loss is 3.413988275527954 and perplexity is 30.386191417403758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29830634884718 and perplexity of 73.57507760162729
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 541.8770446777344 and batch: 50, loss is 3.704611482620239 and perplexity is 40.63425713458681
At time: 542.3757379055023 and batch: 100, loss is 3.5745590829849245 and perplexity is 35.6788859191383
At time: 542.8620364665985 and batch: 150, loss is 3.687711033821106 and perplexity is 39.953290481775284
At time: 543.3628981113434 and batch: 200, loss is 3.7513196992874147 and perplexity is 42.577234085462344
At time: 543.860773563385 and batch: 250, loss is 3.721016502380371 and perplexity is 41.306360789448654
At time: 544.3560621738434 and batch: 300, loss is 3.5565840816497802 and perplexity is 35.043287452183606
At time: 544.8413650989532 and batch: 350, loss is 3.6006854915618898 and perplexity is 36.62333082526537
At time: 545.3328397274017 and batch: 400, loss is 3.609513511657715 and perplexity is 36.94807363475316
At time: 545.8242335319519 and batch: 450, loss is 3.660825839042664 and perplexity is 38.89344934108934
At time: 546.3172588348389 and batch: 500, loss is 3.688210434913635 and perplexity is 39.9732481817258
At time: 546.8106164932251 and batch: 550, loss is 3.5894520902633666 and perplexity is 36.21422836131207
At time: 547.3037900924683 and batch: 600, loss is 3.5337485170364378 and perplexity is 34.25212193296964
At time: 547.7963211536407 and batch: 650, loss is 3.4992575788497926 and perplexity is 33.09087547094052
At time: 548.2888917922974 and batch: 700, loss is 3.596512017250061 and perplexity is 36.470802802138635
At time: 548.7834937572479 and batch: 750, loss is 3.557547483444214 and perplexity is 35.07706448601291
At time: 549.279883146286 and batch: 800, loss is 3.6618943548202516 and perplexity is 38.93502981609577
At time: 549.7728791236877 and batch: 850, loss is 3.534372549057007 and perplexity is 34.273503024377234
At time: 550.2658333778381 and batch: 900, loss is 3.4668324327468873 and perplexity is 32.03510820338957
At time: 550.7577991485596 and batch: 950, loss is 3.5001245498657227 and perplexity is 33.119576740652306
At time: 551.2500762939453 and batch: 1000, loss is 3.4139596700668333 and perplexity is 30.385322218818533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29830634884718 and perplexity of 73.57507760162729
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -73.57505021952048
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
SETTINGS FOR THIS RUN
{'lr': 5.313079103827589, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.6165012871400407, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 5.421382921095802, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6959009170532227 and batch: 50, loss is 6.779260807037353 and perplexity is 879.418423878584
At time: 1.2002792358398438 and batch: 100, loss is 6.065910511016845 and perplexity is 430.91485182557136
At time: 1.6897118091583252 and batch: 150, loss is 5.8213578224182125 and perplexity is 337.42991261405217
At time: 2.1792314052581787 and batch: 200, loss is 5.72393196105957 and perplexity is 306.10615729654813
At time: 2.6678500175476074 and batch: 250, loss is 5.749248323440551 and perplexity is 313.9545792705582
At time: 3.157543659210205 and batch: 300, loss is 5.621072292327881 and perplexity is 276.18537597077744
At time: 3.649545431137085 and batch: 350, loss is 5.5760276412963865 and perplexity is 264.0207348858477
At time: 4.142292022705078 and batch: 400, loss is 5.542876539230346 and perplexity is 255.41164538987658
At time: 4.63290548324585 and batch: 450, loss is 5.5612475872039795 and perplexity is 260.14719025668694
At time: 5.13493275642395 and batch: 500, loss is 5.55478982925415 and perplexity is 258.4726354250115
At time: 5.623523712158203 and batch: 550, loss is 5.471969938278198 and perplexity is 237.92843573789273
At time: 6.111895799636841 and batch: 600, loss is 5.373450546264649 and perplexity is 215.6055425499878
At time: 6.600253582000732 and batch: 650, loss is 5.340007495880127 and perplexity is 208.5142732812356
At time: 7.0907206535339355 and batch: 700, loss is 5.397613544464111 and perplexity is 220.87866960891435
At time: 7.583146810531616 and batch: 750, loss is 5.315333986282349 and perplexity is 203.43244543014626
At time: 8.074908256530762 and batch: 800, loss is 5.394404745101928 and perplexity is 220.17105018629007
At time: 8.56511378288269 and batch: 850, loss is 5.324115161895752 and perplexity is 205.2266877078549
At time: 9.054094791412354 and batch: 900, loss is 5.3586883640289305 and perplexity is 212.44611166268325
At time: 9.54274296760559 and batch: 950, loss is 5.309545793533325 and perplexity is 202.25834047550237
At time: 10.032017707824707 and batch: 1000, loss is 5.217708549499512 and perplexity is 184.51090169255818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.099406544755145 and perplexity of 163.92459651639692
Finished 1 epochs...
Completing Train Step...
At time: 11.467784881591797 and batch: 50, loss is 5.01750807762146 and perplexity is 151.03446829517296
At time: 11.956350803375244 and batch: 100, loss is 4.874221267700196 and perplexity is 130.87219910990186
At time: 12.442960023880005 and batch: 150, loss is 4.846405162811279 and perplexity is 127.28200833918535
At time: 12.927892923355103 and batch: 200, loss is 4.834910144805908 and perplexity is 125.82727646454697
At time: 13.414121389389038 and batch: 250, loss is 4.848922615051269 and perplexity is 127.60283838390659
At time: 13.902708292007446 and batch: 300, loss is 4.714338617324829 and perplexity is 111.53501944457682
At time: 14.405303239822388 and batch: 350, loss is 4.69201813697815 and perplexity is 109.07308225677053
At time: 14.902461528778076 and batch: 400, loss is 4.698039646148682 and perplexity is 109.73184821397255
At time: 15.392047643661499 and batch: 450, loss is 4.710645341873169 and perplexity is 111.12384964429928
At time: 15.87959361076355 and batch: 500, loss is 4.71727481842041 and perplexity is 111.86298994891571
At time: 16.36711359024048 and batch: 550, loss is 4.606671471595764 and perplexity is 100.15024131010695
At time: 16.85372519493103 and batch: 600, loss is 4.531410455703735 and perplexity is 92.88948523331332
At time: 17.361408472061157 and batch: 650, loss is 4.508538742065429 and perplexity is 90.78905530607223
At time: 17.883930921554565 and batch: 700, loss is 4.5594176197052 and perplexity is 95.52783010113048
At time: 18.385050058364868 and batch: 750, loss is 4.515030441284179 and perplexity is 91.3803477151821
At time: 18.873554944992065 and batch: 800, loss is 4.609880113601685 and perplexity is 100.47210367568955
At time: 19.3624529838562 and batch: 850, loss is 4.485140371322632 and perplexity is 88.68939938752416
At time: 19.84891366958618 and batch: 900, loss is 4.4701487255096435 and perplexity is 87.36971615277365
At time: 20.33439302444458 and batch: 950, loss is 4.472316675186157 and perplexity is 87.55933476818473
At time: 20.821704387664795 and batch: 1000, loss is 4.364664521217346 and perplexity is 78.62301916217132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.583165238543255 and perplexity of 97.82353952735824
Finished 2 epochs...
Completing Train Step...
At time: 22.24081778526306 and batch: 50, loss is 4.452537508010864 and perplexity is 85.84449896554771
At time: 22.74025797843933 and batch: 100, loss is 4.320891695022583 and perplexity is 75.25570351850432
At time: 23.22720718383789 and batch: 150, loss is 4.398760528564453 and perplexity is 81.34997518007047
At time: 23.710735082626343 and batch: 200, loss is 4.431268291473389 and perplexity is 84.03793394372994
At time: 24.194114446640015 and batch: 250, loss is 4.4308594131469725 and perplexity is 84.00357967777954
At time: 24.68304181098938 and batch: 300, loss is 4.294452776908875 and perplexity is 73.29209634173374
At time: 25.170225620269775 and batch: 350, loss is 4.3106437063217165 and perplexity is 74.48842217947741
At time: 25.658973932266235 and batch: 400, loss is 4.330454626083374 and perplexity is 75.9788206722021
At time: 26.14710521697998 and batch: 450, loss is 4.364578633308411 and perplexity is 78.61626668544352
At time: 26.634160041809082 and batch: 500, loss is 4.384749298095703 and perplexity is 80.21810985887761
At time: 27.11937427520752 and batch: 550, loss is 4.2784935474395756 and perplexity is 72.13169514156633
At time: 27.601848125457764 and batch: 600, loss is 4.224908809661866 and perplexity is 68.36826873874129
At time: 28.08679747581482 and batch: 650, loss is 4.197171492576599 and perplexity is 66.49797476767304
At time: 28.572566986083984 and batch: 700, loss is 4.269169092178345 and perplexity is 71.46223241547665
At time: 29.05780553817749 and batch: 750, loss is 4.226502265930176 and perplexity is 68.47729742827133
At time: 29.54446244239807 and batch: 800, loss is 4.345982666015625 and perplexity is 77.16783043809174
At time: 30.04680633544922 and batch: 850, loss is 4.222261185646057 and perplexity is 68.1874946849733
At time: 30.531232357025146 and batch: 900, loss is 4.178087973594666 and perplexity is 65.24099138425491
At time: 31.012354850769043 and batch: 950, loss is 4.210870780944824 and perplexity is 67.41521814577303
At time: 31.49420714378357 and batch: 1000, loss is 4.114555268287659 and perplexity is 61.224979517144625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4648802222275155 and perplexity of 86.91061895594277
Finished 3 epochs...
Completing Train Step...
At time: 32.90854024887085 and batch: 50, loss is 4.2269401407241824 and perplexity is 68.5072884764597
At time: 33.40828251838684 and batch: 100, loss is 4.09987165927887 and perplexity is 60.33254397825999
At time: 33.895923137664795 and batch: 150, loss is 4.19437171459198 and perplexity is 66.31205558955556
At time: 34.38058805465698 and batch: 200, loss is 4.242764725685119 and perplexity is 69.60001102079515
At time: 34.86520481109619 and batch: 250, loss is 4.23448459148407 and perplexity is 69.02609292757597
At time: 35.35183238983154 and batch: 300, loss is 4.088848114013672 and perplexity is 59.671117777531926
At time: 35.84035515785217 and batch: 350, loss is 4.117488198280334 and perplexity is 61.40481168453924
At time: 36.32763648033142 and batch: 400, loss is 4.139473919868469 and perplexity is 62.76979082177338
At time: 36.81815052032471 and batch: 450, loss is 4.179838981628418 and perplexity is 65.35532895809034
At time: 37.30897068977356 and batch: 500, loss is 4.207861227989197 and perplexity is 67.2126334743042
At time: 37.79737138748169 and batch: 550, loss is 4.1015569639205935 and perplexity is 60.43430842263239
At time: 38.28336453437805 and batch: 600, loss is 4.057184314727783 and perplexity is 57.811303262136825
At time: 38.770196199417114 and batch: 650, loss is 4.0289736032485965 and perplexity is 56.203194855618136
At time: 39.25884509086609 and batch: 700, loss is 4.107876968383789 and perplexity is 60.81746301561867
At time: 39.74612021446228 and batch: 750, loss is 4.065695066452026 and perplexity is 58.30542058345258
At time: 40.23500728607178 and batch: 800, loss is 4.193898429870606 and perplexity is 66.28067853252867
At time: 40.737406492233276 and batch: 850, loss is 4.069653806686401 and perplexity is 58.53669407159392
At time: 41.24403405189514 and batch: 900, loss is 4.012365660667419 and perplexity is 55.27748377844663
At time: 41.73222732543945 and batch: 950, loss is 4.05654052734375 and perplexity is 57.774097052171506
At time: 42.23792910575867 and batch: 1000, loss is 3.9653388261795044 and perplexity is 52.73813542367615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.419241556307164 and perplexity of 83.03328539879664
Finished 4 epochs...
Completing Train Step...
At time: 43.68797039985657 and batch: 50, loss is 4.0863850831985475 and perplexity is 59.524326825201626
At time: 44.17658472061157 and batch: 100, loss is 3.9602002763748168 and perplexity is 52.467832964291055
At time: 44.670130014419556 and batch: 150, loss is 4.064531383514404 and perplexity is 58.2376110224154
At time: 45.169397830963135 and batch: 200, loss is 4.116547613143921 and perplexity is 61.34708238528102
At time: 45.66542029380798 and batch: 250, loss is 4.105672245025635 and perplexity is 60.68352503667366
At time: 46.16577744483948 and batch: 300, loss is 3.955615367889404 and perplexity is 52.227823383148795
At time: 46.66755390167236 and batch: 350, loss is 3.9904236125946047 and perplexity is 54.07779254595667
At time: 47.16881990432739 and batch: 400, loss is 4.010440645217895 and perplexity is 55.171176122930476
At time: 47.67166042327881 and batch: 450, loss is 4.054274430274964 and perplexity is 57.643323568855685
At time: 48.17390251159668 and batch: 500, loss is 4.087009415626526 and perplexity is 59.561501396129735
At time: 48.67509937286377 and batch: 550, loss is 3.982975220680237 and perplexity is 53.67649631428989
At time: 49.17452836036682 and batch: 600, loss is 3.9417112970352175 and perplexity is 51.506669134106296
At time: 49.67604112625122 and batch: 650, loss is 3.915682077407837 and perplexity is 50.18328872780718
At time: 50.180349826812744 and batch: 700, loss is 3.9977436447143555 and perplexity is 54.47509608764777
At time: 50.684250831604004 and batch: 750, loss is 3.954574155807495 and perplexity is 52.17347144328902
At time: 51.18584752082825 and batch: 800, loss is 4.087250962257385 and perplexity is 59.575890013811964
At time: 51.688889503479004 and batch: 850, loss is 3.959507827758789 and perplexity is 52.43151426183777
At time: 52.190850496292114 and batch: 900, loss is 3.898720145225525 and perplexity is 49.33926158921916
At time: 52.691797733306885 and batch: 950, loss is 3.948577046394348 and perplexity is 51.86151777168907
At time: 53.19419860839844 and batch: 1000, loss is 3.862053713798523 and perplexity is 47.562931782735305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.406698831697789 and perplexity of 81.99832594063847
Finished 5 epochs...
Completing Train Step...
At time: 54.65980768203735 and batch: 50, loss is 3.9812627267837524 and perplexity is 53.58465430385308
At time: 55.169047355651855 and batch: 100, loss is 3.8623333024978637 and perplexity is 47.57623170013594
At time: 55.66262936592102 and batch: 150, loss is 3.9680247640609743 and perplexity is 52.87997718321262
At time: 56.154218435287476 and batch: 200, loss is 4.0224588680267335 and perplexity is 55.83823601839752
At time: 56.64637732505798 and batch: 250, loss is 4.007789134979248 and perplexity is 55.02508295392145
At time: 57.138654470443726 and batch: 300, loss is 3.858018298149109 and perplexity is 47.37138233425005
At time: 57.63039755821228 and batch: 350, loss is 3.8956746912002562 and perplexity is 49.189229709963286
At time: 58.12439465522766 and batch: 400, loss is 3.914103627204895 and perplexity is 50.104139388609156
At time: 58.61900973320007 and batch: 450, loss is 3.961042227745056 and perplexity is 52.51202693012318
At time: 59.11496543884277 and batch: 500, loss is 3.9985655784606933 and perplexity is 54.5198894135085
At time: 59.609618186950684 and batch: 550, loss is 3.8955637216567993 and perplexity is 49.18377150645215
At time: 60.10311484336853 and batch: 600, loss is 3.854413275718689 and perplexity is 47.20091489246957
At time: 60.595595359802246 and batch: 650, loss is 3.8307763624191282 and perplexity is 46.09831334325735
At time: 61.08843803405762 and batch: 700, loss is 3.914657053947449 and perplexity is 50.13187603365208
At time: 61.5807044506073 and batch: 750, loss is 3.870636477470398 and perplexity is 47.972910042313266
At time: 62.074589252471924 and batch: 800, loss is 4.003018822669983 and perplexity is 54.76322120113369
At time: 62.571006298065186 and batch: 850, loss is 3.874204411506653 and perplexity is 48.14437993560253
At time: 63.06715631484985 and batch: 900, loss is 3.8117415380477904 and perplexity is 45.22913858286314
At time: 63.56228065490723 and batch: 950, loss is 3.8649344396591188 and perplexity is 47.700145092402735
At time: 64.05680513381958 and batch: 1000, loss is 3.783645739555359 and perplexity is 43.97607515491629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.403669403820503 and perplexity of 81.7502938135547
Finished 6 epochs...
Completing Train Step...
At time: 65.49739861488342 and batch: 50, loss is 3.9028560638427736 and perplexity is 49.54374733650056
At time: 66.0021140575409 and batch: 100, loss is 3.78842312335968 and perplexity is 44.18666838594703
At time: 66.49646377563477 and batch: 150, loss is 3.893529939651489 and perplexity is 49.08384408674695
At time: 66.99231958389282 and batch: 200, loss is 3.9509140062332153 and perplexity is 51.982857784048406
At time: 67.48751997947693 and batch: 250, loss is 3.934814968109131 and perplexity is 51.15268420307011
At time: 67.99277472496033 and batch: 300, loss is 3.7859127807617186 and perplexity is 44.07588382181974
At time: 68.48575639724731 and batch: 350, loss is 3.824034767150879 and perplexity is 45.788582385609494
At time: 68.97787833213806 and batch: 400, loss is 3.8410926771163942 and perplexity is 46.57633954554666
At time: 69.47186803817749 and batch: 450, loss is 3.8859955310821532 and perplexity is 48.715416037855725
At time: 69.9675407409668 and batch: 500, loss is 3.9267562198638917 and perplexity is 50.742114161294126
At time: 70.46431159973145 and batch: 550, loss is 3.824971122741699 and perplexity is 45.83147685982834
At time: 70.96069598197937 and batch: 600, loss is 3.7839729022979736 and perplexity is 43.99046484202979
At time: 71.45600485801697 and batch: 650, loss is 3.76192994594574 and perplexity is 43.031394155652386
At time: 71.95082545280457 and batch: 700, loss is 3.847538695335388 and perplexity is 46.877541211605866
At time: 72.44618225097656 and batch: 750, loss is 3.804690818786621 and perplexity is 44.91136221672108
At time: 72.94234156608582 and batch: 800, loss is 3.9341548442840577 and perplexity is 51.11892824029465
At time: 73.43696713447571 and batch: 850, loss is 3.807664098739624 and perplexity is 45.045094983618704
At time: 73.93137431144714 and batch: 900, loss is 3.7410372352600096 and perplexity is 42.14167833831163
At time: 74.42770862579346 and batch: 950, loss is 3.797173662185669 and perplexity is 44.575022217810954
At time: 74.92352843284607 and batch: 1000, loss is 3.7175458431243897 and perplexity is 41.16324897586819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.401462834055831 and perplexity of 81.57010495972193
Finished 7 epochs...
Completing Train Step...
At time: 76.38368892669678 and batch: 50, loss is 3.8382701015472414 and perplexity is 46.445059668290114
At time: 76.87596106529236 and batch: 100, loss is 3.727874493598938 and perplexity is 41.59061303463486
At time: 77.37052822113037 and batch: 150, loss is 3.8302838230133056 and perplexity is 46.075613698087615
At time: 77.86639094352722 and batch: 200, loss is 3.8889296436309815 and perplexity is 48.85856245252378
At time: 78.3606538772583 and batch: 250, loss is 3.87490526676178 and perplexity is 48.178134004261764
At time: 78.85615754127502 and batch: 300, loss is 3.7238627576828005 and perplexity is 41.42409671163288
At time: 79.3520438671112 and batch: 350, loss is 3.7647027921676637 and perplexity is 43.150879174595886
At time: 79.88207244873047 and batch: 400, loss is 3.7797897815704347 and perplexity is 43.806831764177325
At time: 80.39297008514404 and batch: 450, loss is 3.8254860258102417 and perplexity is 45.85508170448126
At time: 80.88807797431946 and batch: 500, loss is 3.8695581769943237 and perplexity is 47.92120871037096
At time: 81.39502429962158 and batch: 550, loss is 3.7645395612716674 and perplexity is 43.14383619275694
At time: 81.89764761924744 and batch: 600, loss is 3.7252385091781615 and perplexity is 41.48112519414315
At time: 82.39312815666199 and batch: 650, loss is 3.702917456626892 and perplexity is 40.565479918424074
At time: 82.89164280891418 and batch: 700, loss is 3.793230791091919 and perplexity is 44.39961468309801
At time: 83.38772010803223 and batch: 750, loss is 3.747025103569031 and perplexity is 42.39477415451144
At time: 83.88339638710022 and batch: 800, loss is 3.8814647006988525 and perplexity is 48.49519402177116
At time: 84.37871718406677 and batch: 850, loss is 3.7521242332458495 and perplexity is 42.61150269942372
At time: 84.87639021873474 and batch: 900, loss is 3.6890980195999146 and perplexity is 40.00874357492617
At time: 85.37439203262329 and batch: 950, loss is 3.7425438070297243 and perplexity is 42.205215650967446
At time: 85.87360095977783 and batch: 1000, loss is 3.663971128463745 and perplexity is 39.01597308115393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4140513350323936 and perplexity of 82.60344073381162
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 87.32015466690063 and batch: 50, loss is 3.817734956741333 and perplexity is 45.50102971234304
At time: 87.82698392868042 and batch: 100, loss is 3.6932322072982786 and perplexity is 40.174489606973275
At time: 88.32258462905884 and batch: 150, loss is 3.785073699951172 and perplexity is 44.038916105116755
At time: 88.81731939315796 and batch: 200, loss is 3.8385881423950194 and perplexity is 46.45983344364961
At time: 89.31128215789795 and batch: 250, loss is 3.8067818689346313 and perplexity is 45.00537238306294
At time: 89.80553460121155 and batch: 300, loss is 3.641924934387207 and perplexity is 38.165231629307854
At time: 90.2995331287384 and batch: 350, loss is 3.67824773311615 and perplexity is 39.5769838395615
At time: 90.79358172416687 and batch: 400, loss is 3.6859739780426026 and perplexity is 39.88394962957323
At time: 91.28792715072632 and batch: 450, loss is 3.7216208219528197 and perplexity is 41.33133057584574
At time: 91.78439784049988 and batch: 500, loss is 3.75682981967926 and perplexity is 42.812487312809324
At time: 92.28051161766052 and batch: 550, loss is 3.639700884819031 and perplexity is 38.08044458262981
At time: 92.7769570350647 and batch: 600, loss is 3.58868483543396 and perplexity is 36.1864534762785
At time: 93.28637075424194 and batch: 650, loss is 3.552944359779358 and perplexity is 34.91597147036644
At time: 93.78219771385193 and batch: 700, loss is 3.6292021083831787 and perplexity is 37.68273787833872
At time: 94.27941942214966 and batch: 750, loss is 3.575279951095581 and perplexity is 35.7046149627305
At time: 94.77492070198059 and batch: 800, loss is 3.694123320579529 and perplexity is 40.21030558390579
At time: 95.26951599121094 and batch: 850, loss is 3.5459998083114623 and perplexity is 34.67433570654716
At time: 95.76449036598206 and batch: 900, loss is 3.4710081386566163 and perplexity is 32.169157073584245
At time: 96.26182889938354 and batch: 950, loss is 3.5048065900802614 and perplexity is 33.27500751346544
At time: 96.75712251663208 and batch: 1000, loss is 3.4046846628189087 and perplexity is 30.104801063881983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.337454074766578 and perplexity of 76.51249606644578
Finished 9 epochs...
Completing Train Step...
At time: 98.19370889663696 and batch: 50, loss is 3.7244833517074585 and perplexity is 41.449812237154376
At time: 98.70038270950317 and batch: 100, loss is 3.6031654596328737 and perplexity is 36.714268230688816
At time: 99.19099688529968 and batch: 150, loss is 3.7054698038101197 and perplexity is 40.66914935074259
At time: 99.68346214294434 and batch: 200, loss is 3.7661336040496827 and perplexity is 43.21266415604062
At time: 100.17667579650879 and batch: 250, loss is 3.7353646993637084 and perplexity is 41.903304885382795
At time: 100.66988945007324 and batch: 300, loss is 3.570846996307373 and perplexity is 35.54668831832671
At time: 101.16767859458923 and batch: 350, loss is 3.6126824522018435 and perplexity is 37.065345599124186
At time: 101.6621401309967 and batch: 400, loss is 3.624446301460266 and perplexity is 37.50395152632579
At time: 102.15497159957886 and batch: 450, loss is 3.6648965883255005 and perplexity is 39.05209751148471
At time: 102.6480131149292 and batch: 500, loss is 3.702629427909851 and perplexity is 40.55379757779257
At time: 103.13929390907288 and batch: 550, loss is 3.5879755926132204 and perplexity is 36.16079759314334
At time: 103.6338438987732 and batch: 600, loss is 3.5445687484741213 and perplexity is 34.62475014574565
At time: 104.12855219841003 and batch: 650, loss is 3.5120312356948853 and perplexity is 33.51627814855608
At time: 104.62260293960571 and batch: 700, loss is 3.5918003797531126 and perplexity is 36.29936978237855
At time: 105.1177008152008 and batch: 750, loss is 3.544569482803345 and perplexity is 34.62477557172088
At time: 105.62492847442627 and batch: 800, loss is 3.670200853347778 and perplexity is 39.25979052880774
At time: 106.11831474304199 and batch: 850, loss is 3.525027642250061 and perplexity is 33.95471218792041
At time: 106.61009120941162 and batch: 900, loss is 3.453769369125366 and perplexity is 31.619352977438474
At time: 107.11278676986694 and batch: 950, loss is 3.496648755073547 and perplexity is 33.00465971798666
At time: 107.61623287200928 and batch: 1000, loss is 3.4030834531784055 and perplexity is 30.056635538036083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.334901018840511 and perplexity of 76.31740453071461
Finished 10 epochs...
Completing Train Step...
At time: 109.08388233184814 and batch: 50, loss is 3.6871237468719484 and perplexity is 39.92983332441483
At time: 109.58959174156189 and batch: 100, loss is 3.5661563158035277 and perplexity is 35.38034060744294
At time: 110.09123945236206 and batch: 150, loss is 3.6698081350326537 and perplexity is 39.24437551709596
At time: 110.5830569267273 and batch: 200, loss is 3.732411141395569 and perplexity is 41.779723637375525
At time: 111.07631039619446 and batch: 250, loss is 3.7016194581985475 and perplexity is 40.51286014682315
At time: 111.57030439376831 and batch: 300, loss is 3.535909905433655 and perplexity is 34.32623413567125
At time: 112.06486511230469 and batch: 350, loss is 3.579664120674133 and perplexity is 35.861493689655404
At time: 112.5593945980072 and batch: 400, loss is 3.592336096763611 and perplexity is 36.318821181998956
At time: 113.06496071815491 and batch: 450, loss is 3.635345802307129 and perplexity is 37.91496171171696
At time: 113.55994915962219 and batch: 500, loss is 3.6738987302780153 and perplexity is 39.40523715861868
At time: 114.05573058128357 and batch: 550, loss is 3.5602699422836306 and perplexity is 35.172690460109635
At time: 114.55086088180542 and batch: 600, loss is 3.5199279403686523 and perplexity is 33.7819940581014
At time: 115.04582738876343 and batch: 650, loss is 3.48828932762146 and perplexity is 32.72990963614173
At time: 115.55440330505371 and batch: 700, loss is 3.5704234886169433 and perplexity is 35.53163720980965
At time: 116.05561637878418 and batch: 750, loss is 3.5254866886138916 and perplexity is 33.97030255316166
At time: 116.55014991760254 and batch: 800, loss is 3.6543044519424437 and perplexity is 38.64063534721497
At time: 117.04656982421875 and batch: 850, loss is 3.510388593673706 and perplexity is 33.461268094957894
At time: 117.53997707366943 and batch: 900, loss is 3.4402677536010744 and perplexity is 31.195309706692928
At time: 118.03145813941956 and batch: 950, loss is 3.4875715923309327 and perplexity is 32.70642665323097
At time: 118.53742933273315 and batch: 1000, loss is 3.3964841175079346 and perplexity is 29.858934775351987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.336691507479039 and perplexity of 76.45417238055137
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 119.97136998176575 and batch: 50, loss is 3.682350058555603 and perplexity is 39.73967498503407
At time: 120.47489356994629 and batch: 100, loss is 3.565060076713562 and perplexity is 35.34157654627436
At time: 120.96822476387024 and batch: 150, loss is 3.670569257736206 and perplexity is 39.2742566724591
At time: 121.48761367797852 and batch: 200, loss is 3.7317316007614134 and perplexity is 41.7513422617213
At time: 121.98324799537659 and batch: 250, loss is 3.6976460599899292 and perplexity is 40.35220580409464
At time: 122.4757866859436 and batch: 300, loss is 3.526922378540039 and perplexity is 34.019108400974204
At time: 122.96897268295288 and batch: 350, loss is 3.5683945703506468 and perplexity is 35.45961950576599
At time: 123.46177077293396 and batch: 400, loss is 3.581375827789307 and perplexity is 35.9229306295878
At time: 123.9555881023407 and batch: 450, loss is 3.6195360374450685 and perplexity is 37.32024860673638
At time: 124.45015597343445 and batch: 500, loss is 3.655651512145996 and perplexity is 38.69272168317014
At time: 124.94648051261902 and batch: 550, loss is 3.5395523262023927 and perplexity is 34.45149270752522
At time: 125.4437608718872 and batch: 600, loss is 3.4971182537078858 and perplexity is 33.02015899881195
At time: 125.9463038444519 and batch: 650, loss is 3.4591054630279543 and perplexity is 31.78852777895661
At time: 126.44794273376465 and batch: 700, loss is 3.5355646896362303 and perplexity is 34.31438622254257
At time: 126.95090770721436 and batch: 750, loss is 3.4889289951324463 and perplexity is 32.75085259352408
At time: 127.45373201370239 and batch: 800, loss is 3.61559401512146 and perplexity is 37.17342094270681
At time: 127.9578013420105 and batch: 850, loss is 3.4661139488220214 and perplexity is 32.012099759702345
At time: 128.45743656158447 and batch: 900, loss is 3.3898182582855223 and perplexity is 29.660561219245896
At time: 128.95615458488464 and batch: 950, loss is 3.436309366226196 and perplexity is 31.07207066145252
At time: 129.45680022239685 and batch: 1000, loss is 3.3388599157333374 and perplexity is 28.186972855564754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.322378391172828 and perplexity of 75.36766909197998
Finished 12 epochs...
Completing Train Step...
At time: 130.92528986930847 and batch: 50, loss is 3.6606640911102293 and perplexity is 38.887158914818606
At time: 131.4330976009369 and batch: 100, loss is 3.5395915269851685 and perplexity is 34.45284325947832
At time: 131.92540645599365 and batch: 150, loss is 3.6491430616378784 and perplexity is 38.44170975449819
At time: 132.41847944259644 and batch: 200, loss is 3.7126975536346434 and perplexity is 40.96416063742333
At time: 132.912513256073 and batch: 250, loss is 3.6789581775665283 and perplexity is 39.60511107833032
At time: 133.40776419639587 and batch: 300, loss is 3.508340849876404 and perplexity is 33.39281809871322
At time: 133.90222072601318 and batch: 350, loss is 3.5523914432525636 and perplexity is 34.896671188905394
At time: 134.39690232276917 and batch: 400, loss is 3.566956958770752 and perplexity is 35.40867897127033
At time: 134.88823986053467 and batch: 450, loss is 3.6065155935287474 and perplexity is 36.837472204825005
At time: 135.38080739974976 and batch: 500, loss is 3.6436377954483032 and perplexity is 38.23065938678595
At time: 135.87436652183533 and batch: 550, loss is 3.5280547952651977 and perplexity is 34.05765402904026
At time: 136.36883115768433 and batch: 600, loss is 3.487822594642639 and perplexity is 32.71463707230256
At time: 136.86266613006592 and batch: 650, loss is 3.4511852979660036 and perplexity is 31.53775179636819
At time: 137.3792278766632 and batch: 700, loss is 3.528940186500549 and perplexity is 34.08782173055271
At time: 137.87777709960938 and batch: 750, loss is 3.4840234184265135 and perplexity is 32.590584199791884
At time: 138.39092350006104 and batch: 800, loss is 3.612043890953064 and perplexity is 37.041684661035056
At time: 138.89040613174438 and batch: 850, loss is 3.4639931821823122 and perplexity is 31.944281505229316
At time: 139.38550782203674 and batch: 900, loss is 3.389472451210022 and perplexity is 29.65030616055109
At time: 139.8796739578247 and batch: 950, loss is 3.438030605316162 and perplexity is 31.125599178553646
At time: 140.37481904029846 and batch: 1000, loss is 3.3426359558105467 and perplexity is 28.293609199570934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.321590144459794 and perplexity of 75.30828418260381
Finished 13 epochs...
Completing Train Step...
At time: 141.8351218700409 and batch: 50, loss is 3.651870970726013 and perplexity is 38.54671840580967
At time: 142.32250094413757 and batch: 100, loss is 3.5303077363967894 and perplexity is 34.13447041755675
At time: 142.8115632534027 and batch: 150, loss is 3.6399527835845946 and perplexity is 38.090038207872745
At time: 143.30035519599915 and batch: 200, loss is 3.703385558128357 and perplexity is 40.584473125509085
At time: 143.8037235736847 and batch: 250, loss is 3.6698948764801025 and perplexity is 39.24777977867569
At time: 144.29522228240967 and batch: 300, loss is 3.49900032043457 and perplexity is 33.082363659673014
At time: 144.7885971069336 and batch: 350, loss is 3.5438983488082885 and perplexity is 34.60154550387956
At time: 145.28231167793274 and batch: 400, loss is 3.558924551010132 and perplexity is 35.12540124767909
At time: 145.78091144561768 and batch: 450, loss is 3.5989209413528442 and perplexity is 36.55876410156772
At time: 146.27971577644348 and batch: 500, loss is 3.636786255836487 and perplexity is 37.96961580602142
At time: 146.7795433998108 and batch: 550, loss is 3.521271433830261 and perplexity is 33.82741044771853
At time: 147.28009033203125 and batch: 600, loss is 3.482199959754944 and perplexity is 32.53121076537458
At time: 147.78007864952087 and batch: 650, loss is 3.44614520072937 and perplexity is 31.37919835869993
At time: 148.2816035747528 and batch: 700, loss is 3.524582014083862 and perplexity is 33.93958438273269
At time: 148.77849960327148 and batch: 750, loss is 3.480589737892151 and perplexity is 32.47887044964646
At time: 149.2774465084076 and batch: 800, loss is 3.6094183683395387 and perplexity is 36.94455843965363
At time: 149.77557730674744 and batch: 850, loss is 3.462042784690857 and perplexity is 31.88203817806116
At time: 150.2714557647705 and batch: 900, loss is 3.3883549451828 and perplexity is 29.61719027175668
At time: 150.76571679115295 and batch: 950, loss is 3.437840991020203 and perplexity is 31.11969787948211
At time: 151.26115322113037 and batch: 1000, loss is 3.343410415649414 and perplexity is 28.315529950867713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.321565953696647 and perplexity of 75.30646243977289
Finished 14 epochs...
Completing Train Step...
At time: 152.71258807182312 and batch: 50, loss is 3.6444580125808717 and perplexity is 38.262029692076595
At time: 153.22040224075317 and batch: 100, loss is 3.522851896286011 and perplexity is 33.88091567026672
At time: 153.71675992012024 and batch: 150, loss is 3.6327010679244993 and perplexity is 37.81481919241545
At time: 154.21307754516602 and batch: 200, loss is 3.6960852336883545 and perplexity is 40.289272146979165
At time: 154.70869636535645 and batch: 250, loss is 3.6628008270263672 and perplexity is 38.97033933960138
At time: 155.2048671245575 and batch: 300, loss is 3.4917442655563353 and perplexity is 32.84318500956419
At time: 155.70234441757202 and batch: 350, loss is 3.5371951770782473 and perplexity is 34.370381035373384
At time: 156.19858932495117 and batch: 400, loss is 3.552546730041504 and perplexity is 34.90209060168975
At time: 156.70697331428528 and batch: 450, loss is 3.5928507375717165 and perplexity is 36.33751713992031
At time: 157.2038173675537 and batch: 500, loss is 3.6312812423706053 and perplexity is 37.761166843311486
At time: 157.70074009895325 and batch: 550, loss is 3.5157757949829103 and perplexity is 33.64201711077366
At time: 158.19643354415894 and batch: 600, loss is 3.4775098371505737 and perplexity is 32.378992638287265
At time: 158.69376754760742 and batch: 650, loss is 3.4418332290649416 and perplexity is 31.24418344396346
At time: 159.18912649154663 and batch: 700, loss is 3.5207719898223875 and perplexity is 33.810519768593885
At time: 159.68520712852478 and batch: 750, loss is 3.4773959016799925 and perplexity is 32.37530373267666
At time: 160.18008756637573 and batch: 800, loss is 3.6068334865570066 and perplexity is 36.84918444193824
At time: 160.67377376556396 and batch: 850, loss is 3.459884972572327 and perplexity is 31.813316900184926
At time: 161.1726369857788 and batch: 900, loss is 3.38672691822052 and perplexity is 29.56901191591971
At time: 161.68279337882996 and batch: 950, loss is 3.4368717670440674 and perplexity is 31.089550534292137
At time: 162.1785011291504 and batch: 1000, loss is 3.3430527877807616 and perplexity is 28.305405338771255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.321841728396532 and perplexity of 75.32723292070655
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 163.62046599388123 and batch: 50, loss is 3.6434980821609497 and perplexity is 38.22531842879542
At time: 164.12568998336792 and batch: 100, loss is 3.524846119880676 and perplexity is 33.94854920748861
At time: 164.61516618728638 and batch: 150, loss is 3.6368008041381836 and perplexity is 37.97016820346569
At time: 165.1059181690216 and batch: 200, loss is 3.6995537757873533 and perplexity is 40.42925981978044
At time: 165.59338212013245 and batch: 250, loss is 3.6660350561141968 and perplexity is 39.096582384075056
At time: 166.0871229171753 and batch: 300, loss is 3.4912077808380126 and perplexity is 32.825569868255066
At time: 166.58213710784912 and batch: 350, loss is 3.5377558040618897 and perplexity is 34.38965540077972
At time: 167.0790615081787 and batch: 400, loss is 3.552738571166992 and perplexity is 34.908786900324856
At time: 167.57519459724426 and batch: 450, loss is 3.593035326004028 and perplexity is 36.34422524434346
At time: 168.07137370109558 and batch: 500, loss is 3.6325662565231323 and perplexity is 37.809721667257655
At time: 168.5672688484192 and batch: 550, loss is 3.513408522605896 and perplexity is 33.56247148319139
At time: 169.0750925540924 and batch: 600, loss is 3.4749833726882935 and perplexity is 32.29729151501278
At time: 169.57079315185547 and batch: 650, loss is 3.435185737609863 and perplexity is 31.03717680122978
At time: 170.06771874427795 and batch: 700, loss is 3.5139802360534667 and perplexity is 33.58166508556953
At time: 170.56481504440308 and batch: 750, loss is 3.4700938987731935 and perplexity is 32.139760187122086
At time: 171.06175565719604 and batch: 800, loss is 3.5947894477844238 and perplexity is 36.40803338870289
At time: 171.55970907211304 and batch: 850, loss is 3.4490380859375 and perplexity is 31.470106207060695
At time: 172.05745005607605 and batch: 900, loss is 3.372986760139465 and perplexity is 29.16550747348614
At time: 172.56847381591797 and batch: 950, loss is 3.421779918670654 and perplexity is 30.62387454598195
At time: 173.0720899105072 and batch: 1000, loss is 3.324703869819641 and perplexity is 27.790767738957463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.316891926090892 and perplexity of 74.95529926792528
Finished 16 epochs...
Completing Train Step...
At time: 174.52170515060425 and batch: 50, loss is 3.638950157165527 and perplexity is 38.051867268056235
At time: 175.01582169532776 and batch: 100, loss is 3.5186501502990724 and perplexity is 33.73885532855302
At time: 175.51132607460022 and batch: 150, loss is 3.6309988117218017 and perplexity is 37.75050343836742
At time: 176.00501203536987 and batch: 200, loss is 3.6939963054656983 and perplexity is 40.20519859170435
At time: 176.4995367527008 and batch: 250, loss is 3.6602605962753296 and perplexity is 38.87147131219871
At time: 176.99191308021545 and batch: 300, loss is 3.486138734817505 and perplexity is 32.65959656249935
At time: 177.4853799343109 and batch: 350, loss is 3.5325514698028564 and perplexity is 34.21114505568733
At time: 177.97945547103882 and batch: 400, loss is 3.5484507036209108 and perplexity is 34.75942310084383
At time: 178.4745054244995 and batch: 450, loss is 3.5894475841522215 and perplexity is 36.214065176341705
At time: 178.96820521354675 and batch: 500, loss is 3.6285722064971924 and perplexity is 37.65900892492087
At time: 179.4616768360138 and batch: 550, loss is 3.5096315813064574 and perplexity is 33.43594708655367
At time: 179.95607209205627 and batch: 600, loss is 3.471929268836975 and perplexity is 32.198802706688966
At time: 180.448739528656 and batch: 650, loss is 3.433006000518799 and perplexity is 30.96959759496263
At time: 180.94272637367249 and batch: 700, loss is 3.5121494817733763 and perplexity is 33.52024155133657
At time: 181.45297598838806 and batch: 750, loss is 3.46894410610199 and perplexity is 32.10282736301877
At time: 181.94813537597656 and batch: 800, loss is 3.5945118999481203 and perplexity is 36.39792981998847
At time: 182.4440574645996 and batch: 850, loss is 3.4493148708343506 and perplexity is 31.478817862732836
At time: 182.94166588783264 and batch: 900, loss is 3.374013385772705 and perplexity is 29.195464905967754
At time: 183.43795585632324 and batch: 950, loss is 3.42351758480072 and perplexity is 30.677134876486534
At time: 183.93482899665833 and batch: 1000, loss is 3.3271998834609984 and perplexity is 27.86022051581888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.316514922351372 and perplexity of 74.92704616590163
Finished 17 epochs...
Completing Train Step...
At time: 185.37941479682922 and batch: 50, loss is 3.6370745134353637 and perplexity is 37.9805624139496
At time: 185.8805055618286 and batch: 100, loss is 3.5163922691345215 and perplexity is 33.66276293870683
At time: 186.36881566047668 and batch: 150, loss is 3.6285387563705442 and perplexity is 37.65774924737119
At time: 186.85621166229248 and batch: 200, loss is 3.691561288833618 and perplexity is 40.10741736221208
At time: 187.347026348114 and batch: 250, loss is 3.657696256637573 and perplexity is 38.771919354603476
At time: 187.83973479270935 and batch: 300, loss is 3.4836770057678224 and perplexity is 32.5792963641084
At time: 188.33511638641357 and batch: 350, loss is 3.530174231529236 and perplexity is 34.129913603789824
At time: 188.8283290863037 and batch: 400, loss is 3.5463575172424315 and perplexity is 34.686741244758444
At time: 189.32536625862122 and batch: 450, loss is 3.5875783252716062 and perplexity is 36.14643494230782
At time: 189.8184278011322 and batch: 500, loss is 3.6266719007492068 and perplexity is 37.58751324713633
At time: 190.3109254837036 and batch: 550, loss is 3.5078151178359986 and perplexity is 33.375267038300954
At time: 190.80368542671204 and batch: 600, loss is 3.470354423522949 and perplexity is 32.14813448091007
At time: 191.2963044643402 and batch: 650, loss is 3.431910638809204 and perplexity is 30.935693255742944
At time: 191.78918552398682 and batch: 700, loss is 3.511248288154602 and perplexity is 33.490046931193774
At time: 192.28349900245667 and batch: 750, loss is 3.4684220933914185 and perplexity is 32.08607365229541
At time: 192.7783751487732 and batch: 800, loss is 3.5943244647979737 and perplexity is 36.39110820787255
At time: 193.2737579345703 and batch: 850, loss is 3.4494354057312013 and perplexity is 31.482612387478632
At time: 193.76811170578003 and batch: 900, loss is 3.3744729804992675 and perplexity is 29.208886071590303
At time: 194.27425599098206 and batch: 950, loss is 3.4242986011505128 and perplexity is 30.70110357914581
At time: 194.7686378955841 and batch: 1000, loss is 3.328266968727112 and perplexity is 27.88996561409755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.316409227324695 and perplexity of 74.91912716826502
Finished 18 epochs...
Completing Train Step...
At time: 196.20936799049377 and batch: 50, loss is 3.6354556846618653 and perplexity is 37.919128125893096
At time: 196.71929907798767 and batch: 100, loss is 3.5146286487579346 and perplexity is 33.603446924895714
At time: 197.21708607673645 and batch: 150, loss is 3.626678032875061 and perplexity is 37.58774373920481
At time: 197.71361994743347 and batch: 200, loss is 3.6897227287292482 and perplexity is 40.03374521085261
At time: 198.2145218849182 and batch: 250, loss is 3.655800948143005 and perplexity is 38.69850420065924
At time: 198.7195177078247 and batch: 300, loss is 3.4818342828750612 and perplexity is 32.51931702848792
At time: 199.2174937725067 and batch: 350, loss is 3.5284543657302856 and perplexity is 34.07126518082647
At time: 199.71338772773743 and batch: 400, loss is 3.5447826480865476 and perplexity is 34.63215715853253
At time: 200.2105529308319 and batch: 450, loss is 3.586128497123718 and perplexity is 36.09406679506279
At time: 200.7083282470703 and batch: 500, loss is 3.6252798509597777 and perplexity is 37.535225958933296
At time: 201.2068076133728 and batch: 550, loss is 3.506476526260376 and perplexity is 33.33062107503097
At time: 201.7063136100769 and batch: 600, loss is 3.469190282821655 and perplexity is 32.11073130459427
At time: 202.206374168396 and batch: 650, loss is 3.431039447784424 and perplexity is 30.90875409371512
At time: 202.70421361923218 and batch: 700, loss is 3.5105312633514405 and perplexity is 33.46604234385577
At time: 203.20274567604065 and batch: 750, loss is 3.4679568815231323 and perplexity is 32.071150301554745
At time: 203.7034468650818 and batch: 800, loss is 3.5940406274795533 and perplexity is 36.38078051906557
At time: 204.20095372200012 and batch: 850, loss is 3.449371590614319 and perplexity is 31.48060338499241
At time: 204.6983721256256 and batch: 900, loss is 3.3746185445785524 and perplexity is 29.21313814566537
At time: 205.19634175300598 and batch: 950, loss is 3.4246307706832884 and perplexity is 30.71130324429261
At time: 205.69451379776 and batch: 1000, loss is 3.3287692785263063 and perplexity is 27.90397853624412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.316385408727134 and perplexity of 74.91734272097698
Finished 19 epochs...
Completing Train Step...
At time: 207.143901348114 and batch: 50, loss is 3.633955078125 and perplexity is 37.86226910653291
At time: 207.63812804222107 and batch: 100, loss is 3.513060727119446 and perplexity is 33.55080063674692
At time: 208.1316785812378 and batch: 150, loss is 3.62507625579834 and perplexity is 37.52758474642792
At time: 208.62333273887634 and batch: 200, loss is 3.6881421279907225 and perplexity is 39.970517825395866
At time: 209.11553716659546 and batch: 250, loss is 3.654196219444275 and perplexity is 38.63645340103584
At time: 209.60960578918457 and batch: 300, loss is 3.480254440307617 and perplexity is 32.467982188342155
At time: 210.10443687438965 and batch: 350, loss is 3.526988663673401 and perplexity is 34.021363436848254
At time: 210.59843587875366 and batch: 400, loss is 3.5434266996383665 and perplexity is 34.5852295616674
At time: 211.09229922294617 and batch: 450, loss is 3.584863977432251 and perplexity is 36.048453982083345
At time: 211.5859682559967 and batch: 500, loss is 3.6240916442871094 and perplexity is 37.49065283927204
At time: 212.07973885536194 and batch: 550, loss is 3.50532030582428 and perplexity is 33.29210580016095
At time: 212.57521224021912 and batch: 600, loss is 3.468197965621948 and perplexity is 32.07888307801045
At time: 213.0684003829956 and batch: 650, loss is 3.430244278907776 and perplexity is 30.88418618355711
At time: 213.5623857975006 and batch: 700, loss is 3.509863996505737 and perplexity is 33.44371901198071
At time: 214.0568437576294 and batch: 750, loss is 3.467477068901062 and perplexity is 32.05576584995792
At time: 214.55218029022217 and batch: 800, loss is 3.593689594268799 and perplexity is 36.36801189810633
At time: 215.04706406593323 and batch: 850, loss is 3.4491901874542235 and perplexity is 31.474893221993124
At time: 215.54278087615967 and batch: 900, loss is 3.37458655834198 and perplexity is 29.212203742261725
At time: 216.03808856010437 and batch: 950, loss is 3.4247379159927367 and perplexity is 30.714593992673258
At time: 216.53435015678406 and batch: 1000, loss is 3.3290020036697388 and perplexity is 27.910473249363513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.316406622165587 and perplexity of 74.91893199227275
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 217.98030710220337 and batch: 50, loss is 3.6336348152160642 and perplexity is 37.85014516761754
At time: 218.487078666687 and batch: 100, loss is 3.5133187198638915 and perplexity is 33.55945761655252
At time: 218.98186659812927 and batch: 150, loss is 3.625899329185486 and perplexity is 37.55848541773221
At time: 219.48856592178345 and batch: 200, loss is 3.6890442419052123 and perplexity is 40.006592054781194
At time: 219.98076057434082 and batch: 250, loss is 3.655071988105774 and perplexity is 38.67030481695696
At time: 220.47214722633362 and batch: 300, loss is 3.479619517326355 and perplexity is 32.44737406327399
At time: 220.96410727500916 and batch: 350, loss is 3.5263948392868043 and perplexity is 34.00116671881953
At time: 221.46743321418762 and batch: 400, loss is 3.542938332557678 and perplexity is 34.56834339773034
At time: 221.97376084327698 and batch: 450, loss is 3.5850951147079466 and perplexity is 36.05678708653823
At time: 222.47104048728943 and batch: 500, loss is 3.624400005340576 and perplexity is 37.502215279088155
At time: 222.96868252754211 and batch: 550, loss is 3.5041931581497194 and perplexity is 33.25460182075928
At time: 223.4662172794342 and batch: 600, loss is 3.466724739074707 and perplexity is 32.031658410712204
At time: 223.96153259277344 and batch: 650, loss is 3.428101305961609 and perplexity is 30.81807307270644
At time: 224.45597648620605 and batch: 700, loss is 3.507394642829895 and perplexity is 33.36123652263662
At time: 224.95036029815674 and batch: 750, loss is 3.464304418563843 and perplexity is 31.95422527516682
At time: 225.44455218315125 and batch: 800, loss is 3.589904932975769 and perplexity is 36.23063142443424
At time: 225.9385917186737 and batch: 850, loss is 3.445360765457153 and perplexity is 31.354593060587057
At time: 226.43023824691772 and batch: 900, loss is 3.369801287651062 and perplexity is 29.07274936944978
At time: 226.92168521881104 and batch: 950, loss is 3.4199637031555175 and perplexity is 30.56830546788403
At time: 227.41399121284485 and batch: 1000, loss is 3.323948316574097 and perplexity is 27.769778264527396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.315685365258194 and perplexity of 74.86491567724059
Finished 21 epochs...
Completing Train Step...
At time: 228.84646201133728 and batch: 50, loss is 3.6330424308776856 and perplexity is 37.827729974275115
At time: 229.3504617214203 and batch: 100, loss is 3.512596535682678 and perplexity is 33.53523025649282
At time: 229.84324407577515 and batch: 150, loss is 3.6250970935821534 and perplexity is 37.528366746273456
At time: 230.33519196510315 and batch: 200, loss is 3.6881100034713743 and perplexity is 39.969233812346886
At time: 230.82889866828918 and batch: 250, loss is 3.654300742149353 and perplexity is 38.64049199871884
At time: 231.32171487808228 and batch: 300, loss is 3.4788916158676146 and perplexity is 32.423764166242734
At time: 231.81507205963135 and batch: 350, loss is 3.5257158851623536 and perplexity is 33.97808932157319
At time: 232.3209023475647 and batch: 400, loss is 3.5422040033340454 and perplexity is 34.54296817098637
At time: 232.8144268989563 and batch: 450, loss is 3.5844392824172973 and perplexity is 36.03314763387531
At time: 233.30760645866394 and batch: 500, loss is 3.6237386226654054 and perplexity is 37.47742016405564
At time: 233.80125212669373 and batch: 550, loss is 3.5037618923187255 and perplexity is 33.24026333934146
At time: 234.2933166027069 and batch: 600, loss is 3.4663734149932863 and perplexity is 32.02040689432474
At time: 234.7846565246582 and batch: 650, loss is 3.4277128171920777 and perplexity is 30.806102922704074
At time: 235.27606415748596 and batch: 700, loss is 3.5072169589996336 and perplexity is 33.355309296950686
At time: 235.7671399116516 and batch: 750, loss is 3.464344930648804 and perplexity is 31.955519833678483
At time: 236.25932002067566 and batch: 800, loss is 3.589970030784607 and perplexity is 36.232990035922185
At time: 236.75340843200684 and batch: 850, loss is 3.4455480432510375 and perplexity is 31.36046562948729
At time: 237.2484700679779 and batch: 900, loss is 3.3701314687728883 and perplexity is 29.08235022737496
At time: 237.74278569221497 and batch: 950, loss is 3.420291905403137 and perplexity is 30.57833970098337
At time: 238.23456978797913 and batch: 1000, loss is 3.324475631713867 and perplexity is 27.78442555056278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.315339995593559 and perplexity of 74.83906407085755
Finished 22 epochs...
Completing Train Step...
At time: 239.68684148788452 and batch: 50, loss is 3.6325631284713746 and perplexity is 37.809603396676316
At time: 240.1770157814026 and batch: 100, loss is 3.5120309734344484 and perplexity is 33.516269358563484
At time: 240.66735816001892 and batch: 150, loss is 3.6244688653945922 and perplexity is 37.504797772572296
At time: 241.1560823917389 and batch: 200, loss is 3.6874521589279174 and perplexity is 39.94294891661282
At time: 241.64839386940002 and batch: 250, loss is 3.6537310171127317 and perplexity is 38.61848381290233
At time: 242.14151573181152 and batch: 300, loss is 3.478350796699524 and perplexity is 32.40623351396366
At time: 242.63473892211914 and batch: 350, loss is 3.5252065515518187 and perplexity is 33.960787545222104
At time: 243.13061547279358 and batch: 400, loss is 3.5416860961914063 and perplexity is 34.52508275293532
At time: 243.62771701812744 and batch: 450, loss is 3.5839525651931763 and perplexity is 36.015613947602304
At time: 244.12572813034058 and batch: 500, loss is 3.6232604598999023 and perplexity is 37.459504140915016
At time: 244.63444066047668 and batch: 550, loss is 3.503427653312683 and perplexity is 33.22915500328534
At time: 245.13003420829773 and batch: 600, loss is 3.466103754043579 and perplexity is 32.01177340510138
At time: 245.62707829475403 and batch: 650, loss is 3.4274744033813476 and perplexity is 30.798759197770035
At time: 246.12282371520996 and batch: 700, loss is 3.507104721069336 and perplexity is 33.351565776157194
At time: 246.61648154258728 and batch: 750, loss is 3.464381036758423 and perplexity is 31.95667364401021
At time: 247.1104302406311 and batch: 800, loss is 3.590033082962036 and perplexity is 36.235274676863746
At time: 247.6062867641449 and batch: 850, loss is 3.4457183694839477 and perplexity is 31.36580759438574
At time: 248.10173225402832 and batch: 900, loss is 3.3703802919387815 and perplexity is 29.089587490192137
At time: 248.59861040115356 and batch: 950, loss is 3.4205399799346923 and perplexity is 30.58592634926863
At time: 249.09427881240845 and batch: 1000, loss is 3.324852828979492 and perplexity is 27.79490773670911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.31515800662157 and perplexity of 74.82544542578187
Finished 23 epochs...
Completing Train Step...
At time: 250.52697730064392 and batch: 50, loss is 3.63214777469635 and perplexity is 37.793902296154734
At time: 251.02631497383118 and batch: 100, loss is 3.5115591955184935 and perplexity is 33.50046085220023
At time: 251.51586389541626 and batch: 150, loss is 3.623950424194336 and perplexity is 37.48535877962254
At time: 252.0066783428192 and batch: 200, loss is 3.6869274091720583 and perplexity is 39.92199436234999
At time: 252.49643898010254 and batch: 250, loss is 3.6532647085189818 and perplexity is 38.60047988004333
At time: 252.98578810691833 and batch: 300, loss is 3.4778999710083007 and perplexity is 32.391627244028044
At time: 253.47462034225464 and batch: 350, loss is 3.524782032966614 and perplexity is 33.94637361944712
At time: 253.96636033058167 and batch: 400, loss is 3.541273527145386 and perplexity is 34.51084171038883
At time: 254.46047043800354 and batch: 450, loss is 3.583560280799866 and perplexity is 36.00148835514104
At time: 254.95259857177734 and batch: 500, loss is 3.622878460884094 and perplexity is 37.4451973799588
At time: 255.4697709083557 and batch: 550, loss is 3.5031395864486696 and perplexity is 33.21958416339683
At time: 255.97041487693787 and batch: 600, loss is 3.4658710813522338 and perplexity is 32.00432600606526
At time: 256.46377968788147 and batch: 650, loss is 3.427296938896179 and perplexity is 30.793293996779063
At time: 256.9582402706146 and batch: 700, loss is 3.507008967399597 and perplexity is 33.348372394234055
At time: 257.47488617897034 and batch: 750, loss is 3.464396119117737 and perplexity is 31.957155629679313
At time: 257.9688551425934 and batch: 800, loss is 3.5900764894485473 and perplexity is 36.2368475569616
At time: 258.4618487358093 and batch: 850, loss is 3.4458493423461913 and perplexity is 31.36991593301752
At time: 258.95431542396545 and batch: 900, loss is 3.3705614233016967 and perplexity is 29.094857004043583
At time: 259.4470901489258 and batch: 950, loss is 3.4207284545898435 and perplexity is 30.591691564471198
At time: 259.9393346309662 and batch: 1000, loss is 3.3251270866394043 and perplexity is 27.80253174848648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.315058266244283 and perplexity of 74.81798267979916
Finished 24 epochs...
Completing Train Step...
At time: 261.3711926937103 and batch: 50, loss is 3.6317733001708983 and perplexity is 37.77975209213824
At time: 261.86775040626526 and batch: 100, loss is 3.5111457824707033 and perplexity is 33.48661418697019
At time: 262.3533935546875 and batch: 150, loss is 3.623501935005188 and perplexity is 37.46855077084539
At time: 262.86201548576355 and batch: 200, loss is 3.686480851173401 and perplexity is 39.904170856355904
At time: 263.3564603328705 and batch: 250, loss is 3.652857894897461 and perplexity is 38.58477987273583
At time: 263.8486530780792 and batch: 300, loss is 3.4775035095214846 and perplexity is 32.37878775667978
At time: 264.34040451049805 and batch: 350, loss is 3.5244099140167235 and perplexity is 33.93374388057159
At time: 264.8332631587982 and batch: 400, loss is 3.540921459197998 and perplexity is 34.4986936877754
At time: 265.3256618976593 and batch: 450, loss is 3.5832251644134523 and perplexity is 35.9894256877696
At time: 265.8173096179962 and batch: 500, loss is 3.6225549745559693 and perplexity is 37.43308632953767
At time: 266.3104684352875 and batch: 550, loss is 3.5028795862197875 and perplexity is 33.210948186637594
At time: 266.80412578582764 and batch: 600, loss is 3.4656590938568117 and perplexity is 31.997542208218128
At time: 267.29795718193054 and batch: 650, loss is 3.4271459865570066 and perplexity is 30.78864602783914
At time: 267.7908320426941 and batch: 700, loss is 3.5069150733947754 and perplexity is 33.3452413289921
At time: 268.284250497818 and batch: 750, loss is 3.464390606880188 and perplexity is 31.95697947473161
At time: 268.77822160720825 and batch: 800, loss is 3.59009904384613 and perplexity is 36.23766486644548
At time: 269.270569562912 and batch: 850, loss is 3.4459427118301393 and perplexity is 31.37284506262321
At time: 269.77702927589417 and batch: 900, loss is 3.370691623687744 and perplexity is 29.098645412278252
At time: 270.2714409828186 and batch: 950, loss is 3.4208727264404297 and perplexity is 30.596105402814423
At time: 270.76491379737854 and batch: 1000, loss is 3.3253309202194212 and perplexity is 27.808199415677134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.315001324909489 and perplexity of 74.81372256528823
Finished 25 epochs...
Completing Train Step...
At time: 272.22133350372314 and batch: 50, loss is 3.6314270639419557 and perplexity is 37.766673637491536
At time: 272.71134209632874 and batch: 100, loss is 3.5107710123062135 and perplexity is 33.47406675441168
At time: 273.20288372039795 and batch: 150, loss is 3.6231008672714236 and perplexity is 37.453526357205455
At time: 273.6950452327728 and batch: 200, loss is 3.6860847997665407 and perplexity is 39.888369882554144
At time: 274.18701910972595 and batch: 250, loss is 3.652489309310913 and perplexity is 38.570560699666714
At time: 274.68150186538696 and batch: 300, loss is 3.477143197059631 and perplexity is 32.36712337748813
At time: 275.1778974533081 and batch: 350, loss is 3.524072976112366 and perplexity is 33.92231224200578
At time: 275.67274355888367 and batch: 400, loss is 3.540607237815857 and perplexity is 34.48785516349999
At time: 276.1725206375122 and batch: 450, loss is 3.582926478385925 and perplexity is 35.9786777543866
At time: 276.6728639602661 and batch: 500, loss is 3.622269320487976 and perplexity is 37.42239494324179
At time: 277.18364810943604 and batch: 550, loss is 3.5026381397247315 and perplexity is 33.202930487562064
At time: 277.6853275299072 and batch: 600, loss is 3.4654605102539064 and perplexity is 31.99118865187924
At time: 278.1837224960327 and batch: 650, loss is 3.4270072174072266 and perplexity is 30.78437381003985
At time: 278.6834247112274 and batch: 700, loss is 3.5068191862106324 and perplexity is 33.34204410098554
At time: 279.180775642395 and batch: 750, loss is 3.4643681621551514 and perplexity is 31.956262217163648
At time: 279.6804611682892 and batch: 800, loss is 3.590103478431702 and perplexity is 36.237825565827556
At time: 280.17995834350586 and batch: 850, loss is 3.4460053062438964 and perplexity is 31.374808888929433
At time: 280.68632078170776 and batch: 900, loss is 3.370784296989441 and perplexity is 29.101342204781922
At time: 281.1879208087921 and batch: 950, loss is 3.4209836816787718 and perplexity is 30.599500389324014
At time: 281.6879417896271 and batch: 1000, loss is 3.325485725402832 and perplexity is 27.812504602312007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314969690834603 and perplexity of 74.81135593981931
Finished 26 epochs...
Completing Train Step...
At time: 283.1508221626282 and batch: 50, loss is 3.6311016654968262 and perplexity is 37.75438641984136
At time: 283.6574261188507 and batch: 100, loss is 3.510423846244812 and perplexity is 33.46244771148284
At time: 284.15155482292175 and batch: 150, loss is 3.6227337551116943 and perplexity is 37.43977923577709
At time: 284.64472556114197 and batch: 200, loss is 3.68572368144989 and perplexity is 39.873968062105305
At time: 285.13934326171875 and batch: 250, loss is 3.6521472692489625 and perplexity is 38.55737027865034
At time: 285.6340184211731 and batch: 300, loss is 3.4768084955215453 and perplexity is 32.35629186427345
At time: 286.12922072410583 and batch: 350, loss is 3.5237611818313597 and perplexity is 33.91173710776908
At time: 286.62197256088257 and batch: 400, loss is 3.5403185844421388 and perplexity is 34.47790156439383
At time: 287.1151399612427 and batch: 450, loss is 3.5826526212692262 and perplexity is 35.9688260864704
At time: 287.6256287097931 and batch: 500, loss is 3.622009234428406 and perplexity is 37.412663165606034
At time: 288.12620735168457 and batch: 550, loss is 3.5024102878570558 and perplexity is 33.19536599966224
At time: 288.62332820892334 and batch: 600, loss is 3.4652710866928103 and perplexity is 31.98512934090735
At time: 289.11971974372864 and batch: 650, loss is 3.4268736982345582 and perplexity is 30.78026378030761
At time: 289.61441373825073 and batch: 700, loss is 3.506719932556152 and perplexity is 33.33873494548628
At time: 290.1104996204376 and batch: 750, loss is 3.4643319129943846 and perplexity is 31.955103850472067
At time: 290.6065227985382 and batch: 800, loss is 3.5900929975509643 and perplexity is 36.23744576348995
At time: 291.1021156311035 and batch: 850, loss is 3.4460437393188474 and perplexity is 31.376014742483218
At time: 291.59405303001404 and batch: 900, loss is 3.370848927497864 and perplexity is 29.10322310010536
At time: 292.08947372436523 and batch: 950, loss is 3.4210692930221556 and perplexity is 30.602120165798908
At time: 292.5853669643402 and batch: 1000, loss is 3.3256055736541748 and perplexity is 27.815838082105927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3149525712176064 and perplexity of 74.81007520902145
Finished 27 epochs...
Completing Train Step...
At time: 294.0225520133972 and batch: 50, loss is 3.6307918500900267 and perplexity is 37.74269134100558
At time: 294.5272624492645 and batch: 100, loss is 3.5100969648361207 and perplexity is 33.45151124699749
At time: 295.01890802383423 and batch: 150, loss is 3.6223918342590333 and perplexity is 37.426979982827795
At time: 295.5233156681061 and batch: 200, loss is 3.6853878545761107 and perplexity is 39.86057956030101
At time: 296.0161142349243 and batch: 250, loss is 3.651825022697449 and perplexity is 38.544947300781004
At time: 296.5083005428314 and batch: 300, loss is 3.476492519378662 and perplexity is 32.34606966304308
At time: 297.0000846385956 and batch: 350, loss is 3.5234676694869993 and perplexity is 33.90178505490587
At time: 297.49184918403625 and batch: 400, loss is 3.5400477170944216 and perplexity is 34.46856389133469
At time: 297.99110293388367 and batch: 450, loss is 3.5823960304260254 and perplexity is 35.959597999027935
At time: 298.4933650493622 and batch: 500, loss is 3.6217671966552736 and perplexity is 37.403608983697694
At time: 298.9864194393158 and batch: 550, loss is 3.5021921730041505 and perplexity is 33.18812638685233
At time: 299.48543071746826 and batch: 600, loss is 3.465088348388672 and perplexity is 31.97928496662573
At time: 299.98696541786194 and batch: 650, loss is 3.426742262840271 and perplexity is 30.7762184300583
At time: 300.49127745628357 and batch: 700, loss is 3.5066176509857176 and perplexity is 33.335325181700725
At time: 300.9934673309326 and batch: 750, loss is 3.4642848110198976 and perplexity is 31.95359873743295
At time: 301.49585938453674 and batch: 800, loss is 3.590070242881775 and perplexity is 36.236621201780686
At time: 301.9988043308258 and batch: 850, loss is 3.4460631227493286 and perplexity is 31.376622923178047
At time: 302.51777243614197 and batch: 900, loss is 3.370892353057861 and perplexity is 29.104486951307734
At time: 303.04086804389954 and batch: 950, loss is 3.4211350202560427 and perplexity is 30.604131624611576
At time: 303.5434651374817 and batch: 1000, loss is 3.3256996154785154 and perplexity is 27.818454057268344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314945500071456 and perplexity of 74.8095462179164
Finished 28 epochs...
Completing Train Step...
At time: 304.98385667800903 and batch: 50, loss is 3.6304942035675047 and perplexity is 37.73145903188911
At time: 305.4686267375946 and batch: 100, loss is 3.50978554725647 and perplexity is 33.4410954802399
At time: 305.9556131362915 and batch: 150, loss is 3.622068877220154 and perplexity is 37.414894627828616
At time: 306.4465579986572 and batch: 200, loss is 3.6850709104537964 and perplexity is 39.84794798575473
At time: 306.93661093711853 and batch: 250, loss is 3.651517972946167 and perplexity is 38.5331139011133
At time: 307.4270534515381 and batch: 300, loss is 3.4761908197402955 and perplexity is 32.33631233748797
At time: 307.9317672252655 and batch: 350, loss is 3.5231881809234618 and perplexity is 33.89231121767577
At time: 308.4248626232147 and batch: 400, loss is 3.539790000915527 and perplexity is 34.45968192931846
At time: 308.9171040058136 and batch: 450, loss is 3.5821521043777467 and perplexity is 35.950827586099905
At time: 309.410710811615 and batch: 500, loss is 3.6215382194519044 and perplexity is 37.395045390387935
At time: 309.9040849208832 and batch: 550, loss is 3.501981678009033 and perplexity is 33.18114118755112
At time: 310.39537596702576 and batch: 600, loss is 3.4649109554290773 and perplexity is 31.973612569756295
At time: 310.8882896900177 and batch: 650, loss is 3.4266116714477537 and perplexity is 30.77219958325622
At time: 311.3843946456909 and batch: 700, loss is 3.506512460708618 and perplexity is 33.33181881402879
At time: 311.88152623176575 and batch: 750, loss is 3.464229311943054 and perplexity is 31.951825391411184
At time: 312.3747477531433 and batch: 800, loss is 3.5900379753112794 and perplexity is 36.23545195291605
At time: 312.8720245361328 and batch: 850, loss is 3.446067976951599 and perplexity is 31.376775232021945
At time: 313.3711688518524 and batch: 900, loss is 3.3709196996688844 and perplexity is 29.105282871274227
At time: 313.8799340724945 and batch: 950, loss is 3.421184959411621 and perplexity is 30.60566000726487
At time: 314.3755841255188 and batch: 1000, loss is 3.3257741737365722 and perplexity is 27.820528230067083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314943267077934 and perplexity of 74.80937916887086
Finished 29 epochs...
Completing Train Step...
At time: 315.824914932251 and batch: 50, loss is 3.630206322669983 and perplexity is 37.720598428953274
At time: 316.3317012786865 and batch: 100, loss is 3.5094863033294676 and perplexity is 33.43108993262948
At time: 316.8258218765259 and batch: 150, loss is 3.621761269569397 and perplexity is 37.40338728995215
At time: 317.3224380016327 and batch: 200, loss is 3.684768781661987 and perplexity is 39.835910591886915
At time: 317.8167049884796 and batch: 250, loss is 3.651223087310791 and perplexity is 38.52175271454525
At time: 318.31407403945923 and batch: 300, loss is 3.4759005212783816 and perplexity is 32.3269265181628
At time: 318.81570267677307 and batch: 350, loss is 3.522919497489929 and perplexity is 33.883206138374206
At time: 319.3168423175812 and batch: 400, loss is 3.539542279243469 and perplexity is 34.45114657653192
At time: 319.81784081459045 and batch: 450, loss is 3.5819178199768067 and perplexity is 35.94240585457428
At time: 320.32863664627075 and batch: 500, loss is 3.6213189220428466 and perplexity is 37.386845652945674
At time: 320.8383867740631 and batch: 550, loss is 3.5017771434783938 and perplexity is 33.17435519242107
At time: 321.33286571502686 and batch: 600, loss is 3.4647378158569335 and perplexity is 31.96807715136989
At time: 321.8270773887634 and batch: 650, loss is 3.426481103897095 and perplexity is 30.768181994817297
At time: 322.3218033313751 and batch: 700, loss is 3.506405029296875 and perplexity is 33.32823812202048
At time: 322.817307472229 and batch: 750, loss is 3.464167013168335 and perplexity is 31.949834893842688
At time: 323.3115043640137 and batch: 800, loss is 3.589997434616089 and perplexity is 36.23398297228028
At time: 323.8055672645569 and batch: 850, loss is 3.446061272621155 and perplexity is 31.37656487245768
At time: 324.3020920753479 and batch: 900, loss is 3.370934147834778 and perplexity is 29.1057033922674
At time: 324.7969355583191 and batch: 950, loss is 3.4212221479415894 and perplexity is 30.606798207933124
At time: 325.2904236316681 and batch: 1000, loss is 3.3258336210250854 and perplexity is 27.822182134194932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314948849561738 and perplexity of 74.80979679218409
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 326.7282884120941 and batch: 50, loss is 3.630131411552429 and perplexity is 37.71777284260539
At time: 327.2333769798279 and batch: 100, loss is 3.509520831108093 and perplexity is 33.43224425382985
At time: 327.72633814811707 and batch: 150, loss is 3.6219092750549318 and perplexity is 37.40892360614113
At time: 328.2193605899811 and batch: 200, loss is 3.684866051673889 and perplexity is 39.83978561984326
At time: 328.70999097824097 and batch: 250, loss is 3.651408843994141 and perplexity is 38.52890905221452
At time: 329.20119190216064 and batch: 300, loss is 3.4756475639343263 and perplexity is 32.318750218860366
At time: 329.69289231300354 and batch: 350, loss is 3.52268443107605 and perplexity is 33.87524227067211
At time: 330.1849181652069 and batch: 400, loss is 3.5392652940750122 and perplexity is 34.44160544133135
At time: 330.6767063140869 and batch: 450, loss is 3.581854853630066 and perplexity is 35.94014276383458
At time: 331.16889929771423 and batch: 500, loss is 3.6211967945098875 and perplexity is 37.38227996852452
At time: 331.6612219810486 and batch: 550, loss is 3.501399025917053 and perplexity is 33.1618137573545
At time: 332.15322947502136 and batch: 600, loss is 3.464297680854797 and perplexity is 31.95400997761944
At time: 332.6439485549927 and batch: 650, loss is 3.425973720550537 and perplexity is 30.752574691447
At time: 333.14878845214844 and batch: 700, loss is 3.5058323097229005 and perplexity is 33.30915585259811
At time: 333.6452775001526 and batch: 750, loss is 3.4633909702301025 and perplexity is 31.925050068383882
At time: 334.1422073841095 and batch: 800, loss is 3.5891558837890627 and perplexity is 36.203503060939646
At time: 334.63951873779297 and batch: 850, loss is 3.445197734832764 and perplexity is 31.349481718366892
At time: 335.1358571052551 and batch: 900, loss is 3.369919900894165 and perplexity is 29.07619798706023
At time: 335.63225865364075 and batch: 950, loss is 3.4201827526092528 and perplexity is 30.575002171926286
At time: 336.12742352485657 and batch: 1000, loss is 3.3247601270675657 and perplexity is 27.792331215046186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314929124785633 and perplexity of 74.80832120024482
Finished 31 epochs...
Completing Train Step...
At time: 337.57838344573975 and batch: 50, loss is 3.6300581979751585 and perplexity is 37.715011490614366
At time: 338.0697371959686 and batch: 100, loss is 3.509438409805298 and perplexity is 33.42948883825711
At time: 338.56181359291077 and batch: 150, loss is 3.621824150085449 and perplexity is 37.40573930819433
At time: 339.0537085533142 and batch: 200, loss is 3.6847865915298463 and perplexity is 39.836620070508445
At time: 339.5463192462921 and batch: 250, loss is 3.6513270854949953 and perplexity is 38.52575911520552
At time: 340.0381143093109 and batch: 300, loss is 3.475583872795105 and perplexity is 32.316691866390826
At time: 340.53023862838745 and batch: 350, loss is 3.522620177268982 and perplexity is 33.87306572731722
At time: 341.03657746315 and batch: 400, loss is 3.5392051696777345 and perplexity is 34.43953472281389
At time: 341.5325675010681 and batch: 450, loss is 3.5817907667160034 and perplexity is 35.93783954479776
At time: 342.0243031978607 and batch: 500, loss is 3.621145725250244 and perplexity is 37.3803709319097
At time: 342.5162205696106 and batch: 550, loss is 3.5013632869720457 and perplexity is 33.16062861029436
At time: 343.0077874660492 and batch: 600, loss is 3.4642691898345945 and perplexity is 31.953099588244633
At time: 343.49942564964294 and batch: 650, loss is 3.425945439338684 and perplexity is 30.75170498366538
At time: 343.9928901195526 and batch: 700, loss is 3.505814027786255 and perplexity is 33.30854690228751
At time: 344.4881374835968 and batch: 750, loss is 3.4633987855911257 and perplexity is 31.92529957515084
At time: 344.9834921360016 and batch: 800, loss is 3.5891621589660643 and perplexity is 36.203730245042244
At time: 345.47473311424255 and batch: 850, loss is 3.4452104663848875 and perplexity is 31.349880848468207
At time: 345.9792001247406 and batch: 900, loss is 3.369941682815552 and perplexity is 29.0768313294167
At time: 346.4745247364044 and batch: 950, loss is 3.420201072692871 and perplexity is 30.57556231365361
At time: 346.971896648407 and batch: 1000, loss is 3.324797420501709 and perplexity is 27.793367705847068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314911260837462 and perplexity of 74.80698484020854
Finished 32 epochs...
Completing Train Step...
At time: 348.4103651046753 and batch: 50, loss is 3.629987988471985 and perplexity is 37.71236363134896
At time: 348.91845703125 and batch: 100, loss is 3.5093613386154177 and perplexity is 33.426912487057756
At time: 349.41435861587524 and batch: 150, loss is 3.62174400806427 and perplexity is 37.402741656763006
At time: 349.9107406139374 and batch: 200, loss is 3.6847090244293215 and perplexity is 39.833530179233364
At time: 350.4043126106262 and batch: 250, loss is 3.6512503910064695 and perplexity is 38.52280451511733
At time: 350.8974447250366 and batch: 300, loss is 3.475520443916321 and perplexity is 32.31464211986698
At time: 351.3905243873596 and batch: 350, loss is 3.5225576877593996 and perplexity is 33.87094908218661
At time: 351.88638138771057 and batch: 400, loss is 3.5391463947296145 and perplexity is 34.4375106004317
At time: 352.38203620910645 and batch: 450, loss is 3.5817303895950316 and perplexity is 35.935669787014625
At time: 352.87832832336426 and batch: 500, loss is 3.6210952377319336 and perplexity is 37.37848373738811
At time: 353.37522196769714 and batch: 550, loss is 3.501326403617859 and perplexity is 33.15940555763955
At time: 353.86989092826843 and batch: 600, loss is 3.4642394399642944 and perplexity is 31.952149001816178
At time: 354.3615345954895 and batch: 650, loss is 3.4259170389175413 and perplexity is 30.750831634694784
At time: 354.8551242351532 and batch: 700, loss is 3.5057957935333253 and perplexity is 33.30793955135589
At time: 355.3524205684662 and batch: 750, loss is 3.463403191566467 and perplexity is 31.925440237543416
At time: 355.8476629257202 and batch: 800, loss is 3.589167013168335 and perplexity is 36.20390598569835
At time: 356.3444368839264 and batch: 850, loss is 3.445222191810608 and perplexity is 31.35024844132253
At time: 356.8405373096466 and batch: 900, loss is 3.3699611377716066 and perplexity is 29.07739702339518
At time: 357.33571791648865 and batch: 950, loss is 3.420218758583069 and perplexity is 30.576103074473338
At time: 357.8309199810028 and batch: 1000, loss is 3.324832010269165 and perplexity is 27.794329088599742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314894141220465 and perplexity of 74.80570418424158
Finished 33 epochs...
Completing Train Step...
At time: 359.2788836956024 and batch: 50, loss is 3.62992036819458 and perplexity is 37.70981359707659
At time: 359.7789480686188 and batch: 100, loss is 3.5092880821228025 and perplexity is 33.4244638383808
At time: 360.2673571109772 and batch: 150, loss is 3.621667594909668 and perplexity is 37.39988370447622
At time: 360.7552261352539 and batch: 200, loss is 3.6846334886550904 and perplexity is 39.830521436326215
At time: 361.2446765899658 and batch: 250, loss is 3.6511774587631227 and perplexity is 38.5199950630151
At time: 361.73545575141907 and batch: 300, loss is 3.475457439422607 and perplexity is 32.31260621633689
At time: 362.2287425994873 and batch: 350, loss is 3.5224965620040893 and perplexity is 33.868878758116594
At time: 362.72120237350464 and batch: 400, loss is 3.539089069366455 and perplexity is 34.43553651421337
At time: 363.2124185562134 and batch: 450, loss is 3.581672878265381 and perplexity is 35.933603138291716
At time: 363.7032506465912 and batch: 500, loss is 3.6210455083847046 and perplexity is 37.37662497600932
At time: 364.1943337917328 and batch: 550, loss is 3.5012888622283938 and perplexity is 33.15816073084749
At time: 364.6862006187439 and batch: 600, loss is 3.464208745956421 and perplexity is 31.951168277354398
At time: 365.1777892112732 and batch: 650, loss is 3.42588885307312 and perplexity is 30.749964908753267
At time: 365.6693911552429 and batch: 700, loss is 3.5057777786254882 and perplexity is 33.30733951729942
At time: 366.1607666015625 and batch: 750, loss is 3.463404960632324 and perplexity is 31.925496715799664
At time: 366.6527283191681 and batch: 800, loss is 3.5891704320907594 and perplexity is 36.204029764255964
At time: 367.147647857666 and batch: 850, loss is 3.445232882499695 and perplexity is 31.350583598872944
At time: 367.6420364379883 and batch: 900, loss is 3.3699785470962524 and perplexity is 29.077903245646297
At time: 368.13545632362366 and batch: 950, loss is 3.420235586166382 and perplexity is 30.576617600724315
At time: 368.6284649372101 and batch: 1000, loss is 3.324863815307617 and perplexity is 27.795213102363128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314879998928163 and perplexity of 74.80464626758781
Finished 34 epochs...
Completing Train Step...
At time: 370.0835864543915 and batch: 50, loss is 3.629854645729065 and perplexity is 37.707335296593776
At time: 370.57379627227783 and batch: 100, loss is 3.5092176580429078 and perplexity is 33.42211003415223
At time: 371.0781488418579 and batch: 150, loss is 3.6215938425064085 and perplexity is 37.397125474885655
At time: 371.5910151004791 and batch: 200, loss is 3.684559869766235 and perplexity is 39.82758926552845
At time: 372.08711886405945 and batch: 250, loss is 3.651107473373413 and perplexity is 38.51729932048139
At time: 372.57532382011414 and batch: 300, loss is 3.4753949451446533 and perplexity is 32.31058692644027
At time: 373.06603240966797 and batch: 350, loss is 3.5224368381500244 and perplexity is 33.866856038547205
At time: 373.55690145492554 and batch: 400, loss is 3.539032835960388 and perplexity is 34.43360014115038
At time: 374.04822635650635 and batch: 450, loss is 3.581617708206177 and perplexity is 35.931620733964344
At time: 374.5395407676697 and batch: 500, loss is 3.620996651649475 and perplexity is 37.374798920747004
At time: 375.0314886569977 and batch: 550, loss is 3.5012507486343383 and perplexity is 33.156896978252995
At time: 375.5208315849304 and batch: 600, loss is 3.4641771364212035 and perplexity is 31.95015833173754
At time: 376.0103282928467 and batch: 650, loss is 3.42586088180542 and perplexity is 30.74910480528218
At time: 376.5002546310425 and batch: 700, loss is 3.505759563446045 and perplexity is 33.30673282365886
At time: 376.9932427406311 and batch: 750, loss is 3.4634046745300293 and perplexity is 31.9254875818431
At time: 377.48556113243103 and batch: 800, loss is 3.589172863960266 and perplexity is 36.204117807839026
At time: 377.9780728816986 and batch: 850, loss is 3.4452424478530883 and perplexity is 31.350883479718384
At time: 378.47268533706665 and batch: 900, loss is 3.369994406700134 and perplexity is 29.078364413330444
At time: 378.96462893486023 and batch: 950, loss is 3.4202515840530396 and perplexity is 30.57710676589986
At time: 379.4562804698944 and batch: 1000, loss is 3.3248934316635133 and perplexity is 27.796036307476687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314866228801448 and perplexity of 74.80361620522194
Finished 35 epochs...
Completing Train Step...
At time: 380.8913960456848 and batch: 50, loss is 3.629790382385254 and perplexity is 37.704912175001205
At time: 381.3962652683258 and batch: 100, loss is 3.5091496467590333 and perplexity is 33.41983703083483
At time: 381.8872535228729 and batch: 150, loss is 3.6215222024917604 and perplexity is 37.394446440233025
At time: 382.3785352706909 and batch: 200, loss is 3.684488034248352 and perplexity is 39.82472833278705
At time: 382.8684871196747 and batch: 250, loss is 3.6510398101806643 and perplexity is 38.51469320520334
At time: 383.36872148513794 and batch: 300, loss is 3.4753329801559447 and perplexity is 32.30858486331576
At time: 383.8570055961609 and batch: 350, loss is 3.5223782205581666 and perplexity is 33.86487090318493
At time: 384.3472547531128 and batch: 400, loss is 3.5389777565002443 and perplexity is 34.431703609274244
At time: 384.8388411998749 and batch: 450, loss is 3.581564340591431 and perplexity is 35.929703200239366
At time: 385.3310103416443 and batch: 500, loss is 3.6209482192993163 and perplexity is 37.372988815232745
At time: 385.8227164745331 and batch: 550, loss is 3.5012125825881957 and perplexity is 33.15563153474162
At time: 386.312584400177 and batch: 600, loss is 3.4641452741622927 and perplexity is 31.949140343738318
At time: 386.8024251461029 and batch: 650, loss is 3.425833225250244 and perplexity is 30.748254402728183
At time: 387.2904226779938 and batch: 700, loss is 3.5057415103912355 and perplexity is 33.30613154081319
At time: 387.7825291156769 and batch: 750, loss is 3.463402771949768 and perplexity is 31.925426841098375
At time: 388.2763788700104 and batch: 800, loss is 3.589173936843872 and perplexity is 36.20415665066432
At time: 388.770480632782 and batch: 850, loss is 3.4452509689331055 and perplexity is 31.351150624243303
At time: 389.2652213573456 and batch: 900, loss is 3.3700088119506835 and perplexity is 29.078783297472437
At time: 389.75867462158203 and batch: 950, loss is 3.4202665996551516 and perplexity is 30.57756590301589
At time: 390.2513916492462 and batch: 1000, loss is 3.324920930862427 and perplexity is 27.796800686717972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314854319502667 and perplexity of 74.80272535191139
Finished 36 epochs...
Completing Train Step...
At time: 391.6851375102997 and batch: 50, loss is 3.629727530479431 and perplexity is 37.70254242388458
At time: 392.19246792793274 and batch: 100, loss is 3.50908341884613 and perplexity is 33.41762377806908
At time: 392.6867835521698 and batch: 150, loss is 3.621452445983887 and perplexity is 37.39183802521351
At time: 393.1816041469574 and batch: 200, loss is 3.684417862892151 and perplexity is 39.82193387563616
At time: 393.6765196323395 and batch: 250, loss is 3.6509738683700563 and perplexity is 38.512153560333694
At time: 394.17092204093933 and batch: 300, loss is 3.475271692276001 and perplexity is 32.3066047993231
At time: 394.66584634780884 and batch: 350, loss is 3.5223206520080566 and perplexity is 33.862921407782714
At time: 395.160329580307 and batch: 400, loss is 3.5389238500595095 and perplexity is 34.429847568710954
At time: 395.65732312202454 and batch: 450, loss is 3.5815123414993284 and perplexity is 35.92783493686787
At time: 396.16623306274414 and batch: 500, loss is 3.6209008646011354 and perplexity is 37.37121907053047
At time: 396.68395805358887 and batch: 550, loss is 3.5011741399765013 and perplexity is 33.154356970172
At time: 397.19042015075684 and batch: 600, loss is 3.4641130352020264 and perplexity is 31.948110353275236
At time: 397.69249749183655 and batch: 650, loss is 3.4258058404922487 and perplexity is 30.747412380751918
At time: 398.20273423194885 and batch: 700, loss is 3.5057233285903933 and perplexity is 33.30552598086779
At time: 398.70138144493103 and batch: 750, loss is 3.4633996438980104 and perplexity is 31.925326976867023
At time: 399.1953983306885 and batch: 800, loss is 3.589174304008484 and perplexity is 36.204169943551896
At time: 399.6917881965637 and batch: 850, loss is 3.4452585887908938 and perplexity is 31.35138951646272
At time: 400.1870446205139 and batch: 900, loss is 3.370021939277649 and perplexity is 29.07916502667408
At time: 400.68249320983887 and batch: 950, loss is 3.4202806663513186 and perplexity is 30.57799603137021
At time: 401.1951193809509 and batch: 1000, loss is 3.324946584701538 and perplexity is 27.797513790517485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314844271031822 and perplexity of 74.80197370268299
Finished 37 epochs...
Completing Train Step...
At time: 402.74375915527344 and batch: 50, loss is 3.62966570854187 and perplexity is 37.70021165170814
At time: 403.23705315589905 and batch: 100, loss is 3.509018635749817 and perplexity is 33.41545895105216
At time: 403.7314188480377 and batch: 150, loss is 3.6213842391967774 and perplexity is 37.389287735052235
At time: 404.23429560661316 and batch: 200, loss is 3.6843492364883423 and perplexity is 39.81920113329179
At time: 404.72529911994934 and batch: 250, loss is 3.6509095811843872 and perplexity is 38.509677801947866
At time: 405.2108392715454 and batch: 300, loss is 3.4752110290527343 and perplexity is 32.30464503598654
At time: 405.69448018074036 and batch: 350, loss is 3.5222640466690063 and perplexity is 33.86100463988536
At time: 406.1851797103882 and batch: 400, loss is 3.538870873451233 and perplexity is 34.428023640476475
At time: 406.69015431404114 and batch: 450, loss is 3.5814614629745485 and perplexity is 35.92600702812879
At time: 407.1819591522217 and batch: 500, loss is 3.620854072570801 and perplexity is 37.369470436225484
At time: 407.6724896430969 and batch: 550, loss is 3.501135478019714 and perplexity is 33.153075182633884
At time: 408.16291785240173 and batch: 600, loss is 3.4640806198120115 and perplexity is 31.94707475960257
At time: 408.66511392593384 and batch: 650, loss is 3.425778923034668 and perplexity is 30.746584749722356
At time: 409.15427827835083 and batch: 700, loss is 3.505705199241638 and perplexity is 33.304922178845096
At time: 409.6446659564972 and batch: 750, loss is 3.463395314216614 and perplexity is 31.92518875067197
At time: 410.13821053504944 and batch: 800, loss is 3.5891736364364624 and perplexity is 36.204145774669044
At time: 410.6294357776642 and batch: 850, loss is 3.4452653789520262 and perplexity is 31.351602398172012
At time: 411.12171459198 and batch: 900, loss is 3.370033793449402 and perplexity is 29.079509738133872
At time: 411.6159760951996 and batch: 950, loss is 3.4202939081192016 and perplexity is 30.578400940776838
At time: 412.1232810020447 and batch: 1000, loss is 3.3249704885482787 and perplexity is 27.79817826596863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314835711223323 and perplexity of 74.80133341485316
Finished 38 epochs...
Completing Train Step...
At time: 413.57690382003784 and batch: 50, loss is 3.6296049213409423 and perplexity is 37.69792003101875
At time: 414.0983073711395 and batch: 100, loss is 3.5089549922943117 and perplexity is 33.41333234345027
At time: 414.59747767448425 and batch: 150, loss is 3.6213172912597655 and perplexity is 37.386784683160045
At time: 415.08928871154785 and batch: 200, loss is 3.68428204536438 and perplexity is 39.81652572629516
At time: 415.58165192604065 and batch: 250, loss is 3.6508467292785642 and perplexity is 38.50725747136738
At time: 416.0742778778076 and batch: 300, loss is 3.475151290893555 and perplexity is 32.302715273599944
At time: 416.5810880661011 and batch: 350, loss is 3.522208390235901 and perplexity is 33.85912010958934
At time: 417.07643485069275 and batch: 400, loss is 3.5388186836242674 and perplexity is 34.426226894766245
At time: 417.5670471191406 and batch: 450, loss is 3.5814115715026857 and perplexity is 35.92421467147203
At time: 418.05812096595764 and batch: 500, loss is 3.620807938575745 and perplexity is 37.36774647302809
At time: 418.5506761074066 and batch: 550, loss is 3.5010970497131346 and perplexity is 33.15180119057555
At time: 419.0430574417114 and batch: 600, loss is 3.4640480947494505 and perplexity is 31.946035695895272
At time: 419.5350387096405 and batch: 650, loss is 3.4257521295547484 and perplexity is 30.74576095275751
At time: 420.0264358520508 and batch: 700, loss is 3.50568687915802 and perplexity is 33.304312035474844
At time: 420.5188932418823 and batch: 750, loss is 3.4633900928497314 and perplexity is 31.925022057983895
At time: 421.010977268219 and batch: 800, loss is 3.5891720819473267 and perplexity is 36.204089495761515
At time: 421.51739263534546 and batch: 850, loss is 3.445271348953247 and perplexity is 31.351789567835308
At time: 422.00964546203613 and batch: 900, loss is 3.370044593811035 and perplexity is 29.07982380905119
At time: 422.5020875930786 and batch: 950, loss is 3.4203063106536864 and perplexity is 30.578780192800835
At time: 422.99373626708984 and batch: 1000, loss is 3.3249928188323974 and perplexity is 27.798799014118007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314827151414825 and perplexity of 74.80069313250405
Finished 39 epochs...
Completing Train Step...
At time: 424.4198396205902 and batch: 50, loss is 3.6295450115203858 and perplexity is 37.695661623045424
At time: 424.92373871803284 and batch: 100, loss is 3.5088925170898437 and perplexity is 33.41124490388744
At time: 425.41443133354187 and batch: 150, loss is 3.6212515211105347 and perplexity is 37.38432582961264
At time: 425.90654706954956 and batch: 200, loss is 3.684216032028198 and perplexity is 39.81389739135034
At time: 426.39788699150085 and batch: 250, loss is 3.6507849311828613 and perplexity is 38.50487786971309
At time: 426.89022183418274 and batch: 300, loss is 3.475092010498047 and perplexity is 32.300800412619964
At time: 427.3845782279968 and batch: 350, loss is 3.522153639793396 and perplexity is 33.857266358527816
At time: 427.8792040348053 and batch: 400, loss is 3.538767309188843 and perplexity is 34.424458312226086
At time: 428.37435245513916 and batch: 450, loss is 3.5813627099990843 and perplexity is 35.922459403210354
At time: 428.8701162338257 and batch: 500, loss is 3.620762462615967 and perplexity is 37.366047177531335
At time: 429.3658287525177 and batch: 550, loss is 3.50105863571167 and perplexity is 33.15052772169573
At time: 429.86814546585083 and batch: 600, loss is 3.4640155267715453 and perplexity is 31.94499529505254
At time: 430.36925745010376 and batch: 650, loss is 3.4257258939743043 and perplexity is 30.744954330453865
At time: 430.86873269081116 and batch: 700, loss is 3.5056684494018553 and perplexity is 33.30369825078076
At time: 431.36956691741943 and batch: 750, loss is 3.4633840799331663 and perplexity is 31.924830096067048
At time: 431.8701243400574 and batch: 800, loss is 3.589169840812683 and perplexity is 36.20400835761322
At time: 432.36972093582153 and batch: 850, loss is 3.44527645111084 and perplexity is 31.35194953001458
At time: 432.87115383148193 and batch: 900, loss is 3.3700543785095216 and perplexity is 29.080108347751267
At time: 433.3719003200531 and batch: 950, loss is 3.4203177881240845 and perplexity is 30.579131161859422
At time: 433.88604736328125 and batch: 1000, loss is 3.325013585090637 and perplexity is 27.79937629715107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314820080268674 and perplexity of 74.80016420774075
Finished 40 epochs...
Completing Train Step...
At time: 435.4592604637146 and batch: 50, loss is 3.6294859409332276 and perplexity is 37.69343498394512
At time: 435.9511125087738 and batch: 100, loss is 3.5088310384750367 and perplexity is 33.409190889971384
At time: 436.4437756538391 and batch: 150, loss is 3.6211869716644287 and perplexity is 37.38191276996896
At time: 436.9381275177002 and batch: 200, loss is 3.6841513109207153 and perplexity is 39.81132067520283
At time: 437.4320969581604 and batch: 250, loss is 3.65072416305542 and perplexity is 38.50253807148089
At time: 437.9243619441986 and batch: 300, loss is 3.4750334453582763 and perplexity is 32.29890876712188
At time: 438.4170858860016 and batch: 350, loss is 3.5220993900299074 and perplexity is 33.85542965965619
At time: 438.91842007637024 and batch: 400, loss is 3.5387167501449586 and perplexity is 34.422717888525035
At time: 439.4143922328949 and batch: 450, loss is 3.5813146591186524 and perplexity is 35.92073333887854
At time: 439.9062509536743 and batch: 500, loss is 3.6207176780700685 and perplexity is 37.36437379354762
At time: 440.3993229866028 and batch: 550, loss is 3.501020140647888 and perplexity is 33.149251614578745
At time: 440.89203429222107 and batch: 600, loss is 3.463982996940613 and perplexity is 31.943956146658202
At time: 441.3837797641754 and batch: 650, loss is 3.4256997394561766 and perplexity is 30.744150221504082
At time: 441.8763520717621 and batch: 700, loss is 3.5056500673294066 and perplexity is 33.303086065413346
At time: 442.3681221008301 and batch: 750, loss is 3.4633774042129515 and perplexity is 31.924616975544787
At time: 442.85963797569275 and batch: 800, loss is 3.5891670417785644 and perplexity is 36.20390702150042
At time: 443.3497648239136 and batch: 850, loss is 3.4452807331085205 and perplexity is 31.35208377927717
At time: 443.84244108200073 and batch: 900, loss is 3.3700633430480957 and perplexity is 29.080369038672774
At time: 444.3372302055359 and batch: 950, loss is 3.420328764915466 and perplexity is 30.579466824445063
At time: 444.8318212032318 and batch: 1000, loss is 3.3250332498550415 and perplexity is 27.79992297071163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314814125619283 and perplexity of 74.79971880031466
Finished 41 epochs...
Completing Train Step...
At time: 446.26379919052124 and batch: 50, loss is 3.629427423477173 and perplexity is 37.69122932455532
At time: 446.76195001602173 and batch: 100, loss is 3.5087702655792237 and perplexity is 33.40716057838881
At time: 447.24780225753784 and batch: 150, loss is 3.62112334728241 and perplexity is 37.37953444453083
At time: 447.73340606689453 and batch: 200, loss is 3.684087800979614 and perplexity is 39.80879234085961
At time: 448.22050189971924 and batch: 250, loss is 3.6506644344329833 and perplexity is 38.500238436599254
At time: 448.70756936073303 and batch: 300, loss is 3.4749754428863526 and perplexity is 32.29703540490329
At time: 449.1941637992859 and batch: 350, loss is 3.5220457983016966 and perplexity is 33.85361533728818
At time: 449.6829764842987 and batch: 400, loss is 3.5386667680740356 and perplexity is 34.420997412794996
At time: 450.17316246032715 and batch: 450, loss is 3.5812671756744385 and perplexity is 35.91902773923511
At time: 450.6634588241577 and batch: 500, loss is 3.6206733179092407 and perplexity is 37.36271634067962
At time: 451.1544597148895 and batch: 550, loss is 3.500981960296631 and perplexity is 33.14798598866937
At time: 451.6449520587921 and batch: 600, loss is 3.4639505338668823 and perplexity is 31.942919164486476
At time: 452.13613748550415 and batch: 650, loss is 3.4256739521026613 and perplexity is 30.743357421455947
At time: 452.62634897232056 and batch: 700, loss is 3.505631365776062 and perplexity is 33.30246325179656
At time: 453.1178410053253 and batch: 750, loss is 3.4633701515197752 and perplexity is 31.924385436932734
At time: 453.6082229614258 and batch: 800, loss is 3.58916344165802 and perplexity is 36.20377668330558
At time: 454.0992274284363 and batch: 850, loss is 3.445284481048584 and perplexity is 31.352201285228247
At time: 454.5900614261627 and batch: 900, loss is 3.370071415901184 and perplexity is 29.080603801167374
At time: 455.08302783966064 and batch: 950, loss is 3.4203387451171876 and perplexity is 30.579772015215436
At time: 455.575585603714 and batch: 1000, loss is 3.325051565170288 and perplexity is 27.80043213972745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314807426638719 and perplexity of 74.79921772013059
Finished 42 epochs...
Completing Train Step...
At time: 457.0018661022186 and batch: 50, loss is 3.62936975479126 and perplexity is 37.68905578356294
At time: 457.5046010017395 and batch: 100, loss is 3.508710308074951 and perplexity is 33.405157628462234
At time: 457.9944157600403 and batch: 150, loss is 3.6210606288909912 and perplexity is 37.37719013377497
At time: 458.48749709129333 and batch: 200, loss is 3.6840251779556272 and perplexity is 39.80629947195827
At time: 458.98087191581726 and batch: 250, loss is 3.650605363845825 and perplexity is 38.49796427207786
At time: 459.47373032569885 and batch: 300, loss is 3.4749180698394775 and perplexity is 32.2951824787316
At time: 459.9789390563965 and batch: 350, loss is 3.5219929361343385 and perplexity is 33.85182580910813
At time: 460.47412276268005 and batch: 400, loss is 3.5386174964904784 and perplexity is 34.4193014775259
At time: 460.96259903907776 and batch: 450, loss is 3.5812202644348146 and perplexity is 35.91734277264002
At time: 461.4698612689972 and batch: 500, loss is 3.6206295251846314 and perplexity is 37.36108016135889
At time: 461.9623022079468 and batch: 550, loss is 3.5009437370300294 and perplexity is 33.146718988578215
At time: 462.4544429779053 and batch: 600, loss is 3.4639180374145506 and perplexity is 31.941881149802498
At time: 462.9474844932556 and batch: 650, loss is 3.4256482982635497 and perplexity is 30.742568746427217
At time: 463.44256472587585 and batch: 700, loss is 3.505612668991089 and perplexity is 33.3018406086228
At time: 463.9357645511627 and batch: 750, loss is 3.463362307548523 and perplexity is 31.924135023953244
At time: 464.4411952495575 and batch: 800, loss is 3.5891592741012572 and perplexity is 36.203625802325625
At time: 464.93589329719543 and batch: 850, loss is 3.4452873182296755 and perplexity is 31.352290237227095
At time: 465.42739367485046 and batch: 900, loss is 3.3700787830352783 and perplexity is 29.080818042664294
At time: 465.91954922676086 and batch: 950, loss is 3.4203481483459472 and perplexity is 30.58005956515906
At time: 466.41292786598206 and batch: 1000, loss is 3.3250686931610107 and perplexity is 27.80090830934913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3148040771484375 and perplexity of 74.79896718129736
Finished 43 epochs...
Completing Train Step...
At time: 467.866242647171 and batch: 50, loss is 3.62931263923645 and perplexity is 37.68690321370481
At time: 468.3565194606781 and batch: 100, loss is 3.5086511182785034 and perplexity is 33.4031804424971
At time: 468.862140417099 and batch: 150, loss is 3.620998797416687 and perplexity is 37.374879118451126
At time: 469.3562202453613 and batch: 200, loss is 3.6839636039733885 and perplexity is 39.80384851503996
At time: 469.8514382839203 and batch: 250, loss is 3.6505470991134645 and perplexity is 38.495721263837886
At time: 470.34677505493164 and batch: 300, loss is 3.4748612451553345 and perplexity is 32.29334736732821
At time: 470.84233593940735 and batch: 350, loss is 3.521940541267395 and perplexity is 33.85005219366365
At time: 471.3359203338623 and batch: 400, loss is 3.53856876373291 and perplexity is 34.41762417092154
At time: 471.8286340236664 and batch: 450, loss is 3.581174054145813 and perplexity is 35.915683060198525
At time: 472.3204972743988 and batch: 500, loss is 3.620586404800415 and perplexity is 37.35946917196108
At time: 472.8145067691803 and batch: 550, loss is 3.500905728340149 and perplexity is 33.1454591491582
At time: 473.30970883369446 and batch: 600, loss is 3.4638856744766233 and perplexity is 31.940847433412706
At time: 473.8061213493347 and batch: 650, loss is 3.4256229543685914 and perplexity is 30.741789619867255
At time: 474.3042538166046 and batch: 700, loss is 3.505593776702881 and perplexity is 33.30121146659515
At time: 474.8008246421814 and batch: 750, loss is 3.463353910446167 and perplexity is 31.92386695484932
At time: 475.2993149757385 and batch: 800, loss is 3.5891546392440796 and perplexity is 36.203458004079586
At time: 475.7927756309509 and batch: 850, loss is 3.445289731025696 and perplexity is 31.352365883999468
At time: 476.28649163246155 and batch: 900, loss is 3.3700851917266847 and perplexity is 29.081004413250174
At time: 476.78112864494324 and batch: 950, loss is 3.4203569507598877 and perplexity is 30.580328744686387
At time: 477.2782144546509 and batch: 1000, loss is 3.325084939002991 and perplexity is 27.801359962181156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314798866830221 and perplexity of 74.79857745589135
Finished 44 epochs...
Completing Train Step...
At time: 478.7237811088562 and batch: 50, loss is 3.629256076812744 and perplexity is 37.68477161140193
At time: 479.22881531715393 and batch: 100, loss is 3.508592677116394 and perplexity is 33.40122837885489
At time: 479.72179222106934 and batch: 150, loss is 3.6209379053115844 and perplexity is 37.372603352672435
At time: 480.2197585105896 and batch: 200, loss is 3.6839028120040895 and perplexity is 39.80142883425238
At time: 480.7132365703583 and batch: 250, loss is 3.6504895067214966 and perplexity is 38.49350426701146
At time: 481.2183382511139 and batch: 300, loss is 3.4748049068450926 and perplexity is 32.29152806595415
At time: 481.7109076976776 and batch: 350, loss is 3.5218886518478394 and perplexity is 33.848295779673435
At time: 482.20339131355286 and batch: 400, loss is 3.538520555496216 and perplexity is 34.41596499794228
At time: 482.69630694389343 and batch: 450, loss is 3.581128268241882 and perplexity is 35.91403866582964
At time: 483.19649481773376 and batch: 500, loss is 3.6205434894561765 and perplexity is 37.35786591188348
At time: 483.6995515823364 and batch: 550, loss is 3.500867705345154 and perplexity is 33.144198883490546
At time: 484.19290375709534 and batch: 600, loss is 3.463853464126587 and perplexity is 31.939818624105655
At time: 484.6868600845337 and batch: 650, loss is 3.4255975818634035 and perplexity is 30.741009633545783
At time: 485.1796534061432 and batch: 700, loss is 3.5055748414993286 and perplexity is 33.300580907347396
At time: 485.67293524742126 and batch: 750, loss is 3.463345127105713 and perplexity is 31.923586557888658
At time: 486.1655044555664 and batch: 800, loss is 3.5891494226455687 and perplexity is 36.203269145667065
At time: 486.65839743614197 and batch: 850, loss is 3.4452912998199463 and perplexity is 31.352415069449385
At time: 487.15106105804443 and batch: 900, loss is 3.370090951919556 and perplexity is 29.08117192592693
At time: 487.64622712135315 and batch: 950, loss is 3.4203650903701783 and perplexity is 30.580577657657955
At time: 488.14051604270935 and batch: 1000, loss is 3.3251001262664794 and perplexity is 27.801782191966492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314797006002286 and perplexity of 74.79843826873847
Finished 45 epochs...
Completing Train Step...
At time: 489.5734124183655 and batch: 50, loss is 3.6292000102996824 and perplexity is 37.68265881689122
At time: 490.0770831108093 and batch: 100, loss is 3.508534631729126 and perplexity is 33.39928964788614
At time: 490.5673756599426 and batch: 150, loss is 3.6208776092529296 and perplexity is 37.37034999992343
At time: 491.05888772010803 and batch: 200, loss is 3.683842840194702 and perplexity is 39.79904194212282
At time: 491.54927682876587 and batch: 250, loss is 3.6504327201843263 and perplexity is 38.49131841626462
At time: 492.0409255027771 and batch: 300, loss is 3.4747492408752443 and perplexity is 32.28973057675642
At time: 492.53312277793884 and batch: 350, loss is 3.5218373012542723 and perplexity is 33.84655769422016
At time: 493.02456545829773 and batch: 400, loss is 3.5384727573394774 and perplexity is 34.41432001756684
At time: 493.5151174068451 and batch: 450, loss is 3.5810829639434814 and perplexity is 35.91241164236101
At time: 494.01827597618103 and batch: 500, loss is 3.620501146316528 and perplexity is 37.35628409603997
At time: 494.51010251045227 and batch: 550, loss is 3.5008299493789674 and perplexity is 33.14294751586167
At time: 495.0006728172302 and batch: 600, loss is 3.4638212537765503 and perplexity is 31.93878984793638
At time: 495.4933135509491 and batch: 650, loss is 3.4255723905563356 and perplexity is 30.740235237086598
At time: 495.9858956336975 and batch: 700, loss is 3.5055556297302246 and perplexity is 33.299941150421425
At time: 496.47798442840576 and batch: 750, loss is 3.463335938453674 and perplexity is 31.92329322450762
At time: 496.970064163208 and batch: 800, loss is 3.589143853187561 and perplexity is 36.20306751364131
At time: 497.46081256866455 and batch: 850, loss is 3.4452923917770386 and perplexity is 31.352449304960075
At time: 497.95270562171936 and batch: 900, loss is 3.3700961303710937 and perplexity is 29.081322521756338
At time: 498.4443693161011 and batch: 950, loss is 3.420372767448425 and perplexity is 30.580812428046645
At time: 498.93611097335815 and batch: 1000, loss is 3.3251144218444826 and perplexity is 27.802179637353294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314794400843178 and perplexity of 74.79824340715956
Finished 46 epochs...
Completing Train Step...
At time: 500.3837466239929 and batch: 50, loss is 3.629144535064697 and perplexity is 37.68056842052164
At time: 500.8711473941803 and batch: 100, loss is 3.508477234840393 and perplexity is 33.39737268758876
At time: 501.36041593551636 and batch: 150, loss is 3.62081814289093 and perplexity is 37.36812778723639
At time: 501.850389957428 and batch: 200, loss is 3.6837836933135986 and perplexity is 39.79668802253522
At time: 502.34400963783264 and batch: 250, loss is 3.650376410484314 and perplexity is 38.48915104269418
At time: 502.8379657268524 and batch: 300, loss is 3.474693899154663 and perplexity is 32.287943656955264
At time: 503.33326506614685 and batch: 350, loss is 3.5217865085601807 and perplexity is 33.84483858002873
At time: 503.82810378074646 and batch: 400, loss is 3.538425507545471 and perplexity is 34.41269398645026
At time: 504.3229637145996 and batch: 450, loss is 3.5810381269454954 and perplexity is 35.91080147373036
At time: 504.8168387413025 and batch: 500, loss is 3.6204591894149782 and perplexity is 37.35471677498607
At time: 505.31203293800354 and batch: 550, loss is 3.5007921934127806 and perplexity is 33.14169619547849
At time: 505.80624771118164 and batch: 600, loss is 3.4637892389297487 and perplexity is 31.937767348840033
At time: 506.31235814094543 and batch: 650, loss is 3.4255473470687865 and perplexity is 30.739465404027875
At time: 506.8076841831207 and batch: 700, loss is 3.505536427497864 and perplexity is 33.29930172335309
At time: 507.3025441169739 and batch: 750, loss is 3.4633262968063354 and perplexity is 31.92298543285627
At time: 507.79721879959106 and batch: 800, loss is 3.589137830734253 and perplexity is 36.20284948301414
At time: 508.294376373291 and batch: 850, loss is 3.445292854309082 and perplexity is 31.352463806475868
At time: 508.79583168029785 and batch: 900, loss is 3.370100598335266 and perplexity is 29.081452456353723
At time: 509.2965612411499 and batch: 950, loss is 3.420379810333252 and perplexity is 30.581027805944927
At time: 509.80101466178894 and batch: 1000, loss is 3.325127921104431 and perplexity is 27.80255494873656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.31479179568407 and perplexity of 74.79804854608831
Finished 47 epochs...
Completing Train Step...
At time: 511.2649483680725 and batch: 50, loss is 3.6290893602371215 and perplexity is 37.678489459009775
At time: 511.7667932510376 and batch: 100, loss is 3.5084204149246214 and perplexity is 33.3954751055964
At time: 512.2572922706604 and batch: 150, loss is 3.620759296417236 and perplexity is 37.36592886938748
At time: 512.7506494522095 and batch: 200, loss is 3.6837251949310303 and perplexity is 39.79436004874634
At time: 513.2434992790222 and batch: 250, loss is 3.65032066822052 and perplexity is 38.48700563007918
At time: 513.7356796264648 and batch: 300, loss is 3.474639024734497 and perplexity is 32.28617192338061
At time: 514.2267076969147 and batch: 350, loss is 3.521736001968384 and perplexity is 33.84312923574908
At time: 514.718977689743 and batch: 400, loss is 3.538378567695618 and perplexity is 34.411078697672494
At time: 515.2089507579803 and batch: 450, loss is 3.5809937143325805 and perplexity is 35.90920661662119
At time: 515.6983034610748 and batch: 500, loss is 3.620417609214783 and perplexity is 37.35316359067541
At time: 516.191059589386 and batch: 550, loss is 3.500754704475403 and perplexity is 33.14045377179394
At time: 516.6833357810974 and batch: 600, loss is 3.4637573194503783 and perplexity is 31.936747928203783
At time: 517.1845815181732 and batch: 650, loss is 3.425522389411926 and perplexity is 30.738698228571742
At time: 517.6855483055115 and batch: 700, loss is 3.5055169820785523 and perplexity is 33.29865421076389
At time: 518.1991527080536 and batch: 750, loss is 3.463316330909729 and perplexity is 31.922667293269345
At time: 518.7042262554169 and batch: 800, loss is 3.589131307601929 and perplexity is 36.202613327806695
At time: 519.213559627533 and batch: 850, loss is 3.445292887687683 and perplexity is 31.35246485297727
At time: 519.7123761177063 and batch: 900, loss is 3.3701044750213622 and perplexity is 29.081565196234646
At time: 520.2086234092712 and batch: 950, loss is 3.4203863906860352 and perplexity is 30.58122904055846
At time: 520.7032887935638 and batch: 1000, loss is 3.325140552520752 and perplexity is 27.8029061366009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314789562690549 and perplexity of 74.79788152271696
Finished 48 epochs...
Completing Train Step...
At time: 522.1591534614563 and batch: 50, loss is 3.62903489112854 and perplexity is 37.676437201169094
At time: 522.6655039787292 and batch: 100, loss is 3.5083640003204346 and perplexity is 33.393591166228035
At time: 523.1564922332764 and batch: 150, loss is 3.6207009458541872 and perplexity is 37.3637486100094
At time: 523.6475791931152 and batch: 200, loss is 3.683667531013489 and perplexity is 39.79206541620927
At time: 524.1413233280182 and batch: 250, loss is 3.650265407562256 and perplexity is 38.48487887157703
At time: 524.635716676712 and batch: 300, loss is 3.4745845699310305 and perplexity is 32.28441383410247
At time: 525.1493661403656 and batch: 350, loss is 3.5216858339309693 and perplexity is 33.841431434963376
At time: 525.646858215332 and batch: 400, loss is 3.5383321857452392 and perplexity is 34.409482681741316
At time: 526.1401517391205 and batch: 450, loss is 3.580949583053589 and perplexity is 35.907621932372955
At time: 526.6317558288574 and batch: 500, loss is 3.620376386642456 and perplexity is 37.351623828924346
At time: 527.1235318183899 and batch: 550, loss is 3.5007172775268556 and perplexity is 33.139213448946684
At time: 527.6146838665009 and batch: 600, loss is 3.4637253284454346 and perplexity is 31.93572625588517
At time: 528.1077682971954 and batch: 650, loss is 3.425497555732727 and perplexity is 30.73793488307931
At time: 528.6025004386902 and batch: 700, loss is 3.505497546195984 and perplexity is 33.29800702832027
At time: 529.096352815628 and batch: 750, loss is 3.4633059167861937 and perplexity is 31.92233484839965
At time: 529.5896861553192 and batch: 800, loss is 3.5891243743896486 and perplexity is 36.20236232827351
At time: 530.0834324359894 and batch: 850, loss is 3.4452923011779784 and perplexity is 31.35244646445776
At time: 530.5754568576813 and batch: 900, loss is 3.3701078414916994 and perplexity is 29.08166309862603
At time: 531.0670945644379 and batch: 950, loss is 3.4203924560546874 and perplexity is 30.581414527548947
At time: 531.5707705020905 and batch: 1000, loss is 3.325152597427368 and perplexity is 27.803241022025798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314788446193788 and perplexity of 74.79779801117114
Finished 49 epochs...
Completing Train Step...
At time: 533.0131151676178 and batch: 50, loss is 3.628980712890625 and perplexity is 37.67439601348509
At time: 533.5049169063568 and batch: 100, loss is 3.508308186531067 and perplexity is 33.391727395376996
At time: 533.9966473579407 and batch: 150, loss is 3.6206432676315305 and perplexity is 37.36159359754703
At time: 534.485337972641 and batch: 200, loss is 3.683610324859619 and perplexity is 39.78978913030167
At time: 534.9739241600037 and batch: 250, loss is 3.6502106904983522 and perplexity is 38.48277314961047
At time: 535.4629728794098 and batch: 300, loss is 3.4745306968688965 and perplexity is 32.28267462071882
At time: 535.9548556804657 and batch: 350, loss is 3.5216361570358274 and perplexity is 33.83975033947868
At time: 536.4453735351562 and batch: 400, loss is 3.5382859897613526 and perplexity is 34.40789313854934
At time: 536.9372320175171 and batch: 450, loss is 3.5809058141708374 and perplexity is 35.9060503302726
At time: 537.4291868209839 and batch: 500, loss is 3.620335521697998 and perplexity is 37.35009748807829
At time: 537.9194195270538 and batch: 550, loss is 3.5006798553466796 and perplexity is 33.137973330534216
At time: 538.4065341949463 and batch: 600, loss is 3.4636935138702394 and perplexity is 31.934710250482766
At time: 538.895491361618 and batch: 650, loss is 3.42547269821167 and perplexity is 30.737170823712052
At time: 539.3859341144562 and batch: 700, loss is 3.5054779767990114 and perplexity is 33.29735541277822
At time: 539.8772537708282 and batch: 750, loss is 3.463295216560364 and perplexity is 31.92199327403522
At time: 540.3691952228546 and batch: 800, loss is 3.5891171407699587 and perplexity is 36.202100455099696
At time: 540.8611605167389 and batch: 850, loss is 3.445291266441345 and perplexity is 31.352414022949645
At time: 541.3521361351013 and batch: 900, loss is 3.3701105880737305 and perplexity is 29.081742973909023
At time: 541.8406739234924 and batch: 950, loss is 3.420398015975952 and perplexity is 30.581584558278564
At time: 542.3305652141571 and batch: 1000, loss is 3.3251638269424437 and perplexity is 27.803553240693034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.314787701862614 and perplexity of 74.79774233685909
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
SETTINGS FOR THIS RUN
{'lr': 21.122041927612262, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.38984190782347494, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 2.0761984272186647, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7144849300384521 and batch: 50, loss is 6.724029912948608 and perplexity is 832.1643130330846
At time: 1.2221879959106445 and batch: 100, loss is 5.924477300643921 and perplexity is 374.0828513833772
At time: 1.715886116027832 and batch: 150, loss is 5.857000255584717 and perplexity is 349.6736376406525
At time: 2.208282470703125 and batch: 200, loss is 5.882354583740234 and perplexity is 358.6527261691892
At time: 2.701986312866211 and batch: 250, loss is 5.999415340423584 and perplexity is 403.19299392317293
At time: 3.228376626968384 and batch: 300, loss is 5.989749345779419 and perplexity is 399.31450751544
At time: 3.724984884262085 and batch: 350, loss is 6.016417741775513 and perplexity is 410.10685257296336
At time: 4.221228837966919 and batch: 400, loss is 6.0294755268096925 and perplexity is 415.4970551600815
At time: 4.717079162597656 and batch: 450, loss is 6.0852468395233155 and perplexity is 439.3282428641533
At time: 5.213004112243652 and batch: 500, loss is 6.144339056015014 and perplexity is 466.0715000851446
At time: 5.70728611946106 and batch: 550, loss is 6.127442169189453 and perplexity is 458.2625023622588
At time: 6.200336217880249 and batch: 600, loss is 6.100002632141114 and perplexity is 445.8589436446289
At time: 6.692035913467407 and batch: 650, loss is 6.122989349365234 and perplexity is 456.22647839510773
At time: 7.187801122665405 and batch: 700, loss is 6.217747564315796 and perplexity is 501.57219958896394
At time: 7.685090780258179 and batch: 750, loss is 6.101300945281983 and perplexity is 446.4381841065236
At time: 8.181977272033691 and batch: 800, loss is 6.219865293502807 and perplexity is 502.63551918950765
At time: 8.678246021270752 and batch: 850, loss is 6.23424204826355 and perplexity is 509.9139817529143
At time: 9.173415184020996 and batch: 900, loss is 6.251483297348022 and perplexity is 518.7817618576781
At time: 9.667771816253662 and batch: 950, loss is 6.24181960105896 and perplexity is 513.7925583715675
At time: 10.161547422409058 and batch: 1000, loss is 6.231150970458985 and perplexity is 508.34023150697533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.314351151629192 and perplexity of 552.4434922160067
Finished 1 epochs...
Completing Train Step...
At time: 11.618938207626343 and batch: 50, loss is 6.271959848403931 and perplexity is 529.514129028932
At time: 12.10706090927124 and batch: 100, loss is 6.284874839782715 and perplexity is 536.3971508520194
At time: 12.59620475769043 and batch: 150, loss is 6.246202144622803 and perplexity is 516.0492179840271
At time: 13.08384108543396 and batch: 200, loss is 6.260327701568603 and perplexity is 523.3904278722672
At time: 13.569711685180664 and batch: 250, loss is 6.360787343978882 and perplexity is 578.7018144582972
At time: 14.056097745895386 and batch: 300, loss is 6.226785373687744 and perplexity is 506.1258600768698
At time: 14.543425559997559 and batch: 350, loss is 6.245016498565674 and perplexity is 515.4377288399198
At time: 15.032215118408203 and batch: 400, loss is 6.183699493408203 and perplexity is 484.78209116581235
At time: 15.5321044921875 and batch: 450, loss is 6.238929109573364 and perplexity is 512.309589642983
At time: 16.01941180229187 and batch: 500, loss is 6.260289449691772 and perplexity is 523.3704075889946
At time: 16.508007049560547 and batch: 550, loss is 6.254219627380371 and perplexity is 520.2032639353389
At time: 16.996532201766968 and batch: 600, loss is 6.186089420318604 and perplexity is 485.94207051197134
At time: 17.483648777008057 and batch: 650, loss is 6.172436170578003 and perplexity is 479.35246916457226
At time: 17.972035884857178 and batch: 700, loss is 6.298261375427246 and perplexity is 543.6259266147358
At time: 18.460572242736816 and batch: 750, loss is 6.168518915176391 and perplexity is 477.47839612328664
At time: 18.949090480804443 and batch: 800, loss is 6.263648767471313 and perplexity is 525.1315315357948
At time: 19.43659234046936 and batch: 850, loss is 6.2263037109375 and perplexity is 505.8821368040581
At time: 19.92464590072632 and batch: 900, loss is 6.28505389213562 and perplexity is 536.4932026228623
At time: 20.411049842834473 and batch: 950, loss is 6.223765029907226 and perplexity is 504.59949222141114
At time: 20.89657497406006 and batch: 1000, loss is 6.282795066833496 and perplexity is 535.282725844831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.3538014481707314 and perplexity of 574.6731521701329
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 22.324450731277466 and batch: 50, loss is 6.072242765426636 and perplexity is 433.6521718530733
At time: 22.82546377182007 and batch: 100, loss is 6.009498205184936 and perplexity is 407.2788985497331
At time: 23.313064575195312 and batch: 150, loss is 5.900988788604736 and perplexity is 365.39859126178277
At time: 23.80093812942505 and batch: 200, loss is 5.887228240966797 and perplexity is 360.4049430037313
At time: 24.288150548934937 and batch: 250, loss is 5.932593326568604 and perplexity is 377.1312712974748
At time: 24.77478051185608 and batch: 300, loss is 5.8456528377532955 and perplexity is 345.72817252633587
At time: 25.262896060943604 and batch: 350, loss is 5.860903263092041 and perplexity is 351.0410833129284
At time: 25.75158381462097 and batch: 400, loss is 5.85568244934082 and perplexity is 349.2131390282592
At time: 26.239975690841675 and batch: 450, loss is 5.871439895629883 and perplexity is 354.759429234769
At time: 26.72788166999817 and batch: 500, loss is 5.899361047744751 and perplexity is 364.8043008512134
At time: 27.216846704483032 and batch: 550, loss is 5.83839714050293 and perplexity is 343.2287520598516
At time: 27.705750942230225 and batch: 600, loss is 5.799427709579468 and perplexity is 330.1105867145386
At time: 28.2058744430542 and batch: 650, loss is 5.730226917266846 and perplexity is 308.0391598515548
At time: 28.694542407989502 and batch: 700, loss is 5.815199365615845 and perplexity is 335.35825074105395
At time: 29.182835817337036 and batch: 750, loss is 5.7116478824615475 and perplexity is 302.3689264150425
At time: 29.670826196670532 and batch: 800, loss is 5.79115385055542 and perplexity is 327.39056630737315
At time: 30.156545877456665 and batch: 850, loss is 5.739298295974732 and perplexity is 310.84621239645537
At time: 30.642603635787964 and batch: 900, loss is 5.7798215103149415 and perplexity is 323.7014078982556
At time: 31.13182806968689 and batch: 950, loss is 5.719748411178589 and perplexity is 304.8282219356638
At time: 31.61823296546936 and batch: 1000, loss is 5.697669467926025 and perplexity is 298.1716919042719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.84773403260766 and perplexity of 346.4484494783673
Finished 3 epochs...
Completing Train Step...
At time: 33.07709288597107 and batch: 50, loss is 5.789239091873169 and perplexity is 326.7642921513302
At time: 33.58024454116821 and batch: 100, loss is 5.801411590576172 and perplexity is 330.7661368860877
At time: 34.0705292224884 and batch: 150, loss is 5.735692644119263 and perplexity is 309.7274273607172
At time: 34.56118559837341 and batch: 200, loss is 5.734701681137085 and perplexity is 309.4206509727643
At time: 35.055227518081665 and batch: 250, loss is 5.788579902648926 and perplexity is 326.5489636299943
At time: 35.55974888801575 and batch: 300, loss is 5.710909109115601 and perplexity is 302.1456268056205
At time: 36.05462431907654 and batch: 350, loss is 5.729397258758545 and perplexity is 307.7836985291775
At time: 36.548099517822266 and batch: 400, loss is 5.7142760276794435 and perplexity is 303.1646410318132
At time: 37.04125213623047 and batch: 450, loss is 5.759760227203369 and perplexity is 317.2722465431261
At time: 37.53351378440857 and batch: 500, loss is 5.780521392822266 and perplexity is 323.928040149963
At time: 38.0261709690094 and batch: 550, loss is 5.711944665908813 and perplexity is 302.45867782507764
At time: 38.51948809623718 and batch: 600, loss is 5.667986469268799 and perplexity is 289.4511284935885
At time: 39.01050877571106 and batch: 650, loss is 5.622928171157837 and perplexity is 276.6984184883033
At time: 39.50323820114136 and batch: 700, loss is 5.722174997329712 and perplexity is 305.5688120669598
At time: 39.99565076828003 and batch: 750, loss is 5.59420654296875 and perplexity is 268.86423321860485
At time: 40.50086283683777 and batch: 800, loss is 5.6908683013916015 and perplexity is 296.1506570594429
At time: 40.99333453178406 and batch: 850, loss is 5.658838014602662 and perplexity is 286.8151738069681
At time: 41.48525357246399 and batch: 900, loss is 5.684563398361206 and perplexity is 294.2893297941749
At time: 41.978227376937866 and batch: 950, loss is 5.6412748527526855 and perplexity is 281.821770817511
At time: 42.47100067138672 and batch: 1000, loss is 5.6219441604614255 and perplexity is 276.4262782012662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.770885560570694 and perplexity of 320.821713947571
Finished 4 epochs...
Completing Train Step...
At time: 43.926044940948486 and batch: 50, loss is 5.722315616607666 and perplexity is 305.61178395394506
At time: 44.43027138710022 and batch: 100, loss is 5.710246667861939 and perplexity is 301.9455393582176
At time: 44.924501180648804 and batch: 150, loss is 5.658209075927735 and perplexity is 286.6348413665725
At time: 45.41420555114746 and batch: 200, loss is 5.669831171035766 and perplexity is 289.9855722949277
At time: 45.90334963798523 and batch: 250, loss is 5.718168611526489 and perplexity is 304.3470346065648
At time: 46.39174509048462 and batch: 300, loss is 5.645337896347046 and perplexity is 282.9691543143182
At time: 46.88118886947632 and batch: 350, loss is 5.658969430923462 and perplexity is 286.8528684786529
At time: 47.36863565444946 and batch: 400, loss is 5.63476529121399 and perplexity is 279.9931927352765
At time: 47.85826921463013 and batch: 450, loss is 5.6757362365722654 and perplexity is 291.7030219397318
At time: 48.34847283363342 and batch: 500, loss is 5.715251359939575 and perplexity is 303.46047152940537
At time: 48.83680868148804 and batch: 550, loss is 5.647516565322876 and perplexity is 283.5863224903979
At time: 49.326629877090454 and batch: 600, loss is 5.614426164627075 and perplexity is 274.3558988962619
At time: 49.814998149871826 and batch: 650, loss is 5.565477457046509 and perplexity is 261.24990954677185
At time: 50.30493974685669 and batch: 700, loss is 5.666997385025025 and perplexity is 289.16497847961597
At time: 50.794756174087524 and batch: 750, loss is 5.555667877197266 and perplexity is 258.69968645715653
At time: 51.298003911972046 and batch: 800, loss is 5.660618181228638 and perplexity is 287.32620733466314
At time: 51.79353976249695 and batch: 850, loss is 5.605879468917847 and perplexity is 272.0210543279626
At time: 52.2935254573822 and batch: 900, loss is 5.64234691619873 and perplexity is 282.124063645897
At time: 52.792925119400024 and batch: 950, loss is 5.607027788162231 and perplexity is 272.33360075672
At time: 53.299044370651245 and batch: 1000, loss is 5.585809116363525 and perplexity is 266.61591879304314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.752875351324314 and perplexity of 315.09536887103087
Finished 5 epochs...
Completing Train Step...
At time: 54.75287365913391 and batch: 50, loss is 5.681348886489868 and perplexity is 293.3448520806353
At time: 55.260857820510864 and batch: 100, loss is 5.680271291732788 and perplexity is 293.028915462411
At time: 55.75515913963318 and batch: 150, loss is 5.624253625869751 and perplexity is 277.06541287435795
At time: 56.24996042251587 and batch: 200, loss is 5.631109046936035 and perplexity is 278.971338439289
At time: 56.744938373565674 and batch: 250, loss is 5.686954355239868 and perplexity is 294.993804740568
At time: 57.24175763130188 and batch: 300, loss is 5.603166856765747 and perplexity is 271.28416660750946
At time: 57.739497900009155 and batch: 350, loss is 5.618846588134765 and perplexity is 275.5713525920834
At time: 58.23637413978577 and batch: 400, loss is 5.608931064605713 and perplexity is 272.8524204558477
At time: 58.735732316970825 and batch: 450, loss is 5.653904886245727 and perplexity is 285.4037619401271
At time: 59.23584532737732 and batch: 500, loss is 5.69882872581482 and perplexity is 298.51755022108546
At time: 59.7362380027771 and batch: 550, loss is 5.618602914810181 and perplexity is 275.50421138502975
At time: 60.233723640441895 and batch: 600, loss is 5.5904370880126955 and perplexity is 267.85266932222254
At time: 60.73179054260254 and batch: 650, loss is 5.537956609725952 and perplexity is 254.15812424631565
At time: 61.231765270233154 and batch: 700, loss is 5.64712532043457 and perplexity is 283.47539249314235
At time: 61.73206353187561 and batch: 750, loss is 5.530517663955688 and perplexity is 252.27447062798097
At time: 62.229501724243164 and batch: 800, loss is 5.64032904624939 and perplexity is 281.5553479659949
At time: 62.729252576828 and batch: 850, loss is 5.580286598205566 and perplexity is 265.147585720128
At time: 63.22946548461914 and batch: 900, loss is 5.612149715423584 and perplexity is 273.73205197606796
At time: 63.741162061691284 and batch: 950, loss is 5.5697646331787105 and perplexity is 262.37233822907433
At time: 64.25297141075134 and batch: 1000, loss is 5.543130912780762 and perplexity is 255.47662362095315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.724443389148247 and perplexity of 306.2627486226947
Finished 6 epochs...
Completing Train Step...
At time: 65.71268486976624 and batch: 50, loss is 5.655620288848877 and perplexity is 285.89376445194307
At time: 66.21984124183655 and batch: 100, loss is 5.637726049423218 and perplexity is 280.8234133139102
At time: 66.71663284301758 and batch: 150, loss is 5.587111263275147 and perplexity is 266.96331802161774
At time: 67.21457314491272 and batch: 200, loss is 5.609350271224976 and perplexity is 272.9668259746847
At time: 67.70833849906921 and batch: 250, loss is 5.660841083526611 and perplexity is 287.39026014504003
At time: 68.20363521575928 and batch: 300, loss is 5.579214181900024 and perplexity is 264.86338954137204
At time: 68.69981741905212 and batch: 350, loss is 5.602716913223267 and perplexity is 271.16213150507923
At time: 69.19388771057129 and batch: 400, loss is 5.571680774688721 and perplexity is 262.87556272813947
At time: 69.69225835800171 and batch: 450, loss is 5.615358457565308 and perplexity is 274.61179823140185
At time: 70.1885883808136 and batch: 500, loss is 5.655393953323364 and perplexity is 285.8290638588226
At time: 70.68270540237427 and batch: 550, loss is 5.5814547443389895 and perplexity is 265.45749782324475
At time: 71.17900466918945 and batch: 600, loss is 5.549622440338135 and perplexity is 257.14045171361454
At time: 71.67560005187988 and batch: 650, loss is 5.50620756149292 and perplexity is 246.21559670822276
At time: 72.17318105697632 and batch: 700, loss is 5.612337551116943 and perplexity is 273.78347345508894
At time: 72.67038178443909 and batch: 750, loss is 5.495653066635132 and perplexity is 243.63058122213388
At time: 73.16868710517883 and batch: 800, loss is 5.602203121185303 and perplexity is 271.0228463457816
At time: 73.66603779792786 and batch: 850, loss is 5.549098691940308 and perplexity is 257.00581007626164
At time: 74.16230368614197 and batch: 900, loss is 5.583522500991822 and perplexity is 266.0069672191357
At time: 74.65833067893982 and batch: 950, loss is 5.546468935012817 and perplexity is 256.3308351657709
At time: 75.15532755851746 and batch: 1000, loss is 5.515396728515625 and perplexity is 248.48854018451522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.712078187523819 and perplexity of 302.49906529245317
Finished 7 epochs...
Completing Train Step...
At time: 76.63400268554688 and batch: 50, loss is 5.632307796478272 and perplexity is 279.3059557246287
At time: 77.13084816932678 and batch: 100, loss is 5.614224996566772 and perplexity is 274.30071280327394
At time: 77.62653279304504 and batch: 150, loss is 5.562535171508789 and perplexity is 260.48236743393136
At time: 78.12007784843445 and batch: 200, loss is 5.57905351638794 and perplexity is 264.820838547589
At time: 78.6238477230072 and batch: 250, loss is 5.6304998779296875 and perplexity is 278.80144949704527
At time: 79.11617088317871 and batch: 300, loss is 5.545877704620361 and perplexity is 256.1793293773239
At time: 79.61100935935974 and batch: 350, loss is 5.564239997863769 and perplexity is 260.92682339140686
At time: 80.10633587837219 and batch: 400, loss is 5.553896923065185 and perplexity is 258.2419466162219
At time: 80.60467410087585 and batch: 450, loss is 5.598985385894776 and perplexity is 270.1521681258102
At time: 81.10257124900818 and batch: 500, loss is 5.637297601699829 and perplexity is 280.7031209331399
At time: 81.59659099578857 and batch: 550, loss is 5.56349850654602 and perplexity is 260.73342012957534
At time: 82.08933448791504 and batch: 600, loss is 5.530300588607788 and perplexity is 252.21971400287472
At time: 82.58212041854858 and batch: 650, loss is 5.483432607650757 and perplexity is 240.67142166750685
At time: 83.07734251022339 and batch: 700, loss is 5.589577388763428 and perplexity is 267.6224955377832
At time: 83.57586407661438 and batch: 750, loss is 5.474981498718262 and perplexity is 238.64605163197837
At time: 84.07767581939697 and batch: 800, loss is 5.587715826034546 and perplexity is 267.12476289868215
At time: 84.57928109169006 and batch: 850, loss is 5.526654310226441 and perplexity is 251.30172535052637
At time: 85.07999873161316 and batch: 900, loss is 5.568723039627075 and perplexity is 262.0991951701667
At time: 85.57954525947571 and batch: 950, loss is 5.532327146530151 and perplexity is 252.73137013774868
At time: 86.07919669151306 and batch: 1000, loss is 5.4975560188293455 and perplexity is 244.09463997201772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.715683727729611 and perplexity of 303.59170643167164
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 87.53728365898132 and batch: 50, loss is 5.538190698623657 and perplexity is 254.2176268056358
At time: 88.04772448539734 and batch: 100, loss is 5.447052612304687 and perplexity is 232.0731473570075
At time: 88.54347586631775 and batch: 150, loss is 5.403543863296509 and perplexity is 222.19244223795084
At time: 89.0392918586731 and batch: 200, loss is 5.4099861431121825 and perplexity is 223.62848886419312
At time: 89.53588724136353 and batch: 250, loss is 5.461557178497315 and perplexity is 235.46379819993078
At time: 90.03323864936829 and batch: 300, loss is 5.390342950820923 and perplexity is 219.2785744272853
At time: 90.5281913280487 and batch: 350, loss is 5.3800443840026855 and perplexity is 217.0319079559663
At time: 91.02407741546631 and batch: 400, loss is 5.379273099899292 and perplexity is 216.86457923271786
At time: 91.53683805465698 and batch: 450, loss is 5.41442120552063 and perplexity is 224.6224977850845
At time: 92.04194045066833 and batch: 500, loss is 5.448440217971802 and perplexity is 232.39539689740326
At time: 92.54895424842834 and batch: 550, loss is 5.394072208404541 and perplexity is 220.0978474043811
At time: 93.06101393699646 and batch: 600, loss is 5.32782205581665 and perplexity is 205.98885302917682
At time: 93.57274079322815 and batch: 650, loss is 5.307829675674438 and perplexity is 201.91153898653045
At time: 94.07206106185913 and batch: 700, loss is 5.3973157787323 and perplexity is 220.81290930128574
At time: 94.57468128204346 and batch: 750, loss is 5.303906316757202 and perplexity is 201.12091950570004
At time: 95.08270955085754 and batch: 800, loss is 5.376756792068481 and perplexity is 216.31956719037996
At time: 95.58060002326965 and batch: 850, loss is 5.346419343948364 and perplexity is 209.85553049519064
At time: 96.07668924331665 and batch: 900, loss is 5.371958885192871 and perplexity is 215.28417190293732
At time: 96.57559251785278 and batch: 950, loss is 5.328114461898804 and perplexity is 206.04909422967592
At time: 97.07225060462952 and batch: 1000, loss is 5.286354846954346 and perplexity is 197.62174937704242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.495264565072408 and perplexity of 243.5359487442248
Finished 9 epochs...
Completing Train Step...
At time: 98.52652597427368 and batch: 50, loss is 5.428115377426147 and perplexity is 227.7196751219603
At time: 99.03042459487915 and batch: 100, loss is 5.4019807434082034 and perplexity is 221.84540011733986
At time: 99.51980066299438 and batch: 150, loss is 5.366478490829468 and perplexity is 214.10755684292258
At time: 100.00960993766785 and batch: 200, loss is 5.375783748626709 and perplexity is 216.10918122813845
At time: 100.49951457977295 and batch: 250, loss is 5.425653305053711 and perplexity is 227.15970243069876
At time: 100.99029207229614 and batch: 300, loss is 5.359817552566528 and perplexity is 212.68613886931095
At time: 101.47977566719055 and batch: 350, loss is 5.352629976272583 and perplexity is 211.16292169694583
At time: 101.97124981880188 and batch: 400, loss is 5.350356760025025 and perplexity is 210.6834478927573
At time: 102.46465039253235 and batch: 450, loss is 5.391352109909057 and perplexity is 219.4999730879476
At time: 102.95858836174011 and batch: 500, loss is 5.430786256790161 and perplexity is 228.32869985677698
At time: 103.4513590335846 and batch: 550, loss is 5.378288888931275 and perplexity is 216.65124373605144
At time: 103.9570825099945 and batch: 600, loss is 5.308300008773804 and perplexity is 202.00652700271266
At time: 104.45263385772705 and batch: 650, loss is 5.293045473098755 and perplexity is 198.94839571859467
At time: 104.94972205162048 and batch: 700, loss is 5.383851022720337 and perplexity is 217.85964446715698
At time: 105.4463472366333 and batch: 750, loss is 5.284299983978271 and perplexity is 197.21608070049575
At time: 105.9449348449707 and batch: 800, loss is 5.360833702087402 and perplexity is 212.9023696302056
At time: 106.44514060020447 and batch: 850, loss is 5.331365251541138 and perplexity is 206.72000639734347
At time: 106.94456434249878 and batch: 900, loss is 5.356749000549317 and perplexity is 212.03450069299015
At time: 107.44295763969421 and batch: 950, loss is 5.321687211990357 and perplexity is 204.72901200117323
At time: 107.93986201286316 and batch: 1000, loss is 5.277985286712647 and perplexity is 195.974644626677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.485274152057927 and perplexity of 241.11503712145444
Finished 10 epochs...
Completing Train Step...
At time: 109.40402913093567 and batch: 50, loss is 5.41283353805542 and perplexity is 224.26615490524773
At time: 109.90257239341736 and batch: 100, loss is 5.389361486434937 and perplexity is 219.06346589382483
At time: 110.40125179290771 and batch: 150, loss is 5.351051330566406 and perplexity is 210.82983324081047
At time: 110.89876198768616 and batch: 200, loss is 5.3608127880096434 and perplexity is 212.89791702005348
At time: 111.39651322364807 and batch: 250, loss is 5.409281768798828 and perplexity is 223.471026163754
At time: 111.89171504974365 and batch: 300, loss is 5.3476041889190675 and perplexity is 210.10432412690793
At time: 112.38790464401245 and batch: 350, loss is 5.342194595336914 and perplexity is 208.97081380283993
At time: 112.88293170928955 and batch: 400, loss is 5.33899299621582 and perplexity is 208.30284288715436
At time: 113.38304805755615 and batch: 450, loss is 5.378485994338989 and perplexity is 216.69395107656553
At time: 113.88293719291687 and batch: 500, loss is 5.417389001846313 and perplexity is 225.29012180470747
At time: 114.38162589073181 and batch: 550, loss is 5.365838432312012 and perplexity is 213.9705593254047
At time: 114.88131642341614 and batch: 600, loss is 5.292900905609131 and perplexity is 198.91963632734752
At time: 115.3796796798706 and batch: 650, loss is 5.281089735031128 and perplexity is 196.58398312320878
At time: 115.87850499153137 and batch: 700, loss is 5.3723407745361325 and perplexity is 215.36640233442324
At time: 116.39061212539673 and batch: 750, loss is 5.274633712768555 and perplexity is 195.3189205816703
At time: 116.89223074913025 and batch: 800, loss is 5.351841344833374 and perplexity is 210.99645762612772
At time: 117.39391779899597 and batch: 850, loss is 5.321607084274292 and perplexity is 204.71260819023806
At time: 117.89338898658752 and batch: 900, loss is 5.34601734161377 and perplexity is 209.7711850366756
At time: 118.39426255226135 and batch: 950, loss is 5.309095115661621 and perplexity is 202.16720765439783
At time: 118.89592909812927 and batch: 1000, loss is 5.26774905204773 and perplexity is 193.97883439365302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.480260709436928 and perplexity of 239.90924582611194
Finished 11 epochs...
Completing Train Step...
At time: 120.36070275306702 and batch: 50, loss is 5.3997798347473145 and perplexity is 221.35767557031116
At time: 120.86614632606506 and batch: 100, loss is 5.377596254348755 and perplexity is 216.50123554872013
At time: 121.35882878303528 and batch: 150, loss is 5.337938842773437 and perplexity is 208.08337542478054
At time: 121.85128402709961 and batch: 200, loss is 5.348399362564087 and perplexity is 210.27145999036844
At time: 122.34487509727478 and batch: 250, loss is 5.39870475769043 and perplexity is 221.1198268876797
At time: 122.84639191627502 and batch: 300, loss is 5.335501585006714 and perplexity is 207.5768401311842
At time: 123.34520721435547 and batch: 350, loss is 5.3292432880401615 and perplexity is 206.2818191619266
At time: 123.84305381774902 and batch: 400, loss is 5.3233438491821286 and perplexity is 205.06845478582795
At time: 124.3426742553711 and batch: 450, loss is 5.3667952346801755 and perplexity is 214.17538483642554
At time: 124.84266495704651 and batch: 500, loss is 5.4052894401550295 and perplexity is 222.58063493478386
At time: 125.34137630462646 and batch: 550, loss is 5.354110660552979 and perplexity is 211.47581890952998
At time: 125.83742046356201 and batch: 600, loss is 5.283967189788818 and perplexity is 197.15045925459552
At time: 126.33358860015869 and batch: 650, loss is 5.272446660995484 and perplexity is 194.89221477402913
At time: 126.82975387573242 and batch: 700, loss is 5.364485502243042 and perplexity is 213.68126786150617
At time: 127.32599401473999 and batch: 750, loss is 5.265564212799072 and perplexity is 193.55548446703452
At time: 127.82472038269043 and batch: 800, loss is 5.342991418838501 and perplexity is 209.13739301672626
At time: 128.32654738426208 and batch: 850, loss is 5.308142309188843 and perplexity is 201.97467316897908
At time: 128.82578325271606 and batch: 900, loss is 5.336146821975708 and perplexity is 207.71081960194937
At time: 129.33466744422913 and batch: 950, loss is 5.2983193111419675 and perplexity is 200.00038891916432
At time: 129.8281433582306 and batch: 1000, loss is 5.253282880783081 and perplexity is 191.192902819421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.469655851038491 and perplexity of 237.37848514303064
Finished 12 epochs...
Completing Train Step...
At time: 131.28086185455322 and batch: 50, loss is 5.386740970611572 and perplexity is 218.49015812425012
At time: 131.78793478012085 and batch: 100, loss is 5.361954822540283 and perplexity is 213.1411926809533
At time: 132.28296494483948 and batch: 150, loss is 5.326572160720826 and perplexity is 205.7315494067465
At time: 132.7862298488617 and batch: 200, loss is 5.332696590423584 and perplexity is 206.99540406277214
At time: 133.28963661193848 and batch: 250, loss is 5.383545179367065 and perplexity is 217.7930237312245
At time: 133.78467512130737 and batch: 300, loss is 5.319969005584717 and perplexity is 204.37754733222246
At time: 134.27886295318604 and batch: 350, loss is 5.317356872558594 and perplexity is 203.84438264274385
At time: 134.77331733703613 and batch: 400, loss is 5.312838296890259 and perplexity is 202.9253742433599
At time: 135.26963996887207 and batch: 450, loss is 5.356415195465088 and perplexity is 211.96373431037313
At time: 135.76450443267822 and batch: 500, loss is 5.396241798400879 and perplexity is 220.57588788072653
At time: 136.25901794433594 and batch: 550, loss is 5.346975402832031 and perplexity is 209.97225497704412
At time: 136.7534852027893 and batch: 600, loss is 5.273966598510742 and perplexity is 195.18866399777258
At time: 137.2482831478119 and batch: 650, loss is 5.264441146850586 and perplexity is 193.33823091115693
At time: 137.7420928478241 and batch: 700, loss is 5.356509475708008 and perplexity is 211.98371924481162
At time: 138.23574542999268 and batch: 750, loss is 5.258428478240967 and perplexity is 192.1792400049076
At time: 138.73159646987915 and batch: 800, loss is 5.332465362548828 and perplexity is 206.9475464886223
At time: 139.22777652740479 and batch: 850, loss is 5.302200927734375 and perplexity is 200.77822239631524
At time: 139.72367143630981 and batch: 900, loss is 5.3260273838043215 and perplexity is 205.61950213078896
At time: 140.21979928016663 and batch: 950, loss is 5.290245237350464 and perplexity is 198.39207259031966
At time: 140.71784949302673 and batch: 1000, loss is 5.244750814437866 and perplexity is 189.56857159247286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.468385649890434 and perplexity of 237.07715813214935
Finished 13 epochs...
Completing Train Step...
At time: 142.18186330795288 and batch: 50, loss is 5.378130426406861 and perplexity is 216.6169153530045
At time: 142.67820286750793 and batch: 100, loss is 5.351011533737182 and perplexity is 210.82144304889428
At time: 143.17303919792175 and batch: 150, loss is 5.317179508209229 and perplexity is 203.80823112253495
At time: 143.66726994514465 and batch: 200, loss is 5.3273020362854 and perplexity is 205.88176264933966
At time: 144.16193008422852 and batch: 250, loss is 5.3796268081665035 and perplexity is 216.94129959477112
At time: 144.65781426429749 and batch: 300, loss is 5.313342142105102 and perplexity is 203.0276429837869
At time: 145.15349888801575 and batch: 350, loss is 5.303102741241455 and perplexity is 200.95936857695935
At time: 145.64894080162048 and batch: 400, loss is 5.301342973709106 and perplexity is 200.60603778602572
At time: 146.14578199386597 and batch: 450, loss is 5.3457630920410155 and perplexity is 209.7178575820324
At time: 146.64295482635498 and batch: 500, loss is 5.387244529724121 and perplexity is 218.60020854049543
At time: 147.13843202590942 and batch: 550, loss is 5.336201343536377 and perplexity is 207.72214462872813
At time: 147.63334131240845 and batch: 600, loss is 5.263624811172486 and perplexity is 193.18046641847673
At time: 148.12793517112732 and batch: 650, loss is 5.252440299987793 and perplexity is 191.03187520022644
At time: 148.62291026115417 and batch: 700, loss is 5.348134746551514 and perplexity is 210.2158261561942
At time: 149.11934161186218 and batch: 750, loss is 5.248369808197022 and perplexity is 190.25586196966427
At time: 149.61613869667053 and batch: 800, loss is 5.322586708068847 and perplexity is 204.9132477918852
At time: 150.1134889125824 and batch: 850, loss is 5.289231014251709 and perplexity is 198.19096077105436
At time: 150.61053800582886 and batch: 900, loss is 5.316355209350586 and perplexity is 203.6403014518652
At time: 151.10768604278564 and batch: 950, loss is 5.277114219665528 and perplexity is 195.80401189873413
At time: 151.60356378555298 and batch: 1000, loss is 5.232283411026001 and perplexity is 187.2198156038181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.459208790848895 and perplexity of 234.911486699171
Finished 14 epochs...
Completing Train Step...
At time: 153.06842470169067 and batch: 50, loss is 5.365177011489868 and perplexity is 213.82908153547686
At time: 153.57377910614014 and batch: 100, loss is 5.3371308898925784 and perplexity is 207.91532176101015
At time: 154.08356881141663 and batch: 150, loss is 5.30226411819458 and perplexity is 200.79091006545337
At time: 154.5870909690857 and batch: 200, loss is 5.313193016052246 and perplexity is 202.99736853017896
At time: 155.08023929595947 and batch: 250, loss is 5.368020849227905 and perplexity is 214.43804222932877
At time: 155.57336020469666 and batch: 300, loss is 5.299686260223389 and perplexity is 200.27396620759345
At time: 156.06486582756042 and batch: 350, loss is 5.291545906066895 and perplexity is 198.6502828393202
At time: 156.5603404045105 and batch: 400, loss is 5.290458745956421 and perplexity is 198.43443552743622
At time: 157.05881524085999 and batch: 450, loss is 5.336585206985474 and perplexity is 207.80189687362562
At time: 157.5577895641327 and batch: 500, loss is 5.378353023529053 and perplexity is 216.66513902200614
At time: 158.05359601974487 and batch: 550, loss is 5.325043716430664 and perplexity is 205.41734038141473
At time: 158.55026483535767 and batch: 600, loss is 5.25329065322876 and perplexity is 191.19438886164747
At time: 159.0449459552765 and batch: 650, loss is 5.244697923660278 and perplexity is 189.55854542846328
At time: 159.54050755500793 and batch: 700, loss is 5.33778341293335 and perplexity is 208.05103557236757
At time: 160.0377118587494 and batch: 750, loss is 5.238685274124146 and perplexity is 188.42221592712306
At time: 160.53699803352356 and batch: 800, loss is 5.31478214263916 and perplexity is 203.32021349840448
At time: 161.05353808403015 and batch: 850, loss is 5.283587942123413 and perplexity is 197.07570457935367
At time: 161.55536317825317 and batch: 900, loss is 5.3093188285827635 and perplexity is 202.21244013033746
At time: 162.05479431152344 and batch: 950, loss is 5.267170238494873 and perplexity is 193.86658930296215
At time: 162.5538136959076 and batch: 1000, loss is 5.223939504623413 and perplexity is 185.66417009141227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.455366181164253 and perplexity of 234.01054563635512
Finished 15 epochs...
Completing Train Step...
At time: 164.01959443092346 and batch: 50, loss is 5.3574269580841065 and perplexity is 212.17829981971326
At time: 164.543842792511 and batch: 100, loss is 5.329598236083984 and perplexity is 206.35505148618057
At time: 165.04183387756348 and batch: 150, loss is 5.29367226600647 and perplexity is 199.0731342505621
At time: 165.5380392074585 and batch: 200, loss is 5.307930278778076 and perplexity is 201.93185293581882
At time: 166.0343804359436 and batch: 250, loss is 5.361481971740723 and perplexity is 213.04043252171329
At time: 166.5296745300293 and batch: 300, loss is 5.292156352996826 and perplexity is 198.77158531520453
At time: 167.02453875541687 and batch: 350, loss is 5.283350658416748 and perplexity is 197.02894727327038
At time: 167.5323543548584 and batch: 400, loss is 5.28388072013855 and perplexity is 197.1334124603593
At time: 168.02853679656982 and batch: 450, loss is 5.328026618957519 and perplexity is 206.03099506614328
At time: 168.52545309066772 and batch: 500, loss is 5.37049054145813 and perplexity is 214.96829270430968
At time: 169.0219283103943 and batch: 550, loss is 5.315643482208252 and perplexity is 203.49541668737442
At time: 169.51972699165344 and batch: 600, loss is 5.247863121032715 and perplexity is 190.15948618471865
At time: 170.01681208610535 and batch: 650, loss is 5.235808563232422 and perplexity is 187.88095858003854
At time: 170.5124123096466 and batch: 700, loss is 5.329080743789673 and perplexity is 206.248291963142
At time: 171.0093014240265 and batch: 750, loss is 5.2306947422027585 and perplexity is 186.92262145372212
At time: 171.51994228363037 and batch: 800, loss is 5.306810417175293 and perplexity is 201.70584378044433
At time: 172.01798391342163 and batch: 850, loss is 5.27381142616272 and perplexity is 195.15837846427212
At time: 172.5134632587433 and batch: 900, loss is 5.298869333267212 and perplexity is 200.11042381616727
At time: 173.00961303710938 and batch: 950, loss is 5.257908611297608 and perplexity is 192.07935833566765
At time: 173.50526452064514 and batch: 1000, loss is 5.216485471725464 and perplexity is 184.28536846008538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.45012348454173 and perplexity of 232.78690971778465
Finished 16 epochs...
Completing Train Step...
At time: 174.9648082256317 and batch: 50, loss is 5.349103384017944 and perplexity is 210.4195477316924
At time: 175.4538402557373 and batch: 100, loss is 5.320519514083863 and perplexity is 204.49008988403673
At time: 175.94253396987915 and batch: 150, loss is 5.286892700195312 and perplexity is 197.72806946516923
At time: 176.42965364456177 and batch: 200, loss is 5.297064476013183 and perplexity is 199.74957880096224
At time: 176.91596865653992 and batch: 250, loss is 5.351594476699829 and perplexity is 210.94437575339097
At time: 177.402987241745 and batch: 300, loss is 5.2814992237091065 and perplexity is 196.66449852251677
At time: 177.89317178726196 and batch: 350, loss is 5.273242254257202 and perplexity is 195.0473314035553
At time: 178.382666349411 and batch: 400, loss is 5.275094366073608 and perplexity is 195.40891561463891
At time: 178.87256264686584 and batch: 450, loss is 5.3201744556427 and perplexity is 204.41954102482788
At time: 179.36178874969482 and batch: 500, loss is 5.362291049957276 and perplexity is 213.21286864265966
At time: 179.8643786907196 and batch: 550, loss is 5.306174726486206 and perplexity is 201.57766199991735
At time: 180.35562229156494 and batch: 600, loss is 5.239447059631348 and perplexity is 188.5658079266593
At time: 180.8460192680359 and batch: 650, loss is 5.228222942352295 and perplexity is 186.46115670516727
At time: 181.35030055046082 and batch: 700, loss is 5.320068140029907 and perplexity is 204.39780919129396
At time: 181.87150692939758 and batch: 750, loss is 5.221379270553589 and perplexity is 185.1894343342449
At time: 182.37095308303833 and batch: 800, loss is 5.298755264282226 and perplexity is 200.08759872507974
At time: 182.86967873573303 and batch: 850, loss is 5.2619196224212645 and perplexity is 192.8513379530125
At time: 183.3678493499756 and batch: 900, loss is 5.285873107910156 and perplexity is 197.52657019199188
At time: 183.86385917663574 and batch: 950, loss is 5.243610897064209 and perplexity is 189.35260220123195
At time: 184.35989809036255 and batch: 1000, loss is 5.198387327194214 and perplexity is 180.98014465216448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.429969229349276 and perplexity of 228.14222523126867
Finished 17 epochs...
Completing Train Step...
At time: 185.82581877708435 and batch: 50, loss is 5.315151529312134 and perplexity is 203.39533114849226
At time: 186.33428859710693 and batch: 100, loss is 5.273593282699585 and perplexity is 195.11581058285546
At time: 186.83046197891235 and batch: 150, loss is 5.240182752609253 and perplexity is 188.7045855100113
At time: 187.32441234588623 and batch: 200, loss is 5.242116832733155 and perplexity is 189.06990846619024
At time: 187.8158197402954 and batch: 250, loss is 5.293665075302124 and perplexity is 199.0717027796572
At time: 188.3118770122528 and batch: 300, loss is 5.224059829711914 and perplexity is 185.6865114931986
At time: 188.8183274269104 and batch: 350, loss is 5.2187814521789555 and perplexity is 184.70897016846993
At time: 189.3159408569336 and batch: 400, loss is 5.218925151824951 and perplexity is 184.73551468926883
At time: 189.8138782978058 and batch: 450, loss is 5.260222778320313 and perplexity is 192.52437677736842
At time: 190.31457567214966 and batch: 500, loss is 5.300450162887573 and perplexity is 200.42701447349268
At time: 190.8199782371521 and batch: 550, loss is 5.2442755603790285 and perplexity is 189.4784997645917
At time: 191.31569743156433 and batch: 600, loss is 5.177006826400757 and perplexity is 177.15177063891412
At time: 191.80881929397583 and batch: 650, loss is 5.1604402637481686 and perplexity is 174.2411507829476
At time: 192.30252265930176 and batch: 700, loss is 5.25470874786377 and perplexity is 191.4657129348733
At time: 192.81086945533752 and batch: 750, loss is 5.162792186737061 and perplexity is 174.65143484021297
At time: 193.31996655464172 and batch: 800, loss is 5.238174877166748 and perplexity is 188.32607033970254
At time: 193.82884001731873 and batch: 850, loss is 5.199237871170044 and perplexity is 181.13414170529228
At time: 194.3278362751007 and batch: 900, loss is 5.224737339019775 and perplexity is 185.8123584595112
At time: 194.8269340991974 and batch: 950, loss is 5.181593675613403 and perplexity is 177.9662055156545
At time: 195.32355785369873 and batch: 1000, loss is 5.144113683700562 and perplexity is 171.41948543732894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.383530035251525 and perplexity of 217.78972547348394
Finished 18 epochs...
Completing Train Step...
At time: 196.79943323135376 and batch: 50, loss is 5.271702537536621 and perplexity is 194.74724484951628
At time: 197.32738590240479 and batch: 100, loss is 5.233380327224731 and perplexity is 187.42529272726694
At time: 197.83155131340027 and batch: 150, loss is 5.211299352645874 and perplexity is 183.33211657007337
At time: 198.34193229675293 and batch: 200, loss is 5.219476184844971 and perplexity is 184.83733810928976
At time: 198.84166193008423 and batch: 250, loss is 5.273044624328613 and perplexity is 195.00878802216695
At time: 199.33641505241394 and batch: 300, loss is 5.2028265380859375 and perplexity is 181.78533957422036
At time: 199.83151841163635 and batch: 350, loss is 5.198303651809693 and perplexity is 180.96500170252483
At time: 200.32565474510193 and batch: 400, loss is 5.202257213592529 and perplexity is 181.68187418334347
At time: 200.81925106048584 and batch: 450, loss is 5.243995170593262 and perplexity is 189.42537937619056
At time: 201.3289303779602 and batch: 500, loss is 5.286647577285766 and perplexity is 197.67960772526698
At time: 201.84662079811096 and batch: 550, loss is 5.229882974624633 and perplexity is 186.77094530122844
At time: 202.35525131225586 and batch: 600, loss is 5.165065326690674 and perplexity is 175.0488935632111
At time: 202.86375832557678 and batch: 650, loss is 5.147963752746582 and perplexity is 172.08073440237263
At time: 203.38051891326904 and batch: 700, loss is 5.243044519424439 and perplexity is 189.24538748618755
At time: 203.88837432861328 and batch: 750, loss is 5.152570037841797 and perplexity is 172.87521572339185
At time: 204.40312790870667 and batch: 800, loss is 5.225495100021362 and perplexity is 185.9532131787314
At time: 204.92068076133728 and batch: 850, loss is 5.187419929504395 and perplexity is 179.00610824000864
At time: 205.4593861103058 and batch: 900, loss is 5.211228151321411 and perplexity is 183.3190635452588
At time: 205.9760103225708 and batch: 950, loss is 5.165413780212402 and perplexity is 175.10990059508813
At time: 206.49108028411865 and batch: 1000, loss is 5.13442982673645 and perplexity is 169.76749538704954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.374684403582317 and perplexity of 215.87173321327106
Finished 19 epochs...
Completing Train Step...
At time: 208.0023272037506 and batch: 50, loss is 5.260234022140503 and perplexity is 192.526541499013
At time: 208.52934622764587 and batch: 100, loss is 5.2220464611053465 and perplexity is 185.31303220219797
At time: 209.03309297561646 and batch: 150, loss is 5.200948162078857 and perplexity is 181.4441988495052
At time: 209.54115891456604 and batch: 200, loss is 5.2097712326049805 and perplexity is 183.05217703368626
At time: 210.043776512146 and batch: 250, loss is 5.261328935623169 and perplexity is 192.7374568510315
At time: 210.54420971870422 and batch: 300, loss is 5.194256362915039 and perplexity is 180.23406421616153
At time: 211.05709505081177 and batch: 350, loss is 5.187943487167359 and perplexity is 179.09985279789453
At time: 211.56860184669495 and batch: 400, loss is 5.192153091430664 and perplexity is 179.85538142434143
At time: 212.0830864906311 and batch: 450, loss is 5.235320272445679 and perplexity is 187.78924043334482
At time: 212.58400082588196 and batch: 500, loss is 5.278720560073853 and perplexity is 196.11879254991265
At time: 213.08794975280762 and batch: 550, loss is 5.220264434814453 and perplexity is 184.98309357373688
At time: 213.58665490150452 and batch: 600, loss is 5.157936801910401 and perplexity is 173.80549026866495
At time: 214.08442902565002 and batch: 650, loss is 5.137553625106811 and perplexity is 170.29864398111883
At time: 214.5854296684265 and batch: 700, loss is 5.231850748062134 and perplexity is 187.13883004449377
At time: 215.0877468585968 and batch: 750, loss is 5.139527034759522 and perplexity is 170.63504478812277
At time: 215.59394669532776 and batch: 800, loss is 5.215956249237061 and perplexity is 184.1878663012559
At time: 216.09223198890686 and batch: 850, loss is 5.176623439788818 and perplexity is 177.08386603945874
At time: 216.6086766719818 and batch: 900, loss is 5.199907732009888 and perplexity is 181.25551702132643
At time: 217.12037444114685 and batch: 950, loss is 5.159863681793213 and perplexity is 174.14071543698194
At time: 217.6383056640625 and batch: 1000, loss is 5.124243955612183 and perplexity is 168.0470425881947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3685150146484375 and perplexity of 214.5440362817859
Finished 20 epochs...
Completing Train Step...
At time: 219.1241331100464 and batch: 50, loss is 5.251418504714966 and perplexity is 190.83677942413286
At time: 219.6307246685028 and batch: 100, loss is 5.213120021820068 and perplexity is 183.66620774581676
At time: 220.13568687438965 and batch: 150, loss is 5.190624761581421 and perplexity is 179.58071302178672
At time: 220.63633513450623 and batch: 200, loss is 5.200528383255005 and perplexity is 181.3680484014083
At time: 221.14313125610352 and batch: 250, loss is 5.251752576828003 and perplexity is 190.90054332055703
At time: 221.64768028259277 and batch: 300, loss is 5.181445560455322 and perplexity is 177.9398479750154
At time: 222.14059782028198 and batch: 350, loss is 5.175845670700073 and perplexity is 176.9461892296616
At time: 222.6582305431366 and batch: 400, loss is 5.181958179473877 and perplexity is 178.03108670860325
At time: 223.1718397140503 and batch: 450, loss is 5.2246933269500735 and perplexity is 185.80418065300168
At time: 223.6816794872284 and batch: 500, loss is 5.269926195144653 and perplexity is 194.4016141329015
At time: 224.1836769580841 and batch: 550, loss is 5.211862173080444 and perplexity is 183.43532867380807
At time: 224.6821894645691 and batch: 600, loss is 5.146322584152221 and perplexity is 171.7985525227328
At time: 225.19380235671997 and batch: 650, loss is 5.131239404678345 and perplexity is 169.22672852098245
At time: 225.6979570388794 and batch: 700, loss is 5.224768505096436 and perplexity is 185.81814959196242
At time: 226.20021390914917 and batch: 750, loss is 5.133139896392822 and perplexity is 169.54864832246804
At time: 226.7087104320526 and batch: 800, loss is 5.208780136108398 and perplexity is 182.87084453618107
At time: 227.2325723171234 and batch: 850, loss is 5.16938817024231 and perplexity is 175.8072404705571
At time: 227.73612689971924 and batch: 900, loss is 5.190228776931763 and perplexity is 179.50961589367017
At time: 228.23461961746216 and batch: 950, loss is 5.15068868637085 and perplexity is 172.5502824347019
At time: 228.73219895362854 and batch: 1000, loss is 5.115904188156128 and perplexity is 166.65139709986266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.364776239162538 and perplexity of 213.74340192697392
Finished 21 epochs...
Completing Train Step...
At time: 230.22939252853394 and batch: 50, loss is 5.242104215621948 and perplexity is 189.0675229651784
At time: 230.74392461776733 and batch: 100, loss is 5.201251153945923 and perplexity is 181.49918329558213
At time: 231.2523741722107 and batch: 150, loss is 5.182100687026978 and perplexity is 178.05645929099526
At time: 231.78120708465576 and batch: 200, loss is 5.192275228500367 and perplexity is 179.8773497751465
At time: 232.2871789932251 and batch: 250, loss is 5.246724252700806 and perplexity is 189.9430428415196
At time: 232.7976906299591 and batch: 300, loss is 5.173754138946533 and perplexity is 176.57648741253666
At time: 233.30497884750366 and batch: 350, loss is 5.170283336639404 and perplexity is 175.96468766479717
At time: 233.81248545646667 and batch: 400, loss is 5.17358154296875 and perplexity is 176.54601365093825
At time: 234.32584476470947 and batch: 450, loss is 5.2179797267913814 and perplexity is 184.5609436440134
At time: 234.834538936615 and batch: 500, loss is 5.262767324447632 and perplexity is 193.01488773392933
At time: 235.33241391181946 and batch: 550, loss is 5.202838630676269 and perplexity is 181.7875378431515
At time: 235.84924054145813 and batch: 600, loss is 5.139371652603149 and perplexity is 170.60853320667752
At time: 236.35933876037598 and batch: 650, loss is 5.121222305297851 and perplexity is 167.54002958290698
At time: 236.8547191619873 and batch: 700, loss is 5.2162068080902095 and perplexity is 184.23402198391628
At time: 237.35289549827576 and batch: 750, loss is 5.125956420898437 and perplexity is 168.33506385791165
At time: 237.86560201644897 and batch: 800, loss is 5.201436233520508 and perplexity is 181.53277819598307
At time: 238.37421798706055 and batch: 850, loss is 5.162624235153198 and perplexity is 174.62210431823044
At time: 238.87226819992065 and batch: 900, loss is 5.183276224136352 and perplexity is 178.26589434167477
At time: 239.3742229938507 and batch: 950, loss is 5.146119060516358 and perplexity is 171.76359101455458
At time: 239.86929988861084 and batch: 1000, loss is 5.110278015136719 and perplexity is 165.71642014321452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.363596846417683 and perplexity of 213.4914631060926
Finished 22 epochs...
Completing Train Step...
At time: 241.35665488243103 and batch: 50, loss is 5.2377002239227295 and perplexity is 188.23670197065354
At time: 241.8540608882904 and batch: 100, loss is 5.19489839553833 and perplexity is 180.34981751993922
At time: 242.37526607513428 and batch: 150, loss is 5.175152015686035 and perplexity is 176.82349217791986
At time: 242.88811683654785 and batch: 200, loss is 5.185789489746094 and perplexity is 178.71448736389303
At time: 243.38110613822937 and batch: 250, loss is 5.241128845214844 and perplexity is 188.8832020035521
At time: 243.8872034549713 and batch: 300, loss is 5.168576498031616 and perplexity is 175.66460051530336
At time: 244.41248679161072 and batch: 350, loss is 5.165185289382935 and perplexity is 175.06989415937906
At time: 244.92160153388977 and batch: 400, loss is 5.167571668624878 and perplexity is 175.4881762119911
At time: 245.42420935630798 and batch: 450, loss is 5.208737831115723 and perplexity is 182.86310835008325
At time: 245.92588305473328 and batch: 500, loss is 5.254575414657593 and perplexity is 191.44018589933324
At time: 246.44060444831848 and batch: 550, loss is 5.198211727142334 and perplexity is 180.94836731950647
At time: 246.94242453575134 and batch: 600, loss is 5.134323234558106 and perplexity is 169.74940046430993
At time: 247.4427764415741 and batch: 650, loss is 5.1138170719146725 and perplexity is 166.30393898118456
At time: 247.94585609436035 and batch: 700, loss is 5.207870845794678 and perplexity is 182.7046374252922
At time: 248.4435110092163 and batch: 750, loss is 5.1210970783233645 and perplexity is 167.51905036550625
At time: 248.96539878845215 and batch: 800, loss is 5.196216287612915 and perplexity is 180.587655803272
At time: 249.48671317100525 and batch: 850, loss is 5.156780242919922 and perplexity is 173.60459016510478
At time: 249.98473954200745 and batch: 900, loss is 5.1782785987854005 and perplexity is 177.377210692535
At time: 250.48754286766052 and batch: 950, loss is 5.137893171310425 and perplexity is 170.35647805727797
At time: 250.98845148086548 and batch: 1000, loss is 5.103091859817505 and perplexity is 164.52982484457252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3593861649676064 and perplexity of 212.594408493636
Finished 23 epochs...
Completing Train Step...
At time: 252.55300378799438 and batch: 50, loss is 5.233052244186402 and perplexity is 187.36381175375294
At time: 253.06886172294617 and batch: 100, loss is 5.187708311080932 and perplexity is 179.05773774785442
At time: 253.5695617198944 and batch: 150, loss is 5.170877265930176 and perplexity is 176.06922928903606
At time: 254.0657558441162 and batch: 200, loss is 5.177480430603027 and perplexity is 177.23569033271946
At time: 254.57237362861633 and batch: 250, loss is 5.231083478927612 and perplexity is 186.99529926673938
At time: 255.0698275566101 and batch: 300, loss is 5.160498065948486 and perplexity is 174.25122259593243
At time: 255.5926489830017 and batch: 350, loss is 5.155755968093872 and perplexity is 173.42686239029317
At time: 256.1049122810364 and batch: 400, loss is 5.15852879524231 and perplexity is 173.9084124215615
At time: 256.62020564079285 and batch: 450, loss is 5.204907484054566 and perplexity is 182.16401891273375
At time: 257.13696026802063 and batch: 500, loss is 5.249029741287232 and perplexity is 190.38145954702492
At time: 257.66563630104065 and batch: 550, loss is 5.192189531326294 and perplexity is 179.86193545508237
At time: 258.16388750076294 and batch: 600, loss is 5.127184753417969 and perplexity is 168.5419623350755
At time: 258.67196106910706 and batch: 650, loss is 5.107644586563111 and perplexity is 165.2805919004376
At time: 259.1745502948761 and batch: 700, loss is 5.20260256767273 and perplexity is 181.74462959568638
At time: 259.6679837703705 and batch: 750, loss is 5.113182792663574 and perplexity is 166.198489289124
At time: 260.17798686027527 and batch: 800, loss is 5.190523557662964 and perplexity is 179.56253966957232
At time: 260.6820206642151 and batch: 850, loss is 5.152808208465576 and perplexity is 172.9163944249404
At time: 261.19673323631287 and batch: 900, loss is 5.169440383911133 and perplexity is 175.8164202512407
At time: 261.7069959640503 and batch: 950, loss is 5.131946668624878 and perplexity is 169.34645882032615
At time: 262.21572971343994 and batch: 1000, loss is 5.098231468200684 and perplexity is 163.7320856959584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.356426704220656 and perplexity of 211.96617376321814
Finished 24 epochs...
Completing Train Step...
At time: 263.7878098487854 and batch: 50, loss is 5.225525283813477 and perplexity is 185.95882603656926
At time: 264.2948296070099 and batch: 100, loss is 5.18073431968689 and perplexity is 177.81333489676953
At time: 264.81261563301086 and batch: 150, loss is 5.1617334365844725 and perplexity is 174.46662046031656
At time: 265.31767320632935 and batch: 200, loss is 5.174866104125977 and perplexity is 176.7729435239829
At time: 265.8210151195526 and batch: 250, loss is 5.225060148239136 and perplexity is 185.87235008429687
At time: 266.3144853115082 and batch: 300, loss is 5.1537966060638425 and perplexity is 173.08738906531607
At time: 266.8370418548584 and batch: 350, loss is 5.148945121765137 and perplexity is 172.2496919951187
At time: 267.3472743034363 and batch: 400, loss is 5.149721717834472 and perplexity is 172.38351238433484
At time: 267.8543562889099 and batch: 450, loss is 5.1946008682250975 and perplexity is 180.29616650500742
At time: 268.3543620109558 and batch: 500, loss is 5.239215383529663 and perplexity is 188.52212679550018
At time: 268.8516902923584 and batch: 550, loss is 5.184178600311279 and perplexity is 178.42682983878925
At time: 269.36820816993713 and batch: 600, loss is 5.118000545501709 and perplexity is 167.00112442897273
At time: 269.8727607727051 and batch: 650, loss is 5.0985713386535645 and perplexity is 163.78774285165062
At time: 270.39055609703064 and batch: 700, loss is 5.193094921112061 and perplexity is 180.02485435571677
At time: 270.8865373134613 and batch: 750, loss is 5.103666620254517 and perplexity is 164.62441725998656
At time: 271.38538241386414 and batch: 800, loss is 5.1801395511627195 and perplexity is 177.70760856645867
At time: 271.8846392631531 and batch: 850, loss is 5.140913991928101 and perplexity is 170.87187248367306
At time: 272.39929580688477 and batch: 900, loss is 5.157511644363403 and perplexity is 173.73161125898972
At time: 272.9146957397461 and batch: 950, loss is 5.118904113769531 and perplexity is 167.1520895390678
At time: 273.41193437576294 and batch: 1000, loss is 5.085182723999023 and perplexity is 161.609466467844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.346178008288872 and perplexity of 209.80489098314632
Finished 25 epochs...
Completing Train Step...
At time: 274.9987242221832 and batch: 50, loss is 5.213640279769898 and perplexity is 183.76178641113592
At time: 275.49821615219116 and batch: 100, loss is 5.167762432098389 and perplexity is 175.52165613931749
At time: 275.9926075935364 and batch: 150, loss is 5.146709156036377 and perplexity is 171.86497785113173
At time: 276.48366117477417 and batch: 200, loss is 5.1611245918273925 and perplexity is 174.360429703287
At time: 276.9945058822632 and batch: 250, loss is 5.209332304000855 and perplexity is 182.97184782782142
At time: 277.4925105571747 and batch: 300, loss is 5.1389276313781735 and perplexity is 170.53279621243266
At time: 277.9868621826172 and batch: 350, loss is 5.135571880340576 and perplexity is 169.9614897218991
At time: 278.48350739479065 and batch: 400, loss is 5.135716066360474 and perplexity is 169.98599755943934
At time: 278.9854738712311 and batch: 450, loss is 5.1799149990081785 and perplexity is 177.66770842007634
At time: 279.5014274120331 and batch: 500, loss is 5.223759641647339 and perplexity is 185.6307789842313
At time: 280.0159022808075 and batch: 550, loss is 5.166968002319336 and perplexity is 175.38227188164487
At time: 280.53227162361145 and batch: 600, loss is 5.105688886642456 and perplexity is 164.95766853349917
At time: 281.03096413612366 and batch: 650, loss is 5.083375902175903 and perplexity is 161.31773059374592
At time: 281.5414309501648 and batch: 700, loss is 5.177797145843506 and perplexity is 177.29183246707015
At time: 282.0544488430023 and batch: 750, loss is 5.094785966873169 and perplexity is 163.16891733314375
At time: 282.5615453720093 and batch: 800, loss is 5.168315105438232 and perplexity is 175.61868909052356
At time: 283.09033942222595 and batch: 850, loss is 5.128630409240722 and perplexity is 168.78579220887656
At time: 283.6218032836914 and batch: 900, loss is 5.1450040149688725 and perplexity is 171.57217352661542
At time: 284.1399028301239 and batch: 950, loss is 5.106817951202393 and perplexity is 165.14402157343994
At time: 284.6387448310852 and batch: 1000, loss is 5.076168718338013 and perplexity is 160.15926370951226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.340327565262958 and perplexity of 208.5810229976965
Finished 26 epochs...
Completing Train Step...
At time: 286.1733009815216 and batch: 50, loss is 5.200302658081054 and perplexity is 181.32711368730534
At time: 286.7071657180786 and batch: 100, loss is 5.157985963821411 and perplexity is 173.8140350887488
At time: 287.21185851097107 and batch: 150, loss is 5.135249109268188 and perplexity is 169.90663992203747
At time: 287.70870184898376 and batch: 200, loss is 5.148999090194702 and perplexity is 172.25898829133976
At time: 288.20580434799194 and batch: 250, loss is 5.19743145942688 and perplexity is 180.80723421828455
At time: 288.7027208805084 and batch: 300, loss is 5.127740592956543 and perplexity is 168.63567066263434
At time: 289.21114683151245 and batch: 350, loss is 5.125328369140625 and perplexity is 168.22937391800465
At time: 289.7235200405121 and batch: 400, loss is 5.125451221466064 and perplexity is 168.2500425573668
At time: 290.2378053665161 and batch: 450, loss is 5.169047765731811 and perplexity is 175.74740507761925
At time: 290.75239610671997 and batch: 500, loss is 5.213301906585693 and perplexity is 183.69961686917898
At time: 291.25832414627075 and batch: 550, loss is 5.155865430831909 and perplexity is 173.44584720854579
At time: 291.7633967399597 and batch: 600, loss is 5.09246657371521 and perplexity is 162.79090301406572
At time: 292.2721803188324 and batch: 650, loss is 5.072821054458618 and perplexity is 159.62400076753423
At time: 292.7889196872711 and batch: 700, loss is 5.166962852478028 and perplexity is 175.38136869310205
At time: 293.29573822021484 and batch: 750, loss is 5.082408666610718 and perplexity is 161.16177378306634
At time: 293.8059115409851 and batch: 800, loss is 5.155118112564087 and perplexity is 173.31627637980282
At time: 294.30367255210876 and batch: 850, loss is 5.1164517498016355 and perplexity is 166.74267400066927
At time: 294.81461906433105 and batch: 900, loss is 5.128341245651245 and perplexity is 168.7369925592296
At time: 295.31195425987244 and batch: 950, loss is 5.093077478408813 and perplexity is 162.89038312413206
At time: 295.82256269454956 and batch: 1000, loss is 5.057717542648316 and perplexity is 157.23123287138438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.329430184713224 and perplexity of 206.3203761506154
Finished 27 epochs...
Completing Train Step...
At time: 297.3966827392578 and batch: 50, loss is 5.187288112640381 and perplexity is 178.9825137712898
At time: 297.91300654411316 and batch: 100, loss is 5.140695343017578 and perplexity is 170.8345156190832
At time: 298.4170649051666 and batch: 150, loss is 5.122232894897461 and perplexity is 167.7094293767395
At time: 298.92505145072937 and batch: 200, loss is 5.134252347946167 and perplexity is 169.7373679309102
At time: 299.4386200904846 and batch: 250, loss is 5.18305474281311 and perplexity is 178.2264161475097
At time: 299.93900752067566 and batch: 300, loss is 5.1115809917449955 and perplexity is 165.9324854957499
At time: 300.44432067871094 and batch: 350, loss is 5.110345735549926 and perplexity is 165.72764290766276
At time: 300.9505286216736 and batch: 400, loss is 5.109076461791992 and perplexity is 165.51742260133793
At time: 301.46408319473267 and batch: 450, loss is 5.154194469451904 and perplexity is 173.15626790163537
At time: 301.97830605506897 and batch: 500, loss is 5.199451608657837 and perplexity is 181.17286099942885
At time: 302.48372411727905 and batch: 550, loss is 5.142440462112427 and perplexity is 171.13290247892613
At time: 302.98778009414673 and batch: 600, loss is 5.074587011337281 and perplexity is 159.9061389182702
At time: 303.50312995910645 and batch: 650, loss is 5.057656135559082 and perplexity is 157.22157805547718
At time: 304.01732063293457 and batch: 700, loss is 5.148907461166382 and perplexity is 172.24320509073388
At time: 304.52561807632446 and batch: 750, loss is 5.065525121688843 and perplexity is 158.46363291965014
At time: 305.03821992874146 and batch: 800, loss is 5.1390096473693845 and perplexity is 170.54678320231855
At time: 305.5373582839966 and batch: 850, loss is 5.100654001235962 and perplexity is 164.12921291517853
At time: 306.0464653968811 and batch: 900, loss is 5.11303786277771 and perplexity is 166.1744039064286
At time: 306.55172085762024 and batch: 950, loss is 5.0808794307708744 and perplexity is 160.9155077699836
At time: 307.04883098602295 and batch: 1000, loss is 5.041489820480347 and perplexity is 154.7003191311644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.3150575219131095 and perplexity of 203.3762113811512
Finished 28 epochs...
Completing Train Step...
At time: 308.58787059783936 and batch: 50, loss is 5.172864732742309 and perplexity is 176.4195090082433
At time: 309.09252071380615 and batch: 100, loss is 5.123863906860351 and perplexity is 167.98318865398264
At time: 309.6109538078308 and batch: 150, loss is 5.107785472869873 and perplexity is 165.30387931301019
At time: 310.1050353050232 and batch: 200, loss is 5.120857496261596 and perplexity is 167.47892061341054
At time: 310.60757994651794 and batch: 250, loss is 5.168440647125244 and perplexity is 175.64073794101887
At time: 311.12379479408264 and batch: 300, loss is 5.097746801376343 and perplexity is 163.65274941332655
At time: 311.6359188556671 and batch: 350, loss is 5.094982604980469 and perplexity is 163.2010057150143
At time: 312.13761162757874 and batch: 400, loss is 5.094678344726563 and perplexity is 163.15135768892122
At time: 312.63593339920044 and batch: 450, loss is 5.141271677017212 and perplexity is 170.9330017364683
At time: 313.13386273384094 and batch: 500, loss is 5.183902807235718 and perplexity is 178.3776277397364
At time: 313.6420774459839 and batch: 550, loss is 5.127993850708008 and perplexity is 168.67838436196263
At time: 314.1457815170288 and batch: 600, loss is 5.063236980438233 and perplexity is 158.1014602535693
At time: 314.64251255989075 and batch: 650, loss is 5.047430229187012 and perplexity is 155.6220372344789
At time: 315.1544201374054 and batch: 700, loss is 5.141072263717652 and perplexity is 170.89891882099457
At time: 315.66665267944336 and batch: 750, loss is 5.056691255569458 and perplexity is 157.06995126346672
At time: 316.1738963127136 and batch: 800, loss is 5.1302704238891605 and perplexity is 169.06283049187388
At time: 316.6865050792694 and batch: 850, loss is 5.092177572250367 and perplexity is 162.74386300227496
At time: 317.1893196105957 and batch: 900, loss is 5.101831140518189 and perplexity is 164.32252961708127
At time: 317.69074392318726 and batch: 950, loss is 5.070769720077514 and perplexity is 159.2968941839057
At time: 318.19815945625305 and batch: 1000, loss is 5.032402267456055 and perplexity is 153.30084032825448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.313610169945694 and perplexity of 203.08206733781142
Finished 29 epochs...
Completing Train Step...
At time: 319.81526017189026 and batch: 50, loss is 5.166796875 and perplexity is 175.3522617514486
At time: 320.32099413871765 and batch: 100, loss is 5.1134135532379155 and perplexity is 166.236845773379
At time: 320.8200788497925 and batch: 150, loss is 5.095764646530151 and perplexity is 163.32868560141415
At time: 321.31263160705566 and batch: 200, loss is 5.108717470169068 and perplexity is 165.45801389742752
At time: 321.8143792152405 and batch: 250, loss is 5.157347068786621 and perplexity is 173.70302163150333
At time: 322.33052015304565 and batch: 300, loss is 5.087381982803345 and perplexity is 161.96527862793386
At time: 322.8399713039398 and batch: 350, loss is 5.084296684265137 and perplexity is 161.46633747753143
At time: 323.3332326412201 and batch: 400, loss is 5.083351840972901 and perplexity is 161.3138491417786
At time: 323.83012866973877 and batch: 450, loss is 5.129997806549072 and perplexity is 169.01674731453943
At time: 324.32566833496094 and batch: 500, loss is 5.176685314178467 and perplexity is 177.09482333457112
At time: 324.8424940109253 and batch: 550, loss is 5.119815864562988 and perplexity is 167.304560086342
At time: 325.36016488075256 and batch: 600, loss is 5.053074254989624 and perplexity is 156.50285537002162
At time: 325.86588048934937 and batch: 650, loss is 5.0360524559021 and perplexity is 153.8614398088167
At time: 326.3658187389374 and batch: 700, loss is 5.127754001617432 and perplexity is 168.63793185631582
At time: 326.8834204673767 and batch: 750, loss is 5.047257499694824 and perplexity is 155.5951590404094
At time: 327.3905475139618 and batch: 800, loss is 5.122726707458496 and perplexity is 167.79226685097908
At time: 327.8974857330322 and batch: 850, loss is 5.084247188568115 and perplexity is 161.45834578639122
At time: 328.4045658111572 and batch: 900, loss is 5.090293827056885 and perplexity is 162.4375835993217
At time: 328.9140782356262 and batch: 950, loss is 5.0577538871765135 and perplexity is 157.2369474702076
At time: 329.43376564979553 and batch: 1000, loss is 5.018013935089112 and perplexity is 151.11088953633106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.305616797470465 and perplexity of 201.4652273415582
Finished 30 epochs...
Completing Train Step...
At time: 330.9245035648346 and batch: 50, loss is 5.1510090637207036 and perplexity is 172.60557249327317
At time: 331.4464318752289 and batch: 100, loss is 5.1017997932434085 and perplexity is 164.3173786343278
At time: 331.9454815387726 and batch: 150, loss is 5.082338485717774 and perplexity is 161.1504637027542
At time: 332.4481325149536 and batch: 200, loss is 5.096255512237549 and perplexity is 163.40887773238356
At time: 332.9691319465637 and batch: 250, loss is 5.147606773376465 and perplexity is 172.01931609338317
At time: 333.46925497055054 and batch: 300, loss is 5.075823202133178 and perplexity is 160.10393564746767
At time: 333.9776248931885 and batch: 350, loss is 5.070990924835205 and perplexity is 159.33213531239343
At time: 334.481742143631 and batch: 400, loss is 5.070373573303223 and perplexity is 159.23380173088103
At time: 334.98256611824036 and batch: 450, loss is 5.1149273777008055 and perplexity is 166.48868975283773
At time: 335.49440479278564 and batch: 500, loss is 5.164225854873657 and perplexity is 174.9020066128185
At time: 335.99741554260254 and batch: 550, loss is 5.102892303466797 and perplexity is 164.49699514893996
At time: 336.4984631538391 and batch: 600, loss is 5.037960119247437 and perplexity is 154.15523578060595
At time: 337.013685464859 and batch: 650, loss is 5.019356451034546 and perplexity is 151.31389455330796
At time: 337.51701164245605 and batch: 700, loss is 5.113155536651611 and perplexity is 166.19395944284477
At time: 338.0321261882782 and batch: 750, loss is 5.028571891784668 and perplexity is 152.71476368248605
At time: 338.5346169471741 and batch: 800, loss is 5.106864995956421 and perplexity is 165.1517909160661
At time: 339.0307385921478 and batch: 850, loss is 5.066788082122803 and perplexity is 158.66389265168917
At time: 339.5277724266052 and batch: 900, loss is 5.069630556106567 and perplexity is 159.11553222148692
At time: 340.02969121932983 and batch: 950, loss is 5.040689907073975 and perplexity is 154.57662175211297
At time: 340.530481338501 and batch: 1000, loss is 5.003281126022339 and perplexity is 148.90092114773995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.290015988233613 and perplexity of 198.3465965957529
Finished 31 epochs...
Completing Train Step...
At time: 342.0602443218231 and batch: 50, loss is 5.133710584640503 and perplexity is 169.645435358461
At time: 342.55794072151184 and batch: 100, loss is 5.080439910888672 and perplexity is 160.84479774533318
At time: 343.05369210243225 and batch: 150, loss is 5.062645311355591 and perplexity is 158.00794417563088
At time: 343.5681800842285 and batch: 200, loss is 5.083510227203369 and perplexity is 161.339401057749
At time: 344.07940554618835 and batch: 250, loss is 5.130952444076538 and perplexity is 169.17817408406265
At time: 344.58787083625793 and batch: 300, loss is 5.056784934997559 and perplexity is 157.0846661759045
At time: 345.09197998046875 and batch: 350, loss is 5.05647494316101 and perplexity is 157.03597875848385
At time: 345.6009142398834 and batch: 400, loss is 5.051990022659302 and perplexity is 156.33326187043335
At time: 346.1056056022644 and batch: 450, loss is 5.098688583374024 and perplexity is 163.80694722555964
At time: 346.6192343235016 and batch: 500, loss is 5.144232015609742 and perplexity is 171.43977103250427
At time: 347.1444299221039 and batch: 550, loss is 5.080916948318482 and perplexity is 160.92154503845816
At time: 347.65102910995483 and batch: 600, loss is 5.015754947662353 and perplexity is 150.76991720792273
At time: 348.1931834220886 and batch: 650, loss is 5.001786432266235 and perplexity is 148.67852611825793
At time: 348.70171761512756 and batch: 700, loss is 5.09576340675354 and perplexity is 163.32848311045535
At time: 349.2174484729767 and batch: 750, loss is 5.012133111953736 and perplexity is 150.2248410220358
At time: 349.7403197288513 and batch: 800, loss is 5.091631336212158 and perplexity is 162.65499071413893
At time: 350.2527120113373 and batch: 850, loss is 5.050952453613281 and perplexity is 156.17113943820615
At time: 350.76598262786865 and batch: 900, loss is 5.058460350036621 and perplexity is 157.34806878074983
At time: 351.2740170955658 and batch: 950, loss is 5.024490070343018 and perplexity is 152.0926797670878
At time: 351.7730393409729 and batch: 1000, loss is 4.986708307266236 and perplexity is 146.45354911120504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.278674614138719 and perplexity of 196.1097818955947
Finished 32 epochs...
Completing Train Step...
At time: 353.2611916065216 and batch: 50, loss is 5.116152706146241 and perplexity is 166.69281811684021
At time: 353.78176712989807 and batch: 100, loss is 5.068518009185791 and perplexity is 158.93860716302999
At time: 354.2894570827484 and batch: 150, loss is 5.051861944198609 and perplexity is 156.3132402290957
At time: 354.7965443134308 and batch: 200, loss is 5.067420578002929 and perplexity is 158.76427865363593
At time: 355.2927212715149 and batch: 250, loss is 5.118163185119629 and perplexity is 167.02828763688908
At time: 355.80307626724243 and batch: 300, loss is 5.043570499420166 and perplexity is 155.0225359257675
At time: 356.30525517463684 and batch: 350, loss is 5.042212867736817 and perplexity is 154.81221522069254
At time: 356.8033285140991 and batch: 400, loss is 5.036577243804931 and perplexity is 153.9422056217478
At time: 357.3123950958252 and batch: 450, loss is 5.085154390335083 and perplexity is 161.60488754440064
At time: 357.8183562755585 and batch: 500, loss is 5.130823945999145 and perplexity is 169.15643641061055
At time: 358.3168704509735 and batch: 550, loss is 5.068116722106933 and perplexity is 158.87483994897906
At time: 358.81354999542236 and batch: 600, loss is 5.002368936538696 and perplexity is 148.76515722399003
At time: 359.31364393234253 and batch: 650, loss is 4.989985790252685 and perplexity is 146.93433558108
At time: 359.82588601112366 and batch: 700, loss is 5.084400768280029 and perplexity is 161.48314441685773
At time: 360.33385848999023 and batch: 750, loss is 4.998887434005737 and perplexity is 148.24813148773677
At time: 360.84744477272034 and batch: 800, loss is 5.081177139282227 and perplexity is 160.96342081796254
At time: 361.3759808540344 and batch: 850, loss is 5.036970205307007 and perplexity is 154.0027108694397
At time: 361.8834652900696 and batch: 900, loss is 5.04624153137207 and perplexity is 155.43715956247325
At time: 362.3924195766449 and batch: 950, loss is 5.0153655910491945 and perplexity is 150.71122537036143
At time: 362.89733505249023 and batch: 1000, loss is 4.975131416320801 and perplexity is 144.7678487562011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.278998398199314 and perplexity of 196.17328939790292
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 364.3830637931824 and batch: 50, loss is 5.079849672317505 and perplexity is 160.7498889539024
At time: 364.9024622440338 and batch: 100, loss is 5.006704597473145 and perplexity is 149.41155276795135
At time: 365.41671657562256 and batch: 150, loss is 4.981059045791626 and perplexity is 145.62852729452854
At time: 365.9215180873871 and batch: 200, loss is 4.985483226776123 and perplexity is 146.2742415809853
At time: 366.4176404476166 and batch: 250, loss is 5.037601346969605 and perplexity is 154.09993907558072
At time: 366.9287853240967 and batch: 300, loss is 4.968737344741822 and perplexity is 143.8451458268656
At time: 367.4316031932831 and batch: 350, loss is 4.959550237655639 and perplexity is 142.52967699726935
At time: 367.940881729126 and batch: 400, loss is 4.953330554962158 and perplexity is 141.6459387665406
At time: 368.44210290908813 and batch: 450, loss is 5.0049738025665285 and perplexity is 149.15317567680825
At time: 368.94364643096924 and batch: 500, loss is 5.037510662078858 and perplexity is 154.08596517306202
At time: 369.4489107131958 and batch: 550, loss is 4.9714000797271725 and perplexity is 144.22867772442405
At time: 369.96318197250366 and batch: 600, loss is 4.91152250289917 and perplexity is 135.84608309706084
At time: 370.4692223072052 and batch: 650, loss is 4.898832244873047 and perplexity is 134.13305363284638
At time: 370.966322183609 and batch: 700, loss is 4.987658395767212 and perplexity is 146.59275906455895
At time: 371.47593355178833 and batch: 750, loss is 4.900386257171631 and perplexity is 134.34166009436058
At time: 371.97607469558716 and batch: 800, loss is 4.981782035827637 and perplexity is 145.73385333897383
At time: 372.485773563385 and batch: 850, loss is 4.93942343711853 and perplexity is 139.6896864492543
At time: 372.9830095767975 and batch: 900, loss is 4.9414835453033445 and perplexity is 139.97775894401195
At time: 373.4805097579956 and batch: 950, loss is 4.914234018325805 and perplexity is 136.214931690361
At time: 373.99315547943115 and batch: 1000, loss is 4.868383340835571 and perplexity is 130.11040260262234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2186431884765625 and perplexity of 184.68343338783484
Finished 34 epochs...
Completing Train Step...
At time: 375.4932463169098 and batch: 50, loss is 5.027701711654663 and perplexity is 152.58193213164103
At time: 376.00057315826416 and batch: 100, loss is 4.985118885040283 and perplexity is 146.22095747729986
At time: 376.5086326599121 and batch: 150, loss is 4.954412641525269 and perplexity is 141.79929489093598
At time: 377.01485776901245 and batch: 200, loss is 4.964604940414429 and perplexity is 143.25194603862303
At time: 377.51970648765564 and batch: 250, loss is 5.022107372283935 and perplexity is 151.73072022559913
At time: 378.02530789375305 and batch: 300, loss is 4.953640451431275 and perplexity is 141.68984114507373
At time: 378.54133582115173 and batch: 350, loss is 4.9432582187652585 and perplexity is 140.22639431611063
At time: 379.04678535461426 and batch: 400, loss is 4.936309871673584 and perplexity is 139.25542986247925
At time: 379.5496623516083 and batch: 450, loss is 4.992079401016236 and perplexity is 147.24228113416754
At time: 380.0610182285309 and batch: 500, loss is 5.023394899368286 and perplexity is 151.9262034553893
At time: 380.56441164016724 and batch: 550, loss is 4.961228637695313 and perplexity is 142.7690996801114
At time: 381.0746729373932 and batch: 600, loss is 4.901831254959107 and perplexity is 134.53592381748567
At time: 381.58364033699036 and batch: 650, loss is 4.891318893432617 and perplexity is 133.12904133282944
At time: 382.07948303222656 and batch: 700, loss is 4.980072078704834 and perplexity is 145.48486763652954
At time: 382.5826416015625 and batch: 750, loss is 4.8921852302551265 and perplexity is 133.2444258972132
At time: 383.09456396102905 and batch: 800, loss is 4.9777670574188235 and perplexity is 145.14990811255808
At time: 383.59962916374207 and batch: 850, loss is 4.933789520263672 and perplexity is 138.90489915931605
At time: 384.09719347953796 and batch: 900, loss is 4.9409716796875 and perplexity is 139.90612747663215
At time: 384.606906414032 and batch: 950, loss is 4.91356466293335 and perplexity is 136.12378599913413
At time: 385.101149559021 and batch: 1000, loss is 4.863941745758057 and perplexity is 129.5337863747344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.212128057712462 and perplexity of 183.48410779319335
Finished 35 epochs...
Completing Train Step...
At time: 386.6637771129608 and batch: 50, loss is 5.018809862136841 and perplexity is 151.2312106575849
At time: 387.1855719089508 and batch: 100, loss is 4.9764374828338624 and perplexity is 144.95704872260475
At time: 387.68339109420776 and batch: 150, loss is 4.945051078796387 and perplexity is 140.47802611668544
At time: 388.1884801387787 and batch: 200, loss is 4.956578769683838 and perplexity is 142.10678324582568
At time: 388.700572013855 and batch: 250, loss is 5.015583887100219 and perplexity is 150.74412862689974
At time: 389.23259568214417 and batch: 300, loss is 4.9467731475830075 and perplexity is 140.7201473555465
At time: 389.7333846092224 and batch: 350, loss is 4.935875082015992 and perplexity is 139.19489620246978
At time: 390.2463843822479 and batch: 400, loss is 4.92879503250122 and perplexity is 138.2128699445922
At time: 390.7505223751068 and batch: 450, loss is 4.9842174530029295 and perplexity is 146.0892086119428
At time: 391.2596423625946 and batch: 500, loss is 5.01453652381897 and perplexity is 150.58632741373546
At time: 391.77080726623535 and batch: 550, loss is 4.95571985244751 and perplexity is 141.98477768413736
At time: 392.2947645187378 and batch: 600, loss is 4.89440221786499 and perplexity is 133.5401548310754
At time: 392.8070149421692 and batch: 650, loss is 4.8831181240081785 and perplexity is 132.04174518727456
At time: 393.31634736061096 and batch: 700, loss is 4.9734893226623536 and perplexity is 144.53032146421518
At time: 393.83102011680603 and batch: 750, loss is 4.885189838409424 and perplexity is 132.31558152974966
At time: 394.34988594055176 and batch: 800, loss is 4.973096523284912 and perplexity is 144.47356119235218
At time: 394.8708131313324 and batch: 850, loss is 4.928824014663697 and perplexity is 138.21687571049281
At time: 395.3840572834015 and batch: 900, loss is 4.938139705657959 and perplexity is 139.5104774567638
At time: 395.8959152698517 and batch: 950, loss is 4.911095476150512 and perplexity is 135.78808557000704
At time: 396.41190457344055 and batch: 1000, loss is 4.858256196975708 and perplexity is 128.79940537111514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.210673262433308 and perplexity of 183.21737005082446
Finished 36 epochs...
Completing Train Step...
At time: 397.91360449790955 and batch: 50, loss is 5.011940670013428 and perplexity is 150.19593424367457
At time: 398.4374885559082 and batch: 100, loss is 4.971157636642456 and perplexity is 144.19371471733285
At time: 398.94496178627014 and batch: 150, loss is 4.937628183364868 and perplexity is 139.43913298612415
At time: 399.45856046676636 and batch: 200, loss is 4.949169931411743 and perplexity is 141.05782764077657
At time: 399.9671006202698 and batch: 250, loss is 5.0098051071167 and perplexity is 149.8755236295468
At time: 400.49119782447815 and batch: 300, loss is 4.941227445602417 and perplexity is 139.94191527178262
At time: 400.99494552612305 and batch: 350, loss is 4.929073495864868 and perplexity is 138.25136252439424
At time: 401.5113146305084 and batch: 400, loss is 4.923458623886108 and perplexity is 137.47727406045243
At time: 402.019651889801 and batch: 450, loss is 4.9793175601959225 and perplexity is 145.37513801285306
At time: 402.52968287467957 and batch: 500, loss is 5.0092527961730955 and perplexity is 149.79276859302763
At time: 403.03188133239746 and batch: 550, loss is 4.9493020725250245 and perplexity is 141.07646841073705
At time: 403.541090965271 and batch: 600, loss is 4.887299957275391 and perplexity is 132.5950779161888
At time: 404.053409576416 and batch: 650, loss is 4.877135753631592 and perplexity is 131.25418066256267
At time: 404.5646641254425 and batch: 700, loss is 4.967443675994873 and perplexity is 143.65917817351558
At time: 405.08502411842346 and batch: 750, loss is 4.879392223358154 and perplexity is 131.55068614973166
At time: 405.5925545692444 and batch: 800, loss is 4.970496454238892 and perplexity is 144.09840788153062
At time: 406.0930507183075 and batch: 850, loss is 4.92396918296814 and perplexity is 137.54748225248687
At time: 406.58966064453125 and batch: 900, loss is 4.934828386306763 and perplexity is 139.04927772428093
At time: 407.0982553958893 and batch: 950, loss is 4.905932750701904 and perplexity is 135.08885548458557
At time: 407.59904384613037 and batch: 1000, loss is 4.85075421333313 and perplexity is 127.83676969214375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.207282833936738 and perplexity of 182.59723651158546
Finished 37 epochs...
Completing Train Step...
At time: 409.1087009906769 and batch: 50, loss is 5.006143703460693 and perplexity is 149.3277722208128
At time: 409.60133814811707 and batch: 100, loss is 4.96717945098877 and perplexity is 143.62122484061655
At time: 410.10435581207275 and batch: 150, loss is 4.928975782394409 and perplexity is 138.23785416395145
At time: 410.60159945487976 and batch: 200, loss is 4.941902465820313 and perplexity is 140.03641078352453
At time: 411.09422492980957 and batch: 250, loss is 5.0028879261016845 and perplexity is 148.84238482635197
At time: 411.594042301178 and batch: 300, loss is 4.932908926010132 and perplexity is 138.7826341441635
At time: 412.09002900123596 and batch: 350, loss is 4.921329326629639 and perplexity is 137.18485551145886
At time: 412.5989043712616 and batch: 400, loss is 4.9161435413360595 and perplexity is 136.4752857336719
At time: 413.1368236541748 and batch: 450, loss is 4.973511276245117 and perplexity is 144.53349445741839
At time: 413.6385324001312 and batch: 500, loss is 5.002585201263428 and perplexity is 148.79733335892283
At time: 414.1518762111664 and batch: 550, loss is 4.944084920883179 and perplexity is 140.3423677043979
At time: 414.6746745109558 and batch: 600, loss is 4.878987474441528 and perplexity is 131.49745192600506
At time: 415.1852557659149 and batch: 650, loss is 4.869198799133301 and perplexity is 130.21654548170952
At time: 415.701589345932 and batch: 700, loss is 4.9606006145477295 and perplexity is 142.67946552985933
At time: 416.22132897377014 and batch: 750, loss is 4.871158027648926 and perplexity is 130.47191953719997
At time: 416.7275815010071 and batch: 800, loss is 4.964656829833984 and perplexity is 143.25937949181034
At time: 417.228111743927 and batch: 850, loss is 4.917304801940918 and perplexity is 136.6338611624101
At time: 417.72566199302673 and batch: 900, loss is 4.927108335494995 and perplexity is 137.9799432042746
At time: 418.2394025325775 and batch: 950, loss is 4.898684453964234 and perplexity is 134.11323145175325
At time: 418.74741888046265 and batch: 1000, loss is 4.842423458099365 and perplexity is 126.77621659176516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.201402989829459 and perplexity of 181.52674347669821
Finished 38 epochs...
Completing Train Step...
At time: 420.25005173683167 and batch: 50, loss is 4.998852853775024 and perplexity is 148.24300512178317
At time: 420.7781836986542 and batch: 100, loss is 4.9578648376464844 and perplexity is 142.28965979766406
At time: 421.2848982810974 and batch: 150, loss is 4.919129629135131 and perplexity is 136.88342198134117
At time: 421.7877428531647 and batch: 200, loss is 4.933847713470459 and perplexity is 138.91298271603827
At time: 422.30601477622986 and batch: 250, loss is 4.995552492141724 and perplexity is 147.75455606789728
At time: 422.83254194259644 and batch: 300, loss is 4.92761643409729 and perplexity is 138.0500684343185
At time: 423.3520772457123 and batch: 350, loss is 4.913928327560424 and perplexity is 136.17329840741553
At time: 423.8607347011566 and batch: 400, loss is 4.909157228469849 and perplexity is 135.52514952799262
At time: 424.36051392555237 and batch: 450, loss is 4.965567111968994 and perplexity is 143.38984531698046
At time: 424.86930894851685 and batch: 500, loss is 4.99156852722168 and perplexity is 147.167078122545
At time: 425.3691999912262 and batch: 550, loss is 4.935695743560791 and perplexity is 139.1699354430919
At time: 425.87916254997253 and batch: 600, loss is 4.870001735687256 and perplexity is 130.32114309300982
At time: 426.4089434146881 and batch: 650, loss is 4.857498664855957 and perplexity is 128.7018726312974
At time: 426.9092388153076 and batch: 700, loss is 4.947838935852051 and perplexity is 140.87020518855087
At time: 427.411593914032 and batch: 750, loss is 4.857588815689087 and perplexity is 128.7134757353474
At time: 427.9095823764801 and batch: 800, loss is 4.955287818908691 and perplexity is 141.92344874720882
At time: 428.41850423812866 and batch: 850, loss is 4.897959690093995 and perplexity is 134.01606624224596
At time: 428.9176206588745 and batch: 900, loss is 4.9068936920166015 and perplexity is 135.21873033803814
At time: 429.4336965084076 and batch: 950, loss is 4.878940830230713 and perplexity is 131.49131847418184
At time: 429.93846821784973 and batch: 1000, loss is 4.8204092502594 and perplexity is 124.015833907993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.181705753977706 and perplexity of 177.98615279467828
Finished 39 epochs...
Completing Train Step...
At time: 431.49270153045654 and batch: 50, loss is 4.97692837715149 and perplexity is 145.0282247826551
At time: 432.00757908821106 and batch: 100, loss is 4.930882301330566 and perplexity is 138.50165864482116
At time: 432.5071632862091 and batch: 150, loss is 4.89630970954895 and perplexity is 133.79512466499978
At time: 433.01339840888977 and batch: 200, loss is 4.912894325256348 and perplexity is 136.0325676735987
At time: 433.5057101249695 and batch: 250, loss is 4.975117912292481 and perplexity is 144.76589382027137
At time: 434.00231671333313 and batch: 300, loss is 4.8975754070281985 and perplexity is 133.9645760314861
At time: 434.50126338005066 and batch: 350, loss is 4.884840660095215 and perplexity is 132.26938786343007
At time: 435.00651717185974 and batch: 400, loss is 4.883816394805908 and perplexity is 132.1339782801102
At time: 435.51629424095154 and batch: 450, loss is 4.94641749382019 and perplexity is 140.67010860439783
At time: 436.033545255661 and batch: 500, loss is 4.97085054397583 and perplexity is 144.14944068343314
At time: 436.52895402908325 and batch: 550, loss is 4.915396327972412 and perplexity is 136.3733476658503
At time: 437.03677797317505 and batch: 600, loss is 4.8457207679748535 and perplexity is 127.19492699231928
At time: 437.5414443016052 and batch: 650, loss is 4.83522819519043 and perplexity is 125.8673022429795
At time: 438.0351104736328 and batch: 700, loss is 4.921395025253296 and perplexity is 137.19386866372517
At time: 438.5395028591156 and batch: 750, loss is 4.83882773399353 and perplexity is 126.32118287184707
At time: 439.06229281425476 and batch: 800, loss is 4.933463077545166 and perplexity is 138.8595620668052
At time: 439.5706751346588 and batch: 850, loss is 4.881253776550293 and perplexity is 131.79580282754347
At time: 440.0810248851776 and batch: 900, loss is 4.88445032119751 and perplexity is 132.217768051637
At time: 440.59349751472473 and batch: 950, loss is 4.8614417552948 and perplexity is 129.21035759700916
At time: 441.10567593574524 and batch: 1000, loss is 4.797275619506836 and perplexity is 121.17982743859022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.167168593988186 and perplexity of 175.41745563291408
Finished 40 epochs...
Completing Train Step...
At time: 442.61043787002563 and batch: 50, loss is 4.964223461151123 and perplexity is 143.19730881392786
At time: 443.1135792732239 and batch: 100, loss is 4.910983734130859 and perplexity is 135.772913182793
At time: 443.6345784664154 and batch: 150, loss is 4.884540452957153 and perplexity is 132.22968560879474
At time: 444.1477303504944 and batch: 200, loss is 4.903683376312256 and perplexity is 134.7853315701695
At time: 444.6537375450134 and batch: 250, loss is 4.964411573410034 and perplexity is 143.2242485169236
At time: 445.15237617492676 and batch: 300, loss is 4.879147176742554 and perplexity is 131.51845404865563
At time: 445.66680788993835 and batch: 350, loss is 4.871795387268066 and perplexity is 130.55510357640244
At time: 446.1896240711212 and batch: 400, loss is 4.869516086578369 and perplexity is 130.2578681119603
At time: 446.70348143577576 and batch: 450, loss is 4.934535512924194 and perplexity is 139.00855985485117
At time: 447.2078700065613 and batch: 500, loss is 4.9608000373840335 and perplexity is 142.70792191088944
At time: 447.71290397644043 and batch: 550, loss is 4.905311985015869 and perplexity is 135.00502298139773
At time: 448.23146891593933 and batch: 600, loss is 4.835074853897095 and perplexity is 125.84800306778304
At time: 448.7437903881073 and batch: 650, loss is 4.8248668384552005 and perplexity is 124.56987936113872
At time: 449.2468228340149 and batch: 700, loss is 4.912057266235352 and perplexity is 135.91874802920415
At time: 449.74351620674133 and batch: 750, loss is 4.830841884613037 and perplexity is 125.3164182212466
At time: 450.24722719192505 and batch: 800, loss is 4.92476149559021 and perplexity is 137.65650604356898
At time: 450.7593216896057 and batch: 850, loss is 4.872911205291748 and perplexity is 130.7008606181009
At time: 451.2652266025543 and batch: 900, loss is 4.875879430770874 and perplexity is 131.08938657379724
At time: 451.7757656574249 and batch: 950, loss is 4.853442621231079 and perplexity is 128.18090946019407
At time: 452.31392121315 and batch: 1000, loss is 4.788191385269165 and perplexity is 120.0839864752816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.162888317573361 and perplexity of 174.66822503572
Finished 41 epochs...
Completing Train Step...
At time: 453.82928133010864 and batch: 50, loss is 4.957052001953125 and perplexity is 142.17404867617293
At time: 454.3653333187103 and batch: 100, loss is 4.8995630741119385 and perplexity is 134.231117820066
At time: 454.8657534122467 and batch: 150, loss is 4.877332763671875 and perplexity is 131.280041601331
At time: 455.3596131801605 and batch: 200, loss is 4.8950784969329835 and perplexity is 133.63049578692977
At time: 455.86198925971985 and batch: 250, loss is 4.9543247985839844 and perplexity is 141.78683937087348
At time: 456.39114451408386 and batch: 300, loss is 4.867915277481079 and perplexity is 130.04951694141087
At time: 456.9016649723053 and batch: 350, loss is 4.864225759506225 and perplexity is 129.5705809757561
At time: 457.4130802154541 and batch: 400, loss is 4.861244859695435 and perplexity is 129.18491915064718
At time: 457.9224965572357 and batch: 450, loss is 4.92734956741333 and perplexity is 138.01323238571067
At time: 458.42454195022583 and batch: 500, loss is 4.95253885269165 and perplexity is 141.5338417348033
At time: 458.9229328632355 and batch: 550, loss is 4.898184032440185 and perplexity is 134.04613509370623
At time: 459.435928106308 and batch: 600, loss is 4.824395160675049 and perplexity is 124.51113637193842
At time: 459.93927240371704 and batch: 650, loss is 4.817283267974854 and perplexity is 123.62876790412207
At time: 460.4648141860962 and batch: 700, loss is 4.906268892288208 and perplexity is 135.13427209953895
At time: 460.9717342853546 and batch: 750, loss is 4.826718683242798 and perplexity is 124.80077717039218
At time: 461.47586464881897 and batch: 800, loss is 4.920159635543823 and perplexity is 137.02448541887657
At time: 461.97760033607483 and batch: 850, loss is 4.866648979187012 and perplexity is 129.8849396839057
At time: 462.48101019859314 and batch: 900, loss is 4.866753044128418 and perplexity is 129.89845685586315
At time: 462.9848937988281 and batch: 950, loss is 4.846139249801635 and perplexity is 127.24816689691369
At time: 463.4967257976532 and batch: 1000, loss is 4.779732694625855 and perplexity is 119.07251706671539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.154707745807927 and perplexity of 173.24516773298535
Finished 42 epochs...
Completing Train Step...
At time: 464.97416615486145 and batch: 50, loss is 4.947675552368164 and perplexity is 140.847191203755
At time: 465.482625246048 and batch: 100, loss is 4.891646909713745 and perplexity is 133.17271698864909
At time: 465.98811769485474 and batch: 150, loss is 4.8683846950531 and perplexity is 130.1105788005296
At time: 466.4970850944519 and batch: 200, loss is 4.886573314666748 and perplexity is 132.49876368013034
At time: 467.0064778327942 and batch: 250, loss is 4.950884628295898 and perplexity is 141.29990654497627
At time: 467.50497698783875 and batch: 300, loss is 4.864189910888672 and perplexity is 129.5659361328086
At time: 468.0202715396881 and batch: 350, loss is 4.858976631164551 and perplexity is 128.89223029935826
At time: 468.53476333618164 and batch: 400, loss is 4.853299713134765 and perplexity is 128.16259267927865
At time: 469.04224157333374 and batch: 450, loss is 4.923856287002564 and perplexity is 137.53195457318807
At time: 469.5549566745758 and batch: 500, loss is 4.947507762908936 and perplexity is 140.82356051225503
At time: 470.0743451118469 and batch: 550, loss is 4.892086563110351 and perplexity is 133.23127969871229
At time: 470.58929204940796 and batch: 600, loss is 4.818775691986084 and perplexity is 123.81341219533591
At time: 471.1102499961853 and batch: 650, loss is 4.806521711349487 and perplexity is 122.30546309868659
At time: 471.62275099754333 and batch: 700, loss is 4.901534986495972 and perplexity is 134.49607096996226
At time: 472.13158082962036 and batch: 750, loss is 4.820414571762085 and perplexity is 124.01649386034217
At time: 472.65015053749084 and batch: 800, loss is 4.9178604412078855 and perplexity is 136.70980139660358
At time: 473.15976905822754 and batch: 850, loss is 4.859504117965698 and perplexity is 128.96023718440182
At time: 473.6631956100464 and batch: 900, loss is 4.858969850540161 and perplexity is 128.89135633252084
At time: 474.16705560684204 and batch: 950, loss is 4.842022199630737 and perplexity is 126.72535676588743
At time: 474.6792736053467 and batch: 1000, loss is 4.7725372409820555 and perplexity is 118.21881137313254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.153706248213605 and perplexity of 173.07174996750516
Finished 43 epochs...
Completing Train Step...
At time: 476.2318170070648 and batch: 50, loss is 4.946407270431519 and perplexity is 140.6686704865543
At time: 476.7282636165619 and batch: 100, loss is 4.888953742980957 and perplexity is 132.81454318491717
At time: 477.2352497577667 and batch: 150, loss is 4.864751892089844 and perplexity is 129.63877021700367
At time: 477.7307710647583 and batch: 200, loss is 4.884918575286865 and perplexity is 132.27969405963435
At time: 478.25644397735596 and batch: 250, loss is 4.945103435516358 and perplexity is 140.4853812779053
At time: 478.7502944469452 and batch: 300, loss is 4.856580533981323 and perplexity is 128.58376169735595
At time: 479.2514352798462 and batch: 350, loss is 4.854451093673706 and perplexity is 128.31024157803287
At time: 479.7507920265198 and batch: 400, loss is 4.84807897567749 and perplexity is 127.49523300169069
At time: 480.2645637989044 and batch: 450, loss is 4.919920330047607 and perplexity is 136.99169862958564
At time: 480.7795331478119 and batch: 500, loss is 4.940241193771362 and perplexity is 139.80396533947308
At time: 481.28725814819336 and batch: 550, loss is 4.884429292678833 and perplexity is 132.2149877370652
At time: 481.78637623786926 and batch: 600, loss is 4.815009717941284 and perplexity is 123.34801099308248
At time: 482.2834346294403 and batch: 650, loss is 4.802096796035767 and perplexity is 121.76546738081737
At time: 482.79430890083313 and batch: 700, loss is 4.895647010803223 and perplexity is 133.70648817658858
At time: 483.2974576950073 and batch: 750, loss is 4.818027229309082 and perplexity is 123.7207771486574
At time: 483.8177099227905 and batch: 800, loss is 4.914489536285401 and perplexity is 136.24974149884915
At time: 484.3340468406677 and batch: 850, loss is 4.8566867542266845 and perplexity is 128.5974206214872
At time: 484.8524408340454 and batch: 900, loss is 4.8545208930969235 and perplexity is 128.3191978714563
At time: 485.36805629730225 and batch: 950, loss is 4.8359395694732665 and perplexity is 125.95687286018469
At time: 485.8753774166107 and batch: 1000, loss is 4.767978744506836 and perplexity is 117.68113776088285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.153929175400153 and perplexity of 173.1103366666487
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 487.42819690704346 and batch: 50, loss is 4.933285732269287 and perplexity is 138.83493816299298
At time: 487.9458427429199 and batch: 100, loss is 4.856044034957886 and perplexity is 128.5147951366759
At time: 488.4649348258972 and batch: 150, loss is 4.8395326137542725 and perplexity is 126.41025550606699
At time: 488.960373878479 and batch: 200, loss is 4.851221866607666 and perplexity is 127.89656695720002
At time: 489.4670042991638 and batch: 250, loss is 4.912185935974121 and perplexity is 135.9362377841839
At time: 489.97298216819763 and batch: 300, loss is 4.823491621017456 and perplexity is 124.39868643155486
At time: 490.4764802455902 and batch: 350, loss is 4.811394929885864 and perplexity is 122.9029389814155
At time: 490.97980308532715 and batch: 400, loss is 4.806462068557739 and perplexity is 122.29816867695334
At time: 491.5014007091522 and batch: 450, loss is 4.875939626693725 and perplexity is 131.0972778579072
At time: 492.00495529174805 and batch: 500, loss is 4.900425672531128 and perplexity is 134.34695532354465
At time: 492.51385045051575 and batch: 550, loss is 4.839325284957885 and perplexity is 126.38404973664113
At time: 493.01923727989197 and batch: 600, loss is 4.765951957702637 and perplexity is 117.44286472964977
At time: 493.52850103378296 and batch: 650, loss is 4.75569375038147 and perplexity is 116.244269703739
At time: 494.04359912872314 and batch: 700, loss is 4.847965497970581 and perplexity is 127.4807659558669
At time: 494.5464632511139 and batch: 750, loss is 4.76604775428772 and perplexity is 117.4541158939362
At time: 495.05357789993286 and batch: 800, loss is 4.863398675918579 and perplexity is 129.4634595800699
At time: 495.563818693161 and batch: 850, loss is 4.802388124465942 and perplexity is 121.80094629103567
At time: 496.0863060951233 and batch: 900, loss is 4.8042316818237305 and perplexity is 122.02570043167488
At time: 496.5952377319336 and batch: 950, loss is 4.778486309051513 and perplexity is 118.92419924893785
At time: 497.1020634174347 and batch: 1000, loss is 4.710854568481445 and perplexity is 111.14710214289482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.114570803758575 and perplexity of 166.42933480723812
Finished 45 epochs...
Completing Train Step...
At time: 498.6358904838562 and batch: 50, loss is 4.904266357421875 and perplexity is 134.86393178132164
At time: 499.16197681427 and batch: 100, loss is 4.836886005401611 and perplexity is 126.07613940024815
At time: 499.67743849754333 and batch: 150, loss is 4.824128332138062 and perplexity is 124.47791767962629
At time: 500.17793798446655 and batch: 200, loss is 4.839289608001709 and perplexity is 126.37954081887004
At time: 500.69424986839294 and batch: 250, loss is 4.90179892539978 and perplexity is 134.5315744006628
At time: 501.2007122039795 and batch: 300, loss is 4.813100786209106 and perplexity is 123.11277265919128
At time: 501.7081606388092 and batch: 350, loss is 4.8014626121521 and perplexity is 121.68827016501965
At time: 502.2160630226135 and batch: 400, loss is 4.798062725067139 and perplexity is 121.27524630199633
At time: 502.7231857776642 and batch: 450, loss is 4.867815742492676 and perplexity is 130.03657310844304
At time: 503.2275450229645 and batch: 500, loss is 4.894546661376953 and perplexity is 133.55944523318044
At time: 503.7355935573578 and batch: 550, loss is 4.832378320693969 and perplexity is 125.50910687673058
At time: 504.2616651058197 and batch: 600, loss is 4.76197075843811 and perplexity is 116.97623078067494
At time: 504.778840303421 and batch: 650, loss is 4.753965187072754 and perplexity is 116.04350768919527
At time: 505.292382478714 and batch: 700, loss is 4.845299243927002 and perplexity is 127.14132257039746
At time: 505.79984426498413 and batch: 750, loss is 4.7635869121551515 and perplexity is 117.16543520138775
At time: 506.3221278190613 and batch: 800, loss is 4.861002330780029 and perplexity is 129.15359187135786
At time: 506.8352618217468 and batch: 850, loss is 4.802208013534546 and perplexity is 121.77901058464384
At time: 507.3497643470764 and batch: 900, loss is 4.805642604827881 and perplexity is 122.19799081516263
At time: 507.85931849479675 and batch: 950, loss is 4.777915172576904 and perplexity is 118.8562966936919
At time: 508.3729727268219 and batch: 1000, loss is 4.708824148178101 and perplexity is 110.92165576295179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.112143911966464 and perplexity of 166.02591854244238
Finished 46 epochs...
Completing Train Step...
At time: 509.88622641563416 and batch: 50, loss is 4.897117385864258 and perplexity is 133.90323147007146
At time: 510.3785402774811 and batch: 100, loss is 4.830042715072632 and perplexity is 125.21630916427405
At time: 510.87776160240173 and batch: 150, loss is 4.819600467681885 and perplexity is 123.91557261245207
At time: 511.37363839149475 and batch: 200, loss is 4.834664630889892 and perplexity is 125.79638790910079
At time: 511.8757016658783 and batch: 250, loss is 4.898455848693848 and perplexity is 134.08257596435143
At time: 512.37819480896 and batch: 300, loss is 4.808847074508667 and perplexity is 122.59019864521319
At time: 512.8742876052856 and batch: 350, loss is 4.796353712081909 and perplexity is 121.06816233627546
At time: 513.3848731517792 and batch: 400, loss is 4.794009771347046 and perplexity is 120.78471805694824
At time: 513.894289970398 and batch: 450, loss is 4.862866535186767 and perplexity is 129.3945851270223
At time: 514.3956587314606 and batch: 500, loss is 4.890159749984742 and perplexity is 132.97481507929058
At time: 514.9051899909973 and batch: 550, loss is 4.829590244293213 and perplexity is 125.15966525909286
At time: 515.4098298549652 and batch: 600, loss is 4.760961084365845 and perplexity is 116.85818251854775
At time: 515.919949054718 and batch: 650, loss is 4.752507209777832 and perplexity is 115.87444216655655
At time: 516.4459340572357 and batch: 700, loss is 4.843418560028076 and perplexity is 126.90243463890604
At time: 516.9675116539001 and batch: 750, loss is 4.7621822166442875 and perplexity is 117.00096898005664
At time: 517.4816105365753 and batch: 800, loss is 4.858308324813843 and perplexity is 128.80611958066729
At time: 518.0032544136047 and batch: 850, loss is 4.80015869140625 and perplexity is 121.52970170784205
At time: 518.5168406963348 and batch: 900, loss is 4.805555353164673 and perplexity is 122.18732930234742
At time: 519.0310642719269 and batch: 950, loss is 4.776779861450195 and perplexity is 118.72143438740027
At time: 519.5489690303802 and batch: 1000, loss is 4.706033458709717 and perplexity is 110.61253939091554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.110171806521532 and perplexity of 165.69882056634722
Finished 47 epochs...
Completing Train Step...
At time: 521.0576684474945 and batch: 50, loss is 4.891804752349853 and perplexity is 133.19373898039592
At time: 521.5623707771301 and batch: 100, loss is 4.825650091171265 and perplexity is 124.66748727833117
At time: 522.0732777118683 and batch: 150, loss is 4.815994234085083 and perplexity is 123.46950889972217
At time: 522.5660409927368 and batch: 200, loss is 4.830472106933594 and perplexity is 125.2700875734682
At time: 523.0744738578796 and batch: 250, loss is 4.89602876663208 and perplexity is 133.7575411520715
At time: 523.5757119655609 and batch: 300, loss is 4.80506049156189 and perplexity is 122.12687844336725
At time: 524.078381061554 and batch: 350, loss is 4.7925276279449465 and perplexity is 120.6058303853444
At time: 524.5796844959259 and batch: 400, loss is 4.790172271728515 and perplexity is 120.3220949731652
At time: 525.0905957221985 and batch: 450, loss is 4.860091304779052 and perplexity is 129.03598317146958
At time: 525.596465587616 and batch: 500, loss is 4.886942348480225 and perplexity is 132.54766922751745
At time: 526.1092267036438 and batch: 550, loss is 4.825523118972779 and perplexity is 124.65165897829
At time: 526.623605966568 and batch: 600, loss is 4.757751817703247 and perplexity is 116.48375459003842
At time: 527.1253771781921 and batch: 650, loss is 4.750487232208252 and perplexity is 115.64061463518418
At time: 527.6273038387299 and batch: 700, loss is 4.842370290756225 and perplexity is 126.76947641633588
At time: 528.1302406787872 and batch: 750, loss is 4.760236053466797 and perplexity is 116.77348743240259
At time: 528.6536843776703 and batch: 800, loss is 4.856132469177246 and perplexity is 128.52616074480696
At time: 529.1760008335114 and batch: 850, loss is 4.798411273956299 and perplexity is 121.31752402187571
At time: 529.6918995380402 and batch: 900, loss is 4.804943656921386 and perplexity is 122.11261062693232
At time: 530.2322018146515 and batch: 950, loss is 4.774036979675293 and perplexity is 118.39624171513536
At time: 530.7435879707336 and batch: 1000, loss is 4.703280601501465 and perplexity is 110.30845760343375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.1088781589415015 and perplexity of 165.48460327882037
Finished 48 epochs...
Completing Train Step...
At time: 532.3272018432617 and batch: 50, loss is 4.887411937713623 and perplexity is 132.6099268024982
At time: 532.8540122509003 and batch: 100, loss is 4.820897340774536 and perplexity is 124.07637963494578
At time: 533.3626787662506 and batch: 150, loss is 4.812741050720215 and perplexity is 123.06849259076348
At time: 533.8598170280457 and batch: 200, loss is 4.82820987701416 and perplexity is 124.98701813814401
At time: 534.3634798526764 and batch: 250, loss is 4.894274578094483 and perplexity is 133.5231108841251
At time: 534.8678863048553 and batch: 300, loss is 4.803192930221558 and perplexity is 121.89901185022016
At time: 535.3693602085114 and batch: 350, loss is 4.789926128387451 and perplexity is 120.2924821353556
At time: 535.8676023483276 and batch: 400, loss is 4.787594652175903 and perplexity is 120.01234976270524
At time: 536.3835601806641 and batch: 450, loss is 4.857306642532349 and perplexity is 128.67716137129474
At time: 536.8917367458344 and batch: 500, loss is 4.884306907653809 and perplexity is 132.19880759260627
At time: 537.4022953510284 and batch: 550, loss is 4.822664756774902 and perplexity is 124.29586812017462
At time: 537.9028012752533 and batch: 600, loss is 4.755450458526611 and perplexity is 116.21599185977011
At time: 538.3998174667358 and batch: 650, loss is 4.748245553970337 and perplexity is 115.38167592300856
At time: 538.9076640605927 and batch: 700, loss is 4.840116167068482 and perplexity is 126.48404415732965
At time: 539.416154384613 and batch: 750, loss is 4.758306093215943 and perplexity is 116.54833657928869
At time: 539.9229543209076 and batch: 800, loss is 4.8536709022521975 and perplexity is 128.21017406924386
At time: 540.4299013614655 and batch: 850, loss is 4.795628032684326 and perplexity is 120.98033753534327
At time: 540.9424526691437 and batch: 900, loss is 4.804792470932007 and perplexity is 122.09415030658506
At time: 541.4488968849182 and batch: 950, loss is 4.772135744094848 and perplexity is 118.17135641550328
At time: 541.9509170055389 and batch: 1000, loss is 4.699922342300415 and perplexity is 109.93863453915658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.107727422946837 and perplexity of 165.29428371405317
Finished 49 epochs...
Completing Train Step...
At time: 543.4597110748291 and batch: 50, loss is 4.883826780319214 and perplexity is 132.13535056642564
At time: 543.9684119224548 and batch: 100, loss is 4.818100261688232 and perplexity is 123.72981310131746
At time: 544.4747641086578 and batch: 150, loss is 4.809357347488404 and perplexity is 122.65276907380427
At time: 544.9761629104614 and batch: 200, loss is 4.826278610229492 and perplexity is 124.74586779927226
At time: 545.4794943332672 and batch: 250, loss is 4.8928701782226565 and perplexity is 133.3357226591126
At time: 545.9747517108917 and batch: 300, loss is 4.800527601242066 and perplexity is 121.57454348092281
At time: 546.4786200523376 and batch: 350, loss is 4.7869038581848145 and perplexity is 119.92947458076723
At time: 546.9807806015015 and batch: 400, loss is 4.784495496749878 and perplexity is 119.64098858798434
At time: 547.4909126758575 and batch: 450, loss is 4.855438385009766 and perplexity is 128.43698372328643
At time: 548.0028033256531 and batch: 500, loss is 4.882473497390747 and perplexity is 131.9566549923636
At time: 548.5104105472565 and batch: 550, loss is 4.8207857799530025 and perplexity is 124.06253834418801
At time: 549.0132329463959 and batch: 600, loss is 4.753779172897339 and perplexity is 116.02192395930231
At time: 549.5290842056274 and batch: 650, loss is 4.747433443069458 and perplexity is 115.28801124443083
At time: 550.0473093986511 and batch: 700, loss is 4.8385426902771 and perplexity is 126.28518094372286
At time: 550.5505075454712 and batch: 750, loss is 4.756695709228516 and perplexity is 116.36080004774266
At time: 551.0864417552948 and batch: 800, loss is 4.851283798217773 and perplexity is 127.90448804279909
At time: 551.5912330150604 and batch: 850, loss is 4.79423830986023 and perplexity is 120.81232517135263
At time: 552.1040856838226 and batch: 900, loss is 4.803282642364502 and perplexity is 121.90994816234874
At time: 552.6213235855103 and batch: 950, loss is 4.770309734344482 and perplexity is 117.95577125667793
At time: 553.1315569877625 and batch: 1000, loss is 4.6971830463409425 and perplexity is 109.63789218099461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.106007645769817 and perplexity of 165.0102586773213
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
SETTINGS FOR THIS RUN
{'lr': 1.6434782813015458, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.7410780142434373, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 7.956156059900165, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7235672473907471 and batch: 50, loss is 7.312224321365356 and perplexity is 1498.506645275779
At time: 1.2404038906097412 and batch: 100, loss is 6.396694736480713 and perplexity is 599.8590653134872
At time: 1.7481520175933838 and batch: 150, loss is 6.129443817138672 and perplexity is 459.18070120916457
At time: 2.2572176456451416 and batch: 200, loss is 6.049974184036255 and perplexity is 424.10208129948376
At time: 2.762312412261963 and batch: 250, loss is 6.087079048156738 and perplexity is 440.13392172412915
At time: 3.2639176845550537 and batch: 300, loss is 5.968918981552124 and perplexity is 391.08267457188845
At time: 3.7613627910614014 and batch: 350, loss is 5.936838064193726 and perplexity is 378.735496944125
At time: 4.27628755569458 and batch: 400, loss is 5.899932842254639 and perplexity is 365.0129535951855
At time: 4.782894134521484 and batch: 450, loss is 5.919376516342163 and perplexity is 372.1795936277037
At time: 5.286526918411255 and batch: 500, loss is 5.908673801422119 and perplexity is 368.21750192779376
At time: 5.791913747787476 and batch: 550, loss is 5.841431713104248 and perplexity is 344.2718865600373
At time: 6.290174245834351 and batch: 600, loss is 5.745006875991821 and perplexity is 312.62577743495285
At time: 6.81415319442749 and batch: 650, loss is 5.709330768585205 and perplexity is 301.66911426520153
At time: 7.332254886627197 and batch: 700, loss is 5.786958427429199 and perplexity is 326.0199016238239
At time: 7.8363401889801025 and batch: 750, loss is 5.6693061828613285 and perplexity is 289.8333732535594
At time: 8.344749450683594 and batch: 800, loss is 5.748599748611451 and perplexity is 313.7510222510775
At time: 8.859470129013062 and batch: 850, loss is 5.715775928497314 and perplexity is 303.6196991105264
At time: 9.381471633911133 and batch: 900, loss is 5.743082933425903 and perplexity is 312.0248816244103
At time: 9.887362241744995 and batch: 950, loss is 5.685307149887085 and perplexity is 294.50828934791684
At time: 10.402525901794434 and batch: 1000, loss is 5.607562189102173 and perplexity is 272.47917498297926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.510771960746951 and perplexity of 247.3419916968072
Finished 1 epochs...
Completing Train Step...
At time: 11.926133155822754 and batch: 50, loss is 5.418889446258545 and perplexity is 225.62841083757806
At time: 12.440063238143921 and batch: 100, loss is 5.306969833374024 and perplexity is 201.73800152248594
At time: 12.955101490020752 and batch: 150, loss is 5.238312873840332 and perplexity is 188.35206050419546
At time: 13.471409320831299 and batch: 200, loss is 5.199693956375122 and perplexity is 181.21677314951583
At time: 13.977997064590454 and batch: 250, loss is 5.240979022979737 and perplexity is 188.85490521985037
At time: 14.492505550384521 and batch: 300, loss is 5.114544191360474 and perplexity is 166.42490578245824
At time: 15.004631280899048 and batch: 350, loss is 5.092090473175049 and perplexity is 162.72968877958368
At time: 15.506098985671997 and batch: 400, loss is 5.0766908550262455 and perplexity is 160.24291057270298
At time: 16.01254916191101 and batch: 450, loss is 5.080986166000367 and perplexity is 160.93268404027452
At time: 16.515146255493164 and batch: 500, loss is 5.085633869171143 and perplexity is 161.68239224722953
At time: 17.047419786453247 and batch: 550, loss is 4.990447883605957 and perplexity is 147.0022486508
At time: 17.556538581848145 and batch: 600, loss is 4.886821765899658 and perplexity is 132.53168725110723
At time: 18.05993938446045 and batch: 650, loss is 4.860806264877319 and perplexity is 129.1282717380679
At time: 18.561113595962524 and batch: 700, loss is 4.907016735076905 and perplexity is 135.23536908804962
At time: 19.06125545501709 and batch: 750, loss is 4.857596158981323 and perplexity is 128.71442091948487
At time: 19.558714151382446 and batch: 800, loss is 4.929447288513184 and perplexity is 138.30304952683443
At time: 20.064033031463623 and batch: 850, loss is 4.825467309951782 and perplexity is 124.64470248535616
At time: 20.562300443649292 and batch: 900, loss is 4.856755867004394 and perplexity is 128.60630865356742
At time: 21.04698133468628 and batch: 950, loss is 4.813012428283692 and perplexity is 123.10189515057121
At time: 21.548911333084106 and batch: 1000, loss is 4.70473274230957 and perplexity is 110.46875737695132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.888378608517531 and perplexity of 132.73817892585095
Finished 2 epochs...
Completing Train Step...
At time: 23.038207054138184 and batch: 50, loss is 4.827346220016479 and perplexity is 124.87911882601458
At time: 23.57283043861389 and batch: 100, loss is 4.732965469360352 and perplexity is 113.63203552133173
At time: 24.07472062110901 and batch: 150, loss is 4.7620499801635745 and perplexity is 116.98549820660156
At time: 24.58438515663147 and batch: 200, loss is 4.778217277526855 and perplexity is 118.89220919365482
At time: 25.092210292816162 and batch: 250, loss is 4.820228624343872 and perplexity is 123.99343545738466
At time: 25.608546495437622 and batch: 300, loss is 4.704330558776856 and perplexity is 110.42433759490605
At time: 26.114541053771973 and batch: 350, loss is 4.708550729751587 and perplexity is 110.89133188410929
At time: 26.619171142578125 and batch: 400, loss is 4.713420877456665 and perplexity is 111.43270626615856
At time: 27.115046739578247 and batch: 450, loss is 4.743481969833374 and perplexity is 114.83335263082056
At time: 27.602189540863037 and batch: 500, loss is 4.766441307067871 and perplexity is 117.50034938484919
At time: 28.103681564331055 and batch: 550, loss is 4.672087106704712 and perplexity is 106.92066455626787
At time: 28.610459566116333 and batch: 600, loss is 4.591455268859863 and perplexity is 98.63787038576156
At time: 29.110891580581665 and batch: 650, loss is 4.5772857666015625 and perplexity is 97.25007625404731
At time: 29.601667165756226 and batch: 700, loss is 4.633707265853882 and perplexity is 102.8948163386978
At time: 30.122699737548828 and batch: 750, loss is 4.603515977859497 and perplexity is 99.83471593195536
At time: 30.616755962371826 and batch: 800, loss is 4.689645395278931 and perplexity is 108.8145867989888
At time: 31.121315240859985 and batch: 850, loss is 4.580672302246094 and perplexity is 97.57997539602603
At time: 31.634580612182617 and batch: 900, loss is 4.59600788116455 and perplexity is 99.08795411917956
At time: 32.13054442405701 and batch: 950, loss is 4.580336418151855 and perplexity is 97.54720533815302
At time: 32.62162733078003 and batch: 1000, loss is 4.4740842628479 and perplexity is 87.71424043232993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.714330161490092 and perplexity of 111.53407632687241
Finished 3 epochs...
Completing Train Step...
At time: 34.097134590148926 and batch: 50, loss is 4.615566453933716 and perplexity is 101.04504969025548
At time: 34.60603618621826 and batch: 100, loss is 4.51744421005249 and perplexity is 91.60118516257938
At time: 35.092785120010376 and batch: 150, loss is 4.573310623168945 and perplexity is 96.8642605964405
At time: 35.59917902946472 and batch: 200, loss is 4.60075159072876 and perplexity is 99.5591152370473
At time: 36.08878302574158 and batch: 250, loss is 4.633561849594116 and perplexity is 102.87985484720494
At time: 36.612465381622314 and batch: 300, loss is 4.509507360458374 and perplexity is 90.87703785880029
At time: 37.11374521255493 and batch: 350, loss is 4.526125469207764 and perplexity is 92.39986052786084
At time: 37.61752104759216 and batch: 400, loss is 4.538998565673828 and perplexity is 93.59702190038892
At time: 38.10973763465881 and batch: 450, loss is 4.572630462646484 and perplexity is 96.79839975085802
At time: 38.6018705368042 and batch: 500, loss is 4.600283308029175 and perplexity is 99.51250434018586
At time: 39.094385385513306 and batch: 550, loss is 4.502422208786011 and perplexity is 90.23543586973776
At time: 39.59922122955322 and batch: 600, loss is 4.433639912605286 and perplexity is 84.23747661002517
At time: 40.0968599319458 and batch: 650, loss is 4.419580698013306 and perplexity is 83.06145022453688
At time: 40.5924756526947 and batch: 700, loss is 4.482501382827759 and perplexity is 88.45565763946844
At time: 41.084659814834595 and batch: 750, loss is 4.454581642150879 and perplexity is 86.02015610883241
At time: 41.5743191242218 and batch: 800, loss is 4.550020837783814 and perplexity is 94.63438026513579
At time: 42.074265003204346 and batch: 850, loss is 4.435361042022705 and perplexity is 84.3825850484478
At time: 42.60060524940491 and batch: 900, loss is 4.439107370376587 and perplexity is 84.69930281333296
At time: 43.11046266555786 and batch: 950, loss is 4.439856367111206 and perplexity is 84.76276607848696
At time: 43.61115741729736 and batch: 1000, loss is 4.334226803779602 and perplexity is 76.26596752891965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6181089819931405 and perplexity of 101.30228644161357
Finished 4 epochs...
Completing Train Step...
At time: 45.074384927749634 and batch: 50, loss is 4.4815869140625 and perplexity is 88.3748046778352
At time: 45.575621604919434 and batch: 100, loss is 4.38144889831543 and perplexity is 79.95379443992508
At time: 46.088645935058594 and batch: 150, loss is 4.4520682430267335 and perplexity is 85.80422459851714
At time: 46.602407455444336 and batch: 200, loss is 4.484440307617188 and perplexity is 88.6273328857428
At time: 47.093761682510376 and batch: 250, loss is 4.509811687469482 and perplexity is 90.90469840482237
At time: 47.587414026260376 and batch: 300, loss is 4.3801266479492185 and perplexity is 79.84814536860111
At time: 48.08576226234436 and batch: 350, loss is 4.402441215515137 and perplexity is 81.6499506914707
At time: 48.58811140060425 and batch: 400, loss is 4.421439485549927 and perplexity is 83.21598739437555
At time: 49.077162981033325 and batch: 450, loss is 4.457220153808594 and perplexity is 86.24742098218623
At time: 49.58398962020874 and batch: 500, loss is 4.483986349105835 and perplexity is 88.58710888434545
At time: 50.09813714027405 and batch: 550, loss is 4.383533730506897 and perplexity is 80.12065856581405
At time: 50.61491894721985 and batch: 600, loss is 4.322183403968811 and perplexity is 75.35297479355711
At time: 51.123481035232544 and batch: 650, loss is 4.308339171409607 and perplexity is 74.31695865773803
At time: 51.64455246925354 and batch: 700, loss is 4.377090783119201 and perplexity is 79.60610477948718
At time: 52.16431021690369 and batch: 750, loss is 4.347965593338013 and perplexity is 77.32100044978868
At time: 52.68008828163147 and batch: 800, loss is 4.450189895629883 and perplexity is 85.64320572863336
At time: 53.19784069061279 and batch: 850, loss is 4.331541500091553 and perplexity is 76.06144497053029
At time: 53.70663261413574 and batch: 900, loss is 4.327142634391785 and perplexity is 75.72759570654506
At time: 54.217888593673706 and batch: 950, loss is 4.336150026321411 and perplexity is 76.41278509301463
At time: 54.73422050476074 and batch: 1000, loss is 4.2334071111679075 and perplexity is 68.95175872515648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.556480965963224 and perplexity of 95.2477094513901
Finished 5 epochs...
Completing Train Step...
At time: 56.34054088592529 and batch: 50, loss is 4.38143630027771 and perplexity is 79.95278718535161
At time: 56.86129903793335 and batch: 100, loss is 4.28004777431488 and perplexity is 72.24389132729493
At time: 57.364763259887695 and batch: 150, loss is 4.361692028045654 and perplexity is 78.38965977598971
At time: 57.87503981590271 and batch: 200, loss is 4.395976457595825 and perplexity is 81.12380605740462
At time: 58.39026856422424 and batch: 250, loss is 4.415684990882873 and perplexity is 82.73849661541496
At time: 58.89763164520264 and batch: 300, loss is 4.281693601608277 and perplexity is 72.36289019435272
At time: 59.406198501586914 and batch: 350, loss is 4.308164701461792 and perplexity is 74.3039937128688
At time: 59.916088581085205 and batch: 400, loss is 4.332169752120972 and perplexity is 76.10924574158706
At time: 60.4180645942688 and batch: 450, loss is 4.368964529037475 and perplexity is 78.96182667484017
At time: 60.92955493927002 and batch: 500, loss is 4.393999595642089 and perplexity is 80.9635939024998
At time: 61.430254220962524 and batch: 550, loss is 4.293271126747132 and perplexity is 73.20554187285944
At time: 61.94177746772766 and batch: 600, loss is 4.236258473396301 and perplexity is 69.1486457302562
At time: 62.46412253379822 and batch: 650, loss is 4.221654934883118 and perplexity is 68.14616849258118
At time: 62.973275661468506 and batch: 700, loss is 4.2959289598464965 and perplexity is 73.40036877911676
At time: 63.482136249542236 and batch: 750, loss is 4.264933490753174 and perplexity is 71.16018700556194
At time: 63.98212933540344 and batch: 800, loss is 4.372267408370972 and perplexity is 79.22305923262542
At time: 64.48198080062866 and batch: 850, loss is 4.250725355148315 and perplexity is 70.15628211555227
At time: 64.98647379875183 and batch: 900, loss is 4.2393427753448485 and perplexity is 69.36225027414339
At time: 65.50971055030823 and batch: 950, loss is 4.25461658000946 and perplexity is 70.42980781455053
At time: 66.02434396743774 and batch: 1000, loss is 4.154937868118286 and perplexity is 63.74800364134316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.513153820503049 and perplexity of 91.20902226240516
Finished 6 epochs...
Completing Train Step...
At time: 67.5947208404541 and batch: 50, loss is 4.301123700141907 and perplexity is 73.78265671610696
At time: 68.12185311317444 and batch: 100, loss is 4.199231672286987 and perplexity is 66.63511376305381
At time: 68.64105916023254 and batch: 150, loss is 4.288830575942993 and perplexity is 72.88118962931433
At time: 69.1761405467987 and batch: 200, loss is 4.3245148277282714 and perplexity is 75.52885946051359
At time: 69.6901273727417 and batch: 250, loss is 4.340772085189819 and perplexity is 76.76678696252955
At time: 70.19983887672424 and batch: 300, loss is 4.202406115531922 and perplexity is 66.84697924937974
At time: 70.6993350982666 and batch: 350, loss is 4.232447147369385 and perplexity is 68.88559929334626
At time: 71.21108102798462 and batch: 400, loss is 4.258570890426636 and perplexity is 70.70886050413114
At time: 71.7115969657898 and batch: 450, loss is 4.296502771377564 and perplexity is 73.44249884330917
At time: 72.21886682510376 and batch: 500, loss is 4.32065505027771 and perplexity is 75.23789675876613
At time: 72.72398710250854 and batch: 550, loss is 4.220533185005188 and perplexity is 68.06976839530446
At time: 73.23762655258179 and batch: 600, loss is 4.1664627647399906 and perplexity is 64.48694271234572
At time: 73.75302338600159 and batch: 650, loss is 4.150007891654968 and perplexity is 63.434500899342204
At time: 74.26143002510071 and batch: 700, loss is 4.229508566856384 and perplexity is 68.68347054488989
At time: 74.78295159339905 and batch: 750, loss is 4.196653861999511 and perplexity is 66.46356228986264
At time: 75.29123830795288 and batch: 800, loss is 4.307761788368225 and perplexity is 74.27406169130418
At time: 75.80900239944458 and batch: 850, loss is 4.18393455028534 and perplexity is 65.62354506876778
At time: 76.31497955322266 and batch: 900, loss is 4.16684630393982 and perplexity is 64.51168072644884
At time: 76.8169527053833 and batch: 950, loss is 4.1869167041778566 and perplexity is 65.81953667244721
At time: 77.31598591804504 and batch: 1000, loss is 4.090039119720459 and perplexity is 59.74222875772109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.480688699861852 and perplexity of 88.29546081252178
Finished 7 epochs...
Completing Train Step...
At time: 78.82102704048157 and batch: 50, loss is 4.234490866661072 and perplexity is 69.0265260798859
At time: 79.32640981674194 and batch: 100, loss is 4.132503085136413 and perplexity is 62.33375451796899
At time: 79.83277106285095 and batch: 150, loss is 4.227568039894104 and perplexity is 68.55031765360623
At time: 80.33802533149719 and batch: 200, loss is 4.264651374816895 and perplexity is 71.1401144143111
At time: 80.83916163444519 and batch: 250, loss is 4.278246307373047 and perplexity is 72.11386350089957
At time: 81.33573198318481 and batch: 300, loss is 4.135894050598145 and perplexity is 62.54548490897665
At time: 81.8445315361023 and batch: 350, loss is 4.169263243675232 and perplexity is 64.66779014853346
At time: 82.35583257675171 and batch: 400, loss is 4.195348343849182 and perplexity is 66.3768495178225
At time: 82.87272596359253 and batch: 450, loss is 4.235031566619873 and perplexity is 69.06385881168721
At time: 83.3878846168518 and batch: 500, loss is 4.258561134338379 and perplexity is 70.70817066561256
At time: 83.8952887058258 and batch: 550, loss is 4.1592378997802735 and perplexity is 64.02271228002517
At time: 84.39591789245605 and batch: 600, loss is 4.107213101387024 and perplexity is 60.777101707848026
At time: 84.89564728736877 and batch: 650, loss is 4.089496455192566 and perplexity is 59.70981756435483
At time: 85.40629363059998 and batch: 700, loss is 4.173578367233277 and perplexity is 64.94744258672813
At time: 85.91349029541016 and batch: 750, loss is 4.138188710212708 and perplexity is 62.6891702987536
At time: 86.41697525978088 and batch: 800, loss is 4.252709670066833 and perplexity is 70.2956324845656
At time: 86.92501831054688 and batch: 850, loss is 4.127278122901917 and perplexity is 62.00891238763156
At time: 87.43058347702026 and batch: 900, loss is 4.10435314655304 and perplexity is 60.60353026357698
At time: 87.93749618530273 and batch: 950, loss is 4.128796725273133 and perplexity is 62.103150806252245
At time: 88.44301557540894 and batch: 1000, loss is 4.034631571769714 and perplexity is 56.52209206736307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4557342529296875 and perplexity of 86.11936102930564
Finished 8 epochs...
Completing Train Step...
At time: 89.93516874313354 and batch: 50, loss is 4.177216000556946 and perplexity is 65.18412779417538
At time: 90.46457099914551 and batch: 100, loss is 4.07499165058136 and perplexity is 58.849989223099115
At time: 90.97714400291443 and batch: 150, loss is 4.174880790710449 and perplexity is 65.03208677005597
At time: 91.49204564094543 and batch: 200, loss is 4.213130493164062 and perplexity is 67.56772938884205
At time: 91.99006009101868 and batch: 250, loss is 4.224197673797607 and perplexity is 68.3196668941695
At time: 92.48689460754395 and batch: 300, loss is 4.078801999092102 and perplexity is 59.074655949332
At time: 92.98417639732361 and batch: 350, loss is 4.114813361167908 and perplexity is 61.240783287784
At time: 93.48692965507507 and batch: 400, loss is 4.14001356124878 and perplexity is 62.803673139662386
At time: 93.99252462387085 and batch: 450, loss is 4.181548285484314 and perplexity is 65.46713660327872
At time: 94.50441360473633 and batch: 500, loss is 4.204837617874145 and perplexity is 67.0097156027298
At time: 95.02113461494446 and batch: 550, loss is 4.106149582862854 and perplexity is 60.71249849378306
At time: 95.52175211906433 and batch: 600, loss is 4.05572039604187 and perplexity is 57.72673413130291
At time: 96.01858735084534 and batch: 650, loss is 4.037320799827576 and perplexity is 56.67429742897575
At time: 96.52698707580566 and batch: 700, loss is 4.1247485065460205 and perplexity is 61.852251857702576
At time: 97.02843141555786 and batch: 750, loss is 4.086856746673584 and perplexity is 59.55240889816468
At time: 97.5272524356842 and batch: 800, loss is 4.20451943397522 and perplexity is 66.98839758185879
At time: 98.03371405601501 and batch: 850, loss is 4.078048129081726 and perplexity is 59.03013812027499
At time: 98.55037140846252 and batch: 900, loss is 4.049689741134643 and perplexity is 57.37965173781902
At time: 99.06737685203552 and batch: 950, loss is 4.078207936286926 and perplexity is 59.0395723154767
At time: 99.58785820007324 and batch: 1000, loss is 3.986339764595032 and perplexity is 53.85739739749865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.437454967963986 and perplexity of 84.55946105027097
Finished 9 epochs...
Completing Train Step...
At time: 101.11022472381592 and batch: 50, loss is 4.126516847610474 and perplexity is 61.96172449854756
At time: 101.62876725196838 and batch: 100, loss is 4.0255354356765745 and perplexity is 56.01029066211942
At time: 102.13857007026672 and batch: 150, loss is 4.1287537813186646 and perplexity is 62.100483908635674
At time: 102.6410779953003 and batch: 200, loss is 4.167542009353638 and perplexity is 64.55657746762685
At time: 103.15384411811829 and batch: 250, loss is 4.176634640693664 and perplexity is 65.14624337186112
At time: 103.67232847213745 and batch: 300, loss is 4.029217991828919 and perplexity is 56.21693195314497
At time: 104.17274165153503 and batch: 350, loss is 4.066727123260498 and perplexity is 58.36562615217616
At time: 104.6778655052185 and batch: 400, loss is 4.091462326049805 and perplexity is 59.827314808959684
At time: 105.20048666000366 and batch: 450, loss is 4.134165778160095 and perplexity is 62.437482626850624
At time: 105.71706080436707 and batch: 500, loss is 4.157527627944947 and perplexity is 63.91330961920334
At time: 106.24089097976685 and batch: 550, loss is 4.059204740524292 and perplexity is 57.92822478643541
At time: 106.74496579170227 and batch: 600, loss is 4.010985422134399 and perplexity is 55.201240294528446
At time: 107.24613380432129 and batch: 650, loss is 3.9915571641921996 and perplexity is 54.13912727056147
At time: 107.77135014533997 and batch: 700, loss is 4.081311225891113 and perplexity is 59.22307378831937
At time: 108.29376173019409 and batch: 750, loss is 4.041108746528625 and perplexity is 56.88938375754932
At time: 108.81210017204285 and batch: 800, loss is 4.161228604316712 and perplexity is 64.15028952596586
At time: 109.32692050933838 and batch: 850, loss is 4.034689965248108 and perplexity is 56.52539268529133
At time: 109.83383274078369 and batch: 900, loss is 4.001634545326233 and perplexity is 54.687466159844845
At time: 110.34738278388977 and batch: 950, loss is 4.0334610176086425 and perplexity is 56.45596860539809
At time: 110.86902475357056 and batch: 1000, loss is 3.943527841567993 and perplexity is 51.60031832553445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.423054764910442 and perplexity of 83.350513080311
Finished 10 epochs...
Completing Train Step...
At time: 112.4989001750946 and batch: 50, loss is 4.081045722961425 and perplexity is 59.207351975909425
At time: 113.00350451469421 and batch: 100, loss is 3.9818200492858886 and perplexity is 53.61452656093312
At time: 113.51789736747742 and batch: 150, loss is 4.087656002044678 and perplexity is 59.600025507218554
At time: 114.03434610366821 and batch: 200, loss is 4.126887078285217 and perplexity is 61.984668876711154
At time: 114.54004621505737 and batch: 250, loss is 4.1343857860565185 and perplexity is 62.451220877267716
At time: 115.05105209350586 and batch: 300, loss is 3.984683828353882 and perplexity is 53.768286782405326
At time: 115.55333542823792 and batch: 350, loss is 4.023446183204651 and perplexity is 55.89339318059778
At time: 116.06985712051392 and batch: 400, loss is 4.047932362556457 and perplexity is 57.2789025202189
At time: 116.57431936264038 and batch: 450, loss is 4.09156512260437 and perplexity is 59.83346516690333
At time: 117.08262228965759 and batch: 500, loss is 4.115158042907715 and perplexity is 61.26189550580377
At time: 117.58601927757263 and batch: 550, loss is 4.017532639503479 and perplexity is 55.56384052978946
At time: 118.10383701324463 and batch: 600, loss is 3.9707715702056885 and perplexity is 53.02542790048234
At time: 118.60856938362122 and batch: 650, loss is 3.9505908250808717 and perplexity is 51.96606061857749
At time: 119.115234375 and batch: 700, loss is 4.0425008773803714 and perplexity is 56.968636376023184
At time: 119.62485313415527 and batch: 750, loss is 4.000214781761169 and perplexity is 54.609877979394334
At time: 120.11954927444458 and batch: 800, loss is 4.122326002120972 and perplexity is 61.70259584823902
At time: 120.62736916542053 and batch: 850, loss is 3.995756688117981 and perplexity is 54.366963898696795
At time: 121.15673232078552 and batch: 900, loss is 3.958615250587463 and perplexity is 52.384735968873485
At time: 121.65673589706421 and batch: 950, loss is 3.9934600162506104 and perplexity is 54.2422440972476
At time: 122.16555714607239 and batch: 1000, loss is 3.904928488731384 and perplexity is 49.64652949892454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.412232561809261 and perplexity of 82.45334034832416
Finished 11 epochs...
Completing Train Step...
At time: 123.69522309303284 and batch: 50, loss is 4.0420678520202635 and perplexity is 56.94397285209266
At time: 124.23091673851013 and batch: 100, loss is 3.943239679336548 and perplexity is 51.58545120483671
At time: 124.73758149147034 and batch: 150, loss is 4.050704469680786 and perplexity is 57.43790605956816
At time: 125.24228763580322 and batch: 200, loss is 4.089986953735352 and perplexity is 59.73911232679197
At time: 125.75314927101135 and batch: 250, loss is 4.095926342010498 and perplexity is 60.09498188771501
At time: 126.2642731666565 and batch: 300, loss is 3.9453636741638185 and perplexity is 51.695134878901214
At time: 126.76931881904602 and batch: 350, loss is 3.984560694694519 and perplexity is 53.761666504094045
At time: 127.2704713344574 and batch: 400, loss is 4.0084266138076785 and perplexity is 55.060171462239516
At time: 127.76685357093811 and batch: 450, loss is 4.052767367362976 and perplexity is 57.556516881780546
At time: 128.28366708755493 and batch: 500, loss is 4.076667075157165 and perplexity is 58.94867058483508
At time: 128.7836046218872 and batch: 550, loss is 3.9798477125167846 and perplexity is 53.50888487357755
At time: 129.29864025115967 and batch: 600, loss is 3.9344549226760863 and perplexity is 51.13427022786758
At time: 129.8112392425537 and batch: 650, loss is 3.913516640663147 and perplexity is 50.0747375631843
At time: 130.32232809066772 and batch: 700, loss is 4.007190632820129 and perplexity is 54.992160176127385
At time: 130.8288962841034 and batch: 750, loss is 3.9635657453536988 and perplexity is 52.64470929748039
At time: 131.330313205719 and batch: 800, loss is 4.0869149351119995 and perplexity is 59.555874260663366
At time: 131.8330500125885 and batch: 850, loss is 3.9602954530715944 and perplexity is 52.472826916969865
At time: 132.33792209625244 and batch: 900, loss is 3.9200342750549315 and perplexity is 50.40217228568387
At time: 132.84913277626038 and batch: 950, loss is 3.9571330642700193 and perplexity is 52.307149542950945
At time: 133.34734225273132 and batch: 1000, loss is 3.869996647834778 and perplexity is 47.94222537028933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.403576734589367 and perplexity of 81.7427184276899
Finished 12 epochs...
Completing Train Step...
At time: 134.87315225601196 and batch: 50, loss is 4.005662336349487 and perplexity is 54.90818004143491
At time: 135.40977883338928 and batch: 100, loss is 3.9078937196731567 and perplexity is 49.79396140113977
At time: 135.93177461624146 and batch: 150, loss is 4.016487302780152 and perplexity is 55.505787954325406
At time: 136.4468479156494 and batch: 200, loss is 4.056266169548035 and perplexity is 57.7582484524477
At time: 136.9548466205597 and batch: 250, loss is 4.060384182929993 and perplexity is 57.996588098605265
At time: 137.4760000705719 and batch: 300, loss is 3.9090900087356566 and perplexity is 49.85356501701111
At time: 137.9725661277771 and batch: 350, loss is 3.949133439064026 and perplexity is 51.89038116896232
At time: 138.47154116630554 and batch: 400, loss is 3.9727319574356077 and perplexity is 53.129480230304985
At time: 138.96941542625427 and batch: 450, loss is 4.017426319122315 and perplexity is 55.55793327512175
At time: 139.4758961200714 and batch: 500, loss is 4.041659126281738 and perplexity is 56.92070314052308
At time: 139.97838735580444 and batch: 550, loss is 3.945502223968506 and perplexity is 51.702297725936035
At time: 140.50215458869934 and batch: 600, loss is 3.9011373996734617 and perplexity is 49.45867140256103
At time: 141.00760889053345 and batch: 650, loss is 3.8800886726379393 and perplexity is 48.42850916461717
At time: 141.5250322818756 and batch: 700, loss is 3.975187611579895 and perplexity is 53.260108181508116
At time: 142.03930044174194 and batch: 750, loss is 3.9300806427001955 and perplexity is 50.911083110874195
At time: 142.5409653186798 and batch: 800, loss is 4.0546985912323 and perplexity is 57.66777880227565
At time: 143.05100989341736 and batch: 850, loss is 3.9282560777664184 and perplexity is 50.81827722481834
At time: 143.5505347251892 and batch: 900, loss is 3.8850809049606325 and perplexity is 48.670880015835984
At time: 144.06138348579407 and batch: 950, loss is 3.924137840270996 and perplexity is 50.60942583508486
At time: 144.56619548797607 and batch: 1000, loss is 3.8384277963638307 and perplexity is 46.452384390976206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.396987170707889 and perplexity of 81.2058404014975
Finished 13 epochs...
Completing Train Step...
At time: 146.0963261127472 and batch: 50, loss is 3.9717278575897215 and perplexity is 53.076159701441014
At time: 146.6195695400238 and batch: 100, loss is 3.8756296920776365 and perplexity is 48.21304810900898
At time: 147.14975786209106 and batch: 150, loss is 3.9852084493637085 and perplexity is 53.79650215585548
At time: 147.67553281784058 and batch: 200, loss is 4.0254012393951415 and perplexity is 56.002774793702656
At time: 148.18220949172974 and batch: 250, loss is 4.0277365922927855 and perplexity is 56.13371387105967
At time: 148.6829161643982 and batch: 300, loss is 3.8764619064331054 and perplexity is 48.253188400112755
At time: 149.18150663375854 and batch: 350, loss is 3.9166680765151978 and perplexity is 50.2327938076688
At time: 149.6929223537445 and batch: 400, loss is 3.940104880332947 and perplexity is 51.423994383360196
At time: 150.20254826545715 and batch: 450, loss is 3.9848481464385985 and perplexity is 53.77712261023118
At time: 150.7142219543457 and batch: 500, loss is 4.009636125564575 and perplexity is 55.126807677479114
At time: 151.2142791748047 and batch: 550, loss is 3.9140205144882203 and perplexity is 50.099975270515905
At time: 151.72049570083618 and batch: 600, loss is 3.8703927516937258 and perplexity is 47.961219232287725
At time: 152.22356963157654 and batch: 650, loss is 3.8492920351028443 and perplexity is 46.95980556642036
At time: 152.7357530593872 and batch: 700, loss is 3.9456678152084352 and perplexity is 51.71085988241317
At time: 153.24688482284546 and batch: 750, loss is 3.899615592956543 and perplexity is 49.38346210573201
At time: 153.74673700332642 and batch: 800, loss is 4.0248923063278195 and perplexity is 55.974280381237755
At time: 154.24806189537048 and batch: 850, loss is 3.899061346054077 and perplexity is 49.35609905846943
At time: 154.7473602294922 and batch: 900, loss is 3.8536134862899782 and perplexity is 47.163179192031755
At time: 155.25673985481262 and batch: 950, loss is 3.8934452819824217 and perplexity is 49.07968893880275
At time: 155.76574778556824 and batch: 1000, loss is 3.8093448781967165 and perplexity is 45.120869516210114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3926782375428735 and perplexity of 80.85668265153426
Finished 14 epochs...
Completing Train Step...
At time: 157.2744598388672 and batch: 50, loss is 3.9409558248519896 and perplexity is 51.46777197303636
At time: 157.78554964065552 and batch: 100, loss is 3.8462105226516723 and perplexity is 46.81532107058127
At time: 158.2938756942749 and batch: 150, loss is 3.9560719680786134 and perplexity is 52.251676062342035
At time: 158.80116248130798 and batch: 200, loss is 3.9968438577651977 and perplexity is 54.42610215249123
At time: 159.30083441734314 and batch: 250, loss is 3.9977356958389283 and perplexity is 54.47466307361608
At time: 159.82692790031433 and batch: 300, loss is 3.8464164113998414 and perplexity is 46.8249608107546
At time: 160.33789777755737 and batch: 350, loss is 3.8868986988067626 and perplexity is 48.759434104168264
At time: 160.84531140327454 and batch: 400, loss is 3.909511561393738 and perplexity is 49.87458535013622
At time: 161.35009574890137 and batch: 450, loss is 3.9547150993347167 and perplexity is 52.18082547462102
At time: 161.85095620155334 and batch: 500, loss is 3.9802274990081785 and perplexity is 53.52921068471203
At time: 162.35116577148438 and batch: 550, loss is 3.884988613128662 and perplexity is 48.666388298433304
At time: 162.8512613773346 and batch: 600, loss is 3.8419785499572754 and perplexity is 46.61761854104868
At time: 163.35008788108826 and batch: 650, loss is 3.820990967750549 and perplexity is 45.64942302003826
At time: 163.8568115234375 and batch: 700, loss is 3.9181908893585207 and perplexity is 50.30934722470475
At time: 164.35358881950378 and batch: 750, loss is 3.871422929763794 and perplexity is 48.01065328712227
At time: 164.8685381412506 and batch: 800, loss is 3.99706533908844 and perplexity is 54.43815785262182
At time: 165.38160228729248 and batch: 850, loss is 3.871991639137268 and perplexity is 48.03796516119654
At time: 165.8880693912506 and batch: 900, loss is 3.8246345376968383 and perplexity is 45.816053265954565
At time: 166.3995361328125 and batch: 950, loss is 3.865011625289917 and perplexity is 47.70382700028423
At time: 166.90916657447815 and batch: 1000, loss is 3.7821027612686158 and perplexity is 43.90827334762606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.389905976086128 and perplexity of 80.63283720910769
Finished 15 epochs...
Completing Train Step...
At time: 168.4536943435669 and batch: 50, loss is 3.913589596748352 and perplexity is 50.07839095327148
At time: 168.95488715171814 and batch: 100, loss is 3.818371696472168 and perplexity is 45.530011251624366
At time: 169.46665334701538 and batch: 150, loss is 3.928922176361084 and perplexity is 50.852138484077734
At time: 169.96393203735352 and batch: 200, loss is 3.9702906703948972 and perplexity is 52.999934112708054
At time: 170.46579098701477 and batch: 250, loss is 3.969836349487305 and perplexity is 52.97586060350298
At time: 170.96434688568115 and batch: 300, loss is 3.8185626602172853 and perplexity is 45.53870666331598
At time: 171.47273087501526 and batch: 350, loss is 3.8595799016952514 and perplexity is 47.4454154430267
At time: 171.98970746994019 and batch: 400, loss is 3.8813958501815797 and perplexity is 48.49185521751804
At time: 172.49705386161804 and batch: 450, loss is 3.9263578987121583 and perplexity is 50.72190652877092
At time: 173.03588700294495 and batch: 500, loss is 3.9533092308044435 and perplexity is 52.10751763686784
At time: 173.5397789478302 and batch: 550, loss is 3.8583084964752197 and perplexity is 47.38513142499407
At time: 174.06747460365295 and batch: 600, loss is 3.8157819271087647 and perplexity is 45.41225157438859
At time: 174.57812356948853 and batch: 650, loss is 3.794393391609192 and perplexity is 44.45126371587384
At time: 175.09068179130554 and batch: 700, loss is 3.8927090883255007 and perplexity is 49.043570079991035
At time: 175.59252667427063 and batch: 750, loss is 3.8454971075057984 and perplexity is 46.781934222228266
At time: 176.1021227836609 and batch: 800, loss is 3.9710896587371827 and perplexity is 53.04229736382423
At time: 176.6182155609131 and batch: 850, loss is 3.8466305541992187 and perplexity is 46.83498911264934
At time: 177.11800050735474 and batch: 900, loss is 3.7976703214645386 and perplexity is 44.59716631478168
At time: 177.62818145751953 and batch: 950, loss is 3.8386904287338255 and perplexity is 46.464585892965275
At time: 178.133220911026 and batch: 1000, loss is 3.756871461868286 and perplexity is 42.814270155619155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.387626461866425 and perplexity of 80.44924284259828
Finished 16 epochs...
Completing Train Step...
At time: 179.63166999816895 and batch: 50, loss is 3.887687258720398 and perplexity is 48.7978990032599
At time: 180.13828372955322 and batch: 100, loss is 3.792763786315918 and perplexity is 44.37888469186761
At time: 180.64963793754578 and batch: 150, loss is 3.9037537956237793 and perplexity is 49.58824430321682
At time: 181.1529724597931 and batch: 200, loss is 3.9451929569244384 and perplexity is 51.68631038145366
At time: 181.66820454597473 and batch: 250, loss is 3.9434006881713866 and perplexity is 51.59375758691232
At time: 182.1745719909668 and batch: 300, loss is 3.792325983047485 and perplexity is 44.35945972356768
At time: 182.6882243156433 and batch: 350, loss is 3.834909658432007 and perplexity is 46.289245636015636
At time: 183.1927454471588 and batch: 400, loss is 3.8552967357635497 and perplexity is 47.242633440479636
At time: 183.70824885368347 and batch: 450, loss is 3.8999079132080077 and perplexity is 49.39790000193487
At time: 184.222163438797 and batch: 500, loss is 3.9282841062545777 and perplexity is 50.81970160426132
At time: 184.72084164619446 and batch: 550, loss is 3.833033242225647 and perplexity is 46.20246918515461
At time: 185.2345335483551 and batch: 600, loss is 3.791036777496338 and perplexity is 44.30230810985074
At time: 185.75504612922668 and batch: 650, loss is 3.769819111824036 and perplexity is 43.37221860471686
At time: 186.2592260837555 and batch: 700, loss is 3.8690199089050292 and perplexity is 47.89542119384299
At time: 186.7752058506012 and batch: 750, loss is 3.8215842914581297 and perplexity is 45.67651594159496
At time: 187.29739546775818 and batch: 800, loss is 3.9467887830734254 and perplexity is 51.76885859588609
At time: 187.81051588058472 and batch: 850, loss is 3.822567915916443 and perplexity is 45.721466583496834
At time: 188.31587600708008 and batch: 900, loss is 3.7726463747024535 and perplexity is 43.49501677789282
At time: 188.8262050151825 and batch: 950, loss is 3.813664646148682 and perplexity is 45.31620279578198
At time: 189.3286852836609 and batch: 1000, loss is 3.7332962703704835 and perplexity is 41.81672045239193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.387495831745427 and perplexity of 80.43873443464376
Finished 17 epochs...
Completing Train Step...
At time: 190.804869890213 and batch: 50, loss is 3.8635147523880007 and perplexity is 47.63247385095064
At time: 191.31324243545532 and batch: 100, loss is 3.7687151861190795 and perplexity is 43.32436531581568
At time: 191.82344388961792 and batch: 150, loss is 3.8799056100845335 and perplexity is 48.41964452948798
At time: 192.32675957679749 and batch: 200, loss is 3.9216675090789797 and perplexity is 50.48455808768089
At time: 192.83385586738586 and batch: 250, loss is 3.918710389137268 and perplexity is 50.33548970937647
At time: 193.32944178581238 and batch: 300, loss is 3.7676301527023317 and perplexity is 43.27738242530115
At time: 193.82959651947021 and batch: 350, loss is 3.8113188552856445 and perplexity is 45.21002504540344
At time: 194.32678246498108 and batch: 400, loss is 3.8312414407730104 and perplexity is 46.11975765720037
At time: 194.82478141784668 and batch: 450, loss is 3.8758163261413574 and perplexity is 48.22204714583922
At time: 195.33001255989075 and batch: 500, loss is 3.9043002796173094 and perplexity is 49.61535089097935
At time: 195.8255410194397 and batch: 550, loss is 3.8091972208023073 and perplexity is 45.1142075780382
At time: 196.3323884010315 and batch: 600, loss is 3.7679294872283937 and perplexity is 43.290338779103905
At time: 196.8360984325409 and batch: 650, loss is 3.7467169094085695 and perplexity is 42.3817103458811
At time: 197.33724856376648 and batch: 700, loss is 3.8468557167053223 and perplexity is 46.845535783484145
At time: 197.83672618865967 and batch: 750, loss is 3.798781566619873 and perplexity is 44.64675224575075
At time: 198.34210181236267 and batch: 800, loss is 3.923954110145569 and perplexity is 50.60012821308118
At time: 198.85938501358032 and batch: 850, loss is 3.8000668334960936 and perplexity is 44.704172129576015
At time: 199.3608365058899 and batch: 900, loss is 3.7489255905151366 and perplexity is 42.47542147969572
At time: 199.86756658554077 and batch: 950, loss is 3.7904173517227173 and perplexity is 44.274874615760424
At time: 200.37021660804749 and batch: 1000, loss is 3.711454095840454 and perplexity is 40.91325508860615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.388036216177592 and perplexity of 80.48221402126312
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 201.8640079498291 and batch: 50, loss is 3.8727421379089355 and perplexity is 48.07403112708213
At time: 202.38175797462463 and batch: 100, loss is 3.7697786331176757 and perplexity is 43.370462988948546
At time: 202.88145995140076 and batch: 150, loss is 3.884340510368347 and perplexity is 48.634857696481276
At time: 203.37810063362122 and batch: 200, loss is 3.921799659729004 and perplexity is 50.49123009569375
At time: 203.87402749061584 and batch: 250, loss is 3.9117835569381714 and perplexity is 49.98802900877635
At time: 204.3665943145752 and batch: 300, loss is 3.756583776473999 and perplexity is 42.801954886974706
At time: 204.8878390789032 and batch: 350, loss is 3.7942080307006836 and perplexity is 44.44302495284301
At time: 205.3899896144867 and batch: 400, loss is 3.8133051061630248 and perplexity is 45.2999127375179
At time: 205.88871335983276 and batch: 450, loss is 3.8532539319992067 and perplexity is 47.1462245168326
At time: 206.3829426765442 and batch: 500, loss is 3.8775355768203736 and perplexity is 48.30502424191384
At time: 206.89610648155212 and batch: 550, loss is 3.7742557096481324 and perplexity is 43.565071183738254
At time: 207.40386271476746 and batch: 600, loss is 3.7283608150482177 and perplexity is 41.610844360907514
At time: 207.89840602874756 and batch: 650, loss is 3.6907388019561767 and perplexity is 40.07444310005144
At time: 208.4048593044281 and batch: 700, loss is 3.789733123779297 and perplexity is 44.24459087103057
At time: 208.90209674835205 and batch: 750, loss is 3.7371377897262574 and perplexity is 41.97766913922242
At time: 209.40573644638062 and batch: 800, loss is 3.857619218826294 and perplexity is 47.3524811668507
At time: 209.90009379386902 and batch: 850, loss is 3.7250974702835085 and perplexity is 41.47527515464805
At time: 210.40551686286926 and batch: 900, loss is 3.662191143035889 and perplexity is 38.94658698905221
At time: 210.91564631462097 and batch: 950, loss is 3.6872116136550903 and perplexity is 39.93334198456552
At time: 211.44302105903625 and batch: 1000, loss is 3.6064505529403688 and perplexity is 36.835076351873106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.344073411895008 and perplexity of 77.02063799844746
Finished 19 epochs...
Completing Train Step...
At time: 212.92815327644348 and batch: 50, loss is 3.8365969324111937 and perplexity is 46.367414203043246
At time: 213.43638443946838 and batch: 100, loss is 3.732903070449829 and perplexity is 41.800281353366024
At time: 213.9317762851715 and batch: 150, loss is 3.8508216619491575 and perplexity is 47.03169151103146
At time: 214.43693900108337 and batch: 200, loss is 3.8920320653915406 and perplexity is 49.01037769555418
At time: 214.94156050682068 and batch: 250, loss is 3.882737259864807 and perplexity is 48.556946308814254
At time: 215.43898844718933 and batch: 300, loss is 3.7277427101135254 and perplexity is 41.58513243982259
At time: 215.9453318119049 and batch: 350, loss is 3.7693725395202637 and perplexity is 43.352854097283014
At time: 216.44978284835815 and batch: 400, loss is 3.789019808769226 and perplexity is 44.21304179380071
At time: 216.9818012714386 and batch: 450, loss is 3.8318413209915163 and perplexity is 46.14743228740291
At time: 217.48658180236816 and batch: 500, loss is 3.857925763130188 and perplexity is 47.36699902529737
At time: 218.00745677947998 and batch: 550, loss is 3.755719165802002 and perplexity is 42.76496385372255
At time: 218.50376319885254 and batch: 600, loss is 3.7128649234771727 and perplexity is 40.97101737632821
At time: 219.0197238922119 and batch: 650, loss is 3.6769476890563966 and perplexity is 39.52556544714765
At time: 219.52899646759033 and batch: 700, loss is 3.777959246635437 and perplexity is 43.72671517871732
At time: 220.04856371879578 and batch: 750, loss is 3.7277412891387938 and perplexity is 41.58507334844216
At time: 220.55584836006165 and batch: 800, loss is 3.850110821723938 and perplexity is 46.998271372441984
At time: 221.05554342269897 and batch: 850, loss is 3.7198939990997313 and perplexity is 41.260020277505355
At time: 221.553697347641 and batch: 900, loss is 3.659724307060242 and perplexity is 38.850630550196826
At time: 222.04808378219604 and batch: 950, loss is 3.6880104970932006 and perplexity is 39.96525681652335
At time: 222.54586696624756 and batch: 1000, loss is 3.6109184551239015 and perplexity is 37.0000200717535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.341189500762195 and perplexity of 76.79883730353936
Finished 20 epochs...
Completing Train Step...
At time: 224.01454257965088 and batch: 50, loss is 3.8257246446609496 and perplexity is 45.86602489695078
At time: 224.5288691520691 and batch: 100, loss is 3.721281638145447 and perplexity is 41.31731403500347
At time: 225.05174207687378 and batch: 150, loss is 3.8391500425338747 and perplexity is 46.48594656630935
At time: 225.56658506393433 and batch: 200, loss is 3.880610775947571 and perplexity is 48.45380045129026
At time: 226.07495522499084 and batch: 250, loss is 3.8709131050109864 and perplexity is 47.98618250611301
At time: 226.5741012096405 and batch: 300, loss is 3.716024332046509 and perplexity is 41.100666258771746
At time: 227.08311772346497 and batch: 350, loss is 3.7582016658782957 and perplexity is 42.87125976496591
At time: 227.58716869354248 and batch: 400, loss is 3.778203372955322 and perplexity is 43.73739132388556
At time: 228.09564113616943 and batch: 450, loss is 3.821839828491211 and perplexity is 45.68818947440661
At time: 228.6091547012329 and batch: 500, loss is 3.8485787439346315 and perplexity is 46.92632149521465
At time: 229.11969900131226 and batch: 550, loss is 3.746761794090271 and perplexity is 42.38361267815242
At time: 229.61633205413818 and batch: 600, loss is 3.705133490562439 and perplexity is 40.65547407676083
At time: 230.1240074634552 and batch: 650, loss is 3.6698638200759888 and perplexity is 39.24656090269337
At time: 230.6258773803711 and batch: 700, loss is 3.771895751953125 and perplexity is 43.46238067904839
At time: 231.140310049057 and batch: 750, loss is 3.7228665208816527 and perplexity is 41.38284905166434
At time: 231.6522078514099 and batch: 800, loss is 3.8463437223434447 and perplexity is 46.82155727223895
At time: 232.1657030582428 and batch: 850, loss is 3.717014946937561 and perplexity is 41.14140136386953
At time: 232.67631483078003 and batch: 900, loss is 3.6578402090072633 and perplexity is 38.77750106601266
At time: 233.1825349330902 and batch: 950, loss is 3.687422742843628 and perplexity is 39.94177396874199
At time: 233.7019600868225 and batch: 1000, loss is 3.611887044906616 and perplexity is 37.03587527484373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3400156905011436 and perplexity of 76.70874292747173
Finished 21 epochs...
Completing Train Step...
At time: 235.18678784370422 and batch: 50, loss is 3.8174316692352295 and perplexity is 45.48723191097259
At time: 235.70408010482788 and batch: 100, loss is 3.7126444482803347 and perplexity is 40.96198527892083
At time: 236.19782209396362 and batch: 150, loss is 3.8309006595611574 and perplexity is 46.104043587977436
At time: 236.6921582221985 and batch: 200, loss is 3.8725467348098754 and perplexity is 48.06463823014606
At time: 237.19249534606934 and batch: 250, loss is 3.8626580238342285 and perplexity is 47.591683226257146
At time: 237.70747327804565 and batch: 300, loss is 3.7077610158920287 and perplexity is 40.762437828128206
At time: 238.20385479927063 and batch: 350, loss is 3.7503251218795777 and perplexity is 42.534908781737144
At time: 238.69326543807983 and batch: 400, loss is 3.7705849552154542 and perplexity is 43.405447654206306
At time: 239.20183181762695 and batch: 450, loss is 3.814793152809143 and perplexity is 45.367371299031824
At time: 239.71246004104614 and batch: 500, loss is 3.841884708404541 and perplexity is 46.61324407659649
At time: 240.22711038589478 and batch: 550, loss is 3.740297613143921 and perplexity is 42.11052094477255
At time: 240.72996020317078 and batch: 600, loss is 3.6994937801361085 and perplexity is 40.426834312768875
At time: 241.23448538780212 and batch: 650, loss is 3.6646061086654664 and perplexity is 39.04075531889384
At time: 241.73569464683533 and batch: 700, loss is 3.767327013015747 and perplexity is 43.26426532141437
At time: 242.2380452156067 and batch: 750, loss is 3.71907169342041 and perplexity is 41.22610587441642
At time: 242.7402379512787 and batch: 800, loss is 3.8432757091522216 and perplexity is 46.67812825046409
At time: 243.24614357948303 and batch: 850, loss is 3.7143795490264893 and perplexity is 41.03312014537578
At time: 243.76912665367126 and batch: 900, loss is 3.655621967315674 and perplexity is 38.69157853016052
At time: 244.2796778678894 and batch: 950, loss is 3.6859544134140014 and perplexity is 39.88316932254481
At time: 244.7886245250702 and batch: 1000, loss is 3.611366558074951 and perplexity is 37.01660360522419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339447765815549 and perplexity of 76.66519050718018
Finished 22 epochs...
Completing Train Step...
At time: 246.30422520637512 and batch: 50, loss is 3.810481605529785 and perplexity is 45.17218880437379
At time: 246.807466506958 and batch: 100, loss is 3.70546103477478 and perplexity is 40.66879272309834
At time: 247.2969286441803 and batch: 150, loss is 3.8241566467285155 and perplexity is 45.7941634187913
At time: 247.81573629379272 and batch: 200, loss is 3.8659796619415285 and perplexity is 47.75002841196464
At time: 248.3192002773285 and batch: 250, loss is 3.8559804487228395 and perplexity is 47.274944885812126
At time: 248.82555651664734 and batch: 300, loss is 3.7010458755493163 and perplexity is 40.489629336204054
At time: 249.3177900314331 and batch: 350, loss is 3.7439027404785157 and perplexity is 42.262608718075995
At time: 249.82861185073853 and batch: 400, loss is 3.764371542930603 and perplexity is 43.13658784591733
At time: 250.35444521903992 and batch: 450, loss is 3.8090399742126464 and perplexity is 45.10711408048006
At time: 250.85585188865662 and batch: 500, loss is 3.836382942199707 and perplexity is 46.357493091820054
At time: 251.35073828697205 and batch: 550, loss is 3.7349344158172606 and perplexity is 41.8852784612638
At time: 251.85307383537292 and batch: 600, loss is 3.694776482582092 and perplexity is 40.23657800676479
At time: 252.35994458198547 and batch: 650, loss is 3.6601492309570314 and perplexity is 38.8671426194609
At time: 252.85691928863525 and batch: 700, loss is 3.7633798551559448 and perplexity is 43.093831023323276
At time: 253.35695362091064 and batch: 750, loss is 3.7156845045089724 and perplexity is 41.086701493506204
At time: 253.85685443878174 and batch: 800, loss is 3.840418357849121 and perplexity is 46.54494280929382
At time: 254.3585786819458 and batch: 850, loss is 3.711749777793884 and perplexity is 40.92535418844652
At time: 254.85687136650085 and batch: 900, loss is 3.653149433135986 and perplexity is 38.59603045140106
At time: 255.36747694015503 and batch: 950, loss is 3.684004111289978 and perplexity is 39.805460894789604
At time: 255.87469911575317 and batch: 1000, loss is 3.6100969171524047 and perplexity is 36.96963563301242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339171618950076 and perplexity of 76.64402257799397
Finished 23 epochs...
Completing Train Step...
At time: 257.37134742736816 and batch: 50, loss is 3.804367461204529 and perplexity is 44.896842134942
At time: 257.902010679245 and batch: 100, loss is 3.6991729736328125 and perplexity is 40.41386720149169
At time: 258.41216683387756 and batch: 150, loss is 3.8182848930358886 and perplexity is 45.526059261719524
At time: 258.91906547546387 and batch: 200, loss is 3.8602724742889403 and perplexity is 47.478286218844055
At time: 259.4192202091217 and batch: 250, loss is 3.850200719833374 and perplexity is 47.002496618103066
At time: 259.9355640411377 and batch: 300, loss is 3.6952112770080565 and perplexity is 40.25407645043923
At time: 260.4431698322296 and batch: 350, loss is 3.738303027153015 and perplexity is 42.02661159964706
At time: 260.95786333084106 and batch: 400, loss is 3.7589612102508543 and perplexity is 42.903834758575364
At time: 261.4633114337921 and batch: 450, loss is 3.8040094900131227 and perplexity is 44.88077323514578
At time: 261.9752616882324 and batch: 500, loss is 3.8315471029281616 and perplexity is 46.13385687641001
At time: 262.48373198509216 and batch: 550, loss is 3.730185379981995 and perplexity is 41.68683535256121
At time: 263.0166132450104 and batch: 600, loss is 3.690569953918457 and perplexity is 40.06767718019351
At time: 263.5732500553131 and batch: 650, loss is 3.6561236429214476 and perplexity is 38.710994020989844
At time: 264.078147649765 and batch: 700, loss is 3.7597623252868653 and perplexity is 42.938219436905435
At time: 264.5928089618683 and batch: 750, loss is 3.7125049114227293 and perplexity is 40.95626997097106
At time: 265.1087028980255 and batch: 800, loss is 3.8376394081115723 and perplexity is 46.415776309419854
At time: 265.6317608356476 and batch: 850, loss is 3.709096384048462 and perplexity is 40.816907049720335
At time: 266.1585440635681 and batch: 900, loss is 3.6505223989486693 and perplexity is 38.49477042492175
At time: 266.66412472724915 and batch: 950, loss is 3.681772618293762 and perplexity is 39.71673432077426
At time: 267.1680953502655 and batch: 1000, loss is 3.6084056901931763 and perplexity is 36.90716442993869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339043966153773 and perplexity of 76.63423937863162
Finished 24 epochs...
Completing Train Step...
At time: 268.64552664756775 and batch: 50, loss is 3.7988542318344116 and perplexity is 44.64999662945662
At time: 269.16539430618286 and batch: 100, loss is 3.693502688407898 and perplexity is 40.18535751721651
At time: 269.6502203941345 and batch: 150, loss is 3.8129876375198366 and perplexity is 45.28553371824921
At time: 270.14656496047974 and batch: 200, loss is 3.8551332092285158 and perplexity is 47.23490864794873
At time: 270.6383867263794 and batch: 250, loss is 3.8449972677230835 and perplexity is 46.75855659341282
At time: 271.1495580673218 and batch: 300, loss is 3.6899492073059084 and perplexity is 40.0428130232802
At time: 271.64753460884094 and batch: 350, loss is 3.733241891860962 and perplexity is 41.81444658328601
At time: 272.163099527359 and batch: 400, loss is 3.7540690994262693 and perplexity is 42.6944570112906
At time: 272.6745126247406 and batch: 450, loss is 3.7994449281692506 and perplexity is 44.676379010036854
At time: 273.1870517730713 and batch: 500, loss is 3.8271412324905394 and perplexity is 45.93104419150918
At time: 273.69776606559753 and batch: 550, loss is 3.7258379220962525 and perplexity is 41.505996969928624
At time: 274.2046637535095 and batch: 600, loss is 3.686694130897522 and perplexity is 39.91268251455721
At time: 274.70809149742126 and batch: 650, loss is 3.6523765563964843 and perplexity is 38.566212001705466
At time: 275.21284675598145 and batch: 700, loss is 3.756344723701477 and perplexity is 42.79172418387731
At time: 275.7145059108734 and batch: 750, loss is 3.7094544172286987 and perplexity is 40.83152347318492
At time: 276.26358461380005 and batch: 800, loss is 3.834902591705322 and perplexity is 46.28891852372409
At time: 276.76575779914856 and batch: 850, loss is 3.7064255046844483 and perplexity is 40.70803547112516
At time: 277.25896644592285 and batch: 900, loss is 3.6477974367141726 and perplexity is 38.390016419462796
At time: 277.76102209091187 and batch: 950, loss is 3.679370250701904 and perplexity is 39.62143464364966
At time: 278.25771498680115 and batch: 1000, loss is 3.6064586544036867 and perplexity is 36.835374771101804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.338993723799542 and perplexity of 76.63038919075261
Finished 25 epochs...
Completing Train Step...
At time: 279.74518942832947 and batch: 50, loss is 3.793793044090271 and perplexity is 44.42458551888507
At time: 280.2425389289856 and batch: 100, loss is 3.6882777643203735 and perplexity is 39.97593964741766
At time: 280.7497260570526 and batch: 150, loss is 3.8081047916412354 and perplexity is 45.064950411982224
At time: 281.2598307132721 and batch: 200, loss is 3.8504008865356445 and perplexity is 47.011905894530095
At time: 281.7794852256775 and batch: 250, loss is 3.840204315185547 and perplexity is 46.53498127189405
At time: 282.2864124774933 and batch: 300, loss is 3.68509295463562 and perplexity is 39.848826410847465
At time: 282.7784481048584 and batch: 350, loss is 3.728564829826355 and perplexity is 41.61933445411071
At time: 283.27223205566406 and batch: 400, loss is 3.749542031288147 and perplexity is 42.50161313331881
At time: 283.7656795978546 and batch: 450, loss is 3.795197048187256 and perplexity is 44.487001624941726
At time: 284.2672827243805 and batch: 500, loss is 3.823032693862915 and perplexity is 45.74272185195259
At time: 284.7604022026062 and batch: 550, loss is 3.72177631855011 and perplexity is 41.33775795681675
At time: 285.26267647743225 and batch: 600, loss is 3.683052043914795 and perplexity is 39.76758144887114
At time: 285.7697055339813 and batch: 650, loss is 3.648820924758911 and perplexity is 38.429328256471344
At time: 286.2635169029236 and batch: 700, loss is 3.7530639076232912 and perplexity is 42.65156245531005
At time: 286.75932455062866 and batch: 750, loss is 3.706492667198181 and perplexity is 40.71076961693154
At time: 287.2732951641083 and batch: 800, loss is 3.8321955156326295 and perplexity is 46.16378035564841
At time: 287.787056684494 and batch: 850, loss is 3.7037490797042847 and perplexity is 40.59922913904002
At time: 288.30470061302185 and batch: 900, loss is 3.6450069761276245 and perplexity is 38.28303991799875
At time: 288.8279755115509 and batch: 950, loss is 3.6768621492385862 and perplexity is 39.5221845820818
At time: 289.3605830669403 and batch: 1000, loss is 3.6043477487564086 and perplexity is 36.757700780552966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3390391280011436 and perplexity of 76.63386861138176
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 290.8622844219208 and batch: 50, loss is 3.7990588092803956 and perplexity is 44.65913194613829
At time: 291.3858120441437 and batch: 100, loss is 3.6973816108703614 and perplexity is 40.34153610965496
At time: 291.89700865745544 and batch: 150, loss is 3.818519368171692 and perplexity is 45.53673524222459
At time: 292.3983840942383 and batch: 200, loss is 3.8644665241241456 and perplexity is 47.67783067453205
At time: 292.89085817337036 and batch: 250, loss is 3.849915471076965 and perplexity is 46.98909112643546
At time: 293.4031572341919 and batch: 300, loss is 3.690079164505005 and perplexity is 40.04801721325831
At time: 293.89971804618835 and batch: 350, loss is 3.7326467514038084 and perplexity is 41.78956851813665
At time: 294.3990867137909 and batch: 400, loss is 3.7559869861602784 and perplexity is 42.77641871551766
At time: 294.9030222892761 and batch: 450, loss is 3.800070629119873 and perplexity is 44.7043418101168
At time: 295.3968279361725 and batch: 500, loss is 3.8278783845901487 and perplexity is 45.9649148395481
At time: 295.89392590522766 and batch: 550, loss is 3.722173500061035 and perplexity is 41.35417981099283
At time: 296.40605902671814 and batch: 600, loss is 3.6831140184402464 and perplexity is 39.77004610223186
At time: 296.9116840362549 and batch: 650, loss is 3.6479830598831176 and perplexity is 38.397143157389806
At time: 297.4129304885864 and batch: 700, loss is 3.7459234380722046 and perplexity is 42.34809501170127
At time: 297.9053337574005 and batch: 750, loss is 3.700238814353943 and perplexity is 40.45696491042001
At time: 298.4121963977814 and batch: 800, loss is 3.8262303256988526 and perplexity is 45.88922434129033
At time: 298.9210948944092 and batch: 850, loss is 3.692635531425476 and perplexity is 40.1505256084035
At time: 299.4250156879425 and batch: 900, loss is 3.62896315574646 and perplexity is 37.673734564489536
At time: 299.94205141067505 and batch: 950, loss is 3.6647934436798097 and perplexity is 39.04806970445034
At time: 300.47066593170166 and batch: 1000, loss is 3.5847493314743044 and perplexity is 36.04432140943997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.332189420374428 and perplexity of 76.11074269224382
Finished 27 epochs...
Completing Train Step...
At time: 302.0612151622772 and batch: 50, loss is 3.7951276874542237 and perplexity is 44.48391608090765
At time: 302.5863130092621 and batch: 100, loss is 3.689712142944336 and perplexity is 40.03332142447964
At time: 303.09367394447327 and batch: 150, loss is 3.811131167411804 and perplexity is 45.20154046817743
At time: 303.595698595047 and batch: 200, loss is 3.8565873432159425 and perplexity is 47.303644497462315
At time: 304.1037573814392 and batch: 250, loss is 3.844442081451416 and perplexity is 46.73260408961271
At time: 304.61429381370544 and batch: 300, loss is 3.684180908203125 and perplexity is 39.81249899954147
At time: 305.1378676891327 and batch: 350, loss is 3.7285541343688964 and perplexity is 41.61888931867006
At time: 305.6535439491272 and batch: 400, loss is 3.750731358528137 and perplexity is 42.552191530732756
At time: 306.1474497318268 and batch: 450, loss is 3.7952381992340087 and perplexity is 44.48883234929334
At time: 306.6544563770294 and batch: 500, loss is 3.8238527631759642 and perplexity is 45.78024943994722
At time: 307.15101385116577 and batch: 550, loss is 3.718439025878906 and perplexity is 41.20003170437733
At time: 307.6579546928406 and batch: 600, loss is 3.679906392097473 and perplexity is 39.64268303047485
At time: 308.1593129634857 and batch: 650, loss is 3.6452956914901735 and perplexity is 38.29409441547299
At time: 308.67280411720276 and batch: 700, loss is 3.7438381052017213 and perplexity is 42.2598771509422
At time: 309.19090580940247 and batch: 750, loss is 3.6991841793060303 and perplexity is 40.41432006861835
At time: 309.702187538147 and batch: 800, loss is 3.825580983161926 and perplexity is 45.85943618834305
At time: 310.21472358703613 and batch: 850, loss is 3.692191514968872 and perplexity is 40.13270207155673
At time: 310.710063457489 and batch: 900, loss is 3.6294912910461425 and perplexity is 37.6936366486179
At time: 311.2145392894745 and batch: 950, loss is 3.665953588485718 and perplexity is 39.09339740796459
At time: 311.7222013473511 and batch: 1000, loss is 3.5868454074859617 and perplexity is 36.119952283242355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3317387278487045 and perplexity of 76.07644787817384
Finished 28 epochs...
Completing Train Step...
At time: 313.2387578487396 and batch: 50, loss is 3.7937487316131593 and perplexity is 44.42261699907139
At time: 313.73822808265686 and batch: 100, loss is 3.6876186084747316 and perplexity is 39.94959795570783
At time: 314.23799777030945 and batch: 150, loss is 3.8087869024276735 and perplexity is 45.09570018693286
At time: 314.74146127700806 and batch: 200, loss is 3.8543291521072387 and perplexity is 47.196944348055666
At time: 315.25926423072815 and batch: 250, loss is 3.842251601219177 and perplexity is 46.63034927861105
At time: 315.7616169452667 and batch: 300, loss is 3.6817790174484255 and perplexity is 39.71698847511309
At time: 316.2739293575287 and batch: 350, loss is 3.726554284095764 and perplexity is 41.53574094136316
At time: 316.7840371131897 and batch: 400, loss is 3.7485077285766604 and perplexity is 42.457676325509084
At time: 317.2842881679535 and batch: 450, loss is 3.7931938934326173 and perplexity is 44.39797647146558
At time: 317.78230261802673 and batch: 500, loss is 3.82208062171936 and perplexity is 45.69919220567687
At time: 318.28239011764526 and batch: 550, loss is 3.7168115854263304 and perplexity is 41.13303563697628
At time: 318.78945541381836 and batch: 600, loss is 3.6785736751556395 and perplexity is 39.58988574491346
At time: 319.2820394039154 and batch: 650, loss is 3.6442161989212036 and perplexity is 38.25277852922428
At time: 319.8026547431946 and batch: 700, loss is 3.742964859008789 and perplexity is 42.22298998224023
At time: 320.3137352466583 and batch: 750, loss is 3.698819179534912 and perplexity is 40.39957154281142
At time: 320.8218536376953 and batch: 800, loss is 3.82543550491333 and perplexity is 45.85276512314396
At time: 321.358167886734 and batch: 850, loss is 3.6922088098526 and perplexity is 40.13339616797488
At time: 321.86862874031067 and batch: 900, loss is 3.6299297857284545 and perplexity is 37.71016873219579
At time: 322.3738970756531 and batch: 950, loss is 3.6666653156280518 and perplexity is 39.12123114382357
At time: 322.89516401290894 and batch: 1000, loss is 3.587857003211975 and perplexity is 36.15650956007089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3315344089415015 and perplexity of 76.06090560932324
Finished 29 epochs...
Completing Train Step...
At time: 324.4188959598541 and batch: 50, loss is 3.792522306442261 and perplexity is 44.36816937821754
At time: 324.9317843914032 and batch: 100, loss is 3.686094355583191 and perplexity is 39.8887510503244
At time: 325.46068811416626 and batch: 150, loss is 3.807167148590088 and perplexity is 45.022715378165444
At time: 325.96699023246765 and batch: 200, loss is 3.8527987670898436 and perplexity is 47.124770092844464
At time: 326.47083592414856 and batch: 250, loss is 3.840673842430115 and perplexity is 46.556835843683594
At time: 326.97521710395813 and batch: 300, loss is 3.680111541748047 and perplexity is 39.650816547311905
At time: 327.47756361961365 and batch: 350, loss is 3.7251081323623656 and perplexity is 41.47571736965983
At time: 327.97508335113525 and batch: 400, loss is 3.7469988584518434 and perplexity is 42.39366151329638
At time: 328.50126242637634 and batch: 450, loss is 3.791850357055664 and perplexity is 44.33836622824497
At time: 329.01052737236023 and batch: 500, loss is 3.8208611583709717 and perplexity is 45.643497681348556
At time: 329.5160231590271 and batch: 550, loss is 3.7156983709335325 and perplexity is 41.08727122310293
At time: 330.01809906959534 and batch: 600, loss is 3.6776910829544067 and perplexity is 39.55495943561861
At time: 330.5117585659027 and batch: 650, loss is 3.643494863510132 and perplexity is 38.225195395041
At time: 331.0189723968506 and batch: 700, loss is 3.7423652458190917 and perplexity is 42.19768010936354
At time: 331.5286693572998 and batch: 750, loss is 3.698572597503662 and perplexity is 40.3896109624993
At time: 332.02423763275146 and batch: 800, loss is 3.825310049057007 and perplexity is 45.84701298605785
At time: 332.5205035209656 and batch: 850, loss is 3.6921852970123292 and perplexity is 40.13245252893512
At time: 333.0233860015869 and batch: 900, loss is 3.6301487922668456 and perplexity is 37.718428410140696
At time: 333.5283029079437 and batch: 950, loss is 3.6670733308792114 and perplexity is 39.137196459599075
At time: 334.0340600013733 and batch: 1000, loss is 3.5883906078338623 and perplexity is 36.175807989091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331401917992569 and perplexity of 76.05082889531381
Finished 30 epochs...
Completing Train Step...
At time: 335.5490481853485 and batch: 50, loss is 3.791388258934021 and perplexity is 44.31788228565438
At time: 336.07624411582947 and batch: 100, loss is 3.6848313426971435 and perplexity is 39.83840284564811
At time: 336.59320092201233 and batch: 150, loss is 3.80585328578949 and perplexity is 44.96360055013874
At time: 337.10015845298767 and batch: 200, loss is 3.851563563346863 and perplexity is 47.06659733543884
At time: 337.62274956703186 and batch: 250, loss is 3.8393830490112304 and perplexity is 46.49677935497126
At time: 338.1238524913788 and batch: 300, loss is 3.6787783908843994 and perplexity is 39.59799124685881
At time: 338.6277618408203 and batch: 350, loss is 3.7239185619354247 and perplexity is 41.42640841689139
At time: 339.1348738670349 and batch: 400, loss is 3.74580304145813 and perplexity is 42.34299675136214
At time: 339.6384816169739 and batch: 450, loss is 3.7907969999313353 and perplexity is 44.2916866837289
At time: 340.14186453819275 and batch: 500, loss is 3.819886465072632 and perplexity is 45.599030944281445
At time: 340.64361476898193 and batch: 550, loss is 3.714800229072571 and perplexity is 41.05038559160916
At time: 341.15619111061096 and batch: 600, loss is 3.676981291770935 and perplexity is 39.526893635755584
At time: 341.65686321258545 and batch: 650, loss is 3.6429044151306154 and perplexity is 38.202632052263404
At time: 342.1715295314789 and batch: 700, loss is 3.7418573570251463 and perplexity is 42.176253822051684
At time: 342.6808702945709 and batch: 750, loss is 3.6983348608016966 and perplexity is 40.38001001088991
At time: 343.20206117630005 and batch: 800, loss is 3.825156021118164 and perplexity is 45.839951808968735
At time: 343.7113745212555 and batch: 850, loss is 3.6920940399169924 and perplexity is 40.128790324992174
At time: 344.2115409374237 and batch: 900, loss is 3.6302125453948975 and perplexity is 37.72083315459122
At time: 344.70973801612854 and batch: 950, loss is 3.6672902393341062 and perplexity is 39.145686569167026
At time: 345.2118067741394 and batch: 1000, loss is 3.5886630249023437 and perplexity is 36.18566423909775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331305154939977 and perplexity of 76.0434703409816
Finished 31 epochs...
Completing Train Step...
At time: 346.7368495464325 and batch: 50, loss is 3.790338258743286 and perplexity is 44.27137292249462
At time: 347.24396085739136 and batch: 100, loss is 3.683720474243164 and perplexity is 39.79417219243712
At time: 347.74990797042847 and batch: 150, loss is 3.804712162017822 and perplexity is 44.91232078053782
At time: 348.24401211738586 and batch: 200, loss is 3.850495915412903 and perplexity is 47.01637359544197
At time: 348.758145570755 and batch: 250, loss is 3.838264389038086 and perplexity is 46.44479435121934
At time: 349.2544684410095 and batch: 300, loss is 3.6776359462738037 and perplexity is 39.552778566577445
At time: 349.76304507255554 and batch: 350, loss is 3.7228830337524412 and perplexity is 41.38353240694565
At time: 350.29033756256104 and batch: 400, loss is 3.7447801303863524 and perplexity is 42.29970577635654
At time: 350.7976930141449 and batch: 450, loss is 3.789896273612976 and perplexity is 44.251809957552744
At time: 351.3144872188568 and batch: 500, loss is 3.8190470600128172 and perplexity is 45.560770947048304
At time: 351.81473898887634 and batch: 550, loss is 3.714017071723938 and perplexity is 41.01824926601117
At time: 352.33979415893555 and batch: 600, loss is 3.6763537311553955 and perplexity is 39.50209589591145
At time: 352.8456287384033 and batch: 650, loss is 3.642372517585754 and perplexity is 38.18231756915836
At time: 353.3564438819885 and batch: 700, loss is 3.741386866569519 and perplexity is 42.15641496453682
At time: 353.88486075401306 and batch: 750, loss is 3.698082695007324 and perplexity is 40.36982883731455
At time: 354.3946464061737 and batch: 800, loss is 3.824971375465393 and perplexity is 45.83148844252994
At time: 354.903822183609 and batch: 850, loss is 3.6919467782974245 and perplexity is 40.122881329432424
At time: 355.5865902900696 and batch: 900, loss is 3.6301689529418946 and perplexity is 37.71918884678466
At time: 356.08616280555725 and batch: 950, loss is 3.6673809719085693 and perplexity is 39.149238519224916
At time: 356.57557678222656 and batch: 1000, loss is 3.5887764835357667 and perplexity is 36.18977004802715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331229605325839 and perplexity of 76.03772550315249
Finished 32 epochs...
Completing Train Step...
At time: 358.0711159706116 and batch: 50, loss is 3.7893519830703735 and perplexity is 44.227730669558625
At time: 358.5663981437683 and batch: 100, loss is 3.682708988189697 and perplexity is 39.7539412921825
At time: 359.07762455940247 and batch: 150, loss is 3.803683948516846 and perplexity is 44.86616505898552
At time: 359.6011824607849 and batch: 200, loss is 3.8495379877090454 and perplexity is 46.97135687346492
At time: 360.1232862472534 and batch: 250, loss is 3.837261142730713 and perplexity is 46.3982221483913
At time: 360.6237602233887 and batch: 300, loss is 3.6766166257858277 and perplexity is 39.512482149998704
At time: 361.1279275417328 and batch: 350, loss is 3.721949186325073 and perplexity is 41.344904540745944
At time: 361.6348087787628 and batch: 400, loss is 3.7438655614852907 and perplexity is 42.26103746604176
At time: 362.1348252296448 and batch: 450, loss is 3.789085750579834 and perplexity is 44.21595737795747
At time: 362.64172625541687 and batch: 500, loss is 3.818290629386902 and perplexity is 45.52632041592474
At time: 363.13885951042175 and batch: 550, loss is 3.71330304145813 and perplexity is 40.988971448452624
At time: 363.64424228668213 and batch: 600, loss is 3.6757725048065186 and perplexity is 39.47914290802773
At time: 364.1518430709839 and batch: 650, loss is 3.641870517730713 and perplexity is 38.16315486151415
At time: 364.6551339626312 and batch: 700, loss is 3.740934519767761 and perplexity is 42.13734995737662
At time: 365.1705298423767 and batch: 750, loss is 3.6978127574920654 and perplexity is 40.35893297669372
At time: 365.68004751205444 and batch: 800, loss is 3.824760317802429 and perplexity is 45.821816376406844
At time: 366.1789243221283 and batch: 850, loss is 3.6917561006546022 and perplexity is 40.11523152234406
At time: 366.69700717926025 and batch: 900, loss is 3.630050387382507 and perplexity is 37.714716915173184
At time: 367.22680926322937 and batch: 950, loss is 3.6673850536346437 and perplexity is 39.149398316018704
At time: 367.7301995754242 and batch: 1000, loss is 3.588785810470581 and perplexity is 36.19010758922744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331169686666349 and perplexity of 76.033169561064
Finished 33 epochs...
Completing Train Step...
At time: 369.2205853462219 and batch: 50, loss is 3.7884187412261965 and perplexity is 44.186474754492224
At time: 369.71696853637695 and batch: 100, loss is 3.681767921447754 and perplexity is 39.716547777827294
At time: 370.2202718257904 and batch: 150, loss is 3.8027351808547976 and perplexity is 44.823617679437966
At time: 370.72637391090393 and batch: 200, loss is 3.8486569499969483 and perplexity is 46.92999156154665
At time: 371.2436218261719 and batch: 250, loss is 3.8363401651382447 and perplexity is 46.35551009690247
At time: 371.7577335834503 and batch: 300, loss is 3.6756829118728636 and perplexity is 39.475606014239126
At time: 372.2616996765137 and batch: 350, loss is 3.7210878801345824 and perplexity is 41.30930924994244
At time: 372.76902627944946 and batch: 400, loss is 3.743025221824646 and perplexity is 42.22553875773431
At time: 373.27321314811707 and batch: 450, loss is 3.7883357000350952 and perplexity is 44.182805609345245
At time: 373.7766981124878 and batch: 500, loss is 3.8175889158248903 and perplexity is 45.494385185462974
At time: 374.28906178474426 and batch: 550, loss is 3.7126346588134767 and perplexity is 40.961584284886264
At time: 374.79141306877136 and batch: 600, loss is 3.6752199029922483 and perplexity is 39.4573326887693
At time: 375.30547618865967 and batch: 650, loss is 3.641385221481323 and perplexity is 38.144638918817144
At time: 375.83045721054077 and batch: 700, loss is 3.740492105484009 and perplexity is 42.11871191504808
At time: 376.3339285850525 and batch: 750, loss is 3.6975270080566407 and perplexity is 40.34740208193308
At time: 376.87847352027893 and batch: 800, loss is 3.824528498649597 and perplexity is 45.81119523289305
At time: 377.38345742225647 and batch: 850, loss is 3.6915320491790773 and perplexity is 40.10624465232879
At time: 377.88395071029663 and batch: 900, loss is 3.6298776483535766 and perplexity is 37.70820267424489
At time: 378.3888382911682 and batch: 950, loss is 3.6673268365859983 and perplexity is 39.14711921993427
At time: 378.89829206466675 and batch: 1000, loss is 3.5887232065200805 and perplexity is 36.18784201644096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331120932974467 and perplexity of 76.02946275370343
Finished 34 epochs...
Completing Train Step...
At time: 380.42616152763367 and batch: 50, loss is 3.7875288915634155 and perplexity is 44.14717292378623
At time: 380.9219934940338 and batch: 100, loss is 3.680879592895508 and perplexity is 39.68128210051273
At time: 381.4356174468994 and batch: 150, loss is 3.801845836639404 and perplexity is 44.7837717753302
At time: 381.93803811073303 and batch: 200, loss is 3.8478329467773436 and perplexity is 46.8913370253222
At time: 382.44590425491333 and batch: 250, loss is 3.8354805755615233 and perplexity is 46.31568050460325
At time: 382.96506214141846 and batch: 300, loss is 3.6748123455047605 and perplexity is 39.441254833943304
At time: 383.4737617969513 and batch: 350, loss is 3.7202805376052854 and perplexity is 41.27597194685115
At time: 383.9845139980316 and batch: 400, loss is 3.7422380161285402 and perplexity is 42.19231165310199
At time: 384.4819710254669 and batch: 450, loss is 3.787628378868103 and perplexity is 44.15156522551549
At time: 384.9790678024292 and batch: 500, loss is 3.8169253301620483 and perplexity is 45.464205778132545
At time: 385.4763469696045 and batch: 550, loss is 3.7119984912872312 and perplexity is 40.93553414214664
At time: 385.9882128238678 and batch: 600, loss is 3.6746870040893556 and perplexity is 39.43631152104445
At time: 386.48898220062256 and batch: 650, loss is 3.640910797119141 and perplexity is 38.12654646491721
At time: 386.99257254600525 and batch: 700, loss is 3.740055437088013 and perplexity is 42.100324019673295
At time: 387.4901533126831 and batch: 750, loss is 3.6972281169891357 and perplexity is 40.33534440590935
At time: 387.9978630542755 and batch: 800, loss is 3.8242797660827637 and perplexity is 45.799801913716564
At time: 388.4977066516876 and batch: 850, loss is 3.6912820529937744 and perplexity is 40.09621949733635
At time: 389.02563524246216 and batch: 900, loss is 3.629665331840515 and perplexity is 37.70019744999
At time: 389.56161856651306 and batch: 950, loss is 3.6672225427627563 and perplexity is 39.143036630100056
At time: 390.06680965423584 and batch: 1000, loss is 3.588608841896057 and perplexity is 36.183703644140735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331079622594322 and perplexity of 76.0263220125679
Finished 35 epochs...
Completing Train Step...
At time: 391.5731625556946 and batch: 50, loss is 3.7866751909255982 and perplexity is 44.109500536861724
At time: 392.1024854183197 and batch: 100, loss is 3.6800325775146483 and perplexity is 39.647685674594726
At time: 392.6024217605591 and batch: 150, loss is 3.8010027837753295 and perplexity is 44.74603259856669
At time: 393.0943524837494 and batch: 200, loss is 3.847052984237671 and perplexity is 46.854777798274455
At time: 393.609646320343 and batch: 250, loss is 3.8346685409545898 and perplexity is 46.278085835341905
At time: 394.11736369132996 and batch: 300, loss is 3.67399001121521 and perplexity is 39.40883426976824
At time: 394.6219861507416 and batch: 350, loss is 3.7195146703720092 and perplexity is 41.24437213459041
At time: 395.1224443912506 and batch: 400, loss is 3.741490864753723 and perplexity is 42.16079938312751
At time: 395.6258361339569 and batch: 450, loss is 3.786952409744263 and perplexity is 44.12173021556266
At time: 396.12837958335876 and batch: 500, loss is 3.8162900400161743 and perplexity is 45.43533198879973
At time: 396.6379323005676 and batch: 550, loss is 3.7113863945007326 and perplexity is 40.91048530018454
At time: 397.1451210975647 and batch: 600, loss is 3.674168562889099 and perplexity is 39.415871411321135
At time: 397.64516973495483 and batch: 650, loss is 3.6404436349868776 and perplexity is 38.108739345904524
At time: 398.1505534648895 and batch: 700, loss is 3.739622597694397 and perplexity is 42.082105284131245
At time: 398.65312600135803 and batch: 750, loss is 3.696918568611145 and perplexity is 40.32286059774378
At time: 399.170747756958 and batch: 800, loss is 3.8240177249908447 and perplexity is 45.78780205591001
At time: 399.6766166687012 and batch: 850, loss is 3.691011757850647 and perplexity is 40.085383148520656
At time: 400.1950843334198 and batch: 900, loss is 3.6294231462478637 and perplexity is 37.69106811086932
At time: 400.70830154418945 and batch: 950, loss is 3.667083010673523 and perplexity is 39.13757530144428
At time: 401.21662998199463 and batch: 1000, loss is 3.5884561204910277 and perplexity is 36.17817804003079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331047616353849 and perplexity of 76.02388873476353
Finished 36 epochs...
Completing Train Step...
At time: 402.74061822891235 and batch: 50, loss is 3.7858521604537962 and perplexity is 44.07321200915438
At time: 403.23444962501526 and batch: 100, loss is 3.679218544960022 and perplexity is 39.61542430042593
At time: 403.7338635921478 and batch: 150, loss is 3.800196900367737 and perplexity is 44.70998703954969
At time: 404.2235391139984 and batch: 200, loss is 3.8463079023361204 and perplexity is 46.8198801537519
At time: 404.71797728538513 and batch: 250, loss is 3.833894143104553 and perplexity is 46.24226205788559
At time: 405.22278571128845 and batch: 300, loss is 3.673205966949463 and perplexity is 39.37794810888013
At time: 405.7292642593384 and batch: 350, loss is 3.718781337738037 and perplexity is 41.21413737795824
At time: 406.22364020347595 and batch: 400, loss is 3.7407745933532714 and perplexity is 42.1306116209152
At time: 406.7321825027466 and batch: 450, loss is 3.7863009691238405 and perplexity is 44.092996888301336
At time: 407.23466205596924 and batch: 500, loss is 3.81567636013031 and perplexity is 45.407457793242
At time: 407.7447409629822 and batch: 550, loss is 3.7107931184768677 and perplexity is 40.886221288471155
At time: 408.24682426452637 and batch: 600, loss is 3.6736616134643554 and perplexity is 39.39589462202215
At time: 408.74894762039185 and batch: 650, loss is 3.639981813430786 and perplexity is 38.09114397187334
At time: 409.2671585083008 and batch: 700, loss is 3.739192690849304 and perplexity is 42.06401778726162
At time: 409.7621397972107 and batch: 750, loss is 3.696600708961487 and perplexity is 40.310045624190366
At time: 410.2684841156006 and batch: 800, loss is 3.823744983673096 and perplexity is 45.775315533313176
At time: 410.773574590683 and batch: 850, loss is 3.690725498199463 and perplexity is 40.07390996295638
At time: 411.28060698509216 and batch: 900, loss is 3.6291577577590943 and perplexity is 37.681066662456615
At time: 411.78058075904846 and batch: 950, loss is 3.6669159936904907 and perplexity is 39.13103920752878
At time: 412.3011567592621 and batch: 1000, loss is 3.588274302482605 and perplexity is 36.17160079370022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331019331769245 and perplexity of 76.02173846106068
Finished 37 epochs...
Completing Train Step...
At time: 413.85427379608154 and batch: 50, loss is 3.7850556421279906 and perplexity is 44.03812086533681
At time: 414.39665484428406 and batch: 100, loss is 3.6784318542480468 and perplexity is 39.58427146950585
At time: 414.8980014324188 and batch: 150, loss is 3.799421682357788 and perplexity is 44.67534048342431
At time: 415.4012532234192 and batch: 200, loss is 3.845591650009155 and perplexity is 46.786357312483226
At time: 415.90853095054626 and batch: 250, loss is 3.8331504821777345 and perplexity is 46.20788627797185
At time: 416.4104976654053 and batch: 300, loss is 3.6724531745910642 and perplexity is 39.3483158453245
At time: 416.9273648262024 and batch: 350, loss is 3.7180741310119627 and perplexity is 41.18500076681325
At time: 417.4267418384552 and batch: 400, loss is 3.7400830364227295 and perplexity is 42.101485976642145
At time: 417.9428493976593 and batch: 450, loss is 3.7856698703765868 and perplexity is 44.06517863215907
At time: 418.4557104110718 and batch: 500, loss is 3.8150800895690917 and perplexity is 45.38039073334758
At time: 418.95305132865906 and batch: 550, loss is 3.7102147722244263 and perplexity is 40.86258173219579
At time: 419.45455622673035 and batch: 600, loss is 3.6731642580032347 and perplexity is 39.376305730411055
At time: 419.9692678451538 and batch: 650, loss is 3.6395240116119383 and perplexity is 38.07370976789067
At time: 420.48498034477234 and batch: 700, loss is 3.7387651014328003 and perplexity is 42.04603550323131
At time: 420.9946975708008 and batch: 750, loss is 3.696275863647461 and perplexity is 40.29695322137916
At time: 421.49594140052795 and batch: 800, loss is 3.8234635019302368 and perplexity is 45.76243243098104
At time: 421.9949927330017 and batch: 850, loss is 3.690426383018494 and perplexity is 40.06192504065112
At time: 422.52793741226196 and batch: 900, loss is 3.6288745069503783 and perplexity is 37.6703949813037
At time: 423.0417830944061 and batch: 950, loss is 3.6667270040512085 and perplexity is 39.123644545323494
At time: 423.558278799057 and batch: 1000, loss is 3.5880697441101073 and perplexity is 36.16420234664417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330997746165206 and perplexity of 76.02009750362647
Finished 38 epochs...
Completing Train Step...
At time: 425.0633680820465 and batch: 50, loss is 3.784281768798828 and perplexity is 44.00405412150508
At time: 425.5990614891052 and batch: 100, loss is 3.6776680183410644 and perplexity is 39.554047126294535
At time: 426.1065037250519 and batch: 150, loss is 3.798672065734863 and perplexity is 44.641863654524435
At time: 426.6017744541168 and batch: 200, loss is 3.8448987293243406 and perplexity is 46.75394930711961
At time: 427.1113188266754 and batch: 250, loss is 3.8324323654174806 and perplexity is 46.17471553203953
At time: 427.61049604415894 and batch: 300, loss is 3.671726746559143 and perplexity is 39.3197425051797
At time: 428.1040782928467 and batch: 350, loss is 3.7173882341384887 and perplexity is 41.15676178917361
At time: 428.6278495788574 and batch: 400, loss is 3.7394116067886354 and perplexity is 42.07322727924308
At time: 429.14365434646606 and batch: 450, loss is 3.7850554895401003 and perplexity is 44.03811414565337
At time: 429.6430723667145 and batch: 500, loss is 3.8144981050491333 and perplexity is 45.35398773224531
At time: 430.13875222206116 and batch: 550, loss is 3.709649019241333 and perplexity is 40.83947014302563
At time: 430.63724541664124 and batch: 600, loss is 3.6726751947402954 and perplexity is 39.357052934149465
At time: 431.1312460899353 and batch: 650, loss is 3.639069633483887 and perplexity is 38.056413836662365
At time: 431.6331470012665 and batch: 700, loss is 3.7383397579193116 and perplexity is 42.02815529764639
At time: 432.12766551971436 and batch: 750, loss is 3.695945477485657 and perplexity is 40.28364186473704
At time: 432.64898681640625 and batch: 800, loss is 3.8231747817993162 and perplexity is 45.74922180267726
At time: 433.1431667804718 and batch: 850, loss is 3.6901166677474975 and perplexity is 40.04951917192319
At time: 433.64286255836487 and batch: 900, loss is 3.628576807975769 and perplexity is 37.65918221244235
At time: 434.1385281085968 and batch: 950, loss is 3.6665204477310183 and perplexity is 39.115564143831506
At time: 434.63820910453796 and batch: 1000, loss is 3.5878471088409425 and perplexity is 36.156151815919884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330979882217035 and perplexity of 76.0187394966744
Finished 39 epochs...
Completing Train Step...
At time: 436.11066937446594 and batch: 50, loss is 3.783527693748474 and perplexity is 43.970884270028115
At time: 436.6093955039978 and batch: 100, loss is 3.6769237899780274 and perplexity is 39.52462083384918
At time: 437.1132709980011 and batch: 150, loss is 3.7979442834854127 and perplexity is 44.60938591836775
At time: 437.60998487472534 and batch: 200, loss is 3.8442259931564333 and perplexity is 46.72250681186593
At time: 438.1304876804352 and batch: 250, loss is 3.8317357778549193 and perplexity is 46.14256199967064
At time: 438.63137102127075 and batch: 300, loss is 3.671023654937744 and perplexity is 39.2921068400095
At time: 439.1321885585785 and batch: 350, loss is 3.716718273162842 and perplexity is 41.129197599387616
At time: 439.6341254711151 and batch: 400, loss is 3.73875684261322 and perplexity is 42.04568825404396
At time: 440.142963886261 and batch: 450, loss is 3.7844549131393435 and perplexity is 44.01167383407196
At time: 440.6453928947449 and batch: 500, loss is 3.813928289413452 and perplexity is 45.32815168248659
At time: 441.15777826309204 and batch: 550, loss is 3.709093837738037 and perplexity is 40.81680311733671
At time: 441.65300035476685 and batch: 600, loss is 3.672193250656128 and perplexity is 39.33808960531671
At time: 442.16310691833496 and batch: 650, loss is 3.6386182069778443 and perplexity is 38.03923803982823
At time: 442.65646028518677 and batch: 700, loss is 3.7379163885116578 and perplexity is 42.01036562849981
At time: 443.1767358779907 and batch: 750, loss is 3.695610399246216 and perplexity is 40.27014595416199
At time: 443.6869864463806 and batch: 800, loss is 3.8228801250457765 and perplexity is 45.73574347134211
At time: 444.1871156692505 and batch: 850, loss is 3.689798936843872 and perplexity is 40.03679622335093
At time: 444.6875591278076 and batch: 900, loss is 3.628267555236816 and perplexity is 37.64753780782124
At time: 445.1947956085205 and batch: 950, loss is 3.6662991094589232 and perplexity is 39.106907330529275
At time: 445.69903469085693 and batch: 1000, loss is 3.587609915733337 and perplexity is 36.14757684291407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330964995593559 and perplexity of 76.01760784274572
Finished 40 epochs...
Completing Train Step...
At time: 447.1742408275604 and batch: 50, loss is 3.782791299819946 and perplexity is 43.93851629707174
At time: 447.689603805542 and batch: 100, loss is 3.6761965131759644 and perplexity is 39.49588594438221
At time: 448.1900463104248 and batch: 150, loss is 3.7972356128692626 and perplexity is 44.577783756447275
At time: 448.7009537220001 and batch: 200, loss is 3.8435702991485594 and perplexity is 46.691881185733564
At time: 449.2061948776245 and batch: 250, loss is 3.831057615280151 and perplexity is 46.11128044920583
At time: 449.7333073616028 and batch: 300, loss is 3.670340352058411 and perplexity is 39.26526760097959
At time: 450.25435423851013 and batch: 350, loss is 3.7160600519180296 and perplexity is 41.102134395510596
At time: 450.75753903388977 and batch: 400, loss is 3.738115892410278 and perplexity is 42.01874769632497
At time: 451.26081252098083 and batch: 450, loss is 3.7838637113571165 and perplexity is 43.985661744017115
At time: 451.77448558807373 and batch: 500, loss is 3.8133676528930662 and perplexity is 45.30274618754139
At time: 452.28075909614563 and batch: 550, loss is 3.7085475301742554 and perplexity is 40.79451067888264
At time: 452.79245138168335 and batch: 600, loss is 3.6717165756225585 and perplexity is 39.31934258860593
At time: 453.2996356487274 and batch: 650, loss is 3.6381720066070558 and perplexity is 38.02226870385389
At time: 453.7914078235626 and batch: 700, loss is 3.737495069503784 and perplexity is 41.99266959103304
At time: 454.3140847682953 and batch: 750, loss is 3.6952707672119143 and perplexity is 40.25647124488607
At time: 454.8235080242157 and batch: 800, loss is 3.8225812911987305 and perplexity is 45.72207812510875
At time: 455.3507807254791 and batch: 850, loss is 3.6894755220413207 and perplexity is 40.02384982444694
At time: 455.85283875465393 and batch: 900, loss is 3.6279497289657594 and perplexity is 37.63557433251939
At time: 456.35680961608887 and batch: 950, loss is 3.666064591407776 and perplexity is 39.097737130166514
At time: 456.86168456077576 and batch: 1000, loss is 3.5873606777191163 and perplexity is 36.138568615285855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330956435785061 and perplexity of 76.01695714936498
Finished 41 epochs...
Completing Train Step...
At time: 458.3880841732025 and batch: 50, loss is 3.782071256637573 and perplexity is 43.90689005546469
At time: 458.9209349155426 and batch: 100, loss is 3.67548535823822 and perplexity is 39.46780823505643
At time: 459.4204726219177 and batch: 150, loss is 3.7965441226959227 and perplexity is 44.546969312200964
At time: 459.9273681640625 and batch: 200, loss is 3.84293035030365 and perplexity is 46.66201032923159
At time: 460.4254894256592 and batch: 250, loss is 3.830395531654358 and perplexity is 46.08076102977485
At time: 460.94633960723877 and batch: 300, loss is 3.6696709489822386 and perplexity is 39.238992105490695
At time: 461.44710993766785 and batch: 350, loss is 3.715423274040222 and perplexity is 41.07596979699973
At time: 461.94568490982056 and batch: 400, loss is 3.7374893951416017 and perplexity is 41.992431310092826
At time: 462.4511835575104 and batch: 450, loss is 3.7832799434661863 and perplexity is 43.95999182039722
At time: 462.96113204956055 and batch: 500, loss is 3.812814826965332 and perplexity is 45.27770857620937
At time: 463.48138523101807 and batch: 550, loss is 3.7080097246170043 and perplexity is 40.77257706287315
At time: 463.9930567741394 and batch: 600, loss is 3.6712448692321775 and perplexity is 39.30079977716645
At time: 464.50279474258423 and batch: 650, loss is 3.637729997634888 and perplexity is 38.00546623363946
At time: 465.01635670661926 and batch: 700, loss is 3.7370756244659424 and perplexity is 41.97505966760304
At time: 465.5295948982239 and batch: 750, loss is 3.694928307533264 and perplexity is 40.242687387022436
At time: 466.0367314815521 and batch: 800, loss is 3.822278904914856 and perplexity is 45.708254485958236
At time: 466.5519688129425 and batch: 850, loss is 3.6891466569900513 and perplexity is 40.010689543119334
At time: 467.0926947593689 and batch: 900, loss is 3.6276237869262697 and perplexity is 37.62330931561498
At time: 467.58534955978394 and batch: 950, loss is 3.6658189058303834 and perplexity is 39.08813255994545
At time: 468.09289145469666 and batch: 1000, loss is 3.58710147857666 and perplexity is 36.12920274315663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3309478759765625 and perplexity of 76.01630646155404
Finished 42 epochs...
Completing Train Step...
At time: 469.65148878097534 and batch: 50, loss is 3.781366214752197 and perplexity is 43.87594476906239
At time: 470.1578106880188 and batch: 100, loss is 3.6747883796691894 and perplexity is 39.4403096026419
At time: 470.6706075668335 and batch: 150, loss is 3.7958679676055906 and perplexity is 44.51685883296573
At time: 471.19598865509033 and batch: 200, loss is 3.842303991317749 and perplexity is 46.632792311200994
At time: 471.69891381263733 and batch: 250, loss is 3.8297472190856934 and perplexity is 46.050895975216775
At time: 472.20848751068115 and batch: 300, loss is 3.6690149211883547 and perplexity is 39.213258677910275
At time: 472.7120921611786 and batch: 350, loss is 3.7148018789291384 and perplexity is 41.050453318913284
At time: 473.2285087108612 and batch: 400, loss is 3.7368758010864256 and perplexity is 41.96667290728814
At time: 473.7464723587036 and batch: 450, loss is 3.782707872390747 and perplexity is 43.93485077251902
At time: 474.2505738735199 and batch: 500, loss is 3.8122707653045653 and perplexity is 45.2530814108414
At time: 474.75142097473145 and batch: 550, loss is 3.7074800157546997 and perplexity is 40.750985186672914
At time: 475.2469263076782 and batch: 600, loss is 3.6707782554626465 and perplexity is 39.28246576062178
At time: 475.7471239566803 and batch: 650, loss is 3.6372898530960085 and perplexity is 37.98874201603568
At time: 476.2394654750824 and batch: 700, loss is 3.736657748222351 and perplexity is 41.95752295168829
At time: 476.73676347732544 and batch: 750, loss is 3.6945839595794676 and perplexity is 40.228832285590265
At time: 477.2301833629608 and batch: 800, loss is 3.8219735431671142 and perplexity is 45.6942990643166
At time: 477.7279460430145 and batch: 850, loss is 3.6888126516342163 and perplexity is 39.99732799006064
At time: 478.2243208885193 and batch: 900, loss is 3.6272907638549805 and perplexity is 37.61078197165769
At time: 478.74205493927 and batch: 950, loss is 3.6655653429031374 and perplexity is 39.07822251509605
At time: 479.25517749786377 and batch: 1000, loss is 3.586834168434143 and perplexity is 36.11954633150822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330943037823933 and perplexity of 76.01593868395072
Finished 43 epochs...
Completing Train Step...
At time: 480.731130361557 and batch: 50, loss is 3.7806748247146604 and perplexity is 43.845619862338744
At time: 481.2672154903412 and batch: 100, loss is 3.674103708267212 and perplexity is 39.413315192776594
At time: 481.7659709453583 and batch: 150, loss is 3.7952050018310546 and perplexity is 44.487355460113456
At time: 482.27224683761597 and batch: 200, loss is 3.841688985824585 and perplexity is 46.60412170496188
At time: 482.7722158432007 and batch: 250, loss is 3.829111671447754 and perplexity is 46.02163773554213
At time: 483.2848834991455 and batch: 300, loss is 3.668370313644409 and perplexity is 39.18798966071834
At time: 483.7828199863434 and batch: 350, loss is 3.714190969467163 and perplexity is 41.025382867230164
At time: 484.28604769706726 and batch: 400, loss is 3.7362719202041625 and perplexity is 41.94133768632496
At time: 484.79406237602234 and batch: 450, loss is 3.7821439599990843 and perplexity is 43.91008235000909
At time: 485.29653668403625 and batch: 500, loss is 3.811733675003052 and perplexity is 45.22878294552119
At time: 485.80281496047974 and batch: 550, loss is 3.706956911087036 and perplexity is 40.7296737306569
At time: 486.3008782863617 and batch: 600, loss is 3.6703164339065553 and perplexity is 39.26432845957777
At time: 486.81586146354675 and batch: 650, loss is 3.636851978302002 and perplexity is 37.97211134479244
At time: 487.3201322555542 and batch: 700, loss is 3.7362418365478516 and perplexity is 41.9400759565156
At time: 487.82561707496643 and batch: 750, loss is 3.694237699508667 and perplexity is 40.21490505863528
At time: 488.3279564380646 and batch: 800, loss is 3.821665506362915 and perplexity is 45.68022570613013
At time: 488.83994722366333 and batch: 850, loss is 3.6884748458862306 and perplexity is 39.9838189446066
At time: 489.3469934463501 and batch: 900, loss is 3.626952357292175 and perplexity is 37.59805638953793
At time: 489.8740038871765 and batch: 950, loss is 3.6653041458129882 and perplexity is 39.06801673000562
At time: 490.3906264305115 and batch: 1000, loss is 3.5865598249435426 and perplexity is 36.10963852822154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330940804830411 and perplexity of 76.01576894104161
Finished 44 epochs...
Completing Train Step...
At time: 491.8854913711548 and batch: 50, loss is 3.7799957275390623 and perplexity is 43.81585453364712
At time: 492.40300989151 and batch: 100, loss is 3.67343008518219 and perplexity is 39.386774414049725
At time: 492.9075222015381 and batch: 150, loss is 3.7945539379119873 and perplexity is 44.45840077481688
At time: 493.4287414550781 and batch: 200, loss is 3.8410845136642457 and perplexity is 46.575959323379486
At time: 493.9197964668274 and batch: 250, loss is 3.8284871673583982 and perplexity is 45.99290600705219
At time: 494.42313051223755 and batch: 300, loss is 3.6677366590499876 and perplexity is 39.16316587670668
At time: 494.9184637069702 and batch: 350, loss is 3.713589835166931 and perplexity is 41.00072851343981
At time: 495.4338467121124 and batch: 400, loss is 3.73567723274231 and perplexity is 41.91640311354332
At time: 495.94328331947327 and batch: 450, loss is 3.7815877056121825 and perplexity is 43.885663966118884
At time: 496.4591543674469 and batch: 500, loss is 3.811203188896179 and perplexity is 45.204796067453124
At time: 496.96237659454346 and batch: 550, loss is 3.706440110206604 and perplexity is 40.70863003758111
At time: 497.4774799346924 and batch: 600, loss is 3.669858980178833 and perplexity is 39.24637095383454
At time: 497.9828746318817 and batch: 650, loss is 3.6364160060882567 and perplexity is 37.95556016753772
At time: 498.4826214313507 and batch: 700, loss is 3.7358275270462036 and perplexity is 41.92270338360649
At time: 499.0010063648224 and batch: 750, loss is 3.6938900899887086 and perplexity is 40.20092840414249
At time: 499.5095932483673 and batch: 800, loss is 3.821355299949646 and perplexity is 45.66605760478808
At time: 500.02545857429504 and batch: 850, loss is 3.6881338119506837 and perplexity is 39.970185430351364
At time: 500.53349590301514 and batch: 900, loss is 3.626609373092651 and perplexity is 37.5851630614938
At time: 501.04423332214355 and batch: 950, loss is 3.665036664009094 and perplexity is 39.05756814388174
At time: 501.54687237739563 and batch: 1000, loss is 3.586279845237732 and perplexity is 36.09952997741005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3309385718368905 and perplexity of 76.01559919851161
Finished 45 epochs...
Completing Train Step...
At time: 503.1528265476227 and batch: 50, loss is 3.7793279886245728 and perplexity is 43.786606748532265
At time: 503.6642825603485 and batch: 100, loss is 3.6727665519714354 and perplexity is 39.360648649776174
At time: 504.16186261177063 and batch: 150, loss is 3.793913917541504 and perplexity is 44.429955596399694
At time: 504.6667323112488 and batch: 200, loss is 3.840489721298218 and perplexity is 46.54826453547422
At time: 505.1667137145996 and batch: 250, loss is 3.8278730154037475 and perplexity is 45.96466804601495
At time: 505.6689567565918 and batch: 300, loss is 3.6671128463745117 and perplexity is 39.138743015858104
At time: 506.1701145172119 and batch: 350, loss is 3.7129973125457765 and perplexity is 40.976441850221015
At time: 506.6789062023163 and batch: 400, loss is 3.735090913772583 and perplexity is 41.891833934646804
At time: 507.1844804286957 and batch: 450, loss is 3.7810383558273317 and perplexity is 43.861562006870265
At time: 507.7013065814972 and batch: 500, loss is 3.8106786155700685 and perplexity is 45.181089055800484
At time: 508.2027904987335 and batch: 550, loss is 3.7059285640716553 and perplexity is 40.687811020624146
At time: 508.7218849658966 and batch: 600, loss is 3.669405641555786 and perplexity is 39.22858309033416
At time: 509.2417960166931 and batch: 650, loss is 3.635982131958008 and perplexity is 37.939095803870934
At time: 509.757910490036 and batch: 700, loss is 3.7354147958755495 and perplexity is 41.905404147374576
At time: 510.2818009853363 and batch: 750, loss is 3.693541417121887 and perplexity is 40.18691387457201
At time: 510.80396795272827 and batch: 800, loss is 3.8210433864593507 and perplexity is 45.65181596656755
At time: 511.32407784461975 and batch: 850, loss is 3.6877902507781983 and perplexity is 39.95645558523635
At time: 511.81951427459717 and batch: 900, loss is 3.626262559890747 and perplexity is 37.572130290847454
At time: 512.3366103172302 and batch: 950, loss is 3.66476363658905 and perplexity is 39.04690581243888
At time: 512.8485372066498 and batch: 1000, loss is 3.585994772911072 and perplexity is 36.08924046710506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.330940060499238 and perplexity of 76.01571236015616
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 514.3619422912598 and batch: 50, loss is 3.7807542848587037 and perplexity is 43.84910398003108
At time: 514.9047029018402 and batch: 100, loss is 3.675237340927124 and perplexity is 39.45802074916625
At time: 515.4086077213287 and batch: 150, loss is 3.796522355079651 and perplexity is 44.54599964142066
At time: 515.9126162528992 and batch: 200, loss is 3.844051914215088 and perplexity is 46.71437411522927
At time: 516.408694267273 and batch: 250, loss is 3.83052604675293 and perplexity is 46.08677565733423
At time: 516.9176270961761 and batch: 300, loss is 3.6679462671279905 and perplexity is 39.17137565302227
At time: 517.4254786968231 and batch: 350, loss is 3.715328087806702 and perplexity is 41.07206011622341
At time: 517.9243538379669 and batch: 400, loss is 3.7379646492004395 and perplexity is 42.01239312660485
At time: 518.4185028076172 and batch: 450, loss is 3.7837748241424563 and perplexity is 43.98175215481852
At time: 518.9231145381927 and batch: 500, loss is 3.8106266260147095 and perplexity is 45.178740172129075
At time: 519.4641668796539 and batch: 550, loss is 3.7058612632751466 and perplexity is 40.68507279067783
At time: 519.9821064472198 and batch: 600, loss is 3.6695929288864138 and perplexity is 39.235930794989976
At time: 520.4882533550262 and batch: 650, loss is 3.635274901390076 and perplexity is 37.912273601457436
At time: 520.9900093078613 and batch: 700, loss is 3.7337309217453 and perplexity is 41.834900098044656
At time: 521.5056805610657 and batch: 750, loss is 3.691605553627014 and perplexity is 40.10919274804852
At time: 522.0068383216858 and batch: 800, loss is 3.820051522254944 and perplexity is 45.606558013026564
At time: 522.5102808475494 and batch: 850, loss is 3.685144386291504 and perplexity is 39.85087595468007
At time: 523.0100166797638 and batch: 900, loss is 3.6219705390930175 and perplexity is 37.41121549806624
At time: 523.5233154296875 and batch: 950, loss is 3.661740236282349 and perplexity is 38.92902966860616
At time: 524.0238955020905 and batch: 1000, loss is 3.580812120437622 and perplexity is 35.902686315968126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.329676930497333 and perplexity of 75.91975524916903
Finished 47 epochs...
Completing Train Step...
At time: 525.532749414444 and batch: 50, loss is 3.780706672668457 and perplexity is 43.84701627785066
At time: 526.044403553009 and batch: 100, loss is 3.6746965074539184 and perplexity is 39.43668630047068
At time: 526.5482065677643 and batch: 150, loss is 3.79537850856781 and perplexity is 44.495074985661695
At time: 527.046549320221 and batch: 200, loss is 3.8424503564834596 and perplexity is 46.63961822710114
At time: 527.540981054306 and batch: 250, loss is 3.8297047853469848 and perplexity is 46.04894190498921
At time: 528.0396790504456 and batch: 300, loss is 3.6668023920059203 and perplexity is 39.126594108045985
At time: 528.540036201477 and batch: 350, loss is 3.714293646812439 and perplexity is 41.02959546089716
At time: 529.0396709442139 and batch: 400, loss is 3.736790180206299 and perplexity is 41.96307983764092
At time: 529.5390486717224 and batch: 450, loss is 3.782728433609009 and perplexity is 43.93575413586215
At time: 530.0452663898468 and batch: 500, loss is 3.8099621963500976 and perplexity is 45.14873204719707
At time: 530.5523784160614 and batch: 550, loss is 3.705208101272583 and perplexity is 40.65850752371535
At time: 531.0513582229614 and batch: 600, loss is 3.668821444511414 and perplexity is 39.20567256082128
At time: 531.545731306076 and batch: 650, loss is 3.634979476928711 and perplexity is 37.90107504269516
At time: 532.0807876586914 and batch: 700, loss is 3.733337435722351 and perplexity is 41.81844188783481
At time: 532.5851974487305 and batch: 750, loss is 3.691980233192444 and perplexity is 40.1242236586688
At time: 533.0828721523285 and batch: 800, loss is 3.819537811279297 and perplexity is 45.583135440345224
At time: 533.5860388278961 and batch: 850, loss is 3.6851230669021606 and perplexity is 39.850026367396296
At time: 534.0788879394531 and batch: 900, loss is 3.622308382987976 and perplexity is 37.42385678409565
At time: 534.579184293747 and batch: 950, loss is 3.6622699069976807 and perplexity is 38.94965469735258
At time: 535.0880470275879 and batch: 1000, loss is 3.581355013847351 and perplexity is 35.92218293957601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.329417158917683 and perplexity of 75.90003601578043
Finished 48 epochs...
Completing Train Step...
At time: 536.625729560852 and batch: 50, loss is 3.7805326795578003 and perplexity is 43.83938786276055
At time: 537.121505022049 and batch: 100, loss is 3.674391541481018 and perplexity is 39.4246612867683
At time: 537.6295087337494 and batch: 150, loss is 3.7949536418914795 and perplexity is 44.47617452641143
At time: 538.1296274662018 and batch: 200, loss is 3.8418523502349853 and perplexity is 46.61173578174407
At time: 538.6342623233795 and batch: 250, loss is 3.8293102312088014 and perplexity is 46.03077668821801
At time: 539.1383142471313 and batch: 300, loss is 3.666337261199951 and perplexity is 39.108399355591644
At time: 539.6379888057709 and batch: 350, loss is 3.7139101886749266 and perplexity is 41.01386534475212
At time: 540.139021396637 and batch: 400, loss is 3.7364120626449586 and perplexity is 41.94721585963973
At time: 540.6435792446136 and batch: 450, loss is 3.7823266649246214 and perplexity is 43.918105671263014
At time: 541.160637140274 and batch: 500, loss is 3.8096809720993043 and perplexity is 45.136036914026164
At time: 541.6578876972198 and batch: 550, loss is 3.704891266822815 and perplexity is 40.645627548373376
At time: 542.1571872234344 and batch: 600, loss is 3.6684637689590454 and perplexity is 39.191652157759286
At time: 542.654646396637 and batch: 650, loss is 3.6348262643814087 and perplexity is 37.89526856726617
At time: 543.1769006252289 and batch: 700, loss is 3.733173475265503 and perplexity is 41.81158587907071
At time: 543.6766192913055 and batch: 750, loss is 3.6921435403823852 and perplexity is 40.130776767953435
At time: 544.1890337467194 and batch: 800, loss is 3.8194645929336546 and perplexity is 45.57979804076002
At time: 544.7038986682892 and batch: 850, loss is 3.685198254585266 and perplexity is 39.85302271119321
At time: 545.2331857681274 and batch: 900, loss is 3.622543787956238 and perplexity is 37.43266758292638
At time: 545.7361898422241 and batch: 950, loss is 3.662575950622559 and perplexity is 38.961576815114924
At time: 546.2338311672211 and batch: 1000, loss is 3.5816177845001222 and perplexity is 35.93162347532956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.329344958793826 and perplexity of 75.89455622160267
Finished 49 epochs...
Completing Train Step...
At time: 547.7480313777924 and batch: 50, loss is 3.7803440284729004 and perplexity is 43.831118294734914
At time: 548.2676050662994 and batch: 100, loss is 3.67412917137146 and perplexity is 39.41431879090741
At time: 548.7688210010529 and batch: 150, loss is 3.794676547050476 and perplexity is 44.463852115219716
At time: 549.2821979522705 and batch: 200, loss is 3.841505222320557 and perplexity is 46.595558355094894
At time: 549.7789590358734 and batch: 250, loss is 3.8290313720703124 and perplexity is 46.01794237505268
At time: 550.2839186191559 and batch: 300, loss is 3.6660453605651857 and perplexity is 39.09698525496775
At time: 550.7847986221313 and batch: 350, loss is 3.713670029640198 and perplexity is 41.004016677111025
At time: 551.3008530139923 and batch: 400, loss is 3.7361954975128175 and perplexity is 41.93813253889492
At time: 551.8077108860016 and batch: 450, loss is 3.7820930099487304 and perplexity is 43.907845186094605
At time: 552.308343410492 and batch: 500, loss is 3.809504871368408 and perplexity is 45.128089124762134
At time: 552.8119683265686 and batch: 550, loss is 3.7046858215332032 and perplexity is 40.6372779533721
At time: 553.3140075206757 and batch: 600, loss is 3.6682425689697267 and perplexity is 39.18298392346262
At time: 553.8149843215942 and batch: 650, loss is 3.6347138833999635 and perplexity is 37.89101009908235
At time: 554.3140120506287 and batch: 700, loss is 3.7330780029296875 and perplexity is 41.80759421985224
At time: 554.8102922439575 and batch: 750, loss is 3.69222589969635 and perplexity is 40.13408204730531
At time: 555.3142411708832 and batch: 800, loss is 3.8194748973846435 and perplexity is 45.58026771797489
At time: 555.8203766345978 and batch: 850, loss is 3.685266833305359 and perplexity is 39.855755874199915
At time: 556.3263063430786 and batch: 900, loss is 3.622698230743408 and perplexity is 37.438449234894826
At time: 556.8278818130493 and batch: 950, loss is 3.6627583503723145 and perplexity is 38.968684045134864
At time: 557.3338208198547 and batch: 1000, loss is 3.581760673522949 and perplexity is 35.93675807672688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.329318535037157 and perplexity of 75.89255082881164
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
SETTINGS FOR THIS RUN
{'lr': 26.71583680783388, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.7543925472838893, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 3.1137667707572545, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7301793098449707 and batch: 50, loss is 6.664792995452881 and perplexity is 784.3010942829939
At time: 1.2528564929962158 and batch: 100, loss is 6.195311117172241 and perplexity is 490.44400681702984
At time: 1.759709119796753 and batch: 150, loss is 6.069155111312866 and perplexity is 432.31526894970597
At time: 2.263747215270996 and batch: 200, loss is 6.074150171279907 and perplexity is 434.4801119017797
At time: 2.7667551040649414 and batch: 250, loss is 6.176556749343872 and perplexity is 481.3317538690913
At time: 3.270730972290039 and batch: 300, loss is 6.149682455062866 and perplexity is 468.5685715783051
At time: 3.7740328311920166 and batch: 350, loss is 6.1570298290252685 and perplexity is 472.02399871362803
At time: 4.284966230392456 and batch: 400, loss is 6.152077226638794 and perplexity is 469.6920309522685
At time: 4.794510841369629 and batch: 450, loss is 6.203800821304322 and perplexity is 494.6254558459835
At time: 5.299118995666504 and batch: 500, loss is 6.165742883682251 and perplexity is 476.154739164626
At time: 5.808192729949951 and batch: 550, loss is 6.177831287384033 and perplexity is 481.94562061449477
At time: 6.311178922653198 and batch: 600, loss is 6.124627599716186 and perplexity is 456.97450414287414
At time: 6.824443817138672 and batch: 650, loss is 6.0722055339813235 and perplexity is 433.63602665650865
At time: 7.338142156600952 and batch: 700, loss is 6.194240989685059 and perplexity is 489.91944992584416
At time: 7.856299638748169 and batch: 750, loss is 6.066651906967163 and perplexity is 431.23444881096253
At time: 8.367087364196777 and batch: 800, loss is 6.2278995704650875 and perplexity is 506.6900981568506
At time: 8.875448942184448 and batch: 850, loss is 6.29385890007019 and perplexity is 541.237887368363
At time: 9.383110523223877 and batch: 900, loss is 6.6066247081756595 and perplexity is 739.9811465034845
At time: 9.875218629837036 and batch: 950, loss is 6.579844274520874 and perplexity is 720.4271316531226
At time: 10.375439643859863 and batch: 1000, loss is 6.392217397689819 and perplexity is 597.1792966440748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.705162234422637 and perplexity of 816.6104978991244
Finished 1 epochs...
Completing Train Step...
At time: 11.869032382965088 and batch: 50, loss is 6.8761098670959475 and perplexity is 968.8500638950358
At time: 12.362911462783813 and batch: 100, loss is 7.116962232589722 and perplexity is 1232.7000838633548
At time: 12.857234001159668 and batch: 150, loss is 6.8396111106872555 and perplexity is 934.1257925463632
At time: 13.379488706588745 and batch: 200, loss is 6.847859869003296 and perplexity is 941.8630379079376
At time: 13.884480237960815 and batch: 250, loss is 7.076634998321533 and perplexity is 1183.977720736674
At time: 14.386600971221924 and batch: 300, loss is 6.727720680236817 and perplexity is 835.2413126098878
At time: 14.891170740127563 and batch: 350, loss is 6.718548402786255 and perplexity is 827.6152750837092
At time: 15.389398097991943 and batch: 400, loss is 6.6986194896698 and perplexity is 811.2850643225429
At time: 15.888917684555054 and batch: 450, loss is 6.835406999588013 and perplexity is 930.2068675040656
At time: 16.380791664123535 and batch: 500, loss is 7.041005249023438 and perplexity is 1142.535562299884
At time: 16.877536058425903 and batch: 550, loss is 7.17484829902649 and perplexity is 1306.1619399214803
At time: 17.370952367782593 and batch: 600, loss is 6.92686502456665 and perplexity is 1019.2935054409447
At time: 17.873038053512573 and batch: 650, loss is 6.679165658950805 and perplexity is 795.6549873842396
At time: 18.36959195137024 and batch: 700, loss is 6.851069087982178 and perplexity is 944.8905380017057
At time: 18.865487337112427 and batch: 750, loss is 6.6334850788116455 and perplexity is 760.126661157229
At time: 19.35478377342224 and batch: 800, loss is 6.6807275485992434 and perplexity is 796.8986836781138
At time: 19.851619482040405 and batch: 850, loss is 6.861607074737549 and perplexity is 954.9004314061486
At time: 20.349119186401367 and batch: 900, loss is 6.728893346786499 and perplexity is 836.2213466723721
At time: 20.846290111541748 and batch: 950, loss is 6.646082906723023 and perplexity is 769.7631781700027
At time: 21.342849016189575 and batch: 1000, loss is 6.460813179016113 and perplexity is 639.5809389608477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.79659736447218 and perplexity of 894.7974362730051
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 22.80155301094055 and batch: 50, loss is 6.203581047058106 and perplexity is 494.5167618537733
At time: 23.30916976928711 and batch: 100, loss is 6.201494731903076 and perplexity is 493.4861195352866
At time: 23.809001207351685 and batch: 150, loss is 6.141494150161743 and perplexity is 464.7474548315675
At time: 24.30367374420166 and batch: 200, loss is 6.166185464859009 and perplexity is 476.36552293042723
At time: 24.808955192565918 and batch: 250, loss is 6.227727966308594 and perplexity is 506.6031554900276
At time: 25.309130907058716 and batch: 300, loss is 6.171928596496582 and perplexity is 479.1092240130489
At time: 25.80488920211792 and batch: 350, loss is 6.171877136230469 and perplexity is 479.0845695592517
At time: 26.30585741996765 and batch: 400, loss is 6.1397763538360595 and perplexity is 463.94979866285007
At time: 26.80218505859375 and batch: 450, loss is 6.182815933227539 and perplexity is 484.3539461875402
At time: 27.29469132423401 and batch: 500, loss is 6.166948375701904 and perplexity is 476.72908601852953
At time: 27.815627336502075 and batch: 550, loss is 6.114109783172608 and perplexity is 452.1933180386241
At time: 28.31990671157837 and batch: 600, loss is 6.069549961090088 and perplexity is 432.48600224212345
At time: 28.81018328666687 and batch: 650, loss is 6.0191122341156005 and perplexity is 411.21337243100845
At time: 29.32192039489746 and batch: 700, loss is 6.136672124862671 and perplexity is 462.51182530963706
At time: 29.817166090011597 and batch: 750, loss is 5.988022222518921 and perplexity is 398.62543736708506
At time: 30.3086895942688 and batch: 800, loss is 6.127074708938599 and perplexity is 458.09414004330813
At time: 30.834081888198853 and batch: 850, loss is 6.0910861206054685 and perplexity is 441.9011084937416
At time: 31.333544969558716 and batch: 900, loss is 6.113639516830444 and perplexity is 451.98071673453626
At time: 31.83006238937378 and batch: 950, loss is 6.1075968742370605 and perplexity is 449.25779391420286
At time: 32.32558035850525 and batch: 1000, loss is 6.074561939239502 and perplexity is 434.6590537296695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.14219516661109 and perplexity of 465.07336466292634
Finished 3 epochs...
Completing Train Step...
At time: 33.7835476398468 and batch: 50, loss is 6.1253602504730225 and perplexity is 457.3094295358789
At time: 34.29440116882324 and batch: 100, loss is 6.185501022338867 and perplexity is 485.65622728244927
At time: 34.79096579551697 and batch: 150, loss is 6.079373483657837 and perplexity is 436.75547453993386
At time: 35.298492670059204 and batch: 200, loss is 6.103370475769043 and perplexity is 447.3630582366247
At time: 35.79694724082947 and batch: 250, loss is 6.175445709228516 and perplexity is 480.7972719521541
At time: 36.302268743515015 and batch: 300, loss is 6.110568542480468 and perplexity is 450.59482465545614
At time: 36.8053297996521 and batch: 350, loss is 6.115497999191284 and perplexity is 452.8214959687138
At time: 37.29214000701904 and batch: 400, loss is 6.103605670928955 and perplexity is 447.46828823695375
At time: 37.808183908462524 and batch: 450, loss is 6.153236331939698 and perplexity is 470.2367691185964
At time: 38.309366941452026 and batch: 500, loss is 6.130637683868408 and perplexity is 459.729229140747
At time: 38.80777430534363 and batch: 550, loss is 6.088750410079956 and perplexity is 440.87015989084733
At time: 39.31636691093445 and batch: 600, loss is 6.060539693832397 and perplexity is 428.6066908380703
At time: 39.80400633811951 and batch: 650, loss is 6.004300889968872 and perplexity is 405.1676329420244
At time: 40.315505266189575 and batch: 700, loss is 6.126651592254639 and perplexity is 457.90035376982536
At time: 40.806236743927 and batch: 750, loss is 5.965132627487183 and perplexity is 389.604696936358
At time: 41.304017305374146 and batch: 800, loss is 6.103067436218262 and perplexity is 447.22751007568485
At time: 41.80043697357178 and batch: 850, loss is 6.055280914306641 and perplexity is 426.3586588806344
At time: 42.29281044006348 and batch: 900, loss is 6.085217361450195 and perplexity is 439.31529250496317
At time: 42.78895330429077 and batch: 950, loss is 6.066151971817017 and perplexity is 431.0189134333928
At time: 43.288145303726196 and batch: 1000, loss is 6.027919979095459 and perplexity is 414.85123210017514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.102355584865663 and perplexity of 446.90926385315873
Finished 4 epochs...
Completing Train Step...
At time: 44.80756688117981 and batch: 50, loss is 6.085132617950439 and perplexity is 439.2780649669981
At time: 45.3114652633667 and batch: 100, loss is 6.120708179473877 and perplexity is 455.186934427945
At time: 45.81224727630615 and batch: 150, loss is 6.002749500274658 and perplexity is 404.5395473805562
At time: 46.3121862411499 and batch: 200, loss is 6.019751958847046 and perplexity is 411.47651995726943
At time: 46.81131100654602 and batch: 250, loss is 6.0858868217468265 and perplexity is 439.60949511849475
At time: 47.30549693107605 and batch: 300, loss is 6.030799741744995 and perplexity is 416.0476270233866
At time: 47.812567472457886 and batch: 350, loss is 6.006161909103394 and perplexity is 405.92235972228207
At time: 48.31620526313782 and batch: 400, loss is 5.991055383682251 and perplexity is 399.83636810808304
At time: 48.810447454452515 and batch: 450, loss is 6.042525405883789 and perplexity is 420.95477532602973
At time: 49.29933524131775 and batch: 500, loss is 6.02720757484436 and perplexity is 414.5557955664698
At time: 49.80119562149048 and batch: 550, loss is 5.9805591011047365 and perplexity is 395.66152111968944
At time: 50.3093683719635 and batch: 600, loss is 5.952214126586914 and perplexity is 384.60395887441257
At time: 50.825249910354614 and batch: 650, loss is 5.903316411972046 and perplexity is 366.25009216344324
At time: 51.330281257629395 and batch: 700, loss is 6.012387599945068 and perplexity is 408.45738980845283
At time: 51.845688343048096 and batch: 750, loss is 5.864323244094849 and perplexity is 352.2436924270559
At time: 52.345194816589355 and batch: 800, loss is 6.004893283843995 and perplexity is 405.40772287305833
At time: 52.855515003204346 and batch: 850, loss is 5.953821525573731 and perplexity is 385.22266801123806
At time: 53.37997364997864 and batch: 900, loss is 5.986276397705078 and perplexity is 397.93011431982194
At time: 53.88069438934326 and batch: 950, loss is 5.967670240402222 and perplexity is 390.5946183345111
At time: 54.38972997665405 and batch: 1000, loss is 5.9336051082611085 and perplexity is 377.51303891361897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.02696190810785 and perplexity of 414.45396550571314
Finished 5 epochs...
Completing Train Step...
At time: 55.909356117248535 and batch: 50, loss is 6.0000705146789555 and perplexity is 403.45724214760173
At time: 56.42230677604675 and batch: 100, loss is 6.030729942321777 and perplexity is 416.0185881524491
At time: 56.92268085479736 and batch: 150, loss is 5.9397461891174315 and perplexity is 379.83851015501915
At time: 57.43432807922363 and batch: 200, loss is 5.965474777221679 and perplexity is 389.7380228873921
At time: 57.943687915802 and batch: 250, loss is 6.032940931320191 and perplexity is 416.9394182715001
At time: 58.47476053237915 and batch: 300, loss is 5.9692169952392575 and perplexity is 391.19923992988487
At time: 59.00430607795715 and batch: 350, loss is 5.959965696334839 and perplexity is 387.5968280179244
At time: 59.52856421470642 and batch: 400, loss is 5.963097200393677 and perplexity is 388.81249149217547
At time: 60.03868103027344 and batch: 450, loss is 6.0080622291564945 and perplexity is 406.69447552367814
At time: 60.555381536483765 and batch: 500, loss is 5.992646560668946 and perplexity is 400.4730849657268
At time: 61.07631874084473 and batch: 550, loss is 5.948618183135986 and perplexity is 383.22342842961746
At time: 61.58821940422058 and batch: 600, loss is 5.920627040863037 and perplexity is 372.64530446642533
At time: 62.11860513687134 and batch: 650, loss is 5.87148419380188 and perplexity is 354.77514477706524
At time: 62.634374141693115 and batch: 700, loss is 5.982131681442261 and perplexity is 396.28422014190807
At time: 63.15221881866455 and batch: 750, loss is 5.842900552749634 and perplexity is 344.77793831926834
At time: 63.67132377624512 and batch: 800, loss is 5.980970964431763 and perplexity is 395.82451315307065
At time: 64.19251275062561 and batch: 850, loss is 5.933543300628662 and perplexity is 377.4897064475356
At time: 64.70468401908875 and batch: 900, loss is 5.964807043075561 and perplexity is 389.47786836816374
At time: 65.21681880950928 and batch: 950, loss is 5.939099960327148 and perplexity is 379.5931268695101
At time: 65.73095345497131 and batch: 1000, loss is 5.910279912948608 and perplexity is 368.80937548213114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.019151641101372 and perplexity of 411.2295774298178
Finished 6 epochs...
Completing Train Step...
At time: 67.24854564666748 and batch: 50, loss is 5.977534341812134 and perplexity is 394.46654842042926
At time: 67.77644109725952 and batch: 100, loss is 6.006976137161255 and perplexity is 406.25300769003985
At time: 68.3009865283966 and batch: 150, loss is 5.930111312866211 and perplexity is 376.1963869922793
At time: 68.81358075141907 and batch: 200, loss is 5.944825019836426 and perplexity is 381.7725528290168
At time: 69.31872916221619 and batch: 250, loss is 6.004406995773316 and perplexity is 405.21062586051113
At time: 69.82770776748657 and batch: 300, loss is 5.954649715423584 and perplexity is 385.5418376631083
At time: 70.34517431259155 and batch: 350, loss is 5.947687587738037 and perplexity is 382.86696835651117
At time: 70.86055254936218 and batch: 400, loss is 5.945367383956909 and perplexity is 381.9796687248948
At time: 71.36564469337463 and batch: 450, loss is 6.0002580261230465 and perplexity is 403.53290209103676
At time: 71.86257982254028 and batch: 500, loss is 5.980518198013305 and perplexity is 395.64533767129404
At time: 72.36644864082336 and batch: 550, loss is 5.928450384140015 and perplexity is 375.5720702227852
At time: 72.86589241027832 and batch: 600, loss is 5.903812093734741 and perplexity is 366.43168065604806
At time: 73.38177633285522 and batch: 650, loss is 5.856533746719361 and perplexity is 349.51054983262355
At time: 73.89952659606934 and batch: 700, loss is 5.973090686798096 and perplexity is 392.7175639843631
At time: 74.39843368530273 and batch: 750, loss is 5.8275914478302 and perplexity is 339.539893876013
At time: 74.90953946113586 and batch: 800, loss is 5.9710001945495605 and perplexity is 391.89744848220977
At time: 75.41385626792908 and batch: 850, loss is 5.92358060836792 and perplexity is 373.7475645269903
At time: 75.91507887840271 and batch: 900, loss is 5.953127584457397 and perplexity is 384.95543889432105
At time: 76.41761755943298 and batch: 950, loss is 5.930139284133912 and perplexity is 376.2069098292956
At time: 76.91933250427246 and batch: 1000, loss is 5.900391054153443 and perplexity is 365.1802451983147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.012973692358994 and perplexity of 408.6968537531774
Finished 7 epochs...
Completing Train Step...
At time: 78.40034294128418 and batch: 50, loss is 5.966028556823731 and perplexity is 389.9539116264837
At time: 78.92115235328674 and batch: 100, loss is 6.00244029045105 and perplexity is 404.4144791156328
At time: 79.4296703338623 and batch: 150, loss is 5.9212423515319825 and perplexity is 372.8746676555681
At time: 79.93673658370972 and batch: 200, loss is 5.93281849861145 and perplexity is 377.21620027771115
At time: 80.43515872955322 and batch: 250, loss is 5.999893341064453 and perplexity is 403.3857665016996
At time: 80.93966126441956 and batch: 300, loss is 5.948362655639649 and perplexity is 383.12551681650046
At time: 81.440833568573 and batch: 350, loss is 5.9372689914703365 and perplexity is 378.898739570703
At time: 81.9465582370758 and batch: 400, loss is 5.940285863876343 and perplexity is 380.04355473512805
At time: 82.46430945396423 and batch: 450, loss is 5.99333945274353 and perplexity is 400.7506657480406
At time: 82.96304845809937 and batch: 500, loss is 5.969244527816772 and perplexity is 391.21001080155617
At time: 83.47184586524963 and batch: 550, loss is 5.920257730484009 and perplexity is 372.50770809723724
At time: 83.96914839744568 and batch: 600, loss is 5.899095773696899 and perplexity is 364.70754057221575
At time: 84.48297786712646 and batch: 650, loss is 5.849805717468262 and perplexity is 347.16692545706695
At time: 85.00565218925476 and batch: 700, loss is 5.968167819976807 and perplexity is 390.7890185993268
At time: 85.51828217506409 and batch: 750, loss is 5.8253473281860355 and perplexity is 338.77878006483445
At time: 86.0306224822998 and batch: 800, loss is 5.966493272781372 and perplexity is 390.1351715458878
At time: 86.54798340797424 and batch: 850, loss is 5.917449073791504 and perplexity is 371.46292972857464
At time: 87.07477498054504 and batch: 900, loss is 5.948748931884766 and perplexity is 383.27353768917783
At time: 87.58154606819153 and batch: 950, loss is 5.926522483825684 and perplexity is 374.8487022252948
At time: 88.07711839675903 and batch: 1000, loss is 5.896255187988281 and perplexity is 363.673027551709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.01322192680545 and perplexity of 408.79831898349875
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 89.56931972503662 and batch: 50, loss is 5.889379787445068 and perplexity is 361.1812057728629
At time: 90.10029911994934 and batch: 100, loss is 5.858633632659912 and perplexity is 350.24525324900463
At time: 90.59952425956726 and batch: 150, loss is 5.759529008865356 and perplexity is 317.19889586192085
At time: 91.10457038879395 and batch: 200, loss is 5.756541957855225 and perplexity is 316.25282027109307
At time: 91.60577511787415 and batch: 250, loss is 5.832268600463867 and perplexity is 341.1316934278058
At time: 92.13442540168762 and batch: 300, loss is 5.773508110046387 and perplexity is 321.6641889986648
At time: 92.64402413368225 and batch: 350, loss is 5.76080018043518 and perplexity is 317.6023664661709
At time: 93.16105937957764 and batch: 400, loss is 5.767110214233399 and perplexity is 319.6127843647606
At time: 93.67631602287292 and batch: 450, loss is 5.806851949691772 and perplexity is 332.57052726941026
At time: 94.18748331069946 and batch: 500, loss is 5.788493738174439 and perplexity is 326.52082792231556
At time: 94.70030164718628 and batch: 550, loss is 5.754245615005493 and perplexity is 315.5274285613496
At time: 95.20546579360962 and batch: 600, loss is 5.698983945846558 and perplexity is 298.56388972102144
At time: 95.72168469429016 and batch: 650, loss is 5.669858751296997 and perplexity is 289.9935702830574
At time: 96.2212381362915 and batch: 700, loss is 5.769863538742065 and perplexity is 320.4939946494828
At time: 96.7449722290039 and batch: 750, loss is 5.6513926219940185 and perplexity is 284.6876521768232
At time: 97.26155042648315 and batch: 800, loss is 5.763303833007813 and perplexity is 318.3985286889664
At time: 97.77695965766907 and batch: 850, loss is 5.730233278274536 and perplexity is 308.04111929725156
At time: 98.29989123344421 and batch: 900, loss is 5.753589725494384 and perplexity is 315.32054528419155
At time: 98.81558060646057 and batch: 950, loss is 5.7260830688476565 and perplexity is 306.76533336069076
At time: 99.33226585388184 and batch: 1000, loss is 5.674273414611816 and perplexity is 291.27662430125923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.824952660537347 and perplexity of 338.6451014213339
Finished 9 epochs...
Completing Train Step...
At time: 100.87815046310425 and batch: 50, loss is 5.773314695358277 and perplexity is 321.60198043609233
At time: 101.40065574645996 and batch: 100, loss is 5.811560201644897 and perplexity is 334.1400450471757
At time: 101.9105703830719 and batch: 150, loss is 5.722451848983765 and perplexity is 305.6534210095299
At time: 102.41177201271057 and batch: 200, loss is 5.732197074890137 and perplexity is 308.6466437736519
At time: 102.91851449012756 and batch: 250, loss is 5.807584438323975 and perplexity is 332.81422064045677
At time: 103.42631006240845 and batch: 300, loss is 5.758997268676758 and perplexity is 317.03027329688257
At time: 103.93422079086304 and batch: 350, loss is 5.74171703338623 and perplexity is 311.59897776348765
At time: 104.44292163848877 and batch: 400, loss is 5.747880725860596 and perplexity is 313.5255092121935
At time: 104.95540714263916 and batch: 450, loss is 5.792572765350342 and perplexity is 327.8554353524132
At time: 105.48483467102051 and batch: 500, loss is 5.776521253585815 and perplexity is 322.63487103936706
At time: 106.0126085281372 and batch: 550, loss is 5.744048223495484 and perplexity is 312.3262215609299
At time: 106.51769065856934 and batch: 600, loss is 5.685418939590454 and perplexity is 294.5412141825172
At time: 107.02470827102661 and batch: 650, loss is 5.6618700695037845 and perplexity is 287.6861328910527
At time: 107.5336582660675 and batch: 700, loss is 5.764169645309448 and perplexity is 318.67432142707213
At time: 108.04528903961182 and batch: 750, loss is 5.643091173171997 and perplexity is 282.334114603841
At time: 108.54662799835205 and batch: 800, loss is 5.757350940704345 and perplexity is 316.50876689293324
At time: 109.05274033546448 and batch: 850, loss is 5.7256276988983155 and perplexity is 306.62567344725596
At time: 109.55827951431274 and batch: 900, loss is 5.747863874435425 and perplexity is 313.5202259050517
At time: 110.0676805973053 and batch: 950, loss is 5.724627094268799 and perplexity is 306.3190158259795
At time: 110.58941698074341 and batch: 1000, loss is 5.6693142318725585 and perplexity is 289.83570613502417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.8190128977705795 and perplexity of 336.6395918785438
Finished 10 epochs...
Completing Train Step...
At time: 112.12017345428467 and batch: 50, loss is 5.76336446762085 and perplexity is 318.4178352458619
At time: 112.6290910243988 and batch: 100, loss is 5.799991903305053 and perplexity is 330.2968855856976
At time: 113.14013123512268 and batch: 150, loss is 5.711184949874878 and perplexity is 302.22898238063476
At time: 113.63886904716492 and batch: 200, loss is 5.722691812515259 and perplexity is 305.7267754846961
At time: 114.1559853553772 and batch: 250, loss is 5.800717401504516 and perplexity is 330.53660232799626
At time: 114.66522359848022 and batch: 300, loss is 5.749900884628296 and perplexity is 314.1595207048452
At time: 115.17422437667847 and batch: 350, loss is 5.73394208908081 and perplexity is 309.18570674641853
At time: 115.67808866500854 and batch: 400, loss is 5.741316356658936 and perplexity is 311.47415231384537
At time: 116.1752359867096 and batch: 450, loss is 5.785006380081176 and perplexity is 325.38411608308365
At time: 116.67051410675049 and batch: 500, loss is 5.770332345962524 and perplexity is 320.6442797729112
At time: 117.16917681694031 and batch: 550, loss is 5.736011505126953 and perplexity is 309.82620310734484
At time: 117.6684582233429 and batch: 600, loss is 5.678132314682006 and perplexity is 292.4028031956232
At time: 118.21081376075745 and batch: 650, loss is 5.655140514373779 and perplexity is 285.75663281992826
At time: 118.71225810050964 and batch: 700, loss is 5.756720485687256 and perplexity is 316.3092852416043
At time: 119.22428607940674 and batch: 750, loss is 5.637125473022461 and perplexity is 280.65480803433775
At time: 119.73044419288635 and batch: 800, loss is 5.752299089431762 and perplexity is 314.9138437254663
At time: 120.23312902450562 and batch: 850, loss is 5.713681325912476 and perplexity is 302.984402083632
At time: 120.72999095916748 and batch: 900, loss is 5.737376937866211 and perplexity is 310.2495389010512
At time: 121.22762107849121 and batch: 950, loss is 5.717308187484742 and perplexity is 304.08527972719907
At time: 121.74154782295227 and batch: 1000, loss is 5.66339111328125 and perplexity is 288.1240490537476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.812808153105945 and perplexity of 334.55729590484407
Finished 11 epochs...
Completing Train Step...
At time: 123.24756240844727 and batch: 50, loss is 5.757207660675049 and perplexity is 316.4634207562158
At time: 123.76923131942749 and batch: 100, loss is 5.793343210220337 and perplexity is 328.10812722075786
At time: 124.26614236831665 and batch: 150, loss is 5.703638553619385 and perplexity is 299.9568267957958
At time: 124.7704758644104 and batch: 200, loss is 5.716331119537354 and perplexity is 303.78831284913934
At time: 125.27960920333862 and batch: 250, loss is 5.791677827835083 and perplexity is 327.5621564764433
At time: 125.78647804260254 and batch: 300, loss is 5.74114013671875 and perplexity is 311.4192691932473
At time: 126.29506611824036 and batch: 350, loss is 5.724755945205689 and perplexity is 306.3584878611056
At time: 126.80411982536316 and batch: 400, loss is 5.731703281402588 and perplexity is 308.4942736937752
At time: 127.31794166564941 and batch: 450, loss is 5.775880498886108 and perplexity is 322.4282074468641
At time: 127.82437062263489 and batch: 500, loss is 5.76421365737915 and perplexity is 318.6883472521695
At time: 128.336651802063 and batch: 550, loss is 5.728590211868286 and perplexity is 307.5354028590082
At time: 128.8383948802948 and batch: 600, loss is 5.671502561569214 and perplexity is 290.4706567051178
At time: 129.35568189620972 and batch: 650, loss is 5.647492980957031 and perplexity is 283.5796343656875
At time: 129.87817072868347 and batch: 700, loss is 5.751684856414795 and perplexity is 314.7204726386827
At time: 130.37787055969238 and batch: 750, loss is 5.633252544403076 and perplexity is 279.5699541330233
At time: 130.87684535980225 and batch: 800, loss is 5.748425664901734 and perplexity is 313.6964080630547
At time: 131.40735936164856 and batch: 850, loss is 5.715069608688355 and perplexity is 303.4053222208886
At time: 131.91139602661133 and batch: 900, loss is 5.734245262145996 and perplexity is 309.2794577355138
At time: 132.42405223846436 and batch: 950, loss is 5.715538120269775 and perplexity is 303.54750443261935
At time: 132.92715549468994 and batch: 1000, loss is 5.6611291122436525 and perplexity is 287.4730487151303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.810764591868331 and perplexity of 333.874305687028
Finished 12 epochs...
Completing Train Step...
At time: 134.42154788970947 and batch: 50, loss is 5.755616912841797 and perplexity is 315.96040744508366
At time: 134.94541931152344 and batch: 100, loss is 5.790402355194092 and perplexity is 327.14462623843554
At time: 135.45765590667725 and batch: 150, loss is 5.701615943908691 and perplexity is 299.35074434587517
At time: 135.97130489349365 and batch: 200, loss is 5.715063734054565 and perplexity is 303.40353983096634
At time: 136.4824197292328 and batch: 250, loss is 5.791632795333863 and perplexity is 327.547405865363
At time: 136.995920419693 and batch: 300, loss is 5.740258455276489 and perplexity is 311.14481761005015
At time: 137.498615026474 and batch: 350, loss is 5.723122901916504 and perplexity is 305.85859946920425
At time: 137.99835753440857 and batch: 400, loss is 5.729520244598389 and perplexity is 307.8215538936205
At time: 138.50644421577454 and batch: 450, loss is 5.773436098098755 and perplexity is 321.64102616794196
At time: 139.01928114891052 and batch: 500, loss is 5.76261456489563 and perplexity is 318.1791423528638
At time: 139.5305416584015 and batch: 550, loss is 5.7254957008361815 and perplexity is 306.5852021236873
At time: 140.03149604797363 and batch: 600, loss is 5.67023775100708 and perplexity is 290.10349859220344
At time: 140.54549074172974 and batch: 650, loss is 5.64514570236206 and perplexity is 282.9147745708197
At time: 141.0521731376648 and batch: 700, loss is 5.748956899642945 and perplexity is 313.86309876523677
At time: 141.5723729133606 and batch: 750, loss is 5.630128374099732 and perplexity is 278.69789292778194
At time: 142.0869402885437 and batch: 800, loss is 5.746164951324463 and perplexity is 312.98803135432627
At time: 142.60930848121643 and batch: 850, loss is 5.712388515472412 and perplexity is 302.5929537741933
At time: 143.11709761619568 and batch: 900, loss is 5.732997665405273 and perplexity is 308.8938422883754
At time: 143.61660504341125 and batch: 950, loss is 5.71398097038269 and perplexity is 303.07520328763746
At time: 144.1509165763855 and batch: 1000, loss is 5.660938110351562 and perplexity is 287.418146062323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.8083038330078125 and perplexity of 333.0537315626037
Finished 13 epochs...
Completing Train Step...
At time: 145.6329505443573 and batch: 50, loss is 5.753601732254029 and perplexity is 315.32433128491846
At time: 146.13309693336487 and batch: 100, loss is 5.787429103851318 and perplexity is 326.17338762294673
At time: 146.64076733589172 and batch: 150, loss is 5.6988675403594975 and perplexity is 298.5291372687472
At time: 147.1503140926361 and batch: 200, loss is 5.712875595092774 and perplexity is 302.7403765356384
At time: 147.65032696723938 and batch: 250, loss is 5.788440923690796 and perplexity is 326.5035833487759
At time: 148.14375805854797 and batch: 300, loss is 5.737695341110229 and perplexity is 310.34833908900805
At time: 148.6641857624054 and batch: 350, loss is 5.720975914001465 and perplexity is 305.20262918454756
At time: 149.16634321212769 and batch: 400, loss is 5.726904649734497 and perplexity is 307.01746945624103
At time: 149.6887993812561 and batch: 450, loss is 5.770731267929077 and perplexity is 320.7722173364176
At time: 150.1968173980713 and batch: 500, loss is 5.760029134750366 and perplexity is 317.35757491688184
At time: 150.70948100090027 and batch: 550, loss is 5.719073171615601 and perplexity is 304.62245933763893
At time: 151.21231079101562 and batch: 600, loss is 5.660758333206177 and perplexity is 287.3664794928652
At time: 151.72010827064514 and batch: 650, loss is 5.638662433624267 and perplexity is 281.0864950750015
At time: 152.2216739654541 and batch: 700, loss is 5.740056591033936 and perplexity is 311.0820149361392
At time: 152.74432945251465 and batch: 750, loss is 5.624511260986328 and perplexity is 277.1368038503195
At time: 153.25820875167847 and batch: 800, loss is 5.738373470306397 and perplexity is 310.5588667331296
At time: 153.76942133903503 and batch: 850, loss is 5.702265033721924 and perplexity is 299.54511293912634
At time: 154.2908616065979 and batch: 900, loss is 5.723356790542603 and perplexity is 305.9301446832943
At time: 154.80045413970947 and batch: 950, loss is 5.6997040843963624 and perplexity is 298.7789745237229
At time: 155.32280945777893 and batch: 1000, loss is 5.6454969501495365 and perplexity is 283.01416521378184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.798601476157584 and perplexity of 329.8379509607732
Finished 14 epochs...
Completing Train Step...
At time: 156.88120579719543 and batch: 50, loss is 5.742589263916016 and perplexity is 311.8708824694974
At time: 157.3912320137024 and batch: 100, loss is 5.772517204284668 and perplexity is 321.3456079684044
At time: 157.8830988407135 and batch: 150, loss is 5.6873252391815186 and perplexity is 295.10323349706266
At time: 158.38468050956726 and batch: 200, loss is 5.698091287612915 and perplexity is 298.29749312484057
At time: 158.89121794700623 and batch: 250, loss is 5.771119213104248 and perplexity is 320.8966835119273
At time: 159.38346028327942 and batch: 300, loss is 5.721141786575317 and perplexity is 305.2532581290575
At time: 159.88554215431213 and batch: 350, loss is 5.700362348556519 and perplexity is 298.9757147608795
At time: 160.38285398483276 and batch: 400, loss is 5.701670684814453 and perplexity is 299.36713152528165
At time: 160.89211916923523 and batch: 450, loss is 5.742399444580078 and perplexity is 311.8116889639025
At time: 161.40845346450806 and batch: 500, loss is 5.7327430725097654 and perplexity is 308.81521012068305
At time: 161.9209966659546 and batch: 550, loss is 5.6897288513183595 and perplexity is 295.8134003516457
At time: 162.42900466918945 and batch: 600, loss is 5.631587762832641 and perplexity is 279.1049184245776
At time: 162.93248009681702 and batch: 650, loss is 5.6050599098205565 and perplexity is 271.7982083284401
At time: 163.43167519569397 and batch: 700, loss is 5.697855138778687 and perplexity is 298.22705883640094
At time: 163.9377179145813 and batch: 750, loss is 5.585646505355835 and perplexity is 266.5725676346052
At time: 164.44711327552795 and batch: 800, loss is 5.696149053573609 and perplexity is 297.71869184557687
At time: 164.9462866783142 and batch: 850, loss is 5.656663513183593 and perplexity is 286.1921714097996
At time: 165.4534571170807 and batch: 900, loss is 5.681242685317994 and perplexity is 293.3137001677951
At time: 165.9700849056244 and batch: 950, loss is 5.653221797943115 and perplexity is 285.20887253973734
At time: 166.48309707641602 and batch: 1000, loss is 5.596101779937744 and perplexity is 269.3742778280707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.741655303210747 and perplexity of 311.5797432975895
Finished 15 epochs...
Completing Train Step...
At time: 168.07142210006714 and batch: 50, loss is 5.693501386642456 and perplexity is 296.93147451491365
At time: 168.6127917766571 and batch: 100, loss is 5.720764961242676 and perplexity is 305.13825263837515
At time: 169.12596940994263 and batch: 150, loss is 5.639082336425782 and perplexity is 281.2045488655755
At time: 169.6358518600464 and batch: 200, loss is 5.64571626663208 and perplexity is 283.07624169195515
At time: 170.1490023136139 and batch: 250, loss is 5.7193989658355715 and perplexity is 304.72171974251955
At time: 170.69125866889954 and batch: 300, loss is 5.674008350372315 and perplexity is 291.19942751585984
At time: 171.20611715316772 and batch: 350, loss is 5.650141725540161 and perplexity is 284.33176004077023
At time: 171.70852136611938 and batch: 400, loss is 5.65964189529419 and perplexity is 287.0458316855149
At time: 172.21094155311584 and batch: 450, loss is 5.695234603881836 and perplexity is 297.4465675205493
At time: 172.71659517288208 and batch: 500, loss is 5.691886034011841 and perplexity is 296.45221266909914
At time: 173.23067688941956 and batch: 550, loss is 5.647285718917846 and perplexity is 283.5208651629204
At time: 173.75801396369934 and batch: 600, loss is 5.588811264038086 and perplexity is 267.41754184696384
At time: 174.2790493965149 and batch: 650, loss is 5.569605283737182 and perplexity is 262.33053267443944
At time: 174.79081559181213 and batch: 700, loss is 5.664625940322876 and perplexity is 288.4800521767674
At time: 175.30693864822388 and batch: 750, loss is 5.5576011848449705 and perplexity is 259.2003163192808
At time: 175.82195734977722 and batch: 800, loss is 5.670213499069214 and perplexity is 290.09646310549334
At time: 176.33682227134705 and batch: 850, loss is 5.628235092163086 and perplexity is 278.17073842483586
At time: 176.85147738456726 and batch: 900, loss is 5.654042062759399 and perplexity is 285.4429153185794
At time: 177.36646842956543 and batch: 950, loss is 5.627328052520752 and perplexity is 277.9185409315998
At time: 177.89471340179443 and batch: 1000, loss is 5.569182367324829 and perplexity is 262.2196122433986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.716923411299542 and perplexity of 303.9682974607288
Finished 16 epochs...
Completing Train Step...
At time: 179.41862869262695 and batch: 50, loss is 5.669334030151367 and perplexity is 289.84144443994694
At time: 179.91605353355408 and batch: 100, loss is 5.694460678100586 and perplexity is 297.21645500988865
At time: 180.4134111404419 and batch: 150, loss is 5.615979681015014 and perplexity is 274.7824465199005
At time: 180.91341185569763 and batch: 200, loss is 5.627134113311768 and perplexity is 277.8646468558657
At time: 181.41930627822876 and batch: 250, loss is 5.700737934112549 and perplexity is 299.0880268110192
At time: 181.9176962375641 and batch: 300, loss is 5.651647396087647 and perplexity is 284.76019245567323
At time: 182.43338680267334 and batch: 350, loss is 5.630451803207397 and perplexity is 278.78804651696146
At time: 182.9369637966156 and batch: 400, loss is 5.636767520904541 and perplexity is 280.5543650293629
At time: 183.45686101913452 and batch: 450, loss is 5.674590663909912 and perplexity is 291.369046265497
At time: 183.9629921913147 and batch: 500, loss is 5.670632371902466 and perplexity is 290.21800208582215
At time: 184.47153043746948 and batch: 550, loss is 5.623337812423706 and perplexity is 276.81178879771
At time: 184.97548365592957 and batch: 600, loss is 5.566465530395508 and perplexity is 261.5081711895352
At time: 185.48694109916687 and batch: 650, loss is 5.549998092651367 and perplexity is 257.2370652645421
At time: 186.0009093284607 and batch: 700, loss is 5.646042280197143 and perplexity is 283.1685434316859
At time: 186.51031804084778 and batch: 750, loss is 5.538119840621948 and perplexity is 254.1996140907812
At time: 187.02471590042114 and batch: 800, loss is 5.650303506851197 and perplexity is 284.3777633268144
At time: 187.54376363754272 and batch: 850, loss is 5.60887716293335 and perplexity is 272.8377136504411
At time: 188.0636727809906 and batch: 900, loss is 5.637990980148316 and perplexity is 280.8978219205516
At time: 188.5641267299652 and batch: 950, loss is 5.609884977340698 and perplexity is 273.1128220349373
At time: 189.0727186203003 and batch: 1000, loss is 5.555018281936645 and perplexity is 258.53169093736415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.705969461580602 and perplexity of 300.6568140373857
Finished 17 epochs...
Completing Train Step...
At time: 190.57148575782776 and batch: 50, loss is 5.656411848068237 and perplexity is 286.12015588624536
At time: 191.08718943595886 and batch: 100, loss is 5.67528733253479 and perplexity is 291.5721046622849
At time: 191.58312249183655 and batch: 150, loss is 5.60496416091919 and perplexity is 271.7721851944624
At time: 192.087984085083 and batch: 200, loss is 5.614002981185913 and perplexity is 274.23982058580555
At time: 192.5826644897461 and batch: 250, loss is 5.686631641387939 and perplexity is 294.8986215128443
At time: 193.08023810386658 and batch: 300, loss is 5.6408546257019045 and perplexity is 281.7033665659916
At time: 193.5893909931183 and batch: 350, loss is 5.621163492202759 and perplexity is 276.21056519111846
At time: 194.08179688453674 and batch: 400, loss is 5.624144582748413 and perplexity is 277.03520244407315
At time: 194.58513236045837 and batch: 450, loss is 5.6646888637542725 and perplexity is 288.49820490264983
At time: 195.08267211914062 and batch: 500, loss is 5.660612621307373 and perplexity is 287.32460982801405
At time: 195.58862686157227 and batch: 550, loss is 5.61196798324585 and perplexity is 273.6823105740903
At time: 196.10051798820496 and batch: 600, loss is 5.552328643798828 and perplexity is 257.83726853270605
At time: 196.64571142196655 and batch: 650, loss is 5.538819646835327 and perplexity is 254.37756681911054
At time: 197.1514971256256 and batch: 700, loss is 5.631918678283691 and perplexity is 279.19729383793117
At time: 197.66240763664246 and batch: 750, loss is 5.5274967765808105 and perplexity is 251.5135278046227
At time: 198.1724569797516 and batch: 800, loss is 5.638503360748291 and perplexity is 281.04178539397486
At time: 198.69348287582397 and batch: 850, loss is 5.59813419342041 and perplexity is 269.9223144720961
At time: 199.20890522003174 and batch: 900, loss is 5.628510313034058 and perplexity is 278.24730735394434
At time: 199.7109715938568 and batch: 950, loss is 5.599217004776001 and perplexity is 270.2147477157797
At time: 200.22713351249695 and batch: 1000, loss is 5.546601467132568 and perplexity is 256.3648094860075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.69488041575362 and perplexity of 297.3412341325739
Finished 18 epochs...
Completing Train Step...
At time: 201.74428415298462 and batch: 50, loss is 5.648370780944824 and perplexity is 283.82866985095427
At time: 202.26573014259338 and batch: 100, loss is 5.667111072540283 and perplexity is 289.1978547962971
At time: 202.76219511032104 and batch: 150, loss is 5.594303874969483 and perplexity is 268.890403585936
At time: 203.25211334228516 and batch: 200, loss is 5.605757837295532 and perplexity is 271.98796997791334
At time: 203.77074909210205 and batch: 250, loss is 5.680889263153076 and perplexity is 293.2100549212243
At time: 204.27739787101746 and batch: 300, loss is 5.6316636943817135 and perplexity is 279.12611209801133
At time: 204.78207659721375 and batch: 350, loss is 5.61206615447998 and perplexity is 273.70917962314127
At time: 205.28115892410278 and batch: 400, loss is 5.6147387599945064 and perplexity is 274.4416746851429
At time: 205.78352403640747 and batch: 450, loss is 5.657858562469483 and perplexity is 286.53438960294557
At time: 206.28107929229736 and batch: 500, loss is 5.653844928741455 and perplexity is 285.38665035584023
At time: 206.77945113182068 and batch: 550, loss is 5.603801612854004 and perplexity is 271.4564205477462
At time: 207.2767415046692 and batch: 600, loss is 5.546039237976074 and perplexity is 256.22071422648924
At time: 207.78596925735474 and batch: 650, loss is 5.534253101348877 and perplexity is 253.21858836745412
At time: 208.29199981689453 and batch: 700, loss is 5.6274809074401855 and perplexity is 277.96102539467995
At time: 208.81553864479065 and batch: 750, loss is 5.5240443897247316 and perplexity is 250.64670297539922
At time: 209.35400342941284 and batch: 800, loss is 5.635601806640625 and perplexity is 280.22750935142074
At time: 209.8530011177063 and batch: 850, loss is 5.594718570709229 and perplexity is 269.00193441484305
At time: 210.3616383075714 and batch: 900, loss is 5.623194961547852 and perplexity is 276.7722488154612
At time: 210.8595049381256 and batch: 950, loss is 5.594159908294678 and perplexity is 268.8516951150766
At time: 211.35717272758484 and batch: 1000, loss is 5.542805137634278 and perplexity is 255.39340924179405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6886708329363564 and perplexity of 295.50058984524384
Finished 19 epochs...
Completing Train Step...
At time: 212.85756421089172 and batch: 50, loss is 5.643389329910279 and perplexity is 282.4183069731944
At time: 213.36330938339233 and batch: 100, loss is 5.6586858177185055 and perplexity is 286.7715247528953
At time: 213.8903956413269 and batch: 150, loss is 5.589385786056519 and perplexity is 267.57122325531924
At time: 214.40468454360962 and batch: 200, loss is 5.601463422775269 and perplexity is 270.82244530456
At time: 214.92525482177734 and batch: 250, loss is 5.675239219665527 and perplexity is 291.5580766291998
At time: 215.44511198997498 and batch: 300, loss is 5.626930131912231 and perplexity is 277.8079734166785
At time: 215.96278476715088 and batch: 350, loss is 5.607405586242676 and perplexity is 272.4365073060508
At time: 216.4596288204193 and batch: 400, loss is 5.613069429397583 and perplexity is 273.9839229762867
At time: 216.95751404762268 and batch: 450, loss is 5.651133756637574 and perplexity is 284.61396594408495
At time: 217.45883226394653 and batch: 500, loss is 5.65044207572937 and perplexity is 284.4171719647984
At time: 217.97115921974182 and batch: 550, loss is 5.600146036148072 and perplexity is 270.4659023408024
At time: 218.47887706756592 and batch: 600, loss is 5.5395748329162595 and perplexity is 254.5697417716653
At time: 218.99211239814758 and batch: 650, loss is 5.5293990421295165 and perplexity is 251.9924286775017
At time: 219.48828601837158 and batch: 700, loss is 5.622656879425048 and perplexity is 276.62336267636266
At time: 219.99186086654663 and batch: 750, loss is 5.520206995010376 and perplexity is 249.6867157452242
At time: 220.48825311660767 and batch: 800, loss is 5.630583829879761 and perplexity is 278.82485640492786
At time: 220.9893126487732 and batch: 850, loss is 5.589769344329834 and perplexity is 267.67387209634774
At time: 221.4950680732727 and batch: 900, loss is 5.621215152740478 and perplexity is 276.22483474602336
At time: 222.00904655456543 and batch: 950, loss is 5.590758724212646 and perplexity is 267.93883429307414
At time: 222.55992007255554 and batch: 1000, loss is 5.5368263721466064 and perplexity is 253.87102745754865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.692752372927782 and perplexity of 296.7091520398688
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 224.06006956100464 and batch: 50, loss is 5.621046438217163 and perplexity is 276.1782355357936
At time: 224.60414600372314 and batch: 100, loss is 5.618022861480713 and perplexity is 275.3444505893038
At time: 225.1069312095642 and batch: 150, loss is 5.545750885009766 and perplexity is 256.14684287453605
At time: 225.62082386016846 and batch: 200, loss is 5.55146354675293 and perplexity is 257.61431072735047
At time: 226.14121460914612 and batch: 250, loss is 5.628528957366943 and perplexity is 278.2524951377284
At time: 226.66117143630981 and batch: 300, loss is 5.57616530418396 and perplexity is 264.057083244444
At time: 227.17459392547607 and batch: 350, loss is 5.548865880966186 and perplexity is 256.945983267702
At time: 227.68404459953308 and batch: 400, loss is 5.558780403137207 and perplexity is 259.506150360746
At time: 228.19196772575378 and batch: 450, loss is 5.599629993438721 and perplexity is 270.326366390123
At time: 228.6893928050995 and batch: 500, loss is 5.5961427402496335 and perplexity is 269.3853117084797
At time: 229.19886565208435 and batch: 550, loss is 5.542511472702026 and perplexity is 255.31842016494156
At time: 229.71737241744995 and batch: 600, loss is 5.483781175613403 and perplexity is 240.75532663706733
At time: 230.23020243644714 and batch: 650, loss is 5.468665208816528 and perplexity is 237.14344443290997
At time: 230.7421841621399 and batch: 700, loss is 5.5614774131774904 and perplexity is 260.2069857089547
At time: 231.26856780052185 and batch: 750, loss is 5.451788988113403 and perplexity is 233.17494019073115
At time: 231.79304552078247 and batch: 800, loss is 5.566177673339844 and perplexity is 261.43290505080876
At time: 232.30661702156067 and batch: 850, loss is 5.526177072525025 and perplexity is 251.181823305922
At time: 232.81767082214355 and batch: 900, loss is 5.558830766677857 and perplexity is 259.519220338421
At time: 233.32259488105774 and batch: 950, loss is 5.517209033966065 and perplexity is 248.93928564111647
At time: 233.82459998130798 and batch: 1000, loss is 5.462968587875366 and perplexity is 235.796368654222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6343562428544205 and perplexity of 279.878685400213
Finished 21 epochs...
Completing Train Step...
At time: 235.43195700645447 and batch: 50, loss is 5.585516061782837 and perplexity is 266.53779722425725
At time: 235.9649670124054 and batch: 100, loss is 5.599750995635986 and perplexity is 270.35907845351085
At time: 236.45695757865906 and batch: 150, loss is 5.529132242202759 and perplexity is 251.92520608387736
At time: 236.96384477615356 and batch: 200, loss is 5.5391504764556885 and perplexity is 254.46173637509327
At time: 237.47612929344177 and batch: 250, loss is 5.616533908843994 and perplexity is 274.9347808087287
At time: 237.9806900024414 and batch: 300, loss is 5.566540622711182 and perplexity is 261.5278091809994
At time: 238.4937038421631 and batch: 350, loss is 5.537075872421265 and perplexity is 253.93437625106918
At time: 238.9986023902893 and batch: 400, loss is 5.550176458358765 and perplexity is 257.28295162781205
At time: 239.51478362083435 and batch: 450, loss is 5.591564254760742 and perplexity is 268.1547541624683
At time: 240.02379608154297 and batch: 500, loss is 5.589139204025269 and perplexity is 267.5052531334426
At time: 240.53354501724243 and batch: 550, loss is 5.53712911605835 and perplexity is 253.94789700078545
At time: 241.02628660202026 and batch: 600, loss is 5.480037431716919 and perplexity is 239.85568541634046
At time: 241.52631306648254 and batch: 650, loss is 5.468365058898926 and perplexity is 237.07227652865814
At time: 242.0282711982727 and batch: 700, loss is 5.55814567565918 and perplexity is 259.34148694012214
At time: 242.5380358695984 and batch: 750, loss is 5.449253702163697 and perplexity is 232.58452379448923
At time: 243.04221272468567 and batch: 800, loss is 5.563605461120606 and perplexity is 260.7613082529601
At time: 243.54709696769714 and batch: 850, loss is 5.524585428237915 and perplexity is 250.78234918651478
At time: 244.05024242401123 and batch: 900, loss is 5.56094069480896 and perplexity is 260.06736531187227
At time: 244.55413460731506 and batch: 950, loss is 5.5191457653045655 and perplexity is 249.42188133525542
At time: 245.06330585479736 and batch: 1000, loss is 5.463116636276245 and perplexity is 235.83128051379308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.630790524366425 and perplexity of 278.88249392196497
Finished 22 epochs...
Completing Train Step...
At time: 246.5872368812561 and batch: 50, loss is 5.57694224357605 and perplexity is 264.26231931184566
At time: 247.1109278202057 and batch: 100, loss is 5.59288423538208 and perplexity is 268.5089469538654
At time: 247.62260675430298 and batch: 150, loss is 5.523638763427734 and perplexity is 250.54505469841757
At time: 248.13758325576782 and batch: 200, loss is 5.5336297988891605 and perplexity is 253.06080577672537
At time: 248.67745113372803 and batch: 250, loss is 5.6120509433746335 and perplexity is 273.7050162356406
At time: 249.19985699653625 and batch: 300, loss is 5.561606693267822 and perplexity is 260.24062746612987
At time: 249.72019815444946 and batch: 350, loss is 5.533300876617432 and perplexity is 252.97758212940965
At time: 250.22874760627747 and batch: 400, loss is 5.546153297424317 and perplexity is 256.24994028650474
At time: 250.73798322677612 and batch: 450, loss is 5.5885068225860595 and perplexity is 267.33614125371264
At time: 251.2526557445526 and batch: 500, loss is 5.5860659122467045 and perplexity is 266.68439345500855
At time: 251.76402306556702 and batch: 550, loss is 5.534313049316406 and perplexity is 253.23376876217958
At time: 252.28163409233093 and batch: 600, loss is 5.4778297233581545 and perplexity is 239.32673811028369
At time: 252.79256939888 and batch: 650, loss is 5.467684993743896 and perplexity is 236.91110674335653
At time: 253.29140424728394 and batch: 700, loss is 5.5583017063140865 and perplexity is 259.38195531924566
At time: 253.79075527191162 and batch: 750, loss is 5.448685369491577 and perplexity is 232.45237596610866
At time: 254.3018033504486 and batch: 800, loss is 5.563345651626587 and perplexity is 260.69356878946223
At time: 254.818097114563 and batch: 850, loss is 5.524631242752076 and perplexity is 250.793838921199
At time: 255.34258818626404 and batch: 900, loss is 5.562203722000122 and perplexity is 260.39604498778544
At time: 255.857031583786 and batch: 950, loss is 5.5194486808776855 and perplexity is 249.49744655175138
At time: 256.36452078819275 and batch: 1000, loss is 5.46260142326355 and perplexity is 235.70980846394912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6299896240234375 and perplexity of 278.659226256439
Finished 23 epochs...
Completing Train Step...
At time: 257.8816497325897 and batch: 50, loss is 5.573736162185669 and perplexity is 263.41642952804006
At time: 258.39831614494324 and batch: 100, loss is 5.5903024482727055 and perplexity is 267.8166081361582
At time: 258.91067242622375 and batch: 150, loss is 5.521393585205078 and perplexity is 249.983167402406
At time: 259.41175746917725 and batch: 200, loss is 5.531953277587891 and perplexity is 252.63689938865926
At time: 259.91640400886536 and batch: 250, loss is 5.610812606811524 and perplexity is 273.3662870801647
At time: 260.4224536418915 and batch: 300, loss is 5.561545400619507 and perplexity is 260.22467711769787
At time: 260.9442677497864 and batch: 350, loss is 5.5317815399169925 and perplexity is 252.59351584137653
At time: 261.4524838924408 and batch: 400, loss is 5.545753507614136 and perplexity is 256.1475146472465
At time: 261.9701805114746 and batch: 450, loss is 5.586178960800171 and perplexity is 266.71454344409506
At time: 262.48544573783875 and batch: 500, loss is 5.585379686355591 and perplexity is 266.5014504967401
At time: 262.9939856529236 and batch: 550, loss is 5.533414440155029 and perplexity is 253.00631278991605
At time: 263.49675154685974 and batch: 600, loss is 5.477208423614502 and perplexity is 239.17809065134225
At time: 264.01469707489014 and batch: 650, loss is 5.468337411880493 and perplexity is 237.06572227766222
At time: 264.5192358493805 and batch: 700, loss is 5.557941904067993 and perplexity is 259.2886458966023
At time: 265.03315901756287 and batch: 750, loss is 5.4479108333587645 and perplexity is 232.27240290858
At time: 265.55193519592285 and batch: 800, loss is 5.563013572692871 and perplexity is 260.60701231969733
At time: 266.0505437850952 and batch: 850, loss is 5.52475606918335 and perplexity is 250.82514657506778
At time: 266.5646073818207 and batch: 900, loss is 5.562766265869141 and perplexity is 260.542570396031
At time: 267.0783622264862 and batch: 950, loss is 5.519346094131469 and perplexity is 249.47185273333614
At time: 267.5945281982422 and batch: 1000, loss is 5.461847677230835 and perplexity is 235.53221007139757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.629981064214939 and perplexity of 278.65684099703464
Finished 24 epochs...
Completing Train Step...
At time: 269.1053376197815 and batch: 50, loss is 5.572476406097412 and perplexity is 263.084798008466
At time: 269.6378405094147 and batch: 100, loss is 5.588207778930664 and perplexity is 267.25620802917024
At time: 270.1497845649719 and batch: 150, loss is 5.519721813201905 and perplexity is 249.5656016764749
At time: 270.6586196422577 and batch: 200, loss is 5.530478677749634 and perplexity is 252.26463559520332
At time: 271.16180753707886 and batch: 250, loss is 5.609711837768555 and perplexity is 273.0655394911412
At time: 271.6665198802948 and batch: 300, loss is 5.559835901260376 and perplexity is 259.78020322109955
At time: 272.17561531066895 and batch: 350, loss is 5.529956474304199 and perplexity is 252.13293652318154
At time: 272.6873574256897 and batch: 400, loss is 5.544278020858765 and perplexity is 255.76985106928325
At time: 273.2034442424774 and batch: 450, loss is 5.585118532180786 and perplexity is 266.4318616174601
At time: 273.7080714702606 and batch: 500, loss is 5.5844290065765385 and perplexity is 266.2482133494552
At time: 274.2260570526123 and batch: 550, loss is 5.532956848144531 and perplexity is 252.89056560709048
At time: 274.77011036872864 and batch: 600, loss is 5.476703481674194 and perplexity is 239.05735008822225
At time: 275.2805757522583 and batch: 650, loss is 5.468295736312866 and perplexity is 237.05584263499264
At time: 275.7919454574585 and batch: 700, loss is 5.557745609283447 and perplexity is 259.2377538828027
At time: 276.2969398498535 and batch: 750, loss is 5.447365932464599 and perplexity is 232.14587194509357
At time: 276.8035795688629 and batch: 800, loss is 5.562723512649536 and perplexity is 260.5314316004139
At time: 277.3122022151947 and batch: 850, loss is 5.524637680053711 and perplexity is 250.79545336198484
At time: 277.8166837692261 and batch: 900, loss is 5.563615417480468 and perplexity is 260.7639044993079
At time: 278.3183753490448 and batch: 950, loss is 5.5192342948913575 and perplexity is 249.44396352879625
At time: 278.8235149383545 and batch: 1000, loss is 5.461733083724976 and perplexity is 235.5052211561096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.629209937118903 and perplexity of 278.4420439849879
Finished 25 epochs...
Completing Train Step...
At time: 280.3126816749573 and batch: 50, loss is 5.571291847229004 and perplexity is 262.773343082608
At time: 280.8134512901306 and batch: 100, loss is 5.587146635055542 and perplexity is 266.9727611564857
At time: 281.32817482948303 and batch: 150, loss is 5.518847913742065 and perplexity is 249.3476017008797
At time: 281.84391236305237 and batch: 200, loss is 5.529318237304688 and perplexity is 251.97206729610426
At time: 282.35147643089294 and batch: 250, loss is 5.609165477752685 and perplexity is 272.91638814752685
At time: 282.8547658920288 and batch: 300, loss is 5.559279794692993 and perplexity is 259.63577790566745
At time: 283.36842346191406 and batch: 350, loss is 5.528754539489746 and perplexity is 251.83007121754278
At time: 283.8775517940521 and batch: 400, loss is 5.543317975997925 and perplexity is 255.52441837025816
At time: 284.38020157814026 and batch: 450, loss is 5.58428689956665 and perplexity is 266.2103803002024
At time: 284.8755156993866 and batch: 500, loss is 5.5837193775177 and perplexity is 266.0593429023033
At time: 285.3939278125763 and batch: 550, loss is 5.532079696655273 and perplexity is 252.66883952874517
At time: 285.9082636833191 and batch: 600, loss is 5.4759743118286135 and perplexity is 238.88310021377103
At time: 286.4127051830292 and batch: 650, loss is 5.468044080734253 and perplexity is 236.99619371555718
At time: 286.9301986694336 and batch: 700, loss is 5.55711501121521 and perplexity is 259.0743305886122
At time: 287.46648478507996 and batch: 750, loss is 5.446450490951538 and perplexity is 231.9334532201231
At time: 287.977995634079 and batch: 800, loss is 5.562330493927002 and perplexity is 260.429057988677
At time: 288.4864239692688 and batch: 850, loss is 5.524270486831665 and perplexity is 250.70337987680472
At time: 289.0075407028198 and batch: 900, loss is 5.563364210128785 and perplexity is 260.69840691652547
At time: 289.5101463794708 and batch: 950, loss is 5.518810253143311 and perplexity is 249.33821129772693
At time: 290.0180230140686 and batch: 1000, loss is 5.461521968841553 and perplexity is 235.4555077466047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6287782250381095 and perplexity of 278.3218631344279
Finished 26 epochs...
Completing Train Step...
At time: 291.50820565223694 and batch: 50, loss is 5.57008768081665 and perplexity is 262.4571106852336
At time: 292.03166246414185 and batch: 100, loss is 5.586478576660157 and perplexity is 266.7944673240327
At time: 292.54304814338684 and batch: 150, loss is 5.51798342704773 and perplexity is 249.1321371634764
At time: 293.04153513908386 and batch: 200, loss is 5.527953214645386 and perplexity is 251.6283543560793
At time: 293.5435791015625 and batch: 250, loss is 5.608235569000244 and perplexity is 272.6627187724694
At time: 294.05330896377563 and batch: 300, loss is 5.557519865036011 and perplexity is 259.17923905608654
At time: 294.56945729255676 and batch: 350, loss is 5.526229887008667 and perplexity is 251.1950896945459
At time: 295.08289790153503 and batch: 400, loss is 5.539179086685181 and perplexity is 254.46901668791315
At time: 295.59967494010925 and batch: 450, loss is 5.582823657989502 and perplexity is 265.82113505284394
At time: 296.10356307029724 and batch: 500, loss is 5.583682794570922 and perplexity is 266.04960984555527
At time: 296.62448501586914 and batch: 550, loss is 5.532139558792114 and perplexity is 252.68396527811768
At time: 297.13049387931824 and batch: 600, loss is 5.475572090148926 and perplexity is 238.78703557288986
At time: 297.6475291252136 and batch: 650, loss is 5.467772760391235 and perplexity is 236.93190054940075
At time: 298.1553213596344 and batch: 700, loss is 5.556750278472901 and perplexity is 258.97985492778554
At time: 298.65841841697693 and batch: 750, loss is 5.445838623046875 and perplexity is 231.79158399112015
At time: 299.1715703010559 and batch: 800, loss is 5.56179575920105 and perplexity is 260.28983475478987
At time: 299.6783127784729 and batch: 850, loss is 5.523542013168335 and perplexity is 250.52081557197403
At time: 300.19374895095825 and batch: 900, loss is 5.563254041671753 and perplexity is 260.6696877572866
At time: 300.7187521457672 and batch: 950, loss is 5.518405275344849 and perplexity is 249.23725530169133
At time: 301.23117184638977 and batch: 1000, loss is 5.461383724212647 and perplexity is 235.42295953717175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.627412377334222 and perplexity of 277.94197734884574
Finished 27 epochs...
Completing Train Step...
At time: 302.719229221344 and batch: 50, loss is 5.569210968017578 and perplexity is 262.22711201321005
At time: 303.241238117218 and batch: 100, loss is 5.58564154624939 and perplexity is 266.57124567614477
At time: 303.75360679626465 and batch: 150, loss is 5.516778554916382 and perplexity is 248.8321455564403
At time: 304.27083826065063 and batch: 200, loss is 5.5280193424224855 and perplexity is 251.64499452999087
At time: 304.7776679992676 and batch: 250, loss is 5.608482398986816 and perplexity is 272.73002841436016
At time: 305.2724266052246 and batch: 300, loss is 5.556594905853271 and perplexity is 258.93961967510336
At time: 305.7826681137085 and batch: 350, loss is 5.525294361114502 and perplexity is 250.96020007344225
At time: 306.29539036750793 and batch: 400, loss is 5.540688953399658 and perplexity is 254.85352118830326
At time: 306.81333780288696 and batch: 450, loss is 5.583617315292359 and perplexity is 266.0321896793766
At time: 307.3125925064087 and batch: 500, loss is 5.5825315189361575 and perplexity is 265.74348966026855
At time: 307.8338484764099 and batch: 550, loss is 5.531320476531983 and perplexity is 252.47708106390016
At time: 308.3454306125641 and batch: 600, loss is 5.474964809417725 and perplexity is 238.64206882953587
At time: 308.8480966091156 and batch: 650, loss is 5.467680501937866 and perplexity is 236.91004258700863
At time: 309.34659719467163 and batch: 700, loss is 5.556304874420166 and perplexity is 258.8645299358401
At time: 309.8376295566559 and batch: 750, loss is 5.444995527267456 and perplexity is 231.596243841751
At time: 310.3436803817749 and batch: 800, loss is 5.561384534835815 and perplexity is 260.1828192379179
At time: 310.84557271003723 and batch: 850, loss is 5.523118410110474 and perplexity is 250.41471666193777
At time: 311.3558647632599 and batch: 900, loss is 5.562678861618042 and perplexity is 260.51979886296505
At time: 311.8524444103241 and batch: 950, loss is 5.518421936035156 and perplexity is 249.24140780100663
At time: 312.3774211406708 and batch: 1000, loss is 5.461403245925903 and perplexity is 235.42755544154167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.626999645698361 and perplexity of 277.82728557195054
Finished 28 epochs...
Completing Train Step...
At time: 313.8770866394043 and batch: 50, loss is 5.568404693603515 and perplexity is 262.0157702133122
At time: 314.387531042099 and batch: 100, loss is 5.584886665344238 and perplexity is 266.37009206598333
At time: 314.89325857162476 and batch: 150, loss is 5.516815824508667 and perplexity is 248.84141960187154
At time: 315.3976149559021 and batch: 200, loss is 5.5269077777862545 and perplexity is 251.36543025885072
At time: 315.9076933860779 and batch: 250, loss is 5.607165908813476 and perplexity is 272.3712182488289
At time: 316.4153895378113 and batch: 300, loss is 5.553795986175537 and perplexity is 258.215881792827
At time: 316.92428398132324 and batch: 350, loss is 5.523859806060791 and perplexity is 250.60044195825486
At time: 317.4272356033325 and batch: 400, loss is 5.538425483703613 and perplexity is 254.27732031877062
At time: 317.9255020618439 and batch: 450, loss is 5.582130289077758 and perplexity is 265.636886825092
At time: 318.41814517974854 and batch: 500, loss is 5.582003087997436 and perplexity is 265.603099675041
At time: 318.9260585308075 and batch: 550, loss is 5.530952739715576 and perplexity is 252.3842530150857
At time: 319.4314467906952 and batch: 600, loss is 5.474610166549683 and perplexity is 238.55745112722985
At time: 319.9374499320984 and batch: 650, loss is 5.4674898052215575 and perplexity is 236.86486892719796
At time: 320.4371249675751 and batch: 700, loss is 5.555603885650635 and perplexity is 258.683132393773
At time: 320.94234108924866 and batch: 750, loss is 5.443869094848633 and perplexity is 231.33551319990286
At time: 321.4436433315277 and batch: 800, loss is 5.560678586959839 and perplexity is 259.9992085467256
At time: 321.9566230773926 and batch: 850, loss is 5.522690267562866 and perplexity is 250.3075264151777
At time: 322.46562099456787 and batch: 900, loss is 5.563000841140747 and perplexity is 260.6036944090573
At time: 322.970529794693 and batch: 950, loss is 5.518070306777954 and perplexity is 249.15378263660617
At time: 323.478063583374 and batch: 1000, loss is 5.462530307769775 and perplexity is 235.69304644055953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.626668046160442 and perplexity of 277.73517344544695
Finished 29 epochs...
Completing Train Step...
At time: 325.0185513496399 and batch: 50, loss is 5.564946975708008 and perplexity is 261.11135809780046
At time: 325.5615200996399 and batch: 100, loss is 5.584296436309814 and perplexity is 266.2129190923328
At time: 326.0649383068085 and batch: 150, loss is 5.515448226928711 and perplexity is 248.50133727951697
At time: 326.5750277042389 and batch: 200, loss is 5.526034250259399 and perplexity is 251.14595151054
At time: 327.07842803001404 and batch: 250, loss is 5.605319442749024 and perplexity is 271.86875806799407
At time: 327.59453201293945 and batch: 300, loss is 5.552573843002319 and perplexity is 257.9004977771441
At time: 328.10647416114807 and batch: 350, loss is 5.523239402770996 and perplexity is 250.44501683775317
At time: 328.61064052581787 and batch: 400, loss is 5.538115644454956 and perplexity is 254.19854742898923
At time: 329.1175756454468 and batch: 450, loss is 5.581403656005859 and perplexity is 265.44393638858253
At time: 329.6168749332428 and batch: 500, loss is 5.58060622215271 and perplexity is 265.2323467831627
At time: 330.1287291049957 and batch: 550, loss is 5.530312519073487 and perplexity is 252.22272311947128
At time: 330.63992524147034 and batch: 600, loss is 5.473957481384278 and perplexity is 238.40179901921886
At time: 331.1535654067993 and batch: 650, loss is 5.465993118286133 and perplexity is 236.5106215372621
At time: 331.65978169441223 and batch: 700, loss is 5.55512001991272 and perplexity is 258.55799476637776
At time: 332.1787943840027 and batch: 750, loss is 5.443203601837158 and perplexity is 231.18161224824624
At time: 332.6895513534546 and batch: 800, loss is 5.559032430648804 and perplexity is 259.5715612923965
At time: 333.2045042514801 and batch: 850, loss is 5.52234070777893 and perplexity is 250.22004426133876
At time: 333.7347857952118 and batch: 900, loss is 5.5615921974182125 and perplexity is 260.23685508447386
At time: 334.24525594711304 and batch: 950, loss is 5.517299900054931 and perplexity is 248.96190680810082
At time: 334.76241159439087 and batch: 1000, loss is 5.46010232925415 and perplexity is 235.12148294025351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.626495361328125 and perplexity of 277.68721693439215
Finished 30 epochs...
Completing Train Step...
At time: 336.2975935935974 and batch: 50, loss is 5.56523642539978 and perplexity is 261.18694763907706
At time: 336.81092739105225 and batch: 100, loss is 5.5833961963653564 and perplexity is 265.97337143020036
At time: 337.31822323799133 and batch: 150, loss is 5.51396222114563 and perplexity is 248.13233709132575
At time: 337.82101249694824 and batch: 200, loss is 5.524464426040649 and perplexity is 250.7520058070724
At time: 338.3240144252777 and batch: 250, loss is 5.60400839805603 and perplexity is 271.51255952266513
At time: 338.83093452453613 and batch: 300, loss is 5.550784854888916 and perplexity is 257.43952930867874
At time: 339.3408913612366 and batch: 350, loss is 5.5216910266876225 and perplexity is 250.05753382561065
At time: 339.87176275253296 and batch: 400, loss is 5.536886796951294 and perplexity is 253.88636802826952
At time: 340.39281034469604 and batch: 450, loss is 5.580455665588379 and perplexity is 265.1924173181785
At time: 340.89801716804504 and batch: 500, loss is 5.579717893600463 and perplexity is 264.99683793663945
At time: 341.4118616580963 and batch: 550, loss is 5.530314502716064 and perplexity is 252.22322343970006
At time: 341.9166145324707 and batch: 600, loss is 5.473112831115722 and perplexity is 238.2005178936983
At time: 342.4212644100189 and batch: 650, loss is 5.466010284423828 and perplexity is 236.51468154600502
At time: 342.91748237609863 and batch: 700, loss is 5.554536094665528 and perplexity is 258.4070602968826
At time: 343.42039465904236 and batch: 750, loss is 5.44228455543518 and perplexity is 230.96924322274847
At time: 343.9314432144165 and batch: 800, loss is 5.558148059844971 and perplexity is 259.34210525914744
At time: 344.4493486881256 and batch: 850, loss is 5.521174831390381 and perplexity is 249.9284886117417
At time: 344.96346068382263 and batch: 900, loss is 5.561526317596435 and perplexity is 260.2197112915622
At time: 345.47348952293396 and batch: 950, loss is 5.516744470596313 and perplexity is 248.82366442648544
At time: 346.0017991065979 and batch: 1000, loss is 5.460108251571655 and perplexity is 235.12287540845102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.623573117139863 and perplexity of 276.87693158100325
Finished 31 epochs...
Completing Train Step...
At time: 347.5897226333618 and batch: 50, loss is 5.563579130172729 and perplexity is 260.7544422509389
At time: 348.1111948490143 and batch: 100, loss is 5.582822656631469 and perplexity is 265.8208688708483
At time: 348.6307837963104 and batch: 150, loss is 5.513874120712281 and perplexity is 248.11047748783437
At time: 349.1276111602783 and batch: 200, loss is 5.524191427230835 and perplexity is 250.68356015114492
At time: 349.6195685863495 and batch: 250, loss is 5.603334255218506 and perplexity is 271.3295829584649
At time: 350.112256526947 and batch: 300, loss is 5.550853042602539 and perplexity is 257.4570841200829
At time: 350.61984395980835 and batch: 350, loss is 5.52174898147583 and perplexity is 250.07202627697265
At time: 351.1291468143463 and batch: 400, loss is 5.53701717376709 and perplexity is 253.91947108239552
At time: 351.64565682411194 and batch: 450, loss is 5.580428466796875 and perplexity is 265.18520450300196
At time: 352.14905405044556 and batch: 500, loss is 5.579043416976929 and perplexity is 264.8181640266019
At time: 352.6955325603485 and batch: 550, loss is 5.52927261352539 and perplexity is 251.96057164035642
At time: 353.19730591773987 and batch: 600, loss is 5.47231017112732 and perplexity is 238.00940058013938
At time: 353.7059118747711 and batch: 650, loss is 5.465389804840088 and perplexity is 236.36797453391057
At time: 354.2198917865753 and batch: 700, loss is 5.554172582626343 and perplexity is 258.3131432904699
At time: 354.7256133556366 and batch: 750, loss is 5.441723051071167 and perplexity is 230.83958938872945
At time: 355.2387194633484 and batch: 800, loss is 5.557795143127441 and perplexity is 259.25059524329913
At time: 355.7577893733978 and batch: 850, loss is 5.520535087585449 and perplexity is 249.76864954295485
At time: 356.265741109848 and batch: 900, loss is 5.560917873382568 and perplexity is 260.06143027136113
At time: 356.76151847839355 and batch: 950, loss is 5.516377925872803 and perplexity is 248.73247613851925
At time: 357.2827332019806 and batch: 1000, loss is 5.45998782157898 and perplexity is 235.09456126725794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.625161519864711 and perplexity of 277.31707312204503
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 358.89415740966797 and batch: 50, loss is 5.558970365524292 and perplexity is 259.55545145106
At time: 359.4057340621948 and batch: 100, loss is 5.572575855255127 and perplexity is 263.1109628710508
At time: 359.9169068336487 and batch: 150, loss is 5.4983875179290775 and perplexity is 244.29768885118514
At time: 360.4103171825409 and batch: 200, loss is 5.5131876754760745 and perplexity is 247.9402216748414
At time: 360.9171950817108 and batch: 250, loss is 5.58999997138977 and perplexity is 267.7356120536697
At time: 361.4180209636688 and batch: 300, loss is 5.533294525146484 and perplexity is 252.97597535474912
At time: 361.91419529914856 and batch: 350, loss is 5.506731805801391 and perplexity is 246.34470767334133
At time: 362.42352747917175 and batch: 400, loss is 5.521890745162964 and perplexity is 250.10747992242716
At time: 362.9266815185547 and batch: 450, loss is 5.5604111766815185 and perplexity is 259.9296913812215
At time: 363.4282910823822 and batch: 500, loss is 5.558386812210083 and perplexity is 259.4040311923385
At time: 363.9294936656952 and batch: 550, loss is 5.5090955543518065 and perplexity is 246.9276933633532
At time: 364.4360308647156 and batch: 600, loss is 5.452666072845459 and perplexity is 233.37954408503
At time: 364.9443356990814 and batch: 650, loss is 5.439301033020019 and perplexity is 230.28116826231727
At time: 365.4504852294922 and batch: 700, loss is 5.526803140640259 and perplexity is 251.33912947366997
At time: 365.97249269485474 and batch: 750, loss is 5.418159112930298 and perplexity is 225.46368704830246
At time: 366.48536443710327 and batch: 800, loss is 5.533504180908203 and perplexity is 253.0290187857948
At time: 366.99373841285706 and batch: 850, loss is 5.498263664245606 and perplexity is 244.26743355621082
At time: 367.5079426765442 and batch: 900, loss is 5.532980585098267 and perplexity is 252.8965685299918
At time: 368.0075988769531 and batch: 950, loss is 5.486992778778077 and perplexity is 241.5297801590297
At time: 368.52316975593567 and batch: 1000, loss is 5.436103048324585 and perplexity is 229.54590891127484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.605193347465701 and perplexity of 271.8344788611875
Finished 33 epochs...
Completing Train Step...
At time: 370.0523269176483 and batch: 50, loss is 5.549658555984497 and perplexity is 257.1497386749352
At time: 370.572163105011 and batch: 100, loss is 5.566082468032837 and perplexity is 261.4080164356046
At time: 371.0796480178833 and batch: 150, loss is 5.4928215217590335 and perplexity is 242.94170604895996
At time: 371.584267616272 and batch: 200, loss is 5.509457540512085 and perplexity is 247.01709395085174
At time: 372.0879228115082 and batch: 250, loss is 5.587432909011841 and perplexity is 267.0491994456696
At time: 372.5893383026123 and batch: 300, loss is 5.530613193511963 and perplexity is 252.2985714473714
At time: 373.09778094291687 and batch: 350, loss is 5.5041061210632325 and perplexity is 245.69873256868854
At time: 373.5983135700226 and batch: 400, loss is 5.519590845108032 and perplexity is 249.53291868558853
At time: 374.11447930336 and batch: 450, loss is 5.558003463745117 and perplexity is 259.304608113235
At time: 374.62035274505615 and batch: 500, loss is 5.557659788131714 and perplexity is 259.21550675484235
At time: 375.12308835983276 and batch: 550, loss is 5.508258876800537 and perplexity is 246.7211809097433
At time: 375.6312301158905 and batch: 600, loss is 5.452453765869141 and perplexity is 233.33000123902275
At time: 376.15957951545715 and batch: 650, loss is 5.437831144332886 and perplexity is 229.9429292259944
At time: 376.6721844673157 and batch: 700, loss is 5.525215845108033 and perplexity is 250.94049645428473
At time: 377.1723349094391 and batch: 750, loss is 5.41871976852417 and perplexity is 225.590129967814
At time: 377.6780250072479 and batch: 800, loss is 5.534463090896606 and perplexity is 253.27176720760684
At time: 378.17655777931213 and batch: 850, loss is 5.498852672576905 and perplexity is 244.41135148991063
At time: 378.71460819244385 and batch: 900, loss is 5.533450984954834 and perplexity is 253.0155590239162
At time: 379.21893215179443 and batch: 950, loss is 5.488779811859131 and perplexity is 241.96178775720077
At time: 379.7409875392914 and batch: 1000, loss is 5.437835865020752 and perplexity is 229.94401471735227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604738561118522 and perplexity of 271.7108803590947
Finished 34 epochs...
Completing Train Step...
At time: 381.30539989471436 and batch: 50, loss is 5.547767724990845 and perplexity is 256.6639713756996
At time: 381.8150336742401 and batch: 100, loss is 5.564542140960693 and perplexity is 261.0056725411602
At time: 382.3296654224396 and batch: 150, loss is 5.491358346939087 and perplexity is 242.58649978975268
At time: 382.8270502090454 and batch: 200, loss is 5.508533477783203 and perplexity is 246.78894009140888
At time: 383.33860039711 and batch: 250, loss is 5.586870307922363 and perplexity is 266.8989995303972
At time: 383.84780836105347 and batch: 300, loss is 5.529972124099731 and perplexity is 252.13688238296098
At time: 384.35074186325073 and batch: 350, loss is 5.503301315307617 and perplexity is 245.5010723642738
At time: 384.84242272377014 and batch: 400, loss is 5.518842172622681 and perplexity is 249.34617017063943
At time: 385.33732771873474 and batch: 450, loss is 5.557095899581909 and perplexity is 259.0693793023221
At time: 385.8281452655792 and batch: 500, loss is 5.557745552062988 and perplexity is 259.23773904909973
At time: 386.33813762664795 and batch: 550, loss is 5.508157081604004 and perplexity is 246.69606715689525
At time: 386.83917331695557 and batch: 600, loss is 5.45277437210083 and perplexity is 233.40482028454525
At time: 387.3518395423889 and batch: 650, loss is 5.436992740631103 and perplexity is 229.7502250161819
At time: 387.85561776161194 and batch: 700, loss is 5.524319925308228 and perplexity is 250.71577457635985
At time: 388.36770820617676 and batch: 750, loss is 5.419123849868774 and perplexity is 225.68130515071553
At time: 388.87297892570496 and batch: 800, loss is 5.534983987808228 and perplexity is 253.40373005548227
At time: 389.3851947784424 and batch: 850, loss is 5.499077844619751 and perplexity is 244.46639228981252
At time: 389.8997519016266 and batch: 900, loss is 5.533524341583252 and perplexity is 253.03412007304314
At time: 390.4045989513397 and batch: 950, loss is 5.48969443321228 and perplexity is 242.183192410207
At time: 390.9095413684845 and batch: 1000, loss is 5.438587579727173 and perplexity is 230.11693199895808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604594160870808 and perplexity of 271.67164807330835
Finished 35 epochs...
Completing Train Step...
At time: 392.41093039512634 and batch: 50, loss is 5.546679258346558 and perplexity is 256.38475319147415
At time: 392.91634249687195 and batch: 100, loss is 5.563611822128296 and perplexity is 260.76296696292275
At time: 393.41454672813416 and batch: 150, loss is 5.490489511489868 and perplexity is 242.37582357396857
At time: 393.92506766319275 and batch: 200, loss is 5.50800817489624 and perplexity is 246.65933519260233
At time: 394.42875814437866 and batch: 250, loss is 5.586643161773682 and perplexity is 266.83838133542235
At time: 394.927659034729 and batch: 300, loss is 5.529659337997437 and perplexity is 252.0580298029142
At time: 395.4406499862671 and batch: 350, loss is 5.502889280319214 and perplexity is 245.39993816961342
At time: 395.9523239135742 and batch: 400, loss is 5.518451080322266 and perplexity is 249.2486718700078
At time: 396.47410702705383 and batch: 450, loss is 5.556575698852539 and perplexity is 258.9346462694009
At time: 396.9687395095825 and batch: 500, loss is 5.557899084091186 and perplexity is 259.2775434005047
At time: 397.4653308391571 and batch: 550, loss is 5.508170595169068 and perplexity is 246.69940092277514
At time: 397.9640920162201 and batch: 600, loss is 5.453080148696899 and perplexity is 233.47620092870704
At time: 398.47132086753845 and batch: 650, loss is 5.436359424591064 and perplexity is 229.6047665789191
At time: 398.96957206726074 and batch: 700, loss is 5.523654260635376 and perplexity is 250.54893747723986
At time: 399.48099160194397 and batch: 750, loss is 5.419335527420044 and perplexity is 225.72908187320854
At time: 399.9852809906006 and batch: 800, loss is 5.535269231796264 and perplexity is 253.4760222559943
At time: 400.4902992248535 and batch: 850, loss is 5.4991121673583985 and perplexity is 244.47478318990173
At time: 400.99292159080505 and batch: 900, loss is 5.533409976959229 and perplexity is 253.00518357572327
At time: 401.49430561065674 and batch: 950, loss is 5.490253896713257 and perplexity is 242.31872297557763
At time: 402.0013861656189 and batch: 1000, loss is 5.4390231895446775 and perplexity is 230.21719502990567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6045048411299545 and perplexity of 271.6473835157734
Finished 36 epochs...
Completing Train Step...
At time: 403.4981551170349 and batch: 50, loss is 5.545917882919311 and perplexity is 256.18962243378184
At time: 404.01701378822327 and batch: 100, loss is 5.562945108413697 and perplexity is 260.5891706592164
At time: 404.5222952365875 and batch: 150, loss is 5.489884338378906 and perplexity is 242.22918861703607
At time: 405.04473853111267 and batch: 200, loss is 5.507649698257446 and perplexity is 246.5709294298425
At time: 405.5694270133972 and batch: 250, loss is 5.586534929275513 and perplexity is 266.80950231365557
At time: 406.07962584495544 and batch: 300, loss is 5.529445819854736 and perplexity is 252.0042165857914
At time: 406.5947992801666 and batch: 350, loss is 5.502619190216064 and perplexity is 245.33366702499333
At time: 407.10265946388245 and batch: 400, loss is 5.5181879043579105 and perplexity is 249.18308424134636
At time: 407.60964727401733 and batch: 450, loss is 5.556211357116699 and perplexity is 258.84032275494997
At time: 408.11292266845703 and batch: 500, loss is 5.558040018081665 and perplexity is 259.3140869943945
At time: 408.63264179229736 and batch: 550, loss is 5.508201942443848 and perplexity is 246.70713439789506
At time: 409.14195919036865 and batch: 600, loss is 5.453309650421143 and perplexity is 233.52979026857773
At time: 409.65681195259094 and batch: 650, loss is 5.4358625411987305 and perplexity is 229.4907081228284
At time: 410.1620502471924 and batch: 700, loss is 5.523097829818726 and perplexity is 250.40956310704186
At time: 410.6648073196411 and batch: 750, loss is 5.419435873031616 and perplexity is 225.75173393247707
At time: 411.1667277812958 and batch: 800, loss is 5.535424718856811 and perplexity is 253.51543756181962
At time: 411.677264213562 and batch: 850, loss is 5.499054346084595 and perplexity is 244.46064775519318
At time: 412.18831849098206 and batch: 900, loss is 5.533211307525635 and perplexity is 252.95492417187486
At time: 412.69198274612427 and batch: 950, loss is 5.490639057159424 and perplexity is 242.41207253915928
At time: 413.20811009407043 and batch: 1000, loss is 5.439316015243531 and perplexity is 230.28461841209605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.60443599049638 and perplexity of 271.6286810651545
Finished 37 epochs...
Completing Train Step...
At time: 414.7358431816101 and batch: 50, loss is 5.545333938598633 and perplexity is 256.04006562941794
At time: 415.2307724952698 and batch: 100, loss is 5.562425260543823 and perplexity is 260.4537391388807
At time: 415.724023103714 and batch: 150, loss is 5.489419870376587 and perplexity is 242.1167070337659
At time: 416.2306001186371 and batch: 200, loss is 5.507371129989624 and perplexity is 246.50225215923425
At time: 416.72777485847473 and batch: 250, loss is 5.5864832305908205 and perplexity is 266.79570896987445
At time: 417.23816204071045 and batch: 300, loss is 5.529264936447143 and perplexity is 251.95863732675775
At time: 417.76480317115784 and batch: 350, loss is 5.502418479919434 and perplexity is 245.2844309731678
At time: 418.27021527290344 and batch: 400, loss is 5.5179887294769285 and perplexity is 249.13345817249711
At time: 418.7692713737488 and batch: 450, loss is 5.555932064056396 and perplexity is 258.76804054350777
At time: 419.2575397491455 and batch: 500, loss is 5.558158588409424 and perplexity is 259.34483577359225
At time: 419.77093625068665 and batch: 550, loss is 5.508230333328247 and perplexity is 246.7141387310573
At time: 420.27481722831726 and batch: 600, loss is 5.453475103378296 and perplexity is 233.56843165953916
At time: 420.77750182151794 and batch: 650, loss is 5.435455360412598 and perplexity is 229.39728293764577
At time: 421.27258682250977 and batch: 700, loss is 5.52254693031311 and perplexity is 250.27165059398038
At time: 421.7741994857788 and batch: 750, loss is 5.419476375579834 and perplexity is 225.7608776381365
At time: 422.2745966911316 and batch: 800, loss is 5.535516109466553 and perplexity is 253.53860755098083
At time: 422.7781717777252 and batch: 850, loss is 5.498956937789917 and perplexity is 244.436836420109
At time: 423.2722542285919 and batch: 900, loss is 5.532970533370972 and perplexity is 252.89402649542706
At time: 423.7673316001892 and batch: 950, loss is 5.4909196472167965 and perplexity is 242.4801005000393
At time: 424.2720630168915 and batch: 1000, loss is 5.439521598815918 and perplexity is 230.33196601339276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604371978015434 and perplexity of 271.61129399588435
Finished 38 epochs...
Completing Train Step...
At time: 425.76499581336975 and batch: 50, loss is 5.5448651790618895 and perplexity is 255.9200725330171
At time: 426.28679060935974 and batch: 100, loss is 5.5619940757751465 and perplexity is 260.3414596619588
At time: 426.7832381725311 and batch: 150, loss is 5.489035911560059 and perplexity is 242.0237620341395
At time: 427.29197120666504 and batch: 200, loss is 5.507135543823242 and perplexity is 246.4441864786476
At time: 427.78408551216125 and batch: 250, loss is 5.586466569900512 and perplexity is 266.79126400621993
At time: 428.2893249988556 and batch: 300, loss is 5.529101409912109 and perplexity is 251.91743877244406
At time: 428.7991609573364 and batch: 350, loss is 5.502253875732422 and perplexity is 245.2440594515728
At time: 429.30963611602783 and batch: 400, loss is 5.517821731567383 and perplexity is 249.09185687954567
At time: 429.812442779541 and batch: 450, loss is 5.555703573226928 and perplexity is 258.70892117365844
At time: 430.31134557724 and batch: 500, loss is 5.558257846832276 and perplexity is 259.3705792105712
At time: 430.8251357078552 and batch: 550, loss is 5.508253221511841 and perplexity is 246.71978563418318
At time: 431.327406167984 and batch: 600, loss is 5.453596754074097 and perplexity is 233.59684715011463
At time: 431.82603669166565 and batch: 650, loss is 5.435074234008789 and perplexity is 229.30987023485898
At time: 432.327698469162 and batch: 700, loss is 5.521984434127807 and perplexity is 250.13091333102935
At time: 432.8284680843353 and batch: 750, loss is 5.419510812759399 and perplexity is 225.76865233988707
At time: 433.3235869407654 and batch: 800, loss is 5.535567007064819 and perplexity is 253.5515123855828
At time: 433.82107162475586 and batch: 850, loss is 5.498842477798462 and perplexity is 244.4088597830345
At time: 434.3167734146118 and batch: 900, loss is 5.532718477249145 and perplexity is 252.83029104066864
At time: 434.81520199775696 and batch: 950, loss is 5.4911256027221675 and perplexity is 242.53004575475376
At time: 435.31769371032715 and batch: 1000, loss is 5.439670343399047 and perplexity is 230.36622919382694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604306476872142 and perplexity of 271.5935037282443
Finished 39 epochs...
Completing Train Step...
At time: 436.7934112548828 and batch: 50, loss is 5.54447154045105 and perplexity is 255.819352336159
At time: 437.3210484981537 and batch: 100, loss is 5.561624984741211 and perplexity is 260.2453876941776
At time: 437.81446051597595 and batch: 150, loss is 5.488709335327148 and perplexity is 241.9447357304182
At time: 438.32229113578796 and batch: 200, loss is 5.506927738189697 and perplexity is 246.3929793090963
At time: 438.8180019855499 and batch: 250, loss is 5.586467866897583 and perplexity is 266.7916100339322
At time: 439.31370854377747 and batch: 300, loss is 5.52895260810852 and perplexity is 251.87995579203587
At time: 439.81548142433167 and batch: 350, loss is 5.502102041244507 and perplexity is 245.20682577214166
At time: 440.3163712024689 and batch: 400, loss is 5.517673835754395 and perplexity is 249.05501996094387
At time: 440.8130614757538 and batch: 450, loss is 5.555513439178466 and perplexity is 258.65973647509355
At time: 441.3151934146881 and batch: 500, loss is 5.558343410491943 and perplexity is 259.3927728560098
At time: 441.81672406196594 and batch: 550, loss is 5.5082910442352295 and perplexity is 246.72911742486554
At time: 442.3305447101593 and batch: 600, loss is 5.453683242797852 and perplexity is 233.61705151701025
At time: 442.83995175361633 and batch: 650, loss is 5.434713897705078 and perplexity is 229.22725644908095
At time: 443.3689148426056 and batch: 700, loss is 5.5214620780944825 and perplexity is 250.00029005822728
At time: 443.8740940093994 and batch: 750, loss is 5.419516716003418 and perplexity is 225.7699851112675
At time: 444.3776295185089 and batch: 800, loss is 5.535595989227295 and perplexity is 253.55886096319895
At time: 444.8817608356476 and batch: 850, loss is 5.4987109375 and perplexity is 244.3767122830642
At time: 445.40351033210754 and batch: 900, loss is 5.532472734451294 and perplexity is 252.76816745109176
At time: 445.9100012779236 and batch: 950, loss is 5.491283102035522 and perplexity is 242.56824707869023
At time: 446.40736293792725 and batch: 1000, loss is 5.439778261184692 and perplexity is 230.39109114866827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604255118021151 and perplexity of 271.5795553561455
Finished 40 epochs...
Completing Train Step...
At time: 447.9323847293854 and batch: 50, loss is 5.544134073257446 and perplexity is 255.7330362624973
At time: 448.4367175102234 and batch: 100, loss is 5.561304540634155 and perplexity is 260.16200695344946
At time: 448.9600648880005 and batch: 150, loss is 5.48842565536499 and perplexity is 241.8761105911905
At time: 449.4681990146637 and batch: 200, loss is 5.506751832962036 and perplexity is 246.34964130777902
At time: 449.9644458293915 and batch: 250, loss is 5.586474981307983 and perplexity is 266.79350810568917
At time: 450.4599406719208 and batch: 300, loss is 5.528794269561768 and perplexity is 251.8400766431663
At time: 450.9704122543335 and batch: 350, loss is 5.501971807479858 and perplexity is 245.17489364346952
At time: 451.482075214386 and batch: 400, loss is 5.517542343139649 and perplexity is 249.02227321817819
At time: 451.97937512397766 and batch: 450, loss is 5.55535177230835 and perplexity is 258.6179231450785
At time: 452.47845816612244 and batch: 500, loss is 5.55840217590332 and perplexity is 259.40801662691376
At time: 452.9790692329407 and batch: 550, loss is 5.5083218193054195 and perplexity is 246.73671064761257
At time: 453.48041129112244 and batch: 600, loss is 5.453743343353271 and perplexity is 233.6310924534917
At time: 453.9828975200653 and batch: 650, loss is 5.434367475509643 and perplexity is 229.14786079265124
At time: 454.4795331954956 and batch: 700, loss is 5.521012163162231 and perplexity is 249.88783649382876
At time: 454.9754841327667 and batch: 750, loss is 5.419482021331787 and perplexity is 225.76215223165036
At time: 455.48160576820374 and batch: 800, loss is 5.535596151351928 and perplexity is 253.55890207133973
At time: 455.9847011566162 and batch: 850, loss is 5.498551540374756 and perplexity is 244.33776244197836
At time: 456.501323223114 and batch: 900, loss is 5.532228298187256 and perplexity is 252.70638929529076
At time: 457.00248169898987 and batch: 950, loss is 5.491401844024658 and perplexity is 242.59705182498206
At time: 457.5140628814697 and batch: 1000, loss is 5.439843015670776 and perplexity is 230.4060104884159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6041929663681405 and perplexity of 271.56267676237786
Finished 41 epochs...
Completing Train Step...
At time: 458.99841141700745 and batch: 50, loss is 5.543804025650024 and perplexity is 255.64864611288965
At time: 459.5129954814911 and batch: 100, loss is 5.560830965042114 and perplexity is 260.0388297461414
At time: 460.0132658481598 and batch: 150, loss is 5.488120279312134 and perplexity is 241.80225869613193
At time: 460.5033550262451 and batch: 200, loss is 5.50627571105957 and perplexity is 246.23237676621133
At time: 461.00627851486206 and batch: 250, loss is 5.586514978408814 and perplexity is 266.8041792859405
At time: 461.50532579421997 and batch: 300, loss is 5.528401365280152 and perplexity is 251.74114703497656
At time: 462.01399779319763 and batch: 350, loss is 5.501944751739502 and perplexity is 245.16826034493997
At time: 462.5182373523712 and batch: 400, loss is 5.517397241592407 and perplexity is 248.98614232242443
At time: 463.02824997901917 and batch: 450, loss is 5.555158414840698 and perplexity is 258.56792227254675
At time: 463.5313115119934 and batch: 500, loss is 5.558301887512207 and perplexity is 259.3820023187727
At time: 464.0423581600189 and batch: 550, loss is 5.508460559844971 and perplexity is 246.77094540679448
At time: 464.55832982063293 and batch: 600, loss is 5.453646564483643 and perplexity is 233.6084829945306
At time: 465.06193947792053 and batch: 650, loss is 5.433856010437012 and perplexity is 229.03068963241986
At time: 465.5585265159607 and batch: 700, loss is 5.5204212093353275 and perplexity is 249.74020794568014
At time: 466.0566291809082 and batch: 750, loss is 5.419416875839233 and perplexity is 225.7474453240927
At time: 466.565575838089 and batch: 800, loss is 5.535240602493286 and perplexity is 253.46876551803365
At time: 467.06561040878296 and batch: 850, loss is 5.497651414871216 and perplexity is 244.11792674522817
At time: 467.58423590660095 and batch: 900, loss is 5.532066040039062 and perplexity is 252.66538895093686
At time: 468.08548951148987 and batch: 950, loss is 5.491086988449097 and perplexity is 242.5206808141505
At time: 468.60206484794617 and batch: 1000, loss is 5.439521398544311 and perplexity is 230.33191988444455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604179196241425 and perplexity of 271.5589373356539
Finished 42 epochs...
Completing Train Step...
At time: 470.2024595737457 and batch: 50, loss is 5.543098630905152 and perplexity is 255.4683764894853
At time: 470.7162926197052 and batch: 100, loss is 5.5601576232910155 and perplexity is 259.86379368132503
At time: 471.2216076850891 and batch: 150, loss is 5.487773380279541 and perplexity is 241.71839227394287
At time: 471.72636914253235 and batch: 200, loss is 5.506156539916992 and perplexity is 246.20303472092965
At time: 472.22862219810486 and batch: 250, loss is 5.586457271575927 and perplexity is 266.78878330598394
At time: 472.72358322143555 and batch: 300, loss is 5.5281494140625 and perplexity is 251.67772853596756
At time: 473.233190536499 and batch: 350, loss is 5.50187331199646 and perplexity is 245.1507462130288
At time: 473.7304801940918 and batch: 400, loss is 5.517260789871216 and perplexity is 248.95217005259178
At time: 474.2563202381134 and batch: 450, loss is 5.554967184066772 and perplexity is 258.51848085616894
At time: 474.77608036994934 and batch: 500, loss is 5.558241901397705 and perplexity is 259.36644346694396
At time: 475.27902340888977 and batch: 550, loss is 5.508493137359619 and perplexity is 246.77898472183307
At time: 475.78307342529297 and batch: 600, loss is 5.453549108505249 and perplexity is 233.58571756059106
At time: 476.27834153175354 and batch: 650, loss is 5.433412866592407 and perplexity is 228.92921857688114
At time: 476.7865402698517 and batch: 700, loss is 5.520032205581665 and perplexity is 249.64307696073254
At time: 477.27726554870605 and batch: 750, loss is 5.419308385848999 and perplexity is 225.72295531443856
At time: 477.7901875972748 and batch: 800, loss is 5.535194473266602 and perplexity is 253.4570734695662
At time: 478.3026239871979 and batch: 850, loss is 5.497364845275879 and perplexity is 244.04797999253125
At time: 478.8064296245575 and batch: 900, loss is 5.5318418216705325 and perplexity is 252.608743080402
At time: 479.3049142360687 and batch: 950, loss is 5.491070070266724 and perplexity is 242.51657783975074
At time: 479.80177879333496 and batch: 1000, loss is 5.439499769210816 and perplexity is 230.3269380124121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604090992997333 and perplexity of 271.53498601272634
Finished 43 epochs...
Completing Train Step...
At time: 481.29676270484924 and batch: 50, loss is 5.542786779403687 and perplexity is 255.38872071373248
At time: 481.79531025886536 and batch: 100, loss is 5.559807510375976 and perplexity is 259.77282793607645
At time: 482.30472230911255 and batch: 150, loss is 5.4874726581573485 and perplexity is 241.64571313470552
At time: 482.81370282173157 and batch: 200, loss is 5.5060192966461186 and perplexity is 246.1692473297447
At time: 483.3120012283325 and batch: 250, loss is 5.586440706253052 and perplexity is 266.78436390025354
At time: 483.8090286254883 and batch: 300, loss is 5.527963800430298 and perplexity is 251.63101805381487
At time: 484.3084523677826 and batch: 350, loss is 5.501774005889892 and perplexity is 245.12640245566223
At time: 484.8074278831482 and batch: 400, loss is 5.517109718322754 and perplexity is 248.91456330349538
At time: 485.3153111934662 and batch: 450, loss is 5.554803838729859 and perplexity is 258.4762565164835
At time: 485.81333899497986 and batch: 500, loss is 5.558182754516602 and perplexity is 259.3511032044189
At time: 486.3229501247406 and batch: 550, loss is 5.508500299453735 and perplexity is 246.78075218247685
At time: 486.82565546035767 and batch: 600, loss is 5.453480110168457 and perplexity is 233.5696010905923
At time: 487.3314747810364 and batch: 650, loss is 5.433087978363037 and perplexity is 228.85485424910812
At time: 487.8370158672333 and batch: 700, loss is 5.519685888290406 and perplexity is 249.55663621536416
At time: 488.3527948856354 and batch: 750, loss is 5.419196338653564 and perplexity is 225.6976651072252
At time: 488.864803314209 and batch: 800, loss is 5.535154371261597 and perplexity is 253.44690953653588
At time: 489.37322640419006 and batch: 850, loss is 5.497120819091797 and perplexity is 243.9884331610293
At time: 489.88063955307007 and batch: 900, loss is 5.531617879867554 and perplexity is 252.55217975671113
At time: 490.3836419582367 and batch: 950, loss is 5.491061315536499 and perplexity is 242.51445468183067
At time: 490.8931291103363 and batch: 1000, loss is 5.439482622146606 and perplexity is 230.32298861547724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.604019909370236 and perplexity of 271.5156850070378
Finished 44 epochs...
Completing Train Step...
At time: 492.37333703041077 and batch: 50, loss is 5.542518396377563 and perplexity is 255.32018791296102
At time: 492.8841121196747 and batch: 100, loss is 5.559510698318482 and perplexity is 259.6957356700586
At time: 493.3811957836151 and batch: 150, loss is 5.487212409973145 and perplexity is 241.58283345915717
At time: 493.88648200035095 and batch: 200, loss is 5.5058830451965335 and perplexity is 246.13570869784823
At time: 494.38659405708313 and batch: 250, loss is 5.586427898406982 and perplexity is 266.7809469890685
At time: 494.9028465747833 and batch: 300, loss is 5.527792053222656 and perplexity is 251.58780484008878
At time: 495.4081175327301 and batch: 350, loss is 5.5016650390625 and perplexity is 245.09969326451088
At time: 495.90299129486084 and batch: 400, loss is 5.516969480514526 and perplexity is 248.87965851824433
At time: 496.4040460586548 and batch: 450, loss is 5.55465874671936 and perplexity is 258.438756397309
At time: 496.90040349960327 and batch: 500, loss is 5.558129901885986 and perplexity is 259.337396178591
At time: 497.3974723815918 and batch: 550, loss is 5.508498983383179 and perplexity is 246.78042740180868
At time: 497.8942070007324 and batch: 600, loss is 5.453426218032837 and perplexity is 233.5570138651527
At time: 498.3934817314148 and batch: 650, loss is 5.4328004741668705 and perplexity is 228.7890669757101
At time: 498.88967275619507 and batch: 700, loss is 5.519343919754029 and perplexity is 249.4713102879572
At time: 499.38803243637085 and batch: 750, loss is 5.419089498519898 and perplexity is 225.67355282661953
At time: 499.88740849494934 and batch: 800, loss is 5.535112895965576 and perplexity is 253.436397968924
At time: 500.39991188049316 and batch: 850, loss is 5.496898946762085 and perplexity is 243.9343048839465
At time: 500.9066812992096 and batch: 900, loss is 5.53140230178833 and perplexity is 252.4977409110172
At time: 501.4065430164337 and batch: 950, loss is 5.491046686172485 and perplexity is 242.51090687554552
At time: 501.9253263473511 and batch: 1000, loss is 5.439454917907715 and perplexity is 230.31660778076693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.603955896889291 and perplexity of 271.49830517069415
Finished 45 epochs...
Completing Train Step...
At time: 503.4470579624176 and batch: 50, loss is 5.542265548706054 and perplexity is 255.25563895882834
At time: 503.96493196487427 and batch: 100, loss is 5.5592371845245365 and perplexity is 259.62471501713094
At time: 504.4632098674774 and batch: 150, loss is 5.486960859298706 and perplexity is 241.5220707772349
At time: 504.9683210849762 and batch: 200, loss is 5.505732908248901 and perplexity is 246.09875740778736
At time: 505.4672689437866 and batch: 250, loss is 5.586428651809692 and perplexity is 266.7811479826327
At time: 505.96401143074036 and batch: 300, loss is 5.527619028091431 and perplexity is 251.54427759290405
At time: 506.4600467681885 and batch: 350, loss is 5.501556272506714 and perplexity is 245.07303606478249
At time: 506.9732460975647 and batch: 400, loss is 5.516839456558228 and perplexity is 248.8473003041186
At time: 507.47685265541077 and batch: 450, loss is 5.554517698287964 and perplexity is 258.402306586762
At time: 507.9883008003235 and batch: 500, loss is 5.5580699920654295 and perplexity is 259.3218597871181
At time: 508.49699568748474 and batch: 550, loss is 5.508503217697143 and perplexity is 246.7814723498309
At time: 509.001446723938 and batch: 600, loss is 5.453370418548584 and perplexity is 233.54398186782817
At time: 509.51022696495056 and batch: 650, loss is 5.4325182819366455 and perplexity is 228.72451358730984
At time: 510.01141119003296 and batch: 700, loss is 5.518984718322754 and perplexity is 249.3817159284142
At time: 510.51770067214966 and batch: 750, loss is 5.41898286819458 and perplexity is 225.6494904651775
At time: 511.0173726081848 and batch: 800, loss is 5.535054759979248 and perplexity is 253.42166462222963
At time: 511.51821541786194 and batch: 850, loss is 5.496664752960205 and perplexity is 243.8771836706551
At time: 512.0302700996399 and batch: 900, loss is 5.5312079811096195 and perplexity is 252.44868014554544
At time: 512.5414624214172 and batch: 950, loss is 5.4910229969024655 and perplexity is 242.50516203723566
At time: 513.0514059066772 and batch: 1000, loss is 5.439366474151611 and perplexity is 230.29623861565776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.603932822622904 and perplexity of 271.4920406187524
Finished 46 epochs...
Completing Train Step...
At time: 514.5633273124695 and batch: 50, loss is 5.541955404281616 and perplexity is 255.17648512079404
At time: 515.068443775177 and batch: 100, loss is 5.5588549041748045 and perplexity is 259.5254845584108
At time: 515.5730402469635 and batch: 150, loss is 5.486752996444702 and perplexity is 241.47187252765465
At time: 516.0748682022095 and batch: 200, loss is 5.504949836730957 and perplexity is 245.90611991457087
At time: 516.5848968029022 and batch: 250, loss is 5.586716804504395 and perplexity is 266.85803276606646
At time: 517.10222697258 and batch: 300, loss is 5.527121133804322 and perplexity is 251.4190663077139
At time: 517.6073410511017 and batch: 350, loss is 5.5013613033294675 and perplexity is 245.02525903425115
At time: 518.1184916496277 and batch: 400, loss is 5.516493663787842 and perplexity is 248.76126558269308
At time: 518.6206305027008 and batch: 450, loss is 5.554396238327026 and perplexity is 258.3709229586634
At time: 519.126745223999 and batch: 500, loss is 5.557804164886474 and perplexity is 259.25293415024913
At time: 519.6344695091248 and batch: 550, loss is 5.508313436508178 and perplexity is 246.73464231246447
At time: 520.1439301967621 and batch: 600, loss is 5.453230495452881 and perplexity is 233.51130595701306
At time: 520.6625039577484 and batch: 650, loss is 5.432495775222779 and perplexity is 228.71936580805823
At time: 521.1574869155884 and batch: 700, loss is 5.518540334701538 and perplexity is 249.2709193983299
At time: 521.6626133918762 and batch: 750, loss is 5.418884363174438 and perplexity is 225.6272639523038
At time: 522.1674611568451 and batch: 800, loss is 5.534788446426392 and perplexity is 253.35418398425634
At time: 522.6759650707245 and batch: 850, loss is 5.496039600372314 and perplexity is 243.7247708637499
At time: 523.1786437034607 and batch: 900, loss is 5.530843067169189 and perplexity is 252.35657490918155
At time: 523.6859300136566 and batch: 950, loss is 5.491030254364014 and perplexity is 242.50692201551087
At time: 524.1871728897095 and batch: 1000, loss is 5.438915395736695 and perplexity is 230.19238037924717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.603761626452934 and perplexity of 271.4455661994559
Finished 47 epochs...
Completing Train Step...
At time: 525.6797826290131 and batch: 50, loss is 5.541614074707031 and perplexity is 255.0894007027728
At time: 526.1933043003082 and batch: 100, loss is 5.558447694778442 and perplexity is 259.41982485677477
At time: 526.6885859966278 and batch: 150, loss is 5.4863478946685795 and perplexity is 241.37407165420134
At time: 527.1863870620728 and batch: 200, loss is 5.504650430679321 and perplexity is 245.83250515503633
At time: 527.685361623764 and batch: 250, loss is 5.586701803207397 and perplexity is 266.85402957948736
At time: 528.2031059265137 and batch: 300, loss is 5.526877336502075 and perplexity is 251.35777848882074
At time: 528.7135894298553 and batch: 350, loss is 5.501257390975952 and perplexity is 244.9997992057324
At time: 529.2196133136749 and batch: 400, loss is 5.516406707763672 and perplexity is 248.73963523252863
At time: 529.7251992225647 and batch: 450, loss is 5.55424916267395 and perplexity is 258.33292568073915
At time: 530.235554933548 and batch: 500, loss is 5.557730255126953 and perplexity is 259.23377353631776
At time: 530.7330615520477 and batch: 550, loss is 5.508207387924195 and perplexity is 246.70847784040464
At time: 531.2359662055969 and batch: 600, loss is 5.453112096786499 and perplexity is 233.48366016644724
At time: 531.7372848987579 and batch: 650, loss is 5.4321425247192385 and perplexity is 228.63858484571497
At time: 532.2425901889801 and batch: 700, loss is 5.518173103332519 and perplexity is 249.17939610348367
At time: 532.749085187912 and batch: 750, loss is 5.418574657440185 and perplexity is 225.55739671455453
At time: 533.2482526302338 and batch: 800, loss is 5.534907808303833 and perplexity is 253.38442662018736
At time: 533.7763538360596 and batch: 850, loss is 5.495839700698853 and perplexity is 243.67605523092158
At time: 534.2786250114441 and batch: 900, loss is 5.530594215393067 and perplexity is 252.29378334051995
At time: 534.7793033123016 and batch: 950, loss is 5.49099437713623 and perplexity is 242.4982216955033
At time: 535.2786045074463 and batch: 1000, loss is 5.438833360671997 and perplexity is 230.17349730697745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.6036291355040015 and perplexity of 271.4096045011589
Finished 48 epochs...
Completing Train Step...
At time: 536.7467744350433 and batch: 50, loss is 5.541383123397827 and perplexity is 255.03049427423664
At time: 537.2686583995819 and batch: 100, loss is 5.558200216293335 and perplexity is 259.3556319750186
At time: 537.7683579921722 and batch: 150, loss is 5.486084384918213 and perplexity is 241.31047561229133
At time: 538.2674312591553 and batch: 200, loss is 5.504444351196289 and perplexity is 245.78184933920292
At time: 538.7744753360748 and batch: 250, loss is 5.586566562652588 and perplexity is 266.81794253274506
At time: 539.3005776405334 and batch: 300, loss is 5.526743450164795 and perplexity is 251.32412736927486
At time: 539.8040406703949 and batch: 350, loss is 5.501208019256592 and perplexity is 244.98770344299953
At time: 540.3134291172028 and batch: 400, loss is 5.516274261474609 and perplexity is 248.70669277250082
At time: 540.8147659301758 and batch: 450, loss is 5.554092235565186 and perplexity is 258.2923894223155
At time: 541.3171315193176 and batch: 500, loss is 5.557687559127808 and perplexity is 259.222705527626
At time: 541.8214569091797 and batch: 550, loss is 5.508159666061402 and perplexity is 246.69670473319482
At time: 542.3232402801514 and batch: 600, loss is 5.453017587661743 and perplexity is 233.46159487278243
At time: 542.8170368671417 and batch: 650, loss is 5.431706142425537 and perplexity is 228.5388327822315
At time: 543.3292849063873 and batch: 700, loss is 5.517762241363525 and perplexity is 249.0770387949703
At time: 543.8286030292511 and batch: 750, loss is 5.418441867828369 and perplexity is 225.52744702395032
At time: 544.3383252620697 and batch: 800, loss is 5.534819135665893 and perplexity is 253.3619593507968
At time: 544.8441071510315 and batch: 850, loss is 5.495626440048218 and perplexity is 243.6240942576513
At time: 545.3480858802795 and batch: 900, loss is 5.530401678085327 and perplexity is 252.2452120507588
At time: 545.8514580726624 and batch: 950, loss is 5.490929470062256 and perplexity is 242.48248235629185
At time: 546.3770592212677 and batch: 1000, loss is 5.4387545394897465 and perplexity is 230.15535547478672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.603548003406059 and perplexity of 271.38758536378555
Finished 49 epochs...
Completing Train Step...
At time: 547.9070115089417 and batch: 50, loss is 5.541192417144775 and perplexity is 254.981863001551
At time: 548.4000670909882 and batch: 100, loss is 5.557982349395752 and perplexity is 259.299133122948
At time: 548.8952074050903 and batch: 150, loss is 5.4858760643005375 and perplexity is 241.2602109007299
At time: 549.3965389728546 and batch: 200, loss is 5.504283275604248 and perplexity is 245.74226307058402
At time: 549.9087855815887 and batch: 250, loss is 5.586468887329102 and perplexity is 266.79188227663894
At time: 550.4165637493134 and batch: 300, loss is 5.526620197296142 and perplexity is 251.29315285850288
At time: 550.9153070449829 and batch: 350, loss is 5.501111679077148 and perplexity is 244.96410242056993
At time: 551.4224750995636 and batch: 400, loss is 5.516161832809448 and perplexity is 248.6787325828087
At time: 551.9317164421082 and batch: 450, loss is 5.553964385986328 and perplexity is 258.25936895997677
At time: 552.4324779510498 and batch: 500, loss is 5.557646369934082 and perplexity is 259.21202857327893
At time: 552.9316079616547 and batch: 550, loss is 5.5081077003479 and perplexity is 246.68388529600344
At time: 553.4395697116852 and batch: 600, loss is 5.4529330444335935 and perplexity is 233.44185811021944
At time: 553.9420802593231 and batch: 650, loss is 5.431337432861328 and perplexity is 228.45458386143738
At time: 554.4405965805054 and batch: 700, loss is 5.517410039901733 and perplexity is 248.98932894448345
At time: 554.9403696060181 and batch: 750, loss is 5.418327293395996 and perplexity is 225.50160882494984
At time: 555.4401986598969 and batch: 800, loss is 5.534689950942993 and perplexity is 253.32923097033347
At time: 555.9388477802277 and batch: 850, loss is 5.495434856414795 and perplexity is 243.57742433922297
At time: 556.4391615390778 and batch: 900, loss is 5.5302223777771 and perplexity is 252.19998846091218
At time: 556.9553892612457 and batch: 950, loss is 5.490812587738037 and perplexity is 242.4541420964416
At time: 557.4506092071533 and batch: 1000, loss is 5.438671674728393 and perplexity is 230.1362844963481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.603477664110137 and perplexity of 271.3684968234542
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
SETTINGS FOR THIS RUN
{'lr': 11.339984217533289, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.0, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 2.0, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7250146865844727 and batch: 50, loss is 6.7596834278106686 and perplexity is 862.3691504822622
At time: 1.240187168121338 and batch: 100, loss is 5.832106885910034 and perplexity is 341.0765319285336
At time: 1.7425777912139893 and batch: 150, loss is 5.466834917068481 and perplexity is 236.70979971269543
At time: 2.2345306873321533 and batch: 200, loss is 5.339388608932495 and perplexity is 208.38526644354857
At time: 2.744771957397461 and batch: 250, loss is 5.350645599365234 and perplexity is 210.74431035015252
At time: 3.242575168609619 and batch: 300, loss is 5.235952968597412 and perplexity is 187.9080915574636
At time: 3.737795114517212 and batch: 350, loss is 5.19522274017334 and perplexity is 180.40832250305255
At time: 4.232028245925903 and batch: 400, loss is 5.181247673034668 and perplexity is 177.90463940124982
At time: 4.735950231552124 and batch: 450, loss is 5.187238359451294 and perplexity is 178.9736090419601
At time: 5.240048885345459 and batch: 500, loss is 5.194285316467285 and perplexity is 180.23928270810296
At time: 5.745335578918457 and batch: 550, loss is 5.110213670730591 and perplexity is 165.70575756161722
At time: 6.247089624404907 and batch: 600, loss is 5.020250968933105 and perplexity is 151.4493080961761
At time: 6.74968409538269 and batch: 650, loss is 4.996434555053711 and perplexity is 147.88494237789232
At time: 7.24847936630249 and batch: 700, loss is 5.051214532852173 and perplexity is 156.21207401538413
At time: 7.750824689865112 and batch: 750, loss is 4.995574197769165 and perplexity is 147.7577632080504
At time: 8.256197929382324 and batch: 800, loss is 5.087242641448975 and perplexity is 161.9427117389358
At time: 8.750365495681763 and batch: 850, loss is 4.988303098678589 and perplexity is 146.6872983147051
At time: 9.249164819717407 and batch: 900, loss is 5.012030277252197 and perplexity is 150.2093934896305
At time: 9.740769147872925 and batch: 950, loss is 4.980049934387207 and perplexity is 145.4816460090812
At time: 10.248979091644287 and batch: 1000, loss is 4.877795667648315 and perplexity is 131.3408257221357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.093726553568026 and perplexity of 162.9961455456598
Finished 1 epochs...
Completing Train Step...
At time: 11.72591519355774 and batch: 50, loss is 5.0517469501495365 and perplexity is 156.29526617015287
At time: 12.225597381591797 and batch: 100, loss is 4.928006868362427 and perplexity is 138.10397843481064
At time: 12.72959280014038 and batch: 150, loss is 4.95532790184021 and perplexity is 141.92913756909743
At time: 13.231052160263062 and batch: 200, loss is 4.967151355743408 and perplexity is 143.61718982374822
At time: 13.74247121810913 and batch: 250, loss is 4.999500951766968 and perplexity is 148.33911225577853
At time: 14.24746036529541 and batch: 300, loss is 4.905550651550293 and perplexity is 135.03724800772721
At time: 14.767077922821045 and batch: 350, loss is 4.892292165756226 and perplexity is 133.2586752185308
At time: 15.27279257774353 and batch: 400, loss is 4.902992954254151 and perplexity is 134.6923049216648
At time: 15.774897336959839 and batch: 450, loss is 4.927654619216919 and perplexity is 138.05533999334335
At time: 16.262558221817017 and batch: 500, loss is 4.9468483543396 and perplexity is 140.73073085938677
At time: 16.765499591827393 and batch: 550, loss is 4.853803844451904 and perplexity is 128.22721974482909
At time: 17.256345748901367 and batch: 600, loss is 4.7904836940765385 and perplexity is 120.35957179775828
At time: 17.76681637763977 and batch: 650, loss is 4.759744901657104 and perplexity is 116.71614800506042
At time: 18.266596794128418 and batch: 700, loss is 4.82851655960083 and perplexity is 125.02535535854526
At time: 18.763095140457153 and batch: 750, loss is 4.792846298217773 and perplexity is 120.64427000267416
At time: 19.260271072387695 and batch: 800, loss is 4.8917490673065185 and perplexity is 133.18632228777022
At time: 19.75952410697937 and batch: 850, loss is 4.794976425170899 and perplexity is 120.90153151651359
At time: 20.25626802444458 and batch: 900, loss is 4.803799571990967 and perplexity is 121.97298331728054
At time: 20.762652158737183 and batch: 950, loss is 4.784506292343139 and perplexity is 119.64228019040628
At time: 21.265068769454956 and batch: 1000, loss is 4.690576658248902 and perplexity is 108.91596899368224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.978044463366997 and perplexity of 145.19017914589918
Finished 2 epochs...
Completing Train Step...
At time: 22.72822666168213 and batch: 50, loss is 4.856705694198609 and perplexity is 128.59985627608884
At time: 23.260364532470703 and batch: 100, loss is 4.744094543457031 and perplexity is 114.90371806356202
At time: 23.75748610496521 and batch: 150, loss is 4.793028326034546 and perplexity is 120.66623261459138
At time: 24.24820852279663 and batch: 200, loss is 4.8107655811309815 and perplexity is 122.82561450438001
At time: 24.734888792037964 and batch: 250, loss is 4.851931619644165 and perplexity is 127.98737415548598
At time: 25.251096963882446 and batch: 300, loss is 4.744295949935913 and perplexity is 114.9268627474941
At time: 25.745734214782715 and batch: 350, loss is 4.740767955780029 and perplexity is 114.52211583954475
At time: 26.248461961746216 and batch: 400, loss is 4.739576644897461 and perplexity is 114.38576563052024
At time: 26.770064115524292 and batch: 450, loss is 4.78065092086792 and perplexity is 119.18190278932973
At time: 27.28430151939392 and batch: 500, loss is 4.821598129272461 and perplexity is 124.16336140901174
At time: 27.80619740486145 and batch: 550, loss is 4.724717578887939 and perplexity is 112.69866539671924
At time: 28.299045085906982 and batch: 600, loss is 4.66808331489563 and perplexity is 106.49343232048228
At time: 28.79078984260559 and batch: 650, loss is 4.633766040802002 and perplexity is 102.90086415391819
At time: 29.279749631881714 and batch: 700, loss is 4.707579221725464 and perplexity is 110.78365237937717
At time: 29.787635803222656 and batch: 750, loss is 4.670167026519775 and perplexity is 106.71556527343928
At time: 30.286508798599243 and batch: 800, loss is 4.774670934677124 and perplexity is 118.47132340145937
At time: 30.78513479232788 and batch: 850, loss is 4.678739223480225 and perplexity is 107.6342842117427
At time: 31.274397373199463 and batch: 900, loss is 4.681283025741577 and perplexity is 107.90843308975435
At time: 31.77219295501709 and batch: 950, loss is 4.669088487625122 and perplexity is 106.60053043155376
At time: 32.261993169784546 and batch: 1000, loss is 4.5750137424469 and perplexity is 97.02937254877112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.92662011123285 and perplexity of 137.91259449028882
Finished 3 epochs...
Completing Train Step...
At time: 33.74232792854309 and batch: 50, loss is 4.753928451538086 and perplexity is 116.03924484719498
At time: 34.251612424850464 and batch: 100, loss is 4.63889708518982 and perplexity is 103.43020994243818
At time: 34.744099855422974 and batch: 150, loss is 4.682619504928589 and perplexity is 108.05274687938909
At time: 35.23883652687073 and batch: 200, loss is 4.719757738113404 and perplexity is 112.14108186665935
At time: 35.72931480407715 and batch: 250, loss is 4.745559673309327 and perplexity is 115.07219031776421
At time: 36.23725509643555 and batch: 300, loss is 4.641799964904785 and perplexity is 103.73089161100646
At time: 36.736000299453735 and batch: 350, loss is 4.646086473464965 and perplexity is 104.1764893127437
At time: 37.22867178916931 and batch: 400, loss is 4.648298120498657 and perplexity is 104.40714590778059
At time: 37.71912598609924 and batch: 450, loss is 4.68693959236145 and perplexity is 108.52055394942829
At time: 38.22123098373413 and batch: 500, loss is 4.727759342193604 and perplexity is 113.04198995302653
At time: 38.712321281433105 and batch: 550, loss is 4.628507204055786 and perplexity is 102.36114569775509
At time: 39.22062039375305 and batch: 600, loss is 4.589576683044434 and perplexity is 98.45274462335006
At time: 39.708903074264526 and batch: 650, loss is 4.5603423690795895 and perplexity is 95.61621026070365
At time: 40.21643352508545 and batch: 700, loss is 4.641310796737671 and perplexity is 103.68016216950953
At time: 40.720218896865845 and batch: 750, loss is 4.596427173614502 and perplexity is 99.12950966157663
At time: 41.218785524368286 and batch: 800, loss is 4.709221506118775 and perplexity is 110.96574012171685
At time: 41.71434736251831 and batch: 850, loss is 4.601773824691772 and perplexity is 99.66093998145915
At time: 42.21589779853821 and batch: 900, loss is 4.593329515457153 and perplexity is 98.82291543455368
At time: 42.71578359603882 and batch: 950, loss is 4.5960245609283445 and perplexity is 99.08960689663304
At time: 43.22000741958618 and batch: 1000, loss is 4.512267093658448 and perplexity is 91.12818062141893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.928966987423781 and perplexity of 138.23663837143081
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 44.71877646446228 and batch: 50, loss is 4.625641641616821 and perplexity is 102.06824330890538
At time: 45.22152018547058 and batch: 100, loss is 4.460863075256348 and perplexity is 86.56218654690788
At time: 45.71521019935608 and batch: 150, loss is 4.4972686672210695 and perplexity is 89.7716000255874
At time: 46.212106466293335 and batch: 200, loss is 4.530283708572387 and perplexity is 92.78488121450009
At time: 46.74069905281067 and batch: 250, loss is 4.543352222442627 and perplexity is 94.00539953130006
At time: 47.25607967376709 and batch: 300, loss is 4.413623189926147 and perplexity is 82.5680820447673
At time: 47.768335580825806 and batch: 350, loss is 4.426592369079589 and perplexity is 83.64589636941068
At time: 48.26924157142639 and batch: 400, loss is 4.420184078216553 and perplexity is 83.1115829822912
At time: 48.78487014770508 and batch: 450, loss is 4.4509280395507815 and perplexity is 85.7064460776764
At time: 49.30523610115051 and batch: 500, loss is 4.49826229095459 and perplexity is 89.86084354786739
At time: 49.8165385723114 and batch: 550, loss is 4.3933949422836305 and perplexity is 80.91465379092622
At time: 50.33258605003357 and batch: 600, loss is 4.338985905647278 and perplexity is 76.6297900850438
At time: 50.852356910705566 and batch: 650, loss is 4.316085648536682 and perplexity is 74.8948888496113
At time: 51.36561918258667 and batch: 700, loss is 4.394881963729858 and perplexity is 81.03506512135468
At time: 51.886314392089844 and batch: 750, loss is 4.332304248809814 and perplexity is 76.1194828715442
At time: 52.392823696136475 and batch: 800, loss is 4.448657464981079 and perplexity is 85.5120639638067
At time: 52.91355085372925 and batch: 850, loss is 4.346930437088012 and perplexity is 77.24100254522571
At time: 53.42022681236267 and batch: 900, loss is 4.328440370559693 and perplexity is 75.82593394114292
At time: 53.93032145500183 and batch: 950, loss is 4.307288870811463 and perplexity is 74.23894448794383
At time: 54.437220335006714 and batch: 1000, loss is 4.234010052680969 and perplexity is 68.99334513871821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.740750940834603 and perplexity of 114.52016726857114
Finished 5 epochs...
Completing Train Step...
At time: 55.95337152481079 and batch: 50, loss is 4.458012447357178 and perplexity is 86.31578133456955
At time: 56.46721076965332 and batch: 100, loss is 4.34378044128418 and perplexity is 76.99807652017309
At time: 56.99146270751953 and batch: 150, loss is 4.3923317527771 and perplexity is 80.8286718957148
At time: 57.496575117111206 and batch: 200, loss is 4.440784597396851 and perplexity is 84.84148197264261
At time: 58.00786828994751 and batch: 250, loss is 4.446099414825439 and perplexity is 85.29359935611593
At time: 58.54013133049011 and batch: 300, loss is 4.322412919998169 and perplexity is 75.37027149399111
At time: 59.057244062423706 and batch: 350, loss is 4.343067693710327 and perplexity is 76.94321588115817
At time: 59.56792950630188 and batch: 400, loss is 4.338370771408081 and perplexity is 76.58266697242591
At time: 60.076329946517944 and batch: 450, loss is 4.375808382034302 and perplexity is 79.5040832545921
At time: 60.57600688934326 and batch: 500, loss is 4.43027813911438 and perplexity is 83.9547647670625
At time: 61.07411336898804 and batch: 550, loss is 4.322613868713379 and perplexity is 75.38541857505511
At time: 61.582183837890625 and batch: 600, loss is 4.281221103668213 and perplexity is 72.32870695419007
At time: 62.09167504310608 and batch: 650, loss is 4.257410912513733 and perplexity is 70.62688734042105
At time: 62.59968900680542 and batch: 700, loss is 4.338082523345947 and perplexity is 76.56059534828233
At time: 63.11040759086609 and batch: 750, loss is 4.271764545440674 and perplexity is 71.64795020640048
At time: 63.62693810462952 and batch: 800, loss is 4.398762264251709 and perplexity is 81.35011637830821
At time: 64.13210415840149 and batch: 850, loss is 4.293347668647766 and perplexity is 73.21114537862012
At time: 64.64229655265808 and batch: 900, loss is 4.271048250198365 and perplexity is 71.59664749668339
At time: 65.18548560142517 and batch: 950, loss is 4.2593107461929325 and perplexity is 70.76119421961671
At time: 65.70171451568604 and batch: 1000, loss is 4.188711800575256 and perplexity is 65.93779519657569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.729012838224086 and perplexity of 113.18377648457245
Finished 6 epochs...
Completing Train Step...
At time: 67.200519323349 and batch: 50, loss is 4.389518918991089 and perplexity is 80.60163373652507
At time: 67.71327018737793 and batch: 100, loss is 4.278718509674072 and perplexity is 72.14792387424096
At time: 68.23346257209778 and batch: 150, loss is 4.328967294692993 and perplexity is 75.86589898401465
At time: 68.74340271949768 and batch: 200, loss is 4.382578430175781 and perplexity is 80.04415582152342
At time: 69.24361228942871 and batch: 250, loss is 4.390015845298767 and perplexity is 80.64169676213183
At time: 69.74516177177429 and batch: 300, loss is 4.2601817607879635 and perplexity is 70.82285510240992
At time: 70.26832032203674 and batch: 350, loss is 4.283383159637451 and perplexity is 72.48525483846977
At time: 70.78236174583435 and batch: 400, loss is 4.280306015014649 and perplexity is 72.26255004946232
At time: 71.28812050819397 and batch: 450, loss is 4.319753141403198 and perplexity is 75.17006962352512
At time: 71.81613492965698 and batch: 500, loss is 4.380672512054443 and perplexity is 79.8917435032724
At time: 72.3354332447052 and batch: 550, loss is 4.276566419601441 and perplexity is 71.99282199997488
At time: 72.84131693840027 and batch: 600, loss is 4.231092562675476 and perplexity is 68.79235108546877
At time: 73.34181880950928 and batch: 650, loss is 4.210702743530273 and perplexity is 67.40389081854846
At time: 73.85257649421692 and batch: 700, loss is 4.297859358787536 and perplexity is 73.54219762236127
At time: 74.3688497543335 and batch: 750, loss is 4.225344591140747 and perplexity is 68.39806885669894
At time: 74.87777709960938 and batch: 800, loss is 4.357212963104248 and perplexity is 78.03933255399656
At time: 75.38428688049316 and batch: 850, loss is 4.253339056968689 and perplexity is 70.33988956086014
At time: 75.89121222496033 and batch: 900, loss is 4.227329759597779 and perplexity is 68.5339854095058
At time: 76.39670944213867 and batch: 950, loss is 4.218406791687012 and perplexity is 67.9251790759808
At time: 76.91331052780151 and batch: 1000, loss is 4.1497289133071895 and perplexity is 63.41680651537891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.722173551233803 and perplexity of 112.41232126349925
Finished 7 epochs...
Completing Train Step...
At time: 78.42321634292603 and batch: 50, loss is 4.342253198623657 and perplexity is 76.88057152509371
At time: 78.91732001304626 and batch: 100, loss is 4.231376886367798 and perplexity is 68.81191316158194
At time: 79.4273087978363 and batch: 150, loss is 4.283165626525879 and perplexity is 72.46948861034211
At time: 79.9247145652771 and batch: 200, loss is 4.33907753944397 and perplexity is 76.63681228537952
At time: 80.42700362205505 and batch: 250, loss is 4.351733312606812 and perplexity is 77.61287377616512
At time: 80.93613243103027 and batch: 300, loss is 4.214799280166626 and perplexity is 67.6805796727932
At time: 81.43839192390442 and batch: 350, loss is 4.239145359992981 and perplexity is 69.34855845263154
At time: 81.94738864898682 and batch: 400, loss is 4.235121583938598 and perplexity is 69.07007603490294
At time: 82.46754097938538 and batch: 450, loss is 4.279966611862182 and perplexity is 72.23802807382359
At time: 82.98808407783508 and batch: 500, loss is 4.337923231124878 and perplexity is 76.54840081227611
At time: 83.50962209701538 and batch: 550, loss is 4.229204478263855 and perplexity is 68.66258786025591
At time: 84.03446364402771 and batch: 600, loss is 4.186154408454895 and perplexity is 65.76938183999036
At time: 84.55004739761353 and batch: 650, loss is 4.1673170900344845 and perplexity is 64.54205907897021
At time: 85.06004691123962 and batch: 700, loss is 4.255590505599976 and perplexity is 70.49843461999266
At time: 85.56096196174622 and batch: 750, loss is 4.1862239122390745 and perplexity is 65.77395321977365
At time: 86.06416392326355 and batch: 800, loss is 4.317159161567688 and perplexity is 74.97533265976084
At time: 86.57851529121399 and batch: 850, loss is 4.210916633605957 and perplexity is 67.41830938379607
At time: 87.08921933174133 and batch: 900, loss is 4.189261236190796 and perplexity is 65.97403372412406
At time: 87.59751152992249 and batch: 950, loss is 4.172247920036316 and perplexity is 64.86109089970635
At time: 88.10671043395996 and batch: 1000, loss is 4.109711866378785 and perplexity is 60.92915930087294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.705238156202363 and perplexity of 110.52460393327307
Finished 8 epochs...
Completing Train Step...
At time: 89.69488525390625 and batch: 50, loss is 4.2971405410766605 and perplexity is 73.48935318324834
At time: 90.21495532989502 and batch: 100, loss is 4.188818135261536 and perplexity is 65.94480704413657
At time: 90.72241258621216 and batch: 150, loss is 4.238691391944886 and perplexity is 69.31708356774915
At time: 91.24811816215515 and batch: 200, loss is 4.295578174591064 and perplexity is 73.37462552744628
At time: 91.75220966339111 and batch: 250, loss is 4.309472303390503 and perplexity is 74.40121730938692
At time: 92.25560069084167 and batch: 300, loss is 4.173717260360718 and perplexity is 64.95646396663763
At time: 92.76218366622925 and batch: 350, loss is 4.198880009651184 and perplexity is 66.6116848030972
At time: 93.26842951774597 and batch: 400, loss is 4.191552290916443 and perplexity is 66.12535712476324
At time: 93.78071331977844 and batch: 450, loss is 4.242909188270569 and perplexity is 69.61006634462609
At time: 94.28582310676575 and batch: 500, loss is 4.303531894683838 and perplexity is 73.96055382675058
At time: 94.78561019897461 and batch: 550, loss is 4.191586871147155 and perplexity is 66.12764379440513
At time: 95.28864192962646 and batch: 600, loss is 4.151370186805725 and perplexity is 63.52097630144475
At time: 95.78951716423035 and batch: 650, loss is 4.133866868019104 and perplexity is 62.418822219146996
At time: 96.30046105384827 and batch: 700, loss is 4.221911525726318 and perplexity is 68.1636564189405
At time: 96.80303931236267 and batch: 750, loss is 4.15451397895813 and perplexity is 63.72098727999391
At time: 97.31358575820923 and batch: 800, loss is 4.28464029788971 and perplexity is 72.57643612678608
At time: 97.81146001815796 and batch: 850, loss is 4.173795804977417 and perplexity is 64.96156614757385
At time: 98.32116866111755 and batch: 900, loss is 4.156142206192016 and perplexity is 63.82482403883228
At time: 98.8293616771698 and batch: 950, loss is 4.1362490606307984 and perplexity is 62.56769312545311
At time: 99.33239436149597 and batch: 1000, loss is 4.075802564620972 and perplexity is 58.89773086015683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.707491246665397 and perplexity of 110.77390660960327
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 100.84095764160156 and batch: 50, loss is 4.2429690933227535 and perplexity is 69.6142364641874
At time: 101.36963486671448 and batch: 100, loss is 4.114321150779724 and perplexity is 61.210647355287186
At time: 101.87900900840759 and batch: 150, loss is 4.156506104469299 and perplexity is 63.84805400876457
At time: 102.37847876548767 and batch: 200, loss is 4.20681140422821 and perplexity is 67.14210908021396
At time: 102.87928891181946 and batch: 250, loss is 4.217548680305481 and perplexity is 67.86691670809883
At time: 103.38431859016418 and batch: 300, loss is 4.072186260223389 and perplexity is 58.68512339539507
At time: 103.89482927322388 and batch: 350, loss is 4.099154033660889 and perplexity is 60.28926333061138
At time: 104.4367504119873 and batch: 400, loss is 4.083254752159118 and perplexity is 59.33828731188742
At time: 104.95628094673157 and batch: 450, loss is 4.138945484161377 and perplexity is 62.73662978548597
At time: 105.46824645996094 and batch: 500, loss is 4.195018496513367 and perplexity is 66.35495890132846
At time: 105.97548007965088 and batch: 550, loss is 4.080188760757446 and perplexity is 59.15663524733192
At time: 106.49094104766846 and batch: 600, loss is 4.034958486557007 and perplexity is 56.54057299574839
At time: 106.9911859035492 and batch: 650, loss is 4.008512392044067 and perplexity is 55.06489462921237
At time: 107.4994387626648 and batch: 700, loss is 4.094308266639709 and perplexity is 59.99782230457711
At time: 108.00348138809204 and batch: 750, loss is 4.034266328811645 and perplexity is 56.50145154089306
At time: 108.51299381256104 and batch: 800, loss is 4.152375755310058 and perplexity is 63.584883120535196
At time: 109.02649426460266 and batch: 850, loss is 4.039163112640381 and perplexity is 56.77880545196139
At time: 109.5315260887146 and batch: 900, loss is 4.012060804367065 and perplexity is 55.26063465765986
At time: 110.04314398765564 and batch: 950, loss is 3.99248929977417 and perplexity is 54.18961580489115
At time: 110.54943370819092 and batch: 1000, loss is 3.936502232551575 and perplexity is 51.23906516152849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643702809403583 and perplexity of 103.92846328193315
Finished 10 epochs...
Completing Train Step...
At time: 112.06261920928955 and batch: 50, loss is 4.172956304550171 and perplexity is 64.9070537698221
At time: 112.56112909317017 and batch: 100, loss is 4.061173782348633 and perplexity is 58.04240025509691
At time: 113.06999230384827 and batch: 150, loss is 4.108692994117737 and perplexity is 60.86711188513534
At time: 113.58190155029297 and batch: 200, loss is 4.161500964164734 and perplexity is 64.16776386861837
At time: 114.1014678478241 and batch: 250, loss is 4.173541584014893 and perplexity is 64.94505365470054
At time: 114.61562514305115 and batch: 300, loss is 4.030894184112549 and perplexity is 56.311241358962235
At time: 115.11839199066162 and batch: 350, loss is 4.059574227333069 and perplexity is 57.9496324560321
At time: 115.61805963516235 and batch: 400, loss is 4.047156343460083 and perplexity is 57.23447024042243
At time: 116.11505126953125 and batch: 450, loss is 4.106572017669678 and perplexity is 60.73815098422702
At time: 116.62911200523376 and batch: 500, loss is 4.164295372962951 and perplexity is 64.34732560013035
At time: 117.16297554969788 and batch: 550, loss is 4.051225080490112 and perplexity is 57.46781663953785
At time: 117.67699909210205 and batch: 600, loss is 4.007177605628967 and perplexity is 54.991443787410624
At time: 118.17640256881714 and batch: 650, loss is 3.9827334833145143 and perplexity is 53.66352226768876
At time: 118.68640208244324 and batch: 700, loss is 4.07241304397583 and perplexity is 58.69843373711873
At time: 119.18781614303589 and batch: 750, loss is 4.0137551975250245 and perplexity is 55.354347269511955
At time: 119.6883807182312 and batch: 800, loss is 4.134056463241577 and perplexity is 62.4306576515674
At time: 120.1972827911377 and batch: 850, loss is 4.021185474395752 and perplexity is 55.767177216801144
At time: 120.71035146713257 and batch: 900, loss is 3.993228440284729 and perplexity is 54.229684351502954
At time: 121.21354150772095 and batch: 950, loss is 3.9794942569732665 and perplexity is 53.489975203652214
At time: 121.71719908714294 and batch: 1000, loss is 3.9247634506225584 and perplexity is 50.64109752180846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641111420422066 and perplexity of 103.6594928613285
Finished 11 epochs...
Completing Train Step...
At time: 123.22703146934509 and batch: 50, loss is 4.142921094894409 and perplexity is 62.98654265318897
At time: 123.75045585632324 and batch: 100, loss is 4.033654284477234 and perplexity is 56.466880728105686
At time: 124.26516795158386 and batch: 150, loss is 4.080841617584229 and perplexity is 59.19526867017734
At time: 124.7740969657898 and batch: 200, loss is 4.134925980567932 and perplexity is 62.484965797606456
At time: 125.29714274406433 and batch: 250, loss is 4.149286231994629 and perplexity is 63.38873929310803
At time: 125.80402255058289 and batch: 300, loss is 4.006609735488891 and perplexity is 54.96022465357
At time: 126.31018471717834 and batch: 350, loss is 4.035414185523987 and perplexity is 56.566344347998324
At time: 126.82138895988464 and batch: 400, loss is 4.023387250900268 and perplexity is 55.89009935119529
At time: 127.32530999183655 and batch: 450, loss is 4.085108695030212 and perplexity is 59.44839914561798
At time: 127.82970690727234 and batch: 500, loss is 4.143292722702026 and perplexity is 63.00995455393176
At time: 128.33528923988342 and batch: 550, loss is 4.029856090545654 and perplexity is 56.252815352641115
At time: 128.84557271003723 and batch: 600, loss is 3.9855697345733643 and perplexity is 53.81594154778655
At time: 129.3583219051361 and batch: 650, loss is 3.960627303123474 and perplexity is 52.49024291689494
At time: 129.86919975280762 and batch: 700, loss is 4.052713913917541 and perplexity is 57.55344036987182
At time: 130.38585662841797 and batch: 750, loss is 3.996325693130493 and perplexity is 54.39790777644611
At time: 130.9194278717041 and batch: 800, loss is 4.116137499809265 and perplexity is 61.321928287120684
At time: 131.43679118156433 and batch: 850, loss is 4.002862610816956 and perplexity is 54.75466720500731
At time: 131.95488572120667 and batch: 900, loss is 3.973094973564148 and perplexity is 53.1487705896735
At time: 132.4645438194275 and batch: 950, loss is 3.96263566493988 and perplexity is 52.595768247555654
At time: 132.9848861694336 and batch: 1000, loss is 3.909836840629578 and perplexity is 49.89081115596254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641009447051258 and perplexity of 103.64892289236205
Finished 12 epochs...
Completing Train Step...
At time: 134.50524306297302 and batch: 50, loss is 4.119363007545471 and perplexity is 61.52004197806834
At time: 135.0389051437378 and batch: 100, loss is 4.010774726867676 and perplexity is 55.18961087965548
At time: 135.54931116104126 and batch: 150, loss is 4.058216724395752 and perplexity is 57.871019030806906
At time: 136.05468153953552 and batch: 200, loss is 4.112149848937988 and perplexity is 61.077884749955196
At time: 136.5594882965088 and batch: 250, loss is 4.126068859100342 and perplexity is 61.93397257463992
At time: 137.06287264823914 and batch: 300, loss is 3.985886325836182 and perplexity is 53.832981901952174
At time: 137.56203198432922 and batch: 350, loss is 4.013527307510376 and perplexity is 55.341734003775095
At time: 138.06405377388 and batch: 400, loss is 4.003101034164429 and perplexity is 54.76772355245928
At time: 138.5742769241333 and batch: 450, loss is 4.064142346382141 and perplexity is 58.214958835789965
At time: 139.0821874141693 and batch: 500, loss is 4.123668427467346 and perplexity is 61.7854825990328
At time: 139.57911944389343 and batch: 550, loss is 4.011250424385071 and perplexity is 55.215870685904044
At time: 140.09503746032715 and batch: 600, loss is 3.965835371017456 and perplexity is 52.76432877513252
At time: 140.59824013710022 and batch: 650, loss is 3.9425165700912475 and perplexity is 51.5481627715777
At time: 141.1133213043213 and batch: 700, loss is 4.037125582695007 and perplexity is 56.66323471499211
At time: 141.6283950805664 and batch: 750, loss is 3.979984345436096 and perplexity is 53.516196448216455
At time: 142.14082551002502 and batch: 800, loss is 4.1000710344314575 and perplexity is 60.34457398762427
At time: 142.6517949104309 and batch: 850, loss is 3.986144165992737 and perplexity is 53.84686399603734
At time: 143.16955018043518 and batch: 900, loss is 3.95584979057312 and perplexity is 52.24006820484682
At time: 143.67326426506042 and batch: 950, loss is 3.94728919506073 and perplexity is 51.79477083615363
At time: 144.18918132781982 and batch: 1000, loss is 3.894561376571655 and perplexity is 49.1344970939219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.641083508003049 and perplexity of 103.65659951450901
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 145.70141553878784 and batch: 50, loss is 4.094799757003784 and perplexity is 60.027317903912966
At time: 146.20054030418396 and batch: 100, loss is 3.9760947322845457 and perplexity is 53.308443448017535
At time: 146.7255823612213 and batch: 150, loss is 4.020419998168945 and perplexity is 55.72450510273365
At time: 147.2258801460266 and batch: 200, loss is 4.072659292221069 and perplexity is 58.712889903254506
At time: 147.7257583141327 and batch: 250, loss is 4.087117223739624 and perplexity is 59.56792295535041
At time: 148.23229598999023 and batch: 300, loss is 3.939333186149597 and perplexity is 51.38432609387265
At time: 148.7445433139801 and batch: 350, loss is 3.965825753211975 and perplexity is 52.76382130052243
At time: 149.25896000862122 and batch: 400, loss is 3.9549113941192626 and perplexity is 52.191069303887225
At time: 149.76708984375 and batch: 450, loss is 4.013457846641541 and perplexity is 55.33789005235199
At time: 150.26815342903137 and batch: 500, loss is 4.072014517784119 and perplexity is 58.67504553457726
At time: 150.77827525138855 and batch: 550, loss is 3.9598946380615234 and perplexity is 52.451799234707536
At time: 151.28653001785278 and batch: 600, loss is 3.908960099220276 and perplexity is 49.847088985197935
At time: 151.7871241569519 and batch: 650, loss is 3.8793859672546387 and perplexity is 48.39449014459609
At time: 152.2957079410553 and batch: 700, loss is 3.9736142206192016 and perplexity is 53.17637509844162
At time: 152.801527261734 and batch: 750, loss is 3.91336633682251 and perplexity is 50.067211703406635
At time: 153.30824041366577 and batch: 800, loss is 4.034006218910218 and perplexity is 56.48675686510044
At time: 153.80822730064392 and batch: 850, loss is 3.9169133377075194 and perplexity is 50.24511547352318
At time: 154.32040858268738 and batch: 900, loss is 3.8809878063201904 and perplexity is 48.47207245006204
At time: 154.8244445323944 and batch: 950, loss is 3.8706660318374633 and perplexity is 47.97432787225728
At time: 155.33124113082886 and batch: 1000, loss is 3.8180967140197755 and perplexity is 45.517493018699014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.604577413419398 and perplexity of 99.94074030862564
Finished 14 epochs...
Completing Train Step...
At time: 156.8319525718689 and batch: 50, loss is 4.059213161468506 and perplexity is 57.928712598838665
At time: 157.35541129112244 and batch: 100, loss is 3.9452732467651366 and perplexity is 51.69046043368175
At time: 157.86670231819153 and batch: 150, loss is 3.994682397842407 and perplexity is 54.30858935928214
At time: 158.3730432987213 and batch: 200, loss is 4.04873863697052 and perplexity is 57.3251036566808
At time: 158.88853549957275 and batch: 250, loss is 4.063495388031006 and perplexity is 58.17730836246747
At time: 159.40083837509155 and batch: 300, loss is 3.917146339416504 and perplexity is 50.256824035301115
At time: 159.91431736946106 and batch: 350, loss is 3.9469501829147338 and perplexity is 51.777214755771475
At time: 160.41384840011597 and batch: 400, loss is 3.938654980659485 and perplexity is 51.34948877657532
At time: 160.93108415603638 and batch: 450, loss is 3.9964704847335817 and perplexity is 54.40578470696067
At time: 161.4335277080536 and batch: 500, loss is 4.055734891891479 and perplexity is 57.72757093542436
At time: 161.93558192253113 and batch: 550, loss is 3.94522310256958 and perplexity is 51.68786852211055
At time: 162.4443826675415 and batch: 600, loss is 3.8954472970962524 and perplexity is 49.178045640790636
At time: 162.95413637161255 and batch: 650, loss is 3.867557101249695 and perplexity is 47.825410623548365
At time: 163.46840143203735 and batch: 700, loss is 3.963016986846924 and perplexity is 52.615827990572896
At time: 163.9878695011139 and batch: 750, loss is 3.9053057718276976 and perplexity is 49.665263829145935
At time: 164.50472259521484 and batch: 800, loss is 4.027342000007629 and perplexity is 56.111568310150076
At time: 165.01126146316528 and batch: 850, loss is 3.9099154758453367 and perplexity is 49.89473448491606
At time: 165.5319321155548 and batch: 900, loss is 3.8766053819656374 and perplexity is 48.26011204869003
At time: 166.03530764579773 and batch: 950, loss is 3.8657107734680176 and perplexity is 47.73719070574781
At time: 166.5521159172058 and batch: 1000, loss is 3.8129085302352905 and perplexity is 45.28195144434139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.603312422589558 and perplexity of 99.81439611757733
Finished 15 epochs...
Completing Train Step...
At time: 168.12705612182617 and batch: 50, loss is 4.0422350120544435 and perplexity is 56.953492404163114
At time: 168.63826775550842 and batch: 100, loss is 3.929273009300232 and perplexity is 50.869982219186056
At time: 169.13538885116577 and batch: 150, loss is 3.9801772737503054 and perplexity is 53.52652223381627
At time: 169.6459572315216 and batch: 200, loss is 4.033801522254944 and perplexity is 56.47519539824004
At time: 170.14668011665344 and batch: 250, loss is 4.050432624816895 and perplexity is 57.42229398194089
At time: 170.6482424736023 and batch: 300, loss is 3.9029786682128904 and perplexity is 49.549821988817826
At time: 171.1581666469574 and batch: 350, loss is 3.9341719341278076 and perplexity is 51.11980186225595
At time: 171.65415477752686 and batch: 400, loss is 3.927097215652466 and perplexity is 50.75941995896085
At time: 172.16192078590393 and batch: 450, loss is 3.984466609954834 and perplexity is 53.756608589635995
At time: 172.66299152374268 and batch: 500, loss is 4.0437870216369625 and perplexity is 57.041953398531184
At time: 173.16313815116882 and batch: 550, loss is 3.933783588409424 and perplexity is 51.0999535603291
At time: 173.68356943130493 and batch: 600, loss is 3.8845258951187134 and perplexity is 48.643874693215466
At time: 174.1914587020874 and batch: 650, loss is 3.857245397567749 and perplexity is 47.33478311090656
At time: 174.70005631446838 and batch: 700, loss is 3.953704209327698 and perplexity is 52.12810305236539
At time: 175.2026834487915 and batch: 750, loss is 3.89730833530426 and perplexity is 49.26965307875448
At time: 175.71726751327515 and batch: 800, loss is 4.020036239624023 and perplexity is 55.70312445050531
At time: 176.22608160972595 and batch: 850, loss is 3.902093195915222 and perplexity is 49.50596641340635
At time: 176.74703526496887 and batch: 900, loss is 3.869456090927124 and perplexity is 47.916316872336765
At time: 177.25924015045166 and batch: 950, loss is 3.858595404624939 and perplexity is 47.398728555845985
At time: 177.77413415908813 and batch: 1000, loss is 3.80499547958374 and perplexity is 44.925047032641345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.602315763147866 and perplexity of 99.71496471512366
Finished 16 epochs...
Completing Train Step...
At time: 179.2813811302185 and batch: 50, loss is 4.027496733665466 and perplexity is 56.12025133012207
At time: 179.78811025619507 and batch: 100, loss is 3.915891032218933 and perplexity is 50.193775863054015
At time: 180.3073570728302 and batch: 150, loss is 3.9680832386016847 and perplexity is 52.883069405998455
At time: 180.8062241077423 and batch: 200, loss is 4.0213129472732545 and perplexity is 55.77428647246017
At time: 181.30892395973206 and batch: 250, loss is 4.038754386901855 and perplexity is 56.75560323476513
At time: 181.8092679977417 and batch: 300, loss is 3.891009645462036 and perplexity is 48.960294116233406
At time: 182.33535242080688 and batch: 350, loss is 3.9233066701889037 and perplexity is 50.567378271218125
At time: 182.8351538181305 and batch: 400, loss is 3.9165252351760866 and perplexity is 50.22561900057574
At time: 183.34625625610352 and batch: 450, loss is 3.9741902637481687 and perplexity is 53.20701580827836
At time: 183.8480942249298 and batch: 500, loss is 4.03279405593872 and perplexity is 56.41832719238516
At time: 184.36176204681396 and batch: 550, loss is 3.923518304824829 and perplexity is 50.578081212424834
At time: 184.87230491638184 and batch: 600, loss is 3.8745202350616457 and perplexity is 48.15958746614849
At time: 185.3898024559021 and batch: 650, loss is 3.8476221323013307 and perplexity is 46.88145269459427
At time: 185.9003517627716 and batch: 700, loss is 3.945187039375305 and perplexity is 51.68600452607741
At time: 186.4066867828369 and batch: 750, loss is 3.8890959453582763 and perplexity is 48.866688391512895
At time: 186.9092779159546 and batch: 800, loss is 4.012575702667236 and perplexity is 55.28909559112579
At time: 187.41055154800415 and batch: 850, loss is 3.8939732694625855 and perplexity is 49.10560924228848
At time: 187.91456532478333 and batch: 900, loss is 3.86167275428772 and perplexity is 47.54481568248044
At time: 188.4250476360321 and batch: 950, loss is 3.850947632789612 and perplexity is 47.03761650591956
At time: 188.94059038162231 and batch: 1000, loss is 3.7969254446029663 and perplexity is 44.56395928661096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.601396886313834 and perplexity of 99.62338102754904
Finished 17 epochs...
Completing Train Step...
At time: 190.45681619644165 and batch: 50, loss is 4.014971103668213 and perplexity is 55.42169389570012
At time: 190.9632866382599 and batch: 100, loss is 3.903829584121704 and perplexity is 49.59200266418574
At time: 191.45811104774475 and batch: 150, loss is 3.957145581245422 and perplexity is 52.30780427435279
At time: 191.94644141197205 and batch: 200, loss is 4.0103789329528805 and perplexity is 55.16777148974331
At time: 192.44565510749817 and batch: 250, loss is 4.028234086036682 and perplexity is 56.1616469902321
At time: 192.94514918327332 and batch: 300, loss is 3.880216760635376 and perplexity is 48.434712672663885
At time: 193.45293259620667 and batch: 350, loss is 3.913103222846985 and perplexity is 50.05404005319052
At time: 193.96398305892944 and batch: 400, loss is 3.9064921617507933 and perplexity is 49.72422116395938
At time: 194.4668037891388 and batch: 450, loss is 3.9644932556152344 and perplexity is 52.693560457043134
At time: 194.9674792289734 and batch: 500, loss is 4.022275905609131 and perplexity is 55.8280206542833
At time: 195.48621940612793 and batch: 550, loss is 3.9139077425003053 and perplexity is 50.09432571527193
At time: 195.98480677604675 and batch: 600, loss is 3.864508242607117 and perplexity is 47.67981976278974
At time: 196.49495148658752 and batch: 650, loss is 3.8385955190658567 and perplexity is 46.46017616381214
At time: 197.0033986568451 and batch: 700, loss is 3.936908149719238 and perplexity is 51.259868199602074
At time: 197.5107717514038 and batch: 750, loss is 3.8811008071899415 and perplexity is 48.47755014589391
At time: 198.01617693901062 and batch: 800, loss is 4.0051936912536625 and perplexity is 54.88245362088911
At time: 198.52899599075317 and batch: 850, loss is 3.8858172845840455 and perplexity is 48.70673345938571
At time: 199.03679633140564 and batch: 900, loss is 3.85333881855011 and perplexity is 47.150226767086146
At time: 199.5399317741394 and batch: 950, loss is 3.8435914659500123 and perplexity is 46.69286951397192
At time: 200.04367113113403 and batch: 1000, loss is 3.7892030620574952 and perplexity is 44.22114472151521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.601650331078506 and perplexity of 99.64863325179624
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 201.59278059005737 and batch: 50, loss is 4.006064105033874 and perplexity is 54.93024486087219
At time: 202.1208004951477 and batch: 100, loss is 3.890657315254211 and perplexity is 48.94304696415702
At time: 202.6259913444519 and batch: 150, loss is 3.941802945137024 and perplexity is 51.511389838881314
At time: 203.14593291282654 and batch: 200, loss is 3.993398790359497 and perplexity is 54.23892316918118
At time: 203.66328144073486 and batch: 250, loss is 4.010247163772583 and perplexity is 55.160502556636445
At time: 204.17236733436584 and batch: 300, loss is 3.861229829788208 and perplexity is 47.523761581822605
At time: 204.68021774291992 and batch: 350, loss is 3.891815629005432 and perplexity is 48.99977121437958
At time: 205.17935729026794 and batch: 400, loss is 3.8847961378097535 and perplexity is 48.657022121233325
At time: 205.69886827468872 and batch: 450, loss is 3.9409325027465822 and perplexity is 51.46657165023041
At time: 206.1962857246399 and batch: 500, loss is 3.9950566577911375 and perplexity is 54.328918693141794
At time: 206.7177333831787 and batch: 550, loss is 3.8868517208099367 and perplexity is 48.75714353743122
At time: 207.21973967552185 and batch: 600, loss is 3.835506591796875 and perplexity is 46.316885479922135
At time: 207.734543800354 and batch: 650, loss is 3.8071250820159914 and perplexity is 45.020821466608425
At time: 208.2648959159851 and batch: 700, loss is 3.9056993246078493 and perplexity is 49.68481357847964
At time: 208.77161121368408 and batch: 750, loss is 3.8483245468139646 and perplexity is 46.91439447537835
At time: 209.27827763557434 and batch: 800, loss is 3.9708994674682616 and perplexity is 53.03221014126335
At time: 209.78048300743103 and batch: 850, loss is 3.8491045808792115 and perplexity is 46.951003577536724
At time: 210.28770875930786 and batch: 900, loss is 3.814967908859253 and perplexity is 45.375300214436706
At time: 210.79058003425598 and batch: 950, loss is 3.802673740386963 and perplexity is 44.820863779998994
At time: 211.30305552482605 and batch: 1000, loss is 3.7475310277938845 and perplexity is 42.41622812434731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.586710487923971 and perplexity of 98.17096385934398
Finished 19 epochs...
Completing Train Step...
At time: 212.84931206703186 and batch: 50, loss is 3.9894356393814085 and perplexity is 54.02439151922816
At time: 213.3511574268341 and batch: 100, loss is 3.875450110435486 and perplexity is 48.204390708034886
At time: 213.84910440444946 and batch: 150, loss is 3.927930178642273 and perplexity is 50.80171829119773
At time: 214.35101079940796 and batch: 200, loss is 3.981173663139343 and perplexity is 53.57988207177571
At time: 214.85570669174194 and batch: 250, loss is 3.998160834312439 and perplexity is 54.49782727236736
At time: 215.35841870307922 and batch: 300, loss is 3.8503224802017213 and perplexity is 47.00822000783848
At time: 215.86235666275024 and batch: 350, loss is 3.881939425468445 and perplexity is 48.51822135697022
At time: 216.35671186447144 and batch: 400, loss is 3.8761463832855223 and perplexity is 48.23796580389486
At time: 216.85928964614868 and batch: 450, loss is 3.9327261018753052 and perplexity is 51.04594460944719
At time: 217.35621190071106 and batch: 500, loss is 3.9872467517852783 and perplexity is 53.90626752597831
At time: 217.87104439735413 and batch: 550, loss is 3.8796048593521117 and perplexity is 48.40508447551534
At time: 218.37687516212463 and batch: 600, loss is 3.8293698740005495 and perplexity is 46.03352217411944
At time: 218.87461376190186 and batch: 650, loss is 3.8016151285171507 and perplexity is 44.77344098718068
At time: 219.37009596824646 and batch: 700, loss is 3.9011781644821166 and perplexity is 49.4606876169321
At time: 219.87935948371887 and batch: 750, loss is 3.8448252487182617 and perplexity is 46.75051392480648
At time: 220.38544964790344 and batch: 800, loss is 3.9683544731140135 and perplexity is 52.897415064970204
At time: 220.89774227142334 and batch: 850, loss is 3.846093897819519 and perplexity is 46.80986155998265
At time: 221.4315481185913 and batch: 900, loss is 3.8125022268295288 and perplexity is 45.26355697037178
At time: 221.93232989311218 and batch: 950, loss is 3.8010303497314455 and perplexity is 44.747266082738676
At time: 222.4392421245575 and batch: 1000, loss is 3.746329960823059 and perplexity is 42.36531397549201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.58600932795827 and perplexity of 98.10215443571717
Finished 20 epochs...
Completing Train Step...
At time: 223.94106817245483 and batch: 50, loss is 3.980530438423157 and perplexity is 53.54542924897825
At time: 224.45576167106628 and batch: 100, loss is 3.8672341775894163 and perplexity is 47.80996916023659
At time: 224.95237755775452 and batch: 150, loss is 3.919793562889099 and perplexity is 50.390041329723246
At time: 225.47405004501343 and batch: 200, loss is 3.97390820980072 and perplexity is 53.19201067566559
At time: 225.97124695777893 and batch: 250, loss is 3.990876441001892 and perplexity is 54.10228605188397
At time: 226.47922778129578 and batch: 300, loss is 3.8433358478546142 and perplexity is 46.680935496938424
At time: 226.99599766731262 and batch: 350, loss is 3.875301728248596 and perplexity is 48.19723856576231
At time: 227.5038013458252 and batch: 400, loss is 3.870362038612366 and perplexity is 47.95974621807984
At time: 228.01254224777222 and batch: 450, loss is 3.9269041776657105 and perplexity is 50.74962240840324
At time: 228.51222395896912 and batch: 500, loss is 3.9814405632019043 and perplexity is 53.59418445422117
At time: 229.02038073539734 and batch: 550, loss is 3.8741645860671996 and perplexity is 48.14246260269395
At time: 229.54830956459045 and batch: 600, loss is 3.8244659996032713 and perplexity is 45.80833216635034
At time: 230.05205941200256 and batch: 650, loss is 3.7967826795578 and perplexity is 44.55759756507716
At time: 230.55098962783813 and batch: 700, loss is 3.8970618295669555 and perplexity is 49.25750932340973
At time: 231.0581624507904 and batch: 750, loss is 3.8413621377944946 and perplexity is 46.588891728667974
At time: 231.556791305542 and batch: 800, loss is 3.9651968097686767 and perplexity is 52.73064627477315
At time: 232.05546522140503 and batch: 850, loss is 3.8423188972473143 and perplexity is 46.63348742149924
At time: 232.55538773536682 and batch: 900, loss is 3.8088503885269165 and perplexity is 45.09856322791105
At time: 233.06978273391724 and batch: 950, loss is 3.797946891784668 and perplexity is 44.60950227314756
At time: 233.58949613571167 and batch: 1000, loss is 3.743321943283081 and perplexity is 42.2380698402063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.585641256192836 and perplexity of 98.06605244701004
Finished 21 epochs...
Completing Train Step...
At time: 235.11754822731018 and batch: 50, loss is 3.973310642242432 and perplexity is 53.16023435092166
At time: 235.64200806617737 and batch: 100, loss is 3.860330753326416 and perplexity is 47.48105328829619
At time: 236.1383810043335 and batch: 150, loss is 3.9129473304748537 and perplexity is 50.04623761833771
At time: 236.658762216568 and batch: 200, loss is 3.9676380586624145 and perplexity is 52.8595321639146
At time: 237.17065048217773 and batch: 250, loss is 3.9846369075775145 and perplexity is 53.76576399183166
At time: 237.68715906143188 and batch: 300, loss is 3.8372762632369994 and perplexity is 46.398923718305014
At time: 238.19903182983398 and batch: 350, loss is 3.8695995664596556 and perplexity is 47.923192184624725
At time: 238.71468019485474 and batch: 400, loss is 3.8651540422439576 and perplexity is 47.710621317823204
At time: 239.2220859527588 and batch: 450, loss is 3.9216237926483153 and perplexity is 50.482351131238104
At time: 239.72613406181335 and batch: 500, loss is 3.9761360359191893 and perplexity is 53.310645325961595
At time: 240.23807382583618 and batch: 550, loss is 3.868972644805908 and perplexity is 47.89315751340407
At time: 240.7508885860443 and batch: 600, loss is 3.819661092758179 and perplexity is 45.58875534310222
At time: 241.26456332206726 and batch: 650, loss is 3.792124004364014 and perplexity is 44.350500963063
At time: 241.76832747459412 and batch: 700, loss is 3.8929691219329836 and perplexity is 49.05632471468774
At time: 242.28577709197998 and batch: 750, loss is 3.8376543712615967 and perplexity is 46.41647084084045
At time: 242.8030264377594 and batch: 800, loss is 3.961865572929382 and perplexity is 52.555280258381615
At time: 243.31996822357178 and batch: 850, loss is 3.838327522277832 and perplexity is 46.447726654118
At time: 243.82956266403198 and batch: 900, loss is 3.804795422554016 and perplexity is 44.916060360125265
At time: 244.33092880249023 and batch: 950, loss is 3.7944995164871216 and perplexity is 44.455981351134305
At time: 244.85214352607727 and batch: 1000, loss is 3.7397979307174682 and perplexity is 42.0894843137428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.585544493140244 and perplexity of 98.05656373550538
Finished 22 epochs...
Completing Train Step...
At time: 246.3360936641693 and batch: 50, loss is 3.9666742372512815 and perplexity is 52.808609559123006
At time: 246.8475377559662 and batch: 100, loss is 3.853923659324646 and perplexity is 47.177810207402246
At time: 247.3648808002472 and batch: 150, loss is 3.9067576217651365 and perplexity is 49.73742270858636
At time: 247.8621907234192 and batch: 200, loss is 3.9619345092773437 and perplexity is 52.55890335234872
At time: 248.3601884841919 and batch: 250, loss is 3.9789229249954223 and perplexity is 53.45942339876689
At time: 248.85844373703003 and batch: 300, loss is 3.8318064832687377 and perplexity is 46.14582464395342
At time: 249.3592472076416 and batch: 350, loss is 3.864240527153015 and perplexity is 47.667056846681625
At time: 249.86477375030518 and batch: 400, loss is 3.8600963354110718 and perplexity is 47.469924183247755
At time: 250.36505436897278 and batch: 450, loss is 3.916652798652649 and perplexity is 50.232026363812025
At time: 250.86190485954285 and batch: 500, loss is 3.9710672330856323 and perplexity is 53.04110786908376
At time: 251.36635184288025 and batch: 550, loss is 3.8640636301040647 and perplexity is 47.658625430761575
At time: 251.8707959651947 and batch: 600, loss is 3.8148315811157225 and perplexity is 45.36911472378303
At time: 252.3792381286621 and batch: 650, loss is 3.7874539995193484 and perplexity is 44.143866775569826
At time: 252.88509917259216 and batch: 700, loss is 3.8889113807678224 and perplexity is 48.85767016343146
At time: 253.38282799720764 and batch: 750, loss is 3.833865180015564 and perplexity is 46.24092275852978
At time: 253.87854743003845 and batch: 800, loss is 3.9583751010894774 and perplexity is 52.37215731126805
At time: 254.39145064353943 and batch: 850, loss is 3.834232201576233 and perplexity is 46.25789728898617
At time: 254.894540309906 and batch: 900, loss is 3.8005430698394775 and perplexity is 44.72546695132655
At time: 255.40984177589417 and batch: 950, loss is 3.790858492851257 and perplexity is 44.29441039261489
At time: 255.9081676006317 and batch: 1000, loss is 3.7360974502563478 and perplexity is 41.93402082163261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.585406791873094 and perplexity of 98.04306215204056
Finished 23 epochs...
Completing Train Step...
At time: 257.41802620887756 and batch: 50, loss is 3.960614686012268 and perplexity is 52.48958064584081
At time: 257.9282052516937 and batch: 100, loss is 3.8479844760894775 and perplexity is 46.89844297573298
At time: 258.4249060153961 and batch: 150, loss is 3.901009497642517 and perplexity is 49.45234594256903
At time: 258.93073320388794 and batch: 200, loss is 3.9565608406066897 and perplexity is 52.277226716312384
At time: 259.4400737285614 and batch: 250, loss is 3.973509588241577 and perplexity is 53.17081141895721
At time: 259.9535093307495 and batch: 300, loss is 3.826577653884888 and perplexity is 45.905165730626024
At time: 260.4507007598877 and batch: 350, loss is 3.8590930271148682 and perplexity is 47.42232109877255
At time: 260.94995760917664 and batch: 400, loss is 3.855165958404541 and perplexity is 47.23645557761678
At time: 261.45060205459595 and batch: 450, loss is 3.9117394304275512 and perplexity is 49.985823260149765
At time: 261.95704650878906 and batch: 500, loss is 3.9661605596542358 and perplexity is 52.781489925431806
At time: 262.4565246105194 and batch: 550, loss is 3.859324026107788 and perplexity is 47.433276872525916
At time: 262.966500043869 and batch: 600, loss is 3.809763193130493 and perplexity is 45.139748198095724
At time: 263.4860053062439 and batch: 650, loss is 3.7828448390960694 and perplexity is 43.94086879640321
At time: 264.0206460952759 and batch: 700, loss is 3.884722156524658 and perplexity is 48.65342254536015
At time: 264.53324818611145 and batch: 750, loss is 3.829959959983826 and perplexity is 46.06069392636186
At time: 265.0384113788605 and batch: 800, loss is 3.954780521392822 and perplexity is 52.18423936328781
At time: 265.55623149871826 and batch: 850, loss is 3.830064835548401 and perplexity is 46.06552482095907
At time: 266.068794965744 and batch: 900, loss is 3.7961498641967775 and perplexity is 44.52940975266871
At time: 266.5747666358948 and batch: 950, loss is 3.78710777759552 and perplexity is 44.12858584653854
At time: 267.08549642562866 and batch: 1000, loss is 3.732174825668335 and perplexity is 41.76985159810555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5852467606707314 and perplexity of 98.0273734582948
Finished 24 epochs...
Completing Train Step...
At time: 268.59939408302307 and batch: 50, loss is 3.9547146940231324 and perplexity is 52.180804325132264
At time: 269.1222972869873 and batch: 100, loss is 3.842303686141968 and perplexity is 46.63277808000435
At time: 269.62310552597046 and batch: 150, loss is 3.8954588270187376 and perplexity is 49.178612663113704
At time: 270.1381244659424 and batch: 200, loss is 3.9514057636260986 and perplexity is 52.00842702508329
At time: 270.6466088294983 and batch: 250, loss is 3.968263020515442 and perplexity is 52.892577680103884
At time: 271.15553426742554 and batch: 300, loss is 3.821487364768982 and perplexity is 45.67208888268649
At time: 271.6621298789978 and batch: 350, loss is 3.8539870882034304 and perplexity is 47.18080273791263
At time: 272.1630153656006 and batch: 400, loss is 3.8502650594711305 and perplexity is 47.00552083899673
At time: 272.66195607185364 and batch: 450, loss is 3.907330198287964 and perplexity is 49.765909343746216
At time: 273.1853153705597 and batch: 500, loss is 3.9612727403640746 and perplexity is 52.52413301022932
At time: 273.68827724456787 and batch: 550, loss is 3.8549891185760496 and perplexity is 47.22810302946725
At time: 274.42027163505554 and batch: 600, loss is 3.805056929588318 and perplexity is 44.927807761809696
At time: 274.9226243495941 and batch: 650, loss is 3.77823495388031 and perplexity is 43.73877261297121
At time: 275.4153347015381 and batch: 700, loss is 3.8807009506225585 and perplexity is 48.458169954004155
At time: 275.9264929294586 and batch: 750, loss is 3.8260085201263427 and perplexity is 45.87904698435225
At time: 276.4354124069214 and batch: 800, loss is 3.951114897727966 and perplexity is 51.99330174706661
At time: 276.9469063282013 and batch: 850, loss is 3.8258816051483153 and perplexity is 45.873224615593024
At time: 277.4452872276306 and batch: 900, loss is 3.791787304878235 and perplexity is 44.335570685844075
At time: 277.94404006004333 and batch: 950, loss is 3.7833021593093874 and perplexity is 43.960968439530795
At time: 278.44961071014404 and batch: 1000, loss is 3.728253812789917 and perplexity is 41.60639214479394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.585308167992569 and perplexity of 98.0333932415931
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 279.9762156009674 and batch: 50, loss is 3.9512129974365235 and perplexity is 51.99840252500326
At time: 280.476359128952 and batch: 100, loss is 3.8390633344650267 and perplexity is 46.481916034396356
At time: 280.97545313835144 and batch: 150, loss is 3.890634088516235 and perplexity is 48.94191019003126
At time: 281.47905564308167 and batch: 200, loss is 3.9454661655426024 and perplexity is 51.70043345607597
At time: 281.9805133342743 and batch: 250, loss is 3.9609698963165285 and perplexity is 52.50822879756428
At time: 282.4959840774536 and batch: 300, loss is 3.8131565189361574 and perplexity is 45.29318224915158
At time: 283.00163078308105 and batch: 350, loss is 3.844810700416565 and perplexity is 46.74983378917284
At time: 283.51322054862976 and batch: 400, loss is 3.841929521560669 and perplexity is 46.61533300998641
At time: 284.0217795372009 and batch: 450, loss is 3.8965310192108156 and perplexity is 49.23136986550444
At time: 284.52511739730835 and batch: 500, loss is 3.9501908922195437 and perplexity is 51.94528183859778
At time: 285.0335581302643 and batch: 550, loss is 3.8423263216018677 and perplexity is 46.63383364632916
At time: 285.54849338531494 and batch: 600, loss is 3.789022912979126 and perplexity is 44.21317904057579
At time: 286.0811257362366 and batch: 650, loss is 3.762120523452759 and perplexity is 43.039595752969284
At time: 286.5843884944916 and batch: 700, loss is 3.863385615348816 and perplexity is 47.626323131459884
At time: 287.102502822876 and batch: 750, loss is 3.809427456855774 and perplexity is 45.12459569095941
At time: 287.6029624938965 and batch: 800, loss is 3.933352346420288 and perplexity is 51.077921865548376
At time: 288.10886430740356 and batch: 850, loss is 3.8063224363327026 and perplexity is 44.98470019683
At time: 288.62058091163635 and batch: 900, loss is 3.7717623043060304 and perplexity is 43.45658111358743
At time: 289.11911487579346 and batch: 950, loss is 3.7614873266220092 and perplexity is 43.01235184363054
At time: 289.62251567840576 and batch: 1000, loss is 3.705600962638855 and perplexity is 40.67448381856072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.579846265839367 and perplexity of 97.4994040657889
Finished 26 epochs...
Completing Train Step...
At time: 291.1615512371063 and batch: 50, loss is 3.9427485418319703 and perplexity is 51.560121875660364
At time: 291.67549896240234 and batch: 100, loss is 3.8308941745758056 and perplexity is 46.10374460489955
At time: 292.18417716026306 and batch: 150, loss is 3.8836041355133055 and perplexity is 48.599057393034414
At time: 292.6861755847931 and batch: 200, loss is 3.939405574798584 and perplexity is 51.388045870450874
At time: 293.1838583946228 and batch: 250, loss is 3.9547977018356324 and perplexity is 52.185135919329376
At time: 293.69729113578796 and batch: 300, loss is 3.807197332382202 and perplexity is 45.02407435495626
At time: 294.2003242969513 and batch: 350, loss is 3.8391204357147215 and perplexity is 46.48457028566994
At time: 294.7079334259033 and batch: 400, loss is 3.837620277404785 and perplexity is 46.41488835130664
At time: 295.2124705314636 and batch: 450, loss is 3.8920583486557008 and perplexity is 49.01166586518633
At time: 295.7182104587555 and batch: 500, loss is 3.9466260385513308 and perplexity is 51.76043418326661
At time: 296.2360932826996 and batch: 550, loss is 3.838636498451233 and perplexity is 46.462080112286856
At time: 296.75917506217957 and batch: 600, loss is 3.785737452507019 and perplexity is 44.0681567514416
At time: 297.2707691192627 and batch: 650, loss is 3.7593021774291993 and perplexity is 42.9184660523069
At time: 297.76605319976807 and batch: 700, loss is 3.8612258720397947 and perplexity is 47.5235734951028
At time: 298.27363896369934 and batch: 750, loss is 3.808236083984375 and perplexity is 45.07086748333986
At time: 298.7801194190979 and batch: 800, loss is 3.932417464256287 and perplexity is 51.03019234163864
At time: 299.2988953590393 and batch: 850, loss is 3.805234088897705 and perplexity is 44.93576784628578
At time: 299.7994587421417 and batch: 900, loss is 3.771108379364014 and perplexity is 43.428173060681985
At time: 300.3030216693878 and batch: 950, loss is 3.761225743293762 and perplexity is 43.00110200092935
At time: 300.808123588562 and batch: 1000, loss is 3.7052890777587892 and perplexity is 40.661800040095414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5796091963605186 and perplexity of 97.47629267249017
Finished 27 epochs...
Completing Train Step...
At time: 302.37138509750366 and batch: 50, loss is 3.9380454778671266 and perplexity is 51.318200655846915
At time: 302.90253615379333 and batch: 100, loss is 3.8265074157714842 and perplexity is 45.90194155162107
At time: 303.4043836593628 and batch: 150, loss is 3.8797556066513064 and perplexity is 48.41238196129162
At time: 303.90721678733826 and batch: 200, loss is 3.93604887008667 and perplexity is 51.215840557627054
At time: 304.4053997993469 and batch: 250, loss is 3.951242070198059 and perplexity is 51.99991428413549
At time: 304.9195251464844 and batch: 300, loss is 3.803095889091492 and perplexity is 44.83978884389415
At time: 305.42431926727295 and batch: 350, loss is 3.8357929468154905 and perplexity is 46.33015045168092
At time: 305.94265389442444 and batch: 400, loss is 3.8347559785842895 and perplexity is 46.28213245838537
At time: 306.45425844192505 and batch: 450, loss is 3.8890255546569823 and perplexity is 48.863248752107836
At time: 306.95412397384644 and batch: 500, loss is 3.9443563175201417 and perplexity is 51.64308566179899
At time: 307.4545159339905 and batch: 550, loss is 3.835772008895874 and perplexity is 46.329180404870364
At time: 307.95217633247375 and batch: 600, loss is 3.783159990310669 and perplexity is 43.954718996914075
At time: 308.4551217556 and batch: 650, loss is 3.7568203020095825 and perplexity is 42.8120798396361
At time: 308.97705817222595 and batch: 700, loss is 3.8590836572647094 and perplexity is 47.421876760811365
At time: 309.4787714481354 and batch: 750, loss is 3.806788206100464 and perplexity is 45.00565759047479
At time: 309.9759576320648 and batch: 800, loss is 3.9309852170944213 and perplexity is 50.95715680844045
At time: 310.4735908508301 and batch: 850, loss is 3.8036140727996828 and perplexity is 44.86303011305524
At time: 310.971479177475 and batch: 900, loss is 3.7696109199523926 and perplexity is 43.36318980124259
At time: 311.4675133228302 and batch: 950, loss is 3.7599391078948976 and perplexity is 42.94581083831586
At time: 311.9677450656891 and batch: 1000, loss is 3.7038060331344607 and perplexity is 40.60154147024896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.579222144150153 and perplexity of 97.43857155844424
Finished 28 epochs...
Completing Train Step...
At time: 313.4940764904022 and batch: 50, loss is 3.934299278259277 and perplexity is 51.12631208353543
At time: 313.9943232536316 and batch: 100, loss is 3.822711944580078 and perplexity is 45.728052259479966
At time: 314.4934289455414 and batch: 150, loss is 3.876783375740051 and perplexity is 48.26870281271579
At time: 314.9943506717682 and batch: 200, loss is 3.933014669418335 and perplexity is 51.06067693779843
At time: 315.4921383857727 and batch: 250, loss is 3.9481421089172364 and perplexity is 51.83896615861663
At time: 316.01541662216187 and batch: 300, loss is 3.7999263763427735 and perplexity is 44.69789354976321
At time: 316.52520728111267 and batch: 350, loss is 3.8326862907409667 and perplexity is 46.186441950372156
At time: 317.05014300346375 and batch: 400, loss is 3.8322528314590456 and perplexity is 46.16642634669785
At time: 317.5594618320465 and batch: 450, loss is 3.886047430038452 and perplexity is 48.71794438271241
At time: 318.06677770614624 and batch: 500, loss is 3.941705026626587 and perplexity is 51.5063461672562
At time: 318.5649244785309 and batch: 550, loss is 3.8326879215240477 and perplexity is 46.186517270501675
At time: 319.06650733947754 and batch: 600, loss is 3.780384216308594 and perplexity is 43.832879807910665
At time: 319.5840916633606 and batch: 650, loss is 3.7542350912094116 and perplexity is 42.70154452855874
At time: 320.0922300815582 and batch: 700, loss is 3.8568512105941775 and perplexity is 47.31612803304329
At time: 320.59434366226196 and batch: 750, loss is 3.804786148071289 and perplexity is 44.91564378883103
At time: 321.0933382511139 and batch: 800, loss is 3.9290620279312134 and perplexity is 50.859250732807155
At time: 321.6032371520996 and batch: 850, loss is 3.801795678138733 and perplexity is 44.7815255448178
At time: 322.140154838562 and batch: 900, loss is 3.7674912786483765 and perplexity is 43.27137273706372
At time: 322.6467685699463 and batch: 950, loss is 3.7584340000152587 and perplexity is 42.88122137927064
At time: 323.16613030433655 and batch: 1000, loss is 3.70200945854187 and perplexity is 40.5286632575984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5786352390196265 and perplexity of 97.38140113933264
Finished 29 epochs...
Completing Train Step...
At time: 324.6599860191345 and batch: 50, loss is 3.930342607498169 and perplexity is 50.92442176952918
At time: 325.18975949287415 and batch: 100, loss is 3.819219093322754 and perplexity is 45.568609591512235
At time: 325.7037913799286 and batch: 150, loss is 3.8735884284973143 and perplexity is 48.11473294752495
At time: 326.2106223106384 and batch: 200, loss is 3.9298034572601317 and perplexity is 50.896973255511554
At time: 326.71600365638733 and batch: 250, loss is 3.944845771789551 and perplexity is 51.668368777521344
At time: 327.23179364204407 and batch: 300, loss is 3.79697154045105 and perplexity is 44.56601354745432
At time: 327.7382619380951 and batch: 350, loss is 3.8294871473312377 and perplexity is 46.03892099515078
At time: 328.24894762039185 and batch: 400, loss is 3.8299616765975952 and perplexity is 46.06077299485114
At time: 328.75317430496216 and batch: 450, loss is 3.8835920429229738 and perplexity is 48.598469708096175
At time: 329.2595365047455 and batch: 500, loss is 3.939121251106262 and perplexity is 51.37343710841477
At time: 329.76136469841003 and batch: 550, loss is 3.8299476099014282 and perplexity is 46.06012507650926
At time: 330.27112102508545 and batch: 600, loss is 3.7776800632476806 and perplexity is 43.71450911018333
At time: 330.77985429763794 and batch: 650, loss is 3.75170286655426 and perplexity is 42.593551413810125
At time: 331.2873184680939 and batch: 700, loss is 3.8542279481887816 and perplexity is 47.19216807404134
At time: 331.7925169467926 and batch: 750, loss is 3.804157772064209 and perplexity is 44.88742874168931
At time: 332.29406809806824 and batch: 800, loss is 3.927600288391113 and perplexity is 50.78496206360133
At time: 332.80953764915466 and batch: 850, loss is 3.79980619430542 and perplexity is 44.69252198864003
At time: 333.31036281585693 and batch: 900, loss is 3.7654364013671877 and perplexity is 43.18254667088898
At time: 333.82558488845825 and batch: 950, loss is 3.7566620016098025 and perplexity is 42.80530320666807
At time: 334.3352265357971 and batch: 1000, loss is 3.7002259922027587 and perplexity is 40.456446168425174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.578517262528583 and perplexity of 97.36991310100584
Finished 30 epochs...
Completing Train Step...
At time: 335.8417139053345 and batch: 50, loss is 3.927192363739014 and perplexity is 50.76424985041805
At time: 336.37817215919495 and batch: 100, loss is 3.8160199689865113 and perplexity is 45.423062878746606
At time: 336.88339042663574 and batch: 150, loss is 3.8709029245376585 and perplexity is 47.98569398654858
At time: 337.39070653915405 and batch: 200, loss is 3.9272364521026613 and perplexity is 50.76648801246383
At time: 337.88614439964294 and batch: 250, loss is 3.942089247703552 and perplexity is 51.52613979337178
At time: 338.388507604599 and batch: 300, loss is 3.793917202949524 and perplexity is 44.430101567171924
At time: 338.90200114250183 and batch: 350, loss is 3.826537628173828 and perplexity is 45.90332838049721
At time: 339.42230200767517 and batch: 400, loss is 3.8276097345352174 and perplexity is 45.95256802121251
At time: 339.92624139785767 and batch: 450, loss is 3.881026515960693 and perplexity is 48.47394882287763
At time: 340.4343936443329 and batch: 500, loss is 3.9368224048614504 and perplexity is 51.2554731179236
At time: 340.94575548171997 and batch: 550, loss is 3.8272338914871216 and perplexity is 45.93530031315687
At time: 341.45589542388916 and batch: 600, loss is 3.7751879358291625 and perplexity is 43.60570261957575
At time: 341.9622151851654 and batch: 650, loss is 3.7492650508880616 and perplexity is 42.48984264968075
At time: 342.4617099761963 and batch: 700, loss is 3.852734298706055 and perplexity is 47.12173213300739
At time: 342.96818566322327 and batch: 750, loss is 3.8021536302566528 and perplexity is 44.79755805599472
At time: 343.4694678783417 and batch: 800, loss is 3.925645475387573 and perplexity is 50.68578392831178
At time: 343.97977471351624 and batch: 850, loss is 3.7979486846923827 and perplexity is 44.60958225394004
At time: 344.48812222480774 and batch: 900, loss is 3.763432755470276 and perplexity is 43.096110760829035
At time: 344.9978013038635 and batch: 950, loss is 3.754759306907654 and perplexity is 42.72393521680219
At time: 345.503399848938 and batch: 1000, loss is 3.6983310079574583 and perplexity is 40.37985443330071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5783843994140625 and perplexity of 97.35697709046903
Finished 31 epochs...
Completing Train Step...
At time: 347.0287847518921 and batch: 50, loss is 3.924043517112732 and perplexity is 50.604652419327536
At time: 347.5329701900482 and batch: 100, loss is 3.8130929183959963 and perplexity is 45.29030166989909
At time: 348.04428815841675 and batch: 150, loss is 3.868115978240967 and perplexity is 47.85214661551346
At time: 348.5502562522888 and batch: 200, loss is 3.9247384452819825 and perplexity is 50.63983123974977
At time: 349.0701518058777 and batch: 250, loss is 3.9394814205169677 and perplexity is 51.39194358151673
At time: 349.5778887271881 and batch: 300, loss is 3.7910505151748657 and perplexity is 44.30291672489806
At time: 350.0747380256653 and batch: 350, loss is 3.823676710128784 and perplexity is 45.772190396963005
At time: 350.579110622406 and batch: 400, loss is 3.8252967166900635 and perplexity is 45.846401740932144
At time: 351.0792269706726 and batch: 450, loss is 3.8789280128479002 and perplexity is 48.372332748498344
At time: 351.5851466655731 and batch: 500, loss is 3.9345231199264528 and perplexity is 51.13775756340861
At time: 352.08634757995605 and batch: 550, loss is 3.8247309160232543 and perplexity is 45.82046915328583
At time: 352.5960490703583 and batch: 600, loss is 3.7727833080291746 and perplexity is 43.50097310303641
At time: 353.1115834712982 and batch: 650, loss is 3.7469029951095583 and perplexity is 42.38959771
At time: 353.62741804122925 and batch: 700, loss is 3.850815167427063 and perplexity is 47.03138606366368
At time: 354.1413152217865 and batch: 750, loss is 3.800321626663208 and perplexity is 44.715563898386925
At time: 354.66934394836426 and batch: 800, loss is 3.9241459465026853 and perplexity is 50.609836088479156
At time: 355.1895797252655 and batch: 850, loss is 3.7957868862152098 and perplexity is 44.513249490483396
At time: 355.7125265598297 and batch: 900, loss is 3.7613987350463867 and perplexity is 43.00854148039502
At time: 356.22958755493164 and batch: 950, loss is 3.7531765127182006 and perplexity is 42.65636550896744
At time: 356.7406976222992 and batch: 1000, loss is 3.6964019775390624 and perplexity is 40.30203554743881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.578334157059833 and perplexity of 97.35208576961607
Finished 32 epochs...
Completing Train Step...
At time: 358.2418131828308 and batch: 50, loss is 3.920928997993469 and perplexity is 50.44728844560185
At time: 358.7572627067566 and batch: 100, loss is 3.810248465538025 and perplexity is 45.1616585882031
At time: 359.2639660835266 and batch: 150, loss is 3.8657078838348387 and perplexity is 47.73705276297697
At time: 359.7711775302887 and batch: 200, loss is 3.9225651264190673 and perplexity is 50.529894246644254
At time: 360.2774257659912 and batch: 250, loss is 3.936925392150879 and perplexity is 51.260752051995276
At time: 360.794881105423 and batch: 300, loss is 3.788213005065918 and perplexity is 44.17738493392445
At time: 361.3033685684204 and batch: 350, loss is 3.8209433269500734 and perplexity is 45.647248296787524
At time: 361.8090353012085 and batch: 400, loss is 3.8232615327835084 and perplexity is 45.75319076484787
At time: 362.3206810951233 and batch: 450, loss is 3.876435732841492 and perplexity is 48.25192545739346
At time: 362.8354651927948 and batch: 500, loss is 3.9323120403289793 and perplexity is 51.024812821920754
At time: 363.33504366874695 and batch: 550, loss is 3.8222965002059937 and perplexity is 45.70905874307887
At time: 363.84542441368103 and batch: 600, loss is 3.7703936386108396 and perplexity is 43.397144265652706
At time: 364.35985469818115 and batch: 650, loss is 3.7446332120895387 and perplexity is 42.29349163212508
At time: 364.86121368408203 and batch: 700, loss is 3.848801498413086 and perplexity is 46.93677570780321
At time: 365.36374831199646 and batch: 750, loss is 3.79866660118103 and perplexity is 44.64161970732382
At time: 365.8619775772095 and batch: 800, loss is 3.9224553012847903 and perplexity is 50.52434509894713
At time: 366.36375284194946 and batch: 850, loss is 3.7938525438308717 and perplexity is 44.42722884883768
At time: 366.8633599281311 and batch: 900, loss is 3.759413514137268 and perplexity is 42.9232447190481
At time: 367.38333010673523 and batch: 950, loss is 3.7512766551971435 and perplexity is 42.57540142659772
At time: 367.8875916004181 and batch: 1000, loss is 3.6944304943084716 and perplexity is 40.22265903064394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.578354254001525 and perplexity of 97.35404226846714
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 369.3582088947296 and batch: 50, loss is 3.9191808605194094 and perplexity is 50.35917668837676
At time: 369.8702201843262 and batch: 100, loss is 3.8095360660552977 and perplexity is 45.129496903329795
At time: 370.37427830696106 and batch: 150, loss is 3.865861539840698 and perplexity is 47.74438841140482
At time: 370.8764295578003 and batch: 200, loss is 3.921543779373169 and perplexity is 50.47831203457984
At time: 371.37620306015015 and batch: 250, loss is 3.934946141242981 and perplexity is 51.15939450105765
At time: 371.87931728363037 and batch: 300, loss is 3.7841918659210205 and perplexity is 44.000098208231016
At time: 372.3955285549164 and batch: 350, loss is 3.816667242050171 and perplexity is 45.452473521148406
At time: 372.905077457428 and batch: 400, loss is 3.8201770210266113 and perplexity is 45.61228193920251
At time: 373.4032382965088 and batch: 450, loss is 3.872439761161804 and perplexity is 48.05949685545207
At time: 373.92021465301514 and batch: 500, loss is 3.9281293869018556 and perplexity is 50.811839421155206
At time: 374.4264016151428 and batch: 550, loss is 3.815817928314209 and perplexity is 45.41388649961667
At time: 374.9279315471649 and batch: 600, loss is 3.7617105197906495 and perplexity is 43.021952978143155
At time: 375.4255108833313 and batch: 650, loss is 3.7357530927658082 and perplexity is 41.9195830134806
At time: 375.9373586177826 and batch: 700, loss is 3.840397696495056 and perplexity is 46.54398113768526
At time: 376.43591237068176 and batch: 750, loss is 3.790348572731018 and perplexity is 44.27182953924658
At time: 376.93579387664795 and batch: 800, loss is 3.9123685503005983 and perplexity is 50.01728022899793
At time: 377.4361147880554 and batch: 850, loss is 3.7836245918273925 and perplexity is 43.97514517067543
At time: 377.93402671813965 and batch: 900, loss is 3.7477248668670655 and perplexity is 42.42445084361117
At time: 378.4370491504669 and batch: 950, loss is 3.7396286964416503 and perplexity is 42.08236193303789
At time: 378.9474081993103 and batch: 1000, loss is 3.6825715351104735 and perplexity is 39.748477366065956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.574387713176448 and perplexity of 96.96864833101414
Finished 34 epochs...
Completing Train Step...
At time: 380.50633907318115 and batch: 50, loss is 3.914880428314209 and perplexity is 50.14307546050148
At time: 381.0075876712799 and batch: 100, loss is 3.8046105337142944 and perplexity is 44.90775664949577
At time: 381.5018882751465 and batch: 150, loss is 3.861932325363159 and perplexity is 47.55715854327422
At time: 381.9994924068451 and batch: 200, loss is 3.918167161941528 and perplexity is 50.30815352800626
At time: 382.51048517227173 and batch: 250, loss is 3.931322245597839 and perplexity is 50.974333717129355
At time: 383.0084044933319 and batch: 300, loss is 3.780699095726013 and perplexity is 43.84668405279063
At time: 383.524361371994 and batch: 350, loss is 3.8133254194259645 and perplexity is 45.30083293590256
At time: 384.0390954017639 and batch: 400, loss is 3.817653784751892 and perplexity is 45.4973364531363
At time: 384.5480201244354 and batch: 450, loss is 3.8700010681152346 and perplexity is 47.94243728883956
At time: 385.0681495666504 and batch: 500, loss is 3.926158056259155 and perplexity is 50.71177115132229
At time: 385.5770699977875 and batch: 550, loss is 3.813621611595154 and perplexity is 45.314252675188676
At time: 386.08950567245483 and batch: 600, loss is 3.760054712295532 and perplexity is 42.950775850020655
At time: 386.60823225975037 and batch: 650, loss is 3.734579725265503 and perplexity is 41.870424783121216
At time: 387.1191883087158 and batch: 700, loss is 3.839740500450134 and perplexity is 46.5134026664924
At time: 387.62488555908203 and batch: 750, loss is 3.789991912841797 and perplexity is 44.25604236892006
At time: 388.1291992664337 and batch: 800, loss is 3.9121317768096926 and perplexity is 50.00543886486839
At time: 388.6312003135681 and batch: 850, loss is 3.783453845977783 and perplexity is 43.96763723814397
At time: 389.1479740142822 and batch: 900, loss is 3.7475298404693604 and perplexity is 42.416177762549346
At time: 389.6527855396271 and batch: 950, loss is 3.7396209239959717 and perplexity is 42.082034851436845
At time: 390.1503551006317 and batch: 1000, loss is 3.6827271032333373 and perplexity is 39.75466144308665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.574346030630717 and perplexity of 96.96460651513274
Finished 35 epochs...
Completing Train Step...
At time: 391.6403298377991 and batch: 50, loss is 3.912420573234558 and perplexity is 50.01988234234833
At time: 392.16732478141785 and batch: 100, loss is 3.8023304653167727 and perplexity is 44.80548053533219
At time: 392.67791271209717 and batch: 150, loss is 3.860250768661499 and perplexity is 47.47725568403609
At time: 393.18036937713623 and batch: 200, loss is 3.916430344581604 and perplexity is 50.22085328784476
At time: 393.69377398490906 and batch: 250, loss is 3.929367208480835 and perplexity is 50.87477435553251
At time: 394.19776606559753 and batch: 300, loss is 3.7787055158615113 and perplexity is 43.75935925973405
At time: 394.7021448612213 and batch: 350, loss is 3.811315169334412 and perplexity is 45.20985840376301
At time: 395.2024624347687 and batch: 400, loss is 3.8161975288391115 and perplexity is 45.431128907176216
At time: 395.6997666358948 and batch: 450, loss is 3.868542137145996 and perplexity is 47.872543579783716
At time: 396.20255398750305 and batch: 500, loss is 3.924950165748596 and perplexity is 50.650553863508485
At time: 396.71351766586304 and batch: 550, loss is 3.8120226860046387 and perplexity is 45.241856450483205
At time: 397.2259638309479 and batch: 600, loss is 3.7586913681030274 and perplexity is 42.89225905753036
At time: 397.736004114151 and batch: 650, loss is 3.733480076789856 and perplexity is 41.82440734047623
At time: 398.24746894836426 and batch: 700, loss is 3.8389979934692384 and perplexity is 46.47887895894037
At time: 398.7590265274048 and batch: 750, loss is 3.789415326118469 and perplexity is 44.230532277559846
At time: 399.26286578178406 and batch: 800, loss is 3.9114938259124754 and perplexity is 49.97354802375574
At time: 399.7953464984894 and batch: 850, loss is 3.7829017877578734 and perplexity is 43.94337124133444
At time: 400.3157002925873 and batch: 900, loss is 3.7468955850601198 and perplexity is 42.38928360214907
At time: 400.83088970184326 and batch: 950, loss is 3.7391083145141604 and perplexity is 42.06046872932966
At time: 401.344131231308 and batch: 1000, loss is 3.6822484588623046 and perplexity is 39.73563765134321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5742909501238564 and perplexity of 96.9592658025443
Finished 36 epochs...
Completing Train Step...
At time: 402.8354437351227 and batch: 50, loss is 3.9103273010253905 and perplexity is 49.91528662458288
At time: 403.34997510910034 and batch: 100, loss is 3.8004398727416993 and perplexity is 44.720851651087294
At time: 403.8507182598114 and batch: 150, loss is 3.8588649940490725 and perplexity is 47.411508474370045
At time: 404.35788130760193 and batch: 200, loss is 3.914990210533142 and perplexity is 50.14858058076628
At time: 404.8650357723236 and batch: 250, loss is 3.9277432346343994 and perplexity is 50.792222102029
At time: 405.36215257644653 and batch: 300, loss is 3.7770080852508543 and perplexity is 43.68514378944893
At time: 405.87943148612976 and batch: 350, loss is 3.809579315185547 and perplexity is 45.13144875702713
At time: 406.38027691841125 and batch: 400, loss is 3.814935178756714 and perplexity is 45.37381510051205
At time: 406.8895788192749 and batch: 450, loss is 3.8672523641586305 and perplexity is 47.8108386674565
At time: 407.4002010822296 and batch: 500, loss is 3.9238270473480226 and perplexity is 50.593699227685306
At time: 407.89744782447815 and batch: 550, loss is 3.810535488128662 and perplexity is 45.174622864879694
At time: 408.40536999702454 and batch: 600, loss is 3.7573927783966066 and perplexity is 42.836595761147215
At time: 408.9041202068329 and batch: 650, loss is 3.7323678159713745 and perplexity is 41.77791355233784
At time: 409.4060354232788 and batch: 700, loss is 3.8381901597976684 and perplexity is 46.44134691736509
At time: 409.90306544303894 and batch: 750, loss is 3.788742780685425 and perplexity is 44.2007952359521
At time: 410.41139459609985 and batch: 800, loss is 3.910750331878662 and perplexity is 49.93640679780246
At time: 410.91521310806274 and batch: 850, loss is 3.7822070789337157 and perplexity is 43.91285399509732
At time: 411.422474861145 and batch: 900, loss is 3.746113114356995 and perplexity is 42.35612820285987
At time: 411.92036962509155 and batch: 950, loss is 3.7384333086013792 and perplexity is 42.032087244155804
At time: 412.4523389339447 and batch: 1000, loss is 3.6815660572052002 and perplexity is 39.70853123614705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.574259688214558 and perplexity of 96.9562347181501
Finished 37 epochs...
Completing Train Step...
At time: 414.01972675323486 and batch: 50, loss is 3.9084117555618287 and perplexity is 49.81976314272176
At time: 414.5161817073822 and batch: 100, loss is 3.798738751411438 and perplexity is 44.64484072666873
At time: 415.0361123085022 and batch: 150, loss is 3.857594289779663 and perplexity is 47.35130072935325
At time: 415.5480079650879 and batch: 200, loss is 3.913677005767822 and perplexity is 50.0827684476348
At time: 416.06243681907654 and batch: 250, loss is 3.9262658739089966 and perplexity is 50.7172390700709
At time: 416.5686295032501 and batch: 300, loss is 3.7754408359527587 and perplexity is 43.616731901752374
At time: 417.0750136375427 and batch: 350, loss is 3.8079714155197144 and perplexity is 45.058940224496396
At time: 417.57918667793274 and batch: 400, loss is 3.813752279281616 and perplexity is 45.32017417061513
At time: 418.0770637989044 and batch: 450, loss is 3.8660347509384154 and perplexity is 47.75265898558834
At time: 418.5839726924896 and batch: 500, loss is 3.9227408742904664 and perplexity is 50.5387755484122
At time: 419.09114360809326 and batch: 550, loss is 3.8091099214553834 and perplexity is 45.1102693090863
At time: 419.5958249568939 and batch: 600, loss is 3.756135039329529 and perplexity is 42.78275236872483
At time: 420.1053342819214 and batch: 650, loss is 3.731263976097107 and perplexity is 41.73182286854235
At time: 420.6147515773773 and batch: 700, loss is 3.837364616394043 and perplexity is 46.40302339080578
At time: 421.1211473941803 and batch: 750, loss is 3.7880272483825683 and perplexity is 44.169179451555244
At time: 421.635137796402 and batch: 800, loss is 3.9099532699584962 and perplexity is 49.89662024779239
At time: 422.13783264160156 and batch: 850, loss is 3.7814391899108886 and perplexity is 43.879146739925396
At time: 422.64670157432556 and batch: 900, loss is 3.7452453899383547 and perplexity is 42.31939069745957
At time: 423.15145206451416 and batch: 950, loss is 3.737673954963684 and perplexity is 42.00018214096821
At time: 423.6568913459778 and batch: 1000, loss is 3.680791277885437 and perplexity is 39.67777780242774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.574248523246951 and perplexity of 96.95515221097332
Finished 38 epochs...
Completing Train Step...
At time: 425.15636348724365 and batch: 50, loss is 3.9066064834594725 and perplexity is 49.729906046832156
At time: 425.6801052093506 and batch: 100, loss is 3.797144160270691 and perplexity is 44.57370718869337
At time: 426.1878662109375 and batch: 150, loss is 3.8563810062408446 and perplexity is 47.29388501345143
At time: 426.6854374408722 and batch: 200, loss is 3.912416543960571 and perplexity is 50.01968079894362
At time: 427.19798040390015 and batch: 250, loss is 3.9248692750930787 and perplexity is 50.64645687271051
At time: 427.7006096839905 and batch: 300, loss is 3.7739512443542482 and perplexity is 43.55180915055407
At time: 428.20280385017395 and batch: 350, loss is 3.806445679664612 and perplexity is 44.9902446028157
At time: 428.6997056007385 and batch: 400, loss is 3.812611665725708 and perplexity is 45.26851083515156
At time: 429.2044367790222 and batch: 450, loss is 3.8648549365997313 and perplexity is 47.69635293568069
At time: 429.7074065208435 and batch: 500, loss is 3.9216695499420164 and perplexity is 50.48466111985456
At time: 430.21028327941895 and batch: 550, loss is 3.8077296352386476 and perplexity is 45.04804717817931
At time: 430.7118854522705 and batch: 600, loss is 3.7548931550979616 and perplexity is 42.72965412093773
At time: 431.21511816978455 and batch: 650, loss is 3.730144810676575 and perplexity is 41.68514418091086
At time: 431.7197256088257 and batch: 700, loss is 3.836511449813843 and perplexity is 46.36345076544961
At time: 432.22962188720703 and batch: 750, loss is 3.787275152206421 and perplexity is 44.13597246957354
At time: 432.73276591300964 and batch: 800, loss is 3.9091306495666505 and perplexity is 49.855591148492955
At time: 433.2254078388214 and batch: 850, loss is 3.7806288051605224 and perplexity is 43.84360215288917
At time: 433.73054599761963 and batch: 900, loss is 3.744339871406555 and perplexity is 42.281087049877215
At time: 434.2367334365845 and batch: 950, loss is 3.7368751859664915 and perplexity is 41.96664709275901
At time: 434.7358570098877 and batch: 1000, loss is 3.6799700689315795 and perplexity is 39.64520743139779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.574256710889863 and perplexity of 96.95594604838786
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 436.25558710098267 and batch: 50, loss is 3.9056039524078368 and perplexity is 49.68007525445723
At time: 436.7746503353119 and batch: 100, loss is 3.7972307586669922 and perplexity is 44.57756736739335
At time: 437.2750234603882 and batch: 150, loss is 3.8568523025512693 and perplexity is 47.316179700253066
At time: 437.7899489402771 and batch: 200, loss is 3.912930040359497 and perplexity is 50.04537232059669
At time: 438.29420828819275 and batch: 250, loss is 3.92489164352417 and perplexity is 50.647589767161584
At time: 438.82718992233276 and batch: 300, loss is 3.7720124769210814 and perplexity is 43.467454120133326
At time: 439.34058713912964 and batch: 350, loss is 3.8045721578598024 and perplexity is 44.90603330902857
At time: 439.8448746204376 and batch: 400, loss is 3.8103810024261473 and perplexity is 45.167644570567795
At time: 440.3480587005615 and batch: 450, loss is 3.86313364982605 and perplexity is 47.61432445174491
At time: 440.8464436531067 and batch: 500, loss is 3.9200512027740477 and perplexity is 50.40302548672052
At time: 441.3615610599518 and batch: 550, loss is 3.80505446434021 and perplexity is 44.927697003753146
At time: 441.8683638572693 and batch: 600, loss is 3.7507545375823974 and perplexity is 42.55317786172021
At time: 442.3774878978729 and batch: 650, loss is 3.725191206932068 and perplexity is 41.479163090157314
At time: 442.8835189342499 and batch: 700, loss is 3.832617621421814 and perplexity is 46.18327046774236
At time: 443.3922414779663 and batch: 750, loss is 3.7826450634002686 and perplexity is 43.932091355554185
At time: 443.8970830440521 and batch: 800, loss is 3.903493323326111 and perplexity is 49.575329621317266
At time: 444.4022750854492 and batch: 850, loss is 3.774737777709961 and perplexity is 43.58607757601582
At time: 444.91075682640076 and batch: 900, loss is 3.7376990365982055 and perplexity is 42.001235587397524
At time: 445.42172408103943 and batch: 950, loss is 3.7307959365844727 and perplexity is 41.71229529669971
At time: 445.93895649909973 and batch: 1000, loss is 3.67348352432251 and perplexity is 39.388879265654616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5718298190977515 and perplexity of 96.720929753635
Finished 40 epochs...
Completing Train Step...
At time: 447.42644739151 and batch: 50, loss is 3.90342306137085 and perplexity is 49.57184648409281
At time: 447.9322991371155 and batch: 100, loss is 3.7944152975082397 and perplexity is 44.4522374714348
At time: 448.43985962867737 and batch: 150, loss is 3.854294304847717 and perplexity is 47.19529969254339
At time: 448.94713973999023 and batch: 200, loss is 3.910575590133667 and perplexity is 49.92768158529146
At time: 449.4490044116974 and batch: 250, loss is 3.922520332336426 and perplexity is 50.52763085707912
At time: 449.9472472667694 and batch: 300, loss is 3.769884819984436 and perplexity is 43.37506860704704
At time: 450.45153164863586 and batch: 350, loss is 3.8026476812362673 and perplexity is 44.81969580157378
At time: 450.94657158851624 and batch: 400, loss is 3.8087207889556884 and perplexity is 45.09271885217619
At time: 451.4627151489258 and batch: 450, loss is 3.861590166091919 and perplexity is 47.54088920407615
At time: 451.97114658355713 and batch: 500, loss is 3.9187714767456057 and perplexity is 50.3385646779776
At time: 452.47604274749756 and batch: 550, loss is 3.803960952758789 and perplexity is 44.87859489850624
At time: 452.9741997718811 and batch: 600, loss is 3.749897723197937 and perplexity is 42.5167333021648
At time: 453.4811587333679 and batch: 650, loss is 3.7246920680999756 and perplexity is 41.45846439532723
At time: 453.9790823459625 and batch: 700, loss is 3.832431073188782 and perplexity is 46.17465586378551
At time: 454.50236773490906 and batch: 750, loss is 3.7826449918746947 and perplexity is 43.93208821328625
At time: 455.0103771686554 and batch: 800, loss is 3.903328471183777 and perplexity is 49.56715769562052
At time: 455.5082018375397 and batch: 850, loss is 3.7747619676589967 and perplexity is 43.58713193376343
At time: 456.01602697372437 and batch: 900, loss is 3.7378186130523683 and perplexity is 42.00625824650942
At time: 456.5305371284485 and batch: 950, loss is 3.7310753774642946 and perplexity is 41.7239530459468
At time: 457.0332498550415 and batch: 1000, loss is 3.673818302154541 and perplexity is 39.40206799678554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.571737149866616 and perplexity of 96.71196711472683
Finished 41 epochs...
Completing Train Step...
At time: 458.51929545402527 and batch: 50, loss is 3.9021211290359497 and perplexity is 49.50734928885684
At time: 459.04355907440186 and batch: 100, loss is 3.7931786346435548 and perplexity is 44.39729901727638
At time: 459.5448818206787 and batch: 150, loss is 3.8532647132873534 and perplexity is 47.14673281660421
At time: 460.04296946525574 and batch: 200, loss is 3.9095507049560547 and perplexity is 49.87653765728577
At time: 460.5476498603821 and batch: 250, loss is 3.921376709938049 and perplexity is 50.46987935594347
At time: 461.05947065353394 and batch: 300, loss is 3.7687400007247924 and perplexity is 43.32544040619768
At time: 461.5617299079895 and batch: 350, loss is 3.801523962020874 and perplexity is 44.7693593354974
At time: 462.0702860355377 and batch: 400, loss is 3.8077795457839967 and perplexity is 45.050295606890565
At time: 462.56883692741394 and batch: 450, loss is 3.8607686805725097 and perplexity is 47.50185108883372
At time: 463.0827798843384 and batch: 500, loss is 3.918120036125183 and perplexity is 50.30578277106479
At time: 463.5900375843048 and batch: 550, loss is 3.8032024335861205 and perplexity is 44.84456653104939
At time: 464.0893590450287 and batch: 600, loss is 3.749280104637146 and perplexity is 42.490482285925076
At time: 464.61631441116333 and batch: 650, loss is 3.724180345535278 and perplexity is 41.437254590829056
At time: 465.1193573474884 and batch: 700, loss is 3.8321843433380125 and perplexity is 46.16326460317472
At time: 465.6276080608368 and batch: 750, loss is 3.7824360275268556 and perplexity is 43.92290893222819
At time: 466.1261215209961 and batch: 800, loss is 3.9030128145217895 and perplexity is 49.551513961232494
At time: 466.6395468711853 and batch: 850, loss is 3.7745117568969726 and perplexity is 43.57622732854952
At time: 467.14742136001587 and batch: 900, loss is 3.7376157093048095 and perplexity is 41.99773588392917
At time: 467.65701961517334 and batch: 950, loss is 3.730991463661194 and perplexity is 41.720451977262364
At time: 468.15968346595764 and batch: 1000, loss is 3.6737102127075194 and perplexity is 39.39780927920961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.571715564262576 and perplexity of 96.70987955102957
Finished 42 epochs...
Completing Train Step...
At time: 469.66274309158325 and batch: 50, loss is 3.9009692811965944 and perplexity is 49.45035718496332
At time: 470.18235206604004 and batch: 100, loss is 3.792167835235596 and perplexity is 44.352444926777814
At time: 470.68223214149475 and batch: 150, loss is 3.8524681377410888 and perplexity is 47.10919183625477
At time: 471.1939127445221 and batch: 200, loss is 3.9087468814849853 and perplexity is 49.83646183476241
At time: 471.70458364486694 and batch: 250, loss is 3.9204647874832155 and perplexity is 50.423875718728986
At time: 472.2256600856781 and batch: 300, loss is 3.7677876949310303 and perplexity is 43.28420097767154
At time: 472.7292158603668 and batch: 350, loss is 3.800574498176575 and perplexity is 44.72687262046754
At time: 473.2437949180603 and batch: 400, loss is 3.8069979906082154 and perplexity is 45.015100070606834
At time: 473.7483100891113 and batch: 450, loss is 3.860081796646118 and perplexity is 47.46923403419465
At time: 474.24806809425354 and batch: 500, loss is 3.917569808959961 and perplexity is 50.278110776456124
At time: 474.75783252716064 and batch: 550, loss is 3.802500443458557 and perplexity is 44.813097134964735
At time: 475.2693712711334 and batch: 600, loss is 3.748687992095947 and perplexity is 42.465330585535455
At time: 475.7792603969574 and batch: 650, loss is 3.723652286529541 and perplexity is 41.4153790516654
At time: 476.2927324771881 and batch: 700, loss is 3.8318902254104614 and perplexity is 46.1496891559497
At time: 476.797465801239 and batch: 750, loss is 3.7821551847457884 and perplexity is 43.91057523232746
At time: 477.3126587867737 and batch: 800, loss is 3.9026361227035524 and perplexity is 49.532851826499446
At time: 477.8225288391113 and batch: 850, loss is 3.7741765832901 and perplexity is 43.561624174694494
At time: 478.3267812728882 and batch: 900, loss is 3.737305359840393 and perplexity is 41.98470393142523
At time: 478.83302116394043 and batch: 950, loss is 3.7307872915267946 and perplexity is 41.7119346930597
At time: 479.3448135852814 and batch: 1000, loss is 3.6734666061401366 and perplexity is 39.38821288304872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.571709609613186 and perplexity of 96.70930367931882
Finished 43 epochs...
Completing Train Step...
At time: 480.8376190662384 and batch: 50, loss is 3.8999053049087524 and perplexity is 49.39777115759712
At time: 481.338086605072 and batch: 100, loss is 3.791257996559143 and perplexity is 44.31210970904122
At time: 481.84165930747986 and batch: 150, loss is 3.851758532524109 and perplexity is 47.07577476582606
At time: 482.3479163646698 and batch: 200, loss is 3.9080275869369507 and perplexity is 49.80062762869087
At time: 482.85272574424744 and batch: 250, loss is 3.9196464157104494 and perplexity is 50.38262712281272
At time: 483.34875655174255 and batch: 300, loss is 3.7669115114212035 and perplexity is 43.24629268427483
At time: 483.8564205169678 and batch: 350, loss is 3.799703040122986 and perplexity is 44.68791200584696
At time: 484.36127829551697 and batch: 400, loss is 3.806282806396484 and perplexity is 44.982917491354875
At time: 484.8598310947418 and batch: 450, loss is 3.859443926811218 and perplexity is 47.438964496757194
At time: 485.36523246765137 and batch: 500, loss is 3.9170507907867433 and perplexity is 50.252022294031974
At time: 485.86856269836426 and batch: 550, loss is 3.801822247505188 and perplexity is 44.782715377386886
At time: 486.37135338783264 and batch: 600, loss is 3.7481027746200564 and perplexity is 42.440486402291484
At time: 486.89148592948914 and batch: 650, loss is 3.7231141901016236 and perplexity is 41.39309957892678
At time: 487.39719319343567 and batch: 700, loss is 3.8315665483474732 and perplexity is 46.134753977323655
At time: 487.9002330303192 and batch: 750, loss is 3.781840090751648 and perplexity is 43.89674145337729
At time: 488.4038918018341 and batch: 800, loss is 3.902230324745178 and perplexity is 49.51275557414066
At time: 488.9084222316742 and batch: 850, loss is 3.773801493644714 and perplexity is 43.54528772453856
At time: 489.41157937049866 and batch: 900, loss is 3.736945481300354 and perplexity is 41.96959725591763
At time: 489.9211015701294 and batch: 950, loss is 3.7305253028869627 and perplexity is 41.70100807141258
At time: 490.42825150489807 and batch: 1000, loss is 3.6731607151031493 and perplexity is 39.37616622434125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.571704399294969 and perplexity of 96.70879979438482
Finished 44 epochs...
Completing Train Step...
At time: 491.9041042327881 and batch: 50, loss is 3.8989006090164184 and perplexity is 49.34816634287407
At time: 492.4320697784424 and batch: 100, loss is 3.790407462120056 and perplexity is 44.27443675700773
At time: 492.9323728084564 and batch: 150, loss is 3.8510917139053347 and perplexity is 47.04439422644571
At time: 493.42905139923096 and batch: 200, loss is 3.9073539638519286 and perplexity is 49.76709207270205
At time: 493.925906419754 and batch: 250, loss is 3.9188798475265503 and perplexity is 50.34402020314781
At time: 494.4293873310089 and batch: 300, loss is 3.7660779666900637 and perplexity is 43.210259984386376
At time: 494.9349021911621 and batch: 350, loss is 3.7988755083084107 and perplexity is 44.6509466340553
At time: 495.43274092674255 and batch: 400, loss is 3.805605955123901 and perplexity is 44.95248104804103
At time: 495.93096446990967 and batch: 450, loss is 3.8588294506073 and perplexity is 47.409823336127225
At time: 496.4444274902344 and batch: 500, loss is 3.916543974876404 and perplexity is 50.22656022244314
At time: 496.9501938819885 and batch: 550, loss is 3.801158947944641 and perplexity is 44.753020871222134
At time: 497.4612195491791 and batch: 600, loss is 3.7475215768814087 and perplexity is 42.415827254182055
At time: 497.9665229320526 and batch: 650, loss is 3.7225724124908446 and perplexity is 41.3706797981502
At time: 498.4805383682251 and batch: 700, loss is 3.8312231063842774 and perplexity is 46.11891208738674
At time: 498.9875681400299 and batch: 750, loss is 3.7815041160583496 and perplexity is 43.88199573636327
At time: 499.4910469055176 and batch: 800, loss is 3.901809368133545 and perplexity is 49.49191723864682
At time: 500.0020899772644 and batch: 850, loss is 3.77340500831604 and perplexity is 43.52802607904361
At time: 500.51079392433167 and batch: 900, loss is 3.736556649208069 and perplexity is 41.95328130189308
At time: 501.01831912994385 and batch: 950, loss is 3.7302283954620363 and perplexity is 41.68862857036309
At time: 501.5236220359802 and batch: 1000, loss is 3.67281952381134 and perplexity is 39.36273371097927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5717051436261436 and perplexity of 96.70887177778614
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 503.09139752388 and batch: 50, loss is 3.898386869430542 and perplexity is 49.32282074740864
At time: 503.6180498600006 and batch: 100, loss is 3.7907005453109743 and perplexity is 44.28741475193176
At time: 504.1155936717987 and batch: 150, loss is 3.8513857173919677 and perplexity is 47.05822747578695
At time: 504.6194896697998 and batch: 200, loss is 3.9081590032577513 and perplexity is 49.80717267400086
At time: 505.1130561828613 and batch: 250, loss is 3.9192965173721315 and perplexity is 50.365001409086155
At time: 505.6100969314575 and batch: 300, loss is 3.765167908668518 and perplexity is 43.17095402873742
At time: 506.1081573963165 and batch: 350, loss is 3.7982420349121093 and perplexity is 44.62267040430818
At time: 506.6130199432373 and batch: 400, loss is 3.8039617776870727 and perplexity is 44.87863192014377
At time: 507.1346068382263 and batch: 450, loss is 3.857734718322754 and perplexity is 47.357950670437965
At time: 507.6498157978058 and batch: 500, loss is 3.9151974058151247 and perplexity is 50.158972206571484
At time: 508.1530213356018 and batch: 550, loss is 3.8003637266159056 and perplexity is 44.717446461139524
At time: 508.65211153030396 and batch: 600, loss is 3.7453908348083496 and perplexity is 42.32554628337617
At time: 509.16565561294556 and batch: 650, loss is 3.7196277379989624 and perplexity is 41.249035801522744
At time: 509.6841998100281 and batch: 700, loss is 3.828755679130554 and perplexity is 46.005257301910575
At time: 510.19062972068787 and batch: 750, loss is 3.7784926795959475 and perplexity is 43.75004667218907
At time: 510.6883862018585 and batch: 800, loss is 3.898847508430481 and perplexity is 49.345545995897915
At time: 511.1890859603882 and batch: 850, loss is 3.7699035930633547 and perplexity is 43.37588289827645
At time: 511.6970057487488 and batch: 900, loss is 3.732823357582092 and perplexity is 41.796949465866064
At time: 512.2020611763 and batch: 950, loss is 3.7268405103683473 and perplexity is 41.54763126325187
At time: 512.7080693244934 and batch: 1000, loss is 3.6691103267669676 and perplexity is 39.21700002001335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.570175915229611 and perplexity of 96.56109484600566
Finished 46 epochs...
Completing Train Step...
At time: 514.2066552639008 and batch: 50, loss is 3.8973285245895384 and perplexity is 49.270647807877445
At time: 514.698215007782 and batch: 100, loss is 3.789393734931946 and perplexity is 44.22957729819703
At time: 515.2133700847626 and batch: 150, loss is 3.8501780366897584 and perplexity is 47.00143046581388
At time: 515.7160365581512 and batch: 200, loss is 3.906780734062195 and perplexity is 49.73857226795935
At time: 516.2379417419434 and batch: 250, loss is 3.9179519414901733 and perplexity is 50.29732734954643
At time: 516.7363474369049 and batch: 300, loss is 3.7639179468154906 and perplexity is 43.11702569424464
At time: 517.2352383136749 and batch: 350, loss is 3.797234983444214 and perplexity is 44.5777556980824
At time: 517.7532570362091 and batch: 400, loss is 3.803095664978027 and perplexity is 44.83977879469484
At time: 518.2701933383942 and batch: 450, loss is 3.856886987686157 and perplexity is 47.31782089679074
At time: 518.7896218299866 and batch: 500, loss is 3.9145361757278443 and perplexity is 50.125816547968995
At time: 519.2972061634064 and batch: 550, loss is 3.7998323535919187 and perplexity is 44.69369112841895
At time: 519.8157606124878 and batch: 600, loss is 3.7448936748504638 and perplexity is 42.30450894646251
At time: 520.3193876743317 and batch: 650, loss is 3.719382781982422 and perplexity is 41.23893283946786
At time: 520.833160161972 and batch: 700, loss is 3.8286939096450805 and perplexity is 46.00241566860197
At time: 521.3444035053253 and batch: 750, loss is 3.7785532808303834 and perplexity is 43.7526980593619
At time: 521.8464250564575 and batch: 800, loss is 3.8988033390045165 and perplexity is 49.34336647959173
At time: 522.3579502105713 and batch: 850, loss is 3.7700047874450684 and perplexity is 43.38027251602621
At time: 522.8670620918274 and batch: 900, loss is 3.7329783391952516 and perplexity is 41.803427726512105
At time: 523.3820910453796 and batch: 950, loss is 3.7271317529678343 and perplexity is 41.559733465636405
At time: 523.9037797451019 and batch: 1000, loss is 3.6694152641296385 and perplexity is 39.22896057208825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.570067615043826 and perplexity of 96.550637827753
Finished 47 epochs...
Completing Train Step...
At time: 525.4143092632294 and batch: 50, loss is 3.8966901779174803 and perplexity is 49.23920609024472
At time: 525.9375743865967 and batch: 100, loss is 3.788789324760437 and perplexity is 44.20285256895913
At time: 526.4357573986053 and batch: 150, loss is 3.8495708990097044 and perplexity is 46.97290278735222
At time: 526.932783126831 and batch: 200, loss is 3.90618191242218 and perplexity is 49.70879665057363
At time: 527.4537589550018 and batch: 250, loss is 3.9172765684127806 and perplexity is 50.26336935723739
At time: 527.9678988456726 and batch: 300, loss is 3.763210792541504 and perplexity is 43.08654608340705
At time: 528.4780809879303 and batch: 350, loss is 3.7966525506973268 and perplexity is 44.55179971292271
At time: 528.9938397407532 and batch: 400, loss is 3.8025630426406862 and perplexity is 44.81590248599948
At time: 529.5092585086823 and batch: 450, loss is 3.856408715248108 and perplexity is 47.295195498210816
At time: 530.0307767391205 and batch: 500, loss is 3.9141420793533324 and perplexity is 50.10606603745591
At time: 530.5495629310608 and batch: 550, loss is 3.7995169305801393 and perplexity is 44.679595932847064
At time: 531.0530815124512 and batch: 600, loss is 3.744537582397461 and perplexity is 42.289447311925045
At time: 531.5511131286621 and batch: 650, loss is 3.719134850502014 and perplexity is 41.22870967717278
At time: 532.0702769756317 and batch: 700, loss is 3.828586072921753 and perplexity is 45.99745518629701
At time: 532.5656449794769 and batch: 750, loss is 3.778486351966858 and perplexity is 43.749769838996926
At time: 533.0778431892395 and batch: 800, loss is 3.898679313659668 and perplexity is 49.33724703103931
At time: 533.5954127311707 and batch: 850, loss is 3.769927830696106 and perplexity is 43.37693423973737
At time: 534.0988566875458 and batch: 900, loss is 3.7329368877410887 and perplexity is 41.80169494955715
At time: 534.5996053218842 and batch: 950, loss is 3.727181510925293 and perplexity is 41.56180144453496
At time: 535.1040458679199 and batch: 1000, loss is 3.66943244934082 and perplexity is 39.22963473585292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.570037841796875 and perplexity of 96.54776324456269
Finished 48 epochs...
Completing Train Step...
At time: 536.6302089691162 and batch: 50, loss is 3.896127905845642 and perplexity is 49.2115280418467
At time: 537.1354956626892 and batch: 100, loss is 3.788315186500549 and perplexity is 44.181899273132146
At time: 537.6285507678986 and batch: 150, loss is 3.84910701751709 and perplexity is 46.951117980269856
At time: 538.1246540546417 and batch: 200, loss is 3.9057381772994995 and perplexity is 49.686744004722186
At time: 538.6301543712616 and batch: 250, loss is 3.9167508840560914 and perplexity is 50.23695363402634
At time: 539.1270561218262 and batch: 300, loss is 3.7626414918899536 and perplexity is 43.06202386556832
At time: 539.6336741447449 and batch: 350, loss is 3.79617253780365 and perplexity is 44.530419406445915
At time: 540.1427009105682 and batch: 400, loss is 3.8021177864074707 and perplexity is 44.79595236785723
At time: 540.6534683704376 and batch: 450, loss is 3.8560133504867555 and perplexity is 47.27650034047636
At time: 541.1684055328369 and batch: 500, loss is 3.9138177633285522 and perplexity is 50.08981847211658
At time: 541.6741893291473 and batch: 550, loss is 3.7992280530929565 and perplexity is 44.6666908675268
At time: 542.1940183639526 and batch: 600, loss is 3.744204740524292 and perplexity is 42.275373955297574
At time: 542.6985383033752 and batch: 650, loss is 3.718871555328369 and perplexity is 41.21785578585047
At time: 543.2032034397125 and batch: 700, loss is 3.828447937965393 and perplexity is 45.99110176865681
At time: 543.7003426551819 and batch: 750, loss is 3.7783651161193847 and perplexity is 43.744466120080666
At time: 544.2033686637878 and batch: 800, loss is 3.898514275550842 and perplexity is 49.32910517697122
At time: 544.7001304626465 and batch: 850, loss is 3.7697830057144164 and perplexity is 43.37065263090827
At time: 545.2120971679688 and batch: 900, loss is 3.732822937965393 and perplexity is 41.79693192717178
At time: 545.7178435325623 and batch: 950, loss is 3.727150549888611 and perplexity is 41.56051466799594
At time: 546.2283706665039 and batch: 1000, loss is 3.669359164237976 and perplexity is 39.22675989337962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.57002407167016 and perplexity of 96.5464337787822
Finished 49 epochs...
Completing Train Step...
At time: 547.7580120563507 and batch: 50, loss is 3.8956052446365357 and perplexity is 49.18581380560065
At time: 548.255490064621 and batch: 100, loss is 3.7878933906555177 and perplexity is 44.16326746127979
At time: 548.7667663097382 and batch: 150, loss is 3.8487032985687257 and perplexity is 46.932166750037105
At time: 549.2652184963226 and batch: 200, loss is 3.905353755950928 and perplexity is 49.66764703046324
At time: 549.7749593257904 and batch: 250, loss is 3.9162869167327883 and perplexity is 50.213650735427635
At time: 550.2777509689331 and batch: 300, loss is 3.7621291399002077 and perplexity is 43.03996660298201
At time: 550.779205083847 and batch: 350, loss is 3.7957353925704957 and perplexity is 44.510957400043615
At time: 551.2795889377594 and batch: 400, loss is 3.801711893081665 and perplexity is 44.77777367932177
At time: 551.8025832176208 and batch: 450, loss is 3.8556520891189576 and perplexity is 47.25942425194993
At time: 552.3202021121979 and batch: 500, loss is 3.9135202407836913 and perplexity is 50.07491783860026
At time: 552.8263757228851 and batch: 550, loss is 3.7989461326599123 and perplexity is 44.65410018956286
At time: 553.337188243866 and batch: 600, loss is 3.7438770008087157 and perplexity is 42.26152090648272
At time: 553.8383955955505 and batch: 650, loss is 3.718598608970642 and perplexity is 41.206607057459955
At time: 554.3522894382477 and batch: 700, loss is 3.8282910919189455 and perplexity is 45.9838888118493
At time: 554.8683376312256 and batch: 750, loss is 3.778216848373413 and perplexity is 43.737980707490934
At time: 555.3750081062317 and batch: 800, loss is 3.898327088356018 and perplexity is 49.31987226431843
At time: 555.9031445980072 and batch: 850, loss is 3.7696065711975097 and perplexity is 43.36300122576924
At time: 556.4346318244934 and batch: 900, loss is 3.7326746368408203 and perplexity is 41.7907338547652
At time: 556.9586775302887 and batch: 950, loss is 3.7270811367034913 and perplexity is 41.557629920418535
At time: 557.4763207435608 and batch: 1000, loss is 3.669245066642761 and perplexity is 39.222284469730106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5700158840272485 and perplexity of 96.54564329429418
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fde042f08d0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'lr': 4.567992145082377, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.372063828199916, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 7.916763378916041, 'seq_len': 50}, 'best_accuracy': -73.57505021952048}, {'params': {'lr': 5.313079103827589, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.6165012871400407, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 5.421382921095802, 'seq_len': 50}, 'best_accuracy': -74.79774233685909}, {'params': {'lr': 21.122041927612262, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.38984190782347494, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 2.0761984272186647, 'seq_len': 50}, 'best_accuracy': -165.0102586773213}, {'params': {'lr': 1.6434782813015458, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.7410780142434373, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 7.956156059900165, 'seq_len': 50}, 'best_accuracy': -75.89255082881164}, {'params': {'lr': 26.71583680783388, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.7543925472838893, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 3.1137667707572545, 'seq_len': 50}, 'best_accuracy': -271.3684968234542}, {'params': {'lr': 11.339984217533289, 'wordvec_dim': 200, 'tune_wordvecs': True, 'dropout': 0.0, 'data': 'ptb', 'batch_size': 20, 'num_layers': 1, 'wordvec_source': 'glove', 'anneal': 2.0, 'seq_len': 50}, 'best_accuracy': -96.54564329429418}]
