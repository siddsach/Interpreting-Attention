Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.8204744866035563, 'wordvec_source': 'glove', 'anneal': 3.6633097241630748, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 11.519108002862913}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5134036540985107 and batch: 50, loss is 6.637283716201782 and perplexity is 763.0195978436027
At time: 2.470567464828491 and batch: 100, loss is 6.019441499710083 and perplexity is 411.3487931400076
At time: 3.414534330368042 and batch: 150, loss is 5.942613687515259 and perplexity is 380.9292595880528
At time: 4.36138916015625 and batch: 200, loss is 5.92120403289795 and perplexity is 372.8603798813838
At time: 5.303483724594116 and batch: 250, loss is 5.930986032485962 and perplexity is 376.5255973152343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.096115493774414 and perplexity of 444.12919230806676
Finished 1 epochs...
Completing Train Step...
At time: 6.887282371520996 and batch: 50, loss is 5.612026948928833 and perplexity is 273.698448914253
At time: 7.824733734130859 and batch: 100, loss is 5.368285531997681 and perplexity is 214.49480779639362
At time: 8.753679752349854 and batch: 150, loss is 5.2922602558135985 and perplexity is 198.79223931579898
At time: 9.682793140411377 and batch: 200, loss is 5.232079401016235 and perplexity is 187.18162478319576
At time: 10.61095118522644 and batch: 250, loss is 5.231039381027221 and perplexity is 186.98705334847392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4145252227783205 and perplexity of 224.64586361652354
Finished 2 epochs...
Completing Train Step...
At time: 12.184362173080444 and batch: 50, loss is 5.164167232513428 and perplexity is 174.8917537449086
At time: 13.11599326133728 and batch: 100, loss is 5.0854098510742185 and perplexity is 161.64617652205254
At time: 14.045442819595337 and batch: 150, loss is 5.089838619232178 and perplexity is 162.36365756737138
At time: 14.972793340682983 and batch: 200, loss is 5.050239772796631 and perplexity is 156.05987891434387
At time: 15.903642416000366 and batch: 250, loss is 5.084091663360596 and perplexity is 161.4332368962403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.324940490722656 and perplexity of 205.39613712539614
Finished 3 epochs...
Completing Train Step...
At time: 17.44426441192627 and batch: 50, loss is 5.0499994087219235 and perplexity is 156.0223722337596
At time: 18.39182686805725 and batch: 100, loss is 4.98147590637207 and perplexity is 145.6892467418386
At time: 19.32552409172058 and batch: 150, loss is 4.9953734493255615 and perplexity is 147.72810404417115
At time: 20.262511014938354 and batch: 200, loss is 4.972189989089966 and perplexity is 144.34265031542407
At time: 21.19931650161743 and batch: 250, loss is 5.024765911102295 and perplexity is 152.13463891411072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.287139892578125 and perplexity of 197.77695237930237
Finished 4 epochs...
Completing Train Step...
At time: 22.76430320739746 and batch: 50, loss is 4.977580146789551 and perplexity is 145.12278058718428
At time: 23.720218896865845 and batch: 100, loss is 4.901963701248169 and perplexity is 134.55374378140885
At time: 24.662072896957397 and batch: 150, loss is 4.93226809501648 and perplexity is 138.693726421278
At time: 25.59429669380188 and batch: 200, loss is 4.910999698638916 and perplexity is 135.7750807478614
At time: 26.524914979934692 and batch: 250, loss is 4.950965032577515 and perplexity is 141.311268119208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.25887451171875 and perplexity of 192.26497749915953
Finished 5 epochs...
Completing Train Step...
At time: 28.086193799972534 and batch: 50, loss is 4.916121292114258 and perplexity is 136.47224929854843
At time: 29.018102645874023 and batch: 100, loss is 4.849820032119751 and perplexity is 127.71740274737779
At time: 29.95273518562317 and batch: 150, loss is 4.883544998168945 and perplexity is 132.09812242857527
At time: 30.88278079032898 and batch: 200, loss is 4.8672872734069825 and perplexity is 129.9678709546269
At time: 31.820183038711548 and batch: 250, loss is 4.907764310836792 and perplexity is 135.33650557074054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.255198287963867 and perplexity of 191.55946602522488
Finished 6 epochs...
Completing Train Step...
At time: 33.38326454162598 and batch: 50, loss is 4.8743619537353515 and perplexity is 130.89061229591545
At time: 34.331135272979736 and batch: 100, loss is 4.810767869949341 and perplexity is 122.82589563022326
At time: 35.262470722198486 and batch: 150, loss is 4.842028751373291 and perplexity is 126.72618704051987
At time: 36.19570517539978 and batch: 200, loss is 4.830228567123413 and perplexity is 125.23958303480478
At time: 37.12818765640259 and batch: 250, loss is 4.877060861587524 and perplexity is 131.24435113676165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.218729782104492 and perplexity of 184.6994264887907
Finished 7 epochs...
Completing Train Step...
At time: 38.67474412918091 and batch: 50, loss is 4.83285966873169 and perplexity is 125.56953498134975
At time: 39.622886657714844 and batch: 100, loss is 4.77175009727478 and perplexity is 118.12579279397606
At time: 40.563472747802734 and batch: 150, loss is 4.8072997665405275 and perplexity is 122.400660528768
At time: 41.49856233596802 and batch: 200, loss is 4.804810028076172 and perplexity is 122.09629395000181
At time: 42.43238115310669 and batch: 250, loss is 4.830428123474121 and perplexity is 125.26457788281678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.227934646606445 and perplexity of 186.40740849498545
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 44.005783796310425 and batch: 50, loss is 4.712531986236573 and perplexity is 111.33369872191491
At time: 44.946038484573364 and batch: 100, loss is 4.530415802001953 and perplexity is 92.79713829719383
At time: 45.884581565856934 and batch: 150, loss is 4.510379590988159 and perplexity is 90.95633816472353
At time: 46.81771802902222 and batch: 200, loss is 4.481989622116089 and perplexity is 88.41040109041322
At time: 47.749128341674805 and batch: 250, loss is 4.565320062637329 and perplexity is 96.09334498457368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.031028366088867 and perplexity of 153.09036471361168
Finished 9 epochs...
Completing Train Step...
At time: 49.36659383773804 and batch: 50, loss is 4.545283393859863 and perplexity is 94.18711547778715
At time: 50.298747062683105 and batch: 100, loss is 4.437433013916015 and perplexity is 84.55760464818455
At time: 51.23343372344971 and batch: 150, loss is 4.479113063812256 and perplexity is 88.1564488463421
At time: 52.16436314582825 and batch: 200, loss is 4.485137071609497 and perplexity is 88.68910673843092
At time: 53.096473932266235 and batch: 250, loss is 4.539373855590821 and perplexity is 93.6321545110136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.018676376342773 and perplexity of 151.2110247866359
Finished 10 epochs...
Completing Train Step...
At time: 54.665592432022095 and batch: 50, loss is 4.506039590835571 and perplexity is 90.56244301398772
At time: 55.62395453453064 and batch: 100, loss is 4.419960813522339 and perplexity is 83.09302917141352
At time: 56.558359146118164 and batch: 150, loss is 4.462398080825806 and perplexity is 86.69516201829626
At time: 57.50570583343506 and batch: 200, loss is 4.461160287857056 and perplexity is 86.58791774312294
At time: 58.43590211868286 and batch: 250, loss is 4.518291835784912 and perplexity is 91.67886159986635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.008792114257813 and perplexity of 149.72377766615298
Finished 11 epochs...
Completing Train Step...
At time: 60.01473641395569 and batch: 50, loss is 4.482851247787476 and perplexity is 88.4866105889257
At time: 60.94637632369995 and batch: 100, loss is 4.3941566753387455 and perplexity is 80.9763126381716
At time: 61.883554458618164 and batch: 150, loss is 4.444760150909424 and perplexity is 85.17944517458186
At time: 62.815590381622314 and batch: 200, loss is 4.445401134490967 and perplexity is 85.23406130257442
At time: 63.744691133499146 and batch: 250, loss is 4.498910036087036 and perplexity is 89.91906932756989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.012339782714844 and perplexity of 150.2558913127503
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 65.29608988761902 and batch: 50, loss is 4.446246471405029 and perplexity is 85.30614326340762
At time: 66.24533772468567 and batch: 100, loss is 4.3196229648590085 and perplexity is 75.16028488052089
At time: 67.18122887611389 and batch: 150, loss is 4.346354541778564 and perplexity is 77.19653262040245
At time: 68.11398267745972 and batch: 200, loss is 4.340432214736938 and perplexity is 76.74070063311581
At time: 69.04285979270935 and batch: 250, loss is 4.420437173843384 and perplexity is 83.13262082266338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.948028945922852 and perplexity of 140.8969744893503
Finished 13 epochs...
Completing Train Step...
At time: 70.61209797859192 and batch: 50, loss is 4.4002663516998295 and perplexity is 81.47256613173992
At time: 71.55816411972046 and batch: 100, loss is 4.291237678527832 and perplexity is 73.05683344078474
At time: 72.49468517303467 and batch: 150, loss is 4.3384005641937256 and perplexity is 76.58494861739523
At time: 73.42925548553467 and batch: 200, loss is 4.345017709732056 and perplexity is 77.09340277071895
At time: 74.3632824420929 and batch: 250, loss is 4.411463375091553 and perplexity is 82.38994271953034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.940947341918945 and perplexity of 139.90272251511692
Finished 14 epochs...
Completing Train Step...
At time: 75.92727541923523 and batch: 50, loss is 4.384521541595459 and perplexity is 80.19984174333992
At time: 76.86629748344421 and batch: 100, loss is 4.2848389625549315 and perplexity is 72.5908559324777
At time: 77.80848550796509 and batch: 150, loss is 4.335586099624634 and perplexity is 76.36970603138373
At time: 78.74621176719666 and batch: 200, loss is 4.338430404663086 and perplexity is 76.58723398230592
At time: 79.68383812904358 and batch: 250, loss is 4.403736143112183 and perplexity is 81.75574995232583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.939682388305664 and perplexity of 139.72586394329107
Finished 15 epochs...
Completing Train Step...
At time: 81.25145483016968 and batch: 50, loss is 4.3749775695800786 and perplexity is 79.43805770328557
At time: 82.20323872566223 and batch: 100, loss is 4.277076225280762 and perplexity is 72.02953370662401
At time: 83.13925981521606 and batch: 150, loss is 4.3306668090820315 and perplexity is 75.99494379667274
At time: 84.07633233070374 and batch: 200, loss is 4.333932647705078 and perplexity is 76.24353673053109
At time: 85.01368045806885 and batch: 250, loss is 4.3951757049560545 and perplexity is 81.05887195710169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.937200927734375 and perplexity of 139.37956955675614
Finished 16 epochs...
Completing Train Step...
At time: 86.59024453163147 and batch: 50, loss is 4.366101160049438 and perplexity is 78.73605321971301
At time: 87.52656650543213 and batch: 100, loss is 4.2708945560455325 and perplexity is 71.5856443561816
At time: 88.45992946624756 and batch: 150, loss is 4.324747886657715 and perplexity is 75.5464641870312
At time: 89.3953046798706 and batch: 200, loss is 4.328181676864624 and perplexity is 75.80632078711865
At time: 90.33100628852844 and batch: 250, loss is 4.3882568740844725 and perplexity is 80.49997501763023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.935540390014649 and perplexity of 139.1483165794315
Finished 17 epochs...
Completing Train Step...
At time: 91.91201090812683 and batch: 50, loss is 4.358204946517945 and perplexity is 78.1167846867703
At time: 92.84610152244568 and batch: 100, loss is 4.264072675704956 and perplexity is 71.09895760312993
At time: 93.78246235847473 and batch: 150, loss is 4.318800258636474 and perplexity is 75.09847547543961
At time: 94.71688961982727 and batch: 200, loss is 4.322321844100952 and perplexity is 75.36340739147322
At time: 95.6507740020752 and batch: 250, loss is 4.381217212677002 and perplexity is 79.93527243973908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9339241027832035 and perplexity of 138.92359458863095
Finished 18 epochs...
Completing Train Step...
At time: 97.20833396911621 and batch: 50, loss is 4.350995845794678 and perplexity is 77.55565795753765
At time: 98.15490102767944 and batch: 100, loss is 4.257625608444214 and perplexity is 70.64205227358205
At time: 99.09069800376892 and batch: 150, loss is 4.312532157897949 and perplexity is 74.629222863546
At time: 100.03878164291382 and batch: 200, loss is 4.3166764545440675 and perplexity is 74.93915027353621
At time: 100.97697067260742 and batch: 250, loss is 4.374663324356079 and perplexity is 79.4130985948942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.930976867675781 and perplexity of 138.51475685980083
Finished 19 epochs...
Completing Train Step...
At time: 102.55957770347595 and batch: 50, loss is 4.343548059463501 and perplexity is 76.98018564580006
At time: 103.5015504360199 and batch: 100, loss is 4.2504521751403805 and perplexity is 70.13711943939646
At time: 104.43320059776306 and batch: 150, loss is 4.306290092468262 and perplexity is 74.16483325447554
At time: 105.3619384765625 and batch: 200, loss is 4.310586280822754 and perplexity is 74.48414476748428
At time: 106.34047985076904 and batch: 250, loss is 4.369375324249267 and perplexity is 78.99427047857591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.927772140502929 and perplexity of 138.0715653878359
Finished 20 epochs...
Completing Train Step...
At time: 107.91006255149841 and batch: 50, loss is 4.337286329269409 and perplexity is 76.4996625161687
At time: 108.86715388298035 and batch: 100, loss is 4.2438753223419186 and perplexity is 69.67735149943707
At time: 109.79993224143982 and batch: 150, loss is 4.300174884796142 and perplexity is 73.71268380010113
At time: 110.73339986801147 and batch: 200, loss is 4.303714103698731 and perplexity is 73.97403133422868
At time: 111.66888546943665 and batch: 250, loss is 4.362688779830933 and perplexity is 78.46783376287704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.926591491699218 and perplexity of 137.90864755263257
Finished 21 epochs...
Completing Train Step...
At time: 113.23344326019287 and batch: 50, loss is 4.330145330429077 and perplexity is 75.95532438698633
At time: 114.19479155540466 and batch: 100, loss is 4.237408208847046 and perplexity is 69.22819410065115
At time: 115.12744426727295 and batch: 150, loss is 4.294042682647705 and perplexity is 73.26204583582604
At time: 116.06005334854126 and batch: 200, loss is 4.297275657653809 and perplexity is 73.49928348396631
At time: 116.99243211746216 and batch: 250, loss is 4.356539134979248 and perplexity is 77.98676516954545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.924105072021485 and perplexity of 137.5661747196622
Finished 22 epochs...
Completing Train Step...
At time: 118.56025886535645 and batch: 50, loss is 4.323042659759522 and perplexity is 75.41775009878839
At time: 119.4942557811737 and batch: 100, loss is 4.23068736076355 and perplexity is 68.76448193997987
At time: 120.42726492881775 and batch: 150, loss is 4.288327808380127 and perplexity is 72.84455654096159
At time: 121.3579773902893 and batch: 200, loss is 4.291132164001465 and perplexity is 73.04912529027445
At time: 122.29386639595032 and batch: 250, loss is 4.349865617752076 and perplexity is 77.46805189479174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.921953582763672 and perplexity of 137.27052073474698
Finished 23 epochs...
Completing Train Step...
At time: 123.86041903495789 and batch: 50, loss is 4.316492509841919 and perplexity is 74.92536688158972
At time: 124.80647563934326 and batch: 100, loss is 4.224348707199097 and perplexity is 68.32998622511137
At time: 125.73892307281494 and batch: 150, loss is 4.282739734649658 and perplexity is 72.43863101533503
At time: 126.67493033409119 and batch: 200, loss is 4.285816679000854 and perplexity is 72.66186391332528
At time: 127.60922431945801 and batch: 250, loss is 4.343078594207764 and perplexity is 76.94405460505693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.918500137329102 and perplexity of 136.79728210384062
Finished 24 epochs...
Completing Train Step...
At time: 129.17857098579407 and batch: 50, loss is 4.3101603794097905 and perplexity is 74.45242861943997
At time: 130.1118586063385 and batch: 100, loss is 4.218005781173706 and perplexity is 67.89794582582789
At time: 131.05725860595703 and batch: 150, loss is 4.276567592620849 and perplexity is 71.99290644900191
At time: 131.99030804634094 and batch: 200, loss is 4.2797363662719725 and perplexity is 72.22139750104564
At time: 132.92226314544678 and batch: 250, loss is 4.337106533050537 and perplexity is 76.48590940251931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9162109375 and perplexity of 136.48448395436193
Finished 25 epochs...
Completing Train Step...
At time: 134.56435108184814 and batch: 50, loss is 4.303876581192017 and perplexity is 73.98605142587888
At time: 135.49717783927917 and batch: 100, loss is 4.2120717334747315 and perplexity is 67.4962292580728
At time: 136.4359529018402 and batch: 150, loss is 4.2710521793365475 and perplexity is 71.59692881035748
At time: 137.37392497062683 and batch: 200, loss is 4.273707389831543 and perplexity is 71.78728633495545
At time: 138.30841755867004 and batch: 250, loss is 4.331184310913086 and perplexity is 76.03428149702647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.914542770385742 and perplexity of 136.25699482431997
Finished 26 epochs...
Completing Train Step...
At time: 139.88946294784546 and batch: 50, loss is 4.297500333786011 and perplexity is 73.5157988739366
At time: 140.83935976028442 and batch: 100, loss is 4.206243343353272 and perplexity is 67.10397910608324
At time: 141.77265787124634 and batch: 150, loss is 4.26501350402832 and perplexity is 71.1658809929785
At time: 142.70727944374084 and batch: 200, loss is 4.267436800003051 and perplexity is 71.33854611077344
At time: 143.64157271385193 and batch: 250, loss is 4.325082902908325 and perplexity is 75.57177772019574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.914232635498047 and perplexity of 136.2147433287026
Finished 27 epochs...
Completing Train Step...
At time: 145.22773551940918 and batch: 50, loss is 4.291609191894532 and perplexity is 73.08398007329292
At time: 146.16495633125305 and batch: 100, loss is 4.201107149124145 and perplexity is 66.76020364037551
At time: 147.10028719902039 and batch: 150, loss is 4.259773187637329 and perplexity is 70.79392469585991
At time: 148.0325152873993 and batch: 200, loss is 4.26143367767334 and perplexity is 70.91157495393577
At time: 148.96672892570496 and batch: 250, loss is 4.319342088699341 and perplexity is 75.13917711281726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.911335372924805 and perplexity of 135.8206646013687
Finished 28 epochs...
Completing Train Step...
At time: 150.5360505580902 and batch: 50, loss is 4.285424032211304 and perplexity is 72.63333906619825
At time: 151.4911286830902 and batch: 100, loss is 4.194807739257812 and perplexity is 66.34097558589235
At time: 152.42679500579834 and batch: 150, loss is 4.253734827041626 and perplexity is 70.36773349361624
At time: 153.36616230010986 and batch: 200, loss is 4.255504131317139 and perplexity is 70.49234563123102
At time: 154.30297374725342 and batch: 250, loss is 4.3132461738586425 and perplexity is 74.68252834802341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.909545516967773 and perplexity of 135.57778260248125
Finished 29 epochs...
Completing Train Step...
At time: 155.87842869758606 and batch: 50, loss is 4.278357467651367 and perplexity is 72.12188014359492
At time: 156.84008836746216 and batch: 100, loss is 4.18838662147522 and perplexity is 65.91635708947639
At time: 157.77534770965576 and batch: 150, loss is 4.247771301269531 and perplexity is 69.94934248414525
At time: 158.71043133735657 and batch: 200, loss is 4.250176076889038 and perplexity is 70.1177573764039
At time: 159.6480975151062 and batch: 250, loss is 4.306938505172729 and perplexity is 74.21293826884822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.907493591308594 and perplexity of 135.29987229469975
Finished 30 epochs...
Completing Train Step...
At time: 161.23152375221252 and batch: 50, loss is 4.271976280212402 and perplexity is 71.66312217494219
At time: 162.1669638156891 and batch: 100, loss is 4.182207641601562 and perplexity is 65.51031699433993
At time: 163.1049325466156 and batch: 150, loss is 4.242529315948486 and perplexity is 69.58362842891756
At time: 164.03961968421936 and batch: 200, loss is 4.244522867202758 and perplexity is 69.72248532182772
At time: 164.9753601551056 and batch: 250, loss is 4.301071319580078 and perplexity is 73.77879204031281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905771636962891 and perplexity of 135.06709256717903
Finished 31 epochs...
Completing Train Step...
At time: 166.5439853668213 and batch: 50, loss is 4.266049108505249 and perplexity is 71.23961887298856
At time: 167.49898409843445 and batch: 100, loss is 4.1760484313964845 and perplexity is 65.10806522960446
At time: 168.43393850326538 and batch: 150, loss is 4.236054067611694 and perplexity is 69.13451279154764
At time: 169.36884021759033 and batch: 200, loss is 4.238889856338501 and perplexity is 69.33084190593156
At time: 170.30228996276855 and batch: 250, loss is 4.296427021026611 and perplexity is 73.43693575895239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9042816162109375 and perplexity of 134.86598965730914
Finished 32 epochs...
Completing Train Step...
At time: 171.8859338760376 and batch: 50, loss is 4.260441589355469 and perplexity is 70.84125929426166
At time: 172.82095766067505 and batch: 100, loss is 4.170049047470092 and perplexity is 64.71862631444758
At time: 173.75718188285828 and batch: 150, loss is 4.229357328414917 and perplexity is 68.6730837493113
At time: 174.6913959980011 and batch: 200, loss is 4.232033157348633 and perplexity is 68.85708724492429
At time: 175.6276340484619 and batch: 250, loss is 4.290821552276611 and perplexity is 73.02643889898094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.90303955078125 and perplexity of 134.6985812615278
Finished 33 epochs...
Completing Train Step...
At time: 177.20985984802246 and batch: 50, loss is 4.2542069625854495 and perplexity is 70.40096444587644
At time: 178.14385509490967 and batch: 100, loss is 4.1640208721160885 and perplexity is 64.32966462884643
At time: 179.09289455413818 and batch: 150, loss is 4.223574542999268 and perplexity is 68.27710806683059
At time: 180.02722263336182 and batch: 200, loss is 4.225668897628784 and perplexity is 68.42025439146039
At time: 180.96226263046265 and batch: 250, loss is 4.28524790763855 and perplexity is 72.6205476768596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.90156135559082 and perplexity of 134.4996175563743
Finished 34 epochs...
Completing Train Step...
At time: 182.51992511749268 and batch: 50, loss is 4.247808113098144 and perplexity is 69.9519174947475
At time: 183.47408866882324 and batch: 100, loss is 4.1577695369720455 and perplexity is 63.92877269600552
At time: 184.40802764892578 and batch: 150, loss is 4.217371788024902 and perplexity is 67.8549126361716
At time: 185.34272360801697 and batch: 200, loss is 4.2195854663848875 and perplexity is 68.00528796779382
At time: 186.27637648582458 and batch: 250, loss is 4.279077119827271 and perplexity is 72.17380149198536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.898654174804688 and perplexity of 134.10917067729966
Finished 35 epochs...
Completing Train Step...
At time: 187.84799981117249 and batch: 50, loss is 4.241558704376221 and perplexity is 69.51612252023203
At time: 188.81797051429749 and batch: 100, loss is 4.15049834728241 and perplexity is 63.46562033801034
At time: 189.77587747573853 and batch: 150, loss is 4.211902046203614 and perplexity is 67.48477697879974
At time: 190.72605061531067 and batch: 200, loss is 4.214149465560913 and perplexity is 67.63661412987175
At time: 191.6644082069397 and batch: 250, loss is 4.274197311401367 and perplexity is 71.82246509168229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.898481369018555 and perplexity of 134.08599783889105
Finished 36 epochs...
Completing Train Step...
At time: 193.2025203704834 and batch: 50, loss is 4.234778032302857 and perplexity is 69.04635097292564
At time: 194.1606194972992 and batch: 100, loss is 4.143735337257385 and perplexity is 63.03784984992646
At time: 195.09537506103516 and batch: 150, loss is 4.2062760353088375 and perplexity is 67.10617290224602
At time: 196.03172278404236 and batch: 200, loss is 4.209112501144409 and perplexity is 67.29678747710412
At time: 196.96875548362732 and batch: 250, loss is 4.268719253540039 and perplexity is 71.43009317143542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.895256423950196 and perplexity of 133.65427437781787
Finished 37 epochs...
Completing Train Step...
At time: 198.52266311645508 and batch: 50, loss is 4.228976030349731 and perplexity is 68.64690382684142
At time: 199.4752390384674 and batch: 100, loss is 4.137821292877197 and perplexity is 62.666141441696766
At time: 200.41228365898132 and batch: 150, loss is 4.200505781173706 and perplexity is 66.7200682628158
At time: 201.3499093055725 and batch: 200, loss is 4.202811393737793 and perplexity is 66.87407636376192
At time: 202.28504419326782 and batch: 250, loss is 4.2632176971435545 and perplexity is 71.03819549749149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.895536041259765 and perplexity of 133.69165165185476
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 203.8508050441742 and batch: 50, loss is 4.21989839553833 and perplexity is 68.0265721350317
At time: 204.78878545761108 and batch: 100, loss is 4.117454352378846 and perplexity is 61.40273341850263
At time: 205.72436356544495 and batch: 150, loss is 4.171918678283691 and perplexity is 64.83973943555098
At time: 206.6590485572815 and batch: 200, loss is 4.170523910522461 and perplexity is 64.74936609690019
At time: 207.59441781044006 and batch: 250, loss is 4.2410107564926145 and perplexity is 69.47804174211423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.883700942993164 and perplexity of 132.1187240533736
Finished 39 epochs...
Completing Train Step...
At time: 209.15138220787048 and batch: 50, loss is 4.209795923233032 and perplexity is 67.34279530775248
At time: 210.10484218597412 and batch: 100, loss is 4.110173592567444 and perplexity is 60.95729838515128
At time: 211.0414915084839 and batch: 150, loss is 4.169063119888306 and perplexity is 64.65484988034773
At time: 211.97941160202026 and batch: 200, loss is 4.172308626174927 and perplexity is 64.86502848559758
At time: 212.9158582687378 and batch: 250, loss is 4.240403800010681 and perplexity is 69.43588438946028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.882033920288086 and perplexity of 131.8986626152795
Finished 40 epochs...
Completing Train Step...
At time: 214.4839677810669 and batch: 50, loss is 4.205180616378784 and perplexity is 67.03270377720717
At time: 215.42199063301086 and batch: 100, loss is 4.107879610061645 and perplexity is 60.81762367597618
At time: 216.35743236541748 and batch: 150, loss is 4.168297700881958 and perplexity is 64.60538076411281
At time: 217.29915142059326 and batch: 200, loss is 4.172369184494019 and perplexity is 64.868956721633
At time: 218.2358582019806 and batch: 250, loss is 4.239259452819824 and perplexity is 69.35647107708115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.881509399414062 and perplexity of 131.82949715443175
Finished 41 epochs...
Completing Train Step...
At time: 219.81753063201904 and batch: 50, loss is 4.2022364044189455 and perplexity is 66.8356355367365
At time: 220.75482177734375 and batch: 100, loss is 4.10599648475647 and perplexity is 60.70320423671453
At time: 221.69016313552856 and batch: 150, loss is 4.167365884780883 and perplexity is 64.54520846921126
At time: 222.63096523284912 and batch: 200, loss is 4.172149333953858 and perplexity is 64.85469681403967
At time: 223.5679247379303 and batch: 250, loss is 4.238196535110474 and perplexity is 69.28279002110193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.880986022949219 and perplexity of 131.76051875066005
Finished 42 epochs...
Completing Train Step...
At time: 225.12110090255737 and batch: 50, loss is 4.199823789596557 and perplexity is 66.67458125087633
At time: 226.07512545585632 and batch: 100, loss is 4.104363121986389 and perplexity is 60.60413481306919
At time: 227.01059985160828 and batch: 150, loss is 4.166345300674439 and perplexity is 64.47936825875149
At time: 227.96551895141602 and batch: 200, loss is 4.171734457015991 and perplexity is 64.8277956767358
At time: 228.90052318572998 and batch: 250, loss is 4.237144293785096 and perplexity is 69.20992614821631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.880430603027344 and perplexity of 131.68735665335345
Finished 43 epochs...
Completing Train Step...
At time: 230.47051858901978 and batch: 50, loss is 4.197704029083252 and perplexity is 66.53339679778028
At time: 231.40550756454468 and batch: 100, loss is 4.102772307395935 and perplexity is 60.50780151569005
At time: 232.3432219028473 and batch: 150, loss is 4.165262727737427 and perplexity is 64.40960240980253
At time: 233.28296613693237 and batch: 200, loss is 4.17128472328186 and perplexity is 64.79864698517687
At time: 234.2186312675476 and batch: 250, loss is 4.236091170310974 and perplexity is 69.13707791617183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.879904937744141 and perplexity of 131.61815137272427
Finished 44 epochs...
Completing Train Step...
At time: 235.77050018310547 and batch: 50, loss is 4.195694608688354 and perplexity is 66.39983746667508
At time: 236.7203333377838 and batch: 100, loss is 4.101353130340576 and perplexity is 60.421991136571634
At time: 237.65789103507996 and batch: 150, loss is 4.164282197952271 and perplexity is 64.346477829013
At time: 238.59690356254578 and batch: 200, loss is 4.170796432495117 and perplexity is 64.76701412650584
At time: 239.53452134132385 and batch: 250, loss is 4.2349884843826295 and perplexity is 69.06088345023001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.87926025390625 and perplexity of 131.53332662327165
Finished 45 epochs...
Completing Train Step...
At time: 241.09112668037415 and batch: 50, loss is 4.193759841918945 and perplexity is 66.27149346554069
At time: 242.0416440963745 and batch: 100, loss is 4.1000149631500244 and perplexity is 60.34119048489281
At time: 242.97767901420593 and batch: 150, loss is 4.163380832672119 and perplexity is 64.28850427959527
At time: 243.91516947746277 and batch: 200, loss is 4.170274686813355 and perplexity is 64.7332310304233
At time: 244.85455656051636 and batch: 250, loss is 4.233842482566834 and perplexity is 68.98178488460232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8786369323730465 and perplexity of 131.45136461544936
Finished 46 epochs...
Completing Train Step...
At time: 246.4218032360077 and batch: 50, loss is 4.191819171905518 and perplexity is 66.14300708058974
At time: 247.35857152938843 and batch: 100, loss is 4.098832173347473 and perplexity is 60.26986173189
At time: 248.2960901260376 and batch: 150, loss is 4.1625744724273686 and perplexity is 64.23668548066723
At time: 249.23304533958435 and batch: 200, loss is 4.1696062183380125 and perplexity is 64.68997336597572
At time: 250.17265582084656 and batch: 250, loss is 4.23257682800293 and perplexity is 68.89453300075667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8780559539794925 and perplexity of 131.37501639328585
Finished 47 epochs...
Completing Train Step...
At time: 251.72875118255615 and batch: 50, loss is 4.189946880340576 and perplexity is 66.01928394538645
At time: 252.6790475845337 and batch: 100, loss is 4.097723069190979 and perplexity is 60.20305323337206
At time: 253.6138253211975 and batch: 150, loss is 4.161720542907715 and perplexity is 64.18185529258051
At time: 254.5489695072174 and batch: 200, loss is 4.1688377475738525 and perplexity is 64.64028010906276
At time: 255.48844981193542 and batch: 250, loss is 4.231249294281006 and perplexity is 68.80313386608172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.877597427368164 and perplexity of 131.31479126067393
Finished 48 epochs...
Completing Train Step...
At time: 257.0555694103241 and batch: 50, loss is 4.188106708526611 and perplexity is 65.89790882967576
At time: 257.9939057826996 and batch: 100, loss is 4.096685361862183 and perplexity is 60.14061248702911
At time: 258.9301793575287 and batch: 150, loss is 4.16081636428833 and perplexity is 64.12384965895116
At time: 259.865061044693 and batch: 200, loss is 4.167957820892334 and perplexity is 64.58342641910168
At time: 260.80345249176025 and batch: 250, loss is 4.22991828918457 and perplexity is 68.71161746216592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.877125549316406 and perplexity of 131.25284131036733
Finished 49 epochs...
Completing Train Step...
At time: 262.3814945220947 and batch: 50, loss is 4.186395616531372 and perplexity is 65.78524785950415
At time: 263.3167006969452 and batch: 100, loss is 4.095560002326965 and perplexity is 60.07297074309241
At time: 264.2554862499237 and batch: 150, loss is 4.159890956878662 and perplexity is 64.06453642202773
At time: 265.19263887405396 and batch: 200, loss is 4.167080764770508 and perplexity is 64.52680796200387
At time: 266.1299500465393 and batch: 250, loss is 4.228574752807617 and perplexity is 68.61936289214891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8767036437988285 and perplexity of 131.19747669256088
Finished Training.
Improved accuracyfrom -10000000 to -131.19747669256088
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b8d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.4781280975888139, 'wordvec_source': 'glove', 'anneal': 2.655126351271187, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 24.944057254752767}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.220836877822876 and batch: 50, loss is 6.650425243377685 and perplexity is 773.1130168339084
At time: 2.172297716140747 and batch: 100, loss is 5.804036531448364 and perplexity is 331.6355189770363
At time: 3.120610237121582 and batch: 150, loss is 5.791261749267578 and perplexity is 327.42589323368105
At time: 4.060581922531128 and batch: 200, loss is 5.863383359909058 and perplexity is 351.9127796851858
At time: 5.001145839691162 and batch: 250, loss is 5.938962469100952 and perplexity is 379.5409397327601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.041550064086914 and perplexity of 420.54440069935526
Finished 1 epochs...
Completing Train Step...
At time: 6.56360125541687 and batch: 50, loss is 5.773392715454102 and perplexity is 321.6270728322662
At time: 7.492950201034546 and batch: 100, loss is 5.800443868637085 and perplexity is 330.44620206765546
At time: 8.42551302909851 and batch: 150, loss is 5.820104064941407 and perplexity is 337.0071224317727
At time: 9.355685234069824 and batch: 200, loss is 5.880453233718872 and perplexity is 357.97144967840296
At time: 10.285012245178223 and batch: 250, loss is 5.9318904685974125 and perplexity is 376.86629470862005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.197265243530273 and perplexity of 491.4033333952916
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 11.838667631149292 and batch: 50, loss is 5.750894327163696 and perplexity is 314.4717752133884
At time: 12.76792311668396 and batch: 100, loss is 5.565788383483887 and perplexity is 261.3311516799216
At time: 13.69671893119812 and batch: 150, loss is 5.481037588119507 and perplexity is 240.09569862117095
At time: 14.628001928329468 and batch: 200, loss is 5.469552011489868 and perplexity is 237.3538371480211
At time: 15.557946681976318 and batch: 250, loss is 5.522755136489868 and perplexity is 250.3237641224925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.766786193847656 and perplexity of 319.5092400832173
Finished 3 epochs...
Completing Train Step...
At time: 17.1003360748291 and batch: 50, loss is 5.468760004043579 and perplexity is 237.16592556510386
At time: 18.04562735557556 and batch: 100, loss is 5.438964462280273 and perplexity is 230.20367540081202
At time: 18.974661827087402 and batch: 150, loss is 5.445161571502686 and perplexity is 231.63470225575242
At time: 19.907212257385254 and batch: 200, loss is 5.435441341400146 and perplexity is 229.39406703682187
At time: 20.8350567817688 and batch: 250, loss is 5.465309791564941 and perplexity is 236.34906271473668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.718833160400391 and perplexity of 304.54935530423796
Finished 4 epochs...
Completing Train Step...
At time: 22.400723457336426 and batch: 50, loss is 5.459801845550537 and perplexity is 235.05084337981043
At time: 23.32930326461792 and batch: 100, loss is 5.435076847076416 and perplexity is 229.31046943784017
At time: 24.25730609893799 and batch: 150, loss is 5.410557661056519 and perplexity is 223.75633308759782
At time: 25.19261336326599 and batch: 200, loss is 5.399801111221313 and perplexity is 221.36238533124322
At time: 26.11924910545349 and batch: 250, loss is 5.454744939804077 and perplexity is 233.86521375445056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.729734802246094 and perplexity of 307.88760644792393
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 27.66352391242981 and batch: 50, loss is 5.356403493881226 and perplexity is 211.96125401347217
At time: 28.60819149017334 and batch: 100, loss is 5.2368728542327885 and perplexity is 188.0810250389765
At time: 29.53675866127014 and batch: 150, loss is 5.191691989898682 and perplexity is 179.77246894943502
At time: 30.473335027694702 and batch: 200, loss is 5.192155628204346 and perplexity is 179.85583767731828
At time: 31.40002155303955 and batch: 250, loss is 5.2521217918396 and perplexity is 190.9710396802293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.552711868286133 and perplexity of 257.936097023287
Finished 6 epochs...
Completing Train Step...
At time: 32.951252698898315 and batch: 50, loss is 5.217651300430298 and perplexity is 184.50033891753384
At time: 33.89988040924072 and batch: 100, loss is 5.178719730377197 and perplexity is 177.45547464491798
At time: 34.835613489151 and batch: 150, loss is 5.183132162094116 and perplexity is 178.24021484263983
At time: 35.78823518753052 and batch: 200, loss is 5.184289722442627 and perplexity is 178.4466581100701
At time: 36.72180938720703 and batch: 250, loss is 5.226620445251465 and perplexity is 186.16259253013493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.530018997192383 and perplexity of 252.14870109539746
Finished 7 epochs...
Completing Train Step...
At time: 38.283740758895874 and batch: 50, loss is 5.212334232330322 and perplexity is 183.5219414590378
At time: 39.216423988342285 and batch: 100, loss is 5.168802843093872 and perplexity is 175.70436583041487
At time: 40.14960956573486 and batch: 150, loss is 5.161310186386109 and perplexity is 174.39279305343328
At time: 41.08511257171631 and batch: 200, loss is 5.16926830291748 and perplexity is 175.78616818992043
At time: 42.0183048248291 and batch: 250, loss is 5.214892597198486 and perplexity is 183.99205865584514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.512077713012696 and perplexity of 247.66517001236994
Finished 8 epochs...
Completing Train Step...
At time: 43.56440877914429 and batch: 50, loss is 5.186408681869507 and perplexity is 178.8251802333483
At time: 44.52473068237305 and batch: 100, loss is 5.148083667755127 and perplexity is 172.10137070238525
At time: 45.464905977249146 and batch: 150, loss is 5.1459234428405765 and perplexity is 171.7299943062586
At time: 46.39845108985901 and batch: 200, loss is 5.147868614196778 and perplexity is 172.06436366960853
At time: 47.336546421051025 and batch: 250, loss is 5.193999910354615 and perplexity is 180.18784865522042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.511832427978516 and perplexity of 247.60442890242547
Finished 9 epochs...
Completing Train Step...
At time: 48.901042461395264 and batch: 50, loss is 5.174150438308716 and perplexity is 176.64647842965636
At time: 49.83332085609436 and batch: 100, loss is 5.126061353683472 and perplexity is 168.35272865177302
At time: 50.764482736587524 and batch: 150, loss is 5.1318435382843015 and perplexity is 169.32899496289482
At time: 51.69708824157715 and batch: 200, loss is 5.138353805541993 and perplexity is 170.43496815886647
At time: 52.62952518463135 and batch: 250, loss is 5.183273677825928 and perplexity is 178.26544042194755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.491546249389648 and perplexity of 242.63208667034533
Finished 10 epochs...
Completing Train Step...
At time: 54.196627616882324 and batch: 50, loss is 5.159634113311768 and perplexity is 174.100742805785
At time: 55.13092255592346 and batch: 100, loss is 5.121770362854004 and perplexity is 167.63187632843045
At time: 56.063589572906494 and batch: 150, loss is 5.123608961105346 and perplexity is 167.94036751188924
At time: 56.99669075012207 and batch: 200, loss is 5.128267650604248 and perplexity is 168.7245748092801
At time: 57.928988456726074 and batch: 250, loss is 5.172037477493286 and perplexity is 176.27362539319896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.487202453613281 and perplexity of 241.58042818550427
Finished 11 epochs...
Completing Train Step...
At time: 59.47553896903992 and batch: 50, loss is 5.152616672515869 and perplexity is 172.883277890719
At time: 60.42515587806702 and batch: 100, loss is 5.111452245712281 and perplexity is 165.91112372169513
At time: 61.35941815376282 and batch: 150, loss is 5.113731822967529 and perplexity is 166.28976234976068
At time: 62.29447340965271 and batch: 200, loss is 5.12032790184021 and perplexity is 167.39024819363584
At time: 63.227999448776245 and batch: 250, loss is 5.167097425460815 and perplexity is 175.40497187515214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4886619567871096 and perplexity of 241.93327301361475
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 64.79713416099548 and batch: 50, loss is 5.117840137481689 and perplexity is 166.9743382576778
At time: 65.7342267036438 and batch: 100, loss is 5.033313255310059 and perplexity is 153.44055916322026
At time: 66.66974949836731 and batch: 150, loss is 5.024677839279175 and perplexity is 152.12124072911172
At time: 67.60406184196472 and batch: 200, loss is 5.032564973831176 and perplexity is 153.32578538159194
At time: 68.53972363471985 and batch: 250, loss is 5.096887922286987 and perplexity is 163.51225183279362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.426663970947265 and perplexity of 227.38940104906308
Finished 13 epochs...
Completing Train Step...
At time: 70.08742499351501 and batch: 50, loss is 5.06990161895752 and perplexity is 159.1586683773223
At time: 71.03672933578491 and batch: 100, loss is 5.013873863220215 and perplexity is 150.48657284320916
At time: 71.96934127807617 and batch: 150, loss is 5.02691011428833 and perplexity is 152.46119646933647
At time: 72.90562343597412 and batch: 200, loss is 5.037269229888916 and perplexity is 154.04876835148676
At time: 73.84167385101318 and batch: 250, loss is 5.086319875717163 and perplexity is 161.793345479658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.422691345214844 and perplexity of 226.48785999143587
Finished 14 epochs...
Completing Train Step...
At time: 75.38904809951782 and batch: 50, loss is 5.070206155776978 and perplexity is 159.20714543312863
At time: 76.34135675430298 and batch: 100, loss is 5.012063150405884 and perplexity is 150.21433142727014
At time: 77.27540302276611 and batch: 150, loss is 5.02278205871582 and perplexity is 151.83312542565037
At time: 78.20861721038818 and batch: 200, loss is 5.035732011795044 and perplexity is 153.81214371586998
At time: 79.14434432983398 and batch: 250, loss is 5.084734287261963 and perplexity is 161.53701109306323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.417074203491211 and perplexity of 225.219212006672
Finished 15 epochs...
Completing Train Step...
At time: 80.70992803573608 and batch: 50, loss is 5.060256252288818 and perplexity is 157.630904428422
At time: 81.64641809463501 and batch: 100, loss is 5.004914379119873 and perplexity is 149.1443127443655
At time: 82.57927107810974 and batch: 150, loss is 5.019671449661255 and perplexity is 151.36156573007764
At time: 83.51156067848206 and batch: 200, loss is 5.029412059783936 and perplexity is 152.84312365436764
At time: 84.47074723243713 and batch: 250, loss is 5.077599124908447 and perplexity is 160.3885204987576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.414701080322265 and perplexity of 224.68537276024514
Finished 16 epochs...
Completing Train Step...
At time: 86.06891465187073 and batch: 50, loss is 5.056543197631836 and perplexity is 157.0466975319125
At time: 87.02194333076477 and batch: 100, loss is 5.000736246109009 and perplexity is 148.52246794765983
At time: 87.96439409255981 and batch: 150, loss is 5.014484462738037 and perplexity is 150.57848793082425
At time: 88.89922046661377 and batch: 200, loss is 5.025709066390991 and perplexity is 152.2781931897421
At time: 89.83486914634705 and batch: 250, loss is 5.073843755722046 and perplexity is 159.78733194005488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.410754776000976 and perplexity of 223.800443152
Finished 17 epochs...
Completing Train Step...
At time: 91.39565062522888 and batch: 50, loss is 5.049886236190796 and perplexity is 156.0047157861126
At time: 92.33299589157104 and batch: 100, loss is 4.99427098274231 and perplexity is 147.56532848984685
At time: 93.26768755912781 and batch: 150, loss is 5.009966773986816 and perplexity is 149.89975549505124
At time: 94.20291328430176 and batch: 200, loss is 5.0204168224334715 and perplexity is 151.4744285771542
At time: 95.1369400024414 and batch: 250, loss is 5.067428007125854 and perplexity is 158.7654581373594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.408671188354492 and perplexity of 223.33462077277255
Finished 18 epochs...
Completing Train Step...
At time: 96.69722032546997 and batch: 50, loss is 5.045928258895874 and perplexity is 155.38847300508152
At time: 97.63929867744446 and batch: 100, loss is 4.989972515106201 and perplexity is 146.93238501919862
At time: 98.58483171463013 and batch: 150, loss is 5.005655574798584 and perplexity is 149.2548988423822
At time: 99.52765703201294 and batch: 200, loss is 5.016561889648438 and perplexity is 150.89162888488303
At time: 100.46385145187378 and batch: 250, loss is 5.065071983337402 and perplexity is 158.3918432368233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.405709838867187 and perplexity of 222.6742272187211
Finished 19 epochs...
Completing Train Step...
At time: 102.00967383384705 and batch: 50, loss is 5.040356178283691 and perplexity is 154.5250436901503
At time: 102.96147537231445 and batch: 100, loss is 4.9844991493225095 and perplexity is 146.1303672011785
At time: 103.89819025993347 and batch: 150, loss is 5.001646099090576 and perplexity is 148.65766305245975
At time: 104.83258652687073 and batch: 200, loss is 5.010420894622802 and perplexity is 149.96784352627068
At time: 105.76673173904419 and batch: 250, loss is 5.059510021209717 and perplexity is 157.5133192268332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.402447509765625 and perplexity of 221.94897425724486
Finished 20 epochs...
Completing Train Step...
At time: 107.32919955253601 and batch: 50, loss is 5.036365003585815 and perplexity is 153.90953636128356
At time: 108.26706767082214 and batch: 100, loss is 4.979746217727661 and perplexity is 145.4374675187415
At time: 109.22905731201172 and batch: 150, loss is 4.997287788391113 and perplexity is 148.0111765867883
At time: 110.16700959205627 and batch: 200, loss is 5.005840282440186 and perplexity is 149.2824699089599
At time: 111.10089755058289 and batch: 250, loss is 5.057035760879517 and perplexity is 157.124072017636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.399760437011719 and perplexity of 221.3533817742935
Finished 21 epochs...
Completing Train Step...
At time: 112.65020251274109 and batch: 50, loss is 5.030776243209839 and perplexity is 153.05177199536675
At time: 113.60098052024841 and batch: 100, loss is 4.976300926208496 and perplexity is 144.93725522870523
At time: 114.5366427898407 and batch: 150, loss is 4.995234756469727 and perplexity is 147.7076166322959
At time: 115.47182440757751 and batch: 200, loss is 5.002612724304199 and perplexity is 148.8014287703543
At time: 116.40848064422607 and batch: 250, loss is 5.052581920623779 and perplexity is 156.42582260046603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.396809005737305 and perplexity of 220.7010356315772
Finished 22 epochs...
Completing Train Step...
At time: 117.95387125015259 and batch: 50, loss is 5.028824119567871 and perplexity is 152.75328744696975
At time: 118.90524387359619 and batch: 100, loss is 4.973807878494263 and perplexity is 144.57636977509492
At time: 119.84096264839172 and batch: 150, loss is 4.992357444763184 and perplexity is 147.28322662177789
At time: 120.77781748771667 and batch: 200, loss is 4.998939914703369 and perplexity is 148.25591185725773
At time: 121.71254467964172 and batch: 250, loss is 5.0521870040893555 and perplexity is 156.36405965311758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.397195434570312 and perplexity of 220.78633735568116
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 123.27655005455017 and batch: 50, loss is 5.016522865295411 and perplexity is 150.88574055158352
At time: 124.21321368217468 and batch: 100, loss is 4.946178903579712 and perplexity is 140.63655009288365
At time: 125.15273070335388 and batch: 150, loss is 4.956505651473999 and perplexity is 142.09639303209033
At time: 126.08771586418152 and batch: 200, loss is 4.963800678253174 and perplexity is 143.13678023685415
At time: 127.02086687088013 and batch: 250, loss is 5.023105926513672 and perplexity is 151.88230724939757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.374879837036133 and perplexity of 215.91392589447054
Finished 24 epochs...
Completing Train Step...
At time: 128.57163953781128 and batch: 50, loss is 4.99592059135437 and perplexity is 147.80895441503088
At time: 129.5220708847046 and batch: 100, loss is 4.939330568313599 and perplexity is 139.67671423737875
At time: 130.46236777305603 and batch: 150, loss is 4.958707990646363 and perplexity is 142.40968234274817
At time: 131.3950536251068 and batch: 200, loss is 4.966432065963745 and perplexity is 143.5139245901898
At time: 132.33187818527222 and batch: 250, loss is 5.018500852584839 and perplexity is 151.18448598848718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.371431732177735 and perplexity of 215.17071411004372
Finished 25 epochs...
Completing Train Step...
At time: 133.90058708190918 and batch: 50, loss is 4.992917509078979 and perplexity is 147.36573780495183
At time: 134.83605217933655 and batch: 100, loss is 4.938863792419434 and perplexity is 139.61153172819203
At time: 135.7731215953827 and batch: 150, loss is 4.956915979385376 and perplexity is 142.1547111122071
At time: 136.70696353912354 and batch: 200, loss is 4.966154327392578 and perplexity is 143.474070772559
At time: 137.6450650691986 and batch: 250, loss is 5.017797374725342 and perplexity is 151.07816845029654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.36957893371582 and perplexity of 214.7724152396009
Finished 26 epochs...
Completing Train Step...
At time: 139.2177426815033 and batch: 50, loss is 4.988143081665039 and perplexity is 146.66382772919965
At time: 140.15049076080322 and batch: 100, loss is 4.936648435592652 and perplexity is 139.3025847085793
At time: 141.08767938613892 and batch: 150, loss is 4.9577064037323 and perplexity is 142.2671180756481
At time: 142.02365517616272 and batch: 200, loss is 4.965304021835327 and perplexity is 143.3521258254401
At time: 142.95906114578247 and batch: 250, loss is 5.0144189262390135 and perplexity is 150.56861986725977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369844818115235 and perplexity of 214.82952746652634
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 144.51712369918823 and batch: 50, loss is 4.9817601013183594 and perplexity is 145.73065677347344
At time: 145.46788334846497 and batch: 100, loss is 4.925546045303345 and perplexity is 137.76454679202396
At time: 146.40435218811035 and batch: 150, loss is 4.943966817855835 and perplexity is 140.3257938246394
At time: 147.34015083312988 and batch: 200, loss is 4.952097654342651 and perplexity is 141.47141101068135
At time: 148.27568697929382 and batch: 250, loss is 5.00480504989624 and perplexity is 149.12800780376344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.358316040039062 and perplexity of 212.36702760211708
Finished 28 epochs...
Completing Train Step...
At time: 149.83664989471436 and batch: 50, loss is 4.976432600021362 and perplexity is 144.9563409262433
At time: 150.7692587375641 and batch: 100, loss is 4.923629875183106 and perplexity is 137.50081923795648
At time: 151.7020251750946 and batch: 150, loss is 4.945765523910523 and perplexity is 140.57842581685503
At time: 152.64328360557556 and batch: 200, loss is 4.9536367893219 and perplexity is 141.68932226232823
At time: 153.57713150978088 and batch: 250, loss is 5.002947416305542 and perplexity is 148.851239753556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.356761169433594 and perplexity of 212.03708093199114
Finished 29 epochs...
Completing Train Step...
At time: 155.12291836738586 and batch: 50, loss is 4.974408922195434 and perplexity is 144.6632926110812
At time: 156.0723991394043 and batch: 100, loss is 4.923328008651733 and perplexity is 137.4593186067332
At time: 157.007568359375 and batch: 150, loss is 4.94606692314148 and perplexity is 140.6208024321042
At time: 157.95984888076782 and batch: 200, loss is 4.9533951950073245 and perplexity is 141.65509506234858
At time: 158.895179271698 and batch: 250, loss is 5.001065120697022 and perplexity is 148.57132124598067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.355425262451172 and perplexity of 211.75400823653573
Finished 30 epochs...
Completing Train Step...
At time: 160.44674825668335 and batch: 50, loss is 4.972074861526489 and perplexity is 144.32603345433526
At time: 161.39290928840637 and batch: 100, loss is 4.922045049667358 and perplexity is 137.28307701847936
At time: 162.32945203781128 and batch: 150, loss is 4.946383857727051 and perplexity is 140.66537709109815
At time: 163.27353811264038 and batch: 200, loss is 4.953248176574707 and perplexity is 141.63427068312157
At time: 164.20821070671082 and batch: 250, loss is 4.999533090591431 and perplexity is 148.34387977707917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.355110931396484 and perplexity of 211.6874578357691
Finished 31 epochs...
Completing Train Step...
At time: 165.7908480167389 and batch: 50, loss is 4.970701456069946 and perplexity is 144.1279513471272
At time: 166.74718117713928 and batch: 100, loss is 4.92172194480896 and perplexity is 137.2387273545006
At time: 167.70220971107483 and batch: 150, loss is 4.946782417297364 and perplexity is 140.7214517971625
At time: 168.6450731754303 and batch: 200, loss is 4.953386077880859 and perplexity is 141.65380358081975
At time: 169.58193588256836 and batch: 250, loss is 4.998880681991577 and perplexity is 148.24713051763302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.354410934448242 and perplexity of 211.53932911217868
Finished 32 epochs...
Completing Train Step...
At time: 171.1390564441681 and batch: 50, loss is 4.969348020553589 and perplexity is 143.93301540524456
At time: 172.08811116218567 and batch: 100, loss is 4.921284732818603 and perplexity is 137.17873805233666
At time: 173.02278399467468 and batch: 150, loss is 4.947111434936524 and perplexity is 140.7677592545794
At time: 173.95840167999268 and batch: 200, loss is 4.952863512039184 and perplexity is 141.5797994794169
At time: 174.8974165916443 and batch: 250, loss is 4.997789487838745 and perplexity is 148.08545234282022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.354269409179688 and perplexity of 211.50939307021946
Finished 33 epochs...
Completing Train Step...
At time: 176.4558720588684 and batch: 50, loss is 4.968417530059814 and perplexity is 143.7991493928542
At time: 177.38804030418396 and batch: 100, loss is 4.920572052001953 and perplexity is 137.08100822649703
At time: 178.32162499427795 and batch: 150, loss is 4.947246026992798 and perplexity is 140.78670675181732
At time: 179.25975155830383 and batch: 200, loss is 4.952614221572876 and perplexity is 141.5445093841098
At time: 180.1972780227661 and batch: 250, loss is 4.996931390762329 and perplexity is 147.9584351534384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.353774642944336 and perplexity of 211.4047712478753
Finished 34 epochs...
Completing Train Step...
At time: 181.7624340057373 and batch: 50, loss is 4.967464447021484 and perplexity is 143.6621621531185
At time: 182.69555735588074 and batch: 100, loss is 4.919795007705688 and perplexity is 136.9745315848195
At time: 183.6289792060852 and batch: 150, loss is 4.9473228931427 and perplexity is 140.79752889984582
At time: 184.5649697780609 and batch: 200, loss is 4.952454824447631 and perplexity is 141.52194939426633
At time: 185.50479555130005 and batch: 250, loss is 4.995976161956787 and perplexity is 147.81716847589846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.353413009643555 and perplexity of 211.32833406459665
Finished 35 epochs...
Completing Train Step...
At time: 187.05622100830078 and batch: 50, loss is 4.966393537521363 and perplexity is 143.50839532873277
At time: 188.00324726104736 and batch: 100, loss is 4.919260177612305 and perplexity is 136.90129307012745
At time: 188.93632245063782 and batch: 150, loss is 4.947390556335449 and perplexity is 140.80705603249683
At time: 189.87355017662048 and batch: 200, loss is 4.952134847640991 and perplexity is 141.47667289693047
At time: 190.81289172172546 and batch: 250, loss is 4.995060186386109 and perplexity is 147.68183355185212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.353202438354492 and perplexity of 211.2838390697257
Finished 36 epochs...
Completing Train Step...
At time: 192.37269353866577 and batch: 50, loss is 4.965328350067138 and perplexity is 143.3556133716106
At time: 193.30478715896606 and batch: 100, loss is 4.918585577011108 and perplexity is 136.8089705194431
At time: 194.2380142211914 and batch: 150, loss is 4.947165298461914 and perplexity is 140.7753417065611
At time: 195.1716902256012 and batch: 200, loss is 4.951293725967407 and perplexity is 141.35772383338085
At time: 196.11027073860168 and batch: 250, loss is 4.9938014698028566 and perplexity is 147.49606092098855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.35248031616211 and perplexity of 211.13132139546806
Finished 37 epochs...
Completing Train Step...
At time: 197.65724325180054 and batch: 50, loss is 4.964053630828857 and perplexity is 143.17299163378013
At time: 198.60300254821777 and batch: 100, loss is 4.917851610183716 and perplexity is 136.708594114374
At time: 199.53703546524048 and batch: 150, loss is 4.947081003189087 and perplexity is 140.7634755108638
At time: 200.4749345779419 and batch: 200, loss is 4.951196222305298 and perplexity is 141.34394160955932
At time: 201.414235830307 and batch: 250, loss is 4.9931473064422605 and perplexity is 147.39960595419407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.351744079589844 and perplexity of 210.97593600232938
Finished 38 epochs...
Completing Train Step...
At time: 202.9591794013977 and batch: 50, loss is 4.962953186035156 and perplexity is 143.01552431847253
At time: 203.9066982269287 and batch: 100, loss is 4.917140378952026 and perplexity is 136.61139726141545
At time: 204.84187126159668 and batch: 150, loss is 4.946799983978272 and perplexity is 140.7239238277158
At time: 205.79418635368347 and batch: 200, loss is 4.9506674575805665 and perplexity is 141.26922367502658
At time: 206.7304151058197 and batch: 250, loss is 4.992392950057983 and perplexity is 147.2884560489937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.351569366455078 and perplexity of 210.93907895498916
Finished 39 epochs...
Completing Train Step...
At time: 208.29590702056885 and batch: 50, loss is 4.9619951248168945 and perplexity is 142.87857230569526
At time: 209.22770857810974 and batch: 100, loss is 4.916540441513061 and perplexity is 136.5294635496171
At time: 210.16174268722534 and batch: 150, loss is 4.946589460372925 and perplexity is 140.69430123814962
At time: 211.0962417125702 and batch: 200, loss is 4.950437812805176 and perplexity is 141.23678566063896
At time: 212.03414154052734 and batch: 250, loss is 4.991931056976318 and perplexity is 147.22044023936076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.351064682006836 and perplexity of 210.83264814156124
Finished 40 epochs...
Completing Train Step...
At time: 213.57729363441467 and batch: 50, loss is 4.961051054000855 and perplexity is 142.74374846698345
At time: 214.52369332313538 and batch: 100, loss is 4.9158541870117185 and perplexity is 136.43580173228796
At time: 215.4551877975464 and batch: 150, loss is 4.946420764923095 and perplexity is 140.67056875155131
At time: 216.39174103736877 and batch: 200, loss is 4.950240564346314 and perplexity is 141.20892966969868
At time: 217.3244390487671 and batch: 250, loss is 4.991396579742432 and perplexity is 147.14177528987506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.350970840454101 and perplexity of 210.81286420678455
Finished 41 epochs...
Completing Train Step...
At time: 218.8917875289917 and batch: 50, loss is 4.960307703018189 and perplexity is 142.63767918952612
At time: 219.82534050941467 and batch: 100, loss is 4.9152974605560305 and perplexity is 136.35986545179009
At time: 220.76336407661438 and batch: 150, loss is 4.946452903747558 and perplexity is 140.6750898109179
At time: 221.69921231269836 and batch: 200, loss is 4.950090570449829 and perplexity is 141.18775078051087
At time: 222.63367366790771 and batch: 250, loss is 4.990484209060669 and perplexity is 147.00758867131464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3507129669189455 and perplexity of 210.75850815702984
Finished 42 epochs...
Completing Train Step...
At time: 224.19248461723328 and batch: 50, loss is 4.959540071487427 and perplexity is 142.52822802396304
At time: 225.12913346290588 and batch: 100, loss is 4.914898443222046 and perplexity is 136.3054663556279
At time: 226.0611503124237 and batch: 150, loss is 4.946265239715576 and perplexity is 140.64869263334234
At time: 226.99610567092896 and batch: 200, loss is 4.949700412750244 and perplexity is 141.13267603707294
At time: 227.92953038215637 and batch: 250, loss is 4.989748640060425 and perplexity is 146.8994942066299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.350536346435547 and perplexity of 210.72128717452946
Finished 43 epochs...
Completing Train Step...
At time: 229.4770793914795 and batch: 50, loss is 4.958691806793213 and perplexity is 142.40737762401167
At time: 230.42680168151855 and batch: 100, loss is 4.914445009231567 and perplexity is 136.243674834341
At time: 231.36050081253052 and batch: 150, loss is 4.945947704315185 and perplexity is 140.6040387843767
At time: 232.29640197753906 and batch: 200, loss is 4.949285736083985 and perplexity is 141.07416374215381
At time: 233.23365139961243 and batch: 250, loss is 4.989447393417358 and perplexity is 146.85524789199349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.350450134277343 and perplexity of 210.70312122065704
Finished 44 epochs...
Completing Train Step...
At time: 234.8078968524933 and batch: 50, loss is 4.957838735580444 and perplexity is 142.285945792039
At time: 235.74471831321716 and batch: 100, loss is 4.913990526199341 and perplexity is 136.18176846464354
At time: 236.67851758003235 and batch: 150, loss is 4.945941572189331 and perplexity is 140.60317658535882
At time: 237.61633276939392 and batch: 200, loss is 4.949173812866211 and perplexity is 141.0583751513745
At time: 238.55109453201294 and batch: 250, loss is 4.988645973205567 and perplexity is 146.73760227620906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.349870300292968 and perplexity of 210.580983803494
Finished 45 epochs...
Completing Train Step...
At time: 240.09896111488342 and batch: 50, loss is 4.9571483707427975 and perplexity is 142.18775047736912
At time: 241.05012917518616 and batch: 100, loss is 4.913486528396606 and perplexity is 136.11315044568272
At time: 241.9855101108551 and batch: 150, loss is 4.9452487659454345 and perplexity is 140.5057995623085
At time: 242.92116141319275 and batch: 200, loss is 4.9487177181243895 and perplexity is 140.9940538375996
At time: 243.85536980628967 and batch: 250, loss is 4.988305282592774 and perplexity is 146.68761866752638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.350562286376953 and perplexity of 210.7267533432676
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 245.40432620048523 and batch: 50, loss is 4.954923677444458 and perplexity is 141.87177794308505
At time: 246.35573029518127 and batch: 100, loss is 4.908870992660522 and perplexity is 135.48636292845504
At time: 247.29162526130676 and batch: 150, loss is 4.94101583480835 and perplexity is 139.91230518498617
At time: 248.23721647262573 and batch: 200, loss is 4.943054857254029 and perplexity is 140.19788056404633
At time: 249.17308115959167 and batch: 250, loss is 4.983873052597046 and perplexity is 146.03890409214736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.346335601806641 and perplexity of 209.83795747942577
Finished 47 epochs...
Completing Train Step...
At time: 250.7324674129486 and batch: 50, loss is 4.953482809066773 and perplexity is 141.66750658397248
At time: 251.66886234283447 and batch: 100, loss is 4.909053392410279 and perplexity is 135.51107786107886
At time: 252.605562210083 and batch: 150, loss is 4.940736513137818 and perplexity is 139.8732301036865
At time: 253.53886365890503 and batch: 200, loss is 4.942359399795532 and perplexity is 140.10041279861335
At time: 254.48889255523682 and batch: 250, loss is 4.983926668167114 and perplexity is 146.04673426115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.346143341064453 and perplexity of 209.7976177559784
Finished 48 epochs...
Completing Train Step...
At time: 256.0406086444855 and batch: 50, loss is 4.953260555267334 and perplexity is 141.63602394107522
At time: 256.9896967411041 and batch: 100, loss is 4.908754806518555 and perplexity is 135.4706222051019
At time: 257.9276931285858 and batch: 150, loss is 4.940576400756836 and perplexity is 139.85083646057655
At time: 258.8626141548157 and batch: 200, loss is 4.941784172058106 and perplexity is 140.0198463294021
At time: 259.79689621925354 and batch: 250, loss is 4.98349967956543 and perplexity is 145.98438728197928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.345592498779297 and perplexity of 209.68208418010423
Finished 49 epochs...
Completing Train Step...
At time: 261.3585982322693 and batch: 50, loss is 4.952808694839478 and perplexity is 141.5720386839889
At time: 262.2906095981598 and batch: 100, loss is 4.908696403503418 and perplexity is 135.46271054333735
At time: 263.2281274795532 and batch: 150, loss is 4.940248775482178 and perplexity is 139.8050252967273
At time: 264.16431164741516 and batch: 200, loss is 4.940884981155396 and perplexity is 139.89399834654395
At time: 265.09864592552185 and batch: 250, loss is 4.983417987823486 and perplexity is 145.97246205018877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.345477294921875 and perplexity of 209.6579293865639
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b8d0>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.6106093655028537, 'wordvec_source': 'glove', 'anneal': 7.091284921166074, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 9.700055246834742}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2156851291656494 and batch: 50, loss is 6.630009689331055 and perplexity is 757.4895101728864
At time: 2.1705405712127686 and batch: 100, loss is 5.837800025939941 and perplexity is 343.0238663497686
At time: 3.1091842651367188 and batch: 150, loss is 5.626663742065429 and perplexity is 277.7339780494446
At time: 4.049125909805298 and batch: 200, loss is 5.5461729145050045 and perplexity is 256.2549672115662
At time: 4.98641562461853 and batch: 250, loss is 5.544082918167114 and perplexity is 255.7199545505274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.495356750488281 and perplexity of 243.5584002417743
Finished 1 epochs...
Completing Train Step...
At time: 6.5520007610321045 and batch: 50, loss is 5.1278950881958005 and perplexity is 168.66172608358818
At time: 7.484363317489624 and batch: 100, loss is 4.925159873962403 and perplexity is 137.71135634322948
At time: 8.417790174484253 and batch: 150, loss is 4.872915296554566 and perplexity is 130.701395350766
At time: 9.350677251815796 and batch: 200, loss is 4.812088918685913 and perplexity is 122.98826184769408
At time: 10.282845735549927 and batch: 250, loss is 4.809679088592529 and perplexity is 122.69223786017507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.006264495849609 and perplexity of 149.34581096860063
Finished 2 epochs...
Completing Train Step...
At time: 11.846866130828857 and batch: 50, loss is 4.704466609954834 and perplexity is 110.4393619781327
At time: 12.780004262924194 and batch: 100, loss is 4.588449745178223 and perplexity is 98.34185699087311
At time: 13.71100378036499 and batch: 150, loss is 4.620955505371094 and perplexity is 101.59105656868385
At time: 14.639285564422607 and batch: 200, loss is 4.584904346466065 and perplexity is 97.99381323925226
At time: 15.569978952407837 and batch: 250, loss is 4.606486511230469 and perplexity is 100.131719197871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.899463653564453 and perplexity of 134.21777315222084
Finished 3 epochs...
Completing Train Step...
At time: 17.125552892684937 and batch: 50, loss is 4.541650323867798 and perplexity is 93.8455479399929
At time: 18.071254014968872 and batch: 100, loss is 4.435742807388306 and perplexity is 84.41480554682302
At time: 19.005603313446045 and batch: 150, loss is 4.484900741577149 and perplexity is 88.66814931549825
At time: 19.938765287399292 and batch: 200, loss is 4.459597663879395 and perplexity is 86.45271904655489
At time: 20.872565269470215 and batch: 250, loss is 4.485784664154052 and perplexity is 88.74655974379911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.859864044189453 and perplexity of 129.006661709787
Finished 4 epochs...
Completing Train Step...
At time: 22.4437096118927 and batch: 50, loss is 4.43675238609314 and perplexity is 84.50007197121664
At time: 23.37746262550354 and batch: 100, loss is 4.337428379058838 and perplexity is 76.5105300489676
At time: 24.30984330177307 and batch: 150, loss is 4.382088661193848 and perplexity is 80.00496227549203
At time: 25.247154474258423 and batch: 200, loss is 4.369804105758667 and perplexity is 79.02814902383348
At time: 26.183335304260254 and batch: 250, loss is 4.409413604736328 and perplexity is 82.22123522225804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.848734664916992 and perplexity of 127.57885766695823
Finished 5 epochs...
Completing Train Step...
At time: 27.729027271270752 and batch: 50, loss is 4.359106035232544 and perplexity is 78.18720656328642
At time: 28.684807062149048 and batch: 100, loss is 4.256790599822998 and perplexity is 70.58309017127098
At time: 29.621387243270874 and batch: 150, loss is 4.300799360275269 and perplexity is 73.75872993948099
At time: 30.55962634086609 and batch: 200, loss is 4.293599405288696 and perplexity is 73.2295776263751
At time: 31.4964497089386 and batch: 250, loss is 4.341150398254395 and perplexity is 76.79583433511681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.818603515625 and perplexity of 123.79209628767109
Finished 6 epochs...
Completing Train Step...
At time: 33.04348063468933 and batch: 50, loss is 4.294931554794312 and perplexity is 73.32719537829027
At time: 33.998353004455566 and batch: 100, loss is 4.199024381637574 and perplexity is 66.62130235858508
At time: 34.934560775756836 and batch: 150, loss is 4.247846021652221 and perplexity is 69.95456932105778
At time: 35.875495195388794 and batch: 200, loss is 4.242301654815674 and perplexity is 69.56778874435292
At time: 36.81148362159729 and batch: 250, loss is 4.277251644134521 and perplexity is 72.04217015316672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.831097412109375 and perplexity of 125.34844410341712
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 38.37472653388977 and batch: 50, loss is 4.210928249359131 and perplexity is 67.4190925027855
At time: 39.3169219493866 and batch: 100, loss is 4.029309639930725 and perplexity is 56.222084364348596
At time: 40.268524169921875 and batch: 150, loss is 4.022953720092773 and perplexity is 55.86587452277672
At time: 41.20503759384155 and batch: 200, loss is 3.9413064670562745 and perplexity is 51.485821910400595
At time: 42.14200758934021 and batch: 250, loss is 3.9766046142578126 and perplexity is 53.33563139308711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.649768829345703 and perplexity of 104.56081139185747
Finished 8 epochs...
Completing Train Step...
At time: 43.69773244857788 and batch: 50, loss is 4.059147353172302 and perplexity is 57.924900534395384
At time: 44.654057025909424 and batch: 100, loss is 3.9275445985794066 and perplexity is 50.78213393737612
At time: 45.59285616874695 and batch: 150, loss is 3.948550887107849 and perplexity is 51.86016112913188
At time: 46.528329610824585 and batch: 200, loss is 3.9012761640548708 and perplexity is 49.46553498070259
At time: 47.464680671691895 and batch: 250, loss is 3.9594482135772706 and perplexity is 52.428388693194314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.631398010253906 and perplexity of 102.65748004840735
Finished 9 epochs...
Completing Train Step...
At time: 49.03081703186035 and batch: 50, loss is 4.009592742919922 and perplexity is 55.12441618264582
At time: 49.9696409702301 and batch: 100, loss is 3.88365122795105 and perplexity is 48.60134609500899
At time: 50.90570950508118 and batch: 150, loss is 3.915273814201355 and perplexity is 50.162804919116574
At time: 51.84694147109985 and batch: 200, loss is 3.8813432359695437 and perplexity is 48.48930392388334
At time: 52.782655000686646 and batch: 250, loss is 3.941918125152588 and perplexity is 51.51732326326409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.623221588134766 and perplexity of 101.82153134975529
Finished 10 epochs...
Completing Train Step...
At time: 54.342209339141846 and batch: 50, loss is 3.9743160247802733 and perplexity is 53.213707598276
At time: 55.282057762145996 and batch: 100, loss is 3.8531265878677368 and perplexity is 47.14022110407723
At time: 56.21955680847168 and batch: 150, loss is 3.891424994468689 and perplexity is 48.98063394953238
At time: 57.15562701225281 and batch: 200, loss is 3.8654276847839357 and perplexity is 47.72367875987887
At time: 58.09626746177673 and batch: 250, loss is 3.926114912033081 and perplexity is 50.70958327840051
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618743896484375 and perplexity of 101.3666251538996
Finished 11 epochs...
Completing Train Step...
At time: 59.66132426261902 and batch: 50, loss is 3.9470798349380494 and perplexity is 51.78392821162337
At time: 60.65299892425537 and batch: 100, loss is 3.829330630302429 and perplexity is 46.031715683918726
At time: 61.59951305389404 and batch: 150, loss is 3.8721227264404297 and perplexity is 48.04426274125627
At time: 62.54052782058716 and batch: 200, loss is 3.851375308036804 and perplexity is 47.05773763253327
At time: 63.48533487319946 and batch: 250, loss is 3.9108001470565794 and perplexity is 49.93889445075257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.615848159790039 and perplexity of 101.07351868225828
Finished 12 epochs...
Completing Train Step...
At time: 65.04359698295593 and batch: 50, loss is 3.9240056562423704 and perplexity is 50.60273651941164
At time: 65.98008704185486 and batch: 100, loss is 3.809231743812561 and perplexity is 45.11576508317372
At time: 66.91972351074219 and batch: 150, loss is 3.8552160835266114 and perplexity is 47.238823370061226
At time: 67.85542917251587 and batch: 200, loss is 3.838099603652954 and perplexity is 46.43714155844633
At time: 68.79527688026428 and batch: 250, loss is 3.8960854482650755 and perplexity is 49.20943868378492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.614319610595703 and perplexity of 100.91914085381865
Finished 13 epochs...
Completing Train Step...
At time: 70.3443706035614 and batch: 50, loss is 3.9036052083969115 and perplexity is 49.58087667089214
At time: 71.29877042770386 and batch: 100, loss is 3.7910739469528196 and perplexity is 44.30395483316778
At time: 72.24065661430359 and batch: 150, loss is 3.8396056270599366 and perplexity is 46.50712966922486
At time: 73.17778754234314 and batch: 200, loss is 3.8254512548446655 and perplexity is 45.853487306733356
At time: 74.11418747901917 and batch: 250, loss is 3.882008767127991 and perplexity is 48.521585807604275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613383865356445 and perplexity of 100.82475041780276
Finished 14 epochs...
Completing Train Step...
At time: 75.66273188591003 and batch: 50, loss is 3.8850741529464723 and perplexity is 48.670551390474365
At time: 76.61545205116272 and batch: 100, loss is 3.774087462425232 and perplexity is 43.557742098062576
At time: 77.55410647392273 and batch: 150, loss is 3.8251004314422605 and perplexity is 45.83740365173052
At time: 78.48897171020508 and batch: 200, loss is 3.81341682434082 and perplexity is 45.30497384392675
At time: 79.42602968215942 and batch: 250, loss is 3.868445048332214 and perplexity is 47.86789591693652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613026428222656 and perplexity of 100.78871834798194
Finished 15 epochs...
Completing Train Step...
At time: 80.98639273643494 and batch: 50, loss is 3.8681711387634277 and perplexity is 47.85478623772245
At time: 81.92480301856995 and batch: 100, loss is 3.7582451629638673 and perplexity is 42.873124580377194
At time: 82.86396217346191 and batch: 150, loss is 3.8114948511123656 and perplexity is 45.217982521358124
At time: 83.80130934715271 and batch: 200, loss is 3.8017231941223146 and perplexity is 44.778279717621686
At time: 84.73966598510742 and batch: 250, loss is 3.855214433670044 and perplexity is 47.23874543284254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612847900390625 and perplexity of 100.7707263626849
Finished 16 epochs...
Completing Train Step...
At time: 86.29995465278625 and batch: 50, loss is 3.8523507070541383 and perplexity is 47.103660096300125
At time: 87.25126624107361 and batch: 100, loss is 3.7432803964614867 and perplexity is 42.23631501910804
At time: 88.18784666061401 and batch: 150, loss is 3.7986057567596436 and perplexity is 44.638903596433906
At time: 89.13909101486206 and batch: 200, loss is 3.790356478691101 and perplexity is 44.27217955194732
At time: 90.08379435539246 and batch: 250, loss is 3.842499122619629 and perplexity is 46.64189271653306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613234329223633 and perplexity of 100.80967460175121
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 91.64673447608948 and batch: 50, loss is 3.8417900371551514 and perplexity is 46.60883135142405
At time: 92.58175206184387 and batch: 100, loss is 3.721290502548218 and perplexity is 41.3176802899398
At time: 93.5175313949585 and batch: 150, loss is 3.760704975128174 and perplexity is 42.97871422584829
At time: 94.45301175117493 and batch: 200, loss is 3.7361915349960326 and perplexity is 41.93796635867005
At time: 95.39265775680542 and batch: 250, loss is 3.7850441455841066 and perplexity is 44.03761458205797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.596559143066406 and perplexity of 99.14259259188879
Finished 18 epochs...
Completing Train Step...
At time: 96.96028900146484 and batch: 50, loss is 3.8235203886032103 and perplexity is 45.76503577755638
At time: 97.89810562133789 and batch: 100, loss is 3.7043725728988646 and perplexity is 40.624550375102295
At time: 98.83296227455139 and batch: 150, loss is 3.7500771427154542 and perplexity is 42.524362318317024
At time: 99.7715311050415 and batch: 200, loss is 3.7329295587539675 and perplexity is 41.80138858659589
At time: 100.70984816551208 and batch: 250, loss is 3.7874425745010374 and perplexity is 44.14336243396466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.594036102294922 and perplexity of 98.89276708109047
Finished 19 epochs...
Completing Train Step...
At time: 102.25871729850769 and batch: 50, loss is 3.816886487007141 and perplexity is 45.46243983924231
At time: 103.2104344367981 and batch: 100, loss is 3.6982541370391844 and perplexity is 40.37675051611266
At time: 104.14947748184204 and batch: 150, loss is 3.7460985469818113 and perplexity is 42.35551118974317
At time: 105.08667635917664 and batch: 200, loss is 3.7325531435012818 and perplexity is 41.78565686736374
At time: 106.03364849090576 and batch: 250, loss is 3.787721929550171 and perplexity is 44.15569582776403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.592947006225586 and perplexity of 98.78512198574191
Finished 20 epochs...
Completing Train Step...
At time: 107.59849643707275 and batch: 50, loss is 3.8121104288101195 and perplexity is 45.245826272052426
At time: 108.53575778007507 and batch: 100, loss is 3.6941228437423708 and perplexity is 40.210286410142515
At time: 109.47267198562622 and batch: 150, loss is 3.7435752820968626 and perplexity is 42.24877173826189
At time: 110.40881872177124 and batch: 200, loss is 3.7321870470047 and perplexity is 41.77036208463124
At time: 111.34692716598511 and batch: 250, loss is 3.787190999984741 and perplexity is 44.13225848570641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.592326736450195 and perplexity of 98.72386755941436
Finished 21 epochs...
Completing Train Step...
At time: 112.89651775360107 and batch: 50, loss is 3.8081527519226075 and perplexity is 45.067111791513774
At time: 113.84729075431824 and batch: 100, loss is 3.690794596672058 and perplexity is 40.076679104596344
At time: 114.78105545043945 and batch: 150, loss is 3.741515107154846 and perplexity is 42.16182147452675
At time: 115.72006940841675 and batch: 200, loss is 3.7316363048553467 and perplexity is 41.74736371930338
At time: 116.65736389160156 and batch: 250, loss is 3.786309781074524 and perplexity is 44.09338543532732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.591918182373047 and perplexity of 98.68354175900716
Finished 22 epochs...
Completing Train Step...
At time: 118.20628571510315 and batch: 50, loss is 3.804667172431946 and perplexity is 44.910300239277156
At time: 119.1582202911377 and batch: 100, loss is 3.6878769493103025 and perplexity is 39.95991990145707
At time: 120.09569144248962 and batch: 150, loss is 3.7396468257904054 and perplexity is 42.08312486576953
At time: 121.03276872634888 and batch: 200, loss is 3.730936803817749 and perplexity is 41.718171606211776
At time: 121.97195386886597 and batch: 250, loss is 3.785247745513916 and perplexity is 44.046581550101905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5916187286376955 and perplexity of 98.65399502796998
Finished 23 epochs...
Completing Train Step...
At time: 123.536376953125 and batch: 50, loss is 3.801488928794861 and perplexity is 44.76779094788633
At time: 124.47407793998718 and batch: 100, loss is 3.6852043294906616 and perplexity is 39.85326481527129
At time: 125.41067600250244 and batch: 150, loss is 3.7378729391098022 and perplexity is 42.00854034289559
At time: 126.34938144683838 and batch: 200, loss is 3.7301270246505736 and perplexity is 41.68440277444595
At time: 127.28529596328735 and batch: 250, loss is 3.7840807247161865 and perplexity is 43.995208256045956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.591386032104492 and perplexity of 98.63104125607549
Finished 24 epochs...
Completing Train Step...
At time: 128.83601307868958 and batch: 50, loss is 3.7985281991958617 and perplexity is 44.635441646073026
At time: 129.78841590881348 and batch: 100, loss is 3.6826963901519774 and perplexity is 39.75344047368527
At time: 130.7254626750946 and batch: 150, loss is 3.736150426864624 and perplexity is 41.93624240267251
At time: 131.6633322238922 and batch: 200, loss is 3.729233841896057 and perplexity is 41.6471876072024
At time: 132.59973430633545 and batch: 250, loss is 3.7828475618362427 and perplexity is 43.9409884361348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.591194534301758 and perplexity of 98.61215543674773
Finished 25 epochs...
Completing Train Step...
At time: 134.15940117835999 and batch: 50, loss is 3.795730695724487 and perplexity is 44.51074833942198
At time: 135.09634566307068 and batch: 100, loss is 3.6803057622909545 and perplexity is 39.65851829832337
At time: 136.03340935707092 and batch: 150, loss is 3.734457173347473 and perplexity is 41.86529379666786
At time: 136.96835207939148 and batch: 200, loss is 3.7282746267318725 and perplexity is 41.60725814683744
At time: 137.92034220695496 and batch: 250, loss is 3.7815707445144655 and perplexity is 43.884919623396435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.591033935546875 and perplexity of 98.59631971900055
Finished 26 epochs...
Completing Train Step...
At time: 139.48776865005493 and batch: 50, loss is 3.7930611181259155 and perplexity is 44.39208190785755
At time: 140.42381978034973 and batch: 100, loss is 3.6780010318756102 and perplexity is 39.56722135280977
At time: 141.3517611026764 and batch: 150, loss is 3.7327818489074707 and perplexity is 41.79521456589746
At time: 142.3080554008484 and batch: 200, loss is 3.7272627353668213 and perplexity is 41.565177415750036
At time: 143.2596788406372 and batch: 250, loss is 3.780262951850891 and perplexity is 43.827564759780934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590889739990234 and perplexity of 98.58210359277169
Finished 27 epochs...
Completing Train Step...
At time: 144.81358551979065 and batch: 50, loss is 3.7904933738708495 and perplexity is 44.278240614780614
At time: 145.777911901474 and batch: 100, loss is 3.6757609701156615 and perplexity is 39.47868753094531
At time: 146.71389293670654 and batch: 150, loss is 3.7311194372177123 and perplexity is 41.72579143352877
At time: 147.65013360977173 and batch: 200, loss is 3.726208038330078 and perplexity is 41.52136185642987
At time: 148.58654236793518 and batch: 250, loss is 3.778932504653931 and perplexity is 43.76929327126145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590754699707031 and perplexity of 98.56879193640894
Finished 28 epochs...
Completing Train Step...
At time: 150.1577889919281 and batch: 50, loss is 3.788010663986206 and perplexity is 44.16844693845038
At time: 151.09345769882202 and batch: 100, loss is 3.673571524620056 and perplexity is 39.39234565126925
At time: 152.0303988456726 and batch: 150, loss is 3.7294708395004275 and perplexity is 41.65705906060327
At time: 152.96875262260437 and batch: 200, loss is 3.7251219177246093 and perplexity is 41.47628913138905
At time: 153.90629196166992 and batch: 250, loss is 3.7775862264633178 and perplexity is 43.71040727367301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.59063606262207 and perplexity of 98.55709871590396
Finished 29 epochs...
Completing Train Step...
At time: 155.45908999443054 and batch: 50, loss is 3.785601382255554 and perplexity is 44.062160794215785
At time: 156.41266703605652 and batch: 100, loss is 3.671426634788513 and perplexity is 39.30794395816573
At time: 157.34955048561096 and batch: 150, loss is 3.72783607006073 and perplexity is 41.58901500682697
At time: 158.2869656085968 and batch: 200, loss is 3.724012894630432 and perplexity is 41.43031646596734
At time: 159.22409462928772 and batch: 250, loss is 3.776228928565979 and perplexity is 43.651119474493164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590537643432617 and perplexity of 98.54739928344645
Finished 30 epochs...
Completing Train Step...
At time: 160.77455282211304 and batch: 50, loss is 3.783251333236694 and perplexity is 43.95873413293422
At time: 161.7259967327118 and batch: 100, loss is 3.6693195867538453 and perplexity is 39.22520742763398
At time: 162.66448950767517 and batch: 150, loss is 3.726209969520569 and perplexity is 41.52144204216649
At time: 163.59971356391907 and batch: 200, loss is 3.722880868911743 and perplexity is 41.38344281828743
At time: 164.5370535850525 and batch: 250, loss is 3.7748591995239256 and perplexity is 43.59137019793207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590449523925781 and perplexity of 98.53871571782305
Finished 31 epochs...
Completing Train Step...
At time: 166.10551190376282 and batch: 50, loss is 3.7809458017349242 and perplexity is 43.85750262766612
At time: 167.04266667366028 and batch: 100, loss is 3.6672344398498535 and perplexity is 39.14350232098627
At time: 167.97996592521667 and batch: 150, loss is 3.7245752382278443 and perplexity is 41.45362109115998
At time: 168.91687560081482 and batch: 200, loss is 3.7217068099975585 and perplexity is 41.33488472895356
At time: 169.8527491092682 and batch: 250, loss is 3.773439464569092 and perplexity is 43.52952591756429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590303802490235 and perplexity of 98.52435756088276
Finished 32 epochs...
Completing Train Step...
At time: 171.40563106536865 and batch: 50, loss is 3.7786610984802245 and perplexity is 43.757415626754806
At time: 172.355633020401 and batch: 100, loss is 3.6651032638549803 and perplexity is 39.060169458523426
At time: 173.2935085296631 and batch: 150, loss is 3.722881259918213 and perplexity is 41.38345899948448
At time: 174.2294843196869 and batch: 200, loss is 3.7204559993743898 and perplexity is 41.28321493732583
At time: 175.1650035381317 and batch: 250, loss is 3.7719686365127565 and perplexity is 43.46554853096706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590125274658203 and perplexity of 98.50676979092506
Finished 33 epochs...
Completing Train Step...
At time: 176.7301950454712 and batch: 50, loss is 3.776425075531006 and perplexity is 43.65968234886158
At time: 177.66626381874084 and batch: 100, loss is 3.6630389213562013 and perplexity is 38.97961906111055
At time: 178.6022264957428 and batch: 150, loss is 3.7212393951416014 and perplexity is 41.31556870441206
At time: 179.53989338874817 and batch: 200, loss is 3.719234519004822 and perplexity is 41.2328190857249
At time: 180.47729420661926 and batch: 250, loss is 3.7705393409729004 and perplexity is 43.40346779274414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.590039825439453 and perplexity of 98.49835282402152
Finished 34 epochs...
Completing Train Step...
At time: 182.04336071014404 and batch: 50, loss is 3.7742219829559325 and perplexity is 43.5636019027689
At time: 182.9801743030548 and batch: 100, loss is 3.661033577919006 and perplexity is 38.901529861842626
At time: 183.914715051651 and batch: 150, loss is 3.7196276426315307 and perplexity is 41.249031867708325
At time: 184.8527843952179 and batch: 200, loss is 3.7180050134658815 and perplexity is 41.18215425899785
At time: 185.79051899909973 and batch: 250, loss is 3.7691195964813233 and perplexity is 43.34188968136571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589968872070313 and perplexity of 98.49136428196715
Finished 35 epochs...
Completing Train Step...
At time: 187.35350275039673 and batch: 50, loss is 3.7720537042617797 and perplexity is 43.469246204614805
At time: 188.30847597122192 and batch: 100, loss is 3.659058856964111 and perplexity is 38.824785994447225
At time: 189.24568009376526 and batch: 150, loss is 3.718030333518982 and perplexity is 41.183197006531636
At time: 190.1825647354126 and batch: 200, loss is 3.7167676877975464 and perplexity is 41.13123003387827
At time: 191.1209852695465 and batch: 250, loss is 3.767704496383667 and perplexity is 43.280599944828836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589908218383789 and perplexity of 98.48539059879745
Finished 36 epochs...
Completing Train Step...
At time: 192.6816918849945 and batch: 50, loss is 3.7699159002304077 and perplexity is 43.37641673579836
At time: 193.62130403518677 and batch: 100, loss is 3.6571070337295533 and perplexity is 38.74908078069808
At time: 194.55716943740845 and batch: 150, loss is 3.7164422273635864 and perplexity is 41.117845624068195
At time: 195.4951572418213 and batch: 200, loss is 3.715521593093872 and perplexity is 41.080008546017666
At time: 196.43239879608154 and batch: 250, loss is 3.7662904691696166 and perplexity is 43.21944324747372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589850234985351 and perplexity of 98.47968024670853
Finished 37 epochs...
Completing Train Step...
At time: 197.97996520996094 and batch: 50, loss is 3.767805047035217 and perplexity is 43.284952056152925
At time: 198.93774318695068 and batch: 100, loss is 3.655171856880188 and perplexity is 38.67416696575606
At time: 199.8748073577881 and batch: 150, loss is 3.7148596906661986 and perplexity is 41.052826585527434
At time: 200.81250762939453 and batch: 200, loss is 3.7142660093307494 and perplexity is 41.028461521873574
At time: 201.74888014793396 and batch: 250, loss is 3.7648758268356324 and perplexity is 43.15834641867384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589791870117187 and perplexity of 98.47393266088424
Finished 38 epochs...
Completing Train Step...
At time: 203.30124187469482 and batch: 50, loss is 3.7657197427749636 and perplexity is 43.19478380801602
At time: 204.25621724128723 and batch: 100, loss is 3.653249969482422 and perplexity is 38.599910950351905
At time: 205.1925187110901 and batch: 150, loss is 3.7132826471328735 and perplexity is 40.98813551456116
At time: 206.13219809532166 and batch: 200, loss is 3.7130029439926147 and perplexity is 40.976672607524655
At time: 207.0690474510193 and batch: 250, loss is 3.7634644317626953 and perplexity is 43.097475907456904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589743804931641 and perplexity of 98.4691996067879
Finished 39 epochs...
Completing Train Step...
At time: 208.64720177650452 and batch: 50, loss is 3.7636604070663453 and perplexity is 43.1059227760462
At time: 209.5857813358307 and batch: 100, loss is 3.6513453197479246 and perplexity is 38.52646161004623
At time: 210.53860187530518 and batch: 150, loss is 3.711713604927063 and perplexity is 40.92387382783447
At time: 211.47585582733154 and batch: 200, loss is 3.711737961769104 and perplexity is 40.92487061630426
At time: 212.41319727897644 and batch: 250, loss is 3.7620624446868898 and perplexity is 43.0370961389524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589714431762696 and perplexity of 98.46630729683031
Finished 40 epochs...
Completing Train Step...
At time: 213.95932507514954 and batch: 50, loss is 3.761623044013977 and perplexity is 43.01818976398987
At time: 214.91366052627563 and batch: 100, loss is 3.6494617462158203 and perplexity is 38.45396248682138
At time: 215.85100603103638 and batch: 150, loss is 3.7101538038253783 and perplexity is 40.860090481951154
At time: 216.78886127471924 and batch: 200, loss is 3.710473685264587 and perplexity is 40.873162957210354
At time: 217.72584295272827 and batch: 250, loss is 3.7606676626205444 and perplexity is 42.97711061216344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589699172973633 and perplexity of 98.46480483168044
Finished 41 epochs...
Completing Train Step...
At time: 219.288747549057 and batch: 50, loss is 3.759602198600769 and perplexity is 42.93134443257064
At time: 220.22895050048828 and batch: 100, loss is 3.64759783744812 and perplexity is 38.382354565037424
At time: 221.16928219795227 and batch: 150, loss is 3.708602104187012 and perplexity is 40.79673705977954
At time: 222.1081235408783 and batch: 200, loss is 3.7092093563079835 and perplexity is 40.82151848841384
At time: 223.04485416412354 and batch: 250, loss is 3.759276661872864 and perplexity is 42.91737097773929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589694595336914 and perplexity of 98.46435409660592
Finished 42 epochs...
Completing Train Step...
At time: 224.62013363838196 and batch: 50, loss is 3.7575967264175416 and perplexity is 42.84533309102658
At time: 225.55871534347534 and batch: 100, loss is 3.6457510805130005 and perplexity is 38.31153709701031
At time: 226.49510407447815 and batch: 150, loss is 3.7070572710037233 and perplexity is 40.73376156244301
At time: 227.4323616027832 and batch: 200, loss is 3.7079437017440795 and perplexity is 40.76988522906119
At time: 228.36953353881836 and batch: 250, loss is 3.757888412475586 and perplexity is 42.8578323001754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.589697265625 and perplexity of 98.46461702514867
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 229.9216148853302 and batch: 50, loss is 3.7565280532836915 and perplexity is 42.7995698919474
At time: 230.87419533729553 and batch: 100, loss is 3.642647271156311 and perplexity is 38.19280973855711
At time: 231.8140926361084 and batch: 150, loss is 3.7008434963226318 and perplexity is 40.48143590544831
At time: 232.74922060966492 and batch: 200, loss is 3.696585326194763 and perplexity is 40.309425548931145
At time: 233.6844778060913 and batch: 250, loss is 3.7456893062591554 and perplexity is 42.33818113605913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.588526916503906 and perplexity of 98.34944645518802
Finished 44 epochs...
Completing Train Step...
At time: 235.24930453300476 and batch: 50, loss is 3.754313054084778 and perplexity is 42.704873793530616
At time: 236.18678879737854 and batch: 100, loss is 3.640530424118042 and perplexity is 38.112046913810524
At time: 237.1269178390503 and batch: 150, loss is 3.699304447174072 and perplexity is 40.41918090502673
At time: 238.05966138839722 and batch: 200, loss is 3.6959844875335692 and perplexity is 40.28521336218817
At time: 238.99704813957214 and batch: 250, loss is 3.7462187147140504 and perplexity is 42.36060126129572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.588360595703125 and perplexity of 98.33309025672288
Finished 45 epochs...
Completing Train Step...
At time: 240.54075574874878 and batch: 50, loss is 3.753253307342529 and perplexity is 42.659641414316276
At time: 241.48844957351685 and batch: 100, loss is 3.63958092212677 and perplexity is 38.07587662397298
At time: 242.4262192249298 and batch: 150, loss is 3.6987440299987795 and perplexity is 40.3965356478245
At time: 243.360666513443 and batch: 200, loss is 3.695861849784851 and perplexity is 40.28027317724758
At time: 244.2947404384613 and batch: 250, loss is 3.7464370346069336 and perplexity is 42.36985043282788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5883033752441404 and perplexity of 98.32746375314214
Finished 46 epochs...
Completing Train Step...
At time: 245.85242629051208 and batch: 50, loss is 3.7525176572799683 and perplexity is 42.62827038690493
At time: 246.8024594783783 and batch: 100, loss is 3.6389683866500855 and perplexity is 38.05256094030564
At time: 247.74276685714722 and batch: 150, loss is 3.6984334802627563 and perplexity is 40.38399246208514
At time: 248.67937588691711 and batch: 200, loss is 3.6957859134674074 and perplexity is 40.27721455776849
At time: 249.6172158718109 and batch: 250, loss is 3.746472806930542 and perplexity is 42.37136612793862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.588276672363281 and perplexity of 98.32483816164792
Finished 47 epochs...
Completing Train Step...
At time: 251.1773648262024 and batch: 50, loss is 3.7519311475753785 and perplexity is 42.60327582312674
At time: 252.11227560043335 and batch: 100, loss is 3.638504605293274 and perplexity is 38.034916963752536
At time: 253.05399703979492 and batch: 150, loss is 3.6982169055938723 and perplexity is 40.37524725931832
At time: 253.98937582969666 and batch: 200, loss is 3.6956920051574706 and perplexity is 40.2734323702126
At time: 254.92491722106934 and batch: 250, loss is 3.746407012939453 and perplexity is 42.368578438360785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.588260650634766 and perplexity of 98.32326284040428
Finished 48 epochs...
Completing Train Step...
At time: 256.4729142189026 and batch: 50, loss is 3.7514263439178466 and perplexity is 42.58177496098189
At time: 257.42468547821045 and batch: 100, loss is 3.6381190395355225 and perplexity is 38.02025482896259
At time: 258.3658664226532 and batch: 150, loss is 3.698041081428528 and perplexity is 40.36814893921477
At time: 259.31998777389526 and batch: 200, loss is 3.695572943687439 and perplexity is 40.26863764159078
At time: 260.2579708099365 and batch: 250, loss is 3.746283268928528 and perplexity is 42.36333590490037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.588249588012696 and perplexity of 98.32217513332326
Finished 49 epochs...
Completing Train Step...
At time: 261.8175346851349 and batch: 50, loss is 3.7509718990325926 and perplexity is 42.5624282874772
At time: 262.75343799591064 and batch: 100, loss is 3.6377785444259643 and perplexity is 38.007311321854665
At time: 263.6933708190918 and batch: 150, loss is 3.697884588241577 and perplexity is 40.361832093220535
At time: 264.6318943500519 and batch: 200, loss is 3.6954322052001953 and perplexity is 40.26297069323395
At time: 265.56849002838135 and batch: 250, loss is 3.7461244297027587 and perplexity is 42.356607479807295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5882408142089846 and perplexity of 98.32131247764258
Finished Training.
Improved accuracyfrom -131.19747669256088 to -98.32131247764258
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.34062417461774785, 'wordvec_source': 'glove', 'anneal': 6.266342664128619, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 28.299008542620673}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2396588325500488 and batch: 50, loss is 6.85145824432373 and perplexity is 945.2583197042914
At time: 2.1819827556610107 and batch: 100, loss is 5.815876140594482 and perplexity is 335.5852896324897
At time: 3.124680519104004 and batch: 150, loss is 5.796948499679566 and perplexity is 329.2931869513637
At time: 4.066264390945435 and batch: 200, loss is 5.835725479125976 and perplexity is 342.3129849140817
At time: 5.004892826080322 and batch: 250, loss is 5.918396234512329 and perplexity is 371.81493149966633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.040711212158203 and perplexity of 420.1917741391327
Finished 1 epochs...
Completing Train Step...
At time: 6.5840795040130615 and batch: 50, loss is 5.779946937561035 and perplexity is 323.7420114207456
At time: 7.517130374908447 and batch: 100, loss is 5.8110207843780515 and perplexity is 333.95985274112246
At time: 8.462259292602539 and batch: 150, loss is 5.866822881698608 and perplexity is 353.12527536624185
At time: 9.393396139144897 and batch: 200, loss is 5.8853291988372805 and perplexity is 359.7211682972634
At time: 10.323571920394897 and batch: 250, loss is 5.9893434715271 and perplexity is 399.1524689241325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.0279991149902346 and perplexity of 414.8840630226613
Finished 2 epochs...
Completing Train Step...
At time: 11.892449855804443 and batch: 50, loss is 5.880517635345459 and perplexity is 357.99450436440566
At time: 12.824447393417358 and batch: 100, loss is 6.3514546871185305 and perplexity is 573.3261128068423
At time: 13.755980014801025 and batch: 150, loss is 6.096556825637817 and perplexity is 444.32524393083156
At time: 14.684311866760254 and batch: 200, loss is 6.335508604049682 and perplexity is 564.2563130629533
At time: 15.616029739379883 and batch: 250, loss is 6.308462162017822 and perplexity is 549.1997188777759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.667630767822265 and perplexity of 786.5299232164888
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 17.182406425476074 and batch: 50, loss is 5.5606620979309085 and perplexity is 259.99492144759904
At time: 18.116528272628784 and batch: 100, loss is 5.305544862747192 and perplexity is 201.45073551741916
At time: 19.04477095603943 and batch: 150, loss is 5.280410938262939 and perplexity is 196.45058783005598
At time: 19.98851490020752 and batch: 200, loss is 5.28928318977356 and perplexity is 198.2013017576295
At time: 20.918179035186768 and batch: 250, loss is 5.354505529403687 and perplexity is 211.5593406120694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.599801254272461 and perplexity of 270.37266667361234
Finished 4 epochs...
Completing Train Step...
At time: 22.473620414733887 and batch: 50, loss is 5.3182454013824465 and perplexity is 204.02558474202516
At time: 23.423144340515137 and batch: 100, loss is 5.266772184371948 and perplexity is 193.7894352645531
At time: 24.35440444946289 and batch: 150, loss is 5.27417140007019 and perplexity is 195.2286430342907
At time: 25.284138917922974 and batch: 200, loss is 5.273139314651489 and perplexity is 195.0272543415455
At time: 26.217897653579712 and batch: 250, loss is 5.305110483169556 and perplexity is 201.36324843468725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.566112518310547 and perplexity of 261.4158719371235
Finished 5 epochs...
Completing Train Step...
At time: 27.78359341621399 and batch: 50, loss is 5.283702211380005 and perplexity is 197.09822556031168
At time: 28.718053817749023 and batch: 100, loss is 5.257524261474609 and perplexity is 192.00554685391327
At time: 29.648046016693115 and batch: 150, loss is 5.25022253036499 and perplexity is 190.60867995865317
At time: 30.578211069107056 and batch: 200, loss is 5.243523588180542 and perplexity is 189.33607075859575
At time: 31.51114845275879 and batch: 250, loss is 5.285141859054566 and perplexity is 197.3821819119066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.561115646362305 and perplexity of 260.112868481717
Finished 6 epochs...
Completing Train Step...
At time: 33.065035581588745 and batch: 50, loss is 5.2667942810058594 and perplexity is 193.79371740607024
At time: 34.029815912246704 and batch: 100, loss is 5.229763450622559 and perplexity is 186.7486230244253
At time: 35.009024143218994 and batch: 150, loss is 5.232335605621338 and perplexity is 187.22958772135632
At time: 35.95196509361267 and batch: 200, loss is 5.2331974315643315 and perplexity is 187.39101658915197
At time: 36.89259910583496 and batch: 250, loss is 5.261245336532593 and perplexity is 192.72134484840257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.547210311889648 and perplexity of 256.52094338199174
Finished 7 epochs...
Completing Train Step...
At time: 38.476672410964966 and batch: 50, loss is 5.2454337215423585 and perplexity is 189.69807353066574
At time: 39.41728901863098 and batch: 100, loss is 5.215466041564941 and perplexity is 184.0975981229681
At time: 40.35419487953186 and batch: 150, loss is 5.207266149520874 and perplexity is 182.5941900087705
At time: 41.29194903373718 and batch: 200, loss is 5.200526390075684 and perplexity is 181.36768690272496
At time: 42.22781729698181 and batch: 250, loss is 5.224342126846313 and perplexity is 185.73893766282026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.504892730712891 and perplexity of 245.89207759619867
Finished 8 epochs...
Completing Train Step...
At time: 43.801544427871704 and batch: 50, loss is 5.199842882156372 and perplexity is 181.2437630087258
At time: 44.73916792869568 and batch: 100, loss is 5.167893533706665 and perplexity is 175.54466881919134
At time: 45.67836093902588 and batch: 150, loss is 5.1741835880279545 and perplexity is 176.65233430788066
At time: 46.613505125045776 and batch: 200, loss is 5.172153644561767 and perplexity is 176.29410377294505
At time: 47.54985737800598 and batch: 250, loss is 5.201324081420898 and perplexity is 181.51242005538583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4987945556640625 and perplexity of 244.39714746944935
Finished 9 epochs...
Completing Train Step...
At time: 49.1054847240448 and batch: 50, loss is 5.1836707878112795 and perplexity is 178.3362454661468
At time: 50.05972337722778 and batch: 100, loss is 5.158495054244995 and perplexity is 173.90254467727738
At time: 50.99637484550476 and batch: 150, loss is 5.157988843917846 and perplexity is 173.81453569065252
At time: 51.93306279182434 and batch: 200, loss is 5.15364444732666 and perplexity is 173.06105431035604
At time: 52.86656999588013 and batch: 250, loss is 5.187587890625 and perplexity is 179.03617683165038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4961498260498045 and perplexity of 243.75163707238536
Finished 10 epochs...
Completing Train Step...
At time: 54.43260216712952 and batch: 50, loss is 5.1715406322479245 and perplexity is 176.18606643396714
At time: 55.36971473693848 and batch: 100, loss is 5.144932689666748 and perplexity is 171.5599365259116
At time: 56.30879044532776 and batch: 150, loss is 5.146678895950317 and perplexity is 171.8597772807965
At time: 57.24658226966858 and batch: 200, loss is 5.143468914031982 and perplexity is 171.308994976792
At time: 58.18178915977478 and batch: 250, loss is 5.180735883712768 and perplexity is 177.81361300164434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.493810653686523 and perplexity of 243.18212633101845
Finished 11 epochs...
Completing Train Step...
At time: 59.76568150520325 and batch: 50, loss is 5.159832372665405 and perplexity is 174.1352633284166
At time: 60.70160388946533 and batch: 100, loss is 5.1352846145629885 and perplexity is 169.91267261447206
At time: 61.656681060791016 and batch: 150, loss is 5.137118482589722 and perplexity is 170.2245559211215
At time: 62.595574378967285 and batch: 200, loss is 5.133573961257935 and perplexity is 169.62225940846946
At time: 63.53120541572571 and batch: 250, loss is 5.168898839950561 and perplexity is 175.72123370685966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.486218643188477 and perplexity of 241.34287571428575
Finished 12 epochs...
Completing Train Step...
At time: 65.09001731872559 and batch: 50, loss is 5.150119838714599 and perplexity is 172.45215552323143
At time: 66.04378843307495 and batch: 100, loss is 5.119978666305542 and perplexity is 167.33179977753568
At time: 66.98018145561218 and batch: 150, loss is 5.123196411132812 and perplexity is 167.87109800743337
At time: 67.91769862174988 and batch: 200, loss is 5.122496719360352 and perplexity is 167.7536810639486
At time: 68.85256361961365 and batch: 250, loss is 5.15688814163208 and perplexity is 173.62332288740842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.477818298339844 and perplexity of 239.3240038135382
Finished 13 epochs...
Completing Train Step...
At time: 70.42801713943481 and batch: 50, loss is 5.131622772216797 and perplexity is 169.29161699260797
At time: 71.36629867553711 and batch: 100, loss is 5.105909900665283 and perplexity is 164.99413052057474
At time: 72.30712914466858 and batch: 150, loss is 5.10940990447998 and perplexity is 165.57262237810846
At time: 73.246821641922 and batch: 200, loss is 5.107092323303223 and perplexity is 165.18933870217222
At time: 74.18172025680542 and batch: 250, loss is 5.142892112731934 and perplexity is 171.21021221753017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.469187927246094 and perplexity of 237.26743608530342
Finished 14 epochs...
Completing Train Step...
At time: 75.7426815032959 and batch: 50, loss is 5.115706901550293 and perplexity is 166.61852225436127
At time: 76.69382429122925 and batch: 100, loss is 5.084124326705933 and perplexity is 161.4385099319229
At time: 77.62285351753235 and batch: 150, loss is 5.084974756240845 and perplexity is 161.5758604040123
At time: 78.55635046958923 and batch: 200, loss is 5.0830307292938235 and perplexity is 161.26205769668627
At time: 79.5084228515625 and batch: 250, loss is 5.113161373138428 and perplexity is 166.19492943452872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.444701385498047 and perplexity of 231.52813173059846
Finished 15 epochs...
Completing Train Step...
At time: 81.07425165176392 and batch: 50, loss is 5.089058694839477 and perplexity is 162.23707555896095
At time: 82.04893493652344 and batch: 100, loss is 5.062579460144043 and perplexity is 157.99753950365664
At time: 82.98635530471802 and batch: 150, loss is 5.068799829483032 and perplexity is 158.98340560080175
At time: 83.95033359527588 and batch: 200, loss is 5.063935279846191 and perplexity is 158.21190096551587
At time: 84.88846278190613 and batch: 250, loss is 5.102489061355591 and perplexity is 164.43067640548276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.453143310546875 and perplexity of 233.4909481832101
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 86.48645257949829 and batch: 50, loss is 5.042888116836548 and perplexity is 154.91678733178622
At time: 87.42610025405884 and batch: 100, loss is 4.9526714515686034 and perplexity is 141.5526102075799
At time: 88.36694049835205 and batch: 150, loss is 4.937488698959351 and perplexity is 139.41968475794752
At time: 89.31279826164246 and batch: 200, loss is 4.920914421081543 and perplexity is 137.12794856011368
At time: 90.25101947784424 and batch: 250, loss is 4.977441816329956 and perplexity is 145.10270707466915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.334777450561523 and perplexity of 207.426581001693
Finished 17 epochs...
Completing Train Step...
At time: 91.83008551597595 and batch: 50, loss is 4.970201263427734 and perplexity is 144.05587763319966
At time: 92.78117513656616 and batch: 100, loss is 4.912204856872559 and perplexity is 135.93880984426573
At time: 93.71800947189331 and batch: 150, loss is 4.921890478134156 and perplexity is 137.26185860270567
At time: 94.65390849113464 and batch: 200, loss is 4.928759908676147 and perplexity is 138.2080154651798
At time: 95.59347558021545 and batch: 250, loss is 4.973193120956421 and perplexity is 144.487517676029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.329557037353515 and perplexity of 206.34655009515797
Finished 18 epochs...
Completing Train Step...
At time: 97.18446946144104 and batch: 50, loss is 4.954854431152344 and perplexity is 141.8619541886401
At time: 98.1228199005127 and batch: 100, loss is 4.908057956695557 and perplexity is 135.37625241060294
At time: 99.05859446525574 and batch: 150, loss is 4.9229779720306395 and perplexity is 137.411211231473
At time: 99.99635910987854 and batch: 200, loss is 4.922932434082031 and perplexity is 137.4049539492707
At time: 100.93603944778442 and batch: 250, loss is 4.963246698379517 and perplexity is 143.05750730125567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.323419189453125 and perplexity of 205.0839052807992
Finished 19 epochs...
Completing Train Step...
At time: 102.53232645988464 and batch: 50, loss is 4.948768396377563 and perplexity is 141.0011993510155
At time: 103.46931171417236 and batch: 100, loss is 4.903863716125488 and perplexity is 134.80964092359753
At time: 104.40912914276123 and batch: 150, loss is 4.916246223449707 and perplexity is 136.48930002396784
At time: 105.34710001945496 and batch: 200, loss is 4.9181657314300535 and perplexity is 136.75154393371452
At time: 106.28524160385132 and batch: 250, loss is 4.958918209075928 and perplexity is 142.4396226294149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.319833755493164 and perplexity of 204.34990711944718
Finished 20 epochs...
Completing Train Step...
At time: 107.85783076286316 and batch: 50, loss is 4.9439702987670895 and perplexity is 140.32628228712454
At time: 108.80974173545837 and batch: 100, loss is 4.900705318450928 and perplexity is 134.38453015501776
At time: 109.74669861793518 and batch: 150, loss is 4.912975702285767 and perplexity is 136.0436380502913
At time: 110.69957113265991 and batch: 200, loss is 4.914028072357178 and perplexity is 136.1868816628067
At time: 111.64119458198547 and batch: 250, loss is 4.954397459030151 and perplexity is 141.79714204017645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3186603546142575 and perplexity of 204.11026338540933
Finished 21 epochs...
Completing Train Step...
At time: 113.2111165523529 and batch: 50, loss is 4.940854759216308 and perplexity is 139.8897705425338
At time: 114.15602397918701 and batch: 100, loss is 4.896939554214478 and perplexity is 133.87942135466704
At time: 115.09255409240723 and batch: 150, loss is 4.909929151535034 and perplexity is 135.62980490457883
At time: 116.03437662124634 and batch: 200, loss is 4.911659002304077 and perplexity is 135.86462727216818
At time: 116.97100186347961 and batch: 250, loss is 4.951474676132202 and perplexity is 141.38330485117845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.317718505859375 and perplexity of 203.9181128905376
Finished 22 epochs...
Completing Train Step...
At time: 118.54544687271118 and batch: 50, loss is 4.936064395904541 and perplexity is 139.22125022404856
At time: 119.51367139816284 and batch: 100, loss is 4.89239670753479 and perplexity is 133.27260704566348
At time: 120.47475051879883 and batch: 150, loss is 4.906832895278931 and perplexity is 135.2105097302572
At time: 121.40848875045776 and batch: 200, loss is 4.910394077301025 and perplexity is 135.69287735638318
At time: 122.36984610557556 and batch: 250, loss is 4.948290996551513 and perplexity is 140.93390146824945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.314237976074219 and perplexity of 203.20960353409362
Finished 23 epochs...
Completing Train Step...
At time: 123.97355365753174 and batch: 50, loss is 4.933487730026245 and perplexity is 138.8629853417276
At time: 124.92321419715881 and batch: 100, loss is 4.8886826229095455 and perplexity is 132.77853937738243
At time: 125.85769772529602 and batch: 150, loss is 4.904124155044555 and perplexity is 134.8447551731196
At time: 126.79338192939758 and batch: 200, loss is 4.907909784317017 and perplexity is 135.35619487530846
At time: 127.73013854026794 and batch: 250, loss is 4.946767683029175 and perplexity is 140.71937838482697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3142852783203125 and perplexity of 203.2192160321131
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 129.30429005622864 and batch: 50, loss is 4.92460220336914 and perplexity is 137.63458017933277
At time: 130.24278807640076 and batch: 100, loss is 4.871073932647705 and perplexity is 130.4609479623009
At time: 131.17992997169495 and batch: 150, loss is 4.882152881622314 and perplexity is 131.9143543895059
At time: 132.1169629096985 and batch: 200, loss is 4.881525440216064 and perplexity is 131.83161182225408
At time: 133.05432176589966 and batch: 250, loss is 4.926413507461548 and perplexity is 137.884104171362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.302207565307617 and perplexity of 200.77955508089474
Finished 25 epochs...
Completing Train Step...
At time: 134.63079118728638 and batch: 50, loss is 4.913825883865356 and perplexity is 136.1593490260823
At time: 135.58205461502075 and batch: 100, loss is 4.863849210739136 and perplexity is 129.52180051792558
At time: 136.51879692077637 and batch: 150, loss is 4.879441862106323 and perplexity is 131.55721632318634
At time: 137.4528102874756 and batch: 200, loss is 4.881777248382568 and perplexity is 131.8648122786119
At time: 138.39035153388977 and batch: 250, loss is 4.925145530700684 and perplexity is 137.70938112736937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.299834060668945 and perplexity of 200.30356897662958
Finished 26 epochs...
Completing Train Step...
At time: 139.9718587398529 and batch: 50, loss is 4.911427974700928 and perplexity is 135.83324241850005
At time: 140.90703082084656 and batch: 100, loss is 4.863265132904052 and perplexity is 129.44617179376377
At time: 141.84673810005188 and batch: 150, loss is 4.88023265838623 and perplexity is 131.6612924265236
At time: 142.7871994972229 and batch: 200, loss is 4.881368036270142 and perplexity is 131.8108626394004
At time: 143.72505021095276 and batch: 250, loss is 4.923500070571899 and perplexity is 137.48297215591657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.299070358276367 and perplexity of 200.15065505955454
Finished 27 epochs...
Completing Train Step...
At time: 145.3145112991333 and batch: 50, loss is 4.910679235458374 and perplexity is 135.73157680472488
At time: 146.25161600112915 and batch: 100, loss is 4.863332405090332 and perplexity is 129.45488021365978
At time: 147.18838596343994 and batch: 150, loss is 4.880201463699341 and perplexity is 131.65718535779055
At time: 148.12464952468872 and batch: 200, loss is 4.880648965835571 and perplexity is 131.71611541416792
At time: 149.05986785888672 and batch: 250, loss is 4.922637443542481 and perplexity is 137.3644267656301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.298783874511718 and perplexity of 200.09332335908894
Finished 28 epochs...
Completing Train Step...
At time: 150.6326150894165 and batch: 50, loss is 4.91010235786438 and perplexity is 135.65329887983012
At time: 151.59545850753784 and batch: 100, loss is 4.863228540420533 and perplexity is 129.4414351235195
At time: 152.5346291065216 and batch: 150, loss is 4.880334501266479 and perplexity is 131.67470187457843
At time: 153.4700858592987 and batch: 200, loss is 4.880110788345337 and perplexity is 131.64524783712872
At time: 154.41516375541687 and batch: 250, loss is 4.921822576522827 and perplexity is 137.2525386177572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.298512649536133 and perplexity of 200.03906041141207
Finished 29 epochs...
Completing Train Step...
At time: 155.994690656662 and batch: 50, loss is 4.909625511169434 and perplexity is 135.5886284727611
At time: 156.9292013645172 and batch: 100, loss is 4.863112840652466 and perplexity is 129.42645964584386
At time: 157.8642122745514 and batch: 150, loss is 4.8803657531738285 and perplexity is 131.67881702446445
At time: 158.80053424835205 and batch: 200, loss is 4.8795793342590335 and perplexity is 131.57530302009897
At time: 159.76348209381104 and batch: 250, loss is 4.921082973480225 and perplexity is 137.15106375277566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.298251342773438 and perplexity of 199.9867956809845
Finished 30 epochs...
Completing Train Step...
At time: 161.32679414749146 and batch: 50, loss is 4.909229383468628 and perplexity is 135.53492869779515
At time: 162.27703666687012 and batch: 100, loss is 4.862997531890869 and perplexity is 129.4115365014654
At time: 163.2122130393982 and batch: 150, loss is 4.880347576141357 and perplexity is 131.67642351608512
At time: 164.1494860649109 and batch: 200, loss is 4.879052848815918 and perplexity is 131.50604877066215
At time: 165.08736753463745 and batch: 250, loss is 4.92039309501648 and perplexity is 137.05647881741922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.297984313964844 and perplexity of 199.9334005745324
Finished 31 epochs...
Completing Train Step...
At time: 166.6465072631836 and batch: 50, loss is 4.908881425857544 and perplexity is 135.4877764917472
At time: 167.59769773483276 and batch: 100, loss is 4.862882289886475 and perplexity is 129.3966237159134
At time: 168.5374493598938 and batch: 150, loss is 4.880309257507324 and perplexity is 131.67137795207182
At time: 169.47079920768738 and batch: 200, loss is 4.878520498275757 and perplexity is 131.43606008547468
At time: 170.40677046775818 and batch: 250, loss is 4.919739894866943 and perplexity is 136.9669827375692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.297727966308594 and perplexity of 199.8821546845516
Finished 32 epochs...
Completing Train Step...
At time: 171.99122881889343 and batch: 50, loss is 4.908562545776367 and perplexity is 135.44457902635273
At time: 172.92587780952454 and batch: 100, loss is 4.862769002914429 and perplexity is 129.38196559452243
At time: 173.86209225654602 and batch: 150, loss is 4.8802581214904786 and perplexity is 131.66464497442107
At time: 174.79634428024292 and batch: 200, loss is 4.877980661392212 and perplexity is 131.365125200769
At time: 175.73212122917175 and batch: 250, loss is 4.919111366271973 and perplexity is 136.88092212096421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.297471618652343 and perplexity of 199.83092192965077
Finished 33 epochs...
Completing Train Step...
At time: 177.28755950927734 and batch: 50, loss is 4.908258390426636 and perplexity is 135.40338909743596
At time: 178.23884534835815 and batch: 100, loss is 4.862655849456787 and perplexity is 129.3673264060119
At time: 179.17599320411682 and batch: 150, loss is 4.880192470550537 and perplexity is 131.65600135045557
At time: 180.1129059791565 and batch: 200, loss is 4.877430934906005 and perplexity is 131.29293015766407
At time: 181.05010843276978 and batch: 250, loss is 4.918501634597778 and perplexity is 136.79748692627945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.29720458984375 and perplexity of 199.77756844042406
Finished 34 epochs...
Completing Train Step...
At time: 182.62410163879395 and batch: 50, loss is 4.907959079742431 and perplexity is 135.36286748098047
At time: 183.55981755256653 and batch: 100, loss is 4.862535552978516 and perplexity is 129.35176490825737
At time: 184.51494979858398 and batch: 150, loss is 4.880104579925537 and perplexity is 131.6444305307026
At time: 185.45025205612183 and batch: 200, loss is 4.876870985031128 and perplexity is 131.21943327705492
At time: 186.38475513458252 and batch: 250, loss is 4.917902526855468 and perplexity is 136.71555503819832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.296927642822266 and perplexity of 199.72224829861304
Finished 35 epochs...
Completing Train Step...
At time: 187.96469807624817 and batch: 50, loss is 4.907662000656128 and perplexity is 135.32265997668983
At time: 188.9017105102539 and batch: 100, loss is 4.862401037216187 and perplexity is 129.33436622721678
At time: 189.83549404144287 and batch: 150, loss is 4.879989986419678 and perplexity is 131.6293457982039
At time: 190.76869750022888 and batch: 200, loss is 4.876301097869873 and perplexity is 131.14467431083398
At time: 191.7054045200348 and batch: 250, loss is 4.917308683395386 and perplexity is 136.63439150155025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.296657943725586 and perplexity of 199.66839065166593
Finished 36 epochs...
Completing Train Step...
At time: 193.2700674533844 and batch: 50, loss is 4.90736496925354 and perplexity is 135.28247086620445
At time: 194.2217137813568 and batch: 100, loss is 4.8622421169281 and perplexity is 129.3138140056022
At time: 195.15709972381592 and batch: 150, loss is 4.879850330352784 and perplexity is 131.61096424505948
At time: 196.09346270561218 and batch: 200, loss is 4.875735263824463 and perplexity is 131.07048917945056
At time: 197.02939105033875 and batch: 250, loss is 4.916737127304077 and perplexity is 136.55631959616716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.29638557434082 and perplexity of 199.61401450048245
Finished 37 epochs...
Completing Train Step...
At time: 198.59951829910278 and batch: 50, loss is 4.907080402374268 and perplexity is 135.24397943260314
At time: 199.53402400016785 and batch: 100, loss is 4.862100429534912 and perplexity is 129.2954931663392
At time: 200.47070860862732 and batch: 150, loss is 4.879692096710205 and perplexity is 131.5901406103269
At time: 201.40484809875488 and batch: 200, loss is 4.875176696777344 and perplexity is 130.99729796634796
At time: 202.34209156036377 and batch: 250, loss is 4.916173677444458 and perplexity is 136.4793986296496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.296094131469727 and perplexity of 199.55584689566456
Finished 38 epochs...
Completing Train Step...
At time: 203.90864396095276 and batch: 50, loss is 4.906799793243408 and perplexity is 135.2060340612384
At time: 204.86044645309448 and batch: 100, loss is 4.861938009262085 and perplexity is 129.27449466240128
At time: 205.81144881248474 and batch: 150, loss is 4.879494333267212 and perplexity is 131.56411946415648
At time: 206.746994972229 and batch: 200, loss is 4.874615364074707 and perplexity is 130.92378553343892
At time: 207.6822304725647 and batch: 250, loss is 4.915609884262085 and perplexity is 136.40247416195058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.295780944824219 and perplexity of 199.49335845516705
Finished 39 epochs...
Completing Train Step...
At time: 209.23793077468872 and batch: 50, loss is 4.906504306793213 and perplexity is 135.16608841217834
At time: 210.18765950202942 and batch: 100, loss is 4.861730737686157 and perplexity is 129.24770251088702
At time: 211.1234667301178 and batch: 150, loss is 4.879244031906128 and perplexity is 131.53119290693775
At time: 212.06150102615356 and batch: 200, loss is 4.874039602279663 and perplexity is 130.8484263162272
At time: 212.99821710586548 and batch: 250, loss is 4.915046100616455 and perplexity is 136.32559435162088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.29547119140625 and perplexity of 199.43157427494805
Finished 40 epochs...
Completing Train Step...
At time: 214.58697962760925 and batch: 50, loss is 4.906232328414917 and perplexity is 135.1293311574672
At time: 215.5246889591217 and batch: 100, loss is 4.861550054550171 and perplexity is 129.2243517402871
At time: 216.4602336883545 and batch: 150, loss is 4.879054012298584 and perplexity is 131.50620177575945
At time: 217.3967525959015 and batch: 200, loss is 4.873525362014771 and perplexity is 130.78115608483083
At time: 218.33379364013672 and batch: 250, loss is 4.9145396900177 and perplexity is 136.25657510327414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.29522705078125 and perplexity of 199.38289086880025
Finished 41 epochs...
Completing Train Step...
At time: 219.89021015167236 and batch: 50, loss is 4.906012115478515 and perplexity is 135.0995772068817
At time: 220.84272980690002 and batch: 100, loss is 4.861422147750854 and perplexity is 129.207824124082
At time: 221.78004932403564 and batch: 150, loss is 4.87888687133789 and perplexity is 131.48422353964034
At time: 222.71743822097778 and batch: 200, loss is 4.873035116195679 and perplexity is 130.71705688331076
At time: 223.65382885932922 and batch: 250, loss is 4.914054098129273 and perplexity is 136.19042607767403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.295011138916015 and perplexity of 199.3398463840116
Finished 42 epochs...
Completing Train Step...
At time: 225.22982001304626 and batch: 50, loss is 4.905793352127075 and perplexity is 135.07002560311557
At time: 226.16543865203857 and batch: 100, loss is 4.861294956207275 and perplexity is 129.1913910265868
At time: 227.10240054130554 and batch: 150, loss is 4.878724498748779 and perplexity is 131.46287583902455
At time: 228.03799962997437 and batch: 200, loss is 4.872552452087402 and perplexity is 130.65397967539036
At time: 228.97408533096313 and batch: 250, loss is 4.913590335845948 and perplexity is 136.1272807380544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.294817733764648 and perplexity of 199.30129675881642
Finished 43 epochs...
Completing Train Step...
At time: 230.5473027229309 and batch: 50, loss is 4.905587110519409 and perplexity is 135.04217141633256
At time: 231.4810152053833 and batch: 100, loss is 4.861165361404419 and perplexity is 129.17464957856177
At time: 232.43398118019104 and batch: 150, loss is 4.878567295074463 and perplexity is 131.44221101624208
At time: 233.36975741386414 and batch: 200, loss is 4.872097663879394 and perplexity is 130.5945732957891
At time: 234.30426812171936 and batch: 250, loss is 4.913142251968384 and perplexity is 136.0662979619788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.294623184204101 and perplexity of 199.26252655060134
Finished 44 epochs...
Completing Train Step...
At time: 235.8618941307068 and batch: 50, loss is 4.905390872955322 and perplexity is 135.01567366957644
At time: 236.81301999092102 and batch: 100, loss is 4.861027698516846 and perplexity is 129.15686824724236
At time: 237.749760389328 and batch: 150, loss is 4.878405542373657 and perplexity is 131.42095160304038
At time: 238.6871633529663 and batch: 200, loss is 4.871654939651489 and perplexity is 130.53676871084744
At time: 239.62355780601501 and batch: 250, loss is 4.912698860168457 and perplexity is 136.00598065430407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.294418716430664 and perplexity of 199.2217879504755
Finished 45 epochs...
Completing Train Step...
At time: 241.19898104667664 and batch: 50, loss is 4.905184412002564 and perplexity is 134.9878010823528
At time: 242.135751247406 and batch: 100, loss is 4.860863723754883 and perplexity is 129.13569151678772
At time: 243.0713517665863 and batch: 150, loss is 4.878216695785523 and perplexity is 131.3961355480082
At time: 244.00807070732117 and batch: 200, loss is 4.871195068359375 and perplexity is 130.47675239929904
At time: 244.94874835014343 and batch: 250, loss is 4.912238063812256 and perplexity is 135.94332403107757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2941650390625 and perplexity of 199.17125630126614
Finished 46 epochs...
Completing Train Step...
At time: 246.50439500808716 and batch: 50, loss is 4.90493673324585 and perplexity is 134.95437161166495
At time: 247.45616674423218 and batch: 100, loss is 4.860665235519409 and perplexity is 129.11006214489046
At time: 248.3913185596466 and batch: 150, loss is 4.8780285739898686 and perplexity is 131.37141939594324
At time: 249.32730984687805 and batch: 200, loss is 4.870763959884644 and perplexity is 130.420514888689
At time: 250.26291131973267 and batch: 250, loss is 4.911816539764405 and perplexity is 135.8860327265332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.293983078002929 and perplexity of 199.13501818549642
Finished 47 epochs...
Completing Train Step...
At time: 251.8195517063141 and batch: 50, loss is 4.904727029800415 and perplexity is 134.9260741820994
At time: 252.7709801197052 and batch: 100, loss is 4.8604990577697755 and perplexity is 129.08860870789945
At time: 253.70827865600586 and batch: 150, loss is 4.877856435775757 and perplexity is 131.34880730068184
At time: 254.64402222633362 and batch: 200, loss is 4.870352668762207 and perplexity is 130.36688511821188
At time: 255.58056497573853 and batch: 250, loss is 4.911411571502685 and perplexity is 135.83101433717064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.293810272216797 and perplexity of 199.1006094752302
Finished 48 epochs...
Completing Train Step...
At time: 257.1528480052948 and batch: 50, loss is 4.904514932632447 and perplexity is 134.89745977850455
At time: 258.0862190723419 and batch: 100, loss is 4.860325756072998 and perplexity is 129.06623937135373
At time: 259.02214670181274 and batch: 150, loss is 4.877680950164795 and perplexity is 131.32575949732154
At time: 259.95916414260864 and batch: 200, loss is 4.869957075119019 and perplexity is 130.3153230066711
At time: 260.8952696323395 and batch: 250, loss is 4.911013860702514 and perplexity is 135.77700361680587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.293628311157226 and perplexity of 199.06438421326231
Finished 49 epochs...
Completing Train Step...
At time: 262.4608657360077 and batch: 50, loss is 4.9042979335784915 and perplexity is 134.8681903331875
At time: 263.41240429878235 and batch: 100, loss is 4.860155382156372 and perplexity is 129.04425172376182
At time: 264.34815096855164 and batch: 150, loss is 4.877503576278687 and perplexity is 131.3024678027437
At time: 265.28709077835083 and batch: 200, loss is 4.869573774337769 and perplexity is 130.26538261326112
At time: 266.2236661911011 and batch: 250, loss is 4.910627822875977 and perplexity is 135.72459867323198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2934528350830075 and perplexity of 199.02945624120514
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.29596629794564455, 'wordvec_source': 'glove', 'anneal': 7.578000455271355, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 24.787014096733614}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2255783081054688 and batch: 50, loss is 6.765630226135254 and perplexity is 867.5127647615154
At time: 2.179473400115967 and batch: 100, loss is 5.753040237426758 and perplexity is 315.1473280018536
At time: 3.1174702644348145 and batch: 150, loss is 5.681822814941406 and perplexity is 293.4839095011798
At time: 4.054811000823975 and batch: 200, loss is 5.712872171401978 and perplexity is 302.73934004797206
At time: 4.992156505584717 and batch: 250, loss is 5.79368706703186 and perplexity is 328.2209688347857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.8694721221923825 and perplexity of 354.06202944064853
Finished 1 epochs...
Completing Train Step...
At time: 6.561761140823364 and batch: 50, loss is 5.63147834777832 and perplexity is 279.0743818153798
At time: 7.490804195404053 and batch: 100, loss is 5.641704320907593 and perplexity is 281.9428302871695
At time: 8.419178485870361 and batch: 150, loss is 5.667218542098999 and perplexity is 289.2289364322684
At time: 9.348457098007202 and batch: 200, loss is 5.712207832336426 and perplexity is 302.5382852693652
At time: 10.279049634933472 and batch: 250, loss is 5.773703823089599 and perplexity is 321.72714903683567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.993735504150391 and perplexity of 400.90941504737697
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 11.846790075302124 and batch: 50, loss is 5.598787040710449 and perplexity is 270.09859005787854
At time: 12.779338359832764 and batch: 100, loss is 5.38556604385376 and perplexity is 218.23359893979062
At time: 13.737258672714233 and batch: 150, loss is 5.275684194564819 and perplexity is 195.5242073583403
At time: 14.713917970657349 and batch: 200, loss is 5.185935325622559 and perplexity is 178.74055224834612
At time: 15.649055004119873 and batch: 250, loss is 5.2482261466979985 and perplexity is 190.22853149055157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.507986831665039 and perplexity of 246.65407074156383
Finished 3 epochs...
Completing Train Step...
At time: 17.209625244140625 and batch: 50, loss is 5.265897941589356 and perplexity is 193.62009028452948
At time: 18.154247760772705 and batch: 100, loss is 5.1713651847839355 and perplexity is 176.15515774692577
At time: 19.08365535736084 and batch: 150, loss is 5.142433319091797 and perplexity is 171.13168007743906
At time: 20.016448259353638 and batch: 200, loss is 5.131990928649902 and perplexity is 169.35395426470427
At time: 20.94493341445923 and batch: 250, loss is 5.199769830703735 and perplexity is 181.2305233721498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4615013122558596 and perplexity of 235.45064408996535
Finished 4 epochs...
Completing Train Step...
At time: 22.514767169952393 and batch: 50, loss is 5.186169567108155 and perplexity is 178.78242560488815
At time: 23.44526481628418 and batch: 100, loss is 5.114292325973511 and perplexity is 166.38299438739958
At time: 24.37462544441223 and batch: 150, loss is 5.113749837875366 and perplexity is 166.29275807148733
At time: 25.303125381469727 and batch: 200, loss is 5.115995397567749 and perplexity is 166.66659796895868
At time: 26.233211040496826 and batch: 250, loss is 5.1697523880004885 and perplexity is 175.87128425178614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.440554428100586 and perplexity of 230.56998250713062
Finished 5 epochs...
Completing Train Step...
At time: 27.783194065093994 and batch: 50, loss is 5.144925308227539 and perplexity is 171.55867017134318
At time: 28.72771954536438 and batch: 100, loss is 5.0947990894317625 and perplexity is 163.17105854087114
At time: 29.656495809555054 and batch: 150, loss is 5.0982707977294925 and perplexity is 163.73852532837273
At time: 30.58797574043274 and batch: 200, loss is 5.094794626235962 and perplexity is 163.17033027811308
At time: 31.52397584915161 and batch: 250, loss is 5.145964822769165 and perplexity is 171.73710062818802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.429189300537109 and perplexity of 227.96435990670062
Finished 6 epochs...
Completing Train Step...
At time: 33.08072090148926 and batch: 50, loss is 5.121296739578247 and perplexity is 167.5525007685825
At time: 34.0327672958374 and batch: 100, loss is 5.07379864692688 and perplexity is 159.78012428859375
At time: 34.965800762176514 and batch: 150, loss is 5.073062553405761 and perplexity is 159.66255445080793
At time: 35.8999457359314 and batch: 200, loss is 5.07569185256958 and perplexity is 160.08290744644276
At time: 36.83422613143921 and batch: 250, loss is 5.131154708862304 and perplexity is 169.21239633206127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.415915298461914 and perplexity of 224.95835551233026
Finished 7 epochs...
Completing Train Step...
At time: 38.40656232833862 and batch: 50, loss is 5.101388940811157 and perplexity is 164.24988230611947
At time: 39.34086465835571 and batch: 100, loss is 5.0498034858703615 and perplexity is 155.9918068800075
At time: 40.275166034698486 and batch: 150, loss is 5.054180030822754 and perplexity is 156.6760081617485
At time: 41.20808291435242 and batch: 200, loss is 5.057175378799439 and perplexity is 157.14601088523472
At time: 42.15740895271301 and batch: 250, loss is 5.104234943389892 and perplexity is 164.71800371611826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.396414566040039 and perplexity of 220.613999548268
Finished 8 epochs...
Completing Train Step...
At time: 43.71126174926758 and batch: 50, loss is 5.077473964691162 and perplexity is 160.36844749287897
At time: 44.65922570228577 and batch: 100, loss is 5.029028720855713 and perplexity is 152.7845441637767
At time: 45.595101833343506 and batch: 150, loss is 5.032142677307129 and perplexity is 153.2610501050816
At time: 46.531943559646606 and batch: 200, loss is 5.039388046264649 and perplexity is 154.3755154408321
At time: 47.465755462646484 and batch: 250, loss is 5.092535667419433 and perplexity is 162.8021512291547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3892463684082035 and perplexity of 219.03824919137813
Finished 9 epochs...
Completing Train Step...
At time: 49.04569387435913 and batch: 50, loss is 5.063051567077637 and perplexity is 158.07214884795567
At time: 49.985512495040894 and batch: 100, loss is 5.011441011428833 and perplexity is 150.12090630149706
At time: 50.918341398239136 and batch: 150, loss is 5.019150247573853 and perplexity is 151.28269632130238
At time: 51.85259795188904 and batch: 200, loss is 5.026988935470581 and perplexity is 152.47321411470594
At time: 52.78848910331726 and batch: 250, loss is 5.07715838432312 and perplexity is 160.3178463439814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.382058334350586 and perplexity of 217.46943987839782
Finished 10 epochs...
Completing Train Step...
At time: 54.365315437316895 and batch: 50, loss is 5.048451356887817 and perplexity is 155.7810283688655
At time: 55.298105239868164 and batch: 100, loss is 4.996554489135742 and perplexity is 147.90267988634727
At time: 56.232980251312256 and batch: 150, loss is 5.001315622329712 and perplexity is 148.60854326641757
At time: 57.16623520851135 and batch: 200, loss is 5.013061809539795 and perplexity is 150.36441927221915
At time: 58.10061478614807 and batch: 250, loss is 5.062518634796143 and perplexity is 157.98792954061653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.371094131469727 and perplexity of 215.09808458519697
Finished 11 epochs...
Completing Train Step...
At time: 59.65408682823181 and batch: 50, loss is 5.0338109016418455 and perplexity is 153.51693729770807
At time: 60.60566163063049 and batch: 100, loss is 4.984405040740967 and perplexity is 146.11661572667694
At time: 61.54085111618042 and batch: 150, loss is 4.989106950759887 and perplexity is 146.80526061052183
At time: 62.480363607406616 and batch: 200, loss is 5.000323877334595 and perplexity is 148.46123454581854
At time: 63.41499948501587 and batch: 250, loss is 5.050212841033936 and perplexity is 156.05567600331494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.365436553955078 and perplexity of 213.8845864650638
Finished 12 epochs...
Completing Train Step...
At time: 64.98374199867249 and batch: 50, loss is 5.0211191082000735 and perplexity is 151.58084427508936
At time: 65.91792726516724 and batch: 100, loss is 4.973585796356201 and perplexity is 144.54426551080587
At time: 66.86624455451965 and batch: 150, loss is 4.975978116989136 and perplexity is 144.89047569732693
At time: 67.79809141159058 and batch: 200, loss is 4.9883011531829835 and perplexity is 146.6870129354884
At time: 68.73140001296997 and batch: 250, loss is 5.037605266571045 and perplexity is 154.10054308710767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.357096862792969 and perplexity of 212.1082723205709
Finished 13 epochs...
Completing Train Step...
At time: 70.28619694709778 and batch: 50, loss is 5.009010953903198 and perplexity is 149.7565467500198
At time: 71.23422431945801 and batch: 100, loss is 4.960351343154907 and perplexity is 142.64390405317315
At time: 72.16715455055237 and batch: 150, loss is 4.9642133617401125 and perplexity is 143.19586261275344
At time: 73.10163044929504 and batch: 200, loss is 4.9751473236083985 and perplexity is 144.77015163832237
At time: 74.03569316864014 and batch: 250, loss is 5.026252756118774 and perplexity is 152.3610077898284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3551887512207035 and perplexity of 211.7039319575256
Finished 14 epochs...
Completing Train Step...
At time: 75.59490895271301 and batch: 50, loss is 4.997423582077026 and perplexity is 148.0312769347326
At time: 76.54528069496155 and batch: 100, loss is 4.94529860496521 and perplexity is 140.51280240813747
At time: 77.47889947891235 and batch: 150, loss is 4.952255783081054 and perplexity is 141.49378347524274
At time: 78.41211009025574 and batch: 200, loss is 4.963744192123413 and perplexity is 143.12869522245995
At time: 79.34661889076233 and batch: 250, loss is 5.0115815544128415 and perplexity is 150.14200622431912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.34854736328125 and perplexity of 210.302582620275
Finished 15 epochs...
Completing Train Step...
At time: 80.91451120376587 and batch: 50, loss is 4.98467791557312 and perplexity is 146.15649271413383
At time: 81.84773302078247 and batch: 100, loss is 4.933262357711792 and perplexity is 138.8316929956759
At time: 82.78347659111023 and batch: 150, loss is 4.937727174758911 and perplexity is 139.45293694350778
At time: 83.71724891662598 and batch: 200, loss is 4.95641300201416 and perplexity is 142.08322848788418
At time: 84.64967346191406 and batch: 250, loss is 5.007172012329102 and perplexity is 149.48140627137755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.344834518432617 and perplexity of 209.52320950079374
Finished 16 epochs...
Completing Train Step...
At time: 86.20346164703369 and batch: 50, loss is 4.977632188796997 and perplexity is 145.13033326453882
At time: 87.15211367607117 and batch: 100, loss is 4.924999628067017 and perplexity is 137.68929043166054
At time: 88.08295750617981 and batch: 150, loss is 4.931923027038574 and perplexity is 138.64587591386737
At time: 89.01848673820496 and batch: 200, loss is 4.948652935028076 and perplexity is 140.9849201020893
At time: 89.95272493362427 and batch: 250, loss is 4.996344871520996 and perplexity is 147.87168012853613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.335427856445312 and perplexity of 207.5615363535507
Finished 17 epochs...
Completing Train Step...
At time: 91.52124500274658 and batch: 50, loss is 4.966408901214599 and perplexity is 143.51060016463268
At time: 92.45709681510925 and batch: 100, loss is 4.916759567260742 and perplexity is 136.5593839484431
At time: 93.39155888557434 and batch: 150, loss is 4.922625713348388 and perplexity is 137.3628154636932
At time: 94.35510230064392 and batch: 200, loss is 4.9391377162933345 and perplexity is 139.64977989810853
At time: 95.34753394126892 and batch: 250, loss is 4.990982885360718 and perplexity is 147.08091615353158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.336415481567383 and perplexity of 207.7666306026747
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 96.97367930412292 and batch: 50, loss is 4.9360099792480465 and perplexity is 139.2136744752237
At time: 97.90990829467773 and batch: 100, loss is 4.844819536209107 and perplexity is 127.08034652320656
At time: 98.84703660011292 and batch: 150, loss is 4.831172189712524 and perplexity is 125.35781771008926
At time: 99.78183078765869 and batch: 200, loss is 4.834356822967529 and perplexity is 125.75767274302079
At time: 100.71723747253418 and batch: 250, loss is 4.892651138305664 and perplexity is 133.30652001187863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.257256698608399 and perplexity of 191.95418017168353
Finished 19 epochs...
Completing Train Step...
At time: 102.27649235725403 and batch: 50, loss is 4.881667108535766 and perplexity is 131.8502895081715
At time: 103.22676467895508 and batch: 100, loss is 4.811829490661621 and perplexity is 122.95635938428724
At time: 104.15849161148071 and batch: 150, loss is 4.816958370208741 and perplexity is 123.58860771793339
At time: 105.09071493148804 and batch: 200, loss is 4.833241739273071 and perplexity is 125.61752056791691
At time: 106.0234067440033 and batch: 250, loss is 4.890843334197998 and perplexity is 133.06574563944636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.252849197387695 and perplexity of 191.11000360945735
Finished 20 epochs...
Completing Train Step...
At time: 107.6091194152832 and batch: 50, loss is 4.8679875564575195 and perplexity is 130.05891712709578
At time: 108.54222583770752 and batch: 100, loss is 4.80735224723816 and perplexity is 122.40708436938552
At time: 109.4739670753479 and batch: 150, loss is 4.8200816822052 and perplexity is 123.97521693536434
At time: 110.4069926738739 and batch: 200, loss is 4.834466638565064 and perplexity is 125.77148365531053
At time: 111.3410222530365 and batch: 250, loss is 4.886729784011841 and perplexity is 132.51949729695443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.248404693603516 and perplexity of 190.26249923851822
Finished 21 epochs...
Completing Train Step...
At time: 112.89540481567383 and batch: 50, loss is 4.862799701690673 and perplexity is 129.38593752350064
At time: 113.84492421150208 and batch: 100, loss is 4.8068757820129395 and perplexity is 122.34877554253019
At time: 114.7791051864624 and batch: 150, loss is 4.819145536422729 and perplexity is 123.85921236596549
At time: 115.72872281074524 and batch: 200, loss is 4.831500988006592 and perplexity is 125.39904192355455
At time: 116.66326332092285 and batch: 250, loss is 4.88528263092041 and perplexity is 132.32785999446838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.249967193603515 and perplexity of 190.5600167686186
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 118.21821141242981 and batch: 50, loss is 4.859103384017945 and perplexity is 128.90856879278243
At time: 119.16792607307434 and batch: 100, loss is 4.797402582168579 and perplexity is 121.19521372875289
At time: 120.10393333435059 and batch: 150, loss is 4.8063375186920165 and perplexity is 122.28293740501267
At time: 121.04117155075073 and batch: 200, loss is 4.817460289001465 and perplexity is 123.65065473269244
At time: 121.97878956794739 and batch: 250, loss is 4.872506971359253 and perplexity is 130.6480375723858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.237636566162109 and perplexity of 188.22471962512824
Finished 23 epochs...
Completing Train Step...
At time: 123.55489134788513 and batch: 50, loss is 4.851134204864502 and perplexity is 127.88535581259728
At time: 124.49146437644958 and batch: 100, loss is 4.791930074691773 and perplexity is 120.53378350711296
At time: 125.42780256271362 and batch: 150, loss is 4.803720092773437 and perplexity is 121.96328938524483
At time: 126.36093068122864 and batch: 200, loss is 4.816875820159912 and perplexity is 123.57840589341795
At time: 127.29503154754639 and batch: 250, loss is 4.872535305023193 and perplexity is 130.65173936241922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.234589385986328 and perplexity of 187.65203796612073
Finished 24 epochs...
Completing Train Step...
At time: 128.8497154712677 and batch: 50, loss is 4.848729381561279 and perplexity is 127.57818362425233
At time: 129.79927349090576 and batch: 100, loss is 4.790873441696167 and perplexity is 120.4064907970562
At time: 130.73381066322327 and batch: 150, loss is 4.804768009185791 and perplexity is 122.09116370699466
At time: 131.6673948764801 and batch: 200, loss is 4.817231178283691 and perplexity is 123.62232828750345
At time: 132.60292720794678 and batch: 250, loss is 4.871179361343383 and perplexity is 130.47470301495744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.233733367919922 and perplexity of 187.49147316441412
Finished 25 epochs...
Completing Train Step...
At time: 134.16913890838623 and batch: 50, loss is 4.847244501113892 and perplexity is 127.38888585093805
At time: 135.1009817123413 and batch: 100, loss is 4.790995569229126 and perplexity is 120.42119664270548
At time: 136.0339229106903 and batch: 150, loss is 4.805249300003052 and perplexity is 122.14993920587489
At time: 136.96813440322876 and batch: 200, loss is 4.817081556320191 and perplexity is 123.60383305569351
At time: 137.90215492248535 and batch: 250, loss is 4.870201597213745 and perplexity is 130.34719187859045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.233490753173828 and perplexity of 187.44599048586488
Finished 26 epochs...
Completing Train Step...
At time: 139.46982145309448 and batch: 50, loss is 4.8463483238220215 and perplexity is 127.27477396407993
At time: 140.40294075012207 and batch: 100, loss is 4.790713443756103 and perplexity is 120.38722754763954
At time: 141.33496713638306 and batch: 150, loss is 4.805449295043945 and perplexity is 122.174371031002
At time: 142.26834964752197 and batch: 200, loss is 4.8168070602416995 and perplexity is 123.56990894446399
At time: 143.2010955810547 and batch: 250, loss is 4.86931866645813 and perplexity is 130.23215512618944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.233198165893555 and perplexity of 187.39115419590215
Finished 27 epochs...
Completing Train Step...
At time: 144.7560224533081 and batch: 50, loss is 4.845539903640747 and perplexity is 127.17192404681596
At time: 145.70392036437988 and batch: 100, loss is 4.790678768157959 and perplexity is 120.38305312089122
At time: 146.63676857948303 and batch: 150, loss is 4.8057750415802 and perplexity is 122.21417539189888
At time: 147.57108402252197 and batch: 200, loss is 4.816539096832275 and perplexity is 123.53680116639563
At time: 148.50430989265442 and batch: 250, loss is 4.868406019210815 and perplexity is 130.11335332861447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.233120727539062 and perplexity of 187.37664349512445
Finished 28 epochs...
Completing Train Step...
At time: 150.07269525527954 and batch: 50, loss is 4.844901208877563 and perplexity is 127.090725938067
At time: 151.00572562217712 and batch: 100, loss is 4.790571546554565 and perplexity is 120.3701461488796
At time: 151.93817138671875 and batch: 150, loss is 4.805956964492798 and perplexity is 122.2364109731664
At time: 152.87914204597473 and batch: 200, loss is 4.81618670463562 and perplexity is 123.49327543118001
At time: 153.81191635131836 and batch: 250, loss is 4.86759202003479 and perplexity is 130.00748426072778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.232908248901367 and perplexity of 187.3368341906419
Finished 29 epochs...
Completing Train Step...
At time: 155.36489725112915 and batch: 50, loss is 4.844313611984253 and perplexity is 127.01606975835493
At time: 156.3141484260559 and batch: 100, loss is 4.790451469421387 and perplexity is 120.35569331455466
At time: 157.24693512916565 and batch: 150, loss is 4.806098518371582 and perplexity is 122.25371523598213
At time: 158.1810336112976 and batch: 200, loss is 4.815783672332763 and perplexity is 123.44351368046574
At time: 159.11552095413208 and batch: 250, loss is 4.866820917129517 and perplexity is 129.90727375317928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.232655334472656 and perplexity of 187.28945999330756
Finished 30 epochs...
Completing Train Step...
At time: 160.6674997806549 and batch: 50, loss is 4.843731279373169 and perplexity is 126.94212569091344
At time: 161.6160125732422 and batch: 100, loss is 4.790309829711914 and perplexity is 120.33864737633951
At time: 162.5518877506256 and batch: 150, loss is 4.806173648834228 and perplexity is 122.26290055921257
At time: 163.50045704841614 and batch: 200, loss is 4.815320081710816 and perplexity is 123.38629968814304
At time: 164.43302249908447 and batch: 250, loss is 4.8660157203674315 and perplexity is 129.80271493784082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.232343292236328 and perplexity of 187.231026888643
Finished 31 epochs...
Completing Train Step...
At time: 166.00537633895874 and batch: 50, loss is 4.843085441589356 and perplexity is 126.8601681382755
At time: 166.93927574157715 and batch: 100, loss is 4.790096158981323 and perplexity is 120.31293727648205
At time: 167.8745927810669 and batch: 150, loss is 4.806226396560669 and perplexity is 122.26934981933555
At time: 168.80657601356506 and batch: 200, loss is 4.814901514053345 and perplexity is 123.33466498078167
At time: 169.74137997627258 and batch: 250, loss is 4.8653153038024906 and perplexity is 129.711830798218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2322235107421875 and perplexity of 187.20860141959776
Finished 32 epochs...
Completing Train Step...
At time: 171.3103277683258 and batch: 50, loss is 4.842605743408203 and perplexity is 126.79932813994381
At time: 172.25880646705627 and batch: 100, loss is 4.789952383041382 and perplexity is 120.29564041430413
At time: 173.194251537323 and batch: 150, loss is 4.806289281845093 and perplexity is 122.27703900394094
At time: 174.12917280197144 and batch: 200, loss is 4.814501314163208 and perplexity is 123.28531633672594
At time: 175.06174278259277 and batch: 250, loss is 4.8646594905853275 and perplexity is 129.62679195300532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.232059478759766 and perplexity of 187.17789574000608
Finished 33 epochs...
Completing Train Step...
At time: 176.63825869560242 and batch: 50, loss is 4.842121782302857 and perplexity is 126.73797704390961
At time: 177.58240866661072 and batch: 100, loss is 4.789808444976806 and perplexity is 120.27832653873875
At time: 178.5296595096588 and batch: 150, loss is 4.806322412490845 and perplexity is 122.2810901883126
At time: 179.4634816646576 and batch: 200, loss is 4.814114456176758 and perplexity is 123.23763165168432
At time: 180.39864039421082 and batch: 250, loss is 4.864026355743408 and perplexity is 129.5447466901716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.231871032714844 and perplexity of 187.14262612917085
Finished 34 epochs...
Completing Train Step...
At time: 181.97045350074768 and batch: 50, loss is 4.841630821228027 and perplexity is 126.67576890261593
At time: 182.9086537361145 and batch: 100, loss is 4.7896657371521 and perplexity is 120.26116310510632
At time: 183.8441743850708 and batch: 150, loss is 4.806335277557373 and perplexity is 122.28266335279247
At time: 184.77722144126892 and batch: 200, loss is 4.81372727394104 and perplexity is 123.18992546602662
At time: 185.7100794315338 and batch: 250, loss is 4.863414306640625 and perplexity is 129.46548320323697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.23167610168457 and perplexity of 187.10614977955305
Finished 35 epochs...
Completing Train Step...
At time: 187.2645559310913 and batch: 50, loss is 4.841132278442383 and perplexity is 126.61263135160657
At time: 188.2124707698822 and batch: 100, loss is 4.789504499435425 and perplexity is 120.24177403292943
At time: 189.14695954322815 and batch: 150, loss is 4.8063226985931395 and perplexity is 122.28112517321811
At time: 190.08026933670044 and batch: 200, loss is 4.813328161239624 and perplexity is 123.1407686123015
At time: 191.01243662834167 and batch: 250, loss is 4.862811737060547 and perplexity is 129.38749474048595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.231459426879883 and perplexity of 187.06561298290356
Finished 36 epochs...
Completing Train Step...
At time: 192.58843111991882 and batch: 50, loss is 4.8406057548522945 and perplexity is 126.54598436154272
At time: 193.52096581459045 and batch: 100, loss is 4.789320697784424 and perplexity is 120.21967542728527
At time: 194.4696159362793 and batch: 150, loss is 4.80627025604248 and perplexity is 122.27471260726367
At time: 195.4047634601593 and batch: 200, loss is 4.812923879623413 and perplexity is 123.09099512527628
At time: 196.3361029624939 and batch: 250, loss is 4.862241249084473 and perplexity is 129.3137017814815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.231260681152344 and perplexity of 187.02843818584222
Finished 37 epochs...
Completing Train Step...
At time: 197.88369488716125 and batch: 50, loss is 4.840078220367432 and perplexity is 126.47924459618277
At time: 198.8313591480255 and batch: 100, loss is 4.789159116744995 and perplexity is 120.20025177645897
At time: 199.76749682426453 and batch: 150, loss is 4.806223754882812 and perplexity is 122.26902682352818
At time: 200.70029830932617 and batch: 200, loss is 4.81256552696228 and perplexity is 123.04689304213306
At time: 201.63126802444458 and batch: 250, loss is 4.8617006778717045 and perplexity is 129.24381740732414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.231047058105469 and perplexity of 186.9884888682241
Finished 38 epochs...
Completing Train Step...
At time: 203.17949843406677 and batch: 50, loss is 4.839571142196656 and perplexity is 126.41512599013838
At time: 204.12521743774414 and batch: 100, loss is 4.788983306884766 and perplexity is 120.17912124452701
At time: 205.05948424339294 and batch: 150, loss is 4.806175050735473 and perplexity is 122.2630719598452
At time: 205.99027633666992 and batch: 200, loss is 4.812200984954834 and perplexity is 123.00204545564472
At time: 206.92065811157227 and batch: 250, loss is 4.861118545532227 and perplexity is 129.16860229623214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.230772399902344 and perplexity of 186.93713799815816
Finished 39 epochs...
Completing Train Step...
At time: 208.48584842681885 and batch: 50, loss is 4.839065608978271 and perplexity is 126.35123509548959
At time: 209.41807913780212 and batch: 100, loss is 4.788764591217041 and perplexity is 120.15283906204479
At time: 210.35073041915894 and batch: 150, loss is 4.806092643737793 and perplexity is 122.25299704228543
At time: 211.28389167785645 and batch: 200, loss is 4.8118254089355466 and perplexity is 122.95585751113337
At time: 212.23066663742065 and batch: 250, loss is 4.860550985336304 and perplexity is 129.09531213926113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.230474472045898 and perplexity of 186.8814525128848
Finished 40 epochs...
Completing Train Step...
At time: 213.7898497581482 and batch: 50, loss is 4.838596830368042 and perplexity is 126.29201821998748
At time: 214.736332654953 and batch: 100, loss is 4.7885926151275635 and perplexity is 120.1321774233481
At time: 215.66990661621094 and batch: 150, loss is 4.806053562164307 and perplexity is 122.24821929615906
At time: 216.60249543190002 and batch: 200, loss is 4.811499757766724 and perplexity is 122.915823311367
At time: 217.53354930877686 and batch: 250, loss is 4.860067129135132 and perplexity is 129.0328636811955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.230283737182617 and perplexity of 186.84581110372764
Finished 41 epochs...
Completing Train Step...
At time: 219.10004901885986 and batch: 50, loss is 4.838149538040161 and perplexity is 126.2355414009307
At time: 220.03079271316528 and batch: 100, loss is 4.788457508087158 and perplexity is 120.11594781679074
At time: 220.9631028175354 and batch: 150, loss is 4.806018257141114 and perplexity is 122.24390339612845
At time: 221.895272731781 and batch: 200, loss is 4.8111966037750244 and perplexity is 122.87856653644253
At time: 222.82515239715576 and batch: 250, loss is 4.85958402633667 and perplexity is 128.9705425986143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.230136108398438 and perplexity of 186.8182293197881
Finished 42 epochs...
Completing Train Step...
At time: 224.39006924629211 and batch: 50, loss is 4.837700290679932 and perplexity is 126.17884315391164
At time: 225.32122015953064 and batch: 100, loss is 4.788314647674561 and perplexity is 120.09878922859677
At time: 226.25384664535522 and batch: 150, loss is 4.805971899032593 and perplexity is 122.23823653134238
At time: 227.18659162521362 and batch: 200, loss is 4.810891590118408 and perplexity is 122.84109261086356
At time: 228.12016105651855 and batch: 250, loss is 4.859110240936279 and perplexity is 128.90945271134174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.229981231689453 and perplexity of 186.7892977677223
Finished 43 epochs...
Completing Train Step...
At time: 229.6652500629425 and batch: 50, loss is 4.837253818511963 and perplexity is 126.12252038646655
At time: 230.60986399650574 and batch: 100, loss is 4.788163480758667 and perplexity is 120.08063563717224
At time: 231.54288911819458 and batch: 150, loss is 4.805915069580078 and perplexity is 122.23128999666972
At time: 232.47500133514404 and batch: 200, loss is 4.810564374923706 and perplexity is 122.8009037143948
At time: 233.40717816352844 and batch: 250, loss is 4.858640871047974 and perplexity is 128.84896069359746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.229798126220703 and perplexity of 186.75509875690557
Finished 44 epochs...
Completing Train Step...
At time: 234.9850344657898 and batch: 50, loss is 4.83679841041565 and perplexity is 126.06509624625683
At time: 235.91545701026917 and batch: 100, loss is 4.787975702285767 and perplexity is 120.05808919572186
At time: 236.86389327049255 and batch: 150, loss is 4.805820922851563 and perplexity is 122.21978286228234
At time: 237.7950141429901 and batch: 200, loss is 4.810185012817382 and perplexity is 122.75432654026935
At time: 238.7281997203827 and batch: 250, loss is 4.858165302276611 and perplexity is 128.78769871996448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.229574203491211 and perplexity of 186.7132847271759
Finished 45 epochs...
Completing Train Step...
At time: 240.28056621551514 and batch: 50, loss is 4.836345310211182 and perplexity is 126.00798906398634
At time: 241.22552180290222 and batch: 100, loss is 4.787781267166138 and perplexity is 120.03474795603859
At time: 242.1567165851593 and batch: 150, loss is 4.805718479156494 and perplexity is 122.20726285742411
At time: 243.09032082557678 and batch: 200, loss is 4.809822158813477 and perplexity is 122.7097927215133
At time: 244.02613162994385 and batch: 250, loss is 4.857736568450928 and perplexity is 128.73249491190683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.229349136352539 and perplexity of 186.67126643107656
Finished 46 epochs...
Completing Train Step...
At time: 245.57721304893494 and batch: 50, loss is 4.8359213542938235 and perplexity is 125.95457855403912
At time: 246.52252745628357 and batch: 100, loss is 4.7876190757751464 and perplexity is 120.01528093203481
At time: 247.45644187927246 and batch: 150, loss is 4.805630350112915 and perplexity is 122.19649332279153
At time: 248.39032912254333 and batch: 200, loss is 4.809479713439941 and perplexity is 122.66777851490426
At time: 249.32153582572937 and batch: 250, loss is 4.8573258590698245 and perplexity is 128.67963412454733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.229157257080078 and perplexity of 186.63545152046373
Finished 47 epochs...
Completing Train Step...
At time: 250.88998699188232 and batch: 50, loss is 4.835519256591797 and perplexity is 125.90394268840771
At time: 251.8219313621521 and batch: 100, loss is 4.787457647323609 and perplexity is 119.99590861473676
At time: 252.75514364242554 and batch: 150, loss is 4.805536680221557 and perplexity is 122.18504772659988
At time: 253.68788838386536 and batch: 200, loss is 4.8091465187072755 and perplexity is 122.62691306568955
At time: 254.6183009147644 and batch: 250, loss is 4.856914520263672 and perplexity is 128.62671408225006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2289894104003904 and perplexity of 186.60412800846134
Finished 48 epochs...
Completing Train Step...
At time: 256.1653470993042 and batch: 50, loss is 4.83513388633728 and perplexity is 125.85543240178025
At time: 257.1127555370331 and batch: 100, loss is 4.787292404174805 and perplexity is 119.97608175112545
At time: 258.04731011390686 and batch: 150, loss is 4.8054335498809815 and perplexity is 122.17244739076419
At time: 258.97935247421265 and batch: 200, loss is 4.808811912536621 and perplexity is 122.5858882078575
At time: 259.91143560409546 and batch: 250, loss is 4.856497411727905 and perplexity is 128.57307396953087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.228841781616211 and perplexity of 186.57658190126975
Finished 49 epochs...
Completing Train Step...
At time: 261.48548579216003 and batch: 50, loss is 4.834759502410889 and perplexity is 125.80832296989732
At time: 262.44109296798706 and batch: 100, loss is 4.787121124267578 and perplexity is 119.95553401873083
At time: 263.3744320869446 and batch: 150, loss is 4.805322694778442 and perplexity is 122.15890470223314
At time: 264.32095551490784 and batch: 200, loss is 4.808472366333008 and perplexity is 122.5442717006633
At time: 265.2541592121124 and batch: 250, loss is 4.856078948974609 and perplexity is 128.5192821827124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2287036895751955 and perplexity of 186.55081893914027
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b898>
SETTINGS FOR THIS RUN
{'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.0, 'wordvec_source': 'glove', 'anneal': 8.0, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 0.0}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.207892656326294 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 2.151499032974243 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 3.0832953453063965 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 4.015308141708374 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 4.945982217788696 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 1 epochs...
Completing Train Step...
At time: 6.506702899932861 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 7.438013315200806 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 8.368449211120605 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 9.297720909118652 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 10.228734016418457 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 2 epochs...
Completing Train Step...
At time: 11.7861328125 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 12.71510362625122 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 13.643532752990723 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 14.570277452468872 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 15.498550653457642 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 3 epochs...
Completing Train Step...
At time: 17.041542053222656 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 17.987621307373047 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 18.91667127609253 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 19.848443508148193 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 20.775630235671997 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 4 epochs...
Completing Train Step...
At time: 22.350005865097046 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 23.278874158859253 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 24.20817279815674 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 25.137574911117554 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 26.067493200302124 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 5 epochs...
Completing Train Step...
At time: 27.60628080368042 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 28.54855227470398 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 29.479088306427002 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 30.41023826599121 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 31.34487533569336 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 6 epochs...
Completing Train Step...
At time: 32.892807483673096 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 33.8358473777771 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 34.763556718826294 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 35.69471216201782 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 36.62891674041748 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 7 epochs...
Completing Train Step...
At time: 38.191652059555054 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 39.12666440010071 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 40.063204526901245 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 40.99620223045349 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 41.94688391685486 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 8 epochs...
Completing Train Step...
At time: 43.495126247406006 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 44.44518280029297 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 45.37761044502258 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 46.312278747558594 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 47.24633836746216 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 9 epochs...
Completing Train Step...
At time: 48.80893087387085 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 49.744473934173584 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 50.67970085144043 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 51.614468812942505 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 52.55053353309631 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 10 epochs...
Completing Train Step...
At time: 54.113184452056885 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 55.04861402511597 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 55.983094930648804 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 56.918333768844604 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 57.851415157318115 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 11 epochs...
Completing Train Step...
At time: 59.39849305152893 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 60.348063707351685 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 61.287702322006226 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 62.22229552268982 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 63.15519404411316 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 12 epochs...
Completing Train Step...
At time: 64.71354722976685 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 65.64638018608093 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 66.59548282623291 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 67.52931904792786 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 68.46411156654358 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 13 epochs...
Completing Train Step...
At time: 70.0426778793335 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 71.03315901756287 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 71.98732876777649 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 72.91743326187134 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 73.84865880012512 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 14 epochs...
Completing Train Step...
At time: 75.38753533363342 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 76.3537540435791 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 77.29473662376404 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 78.25064516067505 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 79.18273496627808 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 15 epochs...
Completing Train Step...
At time: 80.74299955368042 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 81.67790651321411 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 82.61580085754395 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 83.55017256736755 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 84.48559188842773 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 16 epochs...
Completing Train Step...
At time: 86.0501549243927 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 87.00001811981201 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 87.93309617042542 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 88.86773443222046 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 89.8057312965393 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 17 epochs...
Completing Train Step...
At time: 91.37664771080017 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 92.31069231033325 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 93.24641442298889 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 94.17926025390625 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 95.11510992050171 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 18 epochs...
Completing Train Step...
At time: 96.67889595031738 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 97.6123399734497 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 98.54772734642029 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 99.48179721832275 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 100.41499018669128 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 19 epochs...
Completing Train Step...
At time: 101.9674711227417 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 102.91600251197815 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 103.85153889656067 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 104.785325050354 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 105.72037363052368 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 20 epochs...
Completing Train Step...
At time: 107.2829852104187 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 108.21982979774475 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 109.15380692481995 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 110.08753180503845 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 111.02179265022278 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 21 epochs...
Completing Train Step...
At time: 112.56833529472351 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 113.51749587059021 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 114.45072627067566 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 115.40039157867432 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 116.33451318740845 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 22 epochs...
Completing Train Step...
At time: 117.88101696968079 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 118.83011841773987 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 119.7651298046112 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 120.69822239875793 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 121.63966655731201 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 23 epochs...
Completing Train Step...
At time: 123.1996898651123 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 124.13365054130554 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 125.06902837753296 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 126.00336694717407 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 126.9357807636261 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 24 epochs...
Completing Train Step...
At time: 128.48411965370178 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 129.4345052242279 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 130.37068223953247 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 131.30573797225952 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 132.2389509677887 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 25 epochs...
Completing Train Step...
At time: 133.79695749282837 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 134.73122715950012 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 135.66457343101501 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 136.60031962394714 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 137.53252530097961 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 26 epochs...
Completing Train Step...
At time: 139.09685611724854 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 140.03204202651978 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 140.96961402893066 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 141.9041302204132 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 142.83645033836365 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 27 epochs...
Completing Train Step...
At time: 144.38463640213013 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 145.3431568145752 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 146.27884435653687 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 147.21354722976685 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 148.14754986763 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 28 epochs...
Completing Train Step...
At time: 149.71581292152405 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 150.6489179134369 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 151.589173078537 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 152.5242383480072 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 153.46198773384094 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 29 epochs...
Completing Train Step...
At time: 155.0101671218872 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 155.95768094062805 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 156.8921127319336 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 157.82806253433228 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 158.7721028327942 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 30 epochs...
Completing Train Step...
At time: 160.32653617858887 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 161.27523970603943 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 162.20802974700928 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 163.15662336349487 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 164.09016180038452 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 31 epochs...
Completing Train Step...
At time: 165.65167212486267 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 166.5869927406311 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 167.52080988883972 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 168.4548795223236 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 169.3873107433319 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 32 epochs...
Completing Train Step...
At time: 170.93883895874023 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 171.8879325389862 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 172.8223156929016 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 173.75499057769775 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 174.6879096031189 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 33 epochs...
Completing Train Step...
At time: 176.24755668640137 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 177.1816828250885 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 178.11569166183472 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 179.04839158058167 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 179.98553276062012 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 34 epochs...
Completing Train Step...
At time: 181.5522153377533 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 182.48773837089539 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 183.42218136787415 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 184.35482692718506 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 185.28784155845642 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 35 epochs...
Completing Train Step...
At time: 186.8393430709839 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 187.78702473640442 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 188.7203176021576 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 189.65428805351257 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 190.5864279270172 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 36 epochs...
Completing Train Step...
At time: 192.14957094192505 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 193.08392357826233 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 194.01890587806702 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 194.95221519470215 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 195.8855369091034 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 37 epochs...
Completing Train Step...
At time: 197.4319362640381 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 198.38315987586975 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 199.31571125984192 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 200.25057792663574 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 201.18350863456726 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 38 epochs...
Completing Train Step...
At time: 202.7282156944275 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 203.67713332176208 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 204.61391830444336 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 205.54740571975708 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 206.48328495025635 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 39 epochs...
Completing Train Step...
At time: 208.0556926727295 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 208.98995661735535 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 209.92434978485107 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 210.85931038856506 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 211.81190395355225 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 40 epochs...
Completing Train Step...
At time: 213.35996770858765 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 214.31010484695435 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 215.24302458763123 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 216.17494297027588 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 217.10859155654907 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 41 epochs...
Completing Train Step...
At time: 218.6747763156891 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 219.60783624649048 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 220.54278540611267 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 221.47577810287476 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 222.4106068611145 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 42 epochs...
Completing Train Step...
At time: 223.9733431339264 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 224.9075789451599 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 225.84019303321838 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 226.77414298057556 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 227.7094943523407 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 43 epochs...
Completing Train Step...
At time: 229.25512719154358 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 230.2043855190277 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 231.13790321350098 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 232.07074308395386 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 233.00335454940796 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 44 epochs...
Completing Train Step...
At time: 234.564701795578 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 235.49792861938477 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 236.44437217712402 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 237.38150334358215 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 238.3156988620758 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 45 epochs...
Completing Train Step...
At time: 239.86872720718384 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 240.8191647529602 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 241.75050449371338 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 242.71572875976562 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 243.65677785873413 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 46 epochs...
Completing Train Step...
At time: 245.21977305412292 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 246.16928458213806 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 247.10328078269958 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 248.0356514453888 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 248.96974110603333 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 47 epochs...
Completing Train Step...
At time: 250.53091311454773 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 251.46518230438232 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 252.39965438842773 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 253.34125590324402 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 254.27332258224487 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 48 epochs...
Completing Train Step...
At time: 255.82516431808472 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 256.7739791870117 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 257.7078878879547 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 258.64222598075867 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 259.57722449302673 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished 49 epochs...
Completing Train Step...
At time: 261.1411271095276 and batch: 50, loss is 9.168866901397704 and perplexity is 9593.747866619791
At time: 262.0750057697296 and batch: 100, loss is 9.168632678985595 and perplexity is 9591.501058989928
At time: 263.0106716156006 and batch: 150, loss is 9.168483180999756 and perplexity is 9590.067256078424
At time: 263.9448733329773 and batch: 200, loss is 9.168454666137695 and perplexity is 9589.793800532256
At time: 264.8787543773651 and batch: 250, loss is 9.16885181427002 and perplexity is 9593.603125612624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.626936340332032 and perplexity of 15167.895581378501
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f5e5eb8b898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.8204744866035563, 'wordvec_source': 'glove', 'anneal': 3.6633097241630748, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 11.519108002862913}, 'best_accuracy': -131.19747669256088}, {'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.4781280975888139, 'wordvec_source': 'glove', 'anneal': 2.655126351271187, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 24.944057254752767}, 'best_accuracy': -209.6579293865639}, {'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.6106093655028537, 'wordvec_source': 'glove', 'anneal': 7.091284921166074, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 9.700055246834742}, 'best_accuracy': -98.32131247764258}, {'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.34062417461774785, 'wordvec_source': 'glove', 'anneal': 6.266342664128619, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 28.299008542620673}, 'best_accuracy': -199.02945624120514}, {'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.29596629794564455, 'wordvec_source': 'glove', 'anneal': 7.578000455271355, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 24.787014096733614}, 'best_accuracy': -186.55081893914027}, {'params': {'tune_wordvecs': True, 'seq_len': 50, 'num_layers': 1, 'dropout': 0.0, 'wordvec_source': 'glove', 'anneal': 8.0, 'data': 'ptb', 'batch_size': 80, 'wordvec_dim': 200, 'lr': 0.0}, 'best_accuracy': -15167.895581378501}]
