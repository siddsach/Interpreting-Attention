Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 2.6228361864643315, 'wordvec_source': '', 'dropout': 0.042943023229184196, 'anneal': 2.149867874059633}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5030136108398438 and batch: 50, loss is 6.711075639724731 and perplexity is 821.4537526980678
At time: 2.476759910583496 and batch: 100, loss is 5.863754062652588 and perplexity is 352.0432589011059
At time: 3.4403281211853027 and batch: 150, loss is 5.605894012451172 and perplexity is 272.0250105039997
At time: 4.399505615234375 and batch: 200, loss is 5.438107385635376 and perplexity is 230.00645773445703
At time: 5.358487844467163 and batch: 250, loss is 5.30357008934021 and perplexity is 201.05330850540437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.401319885253907 and perplexity of 221.69884020866843
Finished 1 epochs...
Completing Train Step...
At time: 6.961402177810669 and batch: 50, loss is 5.099430437088013 and perplexity is 163.9285131042101
At time: 7.9134461879730225 and batch: 100, loss is 4.959177312850952 and perplexity is 142.47653405508953
At time: 8.87486720085144 and batch: 150, loss is 4.9253997707366945 and perplexity is 137.7443968163926
At time: 9.832131385803223 and batch: 200, loss is 4.848195819854737 and perplexity is 127.51013094764579
At time: 10.784705877304077 and batch: 250, loss is 4.832442817687988 and perplexity is 125.51720209790197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.05611572265625 and perplexity of 156.97957834562752
Finished 2 epochs...
Completing Train Step...
At time: 12.400336742401123 and batch: 50, loss is 4.74234561920166 and perplexity is 114.70293579167935
At time: 13.354991436004639 and batch: 100, loss is 4.63440523147583 and perplexity is 102.9666584519249
At time: 14.307941198348999 and batch: 150, loss is 4.660476570129394 and perplexity is 105.68643715589282
At time: 15.260807275772095 and batch: 200, loss is 4.6093378925323485 and perplexity is 100.41764035111144
At time: 16.218395709991455 and batch: 250, loss is 4.622507333755493 and perplexity is 101.74883084150342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.91151123046875 and perplexity of 135.84455179017206
Finished 3 epochs...
Completing Train Step...
At time: 17.805819511413574 and batch: 50, loss is 4.555132322311401 and perplexity is 95.11934081391867
At time: 18.777784824371338 and batch: 100, loss is 4.4502467918396 and perplexity is 85.64807864105114
At time: 19.73525047302246 and batch: 150, loss is 4.501188344955445 and perplexity is 90.12416628905481
At time: 20.69521927833557 and batch: 200, loss is 4.460207319259643 and perplexity is 86.5054414814838
At time: 21.65273690223694 and batch: 250, loss is 4.4847189235687255 and perplexity is 88.65202931467677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.833209228515625 and perplexity of 125.6134367135597
Finished 4 epochs...
Completing Train Step...
At time: 23.243961334228516 and batch: 50, loss is 4.425761947631836 and perplexity is 83.57646385616307
At time: 24.216100454330444 and batch: 100, loss is 4.320586071014405 and perplexity is 75.2327070830671
At time: 25.17456364631653 and batch: 150, loss is 4.385194454193115 and perplexity is 80.2538273889525
At time: 26.131455183029175 and batch: 200, loss is 4.349488105773926 and perplexity is 77.43881229677402
At time: 27.105881929397583 and batch: 250, loss is 4.380053157806397 and perplexity is 79.84227753261061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.78277702331543 and perplexity of 119.43556528522288
Finished 5 epochs...
Completing Train Step...
At time: 28.750376224517822 and batch: 50, loss is 4.32622652053833 and perplexity is 75.65825237507256
At time: 29.70823836326599 and batch: 100, loss is 4.220546236038208 and perplexity is 68.07065678189666
At time: 30.665106773376465 and batch: 150, loss is 4.292710256576538 and perplexity is 73.16449458027786
At time: 31.622633934020996 and batch: 200, loss is 4.260734872817993 and perplexity is 70.86203891108713
At time: 32.58150839805603 and batch: 250, loss is 4.293940744400024 and perplexity is 73.25457801188696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7458251953125 and perplexity of 115.10274857301202
Finished 6 epochs...
Completing Train Step...
At time: 34.18881940841675 and batch: 50, loss is 4.24415210723877 and perplexity is 69.6966398072177
At time: 35.16511392593384 and batch: 100, loss is 4.13830726146698 and perplexity is 62.696602619068535
At time: 36.12197399139404 and batch: 150, loss is 4.214677710533142 and perplexity is 67.67235226963969
At time: 37.08195614814758 and batch: 200, loss is 4.187208414077759 and perplexity is 65.83873968361961
At time: 38.047202825546265 and batch: 250, loss is 4.22103892326355 and perplexity is 68.10420258803023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.716570663452148 and perplexity of 111.78424879501945
Finished 7 epochs...
Completing Train Step...
At time: 39.64136981964111 and batch: 50, loss is 4.173551621437073 and perplexity is 64.94570553889423
At time: 40.621081590652466 and batch: 100, loss is 4.06797833442688 and perplexity is 58.43869958096246
At time: 41.58492970466614 and batch: 150, loss is 4.147573552131653 and perplexity is 63.28026759094603
At time: 42.546769857406616 and batch: 200, loss is 4.1233819341659546 and perplexity is 61.76778400752993
At time: 43.50794339179993 and batch: 250, loss is 4.158542432785034 and perplexity is 63.97820207617162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.700419998168945 and perplexity of 109.99335976184787
Finished 8 epochs...
Completing Train Step...
At time: 45.11379408836365 and batch: 50, loss is 4.112104511260986 and perplexity is 61.07511568331644
At time: 46.06932497024536 and batch: 100, loss is 4.007249846458435 and perplexity is 54.99541655841986
At time: 47.02912783622742 and batch: 150, loss is 4.088753113746643 and perplexity is 59.66544927466805
At time: 47.987937927246094 and batch: 200, loss is 4.067283940315247 and perplexity is 58.39813417790044
At time: 48.945900201797485 and batch: 250, loss is 4.103612670898437 and perplexity is 60.558671435318644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6898761749267575 and perplexity of 108.83970188892299
Finished 9 epochs...
Completing Train Step...
At time: 50.56824707984924 and batch: 50, loss is 4.056866188049316 and perplexity is 57.792914869326445
At time: 51.52662372589111 and batch: 100, loss is 3.9533053731918333 and perplexity is 52.10731662663843
At time: 52.48191690444946 and batch: 150, loss is 4.035612111091614 and perplexity is 56.577541381865075
At time: 53.44166278839111 and batch: 200, loss is 4.0172642993927 and perplexity is 55.54893252296396
At time: 54.39640784263611 and batch: 250, loss is 4.054233694076538 and perplexity is 57.64097544681601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.681970596313477 and perplexity of 107.98265326568703
Finished 10 epochs...
Completing Train Step...
At time: 55.98572087287903 and batch: 50, loss is 4.006694173812866 and perplexity is 54.96486559875908
At time: 56.95717215538025 and batch: 100, loss is 3.90510947227478 and perplexity is 49.65551551688672
At time: 57.91275930404663 and batch: 150, loss is 3.9877742767333983 and perplexity is 53.93471192886474
At time: 58.885597229003906 and batch: 200, loss is 3.9721725034713744 and perplexity is 53.09976504488667
At time: 59.84561538696289 and batch: 250, loss is 4.009210991859436 and perplexity is 55.103376394545656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.678945541381836 and perplexity of 107.65649338239865
Finished 11 epochs...
Completing Train Step...
At time: 61.44720673561096 and batch: 50, loss is 3.960620193481445 and perplexity is 52.4898697313844
At time: 62.40241765975952 and batch: 100, loss is 3.8608396577835085 and perplexity is 47.505222757395686
At time: 63.358566761016846 and batch: 150, loss is 3.9437489318847656 and perplexity is 51.61172791748728
At time: 64.31576895713806 and batch: 200, loss is 3.9309324741363527 and perplexity is 50.954469248131176
At time: 65.27377128601074 and batch: 250, loss is 3.968066930770874 and perplexity is 52.8822070048818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.67781982421875 and perplexity of 107.53537080775189
Finished 12 epochs...
Completing Train Step...
At time: 66.86600279808044 and batch: 50, loss is 3.918666648864746 and perplexity is 50.33328806948956
At time: 67.83677649497986 and batch: 100, loss is 3.8203328895568847 and perplexity is 45.61939201265474
At time: 68.79308605194092 and batch: 150, loss is 3.9027250146865846 and perplexity is 49.5372550956283
At time: 69.7486662864685 and batch: 200, loss is 3.8925102233886717 and perplexity is 49.03381800323041
At time: 70.70781826972961 and batch: 250, loss is 3.930464677810669 and perplexity is 50.93063850904041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.680657196044922 and perplexity of 107.8409219152596
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 72.30369853973389 and batch: 50, loss is 3.875887031555176 and perplexity is 48.22545682617785
At time: 73.27670931816101 and batch: 100, loss is 3.7637228775024414 and perplexity is 43.108615705953724
At time: 74.23594737052917 and batch: 150, loss is 3.830686926841736 and perplexity is 46.094190698344995
At time: 75.19564580917358 and batch: 200, loss is 3.80532591342926 and perplexity is 44.93989424156758
At time: 76.15711283683777 and batch: 250, loss is 3.828684439659119 and perplexity is 46.00198002843414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6495918273925785 and perplexity of 104.54230556185315
Finished 14 epochs...
Completing Train Step...
At time: 77.7729766368866 and batch: 50, loss is 3.832977638244629 and perplexity is 46.199900215358184
At time: 78.73910236358643 and batch: 100, loss is 3.729712448120117 and perplexity is 41.66712498110085
At time: 79.70406532287598 and batch: 150, loss is 3.804249129295349 and perplexity is 44.89152972021632
At time: 80.6666202545166 and batch: 200, loss is 3.7856372928619386 and perplexity is 44.06374312153952
At time: 81.62985682487488 and batch: 250, loss is 3.8151588916778563 and perplexity is 45.383966944738575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.651457977294922 and perplexity of 104.73757932355352
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 83.22914052009583 and batch: 50, loss is 3.8108954429626465 and perplexity is 45.190886615685585
At time: 84.20967531204224 and batch: 100, loss is 3.7027265453338623 and perplexity is 40.557736249400925
At time: 85.17954540252686 and batch: 150, loss is 3.7702981281280517 and perplexity is 43.39299958138582
At time: 86.14445352554321 and batch: 200, loss is 3.7435368871688843 and perplexity is 42.24714963085437
At time: 87.11004304885864 and batch: 250, loss is 3.765311918258667 and perplexity is 43.17717150781099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.640689086914063 and perplexity of 103.61572322741269
Finished 16 epochs...
Completing Train Step...
At time: 88.73625993728638 and batch: 50, loss is 3.789315962791443 and perplexity is 44.22613760305496
At time: 89.70040798187256 and batch: 100, loss is 3.684248571395874 and perplexity is 39.81519293147407
At time: 90.66364669799805 and batch: 150, loss is 3.7567294073104858 and perplexity is 42.808188625369425
At time: 91.62489557266235 and batch: 200, loss is 3.735091471672058 and perplexity is 41.891857306085484
At time: 92.58552408218384 and batch: 250, loss is 3.761397066116333 and perplexity is 43.00846970220747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.64189567565918 and perplexity of 103.74082024802769
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 94.2102575302124 and batch: 50, loss is 3.7792266702651975 and perplexity is 43.78217058611004
At time: 95.16937518119812 and batch: 100, loss is 3.6721475315093994 and perplexity is 39.336291142538435
At time: 96.12863612174988 and batch: 150, loss is 3.740244274139404 and perplexity is 42.108274871407865
At time: 97.09248304367065 and batch: 200, loss is 3.714261736869812 and perplexity is 41.02828622974886
At time: 98.0523190498352 and batch: 250, loss is 3.7369381380081177 and perplexity is 41.969289062031514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6389007568359375 and perplexity of 103.43058970226416
Finished 18 epochs...
Completing Train Step...
At time: 99.64744567871094 and batch: 50, loss is 3.7680857944488526 and perplexity is 43.297105900492376
At time: 100.6206955909729 and batch: 100, loss is 3.6623794651031494 and perplexity is 38.95392218149435
At time: 101.5810341835022 and batch: 150, loss is 3.7334336614608765 and perplexity is 41.82246609190234
At time: 102.5400037765503 and batch: 200, loss is 3.7107283163070677 and perplexity is 40.88357185846207
At time: 103.50204610824585 and batch: 250, loss is 3.7359894561767577 and perplexity is 41.92949244017406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.639387893676758 and perplexity of 103.48098682712737
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 105.10792422294617 and batch: 50, loss is 3.7633003997802734 and perplexity is 43.090407122815655
At time: 106.06884002685547 and batch: 100, loss is 3.6569632959365843 and perplexity is 38.74351147361663
At time: 107.0282690525055 and batch: 150, loss is 3.725313277244568 and perplexity is 41.48422677361439
At time: 107.99927067756653 and batch: 200, loss is 3.700034146308899 and perplexity is 40.44868550979664
At time: 108.98826885223389 and batch: 250, loss is 3.722594766616821 and perplexity is 41.37160461387482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.637172698974609 and perplexity of 103.2520100010949
Finished 20 epochs...
Completing Train Step...
At time: 110.59920239448547 and batch: 50, loss is 3.757367506027222 and perplexity is 42.835513192555545
At time: 111.57911205291748 and batch: 100, loss is 3.651779179573059 and perplexity is 38.54318032046955
At time: 112.5393443107605 and batch: 150, loss is 3.721883873939514 and perplexity is 41.3422042945804
At time: 113.49937796592712 and batch: 200, loss is 3.698633871078491 and perplexity is 40.39208585417088
At time: 114.45998001098633 and batch: 250, loss is 3.722708740234375 and perplexity is 41.37632015403516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.637507629394531 and perplexity of 103.28659803213091
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 116.07570552825928 and batch: 50, loss is 3.7549376010894777 and perplexity is 42.73155332498796
At time: 117.05110001564026 and batch: 100, loss is 3.648929715156555 and perplexity is 38.4335092257941
At time: 118.01250386238098 and batch: 150, loss is 3.717737970352173 and perplexity is 41.171158316555996
At time: 118.9721200466156 and batch: 200, loss is 3.6931384563446046 and perplexity is 40.17072338680538
At time: 119.93543219566345 and batch: 250, loss is 3.715306715965271 and perplexity is 41.07118234004725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.63664665222168 and perplexity of 103.19770890021151
Finished 22 epochs...
Completing Train Step...
At time: 121.56080651283264 and batch: 50, loss is 3.751915678977966 and perplexity is 42.60261681530156
At time: 122.52287602424622 and batch: 100, loss is 3.646273627281189 and perplexity is 38.33156189839666
At time: 123.48322319984436 and batch: 150, loss is 3.7160606384277344 and perplexity is 41.102158502318375
At time: 124.4439377784729 and batch: 200, loss is 3.6925680828094483 and perplexity is 40.14781760234507
At time: 125.40517210960388 and batch: 250, loss is 3.7156851577758787 and perplexity is 41.086728334097344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636801528930664 and perplexity of 103.2136930594957
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 126.99749088287354 and batch: 50, loss is 3.7506058025360107 and perplexity is 42.54684918349684
At time: 127.9745979309082 and batch: 100, loss is 3.644629693031311 and perplexity is 38.26859909847205
At time: 128.95811295509338 and batch: 150, loss is 3.713942675590515 and perplexity is 41.015197780376766
At time: 129.9180130958557 and batch: 200, loss is 3.689724063873291 and perplexity is 40.03379866170472
At time: 130.87782764434814 and batch: 250, loss is 3.7118147802352905 and perplexity is 40.92801452284737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6364906311035154 and perplexity of 103.18160913426128
Finished 24 epochs...
Completing Train Step...
At time: 132.4982922077179 and batch: 50, loss is 3.7492067050933837 and perplexity is 42.48736361836684
At time: 133.46161890029907 and batch: 100, loss is 3.6433225202560426 and perplexity is 38.21860810813196
At time: 134.42346954345703 and batch: 150, loss is 3.713126873970032 and perplexity is 40.98175116032151
At time: 135.38893914222717 and batch: 200, loss is 3.689482374191284 and perplexity is 40.02412407480765
At time: 136.3652229309082 and batch: 250, loss is 3.712102994918823 and perplexity is 40.93981227766211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636544036865234 and perplexity of 103.1871197738411
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 137.9718759059906 and batch: 50, loss is 3.7484906482696534 and perplexity is 42.45695114155584
At time: 138.9361720085144 and batch: 100, loss is 3.642393741607666 and perplexity is 38.18312796010294
At time: 139.89760422706604 and batch: 150, loss is 3.712042598724365 and perplexity is 40.93733974346529
At time: 140.85612201690674 and batch: 200, loss is 3.688120098114014 and perplexity is 39.969637289515276
At time: 141.81667232513428 and batch: 250, loss is 3.71009081363678 and perplexity is 40.85751677820544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636535263061523 and perplexity of 103.18621443427834
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 143.4175956249237 and batch: 50, loss is 3.7480491256713866 and perplexity is 42.438209575890184
At time: 144.3896176815033 and batch: 100, loss is 3.641805763244629 and perplexity is 38.16068370604382
At time: 145.35094571113586 and batch: 150, loss is 3.71144513130188 and perplexity is 40.912888321796665
At time: 146.31112909317017 and batch: 200, loss is 3.6874831676483155 and perplexity is 39.944187515551235
At time: 147.26659512519836 and batch: 250, loss is 3.7091266679763795 and perplexity is 40.81814316470835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636541748046875 and perplexity of 103.18688359753725
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 148.86966800689697 and batch: 50, loss is 3.747834620475769 and perplexity is 42.429107335717255
At time: 149.82911276817322 and batch: 100, loss is 3.6415122318267823 and perplexity is 38.14948399026422
At time: 150.7887785434723 and batch: 150, loss is 3.7111480140686037 and perplexity is 40.900734203301596
At time: 151.75237321853638 and batch: 200, loss is 3.6871784114837647 and perplexity is 39.93201613291404
At time: 152.71135091781616 and batch: 250, loss is 3.7086725997924805 and perplexity is 40.79961315183445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636545562744141 and perplexity of 103.18727722501075
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 154.30689978599548 and batch: 50, loss is 3.7477325630187988 and perplexity is 42.42477734987848
At time: 155.28275322914124 and batch: 100, loss is 3.641373496055603 and perplexity is 38.14419165930904
At time: 156.24339723587036 and batch: 150, loss is 3.711006202697754 and perplexity is 40.894934425362415
At time: 157.20299673080444 and batch: 200, loss is 3.687034320831299 and perplexity is 39.92626271717185
At time: 158.1686351299286 and batch: 250, loss is 3.708460865020752 and perplexity is 40.79097536954883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.636547088623047 and perplexity of 103.18743467642054
Annealing...
Model not improving. Stopping early with 103.18160913426128loss at 28 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -103.18160913426128
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 8.968428148007806, 'wordvec_source': '', 'dropout': 0.6169485602122273, 'anneal': 7.59396827069606}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2784850597381592 and batch: 50, loss is 6.471837463378907 and perplexity is 646.670870006963
At time: 2.268845558166504 and batch: 100, loss is 5.715475368499756 and perplexity is 303.5284568870711
At time: 3.235438823699951 and batch: 150, loss is 5.618886518478393 and perplexity is 275.58235647057927
At time: 4.217272520065308 and batch: 200, loss is 5.554034671783447 and perplexity is 258.2775215635153
At time: 5.186326265335083 and batch: 250, loss is 5.5552441692352295 and perplexity is 258.59009655892413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.469867324829101 and perplexity of 237.4286897793885
Finished 1 epochs...
Completing Train Step...
At time: 6.794939994812012 and batch: 50, loss is 5.108822498321533 and perplexity is 165.47539255954624
At time: 7.7522711753845215 and batch: 100, loss is 4.886296472549438 and perplexity is 132.46208751883955
At time: 8.711198091506958 and batch: 150, loss is 4.84041202545166 and perplexity is 126.52147105839009
At time: 9.669766426086426 and batch: 200, loss is 4.77613507270813 and perplexity is 118.64490881712989
At time: 10.628950357437134 and batch: 250, loss is 4.775231380462646 and perplexity is 118.53773876473177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.966387939453125 and perplexity of 143.50759196119174
Finished 2 epochs...
Completing Train Step...
At time: 12.23852825164795 and batch: 50, loss is 4.632583093643189 and perplexity is 102.77920983852778
At time: 13.195817947387695 and batch: 100, loss is 4.50312294960022 and perplexity is 90.29868968221534
At time: 14.152514934539795 and batch: 150, loss is 4.532805013656616 and perplexity is 93.01911537104554
At time: 15.114048957824707 and batch: 200, loss is 4.501575736999512 and perplexity is 90.15908643751052
At time: 16.07343316078186 and batch: 250, loss is 4.520339622497558 and perplexity is 91.86679271022344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.861105346679688 and perplexity of 129.16689743014894
Finished 3 epochs...
Completing Train Step...
At time: 17.660733699798584 and batch: 50, loss is 4.418595085144043 and perplexity is 82.97962412131017
At time: 18.63323140144348 and batch: 100, loss is 4.307989645004272 and perplexity is 74.29098745739756
At time: 19.59254789352417 and batch: 150, loss is 4.350439672470093 and perplexity is 77.51253556228555
At time: 20.55297303199768 and batch: 200, loss is 4.336130418777466 and perplexity is 76.41128684066155
At time: 21.512006998062134 and batch: 250, loss is 4.360616207122803 and perplexity is 78.30537188733406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8137767791748045 and perplexity of 123.19602416304174
Finished 4 epochs...
Completing Train Step...
At time: 23.12034296989441 and batch: 50, loss is 4.275316400527954 and perplexity is 71.90288582200053
At time: 24.078941583633423 and batch: 100, loss is 4.173970117568969 and perplexity is 64.97289075350234
At time: 25.03629159927368 and batch: 150, loss is 4.2217389106750485 and perplexity is 68.15189136133526
At time: 25.99554681777954 and batch: 200, loss is 4.21159215927124 and perplexity is 67.46386756822376
At time: 26.95642375946045 and batch: 250, loss is 4.238164224624634 and perplexity is 69.28055149666011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.791890335083008 and perplexity of 120.52899363688789
Finished 5 epochs...
Completing Train Step...
At time: 28.545754432678223 and batch: 50, loss is 4.159498534202576 and perplexity is 64.03940097738452
At time: 29.520138263702393 and batch: 100, loss is 4.067028932571411 and perplexity is 58.38324410008277
At time: 30.478398084640503 and batch: 150, loss is 4.114455571174622 and perplexity is 61.21887586770418
At time: 31.43725061416626 and batch: 200, loss is 4.111944808959961 and perplexity is 61.06536262562003
At time: 32.394901275634766 and batch: 250, loss is 4.137851037979126 and perplexity is 62.66800548018432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.789045715332032 and perplexity of 120.18662167092022
Finished 6 epochs...
Completing Train Step...
At time: 33.985883951187134 and batch: 50, loss is 4.066249132156372 and perplexity is 58.33773456858414
At time: 34.96091079711914 and batch: 100, loss is 3.97424391746521 and perplexity is 53.209870639034605
At time: 35.91708564758301 and batch: 150, loss is 4.023764328956604 and perplexity is 55.91117825517337
At time: 36.8775475025177 and batch: 200, loss is 4.028449034690857 and perplexity is 56.17372015817799
At time: 37.837687492370605 and batch: 250, loss is 4.054307518005371 and perplexity is 57.64523088715995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.801188278198242 and perplexity of 121.65489151938444
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 39.45780539512634 and batch: 50, loss is 3.967818846702576 and perplexity is 52.869089399029235
At time: 40.41747236251831 and batch: 100, loss is 3.8175634813308714 and perplexity is 45.49322807351042
At time: 41.37743639945984 and batch: 150, loss is 3.8005939865112306 and perplexity is 44.727744281222854
At time: 42.33783793449402 and batch: 200, loss is 3.7129800367355346 and perplexity is 40.975733955101965
At time: 43.302584171295166 and batch: 250, loss is 3.65203004360199 and perplexity is 38.55285063088836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.679690551757813 and perplexity of 107.73672847128553
Finished 8 epochs...
Completing Train Step...
At time: 44.90285873413086 and batch: 50, loss is 3.8430291032791137 and perplexity is 46.66661856912768
At time: 45.874128580093384 and batch: 100, loss is 3.716531028747559 and perplexity is 41.121497107791576
At time: 46.83493638038635 and batch: 150, loss is 3.721203227043152 and perplexity is 41.31407442587843
At time: 47.79500222206116 and batch: 200, loss is 3.6583339405059814 and perplexity is 38.79665146691957
At time: 48.75448703765869 and batch: 250, loss is 3.641027865409851 and perplexity is 38.13101013581461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.670307540893555 and perplexity of 106.7305613978271
Finished 9 epochs...
Completing Train Step...
At time: 50.358553886413574 and batch: 50, loss is 3.7871184062957766 and perplexity is 44.12905487854284
At time: 51.328054904937744 and batch: 100, loss is 3.6662981510162354 and perplexity is 39.106869848817865
At time: 52.296226501464844 and batch: 150, loss is 3.6786801624298096 and perplexity is 39.59410178840465
At time: 53.27734899520874 and batch: 200, loss is 3.628885464668274 and perplexity is 37.670807765126504
At time: 54.24828553199768 and batch: 250, loss is 3.631179971694946 and perplexity is 37.75734293805959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.667494583129883 and perplexity of 106.43075470599346
Finished 10 epochs...
Completing Train Step...
At time: 55.858243227005005 and batch: 50, loss is 3.7474551677703856 and perplexity is 42.41301055032923
At time: 56.820708990097046 and batch: 100, loss is 3.630215644836426 and perplexity is 37.72095006828916
At time: 57.791502475738525 and batch: 150, loss is 3.647383522987366 and perplexity is 38.37412955281736
At time: 58.76020050048828 and batch: 200, loss is 3.6064409828186035 and perplexity is 36.834723837393994
At time: 59.719176292419434 and batch: 250, loss is 3.61934072971344 and perplexity is 37.31296038538335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.668215942382813 and perplexity of 106.50755721346626
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 61.30979132652283 and batch: 50, loss is 3.7200742626190184 and perplexity is 41.26745862437758
At time: 62.28287196159363 and batch: 100, loss is 3.595935263633728 and perplexity is 36.44977419947617
At time: 63.24290442466736 and batch: 150, loss is 3.6031242847442626 and perplexity is 36.71275655590571
At time: 64.20534992218018 and batch: 200, loss is 3.5384853506088256 and perplexity is 34.41475340909715
At time: 65.16960453987122 and batch: 250, loss is 3.536483130455017 and perplexity is 34.34591643262055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.656126022338867 and perplexity of 105.22764198902992
Finished 12 epochs...
Completing Train Step...
At time: 66.77151703834534 and batch: 50, loss is 3.703239517211914 and perplexity is 40.57854656463122
At time: 67.73068451881409 and batch: 100, loss is 3.581517610549927 and perplexity is 35.928024242946776
At time: 68.69215154647827 and batch: 150, loss is 3.592341914176941 and perplexity is 36.319032464208
At time: 69.65183663368225 and batch: 200, loss is 3.5335696220397947 and perplexity is 34.24599494779125
At time: 70.61088681221008 and batch: 250, loss is 3.539495029449463 and perplexity is 34.44951880540912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.655096054077148 and perplexity of 105.11931665293105
Finished 13 epochs...
Completing Train Step...
At time: 72.19974732398987 and batch: 50, loss is 3.694337306022644 and perplexity is 40.21891092463995
At time: 73.17410159111023 and batch: 100, loss is 3.573210163116455 and perplexity is 35.63079040682601
At time: 74.13426494598389 and batch: 150, loss is 3.5860602855682373 and perplexity is 36.091604846590684
At time: 75.09440398216248 and batch: 200, loss is 3.530819034576416 and perplexity is 34.151927772720406
At time: 76.05572772026062 and batch: 250, loss is 3.541107497215271 and perplexity is 34.505112353386984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654890441894532 and perplexity of 105.09770506267812
Finished 14 epochs...
Completing Train Step...
At time: 77.6474096775055 and batch: 50, loss is 3.6872182369232176 and perplexity is 39.933606474672594
At time: 78.62179040908813 and batch: 100, loss is 3.566762018203735 and perplexity is 35.40177705606784
At time: 79.58127784729004 and batch: 150, loss is 3.581213884353638 and perplexity is 35.917113617808496
At time: 80.54265809059143 and batch: 200, loss is 3.5285773992538454 and perplexity is 34.075457346516984
At time: 81.50136494636536 and batch: 250, loss is 3.5418028116226195 and perplexity is 34.529112598024476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654997253417969 and perplexity of 105.10893130820116
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 83.11468243598938 and batch: 50, loss is 3.6820244693756106 and perplexity is 39.72673828298191
At time: 84.080819606781 and batch: 100, loss is 3.5603926849365233 and perplexity is 35.177007914408556
At time: 85.04238414764404 and batch: 150, loss is 3.5743568325042725 and perplexity is 35.67167057698983
At time: 86.00463223457336 and batch: 200, loss is 3.5167768907547 and perplexity is 33.67571285537731
At time: 86.96603894233704 and batch: 250, loss is 3.5281866884231565 and perplexity is 34.06214629682599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654730224609375 and perplexity of 105.08086794253168
Finished 16 epochs...
Completing Train Step...
At time: 88.56014060974121 and batch: 50, loss is 3.6803742933273313 and perplexity is 39.66123623081383
At time: 89.53498864173889 and batch: 100, loss is 3.5588141965866087 and perplexity is 35.12152521814582
At time: 90.49744820594788 and batch: 150, loss is 3.573071851730347 and perplexity is 35.625862603610216
At time: 91.45899152755737 and batch: 200, loss is 3.5164060688018797 and perplexity is 33.66322747684297
At time: 92.42059302330017 and batch: 250, loss is 3.528820028305054 and perplexity is 34.083726045475196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654623031616211 and perplexity of 105.06960461345837
Finished 17 epochs...
Completing Train Step...
At time: 94.02682971954346 and batch: 50, loss is 3.67915958404541 and perplexity is 39.6130886076344
At time: 94.98834419250488 and batch: 100, loss is 3.5576421499252318 and perplexity is 35.080385265453
At time: 95.95150661468506 and batch: 150, loss is 3.5721939611434936 and perplexity is 35.59460071844711
At time: 96.91202592849731 and batch: 200, loss is 3.5161626052856447 and perplexity is 33.65503270671789
At time: 97.8722095489502 and batch: 250, loss is 3.5292687797546387 and perplexity is 34.09902459931272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6545665740966795 and perplexity of 105.06367281165272
Finished 18 epochs...
Completing Train Step...
At time: 99.4954559803009 and batch: 50, loss is 3.6780691003799437 and perplexity is 39.569914726053796
At time: 100.45576667785645 and batch: 100, loss is 3.5566226482391357 and perplexity is 35.04463897832215
At time: 101.4170081615448 and batch: 150, loss is 3.571466145515442 and perplexity is 35.56870383699065
At time: 102.37951254844666 and batch: 200, loss is 3.5159446239471435 and perplexity is 33.64769733715706
At time: 103.34221529960632 and batch: 250, loss is 3.5296012496948244 and perplexity is 34.110363384779866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654542541503906 and perplexity of 105.06114788952908
Finished 19 epochs...
Completing Train Step...
At time: 104.94931077957153 and batch: 50, loss is 3.6770500802993773 and perplexity is 39.52961272612272
At time: 105.92652893066406 and batch: 100, loss is 3.555687885284424 and perplexity is 35.011895853957995
At time: 106.88464045524597 and batch: 150, loss is 3.570813050270081 and perplexity is 35.54548166960003
At time: 107.8451840877533 and batch: 200, loss is 3.5157285261154176 and perplexity is 33.64042692830802
At time: 108.80734729766846 and batch: 250, loss is 3.5298569583892823 and perplexity is 34.11908681654961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654542541503906 and perplexity of 105.06114788952908
Finished 20 epochs...
Completing Train Step...
At time: 110.41226100921631 and batch: 50, loss is 3.6760815477371214 and perplexity is 39.49134554352171
At time: 111.37596106529236 and batch: 100, loss is 3.554809217453003 and perplexity is 34.98114553898876
At time: 112.3382499217987 and batch: 150, loss is 3.5702041387557983 and perplexity is 35.52384420484995
At time: 113.30380892753601 and batch: 200, loss is 3.5155086851119997 and perplexity is 33.63303219595917
At time: 114.26434659957886 and batch: 250, loss is 3.5300581216812135 and perplexity is 34.125951014760524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6545555114746096 and perplexity of 105.06251053837603
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 115.85494375228882 and batch: 50, loss is 3.675267777442932 and perplexity is 39.45922173211532
At time: 116.83281183242798 and batch: 100, loss is 3.553723201751709 and perplexity is 34.943176087126474
At time: 117.79660677909851 and batch: 150, loss is 3.56908371925354 and perplexity is 35.48406488593138
At time: 118.7580897808075 and batch: 200, loss is 3.5137706708908083 and perplexity is 33.57462827582438
At time: 119.72259783744812 and batch: 250, loss is 3.5279899406433106 and perplexity is 34.055445304389615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654541778564453 and perplexity of 105.06106773426495
Finished 22 epochs...
Completing Train Step...
At time: 121.31432151794434 and batch: 50, loss is 3.6751218271255492 and perplexity is 39.4534630664296
At time: 122.29219245910645 and batch: 100, loss is 3.553592367172241 and perplexity is 34.938604610437984
At time: 123.25484442710876 and batch: 150, loss is 3.5689833545684815 and perplexity is 35.48050371764527
At time: 124.21694540977478 and batch: 200, loss is 3.5137409734725953 and perplexity is 33.57363121085233
At time: 125.18824529647827 and batch: 250, loss is 3.5280398654937746 and perplexity is 34.05714555984607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654536437988281 and perplexity of 105.06050664912831
Finished 23 epochs...
Completing Train Step...
At time: 126.79818511009216 and batch: 50, loss is 3.674983253479004 and perplexity is 39.44799623497178
At time: 127.7596070766449 and batch: 100, loss is 3.5534689378738404 and perplexity is 34.93429242911405
At time: 128.7355420589447 and batch: 150, loss is 3.5688913249969483 and perplexity is 35.477238612335775
At time: 129.6985158920288 and batch: 200, loss is 3.513712754249573 and perplexity is 33.57268380243315
At time: 130.65952563285828 and batch: 250, loss is 3.528085165023804 and perplexity is 34.058688367478034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654533004760742 and perplexity of 105.06014595312274
Finished 24 epochs...
Completing Train Step...
At time: 132.25097227096558 and batch: 50, loss is 3.6748481464385985 and perplexity is 39.44266689297446
At time: 133.2248125076294 and batch: 100, loss is 3.553348994255066 and perplexity is 34.93010253494134
At time: 134.18340158462524 and batch: 150, loss is 3.568803791999817 and perplexity is 35.47413331921988
At time: 135.14229226112366 and batch: 200, loss is 3.5136846113204956 and perplexity is 33.57173898206902
At time: 136.10107851028442 and batch: 250, loss is 3.5281267166137695 and perplexity is 34.06010358953401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Finished 25 epochs...
Completing Train Step...
At time: 137.70758247375488 and batch: 50, loss is 3.6747155141830445 and perplexity is 39.43743587000824
At time: 138.67333984375 and batch: 100, loss is 3.5532310724258425 and perplexity is 34.92598375620719
At time: 139.64230751991272 and batch: 150, loss is 3.568718934059143 and perplexity is 35.471123185037925
At time: 140.60840010643005 and batch: 200, loss is 3.5136564064025877 and perplexity is 33.57079210728033
At time: 141.56865453720093 and batch: 250, loss is 3.528165130615234 and perplexity is 34.061411999533654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531860351563 and perplexity of 105.06002572139617
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 143.17404174804688 and batch: 50, loss is 3.6745994234085084 and perplexity is 39.43285781327259
At time: 144.13226628303528 and batch: 100, loss is 3.553077425956726 and perplexity is 34.92061791435451
At time: 145.09284377098083 and batch: 150, loss is 3.5685607767105103 and perplexity is 35.46551360985141
At time: 146.0531530380249 and batch: 200, loss is 3.5134238529205324 and perplexity is 33.56298601038312
At time: 147.0139832496643 and batch: 250, loss is 3.52788791179657 and perplexity is 34.05197084383059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531097412109 and perplexity of 105.0599455669881
Finished 27 epochs...
Completing Train Step...
At time: 148.60618257522583 and batch: 50, loss is 3.674582448005676 and perplexity is 39.43218843030792
At time: 149.5820927619934 and batch: 100, loss is 3.5530622816085815 and perplexity is 34.920089068363914
At time: 150.54126358032227 and batch: 150, loss is 3.568550066947937 and perplexity is 35.46513378465504
At time: 151.50227689743042 and batch: 200, loss is 3.513420557975769 and perplexity is 33.562875422380316
At time: 152.46424460411072 and batch: 250, loss is 3.527893681526184 and perplexity is 34.05216731506198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531860351563 and perplexity of 105.06002572139617
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 154.07108998298645 and batch: 50, loss is 3.6745675086975096 and perplexity is 39.43159934509355
At time: 155.03189420700073 and batch: 100, loss is 3.5530422639846804 and perplexity is 34.91939005815064
At time: 155.99374127388 and batch: 150, loss is 3.5685290813446047 and perplexity is 35.464389535234595
At time: 156.95826935768127 and batch: 200, loss is 3.513390049934387 and perplexity is 33.56185150040704
At time: 157.919695854187 and batch: 250, loss is 3.527857003211975 and perplexity is 34.05091836187459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531097412109 and perplexity of 105.0599455669881
Finished 29 epochs...
Completing Train Step...
At time: 159.514155626297 and batch: 50, loss is 3.6745659255981447 and perplexity is 39.43153692100308
At time: 160.48804450035095 and batch: 100, loss is 3.5530408382415772 and perplexity is 34.91934027210659
At time: 161.4499306678772 and batch: 150, loss is 3.5685281276702883 and perplexity is 35.46435571377327
At time: 162.41351675987244 and batch: 200, loss is 3.5133899593353273 and perplexity is 33.56184845973498
At time: 163.37682604789734 and batch: 250, loss is 3.5278579568862916 and perplexity is 34.05095083537636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 164.98102521896362 and batch: 50, loss is 3.674564685821533 and perplexity is 39.431488034736155
At time: 165.9597406387329 and batch: 100, loss is 3.5530390214920042 and perplexity is 34.91927683246769
At time: 166.92279982566833 and batch: 150, loss is 3.5685262060165406 and perplexity is 35.46428756362669
At time: 167.88183069229126 and batch: 200, loss is 3.5133866930007933 and perplexity is 33.56173883568937
At time: 168.84013295173645 and batch: 250, loss is 3.5278538846969605 and perplexity is 34.05081217373999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 170.44205236434937 and batch: 50, loss is 3.6745645999908447 and perplexity is 39.43148465030454
At time: 171.40446209907532 and batch: 100, loss is 3.553038969039917 and perplexity is 34.919275000878784
At time: 172.36627960205078 and batch: 150, loss is 3.568526101112366 and perplexity is 35.46428384327506
At time: 173.32593965530396 and batch: 200, loss is 3.513386492729187 and perplexity is 33.56173211422669
At time: 174.28701734542847 and batch: 250, loss is 3.52785364151001 and perplexity is 34.05080389302782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 175.87786555290222 and batch: 50, loss is 3.6745646238327025 and perplexity is 39.4314855904244
At time: 176.8551504611969 and batch: 100, loss is 3.553038935661316 and perplexity is 34.91927383532225
At time: 177.81560397148132 and batch: 150, loss is 3.568526101112366 and perplexity is 35.46428384327506
At time: 178.79171228408813 and batch: 200, loss is 3.5133864641189576 and perplexity is 33.561731154017856
At time: 179.7534158229828 and batch: 250, loss is 3.52785364151001 and perplexity is 34.05080389302782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 181.35461449623108 and batch: 50, loss is 3.674564619064331 and perplexity is 39.431485402400426
At time: 182.3149814605713 and batch: 100, loss is 3.553038935661316 and perplexity is 34.91927383532225
At time: 183.27470064163208 and batch: 150, loss is 3.5685261344909667 and perplexity is 35.46428502702326
At time: 184.23725199699402 and batch: 200, loss is 3.5133864784240725 and perplexity is 33.561731634122275
At time: 185.20008254051208 and batch: 250, loss is 3.527853651046753 and perplexity is 34.05080421776158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.654531478881836 and perplexity of 105.0599856441845
Annealing...
Model not improving. Stopping early with 105.0599455669881loss at 33 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 25.624389660383688, 'wordvec_source': '', 'dropout': 0.3363634518100539, 'anneal': 6.133632921919033}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2663013935089111 and batch: 50, loss is 6.268297204971313 and perplexity is 527.5782549541375
At time: 2.2474334239959717 and batch: 100, loss is 5.526751480102539 and perplexity is 251.32614549447393
At time: 3.2150259017944336 and batch: 150, loss is 5.478623723983764 and perplexity is 239.5168391502535
At time: 4.1809000968933105 and batch: 200, loss is 5.447849636077881 and perplexity is 232.25818890403133
At time: 5.145708799362183 and batch: 250, loss is 5.459655179977417 and perplexity is 235.01637204109488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4688560485839846 and perplexity of 237.18870515133446
Finished 1 epochs...
Completing Train Step...
At time: 6.764626979827881 and batch: 50, loss is 5.2207637119293215 and perplexity is 185.07547445890768
At time: 7.7214882373809814 and batch: 100, loss is 5.1537325572967525 and perplexity is 173.07630338646345
At time: 8.679139137268066 and batch: 150, loss is 5.169932098388672 and perplexity is 175.90289298867341
At time: 9.634503364562988 and batch: 200, loss is 5.117342205047607 and perplexity is 166.89121701509862
At time: 10.592225551605225 and batch: 250, loss is 5.134203672409058 and perplexity is 169.72910607443544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4084320068359375 and perplexity of 223.28120964676208
Finished 2 epochs...
Completing Train Step...
At time: 12.20605206489563 and batch: 50, loss is 5.047378587722778 and perplexity is 155.61400089211511
At time: 13.163400173187256 and batch: 100, loss is 4.989456624984741 and perplexity is 146.85660360236753
At time: 14.12504243850708 and batch: 150, loss is 5.053251247406006 and perplexity is 156.53055764003787
At time: 15.085747957229614 and batch: 200, loss is 5.026110258102417 and perplexity is 152.33929819527197
At time: 16.045381784439087 and batch: 250, loss is 5.102532711029053 and perplexity is 164.43785390746157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.424892425537109 and perplexity of 226.98692700509864
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 17.64894437789917 and batch: 50, loss is 4.943642578125 and perplexity is 140.2803020025471
At time: 18.619441986083984 and batch: 100, loss is 4.783226413726807 and perplexity is 119.4892505449564
At time: 19.578441858291626 and batch: 150, loss is 4.794002866744995 and perplexity is 120.78388408941532
At time: 20.551652669906616 and batch: 200, loss is 4.716962070465088 and perplexity is 111.82801049769395
At time: 21.509312868118286 and batch: 250, loss is 4.773181438446045 and perplexity is 118.29499216671746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.155668640136719 and perplexity of 173.4117180379575
Finished 4 epochs...
Completing Train Step...
At time: 23.12939953804016 and batch: 50, loss is 4.776627025604248 and perplexity is 118.70329088306785
At time: 24.08960199356079 and batch: 100, loss is 4.678787794113159 and perplexity is 107.63951220401471
At time: 25.052238702774048 and batch: 150, loss is 4.71305624961853 and perplexity is 111.39208220615691
At time: 26.010966300964355 and batch: 200, loss is 4.676574697494507 and perplexity is 107.40155896739488
At time: 26.972103118896484 and batch: 250, loss is 4.7280224609375 and perplexity is 113.07173733280545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.133652114868164 and perplexity of 169.6355165184563
Finished 5 epochs...
Completing Train Step...
At time: 28.57881259918213 and batch: 50, loss is 4.715526542663574 and perplexity is 111.66759344875153
At time: 29.557518005371094 and batch: 100, loss is 4.626950750350952 and perplexity is 102.20194923640219
At time: 30.526389837265015 and batch: 150, loss is 4.6683183288574215 and perplexity is 106.51846270504643
At time: 31.5101957321167 and batch: 200, loss is 4.639564695358277 and perplexity is 103.4992840570465
At time: 32.469825744628906 and batch: 250, loss is 4.687212543487549 and perplexity is 108.55017879971783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.117856979370117 and perplexity of 166.97715044453426
Finished 6 epochs...
Completing Train Step...
At time: 34.09462642669678 and batch: 50, loss is 4.669015302658081 and perplexity is 106.5927291607189
At time: 35.068310022354126 and batch: 100, loss is 4.592742052078247 and perplexity is 98.76487763994773
At time: 36.03021454811096 and batch: 150, loss is 4.6267698287963865 and perplexity is 102.18346037343406
At time: 36.990601539611816 and batch: 200, loss is 4.60495665550232 and perplexity is 99.97864923102398
At time: 37.94915461540222 and batch: 250, loss is 4.654261465072632 and perplexity is 105.03162182674932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.115577697753906 and perplexity of 166.59699589941093
Finished 7 epochs...
Completing Train Step...
At time: 39.56790542602539 and batch: 50, loss is 4.6358093166351315 and perplexity is 103.11133395355408
At time: 40.52989482879639 and batch: 100, loss is 4.56128758430481 and perplexity is 95.70663088516335
At time: 41.48821687698364 and batch: 150, loss is 4.5999496555328365 and perplexity is 99.47930728314432
At time: 42.446709871292114 and batch: 200, loss is 4.5804813194274905 and perplexity is 97.5613410767597
At time: 43.40748333930969 and batch: 250, loss is 4.6232747554779055 and perplexity is 101.82694507396697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.114971923828125 and perplexity of 166.49610634439767
Finished 8 epochs...
Completing Train Step...
At time: 45.0586302280426 and batch: 50, loss is 4.605458631515503 and perplexity is 100.02884871318224
At time: 46.03681659698486 and batch: 100, loss is 4.539682674407959 and perplexity is 93.66107434748078
At time: 46.99771332740784 and batch: 150, loss is 4.5729458713531494 and perplexity is 96.82893562431775
At time: 47.961241722106934 and batch: 200, loss is 4.5530770397186275 and perplexity is 94.92404445186176
At time: 48.921403646469116 and batch: 250, loss is 4.602551612854004 and perplexity is 99.738485233794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.113731765747071 and perplexity of 166.2897528345845
Finished 9 epochs...
Completing Train Step...
At time: 50.54176163673401 and batch: 50, loss is 4.578492221832275 and perplexity is 97.36747492110936
At time: 51.50080943107605 and batch: 100, loss is 4.512167015075684 and perplexity is 91.11906109859454
At time: 52.46124720573425 and batch: 150, loss is 4.551115770339965 and perplexity is 94.73805527723452
At time: 53.426714181900024 and batch: 200, loss is 4.525010347366333 and perplexity is 92.29688085335444
At time: 54.38771152496338 and batch: 250, loss is 4.5649512672424315 and perplexity is 96.05791273548981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.108986663818359 and perplexity of 165.50256013950684
Finished 10 epochs...
Completing Train Step...
At time: 56.00788402557373 and batch: 50, loss is 4.551181268692017 and perplexity is 94.74426066695091
At time: 56.96612906455994 and batch: 100, loss is 4.496397294998169 and perplexity is 89.69340961834168
At time: 57.92707562446594 and batch: 150, loss is 4.531217498779297 and perplexity is 92.87156329306686
At time: 58.890668630599976 and batch: 200, loss is 4.510821256637573 and perplexity is 90.9965193275539
At time: 59.85051131248474 and batch: 250, loss is 4.555749101638794 and perplexity is 95.1780265531829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.11450309753418 and perplexity of 166.41806688680822
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 61.4553165435791 and batch: 50, loss is 4.507713956832886 and perplexity is 90.71420470605719
At time: 62.431724548339844 and batch: 100, loss is 4.397383127212525 and perplexity is 81.23800074884805
At time: 63.392380475997925 and batch: 150, loss is 4.398386354446411 and perplexity is 81.31954181889787
At time: 64.3536307811737 and batch: 200, loss is 4.357679653167724 and perplexity is 78.07576123485032
At time: 65.3160719871521 and batch: 250, loss is 4.426495475769043 and perplexity is 83.63779203423209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.045211410522461 and perplexity of 155.27712294628327
Finished 12 epochs...
Completing Train Step...
At time: 66.9309434890747 and batch: 50, loss is 4.454394798278809 and perplexity is 86.00408527120447
At time: 67.89144825935364 and batch: 100, loss is 4.357587375640869 and perplexity is 78.06855692909909
At time: 68.85201716423035 and batch: 150, loss is 4.376739091873169 and perplexity is 79.57811293183792
At time: 69.81289267539978 and batch: 200, loss is 4.3615227317810055 and perplexity is 78.37638982271099
At time: 70.78684663772583 and batch: 250, loss is 4.428478860855103 and perplexity is 83.8038426002575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.038241577148438 and perplexity of 154.19863009624706
Finished 13 epochs...
Completing Train Step...
At time: 72.38975882530212 and batch: 50, loss is 4.435104417800903 and perplexity is 84.36093321153785
At time: 73.36657929420471 and batch: 100, loss is 4.344135408401489 and perplexity is 77.02541315695036
At time: 74.32960367202759 and batch: 150, loss is 4.37418571472168 and perplexity is 79.37517918996518
At time: 75.28951096534729 and batch: 200, loss is 4.363364944458008 and perplexity is 78.5209088781663
At time: 76.25303840637207 and batch: 250, loss is 4.425134639739991 and perplexity is 83.52405212167967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.036077499389648 and perplexity of 153.8652930841183
Finished 14 epochs...
Completing Train Step...
At time: 77.86761474609375 and batch: 50, loss is 4.424809989929199 and perplexity is 83.49694045509864
At time: 78.8421540260315 and batch: 100, loss is 4.336642580032349 and perplexity is 76.45043176461809
At time: 79.80230474472046 and batch: 150, loss is 4.3706025218963624 and perplexity is 79.09127156902245
At time: 80.76301717758179 and batch: 200, loss is 4.362093162536621 and perplexity is 78.42111087990057
At time: 81.72383069992065 and batch: 250, loss is 4.421462306976318 and perplexity is 83.21788652357678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.035663223266601 and perplexity of 153.8015635687476
Finished 15 epochs...
Completing Train Step...
At time: 83.34534931182861 and batch: 50, loss is 4.417099351882935 and perplexity is 82.85560151301
At time: 84.307448387146 and batch: 100, loss is 4.330371141433716 and perplexity is 75.97247787174376
At time: 85.26597023010254 and batch: 150, loss is 4.366666316986084 and perplexity is 78.78056402296411
At time: 86.22643804550171 and batch: 200, loss is 4.360178890228272 and perplexity is 78.27113511198098
At time: 87.18832063674927 and batch: 250, loss is 4.417927856445313 and perplexity is 82.9242762016002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.035429763793945 and perplexity of 153.76566132784535
Finished 16 epochs...
Completing Train Step...
At time: 88.79306292533875 and batch: 50, loss is 4.410668487548828 and perplexity is 82.32447800240782
At time: 89.76600790023804 and batch: 100, loss is 4.324775619506836 and perplexity is 75.54855933477621
At time: 90.72640299797058 and batch: 150, loss is 4.362624101638794 and perplexity is 78.46275876937067
At time: 91.68935942649841 and batch: 200, loss is 4.357749099731445 and perplexity is 78.08118351645521
At time: 92.64850759506226 and batch: 250, loss is 4.41449197769165 and perplexity is 82.63984735416625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.035697555541992 and perplexity of 153.80684401702794
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 94.27507281303406 and batch: 50, loss is 4.400457077026367 and perplexity is 81.48810649544261
At time: 95.23502278327942 and batch: 100, loss is 4.304747896194458 and perplexity is 74.05054467534532
At time: 96.22431230545044 and batch: 150, loss is 4.334152154922485 and perplexity is 76.26027457409562
At time: 97.18614912033081 and batch: 200, loss is 4.326020593643189 and perplexity is 75.64267391013665
At time: 98.14611315727234 and batch: 250, loss is 4.3909716796875 and perplexity is 80.71881371871106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.025476455688477 and perplexity of 152.2427757716415
Finished 18 epochs...
Completing Train Step...
At time: 99.76342964172363 and batch: 50, loss is 4.3910150051116945 and perplexity is 80.72231097131537
At time: 100.72155380249023 and batch: 100, loss is 4.2987471866607665 and perplexity is 73.60751942823822
At time: 101.68152713775635 and batch: 150, loss is 4.332293071746826 and perplexity is 76.11863208404418
At time: 102.64222764968872 and batch: 200, loss is 4.327576694488525 and perplexity is 75.76047316894442
At time: 103.6073100566864 and batch: 250, loss is 4.390295801162719 and perplexity is 80.6642760384724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.023796081542969 and perplexity of 151.98716576776167
Finished 19 epochs...
Completing Train Step...
At time: 105.2117555141449 and batch: 50, loss is 4.387151012420654 and perplexity is 80.41100238609835
At time: 106.18821597099304 and batch: 100, loss is 4.2965367412567135 and perplexity is 73.44499371849437
At time: 107.14487886428833 and batch: 150, loss is 4.333084011077881 and perplexity is 76.17886111960223
At time: 108.10589814186096 and batch: 200, loss is 4.328780288696289 and perplexity is 75.851712932439
At time: 109.0677559375763 and batch: 250, loss is 4.388919153213501 and perplexity is 80.55330612906343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.023062515258789 and perplexity of 151.87571399095714
Finished 20 epochs...
Completing Train Step...
At time: 110.68331694602966 and batch: 50, loss is 4.384545135498047 and perplexity is 80.20173399291629
At time: 111.64154887199402 and batch: 100, loss is 4.2954713726043705 and perplexity is 73.36678939013203
At time: 112.60028958320618 and batch: 150, loss is 4.333518619537354 and perplexity is 76.21197629262372
At time: 113.59051942825317 and batch: 200, loss is 4.329276866912842 and perplexity is 75.88938859445179
At time: 114.55730056762695 and batch: 250, loss is 4.38750693321228 and perplexity is 80.43962742754456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022805023193359 and perplexity of 151.83661223408512
Finished 21 epochs...
Completing Train Step...
At time: 116.15608477592468 and batch: 50, loss is 4.382468309402466 and perplexity is 80.03534178249829
At time: 117.12866020202637 and batch: 100, loss is 4.29466817855835 and perplexity is 73.30788528059995
At time: 118.08707451820374 and batch: 150, loss is 4.333951950073242 and perplexity is 76.24500842555076
At time: 119.04898047447205 and batch: 200, loss is 4.329695558547973 and perplexity is 75.92116949938676
At time: 120.00706315040588 and batch: 250, loss is 4.38611927986145 and perplexity is 80.32808251973685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022768783569336 and perplexity of 151.83110983204784
Finished 22 epochs...
Completing Train Step...
At time: 121.60509967803955 and batch: 50, loss is 4.380671844482422 and perplexity is 79.89169016979754
At time: 122.57814693450928 and batch: 100, loss is 4.293953857421875 and perplexity is 73.25553860706724
At time: 123.54085373878479 and batch: 150, loss is 4.3342297077178955 and perplexity is 76.26618900090482
At time: 124.50119042396545 and batch: 200, loss is 4.329897928237915 and perplexity is 75.93653519764172
At time: 125.46408557891846 and batch: 250, loss is 4.3848293495178225 and perplexity is 80.22453168968646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022711563110351 and perplexity of 151.82242223481168
Finished 23 epochs...
Completing Train Step...
At time: 127.0820255279541 and batch: 50, loss is 4.379124307632447 and perplexity is 79.76815045104169
At time: 128.04246854782104 and batch: 100, loss is 4.2933616447448735 and perplexity is 73.21216859184752
At time: 129.00127840042114 and batch: 150, loss is 4.334492139816284 and perplexity is 76.2862063233995
At time: 129.95958137512207 and batch: 200, loss is 4.329989242553711 and perplexity is 75.94346960699781
At time: 130.92172074317932 and batch: 250, loss is 4.383503551483154 and perplexity is 80.11824063904247
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022664260864258 and perplexity of 151.81524086308147
Finished 24 epochs...
Completing Train Step...
At time: 132.52568531036377 and batch: 50, loss is 4.377661409378052 and perplexity is 79.65154307614884
At time: 133.4973177909851 and batch: 100, loss is 4.292764949798584 and perplexity is 73.16849629165803
At time: 134.45748138427734 and batch: 150, loss is 4.334644041061401 and perplexity is 76.29779517328338
At time: 135.41882181167603 and batch: 200, loss is 4.329946413040161 and perplexity is 75.94021705479035
At time: 136.3815152645111 and batch: 250, loss is 4.382274169921875 and perplexity is 80.01980527098964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022615814208985 and perplexity of 151.80788610060043
Finished 25 epochs...
Completing Train Step...
At time: 138.0101833343506 and batch: 50, loss is 4.376375589370728 and perplexity is 79.54919134550059
At time: 138.96837663650513 and batch: 100, loss is 4.292150268554687 and perplexity is 73.12353480924907
At time: 139.92760944366455 and batch: 150, loss is 4.3347697353363035 and perplexity is 76.30738597206594
At time: 140.88865327835083 and batch: 200, loss is 4.329948844909668 and perplexity is 75.94040173171317
At time: 141.84939908981323 and batch: 250, loss is 4.381054830551148 and perplexity is 79.92229343407737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0225471496582035 and perplexity of 151.79746263816153
Finished 26 epochs...
Completing Train Step...
At time: 143.46812987327576 and batch: 50, loss is 4.375061445236206 and perplexity is 79.44472090193308
At time: 144.42748713493347 and batch: 100, loss is 4.291548318862915 and perplexity is 73.07953136526783
At time: 145.40046167373657 and batch: 150, loss is 4.334873104095459 and perplexity is 76.31527417955839
At time: 146.35951113700867 and batch: 200, loss is 4.329865465164184 and perplexity is 75.93407010431322
At time: 147.32037568092346 and batch: 250, loss is 4.37988540649414 and perplexity is 79.82888500911909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.022576141357422 and perplexity of 151.80186356833536
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 148.9223563671112 and batch: 50, loss is 4.372764444351196 and perplexity is 79.26244573097003
At time: 149.89980912208557 and batch: 100, loss is 4.287822332382202 and perplexity is 72.8077446705929
At time: 150.8620240688324 and batch: 150, loss is 4.330374794006348 and perplexity is 75.97275536724399
At time: 151.82200980186462 and batch: 200, loss is 4.323878240585327 and perplexity is 75.48079406019092
At time: 152.7798523902893 and batch: 250, loss is 4.375324039459229 and perplexity is 79.46558536601557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.021139526367188 and perplexity of 151.5839393096965
Finished 28 epochs...
Completing Train Step...
At time: 154.39523434638977 and batch: 50, loss is 4.371699714660645 and perplexity is 79.17809756362207
At time: 155.35598254203796 and batch: 100, loss is 4.2873148918151855 and perplexity is 72.77080843961764
At time: 156.31728434562683 and batch: 150, loss is 4.330188798904419 and perplexity is 75.95862612089168
At time: 157.2756679058075 and batch: 200, loss is 4.323402404785156 and perplexity is 75.44488613996623
At time: 158.23653864860535 and batch: 250, loss is 4.375352296829224 and perplexity is 79.46783088618923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020820617675781 and perplexity of 151.53560558140563
Finished 29 epochs...
Completing Train Step...
At time: 159.84770679473877 and batch: 50, loss is 4.371317386627197 and perplexity is 79.14783134346935
At time: 160.82057571411133 and batch: 100, loss is 4.287227220535279 and perplexity is 72.76442880936114
At time: 161.78221893310547 and batch: 150, loss is 4.3304721546173095 and perplexity is 75.98015248121092
At time: 162.74038577079773 and batch: 200, loss is 4.323054819107056 and perplexity is 75.41866713499735
At time: 163.70084142684937 and batch: 250, loss is 4.375148305892944 and perplexity is 79.45162182226991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020819091796875 and perplexity of 151.535374356598
Finished 30 epochs...
Completing Train Step...
At time: 165.30184388160706 and batch: 50, loss is 4.37100588798523 and perplexity is 79.12318074100509
At time: 166.27798700332642 and batch: 100, loss is 4.287163896560669 and perplexity is 72.75982122240562
At time: 167.23684740066528 and batch: 150, loss is 4.330730333328247 and perplexity is 75.99977147152917
At time: 168.19628930091858 and batch: 200, loss is 4.322684278488159 and perplexity is 75.39072663226607
At time: 169.15337085723877 and batch: 250, loss is 4.374955558776856 and perplexity is 79.43630922707182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020823669433594 and perplexity of 151.5360680320795
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 170.7705855369568 and batch: 50, loss is 4.370691118240356 and perplexity is 79.09827907694049
At time: 171.73361897468567 and batch: 100, loss is 4.287067346572876 and perplexity is 72.75279660167374
At time: 172.6940836906433 and batch: 150, loss is 4.329834756851196 and perplexity is 75.93173833292454
At time: 173.65343308448792 and batch: 200, loss is 4.321409912109375 and perplexity is 75.29471241661193
At time: 174.61121153831482 and batch: 250, loss is 4.374181709289551 and perplexity is 79.37486125870892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020521545410157 and perplexity of 151.49029226083806
Finished 32 epochs...
Completing Train Step...
At time: 176.2193582057953 and batch: 50, loss is 4.370475873947144 and perplexity is 79.08125545595063
At time: 177.19392466545105 and batch: 100, loss is 4.2867997360229495 and perplexity is 72.73332979064467
At time: 178.1565306186676 and batch: 150, loss is 4.329666986465454 and perplexity is 75.9190003044562
At time: 179.1158230304718 and batch: 200, loss is 4.321432228088379 and perplexity is 75.29639271058198
At time: 180.07504987716675 and batch: 250, loss is 4.374213151931762 and perplexity is 79.37735705330907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020509338378906 and perplexity of 151.4884430253931
Finished 33 epochs...
Completing Train Step...
At time: 181.68727374076843 and batch: 50, loss is 4.370442953109741 and perplexity is 79.07865207765109
At time: 182.6475546360016 and batch: 100, loss is 4.286902923583984 and perplexity is 72.74083535278535
At time: 183.60733652114868 and batch: 150, loss is 4.329511013031006 and perplexity is 75.90715988065864
At time: 184.57142162322998 and batch: 200, loss is 4.321368579864502 and perplexity is 75.29160038143483
At time: 185.5281777381897 and batch: 250, loss is 4.374168977737427 and perplexity is 79.3738506999585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0204509735107425 and perplexity of 151.47960168040214
Finished 34 epochs...
Completing Train Step...
At time: 187.14103317260742 and batch: 50, loss is 4.370357074737549 and perplexity is 79.07186122333343
At time: 188.09998202323914 and batch: 100, loss is 4.286954622268677 and perplexity is 72.74459605550743
At time: 189.0561032295227 and batch: 150, loss is 4.329373960494995 and perplexity is 75.89675732476024
At time: 190.0158088207245 and batch: 200, loss is 4.32132040977478 and perplexity is 75.2879736656395
At time: 190.97299146652222 and batch: 250, loss is 4.3741591262817385 and perplexity is 79.37306875583715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020442199707031 and perplexity of 151.47827263394117
Finished 35 epochs...
Completing Train Step...
At time: 192.57441997528076 and batch: 50, loss is 4.3702965831756595 and perplexity is 79.06707818761464
At time: 193.5526831150055 and batch: 100, loss is 4.287011795043945 and perplexity is 72.74875518484302
At time: 194.5184941291809 and batch: 150, loss is 4.329240217208862 and perplexity is 75.8866073217913
At time: 195.49794483184814 and batch: 200, loss is 4.321264152526855 and perplexity is 75.28373829057561
At time: 196.4625587463379 and batch: 250, loss is 4.374127492904663 and perplexity is 79.37055795733632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020436096191406 and perplexity of 151.47734808675875
Finished 36 epochs...
Completing Train Step...
At time: 198.07505130767822 and batch: 50, loss is 4.370236024856568 and perplexity is 79.06229016324292
At time: 199.03358340263367 and batch: 100, loss is 4.28708044052124 and perplexity is 72.75374922927264
At time: 199.98952174186707 and batch: 150, loss is 4.329115753173828 and perplexity is 75.87716275620555
At time: 200.95172262191772 and batch: 200, loss is 4.321205081939698 and perplexity is 75.27929136729384
At time: 201.91102290153503 and batch: 250, loss is 4.374094543457031 and perplexity is 79.36794278437783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020433807373047 and perplexity of 151.47700138302025
Finished 37 epochs...
Completing Train Step...
At time: 203.51025652885437 and batch: 50, loss is 4.370175914764404 and perplexity is 79.05753786452655
At time: 204.4841411113739 and batch: 100, loss is 4.2871364498138425 and perplexity is 72.757824229419
At time: 205.44535517692566 and batch: 150, loss is 4.328995952606201 and perplexity is 75.86807317351689
At time: 206.40779781341553 and batch: 200, loss is 4.321148138046265 and perplexity is 75.2750047933968
At time: 207.36796951293945 and batch: 250, loss is 4.374061098098755 and perplexity is 79.36528833948547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020435333251953 and perplexity of 151.47723251875775
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 208.96473217010498 and batch: 50, loss is 4.3700822162628175 and perplexity is 79.05013063871789
At time: 209.93762707710266 and batch: 100, loss is 4.287065868377685 and perplexity is 72.7526890589192
At time: 210.89670658111572 and batch: 150, loss is 4.328748807907105 and perplexity is 75.84932509824056
At time: 211.85270643234253 and batch: 200, loss is 4.32094178199768 and perplexity is 75.25947294345114
At time: 212.8087944984436 and batch: 250, loss is 4.37396918296814 and perplexity is 79.35799380388566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.020434951782226 and perplexity of 151.47717473479025
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 214.42190217971802 and batch: 50, loss is 4.370062913894653 and perplexity is 79.04860479871908
At time: 215.37908959388733 and batch: 100, loss is 4.2870458889007566 and perplexity is 72.75123551276724
At time: 216.33631253242493 and batch: 150, loss is 4.3287085437774655 and perplexity is 75.84627115266439
At time: 217.29307103157043 and batch: 200, loss is 4.320913505554199 and perplexity is 75.2573449033049
At time: 218.2508668899536 and batch: 250, loss is 4.3739558029174805 and perplexity is 79.35693199701187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.02043571472168 and perplexity of 151.4772903027473
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 219.85566544532776 and batch: 50, loss is 4.370060997009277 and perplexity is 79.04845327174979
At time: 220.83032059669495 and batch: 100, loss is 4.287043142318725 and perplexity is 72.75103569580544
At time: 221.78994154930115 and batch: 150, loss is 4.328703260421753 and perplexity is 75.84587043089296
At time: 222.74844765663147 and batch: 200, loss is 4.320909671783447 and perplexity is 75.25705638445015
At time: 223.70673871040344 and batch: 250, loss is 4.373954076766967 and perplexity is 79.35679501512121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.02043571472168 and perplexity of 151.4772903027473
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 225.33034205436707 and batch: 50, loss is 4.370060729980469 and perplexity is 79.0484321635383
At time: 226.28976583480835 and batch: 100, loss is 4.2870427513122555 and perplexity is 72.75100724968536
At time: 227.2473545074463 and batch: 150, loss is 4.328702869415284 and perplexity is 75.84584077467278
At time: 228.207279920578 and batch: 200, loss is 4.320909280776977 and perplexity is 75.25702695845996
At time: 229.166259765625 and batch: 250, loss is 4.373953943252563 and perplexity is 79.35678441984673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.02043571472168 and perplexity of 151.4772903027473
Annealing...
Model not improving. Stopping early with 151.47700138302025loss at 41 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 14.695813735610955, 'wordvec_source': '', 'dropout': 0.995196728618948, 'anneal': 4.186986656231221}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2731373310089111 and batch: 50, loss is 10.93192491531372 and perplexity is 55933.844061062824
At time: 2.2415223121643066 and batch: 100, loss is 9.996134700775146 and perplexity is 21941.491245470716
At time: 3.209148406982422 and batch: 150, loss is 9.110734519958497 and perplexity is 9051.941281722728
At time: 4.175617933273315 and batch: 200, loss is 8.595953121185303 and perplexity is 5409.722741060151
At time: 5.1425745487213135 and batch: 250, loss is 8.302170467376708 and perplexity is 4032.6155625299925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 7.491522979736328 and perplexity of 1792.7803819444407
Finished 1 epochs...
Completing Train Step...
At time: 6.769017457962036 and batch: 50, loss is 5.708105812072754 and perplexity is 301.2998089567275
At time: 7.729607582092285 and batch: 100, loss is 5.094127407073975 and perplexity is 163.0614962192061
At time: 8.701809167861938 and batch: 150, loss is 5.019856986999511 and perplexity is 151.38965155750182
At time: 9.662621259689331 and batch: 200, loss is 4.9500155258178715 and perplexity is 141.17715579526998
At time: 10.621686697006226 and batch: 250, loss is 4.954187335968018 and perplexity is 141.76735032056254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.152390670776367 and perplexity of 172.84421038401348
Finished 2 epochs...
Completing Train Step...
At time: 12.238437414169312 and batch: 50, loss is 4.8455570030212405 and perplexity is 127.17409862652529
At time: 13.196511030197144 and batch: 100, loss is 4.720230216979981 and perplexity is 112.19407867686692
At time: 14.156755447387695 and batch: 150, loss is 4.755266847610474 and perplexity is 116.19465529390811
At time: 15.115719556808472 and batch: 200, loss is 4.730286254882812 and perplexity is 113.32799839885051
At time: 16.07587504386902 and batch: 250, loss is 4.7422841453552245 and perplexity is 114.69588477774748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.076021957397461 and perplexity of 160.13576031006477
Finished 3 epochs...
Completing Train Step...
At time: 17.69016456604004 and batch: 50, loss is 4.673093271255493 and perplexity is 107.02829847832837
At time: 18.648772716522217 and batch: 100, loss is 4.575932674407959 and perplexity is 97.1185769204645
At time: 19.60753870010376 and batch: 150, loss is 4.6187011337280275 and perplexity is 101.36229053028727
At time: 20.584725618362427 and batch: 200, loss is 4.604667167663575 and perplexity is 99.94971081679904
At time: 21.54999828338623 and batch: 250, loss is 4.627450351715088 and perplexity is 102.2530222266635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.055168533325196 and perplexity of 156.83095936009528
Finished 4 epochs...
Completing Train Step...
At time: 23.16959238052368 and batch: 50, loss is 4.524591684341431 and perplexity is 92.25824764973856
At time: 24.13617515563965 and batch: 100, loss is 4.464286270141602 and perplexity is 86.85901353961529
At time: 25.09610629081726 and batch: 150, loss is 4.507114086151123 and perplexity is 90.65980423249053
At time: 26.061125993728638 and batch: 200, loss is 4.498697671890259 and perplexity is 89.89997576410359
At time: 27.028539419174194 and batch: 250, loss is 4.554630432128906 and perplexity is 95.0716133285788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0662483215332035 and perplexity of 158.5782752439733
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 28.648385286331177 and batch: 50, loss is 4.398788967132568 and perplexity is 81.35228868977713
At time: 29.611976385116577 and batch: 100, loss is 4.218806056976319 and perplexity is 67.95230465704691
At time: 30.574334859848022 and batch: 150, loss is 4.197508182525635 and perplexity is 66.52036773694084
At time: 31.536367893218994 and batch: 200, loss is 4.115193939208984 and perplexity is 61.26409462073098
At time: 32.497315645217896 and batch: 250, loss is 4.14775897026062 and perplexity is 63.292001987614604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.874119567871094 and perplexity of 130.8588901063913
Finished 6 epochs...
Completing Train Step...
At time: 34.120336294174194 and batch: 50, loss is 4.223925838470459 and perplexity is 68.30109771916189
At time: 35.083635330200195 and batch: 100, loss is 4.095225763320923 and perplexity is 60.05289536823481
At time: 36.044803619384766 and batch: 150, loss is 4.103940191268921 and perplexity is 60.57850888222981
At time: 37.007365465164185 and batch: 200, loss is 4.051598563194275 and perplexity is 57.48928388367136
At time: 37.969810485839844 and batch: 250, loss is 4.103294696807861 and perplexity is 60.53941840798665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.863795471191406 and perplexity of 129.51484026196687
Finished 7 epochs...
Completing Train Step...
At time: 39.594778537750244 and batch: 50, loss is 4.1400658130645756 and perplexity is 62.806954831359015
At time: 40.558295011520386 and batch: 100, loss is 4.026108961105347 and perplexity is 56.04242320162055
At time: 41.525718212127686 and batch: 150, loss is 4.045213189125061 and perplexity is 57.12336281585932
At time: 42.50530648231506 and batch: 200, loss is 4.005678901672363 and perplexity is 54.90908962069955
At time: 43.495296001434326 and batch: 250, loss is 4.060764141082764 and perplexity is 58.01862856204796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.864177322387695 and perplexity of 129.56430510216117
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 45.1107394695282 and batch: 50, loss is 4.065161323547363 and perplexity is 58.27430878250287
At time: 46.08955979347229 and batch: 100, loss is 3.9224668884277345 and perplexity is 50.52493053514772
At time: 47.05382204055786 and batch: 150, loss is 3.9119518327713014 and perplexity is 49.99644149379344
At time: 48.017189264297485 and batch: 200, loss is 3.8475821447372436 and perplexity is 46.8795780569815
At time: 48.97995185852051 and batch: 250, loss is 3.9110782766342163 and perplexity is 49.952785866091006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.820670700073242 and perplexity of 124.04826206366465
Finished 9 epochs...
Completing Train Step...
At time: 50.605470180511475 and batch: 50, loss is 4.014988169670105 and perplexity is 55.42263973050382
At time: 51.584028005599976 and batch: 100, loss is 3.8857151746749876 and perplexity is 48.70176027317178
At time: 52.54653787612915 and batch: 150, loss is 3.8864213323593138 and perplexity is 48.73616354106526
At time: 53.50812363624573 and batch: 200, loss is 3.8368354749679567 and perplexity is 46.37847612389472
At time: 54.470925092697144 and batch: 250, loss is 3.9063835287094117 and perplexity is 49.71881976397457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.817764663696289 and perplexity of 123.68829659131555
Finished 10 epochs...
Completing Train Step...
At time: 56.09544849395752 and batch: 50, loss is 3.9886399126052856 and perplexity is 53.98141996340875
At time: 57.0579309463501 and batch: 100, loss is 3.864228458404541 and perplexity is 47.6664815684335
At time: 58.02411937713623 and batch: 150, loss is 3.8705934572219847 and perplexity is 47.97084628019823
At time: 58.985517263412476 and batch: 200, loss is 3.827563991546631 and perplexity is 45.95046606149333
At time: 59.948883056640625 and batch: 250, loss is 3.898553614616394 and perplexity is 49.33104577604381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.817039108276367 and perplexity of 123.59858642612653
Finished 11 epochs...
Completing Train Step...
At time: 61.56044578552246 and batch: 50, loss is 3.968321657180786 and perplexity is 52.895679215411455
At time: 62.53763771057129 and batch: 100, loss is 3.847101845741272 and perplexity is 46.85706724910112
At time: 63.498788356781006 and batch: 150, loss is 3.8570859956741335 and perplexity is 47.32723845817672
At time: 64.48765754699707 and batch: 200, loss is 3.8186064529418946 and perplexity is 45.54070097102372
At time: 65.45151138305664 and batch: 250, loss is 3.8900008392333985 and perplexity is 48.91092757139863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.817303848266602 and perplexity of 123.63131224641556
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 67.07315969467163 and batch: 50, loss is 3.9480339527130126 and perplexity is 51.833359755995104
At time: 68.03656244277954 and batch: 100, loss is 3.8192248058319094 and perplexity is 45.56886990335525
At time: 69.00213432312012 and batch: 150, loss is 3.8191317415237425 and perplexity is 45.564629265332826
At time: 69.96458315849304 and batch: 200, loss is 3.770733742713928 and perplexity is 43.411906322655554
At time: 70.9307587146759 and batch: 250, loss is 3.8466455030441282 and perplexity is 46.83568924687101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.809814071655273 and perplexity of 122.70880035201915
Finished 13 epochs...
Completing Train Step...
At time: 72.55142760276794 and batch: 50, loss is 3.9358779048919676 and perplexity is 51.207085179928036
At time: 73.51137185096741 and batch: 100, loss is 3.8107450437545776 and perplexity is 45.184090453208185
At time: 74.47362780570984 and batch: 150, loss is 3.813768186569214 and perplexity is 45.320895097393624
At time: 75.43558859825134 and batch: 200, loss is 3.769470443725586 and perplexity is 43.35709873179226
At time: 76.39875602722168 and batch: 250, loss is 3.846311025619507 and perplexity is 46.82002638573411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.808997344970703 and perplexity of 122.6086217151917
Finished 14 epochs...
Completing Train Step...
At time: 78.01739764213562 and batch: 50, loss is 3.9287768173217774 and perplexity is 50.844747203288755
At time: 78.9984142780304 and batch: 100, loss is 3.805231351852417 and perplexity is 44.93564485522244
At time: 79.96244668960571 and batch: 150, loss is 3.8101736879348755 and perplexity is 45.158281633881465
At time: 80.92391228675842 and batch: 200, loss is 3.768256254196167 and perplexity is 43.30448694329109
At time: 81.88794922828674 and batch: 250, loss is 3.8450570201873777 and perplexity is 46.76135061587015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.808844375610351 and perplexity of 122.58986778717905
Finished 15 epochs...
Completing Train Step...
At time: 83.50801301002502 and batch: 50, loss is 3.9227811002731325 and perplexity is 50.54080856121107
At time: 84.47124314308167 and batch: 100, loss is 3.8005398750305175 and perplexity is 44.72532406223225
At time: 85.43885970115662 and batch: 150, loss is 3.806886258125305 and perplexity is 45.01007070268461
At time: 86.40503692626953 and batch: 200, loss is 3.7666839265823366 and perplexity is 43.236451603605545
At time: 87.36810111999512 and batch: 250, loss is 3.8433430910110475 and perplexity is 46.6812736154812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.808927154541015 and perplexity of 122.60001606537071
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 88.97084856033325 and batch: 50, loss is 3.917070198059082 and perplexity is 50.252997558177775
At time: 89.95010304450989 and batch: 100, loss is 3.792740478515625 and perplexity is 44.3778503297404
At time: 90.91482901573181 and batch: 150, loss is 3.796643695831299 and perplexity is 44.55140521445156
At time: 91.87930226325989 and batch: 200, loss is 3.7537643098831177 and perplexity is 42.68144617012791
At time: 92.84359073638916 and batch: 250, loss is 3.831728849411011 and perplexity is 46.14224230462553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.808028030395508 and perplexity of 122.48983297224933
Finished 17 epochs...
Completing Train Step...
At time: 94.45241689682007 and batch: 50, loss is 3.914545578956604 and perplexity is 50.12628789470485
At time: 95.42909574508667 and batch: 100, loss is 3.791032590866089 and perplexity is 44.30212263285576
At time: 96.39134669303894 and batch: 150, loss is 3.7957890176773073 and perplexity is 44.51334436888864
At time: 97.3535966873169 and batch: 200, loss is 3.7535435819625853 and perplexity is 42.67202622293039
At time: 98.3148684501648 and batch: 250, loss is 3.831573305130005 and perplexity is 46.13506570087656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807843399047852 and perplexity of 122.46721959694673
Finished 18 epochs...
Completing Train Step...
At time: 99.93334698677063 and batch: 50, loss is 3.9127010774612425 and perplexity is 50.033915098795354
At time: 100.89634823799133 and batch: 100, loss is 3.7896955585479737 and perplexity is 44.2429288439571
At time: 101.86137771606445 and batch: 150, loss is 3.7950991678237913 and perplexity is 44.48264743415159
At time: 102.82605123519897 and batch: 200, loss is 3.753337206840515 and perplexity is 42.66322068696268
At time: 103.79066109657288 and batch: 250, loss is 3.8312399578094483 and perplexity is 46.11968926333098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807792282104492 and perplexity of 122.46095960701629
Finished 19 epochs...
Completing Train Step...
At time: 105.40222334861755 and batch: 50, loss is 3.9110426568984984 and perplexity is 49.95100659274889
At time: 106.37998652458191 and batch: 100, loss is 3.788479461669922 and perplexity is 44.18915785830478
At time: 107.34239983558655 and batch: 150, loss is 3.7944598627090453 and perplexity is 44.45421853846697
At time: 108.30658340454102 and batch: 200, loss is 3.7531024169921876 and perplexity is 42.653204971688574
At time: 109.26825785636902 and batch: 250, loss is 3.8308342361450194 and perplexity is 46.10098130160943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8077949523925785 and perplexity of 122.46128661349441
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 110.9090633392334 and batch: 50, loss is 3.909333095550537 and perplexity is 49.865685234414364
At time: 111.87187337875366 and batch: 100, loss is 3.786415309906006 and perplexity is 44.09803880429638
At time: 112.8366014957428 and batch: 150, loss is 3.7919045066833497 and perplexity is 44.340767199273685
At time: 113.79766845703125 and batch: 200, loss is 3.7497202253341673 and perplexity is 42.50918734254487
At time: 114.77480554580688 and batch: 250, loss is 3.8277317905426025 and perplexity is 45.95817715050127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8077129364013675 and perplexity of 122.45124324155202
Finished 21 epochs...
Completing Train Step...
At time: 116.399747133255 and batch: 50, loss is 3.908836326599121 and perplexity is 49.840919662141815
At time: 117.3622829914093 and batch: 100, loss is 3.7861107444763182 and perplexity is 44.0846101112212
At time: 118.32463002204895 and batch: 150, loss is 3.7916878509521483 and perplexity is 44.331161558530226
At time: 119.28713536262512 and batch: 200, loss is 3.749647035598755 and perplexity is 42.50607622022318
At time: 120.2511339187622 and batch: 250, loss is 3.8276832580566404 and perplexity is 45.955946740037994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807697677612305 and perplexity of 122.44937479811608
Finished 22 epochs...
Completing Train Step...
At time: 121.85497093200684 and batch: 50, loss is 3.908413276672363 and perplexity is 49.819838924145934
At time: 122.83645749092102 and batch: 100, loss is 3.7858441162109373 and perplexity is 44.072857474959385
At time: 123.79694747924805 and batch: 150, loss is 3.791495108604431 and perplexity is 44.32261788976424
At time: 124.7769558429718 and batch: 200, loss is 3.7495782804489135 and perplexity is 42.50315380905005
At time: 125.75676226615906 and batch: 250, loss is 3.8276088190078736 and perplexity is 45.952525950399234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.80769157409668 and perplexity of 122.44862742872448
Finished 23 epochs...
Completing Train Step...
At time: 127.37671732902527 and batch: 50, loss is 3.90801278591156 and perplexity is 49.79989053379176
At time: 128.33864855766296 and batch: 100, loss is 3.785587148666382 and perplexity is 44.06153363598437
At time: 129.30490827560425 and batch: 150, loss is 3.791313362121582 and perplexity is 44.31456314183535
At time: 130.26690006256104 and batch: 200, loss is 3.749505467414856 and perplexity is 42.50005913813174
At time: 131.22862720489502 and batch: 250, loss is 3.8275206518173217 and perplexity is 45.94847462388708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807690048217774 and perplexity of 122.44844058708938
Finished 24 epochs...
Completing Train Step...
At time: 132.83512616157532 and batch: 50, loss is 3.9076258516311646 and perplexity is 49.78062497647694
At time: 133.81190490722656 and batch: 100, loss is 3.7853361129760743 and perplexity is 44.050474006710964
At time: 134.7762734889984 and batch: 150, loss is 3.7911368465423583 and perplexity is 44.306741621384724
At time: 135.7397804260254 and batch: 200, loss is 3.7494288587570193 and perplexity is 42.496803390354025
At time: 136.7044768333435 and batch: 250, loss is 3.827424283027649 and perplexity is 45.94404683835386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8076942443847654 and perplexity of 122.44895440227198
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 138.3078269958496 and batch: 50, loss is 3.9071943616867064 and perplexity is 49.75914977087158
At time: 139.284832239151 and batch: 100, loss is 3.784846134185791 and perplexity is 44.02889549568382
At time: 140.24872469902039 and batch: 150, loss is 3.790462603569031 and perplexity is 44.27687818091426
At time: 141.21081161499023 and batch: 200, loss is 3.7485461759567262 and perplexity is 42.45930874330838
At time: 142.1742672920227 and batch: 250, loss is 3.8266637182235717 and perplexity is 45.90911669837307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807679748535156 and perplexity of 122.44717941350913
Finished 26 epochs...
Completing Train Step...
At time: 143.80337810516357 and batch: 50, loss is 3.907097673416138 and perplexity is 49.7543388773175
At time: 144.76659846305847 and batch: 100, loss is 3.7847764682769776 and perplexity is 44.02582828950616
At time: 145.72948169708252 and batch: 150, loss is 3.790415434837341 and perplexity is 44.27478974598208
At time: 146.6953320503235 and batch: 200, loss is 3.7485283517837527 and perplexity is 42.45855194798965
At time: 147.66020441055298 and batch: 250, loss is 3.8266470003128052 and perplexity is 45.90834920027224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807673645019531 and perplexity of 122.44643205751716
Finished 27 epochs...
Completing Train Step...
At time: 149.26830005645752 and batch: 50, loss is 3.90700249671936 and perplexity is 49.74960364903807
At time: 150.24833989143372 and batch: 100, loss is 3.7847124195098876 and perplexity is 44.02300857978453
At time: 151.21445417404175 and batch: 150, loss is 3.7903701639175416 and perplexity is 44.27278543089529
At time: 152.17845916748047 and batch: 200, loss is 3.748509826660156 and perplexity is 42.457765405352504
At time: 153.1406500339508 and batch: 250, loss is 3.8266288900375365 and perplexity is 45.9075177949596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807670974731446 and perplexity of 122.44610509070505
Finished 28 epochs...
Completing Train Step...
At time: 154.762708902359 and batch: 50, loss is 3.9069084978103636 and perplexity is 49.744927460353814
At time: 155.7238757610321 and batch: 100, loss is 3.784650373458862 and perplexity is 44.02027721068411
At time: 156.68510484695435 and batch: 150, loss is 3.790325632095337 and perplexity is 44.27081392698364
At time: 157.64777779579163 and batch: 200, loss is 3.7484901094436647 and perplexity is 42.45692826465333
At time: 158.60834383964539 and batch: 250, loss is 3.8266094541549682 and perplexity is 45.906625550505545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669830322266 and perplexity of 122.44596496233852
Finished 29 epochs...
Completing Train Step...
At time: 160.22692728042603 and batch: 50, loss is 3.9068154001235964 and perplexity is 49.74029653824626
At time: 161.1885986328125 and batch: 100, loss is 3.784589238166809 and perplexity is 44.01758610044231
At time: 162.15154123306274 and batch: 150, loss is 3.7902818155288696 and perplexity is 44.268874174419594
At time: 163.1142807006836 and batch: 200, loss is 3.748469738960266 and perplexity is 42.456063405309784
At time: 164.0773150920868 and batch: 250, loss is 3.8265891551971434 and perplexity is 45.90569370330741
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669067382813 and perplexity of 122.44587154351663
Finished 30 epochs...
Completing Train Step...
At time: 165.70084047317505 and batch: 50, loss is 3.906723208427429 and perplexity is 49.73571110731311
At time: 166.6787645816803 and batch: 100, loss is 3.78452889919281 and perplexity is 44.01493020458692
At time: 167.6413061618805 and batch: 150, loss is 3.7902383995056153 and perplexity is 44.26695223767072
At time: 168.60410165786743 and batch: 200, loss is 3.7484489059448243 and perplexity is 42.45517892669848
At time: 169.5674695968628 and batch: 250, loss is 3.8265681982040407 and perplexity is 45.904731668081816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669067382813 and perplexity of 122.44587154351663
Finished 31 epochs...
Completing Train Step...
At time: 171.19282507896423 and batch: 50, loss is 3.906631798744202 and perplexity is 49.731164989498474
At time: 172.15467381477356 and batch: 100, loss is 3.784468998908997 and perplexity is 44.01229377673783
At time: 173.1179232597351 and batch: 150, loss is 3.7901952695846557 and perplexity is 44.265043048691474
At time: 174.0801661014557 and batch: 200, loss is 3.7484274911880493 and perplexity is 42.45426976910265
At time: 175.04280638694763 and batch: 250, loss is 3.8265467166900633 and perplexity is 45.90374557553828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669830322266 and perplexity of 122.44596496233852
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 176.6730833053589 and batch: 50, loss is 3.9065276336669923 and perplexity is 49.725985008648856
At time: 177.65005087852478 and batch: 100, loss is 3.7843450784683226 and perplexity is 44.00684009181643
At time: 178.61177730560303 and batch: 150, loss is 3.790029287338257 and perplexity is 44.25769644712889
At time: 179.57319021224976 and batch: 200, loss is 3.748210558891296 and perplexity is 42.445061065723365
At time: 180.53653812408447 and batch: 250, loss is 3.8263651084899903 and perplexity is 45.89540983586995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 182.1386263370514 and batch: 50, loss is 3.906502709388733 and perplexity is 49.724745639807026
At time: 183.1155948638916 and batch: 100, loss is 3.7843150520324706 and perplexity is 44.00551874309316
At time: 184.07632422447205 and batch: 150, loss is 3.7899894237518312 and perplexity is 44.25593221178618
At time: 185.0405240058899 and batch: 200, loss is 3.7481587982177733 and perplexity is 42.44286413763262
At time: 186.00442337989807 and batch: 250, loss is 3.826321759223938 and perplexity is 45.89342034666015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 187.62805080413818 and batch: 50, loss is 3.906497459411621 and perplexity is 49.72448458671579
At time: 188.59383702278137 and batch: 100, loss is 3.7843085145950317 and perplexity is 44.005231060707764
At time: 189.57490134239197 and batch: 150, loss is 3.789980525970459 and perplexity is 44.25553843392882
At time: 190.53644347190857 and batch: 200, loss is 3.7481467390060423 and perplexity is 42.44235231323362
At time: 191.49846291542053 and batch: 250, loss is 3.82631160736084 and perplexity is 45.892954445304575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807668685913086 and perplexity of 122.44582483413237
Finished 35 epochs...
Completing Train Step...
At time: 193.1029179096222 and batch: 50, loss is 3.9064969205856324 and perplexity is 49.724457793878436
At time: 194.0802037715912 and batch: 100, loss is 3.7843080949783325 and perplexity is 44.005212595381835
At time: 195.04192304611206 and batch: 150, loss is 3.7899803829193117 and perplexity is 44.25553210312372
At time: 196.00411915779114 and batch: 200, loss is 3.74814688205719 and perplexity is 42.44235838466127
At time: 196.9666566848755 and batch: 250, loss is 3.826311693191528 and perplexity is 45.892958384328615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 198.58492803573608 and batch: 50, loss is 3.906496162414551 and perplexity is 49.72442009424679
At time: 199.54701232910156 and batch: 100, loss is 3.784307255744934 and perplexity is 44.00517566475322
At time: 200.5101408958435 and batch: 150, loss is 3.7899790048599242 and perplexity is 44.25547111641428
At time: 201.47165322303772 and batch: 200, loss is 3.748144941329956 and perplexity is 42.4422760157004
At time: 202.43380975723267 and batch: 250, loss is 3.8263099813461303 and perplexity is 45.89287982274625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669067382813 and perplexity of 122.44587154351663
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 204.05165243148804 and batch: 50, loss is 3.906496114730835 and perplexity is 49.72441772320172
At time: 205.02054905891418 and batch: 100, loss is 3.784307098388672 and perplexity is 44.0051687402638
At time: 205.98837733268738 and batch: 150, loss is 3.789978790283203 and perplexity is 44.2554616202214
At time: 206.95835304260254 and batch: 200, loss is 3.748144717216492 and perplexity is 42.44226650381596
At time: 207.92129921913147 and batch: 250, loss is 3.8263097620010376 and perplexity is 45.89286975636937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 209.52463054656982 and batch: 50, loss is 3.9064960861206055 and perplexity is 49.724416300574745
At time: 210.5030016899109 and batch: 100, loss is 3.7843070936203005 and perplexity is 44.00516853043081
At time: 211.46424102783203 and batch: 150, loss is 3.789978790283203 and perplexity is 44.2554616202214
At time: 212.42697644233704 and batch: 200, loss is 3.748144726753235 and perplexity is 42.44226690857695
At time: 213.38954377174377 and batch: 250, loss is 3.826309742927551 and perplexity is 45.892868881032356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 215.0174207687378 and batch: 50, loss is 3.9064961004257204 and perplexity is 49.724417011888235
At time: 215.97912669181824 and batch: 100, loss is 3.784307074546814 and perplexity is 44.00516769109884
At time: 216.9421923160553 and batch: 150, loss is 3.7899787855148315 and perplexity is 44.25546140919492
At time: 217.9039421081543 and batch: 200, loss is 3.7481447076797485 and perplexity is 42.442266099054955
At time: 218.86347675323486 and batch: 250, loss is 3.8263097381591797 and perplexity is 45.89286866219811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807669448852539 and perplexity of 122.44591825291862
Annealing...
Model not improving. Stopping early with 122.44582483413237loss at 39 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 11.32832026266865, 'wordvec_source': '', 'dropout': 0.28545682566928654, 'anneal': 7.4015066523513235}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2618193626403809 and batch: 50, loss is 6.184448947906494 and perplexity is 485.1455494654441
At time: 2.247732162475586 and batch: 100, loss is 5.342631130218506 and perplexity is 209.0620567662187
At time: 3.2143170833587646 and batch: 150, loss is 5.2317979049682615 and perplexity is 187.12894131100853
At time: 4.181195974349976 and batch: 200, loss is 5.164521398544312 and perplexity is 174.95370543311097
At time: 5.149080753326416 and batch: 250, loss is 5.171905651092529 and perplexity is 176.25038940720677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.180410766601563 and perplexity of 177.75581214998437
Finished 1 epochs...
Completing Train Step...
At time: 6.766055107116699 and batch: 50, loss is 4.890972728729248 and perplexity is 133.08296473323304
At time: 7.7249016761779785 and batch: 100, loss is 4.739389142990112 and perplexity is 114.36432009189348
At time: 8.685758352279663 and batch: 150, loss is 4.752644023895264 and perplexity is 115.89029651061875
At time: 9.644585132598877 and batch: 200, loss is 4.711417579650879 and perplexity is 111.20969682194976
At time: 10.603017568588257 and batch: 250, loss is 4.728048324584961 and perplexity is 113.07466181817641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.981670379638672 and perplexity of 145.71758216071262
Finished 2 epochs...
Completing Train Step...
At time: 12.22956371307373 and batch: 50, loss is 4.60910774230957 and perplexity is 100.39453186812702
At time: 13.187547445297241 and batch: 100, loss is 4.494341106414795 and perplexity is 89.50917253148934
At time: 14.146509408950806 and batch: 150, loss is 4.541419725418091 and perplexity is 93.82390979708303
At time: 15.105854749679565 and batch: 200, loss is 4.511896667480468 and perplexity is 91.09443060909508
At time: 16.06684970855713 and batch: 250, loss is 4.53767957687378 and perplexity is 93.47364985784179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.917345428466797 and perplexity of 136.63941223426625
Finished 3 epochs...
Completing Train Step...
At time: 17.66486883163452 and batch: 50, loss is 4.446392469406128 and perplexity is 85.31859869901773
At time: 18.643224239349365 and batch: 100, loss is 4.348122043609619 and perplexity is 77.33309828764025
At time: 19.603890895843506 and batch: 150, loss is 4.392732629776001 and perplexity is 80.86108074667696
At time: 20.56357717514038 and batch: 200, loss is 4.373411254882813 and perplexity is 79.31373009947738
At time: 21.524213552474976 and batch: 250, loss is 4.407727308273316 and perplexity is 82.08270268041342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.89391860961914 and perplexity of 133.47558932451105
Finished 4 epochs...
Completing Train Step...
At time: 23.134302139282227 and batch: 50, loss is 4.325542078018189 and perplexity is 75.60648636759666
At time: 24.09369969367981 and batch: 100, loss is 4.23891354560852 and perplexity is 69.33248432241989
At time: 25.06756901741028 and batch: 150, loss is 4.284733257293701 and perplexity is 72.5831831026248
At time: 26.026779890060425 and batch: 200, loss is 4.273661670684814 and perplexity is 71.78400435650343
At time: 26.986667156219482 and batch: 250, loss is 4.306061916351318 and perplexity is 74.14791254133445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.886143493652344 and perplexity of 132.44182516467984
Finished 5 epochs...
Completing Train Step...
At time: 28.58433246612549 and batch: 50, loss is 4.230789995193481 and perplexity is 68.77153990557223
At time: 29.55832266807556 and batch: 100, loss is 4.149650759696961 and perplexity is 63.41185045667002
At time: 30.517747163772583 and batch: 150, loss is 4.196397643089295 and perplexity is 66.4465352497816
At time: 31.47808074951172 and batch: 200, loss is 4.1841206455230715 and perplexity is 65.6357584343775
At time: 32.44049263000488 and batch: 250, loss is 4.226788411140442 and perplexity is 68.49689468263891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.892539596557617 and perplexity of 133.29165159885028
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 34.039270639419556 and batch: 50, loss is 4.112455053329468 and perplexity is 61.096528833586554
At time: 35.01290225982666 and batch: 100, loss is 3.93877245426178 and perplexity is 51.35552134032447
At time: 35.9725067615509 and batch: 150, loss is 3.913428602218628 and perplexity is 50.07032925523267
At time: 36.932814836502075 and batch: 200, loss is 3.8074183177948 and perplexity is 45.03402511805068
At time: 37.89199781417847 and batch: 250, loss is 3.7658500242233277 and perplexity is 43.20041165360628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7405235290527346 and perplexity of 114.4941269943175
Finished 7 epochs...
Completing Train Step...
At time: 39.504968881607056 and batch: 50, loss is 3.95802619934082 and perplexity is 52.35388776132548
At time: 40.46628427505493 and batch: 100, loss is 3.823870463371277 and perplexity is 45.78105976647521
At time: 41.42988705635071 and batch: 150, loss is 3.8219068336486814 and perplexity is 45.69125092130211
At time: 42.3924446105957 and batch: 200, loss is 3.744625964164734 and perplexity is 42.29318509318889
At time: 43.35719895362854 and batch: 250, loss is 3.7455489301681517 and perplexity is 42.33223828481791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7282554626464846 and perplexity of 113.09808631040106
Finished 8 epochs...
Completing Train Step...
At time: 44.96195840835571 and batch: 50, loss is 3.89067626953125 and perplexity is 48.9439746530201
At time: 45.938427686691284 and batch: 100, loss is 3.763131260871887 and perplexity is 43.08311947472278
At time: 46.89843249320984 and batch: 150, loss is 3.770796790122986 and perplexity is 43.41464341715392
At time: 47.85929870605469 and batch: 200, loss is 3.709280776977539 and perplexity is 40.824434092712515
At time: 48.819661140441895 and batch: 250, loss is 3.7270287895202636 and perplexity is 41.55545455248827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.724717712402343 and perplexity of 112.69868044361539
Finished 9 epochs...
Completing Train Step...
At time: 50.42945957183838 and batch: 50, loss is 3.843090658187866 and perplexity is 46.66949121698737
At time: 51.38849496841431 and batch: 100, loss is 3.720132999420166 and perplexity is 41.26988261407668
At time: 52.34844183921814 and batch: 150, loss is 3.7335603427886963 and perplexity is 41.827764553040524
At time: 53.30914521217346 and batch: 200, loss is 3.6820115566253664 and perplexity is 39.726225304844434
At time: 54.26967978477478 and batch: 250, loss is 3.7079861879348757 and perplexity is 40.77161742298067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.723537826538086 and perplexity of 112.56578727844011
Finished 10 epochs...
Completing Train Step...
At time: 55.882601737976074 and batch: 50, loss is 3.8045042419433592 and perplexity is 44.902983578186365
At time: 56.84388470649719 and batch: 100, loss is 3.684906530380249 and perplexity is 39.8413983154665
At time: 57.80552005767822 and batch: 150, loss is 3.7027926063537597 and perplexity is 40.560415623322406
At time: 58.76713991165161 and batch: 200, loss is 3.65838809967041 and perplexity is 38.798752718046146
At time: 59.72850799560547 and batch: 250, loss is 3.6895596027374267 and perplexity is 40.02721519908056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.724758148193359 and perplexity of 112.70323759604099
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 61.324015617370605 and batch: 50, loss is 3.7704461765289308 and perplexity is 43.39942432115768
At time: 62.297669410705566 and batch: 100, loss is 3.6401059818267822 and perplexity is 38.09587398177479
At time: 63.25984263420105 and batch: 150, loss is 3.639298734664917 and perplexity is 38.06513360483457
At time: 64.230877161026 and batch: 200, loss is 3.5722169780731203 and perplexity is 35.59542000629565
At time: 65.2125289440155 and batch: 250, loss is 3.590399980545044 and perplexity is 36.24857175074529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.712447738647461 and perplexity of 111.32431952130372
Finished 12 epochs...
Completing Train Step...
At time: 66.83267593383789 and batch: 50, loss is 3.7514912939071654 and perplexity is 42.58454073662836
At time: 67.8025016784668 and batch: 100, loss is 3.623832631111145 and perplexity is 37.48094352368577
At time: 68.76507258415222 and batch: 150, loss is 3.627544322013855 and perplexity is 37.62031970142175
At time: 69.72484540939331 and batch: 200, loss is 3.5665171337127686 and perplexity is 35.39310877132172
At time: 70.68451118469238 and batch: 250, loss is 3.5922638607025146 and perplexity is 36.31619774816752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.711282348632812 and perplexity of 111.19465883825345
Finished 13 epochs...
Completing Train Step...
At time: 72.28591203689575 and batch: 50, loss is 3.741091961860657 and perplexity is 42.14398467222098
At time: 73.27376294136047 and batch: 100, loss is 3.614788656234741 and perplexity is 37.1434950499602
At time: 74.23790335655212 and batch: 150, loss is 3.620740075111389 and perplexity is 37.36521065434296
At time: 75.21560263633728 and batch: 200, loss is 3.5633853340148924 and perplexity is 35.28243803371253
At time: 76.1761999130249 and batch: 250, loss is 3.59270200252533 and perplexity is 36.33211287953435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.711079406738281 and perplexity of 111.17209507317112
Finished 14 epochs...
Completing Train Step...
At time: 77.78496527671814 and batch: 50, loss is 3.732903242111206 and perplexity is 41.800288528860506
At time: 78.76095366477966 and batch: 100, loss is 3.607781004905701 and perplexity is 36.88411626699053
At time: 79.72207069396973 and batch: 150, loss is 3.6153952264785767 and perplexity is 37.166032023247254
At time: 80.6823377609253 and batch: 200, loss is 3.560714955329895 and perplexity is 35.1883462494935
At time: 81.64313912391663 and batch: 250, loss is 3.59214750289917 and perplexity is 36.31197232100719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.711233520507813 and perplexity of 111.18922954410463
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 83.2607843875885 and batch: 50, loss is 3.726091160774231 and perplexity is 41.51650922472805
At time: 84.2272846698761 and batch: 100, loss is 3.5997711610794068 and perplexity is 36.58986030141366
At time: 85.18954086303711 and batch: 150, loss is 3.6045071935653685 and perplexity is 36.76356207239559
At time: 86.1530454158783 and batch: 200, loss is 3.545512261390686 and perplexity is 34.65743446135328
At time: 87.11412382125854 and batch: 250, loss is 3.575923366546631 and perplexity is 35.72759525581634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710296630859375 and perplexity of 111.08510628954295
Finished 16 epochs...
Completing Train Step...
At time: 88.71293044090271 and batch: 50, loss is 3.7239733028411863 and perplexity is 41.42867619808022
At time: 89.69103574752808 and batch: 100, loss is 3.5979666709899902 and perplexity is 36.52389379697829
At time: 90.65336346626282 and batch: 150, loss is 3.6032994031906127 and perplexity is 36.7191861997532
At time: 91.6170482635498 and batch: 200, loss is 3.545104990005493 and perplexity is 34.6433223539368
At time: 92.57866406440735 and batch: 250, loss is 3.5763015699386598 and perplexity is 35.741110109051704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710147857666016 and perplexity of 111.06858103283368
Finished 17 epochs...
Completing Train Step...
At time: 94.18940925598145 and batch: 50, loss is 3.7224982261657713 and perplexity is 41.36761077329144
At time: 95.15010690689087 and batch: 100, loss is 3.5966643476486206 and perplexity is 36.47635883723105
At time: 96.11687707901001 and batch: 150, loss is 3.6024295330047607 and perplexity is 36.68725916263134
At time: 97.07877326011658 and batch: 200, loss is 3.54482337474823 and perplexity is 34.633567639402344
At time: 98.03982257843018 and batch: 250, loss is 3.576528425216675 and perplexity is 35.749219088269655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710089111328125 and perplexity of 111.06205635209575
Finished 18 epochs...
Completing Train Step...
At time: 99.65537977218628 and batch: 50, loss is 3.721198902130127 and perplexity is 41.31389574648622
At time: 100.61665678024292 and batch: 100, loss is 3.5955292987823486 and perplexity is 36.43497987550141
At time: 101.57913875579834 and batch: 150, loss is 3.6016690874099733 and perplexity is 36.65937110303625
At time: 102.53994727134705 and batch: 200, loss is 3.54456157207489 and perplexity is 34.62450166560692
At time: 103.50307154655457 and batch: 250, loss is 3.576655683517456 and perplexity is 35.753768762630855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7100669860839846 and perplexity of 111.05959910416789
Finished 19 epochs...
Completing Train Step...
At time: 105.09935164451599 and batch: 50, loss is 3.7199961042404173 and perplexity is 41.26423335276515
At time: 106.09006452560425 and batch: 100, loss is 3.594483242034912 and perplexity is 36.39688674622096
At time: 107.04767084121704 and batch: 150, loss is 3.600963521003723 and perplexity is 36.63351460512251
At time: 108.01408815383911 and batch: 200, loss is 3.544299077987671 and perplexity is 34.61541413141097
At time: 108.9776918888092 and batch: 250, loss is 3.5767152404785154 and perplexity is 35.75589821185591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710064697265625 and perplexity of 111.05934490920934
Finished 20 epochs...
Completing Train Step...
At time: 110.57936334609985 and batch: 50, loss is 3.7188585662841795 and perplexity is 41.2173204087773
At time: 111.54248428344727 and batch: 100, loss is 3.593496308326721 and perplexity is 36.3609831519659
At time: 112.5045313835144 and batch: 150, loss is 3.6002908658981325 and perplexity is 36.60888117031931
At time: 113.46750283241272 and batch: 200, loss is 3.5440298080444337 and perplexity is 34.60609449561788
At time: 114.44285559654236 and batch: 250, loss is 3.5767267751693725 and perplexity is 35.756310647466755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710074234008789 and perplexity of 111.06040405870812
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 116.0449960231781 and batch: 50, loss is 3.7177573156356813 and perplexity is 41.17195479199
At time: 117.02935004234314 and batch: 100, loss is 3.5921553230285643 and perplexity is 36.31225628643963
At time: 118.00580954551697 and batch: 150, loss is 3.5986348152160645 and perplexity is 36.54830517998666
At time: 118.9723219871521 and batch: 200, loss is 3.5416603565216063 and perplexity is 34.5241941001423
At time: 119.93926000595093 and batch: 250, loss is 3.574305090904236 and perplexity is 35.669824915427355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.71003532409668 and perplexity of 111.05608279221798
Finished 22 epochs...
Completing Train Step...
At time: 121.57306003570557 and batch: 50, loss is 3.717557263374329 and perplexity is 41.163719073144115
At time: 122.56212735176086 and batch: 100, loss is 3.592002582550049 and perplexity is 36.30671035859308
At time: 123.52758312225342 and batch: 150, loss is 3.5985172176361084 and perplexity is 36.54400744045284
At time: 124.51852345466614 and batch: 200, loss is 3.5416269731521606 and perplexity is 34.52304158545336
At time: 125.48465490341187 and batch: 250, loss is 3.574333701133728 and perplexity is 35.670845451902956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710013580322266 and perplexity of 111.05366804005946
Finished 23 epochs...
Completing Train Step...
At time: 127.10483932495117 and batch: 50, loss is 3.7173718070983885 and perplexity is 41.1560857109503
At time: 128.06897974014282 and batch: 100, loss is 3.5918576431274416 and perplexity is 36.30144846629396
At time: 129.02915334701538 and batch: 150, loss is 3.5984098148345947 and perplexity is 36.54008272244182
At time: 129.9923219680786 and batch: 200, loss is 3.5415942764282224 and perplexity is 34.52191281354681
At time: 130.95505499839783 and batch: 250, loss is 3.5743569898605347 and perplexity is 35.671676190151025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.710000610351562 and perplexity of 111.05222768657914
Finished 24 epochs...
Completing Train Step...
At time: 132.57115030288696 and batch: 50, loss is 3.717195134162903 and perplexity is 41.14881518674598
At time: 133.56168174743652 and batch: 100, loss is 3.5917170095443725 and perplexity is 36.2963436224902
At time: 134.52356576919556 and batch: 150, loss is 3.598308219909668 and perplexity is 36.53637062404919
At time: 135.48549485206604 and batch: 200, loss is 3.541561336517334 and perplexity is 34.520775683543626
At time: 136.44869256019592 and batch: 250, loss is 3.574375982284546 and perplexity is 35.67235368818406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709993743896485 and perplexity of 111.05146515406443
Finished 25 epochs...
Completing Train Step...
At time: 138.06585359573364 and batch: 50, loss is 3.717024078369141 and perplexity is 41.14177704547643
At time: 139.02558135986328 and batch: 100, loss is 3.5915793561935425 and perplexity is 36.29134765303154
At time: 139.9880335330963 and batch: 150, loss is 3.5982099676132204 and perplexity is 36.53278101807792
At time: 140.94965863227844 and batch: 200, loss is 3.541528034210205 and perplexity is 34.519626081211776
At time: 141.9110026359558 and batch: 250, loss is 3.5743911361694334 and perplexity is 35.67289426702144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709988784790039 and perplexity of 111.05091443939334
Finished 26 epochs...
Completing Train Step...
At time: 143.54094886779785 and batch: 50, loss is 3.7168575429916384 and perplexity is 41.13492605458705
At time: 144.5036964416504 and batch: 100, loss is 3.5914441251754763 and perplexity is 36.28644026896418
At time: 145.4651744365692 and batch: 150, loss is 3.5981141662597658 and perplexity is 36.52928129585266
At time: 146.4288408756256 and batch: 200, loss is 3.54149405002594 and perplexity is 34.51845297981184
At time: 147.3891921043396 and batch: 250, loss is 3.574403176307678 and perplexity is 35.67332377618567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709986877441406 and perplexity of 111.05070262678551
Finished 27 epochs...
Completing Train Step...
At time: 148.99319171905518 and batch: 50, loss is 3.7166942453384397 and perplexity is 41.12820936612249
At time: 149.98353576660156 and batch: 100, loss is 3.591310729980469 and perplexity is 36.281600155019525
At time: 150.95493364334106 and batch: 150, loss is 3.598020100593567 and perplexity is 36.52584530627866
At time: 151.923020362854 and batch: 200, loss is 3.5414593696594237 and perplexity is 34.51725588796884
At time: 152.90010285377502 and batch: 250, loss is 3.574412703514099 and perplexity is 35.67366364492401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7099853515625 and perplexity of 111.05053317699007
Finished 28 epochs...
Completing Train Step...
At time: 154.5230872631073 and batch: 50, loss is 3.716533694267273 and perplexity is 41.12160671809888
At time: 155.48532390594482 and batch: 100, loss is 3.591178812980652 and perplexity is 36.27681431085256
At time: 156.4450855255127 and batch: 150, loss is 3.5979273414611814 and perplexity is 36.52245735769242
At time: 157.40569043159485 and batch: 200, loss is 3.541424059867859 and perplexity is 34.51603711237552
At time: 158.3676393032074 and batch: 250, loss is 3.574420018196106 and perplexity is 35.673924587383944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7099853515625 and perplexity of 111.05053317699007
Finished 29 epochs...
Completing Train Step...
At time: 159.981760263443 and batch: 50, loss is 3.7163752937316894 and perplexity is 41.115093549429
At time: 160.96085667610168 and batch: 100, loss is 3.5910483026504516 and perplexity is 36.27208012077531
At time: 161.92156291007996 and batch: 150, loss is 3.5978358602523803 and perplexity is 36.51911639196507
At time: 162.88210010528564 and batch: 200, loss is 3.541388330459595 and perplexity is 34.51480389682504
At time: 163.84265089035034 and batch: 250, loss is 3.5744255590438843 and perplexity is 35.67412225171735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709985733032227 and perplexity of 111.05057553941471
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 165.43945217132568 and batch: 50, loss is 3.7162124824523928 and perplexity is 41.10840009334967
At time: 166.4160122871399 and batch: 100, loss is 3.5908563566207885 and perplexity is 36.26511850715693
At time: 167.3766531944275 and batch: 150, loss is 3.5976002073287963 and perplexity is 36.510511569336224
At time: 168.33942341804504 and batch: 200, loss is 3.541055645942688 and perplexity is 34.50332326578447
At time: 169.29834079742432 and batch: 250, loss is 3.574090714454651 and perplexity is 35.66217896458985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984970092774 and perplexity of 111.05049081458168
Finished 31 epochs...
Completing Train Step...
At time: 170.9134819507599 and batch: 50, loss is 3.716190900802612 and perplexity is 41.107512915829226
At time: 171.87625789642334 and batch: 100, loss is 3.5908390283584595 and perplexity is 36.264490101114646
At time: 172.83864164352417 and batch: 150, loss is 3.5975878763198854 and perplexity is 36.510061360668495
At time: 173.80191445350647 and batch: 200, loss is 3.5410513639450074 and perplexity is 34.5031755229506
At time: 174.7798957824707 and batch: 250, loss is 3.5740922498703003 and perplexity is 35.66223372089955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984970092774 and perplexity of 111.05049081458168
Finished 32 epochs...
Completing Train Step...
At time: 176.37520265579224 and batch: 50, loss is 3.7161695337295533 and perplexity is 41.106634577981275
At time: 177.35115027427673 and batch: 100, loss is 3.5908216857910156 and perplexity is 36.26386118720276
At time: 178.31233048439026 and batch: 150, loss is 3.597575659751892 and perplexity is 36.5096153357459
At time: 179.27555918693542 and batch: 200, loss is 3.5410470867156985 and perplexity is 34.50302794527261
At time: 180.23874282836914 and batch: 250, loss is 3.57409387588501 and perplexity is 35.662291708263304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Finished 33 epochs...
Completing Train Step...
At time: 181.8767430782318 and batch: 50, loss is 3.7161482429504393 and perplexity is 41.10575939502105
At time: 182.83633136749268 and batch: 100, loss is 3.5908044672012327 and perplexity is 36.26323678002875
At time: 183.7968144416809 and batch: 150, loss is 3.5975634765625 and perplexity is 36.50917053489719
At time: 184.75687742233276 and batch: 200, loss is 3.54104275226593 and perplexity is 34.502878393955235
At time: 185.71901655197144 and batch: 250, loss is 3.574095435142517 and perplexity is 35.66234731500272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709983825683594 and perplexity of 111.05036372745329
Finished 34 epochs...
Completing Train Step...
At time: 187.3300166130066 and batch: 50, loss is 3.7161269187927246 and perplexity is 41.104882858670464
At time: 188.29254627227783 and batch: 100, loss is 3.590787286758423 and perplexity is 36.262613766914995
At time: 189.2544391155243 and batch: 150, loss is 3.597551383972168 and perplexity is 36.50872904712391
At time: 190.21549487113953 and batch: 200, loss is 3.5410384798049925 and perplexity is 34.50273098206997
At time: 191.1745789051056 and batch: 250, loss is 3.5740968799591064 and perplexity is 35.66239884059097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 192.77322340011597 and batch: 50, loss is 3.7161049461364746 and perplexity is 41.10397968513181
At time: 193.75152230262756 and batch: 100, loss is 3.5907613801956177 and perplexity is 36.26167433940268
At time: 194.71412825584412 and batch: 150, loss is 3.5975194215774535 and perplexity is 36.50756215936394
At time: 195.67510104179382 and batch: 200, loss is 3.5409932804107664 and perplexity is 34.50117151477419
At time: 196.63619828224182 and batch: 250, loss is 3.574051342010498 and perplexity is 35.660774885081395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 198.24603986740112 and batch: 50, loss is 3.716102647781372 and perplexity is 41.10388521369892
At time: 199.20541548728943 and batch: 100, loss is 3.590758752822876 and perplexity is 36.261579066593114
At time: 200.18272590637207 and batch: 150, loss is 3.597516016960144 and perplexity is 36.507437865297476
At time: 201.14394307136536 and batch: 200, loss is 3.5409882164001463 and perplexity is 34.500996800917605
At time: 202.10434412956238 and batch: 250, loss is 3.5740459537506104 and perplexity is 35.660582736076194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 203.70340824127197 and batch: 50, loss is 3.7161025953292848 and perplexity is 41.10388305771441
At time: 204.68175411224365 and batch: 100, loss is 3.5907586097717283 and perplexity is 36.26157387933298
At time: 205.6422245502472 and batch: 150, loss is 3.5975158071517943 and perplexity is 36.50743020573299
At time: 206.60470390319824 and batch: 200, loss is 3.5409879446029664 and perplexity is 34.50098742364525
At time: 207.56743121147156 and batch: 250, loss is 3.5740456008911132 and perplexity is 35.66057015290312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 209.16703629493713 and batch: 50, loss is 3.716102571487427 and perplexity is 41.10388207772149
At time: 210.14150834083557 and batch: 100, loss is 3.5907586145401003 and perplexity is 36.26157405224165
At time: 211.10362243652344 and batch: 150, loss is 3.5975157833099365 and perplexity is 36.507429335328034
At time: 212.0651569366455 and batch: 200, loss is 3.5409879732131957 and perplexity is 34.50098841072642
At time: 213.02661180496216 and batch: 250, loss is 3.574045624732971 and perplexity is 35.660571003117376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.709984588623047 and perplexity of 111.05044845218936
Annealing...
Model not improving. Stopping early with 111.05036372745329loss at 38 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 0.0, 'wordvec_source': '', 'dropout': 1.0, 'anneal': 8.0}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.253404140472412 and batch: 50, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 2.2457916736602783 and batch: 100, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 3.2100183963775635 and batch: 150, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 4.174753427505493 and batch: 200, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 5.139721870422363 and batch: 250, loss is 9.169513702392578 and perplexity is 9599.955119496755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 1 epochs...
Completing Train Step...
At time: 6.749192237854004 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 7.707066297531128 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 8.665331840515137 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 9.624982595443726 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 10.584338426589966 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 2 epochs...
Completing Train Step...
At time: 12.194742918014526 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 13.158870697021484 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 14.124975442886353 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 15.085686206817627 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 16.04276180267334 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 3 epochs...
Completing Train Step...
At time: 17.630911827087402 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 18.60340118408203 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 19.561466932296753 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 20.518593311309814 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 21.479172945022583 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 4 epochs...
Completing Train Step...
At time: 23.081044673919678 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 24.04112195968628 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 25.000368356704712 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 25.95770263671875 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 26.917861938476562 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 5 epochs...
Completing Train Step...
At time: 28.51146388053894 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 29.487040996551514 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 30.447625160217285 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 31.40920662879944 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 32.36789655685425 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 6 epochs...
Completing Train Step...
At time: 33.962756633758545 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 34.93860602378845 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 35.89905905723572 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 36.875285625457764 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 37.83590340614319 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 7 epochs...
Completing Train Step...
At time: 39.44181728363037 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 40.40437340736389 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 41.36515140533447 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 42.3264262676239 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 43.2887544631958 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 8 epochs...
Completing Train Step...
At time: 44.88141584396362 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 45.85770106315613 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 46.81872844696045 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 47.782307386398315 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 48.7428834438324 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 9 epochs...
Completing Train Step...
At time: 50.35068416595459 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 51.31135940551758 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 52.27325677871704 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 53.2368369102478 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 54.19722366333008 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 10 epochs...
Completing Train Step...
At time: 55.809701442718506 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 56.77040123939514 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 57.73164176940918 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 58.6918740272522 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 59.654043436050415 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 11 epochs...
Completing Train Step...
At time: 61.246527433395386 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 62.234352588653564 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 63.19419622421265 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 64.15685749053955 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 65.11626958847046 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 12 epochs...
Completing Train Step...
At time: 66.7306215763092 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 67.69474458694458 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 68.65927267074585 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 69.62012147903442 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 70.583181142807 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 13 epochs...
Completing Train Step...
At time: 72.17750263214111 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 73.15248656272888 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 74.11299848556519 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 75.07416319847107 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 76.03617262840271 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 14 epochs...
Completing Train Step...
At time: 77.63239431381226 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 78.60830926895142 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 79.56710410118103 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 80.52604985237122 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 81.48536586761475 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 15 epochs...
Completing Train Step...
At time: 83.09206295013428 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 84.0545563697815 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 85.01886487007141 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 85.98005700111389 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 86.95547938346863 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 16 epochs...
Completing Train Step...
At time: 88.55731320381165 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 89.5339560508728 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 90.49577498435974 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 91.45836019515991 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 92.42075228691101 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 17 epochs...
Completing Train Step...
At time: 94.02796673774719 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 94.99047374725342 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 95.95230770111084 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 96.92846918106079 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 97.91581726074219 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 18 epochs...
Completing Train Step...
At time: 99.52878952026367 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 100.49668478965759 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 101.45726561546326 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 102.42045760154724 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 103.3855791091919 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 19 epochs...
Completing Train Step...
At time: 104.98726296424866 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 105.96259880065918 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 106.92739582061768 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 107.88732051849365 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 108.85338091850281 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 20 epochs...
Completing Train Step...
At time: 110.46978521347046 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 111.43172025680542 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 112.40922904014587 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 113.37060713768005 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 114.33214712142944 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 21 epochs...
Completing Train Step...
At time: 115.9307553768158 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 116.90615057945251 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 117.86818218231201 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 118.83167004585266 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 119.79023742675781 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 22 epochs...
Completing Train Step...
At time: 121.39028334617615 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 122.36821818351746 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 123.33115673065186 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 124.29218482971191 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 125.25764346122742 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 23 epochs...
Completing Train Step...
At time: 126.88369846343994 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 127.8432445526123 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 128.80577206611633 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 129.7669529914856 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 130.72916984558105 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 24 epochs...
Completing Train Step...
At time: 132.32591843605042 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 133.3053276538849 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 134.26877117156982 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 135.23130416870117 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 136.1937017440796 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 25 epochs...
Completing Train Step...
At time: 137.8039526939392 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 138.7683448791504 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 139.73123240470886 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 140.69323539733887 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 141.6537528038025 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 26 epochs...
Completing Train Step...
At time: 143.26705884933472 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 144.2292516231537 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 145.19268441200256 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 146.15417098999023 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 147.11633205413818 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 27 epochs...
Completing Train Step...
At time: 148.72195029258728 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 149.70192503929138 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 150.66791343688965 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 151.63145089149475 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 152.5914556980133 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 28 epochs...
Completing Train Step...
At time: 154.20080995559692 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 155.16197991371155 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 156.12282919883728 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 157.08660697937012 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 158.04806232452393 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 29 epochs...
Completing Train Step...
At time: 159.64240837097168 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 160.61959505081177 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 161.5809769630432 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 162.56086206436157 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 163.52035784721375 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 30 epochs...
Completing Train Step...
At time: 165.11751508712769 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 166.09272575378418 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 167.05321645736694 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 168.01512598991394 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 168.97591519355774 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 31 epochs...
Completing Train Step...
At time: 170.58680176734924 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 171.54776310920715 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 172.5113604068756 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 173.47254347801208 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 174.4317193031311 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 32 epochs...
Completing Train Step...
At time: 176.03323411941528 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 177.01743412017822 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 177.97443342208862 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 178.96507477760315 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 179.94111347198486 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 33 epochs...
Completing Train Step...
At time: 181.54817581176758 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 182.50849676132202 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 183.47067284584045 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 184.43132615089417 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 185.39161562919617 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 34 epochs...
Completing Train Step...
At time: 187.00067257881165 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 187.96456718444824 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 188.92949318885803 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 189.8923738002777 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 190.85939693450928 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 35 epochs...
Completing Train Step...
At time: 192.4774308204651 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 193.45340085029602 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 194.417058467865 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 195.38103818893433 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 196.34272146224976 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 36 epochs...
Completing Train Step...
At time: 197.9513702392578 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 198.91073513031006 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 199.87223076820374 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 200.8318591117859 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 201.7932436466217 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 37 epochs...
Completing Train Step...
At time: 203.38534212112427 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 204.3603687286377 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 205.3193485736847 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 206.28025555610657 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 207.24218010902405 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 38 epochs...
Completing Train Step...
At time: 208.83511877059937 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 209.81435704231262 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 210.77413487434387 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 211.75131630897522 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 212.71294403076172 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 39 epochs...
Completing Train Step...
At time: 214.32315111160278 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 215.28248167037964 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 216.24348735809326 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 217.20554971694946 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 218.16773891448975 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 40 epochs...
Completing Train Step...
At time: 219.75940561294556 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 220.73664140701294 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 221.69748163223267 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 222.65792107582092 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 223.61871242523193 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 41 epochs...
Completing Train Step...
At time: 225.22899413108826 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 226.18766236305237 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 227.1487627029419 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 228.11113119125366 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 229.07139444351196 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 42 epochs...
Completing Train Step...
At time: 230.67794942855835 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 231.64174461364746 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 232.603089094162 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 233.5649950504303 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 234.5363848209381 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 43 epochs...
Completing Train Step...
At time: 236.13244080543518 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 237.11027836799622 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 238.07052731513977 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 239.0334939956665 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 239.99539184570312 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 44 epochs...
Completing Train Step...
At time: 241.60617017745972 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 242.56831312179565 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 243.53060674667358 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 244.49319505691528 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 245.45551347732544 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 45 epochs...
Completing Train Step...
At time: 247.05049443244934 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 248.02563047409058 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 248.98623847961426 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 249.94899797439575 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 250.9093906879425 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 46 epochs...
Completing Train Step...
At time: 252.51973247528076 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 253.4969503879547 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 254.45960521697998 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 255.419531583786 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 256.3793349266052 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 47 epochs...
Completing Train Step...
At time: 257.9887478351593 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 258.9502100944519 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 259.9212439060211 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 260.88436698913574 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 261.86502504348755 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 48 epochs...
Completing Train Step...
At time: 263.45919013023376 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 264.43545031547546 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 265.3950834274292 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 266.35718274116516 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 267.3196973800659 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished 49 epochs...
Completing Train Step...
At time: 268.92898750305176 and batch: 50, loss is 9.181493854522705 and perplexity is 9715.655713816663
At time: 269.8898150920868 and batch: 100, loss is 9.183319530487061 and perplexity is 9733.409554380072
At time: 270.8525700569153 and batch: 150, loss is 9.181200332641602 and perplexity is 9712.80437476107
At time: 271.81360816955566 and batch: 200, loss is 9.181214714050293 and perplexity is 9712.944059574753
At time: 272.7791426181793 and batch: 250, loss is 9.180547561645508 and perplexity is 9706.466206685696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640696716308593 and perplexity of 15378.054141673138
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f66085af898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -103.18160913426128, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 2.6228361864643315, 'wordvec_source': '', 'dropout': 0.042943023229184196, 'anneal': 2.149867874059633}}, {'best_accuracy': -105.0599455669881, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 8.968428148007806, 'wordvec_source': '', 'dropout': 0.6169485602122273, 'anneal': 7.59396827069606}}, {'best_accuracy': -151.47700138302025, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 25.624389660383688, 'wordvec_source': '', 'dropout': 0.3363634518100539, 'anneal': 6.133632921919033}}, {'best_accuracy': -122.44582483413237, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 14.695813735610955, 'wordvec_source': '', 'dropout': 0.995196728618948, 'anneal': 4.186986656231221}}, {'best_accuracy': -111.05036372745329, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 11.32832026266865, 'wordvec_source': '', 'dropout': 0.28545682566928654, 'anneal': 7.4015066523513235}}, {'best_accuracy': -15378.054141673138, 'params': {'wordvec_dim': 200, 'num_layers': 1, 'data': 'ptb', 'tune_wordvecs': True, 'batch_size': 80, 'seq_len': 50, 'lr': 0.0, 'wordvec_source': '', 'dropout': 1.0, 'anneal': 8.0}}]
