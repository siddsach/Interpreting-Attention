Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 7.303909839173954, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.2450297751201902, 'lr': 29.827993674328212}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8666105270385742 and batch: 50, loss is 6.763023128509522 and perplexity is 865.2540199557058
At time: 3.2191646099090576 and batch: 100, loss is 5.73633584022522 and perplexity is 309.92670691695065
At time: 4.531077861785889 and batch: 150, loss is 5.723979139328003 and perplexity is 306.12059919567537
At time: 5.834145545959473 and batch: 200, loss is 5.788241243362426 and perplexity is 326.4383935148192
At time: 7.126680850982666 and batch: 250, loss is 5.8899558162689205 and perplexity is 361.3893164911535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.115814971923828 and perplexity of 452.9650507859251
Finished 1 epochs...
Completing Train Step...
At time: 9.167392253875732 and batch: 50, loss is 5.818130769729614 and perplexity is 336.3427635943731
At time: 10.450720310211182 and batch: 100, loss is 5.841318693161011 and perplexity is 344.2329791696571
At time: 11.741245985031128 and batch: 150, loss is 5.875628242492676 and perplexity is 356.2484007652113
At time: 13.028933048248291 and batch: 200, loss is 5.917893018722534 and perplexity is 371.62787542398684
At time: 14.310409307479858 and batch: 250, loss is 6.027716875076294 and perplexity is 414.76698270356525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.351406097412109 and perplexity of 573.2982557361263
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.332337141036987 and batch: 50, loss is 5.831256933212281 and perplexity is 340.7867561753672
At time: 17.613176107406616 and batch: 100, loss is 5.5695444679260255 and perplexity is 262.31457931541684
At time: 18.893629789352417 and batch: 150, loss is 5.429100303649903 and perplexity is 227.94407269103783
At time: 20.169331073760986 and batch: 200, loss is 5.329772577285767 and perplexity is 206.39103081009875
At time: 21.4438738822937 and batch: 250, loss is 5.4014417171478275 and perplexity is 221.725851843657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.637656402587891 and perplexity of 280.8038555329637
Finished 3 epochs...
Completing Train Step...
At time: 23.446147918701172 and batch: 50, loss is 5.419934597015381 and perplexity is 225.86434981635261
At time: 24.741116762161255 and batch: 100, loss is 5.321242961883545 and perplexity is 204.63808131520324
At time: 26.02156949043274 and batch: 150, loss is 5.277575597763062 and perplexity is 195.89437242481443
At time: 27.30102014541626 and batch: 200, loss is 5.272036495208741 and perplexity is 194.81229304712573
At time: 28.579511165618896 and batch: 250, loss is 5.338222923278809 and perplexity is 208.1424962523704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.598378372192383 and perplexity of 269.9882318188528
Finished 4 epochs...
Completing Train Step...
At time: 30.603774309158325 and batch: 50, loss is 5.325457363128662 and perplexity is 205.50232816222075
At time: 31.902823209762573 and batch: 100, loss is 5.2651673412323 and perplexity is 193.47868303981622
At time: 33.19196343421936 and batch: 150, loss is 5.260428056716919 and perplexity is 192.56390192943203
At time: 34.474562644958496 and batch: 200, loss is 5.255179557800293 and perplexity is 191.55587811869324
At time: 35.751505613327026 and batch: 250, loss is 5.302153520584106 and perplexity is 200.76870429857019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.579507446289062 and perplexity of 264.94107593225357
Finished 5 epochs...
Completing Train Step...
At time: 37.806782722473145 and batch: 50, loss is 5.284552345275879 and perplexity is 197.2658566870325
At time: 39.0847053527832 and batch: 100, loss is 5.24083270072937 and perplexity is 188.82727356673792
At time: 40.3650267124176 and batch: 150, loss is 5.233644962310791 and perplexity is 187.4748985991752
At time: 41.6407949924469 and batch: 200, loss is 5.225769414901733 and perplexity is 186.0042299091633
At time: 42.91410565376282 and batch: 250, loss is 5.272230463027954 and perplexity is 194.85008402776265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.550118255615234 and perplexity of 257.2679774899359
Finished 6 epochs...
Completing Train Step...
At time: 44.88859844207764 and batch: 50, loss is 5.248980236053467 and perplexity is 190.3720349016521
At time: 46.202550411224365 and batch: 100, loss is 5.212404470443726 and perplexity is 183.53483214667756
At time: 47.5058434009552 and batch: 150, loss is 5.218037948608399 and perplexity is 184.57168943031934
At time: 48.80753207206726 and batch: 200, loss is 5.208335924148559 and perplexity is 182.7896291596885
At time: 50.10454058647156 and batch: 250, loss is 5.253403606414795 and perplexity is 191.2159860967368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.548393249511719 and perplexity of 256.8245712077499
Finished 7 epochs...
Completing Train Step...
At time: 52.11004042625427 and batch: 50, loss is 5.2337699890136715 and perplexity is 187.49833943295445
At time: 53.42876958847046 and batch: 100, loss is 5.19255669593811 and perplexity is 179.92798651785867
At time: 54.735530853271484 and batch: 150, loss is 5.193745250701904 and perplexity is 180.14196792249297
At time: 56.03905534744263 and batch: 200, loss is 5.186278247833252 and perplexity is 178.80185686442016
At time: 57.33935737609863 and batch: 250, loss is 5.2364865589141845 and perplexity is 188.0083842507872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.531220245361328 and perplexity of 252.45177625868791
Finished 8 epochs...
Completing Train Step...
At time: 59.355743646621704 and batch: 50, loss is 5.210528507232666 and perplexity is 183.1908503031436
At time: 60.660542011260986 and batch: 100, loss is 5.171288833618164 and perplexity is 176.14170860871045
At time: 61.96514391899109 and batch: 150, loss is 5.171845254898071 and perplexity is 176.23974487586267
At time: 63.26769685745239 and batch: 200, loss is 5.163646259307861 and perplexity is 174.80066355716679
At time: 64.57026863098145 and batch: 250, loss is 5.215447158813476 and perplexity is 184.09412188659806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.505711364746094 and perplexity of 246.09345563562582
Finished 9 epochs...
Completing Train Step...
At time: 66.57846713066101 and batch: 50, loss is 5.1933454990386965 and perplexity is 180.0699702627533
At time: 67.88945055007935 and batch: 100, loss is 5.150397453308106 and perplexity is 172.50003740433328
At time: 69.19176769256592 and batch: 150, loss is 5.152973194122314 and perplexity is 172.94492550338006
At time: 70.49271440505981 and batch: 200, loss is 5.1491471290588375 and perplexity is 172.28449120396803
At time: 71.79234290122986 and batch: 250, loss is 5.198074235916137 and perplexity is 180.92349021683637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.493919372558594 and perplexity of 243.2085662547341
Finished 10 epochs...
Completing Train Step...
At time: 73.78955078125 and batch: 50, loss is 5.168106298446656 and perplexity is 175.58202250864252
At time: 75.10575771331787 and batch: 100, loss is 5.132993364334107 and perplexity is 169.52380583013218
At time: 76.41349720954895 and batch: 150, loss is 5.1387287044525145 and perplexity is 170.49887602148956
At time: 77.72212600708008 and batch: 200, loss is 5.127240562438965 and perplexity is 168.5513687594695
At time: 79.0194137096405 and batch: 250, loss is 5.174301519393921 and perplexity is 176.67316838743827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.486219024658203 and perplexity of 241.342967779304
Finished 11 epochs...
Completing Train Step...
At time: 81.02749800682068 and batch: 50, loss is 5.150547838211059 and perplexity is 172.52598075641316
At time: 82.34344911575317 and batch: 100, loss is 5.108753242492676 and perplexity is 165.46393282091043
At time: 83.65094447135925 and batch: 150, loss is 5.119153604507447 and perplexity is 167.19379763990707
At time: 84.94850754737854 and batch: 200, loss is 5.109398460388183 and perplexity is 165.5707275606612
At time: 86.25224375724792 and batch: 250, loss is 5.157516689300537 and perplexity is 173.73248772625755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.4822540283203125 and perplexity of 240.38793839108027
Finished 12 epochs...
Completing Train Step...
At time: 88.26147127151489 and batch: 50, loss is 5.135365190505982 and perplexity is 169.92636403988683
At time: 89.57613015174866 and batch: 100, loss is 5.093435649871826 and perplexity is 162.94873626055664
At time: 90.88568258285522 and batch: 150, loss is 5.099271545410156 and perplexity is 163.90246829692077
At time: 92.19323801994324 and batch: 200, loss is 5.09795449256897 and perplexity is 163.68674217790564
At time: 93.49905395507812 and batch: 250, loss is 5.142356100082398 and perplexity is 171.11846596882341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.461206436157227 and perplexity of 235.38122555805143
Finished 13 epochs...
Completing Train Step...
At time: 95.49855351448059 and batch: 50, loss is 5.123491039276123 and perplexity is 167.92056484415818
At time: 96.81681513786316 and batch: 100, loss is 5.08007246017456 and perplexity is 160.78570606684505
At time: 98.12900733947754 and batch: 150, loss is 5.0883630180358885 and perplexity is 162.12425023834203
At time: 99.42338871955872 and batch: 200, loss is 5.08307788848877 and perplexity is 161.26966286482806
At time: 100.72714948654175 and batch: 250, loss is 5.134572296142578 and perplexity is 169.79168378430964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.454154205322266 and perplexity of 233.72710230620342
Finished 14 epochs...
Completing Train Step...
At time: 102.73671174049377 and batch: 50, loss is 5.1089599609375 and perplexity is 165.49814080336628
At time: 104.04264187812805 and batch: 100, loss is 5.070841312408447 and perplexity is 159.3082990281155
At time: 105.34064936637878 and batch: 150, loss is 5.076751794815063 and perplexity is 160.25267603938255
At time: 106.63806223869324 and batch: 200, loss is 5.072623033523559 and perplexity is 159.59239500304915
At time: 107.94872093200684 and batch: 250, loss is 5.123512372970581 and perplexity is 167.92414724839463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.445818710327148 and perplexity of 231.78696843622745
Finished 15 epochs...
Completing Train Step...
At time: 110.0365343093872 and batch: 50, loss is 5.103123664855957 and perplexity is 164.53505780519498
At time: 111.36300349235535 and batch: 100, loss is 5.059787015914917 and perplexity is 157.55695562550426
At time: 112.66866159439087 and batch: 150, loss is 5.065639753341674 and perplexity is 158.48179890898416
At time: 113.96635103225708 and batch: 200, loss is 5.062449636459351 and perplexity is 157.97702901230855
At time: 115.26580786705017 and batch: 250, loss is 5.113091106414795 and perplexity is 166.18325187162984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.440066528320313 and perplexity of 230.45751490201877
Finished 16 epochs...
Completing Train Step...
At time: 117.26545548439026 and batch: 50, loss is 5.091881875991821 and perplexity is 162.6957473650415
At time: 118.57487940788269 and batch: 100, loss is 5.052768039703369 and perplexity is 156.45493914008026
At time: 119.87564706802368 and batch: 150, loss is 5.0607036685943605 and perplexity is 157.70144684505763
At time: 121.17839574813843 and batch: 200, loss is 5.055031757354737 and perplexity is 156.80951012033452
At time: 122.48109579086304 and batch: 250, loss is 5.105506391525268 and perplexity is 164.9275673111957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.436919021606445 and perplexity of 229.7332886779516
Finished 17 epochs...
Completing Train Step...
At time: 124.49262142181396 and batch: 50, loss is 5.082913026809693 and perplexity is 161.24307786890768
At time: 125.79510712623596 and batch: 100, loss is 5.043196954727173 and perplexity is 154.96463889441566
At time: 127.09177160263062 and batch: 150, loss is 5.053193664550781 and perplexity is 156.5215444231048
At time: 128.40114974975586 and batch: 200, loss is 5.044803819656372 and perplexity is 155.21384630562426
At time: 129.70837020874023 and batch: 250, loss is 5.092930173873901 and perplexity is 162.86639039916628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.431295776367188 and perplexity of 228.44506744261514
Finished 18 epochs...
Completing Train Step...
At time: 131.69628071784973 and batch: 50, loss is 5.07057107925415 and perplexity is 159.26525446026002
At time: 133.01718640327454 and batch: 100, loss is 5.034982929229736 and perplexity is 153.69696886380217
At time: 134.31502223014832 and batch: 150, loss is 5.045927114486695 and perplexity is 155.38829517718847
At time: 135.6177921295166 and batch: 200, loss is 5.035613346099853 and perplexity is 153.79389257382076
At time: 136.91515159606934 and batch: 250, loss is 5.082369003295899 and perplexity is 161.15538169966243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.435245132446289 and perplexity of 229.34906228221973
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 138.93243932724 and batch: 50, loss is 5.032411518096924 and perplexity is 153.30225846583264
At time: 140.238431930542 and batch: 100, loss is 4.9454325008392335 and perplexity is 140.5316177522478
At time: 141.53763031959534 and batch: 150, loss is 4.921508884429931 and perplexity is 137.20949033396903
At time: 142.84255814552307 and batch: 200, loss is 4.9053293895721435 and perplexity is 135.00737270436542
At time: 144.155122756958 and batch: 250, loss is 4.968558111190796 and perplexity is 143.81936626093182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.335294723510742 and perplexity of 207.53390491648005
Finished 20 epochs...
Completing Train Step...
At time: 146.17012906074524 and batch: 50, loss is 4.96071228981018 and perplexity is 142.69540018635564
At time: 147.49164748191833 and batch: 100, loss is 4.8991552925109865 and perplexity is 134.1763919987936
At time: 148.7996861934662 and batch: 150, loss is 4.89745451927185 and perplexity is 133.94838233328915
At time: 150.11589860916138 and batch: 200, loss is 4.901470489501953 and perplexity is 134.48739665741397
At time: 151.42365169525146 and batch: 250, loss is 4.958462038040161 and perplexity is 142.37466061724652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.321088409423828 and perplexity of 204.6064564402982
Finished 21 epochs...
Completing Train Step...
At time: 153.4256465435028 and batch: 50, loss is 4.938783416748047 and perplexity is 139.6003108085465
At time: 154.73424196243286 and batch: 100, loss is 4.886057806015015 and perplexity is 132.4304770238026
At time: 156.0345242023468 and batch: 150, loss is 4.895399379730224 and perplexity is 133.67338239463393
At time: 157.34154677391052 and batch: 200, loss is 4.898563613891602 and perplexity is 134.09702617826667
At time: 158.6500527858734 and batch: 250, loss is 4.948608455657959 and perplexity is 140.9786493211083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.310066986083984 and perplexity of 202.36378349153716
Finished 22 epochs...
Completing Train Step...
At time: 160.7015073299408 and batch: 50, loss is 4.928576784133911 and perplexity is 138.1827085028481
At time: 162.00193238258362 and batch: 100, loss is 4.879570970535278 and perplexity is 131.57420256521343
At time: 163.31430077552795 and batch: 150, loss is 4.889015941619873 and perplexity is 132.82280432564482
At time: 164.62140822410583 and batch: 200, loss is 4.892206916809082 and perplexity is 133.24731554097775
At time: 165.92951107025146 and batch: 250, loss is 4.943649816513061 and perplexity is 140.2813174094853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3078563690185545 and perplexity of 201.9169287526568
Finished 23 epochs...
Completing Train Step...
At time: 167.94775485992432 and batch: 50, loss is 4.922271232604981 and perplexity is 137.31413162000342
At time: 169.2546260356903 and batch: 100, loss is 4.873776168823242 and perplexity is 130.81396100287304
At time: 170.554016828537 and batch: 150, loss is 4.884950914382935 and perplexity is 132.28397193453952
At time: 171.85736894607544 and batch: 200, loss is 4.888648128509521 and perplexity is 132.77395934032376
At time: 173.16397070884705 and batch: 250, loss is 4.939802589416504 and perplexity is 139.74266015679166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.303546905517578 and perplexity of 201.04864737519213
Finished 24 epochs...
Completing Train Step...
At time: 175.1771559715271 and batch: 50, loss is 4.916537313461304 and perplexity is 136.5290364790567
At time: 176.47805976867676 and batch: 100, loss is 4.869302225112915 and perplexity is 130.23001395197085
At time: 177.79602336883545 and batch: 150, loss is 4.881958332061767 and perplexity is 131.88869300611594
At time: 179.1016628742218 and batch: 200, loss is 4.885958871841431 and perplexity is 132.41737577209233
At time: 180.40849900245667 and batch: 250, loss is 4.935928125381469 and perplexity is 139.20227976404442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.301036071777344 and perplexity of 200.54448085196015
Finished 25 epochs...
Completing Train Step...
At time: 182.41561102867126 and batch: 50, loss is 4.911964206695557 and perplexity is 135.90610008157824
At time: 183.70860266685486 and batch: 100, loss is 4.865811109542847 and perplexity is 129.77615861425232
At time: 185.01146531105042 and batch: 150, loss is 4.878948097229004 and perplexity is 131.49227402484047
At time: 186.31409859657288 and batch: 200, loss is 4.883019294738769 and perplexity is 132.02869624288564
At time: 187.62234687805176 and batch: 250, loss is 4.932780113220215 and perplexity is 138.7647583172096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.299411392211914 and perplexity of 200.21892486565037
Finished 26 epochs...
Completing Train Step...
At time: 189.6190104484558 and batch: 50, loss is 4.907688989639282 and perplexity is 135.3263122469656
At time: 190.93171787261963 and batch: 100, loss is 4.86255820274353 and perplexity is 129.3546947285165
At time: 192.2368941307068 and batch: 150, loss is 4.875955047607422 and perplexity is 131.0992995133028
At time: 193.54257535934448 and batch: 200, loss is 4.880325632095337 and perplexity is 131.6735340342913
At time: 194.84332704544067 and batch: 250, loss is 4.929513931274414 and perplexity is 138.31226673106804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.296854782104492 and perplexity of 199.7076969223649
Finished 27 epochs...
Completing Train Step...
At time: 196.83677744865417 and batch: 50, loss is 4.903589534759521 and perplexity is 134.77268369882628
At time: 198.15062737464905 and batch: 100, loss is 4.858805818557739 and perplexity is 128.87021576173683
At time: 199.4510407447815 and batch: 150, loss is 4.872870454788208 and perplexity is 130.6955346007372
At time: 200.7523856163025 and batch: 200, loss is 4.87736855506897 and perplexity is 131.28474038151805
At time: 202.0561125278473 and batch: 250, loss is 4.925971250534058 and perplexity is 137.8231374535719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.293968963623047 and perplexity of 199.13220753803714
Finished 28 epochs...
Completing Train Step...
At time: 204.04294562339783 and batch: 50, loss is 4.899580955505371 and perplexity is 134.23351808095458
At time: 205.3518168926239 and batch: 100, loss is 4.855219449996948 and perplexity is 128.4088674485395
At time: 206.64642024040222 and batch: 150, loss is 4.870002813339234 and perplexity is 130.32128353392304
At time: 207.94862174987793 and batch: 200, loss is 4.874103956222534 and perplexity is 130.85684719933468
At time: 209.25221228599548 and batch: 250, loss is 4.922331790924073 and perplexity is 137.32244738479372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.291464233398438 and perplexity of 198.63405920315387
Finished 29 epochs...
Completing Train Step...
At time: 211.23967719078064 and batch: 50, loss is 4.895436220169067 and perplexity is 133.678307071416
At time: 212.55001544952393 and batch: 100, loss is 4.851649780273437 and perplexity is 127.9513073572535
At time: 213.85065627098083 and batch: 150, loss is 4.86670482635498 and perplexity is 129.89219359250205
At time: 215.15031909942627 and batch: 200, loss is 4.870952177047729 and perplexity is 130.44506457827347
At time: 216.4559485912323 and batch: 250, loss is 4.918734216690064 and perplexity is 136.8293072722861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.288412857055664 and perplexity of 198.02887572487325
Finished 30 epochs...
Completing Train Step...
At time: 218.4633984565735 and batch: 50, loss is 4.891797523498536 and perplexity is 133.1927761461405
At time: 219.76611423492432 and batch: 100, loss is 4.848038110733032 and perplexity is 127.49002302252894
At time: 221.07716703414917 and batch: 150, loss is 4.863232164382935 and perplexity is 129.44190421526366
At time: 222.3821258544922 and batch: 200, loss is 4.867625341415406 and perplexity is 130.01181636176912
At time: 223.68398666381836 and batch: 250, loss is 4.915688323974609 and perplexity is 136.41317395245022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.286112976074219 and perplexity of 197.57395621070896
Finished 31 epochs...
Completing Train Step...
At time: 225.6870927810669 and batch: 50, loss is 4.888359069824219 and perplexity is 132.73558542061903
At time: 226.99823546409607 and batch: 100, loss is 4.844526872634888 and perplexity is 127.04316017659012
At time: 228.30645155906677 and batch: 150, loss is 4.860823736190796 and perplexity is 129.13052779829027
At time: 229.61226296424866 and batch: 200, loss is 4.865163402557373 and perplexity is 129.69212890602216
At time: 230.91336917877197 and batch: 250, loss is 4.912574577331543 and perplexity is 135.98907849553692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.2838279724121096 and perplexity of 197.1230143952862
Finished 32 epochs...
Completing Train Step...
At time: 232.93354964256287 and batch: 50, loss is 4.884718494415283 and perplexity is 132.2532300707116
At time: 234.23701453208923 and batch: 100, loss is 4.8408973598480225 and perplexity is 126.58289118361718
At time: 235.54314017295837 and batch: 150, loss is 4.856556625366211 and perplexity is 128.58068747443815
At time: 236.84721636772156 and batch: 200, loss is 4.860732345581055 and perplexity is 129.11872701986695
At time: 238.15309810638428 and batch: 250, loss is 4.908441333770752 and perplexity is 135.42816251223869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.280816650390625 and perplexity of 196.53030638632862
Finished 33 epochs...
Completing Train Step...
At time: 240.16339707374573 and batch: 50, loss is 4.880582962036133 and perplexity is 131.70742193700772
At time: 241.4614622592926 and batch: 100, loss is 4.836587553024292 and perplexity is 126.03851729120409
At time: 242.77466106414795 and batch: 150, loss is 4.853458738327026 and perplexity is 128.18297538085866
At time: 244.080411195755 and batch: 200, loss is 4.858076276779175 and perplexity is 128.7762338413631
At time: 245.38468170166016 and batch: 250, loss is 4.905661535263062 and perplexity is 135.05222226933392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.279369735717774 and perplexity of 196.24614942737006
Finished 34 epochs...
Completing Train Step...
At time: 247.3858118057251 and batch: 50, loss is 4.87761890411377 and perplexity is 131.31761150532486
At time: 248.70244145393372 and batch: 100, loss is 4.833342046737671 and perplexity is 125.63012157489129
At time: 250.0109305381775 and batch: 150, loss is 4.850558204650879 and perplexity is 127.81171503091892
At time: 251.31012654304504 and batch: 200, loss is 4.8559166622161865 and perplexity is 128.49842689732625
At time: 252.6275463104248 and batch: 250, loss is 4.902674798965454 and perplexity is 134.64945866874962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.276258850097657 and perplexity of 195.6365987159672
Finished 35 epochs...
Completing Train Step...
At time: 254.6580696105957 and batch: 50, loss is 4.874239931106567 and perplexity is 130.87464165372958
At time: 255.96721386909485 and batch: 100, loss is 4.829802484512329 and perplexity is 125.18623199303694
At time: 257.28753542900085 and batch: 150, loss is 4.847320194244385 and perplexity is 127.39852867944148
At time: 258.5949730873108 and batch: 200, loss is 4.85297492980957 and perplexity is 128.12097436509575
At time: 259.8965563774109 and batch: 250, loss is 4.899938144683838 and perplexity is 134.28147340505217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.274377822875977 and perplexity of 195.26894683822474
Finished 36 epochs...
Completing Train Step...
At time: 261.8896062374115 and batch: 50, loss is 4.871287145614624 and perplexity is 130.4887668936561
At time: 263.19654631614685 and batch: 100, loss is 4.8265831089019775 and perplexity is 124.78385853418581
At time: 264.4955167770386 and batch: 150, loss is 4.844431600570679 and perplexity is 127.03105708902864
At time: 265.79806423187256 and batch: 200, loss is 4.8494594955444335 and perplexity is 127.67136425216225
At time: 267.0898542404175 and batch: 250, loss is 4.896336269378662 and perplexity is 133.79867828791885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.270772933959961 and perplexity of 194.56629123475437
Finished 37 epochs...
Completing Train Step...
At time: 269.0719816684723 and batch: 50, loss is 4.867532243728638 and perplexity is 129.99971312581374
At time: 270.3808624744415 and batch: 100, loss is 4.8213365936279295 and perplexity is 124.13089251033068
At time: 271.68242740631104 and batch: 150, loss is 4.839842453002929 and perplexity is 126.44942843299727
At time: 272.9863610267639 and batch: 200, loss is 4.84528265953064 and perplexity is 127.13921402579449
At time: 274.2815225124359 and batch: 250, loss is 4.8920308876037595 and perplexity is 133.22386218621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.267465209960937 and perplexity of 193.92378284984724
Finished 38 epochs...
Completing Train Step...
At time: 276.28671860694885 and batch: 50, loss is 4.86256932258606 and perplexity is 129.35613314034975
At time: 277.5890316963196 and batch: 100, loss is 4.815993204116821 and perplexity is 123.4693817301122
At time: 278.9081199169159 and batch: 150, loss is 4.834916276931763 and perplexity is 125.82804805560788
At time: 280.2207546234131 and batch: 200, loss is 4.839853115081787 and perplexity is 126.45077665396217
At time: 281.5228581428528 and batch: 250, loss is 4.887180461883545 and perplexity is 132.579234362031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.263712310791016 and perplexity of 193.19737037512994
Finished 39 epochs...
Completing Train Step...
At time: 283.5289857387543 and batch: 50, loss is 4.857695770263672 and perplexity is 128.72724296660914
At time: 284.8654136657715 and batch: 100, loss is 4.810642957687378 and perplexity is 122.81055412796324
At time: 286.1860373020172 and batch: 150, loss is 4.830454864501953 and perplexity is 125.26792763116796
At time: 287.505779504776 and batch: 200, loss is 4.834887943267822 and perplexity is 125.82448293648667
At time: 288.8156998157501 and batch: 250, loss is 4.881494827270508 and perplexity is 131.8275761300713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.258920288085937 and perplexity of 192.2737788928132
Finished 40 epochs...
Completing Train Step...
At time: 290.8494987487793 and batch: 50, loss is 4.8518996429443355 and perplexity is 127.98328160707412
At time: 292.15239572525024 and batch: 100, loss is 4.805214614868164 and perplexity is 122.14570249223291
At time: 293.45963978767395 and batch: 150, loss is 4.825199460983276 and perplexity is 124.61132100116333
At time: 294.7654504776001 and batch: 200, loss is 4.829878072738648 and perplexity is 125.19569495591264
At time: 296.0738191604614 and batch: 250, loss is 4.876922702789306 and perplexity is 131.22621982746452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.256252288818359 and perplexity of 191.7614763069049
Finished 41 epochs...
Completing Train Step...
At time: 298.0877158641815 and batch: 50, loss is 4.847758493423462 and perplexity is 127.45437958878706
At time: 299.3959894180298 and batch: 100, loss is 4.800831708908081 and perplexity is 121.611520853854
At time: 300.7089672088623 and batch: 150, loss is 4.822360162734985 and perplexity is 124.25801410490533
At time: 302.00664019584656 and batch: 200, loss is 4.826162548065185 and perplexity is 124.73139036402463
At time: 303.3046028614044 and batch: 250, loss is 4.872826757431031 and perplexity is 130.6898236760574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.252523040771484 and perplexity of 191.0476819812142
Finished 42 epochs...
Completing Train Step...
At time: 305.31015491485596 and batch: 50, loss is 4.8439609336853025 and perplexity is 126.97128184525988
At time: 306.6291275024414 and batch: 100, loss is 4.797113027572632 and perplexity is 121.16012617773688
At time: 307.94042682647705 and batch: 150, loss is 4.819584312438965 and perplexity is 123.9135707424454
At time: 309.25729990005493 and batch: 200, loss is 4.822320470809936 and perplexity is 124.25308216300253
At time: 310.56498670578003 and batch: 250, loss is 4.868849411010742 and perplexity is 130.171057314333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.248996353149414 and perplexity of 190.37510317072991
Finished 43 epochs...
Completing Train Step...
At time: 312.59199833869934 and batch: 50, loss is 4.839967947006226 and perplexity is 126.46529807373744
At time: 313.90070509910583 and batch: 100, loss is 4.793677959442139 and perplexity is 120.74464689797756
At time: 315.20366072654724 and batch: 150, loss is 4.816137609481811 and perplexity is 123.48721265865593
At time: 316.49988985061646 and batch: 200, loss is 4.817927303314209 and perplexity is 123.70841484458249
At time: 317.79997515678406 and batch: 250, loss is 4.866214656829834 and perplexity is 129.82853999944885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.246755218505859 and perplexity of 189.94892467182282
Finished 44 epochs...
Completing Train Step...
At time: 319.78436732292175 and batch: 50, loss is 4.837388210296631 and perplexity is 126.13947135607812
At time: 321.1032211780548 and batch: 100, loss is 4.790077848434448 and perplexity is 120.31073430097325
At time: 322.4071226119995 and batch: 150, loss is 4.812710514068604 and perplexity is 123.06473454846028
At time: 323.7101843357086 and batch: 200, loss is 4.814934587478637 and perplexity is 123.33874414806554
At time: 325.00862312316895 and batch: 250, loss is 4.863289136886596 and perplexity is 129.44927905470504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.244794464111328 and perplexity of 189.576846379316
Finished 45 epochs...
Completing Train Step...
At time: 327.0023536682129 and batch: 50, loss is 4.833977384567261 and perplexity is 125.70996450459491
At time: 328.32197427749634 and batch: 100, loss is 4.78772744178772 and perplexity is 120.028287214184
At time: 329.6322841644287 and batch: 150, loss is 4.810235385894775 and perplexity is 122.76051020920437
At time: 330.9354205131531 and batch: 200, loss is 4.813320941925049 and perplexity is 123.13987962356488
At time: 332.2324469089508 and batch: 250, loss is 4.860399780273437 and perplexity is 129.0757937501501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.23875732421875 and perplexity of 188.43579225468812
Finished 46 epochs...
Completing Train Step...
At time: 334.25200510025024 and batch: 50, loss is 4.829892349243164 and perplexity is 125.19748232557576
At time: 335.5590515136719 and batch: 100, loss is 4.784147205352784 and perplexity is 119.59932591671539
At time: 336.8658413887024 and batch: 150, loss is 4.806174297332763 and perplexity is 122.26297984655015
At time: 338.1688644886017 and batch: 200, loss is 4.810629673004151 and perplexity is 122.80892263949161
At time: 339.48257517814636 and batch: 250, loss is 4.858059511184693 and perplexity is 128.774074849346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.237447357177734 and perplexity of 188.18910918610737
Finished 47 epochs...
Completing Train Step...
At time: 341.4829716682434 and batch: 50, loss is 4.826347360610962 and perplexity is 124.75444442008981
At time: 342.7978367805481 and batch: 100, loss is 4.780126466751098 and perplexity is 119.11941373751375
At time: 344.09309792518616 and batch: 150, loss is 4.803597421646118 and perplexity is 121.94832892867119
At time: 345.39707136154175 and batch: 200, loss is 4.808840446472168 and perplexity is 122.58938611559488
At time: 346.7012026309967 and batch: 250, loss is 4.856025905609131 and perplexity is 128.51246526825418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.236107254028321 and perplexity of 187.93708527493732
Finished 48 epochs...
Completing Train Step...
At time: 348.70818567276 and batch: 50, loss is 4.823188810348511 and perplexity is 124.36102288485034
At time: 350.01508021354675 and batch: 100, loss is 4.777750787734985 and perplexity is 118.83676012595062
At time: 351.3129343986511 and batch: 150, loss is 4.80143515586853 and perplexity is 121.6849291032337
At time: 352.62071418762207 and batch: 200, loss is 4.806078805923462 and perplexity is 122.25130533971561
At time: 353.93399262428284 and batch: 250, loss is 4.853475093841553 and perplexity is 128.18507189651933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.234344863891602 and perplexity of 187.60615850621707
Finished 49 epochs...
Completing Train Step...
At time: 355.94237518310547 and batch: 50, loss is 4.820030393600464 and perplexity is 123.96885858252284
At time: 357.24450731277466 and batch: 100, loss is 4.774096851348877 and perplexity is 118.40333050847978
At time: 358.54475116729736 and batch: 150, loss is 4.797665920257568 and perplexity is 121.2271332473589
At time: 359.84745025634766 and batch: 200, loss is 4.801713066101074 and perplexity is 121.71875128972606
At time: 361.1524670124054 and batch: 250, loss is 4.848262853622437 and perplexity is 127.51867871863462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.22841796875 and perplexity of 186.49752509915456
Finished Training.
Improved accuracyfrom -10000000 to -186.49752509915456
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff016f418d0>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 3.698876580509711, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.9862625980444063, 'lr': 1.1554307956415455}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5263750553131104 and batch: 50, loss is 10.04514663696289 and perplexity is 23043.67568357525
At time: 2.8401529788970947 and batch: 100, loss is 9.615200061798095 and perplexity is 14990.921474136305
At time: 4.155184507369995 and batch: 150, loss is 9.177502689361573 and perplexity is 9676.956206648152
At time: 5.464169979095459 and batch: 200, loss is 8.767100696563721 and perplexity is 6419.533282193193
At time: 6.7700018882751465 and batch: 250, loss is 8.399862041473389 and perplexity is 4446.453279241177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 7.937174224853516 and perplexity of 2799.438736593103
Finished 1 epochs...
Completing Train Step...
At time: 8.771008253097534 and batch: 50, loss is 6.557948169708252 and perplexity is 704.8240306264722
At time: 10.07010817527771 and batch: 100, loss is 5.867581796646118 and perplexity is 353.39336913337706
At time: 11.370679378509521 and batch: 150, loss is 5.677606706619263 and perplexity is 292.2491543077791
At time: 12.667487859725952 and batch: 200, loss is 5.5507934951782225 and perplexity is 257.4417536703004
At time: 13.963134288787842 and batch: 250, loss is 5.4920042133331295 and perplexity is 242.74322886518843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.685510635375977 and perplexity of 294.5682236088268
Finished 2 epochs...
Completing Train Step...
At time: 15.98731017112732 and batch: 50, loss is 5.3857035827636714 and perplexity is 218.26361661534682
At time: 17.29118275642395 and batch: 100, loss is 5.284929056167602 and perplexity is 197.34018288267643
At time: 18.58856749534607 and batch: 150, loss is 5.239066038131714 and perplexity is 188.49397398575036
At time: 19.885392665863037 and batch: 200, loss is 5.1790003490448 and perplexity is 177.50527895145387
At time: 21.1891508102417 and batch: 250, loss is 5.180769853591919 and perplexity is 177.81965341118473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.371064758300781 and perplexity of 215.09176656560905
Finished 3 epochs...
Completing Train Step...
At time: 23.179787397384644 and batch: 50, loss is 5.102836322784424 and perplexity is 164.48778675265174
At time: 24.50817036628723 and batch: 100, loss is 5.02093581199646 and perplexity is 151.55306262800622
At time: 25.793689250946045 and batch: 150, loss is 5.011659154891968 and perplexity is 150.1536577680159
At time: 27.095889568328857 and batch: 200, loss is 4.9688201427459715 and perplexity is 143.8570564109262
At time: 28.40445852279663 and batch: 250, loss is 4.992874603271485 and perplexity is 147.35941509461577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1931415557861325 and perplexity of 180.0332499518859
Finished 4 epochs...
Completing Train Step...
At time: 30.40965962409973 and batch: 50, loss is 4.928079957962036 and perplexity is 138.11407276818977
At time: 31.706825017929077 and batch: 100, loss is 4.849639530181885 and perplexity is 127.69435158913466
At time: 32.99862623214722 and batch: 150, loss is 4.863132781982422 and perplexity is 129.42904060731453
At time: 34.28992438316345 and batch: 200, loss is 4.828943090438843 and perplexity is 125.07869390259746
At time: 35.601078510284424 and batch: 250, loss is 4.861386919021607 and perplexity is 129.20327237680587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.080776977539062 and perplexity of 160.8990223006731
Finished 5 epochs...
Completing Train Step...
At time: 37.67652773857117 and batch: 50, loss is 4.804546766281128 and perplexity is 122.06415489116701
At time: 38.98474645614624 and batch: 100, loss is 4.726280889511108 and perplexity is 112.87498620373646
At time: 40.281588554382324 and batch: 150, loss is 4.751928777694702 and perplexity is 115.80743605270388
At time: 41.584108114242554 and batch: 200, loss is 4.723411874771118 and perplexity is 112.551610311458
At time: 42.88674736022949 and batch: 250, loss is 4.760867481231689 and perplexity is 116.84724473832551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.994723510742188 and perplexity of 147.63212104441038
Finished 6 epochs...
Completing Train Step...
At time: 44.91375970840454 and batch: 50, loss is 4.70933723449707 and perplexity is 110.97858274998144
At time: 46.24294972419739 and batch: 100, loss is 4.629334478378296 and perplexity is 102.44586148197853
At time: 47.54475402832031 and batch: 150, loss is 4.665606384277344 and perplexity is 106.22998188623788
At time: 48.858936071395874 and batch: 200, loss is 4.642251882553101 and perplexity is 103.77778002565402
At time: 50.15493440628052 and batch: 250, loss is 4.682696657180786 and perplexity is 108.06108371376563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.927207946777344 and perplexity of 137.99368824792495
Finished 7 epochs...
Completing Train Step...
At time: 52.26128172874451 and batch: 50, loss is 4.63249945640564 and perplexity is 102.77061402880932
At time: 53.562931060791016 and batch: 100, loss is 4.551317195892334 and perplexity is 94.75713986434633
At time: 54.863956689834595 and batch: 150, loss is 4.594800062179566 and perplexity is 98.96834605399769
At time: 56.16651487350464 and batch: 200, loss is 4.573847303390503 and perplexity is 96.91625968147056
At time: 57.468870401382446 and batch: 250, loss is 4.61645097732544 and perplexity is 101.1344659398768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.883617401123047 and perplexity of 132.1076870691203
Finished 8 epochs...
Completing Train Step...
At time: 59.534773111343384 and batch: 50, loss is 4.568669080734253 and perplexity is 96.41570282574787
At time: 60.848572969436646 and batch: 100, loss is 4.486328582763672 and perplexity is 88.79484377928584
At time: 62.145285844802856 and batch: 150, loss is 4.535365781784058 and perplexity is 93.25762100540983
At time: 63.44850039482117 and batch: 200, loss is 4.514948492050171 and perplexity is 91.37285947251547
At time: 64.75240397453308 and batch: 250, loss is 4.56014536857605 and perplexity is 95.59737567440817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.844503784179688 and perplexity of 127.04022698013951
Finished 9 epochs...
Completing Train Step...
At time: 66.7812716960907 and batch: 50, loss is 4.514171380996704 and perplexity is 91.30188019639455
At time: 68.07502627372742 and batch: 100, loss is 4.430687217712403 and perplexity is 83.98911589020634
At time: 69.37564826011658 and batch: 150, loss is 4.483780727386475 and perplexity is 88.5688953233196
At time: 70.67747139930725 and batch: 200, loss is 4.463858919143677 and perplexity is 86.8219021838526
At time: 71.98033499717712 and batch: 250, loss is 4.510731058120728 and perplexity is 90.98831194662479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.80932502746582 and perplexity of 122.64880499759772
Finished 10 epochs...
Completing Train Step...
At time: 74.01328349113464 and batch: 50, loss is 4.465875444412231 and perplexity is 86.99715738735568
At time: 75.31572556495667 and batch: 100, loss is 4.381776285171509 and perplexity is 79.97997454659593
At time: 76.61600279808044 and batch: 150, loss is 4.437955093383789 and perplexity is 84.60176196322628
At time: 77.91287326812744 and batch: 200, loss is 4.419055185317993 and perplexity is 83.01781184521826
At time: 79.21946406364441 and batch: 250, loss is 4.466855611801147 and perplexity is 87.08247096791966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.780760192871094 and perplexity of 119.19492674615357
Finished 11 epochs...
Completing Train Step...
At time: 81.22858452796936 and batch: 50, loss is 4.4223779296875 and perplexity is 83.29411760458309
At time: 82.54123544692993 and batch: 100, loss is 4.337550659179687 and perplexity is 76.51988633786064
At time: 83.83864259719849 and batch: 150, loss is 4.396630783081054 and perplexity is 81.17690480120292
At time: 85.13389420509338 and batch: 200, loss is 4.378870820999145 and perplexity is 79.74793285369277
At time: 86.44318723678589 and batch: 250, loss is 4.427680797576905 and perplexity is 83.73698851135131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7569843292236325 and perplexity of 116.39438894826522
Finished 12 epochs...
Completing Train Step...
At time: 88.45769691467285 and batch: 50, loss is 4.382835235595703 and perplexity is 80.06471423421439
At time: 89.75890302658081 and batch: 100, loss is 4.2973819065094 and perplexity is 73.50709311359799
At time: 91.05212187767029 and batch: 150, loss is 4.359245853424072 and perplexity is 78.1981393213902
At time: 92.35397696495056 and batch: 200, loss is 4.342355260848999 and perplexity is 76.88841852774355
At time: 93.65677738189697 and batch: 250, loss is 4.3921643733978275 and perplexity is 80.81514397496457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.735613250732422 and perplexity of 113.93330698272584
Finished 13 epochs...
Completing Train Step...
At time: 95.67412900924683 and batch: 50, loss is 4.34662088394165 and perplexity is 77.21709605021594
At time: 96.99525475502014 and batch: 100, loss is 4.260361938476563 and perplexity is 70.83561695040788
At time: 98.29818606376648 and batch: 150, loss is 4.325201711654663 and perplexity is 75.58075684175374
At time: 99.59827065467834 and batch: 200, loss is 4.3090245819091795 and perplexity is 74.36791374207843
At time: 100.90454983711243 and batch: 250, loss is 4.359561700820922 and perplexity is 78.22284190105077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.716105270385742 and perplexity of 111.73223728453377
Finished 14 epochs...
Completing Train Step...
At time: 102.90412259101868 and batch: 50, loss is 4.313364706039429 and perplexity is 74.69138115563607
At time: 104.21457290649414 and batch: 100, loss is 4.226533222198486 and perplexity is 68.47941726267462
At time: 105.51061058044434 and batch: 150, loss is 4.293721494674682 and perplexity is 73.23851872633911
At time: 106.8093945980072 and batch: 200, loss is 4.278410863876343 and perplexity is 72.12573128254996
At time: 108.11342549324036 and batch: 250, loss is 4.329307270050049 and perplexity is 75.89169590502034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.698785018920899 and perplexity of 109.81366983589265
Finished 15 epochs...
Completing Train Step...
At time: 110.12892746925354 and batch: 50, loss is 4.282450723648071 and perplexity is 72.41769847904406
At time: 111.42780709266663 and batch: 100, loss is 4.1954415416717525 and perplexity is 66.38303598394477
At time: 112.73178267478943 and batch: 150, loss is 4.264394340515136 and perplexity is 71.12183131446817
At time: 114.03141975402832 and batch: 200, loss is 4.2503342628479 and perplexity is 70.12884989840526
At time: 115.3424642086029 and batch: 250, loss is 4.301140441894531 and perplexity is 73.78389197743385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.68329963684082 and perplexity of 108.12626199790121
Finished 16 epochs...
Completing Train Step...
At time: 117.31446814537048 and batch: 50, loss is 4.253775262832642 and perplexity is 70.37057892611033
At time: 118.62930679321289 and batch: 100, loss is 4.166522145271301 and perplexity is 64.49077209496085
At time: 119.9313714504242 and batch: 150, loss is 4.236866006851196 and perplexity is 69.19066860975806
At time: 121.2378180027008 and batch: 200, loss is 4.22432207107544 and perplexity is 68.32816620338807
At time: 122.54231286048889 and batch: 250, loss is 4.274880971908569 and perplexity is 71.87158406303267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.668955993652344 and perplexity of 106.58640743940296
Finished 17 epochs...
Completing Train Step...
At time: 124.56013822555542 and batch: 50, loss is 4.226909093856811 and perplexity is 68.50516157277744
At time: 125.85597372055054 and batch: 100, loss is 4.139490427970887 and perplexity is 62.77082704046197
At time: 127.1610815525055 and batch: 150, loss is 4.211147003173828 and perplexity is 67.43384229968225
At time: 128.47450995445251 and batch: 200, loss is 4.199938678741455 and perplexity is 66.68224187655562
At time: 129.7848973274231 and batch: 250, loss is 4.25008264541626 and perplexity is 70.11120647709919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.655909729003906 and perplexity of 105.20488441265883
Finished 18 epochs...
Completing Train Step...
At time: 131.8236494064331 and batch: 50, loss is 4.201869745254516 and perplexity is 66.81113413056356
At time: 133.12154006958008 and batch: 100, loss is 4.1141688346862795 and perplexity is 61.20132469861815
At time: 134.43359112739563 and batch: 150, loss is 4.187015528678894 and perplexity is 65.82604157673407
At time: 135.73850798606873 and batch: 200, loss is 4.177018814086914 and perplexity is 65.17127563329127
At time: 137.0400595664978 and batch: 250, loss is 4.22663990020752 and perplexity is 68.48672290023647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.64442253112793 and perplexity of 104.00328977863278
Finished 19 epochs...
Completing Train Step...
At time: 139.05090379714966 and batch: 50, loss is 4.1783623790740965 and perplexity is 65.25889632626898
At time: 140.37923908233643 and batch: 100, loss is 4.090351839065551 and perplexity is 59.760914229874565
At time: 141.6804597377777 and batch: 150, loss is 4.164282221794128 and perplexity is 64.34647936315257
At time: 142.98321771621704 and batch: 200, loss is 4.155333127975464 and perplexity is 63.773205648500564
At time: 144.29086828231812 and batch: 250, loss is 4.204382967948914 and perplexity is 66.9792565651655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6343849182128904 and perplexity of 102.96456688436113
Finished 20 epochs...
Completing Train Step...
At time: 146.31117272377014 and batch: 50, loss is 4.15641658782959 and perplexity is 63.8423388013246
At time: 147.610515832901 and batch: 100, loss is 4.0679930353164675 and perplexity is 58.439558688147436
At time: 148.932302236557 and batch: 150, loss is 4.1428516530990604 and perplexity is 62.982168906446496
At time: 150.23364901542664 and batch: 200, loss is 4.134720311164856 and perplexity is 62.472115873453085
At time: 151.5380003452301 and batch: 250, loss is 4.183405613899231 and perplexity is 65.58884356624814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.625456619262695 and perplexity of 102.04936014920409
Finished 21 epochs...
Completing Train Step...
At time: 153.57896208763123 and batch: 50, loss is 4.13554151058197 and perplexity is 62.523439008974506
At time: 154.95441937446594 and batch: 100, loss is 4.046950445175171 and perplexity is 57.22268697428088
At time: 156.2670919895172 and batch: 150, loss is 4.12272385597229 and perplexity is 61.72714934766184
At time: 157.5711851119995 and batch: 200, loss is 4.115199904441834 and perplexity is 61.26446007641075
At time: 158.8716094493866 and batch: 250, loss is 4.1634616899490355 and perplexity is 64.29370268314992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618400192260742 and perplexity of 101.33179100336402
Finished 22 epochs...
Completing Train Step...
At time: 160.89784598350525 and batch: 50, loss is 4.1158743095397945 and perplexity is 61.305791075961956
At time: 162.21541786193848 and batch: 100, loss is 4.027106165885925 and perplexity is 56.09833684799667
At time: 163.51761054992676 and batch: 150, loss is 4.103767795562744 and perplexity is 60.568066307565275
At time: 164.8191361427307 and batch: 200, loss is 4.096712846755981 and perplexity is 60.14226546809218
At time: 166.11410236358643 and batch: 250, loss is 4.144431238174438 and perplexity is 63.08173321491208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612565612792968 and perplexity of 100.74228405107068
Finished 23 epochs...
Completing Train Step...
At time: 168.1372001171112 and batch: 50, loss is 4.09721752166748 and perplexity is 60.17262542090353
At time: 169.43981432914734 and batch: 100, loss is 4.008255982398987 and perplexity is 55.05077726911512
At time: 170.75813007354736 and batch: 150, loss is 4.085946764945984 and perplexity is 59.49824194344009
At time: 172.0550479888916 and batch: 200, loss is 4.079110131263733 and perplexity is 59.09286155608057
At time: 173.35581469535828 and batch: 250, loss is 4.126208815574646 and perplexity is 61.942641241685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.606844329833985 and perplexity of 100.16755460070657
Finished 24 epochs...
Completing Train Step...
At time: 175.38639569282532 and batch: 50, loss is 4.079518113136292 and perplexity is 59.116975291041946
At time: 176.7033247947693 and batch: 100, loss is 3.990172209739685 and perplexity is 54.064198943329714
At time: 178.01294231414795 and batch: 150, loss is 4.069169187545777 and perplexity is 58.508332941946165
At time: 179.32061743736267 and batch: 200, loss is 4.0622543478012085 and perplexity is 58.105152765588635
At time: 180.62249040603638 and batch: 250, loss is 4.108692841529846 and perplexity is 60.86710259755182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.601741027832031 and perplexity of 99.6576714691877
Finished 25 epochs...
Completing Train Step...
At time: 182.62161993980408 and batch: 50, loss is 4.062649598121643 and perplexity is 58.128123385111465
At time: 183.92621231079102 and batch: 100, loss is 3.9728137350082395 and perplexity is 53.133825207891846
At time: 185.22595810890198 and batch: 150, loss is 4.053191618919373 and perplexity is 57.580940504161994
At time: 186.51816582679749 and batch: 200, loss is 4.045989556312561 and perplexity is 57.16772874028475
At time: 187.80828619003296 and batch: 250, loss is 4.0918640661239625 and perplexity is 59.85135466741162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.596172332763672 and perplexity of 99.10425063164517
Finished 26 epochs...
Completing Train Step...
At time: 189.81149244308472 and batch: 50, loss is 4.046489577293396 and perplexity is 57.196320951838715
At time: 191.11995792388916 and batch: 100, loss is 3.956215949058533 and perplexity is 52.2591998514922
At time: 192.4261314868927 and batch: 150, loss is 4.037913994789124 and perplexity is 56.70792630991512
At time: 193.72736835479736 and batch: 200, loss is 4.030378198623657 and perplexity is 56.28219307045166
At time: 195.02274298667908 and batch: 250, loss is 4.07565438747406 and perplexity is 58.8890042089995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.591160583496094 and perplexity of 98.60880753145469
Finished 27 epochs...
Completing Train Step...
At time: 197.0614993572235 and batch: 50, loss is 4.030957913398742 and perplexity is 56.31483014853298
At time: 198.3839077949524 and batch: 100, loss is 3.940402669906616 and perplexity is 51.43931019305503
At time: 199.69118285179138 and batch: 150, loss is 4.023362913131714 and perplexity is 55.88873912744528
At time: 200.99618434906006 and batch: 200, loss is 4.015267434120179 and perplexity is 55.43811946487892
At time: 202.29131817817688 and batch: 250, loss is 4.06009886264801 and perplexity is 57.980042856193776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.586469268798828 and perplexity of 98.14728600121832
Finished 28 epochs...
Completing Train Step...
At time: 204.36252641677856 and batch: 50, loss is 4.015969519615173 and perplexity is 55.47705543101414
At time: 205.6680450439453 and batch: 100, loss is 3.925323190689087 and perplexity is 50.669451307739116
At time: 206.9792604446411 and batch: 150, loss is 4.0093896484375 and perplexity is 55.113221854664
At time: 208.27562022209167 and batch: 200, loss is 4.000855388641358 and perplexity is 54.64487265067103
At time: 209.58003997802734 and batch: 250, loss is 4.045074796676635 and perplexity is 57.11545792081844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.582435989379883 and perplexity of 97.75222779816961
Finished 29 epochs...
Completing Train Step...
At time: 211.57286405563354 and batch: 50, loss is 4.001489839553833 and perplexity is 54.67955314035718
At time: 212.89283633232117 and batch: 100, loss is 3.9108980560302733 and perplexity is 49.94378415602476
At time: 214.19697999954224 and batch: 150, loss is 3.9958279132843018 and perplexity is 54.37083633264858
At time: 215.50823879241943 and batch: 200, loss is 3.986844639778137 and perplexity is 53.88459552612365
At time: 216.80538177490234 and batch: 250, loss is 4.030748553276062 and perplexity is 56.303041302884516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.578388595581055 and perplexity of 97.35738561745991
Finished 30 epochs...
Completing Train Step...
At time: 218.78565859794617 and batch: 50, loss is 3.987614464759827 and perplexity is 53.92609320481337
At time: 220.12133860588074 and batch: 100, loss is 3.897035298347473 and perplexity is 49.25620247895489
At time: 221.4171621799469 and batch: 150, loss is 3.9826214599609373 and perplexity is 53.65751103666499
At time: 222.7093596458435 and batch: 200, loss is 3.9733804941177366 and perplexity is 53.16394782267769
At time: 224.00656747817993 and batch: 250, loss is 4.016893701553345 and perplexity is 55.52835002274309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.575350570678711 and perplexity of 97.06206028552769
Finished 31 epochs...
Completing Train Step...
At time: 226.00638365745544 and batch: 50, loss is 3.97421950340271 and perplexity is 53.208571585784846
At time: 227.30519151687622 and batch: 100, loss is 3.8836792659759523 and perplexity is 48.60270879986478
At time: 228.5964252948761 and batch: 150, loss is 3.969641351699829 and perplexity is 52.9655314350109
At time: 229.8921492099762 and batch: 200, loss is 3.9602341985702516 and perplexity is 52.46961281856302
At time: 231.18196558952332 and batch: 250, loss is 4.003630747795105 and perplexity is 54.79674244731967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.572415161132812 and perplexity of 96.77756115224207
Finished 32 epochs...
Completing Train Step...
At time: 233.16858458518982 and batch: 50, loss is 3.961266851425171 and perplexity is 52.52382369972982
At time: 234.4771203994751 and batch: 100, loss is 3.8708104848861695 and perplexity is 47.98125841073471
At time: 235.77623200416565 and batch: 150, loss is 3.957026047706604 and perplexity is 52.30155211107911
At time: 237.0764513015747 and batch: 200, loss is 3.9476566982269286 and perplexity is 51.81380907652099
At time: 238.37977027893066 and batch: 250, loss is 3.990922756195068 and perplexity is 54.10479186774196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5701133728027346 and perplexity of 96.55505586964023
Finished 33 epochs...
Completing Train Step...
At time: 240.42652916908264 and batch: 50, loss is 3.9487576150894164 and perplexity is 51.87088318380204
At time: 241.74122047424316 and batch: 100, loss is 3.8583484411239626 and perplexity is 47.387024245228254
At time: 243.04494047164917 and batch: 150, loss is 3.9447467708587647 and perplexity is 51.66325381411741
At time: 244.33832383155823 and batch: 200, loss is 3.935580024719238 and perplexity is 51.19183387619293
At time: 245.63436722755432 and batch: 250, loss is 3.9787201118469238 and perplexity is 53.448582224194006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.568086242675781 and perplexity of 96.35952445774248
Finished 34 epochs...
Completing Train Step...
At time: 247.6463496685028 and batch: 50, loss is 3.9366461515426634 and perplexity is 51.24643996676539
At time: 248.94726371765137 and batch: 100, loss is 3.8462146759033202 and perplexity is 46.815515506794426
At time: 250.2415735721588 and batch: 150, loss is 3.9327876377105713 and perplexity is 51.04908586093445
At time: 251.53655648231506 and batch: 200, loss is 3.923940134048462 and perplexity is 50.599421025717525
At time: 252.82523655891418 and batch: 250, loss is 3.9670202112197877 and perplexity is 52.82688312425012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5663200378417965 and perplexity of 96.1894840071808
Finished 35 epochs...
Completing Train Step...
At time: 254.78530740737915 and batch: 50, loss is 3.9248666858673094 and perplexity is 50.64632573776902
At time: 256.0932717323303 and batch: 100, loss is 3.8343827533721924 and perplexity is 46.26486202276376
At time: 257.37977504730225 and batch: 150, loss is 3.921244263648987 and perplexity is 50.46319525036561
At time: 258.6688754558563 and batch: 200, loss is 3.912675166130066 and perplexity is 50.03261867024733
At time: 259.9654633998871 and batch: 250, loss is 3.955751419067383 and perplexity is 52.23492952343175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.564934921264649 and perplexity of 96.05634258781521
Finished 36 epochs...
Completing Train Step...
At time: 261.9639947414398 and batch: 50, loss is 3.913408217430115 and perplexity is 50.069308592563054
At time: 263.26662039756775 and batch: 100, loss is 3.822914133071899 and perplexity is 45.73729888014887
At time: 264.5633125305176 and batch: 150, loss is 3.910008888244629 and perplexity is 49.89939548947076
At time: 265.85238099098206 and batch: 200, loss is 3.9017856979370116 and perplexity is 49.49074576910348
At time: 267.1531000137329 and batch: 250, loss is 3.945023527145386 and perplexity is 51.67755392312904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.563908386230469 and perplexity of 95.95778798042677
Finished 37 epochs...
Completing Train Step...
At time: 269.14169669151306 and batch: 50, loss is 3.902197685241699 and perplexity is 49.51113952875644
At time: 270.4548816680908 and batch: 100, loss is 3.8117876434326172 and perplexity is 45.231223937775574
At time: 271.7431631088257 and batch: 150, loss is 3.8991368865966796 and perplexity is 49.35982758579877
At time: 273.0461654663086 and batch: 200, loss is 3.891222071647644 and perplexity is 48.9706956695008
At time: 274.33786821365356 and batch: 250, loss is 3.9348334455490113 and perplexity is 51.15362938244941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5630744934082035 and perplexity of 95.87780282394837
Finished 38 epochs...
Completing Train Step...
At time: 276.32196950912476 and batch: 50, loss is 3.8912713050842287 and perplexity is 48.97310672449231
At time: 277.63042640686035 and batch: 100, loss is 3.8010283422470095 and perplexity is 44.74717625338863
At time: 278.92091131210327 and batch: 150, loss is 3.888586792945862 and perplexity is 48.84181413216359
At time: 280.22734451293945 and batch: 200, loss is 3.881043305397034 and perplexity is 48.47476267998766
At time: 281.52269291877747 and batch: 250, loss is 3.9251301431655885 and perplexity is 50.65967063974438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.562269592285157 and perplexity of 95.8006617224245
Finished 39 epochs...
Completing Train Step...
At time: 283.5195517539978 and batch: 50, loss is 3.8805363845825194 and perplexity is 48.45019604100292
At time: 284.82203435897827 and batch: 100, loss is 3.7906100797653197 and perplexity is 44.28340844800957
At time: 286.1111636161804 and batch: 150, loss is 3.87834246635437 and perplexity is 48.34401678963785
At time: 287.3975863456726 and batch: 200, loss is 3.871176686286926 and perplexity is 47.99883243239407
At time: 288.69737005233765 and batch: 250, loss is 3.9158327341079713 and perplexity is 50.19084974603353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.561318206787109 and perplexity of 95.70956170464781
Finished 40 epochs...
Completing Train Step...
At time: 290.67847895622253 and batch: 50, loss is 3.8700051593780516 and perplexity is 47.942633434351826
At time: 291.9855086803436 and batch: 100, loss is 3.780494213104248 and perplexity is 43.83770154941703
At time: 293.2771580219269 and batch: 150, loss is 3.8683904504776 and perplexity is 47.86528250385858
At time: 294.57786750793457 and batch: 200, loss is 3.861727013587952 and perplexity is 47.54739550089796
At time: 295.885897397995 and batch: 250, loss is 3.906826863288879 and perplexity is 49.74086672275475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.56060562133789 and perplexity of 95.64138475746113
Finished 41 epochs...
Completing Train Step...
At time: 297.9572868347168 and batch: 50, loss is 3.859755988121033 and perplexity is 47.45377067225141
At time: 299.2556631565094 and batch: 100, loss is 3.7706487369537354 and perplexity is 43.40821621739951
At time: 300.55756068229675 and batch: 150, loss is 3.8587009620666506 and perplexity is 47.40373210844918
At time: 301.8634467124939 and batch: 200, loss is 3.852504906654358 and perplexity is 47.11092402188868
At time: 303.1601240634918 and batch: 250, loss is 3.898173279762268 and perplexity is 49.31228702747377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.560063171386719 and perplexity of 95.58951816175741
Finished 42 epochs...
Completing Train Step...
At time: 305.1475341320038 and batch: 50, loss is 3.849711093902588 and perplexity is 46.979488610065495
At time: 306.43524622917175 and batch: 100, loss is 3.76107141494751 and perplexity is 42.99446622402793
At time: 307.728586435318 and batch: 150, loss is 3.849223804473877 and perplexity is 46.956601578656674
At time: 309.03305220603943 and batch: 200, loss is 3.8435765218734743 and perplexity is 46.692171737369954
At time: 310.3316776752472 and batch: 250, loss is 3.889673070907593 and perplexity is 48.89489874556916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.559665298461914 and perplexity of 95.55149324562963
Finished 43 epochs...
Completing Train Step...
At time: 312.3612461090088 and batch: 50, loss is 3.8398860692977905 and perplexity is 46.520174061759285
At time: 313.6611750125885 and batch: 100, loss is 3.751781768798828 and perplexity is 42.59691227320872
At time: 314.967547416687 and batch: 150, loss is 3.840032296180725 and perplexity is 46.52697705918435
At time: 316.25798082351685 and batch: 200, loss is 3.8347057628631593 and perplexity is 46.2798084260805
At time: 317.55736446380615 and batch: 250, loss is 3.8813223886489867 and perplexity is 48.48829306235777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.559606552124023 and perplexity of 95.5458801101987
Finished 44 epochs...
Completing Train Step...
At time: 319.54280495643616 and batch: 50, loss is 3.8303949451446533 and perplexity is 46.08073400296923
At time: 320.83136510849 and batch: 100, loss is 3.742764320373535 and perplexity is 42.21452349041058
At time: 322.1259582042694 and batch: 150, loss is 3.8310318660736082 and perplexity is 46.11009313560783
At time: 323.4237811565399 and batch: 200, loss is 3.8260531282424926 and perplexity is 45.881093607856634
At time: 324.7239809036255 and batch: 250, loss is 3.8730467319488526 and perplexity is 48.08867642075958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.559732818603516 and perplexity of 95.55794511379696
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 326.70464873313904 and batch: 50, loss is 3.825674467086792 and perplexity is 45.86372350882473
At time: 328.0171649456024 and batch: 100, loss is 3.7284481954574584 and perplexity is 41.61448049237764
At time: 329.32406282424927 and batch: 150, loss is 3.807048087120056 and perplexity is 45.01735522658778
At time: 330.6350119113922 and batch: 200, loss is 3.791827082633972 and perplexity is 44.337334290421175
At time: 331.94326090812683 and batch: 250, loss is 3.8294404458999636 and perplexity is 46.0367709618512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.529273223876953 and perplexity of 92.69117086647523
Finished 46 epochs...
Completing Train Step...
At time: 333.9123237133026 and batch: 50, loss is 3.810506658554077 and perplexity is 45.1733205184936
At time: 335.2117705345154 and batch: 100, loss is 3.7175611543655394 and perplexity is 41.163879241124825
At time: 336.50881814956665 and batch: 150, loss is 3.799276270866394 and perplexity is 44.66884464783209
At time: 337.8001160621643 and batch: 200, loss is 3.787824125289917 and perplexity is 44.16020858235137
At time: 339.1004567146301 and batch: 250, loss is 3.829282784461975 and perplexity is 46.02951331048191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5285797119140625 and perplexity of 92.62691071579214
Finished 47 epochs...
Completing Train Step...
At time: 341.0937373638153 and batch: 50, loss is 3.8041846227645872 and perplexity is 44.88863401677037
At time: 342.39218378067017 and batch: 100, loss is 3.7121852350234987 and perplexity is 40.94317931055989
At time: 343.6857314109802 and batch: 150, loss is 3.7949245452880858 and perplexity is 44.47488043962762
At time: 344.99071741104126 and batch: 200, loss is 3.785034351348877 and perplexity is 44.037183269414
At time: 346.30787086486816 and batch: 250, loss is 3.8280704021453857 and perplexity is 45.97374175755675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.528339004516601 and perplexity of 92.60461741636753
Finished 48 epochs...
Completing Train Step...
At time: 348.2858986854553 and batch: 50, loss is 3.799319314956665 and perplexity is 44.67076741899508
At time: 349.59242129325867 and batch: 100, loss is 3.708020706176758 and perplexity is 40.77302481182296
At time: 350.88183093070984 and batch: 150, loss is 3.7914125537872314 and perplexity is 44.318958995178306
At time: 352.1817674636841 and batch: 200, loss is 3.782476906776428 and perplexity is 43.92470450448496
At time: 353.47442388534546 and batch: 250, loss is 3.826458616256714 and perplexity is 45.899701613800715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.528238296508789 and perplexity of 92.59529185942034
Finished 49 epochs...
Completing Train Step...
At time: 355.46358036994934 and batch: 50, loss is 3.7951452255249025 and perplexity is 44.484696249813254
At time: 356.7547011375427 and batch: 100, loss is 3.7044091606140137 and perplexity is 40.62603676177106
At time: 358.0524482727051 and batch: 150, loss is 3.788266558647156 and perplexity is 44.17975085444857
At time: 359.35256481170654 and batch: 200, loss is 3.780010061264038 and perplexity is 43.816482582556716
At time: 360.6539797782898 and batch: 250, loss is 3.8246675062179567 and perplexity is 45.817563778373774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.528183364868164 and perplexity of 92.59020558782429
Finished Training.
Improved accuracyfrom -186.49752509915456 to -92.59020558782429
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff00df36860>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 3.6234654834508007, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.4370688770724037, 'lr': 25.868149321566158}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5474953651428223 and batch: 50, loss is 6.5084818172454835 and perplexity is 670.8072361574742
At time: 2.8920159339904785 and batch: 100, loss is 5.765575141906738 and perplexity is 319.1225320069202
At time: 4.214717864990234 and batch: 150, loss is 5.791487026214599 and perplexity is 327.4996630482917
At time: 5.537372827529907 and batch: 200, loss is 5.866320934295654 and perplexity is 352.94806952907976
At time: 6.862811803817749 and batch: 250, loss is 5.962484827041626 and perplexity is 388.57446597112255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.091215896606445 and perplexity of 441.95846037379744
Finished 1 epochs...
Completing Train Step...
At time: 8.926624298095703 and batch: 50, loss is 5.815018882751465 and perplexity is 335.2977297850288
At time: 10.230488538742065 and batch: 100, loss is 5.818061695098877 and perplexity is 336.3195316445552
At time: 11.534157752990723 and batch: 150, loss is 5.813642005920411 and perplexity is 334.83638379057265
At time: 12.841420888900757 and batch: 200, loss is 5.842711935043335 and perplexity is 344.7129132279946
At time: 14.154478549957275 and batch: 250, loss is 5.9257128715515135 and perplexity is 374.5453429332898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.158683013916016 and perplexity of 472.8049870375486
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.218647003173828 and batch: 50, loss is 5.746558341979981 and perplexity is 313.1111821427464
At time: 17.53001356124878 and batch: 100, loss is 5.543291139602661 and perplexity is 255.51756110797962
At time: 18.839900016784668 and batch: 150, loss is 5.441187152862549 and perplexity is 230.71591600742926
At time: 20.150171995162964 and batch: 200, loss is 5.390284957885743 and perplexity is 219.2658581878617
At time: 21.463574171066284 and batch: 250, loss is 5.45907000541687 and perplexity is 234.87888666925878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.718415451049805 and perplexity of 304.42216875616697
Finished 3 epochs...
Completing Train Step...
At time: 23.531853914260864 and batch: 50, loss is 5.426356906890869 and perplexity is 227.3195886561823
At time: 24.867701053619385 and batch: 100, loss is 5.370844326019287 and perplexity is 215.044358622087
At time: 26.177940607070923 and batch: 150, loss is 5.372436151504517 and perplexity is 215.3869443085698
At time: 27.486823081970215 and batch: 200, loss is 5.384353408813476 and perplexity is 217.9691216203907
At time: 28.79886221885681 and batch: 250, loss is 5.405714654922486 and perplexity is 222.67529963269533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.676796340942383 and perplexity of 292.01242155703403
Finished 4 epochs...
Completing Train Step...
At time: 30.898505210876465 and batch: 50, loss is 5.377681484222412 and perplexity is 216.51968870804166
At time: 32.19677138328552 and batch: 100, loss is 5.36432183265686 and perplexity is 213.6462975986828
At time: 33.50080156326294 and batch: 150, loss is 5.3582923698425295 and perplexity is 212.36200089232852
At time: 34.80203557014465 and batch: 200, loss is 5.344321842193604 and perplexity is 209.415819460274
At time: 36.09989309310913 and batch: 250, loss is 5.397273387908935 and perplexity is 220.80354905864655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.666176605224609 and perplexity of 288.9277350819055
Finished 5 epochs...
Completing Train Step...
At time: 38.0982551574707 and batch: 50, loss is 5.371432476043701 and perplexity is 215.1708741682744
At time: 39.408215045928955 and batch: 100, loss is 5.333782148361206 and perplexity is 207.22023157627098
At time: 40.70427489280701 and batch: 150, loss is 5.342821455001831 and perplexity is 209.10185024359646
At time: 42.00907516479492 and batch: 200, loss is 5.344626398086548 and perplexity is 209.47960799526038
At time: 43.309725522994995 and batch: 250, loss is 5.381890735626221 and perplexity is 217.4329953318008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.649885559082032 and perplexity of 284.25893310917377
Finished 6 epochs...
Completing Train Step...
At time: 45.28612971305847 and batch: 50, loss is 5.347955999374389 and perplexity is 210.17825402873777
At time: 46.59785437583923 and batch: 100, loss is 5.339335289001465 and perplexity is 208.37415565173
At time: 47.90088438987732 and batch: 150, loss is 5.33837384223938 and perplexity is 208.17391127205696
At time: 49.20934581756592 and batch: 200, loss is 5.3240212631225585 and perplexity is 205.20741807836404
At time: 50.511170864105225 and batch: 250, loss is 5.361697940826416 and perplexity is 213.08644763788314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.662528991699219 and perplexity of 287.8757581366139
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 52.52349328994751 and batch: 50, loss is 5.290214233398437 and perplexity is 198.38592174736922
At time: 53.83485174179077 and batch: 100, loss is 5.167751359939575 and perplexity is 175.51971274642423
At time: 55.1491596698761 and batch: 150, loss is 5.135478448867798 and perplexity is 169.94561071141004
At time: 56.45265316963196 and batch: 200, loss is 5.120626306533813 and perplexity is 167.4402056827641
At time: 57.75001287460327 and batch: 250, loss is 5.186110496520996 and perplexity is 178.77186513394372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.496449661254883 and perplexity of 243.8247333523466
Finished 8 epochs...
Completing Train Step...
At time: 59.743306398391724 and batch: 50, loss is 5.168907585144043 and perplexity is 175.72277042976685
At time: 61.05381393432617 and batch: 100, loss is 5.114541788101196 and perplexity is 166.42450582073997
At time: 62.35783243179321 and batch: 150, loss is 5.129364748001098 and perplexity is 168.90978367859546
At time: 63.66511106491089 and batch: 200, loss is 5.127321920394897 and perplexity is 168.5650823121471
At time: 64.96065354347229 and batch: 250, loss is 5.173049545288086 and perplexity is 176.45211655987723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.480204391479492 and perplexity of 239.89573500787137
Finished 9 epochs...
Completing Train Step...
At time: 66.95380783081055 and batch: 50, loss is 5.160657110214234 and perplexity is 174.27893845765183
At time: 68.25785899162292 and batch: 100, loss is 5.116883668899536 and perplexity is 166.81470890151004
At time: 69.55669212341309 and batch: 150, loss is 5.123084936141968 and perplexity is 167.85238562132088
At time: 70.85961627960205 and batch: 200, loss is 5.122301864624023 and perplexity is 167.7209966491167
At time: 72.15719676017761 and batch: 250, loss is 5.171228809356689 and perplexity is 176.13113615004144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.478060913085938 and perplexity of 239.3820743900619
Finished 10 epochs...
Completing Train Step...
At time: 74.16485142707825 and batch: 50, loss is 5.147522783279419 and perplexity is 172.00486878105474
At time: 75.47019267082214 and batch: 100, loss is 5.10375150680542 and perplexity is 164.63839225209824
At time: 76.77798080444336 and batch: 150, loss is 5.11546760559082 and perplexity is 166.57865588533312
At time: 78.07674527168274 and batch: 200, loss is 5.117755928039551 and perplexity is 166.9602780338118
At time: 79.36909246444702 and batch: 250, loss is 5.161712188720703 and perplexity is 174.46291345671574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.471339416503906 and perplexity of 237.77846396364004
Finished 11 epochs...
Completing Train Step...
At time: 81.35740041732788 and batch: 50, loss is 5.142379674911499 and perplexity is 171.12250010496655
At time: 82.68779039382935 and batch: 100, loss is 5.099101734161377 and perplexity is 163.87463817710193
At time: 83.9929313659668 and batch: 150, loss is 5.108846950531006 and perplexity is 165.47943884797775
At time: 85.29143381118774 and batch: 200, loss is 5.111955461502075 and perplexity is 165.99463382889442
At time: 86.59591841697693 and batch: 250, loss is 5.158891563415527 and perplexity is 173.97151230326523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.473310089111328 and perplexity of 238.24750948499135
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 88.59823036193848 and batch: 50, loss is 5.122425222396851 and perplexity is 167.7416876138892
At time: 89.90515875816345 and batch: 100, loss is 5.054425659179688 and perplexity is 156.71449695898076
At time: 91.21073794364929 and batch: 150, loss is 5.056253271102905 and perplexity is 157.00117212784718
At time: 92.51309728622437 and batch: 200, loss is 5.05398494720459 and perplexity is 156.64544622036084
At time: 93.80757427215576 and batch: 250, loss is 5.106598787307739 and perplexity is 165.10783193236932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.435276031494141 and perplexity of 229.35614905935674
Finished 13 epochs...
Completing Train Step...
At time: 95.80019021034241 and batch: 50, loss is 5.092633428573609 and perplexity is 162.81806773335634
At time: 97.11598515510559 and batch: 100, loss is 5.040987997055054 and perplexity is 154.62270636270546
At time: 98.42067003250122 and batch: 150, loss is 5.058038702011109 and perplexity is 157.2817372634896
At time: 99.717045545578 and batch: 200, loss is 5.060644369125367 and perplexity is 157.69209551026748
At time: 101.01479816436768 and batch: 250, loss is 5.103025722503662 and perplexity is 164.51894364374084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.429642105102539 and perplexity of 228.06760658314096
Finished 14 epochs...
Completing Train Step...
At time: 103.00862765312195 and batch: 50, loss is 5.087545585632324 and perplexity is 161.99177877340406
At time: 104.32147812843323 and batch: 100, loss is 5.041926107406616 and perplexity is 154.7678275833477
At time: 105.6257975101471 and batch: 150, loss is 5.05555401802063 and perplexity is 156.8914269486151
At time: 106.92391967773438 and batch: 200, loss is 5.057330236434937 and perplexity is 157.170348029256
At time: 108.21923208236694 and batch: 250, loss is 5.10311261177063 and perplexity is 164.53323919521242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.42902717590332 and perplexity of 227.92740426412476
Finished 15 epochs...
Completing Train Step...
At time: 110.22171759605408 and batch: 50, loss is 5.083654108047486 and perplexity is 161.36261637704223
At time: 111.5327537059784 and batch: 100, loss is 5.03717924118042 and perplexity is 154.0349063254996
At time: 112.85255885124207 and batch: 150, loss is 5.0528933048248295 and perplexity is 156.47453871458063
At time: 114.15400505065918 and batch: 200, loss is 5.055174255371094 and perplexity is 156.8318567566105
At time: 115.45122766494751 and batch: 250, loss is 5.098912220001221 and perplexity is 163.84358455532038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.425550079345703 and perplexity of 227.13625491979823
Finished 16 epochs...
Completing Train Step...
At time: 117.43867564201355 and batch: 50, loss is 5.078718881607056 and perplexity is 160.5682172084871
At time: 118.7563157081604 and batch: 100, loss is 5.033313426971436 and perplexity is 153.44058550304013
At time: 120.06210374832153 and batch: 150, loss is 5.0483917808532714 and perplexity is 155.77174782938945
At time: 121.36461424827576 and batch: 200, loss is 5.050917263031006 and perplexity is 156.16564378157304
At time: 122.65971231460571 and batch: 250, loss is 5.093547163009643 and perplexity is 162.9669081986265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.418048477172851 and perplexity of 225.43874408232648
Finished 17 epochs...
Completing Train Step...
At time: 124.66549897193909 and batch: 50, loss is 5.069251680374146 and perplexity is 159.05525862649915
At time: 125.96763682365417 and batch: 100, loss is 5.021096477508545 and perplexity is 151.57741393457655
At time: 127.27070569992065 and batch: 150, loss is 5.036103076934815 and perplexity is 153.86922863093227
At time: 128.5651969909668 and batch: 200, loss is 5.036659002304077 and perplexity is 153.95479221995834
At time: 129.86346697807312 and batch: 250, loss is 5.078884172439575 and perplexity is 160.59475985635333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.406113815307617 and perplexity of 222.7642005327344
Finished 18 epochs...
Completing Train Step...
At time: 131.87014603614807 and batch: 50, loss is 5.057662429809571 and perplexity is 157.2225676505861
At time: 133.17984318733215 and batch: 100, loss is 5.009823055267334 and perplexity is 149.87821364216157
At time: 134.4832911491394 and batch: 150, loss is 5.02696982383728 and perplexity is 152.47030013039517
At time: 135.78289341926575 and batch: 200, loss is 5.026903991699219 and perplexity is 152.46026301493268
At time: 137.080308675766 and batch: 250, loss is 5.068282127380371 and perplexity is 158.90112085876382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.396100234985352 and perplexity of 220.5446646147435
Finished 19 epochs...
Completing Train Step...
At time: 139.0704004764557 and batch: 50, loss is 5.047747049331665 and perplexity is 155.67134924193297
At time: 140.38449811935425 and batch: 100, loss is 5.001612911224365 and perplexity is 148.6527295036946
At time: 141.70007300376892 and batch: 150, loss is 5.018483724594116 and perplexity is 151.18189652419
At time: 143.00652170181274 and batch: 200, loss is 5.020124254226684 and perplexity is 151.43011845739346
At time: 144.30899357795715 and batch: 250, loss is 5.062398853302002 and perplexity is 157.96900664368908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.390365982055664 and perplexity of 219.283624741764
Finished 20 epochs...
Completing Train Step...
At time: 146.3147497177124 and batch: 50, loss is 5.042770967483521 and perplexity is 154.8986399933725
At time: 147.6248140335083 and batch: 100, loss is 4.9974933624267575 and perplexity is 148.04160696942083
At time: 148.93097639083862 and batch: 150, loss is 5.014862022399902 and perplexity is 150.635351027742
At time: 150.23185539245605 and batch: 200, loss is 5.017188940048218 and perplexity is 150.9862752120062
At time: 151.5321500301361 and batch: 250, loss is 5.058878078460693 and perplexity is 157.41381127184997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.389118957519531 and perplexity of 219.0103431112024
Finished 21 epochs...
Completing Train Step...
At time: 153.5212905406952 and batch: 50, loss is 5.039641857147217 and perplexity is 154.4147025995063
At time: 154.835919380188 and batch: 100, loss is 4.994642324447632 and perplexity is 147.62013582606974
At time: 156.1311159133911 and batch: 150, loss is 5.0120876502990725 and perplexity is 150.2180117074286
At time: 157.4276204109192 and batch: 200, loss is 5.014689073562622 and perplexity is 150.60930107164822
At time: 158.7250747680664 and batch: 250, loss is 5.056317081451416 and perplexity is 157.01119074699966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.388010787963867 and perplexity of 218.76777694363548
Finished 22 epochs...
Completing Train Step...
At time: 160.7249240875244 and batch: 50, loss is 5.036853284835815 and perplexity is 153.98470585251806
At time: 162.0428969860077 and batch: 100, loss is 4.991611013412475 and perplexity is 147.17333082393077
At time: 163.34158492088318 and batch: 150, loss is 5.009098463058471 and perplexity is 149.76965239234602
At time: 164.63785338401794 and batch: 200, loss is 5.012248077392578 and perplexity is 150.24211267961178
At time: 165.9393973350525 and batch: 250, loss is 5.054686632156372 and perplexity is 156.75540054487482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.386831283569336 and perplexity of 218.5098915077493
Finished 23 epochs...
Completing Train Step...
At time: 167.94782042503357 and batch: 50, loss is 5.0345985889434814 and perplexity is 153.63790827720098
At time: 169.24840354919434 and batch: 100, loss is 4.989036874771118 and perplexity is 146.79497344717382
At time: 170.54330110549927 and batch: 150, loss is 5.00669903755188 and perplexity is 149.4107220537913
At time: 171.8458695411682 and batch: 200, loss is 5.010153684616089 and perplexity is 149.92777597125948
At time: 173.151132106781 and batch: 250, loss is 5.053258352279663 and perplexity is 156.53166977382406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.386964797973633 and perplexity of 218.5390676734228
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 175.14971041679382 and batch: 50, loss is 5.028123044967652 and perplexity is 152.64623352781723
At time: 176.46874260902405 and batch: 100, loss is 4.975590515136719 and perplexity is 144.83432676296744
At time: 177.77052092552185 and batch: 150, loss is 4.990564413070679 and perplexity is 147.0193797422673
At time: 179.06320524215698 and batch: 200, loss is 4.993184518814087 and perplexity is 147.4050911451958
At time: 180.3564784526825 and batch: 250, loss is 5.040032739639282 and perplexity is 154.4750724012506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.374175262451172 and perplexity of 215.7618520097344
Finished 25 epochs...
Completing Train Step...
At time: 182.35110211372375 and batch: 50, loss is 5.020890712738037 and perplexity is 151.5462278513928
At time: 183.64770483970642 and batch: 100, loss is 4.971950168609619 and perplexity is 144.30803814221085
At time: 184.94388151168823 and batch: 150, loss is 4.991793098449707 and perplexity is 147.2001313252645
At time: 186.2403781414032 and batch: 200, loss is 4.994863166809082 and perplexity is 147.65274020554463
At time: 187.5562982559204 and batch: 250, loss is 5.037954025268554 and perplexity is 154.15429636471688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.373019409179688 and perplexity of 215.51260704028329
Finished 26 epochs...
Completing Train Step...
At time: 189.56609272956848 and batch: 50, loss is 5.0182763671875 and perplexity is 151.1505510881659
At time: 190.8617582321167 and batch: 100, loss is 4.973021049499511 and perplexity is 144.4626576372701
At time: 192.16100597381592 and batch: 150, loss is 4.992591543197632 and perplexity is 147.3177094305788
At time: 193.46407556533813 and batch: 200, loss is 4.993966732025147 and perplexity is 147.52043846208323
At time: 194.764892578125 and batch: 250, loss is 5.036698617935181 and perplexity is 153.9608913570233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372950744628906 and perplexity of 215.4978094719732
Finished 27 epochs...
Completing Train Step...
At time: 196.75848078727722 and batch: 50, loss is 5.0172083377838135 and perplexity is 150.98920403225748
At time: 198.07047843933105 and batch: 100, loss is 4.97300157546997 and perplexity is 144.45984439460037
At time: 199.36972904205322 and batch: 150, loss is 4.992941408157349 and perplexity is 147.36925975235414
At time: 200.67350435256958 and batch: 200, loss is 4.993465385437012 and perplexity is 147.44649812999543
At time: 201.9833459854126 and batch: 250, loss is 5.035460844039917 and perplexity is 153.77044047668957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372709274291992 and perplexity of 215.44577942542494
Finished 28 epochs...
Completing Train Step...
At time: 203.99758887290955 and batch: 50, loss is 5.016444425582886 and perplexity is 150.8739055816425
At time: 205.29947471618652 and batch: 100, loss is 4.973262577056885 and perplexity is 144.49755356409528
At time: 206.59771084785461 and batch: 150, loss is 4.993070363998413 and perplexity is 147.38826510459177
At time: 207.9044270515442 and batch: 200, loss is 4.99272780418396 and perplexity is 147.3377844546596
At time: 209.20783686637878 and batch: 250, loss is 5.034342460632324 and perplexity is 153.59856229824885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372589111328125 and perplexity of 215.41989237737988
Finished 29 epochs...
Completing Train Step...
At time: 211.1995611190796 and batch: 50, loss is 5.015818796157837 and perplexity is 150.7795439476238
At time: 212.5097439289093 and batch: 100, loss is 4.973570356369018 and perplexity is 144.5420337664281
At time: 213.80469489097595 and batch: 150, loss is 4.993095197677612 and perplexity is 147.3919253429335
At time: 215.11716532707214 and batch: 200, loss is 4.992019233703613 and perplexity is 147.2334222283182
At time: 216.4216182231903 and batch: 250, loss is 5.033237638473511 and perplexity is 153.42895691220647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372432708740234 and perplexity of 215.38620278336745
Finished 30 epochs...
Completing Train Step...
At time: 218.410391330719 and batch: 50, loss is 5.015360183715821 and perplexity is 150.710410426726
At time: 219.7204098701477 and batch: 100, loss is 4.973763055801392 and perplexity is 144.5698896181062
At time: 221.01838040351868 and batch: 150, loss is 4.993028211593628 and perplexity is 147.38205246572022
At time: 222.32309985160828 and batch: 200, loss is 4.991321926116943 and perplexity is 147.13079103290127
At time: 223.62855887413025 and batch: 250, loss is 5.032176685333252 and perplexity is 153.26626229950497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372328186035157 and perplexity of 215.36369121132182
Finished 31 epochs...
Completing Train Step...
At time: 225.66966152191162 and batch: 50, loss is 5.014971418380737 and perplexity is 150.65183083111177
At time: 226.97296524047852 and batch: 100, loss is 4.97388105392456 and perplexity is 144.58694960025096
At time: 228.27583479881287 and batch: 150, loss is 4.992870330810547 and perplexity is 147.35878550861588
At time: 229.57754755020142 and batch: 200, loss is 4.990641412734985 and perplexity is 147.03070062100025
At time: 230.87368869781494 and batch: 250, loss is 5.031264343261719 and perplexity is 153.12649480783875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.372161865234375 and perplexity of 215.3278747283359
Finished 32 epochs...
Completing Train Step...
At time: 232.86456108093262 and batch: 50, loss is 5.014547204971313 and perplexity is 150.58793585782934
At time: 234.17504978179932 and batch: 100, loss is 4.973863973617553 and perplexity is 144.58448003185313
At time: 235.47139620780945 and batch: 150, loss is 4.992657461166382 and perplexity is 147.32742063481328
At time: 236.77492952346802 and batch: 200, loss is 4.990023775100708 and perplexity is 146.93991696548972
At time: 238.07414269447327 and batch: 250, loss is 5.030436706542969 and perplexity is 152.99981412814452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.371847152709961 and perplexity of 215.26011901164898
Finished 33 epochs...
Completing Train Step...
At time: 240.0750560760498 and batch: 50, loss is 5.0139194583892825 and perplexity is 150.4934344603675
At time: 241.37185716629028 and batch: 100, loss is 4.973373222351074 and perplexity is 144.51354242294866
At time: 242.6675615310669 and batch: 150, loss is 4.9921753787994385 and perplexity is 147.25641380010467
At time: 243.96475267410278 and batch: 200, loss is 4.988877601623535 and perplexity is 146.77159481154743
At time: 245.2662708759308 and batch: 250, loss is 5.029164438247681 and perplexity is 152.80528109079253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3710182189941404 and perplexity of 215.0817565768598
Finished 34 epochs...
Completing Train Step...
At time: 247.25517344474792 and batch: 50, loss is 5.012841825485229 and perplexity is 150.33134513563
At time: 248.55135011672974 and batch: 100, loss is 4.972736577987671 and perplexity is 144.4215679713444
At time: 249.84791016578674 and batch: 150, loss is 4.991964168548584 and perplexity is 147.2253150203122
At time: 251.16147017478943 and batch: 200, loss is 4.988174295425415 and perplexity is 146.66840573022208
At time: 252.4782681465149 and batch: 250, loss is 5.028229932785035 and perplexity is 152.66255042257382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.370808410644531 and perplexity of 215.03663536205065
Finished 35 epochs...
Completing Train Step...
At time: 254.5138373374939 and batch: 50, loss is 5.012172832489013 and perplexity is 150.2308081516412
At time: 255.8279891014099 and batch: 100, loss is 4.972433366775513 and perplexity is 144.37778437083267
At time: 257.1340501308441 and batch: 150, loss is 4.991704359054565 and perplexity is 147.18706945420567
At time: 258.43548703193665 and batch: 200, loss is 4.987659959793091 and perplexity is 146.59298833960716
At time: 259.73483061790466 and batch: 250, loss is 5.027488822937012 and perplexity is 152.5494526171566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3704181671142575 and perplexity of 214.95273507816506
Finished 36 epochs...
Completing Train Step...
At time: 261.7442488670349 and batch: 50, loss is 5.011500501632691 and perplexity is 150.12983729046687
At time: 263.0399467945099 and batch: 100, loss is 4.972115230560303 and perplexity is 144.33185987446262
At time: 264.3382408618927 and batch: 150, loss is 4.991540260314942 and perplexity is 147.16291822326684
At time: 265.6428048610687 and batch: 200, loss is 4.987242031097412 and perplexity is 146.53173572369917
At time: 266.95373582839966 and batch: 250, loss is 5.026787328720093 and perplexity is 152.44247758392186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369974517822266 and perplexity of 214.85739260031121
Finished 37 epochs...
Completing Train Step...
At time: 268.96833634376526 and batch: 50, loss is 5.010824813842773 and perplexity is 150.02843065597787
At time: 270.288610458374 and batch: 100, loss is 4.971796913146973 and perplexity is 144.28592384167368
At time: 271.602077960968 and batch: 150, loss is 4.9913328266143795 and perplexity is 147.13239484045283
At time: 272.90926909446716 and batch: 200, loss is 4.986775693893432 and perplexity is 146.46341845444772
At time: 274.21527123451233 and batch: 250, loss is 5.0260959815979005 and perplexity is 152.33712333811798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369723510742188 and perplexity of 214.8034686414922
Finished 38 epochs...
Completing Train Step...
At time: 276.27119493484497 and batch: 50, loss is 5.010183906555175 and perplexity is 149.93230714784238
At time: 277.5785675048828 and batch: 100, loss is 4.971536312103272 and perplexity is 144.24832767834167
At time: 278.8817071914673 and batch: 150, loss is 4.991177282333374 and perplexity is 147.10951101765477
At time: 280.1870322227478 and batch: 200, loss is 4.986391668319702 and perplexity is 146.4071835546562
At time: 281.49011993408203 and batch: 250, loss is 5.025493450164795 and perplexity is 152.24536307987395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369548797607422 and perplexity of 214.76594293233987
Finished 39 epochs...
Completing Train Step...
At time: 283.49426341056824 and batch: 50, loss is 5.009624967575073 and perplexity is 149.84852755302254
At time: 284.8000319004059 and batch: 100, loss is 4.971243448257447 and perplexity is 144.20608874377427
At time: 286.1170036792755 and batch: 150, loss is 4.991053619384766 and perplexity is 147.09132014654557
At time: 287.4211976528168 and batch: 200, loss is 4.985988206863404 and perplexity is 146.3481258137289
At time: 288.731249332428 and batch: 250, loss is 5.024895105361939 and perplexity is 152.15429510585756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369428634643555 and perplexity of 214.74013757055482
Finished 40 epochs...
Completing Train Step...
At time: 290.728634595871 and batch: 50, loss is 5.009091920852661 and perplexity is 149.76867257166106
At time: 292.0488739013672 and batch: 100, loss is 4.970956983566285 and perplexity is 144.16478470745506
At time: 293.3589689731598 and batch: 150, loss is 4.990872316360473 and perplexity is 147.06465446271346
At time: 294.6670444011688 and batch: 200, loss is 4.985608873367309 and perplexity is 146.29262159548725
At time: 295.975444316864 and batch: 250, loss is 5.024306001663208 and perplexity is 152.06468684470354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369262313842773 and perplexity of 214.7044247888857
Finished 41 epochs...
Completing Train Step...
At time: 297.9864740371704 and batch: 50, loss is 5.008558177947998 and perplexity is 149.68875593470133
At time: 299.28384733200073 and batch: 100, loss is 4.970665102005005 and perplexity is 144.12271180546395
At time: 300.59351086616516 and batch: 150, loss is 4.9907012557983395 and perplexity is 147.03949965181042
At time: 301.9029858112335 and batch: 200, loss is 4.985212392807007 and perplexity is 146.23463091175807
At time: 303.22531938552856 and batch: 250, loss is 5.023709573745728 and perplexity is 151.97401826153128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.369146728515625 and perplexity of 214.67960954187325
Finished 42 epochs...
Completing Train Step...
At time: 305.226913690567 and batch: 50, loss is 5.0080569458007815 and perplexity is 149.61374591843614
At time: 306.52949142456055 and batch: 100, loss is 4.970411024093628 and perplexity is 144.08609805943547
At time: 307.831759929657 and batch: 150, loss is 4.990523490905762 and perplexity is 147.01336351406283
At time: 309.1340959072113 and batch: 200, loss is 4.984824934005737 and perplexity is 146.1779819922308
At time: 310.4407203197479 and batch: 250, loss is 5.023148612976074 and perplexity is 151.88879070617244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.368960189819336 and perplexity of 214.6395672222265
Finished 43 epochs...
Completing Train Step...
At time: 312.43177676200867 and batch: 50, loss is 5.00758728981018 and perplexity is 149.54349542446323
At time: 313.746141910553 and batch: 100, loss is 4.970159635543824 and perplexity is 144.049881016663
At time: 315.0569181442261 and batch: 150, loss is 4.99031662940979 and perplexity is 146.9829552550159
At time: 316.36557817459106 and batch: 200, loss is 4.984388399124145 and perplexity is 146.11418413017967
At time: 317.6718304157257 and batch: 250, loss is 5.022593994140625 and perplexity is 151.80457367828686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3687797546386715 and perplexity of 214.6008421869215
Finished 44 epochs...
Completing Train Step...
At time: 319.7259511947632 and batch: 50, loss is 5.0070860385894775 and perplexity is 149.46855534830496
At time: 321.029892206192 and batch: 100, loss is 4.969814167022705 and perplexity is 144.00012491237968
At time: 322.33648800849915 and batch: 150, loss is 4.989859342575073 and perplexity is 146.91575725020246
At time: 323.6445746421814 and batch: 200, loss is 4.983757658004761 and perplexity is 146.02205296463507
At time: 324.94652342796326 and batch: 250, loss is 5.021882123947144 and perplexity is 151.69654698210576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.368453598022461 and perplexity of 214.530860115576
Finished 45 epochs...
Completing Train Step...
At time: 326.94360995292664 and batch: 50, loss is 5.006579475402832 and perplexity is 149.39285925465032
At time: 328.2568733692169 and batch: 100, loss is 4.969467258453369 and perplexity is 143.95017869894764
At time: 329.5625729560852 and batch: 150, loss is 4.989641914367676 and perplexity is 146.88381709293648
At time: 330.8668782711029 and batch: 200, loss is 4.983337144851685 and perplexity is 145.9606616795504
At time: 332.1685883998871 and batch: 250, loss is 5.021324291229248 and perplexity is 151.61194928288324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3682811737060545 and perplexity of 214.49387296750604
Finished 46 epochs...
Completing Train Step...
At time: 334.1632857322693 and batch: 50, loss is 5.006127500534058 and perplexity is 149.32535269347667
At time: 335.4829761981964 and batch: 100, loss is 4.969193906784057 and perplexity is 143.910835054873
At time: 336.7885990142822 and batch: 150, loss is 4.989341783523559 and perplexity is 146.83973934380225
At time: 338.09798526763916 and batch: 200, loss is 4.9829426193237305 and perplexity is 145.90308783036411
At time: 339.39767599105835 and batch: 250, loss is 5.020876264572143 and perplexity is 151.54403830216975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.368133544921875 and perplexity of 214.46220983507828
Finished 47 epochs...
Completing Train Step...
At time: 341.40735602378845 and batch: 50, loss is 5.005645322799682 and perplexity is 149.25336868916682
At time: 342.7101972103119 and batch: 100, loss is 4.968862743377685 and perplexity is 143.86318494294454
At time: 344.0075168609619 and batch: 150, loss is 4.989089860916137 and perplexity is 146.80275175299437
At time: 345.3113057613373 and batch: 200, loss is 4.982584457397461 and perplexity is 145.85084025648254
At time: 346.6150290966034 and batch: 250, loss is 5.0204148101806645 and perplexity is 151.47412377261676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.367886734008789 and perplexity of 214.4092847527591
Finished 48 epochs...
Completing Train Step...
At time: 348.6009976863861 and batch: 50, loss is 5.005148572921753 and perplexity is 149.17924550835463
At time: 349.9126627445221 and batch: 100, loss is 4.96857518196106 and perplexity is 143.82182138924819
At time: 351.2203252315521 and batch: 150, loss is 4.988873176574707 and perplexity is 146.7709453415107
At time: 352.5440514087677 and batch: 200, loss is 4.982255697250366 and perplexity is 145.80289819393747
At time: 353.8430666923523 and batch: 250, loss is 5.019967832565308 and perplexity is 151.4064333591614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.367721939086914 and perplexity of 214.37395410266484
Finished 49 epochs...
Completing Train Step...
At time: 355.8429412841797 and batch: 50, loss is 5.004704627990723 and perplexity is 149.1130328369737
At time: 357.1438229084015 and batch: 100, loss is 4.968310832977295 and perplexity is 143.78380726164008
At time: 358.45615553855896 and batch: 150, loss is 4.988613691329956 and perplexity is 146.7328653876432
At time: 359.75999307632446 and batch: 200, loss is 4.981909074783325 and perplexity is 145.75236839155204
At time: 361.05431938171387 and batch: 250, loss is 5.0195118427276615 and perplexity is 151.33740930252557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.367567443847657 and perplexity of 214.34083690562562
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff00df36860>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 6.048407204416186, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.1307218843073108, 'lr': 18.57332424490116}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5407755374908447 and batch: 50, loss is 6.616815242767334 and perplexity is 747.5605032315705
At time: 2.8569815158843994 and batch: 100, loss is 5.69255970954895 and perplexity is 296.6519925585529
At time: 4.170530796051025 and batch: 150, loss is 5.493139867782593 and perplexity is 243.01905788670905
At time: 5.491806983947754 and batch: 200, loss is 5.467830219268799 and perplexity is 236.94551478159082
At time: 6.810579299926758 and batch: 250, loss is 5.5036591625213624 and perplexity is 245.58893995964033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.61145248413086 and perplexity of 273.5412639430311
Finished 1 epochs...
Completing Train Step...
At time: 8.822247743606567 and batch: 50, loss is 5.373305864334107 and perplexity is 215.57435058036774
At time: 10.123963594436646 and batch: 100, loss is 5.347387170791626 and perplexity is 210.05873262718833
At time: 11.441992998123169 and batch: 150, loss is 5.350784521102906 and perplexity is 210.77358934964846
At time: 12.748557329177856 and batch: 200, loss is 5.35033314704895 and perplexity is 210.67847308827803
At time: 14.056880235671997 and batch: 250, loss is 5.412262344360352 and perplexity is 224.1380920693728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.678110504150391 and perplexity of 292.39642580458707
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 16.069146156311035 and batch: 50, loss is 5.275590181350708 and perplexity is 195.5058263632117
At time: 17.37898564338684 and batch: 100, loss is 5.082088136672974 and perplexity is 161.11012488769978
At time: 18.695363998413086 and batch: 150, loss is 5.003353786468506 and perplexity is 148.91174074817903
At time: 20.003962516784668 and batch: 200, loss is 4.92692684173584 and perplexity is 137.9549029780856
At time: 21.30405616760254 and batch: 250, loss is 4.990903644561768 and perplexity is 147.06926180598128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.288338470458984 and perplexity of 198.01414557863328
Finished 3 epochs...
Completing Train Step...
At time: 23.323338508605957 and batch: 50, loss is 5.007337789535523 and perplexity is 149.50618893546476
At time: 24.637971878051758 and batch: 100, loss is 4.907250642776489 and perplexity is 135.2670053819776
At time: 25.94672393798828 and batch: 150, loss is 4.892229375839233 and perplexity is 133.2503081800608
At time: 27.261749982833862 and batch: 200, loss is 4.886613082885742 and perplexity is 132.50403302475638
At time: 28.562045335769653 and batch: 250, loss is 4.942922601699829 and perplexity is 140.17933984173848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.245754241943359 and perplexity of 189.75888537846117
Finished 4 epochs...
Completing Train Step...
At time: 30.55850601196289 and batch: 50, loss is 4.926669492721557 and perplexity is 137.91940498767107
At time: 31.87515950202942 and batch: 100, loss is 4.84703311920166 and perplexity is 127.3619609904643
At time: 33.18134951591492 and batch: 150, loss is 4.856631088256836 and perplexity is 128.590262320587
At time: 34.48187875747681 and batch: 200, loss is 4.846623010635376 and perplexity is 127.30973946822218
At time: 35.782506227493286 and batch: 250, loss is 4.895109958648682 and perplexity is 133.63470009773394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.208554458618164 and perplexity of 182.82957935943506
Finished 5 epochs...
Completing Train Step...
At time: 37.791102170944214 and batch: 50, loss is 4.868701305389404 and perplexity is 130.15177967660776
At time: 39.10307025909424 and batch: 100, loss is 4.7998292350769045 and perplexity is 121.48966957319307
At time: 40.40664267539978 and batch: 150, loss is 4.811002159118653 and perplexity is 122.85467577858728
At time: 41.71255087852478 and batch: 200, loss is 4.806426677703858 and perplexity is 122.29384051692485
At time: 43.03139352798462 and batch: 250, loss is 4.856383600234985 and perplexity is 128.5584417087086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.184154891967774 and perplexity of 178.42259968436207
Finished 6 epochs...
Completing Train Step...
At time: 45.06063508987427 and batch: 50, loss is 4.830329341888428 and perplexity is 125.25220466031563
At time: 46.38387084007263 and batch: 100, loss is 4.762593984603882 and perplexity is 117.0491561506097
At time: 47.68929600715637 and batch: 150, loss is 4.77893325805664 and perplexity is 118.97736418159955
At time: 48.99433970451355 and batch: 200, loss is 4.777516889572143 and perplexity is 118.80896767650725
At time: 50.289448976516724 and batch: 250, loss is 4.826560049057007 and perplexity is 124.78098107093025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.16638412475586 and perplexity of 175.27989999729326
Finished 7 epochs...
Completing Train Step...
At time: 52.30259561538696 and batch: 50, loss is 4.797940139770508 and perplexity is 121.26038065112814
At time: 53.61351680755615 and batch: 100, loss is 4.730532274246216 and perplexity is 113.35588271077212
At time: 54.93682861328125 and batch: 150, loss is 4.750556516647339 and perplexity is 115.64862700786796
At time: 56.24038338661194 and batch: 200, loss is 4.750148487091065 and perplexity is 115.60144857565817
At time: 57.5387499332428 and batch: 250, loss is 4.7994664287567135 and perplexity is 121.44560034801411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.152192306518555 and perplexity of 172.80992767084837
Finished 8 epochs...
Completing Train Step...
At time: 59.54208779335022 and batch: 50, loss is 4.770593366622925 and perplexity is 117.98923206589338
At time: 60.84929633140564 and batch: 100, loss is 4.700604181289673 and perplexity is 110.01362054789833
At time: 62.15483546257019 and batch: 150, loss is 4.724308929443359 and perplexity is 112.65262055843952
At time: 63.45179009437561 and batch: 200, loss is 4.724160251617431 and perplexity is 112.63587285676657
At time: 64.75686287879944 and batch: 250, loss is 4.773432245254517 and perplexity is 118.32466507708943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1350654602050785 and perplexity of 169.87543959184308
Finished 9 epochs...
Completing Train Step...
At time: 66.73965239524841 and batch: 50, loss is 4.739136514663696 and perplexity is 114.33543207422633
At time: 68.05738234519958 and batch: 100, loss is 4.67086895942688 and perplexity is 106.79049873646007
At time: 69.3571298122406 and batch: 150, loss is 4.69827299118042 and perplexity is 109.75745658325339
At time: 70.67282962799072 and batch: 200, loss is 4.696411371231079 and perplexity is 109.5533199838272
At time: 71.98169040679932 and batch: 250, loss is 4.745642166137696 and perplexity is 115.08168333975789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.123279571533203 and perplexity of 167.88505881564288
Finished 10 epochs...
Completing Train Step...
At time: 74.0402238368988 and batch: 50, loss is 4.711609706878662 and perplexity is 111.23106528536887
At time: 75.35399293899536 and batch: 100, loss is 4.644101009368897 and perplexity is 103.96985583311645
At time: 76.65659070014954 and batch: 150, loss is 4.674802761077881 and perplexity is 107.2114187418027
At time: 77.96525120735168 and batch: 200, loss is 4.671970958709717 and perplexity is 106.90824665662714
At time: 79.2768247127533 and batch: 250, loss is 4.7222870826721195 and perplexity is 112.42508432050717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.110892105102539 and perplexity of 165.81821618676156
Finished 11 epochs...
Completing Train Step...
At time: 81.31362795829773 and batch: 50, loss is 4.686362962722779 and perplexity is 108.45799581978524
At time: 82.62272834777832 and batch: 100, loss is 4.617927093505859 and perplexity is 101.28386239758609
At time: 83.94745016098022 and batch: 150, loss is 4.652047662734986 and perplexity is 104.79935976283065
At time: 85.24807834625244 and batch: 200, loss is 4.648350973129272 and perplexity is 104.4126642459249
At time: 86.55076789855957 and batch: 250, loss is 4.7005020236969 and perplexity is 110.00238239529193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.098749160766602 and perplexity of 163.81687052386943
Finished 12 epochs...
Completing Train Step...
At time: 88.55128335952759 and batch: 50, loss is 4.664109401702881 and perplexity is 106.07107642349803
At time: 89.87175703048706 and batch: 100, loss is 4.592714328765869 and perplexity is 98.76213958834703
At time: 91.17276644706726 and batch: 150, loss is 4.628272409439087 and perplexity is 102.3371146730741
At time: 92.47916674613953 and batch: 200, loss is 4.626758708953857 and perplexity is 102.18232411576311
At time: 93.77696871757507 and batch: 250, loss is 4.681465272903442 and perplexity is 107.92810088757197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.089194488525391 and perplexity of 162.25910782533043
Finished 13 epochs...
Completing Train Step...
At time: 95.78664636611938 and batch: 50, loss is 4.641821947097778 and perplexity is 103.73317186854763
At time: 97.09219264984131 and batch: 100, loss is 4.571681098937988 and perplexity is 96.70654647107816
At time: 98.38697052001953 and batch: 150, loss is 4.607294025421143 and perplexity is 100.21260963775333
At time: 99.68714928627014 and batch: 200, loss is 4.607042331695556 and perplexity is 100.18738992663751
At time: 100.99166202545166 and batch: 250, loss is 4.664313144683838 and perplexity is 106.09268986252042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.08659553527832 and perplexity of 161.83795151002798
Finished 14 epochs...
Completing Train Step...
At time: 103.03747296333313 and batch: 50, loss is 4.624707040786743 and perplexity is 101.97289480731904
At time: 104.37074279785156 and batch: 100, loss is 4.55229305267334 and perplexity is 94.84965439495589
At time: 105.68191456794739 and batch: 150, loss is 4.589736394882202 and perplexity is 98.468469947854
At time: 106.98766207695007 and batch: 200, loss is 4.590378255844116 and perplexity is 98.53169330282407
At time: 108.29379725456238 and batch: 250, loss is 4.648284196853638 and perplexity is 104.40569218986406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0761157989501955 and perplexity of 160.1507884035794
Finished 15 epochs...
Completing Train Step...
At time: 110.33770418167114 and batch: 50, loss is 4.606183500289917 and perplexity is 100.10138278782193
At time: 111.6665289402008 and batch: 100, loss is 4.536717128753662 and perplexity is 93.38372959798927
At time: 112.97713541984558 and batch: 150, loss is 4.5729276561737064 and perplexity is 96.82717188394355
At time: 114.28225135803223 and batch: 200, loss is 4.573820991516113 and perplexity is 96.9137096665675
At time: 115.59399771690369 and batch: 250, loss is 4.632519683837891 and perplexity is 102.77269283546636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.070306396484375 and perplexity of 159.22310526990879
Finished 16 epochs...
Completing Train Step...
At time: 117.6569836139679 and batch: 50, loss is 4.590283651351928 and perplexity is 98.52237220293067
At time: 118.96437811851501 and batch: 100, loss is 4.517980241775513 and perplexity is 91.65029946592934
At time: 120.27462959289551 and batch: 150, loss is 4.5548685264587405 and perplexity is 95.09425203560747
At time: 121.58140563964844 and batch: 200, loss is 4.557960119247436 and perplexity is 95.38869966099867
At time: 122.88919878005981 and batch: 250, loss is 4.6160108089447025 and perplexity is 101.08995954164055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.061190795898438 and perplexity of 157.7782862394724
Finished 17 epochs...
Completing Train Step...
At time: 124.89588809013367 and batch: 50, loss is 4.572394952774048 and perplexity is 96.77560545632676
At time: 126.21182107925415 and batch: 100, loss is 4.501023664474487 and perplexity is 90.10932582000575
At time: 127.50955939292908 and batch: 150, loss is 4.539069232940673 and perplexity is 93.60363637982233
At time: 128.8140172958374 and batch: 200, loss is 4.541252927780151 and perplexity is 93.80826149563273
At time: 130.114679813385 and batch: 250, loss is 4.59922085762024 and perplexity is 99.40683338427074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.058828735351563 and perplexity of 157.40604417660967
Finished 18 epochs...
Completing Train Step...
At time: 132.12839078903198 and batch: 50, loss is 4.554881277084351 and perplexity is 95.09546455454306
At time: 133.43212223052979 and batch: 100, loss is 4.486735944747925 and perplexity is 88.83102279151595
At time: 134.73235416412354 and batch: 150, loss is 4.525317010879516 and perplexity is 92.32518927945164
At time: 136.03771209716797 and batch: 200, loss is 4.526967763900757 and perplexity is 92.47772122624002
At time: 137.34280610084534 and batch: 250, loss is 4.584963731765747 and perplexity is 97.99963280401506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0515998840332035 and perplexity of 156.27228212248474
Finished 19 epochs...
Completing Train Step...
At time: 139.3570954799652 and batch: 50, loss is 4.539857397079468 and perplexity is 93.67744049033362
At time: 140.65356278419495 and batch: 100, loss is 4.472655153274536 and perplexity is 87.5889767007261
At time: 141.95667481422424 and batch: 150, loss is 4.5082297611236575 and perplexity is 90.76100755159513
At time: 143.25941610336304 and batch: 200, loss is 4.515996227264404 and perplexity is 91.46864420476619
At time: 144.56552958488464 and batch: 250, loss is 4.572702360153198 and perplexity is 96.80535956464769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050836181640625 and perplexity of 156.152982167363
Finished 20 epochs...
Completing Train Step...
At time: 146.5610921382904 and batch: 50, loss is 4.529231147766113 and perplexity is 92.687270864545
At time: 147.87191653251648 and batch: 100, loss is 4.459924774169922 and perplexity is 86.48100324637329
At time: 149.18106985092163 and batch: 150, loss is 4.5004809284210205 and perplexity is 90.06043350913976
At time: 150.4992434978485 and batch: 200, loss is 4.506045722961426 and perplexity is 90.56299835598874
At time: 151.80682229995728 and batch: 250, loss is 4.563482828140259 and perplexity is 95.9169610551634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050754165649414 and perplexity of 156.14017565092675
Finished 21 epochs...
Completing Train Step...
At time: 153.82398748397827 and batch: 50, loss is 4.519173536300659 and perplexity is 91.7597305452786
At time: 155.12599515914917 and batch: 100, loss is 4.444435539245606 and perplexity is 85.1517994204705
At time: 156.42216753959656 and batch: 150, loss is 4.488936195373535 and perplexity is 89.02668848282997
At time: 157.7280626296997 and batch: 200, loss is 4.492664947509765 and perplexity is 89.35926660304415
At time: 159.0405433177948 and batch: 250, loss is 4.5496408557891845 and perplexity is 94.59842773565165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.04479751586914 and perplexity of 155.21286787364562
Finished 22 epochs...
Completing Train Step...
At time: 161.0379536151886 and batch: 50, loss is 4.506413841247559 and perplexity is 90.59634238862813
At time: 162.35558199882507 and batch: 100, loss is 4.433196821212769 and perplexity is 84.20015997716183
At time: 163.65818238258362 and batch: 150, loss is 4.474660701751709 and perplexity is 87.764816908657
At time: 164.96508264541626 and batch: 200, loss is 4.479170083999634 and perplexity is 88.16147568688822
At time: 166.2682888507843 and batch: 250, loss is 4.538584651947022 and perplexity is 93.55828882486499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.037891006469726 and perplexity of 154.14458205222112
Finished 23 epochs...
Completing Train Step...
At time: 168.26536679267883 and batch: 50, loss is 4.4962913608551025 and perplexity is 89.68390852710891
At time: 169.57470870018005 and batch: 100, loss is 4.420253839492798 and perplexity is 83.11738115463234
At time: 170.8762617111206 and batch: 150, loss is 4.461751527786255 and perplexity is 86.6391271144997
At time: 172.18512535095215 and batch: 200, loss is 4.468674402236939 and perplexity is 87.24099985495188
At time: 173.49013113975525 and batch: 250, loss is 4.5282559299469 and perplexity is 92.59692464716446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.037559509277344 and perplexity of 154.09349202461806
Finished 24 epochs...
Completing Train Step...
At time: 175.5059015750885 and batch: 50, loss is 4.487710447311401 and perplexity is 88.91763104407059
At time: 176.81226992607117 and batch: 100, loss is 4.408464736938477 and perplexity is 82.14325514210032
At time: 178.11820769309998 and batch: 150, loss is 4.44786358833313 and perplexity is 85.44420487254577
At time: 179.42418909072876 and batch: 200, loss is 4.454469785690308 and perplexity is 86.01053473674862
At time: 180.72537183761597 and batch: 250, loss is 4.516464824676514 and perplexity is 91.51151621880265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.032969665527344 and perplexity of 153.38784761093163
Finished 25 epochs...
Completing Train Step...
At time: 182.72597193717957 and batch: 50, loss is 4.4758398056030275 and perplexity is 87.86836177536338
At time: 184.03842091560364 and batch: 100, loss is 4.3977015018463135 and perplexity is 81.26386898526286
At time: 185.34167551994324 and batch: 150, loss is 4.435566301345825 and perplexity is 84.39990713843741
At time: 186.6435842514038 and batch: 200, loss is 4.440027227401734 and perplexity is 84.77724990664535
At time: 187.9516773223877 and batch: 250, loss is 4.501159601211548 and perplexity is 90.12157582033012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.026029968261719 and perplexity of 152.3270673882992
Finished 26 epochs...
Completing Train Step...
At time: 189.9615397453308 and batch: 50, loss is 4.4610363960266115 and perplexity is 86.57719087199895
At time: 191.2620370388031 and batch: 100, loss is 4.383024587631225 and perplexity is 80.07987608624663
At time: 192.5634000301361 and batch: 150, loss is 4.423995656967163 and perplexity is 83.42897382174777
At time: 193.86591792106628 and batch: 200, loss is 4.4284877109527585 and perplexity is 83.80458427573038
At time: 195.16471433639526 and batch: 250, loss is 4.491295051574707 and perplexity is 89.23693751515536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.019334030151367 and perplexity of 151.31050200018706
Finished 27 epochs...
Completing Train Step...
At time: 197.18490266799927 and batch: 50, loss is 4.448689603805542 and perplexity is 85.51481226518325
At time: 198.4911699295044 and batch: 100, loss is 4.3686373424530025 and perplexity is 78.93599565047946
At time: 199.7961766719818 and batch: 150, loss is 4.412163362503052 and perplexity is 82.44763483178863
At time: 201.1081883907318 and batch: 200, loss is 4.418354597091675 and perplexity is 82.95967091246933
At time: 202.4235541820526 and batch: 250, loss is 4.482035837173462 and perplexity is 88.4144870765883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.015601348876953 and perplexity of 150.74676091019987
Finished 28 epochs...
Completing Train Step...
At time: 204.41199922561646 and batch: 50, loss is 4.43695333480835 and perplexity is 84.51705385830145
At time: 205.72756004333496 and batch: 100, loss is 4.357136545181274 and perplexity is 78.03336917814977
At time: 207.0303349494934 and batch: 150, loss is 4.400622100830078 and perplexity is 81.50155508237165
At time: 208.33615803718567 and batch: 200, loss is 4.40740707397461 and perplexity is 82.05642119202625
At time: 209.63793563842773 and batch: 250, loss is 4.474229135513306 and perplexity is 87.72694874865697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0166069030761715 and perplexity of 150.8984211871867
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 211.64880228042603 and batch: 50, loss is 4.41194935798645 and perplexity is 82.42999255338245
At time: 212.96686601638794 and batch: 100, loss is 4.299210243225097 and perplexity is 73.64161176602481
At time: 214.27263641357422 and batch: 150, loss is 4.318398838043213 and perplexity is 75.06833545067926
At time: 215.57653951644897 and batch: 200, loss is 4.307803888320922 and perplexity is 74.27718869161085
At time: 216.8971245288849 and batch: 250, loss is 4.386090545654297 and perplexity is 80.32577438913482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.95893669128418 and perplexity of 142.44225525251193
Finished 30 epochs...
Completing Train Step...
At time: 218.89422941207886 and batch: 50, loss is 4.369603204727173 and perplexity is 79.01227378190714
At time: 220.20650601387024 and batch: 100, loss is 4.266588125228882 and perplexity is 71.27802856975083
At time: 221.50855231285095 and batch: 150, loss is 4.300215482711792 and perplexity is 73.71567644216746
At time: 222.81583213806152 and batch: 200, loss is 4.305882797241211 and perplexity is 74.1346324226208
At time: 224.1188611984253 and batch: 250, loss is 4.383959369659424 and perplexity is 80.15476831373306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.953205871582031 and perplexity of 141.62827897307815
Finished 31 epochs...
Completing Train Step...
At time: 226.1315941810608 and batch: 50, loss is 4.355718269348144 and perplexity is 77.92277478168504
At time: 227.4517683982849 and batch: 100, loss is 4.2583684158325195 and perplexity is 70.69454520559114
At time: 228.7561011314392 and batch: 150, loss is 4.297805118560791 and perplexity is 73.53820878506555
At time: 230.06759548187256 and batch: 200, loss is 4.304668207168579 and perplexity is 74.04464389469116
At time: 231.37054252624512 and batch: 250, loss is 4.379599847793579 and perplexity is 79.80609243091301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.951813507080078 and perplexity of 141.43121800714667
Finished 32 epochs...
Completing Train Step...
At time: 233.3700385093689 and batch: 50, loss is 4.348320398330689 and perplexity is 77.34843919420086
At time: 234.67379093170166 and batch: 100, loss is 4.254088277816773 and perplexity is 70.39260941951353
At time: 235.97696256637573 and batch: 150, loss is 4.296157617568969 and perplexity is 73.41715425926216
At time: 237.28205561637878 and batch: 200, loss is 4.302649250030518 and perplexity is 73.89530174079749
At time: 238.5838499069214 and batch: 250, loss is 4.37613881111145 and perplexity is 79.53035805619157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9508056640625 and perplexity of 141.2887493466923
Finished 33 epochs...
Completing Train Step...
At time: 240.5782082080841 and batch: 50, loss is 4.342635192871094 and perplexity is 76.90994507106154
At time: 241.8945689201355 and batch: 100, loss is 4.250412454605103 and perplexity is 70.13433361079716
At time: 243.1968104839325 and batch: 150, loss is 4.2945199298858645 and perplexity is 73.29701828945278
At time: 244.50401544570923 and batch: 200, loss is 4.3002363967895505 and perplexity is 73.71721815367827
At time: 245.80309081077576 and batch: 250, loss is 4.3726209449768065 and perplexity is 79.25107243564496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.949277877807617 and perplexity of 141.07305514674266
Finished 34 epochs...
Completing Train Step...
At time: 247.8163652420044 and batch: 50, loss is 4.338111610412597 and perplexity is 76.56282230380967
At time: 249.120285987854 and batch: 100, loss is 4.2472838306427 and perplexity is 69.91525254391539
At time: 250.44410133361816 and batch: 150, loss is 4.292850151062011 and perplexity is 73.17473060556712
At time: 251.74856567382812 and batch: 200, loss is 4.298045301437378 and perplexity is 73.55587352488973
At time: 253.05485010147095 and batch: 250, loss is 4.369691953659058 and perplexity is 79.01928634798539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.948012924194336 and perplexity of 140.89471709436
Finished 35 epochs...
Completing Train Step...
At time: 255.0632631778717 and batch: 50, loss is 4.334113082885742 and perplexity is 76.25729498805507
At time: 256.3680889606476 and batch: 100, loss is 4.244358472824096 and perplexity is 69.71102427926577
At time: 257.6711416244507 and batch: 150, loss is 4.290059318542481 and perplexity is 72.97079689256988
At time: 258.9751787185669 and batch: 200, loss is 4.295366582870483 and perplexity is 73.35910170659784
At time: 260.27178978919983 and batch: 250, loss is 4.367518615722656 and perplexity is 78.8477372198965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9474327087402346 and perplexity of 140.81299151361588
Finished 36 epochs...
Completing Train Step...
At time: 262.26958656311035 and batch: 50, loss is 4.33156533241272 and perplexity is 76.06325771291611
At time: 263.5869810581207 and batch: 100, loss is 4.242111349105835 and perplexity is 69.55455085659679
At time: 264.892941236496 and batch: 150, loss is 4.289633378982544 and perplexity is 72.93972236185901
At time: 266.19515323638916 and batch: 200, loss is 4.293758535385132 and perplexity is 73.2412315833478
At time: 267.4964118003845 and batch: 250, loss is 4.364352226257324 and perplexity is 78.59846942312468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.947015762329102 and perplexity of 140.7542922802311
Finished 37 epochs...
Completing Train Step...
At time: 269.50576663017273 and batch: 50, loss is 4.328857536315918 and perplexity is 75.8575725230246
At time: 270.8135688304901 and batch: 100, loss is 4.240179576873779 and perplexity is 69.42031700299825
At time: 272.12971472740173 and batch: 150, loss is 4.2872279834747316 and perplexity is 72.76448432423582
At time: 273.4352123737335 and batch: 200, loss is 4.2913022708892825 and perplexity is 73.06155250658294
At time: 274.73707246780396 and batch: 250, loss is 4.361586694717407 and perplexity is 78.38140316708102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9466064453125 and perplexity of 140.6966909426491
Finished 38 epochs...
Completing Train Step...
At time: 276.74071431159973 and batch: 50, loss is 4.325861558914185 and perplexity is 75.63064505451848
At time: 278.06128191947937 and batch: 100, loss is 4.237645797729492 and perplexity is 69.2446439039891
At time: 279.36689281463623 and batch: 150, loss is 4.285009403228759 and perplexity is 72.6032294213195
At time: 280.66837334632874 and batch: 200, loss is 4.288976631164551 and perplexity is 72.89183508500632
At time: 281.97000336647034 and batch: 250, loss is 4.359039583206177 and perplexity is 78.18201103760285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.946273422241211 and perplexity of 140.6498434995838
Finished 39 epochs...
Completing Train Step...
At time: 283.9732313156128 and batch: 50, loss is 4.323215522766113 and perplexity is 75.430788164688
At time: 285.2993769645691 and batch: 100, loss is 4.235119581222534 and perplexity is 69.06993770729063
At time: 286.6099843978882 and batch: 150, loss is 4.28252721786499 and perplexity is 72.4232382260569
At time: 287.9043366909027 and batch: 200, loss is 4.286034803390503 and perplexity is 72.67771496672997
At time: 289.2071850299835 and batch: 250, loss is 4.356967096328735 and perplexity is 78.02014763350222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.945611190795899 and perplexity of 140.55673158466203
Finished 40 epochs...
Completing Train Step...
At time: 291.22123503685 and batch: 50, loss is 4.32096230506897 and perplexity is 75.26101751482919
At time: 292.5329475402832 and batch: 100, loss is 4.232510328292847 and perplexity is 68.8899516866155
At time: 293.84421467781067 and batch: 150, loss is 4.280231351852417 and perplexity is 72.25715490037666
At time: 295.14534306526184 and batch: 200, loss is 4.283689050674439 and perplexity is 72.50743081978264
At time: 296.45166754722595 and batch: 250, loss is 4.35465708732605 and perplexity is 77.84012839316159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.945217895507812 and perplexity of 140.50146215373678
Finished 41 epochs...
Completing Train Step...
At time: 298.4521584510803 and batch: 50, loss is 4.31832597732544 and perplexity is 75.06286611712851
At time: 299.7732970714569 and batch: 100, loss is 4.230095148086548 and perplexity is 68.72377079806452
At time: 301.0827286243439 and batch: 150, loss is 4.278071184158325 and perplexity is 72.10123579503173
At time: 302.3838837146759 and batch: 200, loss is 4.280678749084473 and perplexity is 72.2894897842047
At time: 303.6837592124939 and batch: 250, loss is 4.352097835540771 and perplexity is 77.64117060573537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9447895050048825 and perplexity of 140.44128555216494
Finished 42 epochs...
Completing Train Step...
At time: 305.70598793029785 and batch: 50, loss is 4.315536727905274 and perplexity is 74.85378878130442
At time: 307.01021122932434 and batch: 100, loss is 4.2275096130371095 and perplexity is 68.54631259100252
At time: 308.310533285141 and batch: 150, loss is 4.276370573043823 and perplexity is 71.9787238341967
At time: 309.6121802330017 and batch: 200, loss is 4.278221139907837 and perplexity is 72.11204860058703
At time: 310.91460633277893 and batch: 250, loss is 4.349641561508179 and perplexity is 77.45069663841157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.944112014770508 and perplexity of 140.34617017620764
Finished 43 epochs...
Completing Train Step...
At time: 312.9333345890045 and batch: 50, loss is 4.313288774490356 and perplexity is 74.68570993867743
At time: 314.24872374534607 and batch: 100, loss is 4.224652290344238 and perplexity is 68.35073320629462
At time: 315.5621223449707 and batch: 150, loss is 4.27460057258606 and perplexity is 71.85143414469748
At time: 316.8641903400421 and batch: 200, loss is 4.276442632675171 and perplexity is 71.98391078138361
At time: 318.16918325424194 and batch: 250, loss is 4.347353200912476 and perplexity is 77.27366415045324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.943276977539062 and perplexity of 140.2290248159953
Finished 44 epochs...
Completing Train Step...
At time: 320.173264503479 and batch: 50, loss is 4.310464782714844 and perplexity is 74.47509563456309
At time: 321.49938917160034 and batch: 100, loss is 4.22177412033081 and perplexity is 68.15429100821471
At time: 322.80239844322205 and batch: 150, loss is 4.272548418045044 and perplexity is 71.70413508974505
At time: 324.1003682613373 and batch: 200, loss is 4.274576721191406 and perplexity is 71.84972040822282
At time: 325.4118342399597 and batch: 250, loss is 4.345340948104859 and perplexity is 77.11832634469516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9429679870605465 and perplexity of 140.1857020760174
Finished 45 epochs...
Completing Train Step...
At time: 327.4254343509674 and batch: 50, loss is 4.308244400024414 and perplexity is 74.30991587035534
At time: 328.73730969429016 and batch: 100, loss is 4.219632501602173 and perplexity is 68.00848668651557
At time: 330.0372200012207 and batch: 150, loss is 4.270596389770508 and perplexity is 71.56430311303626
At time: 331.3393247127533 and batch: 200, loss is 4.272171573638916 and perplexity is 71.677118878314
At time: 332.64265036582947 and batch: 250, loss is 4.342893180847168 and perplexity is 76.9297894718287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.942170333862305 and perplexity of 140.0739270871738
Finished 46 epochs...
Completing Train Step...
At time: 334.64326643943787 and batch: 50, loss is 4.305293884277344 and perplexity is 74.09098642962255
At time: 335.9586338996887 and batch: 100, loss is 4.216823406219483 and perplexity is 67.81771243756188
At time: 337.25715255737305 and batch: 150, loss is 4.268475437164307 and perplexity is 71.41267946795948
At time: 338.56197476387024 and batch: 200, loss is 4.270229539871216 and perplexity is 71.5380545705783
At time: 339.8621006011963 and batch: 250, loss is 4.34089035987854 and perplexity is 76.77586706732437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.941769027709961 and perplexity of 140.01772583615895
Finished 47 epochs...
Completing Train Step...
At time: 341.86237835884094 and batch: 50, loss is 4.302899484634399 and perplexity is 73.91379521611454
At time: 343.18057560920715 and batch: 100, loss is 4.21469162940979 and perplexity is 67.67329419931873
At time: 344.4844696521759 and batch: 150, loss is 4.266236715316772 and perplexity is 71.25298516450391
At time: 345.79803562164307 and batch: 200, loss is 4.268128261566162 and perplexity is 71.3878910314692
At time: 347.1114835739136 and batch: 250, loss is 4.338808813095093 and perplexity is 76.61622072149629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.941347122192383 and perplexity of 139.95866404519373
Finished 48 epochs...
Completing Train Step...
At time: 349.11363434791565 and batch: 50, loss is 4.300554294586181 and perplexity is 73.74065642019232
At time: 350.41518807411194 and batch: 100, loss is 4.2123623466491695 and perplexity is 67.51584740202756
At time: 351.7171492576599 and batch: 150, loss is 4.264247646331787 and perplexity is 71.11139892070966
At time: 353.02778792381287 and batch: 200, loss is 4.26621379852295 and perplexity is 71.25135229324381
At time: 354.33321380615234 and batch: 250, loss is 4.336975383758545 and perplexity is 76.47587898740906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.940983581542969 and perplexity of 139.90779262904968
Finished 49 epochs...
Completing Train Step...
At time: 356.37717747688293 and batch: 50, loss is 4.298142547607422 and perplexity is 73.56302689968776
At time: 357.7039542198181 and batch: 100, loss is 4.210268211364746 and perplexity is 67.37460802252528
At time: 359.0052213668823 and batch: 150, loss is 4.262196350097656 and perplexity is 70.96567788550246
At time: 360.3061282634735 and batch: 200, loss is 4.264258251190186 and perplexity is 71.11215305102445
At time: 361.6088342666626 and batch: 250, loss is 4.335095157623291 and perplexity is 76.33222213702466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.940716934204102 and perplexity of 139.87049156179458
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff00df36860>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 7.0133934861548335, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.2250128030850751, 'lr': 1.0860396985333176}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5423479080200195 and batch: 50, loss is 7.157717332839966 and perplexity is 1283.9766938011574
At time: 2.8752658367156982 and batch: 100, loss is 5.995640487670898 and perplexity is 401.67386878095965
At time: 4.182285308837891 and batch: 150, loss is 5.809985580444336 and perplexity is 333.61431506969814
At time: 5.501993656158447 and batch: 200, loss is 5.686286115646363 and perplexity is 294.7967440496127
At time: 6.820746421813965 and batch: 250, loss is 5.621063404083252 and perplexity is 276.1829211785022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.748830795288086 and perplexity of 313.82352175712197
Finished 1 epochs...
Completing Train Step...
At time: 8.92635726928711 and batch: 50, loss is 5.458508358001709 and perplexity is 234.74700458876677
At time: 10.229688167572021 and batch: 100, loss is 5.35518099784851 and perplexity is 211.70229054450965
At time: 11.52681589126587 and batch: 150, loss is 5.307416830062866 and perplexity is 201.82819789841912
At time: 12.824522733688354 and batch: 200, loss is 5.237809200286865 and perplexity is 188.25721643980694
At time: 14.124266147613525 and batch: 250, loss is 5.231351432800293 and perplexity is 187.04541209501429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.415417861938477 and perplexity of 224.84648083762758
Finished 2 epochs...
Completing Train Step...
At time: 16.145065784454346 and batch: 50, loss is 5.156519174575806 and perplexity is 173.55927341786088
At time: 17.44183087348938 and batch: 100, loss is 5.074604825973511 and perplexity is 159.90898761334014
At time: 18.74336004257202 and batch: 150, loss is 5.064471759796143 and perplexity is 158.29680124984142
At time: 20.045237064361572 and batch: 200, loss is 5.018812818527222 and perplexity is 151.23165775674224
At time: 21.338227033615112 and batch: 250, loss is 5.036298780441284 and perplexity is 153.8993443252905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.235683441162109 and perplexity of 187.85745199615556
Finished 3 epochs...
Completing Train Step...
At time: 23.404329538345337 and batch: 50, loss is 4.977380418777466 and perplexity is 145.09379839708336
At time: 24.72301721572876 and batch: 100, loss is 4.897613039016724 and perplexity is 133.96961747973342
At time: 26.027342796325684 and batch: 150, loss is 4.91005090713501 and perplexity is 135.64631959820736
At time: 27.325527667999268 and batch: 200, loss is 4.872554273605346 and perplexity is 130.6542176641756
At time: 28.622406482696533 and batch: 250, loss is 4.903163414001465 and perplexity is 134.71526649488337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.123638916015625 and perplexity of 167.9453982258771
Finished 4 epochs...
Completing Train Step...
At time: 30.62964415550232 and batch: 50, loss is 4.8489222240448 and perplexity is 127.60278849038096
At time: 31.929352283477783 and batch: 100, loss is 4.76832872390747 and perplexity is 117.72233094290448
At time: 33.22689890861511 and batch: 150, loss is 4.794849672317505 and perplexity is 120.88620787359245
At time: 34.52805757522583 and batch: 200, loss is 4.76425841331482 and perplexity is 117.24413834866847
At time: 35.83166837692261 and batch: 250, loss is 4.802682600021362 and perplexity is 121.83681897390031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.026185989379883 and perplexity of 152.35083548179378
Finished 5 epochs...
Completing Train Step...
At time: 37.83083415031433 and batch: 50, loss is 4.750659980773926 and perplexity is 115.66059311107286
At time: 39.1434850692749 and batch: 100, loss is 4.669565334320068 and perplexity is 106.65137466365813
At time: 40.44015717506409 and batch: 150, loss is 4.706866703033447 and perplexity is 110.70474507109138
At time: 41.73729848861694 and batch: 200, loss is 4.680199995040893 and perplexity is 107.79162820690972
At time: 43.036715030670166 and batch: 250, loss is 4.723246078491211 and perplexity is 112.53295122001755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.959600830078125 and perplexity of 142.5368881013169
Finished 6 epochs...
Completing Train Step...
At time: 45.02944803237915 and batch: 50, loss is 4.6718848609924315 and perplexity is 106.89904249686528
At time: 46.34805989265442 and batch: 100, loss is 4.59174241065979 and perplexity is 98.66619750816088
At time: 47.64398002624512 and batch: 150, loss is 4.637080726623535 and perplexity is 103.24251410766844
At time: 48.94050312042236 and batch: 200, loss is 4.610634241104126 and perplexity is 100.54790102914171
At time: 50.24052333831787 and batch: 250, loss is 4.658420715332031 and perplexity is 105.46938437799552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9124706268310545 and perplexity of 135.97494309746995
Finished 7 epochs...
Completing Train Step...
At time: 52.24958920478821 and batch: 50, loss is 4.607110977172852 and perplexity is 100.19426757389498
At time: 53.54962134361267 and batch: 100, loss is 4.527528495788574 and perplexity is 92.52959097459612
At time: 54.84277534484863 and batch: 150, loss is 4.5775050449371335 and perplexity is 97.2714034271105
At time: 56.144306659698486 and batch: 200, loss is 4.552879438400269 and perplexity is 94.90528918862753
At time: 57.460488080978394 and batch: 250, loss is 4.602719860076904 and perplexity is 99.75526736868507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.873265075683594 and perplexity of 130.7471199672866
Finished 8 epochs...
Completing Train Step...
At time: 59.45314955711365 and batch: 50, loss is 4.551221914291382 and perplexity is 94.74811168247503
At time: 60.7572877407074 and batch: 100, loss is 4.47120322227478 and perplexity is 87.46189582887106
At time: 62.056238651275635 and batch: 150, loss is 4.525340032577515 and perplexity is 92.32731478654318
At time: 63.35036087036133 and batch: 200, loss is 4.502225008010864 and perplexity is 90.2176431262676
At time: 64.65324473381042 and batch: 250, loss is 4.552469749450683 and perplexity is 94.86641550399295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.840453338623047 and perplexity of 126.52669816958159
Finished 9 epochs...
Completing Train Step...
At time: 66.67552471160889 and batch: 50, loss is 4.502714576721192 and perplexity is 90.26182167479686
At time: 67.97589302062988 and batch: 100, loss is 4.421550483703613 and perplexity is 83.22522472798751
At time: 69.26895475387573 and batch: 150, loss is 4.4794187831878665 and perplexity is 88.18340410100132
At time: 70.56909155845642 and batch: 200, loss is 4.456566696166992 and perplexity is 86.19108035598815
At time: 71.86778426170349 and batch: 250, loss is 4.508010587692261 and perplexity is 90.7411173299167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.811593246459961 and perplexity of 122.92731508823384
Finished 10 epochs...
Completing Train Step...
At time: 73.87533330917358 and batch: 50, loss is 4.459469575881958 and perplexity is 86.44164620006373
At time: 75.16910862922668 and batch: 100, loss is 4.377356462478637 and perplexity is 79.62725728818025
At time: 76.45531964302063 and batch: 150, loss is 4.438720474243164 and perplexity is 84.66653931901416
At time: 77.74973154067993 and batch: 200, loss is 4.415755691528321 and perplexity is 82.74434648732144
At time: 79.0451238155365 and batch: 250, loss is 4.46828218460083 and perplexity is 87.20678910568516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787266540527344 and perplexity of 119.97297877217062
Finished 11 epochs...
Completing Train Step...
At time: 81.0134813785553 and batch: 50, loss is 4.420140676498413 and perplexity is 83.1079758750703
At time: 82.31539511680603 and batch: 100, loss is 4.337394008636474 and perplexity is 76.507900394926
At time: 83.61163687705994 and batch: 150, loss is 4.402206602096558 and perplexity is 81.6307967643842
At time: 84.91432642936707 and batch: 200, loss is 4.379508771896362 and perplexity is 79.79882435042
At time: 86.21261692047119 and batch: 250, loss is 4.4323046684265135 and perplexity is 84.12507406886652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.765967559814453 and perplexity of 117.44469710065175
Finished 12 epochs...
Completing Train Step...
At time: 88.1938681602478 and batch: 50, loss is 4.384729566574097 and perplexity is 80.2165270491254
At time: 89.48936939239502 and batch: 100, loss is 4.3011157512664795 and perplexity is 73.782070229291
At time: 90.7975218296051 and batch: 150, loss is 4.36902533531189 and perplexity is 78.96662819532106
At time: 92.09148263931274 and batch: 200, loss is 4.346540451049805 and perplexity is 77.2108855056501
At time: 93.38938188552856 and batch: 250, loss is 4.399448614120484 and perplexity is 81.40597018544327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.748149490356445 and perplexity of 115.37059247460904
Finished 13 epochs...
Completing Train Step...
At time: 95.35903191566467 and batch: 50, loss is 4.352483901977539 and perplexity is 77.67115104266546
At time: 96.66515612602234 and batch: 100, loss is 4.267785215377808 and perplexity is 71.36340588756394
At time: 97.95776033401489 and batch: 150, loss is 4.338628368377686 and perplexity is 76.60239697644778
At time: 99.25519299507141 and batch: 200, loss is 4.3156904697418215 and perplexity is 74.8652978249523
At time: 100.55301880836487 and batch: 250, loss is 4.369379005432129 and perplexity is 78.9945612714658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.732291030883789 and perplexity of 113.55542354234322
Finished 14 epochs...
Completing Train Step...
At time: 102.54959654808044 and batch: 50, loss is 4.322566556930542 and perplexity is 75.38185204087262
At time: 103.86875772476196 and batch: 100, loss is 4.237086124420166 and perplexity is 69.20590036785514
At time: 105.17222547531128 and batch: 150, loss is 4.310267705917358 and perplexity is 74.46041976740695
At time: 106.47941422462463 and batch: 200, loss is 4.286825695037842 and perplexity is 72.73521790074251
At time: 107.78041195869446 and batch: 250, loss is 4.3415878391265865 and perplexity is 76.82943532055545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.716498565673828 and perplexity of 111.77618968955545
Finished 15 epochs...
Completing Train Step...
At time: 109.78422141075134 and batch: 50, loss is 4.294560222625733 and perplexity is 73.29997168664367
At time: 111.08419799804688 and batch: 100, loss is 4.20885440826416 and perplexity is 67.27942089658445
At time: 112.38508462905884 and batch: 150, loss is 4.283836374282837 and perplexity is 72.51811366302486
At time: 113.68119740486145 and batch: 200, loss is 4.259690351486206 and perplexity is 70.7880606424964
At time: 114.97764825820923 and batch: 250, loss is 4.315483560562134 and perplexity is 74.84980911002619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.700378799438477 and perplexity of 109.9888282684123
Finished 16 epochs...
Completing Train Step...
At time: 116.95909714698792 and batch: 50, loss is 4.268020067214966 and perplexity is 71.38016768273465
At time: 118.26988053321838 and batch: 100, loss is 4.182642421722412 and perplexity is 65.53880577060765
At time: 119.57670593261719 and batch: 150, loss is 4.258974475860596 and perplexity is 70.73740332962936
At time: 120.89202785491943 and batch: 200, loss is 4.234094333648682 and perplexity is 68.99916020965868
At time: 122.20242929458618 and batch: 250, loss is 4.290899734497071 and perplexity is 73.03214849131771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.685343170166016 and perplexity of 108.34744754066355
Finished 17 epochs...
Completing Train Step...
At time: 124.20354986190796 and batch: 50, loss is 4.242987880706787 and perplexity is 69.61554434586785
At time: 125.49718523025513 and batch: 100, loss is 4.1576838874816895 and perplexity is 63.923297463683795
At time: 126.792733669281 and batch: 150, loss is 4.235575122833252 and perplexity is 69.10140910568751
At time: 128.09293293952942 and batch: 200, loss is 4.210003919601441 and perplexity is 67.35680382142448
At time: 129.3903353214264 and batch: 250, loss is 4.2676840591430665 and perplexity is 71.35618739922974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.671784591674805 and perplexity of 106.88832434017928
Finished 18 epochs...
Completing Train Step...
At time: 131.38039183616638 and batch: 50, loss is 4.219377784729004 and perplexity is 67.99116598346914
At time: 132.68221712112427 and batch: 100, loss is 4.133898544311523 and perplexity is 62.420799447327546
At time: 133.98223280906677 and batch: 150, loss is 4.213315191268921 and perplexity is 67.58021017296291
At time: 135.2813367843628 and batch: 200, loss is 4.187252378463745 and perplexity is 65.84163430701359
At time: 136.5755970478058 and batch: 250, loss is 4.245721340179443 and perplexity is 69.80609592887205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6593994140625 and perplexity of 105.57265765906078
Finished 19 epochs...
Completing Train Step...
At time: 138.5590147972107 and batch: 50, loss is 4.197039098739624 and perplexity is 66.48917142840925
At time: 139.86738920211792 and batch: 100, loss is 4.1115038871765135 and perplexity is 61.03844351207219
At time: 141.16126942634583 and batch: 150, loss is 4.1923015832901 and perplexity is 66.17492291787859
At time: 142.45615768432617 and batch: 200, loss is 4.165766682624817 and perplexity is 64.4420701241763
At time: 143.7520136833191 and batch: 250, loss is 4.224932594299316 and perplexity is 68.3698948725648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.648003387451172 and perplexity of 104.37637820583997
Finished 20 epochs...
Completing Train Step...
At time: 145.74133443832397 and batch: 50, loss is 4.175827236175537 and perplexity is 65.09366522939486
At time: 147.03998136520386 and batch: 100, loss is 4.090188012123108 and perplexity is 59.75112458394446
At time: 148.342844247818 and batch: 150, loss is 4.172309718132019 and perplexity is 64.86509931546412
At time: 149.643248796463 and batch: 200, loss is 4.145403962135315 and perplexity is 63.14312418169543
At time: 150.94322323799133 and batch: 250, loss is 4.205360927581787 and perplexity is 67.04479161441928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.639395904541016 and perplexity of 103.48181580258654
Finished 21 epochs...
Completing Train Step...
At time: 152.95002603530884 and batch: 50, loss is 4.1558290243148805 and perplexity is 63.80483839036674
At time: 154.26318860054016 and batch: 100, loss is 4.069971704483033 and perplexity is 58.555305715804984
At time: 155.55976819992065 and batch: 150, loss is 4.153238000869751 and perplexity is 63.639732547108515
At time: 156.87234473228455 and batch: 200, loss is 4.126025094985962 and perplexity is 61.93126214849026
At time: 158.16239953041077 and batch: 250, loss is 4.18676176071167 and perplexity is 65.80933915533214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.63046875 and perplexity of 102.56212884233673
Finished 22 epochs...
Completing Train Step...
At time: 160.15691471099854 and batch: 50, loss is 4.136808886528015 and perplexity is 62.60272994673872
At time: 161.46859049797058 and batch: 100, loss is 4.050640110969543 and perplexity is 57.434209548910275
At time: 162.77277660369873 and batch: 150, loss is 4.134784564971924 and perplexity is 62.47613007369598
At time: 164.07554078102112 and batch: 200, loss is 4.107588386535644 and perplexity is 60.79991473192198
At time: 165.36875343322754 and batch: 250, loss is 4.169075837135315 and perplexity is 64.65567211727227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.622637176513672 and perplexity of 101.76204304807749
Finished 23 epochs...
Completing Train Step...
At time: 167.36941480636597 and batch: 50, loss is 4.1186989736557 and perplexity is 61.47920414564774
At time: 168.66955661773682 and batch: 100, loss is 4.032249813079834 and perplexity is 56.387630274747195
At time: 169.97097086906433 and batch: 150, loss is 4.117171692848205 and perplexity is 61.385379803392425
At time: 171.27329802513123 and batch: 200, loss is 4.089903292655944 and perplexity is 59.73411469722833
At time: 172.5687985420227 and batch: 250, loss is 4.152198486328125 and perplexity is 63.573612492033995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.615804290771484 and perplexity of 101.06908478344793
Finished 24 epochs...
Completing Train Step...
At time: 174.56140518188477 and batch: 50, loss is 4.1014180088043215 and perplexity is 60.425911349700336
At time: 175.87342476844788 and batch: 100, loss is 4.014583916664123 and perplexity is 55.40023948978068
At time: 177.17198586463928 and batch: 150, loss is 4.100167412757873 and perplexity is 60.35039017694626
At time: 178.47479820251465 and batch: 200, loss is 4.072887377738953 and perplexity is 58.726282990481074
At time: 179.7702226638794 and batch: 250, loss is 4.135995059013367 and perplexity is 62.551802848363764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.609415435791016 and perplexity of 100.42542736408325
Finished 25 epochs...
Completing Train Step...
At time: 181.78398537635803 and batch: 50, loss is 4.084824318885803 and perplexity is 59.431495842639656
At time: 183.09610557556152 and batch: 100, loss is 3.998149814605713 and perplexity is 54.49722672560253
At time: 184.39580154418945 and batch: 150, loss is 4.083964829444885 and perplexity is 59.38043704489688
At time: 185.70151662826538 and batch: 200, loss is 4.056508131027222 and perplexity is 57.772225414553525
At time: 186.99661660194397 and batch: 250, loss is 4.120392146110535 and perplexity is 61.58338721571495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6032958984375 and perplexity of 99.81274678294525
Finished 26 epochs...
Completing Train Step...
At time: 188.9992790222168 and batch: 50, loss is 4.068863496780396 and perplexity is 58.490450218298975
At time: 190.29511547088623 and batch: 100, loss is 3.981956372261047 and perplexity is 53.6218359509132
At time: 191.5919210910797 and batch: 150, loss is 4.06852014541626 and perplexity is 58.47037088975728
At time: 192.88676571846008 and batch: 200, loss is 4.040870313644409 and perplexity is 56.875821074657786
At time: 194.18506288528442 and batch: 250, loss is 4.105346503257752 and perplexity is 60.66376109709192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.597486877441407 and perplexity of 99.23461326183222
Finished 27 epochs...
Completing Train Step...
At time: 196.1734220981598 and batch: 50, loss is 4.0534979391098025 and perplexity is 57.59858141056732
At time: 197.48688101768494 and batch: 100, loss is 3.9665016651153566 and perplexity is 52.799497050881286
At time: 198.78578305244446 and batch: 150, loss is 4.053754215240478 and perplexity is 57.61334444376936
At time: 200.08063745498657 and batch: 200, loss is 4.025919585227967 and perplexity is 56.031811123423736
At time: 201.37942576408386 and batch: 250, loss is 4.090790405273437 and perplexity is 59.78712909546507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.592202377319336 and perplexity of 98.71159110840986
Finished 28 epochs...
Completing Train Step...
At time: 203.38537740707397 and batch: 50, loss is 4.038685545921326 and perplexity is 56.75169625786952
At time: 204.68581438064575 and batch: 100, loss is 3.951461730003357 and perplexity is 52.01133782978363
At time: 205.987149477005 and batch: 150, loss is 4.039577746391297 and perplexity is 56.802352742451795
At time: 207.27574634552002 and batch: 200, loss is 4.0115178060531616 and perplexity is 55.23063637146198
At time: 208.5743372440338 and batch: 250, loss is 4.076677708625794 and perplexity is 58.94929741700712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5875907897949215 and perplexity of 98.25742199154692
Finished 29 epochs...
Completing Train Step...
At time: 210.56682872772217 and batch: 50, loss is 4.02444019317627 and perplexity is 55.94897939281465
At time: 211.88301157951355 and batch: 100, loss is 3.9369391441345214 and perplexity is 51.26145699386616
At time: 213.1788773536682 and batch: 150, loss is 4.025925221443177 and perplexity is 56.03212693165982
At time: 214.47442603111267 and batch: 200, loss is 3.997734770774841 and perplexity is 54.47461268108492
At time: 215.76846027374268 and batch: 250, loss is 4.063058366775513 and perplexity is 58.151889196888085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.58349609375 and perplexity of 97.85591030947873
Finished 30 epochs...
Completing Train Step...
At time: 217.75920581817627 and batch: 50, loss is 4.010690894126892 and perplexity is 55.18498437724154
At time: 219.08156085014343 and batch: 100, loss is 3.922812275886536 and perplexity is 50.542384226480905
At time: 220.38391017913818 and batch: 150, loss is 4.01278130531311 and perplexity is 55.30046434415011
At time: 221.69282054901123 and batch: 200, loss is 3.9844835233688354 and perplexity is 53.75751780510132
At time: 222.9887239933014 and batch: 250, loss is 4.049871492385864 and perplexity is 57.390081509100256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.579817581176758 and perplexity of 97.49660736838997
Finished 31 epochs...
Completing Train Step...
At time: 224.9977650642395 and batch: 50, loss is 3.9973636531829833 and perplexity is 54.4543999448923
At time: 226.2962203025818 and batch: 100, loss is 3.9090715312957762 and perplexity is 49.852643859271026
At time: 227.59947204589844 and batch: 150, loss is 3.9999404382705688 and perplexity is 54.59489816974879
At time: 228.89399075508118 and batch: 200, loss is 3.9718477821350096 and perplexity is 53.08252521744202
At time: 230.1891269683838 and batch: 250, loss is 4.037321033477784 and perplexity is 56.67431067093865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.576232528686523 and perplexity of 97.14770270780994
Finished 32 epochs...
Completing Train Step...
At time: 232.18222999572754 and batch: 50, loss is 3.9843769884109497 and perplexity is 53.75179105526086
At time: 233.49683451652527 and batch: 100, loss is 3.8956862115859985 and perplexity is 49.1897963921281
At time: 234.79750752449036 and batch: 150, loss is 3.987555809020996 and perplexity is 53.92293022273861
At time: 236.09354758262634 and batch: 200, loss is 3.959574842453003 and perplexity is 52.435028061469986
At time: 237.38963723182678 and batch: 250, loss is 4.024801797866822 and perplexity is 55.9692144645226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.573123168945313 and perplexity of 96.84610468342711
Finished 33 epochs...
Completing Train Step...
At time: 239.39468836784363 and batch: 50, loss is 3.9718426656723023 and perplexity is 53.08225362337614
At time: 240.69279146194458 and batch: 100, loss is 3.882824559211731 and perplexity is 48.56118548355153
At time: 241.9913158416748 and batch: 150, loss is 3.97553382396698 and perplexity is 53.27855068302505
At time: 243.2853786945343 and batch: 200, loss is 3.9477437353134155 and perplexity is 51.81831899576507
At time: 244.5858223438263 and batch: 250, loss is 4.01269760131836 and perplexity is 55.29583566809498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5704700469970705 and perplexity of 96.58950070882906
Finished 34 epochs...
Completing Train Step...
At time: 246.61700797080994 and batch: 50, loss is 3.9596554374694826 and perplexity is 52.439254233722636
At time: 247.92459630966187 and batch: 100, loss is 3.870402216911316 and perplexity is 47.96167319781208
At time: 249.22446537017822 and batch: 150, loss is 3.963885087966919 and perplexity is 52.66152368115138
At time: 250.5250539779663 and batch: 200, loss is 3.9362821626663207 and perplexity is 51.22779022702013
At time: 251.82453441619873 and batch: 250, loss is 4.001117444038391 and perplexity is 54.65919451094775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.568290328979492 and perplexity of 96.37919212379877
Finished 35 epochs...
Completing Train Step...
At time: 253.808753490448 and batch: 50, loss is 3.9477657794952394 and perplexity is 51.819461300801365
At time: 255.12647795677185 and batch: 100, loss is 3.858414125442505 and perplexity is 47.39013693184979
At time: 256.42164278030396 and batch: 150, loss is 3.9525406551361084 and perplexity is 52.067484452911025
At time: 257.7236297130585 and batch: 200, loss is 3.925211019515991 and perplexity is 50.66376797470483
At time: 259.02305364608765 and batch: 250, loss is 3.9896970129013063 and perplexity is 54.038513910128984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.566064834594727 and perplexity of 96.16493927060964
Finished 36 epochs...
Completing Train Step...
At time: 261.03045988082886 and batch: 50, loss is 3.9362646913528443 and perplexity is 51.22689521805689
At time: 262.3379981517792 and batch: 100, loss is 3.8468249225616455 and perplexity is 46.84409323753571
At time: 263.6310820579529 and batch: 150, loss is 3.9415598154067992 and perplexity is 51.498867410915246
At time: 264.9278402328491 and batch: 200, loss is 3.9144835472106934 and perplexity is 50.12317856999014
At time: 266.22641372680664 and batch: 250, loss is 3.9788425254821775 and perplexity is 53.45512545992569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5643352508544925 and perplexity of 95.99875770915702
Finished 37 epochs...
Completing Train Step...
At time: 268.21987867355347 and batch: 50, loss is 3.925002374649048 and perplexity is 50.653198342265036
At time: 269.5330009460449 and batch: 100, loss is 3.835537543296814 and perplexity is 46.31831907918615
At time: 270.82797169685364 and batch: 150, loss is 3.9308019876480103 and perplexity is 50.9478208121486
At time: 272.1210947036743 and batch: 200, loss is 3.9041882085800172 and perplexity is 49.6097907587101
At time: 273.41648268699646 and batch: 250, loss is 3.9681790971755984 and perplexity is 52.88813894459138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.562768173217774 and perplexity of 95.84843801487591
Finished 38 epochs...
Completing Train Step...
At time: 275.3956186771393 and batch: 50, loss is 3.91414231300354 and perplexity is 50.10607774475001
At time: 276.7186405658722 and batch: 100, loss is 3.824686732292175 and perplexity is 45.81844467872359
At time: 278.020708322525 and batch: 150, loss is 3.920302972793579 and perplexity is 50.415717055042904
At time: 279.32783603668213 and batch: 200, loss is 3.894204273223877 and perplexity is 49.11695413302973
At time: 280.63007164001465 and batch: 250, loss is 3.9578092432022096 and perplexity is 52.342530496054295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5614768981933596 and perplexity of 95.72475119477522
Finished 39 epochs...
Completing Train Step...
At time: 282.6464915275574 and batch: 50, loss is 3.9035469007492067 and perplexity is 49.577985810882765
At time: 283.94908714294434 and batch: 100, loss is 3.8141164779663086 and perplexity is 45.33668272445243
At time: 285.2445373535156 and batch: 150, loss is 3.910036840438843 and perplexity is 49.90079030655865
At time: 286.5473442077637 and batch: 200, loss is 3.8845179796218874 and perplexity is 48.643489654303615
At time: 287.86010813713074 and batch: 250, loss is 3.947852988243103 and perplexity is 51.82398060819508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.560319900512695 and perplexity of 95.61406192562234
Finished 40 epochs...
Completing Train Step...
At time: 289.8533618450165 and batch: 50, loss is 3.893337354660034 and perplexity is 49.07439218522932
At time: 291.1630051136017 and batch: 100, loss is 3.8040285444259645 and perplexity is 44.88162842007516
At time: 292.453693151474 and batch: 150, loss is 3.8999870347976686 and perplexity is 49.401808596934025
At time: 293.75348138809204 and batch: 200, loss is 3.875003204345703 and perplexity is 48.18285268536841
At time: 295.0574154853821 and batch: 250, loss is 3.938000326156616 and perplexity is 51.31588360361681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.559208679199219 and perplexity of 95.50787255301655
Finished 41 epochs...
Completing Train Step...
At time: 297.0624351501465 and batch: 50, loss is 3.8833288669586183 and perplexity is 48.58568144182052
At time: 298.37272667884827 and batch: 100, loss is 3.7942531394958494 and perplexity is 44.44502976936924
At time: 299.67481899261475 and batch: 150, loss is 3.8901464891433717 and perplexity is 48.91805196241697
At time: 300.97012400627136 and batch: 200, loss is 3.8658342456817625 and perplexity is 47.743085286263174
At time: 302.27600860595703 and batch: 250, loss is 3.928457193374634 and perplexity is 50.828498601355605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5585273742675785 and perplexity of 95.4428247296533
Finished 42 epochs...
Completing Train Step...
At time: 304.29161047935486 and batch: 50, loss is 3.873660821914673 and perplexity is 48.11821626355137
At time: 305.58896684646606 and batch: 100, loss is 3.784864597320557 and perplexity is 44.02970841461944
At time: 306.8833737373352 and batch: 150, loss is 3.880793604850769 and perplexity is 48.46266001635016
At time: 308.18423438072205 and batch: 200, loss is 3.8566365146636965 and perplexity is 47.30597054333335
At time: 309.4857976436615 and batch: 250, loss is 3.919032430648804 and perplexity is 50.3517024370119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.557669830322266 and perplexity of 95.36101339660304
Finished 43 epochs...
Completing Train Step...
At time: 311.4716238975525 and batch: 50, loss is 3.864274697303772 and perplexity is 47.66868566502853
At time: 312.7839059829712 and batch: 100, loss is 3.775734510421753 and perplexity is 43.62954290337285
At time: 314.0804274082184 and batch: 150, loss is 3.8714449071884154 and perplexity is 48.011708449230746
At time: 315.3848977088928 and batch: 200, loss is 3.847629437446594 and perplexity is 46.881795171667285
At time: 316.6954686641693 and batch: 250, loss is 3.9097714948654176 and perplexity is 49.88755110929926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.557435607910156 and perplexity of 95.33868032557903
Finished 44 epochs...
Completing Train Step...
At time: 318.70019364356995 and batch: 50, loss is 3.855241451263428 and perplexity is 47.24002172729974
At time: 319.9946253299713 and batch: 100, loss is 3.7670048570632932 and perplexity is 43.25032972565062
At time: 321.3042001724243 and batch: 150, loss is 3.862432909011841 and perplexity is 47.580970838744385
At time: 322.6015205383301 and batch: 200, loss is 3.8388965034484865 and perplexity is 46.474162055913645
At time: 323.89699721336365 and batch: 250, loss is 3.9008650016784667 and perplexity is 49.445200794402474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.556668090820312 and perplexity of 95.26553433309641
Finished 45 epochs...
Completing Train Step...
At time: 325.88911294937134 and batch: 50, loss is 3.846358647346497 and perplexity is 46.82225608933905
At time: 327.2002408504486 and batch: 100, loss is 3.7585591316223144 and perplexity is 42.88658751114369
At time: 328.500949382782 and batch: 150, loss is 3.853834238052368 and perplexity is 47.17359169620534
At time: 329.80418729782104 and batch: 200, loss is 3.830233025550842 and perplexity is 46.07327323327564
At time: 331.1145966053009 and batch: 250, loss is 3.8920860624313356 and perplexity is 49.01302418231956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.55657730102539 and perplexity of 95.25688558738604
Finished 46 epochs...
Completing Train Step...
At time: 333.1022849082947 and batch: 50, loss is 3.8377448081970216 and perplexity is 46.420668794038676
At time: 334.4243516921997 and batch: 100, loss is 3.7502130699157714 and perplexity is 42.53014292869442
At time: 335.7289299964905 and batch: 150, loss is 3.8454991245269774 and perplexity is 46.78202858247555
At time: 337.03056740760803 and batch: 200, loss is 3.8217060661315916 and perplexity is 45.68207852309252
At time: 338.33526730537415 and batch: 250, loss is 3.88342942237854 and perplexity is 48.59056724106277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.556338500976563 and perplexity of 95.23414095427444
Finished 47 epochs...
Completing Train Step...
At time: 340.3479382991791 and batch: 50, loss is 3.829329628944397 and perplexity is 46.03166958971357
At time: 341.64878702163696 and batch: 100, loss is 3.7420528984069823 and perplexity is 42.184501831392005
At time: 342.944708108902 and batch: 150, loss is 3.837557864189148 and perplexity is 46.41199153927182
At time: 344.24694657325745 and batch: 200, loss is 3.8133744287490843 and perplexity is 45.303053153466756
At time: 345.55800700187683 and batch: 250, loss is 3.874996109008789 and perplexity is 48.18251081300798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5563404083251955 and perplexity of 95.23432259915622
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 347.55235528945923 and batch: 50, loss is 3.8293395137786863 and perplexity is 46.03212460738841
At time: 348.8826484680176 and batch: 100, loss is 3.7308418416976927 and perplexity is 41.71421014828837
At time: 350.1891186237335 and batch: 150, loss is 3.8144331407546996 and perplexity is 45.351041438135525
At time: 351.48432636260986 and batch: 200, loss is 3.7806735181808473 and perplexity is 43.845562576591256
At time: 352.7833745479584 and batch: 250, loss is 3.8246184539794923 and perplexity is 45.81531637942988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.524247360229492 and perplexity of 92.22648637894706
Finished 49 epochs...
Completing Train Step...
At time: 354.7867991924286 and batch: 50, loss is 3.813644595146179 and perplexity is 45.315294169595774
At time: 356.0774004459381 and batch: 100, loss is 3.7190213441848754 and perplexity is 41.224030223755726
At time: 357.3851034641266 and batch: 150, loss is 3.8064368772506714 and perplexity is 44.9898485818024
At time: 358.68443536758423 and batch: 200, loss is 3.7765995264053345 and perplexity is 43.66729948300878
At time: 359.9857699871063 and batch: 250, loss is 3.825691828727722 and perplexity is 45.864519785236304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.523151779174805 and perplexity of 92.12550011714865
Finished Training.
Improved accuracyfrom -92.59020558782429 to -92.12550011714865
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff04a0b9e10>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 5.991862107096926, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.4795089303813866, 'lr': 6.925394387142077}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.539557933807373 and batch: 50, loss is 6.730087804794311 and perplexity is 837.2207747248024
At time: 2.869967222213745 and batch: 100, loss is 5.815023555755615 and perplexity is 335.2992966363725
At time: 4.186319828033447 and batch: 150, loss is 5.48054988861084 and perplexity is 239.97863261575745
At time: 5.500527381896973 and batch: 200, loss is 5.33206805229187 and perplexity is 206.86534043748503
At time: 6.820795059204102 and batch: 250, loss is 5.288647441864014 and perplexity is 198.07533573992674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.228650665283203 and perplexity of 186.54092747629159
Finished 1 epochs...
Completing Train Step...
At time: 8.852577447891235 and batch: 50, loss is 4.919699306488037 and perplexity is 136.96142358259556
At time: 10.156200885772705 and batch: 100, loss is 4.718678531646728 and perplexity is 112.02012376705036
At time: 11.458112001419067 and batch: 150, loss is 4.678423643112183 and perplexity is 107.60032230385475
At time: 12.768991470336914 and batch: 200, loss is 4.599665546417237 and perplexity is 99.45104831963627
At time: 14.079690933227539 and batch: 250, loss is 4.595417442321778 and perplexity is 99.02946601074143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7980812072753904 and perplexity of 121.27748775706773
Finished 2 epochs...
Completing Train Step...
At time: 16.06382393836975 and batch: 50, loss is 4.470533189773559 and perplexity is 87.40331314439034
At time: 17.35907292366028 and batch: 100, loss is 4.341653623580933 and perplexity is 76.83448966928269
At time: 18.65603995323181 and batch: 150, loss is 4.370481224060058 and perplexity is 79.08167855072854
At time: 19.9574773311615 and batch: 200, loss is 4.334403553009033 and perplexity is 76.27944867126787
At time: 21.258394479751587 and batch: 250, loss is 4.347903499603271 and perplexity is 77.31619944915438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.690866088867187 and perplexity of 108.94749717231926
Finished 3 epochs...
Completing Train Step...
At time: 23.231024742126465 and batch: 50, loss is 4.263902978897095 and perplexity is 71.08689336064312
At time: 24.550257444381714 and batch: 100, loss is 4.156663274765014 and perplexity is 63.85808981493861
At time: 25.851993560791016 and batch: 150, loss is 4.2024374675750735 and perplexity is 66.84907507161172
At time: 27.157607078552246 and batch: 200, loss is 4.180901336669922 and perplexity is 65.42479641431586
At time: 28.459829092025757 and batch: 250, loss is 4.201624355316162 and perplexity is 66.7947413618704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.637137222290039 and perplexity of 103.2483470270803
Finished 4 epochs...
Completing Train Step...
At time: 30.486177921295166 and batch: 50, loss is 4.131163005828857 and perplexity is 62.250278288253554
At time: 31.78230905532837 and batch: 100, loss is 4.03093834400177 and perplexity is 56.3137281120495
At time: 33.077284812927246 and batch: 150, loss is 4.089385919570923 and perplexity is 59.703217867309675
At time: 34.37740707397461 and batch: 200, loss is 4.070066137313843 and perplexity is 58.560835520176106
At time: 35.68304181098938 and batch: 250, loss is 4.102009582519531 and perplexity is 60.46166830596734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.621132278442383 and perplexity of 101.60901671915775
Finished 5 epochs...
Completing Train Step...
At time: 37.68003726005554 and batch: 50, loss is 4.032107648849487 and perplexity is 56.37961454047694
At time: 38.99083876609802 and batch: 100, loss is 3.9385803079605104 and perplexity is 51.34565451481654
At time: 40.29217171669006 and batch: 150, loss is 4.001738042831421 and perplexity is 54.6931264690663
At time: 41.593069314956665 and batch: 200, loss is 3.986930685043335 and perplexity is 53.889232239916545
At time: 42.895429372787476 and batch: 250, loss is 4.024686717987061 and perplexity is 55.962773904648266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.62596321105957 and perplexity of 102.101070614875
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 44.88068962097168 and batch: 50, loss is 3.9488789510726927 and perplexity is 51.87717737026449
At time: 46.19975757598877 and batch: 100, loss is 3.791662502288818 and perplexity is 44.330037837083324
At time: 47.49648070335388 and batch: 150, loss is 3.824178147315979 and perplexity is 45.79514803079203
At time: 48.79991102218628 and batch: 200, loss is 3.7585087203979493 and perplexity is 42.884425600251156
At time: 50.1015191078186 and batch: 250, loss is 3.7532073259353638 and perplexity is 42.657679909071604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.497365188598633 and perplexity of 89.7802653222744
Finished 7 epochs...
Completing Train Step...
At time: 52.110008239746094 and batch: 50, loss is 3.8439478731155394 and perplexity is 46.70951415320388
At time: 53.40892004966736 and batch: 100, loss is 3.7175669479370117 and perplexity is 41.16411772769213
At time: 54.702563762664795 and batch: 150, loss is 3.770095233917236 and perplexity is 43.38419628607987
At time: 56.00324511528015 and batch: 200, loss is 3.724131898880005 and perplexity is 41.43524714306788
At time: 57.31751227378845 and batch: 250, loss is 3.741559829711914 and perplexity is 42.16370710115844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.486962890625 and perplexity of 88.85118491366354
Finished 8 epochs...
Completing Train Step...
At time: 59.306273221969604 and batch: 50, loss is 3.801621036529541 and perplexity is 44.77370551000619
At time: 60.61768579483032 and batch: 100, loss is 3.6795410966873168 and perplexity is 39.62820438496991
At time: 61.922515630722046 and batch: 150, loss is 3.740181794166565 and perplexity is 42.105644029725916
At time: 63.22857689857483 and batch: 200, loss is 3.7035794973373415 and perplexity is 40.59234480941351
At time: 64.52438592910767 and batch: 250, loss is 3.729707775115967 and perplexity is 41.66693027090782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.48394889831543 and perplexity of 88.58379128922174
Finished 9 epochs...
Completing Train Step...
At time: 66.5212676525116 and batch: 50, loss is 3.7705573272705077 and perplexity is 43.40424846745375
At time: 67.81808376312256 and batch: 100, loss is 3.651318793296814 and perplexity is 38.52543965330037
At time: 69.11966562271118 and batch: 150, loss is 3.717372107505798 and perplexity is 41.156098074545255
At time: 70.42269730567932 and batch: 200, loss is 3.6865700340270995 and perplexity is 39.9077297828826
At time: 71.71912693977356 and batch: 250, loss is 3.717360939979553 and perplexity is 41.15563846530621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.4826007843017575 and perplexity of 88.46445069923563
Finished 10 epochs...
Completing Train Step...
At time: 73.71992254257202 and batch: 50, loss is 3.745122356414795 and perplexity is 42.31418431399358
At time: 75.01751255989075 and batch: 100, loss is 3.627706289291382 and perplexity is 37.62641345566455
At time: 76.31778001785278 and batch: 150, loss is 3.697896294593811 and perplexity is 40.36230458580942
At time: 77.61830854415894 and batch: 200, loss is 3.671180734634399 and perplexity is 39.298279317005594
At time: 78.91924929618835 and batch: 250, loss is 3.7049365043640137 and perplexity is 40.64746629821409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.483646011352539 and perplexity of 88.55696447666773
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 80.9607400894165 and batch: 50, loss is 3.730589733123779 and perplexity is 41.70369496379573
At time: 82.2780659198761 and batch: 100, loss is 3.6019449281692504 and perplexity is 36.66948464659557
At time: 83.57764387130737 and batch: 150, loss is 3.6591316413879396 and perplexity is 38.827611936967145
At time: 84.8828239440918 and batch: 200, loss is 3.6215377330780028 and perplexity is 37.39502720241823
At time: 86.17269563674927 and batch: 250, loss is 3.6445422697067262 and perplexity is 38.26525367654774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.4693855285644535 and perplexity of 87.30306129093258
Finished 12 epochs...
Completing Train Step...
At time: 88.18293619155884 and batch: 50, loss is 3.712640976905823 and perplexity is 40.96184308477598
At time: 89.48499798774719 and batch: 100, loss is 3.5887590885162353 and perplexity is 36.189140531745565
At time: 90.81004309654236 and batch: 150, loss is 3.650908784866333 and perplexity is 38.50964713600838
At time: 92.108731508255 and batch: 200, loss is 3.6178805112838743 and perplexity is 37.25851507366494
At time: 93.40638065338135 and batch: 250, loss is 3.6459941482543945 and perplexity is 38.320850527653235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.468337631225586 and perplexity of 87.21162456185046
Finished 13 epochs...
Completing Train Step...
At time: 95.3883490562439 and batch: 50, loss is 3.7056285572052 and perplexity is 40.67560622878915
At time: 96.70176911354065 and batch: 100, loss is 3.5823549318313597 and perplexity is 35.95812014045459
At time: 98.00817441940308 and batch: 150, loss is 3.6461050128936767 and perplexity is 38.32509919043285
At time: 99.3146824836731 and batch: 200, loss is 3.615713620185852 and perplexity is 37.17786733801274
At time: 100.61037230491638 and batch: 250, loss is 3.6458658361434937 and perplexity is 38.315933813874224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.467943572998047 and perplexity of 87.17726487395835
Finished 14 epochs...
Completing Train Step...
At time: 102.59031987190247 and batch: 50, loss is 3.6999744606018066 and perplexity is 40.44627137344647
At time: 103.9030122756958 and batch: 100, loss is 3.5772556829452515 and perplexity is 35.77522744038339
At time: 105.20414805412292 and batch: 150, loss is 3.6421436929702757 and perplexity is 38.1735815145728
At time: 106.51084232330322 and batch: 200, loss is 3.613699049949646 and perplexity is 37.103045305453264
At time: 107.8106484413147 and batch: 250, loss is 3.6450403785705565 and perplexity is 38.28431868641175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.467801666259765 and perplexity of 87.16489471037329
Finished 15 epochs...
Completing Train Step...
At time: 109.80396175384521 and batch: 50, loss is 3.6949798536300658 and perplexity is 40.24476179394538
At time: 111.10569667816162 and batch: 100, loss is 3.5727634191513062 and perplexity is 35.61487612130847
At time: 112.4180634021759 and batch: 150, loss is 3.6385644769668577 and perplexity is 38.03719424605745
At time: 113.72228908538818 and batch: 200, loss is 3.6116797590255736 and perplexity is 37.02819905642055
At time: 115.02706933021545 and batch: 250, loss is 3.6438440179824827 and perplexity is 38.238544223235586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.467791748046875 and perplexity of 87.16403019467825
Finished 16 epochs...
Completing Train Step...
At time: 117.01885151863098 and batch: 50, loss is 3.690394949913025 and perplexity is 40.060665789727224
At time: 118.34534430503845 and batch: 100, loss is 3.5686331844329833 and perplexity is 35.46808167989117
At time: 119.64885210990906 and batch: 150, loss is 3.6352022457122803 and perplexity is 37.90951915958629
At time: 120.95682168006897 and batch: 200, loss is 3.6096337842941284 and perplexity is 36.952517744226625
At time: 122.25402021408081 and batch: 250, loss is 3.6424271488189697 and perplexity is 38.18440357323415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.467861938476562 and perplexity of 87.17014849013125
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 124.27803039550781 and batch: 50, loss is 3.6883260774612427 and perplexity is 39.977871057277085
At time: 125.57739806175232 and batch: 100, loss is 3.5644119501113893 and perplexity is 35.31867815168291
At time: 126.87619972229004 and batch: 150, loss is 3.629102334976196 and perplexity is 37.678978330750724
At time: 128.1785979270935 and batch: 200, loss is 3.598761920928955 and perplexity is 36.5529509736187
At time: 129.48316526412964 and batch: 250, loss is 3.6269482278823855 and perplexity is 37.597901132076366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466968154907226 and perplexity of 87.09227205119926
Finished 18 epochs...
Completing Train Step...
At time: 131.5264344215393 and batch: 50, loss is 3.6849638748168947 and perplexity is 39.84368306351625
At time: 132.82885909080505 and batch: 100, loss is 3.561457509994507 and perplexity is 35.21448522374861
At time: 134.13098216056824 and batch: 150, loss is 3.626988396644592 and perplexity is 37.599411423559474
At time: 135.43649649620056 and batch: 200, loss is 3.597988972663879 and perplexity is 36.52470835002982
At time: 136.73757004737854 and batch: 250, loss is 3.628147759437561 and perplexity is 37.643028061069316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466749954223633 and perplexity of 87.0732705310497
Finished 19 epochs...
Completing Train Step...
At time: 138.7322962284088 and batch: 50, loss is 3.6833069133758545 and perplexity is 39.777718282652465
At time: 140.04922747612 and batch: 100, loss is 3.559939408302307 and perplexity is 35.161066611842244
At time: 141.35049200057983 and batch: 150, loss is 3.625933499336243 and perplexity is 37.559768818768006
At time: 142.66067838668823 and batch: 200, loss is 3.59766233921051 and perplexity is 36.512780106595876
At time: 143.95829057693481 and batch: 250, loss is 3.628718738555908 and perplexity is 37.664527581348004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466706085205078 and perplexity of 87.06945079591378
Finished 20 epochs...
Completing Train Step...
At time: 145.9609591960907 and batch: 50, loss is 3.6820758962631226 and perplexity is 39.72878135801685
At time: 147.25839138031006 and batch: 100, loss is 3.5588180208206177 and perplexity is 35.12165953133383
At time: 148.56783246994019 and batch: 150, loss is 3.6251687383651734 and perplexity is 37.53105555428447
At time: 149.87926936149597 and batch: 200, loss is 3.5973924160003663 and perplexity is 36.50292578979247
At time: 151.1739010810852 and batch: 250, loss is 3.6289656734466553 and perplexity is 37.67382941577782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466706848144531 and perplexity of 87.06951722465828
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 153.16029024124146 and batch: 50, loss is 3.6813828897476197 and perplexity is 39.70125859151252
At time: 154.47323536872864 and batch: 100, loss is 3.557348566055298 and perplexity is 35.070087741855374
At time: 155.76662254333496 and batch: 150, loss is 3.623754448890686 and perplexity is 37.478013294843585
At time: 157.07336473464966 and batch: 200, loss is 3.5948213720321656 and perplexity is 36.40919570633354
At time: 158.37442255020142 and batch: 250, loss is 3.6255352449417115 and perplexity is 37.54481345399589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466646957397461 and perplexity of 87.06430272237644
Finished 22 epochs...
Completing Train Step...
At time: 160.420654296875 and batch: 50, loss is 3.681068344116211 and perplexity is 39.688772697855725
At time: 161.7474286556244 and batch: 100, loss is 3.557097234725952 and perplexity is 35.061274637633936
At time: 163.05624985694885 and batch: 150, loss is 3.6235741996765136 and perplexity is 37.47125852118806
At time: 164.35400867462158 and batch: 200, loss is 3.5947967386245727 and perplexity is 36.408298834822126
At time: 165.65198826789856 and batch: 250, loss is 3.625675024986267 and perplexity is 37.55006183649434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466602325439453 and perplexity of 87.06041695878864
Finished 23 epochs...
Completing Train Step...
At time: 167.64385080337524 and batch: 50, loss is 3.680791697502136 and perplexity is 39.67779445188939
At time: 168.94381046295166 and batch: 100, loss is 3.556872534751892 and perplexity is 35.0533972551893
At time: 170.24043679237366 and batch: 150, loss is 3.6234197759628297 and perplexity is 37.46547251705043
At time: 171.53718781471252 and batch: 200, loss is 3.5947758531570435 and perplexity is 36.407538438419664
At time: 172.8403832912445 and batch: 250, loss is 3.6257881212234495 and perplexity is 37.554308847349965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.46656608581543 and perplexity of 87.0572619791787
Finished 24 epochs...
Completing Train Step...
At time: 174.82331800460815 and batch: 50, loss is 3.6805392503738403 and perplexity is 39.667779170840554
At time: 176.13637351989746 and batch: 100, loss is 3.5566643333435057 and perplexity is 35.04609984820362
At time: 177.43425846099854 and batch: 150, loss is 3.6232816791534423 and perplexity is 37.46029901206412
At time: 178.7343647480011 and batch: 200, loss is 3.594754738807678 and perplexity is 36.40676972504901
At time: 180.030695438385 and batch: 250, loss is 3.6258797454833984 and perplexity is 37.55774989074512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466538619995117 and perplexity of 87.05487091290075
Finished 25 epochs...
Completing Train Step...
At time: 182.06497025489807 and batch: 50, loss is 3.6803047466278076 and perplexity is 39.65847801864833
At time: 183.3709774017334 and batch: 100, loss is 3.556467833518982 and perplexity is 35.03921397229206
At time: 184.66724681854248 and batch: 150, loss is 3.6231542110443113 and perplexity is 37.455524322898334
At time: 185.96760320663452 and batch: 200, loss is 3.5947314643859865 and perplexity is 36.40592238839867
At time: 187.26831364631653 and batch: 250, loss is 3.625954065322876 and perplexity is 37.560541280414675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466517639160156 and perplexity of 87.05304444818202
Finished 26 epochs...
Completing Train Step...
At time: 189.26756811141968 and batch: 50, loss is 3.68008367061615 and perplexity is 39.649711449574326
At time: 190.56467843055725 and batch: 100, loss is 3.5562801122665406 and perplexity is 35.03263698450013
At time: 191.86713218688965 and batch: 150, loss is 3.6230343627929686 and perplexity is 37.451035612792516
At time: 193.1655888557434 and batch: 200, loss is 3.5947060203552246 and perplexity is 36.40499608677398
At time: 194.475594997406 and batch: 250, loss is 3.6260146141052245 and perplexity is 37.56281559430632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466501235961914 and perplexity of 87.05161651154773
Finished 27 epochs...
Completing Train Step...
At time: 196.50493502616882 and batch: 50, loss is 3.679873571395874 and perplexity is 39.6413819511558
At time: 197.87491106987 and batch: 100, loss is 3.556099576950073 and perplexity is 35.026312927170466
At time: 199.19593024253845 and batch: 150, loss is 3.622920050621033 and perplexity is 37.446754748252495
At time: 200.49661231040955 and batch: 200, loss is 3.594678020477295 and perplexity is 36.403976765598024
At time: 201.79783058166504 and batch: 250, loss is 3.6260636138916014 and perplexity is 37.56465620934066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466491317749023 and perplexity of 87.05075311936433
Finished 28 epochs...
Completing Train Step...
At time: 203.82774233818054 and batch: 50, loss is 3.6796722078323363 and perplexity is 39.63340042484382
At time: 205.13516783714294 and batch: 100, loss is 3.5559252071380616 and perplexity is 35.020205928023515
At time: 206.4396035671234 and batch: 150, loss is 3.6228099632263184 and perplexity is 37.44263255948643
At time: 207.74070119857788 and batch: 200, loss is 3.5946478986740114 and perplexity is 36.402880228686065
At time: 209.0481185913086 and batch: 250, loss is 3.626103253364563 and perplexity is 37.56614528202763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466484451293946 and perplexity of 87.05015539133072
Finished 29 epochs...
Completing Train Step...
At time: 211.04921555519104 and batch: 50, loss is 3.6794782972335813 and perplexity is 39.625715833522676
At time: 212.3663809299469 and batch: 100, loss is 3.5557557678222658 and perplexity is 35.01427263097314
At time: 213.680162191391 and batch: 150, loss is 3.6227031803131102 and perplexity is 37.438634539567445
At time: 214.98415565490723 and batch: 200, loss is 3.5946153354644776 and perplexity is 36.40169485336946
At time: 216.28723645210266 and batch: 250, loss is 3.6261346912384034 and perplexity is 37.56732630032792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.4664794921875 and perplexity of 87.04972370141444
Finished 30 epochs...
Completing Train Step...
At time: 218.29485201835632 and batch: 50, loss is 3.6792907190322874 and perplexity is 39.61828361010498
At time: 219.6110372543335 and batch: 100, loss is 3.5555906295776367 and perplexity is 35.00849091285842
At time: 220.9116804599762 and batch: 150, loss is 3.622599081993103 and perplexity is 37.43473744345261
At time: 222.22307229042053 and batch: 200, loss is 3.5945809507369995 and perplexity is 36.40044321253097
At time: 223.52992129325867 and batch: 250, loss is 3.6261594772338865 and perplexity is 37.56825745544767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466476821899414 and perplexity of 87.04949125388472
Finished 31 epochs...
Completing Train Step...
At time: 225.54356408119202 and batch: 50, loss is 3.6791084575653077 and perplexity is 39.61106338161969
At time: 226.84267115592957 and batch: 100, loss is 3.555429310798645 and perplexity is 35.00284384135161
At time: 228.14722681045532 and batch: 150, loss is 3.6224972677230833 and perplexity is 37.43092624700681
At time: 229.44817233085632 and batch: 200, loss is 3.5945448207855226 and perplexity is 36.39912809004177
At time: 230.75514030456543 and batch: 250, loss is 3.626178460121155 and perplexity is 37.56897061621272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466476821899414 and perplexity of 87.04949125388472
Finished 32 epochs...
Completing Train Step...
At time: 232.7601659297943 and batch: 50, loss is 3.6789307975769043 and perplexity is 39.60402670564506
At time: 234.0770332813263 and batch: 100, loss is 3.5552712631225587 and perplexity is 34.997312160372246
At time: 235.37539100646973 and batch: 150, loss is 3.6223972177505495 and perplexity is 37.42718147119936
At time: 236.68341851234436 and batch: 200, loss is 3.594506902694702 and perplexity is 36.397747930763735
At time: 237.98161458969116 and batch: 250, loss is 3.626192274093628 and perplexity is 37.569489596523226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466477584838867 and perplexity of 87.04955766740129
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 239.99539947509766 and batch: 50, loss is 3.678811202049255 and perplexity is 39.59929052439286
At time: 241.29203248023987 and batch: 100, loss is 3.554995536804199 and perplexity is 34.9876638105509
At time: 242.591566324234 and batch: 150, loss is 3.622116794586182 and perplexity is 37.416687493983765
At time: 243.89427304267883 and batch: 200, loss is 3.5940313053131105 and perplexity is 36.38044137295504
At time: 245.19814133644104 and batch: 250, loss is 3.62558078289032 and perplexity is 37.546523206710496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466474533081055 and perplexity of 87.04929201363895
Finished 34 epochs...
Completing Train Step...
At time: 247.207049369812 and batch: 50, loss is 3.6787776803970336 and perplexity is 39.59796311299632
At time: 248.51041078567505 and batch: 100, loss is 3.55496817111969 and perplexity is 34.98670636228202
At time: 249.81542348861694 and batch: 150, loss is 3.622098989486694 and perplexity is 37.41602129207135
At time: 251.11773490905762 and batch: 200, loss is 3.5940268802642823 and perplexity is 36.38028038808176
At time: 252.42560839653015 and batch: 250, loss is 3.62558669090271 and perplexity is 37.546745032690076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466471099853516 and perplexity of 87.04899315412541
Finished 35 epochs...
Completing Train Step...
At time: 254.42708206176758 and batch: 50, loss is 3.6787445020675658 and perplexity is 39.59664934052441
At time: 255.74610686302185 and batch: 100, loss is 3.554940719604492 and perplexity is 34.98574593736321
At time: 257.0515697002411 and batch: 150, loss is 3.622081365585327 and perplexity is 37.41536188161325
At time: 258.35938596725464 and batch: 200, loss is 3.594022216796875 and perplexity is 36.38011073022549
At time: 259.6609663963318 and batch: 250, loss is 3.6255921268463136 and perplexity is 37.54694913523332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466468429565429 and perplexity of 87.04876070854638
Finished 36 epochs...
Completing Train Step...
At time: 261.6729428768158 and batch: 50, loss is 3.6787117099761963 and perplexity is 39.595350904870635
At time: 262.9738483428955 and batch: 100, loss is 3.554913568496704 and perplexity is 34.98479604849954
At time: 264.28257846832275 and batch: 150, loss is 3.62206383228302 and perplexity is 37.41470587251348
At time: 265.5904715061188 and batch: 200, loss is 3.5940173721313475 and perplexity is 36.37993448118409
At time: 266.89914321899414 and batch: 250, loss is 3.6255974674224856 and perplexity is 37.547149658110655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466465759277344 and perplexity of 87.04852826358812
Finished 37 epochs...
Completing Train Step...
At time: 268.87758231163025 and batch: 50, loss is 3.6786791276931763 and perplexity is 39.59406081895826
At time: 270.1942400932312 and batch: 100, loss is 3.5548864126205446 and perplexity is 34.983846018610095
At time: 271.49757957458496 and batch: 150, loss is 3.6220465326309204 and perplexity is 37.41405861671714
At time: 272.7999458312988 and batch: 200, loss is 3.5940125799179077 and perplexity is 36.379760141190864
At time: 274.09699845314026 and batch: 250, loss is 3.625602478981018 and perplexity is 37.54733782832041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466463088989258 and perplexity of 87.04829581925057
Finished 38 epochs...
Completing Train Step...
At time: 276.0931315422058 and batch: 50, loss is 3.6786468553543092 and perplexity is 39.59278304662885
At time: 277.4061870574951 and batch: 100, loss is 3.5548593044281005 and perplexity is 34.982897682633684
At time: 278.71425318717957 and batch: 150, loss is 3.6220292615890504 and perplexity is 37.41341244252431
At time: 280.02742552757263 and batch: 200, loss is 3.5940074634552004 and perplexity is 36.37957400598098
At time: 281.3298497200012 and batch: 250, loss is 3.625607123374939 and perplexity is 37.547512213352924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466461181640625 and perplexity of 87.04812978796089
Finished 39 epochs...
Completing Train Step...
At time: 283.33767199516296 and batch: 50, loss is 3.678614869117737 and perplexity is 39.59151664275762
At time: 284.64114022254944 and batch: 100, loss is 3.5548323154449464 and perplexity is 34.98195354253819
At time: 285.9470205307007 and batch: 150, loss is 3.622012176513672 and perplexity is 37.41277323701302
At time: 287.2558505535126 and batch: 200, loss is 3.594002480506897 and perplexity is 36.37939272889606
At time: 288.57412695884705 and batch: 250, loss is 3.6256116914749144 and perplexity is 37.547683734534296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466458511352539 and perplexity of 87.04789734468737
Finished 40 epochs...
Completing Train Step...
At time: 290.5610685348511 and batch: 50, loss is 3.678583040237427 and perplexity is 39.59025650916753
At time: 291.8773674964905 and batch: 100, loss is 3.5548054599761962 and perplexity is 34.98101409839268
At time: 293.17822909355164 and batch: 150, loss is 3.621995186805725 and perplexity is 37.412137610321814
At time: 294.4846522808075 and batch: 200, loss is 3.5939974117279054 and perplexity is 36.379208330261804
At time: 295.78382635116577 and batch: 250, loss is 3.625615792274475 and perplexity is 37.54783771037498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.4664558410644535 and perplexity of 87.04766490203455
Finished 41 epochs...
Completing Train Step...
At time: 297.78325295448303 and batch: 50, loss is 3.6785514879226686 and perplexity is 39.5890073646396
At time: 299.0853850841522 and batch: 100, loss is 3.554778652191162 and perplexity is 34.98007634745602
At time: 300.38920307159424 and batch: 150, loss is 3.621978259086609 and perplexity is 37.41150431352496
At time: 301.6947410106659 and batch: 200, loss is 3.593991994857788 and perplexity is 36.379011269349036
At time: 302.9950039386749 and batch: 250, loss is 3.62561984539032 and perplexity is 37.54798989641935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466455459594727 and perplexity of 87.04763169599192
Finished 42 epochs...
Completing Train Step...
At time: 304.9875445365906 and batch: 50, loss is 3.6785201263427734 and perplexity is 39.58776581029081
At time: 306.2864775657654 and batch: 100, loss is 3.554751992225647 and perplexity is 34.97914379225788
At time: 307.58708930015564 and batch: 150, loss is 3.6219614124298096 and perplexity is 37.41087406006029
At time: 308.8958013057709 and batch: 200, loss is 3.593986778259277 and perplexity is 36.378821495148
At time: 310.1997845172882 and batch: 250, loss is 3.625623564720154 and perplexity is 37.548129550038084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466453170776367 and perplexity of 87.04743246000234
Finished 43 epochs...
Completing Train Step...
At time: 312.1727375984192 and batch: 50, loss is 3.678489112854004 and perplexity is 39.58653807459873
At time: 313.4892656803131 and batch: 100, loss is 3.554725475311279 and perplexity is 34.9782162655949
At time: 314.783145904541 and batch: 150, loss is 3.6219447374343874 and perplexity is 37.41025023910772
At time: 316.0927662849426 and batch: 200, loss is 3.593981328010559 and perplexity is 36.378623222063105
At time: 317.3936998844147 and batch: 250, loss is 3.6256272268295286 and perplexity is 37.548267055647095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466451263427734 and perplexity of 87.04726643035939
Finished 44 epochs...
Completing Train Step...
At time: 319.3991014957428 and batch: 50, loss is 3.6784581184387206 and perplexity is 39.5853111320123
At time: 320.70150923728943 and batch: 100, loss is 3.554698877334595 and perplexity is 34.97728592818682
At time: 322.01671385765076 and batch: 150, loss is 3.6219279527664185 and perplexity is 37.409622325748494
At time: 323.3133170604706 and batch: 200, loss is 3.59397575378418 and perplexity is 36.37842043994708
At time: 324.60913729667664 and batch: 250, loss is 3.6256305503845216 and perplexity is 37.54839184958493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466450500488281 and perplexity of 87.04720001859089
Finished 45 epochs...
Completing Train Step...
At time: 326.59564685821533 and batch: 50, loss is 3.678427324295044 and perplexity is 39.58409215502258
At time: 327.91085290908813 and batch: 100, loss is 3.554672598838806 and perplexity is 34.97636678980269
At time: 329.2156708240509 and batch: 150, loss is 3.6219113636016846 and perplexity is 37.40900173650864
At time: 330.52629709243774 and batch: 200, loss is 3.5939702367782593 and perplexity is 36.37821974053976
At time: 331.8271129131317 and batch: 250, loss is 3.6256337213516234 and perplexity is 37.548510914488986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466449356079101 and perplexity of 87.0471004010331
Finished 46 epochs...
Completing Train Step...
At time: 333.8328523635864 and batch: 50, loss is 3.6783967924118044 and perplexity is 39.582883596592644
At time: 335.14857292175293 and batch: 100, loss is 3.5546462535858154 and perplexity is 34.97544534070887
At time: 336.4526345729828 and batch: 150, loss is 3.6218948411941527 and perplexity is 37.4083836548427
At time: 337.7554771900177 and batch: 200, loss is 3.5939644527435304 and perplexity is 36.378009328261925
At time: 339.0554187297821 and batch: 250, loss is 3.625636548995972 and perplexity is 37.548617088473776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466448211669922 and perplexity of 87.04700078358941
Finished 47 epochs...
Completing Train Step...
At time: 341.06698656082153 and batch: 50, loss is 3.6783663272857665 and perplexity is 39.58167771742365
At time: 342.3735828399658 and batch: 100, loss is 3.5546200895309448 and perplexity is 34.97453025320911
At time: 343.6796143054962 and batch: 150, loss is 3.6218783617019654 and perplexity is 37.40776718875606
At time: 344.98393416404724 and batch: 200, loss is 3.5939588069915773 and perplexity is 36.37780394762447
At time: 346.2806043624878 and batch: 250, loss is 3.625639386177063 and perplexity is 37.5487236208513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466447067260742 and perplexity of 87.04690116625963
Finished 48 epochs...
Completing Train Step...
At time: 348.27930426597595 and batch: 50, loss is 3.6783360862731933 and perplexity is 39.58048074550904
At time: 349.5962665081024 and batch: 100, loss is 3.554593954086304 and perplexity is 34.9736161902546
At time: 350.8972704410553 and batch: 150, loss is 3.6218618679046632 and perplexity is 37.4071501977148
At time: 352.20270705223083 and batch: 200, loss is 3.5939530181884765 and perplexity is 36.377593364289694
At time: 353.49672198295593 and batch: 250, loss is 3.625642108917236 and perplexity is 37.54882585640873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466446304321289 and perplexity of 87.0468347547698
Finished 49 epochs...
Completing Train Step...
At time: 355.51247119903564 and batch: 50, loss is 3.6783061122894285 and perplexity is 39.579294378601936
At time: 356.81915521621704 and batch: 100, loss is 3.5545679187774657 and perplexity is 34.97270565320901
At time: 358.12430119514465 and batch: 150, loss is 3.621845455169678 and perplexity is 37.40653624911035
At time: 359.42352056503296 and batch: 200, loss is 3.5939471435546877 and perplexity is 36.37737965987828
At time: 360.7221953868866 and batch: 250, loss is 3.62564444065094 and perplexity is 37.5489134103736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.466445541381836 and perplexity of 87.04676834333057
Finished Training.
Improved accuracyfrom -92.12550011714865 to -87.04676834333057
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff00acb3ef0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 7.303909839173954, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.2450297751201902, 'lr': 29.827993674328212}, 'best_accuracy': -186.49752509915456}, {'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 3.698876580509711, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.9862625980444063, 'lr': 1.1554307956415455}, 'best_accuracy': -92.59020558782429}, {'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 3.6234654834508007, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.4370688770724037, 'lr': 25.868149321566158}, 'best_accuracy': -214.34083690562562}, {'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 6.048407204416186, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.1307218843073108, 'lr': 18.57332424490116}, 'best_accuracy': -139.87049156179458}, {'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 7.0133934861548335, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.2250128030850751, 'lr': 1.0860396985333176}, 'best_accuracy': -92.12550011714865}, {'params': {'wordvec_dim': 200, 'tune_wordvecs': True, 'num_layers': 1, 'seq_len': 50, 'wordvec_source': 'glove', 'anneal': 5.991862107096926, 'data': 'ptb', 'batch_size': 80, 'dropout': 0.4795089303813866, 'lr': 6.925394387142077}, 'best_accuracy': -87.04676834333057}]
