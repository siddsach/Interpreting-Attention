FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1995322704315186 and batch: 50, loss is 7.1000563907623295 and perplexity is 1212.03542016684
At time: 1.8320302963256836 and batch: 100, loss is 6.1765976238250735 and perplexity is 481.351428456908
At time: 2.4692184925079346 and batch: 150, loss is 5.999650783538819 and perplexity is 403.28793411377137
At time: 3.1029539108276367 and batch: 200, loss is 5.85054084777832 and perplexity is 347.42223221699135
At time: 3.743576765060425 and batch: 250, loss is 5.880300750732422 and perplexity is 357.9168692840887
At time: 4.3797383308410645 and batch: 300, loss is 5.7787924575805665 and perplexity is 323.3684734122885
At time: 5.0161542892456055 and batch: 350, loss is 5.7565317058563235 and perplexity is 316.24957806414665
At time: 5.650702238082886 and batch: 400, loss is 5.6212556934356686 and perplexity is 276.2360333198502
At time: 6.284106254577637 and batch: 450, loss is 5.629595165252685 and perplexity is 278.5493283571113
At time: 6.920450687408447 and batch: 500, loss is 5.577664813995361 and perplexity is 264.4533364500749
At time: 7.554150104522705 and batch: 550, loss is 5.622131986618042 and perplexity is 276.47820316296884
At time: 8.186994791030884 and batch: 600, loss is 5.541470241546631 and perplexity is 255.05271302660068
At time: 8.820752382278442 and batch: 650, loss is 5.445721387863159 and perplexity is 231.76441145500698
At time: 9.453224897384644 and batch: 700, loss is 5.545311450958252 and perplexity is 256.03430795723744
At time: 10.084675550460815 and batch: 750, loss is 5.509283742904663 and perplexity is 246.9741667013654
At time: 10.719972610473633 and batch: 800, loss is 5.5018050003051755 and perplexity is 245.13400012291885
At time: 11.35363245010376 and batch: 850, loss is 5.53273157119751 and perplexity is 252.83360160911872
At time: 11.986375570297241 and batch: 900, loss is 5.418412570953369 and perplexity is 225.52083987131078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.331417972094392 and perplexity of 206.73090507773907
finished 1 epochs...
Completing Train Step...
At time: 13.626111030578613 and batch: 50, loss is 5.2280252647399905 and perplexity is 186.42430115180125
At time: 14.247109651565552 and batch: 100, loss is 5.0807429218292235 and perplexity is 160.89354286355987
At time: 14.851564884185791 and batch: 150, loss is 5.038107833862305 and perplexity is 154.1780084437753
At time: 15.458616495132446 and batch: 200, loss is 4.900232172012329 and perplexity is 134.32096163296785
At time: 16.062899827957153 and batch: 250, loss is 4.969780054092407 and perplexity is 143.99521272994053
At time: 16.671149969100952 and batch: 300, loss is 4.889739246368408 and perplexity is 132.91891044354298
At time: 17.275615453720093 and batch: 350, loss is 4.872906761169434 and perplexity is 130.70027976878038
At time: 17.88302516937256 and batch: 400, loss is 4.718436422348023 and perplexity is 111.9930059363162
At time: 18.48864722251892 and batch: 450, loss is 4.735889577865601 and perplexity is 113.96479419700341
At time: 19.095072269439697 and batch: 500, loss is 4.642060890197754 and perplexity is 103.7579611557008
At time: 19.69965672492981 and batch: 550, loss is 4.699654655456543 and perplexity is 109.90920935160075
At time: 20.304818630218506 and batch: 600, loss is 4.63626091003418 and perplexity is 103.15790886700476
At time: 20.90825128555298 and batch: 650, loss is 4.498898010253907 and perplexity is 89.91798798234906
At time: 21.539197206497192 and batch: 700, loss is 4.551157760620117 and perplexity is 94.7420334382381
At time: 22.142191171646118 and batch: 750, loss is 4.580404148101807 and perplexity is 97.55381242923494
At time: 22.747843265533447 and batch: 800, loss is 4.52547119140625 and perplexity is 92.33942512318231
At time: 23.35116744041443 and batch: 850, loss is 4.579528980255127 and perplexity is 97.46847381752718
At time: 23.956727504730225 and batch: 900, loss is 4.503839225769043 and perplexity is 90.36339165118355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.617356391802226 and perplexity of 101.22607601572699
finished 2 epochs...
Completing Train Step...
At time: 25.57756018638611 and batch: 50, loss is 4.547290573120117 and perplexity is 94.37635575860693
At time: 26.197563648223877 and batch: 100, loss is 4.426559748649598 and perplexity is 83.64316784880705
At time: 26.80212378501892 and batch: 150, loss is 4.424564542770386 and perplexity is 83.47644888319631
At time: 27.40687131881714 and batch: 200, loss is 4.317507090568543 and perplexity is 75.0014232909223
At time: 28.01227641105652 and batch: 250, loss is 4.443475952148438 and perplexity is 85.07012804411241
At time: 28.619023084640503 and batch: 300, loss is 4.399434266090393 and perplexity is 81.40480217851277
At time: 29.229677438735962 and batch: 350, loss is 4.398158197402954 and perplexity is 81.30099030907046
At time: 29.820096492767334 and batch: 400, loss is 4.291005544662475 and perplexity is 73.03987644387018
At time: 30.403186559677124 and batch: 450, loss is 4.336041479110718 and perplexity is 76.40449114848194
At time: 31.021360635757446 and batch: 500, loss is 4.2159109926223755 and perplexity is 67.75586285509995
At time: 31.6291663646698 and batch: 550, loss is 4.283611812591553 and perplexity is 72.50183070110513
At time: 32.23065400123596 and batch: 600, loss is 4.2722145318984985 and perplexity is 71.68019806873076
At time: 32.832353830337524 and batch: 650, loss is 4.120352063179016 and perplexity is 61.58091882249303
At time: 33.43451762199402 and batch: 700, loss is 4.147882623672485 and perplexity is 63.299828743496725
At time: 34.03912663459778 and batch: 750, loss is 4.227112278938294 and perplexity is 68.51908221379898
At time: 34.63943028450012 and batch: 800, loss is 4.184562048912048 and perplexity is 65.66473667566494
At time: 35.241549491882324 and batch: 850, loss is 4.256246314048767 and perplexity is 70.54468325250744
At time: 35.844468116760254 and batch: 900, loss is 4.190924081802368 and perplexity is 66.08382961809086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.440883270681721 and perplexity of 84.84985397340171
finished 3 epochs...
Completing Train Step...
At time: 37.47683358192444 and batch: 50, loss is 4.264727182388306 and perplexity is 71.1455075780335
At time: 38.093586444854736 and batch: 100, loss is 4.143697004318238 and perplexity is 63.03543347017795
At time: 38.694002628326416 and batch: 150, loss is 4.143337469100953 and perplexity is 63.012774085576055
At time: 39.29663109779358 and batch: 200, loss is 4.03649290561676 and perplexity is 56.6273965234052
At time: 39.89624905586243 and batch: 250, loss is 4.178977627754211 and perplexity is 65.2990591298885
At time: 40.49893498420715 and batch: 300, loss is 4.1427388000488286 and perplexity is 62.975061577624594
At time: 41.0994348526001 and batch: 350, loss is 4.143930168151855 and perplexity is 63.05013276710521
At time: 41.70293378829956 and batch: 400, loss is 4.0526510429382325 and perplexity is 57.54982204245826
At time: 42.30655550956726 and batch: 450, loss is 4.1030693626403805 and perplexity is 60.52577834538371
At time: 42.90772104263306 and batch: 500, loss is 3.9760567855834963 and perplexity is 53.30642060683092
At time: 43.50832438468933 and batch: 550, loss is 4.046791524887085 and perplexity is 57.21359385094187
At time: 44.10942769050598 and batch: 600, loss is 4.053330068588257 and perplexity is 57.58891311819898
At time: 44.71582651138306 and batch: 650, loss is 3.8976906299591065 and perplexity is 49.28849220459173
At time: 45.32182240486145 and batch: 700, loss is 3.9145120859146116 and perplexity is 50.12460904095459
At time: 45.92559337615967 and batch: 750, loss is 4.008375253677368 and perplexity is 55.05734363727765
At time: 46.53202033042908 and batch: 800, loss is 3.97478449344635 and perplexity is 53.23864239302056
At time: 47.136006355285645 and batch: 850, loss is 4.045730810165406 and perplexity is 57.15293872424393
At time: 47.7485613822937 and batch: 900, loss is 3.986059031486511 and perplexity is 53.84227996499162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371508036574272 and perplexity of 79.1629223118265
finished 4 epochs...
Completing Train Step...
At time: 49.37873816490173 and batch: 50, loss is 4.071670093536377 and perplexity is 58.65483990601174
At time: 49.98221254348755 and batch: 100, loss is 3.952718858718872 and perplexity is 52.0767638919761
At time: 50.59105443954468 and batch: 150, loss is 3.953384861946106 and perplexity is 52.111458736948975
At time: 51.19509983062744 and batch: 200, loss is 3.8505821371078492 and perplexity is 47.02042760163273
At time: 51.826245069503784 and batch: 250, loss is 3.9942516803741457 and perplexity is 54.28520273804316
At time: 52.43181920051575 and batch: 300, loss is 3.964286069869995 and perplexity is 52.68264423333255
At time: 53.038092374801636 and batch: 350, loss is 3.961260256767273 and perplexity is 52.52347732422314
At time: 53.645811557769775 and batch: 400, loss is 3.8832602643966676 and perplexity is 48.58234845392655
At time: 54.25369834899902 and batch: 450, loss is 3.931789908409119 and perplexity is 50.99817809246784
At time: 54.860190868377686 and batch: 500, loss is 3.809943342208862 and perplexity is 45.14788081465112
At time: 55.46821451187134 and batch: 550, loss is 3.8742587184906006 and perplexity is 48.14699458266702
At time: 56.07551956176758 and batch: 600, loss is 3.889987268447876 and perplexity is 48.9102638161947
At time: 56.68160128593445 and batch: 650, loss is 3.7350722789764403 and perplexity is 41.891053296134935
At time: 57.28758263587952 and batch: 700, loss is 3.752536163330078 and perplexity is 42.62905927511238
At time: 57.895737648010254 and batch: 750, loss is 3.847482843399048 and perplexity is 46.874923083272776
At time: 58.50056529045105 and batch: 800, loss is 3.8181300067901613 and perplexity is 45.5190084473689
At time: 59.10687232017517 and batch: 850, loss is 3.8880347633361816 and perplexity is 48.81485944514932
At time: 59.71265745162964 and batch: 900, loss is 3.830019087791443 and perplexity is 46.06341747472898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350588863843108 and perplexity of 77.52410062657447
finished 5 epochs...
Completing Train Step...
At time: 61.31596875190735 and batch: 50, loss is 3.921289687156677 and perplexity is 50.46548751776416
At time: 61.93805003166199 and batch: 100, loss is 3.802862229347229 and perplexity is 44.829312814260945
At time: 62.545361280441284 and batch: 150, loss is 3.805690426826477 and perplexity is 44.95627842103319
At time: 63.151044607162476 and batch: 200, loss is 3.7067485857009888 and perplexity is 40.72118958941506
At time: 63.753546476364136 and batch: 250, loss is 3.848759903907776 and perplexity is 46.93482343643896
At time: 64.35871863365173 and batch: 300, loss is 3.824004120826721 and perplexity is 45.787179155373
At time: 64.96763491630554 and batch: 350, loss is 3.8178593349456786 and perplexity is 45.50668940067818
At time: 65.57384443283081 and batch: 400, loss is 3.7484148168563842 and perplexity is 42.453731693016884
At time: 66.17897963523865 and batch: 450, loss is 3.792008361816406 and perplexity is 44.34537245468656
At time: 66.78674840927124 and batch: 500, loss is 3.6762545537948608 and perplexity is 39.49817837657273
At time: 67.41946339607239 and batch: 550, loss is 3.740706090927124 and perplexity is 42.12772567065252
At time: 68.02805709838867 and batch: 600, loss is 3.7602454566955568 and perplexity is 42.95896925138863
At time: 68.63422060012817 and batch: 650, loss is 3.605223846435547 and perplexity is 36.78991822765189
At time: 69.24064898490906 and batch: 700, loss is 3.6230352544784545 and perplexity is 37.45106900735229
At time: 69.85015106201172 and batch: 750, loss is 3.717183327674866 and perplexity is 41.14832936661966
At time: 70.45566439628601 and batch: 800, loss is 3.692602000236511 and perplexity is 40.14917933611346
At time: 71.05713748931885 and batch: 850, loss is 3.7614865779876707 and perplexity is 43.01231964311903
At time: 71.66356635093689 and batch: 900, loss is 3.702248525619507 and perplexity is 40.538353484944935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354247210776969 and perplexity of 77.80823008758607
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 73.27029752731323 and batch: 50, loss is 3.828712830543518 and perplexity is 46.0032860838712
At time: 73.89165043830872 and batch: 100, loss is 3.712602982521057 and perplexity is 40.96028679431443
At time: 74.49618411064148 and batch: 150, loss is 3.715795679092407 and perplexity is 41.091269544350325
At time: 75.09735155105591 and batch: 200, loss is 3.601128034591675 and perplexity is 36.63954181181459
At time: 75.70503067970276 and batch: 250, loss is 3.7395758295059203 and perplexity is 42.08013722632143
At time: 76.30923914909363 and batch: 300, loss is 3.698530788421631 and perplexity is 40.38792234524138
At time: 76.91417217254639 and batch: 350, loss is 3.6856090211868286 and perplexity is 39.86939636453903
At time: 77.52001166343689 and batch: 400, loss is 3.614230542182922 and perplexity is 37.122770527300766
At time: 78.12508082389832 and batch: 450, loss is 3.6406782150268553 and perplexity is 38.11767994410573
At time: 78.73056864738464 and batch: 500, loss is 3.516373248100281 and perplexity is 33.66212264422989
At time: 79.33638739585876 and batch: 550, loss is 3.561589126586914 and perplexity is 35.219120339319495
At time: 79.94247245788574 and batch: 600, loss is 3.57314781665802 and perplexity is 35.628569022481344
At time: 80.5445544719696 and batch: 650, loss is 3.410668683052063 and perplexity is 30.285488883166956
At time: 81.14812541007996 and batch: 700, loss is 3.4110684251785277 and perplexity is 30.297597688932772
At time: 81.7526376247406 and batch: 750, loss is 3.4921241712570192 and perplexity is 32.855664693179314
At time: 82.37411618232727 and batch: 800, loss is 3.4550421810150147 and perplexity is 31.659624089189307
At time: 82.98132753372192 and batch: 850, loss is 3.5020627689361574 and perplexity is 33.18383198615126
At time: 83.58767199516296 and batch: 900, loss is 3.436826481819153 and perplexity is 31.088142668881694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307632237264555 and perplexity of 74.2644400279
finished 7 epochs...
Completing Train Step...
At time: 85.1999044418335 and batch: 50, loss is 3.7298929595947268 and perplexity is 41.67464705416375
At time: 85.80305027961731 and batch: 100, loss is 3.611946048736572 and perplexity is 37.038060597801305
At time: 86.40524983406067 and batch: 150, loss is 3.612896842956543 and perplexity is 37.07329291842583
At time: 87.01082468032837 and batch: 200, loss is 3.5043239641189574 and perplexity is 33.25895200569094
At time: 87.61349058151245 and batch: 250, loss is 3.644723529815674 and perplexity is 38.27219026924284
At time: 88.21824240684509 and batch: 300, loss is 3.608064727783203 and perplexity is 36.894582619293516
At time: 88.82346415519714 and batch: 350, loss is 3.5972269582748413 and perplexity is 36.496886598345505
At time: 89.42887687683105 and batch: 400, loss is 3.5309452533721926 and perplexity is 34.1562386599691
At time: 90.03192162513733 and batch: 450, loss is 3.5640492486953734 and perplexity is 35.30587033995162
At time: 90.63701891899109 and batch: 500, loss is 3.4442473220825196 and perplexity is 31.319700925492548
At time: 91.24082612991333 and batch: 550, loss is 3.4910159063339234 and perplexity is 32.81927208252673
At time: 91.84580707550049 and batch: 600, loss is 3.510610637664795 and perplexity is 33.46869879341303
At time: 92.44310975074768 and batch: 650, loss is 3.3533920431137085 and perplexity is 28.599580306313435
At time: 93.03723859786987 and batch: 700, loss is 3.3599285173416136 and perplexity is 28.787133024897162
At time: 93.6332676410675 and batch: 750, loss is 3.4451720190048216 and perplexity is 31.348675550854622
At time: 94.22854042053223 and batch: 800, loss is 3.414173192977905 and perplexity is 30.39181087398617
At time: 94.82502913475037 and batch: 850, loss is 3.4691284132003783 and perplexity is 32.10874468726579
At time: 95.42061758041382 and batch: 900, loss is 3.412867794036865 and perplexity is 30.352163319822758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3155931446650255 and perplexity of 74.85801190864906
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 97.00204229354858 and batch: 50, loss is 3.6943650531768797 and perplexity is 40.22002690044707
At time: 97.61250877380371 and batch: 100, loss is 3.5895028352737426 and perplexity is 36.216066099333645
At time: 98.1853551864624 and batch: 150, loss is 3.595513119697571 and perplexity is 36.43439039564177
At time: 98.754958152771 and batch: 200, loss is 3.4805329751968386 and perplexity is 32.477026913741554
At time: 99.35376214981079 and batch: 250, loss is 3.620911455154419 and perplexity is 37.3716148545131
At time: 99.95203399658203 and batch: 300, loss is 3.579723052978516 and perplexity is 35.86360715239214
At time: 100.55215215682983 and batch: 350, loss is 3.5645711326599123 and perplexity is 35.32430071637677
At time: 101.14604139328003 and batch: 400, loss is 3.496853952407837 and perplexity is 33.01143288107367
At time: 101.74449706077576 and batch: 450, loss is 3.529036021232605 and perplexity is 34.091088684355945
At time: 102.35642623901367 and batch: 500, loss is 3.4043588495254515 and perplexity is 30.09499411720204
At time: 102.96412134170532 and batch: 550, loss is 3.444051299095154 and perplexity is 31.313562145844354
At time: 103.57060980796814 and batch: 600, loss is 3.463585600852966 and perplexity is 31.931264265480557
At time: 104.17472219467163 and batch: 650, loss is 3.3018400049209595 and perplexity is 27.16257223437565
At time: 104.7807776927948 and batch: 700, loss is 3.294058585166931 and perplexity is 26.952029082913025
At time: 105.38692831993103 and batch: 750, loss is 3.38127836227417 and perplexity is 29.40834160827914
At time: 105.99050688743591 and batch: 800, loss is 3.34284752368927 and perplexity is 28.29959585172003
At time: 106.5936975479126 and batch: 850, loss is 3.392976665496826 and perplexity is 29.754389445566506
At time: 107.19908952713013 and batch: 900, loss is 3.3409404516220094 and perplexity is 28.245677911996122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310833238575556 and perplexity of 74.50254147600684
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 108.79609251022339 and batch: 50, loss is 3.6807464790344238 and perplexity is 39.67600032338624
At time: 109.41907048225403 and batch: 100, loss is 3.5732141971588134 and perplexity is 35.630934143233695
At time: 110.02463030815125 and batch: 150, loss is 3.5811268377304075 and perplexity is 35.91398729042198
At time: 110.63077592849731 and batch: 200, loss is 3.4635764265060427 and perplexity is 31.93097131832828
At time: 111.23371624946594 and batch: 250, loss is 3.606332721710205 and perplexity is 36.83073628521611
At time: 111.8388991355896 and batch: 300, loss is 3.5654368925094606 and perplexity is 35.35489631996191
At time: 112.44237041473389 and batch: 350, loss is 3.5479149293899535 and perplexity is 34.74080488568878
At time: 113.06294274330139 and batch: 400, loss is 3.480216088294983 and perplexity is 32.46673699975696
At time: 113.66721677780151 and batch: 450, loss is 3.5102317762374877 and perplexity is 33.456021196095996
At time: 114.27250361442566 and batch: 500, loss is 3.3861661052703855 and perplexity is 29.55243388013766
At time: 114.87739515304565 and batch: 550, loss is 3.426356763839722 and perplexity is 30.76435651513791
At time: 115.48419976234436 and batch: 600, loss is 3.44940101146698 and perplexity is 31.48152958481101
At time: 116.0890884399414 and batch: 650, loss is 3.2854282808303834 and perplexity is 26.72042571075823
At time: 116.69638705253601 and batch: 700, loss is 3.2741776752471923 and perplexity is 26.421489499948347
At time: 117.30248951911926 and batch: 750, loss is 3.3619728565216063 and perplexity is 28.846043885192245
At time: 117.90882182121277 and batch: 800, loss is 3.319432091712952 and perplexity is 27.644646475873323
At time: 118.51269698143005 and batch: 850, loss is 3.368020510673523 and perplexity is 29.021023356614624
At time: 119.1172468662262 and batch: 900, loss is 3.3191420888900756 and perplexity is 27.63663061272555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308261819081764 and perplexity of 74.31121029031534
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.72910022735596 and batch: 50, loss is 3.671732797622681 and perplexity is 39.319980432159745
At time: 121.33356165885925 and batch: 100, loss is 3.563910417556763 and perplexity is 35.300969126001114
At time: 121.94254636764526 and batch: 150, loss is 3.573550944328308 and perplexity is 35.64293477992969
At time: 122.54661774635315 and batch: 200, loss is 3.457537612915039 and perplexity is 31.738727182226025
At time: 123.15111660957336 and batch: 250, loss is 3.6013579320907594 and perplexity is 36.647966119171215
At time: 123.75786924362183 and batch: 300, loss is 3.5606372356414795 and perplexity is 35.18561152845934
At time: 124.36469531059265 and batch: 350, loss is 3.5417587661743166 and perplexity is 34.52759178127337
At time: 124.97094392776489 and batch: 400, loss is 3.4744824075698855 and perplexity is 32.28111575062986
At time: 125.5751564502716 and batch: 450, loss is 3.502740797996521 and perplexity is 33.206339217988464
At time: 126.17936277389526 and batch: 500, loss is 3.3792517614364623 and perplexity is 29.34880298942528
At time: 126.783864736557 and batch: 550, loss is 3.418564968109131 and perplexity is 30.525578396609614
At time: 127.39221262931824 and batch: 600, loss is 3.4436715507507323 and perplexity is 31.301673130021438
At time: 128.02390456199646 and batch: 650, loss is 3.278757343292236 and perplexity is 26.54276864826457
At time: 128.62773609161377 and batch: 700, loss is 3.2682763862609865 and perplexity is 26.26602781828328
At time: 129.23509240150452 and batch: 750, loss is 3.355773491859436 and perplexity is 28.667769903750923
At time: 129.83955121040344 and batch: 800, loss is 3.3119301319122316 and perplexity is 27.43803341973095
At time: 130.43198466300964 and batch: 850, loss is 3.359626741409302 and perplexity is 28.77844707166173
At time: 131.028146982193 and batch: 900, loss is 3.3114381980895997 and perplexity is 27.4245390425077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306947054928297 and perplexity of 74.21357277404279
finished 11 epochs...
Completing Train Step...
At time: 132.6445837020874 and batch: 50, loss is 3.667351984977722 and perplexity is 39.148103719402506
At time: 133.26745581626892 and batch: 100, loss is 3.558941640853882 and perplexity is 35.12600154042751
At time: 133.87405681610107 and batch: 150, loss is 3.5682944011688233 and perplexity is 35.45606772258491
At time: 134.4780306816101 and batch: 200, loss is 3.452843346595764 and perplexity is 31.590086297103998
At time: 135.0856647491455 and batch: 250, loss is 3.596463804244995 and perplexity is 36.46904447752576
At time: 135.69378018379211 and batch: 300, loss is 3.556224994659424 and perplexity is 35.03070612259129
At time: 136.30426168441772 and batch: 350, loss is 3.53739387512207 and perplexity is 34.37721104138251
At time: 136.9137246608734 and batch: 400, loss is 3.47067186832428 and perplexity is 32.15834135904801
At time: 137.52412509918213 and batch: 450, loss is 3.4995992469787596 and perplexity is 33.10218350013513
At time: 138.13348269462585 and batch: 500, loss is 3.3760702419281006 and perplexity is 29.255577578036
At time: 138.74140548706055 and batch: 550, loss is 3.4156932401657105 and perplexity is 30.438042989234095
At time: 139.34557366371155 and batch: 600, loss is 3.441373138427734 and perplexity is 31.229811594124
At time: 139.9528467655182 and batch: 650, loss is 3.2768479776382446 and perplexity is 26.49213714984995
At time: 140.55656242370605 and batch: 700, loss is 3.2672403717041014 and perplexity is 26.238829922252776
At time: 141.16213750839233 and batch: 750, loss is 3.355532569885254 and perplexity is 28.660864039950887
At time: 141.767831325531 and batch: 800, loss is 3.3125984907150268 and perplexity is 27.456378000596583
At time: 142.37420177459717 and batch: 850, loss is 3.361591143608093 and perplexity is 28.83503507897292
At time: 142.979590177536 and batch: 900, loss is 3.314623484611511 and perplexity is 27.512033330300483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306326252140411 and perplexity of 74.16751507901766
finished 12 epochs...
Completing Train Step...
At time: 144.58957648277283 and batch: 50, loss is 3.6646432304382324 and perplexity is 39.04220460784131
At time: 145.2164704799652 and batch: 100, loss is 3.555834484100342 and perplexity is 35.01702893267567
At time: 145.82167387008667 and batch: 150, loss is 3.564739966392517 and perplexity is 35.33026515340328
At time: 146.42988801002502 and batch: 200, loss is 3.449631886482239 and perplexity is 31.488798722533122
At time: 147.03523802757263 and batch: 250, loss is 3.5931941938400267 and perplexity is 36.349999631429114
At time: 147.64019751548767 and batch: 300, loss is 3.553148069381714 and perplexity is 34.92308491354429
At time: 148.24474048614502 and batch: 350, loss is 3.534382357597351 and perplexity is 34.27383919906308
At time: 148.85039138793945 and batch: 400, loss is 3.4678615951538085 and perplexity is 32.068094503672725
At time: 149.45488262176514 and batch: 450, loss is 3.497165112495422 and perplexity is 33.0217063196794
At time: 150.05883836746216 and batch: 500, loss is 3.3738244342803956 and perplexity is 29.18994890045025
At time: 150.66678524017334 and batch: 550, loss is 3.4135142230987547 and perplexity is 30.371790183287928
At time: 151.27580189704895 and batch: 600, loss is 3.4396653270721433 and perplexity is 31.176522484076234
At time: 151.88629055023193 and batch: 650, loss is 3.2754932165145876 and perplexity is 26.456270932920585
At time: 152.4961531162262 and batch: 700, loss is 3.266529064178467 and perplexity is 26.220172681369803
At time: 153.10286116600037 and batch: 750, loss is 3.355367732048035 and perplexity is 28.656140034467832
At time: 153.70743417739868 and batch: 800, loss is 3.3130214738845827 and perplexity is 27.467994042909808
At time: 154.312846660614 and batch: 850, loss is 3.3628146839141846 and perplexity is 28.87033749918249
At time: 154.91679787635803 and batch: 900, loss is 3.3165224409103393 and perplexity is 27.56432711537848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306123498367937 and perplexity of 74.15247885991691
finished 13 epochs...
Completing Train Step...
At time: 156.53219771385193 and batch: 50, loss is 3.6624624967575072 and perplexity is 38.95715672437967
At time: 157.1385679244995 and batch: 100, loss is 3.553328194618225 and perplexity is 34.92937600904956
At time: 157.74588012695312 and batch: 150, loss is 3.561904525756836 and perplexity is 35.23023017256389
At time: 158.35365438461304 and batch: 200, loss is 3.44696916103363 and perplexity is 31.405064227288097
At time: 158.972971200943 and batch: 250, loss is 3.5904734897613526 and perplexity is 36.251236452785875
At time: 159.5808265209198 and batch: 300, loss is 3.550554051399231 and perplexity is 34.83261119922831
At time: 160.18878841400146 and batch: 350, loss is 3.5318421936035156 and perplexity is 34.186888508059994
At time: 160.79374027252197 and batch: 400, loss is 3.4654622411727907 and perplexity is 31.99124402607973
At time: 161.40107893943787 and batch: 450, loss is 3.4950466108322145 and perplexity is 32.95182382914176
At time: 162.00639820098877 and batch: 500, loss is 3.3718782806396486 and perplexity is 29.133196017951562
At time: 162.61184191703796 and batch: 550, loss is 3.4116373538970945 and perplexity is 30.314839766653105
At time: 163.21582794189453 and batch: 600, loss is 3.4382022285461424 and perplexity is 31.130941512840966
At time: 163.8228394985199 and batch: 650, loss is 3.2743254995346067 and perplexity is 26.425395526501788
At time: 164.42734837532043 and batch: 700, loss is 3.265873112678528 and perplexity is 26.20297915944554
At time: 165.0358648300171 and batch: 750, loss is 3.3551336574554442 and perplexity is 28.649433145150862
At time: 165.64248490333557 and batch: 800, loss is 3.3132559394836427 and perplexity is 27.47443509766143
At time: 166.24544095993042 and batch: 850, loss is 3.3636247539520263 and perplexity is 28.893733969683407
At time: 166.85080742835999 and batch: 900, loss is 3.317817015647888 and perplexity is 27.600034304745353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30605117588827 and perplexity of 74.14711616269648
finished 14 epochs...
Completing Train Step...
At time: 168.4525649547577 and batch: 50, loss is 3.660532069206238 and perplexity is 38.88202529694031
At time: 169.07158875465393 and batch: 100, loss is 3.551138472557068 and perplexity is 34.852974063860735
At time: 169.67502164840698 and batch: 150, loss is 3.559454770088196 and perplexity is 35.14403034385993
At time: 170.27823615074158 and batch: 200, loss is 3.4446104764938354 and perplexity is 31.331076878533477
At time: 170.8794448375702 and batch: 250, loss is 3.5880625200271608 and perplexity is 36.163941094390374
At time: 171.48505783081055 and batch: 300, loss is 3.5482471418380737 and perplexity is 34.752348130828224
At time: 172.09474635124207 and batch: 350, loss is 3.529575562477112 and perplexity is 34.109487195702826
At time: 172.70190048217773 and batch: 400, loss is 3.463319458961487 and perplexity is 31.922767149181073
At time: 173.30986142158508 and batch: 450, loss is 3.4931312084197996 and perplexity is 32.88876823398351
At time: 173.91951489448547 and batch: 500, loss is 3.3701052713394164 and perplexity is 29.081588354419274
At time: 174.53823232650757 and batch: 550, loss is 3.409948678016663 and perplexity is 30.263691026896332
At time: 175.14194083213806 and batch: 600, loss is 3.4368793392181396 and perplexity is 31.089785950671914
At time: 175.74632954597473 and batch: 650, loss is 3.2732545518875122 and perplexity is 26.397110459955286
At time: 176.3510639667511 and batch: 700, loss is 3.2652240419387817 and perplexity is 26.185977090748523
At time: 176.95531296730042 and batch: 750, loss is 3.3548366975784303 and perplexity is 28.64092667611007
At time: 177.55910515785217 and batch: 800, loss is 3.3133536624908446 and perplexity is 27.47712011327211
At time: 178.16645002365112 and batch: 850, loss is 3.3641746997833253 and perplexity is 28.909628328348248
At time: 178.76865410804749 and batch: 900, loss is 3.318749532699585 and perplexity is 27.625783811423013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306059536868578 and perplexity of 74.14773610786628
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 180.38236236572266 and batch: 50, loss is 3.659461832046509 and perplexity is 38.84043456856337
At time: 181.00672793388367 and batch: 100, loss is 3.550565633773804 and perplexity is 34.833014645915
At time: 181.61550951004028 and batch: 150, loss is 3.5595767211914064 and perplexity is 35.14831645847452
At time: 182.2207372188568 and batch: 200, loss is 3.4447100162506104 and perplexity is 31.334195721527323
At time: 182.82550597190857 and batch: 250, loss is 3.587784209251404 and perplexity is 36.15387768033321
At time: 183.4327483177185 and batch: 300, loss is 3.5484071445465086 and perplexity is 34.757909045522474
At time: 184.03758764266968 and batch: 350, loss is 3.528698663711548 and perplexity is 34.07958973892417
At time: 184.64131116867065 and batch: 400, loss is 3.462494373321533 and perplexity is 31.896438995403113
At time: 185.24790716171265 and batch: 450, loss is 3.4918524694442747 and perplexity is 32.84673896214681
At time: 185.85417008399963 and batch: 500, loss is 3.368475317955017 and perplexity is 29.034225331303162
At time: 186.4638798236847 and batch: 550, loss is 3.4077897214889528 and perplexity is 30.19842351381442
At time: 187.07282423973083 and batch: 600, loss is 3.4351249885559083 and perplexity is 31.035291379371053
At time: 187.6800501346588 and batch: 650, loss is 3.271206803321838 and perplexity is 26.343111122275616
At time: 188.28785824775696 and batch: 700, loss is 3.2630950355529786 and perplexity is 26.13028628238281
At time: 188.89572954177856 and batch: 750, loss is 3.3525721645355224 and perplexity is 28.5761417327803
At time: 189.5167064666748 and batch: 800, loss is 3.3105378913879395 and perplexity is 27.39985965739269
At time: 190.12000179290771 and batch: 850, loss is 3.361074981689453 and perplexity is 28.8201553724418
At time: 190.72911477088928 and batch: 900, loss is 3.3151135301589965 and perplexity is 27.525518783709593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305203372485017 and perplexity of 74.08428062512729
finished 16 epochs...
Completing Train Step...
At time: 192.3437316417694 and batch: 50, loss is 3.6586788272857667 and perplexity is 38.81003422674627
At time: 192.95117449760437 and batch: 100, loss is 3.5497133255004885 and perplexity is 34.80333882761695
At time: 193.5626118183136 and batch: 150, loss is 3.55862539768219 and perplexity is 35.11489493857758
At time: 194.17199039459229 and batch: 200, loss is 3.443709945678711 and perplexity is 31.302874978579176
At time: 194.78280019760132 and batch: 250, loss is 3.5869087028503417 and perplexity is 36.122238581138774
At time: 195.39642786979675 and batch: 300, loss is 3.5474539470672606 and perplexity is 34.72479367948541
At time: 196.00627255439758 and batch: 350, loss is 3.527965326309204 and perplexity is 34.05460706259717
At time: 196.60992908477783 and batch: 400, loss is 3.461836476325989 and perplexity is 31.875461325349168
At time: 197.21789169311523 and batch: 450, loss is 3.4912525415420532 and perplexity is 32.82703919675674
At time: 197.82468914985657 and batch: 500, loss is 3.3679673433303834 and perplexity is 29.01948042692466
At time: 198.43065094947815 and batch: 550, loss is 3.407392077445984 and perplexity is 30.18641767777989
At time: 199.00821948051453 and batch: 600, loss is 3.4348155450820923 and perplexity is 31.025689196738725
At time: 199.58298015594482 and batch: 650, loss is 3.2709572458267213 and perplexity is 26.336537821692684
At time: 200.19178175926208 and batch: 700, loss is 3.2629774713516237 and perplexity is 26.127214476715576
At time: 200.79928350448608 and batch: 750, loss is 3.3526107263565064 and perplexity is 28.577243702089046
At time: 201.4059202671051 and batch: 800, loss is 3.310730061531067 and perplexity is 27.40512559830682
At time: 202.01377940177917 and batch: 850, loss is 3.3614432048797607 and perplexity is 28.83076957607623
At time: 202.62109375 and batch: 900, loss is 3.3156528425216676 and perplexity is 27.54036764000456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3048028815282535 and perplexity of 74.05461648120581
finished 17 epochs...
Completing Train Step...
At time: 204.24346375465393 and batch: 50, loss is 3.658028063774109 and perplexity is 38.784786288694754
At time: 204.86592984199524 and batch: 100, loss is 3.549019956588745 and perplexity is 34.779215638550134
At time: 205.46871757507324 and batch: 150, loss is 3.5578360509872438 and perplexity is 35.08718804892392
At time: 206.07344436645508 and batch: 200, loss is 3.442931265830994 and perplexity is 31.278509548323655
At time: 206.67913341522217 and batch: 250, loss is 3.5861551904678346 and perplexity is 36.095030279267554
At time: 207.28534960746765 and batch: 300, loss is 3.5467071771621703 and perplexity is 34.69887192860146
At time: 207.88868832588196 and batch: 350, loss is 3.5273226165771483 and perplexity is 34.03272686727712
At time: 208.4959101676941 and batch: 400, loss is 3.461257076263428 and perplexity is 31.856998030396742
At time: 209.09982323646545 and batch: 450, loss is 3.490736484527588 and perplexity is 32.81010294332719
At time: 209.70899200439453 and batch: 500, loss is 3.3675084257125856 and perplexity is 29.00616593145935
At time: 210.3195493221283 and batch: 550, loss is 3.40700674533844 and perplexity is 30.174788122601303
At time: 210.929128408432 and batch: 600, loss is 3.4345182752609253 and perplexity is 31.016467566383742
At time: 211.54060697555542 and batch: 650, loss is 3.2707179307937624 and perplexity is 26.330235846384653
At time: 212.14898419380188 and batch: 700, loss is 3.262863121032715 and perplexity is 26.124226992221118
At time: 212.75246357917786 and batch: 750, loss is 3.3526189708709717 and perplexity is 28.577479308559358
At time: 213.35804343223572 and batch: 800, loss is 3.3108595848083495 and perplexity is 27.408675429876606
At time: 213.9654941558838 and batch: 850, loss is 3.36173698425293 and perplexity is 28.83924070575093
At time: 214.57102513313293 and batch: 900, loss is 3.3160849523544313 and perplexity is 27.552270675183795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304594275069563 and perplexity of 74.03916982110455
finished 18 epochs...
Completing Train Step...
At time: 216.17158603668213 and batch: 50, loss is 3.657446904182434 and perplexity is 38.76225268657574
At time: 216.79501938819885 and batch: 100, loss is 3.5483996534347533 and perplexity is 34.75764867111668
At time: 217.4072425365448 and batch: 150, loss is 3.557130665779114 and perplexity is 35.06244679256334
At time: 218.02220344543457 and batch: 200, loss is 3.4422555685043337 and perplexity is 31.25738188179768
At time: 218.6318187713623 and batch: 250, loss is 3.5854763698577883 and perplexity is 36.07053654316074
At time: 219.2420117855072 and batch: 300, loss is 3.5460556030273436 and perplexity is 34.67627040522876
At time: 219.84897375106812 and batch: 350, loss is 3.526728005409241 and perplexity is 34.012496642962205
At time: 220.4701533317566 and batch: 400, loss is 3.460716028213501 and perplexity is 31.839766525692234
At time: 221.0741651058197 and batch: 450, loss is 3.490257019996643 and perplexity is 32.794375433402095
At time: 221.68022894859314 and batch: 500, loss is 3.36707549571991 and perplexity is 28.993611010152183
At time: 222.28601694107056 and batch: 550, loss is 3.40662570476532 and perplexity is 30.163292494330776
At time: 222.89192414283752 and batch: 600, loss is 3.4342252922058107 and perplexity is 31.007381598039657
At time: 223.49774742126465 and batch: 650, loss is 3.2704824781417847 and perplexity is 26.324037052317312
At time: 224.1091628074646 and batch: 700, loss is 3.2627413654327393 and perplexity is 26.12104641492026
At time: 224.7170672416687 and batch: 750, loss is 3.352604727745056 and perplexity is 28.57707227882191
At time: 225.32478380203247 and batch: 800, loss is 3.3109509229660032 and perplexity is 27.411179002128197
At time: 225.93694591522217 and batch: 850, loss is 3.3619743967056275 and perplexity is 28.846088313442326
At time: 226.54523372650146 and batch: 900, loss is 3.3164382648468016 and perplexity is 27.56200695648013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304478475492295 and perplexity of 74.03059661293449
finished 19 epochs...
Completing Train Step...
At time: 228.17692685127258 and batch: 50, loss is 3.656909875869751 and perplexity is 38.74144184792462
At time: 228.78129959106445 and batch: 100, loss is 3.5478223276138308 and perplexity is 34.73758797440058
At time: 229.386155128479 and batch: 150, loss is 3.556478638648987 and perplexity is 35.039592577599734
At time: 229.99500966072083 and batch: 200, loss is 3.4416367959976197 and perplexity is 31.23804665592738
At time: 230.60001683235168 and batch: 250, loss is 3.5848471975326537 and perplexity is 36.04784909771985
At time: 231.20824885368347 and batch: 300, loss is 3.545456600189209 and perplexity is 34.655505440597366
At time: 231.8174843788147 and batch: 350, loss is 3.526164994239807 and perplexity is 33.99335261710777
At time: 232.4245822429657 and batch: 400, loss is 3.460199389457703 and perplexity is 31.82332111686307
At time: 233.02978110313416 and batch: 450, loss is 3.4897983407974245 and perplexity is 32.779336784760744
At time: 233.63668251037598 and batch: 500, loss is 3.36665904045105 and perplexity is 28.981538981988074
At time: 234.2409017086029 and batch: 550, loss is 3.4062488222122194 and perplexity is 30.15192661758045
At time: 234.84776163101196 and batch: 600, loss is 3.4339351797103883 and perplexity is 30.998387273933723
At time: 235.4706301689148 and batch: 650, loss is 3.2702497911453245 and perplexity is 26.317912503779937
At time: 236.0756335258484 and batch: 700, loss is 3.26261239528656 and perplexity is 26.11767779697602
At time: 236.68186783790588 and batch: 750, loss is 3.3525737380981444 and perplexity is 28.57618669916419
At time: 237.28900837898254 and batch: 800, loss is 3.311015625 and perplexity is 27.41295261854156
At time: 237.89572978019714 and batch: 850, loss is 3.3621688795089724 and perplexity is 28.851698927129313
At time: 238.50372743606567 and batch: 900, loss is 3.316733298301697 and perplexity is 27.570139870298185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304414095943922 and perplexity of 74.02583070997373
Finished Training.
Improved accuracyfrom -10000000 to -74.02583070997373
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
252.03538966178894


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9172534942626953 and batch: 50, loss is 9.114129848480225 and perplexity is 9082.72783178306
At time: 1.5716333389282227 and batch: 100, loss is 8.257820568084718 and perplexity is 3857.677386769784
At time: 2.2055513858795166 and batch: 150, loss is 7.7750084400177 and perplexity is 2380.363390317491
At time: 2.8399264812469482 and batch: 200, loss is 7.403359413146973 and perplexity is 1641.4896195238634
At time: 3.4785268306732178 and batch: 250, loss is 7.257868270874024 and perplexity is 1419.2279007793659
At time: 4.114610910415649 and batch: 300, loss is 7.045234422683716 and perplexity is 1147.3777756692336
At time: 4.750222206115723 and batch: 350, loss is 6.945868644714356 and perplexity is 1038.848996080321
At time: 5.38593316078186 and batch: 400, loss is 6.7979902648925785 and perplexity is 896.0446684316069
At time: 6.020402669906616 and batch: 450, loss is 6.71296142578125 and perplexity is 823.0043002719751
At time: 6.659908056259155 and batch: 500, loss is 6.608477296829224 and perplexity is 741.3532978028677
At time: 7.299174785614014 and batch: 550, loss is 6.535122308731079 and perplexity is 688.918039862482
At time: 7.937706232070923 and batch: 600, loss is 6.43669677734375 and perplexity is 624.341052368529
At time: 8.57597279548645 and batch: 650, loss is 6.353371648788452 and perplexity is 574.4262110756841
At time: 9.212954998016357 and batch: 700, loss is 6.412395048141479 and perplexity is 609.3513606020528
At time: 9.846932172775269 and batch: 750, loss is 6.365680589675903 and perplexity is 581.5404841117318
At time: 10.480590105056763 and batch: 800, loss is 6.368254890441895 and perplexity is 583.0394728215083
At time: 11.118107795715332 and batch: 850, loss is 6.393963594436645 and perplexity is 598.2230001798005
At time: 11.750476598739624 and batch: 900, loss is 6.306507978439331 and perplexity is 548.1275297742122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.425067222281678 and perplexity of 617.1223005176525
finished 1 epochs...
Completing Train Step...
At time: 13.383342266082764 and batch: 50, loss is 6.3959607410430905 and perplexity is 599.4189330436299
At time: 13.988403797149658 and batch: 100, loss is 6.415747156143189 and perplexity is 611.3973995296286
At time: 14.60624098777771 and batch: 150, loss is 6.470002250671387 and perplexity is 645.4851797384515
At time: 15.205688953399658 and batch: 200, loss is 6.401292190551758 and perplexity is 602.6232390274491
At time: 15.812952995300293 and batch: 250, loss is 6.49791464805603 and perplexity is 663.7560238903811
At time: 16.411240577697754 and batch: 300, loss is 6.463981800079345 and perplexity is 641.6107427368847
At time: 17.01210117340088 and batch: 350, loss is 6.4833946037292485 and perplexity is 654.187889970539
At time: 17.611384630203247 and batch: 400, loss is 6.405919370651245 and perplexity is 605.4181465700814
At time: 18.20851707458496 and batch: 450, loss is 6.434697647094726 and perplexity is 623.0941600503763
At time: 18.80822205543518 and batch: 500, loss is 6.417701530456543 and perplexity is 612.5934673038821
At time: 19.40856432914734 and batch: 550, loss is 6.463714580535889 and perplexity is 641.4393147126132
At time: 20.00979447364807 and batch: 600, loss is 6.436389226913452 and perplexity is 624.1490655335505
At time: 20.611874103546143 and batch: 650, loss is 6.381004753112793 and perplexity is 590.5207372333273
At time: 21.221346139907837 and batch: 700, loss is 6.4604182624816895 and perplexity is 639.3284077406092
At time: 21.826555967330933 and batch: 750, loss is 6.4434295082092286 and perplexity is 628.5587550386282
At time: 22.42961359024048 and batch: 800, loss is 6.458638706207275 and perplexity is 638.1916985801421
At time: 23.035488843917847 and batch: 850, loss is 6.48713942527771 and perplexity is 656.6422996724912
At time: 23.636605739593506 and batch: 900, loss is 6.408156242370605 and perplexity is 606.7739050641429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.536529122966609 and perplexity of 689.8879016157779
Annealing...
finished 2 epochs...
Completing Train Step...
At time: 25.239630699157715 and batch: 50, loss is 6.414399194717407 and perplexity is 610.5738146242855
At time: 25.85687494277954 and batch: 100, loss is 6.354553709030151 and perplexity is 575.1056189329848
At time: 26.456501722335815 and batch: 150, loss is 6.383407821655274 and perplexity is 591.9415054586555
At time: 27.057883977890015 and batch: 200, loss is 6.323800764083862 and perplexity is 557.6886122661604
At time: 27.66130542755127 and batch: 250, loss is 6.416548871994019 and perplexity is 611.8877630558287
At time: 28.266538858413696 and batch: 300, loss is 6.354168176651001 and perplexity is 574.8839398304367
At time: 28.87167525291443 and batch: 350, loss is 6.381879177093506 and perplexity is 591.0373285540753
At time: 29.492714405059814 and batch: 400, loss is 6.325937957763672 and perplexity is 558.881775399794
At time: 30.095778465270996 and batch: 450, loss is 6.322745485305786 and perplexity is 557.1004057243877
At time: 30.697154998779297 and batch: 500, loss is 6.319897966384888 and perplexity is 555.5163082224219
At time: 31.29744291305542 and batch: 550, loss is 6.359633531570434 and perplexity is 578.0344861838395
At time: 31.894918203353882 and batch: 600, loss is 6.330479335784912 and perplexity is 561.4256407661192
At time: 32.495524168014526 and batch: 650, loss is 6.279737520217895 and perplexity is 533.6485734796364
At time: 33.09738564491272 and batch: 700, loss is 6.349796018600464 and perplexity is 572.3759430591085
At time: 33.69567608833313 and batch: 750, loss is 6.326097764968872 and perplexity is 558.9710958711955
At time: 34.29679894447327 and batch: 800, loss is 6.333685884475708 and perplexity is 563.2287687834153
At time: 34.89907741546631 and batch: 850, loss is 6.371925735473633 and perplexity is 585.1836534432597
At time: 35.50287842750549 and batch: 900, loss is 6.289652900695801 and perplexity is 538.9662218135413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.387796062312714 and perplexity of 594.5447949926707
finished 3 epochs...
Completing Train Step...
At time: 37.08045172691345 and batch: 50, loss is 6.348571176528931 and perplexity is 571.6753020981732
At time: 37.70203495025635 and batch: 100, loss is 6.325553522109986 and perplexity is 558.6669626126278
At time: 38.30385088920593 and batch: 150, loss is 6.366776380538941 and perplexity is 582.1780801328551
At time: 38.90503692626953 and batch: 200, loss is 6.3097522258758545 and perplexity is 549.9086787893965
At time: 39.505532026290894 and batch: 250, loss is 6.395461168289184 and perplexity is 599.119554463423
At time: 40.106942892074585 and batch: 300, loss is 6.346778535842896 and perplexity is 570.6514117004178
At time: 40.707364320755005 and batch: 350, loss is 6.372270708084106 and perplexity is 585.3855606000113
At time: 41.30768609046936 and batch: 400, loss is 6.3147659015655515 and perplexity is 552.6726656407919
At time: 41.91172671318054 and batch: 450, loss is 6.316801681518554 and perplexity is 553.7989315999321
At time: 42.51731634140015 and batch: 500, loss is 6.315393037796021 and perplexity is 553.0193753987672
At time: 43.121567249298096 and batch: 550, loss is 6.355370292663574 and perplexity is 575.5754325638496
At time: 43.72701334953308 and batch: 600, loss is 6.327604732513428 and perplexity is 559.8140821879964
At time: 44.33048748970032 and batch: 650, loss is 6.276179723739624 and perplexity is 531.7533339017718
At time: 44.95128655433655 and batch: 700, loss is 6.347400569915772 and perplexity is 571.0064867451694
At time: 45.55030179023743 and batch: 750, loss is 6.325603876113892 and perplexity is 558.6950944393145
At time: 46.1489999294281 and batch: 800, loss is 6.336197328567505 and perplexity is 564.6450640759062
At time: 46.751128911972046 and batch: 850, loss is 6.377810688018799 and perplexity is 588.6375846162721
At time: 47.3514621257782 and batch: 900, loss is 6.294661521911621 and perplexity is 541.6724710980416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.385611756207192 and perplexity of 593.2475444788568
finished 4 epochs...
Completing Train Step...
At time: 48.95166087150574 and batch: 50, loss is 6.344460668563843 and perplexity is 569.3302491966574
At time: 49.552528619766235 and batch: 100, loss is 6.32334119796753 and perplexity is 557.4323763596783
At time: 50.15805625915527 and batch: 150, loss is 6.364936742782593 and perplexity is 581.1080678749667
At time: 50.76366686820984 and batch: 200, loss is 6.308164844512939 and perplexity is 549.0364564592671
At time: 51.372124433517456 and batch: 250, loss is 6.393027048110962 and perplexity is 597.6629989015452
At time: 51.9803524017334 and batch: 300, loss is 6.3451012516021725 and perplexity is 569.6950693338214
At time: 52.58331298828125 and batch: 350, loss is 6.370819425582885 and perplexity is 584.5366169569772
At time: 53.18340563774109 and batch: 400, loss is 6.313054466247559 and perplexity is 551.7276110520984
At time: 53.782692193984985 and batch: 450, loss is 6.3156040668487545 and perplexity is 553.1360908684454
At time: 54.38310122489929 and batch: 500, loss is 6.3145393371582035 and perplexity is 552.5474638695117
At time: 54.98096966743469 and batch: 550, loss is 6.354552507400513 and perplexity is 575.1049278694431
At time: 55.582340717315674 and batch: 600, loss is 6.327072229385376 and perplexity is 559.5160587943408
At time: 56.182271003723145 and batch: 650, loss is 6.2751559638977055 and perplexity is 531.2092247587848
At time: 56.78119683265686 and batch: 700, loss is 6.347540044784546 and perplexity is 571.0861333541985
At time: 57.38493537902832 and batch: 750, loss is 6.3265976715087895 and perplexity is 559.2505990345552
At time: 57.98948621749878 and batch: 800, loss is 6.3377250385284425 and perplexity is 565.5083372122747
At time: 58.59292125701904 and batch: 850, loss is 6.379870805740357 and perplexity is 589.8514973081105
At time: 59.19619822502136 and batch: 900, loss is 6.296073217391967 and perplexity is 542.437687676629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.383687058540239 and perplexity of 592.1068204406679
finished 5 epochs...
Completing Train Step...
At time: 60.77640628814697 and batch: 50, loss is 6.343020124435425 and perplexity is 568.5106942932744
At time: 61.39014387130737 and batch: 100, loss is 6.322573575973511 and perplexity is 557.0046431971012
At time: 61.988698959350586 and batch: 150, loss is 6.362903919219971 and perplexity is 579.9279775662343
At time: 62.590434551239014 and batch: 200, loss is 6.3075856113433835 and perplexity is 548.718528418513
At time: 63.191489458084106 and batch: 250, loss is 6.392143430709839 and perplexity is 597.1351267285731
At time: 63.791136503219604 and batch: 300, loss is 6.344449138641357 and perplexity is 569.3236849008582
At time: 64.39423322677612 and batch: 350, loss is 6.37024884223938 and perplexity is 584.2031852340122
At time: 64.99883556365967 and batch: 400, loss is 6.312539072036743 and perplexity is 551.443327100859
At time: 65.60384130477905 and batch: 450, loss is 6.3152205181121825 and perplexity is 552.923976900263
At time: 66.20999050140381 and batch: 500, loss is 6.31444299697876 and perplexity is 552.4942339118243
At time: 66.81535315513611 and batch: 550, loss is 6.354579162597656 and perplexity is 575.1202576089815
At time: 67.41806888580322 and batch: 600, loss is 6.3281529521942135 and perplexity is 560.1210674254187
At time: 68.0182147026062 and batch: 650, loss is 6.275829162597656 and perplexity is 531.5669545163879
At time: 68.61996173858643 and batch: 700, loss is 6.347912912368774 and perplexity is 571.2991125651829
At time: 69.2212131023407 and batch: 750, loss is 6.327084321975708 and perplexity is 559.5228248337331
At time: 69.82535290718079 and batch: 800, loss is 6.338360738754273 and perplexity is 565.8679452793039
At time: 70.42721104621887 and batch: 850, loss is 6.380763301849365 and perplexity is 590.3781724671557
At time: 71.02965903282166 and batch: 900, loss is 6.296708307266235 and perplexity is 542.7822937758332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.384097582673373 and perplexity of 592.3499444805802
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.61582899093628 and batch: 50, loss is 6.33628246307373 and perplexity is 564.6931369009279
At time: 73.23817420005798 and batch: 100, loss is 6.3076220226287845 and perplexity is 548.738508329201
At time: 73.84388613700867 and batch: 150, loss is 6.339577121734619 and perplexity is 566.5566762124379
At time: 74.44612836837769 and batch: 200, loss is 6.28589054107666 and perplexity is 536.9422469127333
At time: 75.05247640609741 and batch: 250, loss is 6.379894561767578 and perplexity is 589.8655100027797
At time: 75.67162704467773 and batch: 300, loss is 6.319103736877441 and perplexity is 555.0752759421727
At time: 76.27332067489624 and batch: 350, loss is 6.342443485260009 and perplexity is 568.1829632556611
At time: 76.87670421600342 and batch: 400, loss is 6.288067932128906 and perplexity is 538.1126539111856
At time: 77.47904539108276 and batch: 450, loss is 6.291815252304077 and perplexity is 540.1329172383511
At time: 78.08070969581604 and batch: 500, loss is 6.289015483856201 and perplexity is 538.6227851355712
At time: 78.68615388870239 and batch: 550, loss is 6.321053524017334 and perplexity is 556.1586103694377
At time: 79.27939486503601 and batch: 600, loss is 6.3016830253601075 and perplexity is 545.48921016416
At time: 79.86369752883911 and batch: 650, loss is 6.238846616744995 and perplexity is 512.2673295190352
At time: 80.44965934753418 and batch: 700, loss is 6.318266115188599 and perplexity is 554.6105275209547
At time: 81.04945755004883 and batch: 750, loss is 6.290589475631714 and perplexity is 539.4712405252138
At time: 81.65409135818481 and batch: 800, loss is 6.304767951965332 and perplexity is 547.1746026609108
At time: 82.25842332839966 and batch: 850, loss is 6.341217775344848 and perplexity is 567.4869623986859
At time: 82.86432385444641 and batch: 900, loss is 6.2609872627258305 and perplexity is 523.7357497364818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355964556132277 and perplexity of 575.9175776690049
finished 7 epochs...
Completing Train Step...
At time: 84.48687314987183 and batch: 50, loss is 6.320454492568969 and perplexity is 555.8255536372357
At time: 85.08548951148987 and batch: 100, loss is 6.2987813472747805 and perplexity is 543.9086702951719
At time: 85.68981671333313 and batch: 150, loss is 6.333970794677734 and perplexity is 563.3892612676038
At time: 86.29310274124146 and batch: 200, loss is 6.282132406234741 and perplexity is 534.9281325736897
At time: 86.89844346046448 and batch: 250, loss is 6.374580593109131 and perplexity is 586.7392968259121
At time: 87.5002989768982 and batch: 300, loss is 6.314371356964111 and perplexity is 552.4546546345606
At time: 88.10510206222534 and batch: 350, loss is 6.338564500808716 and perplexity is 565.9832594423041
At time: 88.71251249313354 and batch: 400, loss is 6.286236448287964 and perplexity is 537.1280112347509
At time: 89.31570744514465 and batch: 450, loss is 6.2910243511199955 and perplexity is 539.7058943632541
At time: 89.91849541664124 and batch: 500, loss is 6.287083864212036 and perplexity is 537.5833749787303
At time: 90.53753519058228 and batch: 550, loss is 6.319571876525879 and perplexity is 555.3351895198878
At time: 91.13775277137756 and batch: 600, loss is 6.30073842048645 and perplexity is 544.9741816852128
At time: 91.73740339279175 and batch: 650, loss is 6.238405427932739 and perplexity is 512.0413727528311
At time: 92.33784770965576 and batch: 700, loss is 6.319325523376465 and perplexity is 555.1983977971965
At time: 92.93606472015381 and batch: 750, loss is 6.292492561340332 and perplexity is 540.498878064272
At time: 93.53630423545837 and batch: 800, loss is 6.308248882293701 and perplexity is 549.082598203423
At time: 94.1358757019043 and batch: 850, loss is 6.3439044094085695 and perplexity is 569.0136420991297
At time: 94.73670983314514 and batch: 900, loss is 6.263123416900635 and perplexity is 524.8557258397049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.355156467385488 and perplexity of 575.4523731439982
finished 8 epochs...
Completing Train Step...
At time: 96.33141994476318 and batch: 50, loss is 6.317489824295044 and perplexity is 554.1801554875287
At time: 96.95153522491455 and batch: 100, loss is 6.296448631286621 and perplexity is 542.6413645507389
At time: 97.55729818344116 and batch: 150, loss is 6.332253704071045 and perplexity is 562.4227009324915
At time: 98.16386675834656 and batch: 200, loss is 6.281135768890381 and perplexity is 534.3952688002931
At time: 98.76355957984924 and batch: 250, loss is 6.372833003997803 and perplexity is 585.7148130686575
At time: 99.36575746536255 and batch: 300, loss is 6.312790908813477 and perplexity is 551.5822182991292
At time: 99.97022914886475 and batch: 350, loss is 6.337494764328003 and perplexity is 565.3781302243117
At time: 100.57163071632385 and batch: 400, loss is 6.285936412811279 and perplexity is 536.9668779499191
At time: 101.17338252067566 and batch: 450, loss is 6.291156415939331 and perplexity is 539.7771752314313
At time: 101.77131724357605 and batch: 500, loss is 6.286464109420776 and perplexity is 537.2503083268729
At time: 102.3734200000763 and batch: 550, loss is 6.319149160385132 and perplexity is 555.1004899808894
At time: 102.97940278053284 and batch: 600, loss is 6.300604095458985 and perplexity is 544.9009829296125
At time: 103.58201169967651 and batch: 650, loss is 6.238514585494995 and perplexity is 512.0972689915479
At time: 104.18407487869263 and batch: 700, loss is 6.320292263031006 and perplexity is 555.7353896283114
At time: 104.7910692691803 and batch: 750, loss is 6.2937185573577885 and perplexity is 541.1619339050775
At time: 105.39349055290222 and batch: 800, loss is 6.3102059650421145 and perplexity is 550.1582505107801
At time: 106.00721955299377 and batch: 850, loss is 6.345135869979859 and perplexity is 569.7147915942719
At time: 106.61091589927673 and batch: 900, loss is 6.263969221115112 and perplexity is 525.2998388144495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.354868849662886 and perplexity of 575.2868866425347
finished 9 epochs...
Completing Train Step...
At time: 108.19095087051392 and batch: 50, loss is 6.315951509475708 and perplexity is 553.3283073150794
At time: 108.81065392494202 and batch: 100, loss is 6.295126457214355 and perplexity is 541.9243723067189
At time: 109.41219735145569 and batch: 150, loss is 6.331274480819702 and perplexity is 561.8722331060887
At time: 110.0156090259552 and batch: 200, loss is 6.280668478012085 and perplexity is 534.1456091021714
At time: 110.61869692802429 and batch: 250, loss is 6.372014083862305 and perplexity is 585.2353557599929
At time: 111.22410202026367 and batch: 300, loss is 6.311915512084961 and perplexity is 551.0995763121525
At time: 111.8299036026001 and batch: 350, loss is 6.336963262557983 and perplexity is 565.0777105912225
At time: 112.4376368522644 and batch: 400, loss is 6.285844955444336 and perplexity is 536.9177706187741
At time: 113.04178404808044 and batch: 450, loss is 6.288271713256836 and perplexity is 538.222322288546
At time: 113.64596581459045 and batch: 500, loss is 6.2544581317901615 and perplexity is 520.3273495046645
At time: 114.25013279914856 and batch: 550, loss is 6.1964696025848385 and perplexity is 491.01250828136835
At time: 114.85284399986267 and batch: 600, loss is 6.0841267299652095 and perplexity is 438.8364225978875
At time: 115.45460987091064 and batch: 650, loss is 5.980385837554931 and perplexity is 395.59297333860627
At time: 116.05957126617432 and batch: 700, loss is 6.0641956615448 and perplexity is 430.1765309549316
At time: 116.66357231140137 and batch: 750, loss is 6.006738977432251 and perplexity is 406.15667226072287
At time: 117.27057528495789 and batch: 800, loss is 6.029916868209839 and perplexity is 415.6804716838382
At time: 117.87557077407837 and batch: 850, loss is 6.0652923679351805 and perplexity is 430.6485671006379
At time: 118.48290228843689 and batch: 900, loss is 5.954179096221924 and perplexity is 385.36043695993567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.063716940683861 and perplexity of 429.97064576037775
finished 10 epochs...
Completing Train Step...
At time: 120.08672404289246 and batch: 50, loss is 6.01119213104248 and perplexity is 407.9693834572325
At time: 120.69202089309692 and batch: 100, loss is 5.992749786376953 and perplexity is 400.5144262171635
At time: 121.3100197315216 and batch: 150, loss is 6.045791416168213 and perplexity is 422.33186552279665
At time: 121.91424298286438 and batch: 200, loss is 5.984142036437988 and perplexity is 397.0816934372257
At time: 122.52397084236145 and batch: 250, loss is 6.0851939868927 and perplexity is 439.3050238244135
At time: 123.12869429588318 and batch: 300, loss is 6.006191873550415 and perplexity is 405.934523143559
At time: 123.72809338569641 and batch: 350, loss is 6.040351295471192 and perplexity is 420.0405673204361
At time: 124.32690930366516 and batch: 400, loss is 5.967706422805787 and perplexity is 390.6087512423017
At time: 124.9299373626709 and batch: 450, loss is 5.970700845718384 and perplexity is 391.78015199622445
At time: 125.53389811515808 and batch: 500, loss is 5.974632759094238 and perplexity is 393.32363003872644
At time: 126.13476300239563 and batch: 550, loss is 6.00591272354126 and perplexity is 405.821222332404
At time: 126.73577737808228 and batch: 600, loss is 5.970354881286621 and perplexity is 391.64463344221446
At time: 127.34034419059753 and batch: 650, loss is 5.9090084457397465 and perplexity is 368.3407444425183
At time: 127.94121479988098 and batch: 700, loss is 6.017067499160767 and perplexity is 410.37340911832996
At time: 128.5417263507843 and batch: 750, loss is 5.963139629364013 and perplexity is 388.82898875582185
At time: 129.1411952972412 and batch: 800, loss is 5.987609901428223 and perplexity is 398.4611095722058
At time: 129.74114656448364 and batch: 850, loss is 6.025323448181152 and perplexity is 413.7754552994535
At time: 130.3371479511261 and batch: 900, loss is 5.910408535003662 and perplexity is 368.8568155527831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.024101361836473 and perplexity of 413.27009482573175
finished 11 epochs...
Completing Train Step...
At time: 131.92490196228027 and batch: 50, loss is 5.961666994094848 and perplexity is 388.25680688431385
At time: 132.54375743865967 and batch: 100, loss is 5.943791246414184 and perplexity is 381.3780904380585
At time: 133.14437890052795 and batch: 150, loss is 5.998231248855591 and perplexity is 402.7158590402384
At time: 133.74629068374634 and batch: 200, loss is 5.935609703063965 and perplexity is 378.2705585957765
At time: 134.35115218162537 and batch: 250, loss is 6.038929834365844 and perplexity is 419.4439201471603
At time: 134.95100903511047 and batch: 300, loss is 5.963676242828369 and perplexity is 389.0376956189684
At time: 135.5503706932068 and batch: 350, loss is 6.0056916618347165 and perplexity is 405.7315207156051
At time: 136.1645896434784 and batch: 400, loss is 5.934621982574463 and perplexity is 377.89711747256104
At time: 136.76094102859497 and batch: 450, loss is 5.938105916976928 and perplexity is 379.2159823260245
At time: 137.3592562675476 and batch: 500, loss is 5.945632696151733 and perplexity is 382.08102603425306
At time: 137.9588167667389 and batch: 550, loss is 5.97820122718811 and perplexity is 394.7297001291432
At time: 138.558580160141 and batch: 600, loss is 5.944128675460815 and perplexity is 381.5068001975055
At time: 139.15993857383728 and batch: 650, loss is 5.884067068099975 and perplexity is 359.2674395466289
At time: 139.76413774490356 and batch: 700, loss is 5.993424596786499 and perplexity is 400.7847887326087
At time: 140.3664186000824 and batch: 750, loss is 5.939749841690063 and perplexity is 379.83989754529966
At time: 140.9693522453308 and batch: 800, loss is 5.967138242721558 and perplexity is 390.38687816702054
At time: 141.57219195365906 and batch: 850, loss is 6.005028648376465 and perplexity is 405.46260441434816
At time: 142.1717791557312 and batch: 900, loss is 5.892052307128906 and perplexity is 362.1477606478973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.003081021243578 and perplexity of 404.67368295642
finished 12 epochs...
Completing Train Step...
At time: 143.75732493400574 and batch: 50, loss is 5.943745975494385 and perplexity is 381.36082549191593
At time: 144.37215185165405 and batch: 100, loss is 5.925947771072388 and perplexity is 374.63333378899097
At time: 144.97397899627686 and batch: 150, loss is 5.980395231246948 and perplexity is 395.5966894346159
At time: 145.57720255851746 and batch: 200, loss is 5.919387311935425 and perplexity is 372.1836115489045
At time: 146.17669081687927 and batch: 250, loss is 6.021537628173828 and perplexity is 412.21193736839143
At time: 146.78216171264648 and batch: 300, loss is 5.949195251464844 and perplexity is 383.4446383535224
At time: 147.38396286964417 and batch: 350, loss is 5.990999727249146 and perplexity is 399.81411526127135
At time: 147.98836398124695 and batch: 400, loss is 5.921178579330444 and perplexity is 372.85088937531856
At time: 148.590398311615 and batch: 450, loss is 5.925467014312744 and perplexity is 374.45326956839773
At time: 149.19419407844543 and batch: 500, loss is 5.930599021911621 and perplexity is 376.3799061213978
At time: 149.7919888496399 and batch: 550, loss is 5.965267820358276 and perplexity is 389.6573722745132
At time: 150.39225959777832 and batch: 600, loss is 5.930639238357544 and perplexity is 376.39504308791425
At time: 150.99231100082397 and batch: 650, loss is 5.8717749881744385 and perplexity is 354.8783263942798
At time: 151.60933423042297 and batch: 700, loss is 5.983713235855102 and perplexity is 396.91146107610285
At time: 152.21054983139038 and batch: 750, loss is 5.930130043029785 and perplexity is 376.2034332781324
At time: 152.81308341026306 and batch: 800, loss is 5.95873046875 and perplexity is 387.11835329758463
At time: 153.41050267219543 and batch: 850, loss is 5.9963525295257565 and perplexity is 401.95997923675617
At time: 154.01394701004028 and batch: 900, loss is 5.883643426895142 and perplexity is 359.1152712903294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.994364699272261 and perplexity of 401.16174466959563
finished 13 epochs...
Completing Train Step...
At time: 155.6258568763733 and batch: 50, loss is 5.936756324768067 and perplexity is 378.70454058732264
At time: 156.22966289520264 and batch: 100, loss is 5.917180156707763 and perplexity is 371.3630504310209
At time: 156.82888913154602 and batch: 150, loss is 5.972767124176025 and perplexity is 392.590515814755
At time: 157.43251037597656 and batch: 200, loss is 5.91274393081665 and perplexity is 369.7192488850325
At time: 158.0341353416443 and batch: 250, loss is 6.013556137084961 and perplexity is 408.9349664171663
At time: 158.6358630657196 and batch: 300, loss is 5.940024366378784 and perplexity is 379.9441872893369
At time: 159.23736095428467 and batch: 350, loss is 5.983934211730957 and perplexity is 396.999178625225
At time: 159.84248208999634 and batch: 400, loss is 5.912347240447998 and perplexity is 369.5726139061593
At time: 160.44270205497742 and batch: 450, loss is 5.9176533412933345 and perplexity is 371.53881528346545
At time: 161.04692387580872 and batch: 500, loss is 5.921551990509033 and perplexity is 372.99014206299506
At time: 161.6536934375763 and batch: 550, loss is 5.958444051742553 and perplexity is 387.0074918943591
At time: 162.26248979568481 and batch: 600, loss is 5.922908668518066 and perplexity is 373.4965129998075
At time: 162.8666980266571 and batch: 650, loss is 5.86496335029602 and perplexity is 352.46923797775355
At time: 163.47401309013367 and batch: 700, loss is 5.976394891738892 and perplexity is 394.0173294628306
At time: 164.07869362831116 and batch: 750, loss is 5.923323068618775 and perplexity is 373.65132206663986
At time: 164.67971420288086 and batch: 800, loss is 5.951795682907105 and perplexity is 384.4430574450329
At time: 165.28161239624023 and batch: 850, loss is 5.987925567626953 and perplexity is 398.5869101304538
At time: 165.8844666481018 and batch: 900, loss is 5.876639652252197 and perplexity is 356.60889614810264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.982599127782534 and perplexity of 396.4695050523244
finished 14 epochs...
Completing Train Step...
At time: 167.5002202987671 and batch: 50, loss is 5.93440613746643 and perplexity is 377.8155590307271
At time: 168.1166865825653 and batch: 100, loss is 5.909702730178833 and perplexity is 368.59656648602214
At time: 168.7237365245819 and batch: 150, loss is 5.96604549407959 and perplexity is 389.9605164315917
At time: 169.33176708221436 and batch: 200, loss is 5.904890909194946 and perplexity is 366.8272061293856
At time: 169.93873357772827 and batch: 250, loss is 6.007429418563842 and perplexity is 406.4371963646678
At time: 170.54503345489502 and batch: 300, loss is 5.9327050304412845 and perplexity is 377.1734006739511
At time: 171.1473491191864 and batch: 350, loss is 5.97766300201416 and perplexity is 394.5173038312724
At time: 171.7444257736206 and batch: 400, loss is 5.905507154464722 and perplexity is 367.05333132712366
At time: 172.347998380661 and batch: 450, loss is 5.910987844467163 and perplexity is 369.0705597027364
At time: 172.94838905334473 and batch: 500, loss is 5.914881582260132 and perplexity is 370.51042509930596
At time: 173.54955554008484 and batch: 550, loss is 5.952437438964844 and perplexity is 384.6898552895395
At time: 174.14967346191406 and batch: 600, loss is 5.917445125579834 and perplexity is 371.4614631171959
At time: 174.75286316871643 and batch: 650, loss is 5.855271968841553 and perplexity is 349.06982326077633
At time: 175.35397291183472 and batch: 700, loss is 5.968570947647095 and perplexity is 390.9465882241729
At time: 175.96380424499512 and batch: 750, loss is 5.917291164398193 and perplexity is 371.40427687374404
At time: 176.57142281532288 and batch: 800, loss is 5.944334421157837 and perplexity is 381.58530165542237
At time: 177.17878651618958 and batch: 850, loss is 5.9821755313873295 and perplexity is 396.30159756418965
At time: 177.79000067710876 and batch: 900, loss is 5.870499687194824 and perplexity is 354.42603818008644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.986131223913741 and perplexity of 397.87234948951027
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 179.37310934066772 and batch: 50, loss is 5.910432119369506 and perplexity is 368.8655149094492
At time: 179.98631262779236 and batch: 100, loss is 5.889093933105468 and perplexity is 361.0779753129512
At time: 180.58358478546143 and batch: 150, loss is 5.945498275756836 and perplexity is 382.02967000357694
At time: 181.18121027946472 and batch: 200, loss is 5.883390798568725 and perplexity is 359.0245600589485
At time: 181.78130722045898 and batch: 250, loss is 5.986948757171631 and perplexity is 398.1977563649601
At time: 182.395343542099 and batch: 300, loss is 5.910826988220215 and perplexity is 369.0111971721885
At time: 182.99674344062805 and batch: 350, loss is 5.954643316268921 and perplexity is 385.5393705291538
At time: 183.5950539112091 and batch: 400, loss is 5.880676403045654 and perplexity is 358.0513468408012
At time: 184.19877910614014 and batch: 450, loss is 5.887690210342408 and perplexity is 360.57147751417784
At time: 184.80379557609558 and batch: 500, loss is 5.890020036697388 and perplexity is 361.41252581315047
At time: 185.4065821170807 and batch: 550, loss is 5.928215074539184 and perplexity is 375.48370490586956
At time: 186.0064492225647 and batch: 600, loss is 5.893971862792969 and perplexity is 362.843591062023
At time: 186.60649371147156 and batch: 650, loss is 5.830119724273682 and perplexity is 340.3994307068231
At time: 187.20330572128296 and batch: 700, loss is 5.941468496322631 and perplexity is 380.49327244692404
At time: 187.80233645439148 and batch: 750, loss is 5.890546684265137 and perplexity is 361.6029129698899
At time: 188.39894890785217 and batch: 800, loss is 5.918050117492676 and perplexity is 371.686262292299
At time: 188.99852538108826 and batch: 850, loss is 5.956509914398193 and perplexity is 386.25968966097224
At time: 189.59761834144592 and batch: 900, loss is 5.8452507019042965 and perplexity is 345.589170784819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.9601507317529965 and perplexity of 387.6685537947731
finished 16 epochs...
Completing Train Step...
At time: 191.21649312973022 and batch: 50, loss is 5.896961832046509 and perplexity is 363.93010575646167
At time: 191.81700682640076 and batch: 100, loss is 5.875481224060058 and perplexity is 356.1960295335708
At time: 192.41630029678345 and batch: 150, loss is 5.933130874633789 and perplexity is 377.3340519799803
At time: 193.0204451084137 and batch: 200, loss is 5.871053743362427 and perplexity is 354.6224645230875
At time: 193.61927390098572 and batch: 250, loss is 5.975273838043213 and perplexity is 393.57586237986226
At time: 194.21845722198486 and batch: 300, loss is 5.899124593734741 and perplexity is 364.7180516087996
At time: 194.81762290000916 and batch: 350, loss is 5.94245436668396 and perplexity is 380.8685744560805
At time: 195.4171175956726 and batch: 400, loss is 5.870086154937744 and perplexity is 354.27950188138124
At time: 196.0175597667694 and batch: 450, loss is 5.8778875541687015 and perplexity is 357.05418685482545
At time: 196.6201548576355 and batch: 500, loss is 5.879488363265991 and perplexity is 357.6262201812872
At time: 197.23418927192688 and batch: 550, loss is 5.9188884735107425 and perplexity is 371.9979983617709
At time: 197.8377549648285 and batch: 600, loss is 5.884159564971924 and perplexity is 359.30067219791437
At time: 198.43932008743286 and batch: 650, loss is 5.81843373298645 and perplexity is 336.4446785309058
At time: 199.04382634162903 and batch: 700, loss is 5.930403966903686 and perplexity is 376.3064984953181
At time: 199.6480643749237 and batch: 750, loss is 5.880841827392578 and perplexity is 358.11058215036365
At time: 200.25131249427795 and batch: 800, loss is 5.908355112075806 and perplexity is 368.1001736294356
At time: 200.85455346107483 and batch: 850, loss is 5.947590169906616 and perplexity is 382.8296721034206
At time: 201.45424914360046 and batch: 900, loss is 5.837324228286743 and perplexity is 342.86069522046677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.953169313195634 and perplexity of 384.9715029342271
finished 17 epochs...
Completing Train Step...
At time: 203.03854608535767 and batch: 50, loss is 5.888826370239258 and perplexity is 360.9813771785641
At time: 203.6522409915924 and batch: 100, loss is 5.866641063690185 and perplexity is 353.0610766683674
At time: 204.25396156311035 and batch: 150, loss is 5.924802417755127 and perplexity is 374.2044918920047
At time: 204.8545365333557 and batch: 200, loss is 5.862409629821777 and perplexity is 351.5702784024716
At time: 205.456866979599 and batch: 250, loss is 5.966784677505493 and perplexity is 390.2488753440292
At time: 206.06098461151123 and batch: 300, loss is 5.891364603042603 and perplexity is 361.89879576994826
At time: 206.67165112495422 and batch: 350, loss is 5.934496488571167 and perplexity is 377.8496966260337
At time: 207.27732276916504 and batch: 400, loss is 5.862695732116699 and perplexity is 351.67087785613
At time: 207.87725496292114 and batch: 450, loss is 5.870521726608277 and perplexity is 354.4338496081595
At time: 208.47648549079895 and batch: 500, loss is 5.872031879425049 and perplexity is 354.96950324212753
At time: 209.07511186599731 and batch: 550, loss is 5.9120578956604 and perplexity is 369.46569546557924
At time: 209.67678260803223 and batch: 600, loss is 5.876519565582275 and perplexity is 356.56607474449106
At time: 210.27640771865845 and batch: 650, loss is 5.811376314163208 and perplexity is 334.0786065248213
At time: 210.87849926948547 and batch: 700, loss is 5.922651119232178 and perplexity is 373.4003316258564
At time: 211.47626304626465 and batch: 750, loss is 5.8740169715881345 and perplexity is 355.67485027896174
At time: 212.07866668701172 and batch: 800, loss is 5.9014663410186765 and perplexity is 365.57312991345043
At time: 212.69749760627747 and batch: 850, loss is 5.941420593261719 and perplexity is 380.47504609106977
At time: 213.29996156692505 and batch: 900, loss is 5.831128282546997 and perplexity is 340.7429165525242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.946039905286815 and perplexity of 382.2366446008962
finished 18 epochs...
Completing Train Step...
At time: 214.8866147994995 and batch: 50, loss is 5.883012485504151 and perplexity is 358.88876206607205
At time: 215.50007820129395 and batch: 100, loss is 5.860704078674316 and perplexity is 350.9711683623667
At time: 216.10154914855957 and batch: 150, loss is 5.918134031295776 and perplexity is 371.7174532087838
At time: 216.70401430130005 and batch: 200, loss is 5.85571891784668 and perplexity is 349.22587454188726
At time: 217.30152678489685 and batch: 250, loss is 5.961643400192261 and perplexity is 388.24764649909844
At time: 217.8985607624054 and batch: 300, loss is 5.885888261795044 and perplexity is 359.922331303735
At time: 218.49861240386963 and batch: 350, loss is 5.928565101623535 and perplexity is 375.61515737693946
At time: 219.09702062606812 and batch: 400, loss is 5.856987009048462 and perplexity is 349.6690057068125
At time: 219.70080757141113 and batch: 450, loss is 5.864966869354248 and perplexity is 352.470478339708
At time: 220.30515575408936 and batch: 500, loss is 5.866423101425171 and perplexity is 352.9841310623322
At time: 220.9081425666809 and batch: 550, loss is 5.906794500350952 and perplexity is 367.52616020509026
At time: 221.51246881484985 and batch: 600, loss is 5.87101749420166 and perplexity is 354.60960998934405
At time: 222.11268162727356 and batch: 650, loss is 5.805139865875244 and perplexity is 332.0016257942875
At time: 222.71156525611877 and batch: 700, loss is 5.916955213546753 and perplexity is 371.27952424724873
At time: 223.31211400032043 and batch: 750, loss is 5.868641042709351 and perplexity is 353.76789799249536
At time: 223.90859007835388 and batch: 800, loss is 5.896459312438965 and perplexity is 363.7472696857461
At time: 224.51266813278198 and batch: 850, loss is 5.93647686958313 and perplexity is 378.59872442602256
At time: 225.11252999305725 and batch: 900, loss is 5.827018423080444 and perplexity is 339.3453848478585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.941838512681935 and perplexity of 380.63408723261483
finished 19 epochs...
Completing Train Step...
At time: 226.7138433456421 and batch: 50, loss is 5.878777341842651 and perplexity is 357.3720306549986
At time: 227.3182544708252 and batch: 100, loss is 5.856148920059204 and perplexity is 349.37607473150354
At time: 227.9393768310547 and batch: 150, loss is 5.913331365585327 and perplexity is 369.9364986302561
At time: 228.5407841205597 and batch: 200, loss is 5.850794086456299 and perplexity is 347.51022410478276
At time: 229.14693427085876 and batch: 250, loss is 5.957778387069702 and perplexity is 386.7499604031936
At time: 229.74457359313965 and batch: 300, loss is 5.881647577285767 and perplexity is 358.39924599349405
At time: 230.34677052497864 and batch: 350, loss is 5.925191984176636 and perplexity is 374.35029779552167
At time: 230.94493126869202 and batch: 400, loss is 5.85218976020813 and perplexity is 347.9955736190632
At time: 231.54583764076233 and batch: 450, loss is 5.861116056442261 and perplexity is 351.11579046942245
At time: 232.14384508132935 and batch: 500, loss is 5.861909265518189 and perplexity is 351.3944091879641
At time: 232.74497532844543 and batch: 550, loss is 5.9031193065643315 and perplexity is 366.1779094037417
At time: 233.34614968299866 and batch: 600, loss is 5.867214336395263 and perplexity is 353.26353497321224
At time: 233.94266748428345 and batch: 650, loss is 5.800625820159912 and perplexity is 330.5063327276005
At time: 234.54728722572327 and batch: 700, loss is 5.912286195755005 and perplexity is 369.55005414798853
At time: 235.15138125419617 and batch: 750, loss is 5.8646278572082515 and perplexity is 352.35100681873826
At time: 235.7549171447754 and batch: 800, loss is 5.892347717285157 and perplexity is 362.25475857781566
At time: 236.3611912727356 and batch: 850, loss is 5.93316611289978 and perplexity is 377.34734881194885
At time: 236.9609055519104 and batch: 900, loss is 5.822724885940552 and perplexity is 337.8915161877473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.9369134772313785 and perplexity of 378.76405961540183
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
497.35953855514526


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8979520797729492 and batch: 50, loss is 7.9923837375640865 and perplexity is 2958.340468512319
At time: 1.5509998798370361 and batch: 100, loss is 7.202895088195801 and perplexity is 1343.3141532537352
At time: 2.1904046535491943 and batch: 150, loss is 6.9793734359741215 and perplexity is 1074.245072769254
At time: 2.8272125720977783 and batch: 200, loss is 6.772568712234497 and perplexity is 873.5529205568647
At time: 3.4642608165740967 and batch: 250, loss is 6.7550883483886714 and perplexity is 858.4155861722609
At time: 4.105948448181152 and batch: 300, loss is 6.610769176483155 and perplexity is 743.0543388886141
At time: 4.745009660720825 and batch: 350, loss is 6.560404329299927 and perplexity is 706.5573186744008
At time: 5.386251926422119 and batch: 400, loss is 6.422993831634521 and perplexity is 615.8440904838853
At time: 6.031905651092529 and batch: 450, loss is 6.362299356460571 and perplexity is 579.5774806672257
At time: 6.673651456832886 and batch: 500, loss is 6.316245775222779 and perplexity is 553.4911568421506
At time: 7.301604270935059 and batch: 550, loss is 6.317509155273438 and perplexity is 554.1908684356864
At time: 7.963539123535156 and batch: 600, loss is 6.24698410987854 and perplexity is 516.4529083580941
At time: 8.600789070129395 and batch: 650, loss is 6.177378120422364 and perplexity is 481.72726826068407
At time: 9.241811275482178 and batch: 700, loss is 6.274748668670655 and perplexity is 530.9929098319842
At time: 9.87929654121399 and batch: 750, loss is 6.201910028457641 and perplexity is 493.69110518242564
At time: 10.516961097717285 and batch: 800, loss is 6.212145986557007 and perplexity is 498.7704583231063
At time: 11.152117729187012 and batch: 850, loss is 6.244084701538086 and perplexity is 514.9576691900919
At time: 11.786800622940063 and batch: 900, loss is 6.120158271789551 and perplexity is 454.93669244619855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.092809389715326 and perplexity of 442.6632795479221
finished 1 epochs...
Completing Train Step...
At time: 13.413105726242065 and batch: 50, loss is 5.932700757980347 and perplexity is 377.17178921877235
At time: 14.014928817749023 and batch: 100, loss is 5.696846265792846 and perplexity is 297.9263373335013
At time: 14.618197441101074 and batch: 150, loss is 5.524555473327637 and perplexity is 250.77483713625745
At time: 15.221200466156006 and batch: 200, loss is 5.28654899597168 and perplexity is 197.66012117029027
At time: 15.823559761047363 and batch: 250, loss is 5.288816728591919 and perplexity is 198.10887010377414
At time: 16.426409244537354 and batch: 300, loss is 5.167961826324463 and perplexity is 175.55665763353463
At time: 17.02974843978882 and batch: 350, loss is 5.111260633468628 and perplexity is 165.87933616457178
At time: 17.633615970611572 and batch: 400, loss is 4.945725049972534 and perplexity is 140.57273616950795
At time: 18.237695455551147 and batch: 450, loss is 4.927080268859863 and perplexity is 137.9760706258986
At time: 18.8403639793396 and batch: 500, loss is 4.8450525283813475 and perplexity is 127.10995869876072
At time: 19.44396710395813 and batch: 550, loss is 4.891180295944213 and perplexity is 133.1105912606635
At time: 20.047189235687256 and batch: 600, loss is 4.799288158416748 and perplexity is 121.42395212922774
At time: 20.647642850875854 and batch: 650, loss is 4.667274007797241 and perplexity is 106.4072812957937
At time: 21.250661611557007 and batch: 700, loss is 4.734376373291016 and perplexity is 113.79247256090856
At time: 21.85414433479309 and batch: 750, loss is 4.726446628570557 and perplexity is 112.89369554817684
At time: 22.459065198898315 and batch: 800, loss is 4.6751041603088375 and perplexity is 107.24373705107332
At time: 23.059481859207153 and batch: 850, loss is 4.7126235675811765 and perplexity is 111.34389527864364
At time: 23.677638053894043 and batch: 900, loss is 4.638109016418457 and perplexity is 103.34873193332116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7242715913955475 and perplexity of 112.6484144080322
finished 2 epochs...
Completing Train Step...
At time: 25.265520811080933 and batch: 50, loss is 4.695223989486695 and perplexity is 109.42331556935008
At time: 25.882708072662354 and batch: 100, loss is 4.571733026504517 and perplexity is 96.71156833708928
At time: 26.48541760444641 and batch: 150, loss is 4.570699834823609 and perplexity is 96.61169835053961
At time: 27.086941242218018 and batch: 200, loss is 4.455173549652099 and perplexity is 86.07108715623686
At time: 27.69029402732849 and batch: 250, loss is 4.573127517700195 and perplexity is 96.84652584431346
At time: 28.294651985168457 and batch: 300, loss is 4.5243744277954105 and perplexity is 92.23820611866805
At time: 28.89559268951416 and batch: 350, loss is 4.515842428207398 and perplexity is 91.45457749529291
At time: 29.497943878173828 and batch: 400, loss is 4.405186514854432 and perplexity is 81.8744122132168
At time: 30.100357055664062 and batch: 450, loss is 4.437688522338867 and perplexity is 84.57921258877616
At time: 30.70562481880188 and batch: 500, loss is 4.331183338165284 and perplexity is 76.03420753488221
At time: 31.31415820121765 and batch: 550, loss is 4.396562442779541 and perplexity is 81.17135733661279
At time: 31.918498992919922 and batch: 600, loss is 4.369142951965332 and perplexity is 78.97591653208386
At time: 32.53537726402283 and batch: 650, loss is 4.220192666053772 and perplexity is 68.04659329515172
At time: 33.106202602386475 and batch: 700, loss is 4.255896673202515 and perplexity is 70.52002226125258
At time: 33.70588827133179 and batch: 750, loss is 4.319787096977234 and perplexity is 75.17262210972486
At time: 34.31103301048279 and batch: 800, loss is 4.281426978111267 and perplexity is 72.3435991193566
At time: 34.8968391418457 and batch: 850, loss is 4.339159049987793 and perplexity is 76.64305924821896
At time: 35.49348497390747 and batch: 900, loss is 4.279700117111206 and perplexity is 72.2187795834458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.476050808005137 and perplexity of 87.88690416694322
finished 3 epochs...
Completing Train Step...
At time: 37.1029999256134 and batch: 50, loss is 4.3691126823425295 and perplexity is 78.9735259970604
At time: 37.7203848361969 and batch: 100, loss is 4.243697905540467 and perplexity is 69.66499066314285
At time: 38.32386255264282 and batch: 150, loss is 4.244297442436218 and perplexity is 69.70676991823856
At time: 38.94258236885071 and batch: 200, loss is 4.128072466850281 and perplexity is 62.05818836037566
At time: 39.54799938201904 and batch: 250, loss is 4.271326622962952 and perplexity is 71.61658082770171
At time: 40.151915550231934 and batch: 300, loss is 4.233427734375 and perplexity is 68.95318074621932
At time: 40.7548668384552 and batch: 350, loss is 4.232059650421142 and perplexity is 68.85891150489446
At time: 41.358962535858154 and batch: 400, loss is 4.140040941238404 and perplexity is 62.80539272712234
At time: 41.96332359313965 and batch: 450, loss is 4.182504372596741 and perplexity is 65.52975882024965
At time: 42.566760301589966 and batch: 500, loss is 4.065523018836975 and perplexity is 58.29539013777854
At time: 43.170737981796265 and batch: 550, loss is 4.13027624130249 and perplexity is 62.19510141777246
At time: 43.776721715927124 and batch: 600, loss is 4.128637986183167 and perplexity is 62.093293391008494
At time: 44.37690496444702 and batch: 650, loss is 3.9766229581832886 and perplexity is 53.33660978690836
At time: 44.97989630699158 and batch: 700, loss is 3.9925186109542845 and perplexity is 54.19120418975895
At time: 45.58357238769531 and batch: 750, loss is 4.084773125648499 and perplexity is 59.42845342984577
At time: 46.187748670578 and batch: 800, loss is 4.053315382003785 and perplexity is 57.588067339972625
At time: 46.79000377655029 and batch: 850, loss is 4.114259157180786 and perplexity is 61.20685280458446
At time: 47.391984701156616 and batch: 900, loss is 4.064242620468139 and perplexity is 58.220796580260995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385218790132705 and perplexity of 80.25578046501255
finished 4 epochs...
Completing Train Step...
At time: 48.98765969276428 and batch: 50, loss is 4.155486307144165 and perplexity is 63.78297512334814
At time: 49.590858697891235 and batch: 100, loss is 4.036158828735352 and perplexity is 56.60848177903772
At time: 50.19726753234863 and batch: 150, loss is 4.03840856552124 and perplexity is 56.735979327075825
At time: 50.801536083221436 and batch: 200, loss is 3.9257702255249023 and perplexity is 50.692107381235076
At time: 51.40815234184265 and batch: 250, loss is 4.077775073051453 and perplexity is 59.01402178552475
At time: 52.01149916648865 and batch: 300, loss is 4.0378823614120485 and perplexity is 56.70613247507169
At time: 52.61299967765808 and batch: 350, loss is 4.040087876319885 and perplexity is 56.831336714767254
At time: 53.215399980545044 and batch: 400, loss is 3.957974739074707 and perplexity is 52.351193685648745
At time: 53.83278846740723 and batch: 450, loss is 3.997851891517639 and perplexity is 54.48099316182178
At time: 54.43301296234131 and batch: 500, loss is 3.884411106109619 and perplexity is 48.63829123150706
At time: 55.03417158126831 and batch: 550, loss is 3.945252695083618 and perplexity is 51.68939811871759
At time: 55.63634181022644 and batch: 600, loss is 3.958445529937744 and perplexity is 52.37584595188055
At time: 56.23780846595764 and batch: 650, loss is 3.8072385120391847 and perplexity is 45.02592846906979
At time: 56.84180951118469 and batch: 700, loss is 3.816364893913269 and perplexity is 45.438733127757544
At time: 57.44464731216431 and batch: 750, loss is 3.9151244592666625 and perplexity is 50.1553134161243
At time: 58.04766392707825 and batch: 800, loss is 3.885761413574219 and perplexity is 48.704012241021296
At time: 58.65454602241516 and batch: 850, loss is 3.945749487876892 and perplexity is 51.715083418799345
At time: 59.25477385520935 and batch: 900, loss is 3.9047037887573244 and perplexity is 49.63537517826868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.354850037457192 and perplexity of 77.85514910522073
finished 5 epochs...
Completing Train Step...
At time: 60.8616259098053 and batch: 50, loss is 3.9971038246154786 and perplexity is 54.44025297413346
At time: 61.47989344596863 and batch: 100, loss is 3.8808073234558105 and perplexity is 48.46332486100253
At time: 62.08454155921936 and batch: 150, loss is 3.8827899885177612 and perplexity is 48.55950671868757
At time: 62.69226884841919 and batch: 200, loss is 3.7738365840911867 and perplexity is 43.54681577493642
At time: 63.29568290710449 and batch: 250, loss is 3.922659845352173 and perplexity is 50.53468061099333
At time: 63.89692950248718 and batch: 300, loss is 3.887614984512329 and perplexity is 48.79437230120035
At time: 64.50022387504578 and batch: 350, loss is 3.891725625991821 and perplexity is 48.99536128576045
At time: 65.10411071777344 and batch: 400, loss is 3.8151591634750366 and perplexity is 45.3839792799745
At time: 65.70342302322388 and batch: 450, loss is 3.8543369722366334 and perplexity is 47.19731343571066
At time: 66.30566239356995 and batch: 500, loss is 3.7431121301651 and perplexity is 42.22920866870315
At time: 66.91001081466675 and batch: 550, loss is 3.799256081581116 and perplexity is 44.66794282488806
At time: 67.51455593109131 and batch: 600, loss is 3.822638897895813 and perplexity is 45.72471209887981
At time: 68.1197497844696 and batch: 650, loss is 3.6727074193954468 and perplexity is 39.35832122204301
At time: 68.72325325012207 and batch: 700, loss is 3.6770578527450564 and perplexity is 39.52991996908437
At time: 69.34177112579346 and batch: 750, loss is 3.7811197996139527 and perplexity is 43.86513440404001
At time: 69.9439377784729 and batch: 800, loss is 3.7550711488723754 and perplexity is 42.7372604102701
At time: 70.54950547218323 and batch: 850, loss is 3.8141315841674803 and perplexity is 45.337367594675
At time: 71.14955854415894 and batch: 900, loss is 3.774946303367615 and perplexity is 43.5951673391984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341984160958904 and perplexity of 76.85989053777271
finished 6 epochs...
Completing Train Step...
At time: 72.75187683105469 and batch: 50, loss is 3.867322201728821 and perplexity is 47.81417777685409
At time: 73.37055015563965 and batch: 100, loss is 3.7560372829437254 and perplexity is 42.77857028589451
At time: 73.97329258918762 and batch: 150, loss is 3.7575453186035155 and perplexity is 42.843130562725214
At time: 74.57888865470886 and batch: 200, loss is 3.648099145889282 and perplexity is 38.40160078711629
At time: 75.18249344825745 and batch: 250, loss is 3.7969798135757444 and perplexity is 44.56638224916669
At time: 75.78957343101501 and batch: 300, loss is 3.764796118736267 and perplexity is 43.154906486006126
At time: 76.39391112327576 and batch: 350, loss is 3.7677937364578247 and perplexity is 43.28446248112146
At time: 77.00168943405151 and batch: 400, loss is 3.698466100692749 and perplexity is 40.38530982677046
At time: 77.60545802116394 and batch: 450, loss is 3.735215497016907 and perplexity is 41.897053280343826
At time: 78.21066403388977 and batch: 500, loss is 3.6278246068954467 and perplexity is 37.63086558613169
At time: 78.81081509590149 and batch: 550, loss is 3.679266052246094 and perplexity is 39.61730636642659
At time: 79.40882968902588 and batch: 600, loss is 3.7049061822891236 and perplexity is 40.64623380138293
At time: 80.0155885219574 and batch: 650, loss is 3.561673192977905 and perplexity is 35.22208120811334
At time: 80.61741590499878 and batch: 700, loss is 3.5646116161346435 and perplexity is 35.32573079575931
At time: 81.2205400466919 and batch: 750, loss is 3.6670673847198487 and perplexity is 39.1369637442838
At time: 81.82563328742981 and batch: 800, loss is 3.645315318107605 and perplexity is 38.29484600638953
At time: 82.42937183380127 and batch: 850, loss is 3.7001812744140623 and perplexity is 40.454637086063386
At time: 83.0334541797638 and batch: 900, loss is 3.663580904006958 and perplexity is 39.00075106444707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342589077884203 and perplexity of 76.90639845172961
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 84.63560152053833 and batch: 50, loss is 3.7840030860900877 and perplexity is 43.99179266111479
At time: 85.23872113227844 and batch: 100, loss is 3.673686356544495 and perplexity is 39.39686940985952
At time: 85.84308409690857 and batch: 150, loss is 3.6847624254226683 and perplexity is 39.83565738611043
At time: 86.44612979888916 and batch: 200, loss is 3.5600582361221313 and perplexity is 35.16524497297828
At time: 87.0492148399353 and batch: 250, loss is 3.700290198326111 and perplexity is 40.4590438033894
At time: 87.65331196784973 and batch: 300, loss is 3.660394997596741 and perplexity is 38.87669604040559
At time: 88.25351619720459 and batch: 350, loss is 3.649917998313904 and perplexity is 38.47151119089773
At time: 88.85788178443909 and batch: 400, loss is 3.5715104293823243 and perplexity is 35.57027899161326
At time: 89.46248364448547 and batch: 450, loss is 3.5932808446884157 and perplexity is 36.353149526204675
At time: 90.06523585319519 and batch: 500, loss is 3.47906888961792 and perplexity is 32.42951255801994
At time: 90.6698489189148 and batch: 550, loss is 3.5143376350402833 and perplexity is 33.59366928366307
At time: 91.27273201942444 and batch: 600, loss is 3.530615487098694 and perplexity is 34.14497694139901
At time: 91.87512397766113 and batch: 650, loss is 3.374987964630127 and perplexity is 29.223932058286916
At time: 92.47589349746704 and batch: 700, loss is 3.361146721839905 and perplexity is 28.822223008889647
At time: 93.07944416999817 and batch: 750, loss is 3.4500249004364014 and perplexity is 31.501176692028938
At time: 93.68113327026367 and batch: 800, loss is 3.414038577079773 and perplexity is 30.38771992842884
At time: 94.2843120098114 and batch: 850, loss is 3.447717475891113 and perplexity is 31.428573898670706
At time: 94.89085817337036 and batch: 900, loss is 3.3992367696762087 and perplexity is 29.941239262747498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318712626418022 and perplexity of 75.09189471777871
finished 8 epochs...
Completing Train Step...
At time: 96.47696423530579 and batch: 50, loss is 3.6943643856048585 and perplexity is 40.220000050691375
At time: 97.09529852867126 and batch: 100, loss is 3.5751793384552 and perplexity is 35.70102280785645
At time: 97.6988320350647 and batch: 150, loss is 3.584237103462219 and perplexity is 36.0258632261403
At time: 98.3014600276947 and batch: 200, loss is 3.462163014411926 and perplexity is 31.88587157705805
At time: 98.90608406066895 and batch: 250, loss is 3.6056608533859253 and perplexity is 36.805999191111695
At time: 99.52242493629456 and batch: 300, loss is 3.569103312492371 and perplexity is 35.484760140500505
At time: 100.12299036979675 and batch: 350, loss is 3.561043577194214 and perplexity is 35.199911809684195
At time: 100.72696161270142 and batch: 400, loss is 3.488320288658142 and perplexity is 32.730923003761944
At time: 101.32933974266052 and batch: 450, loss is 3.514118752479553 and perplexity is 33.58631701997645
At time: 101.93287396430969 and batch: 500, loss is 3.40736035823822 and perplexity is 30.18546020371116
At time: 102.53628754615784 and batch: 550, loss is 3.4434498357772827 and perplexity is 31.29473384969409
At time: 103.14340901374817 and batch: 600, loss is 3.4681058311462403 and perplexity is 32.07592764308758
At time: 103.74376273155212 and batch: 650, loss is 3.317465043067932 and perplexity is 27.59032155887462
At time: 104.34604477882385 and batch: 700, loss is 3.3080334997177125 and perplexity is 27.331325531069243
At time: 104.94859266281128 and batch: 750, loss is 3.4043137311935423 and perplexity is 30.09363631189984
At time: 105.55159592628479 and batch: 800, loss is 3.3750659465789794 and perplexity is 29.226211086322326
At time: 106.15693283081055 and batch: 850, loss is 3.416621036529541 and perplexity is 30.466296399520555
At time: 106.75967502593994 and batch: 900, loss is 3.375480570793152 and perplexity is 29.238331493660777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328959425834761 and perplexity of 75.86530200835962
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 108.34617710113525 and batch: 50, loss is 3.6644020748138426 and perplexity is 39.032790495790216
At time: 108.96475410461426 and batch: 100, loss is 3.5602317953109743 and perplexity is 35.17134875403958
At time: 109.56533932685852 and batch: 150, loss is 3.5736391592025756 and perplexity is 35.646079155628144
At time: 110.16618323326111 and batch: 200, loss is 3.4475337791442873 and perplexity is 31.422801102126353
At time: 110.76878952980042 and batch: 250, loss is 3.5853923177719116 and perplexity is 36.06750486673675
At time: 111.37164878845215 and batch: 300, loss is 3.5446828603744507 and perplexity is 34.628701467225326
At time: 111.97175765037537 and batch: 350, loss is 3.534424753189087 and perplexity is 34.27529228955908
At time: 112.57539248466492 and batch: 400, loss is 3.4590462398529054 and perplexity is 31.786645217157602
At time: 113.17937421798706 and batch: 450, loss is 3.4788704919815063 and perplexity is 32.42307925757528
At time: 113.78231859207153 and batch: 500, loss is 3.3676720333099364 and perplexity is 29.010911948807415
At time: 114.38299942016602 and batch: 550, loss is 3.3989273834228517 and perplexity is 29.931977287748882
At time: 114.99779033660889 and batch: 600, loss is 3.4200323629379272 and perplexity is 30.570404353139768
At time: 115.602947473526 and batch: 650, loss is 3.271995840072632 and perplexity is 26.36390500757236
At time: 116.20489382743835 and batch: 700, loss is 3.2515957307815553 and perplexity is 25.83152720964109
At time: 116.80973792076111 and batch: 750, loss is 3.349407367706299 and perplexity is 28.48584700762841
At time: 117.4131441116333 and batch: 800, loss is 3.3041028928756715 and perplexity is 27.22410768956546
At time: 118.01306223869324 and batch: 850, loss is 3.3419442415237426 and perplexity is 28.274044873104476
At time: 118.61363124847412 and batch: 900, loss is 3.3058947420120237 and perplexity is 27.27293291407146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331831422570634 and perplexity of 76.08350009020164
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.21512746810913 and batch: 50, loss is 3.6612082719802856 and perplexity is 38.908326321717475
At time: 120.81708598136902 and batch: 100, loss is 3.5561225509643553 and perplexity is 35.027117631427515
At time: 121.41956400871277 and batch: 150, loss is 3.563109498023987 and perplexity is 35.27270720957339
At time: 122.02370595932007 and batch: 200, loss is 3.4392302942276003 and perplexity is 31.16296262252345
At time: 122.62585592269897 and batch: 250, loss is 3.5785727453231813 and perplexity is 35.82237668894208
At time: 123.22951221466064 and batch: 300, loss is 3.5322575473785403 and perplexity is 34.20109111060627
At time: 123.83550238609314 and batch: 350, loss is 3.5223421812057496 and perplexity is 33.86365045716006
At time: 124.43774175643921 and batch: 400, loss is 3.4446033811569214 and perplexity is 31.330854574775806
At time: 125.040043592453 and batch: 450, loss is 3.4640624380111693 and perplexity is 31.94649390953227
At time: 125.64199709892273 and batch: 500, loss is 3.3541281700134276 and perplexity is 28.620640977409813
At time: 126.24323296546936 and batch: 550, loss is 3.3853547239303587 and perplexity is 29.528465311874413
At time: 126.85174608230591 and batch: 600, loss is 3.4110266971588135 and perplexity is 30.29633345655625
At time: 127.4559555053711 and batch: 650, loss is 3.255315160751343 and perplexity is 25.927784666575025
At time: 128.056973695755 and batch: 700, loss is 3.233561406135559 and perplexity is 25.369848605594946
At time: 128.6588785648346 and batch: 750, loss is 3.3367324256896973 and perplexity is 28.12706909637458
At time: 129.26332259178162 and batch: 800, loss is 3.2865746068954467 and perplexity is 26.75107359409771
At time: 129.8800048828125 and batch: 850, loss is 3.32150860786438 and perplexity is 27.7021106727991
At time: 130.4836835861206 and batch: 900, loss is 3.2877033710479737 and perplexity is 26.781286295311528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328199830773759 and perplexity of 75.80769698066969
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 132.07454252243042 and batch: 50, loss is 3.6542077589035036 and perplexity is 38.63689924738701
At time: 132.69234776496887 and batch: 100, loss is 3.5503417921066283 and perplexity is 34.8252184384345
At time: 133.29569721221924 and batch: 150, loss is 3.5599174547195434 and perplexity is 35.16029470892937
At time: 133.90096497535706 and batch: 200, loss is 3.4356881189346313 and perplexity is 31.05277321657412
At time: 134.5039427280426 and batch: 250, loss is 3.5756436491012575 and perplexity is 35.71760302170831
At time: 135.1042869091034 and batch: 300, loss is 3.5284878492355345 and perplexity is 34.072406025312624
At time: 135.71151852607727 and batch: 350, loss is 3.5178223419189454 and perplexity is 33.71093757824424
At time: 136.31680750846863 and batch: 400, loss is 3.441795916557312 and perplexity is 31.243017666879503
At time: 136.9214210510254 and batch: 450, loss is 3.4615626430511472 and perplexity is 31.86673395836364
At time: 137.52560091018677 and batch: 500, loss is 3.3510463666915893 and perplexity is 28.53257356389801
At time: 138.12751364707947 and batch: 550, loss is 3.3807684278488157 and perplexity is 29.39334910542205
At time: 138.73358869552612 and batch: 600, loss is 3.408606653213501 and perplexity is 30.22310364363212
At time: 139.33551025390625 and batch: 650, loss is 3.2487066984176636 and perplexity is 25.757006789308843
At time: 139.93875932693481 and batch: 700, loss is 3.2251085329055784 and perplexity is 25.156304294459435
At time: 140.54231882095337 and batch: 750, loss is 3.329913158416748 and perplexity is 27.935915598754466
At time: 141.15211009979248 and batch: 800, loss is 3.2803080081939697 and perplexity is 26.583959516365155
At time: 141.75493454933167 and batch: 850, loss is 3.314362602233887 and perplexity is 27.504856861779782
At time: 142.35692310333252 and batch: 900, loss is 3.2824779558181763 and perplexity is 26.641707949036963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3198045704462755 and perplexity of 75.17393564768605
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 143.93811917304993 and batch: 50, loss is 3.648663969039917 and perplexity is 38.423297026954764
At time: 144.55165004730225 and batch: 100, loss is 3.544021039009094 and perplexity is 34.60579103488281
At time: 145.15081524848938 and batch: 150, loss is 3.554630823135376 and perplexity is 34.97490565799673
At time: 145.76836919784546 and batch: 200, loss is 3.431913347244263 and perplexity is 30.935777043172596
At time: 146.37255311012268 and batch: 250, loss is 3.572957797050476 and perplexity is 35.62179953896601
At time: 146.97391653060913 and batch: 300, loss is 3.526403069496155 and perplexity is 34.00144655669135
At time: 147.57673907279968 and batch: 350, loss is 3.5160600471496584 and perplexity is 33.65158128628558
At time: 148.17902517318726 and batch: 400, loss is 3.440748076438904 and perplexity is 31.21029712548983
At time: 148.78275227546692 and batch: 450, loss is 3.4610231637954714 and perplexity is 31.84954715282632
At time: 149.38495874404907 and batch: 500, loss is 3.350018792152405 and perplexity is 28.5032692765147
At time: 149.9879856109619 and batch: 550, loss is 3.378755931854248 and perplexity is 29.334254591761567
At time: 150.5923399925232 and batch: 600, loss is 3.4064741277694703 and perplexity is 30.1587207795617
At time: 151.19538760185242 and batch: 650, loss is 3.247016096115112 and perplexity is 25.713498722105967
At time: 151.7997691631317 and batch: 700, loss is 3.2225364780426027 and perplexity is 25.09168403871032
At time: 152.4010329246521 and batch: 750, loss is 3.3270078992843626 and perplexity is 27.85487230772435
At time: 153.0054795742035 and batch: 800, loss is 3.2778789949417115 and perplexity is 26.519465087024518
At time: 153.61035776138306 and batch: 850, loss is 3.312033324241638 and perplexity is 27.44086496040799
At time: 154.21450471878052 and batch: 900, loss is 3.2805081939697267 and perplexity is 26.589281779626557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316467703205266 and perplexity of 74.92350825829057
finished 13 epochs...
Completing Train Step...
At time: 155.8302719593048 and batch: 50, loss is 3.6466984033584593 and perplexity is 38.34784768755744
At time: 156.43602061271667 and batch: 100, loss is 3.5411051416397097 and perplexity is 34.50503107408331
At time: 157.03968262672424 and batch: 150, loss is 3.5514670705795286 and perplexity is 34.864428564074984
At time: 157.6510193347931 and batch: 200, loss is 3.4292130661010742 and perplexity is 30.852354430716478
At time: 158.2561435699463 and batch: 250, loss is 3.5704181575775147 and perplexity is 35.53144778975563
At time: 158.8594126701355 and batch: 300, loss is 3.523955645561218 and perplexity is 33.91833235190013
At time: 159.46429347991943 and batch: 350, loss is 3.513947377204895 and perplexity is 33.580561648850626
At time: 160.06695079803467 and batch: 400, loss is 3.43876446723938 and perplexity is 31.148449454076193
At time: 160.6887242794037 and batch: 450, loss is 3.459057331085205 and perplexity is 31.786997772178864
At time: 161.29404091835022 and batch: 500, loss is 3.3484806394577027 and perplexity is 28.45946059692578
At time: 161.8994164466858 and batch: 550, loss is 3.377480163574219 and perplexity is 29.29685474207867
At time: 162.5026285648346 and batch: 600, loss is 3.4058789348602296 and perplexity is 30.14077586367504
At time: 163.10523891448975 and batch: 650, loss is 3.24637188911438 and perplexity is 25.69693924065549
At time: 163.70851612091064 and batch: 700, loss is 3.2224510860443116 and perplexity is 25.08954150114885
At time: 164.31059288978577 and batch: 750, loss is 3.3273267555236816 and perplexity is 27.86375542369832
At time: 164.91427850723267 and batch: 800, loss is 3.278551907539368 and perplexity is 26.537316374669246
At time: 165.52170825004578 and batch: 850, loss is 3.3134277486801147 and perplexity is 27.479155863802987
At time: 166.12052011489868 and batch: 900, loss is 3.282726035118103 and perplexity is 26.648318025171438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314805122270976 and perplexity of 74.79904535542468
finished 14 epochs...
Completing Train Step...
At time: 167.6886761188507 and batch: 50, loss is 3.645441241264343 and perplexity is 38.299668517912025
At time: 168.30228304862976 and batch: 100, loss is 3.539226813316345 and perplexity is 34.44028012772396
At time: 168.9066481590271 and batch: 150, loss is 3.5493804740905763 and perplexity is 34.79175641493665
At time: 169.51014065742493 and batch: 200, loss is 3.427396354675293 and perplexity is 30.796355488270994
At time: 170.11544013023376 and batch: 250, loss is 3.568660020828247 and perplexity is 35.46903352812241
At time: 170.72040796279907 and batch: 300, loss is 3.5222701454162597 and perplexity is 33.86121115022413
At time: 171.3229537010193 and batch: 350, loss is 3.5124267673492433 and perplexity is 33.52953751957772
At time: 171.92706179618835 and batch: 400, loss is 3.4373682594299315 and perplexity is 31.10499009190659
At time: 172.5300109386444 and batch: 450, loss is 3.45762909412384 and perplexity is 31.74163081216623
At time: 173.1293752193451 and batch: 500, loss is 3.3473455905914307 and perplexity is 28.427176044179138
At time: 173.7333493232727 and batch: 550, loss is 3.3765074014663696 and perplexity is 29.268369728723428
At time: 174.33821487426758 and batch: 600, loss is 3.4053767681121827 and perplexity is 30.12564396796138
At time: 174.9406771659851 and batch: 650, loss is 3.2459273672103883 and perplexity is 25.68551892677537
At time: 175.54546236991882 and batch: 700, loss is 3.2224185132980345 and perplexity is 25.088724279188977
At time: 176.16241550445557 and batch: 750, loss is 3.327568507194519 and perplexity is 27.870492347426275
At time: 176.76501083374023 and batch: 800, loss is 3.279061350822449 and perplexity is 26.550839076479654
At time: 177.3683145046234 and batch: 850, loss is 3.3145177507400514 and perplexity is 27.509124530286794
At time: 177.97200536727905 and batch: 900, loss is 3.284381046295166 and perplexity is 26.692457805192547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313913423721105 and perplexity of 74.73237688365677
finished 15 epochs...
Completing Train Step...
At time: 179.560613155365 and batch: 50, loss is 3.644492173194885 and perplexity is 38.263336768829426
At time: 180.18117475509644 and batch: 100, loss is 3.537848587036133 and perplexity is 34.39284632332036
At time: 180.78376126289368 and batch: 150, loss is 3.547827663421631 and perplexity is 34.73777332798796
At time: 181.38582968711853 and batch: 200, loss is 3.42600839138031 and perplexity is 30.753640927212736
At time: 181.99079060554504 and batch: 250, loss is 3.5672854948043824 and perplexity is 35.420313909359194
At time: 182.59223222732544 and batch: 300, loss is 3.520957307815552 and perplexity is 33.81678604687436
At time: 183.19381952285767 and batch: 350, loss is 3.51120334148407 and perplexity is 33.488541698916094
At time: 183.79431319236755 and batch: 400, loss is 3.436260685920715 and perplexity is 31.070558100377067
At time: 184.3973033428192 and batch: 450, loss is 3.456482853889465 and perplexity is 31.70526812199521
At time: 185.00042748451233 and batch: 500, loss is 3.3464182138442995 and perplexity is 28.4008255624297
At time: 185.60443115234375 and batch: 550, loss is 3.3756861591339113 and perplexity is 29.244343171663413
At time: 186.20550441741943 and batch: 600, loss is 3.404905982017517 and perplexity is 30.111464571681605
At time: 186.8091323375702 and batch: 650, loss is 3.2455697870254516 and perplexity is 25.676335936092936
At time: 187.41189312934875 and batch: 700, loss is 3.2223773193359375 and perplexity is 25.08769079651873
At time: 188.01814603805542 and batch: 750, loss is 3.327729082107544 and perplexity is 27.874968008640764
At time: 188.62754106521606 and batch: 800, loss is 3.2794429016113282 and perplexity is 26.560971502969434
At time: 189.23155522346497 and batch: 850, loss is 3.3153842639923097 and perplexity is 27.532971881781226
At time: 189.8364655971527 and batch: 900, loss is 3.2856585454940794 and perplexity is 26.726579189035448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313406748314426 and perplexity of 74.69452141723394
finished 16 epochs...
Completing Train Step...
At time: 191.44128847122192 and batch: 50, loss is 3.64370267868042 and perplexity is 38.233139996007026
At time: 192.0431683063507 and batch: 100, loss is 3.536745114326477 and perplexity is 34.35491568755476
At time: 192.64382004737854 and batch: 150, loss is 3.5465762186050416 and perplexity is 34.69432811193203
At time: 193.24873852729797 and batch: 200, loss is 3.4248610162734985 and perplexity is 30.71837520054634
At time: 193.85344219207764 and batch: 250, loss is 3.566131148338318 and perplexity is 35.37945018515648
At time: 194.4562861919403 and batch: 300, loss is 3.5198564386367797 and perplexity is 33.77957867337326
At time: 195.0584921836853 and batch: 350, loss is 3.510154547691345 and perplexity is 33.453437535986744
At time: 195.66114497184753 and batch: 400, loss is 3.4353193473815917 and perplexity is 31.041323948380814
At time: 196.2633776664734 and batch: 450, loss is 3.4555091285705566 and perplexity is 31.674410925336947
At time: 196.86655378341675 and batch: 500, loss is 3.3456177425384523 and perplexity is 28.37810061305298
At time: 197.46708464622498 and batch: 550, loss is 3.374959959983826 and perplexity is 29.223113663865576
At time: 198.06774377822876 and batch: 600, loss is 3.4044559001922607 and perplexity is 30.097914998183164
At time: 198.67049646377563 and batch: 650, loss is 3.2452588415145875 and perplexity is 25.66835323585489
At time: 199.2733063697815 and batch: 700, loss is 3.222319412231445 and perplexity is 25.086238083047924
At time: 199.87636971473694 and batch: 750, loss is 3.3278286933898924 and perplexity is 27.877744808247513
At time: 200.48339533805847 and batch: 800, loss is 3.2797340154647827 and perplexity is 26.568704895329365
At time: 201.08520650863647 and batch: 850, loss is 3.316089696884155 and perplexity is 27.552401398061082
At time: 201.68869495391846 and batch: 900, loss is 3.286677474975586 and perplexity is 26.753825567222673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313101990582192 and perplexity of 74.671761152636
finished 17 epochs...
Completing Train Step...
At time: 203.28344583511353 and batch: 50, loss is 3.643006796836853 and perplexity is 38.20654350314278
At time: 203.90187907218933 and batch: 100, loss is 3.5358074951171874 and perplexity is 34.322718955168725
At time: 204.50486183166504 and batch: 150, loss is 3.545510549545288 and perplexity is 34.65737513323438
At time: 205.1094834804535 and batch: 200, loss is 3.423863182067871 and perplexity is 30.687738642674628
At time: 205.71261525154114 and batch: 250, loss is 3.5651178312301637 and perplexity is 35.34361774088726
At time: 206.33199858665466 and batch: 300, loss is 3.518889865875244 and perplexity is 33.74694402715227
At time: 206.9348165988922 and batch: 350, loss is 3.5092194747924803 and perplexity is 33.42217075381151
At time: 207.5403504371643 and batch: 400, loss is 3.434484214782715 and perplexity is 31.015411148657186
At time: 208.14604568481445 and batch: 450, loss is 3.4546511936187745 and perplexity is 31.64724799480685
At time: 208.74786138534546 and batch: 500, loss is 3.3449028587341307 and perplexity is 28.357820818239414
At time: 209.35108757019043 and batch: 550, loss is 3.3743014240264895 and perplexity is 29.203875527929977
At time: 209.95664525032043 and batch: 600, loss is 3.404024667739868 and perplexity is 30.084938598609988
At time: 210.5600025653839 and batch: 650, loss is 3.2449757623672486 and perplexity is 25.661088088661643
At time: 211.15997409820557 and batch: 700, loss is 3.222246136665344 and perplexity is 25.084399942097274
At time: 211.76255750656128 and batch: 750, loss is 3.327885332107544 and perplexity is 27.87932381268044
At time: 212.36597871780396 and batch: 800, loss is 3.2799611949920653 and perplexity is 26.574741446812492
At time: 212.96991562843323 and batch: 850, loss is 3.3166758108139036 and perplexity is 27.568554977775115
At time: 213.57471656799316 and batch: 900, loss is 3.287512722015381 and perplexity is 26.77618095566723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312910942182149 and perplexity of 74.65749659479324
finished 18 epochs...
Completing Train Step...
At time: 215.15886545181274 and batch: 50, loss is 3.6423709917068483 and perplexity is 38.18225930760985
At time: 215.77771973609924 and batch: 100, loss is 3.5349774408340453 and perplexity is 34.29424105602707
At time: 216.38184523582458 and batch: 150, loss is 3.5445682907104494 and perplexity is 34.624734295796515
At time: 216.98353743553162 and batch: 200, loss is 3.422965908050537 and perplexity is 30.66021568180522
At time: 217.5871422290802 and batch: 250, loss is 3.5642019939422607 and perplexity is 35.31126355571681
At time: 218.18892288208008 and batch: 300, loss is 3.518015203475952 and perplexity is 33.71743974914299
At time: 218.7915735244751 and batch: 350, loss is 3.5083643913269045 and perplexity is 33.39360422334079
At time: 219.39480304718018 and batch: 400, loss is 3.4337222719192506 and perplexity is 30.991788178294765
At time: 220.00112533569336 and batch: 450, loss is 3.453876256942749 and perplexity is 31.622732881697367
At time: 220.6067612171173 and batch: 500, loss is 3.344249715805054 and perplexity is 28.339305155433298
At time: 221.2114613056183 and batch: 550, loss is 3.373694143295288 and perplexity is 29.186145961002516
At time: 221.831618309021 and batch: 600, loss is 3.403611478805542 and perplexity is 30.072510402664502
At time: 222.43679642677307 and batch: 650, loss is 3.2447107315063475 and perplexity is 25.65428800954906
At time: 223.21085953712463 and batch: 700, loss is 3.2221603775024414 and perplexity is 25.082248817196977
At time: 223.81816864013672 and batch: 750, loss is 3.3279114627838133 and perplexity is 27.880052327783854
At time: 224.42449378967285 and batch: 800, loss is 3.280142216682434 and perplexity is 26.579552486868465
At time: 225.03210425376892 and batch: 850, loss is 3.3171709775924683 and perplexity is 27.58220939065997
At time: 225.63675498962402 and batch: 900, loss is 3.2882124614715575 and perplexity is 26.794923862778685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312787617722603 and perplexity of 74.64829006708133
finished 19 epochs...
Completing Train Step...
At time: 227.2500479221344 and batch: 50, loss is 3.6417769384384155 and perplexity is 38.159583747584136
At time: 227.85629796981812 and batch: 100, loss is 3.5342217302322387 and perplexity is 34.26833432470882
At time: 228.47794389724731 and batch: 150, loss is 3.5437126398086547 and perplexity is 34.59512028210747
At time: 229.08705234527588 and batch: 200, loss is 3.4221406984329223 and perplexity is 30.634925013433026
At time: 229.6939663887024 and batch: 250, loss is 3.5633576011657713 and perplexity is 35.281459564749845
At time: 230.29940390586853 and batch: 300, loss is 3.5172074127197264 and perplexity is 33.69021411077998
At time: 230.90665197372437 and batch: 350, loss is 3.50756875038147 and perplexity is 33.36704547152133
At time: 231.51176714897156 and batch: 400, loss is 3.433013768196106 and perplexity is 30.969838157737385
At time: 232.1192011833191 and batch: 450, loss is 3.4531629514694213 and perplexity is 31.60018425623632
At time: 232.7208502292633 and batch: 500, loss is 3.343643350601196 and perplexity is 28.322126395712964
At time: 233.32818937301636 and batch: 550, loss is 3.373127188682556 and perplexity is 29.169603430797075
At time: 233.93579030036926 and batch: 600, loss is 3.403215084075928 and perplexity is 30.060592180350934
At time: 234.5485508441925 and batch: 650, loss is 3.244457731246948 and perplexity is 25.647798289013043
At time: 235.15399622917175 and batch: 700, loss is 3.222064189910889 and perplexity is 25.079836332119953
At time: 235.76158332824707 and batch: 750, loss is 3.3279153013229372 and perplexity is 27.88015934666089
At time: 236.36946201324463 and batch: 800, loss is 3.2802884578704834 and perplexity is 26.583439796437425
At time: 236.97575974464417 and batch: 850, loss is 3.3175946855545044 and perplexity is 27.593898668634534
At time: 237.58519220352173 and batch: 900, loss is 3.288808698654175 and perplexity is 26.810904756419145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3127069342626285 and perplexity of 74.6422674277245
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
743.2552988529205


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.6422674277245}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.4276470305051885, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.022917934919097882, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9085521697998047 and batch: 50, loss is 7.080324907302856 and perplexity is 1188.3545608669144
At time: 1.5451042652130127 and batch: 100, loss is 6.131374311447144 and perplexity is 460.06800312949565
At time: 2.180480480194092 and batch: 150, loss is 5.840459518432617 and perplexity is 343.9373499100549
At time: 2.8148345947265625 and batch: 200, loss is 5.535218248367309 and perplexity is 253.46309950864762
At time: 3.4513485431671143 and batch: 250, loss is 5.482218799591064 and perplexity is 240.3794699786507
At time: 4.085469961166382 and batch: 300, loss is 5.335796518325806 and perplexity is 207.6380704866028
At time: 4.723259210586548 and batch: 350, loss is 5.260286407470703 and perplexity is 192.53662732963417
At time: 5.3577635288238525 and batch: 400, loss is 5.084989814758301 and perplexity is 161.57829351524617
At time: 5.993582248687744 and batch: 450, loss is 5.058506736755371 and perplexity is 157.35536781064988
At time: 6.630399227142334 and batch: 500, loss is 4.976820087432861 and perplexity is 145.01252056731803
At time: 7.265480279922485 and batch: 550, loss is 5.019031391143799 and perplexity is 151.26471646862046
At time: 7.901630640029907 and batch: 600, loss is 4.916187038421631 and perplexity is 136.48122213996115
At time: 8.538539171218872 and batch: 650, loss is 4.796303215026856 and perplexity is 121.06204890497324
At time: 9.1725013256073 and batch: 700, loss is 4.859457511901855 and perplexity is 128.95422699541112
At time: 9.808005094528198 and batch: 750, loss is 4.852493810653686 and perplexity is 128.0593477361228
At time: 10.444326400756836 and batch: 800, loss is 4.800779705047607 and perplexity is 121.60519674973185
At time: 11.082174062728882 and batch: 850, loss is 4.836177463531494 and perplexity is 125.98684081628909
At time: 11.719169616699219 and batch: 900, loss is 4.750913896560669 and perplexity is 115.68996489038926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.793621742562072 and perplexity of 120.73785920143756
finished 1 epochs...
Completing Train Step...
At time: 13.346408367156982 and batch: 50, loss is 4.7728346157073975 and perplexity is 118.25397188735992
At time: 13.951400518417358 and batch: 100, loss is 4.656908569335937 and perplexity is 105.31001979232461
At time: 14.557456016540527 and batch: 150, loss is 4.645018653869629 and perplexity is 104.06530698797924
At time: 15.164151430130005 and batch: 200, loss is 4.530278978347778 and perplexity is 92.78444232220963
At time: 15.768007040023804 and batch: 250, loss is 4.648418331146241 and perplexity is 104.4196975128057
At time: 16.373151779174805 and batch: 300, loss is 4.5870687294006345 and perplexity is 98.2061390706361
At time: 16.97674560546875 and batch: 350, loss is 4.572775135040283 and perplexity is 96.8124048201149
At time: 17.58021569252014 and batch: 400, loss is 4.461502609252929 and perplexity is 86.61756371392609
At time: 18.183309078216553 and batch: 450, loss is 4.489735898971557 and perplexity is 89.0979119209538
At time: 18.78835940361023 and batch: 500, loss is 4.3791612529754635 and perplexity is 79.7710975671627
At time: 19.394126176834106 and batch: 550, loss is 4.4553237152099605 and perplexity is 86.08401303954234
At time: 19.999528884887695 and batch: 600, loss is 4.421799621582031 and perplexity is 83.24596186700326
At time: 20.604689121246338 and batch: 650, loss is 4.273141527175904 and perplexity is 71.74667608146538
At time: 21.20711374282837 and batch: 700, loss is 4.31096116065979 and perplexity is 74.51207260600118
At time: 21.811193704605103 and batch: 750, loss is 4.368127632141113 and perplexity is 78.89577141174266
At time: 22.415762901306152 and batch: 800, loss is 4.3269126605987545 and perplexity is 75.71018234650835
At time: 23.017295360565186 and batch: 850, loss is 4.389554252624512 and perplexity is 80.60448173541957
At time: 23.618921279907227 and batch: 900, loss is 4.324932675361634 and perplexity is 75.5604256101509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.509239719338613 and perplexity of 90.85271868117827
finished 2 epochs...
Completing Train Step...
At time: 25.21065378189087 and batch: 50, loss is 4.392944622039795 and perplexity is 80.87822448734326
At time: 25.830832958221436 and batch: 100, loss is 4.273221373558044 and perplexity is 71.75240502269568
At time: 26.438019037246704 and batch: 150, loss is 4.273620986938477 and perplexity is 71.78108397368558
At time: 27.044845819473267 and batch: 200, loss is 4.160136661529541 and perplexity is 64.08027931055426
At time: 27.651819467544556 and batch: 250, loss is 4.298789529800415 and perplexity is 73.61063626770049
At time: 28.27050280570984 and batch: 300, loss is 4.257740688323975 and perplexity is 70.65018222025141
At time: 28.875619649887085 and batch: 350, loss is 4.250397987365723 and perplexity is 70.13331896794362
At time: 29.478206396102905 and batch: 400, loss is 4.163696842193604 and perplexity is 64.30882326939766
At time: 30.08114242553711 and batch: 450, loss is 4.200964622497558 and perplexity is 66.75068921180619
At time: 30.686452388763428 and batch: 500, loss is 4.0777024030685425 and perplexity is 59.0097333933907
At time: 31.289623975753784 and batch: 550, loss is 4.155213899612427 and perplexity is 63.765602526848376
At time: 31.893654584884644 and batch: 600, loss is 4.152283630371094 and perplexity is 63.57902563687296
At time: 32.496628284454346 and batch: 650, loss is 3.9963997077941893 and perplexity is 54.401934168300144
At time: 33.098068952560425 and batch: 700, loss is 4.016577920913696 and perplexity is 55.51081801313457
At time: 33.698678493499756 and batch: 750, loss is 4.099775257110596 and perplexity is 60.32672807054057
At time: 34.28622055053711 and batch: 800, loss is 4.070770921707154 and perplexity is 58.60212283072702
At time: 34.874776124954224 and batch: 850, loss is 4.1353676128387455 and perplexity is 62.512567269345084
At time: 35.46877980232239 and batch: 900, loss is 4.077950224876404 and perplexity is 59.0243591044119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.400499735793022 and perplexity of 81.49158275170863
finished 3 epochs...
Completing Train Step...
At time: 37.04781937599182 and batch: 50, loss is 4.158782119750977 and perplexity is 63.99353865522934
At time: 37.66744923591614 and batch: 100, loss is 4.042886419296265 and perplexity is 56.990604407763826
At time: 38.26938724517822 and batch: 150, loss is 4.044332647323609 and perplexity is 57.07308544597193
At time: 38.87671756744385 and batch: 200, loss is 3.9331924152374267 and perplexity is 51.06975356628661
At time: 39.48326587677002 and batch: 250, loss is 4.079846925735474 and perplexity is 59.136416893489944
At time: 40.08728241920471 and batch: 300, loss is 4.046550235748291 and perplexity is 57.19979049752089
At time: 40.692543268203735 and batch: 350, loss is 4.0369376993179324 and perplexity is 56.65258963514595
At time: 41.29754972457886 and batch: 400, loss is 3.9674506330490114 and perplexity is 52.84962586205093
At time: 41.90438938140869 and batch: 450, loss is 4.003815531730652 and perplexity is 54.80686894062205
At time: 42.51386737823486 and batch: 500, loss is 3.878348455429077 and perplexity is 48.344306326433056
At time: 43.12066292762756 and batch: 550, loss is 3.9528324794769287 and perplexity is 52.08268122952647
At time: 43.73828125 and batch: 600, loss is 3.9659682083129884 and perplexity is 52.771338311421204
At time: 44.33927655220032 and batch: 650, loss is 3.811148109436035 and perplexity is 45.202306280258505
At time: 44.940540075302124 and batch: 700, loss is 3.825474376678467 and perplexity is 45.85454753570325
At time: 45.54362487792969 and batch: 750, loss is 3.9127008962631225 and perplexity is 50.033906032744824
At time: 46.14971971511841 and batch: 800, loss is 3.8903267431259154 and perplexity is 48.92687043085963
At time: 46.75216841697693 and batch: 850, loss is 3.9557372331619263 and perplexity is 52.23418852891586
At time: 47.35431885719299 and batch: 900, loss is 3.9003557109832765 and perplexity is 49.42002522510183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357091616277826 and perplexity of 78.02986330319837
finished 4 epochs...
Completing Train Step...
At time: 48.9566969871521 and batch: 50, loss is 3.9873249197006224 and perplexity is 53.910481431228746
At time: 49.56373977661133 and batch: 100, loss is 3.8723777294158936 and perplexity is 48.0565157334176
At time: 50.16943693161011 and batch: 150, loss is 3.8758590269088744 and perplexity is 48.224106308227185
At time: 50.77446937561035 and batch: 200, loss is 3.766527361869812 and perplexity is 43.22968283087894
At time: 51.377930879592896 and batch: 250, loss is 3.917018337249756 and perplexity is 50.25039146463099
At time: 51.97914505004883 and batch: 300, loss is 3.8858203077316285 and perplexity is 48.70688070725181
At time: 52.585052490234375 and batch: 350, loss is 3.8777844142913818 and perplexity is 48.3170458376329
At time: 53.19025421142578 and batch: 400, loss is 3.817177300453186 and perplexity is 45.475662850659134
At time: 53.794670820236206 and batch: 450, loss is 3.8500983190536497 and perplexity is 46.997683772224185
At time: 54.39876198768616 and batch: 500, loss is 3.7300144243240356 and perplexity is 41.679709361326886
At time: 55.001818895339966 and batch: 550, loss is 3.800746726989746 and perplexity is 44.73457654005524
At time: 55.60522770881653 and batch: 600, loss is 3.8199340677261353 and perplexity is 45.60120163081639
At time: 56.212032318115234 and batch: 650, loss is 3.669377717971802 and perplexity is 39.2274877029933
At time: 56.81588554382324 and batch: 700, loss is 3.675716814994812 and perplexity is 39.47694438321113
At time: 57.42526078224182 and batch: 750, loss is 3.7685300970077513 and perplexity is 43.316347189597344
At time: 58.03269577026367 and batch: 800, loss is 3.7479493284225462 and perplexity is 42.43397457065325
At time: 58.653278827667236 and batch: 850, loss is 3.8120055866241453 and perplexity is 45.2410828493796
At time: 59.256741523742676 and batch: 900, loss is 3.760176486968994 and perplexity is 42.956006485197705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.352386892658391 and perplexity of 77.66361658264768
finished 5 epochs...
Completing Train Step...
At time: 60.84658765792847 and batch: 50, loss is 3.852181353569031 and perplexity is 47.09568360274405
At time: 61.466954708099365 and batch: 100, loss is 3.736475415229797 and perplexity is 41.94987340837162
At time: 62.07104301452637 and batch: 150, loss is 3.7428724193572998 and perplexity is 42.21908708415548
At time: 62.67509722709656 and batch: 200, loss is 3.6365640497207643 and perplexity is 37.96117966249374
At time: 63.28268027305603 and batch: 250, loss is 3.7823827600479127 and perplexity is 43.92056933191439
At time: 63.8921685218811 and batch: 300, loss is 3.756732268333435 and perplexity is 42.8083111007547
At time: 64.5020227432251 and batch: 350, loss is 3.7487101125717164 and perplexity is 42.46626994924126
At time: 65.1096076965332 and batch: 400, loss is 3.693157920837402 and perplexity is 40.17150529717113
At time: 65.71775078773499 and batch: 450, loss is 3.7259166717529295 and perplexity is 41.50926568164332
At time: 66.32306027412415 and batch: 500, loss is 3.609126720428467 and perplexity is 36.933785207430944
At time: 66.92648267745972 and batch: 550, loss is 3.6772556686401368 and perplexity is 39.537740389061696
At time: 67.53406691551208 and batch: 600, loss is 3.701207871437073 and perplexity is 40.49618902095994
At time: 68.13645219802856 and batch: 650, loss is 3.5527134990692137 and perplexity is 34.90791167477828
At time: 68.74085831642151 and batch: 700, loss is 3.5541558694839477 and perplexity is 34.95829814305823
At time: 69.34352517127991 and batch: 750, loss is 3.6500186061859132 and perplexity is 38.4753819224814
At time: 69.94981479644775 and batch: 800, loss is 3.630088658332825 and perplexity is 37.71616032085056
At time: 70.55917072296143 and batch: 850, loss is 3.6925009155273436 and perplexity is 40.145121073114574
At time: 71.16754984855652 and batch: 900, loss is 3.6438325881958007 and perplexity is 38.238107167329815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367922848218108 and perplexity of 78.87961648035603
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.76951575279236 and batch: 50, loss is 3.767597641944885 and perplexity is 43.27597546768895
At time: 73.39090991020203 and batch: 100, loss is 3.657879476547241 and perplexity is 38.77902379298269
At time: 73.99300742149353 and batch: 150, loss is 3.673663010597229 and perplexity is 39.395949663360064
At time: 74.60947132110596 and batch: 200, loss is 3.548469109535217 and perplexity is 34.760062885694666
At time: 75.21470928192139 and batch: 250, loss is 3.6888067722320557 and perplexity is 39.99709283037534
At time: 75.8186342716217 and batch: 300, loss is 3.6499477100372313 and perplexity is 38.47265426277546
At time: 76.42316031455994 and batch: 350, loss is 3.633237156867981 and perplexity is 37.83509673368147
At time: 77.03015398979187 and batch: 400, loss is 3.572534899711609 and perplexity is 35.60673835962612
At time: 77.63643765449524 and batch: 450, loss is 3.587380075454712 and perplexity is 36.13926962848367
At time: 78.24284505844116 and batch: 500, loss is 3.466426191329956 and perplexity is 32.02209685869398
At time: 78.84723591804504 and batch: 550, loss is 3.5136471652984618 and perplexity is 33.570481877528316
At time: 79.45494604110718 and batch: 600, loss is 3.5334983253479004 and perplexity is 34.243553408678714
At time: 80.05622386932373 and batch: 650, loss is 3.362284331321716 and perplexity is 28.85503010036248
At time: 80.6593132019043 and batch: 700, loss is 3.3463829278945925 and perplexity is 28.399823430007963
At time: 81.2627260684967 and batch: 750, loss is 3.431917428970337 and perplexity is 30.935903314798075
At time: 81.86628007888794 and batch: 800, loss is 3.391257448196411 and perplexity is 29.703279131925054
At time: 82.46987962722778 and batch: 850, loss is 3.43189332485199 and perplexity is 30.93515764111034
At time: 83.07888436317444 and batch: 900, loss is 3.374639654159546 and perplexity is 29.21375482928007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323951198630137 and perplexity of 75.4863011922386
finished 7 epochs...
Completing Train Step...
At time: 84.68205785751343 and batch: 50, loss is 3.6749462127685546 and perplexity is 39.4465350802267
At time: 85.28135752677917 and batch: 100, loss is 3.5545336246490478 and perplexity is 34.97150631531546
At time: 85.88360595703125 and batch: 150, loss is 3.56625687122345 and perplexity is 35.38389847132796
At time: 86.48973560333252 and batch: 200, loss is 3.447118196487427 and perplexity is 31.409745044081887
At time: 87.0982882976532 and batch: 250, loss is 3.587123680114746 and perplexity is 36.1300048759316
At time: 87.70360493659973 and batch: 300, loss is 3.554696307182312 and perplexity is 34.97719603135106
At time: 88.30814743041992 and batch: 350, loss is 3.540458164215088 and perplexity is 34.48271431793966
At time: 88.91022181510925 and batch: 400, loss is 3.487058401107788 and perplexity is 32.689646308263264
At time: 89.53187608718872 and batch: 450, loss is 3.5082010078430175 and perplexity is 33.388148705626115
At time: 90.13394045829773 and batch: 500, loss is 3.3896384954452516 and perplexity is 29.655229831724142
At time: 90.74113464355469 and batch: 550, loss is 3.440658860206604 and perplexity is 31.207512784577393
At time: 91.34471797943115 and batch: 600, loss is 3.467986807823181 and perplexity is 32.072110086782914
At time: 91.95315408706665 and batch: 650, loss is 3.3025862312316896 and perplexity is 27.1828492251151
At time: 92.55992293357849 and batch: 700, loss is 3.290725154876709 and perplexity is 26.86233594874424
At time: 93.16784262657166 and batch: 750, loss is 3.3841705989837645 and perplexity is 29.493520612991976
At time: 93.77512192726135 and batch: 800, loss is 3.3493502950668335 and perplexity is 28.48422129154467
At time: 94.38175106048584 and batch: 850, loss is 3.3972896051406862 and perplexity is 29.882995467034565
At time: 94.9850845336914 and batch: 900, loss is 3.3480789375305178 and perplexity is 28.448030672622362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336222191379495 and perplexity of 76.4182996250651
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.58005547523499 and batch: 50, loss is 3.6448867559432983 and perplexity is 38.27843780052249
At time: 97.19575071334839 and batch: 100, loss is 3.536728620529175 and perplexity is 34.3543490492121
At time: 97.7994966506958 and batch: 150, loss is 3.554103708267212 and perplexity is 34.95647472324839
At time: 98.4010980129242 and batch: 200, loss is 3.4272200632095338 and perplexity is 30.790926832148855
At time: 99.00691556930542 and batch: 250, loss is 3.5713327360153198 and perplexity is 35.56395895050527
At time: 99.61534643173218 and batch: 300, loss is 3.5317169857025146 and perplexity is 34.1826083074712
At time: 100.22306513786316 and batch: 350, loss is 3.514169330596924 and perplexity is 33.588015795620805
At time: 100.83117461204529 and batch: 400, loss is 3.4611003589630127 and perplexity is 31.852005878854563
At time: 101.43752980232239 and batch: 450, loss is 3.4725377893447877 and perplexity is 32.218402301235805
At time: 102.03918528556824 and batch: 500, loss is 3.352925124168396 and perplexity is 28.586229737499735
At time: 102.64493370056152 and batch: 550, loss is 3.3954627990722654 and perplexity is 29.828454862294485
At time: 103.24867129325867 and batch: 600, loss is 3.424455361366272 and perplexity is 30.70591666800749
At time: 103.85434985160828 and batch: 650, loss is 3.249740595817566 and perplexity is 25.783650662799943
At time: 104.45826935768127 and batch: 700, loss is 3.230299367904663 and perplexity is 25.28722602180981
At time: 105.07919764518738 and batch: 750, loss is 3.31994282245636 and perplexity is 27.65876905282933
At time: 105.68095374107361 and batch: 800, loss is 3.2823053073883055 and perplexity is 26.63710869702873
At time: 106.28709483146667 and batch: 850, loss is 3.3228514766693116 and perplexity is 27.739335961798023
At time: 106.89390468597412 and batch: 900, loss is 3.272636113166809 and perplexity is 26.380790511696286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3262655179794525 and perplexity of 75.66120291084628
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 108.49108576774597 and batch: 50, loss is 3.629960494041443 and perplexity is 37.7113267656406
At time: 109.11122012138367 and batch: 100, loss is 3.520254602432251 and perplexity is 33.79303115659545
At time: 109.71714496612549 and batch: 150, loss is 3.539148335456848 and perplexity is 34.43757743431124
At time: 110.32084727287292 and batch: 200, loss is 3.413285479545593 and perplexity is 30.36484362660584
At time: 110.92641091346741 and batch: 250, loss is 3.55726243019104 and perplexity is 35.06706707963367
At time: 111.52920150756836 and batch: 300, loss is 3.51619930267334 and perplexity is 33.65626778116289
At time: 112.13697934150696 and batch: 350, loss is 3.5005224561691284 and perplexity is 33.13275785125312
At time: 112.7406554222107 and batch: 400, loss is 3.447833242416382 and perplexity is 31.432212486074352
At time: 113.34374332427979 and batch: 450, loss is 3.4571844720840454 and perplexity is 31.727520920544958
At time: 113.9517228603363 and batch: 500, loss is 3.337669405937195 and perplexity is 28.153435955221823
At time: 114.55919790267944 and batch: 550, loss is 3.376770005226135 and perplexity is 29.27605672192888
At time: 115.16825413703918 and batch: 600, loss is 3.409902853965759 and perplexity is 30.26230425375221
At time: 115.77326393127441 and batch: 650, loss is 3.2327162647247314 and perplexity is 25.348416553784865
At time: 116.37523460388184 and batch: 700, loss is 3.20906090259552 and perplexity is 24.755827171996117
At time: 116.97985100746155 and batch: 750, loss is 3.296117753982544 and perplexity is 27.007585040635302
At time: 117.58418703079224 and batch: 800, loss is 3.2589053869247437 and perplexity is 26.021038578853503
At time: 118.18750333786011 and batch: 850, loss is 3.298655800819397 and perplexity is 27.076218617191696
At time: 118.78872442245483 and batch: 900, loss is 3.2503135204315186 and perplexity is 25.798426983356904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32374510046554 and perplexity of 75.47074520719566
finished 10 epochs...
Completing Train Step...
At time: 120.39616966247559 and batch: 50, loss is 3.617721381187439 and perplexity is 37.25258659428048
At time: 121.00525712966919 and batch: 100, loss is 3.5023294258117676 and perplexity is 33.192681862997276
At time: 121.6112060546875 and batch: 150, loss is 3.5200290489196777 and perplexity is 33.78540987925282
At time: 122.21687245368958 and batch: 200, loss is 3.397170910835266 and perplexity is 29.87944873613627
At time: 122.82363367080688 and batch: 250, loss is 3.541357078552246 and perplexity is 34.513725260226316
At time: 123.41041946411133 and batch: 300, loss is 3.5005206441879273 and perplexity is 33.13269781537315
At time: 124.01201105117798 and batch: 350, loss is 3.485432515144348 and perplexity is 32.636539855397906
At time: 124.61703205108643 and batch: 400, loss is 3.434063420295715 and perplexity is 31.002362780167097
At time: 125.21751689910889 and batch: 450, loss is 3.4450122785568236 and perplexity is 31.34366829931893
At time: 125.8219645023346 and batch: 500, loss is 3.3274614715576174 and perplexity is 27.867509371172336
At time: 126.42268013954163 and batch: 550, loss is 3.367161259651184 and perplexity is 28.99609772284785
At time: 127.02473878860474 and batch: 600, loss is 3.402499237060547 and perplexity is 30.039081095399613
At time: 127.62573766708374 and batch: 650, loss is 3.2271768856048584 and perplexity is 25.208390251847064
At time: 128.2279360294342 and batch: 700, loss is 3.205510764122009 and perplexity is 24.668096377885742
At time: 128.8344829082489 and batch: 750, loss is 3.2946145009994505 and perplexity is 26.96701630803063
At time: 129.4432933330536 and batch: 800, loss is 3.259678373336792 and perplexity is 26.041160263995028
At time: 130.04931497573853 and batch: 850, loss is 3.3024396896362305 and perplexity is 27.178866098874494
At time: 130.65521049499512 and batch: 900, loss is 3.2559604692459105 and perplexity is 25.944521485890725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3233621675674225 and perplexity of 75.4418505087288
finished 11 epochs...
Completing Train Step...
At time: 132.25024890899658 and batch: 50, loss is 3.6102849531173704 and perplexity is 36.97658790774147
At time: 132.8685531616211 and batch: 100, loss is 3.4937562561035156 and perplexity is 32.90933170829494
At time: 133.47377514839172 and batch: 150, loss is 3.510714263916016 and perplexity is 33.47216720890858
At time: 134.07636952400208 and batch: 200, loss is 3.3879999113082886 and perplexity is 29.60667703233085
At time: 134.67868494987488 and batch: 250, loss is 3.5318502473831175 and perplexity is 34.18716384283405
At time: 135.3019506931305 and batch: 300, loss is 3.4913109159469604 and perplexity is 32.82895551156604
At time: 135.91047835350037 and batch: 350, loss is 3.4763313484191896 and perplexity is 32.340856836049774
At time: 136.51698780059814 and batch: 400, loss is 3.4255447959899903 and perplexity is 30.7393869853295
At time: 137.12454557418823 and batch: 450, loss is 3.437308974266052 and perplexity is 31.10314608213327
At time: 137.73100018501282 and batch: 500, loss is 3.3206309843063355 and perplexity is 27.67780931314775
At time: 138.339861869812 and batch: 550, loss is 3.3608510065078736 and perplexity is 28.813701095734984
At time: 138.9438145160675 and batch: 600, loss is 3.397361388206482 and perplexity is 29.88514063705687
At time: 139.54828691482544 and batch: 650, loss is 3.223161635398865 and perplexity is 25.107375193777596
At time: 140.15190291404724 and batch: 700, loss is 3.2025736236572264 and perplexity is 24.595749013022004
At time: 140.75553965568542 and batch: 750, loss is 3.2929571866989136 and perplexity is 26.922360500835758
At time: 141.3544521331787 and batch: 800, loss is 3.2591318798065188 and perplexity is 26.02693282634536
At time: 141.9561710357666 and batch: 850, loss is 3.3032879638671875 and perplexity is 27.201931011915416
At time: 142.5551519393921 and batch: 900, loss is 3.257615966796875 and perplexity is 25.987508150012452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323823693680437 and perplexity of 75.47667692878565
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 144.13488745689392 and batch: 50, loss is 3.607548723220825 and perplexity is 36.87554975727905
At time: 144.75960874557495 and batch: 100, loss is 3.493276023864746 and perplexity is 32.89353138046932
At time: 145.36690616607666 and batch: 150, loss is 3.511186180114746 and perplexity is 33.487966994615256
At time: 145.97359561920166 and batch: 200, loss is 3.3896312046051027 and perplexity is 29.65501362097204
At time: 146.58111381530762 and batch: 250, loss is 3.533697557449341 and perplexity is 34.2503765034523
At time: 147.18943691253662 and batch: 300, loss is 3.4925515031814576 and perplexity is 32.86970796795746
At time: 147.7960352897644 and batch: 350, loss is 3.4762387704849242 and perplexity is 32.33786292491871
At time: 148.39838123321533 and batch: 400, loss is 3.426128458976746 and perplexity is 30.757333664645138
At time: 149.0019609928131 and batch: 450, loss is 3.434973564147949 and perplexity is 31.030592234539682
At time: 149.6056842803955 and batch: 500, loss is 3.3163320302963255 and perplexity is 27.55907907458475
At time: 150.20952033996582 and batch: 550, loss is 3.3555082273483277 and perplexity is 28.660166370301216
At time: 150.83925127983093 and batch: 600, loss is 3.392163805961609 and perplexity is 29.73021313369512
At time: 151.44426226615906 and batch: 650, loss is 3.2164159393310547 and perplexity is 24.93857843754993
At time: 152.05192923545837 and batch: 700, loss is 3.195686225891113 and perplexity is 24.426930334252553
At time: 152.65954446792603 and batch: 750, loss is 3.2843614864349364 and perplexity is 26.691935709554762
At time: 153.26599049568176 and batch: 800, loss is 3.2491635608673097 and perplexity is 25.76877688697926
At time: 153.87291193008423 and batch: 850, loss is 3.292298140525818 and perplexity is 26.90462326764681
At time: 154.47565865516663 and batch: 900, loss is 3.248123846054077 and perplexity is 25.741998631217868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320936647180009 and perplexity of 75.25908650077167
finished 13 epochs...
Completing Train Step...
At time: 156.1056501865387 and batch: 50, loss is 3.6031115865707397 and perplexity is 36.712290373912296
At time: 156.7081880569458 and batch: 100, loss is 3.4882465600967407 and perplexity is 32.728509888854376
At time: 157.31511926651 and batch: 150, loss is 3.5059661149978636 and perplexity is 33.313613091550145
At time: 157.9187090396881 and batch: 200, loss is 3.3846429920196535 and perplexity is 29.507456438067603
At time: 158.52219223976135 and batch: 250, loss is 3.529099407196045 and perplexity is 34.09324964934361
At time: 159.13248419761658 and batch: 300, loss is 3.488025412559509 and perplexity is 32.72127285975027
At time: 159.73882412910461 and batch: 350, loss is 3.472078576087952 and perplexity is 32.203610580322106
At time: 160.34753012657166 and batch: 400, loss is 3.4221054792404173 and perplexity is 30.63384609511103
At time: 160.95472383499146 and batch: 450, loss is 3.4320133829116823 and perplexity is 30.938871879070653
At time: 161.55940747261047 and batch: 500, loss is 3.314164047241211 and perplexity is 27.499396177268064
At time: 162.1624710559845 and batch: 550, loss is 3.3533744192123414 and perplexity is 28.59907627457249
At time: 162.76467442512512 and batch: 600, loss is 3.391172833442688 and perplexity is 29.70076590260617
At time: 163.37032747268677 and batch: 650, loss is 3.2154686880111694 and perplexity is 24.914966521183917
At time: 163.9731674194336 and batch: 700, loss is 3.194947533607483 and perplexity is 24.408893012137398
At time: 164.57558703422546 and batch: 750, loss is 3.28391583442688 and perplexity is 26.680043044998637
At time: 165.17898964881897 and batch: 800, loss is 3.2496302843093874 and perplexity is 25.780806586279045
At time: 165.79854083061218 and batch: 850, loss is 3.29403666973114 and perplexity is 26.951438423922518
At time: 166.40394020080566 and batch: 900, loss is 3.2506268978118897 and perplexity is 25.806512893727177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320475121066995 and perplexity of 75.2243604812088
finished 14 epochs...
Completing Train Step...
At time: 167.99898076057434 and batch: 50, loss is 3.6007503604888917 and perplexity is 36.62570661849599
At time: 168.62163615226746 and batch: 100, loss is 3.4855611610412596 and perplexity is 32.64073868241484
At time: 169.22410249710083 and batch: 150, loss is 3.5030547475814817 and perplexity is 33.21676597105577
At time: 169.8314723968506 and batch: 200, loss is 3.3818805837631225 and perplexity is 29.426057277393014
At time: 170.43529868125916 and batch: 250, loss is 3.5264297580718993 and perplexity is 34.00235401898258
At time: 171.04016184806824 and batch: 300, loss is 3.485352997779846 and perplexity is 32.63394478694002
At time: 171.64547729492188 and batch: 350, loss is 3.469503664970398 and perplexity is 32.12079581150423
At time: 172.24793434143066 and batch: 400, loss is 3.419666705131531 and perplexity is 30.559228089611878
At time: 172.85811114311218 and batch: 450, loss is 3.429846348762512 and perplexity is 30.87189887976916
At time: 173.46387791633606 and batch: 500, loss is 3.312402758598328 and perplexity is 27.45100443152082
At time: 174.06900691986084 and batch: 550, loss is 3.3517759704589842 and perplexity is 28.553398633164395
At time: 174.6762683391571 and batch: 600, loss is 3.3901067638397215 and perplexity is 29.669119690422185
At time: 175.28361630439758 and batch: 650, loss is 3.214700541496277 and perplexity is 24.895835525126664
At time: 175.88935327529907 and batch: 700, loss is 3.194426598548889 and perplexity is 24.396180875412227
At time: 176.49347496032715 and batch: 750, loss is 3.283620247840881 and perplexity is 26.672157947582587
At time: 177.0997018814087 and batch: 800, loss is 3.24981360912323 and perplexity is 25.785533281094178
At time: 177.705219745636 and batch: 850, loss is 3.2948399686813357 and perplexity is 26.973097184179725
At time: 178.31837224960327 and batch: 900, loss is 3.25176869392395 and perplexity is 25.8359954981736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320445021537886 and perplexity of 75.22209629745639
finished 15 epochs...
Completing Train Step...
At time: 179.91414761543274 and batch: 50, loss is 3.5987716484069825 and perplexity is 36.553306543375534
At time: 180.53583145141602 and batch: 100, loss is 3.483372144699097 and perplexity is 32.569365718811
At time: 181.1451449394226 and batch: 150, loss is 3.500717492103577 and perplexity is 33.13922055985121
At time: 181.76718163490295 and batch: 200, loss is 3.3796508836746217 and perplexity is 29.36051908728379
At time: 182.3662030696869 and batch: 250, loss is 3.524189257621765 and perplexity is 33.926257009022876
At time: 182.96967124938965 and batch: 300, loss is 3.4831546783447265 and perplexity is 32.562283747657105
At time: 183.57281684875488 and batch: 350, loss is 3.4673501110076903 and perplexity is 32.0516963757866
At time: 184.17815113067627 and batch: 400, loss is 3.4176638412475584 and perplexity is 30.498083368066343
At time: 184.78389501571655 and batch: 450, loss is 3.428007278442383 and perplexity is 30.8151754619778
At time: 185.37690019607544 and batch: 500, loss is 3.3108333492279054 and perplexity is 27.407956356800188
At time: 185.97418308258057 and batch: 550, loss is 3.3503598833084105 and perplexity is 28.512993147851144
At time: 186.5788118839264 and batch: 600, loss is 3.389037275314331 and perplexity is 29.63740586916168
At time: 187.1835958957672 and batch: 650, loss is 3.2139098501205443 and perplexity is 24.876158382983753
At time: 187.78746676445007 and batch: 700, loss is 3.193874969482422 and perplexity is 24.38272694406152
At time: 188.39514565467834 and batch: 750, loss is 3.2832922315597535 and perplexity is 26.663410480257255
At time: 189.002525806427 and batch: 800, loss is 3.2498120260238648 and perplexity is 25.785492460065118
At time: 189.60912656784058 and batch: 850, loss is 3.295230436325073 and perplexity is 26.98363136237556
At time: 190.21621417999268 and batch: 900, loss is 3.2523727941513063 and perplexity is 25.851607744133897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3205499518407535 and perplexity of 75.22998978892923
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 191.8247787952423 and batch: 50, loss is 3.597817039489746 and perplexity is 36.518429080811735
At time: 192.42596340179443 and batch: 100, loss is 3.482932996749878 and perplexity is 32.555066088704656
At time: 193.03085112571716 and batch: 150, loss is 3.5005532455444337 and perplexity is 33.133778003874355
At time: 193.63325905799866 and batch: 200, loss is 3.3799503469467163 and perplexity is 29.36931280103149
At time: 194.237318277359 and batch: 250, loss is 3.524788179397583 and perplexity is 33.946582269130445
At time: 194.83928561210632 and batch: 300, loss is 3.483940615653992 and perplexity is 32.58788572078436
At time: 195.44684267044067 and batch: 350, loss is 3.467239089012146 and perplexity is 32.0481381300198
At time: 196.05340337753296 and batch: 400, loss is 3.418035011291504 and perplexity is 30.509405444088063
At time: 196.67115712165833 and batch: 450, loss is 3.4276070165634156 and perplexity is 30.802843790060404
At time: 197.2783010005951 and batch: 500, loss is 3.3092061853408814 and perplexity is 27.36339538383413
At time: 197.88353967666626 and batch: 550, loss is 3.3485646867752075 and perplexity is 28.461852638767528
At time: 198.4865984916687 and batch: 600, loss is 3.387035565376282 and perplexity is 29.578139715908538
At time: 199.08879375457764 and batch: 650, loss is 3.2120104885101317 and perplexity is 24.828954405638775
At time: 199.69265604019165 and batch: 700, loss is 3.192053880691528 and perplexity is 24.338364239804235
At time: 200.29451251029968 and batch: 750, loss is 3.2805016469955444 and perplexity is 26.589107700855063
At time: 200.89889097213745 and batch: 800, loss is 3.2461826944351198 and perplexity is 25.69207797635506
At time: 201.50095105171204 and batch: 850, loss is 3.2909196853637694 and perplexity is 26.867562000336633
At time: 202.10345935821533 and batch: 900, loss is 3.2486108493804933 and perplexity is 25.754538123319218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31956544640946 and perplexity of 75.15596190179367
finished 17 epochs...
Completing Train Step...
At time: 203.69576048851013 and batch: 50, loss is 3.5968911981582643 and perplexity is 36.48463445645194
At time: 204.3182234764099 and batch: 100, loss is 3.481982545852661 and perplexity is 32.52413879669611
At time: 204.92670011520386 and batch: 150, loss is 3.499591212272644 and perplexity is 33.101917534887406
At time: 205.5297315120697 and batch: 200, loss is 3.3788809633255004 and perplexity is 29.337922526070102
At time: 206.13124346733093 and batch: 250, loss is 3.5236588954925536 and perplexity is 33.908268577732414
At time: 206.73537373542786 and batch: 300, loss is 3.4827655029296873 and perplexity is 32.54961377294624
At time: 207.33915972709656 and batch: 350, loss is 3.466320376396179 and perplexity is 32.01870862190166
At time: 207.9419059753418 and batch: 400, loss is 3.4171545934677123 and perplexity is 30.482556240734713
At time: 208.5465271472931 and batch: 450, loss is 3.426928467750549 and perplexity is 30.78194964662355
At time: 209.14914393424988 and batch: 500, loss is 3.3087884044647216 and perplexity is 27.35196586821873
At time: 209.7515685558319 and batch: 550, loss is 3.348124079704285 and perplexity is 28.449314907552676
At time: 210.3560972213745 and batch: 600, loss is 3.3868285751342775 and perplexity is 29.572017963204097
At time: 210.96108055114746 and batch: 650, loss is 3.2116488456726073 and perplexity is 24.819976815553176
At time: 211.56585788726807 and batch: 700, loss is 3.1918029832839965 and perplexity is 24.332258573292883
At time: 212.18748235702515 and batch: 750, loss is 3.280435771942139 and perplexity is 26.587356199656018
At time: 212.7919623851776 and batch: 800, loss is 3.246403622627258 and perplexity is 25.697754707746565
At time: 213.39204502105713 and batch: 850, loss is 3.2914448022842406 and perplexity is 26.881674316738597
At time: 214.00121808052063 and batch: 900, loss is 3.249377017021179 and perplexity is 25.77427797808346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319099321757277 and perplexity of 75.12093801857455
finished 18 epochs...
Completing Train Step...
At time: 215.59539365768433 and batch: 50, loss is 3.596196246147156 and perplexity is 36.45928819459981
At time: 216.21670031547546 and batch: 100, loss is 3.481259436607361 and perplexity is 32.50062879241194
At time: 216.81960606575012 and batch: 150, loss is 3.4988325452804565 and perplexity is 33.07681372659503
At time: 217.42536616325378 and batch: 200, loss is 3.3780710601806643 and perplexity is 29.31417126976016
At time: 218.03313732147217 and batch: 250, loss is 3.5228315353393556 and perplexity is 33.880225829773046
At time: 218.63769817352295 and batch: 300, loss is 3.4819204568862916 and perplexity is 32.522119469225785
At time: 219.24512243270874 and batch: 350, loss is 3.4656010627746583 and perplexity is 31.995685410094058
At time: 219.85084915161133 and batch: 400, loss is 3.416453995704651 and perplexity is 30.46120770925998
At time: 220.45160675048828 and batch: 450, loss is 3.426370177268982 and perplexity is 30.764769173425336
At time: 221.05508661270142 and batch: 500, loss is 3.3084094619750974 and perplexity is 27.341603009764214
At time: 221.65701174736023 and batch: 550, loss is 3.3477502965927126 and perplexity is 28.438683021237992
At time: 222.2594382762909 and batch: 600, loss is 3.3866296768188477 and perplexity is 29.56613672355108
At time: 222.863299369812 and batch: 650, loss is 3.211412329673767 and perplexity is 24.81410718810322
At time: 223.4675009250641 and batch: 700, loss is 3.191634864807129 and perplexity is 24.328168214884847
At time: 224.06993103027344 and batch: 750, loss is 3.2803758192062378 and perplexity is 26.585762262692388
At time: 224.6755392551422 and batch: 800, loss is 3.246536860466003 and perplexity is 25.70117884915196
At time: 225.28198051452637 and batch: 850, loss is 3.291799726486206 and perplexity is 26.89121696690153
At time: 225.88797283172607 and batch: 900, loss is 3.249897012710571 and perplexity is 25.78768397675794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3188719030928935 and perplexity of 75.1038560576351
finished 19 epochs...
Completing Train Step...
At time: 227.49186158180237 and batch: 50, loss is 3.5956026554107665 and perplexity is 36.43765272081582
At time: 228.0928270816803 and batch: 100, loss is 3.48063627243042 and perplexity is 32.480381874052775
At time: 228.69931507110596 and batch: 150, loss is 3.4981682109832763 and perplexity is 33.0548469622404
At time: 229.30263352394104 and batch: 200, loss is 3.3773861837387087 and perplexity is 29.29410155786297
At time: 229.9072244167328 and batch: 250, loss is 3.522145600318909 and perplexity is 33.85699416499288
At time: 230.511976480484 and batch: 300, loss is 3.4812303400039672 and perplexity is 32.4996831482635
At time: 231.11629366874695 and batch: 350, loss is 3.464976315498352 and perplexity is 31.975702435584957
At time: 231.72247290611267 and batch: 400, loss is 3.4158495235443116 and perplexity is 30.44280032116673
At time: 232.32947897911072 and batch: 450, loss is 3.4258658838272096 and perplexity is 30.749258613359103
At time: 232.93752145767212 and batch: 500, loss is 3.308038911819458 and perplexity is 27.331473451384134
At time: 233.54293084144592 and batch: 550, loss is 3.3473981332778933 and perplexity is 28.428669723617663
At time: 234.14943385124207 and batch: 600, loss is 3.386419777870178 and perplexity is 29.559931473797064
At time: 234.7556552886963 and batch: 650, loss is 3.2112180471420286 and perplexity is 24.809286708818505
At time: 235.35702514648438 and batch: 700, loss is 3.191494221687317 and perplexity is 24.32474686600802
At time: 235.96045541763306 and batch: 750, loss is 3.280309143066406 and perplexity is 26.58398968578518
At time: 236.56406211853027 and batch: 800, loss is 3.246613612174988 and perplexity is 25.703151534254065
At time: 237.166738986969 and batch: 850, loss is 3.292048149108887 and perplexity is 26.897898183395828
At time: 237.7708888053894 and batch: 900, loss is 3.250264449119568 and perplexity is 25.797161051759282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318764046446918 and perplexity of 75.09575604444889
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
989.4262435436249


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.6422674277245}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.4276470305051885, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.022917934919097882, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.09575604444889}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.8025758421055519, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.6311786327412237, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8936903476715088 and batch: 50, loss is 7.037646417617798 and perplexity is 1138.7044156608317
At time: 1.5467689037322998 and batch: 100, loss is 6.1561412525177 and perplexity is 471.6047555697519
At time: 2.1823716163635254 and batch: 150, loss is 5.980777683258057 and perplexity is 395.74801511963983
At time: 2.8228912353515625 and batch: 200, loss is 5.814517822265625 and perplexity is 335.12976742480663
At time: 3.455162763595581 and batch: 250, loss is 5.84830189704895 and perplexity is 346.6452411037611
At time: 4.090271234512329 and batch: 300, loss is 5.748504037857056 and perplexity is 313.72099434106553
At time: 4.724921226501465 and batch: 350, loss is 5.7237026977539065 and perplexity is 306.035986431152
At time: 5.359635353088379 and batch: 400, loss is 5.583058576583863 and perplexity is 265.88358871571796
At time: 5.995936155319214 and batch: 450, loss is 5.5804863548278805 and perplexity is 265.20055599667194
At time: 6.646804571151733 and batch: 500, loss is 5.52627215385437 and perplexity is 251.2057071430247
At time: 7.282010555267334 and batch: 550, loss is 5.572966661453247 and perplexity is 263.2138083611527
At time: 7.920283794403076 and batch: 600, loss is 5.493281059265136 and perplexity is 243.05337253018914
At time: 8.559088230133057 and batch: 650, loss is 5.391184587478637 and perplexity is 219.4632049988046
At time: 9.198635339736938 and batch: 700, loss is 5.482104291915894 and perplexity is 240.35194626025333
At time: 9.837809562683105 and batch: 750, loss is 5.450235977172851 and perplexity is 232.81309800271632
At time: 10.479612588882446 and batch: 800, loss is 5.437426986694336 and perplexity is 229.850014812017
At time: 11.116432905197144 and batch: 850, loss is 5.461363391876221 and perplexity is 235.41817288701807
At time: 11.75276780128479 and batch: 900, loss is 5.358104410171509 and perplexity is 212.32208915151992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.230678610605736 and perplexity of 186.91960611763957
finished 1 epochs...
Completing Train Step...
At time: 13.384641885757446 and batch: 50, loss is 5.133323545455933 and perplexity is 169.579788632241
At time: 13.988967180252075 and batch: 100, loss is 4.978555145263672 and perplexity is 145.26434407769622
At time: 14.590700387954712 and batch: 150, loss is 4.945479812622071 and perplexity is 140.53826671091446
At time: 15.196878671646118 and batch: 200, loss is 4.811860055923462 and perplexity is 122.96011763504247
At time: 15.804784774780273 and batch: 250, loss is 4.891334571838379 and perplexity is 133.13112860032064
At time: 16.411595821380615 and batch: 300, loss is 4.81758152961731 and perplexity is 123.66564712304678
At time: 17.019912004470825 and batch: 350, loss is 4.805669584274292 and perplexity is 122.20128769378115
At time: 17.625954389572144 and batch: 400, loss is 4.661775074005127 and perplexity is 105.82376054233164
At time: 18.22831678390503 and batch: 450, loss is 4.676733226776123 and perplexity is 107.41858660903662
At time: 18.83328151702881 and batch: 500, loss is 4.580131330490112 and perplexity is 97.52720166122444
At time: 19.437310218811035 and batch: 550, loss is 4.643301467895508 and perplexity is 103.88676084476701
At time: 20.035847902297974 and batch: 600, loss is 4.586344394683838 and perplexity is 98.13503071094047
At time: 20.63768219947815 and batch: 650, loss is 4.4484711933135985 and perplexity is 85.4961369724821
At time: 21.243199348449707 and batch: 700, loss is 4.497764444351196 and perplexity is 89.81611776634196
At time: 21.844992637634277 and batch: 750, loss is 4.530139017105102 and perplexity is 92.771457005103
At time: 22.466023683547974 and batch: 800, loss is 4.482469568252563 and perplexity is 88.45284350506248
At time: 23.074323654174805 and batch: 850, loss is 4.535764541625976 and perplexity is 93.29481581502502
At time: 23.683523416519165 and batch: 900, loss is 4.466096000671387 and perplexity is 87.01634727109291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.575102923667594 and perplexity of 97.03802613252108
finished 2 epochs...
Completing Train Step...
At time: 25.29897403717041 and batch: 50, loss is 4.505228691101074 and perplexity is 90.48903571999234
At time: 25.918506622314453 and batch: 100, loss is 4.373940501213074 and perplexity is 79.3557177099862
At time: 26.520190000534058 and batch: 150, loss is 4.378219041824341 and perplexity is 79.69597174724808
At time: 27.126006841659546 and batch: 200, loss is 4.259480056762695 and perplexity is 70.77317585200699
At time: 27.730414152145386 and batch: 250, loss is 4.393078470230103 and perplexity is 80.88905061583893
At time: 28.33402419090271 and batch: 300, loss is 4.346125402450562 and perplexity is 77.17884588523438
At time: 28.937817096710205 and batch: 350, loss is 4.353322296142578 and perplexity is 77.73629738783336
At time: 29.54503107070923 and batch: 400, loss is 4.250494332313537 and perplexity is 70.14007628441107
At time: 30.152482986450195 and batch: 450, loss is 4.2897967433929445 and perplexity is 72.95163908995555
At time: 30.760249614715576 and batch: 500, loss is 4.165980935096741 and perplexity is 64.45587847618026
At time: 31.367714881896973 and batch: 550, loss is 4.239568471908569 and perplexity is 69.37790686243484
At time: 31.973522663116455 and batch: 600, loss is 4.23412253856659 and perplexity is 69.00110635275338
At time: 32.57901978492737 and batch: 650, loss is 4.078230667114258 and perplexity is 59.04091434905344
At time: 33.18236565589905 and batch: 700, loss is 4.1001785850524906 and perplexity is 60.35106443305208
At time: 33.7865846157074 and batch: 750, loss is 4.183931679725647 and perplexity is 65.62335669273476
At time: 34.388883113861084 and batch: 800, loss is 4.146933875083923 and perplexity is 63.23980160015653
At time: 34.99196791648865 and batch: 850, loss is 4.214791893959045 and perplexity is 67.68007977182874
At time: 35.59625697135925 and batch: 900, loss is 4.155568714141846 and perplexity is 63.78823150340948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.414219268380779 and perplexity of 82.61731377104242
finished 3 epochs...
Completing Train Step...
At time: 37.19815707206726 and batch: 50, loss is 4.226409177780152 and perplexity is 68.4709233000176
At time: 37.821850061416626 and batch: 100, loss is 4.098090996742249 and perplexity is 60.22520767069117
At time: 38.42845058441162 and batch: 150, loss is 4.10604950428009 and perplexity is 60.706422777007376
At time: 39.03177094459534 and batch: 200, loss is 3.985477576255798 and perplexity is 53.81098218968183
At time: 39.633967876434326 and batch: 250, loss is 4.131246953010559 and perplexity is 62.25550424302504
At time: 40.237531423568726 and batch: 300, loss is 4.09242953300476 and perplexity is 59.88520819686972
At time: 40.84226751327515 and batch: 350, loss is 4.099147205352783 and perplexity is 60.28885165835141
At time: 41.445478677749634 and batch: 400, loss is 4.016656270027161 and perplexity is 55.51516740689694
At time: 42.04797053337097 and batch: 450, loss is 4.052963771820068 and perplexity is 57.56782234841653
At time: 42.65059947967529 and batch: 500, loss is 3.929354734420776 and perplexity is 50.87413974449982
At time: 43.25281357765198 and batch: 550, loss is 4.0032282638549805 and perplexity is 54.77469207627126
At time: 43.85665559768677 and batch: 600, loss is 4.017274756431579 and perplexity is 55.549513403348186
At time: 44.4655659198761 and batch: 650, loss is 3.858582820892334 and perplexity is 47.3981321066728
At time: 45.074225664138794 and batch: 700, loss is 3.867561111450195 and perplexity is 47.82560241341852
At time: 45.680880308151245 and batch: 750, loss is 3.966790418624878 and perplexity is 52.81474529234136
At time: 46.28572368621826 and batch: 800, loss is 3.936125407218933 and perplexity is 51.21976062120308
At time: 46.88787317276001 and batch: 850, loss is 4.002196412086487 and perplexity is 54.71820186315901
At time: 47.49308156967163 and batch: 900, loss is 3.95108681678772 and perplexity is 51.99184174676623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358963639768835 and perplexity of 78.17607385239437
finished 4 epochs...
Completing Train Step...
At time: 49.094923973083496 and batch: 50, loss is 4.035039858818054 and perplexity is 56.5451740172092
At time: 49.69700026512146 and batch: 100, loss is 3.908320655822754 and perplexity is 49.81522478202357
At time: 50.296761989593506 and batch: 150, loss is 3.9187632179260254 and perplexity is 50.33814894257074
At time: 50.897592306137085 and batch: 200, loss is 3.7998232889175414 and perplexity is 44.693285996498346
At time: 51.50322127342224 and batch: 250, loss is 3.948359098434448 and perplexity is 51.85021589134904
At time: 52.106938123703 and batch: 300, loss is 3.9132214260101317 and perplexity is 50.05995694874408
At time: 52.72776174545288 and batch: 350, loss is 3.917071943283081 and perplexity is 50.25308526099167
At time: 53.33601188659668 and batch: 400, loss is 3.84598331451416 and perplexity is 46.80468545696852
At time: 53.937859296798706 and batch: 450, loss is 3.877880964279175 and perplexity is 48.321711073029306
At time: 54.54283595085144 and batch: 500, loss is 3.759909873008728 and perplexity is 42.94455534077679
At time: 55.150113344192505 and batch: 550, loss is 3.830990915298462 and perplexity is 46.10820493021386
At time: 55.75475573539734 and batch: 600, loss is 3.85477915763855 and perplexity is 47.21818801359829
At time: 56.3564248085022 and batch: 650, loss is 3.6962855768203737 and perplexity is 40.297344634554584
At time: 56.95857381820679 and batch: 700, loss is 3.7039643478393556 and perplexity is 40.607969800141255
At time: 57.56198072433472 and batch: 750, loss is 3.805798444747925 and perplexity is 44.961134767065744
At time: 58.1672797203064 and batch: 800, loss is 3.777333617210388 and perplexity is 43.699367014840846
At time: 58.77417182922363 and batch: 850, loss is 3.842643213272095 and perplexity is 46.64861386150229
At time: 59.38132286071777 and batch: 900, loss is 3.794615364074707 and perplexity is 44.46113176765349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348080151701627 and perplexity of 77.32985872445825
finished 5 epochs...
Completing Train Step...
At time: 60.9833128452301 and batch: 50, loss is 3.885814657211304 and perplexity is 48.70660548881
At time: 61.602519273757935 and batch: 100, loss is 3.7618410205841064 and perplexity is 43.02756774350056
At time: 62.20413064956665 and batch: 150, loss is 3.7725920248031617 and perplexity is 43.4926528923503
At time: 62.80585718154907 and batch: 200, loss is 3.656537585258484 and perplexity is 38.72702145731226
At time: 63.40977215766907 and batch: 250, loss is 3.8030972909927367 and perplexity is 44.83985170489401
At time: 64.01258277893066 and batch: 300, loss is 3.774175429344177 and perplexity is 43.56157390696489
At time: 64.61175227165222 and batch: 350, loss is 3.775910577774048 and perplexity is 43.63722531776503
At time: 65.21510171890259 and batch: 400, loss is 3.706921629905701 and perplexity is 40.728236765001355
At time: 65.82040047645569 and batch: 450, loss is 3.7380829429626465 and perplexity is 42.01736322460712
At time: 66.4267098903656 and batch: 500, loss is 3.6274299097061156 and perplexity is 37.61601572004573
At time: 67.03472232818604 and batch: 550, loss is 3.698085460662842 and perplexity is 40.369940486508824
At time: 67.63942909240723 and batch: 600, loss is 3.7236557722091677 and perplexity is 41.41552341265999
At time: 68.25528001785278 and batch: 650, loss is 3.569863796234131 and perplexity is 35.511755987317855
At time: 68.86129879951477 and batch: 700, loss is 3.5756042051315307 and perplexity is 35.716194205440836
At time: 69.46584558486938 and batch: 750, loss is 3.67597421169281 and perplexity is 39.487106926188815
At time: 70.07060980796814 and batch: 800, loss is 3.6484079265594485 and perplexity is 38.41346029004119
At time: 70.67392897605896 and batch: 850, loss is 3.7170339012145996 and perplexity is 41.142181176779104
At time: 71.27817058563232 and batch: 900, loss is 3.6697677516937257 and perplexity is 39.24279073017814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355212067904538 and perplexity of 77.88334014237847
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.86839842796326 and batch: 50, loss is 3.7916120004653933 and perplexity is 44.32779914586953
At time: 73.49029731750488 and batch: 100, loss is 3.6771247291564944 and perplexity is 39.53256367667618
At time: 74.09714603424072 and batch: 150, loss is 3.6880990171432497 and perplexity is 39.96879469964146
At time: 74.7036361694336 and batch: 200, loss is 3.5588350820541383 and perplexity is 35.12225875528046
At time: 75.30778789520264 and batch: 250, loss is 3.6962471199035645 and perplexity is 40.295794952722524
At time: 75.91179060935974 and batch: 300, loss is 3.656355452537537 and perplexity is 38.71996864181379
At time: 76.51332759857178 and batch: 350, loss is 3.6531725883483888 and perplexity is 38.59692416103104
At time: 77.11544895172119 and batch: 400, loss is 3.5728146123886106 and perplexity is 35.616699408782914
At time: 77.72073221206665 and batch: 450, loss is 3.5896702194213868 and perplexity is 36.22212860205798
At time: 78.32316040992737 and batch: 500, loss is 3.4697454309463502 and perplexity is 32.12856246587052
At time: 78.92552161216736 and batch: 550, loss is 3.527873921394348 and perplexity is 34.051494446394535
At time: 79.52946209907532 and batch: 600, loss is 3.5427815389633177 and perplexity is 34.56292372781379
At time: 80.1357524394989 and batch: 650, loss is 3.378715476989746 and perplexity is 29.33306790247063
At time: 80.74563527107239 and batch: 700, loss is 3.360511074066162 and perplexity is 28.803908048548283
At time: 81.35810804367065 and batch: 750, loss is 3.451569323539734 and perplexity is 31.54986542541875
At time: 81.96318173408508 and batch: 800, loss is 3.405059690475464 and perplexity is 30.11609331419677
At time: 82.56803894042969 and batch: 850, loss is 3.4528745889663695 and perplexity is 31.59107326170502
At time: 83.18329524993896 and batch: 900, loss is 3.3937613916397096 and perplexity is 29.77774765653072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310603729666096 and perplexity of 74.48544444099664
finished 7 epochs...
Completing Train Step...
At time: 84.78743886947632 and batch: 50, loss is 3.6969207429885866 and perplexity is 40.32294827495757
At time: 85.39242053031921 and batch: 100, loss is 3.5729709815979005 and perplexity is 35.6222691993675
At time: 85.99643015861511 and batch: 150, loss is 3.582491550445557 and perplexity is 35.96303302458538
At time: 86.5993127822876 and batch: 200, loss is 3.4579742050170896 and perplexity is 31.752587085184853
At time: 87.20487880706787 and batch: 250, loss is 3.5983445262908935 and perplexity is 36.537697151530224
At time: 87.81116366386414 and batch: 300, loss is 3.562823829650879 and perplexity is 35.262632351792355
At time: 88.41858577728271 and batch: 350, loss is 3.5624705076217653 and perplexity is 35.25017548774976
At time: 89.02567529678345 and batch: 400, loss is 3.486365294456482 and perplexity is 32.66699674716562
At time: 89.63105034828186 and batch: 450, loss is 3.507337198257446 and perplexity is 33.35932015571117
At time: 90.23404026031494 and batch: 500, loss is 3.3926936292648318 and perplexity is 29.745969066984866
At time: 90.83816337585449 and batch: 550, loss is 3.4549515628814698 and perplexity is 31.656755283130497
At time: 91.44127035140991 and batch: 600, loss is 3.4767072200775146 and perplexity is 32.35301513237675
At time: 92.04577207565308 and batch: 650, loss is 3.3177549552917482 and perplexity is 27.598321489936424
At time: 92.64949154853821 and batch: 700, loss is 3.305218229293823 and perplexity is 27.254488667679535
At time: 93.2537088394165 and batch: 750, loss is 3.4029854774475097 and perplexity is 30.053690861456747
At time: 93.85538744926453 and batch: 800, loss is 3.3639152002334596 and perplexity is 28.902127266113485
At time: 94.46515989303589 and batch: 850, loss is 3.418531732559204 and perplexity is 30.524563879083885
At time: 95.07646083831787 and batch: 900, loss is 3.366927523612976 and perplexity is 28.989321081846086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320945426209332 and perplexity of 75.25974720539907
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.67053627967834 and batch: 50, loss is 3.6620023536682127 and perplexity is 38.93923498153366
At time: 97.29271173477173 and batch: 100, loss is 3.5545524835586546 and perplexity is 34.972165846010874
At time: 97.89794731140137 and batch: 150, loss is 3.5658715295791628 and perplexity is 35.37026620841772
At time: 98.51542782783508 and batch: 200, loss is 3.437220268249512 and perplexity is 31.100387168310395
At time: 99.12075781822205 and batch: 250, loss is 3.5775050640106203 and perplexity is 35.78415021725584
At time: 99.72488307952881 and batch: 300, loss is 3.535999050140381 and perplexity is 34.32929427414184
At time: 100.32910585403442 and batch: 350, loss is 3.5344225215911864 and perplexity is 34.27521580097411
At time: 100.92976307868958 and batch: 400, loss is 3.4587651920318603 and perplexity is 31.77771290504321
At time: 101.53915286064148 and batch: 450, loss is 3.47243980884552 and perplexity is 32.215245680738875
At time: 102.14699959754944 and batch: 500, loss is 3.352107496261597 and perplexity is 28.562866390879844
At time: 102.75614905357361 and batch: 550, loss is 3.4049400091171265 and perplexity is 30.112489194918357
At time: 103.36515712738037 and batch: 600, loss is 3.429554777145386 and perplexity is 30.86289882243342
At time: 103.97209787368774 and batch: 650, loss is 3.2619648504257204 and perplexity is 26.10077090351828
At time: 104.57724976539612 and batch: 700, loss is 3.2395146799087526 and perplexity is 25.521332725746934
At time: 105.17908096313477 and batch: 750, loss is 3.3340914344787596 and perplexity is 28.052883758633293
At time: 105.7719235420227 and batch: 800, loss is 3.2939224672317504 and perplexity is 26.948360678038842
At time: 106.37604784965515 and batch: 850, loss is 3.344850845336914 and perplexity is 28.356345869999856
At time: 106.97917103767395 and batch: 900, loss is 3.2913768815994264 and perplexity is 26.879848557014178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314043855013913 and perplexity of 74.74212495990159
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 108.57196617126465 and batch: 50, loss is 3.648586030006409 and perplexity is 38.42030246901827
At time: 109.19405579566956 and batch: 100, loss is 3.5363012599945067 and perplexity is 34.33967049297548
At time: 109.80336546897888 and batch: 150, loss is 3.5502582740783692 and perplexity is 34.822310026310944
At time: 110.41262006759644 and batch: 200, loss is 3.422201704978943 and perplexity is 30.63679400140537
At time: 111.02370548248291 and batch: 250, loss is 3.56201452255249 and perplexity is 35.23410559813132
At time: 111.6317458152771 and batch: 300, loss is 3.5216048669815065 and perplexity is 33.83869150841788
At time: 112.23359274864197 and batch: 350, loss is 3.5163003110885622 and perplexity is 33.65966751913194
At time: 112.83727812767029 and batch: 400, loss is 3.442271556854248 and perplexity is 31.257881639751755
At time: 113.44124698638916 and batch: 450, loss is 3.455213384628296 and perplexity is 31.665044795237492
At time: 114.06064295768738 and batch: 500, loss is 3.337112774848938 and perplexity is 28.13776923822328
At time: 114.6638777256012 and batch: 550, loss is 3.3852838468551636 and perplexity is 29.52637249478536
At time: 115.26967763900757 and batch: 600, loss is 3.4111612129211424 and perplexity is 30.30040906505758
At time: 115.8757336139679 and batch: 650, loss is 3.2432047653198244 and perplexity is 25.615682595787437
At time: 116.48141527175903 and batch: 700, loss is 3.218011984825134 and perplexity is 24.97841332398327
At time: 117.08929014205933 and batch: 750, loss is 3.309764609336853 and perplexity is 27.378680027686475
At time: 117.69520688056946 and batch: 800, loss is 3.2718709659576417 and perplexity is 26.36061304381166
At time: 118.30253434181213 and batch: 850, loss is 3.3213866710662843 and perplexity is 27.69873297206036
At time: 118.90959358215332 and batch: 900, loss is 3.2707652711868285 and perplexity is 26.331482359604124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308668162724743 and perplexity of 74.34141231400348
finished 10 epochs...
Completing Train Step...
At time: 120.50936508178711 and batch: 50, loss is 3.6371414279937744 and perplexity is 37.983103951543704
At time: 121.11139345169067 and batch: 100, loss is 3.5190267419815062 and perplexity is 33.751563493589025
At time: 121.71513819694519 and batch: 150, loss is 3.5331321287155153 and perplexity is 34.23101583048912
At time: 122.31869411468506 and batch: 200, loss is 3.4058556365966797 and perplexity is 30.140073644115645
At time: 122.92155385017395 and batch: 250, loss is 3.5468296432495117 and perplexity is 34.703121623898184
At time: 123.534250497818 and batch: 300, loss is 3.5066920137405395 and perplexity is 33.33780418048557
At time: 124.14200091362 and batch: 350, loss is 3.502584958076477 and perplexity is 33.201164747944745
At time: 124.74942016601562 and batch: 400, loss is 3.430270261764526 and perplexity is 30.884988653367746
At time: 125.35655546188354 and batch: 450, loss is 3.443628678321838 and perplexity is 31.300331180032437
At time: 125.96210980415344 and batch: 500, loss is 3.3263309669494627 and perplexity is 27.836022824606847
At time: 126.56579160690308 and batch: 550, loss is 3.375587215423584 and perplexity is 29.241449770988186
At time: 127.1687924861908 and batch: 600, loss is 3.4038763809204102 and perplexity is 30.080477729503837
At time: 127.77210998535156 and batch: 650, loss is 3.2375957584381103 and perplexity is 25.472406250465372
At time: 128.3789722919464 and batch: 700, loss is 3.21421546459198 and perplexity is 24.88376205881682
At time: 128.99353623390198 and batch: 750, loss is 3.3079914903640746 and perplexity is 27.330177383866243
At time: 129.60013628005981 and batch: 800, loss is 3.2726158475875855 and perplexity is 26.38025589511337
At time: 130.2017376422882 and batch: 850, loss is 3.3248093938827514 and perplexity is 27.79370048842133
At time: 130.80683851242065 and batch: 900, loss is 3.2759460163116456 and perplexity is 26.468253039572932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30812093656357 and perplexity of 74.30074187730513
finished 11 epochs...
Completing Train Step...
At time: 132.40220856666565 and batch: 50, loss is 3.6300989961624146 and perplexity is 37.716550226104125
At time: 133.02117609977722 and batch: 100, loss is 3.5106967210769655 and perplexity is 33.47158001721709
At time: 133.6232831478119 and batch: 150, loss is 3.524224157333374 and perplexity is 33.92744104626963
At time: 134.22643065452576 and batch: 200, loss is 3.3969430208206175 and perplexity is 29.872640283944705
At time: 134.82974982261658 and batch: 250, loss is 3.537917790412903 and perplexity is 34.395226506780084
At time: 135.43252658843994 and batch: 300, loss is 3.4978723335266113 and perplexity is 33.04506822491606
At time: 136.03737473487854 and batch: 350, loss is 3.4939626598358156 and perplexity is 32.916125018245175
At time: 136.63875794410706 and batch: 400, loss is 3.422349286079407 and perplexity is 30.641315746832657
At time: 137.24048614501953 and batch: 450, loss is 3.435992383956909 and perplexity is 31.062222926843898
At time: 137.8447630405426 and batch: 500, loss is 3.3195821046829224 and perplexity is 27.648793842466567
At time: 138.45073318481445 and batch: 550, loss is 3.369200963973999 and perplexity is 29.05530154734035
At time: 139.05409049987793 and batch: 600, loss is 3.398817000389099 and perplexity is 29.928673487634743
At time: 139.6583595275879 and batch: 650, loss is 3.233429985046387 and perplexity is 25.366514691537176
At time: 140.26376748085022 and batch: 700, loss is 3.2111349534988403 and perplexity is 24.807225300447115
At time: 140.867502450943 and batch: 750, loss is 3.3061578369140623 and perplexity is 27.28010922766916
At time: 141.4710612297058 and batch: 800, loss is 3.272118020057678 and perplexity is 26.36712634587947
At time: 142.07491517066956 and batch: 850, loss is 3.325588722229004 and perplexity is 27.81536934954134
At time: 142.67941188812256 and batch: 900, loss is 3.2775368690490723 and perplexity is 26.510393643235687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308438653815283 and perplexity of 74.32435225532815
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 144.27702403068542 and batch: 50, loss is 3.6272309112548826 and perplexity is 37.60853093593081
At time: 144.89567232131958 and batch: 100, loss is 3.5096657609939577 and perplexity is 33.43708993630737
At time: 145.5054841041565 and batch: 150, loss is 3.5235600805282594 and perplexity is 33.90491809892507
At time: 146.1131031513214 and batch: 200, loss is 3.396644787788391 and perplexity is 29.86373260420041
At time: 146.7243275642395 and batch: 250, loss is 3.5365565729141237 and perplexity is 34.348438973813316
At time: 147.3317894935608 and batch: 300, loss is 3.4979411649703978 and perplexity is 33.047342842953725
At time: 147.93631863594055 and batch: 350, loss is 3.492398910522461 and perplexity is 32.86469267447641
At time: 148.53920340538025 and batch: 400, loss is 3.420490574836731 and perplexity is 30.584415285908534
At time: 149.14435338974 and batch: 450, loss is 3.433332767486572 and perplexity is 30.979719090056957
At time: 149.7477822303772 and batch: 500, loss is 3.31761935710907 and perplexity is 27.594579461409285
At time: 150.35192489624023 and batch: 550, loss is 3.3645942640304565 and perplexity is 28.92176031968961
At time: 150.95777130126953 and batch: 600, loss is 3.3930828189849853 and perplexity is 29.75754814544474
At time: 151.56011700630188 and batch: 650, loss is 3.227273564338684 and perplexity is 25.2108274849108
At time: 152.16381335258484 and batch: 700, loss is 3.204462585449219 and perplexity is 24.642253351784607
At time: 152.76879405975342 and batch: 750, loss is 3.297821011543274 and perplexity is 27.053625111972096
At time: 153.37701153755188 and batch: 800, loss is 3.2623081970214844 and perplexity is 26.1097340530002
At time: 153.9837565422058 and batch: 850, loss is 3.3149115467071533 and perplexity is 27.51995964585771
At time: 154.58974623680115 and batch: 900, loss is 3.2674562883377076 and perplexity is 26.244495933750407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306914865154109 and perplexity of 74.2111838943425
finished 13 epochs...
Completing Train Step...
At time: 156.1369206905365 and batch: 50, loss is 3.6242008209228516 and perplexity is 37.494746166064566
At time: 156.7545669078827 and batch: 100, loss is 3.5060681009292605 and perplexity is 33.31701078466499
At time: 157.35814118385315 and batch: 150, loss is 3.519852352142334 and perplexity is 33.77944063359469
At time: 157.96047806739807 and batch: 200, loss is 3.3924678230285643 and perplexity is 29.73925299995917
At time: 158.56360507011414 and batch: 250, loss is 3.5326329135894774 and perplexity is 34.2139314543512
At time: 159.17285704612732 and batch: 300, loss is 3.494319076538086 and perplexity is 32.92785896593896
At time: 159.79514741897583 and batch: 350, loss is 3.488952913284302 and perplexity is 32.7516359427579
At time: 160.40019392967224 and batch: 400, loss is 3.4173218822479248 and perplexity is 30.487656056945106
At time: 161.00661754608154 and batch: 450, loss is 3.430300235748291 and perplexity is 30.885914413390505
At time: 161.6084544658661 and batch: 500, loss is 3.314918851852417 and perplexity is 27.520160683894883
At time: 162.21492743492126 and batch: 550, loss is 3.3623460388183593 and perplexity is 28.856810726973976
At time: 162.82086491584778 and batch: 600, loss is 3.3914893770217898 and perplexity is 29.71016897751003
At time: 163.42858052253723 and batch: 650, loss is 3.2261559629440306 and perplexity is 25.182667567665746
At time: 164.03094458580017 and batch: 700, loss is 3.2035898113250734 and perplexity is 24.62075561339806
At time: 164.63557982444763 and batch: 750, loss is 3.29732572555542 and perplexity is 27.0402291482239
At time: 165.2412292957306 and batch: 800, loss is 3.2628036642074587 and perplexity is 26.122673774797036
At time: 165.84459829330444 and batch: 850, loss is 3.3163536977767945 and perplexity is 27.559676216861604
At time: 166.4506916999817 and batch: 900, loss is 3.2697025823593138 and perplexity is 26.303515050394164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30650392297196 and perplexity of 74.1806936537852
finished 14 epochs...
Completing Train Step...
At time: 168.03859639167786 and batch: 50, loss is 3.622093758583069 and perplexity is 37.41582557298182
At time: 168.65667176246643 and batch: 100, loss is 3.5036421489715575 and perplexity is 33.236283277246194
At time: 169.262104511261 and batch: 150, loss is 3.517231101989746 and perplexity is 33.69101221681232
At time: 169.86833691596985 and batch: 200, loss is 3.3897808027267455 and perplexity is 29.65945028715722
At time: 170.47773933410645 and batch: 250, loss is 3.5299577569961547 and perplexity is 34.12252614630539
At time: 171.08215761184692 and batch: 300, loss is 3.491797590255737 and perplexity is 32.84493640922824
At time: 171.68708491325378 and batch: 350, loss is 3.4866319274902344 and perplexity is 32.67570800891536
At time: 172.28947401046753 and batch: 400, loss is 3.4151760387420653 and perplexity is 30.42230446041268
At time: 172.89395594596863 and batch: 450, loss is 3.4282300806045534 and perplexity is 30.82204191460023
At time: 173.4956078529358 and batch: 500, loss is 3.313060188293457 and perplexity is 27.469057470646998
At time: 174.09766244888306 and batch: 550, loss is 3.3607316493988035 and perplexity is 28.810262180904136
At time: 174.6989450454712 and batch: 600, loss is 3.3903251314163207 and perplexity is 29.67559917161737
At time: 175.31707000732422 and batch: 650, loss is 3.22531129360199 and perplexity is 25.161405521384
At time: 175.92343974113464 and batch: 700, loss is 3.2029595708847047 and perplexity is 24.605243506230146
At time: 176.52717900276184 and batch: 750, loss is 3.297015895843506 and perplexity is 27.031852579538757
At time: 177.13043475151062 and batch: 800, loss is 3.263026685714722 and perplexity is 26.12850034257685
At time: 177.7326328754425 and batch: 850, loss is 3.3170485401153567 and perplexity is 27.578832501262138
At time: 178.33511805534363 and batch: 900, loss is 3.2707363986968994 and perplexity is 26.330722115120007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306441215619649 and perplexity of 74.17604212473758
finished 15 epochs...
Completing Train Step...
At time: 179.92408299446106 and batch: 50, loss is 3.6202533864974975 and perplexity is 37.34702985631439
At time: 180.5440969467163 and batch: 100, loss is 3.501584529876709 and perplexity is 33.167965975728656
At time: 181.1461799144745 and batch: 150, loss is 3.5150347709655763 and perplexity is 33.617096802516826
At time: 181.7522280216217 and batch: 200, loss is 3.387566018104553 and perplexity is 29.59383368290384
At time: 182.35658264160156 and batch: 250, loss is 3.5277416133880615 and perplexity is 34.04698945908277
At time: 182.96943068504333 and batch: 300, loss is 3.4896650314331055 and perplexity is 32.77496728346536
At time: 183.5768072605133 and batch: 350, loss is 3.484636788368225 and perplexity is 32.610580416435056
At time: 184.1879985332489 and batch: 400, loss is 3.413339786529541 and perplexity is 30.3664926944588
At time: 184.79943895339966 and batch: 450, loss is 3.4264535903930664 and perplexity is 30.767335465963576
At time: 185.4061062335968 and batch: 500, loss is 3.3114893007278443 and perplexity is 27.425940544615337
At time: 186.00799226760864 and batch: 550, loss is 3.359302797317505 and perplexity is 28.76912597360061
At time: 186.61764693260193 and batch: 600, loss is 3.389248094558716 and perplexity is 29.64365466333243
At time: 187.22105813026428 and batch: 650, loss is 3.224486217498779 and perplexity is 25.14065400893085
At time: 187.8259768486023 and batch: 700, loss is 3.2023375749588014 and perplexity is 24.589943903651903
At time: 188.42926263809204 and batch: 750, loss is 3.2966693115234373 and perplexity is 27.022485386647137
At time: 189.03544545173645 and batch: 800, loss is 3.263053841590881 and perplexity is 26.12920989453059
At time: 189.63853812217712 and batch: 850, loss is 3.3173866510391234 and perplexity is 27.58815878236546
At time: 190.25875854492188 and batch: 900, loss is 3.271280131340027 and perplexity is 26.3450428812323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306515628344392 and perplexity of 74.1815619715136
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 191.8585286140442 and batch: 50, loss is 3.619430503845215 and perplexity is 37.31631027437032
At time: 192.4616527557373 and batch: 100, loss is 3.501385130882263 and perplexity is 33.16135297599994
At time: 193.06123399734497 and batch: 150, loss is 3.5150815439224243 and perplexity is 33.61866921030772
At time: 193.66399669647217 and batch: 200, loss is 3.3876026582717897 and perplexity is 29.594918025784285
At time: 194.26861548423767 and batch: 250, loss is 3.5270935010910036 and perplexity is 34.02493033570327
At time: 194.8714075088501 and batch: 300, loss is 3.489660954475403 and perplexity is 32.77483366158243
At time: 195.47569966316223 and batch: 350, loss is 3.4839533376693725 and perplexity is 32.5883003070049
At time: 196.07960724830627 and batch: 400, loss is 3.4126408243179323 and perplexity is 30.3452750795856
At time: 196.68649625778198 and batch: 450, loss is 3.4256264638900755 and perplexity is 30.741897509027705
At time: 197.2917206287384 and batch: 500, loss is 3.3105982542037964 and perplexity is 27.401513639994715
At time: 197.89624500274658 and batch: 550, loss is 3.3575913667678834 and perplexity is 28.719931720804503
At time: 198.496351480484 and batch: 600, loss is 3.3874124670028687 and perplexity is 29.589289866001735
At time: 199.0999698638916 and batch: 650, loss is 3.2226837062835694 and perplexity is 25.09537851517319
At time: 199.70692038536072 and batch: 700, loss is 3.200533571243286 and perplexity is 24.54562354255324
At time: 200.3095715045929 and batch: 750, loss is 3.2942283725738526 and perplexity is 26.956605586547635
At time: 200.91372966766357 and batch: 800, loss is 3.259750790596008 and perplexity is 26.04304616173311
At time: 201.51908898353577 and batch: 850, loss is 3.3137436628341677 and perplexity is 27.487838289460157
At time: 202.1225984096527 and batch: 900, loss is 3.2672638845443727 and perplexity is 26.239446878922816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305824175272902 and perplexity of 74.13028663192897
finished 17 epochs...
Completing Train Step...
At time: 203.71370005607605 and batch: 50, loss is 3.6185327100753786 and perplexity is 37.28282295809401
At time: 204.33165740966797 and batch: 100, loss is 3.500569396018982 and perplexity is 33.13431313443399
At time: 204.93347549438477 and batch: 150, loss is 3.514288454055786 and perplexity is 33.59201715456182
At time: 205.55621123313904 and batch: 200, loss is 3.386764965057373 and perplexity is 29.57013694469374
At time: 206.15677118301392 and batch: 250, loss is 3.5264112520217896 and perplexity is 34.00172477553768
At time: 206.76158666610718 and batch: 300, loss is 3.4888413763046264 and perplexity is 32.747983127921174
At time: 207.36344170570374 and batch: 350, loss is 3.4832275199890135 and perplexity is 32.56465572433532
At time: 207.96707725524902 and batch: 400, loss is 3.4120533323287963 and perplexity is 30.327452709330707
At time: 208.56787753105164 and batch: 450, loss is 3.425033240318298 and perplexity is 30.72366609896994
At time: 209.16799545288086 and batch: 500, loss is 3.310116801261902 and perplexity is 27.388324275924216
At time: 209.76914358139038 and batch: 550, loss is 3.35717161655426 and perplexity is 28.707879053061344
At time: 210.37024784088135 and batch: 600, loss is 3.3871135902404785 and perplexity is 29.58044763627948
At time: 210.97551131248474 and batch: 650, loss is 3.222468919754028 and perplexity is 25.08998894473868
At time: 211.5788390636444 and batch: 700, loss is 3.200364155769348 and perplexity is 24.541465486337152
At time: 212.18344831466675 and batch: 750, loss is 3.294153242111206 and perplexity is 26.95458040037607
At time: 212.78932666778564 and batch: 800, loss is 3.2599377536773684 and perplexity is 26.047915705089746
At time: 213.39267873764038 and batch: 850, loss is 3.31410831451416 and perplexity is 27.49786360363451
At time: 213.99718022346497 and batch: 900, loss is 3.2677004098892213 and perplexity is 26.250903562904814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305525270226884 and perplexity of 74.10813202641853
finished 18 epochs...
Completing Train Step...
At time: 215.58386754989624 and batch: 50, loss is 3.617878785133362 and perplexity is 37.25845075991556
At time: 216.2032573223114 and batch: 100, loss is 3.4999293899536132 and perplexity is 33.113113757644605
At time: 216.8043441772461 and batch: 150, loss is 3.5136351680755613 and perplexity is 33.5700791273903
At time: 217.40558505058289 and batch: 200, loss is 3.386086893081665 and perplexity is 29.550093059879973
At time: 218.00763297080994 and batch: 250, loss is 3.525790419578552 and perplexity is 33.980621953007244
At time: 218.61326336860657 and batch: 300, loss is 3.4881948709487913 and perplexity is 32.72681822378531
At time: 219.21540904045105 and batch: 350, loss is 3.48261803150177 and perplexity is 32.54481398884919
At time: 219.81821036338806 and batch: 400, loss is 3.4115283918380737 and perplexity is 30.311536779246662
At time: 220.4230546951294 and batch: 450, loss is 3.424516453742981 and perplexity is 30.707792622738452
At time: 221.04816126823425 and batch: 500, loss is 3.3096787452697756 and perplexity is 27.376329283791748
At time: 221.64692449569702 and batch: 550, loss is 3.356799440383911 and perplexity is 28.697196652567293
At time: 222.24966263771057 and batch: 600, loss is 3.386842660903931 and perplexity is 29.572434510771004
At time: 222.85008263587952 and batch: 650, loss is 3.2222772598266602 and perplexity is 25.085180660072332
At time: 223.4522099494934 and batch: 700, loss is 3.200211973190308 and perplexity is 24.537730986996092
At time: 224.05330753326416 and batch: 750, loss is 3.294074559211731 and perplexity is 26.952459619271735
At time: 224.6561360359192 and batch: 800, loss is 3.2600413274765017 and perplexity is 26.050613726398574
At time: 225.25875759124756 and batch: 850, loss is 3.314356355667114 and perplexity is 27.504685051391434
At time: 225.86231517791748 and batch: 900, loss is 3.2680337238311767 and perplexity is 26.25965481342714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305385641855736 and perplexity of 74.09778515103035
finished 19 epochs...
Completing Train Step...
At time: 227.4700186252594 and batch: 50, loss is 3.617327299118042 and perplexity is 37.23790891016103
At time: 228.07453203201294 and batch: 100, loss is 3.499362530708313 and perplexity is 33.09434860207385
At time: 228.67768549919128 and batch: 150, loss is 3.513043832778931 and perplexity is 33.5502338228844
At time: 229.2819447517395 and batch: 200, loss is 3.385482211112976 and perplexity is 29.532230052697038
At time: 229.88592290878296 and batch: 250, loss is 3.52520694732666 and perplexity is 33.96080098605006
At time: 230.48998928070068 and batch: 300, loss is 3.4876208209991457 and perplexity is 32.70803678668915
At time: 231.09421396255493 and batch: 350, loss is 3.4820690155029297 and perplexity is 32.52695126919814
At time: 231.69856786727905 and batch: 400, loss is 3.411038637161255 and perplexity is 30.296695197011278
At time: 232.29877424240112 and batch: 450, loss is 3.4240392684936523 and perplexity is 30.693142812674054
At time: 232.88961100578308 and batch: 500, loss is 3.309267535209656 and perplexity is 27.36507417604647
At time: 233.46402406692505 and batch: 550, loss is 3.356446385383606 and perplexity is 28.68706675210559
At time: 234.08083200454712 and batch: 600, loss is 3.3865826511383057 and perplexity is 29.564746388541636
At time: 234.68200993537903 and batch: 650, loss is 3.2220916223526 and perplexity is 25.080524342705118
At time: 235.2837839126587 and batch: 700, loss is 3.200064191818237 and perplexity is 24.534105035373983
At time: 235.90505647659302 and batch: 750, loss is 3.2939927339553834 and perplexity is 26.95025431758011
At time: 236.50825905799866 and batch: 800, loss is 3.2601001596450807 and perplexity is 26.052146385581302
At time: 237.11100840568542 and batch: 850, loss is 3.314534931182861 and perplexity is 27.50959715328746
At time: 237.71545314788818 and batch: 900, loss is 3.268291139602661 and perplexity is 26.26641533282468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30531875401327 and perplexity of 74.09282907580256
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
1235.3265166282654


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.6422674277245}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.4276470305051885, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.022917934919097882, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.09575604444889}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8025758421055519, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.6311786327412237, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.09282907580256}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'rnn_dropout': 0.9409394853848363, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.42240237976402184, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9307770729064941 and batch: 50, loss is 7.105456485748291 and perplexity is 1218.5982305135312
At time: 1.568758487701416 and batch: 100, loss is 6.215673866271973 and perplexity is 500.5331679910753
At time: 2.2050445079803467 and batch: 150, loss is 5.986271476745605 and perplexity is 397.9281561266745
At time: 2.840862989425659 and batch: 200, loss is 5.822745780944825 and perplexity is 337.89857650618416
At time: 3.476440191268921 and batch: 250, loss is 5.857729082107544 and perplexity is 349.9285819559024
At time: 4.110379219055176 and batch: 300, loss is 5.755087537765503 and perplexity is 315.7931901445273
At time: 4.7472028732299805 and batch: 350, loss is 5.730304841995239 and perplexity is 308.0631646546925
At time: 5.381160736083984 and batch: 400, loss is 5.5927698516845705 and perplexity is 268.4782356641674
At time: 6.017623424530029 and batch: 450, loss is 5.598238277435303 and perplexity is 269.9504105324454
At time: 6.653394937515259 and batch: 500, loss is 5.539093341827392 and perplexity is 254.4471982136901
At time: 7.290469169616699 and batch: 550, loss is 5.591550216674805 and perplexity is 268.15098980940706
At time: 7.926522254943848 and batch: 600, loss is 5.507932567596436 and perplexity is 246.64068665128877
At time: 8.562946081161499 and batch: 650, loss is 5.413173265457154 and perplexity is 224.34235720662255
At time: 9.201366901397705 and batch: 700, loss is 5.510004692077636 and perplexity is 247.15228672262046
At time: 9.836347579956055 and batch: 750, loss is 5.480268363952637 and perplexity is 239.91108222223107
At time: 10.472153186798096 and batch: 800, loss is 5.462335252761841 and perplexity is 235.64707781486715
At time: 11.10862398147583 and batch: 850, loss is 5.499440174102784 and perplexity is 244.5549857203865
At time: 11.76389193534851 and batch: 900, loss is 5.3879052352905275 and perplexity is 218.7446866385817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.288134849234803 and perplexity of 197.97382980061866
finished 1 epochs...
Completing Train Step...
At time: 13.397571325302124 and batch: 50, loss is 5.200094709396362 and perplexity is 181.28941087277624
At time: 14.002251386642456 and batch: 100, loss is 5.051177730560303 and perplexity is 156.2063251588288
At time: 14.60690975189209 and batch: 150, loss is 5.0100709819793705 and perplexity is 149.91537706158763
At time: 15.209244728088379 and batch: 200, loss is 4.869802694320679 and perplexity is 130.29520637592012
At time: 15.81274676322937 and batch: 250, loss is 4.946888885498047 and perplexity is 140.73643495453365
At time: 16.415853261947632 and batch: 300, loss is 4.871501445770264 and perplexity is 130.51673365324407
At time: 17.021817445755005 and batch: 350, loss is 4.852271032333374 and perplexity is 128.03082206730213
At time: 17.626527070999146 and batch: 400, loss is 4.707216243743897 and perplexity is 110.74344765000309
At time: 18.23064422607422 and batch: 450, loss is 4.7179979228973385 and perplexity is 111.94390783026563
At time: 18.833972215652466 and batch: 500, loss is 4.624171199798584 and perplexity is 101.91826818749814
At time: 19.437093019485474 and batch: 550, loss is 4.686389846801758 and perplexity is 108.46091165230531
At time: 20.04075527191162 and batch: 600, loss is 4.625056095123291 and perplexity is 102.00849510131899
At time: 20.642726182937622 and batch: 650, loss is 4.48476505279541 and perplexity is 88.6561188585561
At time: 21.24551773071289 and batch: 700, loss is 4.536949548721314 and perplexity is 93.40543636384679
At time: 21.84549570083618 and batch: 750, loss is 4.567184734344482 and perplexity is 96.27269468840475
At time: 22.447200059890747 and batch: 800, loss is 4.515906553268433 and perplexity is 91.46044221369246
At time: 23.04845380783081 and batch: 850, loss is 4.568342828750611 and perplexity is 96.38425214214963
At time: 23.650187015533447 and batch: 900, loss is 4.498473863601685 and perplexity is 89.87985765576866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.610187687285959 and perplexity of 100.50301100368053
finished 2 epochs...
Completing Train Step...
At time: 25.24931502342224 and batch: 50, loss is 4.539772701263428 and perplexity is 93.6695067390493
At time: 25.871567010879517 and batch: 100, loss is 4.409039525985718 and perplexity is 82.19048375740583
At time: 26.476625442504883 and batch: 150, loss is 4.413696947097779 and perplexity is 82.57417225756164
At time: 27.093454360961914 and batch: 200, loss is 4.29603919506073 and perplexity is 73.40846053048371
At time: 27.69612145423889 and batch: 250, loss is 4.433846950531006 and perplexity is 84.25491876798219
At time: 28.298595666885376 and batch: 300, loss is 4.386391820907593 and perplexity is 80.3499782029821
At time: 28.902206659317017 and batch: 350, loss is 4.38810564994812 and perplexity is 80.48780239885185
At time: 29.50689697265625 and batch: 400, loss is 4.281641960144043 and perplexity is 72.35915336523539
At time: 30.112077951431274 and batch: 450, loss is 4.317394008636475 and perplexity is 74.99294246459186
At time: 30.71735692024231 and batch: 500, loss is 4.204187331199646 and perplexity is 66.96615424283236
At time: 31.324225664138794 and batch: 550, loss is 4.267873558998108 and perplexity is 71.36971066768645
At time: 31.92899751663208 and batch: 600, loss is 4.260544190406799 and perplexity is 70.8485280548277
At time: 32.5331392288208 and batch: 650, loss is 4.110883975028992 and perplexity is 61.00061676531454
At time: 33.1364004611969 and batch: 700, loss is 4.1332644176483155 and perplexity is 62.381229301627464
At time: 33.74508547782898 and batch: 750, loss is 4.214850044250488 and perplexity is 67.68401550262317
At time: 34.35230827331543 and batch: 800, loss is 4.177260513305664 and perplexity is 65.18702938345467
At time: 34.955771684646606 and batch: 850, loss is 4.244185872077942 and perplexity is 69.69899314278143
At time: 35.561800718307495 and batch: 900, loss is 4.184981799125671 and perplexity is 65.69230524846441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428035370291096 and perplexity of 83.76668462705103
finished 3 epochs...
Completing Train Step...
At time: 37.15020275115967 and batch: 50, loss is 4.260884656906128 and perplexity is 70.87265371191305
At time: 37.76313090324402 and batch: 100, loss is 4.130012264251709 and perplexity is 62.178685505134595
At time: 38.36457586288452 and batch: 150, loss is 4.139171614646911 and perplexity is 62.75081805417971
At time: 38.96799993515015 and batch: 200, loss is 4.021565718650818 and perplexity is 55.78838639763811
At time: 39.57238268852234 and batch: 250, loss is 4.172911057472229 and perplexity is 64.9041169817422
At time: 40.177035331726074 and batch: 300, loss is 4.1323155546188355 and perplexity is 62.322066132720394
At time: 40.78402900695801 and batch: 350, loss is 4.132832288742065 and perplexity is 62.354278392791585
At time: 41.39215087890625 and batch: 400, loss is 4.048462357521057 and perplexity is 57.309268096222816
At time: 41.99676847457886 and batch: 450, loss is 4.088008432388306 and perplexity is 59.621034066497714
At time: 42.61628270149231 and batch: 500, loss is 3.9686519479751587 and perplexity is 52.91315305688402
At time: 43.22087264060974 and batch: 550, loss is 4.032616167068482 and perplexity is 56.4082918925107
At time: 43.81890606880188 and batch: 600, loss is 4.040832619667054 and perplexity is 56.87367723915122
At time: 44.41889834403992 and batch: 650, loss is 3.890361247062683 and perplexity is 48.9285586296278
At time: 45.01846671104431 and batch: 700, loss is 3.9032084226608275 and perplexity is 49.561207588710275
At time: 45.62105464935303 and batch: 750, loss is 4.001160740852356 and perplexity is 54.661561131157164
At time: 46.22415328025818 and batch: 800, loss is 3.969154176712036 and perplexity is 52.93973423726558
At time: 46.8270103931427 and batch: 850, loss is 4.037127060890198 and perplexity is 56.66331847437503
At time: 47.427858114242554 and batch: 900, loss is 3.982626352310181 and perplexity is 53.657773548590676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359860772955908 and perplexity of 78.24623967201897
finished 4 epochs...
Completing Train Step...
At time: 49.067121505737305 and batch: 50, loss is 4.0681098985672 and perplexity is 58.44638852401774
At time: 49.66862869262695 and batch: 100, loss is 3.938628749847412 and perplexity is 51.34814185545069
At time: 50.27295970916748 and batch: 150, loss is 3.9524568843841554 and perplexity is 52.0631229032738
At time: 50.876793384552 and batch: 200, loss is 3.8378551816940307 and perplexity is 46.42579268835284
At time: 51.478864431381226 and batch: 250, loss is 3.988493490219116 and perplexity is 53.97351645372838
At time: 52.083667516708374 and batch: 300, loss is 3.9562176513671874 and perplexity is 52.259288812856106
At time: 52.68616580963135 and batch: 350, loss is 3.9525377893447877 and perplexity is 52.067335238579794
At time: 53.28743267059326 and batch: 400, loss is 3.8766790676116942 and perplexity is 48.26366825724427
At time: 53.89155673980713 and batch: 450, loss is 3.919593958854675 and perplexity is 50.37998427792648
At time: 54.495553493499756 and batch: 500, loss is 3.7998065662384035 and perplexity is 44.692538611266166
At time: 55.09624791145325 and batch: 550, loss is 3.8624147033691405 and perplexity is 47.58010460447515
At time: 55.70009517669678 and batch: 600, loss is 3.878412470817566 and perplexity is 48.34740120504264
At time: 56.30621290206909 and batch: 650, loss is 3.7285122299194335 and perplexity is 41.617145338566424
At time: 56.90822505950928 and batch: 700, loss is 3.7389263820648195 and perplexity is 42.052817261279586
At time: 57.52700996398926 and batch: 750, loss is 3.83885046005249 and perplexity is 46.47202227692823
At time: 58.12625479698181 and batch: 800, loss is 3.8110839033126833 and perplexity is 45.19940410857529
At time: 58.73001503944397 and batch: 850, loss is 3.8798692417144776 and perplexity is 48.417883617958694
At time: 59.33665657043457 and batch: 900, loss is 3.828387713432312 and perplexity is 45.98833205942998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345747020146618 and perplexity of 77.14964829998576
finished 5 epochs...
Completing Train Step...
At time: 60.9320023059845 and batch: 50, loss is 3.9191082763671874 and perplexity is 50.35552154288466
At time: 61.55140709877014 and batch: 100, loss is 3.7908427619934084 and perplexity is 44.29371360902203
At time: 62.153974533081055 and batch: 150, loss is 3.806620993614197 and perplexity is 44.99813271171738
At time: 62.75741744041443 and batch: 200, loss is 3.691499390602112 and perplexity is 40.104934860839094
At time: 63.36277675628662 and batch: 250, loss is 3.843421001434326 and perplexity is 46.68491071494991
At time: 63.96659731864929 and batch: 300, loss is 3.8140461301803588 and perplexity is 45.33349350137916
At time: 64.56824040412903 and batch: 350, loss is 3.8104828453063964 and perplexity is 45.17224480783166
At time: 65.17502284049988 and batch: 400, loss is 3.7408665943145754 and perplexity is 42.134487855990585
At time: 65.78075695037842 and batch: 450, loss is 3.7825518465042114 and perplexity is 43.92799633322632
At time: 66.38545489311218 and batch: 500, loss is 3.6668835401535036 and perplexity is 39.12976928750732
At time: 66.98843955993652 and batch: 550, loss is 3.724544744491577 and perplexity is 41.45235703464411
At time: 67.59399890899658 and batch: 600, loss is 3.748167381286621 and perplexity is 42.443228429220966
At time: 68.19647932052612 and batch: 650, loss is 3.5985703468322754 and perplexity is 36.54594904577037
At time: 68.79853844642639 and batch: 700, loss is 3.609512643814087 and perplexity is 36.9480415696168
At time: 69.40243244171143 and batch: 750, loss is 3.7139748668670656 and perplexity is 41.01651813320311
At time: 70.00449728965759 and batch: 800, loss is 3.684019455909729 and perplexity is 39.80607169913731
At time: 70.60625004768372 and batch: 850, loss is 3.7525166940689085 and perplexity is 42.62822932690321
At time: 71.20882177352905 and batch: 900, loss is 3.704763207435608 and perplexity is 40.64042282748069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351696275684931 and perplexity of 77.60999928745358
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.79703497886658 and batch: 50, loss is 3.823862977027893 and perplexity is 45.780717035024225
At time: 73.41821360588074 and batch: 100, loss is 3.704126410484314 and perplexity is 40.61455136843224
At time: 74.01932978630066 and batch: 150, loss is 3.7187691831588747 and perplexity is 41.21363644050722
At time: 74.62432169914246 and batch: 200, loss is 3.5865995407104494 and perplexity is 36.111072678687414
At time: 75.22918605804443 and batch: 250, loss is 3.7255847358703615 and perplexity is 41.49548955342666
At time: 75.83218288421631 and batch: 300, loss is 3.686281704902649 and perplexity is 39.89622488077437
At time: 76.43573999404907 and batch: 350, loss is 3.677701597213745 and perplexity is 39.555375328906635
At time: 77.0378680229187 and batch: 400, loss is 3.600547161102295 and perplexity is 36.61826505346376
At time: 77.63985633850098 and batch: 450, loss is 3.6325257778167725 and perplexity is 37.808191209612424
At time: 78.24417209625244 and batch: 500, loss is 3.509931879043579 and perplexity is 33.44598933355884
At time: 78.8483407497406 and batch: 550, loss is 3.5472528457641603 and perplexity is 34.717811180344945
At time: 79.45159077644348 and batch: 600, loss is 3.5673310375213623 and perplexity is 35.42192708342479
At time: 80.05545115470886 and batch: 650, loss is 3.404910154342651 and perplexity is 30.111590206764163
At time: 80.65824484825134 and batch: 700, loss is 3.3952505111694338 and perplexity is 29.822123314246365
At time: 81.2599949836731 and batch: 750, loss is 3.487955722808838 and perplexity is 32.718992601859256
At time: 81.86819529533386 and batch: 800, loss is 3.443563542366028 and perplexity is 31.298292469441247
At time: 82.47357749938965 and batch: 850, loss is 3.4923796796798707 and perplexity is 32.864060664821864
At time: 83.07732462882996 and batch: 900, loss is 3.4361023139953613 and perplexity is 31.06563778589901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30421385046554 and perplexity of 74.01100885614471
finished 7 epochs...
Completing Train Step...
At time: 84.68315744400024 and batch: 50, loss is 3.7252346754074095 and perplexity is 41.48096616532346
At time: 85.28951215744019 and batch: 100, loss is 3.604917845726013 and perplexity is 36.77866220883323
At time: 85.89424395561218 and batch: 150, loss is 3.618226366043091 and perplexity is 37.271403337029646
At time: 86.49946212768555 and batch: 200, loss is 3.4884297752380373 and perplexity is 32.734506796763476
At time: 87.10661435127258 and batch: 250, loss is 3.62912606716156 and perplexity is 37.67987254585959
At time: 87.70987129211426 and batch: 300, loss is 3.594825553894043 and perplexity is 36.40934796487941
At time: 88.33024954795837 and batch: 350, loss is 3.5902644491195677 and perplexity is 36.24365926305018
At time: 88.93379926681519 and batch: 400, loss is 3.5162484407424928 and perplexity is 33.65792162580958
At time: 89.53564023971558 and batch: 450, loss is 3.5538230657577516 and perplexity is 34.94666582692119
At time: 90.13797521591187 and batch: 500, loss is 3.436317582130432 and perplexity is 31.072325947658197
At time: 90.74205613136292 and batch: 550, loss is 3.4760499095916746 and perplexity is 32.33175614392513
At time: 91.34378600120544 and batch: 600, loss is 3.5022873163223265 and perplexity is 33.191284165539216
At time: 91.94478797912598 and batch: 650, loss is 3.346804223060608 and perplexity is 28.41179065902558
At time: 92.55032348632812 and batch: 700, loss is 3.3411437129974364 and perplexity is 28.251419750865647
At time: 93.15303635597229 and batch: 750, loss is 3.440003094673157 and perplexity is 31.18705468190109
At time: 93.75805807113647 and batch: 800, loss is 3.4015799522399903 and perplexity is 30.011479313018746
At time: 94.36503982543945 and batch: 850, loss is 3.457953634262085 and perplexity is 31.75193391721326
At time: 94.97178721427917 and batch: 900, loss is 3.410738706588745 and perplexity is 30.287609654459782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313290948737158 and perplexity of 74.6858723240199
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.58336448669434 and batch: 50, loss is 3.6910924243927004 and perplexity is 40.08861682818925
At time: 97.20542120933533 and batch: 100, loss is 3.584184331893921 and perplexity is 36.02396213500078
At time: 97.80998992919922 and batch: 150, loss is 3.5996527004241945 and perplexity is 36.58552609930951
At time: 98.41609144210815 and batch: 200, loss is 3.4695769500732423 and perplexity is 32.12314987358651
At time: 99.01834535598755 and batch: 250, loss is 3.6091081285476685 and perplexity is 36.93309854528212
At time: 99.62016940116882 and batch: 300, loss is 3.566202507019043 and perplexity is 35.38197490612577
At time: 100.22411584854126 and batch: 350, loss is 3.560902271270752 and perplexity is 35.19493820504833
At time: 100.82596850395203 and batch: 400, loss is 3.480754041671753 and perplexity is 32.48420728923798
At time: 101.43049931526184 and batch: 450, loss is 3.5167434883117674 and perplexity is 33.67458802308663
At time: 102.03620457649231 and batch: 500, loss is 3.39396050453186 and perplexity is 29.783677380310976
At time: 102.62424921989441 and batch: 550, loss is 3.426346697807312 and perplexity is 30.764046841686756
At time: 103.22676920890808 and batch: 600, loss is 3.455258316993713 and perplexity is 31.66646761256622
At time: 103.84521222114563 and batch: 650, loss is 3.293799638748169 and perplexity is 26.945050855036182
At time: 104.44678235054016 and batch: 700, loss is 3.2807787227630616 and perplexity is 26.596475919009087
At time: 105.0512068271637 and batch: 750, loss is 3.37261616230011 and perplexity is 29.154700802021004
At time: 105.6549482345581 and batch: 800, loss is 3.3292742395401 and perplexity is 27.918072515690255
At time: 106.25798058509827 and batch: 850, loss is 3.3828012847900393 and perplexity is 29.453162354467842
At time: 106.85938787460327 and batch: 900, loss is 3.337969241142273 and perplexity is 28.16187861210474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310349555864726 and perplexity of 74.4665145982733
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 108.45163178443909 and batch: 50, loss is 3.678257417678833 and perplexity is 39.57736712719829
At time: 109.0721161365509 and batch: 100, loss is 3.5660137939453125 and perplexity is 35.37529849486944
At time: 109.67439699172974 and batch: 150, loss is 3.5791881465911866 and perplexity is 35.84442860967345
At time: 110.27661895751953 and batch: 200, loss is 3.450703272819519 and perplexity is 31.522553470220714
At time: 110.8814446926117 and batch: 250, loss is 3.5935035943984985 and perplexity is 36.36124808166419
At time: 111.48553991317749 and batch: 300, loss is 3.5513035249710083 and perplexity is 34.85872710612667
At time: 112.09110045433044 and batch: 350, loss is 3.5466567277908325 and perplexity is 34.69712143648252
At time: 112.69625186920166 and batch: 400, loss is 3.467193145751953 and perplexity is 32.04666576789382
At time: 113.30069732666016 and batch: 450, loss is 3.500168218612671 and perplexity is 33.12102306264733
At time: 113.90815925598145 and batch: 500, loss is 3.3791155004501343 and perplexity is 29.34480416503046
At time: 114.51445531845093 and batch: 550, loss is 3.4065285444259645 and perplexity is 30.160361960964064
At time: 115.12289476394653 and batch: 600, loss is 3.4374028635025025 and perplexity is 31.10606646986447
At time: 115.7275857925415 and batch: 650, loss is 3.2761750268936156 and perplexity is 26.474315243733212
At time: 116.3368628025055 and batch: 700, loss is 3.2615544414520263 and perplexity is 26.090061110766996
At time: 116.93942403793335 and batch: 750, loss is 3.34987624168396 and perplexity is 28.49920641171564
At time: 117.54184150695801 and batch: 800, loss is 3.305405368804932 and perplexity is 27.2595895366365
At time: 118.15018796920776 and batch: 850, loss is 3.358493280410767 and perplexity is 28.7458463036474
At time: 118.7698438167572 and batch: 900, loss is 3.3143782901763914 and perplexity is 27.505288359777477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307023975947132 and perplexity of 74.21928157723265
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.38048028945923 and batch: 50, loss is 3.670942120552063 and perplexity is 39.288903312816785
At time: 120.98438572883606 and batch: 100, loss is 3.5567220640182495 and perplexity is 35.04812314159747
At time: 121.58807110786438 and batch: 150, loss is 3.5709761667251585 and perplexity is 35.551280195468635
At time: 122.18994879722595 and batch: 200, loss is 3.441007695198059 and perplexity is 31.21840095601011
At time: 122.79114151000977 and batch: 250, loss is 3.5855623483657837 and perplexity is 36.07363796740132
At time: 123.39666604995728 and batch: 300, loss is 3.5446834659576414 and perplexity is 34.628722437791204
At time: 124.0019600391388 and batch: 350, loss is 3.5406862831115724 and perplexity is 34.490581373955614
At time: 124.6070020198822 and batch: 400, loss is 3.463273010253906 and perplexity is 31.921284412340466
At time: 125.21030116081238 and batch: 450, loss is 3.493871660232544 and perplexity is 32.91312980021118
At time: 125.81323003768921 and batch: 500, loss is 3.3743843460083007 and perplexity is 29.206297271571824
At time: 126.41694402694702 and batch: 550, loss is 3.39893714427948 and perplexity is 29.93226945091367
At time: 127.02068781852722 and batch: 600, loss is 3.4301290464401246 and perplexity is 30.88062752761206
At time: 127.62535285949707 and batch: 650, loss is 3.270078411102295 and perplexity is 26.31340252527379
At time: 128.2277226448059 and batch: 700, loss is 3.2550774240493774 and perplexity is 25.921621413203916
At time: 128.83139491081238 and batch: 750, loss is 3.3435202693939208 and perplexity is 28.318640688720482
At time: 129.43426418304443 and batch: 800, loss is 3.2978180932998655 and perplexity is 27.053546163024137
At time: 130.0365011692047 and batch: 850, loss is 3.3497137784957887 and perplexity is 28.494576715869396
At time: 130.64023756980896 and batch: 900, loss is 3.3055172395706176 and perplexity is 27.262639258374417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305268170082406 and perplexity of 74.08908126404936
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 132.2450098991394 and batch: 50, loss is 3.667829642295837 and perplexity is 39.16680756429298
At time: 132.8658971786499 and batch: 100, loss is 3.553762183189392 and perplexity is 34.944538248916906
At time: 133.47168493270874 and batch: 150, loss is 3.569102931022644 and perplexity is 35.48474660414133
At time: 134.08894419670105 and batch: 200, loss is 3.4388621854782104 and perplexity is 31.151493374419992
At time: 134.69449925422668 and batch: 250, loss is 3.583178606033325 and perplexity is 35.98775011741409
At time: 135.30147290229797 and batch: 300, loss is 3.5424977254867556 and perplexity is 34.55311569616207
At time: 135.90553259849548 and batch: 350, loss is 3.5378427934646606 and perplexity is 34.39264706648426
At time: 136.50782680511475 and batch: 400, loss is 3.4608814191818236 and perplexity is 31.845032971008326
At time: 137.1088695526123 and batch: 450, loss is 3.4918962335586547 and perplexity is 32.84817650204386
At time: 137.70572209358215 and batch: 500, loss is 3.3723788261413574 and perplexity is 29.147782158377698
At time: 138.3132631778717 and batch: 550, loss is 3.3967394971847535 and perplexity is 29.866561114229608
At time: 138.91599011421204 and batch: 600, loss is 3.4277920055389406 and perplexity is 30.808542503659726
At time: 139.5207769870758 and batch: 650, loss is 3.2680589818954466 and perplexity is 26.26031808985263
At time: 140.12285232543945 and batch: 700, loss is 3.2529404020309447 and perplexity is 25.866285485597572
At time: 140.7269115447998 and batch: 750, loss is 3.341679644584656 and perplexity is 28.26656463704288
At time: 141.3333809375763 and batch: 800, loss is 3.2956567907333376 and perplexity is 26.99513840542285
At time: 141.93732976913452 and batch: 850, loss is 3.3474146223068235 and perplexity is 28.429138488639914
At time: 142.54180908203125 and batch: 900, loss is 3.3026482009887697 and perplexity is 27.184533791873882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303799145842252 and perplexity of 73.98032251192207
finished 12 epochs...
Completing Train Step...
At time: 144.13068747520447 and batch: 50, loss is 3.6663716411590577 and perplexity is 39.109743923875186
At time: 144.75282621383667 and batch: 100, loss is 3.552128438949585 and perplexity is 34.88749442104069
At time: 145.35728931427002 and batch: 150, loss is 3.567613682746887 and perplexity is 35.43194033702631
At time: 145.96113848686218 and batch: 200, loss is 3.4375763654708864 and perplexity is 31.11146390184467
At time: 146.56162357330322 and batch: 250, loss is 3.5819654083251953 and perplexity is 35.94411633500516
At time: 147.16387677192688 and batch: 300, loss is 3.5408730697631836 and perplexity is 34.497024355875354
At time: 147.76761436462402 and batch: 350, loss is 3.5364466094970703 and perplexity is 34.344662109755554
At time: 148.37036633491516 and batch: 400, loss is 3.4594680213928224 and perplexity is 31.800055065140455
At time: 148.97381114959717 and batch: 450, loss is 3.490772747993469 and perplexity is 32.81129277294937
At time: 149.59262466430664 and batch: 500, loss is 3.3714457321166993 and perplexity is 29.120597222036924
At time: 150.19386839866638 and batch: 550, loss is 3.3959957695007326 and perplexity is 29.844356783913522
At time: 150.80002522468567 and batch: 600, loss is 3.427165050506592 and perplexity is 30.789232986629667
At time: 151.40173983573914 and batch: 650, loss is 3.267502632141113 and perplexity is 26.245712231694156
At time: 152.00585794448853 and batch: 700, loss is 3.2527341842651367 and perplexity is 25.86095194794899
At time: 152.60845375061035 and batch: 750, loss is 3.3415942001342773 and perplexity is 28.264149519144055
At time: 153.2119493484497 and batch: 800, loss is 3.2960410070419313 and perplexity is 27.00551237064665
At time: 153.81126141548157 and batch: 850, loss is 3.348090605735779 and perplexity is 28.448362612020087
At time: 154.4161787033081 and batch: 900, loss is 3.3037311363220216 and perplexity is 27.213988830107294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303152005966395 and perplexity of 73.93246238296511
finished 13 epochs...
Completing Train Step...
At time: 156.017724275589 and batch: 50, loss is 3.6653721857070924 and perplexity is 39.07067500416011
At time: 156.62261986732483 and batch: 100, loss is 3.5510045289993286 and perplexity is 34.84830604514932
At time: 157.22492384910583 and batch: 150, loss is 3.5664932441711428 and perplexity is 35.39226325627598
At time: 157.8278784751892 and batch: 200, loss is 3.436594681739807 and perplexity is 31.080937270082604
At time: 158.43234133720398 and batch: 250, loss is 3.5808891248703003 and perplexity is 35.905451088408014
At time: 159.0358865261078 and batch: 300, loss is 3.539626245498657 and perplexity is 34.45403943174629
At time: 159.63994550704956 and batch: 350, loss is 3.535289611816406 and perplexity is 34.30494839412002
At time: 160.24525809288025 and batch: 400, loss is 3.458354687690735 and perplexity is 31.764670693070123
At time: 160.84853315353394 and batch: 450, loss is 3.4898685121536257 and perplexity is 32.78163703598324
At time: 161.45249319076538 and batch: 500, loss is 3.3706700372695924 and perplexity is 29.098017283530286
At time: 162.0532443523407 and batch: 550, loss is 3.3953348112106325 and perplexity is 29.824637426438784
At time: 162.65404224395752 and batch: 600, loss is 3.426627655029297 and perplexity is 30.772691437148303
At time: 163.2561390399933 and batch: 650, loss is 3.2670692443847655 and perplexity is 26.234340125800045
At time: 163.86230278015137 and batch: 700, loss is 3.252539277076721 and perplexity is 25.855911953696577
At time: 164.48282384872437 and batch: 750, loss is 3.3415253162384033 and perplexity is 28.26220264146664
At time: 165.08960342407227 and batch: 800, loss is 3.296328001022339 and perplexity is 27.01326390240312
At time: 165.69450998306274 and batch: 850, loss is 3.348646111488342 and perplexity is 28.464170231307293
At time: 166.29797267913818 and batch: 900, loss is 3.3046456241607665 and perplexity is 27.238887074771966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302814640410959 and perplexity of 73.90752432358614
finished 14 epochs...
Completing Train Step...
At time: 167.89086437225342 and batch: 50, loss is 3.6645586252212525 and perplexity is 39.03890157337804
At time: 168.51530694961548 and batch: 100, loss is 3.550081582069397 and perplexity is 34.81615774594074
At time: 169.1211814880371 and batch: 150, loss is 3.5655310583114623 and perplexity is 35.35822569888292
At time: 169.72589421272278 and batch: 200, loss is 3.4357352256774902 and perplexity is 31.054236046031384
At time: 170.33202028274536 and batch: 250, loss is 3.5799023342132568 and perplexity is 35.87003740055888
At time: 170.93772077560425 and batch: 300, loss is 3.5385558128356935 and perplexity is 34.41717843469482
At time: 171.5422079563141 and batch: 350, loss is 3.534266152381897 and perplexity is 34.26985663159655
At time: 172.14689588546753 and batch: 400, loss is 3.457400150299072 and perplexity is 31.73436459361339
At time: 172.74726700782776 and batch: 450, loss is 3.4890670585632324 and perplexity is 32.75537460074904
At time: 173.35072255134583 and batch: 500, loss is 3.369976296424866 and perplexity is 29.077837800915127
At time: 173.95655846595764 and batch: 550, loss is 3.3947214317321777 and perplexity is 29.80634921526915
At time: 174.56149744987488 and batch: 600, loss is 3.426135859489441 and perplexity is 30.757561285525643
At time: 175.16716647148132 and batch: 650, loss is 3.2666953945159913 and perplexity is 26.22453425426274
At time: 175.77303409576416 and batch: 700, loss is 3.2523499870300294 and perplexity is 25.851018150104373
At time: 176.37874388694763 and batch: 750, loss is 3.341463007926941 and perplexity is 28.26044172620224
At time: 176.9843168258667 and batch: 800, loss is 3.296556816101074 and perplexity is 27.0194456517216
At time: 177.5868468284607 and batch: 850, loss is 3.3491210889816285 and perplexity is 28.477693282850563
At time: 178.18551754951477 and batch: 900, loss is 3.3054272365570068 and perplexity is 27.26018564909995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302616067128639 and perplexity of 73.89284972093333
finished 15 epochs...
Completing Train Step...
At time: 179.71390199661255 and batch: 50, loss is 3.6638427066802977 and perplexity is 39.01096290202287
At time: 180.3225908279419 and batch: 100, loss is 3.5492615795135496 and perplexity is 34.787620109670684
At time: 180.9102020263672 and batch: 150, loss is 3.5646590995788574 and perplexity is 35.32740822295157
At time: 181.50562977790833 and batch: 200, loss is 3.4349414920806884 and perplexity is 31.02959703525753
At time: 182.1111090183258 and batch: 250, loss is 3.578985195159912 and perplexity is 35.837154669737124
At time: 182.72403573989868 and batch: 300, loss is 3.537591404914856 and perplexity is 34.38400223546556
At time: 183.3279948234558 and batch: 350, loss is 3.533334741592407 and perplexity is 34.237952177758366
At time: 183.9301793575287 and batch: 400, loss is 3.4565464305877684 and perplexity is 31.707283902338876
At time: 184.53364825248718 and batch: 450, loss is 3.488328471183777 and perplexity is 32.731190826474204
At time: 185.13419556617737 and batch: 500, loss is 3.3693363094329833 and perplexity is 29.05923431659943
At time: 185.73756861686707 and batch: 550, loss is 3.394145493507385 and perplexity is 29.78918754192079
At time: 186.3415491580963 and batch: 600, loss is 3.425675745010376 and perplexity is 30.743412541508057
At time: 186.9524405002594 and batch: 650, loss is 3.2663573360443117 and perplexity is 26.21567032663956
At time: 187.56364393234253 and batch: 700, loss is 3.252167296409607 and perplexity is 25.846295842933728
At time: 188.16993498802185 and batch: 750, loss is 3.3414051818847654 and perplexity is 28.25880758395553
At time: 188.77562546730042 and batch: 800, loss is 3.2967466592788695 and perplexity is 27.024575596073454
At time: 189.38579726219177 and batch: 850, loss is 3.349536643028259 and perplexity is 28.489529762711175
At time: 189.98980140686035 and batch: 900, loss is 3.306105628013611 and perplexity is 27.278685000341188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302487726080908 and perplexity of 73.88336684371448
finished 16 epochs...
Completing Train Step...
At time: 191.6001739501953 and batch: 50, loss is 3.6631877565383912 and perplexity is 38.98542103157316
At time: 192.20452046394348 and batch: 100, loss is 3.548505735397339 and perplexity is 34.761336026280055
At time: 192.80644989013672 and batch: 150, loss is 3.563847942352295 and perplexity is 35.29876375962812
At time: 193.41461968421936 and batch: 200, loss is 3.4341910314559936 and perplexity is 31.00631928009425
At time: 194.01927518844604 and batch: 250, loss is 3.5781235218048097 and perplexity is 35.80628804881776
At time: 194.62778043746948 and batch: 300, loss is 3.5367010641098022 and perplexity is 34.3534023794059
At time: 195.254652261734 and batch: 350, loss is 3.532472472190857 and perplexity is 34.20844256367078
At time: 195.86272859573364 and batch: 400, loss is 3.455763611793518 and perplexity is 31.682472557246168
At time: 196.4727578163147 and batch: 450, loss is 3.487634692192078 and perplexity is 32.70849048932454
At time: 197.07848811149597 and batch: 500, loss is 3.368735194206238 and perplexity is 29.041771617447164
At time: 197.6839349269867 and batch: 550, loss is 3.393600540161133 and perplexity is 29.772958246993614
At time: 198.2881736755371 and batch: 600, loss is 3.425240354537964 and perplexity is 30.730030066110594
At time: 198.89285612106323 and batch: 650, loss is 3.2660439729690554 and perplexity is 26.20745659057411
At time: 199.4967863559723 and batch: 700, loss is 3.2519905805587768 and perplexity is 25.841728796319682
At time: 200.10047936439514 and batch: 750, loss is 3.3413498306274416 and perplexity is 28.257243466713614
At time: 200.7042806148529 and batch: 800, loss is 3.296907262802124 and perplexity is 27.028916186676387
At time: 201.30979442596436 and batch: 850, loss is 3.3499050426483152 and perplexity is 28.500027228163752
At time: 201.90051174163818 and batch: 900, loss is 3.306702146530151 and perplexity is 27.294962095362383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3024007718857025 and perplexity of 73.8769426543207
finished 17 epochs...
Completing Train Step...
At time: 203.42896389961243 and batch: 50, loss is 3.662575101852417 and perplexity is 38.96154374570588
At time: 204.0641758441925 and batch: 100, loss is 3.5477958393096922 and perplexity is 34.73666784679165
At time: 204.67482089996338 and batch: 150, loss is 3.5630824756622315 and perplexity is 35.271754070597176
At time: 205.28025102615356 and batch: 200, loss is 3.4334733486175537 and perplexity is 30.984074560145412
At time: 205.88963675498962 and batch: 250, loss is 3.5773072052001953 and perplexity is 35.777070708256495
At time: 206.49279117584229 and batch: 300, loss is 3.5358673477172853 and perplexity is 34.32477332061956
At time: 207.0985095500946 and batch: 350, loss is 3.5316645288467408 and perplexity is 34.18081524234693
At time: 207.70098042488098 and batch: 400, loss is 3.455033497810364 and perplexity is 31.659349183387704
At time: 208.30547165870667 and batch: 450, loss is 3.4869757175445555 and perplexity is 32.686943523565155
At time: 208.91169047355652 and batch: 500, loss is 3.368163981437683 and perplexity is 29.025187323708934
At time: 209.51603293418884 and batch: 550, loss is 3.3930817365646364 and perplexity is 29.757515935286527
At time: 210.1218979358673 and batch: 600, loss is 3.424825372695923 and perplexity is 30.717280307269956
At time: 210.74255275726318 and batch: 650, loss is 3.2657477474212646 and perplexity is 26.199694422122427
At time: 211.35018253326416 and batch: 700, loss is 3.251818857192993 and perplexity is 25.837291548673562
At time: 211.9604115486145 and batch: 750, loss is 3.3412954664230345 and perplexity is 28.25570732590972
At time: 212.57220840454102 and batch: 800, loss is 3.297044243812561 and perplexity is 27.032618888520776
At time: 213.18022513389587 and batch: 850, loss is 3.350234112739563 and perplexity is 28.50940727798657
At time: 213.78332424163818 and batch: 900, loss is 3.3072321701049803 and perplexity is 27.309432903342913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302342245023545 and perplexity of 73.87261899520769
finished 18 epochs...
Completing Train Step...
At time: 215.37720704078674 and batch: 50, loss is 3.6619943475723264 and perplexity is 38.93892323153261
At time: 215.99717378616333 and batch: 100, loss is 3.5471217441558838 and perplexity is 34.7132599178088
At time: 216.6010193824768 and batch: 150, loss is 3.562353644371033 and perplexity is 35.24605627835016
At time: 217.20230531692505 and batch: 200, loss is 3.432782244682312 and perplexity is 30.962668741961664
At time: 217.80819821357727 and batch: 250, loss is 3.5765285015106203 and perplexity is 35.74922181571873
At time: 218.4120535850525 and batch: 300, loss is 3.535079207420349 and perplexity is 34.29773124145833
At time: 219.0193693637848 and batch: 350, loss is 3.530900869369507 and perplexity is 34.154722703023054
At time: 219.62742042541504 and batch: 400, loss is 3.4543448877334595 and perplexity is 31.637555740964995
At time: 220.23457503318787 and batch: 450, loss is 3.486345257759094 and perplexity is 32.66634221499458
At time: 220.83753180503845 and batch: 500, loss is 3.367617282867432 and perplexity is 29.009323632021676
At time: 221.4403476715088 and batch: 550, loss is 3.392585024833679 and perplexity is 29.74273869835461
At time: 222.04614067077637 and batch: 600, loss is 3.4244273328781127 and perplexity is 30.705056039646852
At time: 222.65011739730835 and batch: 650, loss is 3.2654645347595217 and perplexity is 26.192275387560137
At time: 223.25079250335693 and batch: 700, loss is 3.2516504383087157 and perplexity is 25.832940427273975
At time: 223.85355305671692 and batch: 750, loss is 3.34124071598053 and perplexity is 28.254160355779383
At time: 224.45788097381592 and batch: 800, loss is 3.2971612930297853 and perplexity is 27.03578322058889
At time: 225.06488728523254 and batch: 850, loss is 3.3505297708511352 and perplexity is 28.51783756168389
At time: 225.6848521232605 and batch: 900, loss is 3.3077070093154908 and perplexity is 27.322403572149582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302303366465111 and perplexity of 73.86974699010348
finished 19 epochs...
Completing Train Step...
At time: 227.2958266735077 and batch: 50, loss is 3.661438670158386 and perplexity is 38.91729176198583
At time: 227.89977097511292 and batch: 100, loss is 3.5464771366119385 and perplexity is 34.690890699049
At time: 228.5041799545288 and batch: 150, loss is 3.56165518283844 and perplexity is 35.2214468592309
At time: 229.10769319534302 and batch: 200, loss is 3.4321140336990354 and perplexity is 30.94198605760473
At time: 229.7096357345581 and batch: 250, loss is 3.575782060623169 and perplexity is 35.7225470916518
At time: 230.3151969909668 and batch: 300, loss is 3.5343290328979493 and perplexity is 34.27201160561881
At time: 230.91975212097168 and batch: 350, loss is 3.5301739883422854 and perplexity is 34.12990530384122
At time: 231.52217984199524 and batch: 400, loss is 3.4536895513534547 and perplexity is 31.616829291853353
At time: 232.12400436401367 and batch: 450, loss is 3.485738921165466 and perplexity is 32.64654141990961
At time: 232.7283480167389 and batch: 500, loss is 3.367091164588928 and perplexity is 28.994065310804647
At time: 233.33444786071777 and batch: 550, loss is 3.3921070528030395 and perplexity is 29.728525898073265
At time: 233.94237875938416 and batch: 600, loss is 3.42404372215271 and perplexity is 30.693279509771948
At time: 234.54776453971863 and batch: 650, loss is 3.2651913213729857 and perplexity is 26.18512028477986
At time: 235.15122389793396 and batch: 700, loss is 3.251484513282776 and perplexity is 25.82865445154863
At time: 235.75386595726013 and batch: 750, loss is 3.341184878349304 and perplexity is 28.252582754438013
At time: 236.35642862319946 and batch: 800, loss is 3.2972610235214233 and perplexity is 27.038479646997025
At time: 236.95774912834167 and batch: 850, loss is 3.3507962465286254 and perplexity is 28.52543788437429
At time: 237.56065917015076 and batch: 900, loss is 3.3081353855133058 and perplexity is 27.334110346779767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302277865475172 and perplexity of 73.86786326244723
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff4d58a7b70>
ELAPSED
1481.6617834568024


RESULTS SO FAR:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.6422674277245}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.4276470305051885, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.022917934919097882, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.09575604444889}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8025758421055519, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.6311786327412237, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.09282907580256}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9409394853848363, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.42240237976402184, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -73.86786326244723}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9502745003363438, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.41909657609517714, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.02583070997373}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8473086042449095, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9926227260253405, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -378.76405961540183}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.7549857566643324, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.9772558867874839, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.6422674277245}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.4276470305051885, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.022917934919097882, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -75.09575604444889}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.8025758421055519, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.6311786327412237, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -74.09282907580256}, {'params': {'tie_weights': 'FALSE', 'rnn_dropout': 0.9409394853848363, 'tune_wordvecs': 'FALSE', 'num_layers': 2, 'dropout': 0.42240237976402184, 'data': 'ptb', 'wordvec_dim': 300, 'batch_size': 32, 'seq_len': 35, 'wordvec_source': 'gigavec'}, 'best_accuracy': -73.86786326244723}]
