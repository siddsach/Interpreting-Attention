TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.6225166320800781 and batch: 50, loss is 7.0783312892913814 and perplexity is 1185.9877958069733
At time: 2.756021499633789 and batch: 100, loss is 6.417687911987304 and perplexity is 612.5851247753983
At time: 3.86342453956604 and batch: 150, loss is 6.024948482513428 and perplexity is 413.6203327941936
At time: 4.970620632171631 and batch: 200, loss is 5.847367267608643 and perplexity is 346.32140761181415
At time: 6.077855110168457 and batch: 250, loss is 5.846140785217285 and perplexity is 345.89691087572675
At time: 7.184601545333862 and batch: 300, loss is 5.7183647632598875 and perplexity is 304.4067386602929
At time: 8.29150104522705 and batch: 350, loss is 5.674030742645264 and perplexity is 291.20594820592953
At time: 9.397908210754395 and batch: 400, loss is 5.501394472122192 and perplexity is 245.03338636106648
At time: 10.522127866744995 and batch: 450, loss is 5.483374242782593 and perplexity is 240.65737532162086
At time: 11.640673637390137 and batch: 500, loss is 5.41743278503418 and perplexity is 225.29998594037494
At time: 12.755549192428589 and batch: 550, loss is 5.451512546539306 and perplexity is 233.11048985201188
At time: 13.877670288085938 and batch: 600, loss is 5.349721326828003 and perplexity is 210.54961516128657
At time: 14.994731903076172 and batch: 650, loss is 5.232701606750489 and perplexity is 187.2981265037435
At time: 16.109487771987915 and batch: 700, loss is 5.32213565826416 and perplexity is 204.82084255273995
At time: 17.221211671829224 and batch: 750, loss is 5.286339321136475 and perplexity is 197.6186811615725
At time: 18.332865238189697 and batch: 800, loss is 5.249489707946777 and perplexity is 190.46904881353905
At time: 19.44802761077881 and batch: 850, loss is 5.272752323150635 and perplexity is 194.9517950536736
At time: 20.555989503860474 and batch: 900, loss is 5.168617105484008 and perplexity is 175.67173395204023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.089194937928082 and perplexity of 162.25918074502653
finished 1 epochs...
Completing Train Step...
At time: 22.814162492752075 and batch: 50, loss is 5.0226428508758545 and perplexity is 151.81199053532998
At time: 23.697996854782104 and batch: 100, loss is 4.886255197525024 and perplexity is 132.4566202557747
At time: 24.576043605804443 and batch: 150, loss is 4.87015061378479 and perplexity is 130.3405465011973
At time: 25.454248666763306 and batch: 200, loss is 4.7395751094818115 and perplexity is 114.38559000096042
At time: 26.333679676055908 and batch: 250, loss is 4.833891639709472 and perplexity is 125.69918598367387
At time: 27.214872360229492 and batch: 300, loss is 4.761181335449219 and perplexity is 116.88392349448345
At time: 28.091624975204468 and batch: 350, loss is 4.75157527923584 and perplexity is 115.76650553739772
At time: 28.973578214645386 and batch: 400, loss is 4.6136291885375975 and perplexity is 100.84948810055361
At time: 29.855297327041626 and batch: 450, loss is 4.636247158050537 and perplexity is 103.15649025088383
At time: 30.736340045928955 and batch: 500, loss is 4.5319039916992185 and perplexity is 92.93534085264645
At time: 31.615851640701294 and batch: 550, loss is 4.599380016326904 and perplexity is 99.42265610643442
At time: 32.496673345565796 and batch: 600, loss is 4.550109806060791 and perplexity is 94.64280009743425
At time: 33.379268407821655 and batch: 650, loss is 4.411422281265259 and perplexity is 82.38655707110097
At time: 34.259862661361694 and batch: 700, loss is 4.457733211517334 and perplexity is 86.29168223969884
At time: 35.141465187072754 and batch: 750, loss is 4.4966825675964355 and perplexity is 89.71900034034321
At time: 36.03809690475464 and batch: 800, loss is 4.450068321228027 and perplexity is 85.63279434001588
At time: 36.91939425468445 and batch: 850, loss is 4.5033687973022465 and perplexity is 90.32089213666907
At time: 37.80131268501282 and batch: 900, loss is 4.435398435592651 and perplexity is 84.38574047353998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.555877685546875 and perplexity of 95.19026570266253
finished 2 epochs...
Completing Train Step...
At time: 39.91409611701965 and batch: 50, loss is 4.47790491104126 and perplexity is 88.05000670052114
At time: 40.79545736312866 and batch: 100, loss is 4.355056438446045 and perplexity is 77.8712201434654
At time: 41.67020916938782 and batch: 150, loss is 4.359377026557922 and perplexity is 78.20839748916953
At time: 42.54506015777588 and batch: 200, loss is 4.246038856506348 and perplexity is 69.82826402322665
At time: 43.420591831207275 and batch: 250, loss is 4.382420482635498 and perplexity is 80.03151404239266
At time: 44.297362089157104 and batch: 300, loss is 4.34108684539795 and perplexity is 76.7909538955663
At time: 45.176899909973145 and batch: 350, loss is 4.337901477813721 and perplexity is 76.54673564920608
At time: 46.05595898628235 and batch: 400, loss is 4.244249649047852 and perplexity is 69.70343847512324
At time: 46.938783168792725 and batch: 450, loss is 4.279734945297241 and perplexity is 72.22129487633762
At time: 47.820581674575806 and batch: 500, loss is 4.152927474975586 and perplexity is 63.61997383021027
At time: 48.70117449760437 and batch: 550, loss is 4.229450044631958 and perplexity is 68.67945115302528
At time: 49.583415031433105 and batch: 600, loss is 4.22319712638855 and perplexity is 68.25134401431092
At time: 50.46454095840454 and batch: 650, loss is 4.070910525321961 and perplexity is 58.610304469989465
At time: 51.34732484817505 and batch: 700, loss is 4.0900521564483645 and perplexity is 59.743007605978654
At time: 52.23121523857117 and batch: 750, loss is 4.173088932037354 and perplexity is 64.91566280014844
At time: 53.11414694786072 and batch: 800, loss is 4.139092636108399 and perplexity is 62.74586228198171
At time: 53.996639013290405 and batch: 850, loss is 4.204421615600586 and perplexity is 66.98184520616459
At time: 54.87890958786011 and batch: 900, loss is 4.145451788902283 and perplexity is 63.14614418539918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404097047570634 and perplexity of 81.78526129205434
finished 3 epochs...
Completing Train Step...
At time: 57.00098204612732 and batch: 50, loss is 4.213450212478637 and perplexity is 67.58933555073924
At time: 57.8936984539032 and batch: 100, loss is 4.089878544807434 and perplexity is 59.73263642469903
At time: 58.777247190475464 and batch: 150, loss is 4.098417959213257 and perplexity is 60.2449022729306
At time: 59.660327434539795 and batch: 200, loss is 3.9855604934692384 and perplexity is 53.81544423136496
At time: 60.54360318183899 and batch: 250, loss is 4.132183575630188 and perplexity is 62.313841472214925
At time: 61.434651374816895 and batch: 300, loss is 4.097395114898681 and perplexity is 60.183312620840965
At time: 62.3217396736145 and batch: 350, loss is 4.093296594619751 and perplexity is 59.93715487943576
At time: 63.20580554008484 and batch: 400, loss is 4.015846915245056 and perplexity is 55.47025411852093
At time: 64.08769226074219 and batch: 450, loss is 4.054422554969787 and perplexity is 57.65186260097309
At time: 64.97213983535767 and batch: 500, loss is 3.924135055541992 and perplexity is 50.609284901745106
At time: 65.85991907119751 and batch: 550, loss is 4.00037190914154 and perplexity is 54.61845936063093
At time: 66.76666522026062 and batch: 600, loss is 4.012257604598999 and perplexity is 55.27151103357885
At time: 67.66678237915039 and batch: 650, loss is 3.8545541048049925 and perplexity is 47.20756262227244
At time: 68.55347895622253 and batch: 700, loss is 3.867860345840454 and perplexity is 47.839915619790446
At time: 69.438729763031 and batch: 750, loss is 3.965320224761963 and perplexity is 52.737154428722754
At time: 70.32127285003662 and batch: 800, loss is 3.935599060058594 and perplexity is 51.1928083393976
At time: 71.2066216468811 and batch: 850, loss is 3.9992132949829102 and perplexity is 54.55521428568774
At time: 72.09139633178711 and batch: 900, loss is 3.948675122261047 and perplexity is 51.86660438442576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3472051751123715 and perplexity of 77.26222650104485
finished 4 epochs...
Completing Train Step...
At time: 74.19435048103333 and batch: 50, loss is 4.024281392097473 and perplexity is 55.940095339946744
At time: 75.07723116874695 and batch: 100, loss is 3.903908553123474 and perplexity is 49.59591904976641
At time: 75.96587133407593 and batch: 150, loss is 3.914782886505127 and perplexity is 50.13818465274111
At time: 76.84970331192017 and batch: 200, loss is 3.801037745475769 and perplexity is 44.74759702330158
At time: 77.73559594154358 and batch: 250, loss is 3.9520471477508545 and perplexity is 52.04179510426494
At time: 78.63170838356018 and batch: 300, loss is 3.914682779312134 and perplexity is 50.13316571103403
At time: 79.55537295341492 and batch: 350, loss is 3.912374258041382 and perplexity is 50.01756571548292
At time: 80.44312882423401 and batch: 400, loss is 3.8470194578170775 and perplexity is 46.85320695161976
At time: 81.32768964767456 and batch: 450, loss is 3.884077491760254 and perplexity is 48.62206750600792
At time: 82.21078109741211 and batch: 500, loss is 3.755641007423401 and perplexity is 42.76162154410326
At time: 83.0974645614624 and batch: 550, loss is 3.826155495643616 and perplexity is 45.88579057657411
At time: 83.98359489440918 and batch: 600, loss is 3.8486616945266725 and perplexity is 46.93021422281478
At time: 84.87015151977539 and batch: 650, loss is 3.6916267347335814 and perplexity is 40.110042314131796
At time: 85.756338596344 and batch: 700, loss is 3.7029475116729738 and perplexity is 40.566699134114046
At time: 86.64019274711609 and batch: 750, loss is 3.8023898077011107 and perplexity is 44.80813947827179
At time: 87.5237443447113 and batch: 800, loss is 3.777532615661621 and perplexity is 43.7080639865099
At time: 88.40845561027527 and batch: 850, loss is 3.84019323348999 and perplexity is 46.53446558825618
At time: 89.29518246650696 and batch: 900, loss is 3.7911177587509157 and perplexity is 44.30589591161255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336585475973887 and perplexity of 76.44606625933842
finished 5 epochs...
Completing Train Step...
At time: 91.3997004032135 and batch: 50, loss is 3.8714401149749755 and perplexity is 48.01147836742754
At time: 92.29681253433228 and batch: 100, loss is 3.7534582805633545 and perplexity is 42.668386394628996
At time: 93.18013978004456 and batch: 150, loss is 3.763942060470581 and perplexity is 43.118065415866454
At time: 94.06306958198547 and batch: 200, loss is 3.6548171186447145 and perplexity is 38.66045019308393
At time: 94.94714689254761 and batch: 250, loss is 3.8063683032989504 and perplexity is 44.986763555875235
At time: 95.83165502548218 and batch: 300, loss is 3.76515793800354 and perplexity is 43.17052358776391
At time: 96.71603751182556 and batch: 350, loss is 3.7639174032211304 and perplexity is 43.11700225607901
At time: 97.6024808883667 and batch: 400, loss is 3.7085278701782225 and perplexity is 40.79370866684833
At time: 98.48611903190613 and batch: 450, loss is 3.7435102558135984 and perplexity is 42.24602454698406
At time: 99.37110376358032 and batch: 500, loss is 3.618461799621582 and perplexity is 37.28017930993149
At time: 100.25539994239807 and batch: 550, loss is 3.684458031654358 and perplexity is 39.82353350555601
At time: 101.16249346733093 and batch: 600, loss is 3.7125363826751707 and perplexity is 40.95755893636494
At time: 102.04601573944092 and batch: 650, loss is 3.5560960960388184 and perplexity is 35.02619100389579
At time: 102.93114471435547 and batch: 700, loss is 3.5680030727386476 and perplexity is 35.445739866507175
At time: 103.8158597946167 and batch: 750, loss is 3.6659235382080078 and perplexity is 39.09222265816671
At time: 104.71178579330444 and batch: 800, loss is 3.645833382606506 and perplexity is 38.31469034647655
At time: 105.60085344314575 and batch: 850, loss is 3.7076783180236816 and perplexity is 40.75906700079323
At time: 106.4828634262085 and batch: 900, loss is 3.6582090997695924 and perplexity is 38.791808366695406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344825640116652 and perplexity of 77.07859689242383
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 108.60608506202698 and batch: 50, loss is 3.7768935060501097 and perplexity is 43.680138667336045
At time: 109.49042558670044 and batch: 100, loss is 3.6710537767410276 and perplexity is 39.29329040694783
At time: 110.3753228187561 and batch: 150, loss is 3.680881175994873 and perplexity is 39.68134491997497
At time: 111.26155471801758 and batch: 200, loss is 3.5563361978530885 and perplexity is 35.03460186559396
At time: 112.15070247650146 and batch: 250, loss is 3.7016580390930174 and perplexity is 40.51442319935693
At time: 113.03619861602783 and batch: 300, loss is 3.657782654762268 and perplexity is 38.77526932043984
At time: 113.92519474029541 and batch: 350, loss is 3.642129817008972 and perplexity is 38.17305182310788
At time: 114.8145272731781 and batch: 400, loss is 3.582076029777527 and perplexity is 35.94809274529101
At time: 115.69916653633118 and batch: 450, loss is 3.6001746129989622 and perplexity is 36.60462552911798
At time: 116.58367443084717 and batch: 500, loss is 3.4733441781997683 and perplexity is 32.2443933398103
At time: 117.46641254425049 and batch: 550, loss is 3.5146384716033934 and perplexity is 33.60377700798291
At time: 118.35027074813843 and batch: 600, loss is 3.5338821744918825 and perplexity is 34.25670029038976
At time: 119.23631405830383 and batch: 650, loss is 3.366434712409973 and perplexity is 28.975038339286336
At time: 120.12071681022644 and batch: 700, loss is 3.3632121324539184 and perplexity is 28.881814253221652
At time: 121.0008704662323 and batch: 750, loss is 3.4469926357269287 and perplexity is 31.405801460191977
At time: 121.88411068916321 and batch: 800, loss is 3.409800953865051 and perplexity is 30.25922067901205
At time: 122.77049899101257 and batch: 850, loss is 3.4486880111694336 and perplexity is 31.459091245071853
At time: 123.71811246871948 and batch: 900, loss is 3.3926784372329712 and perplexity is 29.745517168707714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316844783417166 and perplexity of 74.95176575799809
finished 7 epochs...
Completing Train Step...
At time: 125.85642457008362 and batch: 50, loss is 3.686229934692383 and perplexity is 39.894159498286584
At time: 126.73897194862366 and batch: 100, loss is 3.5643511152267457 and perplexity is 35.31652960932561
At time: 127.62062072753906 and batch: 150, loss is 3.5732029819488527 and perplexity is 35.63053453706702
At time: 128.5037009716034 and batch: 200, loss is 3.4520987558364866 and perplexity is 31.56657336560448
At time: 129.3932101726532 and batch: 250, loss is 3.600450830459595 and perplexity is 36.61473776235244
At time: 130.28338384628296 and batch: 300, loss is 3.5562898778915404 and perplexity is 35.032979101766166
At time: 131.17286491394043 and batch: 350, loss is 3.545260763168335 and perplexity is 34.64871927416799
At time: 132.06142449378967 and batch: 400, loss is 3.491040115356445 and perplexity is 32.82006661464111
At time: 132.9512710571289 and batch: 450, loss is 3.5141496372222902 and perplexity is 33.5873543407557
At time: 133.83860087394714 and batch: 500, loss is 3.3915653944015505 and perplexity is 29.712427552552338
At time: 134.72476935386658 and batch: 550, loss is 3.435330319404602 and perplexity is 31.041664536369908
At time: 135.61681747436523 and batch: 600, loss is 3.462171587944031 and perplexity is 31.8861449527736
At time: 136.50523853302002 and batch: 650, loss is 3.301309733390808 and perplexity is 27.148172513852412
At time: 137.39541625976562 and batch: 700, loss is 3.3034047079086304 and perplexity is 27.205106860653903
At time: 138.28726243972778 and batch: 750, loss is 3.394035506248474 and perplexity is 29.78591129101409
At time: 139.178218126297 and batch: 800, loss is 3.36264253616333 and perplexity is 28.865367963275077
At time: 140.06800079345703 and batch: 850, loss is 3.408708248138428 and perplexity is 30.226174313557454
At time: 140.95348501205444 and batch: 900, loss is 3.35914391040802 and perplexity is 28.764555299205863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329759571650257 and perplexity of 75.92602960452898
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.0689239501953 and batch: 50, loss is 3.651123948097229 and perplexity is 38.51793388757697
At time: 143.97270011901855 and batch: 100, loss is 3.5434199476242068 and perplexity is 34.584996042496044
At time: 144.8640694618225 and batch: 150, loss is 3.5583992910385134 and perplexity is 35.106956125083016
At time: 145.7716748714447 and batch: 200, loss is 3.4314179849624633 and perplexity is 30.920456421009707
At time: 146.65921711921692 and batch: 250, loss is 3.5764748811721803 and perplexity is 35.7473049817371
At time: 147.54638838768005 and batch: 300, loss is 3.531466875076294 and perplexity is 34.17405994296453
At time: 148.43299102783203 and batch: 350, loss is 3.515459499359131 and perplexity is 33.63137797063184
At time: 149.31989216804504 and batch: 400, loss is 3.458948631286621 and perplexity is 31.783542719708596
At time: 150.20449233055115 and batch: 450, loss is 3.477756910324097 and perplexity is 32.38699360712557
At time: 151.09127712249756 and batch: 500, loss is 3.351348738670349 and perplexity is 28.541202319106784
At time: 151.98178958892822 and batch: 550, loss is 3.3849841260910036 and perplexity is 29.51752415394033
At time: 152.8661289215088 and batch: 600, loss is 3.4126215982437134 and perplexity is 30.344691664683136
At time: 153.75108861923218 and batch: 650, loss is 3.246170649528503 and perplexity is 25.69176851953874
At time: 154.6356818675995 and batch: 700, loss is 3.2381232500076296 and perplexity is 25.4858462744559
At time: 155.52063918113708 and batch: 750, loss is 3.324424829483032 and perplexity is 27.783014075615686
At time: 156.4053418636322 and batch: 800, loss is 3.2867924690246584 and perplexity is 26.756902274850965
At time: 157.29151248931885 and batch: 850, loss is 3.327783408164978 and perplexity is 27.876482386888497
At time: 158.17623925209045 and batch: 900, loss is 3.286187210083008 and perplexity is 26.740712320549797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319141544707834 and perplexity of 75.12410991318916
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.34669613838196 and batch: 50, loss is 3.632143483161926 and perplexity is 37.79374010267005
At time: 161.2449369430542 and batch: 100, loss is 3.5237607765197754 and perplexity is 33.91172336295197
At time: 162.13728618621826 and batch: 150, loss is 3.541394467353821 and perplexity is 34.51501571117573
At time: 163.02021837234497 and batch: 200, loss is 3.4190393829345704 and perplexity is 30.54006361928659
At time: 163.90043687820435 and batch: 250, loss is 3.5620218610763548 and perplexity is 35.234364165404855
At time: 164.78246521949768 and batch: 300, loss is 3.5134982204437257 and perplexity is 33.56548209933651
At time: 165.66684675216675 and batch: 350, loss is 3.500501022338867 and perplexity is 33.13204769695594
At time: 166.54933977127075 and batch: 400, loss is 3.4398238611221315 and perplexity is 31.181465416251356
At time: 167.43000078201294 and batch: 450, loss is 3.4568955278396607 and perplexity is 31.718354760304766
At time: 168.33821606636047 and batch: 500, loss is 3.3310695028305055 and perplexity is 27.968237822910364
At time: 169.21997833251953 and batch: 550, loss is 3.365248737335205 and perplexity is 28.940695035202236
At time: 170.10080790519714 and batch: 600, loss is 3.3973838233947755 and perplexity is 29.885811123335454
At time: 170.98209643363953 and batch: 650, loss is 3.2282554292678833 and perplexity is 25.23559326859191
At time: 171.86949491500854 and batch: 700, loss is 3.216775507926941 and perplexity is 24.94754717952447
At time: 172.7528154850006 and batch: 750, loss is 3.3029329681396487 and perplexity is 27.192276156442997
At time: 173.63558769226074 and batch: 800, loss is 3.2610390901565554 and perplexity is 26.07661902796954
At time: 174.519300699234 and batch: 850, loss is 3.298672065734863 and perplexity is 27.076659013180137
At time: 175.40320491790771 and batch: 900, loss is 3.2609291648864747 and perplexity is 26.07375270612359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316042965405608 and perplexity of 74.8916921694764
finished 10 epochs...
Completing Train Step...
At time: 177.54048228263855 and batch: 50, loss is 3.621078219413757 and perplexity is 37.37784762387172
At time: 178.4223518371582 and batch: 100, loss is 3.5058843803405764 and perplexity is 33.31089032607478
At time: 179.30458426475525 and batch: 150, loss is 3.5217598628997804 and perplexity is 33.84393677396829
At time: 180.18793869018555 and batch: 200, loss is 3.4003791999816895 and perplexity is 29.975464588166975
At time: 181.07138752937317 and batch: 250, loss is 3.5451873970031738 and perplexity is 34.64617732375481
At time: 181.95269441604614 and batch: 300, loss is 3.497501196861267 and perplexity is 33.03280626406137
At time: 182.8350350856781 and batch: 350, loss is 3.4835893201828 and perplexity is 32.57643975469067
At time: 183.71647787094116 and batch: 400, loss is 3.4258299922943114 and perplexity is 30.74815499513738
At time: 184.59922289848328 and batch: 450, loss is 3.444964723587036 and perplexity is 31.342177787560836
At time: 185.48082304000854 and batch: 500, loss is 3.319487662315369 and perplexity is 27.646182748216987
At time: 186.3630030155182 and batch: 550, loss is 3.3548745584487913 and perplexity is 28.642011067049836
At time: 187.24447965621948 and batch: 600, loss is 3.3891976499557495 and perplexity is 29.642159338658313
At time: 188.12531995773315 and batch: 650, loss is 3.2217225933074953 and perplexity is 25.071270608309607
At time: 189.00840520858765 and batch: 700, loss is 3.2125638341903686 and perplexity is 24.84269720221183
At time: 189.9101164340973 and batch: 750, loss is 3.3007102537155153 and perplexity is 27.131902613433578
At time: 190.79376316070557 and batch: 800, loss is 3.260909695625305 and perplexity is 26.073245074364106
At time: 191.67513751983643 and batch: 850, loss is 3.3018746614456176 and perplexity is 27.163513611042394
At time: 192.55524730682373 and batch: 900, loss is 3.2664758253097532 and perplexity is 26.218776786197044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3156805169092465 and perplexity of 74.86455270688539
finished 11 epochs...
Completing Train Step...
At time: 194.6386272907257 and batch: 50, loss is 3.613885736465454 and perplexity is 37.10997259030242
At time: 195.52584767341614 and batch: 100, loss is 3.4972429656982422 and perplexity is 33.02427726535585
At time: 196.41537618637085 and batch: 150, loss is 3.512008032798767 and perplexity is 33.515500482858016
At time: 197.29877519607544 and batch: 200, loss is 3.390899248123169 and perplexity is 29.692641320510617
At time: 198.1856245994568 and batch: 250, loss is 3.5356315326690675 and perplexity is 34.316679976847524
At time: 199.07701444625854 and batch: 300, loss is 3.4878491878509523 and perplexity is 32.715507071029094
At time: 199.96352171897888 and batch: 350, loss is 3.474037027359009 and perplexity is 32.26674158171232
At time: 200.84757924079895 and batch: 400, loss is 3.417193818092346 and perplexity is 30.48375193101123
At time: 201.73225212097168 and batch: 450, loss is 3.4370831298828124 and perplexity is 31.096122404448472
At time: 202.61629271507263 and batch: 500, loss is 3.3121521854400635 and perplexity is 27.444126808352753
At time: 203.50479078292847 and batch: 550, loss is 3.348197646141052 and perplexity is 28.45140789926492
At time: 204.38836765289307 and batch: 600, loss is 3.3836771392822267 and perplexity is 29.478970339395076
At time: 205.2715859413147 and batch: 650, loss is 3.2170625495910645 and perplexity is 24.95470919283161
At time: 206.15513157844543 and batch: 700, loss is 3.2092145538330077 and perplexity is 24.759631227717573
At time: 207.03946995735168 and batch: 750, loss is 3.298634524345398 and perplexity is 27.075642536858794
At time: 207.92343878746033 and batch: 800, loss is 3.2598530054092407 and perplexity is 26.04570828288433
At time: 208.81910276412964 and batch: 850, loss is 3.3023443841934204 and perplexity is 27.176275928436493
At time: 209.72598314285278 and batch: 900, loss is 3.2677915143966674 and perplexity is 26.253295247488893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316228997217466 and perplexity of 74.90562570266378
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 211.83161187171936 and batch: 50, loss is 3.610650405883789 and perplexity is 36.990103573603385
At time: 212.72077465057373 and batch: 100, loss is 3.4963757944107057 and perplexity is 32.995651973629045
At time: 213.6042263507843 and batch: 150, loss is 3.511167869567871 and perplexity is 33.48735381723967
At time: 214.4854016304016 and batch: 200, loss is 3.391245698928833 and perplexity is 29.70293014220078
At time: 215.36717748641968 and batch: 250, loss is 3.5367646551132204 and perplexity is 34.35558701619492
At time: 216.2491102218628 and batch: 300, loss is 3.486205987930298 and perplexity is 32.661793095891774
At time: 217.13209533691406 and batch: 350, loss is 3.473093934059143 and perplexity is 32.236325378831076
At time: 218.015860080719 and batch: 400, loss is 3.4156303548812867 and perplexity is 30.43612894442667
At time: 218.89953207969666 and batch: 450, loss is 3.4332768011093138 and perplexity is 30.977985315927985
At time: 219.78229999542236 and batch: 500, loss is 3.307523365020752 and perplexity is 27.317386429313938
At time: 220.66548132896423 and batch: 550, loss is 3.341304759979248 and perplexity is 28.255969923134337
At time: 221.5481641292572 and batch: 600, loss is 3.378466110229492 and perplexity is 29.325754122304218
At time: 222.42996954917908 and batch: 650, loss is 3.210934977531433 and perplexity is 24.802264947561184
At time: 223.31199836730957 and batch: 700, loss is 3.201825432777405 and perplexity is 24.577353580433662
At time: 224.1938624382019 and batch: 750, loss is 3.2911438083648683 and perplexity is 26.87358431380863
At time: 225.0768904685974 and batch: 800, loss is 3.2502656984329223 and perplexity is 25.797193280517224
At time: 225.96040773391724 and batch: 850, loss is 3.29208993434906 and perplexity is 26.899022142013855
At time: 226.84194493293762 and batch: 900, loss is 3.2557413673400877 and perplexity is 25.93883761448377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315539216342038 and perplexity of 74.85397505045611
finished 13 epochs...
Completing Train Step...
At time: 228.9580066204071 and batch: 50, loss is 3.607141098976135 and perplexity is 36.86052145232053
At time: 229.83790612220764 and batch: 100, loss is 3.492377185821533 and perplexity is 32.863978706612365
At time: 230.7185082435608 and batch: 150, loss is 3.5072735595703124 and perplexity is 33.35719727992201
At time: 231.59983563423157 and batch: 200, loss is 3.3868996286392212 and perplexity is 29.574119233379065
At time: 232.48175072669983 and batch: 250, loss is 3.5323202323913576 and perplexity is 34.20323507363738
At time: 233.36366295814514 and batch: 300, loss is 3.4824271869659422 and perplexity is 32.53860358155988
At time: 234.25022864341736 and batch: 350, loss is 3.4691675233840944 and perplexity is 32.1100004907266
At time: 235.12779259681702 and batch: 400, loss is 3.4120985221862794 and perplexity is 30.32882323356313
At time: 236.00899457931519 and batch: 450, loss is 3.430249614715576 and perplexity is 30.884350976078295
At time: 236.89189672470093 and batch: 500, loss is 3.304761004447937 and perplexity is 27.242030086702187
At time: 237.77249217033386 and batch: 550, loss is 3.3393631553649903 and perplexity is 28.201161227178016
At time: 238.65204429626465 and batch: 600, loss is 3.3770940923690795 and perplexity is 29.285546253143043
At time: 239.53511500358582 and batch: 650, loss is 3.2096890354156495 and perplexity is 24.7713820042712
At time: 240.4171416759491 and batch: 700, loss is 3.200969204902649 and perplexity is 24.556318771790206
At time: 241.29677534103394 and batch: 750, loss is 3.2907907867431643 and perplexity is 26.86409903184649
At time: 242.17798233032227 and batch: 800, loss is 3.2504467725753785 and perplexity is 25.801864908110986
At time: 243.06042528152466 and batch: 850, loss is 3.29318998336792 and perplexity is 26.928628666257847
At time: 243.94016981124878 and batch: 900, loss is 3.257645721435547 and perplexity is 25.98828141043143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315354438677226 and perplexity of 74.84014498552733
finished 14 epochs...
Completing Train Step...
At time: 246.03997707366943 and batch: 50, loss is 3.6049986982345583 and perplexity is 36.781635976150405
At time: 246.9253168106079 and batch: 100, loss is 3.4898606443405153 and perplexity is 32.78137911720422
At time: 247.80693340301514 and batch: 150, loss is 3.5044836521148683 and perplexity is 33.264263485161294
At time: 248.68939638137817 and batch: 200, loss is 3.3840216207504272 and perplexity is 29.48912704767708
At time: 249.5721311569214 and batch: 250, loss is 3.5294732666015625 and perplexity is 34.10599811430796
At time: 250.4547529220581 and batch: 300, loss is 3.4797556734085084 and perplexity is 32.451792271378736
At time: 251.33750796318054 and batch: 350, loss is 3.4664770364761353 and perplexity is 32.023725068282644
At time: 252.21985292434692 and batch: 400, loss is 3.40982150554657 and perplexity is 30.259842563268812
At time: 253.10458111763 and batch: 450, loss is 3.4281782960891722 and perplexity is 30.82044585142266
At time: 253.98662543296814 and batch: 500, loss is 3.3028211259841918 and perplexity is 27.189235083729116
At time: 254.86954855918884 and batch: 550, loss is 3.337705578804016 and perplexity is 28.15445436413046
At time: 255.75442719459534 and batch: 600, loss is 3.3758937740325927 and perplexity is 29.25041536332492
At time: 256.64864206314087 and batch: 650, loss is 3.2086976099014284 and perplexity is 24.74683519430713
At time: 257.5323648452759 and batch: 700, loss is 3.200347933769226 and perplexity is 24.541067377910107
At time: 258.4256935119629 and batch: 750, loss is 3.2904830265045164 and perplexity is 26.855832602421785
At time: 259.3084304332733 and batch: 800, loss is 3.250411043167114 and perplexity is 25.80094303921471
At time: 260.1971914768219 and batch: 850, loss is 3.2937567949295046 and perplexity is 26.943896450893163
At time: 261.08801221847534 and batch: 900, loss is 3.2585792255401613 and perplexity is 26.012552904807176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315390808941567 and perplexity of 74.84286699088346
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 263.22002506256104 and batch: 50, loss is 3.6038983488082885 and perplexity is 36.74118558297159
At time: 264.11104226112366 and batch: 100, loss is 3.489426164627075 and perplexity is 32.767139366664566
At time: 264.9954891204834 and batch: 150, loss is 3.504183502197266 and perplexity is 33.254280717455515
At time: 265.87972474098206 and batch: 200, loss is 3.3838502025604247 and perplexity is 29.4840725081262
At time: 266.7657861709595 and batch: 250, loss is 3.529784369468689 and perplexity is 34.116610238753104
At time: 267.6508288383484 and batch: 300, loss is 3.479463324546814 and perplexity is 32.44230641350568
At time: 268.5345268249512 and batch: 350, loss is 3.4653380155563354 and perplexity is 31.987270140903696
At time: 269.4192044734955 and batch: 400, loss is 3.40979989528656 and perplexity is 30.259188647268836
At time: 270.3048620223999 and batch: 450, loss is 3.4273627853393553 and perplexity is 30.795321692419968
At time: 271.1849446296692 and batch: 500, loss is 3.302016954421997 and perplexity is 27.16737906324986
At time: 272.0706059932709 and batch: 550, loss is 3.3350481605529785 and perplexity is 28.079735526827424
At time: 272.9654223918915 and batch: 600, loss is 3.3739869737625123 and perplexity is 29.19469380523428
At time: 273.8641393184662 and batch: 650, loss is 3.206542143821716 and perplexity is 24.69355167650294
At time: 274.7582085132599 and batch: 700, loss is 3.197994041442871 and perplexity is 24.48336828309857
At time: 275.65171337127686 and batch: 750, loss is 3.288106846809387 and perplexity is 26.792094075383183
At time: 276.5386760234833 and batch: 800, loss is 3.2473543214797975 and perplexity is 25.722197150520227
At time: 277.42642402648926 and batch: 850, loss is 3.2909665489196778 and perplexity is 26.86882113933419
At time: 278.3205451965332 and batch: 900, loss is 3.254422926902771 and perplexity is 25.90466133671982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3147035363602315 and perplexity of 74.7914472122181
finished 16 epochs...
Completing Train Step...
At time: 280.4239056110382 and batch: 50, loss is 3.602931056022644 and perplexity is 36.7056632822235
At time: 281.30532670021057 and batch: 100, loss is 3.488359475135803 and perplexity is 32.73220563847586
At time: 282.18773221969604 and batch: 150, loss is 3.5032870721817018 and perplexity is 33.224483939432844
At time: 283.06527757644653 and batch: 200, loss is 3.382809066772461 and perplexity is 29.453391559351378
At time: 283.94365406036377 and batch: 250, loss is 3.5288424110412597 and perplexity is 34.08448894106199
At time: 284.8201208114624 and batch: 300, loss is 3.4784423446655275 and perplexity is 32.40920037453065
At time: 285.69572710990906 and batch: 350, loss is 3.464585094451904 and perplexity is 31.963195314501224
At time: 286.5727074146271 and batch: 400, loss is 3.408733024597168 and perplexity is 30.2269232203958
At time: 287.45156931877136 and batch: 450, loss is 3.42658362865448 and perplexity is 30.771336656924216
At time: 288.3336455821991 and batch: 500, loss is 3.301282510757446 and perplexity is 27.147433479164906
At time: 289.2146873474121 and batch: 550, loss is 3.334726915359497 and perplexity is 28.07071649549299
At time: 290.1025733947754 and batch: 600, loss is 3.3736990308761596 and perplexity is 29.18628861099984
At time: 290.9932780265808 and batch: 650, loss is 3.2063693237304687 and perplexity is 24.68928450338623
At time: 291.87598967552185 and batch: 700, loss is 3.1978888654708864 and perplexity is 24.48079335645452
At time: 292.7599050998688 and batch: 750, loss is 3.288052825927734 and perplexity is 26.790646781932292
At time: 293.6420474052429 and batch: 800, loss is 3.2474691009521486 and perplexity is 25.725149700179678
At time: 294.5246558189392 and batch: 850, loss is 3.291328663825989 and perplexity is 26.878552501813118
At time: 295.42057037353516 and batch: 900, loss is 3.254984426498413 and perplexity is 25.91921087798413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314420935225813 and perplexity of 74.77031405065951
finished 17 epochs...
Completing Train Step...
At time: 297.5190951824188 and batch: 50, loss is 3.6022386026382445 and perplexity is 36.68025511945649
At time: 298.40569829940796 and batch: 100, loss is 3.48758348941803 and perplexity is 32.70681576675216
At time: 299.28600549697876 and batch: 150, loss is 3.5025501108169554 and perplexity is 33.20000779849873
At time: 300.1708724498749 and batch: 200, loss is 3.3820035791397096 and perplexity is 29.429676768975124
At time: 301.0497622489929 and batch: 250, loss is 3.5280510234832763 and perplexity is 34.05752557123876
At time: 301.93099570274353 and batch: 300, loss is 3.4776773977279665 and perplexity is 32.38441853555968
At time: 302.8105790615082 and batch: 350, loss is 3.46391441822052 and perplexity is 31.941765546146108
At time: 303.6954777240753 and batch: 400, loss is 3.407991428375244 and perplexity is 30.20451535815512
At time: 304.5810024738312 and batch: 450, loss is 3.4259745979309084 and perplexity is 30.75260167316424
At time: 305.46626472473145 and batch: 500, loss is 3.3007226896286013 and perplexity is 27.132240025514346
At time: 306.3493890762329 and batch: 550, loss is 3.3343819522857667 and perplexity is 28.061034804857787
At time: 307.23317670822144 and batch: 600, loss is 3.373435688018799 and perplexity is 29.178603622299118
At time: 308.1177616119385 and batch: 650, loss is 3.206166834831238 and perplexity is 24.68428570346217
At time: 309.01038360595703 and batch: 700, loss is 3.197774124145508 and perplexity is 24.477984558924142
At time: 309.8988153934479 and batch: 750, loss is 3.2880207014083864 and perplexity is 26.789786159105027
At time: 310.7818043231964 and batch: 800, loss is 3.247530446052551 and perplexity is 25.72672786047661
At time: 311.6636643409729 and batch: 850, loss is 3.2915921449661254 and perplexity is 26.885635426539082
At time: 312.5445885658264 and batch: 900, loss is 3.255369687080383 and perplexity is 25.929198452036957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314292594178082 and perplexity of 74.76071856597548
finished 18 epochs...
Completing Train Step...
At time: 314.6605906486511 and batch: 50, loss is 3.601649146080017 and perplexity is 36.65864007370997
At time: 315.5541410446167 and batch: 100, loss is 3.48692672252655 and perplexity is 32.68534206541074
At time: 316.43832564353943 and batch: 150, loss is 3.5018782806396485 and perplexity is 33.177710522203085
At time: 317.32004046440125 and batch: 200, loss is 3.3812984323501585 and perplexity is 29.40893184185292
At time: 318.2022113800049 and batch: 250, loss is 3.527342104911804 and perplexity is 34.03339011491034
At time: 319.08552050590515 and batch: 300, loss is 3.477011117935181 and perplexity is 32.36284863847956
At time: 319.96748328208923 and batch: 350, loss is 3.4632857179641725 and perplexity is 31.921690061351537
At time: 320.84962582588196 and batch: 400, loss is 3.4073834085464476 and perplexity is 30.186155995891927
At time: 321.7314600944519 and batch: 450, loss is 3.425439882278442 and perplexity is 30.736162171316717
At time: 322.6271650791168 and batch: 500, loss is 3.3002343940734864 and perplexity is 27.11899470738774
At time: 323.5097961425781 and batch: 550, loss is 3.3340209245681764 and perplexity is 28.05090582204076
At time: 324.39050793647766 and batch: 600, loss is 3.3731720781326295 and perplexity is 29.170912867643676
At time: 325.27396631240845 and batch: 650, loss is 3.2059508895874025 and perplexity is 24.67895582486871
At time: 326.1553418636322 and batch: 700, loss is 3.1976473903656006 and perplexity is 24.474882567984146
At time: 327.03765201568604 and batch: 750, loss is 3.2879793310165404 and perplexity is 26.788677878079323
At time: 327.9186894893646 and batch: 800, loss is 3.2475565099716186 and perplexity is 25.727398408567954
At time: 328.8001801967621 and batch: 850, loss is 3.291788573265076 and perplexity is 26.890917044884787
At time: 329.68333625793457 and batch: 900, loss is 3.255651650428772 and perplexity is 25.93651056647922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3142374117080475 and perplexity of 74.7565931986885
finished 19 epochs...
Completing Train Step...
At time: 331.80883264541626 and batch: 50, loss is 3.601110143661499 and perplexity is 36.63888630219421
At time: 332.69257712364197 and batch: 100, loss is 3.486330552101135 and perplexity is 32.66586183847133
At time: 333.57659244537354 and batch: 150, loss is 3.501245231628418 and perplexity is 33.15671405196068
At time: 334.4594638347626 and batch: 200, loss is 3.380649333000183 and perplexity is 29.38984871740276
At time: 335.3432857990265 and batch: 250, loss is 3.526688051223755 and perplexity is 34.01113772850984
At time: 336.22492575645447 and batch: 300, loss is 3.4763944816589354 and perplexity is 32.34289868357153
At time: 337.10898327827454 and batch: 350, loss is 3.4626873207092284 and perplexity is 31.90259392375363
At time: 337.9926509857178 and batch: 400, loss is 3.406839618682861 and perplexity is 30.169745532566917
At time: 338.87476229667664 and batch: 450, loss is 3.424945840835571 and perplexity is 30.720980983785957
At time: 339.75759530067444 and batch: 500, loss is 3.299783263206482 and perplexity is 27.106763250994316
At time: 340.6418068408966 and batch: 550, loss is 3.333655605316162 and perplexity is 28.040660157690688
At time: 341.5250084400177 and batch: 600, loss is 3.3729066133499144 and perplexity is 29.163170045366453
At time: 342.4076678752899 and batch: 650, loss is 3.2057298278808593 and perplexity is 24.673500855742965
At time: 343.2895746231079 and batch: 700, loss is 3.1975128841400147 and perplexity is 24.471590765297208
At time: 344.17879009246826 and batch: 750, loss is 3.2879251432418823 and perplexity is 26.787226298568363
At time: 345.08344435691833 and batch: 800, loss is 3.2475590991973875 and perplexity is 25.72746502269712
At time: 345.97746086120605 and batch: 850, loss is 3.2919384145736696 and perplexity is 26.894946716982215
At time: 346.8617057800293 and batch: 900, loss is 3.2558684015274046 and perplexity is 25.942132942945328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314221107796447 and perplexity of 74.75537438373719
Finished Training.
Improved accuracyfrom -10000000 to -74.75537438373719
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
359.094429731369


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.420628309249878 and batch: 50, loss is 7.546692132949829 and perplexity is 1894.4657145797696
At time: 2.5320897102355957 and batch: 100, loss is 6.8571328449249265 and perplexity is 950.6375311370873
At time: 3.645794630050659 and batch: 150, loss is 6.701417169570923 and perplexity is 813.557958173304
At time: 4.75562858581543 and batch: 200, loss is 6.453485956192017 and perplexity is 634.911714022974
At time: 5.867387533187866 and batch: 250, loss is 6.3877661991119385 and perplexity is 594.527040247196
At time: 6.987650394439697 and batch: 300, loss is 6.22923511505127 and perplexity is 507.36725746188523
At time: 8.102161645889282 and batch: 350, loss is 6.21688627243042 and perplexity is 501.14038550920344
At time: 9.217729568481445 and batch: 400, loss is 6.097585783004761 and perplexity is 444.78267095999047
At time: 10.332177877426147 and batch: 450, loss is 6.08159990310669 and perplexity is 437.72895871276484
At time: 11.45458722114563 and batch: 500, loss is 6.052145338058471 and perplexity is 425.0238725524172
At time: 12.580081939697266 and batch: 550, loss is 6.072640962600708 and perplexity is 433.824885307158
At time: 13.698826789855957 and batch: 600, loss is 6.011565551757813 and perplexity is 408.1217561240214
At time: 14.816800594329834 and batch: 650, loss is 5.940130319595337 and perplexity is 379.9844457308086
At time: 15.933209896087646 and batch: 700, loss is 6.0451103210449215 and perplexity is 422.0443152844269
At time: 17.048848152160645 and batch: 750, loss is 5.995342741012573 and perplexity is 401.5542895318379
At time: 18.163211584091187 and batch: 800, loss is 6.003762788772583 and perplexity is 404.9496704022519
At time: 19.2810640335083 and batch: 850, loss is 6.038039121627808 and perplexity is 419.0704824421431
At time: 20.405628442764282 and batch: 900, loss is 5.916074132919311 and perplexity is 370.9525411214705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.010625133775685 and perplexity of 407.7381314976859
finished 1 epochs...
Completing Train Step...
At time: 22.744487524032593 and batch: 50, loss is 5.7601574039459225 and perplexity is 317.3982847285718
At time: 23.643593788146973 and batch: 100, loss is 5.539976615905761 and perplexity is 254.67204411384523
At time: 24.54998779296875 and batch: 150, loss is 5.425741891860962 and perplexity is 227.17982667483142
At time: 25.438831090927124 and batch: 200, loss is 5.227435531616211 and perplexity is 186.3143929777654
At time: 26.3553409576416 and batch: 250, loss is 5.250147132873535 and perplexity is 190.59430908410556
At time: 27.248512029647827 and batch: 300, loss is 5.14293794631958 and perplexity is 171.21805957563268
At time: 28.138624668121338 and batch: 350, loss is 5.099466733932495 and perplexity is 163.93446329994245
At time: 29.029393911361694 and batch: 400, loss is 4.935031366348267 and perplexity is 139.0775048171158
At time: 29.916166305541992 and batch: 450, loss is 4.928522253036499 and perplexity is 138.1751734535592
At time: 30.8042209148407 and batch: 500, loss is 4.8399633693695066 and perplexity is 126.46471916287027
At time: 31.69597363471985 and batch: 550, loss is 4.890598983764648 and perplexity is 133.03323493899893
At time: 32.581016063690186 and batch: 600, loss is 4.799955043792725 and perplexity is 121.50495499400692
At time: 33.46693277359009 and batch: 650, loss is 4.672694435119629 and perplexity is 106.98562023671336
At time: 34.34894156455994 and batch: 700, loss is 4.732532005310059 and perplexity is 113.58279079265249
At time: 35.23192882537842 and batch: 750, loss is 4.7317854690551755 and perplexity is 113.49802876429514
At time: 36.12213850021362 and batch: 800, loss is 4.680962171554565 and perplexity is 107.87381577103639
At time: 37.00944995880127 and batch: 850, loss is 4.711823606491089 and perplexity is 111.25486011188458
At time: 37.89843130111694 and batch: 900, loss is 4.6390162944793705 and perplexity is 103.44254051922856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.72724851843429 and perplexity of 112.9842601648874
finished 2 epochs...
Completing Train Step...
At time: 40.03789758682251 and batch: 50, loss is 4.694126815795898 and perplexity is 109.30332502361958
At time: 40.93113112449646 and batch: 100, loss is 4.56900089263916 and perplexity is 96.44770001199552
At time: 41.816970348358154 and batch: 150, loss is 4.56819640159607 and perplexity is 96.37013990359858
At time: 42.71394944190979 and batch: 200, loss is 4.455029935836792 and perplexity is 86.05872704658518
At time: 43.602680683135986 and batch: 250, loss is 4.589684295654297 and perplexity is 98.46333995023232
At time: 44.492257833480835 and batch: 300, loss is 4.530000810623169 and perplexity is 92.75863627438093
At time: 45.38031721115112 and batch: 350, loss is 4.526958570480347 and perplexity is 92.47687104357829
At time: 46.27207636833191 and batch: 400, loss is 4.417985863685608 and perplexity is 82.92908654953231
At time: 47.158111572265625 and batch: 450, loss is 4.450147171020507 and perplexity is 85.63954673428803
At time: 48.04412817955017 and batch: 500, loss is 4.338829221725464 and perplexity is 76.61778436958134
At time: 48.958284854888916 and batch: 550, loss is 4.401971635818481 and perplexity is 81.61161853309906
At time: 49.84222865104675 and batch: 600, loss is 4.376034860610962 and perplexity is 79.52209126534359
At time: 50.72648072242737 and batch: 650, loss is 4.2329587507247926 and perplexity is 68.92085041361084
At time: 51.609753131866455 and batch: 700, loss is 4.269104695320129 and perplexity is 71.45763062040018
At time: 52.493772745132446 and batch: 750, loss is 4.334886984825134 and perplexity is 76.3163334986014
At time: 53.37570405006409 and batch: 800, loss is 4.288069052696228 and perplexity is 72.82571003634075
At time: 54.259113788604736 and batch: 850, loss is 4.350781626701355 and perplexity is 77.53904583418846
At time: 55.14494848251343 and batch: 900, loss is 4.281234979629517 and perplexity is 72.3297105914921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.486534223164598 and perplexity of 88.81310546416647
finished 3 epochs...
Completing Train Step...
At time: 57.269078731536865 and batch: 50, loss is 4.371901235580444 and perplexity is 79.1940552145173
At time: 58.19085192680359 and batch: 100, loss is 4.248593373298645 and perplexity is 70.00686952450202
At time: 59.077126026153564 and batch: 150, loss is 4.249692988395691 and perplexity is 70.08389247516673
At time: 59.97670316696167 and batch: 200, loss is 4.132854495048523 and perplexity is 62.35566306638071
At time: 60.865782022476196 and batch: 250, loss is 4.284344072341919 and perplexity is 72.55494031619904
At time: 61.752058029174805 and batch: 300, loss is 4.238372464179992 and perplexity is 69.29497995013399
At time: 62.63734412193298 and batch: 350, loss is 4.241706342697143 and perplexity is 69.52638652149926
At time: 63.521952629089355 and batch: 400, loss is 4.151365647315979 and perplexity is 63.52068794927867
At time: 64.40438961982727 and batch: 450, loss is 4.188488020896911 and perplexity is 65.92304130884752
At time: 65.28710103034973 and batch: 500, loss is 4.067585668563843 and perplexity is 58.41575720319579
At time: 66.17337346076965 and batch: 550, loss is 4.13655535697937 and perplexity is 62.586860316666474
At time: 67.06940269470215 and batch: 600, loss is 4.132590985298156 and perplexity is 62.33923390588919
At time: 67.96145176887512 and batch: 650, loss is 3.976501669883728 and perplexity is 53.330141072500595
At time: 68.85337519645691 and batch: 700, loss is 3.99932101726532 and perplexity is 54.561091414431125
At time: 69.74610996246338 and batch: 750, loss is 4.089827332496643 and perplexity is 59.72957745668715
At time: 70.64893674850464 and batch: 800, loss is 4.050902681350708 and perplexity is 57.44929205122596
At time: 71.53960466384888 and batch: 850, loss is 4.11863112449646 and perplexity is 61.47503297484251
At time: 72.4385712146759 and batch: 900, loss is 4.060023574829102 and perplexity is 57.97567782954567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384519812178938 and perplexity of 80.19970304452863
finished 4 epochs...
Completing Train Step...
At time: 74.5913097858429 and batch: 50, loss is 4.15523163318634 and perplexity is 63.766733328900465
At time: 75.49051475524902 and batch: 100, loss is 4.038220934867859 and perplexity is 56.7253349168445
At time: 76.37942600250244 and batch: 150, loss is 4.043912243843079 and perplexity is 57.04909676502165
At time: 77.26255869865417 and batch: 200, loss is 3.9223481750488283 and perplexity is 50.518932905931344
At time: 78.16526412963867 and batch: 250, loss is 4.079123477935791 and perplexity is 59.093650254387974
At time: 79.06249117851257 and batch: 300, loss is 4.039244513511658 and perplexity is 56.78342748431144
At time: 79.971355676651 and batch: 350, loss is 4.0391557264328 and perplexity is 56.77838607346695
At time: 80.86278367042542 and batch: 400, loss is 3.9595174741744996 and perplexity is 52.432020040460145
At time: 81.75803661346436 and batch: 450, loss is 4.00162675857544 and perplexity is 54.68704032383227
At time: 82.64970469474792 and batch: 500, loss is 3.878911352157593 and perplexity is 48.3715268387565
At time: 83.54305720329285 and batch: 550, loss is 3.9504277992248533 and perplexity is 51.95758949758588
At time: 84.43707180023193 and batch: 600, loss is 3.9544016218185423 and perplexity is 52.164470522648095
At time: 85.32924890518188 and batch: 650, loss is 3.7975039958953856 and perplexity is 44.58974928254991
At time: 86.21933841705322 and batch: 700, loss is 3.8169797897338866 and perplexity is 45.46668180673431
At time: 87.11804175376892 and batch: 750, loss is 3.914595952033997 and perplexity is 50.12881297368185
At time: 88.0142810344696 and batch: 800, loss is 3.8760423707962035 and perplexity is 48.23294871391645
At time: 88.90300226211548 and batch: 850, loss is 3.947916479110718 and perplexity is 51.827271062142934
At time: 89.7936143875122 and batch: 900, loss is 3.893956980705261 and perplexity is 49.10480937945067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344407173052226 and perplexity of 77.04634878610797
finished 5 epochs...
Completing Train Step...
At time: 91.95801734924316 and batch: 50, loss is 3.9888023853302004 and perplexity is 53.990191184327756
At time: 92.87251257896423 and batch: 100, loss is 3.8793976211547854 and perplexity is 48.395054132438204
At time: 93.76169037818909 and batch: 150, loss is 3.884069895744324 and perplexity is 48.621698173411325
At time: 94.6578016281128 and batch: 200, loss is 3.762613821029663 and perplexity is 43.06083231880774
At time: 95.54848742485046 and batch: 250, loss is 3.9222645807266234 and perplexity is 50.514709986485066
At time: 96.43735027313232 and batch: 300, loss is 3.880213794708252 and perplexity is 48.434569019048865
At time: 97.33293294906616 and batch: 350, loss is 3.8827372455596922 and perplexity is 48.55694561420157
At time: 98.227219581604 and batch: 400, loss is 3.8090598440170287 and perplexity is 45.1080103589175
At time: 99.12452578544617 and batch: 450, loss is 3.852489056587219 and perplexity is 47.11017731649764
At time: 100.00105381011963 and batch: 500, loss is 3.7318381595611574 and perplexity is 41.75579147168744
At time: 100.87598037719727 and batch: 550, loss is 3.79863461971283 and perplexity is 44.640192025612535
At time: 101.7517921924591 and batch: 600, loss is 3.8125529861450196 and perplexity is 45.265854575852224
At time: 102.6285150051117 and batch: 650, loss is 3.6547949504852295 and perplexity is 38.65959317155762
At time: 103.5034396648407 and batch: 700, loss is 3.6699874877929686 and perplexity is 39.25141473540453
At time: 104.37923049926758 and batch: 750, loss is 3.7716729259490966 and perplexity is 43.45269720934061
At time: 105.2542040348053 and batch: 800, loss is 3.734559588432312 and perplexity is 41.869581653850744
At time: 106.12913870811462 and batch: 850, loss is 3.806301350593567 and perplexity is 44.98375167117677
At time: 107.00513219833374 and batch: 900, loss is 3.756900601387024 and perplexity is 42.81551776102383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338391447720462 and perplexity of 76.58425043594126
finished 6 epochs...
Completing Train Step...
At time: 109.08192229270935 and batch: 50, loss is 3.853589835166931 and perplexity is 47.16206374306826
At time: 109.96704053878784 and batch: 100, loss is 3.745543293952942 and perplexity is 42.33199969188502
At time: 110.84310150146484 and batch: 150, loss is 3.7498173570632933 and perplexity is 42.5133165339497
At time: 111.71960735321045 and batch: 200, loss is 3.6298896360397337 and perplexity is 37.708654711053526
At time: 112.59670853614807 and batch: 250, loss is 3.786222290992737 and perplexity is 44.08952787018113
At time: 113.47340273857117 and batch: 300, loss is 3.749902229309082 and perplexity is 42.51692488772224
At time: 114.35177946090698 and batch: 350, loss is 3.7524510431289673 and perplexity is 42.62543083544267
At time: 115.24750781059265 and batch: 400, loss is 3.68280490398407 and perplexity is 39.75775450591178
At time: 116.12397527694702 and batch: 450, loss is 3.726100263595581 and perplexity is 41.51688714381466
At time: 117.00167894363403 and batch: 500, loss is 3.6108642482757567 and perplexity is 36.99801447164323
At time: 117.88012552261353 and batch: 550, loss is 3.670807514190674 and perplexity is 39.28361513241822
At time: 118.75768256187439 and batch: 600, loss is 3.6934531927108765 and perplexity is 40.183368564158854
At time: 119.63508439064026 and batch: 650, loss is 3.536540231704712 and perplexity is 34.34787768336517
At time: 120.51235365867615 and batch: 700, loss is 3.546364998817444 and perplexity is 34.687000757185785
At time: 121.39122772216797 and batch: 750, loss is 3.6496209478378296 and perplexity is 38.460084907358684
At time: 122.26883506774902 and batch: 800, loss is 3.6139013051986693 and perplexity is 37.11055035006278
At time: 123.14702248573303 and batch: 850, loss is 3.685122742652893 and perplexity is 39.850013446056536
At time: 124.06634020805359 and batch: 900, loss is 3.639231004714966 and perplexity is 38.06255554254763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347339368846319 and perplexity of 77.2725953034107
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 126.17236638069153 and batch: 50, loss is 3.767163248062134 and perplexity is 43.25718073113054
At time: 127.05435943603516 and batch: 100, loss is 3.668154616355896 and perplexity is 39.17953782915759
At time: 127.94627666473389 and batch: 150, loss is 3.6733209037780763 and perplexity is 39.38247434546372
At time: 128.83634972572327 and batch: 200, loss is 3.543489933013916 and perplexity is 34.58741657162202
At time: 129.72348189353943 and batch: 250, loss is 3.692515091896057 and perplexity is 40.14569018918695
At time: 130.6066882610321 and batch: 300, loss is 3.6453020477294924 and perplexity is 38.294337822675175
At time: 131.49699020385742 and batch: 350, loss is 3.632701344490051 and perplexity is 37.81482965069323
At time: 132.3784658908844 and batch: 400, loss is 3.561721558570862 and perplexity is 35.22378478615309
At time: 133.2573709487915 and batch: 450, loss is 3.58774450302124 and perplexity is 36.15244217464417
At time: 134.13646817207336 and batch: 500, loss is 3.4650569438934324 and perplexity is 31.978280689092735
At time: 135.0164132118225 and batch: 550, loss is 3.5051367473602295 and perplexity is 33.285995313188224
At time: 135.89496064186096 and batch: 600, loss is 3.5258024883270265 and perplexity is 33.98103205906133
At time: 136.78490495681763 and batch: 650, loss is 3.3538758611679076 and perplexity is 28.613420647441625
At time: 137.67201209068298 and batch: 700, loss is 3.3442835235595703 and perplexity is 28.340263259900745
At time: 138.56169199943542 and batch: 750, loss is 3.4361528635025023 and perplexity is 31.067208178269055
At time: 139.44440245628357 and batch: 800, loss is 3.378791818618774 and perplexity is 29.335307322138085
At time: 140.32698154449463 and batch: 850, loss is 3.4363819646835325 and perplexity is 31.07432652773401
At time: 141.2211766242981 and batch: 900, loss is 3.3831857681274413 and perplexity is 29.464488781898712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3263917687821065 and perplexity of 75.67075580146093
finished 8 epochs...
Completing Train Step...
At time: 143.3518512248993 and batch: 50, loss is 3.675215859413147 and perplexity is 39.45717314024602
At time: 144.24008059501648 and batch: 100, loss is 3.566841368675232 and perplexity is 35.40458631522561
At time: 145.12218046188354 and batch: 150, loss is 3.5671197605133056 and perplexity is 35.414444035176864
At time: 146.00190258026123 and batch: 200, loss is 3.4409031820297242 and perplexity is 31.215138392509246
At time: 146.88314938545227 and batch: 250, loss is 3.5906991863250735 and perplexity is 36.25941915565313
At time: 147.76546144485474 and batch: 300, loss is 3.5470018196105957 and perplexity is 34.70909719550897
At time: 148.6476058959961 and batch: 350, loss is 3.539094080924988 and perplexity is 34.43570909035267
At time: 149.53013348579407 and batch: 400, loss is 3.4731157779693604 and perplexity is 32.237029553919335
At time: 150.4116666316986 and batch: 450, loss is 3.504754734039307 and perplexity is 33.27328204805134
At time: 151.29335236549377 and batch: 500, loss is 3.385150184631348 and perplexity is 29.522426197917778
At time: 152.1759729385376 and batch: 550, loss is 3.4269516038894654 and perplexity is 30.782661830325246
At time: 153.05610918998718 and batch: 600, loss is 3.4563648653030397 and perplexity is 31.701527482907952
At time: 153.94925165176392 and batch: 650, loss is 3.2893814134597776 and perplexity is 26.826264156382805
At time: 154.83036613464355 and batch: 700, loss is 3.2849748086929322 and perplexity is 26.70831148913615
At time: 155.71197843551636 and batch: 750, loss is 3.383263392448425 and perplexity is 29.4667760316055
At time: 156.5957474708557 and batch: 800, loss is 3.330576415061951 and perplexity is 27.954450426409498
At time: 157.47877979278564 and batch: 850, loss is 3.397736463546753 and perplexity is 29.896351918756576
At time: 158.36165857315063 and batch: 900, loss is 3.3510062980651854 and perplexity is 28.531430325771677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3403299410049225 and perplexity of 76.73285247660152
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.4722878932953 and batch: 50, loss is 3.6403320741653444 and perplexity is 38.10448814077388
At time: 161.3590226173401 and batch: 100, loss is 3.547766170501709 and perplexity is 34.735637266551436
At time: 162.24061393737793 and batch: 150, loss is 3.5490194177627563 and perplexity is 34.77919689860993
At time: 163.12245750427246 and batch: 200, loss is 3.4207228326797487 and perplexity is 30.59151958121501
At time: 164.00168085098267 and batch: 250, loss is 3.5729111862182616 and perplexity is 35.620139215939346
At time: 164.8825340270996 and batch: 300, loss is 3.525205202102661 and perplexity is 33.96074171689687
At time: 165.76241207122803 and batch: 350, loss is 3.510327863693237 and perplexity is 33.459236054503535
At time: 166.6509997844696 and batch: 400, loss is 3.4423108100891113 and perplexity is 31.259108636802736
At time: 167.53670597076416 and batch: 450, loss is 3.4695417881011963 and perplexity is 32.12202038014635
At time: 168.42173528671265 and batch: 500, loss is 3.3454295587539673 and perplexity is 28.372760817130377
At time: 169.3035523891449 and batch: 550, loss is 3.3803036069869994 and perplexity is 29.379689638407754
At time: 170.18417477607727 and batch: 600, loss is 3.411761775016785 and perplexity is 30.318611807616016
At time: 171.0631537437439 and batch: 650, loss is 3.2389251470565794 and perplexity is 25.5062914957591
At time: 171.94161772727966 and batch: 700, loss is 3.2225549697875975 and perplexity is 25.09214803202306
At time: 172.81976103782654 and batch: 750, loss is 3.3169825744628905 and perplexity is 27.577013305584813
At time: 173.6975381374359 and batch: 800, loss is 3.258243885040283 and perplexity is 26.00383130474756
At time: 174.58639550209045 and batch: 850, loss is 3.3235207319259645 and perplexity is 27.757906871852942
At time: 175.46568202972412 and batch: 900, loss is 3.2800505924224854 and perplexity is 26.577117266606425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341133849261558 and perplexity of 76.79456345192047
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 177.56221079826355 and batch: 50, loss is 3.630519642829895 and perplexity is 37.73241890458497
At time: 178.44312357902527 and batch: 100, loss is 3.534354815483093 and perplexity is 34.27289523806719
At time: 179.32868814468384 and batch: 150, loss is 3.534058618545532 and perplexity is 34.26274521473345
At time: 180.23731136322021 and batch: 200, loss is 3.404351396560669 and perplexity is 30.09476982110659
At time: 181.13467454910278 and batch: 250, loss is 3.5525156593322755 and perplexity is 34.90100618582807
At time: 182.0159628391266 and batch: 300, loss is 3.509687595367432 and perplexity is 33.43782002218738
At time: 182.89821934700012 and batch: 350, loss is 3.4951108264923096 and perplexity is 32.95393992020265
At time: 183.7787148952484 and batch: 400, loss is 3.4283240032196045 and perplexity is 30.82493693732999
At time: 184.66022944450378 and batch: 450, loss is 3.453624839782715 and perplexity is 31.614783383365758
At time: 185.5416178703308 and batch: 500, loss is 3.329785804748535 and perplexity is 27.932358083964164
At time: 186.42359685897827 and batch: 550, loss is 3.362021594047546 and perplexity is 28.847449804264592
At time: 187.30559945106506 and batch: 600, loss is 3.3951049709320067 and perplexity is 29.81778331116883
At time: 188.18711400032043 and batch: 650, loss is 3.221729111671448 and perplexity is 25.071434032508815
At time: 189.06726741790771 and batch: 700, loss is 3.202012062072754 and perplexity is 24.58194086266123
At time: 189.9481406211853 and batch: 750, loss is 3.2962451124191285 and perplexity is 27.011024903485126
At time: 190.82922744750977 and batch: 800, loss is 3.2367931175231934 and perplexity is 25.451969257887757
At time: 191.7143816947937 and batch: 850, loss is 3.298806939125061 and perplexity is 27.08031118026102
At time: 192.6071422100067 and batch: 900, loss is 3.260183959007263 and perplexity is 26.054329630307873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337143989458476 and perplexity of 76.48877434359834
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 194.70796656608582 and batch: 50, loss is 3.6244160604476927 and perplexity is 37.502817386005
At time: 195.59561562538147 and batch: 100, loss is 3.5266241979599 and perplexity is 34.00896607569275
At time: 196.47842264175415 and batch: 150, loss is 3.528270182609558 and perplexity is 34.064990406749814
At time: 197.36130261421204 and batch: 200, loss is 3.398340940475464 and perplexity is 29.91442903679404
At time: 198.24391770362854 and batch: 250, loss is 3.5467834281921387 and perplexity is 34.70151785420075
At time: 199.12640929222107 and batch: 300, loss is 3.5023500299453736 and perplexity is 33.19336577649482
At time: 200.0203104019165 and batch: 350, loss is 3.488382182121277 and perplexity is 32.73294889663236
At time: 200.90463519096375 and batch: 400, loss is 3.4209912395477295 and perplexity is 30.599731657212075
At time: 201.7858865261078 and batch: 450, loss is 3.446795229911804 and perplexity is 31.399602384242037
At time: 202.67616534233093 and batch: 500, loss is 3.3231077718734743 and perplexity is 27.746446331709624
At time: 203.56513118743896 and batch: 550, loss is 3.3548089265823364 and perplexity is 28.640131300091465
At time: 204.44834208488464 and batch: 600, loss is 3.388289828300476 and perplexity is 29.61526175545315
At time: 205.3325698375702 and batch: 650, loss is 3.2147514486312865 and perplexity is 24.8971029330467
At time: 206.21612858772278 and batch: 700, loss is 3.194698243141174 and perplexity is 24.402808866207614
At time: 207.0990195274353 and batch: 750, loss is 3.2906808471679687 and perplexity is 26.86114576655426
At time: 207.99035334587097 and batch: 800, loss is 3.230468707084656 and perplexity is 25.291508502514287
At time: 208.87295627593994 and batch: 850, loss is 3.2890973615646364 and perplexity is 26.81864518735215
At time: 209.75339603424072 and batch: 900, loss is 3.2519843769073487 and perplexity is 25.84156848373919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333260314105308 and perplexity of 76.19229286743767
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 211.85906887054443 and batch: 50, loss is 3.620360879898071 and perplexity is 37.351044631332016
At time: 212.7458119392395 and batch: 100, loss is 3.5224967336654665 and perplexity is 33.86888457209546
At time: 213.62673425674438 and batch: 150, loss is 3.5259637308120726 and perplexity is 33.98651168687749
At time: 214.50915336608887 and batch: 200, loss is 3.3969764709472656 and perplexity is 29.87363954425812
At time: 215.39125990867615 and batch: 250, loss is 3.5460160303115846 and perplexity is 34.67489819818759
At time: 216.27379965782166 and batch: 300, loss is 3.50032235622406 and perplexity is 33.1261286515005
At time: 217.15632677078247 and batch: 350, loss is 3.485989966392517 and perplexity is 32.65473820715191
At time: 218.0382080078125 and batch: 400, loss is 3.4190109062194822 and perplexity is 30.539193950978834
At time: 218.9204499721527 and batch: 450, loss is 3.4449951601028443 and perplexity is 31.34313174876808
At time: 219.80307364463806 and batch: 500, loss is 3.3213456439971925 and perplexity is 27.697596597540176
At time: 220.68285131454468 and batch: 550, loss is 3.3524069595336914 and perplexity is 28.571421201171464
At time: 221.56467938423157 and batch: 600, loss is 3.3861264276504515 and perplexity is 29.551261333160127
At time: 222.44741892814636 and batch: 650, loss is 3.2123360776901246 and perplexity is 24.837039760724426
At time: 223.33032965660095 and batch: 700, loss is 3.192373914718628 and perplexity is 24.346154591047167
At time: 224.21443605422974 and batch: 750, loss is 3.2882677268981935 and perplexity is 26.796404736597825
At time: 225.10290122032166 and batch: 800, loss is 3.2282474756240847 and perplexity is 25.23539255447021
At time: 225.98446464538574 and batch: 850, loss is 3.2865169048309326 and perplexity is 26.749530046456737
At time: 226.87281560897827 and batch: 900, loss is 3.2489857292175293 and perplexity is 25.764194790305606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3311219933914815 and perplexity of 76.0295433766821
Annealing...
Model not improving. Stopping early with 75.67075580146093 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
592.9652469158173


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3206956386566162 and batch: 50, loss is 7.13072208404541 and perplexity is 1249.7790867097021
At time: 2.423046827316284 and batch: 100, loss is 6.405970239639283 and perplexity is 605.4489443618567
At time: 3.523801803588867 and batch: 150, loss is 6.010671577453613 and perplexity is 407.7570687958998
At time: 4.630435228347778 and batch: 200, loss is 5.821992607116699 and perplexity is 337.6441759577424
At time: 5.733286142349243 and batch: 250, loss is 5.803901319503784 and perplexity is 331.59068092501195
At time: 6.837881088256836 and batch: 300, loss is 5.668101835250854 and perplexity is 289.48452323355565
At time: 7.94499659538269 and batch: 350, loss is 5.614972219467163 and perplexity is 274.505753173364
At time: 9.050947904586792 and batch: 400, loss is 5.451426944732666 and perplexity is 233.09053602698765
At time: 10.157647848129272 and batch: 450, loss is 5.431307144165039 and perplexity is 228.44766437472265
At time: 11.2632474899292 and batch: 500, loss is 5.362545862197876 and perplexity is 213.26720481391004
At time: 12.369159698486328 and batch: 550, loss is 5.410960140228272 and perplexity is 223.8464084767315
At time: 13.474494457244873 and batch: 600, loss is 5.3084860610961915 and perplexity is 202.0441142826909
At time: 14.577502727508545 and batch: 650, loss is 5.196319913864135 and perplexity is 180.60637039470424
At time: 15.683364868164062 and batch: 700, loss is 5.278374795913696 and perplexity is 196.05099342224537
At time: 16.789255380630493 and batch: 750, loss is 5.250474901199341 and perplexity is 190.6567901007918
At time: 17.892665147781372 and batch: 800, loss is 5.220633325576782 and perplexity is 185.0513447159772
At time: 18.99713921546936 and batch: 850, loss is 5.241007471084595 and perplexity is 188.86027786041745
At time: 20.103626251220703 and batch: 900, loss is 5.144347352981567 and perplexity is 171.4595455854721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.099023688329409 and perplexity of 163.86184894370663
finished 1 epochs...
Completing Train Step...
At time: 22.36020517349243 and batch: 50, loss is 5.036171922683716 and perplexity is 153.87982223786838
At time: 23.241000652313232 and batch: 100, loss is 4.916352081298828 and perplexity is 136.50374925246507
At time: 24.119954586029053 and batch: 150, loss is 4.892970218658447 and perplexity is 133.34906229015397
At time: 25.002126693725586 and batch: 200, loss is 4.772573957443237 and perplexity is 118.22315202921754
At time: 25.89984631538391 and batch: 250, loss is 4.864412069320679 and perplexity is 129.59472349558067
At time: 26.78411078453064 and batch: 300, loss is 4.7996471500396725 and perplexity is 121.4675501360559
At time: 27.66733980178833 and batch: 350, loss is 4.788289041519165 and perplexity is 120.09571399970947
At time: 28.54951024055481 and batch: 400, loss is 4.655638217926025 and perplexity is 105.17632399850346
At time: 29.430493354797363 and batch: 450, loss is 4.675335865020752 and perplexity is 107.26858880929498
At time: 30.313560962677002 and batch: 500, loss is 4.571447257995605 and perplexity is 96.68393516494434
At time: 31.19485306739807 and batch: 550, loss is 4.642586641311645 and perplexity is 103.81252636195563
At time: 32.07573485374451 and batch: 600, loss is 4.589275798797607 and perplexity is 98.42312619951812
At time: 32.957921743392944 and batch: 650, loss is 4.449682931900025 and perplexity is 85.59979873343593
At time: 33.838743686676025 and batch: 700, loss is 4.498316526412964 and perplexity is 89.86571732407164
At time: 34.71967363357544 and batch: 750, loss is 4.5374308204650875 and perplexity is 93.4504005802191
At time: 35.61226487159729 and batch: 800, loss is 4.489038763046264 and perplexity is 89.03582021138052
At time: 36.492586612701416 and batch: 850, loss is 4.5411260795593265 and perplexity is 93.79636283924098
At time: 37.37164378166199 and batch: 900, loss is 4.47104814529419 and perplexity is 87.44833355377475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.601509825824058 and perplexity of 99.63463307879776
finished 2 epochs...
Completing Train Step...
At time: 39.469340801239014 and batch: 50, loss is 4.523272476196289 and perplexity is 92.13662006167411
At time: 40.368343353271484 and batch: 100, loss is 4.400670199394226 and perplexity is 81.50547528442426
At time: 41.25281500816345 and batch: 150, loss is 4.397459888458252 and perplexity is 81.24423691833098
At time: 42.138423681259155 and batch: 200, loss is 4.288366360664368 and perplexity is 72.84736491914508
At time: 43.0318489074707 and batch: 250, loss is 4.429971990585327 and perplexity is 83.92906607333141
At time: 43.91533660888672 and batch: 300, loss is 4.387678480148315 and perplexity is 80.45342778283604
At time: 44.797607421875 and batch: 350, loss is 4.386929121017456 and perplexity is 80.39316185534972
At time: 45.69161581993103 and batch: 400, loss is 4.287529520988464 and perplexity is 72.78642885431022
At time: 46.583088636398315 and batch: 450, loss is 4.324471101760865 and perplexity is 75.52555696026958
At time: 47.47010779380798 and batch: 500, loss is 4.202752027511597 and perplexity is 66.87010642005929
At time: 48.3670916557312 and batch: 550, loss is 4.276226778030395 and perplexity is 71.96837439675302
At time: 49.24761366844177 and batch: 600, loss is 4.2675736618041995 and perplexity is 71.34831030084253
At time: 50.130520820617676 and batch: 650, loss is 4.110559325218201 and perplexity is 60.980816140931935
At time: 51.02311968803406 and batch: 700, loss is 4.137495512962341 and perplexity is 62.645729396571255
At time: 51.910106897354126 and batch: 750, loss is 4.222894549369812 and perplexity is 68.23069585010171
At time: 52.79693245887756 and batch: 800, loss is 4.181679887771606 and perplexity is 65.47575279514105
At time: 53.68790864944458 and batch: 850, loss is 4.249753060340882 and perplexity is 70.0881026773705
At time: 54.570854902267456 and batch: 900, loss is 4.187076501846313 and perplexity is 65.83005532135178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434080359053938 and perplexity of 84.27458687938474
finished 3 epochs...
Completing Train Step...
At time: 56.68758726119995 and batch: 50, loss is 4.261979770660401 and perplexity is 70.95030984318261
At time: 57.58510231971741 and batch: 100, loss is 4.138175854682922 and perplexity is 62.68836440143772
At time: 58.47080445289612 and batch: 150, loss is 4.141394081115723 and perplexity is 62.890434732420516
At time: 59.35342597961426 and batch: 200, loss is 4.030760688781738 and perplexity is 56.303724572907726
At time: 60.23434114456177 and batch: 250, loss is 4.184385371208191 and perplexity is 65.65313620556708
At time: 61.12335515022278 and batch: 300, loss is 4.145711011886597 and perplexity is 63.16251523912789
At time: 62.00897717475891 and batch: 350, loss is 4.14473961353302 and perplexity is 63.10118906671066
At time: 62.89180302619934 and batch: 400, loss is 4.065861630439758 and perplexity is 58.31513297565629
At time: 63.78368806838989 and batch: 450, loss is 4.105229301452637 and perplexity is 60.65665161141668
At time: 64.67073726654053 and batch: 500, loss is 3.9757563066482544 and perplexity is 53.29040555653855
At time: 65.5613341331482 and batch: 550, loss is 4.049475059509278 and perplexity is 57.36733470308885
At time: 66.44455599784851 and batch: 600, loss is 4.057051544189453 and perplexity is 57.8036281338079
At time: 67.32653832435608 and batch: 650, loss is 3.896267042160034 and perplexity is 49.21837562884701
At time: 68.2188708782196 and batch: 700, loss is 3.917685875892639 and perplexity is 50.2839467412295
At time: 69.10204768180847 and batch: 750, loss is 4.015020132064819 and perplexity is 55.424411199101016
At time: 69.9914128780365 and batch: 800, loss is 3.9824078035354615 and perplexity is 53.64604798927745
At time: 70.87317776679993 and batch: 850, loss is 4.047572169303894 and perplexity is 57.25827476122973
At time: 71.75322270393372 and batch: 900, loss is 3.9934515810012816 and perplexity is 54.24178655232424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375471977338399 and perplexity of 79.47734220580115
finished 4 epochs...
Completing Train Step...
At time: 73.84908628463745 and batch: 50, loss is 4.071242637634278 and perplexity is 58.62977290640737
At time: 74.72620749473572 and batch: 100, loss is 3.950051603317261 and perplexity is 51.93804694119386
At time: 75.61170983314514 and batch: 150, loss is 3.9576146841049193 and perplexity is 52.3323477711741
At time: 76.48857402801514 and batch: 200, loss is 3.8494667291641234 and perplexity is 46.968009882173384
At time: 77.3660454750061 and batch: 250, loss is 4.002261033058167 and perplexity is 54.72173792078247
At time: 78.24308466911316 and batch: 300, loss is 3.9654481267929076 and perplexity is 52.74390004926052
At time: 79.12960004806519 and batch: 350, loss is 3.966539936065674 and perplexity is 52.80151777647699
At time: 80.0117974281311 and batch: 400, loss is 3.893227524757385 and perplexity is 49.069002645484794
At time: 80.8888430595398 and batch: 450, loss is 3.936334834098816 and perplexity is 51.23048853917631
At time: 81.76379656791687 and batch: 500, loss is 3.8086179447174073 and perplexity is 45.088081564317065
At time: 82.64059257507324 and batch: 550, loss is 3.879425148963928 and perplexity is 48.396386360588394
At time: 83.51858162879944 and batch: 600, loss is 3.8926746702194213 and perplexity is 49.04188212224183
At time: 84.39534735679626 and batch: 650, loss is 3.733996181488037 and perplexity is 41.84599868482115
At time: 85.29086351394653 and batch: 700, loss is 3.7554193449020388 and perplexity is 42.752143945707246
At time: 86.17107009887695 and batch: 750, loss is 3.856447334289551 and perplexity is 47.29702202859501
At time: 87.05332469940186 and batch: 800, loss is 3.825375828742981 and perplexity is 45.850028887366356
At time: 87.94090604782104 and batch: 850, loss is 3.889474754333496 and perplexity is 48.885203038200636
At time: 88.82382607460022 and batch: 900, loss is 3.840635938644409 and perplexity is 46.55507119679825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357842014260488 and perplexity of 78.08843872990086
finished 5 epochs...
Completing Train Step...
At time: 90.91067123413086 and batch: 50, loss is 3.9202776956558227 and perplexity is 50.41444270612384
At time: 91.80221772193909 and batch: 100, loss is 3.8000093269348145 and perplexity is 44.70160142027905
At time: 92.68371772766113 and batch: 150, loss is 3.8098671007156373 and perplexity is 45.14443880401554
At time: 93.56911587715149 and batch: 200, loss is 3.703338108062744 and perplexity is 40.58254743528385
At time: 94.45061755180359 and batch: 250, loss is 3.8544156885147096 and perplexity is 47.201028778786586
At time: 95.33233618736267 and batch: 300, loss is 3.8222696495056154 and perplexity is 45.707831439315044
At time: 96.21077275276184 and batch: 350, loss is 3.8223704051971437 and perplexity is 45.71243699549407
At time: 97.09087324142456 and batch: 400, loss is 3.75501895904541 and perplexity is 42.735030018246704
At time: 97.97383570671082 and batch: 450, loss is 3.79611732006073 and perplexity is 44.527960605080374
At time: 98.86653780937195 and batch: 500, loss is 3.673277850151062 and perplexity is 39.3807788236018
At time: 99.74974513053894 and batch: 550, loss is 3.741640872955322 and perplexity is 42.167124323205535
At time: 100.6403923034668 and batch: 600, loss is 3.757656421661377 and perplexity is 42.84789082997431
At time: 101.53066325187683 and batch: 650, loss is 3.600210175514221 and perplexity is 36.60592730481898
At time: 102.4136209487915 and batch: 700, loss is 3.6229169464111326 and perplexity is 37.446638505846096
At time: 103.29535484313965 and batch: 750, loss is 3.722752695083618 and perplexity is 41.37813888392047
At time: 104.18456363677979 and batch: 800, loss is 3.6910350847244264 and perplexity is 40.086318226099934
At time: 105.07122445106506 and batch: 850, loss is 3.755460748672485 and perplexity is 42.753914082306174
At time: 105.94976830482483 and batch: 900, loss is 3.71301326751709 and perplexity is 40.977095633390796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361616160771618 and perplexity of 78.38371279178345
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 108.06236171722412 and batch: 50, loss is 3.82776162147522 and perplexity is 45.95954814623601
At time: 108.95830154418945 and batch: 100, loss is 3.712229986190796 and perplexity is 40.945011606625286
At time: 109.85628485679626 and batch: 150, loss is 3.7198286867141723 and perplexity is 41.25732557515249
At time: 110.73918867111206 and batch: 200, loss is 3.6002420663833616 and perplexity is 36.60709471827124
At time: 111.62220406532288 and batch: 250, loss is 3.7434086704254153 and perplexity is 42.241733186154725
At time: 112.51749467849731 and batch: 300, loss is 3.699917740821838 and perplexity is 40.44397733489293
At time: 113.4040699005127 and batch: 350, loss is 3.695633330345154 and perplexity is 40.27106940345092
At time: 114.30269241333008 and batch: 400, loss is 3.6180553531646726 and perplexity is 37.265029992039864
At time: 115.19586825370789 and batch: 450, loss is 3.646612410545349 and perplexity is 38.344550190040636
At time: 116.08146619796753 and batch: 500, loss is 3.5183138942718504 and perplexity is 33.72751234227785
At time: 116.96622228622437 and batch: 550, loss is 3.5713643169403078 and perplexity is 35.56508211096029
At time: 117.85033512115479 and batch: 600, loss is 3.5802631521224977 and perplexity is 35.882982287691256
At time: 118.73354697227478 and batch: 650, loss is 3.4087758541107176 and perplexity is 30.228217852537462
At time: 119.62161231040955 and batch: 700, loss is 3.416593255996704 and perplexity is 30.46545004132921
At time: 120.5088062286377 and batch: 750, loss is 3.5048909091949465 and perplexity is 33.27781335093132
At time: 121.39280819892883 and batch: 800, loss is 3.457123193740845 and perplexity is 31.72557677019685
At time: 122.27595376968384 and batch: 850, loss is 3.507387738227844 and perplexity is 33.36100617736972
At time: 123.15962648391724 and batch: 900, loss is 3.453045325279236 and perplexity is 31.596467465553353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315592726616011 and perplexity of 74.85798061433746
finished 7 epochs...
Completing Train Step...
At time: 125.2682614326477 and batch: 50, loss is 3.7341676998138427 and perplexity is 41.85317665601644
At time: 126.15071225166321 and batch: 100, loss is 3.6108853816986084 and perplexity is 36.99879637458985
At time: 127.03935408592224 and batch: 150, loss is 3.6163274526596068 and perplexity is 37.2006953258531
At time: 127.92279601097107 and batch: 200, loss is 3.499211926460266 and perplexity is 33.089364827886236
At time: 128.81494808197021 and batch: 250, loss is 3.6456646060943605 and perplexity is 38.308224272352
At time: 129.70744633674622 and batch: 300, loss is 3.605551767349243 and perplexity is 36.80198438951718
At time: 130.59020137786865 and batch: 350, loss is 3.605269651412964 and perplexity is 36.791603427620444
At time: 131.482745885849 and batch: 400, loss is 3.532870054244995 and perplexity is 34.2220459305822
At time: 132.36381149291992 and batch: 450, loss is 3.566331777572632 and perplexity is 35.386549049253595
At time: 133.2530243396759 and batch: 500, loss is 3.4402863454818724 and perplexity is 31.195889691563938
At time: 134.13421726226807 and batch: 550, loss is 3.496641230583191 and perplexity is 33.004411375677236
At time: 135.0202386379242 and batch: 600, loss is 3.51129665851593 and perplexity is 33.491666896043554
At time: 135.91744947433472 and batch: 650, loss is 3.345291647911072 and perplexity is 28.36884817557487
At time: 136.80053162574768 and batch: 700, loss is 3.3593397665023805 and perplexity is 28.770189564397356
At time: 137.6812379360199 and batch: 750, loss is 3.453789191246033 and perplexity is 31.619979746280517
At time: 138.5676097869873 and batch: 800, loss is 3.41058479309082 and perplexity is 30.282948341242793
At time: 139.45209860801697 and batch: 850, loss is 3.4679311513900757 and perplexity is 32.0703251172063
At time: 140.33583188056946 and batch: 900, loss is 3.421832981109619 and perplexity is 30.62549956656931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325281430597174 and perplexity of 75.58678229993174
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 142.42824172973633 and batch: 50, loss is 3.696158947944641 and perplexity is 40.29224215017625
At time: 143.30975890159607 and batch: 100, loss is 3.5849174785614015 and perplexity is 36.050382666668455
At time: 144.19144821166992 and batch: 150, loss is 3.597408175468445 and perplexity is 36.5035010610192
At time: 145.07216238975525 and batch: 200, loss is 3.474538731575012 and perplexity is 32.28293400356405
At time: 145.95258593559265 and batch: 250, loss is 3.618643670082092 and perplexity is 37.28696008990358
At time: 146.83374309539795 and batch: 300, loss is 3.577801380157471 and perplexity is 35.794755209904224
At time: 147.72351050376892 and batch: 350, loss is 3.573392424583435 and perplexity is 35.63728511880511
At time: 148.60565185546875 and batch: 400, loss is 3.5008477687835695 and perplexity is 33.143538108715155
At time: 149.48904037475586 and batch: 450, loss is 3.526406378746033 and perplexity is 34.0015590761604
At time: 150.372540473938 and batch: 500, loss is 3.3981213331222535 and perplexity is 29.90786032950508
At time: 151.25289607048035 and batch: 550, loss is 3.445940728187561 and perplexity is 31.372782830173666
At time: 152.1360785961151 and batch: 600, loss is 3.4638933992385863 and perplexity is 31.941094169809016
At time: 153.02664232254028 and batch: 650, loss is 3.2885669040679932 and perplexity is 26.80442280847995
At time: 153.91094255447388 and batch: 700, loss is 3.2940789461135864 and perplexity is 26.9525778573262
At time: 154.79469561576843 and batch: 750, loss is 3.3851812648773194 and perplexity is 29.523343776444893
At time: 155.6880567073822 and batch: 800, loss is 3.337734622955322 and perplexity is 28.155272098238097
At time: 156.57890057563782 and batch: 850, loss is 3.3903147745132447 and perplexity is 29.675291825904605
At time: 157.46366834640503 and batch: 900, loss is 3.3458229160308837 and perplexity is 28.38392364440974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316404577803938 and perplexity of 74.91877883103797
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.57207584381104 and batch: 50, loss is 3.6815374279022217 and perplexity is 39.7073944248486
At time: 160.47088718414307 and batch: 100, loss is 3.5644163131713866 and perplexity is 35.31883224953088
At time: 161.35639095306396 and batch: 150, loss is 3.5770184564590455 and perplexity is 35.766741615455494
At time: 162.23639822006226 and batch: 200, loss is 3.4563505125045775 and perplexity is 31.701072480538333
At time: 163.12444639205933 and batch: 250, loss is 3.602324295043945 and perplexity is 36.683398473438245
At time: 164.0109579563141 and batch: 300, loss is 3.559640736579895 and perplexity is 35.150566563627244
At time: 164.89986777305603 and batch: 350, loss is 3.5537972688674926 and perplexity is 34.94576432324598
At time: 165.78373312950134 and batch: 400, loss is 3.4837717962265016 and perplexity is 32.582384716925084
At time: 166.66571640968323 and batch: 450, loss is 3.5065274000167848 and perplexity is 33.33231677206137
At time: 167.54745364189148 and batch: 500, loss is 3.3788391971588134 and perplexity is 29.336697219095996
At time: 168.4386692047119 and batch: 550, loss is 3.4263062047958375 and perplexity is 30.762801138006306
At time: 169.33230757713318 and batch: 600, loss is 3.447032914161682 and perplexity is 31.407066462192944
At time: 170.21599674224854 and batch: 650, loss is 3.269000005722046 and perplexity is 26.28504130561239
At time: 171.09819889068604 and batch: 700, loss is 3.2724432849884035 and perplexity is 26.37570404234009
At time: 171.98551893234253 and batch: 750, loss is 3.3635923051834107 and perplexity is 28.89279641880665
At time: 172.8838574886322 and batch: 800, loss is 3.3145420360565185 and perplexity is 27.509792606193933
At time: 173.7688910961151 and batch: 850, loss is 3.36203351020813 and perplexity is 28.84779355715699
At time: 174.65445375442505 and batch: 900, loss is 3.3201734399795533 and perplexity is 27.665148385207512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3149434964950775 and perplexity of 74.80939633142687
finished 10 epochs...
Completing Train Step...
At time: 176.76034832000732 and batch: 50, loss is 3.6692117929458616 and perplexity is 39.220979421036965
At time: 177.64517498016357 and batch: 100, loss is 3.550299005508423 and perplexity is 34.823728417682446
At time: 178.52580332756042 and batch: 150, loss is 3.5608436679840088 and perplexity is 35.19287572642739
At time: 179.40694785118103 and batch: 200, loss is 3.4412374925613403 and perplexity is 31.22557568657117
At time: 180.32956528663635 and batch: 250, loss is 3.586408634185791 and perplexity is 36.104179497298226
At time: 181.2125198841095 and batch: 300, loss is 3.5444636440277097 and perplexity is 34.62111112179159
At time: 182.0923948287964 and batch: 350, loss is 3.5396085023880004 and perplexity is 34.45342811533542
At time: 182.9745659828186 and batch: 400, loss is 3.4704224061965943 and perplexity is 32.15032007133509
At time: 183.85562419891357 and batch: 450, loss is 3.4952489709854127 and perplexity is 32.958492639988016
At time: 184.73476576805115 and batch: 500, loss is 3.3681727552413943 and perplexity is 29.025441986122367
At time: 185.6162827014923 and batch: 550, loss is 3.4165838623046874 and perplexity is 30.465163859618524
At time: 186.4969346523285 and batch: 600, loss is 3.4391319513320924 and perplexity is 31.15989811723518
At time: 187.37814807891846 and batch: 650, loss is 3.263234519958496 and perplexity is 26.133931304036835
At time: 188.25904655456543 and batch: 700, loss is 3.2686572074890137 and perplexity is 26.276032384107047
At time: 189.14842629432678 and batch: 750, loss is 3.36179235458374 and perplexity is 28.840837588258672
At time: 190.05623626708984 and batch: 800, loss is 3.314951181411743 and perplexity is 27.521050412944575
At time: 190.9490931034088 and batch: 850, loss is 3.3656216526031493 and perplexity is 28.951489474826356
At time: 191.83408641815186 and batch: 900, loss is 3.325095934867859 and perplexity is 27.801665663859172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3146646578017975 and perplexity of 74.78853948509177
finished 11 epochs...
Completing Train Step...
At time: 193.96387887001038 and batch: 50, loss is 3.6621899461746215 and perplexity is 38.946540375418635
At time: 194.8607621192932 and batch: 100, loss is 3.5421420240402224 and perplexity is 34.540827288558454
At time: 195.7462375164032 and batch: 150, loss is 3.5518113565444946 and perplexity is 34.87643396403292
At time: 196.64047527313232 and batch: 200, loss is 3.432396206855774 and perplexity is 30.950718287428856
At time: 197.5279712677002 and batch: 250, loss is 3.5772207307815553 and perplexity is 35.77397704062984
At time: 198.41197156906128 and batch: 300, loss is 3.535357704162598 and perplexity is 34.30728437807257
At time: 199.29572916030884 and batch: 350, loss is 3.530841546058655 and perplexity is 34.15269659188948
At time: 200.18653512001038 and batch: 400, loss is 3.4621122026443483 and perplexity is 31.8842514407238
At time: 201.0705907344818 and batch: 450, loss is 3.4877340507507326 and perplexity is 32.711740519251805
At time: 201.953458070755 and batch: 500, loss is 3.3611876487731935 and perplexity is 28.823402638227105
At time: 202.84433245658875 and batch: 550, loss is 3.4101712703704834 and perplexity is 30.270428242915877
At time: 203.72738099098206 and batch: 600, loss is 3.4336967706680297 and perplexity is 30.990997858995755
At time: 204.62021565437317 and batch: 650, loss is 3.258924865722656 and perplexity is 26.02154544234198
At time: 205.51136922836304 and batch: 700, loss is 3.2655233097076417 and perplexity is 26.193814882428608
At time: 206.4007089138031 and batch: 750, loss is 3.3596922254562376 and perplexity is 28.78033166254515
At time: 207.29747533798218 and batch: 800, loss is 3.3140343713760374 and perplexity is 27.495830400479623
At time: 208.19207453727722 and batch: 850, loss is 3.3662296628952024 and perplexity is 28.969097630824912
At time: 209.07409119606018 and batch: 900, loss is 3.3263088703155517 and perplexity is 27.835407748996527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315208121521832 and perplexity of 74.82919538948015
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 211.15810418128967 and batch: 50, loss is 3.658938755989075 and perplexity is 38.8201233797916
At time: 212.04485392570496 and batch: 100, loss is 3.5412517738342286 and perplexity is 34.510090993476126
At time: 212.93663501739502 and batch: 150, loss is 3.5511702489852905 and perplexity is 34.85408158448585
At time: 213.82458209991455 and batch: 200, loss is 3.4320134782791136 and perplexity is 30.938874829631533
At time: 214.7095651626587 and batch: 250, loss is 3.5773953676223753 and perplexity is 35.780225040513386
At time: 215.59093737602234 and batch: 300, loss is 3.5341780757904053 and perplexity is 34.26683839235359
At time: 216.47584414482117 and batch: 350, loss is 3.5278926706314087 and perplexity is 34.052132891921346
At time: 217.36607956886292 and batch: 400, loss is 3.459155240058899 and perplexity is 31.79011015687026
At time: 218.2444531917572 and batch: 450, loss is 3.484255256652832 and perplexity is 32.59814081895019
At time: 219.12478375434875 and batch: 500, loss is 3.3569955587387086 and perplexity is 28.70282525147983
At time: 220.01172542572021 and batch: 550, loss is 3.404853401184082 and perplexity is 30.109881327403
At time: 220.89843082427979 and batch: 600, loss is 3.4282457494735716 and perplexity is 30.822524864921498
At time: 221.77843761444092 and batch: 650, loss is 3.2520974826812745 and perplexity is 25.844491479642745
At time: 222.65988612174988 and batch: 700, loss is 3.2576141262054445 and perplexity is 25.987460317671676
At time: 223.55166721343994 and batch: 750, loss is 3.3510184860229493 and perplexity is 28.53177806775856
At time: 224.43867421150208 and batch: 800, loss is 3.3054241132736206 and perplexity is 27.260100507947968
At time: 225.3196051120758 and batch: 850, loss is 3.355172662734985 and perplexity is 28.650550646093453
At time: 226.2012197971344 and batch: 900, loss is 3.315685200691223 and perplexity is 27.541258810308516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314328128344392 and perplexity of 74.76337517298234
finished 13 epochs...
Completing Train Step...
At time: 228.3057255744934 and batch: 50, loss is 3.655174956321716 and perplexity is 38.67428683426098
At time: 229.18793320655823 and batch: 100, loss is 3.5375100231170653 and perplexity is 34.38120411740788
At time: 230.068110704422 and batch: 150, loss is 3.5473479413986206 and perplexity is 34.7211128496109
At time: 230.9492826461792 and batch: 200, loss is 3.428351788520813 and perplexity is 30.82579342938641
At time: 231.83032536506653 and batch: 250, loss is 3.573283281326294 and perplexity is 35.63339576168398
At time: 232.71126437187195 and batch: 300, loss is 3.530649757385254 and perplexity is 34.14614711959444
At time: 233.6030158996582 and batch: 350, loss is 3.5248163509368897 and perplexity is 33.947538610077906
At time: 234.50497555732727 and batch: 400, loss is 3.45625527381897 and perplexity is 31.69805345582919
At time: 235.3896882534027 and batch: 450, loss is 3.4817205381393435 and perplexity is 32.515618337724085
At time: 236.2718324661255 and batch: 500, loss is 3.3545105028152467 and perplexity is 28.63158567939053
At time: 237.15111780166626 and batch: 550, loss is 3.4028845119476316 and perplexity is 30.050656628714734
At time: 238.0305507183075 and batch: 600, loss is 3.426850199699402 and perplexity is 30.77954049769512
At time: 238.90961980819702 and batch: 650, loss is 3.2510588359832764 and perplexity is 25.8176621194311
At time: 239.80323553085327 and batch: 700, loss is 3.256959037780762 and perplexity is 25.970441808153574
At time: 240.68480968475342 and batch: 750, loss is 3.35104905128479 and perplexity is 28.53265016235381
At time: 241.5664665699005 and batch: 800, loss is 3.3058864450454712 and perplexity is 27.272706632398013
At time: 242.4471640586853 and batch: 850, loss is 3.356126461029053 and perplexity is 28.67789052871784
At time: 243.32649421691895 and batch: 900, loss is 3.3171272230148316 and perplexity is 27.581002569139972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314227378531678 and perplexity of 74.7558431563668
finished 14 epochs...
Completing Train Step...
At time: 245.42260026931763 and batch: 50, loss is 3.6528999853134154 and perplexity is 38.58640395634912
At time: 246.31061267852783 and batch: 100, loss is 3.53513801574707 and perplexity is 34.2997482929521
At time: 247.19241976737976 and batch: 150, loss is 3.544758105278015 and perplexity is 34.63130719856056
At time: 248.07605576515198 and batch: 200, loss is 3.4257912826538086 and perplexity is 30.74696476814817
At time: 248.9597089290619 and batch: 250, loss is 3.5705725812911986 and perplexity is 35.53693511155129
At time: 249.85081362724304 and batch: 300, loss is 3.52808376789093 and perplexity is 34.05864078299812
At time: 250.7359824180603 and batch: 350, loss is 3.522496337890625 and perplexity is 33.86887116764569
At time: 251.61851286888123 and batch: 400, loss is 3.4540436935424803 and perplexity is 31.628028127861924
At time: 252.50181221961975 and batch: 450, loss is 3.4797286796569824 and perplexity is 32.45091628758468
At time: 253.38145899772644 and batch: 500, loss is 3.35267569065094 and perplexity is 28.579100262867442
At time: 254.2708191871643 and batch: 550, loss is 3.4013349199295044 and perplexity is 30.004126431785107
At time: 255.1658272743225 and batch: 600, loss is 3.4256636047363282 and perplexity is 30.743039310320206
At time: 256.0447726249695 and batch: 650, loss is 3.2501939868927003 and perplexity is 25.795343390383703
At time: 256.9232647418976 and batch: 700, loss is 3.256397738456726 and perplexity is 25.955868707040263
At time: 257.8054099082947 and batch: 750, loss is 3.350923390388489 and perplexity is 28.529064949225834
At time: 258.6861717700958 and batch: 800, loss is 3.3060396575927733 and perplexity is 27.276885473370243
At time: 259.5644519329071 and batch: 850, loss is 3.356669554710388 and perplexity is 28.693469539906353
At time: 260.4430286884308 and batch: 900, loss is 3.3178739976882934 and perplexity is 27.60160705582415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31427001953125 and perplexity of 74.7590308882064
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 262.52182149887085 and batch: 50, loss is 3.6516351461410523 and perplexity is 38.53762921370962
At time: 263.40732526779175 and batch: 100, loss is 3.534613471031189 and perplexity is 34.281761259143174
At time: 264.28566884994507 and batch: 150, loss is 3.5445577907562256 and perplexity is 34.624370739580066
At time: 265.1896994113922 and batch: 200, loss is 3.4259042501449586 and perplexity is 30.75043837181696
At time: 266.0708124637604 and batch: 250, loss is 3.5709284782409667 and perplexity is 35.5495848492296
At time: 266.95136523246765 and batch: 300, loss is 3.527853798866272 and perplexity is 34.05080925113547
At time: 267.8348717689514 and batch: 350, loss is 3.521782932281494 and perplexity is 33.8447175416703
At time: 268.7262330055237 and batch: 400, loss is 3.4530268478393555 and perplexity is 31.59588364911906
At time: 269.6198420524597 and batch: 450, loss is 3.478924822807312 and perplexity is 32.42484087810123
At time: 270.515478849411 and batch: 500, loss is 3.35142587184906 and perplexity is 28.5434038776714
At time: 271.39855337142944 and batch: 550, loss is 3.3994191980361936 and perplexity is 29.946701892176232
At time: 272.2785222530365 and batch: 600, loss is 3.4239777374267577 and perplexity is 30.69125428895248
At time: 273.1587326526642 and batch: 650, loss is 3.2486841678619385 and perplexity is 25.75642647616948
At time: 274.03789162635803 and batch: 700, loss is 3.253590841293335 and perplexity is 25.88311540607678
At time: 274.9173345565796 and batch: 750, loss is 3.3480540800094603 and perplexity is 28.447323533889765
At time: 275.79586815834045 and batch: 800, loss is 3.30323380947113 and perplexity is 27.200457947656687
At time: 276.6715784072876 and batch: 850, loss is 3.353365797996521 and perplexity is 28.59882971682648
At time: 277.54585003852844 and batch: 900, loss is 3.3146005630493165 and perplexity is 27.51140271874473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31348659567637 and perplexity of 74.70048581583833
finished 16 epochs...
Completing Train Step...
At time: 279.60968255996704 and batch: 50, loss is 3.6506945371627806 and perplexity is 38.501397416318184
At time: 280.48394298553467 and batch: 100, loss is 3.5334789419174193 and perplexity is 34.2428896575747
At time: 281.3602237701416 and batch: 150, loss is 3.543516263961792 and perplexity is 34.58832730307508
At time: 282.2373425960541 and batch: 200, loss is 3.4248708295822143 and perplexity is 30.718676650924543
At time: 283.1163923740387 and batch: 250, loss is 3.5698017740249632 and perplexity is 35.50955353806119
At time: 283.9950885772705 and batch: 300, loss is 3.526939377784729 and perplexity is 34.01968670503739
At time: 284.8736674785614 and batch: 350, loss is 3.5209238433837893 and perplexity is 33.81565440628023
At time: 285.7530345916748 and batch: 400, loss is 3.452349834442139 and perplexity is 31.574500051899363
At time: 286.63177728652954 and batch: 450, loss is 3.478323440551758 and perplexity is 32.405347016376744
At time: 287.50894260406494 and batch: 500, loss is 3.3508058691024782 and perplexity is 28.525712373827663
At time: 288.38691425323486 and batch: 550, loss is 3.3989798069000243 and perplexity is 29.93354646720752
At time: 289.264714717865 and batch: 600, loss is 3.42364372253418 and perplexity is 30.68100466480692
At time: 290.147558927536 and batch: 650, loss is 3.2482639074325563 and perplexity is 25.74560434353559
At time: 291.0243628025055 and batch: 700, loss is 3.253548822402954 and perplexity is 25.882027849136943
At time: 291.9006989002228 and batch: 750, loss is 3.348195209503174 and perplexity is 28.45133857357121
At time: 292.77714371681213 and batch: 800, loss is 3.3034327936172487 and perplexity is 27.205870946088012
At time: 293.6548397541046 and batch: 850, loss is 3.3537812662124633 and perplexity is 28.61071409020572
At time: 294.5334403514862 and batch: 900, loss is 3.3150948905944824 and perplexity is 27.525005724808054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313135016454409 and perplexity of 74.67422729340106
finished 17 epochs...
Completing Train Step...
At time: 296.6113600730896 and batch: 50, loss is 3.6499880266189577 and perplexity is 38.474205379952934
At time: 297.49553537368774 and batch: 100, loss is 3.5326881313323977 and perplexity is 34.21582072258262
At time: 298.3750994205475 and batch: 150, loss is 3.542731876373291 and perplexity is 34.5612072861244
At time: 299.2536551952362 and batch: 200, loss is 3.424104390144348 and perplexity is 30.695141665882666
At time: 300.1322412490845 and batch: 250, loss is 3.568959560394287 and perplexity is 35.479659498403095
At time: 301.01021933555603 and batch: 300, loss is 3.5262008237838747 and perplexity is 33.99457060525322
At time: 301.8911325931549 and batch: 350, loss is 3.520270752906799 and perplexity is 33.793576934492314
At time: 302.7724196910858 and batch: 400, loss is 3.4517814779281615 and perplexity is 31.556559577894863
At time: 303.653550863266 and batch: 450, loss is 3.4778086280822755 and perplexity is 32.388668633143
At time: 304.5345981121063 and batch: 500, loss is 3.3503278350830077 and perplexity is 28.512079371662367
At time: 305.4164819717407 and batch: 550, loss is 3.3986053609848024 and perplexity is 29.922340071230305
At time: 306.2978262901306 and batch: 600, loss is 3.423361349105835 and perplexity is 30.672342387390692
At time: 307.17661118507385 and batch: 650, loss is 3.247996416091919 and perplexity is 25.73871853830191
At time: 308.0552430152893 and batch: 700, loss is 3.25346595287323 and perplexity is 25.879883106528908
At time: 308.9371430873871 and batch: 750, loss is 3.348267526626587 and perplexity is 28.453396166932823
At time: 309.81717920303345 and batch: 800, loss is 3.303575553894043 and perplexity is 27.20975514100248
At time: 310.69771003723145 and batch: 850, loss is 3.3540347480773924 and perplexity is 28.617967306610833
At time: 311.57881450653076 and batch: 900, loss is 3.315418548583984 and perplexity is 27.533915854661547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312969050995291 and perplexity of 74.66183497936053
finished 18 epochs...
Completing Train Step...
At time: 313.70038962364197 and batch: 50, loss is 3.649378581047058 and perplexity is 38.45076458952027
At time: 314.59052753448486 and batch: 100, loss is 3.532031979560852 and perplexity is 34.1933773151466
At time: 315.47335028648376 and batch: 150, loss is 3.542054591178894 and perplexity is 34.53780741721589
At time: 316.3563938140869 and batch: 200, loss is 3.423444542884827 and perplexity is 30.67489424161216
At time: 317.23852133750916 and batch: 250, loss is 3.5682461786270143 and perplexity is 35.45435798210112
At time: 318.1209366321564 and batch: 300, loss is 3.5255447244644165 and perplexity is 33.97227410577273
At time: 319.0032386779785 and batch: 350, loss is 3.5196934652328493 and perplexity is 33.774073949027546
At time: 319.8828113079071 and batch: 400, loss is 3.4512549018859864 and perplexity is 31.539947023917996
At time: 320.7653923034668 and batch: 450, loss is 3.477327766418457 and perplexity is 32.37309790803762
At time: 321.64763259887695 and batch: 500, loss is 3.3498960590362548 and perplexity is 28.49977119612547
At time: 322.5307309627533 and batch: 550, loss is 3.398249878883362 and perplexity is 29.911705105283783
At time: 323.4131484031677 and batch: 600, loss is 3.423091721534729 and perplexity is 30.66407339303719
At time: 324.2959363460541 and batch: 650, loss is 3.2477772665023803 and perplexity is 25.733078526724675
At time: 325.1767666339874 and batch: 700, loss is 3.253359990119934 and perplexity is 25.87714094814586
At time: 326.05967354774475 and batch: 750, loss is 3.3482914400100707 and perplexity is 28.454076592042377
At time: 326.9452419281006 and batch: 800, loss is 3.3036695528030395 and perplexity is 27.212312948513468
At time: 327.82809257507324 and batch: 850, loss is 3.3542069005966186 and perplexity is 28.622894385870186
At time: 328.70922017097473 and batch: 900, loss is 3.3156498146057127 and perplexity is 27.54028425021223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312896310466609 and perplexity of 74.65640423553172
finished 19 epochs...
Completing Train Step...
At time: 330.81422543525696 and batch: 50, loss is 3.648823628425598 and perplexity is 38.429432156706405
At time: 331.6950857639313 and batch: 100, loss is 3.5314424657821655 and perplexity is 34.17322578846442
At time: 332.5765874385834 and batch: 150, loss is 3.541436095237732 and perplexity is 34.51645252814856
At time: 333.45721769332886 and batch: 200, loss is 3.422840723991394 and perplexity is 30.656377751780806
At time: 334.3446481227875 and batch: 250, loss is 3.56760440826416 and perplexity is 35.43161172563152
At time: 335.22686672210693 and batch: 300, loss is 3.5249394083023073 and perplexity is 33.95171636178796
At time: 336.1088149547577 and batch: 350, loss is 3.5191536283493043 and perplexity is 33.75584637860235
At time: 336.9900813102722 and batch: 400, loss is 3.4507527780532836 and perplexity is 31.524114040226994
At time: 337.87767481803894 and batch: 450, loss is 3.4768680477142335 and perplexity is 32.358218809777576
At time: 338.7582013607025 and batch: 500, loss is 3.349485378265381 and perplexity is 28.48806929115906
At time: 339.6407928466797 and batch: 550, loss is 3.3979028272628784 and perplexity is 29.901326000702277
At time: 340.52337765693665 and batch: 600, loss is 3.422825951576233 and perplexity is 30.655924886386295
At time: 341.40436267852783 and batch: 650, loss is 3.247573833465576 and perplexity is 25.727844100859294
At time: 342.2845456600189 and batch: 700, loss is 3.253241262435913 and perplexity is 25.874068797510272
At time: 343.16588306427 and batch: 750, loss is 3.3482837009429933 and perplexity is 28.453856384887104
At time: 344.0483958721161 and batch: 800, loss is 3.3037278699874877 and perplexity is 27.213899940260944
At time: 344.92909836769104 and batch: 850, loss is 3.3543339729309083 and perplexity is 28.62653179497606
At time: 345.8108251094818 and batch: 900, loss is 3.3158265447616575 and perplexity is 27.545151879058267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312869555329623 and perplexity of 74.65440681993022
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
946.2753343582153


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}, {'params': {'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.65440681993022}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.44084991003863383, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.4525796519173104, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3079643249511719 and batch: 50, loss is 7.072432241439819 and perplexity is 1179.0121919870246
At time: 2.4082412719726562 and batch: 100, loss is 6.47930832862854 and perplexity is 651.52015255114
At time: 3.5088205337524414 and batch: 150, loss is 6.029534559249878 and perplexity is 415.52158368912
At time: 4.617779970169067 and batch: 200, loss is 5.838269281387329 and perplexity is 343.1848699405887
At time: 5.720039367675781 and batch: 250, loss is 5.836856775283813 and perplexity is 342.70046141270444
At time: 6.821509599685669 and batch: 300, loss is 5.689246320724488 and perplexity is 295.6706957683313
At time: 7.924503803253174 and batch: 350, loss is 5.630601940155029 and perplexity is 278.82990604555397
At time: 9.028787612915039 and batch: 400, loss is 5.447721214294433 and perplexity is 232.22836380832896
At time: 10.131731510162354 and batch: 450, loss is 5.428875198364258 and perplexity is 227.89276705024585
At time: 11.235103607177734 and batch: 500, loss is 5.353862609863281 and perplexity is 211.4233686922203
At time: 12.340656280517578 and batch: 550, loss is 5.392804708480835 and perplexity is 219.81905012467877
At time: 13.44144344329834 and batch: 600, loss is 5.2888194179534915 and perplexity is 198.10940289087299
At time: 14.54441237449646 and batch: 650, loss is 5.178860759735107 and perplexity is 177.48050284137855
At time: 15.648590326309204 and batch: 700, loss is 5.25380163192749 and perplexity is 191.29211008627837
At time: 16.751786470413208 and batch: 750, loss is 5.220640439987182 and perplexity is 185.05266125187183
At time: 17.85824155807495 and batch: 800, loss is 5.1848766803741455 and perplexity is 178.55142953660265
At time: 18.963654041290283 and batch: 850, loss is 5.2097579193115235 and perplexity is 183.0497400225578
At time: 20.067477464675903 and batch: 900, loss is 5.110906457901001 and perplexity is 165.82059615927938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.049810540186216 and perplexity of 155.9929072993652
finished 1 epochs...
Completing Train Step...
At time: 22.291950464248657 and batch: 50, loss is 4.990355043411255 and perplexity is 146.98860156692243
At time: 23.176488161087036 and batch: 100, loss is 4.86001446723938 and perplexity is 129.02606874489845
At time: 24.067768812179565 and batch: 150, loss is 4.840541725158691 and perplexity is 126.5378819203385
At time: 24.945514678955078 and batch: 200, loss is 4.718558721542358 and perplexity is 112.00670342829244
At time: 25.825250387191772 and batch: 250, loss is 4.820904750823974 and perplexity is 124.07729905045942
At time: 26.7028489112854 and batch: 300, loss is 4.747533655166626 and perplexity is 115.29956507672318
At time: 27.58114719390869 and batch: 350, loss is 4.740258817672729 and perplexity is 114.46382310702721
At time: 28.458232164382935 and batch: 400, loss is 4.6056507873535155 and perplexity is 100.04807168727639
At time: 29.335774421691895 and batch: 450, loss is 4.629607887268066 and perplexity is 102.47387492061492
At time: 30.212488651275635 and batch: 500, loss is 4.525973892211914 and perplexity is 92.38585589600189
At time: 31.090280532836914 and batch: 550, loss is 4.588922166824341 and perplexity is 98.38832678863997
At time: 31.969724893569946 and batch: 600, loss is 4.538886213302613 and perplexity is 93.58650664375779
At time: 32.84997606277466 and batch: 650, loss is 4.401249799728394 and perplexity is 81.55272957811493
At time: 33.73815083503723 and batch: 700, loss is 4.448336868286133 and perplexity is 85.48465347281306
At time: 34.62065029144287 and batch: 750, loss is 4.489156417846679 and perplexity is 89.0462963193074
At time: 35.50291132926941 and batch: 800, loss is 4.450411939620972 and perplexity is 85.66222439925562
At time: 36.38452076911926 and batch: 850, loss is 4.499779796600341 and perplexity is 89.99731140446626
At time: 37.26660418510437 and batch: 900, loss is 4.433352098464966 and perplexity is 84.2132353617663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.562592806881422 and perplexity of 95.83163089920042
finished 2 epochs...
Completing Train Step...
At time: 39.370633602142334 and batch: 50, loss is 4.476747169494629 and perplexity is 87.94812653641189
At time: 40.25724244117737 and batch: 100, loss is 4.352570395469666 and perplexity is 77.67786938229878
At time: 41.14069986343384 and batch: 150, loss is 4.354156246185303 and perplexity is 77.80115261561244
At time: 42.02319359779358 and batch: 200, loss is 4.240935945510865 and perplexity is 69.47284421601988
At time: 42.90603733062744 and batch: 250, loss is 4.381845684051513 and perplexity is 79.98552525985666
At time: 43.78829860687256 and batch: 300, loss is 4.34163387298584 and perplexity is 76.832972157374
At time: 44.67046070098877 and batch: 350, loss is 4.34268666267395 and perplexity is 76.9139037126464
At time: 45.554338693618774 and batch: 400, loss is 4.242701396942139 and perplexity is 69.59560347914929
At time: 46.43728041648865 and batch: 450, loss is 4.279604396820068 and perplexity is 72.21186711167563
At time: 47.318708658218384 and batch: 500, loss is 4.149395785331726 and perplexity is 63.39568412143959
At time: 48.20076036453247 and batch: 550, loss is 4.2260794878005985 and perplexity is 68.44835284354593
At time: 49.08354878425598 and batch: 600, loss is 4.215036702156067 and perplexity is 67.6966504383667
At time: 49.96576476097107 and batch: 650, loss is 4.063444128036499 and perplexity is 58.17432627039206
At time: 50.84537053108215 and batch: 700, loss is 4.085202894210815 and perplexity is 59.453999399871215
At time: 51.72682189941406 and batch: 750, loss is 4.171205949783325 and perplexity is 64.7935427700946
At time: 52.60826587677002 and batch: 800, loss is 4.140571002960205 and perplexity is 62.83869228636087
At time: 53.489786863327026 and batch: 850, loss is 4.20315544128418 and perplexity is 66.89708818400489
At time: 54.3822922706604 and batch: 900, loss is 4.14588415145874 and perplexity is 63.17345211676773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.410976044119221 and perplexity of 82.34980133049997
finished 3 epochs...
Completing Train Step...
At time: 56.478782653808594 and batch: 50, loss is 4.211104707717896 and perplexity is 67.43099021489246
At time: 57.361008405685425 and batch: 100, loss is 4.092933588027954 and perplexity is 59.91540124571603
At time: 58.242249965667725 and batch: 150, loss is 4.094232220649719 and perplexity is 59.993259884261775
At time: 59.13535976409912 and batch: 200, loss is 3.982095646858215 and perplexity is 53.629304630601276
At time: 60.03355431556702 and batch: 250, loss is 4.130368847846984 and perplexity is 62.200861357899186
At time: 60.91735005378723 and batch: 300, loss is 4.099431753158569 and perplexity is 60.30600915975309
At time: 61.79865336418152 and batch: 350, loss is 4.1009916496276855 and perplexity is 60.4001536992853
At time: 62.68028402328491 and batch: 400, loss is 4.013014655113221 and perplexity is 55.31337020218196
At time: 63.56207299232483 and batch: 450, loss is 4.054811415672302 and perplexity is 57.67428550417578
At time: 64.4423177242279 and batch: 500, loss is 3.9174865818023683 and perplexity is 50.27392644633443
At time: 65.32317972183228 and batch: 550, loss is 3.9995422172546387 and perplexity is 54.573161662189335
At time: 66.20506596565247 and batch: 600, loss is 4.0037268447875975 and perplexity is 54.802008502489215
At time: 67.08770227432251 and batch: 650, loss is 3.8503406953811647 and perplexity is 47.00907627879978
At time: 67.96875882148743 and batch: 700, loss is 3.863703546524048 and perplexity is 47.64146743164005
At time: 68.847989320755 and batch: 750, loss is 3.9639261388778686 and perplexity is 52.6636855290431
At time: 69.72717213630676 and batch: 800, loss is 3.934591407775879 and perplexity is 51.141249770133385
At time: 70.60796618461609 and batch: 850, loss is 4.000855531692505 and perplexity is 54.6448804676833
At time: 71.48939204216003 and batch: 900, loss is 3.949921407699585 and perplexity is 51.93128527527067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361563904644692 and perplexity of 78.37961686955832
finished 4 epochs...
Completing Train Step...
At time: 73.57772254943848 and batch: 50, loss is 4.020890460014344 and perplexity is 55.750727524078904
At time: 74.46359825134277 and batch: 100, loss is 3.9051187944412233 and perplexity is 49.6559784160248
At time: 75.35604238510132 and batch: 150, loss is 3.909180235862732 and perplexity is 49.858063363890466
At time: 76.25111675262451 and batch: 200, loss is 3.800545725822449 and perplexity is 44.72558574156292
At time: 77.13420414924622 and batch: 250, loss is 3.9487732601165773 and perplexity is 51.87169471152648
At time: 78.01273560523987 and batch: 300, loss is 3.919945731163025 and perplexity is 50.397709678760364
At time: 78.89166522026062 and batch: 350, loss is 3.923428168296814 and perplexity is 50.5735224852477
At time: 79.78309297561646 and batch: 400, loss is 3.8431088018417356 and perplexity is 46.670337979763936
At time: 80.66570043563843 and batch: 450, loss is 3.884348220825195 and perplexity is 48.63523269489856
At time: 81.54505896568298 and batch: 500, loss is 3.749378185272217 and perplexity is 42.494649983794034
At time: 82.42542791366577 and batch: 550, loss is 3.8275626277923585 and perplexity is 45.95040339639164
At time: 83.30446124076843 and batch: 600, loss is 3.8416601753234865 and perplexity is 46.60277903620386
At time: 84.1844973564148 and batch: 650, loss is 3.6866800022125243 and perplexity is 39.91211860482222
At time: 85.06507349014282 and batch: 700, loss is 3.697879557609558 and perplexity is 40.36162904820639
At time: 85.94472455978394 and batch: 750, loss is 3.8026036548614504 and perplexity is 44.81772259628409
At time: 86.82532453536987 and batch: 800, loss is 3.7747377872467043 and perplexity is 43.58607799168505
At time: 87.70487356185913 and batch: 850, loss is 3.8438517904281615 and perplexity is 46.70502639315939
At time: 88.58184099197388 and batch: 900, loss is 3.7927226066589355 and perplexity is 44.37705722224628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353223408738228 and perplexity of 77.72861062722905
finished 5 epochs...
Completing Train Step...
At time: 90.67858457565308 and batch: 50, loss is 3.8684678220748903 and perplexity is 47.86898606049385
At time: 91.56024932861328 and batch: 100, loss is 3.753864402770996 and perplexity is 42.685718493144925
At time: 92.44229626655579 and batch: 150, loss is 3.7621419620513916 and perplexity is 43.040518471478805
At time: 93.32200765609741 and batch: 200, loss is 3.6538714218139647 and perplexity is 38.62390641025433
At time: 94.20779585838318 and batch: 250, loss is 3.7993778228759765 and perplexity is 44.673381089110265
At time: 95.0882773399353 and batch: 300, loss is 3.773468155860901 and perplexity is 43.530774853811415
At time: 95.96908640861511 and batch: 350, loss is 3.7787986373901368 and perplexity is 43.76343438789812
At time: 96.84968495368958 and batch: 400, loss is 3.703990001678467 and perplexity is 40.6090115638277
At time: 97.72808718681335 and batch: 450, loss is 3.7436297225952146 and perplexity is 42.251071845058995
At time: 98.61574220657349 and batch: 500, loss is 3.6112077331542967 and perplexity is 37.010724912947474
At time: 99.49763298034668 and batch: 550, loss is 3.688289818763733 and perplexity is 39.976421538022336
At time: 100.37776350975037 and batch: 600, loss is 3.7070804595947267 and perplexity is 40.73470613193148
At time: 101.25794315338135 and batch: 650, loss is 3.5522447347640993 and perplexity is 34.891551926551635
At time: 102.13981008529663 and batch: 700, loss is 3.5636269283294677 and perplexity is 35.290963099908126
At time: 103.02158522605896 and batch: 750, loss is 3.6684315729141237 and perplexity is 39.190390361878386
At time: 103.90330767631531 and batch: 800, loss is 3.642997341156006 and perplexity is 38.20618223596916
At time: 104.79358911514282 and batch: 850, loss is 3.7106243705749513 and perplexity is 40.87932240651374
At time: 105.67546892166138 and batch: 900, loss is 3.6602254343032836 and perplexity is 38.870104538640405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366713014367509 and perplexity of 78.78424295490873
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 107.76363492012024 and batch: 50, loss is 3.779606761932373 and perplexity is 43.798814987319396
At time: 108.64688730239868 and batch: 100, loss is 3.670600428581238 and perplexity is 39.27548090330764
At time: 109.53691911697388 and batch: 150, loss is 3.674133634567261 and perplexity is 39.4144947051221
At time: 110.42420077323914 and batch: 200, loss is 3.561808481216431 and perplexity is 35.226846663784976
At time: 111.30972719192505 and batch: 250, loss is 3.6973278284072877 and perplexity is 40.33936650082278
At time: 112.20242691040039 and batch: 300, loss is 3.6614129734039307 and perplexity is 38.91629172674424
At time: 113.08265900611877 and batch: 350, loss is 3.655907196998596 and perplexity is 38.702616090881584
At time: 113.96607375144958 and batch: 400, loss is 3.579575138092041 and perplexity is 35.85830278331887
At time: 114.8488256931305 and batch: 450, loss is 3.601196675300598 and perplexity is 36.6420568622556
At time: 115.73218274116516 and batch: 500, loss is 3.465664405822754 and perplexity is 31.99771217852401
At time: 116.61276650428772 and batch: 550, loss is 3.5166468906402586 and perplexity is 33.67133529340017
At time: 117.49159121513367 and batch: 600, loss is 3.535181875228882 and perplexity is 34.30125269512941
At time: 118.36944913864136 and batch: 650, loss is 3.3674241209030153 and perplexity is 29.003720675239013
At time: 119.24849486351013 and batch: 700, loss is 3.3524235105514526 and perplexity is 28.571894091184625
At time: 120.13295340538025 and batch: 750, loss is 3.4532214164733888 and perplexity is 31.602031815142478
At time: 121.01298999786377 and batch: 800, loss is 3.408452854156494 and perplexity is 30.218455716224497
At time: 121.89600777626038 and batch: 850, loss is 3.4597443342208862 and perplexity is 31.808843042347597
At time: 122.78657746315002 and batch: 900, loss is 3.3988466024398805 and perplexity is 29.92955945086024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318834696730522 and perplexity of 75.10106176833412
finished 7 epochs...
Completing Train Step...
At time: 124.87754940986633 and batch: 50, loss is 3.6836447381973265 and perplexity is 39.79115845331367
At time: 125.7636239528656 and batch: 100, loss is 3.5669050121307375 and perplexity is 35.40683965714393
At time: 126.64319491386414 and batch: 150, loss is 3.5665795755386354 and perplexity is 35.39531885065648
At time: 127.52288389205933 and batch: 200, loss is 3.457280812263489 and perplexity is 31.73057770284685
At time: 128.41549491882324 and batch: 250, loss is 3.5932855463027953 and perplexity is 36.353320445097026
At time: 129.29595637321472 and batch: 300, loss is 3.5638860082626342 and perplexity is 35.300107464779
At time: 130.17648029327393 and batch: 350, loss is 3.5609592151641847 and perplexity is 35.1969423989216
At time: 131.0608937740326 and batch: 400, loss is 3.4906994342803954 and perplexity is 32.808887343422
At time: 131.94556283950806 and batch: 450, loss is 3.5165565490722654 and perplexity is 33.668293509575165
At time: 132.82941269874573 and batch: 500, loss is 3.3848966789245605 and perplexity is 29.514943042949714
At time: 133.71309328079224 and batch: 550, loss is 3.43986918926239 and perplexity is 31.182878846123042
At time: 134.59772372245789 and batch: 600, loss is 3.464412040710449 and perplexity is 31.957664442546037
At time: 135.48144936561584 and batch: 650, loss is 3.303544454574585 and perplexity is 27.208908949293036
At time: 136.36533164978027 and batch: 700, loss is 3.2910023975372313 and perplexity is 26.86978436669241
At time: 137.24602127075195 and batch: 750, loss is 3.4002254486083983 and perplexity is 29.970856173605615
At time: 138.12711143493652 and batch: 800, loss is 3.3606130361557005 and perplexity is 28.80684510493145
At time: 139.00858736038208 and batch: 850, loss is 3.4193552350997924 and perplexity is 30.54971128804509
At time: 139.89125728607178 and batch: 900, loss is 3.366483979225159 and perplexity is 28.976465882310155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330772086365582 and perplexity of 76.00294475907162
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 141.9771671295166 and batch: 50, loss is 3.646781601905823 and perplexity is 38.351038305505114
At time: 142.8729989528656 and batch: 100, loss is 3.5467069292068483 and perplexity is 34.698863324832566
At time: 143.75545167922974 and batch: 150, loss is 3.5523181438446043 and perplexity is 34.89411337731168
At time: 144.63519620895386 and batch: 200, loss is 3.436610293388367 and perplexity is 31.08142249853977
At time: 145.51507592201233 and batch: 250, loss is 3.5714916706085207 and perplexity is 35.569611743053926
At time: 146.39373111724854 and batch: 300, loss is 3.541050271987915 and perplexity is 34.50313784698395
At time: 147.27343821525574 and batch: 350, loss is 3.5330335235595705 and perplexity is 34.227640642243216
At time: 148.16162014007568 and batch: 400, loss is 3.46199077129364 and perplexity is 31.880379928071303
At time: 149.0400161743164 and batch: 450, loss is 3.4804851245880126 and perplexity is 32.475472905411294
At time: 149.92062497138977 and batch: 500, loss is 3.3485817956924437 and perplexity is 28.462339594414345
At time: 150.79878640174866 and batch: 550, loss is 3.3962713098526 and perplexity is 29.852581241515956
At time: 151.6813690662384 and batch: 600, loss is 3.4174073791503905 and perplexity is 30.490262768532713
At time: 152.57234835624695 and batch: 650, loss is 3.251864128112793 and perplexity is 25.83846125310371
At time: 153.45990109443665 and batch: 700, loss is 3.227870545387268 and perplexity is 25.22588236443018
At time: 154.34634256362915 and batch: 750, loss is 3.329200086593628 and perplexity is 27.916002385107486
At time: 155.22500205039978 and batch: 800, loss is 3.2878592205047608 and perplexity is 26.78546046949565
At time: 156.10418820381165 and batch: 850, loss is 3.3447430467605592 and perplexity is 28.35328926103646
At time: 156.9851815700531 and batch: 900, loss is 3.291641983985901 and perplexity is 26.88697541364633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325111284647902 and perplexity of 75.57392260914808
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.0775122642517 and batch: 50, loss is 3.630316743850708 and perplexity is 37.724763811938466
At time: 159.97127294540405 and batch: 100, loss is 3.5281139373779298 and perplexity is 34.059668330218656
At time: 160.8493869304657 and batch: 150, loss is 3.532910695075989 and perplexity is 34.223436771229395
At time: 161.7294557094574 and batch: 200, loss is 3.4165307569503782 and perplexity is 30.463546039255515
At time: 162.6109755039215 and batch: 250, loss is 3.5551505136489867 and perplexity is 34.9930865084802
At time: 163.49190306663513 and batch: 300, loss is 3.5255593156814573 and perplexity is 33.97276980621401
At time: 164.38046193122864 and batch: 350, loss is 3.519287133216858 and perplexity is 33.76035324924017
At time: 165.26881647109985 and batch: 400, loss is 3.4454709005355837 and perplexity is 31.35804649132138
At time: 166.15922451019287 and batch: 450, loss is 3.4630152368545533 and perplexity is 31.91305701479346
At time: 167.04216647148132 and batch: 500, loss is 3.3292583274841308 and perplexity is 27.91762828529215
At time: 167.92151522636414 and batch: 550, loss is 3.377131862640381 and perplexity is 29.286652397059775
At time: 168.8133225440979 and batch: 600, loss is 3.398680634498596 and perplexity is 29.924592515682036
At time: 169.692396402359 and batch: 650, loss is 3.230056414604187 and perplexity is 25.281083153033258
At time: 170.57319498062134 and batch: 700, loss is 3.20636137008667 and perplexity is 24.68908813439257
At time: 171.45922470092773 and batch: 750, loss is 3.3063919067382814 and perplexity is 27.286495425419695
At time: 172.34315514564514 and batch: 800, loss is 3.26482207775116 and perplexity is 26.175453380959084
At time: 173.22432708740234 and batch: 850, loss is 3.3210638475418093 and perplexity is 27.689792612615644
At time: 174.10638332366943 and batch: 900, loss is 3.264130640029907 and perplexity is 26.157360940739224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323506394477739 and perplexity of 75.45273203842211
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 176.21080040931702 and batch: 50, loss is 3.6215196895599364 and perplexity is 37.3943524706566
At time: 177.1010057926178 and batch: 100, loss is 3.5184652757644654 and perplexity is 33.732618449913765
At time: 177.98302459716797 and batch: 150, loss is 3.5244976615905763 and perplexity is 33.93672161491142
At time: 178.86939883232117 and batch: 200, loss is 3.4082097673416136 and perplexity is 30.211110900823893
At time: 179.75705885887146 and batch: 250, loss is 3.547298631668091 and perplexity is 34.719400803103206
At time: 180.63948130607605 and batch: 300, loss is 3.518208966255188 and perplexity is 33.7239735669624
At time: 181.5274534225464 and batch: 350, loss is 3.5118571281433106 and perplexity is 33.510443219397104
At time: 182.41170072555542 and batch: 400, loss is 3.4386104345321655 and perplexity is 31.1436519435775
At time: 183.29446148872375 and batch: 450, loss is 3.4568100690841677 and perplexity is 31.71564426500009
At time: 184.17775702476501 and batch: 500, loss is 3.3217073392868044 and perplexity is 27.707616499729554
At time: 185.0685920715332 and batch: 550, loss is 3.3681567335128784 and perplexity is 29.024976952096146
At time: 185.9549913406372 and batch: 600, loss is 3.3910386037826536 and perplexity is 29.696779446452624
At time: 186.8534722328186 and batch: 650, loss is 3.222308793067932 and perplexity is 25.08597168959823
At time: 187.73596787452698 and batch: 700, loss is 3.1996883726119996 and perplexity is 24.52488637987624
At time: 188.6164789199829 and batch: 750, loss is 3.2994233894348146 and perplexity is 27.097009992941626
At time: 189.49811673164368 and batch: 800, loss is 3.257070860862732 and perplexity is 25.973346065375157
At time: 190.3877146244049 and batch: 850, loss is 3.3124826335906983 and perplexity is 27.453197167861582
At time: 191.27275562286377 and batch: 900, loss is 3.25420117855072 and perplexity is 25.898917657607193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322443295831549 and perplexity of 75.37256096357193
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 193.36506962776184 and batch: 50, loss is 3.618460187911987 and perplexity is 37.28011922515721
At time: 194.26018238067627 and batch: 100, loss is 3.5152166748046874 and perplexity is 33.62321243769687
At time: 195.1417076587677 and batch: 150, loss is 3.5216003608703614 and perplexity is 33.838539027856484
At time: 196.03979110717773 and batch: 200, loss is 3.4057318592071533 and perplexity is 30.13634321535597
At time: 196.92461371421814 and batch: 250, loss is 3.545332112312317 and perplexity is 34.65119151882349
At time: 197.80471897125244 and batch: 300, loss is 3.5165749168395997 and perplexity is 33.66891192663635
At time: 198.6868770122528 and batch: 350, loss is 3.50922034740448 and perplexity is 33.42219991841148
At time: 199.56895422935486 and batch: 400, loss is 3.436421422958374 and perplexity is 31.075552691241636
At time: 200.45828652381897 and batch: 450, loss is 3.454803886413574 and perplexity is 31.652080670498886
At time: 201.3486132621765 and batch: 500, loss is 3.3190699434280395 and perplexity is 27.634636827163032
At time: 202.23064851760864 and batch: 550, loss is 3.3654100036621095 and perplexity is 28.945362571136695
At time: 203.11371040344238 and batch: 600, loss is 3.388546795845032 and perplexity is 29.622872894413792
At time: 203.99599385261536 and batch: 650, loss is 3.219990348815918 and perplexity is 25.027878631508337
At time: 204.88527631759644 and batch: 700, loss is 3.1976808309555054 and perplexity is 24.475701036180023
At time: 205.7642683982849 and batch: 750, loss is 3.2972649240493777 and perplexity is 27.038585111548418
At time: 206.65977025032043 and batch: 800, loss is 3.2545701599121095 and perplexity is 25.908475638753014
At time: 207.53701543807983 and batch: 850, loss is 3.3099414777755736 and perplexity is 27.383522880338475
At time: 208.43586897850037 and batch: 900, loss is 3.2513656425476074 and perplexity is 25.825584362881024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32128446396083 and perplexity of 75.28526742678469
Annealing...
Model not improving. Stopping early with 75.10106176833412 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
1162.0708391666412


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}, {'params': {'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.65440681993022}, {'params': {'rnn_dropout': 0.44084991003863383, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.4525796519173104, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.10106176833412}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.5120771028363554, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.34731854513575366, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.310654640197754 and batch: 50, loss is 7.05400707244873 and perplexity is 1157.4875991766828
At time: 2.4183731079101562 and batch: 100, loss is 6.4271024799346925 and perplexity is 618.3795824149284
At time: 3.5281789302825928 and batch: 150, loss is 6.031017894744873 and perplexity is 416.1383989620392
At time: 4.64008355140686 and batch: 200, loss is 5.82862551689148 and perplexity is 339.8911831727341
At time: 5.746020555496216 and batch: 250, loss is 5.814603462219238 and perplexity is 335.158469151533
At time: 6.851262092590332 and batch: 300, loss is 5.644896812438965 and perplexity is 282.844368696354
At time: 7.957066774368286 and batch: 350, loss is 5.578974018096924 and perplexity is 264.79978658030785
At time: 9.069629192352295 and batch: 400, loss is 5.398360567092896 and perplexity is 221.04373261855372
At time: 10.173393487930298 and batch: 450, loss is 5.369783821105957 and perplexity is 214.81642390748945
At time: 11.27962350845337 and batch: 500, loss is 5.302609024047851 and perplexity is 200.86017597003982
At time: 12.385567665100098 and batch: 550, loss is 5.337628135681152 and perplexity is 208.0187324872791
At time: 13.488398313522339 and batch: 600, loss is 5.236558980941773 and perplexity is 188.02200069223744
At time: 14.59768795967102 and batch: 650, loss is 5.123649301528931 and perplexity is 167.94714243410232
At time: 15.705655574798584 and batch: 700, loss is 5.198414134979248 and perplexity is 180.9849963940096
At time: 16.814139366149902 and batch: 750, loss is 5.166127614974975 and perplexity is 175.23494475452773
At time: 17.91911244392395 and batch: 800, loss is 5.126517038345337 and perplexity is 168.42946188975483
At time: 19.02566432952881 and batch: 850, loss is 5.154374704360962 and perplexity is 173.1874795184604
At time: 20.1258864402771 and batch: 900, loss is 5.063544874191284 and perplexity is 158.15014620019446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.009660276648116 and perplexity of 149.8538186590388
finished 1 epochs...
Completing Train Step...
At time: 22.35604691505432 and batch: 50, loss is 4.962346935272217 and perplexity is 142.92884732439404
At time: 23.240018606185913 and batch: 100, loss is 4.837648878097534 and perplexity is 126.17235614049964
At time: 24.124197959899902 and batch: 150, loss is 4.827508392333985 and perplexity is 124.89937240436312
At time: 25.00548553466797 and batch: 200, loss is 4.70478081703186 and perplexity is 110.47406825944253
At time: 25.894548177719116 and batch: 250, loss is 4.811055383682251 and perplexity is 122.86121483910938
At time: 26.7834415435791 and batch: 300, loss is 4.739347848892212 and perplexity is 114.35959761796913
At time: 27.673206090927124 and batch: 350, loss is 4.734329919815064 and perplexity is 113.78718662779686
At time: 28.555086851119995 and batch: 400, loss is 4.600924854278564 and perplexity is 99.57636669725011
At time: 29.433361530303955 and batch: 450, loss is 4.622572002410888 and perplexity is 101.75541101434514
At time: 30.31856942176819 and batch: 500, loss is 4.519025344848632 and perplexity is 91.74613354507598
At time: 31.199853658676147 and batch: 550, loss is 4.583586692810059 and perplexity is 97.86477636459603
At time: 32.0804488658905 and batch: 600, loss is 4.543760957717896 and perplexity is 94.04383070770797
At time: 32.96881604194641 and batch: 650, loss is 4.398099708557129 and perplexity is 81.29623524704324
At time: 33.85050129890442 and batch: 700, loss is 4.441822662353515 and perplexity is 84.9295986694647
At time: 34.742443323135376 and batch: 750, loss is 4.488451528549194 and perplexity is 88.98355065502287
At time: 35.625182151794434 and batch: 800, loss is 4.444369840621948 and perplexity is 85.1462052482133
At time: 36.50786471366882 and batch: 850, loss is 4.498915967941284 and perplexity is 89.91960271596523
At time: 37.40103554725647 and batch: 900, loss is 4.424807472229004 and perplexity is 83.4967302351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561530544333262 and perplexity of 95.72988659591597
finished 2 epochs...
Completing Train Step...
At time: 39.48824334144592 and batch: 50, loss is 4.47746524810791 and perplexity is 88.01130288523365
At time: 40.385948181152344 and batch: 100, loss is 4.353258304595947 and perplexity is 77.73132308109261
At time: 41.270695209503174 and batch: 150, loss is 4.364700427055359 and perplexity is 78.6258422382436
At time: 42.152501344680786 and batch: 200, loss is 4.247255601882935 and perplexity is 69.91327895090357
At time: 43.03473997116089 and batch: 250, loss is 4.386912546157837 and perplexity is 80.39182936102063
At time: 43.92331290245056 and batch: 300, loss is 4.342635273933411 and perplexity is 76.90995130556013
At time: 44.80826377868652 and batch: 350, loss is 4.343892011642456 and perplexity is 77.00666770240878
At time: 45.69097542762756 and batch: 400, loss is 4.245997495651245 and perplexity is 69.82537592624381
At time: 46.58607316017151 and batch: 450, loss is 4.280201249122619 and perplexity is 72.25497979550521
At time: 47.46711444854736 and batch: 500, loss is 4.153454251289368 and perplexity is 63.65349615411537
At time: 48.34592819213867 and batch: 550, loss is 4.2314083909988405 and perplexity is 68.81408108966714
At time: 49.22943353652954 and batch: 600, loss is 4.229865570068359 and perplexity is 68.70799514190347
At time: 50.118112087249756 and batch: 650, loss is 4.066358733177185 and perplexity is 58.344128794245194
At time: 51.001155853271484 and batch: 700, loss is 4.0886924886703495 and perplexity is 59.66183216189867
At time: 51.88394856452942 and batch: 750, loss is 4.176199913024902 and perplexity is 65.11792865239339
At time: 52.76549959182739 and batch: 800, loss is 4.138748164176941 and perplexity is 62.72425181592365
At time: 53.64815139770508 and batch: 850, loss is 4.207568740844726 and perplexity is 67.19297751776438
At time: 54.53052496910095 and batch: 900, loss is 4.1456041479110715 and perplexity is 63.1557658022875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404292694509846 and perplexity of 81.80126389347775
finished 3 epochs...
Completing Train Step...
At time: 56.624927043914795 and batch: 50, loss is 4.21870653629303 and perplexity is 67.94554233375747
At time: 57.509846448898315 and batch: 100, loss is 4.096318197250366 and perplexity is 60.11853503566836
At time: 58.39534521102905 and batch: 150, loss is 4.105178961753845 and perplexity is 60.653598250698124
At time: 59.28560733795166 and batch: 200, loss is 3.9883263158798217 and perplexity is 53.96449422093975
At time: 60.16725945472717 and batch: 250, loss is 4.139769244194031 and perplexity is 62.78833100546232
At time: 61.04864859580994 and batch: 300, loss is 4.1052672481536865 and perplexity is 60.65895337491395
At time: 61.94028186798096 and batch: 350, loss is 4.104312143325806 and perplexity is 60.60104537419899
At time: 62.8250527381897 and batch: 400, loss is 4.019673595428467 and perplexity is 55.68292769808783
At time: 63.70736265182495 and batch: 450, loss is 4.060887408256531 and perplexity is 58.025780795225366
At time: 64.60116314888 and batch: 500, loss is 3.92948007106781 and perplexity is 50.880516538210756
At time: 65.48413896560669 and batch: 550, loss is 4.004541635513306 and perplexity is 54.84667886679517
At time: 66.3643913269043 and batch: 600, loss is 4.019132618904114 and perplexity is 55.65281268788714
At time: 67.24613738059998 and batch: 650, loss is 3.859018225669861 and perplexity is 47.418773973295465
At time: 68.12731409072876 and batch: 700, loss is 3.871037163734436 and perplexity is 47.99213597994049
At time: 69.00720453262329 and batch: 750, loss is 3.9680577087402344 and perplexity is 52.881719325797214
At time: 69.88872838020325 and batch: 800, loss is 3.935031199455261 and perplexity is 51.16374621276704
At time: 70.76950240135193 and batch: 850, loss is 4.00640902519226 and perplexity is 54.9491946775288
At time: 71.65922331809998 and batch: 900, loss is 3.950668454170227 and perplexity is 51.970094853125644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348295864993578 and perplexity of 77.34654160214438
finished 4 epochs...
Completing Train Step...
At time: 73.75596952438354 and batch: 50, loss is 4.0312248229980465 and perplexity is 56.32986312341621
At time: 74.63692235946655 and batch: 100, loss is 3.91229437828064 and perplexity is 50.01357048387187
At time: 75.51717782020569 and batch: 150, loss is 3.9210483741760256 and perplexity is 50.45331100978502
At time: 76.39484763145447 and batch: 200, loss is 3.807359342575073 and perplexity is 45.03136930483851
At time: 77.27482295036316 and batch: 250, loss is 3.959727559089661 and perplexity is 52.443036374084315
At time: 78.1544623374939 and batch: 300, loss is 3.9276638412475586 and perplexity is 50.78818969556647
At time: 79.03544592857361 and batch: 350, loss is 3.9253303956985475 and perplexity is 50.669816382930335
At time: 79.91620087623596 and batch: 400, loss is 3.8497247886657715 and perplexity is 46.98013198744193
At time: 80.7979748249054 and batch: 450, loss is 3.8948967456817627 and perplexity is 49.150978049935176
At time: 81.67823934555054 and batch: 500, loss is 3.767281575202942 and perplexity is 43.26229953248645
At time: 82.56755328178406 and batch: 550, loss is 3.8346607637405397 and perplexity is 46.2777259221621
At time: 83.45256471633911 and batch: 600, loss is 3.854873065948486 and perplexity is 47.2226224020426
At time: 84.33363056182861 and batch: 650, loss is 3.696219973564148 and perplexity is 40.294701084243016
At time: 85.21416902542114 and batch: 700, loss is 3.7097182703018188 and perplexity is 40.84229841757208
At time: 86.09480595588684 and batch: 750, loss is 3.806152868270874 and perplexity is 44.977072875099026
At time: 86.97573113441467 and batch: 800, loss is 3.7778025007247926 and perplexity is 43.71986173206604
At time: 87.85628080368042 and batch: 850, loss is 3.8504029226303103 and perplexity is 47.01200161531836
At time: 88.73622298240662 and batch: 900, loss is 3.797772989273071 and perplexity is 44.60174524316423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.343106204516267 and perplexity of 76.94617908347064
finished 5 epochs...
Completing Train Step...
At time: 90.82541966438293 and batch: 50, loss is 3.879420671463013 and perplexity is 48.396169666209296
At time: 91.71206259727478 and batch: 100, loss is 3.765597677230835 and perplexity is 43.189511535014965
At time: 92.60348868370056 and batch: 150, loss is 3.776603269577026 and perplexity is 43.667462937513974
At time: 93.49325227737427 and batch: 200, loss is 3.661575856208801 and perplexity is 38.92263103776421
At time: 94.37363147735596 and batch: 250, loss is 3.813937854766846 and perplexity is 45.328585264349805
At time: 95.26210951805115 and batch: 300, loss is 3.78248215675354 and perplexity is 43.92493510878358
At time: 96.1389513015747 and batch: 350, loss is 3.782669072151184 and perplexity is 43.9331461228545
At time: 97.01727962493896 and batch: 400, loss is 3.7125704193115237 and perplexity is 40.95895301762915
At time: 97.89376449584961 and batch: 450, loss is 3.755864324569702 and perplexity is 42.771172013749826
At time: 98.7713885307312 and batch: 500, loss is 3.630750241279602 and perplexity is 37.74112094518803
At time: 99.6487410068512 and batch: 550, loss is 3.695354995727539 and perplexity is 40.259862130505944
At time: 100.52540683746338 and batch: 600, loss is 3.7191925621032715 and perplexity is 41.231089120686114
At time: 101.4039249420166 and batch: 650, loss is 3.5627902507781983 and perplexity is 35.261448292230035
At time: 102.28452205657959 and batch: 700, loss is 3.5762592267990114 and perplexity is 35.739596750275574
At time: 103.16544771194458 and batch: 750, loss is 3.6744986820220946 and perplexity is 39.42888549259813
At time: 104.04670143127441 and batch: 800, loss is 3.6483850049972535 and perplexity is 38.412579803613134
At time: 104.93943691253662 and batch: 850, loss is 3.720328269004822 and perplexity is 41.27794215377939
At time: 105.81963109970093 and batch: 900, loss is 3.670458936691284 and perplexity is 39.26992413441392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355251364511986 and perplexity of 77.88640075355822
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 107.93002319335938 and batch: 50, loss is 3.7831675243377685 and perplexity is 43.95505015420562
At time: 108.82623386383057 and batch: 100, loss is 3.6758055305480957 and perplexity is 39.48044675752927
At time: 109.71757316589355 and batch: 150, loss is 3.6935258531570434 and perplexity is 40.18628841172465
At time: 110.60143423080444 and batch: 200, loss is 3.5708381223678587 and perplexity is 35.54637288056423
At time: 111.48484086990356 and batch: 250, loss is 3.7121465253829955 and perplexity is 40.9415944454827
At time: 112.36848783493042 and batch: 300, loss is 3.664836049079895 and perplexity is 39.049733398523585
At time: 113.25185871124268 and batch: 350, loss is 3.6537204504013063 and perplexity is 38.61807574468417
At time: 114.14301800727844 and batch: 400, loss is 3.5823389148712157 and perplexity is 35.95754420528983
At time: 115.03319573402405 and batch: 450, loss is 3.6118520879745484 and perplexity is 37.0345806368961
At time: 115.9261462688446 and batch: 500, loss is 3.4841319131851196 and perplexity is 32.59412029917813
At time: 116.81471586227417 and batch: 550, loss is 3.5232199096679686 and perplexity is 33.89338659521924
At time: 117.69919085502625 and batch: 600, loss is 3.5458795976638795 and perplexity is 34.67016773271981
At time: 118.59201192855835 and batch: 650, loss is 3.3758094835281374 and perplexity is 29.247949934965973
At time: 119.48061203956604 and batch: 700, loss is 3.3678092336654664 and perplexity is 29.014892529303324
At time: 120.36653423309326 and batch: 750, loss is 3.4592806243896486 and perplexity is 31.794096388457003
At time: 121.25741362571716 and batch: 800, loss is 3.4151818323135377 and perplexity is 30.422480714718496
At time: 122.14607763290405 and batch: 850, loss is 3.4744006395339966 and perplexity is 32.278476295111645
At time: 123.02902698516846 and batch: 900, loss is 3.4080780649185183 and perplexity is 30.207132286316188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307900624732449 and perplexity of 74.28437434785485
finished 7 epochs...
Completing Train Step...
At time: 125.12635445594788 and batch: 50, loss is 3.6895739793777467 and perplexity is 40.027790660093075
At time: 126.00897812843323 and batch: 100, loss is 3.573004112243652 and perplexity is 35.623449407699646
At time: 126.9008481502533 and batch: 150, loss is 3.58728102684021 and perplexity is 36.135690261166474
At time: 127.78531575202942 and batch: 200, loss is 3.4691738796234133 and perplexity is 32.1102045902229
At time: 128.6807382106781 and batch: 250, loss is 3.6108349180221557 and perplexity is 36.99692932640992
At time: 129.56075835227966 and batch: 300, loss is 3.567243480682373 and perplexity is 35.41882578723028
At time: 130.4426019191742 and batch: 350, loss is 3.5606360387802125 and perplexity is 35.18556941618895
At time: 131.3234462738037 and batch: 400, loss is 3.4936333751678466 and perplexity is 32.90528802727312
At time: 132.20629286766052 and batch: 450, loss is 3.5282354879379274 and perplexity is 34.06380855359567
At time: 133.0901005268097 and batch: 500, loss is 3.403880934715271 and perplexity is 30.080614710140622
At time: 133.9721713066101 and batch: 550, loss is 3.4469381093978884 and perplexity is 31.40408906381355
At time: 134.8539834022522 and batch: 600, loss is 3.476447868347168 and perplexity is 32.34462540991178
At time: 135.73803639411926 and batch: 650, loss is 3.3101546573638916 and perplexity is 27.389361110746467
At time: 136.62957096099854 and batch: 700, loss is 3.3088148069381713 and perplexity is 27.352688037304848
At time: 137.52583718299866 and batch: 750, loss is 3.4075510597229 and perplexity is 30.191217164700817
At time: 138.40643525123596 and batch: 800, loss is 3.3680429697036742 and perplexity is 29.02167514797248
At time: 139.28742337226868 and batch: 850, loss is 3.4338225984573363 and perplexity is 30.994897633089586
At time: 140.16838836669922 and batch: 900, loss is 3.3756891536712645 and perplexity is 29.24443074507253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32031709853917 and perplexity of 75.21247427678033
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 142.26838946342468 and batch: 50, loss is 3.652681603431702 and perplexity is 38.57797830488287
At time: 143.15757751464844 and batch: 100, loss is 3.548868651390076 and perplexity is 34.773953760503176
At time: 144.04569911956787 and batch: 150, loss is 3.5704952383041384 and perplexity is 35.534186685125924
At time: 144.93118524551392 and batch: 200, loss is 3.451488380432129 and perplexity is 31.547311784617904
At time: 145.81865239143372 and batch: 250, loss is 3.590530161857605 and perplexity is 36.25329094456302
At time: 146.7068419456482 and batch: 300, loss is 3.543415656089783 and perplexity is 34.58484762011346
At time: 147.59771299362183 and batch: 350, loss is 3.528312602043152 and perplexity is 34.06643545499709
At time: 148.48101258277893 and batch: 400, loss is 3.4594267082214354 and perplexity is 31.79874133115288
At time: 149.36163330078125 and batch: 450, loss is 3.49165256023407 and perplexity is 32.84017325279784
At time: 150.24150371551514 and batch: 500, loss is 3.36135995388031 and perplexity is 28.828369485600444
At time: 151.12007069587708 and batch: 550, loss is 3.398477945327759 and perplexity is 29.918527739490386
At time: 151.99794840812683 and batch: 600, loss is 3.4309606552124023 and perplexity is 30.906318809424512
At time: 152.87570714950562 and batch: 650, loss is 3.2554858350753784 and perplexity is 25.93221025135283
At time: 153.75144171714783 and batch: 700, loss is 3.2452494001388548 and perplexity is 25.66811089243158
At time: 154.6309278011322 and batch: 750, loss is 3.3441240644454955 and perplexity is 28.335744506916424
At time: 155.50755858421326 and batch: 800, loss is 3.292889733314514 and perplexity is 26.920544557750397
At time: 156.3970775604248 and batch: 850, loss is 3.3554469633102415 and perplexity is 28.6584105865609
At time: 157.27730584144592 and batch: 900, loss is 3.2983629417419436 and perplexity is 27.068290261788626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313530490822988 and perplexity of 74.70376487658255
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.3802764415741 and batch: 50, loss is 3.6400819969177247 and perplexity is 38.0949602666596
At time: 160.2662091255188 and batch: 100, loss is 3.530869970321655 and perplexity is 34.153667370916345
At time: 161.15667915344238 and batch: 150, loss is 3.5530161666870117 and perplexity is 34.91847876832504
At time: 162.03961896896362 and batch: 200, loss is 3.4352076768875124 and perplexity is 31.03785774193872
At time: 162.93011808395386 and batch: 250, loss is 3.5742032861709596 and perplexity is 35.66619374325422
At time: 163.81186985969543 and batch: 300, loss is 3.528518919944763 and perplexity is 34.07346469557969
At time: 164.69353866577148 and batch: 350, loss is 3.5122528409957887 and perplexity is 33.52370635649509
At time: 165.57675766944885 and batch: 400, loss is 3.444336953163147 and perplexity is 31.322508269942205
At time: 166.45932602882385 and batch: 450, loss is 3.473335428237915 and perplexity is 32.24411120383294
At time: 167.34938859939575 and batch: 500, loss is 3.3432425165176394 and perplexity is 28.310776197059962
At time: 168.23025226593018 and batch: 550, loss is 3.3798196649551393 and perplexity is 29.365475011513855
At time: 169.11600351333618 and batch: 600, loss is 3.4114202308654784 and perplexity is 30.308258431245612
At time: 169.99471426010132 and batch: 650, loss is 3.235658354759216 and perplexity is 25.42310369178628
At time: 170.87454676628113 and batch: 700, loss is 3.222423014640808 and perplexity is 25.088837212390885
At time: 171.75864338874817 and batch: 750, loss is 3.3184832382202147 and perplexity is 27.618428197128964
At time: 172.6408212184906 and batch: 800, loss is 3.2660424852371217 and perplexity is 26.20741760093304
At time: 173.51922416687012 and batch: 850, loss is 3.329240155220032 and perplexity is 27.917120963387546
At time: 174.3990728855133 and batch: 900, loss is 3.2739319705963137 and perplexity is 26.41499841457266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30768825583262 and perplexity of 74.26860033201457
finished 10 epochs...
Completing Train Step...
At time: 176.49214124679565 and batch: 50, loss is 3.6282429599761965 and perplexity is 37.646611868204076
At time: 177.38228917121887 and batch: 100, loss is 3.51522376537323 and perplexity is 33.623450846234505
At time: 178.2651081085205 and batch: 150, loss is 3.5347487545013427 and perplexity is 34.286399328489274
At time: 179.14969038963318 and batch: 200, loss is 3.417532515525818 and perplexity is 30.494078448236586
At time: 180.03356766700745 and batch: 250, loss is 3.5571817779541015 and perplexity is 35.0642389562796
At time: 180.91566729545593 and batch: 300, loss is 3.5120370769500733 and perplexity is 33.5164739262615
At time: 181.80445647239685 and batch: 350, loss is 3.49724467754364 and perplexity is 33.024333797861296
At time: 182.6870608329773 and batch: 400, loss is 3.4305858707427976 and perplexity is 30.89473777145423
At time: 183.57074785232544 and batch: 450, loss is 3.4608563423156737 and perplexity is 31.844234407391752
At time: 184.4534511566162 and batch: 500, loss is 3.332223505973816 and perplexity is 28.00053188741294
At time: 185.3351912498474 and batch: 550, loss is 3.369716110229492 and perplexity is 29.07027313308176
At time: 186.21617770195007 and batch: 600, loss is 3.4038646030426025 and perplexity is 30.080123447399092
At time: 187.09856462478638 and batch: 650, loss is 3.229692850112915 and perplexity is 25.271893519511373
At time: 187.98233127593994 and batch: 700, loss is 3.2190436029434206 and perplexity is 25.00419480376871
At time: 188.86428213119507 and batch: 750, loss is 3.3174692249298094 and perplexity is 27.59043693802978
At time: 189.74709844589233 and batch: 800, loss is 3.26645565032959 and perplexity is 26.218247828231352
At time: 190.62993264198303 and batch: 850, loss is 3.333045687675476 and perplexity is 28.02356287890002
At time: 191.5123996734619 and batch: 900, loss is 3.2790871906280517 and perplexity is 26.551525153864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307352980522261 and perplexity of 74.24370407777121
finished 11 epochs...
Completing Train Step...
At time: 193.59982585906982 and batch: 50, loss is 3.621152248382568 and perplexity is 37.38061476981091
At time: 194.48748874664307 and batch: 100, loss is 3.506725006103516 and perplexity is 33.33890409156616
At time: 195.36737966537476 and batch: 150, loss is 3.5252109479904177 and perplexity is 33.960936852067526
At time: 196.24923849105835 and batch: 200, loss is 3.407935633659363 and perplexity is 30.202830152815594
At time: 197.13163948059082 and batch: 250, loss is 3.5472909593582154 and perplexity is 34.71913442612342
At time: 198.01393365859985 and batch: 300, loss is 3.502531132698059 and perplexity is 33.19937773078214
At time: 198.8945655822754 and batch: 350, loss is 3.488079481124878 and perplexity is 32.72304209986055
At time: 199.77663898468018 and batch: 400, loss is 3.421903123855591 and perplexity is 30.62764779854624
At time: 200.65821743011475 and batch: 450, loss is 3.4527236366271974 and perplexity is 31.586304875207812
At time: 201.53936386108398 and batch: 500, loss is 3.3250175428390505 and perplexity is 27.79948632030623
At time: 202.42180061340332 and batch: 550, loss is 3.3632130146026613 and perplexity is 28.881839731289027
At time: 203.30439949035645 and batch: 600, loss is 3.3986003732681276 and perplexity is 29.922190827447977
At time: 204.19609761238098 and batch: 650, loss is 3.225269775390625 and perplexity is 25.160360886517164
At time: 205.07604265213013 and batch: 700, loss is 3.216018657684326 and perplexity is 24.928672765842258
At time: 205.9571545124054 and batch: 750, loss is 3.3157857513427733 and perplexity is 27.54402824105811
At time: 206.83557033538818 and batch: 800, loss is 3.2655671405792237 and perplexity is 26.194963005326393
At time: 207.7170000076294 and batch: 850, loss is 3.3337745761871336 and perplexity is 28.043996377904847
At time: 208.59757161140442 and batch: 900, loss is 3.280371618270874 and perplexity is 26.58565057785812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307894772046233 and perplexity of 74.28393958599332
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 210.68315410614014 and batch: 50, loss is 3.6178608083724977 and perplexity is 37.25778097967633
At time: 211.5690839290619 and batch: 100, loss is 3.5052027034759523 and perplexity is 33.28819080054917
At time: 212.4495611190796 and batch: 150, loss is 3.5242230319976806 and perplexity is 33.927402866530706
At time: 213.32948470115662 and batch: 200, loss is 3.406689138412476 and perplexity is 30.16520592267115
At time: 214.20772290229797 and batch: 250, loss is 3.546663513183594 and perplexity is 34.69735687087791
At time: 215.08722734451294 and batch: 300, loss is 3.5024165391921995 and perplexity is 33.195573515668954
At time: 215.9664180278778 and batch: 350, loss is 3.486768112182617 and perplexity is 32.68015824317895
At time: 216.84615898132324 and batch: 400, loss is 3.4215565395355223 and perplexity is 30.617034575353284
At time: 217.725834608078 and batch: 450, loss is 3.4499532699584963 and perplexity is 31.49892032850108
At time: 218.60486245155334 and batch: 500, loss is 3.320184659957886 and perplexity is 27.66545878931432
At time: 219.48422122001648 and batch: 550, loss is 3.3587071228027345 and perplexity is 28.751994041480167
At time: 220.36455655097961 and batch: 600, loss is 3.3919186067581175 and perplexity is 29.722924202771654
At time: 221.245276927948 and batch: 650, loss is 3.219305672645569 and perplexity is 25.010748504379062
At time: 222.12620210647583 and batch: 700, loss is 3.207872362136841 and perplexity is 24.72642134828136
At time: 223.00536847114563 and batch: 750, loss is 3.3053091955184937 and perplexity is 27.25696801838584
At time: 223.8825228214264 and batch: 800, loss is 3.2553706645965574 and perplexity is 25.929223798260217
At time: 224.7726969718933 and batch: 850, loss is 3.3225673770904542 and perplexity is 27.73145634748405
At time: 225.66417121887207 and batch: 900, loss is 3.270419454574585 and perplexity is 26.322378069877225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306455847335188 and perplexity of 74.17712745542592
finished 13 epochs...
Completing Train Step...
At time: 227.76199412345886 and batch: 50, loss is 3.6145766735076905 and perplexity is 37.13562210508096
At time: 228.64942908287048 and batch: 100, loss is 3.5014107179641725 and perplexity is 33.1622014891102
At time: 229.54070448875427 and batch: 150, loss is 3.5198305368423464 and perplexity is 33.77870373300173
At time: 230.4217643737793 and batch: 200, loss is 3.4026581335067747 and perplexity is 30.043854577868277
At time: 231.3029317855835 and batch: 250, loss is 3.54285674571991 and perplexity is 34.56552319095307
At time: 232.18382811546326 and batch: 300, loss is 3.4983018159866335 and perplexity is 33.05926355021231
At time: 233.06328058242798 and batch: 350, loss is 3.4833295249938967 and perplexity is 32.56797765162522
At time: 233.94407844543457 and batch: 400, loss is 3.418096218109131 and perplexity is 30.511272884852463
At time: 234.82488203048706 and batch: 450, loss is 3.4467952060699463 and perplexity is 31.39960163561719
At time: 235.70416522026062 and batch: 500, loss is 3.31776819229126 and perplexity is 27.598686811322384
At time: 236.60219883918762 and batch: 550, loss is 3.356546106338501 and perplexity is 28.689927596435936
At time: 237.48401856422424 and batch: 600, loss is 3.390719590187073 and perplexity is 29.687307281019347
At time: 238.3653907775879 and batch: 650, loss is 3.218179178237915 and perplexity is 24.982589899290975
At time: 239.24962759017944 and batch: 700, loss is 3.2074215173721314 and perplexity is 24.715276083247463
At time: 240.13193440437317 and batch: 750, loss is 3.3053334093093873 and perplexity is 27.257628020900384
At time: 241.0130274295807 and batch: 800, loss is 3.255657992362976 and perplexity is 25.9366750546443
At time: 241.89010047912598 and batch: 850, loss is 3.3239248991012573 and perplexity is 27.769127974113236
At time: 242.76993441581726 and batch: 900, loss is 3.2721576166152953 and perplexity is 26.368170413987638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306118899828767 and perplexity of 74.15213786762236
finished 14 epochs...
Completing Train Step...
At time: 244.8623445034027 and batch: 50, loss is 3.612387113571167 and perplexity is 37.05440038706092
At time: 245.75071692466736 and batch: 100, loss is 3.498870916366577 and perplexity is 33.0780829442136
At time: 246.63406682014465 and batch: 150, loss is 3.517009072303772 and perplexity is 33.68353264232418
At time: 247.52269911766052 and batch: 200, loss is 3.399874701499939 and perplexity is 29.96034582580948
At time: 248.40363764762878 and batch: 250, loss is 3.5401432418823244 and perplexity is 34.4718566508573
At time: 249.28599524497986 and batch: 300, loss is 3.495512762069702 and perplexity is 32.967187943319395
At time: 250.16944694519043 and batch: 350, loss is 3.480834789276123 and perplexity is 32.486830417060666
At time: 251.05175518989563 and batch: 400, loss is 3.415678901672363 and perplexity is 30.437606556686088
At time: 251.93434715270996 and batch: 450, loss is 3.444570097923279 and perplexity is 31.32981179997633
At time: 252.8173201084137 and batch: 500, loss is 3.3159085178375243 and perplexity is 27.54740993243154
At time: 253.7000343799591 and batch: 550, loss is 3.3549084568023684 and perplexity is 28.642982000524576
At time: 254.5844428539276 and batch: 600, loss is 3.3896025371551515 and perplexity is 29.654163499538722
At time: 255.46702432632446 and batch: 650, loss is 3.217253408432007 and perplexity is 24.959472474246933
At time: 256.35015416145325 and batch: 700, loss is 3.2069402170181274 and perplexity is 24.703383474307312
At time: 257.2330846786499 and batch: 750, loss is 3.3052010536193848 and perplexity is 27.254020557475265
At time: 258.1145579814911 and batch: 800, loss is 3.2557155799865725 and perplexity is 25.93816872913286
At time: 258.99655866622925 and batch: 850, loss is 3.3246491622924803 and perplexity is 27.789247416363487
At time: 259.8819992542267 and batch: 900, loss is 3.2730928564071657 and perplexity is 26.392842511562986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3060933988388275 and perplexity of 74.15024693881101
finished 15 epochs...
Completing Train Step...
At time: 262.01170325279236 and batch: 50, loss is 3.6104824352264404 and perplexity is 36.983890843383534
At time: 262.90576243400574 and batch: 100, loss is 3.4967126941680906 and perplexity is 33.006770073511134
At time: 263.79815649986267 and batch: 150, loss is 3.514665503501892 and perplexity is 33.60468539414981
At time: 264.6802592277527 and batch: 200, loss is 3.397546820640564 and perplexity is 29.890682825262928
At time: 265.5622048377991 and batch: 250, loss is 3.5378166675567626 and perplexity is 34.3917485390921
At time: 266.4443371295929 and batch: 300, loss is 3.4932078075408937 and perplexity is 32.89128758121276
At time: 267.328084230423 and batch: 350, loss is 3.4786832189559935 and perplexity is 32.417007857948704
At time: 268.2224209308624 and batch: 400, loss is 3.413629245758057 and perplexity is 30.37528382828289
At time: 269.10583329200745 and batch: 450, loss is 3.442672710418701 and perplexity is 31.27042336579966
At time: 270.0002145767212 and batch: 500, loss is 3.3142662239074707 and perplexity is 27.502206117446335
At time: 270.88138914108276 and batch: 550, loss is 3.353463592529297 and perplexity is 28.601626662777356
At time: 271.7640619277954 and batch: 600, loss is 3.388516721725464 and perplexity is 29.621982025988526
At time: 272.64643955230713 and batch: 650, loss is 3.2163561534881593 and perplexity is 24.937087508186103
At time: 273.53630781173706 and batch: 700, loss is 3.2063786363601685 and perplexity is 24.689514426620956
At time: 274.42774200439453 and batch: 750, loss is 3.3049291515350343 and perplexity is 27.246611139842454
At time: 275.3096926212311 and batch: 800, loss is 3.2556009674072266 and perplexity is 25.935196059067252
At time: 276.1947133541107 and batch: 850, loss is 3.3250155448913574 and perplexity is 27.79943077844215
At time: 277.07562136650085 and batch: 900, loss is 3.2735953950881957 and perplexity is 26.406109269077586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306203763778895 and perplexity of 74.15843097797762
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 279.1659240722656 and batch: 50, loss is 3.609480633735657 and perplexity is 36.946858878837425
At time: 280.0413091182709 and batch: 100, loss is 3.496107020378113 and perplexity is 32.98678479087768
At time: 280.9183020591736 and batch: 150, loss is 3.5140864992141725 and perplexity is 33.58523376904972
At time: 281.8066575527191 and batch: 200, loss is 3.3971166038513183 and perplexity is 29.877826117453555
At time: 282.69957304000854 and batch: 250, loss is 3.5378101682662964 and perplexity is 34.391525017855066
At time: 283.5822162628174 and batch: 300, loss is 3.493129081726074 and perplexity is 32.88869828972084
At time: 284.46258306503296 and batch: 350, loss is 3.4783324337005617 and perplexity is 32.40563844379493
At time: 285.3428270816803 and batch: 400, loss is 3.4138260078430176 and perplexity is 30.381261120493175
At time: 286.22120904922485 and batch: 450, loss is 3.4422547721862795 and perplexity is 31.257356990990075
At time: 287.10022044181824 and batch: 500, loss is 3.3128294134140015 and perplexity is 27.462719033624605
At time: 287.98114705085754 and batch: 550, loss is 3.3517545890808105 and perplexity is 28.55278812867681
At time: 288.8600251674652 and batch: 600, loss is 3.386307101249695 and perplexity is 29.556600948256584
At time: 289.736044883728 and batch: 650, loss is 3.2145196437835692 and perplexity is 24.891332332745105
At time: 290.6210627555847 and batch: 700, loss is 3.2040857744216917 and perplexity is 24.6329696281931
At time: 291.50304412841797 and batch: 750, loss is 3.3021571922302244 and perplexity is 27.17118922410303
At time: 292.3797233104706 and batch: 800, loss is 3.252491717338562 and perplexity is 25.85468228253584
At time: 293.2671663761139 and batch: 850, loss is 3.321050696372986 and perplexity is 27.689428461872826
At time: 294.1542155742645 and batch: 900, loss is 3.270191259384155 and perplexity is 26.31637211509206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30572969619542 and perplexity of 74.1232832016785
finished 17 epochs...
Completing Train Step...
At time: 296.2397871017456 and batch: 50, loss is 3.608682370185852 and perplexity is 36.917377316708425
At time: 297.1332149505615 and batch: 100, loss is 3.4953248786926268 and perplexity is 32.960994538553045
At time: 298.015997171402 and batch: 150, loss is 3.5132819509506223 and perplexity is 33.55822369445118
At time: 298.90322947502136 and batch: 200, loss is 3.3963383626937866 and perplexity is 29.854583009016302
At time: 299.78353905677795 and batch: 250, loss is 3.536935806274414 and perplexity is 34.36146751801924
At time: 300.6643304824829 and batch: 300, loss is 3.4922380161285402 and perplexity is 32.85940535502859
At time: 301.5453791618347 and batch: 350, loss is 3.477445764541626 and perplexity is 32.376918098215135
At time: 302.4314205646515 and batch: 400, loss is 3.4130438566207886 and perplexity is 30.357507670580812
At time: 303.32072353363037 and batch: 450, loss is 3.4415550088882445 and perplexity is 31.23549189086371
At time: 304.19973158836365 and batch: 500, loss is 3.3122755908966064 and perplexity is 27.44751377233158
At time: 305.08810806274414 and batch: 550, loss is 3.351316041946411 and perplexity is 28.54026913054988
At time: 305.96904373168945 and batch: 600, loss is 3.386059775352478 and perplexity is 29.54929173932397
At time: 306.8578591346741 and batch: 650, loss is 3.214279751777649 and perplexity is 24.885361817269875
At time: 307.7435390949249 and batch: 700, loss is 3.2039483118057253 and perplexity is 24.629583748470257
At time: 308.62349104881287 and batch: 750, loss is 3.3020667314529417 and perplexity is 27.168731408375763
At time: 309.5117874145508 and batch: 800, loss is 3.252579436302185 and perplexity is 25.85695032794431
At time: 310.40349674224854 and batch: 850, loss is 3.3214133262634276 and perplexity is 27.699471297088404
At time: 311.28551506996155 and batch: 900, loss is 3.2705580949783326 and perplexity is 26.32602766798546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305485973619435 and perplexity of 74.10521988546452
finished 18 epochs...
Completing Train Step...
At time: 313.4175672531128 and batch: 50, loss is 3.6080584001541136 and perplexity is 36.894349164797895
At time: 314.30491948127747 and batch: 100, loss is 3.4946686267852782 and perplexity is 32.93937091906582
At time: 315.1880166530609 and batch: 150, loss is 3.512590446472168 and perplexity is 33.535026054038
At time: 316.071261882782 and batch: 200, loss is 3.3956604528427126 and perplexity is 29.834351151557335
At time: 316.95441699028015 and batch: 250, loss is 3.5362215566635133 and perplexity is 34.33693361592253
At time: 317.83774185180664 and batch: 300, loss is 3.491510910987854 and perplexity is 32.8355217964564
At time: 318.7205924987793 and batch: 350, loss is 3.4767732667922973 and perplexity is 32.35515201330576
At time: 319.60269689559937 and batch: 400, loss is 3.412416305541992 and perplexity is 30.33846276034457
At time: 320.4987769126892 and batch: 450, loss is 3.440972852706909 and perplexity is 31.21731324810042
At time: 321.3842685222626 and batch: 500, loss is 3.3118115043640137 and perplexity is 27.434778706151608
At time: 322.2767791748047 and batch: 550, loss is 3.3509362936019897 and perplexity is 28.529433068216804
At time: 323.1603887081146 and batch: 600, loss is 3.3858381843566896 and perplexity is 29.542744607762096
At time: 324.056165933609 and batch: 650, loss is 3.2140708589553832 and perplexity is 24.880163986720408
At time: 324.9498841762543 and batch: 700, loss is 3.203841233253479 and perplexity is 24.626946589494143
At time: 325.8441479206085 and batch: 750, loss is 3.3020241928100584 and perplexity is 27.16757571199384
At time: 326.7290768623352 and batch: 800, loss is 3.2526226377487184 and perplexity is 25.85806740973102
At time: 327.6127586364746 and batch: 850, loss is 3.321651964187622 and perplexity is 27.706082230198326
At time: 328.4958825111389 and batch: 900, loss is 3.270822296142578 and perplexity is 26.33298395403427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305363903306935 and perplexity of 74.09617439021966
finished 19 epochs...
Completing Train Step...
At time: 330.62826657295227 and batch: 50, loss is 3.607507572174072 and perplexity is 36.87403232102921
At time: 331.5095925331116 and batch: 100, loss is 3.494073224067688 and perplexity is 32.91976456552221
At time: 332.39102029800415 and batch: 150, loss is 3.5119541358947752 and perplexity is 33.51369414982457
At time: 333.27099347114563 and batch: 200, loss is 3.395036211013794 and perplexity is 29.815733113313602
At time: 334.1521599292755 and batch: 250, loss is 3.535582070350647 and perplexity is 34.31498263627292
At time: 335.0340111255646 and batch: 300, loss is 3.4908634662628173 and perplexity is 32.814269491664156
At time: 335.9224262237549 and batch: 350, loss is 3.4761852073669433 and perplexity is 32.33613085453957
At time: 336.8046200275421 and batch: 400, loss is 3.411855812072754 and perplexity is 30.321463014668936
At time: 337.6862623691559 and batch: 450, loss is 3.440447335243225 and perplexity is 31.200912314685166
At time: 338.5678551197052 and batch: 500, loss is 3.311383776664734 and perplexity is 27.423046600627497
At time: 339.44852471351624 and batch: 550, loss is 3.350575647354126 and perplexity is 28.519145890354622
At time: 340.3298499584198 and batch: 600, loss is 3.3856112575531006 and perplexity is 29.53604132776473
At time: 341.21108961105347 and batch: 650, loss is 3.2138659620285033 and perplexity is 24.875066639812452
At time: 342.09304022789 and batch: 700, loss is 3.20373251914978 and perplexity is 24.624269438593494
At time: 342.97432470321655 and batch: 750, loss is 3.3019827795028687 and perplexity is 27.166450636131998
At time: 343.85559844970703 and batch: 800, loss is 3.252635803222656 and perplexity is 25.858407845684578
At time: 344.7386894226074 and batch: 850, loss is 3.32182213306427 and perplexity is 27.71079734425921
At time: 345.62093329429626 and batch: 900, loss is 3.2710255098342897 and perplexity is 26.33833572067507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305305794493793 and perplexity of 74.09186887456292
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
1514.8586955070496


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}, {'params': {'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.65440681993022}, {'params': {'rnn_dropout': 0.44084991003863383, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.4525796519173104, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.10106176833412}, {'params': {'rnn_dropout': 0.5120771028363554, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.34731854513575366, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.09186887456292}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.5151800569574109, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.3408952937289572, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3109023571014404 and batch: 50, loss is 7.084815015792847 and perplexity is 1193.702398971577
At time: 2.4128432273864746 and batch: 100, loss is 6.482003440856934 and perplexity is 653.278440809122
At time: 3.517005205154419 and batch: 150, loss is 6.021475191116333 and perplexity is 412.1862008714218
At time: 4.622777700424194 and batch: 200, loss is 5.804135026931763 and perplexity is 331.66818518650075
At time: 5.725900888442993 and batch: 250, loss is 5.773135385513306 and perplexity is 321.54431920460564
At time: 6.830190181732178 and batch: 300, loss is 5.629066696166992 and perplexity is 278.402162537952
At time: 7.931084871292114 and batch: 350, loss is 5.566630029678345 and perplexity is 261.5511926345533
At time: 9.035008907318115 and batch: 400, loss is 5.392386627197266 and perplexity is 219.727167102674
At time: 10.138826370239258 and batch: 450, loss is 5.3685520648956295 and perplexity is 214.55198533862006
At time: 11.243857383728027 and batch: 500, loss is 5.293291673660279 and perplexity is 198.99738295542997
At time: 12.343607902526855 and batch: 550, loss is 5.329774522781372 and perplexity is 206.3914323433328
At time: 13.451606750488281 and batch: 600, loss is 5.230608797073364 and perplexity is 186.90655705517278
At time: 14.556760549545288 and batch: 650, loss is 5.112544097900391 and perplexity is 166.09237307590823
At time: 15.662099838256836 and batch: 700, loss is 5.196038932800293 and perplexity is 180.55563055341509
At time: 16.765697240829468 and batch: 750, loss is 5.159928989410401 and perplexity is 174.15208852353308
At time: 17.871421813964844 and batch: 800, loss is 5.124425649642944 and perplexity is 168.07757850673312
At time: 18.97969388961792 and batch: 850, loss is 5.149213809967041 and perplexity is 172.29597967333734
At time: 20.088000535964966 and batch: 900, loss is 5.0508153533935545 and perplexity is 156.14972980834025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.006818797490368 and perplexity of 149.42861654417402
finished 1 epochs...
Completing Train Step...
At time: 22.349586963653564 and batch: 50, loss is 4.956441965103149 and perplexity is 142.08734371666935
At time: 23.241901397705078 and batch: 100, loss is 4.831208162307739 and perplexity is 125.36232723723194
At time: 24.13790464401245 and batch: 150, loss is 4.814501972198486 and perplexity is 123.28539746284002
At time: 25.033112049102783 and batch: 200, loss is 4.6934305191040036 and perplexity is 109.22724397055919
At time: 25.917871475219727 and batch: 250, loss is 4.798880977630615 and perplexity is 121.37452069339282
At time: 26.79655694961548 and batch: 300, loss is 4.734419174194336 and perplexity is 113.7973430857559
At time: 27.68647599220276 and batch: 350, loss is 4.726522207260132 and perplexity is 112.902228228188
At time: 28.5937020778656 and batch: 400, loss is 4.591790256500244 and perplexity is 98.67091838824139
At time: 29.477131366729736 and batch: 450, loss is 4.616855325698853 and perplexity is 101.17536776541165
At time: 30.37303113937378 and batch: 500, loss is 4.50933180809021 and perplexity is 90.86108557986358
At time: 31.254716157913208 and batch: 550, loss is 4.578206796646118 and perplexity is 97.3396877572212
At time: 32.135692834854126 and batch: 600, loss is 4.534727125167847 and perplexity is 93.19808042375966
At time: 33.01656770706177 and batch: 650, loss is 4.399072799682617 and perplexity is 81.37538239454578
At time: 33.89639449119568 and batch: 700, loss is 4.436931056976318 and perplexity is 84.51517102254462
At time: 34.77704858779907 and batch: 750, loss is 4.477810678482055 and perplexity is 88.04170991397163
At time: 35.67099571228027 and batch: 800, loss is 4.435675811767578 and perplexity is 84.40915031396685
At time: 36.565505027770996 and batch: 850, loss is 4.492061281204224 and perplexity is 89.30533970327183
At time: 37.44460487365723 and batch: 900, loss is 4.421791763305664 and perplexity is 83.24530769979876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.562489548774614 and perplexity of 95.82173601729369
finished 2 epochs...
Completing Train Step...
At time: 39.53512001037598 and batch: 50, loss is 4.471728572845459 and perplexity is 87.50785605731647
At time: 40.42160892486572 and batch: 100, loss is 4.348024244308472 and perplexity is 77.32553553449418
At time: 41.30486083030701 and batch: 150, loss is 4.34975691318512 and perplexity is 77.45963122144872
At time: 42.18687963485718 and batch: 200, loss is 4.2355425167083744 and perplexity is 69.0991560132455
At time: 43.06978106498718 and batch: 250, loss is 4.3812978553771975 and perplexity is 79.94171889587582
At time: 43.95358395576477 and batch: 300, loss is 4.33884027004242 and perplexity is 76.6186308718237
At time: 44.8354012966156 and batch: 350, loss is 4.333747863769531 and perplexity is 76.22944945134556
At time: 45.71610951423645 and batch: 400, loss is 4.2367067050933835 and perplexity is 69.17964729250521
At time: 46.60023307800293 and batch: 450, loss is 4.274694795608521 and perplexity is 71.85820452294847
At time: 47.48599290847778 and batch: 500, loss is 4.1497323608398435 and perplexity is 63.417025147267054
At time: 48.378241539001465 and batch: 550, loss is 4.224134912490845 and perplexity is 68.31537919714981
At time: 49.26047682762146 and batch: 600, loss is 4.223035197257996 and perplexity is 68.24029302827796
At time: 50.15209889411926 and batch: 650, loss is 4.06889687538147 and perplexity is 58.492402580286836
At time: 51.03523874282837 and batch: 700, loss is 4.0866285419464115 and perplexity is 59.53882030748887
At time: 51.918232917785645 and batch: 750, loss is 4.1697950315475465 and perplexity is 64.70218884065682
At time: 52.80171489715576 and batch: 800, loss is 4.135997171401978 and perplexity is 62.55193498221924
At time: 53.684043645858765 and batch: 850, loss is 4.19947226524353 and perplexity is 66.65114763082414
At time: 54.568119287490845 and batch: 900, loss is 4.143923254013061 and perplexity is 63.049696831243345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411838897287029 and perplexity of 82.4208877816207
finished 3 epochs...
Completing Train Step...
At time: 56.66870474815369 and batch: 50, loss is 4.212885847091675 and perplexity is 67.55120123108384
At time: 57.56873607635498 and batch: 100, loss is 4.08966534614563 and perplexity is 59.719902863984494
At time: 58.4511821269989 and batch: 150, loss is 4.097926902770996 and perplexity is 60.21532588798761
At time: 59.33943963050842 and batch: 200, loss is 3.9843865633010864 and perplexity is 53.75230572521882
At time: 60.2221565246582 and batch: 250, loss is 4.134829692840576 and perplexity is 62.478949551905856
At time: 61.10459589958191 and batch: 300, loss is 4.102223172187805 and perplexity is 60.47458367288946
At time: 61.99303340911865 and batch: 350, loss is 4.095723767280578 and perplexity is 60.082809395950825
At time: 62.87610745429993 and batch: 400, loss is 4.012721042633057 and perplexity is 55.297131890372846
At time: 63.759920835494995 and batch: 450, loss is 4.05427770614624 and perplexity is 57.64351240127294
At time: 64.64016008377075 and batch: 500, loss is 3.92852746963501 and perplexity is 50.83207076367546
At time: 65.52600383758545 and batch: 550, loss is 4.000396800041199 and perplexity is 54.619818880142155
At time: 66.40841627120972 and batch: 600, loss is 4.014327926635742 and perplexity is 55.38605939596012
At time: 67.29076862335205 and batch: 650, loss is 3.858130483627319 and perplexity is 47.376697013540046
At time: 68.17284488677979 and batch: 700, loss is 3.873405656814575 and perplexity is 48.10593974041869
At time: 69.06628084182739 and batch: 750, loss is 3.966348547935486 and perplexity is 52.791413159701364
At time: 69.95128345489502 and batch: 800, loss is 3.9350004768371583 and perplexity is 51.16217435267738
At time: 70.83374166488647 and batch: 850, loss is 3.998961577415466 and perplexity is 54.54148350806783
At time: 71.71603870391846 and batch: 900, loss is 3.9501417875289917 and perplexity is 51.9427311442335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355620501792594 and perplexity of 77.91515683487304
finished 4 epochs...
Completing Train Step...
At time: 73.81953859329224 and batch: 50, loss is 4.02879376411438 and perplexity is 56.19308823052569
At time: 74.69712924957275 and batch: 100, loss is 3.907491569519043 and perplexity is 49.77394077780656
At time: 75.57731008529663 and batch: 150, loss is 3.915775957107544 and perplexity is 50.18800014103283
At time: 76.45793271064758 and batch: 200, loss is 3.8048005390167234 and perplexity is 44.91629017206096
At time: 77.33856105804443 and batch: 250, loss is 3.9580589723587036 and perplexity is 52.35560358434155
At time: 78.21910500526428 and batch: 300, loss is 3.9277875423431396 and perplexity is 50.79447263886986
At time: 79.09861540794373 and batch: 350, loss is 3.9214794397354127 and perplexity is 50.47506438274648
At time: 79.9775493144989 and batch: 400, loss is 3.845354413986206 and perplexity is 46.7752592196319
At time: 80.86388325691223 and batch: 450, loss is 3.8866883373260497 and perplexity is 48.74917807618582
At time: 81.75199842453003 and batch: 500, loss is 3.7625128698348997 and perplexity is 43.05648549574984
At time: 82.636887550354 and batch: 550, loss is 3.830955243110657 and perplexity is 46.10656017900435
At time: 83.51577138900757 and batch: 600, loss is 3.853777151107788 and perplexity is 47.170898776856546
At time: 84.39525651931763 and batch: 650, loss is 3.6942002058029173 and perplexity is 40.21339728108452
At time: 85.27604699134827 and batch: 700, loss is 3.711765670776367 and perplexity is 40.92600461955238
At time: 86.16888236999512 and batch: 750, loss is 3.8105044555664063 and perplexity is 45.17322100233506
At time: 87.0496027469635 and batch: 800, loss is 3.779279980659485 and perplexity is 43.78450469310228
At time: 87.93817925453186 and batch: 850, loss is 3.843247675895691 and perplexity is 46.67681972886162
At time: 88.81917643547058 and batch: 900, loss is 3.7982583904266356 and perplexity is 44.62340023701056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342533477365154 and perplexity of 76.90212253493019
finished 5 epochs...
Completing Train Step...
At time: 90.92470502853394 and batch: 50, loss is 3.879849786758423 and perplexity is 48.41694165932356
At time: 91.8214464187622 and batch: 100, loss is 3.7599414348602296 and perplexity is 42.9459107718451
At time: 92.69971466064453 and batch: 150, loss is 3.7696784114837647 and perplexity is 43.366116548091654
At time: 93.58083534240723 and batch: 200, loss is 3.6635827732086184 and perplexity is 39.00082396478386
At time: 94.46998286247253 and batch: 250, loss is 3.809575538635254 and perplexity is 45.13127831616295
At time: 95.34928703308105 and batch: 300, loss is 3.7852423572540284 and perplexity is 44.04634421631276
At time: 96.22923803329468 and batch: 350, loss is 3.777527403831482 and perplexity is 43.70783618809832
At time: 97.10943007469177 and batch: 400, loss is 3.7081795835494997 and perplexity is 40.7795032375078
At time: 97.99112606048584 and batch: 450, loss is 3.7490129327774047 and perplexity is 42.47913154111876
At time: 98.87083172798157 and batch: 500, loss is 3.625559163093567 and perplexity is 37.54571146728484
At time: 99.75162196159363 and batch: 550, loss is 3.694087266921997 and perplexity is 40.20885588145271
At time: 100.63272333145142 and batch: 600, loss is 3.7227158975601196 and perplexity is 41.376616298896415
At time: 101.51447582244873 and batch: 650, loss is 3.5630983304977417 and perplexity is 35.27231330288938
At time: 102.39368963241577 and batch: 700, loss is 3.5783099603652953 and perplexity is 35.812964343958
At time: 103.28557920455933 and batch: 750, loss is 3.681860523223877 and perplexity is 39.72022577098472
At time: 104.16642808914185 and batch: 800, loss is 3.647847065925598 and perplexity is 38.39192173298352
At time: 105.04858326911926 and batch: 850, loss is 3.7121270418167116 and perplexity is 40.940796764984405
At time: 105.92892742156982 and batch: 900, loss is 3.670918869972229 and perplexity is 39.28798983365321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3461366418289815 and perplexity of 77.17971333236407
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 108.02646970748901 and batch: 50, loss is 3.7780268383026123 and perplexity is 43.72967084018427
At time: 108.91441226005554 and batch: 100, loss is 3.673403944969177 and perplexity is 39.385744848833234
At time: 109.79561734199524 and batch: 150, loss is 3.6877472686767576 and perplexity is 39.95473820971764
At time: 110.67877316474915 and batch: 200, loss is 3.5655976724624634 and perplexity is 35.36058113552059
At time: 111.55874848365784 and batch: 250, loss is 3.708146266937256 and perplexity is 40.77814462524325
At time: 112.44842457771301 and batch: 300, loss is 3.6706807231903076 and perplexity is 39.278634639305096
At time: 113.33593916893005 and batch: 350, loss is 3.652125277519226 and perplexity is 38.55652234470763
At time: 114.21767687797546 and batch: 400, loss is 3.578415093421936 and perplexity is 35.81672966829341
At time: 115.1089699268341 and batch: 450, loss is 3.604455885887146 and perplexity is 36.76167586777127
At time: 115.99745392799377 and batch: 500, loss is 3.4742774534225465 and perplexity is 32.274500280033266
At time: 116.88016390800476 and batch: 550, loss is 3.5236856603622435 and perplexity is 33.90917614026765
At time: 117.76118397712708 and batch: 600, loss is 3.5465247583389283 and perplexity is 34.69254277851203
At time: 118.64259481430054 and batch: 650, loss is 3.37344117641449 and perplexity is 29.17876376646097
At time: 119.52548027038574 and batch: 700, loss is 3.3725014972686767 and perplexity is 29.151357968993796
At time: 120.40640020370483 and batch: 750, loss is 3.4634899997711184 and perplexity is 31.92821174798632
At time: 121.28738307952881 and batch: 800, loss is 3.4151131868362428 and perplexity is 30.420392420686127
At time: 122.17017698287964 and batch: 850, loss is 3.4629709482192994 and perplexity is 31.911643660349498
At time: 123.05240797996521 and batch: 900, loss is 3.411449112892151 and perplexity is 30.309133807815286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302005297517123 and perplexity of 73.84773199348646
finished 7 epochs...
Completing Train Step...
At time: 125.1974618434906 and batch: 50, loss is 3.686057281494141 and perplexity is 39.887272238628825
At time: 126.0788242816925 and batch: 100, loss is 3.57037118434906 and perplexity is 35.529778802140456
At time: 126.96087169647217 and batch: 150, loss is 3.5820190572738646 and perplexity is 35.94604475078566
At time: 127.84748840332031 and batch: 200, loss is 3.4662548303604126 and perplexity is 32.01660999126034
At time: 128.73385787010193 and batch: 250, loss is 3.607837052345276 and perplexity is 36.8861835852017
At time: 129.61523747444153 and batch: 300, loss is 3.5743131971359254 and perplexity is 35.67011406446439
At time: 130.49442648887634 and batch: 350, loss is 3.5571630334854127 and perplexity is 35.06358170191035
At time: 131.38353514671326 and batch: 400, loss is 3.4909009027481077 and perplexity is 32.81549796557603
At time: 132.28537607192993 and batch: 450, loss is 3.521128816604614 and perplexity is 33.82258642029254
At time: 133.16530394554138 and batch: 500, loss is 3.39510892868042 and perplexity is 29.817901322686954
At time: 134.04293394088745 and batch: 550, loss is 3.446133937835693 and perplexity is 31.378844940115414
At time: 134.93794107437134 and batch: 600, loss is 3.4780996656417846 and perplexity is 32.39809632405727
At time: 135.81644868850708 and batch: 650, loss is 3.3103589820861816 and perplexity is 27.394958006121698
At time: 136.692875623703 and batch: 700, loss is 3.3143585538864135 and perplexity is 27.504745512787395
At time: 137.57201385498047 and batch: 750, loss is 3.4112067222595215 and perplexity is 30.30178804800481
At time: 138.4512858390808 and batch: 800, loss is 3.3678798961639402 and perplexity is 29.01694286654252
At time: 139.3297998905182 and batch: 850, loss is 3.4237239360809326 and perplexity is 30.683465795715723
At time: 140.20896553993225 and batch: 900, loss is 3.3792478132247923 and perplexity is 29.348687114367568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313960663259846 and perplexity of 74.7359072900565
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 142.28948092460632 and batch: 50, loss is 3.648367233276367 and perplexity is 38.411897152032296
At time: 143.17315340042114 and batch: 100, loss is 3.5469607734680175 and perplexity is 34.70767255019502
At time: 144.05410885810852 and batch: 150, loss is 3.5604910802841188 and perplexity is 35.18046933862089
At time: 144.9306161403656 and batch: 200, loss is 3.447400302886963 and perplexity is 31.418607184141063
At time: 145.8045732975006 and batch: 250, loss is 3.5851773118972776 and perplexity is 36.059750974903025
At time: 146.6807291507721 and batch: 300, loss is 3.5476352310180665 and perplexity is 34.731089297904596
At time: 147.5538511276245 and batch: 350, loss is 3.5251878690719605 and perplexity is 33.960153079419534
At time: 148.42554783821106 and batch: 400, loss is 3.4590767097473143 and perplexity is 31.787613767636724
At time: 149.29644894599915 and batch: 450, loss is 3.4841759634017944 and perplexity is 32.59555610886326
At time: 150.17063117027283 and batch: 500, loss is 3.354168405532837 and perplexity is 28.62179256693262
At time: 151.0566611289978 and batch: 550, loss is 3.3971817970275877 and perplexity is 29.87977401133218
At time: 151.9355022907257 and batch: 600, loss is 3.428613739013672 and perplexity is 30.833869318863446
At time: 152.81275629997253 and batch: 650, loss is 3.2522611904144285 and perplexity is 25.848722769095346
At time: 153.69963932037354 and batch: 700, loss is 3.2493830871582032 and perplexity is 25.774434431957335
At time: 154.57616066932678 and batch: 750, loss is 3.344222569465637 and perplexity is 28.338535857478572
At time: 155.4525957107544 and batch: 800, loss is 3.2938767576217653 and perplexity is 26.94712890713461
At time: 156.32887196540833 and batch: 850, loss is 3.3455171728134157 and perplexity is 28.37524677878433
At time: 157.20612692832947 and batch: 900, loss is 3.3067901134490967 and perplexity is 27.297363254690392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308662728087543 and perplexity of 74.34100829649647
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 159.28782081604004 and batch: 50, loss is 3.6357581186294556 and perplexity is 37.93059789259588
At time: 160.169903755188 and batch: 100, loss is 3.5313687181472777 and perplexity is 34.170705686812944
At time: 161.04816794395447 and batch: 150, loss is 3.542661256790161 and perplexity is 34.558766674251885
At time: 161.92789149284363 and batch: 200, loss is 3.4309843969345093 and perplexity is 30.907052587367573
At time: 162.80754375457764 and batch: 250, loss is 3.5701932764053343 and perplexity is 35.523458334500106
At time: 163.685045003891 and batch: 300, loss is 3.5302117538452147 and perplexity is 34.13119426121886
At time: 164.5623631477356 and batch: 350, loss is 3.5079161596298216 and perplexity is 33.37863950552902
At time: 165.43883061408997 and batch: 400, loss is 3.4414171981811523 and perplexity is 31.231187602235202
At time: 166.31561398506165 and batch: 450, loss is 3.4657681655883787 and perplexity is 32.00103242589125
At time: 167.19316411018372 and batch: 500, loss is 3.3342306280136107 and perplexity is 28.056788810459107
At time: 168.07240653038025 and batch: 550, loss is 3.379488258361816 and perplexity is 29.355744711910756
At time: 168.95675420761108 and batch: 600, loss is 3.4116365575790404 and perplexity is 30.3148156264085
At time: 169.83824682235718 and batch: 650, loss is 3.2328281831741332 and perplexity is 25.35125366802011
At time: 170.75669503211975 and batch: 700, loss is 3.22922812461853 and perplexity is 25.260151754861482
At time: 171.64814615249634 and batch: 750, loss is 3.321094427108765 and perplexity is 27.69063936742946
At time: 172.53529262542725 and batch: 800, loss is 3.270019555091858 and perplexity is 26.311853868954433
At time: 173.42593717575073 and batch: 850, loss is 3.3205141162872316 and perplexity is 27.674574851406437
At time: 174.3100619316101 and batch: 900, loss is 3.2832398223876953 and perplexity is 26.662013109607575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304088853809931 and perplexity of 74.00175830571548
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 176.41547322273254 and batch: 50, loss is 3.62684220790863 and perplexity is 37.59391521488224
At time: 177.29341793060303 and batch: 100, loss is 3.5219655323028562 and perplexity is 33.85089815208903
At time: 178.17255783081055 and batch: 150, loss is 3.534649996757507 and perplexity is 34.28301344824105
At time: 179.05088257789612 and batch: 200, loss is 3.421243567466736 and perplexity is 30.607453798038183
At time: 179.93261098861694 and batch: 250, loss is 3.563316888809204 and perplexity is 35.28002320262687
At time: 180.8153223991394 and batch: 300, loss is 3.523303828239441 and perplexity is 33.89623099915215
At time: 181.69549822807312 and batch: 350, loss is 3.5000416803359986 and perplexity is 33.11683225062201
At time: 182.57648634910583 and batch: 400, loss is 3.434986934661865 and perplexity is 31.031007132278667
At time: 183.458416223526 and batch: 450, loss is 3.457795033454895 and perplexity is 31.746898434190534
At time: 184.34012413024902 and batch: 500, loss is 3.3268497943878175 and perplexity is 27.850468664147837
At time: 185.22177290916443 and batch: 550, loss is 3.372561802864075 and perplexity is 29.15311601200217
At time: 186.10314011573792 and batch: 600, loss is 3.40554340839386 and perplexity is 30.130664532060397
At time: 186.98488354682922 and batch: 650, loss is 3.2259498548507692 and perplexity is 25.17747775092013
At time: 187.8650918006897 and batch: 700, loss is 3.2225408601760863 and perplexity is 25.091793994060026
At time: 188.7462499141693 and batch: 750, loss is 3.313692045211792 and perplexity is 27.486419469221794
At time: 189.62770009040833 and batch: 800, loss is 3.2614995718002318 and perplexity is 26.088629597472227
At time: 190.51264643669128 and batch: 850, loss is 3.3126404762268065 and perplexity is 27.457530794878714
At time: 191.39323139190674 and batch: 900, loss is 3.27458300113678 and perplexity is 26.432200984358467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303542045697774 and perplexity of 73.96130460517377
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 193.4789686203003 and batch: 50, loss is 3.623651580810547 and perplexity is 37.47415820185493
At time: 194.36888194084167 and batch: 100, loss is 3.518684973716736 and perplexity is 33.74003025126091
At time: 195.25047707557678 and batch: 150, loss is 3.532186312675476 and perplexity is 34.19865489280919
At time: 196.13049912452698 and batch: 200, loss is 3.4185492849349974 and perplexity is 30.525099662402138
At time: 197.0098421573639 and batch: 250, loss is 3.5605384874343873 and perplexity is 35.18213718395093
At time: 197.88843083381653 and batch: 300, loss is 3.5207826709747314 and perplexity is 33.810880905835035
At time: 198.76947736740112 and batch: 350, loss is 3.497207350730896 and perplexity is 33.02310112774358
At time: 199.65075063705444 and batch: 400, loss is 3.4325466775894165 and perplexity is 30.95537581511834
At time: 200.52985286712646 and batch: 450, loss is 3.455486288070679 and perplexity is 31.673687474220106
At time: 201.40988636016846 and batch: 500, loss is 3.3243082904815675 and perplexity is 27.77977645955557
At time: 202.29071950912476 and batch: 550, loss is 3.3701601314544676 and perplexity is 29.083183817465514
At time: 203.17090344429016 and batch: 600, loss is 3.4036702632904055 and perplexity is 30.07427825165761
At time: 204.0511085987091 and batch: 650, loss is 3.2239792442321775 and perplexity is 25.127911599746767
At time: 204.93077063560486 and batch: 700, loss is 3.2204177713394166 and perplexity is 25.038578397048333
At time: 205.81047129631042 and batch: 750, loss is 3.3116035747528074 and perplexity is 27.429074796308136
At time: 206.69159388542175 and batch: 800, loss is 3.2587037801742555 and perplexity is 26.01579309060198
At time: 207.5703239440918 and batch: 850, loss is 3.310306043624878 and perplexity is 27.393507797583624
At time: 208.4494023323059 and batch: 900, loss is 3.271781158447266 and perplexity is 26.35824576908333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302750260862585 and perplexity of 73.90276634370875
Annealing...
Model not improving. Stopping early with 73.84773199348646 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb71e11ab70>
ELAPSED
1730.9261319637299


RESULTS SO FAR:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}, {'params': {'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.65440681993022}, {'params': {'rnn_dropout': 0.44084991003863383, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.4525796519173104, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.10106176833412}, {'params': {'rnn_dropout': 0.5120771028363554, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.34731854513575366, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.09186887456292}, {'params': {'rnn_dropout': 0.5151800569574109, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.3408952937289572, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -73.84773199348646}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'rnn_dropout': 0.3024818242283678, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.5660111436865694, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.75537438373719}, {'params': {'rnn_dropout': 0.1383256268569013, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.9653578580902311, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.67075580146093}, {'params': {'rnn_dropout': 0.6834656264167353, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.33042925864422856, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.65440681993022}, {'params': {'rnn_dropout': 0.44084991003863383, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.4525796519173104, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.10106176833412}, {'params': {'rnn_dropout': 0.5120771028363554, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.34731854513575366, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.09186887456292}, {'params': {'rnn_dropout': 0.5151800569574109, 'batch_size': 32, 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'dropout': 0.3408952937289572, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -73.84773199348646}]
