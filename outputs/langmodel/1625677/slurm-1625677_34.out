FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'rnn_dropout', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2258610725402832 and batch: 50, loss is 6.809361486434937 and perplexity is 906.2919424616565
At time: 1.8618013858795166 and batch: 100, loss is 5.938715553283691 and perplexity is 379.4472366403059
At time: 2.4764676094055176 and batch: 150, loss is 5.705913677215576 and perplexity is 300.6400425557215
At time: 3.0933260917663574 and batch: 200, loss is 5.448558025360107 and perplexity is 232.42277640488834
At time: 3.7104620933532715 and batch: 250, loss is 5.431261596679687 and perplexity is 228.43725939503793
At time: 4.33296275138855 and batch: 300, loss is 5.3220265674591065 and perplexity is 204.79849970085615
At time: 4.94727635383606 and batch: 350, loss is 5.2790521717071535 and perplexity is 196.1838386074497
At time: 5.558942794799805 and batch: 400, loss is 5.120740013122559 and perplexity is 167.45924581984514
At time: 6.171141862869263 and batch: 450, loss is 5.107776136398315 and perplexity is 165.30233596524735
At time: 6.7932281494140625 and batch: 500, loss is 5.031404495239258 and perplexity is 153.14795729286942
At time: 7.414829730987549 and batch: 550, loss is 5.079533472061157 and perplexity is 160.69906783305638
At time: 8.033646821975708 and batch: 600, loss is 4.990903263092041 and perplexity is 147.06920570352085
At time: 8.661585569381714 and batch: 650, loss is 4.874535608291626 and perplexity is 130.91334402079082
At time: 9.274823904037476 and batch: 700, loss is 4.941312704086304 and perplexity is 139.95384701594492
At time: 9.89073634147644 and batch: 750, loss is 4.936651706695557 and perplexity is 139.3030403824141
At time: 10.507890224456787 and batch: 800, loss is 4.890352764129639 and perplexity is 133.00048357662783
At time: 11.125574350357056 and batch: 850, loss is 4.927531394958496 and perplexity is 138.0383292745395
At time: 11.743367195129395 and batch: 900, loss is 4.846156015396118 and perplexity is 127.25030030596241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.822806580425942 and perplexity of 124.31349746410001
finished 1 epochs...
Completing Train Step...
At time: 13.343875408172607 and batch: 50, loss is 4.791232757568359 and perplexity is 120.4497625339599
At time: 13.950162649154663 and batch: 100, loss is 4.672863664627076 and perplexity is 107.00372689257689
At time: 14.55794906616211 and batch: 150, loss is 4.660157775878906 and perplexity is 105.65275029724663
At time: 15.163665771484375 and batch: 200, loss is 4.542974195480347 and perplexity is 93.96986967172288
At time: 15.76878809928894 and batch: 250, loss is 4.6547524356842045 and perplexity is 105.08320192747271
At time: 16.375324964523315 and batch: 300, loss is 4.599639081954956 and perplexity is 99.44841643594513
At time: 16.9812114238739 and batch: 350, loss is 4.591568861007691 and perplexity is 98.6490755097108
At time: 17.58688974380493 and batch: 400, loss is 4.481803350448608 and perplexity is 88.39393427127801
At time: 18.19156527519226 and batch: 450, loss is 4.512234754562378 and perplexity is 91.12523366608238
At time: 18.79706573486328 and batch: 500, loss is 4.403403091430664 and perplexity is 81.72852559614083
At time: 19.401508331298828 and batch: 550, loss is 4.47794285774231 and perplexity is 88.05334797119754
At time: 20.007192134857178 and batch: 600, loss is 4.446954374313354 and perplexity is 85.36655310995208
At time: 20.61796498298645 and batch: 650, loss is 4.301263151168823 and perplexity is 73.79294650079846
At time: 21.228729248046875 and batch: 700, loss is 4.336794056892395 and perplexity is 76.46201311310199
At time: 21.860992670059204 and batch: 750, loss is 4.397244806289673 and perplexity is 81.22676461072827
At time: 22.472378492355347 and batch: 800, loss is 4.357271385192871 and perplexity is 78.04389190798129
At time: 23.083592176437378 and batch: 850, loss is 4.417373361587525 and perplexity is 82.87830786264222
At time: 23.695164442062378 and batch: 900, loss is 4.359331331253052 and perplexity is 78.20482381425352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.528411447185359 and perplexity of 92.6113261849901
finished 2 epochs...
Completing Train Step...
At time: 25.24614429473877 and batch: 50, loss is 4.418592185974121 and perplexity is 82.97938354962852
At time: 25.872173070907593 and batch: 100, loss is 4.300236577987671 and perplexity is 73.71723151110086
At time: 26.484126091003418 and batch: 150, loss is 4.299006309509277 and perplexity is 73.62659528973332
At time: 27.096342086791992 and batch: 200, loss is 4.189549508094788 and perplexity is 65.99305492594705
At time: 27.708295106887817 and batch: 250, loss is 4.325027933120728 and perplexity is 75.5676236698008
At time: 28.321993589401245 and batch: 300, loss is 4.288927745819092 and perplexity is 72.88827182956327
At time: 28.934807538986206 and batch: 350, loss is 4.289709177017212 and perplexity is 72.94525125900036
At time: 29.54856824874878 and batch: 400, loss is 4.205310349464416 and perplexity is 67.04140070083365
At time: 30.16062831878662 and batch: 450, loss is 4.241685066223145 and perplexity is 69.52490726088101
At time: 30.772944927215576 and batch: 500, loss is 4.1186995029449465 and perplexity is 61.47923668593798
At time: 31.385349988937378 and batch: 550, loss is 4.200093297958374 and perplexity is 66.69255302970764
At time: 31.997080087661743 and batch: 600, loss is 4.190366606712342 and perplexity is 66.04699979603322
At time: 32.6091890335083 and batch: 650, loss is 4.042458562850952 and perplexity is 56.966225825984296
At time: 33.22279334068298 and batch: 700, loss is 4.061294193267822 and perplexity is 58.04938961465273
At time: 33.835710287094116 and batch: 750, loss is 4.146618576049804 and perplexity is 63.21986529490819
At time: 34.4483118057251 and batch: 800, loss is 4.1142015409469606 and perplexity is 61.203326397831646
At time: 35.0608868598938 and batch: 850, loss is 4.180408539772034 and perplexity is 65.39256322045925
At time: 35.67418932914734 and batch: 900, loss is 4.129821305274963 and perplexity is 62.166813060586136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43198133494756 and perplexity of 84.09787801293523
finished 3 epochs...
Completing Train Step...
At time: 37.245558977127075 and batch: 50, loss is 4.208243541717529 and perplexity is 67.23833469945134
At time: 37.863725423812866 and batch: 100, loss is 4.089482908248901 and perplexity is 59.709008684299
At time: 38.48616600036621 and batch: 150, loss is 4.096271648406982 and perplexity is 60.115736652527744
At time: 39.114269495010376 and batch: 200, loss is 3.9843072175979612 and perplexity is 53.74804087992727
At time: 39.73323440551758 and batch: 250, loss is 4.1302361488342285 and perplexity is 62.19260791262855
At time: 40.352373123168945 and batch: 300, loss is 4.098891196250915 and perplexity is 60.27341913910269
At time: 40.97257113456726 and batch: 350, loss is 4.1039283657073975 and perplexity is 60.577792511581755
At time: 41.59270644187927 and batch: 400, loss is 4.027523350715637 and perplexity is 56.1217451055472
At time: 42.214755058288574 and batch: 450, loss is 4.065859217643737 and perplexity is 58.31499227330524
At time: 42.83173871040344 and batch: 500, loss is 3.943624758720398 and perplexity is 51.60531952379677
At time: 43.44844627380371 and batch: 550, loss is 4.023508172035218 and perplexity is 55.896858054068055
At time: 44.06495547294617 and batch: 600, loss is 4.021011986732483 and perplexity is 55.757503138729554
At time: 44.68103337287903 and batch: 650, loss is 3.8756293439865113 and perplexity is 48.21303132647773
At time: 45.29767894744873 and batch: 700, loss is 3.8879475212097168 and perplexity is 48.81060091877239
At time: 45.91498279571533 and batch: 750, loss is 3.9815825605392456 and perplexity is 53.60179522605253
At time: 46.531925439834595 and batch: 800, loss is 3.9536894941329956 and perplexity is 52.12733598282331
At time: 47.14903283119202 and batch: 850, loss is 4.01952232837677 and perplexity is 55.67450534281357
At time: 47.766509771347046 and batch: 900, loss is 3.9709211683273313 and perplexity is 53.03336099826903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392397527825342 and perplexity of 80.83398858035882
finished 4 epochs...
Completing Train Step...
At time: 49.43370747566223 and batch: 50, loss is 4.056627769470214 and perplexity is 57.77913760712406
At time: 50.0529408454895 and batch: 100, loss is 3.9468674087524414 and perplexity is 51.77292911756673
At time: 50.67269968986511 and batch: 150, loss is 3.9536347246170043 and perplexity is 52.124481072043366
At time: 51.29197907447815 and batch: 200, loss is 3.84150869846344 and perplexity is 46.595720328194986
At time: 51.917468786239624 and batch: 250, loss is 3.991427845954895 and perplexity is 54.13212654672401
At time: 52.543715715408325 and batch: 300, loss is 3.9625779390335083 and perplexity is 52.59273219679247
At time: 53.17656493186951 and batch: 350, loss is 3.9653091716766355 and perplexity is 52.73657152367638
At time: 53.79673647880554 and batch: 400, loss is 3.894946789741516 and perplexity is 49.153437825965725
At time: 54.41725707054138 and batch: 450, loss is 3.932326035499573 and perplexity is 51.025526927877706
At time: 55.038268089294434 and batch: 500, loss is 3.8164674615859986 and perplexity is 45.44339391188505
At time: 55.65873980522156 and batch: 550, loss is 3.891772623062134 and perplexity is 48.997663978309276
At time: 56.28193783760071 and batch: 600, loss is 3.8936110877990724 and perplexity is 49.08782731138432
At time: 56.91349482536316 and batch: 650, loss is 3.7508047676086425 and perplexity is 42.55531536264393
At time: 57.532328367233276 and batch: 700, loss is 3.760809774398804 and perplexity is 42.9832185997752
At time: 58.151939392089844 and batch: 750, loss is 3.8591428184509278 and perplexity is 47.424682378284245
At time: 58.77122926712036 and batch: 800, loss is 3.8338438081741333 and perplexity is 46.23993451542129
At time: 59.40170764923096 and batch: 850, loss is 3.898416814804077 and perplexity is 49.324297759814456
At time: 60.02290654182434 and batch: 900, loss is 3.8521938133239746 and perplexity is 47.09627040707637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380417915239726 and perplexity of 79.87140590890752
finished 5 epochs...
Completing Train Step...
At time: 61.57795572280884 and batch: 50, loss is 3.9431026792526245 and perplexity is 51.57838447777456
At time: 62.204264879226685 and batch: 100, loss is 3.8337062883377073 and perplexity is 46.23357604440835
At time: 62.815853118896484 and batch: 150, loss is 3.841405258178711 and perplexity is 46.59090070289307
At time: 63.42803478240967 and batch: 200, loss is 3.7365087556838987 and perplexity is 41.95127205951626
At time: 64.03934288024902 and batch: 250, loss is 3.881451334953308 and perplexity is 48.494545851679376
At time: 64.65855097770691 and batch: 300, loss is 3.854343729019165 and perplexity is 47.19763233877101
At time: 65.27653360366821 and batch: 350, loss is 3.860367112159729 and perplexity is 47.48277967538274
At time: 65.8896017074585 and batch: 400, loss is 3.789226460456848 and perplexity is 44.22217943762454
At time: 66.50075197219849 and batch: 450, loss is 3.8269386529922484 and perplexity is 45.921740446026746
At time: 67.11225175857544 and batch: 500, loss is 3.7152163219451904 and perplexity is 41.06746991855895
At time: 67.72504496574402 and batch: 550, loss is 3.7890218019485475 and perplexity is 44.21312991840918
At time: 68.33715057373047 and batch: 600, loss is 3.7932541179656982 and perplexity is 44.40065039938553
At time: 68.97828507423401 and batch: 650, loss is 3.655514831542969 and perplexity is 38.68743350004198
At time: 69.59647154808044 and batch: 700, loss is 3.662310128211975 and perplexity is 38.951221331266595
At time: 70.21590852737427 and batch: 750, loss is 3.761287260055542 and perplexity is 43.00374737084386
At time: 70.8388922214508 and batch: 800, loss is 3.739476466178894 and perplexity is 42.07595621160811
At time: 71.45029425621033 and batch: 850, loss is 3.8001657629013064 and perplexity is 44.708594905503006
At time: 72.06090474128723 and batch: 900, loss is 3.7548331022262573 and perplexity is 42.72708815954828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382874371254281 and perplexity of 80.06784768084914
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 73.6142189502716 and batch: 50, loss is 3.870630578994751 and perplexity is 47.97262707610621
At time: 74.25340747833252 and batch: 100, loss is 3.7664804744720457 and perplexity is 43.22765595106267
At time: 74.88206195831299 and batch: 150, loss is 3.7696036958694457 and perplexity is 43.362876543094124
At time: 75.49718618392944 and batch: 200, loss is 3.6471589756011964 and perplexity is 38.36551370969847
At time: 76.12401723861694 and batch: 250, loss is 3.7880922460556032 and perplexity is 44.17205043874219
At time: 76.75175738334656 and batch: 300, loss is 3.7469947814941404 and perplexity is 42.393488676483834
At time: 77.36550188064575 and batch: 350, loss is 3.7469724893569945 and perplexity is 42.39254364555358
At time: 77.97737431526184 and batch: 400, loss is 3.6632440805435182 and perplexity is 38.987616908466926
At time: 78.58826565742493 and batch: 450, loss is 3.6885142946243286 and perplexity is 39.98539628692017
At time: 79.2157392501831 and batch: 500, loss is 3.578421277999878 and perplexity is 35.81695118033464
At time: 79.84004282951355 and batch: 550, loss is 3.6271118450164797 and perplexity is 37.604053296193555
At time: 80.46410536766052 and batch: 600, loss is 3.6271761417388917 and perplexity is 37.60647119130044
At time: 81.08330512046814 and batch: 650, loss is 3.4790871953964233 and perplexity is 32.43010621092741
At time: 81.69442963600159 and batch: 700, loss is 3.466235885620117 and perplexity is 32.016003450644305
At time: 82.30507564544678 and batch: 750, loss is 3.5514902544021605 and perplexity is 34.86523686417268
At time: 82.91655468940735 and batch: 800, loss is 3.519582805633545 and perplexity is 33.77033673032084
At time: 83.52838945388794 and batch: 850, loss is 3.5638495302200317 and perplexity is 35.298819809440744
At time: 84.15291166305542 and batch: 900, loss is 3.5112026596069335 and perplexity is 33.48851886385296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333106472067637 and perplexity of 76.18057219143759
finished 7 epochs...
Completing Train Step...
At time: 85.71329092979431 and batch: 50, loss is 3.7786187505722046 and perplexity is 43.75556263097818
At time: 86.32799482345581 and batch: 100, loss is 3.668811526298523 and perplexity is 39.20528371254263
At time: 86.94032263755798 and batch: 150, loss is 3.6782790756225587 and perplexity is 39.578224300870616
At time: 87.55214738845825 and batch: 200, loss is 3.555811200141907 and perplexity is 35.01621360712152
At time: 88.16403937339783 and batch: 250, loss is 3.7005982065200804 and perplexity is 40.47150743975354
At time: 88.77530884742737 and batch: 300, loss is 3.665032105445862 and perplexity is 39.057390097893474
At time: 89.38648700714111 and batch: 350, loss is 3.6689900302886964 and perplexity is 39.212282636770574
At time: 89.99896550178528 and batch: 400, loss is 3.5902039432525634 and perplexity is 36.24146637536502
At time: 90.61045050621033 and batch: 450, loss is 3.620592303276062 and perplexity is 37.359689536530084
At time: 91.22291111946106 and batch: 500, loss is 3.5153653144836428 and perplexity is 33.62821055264896
At time: 91.83376240730286 and batch: 550, loss is 3.565930094718933 and perplexity is 35.37233773366091
At time: 92.44529151916504 and batch: 600, loss is 3.57403678894043 and perplexity is 35.66025591510212
At time: 93.05747246742249 and batch: 650, loss is 3.4297350597381593 and perplexity is 30.86846336743386
At time: 93.66988468170166 and batch: 700, loss is 3.421956443786621 and perplexity is 30.629280906152673
At time: 94.27995038032532 and batch: 750, loss is 3.5141409158706667 and perplexity is 33.58706141490575
At time: 94.89088678359985 and batch: 800, loss is 3.4889069986343384 and perplexity is 32.7501321973801
At time: 95.50132703781128 and batch: 850, loss is 3.5400811433792114 and perplexity is 34.46971606662398
At time: 96.11259150505066 and batch: 900, loss is 3.494383807182312 and perplexity is 32.929990476449085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336643166738014 and perplexity of 76.45047661853268
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 97.67104005813599 and batch: 50, loss is 3.756282114982605 and perplexity is 42.78904513271468
At time: 98.28262042999268 and batch: 100, loss is 3.6506146955490113 and perplexity is 38.49832352532994
At time: 98.89430904388428 and batch: 150, loss is 3.6641389608383177 and perplexity is 39.022521774088375
At time: 99.50525879859924 and batch: 200, loss is 3.535298237800598 and perplexity is 34.305244309338846
At time: 100.1298565864563 and batch: 250, loss is 3.682255868911743 and perplexity is 39.7359320954736
At time: 100.74247646331787 and batch: 300, loss is 3.6389064264297484 and perplexity is 38.05020326828708
At time: 101.35371375083923 and batch: 350, loss is 3.6455894088745118 and perplexity is 38.30534370869592
At time: 101.96472835540771 and batch: 400, loss is 3.5655963706970213 and perplexity is 35.36053510436802
At time: 102.58926033973694 and batch: 450, loss is 3.5859818267822265 and perplexity is 36.088773254172345
At time: 103.2004907131195 and batch: 500, loss is 3.4785152101516723 and perplexity is 32.41156197270996
At time: 103.8117003440857 and batch: 550, loss is 3.522495412826538 and perplexity is 33.8688398367838
At time: 104.42339730262756 and batch: 600, loss is 3.535350079536438 and perplexity is 34.307022798851854
At time: 105.03389501571655 and batch: 650, loss is 3.3829626131057737 and perplexity is 29.457914366850314
At time: 105.64431691169739 and batch: 700, loss is 3.367019271850586 and perplexity is 28.991980922980854
At time: 106.2557635307312 and batch: 750, loss is 3.4583202171325684 and perplexity is 31.76357576601284
At time: 106.86474752426147 and batch: 800, loss is 3.427381019592285 and perplexity is 30.79588322722433
At time: 107.4769446849823 and batch: 850, loss is 3.4713783121109008 and perplexity is 32.181067445895415
At time: 108.08830785751343 and batch: 900, loss is 3.43112832069397 and perplexity is 30.911501166690243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321394410851884 and perplexity of 75.29354526293325
finished 9 epochs...
Completing Train Step...
At time: 109.63970828056335 and batch: 50, loss is 3.7258107089996337 and perplexity is 41.50486747859137
At time: 110.26543283462524 and batch: 100, loss is 3.617730121612549 and perplexity is 37.25291219914672
At time: 110.87758183479309 and batch: 150, loss is 3.630977964401245 and perplexity is 37.7497164497244
At time: 111.49111676216125 and batch: 200, loss is 3.502421245574951 and perplexity is 33.195729747111216
At time: 112.10445165634155 and batch: 250, loss is 3.650491952896118 and perplexity is 38.493598428959864
At time: 112.71898984909058 and batch: 300, loss is 3.6099250745773315 and perplexity is 36.96328322144874
At time: 113.33278346061707 and batch: 350, loss is 3.618174123764038 and perplexity is 37.269456244836526
At time: 113.94621205329895 and batch: 400, loss is 3.5399059724807738 and perplexity is 34.46367850430975
At time: 114.56007695198059 and batch: 450, loss is 3.5623468017578124 and perplexity is 35.24581510404463
At time: 115.17236709594727 and batch: 500, loss is 3.4570711135864256 and perplexity is 31.723924540284187
At time: 115.79834175109863 and batch: 550, loss is 3.50282009601593 and perplexity is 33.208972519326686
At time: 116.41168069839478 and batch: 600, loss is 3.5186039304733274 and perplexity is 33.737295960576034
At time: 117.02772903442383 and batch: 650, loss is 3.3690572929382325 and perplexity is 29.051127441928738
At time: 117.64027214050293 and batch: 700, loss is 3.3562687158584597 and perplexity is 28.681970387325663
At time: 118.25439262390137 and batch: 750, loss is 3.450877709388733 and perplexity is 31.52805263591478
At time: 118.8660614490509 and batch: 800, loss is 3.4226724100112915 and perplexity is 30.65121828904292
At time: 119.47875690460205 and batch: 850, loss is 3.4700866889953614 and perplexity is 32.139528467426885
At time: 120.09225869178772 and batch: 900, loss is 3.4328636264801027 and perplexity is 30.96518864214389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322475903654752 and perplexity of 75.37501873878527
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 121.64042592048645 and batch: 50, loss is 3.719096851348877 and perplexity is 41.22714305088554
At time: 122.28230953216553 and batch: 100, loss is 3.6180565118789674 and perplexity is 37.26507317158782
At time: 122.90936589241028 and batch: 150, loss is 3.628297348022461 and perplexity is 37.648659449553534
At time: 123.54303407669067 and batch: 200, loss is 3.5016120958328245 and perplexity is 33.16888029502517
At time: 124.16818356513977 and batch: 250, loss is 3.6508158016204835 and perplexity is 38.50606655049097
At time: 124.80046677589417 and batch: 300, loss is 3.6075206184387207 and perplexity is 36.87451339255162
At time: 125.41934299468994 and batch: 350, loss is 3.613938694000244 and perplexity is 37.11193789500531
At time: 126.03735327720642 and batch: 400, loss is 3.537271842956543 and perplexity is 34.37301617183405
At time: 126.65547156333923 and batch: 450, loss is 3.55583233833313 and perplexity is 35.016953794363744
At time: 127.2730941772461 and batch: 500, loss is 3.447331385612488 and perplexity is 31.41644197397717
At time: 127.89152812957764 and batch: 550, loss is 3.4899770736694338 and perplexity is 32.7851960533732
At time: 128.508957862854 and batch: 600, loss is 3.5102829360961914 and perplexity is 33.45773284519666
At time: 129.12656807899475 and batch: 650, loss is 3.3546981382369996 and perplexity is 28.636958483093316
At time: 129.75668597221375 and batch: 700, loss is 3.337777171134949 and perplexity is 28.156470079298543
At time: 130.37418055534363 and batch: 750, loss is 3.4324678993225097 and perplexity is 30.952937300312424
At time: 131.00479316711426 and batch: 800, loss is 3.4043430137634276 and perplexity is 30.09451754381055
At time: 131.62036848068237 and batch: 850, loss is 3.444436860084534 and perplexity is 31.325637761639907
At time: 132.24460220336914 and batch: 900, loss is 3.4110483837127688 and perplexity is 30.29699048675074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31993625588613 and perplexity of 75.18383561229372
finished 11 epochs...
Completing Train Step...
At time: 133.8125603199005 and batch: 50, loss is 3.7072172737121583 and perplexity is 40.74027959605535
At time: 134.43451404571533 and batch: 100, loss is 3.6052584075927734 and perplexity is 36.79118975177264
At time: 135.05556845664978 and batch: 150, loss is 3.6168338203430177 and perplexity is 37.21953732585387
At time: 135.67718243598938 and batch: 200, loss is 3.4895720529556273 and perplexity is 32.77192005857331
At time: 136.29923605918884 and batch: 250, loss is 3.6389670467376707 and perplexity is 38.05250995324098
At time: 136.92070889472961 and batch: 300, loss is 3.5972153425216673 and perplexity is 36.49646266198133
At time: 137.5424702167511 and batch: 350, loss is 3.6038676500320435 and perplexity is 36.74005769084894
At time: 138.1634373664856 and batch: 400, loss is 3.5277925443649294 and perplexity is 34.04872354967443
At time: 138.78442907333374 and batch: 450, loss is 3.5480650568008425 and perplexity is 34.74602082429636
At time: 139.40275192260742 and batch: 500, loss is 3.440275025367737 and perplexity is 31.195536552530864
At time: 140.02154397964478 and batch: 550, loss is 3.483717818260193 and perplexity is 32.58062603352608
At time: 140.63978099822998 and batch: 600, loss is 3.5056705713272094 and perplexity is 33.30376891881744
At time: 141.25944447517395 and batch: 650, loss is 3.3511018133163453 and perplexity is 28.534155642657787
At time: 141.87898683547974 and batch: 700, loss is 3.335687656402588 and perplexity is 28.097698144052384
At time: 142.4982304573059 and batch: 750, loss is 3.431538419723511 and perplexity is 30.924180543042624
At time: 143.11709260940552 and batch: 800, loss is 3.4046931648254395 and perplexity is 30.105057016185288
At time: 143.73473834991455 and batch: 850, loss is 3.446553864479065 and perplexity is 31.392024520178737
At time: 144.35375547409058 and batch: 900, loss is 3.414717445373535 and perplexity is 30.408356191867902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319570462997645 and perplexity of 75.15633892924987
finished 12 epochs...
Completing Train Step...
At time: 145.91474723815918 and batch: 50, loss is 3.7014915704727174 and perplexity is 40.50767938055736
At time: 146.52678394317627 and batch: 100, loss is 3.5991477489471437 and perplexity is 36.56705684729746
At time: 147.15188765525818 and batch: 150, loss is 3.610479531288147 and perplexity is 36.983783444602615
At time: 147.76399397850037 and batch: 200, loss is 3.4832596969604492 and perplexity is 32.56570357319059
At time: 148.3753535747528 and batch: 250, loss is 3.632576332092285 and perplexity is 37.81010262364213
At time: 148.98622679710388 and batch: 300, loss is 3.5914032888412475 and perplexity is 36.28495849401671
At time: 149.59953141212463 and batch: 350, loss is 3.598144073486328 and perplexity is 36.53037380168137
At time: 150.21067476272583 and batch: 400, loss is 3.5223133611679076 and perplexity is 33.86267451953577
At time: 150.82334160804749 and batch: 450, loss is 3.5432370710372925 and perplexity is 34.578671834753735
At time: 151.4349308013916 and batch: 500, loss is 3.435787196159363 and perplexity is 31.055849991581283
At time: 152.04716539382935 and batch: 550, loss is 3.479666953086853 and perplexity is 32.44891326564517
At time: 152.65904569625854 and batch: 600, loss is 3.502502098083496 and perplexity is 33.198413813639554
At time: 153.2695460319519 and batch: 650, loss is 3.3485041570663454 and perplexity is 28.460129903252493
At time: 153.88152432441711 and batch: 700, loss is 3.3339160108566284 and perplexity is 28.04796305176986
At time: 154.49445009231567 and batch: 750, loss is 3.430335383415222 and perplexity is 30.887000000300983
At time: 155.10648131370544 and batch: 800, loss is 3.4041343212127684 and perplexity is 30.08823769748484
At time: 155.7197036743164 and batch: 850, loss is 3.446754479408264 and perplexity is 31.398322860704713
At time: 156.3319947719574 and batch: 900, loss is 3.4155421829223633 and perplexity is 30.43344544962183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319905320258989 and perplexity of 75.18150978916378
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 157.88978600502014 and batch: 50, loss is 3.6992699193954466 and perplexity is 40.41778534458915
At time: 158.51507329940796 and batch: 100, loss is 3.5997792148590086 and perplexity is 36.590154989270864
At time: 159.12604022026062 and batch: 150, loss is 3.611102805137634 and perplexity is 37.00684165472197
At time: 159.73842906951904 and batch: 200, loss is 3.4833812952041625 and perplexity is 32.56966374632054
At time: 160.3502335548401 and batch: 250, loss is 3.634030327796936 and perplexity is 37.865118337062405
At time: 160.96295857429504 and batch: 300, loss is 3.593704438209534 and perplexity is 36.36855174672632
At time: 161.57375311851501 and batch: 350, loss is 3.5970423889160155 and perplexity is 36.490151012997366
At time: 162.18598079681396 and batch: 400, loss is 3.5208605670928956 and perplexity is 33.81351474479089
At time: 162.8137834072113 and batch: 450, loss is 3.5416441917419434 and perplexity is 34.5236360286622
At time: 163.428462266922 and batch: 500, loss is 3.432655620574951 and perplexity is 30.95874836988274
At time: 164.03957748413086 and batch: 550, loss is 3.4751236248016357 and perplexity is 32.30182159607229
At time: 164.65158009529114 and batch: 600, loss is 3.498886046409607 and perplexity is 33.078583420818
At time: 165.26358938217163 and batch: 650, loss is 3.3442960214614867 and perplexity is 28.340617455944603
At time: 165.87497067451477 and batch: 700, loss is 3.328279762268066 and perplexity is 27.890322427797297
At time: 166.48631286621094 and batch: 750, loss is 3.42244047164917 and perplexity is 30.64410992005952
At time: 167.09691405296326 and batch: 800, loss is 3.395952467918396 and perplexity is 29.84306450402004
At time: 167.70830368995667 and batch: 850, loss is 3.4373468542099 and perplexity is 31.104324289875436
At time: 168.31992173194885 and batch: 900, loss is 3.406462755203247 and perplexity is 30.15837779946271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320051637414384 and perplexity of 75.1925109386232
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 169.90020275115967 and batch: 50, loss is 3.696350574493408 and perplexity is 40.29996395330919
At time: 170.52465152740479 and batch: 100, loss is 3.5977098178863525 and perplexity is 36.514513726201315
At time: 171.13637971878052 and batch: 150, loss is 3.6103336811065674 and perplexity is 36.97838974641721
At time: 171.7487359046936 and batch: 200, loss is 3.482340683937073 and perplexity is 32.5357890155309
At time: 172.3612744808197 and batch: 250, loss is 3.6331354427337645 and perplexity is 37.83124856528408
At time: 172.97291207313538 and batch: 300, loss is 3.593562574386597 and perplexity is 36.36339273088836
At time: 173.58428525924683 and batch: 350, loss is 3.5954554414749147 and perplexity is 36.43228898536358
At time: 174.19489645957947 and batch: 400, loss is 3.5195328283309935 and perplexity is 33.768649022158705
At time: 174.8217067718506 and batch: 450, loss is 3.540546708106995 and perplexity is 34.48576768684548
At time: 175.4503629207611 and batch: 500, loss is 3.431174054145813 and perplexity is 30.9129148886672
At time: 176.06661343574524 and batch: 550, loss is 3.473075523376465 and perplexity is 32.23573189153709
At time: 176.68144965171814 and batch: 600, loss is 3.497013635635376 and perplexity is 33.01670467411907
At time: 177.29394054412842 and batch: 650, loss is 3.342296109199524 and perplexity is 28.283995346081145
At time: 177.9181845188141 and batch: 700, loss is 3.326637387275696 and perplexity is 27.844553654744924
At time: 178.5298867225647 and batch: 750, loss is 3.4206186866760255 and perplexity is 30.588333762600826
At time: 179.14094948768616 and batch: 800, loss is 3.3935003900527954 and perplexity is 29.769976631306708
At time: 179.75422620773315 and batch: 850, loss is 3.434892997741699 and perplexity is 31.028092311945777
At time: 180.36676359176636 and batch: 900, loss is 3.4037913274765015 and perplexity is 30.077919390077856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3194822546553935 and perplexity of 75.14970980555951
finished 15 epochs...
Completing Train Step...
At time: 181.91606640815735 and batch: 50, loss is 3.695339074134827 and perplexity is 40.25922113448131
At time: 182.53353595733643 and batch: 100, loss is 3.5966227436065674 and perplexity is 36.47484130483199
At time: 183.1616015434265 and batch: 150, loss is 3.6093984603881837 and perplexity is 36.943822956502395
At time: 183.7741515636444 and batch: 200, loss is 3.4813459730148315 and perplexity is 32.50344140176296
At time: 184.38853311538696 and batch: 250, loss is 3.6320559215545654 and perplexity is 37.79043096691722
At time: 185.00167989730835 and batch: 300, loss is 3.5924583196640016 and perplexity is 36.32326044494687
At time: 185.61352491378784 and batch: 350, loss is 3.594543309211731 and perplexity is 36.3990730701153
At time: 186.2355592250824 and batch: 400, loss is 3.5187772417068484 and perplexity is 33.743143519663825
At time: 186.85050535202026 and batch: 450, loss is 3.5398155450820923 and perplexity is 34.46056218441608
At time: 187.45985341072083 and batch: 500, loss is 3.4307353639602662 and perplexity is 30.89935667044497
At time: 188.06811714172363 and batch: 550, loss is 3.4724878597259523 and perplexity is 32.216793688848455
At time: 188.67593383789062 and batch: 600, loss is 3.496757369041443 and perplexity is 33.008244679722615
At time: 189.28503036499023 and batch: 650, loss is 3.342232542037964 and perplexity is 28.28219746992297
At time: 189.89418959617615 and batch: 700, loss is 3.3265447854995727 and perplexity is 27.841975319002216
At time: 190.5028257369995 and batch: 750, loss is 3.4206525039672853 and perplexity is 30.589368194683573
At time: 191.1107518672943 and batch: 800, loss is 3.39374587059021 and perplexity is 29.77728547822212
At time: 191.7180938720703 and batch: 850, loss is 3.435380206108093 and perplexity is 31.043213141311586
At time: 192.33380842208862 and batch: 900, loss is 3.4045963382720945 and perplexity is 30.1021421883948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319144471050942 and perplexity of 75.12432975243213
finished 16 epochs...
Completing Train Step...
At time: 193.88039636611938 and batch: 50, loss is 3.6945696449279786 and perplexity is 40.22825642799789
At time: 194.48854231834412 and batch: 100, loss is 3.595774221420288 and perplexity is 36.44390471978937
At time: 195.0961320400238 and batch: 150, loss is 3.608660740852356 and perplexity is 36.91657882707806
At time: 195.7047688961029 and batch: 200, loss is 3.4805774545669554 and perplexity is 32.47847150356893
At time: 196.31347036361694 and batch: 250, loss is 3.631221628189087 and perplexity is 37.758915809354384
At time: 196.9223554134369 and batch: 300, loss is 3.5916472244262696 and perplexity is 36.293810766242665
At time: 197.53131341934204 and batch: 350, loss is 3.593820219039917 and perplexity is 36.372762771620664
At time: 198.13959980010986 and batch: 400, loss is 3.5181625080108643 and perplexity is 33.72240684675268
At time: 198.74858570098877 and batch: 450, loss is 3.539241189956665 and perplexity is 34.44077526680308
At time: 199.35745000839233 and batch: 500, loss is 3.4303723526000978 and perplexity is 30.88814188862146
At time: 199.97973370552063 and batch: 550, loss is 3.4720310497283937 and perplexity is 32.20208009631605
At time: 200.59050130844116 and batch: 600, loss is 3.4965332078933717 and perplexity is 33.00084634294021
At time: 201.20107555389404 and batch: 650, loss is 3.342134881019592 and perplexity is 28.27943553658507
At time: 201.8106882572174 and batch: 700, loss is 3.3264683055877686 and perplexity is 27.839846048609612
At time: 202.4211070537567 and batch: 750, loss is 3.4206956911087034 and perplexity is 30.59068929058066
At time: 203.03085374832153 and batch: 800, loss is 3.3939516305923463 and perplexity is 29.783413082932086
At time: 203.6512541770935 and batch: 850, loss is 3.435769786834717 and perplexity is 31.055309334912874
At time: 204.26670789718628 and batch: 900, loss is 3.4052241945266726 and perplexity is 30.121047941070454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318927085562928 and perplexity of 75.10800058827313
finished 17 epochs...
Completing Train Step...
At time: 205.84667468070984 and batch: 50, loss is 3.6939295434951784 and perplexity is 40.20251450301975
At time: 206.48396039009094 and batch: 100, loss is 3.595059700012207 and perplexity is 36.41787407050859
At time: 207.10076761245728 and batch: 150, loss is 3.6080303382873535 and perplexity is 36.893313855013865
At time: 207.71623921394348 and batch: 200, loss is 3.479927730560303 and perplexity is 32.457376314701094
At time: 208.33211541175842 and batch: 250, loss is 3.6305256509780883 and perplexity is 37.73264560723047
At time: 208.96648693084717 and batch: 300, loss is 3.590989489555359 and perplexity is 36.26994691020893
At time: 209.580091714859 and batch: 350, loss is 3.593206286430359 and perplexity is 36.35043919974099
At time: 210.2017319202423 and batch: 400, loss is 3.5176258420944215 and perplexity is 33.704314035709366
At time: 210.82629251480103 and batch: 450, loss is 3.5387536001205446 and perplexity is 34.42398638821078
At time: 211.4625267982483 and batch: 500, loss is 3.430043888092041 and perplexity is 30.877997896353147
At time: 212.09581804275513 and batch: 550, loss is 3.4716474342346193 and perplexity is 32.18972924859908
At time: 212.72900223731995 and batch: 600, loss is 3.496325011253357 and perplexity is 32.99397639278905
At time: 213.35234999656677 and batch: 650, loss is 3.3420223569869996 and perplexity is 28.27625359948461
At time: 213.96887516975403 and batch: 700, loss is 3.3263969135284426 and perplexity is 27.83785857561462
At time: 214.58525609970093 and batch: 750, loss is 3.420733060836792 and perplexity is 30.59183247768165
At time: 215.20796513557434 and batch: 800, loss is 3.394117217063904 and perplexity is 29.78834522155187
At time: 215.8218710422516 and batch: 850, loss is 3.4360828399658203 and perplexity is 31.065032818641654
At time: 216.43650889396667 and batch: 900, loss is 3.4057206964492797 and perplexity is 30.136006812531058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318782440603596 and perplexity of 75.09713738025562
finished 18 epochs...
Completing Train Step...
At time: 217.9875853061676 and batch: 50, loss is 3.6933678436279296 and perplexity is 40.179939096855215
At time: 218.62766814231873 and batch: 100, loss is 3.5944314575195313 and perplexity is 36.39500199988019
At time: 219.24371552467346 and batch: 150, loss is 3.6074668645858763 and perplexity is 36.872531298658075
At time: 219.8612539768219 and batch: 200, loss is 3.4793499088287354 and perplexity is 32.43862715467451
At time: 220.47748827934265 and batch: 250, loss is 3.6299164962768553 and perplexity is 37.709667588063596
At time: 221.10016512870789 and batch: 300, loss is 3.5904234409332276 and perplexity is 36.249422166285164
At time: 221.7259464263916 and batch: 350, loss is 3.592662477493286 and perplexity is 36.33067687998772
At time: 222.3439724445343 and batch: 400, loss is 3.5171399831771852 and perplexity is 33.687942471643034
At time: 222.96029019355774 and batch: 450, loss is 3.538320345878601 and perplexity is 34.409075280467064
At time: 223.5766944885254 and batch: 500, loss is 3.429735321998596 and perplexity is 30.868471463011613
At time: 224.1928448677063 and batch: 550, loss is 3.4713092470169067 and perplexity is 32.178844934197315
At time: 224.83530807495117 and batch: 600, loss is 3.4961270761489867 and perplexity is 32.98744637290955
At time: 225.46504998207092 and batch: 650, loss is 3.3419038438796997 and perplexity is 28.272902691374924
At time: 226.08229088783264 and batch: 700, loss is 3.326327095031738 and perplexity is 27.835915046025345
At time: 226.69849514961243 and batch: 750, loss is 3.420760083198547 and perplexity is 30.59265915241492
At time: 227.31554198265076 and batch: 800, loss is 3.394248390197754 and perplexity is 29.792252908432953
At time: 227.9327838420868 and batch: 850, loss is 3.4363373517990112 and perplexity is 31.072940243316353
At time: 228.5502953529358 and batch: 900, loss is 3.406120080947876 and perplexity is 30.14804507028833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318682526888913 and perplexity of 75.0896345211234
finished 19 epochs...
Completing Train Step...
At time: 230.10072875022888 and batch: 50, loss is 3.6928583717346193 and perplexity is 40.15947376090984
At time: 230.70990204811096 and batch: 100, loss is 3.5938635635375977 and perplexity is 36.37433936492034
At time: 231.32106614112854 and batch: 150, loss is 3.606949405670166 and perplexity is 36.85345621430465
At time: 231.9317216873169 and batch: 200, loss is 3.478820924758911 and perplexity is 32.421472175418124
At time: 232.54291796684265 and batch: 250, loss is 3.6293668031692503 and perplexity is 37.68894453988055
At time: 233.15161418914795 and batch: 300, loss is 3.589917612075806 and perplexity is 36.2310907991467
At time: 233.76219773292542 and batch: 350, loss is 3.592166976928711 and perplexity is 36.31267946831434
At time: 234.37207317352295 and batch: 400, loss is 3.5166904306411744 and perplexity is 33.67280137528605
At time: 234.98249197006226 and batch: 450, loss is 3.537924551963806 and perplexity is 34.39545907264119
At time: 235.59259462356567 and batch: 500, loss is 3.429440245628357 and perplexity is 30.859364250225195
At time: 236.20274233818054 and batch: 550, loss is 3.4710011005401613 and perplexity is 32.16893066410725
At time: 236.81252193450928 and batch: 600, loss is 3.495936617851257 and perplexity is 32.98116423828825
At time: 237.42211318016052 and batch: 650, loss is 3.3417827177047728 and perplexity is 28.269478310213117
At time: 238.031583070755 and batch: 700, loss is 3.3262572050094605 and perplexity is 27.833969661284947
At time: 238.64009189605713 and batch: 750, loss is 3.4207756185531615 and perplexity is 30.593134423915195
At time: 239.24932718276978 and batch: 800, loss is 3.394351382255554 and perplexity is 29.795321431880563
At time: 239.85944437980652 and batch: 850, loss is 3.436546368598938 and perplexity is 31.079435688655277
At time: 240.49111104011536 and batch: 900, loss is 3.406446375846863 and perplexity is 30.157883828690238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318614802948416 and perplexity of 75.0845493273798
Finished Training.
Improved accuracyfrom -10000000 to -75.0845493273798
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
252.14929723739624


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8165924549102783 and batch: 50, loss is 8.268919849395752 and perplexity is 3900.733336322046
At time: 1.4458544254302979 and batch: 100, loss is 7.326176509857178 and perplexity is 1519.560625461146
At time: 2.060411214828491 and batch: 150, loss is 7.083815717697144 and perplexity is 1192.5101302525952
At time: 2.6749379634857178 and batch: 200, loss is 6.8647545719146725 and perplexity is 957.9107127592923
At time: 3.2900397777557373 and batch: 250, loss is 6.842438840866089 and perplexity is 936.7709864254606
At time: 3.9032201766967773 and batch: 300, loss is 6.7187038230896 and perplexity is 827.7439132970513
At time: 4.517187595367432 and batch: 350, loss is 6.688045377731323 and perplexity is 802.7516414341329
At time: 5.130854606628418 and batch: 400, loss is 6.573146533966065 and perplexity is 715.6180207033225
At time: 5.7564966678619385 and batch: 450, loss is 6.552734327316284 and perplexity is 701.1587526107822
At time: 6.369213342666626 and batch: 500, loss is 6.508678159713745 and perplexity is 670.9389570367273
At time: 6.984363794326782 and batch: 550, loss is 6.512466669082642 and perplexity is 673.4856365750654
At time: 7.597431182861328 and batch: 600, loss is 6.451663036346435 and perplexity is 633.7553751360549
At time: 8.211368322372437 and batch: 650, loss is 6.3714494800567625 and perplexity is 584.9050229133593
At time: 8.824437379837036 and batch: 700, loss is 6.46437216758728 and perplexity is 641.8612556164541
At time: 9.437044858932495 and batch: 750, loss is 6.389314737319946 and perplexity is 595.4484012819789
At time: 10.049746036529541 and batch: 800, loss is 6.404647598266601 and perplexity is 604.6486818857911
At time: 10.664473295211792 and batch: 850, loss is 6.422346906661987 and perplexity is 615.4458144038252
At time: 11.27742862701416 and batch: 900, loss is 6.306240520477295 and perplexity is 547.9809483052271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.178222238200984 and perplexity of 482.1340744842968
finished 1 epochs...
Completing Train Step...
At time: 12.85909652709961 and batch: 50, loss is 5.8683555316925045 and perplexity is 353.66690777780735
At time: 13.470027685165405 and batch: 100, loss is 5.524615993499756 and perplexity is 250.79001453182872
At time: 14.079964399337769 and batch: 150, loss is 5.384096622467041 and perplexity is 217.91315731175678
At time: 14.688876152038574 and batch: 200, loss is 5.180373973846436 and perplexity is 177.74927214422868
At time: 15.298674821853638 and batch: 250, loss is 5.209188652038574 and perplexity is 182.94556545064162
At time: 15.90767765045166 and batch: 300, loss is 5.1052721118927 and perplexity is 164.88893266715743
At time: 16.529853582382202 and batch: 350, loss is 5.07503415107727 and perplexity is 159.97765529536443
At time: 17.139546871185303 and batch: 400, loss is 4.921011190414429 and perplexity is 137.14121898229052
At time: 17.74788808822632 and batch: 450, loss is 4.9123592185974125 and perplexity is 135.9597952130551
At time: 18.35718607902527 and batch: 500, loss is 4.833409185409546 and perplexity is 125.63855649755295
At time: 18.96692991256714 and batch: 550, loss is 4.884713163375855 and perplexity is 132.25252502540684
At time: 19.57614517211914 and batch: 600, loss is 4.802407989501953 and perplexity is 121.80336589525257
At time: 20.185911417007446 and batch: 650, loss is 4.674294595718384 and perplexity is 107.15695145302232
At time: 20.79574680328369 and batch: 700, loss is 4.738286685943604 and perplexity is 114.238307815663
At time: 21.404584646224976 and batch: 750, loss is 4.746706352233887 and perplexity is 115.20421685476182
At time: 22.013976097106934 and batch: 800, loss is 4.696982669830322 and perplexity is 109.61592552360217
At time: 22.6243634223938 and batch: 850, loss is 4.729477005004883 and perplexity is 113.23632482839609
At time: 23.232445240020752 and batch: 900, loss is 4.6641568851470945 and perplexity is 106.07611316311802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.735568111889983 and perplexity of 113.92816428120206
finished 2 epochs...
Completing Train Step...
At time: 24.768861293792725 and batch: 50, loss is 4.716938753128051 and perplexity is 111.8254029966832
At time: 25.378870964050293 and batch: 100, loss is 4.588217830657959 and perplexity is 98.3190527307089
At time: 25.987878799438477 and batch: 150, loss is 4.593649673461914 and perplexity is 98.85455944725553
At time: 26.59555721282959 and batch: 200, loss is 4.481141185760498 and perplexity is 88.3354223037777
At time: 27.20599937438965 and batch: 250, loss is 4.601046466827393 and perplexity is 99.58847716938502
At time: 27.82510757446289 and batch: 300, loss is 4.548411178588867 and perplexity is 94.48217369799805
At time: 28.43718433380127 and batch: 350, loss is 4.549678325653076 and perplexity is 94.60197239227176
At time: 29.048107147216797 and batch: 400, loss is 4.439213266372681 and perplexity is 84.70827260529697
At time: 29.65905213356018 and batch: 450, loss is 4.475145082473755 and perplexity is 87.80733879160024
At time: 30.273347854614258 and batch: 500, loss is 4.373114585876465 and perplexity is 79.29020366393372
At time: 30.909082412719727 and batch: 550, loss is 4.440063219070435 and perplexity is 84.78030123624829
At time: 31.531472206115723 and batch: 600, loss is 4.412148723602295 and perplexity is 82.44642789787876
At time: 32.16071820259094 and batch: 650, loss is 4.2726808500289915 and perplexity is 71.71363163942178
At time: 32.771145820617676 and batch: 700, loss is 4.307454690933228 and perplexity is 74.25125581946357
At time: 33.38173747062683 and batch: 750, loss is 4.373964118957519 and perplexity is 79.35759193517976
At time: 33.992037773132324 and batch: 800, loss is 4.334252343177796 and perplexity is 76.26791534070585
At time: 34.6022744178772 and batch: 850, loss is 4.390725927352905 and perplexity is 80.69897931906884
At time: 35.2137553691864 and batch: 900, loss is 4.332786388397217 and perplexity is 76.15619193634475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.519644123234161 and perplexity of 91.80292163727955
finished 3 epochs...
Completing Train Step...
At time: 36.74869513511658 and batch: 50, loss is 4.417895488739013 and perplexity is 82.92159217642113
At time: 37.37448477745056 and batch: 100, loss is 4.2948147535324095 and perplexity is 73.31863116950326
At time: 37.98438286781311 and batch: 150, loss is 4.304211459159851 and perplexity is 74.01083187340983
At time: 38.594058990478516 and batch: 200, loss is 4.187882075309753 and perplexity is 65.8831076329136
At time: 39.203421115875244 and batch: 250, loss is 4.335128393173218 and perplexity is 76.3347591225633
At time: 39.81399989128113 and batch: 300, loss is 4.293962345123291 and perplexity is 73.25616038084473
At time: 40.42400574684143 and batch: 350, loss is 4.299568958282471 and perplexity is 73.66803285955326
At time: 41.03387212753296 and batch: 400, loss is 4.205308156013489 and perplexity is 67.04125364897237
At time: 41.64500164985657 and batch: 450, loss is 4.253994126319885 and perplexity is 70.38598216195486
At time: 42.255110025405884 and batch: 500, loss is 4.1402058410644536 and perplexity is 62.815750179405605
At time: 42.86542797088623 and batch: 550, loss is 4.206178965568543 and perplexity is 67.09965923961511
At time: 43.48857235908508 and batch: 600, loss is 4.196920790672302 and perplexity is 66.48130568833918
At time: 44.108664989471436 and batch: 650, loss is 4.053386330604553 and perplexity is 57.59215327771543
At time: 44.720707178115845 and batch: 700, loss is 4.067944340705871 and perplexity is 58.4367130658775
At time: 45.332252979278564 and batch: 750, loss is 4.158432064056396 and perplexity is 63.971141273001024
At time: 45.943986654281616 and batch: 800, loss is 4.126227540969849 and perplexity is 61.943801152982
At time: 46.556397438049316 and batch: 850, loss is 4.188390212059021 and perplexity is 65.91659376810529
At time: 47.168620109558105 and batch: 900, loss is 4.1350118446350095 and perplexity is 62.49033124124716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433505959706764 and perplexity of 84.22619351158784
finished 4 epochs...
Completing Train Step...
At time: 48.72701907157898 and batch: 50, loss is 4.226258544921875 and perplexity is 68.46061010590455
At time: 49.35520148277283 and batch: 100, loss is 4.1061347436904905 and perplexity is 60.711597577237725
At time: 49.96993660926819 and batch: 150, loss is 4.120192317962647 and perplexity is 61.57108235097706
At time: 50.593284130096436 and batch: 200, loss is 4.004301052093506 and perplexity is 54.833485252374835
At time: 51.21601605415344 and batch: 250, loss is 4.159711785316468 and perplexity is 64.05305890720854
At time: 51.83123302459717 and batch: 300, loss is 4.121662487983704 and perplexity is 61.66166888289913
At time: 52.4465696811676 and batch: 350, loss is 4.127058897018433 and perplexity is 61.99531991899442
At time: 53.06175947189331 and batch: 400, loss is 4.042634530067444 and perplexity is 56.976250896192894
At time: 53.67752122879028 and batch: 450, loss is 4.09422875881195 and perplexity is 59.99305219768834
At time: 54.29291558265686 and batch: 500, loss is 3.978751292228699 and perplexity is 53.45024879737515
At time: 54.907915115356445 and batch: 550, loss is 4.0402976989746096 and perplexity is 56.84326246781113
At time: 55.52402973175049 and batch: 600, loss is 4.040574250221252 and perplexity is 56.8589847168134
At time: 56.14090847969055 and batch: 650, loss is 3.8985355567932127 and perplexity is 49.330154972784854
At time: 56.75619125366211 and batch: 700, loss is 3.9065418529510496 and perplexity is 49.726692081581746
At time: 57.37199521064758 and batch: 750, loss is 4.004853816032409 and perplexity is 54.863803604340575
At time: 57.987608194351196 and batch: 800, loss is 3.9754394054412843 and perplexity is 53.27352043829624
At time: 58.60285997390747 and batch: 850, loss is 4.041070685386658 and perplexity is 56.8872185238436
At time: 59.217392444610596 and batch: 900, loss is 3.99182071685791 and perplexity is 54.153397662290146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39471435546875 and perplexity of 81.02148411317725
finished 5 epochs...
Completing Train Step...
At time: 60.766579389572144 and batch: 50, loss is 4.082405304908752 and perplexity is 59.28790396891453
At time: 61.38296842575073 and batch: 100, loss is 3.9673847436904905 and perplexity is 52.846143748823174
At time: 61.9988739490509 and batch: 150, loss is 3.980060682296753 and perplexity is 53.520281862579466
At time: 62.615057706832886 and batch: 200, loss is 3.8669103670120237 and perplexity is 47.79449029276345
At time: 63.24474048614502 and batch: 250, loss is 4.025590400695801 and perplexity is 56.01336935343194
At time: 63.87557864189148 and batch: 300, loss is 3.9900324630737303 and perplexity is 54.05664417966868
At time: 64.49408054351807 and batch: 350, loss is 3.9953232431411743 and perplexity is 54.34340391763726
At time: 65.110759973526 and batch: 400, loss is 3.915216498374939 and perplexity is 50.15992987889075
At time: 65.74002122879028 and batch: 450, loss is 3.9675448179244994 and perplexity is 52.854603731898685
At time: 66.35729551315308 and batch: 500, loss is 3.855838432312012 and perplexity is 47.26823154453097
At time: 66.97388458251953 and batch: 550, loss is 3.9123061990737913 and perplexity is 50.014161687437564
At time: 67.5913028717041 and batch: 600, loss is 3.9209670972824098 and perplexity is 50.4492104880346
At time: 68.20874762535095 and batch: 650, loss is 3.7825309085845946 and perplexity is 43.92707658199904
At time: 68.8257999420166 and batch: 700, loss is 3.7878083181381226 and perplexity is 44.15951054074807
At time: 69.44216108322144 and batch: 750, loss is 3.885846576690674 and perplexity is 48.70816020311178
At time: 70.0587728023529 and batch: 800, loss is 3.8582713127136232 and perplexity is 47.38336950032165
At time: 70.67563581466675 and batch: 850, loss is 3.923558259010315 and perplexity is 50.58010205883348
At time: 71.29272818565369 and batch: 900, loss is 3.876662292480469 and perplexity is 48.262858634666614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381108950262201 and perplexity of 79.92661892255076
finished 6 epochs...
Completing Train Step...
At time: 72.83622741699219 and batch: 50, loss is 3.9682944059371947 and perplexity is 52.894237762013006
At time: 73.45052909851074 and batch: 100, loss is 3.856193790435791 and perplexity is 47.2850316794624
At time: 74.07052803039551 and batch: 150, loss is 3.8724061107635497 and perplexity is 48.05787966145273
At time: 74.68009614944458 and batch: 200, loss is 3.7599717235565184 and perplexity is 42.947211567192916
At time: 75.28949093818665 and batch: 250, loss is 3.9137451219558717 and perplexity is 50.08618001109845
At time: 75.89969754219055 and batch: 300, loss is 3.8848089599609374 and perplexity is 48.65764601292693
At time: 76.50910758972168 and batch: 350, loss is 3.8889031171798707 and perplexity is 48.85726642544512
At time: 77.11839246749878 and batch: 400, loss is 3.8106936407089234 and perplexity is 45.181767913037106
At time: 77.72748303413391 and batch: 450, loss is 3.864643120765686 and perplexity is 47.68625116279819
At time: 78.33508038520813 and batch: 500, loss is 3.754856562614441 and perplexity is 42.72809056538083
At time: 78.95690321922302 and batch: 550, loss is 3.8078764867782593 and perplexity is 45.05466303902671
At time: 79.5651638507843 and batch: 600, loss is 3.821827940940857 and perplexity is 45.68764635698183
At time: 80.17565846443176 and batch: 650, loss is 3.6852443027496338 and perplexity is 39.85485791198706
At time: 80.78502821922302 and batch: 700, loss is 3.693102550506592 and perplexity is 40.16928104921293
At time: 81.39405560493469 and batch: 750, loss is 3.78776141166687 and perplexity is 44.15743922251586
At time: 82.00359916687012 and batch: 800, loss is 3.759303250312805 and perplexity is 42.918512098850215
At time: 82.61592078208923 and batch: 850, loss is 3.828564138412476 and perplexity is 45.99644626575462
At time: 83.22640442848206 and batch: 900, loss is 3.7820575618743897 and perplexity is 43.906288765120536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372339118016909 and perplexity of 79.22874049385128
finished 7 epochs...
Completing Train Step...
At time: 84.76271176338196 and batch: 50, loss is 3.869513301849365 and perplexity is 47.91905828743412
At time: 85.38465809822083 and batch: 100, loss is 3.762645721435547 and perplexity is 43.06220599874677
At time: 85.99339962005615 and batch: 150, loss is 3.7831791925430296 and perplexity is 43.95556303374527
At time: 86.61360383033752 and batch: 200, loss is 3.6685482263565063 and perplexity is 39.194962322484706
At time: 87.22814440727234 and batch: 250, loss is 3.819169340133667 and perplexity is 45.56634246426168
At time: 87.83655023574829 and batch: 300, loss is 3.7954624938964843 and perplexity is 44.49881207608694
At time: 88.44533586502075 and batch: 350, loss is 3.7967309617996214 and perplexity is 44.55529320560993
At time: 89.05458307266235 and batch: 400, loss is 3.722528381347656 and perplexity is 41.36885823992714
At time: 89.67597937583923 and batch: 450, loss is 3.7793422031402586 and perplexity is 43.78722915836434
At time: 90.28845953941345 and batch: 500, loss is 3.6676913785934446 and perplexity is 39.161392590824015
At time: 90.89776349067688 and batch: 550, loss is 3.721643257141113 and perplexity is 41.33225786243151
At time: 91.50736403465271 and batch: 600, loss is 3.738116726875305 and perplexity is 42.01878275951503
At time: 92.11599540710449 and batch: 650, loss is 3.6022073888778685 and perplexity is 36.67911020863124
At time: 92.72515988349915 and batch: 700, loss is 3.6087874937057496 and perplexity is 36.92125840535061
At time: 93.33534336090088 and batch: 750, loss is 3.7057297229766846 and perplexity is 40.679721416028414
At time: 93.94420790672302 and batch: 800, loss is 3.6773608541488647 and perplexity is 39.541899405128746
At time: 94.56804537773132 and batch: 850, loss is 3.7458358669281004 and perplexity is 42.344386702943254
At time: 95.1770076751709 and batch: 900, loss is 3.698482131958008 and perplexity is 40.38595725957442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382927463479238 and perplexity of 80.07209877387902
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.70372128486633 and batch: 50, loss is 3.8144756889343263 and perplexity is 45.35297108344405
At time: 97.32529211044312 and batch: 100, loss is 3.7208852195739746 and perplexity is 41.30093833042779
At time: 97.93610262870789 and batch: 150, loss is 3.72995569229126 and perplexity is 41.67726149915526
At time: 98.5453999042511 and batch: 200, loss is 3.598499646186829 and perplexity is 36.543365314921175
At time: 99.15478777885437 and batch: 250, loss is 3.7407711267471315 and perplexity is 42.13046557093143
At time: 99.76432824134827 and batch: 300, loss is 3.7082540559768677 and perplexity is 40.782540299188035
At time: 100.37408924102783 and batch: 350, loss is 3.6962600231170653 and perplexity is 40.296314901322475
At time: 100.98371744155884 and batch: 400, loss is 3.6194499731063843 and perplexity is 37.3170368024334
At time: 101.592933177948 and batch: 450, loss is 3.6576827812194823 and perplexity is 38.77139689030022
At time: 102.20341038703918 and batch: 500, loss is 3.5381780529022215 and perplexity is 34.404179459059705
At time: 102.8141565322876 and batch: 550, loss is 3.574522843360901 and perplexity is 35.67759295315546
At time: 103.42350721359253 and batch: 600, loss is 3.5886705207824705 and perplexity is 36.1859354835158
At time: 104.03362011909485 and batch: 650, loss is 3.4402304458618165 and perplexity is 31.194145901921917
At time: 104.64370536804199 and batch: 700, loss is 3.4282559061050417 and perplexity is 30.822837919537317
At time: 105.25253391265869 and batch: 750, loss is 3.51066246509552 and perplexity is 33.47043343503182
At time: 105.86250400543213 and batch: 800, loss is 3.474308671951294 and perplexity is 32.27550785817554
At time: 106.47209930419922 and batch: 850, loss is 3.5271621990203856 and perplexity is 34.02726785825526
At time: 107.0810980796814 and batch: 900, loss is 3.469622311592102 and perplexity is 32.124607061505216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346964796928511 and perplexity of 77.24365657934892
finished 9 epochs...
Completing Train Step...
At time: 108.62358331680298 and batch: 50, loss is 3.7291909360885622 and perplexity is 41.645400739322
At time: 109.23165345191956 and batch: 100, loss is 3.6355427837371828 and perplexity is 37.92243099072586
At time: 109.85326528549194 and batch: 150, loss is 3.6440521812438966 and perplexity is 38.24650491184552
At time: 110.4633092880249 and batch: 200, loss is 3.5184605884552003 and perplexity is 33.732460335069334
At time: 111.07263112068176 and batch: 250, loss is 3.66220712184906 and perplexity is 38.94720931426136
At time: 111.68186473846436 and batch: 300, loss is 3.6333444213867185 and perplexity is 37.83915531479097
At time: 112.29193758964539 and batch: 350, loss is 3.623895831108093 and perplexity is 37.4833123940576
At time: 112.90115118026733 and batch: 400, loss is 3.5544328451156617 and perplexity is 34.96798208081553
At time: 113.50913572311401 and batch: 450, loss is 3.5973286247253418 and perplexity is 36.50059729588381
At time: 114.11974096298218 and batch: 500, loss is 3.482297568321228 and perplexity is 32.53438624519136
At time: 114.72706246376038 and batch: 550, loss is 3.519807691574097 and perplexity is 33.77793205826842
At time: 115.33613610267639 and batch: 600, loss is 3.5411284875869753 and perplexity is 34.50583663612244
At time: 115.94683361053467 and batch: 650, loss is 3.398601460456848 and perplexity is 29.922223358534016
At time: 116.55697727203369 and batch: 700, loss is 3.3902688312530516 and perplexity is 29.673928477569515
At time: 117.16562485694885 and batch: 750, loss is 3.477972583770752 and perplexity is 32.39397937495885
At time: 117.77555727958679 and batch: 800, loss is 3.448248643875122 and perplexity is 31.445272185313765
At time: 118.38503384590149 and batch: 850, loss is 3.507416567802429 and perplexity is 33.36196797484958
At time: 118.99402523040771 and batch: 900, loss is 3.455919041633606 and perplexity is 31.687397341608637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351820436242509 and perplexity of 77.61963598647527
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.53277230262756 and batch: 50, loss is 3.712010912895203 and perplexity is 40.936042630461976
At time: 121.14390754699707 and batch: 100, loss is 3.6331857585906984 and perplexity is 37.83315212486373
At time: 121.75462412834167 and batch: 150, loss is 3.6459064865112305 and perplexity is 38.317491402331996
At time: 122.36452651023865 and batch: 200, loss is 3.5120316314697266 and perplexity is 33.516291413458376
At time: 122.97516369819641 and batch: 250, loss is 3.6541267824172974 and perplexity is 38.6337706937194
At time: 123.58590602874756 and batch: 300, loss is 3.616189479827881 and perplexity is 37.195562994646096
At time: 124.1966016292572 and batch: 350, loss is 3.603367247581482 and perplexity is 36.721677475082714
At time: 124.8073570728302 and batch: 400, loss is 3.5288580226898194 and perplexity is 34.085021060278294
At time: 125.4312195777893 and batch: 450, loss is 3.568094973564148 and perplexity is 35.4489975089491
At time: 126.04915595054626 and batch: 500, loss is 3.453006429672241 and perplexity is 31.595238525672727
At time: 126.65989589691162 and batch: 550, loss is 3.488774132728577 and perplexity is 32.74578111046384
At time: 127.270827293396 and batch: 600, loss is 3.5092942237854006 and perplexity is 33.424669120790604
At time: 127.88172459602356 and batch: 650, loss is 3.3534678506851194 and perplexity is 28.601748453219766
At time: 128.49232244491577 and batch: 700, loss is 3.3420078992843627 and perplexity is 28.27584479277359
At time: 129.10307455062866 and batch: 750, loss is 3.4270543098449706 and perplexity is 30.78582355538274
At time: 129.72308444976807 and batch: 800, loss is 3.393969736099243 and perplexity is 29.78395233160474
At time: 130.33810472488403 and batch: 850, loss is 3.4557407760620116 and perplexity is 31.68174907307006
At time: 130.95710062980652 and batch: 900, loss is 3.406489987373352 and perplexity is 30.15919908871973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329293446998074 and perplexity of 75.89064685741391
finished 11 epochs...
Completing Train Step...
At time: 132.49111771583557 and batch: 50, loss is 3.687716770172119 and perplexity is 39.95351966853095
At time: 133.11860346794128 and batch: 100, loss is 3.599604563713074 and perplexity is 36.58376503479474
At time: 133.73374009132385 and batch: 150, loss is 3.610541615486145 and perplexity is 36.98607962441421
At time: 134.34984135627747 and batch: 200, loss is 3.481852674484253 and perplexity is 32.51991511655777
At time: 134.9730761051178 and batch: 250, loss is 3.62686598777771 and perplexity is 37.59480920389367
At time: 135.59601855278015 and batch: 300, loss is 3.5916259145736693 and perplexity is 36.29303735872556
At time: 136.21058082580566 and batch: 350, loss is 3.5796586513519286 and perplexity is 35.86129755212805
At time: 136.8254165649414 and batch: 400, loss is 3.5083969259262084 and perplexity is 33.394690688547264
At time: 137.44070029258728 and batch: 450, loss is 3.5495579481124877 and perplexity is 34.797931595827855
At time: 138.0560803413391 and batch: 500, loss is 3.435088634490967 and perplexity is 31.03416314088092
At time: 138.67117881774902 and batch: 550, loss is 3.471740279197693 and perplexity is 32.19271804156756
At time: 139.28662133216858 and batch: 600, loss is 3.4949797916412355 and perplexity is 32.9496220884923
At time: 139.90259647369385 and batch: 650, loss is 3.3418132734298704 and perplexity is 28.270342117818085
At time: 140.51983976364136 and batch: 700, loss is 3.3330204248428346 and perplexity is 28.02285493326339
At time: 141.14767146110535 and batch: 750, loss is 3.4203317832946776 and perplexity is 30.579559125011322
At time: 141.7629165649414 and batch: 800, loss is 3.390228281021118 and perplexity is 29.672725217283773
At time: 142.37864518165588 and batch: 850, loss is 3.454211211204529 and perplexity is 31.633326824989396
At time: 142.9940891265869 and batch: 900, loss is 3.4072776317596434 and perplexity is 30.182963170171107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329850706335616 and perplexity of 75.93294941466013
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 144.53107595443726 and batch: 50, loss is 3.685937428474426 and perplexity is 39.88249191507668
At time: 145.16079425811768 and batch: 100, loss is 3.6018992185592653 and perplexity is 36.66780853706147
At time: 145.77850222587585 and batch: 150, loss is 3.6171827268600465 and perplexity is 37.23252573072545
At time: 146.39612984657288 and batch: 200, loss is 3.486558871269226 and perplexity is 32.67332093236592
At time: 147.01361966133118 and batch: 250, loss is 3.636810154914856 and perplexity is 37.97052325568878
At time: 147.63077926635742 and batch: 300, loss is 3.5948210191726684 and perplexity is 36.40918285900532
At time: 148.2473316192627 and batch: 350, loss is 3.581146602630615 and perplexity is 35.9146971338118
At time: 148.8647050857544 and batch: 400, loss is 3.5090095615386963 and perplexity is 33.41515573349868
At time: 149.4806170463562 and batch: 450, loss is 3.5410586547851564 and perplexity is 34.503427081005
At time: 150.095632314682 and batch: 500, loss is 3.427682013511658 and perplexity is 30.805153995969956
At time: 150.72363328933716 and batch: 550, loss is 3.4647206830978394 and perplexity is 31.967529454697143
At time: 151.344153881073 and batch: 600, loss is 3.4914094018936157 and perplexity is 32.83218886154446
At time: 151.96060466766357 and batch: 650, loss is 3.329547266960144 and perplexity is 27.925695955658533
At time: 152.5771119594574 and batch: 700, loss is 3.316620683670044 and perplexity is 27.56703524396853
At time: 153.1934278011322 and batch: 750, loss is 3.403263349533081 and perplexity is 30.062043103589268
At time: 153.81001257896423 and batch: 800, loss is 3.3758925199508667 and perplexity is 29.25037868093653
At time: 154.426171541214 and batch: 850, loss is 3.434155902862549 and perplexity is 31.005230090840996
At time: 155.04343056678772 and batch: 900, loss is 3.392813115119934 and perplexity is 29.749523501882795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317516588184931 and perplexity of 75.00213562905206
finished 13 epochs...
Completing Train Step...
At time: 156.59447693824768 and batch: 50, loss is 3.6795765209197997 and perplexity is 39.62960820855947
At time: 157.2057523727417 and batch: 100, loss is 3.5894893074035643 and perplexity is 36.2155761764069
At time: 157.81423830986023 and batch: 150, loss is 3.6014708280563354 and perplexity is 36.65210376024974
At time: 158.42411589622498 and batch: 200, loss is 3.471061110496521 and perplexity is 32.170861178157
At time: 159.03448390960693 and batch: 250, loss is 3.6228603076934816 and perplexity is 37.44451763632301
At time: 159.6436469554901 and batch: 300, loss is 3.584743037223816 and perplexity is 36.04409453816633
At time: 160.25460243225098 and batch: 350, loss is 3.570660557746887 and perplexity is 35.540061662677864
At time: 160.86516880989075 and batch: 400, loss is 3.4997465658187865 and perplexity is 33.10706043463408
At time: 161.47558522224426 and batch: 450, loss is 3.5352343797683714 and perplexity is 34.3030537138865
At time: 162.08672881126404 and batch: 500, loss is 3.42144832611084 and perplexity is 30.61372158044676
At time: 162.69606685638428 and batch: 550, loss is 3.458833532333374 and perplexity is 31.779884677733516
At time: 163.30591940879822 and batch: 600, loss is 3.4863013219833374 and perplexity is 32.66490702543654
At time: 163.91584849357605 and batch: 650, loss is 3.326095881462097 and perplexity is 27.829479748736027
At time: 164.52635407447815 and batch: 700, loss is 3.314365086555481 and perplexity is 27.504925192774508
At time: 165.1357822418213 and batch: 750, loss is 3.4017201709747313 and perplexity is 30.015687779721766
At time: 165.7444064617157 and batch: 800, loss is 3.375841250419617 and perplexity is 29.248879066175284
At time: 166.35501861572266 and batch: 850, loss is 3.4363121604919433 and perplexity is 31.072157485196573
At time: 166.96534705162048 and batch: 900, loss is 3.3970509910583497 and perplexity is 29.87586581414535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316448472950556 and perplexity of 74.92206747399639
finished 14 epochs...
Completing Train Step...
At time: 168.5470666885376 and batch: 50, loss is 3.674999032020569 and perplexity is 39.448618671730586
At time: 169.15464305877686 and batch: 100, loss is 3.58390344619751 and perplexity is 36.013844940263176
At time: 169.7647249698639 and batch: 150, loss is 3.595263609886169 and perplexity is 36.425300791785446
At time: 170.37193846702576 and batch: 200, loss is 3.4649489545822143 and perplexity is 31.974827563040886
At time: 170.9799964427948 and batch: 250, loss is 3.6168947267532348 and perplexity is 37.221804303298356
At time: 171.58747959136963 and batch: 300, loss is 3.579384903907776 and perplexity is 35.85148195713747
At time: 172.20815229415894 and batch: 350, loss is 3.565302324295044 and perplexity is 35.35013899479281
At time: 172.81658577919006 and batch: 400, loss is 3.4950861883163453 and perplexity is 32.95312800523427
At time: 173.42524528503418 and batch: 450, loss is 3.5313442039489744 and perplexity is 34.169868029624865
At time: 174.03394889831543 and batch: 500, loss is 3.417689170837402 and perplexity is 30.498855881792764
At time: 174.64416360855103 and batch: 550, loss is 3.455372543334961 and perplexity is 31.670084963896322
At time: 175.2545886039734 and batch: 600, loss is 3.4832803916931154 and perplexity is 32.566377518693656
At time: 175.86490416526794 and batch: 650, loss is 3.323613791465759 and perplexity is 27.760490130088645
At time: 176.4960494041443 and batch: 700, loss is 3.31262092590332 and perplexity is 27.456993996516847
At time: 177.1126515865326 and batch: 750, loss is 3.4004232501983642 and perplexity is 29.976785042959957
At time: 177.7243459224701 and batch: 800, loss is 3.3752774810791015 and perplexity is 29.232394092211408
At time: 178.33462405204773 and batch: 850, loss is 3.436377501487732 and perplexity is 31.074187837239847
At time: 178.9442915916443 and batch: 900, loss is 3.397945079803467 and perplexity is 29.902589434384225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316468957352312 and perplexity of 74.92360222344603
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 180.47350907325745 and batch: 50, loss is 3.6752945232391356 and perplexity is 39.46027711453157
At time: 181.09606385231018 and batch: 100, loss is 3.5858866024017333 and perplexity is 36.085336886712035
At time: 181.70511412620544 and batch: 150, loss is 3.598272304534912 and perplexity is 36.53505843017011
At time: 182.31495571136475 and batch: 200, loss is 3.465628533363342 and perplexity is 31.99656436248023
At time: 182.92360830307007 and batch: 250, loss is 3.6204402923583983 and perplexity is 37.35401088745927
At time: 183.53352093696594 and batch: 300, loss is 3.5817690229415895 and perplexity is 35.937058129017274
At time: 184.1438856124878 and batch: 350, loss is 3.566410608291626 and perplexity is 35.38933870631175
At time: 184.75363326072693 and batch: 400, loss is 3.496127014160156 and perplexity is 32.9874443280564
At time: 185.36402535438538 and batch: 450, loss is 3.52908570766449 and perplexity is 34.09278259099348
At time: 185.98072028160095 and batch: 500, loss is 3.4154427576065065 and perplexity is 30.430419745113696
At time: 186.5953652858734 and batch: 550, loss is 3.452280697822571 and perplexity is 31.572317173160524
At time: 187.20586490631104 and batch: 600, loss is 3.480939927101135 and perplexity is 32.49024619131265
At time: 187.8280692100525 and batch: 650, loss is 3.3189168119430543 and perplexity is 27.6304054181779
At time: 188.43775844573975 and batch: 700, loss is 3.306976180076599 and perplexity is 27.302442855568348
At time: 189.04786205291748 and batch: 750, loss is 3.3952319622039795 and perplexity is 29.821570149841566
At time: 189.65744495391846 and batch: 800, loss is 3.3690740489959716 and perplexity is 29.05161422837584
At time: 190.26838850975037 and batch: 850, loss is 3.427495183944702 and perplexity is 30.799399219986775
At time: 190.87762141227722 and batch: 900, loss is 3.38818338394165 and perplexity is 29.61210954567458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313890431025257 and perplexity of 74.73065860459911
finished 16 epochs...
Completing Train Step...
At time: 192.40731978416443 and batch: 50, loss is 3.671866159439087 and perplexity is 39.32522456584705
At time: 193.02960538864136 and batch: 100, loss is 3.5823890781402588 and perplexity is 35.95934799849563
At time: 193.6392261981964 and batch: 150, loss is 3.5942913770675657 and perplexity is 36.38990412861516
At time: 194.24991846084595 and batch: 200, loss is 3.4620660495758058 and perplexity is 31.88277991863952
At time: 194.86023545265198 and batch: 250, loss is 3.6164140939712524 and perplexity is 37.20391858252184
At time: 195.47097182273865 and batch: 300, loss is 3.578338007926941 and perplexity is 35.8139688243697
At time: 196.0823245048523 and batch: 350, loss is 3.5630362939834597 and perplexity is 35.27012519939326
At time: 196.69341444969177 and batch: 400, loss is 3.493205909729004 and perplexity is 32.89122515979535
At time: 197.30367708206177 and batch: 450, loss is 3.5270343923568728 and perplexity is 34.02291922457993
At time: 197.9153380393982 and batch: 500, loss is 3.4135540533065796 and perplexity is 30.372999922094852
At time: 198.52639436721802 and batch: 550, loss is 3.4507606840133667 and perplexity is 31.524363269599448
At time: 199.13508653640747 and batch: 600, loss is 3.4798934745788572 and perplexity is 32.45626447446398
At time: 199.74225330352783 and batch: 650, loss is 3.3183434343338014 and perplexity is 27.61456730342064
At time: 200.35505390167236 and batch: 700, loss is 3.306722011566162 and perplexity is 27.295504316152922
At time: 200.97112917900085 and batch: 750, loss is 3.3946644115447997 and perplexity is 29.804649700105642
At time: 201.58803534507751 and batch: 800, loss is 3.36973304271698 and perplexity is 29.07076536928523
At time: 202.2147707939148 and batch: 850, loss is 3.428847861289978 and perplexity is 30.841089059653804
At time: 202.85546159744263 and batch: 900, loss is 3.3903144121170046 and perplexity is 29.67528107169237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313185182336259 and perplexity of 74.67797348582941
finished 17 epochs...
Completing Train Step...
At time: 204.4661774635315 and batch: 50, loss is 3.670246748924255 and perplexity is 39.26159242087555
At time: 205.0828080177307 and batch: 100, loss is 3.580480465888977 and perplexity is 35.8907810010779
At time: 205.70641779899597 and batch: 150, loss is 3.5921609783172608 and perplexity is 36.31246164331282
At time: 206.31934905052185 and batch: 200, loss is 3.4600581121444702 and perplexity is 31.818825521127863
At time: 206.92991042137146 and batch: 250, loss is 3.614124631881714 and perplexity is 37.11883905168809
At time: 207.54014086723328 and batch: 300, loss is 3.5763703060150145 and perplexity is 35.74356689715918
At time: 208.15182495117188 and batch: 350, loss is 3.561093168258667 and perplexity is 35.20165745406332
At time: 208.7622320652008 and batch: 400, loss is 3.491570076942444 and perplexity is 32.83746459892125
At time: 209.37498784065247 and batch: 450, loss is 3.5257787895202637 and perplexity is 33.98022675869132
At time: 209.9871702194214 and batch: 500, loss is 3.4123500871658323 and perplexity is 30.33645386311909
At time: 210.60087776184082 and batch: 550, loss is 3.4497050428390503 and perplexity is 31.491102412591793
At time: 211.21160984039307 and batch: 600, loss is 3.479153718948364 and perplexity is 32.43226364854179
At time: 211.82271337509155 and batch: 650, loss is 3.3178653812408445 and perplexity is 27.60136922905206
At time: 212.4333212375641 and batch: 700, loss is 3.3064685440063477 and perplexity is 27.288586668016737
At time: 213.0561170578003 and batch: 750, loss is 3.3943910360336305 and perplexity is 29.796502952370055
At time: 213.66923069953918 and batch: 800, loss is 3.370005588531494 and perplexity is 29.078689564515425
At time: 214.27939987182617 and batch: 850, loss is 3.429500336647034 and perplexity is 30.86121867657534
At time: 214.89022064208984 and batch: 900, loss is 3.3913658809661866 and perplexity is 29.70650011537967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31288711338827 and perplexity of 74.65571761789091
finished 18 epochs...
Completing Train Step...
At time: 216.4546024799347 and batch: 50, loss is 3.668912229537964 and perplexity is 39.20923201041555
At time: 217.0699110031128 and batch: 100, loss is 3.5789636516571046 and perplexity is 35.836382620211246
At time: 217.68491792678833 and batch: 150, loss is 3.5905338668823243 and perplexity is 36.25342526415096
At time: 218.30054903030396 and batch: 200, loss is 3.458482151031494 and perplexity is 31.768719782164503
At time: 218.92865777015686 and batch: 250, loss is 3.612427806854248 and perplexity is 37.05590828294568
At time: 219.543532371521 and batch: 300, loss is 3.5748639631271364 and perplexity is 35.68976536132996
At time: 220.15912175178528 and batch: 350, loss is 3.5596003770828246 and perplexity is 35.14914793306679
At time: 220.77583861351013 and batch: 400, loss is 3.490296025276184 and perplexity is 32.79565461213045
At time: 221.39112186431885 and batch: 450, loss is 3.5247512531280516 and perplexity is 33.945328771627544
At time: 222.00641250610352 and batch: 500, loss is 3.4113584184646606 and perplexity is 30.306385062926665
At time: 222.6219048500061 and batch: 550, loss is 3.4488348245620726 and perplexity is 31.46371020004081
At time: 223.2371633052826 and batch: 600, loss is 3.478497643470764 and perplexity is 32.410992614143936
At time: 223.85263323783875 and batch: 650, loss is 3.3173681020736696 and perplexity is 27.587647055307293
At time: 224.46809339523315 and batch: 700, loss is 3.306155872344971 and perplexity is 27.280055634062403
At time: 225.0838828086853 and batch: 750, loss is 3.394120121002197 and perplexity is 29.78843172519386
At time: 225.69928121566772 and batch: 800, loss is 3.370056004524231 and perplexity is 29.080155632473637
At time: 226.31514382362366 and batch: 850, loss is 3.429807410240173 and perplexity is 30.870696797048797
At time: 226.9304895401001 and batch: 900, loss is 3.3919241714477537 and perplexity is 29.72308960208012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312774658203125 and perplexity of 74.64732266738075
finished 19 epochs...
Completing Train Step...
At time: 228.47129559516907 and batch: 50, loss is 3.667704634666443 and perplexity is 39.16191172054372
At time: 229.10083985328674 and batch: 100, loss is 3.577633228302002 and perplexity is 35.78873676142044
At time: 229.7168483734131 and batch: 150, loss is 3.589144082069397 and perplexity is 36.203075799866824
At time: 230.33310079574585 and batch: 200, loss is 3.4571141147613527 and perplexity is 31.725288735643506
At time: 230.94996452331543 and batch: 250, loss is 3.6110119342803957 and perplexity is 37.00347896408472
At time: 231.5730323791504 and batch: 300, loss is 3.573577527999878 and perplexity is 35.64388231259606
At time: 232.20015454292297 and batch: 350, loss is 3.558327121734619 and perplexity is 35.104422571921106
At time: 232.81745409965515 and batch: 400, loss is 3.489188799858093 and perplexity is 32.759362525209205
At time: 233.43384957313538 and batch: 450, loss is 3.523819341659546 and perplexity is 33.913709465928214
At time: 234.0511589050293 and batch: 500, loss is 3.410466341972351 and perplexity is 30.279361504577825
At time: 234.6901843547821 and batch: 550, loss is 3.448047571182251 and perplexity is 31.438950035384558
At time: 235.30660724639893 and batch: 600, loss is 3.4778728246688844 and perplexity is 32.3907479418556
At time: 235.92907404899597 and batch: 650, loss is 3.316851053237915 and perplexity is 27.573386581514526
At time: 236.55688500404358 and batch: 700, loss is 3.3057968473434447 and perplexity is 27.270263170021646
At time: 237.1860110759735 and batch: 750, loss is 3.3938210010528564 and perplexity is 29.77952274349878
At time: 237.82008910179138 and batch: 800, loss is 3.3699807119369507 and perplexity is 29.0779661947428
At time: 238.43765425682068 and batch: 850, loss is 3.429918427467346 and perplexity is 30.874124166453107
At time: 239.05465626716614 and batch: 900, loss is 3.392217984199524 and perplexity is 29.731823907889606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31275877234054 and perplexity of 74.64613683968946
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
497.66392993927


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8258402347564697 and batch: 50, loss is 6.861038389205933 and perplexity is 954.3575477263522
At time: 1.457862377166748 and batch: 100, loss is 5.978738431930542 and perplexity is 394.9418077635437
At time: 2.0862324237823486 and batch: 150, loss is 5.738264274597168 and perplexity is 310.52495688885796
At time: 2.7012088298797607 and batch: 200, loss is 5.501672992706299 and perplexity is 245.10164270791884
At time: 3.330238103866577 and batch: 250, loss is 5.508664617538452 and perplexity is 246.82130605479645
At time: 3.945904016494751 and batch: 300, loss is 5.399781265258789 and perplexity is 221.35799222523255
At time: 4.561819791793823 and batch: 350, loss is 5.365020666122437 and perplexity is 213.7956529624261
At time: 5.177037954330444 and batch: 400, loss is 5.210893459320069 and perplexity is 183.25771838743617
At time: 5.793434381484985 and batch: 450, loss is 5.199481611251831 and perplexity is 181.17829673676286
At time: 6.409816265106201 and batch: 500, loss is 5.12691026687622 and perplexity is 168.49570618333067
At time: 7.0251970291137695 and batch: 550, loss is 5.179545278549194 and perplexity is 177.60203317486162
At time: 7.640283107757568 and batch: 600, loss is 5.087940807342529 and perplexity is 162.05581409449107
At time: 8.256223440170288 and batch: 650, loss is 4.971801853179931 and perplexity is 144.28663662065352
At time: 8.871483564376831 and batch: 700, loss is 5.047048826217651 and perplexity is 155.56269384497153
At time: 9.501498937606812 and batch: 750, loss is 5.0387038230896 and perplexity is 154.26992426359212
At time: 10.11935806274414 and batch: 800, loss is 4.999995937347412 and perplexity is 148.4125561526965
At time: 10.733041524887085 and batch: 850, loss is 5.027041988372803 and perplexity is 152.4813034758059
At time: 11.34807825088501 and batch: 900, loss is 4.951567420959472 and perplexity is 141.39641802944615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.884006447988014 and perplexity of 132.15909314965145
finished 1 epochs...
Completing Train Step...
At time: 12.900440692901611 and batch: 50, loss is 4.841110458374024 and perplexity is 126.60986868547623
At time: 13.50961709022522 and batch: 100, loss is 4.716302337646485 and perplexity is 111.75425822020355
At time: 14.119311571121216 and batch: 150, loss is 4.7032717514038085 and perplexity is 110.30748136713153
At time: 14.729023456573486 and batch: 200, loss is 4.588353996276855 and perplexity is 98.33244131688521
At time: 15.3384268283844 and batch: 250, loss is 4.6961568832397464 and perplexity is 109.52544352674292
At time: 15.948309898376465 and batch: 300, loss is 4.636492614746094 and perplexity is 103.18181380989745
At time: 16.557446002960205 and batch: 350, loss is 4.632108240127564 and perplexity is 102.7304163551982
At time: 17.16796112060547 and batch: 400, loss is 4.520242042541504 and perplexity is 91.85782878998454
At time: 17.77855086326599 and batch: 450, loss is 4.551372137069702 and perplexity is 94.76234607619085
At time: 18.39142656326294 and batch: 500, loss is 4.441803932189941 and perplexity is 84.92800793908673
At time: 19.01237964630127 and batch: 550, loss is 4.513523263931274 and perplexity is 91.2427250615304
At time: 19.620920658111572 and batch: 600, loss is 4.478604288101196 and perplexity is 88.1116083942295
At time: 20.230268001556396 and batch: 650, loss is 4.331640796661377 and perplexity is 76.06899798609982
At time: 20.84050178527832 and batch: 700, loss is 4.371024503707885 and perplexity is 79.12465368990337
At time: 21.450231790542603 and batch: 750, loss is 4.430174627304077 and perplexity is 83.94607490713724
At time: 22.075758457183838 and batch: 800, loss is 4.391158409118653 and perplexity is 80.73388770421467
At time: 22.68558645248413 and batch: 850, loss is 4.449690504074097 and perplexity is 85.60044691246651
At time: 23.29604744911194 and batch: 900, loss is 4.38577184677124 and perplexity is 80.30017873342638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5394395802119005 and perplexity of 93.63830865112676
finished 2 epochs...
Completing Train Step...
At time: 24.84707999229431 and batch: 50, loss is 4.434436349868775 and perplexity is 84.30459319890811
At time: 25.455792665481567 and batch: 100, loss is 4.312130417823791 and perplexity is 74.59924733560705
At time: 26.076059818267822 and batch: 150, loss is 4.313646759986877 and perplexity is 74.71245112582585
At time: 26.68872594833374 and batch: 200, loss is 4.201614351272583 and perplexity is 66.79407314770937
At time: 27.299437046051025 and batch: 250, loss is 4.343806266784668 and perplexity is 77.00006505971326
At time: 27.92376446723938 and batch: 300, loss is 4.304014344215393 and perplexity is 73.9962446701209
At time: 28.551891803741455 and batch: 350, loss is 4.303670530319214 and perplexity is 73.97080810591095
At time: 29.161527395248413 and batch: 400, loss is 4.217812032699585 and perplexity is 67.88479197673841
At time: 29.771610975265503 and batch: 450, loss is 4.254336342811585 and perplexity is 70.41007352783141
At time: 30.38109588623047 and batch: 500, loss is 4.1375847339630125 and perplexity is 62.65131896058533
At time: 30.99022078514099 and batch: 550, loss is 4.213254714012146 and perplexity is 67.57612323082415
At time: 31.605565309524536 and batch: 600, loss is 4.205882663726807 and perplexity is 67.0797804322116
At time: 32.22080874443054 and batch: 650, loss is 4.0522070360183715 and perplexity is 57.52427519514283
At time: 32.82935404777527 and batch: 700, loss is 4.072528228759766 and perplexity is 58.70519529293281
At time: 33.439860105514526 and batch: 750, loss is 4.162680230140686 and perplexity is 64.24347936488125
At time: 34.04879021644592 and batch: 800, loss is 4.130306754112244 and perplexity is 62.196999194022716
At time: 34.659953594207764 and batch: 850, loss is 4.198794660568237 and perplexity is 66.60599979949409
At time: 35.26983308792114 and batch: 900, loss is 4.143223824501038 and perplexity is 63.00561343096816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.427821329195205 and perplexity of 83.74875703276373
finished 3 epochs...
Completing Train Step...
At time: 36.79940962791443 and batch: 50, loss is 4.2126395130157475 and perplexity is 67.53456311770205
At time: 37.42049598693848 and batch: 100, loss is 4.092840352058411 and perplexity is 59.909815235602906
At time: 38.0295832157135 and batch: 150, loss is 4.101332473754883 and perplexity is 60.42074303742472
At time: 38.639289140701294 and batch: 200, loss is 3.991298179626465 and perplexity is 54.12510788767629
At time: 39.24928545951843 and batch: 250, loss is 4.14109097480774 and perplexity is 62.871375133629535
At time: 39.85807156562805 and batch: 300, loss is 4.104865622520447 and perplexity is 60.634596075938326
At time: 40.46815729141235 and batch: 350, loss is 4.106611337661743 and perplexity is 60.74053925479484
At time: 41.077056884765625 and batch: 400, loss is 4.030933265686035 and perplexity is 56.31344213388408
At time: 41.68750238418579 and batch: 450, loss is 4.069149284362793 and perplexity is 58.50716845147813
At time: 42.29788541793823 and batch: 500, loss is 3.953463249206543 and perplexity is 52.115543771542
At time: 42.90675354003906 and batch: 550, loss is 4.029359173774719 and perplexity is 56.22486932927899
At time: 43.515565395355225 and batch: 600, loss is 4.032845468521118 and perplexity is 56.421227878845926
At time: 44.136698722839355 and batch: 650, loss is 3.878529443740845 and perplexity is 48.35305687266808
At time: 44.745929479599 and batch: 700, loss is 3.8941243648529054 and perplexity is 49.11302943404811
At time: 45.354278564453125 and batch: 750, loss is 3.9903335523605348 and perplexity is 54.07292250660373
At time: 45.96271300315857 and batch: 800, loss is 3.963886785507202 and perplexity is 52.66161307628507
At time: 46.57100868225098 and batch: 850, loss is 4.033271794319153 and perplexity is 56.44528683195927
At time: 47.18019247055054 and batch: 900, loss is 3.9792966747283938 and perplexity is 53.47940757829524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385232167701199 and perplexity of 80.25685409939405
finished 4 epochs...
Completing Train Step...
At time: 48.70750880241394 and batch: 50, loss is 4.05491126537323 and perplexity is 57.68004455184941
At time: 49.33157157897949 and batch: 100, loss is 3.9382024431228637 and perplexity is 51.32625646256117
At time: 49.94253635406494 and batch: 150, loss is 3.954361610412598 and perplexity is 52.16238339059696
At time: 50.553319215774536 and batch: 200, loss is 3.843704009056091 and perplexity is 46.698124770254644
At time: 51.16853857040405 and batch: 250, loss is 3.9942323970794678 and perplexity is 54.2841559505749
At time: 51.77912998199463 and batch: 300, loss is 3.9604387950897215 and perplexity is 52.4803490169806
At time: 52.389482259750366 and batch: 350, loss is 3.9631261825561523 and perplexity is 52.621573726926144
At time: 53.000386238098145 and batch: 400, loss is 3.8971020460128782 and perplexity is 49.259490325203885
At time: 53.611260175704956 and batch: 450, loss is 3.9343565988540647 and perplexity is 51.12924275814656
At time: 54.222604513168335 and batch: 500, loss is 3.8233072996139525 and perplexity is 45.755284791289995
At time: 54.8329017162323 and batch: 550, loss is 3.895763120651245 and perplexity is 49.193579678870954
At time: 55.442896366119385 and batch: 600, loss is 3.902240743637085 and perplexity is 49.5132714448764
At time: 56.054114818573 and batch: 650, loss is 3.7492402172088624 and perplexity is 42.48878748366104
At time: 56.670650005340576 and batch: 700, loss is 3.764496660232544 and perplexity is 43.14198531705504
At time: 57.29125237464905 and batch: 750, loss is 3.8642918395996095 and perplexity is 47.66950282274435
At time: 57.90329694747925 and batch: 800, loss is 3.8422607517242433 and perplexity is 46.630775971810586
At time: 58.51421308517456 and batch: 850, loss is 3.906256422996521 and perplexity is 49.712500619552564
At time: 59.14625310897827 and batch: 900, loss is 3.854924726486206 and perplexity is 47.22506201112364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374607451974529 and perplexity of 79.40866171989998
finished 5 epochs...
Completing Train Step...
At time: 60.77773118019104 and batch: 50, loss is 3.93578094959259 and perplexity is 51.20212062232814
At time: 61.39391016960144 and batch: 100, loss is 3.823578200340271 and perplexity is 45.767681610250676
At time: 62.00982713699341 and batch: 150, loss is 3.8428550958633423 and perplexity is 46.658498937886186
At time: 62.625545263290405 and batch: 200, loss is 3.7333110857009886 and perplexity is 41.81733998551535
At time: 63.241633892059326 and batch: 250, loss is 3.879882116317749 and perplexity is 48.4185069830143
At time: 63.85760498046875 and batch: 300, loss is 3.8503549003601076 and perplexity is 47.00974404648125
At time: 64.47301435470581 and batch: 350, loss is 3.85500403881073 and perplexity is 47.22880768910478
At time: 65.08859014511108 and batch: 400, loss is 3.7927227306365965 and perplexity is 44.37706272401038
At time: 65.70405316352844 and batch: 450, loss is 3.828692798614502 and perplexity is 46.00236455853986
At time: 66.320791721344 and batch: 500, loss is 3.7222697496414185 and perplexity is 41.35816032500507
At time: 66.93569135665894 and batch: 550, loss is 3.7938084602355957 and perplexity is 44.42527038003037
At time: 67.5512261390686 and batch: 600, loss is 3.795678486824036 and perplexity is 44.508424542854875
At time: 68.16642689704895 and batch: 650, loss is 3.651077356338501 and perplexity is 38.51613931110115
At time: 68.78318214416504 and batch: 700, loss is 3.6633181190490722 and perplexity is 38.990503600219796
At time: 69.398446559906 and batch: 750, loss is 3.7630309677124023 and perplexity is 43.078798749223424
At time: 70.01431107521057 and batch: 800, loss is 3.744627122879028 and perplexity is 42.2932340989354
At time: 70.62977719306946 and batch: 850, loss is 3.808409557342529 and perplexity is 45.078686756275275
At time: 71.24574637413025 and batch: 900, loss is 3.7550446224212646 and perplexity is 42.73612675745718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37596193078446 and perplexity of 79.51629194449887
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.79440712928772 and batch: 50, loss is 3.8640010213851927 and perplexity is 47.655641678685576
At time: 73.41147303581238 and batch: 100, loss is 3.7578321504592895 and perplexity is 42.85542109994623
At time: 74.02810001373291 and batch: 150, loss is 3.771464161872864 and perplexity is 43.44362679396927
At time: 74.6447389125824 and batch: 200, loss is 3.642438883781433 and perplexity is 38.18485166840596
At time: 75.28271532058716 and batch: 250, loss is 3.785110912322998 and perplexity is 44.04055492812974
At time: 75.8993558883667 and batch: 300, loss is 3.7453685855865477 and perplexity is 42.32460458338512
At time: 76.51695585250854 and batch: 350, loss is 3.7352783775329588 and perplexity is 41.8996878715065
At time: 77.13381266593933 and batch: 400, loss is 3.6696874856948853 and perplexity is 39.23964099479292
At time: 77.75109076499939 and batch: 450, loss is 3.6934507751464842 and perplexity is 40.18327141839528
At time: 78.36835646629333 and batch: 500, loss is 3.579803147315979 and perplexity is 35.86647973928349
At time: 78.98596954345703 and batch: 550, loss is 3.6302590656280516 and perplexity is 37.7225879773614
At time: 79.60279607772827 and batch: 600, loss is 3.63109965801239 and perplexity is 37.7543106285743
At time: 80.22058343887329 and batch: 650, loss is 3.4781532859802247 and perplexity is 32.39983356752236
At time: 80.8380606174469 and batch: 700, loss is 3.469244055747986 and perplexity is 32.1124580390085
At time: 81.4552788734436 and batch: 750, loss is 3.5575244283676146 and perplexity is 35.076255790926595
At time: 82.07211899757385 and batch: 800, loss is 3.52527765750885 and perplexity is 33.96320244537792
At time: 82.68960332870483 and batch: 850, loss is 3.570932641029358 and perplexity is 35.54973283493675
At time: 83.30646920204163 and batch: 900, loss is 3.504827642440796 and perplexity is 33.27570803829422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321447503076841 and perplexity of 75.29754287089617
finished 7 epochs...
Completing Train Step...
At time: 84.85096597671509 and batch: 50, loss is 3.7728291368484497 and perplexity is 43.50296674695386
At time: 85.47868227958679 and batch: 100, loss is 3.66058762550354 and perplexity is 38.88418549830326
At time: 86.08991003036499 and batch: 150, loss is 3.6768547248840333 and perplexity is 39.52189115646001
At time: 86.69968247413635 and batch: 200, loss is 3.552053813934326 and perplexity is 34.88489103837713
At time: 87.3094744682312 and batch: 250, loss is 3.6977536153793333 and perplexity is 40.356546134712005
At time: 87.93315935134888 and batch: 300, loss is 3.6644100666046144 and perplexity is 39.03310243893159
At time: 88.5415997505188 and batch: 350, loss is 3.6570685958862303 and perplexity is 38.747591378227014
At time: 89.151034116745 and batch: 400, loss is 3.5970201539993285 and perplexity is 36.48933966654984
At time: 89.76151299476624 and batch: 450, loss is 3.626234998703003 and perplexity is 37.571094772580686
At time: 90.37050914764404 and batch: 500, loss is 3.515114412307739 and perplexity is 33.619774219840934
At time: 90.99213171005249 and batch: 550, loss is 3.569031639099121 and perplexity is 35.48221691847434
At time: 91.60249972343445 and batch: 600, loss is 3.5758222103118897 and perplexity is 35.723981369590554
At time: 92.21293878555298 and batch: 650, loss is 3.429822506904602 and perplexity is 30.871162845116903
At time: 92.82370781898499 and batch: 700, loss is 3.42452467918396 and perplexity is 30.70804520891308
At time: 93.43564200401306 and batch: 750, loss is 3.520464997291565 and perplexity is 33.800141784624984
At time: 94.04468631744385 and batch: 800, loss is 3.4935445404052734 and perplexity is 32.90236502365791
At time: 94.65472722053528 and batch: 850, loss is 3.545720090866089 and perplexity is 34.664638046308916
At time: 95.26373052597046 and batch: 900, loss is 3.4880055141448976 and perplexity is 32.72062176477419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324873414758134 and perplexity of 75.55594798640318
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.79483771324158 and batch: 50, loss is 3.747391996383667 and perplexity is 42.41033134626327
At time: 97.41786217689514 and batch: 100, loss is 3.645099401473999 and perplexity is 38.28657840474386
At time: 98.0285108089447 and batch: 150, loss is 3.663818607330322 and perplexity is 39.010022774503284
At time: 98.63825941085815 and batch: 200, loss is 3.5345129919052125 and perplexity is 34.27831683078439
At time: 99.24779057502747 and batch: 250, loss is 3.679395275115967 and perplexity is 39.62242615924194
At time: 99.86157536506653 and batch: 300, loss is 3.642680673599243 and perplexity is 38.19408549301136
At time: 100.49503302574158 and batch: 350, loss is 3.631979126930237 and perplexity is 37.787528976397965
At time: 101.11240935325623 and batch: 400, loss is 3.571496996879578 and perplexity is 35.56980119695201
At time: 101.72823095321655 and batch: 450, loss is 3.5895128440856934 and perplexity is 36.21642858094283
At time: 102.34841227531433 and batch: 500, loss is 3.473734526634216 and perplexity is 32.25698234515963
At time: 102.9586124420166 and batch: 550, loss is 3.52511248588562 and perplexity is 33.95759315136078
At time: 103.57328844070435 and batch: 600, loss is 3.5346913576126098 and perplexity is 34.28443145231749
At time: 104.18340611457825 and batch: 650, loss is 3.3833860540390015 and perplexity is 29.470390694908335
At time: 104.80130076408386 and batch: 700, loss is 3.3717359590530394 and perplexity is 29.129050030310058
At time: 105.41875720024109 and batch: 750, loss is 3.462541298866272 and perplexity is 31.897935788296838
At time: 106.05570793151855 and batch: 800, loss is 3.4296932506561277 and perplexity is 30.867172812295394
At time: 106.67264604568481 and batch: 850, loss is 3.4785777711868286 and perplexity is 32.413589737006866
At time: 107.28953552246094 and batch: 900, loss is 3.42289843082428 and perplexity is 30.658146885293675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3138365027022685 and perplexity of 74.72662861417102
finished 9 epochs...
Completing Train Step...
At time: 108.8314437866211 and batch: 50, loss is 3.718809041976929 and perplexity is 41.21527920008246
At time: 109.44536352157593 and batch: 100, loss is 3.6126978492736814 and perplexity is 37.065916301306636
At time: 110.05628371238708 and batch: 150, loss is 3.6299203395843507 and perplexity is 37.70981251819019
At time: 110.66640973091125 and batch: 200, loss is 3.501669921875 and perplexity is 33.17079837555299
At time: 111.27540683746338 and batch: 250, loss is 3.647238755226135 and perplexity is 38.36857461809025
At time: 111.88310194015503 and batch: 300, loss is 3.6125385427474974 and perplexity is 37.060011929255836
At time: 112.49055290222168 and batch: 350, loss is 3.603797607421875 and perplexity is 36.7374844114312
At time: 113.10062575340271 and batch: 400, loss is 3.5457657289505007 and perplexity is 34.666220110087075
At time: 113.71180820465088 and batch: 450, loss is 3.565662593841553 and perplexity is 35.36287686773356
At time: 114.3215343952179 and batch: 500, loss is 3.4530090570449827 and perplexity is 31.595321538250246
At time: 114.93049764633179 and batch: 550, loss is 3.505555787086487 and perplexity is 33.29994639037611
At time: 115.54103899002075 and batch: 600, loss is 3.518199129104614 and perplexity is 33.7236418207882
At time: 116.15049934387207 and batch: 650, loss is 3.3702636098861696 and perplexity is 29.086193455429466
At time: 116.75936555862427 and batch: 700, loss is 3.3608256340026856 and perplexity is 28.812970029228982
At time: 117.37011480331421 and batch: 750, loss is 3.455171728134155 and perplexity is 31.66372576795773
At time: 117.98068475723267 and batch: 800, loss is 3.4249072217941285 and perplexity is 30.719794591856996
At time: 118.59049367904663 and batch: 850, loss is 3.4773861455917356 and perplexity is 32.374987877896864
At time: 119.20050692558289 and batch: 900, loss is 3.424356288909912 and perplexity is 30.702874708108112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314543841636344 and perplexity of 74.77950436633652
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.7653489112854 and batch: 50, loss is 3.7122506666183472 and perplexity is 40.94585837572715
At time: 121.37521386146545 and batch: 100, loss is 3.6141485023498534 and perplexity is 37.119725106328275
At time: 121.99830508232117 and batch: 150, loss is 3.6322221422195433 and perplexity is 37.79671303957303
At time: 122.60664010047913 and batch: 200, loss is 3.501210513114929 and perplexity is 33.15556292011952
At time: 123.21450924873352 and batch: 250, loss is 3.644829845428467 and perplexity is 38.27625941690738
At time: 123.82378244400024 and batch: 300, loss is 3.6084988737106323 and perplexity is 36.91060372958013
At time: 124.4378490447998 and batch: 350, loss is 3.601750040054321 and perplexity is 36.6623388961909
At time: 125.04949188232422 and batch: 400, loss is 3.5412273645401 and perplexity is 34.50924863679537
At time: 125.65851259231567 and batch: 450, loss is 3.5554006624221803 and perplexity is 35.0018410810677
At time: 126.28082203865051 and batch: 500, loss is 3.4431783866882326 and perplexity is 31.286240075564304
At time: 126.89163899421692 and batch: 550, loss is 3.493980016708374 and perplexity is 32.916696344190434
At time: 127.50007152557373 and batch: 600, loss is 3.510255665779114 and perplexity is 33.45682045465393
At time: 128.10915899276733 and batch: 650, loss is 3.3538160610198973 and perplexity is 28.61170961181243
At time: 128.71969532966614 and batch: 700, loss is 3.3448435163497923 and perplexity is 28.356138047467724
At time: 129.32931399345398 and batch: 750, loss is 3.437613677978516 and perplexity is 31.11262477023619
At time: 129.9374213218689 and batch: 800, loss is 3.4035348653793336 and perplexity is 30.07020653286267
At time: 130.54595232009888 and batch: 850, loss is 3.4525636434555054 and perplexity is 31.581251686357433
At time: 131.15413117408752 and batch: 900, loss is 3.401956706047058 and perplexity is 30.022788382339346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308401865501926 and perplexity of 74.3216180380613
finished 11 epochs...
Completing Train Step...
At time: 132.69319081306458 and batch: 50, loss is 3.7015346002578737 and perplexity is 40.50942245480008
At time: 133.31753706932068 and batch: 100, loss is 3.599518322944641 and perplexity is 36.58061015882739
At time: 133.92865872383118 and batch: 150, loss is 3.619393367767334 and perplexity is 37.31492451869668
At time: 134.5406370162964 and batch: 200, loss is 3.4893884229660035 and perplexity is 32.76590270373331
At time: 135.15198969841003 and batch: 250, loss is 3.633383388519287 and perplexity is 37.84062982690098
At time: 135.76313614845276 and batch: 300, loss is 3.597959542274475 and perplexity is 36.52363342945795
At time: 136.37455940246582 and batch: 350, loss is 3.5909031438827514 and perplexity is 36.266815292450296
At time: 136.98557782173157 and batch: 400, loss is 3.53251407623291 and perplexity is 34.209865802759815
At time: 137.61831521987915 and batch: 450, loss is 3.5474902868270872 and perplexity is 34.72605559307643
At time: 138.24292826652527 and batch: 500, loss is 3.436045780181885 and perplexity is 31.063881576569973
At time: 138.8570237159729 and batch: 550, loss is 3.4875537538528443 and perplexity is 32.705843225559505
At time: 139.4682400226593 and batch: 600, loss is 3.504907031059265 and perplexity is 33.2783498556477
At time: 140.0794997215271 and batch: 650, loss is 3.3504414987564086 and perplexity is 28.51532034352694
At time: 140.69128894805908 and batch: 700, loss is 3.342804307937622 and perplexity is 28.298372889839765
At time: 141.30279421806335 and batch: 750, loss is 3.4367967557907106 and perplexity is 31.087218555603677
At time: 141.91442584991455 and batch: 800, loss is 3.403525881767273 and perplexity is 30.06993639500601
At time: 142.52418446540833 and batch: 850, loss is 3.4546417713165285 and perplexity is 31.646949806275803
At time: 143.1341269016266 and batch: 900, loss is 3.405683126449585 and perplexity is 30.1348746240326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3079767096532535 and perplexity of 74.29002648361255
finished 12 epochs...
Completing Train Step...
At time: 144.69210600852966 and batch: 50, loss is 3.6958022832870485 and perplexity is 40.277873893903035
At time: 145.32554864883423 and batch: 100, loss is 3.593329081535339 and perplexity is 36.35490312980737
At time: 145.94473266601562 and batch: 150, loss is 3.6131754446029665 and perplexity is 37.08362303779856
At time: 146.56331658363342 and batch: 200, loss is 3.4831355953216554 and perplexity is 32.56166236677404
At time: 147.18077516555786 and batch: 250, loss is 3.6269630098342898 and perplexity is 37.598456906550304
At time: 147.79821133613586 and batch: 300, loss is 3.5919540882110597 and perplexity is 36.30494973136393
At time: 148.41296315193176 and batch: 350, loss is 3.5850152254104612 and perplexity is 36.05390665020786
At time: 149.03877449035645 and batch: 400, loss is 3.5271471786499022 and perplexity is 34.02675675992394
At time: 149.65367603302002 and batch: 450, loss is 3.5424421310424803 and perplexity is 34.55119478829335
At time: 150.2676386833191 and batch: 500, loss is 3.4317675018310547 and perplexity is 30.931265530986494
At time: 150.88139820098877 and batch: 550, loss is 3.4834841680526734 and perplexity is 32.57301445275057
At time: 151.50127291679382 and batch: 600, loss is 3.5014824199676515 and perplexity is 33.164579370645164
At time: 152.11785316467285 and batch: 650, loss is 3.347813572883606 and perplexity is 28.44048257255133
At time: 152.75392985343933 and batch: 700, loss is 3.3408486604690553 and perplexity is 28.243085327644582
At time: 153.37335443496704 and batch: 750, loss is 3.4356518173217774 and perplexity is 31.051645971283307
At time: 154.00329685211182 and batch: 800, loss is 3.4028180980682374 and perplexity is 30.048660914301976
At time: 154.62239360809326 and batch: 850, loss is 3.454821448326111 and perplexity is 31.65263654645233
At time: 155.24117755889893 and batch: 900, loss is 3.406486625671387 and perplexity is 30.159097702651298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308233391748716 and perplexity of 74.30909785081684
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 156.85806679725647 and batch: 50, loss is 3.693805847167969 and perplexity is 40.1975419071834
At time: 157.47402048110962 and batch: 100, loss is 3.593487148284912 and perplexity is 36.360650085365464
At time: 158.09033393859863 and batch: 150, loss is 3.614934120178223 and perplexity is 37.14889848221761
At time: 158.7066993713379 and batch: 200, loss is 3.4836671781539916 and perplexity is 32.57897618893814
At time: 159.32283425331116 and batch: 250, loss is 3.6262247371673584 and perplexity is 37.570709237430556
At time: 159.93881607055664 and batch: 300, loss is 3.5906757926940918 and perplexity is 36.25857092610341
At time: 160.5552773475647 and batch: 350, loss is 3.5836679935455322 and perplexity is 36.00536638315263
At time: 161.1712188720703 and batch: 400, loss is 3.52569100856781 and perplexity is 33.97724407293527
At time: 161.78719902038574 and batch: 450, loss is 3.5397967863082886 and perplexity is 34.45991575258807
At time: 162.40292859077454 and batch: 500, loss is 3.430943741798401 and perplexity is 30.90579608247979
At time: 163.01888012886047 and batch: 550, loss is 3.4805711555480956 and perplexity is 32.47826692170872
At time: 163.63521075248718 and batch: 600, loss is 3.499104590415955 and perplexity is 33.08581333696174
At time: 164.250638961792 and batch: 650, loss is 3.3411836671829223 and perplexity is 28.252548535880305
At time: 164.8668177127838 and batch: 700, loss is 3.334631905555725 and perplexity is 28.068049628918256
At time: 165.4822039604187 and batch: 750, loss is 3.4289025926589964 and perplexity is 30.84277708087349
At time: 166.09870386123657 and batch: 800, loss is 3.3950967025756835 and perplexity is 29.817536768130903
At time: 166.7143156528473 and batch: 850, loss is 3.4446167087554933 and perplexity is 31.331272142611073
At time: 167.32969641685486 and batch: 900, loss is 3.396518211364746 and perplexity is 29.85995279895284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307928215967466 and perplexity of 74.28642397376127
finished 14 epochs...
Completing Train Step...
At time: 168.87173080444336 and batch: 50, loss is 3.6912858057022095 and perplexity is 40.096369967039806
At time: 169.48050379753113 and batch: 100, loss is 3.5898026275634765 and perplexity is 36.22692502434438
At time: 170.0886971950531 and batch: 150, loss is 3.611034812927246 and perplexity is 37.00432556329666
At time: 170.6983892917633 and batch: 200, loss is 3.4806403493881226 and perplexity is 32.480514295465774
At time: 171.30799198150635 and batch: 250, loss is 3.6235937118530273 and perplexity is 37.471989674131684
At time: 171.91717648506165 and batch: 300, loss is 3.5880602312088015 and perplexity is 36.16385832179278
At time: 172.52773761749268 and batch: 350, loss is 3.580689940452576 and perplexity is 35.89829999425674
At time: 173.14038157463074 and batch: 400, loss is 3.5232929182052612 and perplexity is 33.8958611921307
At time: 173.7556869983673 and batch: 450, loss is 3.5375923442840578 and perplexity is 34.38403453475347
At time: 174.3688623905182 and batch: 500, loss is 3.42859233379364 and perplexity is 30.83320932016997
At time: 174.9823751449585 and batch: 550, loss is 3.478808264732361 and perplexity is 32.42106172131777
At time: 175.60266661643982 and batch: 600, loss is 3.4978396272659302 and perplexity is 33.043987461974424
At time: 176.21241784095764 and batch: 650, loss is 3.340560574531555 and perplexity is 28.23495006381389
At time: 176.8205132484436 and batch: 700, loss is 3.3341700553894045 and perplexity is 28.055089388603797
At time: 177.4368314743042 and batch: 750, loss is 3.4289724349975588 and perplexity is 30.844931287779122
At time: 178.05348110198975 and batch: 800, loss is 3.3953975582122804 and perplexity is 29.82650889172633
At time: 178.66178059577942 and batch: 850, loss is 3.4457244348526 and perplexity is 31.365997840148825
At time: 179.26983523368835 and batch: 900, loss is 3.3981698846817014 and perplexity is 29.909312438014613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307605064078553 and perplexity of 74.26242205387533
finished 15 epochs...
Completing Train Step...
At time: 180.8325502872467 and batch: 50, loss is 3.6895259141922 and perplexity is 40.02586676314457
At time: 181.4528386592865 and batch: 100, loss is 3.5876430892944335 and perplexity is 36.14877600665311
At time: 182.06076288223267 and batch: 150, loss is 3.608869347572327 and perplexity is 36.9242806768006
At time: 182.66902208328247 and batch: 200, loss is 3.4787156867980955 and perplexity is 32.418060385327806
At time: 183.2771873474121 and batch: 250, loss is 3.6217442989349364 and perplexity is 37.40275253612497
At time: 183.89823126792908 and batch: 300, loss is 3.5862542963027955 and perplexity is 36.09860768464927
At time: 184.5076081752777 and batch: 350, loss is 3.5787935304641723 and perplexity is 35.83028661059443
At time: 185.11632108688354 and batch: 400, loss is 3.521686062812805 and perplexity is 33.84143918065322
At time: 185.72520923614502 and batch: 450, loss is 3.5361181020736696 and perplexity is 34.333381486284026
At time: 186.33187198638916 and batch: 500, loss is 3.42720130443573 and perplexity is 30.790349237534706
At time: 186.93917179107666 and batch: 550, loss is 3.4776410961151125 and perplexity is 32.38324295027345
At time: 187.548091173172 and batch: 600, loss is 3.496924247741699 and perplexity is 33.01375351233314
At time: 188.1571249961853 and batch: 650, loss is 3.3400145196914672 and perplexity is 28.219536441396464
At time: 188.7653591632843 and batch: 700, loss is 3.3338533782958986 and perplexity is 28.046206391033326
At time: 189.3732452392578 and batch: 750, loss is 3.428944425582886 and perplexity is 30.844067351407357
At time: 189.98122429847717 and batch: 800, loss is 3.3954971551895143 and perplexity is 29.82947966979119
At time: 190.5890815258026 and batch: 850, loss is 3.446248106956482 and perplexity is 31.38242763976654
At time: 191.19779706001282 and batch: 900, loss is 3.3990035915374754 and perplexity is 29.934258434224773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307492608893408 and perplexity of 74.25407132900388
finished 16 epochs...
Completing Train Step...
At time: 192.72136926651 and batch: 50, loss is 3.68801589012146 and perplexity is 39.965472350863955
At time: 193.3418264389038 and batch: 100, loss is 3.585950288772583 and perplexity is 36.08763510404103
At time: 193.95074915885925 and batch: 150, loss is 3.607167444229126 and perplexity is 36.86149256487562
At time: 194.55769157409668 and batch: 200, loss is 3.477122731208801 and perplexity is 32.36646096354774
At time: 195.16599488258362 and batch: 250, loss is 3.620166745185852 and perplexity is 37.34379420083432
At time: 195.77456665039062 and batch: 300, loss is 3.5847467374801636 and perplexity is 36.04422791080269
At time: 196.38340425491333 and batch: 350, loss is 3.577251925468445 and perplexity is 35.77509301604858
At time: 196.99239587783813 and batch: 400, loss is 3.520333833694458 and perplexity is 33.79570872718002
At time: 197.60142731666565 and batch: 450, loss is 3.5348740100860594 and perplexity is 34.290694160455224
At time: 198.20858502388 and batch: 500, loss is 3.426089730262756 and perplexity is 30.756142495732927
At time: 198.81804299354553 and batch: 550, loss is 3.4766534614562987 and perplexity is 32.35127592564015
At time: 199.43867206573486 and batch: 600, loss is 3.4961128759384157 and perplexity is 32.98697794755074
At time: 200.04663252830505 and batch: 650, loss is 3.339462718963623 and perplexity is 28.203969176057846
At time: 200.87420225143433 and batch: 700, loss is 3.3334978342056276 and perplexity is 28.03623650056932
At time: 201.48456978797913 and batch: 750, loss is 3.428799557685852 and perplexity is 30.839599359876267
At time: 202.09314346313477 and batch: 800, loss is 3.395450015068054 and perplexity is 29.828073537639344
At time: 202.7019853591919 and batch: 850, loss is 3.4464805269241334 and perplexity is 31.389722390273395
At time: 203.31097745895386 and batch: 900, loss is 3.3994478750228883 and perplexity is 29.94756068566169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307496789383562 and perplexity of 74.25438174806682
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 204.86824226379395 and batch: 50, loss is 3.6873972415924072 and perplexity is 39.94075541651704
At time: 205.48118352890015 and batch: 100, loss is 3.5857271003723143 and perplexity is 36.07958166124361
At time: 206.09049272537231 and batch: 150, loss is 3.6073969411849975 and perplexity is 36.86995313600866
At time: 206.7116219997406 and batch: 200, loss is 3.477219190597534 and perplexity is 32.369583163168045
At time: 207.3206856250763 and batch: 250, loss is 3.6202377796173097 and perplexity is 37.34644699024243
At time: 207.930006980896 and batch: 300, loss is 3.5843041801452635 and perplexity is 36.02827980259639
At time: 208.53994250297546 and batch: 350, loss is 3.576049218177795 and perplexity is 35.732091914906874
At time: 209.14949107170105 and batch: 400, loss is 3.519927906990051 and perplexity is 33.78199293050571
At time: 209.75952672958374 and batch: 450, loss is 3.5343470573425293 and perplexity is 34.27262934515983
At time: 210.36978244781494 and batch: 500, loss is 3.425790243148804 and perplexity is 30.74693280654103
At time: 210.98030638694763 and batch: 550, loss is 3.475264744758606 and perplexity is 32.30638034940396
At time: 211.58993768692017 and batch: 600, loss is 3.494633812904358 and perplexity is 32.93822419169019
At time: 212.1989426612854 and batch: 650, loss is 3.337358717918396 and perplexity is 28.144690378625793
At time: 212.80763578414917 and batch: 700, loss is 3.331401138305664 and perplexity is 27.977514620922744
At time: 213.41721272468567 and batch: 750, loss is 3.426500973701477 and perplexity is 30.76879335864855
At time: 214.028386592865 and batch: 800, loss is 3.3928923988342286 and perplexity is 29.751882248108366
At time: 214.64145636558533 and batch: 850, loss is 3.4431172943115236 and perplexity is 31.284328783183092
At time: 215.25739407539368 and batch: 900, loss is 3.3960505437850954 and perplexity is 29.8459915319693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307252230709547 and perplexity of 74.23622441528381
finished 18 epochs...
Completing Train Step...
At time: 216.84088134765625 and batch: 50, loss is 3.6868840265274048 and perplexity is 39.92026247822184
At time: 217.4528031349182 and batch: 100, loss is 3.584986071586609 and perplexity is 36.05285555628596
At time: 218.06375765800476 and batch: 150, loss is 3.6066683626174925 and perplexity is 36.84310026177103
At time: 218.67495107650757 and batch: 200, loss is 3.476556339263916 and perplexity is 32.348134051371204
At time: 219.28596472740173 and batch: 250, loss is 3.619619216918945 and perplexity is 37.323353014490145
At time: 219.89777636528015 and batch: 300, loss is 3.583754744529724 and perplexity is 36.00849001960986
At time: 220.50896501541138 and batch: 350, loss is 3.5756117057800294 and perplexity is 35.716462101063975
At time: 221.11988997459412 and batch: 400, loss is 3.5194027328491213 and perplexity is 33.764256159244006
At time: 221.73071217536926 and batch: 450, loss is 3.533868227005005 and perplexity is 34.256222498844004
At time: 222.36076593399048 and batch: 500, loss is 3.4253424072265624 and perplexity is 30.733166308327803
At time: 222.97999382019043 and batch: 550, loss is 3.4749749851226808 and perplexity is 32.29702062049716
At time: 223.5978810787201 and batch: 600, loss is 3.49447895526886 and perplexity is 32.933123851097896
At time: 224.22357487678528 and batch: 650, loss is 3.337296290397644 and perplexity is 28.142933430224662
At time: 224.83727073669434 and batch: 700, loss is 3.3313751554489137 and perplexity is 27.976787694611975
At time: 225.44883584976196 and batch: 750, loss is 3.42648841381073 and perplexity is 30.768406908392443
At time: 226.0604407787323 and batch: 800, loss is 3.3930421590805055 and perplexity is 29.756338230977263
At time: 226.69307231903076 and batch: 850, loss is 3.4433363914489745 and perplexity is 31.29118384100091
At time: 227.3032853603363 and batch: 900, loss is 3.3963558864593506 and perplexity is 29.855106178313896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307137685279324 and perplexity of 74.22772148201543
finished 19 epochs...
Completing Train Step...
At time: 228.84718561172485 and batch: 50, loss is 3.686430559158325 and perplexity is 39.9021640456573
At time: 229.47622776031494 and batch: 100, loss is 3.5843952226638796 and perplexity is 36.031560057250076
At time: 230.09292674064636 and batch: 150, loss is 3.6060737705230714 and perplexity is 36.821200157081186
At time: 230.7102108001709 and batch: 200, loss is 3.476026859283447 and perplexity is 32.33101089556962
At time: 231.3262357711792 and batch: 250, loss is 3.619118776321411 and perplexity is 37.304679566271105
At time: 231.94247245788574 and batch: 300, loss is 3.5833070564270018 and perplexity is 35.99237305498693
At time: 232.55771565437317 and batch: 350, loss is 3.575200300216675 and perplexity is 35.70177117202443
At time: 233.1731858253479 and batch: 400, loss is 3.518986072540283 and perplexity is 33.750190864273556
At time: 233.78846669197083 and batch: 450, loss is 3.533476777076721 and perplexity is 34.24281552725378
At time: 234.4045853614807 and batch: 500, loss is 3.4249772357940675 and perplexity is 30.72194548284904
At time: 235.02009534835815 and batch: 550, loss is 3.4747091245651247 and perplexity is 32.28843525789281
At time: 235.63440775871277 and batch: 600, loss is 3.49430992603302 and perplexity is 32.927557660777225
At time: 236.24989438056946 and batch: 650, loss is 3.3372031354904177 and perplexity is 28.14031189997799
At time: 236.87366032600403 and batch: 700, loss is 3.3313352394104006 and perplexity is 27.975670994364155
At time: 237.49556708335876 and batch: 750, loss is 3.426488981246948 and perplexity is 30.768424367505844
At time: 238.1104598045349 and batch: 800, loss is 3.3931303453445434 and perplexity is 29.758962446985503
At time: 238.7254033088684 and batch: 850, loss is 3.4435086488723754 and perplexity is 31.296574443976887
At time: 239.34020566940308 and batch: 900, loss is 3.3965949630737304 and perplexity is 29.86224468931248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307073723779966 and perplexity of 74.22297391748778
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
743.5280601978302


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}, {'best_accuracy': -74.22297391748778, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.5760335999079107, 'rnn_dropout': 0.08348482176534788}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.826371431350708 and batch: 50, loss is 6.849070463180542 and perplexity is 943.0039422641412
At time: 1.4616162776947021 and batch: 100, loss is 6.005063915252686 and perplexity is 405.4769040659809
At time: 2.08430814743042 and batch: 150, loss is 5.822491321563721 and perplexity is 337.8126059819976
At time: 2.7069244384765625 and batch: 200, loss is 5.630522766113281 and perplexity is 278.8078308288358
At time: 3.3295798301696777 and batch: 250, loss is 5.663907623291015 and perplexity is 288.27290644899597
At time: 3.9560461044311523 and batch: 300, loss is 5.556477384567261 and perplexity is 258.9091905460703
At time: 4.586085796356201 and batch: 350, loss is 5.530513410568237 and perplexity is 252.27339760919534
At time: 5.220844030380249 and batch: 400, loss is 5.379539861679077 and perplexity is 216.92243813077448
At time: 5.856790781021118 and batch: 450, loss is 5.38232982635498 and perplexity is 217.52848910785903
At time: 6.483381271362305 and batch: 500, loss is 5.321524868011474 and perplexity is 204.69577817650097
At time: 7.105835437774658 and batch: 550, loss is 5.3673033618927 and perplexity is 214.28424083171848
At time: 7.728162050247192 and batch: 600, loss is 5.281045093536377 and perplexity is 196.57520751623753
At time: 8.35084867477417 and batch: 650, loss is 5.175295696258545 and perplexity is 176.84890010377813
At time: 8.972825050354004 and batch: 700, loss is 5.261899700164795 and perplexity is 192.84749595746806
At time: 9.598522901535034 and batch: 750, loss is 5.24139139175415 and perplexity is 188.93279914506527
At time: 10.234356164932251 and batch: 800, loss is 5.216582555770874 and perplexity is 184.303260497667
At time: 10.856151103973389 and batch: 850, loss is 5.2400932788848875 and perplexity is 188.6877021632601
At time: 11.477858543395996 and batch: 900, loss is 5.157435646057129 and perplexity is 173.71840845248963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.019366956736944 and perplexity of 151.31548422040308
finished 1 epochs...
Completing Train Step...
At time: 13.03649091720581 and batch: 50, loss is 4.94972716331482 and perplexity is 141.13645146633425
At time: 13.644567251205444 and batch: 100, loss is 4.816580772399902 and perplexity is 123.54194973999121
At time: 14.253702163696289 and batch: 150, loss is 4.796950750350952 and perplexity is 121.14046624430522
At time: 14.862783193588257 and batch: 200, loss is 4.678397493362427 and perplexity is 107.5975086191416
At time: 15.47147798538208 and batch: 250, loss is 4.780596714019776 and perplexity is 119.17544248912151
At time: 16.08078956604004 and batch: 300, loss is 4.714580249786377 and perplexity is 111.56197318219185
At time: 16.68914556503296 and batch: 350, loss is 4.705063714981079 and perplexity is 110.50532556790071
At time: 17.2975013256073 and batch: 400, loss is 4.585223293304443 and perplexity is 98.02507304100803
At time: 17.906837940216064 and batch: 450, loss is 4.6063908767700195 and perplexity is 100.12214361281688
At time: 18.515816688537598 and batch: 500, loss is 4.505296106338501 and perplexity is 90.4951362654525
At time: 19.13067078590393 and batch: 550, loss is 4.575907335281372 and perplexity is 97.1161160517283
At time: 19.744741678237915 and batch: 600, loss is 4.530804033279419 and perplexity is 92.83317204300108
At time: 20.352566957473755 and batch: 650, loss is 4.388583469390869 and perplexity is 80.52627022534801
At time: 20.97434425354004 and batch: 700, loss is 4.43018009185791 and perplexity is 83.94653363623597
At time: 21.583292484283447 and batch: 750, loss is 4.482251672744751 and perplexity is 88.43357212745907
At time: 22.191375732421875 and batch: 800, loss is 4.441022672653198 and perplexity is 84.86168303480325
At time: 22.80052137374878 and batch: 850, loss is 4.498604564666748 and perplexity is 89.89160581662385
At time: 23.40860366821289 and batch: 900, loss is 4.43375638961792 and perplexity is 84.2472889110952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.558407300139127 and perplexity of 95.43136520366697
finished 2 epochs...
Completing Train Step...
At time: 24.943110704421997 and batch: 50, loss is 4.463535556793213 and perplexity is 86.79383178818982
At time: 25.552303552627563 and batch: 100, loss is 4.339732928276062 and perplexity is 76.687055658953
At time: 26.16102695465088 and batch: 150, loss is 4.340900192260742 and perplexity is 76.77662196070445
At time: 26.7705659866333 and batch: 200, loss is 4.232005934715271 and perplexity is 68.85521279919766
At time: 27.379512071609497 and batch: 250, loss is 4.369443473815918 and perplexity is 78.99965408732011
At time: 27.98924994468689 and batch: 300, loss is 4.330938172340393 and perplexity is 76.01556883055206
At time: 28.597830772399902 and batch: 350, loss is 4.329955053329468 and perplexity is 75.94087320307041
At time: 29.206170558929443 and batch: 400, loss is 4.243413200378418 and perplexity is 69.64515950383765
At time: 29.815028429031372 and batch: 450, loss is 4.2709395122528075 and perplexity is 71.58886264758775
At time: 30.42356824874878 and batch: 500, loss is 4.158752059936523 and perplexity is 63.991615050242885
At time: 31.03326678276062 and batch: 550, loss is 4.232699637413025 and perplexity is 68.90299441727339
At time: 31.64212203025818 and batch: 600, loss is 4.225225095748901 and perplexity is 68.38989609097308
At time: 32.25095844268799 and batch: 650, loss is 4.076240668296814 and perplexity is 58.92353982563394
At time: 32.85934591293335 and batch: 700, loss is 4.0897309637069705 and perplexity is 59.723821666943685
At time: 33.46889543533325 and batch: 750, loss is 4.181859745979309 and perplexity is 65.48753020578698
At time: 34.077280044555664 and batch: 800, loss is 4.1504678058624265 and perplexity is 63.46368203744448
At time: 34.68516421318054 and batch: 850, loss is 4.215084443092346 and perplexity is 67.69988241698981
At time: 35.29351997375488 and batch: 900, loss is 4.162038249969482 and perplexity is 64.20224956077425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4243795316513275 and perplexity of 83.46100624054773
finished 3 epochs...
Completing Train Step...
At time: 36.82778263092041 and batch: 50, loss is 4.223253140449524 and perplexity is 68.25516715632992
At time: 37.4495050907135 and batch: 100, loss is 4.105927863121033 and perplexity is 60.6990388264838
At time: 38.0658700466156 and batch: 150, loss is 4.110554256439209 and perplexity is 60.98050704343554
At time: 38.68285250663757 and batch: 200, loss is 4.001595344543457 and perplexity is 54.68532241038193
At time: 39.28991889953613 and batch: 250, loss is 4.143784923553467 and perplexity is 63.04097574091351
At time: 39.89738726615906 and batch: 300, loss is 4.115013508796692 and perplexity is 61.2530417120503
At time: 40.504948139190674 and batch: 350, loss is 4.109278454780578 and perplexity is 60.90275761837397
At time: 41.11289834976196 and batch: 400, loss is 4.043737902641296 and perplexity is 57.03915162388014
At time: 41.720219373703 and batch: 450, loss is 4.071152086257935 and perplexity is 58.6244641401377
At time: 42.32927203178406 and batch: 500, loss is 3.9602941846847535 and perplexity is 52.47276036116891
At time: 42.93805956840515 and batch: 550, loss is 4.031494526863098 and perplexity is 56.34505755412468
At time: 43.5460844039917 and batch: 600, loss is 4.039157671928406 and perplexity is 56.778496535674996
At time: 44.15529012680054 and batch: 650, loss is 3.8907646465301515 and perplexity is 48.94830036575819
At time: 44.765451431274414 and batch: 700, loss is 3.9005420064926146 and perplexity is 49.429232811512016
At time: 45.37487244606018 and batch: 750, loss is 3.997181620597839 and perplexity is 54.44448837183987
At time: 45.98504447937012 and batch: 800, loss is 3.971931467056274 and perplexity is 53.08696761026343
At time: 46.604737281799316 and batch: 850, loss is 4.032950935363769 and perplexity is 56.42717876141369
At time: 47.222466707229614 and batch: 900, loss is 3.9851792907714843 and perplexity is 53.794933548455326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379298379976455 and perplexity of 79.78203708857264
finished 4 epochs...
Completing Train Step...
At time: 48.75615429878235 and batch: 50, loss is 4.055923528671265 and perplexity is 57.73846150565923
At time: 49.378628969192505 and batch: 100, loss is 3.94881010055542 and perplexity is 51.87360572272419
At time: 49.98647475242615 and batch: 150, loss is 3.951368770599365 and perplexity is 52.0065031115379
At time: 50.59591221809387 and batch: 200, loss is 3.8474886560440065 and perplexity is 46.875195551350004
At time: 51.20426869392395 and batch: 250, loss is 3.9905452489852906 and perplexity is 54.08437077352605
At time: 51.82805395126343 and batch: 300, loss is 3.9629668855667113 and perplexity is 52.613191936266524
At time: 52.435890674591064 and batch: 350, loss is 3.9572076654434203 and perplexity is 52.31105186324114
At time: 53.044395446777344 and batch: 400, loss is 3.9031775188446045 and perplexity is 49.55967598192553
At time: 53.6586172580719 and batch: 450, loss is 3.927420516014099 and perplexity is 50.775833150842345
At time: 54.277294397354126 and batch: 500, loss is 3.8229299354553223 and perplexity is 45.738021644193196
At time: 54.88618516921997 and batch: 550, loss is 3.8893532657623293 and perplexity is 48.879264405477635
At time: 55.49488544464111 and batch: 600, loss is 3.90201961517334 and perplexity is 49.502323861682534
At time: 56.10381269454956 and batch: 650, loss is 3.7556481742858887 and perplexity is 42.76192801186283
At time: 56.71318578720093 and batch: 700, loss is 3.7652127933502197 and perplexity is 43.17289178675525
At time: 57.322149991989136 and batch: 750, loss is 3.865118832588196 and perplexity is 47.70894147284399
At time: 57.93247056007385 and batch: 800, loss is 3.8411969184875487 and perplexity is 46.58119498010842
At time: 58.54100489616394 and batch: 850, loss is 3.8976043367385866 and perplexity is 49.28423912537342
At time: 59.14994692802429 and batch: 900, loss is 3.854513545036316 and perplexity is 47.20564793328263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36726316687179 and perplexity of 78.82759822838086
finished 5 epochs...
Completing Train Step...
At time: 60.68928790092468 and batch: 50, loss is 3.930670490264893 and perplexity is 50.94112174750065
At time: 61.299083948135376 and batch: 100, loss is 3.8291652631759643 and perplexity is 46.02410418073338
At time: 61.90960955619812 and batch: 150, loss is 3.8319406652450563 and perplexity is 46.15201699734481
At time: 62.5193190574646 and batch: 200, loss is 3.7317991971969606 and perplexity is 41.7541645990264
At time: 63.131104946136475 and batch: 250, loss is 3.872831630706787 and perplexity is 48.07833359914895
At time: 63.741676568984985 and batch: 300, loss is 3.8458874130249026 and perplexity is 46.800197033155676
At time: 64.35227370262146 and batch: 350, loss is 3.841322431564331 and perplexity is 46.5870418961351
At time: 64.96275734901428 and batch: 400, loss is 3.793967995643616 and perplexity is 44.43235834904283
At time: 65.57239103317261 and batch: 450, loss is 3.8143905544281007 and perplexity is 45.34911014499687
At time: 66.18232440948486 and batch: 500, loss is 3.715100574493408 and perplexity is 41.06271673865427
At time: 66.79327940940857 and batch: 550, loss is 3.7803058052062988 and perplexity is 43.82944295823351
At time: 67.4160692691803 and batch: 600, loss is 3.793143825531006 and perplexity is 44.39575361359525
At time: 68.02558398246765 and batch: 650, loss is 3.6529359579086305 and perplexity is 38.58779203440568
At time: 68.63570618629456 and batch: 700, loss is 3.6611022090911867 and perplexity is 38.90419981105648
At time: 69.24594902992249 and batch: 750, loss is 3.759365382194519 and perplexity is 42.92117878960967
At time: 69.85672283172607 and batch: 800, loss is 3.7372161245346067 and perplexity is 41.980957580687424
At time: 70.46666288375854 and batch: 850, loss is 3.796747636795044 and perplexity is 44.55603617111464
At time: 71.07695508003235 and batch: 900, loss is 3.752596015930176 and perplexity is 42.631610811507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371494240956764 and perplexity of 79.16183021796256
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.62081122398376 and batch: 50, loss is 3.8541015672683714 and perplexity is 47.186204261268
At time: 73.23550629615784 and batch: 100, loss is 3.758942894935608 and perplexity is 42.90304896851251
At time: 73.85087823867798 and batch: 150, loss is 3.7601840925216674 and perplexity is 42.95633319061005
At time: 74.46553659439087 and batch: 200, loss is 3.6464105606079102 and perplexity is 38.33681112607473
At time: 75.07994294166565 and batch: 250, loss is 3.781905927658081 and perplexity is 43.899631574174336
At time: 75.69510126113892 and batch: 300, loss is 3.7386594438552856 and perplexity is 42.041593255658626
At time: 76.31053590774536 and batch: 350, loss is 3.7228803539276125 and perplexity is 41.3834215064766
At time: 76.92567539215088 and batch: 400, loss is 3.672724747657776 and perplexity is 39.35900323926704
At time: 77.5647599697113 and batch: 450, loss is 3.6827793979644774 and perplexity is 39.75674045677864
At time: 78.19301629066467 and batch: 500, loss is 3.567825870513916 and perplexity is 35.43945935902196
At time: 78.81346774101257 and batch: 550, loss is 3.62019278049469 and perplexity is 37.34476647070614
At time: 79.43542265892029 and batch: 600, loss is 3.625331315994263 and perplexity is 37.537157760341366
At time: 80.04902672767639 and batch: 650, loss is 3.474784951210022 and perplexity is 32.29088367443383
At time: 80.66328740119934 and batch: 700, loss is 3.4675198698043825 and perplexity is 32.057137895055114
At time: 81.27831196784973 and batch: 750, loss is 3.547451066970825 and perplexity is 34.72469366887493
At time: 81.89324593544006 and batch: 800, loss is 3.5128172636032104 and perplexity is 33.54263323512389
At time: 82.52136158943176 and batch: 850, loss is 3.5573085021972655 and perplexity is 35.06868272698425
At time: 83.13655853271484 and batch: 900, loss is 3.503885908126831 and perplexity is 33.244385913088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326729134337543 and perplexity of 75.69628881475963
finished 7 epochs...
Completing Train Step...
At time: 84.671555519104 and batch: 50, loss is 3.762546615600586 and perplexity is 43.057938494336796
At time: 85.30112099647522 and batch: 100, loss is 3.662567095756531 and perplexity is 38.96123181709944
At time: 85.93283581733704 and batch: 150, loss is 3.665293564796448 and perplexity is 39.06760335286138
At time: 86.55590271949768 and batch: 200, loss is 3.557339210510254 and perplexity is 35.069759643604584
At time: 87.17236566543579 and batch: 250, loss is 3.693272967338562 and perplexity is 40.17612715416104
At time: 87.78929281234741 and batch: 300, loss is 3.6579226446151734 and perplexity is 38.78069784464866
At time: 88.40538883209229 and batch: 350, loss is 3.6446543550491333 and perplexity is 38.26954289098291
At time: 89.02126049995422 and batch: 400, loss is 3.5982944631576537 and perplexity is 36.53586800571622
At time: 89.6366810798645 and batch: 450, loss is 3.6151747369766234 and perplexity is 37.157838206715326
At time: 90.25321221351624 and batch: 500, loss is 3.504098696708679 and perplexity is 33.2514606915102
At time: 90.86952781677246 and batch: 550, loss is 3.5571112203598023 and perplexity is 35.06176499521231
At time: 91.48628807067871 and batch: 600, loss is 3.5699125576019286 and perplexity is 35.51348763133102
At time: 92.10296249389648 and batch: 650, loss is 3.4263491249084472 and perplexity is 30.764121509230378
At time: 92.71897792816162 and batch: 700, loss is 3.423923931121826 and perplexity is 30.689602950392086
At time: 93.33613300323486 and batch: 750, loss is 3.509999842643738 and perplexity is 33.448262520650935
At time: 93.95278525352478 and batch: 800, loss is 3.479473452568054 and perplexity is 32.44263499153805
At time: 94.56947088241577 and batch: 850, loss is 3.533654899597168 and perplexity is 34.24891548711668
At time: 95.18489265441895 and batch: 900, loss is 3.4874483489990236 and perplexity is 32.702396052612805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332324720408819 and perplexity of 76.12104117502439
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.72407531738281 and batch: 50, loss is 3.739844574928284 and perplexity is 42.09144759030964
At time: 97.34503984451294 and batch: 100, loss is 3.6481276988983153 and perplexity is 38.40269728402452
At time: 97.95451331138611 and batch: 150, loss is 3.655630989074707 and perplexity is 38.691927597833235
At time: 98.5772430896759 and batch: 200, loss is 3.5389665460586546 and perplexity is 34.43131761683568
At time: 99.18667674064636 and batch: 250, loss is 3.6757973718643187 and perplexity is 39.48012465036279
At time: 99.79808592796326 and batch: 300, loss is 3.6330450868606565 and perplexity is 37.82783044421518
At time: 100.4083743095398 and batch: 350, loss is 3.6194147777557375 and perplexity is 37.31572343935031
At time: 101.01915836334229 and batch: 400, loss is 3.5668247079849245 and perplexity is 35.4039964552913
At time: 101.62839484214783 and batch: 450, loss is 3.5851583671569824 and perplexity is 36.05906783875664
At time: 102.2387843132019 and batch: 500, loss is 3.465364165306091 and perplexity is 31.988106610949945
At time: 102.84787082672119 and batch: 550, loss is 3.5132652807235716 and perplexity is 33.5576642759056
At time: 103.45771145820618 and batch: 600, loss is 3.527045226097107 and perplexity is 34.02328782204546
At time: 104.06677269935608 and batch: 650, loss is 3.375657329559326 and perplexity is 29.243500081843763
At time: 104.67725992202759 and batch: 700, loss is 3.368347315788269 and perplexity is 29.030509125397174
At time: 105.28673553466797 and batch: 750, loss is 3.4534528255462646 and perplexity is 31.609345658238002
At time: 105.8966224193573 and batch: 800, loss is 3.4130246686935424 and perplexity is 30.35692517852068
At time: 106.5065369606018 and batch: 850, loss is 3.468475522994995 and perplexity is 32.08778804429022
At time: 107.11632704734802 and batch: 900, loss is 3.4267250728607177 and perplexity is 30.775689392039855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322920289758134 and perplexity of 75.40852179324952
finished 9 epochs...
Completing Train Step...
At time: 108.73032093048096 and batch: 50, loss is 3.712081880569458 and perplexity is 40.938947869288484
At time: 109.34030199050903 and batch: 100, loss is 3.613755798339844 and perplexity is 37.105150903289854
At time: 109.94971919059753 and batch: 150, loss is 3.6206152629852295 and perplexity is 37.36054731398356
At time: 110.55931735038757 and batch: 200, loss is 3.5066038179397583 and perplexity is 33.334864055804815
At time: 111.16989398002625 and batch: 250, loss is 3.6441188764572146 and perplexity is 38.24905585571621
At time: 111.77864098548889 and batch: 300, loss is 3.60398307800293 and perplexity is 36.74429876592308
At time: 112.38925743103027 and batch: 350, loss is 3.5915735912323 and perplexity is 36.29113843542195
At time: 112.99819946289062 and batch: 400, loss is 3.540686206817627 and perplexity is 34.49057874253318
At time: 113.60503649711609 and batch: 450, loss is 3.5617373609542846 and perplexity is 35.22434141030386
At time: 114.23214483261108 and batch: 500, loss is 3.4437153005599974 and perplexity is 31.30304260220741
At time: 114.84065961837769 and batch: 550, loss is 3.493906412124634 and perplexity is 32.914273613621056
At time: 115.45783138275146 and batch: 600, loss is 3.510877084732056 and perplexity is 33.47761761819616
At time: 116.07475352287292 and batch: 650, loss is 3.3625772333145143 and perplexity is 28.863483034061257
At time: 116.6846694946289 and batch: 700, loss is 3.3579936695098875 and perplexity is 28.73148815252217
At time: 117.29206275939941 and batch: 750, loss is 3.4460426998138427 and perplexity is 31.375982126975813
At time: 117.90066885948181 and batch: 800, loss is 3.407925238609314 and perplexity is 30.202516194516352
At time: 118.51091289520264 and batch: 850, loss is 3.467368960380554 and perplexity is 32.05230053585651
At time: 119.12038683891296 and batch: 900, loss is 3.4280063247680665 and perplexity is 30.81514607435042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323844596131207 and perplexity of 75.47825459279791
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.65831589698792 and batch: 50, loss is 3.7065580701828003 and perplexity is 40.71343230984372
At time: 121.26623678207397 and batch: 100, loss is 3.61533269405365 and perplexity is 37.16370801380367
At time: 121.87238717079163 and batch: 150, loss is 3.6216600465774538 and perplexity is 37.3996013987947
At time: 122.48184752464294 and batch: 200, loss is 3.5076849985122682 and perplexity is 33.370924553651314
At time: 123.10043334960938 and batch: 250, loss is 3.643027205467224 and perplexity is 38.20732325432369
At time: 123.7181646823883 and batch: 300, loss is 3.601706962585449 and perplexity is 36.66075960944441
At time: 124.33191251754761 and batch: 350, loss is 3.588635215759277 and perplexity is 36.184657960775894
At time: 124.94539642333984 and batch: 400, loss is 3.5352131032943728 and perplexity is 34.302323873620324
At time: 125.56168937683105 and batch: 450, loss is 3.552675681114197 and perplexity is 34.90659155390713
At time: 126.17860913276672 and batch: 500, loss is 3.436516451835632 and perplexity is 31.078505906442206
At time: 126.79334211349487 and batch: 550, loss is 3.481166467666626 and perplexity is 32.49760738383063
At time: 127.41292953491211 and batch: 600, loss is 3.5007540512084963 and perplexity is 33.14043212223929
At time: 128.03023386001587 and batch: 650, loss is 3.348275909423828 and perplexity is 28.453634686983435
At time: 128.65259504318237 and batch: 700, loss is 3.338984522819519 and perplexity is 28.190485370958182
At time: 129.31303572654724 and batch: 750, loss is 3.4274518871307373 and perplexity is 30.798065732996598
At time: 129.93129110336304 and batch: 800, loss is 3.3872549819946287 and perplexity is 29.58463036335391
At time: 130.55337691307068 and batch: 850, loss is 3.443854455947876 and perplexity is 31.30739889233609
At time: 131.1704695224762 and batch: 900, loss is 3.4075496912002565 and perplexity is 30.191175847364757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316435513431078 and perplexity of 74.92109652629516
finished 11 epochs...
Completing Train Step...
At time: 132.80857920646667 and batch: 50, loss is 3.6936303567886353 and perplexity is 40.19048824424892
At time: 133.44859170913696 and batch: 100, loss is 3.5990324878692626 and perplexity is 36.562842331799764
At time: 134.06542015075684 and batch: 150, loss is 3.6078501749038696 and perplexity is 36.88666762948303
At time: 134.68213772773743 and batch: 200, loss is 3.494112458229065 and perplexity is 32.921056170215
At time: 135.30234217643738 and batch: 250, loss is 3.6317590856552124 and perplexity is 37.779215075076245
At time: 135.9193868637085 and batch: 300, loss is 3.591227774620056 and perplexity is 36.27859052663644
At time: 136.53438997268677 and batch: 350, loss is 3.57836953163147 and perplexity is 35.81509783113607
At time: 137.1513397693634 and batch: 400, loss is 3.526133031845093 and perplexity is 33.9922661255173
At time: 137.7691240310669 and batch: 450, loss is 3.5443537616729737 and perplexity is 34.61730708158023
At time: 138.38558292388916 and batch: 500, loss is 3.42912703037262 and perplexity is 30.84970014011205
At time: 139.0028624534607 and batch: 550, loss is 3.4753385305404665 and perplexity is 32.30876418888277
At time: 139.61798214912415 and batch: 600, loss is 3.496052680015564 and perplexity is 32.984992325734865
At time: 140.2355227470398 and batch: 650, loss is 3.344512209892273 and perplexity is 28.34674503189156
At time: 140.8505516052246 and batch: 700, loss is 3.336834678649902 and perplexity is 28.12994531950023
At time: 141.46685433387756 and batch: 750, loss is 3.4265759944915772 and perplexity is 30.771101744424126
At time: 142.08310437202454 and batch: 800, loss is 3.387847695350647 and perplexity is 29.60217076660527
At time: 142.69938898086548 and batch: 850, loss is 3.445800108909607 and perplexity is 31.368371522269406
At time: 143.3148581981659 and batch: 900, loss is 3.41101713180542 and perplexity is 30.29604366280621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316132845943922 and perplexity of 74.89842377759955
finished 12 epochs...
Completing Train Step...
At time: 144.90778613090515 and batch: 50, loss is 3.6877951431274414 and perplexity is 39.95665106664977
At time: 145.53859090805054 and batch: 100, loss is 3.5926189327239992 and perplexity is 36.329094903488595
At time: 146.15587043762207 and batch: 150, loss is 3.6013632297515867 and perplexity is 36.64816026818
At time: 146.77331566810608 and batch: 200, loss is 3.4876587915420534 and perplexity is 32.709278752182286
At time: 147.39730763435364 and batch: 250, loss is 3.6253895425796507 and perplexity is 37.53934348449593
At time: 148.01370525360107 and batch: 300, loss is 3.585164661407471 and perplexity is 36.05929480427628
At time: 148.62455415725708 and batch: 350, loss is 3.57246675491333 and perplexity is 35.604312028295105
At time: 149.23534035682678 and batch: 400, loss is 3.5207694578170776 and perplexity is 33.81043416028668
At time: 149.84593415260315 and batch: 450, loss is 3.5393564605712893 and perplexity is 34.444745504957716
At time: 150.4564607143402 and batch: 500, loss is 3.4246261310577393 and perplexity is 30.711160755675976
At time: 151.0681667327881 and batch: 550, loss is 3.4714133739471436 and perplexity is 32.18219579299317
At time: 151.6788272857666 and batch: 600, loss is 3.4928466129302977 and perplexity is 32.8794095706687
At time: 152.28919196128845 and batch: 650, loss is 3.341888461112976 and perplexity is 28.272467779253308
At time: 152.8990650177002 and batch: 700, loss is 3.3348894357681274 and perplexity is 28.07527893054275
At time: 153.50925302505493 and batch: 750, loss is 3.4253701972961426 and perplexity is 30.73402039702547
At time: 154.1200656890869 and batch: 800, loss is 3.3872979736328124 and perplexity is 29.585902282419035
At time: 154.73014664649963 and batch: 850, loss is 3.4459286022186277 and perplexity is 31.372402407090217
At time: 155.3414661884308 and batch: 900, loss is 3.411736078262329 and perplexity is 30.317832727702907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316459760273973 and perplexity of 74.9229131483757
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 156.9976727962494 and batch: 50, loss is 3.685785050392151 and perplexity is 39.87641516043617
At time: 157.6191782951355 and batch: 100, loss is 3.5921786308288572 and perplexity is 36.313102655120794
At time: 158.2412474155426 and batch: 150, loss is 3.6014245080947878 and perplexity is 36.65040607553157
At time: 158.86075711250305 and batch: 200, loss is 3.4872814846038818 and perplexity is 32.696939642328765
At time: 159.47640371322632 and batch: 250, loss is 3.625117440223694 and perplexity is 37.5291303302674
At time: 160.09115386009216 and batch: 300, loss is 3.585587749481201 and perplexity is 36.074554289680606
At time: 160.70843172073364 and batch: 350, loss is 3.5722951889038086 and perplexity is 35.59820406253329
At time: 161.3531894683838 and batch: 400, loss is 3.519568133354187 and perplexity is 33.76984124614127
At time: 161.97204399108887 and batch: 450, loss is 3.536829767227173 and perplexity is 34.35782405391996
At time: 162.5975341796875 and batch: 500, loss is 3.4237071561813353 and perplexity is 30.682950934560044
At time: 163.21205711364746 and batch: 550, loss is 3.467762088775635 and perplexity is 32.0649036824899
At time: 163.82708477973938 and batch: 600, loss is 3.489933953285217 and perplexity is 32.783782373602186
At time: 164.4489667415619 and batch: 650, loss is 3.337943949699402 and perplexity is 28.161166366567574
At time: 165.0723922252655 and batch: 700, loss is 3.327815418243408 and perplexity is 27.87737472955796
At time: 165.68762230873108 and batch: 750, loss is 3.417423906326294 and perplexity is 30.49076669063302
At time: 166.30283045768738 and batch: 800, loss is 3.378615117073059 and perplexity is 29.330124185937812
At time: 166.91777968406677 and batch: 850, loss is 3.4366029024124147 and perplexity is 31.081192777342455
At time: 167.53280806541443 and batch: 900, loss is 3.4023915004730223 and perplexity is 30.035844961638155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316455579783819 and perplexity of 74.92259993452966
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 169.1623034477234 and batch: 50, loss is 3.6840096616744997 and perplexity is 39.80568183101678
At time: 169.77835392951965 and batch: 100, loss is 3.5896353101730347 and perplexity is 36.22086413684566
At time: 170.39687085151672 and batch: 150, loss is 3.59958384513855 and perplexity is 36.5830070791844
At time: 171.01270866394043 and batch: 200, loss is 3.4848927450180054 and perplexity is 32.61892837965861
At time: 171.62948393821716 and batch: 250, loss is 3.6238346099853516 and perplexity is 37.48101769383154
At time: 172.24617433547974 and batch: 300, loss is 3.584499831199646 and perplexity is 36.03532946314162
At time: 172.86498975753784 and batch: 350, loss is 3.5705422019958495 and perplexity is 35.53585554090211
At time: 173.48225378990173 and batch: 400, loss is 3.518195557594299 and perplexity is 33.72352137666866
At time: 174.10308241844177 and batch: 450, loss is 3.5358305549621583 and perplexity is 34.32351044087254
At time: 174.72224521636963 and batch: 500, loss is 3.4226146125793457 and perplexity is 30.649446778534674
At time: 175.3391797542572 and batch: 550, loss is 3.4660376930236816 and perplexity is 32.00965874455059
At time: 175.9601080417633 and batch: 600, loss is 3.488410472869873 and perplexity is 32.7338749493597
At time: 176.61295580863953 and batch: 650, loss is 3.3365295124053955 and perplexity is 28.121362319416463
At time: 177.22941040992737 and batch: 700, loss is 3.32589759349823 and perplexity is 27.823962044926375
At time: 177.84668946266174 and batch: 750, loss is 3.4154019260406496 and perplexity is 30.429177248792527
At time: 178.46327304840088 and batch: 800, loss is 3.3761227750778198 and perplexity is 29.25711450604251
At time: 179.07953238487244 and batch: 850, loss is 3.434163842201233 and perplexity is 31.00547625284085
At time: 179.69555521011353 and batch: 900, loss is 3.399447493553162 and perplexity is 29.947549261576082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315997816111944 and perplexity of 74.88831093880427
finished 15 epochs...
Completing Train Step...
At time: 181.2402012348175 and batch: 50, loss is 3.683301124572754 and perplexity is 39.77748801794002
At time: 181.8623435497284 and batch: 100, loss is 3.5886849880218508 and perplexity is 36.18645899789353
At time: 182.47260975837708 and batch: 150, loss is 3.5987174034118654 and perplexity is 36.55132376321901
At time: 183.0814437866211 and batch: 200, loss is 3.4839961242675783 and perplexity is 32.58969467934636
At time: 183.69099259376526 and batch: 250, loss is 3.6230327177047728 and perplexity is 37.450974002586584
At time: 184.29973983764648 and batch: 300, loss is 3.5835459089279174 and perplexity is 36.0009709500785
At time: 184.90852236747742 and batch: 350, loss is 3.5696177291870117 and perplexity is 35.50301878939637
At time: 185.51767468452454 and batch: 400, loss is 3.517476463317871 and perplexity is 33.69927970253487
At time: 186.12678241729736 and batch: 450, loss is 3.5350161266326903 and perplexity is 34.2955677817938
At time: 186.7348976135254 and batch: 500, loss is 3.4218000030517577 and perplexity is 30.624489613725803
At time: 187.34240913391113 and batch: 550, loss is 3.4655645227432252 and perplexity is 31.994516308103094
At time: 187.95179104804993 and batch: 600, loss is 3.48805624961853 and perplexity is 32.72228190313057
At time: 188.56028985977173 and batch: 650, loss is 3.336227879524231 and perplexity is 28.112881271023614
At time: 189.1693639755249 and batch: 700, loss is 3.3257491111755373 and perplexity is 27.8198309851177
At time: 189.78958249092102 and batch: 750, loss is 3.415343928337097 and perplexity is 30.427412477567938
At time: 190.41575527191162 and batch: 800, loss is 3.3763867092132567 and perplexity is 29.26483747639796
At time: 191.0249261856079 and batch: 850, loss is 3.4346063423156736 and perplexity is 31.019199215613394
At time: 191.63396048545837 and batch: 900, loss is 3.400262589454651 and perplexity is 29.971969337238612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3157106164383565 and perplexity of 74.8668061285822
finished 16 epochs...
Completing Train Step...
At time: 193.18157124519348 and batch: 50, loss is 3.6827098083496095 and perplexity is 39.753973896784885
At time: 193.8022620677948 and batch: 100, loss is 3.5878997468948364 and perplexity is 36.15805505547874
At time: 194.4119770526886 and batch: 150, loss is 3.5979809665679934 and perplexity is 36.52441593088314
At time: 195.0219705104828 and batch: 200, loss is 3.4832585144042967 and perplexity is 32.56566506244023
At time: 195.63081526756287 and batch: 250, loss is 3.6223569345474242 and perplexity is 37.42567381481257
At time: 196.23943376541138 and batch: 300, loss is 3.5828157901763915 and perplexity is 35.97469555935967
At time: 196.8481125831604 and batch: 350, loss is 3.5688811779022216 and perplexity is 35.47687862326136
At time: 197.4558560848236 and batch: 400, loss is 3.516900510787964 and perplexity is 33.67987610544594
At time: 198.0660560131073 and batch: 450, loss is 3.5343896198272704 and perplexity is 34.27408810446736
At time: 198.673766374588 and batch: 500, loss is 3.421198992729187 and perplexity is 30.606089509224724
At time: 199.28509187698364 and batch: 550, loss is 3.465165390968323 and perplexity is 31.981748828144863
At time: 199.89608550071716 and batch: 600, loss is 3.4877542781829836 and perplexity is 32.712402200459
At time: 200.51876044273376 and batch: 650, loss is 3.335987858772278 and perplexity is 28.106134405847598
At time: 201.1272954940796 and batch: 700, loss is 3.3256649208068847 and perplexity is 27.817488921882177
At time: 201.73519706726074 and batch: 750, loss is 3.4153330755233764 and perplexity is 30.427082256320237
At time: 202.3434121608734 and batch: 800, loss is 3.376597957611084 and perplexity is 29.271020279458156
At time: 202.95181965827942 and batch: 850, loss is 3.434962148666382 and perplexity is 31.03023800740785
At time: 203.55984115600586 and batch: 900, loss is 3.4009063911437987 and perplexity is 29.991271554458336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315512461205051 and perplexity of 74.85197234889104
finished 17 epochs...
Completing Train Step...
At time: 205.10918593406677 and batch: 50, loss is 3.6821770524978636 and perplexity is 39.73280037522078
At time: 205.7176697254181 and batch: 100, loss is 3.587210612297058 and perplexity is 36.13314587262729
At time: 206.3267104625702 and batch: 150, loss is 3.5973233938217164 and perplexity is 36.500406365276454
At time: 206.95055890083313 and batch: 200, loss is 3.482611207962036 and perplexity is 32.54459191877545
At time: 207.5740451812744 and batch: 250, loss is 3.6217559576034546 and perplexity is 37.40318860496045
At time: 208.18226432800293 and batch: 300, loss is 3.5821973752975462 and perplexity is 35.952455149972664
At time: 208.7908160686493 and batch: 350, loss is 3.5682504081726076 and perplexity is 35.45450793824181
At time: 209.3990457057953 and batch: 400, loss is 3.516394090652466 and perplexity is 33.66282425608943
At time: 210.00812005996704 and batch: 450, loss is 3.5338591814041136 and perplexity is 34.2559126321287
At time: 210.62447357177734 and batch: 500, loss is 3.420705614089966 and perplexity is 30.590992842923367
At time: 211.23504519462585 and batch: 550, loss is 3.464810919761658 and perplexity is 31.97041422805986
At time: 211.84398293495178 and batch: 600, loss is 3.4874800157547 and perplexity is 32.70343164779552
At time: 212.4529893398285 and batch: 650, loss is 3.335780839920044 and perplexity is 28.10031650839017
At time: 213.06234288215637 and batch: 700, loss is 3.325598454475403 and perplexity is 27.81564005688682
At time: 213.67002081871033 and batch: 750, loss is 3.4153341817855836 and perplexity is 30.42711591667003
At time: 214.27996826171875 and batch: 800, loss is 3.37677059173584 and perplexity is 29.276073892625302
At time: 214.8887860774994 and batch: 850, loss is 3.435253372192383 and perplexity is 31.03927605871573
At time: 215.4988570213318 and batch: 900, loss is 3.401427707672119 and perplexity is 30.00691057611107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315369070392766 and perplexity of 74.8412400332509
finished 18 epochs...
Completing Train Step...
At time: 217.0600016117096 and batch: 50, loss is 3.6816840171813965 and perplexity is 39.71321552982061
At time: 217.6691346168518 and batch: 100, loss is 3.5865899562835692 and perplexity is 36.110726576410364
At time: 218.27837371826172 and batch: 150, loss is 3.5967248249053956 and perplexity is 36.47856489405812
At time: 218.88788151741028 and batch: 200, loss is 3.482026000022888 and perplexity is 32.525552136867404
At time: 219.4967987537384 and batch: 250, loss is 3.6212066221237182 and perplexity is 37.3826473489414
At time: 220.10564994812012 and batch: 300, loss is 3.5816460847854614 and perplexity is 35.93264036491568
At time: 220.71428227424622 and batch: 350, loss is 3.5676890802383423 and perplexity is 35.43461191715907
At time: 221.3233287334442 and batch: 400, loss is 3.5159318161010744 and perplexity is 33.647266385388775
At time: 221.9318974018097 and batch: 450, loss is 3.5333879709243776 and perplexity is 34.23977468958594
At time: 222.53988313674927 and batch: 500, loss is 3.420275707244873 and perplexity is 30.577844392209
At time: 223.17027163505554 and batch: 550, loss is 3.4644875288009644 and perplexity is 31.960076956669035
At time: 223.77953171730042 and batch: 600, loss is 3.4872253227233885 and perplexity is 32.69510337227675
At time: 224.38925957679749 and batch: 650, loss is 3.335594153404236 and perplexity is 28.095071047851725
At time: 224.99936437606812 and batch: 700, loss is 3.325535283088684 and perplexity is 27.813882959831666
At time: 225.6091389656067 and batch: 750, loss is 3.415335340499878 and perplexity is 30.4271511730246
At time: 226.21811938285828 and batch: 800, loss is 3.376912751197815 and perplexity is 29.280236059377305
At time: 226.82704997062683 and batch: 850, loss is 3.4354950857162474 and perplexity is 31.04677957832463
At time: 227.43577671051025 and batch: 900, loss is 3.401857075691223 and perplexity is 30.019797350250933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315266648383989 and perplexity of 74.83357503564629
finished 19 epochs...
Completing Train Step...
At time: 228.96284437179565 and batch: 50, loss is 3.681220555305481 and perplexity is 39.694814232931684
At time: 229.58611607551575 and batch: 100, loss is 3.586021513938904 and perplexity is 36.09020554339234
At time: 230.19645929336548 and batch: 150, loss is 3.596172547340393 and perplexity is 36.45842416321247
At time: 230.8075819015503 and batch: 200, loss is 3.481486692428589 and perplexity is 32.50801558881303
At time: 231.4183452129364 and batch: 250, loss is 3.6206956005096433 and perplexity is 37.36354888843342
At time: 232.0290915966034 and batch: 300, loss is 3.58114049911499 and perplexity is 35.914477928565645
At time: 232.63968992233276 and batch: 350, loss is 3.567177348136902 and perplexity is 35.41648352757404
At time: 233.25001525878906 and batch: 400, loss is 3.5155019092559816 and perplexity is 33.63280430414763
At time: 233.86059069633484 and batch: 450, loss is 3.532957887649536 and perplexity is 34.22505190139704
At time: 234.4808955192566 and batch: 500, loss is 3.4198876857757567 and perplexity is 30.565981833718045
At time: 235.0907347202301 and batch: 550, loss is 3.4641868543624876 and perplexity is 31.950468823008894
At time: 235.7003309726715 and batch: 600, loss is 3.4869855260849 and perplexity is 32.68726413634181
At time: 236.31034398078918 and batch: 650, loss is 3.335420889854431 and perplexity is 28.090203617796234
At time: 236.9195375442505 and batch: 700, loss is 3.3254706478118896 and perplexity is 27.812085259905874
At time: 237.5293834209442 and batch: 750, loss is 3.415332531929016 and perplexity is 30.42706571633442
At time: 238.139719247818 and batch: 800, loss is 3.377029495239258 and perplexity is 29.28365455200972
At time: 238.76318526268005 and batch: 850, loss is 3.435697250366211 and perplexity is 31.05305677414275
At time: 239.37358689308167 and batch: 900, loss is 3.4022143650054932 and perplexity is 30.030525019387383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315190981512201 and perplexity of 74.82791282734208
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
989.4082429409027


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}, {'best_accuracy': -74.22297391748778, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}}, {'best_accuracy': -74.82791282734208, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.5760335999079107, 'rnn_dropout': 0.08348482176534788}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.1380715185398046, 'rnn_dropout': 0.8184490040160984}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8311243057250977 and batch: 50, loss is 6.828692541122437 and perplexity is 923.9819539764402
At time: 1.4670124053955078 and batch: 100, loss is 5.9274334144592284 and perplexity is 375.1903189620489
At time: 2.0884475708007812 and batch: 150, loss is 5.666530256271362 and perplexity is 289.0299327479438
At time: 2.7098803520202637 and batch: 200, loss is 5.409396867752076 and perplexity is 223.49674892524965
At time: 3.3311524391174316 and batch: 250, loss is 5.383516874313354 and perplexity is 217.78685917523447
At time: 3.9518113136291504 and batch: 300, loss is 5.263135900497437 and perplexity is 193.08604151078737
At time: 4.572799921035767 and batch: 350, loss is 5.213447942733764 and perplexity is 183.7264456125642
At time: 5.193475723266602 and batch: 400, loss is 5.047764682769776 and perplexity is 155.67409428723764
At time: 5.814381837844849 and batch: 450, loss is 5.037093935012817 and perplexity is 154.02176675841497
At time: 6.436251640319824 and batch: 500, loss is 4.9544137096405025 and perplexity is 141.79944634900386
At time: 7.057102918624878 and batch: 550, loss is 5.007394094467163 and perplexity is 149.5146071082026
At time: 7.677359580993652 and batch: 600, loss is 4.920426406860352 and perplexity is 137.06104449748838
At time: 8.298911094665527 and batch: 650, loss is 4.794453887939453 and perplexity is 120.83837246787178
At time: 8.920197010040283 and batch: 700, loss is 4.854015331268311 and perplexity is 128.25434097910028
At time: 9.540721654891968 and batch: 750, loss is 4.8610586643219 and perplexity is 129.1608677555695
At time: 10.162073135375977 and batch: 800, loss is 4.809264221191406 and perplexity is 122.64134740744095
At time: 10.785633563995361 and batch: 850, loss is 4.846492433547974 and perplexity is 127.29311681853667
At time: 11.41205620765686 and batch: 900, loss is 4.764652128219605 and perplexity is 117.29030820171826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7890382531571065 and perplexity of 120.18572482067188
finished 1 epochs...
Completing Train Step...
At time: 12.977261304855347 and batch: 50, loss is 4.760860834121704 and perplexity is 116.84646804441968
At time: 13.593517303466797 and batch: 100, loss is 4.641621503829956 and perplexity is 103.71238133632748
At time: 14.209614515304565 and batch: 150, loss is 4.634371299743652 and perplexity is 102.96316467412237
At time: 14.825373888015747 and batch: 200, loss is 4.518140020370484 and perplexity is 91.66494439194834
At time: 15.44154953956604 and batch: 250, loss is 4.630938110351562 and perplexity is 102.61027873812915
At time: 16.066397666931152 and batch: 300, loss is 4.576093254089355 and perplexity is 97.13417344281285
At time: 16.695409774780273 and batch: 350, loss is 4.57322506904602 and perplexity is 96.85597381407167
At time: 17.31190514564514 and batch: 400, loss is 4.4668423938751225 and perplexity is 87.08131992586756
At time: 17.927039623260498 and batch: 450, loss is 4.492002725601196 and perplexity is 89.30011052835215
At time: 18.54289197921753 and batch: 500, loss is 4.38480842590332 and perplexity is 80.22285312007268
At time: 19.158393621444702 and batch: 550, loss is 4.455942878723144 and perplexity is 86.13732962362332
At time: 19.774210453033447 and batch: 600, loss is 4.431797580718994 and perplexity is 84.08242609195834
At time: 20.39083456993103 and batch: 650, loss is 4.283003978729248 and perplexity is 72.45777502394851
At time: 21.006957054138184 and batch: 700, loss is 4.317905483245849 and perplexity is 75.0313092615296
At time: 21.622882604599 and batch: 750, loss is 4.384567937850952 and perplexity is 80.20356280200892
At time: 22.238919019699097 and batch: 800, loss is 4.346383848190308 and perplexity is 77.19879500692366
At time: 22.85593271255493 and batch: 850, loss is 4.400815439224243 and perplexity is 81.51731398550469
At time: 23.472146034240723 and batch: 900, loss is 4.341323380470276 and perplexity is 76.80911979775179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5250352833369005 and perplexity of 92.29918239435433
finished 2 epochs...
Completing Train Step...
At time: 25.01229500770569 and batch: 50, loss is 4.417914571762085 and perplexity is 82.92317458617633
At time: 25.620657920837402 and batch: 100, loss is 4.297117114067078 and perplexity is 73.48763156763331
At time: 26.231224060058594 and batch: 150, loss is 4.296954426765442 and perplexity is 73.47567703560162
At time: 26.840329885482788 and batch: 200, loss is 4.188106284141541 and perplexity is 65.89788086359303
At time: 27.450132608413696 and batch: 250, loss is 4.325455627441406 and perplexity is 75.59995042576372
At time: 28.059212684631348 and batch: 300, loss is 4.288253407478333 and perplexity is 72.83913704187445
At time: 28.668851852416992 and batch: 350, loss is 4.285932292938233 and perplexity is 72.67026512314867
At time: 29.278513431549072 and batch: 400, loss is 4.200934896469116 and perplexity is 66.74870500841152
At time: 29.888745546340942 and batch: 450, loss is 4.240257749557495 and perplexity is 69.42574398760097
At time: 30.499393939971924 and batch: 500, loss is 4.118910493850708 and perplexity is 61.49220961430872
At time: 31.108782529830933 and batch: 550, loss is 4.188641867637634 and perplexity is 65.93318413409173
At time: 31.735427618026733 and batch: 600, loss is 4.190488781929016 and perplexity is 66.0550695954987
At time: 32.382110834121704 and batch: 650, loss is 4.037819919586181 and perplexity is 56.70259175116786
At time: 33.0090491771698 and batch: 700, loss is 4.054039969444275 and perplexity is 57.6298100515852
At time: 33.623610734939575 and batch: 750, loss is 4.14855230808258 and perplexity is 63.342233849402284
At time: 34.24634838104248 and batch: 800, loss is 4.119142723083496 and perplexity is 61.50649156124849
At time: 34.86069345474243 and batch: 850, loss is 4.178430519104004 and perplexity is 65.26334322092043
At time: 35.49081063270569 and batch: 900, loss is 4.123687129020691 and perplexity is 61.78663809433632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433450359187714 and perplexity of 84.22151062169773
finished 3 epochs...
Completing Train Step...
At time: 37.08596849441528 and batch: 50, loss is 4.212205634117127 and perplexity is 67.50526765163957
At time: 37.7273154258728 and batch: 100, loss is 4.097657227516175 and perplexity is 60.19908949400987
At time: 38.336928367614746 and batch: 150, loss is 4.100523467063904 and perplexity is 60.371882019143825
At time: 38.945605754852295 and batch: 200, loss is 3.991545114517212 and perplexity is 54.138474915604085
At time: 39.552647829055786 and batch: 250, loss is 4.136104717254638 and perplexity is 62.558662545156736
At time: 40.160741090774536 and batch: 300, loss is 4.101736493110657 and perplexity is 60.44515911905478
At time: 40.771151065826416 and batch: 350, loss is 4.101900000572204 and perplexity is 60.45504316162048
At time: 41.38066792488098 and batch: 400, loss is 4.024756879806518 and perplexity is 55.96670049243396
At time: 41.989964962005615 and batch: 450, loss is 4.06929452419281 and perplexity is 58.51566663980157
At time: 42.599509716033936 and batch: 500, loss is 3.9478416347503664 and perplexity is 51.82339222834775
At time: 43.20933747291565 and batch: 550, loss is 4.016296219825745 and perplexity is 55.495182757644834
At time: 43.818177223205566 and batch: 600, loss is 4.028090434074402 and perplexity is 56.153579838882
At time: 44.42844080924988 and batch: 650, loss is 3.878829445838928 and perplexity is 48.36756506731422
At time: 45.03570508956909 and batch: 700, loss is 3.88642991065979 and perplexity is 48.73658161631336
At time: 45.64512848854065 and batch: 750, loss is 3.988535928726196 and perplexity is 53.9758070577931
At time: 46.256386518478394 and batch: 800, loss is 3.9633869695663453 and perplexity is 52.63529853935865
At time: 46.86769938468933 and batch: 850, loss is 4.022102837562561 and perplexity is 55.8183594438523
At time: 47.478588342666626 and batch: 900, loss is 3.9719214057922363 and perplexity is 53.08643349095229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.403490876498288 and perplexity of 81.73570045520522
finished 4 epochs...
Completing Train Step...
At time: 49.05121684074402 and batch: 50, loss is 4.065810375213623 and perplexity is 58.31214409692699
At time: 49.674973249435425 and batch: 100, loss is 3.952371497154236 and perplexity is 52.05867756721829
At time: 50.2848687171936 and batch: 150, loss is 3.9592437601089476 and perplexity is 52.41767062299805
At time: 50.896031618118286 and batch: 200, loss is 3.8480657291412355 and perplexity is 46.902253772164784
At time: 51.50596594810486 and batch: 250, loss is 3.99800696849823 and perplexity is 54.489442564877926
At time: 52.11703610420227 and batch: 300, loss is 3.9662907028198244 and perplexity is 52.78835952262106
At time: 52.72722101211548 and batch: 350, loss is 3.9648521661758425 and perplexity is 52.71247612668095
At time: 53.336894512176514 and batch: 400, loss is 3.8943641233444213 and perplexity is 49.1248061116218
At time: 53.94723701477051 and batch: 450, loss is 3.939189372062683 and perplexity is 51.37693683528625
At time: 54.557106494903564 and batch: 500, loss is 3.8187651348114016 and perplexity is 45.54792802797874
At time: 55.16831731796265 and batch: 550, loss is 3.8893058013916018 and perplexity is 48.876944437009364
At time: 55.77997374534607 and batch: 600, loss is 3.9047771072387696 and perplexity is 49.63901450201593
At time: 56.38991403579712 and batch: 650, loss is 3.759036211967468 and perplexity is 42.90705274050715
At time: 57.00086307525635 and batch: 700, loss is 3.7623231315612795 and perplexity is 43.048316807504634
At time: 57.609861850738525 and batch: 750, loss is 3.8680815362930296 and perplexity is 47.85049852275297
At time: 58.219271659851074 and batch: 800, loss is 3.8437972688674926 and perplexity is 46.70248003164579
At time: 58.82922720909119 and batch: 850, loss is 3.9044820499420165 and perplexity is 49.624370309127706
At time: 59.43896508216858 and batch: 900, loss is 3.856288404464722 and perplexity is 47.28950571846782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395682556988442 and perplexity of 81.09996722482637
finished 5 epochs...
Completing Train Step...
At time: 60.986966371536255 and batch: 50, loss is 3.9522203683853148 and perplexity is 52.05081059784357
At time: 61.59656596183777 and batch: 100, loss is 3.8388179206848143 and perplexity is 46.47051013131099
At time: 62.206247091293335 and batch: 150, loss is 3.8490079259872436 and perplexity is 46.94646575266317
At time: 62.81568932533264 and batch: 200, loss is 3.7383222198486328 and perplexity is 42.027418211351836
At time: 63.43741178512573 and batch: 250, loss is 3.8882430267333983 and perplexity is 48.82502685232465
At time: 64.04672574996948 and batch: 300, loss is 3.8586004161834717 and perplexity is 47.39896609794374
At time: 64.65587973594666 and batch: 350, loss is 3.859535980224609 and perplexity is 47.44333161636791
At time: 65.26628255844116 and batch: 400, loss is 3.7889897775650025 and perplexity is 44.21171404285034
At time: 65.8760986328125 and batch: 450, loss is 3.835303773880005 and perplexity is 46.30749253825382
At time: 66.48546648025513 and batch: 500, loss is 3.7193200063705443 and perplexity is 41.236344121480705
At time: 67.09460854530334 and batch: 550, loss is 3.787630891799927 and perplexity is 44.15167617552823
At time: 67.70362854003906 and batch: 600, loss is 3.8029643630981447 and perplexity is 44.83389163395167
At time: 68.3141222000122 and batch: 650, loss is 3.663209638595581 and perplexity is 38.98627412211939
At time: 68.92366361618042 and batch: 700, loss is 3.6672537183761595 and perplexity is 39.144256957299596
At time: 69.5329601764679 and batch: 750, loss is 3.772565093040466 and perplexity is 43.4914815743165
At time: 70.14419722557068 and batch: 800, loss is 3.7506889724731445 and perplexity is 42.550387949426096
At time: 70.75323748588562 and batch: 850, loss is 3.809921832084656 and perplexity is 45.146909688571704
At time: 71.36244940757751 and batch: 900, loss is 3.762525520324707 and perplexity is 43.057030184826026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.398246033550942 and perplexity of 81.30813178852225
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.90585446357727 and batch: 50, loss is 3.884827289581299 and perplexity is 48.65853789727996
At time: 73.52040696144104 and batch: 100, loss is 3.7703163385391236 and perplexity is 43.393789792940865
At time: 74.13130903244019 and batch: 150, loss is 3.7806227684020994 and perplexity is 43.843337480453464
At time: 74.74214792251587 and batch: 200, loss is 3.655305461883545 and perplexity is 38.679334373151406
At time: 75.35304880142212 and batch: 250, loss is 3.7993741273880004 and perplexity is 44.673215999472646
At time: 75.96434783935547 and batch: 300, loss is 3.755999732017517 and perplexity is 42.77696394111846
At time: 76.57620549201965 and batch: 350, loss is 3.7426997423171997 and perplexity is 42.211797446556695
At time: 77.18750238418579 and batch: 400, loss is 3.6732619476318358 and perplexity is 39.38015257498889
At time: 77.79861211776733 and batch: 450, loss is 3.7009447622299194 and perplexity is 40.48553550235477
At time: 78.40964150428772 and batch: 500, loss is 3.577649555206299 and perplexity is 35.78932108547054
At time: 79.0340359210968 and batch: 550, loss is 3.6279565620422365 and perplexity is 37.63583150015568
At time: 79.64363813400269 and batch: 600, loss is 3.6450400590896606 and perplexity is 38.28430645530527
At time: 80.25339961051941 and batch: 650, loss is 3.4860325050354004 and perplexity is 32.656127324944336
At time: 80.86342787742615 and batch: 700, loss is 3.473545570373535 and perplexity is 32.25088776221777
At time: 81.47391939163208 and batch: 750, loss is 3.5726025772094725 and perplexity is 35.609148216131
At time: 82.08465123176575 and batch: 800, loss is 3.5265761280059813 and perplexity is 34.00733130555264
At time: 82.69578003883362 and batch: 850, loss is 3.5788081407547 and perplexity is 35.83081010531569
At time: 83.31537652015686 and batch: 900, loss is 3.5163000535964968 and perplexity is 33.65965885203575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3385118458369005 and perplexity of 76.59347159053628
finished 7 epochs...
Completing Train Step...
At time: 84.85639357566833 and batch: 50, loss is 3.789055128097534 and perplexity is 44.21460339631655
At time: 85.48559427261353 and batch: 100, loss is 3.672705526351929 and perplexity is 39.358246715098666
At time: 86.10133266448975 and batch: 150, loss is 3.6823439025878906 and perplexity is 39.73943034963094
At time: 86.71560859680176 and batch: 200, loss is 3.5659072971343995 and perplexity is 35.37153133899324
At time: 87.34546661376953 and batch: 250, loss is 3.7113476610183715 and perplexity is 40.90890072531203
At time: 87.97973418235779 and batch: 300, loss is 3.673611226081848 and perplexity is 39.39390961602066
At time: 88.60309219360352 and batch: 350, loss is 3.66446626663208 and perplexity is 39.035296162003796
At time: 89.21870946884155 and batch: 400, loss is 3.5989521408081053 and perplexity is 36.559904732886146
At time: 89.83411979675293 and batch: 450, loss is 3.6311600160598756 and perplexity is 37.75658947382063
At time: 90.45091509819031 and batch: 500, loss is 3.5149795770645142 and perplexity is 33.615241395005974
At time: 91.06665110588074 and batch: 550, loss is 3.5666008138656617 and perplexity is 35.396070595996314
At time: 91.68206644058228 and batch: 600, loss is 3.590679197311401 and perplexity is 36.258694372871744
At time: 92.29913139343262 and batch: 650, loss is 3.4357394218444823 and perplexity is 31.054366355065046
At time: 92.91417479515076 and batch: 700, loss is 3.429631724357605 and perplexity is 30.865273727828814
At time: 93.5300703048706 and batch: 750, loss is 3.534419226646423 and perplexity is 34.27510286621735
At time: 94.14600920677185 and batch: 800, loss is 3.495281710624695 and perplexity is 32.95957170681237
At time: 94.77462244033813 and batch: 850, loss is 3.5532574701309203 and perplexity is 34.92690573419493
At time: 95.39020204544067 and batch: 900, loss is 3.499218158721924 and perplexity is 33.08957105010855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342609562285959 and perplexity of 76.90797384942854
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.92213821411133 and batch: 50, loss is 3.7647713661193847 and perplexity is 43.1538383023595
At time: 97.5532283782959 and batch: 100, loss is 3.658873348236084 and perplexity is 38.81758432578831
At time: 98.17078685760498 and batch: 150, loss is 3.671768069267273 and perplexity is 39.32136733699398
At time: 98.79018902778625 and batch: 200, loss is 3.550900869369507 and perplexity is 34.844693869866376
At time: 99.40709924697876 and batch: 250, loss is 3.6939747190475463 and perplexity is 40.204330714842875
At time: 100.02427172660828 and batch: 300, loss is 3.6525223445892334 and perplexity is 38.57183490992123
At time: 100.64113211631775 and batch: 350, loss is 3.640049767494202 and perplexity is 38.093732507836165
At time: 101.25829720497131 and batch: 400, loss is 3.5738253498077395 and perplexity is 35.65271673858647
At time: 101.87527441978455 and batch: 450, loss is 3.600227909088135 and perplexity is 36.60657646449246
At time: 102.49192810058594 and batch: 500, loss is 3.4848796796798704 and perplexity is 32.6185022051138
At time: 103.10957527160645 and batch: 550, loss is 3.523295693397522 and perplexity is 33.895955259792885
At time: 103.72720241546631 and batch: 600, loss is 3.553053617477417 and perplexity is 34.919786517442624
At time: 104.34481716156006 and batch: 650, loss is 3.385505452156067 and perplexity is 29.532916420504186
At time: 104.96283316612244 and batch: 700, loss is 3.373582887649536 and perplexity is 29.182899018110323
At time: 105.5805926322937 and batch: 750, loss is 3.4833650159835816 and perplexity is 32.56913354189584
At time: 106.19678163528442 and batch: 800, loss is 3.4339502573013307 and perplexity is 30.998854658460424
At time: 106.81206631660461 and batch: 850, loss is 3.4896660614013673 and perplexity is 32.77500104065883
At time: 107.43465089797974 and batch: 900, loss is 3.4363569402694703 and perplexity is 31.073548920649895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327245424871576 and perplexity of 75.73538018251931
finished 9 epochs...
Completing Train Step...
At time: 108.99862098693848 and batch: 50, loss is 3.7371081352233886 and perplexity is 41.97642433076968
At time: 109.60988306999207 and batch: 100, loss is 3.6258128023147584 and perplexity is 37.555235740113986
At time: 110.25379872322083 and batch: 150, loss is 3.636801815032959 and perplexity is 37.97020658732975
At time: 110.8667402267456 and batch: 200, loss is 3.515889039039612 and perplexity is 33.6458270849913
At time: 111.4787712097168 and batch: 250, loss is 3.661990466117859 and perplexity is 38.93877209216839
At time: 112.08958411216736 and batch: 300, loss is 3.6223596000671385 and perplexity is 37.4257735738169
At time: 112.70012521743774 and batch: 350, loss is 3.611144194602966 and perplexity is 37.0083733798101
At time: 113.31113958358765 and batch: 400, loss is 3.547236804962158 and perplexity is 34.717254283276404
At time: 113.92332124710083 and batch: 450, loss is 3.5764914464950563 and perplexity is 35.747897152290804
At time: 114.5342345237732 and batch: 500, loss is 3.4625778865814207 and perplexity is 31.899102882235812
At time: 115.14672422409058 and batch: 550, loss is 3.5033206796646117 and perplexity is 33.22560054947215
At time: 115.76243185997009 and batch: 600, loss is 3.5366984224319458 and perplexity is 34.353311628903406
At time: 116.37942481040955 and batch: 650, loss is 3.371473717689514 and perplexity is 29.12141219003451
At time: 116.99023652076721 and batch: 700, loss is 3.362538719177246 and perplexity is 28.86237140332053
At time: 117.60181498527527 and batch: 750, loss is 3.47543315410614 and perplexity is 32.31182150399758
At time: 118.21844553947449 and batch: 800, loss is 3.428953905105591 and perplexity is 30.84435973982998
At time: 118.84318137168884 and batch: 850, loss is 3.4878365087509153 and perplexity is 32.715092270471835
At time: 119.46396207809448 and batch: 900, loss is 3.438370747566223 and perplexity is 31.136188110662243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328538868525257 and perplexity of 75.8334030092111
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 121.03171944618225 and batch: 50, loss is 3.7316396331787107 and perplexity is 41.747502668260665
At time: 121.65728831291199 and batch: 100, loss is 3.626840376853943 and perplexity is 37.59384637843059
At time: 122.27679896354675 and batch: 150, loss is 3.639579653739929 and perplexity is 38.075828329062745
At time: 122.89239001274109 and batch: 200, loss is 3.514433069229126 and perplexity is 33.596875421226535
At time: 123.49964332580566 and batch: 250, loss is 3.6602051067352295 and perplexity is 38.86931441197583
At time: 124.10777616500854 and batch: 300, loss is 3.6234068727493285 and perplexity is 37.464989095178474
At time: 124.71822929382324 and batch: 350, loss is 3.608786482810974 and perplexity is 36.92122108186225
At time: 125.32697176933289 and batch: 400, loss is 3.53928307056427 and perplexity is 34.44221769760237
At time: 125.95541024208069 and batch: 450, loss is 3.567871527671814 and perplexity is 35.44107746095242
At time: 126.56391739845276 and batch: 500, loss is 3.456231880187988 and perplexity is 31.697311931937307
At time: 127.17146921157837 and batch: 550, loss is 3.492941312789917 and perplexity is 32.88252339357638
At time: 127.78352451324463 and batch: 600, loss is 3.5273641538619995 and perplexity is 34.034140523706796
At time: 128.3934624195099 and batch: 650, loss is 3.358970685005188 and perplexity is 28.759572979071535
At time: 129.00143766403198 and batch: 700, loss is 3.3454709815979005 and perplexity is 28.373936121915726
At time: 129.60850882530212 and batch: 750, loss is 3.4559325265884397 and perplexity is 31.68782464761168
At time: 130.21616077423096 and batch: 800, loss is 3.4075291776657104 and perplexity is 30.19055652598828
At time: 130.8251667022705 and batch: 850, loss is 3.4635068559646607 and perplexity is 31.92874994063893
At time: 131.43327260017395 and batch: 900, loss is 3.4178634309768676 and perplexity is 30.504171079772384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322772300406678 and perplexity of 75.39736296072964
finished 11 epochs...
Completing Train Step...
At time: 132.9652557373047 and batch: 50, loss is 3.71930655002594 and perplexity is 41.23578923475736
At time: 133.58643054962158 and batch: 100, loss is 3.6127998495101927 and perplexity is 37.069697226360226
At time: 134.19581055641174 and batch: 150, loss is 3.6257062482833864 and perplexity is 37.55123429153578
At time: 134.80641174316406 and batch: 200, loss is 3.5021296072006227 and perplexity is 33.186050010013126
At time: 135.4169430732727 and batch: 250, loss is 3.64953330039978 and perplexity is 38.456714127171665
At time: 136.02826762199402 and batch: 300, loss is 3.6123424911499025 and perplexity is 37.05274696688723
At time: 136.63925743103027 and batch: 350, loss is 3.598937849998474 and perplexity is 36.55938226598072
At time: 137.24840378761292 and batch: 400, loss is 3.5306219148635862 and perplexity is 34.145196417988416
At time: 137.85802292823792 and batch: 450, loss is 3.560297613143921 and perplexity is 35.17366373217896
At time: 138.46789479255676 and batch: 500, loss is 3.4487068271636963 and perplexity is 31.45968318472118
At time: 139.07708048820496 and batch: 550, loss is 3.486295323371887 and perplexity is 32.66471108193893
At time: 139.685467004776 and batch: 600, loss is 3.522508487701416 and perplexity is 33.86928267052192
At time: 140.2939910888672 and batch: 650, loss is 3.3553748512268067 and perplexity is 28.656344043377835
At time: 140.9029984474182 and batch: 700, loss is 3.342748637199402 and perplexity is 28.29679754238134
At time: 141.53163647651672 and batch: 750, loss is 3.455034966468811 and perplexity is 31.65939568019246
At time: 142.13982009887695 and batch: 800, loss is 3.408001127243042 and perplexity is 30.204808309177118
At time: 142.76198291778564 and batch: 850, loss is 3.4654938793182373 and perplexity is 31.99225618572259
At time: 143.3720633983612 and batch: 900, loss is 3.421486873626709 and perplexity is 30.61490168611012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322410269959332 and perplexity of 75.37007176010913
finished 12 epochs...
Completing Train Step...
At time: 144.893479347229 and batch: 50, loss is 3.7135072422027586 and perplexity is 40.99734228158005
At time: 145.51337456703186 and batch: 100, loss is 3.606505665779114 and perplexity is 36.83710649343917
At time: 146.12139892578125 and batch: 150, loss is 3.619064884185791 and perplexity is 37.30266919159284
At time: 146.72955894470215 and batch: 200, loss is 3.4955172777175902 and perplexity is 32.96733681186812
At time: 147.3377034664154 and batch: 250, loss is 3.643149256706238 and perplexity is 38.21198679005564
At time: 147.94619798660278 and batch: 300, loss is 3.606240305900574 and perplexity is 36.827332700177955
At time: 148.55385541915894 and batch: 350, loss is 3.5931119871139527 and perplexity is 36.34701153978892
At time: 149.1620454788208 and batch: 400, loss is 3.5252395629882813 and perplexity is 33.96190865810704
At time: 149.77009797096252 and batch: 450, loss is 3.5553503227233887 and perplexity is 35.000079143278604
At time: 150.37906646728516 and batch: 500, loss is 3.4440860414505003 and perplexity is 31.314650071646028
At time: 150.98635292053223 and batch: 550, loss is 3.48221408367157 and perplexity is 32.53167023672793
At time: 151.59437608718872 and batch: 600, loss is 3.519158968925476 and perplexity is 33.75602665475256
At time: 152.2018084526062 and batch: 650, loss is 3.3526721954345704 and perplexity is 28.57900037290294
At time: 152.81045365333557 and batch: 700, loss is 3.3406721687316896 and perplexity is 28.238101096297303
At time: 153.41896438598633 and batch: 750, loss is 3.453819766044617 and perplexity is 31.62094653557211
At time: 154.026846408844 and batch: 800, loss is 3.4074218606948854 and perplexity is 30.18731674075947
At time: 154.6347770690918 and batch: 850, loss is 3.46558198928833 and perplexity is 31.995075146645775
At time: 155.24311232566833 and batch: 900, loss is 3.422306203842163 and perplexity is 30.639995678834133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322748053563784 and perplexity of 75.39553483487857
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 156.78555560112 and batch: 50, loss is 3.7117293643951417 and perplexity is 40.924518771399676
At time: 157.3948106765747 and batch: 100, loss is 3.606609411239624 and perplexity is 36.84092837426415
At time: 158.00420570373535 and batch: 150, loss is 3.6201590967178343 and perplexity is 37.343508579111
At time: 158.61530756950378 and batch: 200, loss is 3.4953373861312866 and perplexity is 32.961406798748556
At time: 159.22471928596497 and batch: 250, loss is 3.6431413078308106 and perplexity is 38.211683048940024
At time: 159.8343162536621 and batch: 300, loss is 3.6076693391799926 and perplexity is 36.879997805330255
At time: 160.44414472579956 and batch: 350, loss is 3.5935070276260377 and perplexity is 36.36137291831676
At time: 161.0534634590149 and batch: 400, loss is 3.523589324951172 and perplexity is 33.90590964318727
At time: 161.6628556251526 and batch: 450, loss is 3.552707543373108 and perplexity is 34.907703774483764
At time: 162.27122354507446 and batch: 500, loss is 3.4419919300079345 and perplexity is 31.24914231881771
At time: 162.87970542907715 and batch: 550, loss is 3.4786927461624146 and perplexity is 32.417316702945335
At time: 163.4884159564972 and batch: 600, loss is 3.5143683338165284 and perplexity is 33.594700584029404
At time: 164.09756875038147 and batch: 650, loss is 3.348106460571289 and perplexity is 28.448813659705472
At time: 164.707377910614 and batch: 700, loss is 3.334761734008789 and perplexity is 28.07169389694183
At time: 165.31657648086548 and batch: 750, loss is 3.4472349214553835 and perplexity is 31.41341155954847
At time: 165.92558312416077 and batch: 800, loss is 3.398990206718445 and perplexity is 29.933857772274223
At time: 166.53463339805603 and batch: 850, loss is 3.4559207725524903 and perplexity is 31.687452189970557
At time: 167.14398097991943 and batch: 900, loss is 3.4129824876785277 and perplexity is 30.355644719609643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321880601856806 and perplexity of 75.33016120781906
finished 14 epochs...
Completing Train Step...
At time: 168.70050716400146 and batch: 50, loss is 3.7085337829589844 and perplexity is 40.79394987181724
At time: 169.31418466567993 and batch: 100, loss is 3.6030942583084107 and perplexity is 36.71165421922574
At time: 169.92845678329468 and batch: 150, loss is 3.6170845794677735 and perplexity is 37.228871634740166
At time: 170.54250574111938 and batch: 200, loss is 3.4925211668014526 and perplexity is 32.86871083513067
At time: 171.15697622299194 and batch: 250, loss is 3.640357928276062 and perplexity is 38.10547331116441
At time: 171.77088117599487 and batch: 300, loss is 3.604240584373474 and perplexity is 36.75376187528966
At time: 172.39758324623108 and batch: 350, loss is 3.5904332113265993 and perplexity is 36.24977633912943
At time: 173.01055121421814 and batch: 400, loss is 3.5211625909805297 and perplexity is 33.823728776331805
At time: 173.6252257823944 and batch: 450, loss is 3.5507504892349244 and perplexity is 34.83945431408517
At time: 174.24010729789734 and batch: 500, loss is 3.43987181186676 and perplexity is 31.182960626584617
At time: 174.85442328453064 and batch: 550, loss is 3.476782579421997 and perplexity is 32.35545332625835
At time: 175.46815824508667 and batch: 600, loss is 3.513283247947693 and perplexity is 33.55826721939724
At time: 176.08192348480225 and batch: 650, loss is 3.3473715114593507 and perplexity is 28.42791291080479
At time: 176.69551062583923 and batch: 700, loss is 3.3342685222625734 and perplexity is 28.05785202154405
At time: 177.30958676338196 and batch: 750, loss is 3.4472063779830933 and perplexity is 31.41251492450268
At time: 177.92258310317993 and batch: 800, loss is 3.3994469165802004 and perplexity is 29.947531982654883
At time: 178.53601932525635 and batch: 850, loss is 3.4568722867965698 and perplexity is 31.717617601221214
At time: 179.14935779571533 and batch: 900, loss is 3.4146873331069947 and perplexity is 30.407440541127432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3216878812607025 and perplexity of 75.31564493308397
finished 15 epochs...
Completing Train Step...
At time: 180.67818403244019 and batch: 50, loss is 3.7065650987625123 and perplexity is 40.7137184684537
At time: 181.30631828308105 and batch: 100, loss is 3.600977029800415 and perplexity is 36.63400948316601
At time: 181.93869304656982 and batch: 150, loss is 3.614982237815857 and perplexity is 37.15068604245954
At time: 182.57146835327148 and batch: 200, loss is 3.49056529045105 and perplexity is 32.804486528814856
At time: 183.1959228515625 and batch: 250, loss is 3.6384350633621216 and perplexity is 38.03227203414349
At time: 183.8116672039032 and batch: 300, loss is 3.602148299217224 and perplexity is 36.67694291648918
At time: 184.42724657058716 and batch: 350, loss is 3.588576636314392 and perplexity is 36.182538345682744
At time: 185.0426149368286 and batch: 400, loss is 3.5196038103103637 and perplexity is 33.771046072779654
At time: 185.65937066078186 and batch: 450, loss is 3.5493616104125976 and perplexity is 34.79110012063741
At time: 186.27478885650635 and batch: 500, loss is 3.438595623970032 and perplexity is 31.143190692001028
At time: 186.89031529426575 and batch: 550, loss is 3.4755700254440307 and perplexity is 32.31624436891133
At time: 187.5055091381073 and batch: 600, loss is 3.5124984550476075 and perplexity is 33.53194126110804
At time: 188.14094042778015 and batch: 650, loss is 3.346839036941528 and perplexity is 28.41277980094015
At time: 188.75590634346008 and batch: 700, loss is 3.333885588645935 and perplexity is 28.047109783707597
At time: 189.3710482120514 and batch: 750, loss is 3.447151403427124 and perplexity is 31.410788082909438
At time: 189.98650240898132 and batch: 800, loss is 3.399641923904419 and perplexity is 29.953372540191015
At time: 190.60186791419983 and batch: 850, loss is 3.457335376739502 and perplexity is 31.732309112429096
At time: 191.2173457145691 and batch: 900, loss is 3.415479831695557 and perplexity is 30.431547946118545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321636461231806 and perplexity of 75.31177230001148
finished 16 epochs...
Completing Train Step...
At time: 192.75952243804932 and batch: 50, loss is 3.7049508094787598 and perplexity is 40.64804776904262
At time: 193.3902611732483 and batch: 100, loss is 3.5992578983306887 and perplexity is 36.57108490790736
At time: 194.00990343093872 and batch: 150, loss is 3.613227334022522 and perplexity is 37.08554733539791
At time: 194.6205472946167 and batch: 200, loss is 3.48889892578125 and perplexity is 32.749867811441426
At time: 195.22932887077332 and batch: 250, loss is 3.6368159580230714 and perplexity is 37.970743603383575
At time: 195.83815932273865 and batch: 300, loss is 3.6004979324340822 and perplexity is 36.61646242941368
At time: 196.4465935230255 and batch: 350, loss is 3.587059874534607 and perplexity is 36.127699653553876
At time: 197.0555760860443 and batch: 400, loss is 3.518271164894104 and perplexity is 33.72607121745194
At time: 197.66357326507568 and batch: 450, loss is 3.548147482872009 and perplexity is 34.74888492031805
At time: 198.27275276184082 and batch: 500, loss is 3.4375084161758425 and perplexity is 31.1093499716256
At time: 198.88121032714844 and batch: 550, loss is 3.474569206237793 and perplexity is 32.2839178300822
At time: 199.4904625415802 and batch: 600, loss is 3.5117689323425294 and perplexity is 33.50748786934948
At time: 200.0990869998932 and batch: 650, loss is 3.3463003253936767 and perplexity is 28.39747763045287
At time: 200.70911383628845 and batch: 700, loss is 3.333475456237793 and perplexity is 28.03560911359055
At time: 201.31711411476135 and batch: 750, loss is 3.446986222267151 and perplexity is 31.405600040993416
At time: 201.92623805999756 and batch: 800, loss is 3.3996593236923216 and perplexity is 29.95389372705444
At time: 202.53523063659668 and batch: 850, loss is 3.4575445365905764 and perplexity is 31.73894693163574
At time: 203.15616965293884 and batch: 900, loss is 3.4158931589126587 and perplexity is 30.444128732954916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321667814907962 and perplexity of 75.31413363794903
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 204.7001554965973 and batch: 50, loss is 3.7043094539642336 and perplexity is 40.62198627768516
At time: 205.31008005142212 and batch: 100, loss is 3.5991064214706423 and perplexity is 36.56554565434199
At time: 205.9206621646881 and batch: 150, loss is 3.613627543449402 and perplexity is 37.10039229139008
At time: 206.5282759666443 and batch: 200, loss is 3.4889355421066286 and perplexity is 32.75106701321236
At time: 207.13653802871704 and batch: 250, loss is 3.6371452379226685 and perplexity is 37.98324866474461
At time: 207.74557065963745 and batch: 300, loss is 3.6005716133117676 and perplexity is 36.61916046189869
At time: 208.3560438156128 and batch: 350, loss is 3.586735191345215 and perplexity is 36.115971500875
At time: 208.96628260612488 and batch: 400, loss is 3.5175659465789795 and perplexity is 33.70229535890302
At time: 209.57889938354492 and batch: 450, loss is 3.5473166799545286 and perplexity is 34.720027434448646
At time: 210.1903054714203 and batch: 500, loss is 3.436451206207275 and perplexity is 31.076478235944982
At time: 210.7988965511322 and batch: 550, loss is 3.473038048744202 and perplexity is 32.23452389197354
At time: 211.40891551971436 and batch: 600, loss is 3.5099038648605347 and perplexity is 33.445052384615536
At time: 212.01862359046936 and batch: 650, loss is 3.3445222043991087 and perplexity is 28.347028345044343
At time: 212.62875533103943 and batch: 700, loss is 3.331432375907898 and perplexity is 27.978388585046172
At time: 213.23750591278076 and batch: 750, loss is 3.4450790071487427 and perplexity is 31.345759887953733
At time: 213.84877038002014 and batch: 800, loss is 3.3969850873947145 and perplexity is 29.873896950012316
At time: 214.45869278907776 and batch: 850, loss is 3.454390435218811 and perplexity is 31.638996784889375
At time: 215.06915283203125 and batch: 900, loss is 3.412253985404968 and perplexity is 30.333538616566873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321225519049658 and perplexity of 75.28082987417048
finished 18 epochs...
Completing Train Step...
At time: 216.60938453674316 and batch: 50, loss is 3.703629174232483 and perplexity is 40.59436136115775
At time: 217.2322506904602 and batch: 100, loss is 3.5983939743041993 and perplexity is 36.53950391273514
At time: 217.8430106639862 and batch: 150, loss is 3.6129535102844237 and perplexity is 37.075393822397004
At time: 218.45375561714172 and batch: 200, loss is 3.488320369720459 and perplexity is 32.7309256570065
At time: 219.07973265647888 and batch: 250, loss is 3.6364609384536744 and perplexity is 37.957265638951945
At time: 219.69477105140686 and batch: 300, loss is 3.599870867729187 and perplexity is 36.59350873568379
At time: 220.31807947158813 and batch: 350, loss is 3.5861692237854004 and perplexity is 36.0955368158442
At time: 220.9311408996582 and batch: 400, loss is 3.517095031738281 and perplexity is 33.68642818419017
At time: 221.54108500480652 and batch: 450, loss is 3.546895241737366 and perplexity is 34.70539817086873
At time: 222.15015959739685 and batch: 500, loss is 3.436035780906677 and perplexity is 31.063570961822027
At time: 222.76050996780396 and batch: 550, loss is 3.4727615690231324 and perplexity is 32.22561293170575
At time: 223.37039422988892 and batch: 600, loss is 3.5097123479843138 and perplexity is 33.43864770597939
At time: 223.9818034172058 and batch: 650, loss is 3.3444439220428466 and perplexity is 28.344809359727297
At time: 224.59114241600037 and batch: 700, loss is 3.3314046621322633 and perplexity is 27.977613209006652
At time: 225.20136404037476 and batch: 750, loss is 3.4450286436080932 and perplexity is 31.344181244254802
At time: 225.81259632110596 and batch: 800, loss is 3.3970743894577025 and perplexity is 29.876564869763037
At time: 226.42279767990112 and batch: 850, loss is 3.454660439491272 and perplexity is 31.64754060257941
At time: 227.03307175636292 and batch: 900, loss is 3.4126024055480957 and perplexity is 30.344109273841248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321039905286815 and perplexity of 75.26685801279262
finished 19 epochs...
Completing Train Step...
At time: 228.60072255134583 and batch: 50, loss is 3.7030906105041503 and perplexity is 40.57250459671238
At time: 229.22468447685242 and batch: 100, loss is 3.597830014228821 and perplexity is 36.51890290097431
At time: 229.83372926712036 and batch: 150, loss is 3.6124072742462157 and perplexity is 37.05514743631673
At time: 230.44324731826782 and batch: 200, loss is 3.48782516002655 and perplexity is 32.71472099801381
At time: 231.05382990837097 and batch: 250, loss is 3.6359197425842287 and perplexity is 37.93672888127816
At time: 231.663715839386 and batch: 300, loss is 3.5993236446380616 and perplexity is 36.57348940073908
At time: 232.27423119544983 and batch: 350, loss is 3.5856932973861695 and perplexity is 36.07836208425739
At time: 232.88281893730164 and batch: 400, loss is 3.5167014312744143 and perplexity is 33.673171799461585
At time: 233.4932520389557 and batch: 450, loss is 3.546536865234375 and perplexity is 34.6929628000428
At time: 234.10297322273254 and batch: 500, loss is 3.4357064628601073 and perplexity is 31.053342851556483
At time: 234.72588467597961 and batch: 550, loss is 3.472503318786621 and perplexity is 32.21729173406629
At time: 235.33491230010986 and batch: 600, loss is 3.509547719955444 and perplexity is 33.43314322042822
At time: 235.9449725151062 and batch: 650, loss is 3.3443524122238157 and perplexity is 28.342215650029086
At time: 236.55494713783264 and batch: 700, loss is 3.331358914375305 and perplexity is 27.976333325233426
At time: 237.16519355773926 and batch: 750, loss is 3.4450170755386353 and perplexity is 31.343818654686302
At time: 237.77523756027222 and batch: 800, loss is 3.3971391582489012 and perplexity is 29.878500001422218
At time: 238.3848032951355 and batch: 850, loss is 3.454840440750122 and perplexity is 31.65323771245548
At time: 238.99388003349304 and batch: 900, loss is 3.4128643321990966 and perplexity is 30.352058245739297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320955041336687 and perplexity of 75.26047084093204
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
1235.075956106186


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}, {'best_accuracy': -74.22297391748778, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}}, {'best_accuracy': -74.82791282734208, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.5760335999079107, 'rnn_dropout': 0.08348482176534788}}, {'best_accuracy': -75.26047084093204, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.1380715185398046, 'rnn_dropout': 0.8184490040160984}}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.38464634051501334, 'rnn_dropout': 0.5618569990909659}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.82210373878479 and batch: 50, loss is 6.838477582931518 and perplexity is 933.0675349286647
At time: 1.450981855392456 and batch: 100, loss is 5.9857244873046875 and perplexity is 397.71055314572135
At time: 2.066950798034668 and batch: 150, loss is 5.73843092918396 and perplexity is 310.5767116096979
At time: 2.6836023330688477 and batch: 200, loss is 5.5108171081542965 and perplexity is 247.35315879854082
At time: 3.300248384475708 and batch: 250, loss is 5.508671398162842 and perplexity is 246.8229796630383
At time: 3.9157371520996094 and batch: 300, loss is 5.405337581634521 and perplexity is 222.59135055378664
At time: 4.531595945358276 and batch: 350, loss is 5.364238634109497 and perplexity is 213.62852327648213
At time: 5.1475608348846436 and batch: 400, loss is 5.206658134460449 and perplexity is 182.48320373538115
At time: 5.763124227523804 and batch: 450, loss is 5.204106855392456 and perplexity is 182.01823154654662
At time: 6.386390924453735 and batch: 500, loss is 5.127131652832031 and perplexity is 168.53301289573287
At time: 7.02039361000061 and batch: 550, loss is 5.177594928741455 and perplexity is 177.25598465113427
At time: 7.650359392166138 and batch: 600, loss is 5.087250509262085 and perplexity is 161.9439858789387
At time: 8.278845310211182 and batch: 650, loss is 4.970092172622681 and perplexity is 144.04016331869516
At time: 8.91873288154602 and batch: 700, loss is 5.046620054244995 and perplexity is 155.4960072195596
At time: 9.546096563339233 and batch: 750, loss is 5.0407896327972415 and perplexity is 154.59203778619215
At time: 10.182558059692383 and batch: 800, loss is 5.000175476074219 and perplexity is 148.43920434619005
At time: 10.80603837966919 and batch: 850, loss is 5.027562770843506 and perplexity is 152.5607337469173
At time: 11.434137344360352 and batch: 900, loss is 4.949610195159912 and perplexity is 141.11994396146105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.875476993926584 and perplexity of 131.03664198863854
finished 1 epochs...
Completing Train Step...
At time: 12.988354206085205 and batch: 50, loss is 4.838312082290649 and perplexity is 126.25606193009446
At time: 13.601770639419556 and batch: 100, loss is 4.709321708679199 and perplexity is 110.97685973009372
At time: 14.215709209442139 and batch: 150, loss is 4.6965491771698 and perplexity is 109.5684181222102
At time: 14.8286771774292 and batch: 200, loss is 4.582457704544067 and perplexity is 97.75435052689325
At time: 15.441461563110352 and batch: 250, loss is 4.692132043838501 and perplexity is 109.0855071367453
At time: 16.05450987815857 and batch: 300, loss is 4.632828950881958 and perplexity is 102.804481957801
At time: 16.667673110961914 and batch: 350, loss is 4.627415437698364 and perplexity is 102.24945222525731
At time: 17.28027105331421 and batch: 400, loss is 4.513447570800781 and perplexity is 91.23581887541447
At time: 17.894060134887695 and batch: 450, loss is 4.54421257019043 and perplexity is 94.08631166636383
At time: 18.507107973098755 and batch: 500, loss is 4.439107475280761 and perplexity is 84.69931169864385
At time: 19.119884967803955 and batch: 550, loss is 4.509217052459717 and perplexity is 90.8506593569459
At time: 19.73330521583557 and batch: 600, loss is 4.478481769561768 and perplexity is 88.10081374994787
At time: 20.34646201133728 and batch: 650, loss is 4.332344846725464 and perplexity is 76.12257322661966
At time: 20.959499835968018 and batch: 700, loss is 4.369958600997925 and perplexity is 79.0403594398263
At time: 21.573182821273804 and batch: 750, loss is 4.423746118545532 and perplexity is 83.40815768462302
At time: 22.187310218811035 and batch: 800, loss is 4.389768352508545 and perplexity is 80.6217409931483
At time: 22.80012822151184 and batch: 850, loss is 4.445120248794556 and perplexity is 85.21012363593893
At time: 23.413281440734863 and batch: 900, loss is 4.378845376968384 and perplexity is 79.74590377065022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.543177774507705 and perplexity of 93.98900191378719
finished 2 epochs...
Completing Train Step...
At time: 24.974623918533325 and batch: 50, loss is 4.432724676132202 and perplexity is 84.16041466935981
At time: 25.589569807052612 and batch: 100, loss is 4.308835010528565 and perplexity is 74.3538170502038
At time: 26.217232942581177 and batch: 150, loss is 4.312011594772339 and perplexity is 74.59038375201203
At time: 26.832045793533325 and batch: 200, loss is 4.203552889823913 and perplexity is 66.92368161842683
At time: 27.447383165359497 and batch: 250, loss is 4.341521406173706 and perplexity is 76.82433148383234
At time: 28.061992406845093 and batch: 300, loss is 4.304988865852356 and perplexity is 74.06839075985897
At time: 28.67678952217102 and batch: 350, loss is 4.304711475372314 and perplexity is 74.04784774274059
At time: 29.29189157485962 and batch: 400, loss is 4.215052900314331 and perplexity is 67.69774700830561
At time: 29.906790018081665 and batch: 450, loss is 4.25590027809143 and perplexity is 70.52027647855735
At time: 30.52144980430603 and batch: 500, loss is 4.1389754056930546 and perplexity is 62.738506989625684
At time: 31.13618016242981 and batch: 550, loss is 4.212484521865845 and perplexity is 67.52409666923064
At time: 31.750977754592896 and batch: 600, loss is 4.2067873620986935 and perplexity is 67.14049486033616
At time: 32.36550998687744 and batch: 650, loss is 4.0566872549057 and perplexity is 57.78257472651488
At time: 32.981520891189575 and batch: 700, loss is 4.076985621452332 and perplexity is 58.96745145658462
At time: 33.59673619270325 and batch: 750, loss is 4.160586056709289 and perplexity is 64.10908315086208
At time: 34.21160817146301 and batch: 800, loss is 4.137687239646912 and perplexity is 62.65774140604553
At time: 34.82621622085571 and batch: 850, loss is 4.196468410491943 and perplexity is 66.45123766488308
At time: 35.44041395187378 and batch: 900, loss is 4.136743593215942 and perplexity is 62.598642540597304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428776153146404 and perplexity of 83.82876054041621
finished 3 epochs...
Completing Train Step...
At time: 36.976548194885254 and batch: 50, loss is 4.213175597190857 and perplexity is 67.57077703424892
At time: 37.59785723686218 and batch: 100, loss is 4.095819401741028 and perplexity is 60.08855565777564
At time: 38.206406354904175 and batch: 150, loss is 4.099390153884888 and perplexity is 60.30350052575243
At time: 38.81320595741272 and batch: 200, loss is 3.9931815338134764 and perplexity is 54.22714068803049
At time: 39.42001438140869 and batch: 250, loss is 4.138586530685425 and perplexity is 62.71411429540338
At time: 40.02619814872742 and batch: 300, loss is 4.108270545005798 and perplexity is 60.8414040582798
At time: 40.63304352760315 and batch: 350, loss is 4.108164401054382 and perplexity is 60.83494645396725
At time: 41.25224280357361 and batch: 400, loss is 4.028558826446533 and perplexity is 56.17988790811488
At time: 41.86044192314148 and batch: 450, loss is 4.074520683288574 and perplexity is 58.82227932873337
At time: 42.4786262512207 and batch: 500, loss is 3.957998580932617 and perplexity is 52.35244185024924
At time: 43.09093236923218 and batch: 550, loss is 4.028938279151917 and perplexity is 56.20120956359388
At time: 43.71724319458008 and batch: 600, loss is 4.03379828453064 and perplexity is 56.475012547423354
At time: 44.330681800842285 and batch: 650, loss is 3.8809632444381714 and perplexity is 48.47088189935845
At time: 44.940054416656494 and batch: 700, loss is 3.898104453086853 and perplexity is 49.308893143496775
At time: 45.549479961395264 and batch: 750, loss is 3.9909810876846312 and perplexity is 54.10794797289337
At time: 46.15910720825195 and batch: 800, loss is 3.969741086959839 and perplexity is 52.97081422949619
At time: 46.77561092376709 and batch: 850, loss is 4.033822045326233 and perplexity is 56.47635445459495
At time: 47.39456510543823 and batch: 900, loss is 3.977198963165283 and perplexity is 53.36734078962399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3814329382491435 and perplexity of 79.95251838224897
finished 4 epochs...
Completing Train Step...
At time: 48.92635202407837 and batch: 50, loss is 4.056779842376709 and perplexity is 57.787924916653665
At time: 49.547372341156006 and batch: 100, loss is 3.944562191963196 and perplexity is 51.65371874779999
At time: 50.154117584228516 and batch: 150, loss is 3.9492138814926148 and perplexity is 51.89455552513938
At time: 50.760953187942505 and batch: 200, loss is 3.8428043031692507 and perplexity is 46.65612908720892
At time: 51.367993116378784 and batch: 250, loss is 3.9962696075439452 and perplexity is 54.394856923436734
At time: 51.97571897506714 and batch: 300, loss is 3.9667487239837644 and perplexity is 52.81254324639797
At time: 52.581650733947754 and batch: 350, loss is 3.965450267791748 and perplexity is 52.744012974010246
At time: 53.18923902511597 and batch: 400, loss is 3.895768632888794 and perplexity is 49.1938508463154
At time: 53.79628801345825 and batch: 450, loss is 3.939028472900391 and perplexity is 51.36867099418959
At time: 54.40295696258545 and batch: 500, loss is 3.825247664451599 and perplexity is 45.84415292745633
At time: 55.00846552848816 and batch: 550, loss is 3.894246187210083 and perplexity is 49.11901286351223
At time: 55.616042137145996 and batch: 600, loss is 3.9038512468338014 and perplexity is 49.59307697309797
At time: 56.22385549545288 and batch: 650, loss is 3.750413188934326 and perplexity is 42.53865487082864
At time: 56.84370017051697 and batch: 700, loss is 3.7680948257446287 and perplexity is 43.29749693122777
At time: 57.45141816139221 and batch: 750, loss is 3.86422146320343 and perplexity is 47.66614813297489
At time: 58.05993103981018 and batch: 800, loss is 3.8447883796691893 and perplexity is 46.74879030958864
At time: 58.666563272476196 and batch: 850, loss is 3.9068012762069704 and perplexity is 49.739594015406205
At time: 59.272820234298706 and batch: 900, loss is 3.8583143901824952 and perplexity is 47.385410699910885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369726311670591 and perplexity of 79.02200134017764
finished 5 epochs...
Completing Train Step...
At time: 60.81501889228821 and batch: 50, loss is 3.937363796234131 and perplexity is 51.28322990183601
At time: 61.42373871803284 and batch: 100, loss is 3.8260029649734495 and perplexity is 45.87879211993956
At time: 62.033997535705566 and batch: 150, loss is 3.8356825351715087 and perplexity is 46.32503534599243
At time: 62.64164471626282 and batch: 200, loss is 3.732043137550354 and perplexity is 41.764351367125734
At time: 63.24918580055237 and batch: 250, loss is 3.8828947067260744 and perplexity is 48.56459204948644
At time: 63.85563397407532 and batch: 300, loss is 3.8552471160888673 and perplexity is 47.24028933453455
At time: 64.46290159225464 and batch: 350, loss is 3.8587396430969236 and perplexity is 47.405565769109636
At time: 65.07034182548523 and batch: 400, loss is 3.789887547492981 and perplexity is 44.25142381263383
At time: 65.67872786521912 and batch: 450, loss is 3.832944540977478 and perplexity is 46.198371150222485
At time: 66.28666353225708 and batch: 500, loss is 3.724197630882263 and perplexity is 41.43797085434317
At time: 66.89435887336731 and batch: 550, loss is 3.7889563131332396 and perplexity is 44.210234547718
At time: 67.51653981208801 and batch: 600, loss is 3.8026277208328247 and perplexity is 44.818801191291826
At time: 68.12397527694702 and batch: 650, loss is 3.650222616195679 and perplexity is 38.4832320862519
At time: 68.73137617111206 and batch: 700, loss is 3.6666490697860716 and perplexity is 39.1205955916469
At time: 69.33889746665955 and batch: 750, loss is 3.758861846923828 and perplexity is 42.89957190260085
At time: 69.94708251953125 and batch: 800, loss is 3.745705966949463 and perplexity is 42.33888652525933
At time: 70.55451250076294 and batch: 850, loss is 3.804821171760559 and perplexity is 44.91721692793086
At time: 71.16366577148438 and batch: 900, loss is 3.7626261615753176 and perplexity is 43.06136371625376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376754969766695 and perplexity of 79.57937647467324
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 72.715656042099 and batch: 50, loss is 3.8728357934951783 and perplexity is 48.0785337394945
At time: 73.33871364593506 and batch: 100, loss is 3.75852334022522 and perplexity is 42.8850525677291
At time: 73.95421409606934 and batch: 150, loss is 3.7648609828948976 and perplexity is 43.157705783492176
At time: 74.56050515174866 and batch: 200, loss is 3.65000373840332 and perplexity is 38.47480988312028
At time: 75.16805601119995 and batch: 250, loss is 3.791600184440613 and perplexity is 44.32727537059083
At time: 75.77617311477661 and batch: 300, loss is 3.74987286567688 and perplexity is 42.51567645470682
At time: 76.38436508178711 and batch: 350, loss is 3.7462682914733887 and perplexity is 42.36270141468884
At time: 76.99167537689209 and batch: 400, loss is 3.665773878097534 and perplexity is 39.08637254957989
At time: 77.59861946105957 and batch: 450, loss is 3.697353148460388 and perplexity is 40.340387908655615
At time: 78.20497846603394 and batch: 500, loss is 3.5841005516052244 and perplexity is 36.020944163478156
At time: 78.81209206581116 and batch: 550, loss is 3.628208794593811 and perplexity is 37.645325679285776
At time: 79.41937923431396 and batch: 600, loss is 3.635644545555115 and perplexity is 37.92629024260279
At time: 80.02597856521606 and batch: 650, loss is 3.4744734621047972 and perplexity is 32.28082698232749
At time: 80.63163805007935 and batch: 700, loss is 3.4695395469665526 and perplexity is 32.12194839045432
At time: 81.23816776275635 and batch: 750, loss is 3.5527452707290648 and perplexity is 34.909020774693005
At time: 81.84279131889343 and batch: 800, loss is 3.518246669769287 and perplexity is 33.725245103245804
At time: 82.44926142692566 and batch: 850, loss is 3.565591607093811 and perplexity is 35.36036666121068
At time: 83.05577826499939 and batch: 900, loss is 3.5112709522247316 and perplexity is 33.49080596056738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329043453686858 and perplexity of 75.87167707457394
finished 7 epochs...
Completing Train Step...
At time: 84.57848954200745 and batch: 50, loss is 3.7763162422180176 and perplexity is 43.65493097954585
At time: 85.19973754882812 and batch: 100, loss is 3.6612602615356447 and perplexity is 38.91034920088654
At time: 85.80846691131592 and batch: 150, loss is 3.6702496290206907 and perplexity is 39.26170549821077
At time: 86.41695713996887 and batch: 200, loss is 3.5599989557266234 and perplexity is 35.16316042513517
At time: 87.02535557746887 and batch: 250, loss is 3.7055514669418335 and perplexity is 40.672470656454976
At time: 87.65271139144897 and batch: 300, loss is 3.6672286319732668 and perplexity is 39.1432749810158
At time: 88.26248025894165 and batch: 350, loss is 3.667015132904053 and perplexity is 39.13491882028938
At time: 88.87232804298401 and batch: 400, loss is 3.59217089176178 and perplexity is 36.312821626671
At time: 89.48331809043884 and batch: 450, loss is 3.6282681798934937 and perplexity is 37.647561324614465
At time: 90.09320306777954 and batch: 500, loss is 3.5203543663024903 and perplexity is 33.796402648344475
At time: 90.70252680778503 and batch: 550, loss is 3.567260003089905 and perplexity is 35.41941099633876
At time: 91.3114492893219 and batch: 600, loss is 3.5812211179733278 and perplexity is 35.91737342948846
At time: 91.9202172756195 and batch: 650, loss is 3.4262751722335816 and perplexity is 30.76184650427727
At time: 92.53011846542358 and batch: 700, loss is 3.4240037965774537 and perplexity is 30.692054087394034
At time: 93.13940978050232 and batch: 750, loss is 3.5160188388824465 and perplexity is 33.65019459150367
At time: 93.74856996536255 and batch: 800, loss is 3.4872308921813966 and perplexity is 32.69528546678913
At time: 94.35687375068665 and batch: 850, loss is 3.539285464286804 and perplexity is 34.44230014281367
At time: 94.96446466445923 and batch: 900, loss is 3.493028259277344 and perplexity is 32.885382537777645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33337862197667 and perplexity of 76.20130754866435
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 96.49526739120483 and batch: 50, loss is 3.7530468034744264 and perplexity is 42.650832942875354
At time: 97.12129807472229 and batch: 100, loss is 3.6462765312194825 and perplexity is 38.331673211048724
At time: 97.73434853553772 and batch: 150, loss is 3.658088984489441 and perplexity is 38.78714915759256
At time: 98.35397624969482 and batch: 200, loss is 3.5431596517562864 and perplexity is 34.57599488246734
At time: 98.9772789478302 and batch: 250, loss is 3.6922434759140015 and perplexity is 40.13478745886584
At time: 99.591073513031 and batch: 300, loss is 3.646413893699646 and perplexity is 38.33693890639602
At time: 100.2042441368103 and batch: 350, loss is 3.6387598609924314 and perplexity is 38.04462683227144
At time: 100.81686186790466 and batch: 400, loss is 3.5631150579452515 and perplexity is 35.27290332359346
At time: 101.42977094650269 and batch: 450, loss is 3.594012670516968 and perplexity is 36.37976343716309
At time: 102.04390263557434 and batch: 500, loss is 3.480913853645325 and perplexity is 32.48939906935806
At time: 102.66543912887573 and batch: 550, loss is 3.525777292251587 and perplexity is 33.98017588120026
At time: 103.29582500457764 and batch: 600, loss is 3.543618860244751 and perplexity is 34.5918761189347
At time: 103.91028070449829 and batch: 650, loss is 3.3815061807632447 and perplexity is 29.415042135453547
At time: 104.52380132675171 and batch: 700, loss is 3.372217450141907 and perplexity is 29.143078785416087
At time: 105.13738512992859 and batch: 750, loss is 3.4597074365615845 and perplexity is 31.807669392146845
At time: 105.75077104568481 and batch: 800, loss is 3.421247968673706 and perplexity is 30.607588508073615
At time: 106.36424803733826 and batch: 850, loss is 3.4725600147247313 and perplexity is 32.21911837542561
At time: 106.97780346870422 and batch: 900, loss is 3.429163694381714 and perplexity is 30.850831234533636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318144915855094 and perplexity of 75.0492763545493
finished 9 epochs...
Completing Train Step...
At time: 108.52306151390076 and batch: 50, loss is 3.7265752840042112 and perplexity is 41.53661319727883
At time: 109.13777589797974 and batch: 100, loss is 3.611596088409424 and perplexity is 37.025101013799954
At time: 109.75320601463318 and batch: 150, loss is 3.6233338832855226 and perplexity is 37.462254645507144
At time: 110.36793041229248 and batch: 200, loss is 3.511140351295471 and perplexity is 33.486432315794495
At time: 110.98321008682251 and batch: 250, loss is 3.6601505041122437 and perplexity is 38.86719210339761
At time: 111.59815311431885 and batch: 300, loss is 3.616052861213684 and perplexity is 37.190481735480695
At time: 112.213134765625 and batch: 350, loss is 3.6101109409332275 and perplexity is 36.970154090715
At time: 112.82805705070496 and batch: 400, loss is 3.537392430305481 and perplexity is 34.37716137265359
At time: 113.44308471679688 and batch: 450, loss is 3.5708497762680054 and perplexity is 35.5467871368582
At time: 114.05787467956543 and batch: 500, loss is 3.4591451501846313 and perplexity is 31.78978940027402
At time: 114.67228055000305 and batch: 550, loss is 3.506581768989563 and perplexity is 33.33412906515039
At time: 115.28728175163269 and batch: 600, loss is 3.527017650604248 and perplexity is 34.022349626050755
At time: 115.90176367759705 and batch: 650, loss is 3.3678539419174194 and perplexity is 29.016189763427235
At time: 116.51666140556335 and batch: 700, loss is 3.360692253112793 and perplexity is 28.809127185932667
At time: 117.13113737106323 and batch: 750, loss is 3.452530264854431 and perplexity is 31.5801975659486
At time: 117.74629759788513 and batch: 800, loss is 3.416237530708313 and perplexity is 30.454614637655105
At time: 118.36153507232666 and batch: 850, loss is 3.471151237487793 and perplexity is 32.17376077174547
At time: 118.98900651931763 and batch: 900, loss is 3.4306164264678953 and perplexity is 30.895681796991163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31876571864298 and perplexity of 75.09588161938139
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 120.52713799476624 and batch: 50, loss is 3.7217364358901976 and perplexity is 41.33610932995065
At time: 121.13358426094055 and batch: 100, loss is 3.6117584037780763 and perplexity is 37.03111124448361
At time: 121.74215984344482 and batch: 150, loss is 3.623646240234375 and perplexity is 37.47395806879298
At time: 122.34898924827576 and batch: 200, loss is 3.5109895706176757 and perplexity is 33.481383589467725
At time: 122.9546856880188 and batch: 250, loss is 3.6574397897720337 and perplexity is 38.76197691698306
At time: 123.56083011627197 and batch: 300, loss is 3.614112305641174 and perplexity is 37.11838151876922
At time: 124.16817903518677 and batch: 350, loss is 3.6071196031570434 and perplexity is 36.85972911373578
At time: 124.77587556838989 and batch: 400, loss is 3.5330338430404664 and perplexity is 34.22765157732226
At time: 125.38345336914062 and batch: 450, loss is 3.5621275758743285 and perplexity is 35.23808915598418
At time: 125.9905059337616 and batch: 500, loss is 3.447958130836487 and perplexity is 31.436138250568305
At time: 126.60916352272034 and batch: 550, loss is 3.495400104522705 and perplexity is 32.96347414999166
At time: 127.23515057563782 and batch: 600, loss is 3.5190746068954466 and perplexity is 33.75317904793488
At time: 127.85240054130554 and batch: 650, loss is 3.355101547241211 and perplexity is 28.648513220484475
At time: 128.46238946914673 and batch: 700, loss is 3.3447199392318727 and perplexity is 28.352634094161175
At time: 129.07632851600647 and batch: 750, loss is 3.432963280677795 and perplexity is 30.96827460693654
At time: 129.69443917274475 and batch: 800, loss is 3.3966680812835692 and perplexity is 29.864428243013727
At time: 130.30868077278137 and batch: 850, loss is 3.4463394498825073 and perplexity is 31.385294333457033
At time: 130.91795825958252 and batch: 900, loss is 3.4116477823257445 and perplexity is 30.315155904445053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314092766748716 and perplexity of 74.74578081630264
finished 11 epochs...
Completing Train Step...
At time: 132.44934725761414 and batch: 50, loss is 3.710430588722229 and perplexity is 40.871401503168215
At time: 133.072163105011 and batch: 100, loss is 3.5985631561279297 and perplexity is 36.54568625560058
At time: 133.68318676948547 and batch: 150, loss is 3.61025484085083 and perplexity is 36.975474475634705
At time: 134.3069553375244 and batch: 200, loss is 3.4992164373397827 and perplexity is 33.08951409036091
At time: 134.91551566123962 and batch: 250, loss is 3.646513137817383 and perplexity is 38.340743810878585
At time: 135.5245063304901 and batch: 300, loss is 3.6037367153167725 and perplexity is 36.735247456776335
At time: 136.15554356575012 and batch: 350, loss is 3.596022062301636 and perplexity is 36.45293812863265
At time: 136.78596138954163 and batch: 400, loss is 3.524062461853027 and perplexity is 33.92195557589254
At time: 137.40295600891113 and batch: 450, loss is 3.5543852424621583 and perplexity is 34.96631755169914
At time: 138.02012658119202 and batch: 500, loss is 3.441105885505676 and perplexity is 31.22146645090178
At time: 138.64297485351562 and batch: 550, loss is 3.489202184677124 and perplexity is 32.75980100628266
At time: 139.25365662574768 and batch: 600, loss is 3.5142590475082396 and perplexity is 33.59102934383631
At time: 139.88129329681396 and batch: 650, loss is 3.3515540409088134 and perplexity is 28.547062493364223
At time: 140.51399302482605 and batch: 700, loss is 3.3420351934432984 and perplexity is 28.276616568707844
At time: 141.13269901275635 and batch: 750, loss is 3.4318400955200197 and perplexity is 30.93351102715918
At time: 141.74510145187378 and batch: 800, loss is 3.396875514984131 and perplexity is 29.870623774438123
At time: 142.35201215744019 and batch: 850, loss is 3.4483693981170656 and perplexity is 31.449069564589422
At time: 142.9594805240631 and batch: 900, loss is 3.4153637170791624 and perplexity is 30.428014603742813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313654233331549 and perplexity of 74.71300947981078
finished 12 epochs...
Completing Train Step...
At time: 144.48178482055664 and batch: 50, loss is 3.7048413276672365 and perplexity is 40.64359779073827
At time: 145.10233736038208 and batch: 100, loss is 3.592348017692566 and perplexity is 36.31925413866662
At time: 145.70959281921387 and batch: 150, loss is 3.6037605237960815 and perplexity is 36.736122077566975
At time: 146.31759881973267 and batch: 200, loss is 3.493032555580139 and perplexity is 32.88552382364207
At time: 146.9252588748932 and batch: 250, loss is 3.6400661277770996 and perplexity is 38.094355737174716
At time: 147.5329258441925 and batch: 300, loss is 3.597589988708496 and perplexity is 36.51013848418774
At time: 148.142409324646 and batch: 350, loss is 3.5899957752227785 and perplexity is 36.23392284590122
At time: 148.75298023223877 and batch: 400, loss is 3.5187000370025636 and perplexity is 33.74053849080829
At time: 149.3628318309784 and batch: 450, loss is 3.549495577812195 and perplexity is 34.79576130606617
At time: 149.9875419139862 and batch: 500, loss is 3.436622643470764 and perplexity is 31.081806359038996
At time: 150.59647059440613 and batch: 550, loss is 3.4852065563201906 and perplexity is 32.62916617433314
At time: 151.2050278186798 and batch: 600, loss is 3.5109443521499633 and perplexity is 33.47986964683426
At time: 151.8146834373474 and batch: 650, loss is 3.3489370727539063 and perplexity is 28.472453407284274
At time: 152.4267122745514 and batch: 700, loss is 3.339856595993042 and perplexity is 28.215080259711407
At time: 153.0498378276825 and batch: 750, loss is 3.4305580282211303 and perplexity is 30.893877596023206
At time: 153.65787744522095 and batch: 800, loss is 3.396262550354004 and perplexity is 29.852319749017752
At time: 154.26646304130554 and batch: 850, loss is 3.4485320520401 and perplexity is 31.454185295165427
At time: 154.87483024597168 and batch: 900, loss is 3.4161552381515503 and perplexity is 30.452108552668832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313925129093536 and perplexity of 74.73325165908064
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 156.40958261489868 and batch: 50, loss is 3.7033959293365477 and perplexity is 40.58489403771187
At time: 157.01751136779785 and batch: 100, loss is 3.5930773305892942 and perplexity is 36.34575190051472
At time: 157.64112401008606 and batch: 150, loss is 3.6051142978668214 and perplexity is 36.78588816551431
At time: 158.25022959709167 and batch: 200, loss is 3.49345175743103 and perplexity is 32.89931238598997
At time: 158.8588263988495 and batch: 250, loss is 3.63864905834198 and perplexity is 38.04041162031563
At time: 159.4668345451355 and batch: 300, loss is 3.5975960445404054 and perplexity is 36.51035958411886
At time: 160.0746512413025 and batch: 350, loss is 3.5892526054382325 and perplexity is 36.207004892810176
At time: 160.6807506084442 and batch: 400, loss is 3.5177713441848755 and perplexity is 33.70921844065081
At time: 161.28860569000244 and batch: 450, loss is 3.5469061803817747 and perplexity is 34.70577780295472
At time: 161.9073030948639 and batch: 500, loss is 3.4323037385940554 and perplexity is 30.947856460625985
At time: 162.51939177513123 and batch: 550, loss is 3.480685381889343 and perplexity is 32.48197700719996
At time: 163.1298267841339 and batch: 600, loss is 3.5076520347595217 and perplexity is 33.36982454087578
At time: 163.74082446098328 and batch: 650, loss is 3.3436937618255613 and perplexity is 28.32355418476919
At time: 164.3513572216034 and batch: 700, loss is 3.3341654777526855 and perplexity is 28.054960962890398
At time: 164.9600374698639 and batch: 750, loss is 3.423781547546387 and perplexity is 30.685233566066863
At time: 165.58104991912842 and batch: 800, loss is 3.390527534484863 and perplexity is 29.681606211851722
At time: 166.18896055221558 and batch: 850, loss is 3.4381562852859497 and perplexity is 31.12951128874983
At time: 166.80031991004944 and batch: 900, loss is 3.4061256980895998 and perplexity is 30.148214416605803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313432667353382 and perplexity of 74.69645745253433
finished 14 epochs...
Completing Train Step...
At time: 168.34008359909058 and batch: 50, loss is 3.7003986501693724 and perplexity is 40.46343189921071
At time: 168.95086026191711 and batch: 100, loss is 3.589860854148865 and perplexity is 36.22903445589957
At time: 169.56109619140625 and batch: 150, loss is 3.601739163398743 and perplexity is 36.66194013472663
At time: 170.1710569858551 and batch: 200, loss is 3.4906621026992797 and perplexity is 32.807662558644545
At time: 170.78154063224792 and batch: 250, loss is 3.6363401556015016 and perplexity is 37.952681329005976
At time: 171.39152693748474 and batch: 300, loss is 3.595047082901001 and perplexity is 36.41741458504025
At time: 172.00141596794128 and batch: 350, loss is 3.5861344385147094 and perplexity is 36.094281244663144
At time: 172.6114535331726 and batch: 400, loss is 3.515566935539246 and perplexity is 33.6349913915156
At time: 173.2218017578125 and batch: 450, loss is 3.5450129604339597 and perplexity is 34.640134290524394
At time: 173.83211135864258 and batch: 500, loss is 3.4306765937805177 and perplexity is 30.89754076306046
At time: 174.44285202026367 and batch: 550, loss is 3.478989543914795 and perplexity is 32.426939517625215
At time: 175.0528450012207 and batch: 600, loss is 3.506571340560913 and perplexity is 33.3337814443764
At time: 175.66271042823792 and batch: 650, loss is 3.343050093650818 and perplexity is 28.305329080433893
At time: 176.27253794670105 and batch: 700, loss is 3.3336937856674194 and perplexity is 28.041730780383244
At time: 176.88277912139893 and batch: 750, loss is 3.4235752916336057 and perplexity is 30.678905207861874
At time: 177.49230670928955 and batch: 800, loss is 3.3906522607803344 and perplexity is 29.685308519520923
At time: 178.1024353504181 and batch: 850, loss is 3.4391015625 and perplexity is 31.158951218710918
At time: 178.71255254745483 and batch: 900, loss is 3.4079348278045654 and perplexity is 30.202805813729825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313224478943707 and perplexity of 74.68090813449899
finished 15 epochs...
Completing Train Step...
At time: 180.24158120155334 and batch: 50, loss is 3.698575382232666 and perplexity is 40.389723436777004
At time: 180.88368701934814 and batch: 100, loss is 3.587816972732544 and perplexity is 36.15506222662756
At time: 181.50015544891357 and batch: 150, loss is 3.599701280593872 and perplexity is 36.58730347354738
At time: 182.26873993873596 and batch: 200, loss is 3.4887485551834105 and perplexity is 32.744943564479705
At time: 182.88389992713928 and batch: 250, loss is 3.6345346355438233 and perplexity is 37.88421882543391
At time: 183.49843454360962 and batch: 300, loss is 3.5931842279434205 and perplexity is 36.349637372896275
At time: 184.11288189888 and batch: 350, loss is 3.584185953140259 and perplexity is 36.02402053876482
At time: 184.72698307037354 and batch: 400, loss is 3.5140117454528808 and perplexity is 33.582723240338524
At time: 185.34119749069214 and batch: 450, loss is 3.543632493019104 and perplexity is 34.59234770539078
At time: 185.95550441741943 and batch: 500, loss is 3.4294866847991945 and perplexity is 30.86079736678967
At time: 186.57023859024048 and batch: 550, loss is 3.477847332954407 and perplexity is 32.38992225668146
At time: 187.18608832359314 and batch: 600, loss is 3.5057530069351195 and perplexity is 33.30651444841711
At time: 187.80106139183044 and batch: 650, loss is 3.3425375843048095 and perplexity is 28.290826051524707
At time: 188.41793847084045 and batch: 700, loss is 3.3333343124389647 and perplexity is 28.03165234046136
At time: 189.0433087348938 and batch: 750, loss is 3.423440418243408 and perplexity is 30.67476771893379
At time: 189.65927076339722 and batch: 800, loss is 3.3907561922073364 and perplexity is 29.68839391632842
At time: 190.27376508712769 and batch: 850, loss is 3.4395721626281737 and perplexity is 31.17361807598998
At time: 190.88822436332703 and batch: 900, loss is 3.40875262260437 and perplexity is 30.227515613659612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313171804767766 and perplexity of 74.67697448280632
finished 16 epochs...
Completing Train Step...
At time: 192.44459056854248 and batch: 50, loss is 3.6970407915115358 and perplexity is 40.327789275910625
At time: 193.0603063106537 and batch: 100, loss is 3.58614764213562 and perplexity is 36.09475782301602
At time: 193.67607951164246 and batch: 150, loss is 3.5980172204971312 and perplexity is 36.52574010847327
At time: 194.29209089279175 and batch: 200, loss is 3.487157940864563 and perplexity is 32.6929003896584
At time: 194.90966844558716 and batch: 250, loss is 3.632951602935791 and perplexity is 37.82429431544428
At time: 195.5296928882599 and batch: 300, loss is 3.5916208839416504 and perplexity is 36.292854782268996
At time: 196.15939044952393 and batch: 350, loss is 3.58260941028595 and perplexity is 35.96727187170772
At time: 196.79090905189514 and batch: 400, loss is 3.5126850414276123 and perplexity is 33.53819844837729
At time: 197.4066617488861 and batch: 450, loss is 3.542424602508545 and perplexity is 34.55058916181088
At time: 198.02240371704102 and batch: 500, loss is 3.4284288597106936 and perplexity is 30.8281693015195
At time: 198.63788890838623 and batch: 550, loss is 3.476874132156372 and perplexity is 32.35841569208659
At time: 199.2536962032318 and batch: 600, loss is 3.505009799003601 and perplexity is 33.28176997898976
At time: 199.8699107170105 and batch: 650, loss is 3.3420108222961424 and perplexity is 28.275927443521798
At time: 200.4854598045349 and batch: 700, loss is 3.332935938835144 and perplexity is 28.02048749413513
At time: 201.10146307945251 and batch: 750, loss is 3.423236484527588 and perplexity is 30.668512737393442
At time: 201.71729493141174 and batch: 800, loss is 3.3907385635375977 and perplexity is 29.687870554050104
At time: 202.33280038833618 and batch: 850, loss is 3.439781746864319 and perplexity is 31.18015225962934
At time: 202.94898438453674 and batch: 900, loss is 3.4091572856903074 and perplexity is 30.23975004865321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313200650149828 and perplexity of 74.67912859973455
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 204.49779987335205 and batch: 50, loss is 3.6964981269836428 and perplexity is 40.305910752068556
At time: 205.1056935787201 and batch: 100, loss is 3.58621120929718 and perplexity is 36.097052337245124
At time: 205.71505331993103 and batch: 150, loss is 3.5984815073013308 and perplexity is 36.54270246501371
At time: 206.3243374824524 and batch: 200, loss is 3.487527222633362 and perplexity is 32.70497551116535
At time: 206.93358516693115 and batch: 250, loss is 3.6329103755950927 and perplexity is 37.82273495252028
At time: 207.54258823394775 and batch: 300, loss is 3.5918500089645384 and perplexity is 36.30117133618058
At time: 208.1562967300415 and batch: 350, loss is 3.581748843193054 and perplexity is 35.93633293553827
At time: 208.80285906791687 and batch: 400, loss is 3.512696371078491 and perplexity is 33.53857842660932
At time: 209.42093181610107 and batch: 450, loss is 3.542129101753235 and perplexity is 34.54038094495934
At time: 210.03859448432922 and batch: 500, loss is 3.426721019744873 and perplexity is 30.775564654858336
At time: 210.6615490913391 and batch: 550, loss is 3.4750394725799563 and perplexity is 32.299103440391704
At time: 211.2760739326477 and batch: 600, loss is 3.5032839584350586 and perplexity is 33.22438048696857
At time: 211.8847782611847 and batch: 650, loss is 3.340136532783508 and perplexity is 28.22297980435705
At time: 212.49499344825745 and batch: 700, loss is 3.330867762565613 and perplexity is 27.962596072306656
At time: 213.10362267494202 and batch: 750, loss is 3.421203713417053 and perplexity is 30.606233991361137
At time: 213.71253728866577 and batch: 800, loss is 3.388755645751953 and perplexity is 29.629060274754938
At time: 214.3198597431183 and batch: 850, loss is 3.4366323232650755 and perplexity is 31.082107225987542
At time: 214.9407024383545 and batch: 900, loss is 3.405443935394287 and perplexity is 30.127667493544944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312566469793451 and perplexity of 74.6317835775735
finished 18 epochs...
Completing Train Step...
At time: 216.5219440460205 and batch: 50, loss is 3.695643091201782 and perplexity is 40.271462485504046
At time: 217.1356098651886 and batch: 100, loss is 3.5854223442077635 and perplexity is 36.068587861617125
At time: 217.74979209899902 and batch: 150, loss is 3.5977465295791626 and perplexity is 36.515854260418834
At time: 218.36555743217468 and batch: 200, loss is 3.4867852830886843 and perplexity is 32.68071939592414
At time: 218.98251342773438 and batch: 250, loss is 3.6322921991348265 and perplexity is 37.79936105345118
At time: 219.5984115600586 and batch: 300, loss is 3.5910844469070433 and perplexity is 36.27339117183965
At time: 220.21483206748962 and batch: 350, loss is 3.5812654161453246 and perplexity is 35.91896453871566
At time: 220.8304409980774 and batch: 400, loss is 3.5120880222320556 and perplexity is 33.518181475972156
At time: 221.44696640968323 and batch: 450, loss is 3.5416039848327636 and perplexity is 34.52224796786884
At time: 222.06603479385376 and batch: 500, loss is 3.426438641548157 and perplexity is 30.766875533275158
At time: 222.68254041671753 and batch: 550, loss is 3.474777970314026 and perplexity is 32.29065825592009
At time: 223.29752469062805 and batch: 600, loss is 3.503173394203186 and perplexity is 33.22070726192798
At time: 223.92330598831177 and batch: 650, loss is 3.340056691169739 and perplexity is 28.220726526057977
At time: 224.52901101112366 and batch: 700, loss is 3.330860619544983 and perplexity is 27.962396335619403
At time: 225.13574838638306 and batch: 750, loss is 3.421192650794983 and perplexity is 30.605895408034304
At time: 225.74184393882751 and batch: 800, loss is 3.3889388704299925 and perplexity is 29.63448954715755
At time: 226.3496537208557 and batch: 850, loss is 3.4368108558654784 and perplexity is 31.087656890799906
At time: 226.95975065231323 and batch: 900, loss is 3.405806751251221 and perplexity is 30.13860027221684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312292229639341 and perplexity of 74.61131935192813
finished 19 epochs...
Completing Train Step...
At time: 228.49739241600037 and batch: 50, loss is 3.6950395250320436 and perplexity is 40.24716332695459
At time: 229.11802458763123 and batch: 100, loss is 3.5848243188858033 and perplexity is 36.04702438114486
At time: 229.72607421875 and batch: 150, loss is 3.597175245285034 and perplexity is 36.4949992840206
At time: 230.33462643623352 and batch: 200, loss is 3.4862388372421265 and perplexity is 32.66286603094055
At time: 230.94270420074463 and batch: 250, loss is 3.6318002843856814 and perplexity is 37.78077156283789
At time: 231.55098867416382 and batch: 300, loss is 3.5905267429351806 and perplexity is 36.25316699758554
At time: 232.15969491004944 and batch: 350, loss is 3.5808164978027346 and perplexity is 35.902843475478626
At time: 232.76903533935547 and batch: 400, loss is 3.511638560295105 and perplexity is 33.50311971430075
At time: 233.37800359725952 and batch: 450, loss is 3.5412200450897218 and perplexity is 34.50899604898679
At time: 233.98766946792603 and batch: 500, loss is 3.4261785793304442 and perplexity is 30.758875271719855
At time: 234.59617710113525 and batch: 550, loss is 3.4745417928695677 and perplexity is 32.28303283128543
At time: 235.20371055603027 and batch: 600, loss is 3.503043737411499 and perplexity is 33.21640025082947
At time: 235.81146430969238 and batch: 650, loss is 3.3399672985076903 and perplexity is 28.218203912942094
At time: 236.41896414756775 and batch: 700, loss is 3.330831432342529 and perplexity is 27.961580203406832
At time: 237.0281159877777 and batch: 750, loss is 3.421185598373413 and perplexity is 30.60567956311848
At time: 237.63522839546204 and batch: 800, loss is 3.389033808708191 and perplexity is 29.637303128126604
At time: 238.2432451248169 and batch: 850, loss is 3.436947112083435 and perplexity is 31.091893065949023
At time: 238.8517439365387 and batch: 900, loss is 3.406068482398987 and perplexity is 30.1464895150434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312164724689641 and perplexity of 74.60180664587857
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fbebb5c2b70>
ELAPSED
1480.7910509109497


RESULTS SO FAR:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}, {'best_accuracy': -74.22297391748778, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}}, {'best_accuracy': -74.82791282734208, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.5760335999079107, 'rnn_dropout': 0.08348482176534788}}, {'best_accuracy': -75.26047084093204, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.1380715185398046, 'rnn_dropout': 0.8184490040160984}}, {'best_accuracy': -74.60180664587857, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.38464634051501334, 'rnn_dropout': 0.5618569990909659}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -75.0845493273798, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.24940355222571853, 'rnn_dropout': 0.6506323726013177}}, {'best_accuracy': -74.64613683968946, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.9698393142108286, 'rnn_dropout': 0.10622945119284921}}, {'best_accuracy': -74.22297391748778, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.37776037121626604, 'rnn_dropout': 0.5663209006130457}}, {'best_accuracy': -74.82791282734208, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.5760335999079107, 'rnn_dropout': 0.08348482176534788}}, {'best_accuracy': -75.26047084093204, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.1380715185398046, 'rnn_dropout': 0.8184490040160984}}, {'best_accuracy': -74.60180664587857, 'params': {'batch_size': 32, 'data': 'ptb', 'tune_wordvecs': 'FALSE', 'wordvec_dim': 300, 'seq_len': 35, 'wordvec_source': 'gigavec', 'num_layers': 1, 'tie_weights': 'FALSE', 'dropout': 0.38464634051501334, 'rnn_dropout': 0.5618569990909659}}]
