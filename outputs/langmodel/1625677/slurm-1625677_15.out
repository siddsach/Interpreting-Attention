FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.542081356048584 and batch: 50, loss is 6.9611304759979244 and perplexity is 1054.8253382023524
At time: 2.6587929725646973 and batch: 100, loss is 6.0495820808410645 and perplexity is 423.9358221158279
At time: 3.7644691467285156 and batch: 150, loss is 5.780053939819336 and perplexity is 323.77665440047997
At time: 4.871083974838257 and batch: 200, loss is 5.548714513778687 and perplexity is 256.90709302030336
At time: 5.979634523391724 and batch: 250, loss is 5.5601764678955075 and perplexity is 259.86869075788036
At time: 7.087320327758789 and batch: 300, loss is 5.448492231369019 and perplexity is 232.40748488585942
At time: 8.193705558776855 and batch: 350, loss is 5.390275897979737 and perplexity is 219.26387166879508
At time: 9.303080797195435 and batch: 400, loss is 5.2222138786315915 and perplexity is 185.34405944881019
At time: 10.41622018814087 and batch: 450, loss is 5.21490481376648 and perplexity is 183.99430642106998
At time: 11.52008056640625 and batch: 500, loss is 5.1393836402893065 and perplexity is 170.61057842048808
At time: 12.627504825592041 and batch: 550, loss is 5.18720609664917 and perplexity is 178.96783494497092
At time: 13.736948013305664 and batch: 600, loss is 5.098846797943115 and perplexity is 163.83286592143267
At time: 14.843771934509277 and batch: 650, loss is 4.975890817642212 and perplexity is 144.87782740552493
At time: 15.949858903884888 and batch: 700, loss is 5.061512470245361 and perplexity is 157.82904763053816
At time: 17.05780005455017 and batch: 750, loss is 5.043983564376831 and perplexity is 155.0865835298515
At time: 18.16650891304016 and batch: 800, loss is 4.993101043701172 and perplexity is 147.39278700212017
At time: 19.275291681289673 and batch: 850, loss is 5.037089052200318 and perplexity is 154.02101470084315
At time: 20.38143491744995 and batch: 900, loss is 4.945336437225341 and perplexity is 140.51811842558786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.932754307577055 and perplexity of 138.76117744957676
finished 1 epochs...
Completing Train Step...
At time: 22.631913900375366 and batch: 50, loss is 4.896584644317627 and perplexity is 133.8319146538414
At time: 23.53195023536682 and batch: 100, loss is 4.762427158355713 and perplexity is 117.02963090774459
At time: 24.418950080871582 and batch: 150, loss is 4.750513973236084 and perplexity is 115.64370702542533
At time: 25.306224822998047 and batch: 200, loss is 4.62811448097229 and perplexity is 102.32095400560561
At time: 26.194759130477905 and batch: 250, loss is 4.737691984176636 and perplexity is 114.17039028948776
At time: 27.0837504863739 and batch: 300, loss is 4.694306507110595 and perplexity is 109.32296764654535
At time: 27.9724862575531 and batch: 350, loss is 4.671255140304566 and perplexity is 106.83174714915211
At time: 28.861133575439453 and batch: 400, loss is 4.553487176895142 and perplexity is 94.96298431623372
At time: 29.75421643257141 and batch: 450, loss is 4.567294883728027 and perplexity is 96.28329965043119
At time: 30.64552330970764 and batch: 500, loss is 4.467366008758545 and perplexity is 87.1269289407863
At time: 31.537529706954956 and batch: 550, loss is 4.5391809844970705 and perplexity is 93.61409731637428
At time: 32.430742025375366 and batch: 600, loss is 4.504442529678345 and perplexity is 90.41792468696728
At time: 33.32397246360779 and batch: 650, loss is 4.3590811824798585 and perplexity is 78.1852634201249
At time: 34.21669936180115 and batch: 700, loss is 4.3954439544677735 and perplexity is 81.08061887659453
At time: 35.107717514038086 and batch: 750, loss is 4.461652355194092 and perplexity is 86.63053531372361
At time: 36.02253246307373 and batch: 800, loss is 4.396924686431885 and perplexity is 81.20076647187477
At time: 36.91709280014038 and batch: 850, loss is 4.4649143981933594 and perplexity is 86.91358926104391
At time: 37.8082377910614 and batch: 900, loss is 4.402534255981445 and perplexity is 81.6575477943706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.547681834599743 and perplexity of 94.41328881597293
finished 2 epochs...
Completing Train Step...
At time: 39.917258501052856 and batch: 50, loss is 4.454939174652099 and perplexity is 86.05091660901296
At time: 40.871403217315674 and batch: 100, loss is 4.304852995872498 and perplexity is 74.05832777274271
At time: 41.76462936401367 and batch: 150, loss is 4.307347097396851 and perplexity is 74.24326729403406
At time: 42.65842008590698 and batch: 200, loss is 4.198764519691467 and perplexity is 66.60399226651657
At time: 43.551854372024536 and batch: 250, loss is 4.3402242088317875 and perplexity is 76.7247397742527
At time: 44.443633794784546 and batch: 300, loss is 4.309305500984192 and perplexity is 74.38880804228381
At time: 45.337000131607056 and batch: 350, loss is 4.303515605926513 and perplexity is 73.9593491110494
At time: 46.23016023635864 and batch: 400, loss is 4.21079047203064 and perplexity is 67.40980432019636
At time: 47.121464252471924 and batch: 450, loss is 4.232958474159241 and perplexity is 68.92083135248045
At time: 48.01606774330139 and batch: 500, loss is 4.111819243431091 and perplexity is 61.057695402445866
At time: 48.90874123573303 and batch: 550, loss is 4.195045480728149 and perplexity is 66.35674946194968
At time: 49.80080223083496 and batch: 600, loss is 4.194435114860535 and perplexity is 66.31625992496497
At time: 50.695343255996704 and batch: 650, loss is 4.036225771903991 and perplexity is 56.61227145702499
At time: 51.58758759498596 and batch: 700, loss is 4.054823231697083 and perplexity is 57.67496698898873
At time: 52.4797682762146 and batch: 750, loss is 4.156789288520813 and perplexity is 63.86613731971184
At time: 53.37249755859375 and batch: 800, loss is 4.0996017742156985 and perplexity is 60.3162633228688
At time: 54.26432490348816 and batch: 850, loss is 4.1752852439880375 and perplexity is 65.05839453046987
At time: 55.15744352340698 and batch: 900, loss is 4.126108689308166 and perplexity is 61.93643946676704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.407234505431293 and perplexity of 82.0422620566972
finished 3 epochs...
Completing Train Step...
At time: 57.306490421295166 and batch: 50, loss is 4.198082509040833 and perplexity is 66.55858312093662
At time: 58.20099329948425 and batch: 100, loss is 4.050242004394531 and perplexity is 57.41134916319506
At time: 59.096463441848755 and batch: 150, loss is 4.05857328414917 and perplexity is 57.891657186231726
At time: 59.99107885360718 and batch: 200, loss is 3.9492072582244875 and perplexity is 51.89421181472204
At time: 60.88483715057373 and batch: 250, loss is 4.098389558792114 and perplexity is 60.243191316630394
At time: 61.779765129089355 and batch: 300, loss is 4.073514895439148 and perplexity is 58.76314633751599
At time: 62.675376653671265 and batch: 350, loss is 4.067800688743591 and perplexity is 58.42831912029419
At time: 63.57094979286194 and batch: 400, loss is 3.9895627784729 and perplexity is 54.03126056793736
At time: 64.46357274055481 and batch: 450, loss is 4.011542277336121 and perplexity is 55.231987952530034
At time: 65.35627722740173 and batch: 500, loss is 3.887416124343872 and perplexity is 48.78467000883649
At time: 66.24918508529663 and batch: 550, loss is 3.9739421701431272 and perplexity is 53.193817125235114
At time: 67.14346837997437 and batch: 600, loss is 3.984203214645386 and perplexity is 53.742451215656445
At time: 68.03873229026794 and batch: 650, loss is 3.8205641078948975 and perplexity is 45.6299412722012
At time: 68.93205142021179 and batch: 700, loss is 3.8346884727478026 and perplexity is 46.279008249771714
At time: 69.82611179351807 and batch: 750, loss is 3.950775966644287 and perplexity is 51.9756825869706
At time: 70.72345161437988 and batch: 800, loss is 3.8983834266662596 and perplexity is 49.32265094085536
At time: 71.61955142021179 and batch: 850, loss is 3.9693467664718627 and perplexity is 52.94993086981961
At time: 72.51726984977722 and batch: 900, loss is 3.928785872459412 and perplexity is 50.84520761155719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362347746548587 and perplexity of 78.44107818250136
finished 4 epochs...
Completing Train Step...
At time: 74.65764880180359 and batch: 50, loss is 4.006404786109925 and perplexity is 54.948961743861986
At time: 75.55011892318726 and batch: 100, loss is 3.8610670614242553 and perplexity is 47.516026846403236
At time: 76.44475531578064 and batch: 150, loss is 3.871898398399353 and perplexity is 48.033486274687554
At time: 77.33850502967834 and batch: 200, loss is 3.766145420074463 and perplexity is 43.21317476096777
At time: 78.2315194606781 and batch: 250, loss is 3.9146659851074217 and perplexity is 50.132323771456086
At time: 79.12346339225769 and batch: 300, loss is 3.8962058448791503 and perplexity is 49.21536369025117
At time: 80.03447937965393 and batch: 350, loss is 3.8884054231643677 and perplexity is 48.83295650628377
At time: 80.92734932899475 and batch: 400, loss is 3.816973547935486 and perplexity is 45.46639801375821
At time: 81.82049489021301 and batch: 450, loss is 3.8402394962310793 and perplexity is 46.53661844998767
At time: 82.71218514442444 and batch: 500, loss is 3.717462921142578 and perplexity is 41.15983577920178
At time: 83.60461235046387 and batch: 550, loss is 3.801462926864624 and perplexity is 44.766626914042874
At time: 84.49807381629944 and batch: 600, loss is 3.8175513792037963 and perplexity is 45.49267751201471
At time: 85.4006941318512 and batch: 650, loss is 3.653818039894104 and perplexity is 38.62184464700855
At time: 86.29861640930176 and batch: 700, loss is 3.6664138555526735 and perplexity is 39.11139495284775
At time: 87.19123315811157 and batch: 750, loss is 3.7835242319107056 and perplexity is 43.97073205022372
At time: 88.08420825004578 and batch: 800, loss is 3.7346249294281004 and perplexity is 41.87231754339114
At time: 88.97854614257812 and batch: 850, loss is 3.803882803916931 and perplexity is 44.875087825329345
At time: 89.87181520462036 and batch: 900, loss is 3.7692981338500977 and perplexity is 43.34962851912244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361729034005779 and perplexity of 78.39256071428925
finished 5 epochs...
Completing Train Step...
At time: 91.9969973564148 and batch: 50, loss is 3.8524311208724975 and perplexity is 47.10744803376638
At time: 92.9019525051117 and batch: 100, loss is 3.7102902698516846 and perplexity is 40.8656668766193
At time: 93.7938072681427 and batch: 150, loss is 3.721019649505615 and perplexity is 41.30649078594399
At time: 94.68393921852112 and batch: 200, loss is 3.612918286323547 and perplexity is 37.07408790317548
At time: 95.5850236415863 and batch: 250, loss is 3.7636216354370116 and perplexity is 43.10425152158508
At time: 96.47979807853699 and batch: 300, loss is 3.7454077196121216 and perplexity is 42.32626094795318
At time: 97.37277460098267 and batch: 350, loss is 3.741087670326233 and perplexity is 42.143803810248095
At time: 98.26538848876953 and batch: 400, loss is 3.6723668813705443 and perplexity is 39.34492049892797
At time: 99.15818977355957 and batch: 450, loss is 3.6984147739410402 and perplexity is 40.383237033195634
At time: 100.04938244819641 and batch: 500, loss is 3.5767181873321534 and perplexity is 35.756003579409885
At time: 100.938640832901 and batch: 550, loss is 3.658366141319275 and perplexity is 38.797900770764066
At time: 101.84296989440918 and batch: 600, loss is 3.677081274986267 and perplexity is 39.53084585924808
At time: 102.7361798286438 and batch: 650, loss is 3.516405062675476 and perplexity is 33.663193607398014
At time: 103.62792181968689 and batch: 700, loss is 3.529129452705383 and perplexity is 34.09427401378302
At time: 104.52047801017761 and batch: 750, loss is 3.6460871791839597 and perplexity is 38.32441571783346
At time: 105.41158866882324 and batch: 800, loss is 3.5986820697784423 and perplexity is 36.5500322949603
At time: 106.30393743515015 and batch: 850, loss is 3.6644442892074585 and perplexity is 39.03443827615191
At time: 107.19639229774475 and batch: 900, loss is 3.632622699737549 and perplexity is 37.81185582971367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374299349850172 and perplexity of 79.38419951115446
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.36055946350098 and batch: 50, loss is 3.7431699514389036 and perplexity is 42.231650485933905
At time: 110.25937700271606 and batch: 100, loss is 3.6100608205795286 and perplexity is 36.96830117995033
At time: 111.15251445770264 and batch: 150, loss is 3.6308742904663087 and perplexity is 37.745802990942664
At time: 112.04473304748535 and batch: 200, loss is 3.5017603158950807 and perplexity is 33.17379695289213
At time: 112.93707537651062 and batch: 250, loss is 3.650362195968628 and perplexity is 38.48860394194133
At time: 113.8302435874939 and batch: 300, loss is 3.614166913032532 and perplexity is 37.12040851209928
At time: 114.722482919693 and batch: 350, loss is 3.5973195600509644 and perplexity is 36.50026643135433
At time: 115.61303162574768 and batch: 400, loss is 3.519855899810791 and perplexity is 33.77956047206329
At time: 116.50489807128906 and batch: 450, loss is 3.5255123472213743 and perplexity is 33.97117419500345
At time: 117.3982446193695 and batch: 500, loss is 3.3949423694610594 and perplexity is 29.812935289900427
At time: 118.2879889011383 and batch: 550, loss is 3.451195864677429 and perplexity is 31.538085048451116
At time: 119.17680835723877 and batch: 600, loss is 3.471427812576294 and perplexity is 32.18266046313806
At time: 120.06378269195557 and batch: 650, loss is 3.2880464792251587 and perplexity is 26.79047675020493
At time: 120.95305919647217 and batch: 700, loss is 3.28132185459137 and perplexity is 26.61092523518809
At time: 121.84015440940857 and batch: 750, loss is 3.3845754432678223 and perplexity is 29.505463313532797
At time: 122.72760772705078 and batch: 800, loss is 3.314899411201477 and perplexity is 27.519625679257647
At time: 123.61571645736694 and batch: 850, loss is 3.3548853492736814 and perplexity is 28.64232013964333
At time: 124.51857566833496 and batch: 900, loss is 3.314204316139221 and perplexity is 27.500503569944666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.35936036828446 and perplexity of 78.20709468314841
finished 7 epochs...
Completing Train Step...
At time: 126.63474774360657 and batch: 50, loss is 3.625644340515137 and perplexity is 37.54890965038319
At time: 127.52881598472595 and batch: 100, loss is 3.484980573654175 and perplexity is 32.62179338146427
At time: 128.4226152896881 and batch: 150, loss is 3.502881097793579 and perplexity is 33.210998387474405
At time: 129.31528759002686 and batch: 200, loss is 3.3778644895553587 and perplexity is 29.30811644846629
At time: 130.2105062007904 and batch: 250, loss is 3.5259841203689577 and perplexity is 33.98720466385558
At time: 131.10776948928833 and batch: 300, loss is 3.4943316078186033 and perplexity is 32.92827159676188
At time: 132.00228691101074 and batch: 350, loss is 3.4827224016189575 and perplexity is 32.54821087216252
At time: 132.89681577682495 and batch: 400, loss is 3.4103322792053223 and perplexity is 30.275302441681347
At time: 133.7894721031189 and batch: 450, loss is 3.421313157081604 and perplexity is 30.609583833073387
At time: 134.6834545135498 and batch: 500, loss is 3.2950830364227297 and perplexity is 26.979654270866526
At time: 135.5761525630951 and batch: 550, loss is 3.356583247184753 and perplexity is 28.690993184414083
At time: 136.46932721138 and batch: 600, loss is 3.3836947536468505 and perplexity is 29.479489597300553
At time: 137.36245107650757 and batch: 650, loss is 3.2062291431427004 and perplexity is 24.68582378754122
At time: 138.2563865184784 and batch: 700, loss is 3.2047827529907225 and perplexity is 24.650144264597028
At time: 139.14873838424683 and batch: 750, loss is 3.316446833610535 and perplexity is 27.56224312981761
At time: 140.04228854179382 and batch: 800, loss is 3.2529074239730833 and perplexity is 25.865432479803484
At time: 140.95069813728333 and batch: 850, loss is 3.3011457395553587 and perplexity is 27.14372074595765
At time: 141.84761714935303 and batch: 900, loss is 3.268584885597229 and perplexity is 26.274132120452595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382173303055437 and perplexity of 80.01173433103104
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.96635794639587 and batch: 50, loss is 3.570791549682617 and perplexity is 35.54471742907829
At time: 144.8722472190857 and batch: 100, loss is 3.444156012535095 and perplexity is 31.31684126833456
At time: 145.7660608291626 and batch: 150, loss is 3.4678223371505736 and perplexity is 32.06683559902617
At time: 146.66658687591553 and batch: 200, loss is 3.342692246437073 and perplexity is 28.29520190938636
At time: 147.55925512313843 and batch: 250, loss is 3.4900264024734495 and perplexity is 32.786813347773204
At time: 148.4533815383911 and batch: 300, loss is 3.4538736963272094 and perplexity is 31.622651908139815
At time: 149.34518361091614 and batch: 350, loss is 3.437164406776428 and perplexity is 31.09864990339324
At time: 150.2375099658966 and batch: 400, loss is 3.3622265100479125 and perplexity is 28.853361714001014
At time: 151.13018774986267 and batch: 450, loss is 3.3651872062683106 and perplexity is 28.938914338144706
At time: 152.022846698761 and batch: 500, loss is 3.234936351776123 and perplexity is 25.404754759870208
At time: 152.9161913394928 and batch: 550, loss is 3.286229157447815 and perplexity is 26.741834046491263
At time: 153.80868244171143 and batch: 600, loss is 3.3120131397247317 and perplexity is 27.440311085395138
At time: 154.70188641548157 and batch: 650, loss is 3.1301692628860476 and perplexity is 22.87785158592094
At time: 155.59555292129517 and batch: 700, loss is 3.1201162099838258 and perplexity is 22.64901153151033
At time: 156.48870611190796 and batch: 750, loss is 3.2248487663269043 and perplexity is 25.149770376044337
At time: 157.38115429878235 and batch: 800, loss is 3.1516040754318237 and perplexity is 23.373527427395416
At time: 158.2743787765503 and batch: 850, loss is 3.194616184234619 and perplexity is 24.40080648055309
At time: 159.1664628982544 and batch: 900, loss is 3.1606940984725953 and perplexity is 23.586961922849238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378412952161815 and perplexity of 79.71142711845462
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.27877521514893 and batch: 50, loss is 3.5470561170578003 and perplexity is 34.71098186204729
At time: 162.17783164978027 and batch: 100, loss is 3.4197898149490356 and perplexity is 30.562990462192825
At time: 163.0688509941101 and batch: 150, loss is 3.442795920372009 and perplexity is 31.274276430565575
At time: 163.95931959152222 and batch: 200, loss is 3.3197042417526244 and perplexity is 27.652170991360617
At time: 164.84866309165955 and batch: 250, loss is 3.4672751998901368 and perplexity is 32.04929543732122
At time: 165.73986959457397 and batch: 300, loss is 3.4309320497512816 and perplexity is 30.905434732568196
At time: 166.6313922405243 and batch: 350, loss is 3.4127850341796875 and perplexity is 30.34965148306244
At time: 167.52365636825562 and batch: 400, loss is 3.3398967123031618 and perplexity is 28.21621216732498
At time: 168.41465997695923 and batch: 450, loss is 3.3401884841918945 and perplexity is 28.224446065993593
At time: 169.3115849494934 and batch: 500, loss is 3.21015163898468 and perplexity is 24.782843984968377
At time: 170.20282745361328 and batch: 550, loss is 3.259494242668152 and perplexity is 26.03636572916746
At time: 171.09536457061768 and batch: 600, loss is 3.2849560499191286 and perplexity is 26.707810478661433
At time: 171.9856264591217 and batch: 650, loss is 3.1035858726501466 and perplexity is 22.27769319455705
At time: 172.87762784957886 and batch: 700, loss is 3.0913571548461913 and perplexity is 22.00692452225484
At time: 173.77026557922363 and batch: 750, loss is 3.1943420791625976 and perplexity is 24.394119012311712
At time: 174.6611728668213 and batch: 800, loss is 3.1177427101135256 and perplexity is 22.59531785175578
At time: 175.55328154563904 and batch: 850, loss is 3.158894896507263 and perplexity is 23.544562368719724
At time: 176.4455804824829 and batch: 900, loss is 3.1271392345428466 and perplexity is 22.80863596279012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375619548640839 and perplexity of 79.48907164614805
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.5662829875946 and batch: 50, loss is 3.5360913848876954 and perplexity is 34.33246420719935
At time: 179.4579873085022 and batch: 100, loss is 3.40874520778656 and perplexity is 30.227291482969434
At time: 180.34969329833984 and batch: 150, loss is 3.4318329238891603 and perplexity is 30.9332891842324
At time: 181.24107384681702 and batch: 200, loss is 3.309468879699707 and perplexity is 27.37058453767352
At time: 182.13287568092346 and batch: 250, loss is 3.457012748718262 and perplexity is 31.722073031642807
At time: 183.02524137496948 and batch: 300, loss is 3.4214263486862184 and perplexity is 30.613048777081612
At time: 183.91596841812134 and batch: 350, loss is 3.4029702854156496 and perplexity is 30.053234288295815
At time: 184.80629634857178 and batch: 400, loss is 3.330256519317627 and perplexity is 27.94550934686607
At time: 185.6981544494629 and batch: 450, loss is 3.3309585857391357 and perplexity is 27.965135839354986
At time: 186.5904083251953 and batch: 500, loss is 3.2009336280822756 and perplexity is 24.55544515158864
At time: 187.48342537879944 and batch: 550, loss is 3.2496367692947388 and perplexity is 25.780973774974214
At time: 188.37606525421143 and batch: 600, loss is 3.2756894540786745 and perplexity is 26.46146315652128
At time: 189.27065062522888 and batch: 650, loss is 3.0946141338348387 and perplexity is 22.07871746361492
At time: 190.16890478134155 and batch: 700, loss is 3.0818954515457153 and perplexity is 21.799683504462134
At time: 191.0684039592743 and batch: 750, loss is 3.1843985652923585 and perplexity is 24.152757729872675
At time: 191.95894861221313 and batch: 800, loss is 3.108013563156128 and perplexity is 22.37655061874534
At time: 192.84966468811035 and batch: 850, loss is 3.147813096046448 and perplexity is 23.285086611329
At time: 193.74149084091187 and batch: 900, loss is 3.1162150287628174 and perplexity is 22.560825759231744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3755430456710185 and perplexity of 79.48299072870594
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.84700345993042 and batch: 50, loss is 3.533192195892334 and perplexity is 34.23307205272481
At time: 196.756902217865 and batch: 100, loss is 3.4057939624786377 and perplexity is 30.138214838976598
At time: 197.6506209373474 and batch: 150, loss is 3.4287468147277833 and perplexity is 30.837972831074726
At time: 198.54270553588867 and batch: 200, loss is 3.306835765838623 and perplexity is 27.298609472996947
At time: 199.44827008247375 and batch: 250, loss is 3.4537063455581665 and perplexity is 31.617360275815532
At time: 200.33965730667114 and batch: 300, loss is 3.4185833024978636 and perplexity is 30.52613806956084
At time: 201.2326476573944 and batch: 350, loss is 3.3999236965179445 and perplexity is 29.96181376945333
At time: 202.13915514945984 and batch: 400, loss is 3.327224473953247 and perplexity is 27.860905620779018
At time: 203.03292632102966 and batch: 450, loss is 3.328399701118469 and perplexity is 27.893667761620392
At time: 203.9258074760437 and batch: 500, loss is 3.1980884981155397 and perplexity is 24.485681009826834
At time: 204.81978058815002 and batch: 550, loss is 3.246656131744385 and perplexity is 25.704244444424315
At time: 205.71346879005432 and batch: 600, loss is 3.27299036026001 and perplexity is 26.390137485522803
At time: 206.60616874694824 and batch: 650, loss is 3.09210072517395 and perplexity is 22.02329430362243
At time: 207.50126957893372 and batch: 700, loss is 3.0792190742492678 and perplexity is 21.741417332371142
At time: 208.3962438106537 and batch: 750, loss is 3.1815176105499265 and perplexity is 24.08327486450164
At time: 209.2887589931488 and batch: 800, loss is 3.105461587905884 and perplexity is 22.31951901794922
At time: 210.1840100288391 and batch: 850, loss is 3.1450113344192503 and perplexity is 23.219938656312532
At time: 211.07793283462524 and batch: 900, loss is 3.113006467819214 and perplexity is 22.488553981049957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3751153815282535 and perplexity of 79.44900597116063
Annealing...
Model not improving. Stopping early with 78.20709468314841 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -78.20709468314841
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
222.93176126480103


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3301007747650146 and batch: 50, loss is 7.048911819458008 and perplexity is 1151.6049066555897
At time: 2.4440503120422363 and batch: 100, loss is 6.183367300033569 and perplexity is 484.62107651246714
At time: 3.558351516723633 and batch: 150, loss is 6.042912683486938 and perplexity is 421.11783325475614
At time: 4.67750096321106 and batch: 200, loss is 5.907550582885742 and perplexity is 367.80414539252945
At time: 5.792167901992798 and batch: 250, loss is 5.974090843200684 and perplexity is 393.11053945609973
At time: 6.908149242401123 and batch: 300, loss is 5.900874090194702 and perplexity is 365.35668302778635
At time: 8.023797273635864 and batch: 350, loss is 5.893569021224976 and perplexity is 362.6974520182803
At time: 9.13760256767273 and batch: 400, loss is 5.769959344863891 and perplexity is 320.5247014071029
At time: 10.252795696258545 and batch: 450, loss is 5.7739579963684085 and perplexity is 321.8089338745081
At time: 11.369031429290771 and batch: 500, loss is 5.725881338119507 and perplexity is 306.70345560815275
At time: 12.484060287475586 and batch: 550, loss is 5.771616201400757 and perplexity is 321.0562050449055
At time: 13.600123405456543 and batch: 600, loss is 5.702541446685791 and perplexity is 299.6279225359015
At time: 14.715015888214111 and batch: 650, loss is 5.6171281623840335 and perplexity is 275.09821032998053
At time: 15.828599214553833 and batch: 700, loss is 5.710731544494629 and perplexity is 302.0919811948406
At time: 16.942665338516235 and batch: 750, loss is 5.677518558502197 and perplexity is 292.22339423048084
At time: 18.05670666694641 and batch: 800, loss is 5.674026098251343 and perplexity is 291.20459573393464
At time: 19.170073986053467 and batch: 850, loss is 5.7120801544189455 and perplexity is 302.4996602769757
At time: 20.286463260650635 and batch: 900, loss is 5.5850669860839846 and perplexity is 266.4181284488788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.58479246374679 and perplexity of 266.34500075963723
finished 1 epochs...
Completing Train Step...
At time: 22.549670696258545 and batch: 50, loss is 5.524620561599732 and perplexity is 250.79116016830469
At time: 23.44105315208435 and batch: 100, loss is 5.396118621826172 and perplexity is 220.54871977166624
At time: 24.33379054069519 and batch: 150, loss is 5.335926818847656 and perplexity is 207.66512759828282
At time: 25.223583936691284 and batch: 200, loss is 5.166032161712646 and perplexity is 175.2182188056628
At time: 26.114556074142456 and batch: 250, loss is 5.213066463470459 and perplexity is 183.6563711502691
At time: 27.006985902786255 and batch: 300, loss is 5.123503875732422 and perplexity is 167.92272036298505
At time: 27.899462461471558 and batch: 350, loss is 5.077091760635376 and perplexity is 160.3071657336416
At time: 28.790419578552246 and batch: 400, loss is 4.925629539489746 and perplexity is 137.7760498109842
At time: 29.687828302383423 and batch: 450, loss is 4.92719500541687 and perplexity is 137.99190243341766
At time: 30.5783531665802 and batch: 500, loss is 4.835096817016602 and perplexity is 125.85076711286757
At time: 31.46933102607727 and batch: 550, loss is 4.891853532791138 and perplexity is 133.20023638823164
At time: 32.36820840835571 and batch: 600, loss is 4.80935754776001 and perplexity is 122.65279363767382
At time: 33.259567975997925 and batch: 650, loss is 4.675093879699707 and perplexity is 107.24263452579837
At time: 34.151047468185425 and batch: 700, loss is 4.736568307876587 and perplexity is 114.04217177929387
At time: 35.04461622238159 and batch: 750, loss is 4.748461313247681 and perplexity is 115.40657327584528
At time: 35.93747425079346 and batch: 800, loss is 4.684731578826904 and perplexity is 108.28120343935696
At time: 36.82930827140808 and batch: 850, loss is 4.736044549942017 and perplexity is 113.98245692637802
At time: 37.72116160392761 and batch: 900, loss is 4.653302812576294 and perplexity is 104.93098124766941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.781584857261344 and perplexity of 119.293263099411
finished 2 epochs...
Completing Train Step...
At time: 39.81274366378784 and batch: 50, loss is 4.704401550292968 and perplexity is 110.43217706431227
At time: 40.709150075912476 and batch: 100, loss is 4.5736028099060055 and perplexity is 96.89256718388528
At time: 41.59814739227295 and batch: 150, loss is 4.572790336608887 and perplexity is 96.81387653171463
At time: 42.4881591796875 and batch: 200, loss is 4.466418924331665 and perplexity is 87.04445144596505
At time: 43.377421140670776 and batch: 250, loss is 4.596805419921875 and perplexity is 99.1670121246948
At time: 44.26683759689331 and batch: 300, loss is 4.556730537414551 and perplexity is 95.27148352701536
At time: 45.15667104721069 and batch: 350, loss is 4.5412703895568844 and perplexity is 93.80989956885249
At time: 46.04591727256775 and batch: 400, loss is 4.4428626871109005 and perplexity is 85.0179735027571
At time: 46.93778443336487 and batch: 450, loss is 4.4599826049804685 and perplexity is 86.48600465750434
At time: 47.831212520599365 and batch: 500, loss is 4.349540643692016 and perplexity is 77.44288087762796
At time: 48.72418165206909 and batch: 550, loss is 4.431589641571045 and perplexity is 84.06494388160023
At time: 49.61692929267883 and batch: 600, loss is 4.407835292816162 and perplexity is 82.09156682212448
At time: 50.510085105895996 and batch: 650, loss is 4.261441283226013 and perplexity is 70.91211427770513
At time: 51.40314555168152 and batch: 700, loss is 4.294980235099793 and perplexity is 73.33076505544713
At time: 52.30245304107666 and batch: 750, loss is 4.373464360237121 and perplexity is 79.31794219505728
At time: 53.194725036621094 and batch: 800, loss is 4.308954033851624 and perplexity is 74.36266741528071
At time: 54.08501648902893 and batch: 850, loss is 4.386135406494141 and perplexity is 80.32937795166384
At time: 54.97727632522583 and batch: 900, loss is 4.316359949111939 and perplexity is 74.91543537854004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.528578248742509 and perplexity of 92.62677518683041
finished 3 epochs...
Completing Train Step...
At time: 57.07207250595093 and batch: 50, loss is 4.409157752990723 and perplexity is 82.20020146657686
At time: 57.97156047821045 and batch: 100, loss is 4.265427446365356 and perplexity is 71.1953456619902
At time: 58.86546730995178 and batch: 150, loss is 4.26926221370697 and perplexity is 71.46888739765438
At time: 59.7599093914032 and batch: 200, loss is 4.162345309257507 and perplexity is 64.22196648479137
At time: 60.654377937316895 and batch: 250, loss is 4.315909481048584 and perplexity is 74.88169596728382
At time: 61.54784870147705 and batch: 300, loss is 4.283433909416199 and perplexity is 72.4889335424612
At time: 62.44102120399475 and batch: 350, loss is 4.271709566116333 and perplexity is 71.64401115879178
At time: 63.33579969406128 and batch: 400, loss is 4.188707575798035 and perplexity is 65.93751662466897
At time: 64.22991228103638 and batch: 450, loss is 4.213843584060669 and perplexity is 67.61592850470657
At time: 65.12462878227234 and batch: 500, loss is 4.0938153648376465 and perplexity is 59.96825655694708
At time: 66.01700782775879 and batch: 550, loss is 4.17185495376587 and perplexity is 64.83560768606804
At time: 66.91136765480042 and batch: 600, loss is 4.17735755443573 and perplexity is 65.19335551339411
At time: 67.80603766441345 and batch: 650, loss is 4.019643330574036 and perplexity is 55.68124248788823
At time: 68.7013349533081 and batch: 700, loss is 4.038851938247681 and perplexity is 56.76114009030327
At time: 69.59562063217163 and batch: 750, loss is 4.1428490734100345 and perplexity is 62.9820064322461
At time: 70.48931860923767 and batch: 800, loss is 4.08293164730072 and perplexity is 59.319117919996515
At time: 71.38397288322449 and batch: 850, loss is 4.164911026954651 and perplexity is 64.38695348527241
At time: 72.27604246139526 and batch: 900, loss is 4.1008857727050785 and perplexity is 60.39375905541521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428303757758989 and perplexity of 83.78916957263576
finished 4 epochs...
Completing Train Step...
At time: 74.40226531028748 and batch: 50, loss is 4.2053565549850465 and perplexity is 67.04449845522292
At time: 75.29668712615967 and batch: 100, loss is 4.059841089248657 and perplexity is 57.96509906959019
At time: 76.1901524066925 and batch: 150, loss is 4.063673143386841 and perplexity is 58.18765060978464
At time: 77.08306956291199 and batch: 200, loss is 3.9564895248413086 and perplexity is 52.273498658813324
At time: 77.97613048553467 and batch: 250, loss is 4.115477585792542 and perplexity is 61.28147443661109
At time: 78.86893630027771 and batch: 300, loss is 4.0875316953659055 and perplexity is 59.59261728644819
At time: 79.7612771987915 and batch: 350, loss is 4.080265998840332 and perplexity is 59.161204568888955
At time: 80.65323758125305 and batch: 400, loss is 4.002311148643494 and perplexity is 54.724480401428416
At time: 81.54488754272461 and batch: 450, loss is 4.029905562400818 and perplexity is 56.25559835261431
At time: 82.43757128715515 and batch: 500, loss is 3.9075481605529787 and perplexity is 49.77675761628139
At time: 83.32967853546143 and batch: 550, loss is 3.985377702713013 and perplexity is 53.80560816461572
At time: 84.22216796875 and batch: 600, loss is 4.006524224281311 and perplexity is 54.95552513932441
At time: 85.11611604690552 and batch: 650, loss is 3.841592860221863 and perplexity is 46.599642070980856
At time: 86.00974607467651 and batch: 700, loss is 3.858937678337097 and perplexity is 47.414954671348355
At time: 86.90288209915161 and batch: 750, loss is 3.9652041149139405 and perplexity is 52.73103148121103
At time: 87.79687809944153 and batch: 800, loss is 3.916696333885193 and perplexity is 50.23421327436441
At time: 88.68960285186768 and batch: 850, loss is 3.9944997787475587 and perplexity is 54.29867247938423
At time: 89.58194661140442 and batch: 900, loss is 3.9402789545059203 and perplexity is 51.43294675181893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386238829730308 and perplexity of 80.33768630552191
finished 5 epochs...
Completing Train Step...
At time: 91.68473076820374 and batch: 50, loss is 4.044652042388916 and perplexity is 57.09131721923943
At time: 92.58495116233826 and batch: 100, loss is 3.9006096935272216 and perplexity is 49.43257864293735
At time: 93.47594022750854 and batch: 150, loss is 3.9046486711502073 and perplexity is 49.632639470554025
At time: 94.36723494529724 and batch: 200, loss is 3.801742339134216 and perplexity is 44.77913700652619
At time: 95.25939631462097 and batch: 250, loss is 3.9601324367523194 and perplexity is 52.46427368704088
At time: 96.15735983848572 and batch: 300, loss is 3.9344029331207278 and perplexity is 51.13161184899941
At time: 97.04793858528137 and batch: 350, loss is 3.9233528709411623 and perplexity is 50.5697145761031
At time: 97.9404673576355 and batch: 400, loss is 3.8535650253295897 and perplexity is 47.16089367445277
At time: 98.83227038383484 and batch: 450, loss is 3.8827281332015993 and perplexity is 48.556503147941186
At time: 99.73172354698181 and batch: 500, loss is 3.760338249206543 and perplexity is 42.96295570696859
At time: 100.6268253326416 and batch: 550, loss is 3.8380647230148317 and perplexity is 46.43552182956495
At time: 101.51862692832947 and batch: 600, loss is 3.8647225666046143 and perplexity is 47.69003978752041
At time: 102.4114420413971 and batch: 650, loss is 3.6969814586639402 and perplexity is 40.32539658431895
At time: 103.30399298667908 and batch: 700, loss is 3.716625852584839 and perplexity is 41.12539659082109
At time: 104.19665718078613 and batch: 750, loss is 3.8269537401199343 and perplexity is 45.92243327841482
At time: 105.08946800231934 and batch: 800, loss is 3.7763990592956542 and perplexity is 43.65854650306549
At time: 105.98188138008118 and batch: 850, loss is 3.852594428062439 and perplexity is 47.11514164692417
At time: 106.87554454803467 and batch: 900, loss is 3.805286464691162 and perplexity is 44.93812145441684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37250759177012 and perplexity of 79.24208958157828
finished 6 epochs...
Completing Train Step...
At time: 108.98292946815491 and batch: 50, loss is 3.9136276865005493 and perplexity is 50.08029846310138
At time: 109.88172936439514 and batch: 100, loss is 3.7693139934539794 and perplexity is 43.350316032511
At time: 110.77501583099365 and batch: 150, loss is 3.7761467552185057 and perplexity is 43.64753266325716
At time: 111.66796684265137 and batch: 200, loss is 3.674314045906067 and perplexity is 39.421606168355225
At time: 112.55993056297302 and batch: 250, loss is 3.828132824897766 and perplexity is 45.97661165462699
At time: 113.4543125629425 and batch: 300, loss is 3.8061265182495116 and perplexity is 44.97588774388215
At time: 114.34700226783752 and batch: 350, loss is 3.7932686042785644 and perplexity is 44.401293605757516
At time: 115.2413444519043 and batch: 400, loss is 3.7288629245758056 and perplexity is 41.63174280852787
At time: 116.13397479057312 and batch: 450, loss is 3.755717282295227 and perplexity is 42.76488330569926
At time: 117.0289409160614 and batch: 500, loss is 3.6335500288009643 and perplexity is 37.846936125541276
At time: 117.92150068283081 and batch: 550, loss is 3.713418126106262 and perplexity is 40.99368892125821
At time: 118.8302390575409 and batch: 600, loss is 3.7421348190307615 and perplexity is 42.18795775364957
At time: 119.72335577011108 and batch: 650, loss is 3.5770541715621946 and perplexity is 35.76801905113334
At time: 120.6186010837555 and batch: 700, loss is 3.5916510009765625 and perplexity is 36.29394783190316
At time: 121.51167130470276 and batch: 750, loss is 3.7079021883010865 and perplexity is 40.76819276588513
At time: 122.40364909172058 and batch: 800, loss is 3.657578220367432 and perplexity is 38.767343131942084
At time: 123.2962281703949 and batch: 850, loss is 3.731041660308838 and perplexity is 41.72254625665229
At time: 124.1884913444519 and batch: 900, loss is 3.691034531593323 and perplexity is 40.08629605311663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3782979886825775 and perplexity of 79.70226374219499
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 126.31271314620972 and batch: 50, loss is 3.817100028991699 and perplexity is 45.472149015489926
At time: 127.22006750106812 and batch: 100, loss is 3.6814188051223753 and perplexity is 39.702684502699015
At time: 128.12990736961365 and batch: 150, loss is 3.6871333503723145 and perplexity is 39.93021679242509
At time: 129.02173781394958 and batch: 200, loss is 3.5647268867492676 and perplexity is 35.32980304916126
At time: 129.92476344108582 and batch: 250, loss is 3.7082397079467775 and perplexity is 40.78195515427051
At time: 130.8183195590973 and batch: 300, loss is 3.68296332359314 and perplexity is 39.76405341276011
At time: 131.7081696987152 and batch: 350, loss is 3.6520399618148804 and perplexity is 38.553233008164696
At time: 132.59845662117004 and batch: 400, loss is 3.580712704658508 and perplexity is 35.89911719985191
At time: 133.48947596549988 and batch: 450, loss is 3.5931471967697144 and perplexity is 36.348291328083526
At time: 134.37630891799927 and batch: 500, loss is 3.4595562744140627 and perplexity is 31.802861639918092
At time: 135.26736783981323 and batch: 550, loss is 3.5187372255325315 and perplexity is 33.741793275166735
At time: 136.15817880630493 and batch: 600, loss is 3.5409278774261477 and perplexity is 34.49891510897292
At time: 137.05037808418274 and batch: 650, loss is 3.360601935386658 and perplexity is 28.80652532857198
At time: 137.94487237930298 and batch: 700, loss is 3.3566702795028687 and perplexity is 28.69349033672486
At time: 138.85871982574463 and batch: 750, loss is 3.459455327987671 and perplexity is 31.79965141671952
At time: 139.7556025981903 and batch: 800, loss is 3.3883184576034546 and perplexity is 29.616109631891735
At time: 140.6904091835022 and batch: 850, loss is 3.4443412160873415 and perplexity is 31.322641795705145
At time: 141.5856261253357 and batch: 900, loss is 3.3866804218292237 and perplexity is 29.567637095533765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359477003959761 and perplexity of 78.21621695243073
finished 8 epochs...
Completing Train Step...
At time: 143.6795072555542 and batch: 50, loss is 3.7058312940597533 and perplexity is 40.68385350923862
At time: 144.57645916938782 and batch: 100, loss is 3.563568630218506 and perplexity is 35.28890576339538
At time: 145.46906089782715 and batch: 150, loss is 3.56971773147583 and perplexity is 35.50656935006441
At time: 146.36150097846985 and batch: 200, loss is 3.4492433738708494 and perplexity is 31.476567303296083
At time: 147.25245118141174 and batch: 250, loss is 3.5944669580459596 and perplexity is 36.396294064544904
At time: 148.14357995986938 and batch: 300, loss is 3.5735447120666506 and perplexity is 35.64271264452611
At time: 149.03575587272644 and batch: 350, loss is 3.5473252391815184 and perplexity is 34.720324612316354
At time: 149.92844676971436 and batch: 400, loss is 3.4813737773895266 and perplexity is 32.50434515219063
At time: 150.82059454917908 and batch: 450, loss is 3.4974603128433226 and perplexity is 33.031455777824156
At time: 151.71160793304443 and batch: 500, loss is 3.3696647548675536 and perplexity is 29.06878025701728
At time: 152.60597801208496 and batch: 550, loss is 3.4330343341827394 and perplexity is 30.970475089564513
At time: 153.4978506565094 and batch: 600, loss is 3.4608640241622926 and perplexity is 31.84447903085574
At time: 154.3898742198944 and batch: 650, loss is 3.2845553874969484 and perplexity is 26.69711180604509
At time: 155.28216528892517 and batch: 700, loss is 3.286988606452942 and perplexity is 26.7621508195532
At time: 156.17367792129517 and batch: 750, loss is 3.3956791019439696 and perplexity is 29.83490754058096
At time: 157.06619429588318 and batch: 800, loss is 3.330971984863281 and perplexity is 27.96551055019223
At time: 157.95834136009216 and batch: 850, loss is 3.3931532526016235 and perplexity is 29.759644150996667
At time: 158.85022902488708 and batch: 900, loss is 3.343138265609741 and perplexity is 28.307824926777087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376714837061216 and perplexity of 79.57618280308068
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.95937252044678 and batch: 50, loss is 3.65704891204834 and perplexity is 38.746828684426085
At time: 161.861670255661 and batch: 100, loss is 3.5283826541900636 and perplexity is 34.06882196552746
At time: 162.75195813179016 and batch: 150, loss is 3.544611344337463 and perplexity is 34.62622504828326
At time: 163.64874529838562 and batch: 200, loss is 3.417740683555603 and perplexity is 30.50042700122722
At time: 164.54003524780273 and batch: 250, loss is 3.5568153429031373 and perplexity is 35.05139254392226
At time: 165.4330132007599 and batch: 300, loss is 3.534034638404846 and perplexity is 34.26192359913418
At time: 166.32356309890747 and batch: 350, loss is 3.503434009552002 and perplexity is 33.229366216418164
At time: 167.2144992351532 and batch: 400, loss is 3.4375413417816163 and perplexity is 31.110374282681587
At time: 168.10635232925415 and batch: 450, loss is 3.4519439840316775 and perplexity is 31.56168812813131
At time: 168.99832034111023 and batch: 500, loss is 3.316992425918579 and perplexity is 27.57728498064761
At time: 169.89250302314758 and batch: 550, loss is 3.370518388748169 and perplexity is 29.09360494680351
At time: 170.78425574302673 and batch: 600, loss is 3.3945385694503782 and perplexity is 29.800899256550668
At time: 171.6758029460907 and batch: 650, loss is 3.213403482437134 and perplexity is 24.863565088978852
At time: 172.56727170944214 and batch: 700, loss is 3.207944197654724 and perplexity is 24.728197647364226
At time: 173.46176505088806 and batch: 750, loss is 3.3143880939483643 and perplexity is 27.50555801667443
At time: 174.3543164730072 and batch: 800, loss is 3.242432770729065 and perplexity is 25.595915058582523
At time: 175.24614000320435 and batch: 850, loss is 3.302520236968994 and perplexity is 27.181055372215177
At time: 176.13867855072021 and batch: 900, loss is 3.2489011907577514 and perplexity is 25.76201681702317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370898939158819 and perplexity of 79.11471906217432
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.2466332912445 and batch: 50, loss is 3.636485457420349 and perplexity is 37.958196323292874
At time: 179.13950943946838 and batch: 100, loss is 3.507783727645874 and perplexity is 33.37421939876605
At time: 180.02929663658142 and batch: 150, loss is 3.526603946685791 and perplexity is 34.00827735777231
At time: 180.92199206352234 and batch: 200, loss is 3.4026124906539916 and perplexity is 30.042483321930998
At time: 181.81382155418396 and batch: 250, loss is 3.541227912902832 and perplexity is 34.50926756038643
At time: 182.70480346679688 and batch: 300, loss is 3.5138091707229613 and perplexity is 33.57592091826069
At time: 183.59821128845215 and batch: 350, loss is 3.486990132331848 and perplexity is 32.687414702299264
At time: 184.49033427238464 and batch: 400, loss is 3.4227847814559937 and perplexity is 30.654662804252894
At time: 185.39905285835266 and batch: 450, loss is 3.4326457929611207 and perplexity is 30.958444120754116
At time: 186.29182887077332 and batch: 500, loss is 3.300622386932373 and perplexity is 27.129518725164235
At time: 187.18676257133484 and batch: 550, loss is 3.351837453842163 and perplexity is 28.555154246683593
At time: 188.08013224601746 and batch: 600, loss is 3.3752761268615723 and perplexity is 29.232354505217714
At time: 188.97243237495422 and batch: 650, loss is 3.194228477478027 and perplexity is 24.391347956699576
At time: 189.86489748954773 and batch: 700, loss is 3.1853690099716188 and perplexity is 24.176208021866252
At time: 190.75767850875854 and batch: 750, loss is 3.2904626417160032 and perplexity is 26.85528515753363
At time: 191.6511685848236 and batch: 800, loss is 3.2147100162506104 and perplexity is 24.896071408169657
At time: 192.54608726501465 and batch: 850, loss is 3.2727975034713745 and perplexity is 26.38504845909812
At time: 193.4392421245575 and batch: 900, loss is 3.2250702953338624 and perplexity is 25.155342396860295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363955563061858 and perplexity of 78.56729848567511
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.5439691543579 and batch: 50, loss is 3.625575294494629 and perplexity is 37.54631713709982
At time: 196.44413828849792 and batch: 100, loss is 3.4967630100250244 and perplexity is 33.00843087921408
At time: 197.3387496471405 and batch: 150, loss is 3.514998617172241 and perplexity is 33.61588143891664
At time: 198.23505401611328 and batch: 200, loss is 3.393103852272034 and perplexity is 29.758174051079145
At time: 199.12885808944702 and batch: 250, loss is 3.535070948600769 and perplexity is 34.2974479838537
At time: 200.02362036705017 and batch: 300, loss is 3.505483102798462 and perplexity is 33.297526095441235
At time: 200.91729927062988 and batch: 350, loss is 3.4781316900253296 and perplexity is 32.39913386973337
At time: 201.81035447120667 and batch: 400, loss is 3.4155985832214357 and perplexity is 30.4351619534522
At time: 202.70542097091675 and batch: 450, loss is 3.424220108985901 and perplexity is 30.698693877642487
At time: 203.60100412368774 and batch: 500, loss is 3.2916219139099123 and perplexity is 26.88643579542178
At time: 204.4958746433258 and batch: 550, loss is 3.342399654388428 and perplexity is 28.28692416935238
At time: 205.39272904396057 and batch: 600, loss is 3.3677090978622437 and perplexity is 29.01198724519839
At time: 206.28766417503357 and batch: 650, loss is 3.1881869316101072 and perplexity is 24.244430759607543
At time: 207.17935609817505 and batch: 700, loss is 3.179399118423462 and perplexity is 24.032308641154547
At time: 208.07996582984924 and batch: 750, loss is 3.283399305343628 and perplexity is 26.6662655853591
At time: 208.97510719299316 and batch: 800, loss is 3.2054910469055176 and perplexity is 24.66760999648409
At time: 209.87000703811646 and batch: 850, loss is 3.2615679025650026 and perplexity is 26.090412314390957
At time: 210.7635133266449 and batch: 900, loss is 3.214457879066467 and perplexity is 24.889794974123028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3618837121414815 and perplexity of 78.4046872672656
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 212.8651602268219 and batch: 50, loss is 3.6225060653686523 and perplexity is 37.4312555524778
At time: 213.7649781703949 and batch: 100, loss is 3.4933524131774902 and perplexity is 32.896044190699904
At time: 214.65704464912415 and batch: 150, loss is 3.5120069360733033 and perplexity is 33.515463725575366
At time: 215.55032396316528 and batch: 200, loss is 3.3905717754364013 and perplexity is 29.68291938340148
At time: 216.4420268535614 and batch: 250, loss is 3.5332190561294556 and perplexity is 34.23399157350676
At time: 217.33538389205933 and batch: 300, loss is 3.5031311225891115 and perplexity is 33.21930299869177
At time: 218.22888350486755 and batch: 350, loss is 3.4747081565856934 and perplexity is 32.288404003366736
At time: 219.12214016914368 and batch: 400, loss is 3.4131145524978637 and perplexity is 30.359653897075187
At time: 220.01661372184753 and batch: 450, loss is 3.4218357896804807 and perplexity is 30.625585580575805
At time: 220.9103672504425 and batch: 500, loss is 3.288657879829407 and perplexity is 26.806861472182163
At time: 221.8028495311737 and batch: 550, loss is 3.339056525230408 and perplexity is 28.192515226945396
At time: 222.6964614391327 and batch: 600, loss is 3.3650104427337646 and perplexity is 28.933799445436946
At time: 223.58902025222778 and batch: 650, loss is 3.1856845903396604 and perplexity is 24.18383876248428
At time: 224.4824891090393 and batch: 700, loss is 3.177143039703369 and perplexity is 23.978150975983883
At time: 225.37682485580444 and batch: 750, loss is 3.2814257383346557 and perplexity is 26.613689821309112
At time: 226.26829028129578 and batch: 800, loss is 3.2030854654312133 and perplexity is 24.608341367200904
At time: 227.1595001220703 and batch: 850, loss is 3.2585314464569093 and perplexity is 26.011310078567124
At time: 228.0511612892151 and batch: 900, loss is 3.2112484312057497 and perplexity is 24.810040527218714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361043433620505 and perplexity of 78.33883316437993
Annealing...
Model not improving. Stopping early with 78.21621695243073 lossat 12 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
457.6150755882263


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3327956199645996 and batch: 50, loss is 6.945865249633789 and perplexity is 1038.8454691102695
At time: 2.4465956687927246 and batch: 100, loss is 6.084562177658081 and perplexity is 439.0275545166122
At time: 3.560281276702881 and batch: 150, loss is 5.833736066818237 and perplexity is 341.6326601963583
At time: 4.677821159362793 and batch: 200, loss is 5.603738040924072 and perplexity is 271.43916408771867
At time: 5.794072866439819 and batch: 250, loss is 5.625810995101928 and perplexity is 277.4972421952381
At time: 6.909538984298706 and batch: 300, loss is 5.51125958442688 and perplexity is 247.4626309198798
At time: 8.024561643600464 and batch: 350, loss is 5.465588321685791 and perplexity is 236.41490221645395
At time: 9.137464046478271 and batch: 400, loss is 5.300005130767822 and perplexity is 200.3378378590868
At time: 10.252929210662842 and batch: 450, loss is 5.286983137130737 and perplexity is 197.74595219443654
At time: 11.366573333740234 and batch: 500, loss is 5.219011745452881 and perplexity is 184.7515123003288
At time: 12.477559566497803 and batch: 550, loss is 5.268881072998047 and perplexity is 194.19854683418467
At time: 13.588532447814941 and batch: 600, loss is 5.179312200546264 and perplexity is 177.5606428714249
At time: 14.696238279342651 and batch: 650, loss is 5.0743927383422855 and perplexity is 159.8750764911375
At time: 15.806684970855713 and batch: 700, loss is 5.157877616882324 and perplexity is 173.7952038902463
At time: 16.91537594795227 and batch: 750, loss is 5.1324913215637205 and perplexity is 169.43871898941057
At time: 18.029722690582275 and batch: 800, loss is 5.091369256973267 and perplexity is 162.61236780350467
At time: 19.14272904396057 and batch: 850, loss is 5.137746410369873 and perplexity is 170.3314782148743
At time: 20.256377458572388 and batch: 900, loss is 5.044278135299683 and perplexity is 155.13227425713325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.006565877836045 and perplexity of 149.39082788907993
finished 1 epochs...
Completing Train Step...
At time: 22.50662922859192 and batch: 50, loss is 4.9593072032928465 and perplexity is 142.49504159700794
At time: 23.397114276885986 and batch: 100, loss is 4.823632307052613 and perplexity is 124.41618882066962
At time: 24.287477731704712 and batch: 150, loss is 4.806928663253784 and perplexity is 122.35524566866937
At time: 25.17917561531067 and batch: 200, loss is 4.6830048179626464 and perplexity is 108.0943890332446
At time: 26.073310613632202 and batch: 250, loss is 4.79049319267273 and perplexity is 120.36071505015819
At time: 26.96584415435791 and batch: 300, loss is 4.736160860061646 and perplexity is 113.99571501058846
At time: 27.868078231811523 and batch: 350, loss is 4.717201414108277 and perplexity is 111.85477902444764
At time: 28.76178288459778 and batch: 400, loss is 4.593725614547729 and perplexity is 98.86206685489448
At time: 29.660401105880737 and batch: 450, loss is 4.607746353149414 and perplexity is 100.2579488331295
At time: 30.5523419380188 and batch: 500, loss is 4.507930917739868 and perplexity is 90.73388827739194
At time: 31.4444637298584 and batch: 550, loss is 4.5779713535308835 and perplexity is 97.31677249562586
At time: 32.33709120750427 and batch: 600, loss is 4.538905925750733 and perplexity is 93.5883514810978
At time: 33.22975301742554 and batch: 650, loss is 4.395281658172608 and perplexity is 81.06746086031865
At time: 34.12248253822327 and batch: 700, loss is 4.442932863235473 and perplexity is 85.02393994400497
At time: 35.01452684402466 and batch: 750, loss is 4.495241355895996 and perplexity is 89.58978939982589
At time: 35.90638446807861 and batch: 800, loss is 4.43199990272522 and perplexity is 84.09943953813286
At time: 36.80196785926819 and batch: 850, loss is 4.504470443725586 and perplexity is 90.42044865241529
At time: 37.700639486312866 and batch: 900, loss is 4.4357203769683835 and perplexity is 84.41291210852233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.562641300567209 and perplexity of 95.83627824088015
finished 2 epochs...
Completing Train Step...
At time: 39.80950331687927 and batch: 50, loss is 4.476629610061646 and perplexity is 87.93778801223178
At time: 40.71009564399719 and batch: 100, loss is 4.334788241386414 and perplexity is 76.30879813344124
At time: 41.60235643386841 and batch: 150, loss is 4.335290222167969 and perplexity is 76.34711329950174
At time: 42.49449706077576 and batch: 200, loss is 4.224328083992004 and perplexity is 68.32857705618567
At time: 43.3885281085968 and batch: 250, loss is 4.373440866470337 and perplexity is 79.31607873971141
At time: 44.283365964889526 and batch: 300, loss is 4.332339096069336 and perplexity is 76.12213547313614
At time: 45.177409648895264 and batch: 350, loss is 4.329947996139526 and perplexity is 75.94033727579495
At time: 46.07175064086914 and batch: 400, loss is 4.237029566764831 and perplexity is 69.20198635507973
At time: 46.96700143814087 and batch: 450, loss is 4.2606954097747805 and perplexity is 70.85924253456056
At time: 47.86327910423279 and batch: 500, loss is 4.137144532203674 and perplexity is 62.62374580907417
At time: 48.756240367889404 and batch: 550, loss is 4.217501974105835 and perplexity is 67.8637469763613
At time: 49.64927053451538 and batch: 600, loss is 4.218910236358642 and perplexity is 67.95938425494121
At time: 50.543680906295776 and batch: 650, loss is 4.066462059020996 and perplexity is 58.35015756204266
At time: 51.437724351882935 and batch: 700, loss is 4.086789631843567 and perplexity is 59.548412182485315
At time: 52.3394558429718 and batch: 750, loss is 4.180183539390564 and perplexity is 65.37785152392006
At time: 53.23438501358032 and batch: 800, loss is 4.126731243133545 and perplexity is 61.97501023903256
At time: 54.12928009033203 and batch: 850, loss is 4.207819666862488 and perplexity is 67.20984009957623
At time: 55.02504920959473 and batch: 900, loss is 4.149917011260986 and perplexity is 63.42873620886107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.424689724020762 and perplexity of 83.48689922352416
finished 3 epochs...
Completing Train Step...
At time: 57.12783455848694 and batch: 50, loss is 4.215500674247742 and perplexity is 67.72806708252942
At time: 58.02314519882202 and batch: 100, loss is 4.070067553520203 and perplexity is 58.56091845446255
At time: 58.91495370864868 and batch: 150, loss is 4.079634017944336 and perplexity is 59.123827629817065
At time: 59.80753445625305 and batch: 200, loss is 3.9680881977081297 and perplexity is 52.88333165941905
At time: 60.70129084587097 and batch: 250, loss is 4.125783252716064 and perplexity is 61.9162863624367
At time: 61.595494508743286 and batch: 300, loss is 4.092383079528808 and perplexity is 59.88242638540377
At time: 62.49065589904785 and batch: 350, loss is 4.089954571723938 and perplexity is 59.73717788549548
At time: 63.38426733016968 and batch: 400, loss is 4.009217281341552 and perplexity is 55.103722967335926
At time: 64.27809429168701 and batch: 450, loss is 4.035948238372803 and perplexity is 56.59656183349339
At time: 65.17131423950195 and batch: 500, loss is 3.907485556602478 and perplexity is 49.77364149215336
At time: 66.06500124931335 and batch: 550, loss is 3.989055576324463 and perplexity is 54.00386274519915
At time: 66.95835971832275 and batch: 600, loss is 4.004663734436035 and perplexity is 54.85337599604792
At time: 67.85147666931152 and batch: 650, loss is 3.8490628814697265 and perplexity is 46.94904578923241
At time: 68.74579524993896 and batch: 700, loss is 3.85975980758667 and perplexity is 47.45395192064397
At time: 69.63803386688232 and batch: 750, loss is 3.9674111652374267 and perplexity is 52.847540044136686
At time: 70.53163743019104 and batch: 800, loss is 3.9184237003326414 and perplexity is 50.32106115635253
At time: 71.42506504058838 and batch: 850, loss is 4.000259799957275 and perplexity is 54.61233647292859
At time: 72.31829762458801 and batch: 900, loss is 3.949709610939026 and perplexity is 51.92028756195884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3777097937178935 and perplexity of 79.65539705671333
finished 4 epochs...
Completing Train Step...
At time: 74.44194960594177 and batch: 50, loss is 4.023207502365112 and perplexity is 55.88005409054469
At time: 75.33598613739014 and batch: 100, loss is 3.8803019523620605 and perplexity is 48.43883908523355
At time: 76.2293701171875 and batch: 150, loss is 3.8891774797439576 and perplexity is 48.870672869364945
At time: 77.12097764015198 and batch: 200, loss is 3.779620060920715 and perplexity is 43.799397471122525
At time: 78.01442170143127 and batch: 250, loss is 3.9371159887313842 and perplexity is 51.270523107185795
At time: 78.90737748146057 and batch: 300, loss is 3.91089967250824 and perplexity is 49.94386488911667
At time: 79.79966878890991 and batch: 350, loss is 3.9061586093902587 and perplexity is 49.70763829839512
At time: 80.69126081466675 and batch: 400, loss is 3.830861463546753 and perplexity is 46.10223652863585
At time: 81.5838725566864 and batch: 450, loss is 3.8608631181716917 and perplexity is 47.506337261435604
At time: 82.47547960281372 and batch: 500, loss is 3.7332472515106203 and perplexity is 41.81467069467094
At time: 83.36742186546326 and batch: 550, loss is 3.8118867015838624 and perplexity is 45.23570468112082
At time: 84.25948667526245 and batch: 600, loss is 3.8378790044784545 and perplexity is 46.42689869317777
At time: 85.1625018119812 and batch: 650, loss is 3.678834090232849 and perplexity is 39.60019689059706
At time: 86.05702209472656 and batch: 700, loss is 3.6880872631073 and perplexity is 39.96832490775267
At time: 86.9491503238678 and batch: 750, loss is 3.8006317043304443 and perplexity is 44.729431346011495
At time: 87.84171962738037 and batch: 800, loss is 3.7525918912887573 and perplexity is 42.63143497176195
At time: 88.7361102104187 and batch: 850, loss is 3.836295714378357 and perplexity is 46.35344960504924
At time: 89.62866520881653 and batch: 900, loss is 3.7875160932540894 and perplexity is 44.14660791822684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374554359749572 and perplexity of 79.4044458492844
finished 5 epochs...
Completing Train Step...
At time: 91.74643325805664 and batch: 50, loss is 3.863697361946106 and perplexity is 47.64117279018258
At time: 92.64582753181458 and batch: 100, loss is 3.727486114501953 and perplexity is 41.57446324622454
At time: 93.53879117965698 and batch: 150, loss is 3.733834981918335 and perplexity is 41.83925367150063
At time: 94.43108224868774 and batch: 200, loss is 3.6273361110687254 and perplexity is 37.612487554498294
At time: 95.32386016845703 and batch: 250, loss is 3.780350742340088 and perplexity is 43.83141257202969
At time: 96.22259426116943 and batch: 300, loss is 3.757802815437317 and perplexity is 42.85416395366592
At time: 97.12474203109741 and batch: 350, loss is 3.755261106491089 and perplexity is 42.74537944960024
At time: 98.02139782905579 and batch: 400, loss is 3.684910616874695 and perplexity is 39.841561127452096
At time: 98.913987159729 and batch: 450, loss is 3.712892117500305 and perplexity is 40.972131558271975
At time: 99.80696487426758 and batch: 500, loss is 3.589764404296875 and perplexity is 36.225540339394776
At time: 100.69855284690857 and batch: 550, loss is 3.6648124837875367 and perplexity is 39.04881319098205
At time: 101.59125590324402 and batch: 600, loss is 3.696606731414795 and perplexity is 40.31028839028901
At time: 102.48182487487793 and batch: 650, loss is 3.540022749900818 and perplexity is 34.46770331876986
At time: 103.37602281570435 and batch: 700, loss is 3.5464929580688476 and perplexity is 34.69143956382324
At time: 104.26898074150085 and batch: 750, loss is 3.6624978399276733 and perplexity is 38.95853361813072
At time: 105.15955257415771 and batch: 800, loss is 3.613636178970337 and perplexity is 37.10071267398773
At time: 106.05111742019653 and batch: 850, loss is 3.6985278367996215 and perplexity is 40.3878031355368
At time: 106.94395351409912 and batch: 900, loss is 3.6533646726608278 and perplexity is 38.604338736759814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.397616033684717 and perplexity of 81.25692380857647
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.04918360710144 and batch: 50, loss is 3.755384316444397 and perplexity is 42.75064643027194
At time: 109.95339059829712 and batch: 100, loss is 3.6322230911254882 and perplexity is 37.79674890511575
At time: 110.8587543964386 and batch: 150, loss is 3.63768358707428 and perplexity is 38.003702419575504
At time: 111.75354075431824 and batch: 200, loss is 3.520492825508118 and perplexity is 33.80108239537778
At time: 112.64834523200989 and batch: 250, loss is 3.6621011447906495 and perplexity is 38.943082022287925
At time: 113.54384136199951 and batch: 300, loss is 3.623810830116272 and perplexity is 37.480126410735146
At time: 114.43833994865417 and batch: 350, loss is 3.604341435432434 and perplexity is 36.757468718011914
At time: 115.33242130279541 and batch: 400, loss is 3.5301436710357668 and perplexity is 34.128870592725576
At time: 116.22577095031738 and batch: 450, loss is 3.542733359336853 and perplexity is 34.56125853917346
At time: 117.11865973472595 and batch: 500, loss is 3.4070717525482177 and perplexity is 30.176749765142553
At time: 118.01102638244629 and batch: 550, loss is 3.4644379901885984 and perplexity is 31.95849373802104
At time: 118.90617632865906 and batch: 600, loss is 3.485555386543274 and perplexity is 32.64055019907927
At time: 119.79676342010498 and batch: 650, loss is 3.3101567125320432 and perplexity is 27.38941740054696
At time: 120.69454431533813 and batch: 700, loss is 3.2997539138793943 and perplexity is 27.105967697407912
At time: 121.58702826499939 and batch: 750, loss is 3.3936160230636596 and perplexity is 29.77341922237269
At time: 122.48244118690491 and batch: 800, loss is 3.328388867378235 and perplexity is 27.893365570506614
At time: 123.37768244743347 and batch: 850, loss is 3.3822667503356936 and perplexity is 29.437422831433388
At time: 124.2717649936676 and batch: 900, loss is 3.3321888732910154 and perplexity is 27.999562170665875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37048799697667 and perplexity of 79.0822141661567
finished 7 epochs...
Completing Train Step...
At time: 126.37175965309143 and batch: 50, loss is 3.636923451423645 and perplexity is 37.974825427116436
At time: 127.26160740852356 and batch: 100, loss is 3.506690936088562 and perplexity is 33.33776825395433
At time: 128.15298676490784 and batch: 150, loss is 3.5102658319473266 and perplexity is 33.457160584047436
At time: 129.04532623291016 and batch: 200, loss is 3.3950152683258055 and perplexity is 29.815108698256346
At time: 129.93677020072937 and batch: 250, loss is 3.538145451545715 and perplexity is 34.40305785442286
At time: 130.8300359249115 and batch: 300, loss is 3.503781867027283 and perplexity is 33.24092731054533
At time: 131.72152876853943 and batch: 350, loss is 3.488558111190796 and perplexity is 32.73870808046343
At time: 132.61338710784912 and batch: 400, loss is 3.4210297775268557 and perplexity is 30.60091093175523
At time: 133.50424313545227 and batch: 450, loss is 3.438592624664307 and perplexity is 31.14309728419097
At time: 134.3968164920807 and batch: 500, loss is 3.3077612590789793 and perplexity is 27.323885846287457
At time: 135.28838539123535 and batch: 550, loss is 3.3695194482803346 and perplexity is 29.064556678627827
At time: 136.17852187156677 and batch: 600, loss is 3.396542959213257 and perplexity is 29.860691777685282
At time: 137.0703489780426 and batch: 650, loss is 3.228968858718872 and perplexity is 25.253603507796523
At time: 137.9647650718689 and batch: 700, loss is 3.2244396734237672 and perplexity is 25.139483887676125
At time: 138.859308719635 and batch: 750, loss is 3.3259561538696287 and perplexity is 27.825591474187036
At time: 139.75460624694824 and batch: 800, loss is 3.266070966720581 and perplexity is 26.208164037693717
At time: 140.66399836540222 and batch: 850, loss is 3.3276116847991943 and perplexity is 27.871695754506888
At time: 141.55751371383667 and batch: 900, loss is 3.2861942863464355 and perplexity is 26.74090154554392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393490726000642 and perplexity of 80.92240446841542
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.6630036830902 and batch: 50, loss is 3.58112669467926 and perplexity is 35.91398215288526
At time: 144.5627841949463 and batch: 100, loss is 3.466630163192749 and perplexity is 32.02862913161807
At time: 145.45626187324524 and batch: 150, loss is 3.4778651428222656 and perplexity is 32.39049912205374
At time: 146.3451862335205 and batch: 200, loss is 3.3594150066375734 and perplexity is 28.772354318786892
At time: 147.2323670387268 and batch: 250, loss is 3.50516206741333 and perplexity is 33.28683812702706
At time: 148.1211495399475 and batch: 300, loss is 3.463263416290283 and perplexity is 31.92097816216809
At time: 149.01299333572388 and batch: 350, loss is 3.443092050552368 and perplexity is 31.283539059089794
At time: 149.90299606323242 and batch: 400, loss is 3.3777557039260864 and perplexity is 29.304928319990083
At time: 150.79428458213806 and batch: 450, loss is 3.383625617027283 and perplexity is 29.47745155549569
At time: 151.68523907661438 and batch: 500, loss is 3.2471148347854615 and perplexity is 25.71603776412857
At time: 152.5756437778473 and batch: 550, loss is 3.2981291580200196 and perplexity is 27.06196287579385
At time: 153.46419620513916 and batch: 600, loss is 3.323726062774658 and perplexity is 27.763607011616138
At time: 154.35324931144714 and batch: 650, loss is 3.15105815410614 and perplexity is 23.36077080268991
At time: 155.24094104766846 and batch: 700, loss is 3.1363745021820066 and perplexity is 23.020255497825843
At time: 156.1304383277893 and batch: 750, loss is 3.2308213567733763 and perplexity is 25.30042911794865
At time: 157.02130818367004 and batch: 800, loss is 3.1677887296676634 and perplexity is 23.754897735628763
At time: 157.91616082191467 and batch: 850, loss is 3.2197958278656005 and perplexity is 25.023010658249245
At time: 158.80796837806702 and batch: 900, loss is 3.1800926971435546 and perplexity is 24.04898272074772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39076170202804 and perplexity of 80.70186635047864
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.91530919075012 and batch: 50, loss is 3.560613541603088 and perplexity is 35.184777849105615
At time: 161.8112063407898 and batch: 100, loss is 3.43928599357605 and perplexity is 31.16469842757844
At time: 162.69872069358826 and batch: 150, loss is 3.4507099914550783 and perplexity is 31.522765259480853
At time: 163.59009671211243 and batch: 200, loss is 3.3335771894454957 and perplexity is 28.03846141111981
At time: 164.47929501533508 and batch: 250, loss is 3.4799250078201296 and perplexity is 32.45728794181899
At time: 165.36744236946106 and batch: 300, loss is 3.4392975187301635 and perplexity is 31.165057607600524
At time: 166.25517654418945 and batch: 350, loss is 3.4189433336257933 and perplexity is 30.537130408154653
At time: 167.14268445968628 and batch: 400, loss is 3.355592031478882 and perplexity is 28.6625683112703
At time: 168.03085279464722 and batch: 450, loss is 3.360573134422302 and perplexity is 28.805695684810107
At time: 168.92203879356384 and batch: 500, loss is 3.2236030864715577 and perplexity is 25.118461318300135
At time: 169.81531190872192 and batch: 550, loss is 3.2717526149749756 and perplexity is 26.357493423962925
At time: 170.70784664154053 and batch: 600, loss is 3.2986071825027468 and perplexity is 27.07490224902133
At time: 171.60062527656555 and batch: 650, loss is 3.124015703201294 and perplexity is 22.73750362338129
At time: 172.49297547340393 and batch: 700, loss is 3.108526587486267 and perplexity is 22.388033278827496
At time: 173.38634538650513 and batch: 750, loss is 3.2001231718063354 and perplexity is 24.535552099270458
At time: 174.2783272266388 and batch: 800, loss is 3.135823731422424 and perplexity is 23.00758010515777
At time: 175.16949248313904 and batch: 850, loss is 3.1843990802764894 and perplexity is 24.152770168162824
At time: 176.06247973442078 and batch: 900, loss is 3.146972270011902 and perplexity is 23.26551613312634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38892061416417 and perplexity of 80.55342381355587
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.18652486801147 and batch: 50, loss is 3.5519292402267455 and perplexity is 34.88054556883318
At time: 179.08233952522278 and batch: 100, loss is 3.428307600021362 and perplexity is 30.82443131392553
At time: 179.97605657577515 and batch: 150, loss is 3.438597011566162 and perplexity is 31.14323390620189
At time: 180.86640667915344 and batch: 200, loss is 3.322737998962402 and perplexity is 27.736188344157483
At time: 181.75849771499634 and batch: 250, loss is 3.471079931259155 and perplexity is 32.1714666639968
At time: 182.65134811401367 and batch: 300, loss is 3.4300038766860963 and perplexity is 30.876762448960708
At time: 183.5428068637848 and batch: 350, loss is 3.4091968059539797 and perplexity is 30.240945155163825
At time: 184.4334533214569 and batch: 400, loss is 3.344481792449951 and perplexity is 28.34588280952291
At time: 185.32963037490845 and batch: 450, loss is 3.3506549644470214 and perplexity is 28.52140803581072
At time: 186.2234492301941 and batch: 500, loss is 3.213701357841492 and perplexity is 24.870972436662086
At time: 187.11844277381897 and batch: 550, loss is 3.2618642902374266 and perplexity is 26.09814633704727
At time: 188.01308274269104 and batch: 600, loss is 3.289802255630493 and perplexity is 26.83755615553062
At time: 188.90729570388794 and batch: 650, loss is 3.1159181594848633 and perplexity is 22.5541291372384
At time: 189.80227255821228 and batch: 700, loss is 3.100018787384033 and perplexity is 22.198368326794686
At time: 190.69660210609436 and batch: 750, loss is 3.1904066610336304 and perplexity is 24.298306608703523
At time: 191.60364413261414 and batch: 800, loss is 3.126048278808594 and perplexity is 22.783766318902956
At time: 192.49933671951294 and batch: 850, loss is 3.1731020784378052 and perplexity is 23.88145170725622
At time: 193.39542961120605 and batch: 900, loss is 3.1358776330947875 and perplexity is 23.008820285626072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388512180276113 and perplexity of 80.52052978344696
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.51406335830688 and batch: 50, loss is 3.5492034196853637 and perplexity is 34.78559692649667
At time: 196.4130096435547 and batch: 100, loss is 3.425528087615967 and perplexity is 30.738873384445228
At time: 197.30590438842773 and batch: 150, loss is 3.4352484464645388 and perplexity is 31.03912316806593
At time: 198.20024847984314 and batch: 200, loss is 3.319525113105774 and perplexity is 27.647218139000575
At time: 199.09249210357666 and batch: 250, loss is 3.4684050559997557 and perplexity is 32.085526993948505
At time: 199.98699522018433 and batch: 300, loss is 3.427417154312134 and perplexity is 30.79699604794285
At time: 200.88012790679932 and batch: 350, loss is 3.4059139585494993 and perplexity is 30.141831523329675
At time: 201.77365398406982 and batch: 400, loss is 3.341231985092163 and perplexity is 28.2539136729363
At time: 202.66885924339294 and batch: 450, loss is 3.3479372787475588 and perplexity is 28.444001044642533
At time: 203.56326866149902 and batch: 500, loss is 3.2107729864120484 and perplexity is 24.798247526301125
At time: 204.45722484588623 and batch: 550, loss is 3.2588339900970458 and perplexity is 26.0191808255652
At time: 205.3529531955719 and batch: 600, loss is 3.287198791503906 and perplexity is 26.76777641477441
At time: 206.24896955490112 and batch: 650, loss is 3.1135206747055055 and perplexity is 22.500120723964322
At time: 207.14403223991394 and batch: 700, loss is 3.097763681411743 and perplexity is 22.148365056333745
At time: 208.0443720817566 and batch: 750, loss is 3.1878199052810667 and perplexity is 24.23553404794975
At time: 208.93966841697693 and batch: 800, loss is 3.123314161300659 and perplexity is 22.72155790582214
At time: 209.8340060710907 and batch: 850, loss is 3.1702080726623536 and perplexity is 23.812438558481787
At time: 210.7261564731598 and batch: 900, loss is 3.13267466545105 and perplexity is 22.935241676586234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388013029751712 and perplexity of 80.48034794800694
Annealing...
Model not improving. Stopping early with 79.0822141661567 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
675.0996925830841


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -79.0822141661567}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9992209419307283, 'rnn_dropout': 0.9039337904170541, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.348780870437622 and batch: 50, loss is 9.769602470397949 and perplexity is 17493.811527974354
At time: 2.461893320083618 and batch: 100, loss is 9.568782367706298 and perplexity is 14310.980199248585
At time: 3.5755717754364014 and batch: 150, loss is 9.427852458953858 and perplexity is 12429.804532939243
At time: 4.688910007476807 and batch: 200, loss is 9.210796279907226 and perplexity is 10004.5601187286
At time: 5.801064491271973 and batch: 250, loss is 9.02247434616089 and perplexity is 8287.257276492868
At time: 6.913296222686768 and batch: 300, loss is 8.789019451141357 and perplexity is 6561.794859471081
At time: 8.027082681655884 and batch: 350, loss is 8.567513790130615 and perplexity is 5258.040932852194
At time: 9.140825748443604 and batch: 400, loss is 8.346844959259034 and perplexity is 4216.855382625907
At time: 10.26249098777771 and batch: 450, loss is 8.126622257232667 and perplexity is 3383.3521495600075
At time: 11.373260259628296 and batch: 500, loss is 7.940405340194702 and perplexity is 2808.498675012402
At time: 12.485552310943604 and batch: 550, loss is 7.799084901809692 and perplexity is 2438.3696087456574
At time: 13.598870992660522 and batch: 600, loss is 7.641068496704102 and perplexity is 2081.967203614081
At time: 14.710419654846191 and batch: 650, loss is 7.490477275848389 and perplexity is 1790.906644386888
At time: 15.82370901107788 and batch: 700, loss is 7.49045539855957 and perplexity is 1790.8674646335562
At time: 16.937288522720337 and batch: 750, loss is 7.351400318145752 and perplexity is 1558.3772245516884
At time: 18.049147367477417 and batch: 800, loss is 7.322461004257202 and perplexity is 1513.9251652228966
At time: 19.165803909301758 and batch: 850, loss is 7.300966024398804 and perplexity is 1481.7306243648768
At time: 20.27834415435791 and batch: 900, loss is 7.132057886123658 and perplexity is 1251.4496597401326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 7.092862063891267 and perplexity of 1203.346932559892
finished 1 epochs...
Completing Train Step...
At time: 22.541611671447754 and batch: 50, loss is 6.381940641403198 and perplexity is 591.0736573719289
At time: 23.441105365753174 and batch: 100, loss is 5.803836307525635 and perplexity is 331.5691242596374
At time: 24.335124492645264 and batch: 150, loss is 5.589826488494873 and perplexity is 267.6891685333183
At time: 25.225693702697754 and batch: 200, loss is 5.31946720123291 and perplexity is 204.27501551716648
At time: 26.1153404712677 and batch: 250, loss is 5.3068734073638915 and perplexity is 201.71854966975445
At time: 27.008007049560547 and batch: 300, loss is 5.18662693977356 and perplexity is 178.86421450199182
At time: 27.901076793670654 and batch: 350, loss is 5.118827505111694 and perplexity is 167.13928473231934
At time: 28.791818857192993 and batch: 400, loss is 4.94949294090271 and perplexity is 141.10339801731558
At time: 29.682687759399414 and batch: 450, loss is 4.9316321849823 and perplexity is 138.60555772561926
At time: 30.57267737388611 and batch: 500, loss is 4.836106538772583 and perplexity is 125.97790554684786
At time: 31.46378183364868 and batch: 550, loss is 4.885617246627808 and perplexity is 132.37214638399638
At time: 32.35499119758606 and batch: 600, loss is 4.805641136169434 and perplexity is 122.19781134818295
At time: 33.24730682373047 and batch: 650, loss is 4.664875535964966 and perplexity is 106.15237224714383
At time: 34.13931322097778 and batch: 700, loss is 4.717749042510986 and perplexity is 111.91605065393125
At time: 35.03285574913025 and batch: 750, loss is 4.729398403167725 and perplexity is 113.22742459502346
At time: 35.92465019226074 and batch: 800, loss is 4.660487279891968 and perplexity is 105.6875690386031
At time: 36.83114814758301 and batch: 850, loss is 4.712085065841674 and perplexity is 111.28395253843662
At time: 37.72122645378113 and batch: 900, loss is 4.637975463867187 and perplexity is 103.33493036813861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.727621836205051 and perplexity of 113.02644707109422
finished 2 epochs...
Completing Train Step...
At time: 39.8281409740448 and batch: 50, loss is 4.706524229049682 and perplexity is 110.66683806747697
At time: 40.728222370147705 and batch: 100, loss is 4.56121597290039 and perplexity is 95.69977744430858
At time: 41.62190771102905 and batch: 150, loss is 4.5485983562469485 and perplexity is 94.49986030521852
At time: 42.532342195510864 and batch: 200, loss is 4.433050966262817 and perplexity is 84.18787986262195
At time: 43.42846965789795 and batch: 250, loss is 4.56501070022583 and perplexity is 96.06362191347779
At time: 44.322630167007446 and batch: 300, loss is 4.519129943847656 and perplexity is 91.75573060072158
At time: 45.217344522476196 and batch: 350, loss is 4.502755041122437 and perplexity is 90.26547413926309
At time: 46.11196851730347 and batch: 400, loss is 4.39882031917572 and perplexity is 81.35483929022568
At time: 47.00599813461304 and batch: 450, loss is 4.417407884597778 and perplexity is 82.88116912070367
At time: 47.902364015579224 and batch: 500, loss is 4.302726244926452 and perplexity is 73.90099152090428
At time: 48.79789352416992 and batch: 550, loss is 4.377071709632873 and perplexity is 79.6045864280162
At time: 49.68955111503601 and batch: 600, loss is 4.363759593963623 and perplexity is 78.55190323158641
At time: 50.58266615867615 and batch: 650, loss is 4.213794684410095 and perplexity is 67.61262219026892
At time: 51.475879192352295 and batch: 700, loss is 4.2406221294403075 and perplexity is 69.4510459415409
At time: 52.369006395339966 and batch: 750, loss is 4.316992311477661 and perplexity is 74.96282406231609
At time: 53.263139486312866 and batch: 800, loss is 4.257805709838867 and perplexity is 70.65477615147735
At time: 54.15715932846069 and batch: 850, loss is 4.33024040222168 and perplexity is 75.96254593911105
At time: 55.05121874809265 and batch: 900, loss is 4.275811409950256 and perplexity is 71.93848723876452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.482525551155822 and perplexity of 88.45779549065536
finished 3 epochs...
Completing Train Step...
At time: 57.168713092803955 and batch: 50, loss is 4.368298110961914 and perplexity is 78.90922261635926
At time: 58.060773849487305 and batch: 100, loss is 4.220482869148254 and perplexity is 68.06634349274057
At time: 58.95331525802612 and batch: 150, loss is 4.218246445655823 and perplexity is 67.91428841625853
At time: 59.845319747924805 and batch: 200, loss is 4.109043927192688 and perplexity is 60.88847591632587
At time: 60.74292969703674 and batch: 250, loss is 4.259020657539367 and perplexity is 70.74067017710075
At time: 61.63723540306091 and batch: 300, loss is 4.223611841201782 and perplexity is 68.27965472772699
At time: 62.531182050704956 and batch: 350, loss is 4.213614077568054 and perplexity is 67.60041199074901
At time: 63.42296886444092 and batch: 400, loss is 4.128357982635498 and perplexity is 62.07590948246418
At time: 64.32322525978088 and batch: 450, loss is 4.149668984413147 and perplexity is 63.413006130178296
At time: 65.2182936668396 and batch: 500, loss is 4.024483337402343 and perplexity is 55.9513933203033
At time: 66.11241888999939 and batch: 550, loss is 4.099413609504699 and perplexity is 60.30491499832269
At time: 67.00474619865417 and batch: 600, loss is 4.112444257736206 and perplexity is 61.095869263871776
At time: 67.9108989238739 and batch: 650, loss is 3.955252709388733 and perplexity is 52.20888595314488
At time: 68.80381202697754 and batch: 700, loss is 3.966653003692627 and perplexity is 52.80748825631914
At time: 69.697674036026 and batch: 750, loss is 4.070373783111572 and perplexity is 58.57885428669219
At time: 70.58794403076172 and batch: 800, loss is 4.01771053314209 and perplexity is 55.573725862799364
At time: 71.47907614707947 and batch: 850, loss is 4.094942412376404 and perplexity is 60.03588173413958
At time: 72.37052202224731 and batch: 900, loss is 4.045560092926025 and perplexity is 57.143182565118416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395744846291738 and perplexity of 81.1050190426176
finished 4 epochs...
Completing Train Step...
At time: 74.47618508338928 and batch: 50, loss is 4.143104114532471 and perplexity is 62.998071482397
At time: 75.37456226348877 and batch: 100, loss is 3.9997983741760255 and perplexity is 54.587142745870295
At time: 76.26792502403259 and batch: 150, loss is 3.9996983432769775 and perplexity is 54.581682618000265
At time: 77.160471200943 and batch: 200, loss is 3.888384952545166 and perplexity is 48.831956875658214
At time: 78.0529215335846 and batch: 250, loss is 4.0416234111785885 and perplexity is 56.91867024804177
At time: 78.94636535644531 and batch: 300, loss is 4.01399185180664 and perplexity is 55.36744866298861
At time: 79.83813071250916 and batch: 350, loss is 4.00458333492279 and perplexity is 54.848965988601584
At time: 80.72895336151123 and batch: 400, loss is 3.9290023851394653 and perplexity is 50.85621743556529
At time: 81.62134003639221 and batch: 450, loss is 3.9533274507522584 and perplexity is 52.108467041768975
At time: 82.51457095146179 and batch: 500, loss is 3.8273331928253174 and perplexity is 45.93986197643428
At time: 83.40743112564087 and batch: 550, loss is 3.904343066215515 and perplexity is 49.61747380847864
At time: 84.29820322990417 and batch: 600, loss is 3.9219234943389893 and perplexity is 50.49748304463809
At time: 85.18902492523193 and batch: 650, loss is 3.765419769287109 and perplexity is 43.18182846128743
At time: 86.08151698112488 and batch: 700, loss is 3.771620678901672 and perplexity is 43.45042699351535
At time: 86.99051356315613 and batch: 750, loss is 3.8843096780776976 and perplexity is 48.63335819552973
At time: 87.88309478759766 and batch: 800, loss is 3.836679930686951 and perplexity is 46.371262778183755
At time: 88.77612733840942 and batch: 850, loss is 3.9093984270095827 and perplexity is 49.86894313880771
At time: 89.66989207267761 and batch: 900, loss is 3.8664063882827757 and perplexity is 47.77040895503135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36944120224208 and perplexity of 78.99947463397653
finished 5 epochs...
Completing Train Step...
At time: 91.77912855148315 and batch: 50, loss is 3.9668029356002807 and perplexity is 52.81540637734647
At time: 92.67629837989807 and batch: 100, loss is 3.8250877475738525 and perplexity is 45.836822259821595
At time: 93.56812882423401 and batch: 150, loss is 3.8285905218124388 and perplexity is 45.997659824402156
At time: 94.46135783195496 and batch: 200, loss is 3.7184351682662964 and perplexity is 41.199872770922056
At time: 95.35378241539001 and batch: 250, loss is 3.8733088541030884 and perplexity is 48.10128318040066
At time: 96.2471387386322 and batch: 300, loss is 3.84910306930542 and perplexity is 46.95093260768387
At time: 97.1407904624939 and batch: 350, loss is 3.8398751258850097 and perplexity is 46.51966497507747
At time: 98.0331346988678 and batch: 400, loss is 3.767701802253723 and perplexity is 43.28048334142562
At time: 98.93589735031128 and batch: 450, loss is 3.7951363039016726 and perplexity is 44.4842993758842
At time: 99.8320562839508 and batch: 500, loss is 3.6678168773651123 and perplexity is 39.16630760589868
At time: 100.73388648033142 and batch: 550, loss is 3.746405210494995 and perplexity is 42.3685020714202
At time: 101.62522578239441 and batch: 600, loss is 3.7687748575210573 and perplexity is 43.32695061856743
At time: 102.51921486854553 and batch: 650, loss is 3.612169413566589 and perplexity is 37.04633452192832
At time: 103.41209292411804 and batch: 700, loss is 3.6179900598526 and perplexity is 37.262596914239914
At time: 104.306223154068 and batch: 750, loss is 3.732917995452881 and perplexity is 41.80090522734845
At time: 105.19855070114136 and batch: 800, loss is 3.6868173933029174 and perplexity is 39.91760255103128
At time: 106.0918481349945 and batch: 850, loss is 3.7630923986434937 and perplexity is 43.081445201227055
At time: 106.99496054649353 and batch: 900, loss is 3.719187521934509 and perplexity is 41.23088130956239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371411467251712 and perplexity of 79.15527797115733
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.12221717834473 and batch: 50, loss is 3.8382762336730956 and perplexity is 46.44534447611454
At time: 110.01647686958313 and batch: 100, loss is 3.701713967323303 and perplexity is 40.51668916271259
At time: 110.91015887260437 and batch: 150, loss is 3.715120482444763 and perplexity is 41.06353422135878
At time: 111.80219531059265 and batch: 200, loss is 3.5837941646575926 and perplexity is 36.00990950686879
At time: 112.69509720802307 and batch: 250, loss is 3.7338315725326536 and perplexity is 41.839111025591414
At time: 113.5871810913086 and batch: 300, loss is 3.7008428239822386 and perplexity is 40.481408688152925
At time: 114.48163199424744 and batch: 350, loss is 3.674137005805969 and perplexity is 39.41462758101629
At time: 115.37486362457275 and batch: 400, loss is 3.5953040075302125 and perplexity is 36.4267723178439
At time: 116.27020955085754 and batch: 450, loss is 3.6132386445999147 and perplexity is 37.085966796723376
At time: 117.16469812393188 and batch: 500, loss is 3.4711250638961793 and perplexity is 32.172918679890685
At time: 118.0584762096405 and batch: 550, loss is 3.527327060699463 and perplexity is 34.032878113214096
At time: 118.95114064216614 and batch: 600, loss is 3.5440931606292723 and perplexity is 34.60828695060346
At time: 119.85951161384583 and batch: 650, loss is 3.3755711460113527 and perplexity is 29.240979881853
At time: 120.75622868537903 and batch: 700, loss is 3.3618041563034056 and perplexity is 28.8411779617473
At time: 121.65290689468384 and batch: 750, loss is 3.4583816003799437 and perplexity is 31.765525577283867
At time: 122.54906725883484 and batch: 800, loss is 3.393190975189209 and perplexity is 29.760766782953826
At time: 123.44399976730347 and batch: 850, loss is 3.4445599937438964 and perplexity is 31.329495239537174
At time: 124.338308095932 and batch: 900, loss is 3.389591484069824 and perplexity is 29.653835731350675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351542015598245 and perplexity of 77.59802808559881
finished 7 epochs...
Completing Train Step...
At time: 126.44900846481323 and batch: 50, loss is 3.7144071865081787 and perplexity is 41.03425421315378
At time: 127.34861445426941 and batch: 100, loss is 3.572140026092529 and perplexity is 35.592680973614534
At time: 128.24193167686462 and batch: 150, loss is 3.5850277614593504 and perplexity is 36.05435862657726
At time: 129.13686990737915 and batch: 200, loss is 3.461208910942078 and perplexity is 31.855463664801267
At time: 130.02787232398987 and batch: 250, loss is 3.6094745635986327 and perplexity is 36.9466346070221
At time: 130.92396187782288 and batch: 300, loss is 3.5824906730651858 and perplexity is 35.963001471339965
At time: 131.816486120224 and batch: 350, loss is 3.560156197547913 and perplexity is 35.1686899792499
At time: 132.70683407783508 and batch: 400, loss is 3.4853848123550417 and perplexity is 32.63498303854605
At time: 133.59678602218628 and batch: 450, loss is 3.5086469221115113 and perplexity is 33.40304027746797
At time: 134.50021600723267 and batch: 500, loss is 3.3722957277297976 and perplexity is 29.145360124614818
At time: 135.39080786705017 and batch: 550, loss is 3.4328613471984863 and perplexity is 30.965118063839107
At time: 136.283301115036 and batch: 600, loss is 3.456737561225891 and perplexity is 31.713344714929303
At time: 137.1779170036316 and batch: 650, loss is 3.2928282737731935 and perplexity is 26.91889008427189
At time: 138.07262587547302 and batch: 700, loss is 3.2851978397369384 and perplexity is 26.71426893605432
At time: 138.96617889404297 and batch: 750, loss is 3.3882267761230467 and perplexity is 29.61339450758197
At time: 139.85994172096252 and batch: 800, loss is 3.329079508781433 and perplexity is 27.91263653754201
At time: 140.7531681060791 and batch: 850, loss is 3.3888797998428344 and perplexity is 29.63273907216117
At time: 141.64717268943787 and batch: 900, loss is 3.3422337675094607 and perplexity is 28.28223212897107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372383013163527 and perplexity of 79.23221832736105
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.74910855293274 and batch: 50, loss is 3.6548076391220095 and perplexity is 38.66008371220557
At time: 144.647602558136 and batch: 100, loss is 3.5254112339019774 and perplexity is 33.96773943046993
At time: 145.53995752334595 and batch: 150, loss is 3.5452737331390383 and perplexity is 34.6491686699562
At time: 146.4331090450287 and batch: 200, loss is 3.418856563568115 and perplexity is 30.534480814542174
At time: 147.32598066329956 and batch: 250, loss is 3.5647413778305053 and perplexity is 35.330315019916846
At time: 148.21884989738464 and batch: 300, loss is 3.5318713569641114 and perplexity is 34.187885527155345
At time: 149.1109483242035 and batch: 350, loss is 3.5050810194015503 and perplexity is 33.28414040430244
At time: 150.0040364265442 and batch: 400, loss is 3.4297124576568603 and perplexity is 30.867765683799835
At time: 150.89623713493347 and batch: 450, loss is 3.44867130279541 and perplexity is 31.458565619200083
At time: 151.7958710193634 and batch: 500, loss is 3.3081778287887573 and perplexity is 27.33527052057505
At time: 152.6915671825409 and batch: 550, loss is 3.3596251249313354 and perplexity is 28.778400551973725
At time: 153.59009671211243 and batch: 600, loss is 3.3820219898223876 and perplexity is 29.430218594403105
At time: 154.48239016532898 and batch: 650, loss is 3.2114583349227903 and perplexity is 24.815248793543415
At time: 155.37589359283447 and batch: 700, loss is 3.197696285247803 and perplexity is 24.476079293740867
At time: 156.2692654132843 and batch: 750, loss is 3.2921516704559326 and perplexity is 26.900682834181385
At time: 157.16321754455566 and batch: 800, loss is 3.227031669616699 and perplexity is 25.20472985632715
At time: 158.06820631027222 and batch: 850, loss is 3.281118621826172 and perplexity is 26.605517572793776
At time: 158.96248078346252 and batch: 900, loss is 3.23556161403656 and perplexity is 25.420644361323596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.373801453472817 and perplexity of 79.34468424386918
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.1109583377838 and batch: 50, loss is 3.6342619943618772 and perplexity is 37.87389143513616
At time: 162.00350785255432 and batch: 100, loss is 3.5012740898132324 and perplexity is 33.157670908349125
At time: 162.89670634269714 and batch: 150, loss is 3.5229375553131104 and perplexity is 33.8838180008439
At time: 163.8010058403015 and batch: 200, loss is 3.39753812789917 and perplexity is 29.890422994416355
At time: 164.69798874855042 and batch: 250, loss is 3.543483896255493 and perplexity is 34.58720777637393
At time: 165.5908854007721 and batch: 300, loss is 3.5094135570526124 and perplexity is 33.42865803376254
At time: 166.4824414253235 and batch: 350, loss is 3.4807588148117063 and perplexity is 32.48436234127569
At time: 167.37509727478027 and batch: 400, loss is 3.4083243942260744 and perplexity is 30.2145741048269
At time: 168.26659727096558 and batch: 450, loss is 3.423616075515747 and perplexity is 30.680156438230938
At time: 169.16009616851807 and batch: 500, loss is 3.285560173988342 and perplexity is 26.723950184509103
At time: 170.05404257774353 and batch: 550, loss is 3.3360337591171265 and perplexity is 28.107424516717224
At time: 170.94681572914124 and batch: 600, loss is 3.357209949493408 and perplexity is 28.7089795315343
At time: 171.84026980400085 and batch: 650, loss is 3.1854302883148193 and perplexity is 24.17768954523089
At time: 172.7320351600647 and batch: 700, loss is 3.169344539642334 and perplexity is 23.791884607284366
At time: 173.62426781654358 and batch: 750, loss is 3.261673059463501 and perplexity is 26.093156045489074
At time: 174.52338290214539 and batch: 800, loss is 3.1945171546936035 and perplexity is 24.39839019953049
At time: 175.42633199691772 and batch: 850, loss is 3.2463257789611815 and perplexity is 25.695754378167678
At time: 176.31819891929626 and batch: 900, loss is 3.2058620071411132 and perplexity is 24.67676239638322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36781248327804 and perplexity of 78.87091141658631
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.42194938659668 and batch: 50, loss is 3.6256541061401366 and perplexity is 37.54927634074446
At time: 179.32253408432007 and batch: 100, loss is 3.4916542291641237 and perplexity is 32.84022806079569
At time: 180.21619987487793 and batch: 150, loss is 3.51273175239563 and perplexity is 33.53976508668168
At time: 181.1079294681549 and batch: 200, loss is 3.3871644496917725 and perplexity is 29.581952119873574
At time: 181.99977016448975 and batch: 250, loss is 3.53354248046875 and perplexity is 34.245065470300176
At time: 182.89344143867493 and batch: 300, loss is 3.501086893081665 and perplexity is 33.151464481658216
At time: 183.78794384002686 and batch: 350, loss is 3.4708507013320924 and perplexity is 32.164092846222054
At time: 184.68284392356873 and batch: 400, loss is 3.399893431663513 and perplexity is 29.96090699324289
At time: 185.57778453826904 and batch: 450, loss is 3.415001320838928 and perplexity is 30.4169896034826
At time: 186.47491240501404 and batch: 500, loss is 3.2772138833999636 and perplexity is 26.501832549166387
At time: 187.37007904052734 and batch: 550, loss is 3.3273595666885374 and perplexity is 27.864669680969865
At time: 188.2644248008728 and batch: 600, loss is 3.3486440563201905 and perplexity is 28.464111732711284
At time: 189.160254240036 and batch: 650, loss is 3.1768005180358885 and perplexity is 23.969939346139345
At time: 190.05414271354675 and batch: 700, loss is 3.1606303739547728 and perplexity is 23.58545890296392
At time: 190.94906449317932 and batch: 750, loss is 3.2537389469146727 and perplexity is 25.886949124856795
At time: 191.84555292129517 and batch: 800, loss is 3.185577492713928 and perplexity is 24.1812488694599
At time: 192.74020862579346 and batch: 850, loss is 3.2349841928482057 and perplexity is 25.405970179647177
At time: 193.63470029830933 and batch: 900, loss is 3.1957108211517333 and perplexity is 24.427531128358588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365422497056935 and perplexity of 78.68263610237763
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.73497676849365 and batch: 50, loss is 3.62224027633667 and perplexity is 37.42130805732475
At time: 196.632750749588 and batch: 100, loss is 3.4885489559173584 and perplexity is 32.73840835001102
At time: 197.52611112594604 and batch: 150, loss is 3.5096317625045774 and perplexity is 33.435953145084966
At time: 198.4272975921631 and batch: 200, loss is 3.3841814661026 and perplexity is 29.493841124326867
At time: 199.32045316696167 and batch: 250, loss is 3.530359492301941 and perplexity is 34.13623712368842
At time: 200.21147060394287 and batch: 300, loss is 3.4990820503234863 and perplexity is 33.08506758807439
At time: 201.10266423225403 and batch: 350, loss is 3.4674364948272705 and perplexity is 32.05446524333452
At time: 201.99556040763855 and batch: 400, loss is 3.397248930931091 and perplexity is 29.881780024532357
At time: 202.8907709121704 and batch: 450, loss is 3.4129276371002195 and perplexity is 30.353979740604803
At time: 203.78252840042114 and batch: 500, loss is 3.273996844291687 and perplexity is 26.41671210871933
At time: 204.6752860546112 and batch: 550, loss is 3.324494605064392 and perplexity is 27.784952719208945
At time: 205.5663299560547 and batch: 600, loss is 3.346315383911133 and perplexity is 28.397905257585183
At time: 206.45709371566772 and batch: 650, loss is 3.1746043729782105 and perplexity is 23.91735564416458
At time: 207.34999918937683 and batch: 700, loss is 3.157532911300659 and perplexity is 23.51251685079195
At time: 208.24282336235046 and batch: 750, loss is 3.250981101989746 and perplexity is 25.815655287451477
At time: 209.136816740036 and batch: 800, loss is 3.183245916366577 and perplexity is 24.12493411812931
At time: 210.03189754486084 and batch: 850, loss is 3.2321522521972654 and perplexity is 25.334123760333274
At time: 210.9247751235962 and batch: 900, loss is 3.192446188926697 and perplexity is 24.347914253678105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363925045483733 and perplexity of 78.56490083859089
Annealing...
Model not improving. Stopping early with 77.59802808559881 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
893.013810634613


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -79.0822141661567}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9992209419307283, 'rnn_dropout': 0.9039337904170541, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.59802808559881}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9437929462699219, 'rnn_dropout': 0.010083005405271739, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3228113651275635 and batch: 50, loss is 7.370064640045166 and perplexity is 1587.7364110807973
At time: 2.4332172870635986 and batch: 100, loss is 6.618109302520752 and perplexity is 748.5285173910439
At time: 3.5434415340423584 and batch: 150, loss is 6.403757305145263 and perplexity is 604.1106068812605
At time: 4.652070045471191 and batch: 200, loss is 6.233033304214477 and perplexity is 509.2979986199041
At time: 5.763831853866577 and batch: 250, loss is 6.282949657440185 and perplexity is 535.3654819226609
At time: 6.876064300537109 and batch: 300, loss is 6.17571949005127 and perplexity is 480.92892304579703
At time: 7.988001823425293 and batch: 350, loss is 6.175864191055298 and perplexity is 480.9985189790071
At time: 9.101943969726562 and batch: 400, loss is 6.068820362091064 and perplexity is 432.17057596913463
At time: 10.21849536895752 and batch: 450, loss is 6.0613916015625 and perplexity is 428.971979765307
At time: 11.333953380584717 and batch: 500, loss is 6.041482973098755 and perplexity is 420.51618690640356
At time: 12.449097394943237 and batch: 550, loss is 6.0744585609436035 and perplexity is 434.61412173993426
At time: 13.577412605285645 and batch: 600, loss is 6.012188568115234 and perplexity is 408.37610187646123
At time: 14.690966606140137 and batch: 650, loss is 5.939101362228394 and perplexity is 379.5936590219606
At time: 15.805407524108887 and batch: 700, loss is 6.045702981948852 and perplexity is 422.2945185853559
At time: 16.91962194442749 and batch: 750, loss is 5.991476707458496 and perplexity is 400.0048641697805
At time: 18.034533500671387 and batch: 800, loss is 6.000447645187378 and perplexity is 403.609426877408
At time: 19.148284912109375 and batch: 850, loss is 6.03939525604248 and perplexity is 419.6391838761913
At time: 20.26317596435547 and batch: 900, loss is 5.912867794036865 and perplexity is 369.76504633802625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.906270066352739 and perplexity of 367.33346752315924
finished 1 epochs...
Completing Train Step...
At time: 22.517696857452393 and batch: 50, loss is 5.682806854248047 and perplexity is 293.77285134574566
At time: 23.409899473190308 and batch: 100, loss is 5.417803535461426 and perplexity is 225.38353149273846
At time: 24.304110765457153 and batch: 150, loss is 5.297622528076172 and perplexity is 199.86108057449687
At time: 25.19562339782715 and batch: 200, loss is 5.111657028198242 and perplexity is 165.94510289311037
At time: 26.087341785430908 and batch: 250, loss is 5.1508293724060055 and perplexity is 172.5745595574887
At time: 26.979707956314087 and batch: 300, loss is 5.06475209236145 and perplexity is 158.34118321878074
At time: 27.87164044380188 and batch: 350, loss is 5.012900238037109 and perplexity is 150.34012662959915
At time: 28.76387667655945 and batch: 400, loss is 4.857523860931397 and perplexity is 128.70511545424256
At time: 29.669520378112793 and batch: 450, loss is 4.855262746810913 and perplexity is 128.41442726374518
At time: 30.561192750930786 and batch: 500, loss is 4.758827238082886 and perplexity is 116.60909097619378
At time: 31.453274965286255 and batch: 550, loss is 4.816719369888306 and perplexity is 123.5590735305674
At time: 32.35373830795288 and batch: 600, loss is 4.744332141876221 and perplexity is 114.93102224892017
At time: 33.2516655921936 and batch: 650, loss is 4.603159246444702 and perplexity is 99.79910810408828
At time: 34.14463543891907 and batch: 700, loss is 4.657332143783569 and perplexity is 105.35463587423733
At time: 35.03757047653198 and batch: 750, loss is 4.680245380401612 and perplexity is 107.79652047985628
At time: 35.92951583862305 and batch: 800, loss is 4.608103609085083 and perplexity is 100.29377297926148
At time: 36.838719606399536 and batch: 850, loss is 4.670618953704834 and perplexity is 106.76380383779362
At time: 37.73363757133484 and batch: 900, loss is 4.595626525878906 and perplexity is 99.05017360848912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.673179730977098 and perplexity of 107.03755251526356
finished 2 epochs...
Completing Train Step...
At time: 39.835460901260376 and batch: 50, loss is 4.642793312072754 and perplexity is 103.8339835930062
At time: 40.74026870727539 and batch: 100, loss is 4.500422387123108 and perplexity is 90.05516140879088
At time: 41.63084530830383 and batch: 150, loss is 4.490025291442871 and perplexity is 89.12369991711948
At time: 42.521297216415405 and batch: 200, loss is 4.378885631561279 and perplexity is 79.74911397415387
At time: 43.41176700592041 and batch: 250, loss is 4.5144134140014645 and perplexity is 91.3239809392455
At time: 44.30437254905701 and batch: 300, loss is 4.4708731079101565 and perplexity is 87.43302816577804
At time: 45.19545388221741 and batch: 350, loss is 4.459226751327515 and perplexity is 86.42065859409608
At time: 46.08869028091431 and batch: 400, loss is 4.364997954368591 and perplexity is 78.64923905425866
At time: 46.98387622833252 and batch: 450, loss is 4.380413799285889 and perplexity is 79.87107716256443
At time: 47.87815856933594 and batch: 500, loss is 4.265988140106201 and perplexity is 71.23527563987845
At time: 48.771947622299194 and batch: 550, loss is 4.343751168251037 and perplexity is 76.99582258591711
At time: 49.66572713851929 and batch: 600, loss is 4.33281533241272 and perplexity is 76.15839623424525
At time: 50.55907607078552 and batch: 650, loss is 4.173550038337708 and perplexity is 64.94560272347039
At time: 51.453646659851074 and batch: 700, loss is 4.202243094444275 and perplexity is 66.83608267032682
At time: 52.34804439544678 and batch: 750, loss is 4.2881605386734005 and perplexity is 72.8323728723601
At time: 53.24130845069885 and batch: 800, loss is 4.226821174621582 and perplexity is 68.49913891612029
At time: 54.13576078414917 and batch: 850, loss is 4.307349467277527 and perplexity is 74.24344324192707
At time: 55.03103518486023 and batch: 900, loss is 4.240587129592895 and perplexity is 69.44861520806822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.453480759712114 and perplexity of 85.92551013619577
finished 3 epochs...
Completing Train Step...
At time: 57.14280104637146 and batch: 50, loss is 4.3283054161071775 and perplexity is 75.81570158420803
At time: 58.04320669174194 and batch: 100, loss is 4.180870008468628 and perplexity is 65.42274680522966
At time: 58.93693423271179 and batch: 150, loss is 4.1784185123443605 and perplexity is 65.26255962434908
At time: 59.83706474304199 and batch: 200, loss is 4.069662742614746 and perplexity is 58.53721715363479
At time: 60.73074746131897 and batch: 250, loss is 4.220191383361817 and perplexity is 68.0465060123899
At time: 61.624215602874756 and batch: 300, loss is 4.1886460494995115 and perplexity is 65.93345985813743
At time: 62.52322030067444 and batch: 350, loss is 4.1839783668518065 and perplexity is 65.62642053018803
At time: 63.41459631919861 and batch: 400, loss is 4.10022801399231 and perplexity is 60.354047595910664
At time: 64.30709147453308 and batch: 450, loss is 4.121771750450134 and perplexity is 61.66840655700621
At time: 65.20049738883972 and batch: 500, loss is 3.9960417699813844 and perplexity is 54.38246514352952
At time: 66.09382176399231 and batch: 550, loss is 4.076969494819641 and perplexity is 58.96650051782202
At time: 66.98494124412537 and batch: 600, loss is 4.089084930419922 and perplexity is 59.68525055057115
At time: 67.87767910957336 and batch: 650, loss is 3.923895559310913 and perplexity is 50.5971656200726
At time: 68.77205038070679 and batch: 700, loss is 3.9457007122039793 and perplexity is 51.71256104232167
At time: 69.66697669029236 and batch: 750, loss is 4.04623471736908 and perplexity is 57.18174575920488
At time: 70.56261682510376 and batch: 800, loss is 3.993109455108643 and perplexity is 54.22323220682369
At time: 71.45581030845642 and batch: 850, loss is 4.073043828010559 and perplexity is 58.73547145215528
At time: 72.34917092323303 and batch: 900, loss is 4.016273427009582 and perplexity is 55.49391788056145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37595106151006 and perplexity of 79.51542766479947
finished 4 epochs...
Completing Train Step...
At time: 74.4601411819458 and batch: 50, loss is 4.108753514289856 and perplexity is 60.87079568469398
At time: 75.35163378715515 and batch: 100, loss is 3.966528282165527 and perplexity is 52.8009024364468
At time: 76.243971824646 and batch: 150, loss is 3.9675699281692505 and perplexity is 52.85593094059781
At time: 77.13610792160034 and batch: 200, loss is 3.8611952114105224 and perplexity is 47.52211641477179
At time: 78.02764201164246 and batch: 250, loss is 4.011688842773437 and perplexity is 55.240083646258114
At time: 78.92068982124329 and batch: 300, loss is 3.987488465309143 and perplexity is 53.91929897473566
At time: 79.81306576728821 and batch: 350, loss is 3.9801780366897583 and perplexity is 53.526563071327445
At time: 80.70447182655334 and batch: 400, loss is 3.9072817611694335 and perplexity is 49.76349888487488
At time: 81.61243867874146 and batch: 450, loss is 3.9310621309280394 and perplexity is 50.96107626944932
At time: 82.50698256492615 and batch: 500, loss is 3.80475519657135 and perplexity is 44.9142536037993
At time: 83.39707279205322 and batch: 550, loss is 3.884780235290527 and perplexity is 48.656248358155956
At time: 84.28415036201477 and batch: 600, loss is 3.9056200313568117 and perplexity is 49.68087406427431
At time: 85.17519688606262 and batch: 650, loss is 3.7438083028793336 and perplexity is 42.25861772722625
At time: 86.063809633255 and batch: 700, loss is 3.7616616344451903 and perplexity is 43.01984988651509
At time: 86.95405125617981 and batch: 750, loss is 3.8651042127609254 and perplexity is 47.708243981459006
At time: 87.84604406356812 and batch: 800, loss is 3.816224732398987 and perplexity is 45.43236481242209
At time: 88.73904323577881 and batch: 850, loss is 3.8910469818115234 and perplexity is 48.96212214901135
At time: 89.632253408432 and batch: 900, loss is 3.839648947715759 and perplexity is 46.50914443222267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.352294085776969 and perplexity of 77.65640919904529
finished 5 epochs...
Completing Train Step...
At time: 91.7673225402832 and batch: 50, loss is 3.937861819267273 and perplexity is 51.30877649240828
At time: 92.66662120819092 and batch: 100, loss is 3.797315616607666 and perplexity is 44.581350288463575
At time: 93.55975770950317 and batch: 150, loss is 3.8032642507553103 and perplexity is 44.84733878089131
At time: 94.45251178741455 and batch: 200, loss is 3.6962064933776855 and perplexity is 40.29415790782001
At time: 95.34526634216309 and batch: 250, loss is 3.850008654594421 and perplexity is 46.993469939242104
At time: 96.24671626091003 and batch: 300, loss is 3.8215354442596436 and perplexity is 45.67428482624692
At time: 97.1423966884613 and batch: 350, loss is 3.8155233669281006 and perplexity is 45.40051129226729
At time: 98.03450894355774 and batch: 400, loss is 3.7532328033447264 and perplexity is 42.65876673008974
At time: 98.92783713340759 and batch: 450, loss is 3.773896632194519 and perplexity is 43.549430757141444
At time: 99.82233214378357 and batch: 500, loss is 3.6517131900787354 and perplexity is 38.54063695940906
At time: 100.71774077415466 and batch: 550, loss is 3.72940505027771 and perplexity is 41.65431856521549
At time: 101.60973620414734 and batch: 600, loss is 3.756248269081116 and perplexity is 42.78759692341642
At time: 102.5017442703247 and batch: 650, loss is 3.595345211029053 and perplexity is 36.42827325923666
At time: 103.39584183692932 and batch: 700, loss is 3.6100534439086913 and perplexity is 36.96802847796693
At time: 104.30461359024048 and batch: 750, loss is 3.718339123725891 and perplexity is 41.19591593809615
At time: 105.19909310340881 and batch: 800, loss is 3.670770359039307 and perplexity is 39.282155570867005
At time: 106.09174656867981 and batch: 850, loss is 3.745989513397217 and perplexity is 42.35089326828966
At time: 106.98606204986572 and batch: 900, loss is 3.6950669384002683 and perplexity is 40.24826665238574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362929252729024 and perplexity of 78.48670541923893
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.1083447933197 and batch: 50, loss is 3.8145630502700807 and perplexity is 45.35693335265037
At time: 110.00913405418396 and batch: 100, loss is 3.685708613395691 and perplexity is 39.87336724352001
At time: 110.90431690216064 and batch: 150, loss is 3.6920375871658324 and perplexity is 40.12652500831989
At time: 111.79811429977417 and batch: 200, loss is 3.567382049560547 and perplexity is 35.42373407424593
At time: 112.6908700466156 and batch: 250, loss is 3.715173325538635 and perplexity is 41.065704202886124
At time: 113.58464121818542 and batch: 300, loss is 3.6786273908615112 and perplexity is 39.59201240068853
At time: 114.47988080978394 and batch: 350, loss is 3.655473384857178 and perplexity is 38.68583006737037
At time: 115.3735044002533 and batch: 400, loss is 3.5861731481552126 and perplexity is 36.095678468357185
At time: 116.26710724830627 and batch: 450, loss is 3.5907642316818236 and perplexity is 36.26177773921428
At time: 117.15970134735107 and batch: 500, loss is 3.458958959579468 and perplexity is 31.783870991140752
At time: 118.05301976203918 and batch: 550, loss is 3.5133530378341673 and perplexity is 33.56060932878358
At time: 118.94383454322815 and batch: 600, loss is 3.530552821159363 and perplexity is 34.14283728138814
At time: 119.83808469772339 and batch: 650, loss is 3.3526995182037354 and perplexity is 28.579781241000784
At time: 120.73302936553955 and batch: 700, loss is 3.3533346462249756 and perplexity is 28.597938826493152
At time: 121.62751030921936 and batch: 750, loss is 3.443829355239868 and perplexity is 31.306613064320473
At time: 122.53374814987183 and batch: 800, loss is 3.3754257440567015 and perplexity is 29.23672849530971
At time: 123.44308805465698 and batch: 850, loss is 3.427263774871826 and perplexity is 30.7922727841609
At time: 124.33974885940552 and batch: 900, loss is 3.367685022354126 and perplexity is 29.011288775272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355538564185574 and perplexity of 77.90877291491653
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 126.54246091842651 and batch: 50, loss is 3.7145600080490113 and perplexity is 41.04052561029963
At time: 127.43445634841919 and batch: 100, loss is 3.59580304145813 and perplexity is 36.44495504963771
At time: 128.32760906219482 and batch: 150, loss is 3.6031032180786133 and perplexity is 36.71198314868487
At time: 129.21993589401245 and batch: 200, loss is 3.474642667770386 and perplexity is 32.28628954327788
At time: 130.11434841156006 and batch: 250, loss is 3.624344000816345 and perplexity is 37.500115044175715
At time: 131.01535964012146 and batch: 300, loss is 3.5893733978271483 and perplexity is 36.211378687581906
At time: 131.9137167930603 and batch: 350, loss is 3.5615601444244387 and perplexity is 35.21809962784287
At time: 132.81721329689026 and batch: 400, loss is 3.495186128616333 and perplexity is 32.9564215153071
At time: 133.71138620376587 and batch: 450, loss is 3.49755708694458 and perplexity is 33.0346525219488
At time: 134.60322213172913 and batch: 500, loss is 3.365988726615906 and perplexity is 28.962118764991907
At time: 135.4955554008484 and batch: 550, loss is 3.4148143482208253 and perplexity is 30.41130299093862
At time: 136.38704228401184 and batch: 600, loss is 3.4326416635513306 and perplexity is 30.958316280915827
At time: 137.28882122039795 and batch: 650, loss is 3.2500926876068115 and perplexity is 25.792730472864573
At time: 138.19046449661255 and batch: 700, loss is 3.25049307346344 and perplexity is 25.803059585026958
At time: 139.08498358726501 and batch: 750, loss is 3.3353642654418945 and perplexity is 28.088613071550643
At time: 139.97927403450012 and batch: 800, loss is 3.2663337087631223 and perplexity is 26.215050928942556
At time: 140.87294554710388 and batch: 850, loss is 3.3156835651397705 and perplexity is 27.5412137651995
At time: 141.76627826690674 and batch: 900, loss is 3.2579835748672483 and perplexity is 25.99706312387244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.359207780393835 and perplexity of 78.1951621379429
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.90606451034546 and batch: 50, loss is 3.6853394508361816 and perplexity is 39.85865020586891
At time: 144.80512762069702 and batch: 100, loss is 3.5652071142196657 and perplexity is 35.34677346561316
At time: 145.7034866809845 and batch: 150, loss is 3.5733552026748656 and perplexity is 35.63595865572365
At time: 146.60725378990173 and batch: 200, loss is 3.4446249485015867 and perplexity is 31.33153030540191
At time: 147.50901222229004 and batch: 250, loss is 3.596185336112976 and perplexity is 36.458890424689265
At time: 148.4100432395935 and batch: 300, loss is 3.561690034866333 and perplexity is 35.22267441947063
At time: 149.30128145217896 and batch: 350, loss is 3.5320296049118043 and perplexity is 34.19329611797213
At time: 150.1929008960724 and batch: 400, loss is 3.466900472640991 and perplexity is 32.03728794291312
At time: 151.08536505699158 and batch: 450, loss is 3.467781276702881 and perplexity is 32.06551894743173
At time: 151.97777605056763 and batch: 500, loss is 3.3370843029022215 and perplexity is 28.13696811256166
At time: 152.87064743041992 and batch: 550, loss is 3.3846967792510987 and perplexity is 29.509043605140675
At time: 153.76212906837463 and batch: 600, loss is 3.4037595701217653 and perplexity is 30.076964210089134
At time: 154.65396285057068 and batch: 650, loss is 3.218682985305786 and perplexity is 24.995179475748465
At time: 155.54504823684692 and batch: 700, loss is 3.2177450704574584 and perplexity is 24.97174711627824
At time: 156.4497365951538 and batch: 750, loss is 3.301040081977844 and perplexity is 27.140852957683347
At time: 157.34274792671204 and batch: 800, loss is 3.232861728668213 and perplexity is 25.35210410261399
At time: 158.23616480827332 and batch: 850, loss is 3.2790433740615845 and perplexity is 26.55036178268494
At time: 159.128755569458 and batch: 900, loss is 3.225823817253113 and perplexity is 25.174304642079065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355925677573844 and perplexity of 77.93893828230767
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.24381756782532 and batch: 50, loss is 3.675094876289368 and perplexity is 39.45239977693804
At time: 162.15005254745483 and batch: 100, loss is 3.553203363418579 and perplexity is 34.92501600527738
At time: 163.0444483757019 and batch: 150, loss is 3.5627358865737917 and perplexity is 35.259531383753476
At time: 163.94922542572021 and batch: 200, loss is 3.4341411638259887 and perplexity is 31.00477310698889
At time: 164.8451328277588 and batch: 250, loss is 3.58613121509552 and perplexity is 36.09416489785187
At time: 165.73977327346802 and batch: 300, loss is 3.5511509704589845 and perplexity is 34.85340965563407
At time: 166.63322687149048 and batch: 350, loss is 3.5226879501342774 and perplexity is 33.87536147983188
At time: 167.52672338485718 and batch: 400, loss is 3.4567068099975584 and perplexity is 31.712369505619304
At time: 168.4190330505371 and batch: 450, loss is 3.457958068847656 and perplexity is 31.752074724193477
At time: 169.3099250793457 and batch: 500, loss is 3.3270203638076783 and perplexity is 27.85521950759352
At time: 170.21837735176086 and batch: 550, loss is 3.374288067817688 and perplexity is 29.20348547747541
At time: 171.12568521499634 and batch: 600, loss is 3.3952730894088745 and perplexity is 29.82279665288856
At time: 172.0260570049286 and batch: 650, loss is 3.210086212158203 and perplexity is 24.78122257517779
At time: 172.92213582992554 and batch: 700, loss is 3.2078294801712035 and perplexity is 24.725361053464688
At time: 173.81421160697937 and batch: 750, loss is 3.2911863040924074 and perplexity is 26.874726350591303
At time: 174.70413255691528 and batch: 800, loss is 3.2234113121032717 and perplexity is 25.113644703114932
At time: 175.5937671661377 and batch: 850, loss is 3.2679985666275027 and perplexity is 26.25873161362311
At time: 176.48387360572815 and batch: 900, loss is 3.2160572814941406 and perplexity is 24.929635624752613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353191637013056 and perplexity of 77.72614109440508
Annealing...
Model not improving. Stopping early with 77.65640919904529 lossat 9 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
1076.1851589679718


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -79.0822141661567}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9992209419307283, 'rnn_dropout': 0.9039337904170541, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.59802808559881}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9437929462699219, 'rnn_dropout': 0.010083005405271739, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.65640919904529}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9722729760825943, 'rnn_dropout': 0.8741438177807345, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3403918743133545 and batch: 50, loss is 8.179774179458619 and perplexity is 3568.048831121253
At time: 2.4549572467803955 and batch: 100, loss is 7.36857666015625 and perplexity is 1585.3756480517404
At time: 3.57053279876709 and batch: 150, loss is 7.142407627105713 and perplexity is 1264.4690972566711
At time: 4.6838295459747314 and batch: 200, loss is 6.93266095161438 and perplexity is 1025.2184098092932
At time: 5.798742294311523 and batch: 250, loss is 6.91116849899292 and perplexity is 1003.4190516792387
At time: 6.913954257965088 and batch: 300, loss is 6.775886421203613 and perplexity is 876.4559279203479
At time: 8.028780460357666 and batch: 350, loss is 6.744154939651489 and perplexity is 849.0812984780187
At time: 9.141864538192749 and batch: 400, loss is 6.635220279693604 and perplexity is 761.4467786133251
At time: 10.253934144973755 and batch: 450, loss is 6.612854242324829 and perplexity is 744.6052724465407
At time: 11.367838859558105 and batch: 500, loss is 6.577119836807251 and perplexity is 718.4670440863794
At time: 12.481797456741333 and batch: 550, loss is 6.580851888656616 and perplexity is 721.1534100574931
At time: 13.594660520553589 and batch: 600, loss is 6.534303197860718 and perplexity is 688.3539706563897
At time: 14.707245349884033 and batch: 650, loss is 6.456582746505737 and perplexity is 636.8809500507906
At time: 15.821568965911865 and batch: 700, loss is 6.5317431640625 and perplexity is 686.5940149606002
At time: 16.936392068862915 and batch: 750, loss is 6.482158117294311 and perplexity is 653.3794954061428
At time: 18.053831577301025 and batch: 800, loss is 6.4821519947052 and perplexity is 653.3754950442051
At time: 19.167606830596924 and batch: 850, loss is 6.505780010223389 and perplexity is 668.9972906191488
At time: 20.28584098815918 and batch: 900, loss is 6.402938661575317 and perplexity is 603.6162579927787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.448842923935145 and perplexity of 631.9706315085832
finished 1 epochs...
Completing Train Step...
At time: 22.53148651123047 and batch: 50, loss is 6.142778959274292 and perplexity is 465.3449503482189
At time: 23.44224500656128 and batch: 100, loss is 5.714757080078125 and perplexity is 303.31051419308096
At time: 24.333411693572998 and batch: 150, loss is 5.515296697616577 and perplexity is 248.46368489560672
At time: 25.228582620620728 and batch: 200, loss is 5.284091539382935 and perplexity is 197.17497635849813
At time: 26.13426685333252 and batch: 250, loss is 5.2921049022674564 and perplexity is 198.76135863524948
At time: 27.03645634651184 and batch: 300, loss is 5.17954574584961 and perplexity is 177.60211616838487
At time: 27.94261884689331 and batch: 350, loss is 5.118763380050659 and perplexity is 167.12856725911746
At time: 28.85108733177185 and batch: 400, loss is 4.9441649532318115 and perplexity is 140.3536000831688
At time: 29.74492335319519 and batch: 450, loss is 4.9276931190490725 and perplexity is 138.0606552030777
At time: 30.652155876159668 and batch: 500, loss is 4.833224802017212 and perplexity is 125.61539296984854
At time: 31.54639768600464 and batch: 550, loss is 4.881348648071289 and perplexity is 131.80830708895843
At time: 32.43843746185303 and batch: 600, loss is 4.80058931350708 and perplexity is 121.58204635287636
At time: 33.348549365997314 and batch: 650, loss is 4.664013319015503 and perplexity is 106.06088531902438
At time: 34.24515962600708 and batch: 700, loss is 4.717368326187134 and perplexity is 111.87345049634844
At time: 35.140626668930054 and batch: 750, loss is 4.725353631973267 and perplexity is 112.770370532276
At time: 36.045448303222656 and batch: 800, loss is 4.651963520050049 and perplexity is 104.7905420342994
At time: 36.93968343734741 and batch: 850, loss is 4.705319118499756 and perplexity is 110.53355262137428
At time: 37.83427119255066 and batch: 900, loss is 4.632695455551147 and perplexity is 102.79075895547194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.719721545911815 and perplexity of 112.13702330746261
finished 2 epochs...
Completing Train Step...
At time: 39.942164182662964 and batch: 50, loss is 4.695591440200806 and perplexity is 109.46353063287087
At time: 40.839972496032715 and batch: 100, loss is 4.547906293869018 and perplexity is 94.43448313233183
At time: 41.73234224319458 and batch: 150, loss is 4.549938707351685 and perplexity is 94.62660822175536
At time: 42.63042211532593 and batch: 200, loss is 4.421656322479248 and perplexity is 83.23403365002922
At time: 43.52130365371704 and batch: 250, loss is 4.558939094543457 and perplexity is 95.48212856631964
At time: 44.42029356956482 and batch: 300, loss is 4.509075689315796 and perplexity is 90.83781732982793
At time: 45.327330112457275 and batch: 350, loss is 4.501056060791016 and perplexity is 90.11224507753357
At time: 46.23558235168457 and batch: 400, loss is 4.39112476348877 and perplexity is 80.73117140740597
At time: 47.14439558982849 and batch: 450, loss is 4.409721689224243 and perplexity is 82.24657021186547
At time: 48.03556823730469 and batch: 500, loss is 4.297702069282532 and perplexity is 73.53063111616916
At time: 48.928282260894775 and batch: 550, loss is 4.372118444442749 and perplexity is 79.21125873346247
At time: 49.820454359054565 and batch: 600, loss is 4.353840188980103 and perplexity is 77.7765668862085
At time: 50.712441205978394 and batch: 650, loss is 4.201919078826904 and perplexity is 66.81443024378937
At time: 51.60623240470886 and batch: 700, loss is 4.225231862068176 and perplexity is 68.39035884041074
At time: 52.49926543235779 and batch: 750, loss is 4.310363364219666 and perplexity is 74.46754286543731
At time: 53.38948464393616 and batch: 800, loss is 4.243712368011475 and perplexity is 69.66599819833631
At time: 54.29105353355408 and batch: 850, loss is 4.322093892097473 and perplexity is 75.34623010964158
At time: 55.1828191280365 and batch: 900, loss is 4.260351510047912 and perplexity is 70.83487825008235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4698419440282535 and perplexity of 87.34291685279834
finished 3 epochs...
Completing Train Step...
At time: 57.28565549850464 and batch: 50, loss is 4.357079095840454 and perplexity is 78.02888634129779
At time: 58.18611145019531 and batch: 100, loss is 4.211221561431885 and perplexity is 67.43887023693301
At time: 59.08809423446655 and batch: 150, loss is 4.214727444648743 and perplexity is 67.67571797792502
At time: 59.990867614746094 and batch: 200, loss is 4.094238057136535 and perplexity is 59.99361003515399
At time: 60.88583445549011 and batch: 250, loss is 4.245799856185913 and perplexity is 69.81157703992629
At time: 61.77946662902832 and batch: 300, loss is 4.206905570030212 and perplexity is 67.14843186845415
At time: 62.672646284103394 and batch: 350, loss is 4.205689315795898 and perplexity is 67.06681194921433
At time: 63.573641300201416 and batch: 400, loss is 4.116630125045776 and perplexity is 61.352144458559664
At time: 64.48787188529968 and batch: 450, loss is 4.1434179782867435 and perplexity is 63.017847396933455
At time: 65.39265537261963 and batch: 500, loss is 4.015283560752868 and perplexity is 55.439013502277426
At time: 66.28622961044312 and batch: 550, loss is 4.0909995222091675 and perplexity is 59.79963290403113
At time: 67.18127727508545 and batch: 600, loss is 4.10531099319458 and perplexity is 60.6616069613501
At time: 68.07694721221924 and batch: 650, loss is 3.940308690071106 and perplexity is 51.434476162298594
At time: 68.96897792816162 and batch: 700, loss is 3.9524335479736328 and perplexity is 52.061907951041
At time: 69.87040543556213 and batch: 750, loss is 4.0601743841171265 and perplexity is 57.984421759558245
At time: 70.78340768814087 and batch: 800, loss is 4.001624293327332 and perplexity is 54.68690550687577
At time: 71.67516446113586 and batch: 850, loss is 4.086261744499207 and perplexity is 59.51698562488822
At time: 72.56830596923828 and batch: 900, loss is 4.0267907238006595 and perplexity is 56.080643862344246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393496578686858 and perplexity of 80.92287808324262
finished 4 epochs...
Completing Train Step...
At time: 74.66749477386475 and batch: 50, loss is 4.1306856966018675 and perplexity is 62.22057274597256
At time: 75.55539631843567 and batch: 100, loss is 3.9910095596313475 and perplexity is 54.10948855343654
At time: 76.44321393966675 and batch: 150, loss is 3.9976948738098144 and perplexity is 54.47243935272282
At time: 77.33163237571716 and batch: 200, loss is 3.8767825746536255 and perplexity is 48.26866414532867
At time: 78.22982668876648 and batch: 250, loss is 4.030759935379028 and perplexity is 56.30368215354503
At time: 79.12063193321228 and batch: 300, loss is 3.9952533435821533 and perplexity is 54.33960547042416
At time: 80.011385679245 and batch: 350, loss is 3.993071713447571 and perplexity is 54.22118577058971
At time: 80.93208050727844 and batch: 400, loss is 3.9187034225463866 and perplexity is 50.33513904383432
At time: 81.82690978050232 and batch: 450, loss is 3.945589179992676 and perplexity is 51.706793747662
At time: 82.722820520401 and batch: 500, loss is 3.815759654045105 and perplexity is 45.41124011568253
At time: 83.62177538871765 and batch: 550, loss is 3.896424026489258 and perplexity is 49.22610274903311
At time: 84.52292513847351 and batch: 600, loss is 3.915854926109314 and perplexity is 50.1919635937977
At time: 85.41709113121033 and batch: 650, loss is 3.7504743051528933 and perplexity is 42.54125475200392
At time: 86.31154465675354 and batch: 700, loss is 3.7627082586288454 and perplexity is 43.06489907245493
At time: 87.21210980415344 and batch: 750, loss is 3.8736902475357056 and perplexity is 48.11963219278011
At time: 88.10753417015076 and batch: 800, loss is 3.8183376121520998 and perplexity is 45.5284594185949
At time: 89.00341463088989 and batch: 850, loss is 3.9056028604507445 and perplexity is 49.68002100597633
At time: 89.89910840988159 and batch: 900, loss is 3.8497227954864504 and perplexity is 46.98003834770767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376974445499786 and perplexity of 79.59684413345741
finished 5 epochs...
Completing Train Step...
At time: 92.0025315284729 and batch: 50, loss is 3.9515608072280886 and perplexity is 52.016491224078166
At time: 92.9024772644043 and batch: 100, loss is 3.8173918962478637 and perplexity is 45.48542278384993
At time: 93.79639005661011 and batch: 150, loss is 3.8256880187988282 and perplexity is 45.86434504501005
At time: 94.69064426422119 and batch: 200, loss is 3.7054687690734864 and perplexity is 40.669107268905684
At time: 95.58467674255371 and batch: 250, loss is 3.8553357028961184 and perplexity is 47.244474386307765
At time: 96.47922396659851 and batch: 300, loss is 3.8290038681030274 and perplexity is 46.01667671647647
At time: 97.37225008010864 and batch: 350, loss is 3.8226026105880737 and perplexity is 45.72305290228467
At time: 98.26343011856079 and batch: 400, loss is 3.757045016288757 and perplexity is 42.821701406308456
At time: 99.15702891349792 and batch: 450, loss is 3.781731824874878 and perplexity is 43.89198919143504
At time: 100.05016040802002 and batch: 500, loss is 3.6571668195724487 and perplexity is 38.75139749640669
At time: 100.94852113723755 and batch: 550, loss is 3.7337418270111082 and perplexity is 41.835356321237896
At time: 101.8443353176117 and batch: 600, loss is 3.7600758123397826 and perplexity is 42.951682122852844
At time: 102.74136257171631 and batch: 650, loss is 3.5947901105880735 and perplexity is 36.4080575200883
At time: 103.63866353034973 and batch: 700, loss is 3.6074637031555175 and perplexity is 36.872414728902484
At time: 104.53344488143921 and batch: 750, loss is 3.7192310905456543 and perplexity is 41.232677720930624
At time: 105.42756152153015 and batch: 800, loss is 3.666702165603638 and perplexity is 39.12267278679821
At time: 106.32250380516052 and batch: 850, loss is 3.7510402393341065 and perplexity is 42.56533711605351
At time: 107.2161934375763 and batch: 900, loss is 3.6999537420272826 and perplexity is 40.445433393039714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387417727953767 and perplexity of 80.43245210982796
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.30923080444336 and batch: 50, loss is 3.8249476146698 and perplexity is 45.83039946283888
At time: 110.20817375183105 and batch: 100, loss is 3.6977810287475585 and perplexity is 40.35765245873546
At time: 111.09898543357849 and batch: 150, loss is 3.7066997385025022 and perplexity is 40.71920052196516
At time: 111.99121594429016 and batch: 200, loss is 3.573895616531372 and perplexity is 35.65522202619844
At time: 112.8828125 and batch: 250, loss is 3.706966004371643 and perplexity is 40.73004409885601
At time: 113.77610421180725 and batch: 300, loss is 3.6742621278762817 and perplexity is 39.419559529361194
At time: 114.66981101036072 and batch: 350, loss is 3.6515252208709716 and perplexity is 38.53339318723748
At time: 115.56280827522278 and batch: 400, loss is 3.5844546031951903 and perplexity is 36.03369969395598
At time: 116.4564106464386 and batch: 450, loss is 3.5930575942993164 and perplexity is 36.34503457729442
At time: 117.35094928741455 and batch: 500, loss is 3.4618282604217527 and perplexity is 31.87519944068725
At time: 118.24247741699219 and batch: 550, loss is 3.512860107421875 and perplexity is 33.544070360405485
At time: 119.13578605651855 and batch: 600, loss is 3.532966022491455 and perplexity is 34.22533031791635
At time: 120.02749443054199 and batch: 650, loss is 3.353826093673706 and perplexity is 28.611996664629785
At time: 120.92131352424622 and batch: 700, loss is 3.3464669704437258 and perplexity is 28.402210323862874
At time: 121.81578493118286 and batch: 750, loss is 3.437284665107727 and perplexity is 31.102390000020655
At time: 122.70657324790955 and batch: 800, loss is 3.3682794523239137 and perplexity is 29.028539081323697
At time: 123.59978532791138 and batch: 850, loss is 3.423757066726685 and perplexity is 30.684482375591333
At time: 124.49361443519592 and batch: 900, loss is 3.3597452878952025 and perplexity is 28.781858857655376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377619077081549 and perplexity of 79.64817131477814
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 126.61151218414307 and batch: 50, loss is 3.725984435081482 and perplexity is 41.51207858295637
At time: 127.50441479682922 and batch: 100, loss is 3.6041906213760377 and perplexity is 36.75192559305278
At time: 128.3974359035492 and batch: 150, loss is 3.6137815380096434 and perplexity is 37.10610598991372
At time: 129.29080724716187 and batch: 200, loss is 3.4804523515701296 and perplexity is 32.47440860359729
At time: 130.1825430393219 and batch: 250, loss is 3.6147666931152345 and perplexity is 37.142679271898075
At time: 131.07527327537537 and batch: 300, loss is 3.5799781131744384 and perplexity is 35.87275569772421
At time: 131.9754183292389 and batch: 350, loss is 3.555336637496948 and perplexity is 34.99960016254758
At time: 132.86837458610535 and batch: 400, loss is 3.493358640670776 and perplexity is 32.89624905123212
At time: 133.76223015785217 and batch: 450, loss is 3.5012038469314577 and perplexity is 33.155341899790685
At time: 134.65583300590515 and batch: 500, loss is 3.367900776863098 and perplexity is 29.017548766922783
At time: 135.54867911338806 and batch: 550, loss is 3.4130483961105345 and perplexity is 30.357645478488383
At time: 136.43981266021729 and batch: 600, loss is 3.4356625843048096 and perplexity is 31.051980305628486
At time: 137.33425045013428 and batch: 650, loss is 3.2535613441467284 and perplexity is 25.882351939287112
At time: 138.22944235801697 and batch: 700, loss is 3.2411520004272463 and perplexity is 25.563153555186336
At time: 139.12208580970764 and batch: 750, loss is 3.331277208328247 and perplexity is 27.974047583007
At time: 140.01396465301514 and batch: 800, loss is 3.257050337791443 and perplexity is 25.972813018012143
At time: 140.90545415878296 and batch: 850, loss is 3.309116716384888 and perplexity is 27.360947318931238
At time: 141.79833054542542 and batch: 900, loss is 3.247666387557983 and perplexity is 25.73022542831874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37994844619542 and perplexity of 79.83391755680702
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.9015121459961 and batch: 50, loss is 3.697372765541077 and perplexity is 40.34117927706238
At time: 144.80325484275818 and batch: 100, loss is 3.5753591060638428 and perplexity is 35.70744127225142
At time: 145.6962594985962 and batch: 150, loss is 3.5842738008499144 and perplexity is 36.02718530546845
At time: 146.58969855308533 and batch: 200, loss is 3.4490282201766966 and perplexity is 31.469795732051935
At time: 147.48163771629333 and batch: 250, loss is 3.5850126457214357 and perplexity is 36.05381364246051
At time: 148.37652611732483 and batch: 300, loss is 3.5522680854797364 and perplexity is 34.892366678771296
At time: 149.2703583240509 and batch: 350, loss is 3.526681480407715 and perplexity is 34.010914248314734
At time: 150.16510105133057 and batch: 400, loss is 3.4653735733032227 and perplexity is 31.98840755638083
At time: 151.05939102172852 and batch: 450, loss is 3.4718395042419434 and perplexity is 32.195912523923475
At time: 151.95193433761597 and batch: 500, loss is 3.3387136840820313 and perplexity is 28.182851329334902
At time: 152.84496521949768 and batch: 550, loss is 3.3810513401031494 and perplexity is 29.401666020503008
At time: 153.73836374282837 and batch: 600, loss is 3.405968952178955 and perplexity is 30.143489177623376
At time: 154.6385006904602 and batch: 650, loss is 3.2241404628753663 and perplexity is 25.131963014134033
At time: 155.5345902442932 and batch: 700, loss is 3.21009416103363 and perplexity is 24.781419558811873
At time: 156.42973113059998 and batch: 750, loss is 3.2991368627548217 and perplexity is 27.089247088823296
At time: 157.32556986808777 and batch: 800, loss is 3.2225931882858276 and perplexity is 25.093107034563918
At time: 158.22224378585815 and batch: 850, loss is 3.274736704826355 and perplexity is 26.4362640234174
At time: 159.11664605140686 and batch: 900, loss is 3.2165349531173706 and perplexity is 24.941546648820644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376838579569777 and perplexity of 79.58603036883117
finished 9 epochs...
Completing Train Step...
At time: 161.21594047546387 and batch: 50, loss is 3.6831530570983886 and perplexity is 39.771598701771445
At time: 162.11740565299988 and batch: 100, loss is 3.560250172615051 and perplexity is 35.17199511454958
At time: 163.01056480407715 and batch: 150, loss is 3.5680481386184693 and perplexity is 35.4473372959547
At time: 163.90531253814697 and batch: 200, loss is 3.4331472539901733 and perplexity is 30.973972467105646
At time: 164.79830074310303 and batch: 250, loss is 3.568670811653137 and perplexity is 35.46941627031728
At time: 165.69205975532532 and batch: 300, loss is 3.53601402759552 and perplexity is 34.32980844345724
At time: 166.585186958313 and batch: 350, loss is 3.5111752414703368 and perplexity is 33.487600683655785
At time: 167.47800087928772 and batch: 400, loss is 3.4500822401046753 and perplexity is 31.502983010837063
At time: 168.37195420265198 and batch: 450, loss is 3.45815447807312 and perplexity is 31.758311737080316
At time: 169.26742100715637 and batch: 500, loss is 3.326447720527649 and perplexity is 27.839272969603435
At time: 170.1624734401703 and batch: 550, loss is 3.369795546531677 and perplexity is 29.072582459803932
At time: 171.05421423912048 and batch: 600, loss is 3.39663321018219 and perplexity is 29.863386855666093
At time: 171.94862914085388 and batch: 650, loss is 3.2159699010849 and perplexity is 24.9274573581598
At time: 172.8447539806366 and batch: 700, loss is 3.2036515951156614 and perplexity is 24.62227682399959
At time: 173.73740601539612 and batch: 750, loss is 3.2947932147979735 and perplexity is 26.971836116620192
At time: 174.6359932422638 and batch: 800, loss is 3.2206499576568604 and perplexity is 25.044392687333573
At time: 175.53797030448914 and batch: 850, loss is 3.2756425380706786 and perplexity is 26.46022171942611
At time: 176.439847946167 and batch: 900, loss is 3.21980993270874 and perplexity is 25.023363606378595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377264153467466 and perplexity of 79.61990731403729
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.5461344718933 and batch: 50, loss is 3.676553177833557 and perplexity is 39.50997524344703
At time: 179.44853520393372 and batch: 100, loss is 3.5566043281555175 and perplexity is 35.0439969634866
At time: 180.35386323928833 and batch: 150, loss is 3.565957498550415 and perplexity is 35.3733070845252
At time: 181.2520763874054 and batch: 200, loss is 3.429647331237793 and perplexity is 30.865755442216866
At time: 182.14408326148987 and batch: 250, loss is 3.5635980367660522 and perplexity is 35.28994350353867
At time: 183.03473353385925 and batch: 300, loss is 3.5314687299728393 and perplexity is 34.17412333236905
At time: 183.92629432678223 and batch: 350, loss is 3.506787738800049 and perplexity is 33.34099559652146
At time: 184.8159215450287 and batch: 400, loss is 3.445527682304382 and perplexity is 31.359827107220003
At time: 185.7077760696411 and batch: 450, loss is 3.4524715995788573 and perplexity is 31.578344959298086
At time: 186.59977793693542 and batch: 500, loss is 3.318480858802795 and perplexity is 27.618362481437995
At time: 187.49148273468018 and batch: 550, loss is 3.360980110168457 and perplexity is 28.817421290161594
At time: 188.38151931762695 and batch: 600, loss is 3.387730808258057 and perplexity is 29.598710857143317
At time: 189.27623653411865 and batch: 650, loss is 3.2072147512435913 and perplexity is 24.710166329576126
At time: 190.16981625556946 and batch: 700, loss is 3.1950258445739745 and perplexity is 24.4108045709872
At time: 191.06308460235596 and batch: 750, loss is 3.2851213216781616 and perplexity is 26.7122248902579
At time: 191.95732355117798 and batch: 800, loss is 3.2097929763793944 and perplexity is 24.77395689940624
At time: 192.85348343849182 and batch: 850, loss is 3.2642925500869753 and perplexity is 26.161596423416263
At time: 193.74682712554932 and batch: 900, loss is 3.2096414518356324 and perplexity is 24.770203321276664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372736682630565 and perplexity of 79.26024529963915
finished 11 epochs...
Completing Train Step...
At time: 195.84669518470764 and batch: 50, loss is 3.6711200046539307 and perplexity is 39.29589280573735
At time: 196.7452747821808 and batch: 100, loss is 3.5511617136001585 and perplexity is 34.853784092745705
At time: 197.63842344284058 and batch: 150, loss is 3.560718412399292 and perplexity is 35.18846789825872
At time: 198.55031514167786 and batch: 200, loss is 3.4248228311538695 and perplexity is 30.7172022381095
At time: 199.44515442848206 and batch: 250, loss is 3.559218711853027 and perplexity is 35.13573528517722
At time: 200.33750796318054 and batch: 300, loss is 3.526970090866089 and perplexity is 34.02073157048844
At time: 201.23035550117493 and batch: 350, loss is 3.502207956314087 and perplexity is 33.18865020947112
At time: 202.12565350532532 and batch: 400, loss is 3.44144570350647 and perplexity is 31.23207787008648
At time: 203.01812195777893 and batch: 450, loss is 3.44872266292572 and perplexity is 31.460181376722055
At time: 203.91041040420532 and batch: 500, loss is 3.3154590702056885 and perplexity is 27.535031596189523
At time: 204.80463862419128 and batch: 550, loss is 3.358313865661621 and perplexity is 28.740689337475576
At time: 205.69676113128662 and batch: 600, loss is 3.385728664398193 and perplexity is 29.539509264769332
At time: 206.58938789367676 and batch: 650, loss is 3.2055742645263674 and perplexity is 24.66966286171614
At time: 207.48211193084717 and batch: 700, loss is 3.1936667919158936 and perplexity is 24.3776515356124
At time: 208.37612581253052 and batch: 750, loss is 3.2845535182952883 and perplexity is 26.69706190380602
At time: 209.2694969177246 and batch: 800, loss is 3.209914517402649 and perplexity is 24.776968134468447
At time: 210.16373801231384 and batch: 850, loss is 3.2652153539657593 and perplexity is 26.185749588674263
At time: 211.05865049362183 and batch: 900, loss is 3.211596670150757 and perplexity is 24.818681854093445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371752177199272 and perplexity of 79.18225155658747
finished 12 epochs...
Completing Train Step...
At time: 213.16154599189758 and batch: 50, loss is 3.6680110597610476 and perplexity is 39.173913751815455
At time: 214.06337332725525 and batch: 100, loss is 3.5478566169738768 and perplexity is 34.73877912448354
At time: 214.9603250026703 and batch: 150, loss is 3.557282814979553 and perplexity is 35.06778192166579
At time: 215.85519528388977 and batch: 200, loss is 3.421504192352295 and perplexity is 30.615431901782582
At time: 216.74927592277527 and batch: 250, loss is 3.555870866775513 and perplexity is 35.018302969041066
At time: 217.64049696922302 and batch: 300, loss is 3.5236314392089843 and perplexity is 33.90733759547571
At time: 218.53393959999084 and batch: 350, loss is 3.498928847312927 and perplexity is 33.07999924436801
At time: 219.4240758419037 and batch: 400, loss is 3.438276824951172 and perplexity is 31.133263855781145
At time: 220.31527137756348 and batch: 450, loss is 3.4458093547821047 and perplexity is 31.36866155157375
At time: 221.23186588287354 and batch: 500, loss is 3.3130212736129763 and perplexity is 27.467988541851067
At time: 222.13468503952026 and batch: 550, loss is 3.356080470085144 and perplexity is 28.67657163579191
At time: 223.03946375846863 and batch: 600, loss is 3.38391499042511 and perplexity is 29.485982780106777
At time: 223.93928742408752 and batch: 650, loss is 3.204030895233154 and perplexity is 24.631617827901824
At time: 224.83583164215088 and batch: 700, loss is 3.192434449195862 and perplexity is 24.347628417396198
At time: 225.7335340976715 and batch: 750, loss is 3.28382493019104 and perplexity is 26.677617826306445
At time: 226.62986135482788 and batch: 800, loss is 3.2096509552001953 and perplexity is 24.770438722667674
At time: 227.5256028175354 and batch: 850, loss is 3.2655736780166627 and perplexity is 26.19513425381802
At time: 228.42112636566162 and batch: 900, loss is 3.212528657913208 and perplexity is 24.841823343979264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371630524935788 and perplexity of 79.17261944235398
finished 13 epochs...
Completing Train Step...
At time: 230.52974915504456 and batch: 50, loss is 3.66542742729187 and perplexity is 39.072833389781444
At time: 231.42251992225647 and batch: 100, loss is 3.5451051330566408 and perplexity is 34.64332730970418
At time: 232.31817483901978 and batch: 150, loss is 3.5543765592575074 and perplexity is 34.966013933326145
At time: 233.21778845787048 and batch: 200, loss is 3.4186512088775634 and perplexity is 30.528211059467182
At time: 234.11150431632996 and batch: 250, loss is 3.5529522562026976 and perplexity is 34.91624718274706
At time: 235.00261425971985 and batch: 300, loss is 3.520737986564636 and perplexity is 33.80937012032149
At time: 235.89636993408203 and batch: 350, loss is 3.4961058950424193 and perplexity is 32.98674766969223
At time: 236.79111862182617 and batch: 400, loss is 3.435518054962158 and perplexity is 31.047492707629488
At time: 237.68452715873718 and batch: 450, loss is 3.4432405614852906 and perplexity is 31.28818535166466
At time: 238.57896780967712 and batch: 500, loss is 3.3108071947097777 and perplexity is 27.407239524283057
At time: 239.48436450958252 and batch: 550, loss is 3.3539987087249754 and perplexity is 28.616935952186395
At time: 240.37711429595947 and batch: 600, loss is 3.3821507596969607 and perplexity is 29.43400856397188
At time: 241.27181339263916 and batch: 650, loss is 3.2025075483322145 and perplexity is 24.594123894602763
At time: 242.16608834266663 and batch: 700, loss is 3.191212635040283 and perplexity is 24.3178983063748
At time: 243.06647610664368 and batch: 750, loss is 3.282980275154114 and perplexity is 26.65509395582572
At time: 243.9607651233673 and batch: 800, loss is 3.2091848850250244 and perplexity is 24.758896649870017
At time: 244.85449075698853 and batch: 850, loss is 3.265620632171631 and perplexity is 26.196364253087747
At time: 245.7478539943695 and batch: 900, loss is 3.212976822853088 and perplexity is 24.85295907338009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371811122110445 and perplexity of 79.18691908493416
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 247.85276436805725 and batch: 50, loss is 3.6636317110061647 and perplexity is 39.002732625913644
At time: 248.75215601921082 and batch: 100, loss is 3.5440410709381105 and perplexity is 34.606484262575705
At time: 249.64519476890564 and batch: 150, loss is 3.5541268587112427 and perplexity is 34.957283990527394
At time: 250.5387237071991 and batch: 200, loss is 3.4183199310302737 and perplexity is 30.518099414400275
At time: 251.43042993545532 and batch: 250, loss is 3.552037363052368 and perplexity is 34.884317155872985
At time: 252.32467341423035 and batch: 300, loss is 3.5198998641967774 and perplexity is 33.78104560234451
At time: 253.23034358024597 and batch: 350, loss is 3.495015387535095 and perplexity is 32.95079498061782
At time: 254.12415671348572 and batch: 400, loss is 3.4345045614242555 and perplexity is 31.01604221453007
At time: 255.01655650138855 and batch: 450, loss is 3.4423611783981323 and perplexity is 31.260683144898596
At time: 255.90861177444458 and batch: 500, loss is 3.3090257596969606 and perplexity is 27.358458770961313
At time: 256.8004159927368 and batch: 550, loss is 3.3516034078598023 and perplexity is 28.548471809585745
At time: 257.6926157474518 and batch: 600, loss is 3.379793157577515 and perplexity is 29.36469662009521
At time: 258.58709168434143 and batch: 650, loss is 3.1996539354324343 and perplexity is 24.52404182650226
At time: 259.4890031814575 and batch: 700, loss is 3.188504066467285 and perplexity is 24.252120733008702
At time: 260.38150787353516 and batch: 750, loss is 3.279860744476318 and perplexity is 26.57207213439746
At time: 261.2730550765991 and batch: 800, loss is 3.205793704986572 and perplexity is 24.67507697790396
At time: 262.1660532951355 and batch: 850, loss is 3.2620891666412355 and perplexity is 26.104015854274852
At time: 263.0582239627838 and batch: 900, loss is 3.2092309045791625 and perplexity is 24.760036069472388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370431560359589 and perplexity of 79.07775115945758
finished 15 epochs...
Completing Train Step...
At time: 265.15676617622375 and batch: 50, loss is 3.662612490653992 and perplexity is 38.96300049836698
At time: 266.0554437637329 and batch: 100, loss is 3.542835931777954 and perplexity is 34.56480375364689
At time: 266.9473354816437 and batch: 150, loss is 3.552871642112732 and perplexity is 34.91343255470627
At time: 267.83956575393677 and batch: 200, loss is 3.417166175842285 and perplexity is 30.482909303163677
At time: 268.732134103775 and batch: 250, loss is 3.551106781959534 and perplexity is 34.851869569787944
At time: 269.6249055862427 and batch: 300, loss is 3.518869409561157 and perplexity is 33.74625369612661
At time: 270.52054262161255 and batch: 350, loss is 3.4940869188308716 and perplexity is 32.920215396989
At time: 271.4150125980377 and batch: 400, loss is 3.4336628913879395 and perplexity is 30.989947924084632
At time: 272.3085103034973 and batch: 450, loss is 3.4415044593811035 and perplexity is 31.233912992049937
At time: 273.2021243572235 and batch: 500, loss is 3.3083334493637087 and perplexity is 27.33952478210664
At time: 274.09472823143005 and batch: 550, loss is 3.3510332775115965 and perplexity is 28.532200098351158
At time: 274.9869689941406 and batch: 600, loss is 3.3793068742752075 and perplexity is 29.350420529845014
At time: 275.8805241584778 and batch: 650, loss is 3.1993704080581664 and perplexity is 24.517089574940144
At time: 276.77245116233826 and batch: 700, loss is 3.188270297050476 and perplexity is 24.246451991503577
At time: 277.66541504859924 and batch: 750, loss is 3.279820261001587 and perplexity is 26.570996426361
At time: 278.5595164299011 and batch: 800, loss is 3.205890235900879 and perplexity is 24.677459000612792
At time: 279.4532206058502 and batch: 850, loss is 3.2623226165771486 and perplexity is 26.110110546478182
At time: 280.35344982147217 and batch: 900, loss is 3.209679822921753 and perplexity is 24.771153799116846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369699138484589 and perplexity of 79.01985408981093
finished 16 epochs...
Completing Train Step...
At time: 282.4785261154175 and batch: 50, loss is 3.6617701625823975 and perplexity is 38.930194687860904
At time: 283.375141620636 and batch: 100, loss is 3.5418882751464844 and perplexity is 34.532063703767186
At time: 284.2706241607666 and batch: 150, loss is 3.5518824243545533 and perplexity is 34.87891264389353
At time: 285.16630363464355 and batch: 200, loss is 3.4162401580810546 and perplexity is 30.454694653384575
At time: 286.06238770484924 and batch: 250, loss is 3.5502840423583986 and perplexity is 34.82320734890815
At time: 286.95740509033203 and batch: 300, loss is 3.5180057430267335 and perplexity is 33.71712076852531
At time: 287.8604531288147 and batch: 350, loss is 3.4932767295837404 and perplexity is 32.893554594067396
At time: 288.75569438934326 and batch: 400, loss is 3.432903847694397 and perplexity is 30.96643412467917
At time: 289.65257573127747 and batch: 450, loss is 3.4407670879364014 and perplexity is 31.210890485615845
At time: 290.5492904186249 and batch: 500, loss is 3.307727031707764 and perplexity is 27.32295063750851
At time: 291.4433784484863 and batch: 550, loss is 3.3505115604400633 and perplexity is 28.51731824486751
At time: 292.33758568763733 and batch: 600, loss is 3.3788697385787962 and perplexity is 29.33759321716913
At time: 293.2299916744232 and batch: 650, loss is 3.199055790901184 and perplexity is 24.509377291192603
At time: 294.13087701797485 and batch: 700, loss is 3.1880198431015017 and perplexity is 24.240380132245996
At time: 295.0230438709259 and batch: 750, loss is 3.2797250986099242 and perplexity is 26.568467987100277
At time: 295.91524958610535 and batch: 800, loss is 3.205912985801697 and perplexity is 24.67802041674355
At time: 296.80955266952515 and batch: 850, loss is 3.2624760818481446 and perplexity is 26.114117849151963
At time: 297.70286297798157 and batch: 900, loss is 3.2100101470947267 and perplexity is 24.779337661598532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369298647527826 and perplexity of 78.9882136891133
finished 17 epochs...
Completing Train Step...
At time: 299.8041355609894 and batch: 50, loss is 3.6610231590270996 and perplexity is 38.90112455311943
At time: 300.70365715026855 and batch: 100, loss is 3.541071472167969 and perplexity is 34.50386932747245
At time: 301.59779691696167 and batch: 150, loss is 3.5510285568237303 and perplexity is 34.84914338418734
At time: 302.4925277233124 and batch: 200, loss is 3.4154295921325684 and perplexity is 30.43001911685285
At time: 303.3863890171051 and batch: 250, loss is 3.5495196533203126 and perplexity is 34.796599041784376
At time: 304.27885270118713 and batch: 300, loss is 3.517225980758667 and perplexity is 33.69083967779527
At time: 305.1741065979004 and batch: 350, loss is 3.49252911567688 and perplexity is 32.868972105456955
At time: 306.0684154033661 and batch: 400, loss is 3.432192268371582 and perplexity is 30.944406888447105
At time: 306.9632296562195 and batch: 450, loss is 3.4400901889801023 and perplexity is 31.18977101510114
At time: 307.8569378852844 and batch: 500, loss is 3.3071618938446044 and perplexity is 27.30751376596117
At time: 308.75130915641785 and batch: 550, loss is 3.350009546279907 and perplexity is 28.50300574013951
At time: 309.64631366729736 and batch: 600, loss is 3.378449821472168 and perplexity is 29.325276446102354
At time: 310.5467004776001 and batch: 650, loss is 3.198723330497742 and perplexity is 24.501230248089914
At time: 311.44140696525574 and batch: 700, loss is 3.1877555894851684 and perplexity is 24.23397537041276
At time: 312.33517479896545 and batch: 750, loss is 3.2795901489257813 and perplexity is 26.5648828226511
At time: 313.22942090034485 and batch: 800, loss is 3.205884127616882 and perplexity is 24.677308264145267
At time: 314.1242871284485 and batch: 850, loss is 3.2625726747512815 and perplexity is 26.11664040943662
At time: 315.01858735084534 and batch: 900, loss is 3.210254406929016 and perplexity is 24.785390997772907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369087532775043 and perplexity of 78.9715398720139
finished 18 epochs...
Completing Train Step...
At time: 317.1155161857605 and batch: 50, loss is 3.6603348779678346 and perplexity is 38.874358858122505
At time: 318.01540660858154 and batch: 100, loss is 3.540330319404602 and perplexity is 34.47830616364847
At time: 318.9154431819916 and batch: 150, loss is 3.5502523088455202 and perplexity is 34.82210230374285
At time: 319.80811834335327 and batch: 200, loss is 3.414685435295105 and perplexity is 30.40738283357999
At time: 320.7007360458374 and batch: 250, loss is 3.548793354034424 and perplexity is 34.77133547231577
At time: 321.591281414032 and batch: 300, loss is 3.51649582862854 and perplexity is 33.666249217919564
At time: 322.484112739563 and batch: 350, loss is 3.4918217039108277 and perplexity is 32.8457284302455
At time: 323.375501871109 and batch: 400, loss is 3.4315129566192626 and perplexity is 30.923393127434732
At time: 324.2694990634918 and batch: 450, loss is 3.439449501037598 and perplexity is 31.169794504920223
At time: 325.1641743183136 and batch: 500, loss is 3.306620507240295 and perplexity is 27.292733844997898
At time: 326.0571086406708 and batch: 550, loss is 3.3495177125930784 and perplexity is 28.488990448619415
At time: 326.9499862194061 and batch: 600, loss is 3.3780363607406616 and perplexity is 29.31315410208063
At time: 327.84020709991455 and batch: 650, loss is 3.198381128311157 and perplexity is 24.492847307937037
At time: 328.7285580635071 and batch: 700, loss is 3.18748206615448 and perplexity is 24.22734771920356
At time: 329.6174690723419 and batch: 750, loss is 3.279428062438965 and perplexity is 26.560577363059522
At time: 330.5056140422821 and batch: 800, loss is 3.2058193731307982 and perplexity is 24.67571034946732
At time: 331.396835565567 and batch: 850, loss is 3.2626284694671632 and perplexity is 26.118097620620013
At time: 332.29266929626465 and batch: 900, loss is 3.21043607711792 and perplexity is 24.789894173471534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368984692717252 and perplexity of 78.96341885187984
finished 19 epochs...
Completing Train Step...
At time: 334.3919951915741 and batch: 50, loss is 3.6596856451034547 and perplexity is 38.8491285378329
At time: 335.2847924232483 and batch: 100, loss is 3.539637484550476 and perplexity is 34.45442666465689
At time: 336.1770079135895 and batch: 150, loss is 3.5495251703262327 and perplexity is 34.796791015356845
At time: 337.06921315193176 and batch: 200, loss is 3.4139832973480226 and perplexity is 30.386040149851972
At time: 337.9616780281067 and batch: 250, loss is 3.548094644546509 and perplexity is 34.74704889593251
At time: 338.85563945770264 and batch: 300, loss is 3.515798578262329 and perplexity is 33.642783594981175
At time: 339.74944257736206 and batch: 350, loss is 3.4911427688598633 and perplexity is 32.823435882392126
At time: 340.6422257423401 and batch: 400, loss is 3.43085832118988 and perplexity is 30.903156203323253
At time: 341.5355348587036 and batch: 450, loss is 3.4388334560394287 and perplexity is 31.150598422353927
At time: 342.42842721939087 and batch: 500, loss is 3.3060948514938353 and perplexity is 27.278391032636804
At time: 343.3190927505493 and batch: 550, loss is 3.3490326690673826 and perplexity is 28.47517539896975
At time: 344.2197744846344 and batch: 600, loss is 3.3776256370544435 and perplexity is 29.301116967510502
At time: 345.1183638572693 and batch: 650, loss is 3.1980336570739745 and perplexity is 24.48433822639698
At time: 346.01090717315674 and batch: 700, loss is 3.18720223903656 and perplexity is 24.220569198767606
At time: 346.9051651954651 and batch: 750, loss is 3.2792471122741698 and perplexity is 26.555771657018386
At time: 347.7976734638214 and batch: 800, loss is 3.205728974342346 and perplexity is 24.673479795968717
At time: 348.69180250167847 and batch: 850, loss is 3.2626540851593018 and perplexity is 26.118766662336856
At time: 349.5842308998108 and batch: 900, loss is 3.210571303367615 and perplexity is 24.79324664455688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368947068305864 and perplexity of 78.96044795561386
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0dfb3dab70>
ELAPSED
1433.086317539215


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -79.0822141661567}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9992209419307283, 'rnn_dropout': 0.9039337904170541, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.59802808559881}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9437929462699219, 'rnn_dropout': 0.010083005405271739, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.65640919904529}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9722729760825943, 'rnn_dropout': 0.8741438177807345, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.96044795561386}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.3384783195436849, 'rnn_dropout': 0.06077393801559883, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.20709468314841}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.09607509383301283, 'rnn_dropout': 0.9439768596093093, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.21621695243073}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.43686500607346057, 'rnn_dropout': 0.136818278915539, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -79.0822141661567}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9992209419307283, 'rnn_dropout': 0.9039337904170541, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.59802808559881}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9437929462699219, 'rnn_dropout': 0.010083005405271739, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -77.65640919904529}, {'params': {'seq_len': 35, 'wordvec_source': 'None', 'wordvec_dim': 300, 'dropout': 0.9722729760825943, 'rnn_dropout': 0.8741438177807345, 'batch_size': 32, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'num_layers': 3, 'data': 'ptb'}, 'best_accuracy': -78.96044795561386}]
