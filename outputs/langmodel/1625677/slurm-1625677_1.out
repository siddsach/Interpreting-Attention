TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0891661643981934 and batch: 50, loss is 6.685304317474365 and perplexity is 800.5542637617103
At time: 1.7255291938781738 and batch: 100, loss is 5.901109523773194 and perplexity is 365.4427103855642
At time: 2.3621721267700195 and batch: 150, loss is 5.7886056709289555 and perplexity is 326.55737834354846
At time: 2.9992215633392334 and batch: 200, loss is 5.626357650756836 and perplexity is 277.64897910204564
At time: 3.6426541805267334 and batch: 250, loss is 5.679228172302246 and perplexity is 292.72341067367375
At time: 4.290743350982666 and batch: 300, loss is 5.60155237197876 and perplexity is 270.8465358167573
At time: 4.94106912612915 and batch: 350, loss is 5.579252014160156 and perplexity is 264.8734101115738
At time: 5.58044695854187 and batch: 400, loss is 5.438981094360352 and perplexity is 230.207504198616
At time: 6.228775262832642 and batch: 450, loss is 5.439607601165772 and perplexity is 230.351775955555
At time: 6.86898946762085 and batch: 500, loss is 5.389300212860108 and perplexity is 219.0500435033782
At time: 7.5111963748931885 and batch: 550, loss is 5.452605428695679 and perplexity is 233.3653914101457
At time: 8.16473126411438 and batch: 600, loss is 5.387293729782105 and perplexity is 218.61096394801194
At time: 8.809733390808105 and batch: 650, loss is 5.2807180786132815 and perplexity is 196.51093499947802
At time: 9.450081586837769 and batch: 700, loss is 5.382050666809082 and perplexity is 217.46777242883434
At time: 10.109018564224243 and batch: 750, loss is 5.363602933883667 and perplexity is 213.4927627320678
At time: 10.76309061050415 and batch: 800, loss is 5.330655317306519 and perplexity is 206.5733008696213
At time: 11.41780710220337 and batch: 850, loss is 5.3677946472167966 and perplexity is 214.3895413986178
At time: 12.087131023406982 and batch: 900, loss is 5.289458141326905 and perplexity is 198.23598041670115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.127769992776113 and perplexity of 168.64062859380567
finished 1 epochs...
Completing Train Step...
At time: 13.786754846572876 and batch: 50, loss is 5.064225168228149 and perplexity is 158.25777140582585
At time: 14.426597833633423 and batch: 100, loss is 4.906807851791382 and perplexity is 135.20712362994038
At time: 15.060271263122559 and batch: 150, loss is 4.867749357223511 and perplexity is 130.0279408820655
At time: 15.703694343566895 and batch: 200, loss is 4.750679197311402 and perplexity is 115.6628157285503
At time: 16.339683532714844 and batch: 250, loss is 4.84808874130249 and perplexity is 127.49647807840496
At time: 16.977861642837524 and batch: 300, loss is 4.799815282821656 and perplexity is 121.48797453013795
At time: 17.61423110961914 and batch: 350, loss is 4.775112419128418 and perplexity is 118.52363819589984
At time: 18.244016647338867 and batch: 400, loss is 4.664421129226684 and perplexity is 106.10414685171229
At time: 18.889952898025513 and batch: 450, loss is 4.6734018230438235 and perplexity is 107.0613273465219
At time: 19.53944754600525 and batch: 500, loss is 4.572305974960327 and perplexity is 96.76699495760904
At time: 20.19332766532898 and batch: 550, loss is 4.654673538208008 and perplexity is 105.07491145510295
At time: 20.847490787506104 and batch: 600, loss is 4.619384260177612 and perplexity is 101.43155744827718
At time: 21.524379014968872 and batch: 650, loss is 4.479889459609986 and perplexity is 88.22491971957884
At time: 22.161115646362305 and batch: 700, loss is 4.513790836334229 and perplexity is 91.26714236327958
At time: 22.80767846107483 and batch: 750, loss is 4.578102054595948 and perplexity is 97.3294927326958
At time: 23.444304943084717 and batch: 800, loss is 4.519983158111573 and perplexity is 91.83405130628644
At time: 24.08642053604126 and batch: 850, loss is 4.576459426879882 and perplexity is 97.16974784694989
At time: 24.717902898788452 and batch: 900, loss is 4.525808877944947 and perplexity is 92.37061216946385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.647042470435574 and perplexity of 104.27612934112929
finished 2 epochs...
Completing Train Step...
At time: 26.342522621154785 and batch: 50, loss is 4.5212459373474125 and perplexity is 91.95009069006146
At time: 26.985004425048828 and batch: 100, loss is 4.373299918174744 and perplexity is 79.30490006142624
At time: 27.62405276298523 and batch: 150, loss is 4.370664138793945 and perplexity is 79.09614507794609
At time: 28.26635456085205 and batch: 200, loss is 4.271080803871155 and perplexity is 71.59897826845626
At time: 28.908945560455322 and batch: 250, loss is 4.415638599395752 and perplexity is 82.73465834254708
At time: 29.559975147247314 and batch: 300, loss is 4.390279331207275 and perplexity is 80.66294751238054
At time: 30.192864894866943 and batch: 350, loss is 4.367877798080444 and perplexity is 78.8760630228172
At time: 30.835413455963135 and batch: 400, loss is 4.296892790794373 and perplexity is 73.47114843046148
At time: 31.478118181228638 and batch: 450, loss is 4.316379680633545 and perplexity is 74.91691358865545
At time: 32.12500596046448 and batch: 500, loss is 4.206734461784363 and perplexity is 67.1369432009965
At time: 32.76712131500244 and batch: 550, loss is 4.296027936935425 and perplexity is 73.4076340934887
At time: 33.41264581680298 and batch: 600, loss is 4.293019828796386 and perplexity is 73.1871477814991
At time: 34.06085824966431 and batch: 650, loss is 4.149729413986206 and perplexity is 63.41683826685117
At time: 34.70902943611145 and batch: 700, loss is 4.1629792499542235 and perplexity is 64.2626923104845
At time: 35.36032009124756 and batch: 750, loss is 4.265787291526794 and perplexity is 71.22096957268722
At time: 36.00634264945984 and batch: 800, loss is 4.2165802526474 and perplexity is 67.80122432318542
At time: 36.64469313621521 and batch: 850, loss is 4.2803461408615116 and perplexity is 72.26544970365468
At time: 37.31188225746155 and batch: 900, loss is 4.239073104858399 and perplexity is 69.34354784423074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.542878451412672 and perplexity of 93.96087304485532
finished 3 epochs...
Completing Train Step...
At time: 38.94944167137146 and batch: 50, loss is 4.271816806793213 and perplexity is 71.6516947230308
At time: 39.6014769077301 and batch: 100, loss is 4.129694147109985 and perplexity is 62.158908545287304
At time: 40.24382448196411 and batch: 150, loss is 4.132262372970581 and perplexity is 62.31875183065166
At time: 40.89965319633484 and batch: 200, loss is 4.03376727104187 and perplexity is 56.47326108741551
At time: 41.54406261444092 and batch: 250, loss is 4.1881786680221555 and perplexity is 65.90265098057193
At time: 42.198962450027466 and batch: 300, loss is 4.167297534942627 and perplexity is 64.54079696541663
At time: 42.85812568664551 and batch: 350, loss is 4.146318774223328 and perplexity is 63.20091470467591
At time: 43.50594711303711 and batch: 400, loss is 4.085884599685669 and perplexity is 59.49454333470509
At time: 44.153432846069336 and batch: 450, loss is 4.109229640960693 and perplexity is 60.89978479469112
At time: 44.80116415023804 and batch: 500, loss is 3.9968992948532103 and perplexity is 54.42911946074106
At time: 45.44959473609924 and batch: 550, loss is 4.084660558700562 and perplexity is 59.42176412672683
At time: 46.086002588272095 and batch: 600, loss is 4.09506724357605 and perplexity is 60.043376553062174
At time: 46.727823972702026 and batch: 650, loss is 3.950532536506653 and perplexity is 51.96303167927339
At time: 47.38009762763977 and batch: 700, loss is 3.958651056289673 and perplexity is 52.38661167471021
At time: 48.03144812583923 and batch: 750, loss is 4.06748477935791 and perplexity is 58.40986398112385
At time: 48.68391442298889 and batch: 800, loss is 4.024473395347595 and perplexity is 55.9508370512529
At time: 49.33707928657532 and batch: 850, loss is 4.0888966369628905 and perplexity is 59.67401326639793
At time: 49.98698425292969 and batch: 900, loss is 4.051924037933349 and perplexity is 57.507998238702704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.502185978301584 and perplexity of 90.2141220265986
finished 4 epochs...
Completing Train Step...
At time: 51.64049696922302 and batch: 50, loss is 4.094105925559997 and perplexity is 59.98568350856161
At time: 52.29725646972656 and batch: 100, loss is 3.961274251937866 and perplexity is 52.5242124043922
At time: 52.940890073776245 and batch: 150, loss is 3.968334641456604 and perplexity is 52.89636603195884
At time: 53.582624435424805 and batch: 200, loss is 3.8695552206039427 and perplexity is 47.9210670367799
At time: 54.2537739276886 and batch: 250, loss is 4.022406115531921 and perplexity is 55.83529048983429
At time: 54.911834955215454 and batch: 300, loss is 4.007332911491394 and perplexity is 54.99998494424285
At time: 55.56612801551819 and batch: 350, loss is 3.9836329555511476 and perplexity is 53.71181283084313
At time: 56.226524353027344 and batch: 400, loss is 3.931654119491577 and perplexity is 50.991253575215055
At time: 56.879897117614746 and batch: 450, loss is 3.957491092681885 and perplexity is 52.32588034150998
At time: 57.53449511528015 and batch: 500, loss is 3.8434491968154907 and perplexity is 46.6862270323591
At time: 58.18538546562195 and batch: 550, loss is 3.92771390914917 and perplexity is 50.79073261731001
At time: 58.83651781082153 and batch: 600, loss is 3.94171142578125 and perplexity is 51.50667576538602
At time: 59.489574909210205 and batch: 650, loss is 3.7985932874679564 and perplexity is 44.63834698439465
At time: 60.13386297225952 and batch: 700, loss is 3.804131951332092 and perplexity is 44.88626973037968
At time: 60.78365111351013 and batch: 750, loss is 3.9166729068756103 and perplexity is 50.233036450753445
At time: 61.421361684799194 and batch: 800, loss is 3.8757369709014893 and perplexity is 48.21822062555025
At time: 62.0583553314209 and batch: 850, loss is 3.9425942420959474 and perplexity is 51.552166776216296
At time: 62.695781230926514 and batch: 900, loss is 3.9076450777053835 and perplexity is 49.78158207166796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.492943332619863 and perplexity of 89.38414635523125
finished 5 epochs...
Completing Train Step...
At time: 64.262699842453 and batch: 50, loss is 3.958192672729492 and perplexity is 52.36260401592335
At time: 64.91385650634766 and batch: 100, loss is 3.8275364685058593 and perplexity is 45.94920138234642
At time: 65.55279755592346 and batch: 150, loss is 3.8343145418167115 and perplexity is 46.26170633218958
At time: 66.19960474967957 and batch: 200, loss is 3.736279649734497 and perplexity is 41.94166187441978
At time: 66.83751702308655 and batch: 250, loss is 3.8902621603012086 and perplexity is 48.923710697396494
At time: 67.4701771736145 and batch: 300, loss is 3.879763116836548 and perplexity is 48.412745548613444
At time: 68.10500836372375 and batch: 350, loss is 3.853631181716919 and perplexity is 47.16401377200754
At time: 68.73923397064209 and batch: 400, loss is 3.805372705459595 and perplexity is 44.94199711966076
At time: 69.372891664505 and batch: 450, loss is 3.8300810766220095 and perplexity is 46.06627298061395
At time: 70.00694179534912 and batch: 500, loss is 3.7180182456970217 and perplexity is 41.1826991943872
At time: 70.65500521659851 and batch: 550, loss is 3.8004262542724607 and perplexity is 44.72024262569177
At time: 71.29345870018005 and batch: 600, loss is 3.81887930393219 and perplexity is 45.553128491735976
At time: 71.92764496803284 and batch: 650, loss is 3.675131907463074 and perplexity is 39.45386077265833
At time: 72.56248712539673 and batch: 700, loss is 3.6792496728897093 and perplexity is 39.61665746576092
At time: 73.1958909034729 and batch: 750, loss is 3.7945486307144165 and perplexity is 44.4581648259264
At time: 73.82873821258545 and batch: 800, loss is 3.7545108366012574 and perplexity is 42.713320906253514
At time: 74.46333026885986 and batch: 850, loss is 3.8215900468826294 and perplexity is 45.676778830090385
At time: 75.09724807739258 and batch: 900, loss is 3.7875637435913085 and perplexity is 44.14871156910067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496713298640839 and perplexity of 89.72175754129212
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.66877388954163 and batch: 50, loss is 3.8523808670043946 and perplexity is 47.10508076176902
At time: 77.32780027389526 and batch: 100, loss is 3.720293936729431 and perplexity is 41.2765250124288
At time: 77.96367263793945 and batch: 150, loss is 3.7286164712905885 and perplexity is 41.62148379297936
At time: 78.59961438179016 and batch: 200, loss is 3.6108408212661742 and perplexity is 36.99714772895631
At time: 79.23475575447083 and batch: 250, loss is 3.752017812728882 and perplexity is 42.606968202563706
At time: 79.87405490875244 and batch: 300, loss is 3.7332388257980345 and perplexity is 41.814318377758056
At time: 80.51744341850281 and batch: 350, loss is 3.6885063743591306 and perplexity is 39.98507959323169
At time: 81.1584198474884 and batch: 400, loss is 3.6355430603027346 and perplexity is 37.92244147876536
At time: 81.79985785484314 and batch: 450, loss is 3.6432333850860594 and perplexity is 38.215201637822126
At time: 82.438236951828 and batch: 500, loss is 3.519385952949524 and perplexity is 33.76368960316906
At time: 83.0714499950409 and batch: 550, loss is 3.581898293495178 and perplexity is 35.94170403269876
At time: 83.71075797080994 and batch: 600, loss is 3.594325008392334 and perplexity is 36.39112798987911
At time: 84.35000014305115 and batch: 650, loss is 3.4323042583465577 and perplexity is 30.947872545856004
At time: 84.98263001441956 and batch: 700, loss is 3.4205713510513305 and perplexity is 30.5868858789823
At time: 85.61582517623901 and batch: 750, loss is 3.516857452392578 and perplexity is 33.678425935245265
At time: 86.26338744163513 and batch: 800, loss is 3.455240149497986 and perplexity is 31.665892317377022
At time: 86.89781832695007 and batch: 850, loss is 3.5060557413101194 and perplexity is 33.31659900164552
At time: 87.53172826766968 and batch: 900, loss is 3.4565806341171266 and perplexity is 31.708368421901792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434137213720034 and perplexity of 84.27937841909156
finished 7 epochs...
Completing Train Step...
At time: 89.1608579158783 and batch: 50, loss is 3.7407528448104856 and perplexity is 42.12969535146957
At time: 89.80967497825623 and batch: 100, loss is 3.6088361024856566 and perplexity is 36.92305314629406
At time: 90.44624090194702 and batch: 150, loss is 3.6192343473434447 and perplexity is 37.308991155357795
At time: 91.07919335365295 and batch: 200, loss is 3.507941064834595 and perplexity is 33.379470817732916
At time: 91.72374844551086 and batch: 250, loss is 3.6517587327957153 and perplexity is 38.54239224470025
At time: 92.36375427246094 and batch: 300, loss is 3.639481573104858 and perplexity is 38.07209401077452
At time: 93.0038001537323 and batch: 350, loss is 3.60087749004364 and perplexity is 36.63036312425422
At time: 93.64000701904297 and batch: 400, loss is 3.55327956199646 and perplexity is 34.92767734322324
At time: 94.28871893882751 and batch: 450, loss is 3.568770155906677 and perplexity is 35.472940128034764
At time: 94.93854188919067 and batch: 500, loss is 3.4474644804000856 and perplexity is 31.420623616920043
At time: 95.58283805847168 and batch: 550, loss is 3.5153649473190307 and perplexity is 33.62819820556234
At time: 96.21797633171082 and batch: 600, loss is 3.534858102798462 and perplexity is 34.290148692859766
At time: 96.85476398468018 and batch: 650, loss is 3.3790003871917724 and perplexity is 29.341426383424487
At time: 97.48659181594849 and batch: 700, loss is 3.373566904067993 and perplexity is 29.182432574591942
At time: 98.11832761764526 and batch: 750, loss is 3.4770115566253663 and perplexity is 32.362862835746746
At time: 98.75674533843994 and batch: 800, loss is 3.4238700580596926 and perplexity is 30.687949652040007
At time: 99.39141201972961 and batch: 850, loss is 3.483911061286926 and perplexity is 32.58692262067985
At time: 100.0247311592102 and batch: 900, loss is 3.443028507232666 and perplexity is 31.28155126232222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.440638712007705 and perplexity of 84.82910574280683
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.61533665657043 and batch: 50, loss is 3.695782103538513 and perplexity is 40.277061104737285
At time: 102.26574730873108 and batch: 100, loss is 3.569433903694153 and perplexity is 35.496493029288345
At time: 102.92218613624573 and batch: 150, loss is 3.581597032546997 and perplexity is 35.93087783170029
At time: 103.56565999984741 and batch: 200, loss is 3.466043586730957 and perplexity is 32.009847400665166
At time: 104.20118141174316 and batch: 250, loss is 3.6088428258895875 and perplexity is 36.92330139572926
At time: 104.83863997459412 and batch: 300, loss is 3.5946387672424316 and perplexity is 36.40254781979364
At time: 105.48371315002441 and batch: 350, loss is 3.5502400016784668 and perplexity is 34.82167374494983
At time: 106.12884950637817 and batch: 400, loss is 3.501585807800293 and perplexity is 33.16800836188169
At time: 106.76317119598389 and batch: 450, loss is 3.5096137714385987 and perplexity is 33.43535160205709
At time: 107.39453148841858 and batch: 500, loss is 3.3861478185653686 and perplexity is 29.55189346843795
At time: 108.03863954544067 and batch: 550, loss is 3.4447255897521973 and perplexity is 31.33468370847394
At time: 108.67278385162354 and batch: 600, loss is 3.462218761444092 and perplexity is 31.88764916931377
At time: 109.32275700569153 and batch: 650, loss is 3.300821614265442 and perplexity is 27.13492420526911
At time: 109.95577478408813 and batch: 700, loss is 3.2887076473236085 and perplexity is 26.808195615703262
At time: 110.58793425559998 and batch: 750, loss is 3.384264430999756 and perplexity is 29.496288179330907
At time: 111.22060251235962 and batch: 800, loss is 3.3250903987884524 and perplexity is 27.80151175205646
At time: 111.85399603843689 and batch: 850, loss is 3.379628496170044 and perplexity is 29.359861785886014
At time: 112.49134516716003 and batch: 900, loss is 3.3362310457229616 and perplexity is 28.112970282133524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4306577917647685 and perplexity of 83.98664446724247
finished 9 epochs...
Completing Train Step...
At time: 114.09454607963562 and batch: 50, loss is 3.6632568311691283 and perplexity is 38.98811402814284
At time: 114.74972748756409 and batch: 100, loss is 3.5359488677978517 and perplexity is 34.32757159296223
At time: 115.38396072387695 and batch: 150, loss is 3.5459256601333617 and perplexity is 34.67176476304424
At time: 116.0196602344513 and batch: 200, loss is 3.4336570167541502 and perplexity is 30.989765870024183
At time: 116.66049385070801 and batch: 250, loss is 3.5760780477523806 and perplexity is 35.73312207076523
At time: 117.29424667358398 and batch: 300, loss is 3.563911643028259 and perplexity is 35.301012386359076
At time: 117.9389808177948 and batch: 350, loss is 3.521620345115662 and perplexity is 33.83921527227812
At time: 118.57512998580933 and batch: 400, loss is 3.474329767227173 and perplexity is 32.27618872609946
At time: 119.23077726364136 and batch: 450, loss is 3.486389241218567 and perplexity is 32.66777902533123
At time: 119.87550687789917 and batch: 500, loss is 3.3646104907989502 and perplexity is 28.922229630206434
At time: 120.5074348449707 and batch: 550, loss is 3.425252389907837 and perplexity is 30.730399915614285
At time: 121.14060759544373 and batch: 600, loss is 3.446047329902649 and perplexity is 31.376127400895758
At time: 121.7750608921051 and batch: 650, loss is 3.2876930809020997 and perplexity is 26.781010713386745
At time: 122.41952395439148 and batch: 700, loss is 3.2788638210296632 and perplexity is 26.545595012685062
At time: 123.06171751022339 and batch: 750, loss is 3.3779196977615356 and perplexity is 29.30973454166743
At time: 123.6993670463562 and batch: 800, loss is 3.322492380142212 and perplexity is 27.729376650874094
At time: 124.33810997009277 and batch: 850, loss is 3.3808544445037843 and perplexity is 29.39587753173225
At time: 124.98420214653015 and batch: 900, loss is 3.340503287315369 and perplexity is 28.233332608455775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433012243819563 and perplexity of 84.18461996533733
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.57665157318115 and batch: 50, loss is 3.6481445455551147 and perplexity is 38.4033442465354
At time: 127.22696733474731 and batch: 100, loss is 3.5233056211471556 and perplexity is 33.896291772020696
At time: 127.86568713188171 and batch: 150, loss is 3.5335614490509033 and perplexity is 34.245715056798744
At time: 128.51439428329468 and batch: 200, loss is 3.420844621658325 and perplexity is 30.595245518022317
At time: 129.1573793888092 and batch: 250, loss is 3.5626118564605713 and perplexity is 35.255158411279595
At time: 129.8055136203766 and batch: 300, loss is 3.5510099792480467 and perplexity is 34.84849597760226
At time: 130.45072650909424 and batch: 350, loss is 3.506497006416321 and perplexity is 33.33130369833842
At time: 131.0963010787964 and batch: 400, loss is 3.4598759746551515 and perplexity is 31.813030647882226
At time: 131.74160718917847 and batch: 450, loss is 3.4683515548706056 and perplexity is 32.08381042794448
At time: 132.3802318572998 and batch: 500, loss is 3.3441830587387087 and perplexity is 28.33741620344607
At time: 133.0189974308014 and batch: 550, loss is 3.40249125957489 and perplexity is 30.038841460016872
At time: 133.65810871124268 and batch: 600, loss is 3.423238549232483 and perplexity is 30.668576058887183
At time: 134.29696941375732 and batch: 650, loss is 3.2632956886291504 and perplexity is 26.135529930766115
At time: 134.94828748703003 and batch: 700, loss is 3.251986756324768 and perplexity is 25.84162997169054
At time: 135.58643555641174 and batch: 750, loss is 3.3491399478912354 and perplexity is 28.478230346158192
At time: 136.22530269622803 and batch: 800, loss is 3.2913357830047607 and perplexity is 26.87874385571465
At time: 136.86967158317566 and batch: 850, loss is 3.346726598739624 and perplexity is 28.409585298663647
At time: 137.51359510421753 and batch: 900, loss is 3.307042202949524 and perplexity is 27.304245500790792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432849622752569 and perplexity of 84.17093088571056
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 139.12456631660461 and batch: 50, loss is 3.64076434135437 and perplexity is 38.120963021270356
At time: 139.78142023086548 and batch: 100, loss is 3.5165149116516115 and perplexity is 33.666891677860136
At time: 140.43154335021973 and batch: 150, loss is 3.5268346071243286 and perplexity is 34.01612262670337
At time: 141.07539105415344 and batch: 200, loss is 3.4139765310287475 and perplexity is 30.385834548898394
At time: 141.73133635520935 and batch: 250, loss is 3.5558573389053345 and perplexity is 35.01782924918885
At time: 142.38273930549622 and batch: 300, loss is 3.544456915855408 and perplexity is 34.6208781857743
At time: 143.02346658706665 and batch: 350, loss is 3.5000320148468016 and perplexity is 33.11651216178456
At time: 143.66342878341675 and batch: 400, loss is 3.4534646463394165 and perplexity is 31.609719307983106
At time: 144.30344700813293 and batch: 450, loss is 3.4622268724441527 and perplexity is 31.887907811087047
At time: 144.94335961341858 and batch: 500, loss is 3.337516899108887 and perplexity is 28.149142691382664
At time: 145.5861110687256 and batch: 550, loss is 3.3954823207855225 and perplexity is 29.829037170521016
At time: 146.22878313064575 and batch: 600, loss is 3.416855902671814 and perplexity is 30.473452741383575
At time: 146.87055921554565 and batch: 650, loss is 3.2563789463043213 and perplexity is 25.955380944982785
At time: 147.5189836025238 and batch: 700, loss is 3.2442089557647704 and perplexity is 25.641418539222467
At time: 148.15966629981995 and batch: 750, loss is 3.341619973182678 and perplexity is 28.26487798182493
At time: 148.8001425266266 and batch: 800, loss is 3.283337073326111 and perplexity is 26.664606141487884
At time: 149.44006633758545 and batch: 850, loss is 3.337628927230835 and perplexity is 28.152296363619516
At time: 150.08019256591797 and batch: 900, loss is 3.297926697731018 and perplexity is 27.056484457568853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432661500695634 and perplexity of 84.15509796636559
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.67138051986694 and batch: 50, loss is 3.6387820196151734 and perplexity is 38.045469858144884
At time: 152.30467677116394 and batch: 100, loss is 3.514567141532898 and perplexity is 33.60138013368564
At time: 152.94708585739136 and batch: 150, loss is 3.524839973449707 and perplexity is 33.94834054571567
At time: 153.58175206184387 and batch: 200, loss is 3.4120764780044555 and perplexity is 30.328154666838294
At time: 154.2225112915039 and batch: 250, loss is 3.554086675643921 and perplexity is 34.955879327853445
At time: 154.85739827156067 and batch: 300, loss is 3.5426217031478884 and perplexity is 34.55739977619055
At time: 155.4905481338501 and batch: 350, loss is 3.498183264732361 and perplexity is 33.055344565357984
At time: 156.1240837574005 and batch: 400, loss is 3.4515572357177735 and perplexity is 31.54948405856757
At time: 156.7666802406311 and batch: 450, loss is 3.4605219030380248 and perplexity is 31.833586225323668
At time: 157.41460585594177 and batch: 500, loss is 3.3357208013534545 and perplexity is 28.098629456310896
At time: 158.0619113445282 and batch: 550, loss is 3.393583745956421 and perplexity is 29.772458238036574
At time: 158.69416403770447 and batch: 600, loss is 3.4151851415634153 and perplexity is 30.42258139047566
At time: 159.32608723640442 and batch: 650, loss is 3.254627833366394 and perplexity is 25.90996991312791
At time: 159.96486115455627 and batch: 700, loss is 3.2420609045028685 and perplexity is 25.586398571783324
At time: 160.59847855567932 and batch: 750, loss is 3.339667444229126 and perplexity is 28.209743832224486
At time: 161.23021578788757 and batch: 800, loss is 3.281271653175354 and perplexity is 26.60958936259142
At time: 161.8615005016327 and batch: 850, loss is 3.3353074645996093 and perplexity is 28.087017659980344
At time: 162.4920687675476 and batch: 900, loss is 3.295588283538818 and perplexity is 26.993289107570856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432616351401969 and perplexity of 84.1512985089062
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.0637390613556 and batch: 50, loss is 3.638285255432129 and perplexity is 38.02657492494386
At time: 164.70316863059998 and batch: 100, loss is 3.5141107416152955 and perplexity is 33.586047965627564
At time: 165.33230257034302 and batch: 150, loss is 3.5243285036087038 and perplexity is 33.93098143308414
At time: 165.97518658638 and batch: 200, loss is 3.4115964937210084 and perplexity is 30.313601122268
At time: 166.60951733589172 and batch: 250, loss is 3.5536153745651244 and perplexity is 34.93940846588833
At time: 167.260888338089 and batch: 300, loss is 3.5421543550491332 and perplexity is 34.54125321443358
At time: 167.8978991508484 and batch: 350, loss is 3.4976930046081542 and perplexity is 33.039142819885065
At time: 168.53803610801697 and batch: 400, loss is 3.4510910415649416 and perplexity is 31.534779301476814
At time: 169.17696046829224 and batch: 450, loss is 3.460082287788391 and perplexity is 31.819594771022164
At time: 169.81805658340454 and batch: 500, loss is 3.335259771347046 and perplexity is 28.085678130696845
At time: 170.45585441589355 and batch: 550, loss is 3.393097448348999 and perplexity is 29.757983482633055
At time: 171.0871124267578 and batch: 600, loss is 3.414747366905212 and perplexity is 30.409266070073357
At time: 171.7187156677246 and batch: 650, loss is 3.2541767311096192 and perplexity is 25.898284503082717
At time: 172.363440990448 and batch: 700, loss is 3.2415213060379027 and perplexity is 25.572595914671187
At time: 173.00168585777283 and batch: 750, loss is 3.3391702795028686 and perplexity is 28.195722428416854
At time: 173.6433551311493 and batch: 800, loss is 3.280753769874573 and perplexity is 26.59581226839132
At time: 174.28413033485413 and batch: 850, loss is 3.3347256135940553 and perplexity is 28.07067995402812
At time: 174.9272837638855 and batch: 900, loss is 3.294994258880615 and perplexity is 26.97725918978954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4326063182255995 and perplexity of 84.15045420832203
Annealing...
Model not improving. Stopping early with 83.98664446724247 lossat 13 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -83.98664446724247
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
186.89874005317688


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8758959770202637 and batch: 50, loss is 6.503583164215088 and perplexity is 667.5292197449329
At time: 1.5272858142852783 and batch: 100, loss is 5.6565581321716305 and perplexity is 286.1620137782093
At time: 2.16340708732605 and batch: 150, loss is 5.442388286590576 and perplexity is 230.99320317188403
At time: 2.801504373550415 and batch: 200, loss is 5.240552978515625 and perplexity is 188.77446177042108
At time: 3.4375271797180176 and batch: 250, loss is 5.272646398544311 and perplexity is 194.9311459551736
At time: 4.089523077011108 and batch: 300, loss is 5.200709228515625 and perplexity is 181.40085091939014
At time: 4.738285064697266 and batch: 350, loss is 5.1547244262695315 and perplexity is 173.24805756645577
At time: 5.378450155258179 and batch: 400, loss is 5.003552579879761 and perplexity is 148.94134636370143
At time: 6.018712282180786 and batch: 450, loss is 5.00153263092041 and perplexity is 148.64079609640152
At time: 6.657652378082275 and batch: 500, loss is 4.916535720825196 and perplexity is 136.5288190381565
At time: 7.297528266906738 and batch: 550, loss is 4.987134485244751 and perplexity is 146.51597769060965
At time: 7.945298671722412 and batch: 600, loss is 4.920762634277343 and perplexity is 137.10713592661685
At time: 8.600529193878174 and batch: 650, loss is 4.792993507385254 and perplexity is 120.6620312524999
At time: 9.256854772567749 and batch: 700, loss is 4.853433885574341 and perplexity is 128.17978972065947
At time: 9.898620128631592 and batch: 750, loss is 4.876836585998535 and perplexity is 131.21491953312744
At time: 10.556465148925781 and batch: 800, loss is 4.804713106155395 and perplexity is 122.08446071613153
At time: 11.203439712524414 and batch: 850, loss is 4.862319869995117 and perplexity is 129.3238689421448
At time: 11.846046209335327 and batch: 900, loss is 4.798761510848999 and perplexity is 121.36002133614845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.799359047249572 and perplexity of 121.43256003656974
finished 1 epochs...
Completing Train Step...
At time: 13.44972276687622 and batch: 50, loss is 4.770008325576782 and perplexity is 117.92022371046052
At time: 14.08765459060669 and batch: 100, loss is 4.61683406829834 and perplexity is 101.1732170629563
At time: 14.73038935661316 and batch: 150, loss is 4.589871196746826 and perplexity is 98.4817445759119
At time: 15.377065420150757 and batch: 200, loss is 4.48435495376587 and perplexity is 88.6197685243772
At time: 16.024168252944946 and batch: 250, loss is 4.614281148910522 and perplexity is 100.91525940823837
At time: 16.675198554992676 and batch: 300, loss is 4.580746002197266 and perplexity is 97.58716730046635
At time: 17.327940464019775 and batch: 350, loss is 4.560468845367431 and perplexity is 95.62830420881764
At time: 17.98149561882019 and batch: 400, loss is 4.470331916809082 and perplexity is 87.38572299071919
At time: 18.631500005722046 and batch: 450, loss is 4.482250566482544 and perplexity is 88.43347429679449
At time: 19.27738308906555 and batch: 500, loss is 4.3749368858337405 and perplexity is 79.43482593123713
At time: 19.915221214294434 and batch: 550, loss is 4.463477735519409 and perplexity is 86.78881340336363
At time: 20.558560132980347 and batch: 600, loss is 4.443755187988281 and perplexity is 85.09388598964297
At time: 21.20534348487854 and batch: 650, loss is 4.300277509689331 and perplexity is 73.72024894458221
At time: 21.854417085647583 and batch: 700, loss is 4.323485245704651 and perplexity is 75.45113632258712
At time: 22.505403995513916 and batch: 750, loss is 4.415346603393555 and perplexity is 82.71050367977203
At time: 23.151188135147095 and batch: 800, loss is 4.350242042541504 and perplexity is 77.49721827904432
At time: 23.797422885894775 and batch: 850, loss is 4.422609891891479 and perplexity is 83.3134409327354
At time: 24.44033122062683 and batch: 900, loss is 4.374863176345825 and perplexity is 79.42897104667803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.585085516106592 and perplexity of 98.01156835146585
finished 2 epochs...
Completing Train Step...
At time: 26.044164180755615 and batch: 50, loss is 4.414300527572632 and perplexity is 82.62402745996326
At time: 26.685750484466553 and batch: 100, loss is 4.264714450836181 and perplexity is 71.1446017910614
At time: 27.327701568603516 and batch: 150, loss is 4.251983146667481 and perplexity is 70.24457961049272
At time: 27.968926668167114 and batch: 200, loss is 4.154873642921448 and perplexity is 63.74390954473475
At time: 28.617671966552734 and batch: 250, loss is 4.301548686027527 and perplexity is 73.81401996781246
At time: 29.270736932754517 and batch: 300, loss is 4.276878361701965 and perplexity is 72.01528309519072
At time: 29.912330389022827 and batch: 350, loss is 4.262411766052246 and perplexity is 70.98096667141235
At time: 30.561278820037842 and batch: 400, loss is 4.193980069160461 and perplexity is 66.2860898609407
At time: 31.21312403678894 and batch: 450, loss is 4.212015318870544 and perplexity is 67.49242159242006
At time: 31.868010759353638 and batch: 500, loss is 4.100390167236328 and perplexity is 60.36383499402568
At time: 32.516571283340454 and batch: 550, loss is 4.1912973642349245 and perplexity is 66.1085021553892
At time: 33.157230854034424 and batch: 600, loss is 4.186413722038269 and perplexity is 65.78643894554558
At time: 33.799640417099 and batch: 650, loss is 4.042195048332214 and perplexity is 56.95121637609226
At time: 34.44888520240784 and batch: 700, loss is 4.054655203819275 and perplexity is 57.66527680081659
At time: 35.092543601989746 and batch: 750, loss is 4.166184005737304 and perplexity is 64.46896890180608
At time: 35.740630865097046 and batch: 800, loss is 4.1060389709472656 and perplexity is 60.705783339419376
At time: 36.38173747062683 and batch: 850, loss is 4.185803852081299 and perplexity is 65.74633000471883
At time: 37.02253460884094 and batch: 900, loss is 4.141266002655029 and perplexity is 62.882380338155826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.516693115234375 and perplexity of 91.53240981870601
finished 3 epochs...
Completing Train Step...
At time: 38.626779079437256 and batch: 50, loss is 4.2002264022827145 and perplexity is 66.70143068773142
At time: 39.281052350997925 and batch: 100, loss is 4.054320316314698 and perplexity is 57.64596865337713
At time: 39.92227530479431 and batch: 150, loss is 4.048522329330444 and perplexity is 57.31270513978702
At time: 40.57159161567688 and batch: 200, loss is 3.950150423049927 and perplexity is 51.94317969871248
At time: 41.21712398529053 and batch: 250, loss is 4.098522591590881 and perplexity is 60.25120617008529
At time: 41.87158441543579 and batch: 300, loss is 4.084083275794983 and perplexity is 59.38747085748856
At time: 42.51396107673645 and batch: 350, loss is 4.067247700691223 and perplexity is 58.39601788982113
At time: 43.162829875946045 and batch: 400, loss is 4.009500713348388 and perplexity is 55.119343339672874
At time: 43.81345224380493 and batch: 450, loss is 4.02897518157959 and perplexity is 56.203283562932484
At time: 44.47503900527954 and batch: 500, loss is 3.9185062885284423 and perplexity is 50.32521725362414
At time: 45.1320903301239 and batch: 550, loss is 4.010041327476501 and perplexity is 55.149149691556126
At time: 45.77958130836487 and batch: 600, loss is 4.011226620674133 and perplexity is 55.214556358922074
At time: 46.423309326171875 and batch: 650, loss is 3.8650932931900024 and perplexity is 47.70772303074952
At time: 47.06627798080444 and batch: 700, loss is 3.877143397331238 and perplexity is 48.28608371646952
At time: 47.71043944358826 and batch: 750, loss is 3.9904396677017213 and perplexity is 54.07866077767839
At time: 48.35340452194214 and batch: 800, loss is 3.934372138977051 and perplexity is 51.13003731904099
At time: 48.99675536155701 and batch: 850, loss is 4.014553670883179 and perplexity is 55.39856389161281
At time: 49.63873076438904 and batch: 900, loss is 3.97344407081604 and perplexity is 53.16732791839555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.492448362585616 and perplexity of 89.33991482879183
finished 4 epochs...
Completing Train Step...
At time: 51.25052285194397 and batch: 50, loss is 4.03717903137207 and perplexity is 56.66626337086383
At time: 51.90946173667908 and batch: 100, loss is 3.9005325412750245 and perplexity is 49.428764955282325
At time: 52.55821466445923 and batch: 150, loss is 3.8969559621810914 and perplexity is 49.25229483569045
At time: 53.212257862091064 and batch: 200, loss is 3.798870759010315 and perplexity is 44.65073457390305
At time: 53.86834168434143 and batch: 250, loss is 3.9444376707077025 and perplexity is 51.64728716233346
At time: 54.52082562446594 and batch: 300, loss is 3.934203658103943 and perplexity is 51.1214236113543
At time: 55.16940188407898 and batch: 350, loss is 3.9219374752044676 and perplexity is 50.49818904809079
At time: 55.83399939537048 and batch: 400, loss is 3.865944881439209 and perplexity is 47.74836767087157
At time: 56.492701292037964 and batch: 450, loss is 3.8861536693573 and perplexity is 48.72312041888372
At time: 57.148590087890625 and batch: 500, loss is 3.7818553018569947 and perplexity is 43.89740917641431
At time: 57.806668758392334 and batch: 550, loss is 3.864069972038269 and perplexity is 47.65892767958675
At time: 58.474252700805664 and batch: 600, loss is 3.8735361003875735 and perplexity is 48.1122152603726
At time: 59.13583278656006 and batch: 650, loss is 3.728271818161011 and perplexity is 41.607141290068675
At time: 59.79236459732056 and batch: 700, loss is 3.7389351463317873 and perplexity is 42.0531858250119
At time: 60.44713735580444 and batch: 750, loss is 3.8540018367767335 and perplexity is 47.18149859257173
At time: 61.1058406829834 and batch: 800, loss is 3.7990494537353516 and perplexity is 44.658714137572154
At time: 61.76485824584961 and batch: 850, loss is 3.8793446397781373 and perplexity is 48.3924901637692
At time: 62.42199420928955 and batch: 900, loss is 3.839733347892761 and perplexity is 46.513069977901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.491319630244007 and perplexity of 89.2391308672866
finished 5 epochs...
Completing Train Step...
At time: 64.06051969528198 and batch: 50, loss is 3.9079207611083984 and perplexity is 49.795307919528234
At time: 64.70696830749512 and batch: 100, loss is 3.7728185987472536 and perplexity is 43.50250831070348
At time: 65.34951996803284 and batch: 150, loss is 3.7732028341293335 and perplexity is 43.519226725303206
At time: 65.99745273590088 and batch: 200, loss is 3.6753474187850954 and perplexity is 39.46236444263796
At time: 66.63699769973755 and batch: 250, loss is 3.818837060928345 and perplexity is 45.55120423139749
At time: 67.2765417098999 and batch: 300, loss is 3.812754306793213 and perplexity is 45.27496844441064
At time: 67.91667103767395 and batch: 350, loss is 3.79684006690979 and perplexity is 44.56015468098481
At time: 68.55777764320374 and batch: 400, loss is 3.7464862775802614 and perplexity is 42.37193690161419
At time: 69.21136140823364 and batch: 450, loss is 3.769919304847717 and perplexity is 43.376564416148994
At time: 69.86341786384583 and batch: 500, loss is 3.663490138053894 and perplexity is 38.99721128475468
At time: 70.52410697937012 and batch: 550, loss is 3.7473309516906737 and perplexity is 42.407742499624966
At time: 71.17116212844849 and batch: 600, loss is 3.759110088348389 and perplexity is 42.91022267536981
At time: 71.81143593788147 and batch: 650, loss is 3.61768168926239 and perplexity is 37.25110799674989
At time: 72.45379447937012 and batch: 700, loss is 3.6219625282287597 and perplexity is 37.41091580309758
At time: 73.09644150733948 and batch: 750, loss is 3.7382795143127443 and perplexity is 42.025623446258585
At time: 73.73730492591858 and batch: 800, loss is 3.6859453821182253 and perplexity is 39.88280912747269
At time: 74.38843393325806 and batch: 850, loss is 3.7688713312149047 and perplexity is 43.331130731168955
At time: 75.05516791343689 and batch: 900, loss is 3.729095907211304 and perplexity is 41.64144341167995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498882554981806 and perplexity of 89.91659828611718
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.66768002510071 and batch: 50, loss is 3.810587239265442 and perplexity is 45.176960763460485
At time: 77.3190085887909 and batch: 100, loss is 3.671095108985901 and perplexity is 39.29491452041271
At time: 77.96685457229614 and batch: 150, loss is 3.673235182762146 and perplexity is 39.37909858444188
At time: 78.61558890342712 and batch: 200, loss is 3.554885687828064 and perplexity is 34.983820662590745
At time: 79.2572865486145 and batch: 250, loss is 3.6901261138916017 and perplexity is 40.0498974872394
At time: 79.89882946014404 and batch: 300, loss is 3.675317726135254 and perplexity is 39.46119271786454
At time: 80.54766893386841 and batch: 350, loss is 3.6409931230545043 and perplexity is 38.12968539772313
At time: 81.1886579990387 and batch: 400, loss is 3.583287615776062 and perplexity is 35.99167334662729
At time: 81.83519005775452 and batch: 450, loss is 3.5892370414733885 and perplexity is 36.20644137264423
At time: 82.49100613594055 and batch: 500, loss is 3.4690494108200074 and perplexity is 32.10620812020351
At time: 83.13631081581116 and batch: 550, loss is 3.532380657196045 and perplexity is 34.20530185987833
At time: 83.78921294212341 and batch: 600, loss is 3.529823651313782 and perplexity is 34.117950428473996
At time: 84.44190239906311 and batch: 650, loss is 3.3739827156066893 and perplexity is 29.19456948994353
At time: 85.09279751777649 and batch: 700, loss is 3.3593416023254394 and perplexity is 28.770242381423248
At time: 85.73704028129578 and batch: 750, loss is 3.4575400161743164 and perplexity is 31.738803458708237
At time: 86.38359880447388 and batch: 800, loss is 3.385699305534363 and perplexity is 29.538642031069777
At time: 87.0287594795227 and batch: 850, loss is 3.4475524187088014 and perplexity is 31.423386814913407
At time: 87.66945505142212 and batch: 900, loss is 3.3970268297195436 and perplexity is 29.87514398194954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434865873153895 and perplexity of 84.34081176253103
finished 7 epochs...
Completing Train Step...
At time: 89.28808665275574 and batch: 50, loss is 3.698431706428528 and perplexity is 40.383920827640566
At time: 89.95523881912231 and batch: 100, loss is 3.5593619966506957 and perplexity is 35.14077006259341
At time: 90.60950350761414 and batch: 150, loss is 3.5625949001312254 and perplexity is 35.25456061827063
At time: 91.27087044715881 and batch: 200, loss is 3.4509012031555177 and perplexity is 31.528793357331725
At time: 91.91094255447388 and batch: 250, loss is 3.5878082704544068 and perplexity is 36.154747596589
At time: 92.56061267852783 and batch: 300, loss is 3.5802037954330443 and perplexity is 35.88085245586546
At time: 93.207279920578 and batch: 350, loss is 3.551796317100525 and perplexity is 34.8759094458027
At time: 93.8596875667572 and batch: 400, loss is 3.498092894554138 and perplexity is 33.05235748295201
At time: 94.50780701637268 and batch: 450, loss is 3.510870714187622 and perplexity is 33.477404348224916
At time: 95.15138649940491 and batch: 500, loss is 3.397210912704468 and perplexity is 29.88064399384255
At time: 95.79503512382507 and batch: 550, loss is 3.465164222717285 and perplexity is 31.98171146545543
At time: 96.43877339363098 and batch: 600, loss is 3.4718338584899904 and perplexity is 32.19573075430058
At time: 97.08230876922607 and batch: 650, loss is 3.319834189414978 and perplexity is 27.655764559822803
At time: 97.72275018692017 and batch: 700, loss is 3.312534508705139 and perplexity is 27.454621342545728
At time: 98.3638505935669 and batch: 750, loss is 3.4184688472747804 and perplexity is 30.522644393556767
At time: 99.00314021110535 and batch: 800, loss is 3.354036612510681 and perplexity is 28.618020662951473
At time: 99.6432626247406 and batch: 850, loss is 3.425748972892761 and perplexity is 30.74566389893582
At time: 100.2856388092041 and batch: 900, loss is 3.3851723957061766 and perplexity is 29.523081930017415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442798771270334 and perplexity of 85.01253968117288
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.93072414398193 and batch: 50, loss is 3.654380507469177 and perplexity is 38.64357429284973
At time: 102.58295226097107 and batch: 100, loss is 3.5213498878479004 and perplexity is 33.83006444808162
At time: 103.23299169540405 and batch: 150, loss is 3.5287728357315062 and perplexity is 34.082117584681015
At time: 103.874844789505 and batch: 200, loss is 3.4124359369277952 and perplexity is 30.339058352257823
At time: 104.51430916786194 and batch: 250, loss is 3.5467722988128663 and perplexity is 34.701131649996334
At time: 105.15699481964111 and batch: 300, loss is 3.536971697807312 and perplexity is 34.3627008258936
At time: 105.80654096603394 and batch: 350, loss is 3.502994666099548 and perplexity is 33.21477031848267
At time: 106.44718146324158 and batch: 400, loss is 3.4483757972717286 and perplexity is 31.44927081269348
At time: 107.08667540550232 and batch: 450, loss is 3.4543287801742553 and perplexity is 31.637046141267035
At time: 107.75486755371094 and batch: 500, loss is 3.3346572971343993 and perplexity is 28.06876233005691
At time: 108.39854502677917 and batch: 550, loss is 3.3953548860549927 and perplexity is 29.825236157402912
At time: 109.03836679458618 and batch: 600, loss is 3.3986599540710447 and perplexity is 29.923973668713543
At time: 109.67870903015137 and batch: 650, loss is 3.240278468132019 and perplexity is 25.54083306524175
At time: 110.31937646865845 and batch: 700, loss is 3.2270375537872313 and perplexity is 25.20487816569218
At time: 110.97416257858276 and batch: 750, loss is 3.327809634208679 and perplexity is 27.87721348632069
At time: 111.62127566337585 and batch: 800, loss is 3.2533347845077514 and perplexity is 25.876488707186823
At time: 112.26152348518372 and batch: 850, loss is 3.3180385971069337 and perplexity is 27.606150638224328
At time: 112.91410064697266 and batch: 900, loss is 3.275752878189087 and perplexity is 26.463141504505494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4315423834813785 and perplexity of 84.06097122681524
finished 9 epochs...
Completing Train Step...
At time: 114.56387162208557 and batch: 50, loss is 3.6236899757385252 and perplexity is 37.47559704708214
At time: 115.20513844490051 and batch: 100, loss is 3.487302942276001 and perplexity is 32.69764125006631
At time: 115.84544491767883 and batch: 150, loss is 3.4924154758453367 and perplexity is 32.865237093230995
At time: 116.48805856704712 and batch: 200, loss is 3.378623104095459 and perplexity is 29.330358447232204
At time: 117.12882494926453 and batch: 250, loss is 3.5134358739852907 and perplexity is 33.56338947563647
At time: 117.7694079875946 and batch: 300, loss is 3.5054156017303466 and perplexity is 33.29527855272081
At time: 118.41060400009155 and batch: 350, loss is 3.473376636505127 and perplexity is 32.24543995516102
At time: 119.05273962020874 and batch: 400, loss is 3.4198743772506712 and perplexity is 30.56557504828891
At time: 119.69446206092834 and batch: 450, loss is 3.4291777992248536 and perplexity is 30.851266383737777
At time: 120.33645129203796 and batch: 500, loss is 3.3126539897918703 and perplexity is 27.457901846514716
At time: 120.97693109512329 and batch: 550, loss is 3.375842661857605 and perplexity is 29.248920349183447
At time: 121.618337392807 and batch: 600, loss is 3.3824104690551757 and perplexity is 29.441653844178163
At time: 122.25996518135071 and batch: 650, loss is 3.2270964908599855 and perplexity is 25.206363711206812
At time: 122.9024350643158 and batch: 700, loss is 3.21719322681427 and perplexity is 24.957970418014114
At time: 123.54555058479309 and batch: 750, loss is 3.321168456077576 and perplexity is 27.692689352785763
At time: 124.20773553848267 and batch: 800, loss is 3.2506174659729004 and perplexity is 25.806269492000553
At time: 124.84871792793274 and batch: 850, loss is 3.319114255905151 and perplexity is 27.63586141350695
At time: 125.49005603790283 and batch: 900, loss is 3.280622515678406 and perplexity is 26.592321685512463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434080777102953 and perplexity of 84.27462211030014
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 127.09395337104797 and batch: 50, loss is 3.609274048805237 and perplexity is 36.93922700290912
At time: 127.74017310142517 and batch: 100, loss is 3.4751921463012696 and perplexity is 32.30403504116238
At time: 128.38554739952087 and batch: 150, loss is 3.480803680419922 and perplexity is 32.48581980464438
At time: 129.0302278995514 and batch: 200, loss is 3.3660207319259645 and perplexity is 28.96304572141662
At time: 129.6758360862732 and batch: 250, loss is 3.5000775766372683 and perplexity is 33.11802104374609
At time: 130.31990694999695 and batch: 300, loss is 3.4913971853256225 and perplexity is 32.83178776732687
At time: 130.97136878967285 and batch: 350, loss is 3.458524465560913 and perplexity is 31.770064089033987
At time: 131.61665153503418 and batch: 400, loss is 3.4051764392852784 and perplexity is 30.11960953750091
At time: 132.26105999946594 and batch: 450, loss is 3.4125954055786134 and perplexity is 30.343896866745787
At time: 132.90472769737244 and batch: 500, loss is 3.2920553255081177 and perplexity is 26.898091214144298
At time: 133.5506784915924 and batch: 550, loss is 3.3527398443222047 and perplexity is 28.580933775883413
At time: 134.19538927078247 and batch: 600, loss is 3.3587548780441283 and perplexity is 28.753367132682058
At time: 134.84037280082703 and batch: 650, loss is 3.2017943620681764 and perplexity is 24.576589956490185
At time: 135.48509192466736 and batch: 700, loss is 3.189427127838135 and perplexity is 24.27451726391456
At time: 136.13013458251953 and batch: 750, loss is 3.2925752878189085 and perplexity is 26.912080844532948
At time: 136.7746081352234 and batch: 800, loss is 3.218925929069519 and perplexity is 25.00125263641387
At time: 137.42039155960083 and batch: 850, loss is 3.2846293306350707 and perplexity is 26.69908594725703
At time: 138.07101678848267 and batch: 900, loss is 3.2445157051086424 and perplexity is 25.64928523402537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433226284915453 and perplexity of 84.20264086218926
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 139.68826270103455 and batch: 50, loss is 3.60243821144104 and perplexity is 36.68757755205381
At time: 140.3498876094818 and batch: 100, loss is 3.4681095361709593 and perplexity is 32.07604648541254
At time: 140.9981551170349 and batch: 150, loss is 3.474544715881348 and perplexity is 32.283127195108605
At time: 141.64634013175964 and batch: 200, loss is 3.3592023468017578 and perplexity is 28.766236245198748
At time: 142.2943708896637 and batch: 250, loss is 3.4934874248504637 and perplexity is 32.90048584049084
At time: 142.95341396331787 and batch: 300, loss is 3.4847820949554444 and perplexity is 32.61531929286925
At time: 143.6011941432953 and batch: 350, loss is 3.4518747091293336 and perplexity is 31.559501770999137
At time: 144.24666333198547 and batch: 400, loss is 3.3986527347564697 and perplexity is 29.92375763891409
At time: 144.89122772216797 and batch: 450, loss is 3.4065629291534423 and perplexity is 30.161399034620366
At time: 145.53722524642944 and batch: 500, loss is 3.2855043840408324 and perplexity is 26.722459298319677
At time: 146.18326544761658 and batch: 550, loss is 3.3454702043533326 and perplexity is 28.373914068436576
At time: 146.8283929824829 and batch: 600, loss is 3.3521705150604246 and perplexity is 28.564666445128907
At time: 147.4740047454834 and batch: 650, loss is 3.194529595375061 and perplexity is 24.39869373401912
At time: 148.1195044517517 and batch: 700, loss is 3.1813887405395507 and perplexity is 24.080171452592623
At time: 148.76686811447144 and batch: 750, loss is 3.28450261592865 and perplexity is 26.695702994758967
At time: 149.41402912139893 and batch: 800, loss is 3.2107436227798463 and perplexity is 24.797519370372207
At time: 150.06164360046387 and batch: 850, loss is 3.2755705404281614 and perplexity is 26.458316714420587
At time: 150.7091679573059 and batch: 900, loss is 3.2349599742889406 and perplexity is 25.40535489110342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432973783310145 and perplexity of 84.1813822442309
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 152.33822393417358 and batch: 50, loss is 3.6004004669189453 and perplexity is 36.612893760954385
At time: 152.98824405670166 and batch: 100, loss is 3.4660050201416017 and perplexity is 32.00861291383026
At time: 153.6247935295105 and batch: 150, loss is 3.47265739440918 and perplexity is 32.22225601577495
At time: 154.26839423179626 and batch: 200, loss is 3.357342553138733 and perplexity is 28.712786699290326
At time: 154.9231743812561 and batch: 250, loss is 3.4916819190979003 and perplexity is 32.84113741712584
At time: 155.56927728652954 and batch: 300, loss is 3.4829737949371338 and perplexity is 32.556394303481255
At time: 156.20824575424194 and batch: 350, loss is 3.449993300437927 and perplexity is 31.50018127062127
At time: 156.86098170280457 and batch: 400, loss is 3.396725196838379 and perplexity is 29.866134015114497
At time: 157.50122046470642 and batch: 450, loss is 3.4048614263534547 and perplexity is 30.110122965270033
At time: 158.14035725593567 and batch: 500, loss is 3.2836863374710084 and perplexity is 26.67392075888717
At time: 158.79200959205627 and batch: 550, loss is 3.3434274196624756 and perplexity is 28.316011432602394
At time: 159.4385907649994 and batch: 600, loss is 3.3503974199295046 and perplexity is 28.51406344935882
At time: 160.09142684936523 and batch: 650, loss is 3.192673134803772 and perplexity is 24.353440539493523
At time: 160.7327742576599 and batch: 700, loss is 3.1793021154403687 and perplexity is 24.029977548589034
At time: 161.3741705417633 and batch: 750, loss is 3.2824406385421754 and perplexity is 26.640713771618405
At time: 162.01619219779968 and batch: 800, loss is 3.208644814491272 and perplexity is 24.745528709481473
At time: 162.65735578536987 and batch: 850, loss is 3.2732477807998657 and perplexity is 26.396931723411868
At time: 163.29833698272705 and batch: 900, loss is 3.232507553100586 and perplexity is 25.34312659665338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432857983732877 and perplexity of 84.17163464014824
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.9175202846527 and batch: 50, loss is 3.5998682594299316 and perplexity is 36.59341328898669
At time: 165.5737738609314 and batch: 100, loss is 3.465470805168152 and perplexity is 31.991518000118326
At time: 166.22422242164612 and batch: 150, loss is 3.4721646213531496 and perplexity is 32.20638166775259
At time: 166.87141060829163 and batch: 200, loss is 3.3568737649917604 and perplexity is 28.6993296397206
At time: 167.51213383674622 and batch: 250, loss is 3.4911957597732544 and perplexity is 32.82517527232569
At time: 168.152268409729 and batch: 300, loss is 3.4825080013275147 and perplexity is 32.54123327429183
At time: 168.7908992767334 and batch: 350, loss is 3.44949640750885 and perplexity is 31.4845329413773
At time: 169.42787170410156 and batch: 400, loss is 3.3962508773803712 and perplexity is 29.851971285710256
At time: 170.0659601688385 and batch: 450, loss is 3.404426784515381 and perplexity is 30.097038689772543
At time: 170.70444631576538 and batch: 500, loss is 3.2832270193099977 and perplexity is 26.661671755967355
At time: 171.35203289985657 and batch: 550, loss is 3.3429011631011964 and perplexity is 28.301113866111606
At time: 171.99089646339417 and batch: 600, loss is 3.349937176704407 and perplexity is 28.500943064352
At time: 172.62861824035645 and batch: 650, loss is 3.1921963787078855 and perplexity is 24.3418326555451
At time: 173.28198051452637 and batch: 700, loss is 3.1787717294692994 and perplexity is 24.017235764942498
At time: 173.92021131515503 and batch: 750, loss is 3.2819128751754763 and perplexity is 26.626657488348982
At time: 174.56080770492554 and batch: 800, loss is 3.2081175661087036 and perplexity is 24.732485108404557
At time: 175.20098638534546 and batch: 850, loss is 3.2726642990112307 and perplexity is 26.381534087032467
At time: 175.848792552948 and batch: 900, loss is 3.231885256767273 and perplexity is 25.327360567977305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432829974448844 and perplexity of 84.16927708594298
Annealing...
Model not improving. Stopping early with 84.06097122681524 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
369.26739287376404


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8581247329711914 and batch: 50, loss is 12.568662166595459 and perplexity is 287409.1722208776
At time: 1.5127127170562744 and batch: 100, loss is 12.526593647003175 and perplexity is 275569.08737353515
At time: 2.154910087585449 and batch: 150, loss is 12.302129707336427 and perplexity is 220164.37541730338
At time: 2.7960588932037354 and batch: 200, loss is 12.188216037750244 and perplexity is 196460.37534138962
At time: 3.4363417625427246 and batch: 250, loss is 12.11907299041748 and perplexity is 183335.48235670032
At time: 4.077421188354492 and batch: 300, loss is 12.009808826446534 and perplexity is 164359.0801534825
At time: 4.718582391738892 and batch: 350, loss is 11.914130516052246 and perplexity is 149362.35062177767
At time: 5.368671178817749 and batch: 400, loss is 11.654992427825928 and perplexity is 115265.38309449516
At time: 6.014288425445557 and batch: 450, loss is 11.607238159179687 and perplexity is 109890.33129013701
At time: 6.654273509979248 and batch: 500, loss is 11.495257091522216 and perplexity is 98248.67970480832
At time: 7.295604944229126 and batch: 550, loss is 11.466436042785645 and perplexity is 95457.46578310987
At time: 7.937511682510376 and batch: 600, loss is 11.285867614746094 and perplexity is 79687.46815777477
At time: 8.579347848892212 and batch: 650, loss is 11.142768802642822 and perplexity is 69062.61270654303
At time: 9.219825744628906 and batch: 700, loss is 11.105963954925537 and perplexity is 66566.98114097318
At time: 9.860604286193848 and batch: 750, loss is 10.972248687744141 and perplexity is 58235.39950554764
At time: 10.502650499343872 and batch: 800, loss is 10.812484054565429 and perplexity is 49636.61542424494
At time: 11.142863988876343 and batch: 850, loss is 10.851390037536621 and perplexity is 51605.835571010175
At time: 11.783839702606201 and batch: 900, loss is 10.642114162445068 and perplexity is 41861.17906399589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 8.251153815282533 and perplexity of 3832.0447434281255
finished 1 epochs...
Completing Train Step...
At time: 13.4012930393219 and batch: 50, loss is 6.189381771087646 and perplexity is 487.54459885668547
At time: 14.039316654205322 and batch: 100, loss is 5.4302904987335205 and perplexity is 228.21553211848757
At time: 14.684009075164795 and batch: 150, loss is 5.257258462905884 and perplexity is 191.9545188362596
At time: 15.33178448677063 and batch: 200, loss is 5.070139474868775 and perplexity is 159.1965297100157
At time: 15.967904567718506 and batch: 250, loss is 5.11250602722168 and perplexity is 166.0860499469002
At time: 16.600592374801636 and batch: 300, loss is 5.03850245475769 and perplexity is 154.23886231383082
At time: 17.23429036140442 and batch: 350, loss is 4.993052253723144 and perplexity is 147.38559588670952
At time: 17.865897178649902 and batch: 400, loss is 4.850491619110107 and perplexity is 127.80320490208553
At time: 18.500112056732178 and batch: 450, loss is 4.849164934158325 and perplexity is 127.63376273639095
At time: 19.135061979293823 and batch: 500, loss is 4.744886217117309 and perplexity is 114.99472032791589
At time: 19.767666339874268 and batch: 550, loss is 4.817371263504028 and perplexity is 123.63964716163096
At time: 20.405790090560913 and batch: 600, loss is 4.757121438980103 and perplexity is 116.41034884869244
At time: 21.04506778717041 and batch: 650, loss is 4.618802499771118 and perplexity is 101.3725657453671
At time: 21.68465757369995 and batch: 700, loss is 4.657990989685058 and perplexity is 105.4240712153692
At time: 22.324293613433838 and batch: 750, loss is 4.700229167938232 and perplexity is 109.97237170626912
At time: 22.96196746826172 and batch: 800, loss is 4.630450296401977 and perplexity is 102.5602362195016
At time: 23.59964632987976 and batch: 850, loss is 4.688116369247436 and perplexity is 108.64833359829319
At time: 24.238334894180298 and batch: 900, loss is 4.6348153018951415 and perplexity is 103.00889069123564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.716872959920805 and perplexity of 111.81804588679496
finished 2 epochs...
Completing Train Step...
At time: 25.850428819656372 and batch: 50, loss is 4.650495033264161 and perplexity is 104.63677144071163
At time: 26.491589546203613 and batch: 100, loss is 4.48972412109375 and perplexity is 89.09686254281405
At time: 27.133055686950684 and batch: 150, loss is 4.474693050384522 and perplexity is 87.76765602641356
At time: 27.77401375770569 and batch: 200, loss is 4.3663441181182865 and perplexity is 78.75518510318064
At time: 28.41604447364807 and batch: 250, loss is 4.5057220458984375 and perplexity is 90.53368993415326
At time: 29.056087732315063 and batch: 300, loss is 4.47779182434082 and perplexity is 88.04004997878671
At time: 29.696850299835205 and batch: 350, loss is 4.454324827194214 and perplexity is 85.99806768260952
At time: 30.336504459381104 and batch: 400, loss is 4.370176515579224 and perplexity is 79.05758536348114
At time: 31.00026512145996 and batch: 450, loss is 4.388906354904175 and perplexity is 80.55227518953151
At time: 31.642210721969604 and batch: 500, loss is 4.275485668182373 and perplexity is 71.91505768495186
At time: 32.29301977157593 and batch: 550, loss is 4.36656928062439 and perplexity is 78.77291981454819
At time: 32.93725848197937 and batch: 600, loss is 4.355862436294555 and perplexity is 77.93400947999781
At time: 33.57848405838013 and batch: 650, loss is 4.210966019630432 and perplexity is 67.42163898829067
At time: 34.21864461898804 and batch: 700, loss is 4.226683630943298 and perplexity is 68.48971794050713
At time: 34.8589723110199 and batch: 750, loss is 4.322148475646973 and perplexity is 75.35034288656635
At time: 35.517372369766235 and batch: 800, loss is 4.269819955825806 and perplexity is 71.50875972454686
At time: 36.16824817657471 and batch: 850, loss is 4.334377012252808 and perplexity is 76.2774241838815
At time: 36.81152606010437 and batch: 900, loss is 4.295789132118225 and perplexity is 73.39010608981707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.566058015170163 and perplexity of 96.16428348329666
finished 3 epochs...
Completing Train Step...
At time: 38.407084465026855 and batch: 50, loss is 4.344600582122803 and perplexity is 77.06125168994997
At time: 39.06498694419861 and batch: 100, loss is 4.19643274307251 and perplexity is 66.4488675629854
At time: 39.70915675163269 and batch: 150, loss is 4.1935520315170285 and perplexity is 66.2577229907215
At time: 40.35511016845703 and batch: 200, loss is 4.0868621969223025 and perplexity is 59.55273347448944
At time: 40.99926018714905 and batch: 250, loss is 4.237502036094665 and perplexity is 69.23468989629782
At time: 41.6441171169281 and batch: 300, loss is 4.220029764175415 and perplexity is 68.03550928011607
At time: 42.28883194923401 and batch: 350, loss is 4.193273677825927 and perplexity is 66.23928247557488
At time: 42.943928480148315 and batch: 400, loss is 4.12767005443573 and perplexity is 62.03322039900066
At time: 43.59746980667114 and batch: 450, loss is 4.146763958930969 and perplexity is 63.22905704921729
At time: 44.242719411849976 and batch: 500, loss is 4.036951193809509 and perplexity is 56.653354138197855
At time: 44.88771986961365 and batch: 550, loss is 4.127440314292908 and perplexity is 62.01897051503437
At time: 45.53324842453003 and batch: 600, loss is 4.131122422218323 and perplexity is 62.24775199846717
At time: 46.177828550338745 and batch: 650, loss is 3.98360417842865 and perplexity is 53.71026718166549
At time: 46.82888078689575 and batch: 700, loss is 3.990091109275818 and perplexity is 54.059814489509826
At time: 47.50665831565857 and batch: 750, loss is 4.100037384033203 and perplexity is 60.342543402842274
At time: 48.15129041671753 and batch: 800, loss is 4.055253143310547 and perplexity is 57.699767457726246
At time: 48.803940296173096 and batch: 850, loss is 4.118877921104431 and perplexity is 61.4902066767877
At time: 49.46059489250183 and batch: 900, loss is 4.0867459058761595 and perplexity is 59.54580842748131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.512903500909674 and perplexity of 91.186193714376
finished 4 epochs...
Completing Train Step...
At time: 51.061014890670776 and batch: 50, loss is 4.141717767715454 and perplexity is 62.910794818360365
At time: 51.72054958343506 and batch: 100, loss is 4.004136924743652 and perplexity is 54.82448631626307
At time: 52.36763572692871 and batch: 150, loss is 4.003189659118652 and perplexity is 54.77257755454171
At time: 53.01513862609863 and batch: 200, loss is 3.902339816093445 and perplexity is 49.518177089303954
At time: 53.66166710853577 and batch: 250, loss is 4.0531415939331055 and perplexity is 57.57806009045095
At time: 54.309603214263916 and batch: 300, loss is 4.04270742893219 and perplexity is 56.980404551597275
At time: 54.957743644714355 and batch: 350, loss is 4.015092940330505 and perplexity is 55.42844670126426
At time: 55.62531614303589 and batch: 400, loss is 3.955796117782593 and perplexity is 52.23726440985336
At time: 56.28341603279114 and batch: 450, loss is 3.9737331104278564 and perplexity is 53.18269760333528
At time: 56.93670439720154 and batch: 500, loss is 3.8698254442214965 and perplexity is 47.93401819064664
At time: 57.58460330963135 and batch: 550, loss is 3.9564596891403196 and perplexity is 52.27193906560359
At time: 58.23343062400818 and batch: 600, loss is 3.9660713624954225 and perplexity is 52.77678217645457
At time: 58.882545709609985 and batch: 650, loss is 3.8209900140762327 and perplexity is 45.64937948537673
At time: 59.53667497634888 and batch: 700, loss is 3.826351914405823 and perplexity is 45.89480429196448
At time: 60.18856430053711 and batch: 750, loss is 3.9387242937088014 and perplexity is 51.35304808957527
At time: 60.839152336120605 and batch: 800, loss is 3.896206488609314 and perplexity is 49.2153953716755
At time: 61.49691724777222 and batch: 850, loss is 3.9593376111984253 and perplexity is 52.42259030934925
At time: 62.14687538146973 and batch: 900, loss is 3.928054304122925 and perplexity is 50.80802447026924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493299092331978 and perplexity of 89.41595129052813
finished 5 epochs...
Completing Train Step...
At time: 63.77293515205383 and batch: 50, loss is 3.987314519882202 and perplexity is 53.90992077492628
At time: 64.42025852203369 and batch: 100, loss is 3.856382579803467 and perplexity is 47.29395943339971
At time: 65.06062984466553 and batch: 150, loss is 3.8584893703460694 and perplexity is 47.393702932293124
At time: 65.70125102996826 and batch: 200, loss is 3.7591107845306397 and perplexity is 42.91025254871562
At time: 66.34214568138123 and batch: 250, loss is 3.9090585136413574 and perplexity is 49.851994899005376
At time: 66.9939558506012 and batch: 300, loss is 3.906819748878479 and perplexity is 49.74051284707402
At time: 67.63768005371094 and batch: 350, loss is 3.871001515388489 and perplexity is 47.99042517016829
At time: 68.27918457984924 and batch: 400, loss is 3.818529944419861 and perplexity is 45.537216852584116
At time: 68.92072296142578 and batch: 450, loss is 3.8363404989242555 and perplexity is 46.35552556972585
At time: 69.56129050254822 and batch: 500, loss is 3.735844407081604 and perplexity is 41.923411046296366
At time: 70.20297694206238 and batch: 550, loss is 3.8175035238265993 and perplexity is 45.49050049486407
At time: 70.84424829483032 and batch: 600, loss is 3.833562049865723 and perplexity is 46.22690786496089
At time: 71.48498582839966 and batch: 650, loss is 3.6928692054748535 and perplexity is 40.15990884057327
At time: 72.1269884109497 and batch: 700, loss is 3.6948593187332155 and perplexity is 40.23991118807329
At time: 72.76835513114929 and batch: 750, loss is 3.809478826522827 and perplexity is 45.12691378595533
At time: 73.41019248962402 and batch: 800, loss is 3.764672055244446 and perplexity is 43.14955286971929
At time: 74.05107235908508 and batch: 850, loss is 3.83303307056427 and perplexity is 46.20246125397581
At time: 74.69137215614319 and batch: 900, loss is 3.800843930244446 and perplexity is 44.738925097836116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499531785102739 and perplexity of 89.97499380410983
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.29559993743896 and batch: 50, loss is 3.875323786735535 and perplexity is 48.198301735645614
At time: 76.93776297569275 and batch: 100, loss is 3.74516131401062 and perplexity is 42.31583280499417
At time: 77.57901978492737 and batch: 150, loss is 3.738088994026184 and perplexity is 42.0176174751109
At time: 78.2182445526123 and batch: 200, loss is 3.62193727016449 and perplexity is 37.40997088771525
At time: 78.8598153591156 and batch: 250, loss is 3.76524112701416 and perplexity is 43.17411505029216
At time: 79.50163722038269 and batch: 300, loss is 3.7491019487380983 and perplexity is 42.48291303012627
At time: 80.1703429222107 and batch: 350, loss is 3.6986675119400023 and perplexity is 40.393444701593516
At time: 80.81100845336914 and batch: 400, loss is 3.6410521602630617 and perplexity is 38.13193653436194
At time: 81.46458148956299 and batch: 450, loss is 3.6430256509780885 and perplexity is 38.20726386150095
At time: 82.11867332458496 and batch: 500, loss is 3.527002100944519 and perplexity is 34.02182059420403
At time: 82.76523947715759 and batch: 550, loss is 3.587979121208191 and perplexity is 36.16092519017698
At time: 83.4062385559082 and batch: 600, loss is 3.5944547033309937 and perplexity is 36.39584804106828
At time: 84.04605960845947 and batch: 650, loss is 3.438266921043396 and perplexity is 31.132955516334036
At time: 84.6846399307251 and batch: 700, loss is 3.4234078884124757 and perplexity is 30.67376989015569
At time: 85.323655128479 and batch: 750, loss is 3.519837050437927 and perplexity is 33.77892375453365
At time: 85.96210432052612 and batch: 800, loss is 3.4540354919433596 and perplexity is 31.627768728517985
At time: 86.60076999664307 and batch: 850, loss is 3.503600287437439 and perplexity is 33.23489198456083
At time: 87.23793268203735 and batch: 900, loss is 3.4567564153671264 and perplexity is 31.71394264844635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432735913420377 and perplexity of 84.16136040950612
finished 7 epochs...
Completing Train Step...
At time: 88.83799600601196 and batch: 50, loss is 3.755233407020569 and perplexity is 42.74419544162059
At time: 89.4909873008728 and batch: 100, loss is 3.6275771331787108 and perplexity is 37.62155408818412
At time: 90.1312415599823 and batch: 150, loss is 3.6254490280151366 and perplexity is 37.54157659510908
At time: 90.7729697227478 and batch: 200, loss is 3.513567705154419 and perplexity is 33.56781446818051
At time: 91.4146203994751 and batch: 250, loss is 3.662153539657593 and perplexity is 38.94512249334348
At time: 92.05615425109863 and batch: 300, loss is 3.651370940208435 and perplexity is 38.52744868837917
At time: 92.69850063323975 and batch: 350, loss is 3.606711230278015 and perplexity is 36.844679673138245
At time: 93.33872842788696 and batch: 400, loss is 3.5552876949310304 and perplexity is 34.99788723422743
At time: 93.97931551933289 and batch: 450, loss is 3.5626137685775756 and perplexity is 35.25522582333193
At time: 94.6195113658905 and batch: 500, loss is 3.4526824426651 and perplexity is 31.58500373696169
At time: 95.25972247123718 and batch: 550, loss is 3.517626461982727 and perplexity is 33.70433492862596
At time: 95.89860391616821 and batch: 600, loss is 3.532124090194702 and perplexity is 34.19652703386364
At time: 96.54979133605957 and batch: 650, loss is 3.382592840194702 and perplexity is 29.447023641772436
At time: 97.1893219947815 and batch: 700, loss is 3.37299973487854 and perplexity is 29.165885890790516
At time: 97.83113217353821 and batch: 750, loss is 3.478052706718445 and perplexity is 32.39657498005626
At time: 98.47254300117493 and batch: 800, loss is 3.4193921995162966 and perplexity is 30.550840561168553
At time: 99.11377501487732 and batch: 850, loss is 3.4778178310394288 and perplexity is 32.38896670604426
At time: 99.75497531890869 and batch: 900, loss is 3.44175678730011 and perplexity is 31.241795174723197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4387449499678935 and perplexity of 84.66861161928477
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.34280490875244 and batch: 50, loss is 3.708718113899231 and perplexity is 40.801470152042356
At time: 101.99210381507874 and batch: 100, loss is 3.58463903427124 and perplexity is 36.04034604084221
At time: 102.63127851486206 and batch: 150, loss is 3.5831628179550172 and perplexity is 35.9871819444823
At time: 103.27072715759277 and batch: 200, loss is 3.4677108001708983 and perplexity is 32.06325916049199
At time: 103.90928626060486 and batch: 250, loss is 3.6171128511428834 and perplexity is 37.22992417218216
At time: 104.54810953140259 and batch: 300, loss is 3.6040979719161985 and perplexity is 36.748520704731504
At time: 105.18715763092041 and batch: 350, loss is 3.552750658988953 and perplexity is 34.909208874076135
At time: 105.82623839378357 and batch: 400, loss is 3.500910019874573 and perplexity is 33.14560139434236
At time: 106.46488785743713 and batch: 450, loss is 3.5009374380111695 and perplexity is 33.14651019742775
At time: 107.10383152961731 and batch: 500, loss is 3.3870294380187986 and perplexity is 29.57795848062806
At time: 107.74247813224792 and batch: 550, loss is 3.444751811027527 and perplexity is 31.335505354615087
At time: 108.39284467697144 and batch: 600, loss is 3.4566983461380003 and perplexity is 31.712101097713436
At time: 109.04214668273926 and batch: 650, loss is 3.301789779663086 and perplexity is 27.16120802143993
At time: 109.68076848983765 and batch: 700, loss is 3.283537368774414 and perplexity is 26.66994747563342
At time: 110.31973195075989 and batch: 750, loss is 3.3825710296630858 and perplexity is 29.446381393536203
At time: 110.95928120613098 and batch: 800, loss is 3.318328471183777 and perplexity is 27.614154105598598
At time: 111.59634590148926 and batch: 850, loss is 3.3685106468200683 and perplexity is 29.035251095651194
At time: 112.24859714508057 and batch: 900, loss is 3.331649775505066 and perplexity is 27.984471736667366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429599709706764 and perplexity of 83.89782670214777
finished 9 epochs...
Completing Train Step...
At time: 113.85885572433472 and batch: 50, loss is 3.67693630695343 and perplexity is 39.52511556565223
At time: 114.4980411529541 and batch: 100, loss is 3.54990873336792 and perplexity is 34.810140338349456
At time: 115.1489896774292 and batch: 150, loss is 3.5481306505203247 and perplexity is 34.74830001978906
At time: 115.79883241653442 and batch: 200, loss is 3.433928146362305 and perplexity is 30.998169252252712
At time: 116.45094466209412 and batch: 250, loss is 3.5833389520645142 and perplexity is 35.99352107297939
At time: 117.09228157997131 and batch: 300, loss is 3.5720033264160156 and perplexity is 35.58781579818072
At time: 117.73525047302246 and batch: 350, loss is 3.522378530502319 and perplexity is 33.86488139940527
At time: 118.37701463699341 and batch: 400, loss is 3.4724503135681153 and perplexity is 32.21558409473556
At time: 119.01883912086487 and batch: 450, loss is 3.4758272123336793 and perplexity is 32.32455675215787
At time: 119.6599988937378 and batch: 500, loss is 3.3639839363098143 and perplexity is 28.90411395321782
At time: 120.30142140388489 and batch: 550, loss is 3.423944592475891 and perplexity is 30.69023704569537
At time: 120.94225978851318 and batch: 600, loss is 3.439500470161438 and perplexity is 31.171383242524357
At time: 121.58444166183472 and batch: 650, loss is 3.2877347564697263 and perplexity is 26.782126850467503
At time: 122.22794985771179 and batch: 700, loss is 3.273138084411621 and perplexity is 26.39403623415633
At time: 122.86994457244873 and batch: 750, loss is 3.375437045097351 and perplexity is 29.237058902633866
At time: 123.51178669929504 and batch: 800, loss is 3.3146540212631224 and perplexity is 27.512873468504846
At time: 124.15238857269287 and batch: 850, loss is 3.368875517845154 and perplexity is 29.04584715046122
At time: 124.79356956481934 and batch: 900, loss is 3.3357731199264524 and perplexity is 28.100099574964165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43193576760488 and perplexity of 84.09404598341756
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.41210198402405 and batch: 50, loss is 3.6622500801086426 and perplexity is 38.948882454526434
At time: 127.05687022209167 and batch: 100, loss is 3.5366709423065186 and perplexity is 34.35236760856196
At time: 127.70294213294983 and batch: 150, loss is 3.5352837324142454 and perplexity is 34.30474670212522
At time: 128.35614204406738 and batch: 200, loss is 3.420283417701721 and perplexity is 30.57808016226764
At time: 129.01397919654846 and batch: 250, loss is 3.5700540256500246 and perplexity is 35.51851201049348
At time: 129.65884017944336 and batch: 300, loss is 3.557969288825989 and perplexity is 35.09186330148056
At time: 130.30428767204285 and batch: 350, loss is 3.506265821456909 and perplexity is 33.323598892897664
At time: 130.94887018203735 and batch: 400, loss is 3.4559451484680177 and perplexity is 31.688224610042603
At time: 131.5932698249817 and batch: 450, loss is 3.4566861963272095 and perplexity is 31.71171580402595
At time: 132.24739408493042 and batch: 500, loss is 3.343654766082764 and perplexity is 28.322449708270167
At time: 132.9037528038025 and batch: 550, loss is 3.4020096969604494 and perplexity is 30.02437935947158
At time: 133.56172347068787 and batch: 600, loss is 3.4159686040878294 and perplexity is 30.446425682225875
At time: 134.21401286125183 and batch: 650, loss is 3.2628030586242676 and perplexity is 26.12265795534968
At time: 134.85829997062683 and batch: 700, loss is 3.245714292526245 and perplexity is 25.6800465759734
At time: 135.50445795059204 and batch: 750, loss is 3.344777851104736 and perplexity is 28.354276095847414
At time: 136.15799975395203 and batch: 800, loss is 3.2823384618759155 and perplexity is 26.637991851359175
At time: 136.807621717453 and batch: 850, loss is 3.333622097969055 and perplexity is 28.039720605298726
At time: 137.45485305786133 and batch: 900, loss is 3.3015681171417235 and perplexity is 27.155188066810457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430645250294306 and perplexity of 83.9855911578267
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 139.0848708152771 and batch: 50, loss is 3.654032392501831 and perplexity is 38.630124227467
At time: 139.74624514579773 and batch: 100, loss is 3.528746738433838 and perplexity is 34.081228145119276
At time: 140.3949911594391 and batch: 150, loss is 3.527972226142883 and perplexity is 34.05484203453255
At time: 141.0458948612213 and batch: 200, loss is 3.413585247993469 and perplexity is 30.373947413095586
At time: 141.70226550102234 and batch: 250, loss is 3.5630247163772584 and perplexity is 35.269716858136846
At time: 142.34888577461243 and batch: 300, loss is 3.551141438484192 and perplexity is 34.85307743539516
At time: 142.99721217155457 and batch: 350, loss is 3.4994768381118773 and perplexity is 33.09813174735184
At time: 143.64566254615784 and batch: 400, loss is 3.4491147708892824 and perplexity is 31.4725195831714
At time: 144.30725479125977 and batch: 450, loss is 3.449880003929138 and perplexity is 31.496612612219177
At time: 144.959370136261 and batch: 500, loss is 3.3368693017959594 and perplexity is 28.130919283566364
At time: 145.6203968524933 and batch: 550, loss is 3.3950115489959716 and perplexity is 29.814997806239283
At time: 146.2688500881195 and batch: 600, loss is 3.40920560836792 and perplexity is 30.241211349652605
At time: 146.92641401290894 and batch: 650, loss is 3.255388340950012 and perplexity is 25.929682136435744
At time: 147.5882716178894 and batch: 700, loss is 3.237888464927673 and perplexity is 25.479863280386947
At time: 148.24348998069763 and batch: 750, loss is 3.336620149612427 and perplexity is 28.123911276668444
At time: 148.89698600769043 and batch: 800, loss is 3.2738023567199708 and perplexity is 26.411574886107747
At time: 149.55102705955505 and batch: 850, loss is 3.3244081687927247 and perplexity is 27.78255119527833
At time: 150.20248436927795 and batch: 900, loss is 3.291834282875061 and perplexity is 26.892146246307995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430215077857449 and perplexity of 83.94947064099982
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.79450964927673 and batch: 50, loss is 3.65195686340332 and perplexity is 38.55002942884923
At time: 152.46254110336304 and batch: 100, loss is 3.5266003608703613 and perplexity is 34.00815541058527
At time: 153.10422134399414 and batch: 150, loss is 3.525919861793518 and perplexity is 33.985020764668576
At time: 153.7458472251892 and batch: 200, loss is 3.41159339427948 and perplexity is 30.313507167179413
At time: 154.38697981834412 and batch: 250, loss is 3.5610002756118773 and perplexity is 35.198387630804646
At time: 155.02737855911255 and batch: 300, loss is 3.5492686557769777 and perplexity is 34.78786627690559
At time: 155.6679139137268 and batch: 350, loss is 3.4975250959396362 and perplexity is 33.03359572712071
At time: 156.30738472938538 and batch: 400, loss is 3.4471403217315673 and perplexity is 31.410440000047384
At time: 156.9483904838562 and batch: 450, loss is 3.4480617475509643 and perplexity is 31.439395728691363
At time: 157.59026646614075 and batch: 500, loss is 3.3349589776992796 and perplexity is 28.07723140754594
At time: 158.23039770126343 and batch: 550, loss is 3.393120918273926 and perplexity is 29.75868190846734
At time: 158.8717279434204 and batch: 600, loss is 3.4074443674087522 and perplexity is 30.187996165705545
At time: 159.51292753219604 and batch: 650, loss is 3.253345685005188 and perplexity is 25.876770775322985
At time: 160.15403962135315 and batch: 700, loss is 3.235763974189758 and perplexity is 25.42578900732907
At time: 160.79498958587646 and batch: 750, loss is 3.334520001411438 and perplexity is 28.064908873577778
At time: 161.4566729068756 and batch: 800, loss is 3.2715810537338257 and perplexity is 26.352971887548797
At time: 162.10398411750793 and batch: 850, loss is 3.322056851387024 and perplexity is 27.717302339524956
At time: 162.7571632862091 and batch: 900, loss is 3.2893311643600462 and perplexity is 26.824916194627004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430021939212328 and perplexity of 83.9332583196449
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.35991096496582 and batch: 50, loss is 3.651410713195801 and perplexity is 38.528981070582596
At time: 165.00013756752014 and batch: 100, loss is 3.526055335998535 and perplexity is 33.98962517022121
At time: 165.64121961593628 and batch: 150, loss is 3.5253794956207276 and perplexity is 33.966661369910454
At time: 166.28195667266846 and batch: 200, loss is 3.411075587272644 and perplexity is 30.297814683955988
At time: 166.9239740371704 and batch: 250, loss is 3.560480146408081 and perplexity is 35.18008468183309
At time: 167.56424021720886 and batch: 300, loss is 3.548779220581055 and perplexity is 34.770844036740144
At time: 168.20442986488342 and batch: 350, loss is 3.497014980316162 and perplexity is 33.01674907107732
At time: 168.84981417655945 and batch: 400, loss is 3.446656198501587 and perplexity is 31.395237156701207
At time: 169.4890115261078 and batch: 450, loss is 3.4475927352905273 and perplexity is 31.424653723994588
At time: 170.1301143169403 and batch: 500, loss is 3.334474496841431 and perplexity is 28.06363182102328
At time: 170.76828336715698 and batch: 550, loss is 3.3926380681991577 and perplexity is 29.744316395156527
At time: 171.40577602386475 and batch: 600, loss is 3.406995191574097 and perplexity is 30.174439492224234
At time: 172.04471158981323 and batch: 650, loss is 3.2528217220306397 and perplexity is 25.86321585698362
At time: 172.68563032150269 and batch: 700, loss is 3.235211811065674 and perplexity is 25.41175369948478
At time: 173.32366228103638 and batch: 750, loss is 3.333982439041138 and perplexity is 28.049826288919625
At time: 173.96165919303894 and batch: 800, loss is 3.2710225629806517 and perplexity is 26.33825810556899
At time: 174.5982859134674 and batch: 850, loss is 3.3214679479599 and perplexity is 27.700984330523895
At time: 175.2433261871338 and batch: 900, loss is 3.288699703216553 and perplexity is 26.807982649373233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429970937232449 and perplexity of 83.9289776664547
Annealing...
Model not improving. Stopping early with 83.89782670214777 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
550.9396646022797


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}, {'best_accuracy': -83.89782670214777, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.6700280883259082, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5807405370853294}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9014790058135986 and batch: 50, loss is 6.686391477584839 and perplexity is 801.4250676894204
At time: 1.5504999160766602 and batch: 100, loss is 5.895712041854859 and perplexity is 363.47555358642944
At time: 2.198845386505127 and batch: 150, loss is 5.782707481384278 and perplexity is 324.63695012111714
At time: 2.8407413959503174 and batch: 200, loss is 5.631893358230591 and perplexity is 279.19022463712236
At time: 3.480886459350586 and batch: 250, loss is 5.68598427772522 and perplexity is 294.7077766407735
At time: 4.1218836307525635 and batch: 300, loss is 5.607619752883911 and perplexity is 272.49486036618697
At time: 4.764245510101318 and batch: 350, loss is 5.591482515335083 and perplexity is 268.13283624266654
At time: 5.4055986404418945 and batch: 400, loss is 5.448170194625854 and perplexity is 232.3326531862648
At time: 6.046207666397095 and batch: 450, loss is 5.460342016220093 and perplexity is 235.17784524951298
At time: 6.689648628234863 and batch: 500, loss is 5.405990295410156 and perplexity is 222.73668642083163
At time: 7.332199811935425 and batch: 550, loss is 5.461570529937744 and perplexity is 235.4669420017929
At time: 7.973383903503418 and batch: 600, loss is 5.389397430419922 and perplexity is 219.07134004926706
At time: 8.6182541847229 and batch: 650, loss is 5.29640233039856 and perplexity is 199.61735927245758
At time: 9.270358562469482 and batch: 700, loss is 5.389975671768188 and perplexity is 219.19805278804552
At time: 9.911016941070557 and batch: 750, loss is 5.379776391983032 and perplexity is 216.97375292951358
At time: 10.550778865814209 and batch: 800, loss is 5.335979499816895 and perplexity is 207.6760678866517
At time: 11.193503379821777 and batch: 850, loss is 5.379662675857544 and perplexity is 216.9490809178271
At time: 11.845247268676758 and batch: 900, loss is 5.299548130035401 and perplexity is 200.24630423751262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.135133508133562 and perplexity of 169.88699965692223
finished 1 epochs...
Completing Train Step...
At time: 13.430577754974365 and batch: 50, loss is 5.073983602523803 and perplexity is 159.80967924995156
At time: 14.074751853942871 and batch: 100, loss is 4.912790794372558 and perplexity is 136.01848483066044
At time: 14.707811117172241 and batch: 150, loss is 4.8725811386108395 and perplexity is 130.6577277375997
At time: 15.339094638824463 and batch: 200, loss is 4.755487213134765 and perplexity is 116.22026341151131
At time: 15.9697425365448 and batch: 250, loss is 4.856375684738159 and perplexity is 128.55742410879873
At time: 16.601015329360962 and batch: 300, loss is 4.80877179145813 and perplexity is 122.58097002847686
At time: 17.23833394050598 and batch: 350, loss is 4.7811190795898435 and perplexity is 119.23771189935746
At time: 17.89664053916931 and batch: 400, loss is 4.67627272605896 and perplexity is 107.36913166076003
At time: 18.557576894760132 and batch: 450, loss is 4.6716181087493895 and perplexity is 106.87053074045676
At time: 19.195449113845825 and batch: 500, loss is 4.577477931976318 and perplexity is 97.26876614711334
At time: 19.844130039215088 and batch: 550, loss is 4.65869833946228 and perplexity is 105.49866928901218
At time: 20.48524022102356 and batch: 600, loss is 4.622477989196778 and perplexity is 101.74584511077026
At time: 21.126290798187256 and batch: 650, loss is 4.478321390151978 and perplexity is 88.08668532642105
At time: 21.772027015686035 and batch: 700, loss is 4.515840082168579 and perplexity is 91.45436293955566
At time: 22.40800452232361 and batch: 750, loss is 4.580235900878907 and perplexity is 97.53740065186709
At time: 23.043909311294556 and batch: 800, loss is 4.51912672996521 and perplexity is 91.7554357090635
At time: 23.681302785873413 and batch: 850, loss is 4.580403528213501 and perplexity is 97.55375195678616
At time: 24.318018674850464 and batch: 900, loss is 4.527005844116211 and perplexity is 92.48124286484095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.646873996682363 and perplexity of 104.25856303002158
finished 2 epochs...
Completing Train Step...
At time: 25.89921545982361 and batch: 50, loss is 4.525529050827027 and perplexity is 92.34476798340067
At time: 26.550099849700928 and batch: 100, loss is 4.375197949409485 and perplexity is 79.4555661780771
At time: 27.190317392349243 and batch: 150, loss is 4.373223781585693 and perplexity is 79.2988622866913
At time: 27.828986406326294 and batch: 200, loss is 4.269447207450867 and perplexity is 71.48210991771026
At time: 28.467787265777588 and batch: 250, loss is 4.4186491107940675 and perplexity is 82.98410727054357
At time: 29.10604453086853 and batch: 300, loss is 4.395784583091736 and perplexity is 81.10824196057193
At time: 29.745161533355713 and batch: 350, loss is 4.37357419013977 and perplexity is 79.326654155335
At time: 30.382880210876465 and batch: 400, loss is 4.303338093757629 and perplexity is 73.94622159176114
At time: 31.022291898727417 and batch: 450, loss is 4.309818334579468 and perplexity is 74.42696690589773
At time: 31.66060209274292 and batch: 500, loss is 4.206666383743286 and perplexity is 67.13237280499308
At time: 32.31224846839905 and batch: 550, loss is 4.29769009590149 and perplexity is 73.52975071117525
At time: 32.95078945159912 and batch: 600, loss is 4.293859076499939 and perplexity is 73.24859570861351
At time: 33.58936429023743 and batch: 650, loss is 4.142708897590637 and perplexity is 62.97317849663319
At time: 34.22834229469299 and batch: 700, loss is 4.160880370140076 and perplexity is 64.12795409191827
At time: 34.86732602119446 and batch: 750, loss is 4.2643747234344485 and perplexity is 71.12043612544942
At time: 35.50572419166565 and batch: 800, loss is 4.2136481618881225 and perplexity is 67.6027161440956
At time: 36.15659713745117 and batch: 850, loss is 4.28458981513977 and perplexity is 72.57277236118885
At time: 36.79835510253906 and batch: 900, loss is 4.2439545011520385 and perplexity is 69.68286868764042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.540542393514555 and perplexity of 93.7416311858064
finished 3 epochs...
Completing Train Step...
At time: 38.38496136665344 and batch: 50, loss is 4.2735224533081055 and perplexity is 71.77401147133506
At time: 39.04346013069153 and batch: 100, loss is 4.128894457817077 and perplexity is 62.10922060177097
At time: 39.6873517036438 and batch: 150, loss is 4.13026584148407 and perplexity is 62.194454603374446
At time: 40.32990217208862 and batch: 200, loss is 4.027860994338989 and perplexity is 56.14069745430327
At time: 40.97309899330139 and batch: 250, loss is 4.185310344696045 and perplexity is 65.7138917102355
At time: 41.61725091934204 and batch: 300, loss is 4.168211197853088 and perplexity is 64.59979244470219
At time: 42.26174020767212 and batch: 350, loss is 4.147808957099914 and perplexity is 63.29516583382126
At time: 42.90650296211243 and batch: 400, loss is 4.086886224746704 and perplexity is 59.5541644143031
At time: 43.54996943473816 and batch: 450, loss is 4.10124638080597 and perplexity is 60.415541461393836
At time: 44.20064878463745 and batch: 500, loss is 3.9953535270690916 and perplexity is 54.34504967428415
At time: 44.863534688949585 and batch: 550, loss is 4.084519991874695 and perplexity is 59.41341198498689
At time: 45.51314878463745 and batch: 600, loss is 4.094082751274109 and perplexity is 59.98429339929022
At time: 46.163121700286865 and batch: 650, loss is 3.9395505952835084 and perplexity is 51.39549873017792
At time: 46.811416149139404 and batch: 700, loss is 3.9503810930252077 and perplexity is 51.95516281270876
At time: 47.45651412010193 and batch: 750, loss is 4.064666476249695 and perplexity is 58.245479032028726
At time: 48.10085844993591 and batch: 800, loss is 4.0146874332427975 and perplexity is 55.40597462986632
At time: 48.7626748085022 and batch: 850, loss is 4.092098417282105 and perplexity is 59.865382545354834
At time: 49.41520547866821 and batch: 900, loss is 4.055274581909179 and perplexity is 57.70100447314184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.504541684503424 and perplexity of 90.42689050496838
finished 4 epochs...
Completing Train Step...
At time: 51.01125764846802 and batch: 50, loss is 4.097342176437378 and perplexity is 60.180126693204464
At time: 51.68002367019653 and batch: 100, loss is 3.9595924615859985 and perplexity is 52.43595192934188
At time: 52.331087827682495 and batch: 150, loss is 3.9570867395401 and perplexity is 52.304726484499724
At time: 52.995633125305176 and batch: 200, loss is 3.8595904302597046 and perplexity is 47.44591497777088
At time: 53.64973473548889 and batch: 250, loss is 4.016764345169068 and perplexity is 55.52116754071925
At time: 54.3075213432312 and batch: 300, loss is 4.005848731994629 and perplexity is 54.91841564098325
At time: 54.9577841758728 and batch: 350, loss is 3.983168797492981 and perplexity is 53.68688784511294
At time: 55.6062912940979 and batch: 400, loss is 3.9269386959075927 and perplexity is 50.75137422637963
At time: 56.254016160964966 and batch: 450, loss is 3.944962501525879 and perplexity is 51.67440036461118
At time: 56.90127205848694 and batch: 500, loss is 3.8390238523483275 and perplexity is 46.480080866191486
At time: 57.54896688461304 and batch: 550, loss is 3.924555974006653 and perplexity is 50.630591768155696
At time: 58.1964693069458 and batch: 600, loss is 3.9420658206939696 and perplexity is 51.5249327041404
At time: 58.84416055679321 and batch: 650, loss is 3.787956881523132 and perplexity is 44.16607151446254
At time: 59.49172019958496 and batch: 700, loss is 3.7956824016571047 and perplexity is 44.50859878624818
At time: 60.13989210128784 and batch: 750, loss is 3.913213176727295 and perplexity is 50.05954399170371
At time: 60.7880859375 and batch: 800, loss is 3.864699335098267 and perplexity is 47.6889318889275
At time: 61.43637442588806 and batch: 850, loss is 3.946093897819519 and perplexity is 51.73289767524
At time: 62.085025787353516 and batch: 900, loss is 3.910652461051941 and perplexity is 49.93151971954077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496085807068707 and perplexity of 89.66547555468095
finished 5 epochs...
Completing Train Step...
At time: 63.68551278114319 and batch: 50, loss is 3.95931227684021 and perplexity is 52.42126223349083
At time: 64.3259871006012 and batch: 100, loss is 3.820892949104309 and perplexity is 45.64494874467699
At time: 64.97947382926941 and batch: 150, loss is 3.8198366451263426 and perplexity is 45.596759259597036
At time: 65.6205382347107 and batch: 200, loss is 3.726197123527527 and perplexity is 41.52090866143683
At time: 66.26080965995789 and batch: 250, loss is 3.8795854568481447 and perplexity is 48.404145304782936
At time: 66.91260027885437 and batch: 300, loss is 3.876449704170227 and perplexity is 48.2525996056155
At time: 67.55363202095032 and batch: 350, loss is 3.84972900390625 and perplexity is 46.98033002041334
At time: 68.19315576553345 and batch: 400, loss is 3.7987154579162596 and perplexity is 44.64380080439828
At time: 68.83384466171265 and batch: 450, loss is 3.8182584762573244 and perplexity is 45.52485662577806
At time: 69.47455930709839 and batch: 500, loss is 3.7138658952713013 and perplexity is 41.01204874129224
At time: 70.1148910522461 and batch: 550, loss is 3.7939487886428833 and perplexity is 44.43150494489917
At time: 70.7582540512085 and batch: 600, loss is 3.8161403799057005 and perplexity is 45.428532640803155
At time: 71.39955449104309 and batch: 650, loss is 3.6637324571609495 and perplexity is 39.00666219919318
At time: 72.04025983810425 and batch: 700, loss is 3.671749691963196 and perplexity is 39.32064472290957
At time: 72.68341016769409 and batch: 750, loss is 3.7873185873031616 and perplexity is 44.13788956144173
At time: 73.32464146614075 and batch: 800, loss is 3.7399669456481934 and perplexity is 42.09659866621731
At time: 73.96551299095154 and batch: 850, loss is 3.8254792404174807 and perplexity is 45.85477056079742
At time: 74.61840033531189 and batch: 900, loss is 3.7893285512924195 and perplexity is 44.22669434733565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.497982077402611 and perplexity of 89.83566684930382
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.23097681999207 and batch: 50, loss is 3.8555753898620604 and perplexity is 47.25579962823316
At time: 76.87195491790771 and batch: 100, loss is 3.7146937704086302 and perplexity is 41.04601565501775
At time: 77.51267170906067 and batch: 150, loss is 3.710237965583801 and perplexity is 40.863529483729614
At time: 78.15453815460205 and batch: 200, loss is 3.5968937730789183 and perplexity is 36.48472840161171
At time: 78.79938435554504 and batch: 250, loss is 3.7422973108291626 and perplexity is 42.19481350776265
At time: 79.44075560569763 and batch: 300, loss is 3.7296618032455444 and perplexity is 41.66501480821754
At time: 80.08301830291748 and batch: 350, loss is 3.684322099685669 and perplexity is 39.81812058214944
At time: 80.72421836853027 and batch: 400, loss is 3.6297763776779175 and perplexity is 37.70438413243859
At time: 81.37873077392578 and batch: 450, loss is 3.6281998586654662 and perplexity is 37.64498928485599
At time: 82.0203230381012 and batch: 500, loss is 3.5151349449157716 and perplexity is 33.620464528574026
At time: 82.66212916374207 and batch: 550, loss is 3.5737680435180663 and perplexity is 35.650673672214246
At time: 83.30310797691345 and batch: 600, loss is 3.58757933139801 and perplexity is 36.146471310208725
At time: 83.94562458992004 and batch: 650, loss is 3.418907642364502 and perplexity is 30.536040518904045
At time: 84.58574986457825 and batch: 700, loss is 3.4108257722854614 and perplexity is 30.290246781096187
At time: 85.22669577598572 and batch: 750, loss is 3.5089468812942504 and perplexity is 33.413061329008684
At time: 85.86770343780518 and batch: 800, loss is 3.4394507789611817 and perplexity is 31.169834337561177
At time: 86.5061423778534 and batch: 850, loss is 3.508177008628845 and perplexity is 33.38734742590959
At time: 87.14583969116211 and batch: 900, loss is 3.457499833106995 and perplexity is 31.737528121855792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4361024621414815 and perplexity of 84.44517119312353
finished 7 epochs...
Completing Train Step...
At time: 88.7418646812439 and batch: 50, loss is 3.7395781517028808 and perplexity is 42.08023494480166
At time: 89.39218068122864 and batch: 100, loss is 3.6020722007751464 and perplexity is 36.67415196446797
At time: 90.02979874610901 and batch: 150, loss is 3.6024010181427 and perplexity is 36.68621304541196
At time: 90.66895604133606 and batch: 200, loss is 3.4971528911590575 and perplexity is 33.0213027527652
At time: 91.30671620368958 and batch: 250, loss is 3.6430327606201174 and perplexity is 38.20753550243554
At time: 91.94599747657776 and batch: 300, loss is 3.6367892694473265 and perplexity is 37.969730231839605
At time: 92.5840675830841 and batch: 350, loss is 3.596489953994751 and perplexity is 36.469998146381734
At time: 93.22201013565063 and batch: 400, loss is 3.546063003540039 and perplexity is 34.6765270283577
At time: 93.86076712608337 and batch: 450, loss is 3.5516911935806275 and perplexity is 34.87224336014142
At time: 94.50111055374146 and batch: 500, loss is 3.4426913404464723 and perplexity is 31.271005940082052
At time: 95.14222645759583 and batch: 550, loss is 3.5069525957107546 and perplexity is 33.34649254314773
At time: 95.78375554084778 and batch: 600, loss is 3.5284179925918577 and perplexity is 34.0700259245196
At time: 96.42872500419617 and batch: 650, loss is 3.3659439945220946 and perplexity is 28.960823257753926
At time: 97.07960414886475 and batch: 700, loss is 3.3644688367843627 and perplexity is 28.918132970429497
At time: 97.739816904068 and batch: 750, loss is 3.4689835929870605 and perplexity is 32.10409502870122
At time: 98.3844096660614 and batch: 800, loss is 3.406575493812561 and perplexity is 30.161778004698597
At time: 99.02551627159119 and batch: 850, loss is 3.4854586362838744 and perplexity is 32.63739237014341
At time: 99.66510772705078 and batch: 900, loss is 3.4432391929626465 and perplexity is 31.28814253310381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4427076365849745 and perplexity of 85.00479244314386
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.24505519866943 and batch: 50, loss is 3.693377499580383 and perplexity is 40.180327074309865
At time: 101.89475202560425 and batch: 100, loss is 3.563085036277771 and perplexity is 35.271844388114395
At time: 102.53293013572693 and batch: 150, loss is 3.566595711708069 and perplexity is 35.39589000012668
At time: 103.17133045196533 and batch: 200, loss is 3.455239968299866 and perplexity is 31.665886579577382
At time: 103.80931735038757 and batch: 250, loss is 3.599899215698242 and perplexity is 36.5945461020406
At time: 104.44871640205383 and batch: 300, loss is 3.590874557495117 and perplexity is 36.265778570028225
At time: 105.0874695777893 and batch: 350, loss is 3.5457970809936525 and perplexity is 34.66730698395365
At time: 105.7260229587555 and batch: 400, loss is 3.495173873901367 and perplexity is 32.95601764622979
At time: 106.36470794677734 and batch: 450, loss is 3.4954345703125 and perplexity is 32.964610281741344
At time: 107.0042176246643 and batch: 500, loss is 3.3793045806884767 and perplexity is 29.350353212187144
At time: 107.65152311325073 and batch: 550, loss is 3.436808481216431 and perplexity is 31.087583068612727
At time: 108.2963035106659 and batch: 600, loss is 3.4557566022872925 and perplexity is 31.68225047953586
At time: 108.93461227416992 and batch: 650, loss is 3.286741089820862 and perplexity is 26.75552756183216
At time: 109.57387900352478 and batch: 700, loss is 3.2781247091293335 and perplexity is 26.525982096472596
At time: 110.21291875839233 and batch: 750, loss is 3.3775987148284914 and perplexity is 29.300328126837574
At time: 110.8516902923584 and batch: 800, loss is 3.3095805835723877 and perplexity is 27.373642108731993
At time: 111.49060392379761 and batch: 850, loss is 3.3796933364868162 and perplexity is 29.36176555034428
At time: 112.12882947921753 and batch: 900, loss is 3.3353179502487182 and perplexity is 28.08731217213612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431008534888699 and perplexity of 84.01610737194493
finished 9 epochs...
Completing Train Step...
At time: 113.73735880851746 and batch: 50, loss is 3.663529567718506 and perplexity is 38.998748962031286
At time: 114.39344716072083 and batch: 100, loss is 3.5286427783966063 and perplexity is 34.07768524353569
At time: 115.03915357589722 and batch: 150, loss is 3.531840400695801 and perplexity is 34.18682721417879
At time: 115.68975472450256 and batch: 200, loss is 3.4235162591934203 and perplexity is 30.677094210679552
At time: 116.33473634719849 and batch: 250, loss is 3.5684070348739625 and perplexity is 35.46006149577468
At time: 116.97623372077942 and batch: 300, loss is 3.560256419181824 and perplexity is 35.172214819451796
At time: 117.61712980270386 and batch: 350, loss is 3.5171412515640257 and perplexity is 33.68798520101304
At time: 118.25873923301697 and batch: 400, loss is 3.467997040748596 and perplexity is 32.07243827997252
At time: 118.89957618713379 and batch: 450, loss is 3.4715703344345092 and perplexity is 32.18724752257993
At time: 119.54082036018372 and batch: 500, loss is 3.357059760093689 and perplexity is 28.704668070907857
At time: 120.1841242313385 and batch: 550, loss is 3.4172522497177122 and perplexity is 30.48553319822448
At time: 120.83383703231812 and batch: 600, loss is 3.4397969675064086 and perplexity is 31.18062684517893
At time: 121.47357106208801 and batch: 650, loss is 3.2736469411849978 and perplexity is 26.407470436023257
At time: 122.11269950866699 and batch: 700, loss is 3.2685978746414186 and perplexity is 26.274473398532184
At time: 122.75294947624207 and batch: 750, loss is 3.3710163593292237 and perplexity is 29.108096314001664
At time: 123.39491415023804 and batch: 800, loss is 3.3065314245224 and perplexity is 27.290302642378872
At time: 124.03637218475342 and batch: 850, loss is 3.3807665634155275 and perplexity is 29.393294303534613
At time: 124.67780876159668 and batch: 900, loss is 3.339314522743225 and perplexity is 28.199789764119906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433187824406036 and perplexity of 84.19940244800334
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.29102778434753 and batch: 50, loss is 3.648856291770935 and perplexity is 38.43068741102034
At time: 126.94391655921936 and batch: 100, loss is 3.515518488883972 and perplexity is 33.63336192815386
At time: 127.58909559249878 and batch: 150, loss is 3.5201551008224485 and perplexity is 33.78966886287485
At time: 128.2339608669281 and batch: 200, loss is 3.4107608127593996 and perplexity is 30.28827920492809
At time: 128.8785240650177 and batch: 250, loss is 3.5559657049179076 and perplexity is 35.02162419733154
At time: 129.52231788635254 and batch: 300, loss is 3.5455362462997435 and perplexity is 34.65826572673588
At time: 130.1803834438324 and batch: 350, loss is 3.501462683677673 and perplexity is 33.16392483134881
At time: 130.8254816532135 and batch: 400, loss is 3.4525702095031736 and perplexity is 31.58145905104221
At time: 131.47111535072327 and batch: 450, loss is 3.454854416847229 and perplexity is 31.65368010427094
At time: 132.11661577224731 and batch: 500, loss is 3.336879029273987 and perplexity is 28.13119292779652
At time: 132.76201510429382 and batch: 550, loss is 3.3951489877700807 and perplexity is 29.819095824594697
At time: 133.40780663490295 and batch: 600, loss is 3.417828931808472 and perplexity is 30.503118729390238
At time: 134.05320596694946 and batch: 650, loss is 3.2500541400909424 and perplexity is 25.791736246339966
At time: 134.69747972488403 and batch: 700, loss is 3.2411836624145507 and perplexity is 25.563962948243088
At time: 135.34309935569763 and batch: 750, loss is 3.3423244094848634 and perplexity is 28.284795802546505
At time: 135.98820877075195 and batch: 800, loss is 3.276444549560547 and perplexity is 26.481451633449076
At time: 136.63310503959656 and batch: 850, loss is 3.346836929321289 and perplexity is 28.412719917653497
At time: 137.2776837348938 and batch: 900, loss is 3.305605340003967 and perplexity is 27.265041214512465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433275614699272 and perplexity of 84.20679466271214
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.879310131073 and batch: 50, loss is 3.641927604675293 and perplexity is 38.16533354160723
At time: 139.5410966873169 and batch: 100, loss is 3.508136715888977 and perplexity is 33.386002185306765
At time: 140.18808817863464 and batch: 150, loss is 3.5138006258010863 and perplexity is 33.57563401586535
At time: 140.83674001693726 and batch: 200, loss is 3.4044566488265993 and perplexity is 30.097937530524284
At time: 141.5021948814392 and batch: 250, loss is 3.5500066566467283 and perplexity is 34.813549228329286
At time: 142.15927648544312 and batch: 300, loss is 3.5386751461029053 and perplexity is 34.421285794113075
At time: 142.808842420578 and batch: 350, loss is 3.495453495979309 and perplexity is 32.96523416487572
At time: 143.4567060470581 and batch: 400, loss is 3.446294946670532 and perplexity is 31.383897618129797
At time: 144.10464072227478 and batch: 450, loss is 3.4484570169448854 and perplexity is 31.4518252159224
At time: 144.76522660255432 and batch: 500, loss is 3.330013518333435 and perplexity is 27.93871938560808
At time: 145.41371703147888 and batch: 550, loss is 3.3881125259399414 and perplexity is 29.610011365103112
At time: 146.06103897094727 and batch: 600, loss is 3.4113200378417967 and perplexity is 30.30522190731266
At time: 146.73245930671692 and batch: 650, loss is 3.2433002185821533 and perplexity is 25.618127812958107
At time: 147.37999629974365 and batch: 700, loss is 3.233768410682678 and perplexity is 25.37510082321374
At time: 148.03517770767212 and batch: 750, loss is 3.3343242597579956 and perplexity is 28.059415939526687
At time: 148.68927550315857 and batch: 800, loss is 3.2686978149414063 and perplexity is 26.277099408505578
At time: 149.33729100227356 and batch: 850, loss is 3.337821798324585 and perplexity is 28.157726651466692
At time: 149.98374366760254 and batch: 900, loss is 3.2962622165679933 and perplexity is 27.011486908027152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433257220542594 and perplexity of 84.20524576398317
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.57494044303894 and batch: 50, loss is 3.6400145578384397 and perplexity is 38.09239126424037
At time: 152.22628164291382 and batch: 100, loss is 3.5061429595947264 and perplexity is 33.31950494498324
At time: 152.8786804676056 and batch: 150, loss is 3.5118286705017088 and perplexity is 33.50948960478292
At time: 153.52059888839722 and batch: 200, loss is 3.402721953392029 and perplexity is 30.04577203440531
At time: 154.17342495918274 and batch: 250, loss is 3.5482494258880615 and perplexity is 34.7524275070192
At time: 154.8135073184967 and batch: 300, loss is 3.536803102493286 and perplexity is 34.35690792389887
At time: 155.45370078086853 and batch: 350, loss is 3.4937419795989992 and perplexity is 32.90886188142593
At time: 156.09497690200806 and batch: 400, loss is 3.444555234909058 and perplexity is 31.329346147998503
At time: 156.7361786365509 and batch: 450, loss is 3.446732211112976 and perplexity is 31.397623681364493
At time: 157.37780833244324 and batch: 500, loss is 3.328174648284912 and perplexity is 27.887390918989446
At time: 158.01741743087769 and batch: 550, loss is 3.386257405281067 and perplexity is 29.55513214084032
At time: 158.65501761436462 and batch: 600, loss is 3.409563784599304 and perplexity is 30.25204497282365
At time: 159.292662858963 and batch: 650, loss is 3.241461310386658 and perplexity is 25.571061716148346
At time: 159.93125224113464 and batch: 700, loss is 3.23178346157074 and perplexity is 25.324782495550547
At time: 160.56955575942993 and batch: 750, loss is 3.3322525358200075 and perplexity is 28.001344750345492
At time: 161.20827984809875 and batch: 800, loss is 3.266650609970093 and perplexity is 26.223359826703064
At time: 161.8469648361206 and batch: 850, loss is 3.3355524826049803 and perplexity is 28.0939003281786
At time: 162.497239112854 and batch: 900, loss is 3.2938627910614016 and perplexity is 26.946752551060303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43321290734696 and perplexity of 84.20151444312822
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.07989192008972 and batch: 50, loss is 3.6395415019989015 and perplexity is 38.074375697631304
At time: 164.72059631347656 and batch: 100, loss is 3.5056463527679442 and perplexity is 33.30296235928302
At time: 165.36226153373718 and batch: 150, loss is 3.5113196992874145 and perplexity is 33.49243857877719
At time: 166.00344586372375 and batch: 200, loss is 3.4022798109054566 and perplexity is 30.032490458437913
At time: 166.64543175697327 and batch: 250, loss is 3.5477810192108152 and perplexity is 34.73615304975418
At time: 167.2880823612213 and batch: 300, loss is 3.5363328647613526 and perplexity is 34.340755807405436
At time: 167.92765593528748 and batch: 350, loss is 3.493286385536194 and perplexity is 32.893872214200044
At time: 168.57025122642517 and batch: 400, loss is 3.4441241788864136 and perplexity is 31.31584435487959
At time: 169.21113181114197 and batch: 450, loss is 3.4462856531143187 and perplexity is 31.3836059514684
At time: 169.85288381576538 and batch: 500, loss is 3.327708101272583 and perplexity is 27.87438317467261
At time: 170.49405312538147 and batch: 550, loss is 3.385789532661438 and perplexity is 29.54130733811763
At time: 171.13569355010986 and batch: 600, loss is 3.4091137170791628 and perplexity is 30.238432573442722
At time: 171.77815747261047 and batch: 650, loss is 3.2409853553771972 and perplexity is 25.55889393711471
At time: 172.41955256462097 and batch: 700, loss is 3.231275086402893 and perplexity is 25.311911276979842
At time: 173.0601089000702 and batch: 750, loss is 3.3317227363586426 and perplexity is 27.986513582098652
At time: 173.70227122306824 and batch: 800, loss is 3.266134252548218 and perplexity is 26.2098226955299
At time: 174.34394454956055 and batch: 850, loss is 3.3349831342697143 and perplexity is 28.077909665356206
At time: 174.991801738739 and batch: 900, loss is 3.2932575035095213 and perplexity is 26.930446952463306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433201620023545 and perplexity of 84.2005640387664
Annealing...
Model not improving. Stopping early with 84.01610737194493 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
732.4582824707031


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}, {'best_accuracy': -83.89782670214777, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}}, {'best_accuracy': -84.01610737194493, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.6700280883259082, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5807405370853294}}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.03749367670519277, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.02256079741414807}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8590748310089111 and batch: 50, loss is 6.530180149078369 and perplexity is 685.5216964704435
At time: 1.513352870941162 and batch: 100, loss is 5.572239027023316 and perplexity is 263.0223545943792
At time: 2.164623260498047 and batch: 150, loss is 5.365637788772583 and perplexity is 213.92763182175514
At time: 2.8081448078155518 and batch: 200, loss is 5.1516810131073 and perplexity is 172.72159367764957
At time: 3.4503672122955322 and batch: 250, loss is 5.175369796752929 and perplexity is 176.86200518024742
At time: 4.0935657024383545 and batch: 300, loss is 5.100041437149048 and perplexity is 164.02870404093943
At time: 4.737002611160278 and batch: 350, loss is 5.050197687149048 and perplexity is 156.05331117148287
At time: 5.390541076660156 and batch: 400, loss is 4.901894435882569 and perplexity is 134.5444241899191
At time: 6.032057285308838 and batch: 450, loss is 4.888835868835449 and perplexity is 132.79888870677553
At time: 6.674413681030273 and batch: 500, loss is 4.799610481262207 and perplexity is 121.46309615115233
At time: 7.3161890506744385 and batch: 550, loss is 4.860206518173218 and perplexity is 129.0508507015131
At time: 7.957755088806152 and batch: 600, loss is 4.802253494262695 and perplexity is 121.78454930867014
At time: 8.599482297897339 and batch: 650, loss is 4.667372999191284 and perplexity is 106.4178152222809
At time: 9.261049747467041 and batch: 700, loss is 4.7056911754608155 and perplexity is 110.57468505038648
At time: 9.914003849029541 and batch: 750, loss is 4.74045654296875 and perplexity is 114.4864577379805
At time: 10.555542945861816 and batch: 800, loss is 4.671483249664306 and perplexity is 106.85611925024078
At time: 11.206520318984985 and batch: 850, loss is 4.729435529708862 and perplexity is 113.23162841569676
At time: 11.85217809677124 and batch: 900, loss is 4.671368083953857 and perplexity is 106.84381379794819
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.742581720221533 and perplexity of 114.7300204690363
finished 1 epochs...
Completing Train Step...
At time: 13.435073614120483 and batch: 50, loss is 4.712242956161499 and perplexity is 111.30152458448576
At time: 14.06686282157898 and batch: 100, loss is 4.534444570541382 and perplexity is 93.1717505949409
At time: 14.709232807159424 and batch: 150, loss is 4.524310054779053 and perplexity is 92.23226865822497
At time: 15.350083112716675 and batch: 200, loss is 4.407705144882202 and perplexity is 82.08088346953028
At time: 15.989405155181885 and batch: 250, loss is 4.540314712524414 and perplexity is 93.72029042793525
At time: 16.62838077545166 and batch: 300, loss is 4.515686130523681 and perplexity is 91.44028447367731
At time: 17.26592779159546 and batch: 350, loss is 4.487811508178711 and perplexity is 88.92661759106953
At time: 17.920908212661743 and batch: 400, loss is 4.403412847518921 and perplexity is 81.7293229507392
At time: 18.565058708190918 and batch: 450, loss is 4.415023012161255 and perplexity is 82.68374361585664
At time: 19.220430374145508 and batch: 500, loss is 4.311245746612549 and perplexity is 74.5332807127973
At time: 19.872076988220215 and batch: 550, loss is 4.395771713256836 and perplexity is 81.10719811760592
At time: 20.52047324180603 and batch: 600, loss is 4.383720645904541 and perplexity is 80.13563575025898
At time: 21.166308641433716 and batch: 650, loss is 4.2435021448135375 and perplexity is 69.65135432870203
At time: 21.817797422409058 and batch: 700, loss is 4.254957008361816 and perplexity is 70.45378819955147
At time: 22.454946994781494 and batch: 750, loss is 4.349665265083313 and perplexity is 77.45253251857694
At time: 23.092041969299316 and batch: 800, loss is 4.294416294097901 and perplexity is 73.28942248881215
At time: 23.72918176651001 and batch: 850, loss is 4.362741098403931 and perplexity is 78.47193919536002
At time: 24.366374969482422 and batch: 900, loss is 4.323049807548523 and perplexity is 75.41828917087965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.571485545537243 and perplexity of 96.6876370260048
finished 2 epochs...
Completing Train Step...
At time: 25.956099033355713 and batch: 50, loss is 4.392040014266968 and perplexity is 80.80509449878456
At time: 26.596059799194336 and batch: 100, loss is 4.225751476287842 and perplexity is 68.42590467761373
At time: 27.234845638275146 and batch: 150, loss is 4.222415442466736 and perplexity is 68.19801388242158
At time: 27.87400984764099 and batch: 200, loss is 4.115947108268738 and perplexity is 61.31025422208289
At time: 28.513790130615234 and batch: 250, loss is 4.263539209365844 and perplexity is 71.06103881759988
At time: 29.161824464797974 and batch: 300, loss is 4.248235239982605 and perplexity is 69.98180222115977
At time: 29.803853273391724 and batch: 350, loss is 4.222736530303955 and perplexity is 68.21991495109692
At time: 30.44446039199829 and batch: 400, loss is 4.156150851249695 and perplexity is 63.82537581050247
At time: 31.085760831832886 and batch: 450, loss is 4.171490898132324 and perplexity is 64.81200821385666
At time: 31.726452350616455 and batch: 500, loss is 4.062536535263061 and perplexity is 58.12155162483553
At time: 32.36726093292236 and batch: 550, loss is 4.1450229740142825 and perplexity is 63.11907198354913
At time: 33.00816869735718 and batch: 600, loss is 4.148952550888062 and perplexity is 63.3675911969931
At time: 33.64978814125061 and batch: 650, loss is 4.005378956794739 and perplexity is 54.892622390286974
At time: 34.29055452346802 and batch: 700, loss is 4.0130037641525265 and perplexity is 55.312767789721654
At time: 34.93126583099365 and batch: 750, loss is 4.125040745735168 and perplexity is 61.87033015109666
At time: 35.573286056518555 and batch: 800, loss is 4.070920686721802 and perplexity is 58.61090003575391
At time: 36.21391725540161 and batch: 850, loss is 4.143883452415467 and perplexity is 63.04718740252157
At time: 36.855045318603516 and batch: 900, loss is 4.1120351219177245 and perplexity is 61.07087786818086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.51695230562393 and perplexity of 91.55613721448744
finished 3 epochs...
Completing Train Step...
At time: 38.45492172241211 and batch: 50, loss is 4.188295183181762 and perplexity is 65.91033008582684
At time: 39.11305618286133 and batch: 100, loss is 4.0278164529800415 and perplexity is 56.13819692703523
At time: 39.757304430007935 and batch: 150, loss is 4.026588759422302 and perplexity is 56.069318713645515
At time: 40.40219449996948 and batch: 200, loss is 3.928580632209778 and perplexity is 50.83477319927164
At time: 41.04632043838501 and batch: 250, loss is 4.07344880104065 and perplexity is 58.759262551054626
At time: 41.71153974533081 and batch: 300, loss is 4.06706946849823 and perplexity is 58.38561076696024
At time: 42.36218452453613 and batch: 350, loss is 4.040311932563782 and perplexity is 56.84407155721441
At time: 43.011237382888794 and batch: 400, loss is 3.9829378938674926 and perplexity is 53.674492779156225
At time: 43.669697761535645 and batch: 450, loss is 3.997660069465637 and perplexity is 54.47054350818741
At time: 44.31521391868591 and batch: 500, loss is 3.8883270263671874 and perplexity is 48.829128308958005
At time: 44.964210987091064 and batch: 550, loss is 3.965793261528015 and perplexity is 52.762106942967534
At time: 45.61167287826538 and batch: 600, loss is 3.9817600011825562 and perplexity is 53.611307206961094
At time: 46.25714993476868 and batch: 650, loss is 3.834075064659119 and perplexity is 46.25062903668432
At time: 46.9139621257782 and batch: 700, loss is 3.842949824333191 and perplexity is 46.662919035447246
At time: 47.559396266937256 and batch: 750, loss is 3.958856840133667 and perplexity is 52.39739310231821
At time: 48.20368504524231 and batch: 800, loss is 3.905946717262268 and perplexity is 49.697106756953445
At time: 48.84917759895325 and batch: 850, loss is 3.9796382856369017 and perplexity is 53.497679848130176
At time: 49.49387717247009 and batch: 900, loss is 3.955010647773743 and perplexity is 52.19624971533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.497407678055437 and perplexity of 89.78408011802335
finished 4 epochs...
Completing Train Step...
At time: 51.11368489265442 and batch: 50, loss is 4.0321284818649294 and perplexity is 56.38078911009218
At time: 51.7751042842865 and batch: 100, loss is 3.876599307060242 and perplexity is 48.259818873965465
At time: 52.43363332748413 and batch: 150, loss is 3.876326994895935 and perplexity is 48.246678927403686
At time: 53.093791246414185 and batch: 200, loss is 3.7820217657089232 and perplexity is 43.90471711647255
At time: 53.75121998786926 and batch: 250, loss is 3.9278495502471924 and perplexity is 50.79762239530955
At time: 54.41230607032776 and batch: 300, loss is 3.927718777656555 and perplexity is 50.79097989296879
At time: 55.060675621032715 and batch: 350, loss is 3.8965095138549803 and perplexity is 49.230311138761415
At time: 55.707507848739624 and batch: 400, loss is 3.844213480949402 and perplexity is 46.72192221384499
At time: 56.355299949645996 and batch: 450, loss is 3.8589601373672484 and perplexity is 47.416019577203265
At time: 57.00374484062195 and batch: 500, loss is 3.7527672243118286 and perplexity is 42.63891032545245
At time: 57.64980697631836 and batch: 550, loss is 3.8305155849456787 and perplexity is 46.08629350889255
At time: 58.297762393951416 and batch: 600, loss is 3.8492328310012818 and perplexity is 46.95702543562063
At time: 58.95445680618286 and batch: 650, loss is 3.704845290184021 and perplexity is 40.643758841995776
At time: 59.611632347106934 and batch: 700, loss is 3.710880494117737 and perplexity is 40.889793904336486
At time: 60.265265464782715 and batch: 750, loss is 3.826779074668884 and perplexity is 45.9144129163534
At time: 60.92665982246399 and batch: 800, loss is 3.773283362388611 and perplexity is 43.52273139398703
At time: 61.574548959732056 and batch: 850, loss is 3.8492366218566896 and perplexity is 46.957203443251835
At time: 62.221911907196045 and batch: 900, loss is 3.8259116125106813 and perplexity is 45.874601170720254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.496064068519906 and perplexity of 89.66352637855104
finished 5 epochs...
Completing Train Step...
At time: 63.82257032394409 and batch: 50, loss is 3.906305079460144 and perplexity is 49.71491951287749
At time: 64.46903228759766 and batch: 100, loss is 3.7540614080429076 and perplexity is 42.69412863311715
At time: 65.11234998703003 and batch: 150, loss is 3.7545366191864016 and perplexity is 42.71442218028335
At time: 65.752605676651 and batch: 200, loss is 3.663022184371948 and perplexity is 38.978966665299716
At time: 66.3932158946991 and batch: 250, loss is 3.807212820053101 and perplexity is 45.02477167840233
At time: 67.0340530872345 and batch: 300, loss is 3.809736981391907 and perplexity is 45.138565022322716
At time: 67.67520260810852 and batch: 350, loss is 3.7781070470809937 and perplexity is 43.733178484331425
At time: 68.3159646987915 and batch: 400, loss is 3.7303038215637208 and perplexity is 41.69177309968903
At time: 68.95705389976501 and batch: 450, loss is 3.745173382759094 and perplexity is 42.31634350721853
At time: 69.59801745414734 and batch: 500, loss is 3.6376867961883543 and perplexity is 38.00382437798751
At time: 70.25236701965332 and batch: 550, loss is 3.7163094091415405 and perplexity is 41.11238478757537
At time: 70.92342853546143 and batch: 600, loss is 3.7341516971588136 and perplexity is 41.852506899427496
At time: 71.56362795829773 and batch: 650, loss is 3.594751405715942 and perplexity is 36.406648378147935
At time: 72.2040524482727 and batch: 700, loss is 3.5984451723098756 and perplexity is 36.54137471035402
At time: 72.8434112071991 and batch: 750, loss is 3.714007682800293 and perplexity is 41.017864150608666
At time: 73.48527431488037 and batch: 800, loss is 3.6605580472946166 and perplexity is 38.88303539074992
At time: 74.12645196914673 and batch: 850, loss is 3.738022708892822 and perplexity is 42.014832424037785
At time: 74.76835775375366 and batch: 900, loss is 3.712233304977417 and perplexity is 40.9451474946075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.511562399668236 and perplexity of 91.06398576171857
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.3774676322937 and batch: 50, loss is 3.81018030166626 and perplexity is 45.15858029961362
At time: 77.01788258552551 and batch: 100, loss is 3.6628901672363283 and perplexity is 38.973821113429125
At time: 77.65910029411316 and batch: 150, loss is 3.659290142059326 and perplexity is 38.83376662727587
At time: 78.30016255378723 and batch: 200, loss is 3.5449961948394777 and perplexity is 34.63955353294846
At time: 78.94108510017395 and batch: 250, loss is 3.6821869564056398 and perplexity is 39.733193887160034
At time: 79.58152103424072 and batch: 300, loss is 3.672650351524353 and perplexity is 39.35607519052977
At time: 80.2225182056427 and batch: 350, loss is 3.621431241035461 and perplexity is 37.3910451416232
At time: 80.86340284347534 and batch: 400, loss is 3.5664935302734375 and perplexity is 35.392273382085165
At time: 81.51427912712097 and batch: 450, loss is 3.5635709142684937 and perplexity is 35.28898636511221
At time: 82.1559841632843 and batch: 500, loss is 3.4443243026733397 and perplexity is 31.32211202737693
At time: 82.79648780822754 and batch: 550, loss is 3.502157015800476 and perplexity is 33.18695960564388
At time: 83.43680024147034 and batch: 600, loss is 3.508001489639282 and perplexity is 33.381487826675716
At time: 84.07676005363464 and batch: 650, loss is 3.352762746810913 and perplexity is 28.581588357892233
At time: 84.72535371780396 and batch: 700, loss is 3.338540110588074 and perplexity is 28.177959957878485
At time: 85.36872959136963 and batch: 750, loss is 3.4326651334762572 and perplexity is 30.959042878801363
At time: 86.00798153877258 and batch: 800, loss is 3.3599472856521606 and perplexity is 28.78767331581969
At time: 86.6579077243805 and batch: 850, loss is 3.4165845918655395 and perplexity is 30.46518608581754
At time: 87.29610800743103 and batch: 900, loss is 3.375004005432129 and perplexity is 29.224400837354576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4526082914169525 and perplexity of 85.85057554663648
finished 7 epochs...
Completing Train Step...
At time: 88.87308835983276 and batch: 50, loss is 3.6973585796356203 and perplexity is 40.34060700496626
At time: 89.52577614784241 and batch: 100, loss is 3.549153027534485 and perplexity is 34.78384404961393
At time: 90.16605758666992 and batch: 150, loss is 3.5471229410171508 and perplexity is 34.71330146478991
At time: 90.80706453323364 and batch: 200, loss is 3.442011904716492 and perplexity is 31.24976651756227
At time: 91.44768643379211 and batch: 250, loss is 3.5800830078125 and perplexity is 35.8765187548082
At time: 92.08808302879333 and batch: 300, loss is 3.576196298599243 and perplexity is 35.737347792553805
At time: 92.72957301139832 and batch: 350, loss is 3.5294478034973142 and perplexity is 34.10512968077904
At time: 93.37050533294678 and batch: 400, loss is 3.48188111782074 and perplexity is 32.52084010460077
At time: 94.01192665100098 and batch: 450, loss is 3.4852294874191285 and perplexity is 32.6299144055498
At time: 94.65230417251587 and batch: 500, loss is 3.3705456018447877 and perplexity is 29.094396684658687
At time: 95.29358768463135 and batch: 550, loss is 3.4344605350494386 and perplexity is 31.01467672068929
At time: 95.93447613716125 and batch: 600, loss is 3.447621674537659 and perplexity is 31.425563142973616
At time: 96.57503461837769 and batch: 650, loss is 3.299618682861328 and perplexity is 27.10230237763857
At time: 97.21644711494446 and batch: 700, loss is 3.2921550846099854 and perplexity is 26.90077467741349
At time: 97.85781788825989 and batch: 750, loss is 3.3938721418380737 and perplexity is 29.781045730618327
At time: 98.4986207485199 and batch: 800, loss is 3.328407173156738 and perplexity is 27.893876184952042
At time: 99.13797950744629 and batch: 850, loss is 3.394380865097046 and perplexity is 29.796199895569274
At time: 99.78650999069214 and batch: 900, loss is 3.362128701210022 and perplexity is 28.850539738231863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.46092203218643 and perplexity of 86.56729013813242
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.37320470809937 and batch: 50, loss is 3.6525694036483767 and perplexity is 38.57365010689192
At time: 102.02127385139465 and batch: 100, loss is 3.5103903150558473 and perplexity is 33.46132569463679
At time: 102.65964126586914 and batch: 150, loss is 3.51222936630249 and perplexity is 33.522919407006874
At time: 103.31165170669556 and batch: 200, loss is 3.4059693908691404 and perplexity is 30.143502401279136
At time: 103.94936275482178 and batch: 250, loss is 3.541284580230713 and perplexity is 34.51122316377508
At time: 104.58748507499695 and batch: 300, loss is 3.535452752113342 and perplexity is 34.310545370121154
At time: 105.22819089889526 and batch: 350, loss is 3.4847712326049805 and perplexity is 32.614965015764746
At time: 105.8664402961731 and batch: 400, loss is 3.4330875873565674 and perplexity is 30.972124409573365
At time: 106.50529980659485 and batch: 450, loss is 3.4314136600494383 and perplexity is 30.920322693014175
At time: 107.14445686340332 and batch: 500, loss is 3.310818223953247 and perplexity is 27.40754180706756
At time: 107.78318500518799 and batch: 550, loss is 3.366823740005493 and perplexity is 28.986312621642853
At time: 108.42158651351929 and batch: 600, loss is 3.376959300041199 and perplexity is 29.2815990522225
At time: 109.06107330322266 and batch: 650, loss is 3.221221995353699 and perplexity is 25.058723122428923
At time: 109.69981956481934 and batch: 700, loss is 3.2048211002349856 and perplexity is 24.651089547824654
At time: 110.33889627456665 and batch: 750, loss is 3.2978349208831785 and perplexity is 27.054001412656472
At time: 110.97690534591675 and batch: 800, loss is 3.2286746978759764 and perplexity is 25.246175979000142
At time: 111.61535310745239 and batch: 850, loss is 3.2886740827560423 and perplexity is 26.807295825310813
At time: 112.25350832939148 and batch: 900, loss is 3.2527278900146483 and perplexity is 25.860789173151836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.447565366144049 and perplexity of 85.41872731389992
finished 9 epochs...
Completing Train Step...
At time: 113.85481667518616 and batch: 50, loss is 3.6237441396713255 and perplexity is 37.47762692777491
At time: 114.50086331367493 and batch: 100, loss is 3.4761954832077024 and perplexity is 32.336463137178235
At time: 115.14199662208557 and batch: 150, loss is 3.4753585481643676 and perplexity is 32.3094109400462
At time: 115.78277635574341 and batch: 200, loss is 3.3716793346405027 and perplexity is 29.127400661662048
At time: 116.42387461662292 and batch: 250, loss is 3.505668807029724 and perplexity is 33.303710161113514
At time: 117.0636236667633 and batch: 300, loss is 3.5019700384140013 and perplexity is 33.18075497475269
At time: 117.70561194419861 and batch: 350, loss is 3.452657880783081 and perplexity is 31.58422795935365
At time: 118.3463249206543 and batch: 400, loss is 3.403798189163208 and perplexity is 30.078125776045567
At time: 118.9879777431488 and batch: 450, loss is 3.4053145170211794 and perplexity is 30.123768672127408
At time: 119.6412844657898 and batch: 500, loss is 3.2867126655578613 and perplexity is 26.75476706648833
At time: 120.28224325180054 and batch: 550, loss is 3.345837926864624 and perplexity is 28.384349713966287
At time: 120.92381477355957 and batch: 600, loss is 3.3598476219177247 and perplexity is 28.784804371758526
At time: 121.56523180007935 and batch: 650, loss is 3.208189854621887 and perplexity is 24.734273047603327
At time: 122.2074875831604 and batch: 700, loss is 3.1950653791427612 and perplexity is 24.411769660696727
At time: 122.8496732711792 and batch: 750, loss is 3.29190137386322 and perplexity is 26.893950527498212
At time: 123.48990631103516 and batch: 800, loss is 3.226194143295288 and perplexity is 25.183629069114193
At time: 124.13060355186462 and batch: 850, loss is 3.2906741285324097 and perplexity is 26.860965296911413
At time: 124.77730178833008 and batch: 900, loss is 3.257506275177002 and perplexity is 25.984657694485414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.450183189078553 and perplexity of 85.64263136000407
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.37322235107422 and batch: 50, loss is 3.6093646812438966 and perplexity is 36.94257504685301
At time: 127.01776719093323 and batch: 100, loss is 3.4631394386291503 and perplexity is 31.917020919264402
At time: 127.66330075263977 and batch: 150, loss is 3.464363718032837 and perplexity is 31.95612019994121
At time: 128.30724048614502 and batch: 200, loss is 3.3603157234191894 and perplexity is 28.79828173604545
At time: 128.95555543899536 and batch: 250, loss is 3.49290114402771 and perplexity is 32.88120256984151
At time: 129.60541439056396 and batch: 300, loss is 3.4875123167037962 and perplexity is 32.704488016737216
At time: 130.25612235069275 and batch: 350, loss is 3.4386325883865356 and perplexity is 31.144341903149815
At time: 130.9100320339203 and batch: 400, loss is 3.3886061239242555 and perplexity is 29.624630414698338
At time: 131.5542860031128 and batch: 450, loss is 3.388805084228516 and perplexity is 29.630525126566635
At time: 132.19969034194946 and batch: 500, loss is 3.26714138507843 and perplexity is 26.236232757561616
At time: 132.8439667224884 and batch: 550, loss is 3.3245481634140015 and perplexity is 27.7864408752718
At time: 133.4886610507965 and batch: 600, loss is 3.3390614080429075 and perplexity is 28.192652886047227
At time: 134.13329792022705 and batch: 650, loss is 3.183967671394348 and perplexity is 24.142352695840575
At time: 134.77791333198547 and batch: 700, loss is 3.167974591255188 and perplexity is 23.759313268959698
At time: 135.4451699256897 and batch: 750, loss is 3.260681266784668 and perplexity is 26.06728987341584
At time: 136.09107398986816 and batch: 800, loss is 3.1938667249679566 and perplexity is 24.382525921145195
At time: 136.7363772392273 and batch: 850, loss is 3.2554895877838135 and perplexity is 25.932307567559583
At time: 137.3810019493103 and batch: 900, loss is 3.222040643692017 and perplexity is 25.079245803756784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.450364622351241 and perplexity of 85.65817119257207
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.97581338882446 and batch: 50, loss is 3.602620439529419 and perplexity is 36.69426366835897
At time: 139.63525295257568 and batch: 100, loss is 3.456136951446533 and perplexity is 31.6943030888232
At time: 140.28287816047668 and batch: 150, loss is 3.4574837398529055 and perplexity is 31.737017365861433
At time: 140.9304075241089 and batch: 200, loss is 3.353347101211548 and perplexity is 28.598295015655395
At time: 141.57877039909363 and batch: 250, loss is 3.4863765478134154 and perplexity is 32.66736436260841
At time: 142.22641921043396 and batch: 300, loss is 3.4802240991592406 and perplexity is 32.466997087421724
At time: 142.87451195716858 and batch: 350, loss is 3.4315840721130373 and perplexity is 30.925592338004225
At time: 143.52220511436462 and batch: 400, loss is 3.3817045402526857 and perplexity is 29.420877466920974
At time: 144.1702263355255 and batch: 450, loss is 3.381798186302185 and perplexity is 29.42363274487699
At time: 144.81809544563293 and batch: 500, loss is 3.2599678087234496 and perplexity is 26.048698588161315
At time: 145.46579337120056 and batch: 550, loss is 3.316968321800232 and perplexity is 27.576620262518
At time: 146.11230564117432 and batch: 600, loss is 3.3321891450881957 and perplexity is 27.999569780868956
At time: 146.755628824234 and batch: 650, loss is 3.1767700004577635 and perplexity is 23.969207852804455
At time: 147.39936661720276 and batch: 700, loss is 3.160739698410034 and perplexity is 23.58803751136049
At time: 148.0448899269104 and batch: 750, loss is 3.252302589416504 and perplexity is 25.849792902573984
At time: 148.6898410320282 and batch: 800, loss is 3.185307054519653 and perplexity is 24.174710220370407
At time: 149.33473181724548 and batch: 850, loss is 3.2461118268966676 and perplexity is 25.690257306545107
At time: 149.9803307056427 and batch: 900, loss is 3.2122268629074098 and perplexity is 24.834327336944764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.450881748983305 and perplexity of 85.70247876947622
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.55275774002075 and batch: 50, loss is 3.6009582328796386 and perplexity is 36.63332088306384
At time: 152.20217418670654 and batch: 100, loss is 3.454374713897705 and perplexity is 31.63849938197138
At time: 152.84239101409912 and batch: 150, loss is 3.455613250732422 and perplexity is 31.677709105182135
At time: 153.48377346992493 and batch: 200, loss is 3.3515072202682497 and perplexity is 28.54572593290162
At time: 154.12323760986328 and batch: 250, loss is 3.4845587825775146 and perplexity is 32.608036701537685
At time: 154.76384043693542 and batch: 300, loss is 3.4780983209609984 and perplexity is 32.398052758988925
At time: 155.40477347373962 and batch: 350, loss is 3.4294060468673706 and perplexity is 30.858308916248674
At time: 156.04549098014832 and batch: 400, loss is 3.3797136640548704 and perplexity is 29.362362409698022
At time: 156.6856095790863 and batch: 450, loss is 3.3799039268493654 and perplexity is 29.36794950631452
At time: 157.32747554779053 and batch: 500, loss is 3.2580000495910646 and perplexity is 25.997491421835477
At time: 157.96907782554626 and batch: 550, loss is 3.3148499727249146 and perplexity is 27.51826518451917
At time: 158.61024570465088 and batch: 600, loss is 3.330369486808777 and perplexity is 27.948666459268072
At time: 159.2499704360962 and batch: 650, loss is 3.174814076423645 and perplexity is 23.922371721974866
At time: 159.89014077186584 and batch: 700, loss is 3.1587808179855346 and perplexity is 23.541876593047522
At time: 160.53105354309082 and batch: 750, loss is 3.250215153694153 and perplexity is 25.795889401074273
At time: 161.17143607139587 and batch: 800, loss is 3.1830765295028685 and perplexity is 24.120848017277535
At time: 161.81147933006287 and batch: 850, loss is 3.2437438058853147 and perplexity is 25.629494209995027
At time: 162.47122740745544 and batch: 900, loss is 3.209664053916931 and perplexity is 24.77076318575294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.450867535316781 and perplexity of 85.70126063167984
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.08108639717102 and batch: 50, loss is 3.6005042791366577 and perplexity is 36.616694823947554
At time: 164.72154259681702 and batch: 100, loss is 3.4539262342453 and perplexity is 31.624313340079283
At time: 165.37341856956482 and batch: 150, loss is 3.4551475191116334 and perplexity is 31.662959229386104
At time: 166.0144326686859 and batch: 200, loss is 3.351039934158325 and perplexity is 28.532390027759746
At time: 166.65503811836243 and batch: 250, loss is 3.4840904903411865 and perplexity is 32.59277018598279
At time: 167.2945580482483 and batch: 300, loss is 3.47757381439209 and perplexity is 32.381064223185774
At time: 167.93426752090454 and batch: 350, loss is 3.4288376569747925 and perplexity is 30.840774349066077
At time: 168.5868546962738 and batch: 400, loss is 3.3792212772369385 and perplexity is 29.347908328295752
At time: 169.22877287864685 and batch: 450, loss is 3.379418830871582 and perplexity is 29.35370668697975
At time: 169.87129044532776 and batch: 500, loss is 3.2575055980682373 and perplexity is 25.9846401000519
At time: 170.51130867004395 and batch: 550, loss is 3.314303131103516 and perplexity is 27.503221165490356
At time: 171.15340304374695 and batch: 600, loss is 3.329900050163269 and perplexity is 27.935549410091678
At time: 171.79478979110718 and batch: 650, loss is 3.1743064594268797 and perplexity is 23.910231401063157
At time: 172.4474925994873 and batch: 700, loss is 3.1582781171798704 and perplexity is 23.53004504683042
At time: 173.09003591537476 and batch: 750, loss is 3.2496863985061646 and perplexity is 25.782253296123006
At time: 173.73086953163147 and batch: 800, loss is 3.1825123405456544 and perplexity is 24.107243139409356
At time: 174.37050461769104 and batch: 850, loss is 3.2431510972976683 and perplexity is 25.61430788965555
At time: 175.01093125343323 and batch: 900, loss is 3.209014401435852 and perplexity is 24.754676024089136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45085666604238 and perplexity of 85.70032912622398
Annealing...
Model not improving. Stopping early with 85.41872731389992 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
914.1886458396912


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}, {'best_accuracy': -83.89782670214777, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}}, {'best_accuracy': -84.01610737194493, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.6700280883259082, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5807405370853294}}, {'best_accuracy': -85.41872731389992, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.03749367670519277, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.02256079741414807}}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.004076452983591409, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9994420467834338}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8620700836181641 and batch: 50, loss is 10.96376075744629 and perplexity is 57743.19335450563
At time: 1.525439977645874 and batch: 100, loss is 11.281873836517335 and perplexity is 79369.84875524475
At time: 2.1658380031585693 and batch: 150, loss is 11.067246150970458 and perplexity is 64038.91029565337
At time: 2.807389259338379 and batch: 200, loss is 10.920185718536377 and perplexity is 55281.06470829512
At time: 3.450247287750244 and batch: 250, loss is 10.877126865386963 and perplexity is 52951.24509986367
At time: 4.090765476226807 and batch: 300, loss is 10.77131109237671 and perplexity is 47634.42972842734
At time: 4.731189966201782 and batch: 350, loss is 10.662565841674805 and perplexity is 42726.125121762794
At time: 5.372039794921875 and batch: 400, loss is 10.612609519958497 and perplexity is 40644.12263746916
At time: 6.018605947494507 and batch: 450, loss is 10.433682899475098 and perplexity is 33985.284445253164
At time: 6.66635274887085 and batch: 500, loss is 10.380457668304443 and perplexity is 32223.705886003245
At time: 7.30764627456665 and batch: 550, loss is 10.310668907165528 and perplexity is 30051.531603455784
At time: 7.9804558753967285 and batch: 600, loss is 10.204985637664794 and perplexity is 27037.650529090624
At time: 8.638297319412231 and batch: 650, loss is 10.032227516174316 and perplexity is 22747.886436526467
At time: 9.288156986236572 and batch: 700, loss is 10.00591682434082 and perplexity is 22157.178845501545
At time: 9.928821325302124 and batch: 750, loss is 9.9168998336792 and perplexity is 20270.052585174235
At time: 10.56881332397461 and batch: 800, loss is 9.837458095550538 and perplexity is 18722.065782087637
At time: 11.208670377731323 and batch: 850, loss is 9.6234401512146 and perplexity is 15114.958343133152
At time: 11.849735260009766 and batch: 900, loss is 9.51547643661499 and perplexity is 13568.096042558225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 7.722437192315924 and perplexity of 2258.457172970289
finished 1 epochs...
Completing Train Step...
At time: 13.449013710021973 and batch: 50, loss is 6.121070461273193 and perplexity is 455.351870244393
At time: 14.080695390701294 and batch: 100, loss is 5.413018817901611 and perplexity is 224.30771075354627
At time: 14.711459398269653 and batch: 150, loss is 5.258636884689331 and perplexity is 192.21929557154044
At time: 15.343578100204468 and batch: 200, loss is 5.065070352554321 and perplexity is 158.39158493429576
At time: 15.974816083908081 and batch: 250, loss is 5.10269229888916 and perplexity is 164.46409828677378
At time: 16.60554552078247 and batch: 300, loss is 5.030551471710205 and perplexity is 153.0173741850222
At time: 17.238086462020874 and batch: 350, loss is 4.98245719909668 and perplexity is 145.83228070733733
At time: 17.869752168655396 and batch: 400, loss is 4.839962291717529 and perplexity is 126.46458287798902
At time: 18.506689071655273 and batch: 450, loss is 4.837351207733154 and perplexity is 126.13480395863576
At time: 19.14356517791748 and batch: 500, loss is 4.7390641021728515 and perplexity is 114.32715306055368
At time: 19.781156301498413 and batch: 550, loss is 4.809729957580567 and perplexity is 122.69847924890033
At time: 20.418898344039917 and batch: 600, loss is 4.749394798278809 and perplexity is 115.5143538824569
At time: 21.05643939971924 and batch: 650, loss is 4.609992837905883 and perplexity is 100.48342996202963
At time: 21.694796800613403 and batch: 700, loss is 4.649026393890381 and perplexity is 104.48321054861611
At time: 22.33381152153015 and batch: 750, loss is 4.697456350326538 and perplexity is 109.66786074897799
At time: 22.986351013183594 and batch: 800, loss is 4.624568643569947 and perplexity is 101.9587830190277
At time: 23.63033938407898 and batch: 850, loss is 4.67859076499939 and perplexity is 107.6183061754897
At time: 24.282636880874634 and batch: 900, loss is 4.6234243202209475 and perplexity is 101.84217593381314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.717853284861944 and perplexity of 111.92771765424848
finished 2 epochs...
Completing Train Step...
At time: 25.88307762145996 and batch: 50, loss is 4.640031023025513 and perplexity is 103.54755989204531
At time: 26.52359127998352 and batch: 100, loss is 4.486245250701904 and perplexity is 88.78744463017678
At time: 27.16482901573181 and batch: 150, loss is 4.4742687320709225 and perplexity is 87.73042250261153
At time: 27.80631136894226 and batch: 200, loss is 4.354947738647461 and perplexity is 77.86275601755301
At time: 28.447229146957397 and batch: 250, loss is 4.500247707366944 and perplexity is 90.03943196900198
At time: 29.087209701538086 and batch: 300, loss is 4.465434608459472 and perplexity is 86.95881436470393
At time: 29.726974487304688 and batch: 350, loss is 4.4451125049591065 and perplexity is 85.20946378531777
At time: 30.366381883621216 and batch: 400, loss is 4.364254488945007 and perplexity is 78.59078779536796
At time: 31.004668951034546 and batch: 450, loss is 4.38075779914856 and perplexity is 79.89855752848979
At time: 31.644559144973755 and batch: 500, loss is 4.274607362747193 and perplexity is 71.85192202916936
At time: 32.283103704452515 and batch: 550, loss is 4.364928827285767 and perplexity is 78.64380244970697
At time: 32.92320275306702 and batch: 600, loss is 4.3514558219909665 and perplexity is 77.59133991988314
At time: 33.562599658966064 and batch: 650, loss is 4.204406085014344 and perplexity is 66.98080494691891
At time: 34.20242619514465 and batch: 700, loss is 4.220408411026001 and perplexity is 68.06127558929107
At time: 34.84321331977844 and batch: 750, loss is 4.32029025554657 and perplexity is 75.21045537598667
At time: 35.48332738876343 and batch: 800, loss is 4.261605730056763 and perplexity is 70.92377650904209
At time: 36.124812602996826 and batch: 850, loss is 4.330481295585632 and perplexity is 75.98084701655227
At time: 36.76532435417175 and batch: 900, loss is 4.290486392974853 and perplexity is 73.0019675098456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5616392370772685 and perplexity of 95.74029230547579
finished 3 epochs...
Completing Train Step...
At time: 38.35930585861206 and batch: 50, loss is 4.338882513046265 and perplexity is 76.62186754130506
At time: 39.01698541641235 and batch: 100, loss is 4.193313508033753 and perplexity is 66.24192085250527
At time: 39.661834955215454 and batch: 150, loss is 4.187686824798584 and perplexity is 65.87024517821217
At time: 40.31986093521118 and batch: 200, loss is 4.083373847007752 and perplexity is 59.34535461706447
At time: 40.96471405029297 and batch: 250, loss is 4.233045363426209 and perplexity is 68.92682009317036
At time: 41.60952854156494 and batch: 300, loss is 4.210872850418091 and perplexity is 67.41535765990909
At time: 42.25369191169739 and batch: 350, loss is 4.188194947242737 and perplexity is 65.90372383309672
At time: 42.89940905570984 and batch: 400, loss is 4.125272512435913 and perplexity is 61.88467129522542
At time: 43.546088457107544 and batch: 450, loss is 4.144714479446411 and perplexity is 63.09960309589055
At time: 44.190980434417725 and batch: 500, loss is 4.036378393173218 and perplexity is 56.62091235312403
At time: 44.83523106575012 and batch: 550, loss is 4.1275980758666995 and perplexity is 62.02875549725454
At time: 45.47836995124817 and batch: 600, loss is 4.1295786952972415 and perplexity is 62.15173260086345
At time: 46.12302303314209 and batch: 650, loss is 3.9800195837020875 and perplexity is 53.51808229940861
At time: 46.76734495162964 and batch: 700, loss is 3.9874891805648804 and perplexity is 53.9193375408374
At time: 47.413485288619995 and batch: 750, loss is 4.101827206611634 and perplexity is 60.45064255976285
At time: 48.058387756347656 and batch: 800, loss is 4.047603921890259 and perplexity is 57.26009288840914
At time: 48.70305943489075 and batch: 850, loss is 4.122060031890869 and perplexity is 61.68618697685568
At time: 49.34782147407532 and batch: 900, loss is 4.082807173728943 and perplexity is 59.31173471703071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.509922811429795 and perplexity of 90.91480065621083
finished 4 epochs...
Completing Train Step...
At time: 50.95917367935181 and batch: 50, loss is 4.142218499183655 and perplexity is 62.942304121206845
At time: 51.61981415748596 and batch: 100, loss is 4.0055182886123655 and perplexity is 54.900271211988645
At time: 52.268486738204956 and batch: 150, loss is 4.001597628593445 and perplexity is 54.685447314534585
At time: 52.91674757003784 and batch: 200, loss is 3.901710982322693 and perplexity is 49.48704817576595
At time: 53.56456780433655 and batch: 250, loss is 4.050008907318115 and perplexity is 57.39796830513206
At time: 54.212820529937744 and batch: 300, loss is 4.035156621932983 and perplexity is 56.55177679333449
At time: 54.86648869514465 and batch: 350, loss is 4.008961343765259 and perplexity is 55.089621658639295
At time: 55.52070164680481 and batch: 400, loss is 3.953718752861023 and perplexity is 52.12886118468226
At time: 56.16795563697815 and batch: 450, loss is 3.976999182701111 and perplexity is 53.35668010244337
At time: 56.82847785949707 and batch: 500, loss is 3.869491310119629 and perplexity is 47.91800447604267
At time: 57.475605964660645 and batch: 550, loss is 3.956565246582031 and perplexity is 52.277457048991636
At time: 58.12381839752197 and batch: 600, loss is 3.966036195755005 and perplexity is 52.774926221689825
At time: 58.771820306777954 and batch: 650, loss is 3.815709977149963 and perplexity is 45.408984282300906
At time: 59.42048168182373 and batch: 700, loss is 3.8278606510162354 and perplexity is 45.964099724560846
At time: 60.067047119140625 and batch: 750, loss is 3.9382945728302 and perplexity is 51.33098535338006
At time: 60.71464490890503 and batch: 800, loss is 3.8859695768356324 and perplexity is 48.714151682346284
At time: 61.3625385761261 and batch: 850, loss is 3.965395441055298 and perplexity is 52.741121271183644
At time: 62.01082897186279 and batch: 900, loss is 3.927481532096863 and perplexity is 50.778931387800455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498704048052226 and perplexity of 89.90054898273738
finished 5 epochs...
Completing Train Step...
At time: 63.63852310180664 and batch: 50, loss is 3.9907522058486937 and perplexity is 54.09556506358809
At time: 64.27857804298401 and batch: 100, loss is 3.8611088752746583 and perplexity is 47.51801371598058
At time: 64.91882538795471 and batch: 150, loss is 3.8584831142425537 and perplexity is 47.39340643330905
At time: 65.55884385108948 and batch: 200, loss is 3.758422226905823 and perplexity is 42.88071653693039
At time: 66.19945907592773 and batch: 250, loss is 3.9086878633499147 and perplexity is 49.833520666518396
At time: 66.83904719352722 and batch: 300, loss is 3.900481824874878 and perplexity is 49.4262581698282
At time: 67.49059319496155 and batch: 350, loss is 3.8692390012741087 and perplexity is 47.90591586474959
At time: 68.13360214233398 and batch: 400, loss is 3.820651807785034 and perplexity is 45.63394318851896
At time: 68.78083753585815 and batch: 450, loss is 3.8430936527252197 and perplexity is 46.66963097073134
At time: 69.42775440216064 and batch: 500, loss is 3.7373020601272584 and perplexity is 41.98456539417478
At time: 70.06788682937622 and batch: 550, loss is 3.8207473278045656 and perplexity is 45.63830235185399
At time: 70.70959901809692 and batch: 600, loss is 3.835518102645874 and perplexity is 46.31741862966549
At time: 71.35022211074829 and batch: 650, loss is 3.684240403175354 and perplexity is 39.8148677135264
At time: 71.99010157585144 and batch: 700, loss is 3.6943335819244383 and perplexity is 40.21876114574483
At time: 72.63060879707336 and batch: 750, loss is 3.8085117053985598 and perplexity is 45.08329169168444
At time: 73.29186511039734 and batch: 800, loss is 3.7557024002075194 and perplexity is 42.76424687969078
At time: 73.93238162994385 and batch: 850, loss is 3.8395963096618653 and perplexity is 46.506696345803306
At time: 74.57269144058228 and batch: 900, loss is 3.8010023021698 and perplexity is 44.746011048635154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498819847629495 and perplexity of 89.91096003109165
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 76.16882300376892 and batch: 50, loss is 3.8781496047973634 and perplexity is 48.334693986321916
At time: 76.81056904792786 and batch: 100, loss is 3.7440581369400023 and perplexity is 42.269176688230424
At time: 77.45263814926147 and batch: 150, loss is 3.7398699808120726 and perplexity is 42.092516974319885
At time: 78.09384560585022 and batch: 200, loss is 3.619644913673401 and perplexity is 37.32431211585087
At time: 78.73509669303894 and batch: 250, loss is 3.7677852058410646 and perplexity is 43.284093239535295
At time: 79.37528395652771 and batch: 300, loss is 3.746233162879944 and perplexity is 42.36121329871148
At time: 80.01625657081604 and batch: 350, loss is 3.697841238975525 and perplexity is 40.360082475345386
At time: 80.65745306015015 and batch: 400, loss is 3.643558564186096 and perplexity is 38.22763044338282
At time: 81.29720830917358 and batch: 450, loss is 3.648270745277405 and perplexity is 38.40819104374012
At time: 81.93801808357239 and batch: 500, loss is 3.5299789714813232 and perplexity is 34.12325004580879
At time: 82.57855916023254 and batch: 550, loss is 3.5955271339416504 and perplexity is 36.43490099965952
At time: 83.23241472244263 and batch: 600, loss is 3.596810607910156 and perplexity is 36.481694269185766
At time: 83.87499952316284 and batch: 650, loss is 3.4316922998428345 and perplexity is 30.9289395257816
At time: 84.51628160476685 and batch: 700, loss is 3.4211133337020874 and perplexity is 30.603467933656454
At time: 85.1578004360199 and batch: 750, loss is 3.516817078590393 and perplexity is 33.67706623658694
At time: 85.80652356147766 and batch: 800, loss is 3.4467039728164672 and perplexity is 31.396737078475446
At time: 86.44995832443237 and batch: 850, loss is 3.5114650392532347 and perplexity is 33.49730672241481
At time: 87.0907301902771 and batch: 900, loss is 3.458470721244812 and perplexity is 31.768356674549356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.438186436483305 and perplexity of 84.6213362611654
finished 7 epochs...
Completing Train Step...
At time: 88.67527365684509 and batch: 50, loss is 3.7603494548797607 and perplexity is 42.96343713850809
At time: 89.32763171195984 and batch: 100, loss is 3.629163703918457 and perplexity is 37.681290720750084
At time: 89.96670269966125 and batch: 150, loss is 3.6280328273773192 and perplexity is 37.638701918911494
At time: 90.60568714141846 and batch: 200, loss is 3.5158400201797484 and perplexity is 33.64417784533069
At time: 91.2447350025177 and batch: 250, loss is 3.6655764627456664 and perplexity is 39.07865706119277
At time: 91.89882230758667 and batch: 300, loss is 3.651718602180481 and perplexity is 38.540845545822066
At time: 92.54669332504272 and batch: 350, loss is 3.6059119510650635 and perplexity is 36.81524225249401
At time: 93.19862198829651 and batch: 400, loss is 3.557992362976074 and perplexity is 35.09267302574296
At time: 93.8383629322052 and batch: 450, loss is 3.5678025102615356 and perplexity is 35.438631493976715
At time: 94.47897553443909 and batch: 500, loss is 3.4533822536468506 and perplexity is 31.607115005387293
At time: 95.11970639228821 and batch: 550, loss is 3.525522718429565 and perplexity is 33.971526518950526
At time: 95.76029968261719 and batch: 600, loss is 3.5339550304412843 and perplexity is 34.2591961857321
At time: 96.40194606781006 and batch: 650, loss is 3.3758157777786253 and perplexity is 29.248134029468492
At time: 97.04267191886902 and batch: 700, loss is 3.3702814388275146 and perplexity is 29.086712036089395
At time: 97.68406105041504 and batch: 750, loss is 3.4747535848617552 and perplexity is 32.28987084321515
At time: 98.32789015769958 and batch: 800, loss is 3.4121176767349244 and perplexity is 30.32940417404691
At time: 98.97860026359558 and batch: 850, loss is 3.486057872772217 and perplexity is 32.65695574749532
At time: 99.61785292625427 and batch: 900, loss is 3.4422788572311402 and perplexity is 31.258109834901532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.44541993859696 and perplexity of 85.23566406796661
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 101.21359705924988 and batch: 50, loss is 3.712235517501831 and perplexity is 40.94523808684618
At time: 101.87165021896362 and batch: 100, loss is 3.5869422912597657 and perplexity is 36.12345189005399
At time: 102.50966572761536 and batch: 150, loss is 3.588536014556885 and perplexity is 36.181068577236566
At time: 103.15271425247192 and batch: 200, loss is 3.471613636016846 and perplexity is 32.18864131150514
At time: 103.79878377914429 and batch: 250, loss is 3.6205806493759156 and perplexity is 37.359254152975694
At time: 104.43588542938232 and batch: 300, loss is 3.6054581022262573 and perplexity is 36.798537488552945
At time: 105.07324767112732 and batch: 350, loss is 3.553739013671875 and perplexity is 34.943728610205326
At time: 105.72459530830383 and batch: 400, loss is 3.5042150831222534 and perplexity is 33.2553309349837
At time: 106.36300683021545 and batch: 450, loss is 3.5075934076309205 and perplexity is 33.367868221228285
At time: 107.0017032623291 and batch: 500, loss is 3.3888935899734496 and perplexity is 29.63314771432107
At time: 107.64101195335388 and batch: 550, loss is 3.454493317604065 and perplexity is 31.64225204779739
At time: 108.27930331230164 and batch: 600, loss is 3.460330452919006 and perplexity is 31.827492264815387
At time: 108.91646575927734 and batch: 650, loss is 3.2959200143814087 and perplexity is 27.002245099518046
At time: 109.55428814888 and batch: 700, loss is 3.2822571182250977 and perplexity is 26.635825107978135
At time: 110.19284749031067 and batch: 750, loss is 3.379540066719055 and perplexity is 29.357265624217447
At time: 110.84058785438538 and batch: 800, loss is 3.310293459892273 and perplexity is 27.393163087175253
At time: 111.48492455482483 and batch: 850, loss is 3.3775151109695436 and perplexity is 29.297878608733743
At time: 112.12297582626343 and batch: 900, loss is 3.332399244308472 and perplexity is 28.00545308666532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.433271016160103 and perplexity of 84.20640743535893
finished 9 epochs...
Completing Train Step...
At time: 113.71607851982117 and batch: 50, loss is 3.679731874465942 and perplexity is 39.63576528697651
At time: 114.35745549201965 and batch: 100, loss is 3.5516575622558593 and perplexity is 34.871070580120765
At time: 115.00584840774536 and batch: 150, loss is 3.5532111501693726 and perplexity is 34.925287958732284
At time: 115.64699816703796 and batch: 200, loss is 3.438223834037781 and perplexity is 31.13161411940349
At time: 116.28779435157776 and batch: 250, loss is 3.5866150856018066 and perplexity is 36.111634025752025
At time: 116.92892670631409 and batch: 300, loss is 3.5735226488113403 and perplexity is 35.64192625893222
At time: 117.57056307792664 and batch: 350, loss is 3.522811298370361 and perplexity is 33.879540203630924
At time: 118.21134376525879 and batch: 400, loss is 3.4756772899627686 and perplexity is 32.319710941227754
At time: 118.85228300094604 and batch: 450, loss is 3.48139976978302 and perplexity is 32.50519002890021
At time: 119.4919695854187 and batch: 500, loss is 3.3643951320648195 and perplexity is 28.916001646094493
At time: 120.13311290740967 and batch: 550, loss is 3.4333788537979126 and perplexity is 30.981146863936345
At time: 120.7764093875885 and batch: 600, loss is 3.442639479637146 and perplexity is 31.269384242457186
At time: 121.4297149181366 and batch: 650, loss is 3.281295642852783 and perplexity is 26.61022772571378
At time: 122.08488941192627 and batch: 700, loss is 3.270978717803955 and perplexity is 26.337103325304422
At time: 122.73727130889893 and batch: 750, loss is 3.3730521059036254 and perplexity is 29.167413378129833
At time: 123.37955284118652 and batch: 800, loss is 3.306944103240967 and perplexity is 27.301567093642426
At time: 124.0199830532074 and batch: 850, loss is 3.3782223892211913 and perplexity is 29.318607690843464
At time: 124.67142796516418 and batch: 900, loss is 3.3364601707458497 and perplexity is 28.119412405090472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.435527644745291 and perplexity of 84.39664458800203
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 126.27228713035583 and batch: 50, loss is 3.66467209815979 and perplexity is 39.043331683600904
At time: 126.91562271118164 and batch: 100, loss is 3.5379716777801513 and perplexity is 34.3970800249226
At time: 127.56264042854309 and batch: 150, loss is 3.5414277935028076 and perplexity is 34.516165982898556
At time: 128.20592832565308 and batch: 200, loss is 3.424741816520691 and perplexity is 30.714713796039383
At time: 128.8567934036255 and batch: 250, loss is 3.5728535556793215 and perplexity is 35.61808646727028
At time: 129.50017285346985 and batch: 300, loss is 3.5588529014587404 and perplexity is 35.122884618596004
At time: 130.14300513267517 and batch: 350, loss is 3.5060048580169676 and perplexity is 33.31490378650113
At time: 130.78707218170166 and batch: 400, loss is 3.4600202941894533 and perplexity is 31.817622220968932
At time: 131.4301106929779 and batch: 450, loss is 3.4631890439987183 and perplexity is 31.91860421415225
At time: 132.07368731498718 and batch: 500, loss is 3.3433488416671753 and perplexity is 28.313786504605456
At time: 132.71857857704163 and batch: 550, loss is 3.4100888919830323 and perplexity is 30.267934716557523
At time: 133.36353993415833 and batch: 600, loss is 3.4208648920059206 and perplexity is 30.595865700569377
At time: 134.00747632980347 and batch: 650, loss is 3.2568825674057007 and perplexity is 25.96845591465998
At time: 134.65204167366028 and batch: 700, loss is 3.2435164976119997 and perplexity is 25.62366907599587
At time: 135.30769634246826 and batch: 750, loss is 3.3432515001297 and perplexity is 28.311030531232866
At time: 135.95827841758728 and batch: 800, loss is 3.274894380569458 and perplexity is 26.440432709633875
At time: 136.6042377948761 and batch: 850, loss is 3.3428804969787596 and perplexity is 28.30052899787085
At time: 137.25409412384033 and batch: 900, loss is 3.301298065185547 and perplexity is 27.147855745251118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43486085656571 and perplexity of 84.34038866047251
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 138.90502047538757 and batch: 50, loss is 3.6568077611923218 and perplexity is 38.737485980066694
At time: 139.56902480125427 and batch: 100, loss is 3.53026798248291 and perplexity is 34.1331134657317
At time: 140.21798944473267 and batch: 150, loss is 3.534963746070862 and perplexity is 34.29377140773312
At time: 140.87480092048645 and batch: 200, loss is 3.4185099267959593 and perplexity is 30.523898274927816
At time: 141.5343677997589 and batch: 250, loss is 3.5664187145233153 and perplexity is 35.38962558165338
At time: 142.180255651474 and batch: 300, loss is 3.551581692695618 and perplexity is 34.86842502769042
At time: 142.82499265670776 and batch: 350, loss is 3.499372296333313 and perplexity is 33.09467179064999
At time: 143.4701850414276 and batch: 400, loss is 3.4537618160247803 and perplexity is 31.619114154186903
At time: 144.11691737174988 and batch: 450, loss is 3.4564820241928103 and perplexity is 31.70524181625122
At time: 144.76218485832214 and batch: 500, loss is 3.335790100097656 and perplexity is 28.100576723516813
At time: 145.42855048179626 and batch: 550, loss is 3.4027871036529542 and perplexity is 30.047729588059926
At time: 146.07536911964417 and batch: 600, loss is 3.414426259994507 and perplexity is 30.399503012165354
At time: 146.72227835655212 and batch: 650, loss is 3.249559268951416 and perplexity is 25.778975818077626
At time: 147.37048316001892 and batch: 700, loss is 3.2358540058135987 and perplexity is 25.428078235450744
At time: 148.01853585243225 and batch: 750, loss is 3.335392532348633 and perplexity is 28.089407060978505
At time: 148.6738817691803 and batch: 800, loss is 3.266526823043823 and perplexity is 26.220113918498292
At time: 149.33584070205688 and batch: 850, loss is 3.3334062623977663 and perplexity is 28.03366928925122
At time: 149.98437905311584 and batch: 900, loss is 3.291715536117554 and perplexity is 26.888953080731625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.43431070406143 and perplexity of 84.29400134565763
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 151.57219529151917 and batch: 50, loss is 3.654428758621216 and perplexity is 38.645438934813455
At time: 152.22292232513428 and batch: 100, loss is 3.5281200551986696 and perplexity is 34.05987670180135
At time: 152.8647975921631 and batch: 150, loss is 3.533028230667114 and perplexity is 34.227459479501704
At time: 153.5059802532196 and batch: 200, loss is 3.4167707061767576 and perplexity is 30.470856620609503
At time: 154.1457118988037 and batch: 250, loss is 3.564514327049255 and perplexity is 35.322294154896056
At time: 154.7981035709381 and batch: 300, loss is 3.549497971534729 and perplexity is 34.795844597563786
At time: 155.43955278396606 and batch: 350, loss is 3.4974827766418457 and perplexity is 33.032197798125935
At time: 156.09082770347595 and batch: 400, loss is 3.451870584487915 and perplexity is 31.559371599639434
At time: 156.73169684410095 and batch: 450, loss is 3.454649019241333 and perplexity is 31.647179181819535
At time: 157.37431025505066 and batch: 500, loss is 3.3337132501602174 and perplexity is 28.042276603762133
At time: 158.02959966659546 and batch: 550, loss is 3.400738778114319 and perplexity is 29.986245047840907
At time: 158.672110080719 and batch: 600, loss is 3.4126419973373414 and perplexity is 30.345310675203123
At time: 159.32496404647827 and batch: 650, loss is 3.2475775623321534 and perplexity is 25.727940036736133
At time: 159.98086071014404 and batch: 700, loss is 3.233748474121094 and perplexity is 25.374594935996313
At time: 160.62864995002747 and batch: 750, loss is 3.3333680248260498 and perplexity is 28.03259737030521
At time: 161.2678735256195 and batch: 800, loss is 3.264324359893799 and perplexity is 26.162428631980816
At time: 161.90838479995728 and batch: 850, loss is 3.331015019416809 and perplexity is 27.96671405934899
At time: 162.54945182800293 and batch: 900, loss is 3.2892248249053955 and perplexity is 26.822063799331584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4340841214950775 and perplexity of 84.2749039581539
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 164.16916918754578 and batch: 50, loss is 3.6538179111480713 and perplexity is 38.621839674599606
At time: 164.80883836746216 and batch: 100, loss is 3.527579483985901 and perplexity is 34.04146988849084
At time: 165.46156787872314 and batch: 150, loss is 3.532518992424011 and perplexity is 34.210033985411044
At time: 166.10336208343506 and batch: 200, loss is 3.416329264640808 and perplexity is 30.45740848736189
At time: 166.75252175331116 and batch: 250, loss is 3.5640000104904175 and perplexity is 35.30413198506876
At time: 167.3973548412323 and batch: 300, loss is 3.5489628887176514 and perplexity is 34.777230919387726
At time: 168.03750276565552 and batch: 350, loss is 3.496967692375183 and perplexity is 33.015187813910536
At time: 168.67939019203186 and batch: 400, loss is 3.4513987064361573 and perplexity is 31.54448293794691
At time: 169.31897735595703 and batch: 450, loss is 3.45417715549469 and perplexity is 31.63224954793363
At time: 169.95828652381897 and batch: 500, loss is 3.333194417953491 and perplexity is 28.02773114116369
At time: 170.59943413734436 and batch: 550, loss is 3.4002155303955077 and perplexity is 29.9705589177477
At time: 171.25356101989746 and batch: 600, loss is 3.412181887626648 and perplexity is 30.331351714660368
At time: 171.89425539970398 and batch: 650, loss is 3.2470737648010255 and perplexity is 25.714981628545726
At time: 172.53564262390137 and batch: 700, loss is 3.23321026802063 and perplexity is 25.360941848621987
At time: 173.1745297908783 and batch: 750, loss is 3.3328462648391723 and perplexity is 28.017974897711426
At time: 173.8145318031311 and batch: 800, loss is 3.263766369819641 and perplexity is 26.147834328605185
At time: 174.4557499885559 and batch: 850, loss is 3.3304172801971434 and perplexity is 27.950002252659278
At time: 175.09604358673096 and batch: 900, loss is 3.2885923957824708 and perplexity is 26.805106107882118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434028102927012 and perplexity of 84.27018313093856
Annealing...
Model not improving. Stopping early with 84.20640743535893 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f583b624b70>
ELAPSED
1096.057294368744


RESULTS SO FAR:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}, {'best_accuracy': -83.89782670214777, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}}, {'best_accuracy': -84.01610737194493, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.6700280883259082, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5807405370853294}}, {'best_accuracy': -85.41872731389992, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.03749367670519277, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.02256079741414807}}, {'best_accuracy': -84.20640743535893, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.004076452983591409, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9994420467834338}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -83.98664446724247, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.45537816972771405, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5722396166277847}}, {'best_accuracy': -84.06097122681524, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.1223289701351119, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.15785183195982333}}, {'best_accuracy': -83.89782670214777, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.01905454422884212, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9980178211457668}}, {'best_accuracy': -84.01610737194493, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.6700280883259082, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.5807405370853294}}, {'best_accuracy': -85.41872731389992, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.03749367670519277, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.02256079741414807}}, {'best_accuracy': -84.20640743535893, 'params': {'wordvec_dim': 300, 'tune_wordvecs': 'TRUE', 'seq_len': 35, 'wordvec_source': 'None', 'rnn_dropout': 0.004076452983591409, 'tie_weights': 'FALSE', 'data': 'ptb', 'num_layers': 1, 'batch_size': 32, 'dropout': 0.9994420467834338}}]
