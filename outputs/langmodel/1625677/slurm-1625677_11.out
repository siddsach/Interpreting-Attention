TRUE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'rnn_dropout', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2416434288024902 and batch: 50, loss is 7.034954195022583 and perplexity is 1135.642892903266
At time: 2.028282880783081 and batch: 100, loss is 6.096626329421997 and perplexity is 444.35612728993397
At time: 2.8240649700164795 and batch: 150, loss is 5.837363538742065 and perplexity is 342.8741734954247
At time: 3.6209323406219482 and batch: 200, loss is 5.529784946441651 and perplexity is 252.089692408393
At time: 4.4173290729522705 and batch: 250, loss is 5.4979345417022705 and perplexity is 244.18705286549908
At time: 5.21040153503418 and batch: 300, loss is 5.35650486946106 and perplexity is 211.98274279770067
At time: 5.994976043701172 and batch: 350, loss is 5.289661264419555 and perplexity is 198.2762508119032
At time: 6.780640363693237 and batch: 400, loss is 5.110648736953736 and perplexity is 165.7778662245973
At time: 7.566129922866821 and batch: 450, loss is 5.085764865875245 and perplexity is 161.70357349503178
At time: 8.359086036682129 and batch: 500, loss is 5.001513042449951 and perplexity is 148.63788447907535
At time: 9.144073247909546 and batch: 550, loss is 5.045733718872071 and perplexity is 155.3582466680559
At time: 9.938804626464844 and batch: 600, loss is 4.947626934051514 and perplexity is 140.8403436168742
At time: 10.72468376159668 and batch: 650, loss is 4.820829811096192 and perplexity is 124.06800107984265
At time: 11.511473655700684 and batch: 700, loss is 4.8899492835998535 and perplexity is 132.94683129560588
At time: 12.296752214431763 and batch: 750, loss is 4.878666467666626 and perplexity is 131.45524712743
At time: 13.083032131195068 and batch: 800, loss is 4.832267265319825 and perplexity is 125.49516918984965
At time: 13.869290828704834 and batch: 850, loss is 4.863199644088745 and perplexity is 129.43769479490425
At time: 14.654165506362915 and batch: 900, loss is 4.779497766494751 and perplexity is 119.04454686843698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.802689225706335 and perplexity of 121.83762622895526
finished 1 epochs...
Completing Train Step...
At time: 16.487544775009155 and batch: 50, loss is 4.768572120666504 and perplexity is 117.75098766405694
At time: 17.2152898311615 and batch: 100, loss is 4.652106342315673 and perplexity is 104.80550952574887
At time: 17.93797278404236 and batch: 150, loss is 4.637941246032715 and perplexity is 103.33139453109074
At time: 18.653009176254272 and batch: 200, loss is 4.52432502746582 and perplexity is 92.23364963343181
At time: 19.367496967315674 and batch: 250, loss is 4.633310499191285 and perplexity is 102.85399920379515
At time: 20.081911325454712 and batch: 300, loss is 4.577705163955688 and perplexity is 97.29087123277186
At time: 20.79662013053894 and batch: 350, loss is 4.570066509246826 and perplexity is 96.55053106240631
At time: 21.51115608215332 and batch: 400, loss is 4.453404417037964 and perplexity is 85.91895060336356
At time: 22.22670078277588 and batch: 450, loss is 4.483588523864746 and perplexity is 88.55187370558268
At time: 22.9415066242218 and batch: 500, loss is 4.376943483352661 and perplexity is 79.59437968241119
At time: 23.656179666519165 and batch: 550, loss is 4.449536905288697 and perplexity is 85.58729979750734
At time: 24.390698432922363 and batch: 600, loss is 4.414154663085937 and perplexity is 82.61197642753937
At time: 25.105096101760864 and batch: 650, loss is 4.268564248085022 and perplexity is 71.41902197541157
At time: 25.82414746284485 and batch: 700, loss is 4.304132256507874 and perplexity is 74.00497025138148
At time: 26.53922128677368 and batch: 750, loss is 4.363500137329101 and perplexity is 78.53152506287849
At time: 27.253175258636475 and batch: 800, loss is 4.319715867042541 and perplexity is 75.16726775945857
At time: 27.969175338745117 and batch: 850, loss is 4.381499576568603 and perplexity is 79.95784646123569
At time: 28.684406280517578 and batch: 900, loss is 4.320908522605896 and perplexity is 75.25696990078008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.492814155474101 and perplexity of 89.37260071206144
finished 2 epochs...
Completing Train Step...
At time: 30.44007921218872 and batch: 50, loss is 4.38368637084961 and perplexity is 80.13288914401201
At time: 31.16763997077942 and batch: 100, loss is 4.2650396490097044 and perplexity is 71.16774164793554
At time: 31.88238263130188 and batch: 150, loss is 4.2610769367218015 and perplexity is 70.88628240292421
At time: 32.59750723838806 and batch: 200, loss is 4.1490660572052 and perplexity is 63.37478422711524
At time: 33.31186652183533 and batch: 250, loss is 4.28680739402771 and perplexity is 72.73388678496316
At time: 34.02682638168335 and batch: 300, loss is 4.251823697090149 and perplexity is 70.2333800348715
At time: 34.750046253204346 and batch: 350, loss is 4.244050908088684 and perplexity is 69.68958692338433
At time: 35.470009326934814 and batch: 400, loss is 4.153882241249084 and perplexity is 63.68074504207945
At time: 36.18591594696045 and batch: 450, loss is 4.195590162277222 and perplexity is 66.39290260411897
At time: 36.90803384780884 and batch: 500, loss is 4.076888427734375 and perplexity is 58.961720469251624
At time: 37.630857706069946 and batch: 550, loss is 4.148605556488037 and perplexity is 63.345606812154465
At time: 38.34526014328003 and batch: 600, loss is 4.1454450178146365 and perplexity is 63.14571661876993
At time: 39.05883455276489 and batch: 650, loss is 3.9932590770721434 and perplexity is 54.231345800264585
At time: 39.773438930511475 and batch: 700, loss is 4.0072818994522095 and perplexity is 54.99717935441574
At time: 40.48836708068848 and batch: 750, loss is 4.0968804359436035 and perplexity is 60.15234550613285
At time: 41.204206705093384 and batch: 800, loss is 4.058571233749389 and perplexity is 57.89153848531222
At time: 41.93042206764221 and batch: 850, loss is 4.1258968210220335 and perplexity is 61.92331848949661
At time: 42.64622473716736 and batch: 900, loss is 4.072848496437072 and perplexity is 58.723999680533126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3876714837061215 and perplexity of 80.45286489704839
finished 3 epochs...
Completing Train Step...
At time: 44.39874601364136 and batch: 50, loss is 4.151809678077698 and perplexity is 63.548899351635434
At time: 45.12968325614929 and batch: 100, loss is 4.034704608917236 and perplexity is 56.52622043049974
At time: 45.846829891204834 and batch: 150, loss is 4.036565465927124 and perplexity is 56.63150557394722
At time: 46.56458234786987 and batch: 200, loss is 3.921726484298706 and perplexity is 50.48753551338321
At time: 47.283164262771606 and batch: 250, loss is 4.069535889625549 and perplexity is 58.529792003620805
At time: 48.001686573028564 and batch: 300, loss is 4.040431971549988 and perplexity is 56.850895471495654
At time: 48.719449520111084 and batch: 350, loss is 4.0351571226119995 and perplexity is 56.55180510762956
At time: 49.43724489212036 and batch: 400, loss is 3.956739654541016 and perplexity is 52.28657544871409
At time: 50.15568494796753 and batch: 450, loss is 4.001576189994812 and perplexity is 54.68427494774552
At time: 50.87713122367859 and batch: 500, loss is 3.879990701675415 and perplexity is 48.42376480936901
At time: 51.594656467437744 and batch: 550, loss is 3.948048405647278 and perplexity is 51.8341089055501
At time: 52.31228804588318 and batch: 600, loss is 3.957766637802124 and perplexity is 52.34030046910694
At time: 53.03029799461365 and batch: 650, loss is 3.8052441358566282 and perplexity is 44.936219316367456
At time: 53.748576402664185 and batch: 700, loss is 3.814475245475769 and perplexity is 45.352950971285374
At time: 54.467092752456665 and batch: 750, loss is 3.9126706314086914 and perplexity is 50.03239178677646
At time: 55.185386657714844 and batch: 800, loss is 3.8806275939941406 and perplexity is 48.45461535641527
At time: 55.90254282951355 and batch: 850, loss is 3.942668104171753 and perplexity is 51.55597466689427
At time: 56.619795083999634 and batch: 900, loss is 3.8960310649871825 and perplexity is 49.20676258597417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355030216582834 and perplexity of 77.86917824175404
finished 4 epochs...
Completing Train Step...
At time: 58.398221492767334 and batch: 50, loss is 3.9819779920578005 and perplexity is 53.62299525663993
At time: 59.116793394088745 and batch: 100, loss is 3.865560975074768 and perplexity is 47.73004028685583
At time: 59.84415912628174 and batch: 150, loss is 3.8720491695404053 and perplexity is 48.04072888419645
At time: 60.56290674209595 and batch: 200, loss is 3.7570877742767332 and perplexity is 42.82353241524715
At time: 61.280471086502075 and batch: 250, loss is 3.9054517459869387 and perplexity is 49.672514203447655
At time: 61.99739670753479 and batch: 300, loss is 3.879731464385986 and perplexity is 48.41121319082997
At time: 62.71354866027832 and batch: 350, loss is 3.8757904386520385 and perplexity is 48.220798814266956
At time: 63.43184685707092 and batch: 400, loss is 3.8074706745147706 and perplexity is 45.036383013618234
At time: 64.1512680053711 and batch: 450, loss is 3.849367699623108 and perplexity is 46.96335889200885
At time: 64.86903357505798 and batch: 500, loss is 3.7308451795578 and perplexity is 41.714349384718716
At time: 65.58670115470886 and batch: 550, loss is 3.800004382133484 and perplexity is 44.70138038028736
At time: 66.30353713035583 and batch: 600, loss is 3.8126217555999755 and perplexity is 45.26896759103847
At time: 67.02192211151123 and batch: 650, loss is 3.66106942653656 and perplexity is 38.902924452905836
At time: 67.74069046974182 and batch: 700, loss is 3.6697495555877686 and perplexity is 39.242076670696534
At time: 68.45947027206421 and batch: 750, loss is 3.7710138463974 and perplexity is 43.42406786068936
At time: 69.1778154373169 and batch: 800, loss is 3.741225323677063 and perplexity is 42.14960544535739
At time: 69.89632487297058 and batch: 850, loss is 3.7997288274765015 and perplexity is 44.68906440369032
At time: 70.6149320602417 and batch: 900, loss is 3.75200355052948 and perplexity is 42.60636053782061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3545160162938785 and perplexity of 77.82914818041799
finished 5 epochs...
Completing Train Step...
At time: 72.37017679214478 and batch: 50, loss is 3.846587872505188 and perplexity is 46.832990158633784
At time: 73.09971761703491 and batch: 100, loss is 3.7344112491607664 and perplexity is 41.86337121124594
At time: 73.817049741745 and batch: 150, loss is 3.7394926071166994 and perplexity is 42.076635362481476
At time: 74.53270649909973 and batch: 200, loss is 3.626396517753601 and perplexity is 37.57716371024961
At time: 75.24946427345276 and batch: 250, loss is 3.770862579345703 and perplexity is 43.41749972675504
At time: 75.9676022529602 and batch: 300, loss is 3.7539977407455445 and perplexity is 42.691410499862826
At time: 76.6854076385498 and batch: 350, loss is 3.7486321783065795 and perplexity is 42.462960500661055
At time: 77.41208744049072 and batch: 400, loss is 3.6830939435958863 and perplexity is 39.76924773275976
At time: 78.12862491607666 and batch: 450, loss is 3.722990288734436 and perplexity is 41.3879712350078
At time: 78.84650659561157 and batch: 500, loss is 3.6107183265686036 and perplexity is 36.99261605209313
At time: 79.57774829864502 and batch: 550, loss is 3.6781110382080078 and perplexity is 39.57157423713198
At time: 80.29685926437378 and batch: 600, loss is 3.6915084457397462 and perplexity is 40.1052980181883
At time: 81.01428985595703 and batch: 650, loss is 3.542051773071289 and perplexity is 34.53771008609529
At time: 81.73158550262451 and batch: 700, loss is 3.548369312286377 and perplexity is 34.756594100139665
At time: 82.44896602630615 and batch: 750, loss is 3.653792691230774 and perplexity is 38.62086564727964
At time: 83.16688060760498 and batch: 800, loss is 3.6278592348098755 and perplexity is 37.63216868708679
At time: 83.88538694381714 and batch: 850, loss is 3.6838728618621825 and perplexity is 39.800236793661675
At time: 84.60305595397949 and batch: 900, loss is 3.6363645648956298 and perplexity is 37.95360773847395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365939623688998 and perplexity of 78.72333551145431
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 86.38382363319397 and batch: 50, loss is 3.763528642654419 and perplexity is 43.100243323664145
At time: 87.11351680755615 and batch: 100, loss is 3.663243889808655 and perplexity is 38.98760947216985
At time: 87.83165335655212 and batch: 150, loss is 3.6668407821655276 and perplexity is 39.12809621307152
At time: 88.54993224143982 and batch: 200, loss is 3.5428739166259766 and perplexity is 34.56611671740061
At time: 89.26755118370056 and batch: 250, loss is 3.682507677078247 and perplexity is 39.74593918755976
At time: 89.98620772361755 and batch: 300, loss is 3.649978451728821 and perplexity is 38.47383699542694
At time: 90.70511960983276 and batch: 350, loss is 3.6330309009552 and perplexity is 37.827293825995085
At time: 91.42328882217407 and batch: 400, loss is 3.560858383178711 and perplexity is 35.19339360025614
At time: 92.14259576797485 and batch: 450, loss is 3.5889406108856203 and perplexity is 36.19571026653977
At time: 92.86145520210266 and batch: 500, loss is 3.4626792764663694 and perplexity is 31.902337292572483
At time: 93.58019137382507 and batch: 550, loss is 3.5119511318206786 and perplexity is 33.51359347235532
At time: 94.29866051673889 and batch: 600, loss is 3.5183222341537475 and perplexity is 33.72779362692041
At time: 95.01743054389954 and batch: 650, loss is 3.357369899749756 and perplexity is 28.713571907435917
At time: 95.74657130241394 and batch: 700, loss is 3.3444758224487305 and perplexity is 28.345713585073074
At time: 96.46463465690613 and batch: 750, loss is 3.4355353260040284 and perplexity is 31.0480289348066
At time: 97.18133473396301 and batch: 800, loss is 3.3903926610946655 and perplexity is 29.67760322294983
At time: 97.89753007888794 and batch: 850, loss is 3.4304429054260255 and perplexity is 30.890321211197133
At time: 98.61562442779541 and batch: 900, loss is 3.3759782218933108 and perplexity is 29.25288560262921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316733582379067 and perplexity of 74.94343150723567
finished 7 epochs...
Completing Train Step...
At time: 100.38466787338257 and batch: 50, loss is 3.6706644916534423 and perplexity is 39.27799709187313
At time: 101.117014169693 and batch: 100, loss is 3.5594734716415406 and perplexity is 35.14468759796397
At time: 101.83594346046448 and batch: 150, loss is 3.5591512298583985 and perplexity is 35.13336433567653
At time: 102.55477333068848 and batch: 200, loss is 3.44350989818573 and perplexity is 31.296613543229714
At time: 103.27391076087952 and batch: 250, loss is 3.5808664655685423 and perplexity is 35.90463750517469
At time: 103.99282431602478 and batch: 300, loss is 3.555640850067139 and perplexity is 35.01024910055695
At time: 104.71135520935059 and batch: 350, loss is 3.542306637763977 and perplexity is 34.54651365077378
At time: 105.43041157722473 and batch: 400, loss is 3.475004696846008 and perplexity is 32.29798023489396
At time: 106.1497950553894 and batch: 450, loss is 3.5073067283630373 and perplexity is 33.35830371623395
At time: 106.86856341362 and batch: 500, loss is 3.3877098894119264 and perplexity is 29.598091692741363
At time: 107.58780527114868 and batch: 550, loss is 3.4386849355697633 and perplexity is 31.14597226439396
At time: 108.30609560012817 and batch: 600, loss is 3.450885763168335 and perplexity is 31.528306556924512
At time: 109.02505850791931 and batch: 650, loss is 3.2968090629577635 and perplexity is 27.026262081632105
At time: 109.74145531654358 and batch: 700, loss is 3.2902017068862914 and perplexity is 26.848278592442377
At time: 110.46048498153687 and batch: 750, loss is 3.3881159353256227 and perplexity is 29.61011231722398
At time: 111.17876529693604 and batch: 800, loss is 3.3479150485992433 and perplexity is 28.443368737308795
At time: 111.89886736869812 and batch: 850, loss is 3.3961895942687987 and perplexity is 29.850141920078475
At time: 112.61828207969666 and batch: 900, loss is 3.3508325958251954 and perplexity is 28.52647478282089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328198158577697 and perplexity of 75.80757021544335
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 114.37406182289124 and batch: 50, loss is 3.6373720932006837 and perplexity is 37.99186634262627
At time: 115.1033923625946 and batch: 100, loss is 3.5430327892303466 and perplexity is 34.571608762642676
At time: 115.81963109970093 and batch: 150, loss is 3.5486854314804077 and perplexity is 34.76758306347346
At time: 116.53620028495789 and batch: 200, loss is 3.427019271850586 and perplexity is 30.784744900766942
At time: 117.25226926803589 and batch: 250, loss is 3.5670055150985718 and perplexity is 35.41039832843662
At time: 117.96781420707703 and batch: 300, loss is 3.539391527175903 and perplexity is 34.44595338640739
At time: 118.6865086555481 and batch: 350, loss is 3.516214466094971 and perplexity is 33.65677812921108
At time: 119.40246105194092 and batch: 400, loss is 3.449983010292053 and perplexity is 31.499857130828662
At time: 120.11836862564087 and batch: 450, loss is 3.4762300729751585 and perplexity is 32.337581667263244
At time: 120.83461380004883 and batch: 500, loss is 3.3537216186523438 and perplexity is 28.609007581811998
At time: 121.54955101013184 and batch: 550, loss is 3.3941718530654907 and perplexity is 29.789972782089965
At time: 122.26466774940491 and batch: 600, loss is 3.4051119089126587 and perplexity is 30.117665970584518
At time: 122.98084688186646 and batch: 650, loss is 3.2476035261154177 and perplexity is 25.72860804006699
At time: 123.69823908805847 and batch: 700, loss is 3.227802257537842 and perplexity is 25.224159801989348
At time: 124.41422820091248 and batch: 750, loss is 3.3242556190490724 and perplexity is 27.778313297468813
At time: 125.13121438026428 and batch: 800, loss is 3.274322075843811 and perplexity is 26.425305054273224
At time: 125.84800410270691 and batch: 850, loss is 3.321075506210327 and perplexity is 27.690115440610906
At time: 126.56392025947571 and batch: 900, loss is 3.2826504802703855 and perplexity is 26.646304691620635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319372307764341 and perplexity of 75.14144778280304
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 128.33093810081482 and batch: 50, loss is 3.62468008518219 and perplexity is 37.512720364666365
At time: 129.05964970588684 and batch: 100, loss is 3.5279266119003294 and perplexity is 34.05328868413557
At time: 129.78744435310364 and batch: 150, loss is 3.534772443771362 and perplexity is 34.287211557881484
At time: 130.50242280960083 and batch: 200, loss is 3.408872480392456 and perplexity is 30.23113883395165
At time: 131.21913719177246 and batch: 250, loss is 3.5495060539245604 and perplexity is 34.796125832280865
At time: 131.96474075317383 and batch: 300, loss is 3.524946160316467 and perplexity is 33.95194560503151
At time: 132.69175243377686 and batch: 350, loss is 3.499328446388245 and perplexity is 33.093220622926964
At time: 133.40706157684326 and batch: 400, loss is 3.437806477546692 and perplexity is 31.11862384914805
At time: 134.12136363983154 and batch: 450, loss is 3.4611596059799195 and perplexity is 31.853893071090084
At time: 134.83751940727234 and batch: 500, loss is 3.337163290977478 and perplexity is 28.13919068529364
At time: 135.55328607559204 and batch: 550, loss is 3.374115114212036 and perplexity is 29.1984350661205
At time: 136.27082228660583 and batch: 600, loss is 3.3893950748443604 and perplexity is 29.648012016377052
At time: 136.98843502998352 and batch: 650, loss is 3.230231137275696 and perplexity is 25.28550071733348
At time: 137.70641469955444 and batch: 700, loss is 3.2084205293655397 and perplexity is 24.739979277814314
At time: 138.42464661598206 and batch: 750, loss is 3.30117871761322 and perplexity is 27.144615907911255
At time: 139.14411973953247 and batch: 800, loss is 3.2516308641433715 and perplexity is 25.832434773975613
At time: 139.86169338226318 and batch: 850, loss is 3.2977179956436156 and perplexity is 27.0508383019875
At time: 140.57787466049194 and batch: 900, loss is 3.258977560997009 and perplexity is 26.022916690946996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315670483732877 and perplexity of 74.86380158139234
finished 10 epochs...
Completing Train Step...
At time: 142.34754705429077 and batch: 50, loss is 3.6147149181365967 and perplexity is 37.14075626025459
At time: 143.06710028648376 and batch: 100, loss is 3.510559358596802 and perplexity is 33.466982593734905
At time: 143.7861831188202 and batch: 150, loss is 3.5161647605895996 and perplexity is 33.65510524362115
At time: 144.50483322143555 and batch: 200, loss is 3.393359966278076 and perplexity is 29.765796512315745
At time: 145.22324061393738 and batch: 250, loss is 3.5348710870742797 and perplexity is 34.29059392849875
At time: 145.9433491230011 and batch: 300, loss is 3.509717583656311 and perplexity is 33.438822780229124
At time: 146.66202592849731 and batch: 350, loss is 3.4847947692871095 and perplexity is 32.61573267286299
At time: 147.38150882720947 and batch: 400, loss is 3.424621500968933 and perplexity is 30.711018560603527
At time: 148.10079264640808 and batch: 450, loss is 3.4494045209884643 and perplexity is 31.481640070109318
At time: 148.8195939064026 and batch: 500, loss is 3.3267623710632326 and perplexity is 27.848033990011174
At time: 149.54881262779236 and batch: 550, loss is 3.364912066459656 and perplexity is 28.93095318605619
At time: 150.26927256584167 and batch: 600, loss is 3.381427917480469 and perplexity is 29.412740107776337
At time: 150.9893298149109 and batch: 650, loss is 3.22415406703949 and perplexity is 25.132304915809264
At time: 151.7086226940155 and batch: 700, loss is 3.2043316745758057 and perplexity is 24.639027624021484
At time: 152.4304440021515 and batch: 750, loss is 3.299520878791809 and perplexity is 27.09965179179385
At time: 153.15091109275818 and batch: 800, loss is 3.2520849990844725 and perplexity is 25.844168849445353
At time: 153.8690848350525 and batch: 850, loss is 3.3009257411956785 and perplexity is 27.137749828738002
At time: 154.5888545513153 and batch: 900, loss is 3.264553608894348 and perplexity is 26.168427030133756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315176767845676 and perplexity of 74.82684925590105
finished 11 epochs...
Completing Train Step...
At time: 156.3476161956787 and batch: 50, loss is 3.60760196685791 and perplexity is 36.87751319793753
At time: 157.07826948165894 and batch: 100, loss is 3.5022568130493164 and perplexity is 33.190271738178005
At time: 157.7956874370575 and batch: 150, loss is 3.506742014884949 and perplexity is 33.33947115052155
At time: 158.51467299461365 and batch: 200, loss is 3.384166746139526 and perplexity is 29.49340697926992
At time: 159.23331022262573 and batch: 250, loss is 3.5255220556259155 and perplexity is 33.97150400250622
At time: 159.95309281349182 and batch: 300, loss is 3.5004762172698975 and perplexity is 33.131225864420564
At time: 160.67121863365173 and batch: 350, loss is 3.475771813392639 and perplexity is 32.322766055546005
At time: 161.3899405002594 and batch: 400, loss is 3.4162492418289183 and perplexity is 30.45497129740855
At time: 162.1081840991974 and batch: 450, loss is 3.44178955078125 and perplexity is 31.242818781458556
At time: 162.8264377117157 and batch: 500, loss is 3.319884090423584 and perplexity is 27.65714464480163
At time: 163.544606924057 and batch: 550, loss is 3.3586321306228637 and perplexity is 28.74983794761746
At time: 164.2631175518036 and batch: 600, loss is 3.3760428333282473 and perplexity is 29.254775734605442
At time: 164.9823248386383 and batch: 650, loss is 3.219765386581421 and perplexity is 25.02224893726471
At time: 165.70089650154114 and batch: 700, loss is 3.201188154220581 and perplexity is 24.561695949679446
At time: 166.4192385673523 and batch: 750, loss is 3.2977623462677004 and perplexity is 27.052038050152806
At time: 167.13794422149658 and batch: 800, loss is 3.251495041847229 and perplexity is 25.828926391633328
At time: 167.86683750152588 and batch: 850, loss is 3.3016311264038087 and perplexity is 27.156899149078782
At time: 168.58553338050842 and batch: 900, loss is 3.266245374679565 and perplexity is 26.21273534871718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315547577322346 and perplexity of 74.85460090568388
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 170.33914351463318 and batch: 50, loss is 3.6050865554809572 and perplexity is 36.784867651366284
At time: 171.08817505836487 and batch: 100, loss is 3.5022358894348145 and perplexity is 33.189577284992204
At time: 171.80758213996887 and batch: 150, loss is 3.5082002782821657 and perplexity is 33.38812434694879
At time: 172.52558493614197 and batch: 200, loss is 3.3843916130065916 and perplexity is 29.500039815020983
At time: 173.24346828460693 and batch: 250, loss is 3.5242065715789797 and perplexity is 33.9268444118703
At time: 173.96227955818176 and batch: 300, loss is 3.4993405723571778 and perplexity is 33.09362191272513
At time: 174.67998218536377 and batch: 350, loss is 3.47468044757843 and perplexity is 32.287509336140914
At time: 175.39813137054443 and batch: 400, loss is 3.4168317317962646 and perplexity is 30.472716180251496
At time: 176.1158413887024 and batch: 450, loss is 3.4393097734451294 and perplexity is 31.165439528838565
At time: 176.83387660980225 and batch: 500, loss is 3.3160298585891725 and perplexity is 27.550752758665123
At time: 177.5518991947174 and batch: 550, loss is 3.3532605504989625 and perplexity is 28.595819919955257
At time: 178.270015001297 and batch: 600, loss is 3.370707368850708 and perplexity is 29.099103578799248
At time: 178.98875951766968 and batch: 650, loss is 3.213916687965393 and perplexity is 24.876328482876765
At time: 179.70714139938354 and batch: 700, loss is 3.194766712188721 and perplexity is 24.404479760489764
At time: 180.42460298538208 and batch: 750, loss is 3.289673533439636 and perplexity is 26.834101788836627
At time: 181.14059257507324 and batch: 800, loss is 3.24361807346344 and perplexity is 25.626271954191836
At time: 181.85783863067627 and batch: 850, loss is 3.2912881803512573 and perplexity is 26.87746438663758
At time: 182.57587504386902 and batch: 900, loss is 3.254974851608276 and perplexity is 25.918962705575655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312724910370291 and perplexity of 74.64360921722003
finished 13 epochs...
Completing Train Step...
At time: 184.35586166381836 and batch: 50, loss is 3.601683111190796 and perplexity is 36.65988520962655
At time: 185.07442569732666 and batch: 100, loss is 3.496909441947937 and perplexity is 33.0132647211258
At time: 185.801771402359 and batch: 150, loss is 3.5037167739868162 and perplexity is 33.23876362793986
At time: 186.51927781105042 and batch: 200, loss is 3.3806700706481934 and perplexity is 29.390458200060138
At time: 187.2371368408203 and batch: 250, loss is 3.5209639072418213 and perplexity is 33.81700921899696
At time: 187.95600986480713 and batch: 300, loss is 3.495372576713562 and perplexity is 32.96256675025598
At time: 188.67519569396973 and batch: 350, loss is 3.4706078290939333 and perplexity is 32.156282029557794
At time: 189.39366960525513 and batch: 400, loss is 3.413197603225708 and perplexity is 30.36217539313303
At time: 190.11260771751404 and batch: 450, loss is 3.4363817739486695 and perplexity is 31.07432060077716
At time: 190.83212089538574 and batch: 500, loss is 3.313683581352234 and perplexity is 27.48618682901217
At time: 191.55104565620422 and batch: 550, loss is 3.3511451053619385 and perplexity is 28.535390971364595
At time: 192.27013778686523 and batch: 600, loss is 3.369248390197754 and perplexity is 29.056679563249585
At time: 192.98688530921936 and batch: 650, loss is 3.212498707771301 and perplexity is 24.841079338986468
At time: 193.70494508743286 and batch: 700, loss is 3.1941832637786867 and perplexity is 24.39024515855753
At time: 194.4230616092682 and batch: 750, loss is 3.289620809555054 and perplexity is 26.832687028047214
At time: 195.1411235332489 and batch: 800, loss is 3.2437990379333494 and perplexity is 25.630909818543458
At time: 195.859965801239 and batch: 850, loss is 3.292600326538086 and perplexity is 26.912754697003866
At time: 196.5799102783203 and batch: 900, loss is 3.2569156455993653 and perplexity is 25.969314918480965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312344903815283 and perplexity of 74.61524954519987
finished 14 epochs...
Completing Train Step...
At time: 198.34121823310852 and batch: 50, loss is 3.599508481025696 and perplexity is 36.580250137198895
At time: 199.07324361801147 and batch: 100, loss is 3.4941645765304568 and perplexity is 32.92277200445542
At time: 199.79236125946045 and batch: 150, loss is 3.500939416885376 and perplexity is 33.14657579026671
At time: 200.51217103004456 and batch: 200, loss is 3.3781163024902345 and perplexity is 29.31549754057309
At time: 201.23282265663147 and batch: 250, loss is 3.5184768295288085 and perplexity is 33.73300819088949
At time: 201.95167326927185 and batch: 300, loss is 3.492788543701172 and perplexity is 32.877500344135015
At time: 202.6713273525238 and batch: 350, loss is 3.4680973052978517 and perplexity is 32.075654169757364
At time: 203.40023112297058 and batch: 400, loss is 3.4109594345092775 and perplexity is 30.29429571342951
At time: 204.11922073364258 and batch: 450, loss is 3.434314546585083 and perplexity is 31.01014926614846
At time: 204.83643555641174 and batch: 500, loss is 3.31196937084198 and perplexity is 27.439110079920102
At time: 205.5548095703125 and batch: 550, loss is 3.3495920944213866 and perplexity is 28.491109590627488
At time: 206.28569436073303 and batch: 600, loss is 3.3680421495437622 and perplexity is 29.021651345567708
At time: 207.01136326789856 and batch: 650, loss is 3.2114531326293947 and perplexity is 24.8151196976743
At time: 207.73563694953918 and batch: 700, loss is 3.1936447525024416 and perplexity is 24.377114272391722
At time: 208.4544711112976 and batch: 750, loss is 3.2894640731811524 and perplexity is 26.828481699552984
At time: 209.17471480369568 and batch: 800, loss is 3.2439395666122435 and perplexity is 25.634511949534538
At time: 209.89343786239624 and batch: 850, loss is 3.2932482719421388 and perplexity is 26.93019834337515
At time: 210.6130063533783 and batch: 900, loss is 3.257843403816223 and perplexity is 25.99341934359302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312322747217466 and perplexity of 74.61359634343943
finished 15 epochs...
Completing Train Step...
At time: 212.37953066825867 and batch: 50, loss is 3.597609295845032 and perplexity is 36.5108433972213
At time: 213.10753679275513 and batch: 100, loss is 3.491962809562683 and perplexity is 32.85036347517415
At time: 213.82479000091553 and batch: 150, loss is 3.4986410903930665 and perplexity is 33.070481615123974
At time: 214.54224395751953 and batch: 200, loss is 3.3759286880493162 and perplexity is 29.25143663064425
At time: 215.2594485282898 and batch: 250, loss is 3.516280965805054 and perplexity is 33.659016369619344
At time: 215.97791957855225 and batch: 300, loss is 3.490597348213196 and perplexity is 32.805538184098076
At time: 216.69240427017212 and batch: 350, loss is 3.465977234840393 and perplexity is 32.007723557234755
At time: 217.40776872634888 and batch: 400, loss is 3.409043073654175 and perplexity is 30.236296502450042
At time: 218.12426090240479 and batch: 450, loss is 3.4325322675704957 and perplexity is 30.95492975078105
At time: 218.84090495109558 and batch: 500, loss is 3.3104462099075316 and perplexity is 27.397347712847484
At time: 219.55963325500488 and batch: 550, loss is 3.348206810951233 and perplexity is 28.451668652212575
At time: 220.27820920944214 and batch: 600, loss is 3.366910014152527 and perplexity is 28.98881349891892
At time: 220.99650168418884 and batch: 650, loss is 3.2105019807815554 and perplexity is 24.791527972154764
At time: 221.72443556785583 and batch: 700, loss is 3.193050365447998 and perplexity is 24.362629136558592
At time: 222.44117164611816 and batch: 750, loss is 3.2891824531555174 and perplexity is 26.820927325630425
At time: 223.15646195411682 and batch: 800, loss is 3.243916540145874 and perplexity is 25.633921684103118
At time: 223.87517857551575 and batch: 850, loss is 3.2935599851608277 and perplexity is 26.93859415065477
At time: 224.59320211410522 and batch: 900, loss is 3.258353142738342 and perplexity is 26.00667257870933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3124193165400255 and perplexity of 74.62080207581282
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 226.37435603141785 and batch: 50, loss is 3.5968137693405153 and perplexity is 36.481809603703894
At time: 227.09275317192078 and batch: 100, loss is 3.4916799592971803 and perplexity is 32.84107305510416
At time: 227.81140565872192 and batch: 150, loss is 3.4990686273574827 and perplexity is 33.08462349131748
At time: 228.52761030197144 and batch: 200, loss is 3.3763887166976927 and perplexity is 29.264896225162687
At time: 229.24438905715942 and batch: 250, loss is 3.5163905096054076 and perplexity is 33.66270370814749
At time: 229.95942163467407 and batch: 300, loss is 3.4905690336227417 and perplexity is 32.80460932187001
At time: 230.67734694480896 and batch: 350, loss is 3.465147647857666 and perplexity is 31.98118137747059
At time: 231.39424991607666 and batch: 400, loss is 3.4087214040756226 and perplexity is 30.226571969824125
At time: 232.11094760894775 and batch: 450, loss is 3.4321747398376465 and perplexity is 30.94386448311465
At time: 232.8263771533966 and batch: 500, loss is 3.30955915927887 and perplexity is 27.373055654071013
At time: 233.5421917438507 and batch: 550, loss is 3.346569972038269 and perplexity is 28.40513594748415
At time: 234.25886607170105 and batch: 600, loss is 3.3650682163238526 and perplexity is 28.935471103194164
At time: 234.97757387161255 and batch: 650, loss is 3.208334002494812 and perplexity is 24.737838697435738
At time: 235.6959617137909 and batch: 700, loss is 3.1902760601043703 and perplexity is 24.29513343449524
At time: 236.41475200653076 and batch: 750, loss is 3.286185173988342 and perplexity is 26.740657873983512
At time: 237.13468503952026 and batch: 800, loss is 3.2416412305831908 and perplexity is 25.575662880506655
At time: 237.85366439819336 and batch: 850, loss is 3.290071039199829 and perplexity is 26.844770619187567
At time: 238.58511519432068 and batch: 900, loss is 3.2541747379302977 and perplexity is 25.898232883209026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311252541738014 and perplexity of 74.53378717751181
finished 17 epochs...
Completing Train Step...
At time: 240.35788488388062 and batch: 50, loss is 3.5958681964874266 and perplexity is 36.44732969911102
At time: 241.09136652946472 and batch: 100, loss is 3.4904833126068113 and perplexity is 32.80179739795395
At time: 241.8135325908661 and batch: 150, loss is 3.4980268764495848 and perplexity is 33.050175500985915
At time: 242.53514218330383 and batch: 200, loss is 3.375355396270752 and perplexity is 29.234671828533486
At time: 243.25771641731262 and batch: 250, loss is 3.515569887161255 and perplexity is 33.63509066944298
At time: 243.97941398620605 and batch: 300, loss is 3.4895125913619993 and perplexity is 32.769971445914756
At time: 244.70031213760376 and batch: 350, loss is 3.4643532848358154 and perplexity is 31.955786797182355
At time: 245.4218783378601 and batch: 400, loss is 3.408017110824585 and perplexity is 30.205291094052033
At time: 246.1388463973999 and batch: 450, loss is 3.4314243936538698 and perplexity is 30.920654581308032
At time: 246.85796809196472 and batch: 500, loss is 3.3089725589752197 and perplexity is 27.357003319924846
At time: 247.5777177810669 and batch: 550, loss is 3.346168327331543 and perplexity is 28.393729465816833
At time: 248.31005191802979 and batch: 600, loss is 3.3648147201538086 and perplexity is 28.92813700171368
At time: 249.04610443115234 and batch: 650, loss is 3.2081047677993775 and perplexity is 24.732168576435278
At time: 249.76513385772705 and batch: 700, loss is 3.190256772041321 and perplexity is 24.294664832948985
At time: 250.4843897819519 and batch: 750, loss is 3.286239767074585 and perplexity is 26.742117768874735
At time: 251.2028329372406 and batch: 800, loss is 3.2418356370925903 and perplexity is 25.58063543918606
At time: 251.92179346084595 and batch: 850, loss is 3.2904245233535767 and perplexity is 26.854261497551175
At time: 252.64084124565125 and batch: 900, loss is 3.2546060848236085 and perplexity is 25.90940641516634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310759243899828 and perplexity of 74.497028888563
finished 18 epochs...
Completing Train Step...
At time: 254.4295449256897 and batch: 50, loss is 3.5951829814910887 and perplexity is 36.42236399663841
At time: 255.1630039215088 and batch: 100, loss is 3.489663166999817 and perplexity is 32.77490617678229
At time: 255.88546562194824 and batch: 150, loss is 3.4972563648223876 and perplexity is 33.024719764711286
At time: 256.6073808670044 and batch: 200, loss is 3.3745938634872434 and perplexity is 29.212417142432987
At time: 257.3291401863098 and batch: 250, loss is 3.514914283752441 and perplexity is 33.61304661621213
At time: 258.0601456165314 and batch: 300, loss is 3.488752603530884 and perplexity is 32.745076127657576
At time: 258.78092408180237 and batch: 350, loss is 3.46368971824646 and perplexity is 31.934589038567193
At time: 259.50305342674255 and batch: 400, loss is 3.4074298620223997 and perplexity is 30.187558280333807
At time: 260.22441959381104 and batch: 450, loss is 3.430850377082825 and perplexity is 30.902910706317154
At time: 260.9457724094391 and batch: 500, loss is 3.308512597084045 and perplexity is 27.344423034388704
At time: 261.66678524017334 and batch: 550, loss is 3.3458005571365357 and perplexity is 28.38328901835459
At time: 262.3899838924408 and batch: 600, loss is 3.364557538032532 and perplexity is 28.920698158684715
At time: 263.1117208003998 and batch: 650, loss is 3.2078800106048586 and perplexity is 24.72661046824747
At time: 263.83369755744934 and batch: 700, loss is 3.1901990795135498 and perplexity is 24.293263252754162
At time: 264.5552484989166 and batch: 750, loss is 3.2862694311141967 and perplexity is 26.742911059881585
At time: 265.27726697921753 and batch: 800, loss is 3.2419372272491453 and perplexity is 25.583234311952815
At time: 265.9994127750397 and batch: 850, loss is 3.290674886703491 and perplexity is 26.86098566212624
At time: 266.732280254364 and batch: 900, loss is 3.2549183988571166 and perplexity is 25.91749955012369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310537677921661 and perplexity of 74.4805247099365
finished 19 epochs...
Completing Train Step...
At time: 268.53523683547974 and batch: 50, loss is 3.594606614112854 and perplexity is 36.40137738277335
At time: 269.25859117507935 and batch: 100, loss is 3.488997473716736 and perplexity is 32.75309540233527
At time: 269.98117685317993 and batch: 150, loss is 3.496596598625183 and perplexity is 33.00293835704673
At time: 270.70591163635254 and batch: 200, loss is 3.3739478826522826 and perplexity is 29.19355257454675
At time: 271.429940700531 and batch: 250, loss is 3.5143250608444214 and perplexity is 33.59324687294152
At time: 272.15379905700684 and batch: 300, loss is 3.488117380142212 and perplexity is 32.724282294501236
At time: 272.8781199455261 and batch: 350, loss is 3.4630948305130005 and perplexity is 31.915597192843194
At time: 273.60170698165894 and batch: 400, loss is 3.4069011545181276 and perplexity is 30.17160211018048
At time: 274.32466435432434 and batch: 450, loss is 3.430352602005005 and perplexity is 30.887531835462323
At time: 275.0480098724365 and batch: 500, loss is 3.3081039571762085 and perplexity is 27.333251294644896
At time: 275.7819998264313 and batch: 550, loss is 3.3454479932785035 and perplexity is 28.373283860306834
At time: 276.50425601005554 and batch: 600, loss is 3.3642973279953003 and perplexity is 28.91317368175475
At time: 277.22532415390015 and batch: 650, loss is 3.2076552963256835 and perplexity is 24.721054670056624
At time: 277.94805932044983 and batch: 700, loss is 3.1901122331619263 and perplexity is 24.291153563082382
At time: 278.6717483997345 and batch: 750, loss is 3.286268949508667 and perplexity is 26.74289818035084
At time: 279.3994903564453 and batch: 800, loss is 3.2419896221160887 and perplexity is 25.584574777227015
At time: 280.1269564628601 and batch: 850, loss is 3.290855689048767 and perplexity is 26.86584263039277
At time: 280.85622119903564 and batch: 900, loss is 3.2551514673233033 and perplexity is 25.923540805977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310438600305009 and perplexity of 74.47314572261351
Finished Training.
Improved accuracyfrom -10000000 to -74.47314572261351
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
292.09096121788025


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.9965496063232422 and batch: 50, loss is 7.106841974258423 and perplexity is 1220.2877544981445
At time: 1.8006222248077393 and batch: 100, loss is 6.177487821578979 and perplexity is 481.78011719792727
At time: 2.5907914638519287 and batch: 150, loss is 5.96916934967041 and perplexity is 391.1806014635893
At time: 3.385711669921875 and batch: 200, loss is 5.772118225097656 and perplexity is 321.2174233321887
At time: 4.179098844528198 and batch: 250, loss is 5.788057146072387 and perplexity is 326.37830262266334
At time: 4.97106671333313 and batch: 300, loss is 5.68770318031311 and perplexity is 295.21478622592133
At time: 5.762625694274902 and batch: 350, loss is 5.653061132431031 and perplexity is 285.1630529910888
At time: 6.553735256195068 and batch: 400, loss is 5.518397226333618 and perplexity is 249.23524919629787
At time: 7.3459625244140625 and batch: 450, loss is 5.516087923049927 and perplexity is 248.6603534766914
At time: 8.136443376541138 and batch: 500, loss is 5.457304048538208 and perplexity is 234.4644667152778
At time: 8.932590007781982 and batch: 550, loss is 5.504756956100464 and perplexity is 245.85869396104056
At time: 9.72665023803711 and batch: 600, loss is 5.419356956481933 and perplexity is 225.73391908750253
At time: 10.51951003074646 and batch: 650, loss is 5.321388921737671 and perplexity is 204.66795243964003
At time: 11.311652660369873 and batch: 700, loss is 5.411260709762574 and perplexity is 223.91369999986674
At time: 12.103830099105835 and batch: 750, loss is 5.387823457717896 and perplexity is 218.7267989604977
At time: 12.897402048110962 and batch: 800, loss is 5.370534133911133 and perplexity is 214.9776639037618
At time: 13.6893789768219 and batch: 850, loss is 5.399139471054077 and perplexity is 221.215971527561
At time: 14.48101019859314 and batch: 900, loss is 5.2987604999542235 and perplexity is 200.08864632085982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.216380916229666 and perplexity of 184.2661014192729
finished 1 epochs...
Completing Train Step...
At time: 16.321751832962036 and batch: 50, loss is 5.130740098953247 and perplexity is 169.14225373771876
At time: 17.047874927520752 and batch: 100, loss is 4.989387674331665 and perplexity is 146.84647809272488
At time: 17.7740797996521 and batch: 150, loss is 4.957626390457153 and perplexity is 142.2557352729789
At time: 18.499159336090088 and batch: 200, loss is 4.82511116027832 and perplexity is 124.60031821945728
At time: 19.224076986312866 and batch: 250, loss is 4.910087032318115 and perplexity is 135.65121993485266
At time: 19.9500515460968 and batch: 300, loss is 4.836186685562134 and perplexity is 125.98800267615262
At time: 20.674052953720093 and batch: 350, loss is 4.820679721832275 and perplexity is 124.04938120224176
At time: 21.398592472076416 and batch: 400, loss is 4.682194080352783 and perplexity is 108.00678836201784
At time: 22.125117778778076 and batch: 450, loss is 4.698119201660156 and perplexity is 109.74057833454226
At time: 22.85342574119568 and batch: 500, loss is 4.601211137771607 and perplexity is 99.6048778482739
At time: 23.576318502426147 and batch: 550, loss is 4.665529527664185 and perplexity is 106.22181772335313
At time: 24.300646781921387 and batch: 600, loss is 4.608549451828003 and perplexity is 100.33849819957094
At time: 25.02753758430481 and batch: 650, loss is 4.472919082641601 and perplexity is 87.61209705484464
At time: 25.754069805145264 and batch: 700, loss is 4.516062021255493 and perplexity is 91.47466248990926
At time: 26.480729818344116 and batch: 750, loss is 4.544309062957764 and perplexity is 94.09539075297079
At time: 27.20761513710022 and batch: 800, loss is 4.50386812210083 and perplexity is 90.36600285945708
At time: 27.933202266693115 and batch: 850, loss is 4.554906949996949 and perplexity is 95.0979059634319
At time: 28.657973766326904 and batch: 900, loss is 4.486407976150513 and perplexity is 88.8018937825256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.603274828767123 and perplexity of 99.810643783426
finished 2 epochs...
Completing Train Step...
At time: 30.464820623397827 and batch: 50, loss is 4.526066026687622 and perplexity is 92.39436821053053
At time: 31.20908212661743 and batch: 100, loss is 4.400536732673645 and perplexity is 81.49459774183975
At time: 31.93943476676941 and batch: 150, loss is 4.396643009185791 and perplexity is 81.17789728461031
At time: 32.665144205093384 and batch: 200, loss is 4.287340984344483 and perplexity is 72.77270723884095
At time: 33.38967275619507 and batch: 250, loss is 4.419589939117432 and perplexity is 83.06221780759394
At time: 34.12437915802002 and batch: 300, loss is 4.382872819900513 and perplexity is 80.06772346738829
At time: 34.84869599342346 and batch: 350, loss is 4.378885908126831 and perplexity is 79.74913603001463
At time: 35.57221484184265 and batch: 400, loss is 4.281781196594238 and perplexity is 72.36922909832748
At time: 36.294822216033936 and batch: 450, loss is 4.314306144714355 and perplexity is 74.76173162066794
At time: 37.01566433906555 and batch: 500, loss is 4.197006540298462 and perplexity is 66.48700667987401
At time: 37.74111032485962 and batch: 550, loss is 4.270019912719727 and perplexity is 71.52305982368357
At time: 38.46302008628845 and batch: 600, loss is 4.258587131500244 and perplexity is 70.71000890126489
At time: 39.181127071380615 and batch: 650, loss is 4.10987380027771 and perplexity is 60.93902659610002
At time: 39.89761137962341 and batch: 700, loss is 4.1306908988952635 and perplexity is 62.22089643648923
At time: 40.612825870513916 and batch: 750, loss is 4.207205018997192 and perplexity is 67.16854240790136
At time: 41.32934546470642 and batch: 800, loss is 4.174671649932861 and perplexity is 65.01848733100893
At time: 42.04573345184326 and batch: 850, loss is 4.236401987075806 and perplexity is 69.15857021897064
At time: 42.76175665855408 and batch: 900, loss is 4.181946907043457 and perplexity is 65.49323841737146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432319118552012 and perplexity of 84.12628969552738
finished 3 epochs...
Completing Train Step...
At time: 44.528122425079346 and batch: 50, loss is 4.2527341508865355 and perplexity is 70.29735340033493
At time: 45.25670123100281 and batch: 100, loss is 4.129175572395325 and perplexity is 62.12668286345889
At time: 45.97427177429199 and batch: 150, loss is 4.129085721969605 and perplexity is 62.12110100532495
At time: 46.69300699234009 and batch: 200, loss is 4.019760336875915 and perplexity is 55.68775792532197
At time: 47.41323161125183 and batch: 250, loss is 4.158210735321045 and perplexity is 63.95698418794678
At time: 48.133141040802 and batch: 300, loss is 4.134442892074585 and perplexity is 62.45478731964709
At time: 48.85272836685181 and batch: 350, loss is 4.131139855384827 and perplexity is 62.24883718335135
At time: 49.57230305671692 and batch: 400, loss is 4.048530864715576 and perplexity is 57.313194327886045
At time: 50.292486906051636 and batch: 450, loss is 4.085129160881042 and perplexity is 59.44961582013707
At time: 51.01290321350098 and batch: 500, loss is 3.961958122253418 and perplexity is 52.5601444391289
At time: 51.73300123214722 and batch: 550, loss is 4.034863085746765 and perplexity is 56.535179236561646
At time: 52.462064027786255 and batch: 600, loss is 4.045311493873596 and perplexity is 57.128978589700516
At time: 53.18129301071167 and batch: 650, loss is 3.888680787086487 and perplexity is 48.846405192272506
At time: 53.90224504470825 and batch: 700, loss is 3.903624248504639 and perplexity is 49.5818207051124
At time: 54.62320804595947 and batch: 750, loss is 3.996143546104431 and perplexity is 54.3880002616602
At time: 55.34362006187439 and batch: 800, loss is 3.964936456680298 and perplexity is 52.71691947514817
At time: 56.06445288658142 and batch: 850, loss is 4.0294742774963375 and perplexity is 56.231341393458464
At time: 56.78418493270874 and batch: 900, loss is 3.981422643661499 and perplexity is 53.59322407967212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3632310841181505 and perplexity of 78.51039874607861
finished 4 epochs...
Completing Train Step...
At time: 58.557297229766846 and batch: 50, loss is 4.061993627548218 and perplexity is 58.090005550138265
At time: 59.27906346321106 and batch: 100, loss is 3.9441322326660155 and perplexity is 51.63151452498852
At time: 60.00168824195862 and batch: 150, loss is 3.945367360115051 and perplexity is 51.695325424998515
At time: 60.72551918029785 and batch: 200, loss is 3.833699131011963 and perplexity is 46.23324513682847
At time: 61.44808292388916 and batch: 250, loss is 3.9764557552337645 and perplexity is 53.327692493954
At time: 62.171239614486694 and batch: 300, loss is 3.9546274948120117 and perplexity is 52.17625439853736
At time: 62.89325547218323 and batch: 350, loss is 3.9523090505599976 and perplexity is 52.05542678160497
At time: 63.61623787879944 and batch: 400, loss is 3.8796141529083252 and perplexity is 48.405534332979315
At time: 64.3355655670166 and batch: 450, loss is 3.9158382701873777 and perplexity is 50.191127607332334
At time: 65.05613255500793 and batch: 500, loss is 3.7951038885116577 and perplexity is 44.482857423341244
At time: 65.77951669692993 and batch: 550, loss is 3.86231915473938 and perplexity is 47.57555860786166
At time: 66.50462627410889 and batch: 600, loss is 3.8844218826293946 and perplexity is 48.63881538583865
At time: 67.2278778553009 and batch: 650, loss is 3.727707476615906 and perplexity is 41.58366727596934
At time: 67.9633150100708 and batch: 700, loss is 3.739275350570679 and perplexity is 42.067494930959704
At time: 68.68596577644348 and batch: 750, loss is 3.8330847454071044 and perplexity is 46.20484882058771
At time: 69.40810513496399 and batch: 800, loss is 3.8084551525115966 and perplexity is 45.08074217347746
At time: 70.14236736297607 and batch: 850, loss is 3.87326105594635 and perplexity is 48.09898408267464
At time: 70.86324548721313 and batch: 900, loss is 3.828974280357361 and perplexity is 46.015315206891465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349654524293665 and perplexity of 77.45170062168502
finished 5 epochs...
Completing Train Step...
At time: 72.62905931472778 and batch: 50, loss is 3.914785137176514 and perplexity is 50.13829749744569
At time: 73.36503577232361 and batch: 100, loss is 3.7994431924819945 and perplexity is 44.67630146588245
At time: 74.08735418319702 and batch: 150, loss is 3.801722068786621 and perplexity is 44.77822932705358
At time: 74.80876183509827 and batch: 200, loss is 3.6901620864868163 and perplexity is 40.05133821190324
At time: 75.52852320671082 and batch: 250, loss is 3.8359845876693726 and perplexity is 46.33903005209389
At time: 76.24923944473267 and batch: 300, loss is 3.8121916103363036 and perplexity is 45.24949954638164
At time: 76.9696774482727 and batch: 350, loss is 3.81049232006073 and perplexity is 45.172672805781495
At time: 77.69219899177551 and batch: 400, loss is 3.742361669540405 and perplexity is 42.19752919896939
At time: 78.41384792327881 and batch: 450, loss is 3.7800537824630736 and perplexity is 43.818398333591894
At time: 79.13471865653992 and batch: 500, loss is 3.6658590745925905 and perplexity is 39.089702713382714
At time: 79.85642910003662 and batch: 550, loss is 3.727887625694275 and perplexity is 41.591159210116565
At time: 80.57834887504578 and batch: 600, loss is 3.750976424217224 and perplexity is 42.562620890759355
At time: 81.30034613609314 and batch: 650, loss is 3.5975332498550414 and perplexity is 36.50806699955807
At time: 82.02334189414978 and batch: 700, loss is 3.607855043411255 and perplexity is 36.88684721293395
At time: 82.74494814872742 and batch: 750, loss is 3.7037629222869874 and perplexity is 40.59979114111682
At time: 83.46605467796326 and batch: 800, loss is 3.6799313116073606 and perplexity is 39.64367091901539
At time: 84.18792247772217 and batch: 850, loss is 3.7462726926803587 and perplexity is 42.362887862115876
At time: 84.90920639038086 and batch: 900, loss is 3.705428333282471 and perplexity is 40.667462814631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358103712944136 and perplexity of 78.10887704574812
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 86.67397975921631 and batch: 50, loss is 3.8198678350448607 and perplexity is 45.598181440981776
At time: 87.40778493881226 and batch: 100, loss is 3.7108953189849854 and perplexity is 40.89040009459627
At time: 88.13070511817932 and batch: 150, loss is 3.7097283792495728 and perplexity is 40.8427112923198
At time: 88.86453747749329 and batch: 200, loss is 3.5823910999298096 and perplexity is 35.959420700803165
At time: 89.58650636672974 and batch: 250, loss is 3.7262573194503785 and perplexity is 41.523408126079374
At time: 90.30807161331177 and batch: 300, loss is 3.6973133420944215 and perplexity is 40.33878213637148
At time: 91.03149271011353 and batch: 350, loss is 3.6814911460876463 and perplexity is 39.705556737108644
At time: 91.75353264808655 and batch: 400, loss is 3.6074863719940184 and perplexity is 36.873250593191116
At time: 92.47601795196533 and batch: 450, loss is 3.628599762916565 and perplexity is 37.6600466866624
At time: 93.19890236854553 and batch: 500, loss is 3.506995964050293 and perplexity is 33.34793875651846
At time: 93.92239880561829 and batch: 550, loss is 3.5547248125076294 and perplexity is 34.97819308191318
At time: 94.64551901817322 and batch: 600, loss is 3.5698861265182495 and perplexity is 35.51254898377249
At time: 95.36721611022949 and batch: 650, loss is 3.401366124153137 and perplexity is 30.0050627018639
At time: 96.08930706977844 and batch: 700, loss is 3.398258285522461 and perplexity is 29.9119565632504
At time: 96.81201505661011 and batch: 750, loss is 3.4766915464401245 and perplexity is 32.352508046923035
At time: 97.54260802268982 and batch: 800, loss is 3.4382921028137208 and perplexity is 31.133739509140508
At time: 98.27218341827393 and batch: 850, loss is 3.485258240699768 and perplexity is 32.63085263612449
At time: 98.99609565734863 and batch: 900, loss is 3.4354661178588866 and perplexity is 31.045880232668413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318858943573416 and perplexity of 75.1028827540565
finished 7 epochs...
Completing Train Step...
At time: 100.79156565666199 and batch: 50, loss is 3.7224348402023315 and perplexity is 41.3649887305286
At time: 101.51396536827087 and batch: 100, loss is 3.6091639852523802 and perplexity is 36.93516156407783
At time: 102.23818063735962 and batch: 150, loss is 3.608472676277161 and perplexity is 36.9096367791604
At time: 102.96132612228394 and batch: 200, loss is 3.48565712928772 and perplexity is 32.64387130718321
At time: 103.68303537368774 and batch: 250, loss is 3.629986686706543 and perplexity is 37.712314538729196
At time: 104.40677380561829 and batch: 300, loss is 3.6044730663299562 and perplexity is 36.762307455066576
At time: 105.1322169303894 and batch: 350, loss is 3.59570734500885 and perplexity is 36.441467563717964
At time: 105.85531878471375 and batch: 400, loss is 3.5243511819839477 and perplexity is 33.93175094133905
At time: 106.5892539024353 and batch: 450, loss is 3.5488929748535156 and perplexity is 34.77479959378288
At time: 107.31368064880371 and batch: 500, loss is 3.4326545095443723 and perplexity is 30.95871397378573
At time: 108.0375747680664 and batch: 550, loss is 3.484655947685242 and perplexity is 32.61120521886873
At time: 108.76197338104248 and batch: 600, loss is 3.5063712978363037 and perplexity is 33.32711393083076
At time: 109.48331022262573 and batch: 650, loss is 3.3409090852737426 and perplexity is 28.24479196212028
At time: 110.20337057113647 and batch: 700, loss is 3.3445854330062867 and perplexity is 28.34882074482911
At time: 110.92253804206848 and batch: 750, loss is 3.4295498371124267 and perplexity is 30.86274635907261
At time: 111.63970232009888 and batch: 800, loss is 3.3975154829025267 and perplexity is 29.889746133551768
At time: 112.35950756072998 and batch: 850, loss is 3.4510022735595705 and perplexity is 31.531980146257954
At time: 113.07906103134155 and batch: 900, loss is 3.4094312381744385 and perplexity is 30.248035438148243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32772325489619 and perplexity of 75.77157746848242
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 114.83555889129639 and batch: 50, loss is 3.6899280023574828 and perplexity is 40.041963926497786
At time: 115.56794214248657 and batch: 100, loss is 3.5873835849761964 and perplexity is 36.13939646024942
At time: 116.28951001167297 and batch: 150, loss is 3.5891627407073976 and perplexity is 36.203751306254674
At time: 117.01034903526306 and batch: 200, loss is 3.461603965759277 and perplexity is 31.868050805317708
At time: 117.73124194145203 and batch: 250, loss is 3.6073732233047484 and perplexity is 36.86907866924568
At time: 118.45194673538208 and batch: 300, loss is 3.575756950378418 and perplexity is 35.72165010101307
At time: 119.1756227016449 and batch: 350, loss is 3.5638940286636354 and perplexity is 35.30039058693163
At time: 119.90199065208435 and batch: 400, loss is 3.4952487325668335 and perplexity is 32.95848478207197
At time: 120.62922358512878 and batch: 450, loss is 3.513413472175598 and perplexity is 33.56263760339448
At time: 121.35568284988403 and batch: 500, loss is 3.390994930267334 and perplexity is 29.69548251202157
At time: 122.0813057422638 and batch: 550, loss is 3.4403163051605223 and perplexity is 31.19682432439488
At time: 122.80648016929626 and batch: 600, loss is 3.4563567352294924 and perplexity is 31.701269748205657
At time: 123.53105211257935 and batch: 650, loss is 3.284667854309082 and perplexity is 26.700114513952414
At time: 124.25602912902832 and batch: 700, loss is 3.280799641609192 and perplexity is 26.597032292415776
At time: 124.99148774147034 and batch: 750, loss is 3.3635150623321532 and perplexity is 28.89056474302207
At time: 125.71299767494202 and batch: 800, loss is 3.3253003549575806 and perplexity is 27.807349463770244
At time: 126.44685816764832 and batch: 850, loss is 3.376829538345337 and perplexity is 29.27779966878449
At time: 127.17379760742188 and batch: 900, loss is 3.33221230506897 and perplexity is 28.000218257876092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312893384123502 and perplexity of 74.65618576559743
finished 9 epochs...
Completing Train Step...
At time: 128.9646556377411 and batch: 50, loss is 3.6605973625183106 and perplexity is 38.884564116035094
At time: 129.71141839027405 and batch: 100, loss is 3.5530171251296996 and perplexity is 34.91851223570172
At time: 130.43773651123047 and batch: 150, loss is 3.55215238571167 and perplexity is 34.888329873572495
At time: 131.16042137145996 and batch: 200, loss is 3.4256547498703003 and perplexity is 30.742767086031076
At time: 131.8821451663971 and batch: 250, loss is 3.572118391990662 and perplexity is 35.591910966257856
At time: 132.60596537590027 and batch: 300, loss is 3.5419380378723146 and perplexity is 34.53378215614271
At time: 133.32986092567444 and batch: 350, loss is 3.5319401311874388 and perplexity is 34.19023685328407
At time: 134.052964925766 and batch: 400, loss is 3.4647276115417482 and perplexity is 31.96775094069915
At time: 134.77516222000122 and batch: 450, loss is 3.484641809463501 and perplexity is 32.61074415767741
At time: 135.50489616394043 and batch: 500, loss is 3.36531081199646 and perplexity is 28.94249157480224
At time: 136.23075580596924 and batch: 550, loss is 3.417456531524658 and perplexity is 30.491761474171998
At time: 136.95314121246338 and batch: 600, loss is 3.4360972833633423 and perplexity is 31.065481506499967
At time: 137.67521715164185 and batch: 650, loss is 3.2675929355621336 and perplexity is 26.248082416312077
At time: 138.39853835105896 and batch: 700, loss is 3.2672993564605712 and perplexity is 26.24037765889178
At time: 139.12148785591125 and batch: 750, loss is 3.353452067375183 and perplexity is 28.60129702652172
At time: 139.8453664779663 and batch: 800, loss is 3.3186823511123658 and perplexity is 27.623927929759624
At time: 140.56756377220154 and batch: 850, loss is 3.373214673995972 and perplexity is 29.172155454326095
At time: 141.290137052536 and batch: 900, loss is 3.3320067262649538 and perplexity is 27.99446259813554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315491976803297 and perplexity of 74.85043906672156
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 143.0791895389557 and batch: 50, loss is 3.652632336616516 and perplexity is 38.576077737573314
At time: 143.80267357826233 and batch: 100, loss is 3.5526066970825196 and perplexity is 34.9041836395444
At time: 144.52731847763062 and batch: 150, loss is 3.554859662055969 and perplexity is 34.98291019349506
At time: 145.25167727470398 and batch: 200, loss is 3.423924345970154 and perplexity is 30.68961568192519
At time: 145.9753441810608 and batch: 250, loss is 3.570507969856262 and perplexity is 35.5346390933558
At time: 146.7003734111786 and batch: 300, loss is 3.5393601989746095 and perplexity is 34.444874273549374
At time: 147.42479300498962 and batch: 350, loss is 3.5269164752960207 and perplexity is 34.01890757846877
At time: 148.14752507209778 and batch: 400, loss is 3.4607228231430054 and perplexity is 31.839982875396252
At time: 148.8699471950531 and batch: 450, loss is 3.4760482358932494 and perplexity is 32.33170203036108
At time: 149.59167051315308 and batch: 500, loss is 3.355694127082825 and perplexity is 28.66549478288
At time: 150.32385563850403 and batch: 550, loss is 3.40319215297699 and perplexity is 30.05990286584091
At time: 151.0527753829956 and batch: 600, loss is 3.423815689086914 and perplexity is 30.686281225096486
At time: 151.77615237236023 and batch: 650, loss is 3.2504857635498046 and perplexity is 25.802870967579256
At time: 152.4988934993744 and batch: 700, loss is 3.2467236614227293 and perplexity is 25.70598030239404
At time: 153.22181701660156 and batch: 750, loss is 3.3309250593185427 and perplexity is 27.964198284165416
At time: 153.94533491134644 and batch: 800, loss is 3.293206844329834 and perplexity is 26.92908271266801
At time: 154.66846013069153 and batch: 850, loss is 3.346545934677124 and perplexity is 28.40445317117911
At time: 155.3894066810608 and batch: 900, loss is 3.3093241834640503 and perplexity is 27.366624403638344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311043517230308 and perplexity of 74.5182094174631
finished 11 epochs...
Completing Train Step...
At time: 157.17834758758545 and batch: 50, loss is 3.640867233276367 and perplexity is 38.124885562219426
At time: 157.92295455932617 and batch: 100, loss is 3.538453235626221 and perplexity is 34.413648197637094
At time: 158.66027903556824 and batch: 150, loss is 3.5396366262435914 and perplexity is 34.45439709219797
At time: 159.388774394989 and batch: 200, loss is 3.4103263092041014 and perplexity is 30.275121698628322
At time: 160.11447548866272 and batch: 250, loss is 3.5570057678222655 and perplexity is 35.05806783806377
At time: 160.85047841072083 and batch: 300, loss is 3.5267634439468383 and perplexity is 34.0137020174615
At time: 161.57450199127197 and batch: 350, loss is 3.5143709850311278 and perplexity is 33.594789650908126
At time: 162.29920554161072 and batch: 400, loss is 3.450024046897888 and perplexity is 31.501149804572897
At time: 163.02544355392456 and batch: 450, loss is 3.466549096107483 and perplexity is 32.026032769250484
At time: 163.76624584197998 and batch: 500, loss is 3.3474680948257447 and perplexity is 28.43065870693025
At time: 164.4978907108307 and batch: 550, loss is 3.3961160373687744 and perplexity is 29.84794631692542
At time: 165.2221977710724 and batch: 600, loss is 3.4180625915527343 and perplexity is 30.510246913064112
At time: 165.9482388496399 and batch: 650, loss is 3.245945692062378 and perplexity is 25.685989614420606
At time: 166.67440176010132 and batch: 700, loss is 3.243633108139038 and perplexity is 25.626657239773774
At time: 167.40018820762634 and batch: 750, loss is 3.3295065689086916 and perplexity is 27.924559457374475
At time: 168.1257073879242 and batch: 800, loss is 3.2936875677108763 and perplexity is 26.94203126444444
At time: 168.85049557685852 and batch: 850, loss is 3.3486855602264405 and perplexity is 28.465293129052238
At time: 169.575692653656 and batch: 900, loss is 3.3128342390060426 and perplexity is 27.462851557822752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310854559075342 and perplexity of 74.50412992435965
finished 12 epochs...
Completing Train Step...
At time: 171.37434315681458 and batch: 50, loss is 3.6348253965377806 and perplexity is 37.89523568011308
At time: 172.1124496459961 and batch: 100, loss is 3.5316391038894652 and perplexity is 34.17994620762739
At time: 172.8379786014557 and batch: 150, loss is 3.532378191947937 and perplexity is 34.20521753542658
At time: 173.56316137313843 and batch: 200, loss is 3.402965488433838 and perplexity is 30.053090123823328
At time: 174.28837370872498 and batch: 250, loss is 3.5495978879928587 and perplexity is 34.79932144880805
At time: 175.01303267478943 and batch: 300, loss is 3.519597973823547 and perplexity is 33.77084896908967
At time: 175.73601841926575 and batch: 350, loss is 3.507434458732605 and perplexity is 33.36256485682852
At time: 176.46272897720337 and batch: 400, loss is 3.4435836696624755 and perplexity is 31.298922425791723
At time: 177.1954083442688 and batch: 450, loss is 3.460468282699585 and perplexity is 31.831879343418723
At time: 177.92396569252014 and batch: 500, loss is 3.342153673171997 and perplexity is 28.279966973041
At time: 178.64927911758423 and batch: 550, loss is 3.391261920928955 and perplexity is 29.7034119870454
At time: 179.38648009300232 and batch: 600, loss is 3.4140199851989745 and perplexity is 30.38715496881404
At time: 180.1108717918396 and batch: 650, loss is 3.2425203037261965 and perplexity is 25.598155643803064
At time: 180.8358702659607 and batch: 700, loss is 3.2410342407226564 and perplexity is 25.560143423014907
At time: 181.557044506073 and batch: 750, loss is 3.327709379196167 and perplexity is 27.874418796027012
At time: 182.27422189712524 and batch: 800, loss is 3.292821350097656 and perplexity is 26.918703707256327
At time: 182.99268436431885 and batch: 850, loss is 3.348378839492798 and perplexity is 28.456563572296808
At time: 183.71005773544312 and batch: 900, loss is 3.313215384483337 and perplexity is 27.473320894531057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311447352579195 and perplexity of 74.54830858170585
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 185.47308921813965 and batch: 50, loss is 3.632630615234375 and perplexity is 37.81215513052304
At time: 186.19047713279724 and batch: 100, loss is 3.5321927213668824 and perplexity is 34.19887406213721
At time: 186.90821743011475 and batch: 150, loss is 3.5338575267791748 and perplexity is 34.25585595148824
At time: 187.632559299469 and batch: 200, loss is 3.4038206815719603 and perplexity is 30.078802313153474
At time: 188.3561110496521 and batch: 250, loss is 3.5492817783355712 and perplexity is 34.78832278571443
At time: 189.07549905776978 and batch: 300, loss is 3.520114121437073 and perplexity is 33.788284211384074
At time: 189.79406929016113 and batch: 350, loss is 3.508184452056885 and perplexity is 33.3875959431525
At time: 190.51567769050598 and batch: 400, loss is 3.443785195350647 and perplexity is 31.305230598280655
At time: 191.2410273551941 and batch: 450, loss is 3.4571887493133544 and perplexity is 31.727656626717565
At time: 191.96758365631104 and batch: 500, loss is 3.3392567443847656 and perplexity is 28.198160473627755
At time: 192.6931164264679 and batch: 550, loss is 3.3864124822616577 and perplexity is 29.559715816895817
At time: 193.4188425540924 and batch: 600, loss is 3.410197639465332 and perplexity is 30.271226457233368
At time: 194.14424657821655 and batch: 650, loss is 3.237383370399475 and perplexity is 25.46699679053514
At time: 194.86941051483154 and batch: 700, loss is 3.2344883728027343 and perplexity is 25.39337651271171
At time: 195.59484195709229 and batch: 750, loss is 3.3190856552124024 and perplexity is 27.63507102002877
At time: 196.31995463371277 and batch: 800, loss is 3.2828563976287843 and perplexity is 26.651792193260356
At time: 197.0530731678009 and batch: 850, loss is 3.3376136445999145 and perplexity is 28.151866125752225
At time: 197.77775359153748 and batch: 900, loss is 3.3050156688690184 and perplexity is 27.248968545975153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310846198095034 and perplexity of 74.50350699940061
finished 14 epochs...
Completing Train Step...
At time: 199.57448291778564 and batch: 50, loss is 3.628761343955994 and perplexity is 37.666132327799836
At time: 200.3126609325409 and batch: 100, loss is 3.5280529832839966 and perplexity is 34.057592317267314
At time: 201.03755903244019 and batch: 150, loss is 3.5297365760803223 and perplexity is 34.114979729314314
At time: 201.7728395462036 and batch: 200, loss is 3.400069761276245 and perplexity is 29.966190454171727
At time: 202.49715566635132 and batch: 250, loss is 3.5457498264312743 and perplexity is 34.66566883423862
At time: 203.22175192832947 and batch: 300, loss is 3.516229305267334 and perplexity is 33.65727757164858
At time: 203.9473421573639 and batch: 350, loss is 3.5039798736572267 and perplexity is 33.2475098862136
At time: 204.67363476753235 and batch: 400, loss is 3.4406393814086913 and perplexity is 31.206904905662892
At time: 205.40007400512695 and batch: 450, loss is 3.4545806407928468 and perplexity is 31.645015270791404
At time: 206.1249852180481 and batch: 500, loss is 3.336853837966919 and perplexity is 28.13048427520327
At time: 206.85063433647156 and batch: 550, loss is 3.3844102144241335 and perplexity is 29.50058856268281
At time: 207.57660937309265 and batch: 600, loss is 3.4087189292907714 and perplexity is 30.22649716565427
At time: 208.31308674812317 and batch: 650, loss is 3.236100435256958 and perplexity is 25.434345234770863
At time: 209.04061150550842 and batch: 700, loss is 3.233690538406372 and perplexity is 25.373124883287613
At time: 209.76477336883545 and batch: 750, loss is 3.3189780044555666 and perplexity is 27.63209624383958
At time: 210.49032378196716 and batch: 800, loss is 3.2832388782501223 and perplexity is 26.66198793701111
At time: 211.2165277004242 and batch: 850, loss is 3.33910569190979 and perplexity is 28.193901393378855
At time: 211.9430913925171 and batch: 900, loss is 3.3073534107208253 and perplexity is 27.3127441165292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310469535932149 and perplexity of 74.47544963171778
finished 15 epochs...
Completing Train Step...
At time: 213.72691893577576 and batch: 50, loss is 3.6267735815048217 and perplexity is 37.59133536819977
At time: 214.4645278453827 and batch: 100, loss is 3.5256827306747436 and perplexity is 33.97696281410626
At time: 215.1897668838501 and batch: 150, loss is 3.5271465301513674 and perplexity is 34.02673469362919
At time: 215.92610096931458 and batch: 200, loss is 3.3977036571502683 and perplexity is 29.895371143270026
At time: 216.65143871307373 and batch: 250, loss is 3.5434130096435545 and perplexity is 34.58475609329503
At time: 217.37692284584045 and batch: 300, loss is 3.513948621749878 and perplexity is 33.58060344139616
At time: 218.1048617362976 and batch: 350, loss is 3.5015948152542116 and perplexity is 33.16830712253412
At time: 218.83169627189636 and batch: 400, loss is 3.4386558151245117 and perplexity is 31.1450652930196
At time: 219.55894088745117 and batch: 450, loss is 3.452872047424316 and perplexity is 31.59099297176585
At time: 220.28147864341736 and batch: 500, loss is 3.3353602361679076 and perplexity is 28.088499895060675
At time: 221.00461864471436 and batch: 550, loss is 3.38309241771698 and perplexity is 29.461738388154085
At time: 221.7307469844818 and batch: 600, loss is 3.407768225669861 and perplexity is 30.19777438094255
At time: 222.45723843574524 and batch: 650, loss is 3.235302333831787 and perplexity is 25.41405414584126
At time: 223.1826252937317 and batch: 700, loss is 3.2331519508361817 and perplexity is 25.35946291302252
At time: 223.9078209400177 and batch: 750, loss is 3.3187692880630495 and perplexity is 27.626329574214054
At time: 224.63216400146484 and batch: 800, loss is 3.2833622026443483 and perplexity is 26.665276213280475
At time: 225.35740184783936 and batch: 850, loss is 3.339711947441101 and perplexity is 28.210999284369635
At time: 226.08210945129395 and batch: 900, loss is 3.308346447944641 and perplexity is 27.339880159441872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31041017297196 and perplexity of 74.47102867978796
finished 16 epochs...
Completing Train Step...
At time: 227.87207317352295 and batch: 50, loss is 3.62510977268219 and perplexity is 37.52884257520618
At time: 228.59418511390686 and batch: 100, loss is 3.523788614273071 and perplexity is 33.91266740228069
At time: 229.31823539733887 and batch: 150, loss is 3.5251220750808714 and perplexity is 33.95791877891302
At time: 230.04091787338257 and batch: 200, loss is 3.3957597637176513 and perplexity is 29.83731417420118
At time: 230.76388692855835 and batch: 250, loss is 3.5414899301528933 and perplexity is 34.518310768460736
At time: 231.4867763519287 and batch: 300, loss is 3.512095584869385 and perplexity is 33.51843496278111
At time: 232.20907402038574 and batch: 350, loss is 3.4997271585464476 and perplexity is 33.1064179231306
At time: 232.93161296844482 and batch: 400, loss is 3.437027807235718 and perplexity is 31.094402132228126
At time: 233.6727020740509 and batch: 450, loss is 3.451398663520813 and perplexity is 31.544481584204597
At time: 234.3950159549713 and batch: 500, loss is 3.3340822219848634 and perplexity is 28.05262532280332
At time: 235.11720204353333 and batch: 550, loss is 3.3819341611862184 and perplexity is 29.427633891948904
At time: 235.83886337280273 and batch: 600, loss is 3.40688364982605 and perplexity is 30.17107397019853
At time: 236.56053638458252 and batch: 650, loss is 3.2345682096481325 and perplexity is 25.39540392071635
At time: 237.27945137023926 and batch: 700, loss is 3.232610068321228 and perplexity is 25.345724786042904
At time: 238.00335669517517 and batch: 750, loss is 3.318446297645569 and perplexity is 27.61740797536445
At time: 238.7424018383026 and batch: 800, loss is 3.2832959604263308 and perplexity is 26.6635099047428
At time: 239.46525645256042 and batch: 850, loss is 3.3399137163162234 and perplexity is 28.21669196024441
At time: 240.1879301071167 and batch: 900, loss is 3.3087833070755006 and perplexity is 27.351826444958093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310482495451627 and perplexity of 74.47641480401198
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 241.97239351272583 and batch: 50, loss is 3.624350137710571 and perplexity is 37.50034517912136
At time: 242.70455861091614 and batch: 100, loss is 3.523725118637085 and perplexity is 33.910514164257314
At time: 243.42941904067993 and batch: 150, loss is 3.5254741430282595 and perplexity is 33.96987637849534
At time: 244.15436482429504 and batch: 200, loss is 3.3960241651535035 and perplexity is 29.845204245937992
At time: 244.87628889083862 and batch: 250, loss is 3.5413140153884886 and perplexity is 34.512239022024836
At time: 245.59933018684387 and batch: 300, loss is 3.511840920448303 and perplexity is 33.50990009675523
At time: 246.3221151828766 and batch: 350, loss is 3.499666166305542 and perplexity is 33.10439875009069
At time: 247.0444040298462 and batch: 400, loss is 3.43721718788147 and perplexity is 31.10029136781922
At time: 247.766943693161 and batch: 450, loss is 3.450546760559082 and perplexity is 31.517620190191828
At time: 248.4965682029724 and batch: 500, loss is 3.3329446268081666 and perplexity is 28.02073093643207
At time: 249.21951007843018 and batch: 550, loss is 3.3802595806121825 and perplexity is 29.378396185652953
At time: 249.94237542152405 and batch: 600, loss is 3.405203313827515 and perplexity is 30.12041899909648
At time: 250.66623759269714 and batch: 650, loss is 3.2331746196746827 and perplexity is 25.36003778910763
At time: 251.3904070854187 and batch: 700, loss is 3.2310150480270385 and perplexity is 25.305330064402604
At time: 252.1254334449768 and batch: 750, loss is 3.3160534286499024 and perplexity is 27.551402139233737
At time: 252.8482346534729 and batch: 800, loss is 3.2800718450546267 and perplexity is 26.57768210630521
At time: 253.5723888874054 and batch: 850, loss is 3.3358058166503906 and perplexity is 28.101018371183336
At time: 254.2940833568573 and batch: 900, loss is 3.3050347328186036 and perplexity is 27.249488023889395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309856676075556 and perplexity of 74.42982060186534
finished 18 epochs...
Completing Train Step...
At time: 256.07784152030945 and batch: 50, loss is 3.623523588180542 and perplexity is 37.46936209272931
At time: 256.81601905822754 and batch: 100, loss is 3.5227984619140624 and perplexity is 33.879105313184915
At time: 257.5406708717346 and batch: 150, loss is 3.5245971727371215 and perplexity is 33.94009886502353
At time: 258.266398191452 and batch: 200, loss is 3.395305576324463 and perplexity is 29.823765519303684
At time: 258.9916310310364 and batch: 250, loss is 3.540650510787964 and perplexity is 34.48934758778508
At time: 259.717978477478 and batch: 300, loss is 3.5111108875274657 and perplexity is 33.48544569385629
At time: 260.4427878856659 and batch: 350, loss is 3.498848280906677 and perplexity is 33.0773342150675
At time: 261.1679790019989 and batch: 400, loss is 3.436455340385437 and perplexity is 31.07660671190823
At time: 261.8940613269806 and batch: 450, loss is 3.4499532699584963 and perplexity is 31.49892032850108
At time: 262.61903381347656 and batch: 500, loss is 3.3324272632598877 and perplexity is 28.006237781087844
At time: 263.33877825737 and batch: 550, loss is 3.3798235940933226 and perplexity is 29.365590392749667
At time: 264.058721780777 and batch: 600, loss is 3.404934873580933 and perplexity is 30.1123345515373
At time: 264.77811574935913 and batch: 650, loss is 3.2328924894332887 and perplexity is 25.352883964726978
At time: 265.497517824173 and batch: 700, loss is 3.23086088180542 and perplexity is 25.30142913798304
At time: 266.21756768226624 and batch: 750, loss is 3.316013216972351 and perplexity is 27.55029427340955
At time: 266.9362576007843 and batch: 800, loss is 3.2801677417755126 and perplexity is 26.58023094107845
At time: 267.6555366516113 and batch: 850, loss is 3.336226077079773 and perplexity is 28.112830599162244
At time: 268.3822784423828 and batch: 900, loss is 3.3056344985961914 and perplexity is 27.26583623632202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30957491103917 and perplexity of 74.40885183502662
finished 19 epochs...
Completing Train Step...
At time: 270.1720087528229 and batch: 50, loss is 3.6229261541366578 and perplexity is 37.4469833058027
At time: 270.8971586227417 and batch: 100, loss is 3.5221172428131102 and perplexity is 33.85603407869741
At time: 271.6227033138275 and batch: 150, loss is 3.5239092350006103 and perplexity is 33.916758219609264
At time: 272.3472993373871 and batch: 200, loss is 3.3947092390060423 and perplexity is 29.80598579683161
At time: 273.0715527534485 and batch: 250, loss is 3.5400790214538573 and perplexity is 34.46964292453711
At time: 273.79532289505005 and batch: 300, loss is 3.5105209064483645 and perplexity is 33.46569574109374
At time: 274.52050495147705 and batch: 350, loss is 3.498208451271057 and perplexity is 33.056177125557596
At time: 275.24479150772095 and batch: 400, loss is 3.435894708633423 and perplexity is 31.059189062340575
At time: 275.9700708389282 and batch: 450, loss is 3.4495011615753173 and perplexity is 31.484682621295384
At time: 276.69699692726135 and batch: 500, loss is 3.332021279335022 and perplexity is 27.994870006475768
At time: 277.4218637943268 and batch: 550, loss is 3.379477858543396 and perplexity is 29.35543941908366
At time: 278.14723658561707 and batch: 600, loss is 3.4047083711624144 and perplexity is 30.105514807307582
At time: 278.87752509117126 and batch: 650, loss is 3.232667188644409 and perplexity is 25.347172583382875
At time: 279.60178780555725 and batch: 700, loss is 3.2307380390167237 and perplexity is 25.298321230765627
At time: 280.3261995315552 and batch: 750, loss is 3.3159852600097657 and perplexity is 27.54952406162977
At time: 281.0494740009308 and batch: 800, loss is 3.280226445198059 and perplexity is 26.581791337406624
At time: 281.77317571640015 and batch: 850, loss is 3.3364986848831175 and perplexity is 28.120495420855306
At time: 282.49723172187805 and batch: 900, loss is 3.3060401248931885 and perplexity is 27.276898219873132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309437790962114 and perplexity of 74.3986495870119
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
581.4189596176147


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0146350860595703 and batch: 50, loss is 7.095136442184448 and perplexity is 1206.08691339783
At time: 1.8227338790893555 and batch: 100, loss is 6.476896820068359 and perplexity is 649.9508990207761
At time: 2.615140199661255 and batch: 150, loss is 6.243817558288574 and perplexity is 514.8201200984571
At time: 3.407588243484497 and batch: 200, loss is 6.114388151168823 and perplexity is 452.31921170803986
At time: 4.200662851333618 and batch: 250, loss is 6.178119134902954 and perplexity is 482.08436743364933
At time: 4.9952428340911865 and batch: 300, loss is 6.093058557510376 and perplexity is 442.77359072366215
At time: 5.788404941558838 and batch: 350, loss is 6.1123800945281985 and perplexity is 451.4118404426337
At time: 6.5833740234375 and batch: 400, loss is 6.005451860427857 and perplexity is 405.6342373909354
At time: 7.378095388412476 and batch: 450, loss is 6.012534284591675 and perplexity is 408.51730863080905
At time: 8.191955804824829 and batch: 500, loss is 5.996330738067627 and perplexity is 401.9512200381371
At time: 8.985729932785034 and batch: 550, loss is 6.036124858856201 and perplexity is 418.26903875057036
At time: 9.779660701751709 and batch: 600, loss is 5.978360786437988 and perplexity is 394.7926879290096
At time: 10.572777032852173 and batch: 650, loss is 5.911955785751343 and perplexity is 369.4279712830548
At time: 11.365678548812866 and batch: 700, loss is 6.002022743225098 and perplexity is 404.2456522207968
At time: 12.16110110282898 and batch: 750, loss is 5.9844723892211915 and perplexity is 397.21289214954857
At time: 12.963009595870972 and batch: 800, loss is 5.994620876312256 and perplexity is 401.2645262624845
At time: 13.76326060295105 and batch: 850, loss is 6.027708320617676 and perplexity is 414.7634346117517
At time: 14.567202806472778 and batch: 900, loss is 5.907871360778809 and perplexity is 367.9221477566127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.977312897982663 and perplexity of 394.379205908436
finished 1 epochs...
Completing Train Step...
At time: 16.41518998146057 and batch: 50, loss is 5.820223083496094 and perplexity is 337.04723491942167
At time: 17.139373779296875 and batch: 100, loss is 5.579700698852539 and perplexity is 264.9922814219846
At time: 17.86305570602417 and batch: 150, loss is 5.442228717803955 and perplexity is 230.956346807377
At time: 18.58666229248047 and batch: 200, loss is 5.222415990829468 and perplexity is 185.3815235298751
At time: 19.312723398208618 and batch: 250, loss is 5.238090095520019 and perplexity is 188.31010442215512
At time: 20.03796362876892 and batch: 300, loss is 5.1399791717529295 and perplexity is 170.7122126481508
At time: 20.763471841812134 and batch: 350, loss is 5.100600700378418 and perplexity is 164.12046492052207
At time: 21.489226818084717 and batch: 400, loss is 4.933265790939331 and perplexity is 138.83216963728586
At time: 22.21503257751465 and batch: 450, loss is 4.932346954345703 and perplexity is 138.70466414677622
At time: 22.940178155899048 and batch: 500, loss is 4.842305774688721 and perplexity is 126.76129801205998
At time: 23.664557218551636 and batch: 550, loss is 4.89865291595459 and perplexity is 134.10900185406328
At time: 24.38874077796936 and batch: 600, loss is 4.809178943634033 and perplexity is 122.63088929882842
At time: 25.11281943321228 and batch: 650, loss is 4.678484582901001 and perplexity is 107.60687964457223
At time: 25.836891651153564 and batch: 700, loss is 4.742179317474365 and perplexity is 114.68386208137085
At time: 26.560418367385864 and batch: 750, loss is 4.747103824615478 and perplexity is 115.25001645063868
At time: 27.29275369644165 and batch: 800, loss is 4.700068712234497 and perplexity is 109.95472742757597
At time: 28.018880367279053 and batch: 850, loss is 4.733421468734742 and perplexity is 113.68386347429912
At time: 28.74584412574768 and batch: 900, loss is 4.656284999847412 and perplexity is 105.24437214720761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.744082934235873 and perplexity of 114.90238412863017
finished 2 epochs...
Completing Train Step...
At time: 30.543839931488037 and batch: 50, loss is 4.717245931625366 and perplexity is 111.85975863232348
At time: 31.280938148498535 and batch: 100, loss is 4.594783210754395 and perplexity is 98.96667831037185
At time: 32.00710868835449 and batch: 150, loss is 4.592135419845581 and perplexity is 98.70498185091098
At time: 32.731040477752686 and batch: 200, loss is 4.483305644989014 and perplexity is 88.52682779375154
At time: 33.45597434043884 and batch: 250, loss is 4.611535987854004 and perplexity is 100.63861066451453
At time: 34.182596921920776 and batch: 300, loss is 4.557933301925659 and perplexity is 95.38614162584591
At time: 34.90842342376709 and batch: 350, loss is 4.557027387619018 and perplexity is 95.2997690844593
At time: 35.633588790893555 and batch: 400, loss is 4.442520837783814 and perplexity is 84.98891513280002
At time: 36.35907483100891 and batch: 450, loss is 4.474026355743408 and perplexity is 87.70916130170407
At time: 37.084707260131836 and batch: 500, loss is 4.371273555755615 and perplexity is 79.14436230106386
At time: 37.80916690826416 and batch: 550, loss is 4.441432733535766 and perplexity is 84.89648862716295
At time: 38.53270769119263 and batch: 600, loss is 4.410073137283325 and perplexity is 82.27548068930057
At time: 39.2582061290741 and batch: 650, loss is 4.268704242706299 and perplexity is 71.42902095423027
At time: 39.98417544364929 and batch: 700, loss is 4.305191583633423 and perplexity is 74.08340726168558
At time: 40.71055197715759 and batch: 750, loss is 4.366444606781005 and perplexity is 78.76309950406096
At time: 41.436540603637695 and batch: 800, loss is 4.33176474571228 and perplexity is 76.07842725056545
At time: 42.16309118270874 and batch: 850, loss is 4.390360164642334 and perplexity is 80.66946803904463
At time: 42.888686180114746 and batch: 900, loss is 4.328671646118164 and perplexity is 75.84347265442146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.511679035343536 and perplexity of 91.07460769062929
finished 3 epochs...
Completing Train Step...
At time: 44.668046951293945 and batch: 50, loss is 4.412762260437011 and perplexity is 82.49702733900816
At time: 45.40037250518799 and batch: 100, loss is 4.289174447059631 and perplexity is 72.90625567487164
At time: 46.12108635902405 and batch: 150, loss is 4.285516085624695 and perplexity is 72.64002552073607
At time: 46.84319448471069 and batch: 200, loss is 4.181391324996948 and perplexity is 65.45686165601249
At time: 47.56521415710449 and batch: 250, loss is 4.329990453720093 and perplexity is 75.94356158723085
At time: 48.2902991771698 and batch: 300, loss is 4.28835298538208 and perplexity is 72.84639057159146
At time: 49.015127658843994 and batch: 350, loss is 4.286312074661255 and perplexity is 72.69786920308033
At time: 49.73832368850708 and batch: 400, loss is 4.194092259407044 and perplexity is 66.29352693088275
At time: 50.465916872024536 and batch: 450, loss is 4.2396927213668825 and perplexity is 69.38652756532913
At time: 51.18696618080139 and batch: 500, loss is 4.123403997421264 and perplexity is 61.76914682095241
At time: 51.908714294433594 and batch: 550, loss is 4.1921389245986935 and perplexity is 66.16415986688845
At time: 52.62998008728027 and batch: 600, loss is 4.186153316497803 and perplexity is 65.76931002268664
At time: 53.34779071807861 and batch: 650, loss is 4.036934146881103 and perplexity is 56.652388380757536
At time: 54.078829288482666 and batch: 700, loss is 4.055823421478271 and perplexity is 57.732681759652024
At time: 54.80063796043396 and batch: 750, loss is 4.144446525573731 and perplexity is 63.082697577927064
At time: 55.520609617233276 and batch: 800, loss is 4.112962493896484 and perplexity is 61.12753955820502
At time: 56.23581051826477 and batch: 850, loss is 4.179431748390198 and perplexity is 65.32871951433651
At time: 56.95314145088196 and batch: 900, loss is 4.127688698768615 and perplexity is 62.03437697779353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4170247953232025 and perplexity of 82.84942431468484
finished 4 epochs...
Completing Train Step...
At time: 58.714921951293945 and batch: 50, loss is 4.216121983528137 and perplexity is 67.77016023422988
At time: 59.431517601013184 and batch: 100, loss is 4.093084335327148 and perplexity is 59.92443401144941
At time: 60.16244339942932 and batch: 150, loss is 4.091860237121582 and perplexity is 59.85112549687089
At time: 60.87926626205444 and batch: 200, loss is 3.991546015739441 and perplexity is 54.1385237064231
At time: 61.603107929229736 and batch: 250, loss is 4.14551429271698 and perplexity is 63.15009118364413
At time: 62.31940937042236 and batch: 300, loss is 4.105597105026245 and perplexity is 60.67896544794533
At time: 63.052417278289795 and batch: 350, loss is 4.105490784645081 and perplexity is 60.672514380156336
At time: 63.77527642250061 and batch: 400, loss is 4.0256567430496215 and perplexity is 56.01708553546902
At time: 64.5002372264862 and batch: 450, loss is 4.073311796188355 and perplexity is 58.75121279840803
At time: 65.22296142578125 and batch: 500, loss is 3.955041079521179 and perplexity is 52.19783816258794
At time: 65.94513368606567 and batch: 550, loss is 4.019938774108887 and perplexity is 55.69769558135411
At time: 66.66668272018433 and batch: 600, loss is 4.026338582038879 and perplexity is 56.05529319270665
At time: 67.38917517662048 and batch: 650, loss is 3.8745492696762085 and perplexity is 48.16098578150775
At time: 68.11129426956177 and batch: 700, loss is 3.89143075466156 and perplexity is 48.98091608824347
At time: 68.82995820045471 and batch: 750, loss is 3.9818673753738403 and perplexity is 53.61706398677538
At time: 69.55291295051575 and batch: 800, loss is 3.9563610315322877 and perplexity is 52.26678229550967
At time: 70.27961254119873 and batch: 850, loss is 4.027127432823181 and perplexity is 56.099529900492804
At time: 71.0101261138916 and batch: 900, loss is 3.9764158630371096 and perplexity is 53.325565177589816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3784769136611725 and perplexity of 79.71652574390515
finished 5 epochs...
Completing Train Step...
At time: 72.80326390266418 and batch: 50, loss is 4.067918829917907 and perplexity is 58.435222318296326
At time: 73.54267454147339 and batch: 100, loss is 3.9466371059417726 and perplexity is 51.76100703937116
At time: 74.26799964904785 and batch: 150, loss is 3.950356197357178 and perplexity is 51.95386937032356
At time: 74.99288487434387 and batch: 200, loss is 3.8514724254608153 and perplexity is 47.06230798071856
At time: 75.71843385696411 and batch: 250, loss is 4.003828520774841 and perplexity is 54.807580834088014
At time: 76.44491529464722 and batch: 300, loss is 3.963690400123596 and perplexity is 52.65127212063929
At time: 77.17011213302612 and batch: 350, loss is 3.965111184120178 and perplexity is 52.7261313722886
At time: 77.894606590271 and batch: 400, loss is 3.8945484066009524 and perplexity is 49.133859825066786
At time: 78.6183910369873 and batch: 450, loss is 3.943375253677368 and perplexity is 51.592445342481795
At time: 79.3447597026825 and batch: 500, loss is 3.823857960700989 and perplexity is 45.78048738455768
At time: 80.07358193397522 and batch: 550, loss is 3.885101261138916 and perplexity is 48.67187077903084
At time: 80.80188202857971 and batch: 600, loss is 3.9002033948898314 and perplexity is 49.41249833316952
At time: 81.54234027862549 and batch: 650, loss is 3.750167684555054 and perplexity is 42.528212726618015
At time: 82.27208948135376 and batch: 700, loss is 3.7658689546585085 and perplexity is 43.2012294639396
At time: 83.00509190559387 and batch: 750, loss is 3.854595880508423 and perplexity is 47.209534792602334
At time: 83.73217535018921 and batch: 800, loss is 3.8309705877304077 and perplexity is 46.107267672066406
At time: 84.45218205451965 and batch: 850, loss is 3.9036744165420534 and perplexity is 49.58430819014421
At time: 85.17914271354675 and batch: 900, loss is 3.8575261926651 and perplexity is 47.3480763521894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370882635247217 and perplexity of 79.11342919330335
finished 6 epochs...
Completing Train Step...
At time: 86.96889400482178 and batch: 50, loss is 3.948468260765076 and perplexity is 51.855876290704856
At time: 87.70649695396423 and batch: 100, loss is 3.8298020505905153 and perplexity is 46.05342108436863
At time: 88.43124580383301 and batch: 150, loss is 3.83656831741333 and perplexity is 46.36608741856794
At time: 89.15545582771301 and batch: 200, loss is 3.738621253967285 and perplexity is 42.039987722578594
At time: 89.88040614128113 and batch: 250, loss is 3.8931233501434326 and perplexity is 49.06389116732492
At time: 90.62674236297607 and batch: 300, loss is 3.851385669708252 and perplexity is 47.058225231875866
At time: 91.3539137840271 and batch: 350, loss is 3.8536917686462404 and perplexity is 47.16687138134248
At time: 92.08162760734558 and batch: 400, loss is 3.787353162765503 and perplexity is 44.13941567576298
At time: 92.80890727043152 and batch: 450, loss is 3.8363556814193727 and perplexity is 46.35622936760915
At time: 93.53710412979126 and batch: 500, loss is 3.7166670322418214 and perplexity is 41.127090155415935
At time: 94.26312756538391 and batch: 550, loss is 3.7762995767593384 and perplexity is 43.65420345615974
At time: 94.99079179763794 and batch: 600, loss is 3.7969893121719362 and perplexity is 44.56680556924588
At time: 95.72004461288452 and batch: 650, loss is 3.6486566781997682 and perplexity is 38.42301688985936
At time: 96.44809651374817 and batch: 700, loss is 3.665456008911133 and perplexity is 39.07395017058854
At time: 97.17552495002747 and batch: 750, loss is 3.7509666538238524 and perplexity is 42.56220503924184
At time: 97.89936947822571 and batch: 800, loss is 3.728591456413269 and perplexity is 41.620442649690524
At time: 98.62870526313782 and batch: 850, loss is 3.8025506925582886 and perplexity is 44.8153490093288
At time: 99.3703248500824 and batch: 900, loss is 3.7592406845092774 and perplexity is 42.9158269516546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372682336258562 and perplexity of 79.25593790990958
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 101.19028878211975 and batch: 50, loss is 3.873740763664246 and perplexity is 48.12206307170405
At time: 101.92530131340027 and batch: 100, loss is 3.766598653793335 and perplexity is 43.23276486798213
At time: 102.65252208709717 and batch: 150, loss is 3.7797150230407714 and perplexity is 43.80355695225702
At time: 103.37984800338745 and batch: 200, loss is 3.650029711723328 and perplexity is 38.475809214647526
At time: 104.10692715644836 and batch: 250, loss is 3.7955275917053224 and perplexity is 44.5017089455379
At time: 104.8401267528534 and batch: 300, loss is 3.7451971054077147 and perplexity is 42.317347374873606
At time: 105.57074403762817 and batch: 350, loss is 3.741006965637207 and perplexity is 42.14040274491003
At time: 106.29966616630554 and batch: 400, loss is 3.662588210105896 and perplexity is 38.962054466844556
At time: 107.0350124835968 and batch: 450, loss is 3.6937736749649046 and perplexity is 40.1962486845055
At time: 107.76450371742249 and batch: 500, loss is 3.573146343231201 and perplexity is 35.62851652643091
At time: 108.50355577468872 and batch: 550, loss is 3.6119673585891725 and perplexity is 37.03884988182298
At time: 109.23349213600159 and batch: 600, loss is 3.631665577888489 and perplexity is 37.77568259020597
At time: 109.96039938926697 and batch: 650, loss is 3.4643448257446288 and perplexity is 31.955516481411212
At time: 110.68865823745728 and batch: 700, loss is 3.4640640878677367 and perplexity is 31.946546616708527
At time: 111.41885733604431 and batch: 750, loss is 3.5440183687210083 and perplexity is 34.60569862757469
At time: 112.145583152771 and batch: 800, loss is 3.5087508487701418 and perplexity is 33.40651192422731
At time: 112.87325263023376 and batch: 850, loss is 3.5613007020950316 and perplexity is 35.20896374720645
At time: 113.6019880771637 and batch: 900, loss is 3.5212103176116942 and perplexity is 33.82534310748271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326843679767766 and perplexity of 75.7049599753396
finished 8 epochs...
Completing Train Step...
At time: 115.41146564483643 and batch: 50, loss is 3.780352077484131 and perplexity is 43.83147109331814
At time: 116.15108871459961 and batch: 100, loss is 3.6712242221832274 and perplexity is 39.29998834000659
At time: 116.89237236976624 and batch: 150, loss is 3.685731596946716 and perplexity is 39.874283685622096
At time: 117.63685870170593 and batch: 200, loss is 3.5622966480255127 and perplexity is 35.24404743919709
At time: 118.38257598876953 and batch: 250, loss is 3.7114962339401245 and perplexity is 40.91497913175061
At time: 119.11648964881897 and batch: 300, loss is 3.663305654525757 and perplexity is 38.99001760520745
At time: 119.8435640335083 and batch: 350, loss is 3.6641186666488648 and perplexity is 39.0217298516743
At time: 120.57038140296936 and batch: 400, loss is 3.5914492797851563 and perplexity is 36.286627311882505
At time: 121.30319809913635 and batch: 450, loss is 3.6279547739028932 and perplexity is 37.63576420210483
At time: 122.0287733078003 and batch: 500, loss is 3.511424770355225 and perplexity is 33.49595784994578
At time: 122.75443434715271 and batch: 550, loss is 3.5513303422927858 and perplexity is 34.859661936363
At time: 123.48445868492126 and batch: 600, loss is 3.5780058526992797 and perplexity is 35.802075002808245
At time: 124.21418786048889 and batch: 650, loss is 3.4143414640426637 and perplexity is 30.396925366660373
At time: 124.94315075874329 and batch: 700, loss is 3.4199400663375856 and perplexity is 30.567582938952377
At time: 125.66984748840332 and batch: 750, loss is 3.505510478019714 and perplexity is 33.29843763506199
At time: 126.39640283584595 and batch: 800, loss is 3.4760097455978394 and perplexity is 32.33045759754827
At time: 127.12243604660034 and batch: 850, loss is 3.536450595855713 and perplexity is 34.34479902016907
At time: 127.84819293022156 and batch: 900, loss is 3.5042654705047607 and perplexity is 33.25700662628042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33103378504923 and perplexity of 76.02283723247196
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 129.6148750782013 and batch: 50, loss is 3.752551212310791 and perplexity is 42.629700803830396
At time: 130.34999251365662 and batch: 100, loss is 3.6589520597457885 and perplexity is 38.820639836704046
At time: 131.07265758514404 and batch: 150, loss is 3.6845067358016967 and perplexity is 39.825473124032754
At time: 131.79549884796143 and batch: 200, loss is 3.552187752723694 and perplexity is 34.88956379137449
At time: 132.5162057876587 and batch: 250, loss is 3.6919779872894285 and perplexity is 40.124133543655084
At time: 133.23792243003845 and batch: 300, loss is 3.638560333251953 and perplexity is 38.03703663109528
At time: 133.97411131858826 and batch: 350, loss is 3.6356569957733154 and perplexity is 37.9267624361313
At time: 134.70473909378052 and batch: 400, loss is 3.567222499847412 and perplexity is 35.41808267848748
At time: 135.43462014198303 and batch: 450, loss is 3.5917312908172607 and perplexity is 36.29686198417974
At time: 136.1759328842163 and batch: 500, loss is 3.4735234069824217 and perplexity is 32.25017298109956
At time: 136.8996980190277 and batch: 550, loss is 3.5114457702636717 and perplexity is 33.49666126937982
At time: 137.62374687194824 and batch: 600, loss is 3.5334962940216066 and perplexity is 34.24348384891893
At time: 138.34821128845215 and batch: 650, loss is 3.3590094947814944 and perplexity is 28.760689153324616
At time: 139.0759105682373 and batch: 700, loss is 3.360088291168213 and perplexity is 28.79173282275124
At time: 139.80690550804138 and batch: 750, loss is 3.4460426330566407 and perplexity is 31.375980032403106
At time: 140.53706741333008 and batch: 800, loss is 3.413701033592224 and perplexity is 30.377464482392423
At time: 141.26754808425903 and batch: 850, loss is 3.4718649196624756 and perplexity is 32.19673080697815
At time: 141.99825191497803 and batch: 900, loss is 3.4356929588317873 and perplexity is 31.052923509166597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3193773243525255 and perplexity of 75.1418247374477
finished 10 epochs...
Completing Train Step...
At time: 143.80511569976807 and batch: 50, loss is 3.728278160095215 and perplexity is 41.607405160657876
At time: 144.52967977523804 and batch: 100, loss is 3.623055963516235 and perplexity is 37.451844590986056
At time: 145.2540988922119 and batch: 150, loss is 3.6486114358901975 and perplexity is 38.421278583157395
At time: 145.98007941246033 and batch: 200, loss is 3.518886156082153 and perplexity is 33.7468188332047
At time: 146.70638823509216 and batch: 250, loss is 3.6628901863098147 and perplexity is 38.97382185679577
At time: 147.43374347686768 and batch: 300, loss is 3.6097531270980836 and perplexity is 36.956928024469796
At time: 148.16048669815063 and batch: 350, loss is 3.607969675064087 and perplexity is 36.891075855561255
At time: 148.88465428352356 and batch: 400, loss is 3.54230742931366 and perplexity is 34.546540996066526
At time: 149.6069052219391 and batch: 450, loss is 3.56837628364563 and perplexity is 35.45897107209296
At time: 150.33022022247314 and batch: 500, loss is 3.4521347379684446 and perplexity is 31.567709218647867
At time: 151.05308437347412 and batch: 550, loss is 3.492052731513977 and perplexity is 32.85331757677585
At time: 151.7792935371399 and batch: 600, loss is 3.5174059200286867 and perplexity is 33.696902528349334
At time: 152.50664353370667 and batch: 650, loss is 3.345482406616211 and perplexity is 28.374260296507305
At time: 153.23353719711304 and batch: 700, loss is 3.3490116786956787 and perplexity is 28.47457770072677
At time: 153.9803545475006 and batch: 750, loss is 3.4380593299865723 and perplexity is 31.126493263972463
At time: 154.70686101913452 and batch: 800, loss is 3.4085902738571168 and perplexity is 30.22260861270066
At time: 155.430082321167 and batch: 850, loss is 3.470179328918457 and perplexity is 32.1425060087899
At time: 156.1547441482544 and batch: 900, loss is 3.4370341396331785 and perplexity is 31.09459903496466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320074212061216 and perplexity of 75.19420840216179
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 157.94901299476624 and batch: 50, loss is 3.7222391510009767 and perplexity is 41.3568948408891
At time: 158.68953442573547 and batch: 100, loss is 3.623524866104126 and perplexity is 37.4694099757414
At time: 159.41998076438904 and batch: 150, loss is 3.65198965549469 and perplexity is 38.55129358566363
At time: 160.14693140983582 and batch: 200, loss is 3.523415904045105 and perplexity is 33.90003015944336
At time: 160.87515830993652 and batch: 250, loss is 3.6612660932540892 and perplexity is 38.910576115749315
At time: 161.60012292861938 and batch: 300, loss is 3.6067622661590577 and perplexity is 36.84656012181186
At time: 162.32371592521667 and batch: 350, loss is 3.599324731826782 and perplexity is 36.57352916304584
At time: 163.04742240905762 and batch: 400, loss is 3.534764151573181 and perplexity is 34.28692724270697
At time: 163.77266812324524 and batch: 450, loss is 3.559886837005615 and perplexity is 35.159218197564584
At time: 164.49755311012268 and batch: 500, loss is 3.442204918861389 and perplexity is 31.25579874665894
At time: 165.22573065757751 and batch: 550, loss is 3.478299560546875 and perplexity is 32.40457318577133
At time: 165.95352435112 and batch: 600, loss is 3.5076673460006713 and perplexity is 33.37033547821798
At time: 166.67963194847107 and batch: 650, loss is 3.3316452360153197 and perplexity is 27.9843447017332
At time: 167.4094226360321 and batch: 700, loss is 3.3324567794799806 and perplexity is 28.007064431565905
At time: 168.1359691619873 and batch: 750, loss is 3.419684329032898 and perplexity is 30.559766667179492
At time: 168.86662912368774 and batch: 800, loss is 3.3887993812561037 and perplexity is 29.63035614498113
At time: 169.59373211860657 and batch: 850, loss is 3.4491642045974733 and perplexity is 31.474075424975705
At time: 170.31975674629211 and batch: 900, loss is 3.415000433921814 and perplexity is 30.41696262614593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316211021109803 and perplexity of 74.90427920317404
finished 12 epochs...
Completing Train Step...
At time: 172.13060069084167 and batch: 50, loss is 3.7153795671463015 and perplexity is 41.074174533178116
At time: 172.8743257522583 and batch: 100, loss is 3.610518159866333 and perplexity is 36.98521210316636
At time: 173.60632395744324 and batch: 150, loss is 3.6385099411010744 and perplexity is 38.03511991130061
At time: 174.33835434913635 and batch: 200, loss is 3.5099434995651246 and perplexity is 33.446377995656725
At time: 175.07102727890015 and batch: 250, loss is 3.650803470611572 and perplexity is 38.50559173476868
At time: 175.80123138427734 and batch: 300, loss is 3.59785945892334 and perplexity is 36.51997820474532
At time: 176.53042459487915 and batch: 350, loss is 3.5896211194992067 and perplexity is 36.220350142023904
At time: 177.2600452899933 and batch: 400, loss is 3.5269563961029053 and perplexity is 34.02026566781647
At time: 177.98906588554382 and batch: 450, loss is 3.5527711057662965 and perplexity is 34.90992266219454
At time: 178.7186861038208 and batch: 500, loss is 3.434924159049988 and perplexity is 31.029059202960642
At time: 179.4499192237854 and batch: 550, loss is 3.472402138710022 and perplexity is 32.21403215092604
At time: 180.18166303634644 and batch: 600, loss is 3.5029341125488282 and perplexity is 33.21275910709715
At time: 180.9228253364563 and batch: 650, loss is 3.3277251291275025 and perplexity is 27.874857819666357
At time: 181.65467286109924 and batch: 700, loss is 3.3297879123687744 and perplexity is 27.93241695482943
At time: 182.3837320804596 and batch: 750, loss is 3.4185005712509153 and perplexity is 30.523612708558403
At time: 183.12200236320496 and batch: 800, loss is 3.3887772703170778 and perplexity is 29.629700997226085
At time: 183.85253620147705 and batch: 850, loss is 3.451069669723511 and perplexity is 31.53410535237582
At time: 184.58141112327576 and batch: 900, loss is 3.4183044385910035 and perplexity is 30.517626618260852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3156980749678935 and perplexity of 74.8658671946323
finished 13 epochs...
Completing Train Step...
At time: 186.41608262062073 and batch: 50, loss is 3.7101329517364503 and perplexity is 40.85923847259394
At time: 187.1463360786438 and batch: 100, loss is 3.6044397449493406 and perplexity is 36.76108250463619
At time: 187.87761926651 and batch: 150, loss is 3.6320319890975954 and perplexity is 37.78952655987661
At time: 188.60654616355896 and batch: 200, loss is 3.5033809518814087 and perplexity is 33.227603190422876
At time: 189.33542013168335 and batch: 250, loss is 3.644616651535034 and perplexity is 38.26810002193373
At time: 190.06406378746033 and batch: 300, loss is 3.5919987249374388 and perplexity is 36.30657030163951
At time: 190.80283761024475 and batch: 350, loss is 3.5838061714172365 and perplexity is 36.010341871792676
At time: 191.5301649570465 and batch: 400, loss is 3.5218209743499758 and perplexity is 33.84600508922309
At time: 192.25761079788208 and batch: 450, loss is 3.5480169343948362 and perplexity is 34.74434880240633
At time: 192.9880554676056 and batch: 500, loss is 3.430402889251709 and perplexity is 30.889085123450773
At time: 193.7204647064209 and batch: 550, loss is 3.468442244529724 and perplexity is 32.086720229717876
At time: 194.45078325271606 and batch: 600, loss is 3.4997257375717163 and perplexity is 33.10637087978071
At time: 195.1797981262207 and batch: 650, loss is 3.3249807167053222 and perplexity is 27.79846259155552
At time: 195.90714955329895 and batch: 700, loss is 3.327717170715332 and perplexity is 27.874635980941374
At time: 196.63506054878235 and batch: 750, loss is 3.4172161388397218 and perplexity is 30.484432358730945
At time: 197.3646969795227 and batch: 800, loss is 3.388038477897644 and perplexity is 29.607818882904635
At time: 198.09378504753113 and batch: 850, loss is 3.4511337995529177 and perplexity is 31.536127694018084
At time: 198.82303762435913 and batch: 900, loss is 3.4189526653289795 and perplexity is 30.537415372921384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315824743819563 and perplexity of 74.87535096869581
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 200.62882590293884 and batch: 50, loss is 3.7087178897857664 and perplexity is 40.80146100788454
At time: 201.3675570487976 and batch: 100, loss is 3.6050787687301638 and perplexity is 36.7845812178841
At time: 202.0961093902588 and batch: 150, loss is 3.6334776306152343 and perplexity is 37.84419617521526
At time: 202.82373666763306 and batch: 200, loss is 3.505067572593689 and perplexity is 33.28369284187074
At time: 203.5507357120514 and batch: 250, loss is 3.6445752000808715 and perplexity is 38.266513786415906
At time: 204.28368067741394 and batch: 300, loss is 3.5922291612625123 and perplexity is 36.31493761830567
At time: 205.02119064331055 and batch: 350, loss is 3.582111649513245 and perplexity is 35.94937322965927
At time: 205.75131940841675 and batch: 400, loss is 3.5201072788238523 and perplexity is 33.788053012014835
At time: 206.48285150527954 and batch: 450, loss is 3.546205201148987 and perplexity is 34.681458298186826
At time: 207.22219014167786 and batch: 500, loss is 3.4279447889328 and perplexity is 30.813249896939904
At time: 207.95116114616394 and batch: 550, loss is 3.4624568796157837 and perplexity is 31.89524310212434
At time: 208.68118524551392 and batch: 600, loss is 3.493722610473633 and perplexity is 32.90822447172754
At time: 209.42805814743042 and batch: 650, loss is 3.3203639507293703 and perplexity is 27.67041939544629
At time: 210.15515518188477 and batch: 700, loss is 3.3221648359298706 and perplexity is 27.720295541353888
At time: 210.88246250152588 and batch: 750, loss is 3.410681290626526 and perplexity is 30.28587071213015
At time: 211.61017847061157 and batch: 800, loss is 3.3815680265426638 and perplexity is 29.416861387917024
At time: 212.33758401870728 and batch: 850, loss is 3.4422496223449706 and perplexity is 31.25719602097632
At time: 213.06438207626343 and batch: 900, loss is 3.4100829935073853 and perplexity is 30.26775618240825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313606157694777 and perplexity of 74.70941769064436
finished 15 epochs...
Completing Train Step...
At time: 214.8627688884735 and batch: 50, loss is 3.7062141942977904 and perplexity is 40.699434349194874
At time: 215.60666298866272 and batch: 100, loss is 3.6017444801330565 and perplexity is 36.662135057039926
At time: 216.33761477470398 and batch: 150, loss is 3.6300487995147703 and perplexity is 37.71465702923853
At time: 217.0706114768982 and batch: 200, loss is 3.5013644218444826 and perplexity is 33.16066624339911
At time: 217.79969573020935 and batch: 250, loss is 3.64143208026886 and perplexity is 38.146426372226536
At time: 218.52688646316528 and batch: 300, loss is 3.5892320108413696 and perplexity is 36.20625923181911
At time: 219.25719571113586 and batch: 350, loss is 3.5792286443710326 and perplexity is 35.84588025884608
At time: 219.98518109321594 and batch: 400, loss is 3.517924957275391 and perplexity is 33.714397015612604
At time: 220.72568941116333 and batch: 450, loss is 3.5441713905334473 and perplexity is 34.610994459477965
At time: 221.46303629875183 and batch: 500, loss is 3.4257918977737427 and perplexity is 30.746983681224933
At time: 222.18738675117493 and batch: 550, loss is 3.4610014867782595 and perplexity is 31.848856757127386
At time: 222.91036105155945 and batch: 600, loss is 3.4929858255386352 and perplexity is 32.883987117654364
At time: 223.6358880996704 and batch: 650, loss is 3.3197021675109863 and perplexity is 27.65211363413565
At time: 224.36171531677246 and batch: 700, loss is 3.321783676147461 and perplexity is 27.709731692922
At time: 225.08805537223816 and batch: 750, loss is 3.410598464012146 and perplexity is 30.28336233987694
At time: 225.81575751304626 and batch: 800, loss is 3.381808109283447 and perplexity is 29.42392471648199
At time: 226.55182790756226 and batch: 850, loss is 3.443302869796753 and perplexity is 31.290134926399364
At time: 227.29256129264832 and batch: 900, loss is 3.4116247844696046 and perplexity is 30.314458728867507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313091121307791 and perplexity of 74.67094952918494
finished 16 epochs...
Completing Train Step...
At time: 229.12247347831726 and batch: 50, loss is 3.70449107170105 and perplexity is 40.62936462089661
At time: 229.86019492149353 and batch: 100, loss is 3.5998274660110474 and perplexity is 36.59192054899718
At time: 230.5862593650818 and batch: 150, loss is 3.6280633401870728 and perplexity is 37.6398503989841
At time: 231.3119194507599 and batch: 200, loss is 3.4992783308029174 and perplexity is 33.09156217836238
At time: 232.0375657081604 and batch: 250, loss is 3.6394558191299438 and perplexity is 38.0711135156463
At time: 232.76479744911194 and batch: 300, loss is 3.5873896503448486 and perplexity is 36.139615659676586
At time: 233.49901604652405 and batch: 350, loss is 3.577436408996582 and perplexity is 35.78169354025289
At time: 234.2326238155365 and batch: 400, loss is 3.5164791202545165 and perplexity is 33.66568671433493
At time: 234.9659640789032 and batch: 450, loss is 3.5427920818328857 and perplexity is 34.56328812213142
At time: 235.69437265396118 and batch: 500, loss is 3.424440498352051 and perplexity is 30.705460288922975
At time: 236.43423628807068 and batch: 550, loss is 3.460016870498657 and perplexity is 31.817513287455057
At time: 237.18381452560425 and batch: 600, loss is 3.4923312187194826 and perplexity is 32.86246807946921
At time: 237.9156255722046 and batch: 650, loss is 3.3191363620758056 and perplexity is 27.636472343328176
At time: 238.64712405204773 and batch: 700, loss is 3.3214350652694704 and perplexity is 27.700073462607534
At time: 239.37964344024658 and batch: 750, loss is 3.4105205965042114 and perplexity is 30.281004341726568
At time: 240.11203575134277 and batch: 800, loss is 3.3818702793121336 and perplexity is 29.42575405959025
At time: 240.84262895584106 and batch: 850, loss is 3.4437636709213257 and perplexity is 31.30455677830908
At time: 241.57192540168762 and batch: 900, loss is 3.4123144054412844 and perplexity is 30.33537142544032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312911360231165 and perplexity of 74.65752780529274
finished 17 epochs...
Completing Train Step...
At time: 243.37151622772217 and batch: 50, loss is 3.7030090045928956 and perplexity is 40.569193775596005
At time: 244.1103241443634 and batch: 100, loss is 3.598219590187073 and perplexity is 36.53313255915266
At time: 244.8366403579712 and batch: 150, loss is 3.626414251327515 and perplexity is 37.577830093568394
At time: 245.5716106891632 and batch: 200, loss is 3.497596249580383 and perplexity is 33.035946271347555
At time: 246.30020141601562 and batch: 250, loss is 3.637836675643921 and perplexity is 38.00952079737219
At time: 247.02858781814575 and batch: 300, loss is 3.5858740615844726 and perplexity is 36.084884349933944
At time: 247.75737380981445 and batch: 350, loss is 3.575941596031189 and perplexity is 35.72824655739879
At time: 248.4865951538086 and batch: 400, loss is 3.515230722427368 and perplexity is 33.62368476721605
At time: 249.21485996246338 and batch: 450, loss is 3.5416177988052366 and perplexity is 34.522724860545864
At time: 249.94242191314697 and batch: 500, loss is 3.423317542076111 and perplexity is 30.67099875260657
At time: 250.66881036758423 and batch: 550, loss is 3.4591216230392456 and perplexity is 31.7890414860752
At time: 251.3961682319641 and batch: 600, loss is 3.491664695739746 and perplexity is 32.84057178732495
At time: 252.1221079826355 and batch: 650, loss is 3.3185759973526 and perplexity is 27.620990177388073
At time: 252.84607076644897 and batch: 700, loss is 3.321035737991333 and perplexity is 27.689014275931925
At time: 253.57441401481628 and batch: 750, loss is 3.410350618362427 and perplexity is 30.275857670300407
At time: 254.30563139915466 and batch: 800, loss is 3.381801042556763 and perplexity is 29.423716786382734
At time: 255.03973841667175 and batch: 850, loss is 3.443955979347229 and perplexity is 31.31057748724469
At time: 255.76661801338196 and batch: 900, loss is 3.4126543617248535 and perplexity is 30.345685878703062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312857431908176 and perplexity of 74.65350175857968
finished 18 epochs...
Completing Train Step...
At time: 257.5778822898865 and batch: 50, loss is 3.7016569709777833 and perplexity is 40.51437992530743
At time: 258.32801818847656 and batch: 100, loss is 3.596773133277893 and perplexity is 36.48032715672487
At time: 259.0623314380646 and batch: 150, loss is 3.6249303913116453 and perplexity is 37.52211120374945
At time: 259.80273938179016 and batch: 200, loss is 3.496108922958374 and perplexity is 32.98684755094301
At time: 260.5440058708191 and batch: 250, loss is 3.6363917636871337 and perplexity is 37.954640044776326
At time: 261.28238797187805 and batch: 300, loss is 3.5845123767852782 and perplexity is 36.03578155028903
At time: 262.01376962661743 and batch: 350, loss is 3.57459596157074 and perplexity is 35.68020173025695
At time: 262.7480251789093 and batch: 400, loss is 3.514077548980713 and perplexity is 33.584933174711885
At time: 263.48421931266785 and batch: 450, loss is 3.540541296005249 and perplexity is 34.485581046867026
At time: 264.2317705154419 and batch: 500, loss is 3.42229483127594 and perplexity is 30.639647225435613
At time: 264.9595265388489 and batch: 550, loss is 3.4582610750198364 and perplexity is 31.761697256584167
At time: 265.68885827064514 and batch: 600, loss is 3.4909892177581785 and perplexity is 32.818396194586015
At time: 266.41805839538574 and batch: 650, loss is 3.318007187843323 and perplexity is 27.60528356297882
At time: 267.1473321914673 and batch: 700, loss is 3.320598998069763 and perplexity is 27.676924018349958
At time: 267.87489318847656 and batch: 750, loss is 3.4101078605651853 and perplexity is 30.26850886180914
At time: 268.60659098625183 and batch: 800, loss is 3.3816411542892455 and perplexity is 29.41901265535954
At time: 269.3341031074524 and batch: 850, loss is 3.4439962673187257 and perplexity is 31.311838952308804
At time: 270.05846309661865 and batch: 900, loss is 3.4128065395355223 and perplexity is 30.350304170135153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3128724816727315 and perplexity of 74.65462528465876
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 271.8834764957428 and batch: 50, loss is 3.701227445602417 and perplexity is 40.49698170781771
At time: 272.6175727844238 and batch: 100, loss is 3.5968572521209716 and perplexity is 36.48339596871107
At time: 273.3472514152527 and batch: 150, loss is 3.6253505992889403 and perplexity is 37.53788160739487
At time: 274.0840229988098 and batch: 200, loss is 3.4965312480926514 and perplexity is 33.000781667921146
At time: 274.82947063446045 and batch: 250, loss is 3.6363354301452637 and perplexity is 37.95250198569499
At time: 275.5626447200775 and batch: 300, loss is 3.5845306730270385 and perplexity is 36.036440875691866
At time: 276.2959086894989 and batch: 350, loss is 3.5742046213150025 and perplexity is 35.666241362792114
At time: 277.03086948394775 and batch: 400, loss is 3.5136331033706667 and perplexity is 33.570009815155174
At time: 277.76569080352783 and batch: 450, loss is 3.54031445980072 and perplexity is 34.47775935570629
At time: 278.5040361881256 and batch: 500, loss is 3.4217579317092897 and perplexity is 30.623201227437615
At time: 279.2457044124603 and batch: 550, loss is 3.4562902879714965 and perplexity is 31.69916335573868
At time: 279.9817678928375 and batch: 600, loss is 3.4887998151779174 and perplexity is 32.74662211312778
At time: 280.7196366786957 and batch: 650, loss is 3.316278910636902 and perplexity is 27.557615184571297
At time: 281.4486894607544 and batch: 700, loss is 3.318739848136902 and perplexity is 27.62551626908354
At time: 282.1895742416382 and batch: 750, loss is 3.4081068944931032 and perplexity is 30.2080031576428
At time: 282.92207646369934 and batch: 800, loss is 3.379125027656555 and perplexity is 29.3450837403689
At time: 283.6520538330078 and batch: 850, loss is 3.4409333658218384 and perplexity is 31.216080597976898
At time: 284.39055848121643 and batch: 900, loss is 3.409566349983215 and perplexity is 30.252122581032648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311912641133348 and perplexity of 74.58300312728188
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
872.7488424777985


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.58300312728188, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15559915797669777, 'rnn_dropout': 0.4043936330561957, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.003040075302124 and batch: 50, loss is 7.0834127712249755 and perplexity is 1192.0297093010488
At time: 1.7957146167755127 and batch: 100, loss is 6.133885545730591 and perplexity is 461.22479354963036
At time: 2.591898202896118 and batch: 150, loss is 5.82421028137207 and perplexity is 338.39379164872724
At time: 3.391805648803711 and batch: 200, loss is 5.546129875183105 and perplexity is 256.24393840888195
At time: 4.191960573196411 and batch: 250, loss is 5.517353372573853 and perplexity is 248.9752197843093
At time: 4.992208957672119 and batch: 300, loss is 5.376908216476441 and perplexity is 216.3523257329306
At time: 5.787691593170166 and batch: 350, loss is 5.3184498691558835 and perplexity is 204.06730566420822
At time: 6.603168487548828 and batch: 400, loss is 5.148825740814209 and perplexity is 172.22912989047742
At time: 7.3978893756866455 and batch: 450, loss is 5.1215118885040285 and perplexity is 167.58855338733693
At time: 8.192126989364624 and batch: 500, loss is 5.044183835983277 and perplexity is 155.11764607944266
At time: 8.983795642852783 and batch: 550, loss is 5.090074501037598 and perplexity is 162.40196071738342
At time: 9.774797916412354 and batch: 600, loss is 4.984825897216797 and perplexity is 146.1781227925475
At time: 10.578376770019531 and batch: 650, loss is 4.865673027038574 and perplexity is 129.75824003442565
At time: 11.370493412017822 and batch: 700, loss is 4.937201995849609 and perplexity is 139.37971843027714
At time: 12.163182735443115 and batch: 750, loss is 4.919512758255005 and perplexity is 136.93587605402985
At time: 12.95952296257019 and batch: 800, loss is 4.8811412525177005 and perplexity is 131.780973466676
At time: 13.758344888687134 and batch: 850, loss is 4.908108816146851 and perplexity is 135.38313774760954
At time: 14.557146310806274 and batch: 900, loss is 4.824116849899292 and perplexity is 124.47648840265961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.832974786627783 and perplexity of 125.583991114094
finished 1 epochs...
Completing Train Step...
At time: 16.42028522491455 and batch: 50, loss is 4.790329093933106 and perplexity is 120.34096562899006
At time: 17.149957180023193 and batch: 100, loss is 4.671798753738403 and perplexity is 106.88983811014553
At time: 17.879423141479492 and batch: 150, loss is 4.662355394363403 and perplexity is 105.88519004763599
At time: 18.608823776245117 and batch: 200, loss is 4.5450887203216555 and perplexity is 94.16878152338927
At time: 19.337687730789185 and batch: 250, loss is 4.659658069610596 and perplexity is 105.59996814454351
At time: 20.065941333770752 and batch: 300, loss is 4.601416301727295 and perplexity is 99.62531527545912
At time: 20.794424295425415 and batch: 350, loss is 4.584407835006714 and perplexity is 97.94517026492991
At time: 21.52379012107849 and batch: 400, loss is 4.474180407524109 and perplexity is 87.72267409499459
At time: 22.26156497001648 and batch: 450, loss is 4.502419710159302 and perplexity is 90.23521040534926
At time: 22.994492769241333 and batch: 500, loss is 4.395205869674682 and perplexity is 81.06131711204507
At time: 23.738709211349487 and batch: 550, loss is 4.463998136520385 and perplexity is 86.83399014272293
At time: 24.46997570991516 and batch: 600, loss is 4.427275505065918 and perplexity is 83.70305741348946
At time: 25.198619842529297 and batch: 650, loss is 4.281424465179444 and perplexity is 72.34341732505257
At time: 25.92741346359253 and batch: 700, loss is 4.321122407913208 and perplexity is 75.2730679824249
At time: 26.66793656349182 and batch: 750, loss is 4.378165664672852 and perplexity is 79.69171791682075
At time: 27.39891266822815 and batch: 800, loss is 4.339772658348084 and perplexity is 76.6901025017227
At time: 28.127363920211792 and batch: 850, loss is 4.397608203887939 and perplexity is 81.256287585867
At time: 28.85627031326294 and batch: 900, loss is 4.329912161827087 and perplexity is 75.9376160547791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.505762387628424 and perplexity of 90.53734229349804
finished 2 epochs...
Completing Train Step...
At time: 30.669480323791504 and batch: 50, loss is 4.390465936660767 and perplexity is 80.67800106277464
At time: 31.41989827156067 and batch: 100, loss is 4.267497401237488 and perplexity is 71.34286944572906
At time: 32.14932990074158 and batch: 150, loss is 4.272407793998719 and perplexity is 71.69405247307624
At time: 32.887720346450806 and batch: 200, loss is 4.160902495384216 and perplexity is 64.12937295425508
At time: 33.61873197555542 and batch: 250, loss is 4.294832448959351 and perplexity is 73.3199285854637
At time: 34.34985852241516 and batch: 300, loss is 4.259780898094177 and perplexity is 70.79447055146578
At time: 35.078800201416016 and batch: 350, loss is 4.251430530548095 and perplexity is 70.20577204733057
At time: 35.807998180389404 and batch: 400, loss is 4.162636876106262 and perplexity is 64.24069421123963
At time: 36.543025970458984 and batch: 450, loss is 4.201817240715027 and perplexity is 66.80762633482166
At time: 37.27371525764465 and batch: 500, loss is 4.080034584999084 and perplexity is 59.14751543127587
At time: 38.002098083496094 and batch: 550, loss is 4.15392189502716 and perplexity is 63.68327027427825
At time: 38.731167793273926 and batch: 600, loss is 4.144544186592102 and perplexity is 63.08885859925508
At time: 39.46242022514343 and batch: 650, loss is 3.991380467414856 and perplexity is 54.12956190635396
At time: 40.20501351356506 and batch: 700, loss is 4.016178164482117 and perplexity is 55.48863164147942
At time: 40.93745303153992 and batch: 750, loss is 4.102101750373841 and perplexity is 60.46724118501929
At time: 41.66652727127075 and batch: 800, loss is 4.070735960006714 and perplexity is 58.60007403667836
At time: 42.40198588371277 and batch: 850, loss is 4.136657590866089 and perplexity is 62.593259141736986
At time: 43.13290500640869 and batch: 900, loss is 4.078994932174683 and perplexity is 59.08605450435054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385485087355522 and perplexity of 80.27715520235567
finished 3 epochs...
Completing Train Step...
At time: 44.93223285675049 and batch: 50, loss is 4.151596841812133 and perplexity is 63.53537528047414
At time: 45.68602752685547 and batch: 100, loss is 4.030500898361206 and perplexity is 56.28909930445797
At time: 46.416608572006226 and batch: 150, loss is 4.041680369377136 and perplexity is 56.9219123252934
At time: 47.14610767364502 and batch: 200, loss is 3.928279151916504 and perplexity is 50.81944982690247
At time: 47.876898527145386 and batch: 250, loss is 4.072813477516174 and perplexity is 58.72194326544042
At time: 48.60552382469177 and batch: 300, loss is 4.04295672416687 and perplexity is 56.994611265681584
At time: 49.33390021324158 and batch: 350, loss is 4.037252674102783 and perplexity is 56.67043658289923
At time: 50.07384252548218 and batch: 400, loss is 3.9618325567245485 and perplexity is 52.55354511112766
At time: 50.802231311798096 and batch: 450, loss is 4.001014223098755 and perplexity is 54.65355282870548
At time: 51.53103566169739 and batch: 500, loss is 3.878980278968811 and perplexity is 48.37486104876216
At time: 52.261600971221924 and batch: 550, loss is 3.947133712768555 and perplexity is 51.78671829249154
At time: 52.99120759963989 and batch: 600, loss is 3.9520920181274413 and perplexity is 52.044130291599494
At time: 53.72174596786499 and batch: 650, loss is 3.7993743133544924 and perplexity is 44.67322430719468
At time: 54.450892210006714 and batch: 700, loss is 3.8171614170074464 and perplexity is 45.474940546172135
At time: 55.17945051193237 and batch: 750, loss is 3.9132257080078126 and perplexity is 50.06017130582258
At time: 55.90722632408142 and batch: 800, loss is 3.884370493888855 and perplexity is 48.63631596259628
At time: 56.63670372962952 and batch: 850, loss is 3.952919616699219 and perplexity is 52.08721976743316
At time: 57.366013288497925 and batch: 900, loss is 3.901254167556763 and perplexity is 49.46444692412275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342665162805009 and perplexity of 76.91225009157321
finished 4 epochs...
Completing Train Step...
At time: 59.17082738876343 and batch: 50, loss is 3.9791932201385496 and perplexity is 53.47387517430034
At time: 59.89622974395752 and batch: 100, loss is 3.8617373514175415 and perplexity is 47.54788704031081
At time: 60.624735593795776 and batch: 150, loss is 3.8759145545959472 and perplexity is 48.22678415565828
At time: 61.35583186149597 and batch: 200, loss is 3.7593889808654786 and perplexity is 42.92219168433656
At time: 62.07830476760864 and batch: 250, loss is 3.9105144929885864 and perplexity is 49.92463123967077
At time: 62.800057888031006 and batch: 300, loss is 3.879535479545593 and perplexity is 48.40172625661754
At time: 63.521406412124634 and batch: 350, loss is 3.8742899799346926 and perplexity is 48.148499750773105
At time: 64.24324631690979 and batch: 400, loss is 3.8073155546188353 and perplexity is 45.02939751638067
At time: 64.96728730201721 and batch: 450, loss is 3.846097288131714 and perplexity is 46.81002026029616
At time: 65.69385600090027 and batch: 500, loss is 3.7272824239730835 and perplexity is 41.565995784218636
At time: 66.42163753509521 and batch: 550, loss is 3.7911662530899046 and perplexity is 44.30804454884604
At time: 67.14793682098389 and batch: 600, loss is 3.8065817308425904 and perplexity is 44.99636599499333
At time: 67.88687777519226 and batch: 650, loss is 3.6536926555633547 and perplexity is 38.617002376443956
At time: 68.61260080337524 and batch: 700, loss is 3.670424418449402 and perplexity is 39.268568629068916
At time: 69.33683133125305 and batch: 750, loss is 3.767270450592041 and perplexity is 43.261818258914474
At time: 70.07091283798218 and batch: 800, loss is 3.7386131381988523 and perplexity is 42.039646537157815
At time: 70.80330419540405 and batch: 850, loss is 3.81314386844635 and perplexity is 45.29260927183543
At time: 71.52904796600342 and batch: 900, loss is 3.7604065799713133 and perplexity is 42.96589149889016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342813988254495 and perplexity of 76.92369744357158
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 73.33540630340576 and batch: 50, loss is 3.8654356956481934 and perplexity is 47.72406106932261
At time: 74.07685542106628 and batch: 100, loss is 3.753216018676758 and perplexity is 42.65805072286322
At time: 74.8058032989502 and batch: 150, loss is 3.7685693979263304 and perplexity is 43.318049595284215
At time: 75.53404664993286 and batch: 200, loss is 3.6339059019088746 and perplexity is 37.860407229184744
At time: 76.26014065742493 and batch: 250, loss is 3.785331015586853 and perplexity is 44.050249464871854
At time: 76.98521089553833 and batch: 300, loss is 3.7361598587036133 and perplexity is 41.93663794042399
At time: 77.71028685569763 and batch: 350, loss is 3.720736207962036 and perplexity is 41.29478446954236
At time: 78.43606734275818 and batch: 400, loss is 3.6502252340316774 and perplexity is 38.48333282917407
At time: 79.1608817577362 and batch: 450, loss is 3.6739968347549437 and perplexity is 39.40910317843219
At time: 79.88490223884583 and batch: 500, loss is 3.5513384675979616 and perplexity is 34.859945182905285
At time: 80.61243295669556 and batch: 550, loss is 3.5929464626312257 and perplexity is 36.34099571740181
At time: 81.33923816680908 and batch: 600, loss is 3.605816535949707 and perplexity is 36.811729689486086
At time: 82.074631690979 and batch: 650, loss is 3.4456984996795654 and perplexity is 31.365184368116257
At time: 82.81120109558105 and batch: 700, loss is 3.44100537776947 and perplexity is 31.218328609679062
At time: 83.53987908363342 and batch: 750, loss is 3.5236088609695435 and perplexity is 33.9065720361312
At time: 84.26594090461731 and batch: 800, loss is 3.485273976325989 and perplexity is 32.631366107064714
At time: 84.99110221862793 and batch: 850, loss is 3.538390302658081 and perplexity is 34.411482512758695
At time: 85.7161214351654 and batch: 900, loss is 3.4786187839508056 and perplexity is 32.414919135173356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.295615418316567 and perplexity of 73.37735832274772
finished 6 epochs...
Completing Train Step...
At time: 87.53311157226562 and batch: 50, loss is 3.76568274974823 and perplexity is 43.19318593177918
At time: 88.27723336219788 and batch: 100, loss is 3.648334846496582 and perplexity is 38.41065313452307
At time: 89.00412321090698 and batch: 150, loss is 3.6630105543136597 and perplexity is 38.978513340281474
At time: 89.73257231712341 and batch: 200, loss is 3.5329188966751097 and perplexity is 34.223717459289375
At time: 90.46032500267029 and batch: 250, loss is 3.686078872680664 and perplexity is 39.88813346146101
At time: 91.19651508331299 and batch: 300, loss is 3.6431311082839968 and perplexity is 38.21129330907752
At time: 91.92648673057556 and batch: 350, loss is 3.6297661113739013 and perplexity is 37.7039970497553
At time: 92.65546774864197 and batch: 400, loss is 3.5654243803024293 and perplexity is 35.35445395494707
At time: 93.38467478752136 and batch: 450, loss is 3.593639440536499 and perplexity is 36.366187952312536
At time: 94.11242651939392 and batch: 500, loss is 3.47515615940094 and perplexity is 32.302872539990645
At time: 94.84195804595947 and batch: 550, loss is 3.5183173322677614 and perplexity is 33.7276282975267
At time: 95.57265090942383 and batch: 600, loss is 3.5381233930587768 and perplexity is 34.402298983390345
At time: 96.29949617385864 and batch: 650, loss is 3.38535165309906 and perplexity is 29.528374635078162
At time: 97.03151893615723 and batch: 700, loss is 3.3830450344085694 and perplexity is 29.46034242659062
At time: 97.7625048160553 and batch: 750, loss is 3.4737499952316284 and perplexity is 32.25748131929247
At time: 98.49372506141663 and batch: 800, loss is 3.4402343893051146 and perplexity is 31.19426891451006
At time: 99.2266149520874 and batch: 850, loss is 3.5007730674743653 and perplexity is 33.141062335499676
At time: 99.9561710357666 and batch: 900, loss is 3.451201095581055 and perplexity is 31.538250021566025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303498150551156 and perplexity of 73.95805813411675
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 101.76205611228943 and batch: 50, loss is 3.7263393354415895 and perplexity is 41.526813849215266
At time: 102.48582148551941 and batch: 100, loss is 3.617500705718994 and perplexity is 37.24436676927323
At time: 103.22295618057251 and batch: 150, loss is 3.6370733690261843 and perplexity is 37.9805189486702
At time: 103.96260285377502 and batch: 200, loss is 3.5040864086151124 and perplexity is 33.25105209696042
At time: 104.71963763237 and batch: 250, loss is 3.655691843032837 and perplexity is 38.69428222641875
At time: 105.44799757003784 and batch: 300, loss is 3.6093170642852783 and perplexity is 36.94081599566641
At time: 106.17532920837402 and batch: 350, loss is 3.592760763168335 and perplexity is 36.334247840574115
At time: 106.90299654006958 and batch: 400, loss is 3.5262681579589845 and perplexity is 33.99685967868871
At time: 107.6303551197052 and batch: 450, loss is 3.5465442514419556 and perplexity is 34.69321905041399
At time: 108.35784459114075 and batch: 500, loss is 3.424538941383362 and perplexity is 30.708483176300277
At time: 109.08588147163391 and batch: 550, loss is 3.4617666578292847 and perplexity is 31.873235906246293
At time: 109.81552457809448 and batch: 600, loss is 3.4845129919052122 and perplexity is 32.60654359180017
At time: 110.5421097278595 and batch: 650, loss is 3.322917618751526 and perplexity is 27.741170759915796
At time: 111.26735615730286 and batch: 700, loss is 3.311591000556946 and perplexity is 27.42872989991775
At time: 112.0022177696228 and batch: 750, loss is 3.402561807632446 and perplexity is 30.040960716686957
At time: 112.72927689552307 and batch: 800, loss is 3.362042784690857 and perplexity is 28.848061106760746
At time: 113.45781373977661 and batch: 850, loss is 3.4171143531799317 and perplexity is 30.481329638578877
At time: 114.18589329719543 and batch: 900, loss is 3.370523371696472 and perplexity is 29.093749919094112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.291948710402397 and perplexity of 73.10879764993118
finished 8 epochs...
Completing Train Step...
At time: 115.99090600013733 and batch: 50, loss is 3.6983505487442017 and perplexity is 40.380643495134336
At time: 116.74305963516235 and batch: 100, loss is 3.58216281414032 and perplexity is 35.95121261298944
At time: 117.47572159767151 and batch: 150, loss is 3.5994877576828004 and perplexity is 36.57949207998687
At time: 118.2060923576355 and batch: 200, loss is 3.4681413316726686 and perplexity is 32.077066375617264
At time: 118.93581557273865 and batch: 250, loss is 3.619877481460571 and perplexity is 37.33299355800007
At time: 119.6646659374237 and batch: 300, loss is 3.5757278299331663 and perplexity is 35.720609885802844
At time: 120.39303231239319 and batch: 350, loss is 3.5600720119476317 and perplexity is 35.165729406593435
At time: 121.12081360816956 and batch: 400, loss is 3.496106367111206 and perplexity is 32.98676324170985
At time: 121.84927725791931 and batch: 450, loss is 3.518558602333069 and perplexity is 33.73576674635127
At time: 122.57861614227295 and batch: 500, loss is 3.39941997051239 and perplexity is 29.94672502529954
At time: 123.32394170761108 and batch: 550, loss is 3.4381160497665406 and perplexity is 31.128258801891572
At time: 124.05646395683289 and batch: 600, loss is 3.4639277172088625 and perplexity is 31.94219034213842
At time: 124.78738188743591 and batch: 650, loss is 3.3059914016723635 and perplexity is 27.275569233914243
At time: 125.51558756828308 and batch: 700, loss is 3.2970283031463623 and perplexity is 27.032187974001143
At time: 126.24528336524963 and batch: 750, loss is 3.3918233823776247 and perplexity is 29.720093990482752
At time: 126.97660899162292 and batch: 800, loss is 3.3545121479034425 and perplexity is 28.631632780912902
At time: 127.71794843673706 and batch: 850, loss is 3.4134510135650635 and perplexity is 30.369870457266206
At time: 128.44895458221436 and batch: 900, loss is 3.3701482629776 and perplexity is 29.08283864641947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.294262193653681 and perplexity of 73.27812942640536
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 130.25785303115845 and batch: 50, loss is 3.6887264394760133 and perplexity is 39.993879882728784
At time: 131.01346707344055 and batch: 100, loss is 3.5795919036865236 and perplexity is 35.85890397412181
At time: 131.74287390708923 and batch: 150, loss is 3.5981334018707276 and perplexity is 36.52998396565451
At time: 132.4820532798767 and batch: 200, loss is 3.4661270999908447 and perplexity is 32.01252075899898
At time: 133.21145486831665 and batch: 250, loss is 3.6153888511657715 and perplexity is 37.16579507892268
At time: 133.93962025642395 and batch: 300, loss is 3.570260510444641 and perplexity is 35.52584680038628
At time: 134.66841983795166 and batch: 350, loss is 3.5544892358779907 and perplexity is 34.96995400758088
At time: 135.4042513370514 and batch: 400, loss is 3.4895022249221803 and perplexity is 32.76963173973867
At time: 136.13628125190735 and batch: 450, loss is 3.5058296060562135 and perplexity is 33.309065795864804
At time: 136.8683307170868 and batch: 500, loss is 3.386624894142151 and perplexity is 29.5659953186161
At time: 137.59647369384766 and batch: 550, loss is 3.422383966445923 and perplexity is 30.64237841732008
At time: 138.32622957229614 and batch: 600, loss is 3.4521357154846193 and perplexity is 31.567740076609308
At time: 139.0562891960144 and batch: 650, loss is 3.286596350669861 and perplexity is 26.75165526973117
At time: 139.7871115207672 and batch: 700, loss is 3.2760431957244873 and perplexity is 26.470825333847262
At time: 140.52548837661743 and batch: 750, loss is 3.3677768325805664 and perplexity is 29.013952430537312
At time: 141.26701879501343 and batch: 800, loss is 3.329588837623596 and perplexity is 27.92685686949658
At time: 141.99812150001526 and batch: 850, loss is 3.386718487739563 and perplexity is 29.568762635978615
At time: 142.72823786735535 and batch: 900, loss is 3.3448256158828737 and perplexity is 28.355630463899672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2885938670537245 and perplexity of 72.86394004551408
finished 10 epochs...
Completing Train Step...
At time: 144.55543184280396 and batch: 50, loss is 3.6758293294906617 and perplexity is 39.481386361594886
At time: 145.29066705703735 and batch: 100, loss is 3.5639943504333496 and perplexity is 35.30393216223249
At time: 146.02188849449158 and batch: 150, loss is 3.5846417999267577 and perplexity is 36.040445716161926
At time: 146.75285983085632 and batch: 200, loss is 3.4526261711120605 and perplexity is 31.583226449754477
At time: 147.4842402935028 and batch: 250, loss is 3.601850543022156 and perplexity is 36.666023755224266
At time: 148.21381092071533 and batch: 300, loss is 3.5583665990829467 and perplexity is 35.105808428793615
At time: 148.94409918785095 and batch: 350, loss is 3.54245011806488 and perplexity is 34.551470750562125
At time: 149.6709394454956 and batch: 400, loss is 3.478479413986206 and perplexity is 32.41040178383984
At time: 150.39775466918945 and batch: 450, loss is 3.4965941524505615 and perplexity is 33.00285762619523
At time: 151.12438225746155 and batch: 500, loss is 3.378881583213806 and perplexity is 29.33794071231082
At time: 151.8509361743927 and batch: 550, loss is 3.415197548866272 and perplexity is 30.422958854998306
At time: 152.58777952194214 and batch: 600, loss is 3.4461273097991945 and perplexity is 31.378636960675113
At time: 153.31634855270386 and batch: 650, loss is 3.282062568664551 and perplexity is 26.630643623952636
At time: 154.0473222732544 and batch: 700, loss is 3.27284029006958 and perplexity is 26.38617740971697
At time: 154.7826702594757 and batch: 750, loss is 3.366297721862793 and perplexity is 28.97106930479598
At time: 155.52124619483948 and batch: 800, loss is 3.329667496681213 and perplexity is 27.929053656137594
At time: 156.26120042800903 and batch: 850, loss is 3.3886023807525634 and perplexity is 29.62451952482792
At time: 157.00229287147522 and batch: 900, loss is 3.348042402267456 and perplexity is 28.4469913353245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.288656574406036 and perplexity of 72.86850929353456
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 158.80487442016602 and batch: 50, loss is 3.672185363769531 and perplexity is 39.337779351488535
At time: 159.54769921302795 and batch: 100, loss is 3.561820554733276 and perplexity is 35.22727197827909
At time: 160.26999711990356 and batch: 150, loss is 3.5848909425735473 and perplexity is 36.04942604684435
At time: 160.9934482574463 and batch: 200, loss is 3.452924523353577 and perplexity is 31.592650781975493
At time: 161.72134399414062 and batch: 250, loss is 3.601093125343323 and perplexity is 36.63826277527521
At time: 162.44885516166687 and batch: 300, loss is 3.5567470264434813 and perplexity is 35.04899803867064
At time: 163.1768057346344 and batch: 350, loss is 3.5408609247207643 and perplexity is 34.49660539059539
At time: 163.90282654762268 and batch: 400, loss is 3.4773781728744506 and perplexity is 32.37472976230035
At time: 164.63507199287415 and batch: 450, loss is 3.493187875747681 and perplexity is 32.89063200540361
At time: 165.36368417739868 and batch: 500, loss is 3.375172119140625 and perplexity is 29.229314272754266
At time: 166.09029030799866 and batch: 550, loss is 3.411155252456665 and perplexity is 30.300228461083936
At time: 166.8183147907257 and batch: 600, loss is 3.4432636451721192 and perplexity is 31.288907606672876
At time: 167.54511213302612 and batch: 650, loss is 3.275783281326294 and perplexity is 26.46394607925832
At time: 168.26583075523376 and batch: 700, loss is 3.26591826915741 and perplexity is 26.204162420436347
At time: 168.9869954586029 and batch: 750, loss is 3.3584610652923583 and perplexity is 28.744920267721238
At time: 169.70995569229126 and batch: 800, loss is 3.3223480939865113 and perplexity is 27.725375974345226
At time: 170.43572688102722 and batch: 850, loss is 3.3788175439834593 and perplexity is 29.33606199332415
At time: 171.15880608558655 and batch: 900, loss is 3.337971777915955 and perplexity is 28.161950052507844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.287892380805864 and perplexity of 72.8128449169675
finished 12 epochs...
Completing Train Step...
At time: 172.9563443660736 and batch: 50, loss is 3.6689954710006716 and perplexity is 39.21249598008666
At time: 173.6986038684845 and batch: 100, loss is 3.5571311473846436 and perplexity is 35.06246367883567
At time: 174.41975331306458 and batch: 150, loss is 3.5804876708984374 and perplexity is 35.89103959542614
At time: 175.14064383506775 and batch: 200, loss is 3.4486715745925904 and perplexity is 31.458574169550676
At time: 175.86212348937988 and batch: 250, loss is 3.5968326139450073 and perplexity is 36.48249709545476
At time: 176.5930256843567 and batch: 300, loss is 3.553607869148254 and perplexity is 34.93914623204668
At time: 177.3301064968109 and batch: 350, loss is 3.5369182872772216 and perplexity is 34.36086554483926
At time: 178.08736872673035 and batch: 400, loss is 3.473798599243164 and perplexity is 32.259049200388965
At time: 178.81535649299622 and batch: 450, loss is 3.490373229980469 and perplexity is 32.798186688689455
At time: 179.5513153076172 and batch: 500, loss is 3.3728631019592283 and perplexity is 29.16190114288702
At time: 180.29171657562256 and batch: 550, loss is 3.4089682149887084 and perplexity is 30.234033138362484
At time: 181.03050065040588 and batch: 600, loss is 3.44160409450531 and perplexity is 31.2370251418875
At time: 181.77113437652588 and batch: 650, loss is 3.274697256088257 and perplexity is 26.43522116673148
At time: 182.50235223770142 and batch: 700, loss is 3.2651601791381837 and perplexity is 26.18430483431317
At time: 183.2325210571289 and batch: 750, loss is 3.358462872505188 and perplexity is 28.74497221595687
At time: 183.9609034061432 and batch: 800, loss is 3.322630352973938 and perplexity is 27.73320281543992
At time: 184.69406056404114 and batch: 850, loss is 3.380003991127014 and perplexity is 29.370888336001375
At time: 185.43140935897827 and batch: 900, loss is 3.3399073028564454 and perplexity is 28.21651099420577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.287566302573844 and perplexity of 72.78910610379408
finished 13 epochs...
Completing Train Step...
At time: 187.27286028862 and batch: 50, loss is 3.6668755054473876 and perplexity is 39.12945489257375
At time: 188.0012502670288 and batch: 100, loss is 3.5546059465408324 and perplexity is 34.97403561227141
At time: 188.73160862922668 and batch: 150, loss is 3.5779590606689453 and perplexity is 35.800399790222286
At time: 189.46943831443787 and batch: 200, loss is 3.4460828351974486 and perplexity is 31.377241439325815
At time: 190.2089376449585 and batch: 250, loss is 3.5942278861999513 and perplexity is 36.38759377537357
At time: 190.94334387779236 and batch: 300, loss is 3.5512613916397093 and perplexity is 34.85725842276933
At time: 191.67799401283264 and batch: 350, loss is 3.534531245231628 and perplexity is 34.278942529801014
At time: 192.41728901863098 and batch: 400, loss is 3.4714949131011963 and perplexity is 32.18482000900041
At time: 193.17147183418274 and batch: 450, loss is 3.488439702987671 and perplexity is 32.734831778364494
At time: 193.9085714817047 and batch: 500, loss is 3.3712787866592406 and perplexity is 29.1157360763966
At time: 194.64202737808228 and batch: 550, loss is 3.40746253490448 and perplexity is 30.188544610978838
At time: 195.37937831878662 and batch: 600, loss is 3.4403843593597414 and perplexity is 31.198947471536172
At time: 196.13567471504211 and batch: 650, loss is 3.2738582706451416 and perplexity is 26.413051702216478
At time: 196.88496780395508 and batch: 700, loss is 3.2645966815948486 and perplexity is 26.16955419922873
At time: 197.62263107299805 and batch: 750, loss is 3.358315906524658 and perplexity is 28.740747993345966
At time: 198.3672080039978 and batch: 800, loss is 3.322756972312927 and perplexity is 27.736714597573542
At time: 199.10485744476318 and batch: 850, loss is 3.380629539489746 and perplexity is 29.389266994882618
At time: 199.8373122215271 and batch: 900, loss is 3.340851707458496 and perplexity is 28.243171384158455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.287534948897688 and perplexity of 72.78682393351103
finished 14 epochs...
Completing Train Step...
At time: 201.74215388298035 and batch: 50, loss is 3.665037202835083 and perplexity is 39.05758918912018
At time: 202.4970188140869 and batch: 100, loss is 3.5525952482223513 and perplexity is 34.90378402871417
At time: 203.22701001167297 and batch: 150, loss is 3.575894637107849 and perplexity is 35.72656883679991
At time: 203.95620465278625 and batch: 200, loss is 3.4440031814575196 and perplexity is 31.312055447457652
At time: 204.6845624446869 and batch: 250, loss is 3.59210636138916 and perplexity is 36.3104784223653
At time: 205.41445541381836 and batch: 300, loss is 3.549273443222046 and perplexity is 34.7880328223031
At time: 206.14650011062622 and batch: 350, loss is 3.532547745704651 and perplexity is 34.211017650260665
At time: 206.87806177139282 and batch: 400, loss is 3.469631066322327 and perplexity is 32.12488830500472
At time: 207.6086142063141 and batch: 450, loss is 3.4867934656143187 and perplexity is 32.6809868078424
At time: 208.33688640594482 and batch: 500, loss is 3.3698974800109864 and perplexity is 29.075546080330078
At time: 209.0639681816101 and batch: 550, loss is 3.40616982460022 and perplexity is 30.1495447814614
At time: 209.7911217212677 and batch: 600, loss is 3.4393216609954833 and perplexity is 31.165810011772326
At time: 210.52075123786926 and batch: 650, loss is 3.273051242828369 and perplexity is 26.391744233783257
At time: 211.24903631210327 and batch: 700, loss is 3.264020490646362 and perplexity is 26.154479882232764
At time: 211.97571420669556 and batch: 750, loss is 3.3580274391174316 and perplexity is 28.732458419983924
At time: 212.7099313735962 and batch: 800, loss is 3.3227099180221558 and perplexity is 27.73540949684536
At time: 213.44593214988708 and batch: 850, loss is 3.3809170961380004 and perplexity is 29.397719289192786
At time: 214.17863035202026 and batch: 900, loss is 3.3413320875167845 and perplexity is 28.256742099763834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.287613960161601 and perplexity of 72.79257513966826
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 216.0151698589325 and batch: 50, loss is 3.6640323877334593 and perplexity is 39.018363244381156
At time: 216.75899243354797 and batch: 100, loss is 3.5519036149978636 and perplexity is 34.87965175832154
At time: 217.48733401298523 and batch: 150, loss is 3.5760482692718507 and perplexity is 35.73205800852853
At time: 218.21680402755737 and batch: 200, loss is 3.444301266670227 and perplexity is 31.32139049941737
At time: 218.94599890708923 and batch: 250, loss is 3.5922786474227903 and perplexity is 36.316734749595334
At time: 219.68450474739075 and batch: 300, loss is 3.549652142524719 and perplexity is 34.80120952092009
At time: 220.41588068008423 and batch: 350, loss is 3.5323687553405763 and perplexity is 34.20489475574198
At time: 221.1446509361267 and batch: 400, loss is 3.4696736431121824 and perplexity is 32.12625610874135
At time: 221.87531447410583 and batch: 450, loss is 3.4865565347671508 and perplexity is 32.67324459117293
At time: 222.61821222305298 and batch: 500, loss is 3.3685566711425783 and perplexity is 29.036587454164042
At time: 223.36104249954224 and batch: 550, loss is 3.4047070932388306 and perplexity is 30.10547633478479
At time: 224.098637342453 and batch: 600, loss is 3.4376725816726683 and perplexity is 31.114457472745883
At time: 224.83698296546936 and batch: 650, loss is 3.2711810064315796 and perplexity is 26.34243156069425
At time: 225.5707015991211 and batch: 700, loss is 3.2617101097106933 and perplexity is 26.094122821280124
At time: 226.30003714561462 and batch: 750, loss is 3.3554138565063476 and perplexity is 28.657461813887203
At time: 227.0386447906494 and batch: 800, loss is 3.320082354545593 and perplexity is 27.662628607920517
At time: 227.77105355262756 and batch: 850, loss is 3.3777786254882813 and perplexity is 29.305600042425624
At time: 228.5102252960205 and batch: 900, loss is 3.3375093030929563 and perplexity is 28.148928870858445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286795420189426 and perplexity of 72.73301588637908
finished 16 epochs...
Completing Train Step...
At time: 230.39466166496277 and batch: 50, loss is 3.6632985925674437 and perplexity is 38.98974226030073
At time: 231.1271367073059 and batch: 100, loss is 3.5509347200393675 and perplexity is 34.84587340605889
At time: 231.86071681976318 and batch: 150, loss is 3.5750014781951904 and perplexity is 35.69467357931168
At time: 232.588707447052 and batch: 200, loss is 3.4432569313049317 and perplexity is 31.28869753780795
At time: 233.33216094970703 and batch: 250, loss is 3.5913355350494385 and perplexity is 36.28250013377567
At time: 234.07542443275452 and batch: 300, loss is 3.5487303066253664 and perplexity is 34.769143298810924
At time: 234.8217225074768 and batch: 350, loss is 3.5314864349365234 and perplexity is 34.17472838933782
At time: 235.55305099487305 and batch: 400, loss is 3.4688910484313964 and perplexity is 32.10112410696522
At time: 236.29094552993774 and batch: 450, loss is 3.4857922840118407 and perplexity is 32.64828357876693
At time: 237.02842450141907 and batch: 500, loss is 3.3681105613708495 and perplexity is 29.023636837676143
At time: 237.7652726173401 and batch: 550, loss is 3.404250531196594 and perplexity is 30.091734454276096
At time: 238.5053470134735 and batch: 600, loss is 3.437506427764893 and perplexity is 31.10928811351498
At time: 239.23787927627563 and batch: 650, loss is 3.270981020927429 and perplexity is 26.33716398297518
At time: 239.97640562057495 and batch: 700, loss is 3.2615944957733154 and perplexity is 26.091106151386235
At time: 240.7182629108429 and batch: 750, loss is 3.3554575061798095 and perplexity is 28.65871273003847
At time: 241.46345472335815 and batch: 800, loss is 3.3202501106262208 and perplexity is 27.667269571339673
At time: 242.19856214523315 and batch: 850, loss is 3.378095269203186 and perplexity is 29.314880945782882
At time: 242.93198919296265 and batch: 900, loss is 3.3379143619537355 and perplexity is 28.16033315346596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286471014153467 and perplexity of 72.70942468378321
finished 17 epochs...
Completing Train Step...
At time: 244.77248787879944 and batch: 50, loss is 3.6627354621887207 and perplexity is 38.967792132947196
At time: 245.52110481262207 and batch: 100, loss is 3.550235095024109 and perplexity is 34.821502887451764
At time: 246.2534909248352 and batch: 150, loss is 3.574252028465271 and perplexity is 35.66793223773538
At time: 246.98784112930298 and batch: 200, loss is 3.4425013494491576 and perplexity is 31.265065294828894
At time: 247.72064518928528 and batch: 250, loss is 3.590614900588989 and perplexity is 36.256363132610986
At time: 248.45922136306763 and batch: 300, loss is 3.5480552673339845 and perplexity is 34.74568068094196
At time: 249.19212937355042 and batch: 350, loss is 3.5308257341384888 and perplexity is 34.15215657644686
At time: 249.9272243976593 and batch: 400, loss is 3.46827889919281 and perplexity is 32.08147944163229
At time: 250.65946865081787 and batch: 450, loss is 3.4852403450012206 and perplexity is 32.63026868944744
At time: 251.3996500968933 and batch: 500, loss is 3.367736620903015 and perplexity is 29.01278575429485
At time: 252.16302275657654 and batch: 550, loss is 3.403887333869934 and perplexity is 30.080807201262402
At time: 252.89697551727295 and batch: 600, loss is 3.437292332649231 and perplexity is 31.102628479801126
At time: 253.63700199127197 and batch: 650, loss is 3.27079785823822 and perplexity is 26.332340438954052
At time: 254.39680981636047 and batch: 700, loss is 3.2614851093292234 and perplexity is 26.088252294151403
At time: 255.12007856369019 and batch: 750, loss is 3.3554675674438474 and perplexity is 28.659001074364785
At time: 255.84388780593872 and batch: 800, loss is 3.320345335006714 and perplexity is 27.669904295387536
At time: 256.56846618652344 and batch: 850, loss is 3.3783082580566406 and perplexity is 29.321125353635708
At time: 257.29192423820496 and batch: 900, loss is 3.3381997203826903 and perplexity is 28.168370088542524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.28633054968429 and perplexity of 72.69921231029538
finished 18 epochs...
Completing Train Step...
At time: 259.0737726688385 and batch: 50, loss is 3.662232909202576 and perplexity is 38.94821367266621
At time: 259.8052430152893 and batch: 100, loss is 3.5496431303024294 and perplexity is 34.80089588609721
At time: 260.5238492488861 and batch: 150, loss is 3.5736264324188234 and perplexity is 35.64562549857392
At time: 261.243284702301 and batch: 200, loss is 3.441871933937073 and perplexity is 31.24539276949175
At time: 261.9691023826599 and batch: 250, loss is 3.589997010231018 and perplexity is 36.23396759512212
At time: 262.69509387016296 and batch: 300, loss is 3.5474812459945677 and perplexity is 34.72574164204294
At time: 263.4133608341217 and batch: 350, loss is 3.530262813568115 and perplexity is 34.13293703503249
At time: 264.1315801143646 and batch: 400, loss is 3.4677480554580686 and perplexity is 32.06445370867111
At time: 264.85103702545166 and batch: 450, loss is 3.4847741746902465 and perplexity is 32.61506097191393
At time: 265.5708465576172 and batch: 500, loss is 3.367388982772827 and perplexity is 29.002701556631052
At time: 266.2890524864197 and batch: 550, loss is 3.4035560369491575 and perplexity is 30.070843173079215
At time: 267.0091850757599 and batch: 600, loss is 3.4370566177368165 and perplexity is 31.095297990439917
At time: 267.7277281284332 and batch: 650, loss is 3.2706168937683104 and perplexity is 26.32757565206661
At time: 268.44707703590393 and batch: 700, loss is 3.2613699054718017 and perplexity is 26.08524699996779
At time: 269.1676354408264 and batch: 750, loss is 3.3554491662979125 and perplexity is 28.658473720755634
At time: 269.90396904945374 and batch: 800, loss is 3.320396327972412 and perplexity is 27.671315301843535
At time: 270.6256773471832 and batch: 850, loss is 3.378459024429321 and perplexity is 29.325546326606844
At time: 271.3524098396301 and batch: 900, loss is 3.3384106063842776 and perplexity is 28.174311029890394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.286268260380993 and perplexity of 72.69468406804235
finished 19 epochs...
Completing Train Step...
At time: 273.1499915122986 and batch: 50, loss is 3.6617609691619872 and perplexity is 38.92983678785965
At time: 273.87081694602966 and batch: 100, loss is 3.549107165336609 and perplexity is 34.78224882265584
At time: 274.5918483734131 and batch: 150, loss is 3.5730657625198363 and perplexity is 35.625645670893675
At time: 275.3248538970947 and batch: 200, loss is 3.441310992240906 and perplexity is 31.227870840723902
At time: 276.0462622642517 and batch: 250, loss is 3.5894374799728395 and perplexity is 36.21369926477963
At time: 276.76740288734436 and batch: 300, loss is 3.546960234642029 and perplexity is 34.70765384880408
At time: 277.48840379714966 and batch: 350, loss is 3.5297523450851442 and perplexity is 34.11551769283573
At time: 278.20942401885986 and batch: 400, loss is 3.4672647094726563 and perplexity is 32.04895922859561
At time: 278.93135476112366 and batch: 450, loss is 3.4843500375747682 and perplexity is 32.6012306472165
At time: 279.6703248023987 and batch: 500, loss is 3.3670553970336914 and perplexity is 28.9930282825182
At time: 280.3927013874054 and batch: 550, loss is 3.4032399797439576 and perplexity is 30.061340568190403
At time: 281.11399579048157 and batch: 600, loss is 3.4368138551712035 and perplexity is 31.087750132327027
At time: 281.8373975753784 and batch: 650, loss is 3.2704352569580077 and perplexity is 26.32279402947465
At time: 282.5633752346039 and batch: 700, loss is 3.2612479066848756 and perplexity is 26.08206482559178
At time: 283.29163575172424 and batch: 750, loss is 3.355409302711487 and perplexity is 28.65733131398201
At time: 284.0149064064026 and batch: 800, loss is 3.320418529510498 and perplexity is 27.671929654423863
At time: 284.747451543808 and batch: 850, loss is 3.3785690307617187 and perplexity is 29.328772499850277
At time: 285.4720947742462 and batch: 900, loss is 3.338571844100952 and perplexity is 28.17885415772167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2862481940282535 and perplexity of 72.69322536550503
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
1165.3597750663757


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.58300312728188, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -72.69322536550503, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15559915797669777, 'rnn_dropout': 0.4043936330561957, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.873817679204861, 'rnn_dropout': 0.3464428463703234, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0148096084594727 and batch: 50, loss is 7.098501482009888 and perplexity is 1210.1522801186366
At time: 1.8282990455627441 and batch: 100, loss is 6.2996805381774905 and perplexity is 544.3979679764501
At time: 2.6214547157287598 and batch: 150, loss is 6.172586040496826 and perplexity is 479.4243150638489
At time: 3.416095495223999 and batch: 200, loss is 6.03976092338562 and perplexity is 419.7926602805808
At time: 4.210966348648071 and batch: 250, loss is 6.084070205688477 and perplexity is 438.81161838752274
At time: 5.029073238372803 and batch: 300, loss is 5.981531314849853 and perplexity is 396.04637573909747
At time: 5.821122407913208 and batch: 350, loss is 5.974794578552246 and perplexity is 393.38728260533424
At time: 6.614964008331299 and batch: 400, loss is 5.842660188674927 and perplexity is 344.69507604810036
At time: 7.409197568893433 and batch: 450, loss is 5.8401062965393065 and perplexity is 343.81588516140664
At time: 8.202651023864746 and batch: 500, loss is 5.79330418586731 and perplexity is 328.09532326320476
At time: 8.994102239608765 and batch: 550, loss is 5.833652029037475 and perplexity is 341.60395135209035
At time: 9.795238494873047 and batch: 600, loss is 5.759409418106079 and perplexity is 317.1609640733139
At time: 10.588871955871582 and batch: 650, loss is 5.669019165039063 and perplexity is 289.75019784693126
At time: 11.381800413131714 and batch: 700, loss is 5.773383445739746 and perplexity is 321.62409145499043
At time: 12.176112174987793 and batch: 750, loss is 5.722882146835327 and perplexity is 305.78497132079144
At time: 12.97410273551941 and batch: 800, loss is 5.717850561141968 and perplexity is 304.2502523068199
At time: 13.771052598953247 and batch: 850, loss is 5.75090178489685 and perplexity is 314.47412046871773
At time: 14.564338207244873 and batch: 900, loss is 5.632998476028442 and perplexity is 279.4989332716397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.5395265344071065 and perplexity of 254.55744672958073
finished 1 epochs...
Completing Train Step...
At time: 16.421164512634277 and batch: 50, loss is 5.360465717315674 and perplexity is 212.82403921340108
At time: 17.145990133285522 and batch: 100, loss is 5.152309999465943 and perplexity is 172.83026737747036
At time: 17.870699882507324 and batch: 150, loss is 5.0895686531066895 and perplexity is 162.3198307959558
At time: 18.59600329399109 and batch: 200, loss is 4.934230365753174 and perplexity is 138.96614825728247
At time: 19.321347951889038 and batch: 250, loss is 4.9947137355804445 and perplexity is 147.63067792360206
At time: 20.046570301055908 and batch: 300, loss is 4.902207021713257 and perplexity is 134.58648744436513
At time: 20.772955417633057 and batch: 350, loss is 4.877403364181519 and perplexity is 131.28931036636
At time: 21.49811053276062 and batch: 400, loss is 4.725026397705078 and perplexity is 112.73347423979642
At time: 22.22282838821411 and batch: 450, loss is 4.735209169387818 and perplexity is 113.88727795920619
At time: 22.945950984954834 and batch: 500, loss is 4.637697067260742 and perplexity is 103.30616627829515
At time: 23.682222843170166 and batch: 550, loss is 4.698797016143799 and perplexity is 109.81498730287015
At time: 24.407705783843994 and batch: 600, loss is 4.63326169013977 and perplexity is 102.84897912016332
At time: 25.133333444595337 and batch: 650, loss is 4.490667638778686 and perplexity is 89.18096667892611
At time: 25.859297037124634 and batch: 700, loss is 4.5368239784240725 and perplexity is 93.3937081518116
At time: 26.585978031158447 and batch: 750, loss is 4.568675928115844 and perplexity is 96.41636302311684
At time: 27.30939483642578 and batch: 800, loss is 4.512320346832276 and perplexity is 91.13303361547972
At time: 28.04297637939453 and batch: 850, loss is 4.564642925262451 and perplexity is 96.02829861435714
At time: 28.771790742874146 and batch: 900, loss is 4.4949969291687015 and perplexity is 89.56789393683106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.578877906276755 and perplexity of 97.40503528434307
finished 2 epochs...
Completing Train Step...
At time: 30.570250511169434 and batch: 50, loss is 4.535867204666138 and perplexity is 93.30439423611439
At time: 31.30923891067505 and batch: 100, loss is 4.399344854354858 and perplexity is 81.39752395925309
At time: 32.034717321395874 and batch: 150, loss is 4.3958125400543215 and perplexity is 81.11050953235488
At time: 32.760035276412964 and batch: 200, loss is 4.2825039100646975 and perplexity is 72.42155021935572
At time: 33.48540139198303 and batch: 250, loss is 4.413576536178589 and perplexity is 82.56423002416749
At time: 34.209930181503296 and batch: 300, loss is 4.366051483154297 and perplexity is 78.73214195420394
At time: 34.934386253356934 and batch: 350, loss is 4.3682285499572755 and perplexity is 78.90373380246476
At time: 35.659300327301025 and batch: 400, loss is 4.265980310440064 and perplexity is 71.23471789363644
At time: 36.38355207443237 and batch: 450, loss is 4.300163984298706 and perplexity is 73.71188029955962
At time: 37.1063711643219 and batch: 500, loss is 4.178581376075744 and perplexity is 65.27318939390763
At time: 37.83087658882141 and batch: 550, loss is 4.2523676919937134 and perplexity is 70.27159702965331
At time: 38.555752754211426 and batch: 600, loss is 4.235270471572876 and perplexity is 69.08036048071754
At time: 39.28109121322632 and batch: 650, loss is 4.083762278556824 and perplexity is 59.368410702654245
At time: 40.01119685173035 and batch: 700, loss is 4.103558225631714 and perplexity is 60.555374392054944
At time: 40.74846792221069 and batch: 750, loss is 4.191063652038574 and perplexity is 66.09305359748154
At time: 41.477861642837524 and batch: 800, loss is 4.146849336624146 and perplexity is 63.23445563070582
At time: 42.223448753356934 and batch: 850, loss is 4.215024228096008 and perplexity is 67.69580599155022
At time: 42.947596073150635 and batch: 900, loss is 4.16012505531311 and perplexity is 64.07953558527957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394091880484803 and perplexity of 80.97106595980212
finished 3 epochs...
Completing Train Step...
At time: 44.756855487823486 and batch: 50, loss is 4.232466068267822 and perplexity is 68.88690268310489
At time: 45.4953248500824 and batch: 100, loss is 4.098444075584411 and perplexity is 60.246475671704175
At time: 46.22057747840881 and batch: 150, loss is 4.1036587953567505 and perplexity is 60.561464735654006
At time: 46.94959115982056 and batch: 200, loss is 3.9935099840164185 and perplexity is 54.244954528714096
At time: 47.67575430870056 and batch: 250, loss is 4.132271165847778 and perplexity is 62.31929979419266
At time: 48.40201807022095 and batch: 300, loss is 4.098288025856018 and perplexity is 60.23707495904735
At time: 49.12748885154724 and batch: 350, loss is 4.100800671577454 and perplexity is 60.388619697103785
At time: 49.85398888587952 and batch: 400, loss is 4.016921672821045 and perplexity is 55.5299032428092
At time: 50.579338788986206 and batch: 450, loss is 4.056284403800964 and perplexity is 57.759301640560736
At time: 51.3058295249939 and batch: 500, loss is 3.930607042312622 and perplexity is 50.93788974017261
At time: 52.031678199768066 and batch: 550, loss is 4.003137068748474 and perplexity is 54.76969712015475
At time: 52.75741982460022 and batch: 600, loss is 4.003672780990601 and perplexity is 54.799045777915325
At time: 53.48365044593811 and batch: 650, loss is 3.8545769262313843 and perplexity is 47.208639978481315
At time: 54.21095013618469 and batch: 700, loss is 3.8678395557403564 and perplexity is 47.83892103349487
At time: 54.93887972831726 and batch: 750, loss is 3.970275459289551 and perplexity is 52.99912793125838
At time: 55.66188073158264 and batch: 800, loss is 3.929641728401184 and perplexity is 50.888742411703475
At time: 56.38882231712341 and batch: 850, loss is 3.9978595113754274 and perplexity is 54.48140830082349
At time: 57.11488699913025 and batch: 900, loss is 3.9503323364257814 and perplexity is 51.95262971740043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3413328405928935 and perplexity of 76.80984642488022
finished 4 epochs...
Completing Train Step...
At time: 58.937466621398926 and batch: 50, loss is 4.030818023681641 and perplexity is 56.306952833865374
At time: 59.66439890861511 and batch: 100, loss is 3.903482174873352 and perplexity is 49.574776936177784
At time: 60.40758681297302 and batch: 150, loss is 3.9114774799346925 and perplexity is 49.97273116392621
At time: 61.13374614715576 and batch: 200, loss is 3.8033954095840454 and perplexity is 44.85322129108084
At time: 61.86075711250305 and batch: 250, loss is 3.941878080368042 and perplexity is 51.51526030445927
At time: 62.58747935295105 and batch: 300, loss is 3.912266674041748 and perplexity is 50.012184915160496
At time: 63.31493425369263 and batch: 350, loss is 3.917138900756836 and perplexity is 50.25645019328156
At time: 64.05211329460144 and batch: 400, loss is 3.8405404186248777 and perplexity is 46.55062446786744
At time: 64.77860999107361 and batch: 450, loss is 3.8793682861328125 and perplexity is 48.39363448328467
At time: 65.5155279636383 and batch: 500, loss is 3.7593965578079223 and perplexity is 42.9225169045446
At time: 66.24300718307495 and batch: 550, loss is 3.828257565498352 and perplexity is 45.9823471624956
At time: 66.97026824951172 and batch: 600, loss is 3.8327578592300413 and perplexity is 46.189747562525696
At time: 67.69737339019775 and batch: 650, loss is 3.692384738922119 and perplexity is 40.140457420140905
At time: 68.42429709434509 and batch: 700, loss is 3.7030605936050414 and perplexity is 40.57128675421332
At time: 69.15059924125671 and batch: 750, loss is 3.809134292602539 and perplexity is 45.11136871149465
At time: 69.87684369087219 and batch: 800, loss is 3.7677920770645144 and perplexity is 43.28439065523357
At time: 70.60385847091675 and batch: 850, loss is 3.8320980930328368 and perplexity is 46.15928317921677
At time: 71.33033490180969 and batch: 900, loss is 3.791625542640686 and perplexity is 44.328399444760564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320355559048587 and perplexity of 75.2153670424838
finished 5 epochs...
Completing Train Step...
At time: 73.1286506652832 and batch: 50, loss is 3.877221064567566 and perplexity is 48.2898341087843
At time: 73.86533999443054 and batch: 100, loss is 3.7551937961578368 and perplexity is 42.742502340695175
At time: 74.59713840484619 and batch: 150, loss is 3.7641255140304564 and perplexity is 43.12597630408004
At time: 75.32006788253784 and batch: 200, loss is 3.656033434867859 and perplexity is 38.70750213506721
At time: 76.04170203208923 and batch: 250, loss is 3.7940635824203492 and perplexity is 44.43660569795204
At time: 76.76504349708557 and batch: 300, loss is 3.76637882232666 and perplexity is 43.223261990426806
At time: 77.48886680603027 and batch: 350, loss is 3.7723507928848266 and perplexity is 43.4821623416383
At time: 78.22347903251648 and batch: 400, loss is 3.6999558925628664 and perplexity is 40.445520372476956
At time: 78.94686150550842 and batch: 450, loss is 3.737302236557007 and perplexity is 41.98457280150175
At time: 79.67043995857239 and batch: 500, loss is 3.6270570039749144 and perplexity is 37.60199110729053
At time: 80.39447355270386 and batch: 550, loss is 3.69173041343689 and perplexity is 40.114201086892926
At time: 81.11782169342041 and batch: 600, loss is 3.699360909461975 and perplexity is 40.421463128884966
At time: 81.85113859176636 and batch: 650, loss is 3.563025059700012 and perplexity is 35.26972896703524
At time: 82.57809782028198 and batch: 700, loss is 3.572823405265808 and perplexity is 35.61701258342384
At time: 83.30160117149353 and batch: 750, loss is 3.6777281379699707 and perplexity is 39.55642517241242
At time: 84.02331948280334 and batch: 800, loss is 3.64100465297699 and perplexity is 38.13012503257464
At time: 84.74684524536133 and batch: 850, loss is 3.6990359687805174 and perplexity is 40.40833068485856
At time: 85.4698600769043 and batch: 900, loss is 3.663042230606079 and perplexity is 38.9797480546236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.331520394103168 and perplexity of 76.05983963549738
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 87.27406430244446 and batch: 50, loss is 3.781969528198242 and perplexity is 43.90242370324484
At time: 88.00940728187561 and batch: 100, loss is 3.6656044340133667 and perplexity is 39.07975015605835
At time: 88.73309564590454 and batch: 150, loss is 3.6775765657424926 and perplexity is 39.550429971302435
At time: 89.45758295059204 and batch: 200, loss is 3.5582633590698243 and perplexity is 35.102184291751946
At time: 90.18063879013062 and batch: 250, loss is 3.686355962753296 and perplexity is 39.89918759868412
At time: 90.90545225143433 and batch: 300, loss is 3.6488518047332765 and perplexity is 38.430514971465556
At time: 91.62973713874817 and batch: 350, loss is 3.6478968048095703 and perplexity is 38.39383135181483
At time: 92.35373973846436 and batch: 400, loss is 3.5693091630935667 and perplexity is 35.49206545158426
At time: 93.07778120040894 and batch: 450, loss is 3.586133074760437 and perplexity is 36.09423202096646
At time: 93.7987904548645 and batch: 500, loss is 3.467739825248718 and perplexity is 32.06418981259034
At time: 94.51898980140686 and batch: 550, loss is 3.512889304161072 and perplexity is 33.54504975217686
At time: 95.24017405509949 and batch: 600, loss is 3.522185969352722 and perplexity is 33.85836096672318
At time: 95.96278953552246 and batch: 650, loss is 3.366028699874878 and perplexity is 28.963276498404717
At time: 96.69801998138428 and batch: 700, loss is 3.354637303352356 and perplexity is 28.6352164100174
At time: 97.42114162445068 and batch: 750, loss is 3.4487446975708007 and perplexity is 31.460874598290278
At time: 98.14478659629822 and batch: 800, loss is 3.3982387733459474 and perplexity is 29.911372921568145
At time: 98.86951470375061 and batch: 850, loss is 3.435384306907654 and perplexity is 31.043340443567423
At time: 99.59280729293823 and batch: 900, loss is 3.3854094552993774 and perplexity is 29.530081489433353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307081666711259 and perplexity of 74.22356346781136
finished 7 epochs...
Completing Train Step...
At time: 101.40176582336426 and batch: 50, loss is 3.685663342475891 and perplexity is 39.871562180368116
At time: 102.1283266544342 and batch: 100, loss is 3.5612266778945925 and perplexity is 35.2063575282796
At time: 102.8532121181488 and batch: 150, loss is 3.5717157745361328 and perplexity is 35.57758392601457
At time: 103.57983589172363 and batch: 200, loss is 3.456897759437561 and perplexity is 31.718425542997625
At time: 104.30657815933228 and batch: 250, loss is 3.586513333320618 and perplexity is 36.10795977154749
At time: 105.03223991394043 and batch: 300, loss is 3.55408887386322 and perplexity is 34.95595616862645
At time: 105.75914072990417 and batch: 350, loss is 3.5559501886367797 and perplexity is 35.021080796180726
At time: 106.49647450447083 and batch: 400, loss is 3.4827771377563477 and perplexity is 32.54999248426346
At time: 107.22295427322388 and batch: 450, loss is 3.5047062110900877 and perplexity is 33.27166756944612
At time: 107.9588668346405 and batch: 500, loss is 3.3918489027023315 and perplexity is 29.720852466609944
At time: 108.68811845779419 and batch: 550, loss is 3.441169204711914 and perplexity is 31.223443431964807
At time: 109.4144332408905 and batch: 600, loss is 3.4559653425216674 and perplexity is 31.68886453021171
At time: 110.1410756111145 and batch: 650, loss is 3.305820345878601 and perplexity is 27.270903988788593
At time: 110.86800909042358 and batch: 700, loss is 3.299990186691284 and perplexity is 27.112372857267538
At time: 111.59592890739441 and batch: 750, loss is 3.4016810417175294 and perplexity is 30.01451331113273
At time: 112.32929253578186 and batch: 800, loss is 3.3575717973709107 and perplexity is 28.719369694558903
At time: 113.05599355697632 and batch: 850, loss is 3.4017916345596313 and perplexity is 30.01783288502129
At time: 113.78833341598511 and batch: 900, loss is 3.359994945526123 and perplexity is 28.78904536539719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3176511999678935 and perplexity of 75.01223247981736
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 115.60354447364807 and batch: 50, loss is 3.649904613494873 and perplexity is 38.47099626012873
At time: 116.34178853034973 and batch: 100, loss is 3.5398653984069823 and perplexity is 34.46228020084261
At time: 117.06726861000061 and batch: 150, loss is 3.5573386764526367 and perplexity is 35.06974091433731
At time: 117.79226493835449 and batch: 200, loss is 3.4375145483016967 and perplexity is 31.109540738659774
At time: 118.51752996444702 and batch: 250, loss is 3.565344896316528 and perplexity is 35.3516439537039
At time: 119.24296307563782 and batch: 300, loss is 3.531154170036316 and perplexity is 34.1633752128554
At time: 119.96952700614929 and batch: 350, loss is 3.528354048728943 and perplexity is 34.06784742510395
At time: 120.69534039497375 and batch: 400, loss is 3.4492016077041625 and perplexity is 31.47525267519304
At time: 121.41829538345337 and batch: 450, loss is 3.4674947166442873 and perplexity is 32.056331566874334
At time: 122.1433322429657 and batch: 500, loss is 3.351745915412903 and perplexity is 28.552540472348795
At time: 122.86852478981018 and batch: 550, loss is 3.3915268182754517 and perplexity is 29.711281384307878
At time: 123.59330868721008 and batch: 600, loss is 3.409015440940857 and perplexity is 30.2354610030806
At time: 124.32048296928406 and batch: 650, loss is 3.2499129152297974 and perplexity is 25.788094069158927
At time: 125.04577732086182 and batch: 700, loss is 3.2370528411865234 and perplexity is 25.45858059510557
At time: 125.77144694328308 and batch: 750, loss is 3.333756628036499 and perplexity is 28.043493044550424
At time: 126.50277209281921 and batch: 800, loss is 3.285418457984924 and perplexity is 26.72016324143497
At time: 127.23232650756836 and batch: 850, loss is 3.3265576553344727 and perplexity is 27.84233364293364
At time: 127.95834994316101 and batch: 900, loss is 3.2868125581741334 and perplexity is 26.757439803659484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316759501418022 and perplexity of 74.94537399412992
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 129.7563977241516 and batch: 50, loss is 3.6374628591537475 and perplexity is 37.99531486708547
At time: 130.49573254585266 and batch: 100, loss is 3.525741686820984 and perplexity is 33.97896602394492
At time: 131.21999168395996 and batch: 150, loss is 3.5441979360580445 and perplexity is 34.6119132386774
At time: 131.94642972946167 and batch: 200, loss is 3.4260074996948244 and perplexity is 30.753613504649724
At time: 132.68014335632324 and batch: 250, loss is 3.5520260763168334 and perplexity is 34.883923428032894
At time: 133.42163729667664 and batch: 300, loss is 3.5164256143569945 and perplexity is 33.66388544974116
At time: 134.15931391716003 and batch: 350, loss is 3.512315230369568 and perplexity is 33.52579794478719
At time: 134.894024848938 and batch: 400, loss is 3.433036308288574 and perplexity is 30.970536228620443
At time: 135.61946392059326 and batch: 450, loss is 3.4509401988983153 and perplexity is 31.53002287002093
At time: 136.34584164619446 and batch: 500, loss is 3.3348274993896485 and perplexity is 28.073540103289897
At time: 137.08112168312073 and batch: 550, loss is 3.372977166175842 and perplexity is 29.165227662010643
At time: 137.8081078529358 and batch: 600, loss is 3.3936257696151735 and perplexity is 29.77370941195106
At time: 138.5381350517273 and batch: 650, loss is 3.2323333263397216 and perplexity is 25.338711530418788
At time: 139.27638149261475 and batch: 700, loss is 3.21496374130249 and perplexity is 24.902388966606825
At time: 140.00624632835388 and batch: 750, loss is 3.310614004135132 and perplexity is 27.401945215351667
At time: 140.7305064201355 and batch: 800, loss is 3.2615044832229616 and perplexity is 26.08875773007528
At time: 141.45611906051636 and batch: 850, loss is 3.3054019021987915 and perplexity is 27.259495038539825
At time: 142.18997359275818 and batch: 900, loss is 3.2686581087112425 and perplexity is 26.27605606466219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312732017203553 and perplexity of 74.64413969878979
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 144.0050013065338 and batch: 50, loss is 3.6280504083633422 and perplexity is 37.63936365022078
At time: 144.73847460746765 and batch: 100, loss is 3.5172530221939087 and perplexity is 33.6917507387728
At time: 145.46610355377197 and batch: 150, loss is 3.53683012008667 and perplexity is 34.357836177406625
At time: 146.1910581588745 and batch: 200, loss is 3.419078917503357 and perplexity is 30.5412710313996
At time: 146.91545748710632 and batch: 250, loss is 3.546600308418274 and perplexity is 34.69516390188343
At time: 147.64165449142456 and batch: 300, loss is 3.5079997968673706 and perplexity is 33.38143131947859
At time: 148.3674156665802 and batch: 350, loss is 3.5060669231414794 and perplexity is 33.31697154431989
At time: 149.092050075531 and batch: 400, loss is 3.426144866943359 and perplexity is 30.757838334089314
At time: 149.8263545036316 and batch: 450, loss is 3.4448899936676027 and perplexity is 31.33983567665387
At time: 150.551860332489 and batch: 500, loss is 3.329397940635681 and perplexity is 27.921526225456393
At time: 151.29448747634888 and batch: 550, loss is 3.3657018041610716 and perplexity is 28.953810074810544
At time: 152.02066707611084 and batch: 600, loss is 3.388476986885071 and perplexity is 29.62080502464473
At time: 152.75177264213562 and batch: 650, loss is 3.227158932685852 and perplexity is 25.20793769172099
At time: 153.4760344028473 and batch: 700, loss is 3.2080765914916993 and perplexity is 24.731471725061315
At time: 154.20297050476074 and batch: 750, loss is 3.3023612451553346 and perplexity is 27.176734150452916
At time: 154.9298541545868 and batch: 800, loss is 3.2548751831054688 and perplexity is 25.916379530101224
At time: 155.65542602539062 and batch: 850, loss is 3.297668499946594 and perplexity is 27.049499435025083
At time: 156.38265585899353 and batch: 900, loss is 3.2608319759368896 and perplexity is 26.071218748624666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31028810265946 and perplexity of 74.46193853287465
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 158.19150590896606 and batch: 50, loss is 3.624362058639526 and perplexity is 37.50079222073661
At time: 158.9307155609131 and batch: 100, loss is 3.513970904350281 and perplexity is 33.58135171290062
At time: 159.6568853855133 and batch: 150, loss is 3.5338621807098387 and perplexity is 34.256015376237656
At time: 160.38301539421082 and batch: 200, loss is 3.416553773880005 and perplexity is 30.464247224620415
At time: 161.108380317688 and batch: 250, loss is 3.5447467803955077 and perplexity is 34.63091500529624
At time: 161.83469414710999 and batch: 300, loss is 3.50517285823822 and perplexity is 33.28719732140643
At time: 162.56752967834473 and batch: 350, loss is 3.503231701850891 and perplexity is 33.222644339696195
At time: 163.29828691482544 and batch: 400, loss is 3.423327097892761 and perplexity is 30.671291840447466
At time: 164.0237090587616 and batch: 450, loss is 3.4430948305130005 and perplexity is 31.2836260262177
At time: 164.7491798400879 and batch: 500, loss is 3.327465534210205 and perplexity is 27.867622587411372
At time: 165.47670364379883 and batch: 550, loss is 3.363236389160156 and perplexity is 28.88251483940253
At time: 166.20216250419617 and batch: 600, loss is 3.3859845399856567 and perplexity is 29.54706867114837
At time: 166.92975068092346 and batch: 650, loss is 3.2253487730026245 and perplexity is 25.16234857345447
At time: 167.656800031662 and batch: 700, loss is 3.2062021350860594 and perplexity is 24.68515708041741
At time: 168.38196206092834 and batch: 750, loss is 3.299944076538086 and perplexity is 27.11112273042352
At time: 169.10882782936096 and batch: 800, loss is 3.252553868293762 and perplexity is 25.856289225672104
At time: 169.8466796875 and batch: 850, loss is 3.295365824699402 and perplexity is 26.987284879675506
At time: 170.57215023040771 and batch: 900, loss is 3.2580762481689454 and perplexity is 25.999472469185825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308653112960188 and perplexity of 74.34029350167044
Annealing...
Model not improving. Stopping early with 74.22356346781136 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
1342.9304749965668


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.58300312728188, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -72.69322536550503, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15559915797669777, 'rnn_dropout': 0.4043936330561957, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.22356346781136, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.873817679204861, 'rnn_dropout': 0.3464428463703234, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15990938683972292, 'rnn_dropout': 0.4139278091460685, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0096712112426758 and batch: 50, loss is 7.0696251106262205 and perplexity is 1175.707191487938
At time: 1.8097760677337646 and batch: 100, loss is 6.114668512344361 and perplexity is 452.4460422322992
At time: 2.6022636890411377 and batch: 150, loss is 5.810776948928833 and perplexity is 333.8784314175143
At time: 3.393123149871826 and batch: 200, loss is 5.548626441955566 and perplexity is 256.88446774058787
At time: 4.187068223953247 and batch: 250, loss is 5.519070987701416 and perplexity is 249.40323086212365
At time: 4.991016626358032 and batch: 300, loss is 5.378671283721924 and perplexity is 216.73410588506113
At time: 5.785297155380249 and batch: 350, loss is 5.317768058776855 and perplexity is 203.9282178783222
At time: 6.586143970489502 and batch: 400, loss is 5.1469264793396 and perplexity is 171.90233217466206
At time: 7.381070137023926 and batch: 450, loss is 5.120666513442993 and perplexity is 167.4469380712504
At time: 8.176623106002808 and batch: 500, loss is 5.045230827331543 and perplexity is 155.28013796180525
At time: 8.969494342803955 and batch: 550, loss is 5.08590723991394 and perplexity is 161.72659752483492
At time: 9.765000581741333 and batch: 600, loss is 4.9892370223999025 and perplexity is 146.8243570534582
At time: 10.560622930526733 and batch: 650, loss is 4.867918872833252 and perplexity is 130.04998451606477
At time: 11.356336116790771 and batch: 700, loss is 4.935990915298462 and perplexity is 139.2110205380494
At time: 12.15176773071289 and batch: 750, loss is 4.92676796913147 and perplexity is 137.93298746429736
At time: 12.946595907211304 and batch: 800, loss is 4.8820625686645505 and perplexity is 131.9024413519484
At time: 13.742015838623047 and batch: 850, loss is 4.912013502120971 and perplexity is 135.91279979572948
At time: 14.53829312324524 and batch: 900, loss is 4.826928825378418 and perplexity is 124.82700582801031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.819073820767337 and perplexity of 123.85033003987905
finished 1 epochs...
Completing Train Step...
At time: 16.387064933776855 and batch: 50, loss is 4.788791980743408 and perplexity is 120.15613003646725
At time: 17.11008071899414 and batch: 100, loss is 4.675939264297486 and perplexity is 107.33333412987409
At time: 17.8444344997406 and batch: 150, loss is 4.66272271156311 and perplexity is 105.92409064312626
At time: 18.567861557006836 and batch: 200, loss is 4.547465229034424 and perplexity is 94.39284058685541
At time: 19.296114444732666 and batch: 250, loss is 4.660166330337525 and perplexity is 105.65365410319279
At time: 20.01925039291382 and batch: 300, loss is 4.59989486694336 and perplexity is 99.4738571015214
At time: 20.741648197174072 and batch: 350, loss is 4.59202748298645 and perplexity is 98.69432852014324
At time: 21.465094327926636 and batch: 400, loss is 4.47456738948822 and perplexity is 87.75662775701659
At time: 22.18890953063965 and batch: 450, loss is 4.504932851791382 and perplexity is 90.46226946558055
At time: 22.911519765853882 and batch: 500, loss is 4.395270433425903 and perplexity is 81.06655090371153
At time: 23.63466739654541 and batch: 550, loss is 4.464489011764527 and perplexity is 86.87662526224024
At time: 24.35795521736145 and batch: 600, loss is 4.433098783493042 and perplexity is 84.19190559010433
At time: 25.0816593170166 and batch: 650, loss is 4.286668567657471 and perplexity is 72.72379010432643
At time: 25.80529236793518 and batch: 700, loss is 4.32201491355896 and perplexity is 75.34027960948907
At time: 26.526916980743408 and batch: 750, loss is 4.380014886856079 and perplexity is 79.83922195124431
At time: 27.251612663269043 and batch: 800, loss is 4.341950736045837 and perplexity is 76.85732154554758
At time: 27.975392818450928 and batch: 850, loss is 4.40499101638794 and perplexity is 81.85840745568794
At time: 28.70823621749878 and batch: 900, loss is 4.333644461631775 and perplexity is 76.22156757082098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498945262334118 and perplexity of 89.92223689471382
finished 2 epochs...
Completing Train Step...
At time: 30.510501623153687 and batch: 50, loss is 4.39124493598938 and perplexity is 80.74087365711131
At time: 31.255350351333618 and batch: 100, loss is 4.2726733589172365 and perplexity is 71.71309442660498
At time: 31.981219053268433 and batch: 150, loss is 4.274378256797791 and perplexity is 71.8354622119495
At time: 32.71362113952637 and batch: 200, loss is 4.162580165863037 and perplexity is 64.23705120914471
At time: 33.44090175628662 and batch: 250, loss is 4.307818679809571 and perplexity is 74.27828736992971
At time: 34.171499490737915 and batch: 300, loss is 4.261625990867615 and perplexity is 70.92521349682008
At time: 34.90255784988403 and batch: 350, loss is 4.261543197631836 and perplexity is 70.91934161197588
At time: 35.64095330238342 and batch: 400, loss is 4.172637009620667 and perplexity is 64.88633258492501
At time: 36.368605613708496 and batch: 450, loss is 4.208735852241516 and perplexity is 67.27144498884248
At time: 37.09626126289368 and batch: 500, loss is 4.082396974563599 and perplexity is 59.28741008226818
At time: 37.83497929573059 and batch: 550, loss is 4.154604535102845 and perplexity is 63.72675786821656
At time: 38.56537485122681 and batch: 600, loss is 4.1559376668930055 and perplexity is 63.8117706890709
At time: 39.297590494155884 and batch: 650, loss is 3.9989594841003417 and perplexity is 54.54136933567499
At time: 40.024264335632324 and batch: 700, loss is 4.016172094345093 and perplexity is 55.48829481890436
At time: 40.75123429298401 and batch: 750, loss is 4.1047336196899415 and perplexity is 60.62659266587776
At time: 41.478107213974 and batch: 800, loss is 4.074164500236511 and perplexity is 58.80133156059426
At time: 42.205045223236084 and batch: 850, loss is 4.143876819610596 and perplexity is 63.04676922421673
At time: 42.93017911911011 and batch: 900, loss is 4.082112369537353 and perplexity is 59.27053898827863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392200626739084 and perplexity of 80.81807384706627
finished 3 epochs...
Completing Train Step...
At time: 44.728617668151855 and batch: 50, loss is 4.1582658576965335 and perplexity is 63.96050974601195
At time: 45.472126483917236 and batch: 100, loss is 4.043055591583252 and perplexity is 57.000246454208785
At time: 46.20793128013611 and batch: 150, loss is 4.047813720703125 and perplexity is 57.27210724817689
At time: 46.938347578048706 and batch: 200, loss is 3.9363941812515257 and perplexity is 51.23352901302389
At time: 47.66875386238098 and batch: 250, loss is 4.08789553642273 and perplexity is 59.61430347222375
At time: 48.406155586242676 and batch: 300, loss is 4.0494800424575805 and perplexity is 57.36762056226416
At time: 49.135701179504395 and batch: 350, loss is 4.050596175193786 and perplexity is 57.43168618779207
At time: 49.86058187484741 and batch: 400, loss is 3.972974810600281 and perplexity is 53.14238445956939
At time: 50.58409357070923 and batch: 450, loss is 4.010317234992981 and perplexity is 55.16436785578991
At time: 51.31714200973511 and batch: 500, loss is 3.8837682247161864 and perplexity is 48.607032627929875
At time: 52.05074381828308 and batch: 550, loss is 3.954103798866272 and perplexity is 52.14893705925922
At time: 52.77610445022583 and batch: 600, loss is 3.9671021795272825 and perplexity is 52.83121343192161
At time: 53.50182223320007 and batch: 650, loss is 3.8104313802719116 and perplexity is 45.169920076516576
At time: 54.23871374130249 and batch: 700, loss is 3.8208491373062134 and perplexity is 45.642949001205
At time: 54.96462917327881 and batch: 750, loss is 3.915554304122925 and perplexity is 50.17687705378779
At time: 55.68957996368408 and batch: 800, loss is 3.8913599395751954 and perplexity is 48.977447623251734
At time: 56.42438840866089 and batch: 850, loss is 3.958422327041626 and perplexity is 52.37463069466662
At time: 57.1498064994812 and batch: 900, loss is 3.9026536750793457 and perplexity is 49.53372125335905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.343401347121147 and perplexity of 76.96889253088905
finished 4 epochs...
Completing Train Step...
At time: 58.95817255973816 and batch: 50, loss is 3.9875798845291137 and perplexity is 53.92422846031072
At time: 59.684911012649536 and batch: 100, loss is 3.874706540107727 and perplexity is 48.16856067616183
At time: 60.41116690635681 and batch: 150, loss is 3.8819693231582644 and perplexity is 48.5196719613877
At time: 61.13729166984558 and batch: 200, loss is 3.767832884788513 and perplexity is 43.28615702874147
At time: 61.86245155334473 and batch: 250, loss is 3.922497673034668 and perplexity is 50.52648594921586
At time: 62.586483001708984 and batch: 300, loss is 3.888634672164917 and perplexity is 48.84415269606534
At time: 63.312827348709106 and batch: 350, loss is 3.8889889478683473 and perplexity is 48.86146005822811
At time: 64.03856658935547 and batch: 400, loss is 3.816589045524597 and perplexity is 45.44891943460074
At time: 64.76434755325317 and batch: 450, loss is 3.853424754142761 and perplexity is 47.154278823872474
At time: 65.49057722091675 and batch: 500, loss is 3.7357980251312255 and perplexity is 41.921466601819425
At time: 66.21829748153687 and batch: 550, loss is 3.8017496061325073 and perplexity is 44.77946241762067
At time: 66.94586396217346 and batch: 600, loss is 3.818828067779541 and perplexity is 45.55079458448166
At time: 67.68357110023499 and batch: 650, loss is 3.6660133838653564 and perplexity is 39.09573508239432
At time: 68.42110085487366 and batch: 700, loss is 3.6721362590789797 and perplexity is 39.335847729432736
At time: 69.15402865409851 and batch: 750, loss is 3.77123526096344 and perplexity is 43.433683646328646
At time: 69.89355802536011 and batch: 800, loss is 3.7474237489700317 and perplexity is 42.41167800535194
At time: 70.63490843772888 and batch: 850, loss is 3.8150173950195314 and perplexity is 45.37754571937605
At time: 71.37112808227539 and batch: 900, loss is 3.760908660888672 and perplexity is 42.98746926954921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335947115127355 and perplexity of 76.39728165651387
finished 5 epochs...
Completing Train Step...
At time: 73.20030784606934 and batch: 50, loss is 3.8514534854888915 and perplexity is 47.061416630367845
At time: 73.94170117378235 and batch: 100, loss is 3.738869032859802 and perplexity is 42.050405634794004
At time: 74.67320919036865 and batch: 150, loss is 3.7476700496673585 and perplexity is 42.4221253177568
At time: 75.39852237701416 and batch: 200, loss is 3.636928834915161 and perplexity is 37.975029864817245
At time: 76.12431788444519 and batch: 250, loss is 3.7867202138900757 and perplexity is 44.11148652204464
At time: 76.85770392417908 and batch: 300, loss is 3.7561272048950194 and perplexity is 42.78241719136622
At time: 77.58865165710449 and batch: 350, loss is 3.759153962135315 and perplexity is 42.91210535063617
At time: 78.31890726089478 and batch: 400, loss is 3.6927406454086302 and perplexity is 40.15474621189426
At time: 79.04496932029724 and batch: 450, loss is 3.728486113548279 and perplexity is 41.61605846394504
At time: 79.77102828025818 and batch: 500, loss is 3.6131339359283445 and perplexity is 37.08208377770263
At time: 80.49755311012268 and batch: 550, loss is 3.681866044998169 and perplexity is 39.72044509771179
At time: 81.22225069999695 and batch: 600, loss is 3.6952090501785277 and perplexity is 40.253986811572936
At time: 81.94880485534668 and batch: 650, loss is 3.5476882600784303 and perplexity is 34.73293110376966
At time: 82.67520499229431 and batch: 700, loss is 3.5502058935165404 and perplexity is 34.820486061918146
At time: 83.40831160545349 and batch: 750, loss is 3.6582519721984865 and perplexity is 38.79347150139234
At time: 84.1369616985321 and batch: 800, loss is 3.631961169242859 and perplexity is 37.786850405858615
At time: 84.86289596557617 and batch: 850, loss is 3.6972622442245484 and perplexity is 40.33672096319226
At time: 85.5878918170929 and batch: 900, loss is 3.6473467683792116 and perplexity is 38.37271915264189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345569349315069 and perplexity of 77.13594227543501
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 87.38850283622742 and batch: 50, loss is 3.770201048851013 and perplexity is 43.3887872248275
At time: 88.13084816932678 and batch: 100, loss is 3.67038161277771 and perplexity is 39.26688774758825
At time: 88.85701870918274 and batch: 150, loss is 3.6748274898529054 and perplexity is 39.44185215056075
At time: 89.5848445892334 and batch: 200, loss is 3.549023675918579 and perplexity is 34.779344994165015
At time: 90.31302380561829 and batch: 250, loss is 3.6876660346984864 and perplexity is 39.951492659208355
At time: 91.051584482193 and batch: 300, loss is 3.6479308557510377 and perplexity is 38.395138720177336
At time: 91.77810978889465 and batch: 350, loss is 3.6424164390563964 and perplexity is 38.18399462952773
At time: 92.51513338088989 and batch: 400, loss is 3.574301905632019 and perplexity is 35.66971129750601
At time: 93.24159693717957 and batch: 450, loss is 3.5910209703445433 and perplexity is 36.27108873473401
At time: 93.96898698806763 and batch: 500, loss is 3.4640474700927735 and perplexity is 31.946015740597
At time: 94.6950056552887 and batch: 550, loss is 3.517447285652161 and perplexity is 33.6982964505616
At time: 95.42229914665222 and batch: 600, loss is 3.5225274991989135 and perplexity is 33.8699265824255
At time: 96.14839267730713 and batch: 650, loss is 3.360302038192749 and perplexity is 28.79788762773553
At time: 96.87423491477966 and batch: 700, loss is 3.341407117843628 and perplexity is 28.258862291897472
At time: 97.59958815574646 and batch: 750, loss is 3.4378685188293456 and perplexity is 31.120554548376976
At time: 98.32479453086853 and batch: 800, loss is 3.3935777044296263 and perplexity is 29.772278367475728
At time: 99.05335783958435 and batch: 850, loss is 3.437026171684265 and perplexity is 31.09435127577513
At time: 99.78869795799255 and batch: 900, loss is 3.3842481136322022 and perplexity is 29.495806881481986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3061021778681505 and perplexity of 74.15089790886063
finished 7 epochs...
Completing Train Step...
At time: 101.59822487831116 and batch: 50, loss is 3.677409634590149 and perplexity is 39.5438283234773
At time: 102.32038426399231 and batch: 100, loss is 3.563534126281738 and perplexity is 35.28768817822818
At time: 103.04209494590759 and batch: 150, loss is 3.567054047584534 and perplexity is 35.412116924799946
At time: 103.76440405845642 and batch: 200, loss is 3.447379217147827 and perplexity is 31.4179447065704
At time: 104.48721623420715 and batch: 250, loss is 3.5858119773864745 and perplexity is 36.082644118371434
At time: 105.20969367027283 and batch: 300, loss is 3.551380920410156 and perplexity is 34.8614251170247
At time: 105.9313530921936 and batch: 350, loss is 3.551094317436218 and perplexity is 34.85143516055445
At time: 106.65291953086853 and batch: 400, loss is 3.4888830614089965 and perplexity is 32.7493482594684
At time: 107.37519764900208 and batch: 450, loss is 3.510120139122009 and perplexity is 33.452286470865126
At time: 108.10466265678406 and batch: 500, loss is 3.3892322540283204 and perplexity is 29.643185095838813
At time: 108.84433817863464 and batch: 550, loss is 3.444927759170532 and perplexity is 31.34101926365916
At time: 109.59267783164978 and batch: 600, loss is 3.455868668556213 and perplexity is 31.685801190091837
At time: 110.31851696968079 and batch: 650, loss is 3.3000126028060914 and perplexity is 27.112980618142
At time: 111.04502940177917 and batch: 700, loss is 3.286506018638611 and perplexity is 26.74923884751316
At time: 111.77474665641785 and batch: 750, loss is 3.390021872520447 and perplexity is 29.666601146612717
At time: 112.51271963119507 and batch: 800, loss is 3.351186819076538 and perplexity is 28.536581313346176
At time: 113.24229073524475 and batch: 850, loss is 3.401383652687073 and perplexity is 30.005588651233264
At time: 113.96676635742188 and batch: 900, loss is 3.357968373298645 and perplexity is 28.73076136392109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316524975920377 and perplexity of 74.92779945391719
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 115.77513837814331 and batch: 50, loss is 3.645293745994568 and perplexity is 38.29401991455306
At time: 116.51428127288818 and batch: 100, loss is 3.5413593244552612 and perplexity is 34.513802774792985
At time: 117.24119520187378 and batch: 150, loss is 3.551191334724426 and perplexity is 34.854816516306215
At time: 117.97995924949646 and batch: 200, loss is 3.431201629638672 and perplexity is 30.91376733928428
At time: 118.7120852470398 and batch: 250, loss is 3.5664118576049804 and perplexity is 35.38938291871282
At time: 119.44462633132935 and batch: 300, loss is 3.5267022371292116 and perplexity is 34.01162021071634
At time: 120.18439483642578 and batch: 350, loss is 3.5235450363159178 and perplexity is 33.90440802997457
At time: 120.90834403038025 and batch: 400, loss is 3.4611242532730104 and perplexity is 31.852766969649917
At time: 121.6321485042572 and batch: 450, loss is 3.474180636405945 and perplexity is 32.27137571046204
At time: 122.35562419891357 and batch: 500, loss is 3.351814603805542 and perplexity is 28.55450176781795
At time: 123.07925724983215 and batch: 550, loss is 3.3977807331085206 and perplexity is 29.897675446450247
At time: 123.80340218544006 and batch: 600, loss is 3.4124323558807372 and perplexity is 30.3389497068567
At time: 124.53734612464905 and batch: 650, loss is 3.2445321702957153 and perplexity is 25.649707557781845
At time: 125.27394151687622 and batch: 700, loss is 3.227079458236694 and perplexity is 25.205934384365467
At time: 126.00234341621399 and batch: 750, loss is 3.3230505657196043 and perplexity is 27.744859109631296
At time: 126.7319700717926 and batch: 800, loss is 3.278579902648926 and perplexity is 26.538059300147626
At time: 127.46682929992676 and batch: 850, loss is 3.326274862289429 and perplexity is 27.83446113781894
At time: 128.1898181438446 and batch: 900, loss is 3.2864800596237185 and perplexity is 26.74854447263624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310022223485659 and perplexity of 74.44214328586648
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 129.9785885810852 and batch: 50, loss is 3.6310298156738283 and perplexity is 37.75167387130893
At time: 130.7184021472931 and batch: 100, loss is 3.5239165163040163 and perplexity is 33.9170051787155
At time: 131.44359612464905 and batch: 150, loss is 3.5359316301345824 and perplexity is 34.326979870942225
At time: 132.17028045654297 and batch: 200, loss is 3.413027048110962 and perplexity is 30.35699741040319
At time: 132.8955943584442 and batch: 250, loss is 3.5521998739242555 and perplexity is 34.88998669733777
At time: 133.62213277816772 and batch: 300, loss is 3.511005706787109 and perplexity is 33.481923855105094
At time: 134.35665369033813 and batch: 350, loss is 3.5079897117614744 and perplexity is 33.38109466590636
At time: 135.08704328536987 and batch: 400, loss is 3.4455189228057863 and perplexity is 31.35955241206159
At time: 135.81455731391907 and batch: 450, loss is 3.4579314184188843 and perplexity is 31.751228529063464
At time: 136.54054307937622 and batch: 500, loss is 3.3360201120376587 and perplexity is 28.1070409350786
At time: 137.26564002037048 and batch: 550, loss is 3.3777194738388063 and perplexity is 29.30386661911219
At time: 137.9902105331421 and batch: 600, loss is 3.396780905723572 and perplexity is 29.867797870490833
At time: 138.71675848960876 and batch: 650, loss is 3.227273483276367 and perplexity is 25.2108254412628
At time: 139.44257473945618 and batch: 700, loss is 3.208180809020996 and perplexity is 24.734049312252925
At time: 140.17325568199158 and batch: 750, loss is 3.297224769592285 and perplexity is 27.03749941364066
At time: 140.9046950340271 and batch: 800, loss is 3.2568676614761354 and perplexity is 25.968068833570104
At time: 141.63075494766235 and batch: 850, loss is 3.3075034284591673 and perplexity is 27.316841819985893
At time: 142.35682439804077 and batch: 900, loss is 3.264678854942322 and perplexity is 26.171704727456003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309074506367723 and perplexity of 74.37162661258574
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 144.1587221622467 and batch: 50, loss is 3.6237351751327513 and perplexity is 37.47729095964855
At time: 144.88425302505493 and batch: 100, loss is 3.5143808460235597 and perplexity is 33.595120930507996
At time: 145.62009024620056 and batch: 150, loss is 3.5286513996124267 and perplexity is 34.077979035881256
At time: 146.34484887123108 and batch: 200, loss is 3.4051841831207277 and perplexity is 30.11984277970406
At time: 147.0695080757141 and batch: 250, loss is 3.5461369609832762 and perplexity is 34.67909171047456
At time: 147.7934546470642 and batch: 300, loss is 3.505911102294922 and perplexity is 33.31178047005843
At time: 148.518714427948 and batch: 350, loss is 3.5008842515945435 and perplexity is 33.1447473002082
At time: 149.2433304786682 and batch: 400, loss is 3.441774435043335 and perplexity is 31.242346526767374
At time: 149.96963357925415 and batch: 450, loss is 3.452253427505493 and perplexity is 31.571456197799918
At time: 150.69533944129944 and batch: 500, loss is 3.3304527378082276 and perplexity is 27.950993310539122
At time: 151.42111158370972 and batch: 550, loss is 3.3708153581619262 and perplexity is 29.102246140630278
At time: 152.1470823287964 and batch: 600, loss is 3.390154285430908 and perplexity is 29.670529647700423
At time: 152.8886125087738 and batch: 650, loss is 3.2221529483795166 and perplexity is 25.08206247877945
At time: 153.61418867111206 and batch: 700, loss is 3.2023352432250975 and perplexity is 24.589886566517777
At time: 154.3400399684906 and batch: 750, loss is 3.2898557806015014 and perplexity is 26.838992673390226
At time: 155.0649516582489 and batch: 800, loss is 3.250289740562439 and perplexity is 25.797813507434995
At time: 155.79721570014954 and batch: 850, loss is 3.2999731302261353 and perplexity is 27.11191041996858
At time: 156.52555537223816 and batch: 900, loss is 3.256376414299011 and perplexity is 25.955315225903604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308906450663527 and perplexity of 74.35912908667288
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 158.3165786266327 and batch: 50, loss is 3.620323362350464 and perplexity is 37.34964333802359
At time: 159.06591701507568 and batch: 100, loss is 3.5107245016098023 and perplexity is 33.472509888460955
At time: 159.79282093048096 and batch: 150, loss is 3.5258240413665773 and perplexity is 33.981764461481994
At time: 160.51868224143982 and batch: 200, loss is 3.4030989265441893 and perplexity is 30.057100618950166
At time: 161.24320030212402 and batch: 250, loss is 3.5441087436676026 and perplexity is 34.60882625706756
At time: 161.97764611244202 and batch: 300, loss is 3.5043824911117554 and perplexity is 33.26089860909931
At time: 162.70286011695862 and batch: 350, loss is 3.498248314857483 and perplexity is 33.05749488959658
At time: 163.44431114196777 and batch: 400, loss is 3.440313014984131 and perplexity is 31.19672168150886
At time: 164.16953587532043 and batch: 450, loss is 3.450988116264343 and perplexity is 31.53153374186587
At time: 164.89469933509827 and batch: 500, loss is 3.328156771659851 and perplexity is 27.886892391014065
At time: 165.62920904159546 and batch: 550, loss is 3.368196377754211 and perplexity is 29.0261276480962
At time: 166.36393880844116 and batch: 600, loss is 3.3879885053634644 and perplexity is 29.60633934213203
At time: 167.09229588508606 and batch: 650, loss is 3.220025758743286 and perplexity is 25.028764882563806
At time: 167.81845903396606 and batch: 700, loss is 3.2003446865081786 and perplexity is 24.540987686787332
At time: 168.54430413246155 and batch: 750, loss is 3.2878733348846434 and perplexity is 26.785838532328103
At time: 169.26859259605408 and batch: 800, loss is 3.247860240936279 and perplexity is 25.735213802933348
At time: 170.0029275417328 and batch: 850, loss is 3.297538981437683 and perplexity is 27.045996251059943
At time: 170.73092555999756 and batch: 900, loss is 3.2536027526855467 and perplexity is 25.883423711852213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307429065443065 and perplexity of 74.24935311901692
Annealing...
Model not improving. Stopping early with 74.15089790886063 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f3e15afbb70>
ELAPSED
1521.5038242340088


RESULTS SO FAR:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.58300312728188, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -72.69322536550503, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15559915797669777, 'rnn_dropout': 0.4043936330561957, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.22356346781136, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.873817679204861, 'rnn_dropout': 0.3464428463703234, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.15089790886063, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15990938683972292, 'rnn_dropout': 0.4139278091460685, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -74.47314572261351, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.11459032236426148, 'rnn_dropout': 0.3137372483610401, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.3986495870119, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.33284530814000446, 'rnn_dropout': 0.9307241546023315, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.58300312728188, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.38311467493620566, 'rnn_dropout': 0.9939356262052047, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -72.69322536550503, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15559915797669777, 'rnn_dropout': 0.4043936330561957, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.22356346781136, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.873817679204861, 'rnn_dropout': 0.3464428463703234, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}, {'best_accuracy': -74.15089790886063, 'params': {'seq_len': 35, 'tune_wordvecs': 'TRUE', 'num_layers': 2, 'wordvec_source': 'gigavec', 'batch_size': 32, 'dropout': 0.15990938683972292, 'rnn_dropout': 0.4139278091460685, 'tie_weights': 'FALSE', 'data': 'ptb', 'wordvec_dim': 300}}]
