FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2606897354125977 and batch: 50, loss is 8.971238479614257 and perplexity is 7873.346548978606
At time: 2.083744764328003 and batch: 100, loss is 8.116067972183227 and perplexity is 3347.831066103478
At time: 2.9204299449920654 and batch: 150, loss is 7.728695640563965 and perplexity is 2272.6359325446006
At time: 3.7445530891418457 and batch: 200, loss is 7.413121566772461 and perplexity is 1657.5925651954897
At time: 4.568974018096924 and batch: 250, loss is 7.298175487518311 and perplexity is 1477.6015642373632
At time: 5.395627975463867 and batch: 300, loss is 7.1014636039733885 and perplexity is 1213.7422130514617
At time: 6.217598915100098 and batch: 350, loss is 7.012437524795533 and perplexity is 1110.3577334761808
At time: 7.039154767990112 and batch: 400, loss is 6.8886510848999025 and perplexity is 981.0771344855058
At time: 7.861872911453247 and batch: 450, loss is 6.83272494316101 and perplexity is 927.7153428949795
At time: 8.700536966323853 and batch: 500, loss is 6.767407846450806 and perplexity is 869.0562445302113
At time: 9.534130573272705 and batch: 550, loss is 6.7484825992584225 and perplexity is 852.7637958667005
At time: 10.358701944351196 and batch: 600, loss is 6.676028785705566 and perplexity is 793.1630290639644
At time: 11.183448314666748 and batch: 650, loss is 6.581375656127929 and perplexity is 721.5312256906402
At time: 12.008262634277344 and batch: 700, loss is 6.645376100540161 and perplexity is 769.219297028252
At time: 12.83137845993042 and batch: 750, loss is 6.572197208404541 and perplexity is 714.9389885862511
At time: 13.658571481704712 and batch: 800, loss is 6.560971641540528 and perplexity is 706.9582710118016
At time: 14.481630086898804 and batch: 850, loss is 6.570647954940796 and perplexity is 713.832224432287
At time: 15.305777788162231 and batch: 900, loss is 6.453959569931031 and perplexity is 635.2124881535542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.509485114110659 and perplexity of 671.480592686412
finished 1 epochs...
Completing Train Step...
At time: 17.177896976470947 and batch: 50, loss is 6.024408569335938 and perplexity is 413.39707400164576
At time: 17.941910982131958 and batch: 100, loss is 5.514738388061524 and perplexity is 248.3250039632929
At time: 18.69391655921936 and batch: 150, loss is 5.326241979598999 and perplexity is 205.6636319461169
At time: 19.44564700126648 and batch: 200, loss is 5.11472409248352 and perplexity is 166.45484850319522
At time: 20.19774031639099 and batch: 250, loss is 5.139826259613037 and perplexity is 170.6861106741142
At time: 20.951632976531982 and batch: 300, loss is 5.056276206970215 and perplexity is 157.0047731271944
At time: 21.705152988433838 and batch: 350, loss is 5.000854768753052 and perplexity is 148.54007226649543
At time: 22.458348751068115 and batch: 400, loss is 4.843905735015869 and perplexity is 126.96427339287617
At time: 23.212133169174194 and batch: 450, loss is 4.836710119247437 and perplexity is 126.05396630298344
At time: 23.965551614761353 and batch: 500, loss is 4.740933542251587 and perplexity is 114.54108072274265
At time: 24.719709157943726 and batch: 550, loss is 4.793454895019531 and perplexity is 120.7177160668011
At time: 25.47350311279297 and batch: 600, loss is 4.7306161308288575 and perplexity is 113.3653887462848
At time: 26.225961446762085 and batch: 650, loss is 4.590400638580323 and perplexity is 98.53389873640499
At time: 26.978071212768555 and batch: 700, loss is 4.637823810577393 and perplexity is 103.31926047422313
At time: 27.729795455932617 and batch: 750, loss is 4.665717458724975 and perplexity is 106.24178197813002
At time: 28.529916048049927 and batch: 800, loss is 4.590069084167481 and perplexity is 98.50123480269932
At time: 29.28260898590088 and batch: 850, loss is 4.652337512969971 and perplexity is 104.82974028457217
At time: 30.036717653274536 and batch: 900, loss is 4.589256381988525 and perplexity is 98.42121515502066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.660170045617509 and perplexity of 105.65404663682831
finished 2 epochs...
Completing Train Step...
At time: 31.900686264038086 and batch: 50, loss is 4.64553689956665 and perplexity is 104.11925236280679
At time: 32.663999795913696 and batch: 100, loss is 4.483569231033325 and perplexity is 88.55016530569122
At time: 33.41399526596069 and batch: 150, loss is 4.465079231262207 and perplexity is 86.92791667546977
At time: 34.16395831108093 and batch: 200, loss is 4.35144850730896 and perplexity is 77.59077236598088
At time: 34.91367244720459 and batch: 250, loss is 4.484076814651489 and perplexity is 88.59512332799869
At time: 35.66503858566284 and batch: 300, loss is 4.4476499319076535 and perplexity is 85.42595111924032
At time: 36.4227409362793 and batch: 350, loss is 4.427436809539795 and perplexity is 83.71656018012655
At time: 37.18198895454407 and batch: 400, loss is 4.334396462440491 and perplexity is 76.27890780852623
At time: 37.94249677658081 and batch: 450, loss is 4.34731017112732 and perplexity is 77.27033915282408
At time: 38.72522234916687 and batch: 500, loss is 4.225679960250854 and perplexity is 68.42101130306337
At time: 39.48413276672363 and batch: 550, loss is 4.308168616294861 and perplexity is 74.3042846011699
At time: 40.24266815185547 and batch: 600, loss is 4.302293767929077 and perplexity is 73.86903795207807
At time: 41.002963066101074 and batch: 650, loss is 4.148784308433533 and perplexity is 63.35693097468866
At time: 41.76343369483948 and batch: 700, loss is 4.166705427169799 and perplexity is 64.5025931693674
At time: 42.5248486995697 and batch: 750, loss is 4.262351665496826 and perplexity is 70.97670080408297
At time: 43.28649854660034 and batch: 800, loss is 4.197613353729248 and perplexity is 66.52736413198375
At time: 44.04895067214966 and batch: 850, loss is 4.27324028968811 and perplexity is 71.75376231335893
At time: 44.80935621261597 and batch: 900, loss is 4.220158977508545 and perplexity is 68.04430094302917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.461492251043451 and perplexity of 86.6166665157033
finished 3 epochs...
Completing Train Step...
At time: 46.681426763534546 and batch: 50, loss is 4.30771770477295 and perplexity is 74.2707874957988
At time: 47.46057844161987 and batch: 100, loss is 4.150243291854858 and perplexity is 63.44943515122782
At time: 48.22607421875 and batch: 150, loss is 4.142467627525329 and perplexity is 62.95798678647081
At time: 48.990742921829224 and batch: 200, loss is 4.037337975502014 and perplexity is 56.67527085661702
At time: 49.756272315979004 and batch: 250, loss is 4.187498998641968 and perplexity is 65.8578741850594
At time: 50.52138590812683 and batch: 300, loss is 4.15039632320404 and perplexity is 63.45914564688002
At time: 51.29042601585388 and batch: 350, loss is 4.140087032318116 and perplexity is 62.808287562197386
At time: 52.05408215522766 and batch: 400, loss is 4.065843167304993 and perplexity is 58.31405630543671
At time: 52.818429470062256 and batch: 450, loss is 4.082188272476197 and perplexity is 59.27503796711538
At time: 53.581759214401245 and batch: 500, loss is 3.9527685546875 and perplexity is 52.079351961508515
At time: 54.34573841094971 and batch: 550, loss is 4.042839207649231 and perplexity is 56.9879138509776
At time: 55.110758543014526 and batch: 600, loss is 4.05794216632843 and perplexity is 57.855132256688634
At time: 55.876041650772095 and batch: 650, loss is 3.8909097146987914 and perplexity is 48.95540172112815
At time: 56.64101719856262 and batch: 700, loss is 3.908258500099182 and perplexity is 49.81212857690677
At time: 57.406686305999756 and batch: 750, loss is 4.018984041213989 and perplexity is 55.64454453577573
At time: 58.17170023918152 and batch: 800, loss is 3.960123047828674 and perplexity is 52.46378110629354
At time: 58.937613010406494 and batch: 850, loss is 4.036645250320435 and perplexity is 56.63602406451183
At time: 59.7034227848053 and batch: 900, loss is 3.9930711936950685 and perplexity is 54.22115758900005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.392727786547517 and perplexity of 80.86068911893581
finished 4 epochs...
Completing Train Step...
At time: 61.5762894153595 and batch: 50, loss is 4.081548047065735 and perplexity is 59.237100727091125
At time: 62.33973526954651 and batch: 100, loss is 3.934362564086914 and perplexity is 51.12954775689472
At time: 63.10142946243286 and batch: 150, loss is 3.928256802558899 and perplexity is 50.8183140575369
At time: 63.86431360244751 and batch: 200, loss is 3.828870177268982 and perplexity is 46.01052511980145
At time: 64.62670969963074 and batch: 250, loss is 3.979927430152893 and perplexity is 53.51315064541683
At time: 65.38972020149231 and batch: 300, loss is 3.945501437187195 and perplexity is 51.70225704755047
At time: 66.17637467384338 and batch: 350, loss is 3.9389103269577026 and perplexity is 51.36260235263009
At time: 66.94286441802979 and batch: 400, loss is 3.870664429664612 and perplexity is 47.97425100915318
At time: 67.71100568771362 and batch: 450, loss is 3.8887670135498045 and perplexity is 48.850617226629815
At time: 68.47601771354675 and batch: 500, loss is 3.761161231994629 and perplexity is 42.99832803345219
At time: 69.2402331829071 and batch: 550, loss is 3.8525880193710327 and perplexity is 47.11483970148832
At time: 70.002521276474 and batch: 600, loss is 3.8775890254974366 and perplexity is 48.30760615055425
At time: 70.76641774177551 and batch: 650, loss is 3.7094980096817016 and perplexity is 40.83330345824964
At time: 71.53067636489868 and batch: 700, loss is 3.719820318222046 and perplexity is 41.256980314992916
At time: 72.29513907432556 and batch: 750, loss is 3.8356950092315674 and perplexity is 46.32561321086971
At time: 73.05875062942505 and batch: 800, loss is 3.7799144077301023 and perplexity is 43.812291581598274
At time: 73.82327008247375 and batch: 850, loss is 3.8601495218276978 and perplexity is 47.4724490055551
At time: 74.58839011192322 and batch: 900, loss is 3.815840654373169 and perplexity is 45.4149185900065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382210509417808 and perplexity of 80.01471133199387
finished 5 epochs...
Completing Train Step...
At time: 76.45706820487976 and batch: 50, loss is 3.9083904361724855 and perplexity is 49.8187010271162
At time: 77.23329186439514 and batch: 100, loss is 3.769085683822632 and perplexity is 43.34041986757669
At time: 77.99569725990295 and batch: 150, loss is 3.76206307888031 and perplexity is 43.03712343280426
At time: 78.75770831108093 and batch: 200, loss is 3.6667084980010984 and perplexity is 39.12292052789642
At time: 79.5217707157135 and batch: 250, loss is 3.8147846269607544 and perplexity is 45.36698450535121
At time: 80.28469181060791 and batch: 300, loss is 3.7852177810668945 and perplexity is 44.045261738416386
At time: 81.0490071773529 and batch: 350, loss is 3.7792843055725096 and perplexity is 43.784694057686394
At time: 81.81228756904602 and batch: 400, loss is 3.70912805557251 and perplexity is 40.81819980384516
At time: 82.57672429084778 and batch: 450, loss is 3.729785556793213 and perplexity is 41.6701713206755
At time: 83.34291243553162 and batch: 500, loss is 3.613241362571716 and perplexity is 37.08606759547234
At time: 84.11366510391235 and batch: 550, loss is 3.697042284011841 and perplexity is 40.32784946519334
At time: 84.87984657287598 and batch: 600, loss is 3.7274974536895753 and perplexity is 41.574934669536354
At time: 85.66999101638794 and batch: 650, loss is 3.560901989936829 and perplexity is 35.194928303519674
At time: 86.43589782714844 and batch: 700, loss is 3.5723540878295896 and perplexity is 35.600300820260095
At time: 87.20116448402405 and batch: 750, loss is 3.6887673282623292 and perplexity is 39.99551521737045
At time: 87.96428990364075 and batch: 800, loss is 3.6322548007965088 and perplexity is 37.79794744659175
At time: 88.7301173210144 and batch: 850, loss is 3.709302339553833 and perplexity is 40.82531438217793
At time: 89.49341344833374 and batch: 900, loss is 3.670482249259949 and perplexity is 39.27083962788797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.401641009605094 and perplexity of 81.58464005283723
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.36994528770447 and batch: 50, loss is 3.7877828741073607 and perplexity is 44.158386959097726
At time: 92.14816093444824 and batch: 100, loss is 3.655885772705078 and perplexity is 38.70178692355674
At time: 92.91388392448425 and batch: 150, loss is 3.6454305934906004 and perplexity is 38.29926071387833
At time: 93.67812061309814 and batch: 200, loss is 3.527787652015686 and perplexity is 34.048556971835005
At time: 94.44054651260376 and batch: 250, loss is 3.672475233078003 and perplexity is 39.349183819208704
At time: 95.20524001121521 and batch: 300, loss is 3.6328631973266603 and perplexity is 37.820950583469894
At time: 95.97084355354309 and batch: 350, loss is 3.6101246213912965 and perplexity is 36.970659862817435
At time: 96.7349112033844 and batch: 400, loss is 3.5370720195770264 and perplexity is 34.36614832577938
At time: 97.497385263443 and batch: 450, loss is 3.537710256576538 and perplexity is 34.388089074125496
At time: 98.26183342933655 and batch: 500, loss is 3.4141407680511473 and perplexity is 30.390825437720977
At time: 99.02568078041077 and batch: 550, loss is 3.471484980583191 and perplexity is 32.18450033428376
At time: 99.79025554656982 and batch: 600, loss is 3.4892927503585813 and perplexity is 32.76276805433954
At time: 100.55531454086304 and batch: 650, loss is 3.305946955680847 and perplexity is 27.274356971135774
At time: 101.31929922103882 and batch: 700, loss is 3.2966387939453123 and perplexity is 27.021660738421414
At time: 102.08436346054077 and batch: 750, loss is 3.3942157316207884 and perplexity is 29.791279951736147
At time: 102.84843325614929 and batch: 800, loss is 3.3137271881103514 and perplexity is 27.48738543864634
At time: 103.61223196983337 and batch: 850, loss is 3.3666209840774535 and perplexity is 28.98043607069971
At time: 104.39668941497803 and batch: 900, loss is 3.3099203300476074 and perplexity is 27.38294378716912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391642531303511 and perplexity of 80.77298223282286
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.27707481384277 and batch: 50, loss is 3.684673204421997 and perplexity is 39.83210336744495
At time: 107.04040336608887 and batch: 100, loss is 3.5605865144729614 and perplexity is 35.18382691838673
At time: 107.80445623397827 and batch: 150, loss is 3.5552038764953613 and perplexity is 34.994953889003654
At time: 108.56973242759705 and batch: 200, loss is 3.440731587409973 and perplexity is 31.209782502240422
At time: 109.33502531051636 and batch: 250, loss is 3.5826254653930665 and perplexity is 35.96784933474592
At time: 110.1010046005249 and batch: 300, loss is 3.5437337875366213 and perplexity is 34.595851898036145
At time: 110.86644244194031 and batch: 350, loss is 3.517927212715149 and perplexity is 33.71447305648981
At time: 111.6406762599945 and batch: 400, loss is 3.448136873245239 and perplexity is 31.441757723845214
At time: 112.41237449645996 and batch: 450, loss is 3.4466142225265504 and perplexity is 31.393919338668585
At time: 113.18910717964172 and batch: 500, loss is 3.3207575941085814 and perplexity is 27.681313816959072
At time: 113.95472121238708 and batch: 550, loss is 3.371138277053833 and perplexity is 29.111645323211143
At time: 114.71916675567627 and batch: 600, loss is 3.393531808853149 and perplexity is 29.77091198295276
At time: 115.48414611816406 and batch: 650, loss is 3.206239309310913 and perplexity is 24.68607474905397
At time: 116.25120568275452 and batch: 700, loss is 3.1926425313949585 and perplexity is 24.3526952526009
At time: 117.01642227172852 and batch: 750, loss is 3.285214214324951 and perplexity is 26.714706374783635
At time: 117.78126001358032 and batch: 800, loss is 3.202320113182068 and perplexity is 24.589514523290454
At time: 118.54769945144653 and batch: 850, loss is 3.2512953424453737 and perplexity is 25.823768885474937
At time: 119.31298542022705 and batch: 900, loss is 3.197146482467651 and perplexity is 24.462625975972603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387581185118793 and perplexity of 80.44560044499148
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.17646288871765 and batch: 50, loss is 3.653675479888916 and perplexity is 38.61633910907939
At time: 121.95069670677185 and batch: 100, loss is 3.5302450180053713 and perplexity is 34.13232962561446
At time: 122.71156811714172 and batch: 150, loss is 3.5257368421554567 and perplexity is 33.97880140761832
At time: 123.48335456848145 and batch: 200, loss is 3.4125338888168333 and perplexity is 30.34203026588496
At time: 124.24474310874939 and batch: 250, loss is 3.552597289085388 and perplexity is 34.90385526262953
At time: 125.00697445869446 and batch: 300, loss is 3.514229974746704 and perplexity is 33.59005277404632
At time: 125.7692756652832 and batch: 350, loss is 3.4880629491806032 and perplexity is 32.722501128823716
At time: 126.53037905693054 and batch: 400, loss is 3.420096139907837 and perplexity is 30.572354103072236
At time: 127.29152750968933 and batch: 450, loss is 3.418654999732971 and perplexity is 30.52832678772052
At time: 128.0534372329712 and batch: 500, loss is 3.2907724714279176 and perplexity is 26.863607011909668
At time: 128.8162922859192 and batch: 550, loss is 3.3398477363586427 and perplexity is 28.214830285523096
At time: 129.57776355743408 and batch: 600, loss is 3.3644312953948976 and perplexity is 28.917047363914765
At time: 130.3410038948059 and batch: 650, loss is 3.1748920774459837 and perplexity is 23.924237764201596
At time: 131.1032853126526 and batch: 700, loss is 3.1625617265701296 and perplexity is 23.631054757363696
At time: 131.86348247528076 and batch: 750, loss is 3.25249183177948 and perplexity is 25.854685241369587
At time: 132.6233856678009 and batch: 800, loss is 3.169081621170044 and perplexity is 23.785630103578825
At time: 133.38551902770996 and batch: 850, loss is 3.2163837814331053 and perplexity is 24.93777647818426
At time: 134.1480085849762 and batch: 900, loss is 3.1650354146957396 and perplexity is 23.689582977348863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.383197523143194 and perplexity of 80.09372593814706
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.0112943649292 and batch: 50, loss is 3.642718505859375 and perplexity is 38.1955304889228
At time: 136.7852816581726 and batch: 100, loss is 3.518902201652527 and perplexity is 33.74736032450543
At time: 137.54700326919556 and batch: 150, loss is 3.516451721191406 and perplexity is 33.66476431869644
At time: 138.30829644203186 and batch: 200, loss is 3.4025972747802733 and perplexity is 30.04202620277633
At time: 139.07057881355286 and batch: 250, loss is 3.5421828746795656 and perplexity is 34.54223833225746
At time: 139.83242011070251 and batch: 300, loss is 3.5027393436431886 and perplexity is 33.20629092427348
At time: 140.59303188323975 and batch: 350, loss is 3.4771329879760744 and perplexity is 32.36679294050781
At time: 141.3512909412384 and batch: 400, loss is 3.4100259590148925 and perplexity is 30.26602992552405
At time: 142.11035633087158 and batch: 450, loss is 3.4093129682540892 and perplexity is 30.244458216949255
At time: 142.88921570777893 and batch: 500, loss is 3.2811925125122072 and perplexity is 26.607483545372187
At time: 143.65091800689697 and batch: 550, loss is 3.3305953693389894 and perplexity is 27.954980287828892
At time: 144.41075801849365 and batch: 600, loss is 3.355592155456543 and perplexity is 28.6625718647887
At time: 145.17163753509521 and batch: 650, loss is 3.165951900482178 and perplexity is 23.71130409546188
At time: 145.93952059745789 and batch: 700, loss is 3.1540525579452514 and perplexity is 23.430827220723174
At time: 146.70915865898132 and batch: 750, loss is 3.2432330417633057 and perplexity is 25.616406926429292
At time: 147.47102856636047 and batch: 800, loss is 3.1595222330093384 and perplexity is 23.55933736608203
At time: 148.2289731502533 and batch: 850, loss is 3.2061294412612913 and perplexity is 24.68336268715568
At time: 148.9879333972931 and batch: 900, loss is 3.1552482414245606 and perplexity is 23.45885982946136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381507350973887 and perplexity of 79.9584680883561
finished 10 epochs...
Completing Train Step...
At time: 150.92801094055176 and batch: 50, loss is 3.639069929122925 and perplexity is 38.056425087625605
At time: 151.69430923461914 and batch: 100, loss is 3.5140567302703856 and perplexity is 33.58423398699488
At time: 152.46025490760803 and batch: 150, loss is 3.5113046407699584 and perplexity is 33.49193423610354
At time: 153.22621297836304 and batch: 200, loss is 3.3975120830535888 and perplexity is 29.889644513102866
At time: 153.99166107177734 and batch: 250, loss is 3.5374815845489502 and perplexity is 34.38022637909552
At time: 154.7563717365265 and batch: 300, loss is 3.4988532161712644 and perplexity is 33.077497460866525
At time: 155.5208294391632 and batch: 350, loss is 3.4728338718414307 and perplexity is 32.22794301857701
At time: 156.28637313842773 and batch: 400, loss is 3.4058182287216185 and perplexity is 30.138946189094412
At time: 157.05167484283447 and batch: 450, loss is 3.405562138557434 and perplexity is 30.131228889620925
At time: 157.81774640083313 and batch: 500, loss is 3.2777762365341188 and perplexity is 26.51674012903053
At time: 158.5820028781891 and batch: 550, loss is 3.3272394227981565 and perplexity is 27.86132211224921
At time: 159.34594321250916 and batch: 600, loss is 3.353084478378296 and perplexity is 28.590785436528847
At time: 160.1111752986908 and batch: 650, loss is 3.1639292526245115 and perplexity is 23.663392947055684
At time: 160.87377071380615 and batch: 700, loss is 3.152480125427246 and perplexity is 23.3940127777766
At time: 161.65669584274292 and batch: 750, loss is 3.242407579421997 and perplexity is 25.595270272148138
At time: 162.41965699195862 and batch: 800, loss is 3.159471855163574 and perplexity is 23.558150527313334
At time: 163.1829845905304 and batch: 850, loss is 3.2074163150787354 and perplexity is 24.71514750746436
At time: 163.94559717178345 and batch: 900, loss is 3.1578127193450927 and perplexity is 23.519096762664887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380882785744863 and perplexity of 79.90854440134441
finished 11 epochs...
Completing Train Step...
At time: 165.80297780036926 and batch: 50, loss is 3.6360671377182006 and perplexity is 37.94232098262821
At time: 166.58046293258667 and batch: 100, loss is 3.5105695629119875 and perplexity is 33.467324103116034
At time: 167.3540120124817 and batch: 150, loss is 3.507632641792297 and perplexity is 33.36917740723711
At time: 168.1184310913086 and batch: 200, loss is 3.3937710523605347 and perplexity is 29.778035332427205
At time: 168.88269972801208 and batch: 250, loss is 3.5337547492980956 and perplexity is 34.25233540182106
At time: 169.64934062957764 and batch: 300, loss is 3.495321178436279 and perplexity is 32.96087257464943
At time: 170.4134225845337 and batch: 350, loss is 3.4692525100708007 and perplexity is 32.11272952924277
At time: 171.17893743515015 and batch: 400, loss is 3.402354989051819 and perplexity is 30.03474833027147
At time: 171.9420530796051 and batch: 450, loss is 3.402342929840088 and perplexity is 30.034386137065955
At time: 172.70668292045593 and batch: 500, loss is 3.2748493814468382 and perplexity is 26.43924294012975
At time: 173.47078156471252 and batch: 550, loss is 3.324484796524048 and perplexity is 27.7846801907158
At time: 174.2336061000824 and batch: 600, loss is 3.3508898878097533 and perplexity is 28.528109167991776
At time: 174.99585962295532 and batch: 650, loss is 3.1621700286865235 and perplexity is 23.62180033581485
At time: 175.7590630054474 and batch: 700, loss is 3.151133208274841 and perplexity is 23.362524191721473
At time: 176.52258014678955 and batch: 750, loss is 3.241671028137207 and perplexity is 25.576424984057194
At time: 177.28809547424316 and batch: 800, loss is 3.159399027824402 and perplexity is 23.556434912367205
At time: 178.05195355415344 and batch: 850, loss is 3.208298406600952 and perplexity is 24.736958147627206
At time: 178.8157079219818 and batch: 900, loss is 3.1595369815826415 and perplexity is 23.559684835258476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380668744648973 and perplexity of 79.89144251924786
finished 12 epochs...
Completing Train Step...
At time: 180.67743277549744 and batch: 50, loss is 3.6333652925491333 and perplexity is 37.83994507018872
At time: 181.45426225662231 and batch: 100, loss is 3.507543168067932 and perplexity is 33.36619187622077
At time: 182.21764183044434 and batch: 150, loss is 3.5044193172454836 and perplexity is 33.262123501953305
At time: 182.98128986358643 and batch: 200, loss is 3.3905386877059938 and perplexity is 29.681937259215434
At time: 183.74186515808105 and batch: 250, loss is 3.530458345413208 and perplexity is 34.139611763729135
At time: 184.50487279891968 and batch: 300, loss is 3.4921244096755983 and perplexity is 32.85567252658114
At time: 185.26739168167114 and batch: 350, loss is 3.466072654724121 and perplexity is 32.01077787621406
At time: 186.02933955192566 and batch: 400, loss is 3.399286608695984 and perplexity is 29.942731541949765
At time: 186.7905421257019 and batch: 450, loss is 3.3994609689712525 and perplexity is 29.94795282004223
At time: 187.55141305923462 and batch: 500, loss is 3.2722155714035033 and perplexity is 26.369698620002413
At time: 188.3137230873108 and batch: 550, loss is 3.3220581340789797 and perplexity is 27.717337892308503
At time: 189.07771110534668 and batch: 600, loss is 3.3488919019699095 and perplexity is 28.47116731328871
At time: 189.8408489227295 and batch: 650, loss is 3.160541214942932 and perplexity is 23.58335614049583
At time: 190.60490441322327 and batch: 700, loss is 3.1498781347274782 and perplexity is 23.33322089834957
At time: 191.36859679222107 and batch: 750, loss is 3.2409326887130736 and perplexity is 25.557547870879073
At time: 192.13255882263184 and batch: 800, loss is 3.1592524814605714 and perplexity is 23.552983055420707
At time: 192.89672660827637 and batch: 850, loss is 3.2089016437530518 and perplexity is 24.751884901545406
At time: 193.66028332710266 and batch: 900, loss is 3.160769338607788 and perplexity is 23.58873667581859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380667908550942 and perplexity of 79.891375722198
finished 13 epochs...
Completing Train Step...
At time: 195.5255901813507 and batch: 50, loss is 3.630852012634277 and perplexity is 37.744962105650316
At time: 196.28979778289795 and batch: 100, loss is 3.5047787046432495 and perplexity is 33.27407963827651
At time: 197.05321955680847 and batch: 150, loss is 3.5014762687683105 and perplexity is 33.16437536933382
At time: 197.81730365753174 and batch: 200, loss is 3.387606201171875 and perplexity is 29.595022877807622
At time: 198.58199405670166 and batch: 250, loss is 3.527441282272339 and perplexity is 34.03676562409626
At time: 199.3457682132721 and batch: 300, loss is 3.489184412956238 and perplexity is 32.75921881341619
At time: 200.12826871871948 and batch: 350, loss is 3.463162784576416 and perplexity is 31.917766061049644
At time: 200.89142394065857 and batch: 400, loss is 3.3964808416366576 and perplexity is 29.858836961485412
At time: 201.6544897556305 and batch: 450, loss is 3.396819543838501 and perplexity is 29.868951928192804
At time: 202.41900849342346 and batch: 500, loss is 3.2697872352600097 and perplexity is 26.305741813491327
At time: 203.18352007865906 and batch: 550, loss is 3.3198389768600465 and perplexity is 27.655896960593388
At time: 203.94790720939636 and batch: 600, loss is 3.3470269203186036 and perplexity is 28.418118591480546
At time: 204.71200442314148 and batch: 650, loss is 3.1589969062805174 and perplexity is 23.54696426669478
At time: 205.475079536438 and batch: 700, loss is 3.1486764097213746 and perplexity is 23.30519762482982
At time: 206.23825526237488 and batch: 750, loss is 3.2401782703399657 and perplexity is 25.538274058366977
At time: 207.0024392604828 and batch: 800, loss is 3.15903160572052 and perplexity is 23.547781347344632
At time: 207.76653146743774 and batch: 850, loss is 3.209300241470337 and perplexity is 24.761752912918414
At time: 208.53188347816467 and batch: 900, loss is 3.161667890548706 and perplexity is 23.609941906516294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380796667647688 and perplexity of 79.90166312585802
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 210.38787198066711 and batch: 50, loss is 3.628999137878418 and perplexity is 37.675090170166634
At time: 211.16523361206055 and batch: 100, loss is 3.5031899070739745 and perplexity is 33.22125583570375
At time: 211.9319155216217 and batch: 150, loss is 3.500352087020874 and perplexity is 33.12711353234124
At time: 212.69764757156372 and batch: 200, loss is 3.386607594490051 and perplexity is 29.565483841607307
At time: 213.4631540775299 and batch: 250, loss is 3.5260888004302977 and perplexity is 33.99076263274552
At time: 214.229070186615 and batch: 300, loss is 3.487440972328186 and perplexity is 32.70215481869128
At time: 214.99485754966736 and batch: 350, loss is 3.4612795066833497 and perplexity is 31.857712604254083
At time: 215.76061916351318 and batch: 400, loss is 3.3945862865447998 and perplexity is 29.802321302802028
At time: 216.52574253082275 and batch: 450, loss is 3.3952170848846435 and perplexity is 29.821126488119603
At time: 217.29025554656982 and batch: 500, loss is 3.2678062820434572 and perplexity is 26.253682949742895
At time: 218.05521154403687 and batch: 550, loss is 3.317588725090027 and perplexity is 27.59373419667189
At time: 218.8199062347412 and batch: 600, loss is 3.3447031688690188 and perplexity is 28.352158614186546
At time: 219.59670519828796 and batch: 650, loss is 3.156489953994751 and perplexity is 23.488007083104492
At time: 220.36151003837585 and batch: 700, loss is 3.146101226806641 and perplexity is 23.2452596867824
At time: 221.1274917125702 and batch: 750, loss is 3.2375141620635985 and perplexity is 25.470327879260175
At time: 221.89302110671997 and batch: 800, loss is 3.156155915260315 and perplexity is 23.480162489216085
At time: 222.65752577781677 and batch: 850, loss is 3.2060379648208617 and perplexity is 24.681104844270514
At time: 223.42145371437073 and batch: 900, loss is 3.1582068967819215 and perplexity is 23.528369287333266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38032552640732 and perplexity of 79.86402702384153
finished 15 epochs...
Completing Train Step...
At time: 225.27300596237183 and batch: 50, loss is 3.628273439407349 and perplexity is 37.64775933300558
At time: 226.05766201019287 and batch: 100, loss is 3.5022361183166506 and perplexity is 33.189584881484464
At time: 226.81968235969543 and batch: 150, loss is 3.4993473529815673 and perplexity is 33.09384630890579
At time: 227.5806121826172 and batch: 200, loss is 3.3856613349914553 and perplexity is 29.53752045408868
At time: 228.34262466430664 and batch: 250, loss is 3.525194220542908 and perplexity is 33.96036877703018
At time: 229.10448741912842 and batch: 300, loss is 3.4866481113433836 and perplexity is 32.67623683205451
At time: 229.86734986305237 and batch: 350, loss is 3.4605300617218018 and perplexity is 31.83384594654666
At time: 230.63707184791565 and batch: 400, loss is 3.393853197097778 and perplexity is 29.780481541785452
At time: 231.40107226371765 and batch: 450, loss is 3.394503364562988 and perplexity is 29.799850137715378
At time: 232.16301250457764 and batch: 500, loss is 3.2671922397613526 and perplexity is 26.237567026786216
At time: 232.92526578903198 and batch: 550, loss is 3.3170553159713747 and perplexity is 27.57901937209342
At time: 233.6863820552826 and batch: 600, loss is 3.3442703771591185 and perplexity is 28.339890689900002
At time: 234.4483823776245 and batch: 650, loss is 3.1561571645736692 and perplexity is 23.480191823314964
At time: 235.21072363853455 and batch: 700, loss is 3.1458639764785765 and perplexity is 23.23974539545535
At time: 235.98147583007812 and batch: 750, loss is 3.237346844673157 and perplexity is 25.46606660696814
At time: 236.7454001903534 and batch: 800, loss is 3.156150097846985 and perplexity is 23.480025895803138
At time: 237.50537967681885 and batch: 850, loss is 3.2062413692474365 and perplexity is 24.686125600853345
At time: 238.27639603614807 and batch: 900, loss is 3.158606672286987 and perplexity is 23.537777233456392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380078877488228 and perplexity of 79.84433107699358
finished 16 epochs...
Completing Train Step...
At time: 240.13542819023132 and batch: 50, loss is 3.627599000930786 and perplexity is 37.62237679599696
At time: 240.89645886421204 and batch: 100, loss is 3.501412434577942 and perplexity is 33.16225841585076
At time: 241.65771412849426 and batch: 150, loss is 3.498481831550598 and perplexity is 33.06521526786939
At time: 242.4202756881714 and batch: 200, loss is 3.3848249959945678 and perplexity is 29.512827401182435
At time: 243.1823854446411 and batch: 250, loss is 3.5243820571899414 and perplexity is 33.932798607312456
At time: 243.94514274597168 and batch: 300, loss is 3.485905523300171 and perplexity is 32.65198085649919
At time: 244.7082324028015 and batch: 350, loss is 3.4598075675964357 and perplexity is 31.81085448646002
At time: 245.47073912620544 and batch: 400, loss is 3.393154296875 and perplexity is 29.75967522821698
At time: 246.2337851524353 and batch: 450, loss is 3.393836340904236 and perplexity is 29.77997956045556
At time: 246.99570679664612 and batch: 500, loss is 3.2666035747528075 and perplexity is 26.222126434282327
At time: 247.7662160396576 and batch: 550, loss is 3.3165254974365235 and perplexity is 27.56441136659481
At time: 248.5262839794159 and batch: 600, loss is 3.3438414430618284 and perplexity is 28.327737351146663
At time: 249.28597712516785 and batch: 650, loss is 3.155816020965576 and perplexity is 23.472183072102453
At time: 250.04280543327332 and batch: 700, loss is 3.145614194869995 and perplexity is 23.233941259380845
At time: 250.80262565612793 and batch: 750, loss is 3.237186098098755 and perplexity is 25.46197335299467
At time: 251.56266522407532 and batch: 800, loss is 3.1561310195922854 and perplexity is 23.479577942161843
At time: 252.3294394016266 and batch: 850, loss is 3.206409115791321 and perplexity is 24.690266960444447
At time: 253.09068655967712 and batch: 900, loss is 3.158938055038452 and perplexity is 23.5455785393771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379949700342466 and perplexity of 79.83401768034166
finished 17 epochs...
Completing Train Step...
At time: 254.95091247558594 and batch: 50, loss is 3.6269534969329835 and perplexity is 37.59809923784172
At time: 255.7367284297943 and batch: 100, loss is 3.500662226676941 and perplexity is 33.13738915729568
At time: 256.5143759250641 and batch: 150, loss is 3.4976923084259033 and perplexity is 33.03911981862825
At time: 257.2923581600189 and batch: 200, loss is 3.38405255317688 and perplexity is 29.490039232038605
At time: 258.0542645454407 and batch: 250, loss is 3.523617148399353 and perplexity is 33.906853035631414
At time: 258.8149652481079 and batch: 300, loss is 3.4851904249191286 and perplexity is 32.62863982441266
At time: 259.5763564109802 and batch: 350, loss is 3.45910361289978 and perplexity is 31.788468966160156
At time: 260.33874583244324 and batch: 400, loss is 3.3924771690368654 and perplexity is 29.73953094456341
At time: 261.11281418800354 and batch: 450, loss is 3.3931961917877196 and perplexity is 29.760922033330438
At time: 261.87559151649475 and batch: 500, loss is 3.2660291814804077 and perplexity is 26.207068946144393
At time: 262.63740372657776 and batch: 550, loss is 3.3160003995895386 and perplexity is 27.549941153004294
At time: 263.4015576839447 and batch: 600, loss is 3.3434142684936523 and perplexity is 28.315639046404304
At time: 264.1662666797638 and batch: 650, loss is 3.155469536781311 and perplexity is 23.464051740668012
At time: 264.9323101043701 and batch: 700, loss is 3.1453553533554075 and perplexity is 23.22792812909288
At time: 265.6978647708893 and batch: 750, loss is 3.2370237636566164 and perplexity is 25.457840333229473
At time: 266.46358370780945 and batch: 800, loss is 3.1560994148254395 and perplexity is 23.47883588730164
At time: 267.228933095932 and batch: 850, loss is 3.2065476894378664 and perplexity is 24.693688617841637
At time: 268.00287437438965 and batch: 900, loss is 3.1592190885543823 and perplexity is 23.55219656599872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379889919333262 and perplexity of 79.8292452648473
finished 18 epochs...
Completing Train Step...
At time: 269.8540382385254 and batch: 50, loss is 3.626327567100525 and perplexity is 37.57457282959353
At time: 270.63116359710693 and batch: 100, loss is 3.499956965446472 and perplexity is 33.114026880666415
At time: 271.3965826034546 and batch: 150, loss is 3.496948342323303 and perplexity is 33.014548974494325
At time: 272.16053557395935 and batch: 200, loss is 3.3833207178115843 and perplexity is 29.468465273660385
At time: 272.92455077171326 and batch: 250, loss is 3.5228830766677857 and perplexity is 33.88197210662215
At time: 273.6878340244293 and batch: 300, loss is 3.4844939613342287 and perplexity is 32.60592307656221
At time: 274.4513885974884 and batch: 350, loss is 3.4584151029586794 and perplexity is 31.766589822132975
At time: 275.2128360271454 and batch: 400, loss is 3.3918164777755737 and perplexity is 29.71988878576926
At time: 275.9860506057739 and batch: 450, loss is 3.392574415206909 and perplexity is 29.74242314067188
At time: 276.7720146179199 and batch: 500, loss is 3.2654652833938598 and perplexity is 26.192294996004225
At time: 277.53338384628296 and batch: 550, loss is 3.3154818201065064 and perplexity is 27.535658022552894
At time: 278.2965898513794 and batch: 600, loss is 3.3429890775680544 and perplexity is 28.303602052820835
At time: 279.06051754951477 and batch: 650, loss is 3.15512065410614 and perplexity is 23.4558669673722
At time: 279.8251712322235 and batch: 700, loss is 3.1450909948349 and perplexity is 23.221788439953542
At time: 280.5875747203827 and batch: 750, loss is 3.236857867240906 and perplexity is 25.4536173190676
At time: 281.35072326660156 and batch: 800, loss is 3.156057262420654 and perplexity is 23.477846218766032
At time: 282.12176489830017 and batch: 850, loss is 3.206662917137146 and perplexity is 24.69653417870836
At time: 282.8893668651581 and batch: 900, loss is 3.1594622850418093 and perplexity is 23.557925074023043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379870689078553 and perplexity of 79.82771014288804
finished 19 epochs...
Completing Train Step...
At time: 284.75740242004395 and batch: 50, loss is 3.6257162952423094 and perplexity is 37.55161156913945
At time: 285.5202171802521 and batch: 100, loss is 3.499281578063965 and perplexity is 33.091669635477714
At time: 286.28363585472107 and batch: 150, loss is 3.4962344789505004 and perplexity is 32.990989507332664
At time: 287.0476186275482 and batch: 200, loss is 3.382617130279541 and perplexity is 29.447738921162014
At time: 287.8090615272522 and batch: 250, loss is 3.522171154022217 and perplexity is 33.857859347630935
At time: 288.5723145008087 and batch: 300, loss is 3.483812189102173 and perplexity is 32.58370083972084
At time: 289.3368501663208 and batch: 350, loss is 3.4577407836914062 and perplexity is 31.745176219174287
At time: 290.0998251438141 and batch: 400, loss is 3.391169548034668 and perplexity is 29.700668323631973
At time: 290.87477827072144 and batch: 450, loss is 3.3919666814804077 and perplexity is 29.7243531584464
At time: 291.64102005958557 and batch: 500, loss is 3.2649102878570555 and perplexity is 26.17776242231258
At time: 292.40140771865845 and batch: 550, loss is 3.314970498085022 and perplexity is 27.521582033218237
At time: 293.1628520488739 and batch: 600, loss is 3.3425663614273073 and perplexity is 28.291640191805808
At time: 293.92437267303467 and batch: 650, loss is 3.1547712421417238 and perplexity is 23.447672638499686
At time: 294.6845233440399 and batch: 700, loss is 3.144823365211487 and perplexity is 23.21557443302164
At time: 295.45658469200134 and batch: 750, loss is 3.2366879320144655 and perplexity is 25.449292220348482
At time: 296.2169940471649 and batch: 800, loss is 3.1560065603256224 and perplexity is 23.476655872952673
At time: 296.9840564727783 and batch: 850, loss is 3.2067594146728515 and perplexity is 24.698917448385053
At time: 297.74863171577454 and batch: 900, loss is 3.1596758556365967 and perplexity is 23.56295689139805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379880304205908 and perplexity of 79.82847770017757
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -79.82771014288804
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
311.5468773841858


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0384788513183594 and batch: 50, loss is 6.727286787033081 and perplexity is 834.8789856921449
At time: 1.876708984375 and batch: 100, loss is 5.908939609527588 and perplexity is 368.31539013354444
At time: 2.708578586578369 and batch: 150, loss is 5.71375997543335 and perplexity is 303.00823259880514
At time: 3.5334510803222656 and batch: 200, loss is 5.523198127746582 and perplexity is 250.43467992689835
At time: 4.358668327331543 and batch: 250, loss is 5.563816747665405 and perplexity is 260.8164094296619
At time: 5.1859142780303955 and batch: 300, loss is 5.478070011138916 and perplexity is 239.3842523107642
At time: 6.009649276733398 and batch: 350, loss is 5.4415625476837155 and perplexity is 230.80254182588374
At time: 6.833713054656982 and batch: 400, loss is 5.291796379089355 and perplexity is 198.70004560793097
At time: 7.6576642990112305 and batch: 450, loss is 5.288335313796997 and perplexity is 198.01352051589362
At time: 8.479630708694458 and batch: 500, loss is 5.231921815872193 and perplexity is 187.15213006391807
At time: 9.304461479187012 and batch: 550, loss is 5.2907278442382815 and perplexity is 198.4878410784473
At time: 10.128693580627441 and batch: 600, loss is 5.204422111511231 and perplexity is 182.07562295379105
At time: 10.950012445449829 and batch: 650, loss is 5.0947972106933594 and perplexity is 163.17075198542517
At time: 11.77471113204956 and batch: 700, loss is 5.191484270095825 and perplexity is 179.735130525725
At time: 12.598108530044556 and batch: 750, loss is 5.177831983566284 and perplexity is 177.29800901836825
At time: 13.419950485229492 and batch: 800, loss is 5.137127828598023 and perplexity is 170.2261468486685
At time: 14.24631118774414 and batch: 850, loss is 5.179231004714966 and perplexity is 177.54622627271286
At time: 15.073079824447632 and batch: 900, loss is 5.096857252120972 and perplexity is 163.5072369617883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.031803496896404 and perplexity of 153.2090757739946
finished 1 epochs...
Completing Train Step...
At time: 17.01875901222229 and batch: 50, loss is 4.990037250518799 and perplexity is 146.94189705564628
At time: 17.77564835548401 and batch: 100, loss is 4.837328653335572 and perplexity is 126.13195909620049
At time: 18.532564401626587 and batch: 150, loss is 4.805829954147339 and perplexity is 122.22088667027546
At time: 19.28996467590332 and batch: 200, loss is 4.679870395660401 and perplexity is 107.7561060073995
At time: 20.06856870651245 and batch: 250, loss is 4.777295560836792 and perplexity is 118.78267474774064
At time: 20.833898067474365 and batch: 300, loss is 4.727424163818359 and perplexity is 113.0041070716156
At time: 21.598662853240967 and batch: 350, loss is 4.7027362442016605 and perplexity is 110.24842672991164
At time: 22.362399339675903 and batch: 400, loss is 4.583916263580322 and perplexity is 97.89703504979207
At time: 23.12797999382019 and batch: 450, loss is 4.596221055984497 and perplexity is 99.1090794275697
At time: 23.892642498016357 and batch: 500, loss is 4.489463052749634 and perplexity is 89.07360520845303
At time: 24.65598750114441 and batch: 550, loss is 4.5692062664031985 and perplexity is 96.46750987332311
At time: 25.421016216278076 and batch: 600, loss is 4.529477043151855 and perplexity is 92.71006503914394
At time: 26.185322046279907 and batch: 650, loss is 4.38564393043518 and perplexity is 80.28990768570928
At time: 26.950396299362183 and batch: 700, loss is 4.4241941452026365 and perplexity is 83.44553513510758
At time: 27.715181589126587 and batch: 750, loss is 4.482614917755127 and perplexity is 88.46570101626006
At time: 28.479889154434204 and batch: 800, loss is 4.412771244049072 and perplexity is 82.49776846362688
At time: 29.244321823120117 and batch: 850, loss is 4.480133457183838 and perplexity is 88.24644901238452
At time: 30.009617805480957 and batch: 900, loss is 4.422204055786133 and perplexity is 83.27963619040071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.545317767417594 and perplexity of 94.19035307968215
finished 2 epochs...
Completing Train Step...
At time: 31.91940402984619 and batch: 50, loss is 4.452898216247559 and perplexity is 85.87546936870349
At time: 32.699211835861206 and batch: 100, loss is 4.303938708305359 and perplexity is 73.99064810846937
At time: 33.46040606498718 and batch: 150, loss is 4.300892224311829 and perplexity is 73.76557979092222
At time: 34.220749378204346 and batch: 200, loss is 4.186985583305359 and perplexity is 65.82407042084434
At time: 34.98146295547485 and batch: 250, loss is 4.334652881622315 and perplexity is 76.29846969157073
At time: 35.74175691604614 and batch: 300, loss is 4.308483252525329 and perplexity is 74.32766709948214
At time: 36.502421379089355 and batch: 350, loss is 4.288388509750366 and perplexity is 72.84897843956435
At time: 37.263429164886475 and batch: 400, loss is 4.210763702392578 and perplexity is 67.40799980828606
At time: 38.023804664611816 and batch: 450, loss is 4.23155526638031 and perplexity is 68.82418892635567
At time: 38.80386686325073 and batch: 500, loss is 4.112643427848816 and perplexity is 61.108038946911435
At time: 39.566049098968506 and batch: 550, loss is 4.198913254737854 and perplexity is 66.61389935114295
At time: 40.328469038009644 and batch: 600, loss is 4.1930496120452885 and perplexity is 66.22444218170114
At time: 41.09043860435486 and batch: 650, loss is 4.041652345657349 and perplexity is 56.920317183923636
At time: 41.85233807563782 and batch: 700, loss is 4.061389956474304 and perplexity is 58.05494887651868
At time: 42.61387872695923 and batch: 750, loss is 4.161487894058228 and perplexity is 64.16692519459112
At time: 43.376410722732544 and batch: 800, loss is 4.100175437927246 and perplexity is 60.35087450099252
At time: 44.13797116279602 and batch: 850, loss is 4.176946148872376 and perplexity is 65.16654012062128
At time: 44.89974355697632 and batch: 900, loss is 4.129351296424866 and perplexity is 62.137600973772734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411899514394264 and perplexity of 82.42588404884188
finished 3 epochs...
Completing Train Step...
At time: 46.79332160949707 and batch: 50, loss is 4.1849333810806275 and perplexity is 65.68912463247011
At time: 47.575210094451904 and batch: 100, loss is 4.044001159667968 and perplexity is 57.05416955804217
At time: 48.33673024177551 and batch: 150, loss is 4.046498665809631 and perplexity is 57.1968407838925
At time: 49.09801483154297 and batch: 200, loss is 3.933089671134949 and perplexity is 51.06450671983855
At time: 49.85999846458435 and batch: 250, loss is 4.089979257583618 and perplexity is 59.738652567288256
At time: 50.622453927993774 and batch: 300, loss is 4.06662980556488 and perplexity is 58.35994642030944
At time: 51.383748292922974 and batch: 350, loss is 4.050935664176941 and perplexity is 57.45118692249182
At time: 52.145721197128296 and batch: 400, loss is 3.9868543672561647 and perplexity is 53.88511968989204
At time: 52.90661811828613 and batch: 450, loss is 4.010478544235229 and perplexity is 55.17326709591337
At time: 53.6681067943573 and batch: 500, loss is 3.8894767713546754 and perplexity is 48.88530164078996
At time: 54.43013048171997 and batch: 550, loss is 3.9735264587402344 and perplexity is 53.17170844462646
At time: 55.193593978881836 and batch: 600, loss is 3.9799911785125732 and perplexity is 53.51656212972891
At time: 55.954814434051514 and batch: 650, loss is 3.8306798124313355 and perplexity is 46.09386276652182
At time: 56.716328144073486 and batch: 700, loss is 3.8421779680252075 and perplexity is 46.62691586346595
At time: 57.478710412979126 and batch: 750, loss is 3.9482828903198244 and perplexity is 51.84626463471395
At time: 58.25732088088989 and batch: 800, loss is 3.894090781211853 and perplexity is 49.111380067392616
At time: 59.02030801773071 and batch: 850, loss is 3.973172173500061 and perplexity is 53.15287382973976
At time: 59.78230309486389 and batch: 900, loss is 3.930088577270508 and perplexity is 50.91148707004543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370384320820848 and perplexity of 79.07401565120064
finished 4 epochs...
Completing Train Step...
At time: 61.70368432998657 and batch: 50, loss is 3.995032215118408 and perplexity is 54.327590765391555
At time: 62.473440408706665 and batch: 100, loss is 3.858485527038574 and perplexity is 47.39352078406944
At time: 63.24124836921692 and batch: 150, loss is 3.8604415130615233 and perplexity is 47.48631256843387
At time: 64.00543451309204 and batch: 200, loss is 3.7508027601242064 and perplexity is 42.555229933596415
At time: 64.7702488899231 and batch: 250, loss is 3.910650887489319 and perplexity is 49.9314411492295
At time: 65.53383994102478 and batch: 300, loss is 3.890867733955383 and perplexity is 48.95334658010852
At time: 66.29902696609497 and batch: 350, loss is 3.8774073457717897 and perplexity is 48.29883043513109
At time: 67.06354236602783 and batch: 400, loss is 3.814655313491821 and perplexity is 45.36111832250618
At time: 67.82842516899109 and batch: 450, loss is 3.8408670473098754 and perplexity is 46.56583172055048
At time: 68.59375548362732 and batch: 500, loss is 3.7222553300857544 and perplexity is 41.35756396300975
At time: 69.35836815834045 and batch: 550, loss is 3.8037127685546874 and perplexity is 44.867458122193305
At time: 70.12288022041321 and batch: 600, loss is 3.818654861450195 and perplexity is 45.54290558178521
At time: 70.88583636283875 and batch: 650, loss is 3.666129674911499 and perplexity is 39.1002818306947
At time: 71.65002274513245 and batch: 700, loss is 3.677972412109375 and perplexity is 39.566088964388506
At time: 72.4140453338623 and batch: 750, loss is 3.7860074853897094 and perplexity is 44.08005820966426
At time: 73.17792057991028 and batch: 800, loss is 3.733937702178955 and perplexity is 41.84355163128191
At time: 73.94213700294495 and batch: 850, loss is 3.8108065128326416 and perplexity is 45.186867962956114
At time: 74.70638942718506 and batch: 900, loss is 3.7727622890472414 and perplexity is 43.50005876647793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365619816192209 and perplexity of 78.69816322394314
finished 5 epochs...
Completing Train Step...
At time: 76.56064534187317 and batch: 50, loss is 3.842319884300232 and perplexity is 46.633533451241775
At time: 77.33693766593933 and batch: 100, loss is 3.7093135499954224 and perplexity is 40.825772054545524
At time: 78.1004786491394 and batch: 150, loss is 3.7131884765625 and perplexity is 40.98427582019886
At time: 78.86399269104004 and batch: 200, loss is 3.603982548713684 and perplexity is 36.74427931756605
At time: 79.62463641166687 and batch: 250, loss is 3.766608281135559 and perplexity is 43.23318108660833
At time: 80.38573718070984 and batch: 300, loss is 3.748187265396118 and perplexity is 42.44407238341308
At time: 81.14693975448608 and batch: 350, loss is 3.73257652759552 and perplexity is 41.786633998526355
At time: 81.92854690551758 and batch: 400, loss is 3.675055160522461 and perplexity is 39.45083292573914
At time: 82.69416117668152 and batch: 450, loss is 3.7003419160842896 and perplexity is 40.46113630854233
At time: 83.45794701576233 and batch: 500, loss is 3.585165023803711 and perplexity is 36.0593078720315
At time: 84.22131967544556 and batch: 550, loss is 3.6686048126220703 and perplexity is 39.19718028178388
At time: 84.98694610595703 and batch: 600, loss is 3.686388945579529 and perplexity is 39.90050360835826
At time: 85.75060677528381 and batch: 650, loss is 3.5320826387405395 and perplexity is 34.195109567469004
At time: 86.5138955116272 and batch: 700, loss is 3.544399209022522 and perplexity is 34.61888038218648
At time: 87.27665901184082 and batch: 750, loss is 3.6513786458969117 and perplexity is 38.5277455700404
At time: 88.03773808479309 and batch: 800, loss is 3.6011756229400635 and perplexity is 36.641285468583675
At time: 88.79885077476501 and batch: 850, loss is 3.6767747163772584 and perplexity is 39.51872919545723
At time: 89.55680465698242 and batch: 900, loss is 3.6422132015228272 and perplexity is 38.17623499718839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385418199513056 and perplexity of 80.27178581622023
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.4055724143982 and batch: 50, loss is 3.7344167232513428 and perplexity is 41.86360037575901
At time: 92.17939901351929 and batch: 100, loss is 3.6091502332687377 and perplexity is 36.93465363583269
At time: 92.98324608802795 and batch: 150, loss is 3.6168426179885866 and perplexity is 37.21986477159187
At time: 93.74785304069519 and batch: 200, loss is 3.4875617361068727 and perplexity is 32.7061042929503
At time: 94.50982785224915 and batch: 250, loss is 3.6389441013336183 and perplexity is 38.05163683304198
At time: 95.27160334587097 and batch: 300, loss is 3.6108604192733766 and perplexity is 36.99787280642898
At time: 96.03606367111206 and batch: 350, loss is 3.5844801664352417 and perplexity is 36.034620843844934
At time: 96.82109546661377 and batch: 400, loss is 3.513481044769287 and perplexity is 33.56490559449454
At time: 97.58600616455078 and batch: 450, loss is 3.519807834625244 and perplexity is 33.7779368902407
At time: 98.34968423843384 and batch: 500, loss is 3.3953614950180055 and perplexity is 29.825433271936888
At time: 99.13132667541504 and batch: 550, loss is 3.450548300743103 and perplexity is 31.5176687331642
At time: 99.90138506889343 and batch: 600, loss is 3.4645089530944824 and perplexity is 31.960761686073504
At time: 100.67389512062073 and batch: 650, loss is 3.2923896980285643 and perplexity is 26.907086700536688
At time: 101.43973517417908 and batch: 700, loss is 3.2814652824401858 and perplexity is 26.61474225667662
At time: 102.20457577705383 and batch: 750, loss is 3.371314034461975 and perplexity is 29.116762360205307
At time: 102.96827530860901 and batch: 800, loss is 3.296641035079956 and perplexity is 27.021721297669284
At time: 103.73351216316223 and batch: 850, loss is 3.3489665031433105 and perplexity is 28.47329137500614
At time: 104.49905395507812 and batch: 900, loss is 3.3010160779953 and perplexity is 27.140201476941822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.355735047222817 and perplexity of 77.92408217120206
finished 7 epochs...
Completing Train Step...
At time: 106.40192818641663 and batch: 50, loss is 3.6185310554504393 and perplexity is 37.28276126905637
At time: 107.17581152915955 and batch: 100, loss is 3.4858013725280763 and perplexity is 32.64858030457069
At time: 107.9465274810791 and batch: 150, loss is 3.4921988344192503 and perplexity is 32.85811789258318
At time: 108.71215152740479 and batch: 200, loss is 3.3692970514297484 and perplexity is 29.058093531477244
At time: 109.4760115146637 and batch: 250, loss is 3.5211126947402955 and perplexity is 33.82204114153883
At time: 110.2427077293396 and batch: 300, loss is 3.4974079370498656 and perplexity is 33.0297257744243
At time: 111.00801491737366 and batch: 350, loss is 3.4750932264328003 and perplexity is 32.30083968830962
At time: 111.7744734287262 and batch: 400, loss is 3.411289024353027 and perplexity is 30.30428205122756
At time: 112.53933429718018 and batch: 450, loss is 3.4238517093658447 and perplexity is 30.68738657341292
At time: 113.30308723449707 and batch: 500, loss is 3.3064845514297487 and perplexity is 27.28902349147375
At time: 114.06654191017151 and batch: 550, loss is 3.3645344400405883 and perplexity is 28.920030156346428
At time: 114.830983877182 and batch: 600, loss is 3.3857803344726562 and perplexity is 29.541035612845565
At time: 115.61564517021179 and batch: 650, loss is 3.2201122093200683 and perplexity is 25.03092872725551
At time: 116.37973141670227 and batch: 700, loss is 3.2143579483032227 and perplexity is 24.887307842186836
At time: 117.14774370193481 and batch: 750, loss is 3.312246961593628 and perplexity is 27.446727980392172
At time: 117.91665124893188 and batch: 800, loss is 3.245600075721741 and perplexity is 25.677113650611847
At time: 118.67977380752563 and batch: 850, loss is 3.306226201057434 and perplexity is 27.28197427271797
At time: 119.44130897521973 and batch: 900, loss is 3.2686109018325804 and perplexity is 26.27481568334932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37484239552119 and perplexity of 79.42732046431004
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.31760263442993 and batch: 50, loss is 3.568131756782532 and perplexity is 35.450301461147596
At time: 122.09694218635559 and batch: 100, loss is 3.4453432512283326 and perplexity is 31.35404391387857
At time: 122.86271977424622 and batch: 150, loss is 3.4565105056762695 and perplexity is 31.706144841431268
At time: 123.62835574150085 and batch: 200, loss is 3.329647870063782 and perplexity is 27.928505508665427
At time: 124.39274644851685 and batch: 250, loss is 3.4812994956970216 and perplexity is 32.501930764092485
At time: 125.1591305732727 and batch: 300, loss is 3.4555331563949583 and perplexity is 31.67517200166416
At time: 125.92245602607727 and batch: 350, loss is 3.430263466835022 and perplexity is 30.884778792760095
At time: 126.69537234306335 and batch: 400, loss is 3.3617438888549804 and perplexity is 28.83943982991888
At time: 127.4620053768158 and batch: 450, loss is 3.366866421699524 and perplexity is 28.98754983297221
At time: 128.22767424583435 and batch: 500, loss is 3.245736141204834 and perplexity is 25.68060765718661
At time: 128.9934515953064 and batch: 550, loss is 3.289338593482971 and perplexity is 26.825115480967124
At time: 129.75805592536926 and batch: 600, loss is 3.311433348655701 and perplexity is 27.424406049340877
At time: 130.5225760936737 and batch: 650, loss is 3.1400733900070192 and perplexity is 23.105562513894427
At time: 131.28701305389404 and batch: 700, loss is 3.124874668121338 and perplexity is 22.75704273186616
At time: 132.05111479759216 and batch: 750, loss is 3.215052533149719 and perplexity is 24.90460019389161
At time: 132.81642365455627 and batch: 800, loss is 3.142978720664978 and perplexity is 23.172789423948622
At time: 133.58188033103943 and batch: 850, loss is 3.194654188156128 and perplexity is 24.40173382450857
At time: 134.34601879119873 and batch: 900, loss is 3.1565756225585937 and perplexity is 23.490019353131743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369524393996147 and perplexity of 79.00604701222174
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.22231125831604 and batch: 50, loss is 3.5421560478210448 and perplexity is 34.5413116849463
At time: 136.99829983711243 and batch: 100, loss is 3.4182716178894044 and perplexity is 30.516625024780687
At time: 137.76027750968933 and batch: 150, loss is 3.430579023361206 and perplexity is 30.894526224119815
At time: 138.52140164375305 and batch: 200, loss is 3.3058612298965455 and perplexity is 27.27201895570864
At time: 139.28345251083374 and batch: 250, loss is 3.455369429588318 and perplexity is 31.669986351429102
At time: 140.04508018493652 and batch: 300, loss is 3.4304939985275267 and perplexity is 30.89189953383454
At time: 140.80661034584045 and batch: 350, loss is 3.408145399093628 and perplexity is 30.209166327130582
At time: 141.56850361824036 and batch: 400, loss is 3.3397490787506103 and perplexity is 28.212046815163607
At time: 142.3302767276764 and batch: 450, loss is 3.342917213439941 and perplexity is 28.301568112221403
At time: 143.09231281280518 and batch: 500, loss is 3.222032675743103 and perplexity is 25.079045974403535
At time: 143.8550319671631 and batch: 550, loss is 3.2622244644165037 and perplexity is 26.107547908479653
At time: 144.61662578582764 and batch: 600, loss is 3.2855312967300416 and perplexity is 26.723178481239206
At time: 145.3787932395935 and batch: 650, loss is 3.11308274269104 and perplexity is 22.49026935804167
At time: 146.14208984375 and batch: 700, loss is 3.097500147819519 and perplexity is 22.14252898716247
At time: 146.90804529190063 and batch: 750, loss is 3.1834535884857176 and perplexity is 24.12994471458433
At time: 147.6745207309723 and batch: 800, loss is 3.1109791088104246 and perplexity is 22.443007793379046
At time: 148.44184851646423 and batch: 850, loss is 3.1601762676239016 and perplexity is 23.574751028200453
At time: 149.20801329612732 and batch: 900, loss is 3.1240199184417725 and perplexity is 22.737599467628947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370316596880351 and perplexity of 79.06866062860352
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.14131355285645 and batch: 50, loss is 3.5335042667388916 and perplexity is 34.24375686362284
At time: 151.9032802581787 and batch: 100, loss is 3.4085426712036133 and perplexity is 30.221169970576764
At time: 152.66417956352234 and batch: 150, loss is 3.4191867208480833 and perplexity is 30.544563660043973
At time: 153.42392992973328 and batch: 200, loss is 3.295855221748352 and perplexity is 27.000495609637248
At time: 154.20294213294983 and batch: 250, loss is 3.4460759925842286 and perplexity is 31.377026737733296
At time: 154.96915817260742 and batch: 300, loss is 3.4200982284545898 and perplexity is 30.572417954929804
At time: 155.73883414268494 and batch: 350, loss is 3.3981510496139524 and perplexity is 29.90874909939379
At time: 156.50080704689026 and batch: 400, loss is 3.3310044050216674 and perplexity is 27.96641721117059
At time: 157.2638328075409 and batch: 450, loss is 3.3345119333267212 and perplexity is 28.064682444428843
At time: 158.02728843688965 and batch: 500, loss is 3.213643798828125 and perplexity is 24.869540929225625
At time: 158.7903974056244 and batch: 550, loss is 3.253899583816528 and perplexity is 25.891107858176774
At time: 159.5523271560669 and batch: 600, loss is 3.277066078186035 and perplexity is 26.497915729607268
At time: 160.31398963928223 and batch: 650, loss is 3.103538393974304 and perplexity is 22.27663550429241
At time: 161.07509684562683 and batch: 700, loss is 3.0891176748275755 and perplexity is 21.957695598686094
At time: 161.83958649635315 and batch: 750, loss is 3.1746612787246704 and perplexity is 23.918716717866946
At time: 162.6033489704132 and batch: 800, loss is 3.10195068359375 and perplexity is 22.24129472174607
At time: 163.36385416984558 and batch: 850, loss is 3.1501970195770266 and perplexity is 23.34066269546033
At time: 164.1266849040985 and batch: 900, loss is 3.113814969062805 and perplexity is 22.50674335698591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370655216582834 and perplexity of 79.09543936858988
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 165.99789834022522 and batch: 50, loss is 3.530901050567627 and perplexity is 34.15472889179516
At time: 166.79079389572144 and batch: 100, loss is 3.4059731912612916 and perplexity is 30.14361695862675
At time: 167.55788850784302 and batch: 150, loss is 3.4160104179382325 and perplexity is 30.447698791131266
At time: 168.32304525375366 and batch: 200, loss is 3.29325270652771 and perplexity is 26.930317767908953
At time: 169.0871181488037 and batch: 250, loss is 3.4434354734420776 and perplexity is 31.294284387464057
At time: 169.85293340682983 and batch: 300, loss is 3.4174352407455446 and perplexity is 30.491112287724537
At time: 170.61749982833862 and batch: 350, loss is 3.3948439598083495 and perplexity is 29.810001553648632
At time: 171.38280630111694 and batch: 400, loss is 3.3280813598632815 and perplexity is 27.88478946965165
At time: 172.1477084159851 and batch: 450, loss is 3.3321659326553346 and perplexity is 27.99891985027854
At time: 172.9141616821289 and batch: 500, loss is 3.2113059997558593 and perplexity is 24.811468846392764
At time: 173.7014467716217 and batch: 550, loss is 3.2514656591415405 and perplexity is 25.828167479040417
At time: 174.4671277999878 and batch: 600, loss is 3.274968137741089 and perplexity is 26.442382953088575
At time: 175.23275589942932 and batch: 650, loss is 3.100818090438843 and perplexity is 22.2161186434062
At time: 175.99713110923767 and batch: 700, loss is 3.0867248153686524 and perplexity is 21.905216731393807
At time: 176.7612602710724 and batch: 750, loss is 3.1723597192764283 and perplexity is 23.863729671665975
At time: 177.5268497467041 and batch: 800, loss is 3.099575567245483 and perplexity is 22.18853174295636
At time: 178.29268097877502 and batch: 850, loss is 3.1476097965240477 and perplexity is 23.28035324550368
At time: 179.05710792541504 and batch: 900, loss is 3.1110499048233033 and perplexity is 22.444596725092175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3705160062607025 and perplexity of 79.08442923337628
Annealing...
Model not improving. Stopping early with 77.92408217120206 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
497.31873083114624


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0352742671966553 and batch: 50, loss is 6.713897094726563 and perplexity is 823.7747202103969
At time: 1.8712961673736572 and batch: 100, loss is 5.836930561065674 and perplexity is 342.7257487671058
At time: 2.692925214767456 and batch: 150, loss is 5.576382646560669 and perplexity is 264.1144802756807
At time: 3.5171029567718506 and batch: 200, loss is 5.3531763553619385 and perplexity is 211.27832822679454
At time: 4.343740701675415 and batch: 250, loss is 5.371081304550171 and perplexity is 215.0953255570643
At time: 5.170825242996216 and batch: 300, loss is 5.274644756317139 and perplexity is 195.32107760756972
At time: 5.999387264251709 and batch: 350, loss is 5.223800258636475 and perplexity is 185.63831890068786
At time: 6.82511830329895 and batch: 400, loss is 5.0629746627807615 and perplexity is 158.05999288792168
At time: 7.65286660194397 and batch: 450, loss is 5.057055912017822 and perplexity is 157.12723827844414
At time: 8.480340957641602 and batch: 500, loss is 4.975169944763183 and perplexity is 144.77342654337176
At time: 9.30725383758545 and batch: 550, loss is 5.03347806930542 and perplexity is 153.46585039894387
At time: 10.134888410568237 and batch: 600, loss is 4.957049322128296 and perplexity is 142.1736676751378
At time: 10.962105512619019 and batch: 650, loss is 4.8379808616638185 and perplexity is 126.21425024294967
At time: 11.789592742919922 and batch: 700, loss is 4.903013229370117 and perplexity is 134.69503585145176
At time: 12.616920709609985 and batch: 750, loss is 4.912393274307251 and perplexity is 135.96442549923398
At time: 13.4453763961792 and batch: 800, loss is 4.851954746246338 and perplexity is 127.99033410279786
At time: 14.27294635772705 and batch: 850, loss is 4.897460641860962 and perplexity is 133.94920244670703
At time: 15.098825931549072 and batch: 900, loss is 4.823911085128784 and perplexity is 124.4508781615228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.81456516213613 and perplexity of 123.29318810556066
finished 1 epochs...
Completing Train Step...
At time: 16.99808979034424 and batch: 50, loss is 4.7980022144317624 and perplexity is 121.26790808180962
At time: 17.749080419540405 and batch: 100, loss is 4.644898881912232 and perplexity is 104.05284362885949
At time: 18.50155544281006 and batch: 150, loss is 4.621409711837768 and perplexity is 101.6372103644164
At time: 19.261677265167236 and batch: 200, loss is 4.511805992126465 and perplexity is 91.08617096383068
At time: 20.022684574127197 and batch: 250, loss is 4.630734138488769 and perplexity is 102.58935126281386
At time: 20.784191608428955 and batch: 300, loss is 4.586869640350342 and perplexity is 98.18658924981781
At time: 21.550363779067993 and batch: 350, loss is 4.572410383224487 and perplexity is 96.77709875903162
At time: 22.31462597846985 and batch: 400, loss is 4.463605709075928 and perplexity is 86.79992078719147
At time: 23.079333066940308 and batch: 450, loss is 4.479311323165893 and perplexity is 88.17392841959654
At time: 23.844136476516724 and batch: 500, loss is 4.37207516670227 and perplexity is 79.20783072334284
At time: 24.607760906219482 and batch: 550, loss is 4.445761394500733 and perplexity is 85.26477325814416
At time: 25.37156057357788 and batch: 600, loss is 4.432415218353271 and perplexity is 84.13437460372057
At time: 26.136481523513794 and batch: 650, loss is 4.27919855594635 and perplexity is 72.18256653052312
At time: 26.90068507194519 and batch: 700, loss is 4.311988496780396 and perplexity is 74.58866088382658
At time: 27.66518473625183 and batch: 750, loss is 4.388833150863648 and perplexity is 80.54637865334175
At time: 28.43118715286255 and batch: 800, loss is 4.325111207962036 and perplexity is 75.57391681369656
At time: 29.19391894340515 and batch: 850, loss is 4.3926114749908445 and perplexity is 80.85128463324658
At time: 29.957149744033813 and batch: 900, loss is 4.334439043998718 and perplexity is 76.28215595243559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.501248712409033 and perplexity of 90.1296070197003
finished 2 epochs...
Completing Train Step...
At time: 31.81789803504944 and batch: 50, loss is 4.387499866485595 and perplexity is 80.43905898469143
At time: 32.59262752532959 and batch: 100, loss is 4.236293334960937 and perplexity is 69.15105640225848
At time: 33.35579824447632 and batch: 150, loss is 4.230676794052124 and perplexity is 68.76375532938182
At time: 34.11989688873291 and batch: 200, loss is 4.127397465705871 and perplexity is 62.01631314671097
At time: 34.882627964019775 and batch: 250, loss is 4.27240611076355 and perplexity is 71.69393179522727
At time: 35.65718150138855 and batch: 300, loss is 4.242597436904907 and perplexity is 69.58836869369121
At time: 36.42123460769653 and batch: 350, loss is 4.2339494514465335 and perplexity is 68.98916418352174
At time: 37.18583559989929 and batch: 400, loss is 4.14587851524353 and perplexity is 63.17309605859945
At time: 37.94951367378235 and batch: 450, loss is 4.169052839279175 and perplexity is 64.65418519252438
At time: 38.71258902549744 and batch: 500, loss is 4.049996566772461 and perplexity is 57.39725998725428
At time: 39.47609615325928 and batch: 550, loss is 4.130731415748596 and perplexity is 62.22341748249646
At time: 40.250168323516846 and batch: 600, loss is 4.142058458328247 and perplexity is 62.93223158704392
At time: 41.01707601547241 and batch: 650, loss is 3.9821580457687378 and perplexity is 53.63265114519066
At time: 41.78048753738403 and batch: 700, loss is 3.999896068572998 and perplexity is 54.59247586436697
At time: 42.54336404800415 and batch: 750, loss is 4.101254215240479 and perplexity is 60.41601478485086
At time: 43.30694246292114 and batch: 800, loss is 4.047235293388367 and perplexity is 57.23898907612181
At time: 44.07172107696533 and batch: 850, loss is 4.121665410995483 and perplexity is 61.661849120947025
At time: 44.83620095252991 and batch: 900, loss is 4.0748093223571775 and perplexity is 58.83926018720366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404237512039812 and perplexity of 81.79675002222861
finished 3 epochs...
Completing Train Step...
At time: 46.70551681518555 and batch: 50, loss is 4.143751406669617 and perplexity is 63.038862839260965
At time: 47.48319673538208 and batch: 100, loss is 3.99986008644104 and perplexity is 54.59051154603689
At time: 48.24878001213074 and batch: 150, loss is 4.001639647483826 and perplexity is 54.68774518462734
At time: 49.013511419296265 and batch: 200, loss is 3.894608597755432 and perplexity is 49.136817337820865
At time: 49.78950643539429 and batch: 250, loss is 4.047937726974487 and perplexity is 57.27920978902046
At time: 50.55599355697632 and batch: 300, loss is 4.018533430099487 and perplexity is 55.619476134020914
At time: 51.322325468063354 and batch: 350, loss is 4.012098560333252 and perplexity is 55.262721115701304
At time: 52.08768558502197 and batch: 400, loss is 3.9363389778137208 and perplexity is 51.23070082415508
At time: 52.852383613586426 and batch: 450, loss is 3.9617118358612062 and perplexity is 52.54720118471999
At time: 53.617130517959595 and batch: 500, loss is 3.846748161315918 and perplexity is 46.84049756458974
At time: 54.38210916519165 and batch: 550, loss is 3.925105872154236 and perplexity is 50.65844109322439
At time: 55.15805506706238 and batch: 600, loss is 3.945498490333557 and perplexity is 51.70210468879068
At time: 55.92397475242615 and batch: 650, loss is 3.784410915374756 and perplexity is 44.009737461405855
At time: 56.68931794166565 and batch: 700, loss is 3.7963575506210328 and perplexity is 44.53865886697894
At time: 57.45483589172363 and batch: 750, loss is 3.9027086687088013 and perplexity is 49.53644536737498
At time: 58.22009253501892 and batch: 800, loss is 3.8558426475524903 and perplexity is 47.26843079191386
At time: 58.98516631126404 and batch: 850, loss is 3.9289079093933106 and perplexity is 50.851412983432134
At time: 59.749160051345825 and batch: 900, loss is 3.8892437648773193 and perplexity is 48.873912375797936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378866953392551 and perplexity of 79.74762442065824
finished 4 epochs...
Completing Train Step...
At time: 61.62313508987427 and batch: 50, loss is 3.9652693700790405 and perplexity is 52.73447256564927
At time: 62.3846070766449 and batch: 100, loss is 3.82431658744812 and perplexity is 45.80148835600444
At time: 63.14646530151367 and batch: 150, loss is 3.8287883472442625 and perplexity is 46.0067602314361
At time: 63.907543420791626 and batch: 200, loss is 3.7252878284454347 and perplexity is 41.48317106329336
At time: 64.66955828666687 and batch: 250, loss is 3.87451623916626 and perplexity is 48.159395025859645
At time: 65.43269944190979 and batch: 300, loss is 3.849826521873474 and perplexity is 46.984911670089396
At time: 66.19496726989746 and batch: 350, loss is 3.84272171497345 and perplexity is 46.652276000796476
At time: 66.9558470249176 and batch: 400, loss is 3.774554762840271 and perplexity is 43.57810140560911
At time: 67.7179205417633 and batch: 450, loss is 3.801282539367676 and perplexity is 44.75855230256841
At time: 68.48016571998596 and batch: 500, loss is 3.6883808851242064 and perplexity is 39.98006221100546
At time: 69.24199199676514 and batch: 550, loss is 3.761960339546204 and perplexity is 43.03270205452954
At time: 70.00294423103333 and batch: 600, loss is 3.791066217422485 and perplexity is 44.30361238572844
At time: 70.76427221298218 and batch: 650, loss is 3.63206907749176 and perplexity is 37.790928138723956
At time: 71.52610659599304 and batch: 700, loss is 3.6386941051483155 and perplexity is 38.04212525796747
At time: 72.28829860687256 and batch: 750, loss is 3.7466943740844725 and perplexity is 42.380755271064174
At time: 73.05196928977966 and batch: 800, loss is 3.704422392845154 and perplexity is 40.62657433843646
At time: 73.82467770576477 and batch: 850, loss is 3.776470603942871 and perplexity is 43.66167015011183
At time: 74.58659982681274 and batch: 900, loss is 3.738599328994751 and perplexity is 42.03906600710676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380862719392123 and perplexity of 79.90694094439337
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.44847083091736 and batch: 50, loss is 3.837863540649414 and perplexity is 46.426180761104504
At time: 77.2224931716919 and batch: 100, loss is 3.6980450773239135 and perplexity is 40.368310246437126
At time: 77.98566770553589 and batch: 150, loss is 3.7057632637023925 and perplexity is 40.681085866288505
At time: 78.74821138381958 and batch: 200, loss is 3.5838275718688966 and perplexity is 36.011112517619225
At time: 79.50971150398254 and batch: 250, loss is 3.729636402130127 and perplexity is 41.6639564838089
At time: 80.27287864685059 and batch: 300, loss is 3.6938902759552 and perplexity is 40.20093588016879
At time: 81.03619718551636 and batch: 350, loss is 3.6688556098937988 and perplexity is 39.20701206049811
At time: 81.81375741958618 and batch: 400, loss is 3.5933122301101683 and perplexity is 36.354290503039515
At time: 82.57687568664551 and batch: 450, loss is 3.6057166719436644 and perplexity is 36.80805370624219
At time: 83.3370795249939 and batch: 500, loss is 3.4814776229858397 and perplexity is 32.50772076056373
At time: 84.10527849197388 and batch: 550, loss is 3.5317787599563597 and perplexity is 34.184719977816634
At time: 84.86511135101318 and batch: 600, loss is 3.550889277458191 and perplexity is 34.84428995560627
At time: 85.62564516067505 and batch: 650, loss is 3.3746344470977783 and perplexity is 29.213602711850257
At time: 86.38560724258423 and batch: 700, loss is 3.3608511877059937 and perplexity is 28.813706316723927
At time: 87.14622497558594 and batch: 750, loss is 3.449036226272583 and perplexity is 31.47004768326266
At time: 87.90710282325745 and batch: 800, loss is 3.388127703666687 and perplexity is 29.610460781175096
At time: 88.66689348220825 and batch: 850, loss is 3.4376398372650145 and perplexity is 31.11343866494669
At time: 89.42642879486084 and batch: 900, loss is 3.3855244970321654 and perplexity is 29.533478876594174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340745481726241 and perplexity of 76.76474472727399
finished 6 epochs...
Completing Train Step...
At time: 91.33471894264221 and batch: 50, loss is 3.714464712142944 and perplexity is 41.03661480257107
At time: 92.11797976493835 and batch: 100, loss is 3.5715217876434324 and perplexity is 35.570683010424204
At time: 92.88174843788147 and batch: 150, loss is 3.57794056892395 and perplexity is 35.799737784479476
At time: 93.66916751861572 and batch: 200, loss is 3.4629514169692994 and perplexity is 31.91102039214588
At time: 94.43448543548584 and batch: 250, loss is 3.6097276639938354 and perplexity is 36.95598699833959
At time: 95.19747066497803 and batch: 300, loss is 3.5792054080963136 and perplexity is 35.845047343801795
At time: 95.96115589141846 and batch: 350, loss is 3.5584845447540285 and perplexity is 35.10994925111885
At time: 96.72316884994507 and batch: 400, loss is 3.4893612241744996 and perplexity is 32.76501152289681
At time: 97.48691701889038 and batch: 450, loss is 3.5064362859725953 and perplexity is 33.32927986823246
At time: 98.25187563896179 and batch: 500, loss is 3.3874976348876955 and perplexity is 29.59181003055017
At time: 99.01752400398254 and batch: 550, loss is 3.4418452835083007 and perplexity is 31.244560077473135
At time: 99.78270983695984 and batch: 600, loss is 3.469706220626831 and perplexity is 32.12730271936823
At time: 100.55122518539429 and batch: 650, loss is 3.3000166177749635 and perplexity is 27.11308947613374
At time: 101.31511926651001 and batch: 700, loss is 3.291940803527832 and perplexity is 26.89501096785387
At time: 102.08099508285522 and batch: 750, loss is 3.3878082466125488 and perplexity is 29.601003021356743
At time: 102.84634351730347 and batch: 800, loss is 3.3333989572525025 and perplexity is 28.03346449997279
At time: 103.61121320724487 and batch: 850, loss is 3.392610411643982 and perplexity is 29.743493781204364
At time: 104.37597012519836 and batch: 900, loss is 3.3496273708343507 and perplexity is 28.49211467250295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356681510193707 and perplexity of 77.99786934242216
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.25959539413452 and batch: 50, loss is 3.658084959983826 and perplexity is 38.7869930588071
At time: 107.02373242378235 and batch: 100, loss is 3.525800576210022 and perplexity is 33.98096708341422
At time: 107.7878429889679 and batch: 150, loss is 3.533842029571533 and perplexity is 34.255325085489105
At time: 108.55003428459167 and batch: 200, loss is 3.4150338649749754 and perplexity is 30.417979514238215
At time: 109.31313395500183 and batch: 250, loss is 3.5594894790649416 and perplexity is 35.145250178361366
At time: 110.07610392570496 and batch: 300, loss is 3.5276920652389525 and perplexity is 34.04530253556466
At time: 110.83891606330872 and batch: 350, loss is 3.503560366630554 and perplexity is 33.23356524733847
At time: 111.59950470924377 and batch: 400, loss is 3.432056622505188 and perplexity is 30.94020969223727
At time: 112.37259697914124 and batch: 450, loss is 3.4430098867416383 and perplexity is 31.280968789900584
At time: 113.13613057136536 and batch: 500, loss is 3.319364547729492 and perplexity is 27.64277930938765
At time: 113.89911580085754 and batch: 550, loss is 3.364834232330322 and perplexity is 28.928701458135937
At time: 114.6626307964325 and batch: 600, loss is 3.3897450971603393 and perplexity is 29.658391298591425
At time: 115.42604422569275 and batch: 650, loss is 3.2128928327560424 and perplexity is 24.850871758593755
At time: 116.18895220756531 and batch: 700, loss is 3.197098708152771 and perplexity is 24.461457318692556
At time: 116.95271897315979 and batch: 750, loss is 3.2875492906570436 and perplexity is 26.777160142137344
At time: 117.7153058052063 and batch: 800, loss is 3.2267663049697877 and perplexity is 25.198042299448154
At time: 118.47867631912231 and batch: 850, loss is 3.277972869873047 and perplexity is 26.5219547168437
At time: 119.24299359321594 and batch: 900, loss is 3.2336396551132203 and perplexity is 25.37183384798234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351228060787672 and perplexity of 77.57366963531675
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.0930061340332 and batch: 50, loss is 3.6314009189605714 and perplexity is 37.76568624142011
At time: 121.86817979812622 and batch: 100, loss is 3.4993532943725585 and perplexity is 33.09404293297022
At time: 122.62989473342896 and batch: 150, loss is 3.506447796821594 and perplexity is 33.32966351874833
At time: 123.39186787605286 and batch: 200, loss is 3.3883651304244995 and perplexity is 29.617491931534293
At time: 124.15558218955994 and batch: 250, loss is 3.5306886863708495 and perplexity is 34.147476420337796
At time: 124.91944360733032 and batch: 300, loss is 3.5016546630859375 and perplexity is 33.17029223319911
At time: 125.68319821357727 and batch: 350, loss is 3.478324460983276 and perplexity is 32.405380083831076
At time: 126.44728231430054 and batch: 400, loss is 3.408721437454224 and perplexity is 30.226572978744834
At time: 127.21088027954102 and batch: 450, loss is 3.417117495536804 and perplexity is 30.48142542194504
At time: 127.97613906860352 and batch: 500, loss is 3.2926922464370727 and perplexity is 26.915228628397085
At time: 128.73961305618286 and batch: 550, loss is 3.337231969833374 and perplexity is 28.141123319080528
At time: 129.504816532135 and batch: 600, loss is 3.362344217300415 and perplexity is 28.8567581638233
At time: 130.270409822464 and batch: 650, loss is 3.1838558387756346 and perplexity is 24.139652944279653
At time: 131.03407263755798 and batch: 700, loss is 3.167006311416626 and perplexity is 23.73631873930959
At time: 131.8110339641571 and batch: 750, loss is 3.2575457811355593 and perplexity is 25.985684263573084
At time: 132.57471370697021 and batch: 800, loss is 3.1953864336013793 and perplexity is 24.419608426457067
At time: 133.33770275115967 and batch: 850, loss is 3.244036765098572 and perplexity is 25.637003706389336
At time: 134.1021866798401 and batch: 900, loss is 3.2010073232650758 and perplexity is 24.55725483628953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349996070339255 and perplexity of 77.47815846178476
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.97317218780518 and batch: 50, loss is 3.622183475494385 and perplexity is 37.419182555873384
At time: 136.75204491615295 and batch: 100, loss is 3.4887539291381837 and perplexity is 32.7451195347983
At time: 137.51821660995483 and batch: 150, loss is 3.497625803947449 and perplexity is 33.03692264225796
At time: 138.28528809547424 and batch: 200, loss is 3.3787442779541017 and perplexity is 29.333912735279675
At time: 139.05019760131836 and batch: 250, loss is 3.520669264793396 and perplexity is 33.807046760355924
At time: 139.81609797477722 and batch: 300, loss is 3.4912331008911135 and perplexity is 32.82640102394961
At time: 140.5814814567566 and batch: 350, loss is 3.4690911388397216 and perplexity is 32.1075478766414
At time: 141.36347699165344 and batch: 400, loss is 3.3998883056640623 and perplexity is 29.960753414043722
At time: 142.135488986969 and batch: 450, loss is 3.4085879373550414 and perplexity is 30.22253799759541
At time: 142.89977979660034 and batch: 500, loss is 3.2844515228271485 and perplexity is 26.694339063340237
At time: 143.66436862945557 and batch: 550, loss is 3.329026665687561 and perplexity is 27.91116158644007
At time: 144.42848348617554 and batch: 600, loss is 3.3540460777282717 and perplexity is 28.618291540026014
At time: 145.19292783737183 and batch: 650, loss is 3.1753588771820067 and perplexity is 23.935408199049657
At time: 145.95357489585876 and batch: 700, loss is 3.1583526706695557 and perplexity is 23.531799359195357
At time: 146.71695160865784 and batch: 750, loss is 3.2493195104599 and perplexity is 25.7727958306045
At time: 147.4815158843994 and batch: 800, loss is 3.185778455734253 and perplexity is 24.18610889459424
At time: 148.24548721313477 and batch: 850, loss is 3.2339971542358397 and perplexity is 25.380905877846978
At time: 149.00901794433594 and batch: 900, loss is 3.1909613037109374 and perplexity is 24.31178722465188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3501348626123715 and perplexity of 77.48891257779137
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.9098904132843 and batch: 50, loss is 3.61930655002594 and perplexity is 37.31168506185287
At time: 151.67406964302063 and batch: 100, loss is 3.4859062004089356 and perplexity is 32.65200296544909
At time: 152.43952298164368 and batch: 150, loss is 3.4950952434539797 and perplexity is 32.95342640169485
At time: 153.2045841217041 and batch: 200, loss is 3.3763360166549683 and perplexity is 29.263354004519197
At time: 153.97021651268005 and batch: 250, loss is 3.5177550172805785 and perplexity is 33.708668077960276
At time: 154.7354691028595 and batch: 300, loss is 3.48856192111969 and perplexity is 32.738832812850916
At time: 155.50073885917664 and batch: 350, loss is 3.466282539367676 and perplexity is 32.01749715203025
At time: 156.26618719100952 and batch: 400, loss is 3.3972041034698486 and perplexity is 29.88044053021974
At time: 157.03188753128052 and batch: 450, loss is 3.4062113761901855 and perplexity is 30.15079756901129
At time: 157.79707288742065 and batch: 500, loss is 3.2818994617462156 and perplexity is 26.62630033595764
At time: 158.56130862236023 and batch: 550, loss is 3.326692233085632 and perplexity is 27.84608085372228
At time: 159.32603979110718 and batch: 600, loss is 3.351845927238464 and perplexity is 28.555396206847075
At time: 160.09124183654785 and batch: 650, loss is 3.172950382232666 and perplexity is 23.877829256421983
At time: 160.85951852798462 and batch: 700, loss is 3.156119318008423 and perplexity is 23.47930319551899
At time: 161.62501168251038 and batch: 750, loss is 3.24723292350769 and perplexity is 25.719074717480364
At time: 162.3922140598297 and batch: 800, loss is 3.183059959411621 and perplexity is 24.120448335936306
At time: 163.1571810245514 and batch: 850, loss is 3.2312191247940065 and perplexity is 25.31049482133473
At time: 163.92175459861755 and batch: 900, loss is 3.1882272720336915 and perplexity is 24.245408809941292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349867729291524 and perplexity of 77.46821547181199
Annealing...
Model not improving. Stopping early with 76.76474472727399 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
667.9454181194305


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.76474472727399}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.7300067782595917, 'rnn_dropout': 0.062097497963998216, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0494959354400635 and batch: 50, loss is 6.7582516098022465 and perplexity is 861.135278353658
At time: 1.8744628429412842 and batch: 100, loss is 6.010280418395996 and perplexity is 407.5976021155842
At time: 2.7147891521453857 and batch: 150, loss is 5.9050514507293705 and perplexity is 366.88610185940524
At time: 3.542843818664551 and batch: 200, loss is 5.744036798477173 and perplexity is 312.32265324851363
At time: 4.370330333709717 and batch: 250, loss is 5.795378093719482 and perplexity is 328.77646880278365
At time: 5.197964429855347 and batch: 300, loss is 5.710208358764649 and perplexity is 301.9339723187369
At time: 6.025382041931152 and batch: 350, loss is 5.6907375526428225 and perplexity is 296.11193826285455
At time: 6.8539323806762695 and batch: 400, loss is 5.549297685623169 and perplexity is 257.0569576977835
At time: 7.6809303760528564 and batch: 450, loss is 5.550693206787109 and perplexity is 257.41593654561996
At time: 8.50789475440979 and batch: 500, loss is 5.50611533164978 and perplexity is 246.19288932952492
At time: 9.33445405960083 and batch: 550, loss is 5.554910202026367 and perplexity is 258.50375036533796
At time: 10.159876346588135 and batch: 600, loss is 5.483787126541138 and perplexity is 240.75675935888088
At time: 10.986050128936768 and batch: 650, loss is 5.381943435668945 and perplexity is 217.44445436188752
At time: 11.81412124633789 and batch: 700, loss is 5.48665036201477 and perplexity is 241.44709047144107
At time: 12.63725471496582 and batch: 750, loss is 5.452722539901734 and perplexity is 233.3927227129548
At time: 13.459871768951416 and batch: 800, loss is 5.437041130065918 and perplexity is 229.7613427687077
At time: 14.283170223236084 and batch: 850, loss is 5.475595998764038 and perplexity is 238.79274470846553
At time: 15.106587886810303 and batch: 900, loss is 5.37791579246521 and perplexity is 216.57042699980173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.260942432978382 and perplexity of 192.6629777083291
finished 1 epochs...
Completing Train Step...
At time: 16.992596864700317 and batch: 50, loss is 5.163247547149658 and perplexity is 174.73098229964995
At time: 17.74405860900879 and batch: 100, loss is 4.984583921432495 and perplexity is 146.14275550582312
At time: 18.497114181518555 and batch: 150, loss is 4.934019756317139 and perplexity is 138.93688375696817
At time: 19.251298666000366 and batch: 200, loss is 4.7970687103271485 and perplexity is 121.1547568136609
At time: 20.011322498321533 and batch: 250, loss is 4.879072914123535 and perplexity is 131.50868750645768
At time: 20.772992610931396 and batch: 300, loss is 4.822202758789063 and perplexity is 124.23845694240036
At time: 21.536060333251953 and batch: 350, loss is 4.789954128265381 and perplexity is 120.29585035732597
At time: 22.29729986190796 and batch: 400, loss is 4.659000978469849 and perplexity is 105.53060213340412
At time: 23.059680461883545 and batch: 450, loss is 4.663680095672607 and perplexity is 106.0255492439974
At time: 23.821694374084473 and batch: 500, loss is 4.564271726608276 and perplexity is 95.99265965412509
At time: 24.597071409225464 and batch: 550, loss is 4.634825611114502 and perplexity is 103.00995263795981
At time: 25.359456062316895 and batch: 600, loss is 4.588312149047852 and perplexity is 98.32832646279313
At time: 26.122567892074585 and batch: 650, loss is 4.439661302566528 and perplexity is 84.74623348063048
At time: 26.88269281387329 and batch: 700, loss is 4.481341409683227 and perplexity is 88.35311093933225
At time: 27.644981145858765 and batch: 750, loss is 4.535007200241089 and perplexity is 93.22418653862572
At time: 28.40763783454895 and batch: 800, loss is 4.469080677032471 and perplexity is 87.27645087527034
At time: 29.170915126800537 and batch: 850, loss is 4.533331174850463 and perplexity is 93.06807129806487
At time: 29.931925773620605 and batch: 900, loss is 4.472649688720703 and perplexity is 87.58849806735556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.575818623581978 and perplexity of 97.10750109816401
finished 2 epochs...
Completing Train Step...
At time: 31.803281784057617 and batch: 50, loss is 4.488581876754761 and perplexity is 88.99515025715296
At time: 32.58448052406311 and batch: 100, loss is 4.34136734008789 and perplexity is 76.8124963715033
At time: 33.3496470451355 and batch: 150, loss is 4.339233913421631 and perplexity is 76.6487972255938
At time: 34.1145122051239 and batch: 200, loss is 4.226693906784058 and perplexity is 68.49042173355838
At time: 34.88076138496399 and batch: 250, loss is 4.373903770446777 and perplexity is 79.35280296719536
At time: 35.64627385139465 and batch: 300, loss is 4.340240497589111 and perplexity is 76.72598953509808
At time: 36.41188430786133 and batch: 350, loss is 4.322213888168335 and perplexity is 75.35527190368803
At time: 37.177191734313965 and batch: 400, loss is 4.2366635036468505 and perplexity is 69.17665869622783
At time: 37.94248175621033 and batch: 450, loss is 4.255118055343628 and perplexity is 70.46513548319656
At time: 38.70849323272705 and batch: 500, loss is 4.13852511882782 and perplexity is 62.710263023404565
At time: 39.47390341758728 and batch: 550, loss is 4.220072207450866 and perplexity is 68.03839699125847
At time: 40.238588094711304 and batch: 600, loss is 4.222793989181518 and perplexity is 68.22383490345445
At time: 41.006098985672 and batch: 650, loss is 4.063330988883973 and perplexity is 58.16774484873427
At time: 41.771552085876465 and batch: 700, loss is 4.080372171401978 and perplexity is 59.16748619899055
At time: 42.54782176017761 and batch: 750, loss is 4.179789900779724 and perplexity is 65.35212134179532
At time: 43.31172561645508 and batch: 800, loss is 4.130792722702027 and perplexity is 62.22723232759141
At time: 44.07559537887573 and batch: 850, loss is 4.203970637321472 and perplexity is 66.95164465928885
At time: 44.83868765830994 and batch: 900, loss is 4.1550622034072875 and perplexity is 63.755930260568206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432255993150685 and perplexity of 84.12097935733854
finished 3 epochs...
Completing Train Step...
At time: 46.69592499732971 and batch: 50, loss is 4.202830657958985 and perplexity is 66.87536465316987
At time: 47.469772815704346 and batch: 100, loss is 4.058105878829956 and perplexity is 57.86460464046915
At time: 48.23035526275635 and batch: 150, loss is 4.059272966384888 and perplexity is 57.93217712424854
At time: 48.991363286972046 and batch: 200, loss is 3.9545723819732665 and perplexity is 52.17337889628164
At time: 49.75384306907654 and batch: 250, loss is 4.106451172828674 and perplexity is 60.730811535504635
At time: 50.51719427108765 and batch: 300, loss is 4.0800967168807984 and perplexity is 59.15119049187637
At time: 51.28045344352722 and batch: 350, loss is 4.063358645439148 and perplexity is 58.16935359042515
At time: 52.045401096343994 and batch: 400, loss is 3.991782784461975 and perplexity is 54.15134353312806
At time: 52.80873250961304 and batch: 450, loss is 4.017726802825928 and perplexity is 55.574630037104164
At time: 53.57263970375061 and batch: 500, loss is 3.9005297422409058 and perplexity is 49.428626602676395
At time: 54.33656406402588 and batch: 550, loss is 3.981838250160217 and perplexity is 53.61550240107379
At time: 55.10032510757446 and batch: 600, loss is 3.991836886405945 and perplexity is 54.15427330533425
At time: 55.862449169158936 and batch: 650, loss is 3.8335912942886354 and perplexity is 46.2282597639721
At time: 56.62659430503845 and batch: 700, loss is 3.8419243144989013 and perplexity is 46.61509028170006
At time: 57.39064884185791 and batch: 750, loss is 3.955890693664551 and perplexity is 52.24220502883401
At time: 58.15514945983887 and batch: 800, loss is 3.909982681274414 and perplexity is 49.89808779463487
At time: 58.919310569763184 and batch: 850, loss is 3.9812981700897216 and perplexity is 53.586553554808496
At time: 59.68356990814209 and batch: 900, loss is 3.9417591524124145 and perplexity is 51.50913406416548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3844102833369005 and perplexity of 80.19091934496538
finished 4 epochs...
Completing Train Step...
At time: 61.555309772491455 and batch: 50, loss is 3.9991266345977783 and perplexity is 54.55048671465631
At time: 62.31936192512512 and batch: 100, loss is 3.8619661569595336 and perplexity is 47.5587675050835
At time: 63.08443880081177 and batch: 150, loss is 3.8650503396987914 and perplexity is 47.705673861497424
At time: 63.848551750183105 and batch: 200, loss is 3.757910580635071 and perplexity is 42.85878238996784
At time: 64.61147952079773 and batch: 250, loss is 3.9109428548812866 and perplexity is 49.946021630287966
At time: 65.37518692016602 and batch: 300, loss is 3.8922811937332153 and perplexity is 49.02258909071349
At time: 66.14022254943848 and batch: 350, loss is 3.8728984689712522 and perplexity is 48.08154717891895
At time: 66.90471267700195 and batch: 400, loss is 3.8090680837631226 and perplexity is 45.10838203900093
At time: 67.66822481155396 and batch: 450, loss is 3.83491277217865 and perplexity is 46.28938976922325
At time: 68.43189191818237 and batch: 500, loss is 3.7199526119232176 and perplexity is 41.26243871466594
At time: 69.19659876823425 and batch: 550, loss is 3.7994549226760865 and perplexity is 44.67682553064365
At time: 69.96160578727722 and batch: 600, loss is 3.819640555381775 and perplexity is 45.58781907928819
At time: 70.7261118888855 and batch: 650, loss is 3.660429711341858 and perplexity is 38.878045619547265
At time: 71.48878788948059 and batch: 700, loss is 3.6658758163452148 and perplexity is 39.09035714899388
At time: 72.25206351280212 and batch: 750, loss is 3.7841132402420046 and perplexity is 43.99663880663331
At time: 73.01634001731873 and batch: 800, loss is 3.739934892654419 and perplexity is 42.095249365827016
At time: 73.78076314926147 and batch: 850, loss is 3.814837303161621 and perplexity is 45.36937432868258
At time: 74.54406452178955 and batch: 900, loss is 3.7763377904891966 and perplexity is 43.65587167797208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378333522848887 and perplexity of 79.70509594600978
finished 5 epochs...
Completing Train Step...
At time: 76.40839743614197 and batch: 50, loss is 3.8383164691925047 and perplexity is 46.447213266269294
At time: 77.1873848438263 and batch: 100, loss is 3.7076788091659547 and perplexity is 40.75908701929897
At time: 77.95231223106384 and batch: 150, loss is 3.7104076147079468 and perplexity is 40.87046253379234
At time: 78.71631622314453 and batch: 200, loss is 3.6033108377456666 and perplexity is 36.71960606970985
At time: 79.48020076751709 and batch: 250, loss is 3.7567958211898804 and perplexity is 42.811031777657234
At time: 80.25603818893433 and batch: 300, loss is 3.742387409210205 and perplexity is 42.198615363416046
At time: 81.0193350315094 and batch: 350, loss is 3.7210180377960205 and perplexity is 41.306424211930114
At time: 81.78364038467407 and batch: 400, loss is 3.6616204595565796 and perplexity is 38.9243671561309
At time: 82.54741334915161 and batch: 450, loss is 3.685747866630554 and perplexity is 39.87493243288837
At time: 83.31180620193481 and batch: 500, loss is 3.5771961641311645 and perplexity is 35.77309820463786
At time: 84.07576251029968 and batch: 550, loss is 3.654693603515625 and perplexity is 38.6556753374772
At time: 84.84097981452942 and batch: 600, loss is 3.6750452089309693 and perplexity is 39.45044032911934
At time: 85.6069278717041 and batch: 650, loss is 3.5241192960739136 and perplexity is 33.92388355859582
At time: 86.37176871299744 and batch: 700, loss is 3.52253125667572 and perplexity is 33.87005384812817
At time: 87.13608312606812 and batch: 750, loss is 3.644936971664429 and perplexity is 38.280360028143015
At time: 87.89976096153259 and batch: 800, loss is 3.5997227811813355 and perplexity is 36.588090130522524
At time: 88.6637659072876 and batch: 850, loss is 3.6753932523727415 and perplexity is 39.46417318582764
At time: 89.42703056335449 and batch: 900, loss is 3.6372904109954836 and perplexity is 37.98876320994082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.398303724315069 and perplexity of 81.3128226520833
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.29191327095032 and batch: 50, loss is 3.728518662452698 and perplexity is 41.617413043099184
At time: 92.07117199897766 and batch: 100, loss is 3.606240096092224 and perplexity is 36.827324973496864
At time: 92.83521604537964 and batch: 150, loss is 3.606915168762207 and perplexity is 36.852194487515206
At time: 93.59952878952026 and batch: 200, loss is 3.4805522918701173 and perplexity is 32.47765426791869
At time: 94.36521315574646 and batch: 250, loss is 3.627586770057678 and perplexity is 37.62191664429438
At time: 95.12985801696777 and batch: 300, loss is 3.598500871658325 and perplexity is 36.543410097801186
At time: 95.89495205879211 and batch: 350, loss is 3.5627645874023437 and perplexity is 35.260543376040985
At time: 96.66064500808716 and batch: 400, loss is 3.495327897071838 and perplexity is 32.961094027483895
At time: 97.4263813495636 and batch: 450, loss is 3.5041273832321167 and perplexity is 33.25241457399838
At time: 98.19065856933594 and batch: 500, loss is 3.380942258834839 and perplexity is 29.398459024397834
At time: 98.96153450012207 and batch: 550, loss is 3.4415906143188475 and perplexity is 31.23660406380216
At time: 99.75497364997864 and batch: 600, loss is 3.449523515701294 and perplexity is 31.485386441723648
At time: 100.52029514312744 and batch: 650, loss is 3.276852555274963 and perplexity is 26.492258421507287
At time: 101.28521752357483 and batch: 700, loss is 3.2543032598495483 and perplexity is 25.901561587705537
At time: 102.05115270614624 and batch: 750, loss is 3.3545904302597047 and perplexity is 28.63387422032206
At time: 102.81749701499939 and batch: 800, loss is 3.2887607526779177 and perplexity is 26.809619312232442
At time: 103.58307695388794 and batch: 850, loss is 3.339168848991394 and perplexity is 28.195682094141233
At time: 104.34878468513489 and batch: 900, loss is 3.2871759510040284 and perplexity is 26.767165032362648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375849475599315 and perplexity of 79.50735042793558
finished 7 epochs...
Completing Train Step...
At time: 106.21623539924622 and batch: 50, loss is 3.609880576133728 and perplexity is 36.96163844947007
At time: 106.97770643234253 and batch: 100, loss is 3.4785836839675905 and perplexity is 32.41378139202329
At time: 107.74021935462952 and batch: 150, loss is 3.4812771320343017 and perplexity is 32.50120391000272
At time: 108.50249409675598 and batch: 200, loss is 3.3571619033813476 and perplexity is 28.707600209822374
At time: 109.26402854919434 and batch: 250, loss is 3.5034624767303466 and perplexity is 33.230312176176874
At time: 110.03339529037476 and batch: 300, loss is 3.4826604175567626 and perplexity is 32.5461934643597
At time: 110.79858326911926 and batch: 350, loss is 3.4527652788162233 and perplexity is 31.5876202254729
At time: 111.56096124649048 and batch: 400, loss is 3.3882080125808716 and perplexity is 29.612838860618147
At time: 112.32390761375427 and batch: 450, loss is 3.4023657846450805 and perplexity is 30.03507257494836
At time: 113.08582925796509 and batch: 500, loss is 3.287260971069336 and perplexity is 26.769440875226582
At time: 113.84822273254395 and batch: 550, loss is 3.351085386276245 and perplexity is 28.533686914788735
At time: 114.610267162323 and batch: 600, loss is 3.367849907875061 and perplexity is 29.016072711124746
At time: 115.37284016609192 and batch: 650, loss is 3.2018178367614745 and perplexity is 24.577166891173384
At time: 116.13447046279907 and batch: 700, loss is 3.185826549530029 and perplexity is 24.187272124347878
At time: 116.89619970321655 and batch: 750, loss is 3.294090747833252 and perplexity is 26.952895945971328
At time: 117.65774154663086 and batch: 800, loss is 3.2369265270233156 and perplexity is 25.45536501889193
At time: 118.42969608306885 and batch: 850, loss is 3.295518579483032 and perplexity is 26.991407631415075
At time: 119.191810131073 and batch: 900, loss is 3.2532616519927977 and perplexity is 25.87459636368626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.397315456442637 and perplexity of 81.23250349679702
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.05005216598511 and batch: 50, loss is 3.555712637901306 and perplexity is 35.012762500728236
At time: 121.82539176940918 and batch: 100, loss is 3.4378463983535767 and perplexity is 31.119866154518
At time: 122.58720588684082 and batch: 150, loss is 3.4439711475372317 and perplexity is 31.311052415635004
At time: 123.34892559051514 and batch: 200, loss is 3.317278628349304 and perplexity is 27.585178796202587
At time: 124.10988306999207 and batch: 250, loss is 3.460007700920105 and perplexity is 31.81722153560526
At time: 124.87231588363647 and batch: 300, loss is 3.4365471029281616 and perplexity is 31.079458511201537
At time: 125.63496828079224 and batch: 350, loss is 3.4042933559417725 and perplexity is 30.093023152729973
At time: 126.39767575263977 and batch: 400, loss is 3.3335983514785767 and perplexity is 28.039054768246032
At time: 127.15957474708557 and batch: 450, loss is 3.3441419982910157 and perplexity is 28.336252680337846
At time: 127.92154264450073 and batch: 500, loss is 3.224032735824585 and perplexity is 25.129255767702634
At time: 128.68354272842407 and batch: 550, loss is 3.2760746765136717 and perplexity is 26.471658669436096
At time: 129.44591999053955 and batch: 600, loss is 3.2933746862411497 and perplexity is 26.933602920710047
At time: 130.20793509483337 and batch: 650, loss is 3.1189707851409914 and perplexity is 22.62308364309503
At time: 130.9691789150238 and batch: 700, loss is 3.0924686908721926 and perplexity is 22.03139961163467
At time: 131.73075580596924 and batch: 750, loss is 3.1928340673446653 and perplexity is 24.357360115944285
At time: 132.49451661109924 and batch: 800, loss is 3.128323879241943 and perplexity is 22.83567210342557
At time: 133.2570686340332 and batch: 850, loss is 3.180552101135254 and perplexity is 24.060033457587895
At time: 134.01912808418274 and batch: 900, loss is 3.1392463064193725 and perplexity is 23.08646018305694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393156286788313 and perplexity of 80.89534536826957
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.87689208984375 and batch: 50, loss is 3.531079339981079 and perplexity is 34.160818861248295
At time: 136.65547943115234 and batch: 100, loss is 3.411762795448303 and perplexity is 30.318642745698885
At time: 137.42068004608154 and batch: 150, loss is 3.4202336740493773 and perplexity is 30.57655913470975
At time: 138.19638657569885 and batch: 200, loss is 3.2932358837127684 and perplexity is 26.92986472796754
At time: 138.96069478988647 and batch: 250, loss is 3.433544921875 and perplexity is 30.98629227065407
At time: 139.7256588935852 and batch: 300, loss is 3.4108469009399416 and perplexity is 30.29088678001568
At time: 140.4902880191803 and batch: 350, loss is 3.3792228841781617 and perplexity is 29.34795548869735
At time: 141.2532377243042 and batch: 400, loss is 3.307817192077637 and perplexity is 27.32541419590001
At time: 142.01770544052124 and batch: 450, loss is 3.3183724975585935 and perplexity is 27.615369883460442
At time: 142.78155207633972 and batch: 500, loss is 3.1978893995285036 and perplexity is 24.48080643061218
At time: 143.5464837551117 and batch: 550, loss is 3.2475698137283326 and perplexity is 25.727740681894023
At time: 144.31206941604614 and batch: 600, loss is 3.266969985961914 and perplexity is 26.231736275803765
At time: 145.0772807598114 and batch: 650, loss is 3.0907575988769533 and perplexity is 21.99373409388811
At time: 145.84297442436218 and batch: 700, loss is 3.0616381311416627 and perplexity is 21.362523129190276
At time: 146.60812544822693 and batch: 750, loss is 3.1617456340789794 and perplexity is 23.611777498101418
At time: 147.37304639816284 and batch: 800, loss is 3.0937791299819946 and perplexity is 22.060289344312977
At time: 148.1384608745575 and batch: 850, loss is 3.1438783693313597 and perplexity is 23.19364617351695
At time: 148.9026644229889 and batch: 900, loss is 3.107023687362671 and perplexity is 22.354411572207663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.390910945526541 and perplexity of 80.71391147815456
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.7818112373352 and batch: 50, loss is 3.521715593338013 and perplexity is 33.8424385508819
At time: 151.54370665550232 and batch: 100, loss is 3.400002417564392 and perplexity is 29.964172487625895
At time: 152.30655598640442 and batch: 150, loss is 3.409983296394348 and perplexity is 30.26473872491715
At time: 153.0693974494934 and batch: 200, loss is 3.283944935798645 and perplexity is 26.68081948214764
At time: 153.8325207233429 and batch: 250, loss is 3.424633865356445 and perplexity is 30.71139828588543
At time: 154.59666299819946 and batch: 300, loss is 3.401375665664673 and perplexity is 30.005348996881647
At time: 155.36022448539734 and batch: 350, loss is 3.36929678440094 and perplexity is 29.058085772130188
At time: 156.12408995628357 and batch: 400, loss is 3.2984305906295774 and perplexity is 27.070121463453663
At time: 156.89874267578125 and batch: 450, loss is 3.309735507965088 and perplexity is 27.37788328213381
At time: 157.66261982917786 and batch: 500, loss is 3.1887394046783446 and perplexity is 24.25782885535972
At time: 158.42386627197266 and batch: 550, loss is 3.237581377029419 and perplexity is 25.47203992401488
At time: 159.1868999004364 and batch: 600, loss is 3.2574986839294433 and perplexity is 25.984460439264836
At time: 159.94983577728271 and batch: 650, loss is 3.0817846345901487 and perplexity is 21.7972678637533
At time: 160.71405601501465 and batch: 700, loss is 3.052562689781189 and perplexity is 21.169525895419465
At time: 161.47768640518188 and batch: 750, loss is 3.153225884437561 and perplexity is 23.411465580576643
At time: 162.23986911773682 and batch: 800, loss is 3.0848748254776 and perplexity is 21.86472976366702
At time: 163.00426411628723 and batch: 850, loss is 3.134056420326233 and perplexity is 22.966954463189477
At time: 163.76679110527039 and batch: 900, loss is 3.0962891244888304 and perplexity is 22.115730098273634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391723214763484 and perplexity of 80.77949953941767
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 165.63617658615112 and batch: 50, loss is 3.518898620605469 and perplexity is 33.74723947383641
At time: 166.4139049053192 and batch: 100, loss is 3.3968083095550536 and perplexity is 29.86861637380542
At time: 167.17739605903625 and batch: 150, loss is 3.406989698410034 and perplexity is 30.174273739532854
At time: 167.9386749267578 and batch: 200, loss is 3.2814183950424196 and perplexity is 26.61349438992483
At time: 168.70021533966064 and batch: 250, loss is 3.42207106590271 and perplexity is 30.63279190035936
At time: 169.46850490570068 and batch: 300, loss is 3.3991312074661253 and perplexity is 29.938078766175725
At time: 170.2328441143036 and batch: 350, loss is 3.366376543045044 and perplexity is 28.973352928727444
At time: 171.01136016845703 and batch: 400, loss is 3.295742630958557 and perplexity is 26.997455773643683
At time: 171.78647637367249 and batch: 450, loss is 3.307543649673462 and perplexity is 27.317940558630742
At time: 172.55370926856995 and batch: 500, loss is 3.186178722381592 and perplexity is 24.195791725041417
At time: 173.31746816635132 and batch: 550, loss is 3.234803624153137 and perplexity is 25.401383070921465
At time: 174.0812647342682 and batch: 600, loss is 3.2550474071502684 and perplexity is 25.92084333818697
At time: 174.84538507461548 and batch: 650, loss is 3.0791614866256714 and perplexity is 21.740165331863565
At time: 175.6070477962494 and batch: 700, loss is 3.049971957206726 and perplexity is 21.11475229760452
At time: 176.38152241706848 and batch: 750, loss is 3.1509358263015748 and perplexity is 23.357913305663892
At time: 177.1460840702057 and batch: 800, loss is 3.082568020820618 and perplexity is 21.8143502334326
At time: 177.91078853607178 and batch: 850, loss is 3.1316600036621094 and perplexity is 22.911981965602426
At time: 178.67589163780212 and batch: 900, loss is 3.0931808948516846 and perplexity is 22.047096050981658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391673048881636 and perplexity of 80.77544726623177
Annealing...
Model not improving. Stopping early with 79.50735042793558 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
853.681316614151


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.76474472727399}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.7300067782595917, 'rnn_dropout': 0.062097497963998216, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.50735042793558}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.23085089072947085, 'rnn_dropout': 0.5525485573246425, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0385394096374512 and batch: 50, loss is 6.709274463653564 and perplexity is 819.9755015500847
At time: 1.8780491352081299 and batch: 100, loss is 5.857806138992309 and perplexity is 349.9555474012416
At time: 2.705629587173462 and batch: 150, loss is 5.62308856010437 and perplexity is 276.7428014153222
At time: 3.5319905281066895 and batch: 200, loss is 5.415257797241211 and perplexity is 224.81049373395035
At time: 4.35821795463562 and batch: 250, loss is 5.446204319000244 and perplexity is 231.8763647364536
At time: 5.185014247894287 and batch: 300, loss is 5.353396644592285 and perplexity is 211.32487569387322
At time: 6.011787414550781 and batch: 350, loss is 5.3094359874725345 and perplexity is 202.23613250317996
At time: 6.840362071990967 and batch: 400, loss is 5.156079969406128 and perplexity is 173.48306202517415
At time: 7.662685871124268 and batch: 450, loss is 5.15836501121521 and perplexity is 173.87993133386584
At time: 8.489320039749146 and batch: 500, loss is 5.089888439178467 and perplexity is 162.37174671756955
At time: 9.316672563552856 and batch: 550, loss is 5.142424831390381 and perplexity is 171.1302275689999
At time: 10.144044637680054 and batch: 600, loss is 5.061575040817261 and perplexity is 157.83892339327363
At time: 10.971296548843384 and batch: 650, loss is 4.946637020111084 and perplexity is 140.70099278139978
At time: 11.797714471817017 and batch: 700, loss is 5.035628204345703 and perplexity is 153.79617769826322
At time: 12.62471604347229 and batch: 750, loss is 5.027172584533691 and perplexity is 152.50121824901524
At time: 13.451390743255615 and batch: 800, loss is 4.975912818908691 and perplexity is 144.88101493627727
At time: 14.27872109413147 and batch: 850, loss is 5.0235745811462404 and perplexity is 151.95350427840077
At time: 15.10664677619934 and batch: 900, loss is 4.943266258239746 and perplexity is 140.2275216671619
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8953660938837755 and perplexity of 133.66893303700397
finished 1 epochs...
Completing Train Step...
At time: 17.00658631324768 and batch: 50, loss is 4.872589845657348 and perplexity is 130.6588653854646
At time: 17.760278940200806 and batch: 100, loss is 4.7194813537597655 and perplexity is 112.11009210897069
At time: 18.514935970306396 and batch: 150, loss is 4.696935167312622 and perplexity is 109.61071861483143
At time: 19.270514488220215 and batch: 200, loss is 4.5787532520294185 and perplexity is 97.39289408972435
At time: 20.042987823486328 and batch: 250, loss is 4.694058408737183 and perplexity is 109.29584816038492
At time: 20.79797601699829 and batch: 300, loss is 4.6512264919281 and perplexity is 104.71333691257374
At time: 21.5551278591156 and batch: 350, loss is 4.626090049743652 and perplexity is 102.11402180165354
At time: 22.31790018081665 and batch: 400, loss is 4.517322292327881 and perplexity is 91.59001803526206
At time: 23.081788778305054 and batch: 450, loss is 4.532783193588257 and perplexity is 93.01708570973318
At time: 23.844340562820435 and batch: 500, loss is 4.423311605453491 and perplexity is 83.37192362078366
At time: 24.606926918029785 and batch: 550, loss is 4.5048675537109375 and perplexity is 90.45636264588592
At time: 25.371853590011597 and batch: 600, loss is 4.476048316955566 and perplexity is 87.88668523658097
At time: 26.13516092300415 and batch: 650, loss is 4.329488859176636 and perplexity is 75.90547826312026
At time: 26.897114753723145 and batch: 700, loss is 4.364429206848144 and perplexity is 78.60452021263212
At time: 27.660594701766968 and batch: 750, loss is 4.435922794342041 and perplexity is 84.43000047792718
At time: 28.422746896743774 and batch: 800, loss is 4.371886796951294 and perplexity is 79.19291176917808
At time: 29.185219764709473 and batch: 850, loss is 4.439901571273804 and perplexity is 84.76659779495114
At time: 29.95909309387207 and batch: 900, loss is 4.382586193084717 and perplexity is 80.04477719942777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.526113849796661 and perplexity of 92.398786902133
finished 2 epochs...
Completing Train Step...
At time: 31.820162057876587 and batch: 50, loss is 4.412878246307373 and perplexity is 82.50659638345255
At time: 32.59022259712219 and batch: 100, loss is 4.263450412750244 and perplexity is 71.05472911799599
At time: 33.34979796409607 and batch: 150, loss is 4.263838973045349 and perplexity is 71.08234352909508
At time: 34.10909557342529 and batch: 200, loss is 4.1566224288940425 and perplexity is 63.855481528910744
At time: 34.867496728897095 and batch: 250, loss is 4.30345272064209 and perplexity is 73.95469830258004
At time: 35.62738609313965 and batch: 300, loss is 4.273384003639221 and perplexity is 71.7640750710739
At time: 36.386186838150024 and batch: 350, loss is 4.257684383392334 and perplexity is 70.6462043785582
At time: 37.14614391326904 and batch: 400, loss is 4.179288444519043 and perplexity is 65.31935832669572
At time: 37.90573835372925 and batch: 450, loss is 4.2000555372238155 and perplexity is 66.69003471746261
At time: 38.66551613807678 and batch: 500, loss is 4.080189695358277 and perplexity is 59.15669053519819
At time: 39.43615531921387 and batch: 550, loss is 4.164593706130981 and perplexity is 64.36652540545785
At time: 40.204461097717285 and batch: 600, loss is 4.16732283115387 and perplexity is 64.54242962370041
At time: 40.96450352668762 and batch: 650, loss is 4.012596259117126 and perplexity is 55.29023215033054
At time: 41.72405242919922 and batch: 700, loss is 4.031804628372193 and perplexity is 56.36253295093564
At time: 42.48186898231506 and batch: 750, loss is 4.133007454872131 and perplexity is 62.365201707100105
At time: 43.24103093147278 and batch: 800, loss is 4.078226442337036 and perplexity is 59.04066491487027
At time: 44.00283598899841 and batch: 850, loss is 4.154966373443603 and perplexity is 63.74982082482485
At time: 44.76629424095154 and batch: 900, loss is 4.106317343711853 and perplexity is 60.72268452846026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.409778333690069 and perplexity of 82.25122915685158
finished 3 epochs...
Completing Train Step...
At time: 46.63098502159119 and batch: 50, loss is 4.162200393676758 and perplexity is 64.21266039553635
At time: 47.40602517127991 and batch: 100, loss is 4.018396234512329 and perplexity is 55.611845910763705
At time: 48.16975116729736 and batch: 150, loss is 4.02143171787262 and perplexity is 55.78091121129799
At time: 48.92899799346924 and batch: 200, loss is 3.914263172149658 and perplexity is 50.11213388848438
At time: 49.688281774520874 and batch: 250, loss is 4.066557054519653 and perplexity is 58.355700827645485
At time: 50.449159383773804 and batch: 300, loss is 4.040627226829529 and perplexity is 56.86199699276316
At time: 51.209752798080444 and batch: 350, loss is 4.026800889968872 and perplexity is 56.0812139905012
At time: 51.96932554244995 and batch: 400, loss is 3.96404899597168 and perplexity is 52.670156033862156
At time: 52.72837710380554 and batch: 450, loss is 3.98294659614563 and perplexity is 53.67495987155364
At time: 53.487403869628906 and batch: 500, loss is 3.8647372007369993 and perplexity is 47.69073769498273
At time: 54.24874567985535 and batch: 550, loss is 3.951207408905029 and perplexity is 51.998111931105086
At time: 55.00874185562134 and batch: 600, loss is 3.9653702354431153 and perplexity is 52.73979191568848
At time: 55.768006324768066 and batch: 650, loss is 3.803363575935364 and perplexity is 44.851793472118494
At time: 56.52770376205444 and batch: 700, loss is 3.8167562103271484 and perplexity is 45.4565175292935
At time: 57.29919958114624 and batch: 750, loss is 3.9270476484298706 and perplexity is 50.75690401784756
At time: 58.07778453826904 and batch: 800, loss is 3.8760517263412475 and perplexity is 48.23339996155158
At time: 58.842822551727295 and batch: 850, loss is 3.957633500099182 and perplexity is 52.333332465593486
At time: 59.6010639667511 and batch: 900, loss is 3.91108259677887 and perplexity is 49.953001669817944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376986986970248 and perplexity of 79.59784240118685
finished 4 epochs...
Completing Train Step...
At time: 61.467602014541626 and batch: 50, loss is 3.979469060897827 and perplexity is 53.48862748318044
At time: 62.230384826660156 and batch: 100, loss is 3.8371512413024904 and perplexity is 46.393123197706316
At time: 62.99221229553223 and batch: 150, loss is 3.842123336791992 and perplexity is 46.624368647130716
At time: 63.75919723510742 and batch: 200, loss is 3.7360767269134523 and perplexity is 41.9331518175445
At time: 64.52696371078491 and batch: 250, loss is 3.8880897617340087 and perplexity is 48.81754425803848
At time: 65.29350090026855 and batch: 300, loss is 3.86646924495697 and perplexity is 47.773411738434675
At time: 66.0621190071106 and batch: 350, loss is 3.8539466381073 and perplexity is 47.17889430850471
At time: 66.82514691352844 and batch: 400, loss is 3.796228499412537 and perplexity is 44.5329114700896
At time: 67.58774161338806 and batch: 450, loss is 3.816436414718628 and perplexity is 45.44198305876279
At time: 68.35008096694946 and batch: 500, loss is 3.7019007635116576 and perplexity is 40.52425823272764
At time: 69.11247873306274 and batch: 550, loss is 3.7875321960449218 and perplexity is 44.14731880754374
At time: 69.8746440410614 and batch: 600, loss is 3.8059576559066772 and perplexity is 44.96829365130283
At time: 70.63641309738159 and batch: 650, loss is 3.643086256980896 and perplexity is 38.20957952121254
At time: 71.39941239356995 and batch: 700, loss is 3.6532999992370607 and perplexity is 38.60184214273396
At time: 72.16107606887817 and batch: 750, loss is 3.767325358390808 and perplexity is 43.26419373534123
At time: 72.92418098449707 and batch: 800, loss is 3.7198358821868895 and perplexity is 41.2576224421811
At time: 73.68695187568665 and batch: 850, loss is 3.7997521495819093 and perplexity is 44.69010665891466
At time: 74.45038962364197 and batch: 900, loss is 3.758093099594116 and perplexity is 42.86660564424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379926289597603 and perplexity of 79.83214872839925
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.3330569267273 and batch: 50, loss is 3.845852017402649 and perplexity is 46.798540540376564
At time: 77.10604310035706 and batch: 100, loss is 3.709188838005066 and perplexity is 40.820680908724825
At time: 77.87495040893555 and batch: 150, loss is 3.711292200088501 and perplexity is 40.90663194255281
At time: 78.64641380310059 and batch: 200, loss is 3.589915881156921 and perplexity is 36.231028086121704
At time: 79.40550446510315 and batch: 250, loss is 3.7385357141494753 and perplexity is 42.036391783488256
At time: 80.16506934165955 and batch: 300, loss is 3.7029146909713746 and perplexity is 40.56536772843585
At time: 80.92586064338684 and batch: 350, loss is 3.6796890926361083 and perplexity is 39.63406963268252
At time: 81.68610286712646 and batch: 400, loss is 3.610842900276184 and perplexity is 36.99722464647673
At time: 82.44475603103638 and batch: 450, loss is 3.614976944923401 and perplexity is 37.15048940839402
At time: 83.20639681816101 and batch: 500, loss is 3.4900911951065066 and perplexity is 32.788937760561964
At time: 83.97094941139221 and batch: 550, loss is 3.5526816177368166 and perplexity is 34.90679878178324
At time: 84.72867107391357 and batch: 600, loss is 3.5622712564468384 and perplexity is 35.24315254855512
At time: 85.48623132705688 and batch: 650, loss is 3.384073371887207 and perplexity is 29.490653183013723
At time: 86.24333214759827 and batch: 700, loss is 3.3761552667617796 and perplexity is 29.25806513440429
At time: 87.00026679039001 and batch: 750, loss is 3.4719953298568726 and perplexity is 32.200929862695524
At time: 87.75776052474976 and batch: 800, loss is 3.401831793785095 and perplexity is 30.019038402146204
At time: 88.51612329483032 and batch: 850, loss is 3.460497465133667 and perplexity is 31.832808288693734
At time: 89.27622771263123 and batch: 900, loss is 3.408518466949463 and perplexity is 30.22043849855048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342315255779109 and perplexity of 76.88534266271215
finished 6 epochs...
Completing Train Step...
At time: 91.12761688232422 and batch: 50, loss is 3.7211680793762207 and perplexity is 41.31262235806949
At time: 91.90206146240234 and batch: 100, loss is 3.581234378814697 and perplexity is 35.91784972723796
At time: 92.66334366798401 and batch: 150, loss is 3.5840842056274416 and perplexity is 36.02035537073735
At time: 93.42529320716858 and batch: 200, loss is 3.470734519958496 and perplexity is 32.16035619480355
At time: 94.18697381019592 and batch: 250, loss is 3.6207013511657715 and perplexity is 37.36376375397262
At time: 94.95007944107056 and batch: 300, loss is 3.5888501024246215 and perplexity is 36.19243439675799
At time: 95.7139184474945 and batch: 350, loss is 3.5700795030593873 and perplexity is 35.51941694169153
At time: 96.4889166355133 and batch: 400, loss is 3.5064255952835084 and perplexity is 33.32892355716851
At time: 97.25314950942993 and batch: 450, loss is 3.5155900621414187 and perplexity is 33.635769263575334
At time: 98.01708912849426 and batch: 500, loss is 3.396907982826233 and perplexity is 29.87159362487871
At time: 98.78137302398682 and batch: 550, loss is 3.464698534011841 and perplexity is 31.966821410979378
At time: 99.54496645927429 and batch: 600, loss is 3.4804288864135744 and perplexity is 32.473646595455726
At time: 100.30926251411438 and batch: 650, loss is 3.3092777919769287 and perplexity is 27.365354854683122
At time: 101.07178020477295 and batch: 700, loss is 3.3075691795349123 and perplexity is 27.318637990770945
At time: 101.83637285232544 and batch: 750, loss is 3.4107738590240477 and perplexity is 30.288674356411956
At time: 102.60039401054382 and batch: 800, loss is 3.3456172800064086 and perplexity is 28.37808748727514
At time: 103.36469888687134 and batch: 850, loss is 3.415305233001709 and perplexity is 30.426235101416758
At time: 104.12680745124817 and batch: 900, loss is 3.371656656265259 and perplexity is 29.126740107030145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357309001765839 and perplexity of 78.04682770695293
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.0120964050293 and batch: 50, loss is 3.6626570892333983 and perplexity is 38.96473823158855
At time: 106.77558064460754 and batch: 100, loss is 3.530747661590576 and perplexity is 34.14949033464772
At time: 107.53998398780823 and batch: 150, loss is 3.5411642932891847 and perplexity is 34.507072163952856
At time: 108.304607629776 and batch: 200, loss is 3.424206099510193 and perplexity is 30.69826380804887
At time: 109.07015991210938 and batch: 250, loss is 3.572698698043823 and perplexity is 35.6125711616736
At time: 109.83516597747803 and batch: 300, loss is 3.538170919418335 and perplexity is 34.40393403827525
At time: 110.59899950027466 and batch: 350, loss is 3.514221510887146 and perplexity is 33.58976847376024
At time: 111.37169575691223 and batch: 400, loss is 3.449686875343323 and perplexity is 31.490530303320174
At time: 112.13871264457703 and batch: 450, loss is 3.4493356466293337 and perplexity is 31.479471866992764
At time: 112.90335249900818 and batch: 500, loss is 3.3281417179107664 and perplexity is 27.886472591893043
At time: 113.66607403755188 and batch: 550, loss is 3.387858362197876 and perplexity is 29.602486530122583
At time: 114.42973709106445 and batch: 600, loss is 3.4008883142471316 and perplexity is 29.990729410241688
At time: 115.21378946304321 and batch: 650, loss is 3.222154197692871 and perplexity is 25.08209381415464
At time: 115.97796869277954 and batch: 700, loss is 3.212670798301697 and perplexity is 24.84535462136277
At time: 116.74276161193848 and batch: 750, loss is 3.3107036781311034 and perplexity is 27.40440256745516
At time: 117.50710606575012 and batch: 800, loss is 3.2396774244308473 and perplexity is 25.525486520839134
At time: 118.27210426330566 and batch: 850, loss is 3.3029049682617186 and perplexity is 27.191514786689158
At time: 119.03659725189209 and batch: 900, loss is 3.258654890060425 and perplexity is 26.014521206607913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351200469552654 and perplexity of 77.57152931149389
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.90603399276733 and batch: 50, loss is 3.635337872505188 and perplexity is 37.91466105477207
At time: 121.6816475391388 and batch: 100, loss is 3.5000553035736086 and perplexity is 33.11728341216979
At time: 122.44454264640808 and batch: 150, loss is 3.5143271255493165 and perplexity is 33.59331623315438
At time: 123.20795702934265 and batch: 200, loss is 3.3997850799560547 and perplexity is 29.957660853678732
At time: 123.97134208679199 and batch: 250, loss is 3.547355513572693 and perplexity is 34.72137576491679
At time: 124.73502373695374 and batch: 300, loss is 3.512230439186096 and perplexity is 33.522955373216824
At time: 125.49869585037231 and batch: 350, loss is 3.489492859840393 and perplexity is 32.76932485089431
At time: 126.26070046424866 and batch: 400, loss is 3.425738253593445 and perplexity is 30.745334328728198
At time: 127.0236930847168 and batch: 450, loss is 3.4243614292144775 and perplexity is 30.703032530640776
At time: 127.78870558738708 and batch: 500, loss is 3.3033419609069825 and perplexity is 27.20339987532355
At time: 128.55378127098083 and batch: 550, loss is 3.3610906314849855 and perplexity is 28.820606405509867
At time: 129.31933569908142 and batch: 600, loss is 3.3745338249206545 and perplexity is 29.21066332343007
At time: 130.08520889282227 and batch: 650, loss is 3.195049104690552 and perplexity is 24.41137237575084
At time: 130.85091137886047 and batch: 700, loss is 3.1829565572738647 and perplexity is 24.11795435895823
At time: 131.6156153678894 and batch: 750, loss is 3.278848810195923 and perplexity is 26.545196544162465
At time: 132.3799388408661 and batch: 800, loss is 3.2070984745025637 and perplexity is 24.707293279002787
At time: 133.1436586380005 and batch: 850, loss is 3.270669045448303 and perplexity is 26.328948715170462
At time: 133.90882539749146 and batch: 900, loss is 3.2258690881729124 and perplexity is 25.17544433180273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348461412403681 and perplexity of 77.35934718172307
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.77475380897522 and batch: 50, loss is 3.6263188028335573 and perplexity is 37.57424351744914
At time: 136.54924154281616 and batch: 100, loss is 3.4899387836456297 and perplexity is 32.7839407314692
At time: 137.31052017211914 and batch: 150, loss is 3.5040810012817385 and perplexity is 33.25087229792282
At time: 138.07185578346252 and batch: 200, loss is 3.3897516441345217 and perplexity is 29.65858547194917
At time: 138.83102416992188 and batch: 250, loss is 3.5378048324584963 and perplexity is 34.3913415117772
At time: 139.59237718582153 and batch: 300, loss is 3.501734962463379 and perplexity is 33.172955893958715
At time: 140.35407876968384 and batch: 350, loss is 3.4801462936401366 and perplexity is 32.46447107412975
At time: 141.11507606506348 and batch: 400, loss is 3.4161624813079836 and perplexity is 30.452329122853612
At time: 141.87670969963074 and batch: 450, loss is 3.4155209732055662 and perplexity is 30.432799971707908
At time: 142.63847494125366 and batch: 500, loss is 3.294567036628723 and perplexity is 26.965736365947233
At time: 143.4003028869629 and batch: 550, loss is 3.3528098011016847 and perplexity is 28.58293327590338
At time: 144.16077518463135 and batch: 600, loss is 3.366382417678833 and perplexity is 28.973523137065502
At time: 144.9241647720337 and batch: 650, loss is 3.187082290649414 and perplexity is 24.21766415478761
At time: 145.6860761642456 and batch: 700, loss is 3.174829363822937 and perplexity is 23.922737435618785
At time: 146.44835591316223 and batch: 750, loss is 3.269585380554199 and perplexity is 26.300432411598337
At time: 147.20975923538208 and batch: 800, loss is 3.197595796585083 and perplexity is 24.473619848839245
At time: 147.97042727470398 and batch: 850, loss is 3.2610515117645265 and perplexity is 26.07694294352009
At time: 148.73101210594177 and batch: 900, loss is 3.2163456535339354 and perplexity is 24.93682567128343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3489141594873715 and perplexity of 77.39437933030891
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.59703469276428 and batch: 50, loss is 3.6236825609207153 and perplexity is 37.47531917338791
At time: 151.3576421737671 and batch: 100, loss is 3.487386803627014 and perplexity is 32.700383433416484
At time: 152.11961579322815 and batch: 150, loss is 3.501234450340271 and perplexity is 33.15635658179947
At time: 152.88325905799866 and batch: 200, loss is 3.3870127630233764 and perplexity is 29.57746527241793
At time: 153.65646886825562 and batch: 250, loss is 3.535289936065674 and perplexity is 34.30495951747622
At time: 154.41878747940063 and batch: 300, loss is 3.499287419319153 and perplexity is 33.0918629329292
At time: 155.18157196044922 and batch: 350, loss is 3.4773072671890257 and perplexity is 32.37243429127804
At time: 155.94066667556763 and batch: 400, loss is 3.4131152486801146 and perplexity is 30.35967503293473
At time: 156.70120334625244 and batch: 450, loss is 3.412956881523132 and perplexity is 30.354867438205456
At time: 157.46358609199524 and batch: 500, loss is 3.291961307525635 and perplexity is 26.895562428753216
At time: 158.2257740497589 and batch: 550, loss is 3.350355134010315 and perplexity is 28.512857731468653
At time: 158.98638153076172 and batch: 600, loss is 3.364196610450745 and perplexity is 28.91026176453925
At time: 159.74884557724 and batch: 650, loss is 3.1848646354675294 and perplexity is 24.164017233554304
At time: 160.51147866249084 and batch: 700, loss is 3.1725528860092163 and perplexity is 23.86833979560574
At time: 161.2747871875763 and batch: 750, loss is 3.267252678871155 and perplexity is 26.239152849903075
At time: 162.03776717185974 and batch: 800, loss is 3.1951936149597167 and perplexity is 24.414900324649828
At time: 162.80161046981812 and batch: 850, loss is 3.258489170074463 and perplexity is 26.010210437718893
At time: 163.56214809417725 and batch: 900, loss is 3.2135194301605225 and perplexity is 24.86644812988403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348842673105736 and perplexity of 77.3888468839213
Annealing...
Model not improving. Stopping early with 76.88534266271215 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
1023.8768951892853


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.76474472727399}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.7300067782595917, 'rnn_dropout': 0.062097497963998216, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.50735042793558}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.23085089072947085, 'rnn_dropout': 0.5525485573246425, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.88534266271215}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.0, 'rnn_dropout': 0.4560174823290351, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0278620719909668 and batch: 50, loss is 6.701737661361694 and perplexity is 813.8187386069769
At time: 1.8452396392822266 and batch: 100, loss is 5.793947849273682 and perplexity is 328.3065741965452
At time: 2.6609113216400146 and batch: 150, loss is 5.519308433532715 and perplexity is 249.46245765090322
At time: 3.4775469303131104 and batch: 200, loss is 5.284392423629761 and perplexity is 197.23431212890438
At time: 4.2957539558410645 and batch: 250, loss is 5.303435001373291 and perplexity is 201.02615045712022
At time: 5.1131181716918945 and batch: 300, loss is 5.205029544830322 and perplexity is 182.18625535126796
At time: 5.932056188583374 and batch: 350, loss is 5.146924057006836 and perplexity is 171.90191577051502
At time: 6.750300884246826 and batch: 400, loss is 4.986470785140991 and perplexity is 146.4187672838608
At time: 7.57691764831543 and batch: 450, loss is 4.984084386825561 and perplexity is 146.0697703727128
At time: 8.412835121154785 and batch: 500, loss is 4.896281471252442 and perplexity is 133.79134657194183
At time: 9.229584455490112 and batch: 550, loss is 4.956355590820312 and perplexity is 142.07507155426282
At time: 10.048336267471313 and batch: 600, loss is 4.876337480545044 and perplexity is 131.14944579171896
At time: 10.867136716842651 and batch: 650, loss is 4.747932968139648 and perplexity is 115.34561488237267
At time: 11.685320854187012 and batch: 700, loss is 4.814899911880493 and perplexity is 123.33446737748811
At time: 12.504131555557251 and batch: 750, loss is 4.825514717102051 and perplexity is 124.65061167556489
At time: 13.320587396621704 and batch: 800, loss is 4.758749876022339 and perplexity is 116.60007020557455
At time: 14.137747764587402 and batch: 850, loss is 4.813760433197022 and perplexity is 123.19401042002617
At time: 14.955183267593384 and batch: 900, loss is 4.736282129287719 and perplexity is 114.0095400209807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.763583248608733 and perplexity of 117.16500596116354
finished 1 epochs...
Completing Train Step...
At time: 16.851738452911377 and batch: 50, loss is 4.758262319564819 and perplexity is 116.54323494472366
At time: 17.60665798187256 and batch: 100, loss is 4.607030277252197 and perplexity is 100.1861822306994
At time: 18.360594987869263 and batch: 150, loss is 4.583723545074463 and perplexity is 97.87817029732105
At time: 19.116894483566284 and batch: 200, loss is 4.4697580909729 and perplexity is 87.33559318941681
At time: 19.871736526489258 and batch: 250, loss is 4.59791111946106 and perplexity is 99.2767216860318
At time: 20.63440728187561 and batch: 300, loss is 4.554058055877686 and perplexity is 95.01721216539646
At time: 21.39776635169983 and batch: 350, loss is 4.535348358154297 and perplexity is 93.25599613330473
At time: 22.158729553222656 and batch: 400, loss is 4.432036762237549 and perplexity is 84.10253945959178
At time: 22.921727180480957 and batch: 450, loss is 4.448415994644165 and perplexity is 85.49141782972592
At time: 23.6851966381073 and batch: 500, loss is 4.331063380241394 and perplexity is 76.02508717624234
At time: 24.447046279907227 and batch: 550, loss is 4.419185342788697 and perplexity is 83.0286179368629
At time: 25.20932626724243 and batch: 600, loss is 4.398965473175049 and perplexity is 81.36664912761523
At time: 25.97356939315796 and batch: 650, loss is 4.251806445121765 and perplexity is 70.23216838127139
At time: 26.735626220703125 and batch: 700, loss is 4.277115893363953 and perplexity is 72.0323910368313
At time: 27.510637283325195 and batch: 750, loss is 4.362268223762512 and perplexity is 78.43484057744068
At time: 28.274031162261963 and batch: 800, loss is 4.296161694526672 and perplexity is 73.4174535785049
At time: 29.037341594696045 and batch: 850, loss is 4.369886255264282 and perplexity is 79.03464141387511
At time: 29.799753427505493 and batch: 900, loss is 4.311821227073669 and perplexity is 74.57618550380107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493655688142123 and perplexity of 89.44784232988606
finished 2 epochs...
Completing Train Step...
At time: 31.64370346069336 and batch: 50, loss is 4.374166612625122 and perplexity is 79.3736629721095
At time: 32.42053961753845 and batch: 100, loss is 4.227822723388672 and perplexity is 68.5677785114618
At time: 33.18401646614075 and batch: 150, loss is 4.220609421730042 and perplexity is 68.07495800932556
At time: 33.9472177028656 and batch: 200, loss is 4.109405789375305 and perplexity is 60.910513140095695
At time: 34.71168255805969 and batch: 250, loss is 4.263440566062927 and perplexity is 71.05402946774059
At time: 35.47482442855835 and batch: 300, loss is 4.225590257644654 and perplexity is 68.41487403529915
At time: 36.2389018535614 and batch: 350, loss is 4.2120888137817385 and perplexity is 67.49738212423597
At time: 37.001344203948975 and batch: 400, loss is 4.133847861289978 and perplexity is 62.41763585277524
At time: 37.762608766555786 and batch: 450, loss is 4.1571293067932125 and perplexity is 63.88785666566609
At time: 38.526411056518555 and batch: 500, loss is 4.026064100265503 and perplexity is 56.03990914784055
At time: 39.2900185585022 and batch: 550, loss is 4.119226508140564 and perplexity is 61.511645102045804
At time: 40.05305552482605 and batch: 600, loss is 4.123259582519531 and perplexity is 61.7602270797714
At time: 40.81742572784424 and batch: 650, loss is 3.9692869138717652 and perplexity is 52.94676177362234
At time: 41.5801739692688 and batch: 700, loss is 3.9826507806777953 and perplexity is 53.65908433641843
At time: 42.343538761138916 and batch: 750, loss is 4.08743043422699 and perplexity is 59.58658317566641
At time: 43.105098247528076 and batch: 800, loss is 4.033189349174499 and perplexity is 56.44063338395114
At time: 43.869086027145386 and batch: 850, loss is 4.108937497138977 and perplexity is 60.881995897404245
At time: 44.634217977523804 and batch: 900, loss is 4.062636594772339 and perplexity is 58.127367529733036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39850856833262 and perplexity of 81.32948080345685
finished 3 epochs...
Completing Train Step...
At time: 46.48576903343201 and batch: 50, loss is 4.138894872665405 and perplexity is 62.7334546711496
At time: 47.26409316062927 and batch: 100, loss is 3.997593278884888 and perplexity is 54.46690551044612
At time: 48.027344703674316 and batch: 150, loss is 3.995721712112427 and perplexity is 54.36506239271925
At time: 48.79153394699097 and batch: 200, loss is 3.886330580711365 and perplexity is 48.73174085459528
At time: 49.55586051940918 and batch: 250, loss is 4.0410679340362545 and perplexity is 56.88706200738727
At time: 50.320430278778076 and batch: 300, loss is 4.009890308380127 and perplexity is 55.14082174565917
At time: 51.08606553077698 and batch: 350, loss is 3.9970915365219115 and perplexity is 54.43958401132124
At time: 51.85046029090881 and batch: 400, loss is 3.9300067615509033 and perplexity is 50.90732188048597
At time: 52.61457061767578 and batch: 450, loss is 3.952320818901062 and perplexity is 52.05603939122628
At time: 53.379103660583496 and batch: 500, loss is 3.825729823112488 and perplexity is 45.86626241255294
At time: 54.14576029777527 and batch: 550, loss is 3.9150132083892824 and perplexity is 50.14973390387005
At time: 54.908512353897095 and batch: 600, loss is 3.9310934019088744 and perplexity is 50.9626698972057
At time: 55.671804428100586 and batch: 650, loss is 3.7760944890975954 and perplexity is 43.64525143565352
At time: 56.436628580093384 and batch: 700, loss is 3.782301983833313 and perplexity is 43.91702173786323
At time: 57.201277017593384 and batch: 750, loss is 3.896607608795166 and perplexity is 49.23514062005787
At time: 57.96500205993652 and batch: 800, loss is 3.8456801319122316 and perplexity is 46.79049724156795
At time: 58.72857046127319 and batch: 850, loss is 3.919079775810242 and perplexity is 50.35408640292665
At time: 59.49291753768921 and batch: 900, loss is 3.8795658826828 and perplexity is 48.40319784331229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3722337696650255 and perplexity of 79.22039431625416
finished 4 epochs...
Completing Train Step...
At time: 61.3568274974823 and batch: 50, loss is 3.9621919631958007 and perplexity is 52.57243658998436
At time: 62.12162137031555 and batch: 100, loss is 3.8239705324172975 and perplexity is 45.78564126268101
At time: 62.88634276390076 and batch: 150, loss is 3.827823615074158 and perplexity is 45.96239743234899
At time: 63.65137815475464 and batch: 200, loss is 3.722509460449219 and perplexity is 41.368075511366904
At time: 64.41673064231873 and batch: 250, loss is 3.8711667013168336 and perplexity is 47.99835316788048
At time: 65.1805911064148 and batch: 300, loss is 3.843991212844849 and perplexity is 46.71153857477207
At time: 65.95619225502014 and batch: 350, loss is 3.833978977203369 and perplexity is 46.24618514491791
At time: 66.72150492668152 and batch: 400, loss is 3.7715337610244752 and perplexity is 43.44665053876086
At time: 67.48680853843689 and batch: 450, loss is 3.7935750532150267 and perplexity is 44.41490242005739
At time: 68.25010418891907 and batch: 500, loss is 3.6718464994430544 and perplexity is 39.32445143968796
At time: 69.01578545570374 and batch: 550, loss is 3.7571734523773195 and perplexity is 42.82720161134751
At time: 69.78133511543274 and batch: 600, loss is 3.778575382232666 and perplexity is 43.75366506602871
At time: 70.54578280448914 and batch: 650, loss is 3.625633187294006 and perplexity is 37.54849086142607
At time: 71.31191897392273 and batch: 700, loss is 3.6300662183761596 and perplexity is 37.71531398134333
At time: 72.07691836357117 and batch: 750, loss is 3.746074151992798 and perplexity is 42.35447794011615
At time: 72.84146356582642 and batch: 800, loss is 3.695452342033386 and perplexity is 40.26378147012178
At time: 73.60603737831116 and batch: 850, loss is 3.7703436040878295 and perplexity is 43.394972964559834
At time: 74.37144351005554 and batch: 900, loss is 3.732622575759888 and perplexity is 41.78855824062067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3819337609696065 and perplexity of 79.9925704486683
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.22886514663696 and batch: 50, loss is 3.8413243389129637 and perplexity is 46.587130753950504
At time: 77.00453114509583 and batch: 100, loss is 3.7031811332702635 and perplexity is 40.57617749829471
At time: 77.76656603813171 and batch: 150, loss is 3.7104532623291018 and perplexity is 40.872328215764156
At time: 78.52832055091858 and batch: 200, loss is 3.5858944511413573 and perplexity is 36.08562011223698
At time: 79.28995895385742 and batch: 250, loss is 3.7299687576293947 and perplexity is 41.67780603022651
At time: 80.0514166355133 and batch: 300, loss is 3.6901590394973756 and perplexity is 40.051216176084544
At time: 80.81197786331177 and batch: 350, loss is 3.6603618383407595 and perplexity is 38.875406939462806
At time: 81.59279274940491 and batch: 400, loss is 3.5940703392028808 and perplexity is 36.38186147080918
At time: 82.35458779335022 and batch: 450, loss is 3.5996345233917237 and perplexity is 36.58486108905762
At time: 83.1158435344696 and batch: 500, loss is 3.4645436286926268 and perplexity is 31.961869963817104
At time: 83.87766003608704 and batch: 550, loss is 3.5276381015777587 and perplexity is 34.04346537596376
At time: 84.63960909843445 and batch: 600, loss is 3.5433604097366334 and perplexity is 34.582936986186475
At time: 85.41401362419128 and batch: 650, loss is 3.372687940597534 and perplexity is 29.156793551912592
At time: 86.17255592346191 and batch: 700, loss is 3.353762216567993 and perplexity is 28.610169071465485
At time: 86.93079948425293 and batch: 750, loss is 3.4544794940948487 and perplexity is 31.641814643857813
At time: 87.68796634674072 and batch: 800, loss is 3.3803728580474854 and perplexity is 29.381724283521816
At time: 88.44592547416687 and batch: 850, loss is 3.4295583248138426 and perplexity is 30.863008313960275
At time: 89.20323944091797 and batch: 900, loss is 3.3834207105636596 and perplexity is 29.47141205392841
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344678486863228 and perplexity of 77.06725536061386
finished 6 epochs...
Completing Train Step...
At time: 91.0341968536377 and batch: 50, loss is 3.715762882232666 and perplexity is 41.08992190184583
At time: 91.82229614257812 and batch: 100, loss is 3.5743157958984373 and perplexity is 35.67020676274007
At time: 92.5816593170166 and batch: 150, loss is 3.5833971214294436 and perplexity is 35.99561485413815
At time: 93.34021210670471 and batch: 200, loss is 3.46231586933136 and perplexity is 31.89074586190876
At time: 94.09657526016235 and batch: 250, loss is 3.6072717094421387 and perplexity is 36.86533613662179
At time: 94.8650393486023 and batch: 300, loss is 3.5747365713119508 and perplexity is 35.68521906692352
At time: 95.63494777679443 and batch: 350, loss is 3.5503632164001466 and perplexity is 34.82596455212862
At time: 96.40038824081421 and batch: 400, loss is 3.487916498184204 and perplexity is 32.71770923682596
At time: 97.16236615180969 and batch: 450, loss is 3.5008938026428225 and perplexity is 33.14506386880164
At time: 97.92356657981873 and batch: 500, loss is 3.370230164527893 and perplexity is 29.085220673536057
At time: 98.6831955909729 and batch: 550, loss is 3.437500991821289 and perplexity is 31.109119005638878
At time: 99.44224762916565 and batch: 600, loss is 3.461474356651306 and perplexity is 31.863920683336552
At time: 100.20338797569275 and batch: 650, loss is 3.297771120071411 and perplexity is 27.052275400465863
At time: 100.96528267860413 and batch: 700, loss is 3.2843401288986205 and perplexity is 26.691365641656148
At time: 101.72726583480835 and batch: 750, loss is 3.393658146858215 and perplexity is 29.77467341818279
At time: 102.48945665359497 and batch: 800, loss is 3.325901165008545 and perplexity is 27.82406141867128
At time: 103.24979090690613 and batch: 850, loss is 3.384436845779419 and perplexity is 29.501374213799384
At time: 104.03136587142944 and batch: 900, loss is 3.3482169246673585 and perplexity is 28.451956405767746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360765849074272 and perplexity of 78.31709053277243
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 105.88621163368225 and batch: 50, loss is 3.6596394777297974 and perplexity is 38.847335017000816
At time: 106.65180444717407 and batch: 100, loss is 3.5272400999069213 and perplexity is 34.02991871583854
At time: 107.4156141281128 and batch: 150, loss is 3.5436315870285036 and perplexity is 34.59231636506311
At time: 108.18051171302795 and batch: 200, loss is 3.415714449882507 and perplexity is 30.438688578352096
At time: 108.9423611164093 and batch: 250, loss is 3.563074712753296 and perplexity is 35.271480260245106
At time: 109.70713448524475 and batch: 300, loss is 3.5249619102478027 and perplexity is 33.952480350054586
At time: 110.4705171585083 and batch: 350, loss is 3.495502624511719 and perplexity is 32.966853738234086
At time: 111.2395339012146 and batch: 400, loss is 3.431865005493164 and perplexity is 30.93428158968543
At time: 112.0095567703247 and batch: 450, loss is 3.4384956550598145 and perplexity is 31.140077496780815
At time: 112.77346277236938 and batch: 500, loss is 3.3019114685058595 and perplexity is 27.1645134385245
At time: 113.5414514541626 and batch: 550, loss is 3.359940934181213 and perplexity is 28.787490472329644
At time: 114.30883312225342 and batch: 600, loss is 3.3836537313461306 and perplexity is 29.478280305617357
At time: 115.0769157409668 and batch: 650, loss is 3.213952031135559 and perplexity is 24.87720770672463
At time: 115.84371066093445 and batch: 700, loss is 3.1928480672836304 and perplexity is 24.35770111988627
At time: 116.60771799087524 and batch: 750, loss is 3.292498006820679 and perplexity is 26.910001132422995
At time: 117.3694269657135 and batch: 800, loss is 3.2176404190063477 and perplexity is 24.969133923445348
At time: 118.1326653957367 and batch: 850, loss is 3.270884141921997 and perplexity is 26.334612588312893
At time: 118.89537906646729 and batch: 900, loss is 3.2353227853775026 and perplexity is 25.414573907846393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353801570526541 and perplexity of 77.77356333346872
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.75120997428894 and batch: 50, loss is 3.6337680292129515 and perplexity is 37.85518767259674
At time: 121.52630543708801 and batch: 100, loss is 3.498700723648071 and perplexity is 33.07245377438981
At time: 122.28617978096008 and batch: 150, loss is 3.5168240451812744 and perplexity is 33.677300851746736
At time: 123.05822682380676 and batch: 200, loss is 3.388041100502014 and perplexity is 29.607896532601647
At time: 123.81859159469604 and batch: 250, loss is 3.534881691932678 and perplexity is 34.29095757731998
At time: 124.5788426399231 and batch: 300, loss is 3.4992413234710695 and perplexity is 33.09033757059935
At time: 125.34159970283508 and batch: 350, loss is 3.4690292167663572 and perplexity is 32.10555977226064
At time: 126.1034255027771 and batch: 400, loss is 3.407052869796753 and perplexity is 30.17617995045658
At time: 126.86467742919922 and batch: 450, loss is 3.413378610610962 and perplexity is 30.36767166852978
At time: 127.6253011226654 and batch: 500, loss is 3.2766269874572753 and perplexity is 26.486283294513036
At time: 128.3860867023468 and batch: 550, loss is 3.3323580503463743 and perplexity is 28.004299454853843
At time: 129.14728474617004 and batch: 600, loss is 3.3568494272232057 and perplexity is 28.69863117057778
At time: 129.91273045539856 and batch: 650, loss is 3.1861029195785524 and perplexity is 24.193957685720438
At time: 130.67253828048706 and batch: 700, loss is 3.1639124393463134 and perplexity is 23.662995091191593
At time: 131.43397903442383 and batch: 750, loss is 3.2613910102844237 and perplexity is 26.085797530027317
At time: 132.19402050971985 and batch: 800, loss is 3.1852434349060057 and perplexity is 24.17317228356814
At time: 132.95457100868225 and batch: 850, loss is 3.236256957054138 and perplexity is 25.438326575772308
At time: 133.71512079238892 and batch: 900, loss is 3.2004705476760864 and perplexity is 24.544076638544663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.352260641855736 and perplexity of 77.65381210764147
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.56186318397522 and batch: 50, loss is 3.6259613609313965 and perplexity is 37.56081530841927
At time: 136.33554530143738 and batch: 100, loss is 3.4893643188476564 and perplexity is 32.76511292005535
At time: 137.09757542610168 and batch: 150, loss is 3.506998243331909 and perplexity is 33.34801476594883
At time: 137.85929584503174 and batch: 200, loss is 3.377639055252075 and perplexity is 29.301510138326616
At time: 138.62092852592468 and batch: 250, loss is 3.5243969488143922 and perplexity is 33.933303925568374
At time: 139.38218760490417 and batch: 300, loss is 3.4886998462677004 and perplexity is 32.74334863262771
At time: 140.14404249191284 and batch: 350, loss is 3.4594054889678953 and perplexity is 31.798066592757067
At time: 140.90492868423462 and batch: 400, loss is 3.398693532943726 and perplexity is 29.924978498885917
At time: 141.66660833358765 and batch: 450, loss is 3.4043678522109984 and perplexity is 30.09526505419019
At time: 142.4469428062439 and batch: 500, loss is 3.2674519777297975 and perplexity is 26.244382804262465
At time: 143.20721578598022 and batch: 550, loss is 3.3234209871292113 and perplexity is 27.75513830315117
At time: 143.9692211151123 and batch: 600, loss is 3.348364109992981 and perplexity is 28.45614442443581
At time: 144.73173451423645 and batch: 650, loss is 3.177571277618408 and perplexity is 23.988421528327333
At time: 145.49396014213562 and batch: 700, loss is 3.1554704904556274 and perplexity is 23.464074117742186
At time: 146.256361246109 and batch: 750, loss is 3.2522766160964967 and perplexity is 25.849121506350038
At time: 147.01749539375305 and batch: 800, loss is 3.175823221206665 and perplexity is 23.94652504364507
At time: 147.77940154075623 and batch: 850, loss is 3.2262343883514406 and perplexity is 25.184642606074995
At time: 148.54078912734985 and batch: 900, loss is 3.190206379890442 and perplexity is 24.293440603379214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.352230124277611 and perplexity of 77.65144233752378
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.3913116455078 and batch: 50, loss is 3.623639316558838 and perplexity is 37.473698612164426
At time: 151.15376496315002 and batch: 100, loss is 3.487014570236206 and perplexity is 32.688213523974184
At time: 151.91420102119446 and batch: 150, loss is 3.5045256280899046 and perplexity is 33.26565981436088
At time: 152.6742503643036 and batch: 200, loss is 3.375038323402405 and perplexity is 29.225403776683162
At time: 153.43648266792297 and batch: 250, loss is 3.5213697814941405 and perplexity is 33.83073745811032
At time: 154.19951105117798 and batch: 300, loss is 3.485915126800537 and perplexity is 32.652294431315006
At time: 154.96326684951782 and batch: 350, loss is 3.4564870738983156 and perplexity is 31.7054019187896
At time: 155.7243630886078 and batch: 400, loss is 3.3960207319259643 and perplexity is 29.845101780736755
At time: 156.4869260787964 and batch: 450, loss is 3.401971025466919 and perplexity is 30.023218294329627
At time: 157.24989700317383 and batch: 500, loss is 3.2648010730743406 and perplexity is 26.17490357979444
At time: 158.01322650909424 and batch: 550, loss is 3.32074631690979 and perplexity is 27.68100165104054
At time: 158.77902913093567 and batch: 600, loss is 3.346248779296875 and perplexity is 28.39601388904741
At time: 159.54143476486206 and batch: 650, loss is 3.1750487089157104 and perplexity is 23.92798534621167
At time: 160.30332040786743 and batch: 700, loss is 3.1532156372070315 and perplexity is 23.41122567912097
At time: 161.08493280410767 and batch: 750, loss is 3.249938726425171 and perplexity is 25.788759699283574
At time: 161.84710502624512 and batch: 800, loss is 3.173230977058411 and perplexity is 23.88453019184129
At time: 162.6102590560913 and batch: 850, loss is 3.2236000728607177 and perplexity is 25.118385621146885
At time: 163.37392783164978 and batch: 900, loss is 3.1874898433685304 and perplexity is 24.22753614120535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351910316780822 and perplexity of 77.62661279467638
Annealing...
Model not improving. Stopping early with 77.06725536061386 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f7114522b70>
ELAPSED
1194.0965509414673


RESULTS SO FAR:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.76474472727399}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.7300067782595917, 'rnn_dropout': 0.062097497963998216, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.50735042793558}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.23085089072947085, 'rnn_dropout': 0.5525485573246425, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.88534266271215}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.0, 'rnn_dropout': 0.4560174823290351, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.06725536061386}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.9878062472067416, 'rnn_dropout': 0.44724420635559403, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.82771014288804}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.22888135087778383, 'rnn_dropout': 0.7943454495970339, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.92408217120206}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.1654426330406763, 'rnn_dropout': 0.33665119191917114, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.76474472727399}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.7300067782595917, 'rnn_dropout': 0.062097497963998216, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -79.50735042793558}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.23085089072947085, 'rnn_dropout': 0.5525485573246425, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -76.88534266271215}, {'params': {'seq_len': 35, 'tie_weights': 'TRUE', 'tune_wordvecs': 'FALSE', 'wordvec_source': 'None', 'data': 'ptb', 'dropout': 0.0, 'rnn_dropout': 0.4560174823290351, 'wordvec_dim': 300, 'num_layers': 2, 'batch_size': 32}, 'best_accuracy': -77.06725536061386}]
Exception ignored in: <bound method DropoutDescriptor.__del__ of <torch.backends.cudnn.DropoutDescriptor object at 0x7f710be6a780>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 215, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroyDropoutDescriptor'
Exception ignored in: <bound method CuDNNHandle.__del__ of <torch.backends.cudnn.CuDNNHandle object at 0x7f710f4d90f0>>
Traceback (most recent call last):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/backends/cudnn/__init__.py", line 91, in __del__
AttributeError: 'NoneType' object has no attribute 'cudnnDestroy'
