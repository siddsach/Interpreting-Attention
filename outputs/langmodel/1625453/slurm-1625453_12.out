Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.6553277969360352 and batch: 50, loss is 7.056867141723632 and perplexity is 1160.8028325338178
At time: 2.781320095062256 and batch: 100, loss is 6.381512956619263 and perplexity is 590.8209182126847
At time: 3.9024722576141357 and batch: 150, loss is 5.977551765441895 and perplexity is 394.4734215193991
At time: 5.024467945098877 and batch: 200, loss is 5.766663761138916 and perplexity is 319.4701240960734
At time: 6.148190259933472 and batch: 250, loss is 5.751681823730468 and perplexity is 314.7195181922853
At time: 7.268566131591797 and batch: 300, loss is 5.604581651687622 and perplexity is 271.6682497041516
At time: 8.385377168655396 and batch: 350, loss is 5.5452585124969485 and perplexity is 256.02075425369253
At time: 9.507503032684326 and batch: 400, loss is 5.3732844161987305 and perplexity is 215.569726962097
At time: 10.630118370056152 and batch: 450, loss is 5.347484617233277 and perplexity is 210.07920310059177
At time: 11.751719236373901 and batch: 500, loss is 5.271812019348144 and perplexity is 194.76856729786041
At time: 12.872532844543457 and batch: 550, loss is 5.31041374206543 and perplexity is 202.4339665113786
At time: 13.996194839477539 and batch: 600, loss is 5.198650302886963 and perplexity is 181.0277442895777
At time: 15.118244171142578 and batch: 650, loss is 5.083893537521362 and perplexity is 161.40125596892872
At time: 16.241196632385254 and batch: 700, loss is 5.161840076446533 and perplexity is 174.48522654871437
At time: 17.36485981941223 and batch: 750, loss is 5.1285621929168705 and perplexity is 168.77427865532425
At time: 18.487525939941406 and batch: 800, loss is 5.092324333190918 and perplexity is 162.76774919741553
At time: 19.609110355377197 and batch: 850, loss is 5.1212207698822025 and perplexity is 167.53977233952125
At time: 20.734306573867798 and batch: 900, loss is 5.016647024154663 and perplexity is 150.9044755160397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9637819055008565 and perplexity of 143.13409319075302
finished 1 epochs...
Completing Train Step...
At time: 23.10100030899048 and batch: 50, loss is 4.92509575843811 and perplexity is 137.70252719046243
At time: 24.009454250335693 and batch: 100, loss is 4.800579261779785 and perplexity is 121.5808242494446
At time: 24.903406143188477 and batch: 150, loss is 4.783527135848999 and perplexity is 119.52518900943429
At time: 25.80061960220337 and batch: 200, loss is 4.666656999588013 and perplexity is 106.34164738012717
At time: 26.698588371276855 and batch: 250, loss is 4.772376899719238 and perplexity is 118.19985753920966
At time: 27.597307682037354 and batch: 300, loss is 4.706523704528808 and perplexity is 110.66678002042559
At time: 28.4923312664032 and batch: 350, loss is 4.694842672348022 and perplexity is 109.3815985379681
At time: 29.389219522476196 and batch: 400, loss is 4.564801177978516 and perplexity is 96.04349655595799
At time: 30.28534746170044 and batch: 450, loss is 4.593154411315918 and perplexity is 98.80561264775122
At time: 31.180912017822266 and batch: 500, loss is 4.489259672164917 and perplexity is 89.0554912086233
At time: 32.07243609428406 and batch: 550, loss is 4.5548218250274655 and perplexity is 95.08981110163117
At time: 32.96728253364563 and batch: 600, loss is 4.510967531204224 and perplexity is 91.00983077752477
At time: 33.860228538513184 and batch: 650, loss is 4.372869129180908 and perplexity is 79.27074374093426
At time: 34.75679636001587 and batch: 700, loss is 4.413316078186035 and perplexity is 82.54272831082679
At time: 35.65536952018738 and batch: 750, loss is 4.46304874420166 and perplexity is 86.75158974082073
At time: 36.55692148208618 and batch: 800, loss is 4.417523345947266 and perplexity is 82.89073924481654
At time: 37.45266103744507 and batch: 850, loss is 4.47391282081604 and perplexity is 87.69920381371821
At time: 38.34863877296448 and batch: 900, loss is 4.399865007400512 and perplexity is 81.43987414258278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.542541085857234 and perplexity of 93.92917922923262
finished 2 epochs...
Completing Train Step...
At time: 40.55667018890381 and batch: 50, loss is 4.451892471313476 and perplexity is 85.78914396836765
At time: 41.460914850234985 and batch: 100, loss is 4.330469136238098 and perplexity is 75.9799231446443
At time: 42.357197523117065 and batch: 150, loss is 4.328106207847595 and perplexity is 75.80059997447906
At time: 43.25132513046265 and batch: 200, loss is 4.213773140907287 and perplexity is 67.6111655932431
At time: 44.142152309417725 and batch: 250, loss is 4.3579530429840085 and perplexity is 78.09710927090542
At time: 45.04137849807739 and batch: 300, loss is 4.316627821922302 and perplexity is 74.93550587480459
At time: 45.93372917175293 and batch: 350, loss is 4.318756170272827 and perplexity is 75.09516457953005
At time: 46.8318567276001 and batch: 400, loss is 4.221944966316223 and perplexity is 68.16593588993446
At time: 47.72510647773743 and batch: 450, loss is 4.258448991775513 and perplexity is 70.7002417147333
At time: 48.619840145111084 and batch: 500, loss is 4.133238081932068 and perplexity is 62.37958646890607
At time: 49.51581859588623 and batch: 550, loss is 4.207359313964844 and perplexity is 67.17890697555913
At time: 50.41240978240967 and batch: 600, loss is 4.202514410018921 and perplexity is 66.85421880069967
At time: 51.31095004081726 and batch: 650, loss is 4.04551251411438 and perplexity is 57.14046382507607
At time: 52.207268714904785 and batch: 700, loss is 4.067932591438294 and perplexity is 58.436026481332775
At time: 53.10314321517944 and batch: 750, loss is 4.155196218490601 and perplexity is 63.764475089428984
At time: 54.00257420539856 and batch: 800, loss is 4.119355382919312 and perplexity is 61.51957291253521
At time: 54.89967083930969 and batch: 850, loss is 4.187730617523194 and perplexity is 65.8731298788832
At time: 55.796337366104126 and batch: 900, loss is 4.122500143051147 and perplexity is 61.71334173129615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39909341890518 and perplexity of 81.37706030896504
finished 3 epochs...
Completing Train Step...
At time: 57.99853157997131 and batch: 50, loss is 4.196574296951294 and perplexity is 66.45827432369578
At time: 58.89766478538513 and batch: 100, loss is 4.076017370223999 and perplexity is 58.910383781661764
At time: 59.79437041282654 and batch: 150, loss is 4.080624070167541 and perplexity is 59.18239229306011
At time: 60.691397190093994 and batch: 200, loss is 3.9557352304458617 and perplexity is 52.23408391877212
At time: 61.58572864532471 and batch: 250, loss is 4.11229989528656 and perplexity is 61.0870499511257
At time: 62.48491454124451 and batch: 300, loss is 4.0808373641967775 and perplexity is 59.195016890299875
At time: 63.381168365478516 and batch: 350, loss is 4.084739193916321 and perplexity is 59.426436953691706
At time: 64.2735505104065 and batch: 400, loss is 4.003156995773315 and perplexity is 54.77078852814401
At time: 65.16684913635254 and batch: 450, loss is 4.037860350608826 and perplexity is 56.70488434128452
At time: 66.065256357193 and batch: 500, loss is 3.913004994392395 and perplexity is 50.04912356366365
At time: 66.95881414413452 and batch: 550, loss is 3.9842353534698485 and perplexity is 53.744178462617946
At time: 67.85860848426819 and batch: 600, loss is 3.99842631816864 and perplexity is 54.51229748642533
At time: 68.76227068901062 and batch: 650, loss is 3.8361812162399294 and perplexity is 46.34814252519094
At time: 69.66260957717896 and batch: 700, loss is 3.8506102895736696 and perplexity is 47.021751361247105
At time: 70.56418681144714 and batch: 750, loss is 3.9506289052963255 and perplexity is 51.968039535040674
At time: 71.46092343330383 and batch: 800, loss is 3.9203773164749145 and perplexity is 50.419465284372805
At time: 72.36003470420837 and batch: 850, loss is 3.988751130104065 and perplexity is 53.98742397578707
At time: 73.25963354110718 and batch: 900, loss is 3.9308151245117187 and perplexity is 50.948490111123085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.343855766400899 and perplexity of 77.00387662771827
finished 4 epochs...
Completing Train Step...
At time: 75.47303915023804 and batch: 50, loss is 4.010971541404724 and perplexity is 55.20047406635211
At time: 76.37013244628906 and batch: 100, loss is 3.8941211032867433 and perplexity is 49.112869248914414
At time: 77.26819181442261 and batch: 150, loss is 3.898248314857483 and perplexity is 49.315987318450254
At time: 78.1653983592987 and batch: 200, loss is 3.776902370452881 and perplexity is 43.680525867394444
At time: 79.06475901603699 and batch: 250, loss is 3.9318344974517823 and perplexity is 51.000452103104166
At time: 79.96210765838623 and batch: 300, loss is 3.902430968284607 and perplexity is 49.52269098537064
At time: 80.86556148529053 and batch: 350, loss is 3.91121618270874 and perplexity is 49.959675133726336
At time: 81.76376867294312 and batch: 400, loss is 3.8368468999862673 and perplexity is 46.379006001860574
At time: 82.66793966293335 and batch: 450, loss is 3.8687154245376587 and perplexity is 47.88084000680525
At time: 83.56597399711609 and batch: 500, loss is 3.750833902359009 and perplexity is 42.556555219195154
At time: 84.46352410316467 and batch: 550, loss is 3.813447451591492 and perplexity is 45.30636143196624
At time: 85.36163806915283 and batch: 600, loss is 3.838464517593384 and perplexity is 46.45409021096643
At time: 86.25994992256165 and batch: 650, loss is 3.6778090238571166 and perplexity is 39.55962485835778
At time: 87.15681791305542 and batch: 700, loss is 3.687851252555847 and perplexity is 39.95889307440042
At time: 88.05629754066467 and batch: 750, loss is 3.793344054222107 and perplexity is 44.40464380723796
At time: 88.95937037467957 and batch: 800, loss is 3.766762862205505 and perplexity is 43.23986463455956
At time: 89.86126327514648 and batch: 850, loss is 3.8337946701049805 and perplexity is 46.23766243014485
At time: 90.75861263275146 and batch: 900, loss is 3.779563446044922 and perplexity is 43.79691784386743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333098111087328 and perplexity of 76.1799352498363
finished 5 epochs...
Completing Train Step...
At time: 92.94688057899475 and batch: 50, loss is 3.86129683971405 and perplexity is 47.52694625226301
At time: 93.85345911979675 and batch: 100, loss is 3.750431036949158 and perplexity is 42.53941410814712
At time: 94.75189423561096 and batch: 150, loss is 3.752777991294861 and perplexity is 42.63936942034796
At time: 95.64673280715942 and batch: 200, loss is 3.6332033014297487 and perplexity is 37.833815831583856
At time: 96.5397298336029 and batch: 250, loss is 3.786313042640686 and perplexity is 44.093529249055756
At time: 97.43681001663208 and batch: 300, loss is 3.759406623840332 and perplexity is 42.92294896616544
At time: 98.32954025268555 and batch: 350, loss is 3.7691850757598875 and perplexity is 43.34472776995064
At time: 99.23091268539429 and batch: 400, loss is 3.7003003978729248 and perplexity is 40.45945646940521
At time: 100.11928629875183 and batch: 450, loss is 3.72978581905365 and perplexity is 41.67018224911427
At time: 101.01462936401367 and batch: 500, loss is 3.615472674369812 and perplexity is 37.168910565520115
At time: 101.90483236312866 and batch: 550, loss is 3.675082025527954 and perplexity is 39.451892786818924
At time: 102.82564210891724 and batch: 600, loss is 3.7065205478668215 and perplexity is 40.71190467623236
At time: 103.72323894500732 and batch: 650, loss is 3.548274064064026 and perplexity is 34.75328375399144
At time: 104.62143802642822 and batch: 700, loss is 3.5547375965118406 and perplexity is 34.978640246139115
At time: 105.52125954627991 and batch: 750, loss is 3.6616567850112913 and perplexity is 38.925781127148625
At time: 106.42581248283386 and batch: 800, loss is 3.6382851600646973 and perplexity is 38.026571298447244
At time: 107.32223796844482 and batch: 850, loss is 3.7035233116149904 and perplexity is 40.59006416326894
At time: 108.21588635444641 and batch: 900, loss is 3.6517213201522827 and perplexity is 38.54095029889584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341518872404752 and perplexity of 76.82413682897287
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.43098020553589 and batch: 50, loss is 3.771166663169861 and perplexity is 43.430704293653164
At time: 111.32651114463806 and batch: 100, loss is 3.6617676067352294 and perplexity is 38.930095188360205
At time: 112.22005987167358 and batch: 150, loss is 3.6733037090301512 and perplexity is 39.381797179566554
At time: 113.1175787448883 and batch: 200, loss is 3.543528938293457 and perplexity is 34.58876568978519
At time: 114.02183103561401 and batch: 250, loss is 3.6873799419403075 and perplexity is 39.9400644613204
At time: 114.92014646530151 and batch: 300, loss is 3.6537752771377563 and perplexity is 38.6201931057887
At time: 115.8177981376648 and batch: 350, loss is 3.649574646949768 and perplexity is 38.458304212496735
At time: 116.71491956710815 and batch: 400, loss is 3.5759939432144163 and perplexity is 35.730116879420336
At time: 117.6184561252594 and batch: 450, loss is 3.5886630058288573 and perplexity is 36.18566354891098
At time: 118.51285314559937 and batch: 500, loss is 3.469429292678833 and perplexity is 32.11840700314452
At time: 119.41336369514465 and batch: 550, loss is 3.5082982540130616 and perplexity is 33.39139573309053
At time: 120.30821537971497 and batch: 600, loss is 3.5383222484588623 and perplexity is 34.40914074655678
At time: 121.20485186576843 and batch: 650, loss is 3.36181734085083 and perplexity is 28.84155822213269
At time: 122.10319948196411 and batch: 700, loss is 3.3490780544281007 and perplexity is 28.47646778440421
At time: 122.99799585342407 and batch: 750, loss is 3.445327572822571 and perplexity is 31.353552336309406
At time: 123.90013551712036 and batch: 800, loss is 3.4089758253097533 and perplexity is 30.234263229936683
At time: 124.79561305046082 and batch: 850, loss is 3.4487762355804445 and perplexity is 31.461866827303144
At time: 125.71518611907959 and batch: 900, loss is 3.3907130050659178 and perplexity is 29.68711178714781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310713258508134 and perplexity of 74.49360319227607
finished 7 epochs...
Completing Train Step...
At time: 127.91400361061096 and batch: 50, loss is 3.678689732551575 and perplexity is 39.59448071059311
At time: 128.81294631958008 and batch: 100, loss is 3.558713517189026 and perplexity is 35.11798938214057
At time: 129.71449446678162 and batch: 150, loss is 3.566508021354675 and perplexity is 35.392786258109986
At time: 130.61496233940125 and batch: 200, loss is 3.4411917781829833 and perplexity is 31.224148261416993
At time: 131.5146942138672 and batch: 250, loss is 3.5858945512771605 and perplexity is 36.08562372569972
At time: 132.41709232330322 and batch: 300, loss is 3.556222720146179 and perplexity is 35.03062644487685
At time: 133.32422256469727 and batch: 350, loss is 3.5546377658843995 and perplexity is 34.97514848083176
At time: 134.23279404640198 and batch: 400, loss is 3.487442145347595 and perplexity is 32.702193178976096
At time: 135.15055179595947 and batch: 450, loss is 3.504856743812561 and perplexity is 33.276676421135164
At time: 136.04818320274353 and batch: 500, loss is 3.3888965463638305 and perplexity is 29.63323532160343
At time: 136.97666001319885 and batch: 550, loss is 3.4314746952056883 and perplexity is 30.922209977335804
At time: 137.88863801956177 and batch: 600, loss is 3.4688502073287966 and perplexity is 32.099813088433905
At time: 138.79914689064026 and batch: 650, loss is 3.296503381729126 and perplexity is 27.018001923185523
At time: 139.72242736816406 and batch: 700, loss is 3.2891600465774538 and perplexity is 26.8203263671613
At time: 140.62841510772705 and batch: 750, loss is 3.392359790802002 and perplexity is 29.73604037577788
At time: 141.52227354049683 and batch: 800, loss is 3.3625976133346556 and perplexity is 28.864071278421036
At time: 142.43297934532166 and batch: 850, loss is 3.4089167070388795 and perplexity is 30.232475885406267
At time: 143.34824109077454 and batch: 900, loss is 3.358478388786316 and perplexity is 28.745418234487055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324170674363228 and perplexity of 75.50287042173646
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.55613446235657 and batch: 50, loss is 3.641283965110779 and perplexity is 38.14077672666358
At time: 146.47924757003784 and batch: 100, loss is 3.536268882751465 and perplexity is 34.33855868711665
At time: 147.39217948913574 and batch: 150, loss is 3.54874942779541 and perplexity is 34.769808131868395
At time: 148.30330681800842 and batch: 200, loss is 3.4228229665756227 and perplexity is 30.655833378568378
At time: 149.19926166534424 and batch: 250, loss is 3.5681016206741334 and perplexity is 35.44923314311756
At time: 150.10422277450562 and batch: 300, loss is 3.5298248100280762 and perplexity is 34.11798996145375
At time: 151.0062611103058 and batch: 350, loss is 3.5265882349014284 and perplexity is 34.00774303124955
At time: 151.90346312522888 and batch: 400, loss is 3.4573799705505373 and perplexity is 31.733724208577403
At time: 152.8014726638794 and batch: 450, loss is 3.469965057373047 and perplexity is 32.13561952216946
At time: 153.6964168548584 and batch: 500, loss is 3.345304799079895 and perplexity is 28.3692212615398
At time: 154.5963191986084 and batch: 550, loss is 3.3837338876724243 and perplexity is 29.48064327097416
At time: 155.49064564704895 and batch: 600, loss is 3.4201133251190186 and perplexity is 30.572879499948332
At time: 156.3861722946167 and batch: 650, loss is 3.241510019302368 and perplexity is 25.57230728517301
At time: 157.28695511817932 and batch: 700, loss is 3.2254352045059203 and perplexity is 25.1645234870573
At time: 158.18875932693481 and batch: 750, loss is 3.328002343177795 and perplexity is 27.88258619306123
At time: 159.0871479511261 and batch: 800, loss is 3.287971453666687 and perplexity is 26.78846685512272
At time: 159.98666620254517 and batch: 850, loss is 3.330438871383667 and perplexity is 27.950605732886153
At time: 160.88397312164307 and batch: 900, loss is 3.284292697906494 and perplexity is 26.690099673725854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3156675573897685 and perplexity of 74.86358250454307
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.0669765472412 and batch: 50, loss is 3.625812335014343 and perplexity is 37.555218190540835
At time: 163.96888542175293 and batch: 100, loss is 3.518349680900574 and perplexity is 33.72871935783719
At time: 164.86752891540527 and batch: 150, loss is 3.531346254348755 and perplexity is 34.1699380915866
At time: 165.76470851898193 and batch: 200, loss is 3.407647953033447 and perplexity is 30.19414263340823
At time: 166.66687488555908 and batch: 250, loss is 3.553406286239624 and perplexity is 34.93210380716425
At time: 167.56336045265198 and batch: 300, loss is 3.5123980617523194 and perplexity is 33.528575048002864
At time: 168.46337223052979 and batch: 350, loss is 3.5084968757629396 and perplexity is 33.39802864924081
At time: 169.3618757724762 and batch: 400, loss is 3.439785580635071 and perplexity is 31.180271797414257
At time: 170.2587070465088 and batch: 450, loss is 3.45259033203125 and perplexity is 31.58209455623262
At time: 171.16629815101624 and batch: 500, loss is 3.3266458082199097 and perplexity is 27.844788133165174
At time: 172.058735370636 and batch: 550, loss is 3.3648037910461426 and perplexity is 28.92782084471747
At time: 172.958566904068 and batch: 600, loss is 3.4013716888427736 and perplexity is 30.005229671189927
At time: 173.85612630844116 and batch: 650, loss is 3.2216667413711546 and perplexity is 25.06987036840303
At time: 174.75440955162048 and batch: 700, loss is 3.2062154102325437 and perplexity is 24.68548478166878
At time: 175.65610671043396 and batch: 750, loss is 3.306907868385315 and perplexity is 27.300577843222495
At time: 176.55319929122925 and batch: 800, loss is 3.263998951911926 and perplexity is 26.153916553902977
At time: 177.45047664642334 and batch: 850, loss is 3.304557147026062 and perplexity is 27.23647716270368
At time: 178.3515739440918 and batch: 900, loss is 3.2585474491119384 and perplexity is 26.01172633191974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310824041497217 and perplexity of 74.50185627344737
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 180.5584056377411 and batch: 50, loss is 3.618661394119263 and perplexity is 37.28762097122692
At time: 181.45486879348755 and batch: 100, loss is 3.509496831893921 and perplexity is 33.43144191586752
At time: 182.35577988624573 and batch: 150, loss is 3.524350085258484 and perplexity is 33.931713727544164
At time: 183.2544810771942 and batch: 200, loss is 3.3984966039657594 and perplexity is 29.919085983677004
At time: 184.1579852104187 and batch: 250, loss is 3.54644718170166 and perplexity is 34.68985155209449
At time: 185.0599114894867 and batch: 300, loss is 3.5045695686340332 and perplexity is 33.26712155766859
At time: 185.95886397361755 and batch: 350, loss is 3.4995940828323366 and perplexity is 33.10201255605401
At time: 186.8607177734375 and batch: 400, loss is 3.431297287940979 and perplexity is 30.916724639228757
At time: 187.75813150405884 and batch: 450, loss is 3.44538724899292 and perplexity is 31.355423452069626
At time: 188.65517497062683 and batch: 500, loss is 3.3196858501434328 and perplexity is 27.651662428115106
At time: 189.54488611221313 and batch: 550, loss is 3.3573224115371705 and perplexity is 28.712208383605
At time: 190.44516611099243 and batch: 600, loss is 3.394960765838623 and perplexity is 29.813483744959072
At time: 191.33983278274536 and batch: 650, loss is 3.2140677261352537 and perplexity is 24.88008604176394
At time: 192.23785376548767 and batch: 700, loss is 3.2005630254745485 and perplexity is 24.546346525672945
At time: 193.14565682411194 and batch: 750, loss is 3.2982061958312987 and perplexity is 27.064047750488776
At time: 194.04211282730103 and batch: 800, loss is 3.2557964277267457 and perplexity is 25.940265856231694
At time: 194.93866896629333 and batch: 850, loss is 3.296906380653381 and perplexity is 27.028892343162465
At time: 195.832665681839 and batch: 900, loss is 3.2489206075668333 and perplexity is 25.7625170380416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30828857421875 and perplexity of 74.31319852352385
finished 11 epochs...
Completing Train Step...
At time: 198.0205638408661 and batch: 50, loss is 3.6151202487945557 and perplexity is 37.155813598820984
At time: 198.92845010757446 and batch: 100, loss is 3.5042329931259157 and perplexity is 33.255926543416194
At time: 199.8284125328064 and batch: 150, loss is 3.5183344650268555 and perplexity is 33.72820614980723
At time: 200.72731018066406 and batch: 200, loss is 3.392945489883423 and perplexity is 29.753461848683376
At time: 201.6274697780609 and batch: 250, loss is 3.540976357460022 and perplexity is 34.50058765808833
At time: 202.52930068969727 and batch: 300, loss is 3.499667263031006 and perplexity is 33.104435056547665
At time: 203.431823015213 and batch: 350, loss is 3.4946577882766725 and perplexity is 32.93901390734539
At time: 204.336017370224 and batch: 400, loss is 3.4269988632202146 and perplexity is 30.78411663269825
At time: 205.2357301712036 and batch: 450, loss is 3.441353621482849 and perplexity is 31.2292020895602
At time: 206.13517355918884 and batch: 500, loss is 3.316017189025879 and perplexity is 27.550403704870444
At time: 207.03535199165344 and batch: 550, loss is 3.353769989013672 and perplexity is 28.610391443314647
At time: 207.93223643302917 and batch: 600, loss is 3.3923649549484254 and perplexity is 29.736193937440937
At time: 208.8300018310547 and batch: 650, loss is 3.212333836555481 and perplexity is 24.83698409763655
At time: 209.7311565876007 and batch: 700, loss is 3.199374737739563 and perplexity is 24.517195726356576
At time: 210.63020706176758 and batch: 750, loss is 3.29824812412262 and perplexity is 27.06518252355657
At time: 211.529767036438 and batch: 800, loss is 3.2566850090026858 and perplexity is 25.963326134712965
At time: 212.43195629119873 and batch: 850, loss is 3.299023675918579 and perplexity is 27.08618111616626
At time: 213.33283376693726 and batch: 900, loss is 3.251923279762268 and perplexity is 25.839989685910943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307617605549016 and perplexity of 74.26335341968836
finished 12 epochs...
Completing Train Step...
At time: 215.51906728744507 and batch: 50, loss is 3.6124233913421633 and perplexity is 37.055744662496075
At time: 216.42387104034424 and batch: 100, loss is 3.5010084390640257 and perplexity is 33.148863718100515
At time: 217.32711243629456 and batch: 150, loss is 3.514667720794678 and perplexity is 33.604759905658916
At time: 218.22981452941895 and batch: 200, loss is 3.3891657257080077 and perplexity is 29.64121305012484
At time: 219.13105249404907 and batch: 250, loss is 3.53712703704834 and perplexity is 34.368039116371826
At time: 220.03346347808838 and batch: 300, loss is 3.495981526374817 and perplexity is 32.98264540693778
At time: 220.93433713912964 and batch: 350, loss is 3.4910334968566894 and perplexity is 32.81984939575706
At time: 221.83299088478088 and batch: 400, loss is 3.4237020444869994 and perplexity is 30.682794093094408
At time: 222.7326340675354 and batch: 450, loss is 3.4383346176147462 and perplexity is 31.135063182018563
At time: 223.63187527656555 and batch: 500, loss is 3.3132299470901487 and perplexity is 27.473720980614193
At time: 224.53607869148254 and batch: 550, loss is 3.351248893737793 and perplexity is 28.538352766945206
At time: 225.43619632720947 and batch: 600, loss is 3.390432777404785 and perplexity is 29.678793802765
At time: 226.33135151863098 and batch: 650, loss is 3.2110359859466553 and perplexity is 24.804770311566752
At time: 227.230619430542 and batch: 700, loss is 3.19859724521637 and perplexity is 24.49814119832607
At time: 228.1243543624878 and batch: 750, loss is 3.2981403970718386 and perplexity is 27.062267028306124
At time: 229.02433729171753 and batch: 800, loss is 3.2572393083572386 and perplexity is 25.977721578955627
At time: 229.9193708896637 and batch: 850, loss is 3.300467891693115 and perplexity is 27.125327667435414
At time: 230.82490730285645 and batch: 900, loss is 3.2538649320602415 and perplexity is 25.890210701361404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307362595649614 and perplexity of 74.2444179438731
finished 13 epochs...
Completing Train Step...
At time: 233.04085731506348 and batch: 50, loss is 3.610115098953247 and perplexity is 36.97030781367542
At time: 233.9371178150177 and batch: 100, loss is 3.4983056592941284 and perplexity is 33.059390607371846
At time: 234.83801770210266 and batch: 150, loss is 3.5116098070144655 and perplexity is 33.50215640354654
At time: 235.72846937179565 and batch: 200, loss is 3.386020679473877 and perplexity is 29.548136506383948
At time: 236.62678408622742 and batch: 250, loss is 3.5339131355285645 and perplexity is 34.25776092976322
At time: 237.5236041545868 and batch: 300, loss is 3.492884426116943 and perplexity is 32.88065286942598
At time: 238.42949533462524 and batch: 350, loss is 3.4880043840408326 and perplexity is 32.72058478708742
At time: 239.32471203804016 and batch: 400, loss is 3.4208987855911257 and perplexity is 30.596902721724504
At time: 240.21451592445374 and batch: 450, loss is 3.4357700729370118 and perplexity is 31.05531821990941
At time: 241.10770058631897 and batch: 500, loss is 3.310830497741699 and perplexity is 27.407878203502122
At time: 242.00307893753052 and batch: 550, loss is 3.3491148376464843 and perplexity is 28.477515259802157
At time: 242.9037024974823 and batch: 600, loss is 3.3887775230407713 and perplexity is 29.629708485354506
At time: 243.80053615570068 and batch: 650, loss is 3.2098741006851195 and perplexity is 24.77596675098257
At time: 244.7004313468933 and batch: 700, loss is 3.197880382537842 and perplexity is 24.48058568840442
At time: 245.603999376297 and batch: 750, loss is 3.297911424636841 and perplexity is 27.056071224489305
At time: 246.500173330307 and batch: 800, loss is 3.257552242279053 and perplexity is 25.985852161350294
At time: 247.39950037002563 and batch: 850, loss is 3.301469602584839 and perplexity is 27.152513017258762
At time: 248.29297590255737 and batch: 900, loss is 3.2552081108093263 and perplexity is 25.925009247286734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307294453660103 and perplexity of 74.23935895389106
finished 14 epochs...
Completing Train Step...
At time: 250.49227571487427 and batch: 50, loss is 3.6080300426483154 and perplexity is 36.89330294791166
At time: 251.3978464603424 and batch: 100, loss is 3.4958999586105346 and perplexity is 32.97995519601055
At time: 252.29653882980347 and batch: 150, loss is 3.5089029264450073 and perplexity is 33.41159269521224
At time: 253.1969895362854 and batch: 200, loss is 3.3832509326934814 and perplexity is 29.466408885084444
At time: 254.09098553657532 and batch: 250, loss is 3.531076917648315 and perplexity is 34.16073611247776
At time: 254.9929554462433 and batch: 300, loss is 3.4901564168930053 and perplexity is 32.791076383401744
At time: 255.8872082233429 and batch: 350, loss is 3.485331516265869 and perplexity is 32.633243767928455
At time: 256.78471183776855 and batch: 400, loss is 3.418409070968628 and perplexity is 30.520819917151687
At time: 257.68742632865906 and batch: 450, loss is 3.433484582901001 and perplexity is 30.984422645976608
At time: 258.5886721611023 and batch: 500, loss is 3.3086806297302247 and perplexity is 27.349018176205533
At time: 259.48329424858093 and batch: 550, loss is 3.3472029781341552 and perplexity is 28.423122263816733
At time: 260.3826551437378 and batch: 600, loss is 3.387275815010071 and perplexity is 29.585246706835242
At time: 261.28964948654175 and batch: 650, loss is 3.208774194717407 and perplexity is 24.748730498701384
At time: 262.1879312992096 and batch: 700, loss is 3.197167897224426 and perplexity is 24.463149842767177
At time: 263.08261799812317 and batch: 750, loss is 3.2975893926620485 and perplexity is 27.0473597072116
At time: 263.9838125705719 and batch: 800, loss is 3.257683906555176 and perplexity is 25.989273795013077
At time: 264.87694215774536 and batch: 850, loss is 3.3021621751785277 and perplexity is 27.171324617071605
At time: 265.7796311378479 and batch: 900, loss is 3.2561534738540647 and perplexity is 25.949529381351763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307332496120505 and perplexity of 74.24218325548571
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 267.9713921546936 and batch: 50, loss is 3.607046537399292 and perplexity is 36.85703602808495
At time: 268.88639092445374 and batch: 100, loss is 3.4955353784561156 and perplexity is 32.96793355041232
At time: 269.7893271446228 and batch: 150, loss is 3.5093510293960573 and perplexity is 33.426567883460685
At time: 270.6888163089752 and batch: 200, loss is 3.3825566577911377 and perplexity is 29.44595819695455
At time: 271.59073519706726 and batch: 250, loss is 3.5305477237701415 and perplexity is 34.142663242500966
At time: 272.4885048866272 and batch: 300, loss is 3.489613747596741 and perplexity is 32.77328650050521
At time: 273.3869996070862 and batch: 350, loss is 3.484054727554321 and perplexity is 32.59160459853174
At time: 274.29023361206055 and batch: 400, loss is 3.4174442005157473 and perplexity is 30.491385482307734
At time: 275.1904683113098 and batch: 450, loss is 3.4326741743087767 and perplexity is 30.95932277558824
At time: 276.0897579193115 and batch: 500, loss is 3.3079203844070433 and perplexity is 27.32823411453736
At time: 276.98995184898376 and batch: 550, loss is 3.34547981262207 and perplexity is 28.37418669393781
At time: 277.88362646102905 and batch: 600, loss is 3.3857682657241823 and perplexity is 29.540679091668483
At time: 278.786190032959 and batch: 650, loss is 3.2063699913024903 and perplexity is 24.689300985267298
At time: 279.6885461807251 and batch: 700, loss is 3.1949271535873414 and perplexity is 24.408395563475196
At time: 280.583279132843 and batch: 750, loss is 3.2944437313079833 and perplexity is 26.962411552163477
At time: 281.4834198951721 and batch: 800, loss is 3.2542489099502565 and perplexity is 25.900153878696514
At time: 282.3823034763336 and batch: 850, loss is 3.299274749755859 and perplexity is 27.092982601398134
At time: 283.29629921913147 and batch: 900, loss is 3.2524411344528197 and perplexity is 25.853374511171747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306418222923801 and perplexity of 74.17433663716878
finished 16 epochs...
Completing Train Step...
At time: 285.5011787414551 and batch: 50, loss is 3.6061935949325563 and perplexity is 36.82561249999445
At time: 286.40147519111633 and batch: 100, loss is 3.494283561706543 and perplexity is 32.92668955934047
At time: 287.2972447872162 and batch: 150, loss is 3.5078833866119385 and perplexity is 33.377545604704906
At time: 288.1982698440552 and batch: 200, loss is 3.381553192138672 and perplexity is 29.416425009547737
At time: 289.0951473712921 and batch: 250, loss is 3.529610109329224 and perplexity is 34.1106655914674
At time: 289.99415493011475 and batch: 300, loss is 3.488766713142395 and perplexity is 32.745538151220124
At time: 290.89908742904663 and batch: 350, loss is 3.4832003688812256 and perplexity is 32.56377156986061
At time: 291.8013164997101 and batch: 400, loss is 3.416600422859192 and perplexity is 30.4656683838027
At time: 292.69900727272034 and batch: 450, loss is 3.4318562507629395 and perplexity is 30.934010769580908
At time: 293.6019654273987 and batch: 500, loss is 3.307125201225281 and perplexity is 27.306511800136374
At time: 294.50178694725037 and batch: 550, loss is 3.344840154647827 and perplexity is 28.35604272274295
At time: 295.4020550251007 and batch: 600, loss is 3.3852572679519652 and perplexity is 29.525587726618205
At time: 296.30069231987 and batch: 650, loss is 3.206113409996033 and perplexity is 24.682966984792785
At time: 297.2043216228485 and batch: 700, loss is 3.1948536443710327 and perplexity is 24.40660138739102
At time: 298.10317945480347 and batch: 750, loss is 3.2945220041275025 and perplexity is 26.964522058733273
At time: 299.0074689388275 and batch: 800, loss is 3.2545550775527956 and perplexity is 25.90808488076094
At time: 299.90931153297424 and batch: 850, loss is 3.2997424840927123 and perplexity is 27.105657883749878
At time: 300.81001472473145 and batch: 900, loss is 3.2529564094543457 and perplexity is 25.866699541495127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306015223672945 and perplexity of 74.14445045752977
finished 17 epochs...
Completing Train Step...
At time: 303.00464606285095 and batch: 50, loss is 3.605518217086792 and perplexity is 36.8007496939945
At time: 303.91212010383606 and batch: 100, loss is 3.493417248725891 and perplexity is 32.89817709290842
At time: 304.80729389190674 and batch: 150, loss is 3.5068905401229857 and perplexity is 33.34442327115855
At time: 305.7140417098999 and batch: 200, loss is 3.3807347917556765 and perplexity is 29.392360444621303
At time: 306.61519837379456 and batch: 250, loss is 3.5287936878204347 and perplexity is 34.08282827543743
At time: 307.51394510269165 and batch: 300, loss is 3.488029274940491 and perplexity is 32.72139924201633
At time: 308.4109501838684 and batch: 350, loss is 3.482454090118408 and perplexity is 32.539478984348555
At time: 309.3067581653595 and batch: 400, loss is 3.4159098434448243 and perplexity is 30.44463668323746
At time: 310.2076425552368 and batch: 450, loss is 3.4311948251724242 and perplexity is 30.91355698831349
At time: 311.107460975647 and batch: 500, loss is 3.306518201828003 and perplexity is 27.289941793432728
At time: 312.006591796875 and batch: 550, loss is 3.3443095827102662 and perplexity is 28.341001792714525
At time: 312.9072880744934 and batch: 600, loss is 3.384850568771362 and perplexity is 29.513582135780425
At time: 313.80080127716064 and batch: 650, loss is 3.205869655609131 and perplexity is 24.676951136532974
At time: 314.7006869316101 and batch: 700, loss is 3.1947453832626342 and perplexity is 24.403959244695812
At time: 315.5987696647644 and batch: 750, loss is 3.2945503902435305 and perplexity is 26.965287487648798
At time: 316.4951455593109 and batch: 800, loss is 3.254746603965759 and perplexity is 25.91304743854018
At time: 317.3971185684204 and batch: 850, loss is 3.3000878286361695 and perplexity is 27.115020291331984
At time: 318.2997965812683 and batch: 900, loss is 3.253341279029846 and perplexity is 25.876656763160188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305819994782748 and perplexity of 74.12997673164335
finished 18 epochs...
Completing Train Step...
At time: 320.48208475112915 and batch: 50, loss is 3.604910593032837 and perplexity is 36.77839546544811
At time: 321.3845694065094 and batch: 100, loss is 3.4927010488510133 and perplexity is 32.87462385800951
At time: 322.2801959514618 and batch: 150, loss is 3.5060856342315674 and perplexity is 33.31759494700817
At time: 323.1810817718506 and batch: 200, loss is 3.3799944019317625 and perplexity is 29.37060669416877
At time: 324.0778250694275 and batch: 250, loss is 3.528038749694824 and perplexity is 34.05710755894
At time: 324.97394585609436 and batch: 300, loss is 3.4873369979858397 and perplexity is 32.69875481041058
At time: 325.8775360584259 and batch: 350, loss is 3.481759819984436 and perplexity is 32.516895636293874
At time: 326.77452301979065 and batch: 400, loss is 3.415281229019165 and perplexity is 30.425504759366103
At time: 327.67020869255066 and batch: 450, loss is 3.43059974193573 and perplexity is 30.895166321294703
At time: 328.574725151062 and batch: 500, loss is 3.3059798526763915 and perplexity is 27.275254230294017
At time: 329.469886302948 and batch: 550, loss is 3.3438252925872805 and perplexity is 28.327279848440025
At time: 330.36762142181396 and batch: 600, loss is 3.384480276107788 and perplexity is 29.502655495991885
At time: 331.2774157524109 and batch: 650, loss is 3.2056294822692872 and perplexity is 24.671025102427542
At time: 332.17115592956543 and batch: 700, loss is 3.1946174097061157 and perplexity is 24.400836383064252
At time: 333.0694320201874 and batch: 750, loss is 3.2945427083969117 and perplexity is 26.965080345241905
At time: 333.96908926963806 and batch: 800, loss is 3.2548699283599856 and perplexity is 25.916243346480755
At time: 334.86087679862976 and batch: 850, loss is 3.3003564071655274 and perplexity is 27.122303781656115
At time: 335.7566294670105 and batch: 900, loss is 3.2536477041244507 and perplexity is 25.88458723514265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305722589362158 and perplexity of 74.1227564217358
finished 19 epochs...
Completing Train Step...
At time: 337.94288086891174 and batch: 50, loss is 3.604341549873352 and perplexity is 36.75747292457062
At time: 338.8473026752472 and batch: 100, loss is 3.492057318687439 and perplexity is 32.853468280988025
At time: 339.7461712360382 and batch: 150, loss is 3.5053697538375856 and perplexity is 33.293752069353566
At time: 340.64560627937317 and batch: 200, loss is 3.3792996644973754 and perplexity is 29.350208920596543
At time: 341.5454070568085 and batch: 250, loss is 3.527326431274414 and perplexity is 34.032856692074866
At time: 342.4451096057892 and batch: 300, loss is 3.486674499511719 and perplexity is 32.67709910946951
At time: 343.34665727615356 and batch: 350, loss is 3.4811004304885866 and perplexity is 32.495461404403976
At time: 344.2435338497162 and batch: 400, loss is 3.4146861600875855 and perplexity is 30.407404872630412
At time: 345.14416766166687 and batch: 450, loss is 3.4300415182113646 and perplexity is 30.87792471926931
At time: 346.04212737083435 and batch: 500, loss is 3.3054732847213746 and perplexity is 27.261440959511564
At time: 346.94517183303833 and batch: 550, loss is 3.343367156982422 and perplexity is 28.314305085280083
At time: 347.8445358276367 and batch: 600, loss is 3.384127826690674 and perplexity is 29.492259134462405
At time: 348.74183917045593 and batch: 650, loss is 3.2053911304473877 and perplexity is 24.665145419390665
At time: 349.64126539230347 and batch: 700, loss is 3.194478840827942 and perplexity is 24.397455420793637
At time: 350.5627546310425 and batch: 750, loss is 3.294510817527771 and perplexity is 26.964220419105217
At time: 351.46412444114685 and batch: 800, loss is 3.2549513387680054 and perplexity is 25.918353284310218
At time: 352.36188888549805 and batch: 850, loss is 3.3005746459960936 and perplexity is 27.128223567455755
At time: 353.25947523117065 and batch: 900, loss is 3.253903031349182 and perplexity is 25.891197118770428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305678276166524 and perplexity of 74.11947187830441
Finished Training.
Improved accuracyfrom -10000000 to -74.11947187830441
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
369.59056782722473


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.4081404209136963 and batch: 50, loss is 7.110503368377685 and perplexity is 1224.7638983691697
At time: 2.526918649673462 and batch: 100, loss is 6.447585010528565 and perplexity is 631.1761669672021
At time: 3.6509928703308105 and batch: 150, loss is 5.9913880729675295 and perplexity is 399.96941151344805
At time: 4.771013259887695 and batch: 200, loss is 5.740267171859741 and perplexity is 311.1475297415764
At time: 5.895137071609497 and batch: 250, loss is 5.681461153030395 and perplexity is 293.37778674105544
At time: 7.018140077590942 and batch: 300, loss is 5.509999332427978 and perplexity is 247.1509620765012
At time: 8.14321255683899 and batch: 350, loss is 5.428308744430542 and perplexity is 227.76371285096494
At time: 9.266210794448853 and batch: 400, loss is 5.235408458709717 and perplexity is 187.8058015951079
At time: 10.389380931854248 and batch: 450, loss is 5.192919864654541 and perplexity is 179.99334260069628
At time: 11.514423131942749 and batch: 500, loss is 5.112338743209839 and perplexity is 166.05826872989576
At time: 12.636428356170654 and batch: 550, loss is 5.149677467346192 and perplexity is 172.37588449851037
At time: 13.758810043334961 and batch: 600, loss is 5.033622102737427 and perplexity is 153.487956204022
At time: 14.878450155258179 and batch: 650, loss is 4.908406686782837 and perplexity is 135.42347041561274
At time: 16.00591278076172 and batch: 700, loss is 4.97429105758667 and perplexity is 144.64624293349155
At time: 17.123923778533936 and batch: 750, loss is 4.950472326278686 and perplexity is 141.2416603168197
At time: 18.245084524154663 and batch: 800, loss is 4.9021274185180665 and perplexity is 134.5757743563388
At time: 19.367717504501343 and batch: 850, loss is 4.930403146743775 and perplexity is 138.43531083650097
At time: 20.493499279022217 and batch: 900, loss is 4.8352577495574955 and perplexity is 125.87102222640218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.874210305409889 and perplexity of 130.8707644587257
finished 1 epochs...
Completing Train Step...
At time: 22.852878093719482 and batch: 50, loss is 4.8438967418670655 and perplexity is 126.96313158940704
At time: 23.750696897506714 and batch: 100, loss is 4.725191049575805 and perplexity is 112.75203754542312
At time: 24.647670030593872 and batch: 150, loss is 4.727302532196045 and perplexity is 112.99036303461608
At time: 25.546828031539917 and batch: 200, loss is 4.610267505645752 and perplexity is 100.51103330933253
At time: 26.452403783798218 and batch: 250, loss is 4.72227668762207 and perplexity is 112.42391566220302
At time: 27.350234270095825 and batch: 300, loss is 4.662234106063843 and perplexity is 105.87234819178566
At time: 28.249547004699707 and batch: 350, loss is 4.650839023590088 and perplexity is 104.67277166933366
At time: 29.15264105796814 and batch: 400, loss is 4.533641662597656 and perplexity is 93.09697228032603
At time: 30.05245804786682 and batch: 450, loss is 4.559498300552368 and perplexity is 95.53553767831394
At time: 30.94861125946045 and batch: 500, loss is 4.452825937271118 and perplexity is 85.86926260198824
At time: 31.850318431854248 and batch: 550, loss is 4.523513488769531 and perplexity is 92.15882882175269
At time: 32.7458221912384 and batch: 600, loss is 4.477723627090454 and perplexity is 88.03404609418261
At time: 33.64552164077759 and batch: 650, loss is 4.336814174652099 and perplexity is 76.46355137298143
At time: 34.54242157936096 and batch: 700, loss is 4.375708866119385 and perplexity is 79.49617172667534
At time: 35.438666105270386 and batch: 750, loss is 4.429319305419922 and perplexity is 83.87430468987439
At time: 36.33716702461243 and batch: 800, loss is 4.38617582321167 and perplexity is 80.33262466705216
At time: 37.23082494735718 and batch: 850, loss is 4.454378566741943 and perplexity is 86.00268930405316
At time: 38.12949848175049 and batch: 900, loss is 4.375888996124267 and perplexity is 79.510492662253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.540242234321489 and perplexity of 93.71349799586207
finished 2 epochs...
Completing Train Step...
At time: 40.321200370788574 and batch: 50, loss is 4.44628752708435 and perplexity is 85.30964563696523
At time: 41.22838592529297 and batch: 100, loss is 4.319959635734558 and perplexity is 75.18559341952312
At time: 42.12630295753479 and batch: 150, loss is 4.330878577232361 and perplexity is 76.01103880950035
At time: 43.02269411087036 and batch: 200, loss is 4.212692179679871 and perplexity is 67.53812003151904
At time: 43.917431592941284 and batch: 250, loss is 4.356004295349121 and perplexity is 77.94506590910989
At time: 44.81942653656006 and batch: 300, loss is 4.315442771911621 and perplexity is 74.84675614958492
At time: 45.713541746139526 and batch: 350, loss is 4.313501396179199 and perplexity is 74.70159142877182
At time: 46.61217474937439 and batch: 400, loss is 4.223032813072205 and perplexity is 68.2401303309349
At time: 47.5102903842926 and batch: 450, loss is 4.258617310523987 and perplexity is 70.71214289250311
At time: 48.41272306442261 and batch: 500, loss is 4.133878903388977 and perplexity is 62.41957345728016
At time: 49.3228075504303 and batch: 550, loss is 4.2071575260162355 and perplexity is 67.1653524493468
At time: 50.2195839881897 and batch: 600, loss is 4.199053339958191 and perplexity is 66.62323162755277
At time: 51.11602759361267 and batch: 650, loss is 4.045036792755127 and perplexity is 57.113287350684054
At time: 52.009673833847046 and batch: 700, loss is 4.0612557554245 and perplexity is 58.04715836419226
At time: 52.90772581100464 and batch: 750, loss is 4.1529935836792 and perplexity is 63.624179803228294
At time: 53.80840253829956 and batch: 800, loss is 4.117741193771362 and perplexity is 61.42034879034883
At time: 54.70681190490723 and batch: 850, loss is 4.19164701461792 and perplexity is 66.13162106001849
At time: 55.60291314125061 and batch: 900, loss is 4.123082056045532 and perplexity is 61.749263977573754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.409963947452911 and perplexity of 82.26649753396042
finished 3 epochs...
Completing Train Step...
At time: 57.81162905693054 and batch: 50, loss is 4.206499147415161 and perplexity is 67.12114677219326
At time: 58.718278646469116 and batch: 100, loss is 4.0826988649368285 and perplexity is 59.30531108255962
At time: 59.616469860076904 and batch: 150, loss is 4.094842734336853 and perplexity is 60.029897773385336
At time: 60.510770082473755 and batch: 200, loss is 3.9799536514282225 and perplexity is 53.51455384687045
At time: 61.41497588157654 and batch: 250, loss is 4.127841668128967 and perplexity is 62.043867062587246
At time: 62.313536167144775 and batch: 300, loss is 4.096246309280396 and perplexity is 60.11421339156601
At time: 63.21642994880676 and batch: 350, loss is 4.096266961097717 and perplexity is 60.11545487213875
At time: 64.11640906333923 and batch: 400, loss is 4.015438494682312 and perplexity is 55.44760355191168
At time: 65.01473188400269 and batch: 450, loss is 4.053448762893677 and perplexity is 57.59574899992352
At time: 65.91738557815552 and batch: 500, loss is 3.926414313316345 and perplexity is 50.724768065766824
At time: 66.81305408477783 and batch: 550, loss is 3.996865124702454 and perplexity is 54.42725964129888
At time: 67.71232151985168 and batch: 600, loss is 4.004866347312928 and perplexity is 54.864491122361564
At time: 68.61753964424133 and batch: 650, loss is 3.852293515205383 and perplexity is 47.10096622793048
At time: 69.51669573783875 and batch: 700, loss is 3.859153542518616 and perplexity is 47.42519096651522
At time: 70.41539454460144 and batch: 750, loss is 3.9626217699050903 and perplexity is 52.59503743260341
At time: 71.32743263244629 and batch: 800, loss is 3.9322207498550417 and perplexity is 51.02015495518833
At time: 72.22914457321167 and batch: 850, loss is 4.004785833358764 and perplexity is 54.860073943062815
At time: 73.12972831726074 and batch: 900, loss is 3.936450147628784 and perplexity is 51.23639644827614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363721873662243 and perplexity of 78.54894028600447
finished 4 epochs...
Completing Train Step...
At time: 75.32840657234192 and batch: 50, loss is 4.030246419906616 and perplexity is 56.27477676392301
At time: 76.22814559936523 and batch: 100, loss is 3.9096557188034056 and perplexity is 49.88177565942427
At time: 77.12976431846619 and batch: 150, loss is 3.919989056587219 and perplexity is 50.399893228211866
At time: 78.02714419364929 and batch: 200, loss is 3.808537302017212 and perplexity is 45.08444568627858
At time: 78.92757105827332 and batch: 250, loss is 3.956626286506653 and perplexity is 52.28064815842088
At time: 79.82570314407349 and batch: 300, loss is 3.9281587696075437 and perplexity is 50.813332432412636
At time: 80.7262794971466 and batch: 350, loss is 3.9290867233276368 and perplexity is 50.860506737674505
At time: 81.62463116645813 and batch: 400, loss is 3.8541913986206056 and perplexity is 47.19044325219789
At time: 82.52180647850037 and batch: 450, loss is 3.8897948360443113 and perplexity is 48.90085280209064
At time: 83.42133855819702 and batch: 500, loss is 3.765668091773987 and perplexity is 43.192552811812455
At time: 84.31973075866699 and batch: 550, loss is 3.8333284044265747 and perplexity is 46.216108420441934
At time: 85.22104263305664 and batch: 600, loss is 3.8501022052764893 and perplexity is 46.99786641605116
At time: 86.11961698532104 and batch: 650, loss is 3.699709219932556 and perplexity is 40.43554479998334
At time: 87.01627516746521 and batch: 700, loss is 3.7057699584960937 and perplexity is 40.68135821867759
At time: 87.91458463668823 and batch: 750, loss is 3.8081307125091555 and perplexity is 45.066118549749184
At time: 88.81664252281189 and batch: 800, loss is 3.7847223043441773 and perplexity is 44.023443742080026
At time: 89.71794939041138 and batch: 850, loss is 3.852074580192566 and perplexity is 47.09065530603776
At time: 90.61547136306763 and batch: 900, loss is 3.788329529762268 and perplexity is 44.182532990221425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.350514451118364 and perplexity of 77.5183320616435
finished 5 epochs...
Completing Train Step...
At time: 92.81252002716064 and batch: 50, loss is 3.8865082120895384 and perplexity is 48.740397909743635
At time: 93.72253918647766 and batch: 100, loss is 3.7666420984268187 and perplexity is 43.23464314040643
At time: 94.617840051651 and batch: 150, loss is 3.777712998390198 and perplexity is 43.71594887748029
At time: 95.51937580108643 and batch: 200, loss is 3.6694927072525023 and perplexity is 39.231998702941304
At time: 96.41658878326416 and batch: 250, loss is 3.815811204910278 and perplexity is 45.41358116474011
At time: 97.31479454040527 and batch: 300, loss is 3.7871324825286865 and perplexity is 44.129676053769295
At time: 98.21263456344604 and batch: 350, loss is 3.7903346014022827 and perplexity is 44.27121100728325
At time: 99.11354541778564 and batch: 400, loss is 3.7203791618347166 and perplexity is 41.28004295852533
At time: 100.00967526435852 and batch: 450, loss is 3.7572627878189087 and perplexity is 42.83102776921869
At time: 100.91056776046753 and batch: 500, loss is 3.634882426261902 and perplexity is 37.897396896575636
At time: 101.80493330955505 and batch: 550, loss is 3.6977870941162108 and perplexity is 40.35789724351791
At time: 102.70287704467773 and batch: 600, loss is 3.717747459411621 and perplexity is 41.171548993978384
At time: 103.6030433177948 and batch: 650, loss is 3.572569375038147 and perplexity is 35.60796593471843
At time: 104.50607967376709 and batch: 700, loss is 3.5810624456405638 and perplexity is 35.91167478817995
At time: 105.40427923202515 and batch: 750, loss is 3.679025821685791 and perplexity is 39.60779022180065
At time: 106.30776333808899 and batch: 800, loss is 3.6612561082839967 and perplexity is 38.9101875967502
At time: 107.20392870903015 and batch: 850, loss is 3.724802837371826 and perplexity is 41.46305697359392
At time: 108.10245442390442 and batch: 900, loss is 3.6628639316558838 and perplexity is 38.9727986260229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3597040045751285 and perplexity of 78.23397409717582
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.29366517066956 and batch: 50, loss is 3.7938925504684446 and perplexity is 44.42900626843466
At time: 111.19986867904663 and batch: 100, loss is 3.6852515602111815 and perplexity is 39.85514715813544
At time: 112.09564590454102 and batch: 150, loss is 3.6991981506347655 and perplexity is 40.41488471431406
At time: 112.9923415184021 and batch: 200, loss is 3.5747093534469605 and perplexity is 35.68424780466672
At time: 113.89452648162842 and batch: 250, loss is 3.712129011154175 and perplexity is 40.940877391308646
At time: 114.79042744636536 and batch: 300, loss is 3.6751996421813966 and perplexity is 39.45653325931355
At time: 115.68886756896973 and batch: 350, loss is 3.6719425344467163 and perplexity is 39.32822814487102
At time: 116.59671473503113 and batch: 400, loss is 3.594755883216858 and perplexity is 36.406811389314335
At time: 117.49208092689514 and batch: 450, loss is 3.6177014970779418 and perplexity is 37.25184586713396
At time: 118.39053320884705 and batch: 500, loss is 3.487270884513855 and perplexity is 32.6965930536619
At time: 119.28699207305908 and batch: 550, loss is 3.53348060131073 and perplexity is 34.242946480043884
At time: 120.18786716461182 and batch: 600, loss is 3.5499724912643433 and perplexity is 34.812359830425954
At time: 121.08501696586609 and batch: 650, loss is 3.387743649482727 and perplexity is 29.599090943279766
At time: 121.97882223129272 and batch: 700, loss is 3.378812460899353 and perplexity is 29.335912876032673
At time: 122.86963486671448 and batch: 750, loss is 3.46340238571167 and perplexity is 31.925414510284618
At time: 123.77484560012817 and batch: 800, loss is 3.4317156982421877 and perplexity is 30.929663221926827
At time: 124.66280627250671 and batch: 850, loss is 3.476493854522705 and perplexity is 32.34611284973413
At time: 125.56036353111267 and batch: 900, loss is 3.4010144233703614 and perplexity is 29.99451175332161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324908530875428 and perplexity of 75.55860126452107
finished 7 epochs...
Completing Train Step...
At time: 127.76359248161316 and batch: 50, loss is 3.700374331474304 and perplexity is 40.46244789331385
At time: 128.66031336784363 and batch: 100, loss is 3.5826653623580933 and perplexity is 35.96928437139953
At time: 129.56202220916748 and batch: 150, loss is 3.5949944257736206 and perplexity is 36.41549699908931
At time: 130.4572856426239 and batch: 200, loss is 3.474769802093506 and perplexity is 32.290394499779936
At time: 131.35812306404114 and batch: 250, loss is 3.613838415145874 and perplexity is 37.108216538979526
At time: 132.25462794303894 and batch: 300, loss is 3.579267797470093 and perplexity is 35.84728376362235
At time: 133.16221070289612 and batch: 350, loss is 3.5785284948349 and perplexity is 35.820791566353684
At time: 134.07061219215393 and batch: 400, loss is 3.506643466949463 and perplexity is 33.33618577635555
At time: 134.97050023078918 and batch: 450, loss is 3.534865503311157 and perplexity is 34.290402458479484
At time: 135.87479734420776 and batch: 500, loss is 3.407762551307678 and perplexity is 30.197603028319758
At time: 136.7764914035797 and batch: 550, loss is 3.4578261375427246 and perplexity is 31.74788590786491
At time: 137.6733374595642 and batch: 600, loss is 3.4806817770004272 and perplexity is 32.48185991349214
At time: 138.5804226398468 and batch: 650, loss is 3.324078793525696 and perplexity is 27.773401816931827
At time: 139.48147416114807 and batch: 700, loss is 3.3207233810424803 and perplexity is 27.680366770540463
At time: 140.37932586669922 and batch: 750, loss is 3.412465214729309 and perplexity is 30.33994662618967
At time: 141.27936625480652 and batch: 800, loss is 3.3857134056091307 and perplexity is 29.539058531067287
At time: 142.1790018081665 and batch: 850, loss is 3.437025294303894 and perplexity is 31.09432399421364
At time: 143.07816886901855 and batch: 900, loss is 3.3680184125900268 and perplexity is 29.020962468148348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336741408256636 and perplexity of 76.45798759839401
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.26409244537354 and batch: 50, loss is 3.6617053174972534 and perplexity is 38.92767033791842
At time: 146.1734800338745 and batch: 100, loss is 3.5600179290771483 and perplexity is 35.163827594432675
At time: 147.06838822364807 and batch: 150, loss is 3.5795359945297243 and perplexity is 35.85689918908031
At time: 147.9695131778717 and batch: 200, loss is 3.458124585151672 and perplexity is 31.757362402551504
At time: 148.8692455291748 and batch: 250, loss is 3.5972562885284423 and perplexity is 36.49795707698371
At time: 149.7735424041748 and batch: 300, loss is 3.555529556274414 and perplexity is 35.00635289396614
At time: 150.67274522781372 and batch: 350, loss is 3.5452175426483152 and perplexity is 34.64722177086461
At time: 151.57123637199402 and batch: 400, loss is 3.471726403236389 and perplexity is 32.19227133975696
At time: 152.47449278831482 and batch: 450, loss is 3.500332779884338 and perplexity is 33.126473948811515
At time: 153.37362408638 and batch: 500, loss is 3.3706765031814574 and perplexity is 29.098205429353754
At time: 154.26958799362183 and batch: 550, loss is 3.4107149934768675 and perplexity is 30.28689144949901
At time: 155.1699697971344 and batch: 600, loss is 3.4349594068527223 and perplexity is 31.030152928394056
At time: 156.0704164505005 and batch: 650, loss is 3.2722756052017212 and perplexity is 26.371281740688335
At time: 156.97032070159912 and batch: 700, loss is 3.25709538936615 and perplexity is 25.973983160496363
At time: 157.87062621116638 and batch: 750, loss is 3.3447163009643557 and perplexity is 28.352530939881177
At time: 158.77438521385193 and batch: 800, loss is 3.3138180923461915 and perplexity is 27.48988427199043
At time: 159.67333054542542 and batch: 850, loss is 3.360597376823425 and perplexity is 28.80639401250406
At time: 160.5725531578064 and batch: 900, loss is 3.296538414955139 and perplexity is 27.018948467533473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326389260488014 and perplexity of 75.67056599718921
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 162.78989052772522 and batch: 50, loss is 3.6435994338989257 and perplexity is 38.22919282758807
At time: 163.69771885871887 and batch: 100, loss is 3.540132212638855 and perplexity is 34.47147645445409
At time: 164.6033809185028 and batch: 150, loss is 3.560550136566162 and perplexity is 35.18254702769026
At time: 165.50092959403992 and batch: 200, loss is 3.4409577798843385 and perplexity is 31.216842718622814
At time: 166.39708423614502 and batch: 250, loss is 3.581368579864502 and perplexity is 35.922670263831385
At time: 167.29330372810364 and batch: 300, loss is 3.5392027616500856 and perplexity is 34.439451791561915
At time: 168.19850635528564 and batch: 350, loss is 3.5294791746139524 and perplexity is 34.10619961356262
At time: 169.0961639881134 and batch: 400, loss is 3.455502820014954 and perplexity is 31.674211106184725
At time: 169.99900817871094 and batch: 450, loss is 3.4848040533065796 and perplexity is 32.616035479365785
At time: 170.89724206924438 and batch: 500, loss is 3.355939688682556 and perplexity is 28.672534791978542
At time: 171.79701328277588 and batch: 550, loss is 3.394201912879944 and perplexity is 29.79086827660349
At time: 172.69399857521057 and batch: 600, loss is 3.418265781402588 and perplexity is 30.516446915420815
At time: 173.59141731262207 and batch: 650, loss is 3.255207371711731 and perplexity is 25.92499008618183
At time: 174.48980069160461 and batch: 700, loss is 3.237735695838928 and perplexity is 25.475971042206666
At time: 175.38529443740845 and batch: 750, loss is 3.3226402759552003 and perplexity is 27.733478012857194
At time: 176.28941535949707 and batch: 800, loss is 3.288713827133179 and perplexity is 26.808361285758995
At time: 177.18832087516785 and batch: 850, loss is 3.334071593284607 and perplexity is 28.052327161441895
At time: 178.0868215560913 and batch: 900, loss is 3.273020887374878 and perplexity is 26.390943112577876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320220111167594 and perplexity of 75.20517997032404
finished 10 epochs...
Completing Train Step...
At time: 180.29010653495789 and batch: 50, loss is 3.6324062824249266 and perplexity is 37.80367357491248
At time: 181.18822503089905 and batch: 100, loss is 3.5237331628799438 and perplexity is 33.9107869497659
At time: 182.08687472343445 and batch: 150, loss is 3.542726788520813 and perplexity is 34.5610314442476
At time: 182.9959840774536 and batch: 200, loss is 3.4230248022079466 and perplexity is 30.66202144254765
At time: 183.92227482795715 and batch: 250, loss is 3.564577589035034 and perplexity is 35.32452878404935
At time: 184.81660795211792 and batch: 300, loss is 3.523682670593262 and perplexity is 33.90907475981618
At time: 185.7263433933258 and batch: 350, loss is 3.514470682144165 and perplexity is 33.598139121412906
At time: 186.63414096832275 and batch: 400, loss is 3.44182345867157 and perplexity is 31.243878177491926
At time: 187.5118110179901 and batch: 450, loss is 3.4718772983551025 and perplexity is 32.197129362879195
At time: 188.38176941871643 and batch: 500, loss is 3.343960709571838 and perplexity is 28.331116102998973
At time: 189.25945711135864 and batch: 550, loss is 3.3835952711105346 and perplexity is 29.476557048777217
At time: 190.14140272140503 and batch: 600, loss is 3.4098167848587035 and perplexity is 30.259699716334353
At time: 191.02020359039307 and batch: 650, loss is 3.248478627204895 and perplexity is 25.751133027376586
At time: 191.90910482406616 and batch: 700, loss is 3.2332286834716797 and perplexity is 25.361408886105515
At time: 192.80278491973877 and batch: 750, loss is 3.3205475854873656 and perplexity is 27.675501112791288
At time: 193.7024326324463 and batch: 800, loss is 3.289037628173828 and perplexity is 26.81704326658278
At time: 194.60539484024048 and batch: 850, loss is 3.337594609260559 and perplexity is 28.15133025052733
At time: 195.50420808792114 and batch: 900, loss is 3.278966565132141 and perplexity is 26.548322556136487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319801644103168 and perplexity of 75.17371566327948
finished 11 epochs...
Completing Train Step...
At time: 197.67471075057983 and batch: 50, loss is 3.625493121147156 and perplexity is 37.54323195729688
At time: 198.58911108970642 and batch: 100, loss is 3.5152597618103028 and perplexity is 33.62466119245101
At time: 199.4875524044037 and batch: 150, loss is 3.5333101844787596 and perplexity is 34.23711140279923
At time: 200.39372777938843 and batch: 200, loss is 3.4134064865112306 and perplexity is 30.36851820651555
At time: 201.29879879951477 and batch: 250, loss is 3.5549131298065184 and perplexity is 34.98478070101624
At time: 202.19798684120178 and batch: 300, loss is 3.514293322563171 and perplexity is 33.592180697943526
At time: 203.10245633125305 and batch: 350, loss is 3.5053077268600465 and perplexity is 33.291687022586736
At time: 203.99868083000183 and batch: 400, loss is 3.4333295011520386 and perplexity is 30.979617900095914
At time: 204.90080308914185 and batch: 450, loss is 3.4637553882598877 and perplexity is 31.936686252319436
At time: 205.80594515800476 and batch: 500, loss is 3.3365172243118284 and perplexity is 28.12101676360816
At time: 206.7274603843689 and batch: 550, loss is 3.376912178993225 and perplexity is 29.28021930509663
At time: 207.6289300918579 and batch: 600, loss is 3.4042562770843507 and perplexity is 30.09190735850142
At time: 208.5231373310089 and batch: 650, loss is 3.2437910413742066 and perplexity is 25.63070486027669
At time: 209.4168291091919 and batch: 700, loss is 3.2297691392898558 and perplexity is 25.27382156501127
At time: 210.31035947799683 and batch: 750, loss is 3.318458204269409 and perplexity is 27.617736807410292
At time: 211.21319317817688 and batch: 800, loss is 3.2882187747955323 and perplexity is 26.795093028347907
At time: 212.11193084716797 and batch: 850, loss is 3.338140377998352 and perplexity is 28.16669855989528
At time: 213.01752924919128 and batch: 900, loss is 3.280503149032593 and perplexity is 26.58914763870991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320308737558861 and perplexity of 75.21184542939281
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 215.23250579833984 and batch: 50, loss is 3.6219452095031737 and perplexity is 37.41026789932331
At time: 216.1422061920166 and batch: 100, loss is 3.514225287437439 and perplexity is 33.58989532744974
At time: 217.0403711795807 and batch: 150, loss is 3.533173751831055 and perplexity is 34.23244066166882
At time: 217.94270396232605 and batch: 200, loss is 3.413987789154053 and perplexity is 30.386176638356893
At time: 218.84427094459534 and batch: 250, loss is 3.5545681953430175 and perplexity is 34.97271532545599
At time: 219.7472140789032 and batch: 300, loss is 3.5127994775772096 and perplexity is 33.54203665028248
At time: 220.64755988121033 and batch: 350, loss is 3.504176964759827 and perplexity is 33.25406332038636
At time: 221.5455391407013 and batch: 400, loss is 3.4315810060501097 and perplexity is 30.925497518337405
At time: 222.44585824012756 and batch: 450, loss is 3.4612426471710207 and perplexity is 31.856538366144626
At time: 223.34324598312378 and batch: 500, loss is 3.33181254863739 and perplexity is 27.989027227534077
At time: 224.24489068984985 and batch: 550, loss is 3.372292881011963 and perplexity is 29.145277156116443
At time: 225.14179491996765 and batch: 600, loss is 3.3997671890258787 and perplexity is 29.957124888054636
At time: 226.04145216941833 and batch: 650, loss is 3.2387232112884523 and perplexity is 25.501141383207298
At time: 226.94274997711182 and batch: 700, loss is 3.2224870538711547 and perplexity is 25.090443933662307
At time: 227.84374380111694 and batch: 750, loss is 3.3102670240402223 and perplexity is 27.392438935140515
At time: 228.77062439918518 and batch: 800, loss is 3.2784655475616455 and perplexity is 26.535024711573733
At time: 229.66924786567688 and batch: 850, loss is 3.3270896291732788 and perplexity is 27.857148976378504
At time: 230.56614208221436 and batch: 900, loss is 3.2714066696166992 and perplexity is 26.34837674848381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319197981324915 and perplexity of 75.12834978345472
finished 13 epochs...
Completing Train Step...
At time: 232.77565693855286 and batch: 50, loss is 3.618820743560791 and perplexity is 37.29356320623794
At time: 233.6745662689209 and batch: 100, loss is 3.5097836685180663 and perplexity is 33.44103265322859
At time: 234.57675051689148 and batch: 150, loss is 3.5288911390304567 and perplexity is 34.08614985013695
At time: 235.4733681678772 and batch: 200, loss is 3.409293742179871 and perplexity is 30.243876740340653
At time: 236.37587928771973 and batch: 250, loss is 3.5508384609222414 and perplexity is 34.84251933448191
At time: 237.27391242980957 and batch: 300, loss is 3.5091180086135862 and perplexity is 33.41877970589602
At time: 238.1683166027069 and batch: 350, loss is 3.5005139923095703 and perplexity is 33.13247742143066
At time: 239.06037831306458 and batch: 400, loss is 3.4282768249511717 and perplexity is 30.8234827044851
At time: 239.95287990570068 and batch: 450, loss is 3.458344678878784 and perplexity is 31.764352768045505
At time: 240.8566358089447 and batch: 500, loss is 3.3293587303161623 and perplexity is 27.920431434955326
At time: 241.75453639030457 and batch: 550, loss is 3.3702559518814086 and perplexity is 29.085970714074385
At time: 242.65673351287842 and batch: 600, loss is 3.397914628982544 and perplexity is 29.90167888985171
At time: 243.5586051940918 and batch: 650, loss is 3.2372817850112914 and perplexity is 25.46440984718035
At time: 244.46281623840332 and batch: 700, loss is 3.221871089935303 and perplexity is 25.074993883889885
At time: 245.36319613456726 and batch: 750, loss is 3.3100696802139282 and perplexity is 27.387033739788034
At time: 246.26401352882385 and batch: 800, loss is 3.2789332818984986 and perplexity is 26.547438956818635
At time: 247.1648769378662 and batch: 850, loss is 3.3284430599212644 and perplexity is 27.894877223880332
At time: 248.05928778648376 and batch: 900, loss is 3.273501296043396 and perplexity is 26.403624596322988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318922068974743 and perplexity of 75.10762380330988
finished 14 epochs...
Completing Train Step...
At time: 250.27869653701782 and batch: 50, loss is 3.6167481565475463 and perplexity is 37.21634909558075
At time: 251.183522939682 and batch: 100, loss is 3.507213830947876 and perplexity is 33.35520495998
At time: 252.08284163475037 and batch: 150, loss is 3.5261810779571534 and perplexity is 33.993899360979746
At time: 252.98157835006714 and batch: 200, loss is 3.406532678604126 and perplexity is 30.16048664953157
At time: 253.878901720047 and batch: 250, loss is 3.548106327056885 and perplexity is 34.74745483106294
At time: 254.7794098854065 and batch: 300, loss is 3.506556301116943 and perplexity is 33.333280126607846
At time: 255.67674279212952 and batch: 350, loss is 3.497966079711914 and perplexity is 33.04816621921551
At time: 256.5741708278656 and batch: 400, loss is 3.425952310562134 and perplexity is 30.75191628622774
At time: 257.476744890213 and batch: 450, loss is 3.4561829566955566 and perplexity is 31.695761226670175
At time: 258.37368273735046 and batch: 500, loss is 3.327515139579773 and perplexity is 27.869005005416195
At time: 259.27328395843506 and batch: 550, loss is 3.3686242437362672 and perplexity is 29.03854959798302
At time: 260.1714823246002 and batch: 600, loss is 3.396570978164673 and perplexity is 29.861528454678798
At time: 261.0687973499298 and batch: 650, loss is 3.2362196159362795 and perplexity is 25.437376697956374
At time: 261.96930408477783 and batch: 700, loss is 3.221270351409912 and perplexity is 25.05993489275084
At time: 262.86498165130615 and batch: 750, loss is 3.3097568464279177 and perplexity is 27.378467490311607
At time: 263.7675087451935 and batch: 800, loss is 3.2790226411819456 and perplexity is 26.54981132293606
At time: 264.6669135093689 and batch: 850, loss is 3.329114785194397 and perplexity is 27.913621212603225
At time: 265.56280517578125 and batch: 900, loss is 3.2745284175872804 and perplexity is 26.430758260382515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318933356298159 and perplexity of 75.10847157213529
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 267.76625180244446 and batch: 50, loss is 3.615593514442444 and perplexity is 37.17340233075965
At time: 268.67130947113037 and batch: 100, loss is 3.5065718030929567 and perplexity is 33.333796862322025
At time: 269.5724983215332 and batch: 150, loss is 3.5258216524124144 and perplexity is 33.98168328070129
At time: 270.4727041721344 and batch: 200, loss is 3.4064023733139037 and perplexity is 30.156556834608853
At time: 271.37501215934753 and batch: 250, loss is 3.548257169723511 and perplexity is 34.752696625141276
At time: 272.28105998039246 and batch: 300, loss is 3.5064545345306395 and perplexity is 33.32988808508023
At time: 273.18352341651917 and batch: 350, loss is 3.4976109409332277 and perplexity is 33.03643161765597
At time: 274.0962359905243 and batch: 400, loss is 3.4250725746154784 and perplexity is 30.724874616550682
At time: 274.99546122550964 and batch: 450, loss is 3.4555639028549194 and perplexity is 31.676145916044
At time: 275.89496636390686 and batch: 500, loss is 3.325907464027405 and perplexity is 27.824236683510915
At time: 276.7933397293091 and batch: 550, loss is 3.366909189224243 and perplexity is 28.988789585236614
At time: 277.693808555603 and batch: 600, loss is 3.3954728651046753 and perplexity is 29.828755117999055
At time: 278.5934431552887 and batch: 650, loss is 3.2351102018356324 and perplexity is 25.40917176193394
At time: 279.49683928489685 and batch: 700, loss is 3.218977346420288 and perplexity is 25.002538167639365
At time: 280.3994333744049 and batch: 750, loss is 3.3068277740478518 and perplexity is 27.29839130909345
At time: 281.29894256591797 and batch: 800, loss is 3.2756325387954712 and perplexity is 26.459957137709903
At time: 282.2045564651489 and batch: 850, loss is 3.3255869579315185 and perplexity is 27.81532027499843
At time: 283.1011664867401 and batch: 900, loss is 3.270928912162781 and perplexity is 26.33579162165203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318274929098887 and perplexity of 75.05903438873558
finished 16 epochs...
Completing Train Step...
At time: 285.32293820381165 and batch: 50, loss is 3.6148989391326904 and perplexity is 37.147591568118074
At time: 286.225070476532 and batch: 100, loss is 3.5055834245681763 and perplexity is 33.300866729752606
At time: 287.1261546611786 and batch: 150, loss is 3.524825439453125 and perplexity is 33.947847144235766
At time: 288.02570486068726 and batch: 200, loss is 3.405298390388489 and perplexity is 30.123282881091875
At time: 288.92612743377686 and batch: 250, loss is 3.547313780784607 and perplexity is 34.719926775335345
At time: 289.82524943351746 and batch: 300, loss is 3.505437593460083 and perplexity is 33.2960107815397
At time: 290.72476387023926 and batch: 350, loss is 3.496637377738953 and perplexity is 33.004284215065994
At time: 291.6188156604767 and batch: 400, loss is 3.4243107271194457 and perplexity is 30.701475862031156
At time: 292.5193259716034 and batch: 450, loss is 3.454857497215271 and perplexity is 31.65377760940572
At time: 293.4184834957123 and batch: 500, loss is 3.325307946205139 and perplexity is 27.8075605570452
At time: 294.31131744384766 and batch: 550, loss is 3.3664506006240846 and perplexity is 28.97549870455654
At time: 295.2028636932373 and batch: 600, loss is 3.3950176525115965 and perplexity is 29.8151797830996
At time: 296.10753178596497 and batch: 650, loss is 3.2347656440734864 and perplexity is 25.40041834268954
At time: 296.9997327327728 and batch: 700, loss is 3.218903489112854 and perplexity is 25.000691615682797
At time: 297.89982509613037 and batch: 750, loss is 3.3068833446502683 and perplexity is 27.299908339294248
At time: 298.80183815956116 and batch: 800, loss is 3.2758587646484374 and perplexity is 26.46594374121954
At time: 299.7064538002014 and batch: 850, loss is 3.3260095071792604 and perplexity is 27.82707610118917
At time: 300.60761308670044 and batch: 900, loss is 3.2715484428405763 and perplexity is 26.35211250760842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3179822947880995 and perplexity of 75.03707275345955
finished 17 epochs...
Completing Train Step...
At time: 302.800430059433 and batch: 50, loss is 3.614323592185974 and perplexity is 37.12622496192672
At time: 303.70259308815 and batch: 100, loss is 3.5048196125030517 and perplexity is 33.27544083750309
At time: 304.605592250824 and batch: 150, loss is 3.524037103652954 and perplexity is 33.92109538706264
At time: 305.5024893283844 and batch: 200, loss is 3.4044679832458495 and perplexity is 30.098278675100158
At time: 306.4058213233948 and batch: 250, loss is 3.5465439128875733 and perplexity is 34.69320730487463
At time: 307.30447936058044 and batch: 300, loss is 3.5046789503097533 and perplexity is 33.27076057018796
At time: 308.2113082408905 and batch: 350, loss is 3.4958799123764037 and perplexity is 32.979294078733545
At time: 309.1093919277191 and batch: 400, loss is 3.4236763525009155 and perplexity is 30.682005801301987
At time: 310.0082941055298 and batch: 450, loss is 3.4542745685577394 and perplexity is 31.63533109234214
At time: 310.9013650417328 and batch: 500, loss is 3.32481041431427 and perplexity is 27.79372885000379
At time: 311.8089587688446 and batch: 550, loss is 3.366055293083191 and perplexity is 28.964046735091546
At time: 312.708589553833 and batch: 600, loss is 3.394654154777527 and perplexity is 29.804344002317755
At time: 313.6105098724365 and batch: 650, loss is 3.234485068321228 and perplexity is 25.393292600907287
At time: 314.51300525665283 and batch: 700, loss is 3.218804759979248 and perplexity is 24.998223440902414
At time: 315.4120526313782 and batch: 750, loss is 3.3068876457214356 and perplexity is 27.30002575839539
At time: 316.3120584487915 and batch: 800, loss is 3.2759961700439453 and perplexity is 26.469580554539956
At time: 317.2142901420593 and batch: 850, loss is 3.3263090801239015 and perplexity is 27.835413589098106
At time: 318.10739517211914 and batch: 900, loss is 3.271999115943909 and perplexity is 26.36399137247298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317841412269906 and perplexity of 75.02650208632076
finished 18 epochs...
Completing Train Step...
At time: 320.31557035446167 and batch: 50, loss is 3.6137960290908815 and perplexity is 37.10664370140606
At time: 321.22548270225525 and batch: 100, loss is 3.5041585111618043 and perplexity is 33.25344966893128
At time: 322.1332845687866 and batch: 150, loss is 3.5233480882644654 and perplexity is 33.897731280385415
At time: 323.0269064903259 and batch: 200, loss is 3.4037591314315794 and perplexity is 30.076951015623006
At time: 323.92495584487915 and batch: 250, loss is 3.5458598852157595 and perplexity is 34.6694843055731
At time: 324.8247663974762 and batch: 300, loss is 3.504024987220764 and perplexity is 33.249009833697045
At time: 325.7226254940033 and batch: 350, loss is 3.4952214193344116 and perplexity is 32.95758459161045
At time: 326.620596408844 and batch: 400, loss is 3.42310302734375 and perplexity is 30.664420077154542
At time: 327.51883935928345 and batch: 450, loss is 3.4537458705902098 and perplexity is 31.61860997769065
At time: 328.4115867614746 and batch: 500, loss is 3.324358820915222 and perplexity is 27.781180219172896
At time: 329.3117311000824 and batch: 550, loss is 3.365681505203247 and perplexity is 28.953222348606115
At time: 330.2128174304962 and batch: 600, loss is 3.394326982498169 and perplexity is 29.794594442135633
At time: 331.1046779155731 and batch: 650, loss is 3.2342289972305296 and perplexity is 25.386790945253104
At time: 332.00003480911255 and batch: 700, loss is 3.218687071800232 and perplexity is 24.995281618619273
At time: 332.8975965976715 and batch: 750, loss is 3.306856622695923 and perplexity is 27.299178842136815
At time: 333.7999334335327 and batch: 800, loss is 3.276075291633606 and perplexity is 26.47167495268604
At time: 334.6943578720093 and batch: 850, loss is 3.3265297698974607 and perplexity is 27.84155725811754
At time: 335.595867395401 and batch: 900, loss is 3.2723366355895998 and perplexity is 26.372891239355486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317775360525471 and perplexity of 75.02154661863955
finished 19 epochs...
Completing Train Step...
At time: 337.8226773738861 and batch: 50, loss is 3.613295874595642 and perplexity is 37.08808928717918
At time: 338.7199053764343 and batch: 100, loss is 3.5035563039779665 and perplexity is 33.233430231182886
At time: 339.6264159679413 and batch: 150, loss is 3.5227163219451905 and perplexity is 33.87632259881668
At time: 340.5248248577118 and batch: 200, loss is 3.4031161880493164 and perplexity is 30.057619454224533
At time: 341.4391601085663 and batch: 250, loss is 3.545227017402649 and perplexity is 34.64755004633439
At time: 342.334929227829 and batch: 300, loss is 3.5034243202209474 and perplexity is 33.229044247648
At time: 343.23945593833923 and batch: 350, loss is 3.4946184396743774 and perplexity is 32.937717828686765
At time: 344.13931155204773 and batch: 400, loss is 3.4225662326812745 and perplexity is 30.64796399729228
At time: 345.04109358787537 and batch: 450, loss is 3.453248829841614 and perplexity is 31.602898145151748
At time: 345.93915152549744 and batch: 500, loss is 3.3239322900772095 and perplexity is 27.76933321582877
At time: 346.8365340232849 and batch: 550, loss is 3.3653180170059205 and perplexity is 28.942700106479084
At time: 347.7363202571869 and batch: 600, loss is 3.3940182161331176 and perplexity is 29.78539629362407
At time: 348.6368992328644 and batch: 650, loss is 3.233984823226929 and perplexity is 25.380592907600054
At time: 349.53860545158386 and batch: 700, loss is 3.218556532859802 and perplexity is 24.992018973996746
At time: 350.43774008750916 and batch: 750, loss is 3.3068020009994505 and perplexity is 27.297687755399366
At time: 351.3365807533264 and batch: 800, loss is 3.2761162996292112 and perplexity is 26.472760525274595
At time: 352.2358589172363 and batch: 850, loss is 3.3266974306106567 and perplexity is 27.84622558480048
At time: 353.1362416744232 and batch: 900, loss is 3.2725974559783935 and perplexity is 26.37977072421811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.317750695633562 and perplexity of 75.01969624312107
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
731.2758815288544


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3862347602844238 and batch: 50, loss is 7.094779205322266 and perplexity is 1205.6561316435107
At time: 2.510918140411377 and batch: 100, loss is 6.438490104675293 and perplexity is 625.4617047899172
At time: 3.6353774070739746 and batch: 150, loss is 5.990190143585205 and perplexity is 399.4905632738167
At time: 4.758498668670654 and batch: 200, loss is 5.808643217086792 and perplexity is 333.1667838795339
At time: 5.8829429149627686 and batch: 250, loss is 5.782712306976318 and perplexity is 324.63851669037956
At time: 7.009345293045044 and batch: 300, loss is 5.630319890975952 and perplexity is 278.751273389111
At time: 8.133705139160156 and batch: 350, loss is 5.574197235107422 and perplexity is 263.5379117147617
At time: 9.2569580078125 and batch: 400, loss is 5.39835355758667 and perplexity is 221.04218321656418
At time: 10.381938934326172 and batch: 450, loss is 5.371744871139526 and perplexity is 215.23810299456014
At time: 11.503765106201172 and batch: 500, loss is 5.298236684799194 and perplexity is 199.9838643011685
At time: 12.627059698104858 and batch: 550, loss is 5.339380235671997 and perplexity is 208.3835215867337
At time: 13.754940032958984 and batch: 600, loss is 5.23557557106018 and perplexity is 187.8371888865721
At time: 14.8809072971344 and batch: 650, loss is 5.1194869422912594 and perplexity is 167.24953893970414
At time: 16.011579990386963 and batch: 700, loss is 5.203742532730103 and perplexity is 181.9519302580971
At time: 17.133968114852905 and batch: 750, loss is 5.176302890777588 and perplexity is 177.0271110781544
At time: 18.25726556777954 and batch: 800, loss is 5.143763198852539 and perplexity is 171.35941603235028
At time: 19.379575967788696 and batch: 850, loss is 5.173689346313477 and perplexity is 176.56504692761496
At time: 20.503700017929077 and batch: 900, loss is 5.080604238510132 and perplexity is 160.87123116018142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.071972572640197 and perplexity of 159.48862014721874
finished 1 epochs...
Completing Train Step...
At time: 22.865936756134033 and batch: 50, loss is 5.020839405059815 and perplexity is 151.53845256576602
At time: 23.764840602874756 and batch: 100, loss is 4.906569499969482 and perplexity is 135.1749006060501
At time: 24.659282207489014 and batch: 150, loss is 4.890414276123047 and perplexity is 133.00866495312067
At time: 25.5568745136261 and batch: 200, loss is 4.77241325378418 and perplexity is 118.2041546626152
At time: 26.457380533218384 and batch: 250, loss is 4.868946104049683 and perplexity is 130.1836445579865
At time: 27.356395721435547 and batch: 300, loss is 4.8093548774719235 and perplexity is 122.65246611981746
At time: 28.255967378616333 and batch: 350, loss is 4.802009181976318 and perplexity is 121.75479948125533
At time: 29.157825469970703 and batch: 400, loss is 4.661234340667725 and perplexity is 105.7665535753681
At time: 30.06008553504944 and batch: 450, loss is 4.684025220870971 and perplexity is 108.20474515646684
At time: 30.955320596694946 and batch: 500, loss is 4.584042644500732 and perplexity is 97.9094081490324
At time: 31.855660915374756 and batch: 550, loss is 4.647961750030517 and perplexity is 104.37203233315032
At time: 32.75288653373718 and batch: 600, loss is 4.5969373321533205 and perplexity is 99.18009432938241
At time: 33.64941906929016 and batch: 650, loss is 4.465217189788818 and perplexity is 86.93990995004394
At time: 34.547767877578735 and batch: 700, loss is 4.505792598724366 and perplexity is 90.54007756714995
At time: 35.446356534957886 and batch: 750, loss is 4.54992151260376 and perplexity is 94.62498115506853
At time: 36.34693789482117 and batch: 800, loss is 4.498614234924316 and perplexity is 89.89247509580838
At time: 37.24348449707031 and batch: 850, loss is 4.557894325256347 and perplexity is 95.38242386420029
At time: 38.14850735664368 and batch: 900, loss is 4.485926828384399 and perplexity is 88.75917722701718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.62385496374679 and perplexity of 101.88604305240408
finished 2 epochs...
Completing Train Step...
At time: 40.36339735984802 and batch: 50, loss is 4.544305591583252 and perplexity is 94.09506411319663
At time: 41.27226448059082 and batch: 100, loss is 4.416679697036743 and perplexity is 82.82083805313835
At time: 42.17197823524475 and batch: 150, loss is 4.4205044078826905 and perplexity is 83.13821035246067
At time: 43.07039999961853 and batch: 200, loss is 4.306359663009643 and perplexity is 74.16999312156179
At time: 43.966801166534424 and batch: 250, loss is 4.44684760093689 and perplexity is 85.35743872143505
At time: 44.866591453552246 and batch: 300, loss is 4.399661440849304 and perplexity is 81.42329739556557
At time: 45.765236377716064 and batch: 350, loss is 4.40625714302063 and perplexity is 81.96211620582808
At time: 46.66419267654419 and batch: 400, loss is 4.299489088058472 and perplexity is 73.66214921223445
At time: 47.560144662857056 and batch: 450, loss is 4.3389600563049315 and perplexity is 76.62780928096728
At time: 48.457966804504395 and batch: 500, loss is 4.212403082847596 and perplexity is 67.51859779700418
At time: 49.355616331100464 and batch: 550, loss is 4.289510068893432 and perplexity is 72.93072871271
At time: 50.256985664367676 and batch: 600, loss is 4.28091456413269 and perplexity is 72.3065387438454
At time: 51.160820960998535 and batch: 650, loss is 4.132014932632447 and perplexity is 62.30333356525562
At time: 52.0595645904541 and batch: 700, loss is 4.147063522338867 and perplexity is 63.24800099834045
At time: 52.964160203933716 and batch: 750, loss is 4.234872689247132 and perplexity is 69.0528869988573
At time: 53.862942934036255 and batch: 800, loss is 4.1907067346572875 and perplexity is 66.06946804716712
At time: 54.76220107078552 and batch: 850, loss is 4.267721624374389 and perplexity is 71.35886796126336
At time: 55.66222548484802 and batch: 900, loss is 4.202228021621704 and perplexity is 66.83507526950362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.448271032882063 and perplexity of 85.47902574136394
finished 3 epochs...
Completing Train Step...
At time: 57.8555212020874 and batch: 50, loss is 4.278760728836059 and perplexity is 72.15096996342723
At time: 58.77107834815979 and batch: 100, loss is 4.154475231170654 and perplexity is 63.71851828055545
At time: 59.673277854919434 and batch: 150, loss is 4.160469541549682 and perplexity is 64.10161390595509
At time: 60.57579946517944 and batch: 200, loss is 4.046847629547119 and perplexity is 57.21680389022246
At time: 61.479504108428955 and batch: 250, loss is 4.19284432888031 and perplexity is 66.21084881390364
At time: 62.39516544342041 and batch: 300, loss is 4.156331014633179 and perplexity is 63.83687584207181
At time: 63.29898691177368 and batch: 350, loss is 4.161336174011231 and perplexity is 64.15719052417595
At time: 64.20407700538635 and batch: 400, loss is 4.076889009475708 and perplexity is 58.96175476973143
At time: 65.10582756996155 and batch: 450, loss is 4.1142391061782835 and perplexity is 61.20562555812948
At time: 66.00301456451416 and batch: 500, loss is 3.984262857437134 and perplexity is 53.74565666107223
At time: 66.9078049659729 and batch: 550, loss is 4.065577788352966 and perplexity is 58.29858303551682
At time: 67.80502319335938 and batch: 600, loss is 4.075256495475769 and perplexity is 58.865577406415156
At time: 68.70457887649536 and batch: 650, loss is 3.91861741065979 and perplexity is 50.330809809748516
At time: 69.5965964794159 and batch: 700, loss is 3.929409236907959 and perplexity is 50.87691258721191
At time: 70.49611830711365 and batch: 750, loss is 4.025435876846314 and perplexity is 56.004714620675216
At time: 71.39141511917114 and batch: 800, loss is 3.9889169216156004 and perplexity is 53.99637537442437
At time: 72.28985285758972 and batch: 850, loss is 4.065611453056335 and perplexity is 58.300545673057165
At time: 73.19282913208008 and batch: 900, loss is 4.00943621635437 and perplexity is 55.11578842235716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385328737023759 and perplexity of 80.26460482366028
finished 4 epochs...
Completing Train Step...
At time: 75.40236043930054 and batch: 50, loss is 4.092496252059936 and perplexity is 59.889203814670054
At time: 76.30143165588379 and batch: 100, loss is 3.966781048774719 and perplexity is 52.81425042841019
At time: 77.20000576972961 and batch: 150, loss is 3.9737644577026368 and perplexity is 53.18436476210092
At time: 78.10021376609802 and batch: 200, loss is 3.8629249286651612 and perplexity is 47.604387371745936
At time: 78.99858403205872 and batch: 250, loss is 4.009373602867126 and perplexity is 55.1123375386789
At time: 79.90049076080322 and batch: 300, loss is 3.976896300315857 and perplexity is 53.35119092230003
At time: 80.79938507080078 and batch: 350, loss is 3.981229000091553 and perplexity is 53.58284710118641
At time: 81.70297598838806 and batch: 400, loss is 3.9097343921661376 and perplexity is 49.88570018083007
At time: 82.60283541679382 and batch: 450, loss is 3.941915683746338 and perplexity is 51.51719748870262
At time: 83.50522255897522 and batch: 500, loss is 3.816594123840332 and perplexity is 45.44915023914949
At time: 84.41946983337402 and batch: 550, loss is 3.8921238136291505 and perplexity is 49.014874517616946
At time: 85.32016396522522 and batch: 600, loss is 3.90987060546875 and perplexity is 49.892495739617075
At time: 86.22170209884644 and batch: 650, loss is 3.7592290019989014 and perplexity is 42.915325589989536
At time: 87.12403631210327 and batch: 700, loss is 3.767453966140747 and perplexity is 43.269758203759665
At time: 88.03142189979553 and batch: 750, loss is 3.865802040100098 and perplexity is 47.74154771719035
At time: 88.93355512619019 and batch: 800, loss is 3.834195837974548 and perplexity is 46.256215215817676
At time: 89.83659863471985 and batch: 850, loss is 3.9075728368759157 and perplexity is 49.777985938782265
At time: 90.7378294467926 and batch: 900, loss is 3.8555826854705813 and perplexity is 47.25614438930521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370089596264983 and perplexity of 79.05071403100592
finished 5 epochs...
Completing Train Step...
At time: 92.92697763442993 and batch: 50, loss is 3.943458442687988 and perplexity is 51.59673744549073
At time: 93.83116483688354 and batch: 100, loss is 3.8192828702926636 and perplexity is 45.571515912032154
At time: 94.73050904273987 and batch: 150, loss is 3.824733691215515 and perplexity is 45.82059631407366
At time: 95.6303436756134 and batch: 200, loss is 3.7169483947753905 and perplexity is 41.13866340576356
At time: 96.5279016494751 and batch: 250, loss is 3.8629156494140626 and perplexity is 47.60394564073159
At time: 97.4311249256134 and batch: 300, loss is 3.8301455307006838 and perplexity is 46.06924223548614
At time: 98.33218288421631 and batch: 350, loss is 3.8355660104751585 and perplexity is 46.319637649803894
At time: 99.23231673240662 and batch: 400, loss is 3.770779809951782 and perplexity is 43.4139062353347
At time: 100.13483929634094 and batch: 450, loss is 3.804538779258728 and perplexity is 44.90453443347225
At time: 101.03135299682617 and batch: 500, loss is 3.6782751417160036 and perplexity is 39.578068604140846
At time: 101.9370186328888 and batch: 550, loss is 3.751903944015503 and perplexity is 42.602116878125784
At time: 102.83989810943604 and batch: 600, loss is 3.774047703742981 and perplexity is 43.55601033406149
At time: 103.73793411254883 and batch: 650, loss is 3.6258487367630003 and perplexity is 37.55658529103644
At time: 104.64210939407349 and batch: 700, loss is 3.6337285470962524 and perplexity is 37.85369309916404
At time: 105.54076647758484 and batch: 750, loss is 3.7377547788619996 and perplexity is 42.003576896605644
At time: 106.44159722328186 and batch: 800, loss is 3.7031830739974976 and perplexity is 40.57625624566385
At time: 107.34723091125488 and batch: 850, loss is 3.7775794649124146 and perplexity is 43.710111724528424
At time: 108.24916291236877 and batch: 900, loss is 3.7280080938339233 and perplexity is 41.596169921501904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.370746351268194 and perplexity of 79.10264803504494
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.43307614326477 and batch: 50, loss is 3.847361850738525 and perplexity is 46.86925190470955
At time: 111.33366966247559 and batch: 100, loss is 3.727141752243042 and perplexity is 41.56014903492658
At time: 112.22881865501404 and batch: 150, loss is 3.7273203039169314 and perplexity is 41.56757033162661
At time: 113.12728571891785 and batch: 200, loss is 3.618533143997192 and perplexity is 37.282839135927674
At time: 114.02848505973816 and batch: 250, loss is 3.7494293785095216 and perplexity is 42.496825478179666
At time: 114.9288854598999 and batch: 300, loss is 3.7096754360198974 and perplexity is 40.84054900451503
At time: 115.83142232894897 and batch: 350, loss is 3.7022910261154176 and perplexity is 40.54007642168402
At time: 116.73026037216187 and batch: 400, loss is 3.630903763771057 and perplexity is 37.746915500891774
At time: 117.63194441795349 and batch: 450, loss is 3.6607030630111694 and perplexity is 38.88867445085514
At time: 118.52767181396484 and batch: 500, loss is 3.5244951009750367 and perplexity is 33.936634716125944
At time: 119.42686152458191 and batch: 550, loss is 3.578133292198181 and perplexity is 35.80663789204621
At time: 120.32474160194397 and batch: 600, loss is 3.602736301422119 and perplexity is 36.698515381499824
At time: 121.22309851646423 and batch: 650, loss is 3.4346461296081543 and perplexity is 31.02043341011757
At time: 122.12091374397278 and batch: 700, loss is 3.4293967056274415 and perplexity is 30.858020662727608
At time: 123.02559089660645 and batch: 750, loss is 3.517595157623291 and perplexity is 33.70327985252512
At time: 123.92628383636475 and batch: 800, loss is 3.4724344873428343 and perplexity is 32.21507424767861
At time: 124.8246419429779 and batch: 850, loss is 3.523045539855957 and perplexity is 33.887477127001596
At time: 125.72644972801208 and batch: 900, loss is 3.46818953037262 and perplexity is 32.07861248577477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316443874411387 and perplexity of 74.92172294272666
finished 7 epochs...
Completing Train Step...
At time: 127.93835926055908 and batch: 50, loss is 3.747540678977966 and perplexity is 42.416637493148436
At time: 128.83779644966125 and batch: 100, loss is 3.6253706121444704 and perplexity is 37.53863285511367
At time: 129.7527792453766 and batch: 150, loss is 3.623291687965393 and perplexity is 37.46067394702888
At time: 130.6551284790039 and batch: 200, loss is 3.519715542793274 and perplexity is 33.77481960641706
At time: 131.55635404586792 and batch: 250, loss is 3.650701308250427 and perplexity is 38.50165811353721
At time: 132.46444582939148 and batch: 300, loss is 3.61525887966156 and perplexity is 37.16096489853075
At time: 133.373854637146 and batch: 350, loss is 3.6117412662506103 and perplexity is 37.030476628235455
At time: 134.27292561531067 and batch: 400, loss is 3.546947569847107 and perplexity is 34.70721428626936
At time: 135.17561626434326 and batch: 450, loss is 3.579902033805847 and perplexity is 35.87002662493548
At time: 136.07730054855347 and batch: 500, loss is 3.448280920982361 and perplexity is 31.44628716411645
At time: 136.97532987594604 and batch: 550, loss is 3.5039272689819336 and perplexity is 33.24576095775304
At time: 137.87729787826538 and batch: 600, loss is 3.53536461353302 and perplexity is 34.30752142062742
At time: 138.77799940109253 and batch: 650, loss is 3.3718000411987306 and perplexity is 29.13091674214876
At time: 139.6829845905304 and batch: 700, loss is 3.373134903907776 and perplexity is 29.16982848172528
At time: 140.5841841697693 and batch: 750, loss is 3.467365369796753 and perplexity is 32.05218544959202
At time: 141.48778128623962 and batch: 800, loss is 3.427788758277893 and perplexity is 30.808442460442155
At time: 142.39503645896912 and batch: 850, loss is 3.4827630710601807 and perplexity is 32.5495346166293
At time: 143.29769349098206 and batch: 900, loss is 3.4368564319610595 and perplexity is 31.08907377710958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32505233973673 and perplexity of 75.56946804228124
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.49036574363708 and batch: 50, loss is 3.709164958000183 and perplexity is 40.8197061223044
At time: 146.40091514587402 and batch: 100, loss is 3.602768177986145 and perplexity is 36.69968522272021
At time: 147.30401945114136 and batch: 150, loss is 3.6005639743804934 and perplexity is 36.61888073171702
At time: 148.2020354270935 and batch: 200, loss is 3.4936429119110106 and perplexity is 32.905601838050146
At time: 149.10100197792053 and batch: 250, loss is 3.630167593955994 and perplexity is 37.719137586973666
At time: 150.00175595283508 and batch: 300, loss is 3.5925033044815065 and perplexity is 36.324894476942205
At time: 150.8973958492279 and batch: 350, loss is 3.5834790229797364 and perplexity is 35.99856307152859
At time: 151.80001211166382 and batch: 400, loss is 3.5152153301239015 and perplexity is 33.62316722523954
At time: 152.6948778629303 and batch: 450, loss is 3.5403396797180178 and perplexity is 34.478628892910635
At time: 153.59454035758972 and batch: 500, loss is 3.4078830766677854 and perplexity is 30.2012428246386
At time: 154.4944715499878 and batch: 550, loss is 3.450658302307129 and perplexity is 31.521135916713614
At time: 155.401837348938 and batch: 600, loss is 3.488844566345215 and perplexity is 32.74808759548318
At time: 156.3033537864685 and batch: 650, loss is 3.3156775522232054 and perplexity is 27.54104816267691
At time: 157.20362639427185 and batch: 700, loss is 3.308681969642639 and perplexity is 27.349054821519058
At time: 158.1063301563263 and batch: 750, loss is 3.4037207078933718 and perplexity is 30.075795374948534
At time: 159.00647687911987 and batch: 800, loss is 3.3559575366973875 and perplexity is 28.67304654437163
At time: 159.90888667106628 and batch: 850, loss is 3.4043878173828124 and perplexity is 30.095865917325934
At time: 160.8065538406372 and batch: 900, loss is 3.363282446861267 and perplexity is 28.883845132273212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31155897166631 and perplexity of 74.55663006026495
finished 9 epochs...
Completing Train Step...
At time: 163.0081741809845 and batch: 50, loss is 3.6798337936401366 and perplexity is 39.63980513730873
At time: 163.91664028167725 and batch: 100, loss is 3.5628124284744263 and perplexity is 35.262230318590554
At time: 164.81208276748657 and batch: 150, loss is 3.561359724998474 and perplexity is 35.211041943804005
At time: 165.71110081672668 and batch: 200, loss is 3.455224871635437 and perplexity is 31.665408533922307
At time: 166.61402916908264 and batch: 250, loss is 3.5910778856277465 and perplexity is 36.273153172769916
At time: 167.51585292816162 and batch: 300, loss is 3.554917097091675 and perplexity is 34.98491949589273
At time: 168.41705298423767 and batch: 350, loss is 3.548292474746704 and perplexity is 34.75392359156057
At time: 169.3165180683136 and batch: 400, loss is 3.482497463226318 and perplexity is 32.54089035328939
At time: 170.21728515625 and batch: 450, loss is 3.510438013076782 and perplexity is 33.46292177171483
At time: 171.1177363395691 and batch: 500, loss is 3.3796981239318846 and perplexity is 29.361906118520444
At time: 172.01754546165466 and batch: 550, loss is 3.4263736724853517 and perplexity is 30.76487670313808
At time: 172.91290402412415 and batch: 600, loss is 3.4674975061416626 and perplexity is 32.056420988051826
At time: 173.8132050037384 and batch: 650, loss is 3.297543931007385 and perplexity is 27.04613011743484
At time: 174.7227120399475 and batch: 700, loss is 3.2939443683624265 and perplexity is 26.948950884070626
At time: 175.6205337047577 and batch: 750, loss is 3.3926254224777224 and perplexity is 29.74394025919537
At time: 176.52473425865173 and batch: 800, loss is 3.3485624980926514 and perplexity is 28.461790344875315
At time: 177.42387413978577 and batch: 850, loss is 3.4002458381652834 and perplexity is 29.971467272312456
At time: 178.3199553489685 and batch: 900, loss is 3.3617607879638673 and perplexity is 28.83992719487081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314421771323844 and perplexity of 74.77037656599799
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 180.5236999988556 and batch: 50, loss is 3.668737645149231 and perplexity is 39.20238728812053
At time: 181.4275336265564 and batch: 100, loss is 3.5572248458862306 and perplexity is 35.06574913306299
At time: 182.32663798332214 and batch: 150, loss is 3.5581614446640013 and perplexity is 35.0986070557858
At time: 183.2282907962799 and batch: 200, loss is 3.451136336326599 and perplexity is 31.5362076941383
At time: 184.12719106674194 and batch: 250, loss is 3.5881175565719605 and perplexity is 36.16593148752623
At time: 185.0260510444641 and batch: 300, loss is 3.552630567550659 and perplexity is 34.90501682869217
At time: 185.92811942100525 and batch: 350, loss is 3.5450020599365235 and perplexity is 34.63975669788734
At time: 186.83041858673096 and batch: 400, loss is 3.4802344703674315 and perplexity is 32.46733381115396
At time: 187.72596383094788 and batch: 450, loss is 3.501897277832031 and perplexity is 33.17834081153954
At time: 188.62333869934082 and batch: 500, loss is 3.3681799221038817 and perplexity is 29.025650008219152
At time: 189.52059531211853 and batch: 550, loss is 3.411492657661438 and perplexity is 30.310453640789948
At time: 190.42327427864075 and batch: 600, loss is 3.454373779296875 and perplexity is 31.638469812617412
At time: 191.3151490688324 and batch: 650, loss is 3.2808677434921263 and perplexity is 26.59884366207349
At time: 192.20950746536255 and batch: 700, loss is 3.2724134397506712 and perplexity is 26.374916864929396
At time: 193.09928393363953 and batch: 750, loss is 3.367058582305908 and perplexity is 28.993120633352746
At time: 193.99302434921265 and batch: 800, loss is 3.3231704902648924 and perplexity is 27.748186598763922
At time: 194.89227318763733 and batch: 850, loss is 3.371336498260498 and perplexity is 29.11741644063515
At time: 195.79021573066711 and batch: 900, loss is 3.333940405845642 and perplexity is 28.04864728986632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313109933513484 and perplexity of 74.67235426765983
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 197.99244689941406 and batch: 50, loss is 3.6619268751144407 and perplexity is 38.93629601530812
At time: 198.90361070632935 and batch: 100, loss is 3.548587164878845 and perplexity is 34.76416673909931
At time: 199.80716562271118 and batch: 150, loss is 3.551556797027588 and perplexity is 34.86755696576174
At time: 200.7075092792511 and batch: 200, loss is 3.4438056898117066 and perplexity is 31.30587218868462
At time: 201.60821628570557 and batch: 250, loss is 3.581777100563049 and perplexity is 35.93734841614163
At time: 202.50926780700684 and batch: 300, loss is 3.545118227005005 and perplexity is 34.643780930612984
At time: 203.4109170436859 and batch: 350, loss is 3.5376949882507325 and perplexity is 34.387564029585974
At time: 204.31513261795044 and batch: 400, loss is 3.4755492496490477 and perplexity is 32.315572980218036
At time: 205.21778678894043 and batch: 450, loss is 3.495420870780945 and perplexity is 32.964158685115926
At time: 206.1184298992157 and batch: 500, loss is 3.361792187690735 and perplexity is 28.840832774925016
At time: 207.01945686340332 and batch: 550, loss is 3.4057746648788454 and perplexity is 30.13763324937984
At time: 207.92237854003906 and batch: 600, loss is 3.4492852163314818 and perplexity is 31.47788438787915
At time: 208.81852841377258 and batch: 650, loss is 3.2752345514297487 and perplexity is 26.449428504342038
At time: 209.71988224983215 and batch: 700, loss is 3.266055397987366 and perplexity is 26.20775601295598
At time: 210.62285447120667 and batch: 750, loss is 3.3586670351028443 and perplexity is 28.750841463274043
At time: 211.52456831932068 and batch: 800, loss is 3.3144737577438352 and perplexity is 27.50791434809539
At time: 212.42641401290894 and batch: 850, loss is 3.3622585916519165 and perplexity is 28.85428739097423
At time: 213.3294324874878 and batch: 900, loss is 3.325074510574341 and perplexity is 27.801070039194144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3130514066513275 and perplexity of 74.66798405696329
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 215.53328800201416 and batch: 50, loss is 3.6598844575881957 and perplexity is 38.85685299744158
At time: 216.44239473342896 and batch: 100, loss is 3.5462841987609863 and perplexity is 34.68419815879284
At time: 217.34468579292297 and batch: 150, loss is 3.5493863582611085 and perplexity is 34.79196113616682
At time: 218.24514508247375 and batch: 200, loss is 3.4422495126724244 and perplexity is 31.257192592920234
At time: 219.14586687088013 and batch: 250, loss is 3.5798478174209594 and perplexity is 35.86808193448357
At time: 220.05769538879395 and batch: 300, loss is 3.543306841850281 and perplexity is 34.58108450096557
At time: 220.96016097068787 and batch: 350, loss is 3.53526207447052 and perplexity is 34.30400373989717
At time: 221.86621832847595 and batch: 400, loss is 3.473362283706665 and perplexity is 32.24497714618134
At time: 222.76512742042542 and batch: 450, loss is 3.4933082675933838 and perplexity is 32.89459200766828
At time: 223.66251707077026 and batch: 500, loss is 3.3598316144943237 and perplexity is 28.784343604895287
At time: 224.55802130699158 and batch: 550, loss is 3.4031894636154174 and perplexity is 30.059822024001978
At time: 225.46062898635864 and batch: 600, loss is 3.4470607900619505 and perplexity is 31.407941974648164
At time: 226.35719227790833 and batch: 650, loss is 3.273469843864441 and perplexity is 26.402794157856743
At time: 227.27312922477722 and batch: 700, loss is 3.2641134786605837 and perplexity is 26.15691204845941
At time: 228.17145824432373 and batch: 750, loss is 3.356436676979065 and perplexity is 28.686788247808376
At time: 229.07550597190857 and batch: 800, loss is 3.312246923446655 and perplexity is 27.44672693338261
At time: 229.9868712425232 and batch: 850, loss is 3.359734854698181 and perplexity is 28.78155857241774
At time: 230.88808703422546 and batch: 900, loss is 3.322334232330322 and perplexity is 27.724991657379398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312343649668236 and perplexity of 74.61515596676371
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 233.0739941596985 and batch: 50, loss is 3.6589961147308347 and perplexity is 38.82235011708445
At time: 233.9721019268036 and batch: 100, loss is 3.5454458951950074 and perplexity is 34.655134455598265
At time: 234.88344478607178 and batch: 150, loss is 3.5486754035949706 and perplexity is 34.76723441988165
At time: 235.81490015983582 and batch: 200, loss is 3.441666884422302 and perplexity is 31.238986573681665
At time: 236.71267294883728 and batch: 250, loss is 3.579303240776062 and perplexity is 35.8485543323854
At time: 237.6057059764862 and batch: 300, loss is 3.5429088306427 and perplexity is 34.567323580445844
At time: 238.50839519500732 and batch: 350, loss is 3.534574451446533 and perplexity is 34.280423625154704
At time: 239.40373873710632 and batch: 400, loss is 3.472706689834595 and perplexity is 32.22384446474439
At time: 240.31048464775085 and batch: 450, loss is 3.4928323793411256 and perplexity is 32.87894158199123
At time: 241.20516419410706 and batch: 500, loss is 3.359311809539795 and perplexity is 28.769385248527296
At time: 242.11489701271057 and batch: 550, loss is 3.4024296760559083 and perplexity is 30.036991619414064
At time: 243.0152142047882 and batch: 600, loss is 3.4463767623901367 and perplexity is 31.386465419339242
At time: 243.91546773910522 and batch: 650, loss is 3.2729479694366455 and perplexity is 26.389018809577056
At time: 244.81309461593628 and batch: 700, loss is 3.2635678911209105 and perplexity is 26.142645055470144
At time: 245.70972228050232 and batch: 750, loss is 3.355798749923706 and perplexity is 28.66849400527029
At time: 246.61078333854675 and batch: 800, loss is 3.3116609621047974 and perplexity is 27.430648923445293
At time: 247.5020031929016 and batch: 850, loss is 3.359070053100586 and perplexity is 28.76243090505391
At time: 248.39661693572998 and batch: 900, loss is 3.3216370344161987 and perplexity is 27.7056685878114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312137133454623 and perplexity of 74.5997483182947
Annealing...
Model not improving. Stopping early with 74.55663006026495 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
988.2708780765533


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.55663006026495}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.8545719625116199, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.1564310047238061, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3891856670379639 and batch: 50, loss is 7.105470161437989 and perplexity is 1218.6148957987527
At time: 2.515153408050537 and batch: 100, loss is 6.395306749343872 and perplexity is 599.0270461964051
At time: 3.636634588241577 and batch: 150, loss is 6.005799989700318 and perplexity is 405.77547512595214
At time: 4.762386083602905 and batch: 200, loss is 5.836198291778564 and perplexity is 342.47487309286805
At time: 5.890352010726929 and batch: 250, loss is 5.870704669952392 and perplexity is 354.4986968533796
At time: 7.0191123485565186 and batch: 300, loss is 5.731972064971924 and perplexity is 308.57720303030055
At time: 8.150340557098389 and batch: 350, loss is 5.689643182754517 and perplexity is 295.7880595279436
At time: 9.272973537445068 and batch: 400, loss is 5.537953815460205 and perplexity is 254.15741406196702
At time: 10.399978876113892 and batch: 450, loss is 5.52601845741272 and perplexity is 251.14198523235314
At time: 11.524595260620117 and batch: 500, loss is 5.456619863510132 and perplexity is 234.30410450250346
At time: 12.646010875701904 and batch: 550, loss is 5.496499919891358 and perplexity is 243.83698795893105
At time: 13.771042108535767 and batch: 600, loss is 5.399628839492798 and perplexity is 221.32425413505177
At time: 14.899195432662964 and batch: 650, loss is 5.288404817581177 and perplexity is 198.02728368317875
At time: 16.02397608757019 and batch: 700, loss is 5.369530601501465 and perplexity is 214.76203506404042
At time: 17.14900517463684 and batch: 750, loss is 5.337867469787597 and perplexity is 208.06852442295934
At time: 18.26802921295166 and batch: 800, loss is 5.320973424911499 and perplexity is 204.5829312192291
At time: 19.389729499816895 and batch: 850, loss is 5.353341064453125 and perplexity is 211.31313055427538
At time: 20.51236128807068 and batch: 900, loss is 5.238734169006348 and perplexity is 188.43142903441006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.2263814847763275 and perplexity of 186.11811234328613
finished 1 epochs...
Completing Train Step...
At time: 22.865317344665527 and batch: 50, loss is 5.156583423614502 and perplexity is 173.57042479256353
At time: 23.774070024490356 and batch: 100, loss is 5.041517896652222 and perplexity is 154.70466258488688
At time: 24.675652027130127 and batch: 150, loss is 5.0200135612487795 and perplexity is 151.41335713433443
At time: 25.576051235198975 and batch: 200, loss is 4.890587959289551 and perplexity is 133.03176832549755
At time: 26.47612190246582 and batch: 250, loss is 4.972520141601563 and perplexity is 144.39031327154566
At time: 27.375901699066162 and batch: 300, loss is 4.907748517990112 and perplexity is 135.33436823893516
At time: 28.274154901504517 and batch: 350, loss is 4.89620813369751 and perplexity is 133.78153500149588
At time: 29.17917013168335 and batch: 400, loss is 4.751857690811157 and perplexity is 115.79920395558409
At time: 30.086360454559326 and batch: 450, loss is 4.7707083034515385 and perplexity is 118.00279415341433
At time: 30.989997386932373 and batch: 500, loss is 4.6713133144378665 and perplexity is 106.8379621742266
At time: 31.887986421585083 and batch: 550, loss is 4.738632116317749 and perplexity is 114.27777601343823
At time: 32.791351318359375 and batch: 600, loss is 4.67020902633667 and perplexity is 106.72004740176419
At time: 33.69899392127991 and batch: 650, loss is 4.53967978477478 and perplexity is 93.66080370172381
At time: 34.6007866859436 and batch: 700, loss is 4.585405473709106 and perplexity is 98.04293291529197
At time: 35.501614809036255 and batch: 750, loss is 4.617439556121826 and perplexity is 101.23449476352945
At time: 36.40196657180786 and batch: 800, loss is 4.569752149581909 and perplexity is 96.52018423997237
At time: 37.30671572685242 and batch: 850, loss is 4.6208281898498536 and perplexity is 101.57812327368553
At time: 38.207907915115356 and batch: 900, loss is 4.545482749938965 and perplexity is 94.20589412358709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.662227264822346 and perplexity of 105.87162389596139
finished 2 epochs...
Completing Train Step...
At time: 40.42676544189453 and batch: 50, loss is 4.595903530120849 and perplexity is 99.07761472722157
At time: 41.329166650772095 and batch: 100, loss is 4.4764165019989015 and perplexity is 87.91904975729402
At time: 42.22892618179321 and batch: 150, loss is 4.475608682632446 and perplexity is 87.84805572525742
At time: 43.131345987319946 and batch: 200, loss is 4.362700114250183 and perplexity is 78.46872315524293
At time: 44.02985072135925 and batch: 250, loss is 4.490943527221679 and perplexity is 89.20557407125929
At time: 44.931838512420654 and batch: 300, loss is 4.4582546615600585 and perplexity is 86.33669077492098
At time: 45.82980394363403 and batch: 350, loss is 4.463549089431763 and perplexity is 86.7950063456912
At time: 46.73461627960205 and batch: 400, loss is 4.353993635177613 and perplexity is 77.78850232035266
At time: 47.630003690719604 and batch: 450, loss is 4.391852264404297 and perplexity is 80.7899247774904
At time: 48.53699731826782 and batch: 500, loss is 4.269374189376831 and perplexity is 71.4768906222698
At time: 49.44005751609802 and batch: 550, loss is 4.344916739463806 and perplexity is 77.08561902213113
At time: 50.343310594558716 and batch: 600, loss is 4.325749087333679 and perplexity is 75.62213923467499
At time: 51.24414324760437 and batch: 650, loss is 4.178763093948365 and perplexity is 65.28505177679429
At time: 52.14074158668518 and batch: 700, loss is 4.1972235631942745 and perplexity is 66.50143744844453
At time: 53.04558968544006 and batch: 750, loss is 4.282063641548157 and perplexity is 72.38967230881019
At time: 53.941490173339844 and batch: 800, loss is 4.23900906085968 and perplexity is 69.33910694834934
At time: 54.84445285797119 and batch: 850, loss is 4.3063292312622075 and perplexity is 74.16773603340754
At time: 55.741469621658325 and batch: 900, loss is 4.244705371856689 and perplexity is 69.7352111611097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.460570871013484 and perplexity of 86.53689640385083
finished 3 epochs...
Completing Train Step...
At time: 57.94363284111023 and batch: 50, loss is 4.32190050125122 and perplexity is 75.33166024732314
At time: 58.84311628341675 and batch: 100, loss is 4.196886739730835 and perplexity is 66.4790419758315
At time: 59.741023778915405 and batch: 150, loss is 4.202612872123718 and perplexity is 66.86080173187669
At time: 60.63731908798218 and batch: 200, loss is 4.088372540473938 and perplexity is 59.642746519675086
At time: 61.54400110244751 and batch: 250, loss is 4.230872731208802 and perplexity is 68.7772300241371
At time: 62.44457983970642 and batch: 300, loss is 4.201369342803955 and perplexity is 66.777710038766
At time: 63.343841552734375 and batch: 350, loss is 4.207524299621582 and perplexity is 67.18999144600963
At time: 64.2482397556305 and batch: 400, loss is 4.116725897789001 and perplexity is 61.35802060311995
At time: 65.15032935142517 and batch: 450, loss is 4.162405757904053 and perplexity is 64.22584873308104
At time: 66.05115079879761 and batch: 500, loss is 4.025701766014099 and perplexity is 56.019607647497274
At time: 66.95359325408936 and batch: 550, loss is 4.1014407348632815 and perplexity is 60.4272846081287
At time: 67.85451745986938 and batch: 600, loss is 4.107440514564514 and perplexity is 60.79092479338238
At time: 68.75226998329163 and batch: 650, loss is 3.9550239753723146 and perplexity is 52.19694537062885
At time: 69.65516138076782 and batch: 700, loss is 3.9654234981536867 and perplexity is 52.7426010547714
At time: 70.55703496932983 and batch: 750, loss is 4.0646627187728885 and perplexity is 58.24526017640337
At time: 71.45678353309631 and batch: 800, loss is 4.027892961502075 and perplexity is 56.14249214181991
At time: 72.3556797504425 and batch: 850, loss is 4.098433566093445 and perplexity is 60.24584251523943
At time: 73.25758838653564 and batch: 900, loss is 4.0421504259109495 and perplexity is 56.94867513162226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382111431801156 and perplexity of 80.00678405781218
finished 4 epochs...
Completing Train Step...
At time: 75.44231820106506 and batch: 50, loss is 4.127074398994446 and perplexity is 61.99628097640585
At time: 76.34714007377625 and batch: 100, loss is 4.0020614433288575 and perplexity is 54.71081711380024
At time: 77.24851250648499 and batch: 150, loss is 4.01165738105774 and perplexity is 55.238345725790445
At time: 78.15675497055054 and batch: 200, loss is 3.9000684213638306 and perplexity is 49.40582940411554
At time: 79.05542063713074 and batch: 250, loss is 4.043122978210449 and perplexity is 57.00408763798751
At time: 79.9542486667633 and batch: 300, loss is 4.015580539703369 and perplexity is 55.455480167329554
At time: 80.84998273849487 and batch: 350, loss is 4.0205188703536985 and perplexity is 55.7300149786797
At time: 81.75299978256226 and batch: 400, loss is 3.9442204809188843 and perplexity is 51.636071116991
At time: 82.65169501304626 and batch: 450, loss is 3.9857664823532106 and perplexity is 53.82653075647391
At time: 83.55892634391785 and batch: 500, loss is 3.8524062919616697 and perplexity is 47.10627842165999
At time: 84.46030688285828 and batch: 550, loss is 3.922651243209839 and perplexity is 50.5342459063476
At time: 85.35797691345215 and batch: 600, loss is 3.942669892311096 and perplexity is 51.55606685624338
At time: 86.25821447372437 and batch: 650, loss is 3.7883076906204223 and perplexity is 44.181568092152666
At time: 87.15740275382996 and batch: 700, loss is 3.7978115129470824 and perplexity is 44.603463499354866
At time: 88.05154299736023 and batch: 750, loss is 3.902587399482727 and perplexity is 49.53043848521514
At time: 88.94707584381104 and batch: 800, loss is 3.8668319940567017 and perplexity is 47.79074464409179
At time: 89.83814883232117 and batch: 850, loss is 3.9375766801834104 and perplexity is 51.294148440498894
At time: 90.73836159706116 and batch: 900, loss is 3.8865973472595217 and perplexity is 48.744742587025264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358626692262415 and perplexity of 78.14973705655605
finished 5 epochs...
Completing Train Step...
At time: 92.92801570892334 and batch: 50, loss is 3.969930763244629 and perplexity is 52.98086248966976
At time: 93.83838939666748 and batch: 100, loss is 3.8492050266265867 and perplexity is 46.95571984304153
At time: 94.73881649971008 and batch: 150, loss is 3.8586999940872193 and perplexity is 47.40368622263374
At time: 95.6379246711731 and batch: 200, loss is 3.7512905311584475 and perplexity is 42.57599220531922
At time: 96.54327392578125 and batch: 250, loss is 3.893377590179443 and perplexity is 49.07636675861231
At time: 97.4464910030365 and batch: 300, loss is 3.8671119928359987 and perplexity is 47.80412786780955
At time: 98.3523919582367 and batch: 350, loss is 3.873309302330017 and perplexity is 48.10130474069591
At time: 99.25385761260986 and batch: 400, loss is 3.805702028274536 and perplexity is 44.95679998198765
At time: 100.16202282905579 and batch: 450, loss is 3.8391052532196044 and perplexity is 46.483864539266065
At time: 101.06681275367737 and batch: 500, loss is 3.7148447322845457 and perplexity is 41.05221250627226
At time: 101.9709722995758 and batch: 550, loss is 3.7785495853424074 and perplexity is 43.75253637209105
At time: 102.87306547164917 and batch: 600, loss is 3.802325644493103 and perplexity is 44.805264536531745
At time: 103.77362751960754 and batch: 650, loss is 3.6565431118011475 and perplexity is 38.72723548443999
At time: 104.67574524879456 and batch: 700, loss is 3.6608148574829102 and perplexity is 38.89302223269654
At time: 105.57529044151306 and batch: 750, loss is 3.7677931928634645 and perplexity is 43.284438951938164
At time: 106.4994957447052 and batch: 800, loss is 3.7331227016448976 and perplexity is 41.809463007365785
At time: 107.4008321762085 and batch: 850, loss is 3.8022780084609984 and perplexity is 44.80313024234688
At time: 108.298983335495 and batch: 900, loss is 3.75629478931427 and perplexity is 42.78958745870113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.357085763591609 and perplexity of 78.02940662022934
finished 6 epochs...
Completing Train Step...
At time: 110.49657487869263 and batch: 50, loss is 3.83820848941803 and perplexity is 46.44219817742478
At time: 111.39552450180054 and batch: 100, loss is 3.7219834089279176 and perplexity is 41.34631949520526
At time: 112.29731678962708 and batch: 150, loss is 3.7324338150024414 and perplexity is 41.780670945143925
At time: 113.20451045036316 and batch: 200, loss is 3.6262154626846312 and perplexity is 37.57036079015253
At time: 114.10838055610657 and batch: 250, loss is 3.7675867319107055 and perplexity is 43.27550332789298
At time: 115.00834345817566 and batch: 300, loss is 3.7433133363723754 and perplexity is 42.23770630247528
At time: 115.90827488899231 and batch: 350, loss is 3.750933165550232 and perplexity is 42.560779728339334
At time: 116.80803346633911 and batch: 400, loss is 3.6860168743133546 and perplexity is 39.885660538970754
At time: 117.71216368675232 and batch: 450, loss is 3.7180956697463987 and perplexity is 41.1858878491608
At time: 118.61832523345947 and batch: 500, loss is 3.594932451248169 and perplexity is 36.41324023587535
At time: 119.52012181282043 and batch: 550, loss is 3.6597129917144775 and perplexity is 38.85019094436613
At time: 120.41994380950928 and batch: 600, loss is 3.685358076095581 and perplexity is 39.85939259048183
At time: 121.32558369636536 and batch: 650, loss is 3.5424910259246825 and perplexity is 34.55288420619408
At time: 122.22719979286194 and batch: 700, loss is 3.545218300819397 and perplexity is 34.64724803939618
At time: 123.13023471832275 and batch: 750, loss is 3.6528464412689208 and perplexity is 38.584337939530734
At time: 124.03615117073059 and batch: 800, loss is 3.61938588142395 and perplexity is 37.314645167404045
At time: 124.93727087974548 and batch: 850, loss is 3.6866477346420288 and perplexity is 39.91083075849946
At time: 125.841068983078 and batch: 900, loss is 3.64393470287323 and perplexity is 38.24201203867766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37168319911173 and perplexity of 79.17678990467714
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 128.05165123939514 and batch: 50, loss is 3.7591511487960814 and perplexity is 42.91198462449641
At time: 128.97086191177368 and batch: 100, loss is 3.6512710762023928 and perplexity is 38.52360137511781
At time: 129.8697156906128 and batch: 150, loss is 3.663804588317871 and perplexity is 39.009475896341655
At time: 130.7702431678772 and batch: 200, loss is 3.546050887107849 and perplexity is 34.67610687511476
At time: 131.6693456172943 and batch: 250, loss is 3.676974115371704 and perplexity is 39.52660997600433
At time: 132.56891250610352 and batch: 300, loss is 3.653263487815857 and perplexity is 38.60043276034568
At time: 133.4718635082245 and batch: 350, loss is 3.6429354333877564 and perplexity is 38.203817049706046
At time: 134.36873841285706 and batch: 400, loss is 3.5733770990371703 and perplexity is 35.63673896212835
At time: 135.27228569984436 and batch: 450, loss is 3.5906322717666628 and perplexity is 36.256992953807114
At time: 136.17074346542358 and batch: 500, loss is 3.4592403984069824 and perplexity is 31.792817465409932
At time: 137.07007241249084 and batch: 550, loss is 3.499329099655151 and perplexity is 33.09324224163988
At time: 137.9710612297058 and batch: 600, loss is 3.52582239151001 and perplexity is 33.98170839649098
At time: 138.86778688430786 and batch: 650, loss is 3.3694745779037474 and perplexity is 29.06325257028245
At time: 139.76800966262817 and batch: 700, loss is 3.352364139556885 and perplexity is 28.5701977997715
At time: 140.6660258769989 and batch: 750, loss is 3.4472417974472047 and perplexity is 31.413627558652035
At time: 141.56990957260132 and batch: 800, loss is 3.406318063735962 and perplexity is 30.154014455204926
At time: 142.47015929222107 and batch: 850, loss is 3.4516976928710936 and perplexity is 31.55391572050918
At time: 143.37432289123535 and batch: 900, loss is 3.4007590866088866 and perplexity is 29.9868540295193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325383852605951 and perplexity of 75.59452444648818
finished 8 epochs...
Completing Train Step...
At time: 145.57146668434143 and batch: 50, loss is 3.6748405122756957 and perplexity is 39.44236578237945
At time: 146.48586106300354 and batch: 100, loss is 3.550245313644409 and perplexity is 34.82185871698609
At time: 147.38322687149048 and batch: 150, loss is 3.5620821142196655 and perplexity is 35.236487210557804
At time: 148.2857415676117 and batch: 200, loss is 3.446399493217468 and perplexity is 31.38717886777384
At time: 149.1828258037567 and batch: 250, loss is 3.579722595214844 and perplexity is 35.8635907353394
At time: 150.0869812965393 and batch: 300, loss is 3.5606694793701172 and perplexity is 35.18674606206012
At time: 150.98178458213806 and batch: 350, loss is 3.552861557006836 and perplexity is 34.91308045081726
At time: 151.89547324180603 and batch: 400, loss is 3.4887750053405764 and perplexity is 32.74580968483783
At time: 152.7915358543396 and batch: 450, loss is 3.508062777519226 and perplexity is 33.383533769989114
At time: 153.69636416435242 and batch: 500, loss is 3.3834796857833864 and perplexity is 29.473150188182874
At time: 154.59609746932983 and batch: 550, loss is 3.4266271686553953 and perplexity is 30.772676470117947
At time: 155.5001256465912 and batch: 600, loss is 3.4610776901245117 and perplexity is 31.851283839061296
At time: 156.40941953659058 and batch: 650, loss is 3.309871206283569 and perplexity is 27.3815986669388
At time: 157.31439995765686 and batch: 700, loss is 3.2970056772232055 and perplexity is 27.03157635271256
At time: 158.21916151046753 and batch: 750, loss is 3.3992133855819704 and perplexity is 29.940539122173075
At time: 159.12464356422424 and batch: 800, loss is 3.36240626335144 and perplexity is 28.85854866725901
At time: 160.0204315185547 and batch: 850, loss is 3.4151855754852294 and perplexity is 30.42259459150023
At time: 160.92415833473206 and batch: 900, loss is 3.3704468774795533 and perplexity is 29.091524500593724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337151096291738 and perplexity of 76.48931793849557
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 163.158540725708 and batch: 50, loss is 3.6419637393951416 and perplexity is 38.16671266015957
At time: 164.05689239501953 and batch: 100, loss is 3.535675573348999 and perplexity is 34.318191340047186
At time: 164.96045470237732 and batch: 150, loss is 3.552605128288269 and perplexity is 34.90412888210474
At time: 165.86326432228088 and batch: 200, loss is 3.435261969566345 and perplexity is 31.039542916126653
At time: 166.76886892318726 and batch: 250, loss is 3.5664375972747804 and perplexity is 35.39029384146695
At time: 167.67319059371948 and batch: 300, loss is 3.5410440492630006 and perplexity is 34.50292314411646
At time: 168.579772233963 and batch: 350, loss is 3.5272837495803833 and perplexity is 34.03140414309739
At time: 169.48098731040955 and batch: 400, loss is 3.46470064163208 and perplexity is 31.966888784970166
At time: 170.38711023330688 and batch: 450, loss is 3.4775337171554566 and perplexity is 32.37976585802178
At time: 171.2839274406433 and batch: 500, loss is 3.350497455596924 and perplexity is 28.516916015403613
At time: 172.188086271286 and batch: 550, loss is 3.385879545211792 and perplexity is 29.543966546211173
At time: 173.08842658996582 and batch: 600, loss is 3.422647433280945 and perplexity is 30.65045273138952
At time: 174.00381422042847 and batch: 650, loss is 3.262319493293762 and perplexity is 26.11002899733104
At time: 174.90291500091553 and batch: 700, loss is 3.2470784091949465 and perplexity is 25.715101059327424
At time: 175.8069703578949 and batch: 750, loss is 3.336202220916748 and perplexity is 28.11215994289205
At time: 176.708651304245 and batch: 800, loss is 3.2952996635437013 and perplexity is 26.98549942878304
At time: 177.61192321777344 and batch: 850, loss is 3.3519168424606325 and perplexity is 28.55742129091698
At time: 178.511394739151 and batch: 900, loss is 3.3051236963272093 and perplexity is 27.25191234178802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327319001498288 and perplexity of 75.74095274131842
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 180.7223026752472 and batch: 50, loss is 3.6279508781433107 and perplexity is 37.635617582501396
At time: 181.6345353126526 and batch: 100, loss is 3.518237671852112 and perplexity is 33.72494164764888
At time: 182.53822231292725 and batch: 150, loss is 3.5335776376724244 and perplexity is 34.24626945220596
At time: 183.43928456306458 and batch: 200, loss is 3.4189573764801025 and perplexity is 30.537559239639
At time: 184.34271955490112 and batch: 250, loss is 3.551934151649475 and perplexity is 34.88071688235819
At time: 185.2441647052765 and batch: 300, loss is 3.5262622928619383 and perplexity is 33.99666028439216
At time: 186.14732360839844 and batch: 350, loss is 3.5096367311477663 and perplexity is 33.43611927681855
At time: 187.04200267791748 and batch: 400, loss is 3.448737773895264 and perplexity is 31.460656774156526
At time: 187.9433867931366 and batch: 450, loss is 3.459674515724182 and perplexity is 31.80662227427073
At time: 188.84323525428772 and batch: 500, loss is 3.337733097076416 and perplexity is 28.155229136735077
At time: 189.74253368377686 and batch: 550, loss is 3.3707419061660766 and perplexity is 29.100108601071774
At time: 190.63883352279663 and batch: 600, loss is 3.4086686468124388 and perplexity is 30.22497734067607
At time: 191.53302121162415 and batch: 650, loss is 3.2450440073013307 and perplexity is 25.66283938768469
At time: 192.42961025238037 and batch: 700, loss is 3.2288578939437866 and perplexity is 25.250801402833535
At time: 193.33038902282715 and batch: 750, loss is 3.311700077056885 and perplexity is 27.431721892948104
At time: 194.22961521148682 and batch: 800, loss is 3.269417486190796 and perplexity is 26.296017087905717
At time: 195.13264966011047 and batch: 850, loss is 3.3261265754699707 and perplexity is 27.83033396011607
At time: 196.03232336044312 and batch: 900, loss is 3.2810120725631715 and perplexity is 26.602682925522117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326067362746147 and perplexity of 75.64621173287055
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 198.24593210220337 and batch: 50, loss is 3.6197044897079467 and perplexity is 37.326535816597854
At time: 199.1494162082672 and batch: 100, loss is 3.5093641090393066 and perplexity is 33.42700509390293
At time: 200.05274748802185 and batch: 150, loss is 3.524232110977173 and perplexity is 33.92771089412384
At time: 200.9457974433899 and batch: 200, loss is 3.4091945695877075 and perplexity is 30.240877525409662
At time: 201.8519275188446 and batch: 250, loss is 3.5427200746536256 and perplexity is 34.56079940685155
At time: 202.7538938522339 and batch: 300, loss is 3.516631851196289 and perplexity is 33.67082889904759
At time: 203.65964674949646 and batch: 350, loss is 3.501084818840027 and perplexity is 33.15139571758154
At time: 204.56080198287964 and batch: 400, loss is 3.442087197303772 and perplexity is 31.25211948191455
At time: 205.46312499046326 and batch: 450, loss is 3.4521126794815062 and perplexity is 31.567012890426394
At time: 206.36252880096436 and batch: 500, loss is 3.331407685279846 and perplexity is 27.97769778958825
At time: 207.26100516319275 and batch: 550, loss is 3.3633147621154786 and perplexity is 28.88477853615278
At time: 208.15995907783508 and batch: 600, loss is 3.4017179918289187 and perplexity is 30.015622371232695
At time: 209.060320854187 and batch: 650, loss is 3.2378129863739016 and perplexity is 25.477940169733966
At time: 209.95978951454163 and batch: 700, loss is 3.2212005710601805 and perplexity is 25.05818626274049
At time: 210.8598952293396 and batch: 750, loss is 3.3040650987625124 and perplexity is 27.223078798001932
At time: 211.7636148929596 and batch: 800, loss is 3.262019567489624 and perplexity is 26.102199100140755
At time: 212.6592767238617 and batch: 850, loss is 3.31681574344635 and perplexity is 27.572412988170374
At time: 213.55979800224304 and batch: 900, loss is 3.2713076496124267 and perplexity is 26.345767861273742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.326073215432363 and perplexity of 75.64665446770682
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 215.77697801589966 and batch: 50, loss is 3.6167871284484865 and perplexity is 37.217799515713686
At time: 216.67955350875854 and batch: 100, loss is 3.506576280593872 and perplexity is 33.33394611476213
At time: 217.58017921447754 and batch: 150, loss is 3.5220982933044436 and perplexity is 33.85539252956475
At time: 218.48027658462524 and batch: 200, loss is 3.407006092071533 and perplexity is 30.17476841041724
At time: 219.3928713798523 and batch: 250, loss is 3.5402207517623903 and perplexity is 34.47452866388432
At time: 220.29376602172852 and batch: 300, loss is 3.5138338899612425 and perplexity is 33.57675089970859
At time: 221.19331645965576 and batch: 350, loss is 3.4986276626586914 and perplexity is 33.070037556462545
At time: 222.0960385799408 and batch: 400, loss is 3.4403292322158814 and perplexity is 31.197227610076588
At time: 223.00427627563477 and batch: 450, loss is 3.450184350013733 and perplexity is 31.50619994180481
At time: 223.89869689941406 and batch: 500, loss is 3.3290074014663698 and perplexity is 27.91062390482859
At time: 224.80004000663757 and batch: 550, loss is 3.360869517326355 and perplexity is 28.814234465862295
At time: 225.69686818122864 and batch: 600, loss is 3.3995469570159913 and perplexity is 29.950528096669068
At time: 226.59986901283264 and batch: 650, loss is 3.235361571311951 and perplexity is 25.415559654960447
At time: 227.496568441391 and batch: 700, loss is 3.2188776254653932 and perplexity is 25.00004501497034
At time: 228.39431476593018 and batch: 750, loss is 3.3020649671554567 and perplexity is 27.16868347469355
At time: 229.29463839530945 and batch: 800, loss is 3.2594345092773436 and perplexity is 26.034810535207097
At time: 230.1893174648285 and batch: 850, loss is 3.314465298652649 and perplexity is 27.507681657123758
At time: 231.08990478515625 and batch: 900, loss is 3.2684019899368284 and perplexity is 26.269327135125845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325277668156036 and perplexity of 75.58649790964752
finished 13 epochs...
Completing Train Step...
At time: 233.28382515907288 and batch: 50, loss is 3.6156259536743165 and perplexity is 37.17460822693642
At time: 234.194810628891 and batch: 100, loss is 3.5052459144592287 and perplexity is 33.28962924708327
At time: 235.09470224380493 and batch: 150, loss is 3.5207791709899903 and perplexity is 33.81076256847487
At time: 235.99994492530823 and batch: 200, loss is 3.4056336212158205 and perplexity is 30.133382826946068
At time: 236.897691488266 and batch: 250, loss is 3.5387407541275024 and perplexity is 34.42354418076145
At time: 237.799320936203 and batch: 300, loss is 3.5123526859283447 and perplexity is 33.527053695799935
At time: 238.70186257362366 and batch: 350, loss is 3.4973678302764895 and perplexity is 33.028401085262665
At time: 239.600923538208 and batch: 400, loss is 3.439086637496948 and perplexity is 31.15848617473929
At time: 240.50490808486938 and batch: 450, loss is 3.4490993642807006 and perplexity is 31.472034702116297
At time: 241.42168140411377 and batch: 500, loss is 3.328029727935791 and perplexity is 27.883349761391447
At time: 242.3328206539154 and batch: 550, loss is 3.3599388647079467 and perplexity is 28.787430897449344
At time: 243.24102997779846 and batch: 600, loss is 3.3988918352127073 and perplexity is 29.930913278442148
At time: 244.14156794548035 and batch: 650, loss is 3.235039119720459 and perplexity is 25.407365688450817
At time: 245.0440502166748 and batch: 700, loss is 3.218681755065918 and perplexity is 24.99514872570108
At time: 245.94578552246094 and batch: 750, loss is 3.302031407356262 and perplexity is 27.167771714431087
At time: 246.84996390342712 and batch: 800, loss is 3.2597933483123778 and perplexity is 26.04415451788945
At time: 247.75027561187744 and batch: 850, loss is 3.3152893495559694 and perplexity is 27.5303587292892
At time: 248.65422892570496 and batch: 900, loss is 3.2693382501602173 and perplexity is 26.293933578437244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324867144022902 and perplexity of 75.5554741965437
finished 14 epochs...
Completing Train Step...
At time: 250.85917901992798 and batch: 50, loss is 3.614730730056763 and perplexity is 37.14134353157042
At time: 251.76545810699463 and batch: 100, loss is 3.504140701293945 and perplexity is 33.25285743466064
At time: 252.67188692092896 and batch: 150, loss is 3.519659547805786 and perplexity is 33.77292843876423
At time: 253.57515478134155 and batch: 200, loss is 3.404494490623474 and perplexity is 30.099076512113083
At time: 254.4783148765564 and batch: 250, loss is 3.537466459274292 and perplexity is 34.37970637266136
At time: 255.38185930252075 and batch: 300, loss is 3.511124639511108 and perplexity is 33.48590618832408
At time: 256.2805972099304 and batch: 350, loss is 3.4962508058547974 and perplexity is 32.99152815245821
At time: 257.18244075775146 and batch: 400, loss is 3.438023762702942 and perplexity is 31.125386198845895
At time: 258.0865366458893 and batch: 450, loss is 3.448137674331665 and perplexity is 31.441782911420628
At time: 258.9918692111969 and batch: 500, loss is 3.327136015892029 and perplexity is 27.858441208084216
At time: 259.8898768424988 and batch: 550, loss is 3.3591241312026976 and perplexity is 28.763986364787144
At time: 260.79120898246765 and batch: 600, loss is 3.398326816558838 and perplexity is 29.914006530876136
At time: 261.6904225349426 and batch: 650, loss is 3.2347062921524046 and perplexity is 25.398910823802133
At time: 262.58917117118835 and batch: 700, loss is 3.2185120248794554 and perplexity is 24.990906654461163
At time: 263.4898600578308 and batch: 750, loss is 3.30203094959259 and perplexity is 27.167759278014998
At time: 264.4031240940094 and batch: 800, loss is 3.2600836610794066 and perplexity is 26.05171656607892
At time: 265.3065266609192 and batch: 850, loss is 3.3159961605072024 and perplexity is 27.54982436678292
At time: 266.2084891796112 and batch: 900, loss is 3.270183410644531 and perplexity is 26.316165565550058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3246133882705475 and perplexity of 75.53630399272205
finished 15 epochs...
Completing Train Step...
At time: 268.4061164855957 and batch: 50, loss is 3.613955268859863 and perplexity is 37.11255302526394
At time: 269.30897521972656 and batch: 100, loss is 3.503158140182495 and perplexity is 33.220200516437
At time: 270.2043516635895 and batch: 150, loss is 3.5186501932144165 and perplexity is 33.738856776467635
At time: 271.1051199436188 and batch: 200, loss is 3.4034694766998292 and perplexity is 30.068240346048945
At time: 271.9997081756592 and batch: 250, loss is 3.536316499710083 and perplexity is 34.34019382377446
At time: 272.9005181789398 and batch: 300, loss is 3.510025463104248 and perplexity is 33.449119491517926
At time: 273.7959523200989 and batch: 350, loss is 3.495219502449036 and perplexity is 32.957521415759075
At time: 274.6955442428589 and batch: 400, loss is 3.4370614767074583 and perplexity is 31.095449081947027
At time: 275.58532762527466 and batch: 450, loss is 3.447251281738281 and perplexity is 31.41392549605243
At time: 276.4794993400574 and batch: 500, loss is 3.326304063796997 and perplexity is 27.83527395791424
At time: 277.3743734359741 and batch: 550, loss is 3.358373317718506 and perplexity is 28.74239808136665
At time: 278.26964569091797 and batch: 600, loss is 3.3978044843673705 and perplexity is 29.898385562311823
At time: 279.17385721206665 and batch: 650, loss is 3.2343757343292237 and perplexity is 25.390516402626282
At time: 280.0751402378082 and batch: 700, loss is 3.218349232673645 and perplexity is 24.9868386607695
At time: 280.97872853279114 and batch: 750, loss is 3.3020405626296996 and perplexity is 27.168020443948414
At time: 281.87969040870667 and batch: 800, loss is 3.2603355550765993 and perplexity is 26.05827966366622
At time: 282.77839255332947 and batch: 850, loss is 3.316620955467224 and perplexity is 27.567042736611985
At time: 283.6741955280304 and batch: 900, loss is 3.27094621181488 and perplexity is 26.33624722562573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324434881340967 and perplexity of 75.52282144242461
finished 16 epochs...
Completing Train Step...
At time: 285.85976815223694 and batch: 50, loss is 3.613249502182007 and perplexity is 37.08636946283832
At time: 286.7665913105011 and batch: 100, loss is 3.502257504463196 and perplexity is 33.190294686400485
At time: 287.66351771354675 and batch: 150, loss is 3.517716045379639 and perplexity is 33.70735441268531
At time: 288.5665636062622 and batch: 200, loss is 3.402517557144165 and perplexity is 30.039631418918063
At time: 289.4675328731537 and batch: 250, loss is 3.5352559757232664 and perplexity is 34.303794529086545
At time: 290.36412739753723 and batch: 300, loss is 3.509011120796204 and perplexity is 33.41520783637174
At time: 291.26225113868713 and batch: 350, loss is 3.494252963066101 and perplexity is 32.92568206281979
At time: 292.1605215072632 and batch: 400, loss is 3.436167669296265 and perplexity is 31.06766815635128
At time: 293.06288957595825 and batch: 450, loss is 3.446421046257019 and perplexity is 31.387855364171617
At time: 293.96500873565674 and batch: 500, loss is 3.3255224084854125 and perplexity is 27.81352486942826
At time: 294.8651716709137 and batch: 550, loss is 3.3576674938201903 and perplexity is 28.722118167771686
At time: 295.7648663520813 and batch: 600, loss is 3.397309341430664 and perplexity is 29.883585252318575
At time: 296.66436672210693 and batch: 650, loss is 3.2340519428253174 and perplexity is 25.382296499974455
At time: 297.5587122440338 and batch: 700, loss is 3.218188819885254 and perplexity is 24.982830773773603
At time: 298.4544997215271 and batch: 750, loss is 3.3020519638061523 and perplexity is 27.168330193109117
At time: 299.3620767593384 and batch: 800, loss is 3.260561017990112 and perplexity is 26.06415550168721
At time: 300.2606911659241 and batch: 850, loss is 3.317182321548462 and perplexity is 27.582522283804224
At time: 301.16181802749634 and batch: 900, loss is 3.271638240814209 and perplexity is 26.35447898016317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324300269558005 and perplexity of 75.51265586499451
finished 17 epochs...
Completing Train Step...
At time: 303.3608102798462 and batch: 50, loss is 3.6125913715362548 and perplexity is 37.06196981651334
At time: 304.26759457588196 and batch: 100, loss is 3.5014176177978515 and perplexity is 33.1624303035743
At time: 305.17098093032837 and batch: 150, loss is 3.5168380784988402 and perplexity is 33.67777345932046
At time: 306.0691955089569 and batch: 200, loss is 3.401619815826416 and perplexity is 30.012675702064122
At time: 306.96920704841614 and batch: 250, loss is 3.5342651081085203 and perplexity is 34.26982084451633
At time: 307.8688395023346 and batch: 300, loss is 3.50806143283844 and perplexity is 33.383488879822856
At time: 308.7706606388092 and batch: 350, loss is 3.493339695930481 and perplexity is 32.89562584624041
At time: 309.68166422843933 and batch: 400, loss is 3.435325512886047 and perplexity is 31.041515334391917
At time: 310.5806555747986 and batch: 450, loss is 3.4456360816955565 and perplexity is 31.36322667763811
At time: 311.48260951042175 and batch: 500, loss is 3.3247830200195314 and perplexity is 27.792967470832565
At time: 312.38869857788086 and batch: 550, loss is 3.356996941566467 and perplexity is 28.702864942570777
At time: 313.29310035705566 and batch: 600, loss is 3.396834735870361 and perplexity is 29.869405701708985
At time: 314.20046639442444 and batch: 650, loss is 3.233735499382019 and perplexity is 25.374265709383714
At time: 315.1004023551941 and batch: 700, loss is 3.2180288076400756 and perplexity is 24.978833534742705
At time: 316.0048656463623 and batch: 750, loss is 3.3020616960525513 and perplexity is 27.168594603279455
At time: 316.9063527584076 and batch: 800, loss is 3.2607648611068725 and perplexity is 26.069469041926304
At time: 317.8107862472534 and batch: 850, loss is 3.3176913928985594 and perplexity is 27.59656733032441
At time: 318.7163393497467 and batch: 900, loss is 3.2722693634033204 and perplexity is 26.37111713697785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324197011451199 and perplexity of 75.50485897366303
finished 18 epochs...
Completing Train Step...
At time: 320.9248676300049 and batch: 50, loss is 3.6119688606262206 and perplexity is 37.038905515589505
At time: 321.8281445503235 and batch: 100, loss is 3.500625467300415 and perplexity is 33.13617106991877
At time: 322.72918176651 and batch: 150, loss is 3.516005163192749 and perplexity is 33.64973440503086
At time: 323.6246430873871 and batch: 200, loss is 3.400766158103943 and perplexity is 29.987066082159085
At time: 324.5275399684906 and batch: 250, loss is 3.5333309173583984 and perplexity is 34.237821244067625
At time: 325.4233031272888 and batch: 300, loss is 3.507164568901062 and perplexity is 33.35356185478347
At time: 326.3250160217285 and batch: 350, loss is 3.49247145652771 and perplexity is 32.867076963127936
At time: 327.2251296043396 and batch: 400, loss is 3.4345256185531614 and perplexity is 31.01669533020548
At time: 328.12774419784546 and batch: 450, loss is 3.444889030456543 and perplexity is 31.33980548979207
At time: 329.028422832489 and batch: 500, loss is 3.3240796184539794 and perplexity is 27.773424728005963
At time: 329.93213987350464 and batch: 550, loss is 3.3563564157485963 and perplexity is 28.68448590328116
At time: 330.8318247795105 and batch: 600, loss is 3.396377401351929 and perplexity is 29.855748514626107
At time: 331.7423348426819 and batch: 650, loss is 3.233425359725952 and perplexity is 25.36639736354976
At time: 332.64375352859497 and batch: 700, loss is 3.217868628501892 and perplexity is 24.974832767142587
At time: 333.5442156791687 and batch: 750, loss is 3.3020675134658815 and perplexity is 27.168752654683587
At time: 334.4459083080292 and batch: 800, loss is 3.2609498834609987 and perplexity is 26.074292922708388
At time: 335.34700751304626 and batch: 850, loss is 3.3181554460525513 and perplexity is 27.609376576289026
At time: 336.2469940185547 and batch: 900, loss is 3.272846622467041 and perplexity is 26.386344498008842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324113401648116 and perplexity of 75.49854629117719
finished 19 epochs...
Completing Train Step...
At time: 338.4317147731781 and batch: 50, loss is 3.611374650001526 and perplexity is 37.01690314207453
At time: 339.3361449241638 and batch: 100, loss is 3.4998727083206176 and perplexity is 33.11123690547528
At time: 340.2392339706421 and batch: 150, loss is 3.5152096557617187 and perplexity is 33.62297643575228
At time: 341.13840913772583 and batch: 200, loss is 3.3999497175216673 and perplexity is 29.962593416064518
At time: 342.0416889190674 and batch: 250, loss is 3.532444272041321 and perplexity is 34.207477894077066
At time: 342.94117069244385 and batch: 300, loss is 3.5063124465942384 and perplexity is 33.325152646494054
At time: 343.83994221687317 and batch: 350, loss is 3.491642003059387 and perplexity is 32.83982655518227
At time: 344.73798418045044 and batch: 400, loss is 3.4337612867355345 and perplexity is 30.992997340804322
At time: 345.63731718063354 and batch: 450, loss is 3.4441744899749756 and perplexity is 31.317419928732402
At time: 346.5375106334686 and batch: 500, loss is 3.323407406806946 and perplexity is 27.754761381987855
At time: 347.43624687194824 and batch: 550, loss is 3.355741720199585 and perplexity is 28.666859095585874
At time: 348.33434414863586 and batch: 600, loss is 3.395934991836548 and perplexity is 29.84254296873939
At time: 349.24026322364807 and batch: 650, loss is 3.233121061325073 and perplexity is 25.358679583709552
At time: 350.13915157318115 and batch: 700, loss is 3.2177077436447146 and perplexity is 24.970815017945984
At time: 351.04438614845276 and batch: 750, loss is 3.3020684480667115 and perplexity is 27.168778046634234
At time: 351.94416332244873 and batch: 800, loss is 3.2611177492141725 and perplexity is 26.078670270921485
At time: 352.84550380706787 and batch: 850, loss is 3.318579921722412 and perplexity is 27.621098572580788
At time: 353.74627709388733 and batch: 900, loss is 3.273375988006592 and perplexity is 26.40031621724788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324048186001712 and perplexity of 75.49362276522547
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
1350.7728598117828


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.55663006026495}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.8545719625116199, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.1564310047238061, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.49362276522547}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.2517205538203331, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.8811741615359234, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3914380073547363 and batch: 50, loss is 7.14388072013855 and perplexity is 1266.3331504989167
At time: 2.5177886486053467 and batch: 100, loss is 6.590621881484985 and perplexity is 728.2336041058646
At time: 3.641923189163208 and batch: 150, loss is 6.317726573944092 and perplexity is 554.311372977084
At time: 4.7772133350372314 and batch: 200, loss is 6.089580030441284 and perplexity is 441.2360665129999
At time: 5.900770425796509 and batch: 250, loss is 6.132844619750976 and perplexity is 460.7449424676387
At time: 7.023576736450195 and batch: 300, loss is 6.038701429367065 and perplexity is 419.34812799921025
At time: 8.147228956222534 and batch: 350, loss is 6.026965847015381 and perplexity is 414.4555980047742
At time: 9.263339281082153 and batch: 400, loss is 5.89199688911438 and perplexity is 362.1276916941332
At time: 10.387460947036743 and batch: 450, loss is 5.879536752700806 and perplexity is 357.64352593066104
At time: 11.510648012161255 and batch: 500, loss is 5.828137693405151 and perplexity is 339.7254167064615
At time: 12.638372898101807 and batch: 550, loss is 5.870919198989868 and perplexity is 354.5747552756804
At time: 13.75606918334961 and batch: 600, loss is 5.795977697372437 and perplexity is 328.973663487902
At time: 14.882085084915161 and batch: 650, loss is 5.7021621990203855 and perplexity is 299.5143108906269
At time: 15.999036312103271 and batch: 700, loss is 5.79988314628601 and perplexity is 330.26096543434187
At time: 17.12702512741089 and batch: 750, loss is 5.7464769744873045 and perplexity is 313.0857061074012
At time: 18.249664783477783 and batch: 800, loss is 5.744944105148315 and perplexity is 312.6061542670893
At time: 19.37954092025757 and batch: 850, loss is 5.769969148635864 and perplexity is 320.5278437735907
At time: 20.501009941101074 and batch: 900, loss is 5.648756113052368 and perplexity is 283.93805922475474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.611764986221105 and perplexity of 273.6267595178587
finished 1 epochs...
Completing Train Step...
At time: 22.85756540298462 and batch: 50, loss is 5.417741737365723 and perplexity is 225.36960365005092
At time: 23.753970861434937 and batch: 100, loss is 5.231398963928223 and perplexity is 187.05430278571598
At time: 24.647791147232056 and batch: 150, loss is 5.178303365707397 and perplexity is 177.3816038344783
At time: 25.5417902469635 and batch: 200, loss is 5.018226490020752 and perplexity is 151.14301231493315
At time: 26.438563585281372 and batch: 250, loss is 5.067696380615234 and perplexity is 158.8080722953521
At time: 27.337251663208008 and batch: 300, loss is 4.982053861618042 and perplexity is 145.77347294341774
At time: 28.250113248825073 and batch: 350, loss is 4.952956190109253 and perplexity is 141.59292143004254
At time: 29.14937710762024 and batch: 400, loss is 4.794305591583252 and perplexity is 120.82045390620705
At time: 30.061927318572998 and batch: 450, loss is 4.803975658416748 and perplexity is 121.99446299503904
At time: 30.9651939868927 and batch: 500, loss is 4.709363775253296 and perplexity is 110.98152824458013
At time: 31.870630025863647 and batch: 550, loss is 4.766087284088135 and perplexity is 117.45875892346386
At time: 32.78663110733032 and batch: 600, loss is 4.68764437675476 and perplexity is 108.59706450075944
At time: 33.681212186813354 and batch: 650, loss is 4.556765871047974 and perplexity is 95.27484987416223
At time: 34.58731412887573 and batch: 700, loss is 4.609360637664795 and perplexity is 100.41992438961651
At time: 35.48809814453125 and batch: 750, loss is 4.631967220306397 and perplexity is 102.71593035168618
At time: 36.38731646537781 and batch: 800, loss is 4.573806524276733 and perplexity is 96.91230760287259
At time: 37.28914713859558 and batch: 850, loss is 4.620689535140992 and perplexity is 101.56403996495737
At time: 38.19528865814209 and batch: 900, loss is 4.549658155441284 and perplexity is 94.60006426969632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.641487435118793 and perplexity of 103.69847768307332
finished 2 epochs...
Completing Train Step...
At time: 40.36106634140015 and batch: 50, loss is 4.599279289245605 and perplexity is 99.41264205682128
At time: 41.26326942443848 and batch: 100, loss is 4.467677164077759 and perplexity is 87.15404316632194
At time: 42.166632652282715 and batch: 150, loss is 4.4661853218078615 and perplexity is 87.02412001725314
At time: 43.07959771156311 and batch: 200, loss is 4.351759796142578 and perplexity is 77.61492926670189
At time: 43.98106646537781 and batch: 250, loss is 4.484305582046509 and perplexity is 88.61539332204232
At time: 44.88320779800415 and batch: 300, loss is 4.434825310707092 and perplexity is 84.33739076222312
At time: 45.78532433509827 and batch: 350, loss is 4.438850364685059 and perplexity is 84.67753740747783
At time: 46.68520998954773 and batch: 400, loss is 4.3288000869750975 and perplexity is 75.8532146806657
At time: 47.5862991809845 and batch: 450, loss is 4.364766674041748 and perplexity is 78.63105113587918
At time: 48.48924994468689 and batch: 500, loss is 4.246044864654541 and perplexity is 69.8286835630453
At time: 49.38668155670166 and batch: 550, loss is 4.312506055831909 and perplexity is 74.62727491206519
At time: 50.28857946395874 and batch: 600, loss is 4.299335770606994 and perplexity is 73.65085638496274
At time: 51.188729763031006 and batch: 650, loss is 4.149171886444091 and perplexity is 63.38149148720076
At time: 52.08867883682251 and batch: 700, loss is 4.1721240997314455 and perplexity is 64.85306027684426
At time: 52.99842286109924 and batch: 750, loss is 4.257865600585937 and perplexity is 70.65900784552353
At time: 53.89741039276123 and batch: 800, loss is 4.20623966217041 and perplexity is 67.10373208452008
At time: 54.800349712371826 and batch: 850, loss is 4.278877243995667 and perplexity is 72.15937713498032
At time: 55.698312282562256 and batch: 900, loss is 4.214122233390808 and perplexity is 67.63477226316952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.437947312446489 and perplexity of 84.601103684782
finished 3 epochs...
Completing Train Step...
At time: 57.89045000076294 and batch: 50, loss is 4.291489505767823 and perplexity is 73.07523345822815
At time: 58.79751372337341 and batch: 100, loss is 4.164764804840088 and perplexity is 64.37753937707569
At time: 59.69887852668762 and batch: 150, loss is 4.165560450553894 and perplexity is 64.42878147292016
At time: 60.5933141708374 and batch: 200, loss is 4.052293691635132 and perplexity is 57.52926021267533
At time: 61.49048161506653 and batch: 250, loss is 4.199210152626038 and perplexity is 66.63367981342743
At time: 62.38828897476196 and batch: 300, loss is 4.160421032905578 and perplexity is 64.09850449899679
At time: 63.28714442253113 and batch: 350, loss is 4.164420146942138 and perplexity is 64.35535497291349
At time: 64.19532084465027 and batch: 400, loss is 4.070266900062561 and perplexity is 58.572593534728526
At time: 65.09186697006226 and batch: 450, loss is 4.110908269882202 and perplexity is 61.00209878434722
At time: 65.993905544281 and batch: 500, loss is 3.9877503490448 and perplexity is 53.93342141131267
At time: 66.8954336643219 and batch: 550, loss is 4.055202355384827 and perplexity is 57.69683708063705
At time: 67.78934812545776 and batch: 600, loss is 4.063443760871888 and perplexity is 58.17430491084207
At time: 68.6914005279541 and batch: 650, loss is 3.9066484069824217 and perplexity is 49.73199094339237
At time: 69.58797883987427 and batch: 700, loss is 3.921072454452515 and perplexity is 50.45452595409197
At time: 70.48896288871765 and batch: 750, loss is 4.021154613494873 and perplexity is 55.76545621802867
At time: 71.39124536514282 and batch: 800, loss is 3.9785518646240234 and perplexity is 53.43959040511233
At time: 72.29455852508545 and batch: 850, loss is 4.055648455619812 and perplexity is 57.722581395062036
At time: 73.18902730941772 and batch: 900, loss is 3.996954336166382 and perplexity is 54.43211539340024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366743949994649 and perplexity of 78.78668023257246
finished 4 epochs...
Completing Train Step...
At time: 75.38317561149597 and batch: 50, loss is 4.078275909423828 and perplexity is 59.0435855568031
At time: 76.28012609481812 and batch: 100, loss is 3.9603160572052003 and perplexity is 52.473908085244574
At time: 77.1766984462738 and batch: 150, loss is 3.9642414808273316 and perplexity is 52.68029521703179
At time: 78.07847094535828 and batch: 200, loss is 3.851612620353699 and perplexity is 47.06890633846199
At time: 78.9799473285675 and batch: 250, loss is 4.0010643434524535 and perplexity is 54.656292152751504
At time: 79.87838006019592 and batch: 300, loss is 3.9649892044067383 and perplexity is 52.71970024613446
At time: 80.77611184120178 and batch: 350, loss is 3.9699708461761474 and perplexity is 52.98298616051391
At time: 81.67360305786133 and batch: 400, loss is 3.8843744516372682 and perplexity is 48.63650845327953
At time: 82.57157111167908 and batch: 450, loss is 3.9271914768218994 and perplexity is 50.76420482675602
At time: 83.46714091300964 and batch: 500, loss is 3.804360728263855 and perplexity is 44.8965398481851
At time: 84.3655834197998 and batch: 550, loss is 3.8691191816329957 and perplexity is 47.90017613897629
At time: 85.26351475715637 and batch: 600, loss is 3.889731111526489 and perplexity is 48.89773671811127
At time: 86.1637077331543 and batch: 650, loss is 3.7307920694351195 and perplexity is 41.71213398933583
At time: 87.06181001663208 and batch: 700, loss is 3.7410503911972044 and perplexity is 42.14223275523206
At time: 87.9619927406311 and batch: 750, loss is 3.846094598770142 and perplexity is 46.80989437139577
At time: 88.86305785179138 and batch: 800, loss is 3.806078472137451 and perplexity is 44.97372687925057
At time: 89.76150965690613 and batch: 850, loss is 3.884905834197998 and perplexity is 48.66235991358568
At time: 90.66005420684814 and batch: 900, loss is 3.8296065950393676 and perplexity is 46.04442056719753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33990938369542 and perplexity of 76.70058869947269
finished 5 epochs...
Completing Train Step...
At time: 92.8637969493866 and batch: 50, loss is 3.9152765226364137 and perplexity is 50.16294078200006
At time: 93.78477478027344 and batch: 100, loss is 3.804513831138611 and perplexity is 44.90341416372776
At time: 94.68472862243652 and batch: 150, loss is 3.802897992134094 and perplexity is 44.830916064088605
At time: 95.58526468276978 and batch: 200, loss is 3.693222484588623 and perplexity is 40.17409900397413
At time: 96.48326969146729 and batch: 250, loss is 3.8443691825866697 and perplexity is 46.72919745999915
At time: 97.39308738708496 and batch: 300, loss is 3.809310750961304 and perplexity is 45.11932969194923
At time: 98.29205965995789 and batch: 350, loss is 3.8112527990341185 and perplexity is 45.20703873925072
At time: 99.18867182731628 and batch: 400, loss is 3.7319900798797607 and perplexity is 41.76213550671306
At time: 100.0910575389862 and batch: 450, loss is 3.7786008501052857 and perplexity is 43.75477939298697
At time: 100.98759317398071 and batch: 500, loss is 3.6599135112762453 and perplexity is 38.85798194872718
At time: 101.88332009315491 and batch: 550, loss is 3.7224079036712645 and perplexity is 41.36387451623118
At time: 102.77627396583557 and batch: 600, loss is 3.746525597572327 and perplexity is 42.373602998592204
At time: 103.66850543022156 and batch: 650, loss is 3.5905747413635254 and perplexity is 36.254907134385526
At time: 104.5696222782135 and batch: 700, loss is 3.597077569961548 and perplexity is 36.49143479724394
At time: 105.46199131011963 and batch: 750, loss is 3.706532459259033 and perplexity is 40.71238961458479
At time: 106.36252188682556 and batch: 800, loss is 3.665519289970398 and perplexity is 39.07642288978233
At time: 107.2619276046753 and batch: 850, loss is 3.74551540851593 and perplexity is 42.3308192620325
At time: 108.15647625923157 and batch: 900, loss is 3.6890595960617065 and perplexity is 40.00720632697221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349096010809076 and perplexity of 77.4084548802725
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.337242603302 and batch: 50, loss is 3.8144279432296755 and perplexity is 45.35080572557534
At time: 111.24525046348572 and batch: 100, loss is 3.710568165779114 and perplexity is 40.87702485711115
At time: 112.14323735237122 and batch: 150, loss is 3.715484075546265 and perplexity is 41.078467353752394
At time: 113.04355001449585 and batch: 200, loss is 3.5873924398422243 and perplexity is 36.139716471180236
At time: 113.9429702758789 and batch: 250, loss is 3.732931876182556 and perplexity is 41.80148545844109
At time: 114.84475708007812 and batch: 300, loss is 3.685124173164368 and perplexity is 39.8500704519988
At time: 115.74714231491089 and batch: 350, loss is 3.674559850692749 and perplexity is 39.431297378874284
At time: 116.6449339389801 and batch: 400, loss is 3.591707353591919 and perplexity is 36.29599314841402
At time: 117.5444803237915 and batch: 450, loss is 3.6286676025390623 and perplexity is 37.662601616674614
At time: 118.44463562965393 and batch: 500, loss is 3.501114501953125 and perplexity is 33.15237976881477
At time: 119.34293603897095 and batch: 550, loss is 3.5457888221740723 and perplexity is 34.66702067410223
At time: 120.25654315948486 and batch: 600, loss is 3.567196969985962 and perplexity is 35.41717847128606
At time: 121.15570282936096 and batch: 650, loss is 3.3907769870758058 and perplexity is 29.68901128899405
At time: 122.05576729774475 and batch: 700, loss is 3.3847292852401734 and perplexity is 29.510002841380086
At time: 122.9568088054657 and batch: 750, loss is 3.4766530704498293 and perplexity is 32.351263276084445
At time: 123.85500645637512 and batch: 800, loss is 3.4208257246017455 and perplexity is 30.594667363399427
At time: 124.75395035743713 and batch: 850, loss is 3.484893159866333 and perplexity is 32.61894191156931
At time: 125.65057587623596 and batch: 900, loss is 3.4215936136245726 and perplexity is 30.618169695061223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309637200342466 and perplexity of 74.41348685492105
finished 7 epochs...
Completing Train Step...
At time: 127.85206007957458 and batch: 50, loss is 3.7178060960769654 and perplexity is 41.17396322709906
At time: 128.75199031829834 and batch: 100, loss is 3.6053778553009033 and perplexity is 36.795584637542156
At time: 129.6499695777893 and batch: 150, loss is 3.6048286724090577 and perplexity is 36.775382679756305
At time: 130.55269813537598 and batch: 200, loss is 3.481880774497986 and perplexity is 32.520828939458305
At time: 131.45353150367737 and batch: 250, loss is 3.6295762205123903 and perplexity is 37.696838085005616
At time: 132.35314536094666 and batch: 300, loss is 3.586726760864258 and perplexity is 36.11566702714845
At time: 133.25188994407654 and batch: 350, loss is 3.5787074518203736 and perplexity is 35.82720252085511
At time: 134.1506531238556 and batch: 400, loss is 3.500293979644775 and perplexity is 33.12518865862137
At time: 135.04872274398804 and batch: 450, loss is 3.5422117376327513 and perplexity is 34.54323533765356
At time: 135.95191311836243 and batch: 500, loss is 3.419793906211853 and perplexity is 30.563115503675082
At time: 136.85326051712036 and batch: 550, loss is 3.4672093057632445 and perplexity is 32.04718364655893
At time: 137.75430274009705 and batch: 600, loss is 3.4965891790390016 and perplexity is 33.00269348980976
At time: 138.6535587310791 and batch: 650, loss is 3.32461820602417 and perplexity is 27.78838717827927
At time: 139.55466604232788 and batch: 700, loss is 3.3229930210113525 and perplexity is 27.743262585744535
At time: 140.45362663269043 and batch: 750, loss is 3.421996898651123 and perplexity is 30.63052003462574
At time: 141.34987139701843 and batch: 800, loss is 3.372210602760315 and perplexity is 29.142879232318087
At time: 142.2569568157196 and batch: 850, loss is 3.4435136222839358 and perplexity is 31.296730095109083
At time: 143.15549850463867 and batch: 900, loss is 3.3880106830596923 and perplexity is 29.606995949813378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322056600492295 and perplexity of 75.34342038016666
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.34443306922913 and batch: 50, loss is 3.6786186838150026 and perplexity is 39.59166767269597
At time: 146.255850315094 and batch: 100, loss is 3.5818191289901735 and perplexity is 35.93885883811068
At time: 147.15509462356567 and batch: 150, loss is 3.586471829414368 and perplexity is 36.10646118126915
At time: 148.0570673942566 and batch: 200, loss is 3.45482412815094 and perplexity is 31.652721370087313
At time: 148.95893168449402 and batch: 250, loss is 3.601310338973999 and perplexity is 36.64622196974575
At time: 149.85786056518555 and batch: 300, loss is 3.5545604467391967 and perplexity is 34.97244433679029
At time: 150.75684785842896 and batch: 350, loss is 3.546959843635559 and perplexity is 34.707640277889524
At time: 151.65371251106262 and batch: 400, loss is 3.4654597663879394 and perplexity is 31.991164854731608
At time: 152.5505609512329 and batch: 450, loss is 3.5025022554397585 and perplexity is 33.19841903761827
At time: 153.44550848007202 and batch: 500, loss is 3.3749349164962767 and perplexity is 29.222381824346336
At time: 154.34442710876465 and batch: 550, loss is 3.414606647491455 and perplexity is 30.40498719704651
At time: 155.24429488182068 and batch: 600, loss is 3.4438068723678588 and perplexity is 31.305909209658264
At time: 156.14514136314392 and batch: 650, loss is 3.264950141906738 and perplexity is 26.178805732946675
At time: 157.04684448242188 and batch: 700, loss is 3.2585380029678346 and perplexity is 26.01148062256492
At time: 157.95089602470398 and batch: 750, loss is 3.3550340127944946 and perplexity is 28.646578524324937
At time: 158.8505048751831 and batch: 800, loss is 3.297056112289429 and perplexity is 27.032939726436663
At time: 159.74950289726257 and batch: 850, loss is 3.3617975902557373 and perplexity is 28.84098858981971
At time: 160.64784741401672 and batch: 900, loss is 3.3086729621887208 and perplexity is 27.34880847727751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3212781932255995 and perplexity of 75.28479533428607
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 162.84984135627747 and batch: 50, loss is 3.6649636125564573 and perplexity is 39.054715036005476
At time: 163.76315569877625 and batch: 100, loss is 3.5627868032455443 and perplexity is 35.261326727445166
At time: 164.66174221038818 and batch: 150, loss is 3.571550364494324 and perplexity is 35.57169952305299
At time: 165.5687825679779 and batch: 200, loss is 3.4359971714019775 and perplexity is 31.062371635885828
At time: 166.4716408252716 and batch: 250, loss is 3.5848750972747805 and perplexity is 36.04885483744377
At time: 167.37267637252808 and batch: 300, loss is 3.538329267501831 and perplexity is 34.40938226664181
At time: 168.27024269104004 and batch: 350, loss is 3.5318350648880004 and perplexity is 34.18664480032613
At time: 169.17341113090515 and batch: 400, loss is 3.4518215608596803 and perplexity is 31.55782448266177
At time: 170.074116230011 and batch: 450, loss is 3.484827308654785 and perplexity is 32.61679398544756
At time: 170.97478580474854 and batch: 500, loss is 3.358322820663452 and perplexity is 28.740946711553615
At time: 171.87540578842163 and batch: 550, loss is 3.3915688371658326 and perplexity is 29.712529845612735
At time: 172.77904343605042 and batch: 600, loss is 3.4245755958557127 and perplexity is 30.709608800177243
At time: 173.68925523757935 and batch: 650, loss is 3.242536020278931 and perplexity is 25.59855796172766
At time: 174.59805989265442 and batch: 700, loss is 3.235385136604309 and perplexity is 25.41615858711115
At time: 175.50149941444397 and batch: 750, loss is 3.3337134981155394 and perplexity is 28.04228355699472
At time: 176.39999437332153 and batch: 800, loss is 3.273912868499756 and perplexity is 26.414493837541624
At time: 177.3019995689392 and batch: 850, loss is 3.3331628179550172 and perplexity is 28.026845478895943
At time: 178.20530557632446 and batch: 900, loss is 3.284757823944092 and perplexity is 26.70251682157571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318166654403895 and perplexity of 75.05090783463876
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 180.41343593597412 and batch: 50, loss is 3.655879325866699 and perplexity is 38.70153742019571
At time: 181.30804824829102 and batch: 100, loss is 3.5540753746032716 and perplexity is 34.95548429227238
At time: 182.21498847007751 and batch: 150, loss is 3.563108277320862 and perplexity is 35.272664152095764
At time: 183.11561465263367 and batch: 200, loss is 3.4281691455841066 and perplexity is 30.820163830067095
At time: 184.02016711235046 and batch: 250, loss is 3.5790853357315062 and perplexity is 35.84074360258596
At time: 184.91705179214478 and batch: 300, loss is 3.5317974853515626 and perplexity is 34.18536010620143
At time: 185.81557440757751 and batch: 350, loss is 3.524668016433716 and perplexity is 33.942503392261706
At time: 186.71684288978577 and batch: 400, loss is 3.4462731075286865 and perplexity is 31.383212228222238
At time: 187.6256139278412 and batch: 450, loss is 3.477150111198425 and perplexity is 32.36734716904518
At time: 188.5304262638092 and batch: 500, loss is 3.3512352228164675 and perplexity is 28.537962624036577
At time: 189.4307291507721 and batch: 550, loss is 3.3838158893585204 and perplexity is 29.483060832550283
At time: 190.3352460861206 and batch: 600, loss is 3.416666522026062 and perplexity is 30.467682205656253
At time: 191.23726677894592 and batch: 650, loss is 3.2360461807250975 and perplexity is 25.432965343709984
At time: 192.13266348838806 and batch: 700, loss is 3.229363188743591 and perplexity is 25.263563725569394
At time: 193.03447127342224 and batch: 750, loss is 3.3244829416275024 and perplexity is 27.78462865305629
At time: 193.93293833732605 and batch: 800, loss is 3.2669795513153077 and perplexity is 26.23198719283143
At time: 194.83348846435547 and batch: 850, loss is 3.324223155975342 and perplexity is 27.777411542673075
At time: 195.73818969726562 and batch: 900, loss is 3.2767355823516846 and perplexity is 26.48915972583075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314884551583904 and perplexity of 74.80498682816534
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 197.92020893096924 and batch: 50, loss is 3.65193549156189 and perplexity is 38.549205552537046
At time: 198.83165502548218 and batch: 100, loss is 3.550311760902405 and perplexity is 34.82417261089126
At time: 199.72238683700562 and batch: 150, loss is 3.5600947666168214 and perplexity is 35.166529600236935
At time: 200.61367750167847 and batch: 200, loss is 3.4250864458084105 and perplexity is 30.725300810170204
At time: 201.49856424331665 and batch: 250, loss is 3.5764212226867675 and perplexity is 35.745386886955686
At time: 202.38869428634644 and batch: 300, loss is 3.530219955444336 and perplexity is 34.13147419273965
At time: 203.28477454185486 and batch: 350, loss is 3.522375636100769 and perplexity is 33.86478338098191
At time: 204.18343901634216 and batch: 400, loss is 3.4447144889831542 and perplexity is 31.334335871317688
At time: 205.08594799041748 and batch: 450, loss is 3.474994878768921 and perplexity is 32.29766313239092
At time: 205.98384070396423 and batch: 500, loss is 3.3487540769577024 and perplexity is 28.467243544709145
At time: 206.88358330726624 and batch: 550, loss is 3.381521992683411 and perplexity is 29.41550724742862
At time: 207.7818727493286 and batch: 600, loss is 3.414330453872681 and perplexity is 30.396590693187537
At time: 208.68016719818115 and batch: 650, loss is 3.2338411140441896 and perplexity is 25.376945745407493
At time: 209.57842755317688 and batch: 700, loss is 3.227544960975647 and perplexity is 25.21767054725531
At time: 210.48808646202087 and batch: 750, loss is 3.321615524291992 and perplexity is 27.705072641848307
At time: 211.39024662971497 and batch: 800, loss is 3.2646281147003173 and perplexity is 26.17037680251436
At time: 212.28957676887512 and batch: 850, loss is 3.322094383239746 and perplexity is 27.718342640756312
At time: 213.18951177597046 and batch: 900, loss is 3.2741126871109008 and perplexity is 26.419772472381517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313137942797517 and perplexity of 74.67444581613114
Annealing...
Model not improving. Stopping early with 74.41348685492105 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
1572.5057866573334


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.55663006026495}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.8545719625116199, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.1564310047238061, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.49362276522547}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.2517205538203331, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.8811741615359234, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.41348685492105}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'rnn_dropout': 0.3333549557419628, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.37947969550812455, 'data': 'ptb', 'num_layers': 3}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.4033966064453125 and batch: 50, loss is 7.049802761077881 and perplexity is 1152.6313765910745
At time: 2.527876377105713 and batch: 100, loss is 6.486053419113159 and perplexity is 655.9295691720508
At time: 3.654862880706787 and batch: 150, loss is 6.0275786590576175 and perplexity is 414.70965922414035
At time: 4.77822208404541 and batch: 200, loss is 5.807822999954223 and perplexity is 332.8936268147592
At time: 5.902412176132202 and batch: 250, loss is 5.7710849380493165 and perplexity is 320.8856849489619
At time: 7.024480104446411 and batch: 300, loss is 5.625437669754028 and perplexity is 277.39366477599515
At time: 8.148223876953125 and batch: 350, loss is 5.5627534961700436 and perplexity is 260.5392433670477
At time: 9.2721586227417 and batch: 400, loss is 5.384314422607422 and perplexity is 217.9606239969478
At time: 10.395983457565308 and batch: 450, loss is 5.358233489990234 and perplexity is 212.34949741718788
At time: 11.517586708068848 and batch: 500, loss is 5.2870324611663815 and perplexity is 197.75570606337922
At time: 12.641836166381836 and batch: 550, loss is 5.320989351272583 and perplexity is 204.58618950680963
At time: 13.76857614517212 and batch: 600, loss is 5.210736417770386 and perplexity is 183.2289415709864
At time: 14.888859272003174 and batch: 650, loss is 5.09267412185669 and perplexity is 162.8246934698879
At time: 16.014854431152344 and batch: 700, loss is 5.166953639984131 and perplexity is 175.37975300075567
At time: 17.140007972717285 and batch: 750, loss is 5.136541090011597 and perplexity is 170.12629789537513
At time: 18.26779794692993 and batch: 800, loss is 5.100150299072266 and perplexity is 164.04656149310495
At time: 19.392777681350708 and batch: 850, loss is 5.122017793655395 and perplexity is 167.6733587497318
At time: 20.535489320755005 and batch: 900, loss is 5.028497095108032 and perplexity is 152.7033415528634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.982562339469178 and perplexity of 145.8476143737327
finished 1 epochs...
Completing Train Step...
At time: 22.88947319984436 and batch: 50, loss is 4.9319515132904055 and perplexity is 138.6498254714578
At time: 23.7903733253479 and batch: 100, loss is 4.808169565200806 and perplexity is 122.50717077383229
At time: 24.689635276794434 and batch: 150, loss is 4.792338781356811 and perplexity is 120.58305653621646
At time: 25.593448162078857 and batch: 200, loss is 4.675256834030152 and perplexity is 107.26011160144652
At time: 26.49327039718628 and batch: 250, loss is 4.778843116760254 and perplexity is 118.96663989110958
At time: 27.396578550338745 and batch: 300, loss is 4.71574517250061 and perplexity is 111.69200998550392
At time: 28.295894622802734 and batch: 350, loss is 4.705032587051392 and perplexity is 110.50188581943279
At time: 29.1969575881958 and batch: 400, loss is 4.574081764221192 and perplexity is 96.93898541226551
At time: 30.097997188568115 and batch: 450, loss is 4.601446609497071 and perplexity is 99.62833474233473
At time: 30.996382474899292 and batch: 500, loss is 4.492339248657227 and perplexity is 89.33016713153732
At time: 31.89561176300049 and batch: 550, loss is 4.557384099960327 and perplexity is 95.33376975208394
At time: 32.79404664039612 and batch: 600, loss is 4.513570365905761 and perplexity is 91.24702287525533
At time: 33.69526815414429 and batch: 650, loss is 4.378925685882568 and perplexity is 79.75230833476105
At time: 34.59581637382507 and batch: 700, loss is 4.417565031051636 and perplexity is 82.89419462595174
At time: 35.49671721458435 and batch: 750, loss is 4.469996490478516 and perplexity is 87.35641643368277
At time: 36.39796447753906 and batch: 800, loss is 4.421904411315918 and perplexity is 83.2546856462677
At time: 37.298301696777344 and batch: 850, loss is 4.477681941986084 and perplexity is 88.03037646226808
At time: 38.202168464660645 and batch: 900, loss is 4.4085072326660155 and perplexity is 82.14674595366174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.548229478809931 and perplexity of 94.4650078674848
finished 2 epochs...
Completing Train Step...
At time: 40.40124559402466 and batch: 50, loss is 4.461383190155029 and perplexity is 86.60722054020358
At time: 41.31126379966736 and batch: 100, loss is 4.333737120628357 and perplexity is 76.22863051200748
At time: 42.20917320251465 and batch: 150, loss is 4.334370012283325 and perplexity is 76.27689024610879
At time: 43.11784648895264 and batch: 200, loss is 4.224708123207092 and perplexity is 68.35454952994485
At time: 44.02152371406555 and batch: 250, loss is 4.365344657897949 and perplexity is 78.67651175051691
At time: 44.91895651817322 and batch: 300, loss is 4.322556519508362 and perplexity is 75.38109540519633
At time: 45.81916785240173 and batch: 350, loss is 4.3239952564239506 and perplexity is 75.48962702539615
At time: 46.715572357177734 and batch: 400, loss is 4.226851029396057 and perplexity is 68.50118397299154
At time: 47.61620020866394 and batch: 450, loss is 4.265036869049072 and perplexity is 71.16754380469047
At time: 48.51334547996521 and batch: 500, loss is 4.134875631332397 and perplexity is 62.48181980654592
At time: 49.41573715209961 and batch: 550, loss is 4.2025450372695925 and perplexity is 66.85626639297324
At time: 50.31338596343994 and batch: 600, loss is 4.200832748413086 and perplexity is 66.74188710617607
At time: 51.212217807769775 and batch: 650, loss is 4.052455825805664 and perplexity is 57.538588427752
At time: 52.109981060028076 and batch: 700, loss is 4.076104221343994 and perplexity is 58.9155004366629
At time: 53.019026041030884 and batch: 750, loss is 4.161439247131348 and perplexity is 64.16380374679812
At time: 53.9149386882782 and batch: 800, loss is 4.122296814918518 and perplexity is 61.7007949483638
At time: 54.81646704673767 and batch: 850, loss is 4.189194107055664 and perplexity is 65.96960509294577
At time: 55.71116805076599 and batch: 900, loss is 4.128753156661987 and perplexity is 62.10044511716584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393040487211045 and perplexity of 80.88597826383678
finished 3 epochs...
Completing Train Step...
At time: 57.91068363189697 and batch: 50, loss is 4.200962009429932 and perplexity is 66.75051478796907
At time: 58.81492376327515 and batch: 100, loss is 4.075518431663514 and perplexity is 58.88099845092699
At time: 59.71817064285278 and batch: 150, loss is 4.082340912818909 and perplexity is 59.284086419786846
At time: 60.61717486381531 and batch: 200, loss is 3.9744387912750243 and perplexity is 53.22024085965514
At time: 61.506808280944824 and batch: 250, loss is 4.122817711830139 and perplexity is 61.73294307409978
At time: 62.4051878452301 and batch: 300, loss is 4.0837863302230835 and perplexity is 59.36983862902676
At time: 63.29394888877869 and batch: 350, loss is 4.08630485534668 and perplexity is 59.51955150788586
At time: 64.18589639663696 and batch: 400, loss is 4.004528489112854 and perplexity is 54.84595783513083
At time: 65.0879213809967 and batch: 450, loss is 4.0475946712493895 and perplexity is 57.259563198303674
At time: 65.99477171897888 and batch: 500, loss is 3.9108922052383424 and perplexity is 49.94349194619025
At time: 66.89138793945312 and batch: 550, loss is 3.9810212564468386 and perplexity is 53.57171676140415
At time: 67.79706287384033 and batch: 600, loss is 3.995370545387268 and perplexity is 54.34597454350066
At time: 68.69274735450745 and batch: 650, loss is 3.843882064819336 and perplexity is 46.70644038080193
At time: 69.59148025512695 and batch: 700, loss is 3.860317406654358 and perplexity is 47.480419578477935
At time: 70.49276828765869 and batch: 750, loss is 3.9549299573898313 and perplexity is 52.19203814982043
At time: 71.38636422157288 and batch: 800, loss is 3.9254048681259155 and perplexity is 50.67359002766516
At time: 72.28663468360901 and batch: 850, loss is 3.988505063056946 and perplexity is 53.97414108409577
At time: 73.1912043094635 and batch: 900, loss is 3.936957473754883 and perplexity is 51.26239660552338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342798520440924 and perplexity of 76.92250761136245
finished 4 epochs...
Completing Train Step...
At time: 75.39191675186157 and batch: 50, loss is 4.01673125743866 and perplexity is 55.51933050168761
At time: 76.2882125377655 and batch: 100, loss is 3.8950562143325804 and perplexity is 49.15881671508524
At time: 77.18205261230469 and batch: 150, loss is 3.9044802713394167 and perplexity is 49.62428204717215
At time: 78.07659840583801 and batch: 200, loss is 3.7939104080200194 and perplexity is 44.42979966878959
At time: 78.96441125869751 and batch: 250, loss is 3.9418157815933226 and perplexity is 51.51205106682977
At time: 79.85496139526367 and batch: 300, loss is 3.9059069538116455 and perplexity is 49.69513066779116
At time: 80.7528944015503 and batch: 350, loss is 3.909294261932373 and perplexity is 49.86374880703398
At time: 81.65178084373474 and batch: 400, loss is 3.835487732887268 and perplexity is 46.31601200220202
At time: 82.54763889312744 and batch: 450, loss is 3.880396294593811 and perplexity is 48.44340912898692
At time: 83.45795273780823 and batch: 500, loss is 3.74528039932251 and perplexity is 42.320872299200545
At time: 84.3532395362854 and batch: 550, loss is 3.814316735267639 and perplexity is 45.345762635314976
At time: 85.25165057182312 and batch: 600, loss is 3.8356109714508055 and perplexity is 46.321720272722274
At time: 86.15014290809631 and batch: 650, loss is 3.6849385023117067 and perplexity is 39.84267214228587
At time: 87.04583811759949 and batch: 700, loss is 3.696798176765442 and perplexity is 40.318006346344404
At time: 87.9559850692749 and batch: 750, loss is 3.7942867422103883 and perplexity is 44.44652326810992
At time: 88.85218453407288 and batch: 800, loss is 3.77091730594635 and perplexity is 43.419875883942574
At time: 89.75539469718933 and batch: 850, loss is 3.830826287269592 and perplexity is 46.10061485210842
At time: 90.65707159042358 and batch: 900, loss is 3.786811599731445 and perplexity is 44.115517871555845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335661169600813 and perplexity of 76.3754393185925
finished 5 epochs...
Completing Train Step...
At time: 92.85728168487549 and batch: 50, loss is 3.8660336208343504 and perplexity is 47.7526050201448
At time: 93.76609563827515 and batch: 100, loss is 3.7495110750198366 and perplexity is 42.50029746234326
At time: 94.66898655891418 and batch: 150, loss is 3.759778151512146 and perplexity is 42.93889899221677
At time: 95.5676486492157 and batch: 200, loss is 3.650417981147766 and perplexity is 38.49075109549613
At time: 96.47264862060547 and batch: 250, loss is 3.7983359241485597 and perplexity is 44.62686018944561
At time: 97.37449145317078 and batch: 300, loss is 3.7626057767868044 and perplexity is 43.060485928408106
At time: 98.27578949928284 and batch: 350, loss is 3.7627464151382446 and perplexity is 43.06654231003107
At time: 99.17577934265137 and batch: 400, loss is 3.697568974494934 and perplexity is 40.34909535422291
At time: 100.07498717308044 and batch: 450, loss is 3.7414955377578734 and perplexity is 42.160996401178345
At time: 100.9767439365387 and batch: 500, loss is 3.6084227848052977 and perplexity is 36.90779534899176
At time: 101.87747263908386 and batch: 550, loss is 3.6751825046539306 and perplexity is 39.45585707768516
At time: 102.77570462226868 and batch: 600, loss is 3.7030600690841675 and perplexity is 40.57126547373212
At time: 103.6781005859375 and batch: 650, loss is 3.5524199056625365 and perplexity is 34.89766444640264
At time: 104.58138227462769 and batch: 700, loss is 3.5622555732727053 and perplexity is 35.24259982839092
At time: 105.4844012260437 and batch: 750, loss is 3.6613023233413697 and perplexity is 38.911985874855816
At time: 106.38295698165894 and batch: 800, loss is 3.6392632627487185 and perplexity is 38.063783385552824
At time: 107.28275680541992 and batch: 850, loss is 3.70062846660614 and perplexity is 40.47273212958114
At time: 108.1789755821228 and batch: 900, loss is 3.6565798854827882 and perplexity is 38.72865965365433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.35334798734482 and perplexity of 77.73829455242577
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 110.37812376022339 and batch: 50, loss is 3.775590968132019 and perplexity is 43.62328066834301
At time: 111.28598809242249 and batch: 100, loss is 3.6679973888397215 and perplexity is 39.173378211983184
At time: 112.18418145179749 and batch: 150, loss is 3.6825672674179075 and perplexity is 39.748307732146564
At time: 113.08390259742737 and batch: 200, loss is 3.5631392002105713 and perplexity is 35.273754901663565
At time: 113.98660516738892 and batch: 250, loss is 3.695926012992859 and perplexity is 40.2828577717111
At time: 114.88365459442139 and batch: 300, loss is 3.647219567298889 and perplexity is 38.3678384117351
At time: 115.77811765670776 and batch: 350, loss is 3.6429581117630003 and perplexity is 38.2046834600292
At time: 116.67780828475952 and batch: 400, loss is 3.575967860221863 and perplexity is 35.72918494320174
At time: 117.57626533508301 and batch: 450, loss is 3.6028103685379027 and perplexity is 36.70123363535305
At time: 118.47876167297363 and batch: 500, loss is 3.459615740776062 and perplexity is 31.804752896633513
At time: 119.37627100944519 and batch: 550, loss is 3.5080580520629883 and perplexity is 33.38337601793394
At time: 120.27642273902893 and batch: 600, loss is 3.5320467948913574 and perplexity is 34.193883905085265
At time: 121.18142700195312 and batch: 650, loss is 3.3631882858276367 and perplexity is 28.88112552760274
At time: 122.07980060577393 and batch: 700, loss is 3.3580517959594727 and perplexity is 28.73315826045803
At time: 122.97743105888367 and batch: 750, loss is 3.4431374883651733 and perplexity is 31.28496054697585
At time: 123.87552857398987 and batch: 800, loss is 3.406891736984253 and perplexity is 30.171317969433513
At time: 124.78000211715698 and batch: 850, loss is 3.447119827270508 and perplexity is 31.409796266604452
At time: 125.67905306816101 and batch: 900, loss is 3.4034233856201173 and perplexity is 30.066854500324165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311298109080694 and perplexity of 74.53718356151485
finished 7 epochs...
Completing Train Step...
At time: 127.88885807991028 and batch: 50, loss is 3.682981557846069 and perplexity is 39.76477848717811
At time: 128.79321908950806 and batch: 100, loss is 3.564008774757385 and perplexity is 35.30444140126244
At time: 129.68746328353882 and batch: 150, loss is 3.5744885492324827 and perplexity is 35.67636944218081
At time: 130.58827352523804 and batch: 200, loss is 3.459043712615967 and perplexity is 31.786564884875173
At time: 131.48204922676086 and batch: 250, loss is 3.5924371433258058 and perplexity is 36.3224912594436
At time: 132.3833441734314 and batch: 300, loss is 3.549085302352905 and perplexity is 34.781488387229366
At time: 133.285982131958 and batch: 350, loss is 3.5472306537628175 and perplexity is 34.71704073118155
At time: 134.188467502594 and batch: 400, loss is 3.4851364850997926 and perplexity is 32.626879888941076
At time: 135.07886695861816 and batch: 450, loss is 3.5175928831100465 and perplexity is 33.703203194055895
At time: 135.98171472549438 and batch: 500, loss is 3.377686629295349 and perplexity is 29.30290416279735
At time: 136.87761092185974 and batch: 550, loss is 3.4304903745651245 and perplexity is 30.89178758295495
At time: 137.78072023391724 and batch: 600, loss is 3.4595522022247316 and perplexity is 31.802732132907913
At time: 138.67313265800476 and batch: 650, loss is 3.2986899328231813 and perplexity is 27.077142798559994
At time: 139.57390117645264 and batch: 700, loss is 3.297216548919678 and perplexity is 27.03727714812345
At time: 140.47269368171692 and batch: 750, loss is 3.390196900367737 and perplexity is 29.671794082388743
At time: 141.37612581253052 and batch: 800, loss is 3.359506130218506 and perplexity is 28.774976278203628
At time: 142.27475929260254 and batch: 850, loss is 3.4061600017547606 and perplexity is 30.14924862859688
At time: 143.17486953735352 and batch: 900, loss is 3.3713925647735596 and perplexity is 29.11904899840982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321989712649828 and perplexity of 75.33838098987223
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 145.36730337142944 and batch: 50, loss is 3.645995545387268 and perplexity is 38.320904067010645
At time: 146.2707395553589 and batch: 100, loss is 3.538945779800415 and perplexity is 34.4306026146265
At time: 147.1638479232788 and batch: 150, loss is 3.5525962781906126 and perplexity is 34.903819978522435
At time: 148.0610167980194 and batch: 200, loss is 3.4357400703430176 and perplexity is 31.05438649378267
At time: 148.9590060710907 and batch: 250, loss is 3.573113284111023 and perplexity is 35.62733869849038
At time: 149.85526323318481 and batch: 300, loss is 3.5247632026672364 and perplexity is 33.9457344050874
At time: 150.75776863098145 and batch: 350, loss is 3.5186699724197386 and perplexity is 33.7395241108428
At time: 151.65338611602783 and batch: 400, loss is 3.4572582149505617 and perplexity is 31.72986068515451
At time: 152.55154418945312 and batch: 450, loss is 3.479885711669922 and perplexity is 32.45601252041644
At time: 153.45283007621765 and batch: 500, loss is 3.339574723243713 and perplexity is 28.207128318236514
At time: 154.34931707382202 and batch: 550, loss is 3.382416491508484 and perplexity is 29.441831155697674
At time: 155.24425315856934 and batch: 600, loss is 3.409390773773193 and perplexity is 30.246811494268638
At time: 156.15520405769348 and batch: 650, loss is 3.2406620264053343 and perplexity is 25.550631342056235
At time: 157.05076336860657 and batch: 700, loss is 3.2318671131134034 and perplexity is 25.32690104128249
At time: 157.95042824745178 and batch: 750, loss is 3.323130030632019 and perplexity is 27.747063940032568
At time: 158.84431290626526 and batch: 800, loss is 3.2848694467544557 and perplexity is 26.70549759790534
At time: 159.74125361442566 and batch: 850, loss is 3.327029657363892 and perplexity is 27.855478382844787
At time: 160.63888025283813 and batch: 900, loss is 3.296168313026428 and perplexity is 27.008950552831774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313040955425942 and perplexity of 74.66720368911074
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 162.82057452201843 and batch: 50, loss is 3.6349152898788453 and perplexity is 37.898642362575536
At time: 163.73676300048828 and batch: 100, loss is 3.522672815322876 and perplexity is 33.874848786504494
At time: 164.63683605194092 and batch: 150, loss is 3.5346605014801025 and perplexity is 34.28337358367861
At time: 165.53828525543213 and batch: 200, loss is 3.418153262138367 and perplexity is 30.513013420438035
At time: 166.44046783447266 and batch: 250, loss is 3.5564442920684813 and perplexity is 35.03838910808004
At time: 167.34008383750916 and batch: 300, loss is 3.508620090484619 and perplexity is 33.4021440315781
At time: 168.23889327049255 and batch: 350, loss is 3.500457625389099 and perplexity is 33.13060989834459
At time: 169.1398651599884 and batch: 400, loss is 3.4407497119903563 and perplexity is 31.21034817157837
At time: 170.0395884513855 and batch: 450, loss is 3.460868854522705 and perplexity is 31.844632851538115
At time: 170.93786120414734 and batch: 500, loss is 3.3198534631729126 and perplexity is 27.656297595471205
At time: 171.84103274345398 and batch: 550, loss is 3.3620813989639284 and perplexity is 28.849175075177307
At time: 172.74628114700317 and batch: 600, loss is 3.3910348844528198 and perplexity is 29.69666899454026
At time: 173.6464557647705 and batch: 650, loss is 3.222059621810913 and perplexity is 25.07972176518189
At time: 174.54137063026428 and batch: 700, loss is 3.209690794944763 and perplexity is 24.771425590277364
At time: 175.43659663200378 and batch: 750, loss is 3.300723853111267 and perplexity is 27.132271593423663
At time: 176.32981872558594 and batch: 800, loss is 3.25978777885437 and perplexity is 26.044009466468445
At time: 177.22177863121033 and batch: 850, loss is 3.301385374069214 and perplexity is 27.150226097705104
At time: 178.13238525390625 and batch: 900, loss is 3.271844673156738 and perplexity is 26.35991995857282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308676941754067 and perplexity of 74.34206496230696
finished 10 epochs...
Completing Train Step...
At time: 180.33919739723206 and batch: 50, loss is 3.6208287191390993 and perplexity is 37.368523003919435
At time: 181.24402022361755 and batch: 100, loss is 3.5062335681915284 and perplexity is 33.32252411535177
At time: 182.14350152015686 and batch: 150, loss is 3.518847608566284 and perplexity is 33.74551800224226
At time: 183.04447221755981 and batch: 200, loss is 3.4012374258041382 and perplexity is 30.001201348312808
At time: 183.94247031211853 and batch: 250, loss is 3.5390570878982546 and perplexity is 34.43443523280777
At time: 184.8462462425232 and batch: 300, loss is 3.4927646923065185 and perplexity is 32.876716179250835
At time: 185.75015592575073 and batch: 350, loss is 3.4853420734405516 and perplexity is 32.63358828460193
At time: 186.65147590637207 and batch: 400, loss is 3.4265138053894044 and perplexity is 30.769188176735916
At time: 187.5494658946991 and batch: 450, loss is 3.447556414604187 and perplexity is 31.423512379730656
At time: 188.4508900642395 and batch: 500, loss is 3.3084677124023436 and perplexity is 27.34319571620859
At time: 189.35354256629944 and batch: 550, loss is 3.3524877500534056 and perplexity is 28.57372959438617
At time: 190.25391697883606 and batch: 600, loss is 3.3825044584274293 and perplexity is 29.444421176788975
At time: 191.15114331245422 and batch: 650, loss is 3.215672836303711 and perplexity is 24.920053388277893
At time: 192.04923057556152 and batch: 700, loss is 3.205652885437012 and perplexity is 24.671602489322268
At time: 192.9490978717804 and batch: 750, loss is 3.298958215713501 and perplexity is 27.084408107227016
At time: 193.85115122795105 and batch: 800, loss is 3.2602998208999634 and perplexity is 26.05734850913501
At time: 194.75146555900574 and batch: 850, loss is 3.304836850166321 and perplexity is 27.244096356402203
At time: 195.64799094200134 and batch: 900, loss is 3.277094564437866 and perplexity is 26.49867056665893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308588733411815 and perplexity of 74.33550766120565
finished 11 epochs...
Completing Train Step...
At time: 197.85705757141113 and batch: 50, loss is 3.6133268308639526 and perplexity is 37.08923741379304
At time: 198.76357221603394 and batch: 100, loss is 3.497643036842346 and perplexity is 33.03749196897915
At time: 199.66258239746094 and batch: 150, loss is 3.509727945327759 and perplexity is 33.43916926411946
At time: 200.56605887413025 and batch: 200, loss is 3.3919547748565675 and perplexity is 29.723999243861417
At time: 201.47034335136414 and batch: 250, loss is 3.5292497491836547 and perplexity is 34.098375681579256
At time: 202.37175250053406 and batch: 300, loss is 3.483196873664856 and perplexity is 32.563657752632075
At time: 203.26630997657776 and batch: 350, loss is 3.4759303522109986 and perplexity is 32.32789087491324
At time: 204.17022895812988 and batch: 400, loss is 3.417666583061218 and perplexity is 30.498166988242552
At time: 205.06940364837646 and batch: 450, loss is 3.4390983629226684 and perplexity is 31.158851523396418
At time: 205.97258114814758 and batch: 500, loss is 3.3008562994003294 and perplexity is 27.13586540009797
At time: 206.87553334236145 and batch: 550, loss is 3.3458357524871825 and perplexity is 28.384287995743673
At time: 207.76956868171692 and batch: 600, loss is 3.3768640184402465 and perplexity is 29.27880918749986
At time: 208.66884684562683 and batch: 650, loss is 3.211177659034729 and perplexity is 24.80828472891906
At time: 209.56768941879272 and batch: 700, loss is 3.2023726081848145 and perplexity is 24.590805383804454
At time: 210.46500706672668 and batch: 750, loss is 3.2970869636535642 and perplexity is 27.03377374236901
At time: 211.36349034309387 and batch: 800, loss is 3.2595295095443726 and perplexity is 26.037283966646456
At time: 212.26235055923462 and batch: 850, loss is 3.305415143966675 and perplexity is 27.259856004835658
At time: 213.15775299072266 and batch: 900, loss is 3.278303518295288 and perplexity is 26.53072560928654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309250923052226 and perplexity of 74.38474816577029
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 215.35144710540771 and batch: 50, loss is 3.610212707519531 and perplexity is 36.973916608537984
At time: 216.2583863735199 and batch: 100, loss is 3.497143044471741 and perplexity is 33.02097760392303
At time: 217.1631157398224 and batch: 150, loss is 3.509613060951233 and perplexity is 33.43532784667064
At time: 218.06503748893738 and batch: 200, loss is 3.392211995124817 and perplexity is 29.73164584230826
At time: 218.96465396881104 and batch: 250, loss is 3.5302963399887086 and perplexity is 34.1340814094189
At time: 219.8706932067871 and batch: 300, loss is 3.483228259086609 and perplexity is 32.564679792802956
At time: 220.77636671066284 and batch: 350, loss is 3.474120979309082 and perplexity is 32.26945055130065
At time: 221.67425179481506 and batch: 400, loss is 3.41607985496521 and perplexity is 30.44981306221713
At time: 222.5809633731842 and batch: 450, loss is 3.435798177719116 and perplexity is 31.056191035126243
At time: 223.5077781677246 and batch: 500, loss is 3.2962956619262695 and perplexity is 27.01239033199195
At time: 224.4076795578003 and batch: 550, loss is 3.3407372188568116 and perplexity is 28.239938048053148
At time: 225.30718755722046 and batch: 600, loss is 3.371792960166931 and perplexity is 29.130710465931283
At time: 226.22508001327515 and batch: 650, loss is 3.204483709335327 and perplexity is 24.642773897435816
At time: 227.123450756073 and batch: 700, loss is 3.1948625659942627 and perplexity is 24.406819134864254
At time: 228.02201986312866 and batch: 750, loss is 3.289139733314514 and perplexity is 26.819781564353057
At time: 228.9171347618103 and batch: 800, loss is 3.2489796543121336 and perplexity is 25.764038275735068
At time: 229.81739830970764 and batch: 850, loss is 3.294382581710815 and perplexity is 26.960762861967265
At time: 230.71657252311707 and batch: 900, loss is 3.2683703565597533 and perplexity is 26.268496160738405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3084039557470035 and perplexity of 74.32177338861992
finished 13 epochs...
Completing Train Step...
At time: 232.9304814338684 and batch: 50, loss is 3.6063425874710084 and perplexity is 36.83109965024295
At time: 233.83473205566406 and batch: 100, loss is 3.492937469482422 and perplexity is 32.88239701617061
At time: 234.7324616909027 and batch: 150, loss is 3.5056788301467896 and perplexity is 33.30404396977208
At time: 235.63551783561707 and batch: 200, loss is 3.3874971008300783 and perplexity is 29.591794226822834
At time: 236.53644609451294 and batch: 250, loss is 3.5258635091781616 and perplexity is 33.983105673826245
At time: 237.4361412525177 and batch: 300, loss is 3.4791465044021606 and perplexity is 32.43202966532126
At time: 238.33762907981873 and batch: 350, loss is 3.470521674156189 and perplexity is 32.1535117264208
At time: 239.23642539978027 and batch: 400, loss is 3.4124881553649904 and perplexity is 30.340642651835413
At time: 240.13810467720032 and batch: 450, loss is 3.43282808303833 and perplexity is 30.964088052323905
At time: 241.03677344322205 and batch: 500, loss is 3.293690633773804 and perplexity is 26.94211387053433
At time: 241.93625020980835 and batch: 550, loss is 3.338613739013672 and perplexity is 28.18003473308703
At time: 242.8393874168396 and batch: 600, loss is 3.3701565742492674 and perplexity is 29.083080362796803
At time: 243.73657369613647 and batch: 650, loss is 3.203005509376526 and perplexity is 24.606373859970887
At time: 244.63684511184692 and batch: 700, loss is 3.1942205953598024 and perplexity is 24.391155701969005
At time: 245.5512821674347 and batch: 750, loss is 3.288820505142212 and perplexity is 26.811221300914045
At time: 246.44767785072327 and batch: 800, loss is 3.2493025970458986 and perplexity is 25.772359928324953
At time: 247.34723591804504 and batch: 850, loss is 3.2959057664871216 and perplexity is 27.001860377125105
At time: 248.2466151714325 and batch: 900, loss is 3.2704198026657103 and perplexity is 26.322387232465022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3081556346318495 and perplexity of 74.3033200142479
finished 14 epochs...
Completing Train Step...
At time: 250.45643019676208 and batch: 50, loss is 3.603995132446289 and perplexity is 36.74474170066098
At time: 251.3683614730835 and batch: 100, loss is 3.4903142499923705 and perplexity is 32.79625230907431
At time: 252.26958394050598 and batch: 150, loss is 3.503083138465881 and perplexity is 33.21770903780576
At time: 253.16584610939026 and batch: 200, loss is 3.3847575187683105 and perplexity is 29.510836024637435
At time: 254.0645182132721 and batch: 250, loss is 3.523018145561218 and perplexity is 33.88654881618049
At time: 254.9616334438324 and batch: 300, loss is 3.476487202644348 and perplexity is 32.34589768804175
At time: 255.8607575893402 and batch: 350, loss is 3.467942337989807 and perplexity is 32.07068387710329
At time: 256.76336336135864 and batch: 400, loss is 3.4101037216186523 and perplexity is 30.268383582328585
At time: 257.6614806652069 and batch: 450, loss is 3.4306528186798095 and perplexity is 30.896806179649612
At time: 258.5645968914032 and batch: 500, loss is 3.2917901182174685 and perplexity is 26.890958590103512
At time: 259.4664008617401 and batch: 550, loss is 3.336954641342163 and perplexity is 28.133320065891688
At time: 260.3680021762848 and batch: 600, loss is 3.368771572113037 and perplexity is 29.042828115524827
At time: 261.26884865760803 and batch: 650, loss is 3.2019347953796387 and perplexity is 24.580041570757366
At time: 262.167685508728 and batch: 700, loss is 3.1935960245132446 and perplexity is 24.37592645357105
At time: 263.0637164115906 and batch: 750, loss is 3.2885378646850585 and perplexity is 26.803644435883452
At time: 263.964075088501 and batch: 800, loss is 3.249429497718811 and perplexity is 25.7756306656674
At time: 264.86016631126404 and batch: 850, loss is 3.2966410970687865 and perplexity is 27.02172297271424
At time: 265.76303482055664 and batch: 900, loss is 3.2713926076889037 and perplexity is 26.348006242117467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308200365876498 and perplexity of 74.30664376857095
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 267.93834376335144 and batch: 50, loss is 3.6028954648971556 and perplexity is 36.704356909603256
At time: 268.8479745388031 and batch: 100, loss is 3.490025715827942 and perplexity is 32.78679083486284
At time: 269.7340741157532 and batch: 150, loss is 3.503051242828369 and perplexity is 33.21664955469585
At time: 270.6310570240021 and batch: 200, loss is 3.3845191621780395 and perplexity is 29.503802760632265
At time: 271.52728819847107 and batch: 250, loss is 3.52347074508667 and perplexity is 33.90188932338516
At time: 272.4226243495941 and batch: 300, loss is 3.4770661115646364 and perplexity is 32.364628437924075
At time: 273.32059955596924 and batch: 350, loss is 3.467739853858948 and perplexity is 32.06419072995419
At time: 274.2174229621887 and batch: 400, loss is 3.4098382902145388 and perplexity is 30.260350468941528
At time: 275.122127532959 and batch: 450, loss is 3.4300612688064573 and perplexity is 30.878534582680306
At time: 276.0230972766876 and batch: 500, loss is 3.2906294870376587 and perplexity is 26.859766210034817
At time: 276.9197413921356 and batch: 550, loss is 3.334979815483093 and perplexity is 28.077816480919893
At time: 277.8231611251831 and batch: 600, loss is 3.3664065265655516 and perplexity is 28.974221664872978
At time: 278.72148537635803 and batch: 650, loss is 3.200163140296936 and perplexity is 24.536532767851714
At time: 279.6231198310852 and batch: 700, loss is 3.1911099052429197 and perplexity is 24.31540026192348
At time: 280.5229527950287 and batch: 750, loss is 3.2864016771316527 and perplexity is 26.74644793722825
At time: 281.41972064971924 and batch: 800, loss is 3.2462268543243407 and perplexity is 25.693212560723726
At time: 282.3160226345062 and batch: 850, loss is 3.2930225801467894 and perplexity is 26.924121104379203
At time: 283.210489988327 and batch: 900, loss is 3.2678056526184083 and perplexity is 26.25366642502242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307211261906036 and perplexity of 74.23318310829217
finished 16 epochs...
Completing Train Step...
At time: 285.4152092933655 and batch: 50, loss is 3.6019991397857667 and perplexity is 36.67147261252002
At time: 286.31204438209534 and batch: 100, loss is 3.4890018463134767 and perplexity is 32.75323861872659
At time: 287.2082986831665 and batch: 150, loss is 3.5019777488708494 and perplexity is 33.18101081451842
At time: 288.10688948631287 and batch: 200, loss is 3.383202929496765 and perplexity is 29.464994437211494
At time: 289.004522562027 and batch: 250, loss is 3.5222603559494017 and perplexity is 33.86087966864232
At time: 289.89926767349243 and batch: 300, loss is 3.4757950592041014 and perplexity is 32.323517433204835
At time: 290.80959701538086 and batch: 350, loss is 3.4667747449874877 and perplexity is 32.03326022307863
At time: 291.7092115879059 and batch: 400, loss is 3.4088497161865234 and perplexity is 30.23045065391462
At time: 292.60985827445984 and batch: 450, loss is 3.4292547416687014 and perplexity is 30.853640246893384
At time: 293.5114951133728 and batch: 500, loss is 3.290004858970642 and perplexity is 26.84299408490189
At time: 294.41167163848877 and batch: 550, loss is 3.3345590305328368 and perplexity is 28.066004243688777
At time: 295.30806493759155 and batch: 600, loss is 3.3662027168273925 and perplexity is 28.968317038072758
At time: 296.20290899276733 and batch: 650, loss is 3.1997587490081787 and perplexity is 24.526612413731666
At time: 297.10200357437134 and batch: 700, loss is 3.1910606002807618 and perplexity is 24.31420142158834
At time: 298.00100445747375 and batch: 750, loss is 3.286411085128784 and perplexity is 26.746699568917396
At time: 298.9006884098053 and batch: 800, loss is 3.2463259220123293 and perplexity is 25.695758053975098
At time: 299.8017485141754 and batch: 850, loss is 3.293485403060913 and perplexity is 26.936585088655228
At time: 300.7014915943146 and batch: 900, loss is 3.2684588766098024 and perplexity is 26.270821552253636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306839616331335 and perplexity of 74.20559980021869
finished 17 epochs...
Completing Train Step...
At time: 302.9014766216278 and batch: 50, loss is 3.601308994293213 and perplexity is 36.64617269230833
At time: 303.80814719200134 and batch: 100, loss is 3.4882261848449705 and perplexity is 32.72784304401891
At time: 304.7071223258972 and batch: 150, loss is 3.5011893463134767 and perplexity is 33.1548611303295
At time: 305.6072142124176 and batch: 200, loss is 3.382314395904541 and perplexity is 29.43882542760308
At time: 306.5082743167877 and batch: 250, loss is 3.5213872385025025 and perplexity is 33.83132804673196
At time: 307.40847396850586 and batch: 300, loss is 3.4749478912353515 and perplexity is 32.29614558051357
At time: 308.30858063697815 and batch: 350, loss is 3.4660230112075805 and perplexity is 32.00918878807737
At time: 309.21011424064636 and batch: 400, loss is 3.4081381797790526 and perplexity is 30.208948238443032
At time: 310.1144013404846 and batch: 450, loss is 3.4286443328857423 and perplexity is 30.83481266074699
At time: 311.01422691345215 and batch: 500, loss is 3.289494638442993 and perplexity is 26.829301731653104
At time: 311.915251493454 and batch: 550, loss is 3.3341710567474365 and perplexity is 28.05511748180696
At time: 312.81477999687195 and batch: 600, loss is 3.365950655937195 and perplexity is 28.961016178461858
At time: 313.7224624156952 and batch: 650, loss is 3.1994611597061158 and perplexity is 24.519314642184725
At time: 314.62580728530884 and batch: 700, loss is 3.1909840154647826 and perplexity is 24.312339394249207
At time: 315.5302095413208 and batch: 750, loss is 3.286401333808899 and perplexity is 26.746438754565666
At time: 316.4307198524475 and batch: 800, loss is 3.2463918828964236 and perplexity is 25.697453024794083
At time: 317.33083963394165 and batch: 850, loss is 3.2938075637817383 and perplexity is 26.945264396314897
At time: 318.2333014011383 and batch: 900, loss is 3.26889235496521 and perplexity is 26.28221185332144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306686610391695 and perplexity of 74.1942467712573
finished 18 epochs...
Completing Train Step...
At time: 320.4273397922516 and batch: 50, loss is 3.6007033157348634 and perplexity is 36.62398361166654
At time: 321.33693528175354 and batch: 100, loss is 3.4875541591644286 and perplexity is 32.70585648161933
At time: 322.236040353775 and batch: 150, loss is 3.500510768890381 and perplexity is 33.13237062173928
At time: 323.1349594593048 and batch: 200, loss is 3.3815905141830442 and perplexity is 29.417522911155057
At time: 324.03338623046875 and batch: 250, loss is 3.520652632713318 and perplexity is 33.80648448352293
At time: 324.934246301651 and batch: 300, loss is 3.4742560338974 and perplexity is 32.27380898296658
At time: 325.8344535827637 and batch: 350, loss is 3.465363268852234 and perplexity is 31.988077935101238
At time: 326.73434376716614 and batch: 400, loss is 3.4075335454940796 and perplexity is 30.190688393445544
At time: 327.6331593990326 and batch: 450, loss is 3.428107886314392 and perplexity is 30.81827586716658
At time: 328.5360527038574 and batch: 500, loss is 3.2890288829803467 and perplexity is 26.816808747376275
At time: 329.43701553344727 and batch: 550, loss is 3.3337886095047 and perplexity is 28.044389930973264
At time: 330.3391592502594 and batch: 600, loss is 3.3656703519821165 and perplexity is 28.952899428715618
At time: 331.2400734424591 and batch: 650, loss is 3.1991993045806884 and perplexity is 24.512894974521824
At time: 332.14091062545776 and batch: 700, loss is 3.190878448486328 and perplexity is 24.30977294950849
At time: 333.04695868492126 and batch: 750, loss is 3.2863685655593873 and perplexity is 26.745562334946456
At time: 333.949679851532 and batch: 800, loss is 3.2464339256286623 and perplexity is 25.69853343864244
At time: 334.8491461277008 and batch: 850, loss is 3.294045829772949 and perplexity is 26.951685301355997
At time: 335.75701904296875 and batch: 900, loss is 3.269204559326172 and perplexity is 26.29041855549522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306623066941353 and perplexity of 74.18953236260838
finished 19 epochs...
Completing Train Step...
At time: 337.95756363868713 and batch: 50, loss is 3.6001457500457765 and perplexity is 36.603569026771915
At time: 338.85238766670227 and batch: 100, loss is 3.4869408798217774 and perplexity is 32.685804804723524
At time: 339.74853563308716 and batch: 150, loss is 3.4998897743225097 and perplexity is 33.11180198672878
At time: 340.64550375938416 and batch: 200, loss is 3.380947222709656 and perplexity is 29.398604955030432
At time: 341.5409986972809 and batch: 250, loss is 3.519991607666016 and perplexity is 33.784144934832156
At time: 342.44181418418884 and batch: 300, loss is 3.473636016845703 and perplexity is 32.25380487315935
At time: 343.3375084400177 and batch: 350, loss is 3.464755311012268 and perplexity is 31.968636442737832
At time: 344.2383692264557 and batch: 400, loss is 3.406981739997864 and perplexity is 30.174033601181062
At time: 345.1333909034729 and batch: 450, loss is 3.4276082944869994 and perplexity is 30.802883153766086
At time: 346.0262894630432 and batch: 500, loss is 3.2885873556137084 and perplexity is 26.80497100596411
At time: 346.92595767974854 and batch: 550, loss is 3.333411374092102 and perplexity is 28.03381258916599
At time: 347.8212139606476 and batch: 600, loss is 3.3653776597976686 and perplexity is 28.94442638139358
At time: 348.72110056877136 and batch: 650, loss is 3.1989519739151 and perplexity is 24.506832933587496
At time: 349.6216254234314 and batch: 700, loss is 3.1907534074783324 and perplexity is 24.306733421031545
At time: 350.5210497379303 and batch: 750, loss is 3.2863187170028687 and perplexity is 26.744229140499968
At time: 351.4186942577362 and batch: 800, loss is 3.2464577674865724 and perplexity is 25.699146146729202
At time: 352.3144648075104 and batch: 850, loss is 3.2942290830612184 and perplexity is 26.956624738882134
At time: 353.21225357055664 and batch: 900, loss is 3.2694421005249024 and perplexity is 26.296664354821978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306605508882705 and perplexity of 74.1882297498838
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7ff1fbfcfb70>
ELAPSED
1934.7703680992126


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.55663006026495}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.8545719625116199, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.1564310047238061, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.49362276522547}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.2517205538203331, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.8811741615359234, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.41348685492105}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3333549557419628, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.37947969550812455, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.1882297498838}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3269958897558217, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.3765171921419075, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.11947187830441}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.36932513948629464, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.04675343481792571, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.01969624312107}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.7560525992981961, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.10635874069499063, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.55663006026495}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.8545719625116199, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.1564310047238061, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -75.49362276522547}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.2517205538203331, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.8811741615359234, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.41348685492105}, {'params': {'wordvec_dim': 300, 'rnn_dropout': 0.3333549557419628, 'tie_weights': True, 'batch_size': 32, 'tune_wordvecs': True, 'wordvec_source': 'gigavec', 'seq_len': 35, 'dropout': 0.37947969550812455, 'data': 'ptb', 'num_layers': 3}, 'best_accuracy': -74.1882297498838}]
