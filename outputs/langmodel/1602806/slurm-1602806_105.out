Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'batch_size': 80, 'anneal': 3.2790400431199034, 'lr': 8.672763035007602, 'wordvec_source': '', 'num_layers': 1, 'tune_wordvecs': True, 'seq_len': 50, 'dropout': 0.4944332014902323, 'data': 'gigasmall'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 5360 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 11.49657154083252 and batch: 50, loss is 8.37108097076416 and perplexity is 4320.30366142432
At time: 17.507841110229492 and batch: 100, loss is 7.189836883544922 and perplexity is 1325.8869139940887
At time: 23.590564012527466 and batch: 150, loss is 6.849808206558228 and perplexity is 943.6998938629545
At time: 29.679714918136597 and batch: 200, loss is 6.652618703842163 and perplexity is 774.8106708587436
At time: 35.782851457595825 and batch: 250, loss is 6.560219993591309 and perplexity is 706.4270869339324
At time: 41.883666038513184 and batch: 300, loss is 6.4289452171325685 and perplexity is 619.5201440287591
At time: 47.975011348724365 and batch: 350, loss is 6.368097562789917 and perplexity is 582.9477518055539
At time: 54.58204746246338 and batch: 400, loss is 6.280669345855713 and perplexity is 534.1460726572353
At time: 60.659708976745605 and batch: 450, loss is 6.269233932495117 and perplexity is 528.0726835628034
At time: 66.74646210670471 and batch: 500, loss is 6.205796966552734 and perplexity is 495.6137861967063
At time: 72.85531735420227 and batch: 550, loss is 6.202817659378052 and perplexity is 494.1393979060146
At time: 78.95426654815674 and batch: 600, loss is 6.141367197036743 and perplexity is 464.68845743487196
At time: 85.05655908584595 and batch: 650, loss is 6.117733879089355 and perplexity is 453.83508315631684
At time: 91.17116093635559 and batch: 700, loss is 6.13617600440979 and perplexity is 462.28242064428
At time: 97.32169032096863 and batch: 750, loss is 6.111554212570191 and perplexity is 451.03918145533004
At time: 103.51302146911621 and batch: 800, loss is 6.061511926651001 and perplexity is 429.0235989622167
At time: 109.67859649658203 and batch: 850, loss is 6.0639944171905515 and perplexity is 430.08996906708444
At time: 115.83347463607788 and batch: 900, loss is 6.01886365890503 and perplexity is 411.11116768367646
At time: 122.0078854560852 and batch: 950, loss is 6.047248487472534 and perplexity is 422.9476817000395
At time: 128.1639051437378 and batch: 1000, loss is 6.073851232528686 and perplexity is 434.3502483712855
At time: 134.3195161819458 and batch: 1050, loss is 6.045188941955566 and perplexity is 422.07749809724146
At time: 140.50442147254944 and batch: 1100, loss is 6.023373126983643 and perplexity is 412.96924669638946
At time: 146.648677110672 and batch: 1150, loss is 6.012694759368896 and perplexity is 408.5828706152843
At time: 152.76804208755493 and batch: 1200, loss is 6.005999689102173 and perplexity is 405.85651633729356
At time: 158.91952347755432 and batch: 1250, loss is 5.944247188568116 and perplexity is 381.5520164331585
At time: 165.08025217056274 and batch: 1300, loss is 5.963302774429321 and perplexity is 388.8924294614643
At time: 171.22322010993958 and batch: 1350, loss is 5.95009599685669 and perplexity is 383.79017994368206
At time: 177.413414478302 and batch: 1400, loss is 5.913468379974365 and perplexity is 369.98718872615575
At time: 183.54642510414124 and batch: 1450, loss is 5.945637292861939 and perplexity is 382.08278235404134
At time: 189.69284582138062 and batch: 1500, loss is 5.927493085861206 and perplexity is 375.2127077623687
At time: 195.8241457939148 and batch: 1550, loss is 5.932909440994263 and perplexity is 377.2505067777338
At time: 201.99111676216125 and batch: 1600, loss is 5.916445293426514 and perplexity is 371.0902496091764
At time: 208.15399980545044 and batch: 1650, loss is 5.938675241470337 and perplexity is 379.4319407424295
At time: 214.32803630828857 and batch: 1700, loss is 5.912646312713623 and perplexity is 369.6831593548279
At time: 220.49647855758667 and batch: 1750, loss is 5.9279058647155765 and perplexity is 375.3676196039907
At time: 226.64380049705505 and batch: 1800, loss is 5.888501033782959 and perplexity is 360.86395587827366
At time: 232.77790307998657 and batch: 1850, loss is 5.91485294342041 and perplexity is 370.499814262568
At time: 238.9289689064026 and batch: 1900, loss is 5.866212396621704 and perplexity is 352.9097634454522
At time: 245.0653591156006 and batch: 1950, loss is 5.833505525588989 and perplexity is 341.55390886098894
At time: 251.23197555541992 and batch: 2000, loss is 5.845783786773682 and perplexity is 345.7734482561896
At time: 257.41781783103943 and batch: 2050, loss is 5.869714469909668 and perplexity is 354.14784596356003
At time: 263.60170912742615 and batch: 2100, loss is 5.8450573062896725 and perplexity is 345.52234181715056
At time: 269.7647910118103 and batch: 2150, loss is 5.85533221244812 and perplexity is 349.09085311932427
At time: 275.95867681503296 and batch: 2200, loss is 5.837835359573364 and perplexity is 343.03598684344666
At time: 282.13023233413696 and batch: 2250, loss is 5.81822470664978 and perplexity is 336.3743600817204
At time: 288.28699684143066 and batch: 2300, loss is 5.826830997467041 and perplexity is 339.2817887908989
At time: 294.4306526184082 and batch: 2350, loss is 5.829360303878784 and perplexity is 340.1410225693567
At time: 300.5956723690033 and batch: 2400, loss is 5.843289051055908 and perplexity is 344.91190998654406
At time: 306.7617793083191 and batch: 2450, loss is 5.823730144500733 and perplexity is 338.23135531128287
At time: 312.9443316459656 and batch: 2500, loss is 5.809277791976928 and perplexity is 333.3782702496845
At time: 319.1276535987854 and batch: 2550, loss is 5.83657392501831 and perplexity is 342.60354220366406
At time: 325.27895617485046 and batch: 2600, loss is 5.846005668640137 and perplexity is 345.85017762637256
At time: 331.4447703361511 and batch: 2650, loss is 5.844555902481079 and perplexity is 345.34913902490786
At time: 337.5900237560272 and batch: 2700, loss is 5.818214817047119 and perplexity is 336.37103348940315
At time: 343.76166796684265 and batch: 2750, loss is 5.7634745121002195 and perplexity is 318.4528772988205
At time: 349.95171093940735 and batch: 2800, loss is 5.771204271316528 and perplexity is 320.9239795710141
At time: 356.1110963821411 and batch: 2850, loss is 5.777749090194702 and perplexity is 323.0312572442809
At time: 362.2865059375763 and batch: 2900, loss is 5.774907712936401 and perplexity is 322.1147063263357
At time: 368.45436453819275 and batch: 2950, loss is 5.7877670097351075 and perplexity is 326.2836221531593
At time: 374.62588334083557 and batch: 3000, loss is 5.794208011627197 and perplexity is 328.39199831915136
At time: 380.7773153781891 and batch: 3050, loss is 5.729524602890015 and perplexity is 307.82289547264475
At time: 386.94084191322327 and batch: 3100, loss is 5.7502610397338865 and perplexity is 314.27268723772585
At time: 393.09686851501465 and batch: 3150, loss is 5.7484996891021725 and perplexity is 313.7196300483259
At time: 399.24748063087463 and batch: 3200, loss is 5.746405506134034 and perplexity is 313.0633311871119
At time: 405.4115629196167 and batch: 3250, loss is 5.739750957489013 and perplexity is 310.98695236504796
At time: 411.56869530677795 and batch: 3300, loss is 5.735896034240723 and perplexity is 309.7904292665529
At time: 417.73424911499023 and batch: 3350, loss is 5.726621742248535 and perplexity is 306.9306242009766
At time: 423.88062810897827 and batch: 3400, loss is 5.730896129608154 and perplexity is 308.2453724512461
At time: 430.03479170799255 and batch: 3450, loss is 5.736094150543213 and perplexity is 309.8518098809955
At time: 436.167995929718 and batch: 3500, loss is 5.751245603561402 and perplexity is 314.58226113017685
At time: 442.3291931152344 and batch: 3550, loss is 5.703693952560425 and perplexity is 299.9734445466566
At time: 448.4919764995575 and batch: 3600, loss is 5.707914867401123 and perplexity is 301.24228285596985
At time: 454.6493537425995 and batch: 3650, loss is 5.74251205444336 and perplexity is 311.84680401267957
At time: 460.816201210022 and batch: 3700, loss is 5.727872085571289 and perplexity is 307.3146328788095
At time: 466.9949553012848 and batch: 3750, loss is 5.705932455062866 and perplexity is 300.6456879815343
At time: 473.1610338687897 and batch: 3800, loss is 5.671408996582032 and perplexity is 290.4434800932559
At time: 479.3181025981903 and batch: 3850, loss is 5.7229554557800295 and perplexity is 305.8073889160399
At time: 485.46420073509216 and batch: 3900, loss is 5.72245834350586 and perplexity is 305.65540608887216
At time: 491.6040759086609 and batch: 3950, loss is 5.70505539894104 and perplexity is 300.382120439048
At time: 497.757577419281 and batch: 4000, loss is 5.680752248764038 and perplexity is 293.1698836767716
At time: 503.93041825294495 and batch: 4050, loss is 5.700113143920898 and perplexity is 298.90121790968897
At time: 510.11538314819336 and batch: 4100, loss is 5.664347286224365 and perplexity is 288.3996772268146
At time: 516.3109920024872 and batch: 4150, loss is 5.709982013702392 and perplexity is 301.86563878874534
At time: 522.4771108627319 and batch: 4200, loss is 5.639660320281982 and perplexity is 281.367127534507
At time: 528.6430771350861 and batch: 4250, loss is 5.70010516166687 and perplexity is 298.8988320137607
At time: 534.7890665531158 and batch: 4300, loss is 5.695252180099487 and perplexity is 297.451795552104
At time: 540.9678592681885 and batch: 4350, loss is 5.719946355819702 and perplexity is 304.888567020952
At time: 547.1310923099518 and batch: 4400, loss is 5.682361536026001 and perplexity is 293.64205806637574
At time: 553.2995274066925 and batch: 4450, loss is 5.711605453491211 and perplexity is 302.35609748499405
At time: 559.4730541706085 and batch: 4500, loss is 5.657516889572143 and perplexity is 286.4365052910231
At time: 565.6477110385895 and batch: 4550, loss is 5.68763409614563 and perplexity is 295.19439226264524
At time: 571.8113524913788 and batch: 4600, loss is 5.676567144393921 and perplexity is 291.9455009871799
At time: 577.992681980133 and batch: 4650, loss is 5.62684817314148 and perplexity is 277.7852055496983
At time: 584.1504414081573 and batch: 4700, loss is 5.709389419555664 and perplexity is 301.68680797038206
At time: 590.3240416049957 and batch: 4750, loss is 5.654503269195557 and perplexity is 285.5745937914254
At time: 596.5015029907227 and batch: 4800, loss is 5.668001518249512 and perplexity is 289.455484470815
At time: 602.6986718177795 and batch: 4850, loss is 5.630106573104858 and perplexity is 278.69181710267674
At time: 608.8819978237152 and batch: 4900, loss is 5.653220443725586 and perplexity is 285.20848630514416
At time: 615.0662732124329 and batch: 4950, loss is 5.670742959976196 and perplexity is 290.25009851034287
At time: 621.2346336841583 and batch: 5000, loss is 5.629701700210571 and perplexity is 278.5790051788583
At time: 627.4189524650574 and batch: 5050, loss is 5.639080238342285 and perplexity is 281.20395887557135
At time: 633.6012420654297 and batch: 5100, loss is 5.616392183303833 and perplexity is 274.89581828946666
At time: 639.7886145114899 and batch: 5150, loss is 5.63559847831726 and perplexity is 280.2265766652061
At time: 645.9646134376526 and batch: 5200, loss is 5.674518003463745 and perplexity is 291.34787602972466
At time: 652.1084141731262 and batch: 5250, loss is 5.659664745330811 and perplexity is 287.0523907682182
At time: 658.2707967758179 and batch: 5300, loss is 5.63138053894043 and perplexity is 279.0470872092581
At time: 664.4233522415161 and batch: 5350, loss is 5.604084577560425 and perplexity is 271.5332440027432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 356 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
