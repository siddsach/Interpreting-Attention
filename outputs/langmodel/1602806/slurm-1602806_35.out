Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 5.3286642434072204, 'num_layers': 1, 'dropout': 0.1607544487575544, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 4.514278932000416, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.9188289642333984 and batch: 50, loss is 6.55876407623291 and perplexity is 705.3993358175106
At time: 3.321103096008301 and batch: 100, loss is 5.639514036178589 and perplexity is 281.3259710068775
At time: 4.721311092376709 and batch: 150, loss is 5.295563955307006 and perplexity is 199.45007518380675
At time: 6.13496470451355 and batch: 200, loss is 5.131380500793457 and perplexity is 169.25060743952167
At time: 7.544800758361816 and batch: 250, loss is 5.0914515209198 and perplexity is 162.6257454888783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.162197494506836 and perplexity of 174.54760186630745
Finished 1 epochs...
Completing Train Step...
At time: 9.66337251663208 and batch: 50, loss is 4.878445348739624 and perplexity is 131.42618309765834
At time: 11.069022178649902 and batch: 100, loss is 4.711692018508911 and perplexity is 111.24022127250375
At time: 12.481202125549316 and batch: 150, loss is 4.689947919845581 and perplexity is 108.84751086462362
At time: 13.881754875183105 and batch: 200, loss is 4.627259979248047 and perplexity is 102.23355791935269
At time: 15.27613091468811 and batch: 250, loss is 4.6306906414031985 and perplexity is 102.58488902207128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.884921646118164 and perplexity of 132.28010026888273
Finished 2 epochs...
Completing Train Step...
At time: 17.51505756378174 and batch: 50, loss is 4.529599695205689 and perplexity is 92.72143681640368
At time: 18.906798362731934 and batch: 100, loss is 4.393638954162598 and perplexity is 80.93440033673164
At time: 20.301687717437744 and batch: 150, loss is 4.4351747035980225 and perplexity is 84.36686279535468
At time: 21.694941520690918 and batch: 200, loss is 4.398942174911499 and perplexity is 81.36475344806281
At time: 23.089925050735474 and batch: 250, loss is 4.415109720230102 and perplexity is 82.6909132744198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.77191276550293 and perplexity of 118.14500967033388
Finished 3 epochs...
Completing Train Step...
At time: 25.20292043685913 and batch: 50, loss is 4.335723886489868 and perplexity is 76.38022949875777
At time: 26.607566595077515 and batch: 100, loss is 4.211908450126648 and perplexity is 67.48520914750131
At time: 28.085806608200073 and batch: 150, loss is 4.274499406814575 and perplexity is 71.84416560659979
At time: 29.621490240097046 and batch: 200, loss is 4.247682447433472 and perplexity is 69.9431274928527
At time: 31.164302587509155 and batch: 250, loss is 4.270726804733276 and perplexity is 71.57363677757384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.718403625488281 and perplexity of 111.98933297763953
Finished 4 epochs...
Completing Train Step...
At time: 33.47284245491028 and batch: 50, loss is 4.2001396703720095 and perplexity is 66.6956457960721
At time: 35.02965068817139 and batch: 100, loss is 4.082337746620178 and perplexity is 59.28389871488482
At time: 36.57344651222229 and batch: 150, loss is 4.156239986419678 and perplexity is 63.8310651497809
At time: 38.116636514663696 and batch: 200, loss is 4.134343514442444 and perplexity is 62.44858101915621
At time: 39.65946936607361 and batch: 250, loss is 4.158873972892761 and perplexity is 63.999416932773705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6891944885253904 and perplexity of 108.76553262717722
Finished 5 epochs...
Completing Train Step...
At time: 41.96428680419922 and batch: 50, loss is 4.094839868545532 and perplexity is 60.02972574047177
At time: 43.503504514694214 and batch: 100, loss is 3.981345462799072 and perplexity is 53.58908786803765
At time: 45.04569745063782 and batch: 150, loss is 4.058916335105896 and perplexity is 57.911520381465174
At time: 46.58901357650757 and batch: 200, loss is 4.0407849264144895 and perplexity is 56.870964813181125
At time: 48.13295269012451 and batch: 250, loss is 4.06865424156189 and perplexity is 58.47821206685123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.674522018432617 and perplexity of 107.18132414911801
Finished 6 epochs...
Completing Train Step...
At time: 50.43132972717285 and batch: 50, loss is 4.007010655403137 and perplexity is 54.98226371978012
At time: 51.98687982559204 and batch: 100, loss is 3.8988503551483156 and perplexity is 49.34568646894371
At time: 53.52921462059021 and batch: 150, loss is 3.9760283613204956 and perplexity is 53.304905432645924
At time: 55.070693254470825 and batch: 200, loss is 3.961005825996399 and perplexity is 52.51011543530848
At time: 56.611515045166016 and batch: 250, loss is 3.991105751991272 and perplexity is 54.11469372317957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.673218154907227 and perplexity of 107.04166539771865
Finished 7 epochs...
Completing Train Step...
At time: 58.924001932144165 and batch: 50, loss is 3.931076054573059 and perplexity is 50.961785838325
At time: 60.4810848236084 and batch: 100, loss is 3.8268854808807373 and perplexity is 45.919298755038476
At time: 62.02631592750549 and batch: 150, loss is 3.9050728416442873 and perplexity is 49.653696637359445
At time: 63.571410179138184 and batch: 200, loss is 3.8921765422821046 and perplexity is 49.01745907406447
At time: 65.12108325958252 and batch: 250, loss is 3.9226367568969724 and perplexity is 50.53351385675329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.674360275268555 and perplexity of 107.16398970452352
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 67.42024159431458 and batch: 50, loss is 3.866889142990112 and perplexity is 47.79347591221888
At time: 68.96044707298279 and batch: 100, loss is 3.737321825027466 and perplexity is 41.98539522312077
At time: 70.50338315963745 and batch: 150, loss is 3.7762159538269042 and perplexity is 43.65055311628185
At time: 72.04676532745361 and batch: 200, loss is 3.710878391265869 and perplexity is 40.88970791924741
At time: 73.59088110923767 and batch: 250, loss is 3.6823779153823852 and perplexity is 39.7407820216957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6168212890625 and perplexity of 101.17192415481594
Finished 9 epochs...
Completing Train Step...
At time: 75.88645625114441 and batch: 50, loss is 3.7943914794921874 and perplexity is 44.45117871993787
At time: 77.41854071617126 and batch: 100, loss is 3.6740443086624146 and perplexity is 39.41097412696026
At time: 78.96350288391113 and batch: 150, loss is 3.727409987449646 and perplexity is 41.57129842535212
At time: 80.50471115112305 and batch: 200, loss is 3.680196795463562 and perplexity is 39.65419707084482
At time: 82.04650902748108 and batch: 250, loss is 3.677379083633423 and perplexity is 39.54262024014356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.616583633422851 and perplexity of 101.14788293334553
Finished 10 epochs...
Completing Train Step...
At time: 84.33783292770386 and batch: 50, loss is 3.760868902206421 and perplexity is 42.98576017839358
At time: 85.86500120162964 and batch: 100, loss is 3.6428905582427977 and perplexity is 38.20210268634441
At time: 87.37981414794922 and batch: 150, loss is 3.701487774848938 and perplexity is 40.507525628938055
At time: 88.92437434196472 and batch: 200, loss is 3.661847176551819 and perplexity is 38.93319297213756
At time: 90.47364401817322 and batch: 250, loss is 3.6697361040115357 and perplexity is 39.241548806460976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6190025329589846 and perplexity of 101.39284565111478
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 92.76921796798706 and batch: 50, loss is 3.743160185813904 and perplexity is 42.23123806948589
At time: 94.30715227127075 and batch: 100, loss is 3.6213774156570433 and perplexity is 37.38903260863218
At time: 95.86019086837769 and batch: 150, loss is 3.6696328830718996 and perplexity is 39.23749846596397
At time: 97.40488147735596 and batch: 200, loss is 3.613924674987793 and perplexity is 37.11141762593272
At time: 98.95276546478271 and batch: 250, loss is 3.6057858848571778 and perplexity is 36.810601387045146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613874435424805 and perplexity of 100.87422415668311
Finished 12 epochs...
Completing Train Step...
At time: 101.23979878425598 and batch: 50, loss is 3.7284493160247805 and perplexity is 41.614527124230726
At time: 102.7860996723175 and batch: 100, loss is 3.6073720121383666 and perplexity is 36.86903401468411
At time: 104.31891012191772 and batch: 150, loss is 3.6595109272003175 and perplexity is 38.84234149248248
At time: 105.85049819946289 and batch: 200, loss is 3.6095569276809694 and perplexity is 36.949677808000445
At time: 107.39343357086182 and batch: 250, loss is 3.6086867475509643 and perplexity is 36.91753891790162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6138763427734375 and perplexity of 100.87441655918013
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 109.6848156452179 and batch: 50, loss is 3.7219818449020385 and perplexity is 41.346254828542136
At time: 111.22499227523804 and batch: 100, loss is 3.6001344823837282 and perplexity is 36.60315659244995
At time: 112.7524778842926 and batch: 150, loss is 3.651706509590149 and perplexity is 38.540379489983756
At time: 114.29353094100952 and batch: 200, loss is 3.5984039640426637 and perplexity is 36.53986893464609
At time: 115.83257055282593 and batch: 250, loss is 3.593668541908264 and perplexity is 36.36724627366705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6134288787841795 and perplexity of 100.82928898756707
Finished 14 epochs...
Completing Train Step...
At time: 118.12991523742676 and batch: 50, loss is 3.719137263298035 and perplexity is 41.228809153759435
At time: 119.67232871055603 and batch: 100, loss is 3.5975408124923707 and perplexity is 36.50834309787239
At time: 121.21258282661438 and batch: 150, loss is 3.6497573328018187 and perplexity is 38.465330642365345
At time: 122.75446605682373 and batch: 200, loss is 3.5976356887817382 and perplexity is 36.51180703831678
At time: 124.30156922340393 and batch: 250, loss is 3.5947001647949217 and perplexity is 36.404782915748186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613304519653321 and perplexity of 100.81675072446336
Finished 15 epochs...
Completing Train Step...
At time: 126.5627908706665 and batch: 50, loss is 3.717018179893494 and perplexity is 41.14153437242215
At time: 128.07886934280396 and batch: 100, loss is 3.5955251693725585 and perplexity is 36.43482942084946
At time: 129.5898299217224 and batch: 150, loss is 3.648288288116455 and perplexity is 38.40886483836393
At time: 131.1047809123993 and batch: 200, loss is 3.597089023590088 and perplexity is 36.49185275897659
At time: 132.61535787582397 and batch: 250, loss is 3.595437707901001 and perplexity is 36.43164291640257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613283920288086 and perplexity of 100.81467398478323
Finished 16 epochs...
Completing Train Step...
At time: 134.87442803382874 and batch: 50, loss is 3.7151648950576783 and perplexity is 41.06535800070819
At time: 136.38156604766846 and batch: 100, loss is 3.5937648010253906 and perplexity is 36.37074712117722
At time: 137.89896416664124 and batch: 150, loss is 3.6470180702209474 and perplexity is 38.36010818324358
At time: 139.4249074459076 and batch: 200, loss is 3.5966015434265137 and perplexity is 36.47406803982561
At time: 140.95007157325745 and batch: 250, loss is 3.5959842729568483 and perplexity is 36.45156062201289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613322448730469 and perplexity of 100.8185582919689
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 143.2488715648651 and batch: 50, loss is 3.7137926816940308 and perplexity is 41.00904621240698
At time: 144.78148198127747 and batch: 100, loss is 3.592179927825928 and perplexity is 36.31314975313911
At time: 146.3219757080078 and batch: 150, loss is 3.6453043174743653 and perplexity is 38.29442474115075
At time: 147.86259365081787 and batch: 200, loss is 3.5942303419113157 and perplexity is 36.387683132910844
At time: 149.39940547943115 and batch: 250, loss is 3.5927544450759887 and perplexity is 36.33401827816613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613331985473633 and perplexity of 100.81951977725022
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 151.68077993392944 and batch: 50, loss is 3.7134691762924192 and perplexity is 40.99578171012708
At time: 153.20481181144714 and batch: 100, loss is 3.591840376853943 and perplexity is 36.30082168096749
At time: 154.7181351184845 and batch: 150, loss is 3.6449483156204225 and perplexity is 38.28079428132567
At time: 156.23177790641785 and batch: 200, loss is 3.5937715864181516 and perplexity is 36.37099391181873
At time: 157.7452027797699 and batch: 250, loss is 3.592151155471802 and perplexity is 36.312104953365726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613333892822266 and perplexity of 100.81971207540681
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 160.00546741485596 and batch: 50, loss is 3.7134076881408693 and perplexity is 40.99326103278507
At time: 161.52031922340393 and batch: 100, loss is 3.591775975227356 and perplexity is 36.29848392428326
At time: 163.02988839149475 and batch: 150, loss is 3.644880108833313 and perplexity is 38.278183360382016
At time: 164.53013944625854 and batch: 200, loss is 3.593685369491577 and perplexity is 36.36785825168262
At time: 166.0332477092743 and batch: 250, loss is 3.592038145065308 and perplexity is 36.30800153949289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613333892822266 and perplexity of 100.81971207540681
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 168.26458477973938 and batch: 50, loss is 3.713396420478821 and perplexity is 40.992799137175744
At time: 169.78206610679626 and batch: 100, loss is 3.591764087677002 and perplexity is 36.298052426792566
At time: 171.31257939338684 and batch: 150, loss is 3.6448675346374513 and perplexity is 38.27770204603328
At time: 172.8606960773468 and batch: 200, loss is 3.5936692905426026 and perplexity is 36.3672734994466
At time: 174.40153408050537 and batch: 250, loss is 3.592016825675964 and perplexity is 36.30722748332301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613333892822266 and perplexity of 100.81971207540681
Annealing...
Model not improving. Stopping early with 100.81467398478323loss at 20 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -100.81467398478323
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 7.683260297614677, 'num_layers': 1, 'dropout': 0.8573763535128364, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 27.18775406341022, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5982987880706787 and batch: 50, loss is 6.8444936561584475 and perplexity is 938.6978567771578
At time: 3.0088765621185303 and batch: 100, loss is 6.370777397155762 and perplexity is 584.5120503186226
At time: 4.402758598327637 and batch: 150, loss is 6.264874038696289 and perplexity is 525.7753544390046
At time: 5.804845094680786 and batch: 200, loss is 6.219475469589233 and perplexity is 502.43961803027497
At time: 7.221508741378784 and batch: 250, loss is 6.182833471298218 and perplexity is 484.36244089577195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.136778259277344 and perplexity of 462.5609163365732
Finished 1 epochs...
Completing Train Step...
At time: 9.362757444381714 and batch: 50, loss is 5.688010330200195 and perplexity is 295.30547534103925
At time: 10.771799802780151 and batch: 100, loss is 5.418271179199219 and perplexity is 225.4889553383221
At time: 12.162355661392212 and batch: 150, loss is 5.315883445739746 and perplexity is 203.54425402556134
At time: 13.56941270828247 and batch: 200, loss is 5.257123498916626 and perplexity is 191.92861363681533
At time: 15.069198608398438 and batch: 250, loss is 5.230296611785889 and perplexity is 186.84821668490397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.530352020263672 and perplexity of 252.23268641400603
Finished 2 epochs...
Completing Train Step...
At time: 17.382333278656006 and batch: 50, loss is 5.210809297561646 and perplexity is 183.24229574461958
At time: 18.919092655181885 and batch: 100, loss is 5.099651432037353 and perplexity is 163.9647444809872
At time: 20.449915409088135 and batch: 150, loss is 5.095935745239258 and perplexity is 163.3566333195217
At time: 21.979935884475708 and batch: 200, loss is 5.1145031452178955 and perplexity is 166.4180748222402
At time: 23.518620014190674 and batch: 250, loss is 5.140302190780639 and perplexity is 170.76736484822408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.413289260864258 and perplexity of 224.36838139899172
Finished 3 epochs...
Completing Train Step...
At time: 25.796497344970703 and batch: 50, loss is 5.097263412475586 and perplexity is 163.57366060754725
At time: 27.357693910598755 and batch: 100, loss is 5.036109132766724 and perplexity is 153.87016043993825
At time: 28.890865087509155 and batch: 150, loss is 5.057119836807251 and perplexity is 157.13728292511217
At time: 30.422603607177734 and batch: 200, loss is 5.021895542144775 and perplexity is 151.69858249000953
At time: 31.958117246627808 and batch: 250, loss is 5.054604091644287 and perplexity is 156.74246240780937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.413570404052734 and perplexity of 224.4314699091684
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 34.3341121673584 and batch: 50, loss is 4.985898008346558 and perplexity is 146.33492602514698
At time: 35.84409236907959 and batch: 100, loss is 4.819755821228028 and perplexity is 123.93482483149127
At time: 37.36168551445007 and batch: 150, loss is 4.814608697891235 and perplexity is 123.29855588444845
At time: 38.87797665596008 and batch: 200, loss is 4.790363054275513 and perplexity is 120.34505251878417
At time: 40.40647554397583 and batch: 250, loss is 4.7941882610321045 and perplexity is 120.80627880736266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.190086746215821 and perplexity of 179.48412182491958
Finished 5 epochs...
Completing Train Step...
At time: 42.691967725753784 and batch: 50, loss is 4.818955726623535 and perplexity is 123.83570490478184
At time: 44.23806667327881 and batch: 100, loss is 4.718980083465576 and perplexity is 112.0539087328228
At time: 45.769253730773926 and batch: 150, loss is 4.738329391479493 and perplexity is 114.24318652799059
At time: 47.29970645904541 and batch: 200, loss is 4.72924880027771 and perplexity is 113.21048671208484
At time: 48.82797861099243 and batch: 250, loss is 4.751289529800415 and perplexity is 115.73343004967535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.161603927612305 and perplexity of 174.44402693068713
Finished 6 epochs...
Completing Train Step...
At time: 51.162333965301514 and batch: 50, loss is 4.753910131454468 and perplexity is 116.03711901799913
At time: 52.709861040115356 and batch: 100, loss is 4.667293090820312 and perplexity is 106.40931188777245
At time: 54.23542666435242 and batch: 150, loss is 4.693388423919678 and perplexity is 109.22264612636506
At time: 55.76370429992676 and batch: 200, loss is 4.685721921920776 and perplexity is 108.38849209890193
At time: 57.30694222450256 and batch: 250, loss is 4.710250463485718 and perplexity is 111.07997790031615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.148170089721679 and perplexity of 172.11624468399773
Finished 7 epochs...
Completing Train Step...
At time: 59.65379977226257 and batch: 50, loss is 4.7091483211517335 and perplexity is 110.95761939484453
At time: 61.16185522079468 and batch: 100, loss is 4.628671741485595 and perplexity is 102.37798932324593
At time: 62.64281392097473 and batch: 150, loss is 4.650505743026733 and perplexity is 104.63789208169102
At time: 64.13278722763062 and batch: 200, loss is 4.634403581619263 and perplexity is 102.96648857184742
At time: 65.62513446807861 and batch: 250, loss is 4.6647352027893065 and perplexity is 106.13747659284428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1404258728027346 and perplexity of 170.7884870074051
Finished 8 epochs...
Completing Train Step...
At time: 67.85926365852356 and batch: 50, loss is 4.671062793731689 and perplexity is 106.81120040482485
At time: 69.36803340911865 and batch: 100, loss is 4.596829128265381 and perplexity is 99.16936323815314
At time: 70.87941598892212 and batch: 150, loss is 4.620647468566895 and perplexity is 101.55976760360703
At time: 72.37942695617676 and batch: 200, loss is 4.605352945327759 and perplexity is 100.01827760411733
At time: 73.87849259376526 and batch: 250, loss is 4.6398116493225094 and perplexity is 103.5248467718166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.137574005126953 and perplexity of 170.30211470627987
Finished 9 epochs...
Completing Train Step...
At time: 76.12646341323853 and batch: 50, loss is 4.638456916809082 and perplexity is 103.38469325264714
At time: 77.61887788772583 and batch: 100, loss is 4.568485794067382 and perplexity is 96.39803273233869
At time: 79.15707325935364 and batch: 150, loss is 4.586956033706665 and perplexity is 98.19507228524269
At time: 80.68051147460938 and batch: 200, loss is 4.56860803604126 and perplexity is 96.4098173384099
At time: 82.19702291488647 and batch: 250, loss is 4.6064017868041995 and perplexity is 100.12323595478462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.135708236694336 and perplexity of 169.98466663104074
Finished 10 epochs...
Completing Train Step...
At time: 84.46559309959412 and batch: 50, loss is 4.613315162658691 and perplexity is 100.8178237233927
At time: 85.99085450172424 and batch: 100, loss is 4.5457774257659915 and perplexity is 94.23365841388043
At time: 87.51409077644348 and batch: 150, loss is 4.563154821395874 and perplexity is 95.88550480424963
At time: 89.02891397476196 and batch: 200, loss is 4.548254499435425 and perplexity is 94.46737147063816
At time: 90.54011082649231 and batch: 250, loss is 4.589917306900024 and perplexity is 98.48628568893639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.134765625 and perplexity of 169.82451258981084
Finished 11 epochs...
Completing Train Step...
At time: 92.77674841880798 and batch: 50, loss is 4.587975540161133 and perplexity is 98.29523384423679
At time: 94.27851223945618 and batch: 100, loss is 4.522009744644165 and perplexity is 92.02034966901218
At time: 95.77139401435852 and batch: 150, loss is 4.536945829391479 and perplexity is 93.40508895886668
At time: 97.28780174255371 and batch: 200, loss is 4.523611059188843 and perplexity is 92.16782123601381
At time: 98.8041512966156 and batch: 250, loss is 4.56139907836914 and perplexity is 95.71730220130725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.131519317626953 and perplexity of 169.2741039037283
Finished 12 epochs...
Completing Train Step...
At time: 101.06255435943604 and batch: 50, loss is 4.561431484222412 and perplexity is 95.72040405241677
At time: 102.562096118927 and batch: 100, loss is 4.5005707263946535 and perplexity is 90.06852111669333
At time: 104.06730556488037 and batch: 150, loss is 4.514177350997925 and perplexity is 91.30242527035787
At time: 105.57658743858337 and batch: 200, loss is 4.503515634536743 and perplexity is 90.33415558044759
At time: 107.09291911125183 and batch: 250, loss is 4.547631769180298 and perplexity is 94.40856209339086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.131726837158203 and perplexity of 169.30923523151534
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 109.33994174003601 and batch: 50, loss is 4.525121660232544 and perplexity is 92.3071552555305
At time: 110.84857869148254 and batch: 100, loss is 4.418786401748657 and perplexity is 82.995501019958
At time: 112.34441637992859 and batch: 150, loss is 4.3984564018249515 and perplexity is 81.3252382391337
At time: 113.84007120132446 and batch: 200, loss is 4.369636011123657 and perplexity is 79.01486593240732
At time: 115.33471012115479 and batch: 250, loss is 4.432862224578858 and perplexity is 84.17199159984364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0727073669433596 and perplexity of 159.60585454303043
Finished 14 epochs...
Completing Train Step...
At time: 117.56819820404053 and batch: 50, loss is 4.476655521392822 and perplexity is 87.94006662690076
At time: 119.07520341873169 and batch: 100, loss is 4.382164678573608 and perplexity is 80.01104427425794
At time: 120.57118344306946 and batch: 150, loss is 4.376042528152466 and perplexity is 79.52270100661646
At time: 122.07448697090149 and batch: 200, loss is 4.367962923049927 and perplexity is 78.88277763106126
At time: 123.5779197216034 and batch: 250, loss is 4.436810865402221 and perplexity is 84.50501362153359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.067734909057617 and perplexity of 158.81419104088764
Finished 15 epochs...
Completing Train Step...
At time: 125.83818054199219 and batch: 50, loss is 4.4600691223144535 and perplexity is 86.4934875197483
At time: 127.35300993919373 and batch: 100, loss is 4.370181722640991 and perplexity is 79.05799702228309
At time: 128.86367893218994 and batch: 150, loss is 4.373301725387574 and perplexity is 79.30504338238863
At time: 130.36806988716125 and batch: 200, loss is 4.37369083404541 and perplexity is 79.33590766576935
At time: 131.87782430648804 and batch: 250, loss is 4.433534860610962 and perplexity is 84.22862775991328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065927505493164 and perplexity of 158.52740894947112
Finished 16 epochs...
Completing Train Step...
At time: 134.10906100273132 and batch: 50, loss is 4.448379564285278 and perplexity is 85.48830340342282
At time: 135.61030435562134 and batch: 100, loss is 4.362061843872071 and perplexity is 78.41865487389485
At time: 137.09981203079224 and batch: 150, loss is 4.369635782241821 and perplexity is 79.01484784734178
At time: 138.60019278526306 and batch: 200, loss is 4.37222825050354 and perplexity is 79.21995708731158
At time: 140.11156034469604 and batch: 250, loss is 4.430445833206177 and perplexity is 83.96884466561703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065129089355469 and perplexity of 158.40088862255524
Finished 17 epochs...
Completing Train Step...
At time: 142.3514540195465 and batch: 50, loss is 4.43983907699585 and perplexity is 84.7613005331533
At time: 143.8473618030548 and batch: 100, loss is 4.355496873855591 and perplexity is 77.90552494017018
At time: 145.37247276306152 and batch: 150, loss is 4.3662184715271 and perplexity is 78.74529040426476
At time: 146.91281533241272 and batch: 200, loss is 4.370406694412232 and perplexity is 79.07578484070741
At time: 148.44021654129028 and batch: 250, loss is 4.427325477600098 and perplexity is 83.70724037190269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.064415740966797 and perplexity of 158.28793389671787
Finished 18 epochs...
Completing Train Step...
At time: 150.7791657447815 and batch: 50, loss is 4.4327570247650145 and perplexity is 84.16313718774593
At time: 152.294340133667 and batch: 100, loss is 4.3499769115447995 and perplexity is 77.47667408789144
At time: 153.8251748085022 and batch: 150, loss is 4.362489280700683 and perplexity is 78.45218105969103
At time: 155.33655548095703 and batch: 200, loss is 4.368116760253907 and perplexity is 78.89491367047744
At time: 156.83673739433289 and batch: 250, loss is 4.424320049285889 and perplexity is 83.45604193012059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.063900375366211 and perplexity of 158.20637875776146
Finished 19 epochs...
Completing Train Step...
At time: 159.07192015647888 and batch: 50, loss is 4.426285018920899 and perplexity is 83.62019174025099
At time: 160.57925510406494 and batch: 100, loss is 4.344901742935181 and perplexity is 77.08446301410696
At time: 162.0734260082245 and batch: 150, loss is 4.359092502593994 and perplexity is 78.18614849124009
At time: 163.56963062286377 and batch: 200, loss is 4.365704460144043 and perplexity is 78.70482482940884
At time: 165.06481409072876 and batch: 250, loss is 4.421540441513062 and perplexity is 83.22438896861856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0638275146484375 and perplexity of 158.19485214737242
Finished 20 epochs...
Completing Train Step...
At time: 167.30188941955566 and batch: 50, loss is 4.420609321594238 and perplexity is 83.14693314824261
At time: 168.77335357666016 and batch: 100, loss is 4.340422058105469 and perplexity is 76.73992121005895
At time: 170.24828910827637 and batch: 150, loss is 4.355917043685913 and perplexity is 77.93826536915404
At time: 171.74412083625793 and batch: 200, loss is 4.363226299285889 and perplexity is 78.51002308788851
At time: 173.24803709983826 and batch: 250, loss is 4.418495140075684 and perplexity is 82.97133113153356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0635231018066404 and perplexity of 158.14670293186416
Finished 21 epochs...
Completing Train Step...
At time: 175.48251104354858 and batch: 50, loss is 4.4153744506835935 and perplexity is 82.71280697522734
At time: 176.98971557617188 and batch: 100, loss is 4.335926027297973 and perplexity is 76.39567062065977
At time: 178.4889440536499 and batch: 150, loss is 4.352672805786133 and perplexity is 77.68582480483643
At time: 179.98313188552856 and batch: 200, loss is 4.360629463195801 and perplexity is 78.30640991594004
At time: 181.48309803009033 and batch: 250, loss is 4.4153258419036865 and perplexity is 82.70878650431347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.062690734863281 and perplexity of 158.01512161370925
Finished 22 epochs...
Completing Train Step...
At time: 183.7763545513153 and batch: 50, loss is 4.4100970363616945 and perplexity is 82.27744702095812
At time: 185.29280853271484 and batch: 100, loss is 4.3315410232543945 and perplexity is 76.06140870161568
At time: 186.80299043655396 and batch: 150, loss is 4.3493624019622805 and perplexity is 77.4290785546961
At time: 188.3255331516266 and batch: 200, loss is 4.357646036148071 and perplexity is 78.07313660456681
At time: 189.84264063835144 and batch: 250, loss is 4.412173833847046 and perplexity is 82.44849817385459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.062503814697266 and perplexity of 157.98558816122915
Finished 23 epochs...
Completing Train Step...
At time: 192.19985437393188 and batch: 50, loss is 4.4052938747406 and perplexity is 81.88320271265721
At time: 193.692852973938 and batch: 100, loss is 4.327199325561524 and perplexity is 75.73188891421952
At time: 195.1851236820221 and batch: 150, loss is 4.346225738525391 and perplexity is 77.18659009619572
At time: 196.6721317768097 and batch: 200, loss is 4.354822549819946 and perplexity is 77.85300908053668
At time: 198.16252613067627 and batch: 250, loss is 4.409288806915283 and perplexity is 82.21097483150918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.062091445922851 and perplexity of 157.92045326858488
Finished 24 epochs...
Completing Train Step...
At time: 200.38207840919495 and batch: 50, loss is 4.400810832977295 and perplexity is 81.5169384974907
At time: 201.8754427433014 and batch: 100, loss is 4.323370494842529 and perplexity is 75.44247873638827
At time: 203.35969758033752 and batch: 150, loss is 4.343208456039429 and perplexity is 76.9540473497477
At time: 204.84616827964783 and batch: 200, loss is 4.351943120956421 and perplexity is 77.62915931348165
At time: 206.3362171649933 and batch: 250, loss is 4.406271858215332 and perplexity is 81.96332230320019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.062236404418945 and perplexity of 157.94334683926184
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 208.58703327178955 and batch: 50, loss is 4.393152933120728 and perplexity is 80.89507407262691
At time: 210.08110666275024 and batch: 100, loss is 4.309912014007568 and perplexity is 74.43393950818229
At time: 211.5693395137787 and batch: 150, loss is 4.321657276153564 and perplexity is 75.31333992497429
At time: 213.05633330345154 and batch: 200, loss is 4.327575044631958 and perplexity is 75.76034817513334
At time: 214.54380416870117 and batch: 250, loss is 4.388613233566284 and perplexity is 80.52866705905022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.054233551025391 and perplexity of 156.68439371784999
Finished 26 epochs...
Completing Train Step...
At time: 216.77915334701538 and batch: 50, loss is 4.387499322891236 and perplexity is 80.43901525848455
At time: 218.263028383255 and batch: 100, loss is 4.306292476654053 and perplexity is 74.16501007742797
At time: 219.76042675971985 and batch: 150, loss is 4.320374488830566 and perplexity is 75.21679086645972
At time: 221.26061964035034 and batch: 200, loss is 4.327522325515747 and perplexity is 75.75635426181242
At time: 222.76370286941528 and batch: 250, loss is 4.387422571182251 and perplexity is 80.43284166351442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.052752685546875 and perplexity of 156.45253692490252
Finished 27 epochs...
Completing Train Step...
At time: 225.01618766784668 and batch: 50, loss is 4.385062103271484 and perplexity is 80.24320642379591
At time: 226.54994368553162 and batch: 100, loss is 4.305401821136474 and perplexity is 74.0989840095967
At time: 228.06491136550903 and batch: 150, loss is 4.321002855300903 and perplexity is 75.26406942842243
At time: 229.59185481071472 and batch: 200, loss is 4.327912092208862 and perplexity is 75.78588732062298
At time: 231.10389876365662 and batch: 250, loss is 4.385839786529541 and perplexity is 80.30563449349047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.052400207519531 and perplexity of 156.39740056106345
Finished 28 epochs...
Completing Train Step...
At time: 233.36828565597534 and batch: 50, loss is 4.383404750823974 and perplexity is 80.110325295082
At time: 234.87958979606628 and batch: 100, loss is 4.305074949264526 and perplexity is 74.0747670941139
At time: 236.39074325561523 and batch: 150, loss is 4.3217589378356935 and perplexity is 75.3209967949964
At time: 237.90107345581055 and batch: 200, loss is 4.328083906173706 and perplexity is 75.79890951306797
At time: 239.41877102851868 and batch: 250, loss is 4.38424352645874 and perplexity is 80.17754807250543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.052054214477539 and perplexity of 156.3432975088632
Finished 29 epochs...
Completing Train Step...
At time: 241.640065908432 and batch: 50, loss is 4.382004137039185 and perplexity is 79.99820020946984
At time: 243.12704849243164 and batch: 100, loss is 4.304915266036987 and perplexity is 74.06293954058124
At time: 244.6019561290741 and batch: 150, loss is 4.322437591552735 and perplexity is 75.37213101869548
At time: 246.07711791992188 and batch: 200, loss is 4.328106241226196 and perplexity is 75.8006025045971
At time: 247.5486035346985 and batch: 250, loss is 4.382658109664917 and perplexity is 80.05053395306739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051694107055664 and perplexity of 156.28700726294588
Finished 30 epochs...
Completing Train Step...
At time: 249.75000405311584 and batch: 50, loss is 4.380770092010498 and perplexity is 79.89953971646348
At time: 251.23504066467285 and batch: 100, loss is 4.30481598854065 and perplexity is 74.05558712234313
At time: 252.71747469902039 and batch: 150, loss is 4.323064985275269 and perplexity is 75.41943385775116
At time: 254.20074105262756 and batch: 200, loss is 4.328077363967895 and perplexity is 75.79841362262381
At time: 255.70947170257568 and batch: 250, loss is 4.381106872558593 and perplexity is 79.92645285889881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051852035522461 and perplexity of 156.31169137949416
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 257.94856119155884 and batch: 50, loss is 4.379427900314331 and perplexity is 79.79237115419419
At time: 259.436115026474 and batch: 100, loss is 4.302637081146241 and perplexity is 73.89440252289324
At time: 260.92893743515015 and batch: 150, loss is 4.320519304275512 and perplexity is 75.22768420823948
At time: 262.41844177246094 and batch: 200, loss is 4.324479818344116 and perplexity is 75.52621528794363
At time: 263.9091203212738 and batch: 250, loss is 4.3780363655090335 and perplexity is 79.68141451045612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051292800903321 and perplexity of 156.22430090846436
Finished 32 epochs...
Completing Train Step...
At time: 266.1575906276703 and batch: 50, loss is 4.379015426635743 and perplexity is 79.75946568812674
At time: 267.64407777786255 and batch: 100, loss is 4.302514219284058 and perplexity is 73.88532427669094
At time: 269.1205105781555 and batch: 150, loss is 4.320359926223755 and perplexity is 75.21569552188429
At time: 270.6062412261963 and batch: 200, loss is 4.324378519058228 and perplexity is 75.51856492376386
At time: 272.10073828697205 and batch: 250, loss is 4.378020162582398 and perplexity is 79.68012344880208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051105499267578 and perplexity of 156.19504258151326
Finished 33 epochs...
Completing Train Step...
At time: 274.3224937915802 and batch: 50, loss is 4.378890447616577 and perplexity is 79.74949805122158
At time: 275.80264592170715 and batch: 100, loss is 4.302551507949829 and perplexity is 73.88807941322064
At time: 277.2831428050995 and batch: 150, loss is 4.320210676193238 and perplexity is 75.20447041472687
At time: 278.7588620185852 and batch: 200, loss is 4.324299974441528 and perplexity is 75.51263357996883
At time: 280.26392364501953 and batch: 250, loss is 4.377916879653931 and perplexity is 79.67189427728549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051044082641601 and perplexity of 156.18544990358157
Finished 34 epochs...
Completing Train Step...
At time: 282.50488543510437 and batch: 50, loss is 4.3787728214263915 and perplexity is 79.74011797327884
At time: 284.0008063316345 and batch: 100, loss is 4.3026018238067625 and perplexity is 73.89179724878574
At time: 285.5293319225311 and batch: 150, loss is 4.320083503723144 and perplexity is 75.19490708457123
At time: 287.0622968673706 and batch: 200, loss is 4.324216136932373 and perplexity is 75.5063030542312
At time: 288.59614157676697 and batch: 250, loss is 4.377795534133911 and perplexity is 79.66222703639349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.051012420654297 and perplexity of 156.18050484013534
Finished 35 epochs...
Completing Train Step...
At time: 290.8470821380615 and batch: 50, loss is 4.378667087554931 and perplexity is 79.73168718761251
At time: 292.37896037101746 and batch: 100, loss is 4.302651405334473 and perplexity is 73.89546100780521
At time: 293.88646030426025 and batch: 150, loss is 4.319969253540039 and perplexity is 75.18631654341301
At time: 295.38998317718506 and batch: 200, loss is 4.324129457473755 and perplexity is 75.49975849240386
At time: 296.89097905158997 and batch: 250, loss is 4.3776665306091305 and perplexity is 79.6519509911512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050991058349609 and perplexity of 156.17716850024073
Finished 36 epochs...
Completing Train Step...
At time: 299.1180958747864 and batch: 50, loss is 4.378569383621215 and perplexity is 79.72389746868174
At time: 300.59879779815674 and batch: 100, loss is 4.3027004051208495 and perplexity is 73.89908195832099
At time: 302.0792281627655 and batch: 150, loss is 4.319862880706787 and perplexity is 75.17831918725878
At time: 303.5579719543457 and batch: 200, loss is 4.3240421104431155 and perplexity is 75.49316410069
At time: 305.04911947250366 and batch: 250, loss is 4.377533645629883 and perplexity is 79.64136714652925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050978469848633 and perplexity of 156.17520247617716
Finished 37 epochs...
Completing Train Step...
At time: 307.2446873188019 and batch: 50, loss is 4.3784763145446775 and perplexity is 79.71647798443396
At time: 308.727961063385 and batch: 100, loss is 4.302749414443969 and perplexity is 73.902703791058
At time: 310.20222902297974 and batch: 150, loss is 4.319762544631958 and perplexity is 75.17077646820901
At time: 311.6862692832947 and batch: 200, loss is 4.323954210281372 and perplexity is 75.48652853099316
At time: 313.1671402454376 and batch: 250, loss is 4.377399444580078 and perplexity is 79.63067990858563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050970840454101 and perplexity of 156.17401095848678
Finished 38 epochs...
Completing Train Step...
At time: 315.35593008995056 and batch: 50, loss is 4.378387441635132 and perplexity is 79.70939366390353
At time: 316.8349714279175 and batch: 100, loss is 4.302798185348511 and perplexity is 73.90630818066393
At time: 318.3152115345001 and batch: 150, loss is 4.319665803909301 and perplexity is 75.16350474471238
At time: 319.8035771846771 and batch: 200, loss is 4.323866519927979 and perplexity is 75.47990938085185
At time: 321.29715490341187 and batch: 250, loss is 4.377263784408569 and perplexity is 79.6198779296076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.05096664428711 and perplexity of 156.17335562763196
Finished 39 epochs...
Completing Train Step...
At time: 323.5319502353668 and batch: 50, loss is 4.378301973342896 and perplexity is 79.70258132927532
At time: 325.01376938819885 and batch: 100, loss is 4.302846698760987 and perplexity is 73.90989371484979
At time: 326.4949131011963 and batch: 150, loss is 4.3195719718933105 and perplexity is 75.15645233240946
At time: 327.98013257980347 and batch: 200, loss is 4.323778219223023 and perplexity is 75.47324474589382
At time: 329.4623181819916 and batch: 250, loss is 4.377127513885498 and perplexity is 79.60902882641855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.05096321105957 and perplexity of 156.1728194498869
Finished 40 epochs...
Completing Train Step...
At time: 331.6772429943085 and batch: 50, loss is 4.378219051361084 and perplexity is 79.69597250728808
At time: 333.18587613105774 and batch: 100, loss is 4.302894744873047 and perplexity is 73.91344488319483
At time: 334.68698358535767 and batch: 150, loss is 4.319481363296509 and perplexity is 75.14964282022787
At time: 336.196391582489 and batch: 200, loss is 4.3236883354187015 and perplexity is 75.46646122840033
At time: 337.71279072761536 and batch: 250, loss is 4.376990947723389 and perplexity is 79.59815766921547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050955200195313 and perplexity of 156.1715683756407
Finished 41 epochs...
Completing Train Step...
At time: 339.9748885631561 and batch: 50, loss is 4.3781382179260255 and perplexity is 79.68953066843149
At time: 341.49748969078064 and batch: 100, loss is 4.302940416336059 and perplexity is 73.91682069544748
At time: 343.0267357826233 and batch: 150, loss is 4.319393787384033 and perplexity is 75.14306180985886
At time: 344.53436732292175 and batch: 200, loss is 4.3235953903198245 and perplexity is 75.45944731665898
At time: 346.04085183143616 and batch: 250, loss is 4.376853866577148 and perplexity is 79.58724701036344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050936126708985 and perplexity of 156.16858966777372
Finished 42 epochs...
Completing Train Step...
At time: 348.2867965698242 and batch: 50, loss is 4.378058633804321 and perplexity is 79.68318889947962
At time: 349.7793595790863 and batch: 100, loss is 4.302984027862549 and perplexity is 73.92004439112591
At time: 351.264949798584 and batch: 150, loss is 4.319308662414551 and perplexity is 75.13666553126087
At time: 352.7548108100891 and batch: 200, loss is 4.323498973846435 and perplexity is 75.45217213359419
At time: 354.243882894516 and batch: 250, loss is 4.376716260910034 and perplexity is 79.5762961076152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.05090103149414 and perplexity of 156.16310899374065
Finished 43 epochs...
Completing Train Step...
At time: 356.46569561958313 and batch: 50, loss is 4.377980937957764 and perplexity is 79.676998087165
At time: 357.98379778862 and batch: 100, loss is 4.303026885986328 and perplexity is 73.92321253342799
At time: 359.4798758029938 and batch: 150, loss is 4.319226541519165 and perplexity is 75.13049549435907
At time: 360.95232129096985 and batch: 200, loss is 4.323399085998535 and perplexity is 75.44463575490293
At time: 362.42256784439087 and batch: 250, loss is 4.376578521728516 and perplexity is 79.56533608855045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050857162475586 and perplexity of 156.15625842167967
Finished 44 epochs...
Completing Train Step...
At time: 364.64796805381775 and batch: 50, loss is 4.377907457351685 and perplexity is 79.67114358815371
At time: 366.12498474121094 and batch: 100, loss is 4.303068418502807 and perplexity is 73.92628281422854
At time: 367.615763425827 and batch: 150, loss is 4.319147386550903 and perplexity is 75.1245487777319
At time: 369.1093280315399 and batch: 200, loss is 4.323300189971924 and perplexity is 75.43717494912579
At time: 370.596586227417 and batch: 250, loss is 4.376441440582275 and perplexity is 79.55442992860995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0508373260498045 and perplexity of 156.15316087037152
Finished 45 epochs...
Completing Train Step...
At time: 372.80977272987366 and batch: 50, loss is 4.37783712387085 and perplexity is 79.66554023635697
At time: 374.29676628112793 and batch: 100, loss is 4.303107213973999 and perplexity is 73.92915087483732
At time: 375.76360058784485 and batch: 150, loss is 4.31907021522522 and perplexity is 75.11875154040447
At time: 377.24412083625793 and batch: 200, loss is 4.323204622268677 and perplexity is 75.42996593605636
At time: 378.72396087646484 and batch: 250, loss is 4.376305799484253 and perplexity is 79.54363981019023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0508369445800785 and perplexity of 156.15310130267937
Finished 46 epochs...
Completing Train Step...
At time: 380.9229338169098 and batch: 50, loss is 4.377768611907959 and perplexity is 79.66008238078692
At time: 382.41075563430786 and batch: 100, loss is 4.303143672943115 and perplexity is 73.9318463046018
At time: 383.8895082473755 and batch: 150, loss is 4.318994340896606 and perplexity is 75.11305217178567
At time: 385.39935398101807 and batch: 200, loss is 4.323111724853516 and perplexity is 75.42295901266277
At time: 386.91315937042236 and batch: 250, loss is 4.376169204711914 and perplexity is 79.53277530685338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050838851928711 and perplexity of 156.15339914136771
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 389.18387055397034 and batch: 50, loss is 4.377589101791382 and perplexity is 79.64578387351398
At time: 390.6955099105835 and batch: 100, loss is 4.30301796913147 and perplexity is 73.92255337381
At time: 392.2030351161957 and batch: 150, loss is 4.318435449600219 and perplexity is 75.07108386963364
At time: 393.7099406719208 and batch: 200, loss is 4.322491960525513 and perplexity is 75.37622903543654
At time: 395.2231864929199 and batch: 250, loss is 4.375853443145752 and perplexity is 79.50766587766627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050785827636719 and perplexity of 156.14511943745063
Finished 48 epochs...
Completing Train Step...
At time: 397.468603849411 and batch: 50, loss is 4.377556657791137 and perplexity is 79.64319988760016
At time: 398.980580329895 and batch: 100, loss is 4.302912244796753 and perplexity is 73.91473837415907
At time: 400.4693593978882 and batch: 150, loss is 4.318386602401733 and perplexity is 75.06741694705947
At time: 401.9520652294159 and batch: 200, loss is 4.322485990524292 and perplexity is 75.37577904060043
At time: 403.43689036369324 and batch: 250, loss is 4.3758638477325436 and perplexity is 79.50849312638005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050756454467773 and perplexity of 156.14053302783645
Finished 49 epochs...
Completing Train Step...
At time: 405.7186212539673 and batch: 50, loss is 4.377556285858154 and perplexity is 79.64317026567275
At time: 407.1595821380615 and batch: 100, loss is 4.302897176742554 and perplexity is 73.91362463126613
At time: 408.61903381347656 and batch: 150, loss is 4.318350658416748 and perplexity is 75.06471877344372
At time: 410.0788094997406 and batch: 200, loss is 4.32246542930603 and perplexity is 75.37422923868886
At time: 411.5479578971863 and batch: 250, loss is 4.375857248306274 and perplexity is 79.50796841767325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.050740051269531 and perplexity of 156.1379718447254
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 2.778489473231498, 'num_layers': 1, 'dropout': 0.24759354303778403, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 4.054933983154826, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6361172199249268 and batch: 50, loss is 6.582238550186157 and perplexity is 722.154099396476
At time: 3.0584754943847656 and batch: 100, loss is 5.684483385086059 and perplexity is 294.2657836830706
At time: 4.468862771987915 and batch: 150, loss is 5.397776327133179 and perplexity is 220.91462775489552
At time: 5.872359991073608 and batch: 200, loss is 5.2256921195983885 and perplexity is 185.9898532114219
At time: 7.287976264953613 and batch: 250, loss is 5.187512149810791 and perplexity is 179.02261699936716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.235552215576172 and perplexity of 187.83280190934113
Finished 1 epochs...
Completing Train Step...
At time: 9.404663324356079 and batch: 50, loss is 4.936497735977173 and perplexity is 139.28159344435596
At time: 10.789170742034912 and batch: 100, loss is 4.773425006866455 and perplexity is 118.32380860034613
At time: 12.191650152206421 and batch: 150, loss is 4.741977834701538 and perplexity is 114.66075758649723
At time: 13.58154010772705 and batch: 200, loss is 4.67254189491272 and perplexity is 106.96930187270108
At time: 15.010946989059448 and batch: 250, loss is 4.6789438343048095 and perplexity is 107.65630960462892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.915451431274414 and perplexity of 136.38086249465584
Finished 2 epochs...
Completing Train Step...
At time: 17.223559856414795 and batch: 50, loss is 4.566297292709351 and perplexity is 96.18729618950834
At time: 18.700778245925903 and batch: 100, loss is 4.442221975326538 and perplexity is 84.96351893195671
At time: 20.209315299987793 and batch: 150, loss is 4.478421487808228 and perplexity is 88.09550303847784
At time: 21.731695413589478 and batch: 200, loss is 4.435768699645996 and perplexity is 84.41699126501754
At time: 23.27464985847473 and batch: 250, loss is 4.45954047203064 and perplexity is 86.44777479710947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.790127944946289 and perplexity of 120.31676160007085
Finished 3 epochs...
Completing Train Step...
At time: 25.552887678146362 and batch: 50, loss is 4.372511720657348 and perplexity is 79.24241676390537
At time: 27.09088969230652 and batch: 100, loss is 4.255042057037354 and perplexity is 70.45978045573756
At time: 28.60659122467041 and batch: 150, loss is 4.316149711608887 and perplexity is 74.8996869999739
At time: 30.119412183761597 and batch: 200, loss is 4.284083843231201 and perplexity is 72.53606186507018
At time: 31.629850149154663 and batch: 250, loss is 4.3129329490661625 and perplexity is 74.65913959174011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.730716705322266 and perplexity of 113.37679098620562
Finished 4 epochs...
Completing Train Step...
At time: 33.86863470077515 and batch: 50, loss is 4.236451978683472 and perplexity is 69.1620276535004
At time: 35.34877848625183 and batch: 100, loss is 4.124971761703491 and perplexity is 61.866062233492414
At time: 36.828888177871704 and batch: 150, loss is 4.19658953666687 and perplexity is 66.45928713661165
At time: 38.31576132774353 and batch: 200, loss is 4.172791576385498 and perplexity is 64.8963626305703
At time: 39.80448889732361 and batch: 250, loss is 4.202358179092407 and perplexity is 66.84377492000489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.700896072387695 and perplexity of 110.0457372314564
Finished 5 epochs...
Completing Train Step...
At time: 42.03732633590698 and batch: 50, loss is 4.131765723228455 and perplexity is 62.28780892315485
At time: 43.51184296607971 and batch: 100, loss is 4.023959188461304 and perplexity is 55.92207414122436
At time: 44.987404108047485 and batch: 150, loss is 4.0998752355575565 and perplexity is 60.332759744636945
At time: 46.46945762634277 and batch: 200, loss is 4.082882356643677 and perplexity is 59.31619411375778
At time: 47.97430753707886 and batch: 250, loss is 4.113222332000732 and perplexity is 61.14342488591851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.684770965576172 and perplexity of 108.28546836795758
Finished 6 epochs...
Completing Train Step...
At time: 50.20220112800598 and batch: 50, loss is 4.044977464675903 and perplexity is 57.10989902955968
At time: 51.7023503780365 and batch: 100, loss is 3.940567202568054 and perplexity is 51.447774335958506
At time: 53.189716815948486 and batch: 150, loss is 4.0195785331726075 and perplexity is 55.67763460495868
At time: 54.690776109695435 and batch: 200, loss is 4.00552490234375 and perplexity is 54.9006343088361
At time: 56.19256854057312 and batch: 250, loss is 4.038094491958618 and perplexity is 56.71816285390777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6783489227294925 and perplexity of 107.59228266696076
Finished 7 epochs...
Completing Train Step...
At time: 58.518680810928345 and batch: 50, loss is 3.9720097970962525 and perplexity is 53.091126077422985
At time: 60.02473521232605 and batch: 100, loss is 3.868623147010803 and perplexity is 47.87642188515545
At time: 61.520673990249634 and batch: 150, loss is 3.9490102624893186 and perplexity is 51.8839898831861
At time: 63.02034068107605 and batch: 200, loss is 3.9378743267059324 and perplexity is 51.30941823779624
At time: 64.52305030822754 and batch: 250, loss is 3.9705406045913696 and perplexity is 53.01318226416768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.677502822875977 and perplexity of 107.50128735334609
Finished 8 epochs...
Completing Train Step...
At time: 66.74787640571594 and batch: 50, loss is 3.907500991821289 and perplexity is 49.77440976513001
At time: 68.2486822605133 and batch: 100, loss is 3.80713321685791 and perplexity is 45.02118770536375
At time: 69.73472762107849 and batch: 150, loss is 3.8873851680755616 and perplexity is 48.78315984087696
At time: 71.2204225063324 and batch: 200, loss is 3.878317074775696 and perplexity is 48.342789274316445
At time: 72.70828127861023 and batch: 250, loss is 3.9115662002563476 and perplexity is 49.977164957389945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.677157974243164 and perplexity of 107.46422207269964
Finished 9 epochs...
Completing Train Step...
At time: 74.93194222450256 and batch: 50, loss is 3.851035056114197 and perplexity is 47.04172887049073
At time: 76.40995740890503 and batch: 100, loss is 3.7516391944885252 and perplexity is 42.59083948074272
At time: 77.89015293121338 and batch: 150, loss is 3.8318667125701906 and perplexity is 46.148604058436945
At time: 79.3771505355835 and batch: 200, loss is 3.8240953350067137 and perplexity is 45.79135578585491
At time: 80.86609768867493 and batch: 250, loss is 3.856572165489197 and perplexity is 47.30292654112055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.683020782470703 and perplexity of 108.09611472076406
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 83.0888946056366 and batch: 50, loss is 3.793381004333496 and perplexity is 44.40628459408626
At time: 84.57359194755554 and batch: 100, loss is 3.6701555490493774 and perplexity is 39.25801193183182
At time: 86.07835602760315 and batch: 150, loss is 3.724889221191406 and perplexity is 41.4666388655333
At time: 87.58559656143188 and batch: 200, loss is 3.6854622602462768 and perplexity is 39.86354552377728
At time: 89.10033679008484 and batch: 250, loss is 3.6890167951583863 and perplexity is 40.00549401904652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.646286392211914 and perplexity of 104.19731822792458
Finished 11 epochs...
Completing Train Step...
At time: 91.34266757965088 and batch: 50, loss is 3.730801892280579 and perplexity is 41.71254372319416
At time: 92.86103010177612 and batch: 100, loss is 3.6150561237335204 and perplexity is 37.153431056397295
At time: 94.37085032463074 and batch: 150, loss is 3.683759164810181 and perplexity is 39.79571188130873
At time: 95.87820816040039 and batch: 200, loss is 3.657736773490906 and perplexity is 38.77349030259812
At time: 97.38610291481018 and batch: 250, loss is 3.676538558006287 and perplexity is 39.509397618655925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.651342391967773 and perplexity of 104.72547389580104
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 99.72415709495544 and batch: 50, loss is 3.704067668914795 and perplexity is 40.61216567600988
At time: 101.22084498405457 and batch: 100, loss is 3.583104033470154 and perplexity is 35.98506651870773
At time: 102.70671963691711 and batch: 150, loss is 3.640575466156006 and perplexity is 38.11376359673566
At time: 104.19364905357361 and batch: 200, loss is 3.5997008419036867 and perplexity is 36.587287423059955
At time: 105.67997908592224 and batch: 250, loss is 3.607668080329895 and perplexity is 36.879951378970645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.642907333374024 and perplexity of 103.84582355392257
Finished 13 epochs...
Completing Train Step...
At time: 107.90544509887695 and batch: 50, loss is 3.6773596525192263 and perplexity is 39.541851890439006
At time: 109.406170129776 and batch: 100, loss is 3.5577221488952637 and perplexity is 35.0831917723999
At time: 110.88641571998596 and batch: 150, loss is 3.623366479873657 and perplexity is 37.46347580709516
At time: 112.36407709121704 and batch: 200, loss is 3.5908457612991334 and perplexity is 36.264734268597046
At time: 113.86156010627747 and batch: 250, loss is 3.606706585884094 and perplexity is 36.84450855232932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.644777297973633 and perplexity of 104.04019324336612
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 116.08128142356873 and batch: 50, loss is 3.6659300756454467 and perplexity is 39.092478221962054
At time: 117.5920889377594 and batch: 100, loss is 3.5452368593215944 and perplexity is 34.647891046391656
At time: 119.08739161491394 and batch: 150, loss is 3.606105852127075 and perplexity is 36.82238145919249
At time: 120.57652974128723 and batch: 200, loss is 3.566144332885742 and perplexity is 35.37991665027034
At time: 122.06548643112183 and batch: 250, loss is 3.5761290121078493 and perplexity is 35.73494323270723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641580963134766 and perplexity of 103.70817684951525
Finished 15 epochs...
Completing Train Step...
At time: 124.30081510543823 and batch: 50, loss is 3.6550020790100097 and perplexity is 38.667601505408406
At time: 125.77787280082703 and batch: 100, loss is 3.5342242383956908 and perplexity is 34.26842027540032
At time: 127.25811314582825 and batch: 150, loss is 3.5991527891159056 and perplexity is 36.56724115189956
At time: 128.74710941314697 and batch: 200, loss is 3.5634327507019044 and perplexity is 35.28411104969792
At time: 130.2508945465088 and batch: 250, loss is 3.577568349838257 and perplexity is 35.7864149184796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.642173385620117 and perplexity of 103.7696341079291
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 132.47988939285278 and batch: 50, loss is 3.6500138568878175 and perplexity is 38.47519919185722
At time: 133.98987483978271 and batch: 100, loss is 3.5282845449447633 and perplexity is 34.06547966307422
At time: 135.52032947540283 and batch: 150, loss is 3.591864237785339 and perplexity is 36.301687862717145
At time: 137.02882933616638 and batch: 200, loss is 3.553013310432434 and perplexity is 34.91837903240264
At time: 138.5407693386078 and batch: 250, loss is 3.564037322998047 and perplexity is 35.30544929533872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641090011596679 and perplexity of 103.65727365710157
Finished 17 epochs...
Completing Train Step...
At time: 140.7727723121643 and batch: 50, loss is 3.645848455429077 and perplexity is 38.31526786135838
At time: 142.255384683609 and batch: 100, loss is 3.5242175483703613 and perplexity is 33.927216821807576
At time: 143.7396821975708 and batch: 150, loss is 3.5893208742141725 and perplexity is 36.209476785090224
At time: 145.21963357925415 and batch: 200, loss is 3.552201466560364 and perplexity is 34.890042264434655
At time: 146.69944739341736 and batch: 250, loss is 3.565164008140564 and perplexity is 35.34524983763921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641270446777344 and perplexity of 103.67597876347993
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 149.00507974624634 and batch: 50, loss is 3.643603081703186 and perplexity is 38.22933228045489
At time: 150.4686689376831 and batch: 100, loss is 3.521583008766174 and perplexity is 33.837951863096016
At time: 151.93520641326904 and batch: 150, loss is 3.586299934387207 and perplexity is 36.10025519354821
At time: 153.41049885749817 and batch: 200, loss is 3.547990264892578 and perplexity is 34.74342220027355
At time: 154.89058375358582 and batch: 250, loss is 3.5597341203689576 and perplexity is 35.153849209991236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641263961791992 and perplexity of 103.67530642845641
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 157.10014128684998 and batch: 50, loss is 3.6424488067626952 and perplexity is 38.18523057785351
At time: 158.58302903175354 and batch: 100, loss is 3.5202712488174437 and perplexity is 33.79359369309098
At time: 160.0648968219757 and batch: 150, loss is 3.5850354480743407 and perplexity is 36.05463576361587
At time: 161.57601189613342 and batch: 200, loss is 3.5464216375350954 and perplexity is 34.68896544006588
At time: 163.0905647277832 and batch: 250, loss is 3.5577366018295287 and perplexity is 35.08369883112862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641268157958985 and perplexity of 103.67574146826793
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 165.323410987854 and batch: 50, loss is 3.6420110082626342 and perplexity is 38.16851680008242
At time: 166.79776811599731 and batch: 100, loss is 3.5197639894485473 and perplexity is 33.776455923096165
At time: 168.28248000144958 and batch: 150, loss is 3.584561438560486 and perplexity is 36.037549573073704
At time: 169.7961597442627 and batch: 200, loss is 3.545844020843506 and perplexity is 34.66893430033099
At time: 171.30637311935425 and batch: 250, loss is 3.5570059061050414 and perplexity is 35.05807268599105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641264724731445 and perplexity of 103.67538552646815
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 173.53288340568542 and batch: 50, loss is 3.641851954460144 and perplexity is 38.162446435120074
At time: 175.03146934509277 and batch: 100, loss is 3.5195775508880613 and perplexity is 33.77015927626267
At time: 176.51114678382874 and batch: 150, loss is 3.5843868589401247 and perplexity is 36.03125870049553
At time: 177.99276494979858 and batch: 200, loss is 3.5456353712081907 and perplexity is 34.66170139443029
At time: 179.4735975265503 and batch: 250, loss is 3.556741247177124 and perplexity is 35.04879548176073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.641263580322265 and perplexity of 103.67526687947311
Annealing...
Model not improving. Stopping early with 103.65727365710157loss at 21 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 3.1067821438718655, 'num_layers': 1, 'dropout': 0.6823422046714526, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 15.94026313349056, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.604865550994873 and batch: 50, loss is 6.433869285583496 and perplexity is 622.5782265495477
At time: 3.0136239528656006 and batch: 100, loss is 5.857456579208374 and perplexity is 349.8332383941054
At time: 4.408458948135376 and batch: 150, loss is 5.783594331741333 and perplexity is 324.92498221805124
At time: 5.810938835144043 and batch: 200, loss is 5.744922256469726 and perplexity is 312.59932431031285
At time: 7.222137928009033 and batch: 250, loss is 5.742587194442749 and perplexity is 311.8702370617114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.656396865844727 and perplexity of 286.115869202231
Finished 1 epochs...
Completing Train Step...
At time: 9.338385820388794 and batch: 50, loss is 5.305852203369141 and perplexity is 201.51265902708266
At time: 10.729671239852905 and batch: 100, loss is 5.108239021301269 and perplexity is 165.37886963276182
At time: 12.12004542350769 and batch: 150, loss is 5.05911847114563 and perplexity is 157.45165694931748
At time: 13.508804082870483 and batch: 200, loss is 5.01791561126709 and perplexity is 151.0960324665366
At time: 14.923740148544312 and batch: 250, loss is 5.021273679733277 and perplexity is 151.60427616949846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.213808822631836 and perplexity of 183.79276075871175
Finished 2 epochs...
Completing Train Step...
At time: 17.12409543991089 and batch: 50, loss is 4.918078651428223 and perplexity is 136.73963612749185
At time: 18.604637384414673 and batch: 100, loss is 4.799658365249634 and perplexity is 121.46891242777339
At time: 20.113214254379272 and batch: 150, loss is 4.853877973556519 and perplexity is 128.23672546613693
At time: 21.636708736419678 and batch: 200, loss is 4.846045951843262 and perplexity is 127.23629545653466
At time: 23.171283721923828 and batch: 250, loss is 4.8575706291198735 and perplexity is 128.71113490009822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.185303497314453 and perplexity of 178.62765457735716
Finished 3 epochs...
Completing Train Step...
At time: 25.468647003173828 and batch: 50, loss is 4.832674894332886 and perplexity is 125.54633508945456
At time: 27.0015606880188 and batch: 100, loss is 4.70785270690918 and perplexity is 110.81395421026293
At time: 28.521244287490845 and batch: 150, loss is 4.756440486907959 and perplexity is 116.33110596379032
At time: 30.036229372024536 and batch: 200, loss is 4.759996452331543 and perplexity is 116.7455117238858
At time: 31.552053213119507 and batch: 250, loss is 4.766881284713745 and perplexity is 117.55205828650713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.165943908691406 and perplexity of 175.2027559508087
Finished 4 epochs...
Completing Train Step...
At time: 33.80738806724548 and batch: 50, loss is 4.753228588104248 and perplexity is 115.95806163473084
At time: 35.303067684173584 and batch: 100, loss is 4.6464747714996335 and perplexity is 104.2169486934431
At time: 36.79592037200928 and batch: 150, loss is 4.67565993309021 and perplexity is 107.30335676707205
At time: 38.290796756744385 and batch: 200, loss is 4.698578109741211 and perplexity is 109.79095073002527
At time: 39.78524303436279 and batch: 250, loss is 4.7204071807861325 and perplexity is 112.2139347249064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.173169708251953 and perplexity of 176.47332084314294
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 42.00947546958923 and batch: 50, loss is 4.634013404846192 and perplexity is 102.92632127628444
At time: 43.50805950164795 and batch: 100, loss is 4.459654579162597 and perplexity is 86.45763966757102
At time: 44.99462151527405 and batch: 150, loss is 4.448841152191162 and perplexity is 85.52777287898384
At time: 46.476430892944336 and batch: 200, loss is 4.432911338806153 and perplexity is 84.17612574369275
At time: 47.96018314361572 and batch: 250, loss is 4.481157503128052 and perplexity is 88.33686371709149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.013824462890625 and perplexity of 150.479138940532
Finished 6 epochs...
Completing Train Step...
At time: 50.179190158843994 and batch: 50, loss is 4.502113752365112 and perplexity is 90.20760646245147
At time: 51.66355586051941 and batch: 100, loss is 4.3822725391387936 and perplexity is 80.01967477615146
At time: 53.13076043128967 and batch: 150, loss is 4.399347076416015 and perplexity is 81.39770482973033
At time: 54.59920310974121 and batch: 200, loss is 4.393111515045166 and perplexity is 80.89172362372142
At time: 56.07673168182373 and batch: 250, loss is 4.430805673599243 and perplexity is 83.99906548469646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.011948013305664 and perplexity of 150.19703718036206
Finished 7 epochs...
Completing Train Step...
At time: 58.29983925819397 and batch: 50, loss is 4.441419839859009 and perplexity is 84.89539400633761
At time: 59.78341865539551 and batch: 100, loss is 4.340152587890625 and perplexity is 76.71924487295696
At time: 61.310556173324585 and batch: 150, loss is 4.364904041290283 and perplexity is 78.64185320893168
At time: 62.82682538032532 and batch: 200, loss is 4.354843978881836 and perplexity is 77.8546774153619
At time: 64.34447050094604 and batch: 250, loss is 4.391012859344483 and perplexity is 80.72213776021304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0163311004638675 and perplexity of 150.85680874710437
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 66.60212802886963 and batch: 50, loss is 4.367111415863037 and perplexity is 78.81563696842174
At time: 68.12579679489136 and batch: 100, loss is 4.216165533065796 and perplexity is 67.77311165764122
At time: 69.63711142539978 and batch: 150, loss is 4.213070869445801 and perplexity is 67.56370086968025
At time: 71.1537537574768 and batch: 200, loss is 4.193978681564331 and perplexity is 66.28599788268272
At time: 72.67174220085144 and batch: 250, loss is 4.265542821884155 and perplexity is 71.20356033580906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.943555068969727 and perplexity of 140.26802672892464
Finished 9 epochs...
Completing Train Step...
At time: 74.90819931030273 and batch: 50, loss is 4.304356632232666 and perplexity is 74.02157703322952
At time: 76.39001941680908 and batch: 100, loss is 4.178327579498291 and perplexity is 65.25662538387341
At time: 77.87390184402466 and batch: 150, loss is 4.19298981666565 and perplexity is 66.22048238442947
At time: 79.36175346374512 and batch: 200, loss is 4.191444177627563 and perplexity is 66.11820848136479
At time: 80.8552417755127 and batch: 250, loss is 4.25478590965271 and perplexity is 70.44173467853919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.941743469238281 and perplexity of 140.01414724281028
Finished 10 epochs...
Completing Train Step...
At time: 83.09763169288635 and batch: 50, loss is 4.2785661506652835 and perplexity is 72.1369323254253
At time: 84.5710141658783 and batch: 100, loss is 4.159628100395203 and perplexity is 64.04769885629797
At time: 86.04596757888794 and batch: 150, loss is 4.17973822593689 and perplexity is 65.34874436844915
At time: 87.52419519424438 and batch: 200, loss is 4.181271800994873 and perplexity is 65.44903845748338
At time: 89.00337433815002 and batch: 250, loss is 4.243322772979736 and perplexity is 69.63886195797257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.944805145263672 and perplexity of 140.443482107393
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 91.20707511901855 and batch: 50, loss is 4.251628522872925 and perplexity is 70.21967362751235
At time: 92.69871592521667 and batch: 100, loss is 4.114869127273559 and perplexity is 61.244198543001865
At time: 94.19011807441711 and batch: 150, loss is 4.117381134033203 and perplexity is 61.398237776527544
At time: 95.68429160118103 and batch: 200, loss is 4.118489542007446 and perplexity is 61.466329802784664
At time: 97.17808508872986 and batch: 250, loss is 4.196453175544739 and perplexity is 66.45022529149735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.916446304321289 and perplexity of 136.51661165425452
Finished 12 epochs...
Completing Train Step...
At time: 99.40496587753296 and batch: 50, loss is 4.2288901901245115 and perplexity is 68.64101141406294
At time: 100.88636779785156 and batch: 100, loss is 4.100448775291443 and perplexity is 60.367372904667974
At time: 102.52561545372009 and batch: 150, loss is 4.112063388824463 and perplexity is 61.07260417738861
At time: 104.02032017707825 and batch: 200, loss is 4.118546214103699 and perplexity is 61.469813327251906
At time: 105.52852487564087 and batch: 250, loss is 4.1913501214981075 and perplexity is 66.11198995103852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.91478157043457 and perplexity of 136.28953688670563
Finished 13 epochs...
Completing Train Step...
At time: 107.811931848526 and batch: 50, loss is 4.217374544143677 and perplexity is 67.85509965262801
At time: 109.32001662254333 and batch: 100, loss is 4.093703236579895 and perplexity is 59.96153279778769
At time: 110.82746028900146 and batch: 150, loss is 4.107938404083252 and perplexity is 60.82119949377399
At time: 112.34200310707092 and batch: 200, loss is 4.115892190933227 and perplexity is 61.30688731873303
At time: 113.85072946548462 and batch: 250, loss is 4.186852893829346 and perplexity is 65.81533683887118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.914869689941407 and perplexity of 136.30154718264595
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 116.10795831680298 and batch: 50, loss is 4.205537891387939 and perplexity is 67.05665716578163
At time: 117.59138250350952 and batch: 100, loss is 4.076733503341675 and perplexity is 58.95258656806567
At time: 119.07559275627136 and batch: 150, loss is 4.085431089401245 and perplexity is 59.467568064679234
At time: 120.55729246139526 and batch: 200, loss is 4.091865749359131 and perplexity is 59.85145541140144
At time: 122.03951072692871 and batch: 250, loss is 4.169565916061401 and perplexity is 64.68736626531157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.91113052368164 and perplexity of 135.7928446905601
Finished 15 epochs...
Completing Train Step...
At time: 124.29347681999207 and batch: 50, loss is 4.198273668289184 and perplexity is 66.571307625822
At time: 125.7628767490387 and batch: 100, loss is 4.0718930721282955 and perplexity is 58.66792013787484
At time: 127.23558139801025 and batch: 150, loss is 4.084248723983765 and perplexity is 59.39729721983279
At time: 128.7150628566742 and batch: 200, loss is 4.093189244270325 and perplexity is 59.93072095026426
At time: 130.19719696044922 and batch: 250, loss is 4.168288550376892 and perplexity is 64.6047895949536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.910376358032226 and perplexity of 135.690472999117
Finished 16 epochs...
Completing Train Step...
At time: 132.42917799949646 and batch: 50, loss is 4.19377516746521 and perplexity is 66.27250912016255
At time: 133.90128660202026 and batch: 100, loss is 4.068806796073914 and perplexity is 58.487133862469896
At time: 135.38158202171326 and batch: 150, loss is 4.0834468603134155 and perplexity is 59.34968777576805
At time: 136.88317131996155 and batch: 200, loss is 4.093662796020507 and perplexity is 59.95910796889067
At time: 138.3812906742096 and batch: 250, loss is 4.166665625572205 and perplexity is 64.50002591420103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.910031127929687 and perplexity of 135.6436366483342
Finished 17 epochs...
Completing Train Step...
At time: 140.62953329086304 and batch: 50, loss is 4.1899251461029055 and perplexity is 66.01784908217121
At time: 142.12986540794373 and batch: 100, loss is 4.066341180801391 and perplexity is 58.34310472515869
At time: 143.61717343330383 and batch: 150, loss is 4.082886867523193 and perplexity is 59.31646168256627
At time: 145.11605381965637 and batch: 200, loss is 4.093893456459045 and perplexity is 59.972939758191075
At time: 146.59694361686707 and batch: 250, loss is 4.164985399246216 and perplexity is 64.39174226862421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.910115432739258 and perplexity of 135.65507254133493
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 148.85647463798523 and batch: 50, loss is 4.1859431648254395 and perplexity is 65.75548994440231
At time: 150.34097909927368 and batch: 100, loss is 4.059723544120788 and perplexity is 57.958285955040196
At time: 151.8278341293335 and batch: 150, loss is 4.075723886489868 and perplexity is 58.893097079057526
At time: 153.3138997554779 and batch: 200, loss is 4.0864272308349605 and perplexity is 59.52683568775733
At time: 154.80097365379333 and batch: 250, loss is 4.1594469928741455 and perplexity is 64.03610038664543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906608581542969 and perplexity of 135.18018355709404
Finished 19 epochs...
Completing Train Step...
At time: 157.01048350334167 and batch: 50, loss is 4.183481121063233 and perplexity is 65.5937961807926
At time: 158.4954879283905 and batch: 100, loss is 4.05910849571228 and perplexity is 57.92264976361848
At time: 159.9727177619934 and batch: 150, loss is 4.075978636741638 and perplexity is 58.90810202154122
At time: 161.4816608428955 and batch: 200, loss is 4.086224122047424 and perplexity is 59.51474649208741
At time: 162.9984221458435 and batch: 250, loss is 4.158086285591126 and perplexity is 63.949025253792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906205749511718 and perplexity of 135.1257396157849
Finished 20 epochs...
Completing Train Step...
At time: 165.24132132530212 and batch: 50, loss is 4.181962461471557 and perplexity is 65.49425713516223
At time: 166.72741961479187 and batch: 100, loss is 4.058769607543946 and perplexity is 57.90302378862819
At time: 168.22308945655823 and batch: 150, loss is 4.076213879585266 and perplexity is 58.92196136106479
At time: 169.71328496932983 and batch: 200, loss is 4.085990571975708 and perplexity is 59.50084844178453
At time: 171.19934487342834 and batch: 250, loss is 4.1569215059280396 and perplexity is 63.87458209305853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906159973144531 and perplexity of 135.11955419188598
Finished 21 epochs...
Completing Train Step...
At time: 173.42499232292175 and batch: 50, loss is 4.180569391250611 and perplexity is 65.40308255694391
At time: 174.92021226882935 and batch: 100, loss is 4.058370509147644 and perplexity is 57.87991939546294
At time: 176.4137406349182 and batch: 150, loss is 4.076409788131714 and perplexity is 58.93350580765983
At time: 177.9064700603485 and batch: 200, loss is 4.085673551559449 and perplexity is 59.48198844770325
At time: 179.4057800769806 and batch: 250, loss is 4.155754098892212 and perplexity is 63.800057964971955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906093597412109 and perplexity of 135.1105858301561
Finished 22 epochs...
Completing Train Step...
At time: 181.65038919448853 and batch: 50, loss is 4.179286346435547 and perplexity is 65.31922128137185
At time: 183.13341212272644 and batch: 100, loss is 4.058006267547608 and perplexity is 57.85884096006693
At time: 184.61510801315308 and batch: 150, loss is 4.07657895565033 and perplexity is 58.943476285917434
At time: 186.1007936000824 and batch: 200, loss is 4.085322265625 and perplexity is 59.46109693147127
At time: 187.59128618240356 and batch: 250, loss is 4.154571995735169 and perplexity is 63.724684273548405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906087112426758 and perplexity of 135.10970964282723
Finished 23 epochs...
Completing Train Step...
At time: 189.8208076953888 and batch: 50, loss is 4.178065671920776 and perplexity is 65.23953641716497
At time: 191.3107316493988 and batch: 100, loss is 4.05766592502594 and perplexity is 57.839152486836184
At time: 192.81138682365417 and batch: 150, loss is 4.076715641021728 and perplexity is 58.95153354750743
At time: 194.2892301082611 and batch: 200, loss is 4.084950051307678 and perplexity is 59.4389687783329
At time: 195.76850295066833 and batch: 250, loss is 4.153401408195496 and perplexity is 63.65013259531402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.906076049804687 and perplexity of 135.10821498343878
Finished 24 epochs...
Completing Train Step...
At time: 198.00242853164673 and batch: 50, loss is 4.176912269592285 and perplexity is 65.16433236255483
At time: 199.50478768348694 and batch: 100, loss is 4.0573175382614135 and perplexity is 57.819005601296226
At time: 200.987464427948 and batch: 150, loss is 4.076810827255249 and perplexity is 58.957145189017375
At time: 202.5059118270874 and batch: 200, loss is 4.084557085037232 and perplexity is 59.41561585720857
At time: 204.01854252815247 and batch: 250, loss is 4.152235789299011 and perplexity is 63.575984020882196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9060813903808596 and perplexity of 135.10893654107917
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 206.2896921634674 and batch: 50, loss is 4.175357551574707 and perplexity is 65.06309891605028
At time: 207.79927563667297 and batch: 100, loss is 4.055357227325439 and perplexity is 57.70577339373714
At time: 209.31788849830627 and batch: 150, loss is 4.074580678939819 and perplexity is 58.825808515556304
At time: 210.8293251991272 and batch: 200, loss is 4.081435623168946 and perplexity is 59.23044143573164
At time: 212.34068989753723 and batch: 250, loss is 4.149856872558594 and perplexity is 63.42492180166899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905863952636719 and perplexity of 135.07956195239234
Finished 26 epochs...
Completing Train Step...
At time: 214.5929238796234 and batch: 50, loss is 4.1748910331726075 and perplexity is 65.032752862155
At time: 216.07256770133972 and batch: 100, loss is 4.055114398002624 and perplexity is 57.69176244106535
At time: 217.53616166114807 and batch: 150, loss is 4.074473004341126 and perplexity is 58.819474811227245
At time: 219.0040638446808 and batch: 200, loss is 4.08108889579773 and perplexity is 59.20990818040319
At time: 220.4834268093109 and batch: 250, loss is 4.14961462020874 and perplexity is 63.40955882625681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9058269500732425 and perplexity of 135.07456375480047
Finished 27 epochs...
Completing Train Step...
At time: 222.6872787475586 and batch: 50, loss is 4.174556865692138 and perplexity is 65.01102466161429
At time: 224.18371272087097 and batch: 100, loss is 4.054958510398865 and perplexity is 57.68276971140749
At time: 225.6648554801941 and batch: 150, loss is 4.074504818916321 and perplexity is 58.82134615759943
At time: 227.16802167892456 and batch: 200, loss is 4.080781164169312 and perplexity is 59.19169022220511
At time: 228.6768045425415 and batch: 250, loss is 4.1493320512771605 and perplexity is 63.39164378620341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9058483123779295 and perplexity of 135.07744928960759
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 230.93201398849487 and batch: 50, loss is 4.174147491455078 and perplexity is 64.98441626975959
At time: 232.41418051719666 and batch: 100, loss is 4.054432210922241 and perplexity is 57.652419287304895
At time: 233.89874053001404 and batch: 150, loss is 4.073631277084351 and perplexity is 58.7699856871437
At time: 235.3786280155182 and batch: 200, loss is 4.079602184295655 and perplexity is 59.12194553261676
At time: 236.86812567710876 and batch: 250, loss is 4.148655033111572 and perplexity is 63.34874101642959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905642318725586 and perplexity of 135.04962705817906
Finished 29 epochs...
Completing Train Step...
At time: 239.15906405448914 and batch: 50, loss is 4.174046277999878 and perplexity is 64.97783930529903
At time: 240.6484272480011 and batch: 100, loss is 4.05444028377533 and perplexity is 57.65288470869467
At time: 242.13895678520203 and batch: 150, loss is 4.073476581573487 and perplexity is 58.760894937351566
At time: 243.6305854320526 and batch: 200, loss is 4.0795252323150635 and perplexity is 59.117396156855605
At time: 245.12154603004456 and batch: 250, loss is 4.1485737657546995 and perplexity is 63.34359304087001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905601501464844 and perplexity of 135.0441148148364
Finished 30 epochs...
Completing Train Step...
At time: 247.35641431808472 and batch: 50, loss is 4.173909111022949 and perplexity is 64.96892710275807
At time: 248.82434487342834 and batch: 100, loss is 4.054437975883484 and perplexity is 57.65275165222571
At time: 250.2959499359131 and batch: 150, loss is 4.073334803581238 and perplexity is 58.75256452619306
At time: 251.7740683555603 and batch: 200, loss is 4.0794373750686646 and perplexity is 59.11220249336881
At time: 253.25053024291992 and batch: 250, loss is 4.148508529663086 and perplexity is 63.33946088721554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905585479736328 and perplexity of 135.04195119202376
Finished 31 epochs...
Completing Train Step...
At time: 255.47145533561707 and batch: 50, loss is 4.173778419494629 and perplexity is 64.9604367692011
At time: 256.96090269088745 and batch: 100, loss is 4.054423732757568 and perplexity is 57.65193050267241
At time: 258.4435477256775 and batch: 150, loss is 4.073191871643067 and perplexity is 58.7441675083881
At time: 259.92479848861694 and batch: 200, loss is 4.079344701766968 and perplexity is 59.106724624223226
At time: 261.4245104789734 and batch: 250, loss is 4.148435764312744 and perplexity is 63.33485213683427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905574035644531 and perplexity of 135.0404057683809
Finished 32 epochs...
Completing Train Step...
At time: 263.65421295166016 and batch: 50, loss is 4.173647356033325 and perplexity is 64.95192338741917
At time: 265.15615224838257 and batch: 100, loss is 4.054406833648682 and perplexity is 57.65095624465339
At time: 266.63723278045654 and batch: 150, loss is 4.073055129051209 and perplexity is 58.73613522785643
At time: 268.1165096759796 and batch: 200, loss is 4.079251875877381 and perplexity is 59.101238244572365
At time: 269.59319853782654 and batch: 250, loss is 4.148357639312744 and perplexity is 63.32990429478868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905540466308594 and perplexity of 135.0358726277222
Finished 33 epochs...
Completing Train Step...
At time: 271.8343334197998 and batch: 50, loss is 4.17351496219635 and perplexity is 64.94332472228068
At time: 273.30970311164856 and batch: 100, loss is 4.054380645751953 and perplexity is 57.649446507133476
At time: 274.78111243247986 and batch: 150, loss is 4.072912530899048 and perplexity is 58.7277601606566
At time: 276.28600549697876 and batch: 200, loss is 4.079157528877258 and perplexity is 59.09566248307277
At time: 277.82442355155945 and batch: 250, loss is 4.148284931182861 and perplexity is 63.32529986327357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905463027954101 and perplexity of 135.02541607682272
Finished 34 epochs...
Completing Train Step...
At time: 280.138710975647 and batch: 50, loss is 4.1733776521682735 and perplexity is 64.93440796473381
At time: 281.656334400177 and batch: 100, loss is 4.0543509149551396 and perplexity is 57.64773256863145
At time: 283.14305114746094 and batch: 150, loss is 4.072788205146789 and perplexity is 58.7204592415517
At time: 284.6343321800232 and batch: 200, loss is 4.079064574241638 and perplexity is 59.09016952260197
At time: 286.12264227867126 and batch: 250, loss is 4.148223824501038 and perplexity is 63.32143038255025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905417251586914 and perplexity of 135.01923524526583
Finished 35 epochs...
Completing Train Step...
At time: 288.3468508720398 and batch: 50, loss is 4.173255047798157 and perplexity is 64.92644721056797
At time: 289.84958243370056 and batch: 100, loss is 4.054326786994934 and perplexity is 57.646341663214
At time: 291.3230495452881 and batch: 150, loss is 4.072668175697327 and perplexity is 58.71341148013469
At time: 292.7924151420593 and batch: 200, loss is 4.078975563049316 and perplexity is 59.0849100702368
At time: 294.2635226249695 and batch: 250, loss is 4.148144297599792 and perplexity is 63.31639482564325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9054523468017575 and perplexity of 135.0239738574856
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 296.48913073539734 and batch: 50, loss is 4.173078207969666 and perplexity is 64.91496664391937
At time: 297.96868228912354 and batch: 100, loss is 4.054119634628296 and perplexity is 57.634401323887914
At time: 299.4385459423065 and batch: 150, loss is 4.072288579940796 and perplexity is 58.69112834784418
At time: 300.9149761199951 and batch: 200, loss is 4.078618564605713 and perplexity is 59.0638206139768
At time: 302.3980710506439 and batch: 250, loss is 4.147931599617005 and perplexity is 63.30292898831544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905426788330078 and perplexity of 135.02052289517457
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 304.6077768802643 and batch: 50, loss is 4.17301236152649 and perplexity is 64.91069236498154
At time: 306.1035325527191 and batch: 100, loss is 4.05402247428894 and perplexity is 57.628801817926096
At time: 307.57953548431396 and batch: 150, loss is 4.072161588668823 and perplexity is 58.68367556003128
At time: 309.0575909614563 and batch: 200, loss is 4.078506045341491 and perplexity is 59.05717517021643
At time: 310.5404770374298 and batch: 250, loss is 4.14786774635315 and perplexity is 63.298887018735805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905416107177734 and perplexity of 135.01908072810198
Finished 38 epochs...
Completing Train Step...
At time: 312.7605435848236 and batch: 50, loss is 4.172998418807984 and perplexity is 64.90978733977914
At time: 314.23652172088623 and batch: 100, loss is 4.054013047218323 and perplexity is 57.62825854970248
At time: 315.7279884815216 and batch: 150, loss is 4.072145648002625 and perplexity is 58.68274011060374
At time: 317.21626234054565 and batch: 200, loss is 4.078498349189759 and perplexity is 59.05672065898441
At time: 318.7038223743439 and batch: 250, loss is 4.147861213684082 and perplexity is 63.298473509405206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905407333374024 and perplexity of 135.01789610238737
Finished 39 epochs...
Completing Train Step...
At time: 320.9354131221771 and batch: 50, loss is 4.17298394203186 and perplexity is 64.90884766212136
At time: 322.42083644866943 and batch: 100, loss is 4.054004564285278 and perplexity is 57.62776969511717
At time: 323.9070017337799 and batch: 150, loss is 4.0721305465698245 and perplexity is 58.68185392383879
At time: 325.3925006389618 and batch: 200, loss is 4.078489689826966 and perplexity is 59.05620926762902
At time: 326.8753252029419 and batch: 250, loss is 4.1478544616699216 and perplexity is 63.298046118658625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9053997039794925 and perplexity of 135.01686600151876
Finished 40 epochs...
Completing Train Step...
At time: 329.0978147983551 and batch: 50, loss is 4.172969880104065 and perplexity is 64.90793492500974
At time: 330.58522057533264 and batch: 100, loss is 4.053995881080628 and perplexity is 57.627269303571865
At time: 332.0568027496338 and batch: 150, loss is 4.072116093635559 and perplexity is 58.68100580499037
At time: 333.533997297287 and batch: 200, loss is 4.078480315208435 and perplexity is 59.05565564079031
At time: 335.01529574394226 and batch: 250, loss is 4.1478475570678714 and perplexity is 63.29760907234844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905395126342773 and perplexity of 135.01624794476984
Finished 41 epochs...
Completing Train Step...
At time: 337.24904584884644 and batch: 50, loss is 4.1729561233520505 and perplexity is 64.907042008787
At time: 338.7328245639801 and batch: 100, loss is 4.053987426757812 and perplexity is 57.626782106093685
At time: 340.2148530483246 and batch: 150, loss is 4.072102084159851 and perplexity is 58.680183720623496
At time: 341.69960951805115 and batch: 200, loss is 4.078470683097839 and perplexity is 59.05508681292336
At time: 343.1874294281006 and batch: 250, loss is 4.147840394973755 and perplexity is 63.297155730538336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905392456054687 and perplexity of 135.01588741297292
Finished 42 epochs...
Completing Train Step...
At time: 345.4183394908905 and batch: 50, loss is 4.172942452430725 and perplexity is 64.90615467578756
At time: 346.9090175628662 and batch: 100, loss is 4.053979001045227 and perplexity is 57.626296561435964
At time: 348.3962285518646 and batch: 150, loss is 4.072088160514832 and perplexity is 58.67936668426379
At time: 349.8804814815521 and batch: 200, loss is 4.078461122512818 and perplexity is 59.054522214443885
At time: 351.3580689430237 and batch: 250, loss is 4.147833213806153 and perplexity is 63.2967011846864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9053901672363285 and perplexity of 135.01557838648472
Finished 43 epochs...
Completing Train Step...
At time: 353.5928509235382 and batch: 50, loss is 4.1729289197921755 and perplexity is 64.9052763301999
At time: 355.08479166030884 and batch: 100, loss is 4.053970351219177 and perplexity is 57.625798106150576
At time: 356.5590810775757 and batch: 150, loss is 4.072074251174927 and perplexity is 58.6785504986835
At time: 358.0426104068756 and batch: 200, loss is 4.0784514093399045 and perplexity is 59.05394861044408
At time: 359.52807354927063 and batch: 250, loss is 4.1478257703781125 and perplexity is 63.29623004199941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905389785766602 and perplexity of 135.01552688213874
Finished 44 epochs...
Completing Train Step...
At time: 361.8341865539551 and batch: 50, loss is 4.172915415763855 and perplexity is 64.90439985342819
At time: 363.3001079559326 and batch: 100, loss is 4.053962135314942 and perplexity is 57.62532466005676
At time: 364.78103256225586 and batch: 150, loss is 4.072060422897339 and perplexity is 58.677739081009015
At time: 366.264910697937 and batch: 200, loss is 4.078441710472107 and perplexity is 59.0533758567811
At time: 367.749648809433 and batch: 250, loss is 4.147818465232849 and perplexity is 63.295767655533204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905388641357422 and perplexity of 135.01537236921877
Finished 45 epochs...
Completing Train Step...
At time: 369.99652671813965 and batch: 50, loss is 4.172901887893676 and perplexity is 64.90352184107176
At time: 371.48362851142883 and batch: 100, loss is 4.053953676223755 and perplexity is 57.62483720424252
At time: 372.96215534210205 and batch: 150, loss is 4.07204656124115 and perplexity is 58.67692571600122
At time: 374.4355697631836 and batch: 200, loss is 4.078432025909424 and perplexity is 59.05280395343033
At time: 375.9319038391113 and batch: 250, loss is 4.147810821533203 and perplexity is 63.29528384354546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905387878417969 and perplexity of 135.0152693607037
Finished 46 epochs...
Completing Train Step...
At time: 378.1878628730774 and batch: 50, loss is 4.172888412475586 and perplexity is 64.90264724487218
At time: 379.66291189193726 and batch: 100, loss is 4.053945488929749 and perplexity is 57.624365414689606
At time: 381.14117431640625 and batch: 150, loss is 4.072032723426819 and perplexity is 58.67611376121551
At time: 382.6287603378296 and batch: 200, loss is 4.078422303199768 and perplexity is 59.05222980295427
At time: 384.1106495857239 and batch: 250, loss is 4.14780330657959 and perplexity is 63.29480818421072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905387115478516 and perplexity of 135.01516635226727
Finished 47 epochs...
Completing Train Step...
At time: 386.3652367591858 and batch: 50, loss is 4.1728748607635495 and perplexity is 64.90176770884597
At time: 387.85283041000366 and batch: 100, loss is 4.053937253952026 and perplexity is 57.623890881278015
At time: 389.3567202091217 and batch: 150, loss is 4.072019100189209 and perplexity is 58.67531440802061
At time: 390.83844470977783 and batch: 200, loss is 4.078412461280823 and perplexity is 59.051648618554985
At time: 392.32563304901123 and batch: 250, loss is 4.147795734405517 and perplexity is 63.29432890671985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905387115478516 and perplexity of 135.01516635226727
Finished 48 epochs...
Completing Train Step...
At time: 394.55397295951843 and batch: 50, loss is 4.172861585617065 and perplexity is 64.90090613409133
At time: 396.04770588874817 and batch: 100, loss is 4.053929166793823 and perplexity is 57.62342486964056
At time: 397.53231406211853 and batch: 150, loss is 4.072005343437195 and perplexity is 58.674507231823036
At time: 399.0143747329712 and batch: 200, loss is 4.078402767181396 and perplexity is 59.05107616877668
At time: 400.50806736946106 and batch: 250, loss is 4.147788019180298 and perplexity is 63.29384057860099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905387496948242 and perplexity of 135.0152178564756
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 402.82471776008606 and batch: 50, loss is 4.1728415775299075 and perplexity is 64.89960760409537
At time: 404.30512952804565 and batch: 100, loss is 4.053900852203369 and perplexity is 57.621793309063385
At time: 405.78916478157043 and batch: 150, loss is 4.071966061592102 and perplexity is 58.67220243418768
At time: 407.2819845676422 and batch: 200, loss is 4.078366312980652 and perplexity is 59.04892354822813
At time: 408.76582312583923 and batch: 250, loss is 4.147767043113708 and perplexity is 63.292512936710715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.905387115478516 and perplexity of 135.01516635226727
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 7.634060454147596, 'num_layers': 1, 'dropout': 0.9141402102640862, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 12.297035776623481, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6267879009246826 and batch: 50, loss is 7.2862001895904545 and perplexity is 1460.012373389017
At time: 3.063612699508667 and batch: 100, loss is 6.531833381652832 and perplexity is 686.6559606124179
At time: 4.468176603317261 and batch: 150, loss is 6.380620107650757 and perplexity is 590.2936397903126
At time: 5.873703718185425 and batch: 200, loss is 6.333237857818603 and perplexity is 562.9764838003709
At time: 7.282422304153442 and batch: 250, loss is 6.29919319152832 and perplexity is 544.1327220895386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.2591712951660154 and perplexity of 522.7855256542314
Finished 1 epochs...
Completing Train Step...
At time: 9.419269800186157 and batch: 50, loss is 5.591669921875 and perplexity is 268.183090798614
At time: 10.812896728515625 and batch: 100, loss is 5.184113454818726 and perplexity is 178.41520651368242
At time: 12.213160991668701 and batch: 150, loss is 5.066055669784546 and perplexity is 158.5477278045517
At time: 13.608400821685791 and batch: 200, loss is 4.988993711471558 and perplexity is 146.78863742850368
At time: 15.003807306289673 and batch: 250, loss is 4.9820481967926025 and perplexity is 145.7726471644787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1614940643310545 and perplexity of 174.42486299022
Finished 2 epochs...
Completing Train Step...
At time: 17.131612300872803 and batch: 50, loss is 4.8726374053955075 and perplexity is 130.66507963466339
At time: 18.56719422340393 and batch: 100, loss is 4.7506483364105225 and perplexity is 115.65924632493648
At time: 20.04989194869995 and batch: 150, loss is 4.766286315917969 and perplexity is 117.48213928182301
At time: 21.541852712631226 and batch: 200, loss is 4.741535196304321 and perplexity is 114.61001556354775
At time: 23.042113542556763 and batch: 250, loss is 4.749889144897461 and perplexity is 115.5714721296474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.055717086791992 and perplexity of 156.9170131269414
Finished 3 epochs...
Completing Train Step...
At time: 25.3057279586792 and batch: 50, loss is 4.68128532409668 and perplexity is 107.90868110193718
At time: 26.841764211654663 and batch: 100, loss is 4.58674017906189 and perplexity is 98.17387871024397
At time: 28.3366641998291 and batch: 150, loss is 4.629961280822754 and perplexity is 102.51009492711724
At time: 29.829689025878906 and batch: 200, loss is 4.603713674545288 and perplexity is 99.85445487551955
At time: 31.328307390213013 and batch: 250, loss is 4.622807502746582 and perplexity is 101.7793772697175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.03338737487793 and perplexity of 153.4519325326487
Finished 4 epochs...
Completing Train Step...
At time: 33.55906438827515 and batch: 50, loss is 4.567829322814942 and perplexity is 96.33477096209675
At time: 35.03044295310974 and batch: 100, loss is 4.4897034645080565 and perplexity is 89.09502212484635
At time: 36.50740361213684 and batch: 150, loss is 4.538031673431396 and perplexity is 93.50656740292166
At time: 37.99194312095642 and batch: 200, loss is 4.514086008071899 and perplexity is 91.29408582056111
At time: 39.47237467765808 and batch: 250, loss is 4.548812818527222 and perplexity is 94.52012913411703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.044931411743164 and perplexity of 155.23365162764773
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 41.682042837142944 and batch: 50, loss is 4.444353904724121 and perplexity is 85.14484837779762
At time: 43.185214042663574 and batch: 100, loss is 4.262014169692993 and perplexity is 70.95275050718132
At time: 44.68291997909546 and batch: 150, loss is 4.240176877975464 and perplexity is 69.42012964487445
At time: 46.17611241340637 and batch: 200, loss is 4.1378578853607175 and perplexity is 62.66843459340057
At time: 47.66703915596008 and batch: 250, loss is 4.1484952068328855 and perplexity is 63.338617031954406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.861315536499023 and perplexity of 129.1940498504651
Finished 6 epochs...
Completing Train Step...
At time: 49.89879655838013 and batch: 50, loss is 4.294606275558472 and perplexity is 73.30334744304069
At time: 51.40526509284973 and batch: 100, loss is 4.158879580497742 and perplexity is 63.999775817229136
At time: 52.8997004032135 and batch: 150, loss is 4.1590298080444335 and perplexity is 64.0093910687587
At time: 54.39466214179993 and batch: 200, loss is 4.086792635917663 and perplexity is 59.548591070596515
At time: 55.914687633514404 and batch: 250, loss is 4.129482555389404 and perplexity is 62.14575762624062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.847771835327149 and perplexity of 127.45608008418787
Finished 7 epochs...
Completing Train Step...
At time: 58.168851375579834 and batch: 50, loss is 4.232337522506714 and perplexity is 68.87804813288874
At time: 59.65866470336914 and batch: 100, loss is 4.103363027572632 and perplexity is 60.54355525408057
At time: 61.160073041915894 and batch: 150, loss is 4.1133138990402225 and perplexity is 61.149023864656655
At time: 62.66088104248047 and batch: 200, loss is 4.057396945953369 and perplexity is 57.82359705737822
At time: 64.16916608810425 and batch: 250, loss is 4.108211903572083 and perplexity is 60.83783633572578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.841001892089844 and perplexity of 126.59612386861792
Finished 8 epochs...
Completing Train Step...
At time: 66.40541172027588 and batch: 50, loss is 4.1871187877655025 and perplexity is 65.83283906460748
At time: 67.9083206653595 and batch: 100, loss is 4.06348644733429 and perplexity is 58.17678821912286
At time: 69.39097332954407 and batch: 150, loss is 4.0793503522872925 and perplexity is 59.10705860891562
At time: 70.87659883499146 and batch: 200, loss is 4.0336946821212765 and perplexity is 56.469161903130264
At time: 72.34810495376587 and batch: 250, loss is 4.087459168434143 and perplexity is 59.58829537349014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.838203811645508 and perplexity of 126.24239284481003
Finished 9 epochs...
Completing Train Step...
At time: 74.54403424263 and batch: 50, loss is 4.1496631002426145 and perplexity is 63.412632998334054
At time: 76.01761817932129 and batch: 100, loss is 4.03030565738678 and perplexity is 56.278110438633554
At time: 77.50026345252991 and batch: 150, loss is 4.050842385292054 and perplexity is 57.445828189772435
At time: 78.98700261116028 and batch: 200, loss is 4.011790618896485 and perplexity is 55.245706053917026
At time: 80.49915623664856 and batch: 250, loss is 4.066648902893067 and perplexity is 58.36106095000141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.836746597290039 and perplexity of 126.05856458880419
Finished 10 epochs...
Completing Train Step...
At time: 82.7744038105011 and batch: 50, loss is 4.118553037643433 and perplexity is 61.470232770396635
At time: 84.26414060592651 and batch: 100, loss is 4.002465686798096 and perplexity is 54.73293807514102
At time: 85.75538849830627 and batch: 150, loss is 4.025057578086853 and perplexity is 55.983532113520994
At time: 87.25171661376953 and batch: 200, loss is 3.991648817062378 and perplexity is 54.14408950436277
At time: 88.74616718292236 and batch: 250, loss is 4.046463975906372 and perplexity is 57.194856665433605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8365020751953125 and perplexity of 126.02774425281278
Finished 11 epochs...
Completing Train Step...
At time: 90.98145747184753 and batch: 50, loss is 4.090478935241699 and perplexity is 59.768510096248846
At time: 92.47404980659485 and batch: 100, loss is 3.9770461559295653 and perplexity is 53.35918649683366
At time: 93.94807529449463 and batch: 150, loss is 4.001712965965271 and perplexity is 54.69175495405124
At time: 95.42837238311768 and batch: 200, loss is 3.972229828834534 and perplexity is 53.10280909545137
At time: 96.92021584510803 and batch: 250, loss is 4.028136625289917 and perplexity is 56.15617370089662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.838383483886719 and perplexity of 126.26507713627595
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 99.15082097053528 and batch: 50, loss is 4.05973557472229 and perplexity is 57.958983232276566
At time: 100.62401580810547 and batch: 100, loss is 3.929278883934021 and perplexity is 50.870281062580254
At time: 102.1101438999176 and batch: 150, loss is 3.9323465394973756 and perplexity is 51.02657316589571
At time: 103.61992883682251 and batch: 200, loss is 3.8831483840942385 and perplexity is 48.576913350135
At time: 105.1222608089447 and batch: 250, loss is 3.942972140312195 and perplexity is 51.57165192955518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.817868423461914 and perplexity of 123.70113112582303
Finished 13 epochs...
Completing Train Step...
At time: 107.37402129173279 and batch: 50, loss is 4.039061732292176 and perplexity is 56.7730494886697
At time: 108.87935543060303 and batch: 100, loss is 3.9133910036087034 and perplexity is 50.068446715844864
At time: 110.37506222724915 and batch: 150, loss is 3.9213532972335816 and perplexity is 50.46869773340614
At time: 111.86744832992554 and batch: 200, loss is 3.878157072067261 and perplexity is 48.3350549158749
At time: 113.35848069190979 and batch: 250, loss is 3.9433951139450074 and perplexity is 51.59346999242934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.815437316894531 and perplexity of 123.40076575160418
Finished 14 epochs...
Completing Train Step...
At time: 115.66201090812683 and batch: 50, loss is 4.029004731178284 and perplexity is 56.20494437194506
At time: 117.15604376792908 and batch: 100, loss is 3.905137686729431 and perplexity is 49.656916539941896
At time: 118.63607907295227 and batch: 150, loss is 3.91526385307312 and perplexity is 50.16230524347284
At time: 120.1254506111145 and batch: 200, loss is 3.87586989402771 and perplexity is 48.224630368168675
At time: 121.60627055168152 and batch: 250, loss is 3.9426391887664796 and perplexity is 51.55448392654531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8147327423095705 and perplexity of 123.31385133073246
Finished 15 epochs...
Completing Train Step...
At time: 123.87409996986389 and batch: 50, loss is 4.021315007209778 and perplexity is 55.77440136406829
At time: 125.3388741016388 and batch: 100, loss is 3.8989697980880735 and perplexity is 49.351580814811925
At time: 126.81071209907532 and batch: 150, loss is 3.910880799293518 and perplexity is 49.942922296725484
At time: 128.2903642654419 and batch: 200, loss is 3.8739381551742555 and perplexity is 48.13156289596004
At time: 129.7693042755127 and batch: 250, loss is 3.941237168312073 and perplexity is 51.48225413122075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.814450073242187 and perplexity of 123.27899924541667
Finished 16 epochs...
Completing Train Step...
At time: 131.9690442085266 and batch: 50, loss is 4.014802107810974 and perplexity is 55.41232865039671
At time: 133.4582154750824 and batch: 100, loss is 3.893798966407776 and perplexity is 49.0970507304983
At time: 134.93537402153015 and batch: 150, loss is 3.907101564407349 and perplexity is 49.75453247138942
At time: 136.42230224609375 and batch: 200, loss is 3.871960120201111 and perplexity is 48.03645107950076
At time: 137.89809608459473 and batch: 250, loss is 3.9393987464904785 and perplexity is 51.38769497823873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.814222717285157 and perplexity of 123.25097421651078
Finished 17 epochs...
Completing Train Step...
At time: 140.130473613739 and batch: 50, loss is 4.008887414932251 and perplexity is 55.0855490977409
At time: 141.6111352443695 and batch: 100, loss is 3.8890614223480227 and perplexity is 48.86500139544875
At time: 143.0928556919098 and batch: 150, loss is 3.903669304847717 and perplexity is 49.584054730964674
At time: 144.57477641105652 and batch: 200, loss is 3.869950923919678 and perplexity is 47.940033314161866
At time: 146.05482172966003 and batch: 250, loss is 3.9374166774749755 and perplexity is 51.28594189437386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.814306640625 and perplexity of 123.26131828395431
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 148.2838695049286 and batch: 50, loss is 4.0029952383041385 and perplexity is 54.76192966052022
At time: 149.75479578971863 and batch: 100, loss is 3.8807949352264406 and perplexity is 48.4627244899369
At time: 151.23411202430725 and batch: 150, loss is 3.8921902322769166 and perplexity is 49.01813012741824
At time: 152.7359869480133 and batch: 200, loss is 3.854695339202881 and perplexity is 47.21423042480563
At time: 154.24389600753784 and batch: 250, loss is 3.9241156339645387 and perplexity is 50.608301999143315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.813026046752929 and perplexity of 123.10357162135888
Finished 19 epochs...
Completing Train Step...
At time: 156.5076515674591 and batch: 50, loss is 4.001068916320801 and perplexity is 54.65654208935132
At time: 158.01414275169373 and batch: 100, loss is 3.879466094970703 and perplexity is 48.39836803992285
At time: 159.50836491584778 and batch: 150, loss is 3.891290225982666 and perplexity is 48.97403334843706
At time: 161.0032799243927 and batch: 200, loss is 3.8542609310150144 and perplexity is 47.19372463079016
At time: 162.4982144832611 and batch: 250, loss is 3.9241448879241942 and perplexity is 50.60978251402359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812704467773438 and perplexity of 123.06399046500016
Finished 20 epochs...
Completing Train Step...
At time: 164.72923278808594 and batch: 50, loss is 3.999808259010315 and perplexity is 54.587682333397524
At time: 166.20540976524353 and batch: 100, loss is 3.878578281402588 and perplexity is 48.35541838056866
At time: 167.68586039543152 and batch: 150, loss is 3.8906990909576415 and perplexity is 48.945091637080495
At time: 169.1607928276062 and batch: 200, loss is 3.8539874362945556 and perplexity is 47.1808191611342
At time: 170.64081501960754 and batch: 250, loss is 3.924057116508484 and perplexity is 50.60534061670221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812547302246093 and perplexity of 123.04465056786398
Finished 21 epochs...
Completing Train Step...
At time: 172.84655904769897 and batch: 50, loss is 3.9987275314331057 and perplexity is 54.5287197866879
At time: 174.32031202316284 and batch: 100, loss is 3.8778112173080443 and perplexity is 48.31834089757324
At time: 175.79624795913696 and batch: 150, loss is 3.890193157196045 and perplexity is 48.92033492591295
At time: 177.27357840538025 and batch: 200, loss is 3.8537370777130127 and perplexity is 47.16900851668277
At time: 178.7644817829132 and batch: 250, loss is 3.9239034128189085 and perplexity is 50.597562986877705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812462615966797 and perplexity of 123.0342308154313
Finished 22 epochs...
Completing Train Step...
At time: 180.98207330703735 and batch: 50, loss is 3.997747097015381 and perplexity is 54.47528415240249
At time: 182.4787561893463 and batch: 100, loss is 3.877107572555542 and perplexity is 48.28435390933631
At time: 183.9734652042389 and batch: 150, loss is 3.8897272396087645 and perplexity is 48.897547390464304
At time: 185.48031449317932 and batch: 200, loss is 3.8534888648986816 and perplexity is 47.15730201724133
At time: 186.9618535041809 and batch: 250, loss is 3.923706707954407 and perplexity is 50.58761117892286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812417602539062 and perplexity of 123.02869274761825
Finished 23 epochs...
Completing Train Step...
At time: 189.21845054626465 and batch: 50, loss is 3.996830382347107 and perplexity is 54.42536874295119
At time: 190.68815112113953 and batch: 100, loss is 3.87644567489624 and perplexity is 48.2524051830628
At time: 192.16655683517456 and batch: 150, loss is 3.889283146858215 and perplexity is 48.87583716518219
At time: 193.64408349990845 and batch: 200, loss is 3.8532356309890745 and perplexity is 47.14536170119525
At time: 195.13942551612854 and batch: 250, loss is 3.923479175567627 and perplexity is 50.576102168395806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812395858764648 and perplexity of 123.02601766855997
Finished 24 epochs...
Completing Train Step...
At time: 197.38599681854248 and batch: 50, loss is 3.9959586811065675 and perplexity is 54.37794675340755
At time: 198.88313055038452 and batch: 100, loss is 3.875811777114868 and perplexity is 48.22182778296833
At time: 200.36471796035767 and batch: 150, loss is 3.8888546657562255 and perplexity is 48.85489927867767
At time: 201.84769344329834 and batch: 200, loss is 3.8529744148254395 and perplexity is 47.13304817899401
At time: 203.3295488357544 and batch: 250, loss is 3.92322892665863 and perplexity is 50.56344713752673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812388610839844 and perplexity of 123.02512598846636
Finished 25 epochs...
Completing Train Step...
At time: 205.55922389030457 and batch: 50, loss is 3.9951205921173094 and perplexity is 54.332392286989844
At time: 207.0290641784668 and batch: 100, loss is 3.875198221206665 and perplexity is 48.192250070345786
At time: 208.51916193962097 and batch: 150, loss is 3.888437433242798 and perplexity is 48.83451967807
At time: 210.00773859024048 and batch: 200, loss is 3.8527075386047365 and perplexity is 47.12047116755307
At time: 211.50245475769043 and batch: 250, loss is 3.9229637956619263 and perplexity is 50.55004297739806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812390899658203 and perplexity of 123.02540757095557
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 213.7371323108673 and batch: 50, loss is 3.9941640090942383 and perplexity is 54.2804436934579
At time: 215.20961546897888 and batch: 100, loss is 3.873884072303772 and perplexity is 48.128959873267874
At time: 216.6920621395111 and batch: 150, loss is 3.8867306232452394 and perplexity is 48.75123952357531
At time: 218.17354726791382 and batch: 200, loss is 3.8504016256332396 and perplexity is 47.011940640929524
At time: 219.66172409057617 and batch: 250, loss is 3.9210169076919557 and perplexity is 50.4517234464555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.81231689453125 and perplexity of 123.01630339693179
Finished 27 epochs...
Completing Train Step...
At time: 221.8748857975006 and batch: 50, loss is 3.9940138959884646 and perplexity is 54.27229609901795
At time: 223.36295652389526 and batch: 100, loss is 3.873776259422302 and perplexity is 48.12377123112804
At time: 224.8437101840973 and batch: 150, loss is 3.8866380262374878 and perplexity is 48.74672551366637
At time: 226.3253481388092 and batch: 200, loss is 3.850369930267334 and perplexity is 47.01045060388274
At time: 227.80406856536865 and batch: 250, loss is 3.921002993583679 and perplexity is 50.45102146059649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812288284301758 and perplexity of 123.01278392260704
Finished 28 epochs...
Completing Train Step...
At time: 230.017972946167 and batch: 50, loss is 3.9938805294036865 and perplexity is 54.26505847087876
At time: 231.48929953575134 and batch: 100, loss is 3.8736839199066164 and perplexity is 48.11932771055901
At time: 232.97668361663818 and batch: 150, loss is 3.8865595149993895 and perplexity is 48.742898498126884
At time: 234.458571434021 and batch: 200, loss is 3.8503374910354613 and perplexity is 47.00892564570953
At time: 235.94315218925476 and batch: 250, loss is 3.920985765457153 and perplexity is 50.45015229150251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812268829345703 and perplexity of 123.01039073758136
Finished 29 epochs...
Completing Train Step...
At time: 238.16023063659668 and batch: 50, loss is 3.9937538433074953 and perplexity is 54.25818427790303
At time: 239.65043568611145 and batch: 100, loss is 3.873595747947693 and perplexity is 48.115085122214104
At time: 241.12860536575317 and batch: 150, loss is 3.8864863681793214 and perplexity is 48.73933324049606
At time: 242.61014771461487 and batch: 200, loss is 3.850304193496704 and perplexity is 47.00736039024562
At time: 244.09380984306335 and batch: 250, loss is 3.920965299606323 and perplexity is 50.449119796776834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812255859375 and perplexity of 123.00879530676372
Finished 30 epochs...
Completing Train Step...
At time: 246.32375049591064 and batch: 50, loss is 3.9936313819885254 and perplexity is 54.25154015592371
At time: 247.81954383850098 and batch: 100, loss is 3.8735095024108888 and perplexity is 48.110935589811255
At time: 249.30090737342834 and batch: 150, loss is 3.8864159774780274 and perplexity is 48.73590256539389
At time: 250.78020787239075 and batch: 200, loss is 3.850270094871521 and perplexity is 47.005757531210634
At time: 252.26185369491577 and batch: 250, loss is 3.920942225456238 and perplexity is 50.44795573964484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812245559692383 and perplexity of 123.00752836173754
Finished 31 epochs...
Completing Train Step...
At time: 254.47934460639954 and batch: 50, loss is 3.9935119056701662 and perplexity is 54.24505876883441
At time: 255.9585256576538 and batch: 100, loss is 3.8734243440628053 and perplexity is 48.106838716455634
At time: 257.4370620250702 and batch: 150, loss is 3.886347212791443 and perplexity is 48.732551371551786
At time: 258.91680669784546 and batch: 200, loss is 3.8502355813980103 and perplexity is 47.00413522723906
At time: 260.3971035480499 and batch: 250, loss is 3.920917205810547 and perplexity is 50.44669356545605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812237930297852 and perplexity of 123.00658989235333
Finished 32 epochs...
Completing Train Step...
At time: 262.61652851104736 and batch: 50, loss is 3.993394446372986 and perplexity is 54.23868755654231
At time: 264.1157956123352 and batch: 100, loss is 3.873340201377869 and perplexity is 48.102791048175455
At time: 265.60043382644653 and batch: 150, loss is 3.886279821395874 and perplexity is 48.72926732756463
At time: 267.0865740776062 and batch: 200, loss is 3.850200457572937 and perplexity is 47.002484291209385
At time: 268.57943773269653 and batch: 250, loss is 3.920890784263611 and perplexity is 50.445360703382455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8122314453125 and perplexity of 123.00579219900628
Finished 33 epochs...
Completing Train Step...
At time: 270.81577587127686 and batch: 50, loss is 3.99327853679657 and perplexity is 54.23240113757739
At time: 272.29861879348755 and batch: 100, loss is 3.8732566928863523 and perplexity is 48.09877422437907
At time: 273.7799472808838 and batch: 150, loss is 3.886213297843933 and perplexity is 48.726025791438964
At time: 275.26233434677124 and batch: 200, loss is 3.850164966583252 and perplexity is 47.0008161561263
At time: 276.7485327720642 and batch: 250, loss is 3.9208628797531127 and perplexity is 50.44395306992487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812227249145508 and perplexity of 123.00527604724408
Finished 34 epochs...
Completing Train Step...
At time: 278.9642400741577 and batch: 50, loss is 3.9931640625 and perplexity is 54.22619327693288
At time: 280.4387652873993 and batch: 100, loss is 3.873173851966858 and perplexity is 48.094789842732965
At time: 281.91732454299927 and batch: 150, loss is 3.8861474084854124 and perplexity is 48.72281537062374
At time: 283.39815759658813 and batch: 200, loss is 3.8501291275024414 and perplexity is 46.999131720262405
At time: 284.8812825679779 and batch: 250, loss is 3.920834016799927 and perplexity is 50.44249712948039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812222671508789 and perplexity of 123.00471297506465
Finished 35 epochs...
Completing Train Step...
At time: 287.09928154945374 and batch: 50, loss is 3.9930506801605223 and perplexity is 54.220045332818906
At time: 288.58531641960144 and batch: 100, loss is 3.8730915546417237 and perplexity is 48.09083193304094
At time: 290.06573033332825 and batch: 150, loss is 3.886082091331482 and perplexity is 48.71963303892382
At time: 291.54667258262634 and batch: 200, loss is 3.8500928592681887 and perplexity is 46.9974271756541
At time: 293.0246558189392 and batch: 250, loss is 3.920804352760315 and perplexity is 50.44100082344077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.81221923828125 and perplexity of 123.0042906726215
Finished 36 epochs...
Completing Train Step...
At time: 295.248982667923 and batch: 50, loss is 3.9929384088516233 and perplexity is 54.213958319065725
At time: 296.736389875412 and batch: 100, loss is 3.873009819984436 and perplexity is 48.08690140600655
At time: 298.2166199684143 and batch: 150, loss is 3.886017050743103 and perplexity is 48.71646438837191
At time: 299.6958067417145 and batch: 200, loss is 3.850056219100952 and perplexity is 46.99570521360937
At time: 301.1825256347656 and batch: 250, loss is 3.9207740020751953 and perplexity is 50.43946992773963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812217330932617 and perplexity of 123.00405606077959
Finished 37 epochs...
Completing Train Step...
At time: 303.4071617126465 and batch: 50, loss is 3.992826862335205 and perplexity is 54.20791127814343
At time: 304.8948359489441 and batch: 100, loss is 3.8729284524917604 and perplexity is 48.082988854588
At time: 306.37253069877625 and batch: 150, loss is 3.8859526395797728 and perplexity is 48.713326605282546
At time: 307.85081577301025 and batch: 200, loss is 3.8500193309783937 and perplexity is 46.99397166224966
At time: 309.3266215324402 and batch: 250, loss is 3.9207427072525025 and perplexity is 50.43789145817061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812214660644531 and perplexity of 123.00372760495273
Finished 38 epochs...
Completing Train Step...
At time: 311.53436279296875 and batch: 50, loss is 3.9927162075042726 and perplexity is 54.20191324274766
At time: 313.02521777153015 and batch: 100, loss is 3.8728475379943847 and perplexity is 48.07909840111168
At time: 314.49514985084534 and batch: 150, loss is 3.885888419151306 and perplexity is 48.710198315027064
At time: 315.9648485183716 and batch: 200, loss is 3.84998206615448 and perplexity is 46.992220472799744
At time: 317.44695019721985 and batch: 250, loss is 3.920710997581482 and perplexity is 50.43629211458297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812213134765625 and perplexity of 123.00353991630263
Finished 39 epochs...
Completing Train Step...
At time: 319.6915714740753 and batch: 50, loss is 3.9926062631607055 and perplexity is 54.19595437655393
At time: 321.18348932266235 and batch: 100, loss is 3.8727671766281127 and perplexity is 48.07523485431707
At time: 322.7010066509247 and batch: 150, loss is 3.8858245420455932 and perplexity is 48.707086947913616
At time: 324.19495391845703 and batch: 200, loss is 3.8499446535110473 and perplexity is 46.99046240249832
At time: 325.68898725509644 and batch: 250, loss is 3.920678873062134 and perplexity is 50.43467189896555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812211608886718 and perplexity of 123.0033522279388
Finished 40 epochs...
Completing Train Step...
At time: 327.90100049972534 and batch: 50, loss is 3.9924969148635863 and perplexity is 54.19002846523216
At time: 329.3827667236328 and batch: 100, loss is 3.8726869297027586 and perplexity is 48.071377119322094
At time: 330.851931810379 and batch: 150, loss is 3.885760941505432 and perplexity is 48.70398924938275
At time: 332.32872343063354 and batch: 200, loss is 3.8499068450927734 and perplexity is 46.988685801026385
At time: 333.8035731315613 and batch: 250, loss is 3.9206461334228515 and perplexity is 50.43302071303002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812211608886718 and perplexity of 123.0033522279388
Finished 41 epochs...
Completing Train Step...
At time: 336.0391061306 and batch: 50, loss is 3.992388186454773 and perplexity is 54.18413678996563
At time: 337.5048406124115 and batch: 100, loss is 3.8726071453094484 and perplexity is 48.06754192665933
At time: 338.97715067863464 and batch: 150, loss is 3.8856974267959594 and perplexity is 48.70089592789216
At time: 340.45887756347656 and batch: 200, loss is 3.8498689222335813 and perplexity is 46.98690388949893
At time: 341.9388861656189 and batch: 250, loss is 3.9206130743026733 and perplexity is 50.43135346929627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812211227416992 and perplexity of 123.00330530589267
Finished 42 epochs...
Completing Train Step...
At time: 344.15121507644653 and batch: 50, loss is 3.9922800207138063 and perplexity is 54.1782762396222
At time: 345.6208972930908 and batch: 100, loss is 3.8725276041030883 and perplexity is 48.063718728440634
At time: 347.09879660606384 and batch: 150, loss is 3.885634083747864 and perplexity is 48.697811162399354
At time: 348.58293294906616 and batch: 200, loss is 3.8498308658599854 and perplexity is 46.98511577235524
At time: 350.07518458366394 and batch: 250, loss is 3.920579586029053 and perplexity is 50.42966463861042
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8122108459472654 and perplexity of 123.00325838386433
Finished 43 epochs...
Completing Train Step...
At time: 352.29526114463806 and batch: 50, loss is 3.9921724462509154 and perplexity is 54.172448354126814
At time: 353.7903354167938 and batch: 100, loss is 3.8724484252929687 and perplexity is 48.05991325104037
At time: 355.2668368816376 and batch: 150, loss is 3.885571069717407 and perplexity is 48.69474261372542
At time: 356.7376754283905 and batch: 200, loss is 3.8497924184799195 and perplexity is 46.98330935247798
At time: 358.2102224826813 and batch: 250, loss is 3.920545778274536 and perplexity is 50.42795975370728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812211608886718 and perplexity of 123.0033522279388
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 360.43712973594666 and batch: 50, loss is 3.9920401763916016 and perplexity is 54.165283445865484
At time: 361.9106481075287 and batch: 100, loss is 3.8722566509246827 and perplexity is 48.05069747523981
At time: 363.3959789276123 and batch: 150, loss is 3.8853225088119507 and perplexity is 48.68264050852781
At time: 364.87888264656067 and batch: 200, loss is 3.849472074508667 and perplexity is 46.96826094303972
At time: 366.3589942455292 and batch: 250, loss is 3.920288095474243 and perplexity is 50.41496700990007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8122100830078125 and perplexity of 123.00316453986147
Finished 45 epochs...
Completing Train Step...
At time: 368.57502269744873 and batch: 50, loss is 3.992025122642517 and perplexity is 54.1644680614167
At time: 370.05877685546875 and batch: 100, loss is 3.872246336936951 and perplexity is 48.05020188349131
At time: 371.53923869132996 and batch: 150, loss is 3.885314440727234 and perplexity is 48.682247734444424
At time: 373.0163469314575 and batch: 200, loss is 3.849468011856079 and perplexity is 46.96807012770047
At time: 374.49421858787537 and batch: 250, loss is 3.9202847051620484 and perplexity is 50.41479608771237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812207794189453 and perplexity of 123.00288300828235
Finished 46 epochs...
Completing Train Step...
At time: 376.7137987613678 and batch: 50, loss is 3.9920103788375854 and perplexity is 54.16366947695248
At time: 378.2021048069 and batch: 100, loss is 3.872236189842224 and perplexity is 48.049714316014864
At time: 379.67637610435486 and batch: 150, loss is 3.8853062438964843 and perplexity is 48.681848695934654
At time: 381.1602737903595 and batch: 200, loss is 3.84946373462677 and perplexity is 46.96786923492397
At time: 382.6440815925598 and batch: 250, loss is 3.9202812194824217 and perplexity is 50.41462035819103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812207794189453 and perplexity of 123.00288300828235
Finished 47 epochs...
Completing Train Step...
At time: 384.85005807876587 and batch: 50, loss is 3.9919957733154297 and perplexity is 54.16287839405501
At time: 386.32481360435486 and batch: 100, loss is 3.8722261095047 and perplexity is 48.049229961117845
At time: 387.81090474128723 and batch: 150, loss is 3.8852981758117675 and perplexity is 48.68145592823965
At time: 389.2923631668091 and batch: 200, loss is 3.8494595193862917 and perplexity is 46.96767125447765
At time: 390.77482056617737 and batch: 250, loss is 3.920277714729309 and perplexity is 50.41444366770303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812206268310547 and perplexity of 123.002695320921
Finished 48 epochs...
Completing Train Step...
At time: 392.9880225658417 and batch: 50, loss is 3.991981334686279 and perplexity is 54.162096361985895
At time: 394.4868583679199 and batch: 100, loss is 3.8722159910202025 and perplexity is 48.0487437781891
At time: 395.9575369358063 and batch: 150, loss is 3.88529016494751 and perplexity is 48.681065949266376
At time: 397.4413843154907 and batch: 200, loss is 3.849455323219299 and perplexity is 46.96747417069932
At time: 398.9227011203766 and batch: 250, loss is 3.9202741956710816 and perplexity is 50.414266256652425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812204742431641 and perplexity of 123.00250763384604
Finished 49 epochs...
Completing Train Step...
At time: 401.14250326156616 and batch: 50, loss is 3.9919668436050415 and perplexity is 54.16131150033428
At time: 402.61055302619934 and batch: 100, loss is 3.8722058391571044 and perplexity is 48.04825599639617
At time: 404.08196568489075 and batch: 150, loss is 3.885282096862793 and perplexity is 48.68067318788661
At time: 405.5578896999359 and batch: 200, loss is 3.8494509935379027 and perplexity is 46.967270816940385
At time: 407.0363426208496 and batch: 250, loss is 3.920270733833313 and perplexity is 50.41409173094352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812204360961914 and perplexity of 123.00246071212199
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 8.0, 'num_layers': 1, 'dropout': 0.0, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 0.0, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5970776081085205 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 2.992501735687256 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 4.377553224563599 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 5.776751518249512 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 7.179178237915039 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 1 epochs...
Completing Train Step...
At time: 9.288177013397217 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 10.67753529548645 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 12.063369274139404 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 13.451702117919922 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 14.840234279632568 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 2 epochs...
Completing Train Step...
At time: 16.981439113616943 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 18.45395040512085 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 19.938934326171875 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 21.42489266395569 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 22.92858123779297 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 3 epochs...
Completing Train Step...
At time: 25.16953206062317 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 26.68198299407959 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 28.19318985939026 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 29.701833248138428 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 31.19968891143799 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 4 epochs...
Completing Train Step...
At time: 33.45033025741577 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 34.9460175037384 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 36.43581032752991 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 37.9234299659729 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 39.41241383552551 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 5 epochs...
Completing Train Step...
At time: 41.603010177612305 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 43.073450803756714 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 44.53168344497681 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 45.98908591270447 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 47.454331159591675 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 6 epochs...
Completing Train Step...
At time: 49.65085220336914 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 51.12720489501953 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 52.59770369529724 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 54.07481908798218 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 55.55078959465027 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 7 epochs...
Completing Train Step...
At time: 57.766889333724976 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 59.23208045959473 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 60.70240664482117 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 62.18468451499939 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 63.685534954071045 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 8 epochs...
Completing Train Step...
At time: 65.90262866020203 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 67.39580798149109 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 68.87322330474854 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 70.35580587387085 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 71.83628225326538 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 9 epochs...
Completing Train Step...
At time: 74.0409643650055 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 75.5042016506195 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 76.97962546348572 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 78.46967196464539 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 79.96883797645569 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 10 epochs...
Completing Train Step...
At time: 82.20290923118591 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 83.68101525306702 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 85.15985679626465 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 86.63537621498108 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 88.11229109764099 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 11 epochs...
Completing Train Step...
At time: 90.310875415802 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 91.78258657455444 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 93.25625729560852 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 94.738440990448 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 96.22075748443604 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 12 epochs...
Completing Train Step...
At time: 98.43382000923157 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 99.89902687072754 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 101.3807864189148 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 102.86527395248413 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 104.36827945709229 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 13 epochs...
Completing Train Step...
At time: 106.59933614730835 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 108.10523629188538 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 109.59164333343506 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 111.08122205734253 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 112.5655791759491 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 14 epochs...
Completing Train Step...
At time: 114.78419613838196 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 116.27147102355957 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 117.74558305740356 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 119.2255048751831 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 120.7028157711029 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 15 epochs...
Completing Train Step...
At time: 122.9059112071991 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 124.36567449569702 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 125.82472491264343 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 127.28484010696411 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 128.75423431396484 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 16 epochs...
Completing Train Step...
At time: 130.95191049575806 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 132.4292607307434 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 133.89752554893494 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 135.37003016471863 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 136.84533143043518 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 17 epochs...
Completing Train Step...
At time: 139.05477261543274 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 140.51185631752014 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 141.98198294639587 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 143.45812273025513 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 144.94151854515076 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 18 epochs...
Completing Train Step...
At time: 147.16016840934753 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 148.63023972511292 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 150.1052098274231 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 151.59097814559937 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 153.07537007331848 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 19 epochs...
Completing Train Step...
At time: 155.27916145324707 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 156.76279950141907 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 158.23995757102966 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 159.71362733840942 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 161.18164157867432 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 20 epochs...
Completing Train Step...
At time: 163.38747692108154 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 164.85590863227844 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 166.3281819820404 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 167.8120152950287 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 169.2993814945221 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 21 epochs...
Completing Train Step...
At time: 171.5232334136963 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 173.00974011421204 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 174.48308944702148 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 175.96141076087952 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 177.4324381351471 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 22 epochs...
Completing Train Step...
At time: 179.6233308315277 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 181.09600591659546 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 182.55953907966614 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 184.0270059108734 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 185.49830389022827 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 23 epochs...
Completing Train Step...
At time: 187.7074408531189 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 189.17323064804077 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 190.64601826667786 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 192.12064504623413 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 193.5936462879181 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 24 epochs...
Completing Train Step...
At time: 195.79091262817383 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 197.27194476127625 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 198.74436140060425 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 200.21667313575745 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 201.69087290763855 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 25 epochs...
Completing Train Step...
At time: 203.9096040725708 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 205.38454222679138 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 206.85947561264038 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 208.33751773834229 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 209.81853127479553 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 26 epochs...
Completing Train Step...
At time: 212.03382897377014 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 213.50056290626526 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 214.97536277770996 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 216.45098686218262 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 217.93340134620667 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 27 epochs...
Completing Train Step...
At time: 220.14991903305054 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 221.63602137565613 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 223.11058735847473 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 224.58657026290894 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 226.0651195049286 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 28 epochs...
Completing Train Step...
At time: 228.28322529792786 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 229.75715494155884 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 231.22216391563416 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 232.68338871002197 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 234.14706563949585 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 29 epochs...
Completing Train Step...
At time: 236.33791303634644 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 237.81658601760864 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 239.29281902313232 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 240.77990889549255 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 242.26518750190735 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 30 epochs...
Completing Train Step...
At time: 244.48313927650452 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 245.97068238258362 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 247.4473898410797 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 248.93950700759888 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 250.41786575317383 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 31 epochs...
Completing Train Step...
At time: 252.62688732147217 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 254.09118485450745 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 255.56322693824768 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 257.03706669807434 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 258.50515842437744 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 32 epochs...
Completing Train Step...
At time: 260.71642303466797 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 262.20619344711304 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 263.67446064949036 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 265.14795112609863 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 266.6225972175598 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 33 epochs...
Completing Train Step...
At time: 268.8467597961426 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 270.31453561782837 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 271.7897901535034 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 273.26651906967163 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 274.7472577095032 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 34 epochs...
Completing Train Step...
At time: 276.97250151634216 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 278.4375548362732 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 279.90997886657715 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 281.38626980781555 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 282.8664674758911 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 35 epochs...
Completing Train Step...
At time: 285.0856075286865 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 286.55753111839294 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 288.018132686615 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 289.5015347003937 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 290.99586272239685 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 36 epochs...
Completing Train Step...
At time: 293.21416187286377 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 294.6896686553955 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 296.1524465084076 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 297.6216082572937 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 299.0976083278656 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 37 epochs...
Completing Train Step...
At time: 301.30507588386536 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 302.7816243171692 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 304.25113916397095 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 305.7225182056427 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 307.1974549293518 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 38 epochs...
Completing Train Step...
At time: 309.40892219543457 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 310.8879351615906 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 312.350900888443 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 313.8146872520447 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 315.28720116615295 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 39 epochs...
Completing Train Step...
At time: 317.49769163131714 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 318.96403789520264 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 320.43090629577637 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 321.9000334739685 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 323.4077353477478 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 40 epochs...
Completing Train Step...
At time: 325.6250731945038 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 327.1134259700775 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 328.58855724334717 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 330.0642852783203 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 331.5408387184143 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 41 epochs...
Completing Train Step...
At time: 333.77350330352783 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 335.26435470581055 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 336.75233578681946 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 338.219851732254 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 339.69790172576904 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 42 epochs...
Completing Train Step...
At time: 341.91783452033997 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 343.37307238578796 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 344.825635433197 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 346.29015350341797 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 347.76453495025635 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 43 epochs...
Completing Train Step...
At time: 349.97293281555176 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 351.45209765434265 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 352.91444754600525 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 354.3813581466675 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 355.86394810676575 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 44 epochs...
Completing Train Step...
At time: 358.10455298423767 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 359.5794925689697 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 361.0786716938019 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 362.548743724823 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 364.0194602012634 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 45 epochs...
Completing Train Step...
At time: 366.2139608860016 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 367.6853382587433 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 369.14378118515015 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 370.6020655632019 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 372.0685703754425 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 46 epochs...
Completing Train Step...
At time: 374.27449131011963 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 375.7544119358063 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 377.2277743816376 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 378.69467091560364 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 380.172758102417 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 47 epochs...
Completing Train Step...
At time: 382.38514614105225 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 383.85754013061523 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 385.3338108062744 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 386.8131926059723 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 388.2850818634033 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 48 epochs...
Completing Train Step...
At time: 390.48783564567566 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 391.96593832969666 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 393.4318072795868 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 394.91038846969604 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 396.3865225315094 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished 49 epochs...
Completing Train Step...
At time: 398.6098461151123 and batch: 50, loss is 9.175914688110351 and perplexity is 9661.601383050274
At time: 400.0752146244049 and batch: 100, loss is 9.174962749481201 and perplexity is 9652.408507695087
At time: 401.54672288894653 and batch: 150, loss is 9.176832332611085 and perplexity is 9670.471367551525
At time: 403.0227744579315 and batch: 200, loss is 9.176260261535644 and perplexity is 9664.940752699496
At time: 404.5002191066742 and batch: 250, loss is 9.176405582427979 and perplexity is 9666.345372571875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.63385467529297 and perplexity of 15273.19599542868
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd3f3f27898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 5.3286642434072204, 'num_layers': 1, 'dropout': 0.1607544487575544, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 4.514278932000416, 'tune_wordvecs': True}, 'best_accuracy': -100.81467398478323}, {'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 7.683260297614677, 'num_layers': 1, 'dropout': 0.8573763535128364, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 27.18775406341022, 'tune_wordvecs': True}, 'best_accuracy': -156.1379718447254}, {'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 2.778489473231498, 'num_layers': 1, 'dropout': 0.24759354303778403, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 4.054933983154826, 'tune_wordvecs': True}, 'best_accuracy': -103.65727365710157}, {'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 3.1067821438718655, 'num_layers': 1, 'dropout': 0.6823422046714526, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 15.94026313349056, 'tune_wordvecs': True}, 'best_accuracy': -135.01516635226727}, {'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 7.634060454147596, 'num_layers': 1, 'dropout': 0.9141402102640862, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 12.297035776623481, 'tune_wordvecs': True}, 'best_accuracy': -123.00246071212199}, {'params': {'batch_size': 80, 'seq_len': 50, 'data': 'ptb', 'anneal': 8.0, 'num_layers': 1, 'dropout': 0.0, 'wordvec_dim': 200, 'wordvec_source': '', 'lr': 0.0, 'tune_wordvecs': True}, 'best_accuracy': -15273.19599542868}]
