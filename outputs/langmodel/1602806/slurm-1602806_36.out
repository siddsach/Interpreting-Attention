Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'dropout': 0.6280757312144768, 'tune_wordvecs': True, 'lr': 1.0233030793280484, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.650782445341421, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5211925506591797 and batch: 50, loss is 7.387031850814819 and perplexity is 1614.9057112328144
At time: 2.470456838607788 and batch: 100, loss is 6.160534524917603 and perplexity is 473.6812015827769
At time: 3.422226905822754 and batch: 150, loss is 5.940760498046875 and perplexity is 380.22397920691833
At time: 4.38151741027832 and batch: 200, loss is 5.842560424804687 and perplexity is 344.6606896485499
At time: 5.330859899520874 and batch: 250, loss is 5.797697086334228 and perplexity is 329.5397837245978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.831894302368164 and perplexity of 341.0040323777064
Finished 1 epochs...
Completing Train Step...
At time: 6.9037628173828125 and batch: 50, loss is 5.498865966796875 and perplexity is 244.41460076977881
At time: 7.841808795928955 and batch: 100, loss is 5.364315328598022 and perplexity is 213.64490803511154
At time: 8.789244890213013 and batch: 150, loss is 5.290914173126221 and perplexity is 198.52482854295448
At time: 9.730005264282227 and batch: 200, loss is 5.218671588897705 and perplexity is 184.6886785496032
At time: 10.670609951019287 and batch: 250, loss is 5.210524873733521 and perplexity is 183.19018468055492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.39396858215332 and perplexity of 220.07504067126135
Finished 2 epochs...
Completing Train Step...
At time: 12.238218784332275 and batch: 50, loss is 5.125559282302857 and perplexity is 168.26822478013133
At time: 13.180923461914062 and batch: 100, loss is 5.043018999099732 and perplexity is 154.93706451845048
At time: 14.12370252609253 and batch: 150, loss is 5.0307728290557865 and perplexity is 153.05124945393112
At time: 15.069112300872803 and batch: 200, loss is 4.987575340270996 and perplexity is 146.58058423581343
At time: 16.012765645980835 and batch: 250, loss is 5.009136028289795 and perplexity is 149.7752786296582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.200435638427734 and perplexity of 181.35122823309078
Finished 3 epochs...
Completing Train Step...
At time: 17.589574337005615 and batch: 50, loss is 4.94327527999878 and perplexity is 140.22878677177894
At time: 18.552223920822144 and batch: 100, loss is 4.8658139038085935 and perplexity is 129.7765212438337
At time: 19.513853311538696 and batch: 150, loss is 4.878483057022095 and perplexity is 131.43113904673422
At time: 20.455087661743164 and batch: 200, loss is 4.843385677337647 and perplexity is 126.89826181404032
At time: 21.40315890312195 and batch: 250, loss is 4.877098731994629 and perplexity is 131.24932150788376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.082723236083984 and perplexity of 161.21247833198944
Finished 4 epochs...
Completing Train Step...
At time: 22.956637859344482 and batch: 50, loss is 4.818722047805786 and perplexity is 123.80677050447444
At time: 23.914640188217163 and batch: 100, loss is 4.743183326721192 and perplexity is 114.79906356136064
At time: 24.863143920898438 and batch: 150, loss is 4.769899196624756 and perplexity is 117.90735590216231
At time: 25.803802490234375 and batch: 200, loss is 4.739987087249756 and perplexity is 114.4327240294333
At time: 26.749074935913086 and batch: 250, loss is 4.779029302597046 and perplexity is 118.98879185663512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.999491882324219 and perplexity of 148.3377669087933
Finished 5 epochs...
Completing Train Step...
At time: 28.321046829223633 and batch: 50, loss is 4.725490131378174 and perplexity is 112.78576467136632
At time: 29.259503602981567 and batch: 100, loss is 4.650524291992188 and perplexity is 104.63983302433763
At time: 30.205158233642578 and batch: 150, loss is 4.686741580963135 and perplexity is 108.49906777011923
At time: 31.14791464805603 and batch: 200, loss is 4.661072874069214 and perplexity is 105.7494771883963
At time: 32.08889627456665 and batch: 250, loss is 4.702706203460694 and perplexity is 110.24511483522839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.941828918457031 and perplexity of 140.02611185348223
Finished 6 epochs...
Completing Train Step...
At time: 33.662362813949585 and batch: 50, loss is 4.651539916992188 and perplexity is 104.74616184071525
At time: 34.62543869018555 and batch: 100, loss is 4.575663251876831 and perplexity is 97.09241451218045
At time: 35.569774866104126 and batch: 150, loss is 4.619645290374756 and perplexity is 101.45803760362419
At time: 36.510780334472656 and batch: 200, loss is 4.596726770401001 and perplexity is 99.1592129934077
At time: 37.45509076118469 and batch: 250, loss is 4.640899753570556 and perplexity is 103.63755390480233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.899646377563476 and perplexity of 134.24230020124318
Finished 7 epochs...
Completing Train Step...
At time: 39.03381133079529 and batch: 50, loss is 4.589903383255005 and perplexity is 98.48491441040183
At time: 39.9918692111969 and batch: 100, loss is 4.514877300262452 and perplexity is 91.36635470684631
At time: 40.93798804283142 and batch: 150, loss is 4.5637931060791015 and perplexity is 95.94672658969517
At time: 41.88190007209778 and batch: 200, loss is 4.542182607650757 and perplexity is 93.8955137000664
At time: 42.824721813201904 and batch: 250, loss is 4.588328342437745 and perplexity is 98.32991874461325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.863725280761718 and perplexity of 129.5057498787106
Finished 8 epochs...
Completing Train Step...
At time: 44.437504053115845 and batch: 50, loss is 4.538134546279907 and perplexity is 93.51618718466356
At time: 45.38322687149048 and batch: 100, loss is 4.463476600646973 and perplexity is 86.78871490918743
At time: 46.33333659172058 and batch: 150, loss is 4.516295976638794 and perplexity is 91.49606598326805
At time: 47.28179693222046 and batch: 200, loss is 4.495145454406738 and perplexity is 89.58119801756987
At time: 48.23181891441345 and batch: 250, loss is 4.542305936813355 and perplexity is 93.90709446925133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.833014297485351 and perplexity of 125.5889531433062
Finished 9 epochs...
Completing Train Step...
At time: 49.81683087348938 and batch: 50, loss is 4.493777217864991 and perplexity is 89.45871356195101
At time: 50.761075258255005 and batch: 100, loss is 4.418139877319336 and perplexity is 82.94185974309129
At time: 51.707359075546265 and batch: 150, loss is 4.474592370986938 and perplexity is 87.75882007648404
At time: 52.65321326255798 and batch: 200, loss is 4.4536374568939205 and perplexity is 85.93897547644224
At time: 53.599207639694214 and batch: 250, loss is 4.500848731994629 and perplexity is 90.09356415083585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.805325698852539 and perplexity of 122.1592716771857
Finished 10 epochs...
Completing Train Step...
At time: 55.159650802612305 and batch: 50, loss is 4.4536417293548585 and perplexity is 85.93934264814237
At time: 56.124367475509644 and batch: 100, loss is 4.377935247421265 and perplexity is 79.67335768554234
At time: 57.06765413284302 and batch: 150, loss is 4.436821870803833 and perplexity is 84.50594363826427
At time: 58.01443958282471 and batch: 200, loss is 4.416233949661255 and perplexity is 82.78392910858409
At time: 58.95456790924072 and batch: 250, loss is 4.463301639556885 and perplexity is 86.77353158930343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.781848907470703 and perplexity of 119.32476666958662
Finished 11 epochs...
Completing Train Step...
At time: 60.53401327133179 and batch: 50, loss is 4.417123689651489 and perplexity is 82.85761805800205
At time: 61.47945475578308 and batch: 100, loss is 4.341483945846558 and perplexity is 76.82145367314418
At time: 62.44124746322632 and batch: 150, loss is 4.402352151870727 and perplexity is 81.64267897312392
At time: 63.387553691864014 and batch: 200, loss is 4.382103786468506 and perplexity is 80.00617238167206
At time: 64.3330557346344 and batch: 250, loss is 4.42907753944397 and perplexity is 83.85402918780578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.760580062866211 and perplexity of 116.81366552010535
Finished 12 epochs...
Completing Train Step...
At time: 65.90264344215393 and batch: 50, loss is 4.383632459640503 and perplexity is 80.1285691995168
At time: 66.8609688282013 and batch: 100, loss is 4.307309160232544 and perplexity is 74.24045076842988
At time: 67.80584836006165 and batch: 150, loss is 4.370837574005127 and perplexity is 79.10986432423717
At time: 68.75090551376343 and batch: 200, loss is 4.351069602966309 and perplexity is 77.5613784544738
At time: 69.69784688949585 and batch: 250, loss is 4.397656440734863 and perplexity is 81.26020722750769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.743218994140625 and perplexity of 114.80315822073374
Finished 13 epochs...
Completing Train Step...
At time: 71.25726389884949 and batch: 50, loss is 4.352866735458374 and perplexity is 77.70089185230516
At time: 72.21995997428894 and batch: 100, loss is 4.275194005966187 and perplexity is 71.89408583834658
At time: 73.16796779632568 and batch: 150, loss is 4.341837539672851 and perplexity is 76.84862206789515
At time: 74.11544466018677 and batch: 200, loss is 4.322391090393066 and perplexity is 75.36862620868591
At time: 75.05712056159973 and batch: 250, loss is 4.36883918762207 and perplexity is 78.9519301079596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.727103805541992 and perplexity of 112.96791106880583
Finished 14 epochs...
Completing Train Step...
At time: 76.63716173171997 and batch: 50, loss is 4.324491882324219 and perplexity is 75.52712644019816
At time: 77.58332514762878 and batch: 100, loss is 4.245377826690674 and perplexity is 69.78212071146318
At time: 78.53248572349548 and batch: 150, loss is 4.315110397338867 and perplexity is 74.82188312479718
At time: 79.47895741462708 and batch: 200, loss is 4.295284242630005 and perplexity is 73.35306154919945
At time: 80.42895746231079 and batch: 250, loss is 4.342207040786743 and perplexity is 76.87702296610891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7142997741699215 and perplexity of 111.53068715667932
Finished 15 epochs...
Completing Train Step...
At time: 81.98932027816772 and batch: 50, loss is 4.297875204086304 and perplexity is 73.5433629296869
At time: 82.94991660118103 and batch: 100, loss is 4.217884864807129 and perplexity is 67.88973634926062
At time: 83.89465022087097 and batch: 150, loss is 4.290177793502807 and perplexity is 72.97944261697754
At time: 84.84330177307129 and batch: 200, loss is 4.269960165023804 and perplexity is 71.51878661331223
At time: 85.78404474258423 and batch: 250, loss is 4.316953601837159 and perplexity is 74.95992233450835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.702568817138672 and perplexity of 110.229969704771
Finished 16 epochs...
Completing Train Step...
At time: 87.39046263694763 and batch: 50, loss is 4.272903900146485 and perplexity is 71.72962915744263
At time: 88.33083844184875 and batch: 100, loss is 4.192229108810425 and perplexity is 66.1701270985619
At time: 89.27780532836914 and batch: 150, loss is 4.2667083835601805 and perplexity is 71.28660086195933
At time: 90.22511601448059 and batch: 200, loss is 4.246354818344116 and perplexity is 69.85033057576625
At time: 91.16724252700806 and batch: 250, loss is 4.2929847240448 and perplexity is 73.18457860995224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.691373825073242 and perplexity of 109.00282780668725
Finished 17 epochs...
Completing Train Step...
At time: 92.74602198600769 and batch: 50, loss is 4.249495496749878 and perplexity is 70.07005285854615
At time: 93.69131779670715 and batch: 100, loss is 4.168159222602844 and perplexity is 64.59643494157845
At time: 94.64052391052246 and batch: 150, loss is 4.244349069595337 and perplexity is 69.71036877363937
At time: 95.58054542541504 and batch: 200, loss is 4.224305639266968 and perplexity is 68.32704345727215
At time: 96.52449536323547 and batch: 250, loss is 4.270442886352539 and perplexity is 71.55331859100627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.680551147460937 and perplexity of 107.82948614457925
Finished 18 epochs...
Completing Train Step...
At time: 98.08192944526672 and batch: 50, loss is 4.227441320419311 and perplexity is 68.54163154371757
At time: 99.03927278518677 and batch: 100, loss is 4.145251274108887 and perplexity is 63.13348371869195
At time: 99.98256826400757 and batch: 150, loss is 4.223009872436523 and perplexity is 68.23856487692242
At time: 100.92864942550659 and batch: 200, loss is 4.2036673450469975 and perplexity is 66.93134182170284
At time: 101.87681412696838 and batch: 250, loss is 4.249194488525391 and perplexity is 70.0489643704089
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.670838165283203 and perplexity of 106.78721026513175
Finished 19 epochs...
Completing Train Step...
At time: 103.45498633384705 and batch: 50, loss is 4.206617069244385 and perplexity is 67.12906228729707
At time: 104.39693450927734 and batch: 100, loss is 4.123473491668701 and perplexity is 61.773439570484506
At time: 105.34148073196411 and batch: 150, loss is 4.202679462432862 and perplexity is 66.86525416157627
At time: 106.28432273864746 and batch: 200, loss is 4.1842232608795165 and perplexity is 65.64249401670595
At time: 107.23426675796509 and batch: 250, loss is 4.22905571937561 and perplexity is 68.65237444970843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.661924743652344 and perplexity of 105.83960033257755
Finished 20 epochs...
Completing Train Step...
At time: 108.80461120605469 and batch: 50, loss is 4.186981859207154 and perplexity is 65.8238252859983
At time: 109.77153277397156 and batch: 100, loss is 4.102958807945251 and perplexity is 60.51908730628099
At time: 110.71984910964966 and batch: 150, loss is 4.183241558074951 and perplexity is 65.57808421704445
At time: 111.68244314193726 and batch: 200, loss is 4.1661163377761845 and perplexity is 64.46460656572185
At time: 112.62581253051758 and batch: 250, loss is 4.209919757843018 and perplexity is 67.35113519291734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.653339385986328 and perplexity of 104.93481900165128
Finished 21 epochs...
Completing Train Step...
At time: 114.18308067321777 and batch: 50, loss is 4.1684784507751464 and perplexity is 64.61705923519474
At time: 115.14644384384155 and batch: 100, loss is 4.0836128854751585 and perplexity is 59.359542135293594
At time: 116.09252882003784 and batch: 150, loss is 4.164756398200989 and perplexity is 64.37699818061093
At time: 117.04014587402344 and batch: 200, loss is 4.148902068138122 and perplexity is 63.36439230747748
At time: 117.98478174209595 and batch: 250, loss is 4.191691660881043 and perplexity is 66.13457365568377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6449840545654295 and perplexity of 104.06170646305435
Finished 22 epochs...
Completing Train Step...
At time: 119.56596112251282 and batch: 50, loss is 4.151129488945007 and perplexity is 63.50568877824918
At time: 120.51315259933472 and batch: 100, loss is 4.0651777601242065 and perplexity is 58.275266620528946
At time: 121.46125769615173 and batch: 150, loss is 4.147141609191895 and perplexity is 63.25294002853283
At time: 122.40496611595154 and batch: 200, loss is 4.132321944236756 and perplexity is 62.322464348183196
At time: 123.35106420516968 and batch: 250, loss is 4.174325370788575 and perplexity is 64.9959766825621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.637552642822266 and perplexity of 103.29124742058906
Finished 23 epochs...
Completing Train Step...
At time: 124.91514039039612 and batch: 50, loss is 4.134874653816223 and perplexity is 62.481758729586346
At time: 125.88204598426819 and batch: 100, loss is 4.047458000183106 and perplexity is 57.25173800749719
At time: 126.83624148368835 and batch: 150, loss is 4.130422348976135 and perplexity is 62.20418926323855
At time: 127.7959361076355 and batch: 200, loss is 4.116394782066346 and perplexity is 61.33770736098974
At time: 128.7394778728485 and batch: 250, loss is 4.1577228784561155 and perplexity is 63.92578994393223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.630962371826172 and perplexity of 102.6127682450009
Finished 24 epochs...
Completing Train Step...
At time: 130.3189971446991 and batch: 50, loss is 4.119371972084045 and perplexity is 61.52059347932974
At time: 131.2622344493866 and batch: 100, loss is 4.03072039604187 and perplexity is 56.30145598728397
At time: 132.2089717388153 and batch: 150, loss is 4.114423308372498 and perplexity is 61.21690080708647
At time: 133.16317892074585 and batch: 200, loss is 4.100994577407837 and perplexity is 60.40033053791533
At time: 134.1126687526703 and batch: 250, loss is 4.141811556816101 and perplexity is 62.91669544193017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6250732421875 and perplexity of 102.01024426253075
Finished 25 epochs...
Completing Train Step...
At time: 135.6960895061493 and batch: 50, loss is 4.1045347023010255 and perplexity is 60.61453418172663
At time: 136.65432834625244 and batch: 100, loss is 4.014669027328491 and perplexity is 55.404954841630314
At time: 137.6060619354248 and batch: 150, loss is 4.0991767883300785 and perplexity is 60.29063520846242
At time: 138.56158185005188 and batch: 200, loss is 4.086093120574951 and perplexity is 59.50695048331853
At time: 139.51220536231995 and batch: 250, loss is 4.1266269826889035 and perplexity is 61.968549033738306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.619557952880859 and perplexity of 101.4491768998257
Finished 26 epochs...
Completing Train Step...
At time: 141.07130360603333 and batch: 50, loss is 4.090275282859802 and perplexity is 59.75633933614952
At time: 142.0316288471222 and batch: 100, loss is 3.9991530847549437 and perplexity is 54.55192960268558
At time: 142.97802829742432 and batch: 150, loss is 4.084546642303467 and perplexity is 59.41499539899035
At time: 143.93178486824036 and batch: 200, loss is 4.071793422698975 and perplexity is 58.66207420439048
At time: 144.87700247764587 and batch: 250, loss is 4.1120840406417845 and perplexity is 61.07386545067716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.614266204833984 and perplexity of 100.91375133414633
Finished 27 epochs...
Completing Train Step...
At time: 146.45269179344177 and batch: 50, loss is 4.076563177108764 and perplexity is 58.94254625116414
At time: 147.39977097511292 and batch: 100, loss is 3.984187574386597 and perplexity is 53.74161067638462
At time: 148.34488105773926 and batch: 150, loss is 4.070533299446106 and perplexity is 58.58819931613183
At time: 149.28847432136536 and batch: 200, loss is 4.057982168197632 and perplexity is 57.857446616410876
At time: 150.236093044281 and batch: 250, loss is 4.098241634368897 and perplexity is 60.23428053637938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.609295654296875 and perplexity of 100.4133989767473
Finished 28 epochs...
Completing Train Step...
At time: 151.82133531570435 and batch: 50, loss is 4.0634150886535645 and perplexity is 58.17263694838307
At time: 152.7885172367096 and batch: 100, loss is 3.9698076963424684 and perplexity is 52.97434270024273
At time: 153.7390615940094 and batch: 150, loss is 4.057055225372315 and perplexity is 57.803840919924774
At time: 154.6834146976471 and batch: 200, loss is 4.044632153511047 and perplexity is 57.09018174829558
At time: 155.63173174858093 and batch: 250, loss is 4.0849797868728634 and perplexity is 59.44073625594189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.604490661621094 and perplexity of 99.93207064573988
Finished 29 epochs...
Completing Train Step...
At time: 157.2209973335266 and batch: 50, loss is 4.050660529136658 and perplexity is 57.43538226217121
At time: 158.19068551063538 and batch: 100, loss is 3.95605354309082 and perplexity is 52.25071333471756
At time: 159.13677334785461 and batch: 150, loss is 4.044078903198242 and perplexity is 57.05860532302423
At time: 160.1010763645172 and batch: 200, loss is 4.031684803962707 and perplexity is 56.35577974831503
At time: 161.04445457458496 and batch: 250, loss is 4.0721702384948735 and perplexity is 58.68418316581221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.600185394287109 and perplexity of 99.50276117550625
Finished 30 epochs...
Completing Train Step...
At time: 162.62928342819214 and batch: 50, loss is 4.038243818283081 and perplexity is 56.72663300108927
At time: 163.57710480690002 and batch: 100, loss is 3.9431762742996215 and perplexity is 51.58218053108788
At time: 164.52419471740723 and batch: 150, loss is 4.031505627632141 and perplexity is 56.34568303106691
At time: 165.4735701084137 and batch: 200, loss is 4.019072790145874 and perplexity is 55.649483148813545
At time: 166.42335677146912 and batch: 250, loss is 4.060157136917114 and perplexity is 57.98342169926271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.596591186523438 and perplexity of 99.14576951419404
Finished 31 epochs...
Completing Train Step...
At time: 167.9975140094757 and batch: 50, loss is 4.026257033348084 and perplexity is 56.050722143318694
At time: 168.96245002746582 and batch: 100, loss is 3.9303388500213625 and perplexity is 50.924230422554984
At time: 169.9146273136139 and batch: 150, loss is 4.01946033000946 and perplexity is 55.67105372138011
At time: 170.86128950119019 and batch: 200, loss is 4.006755518913269 and perplexity is 54.96823752738247
At time: 171.80312275886536 and batch: 250, loss is 4.048347835540771 and perplexity is 57.302705301151036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.593302917480469 and perplexity of 98.82028697990901
Finished 32 epochs...
Completing Train Step...
At time: 173.39985299110413 and batch: 50, loss is 4.014577569961548 and perplexity is 55.39988788205383
At time: 174.34923195838928 and batch: 100, loss is 3.9184492778778077 and perplexity is 50.322348262027496
At time: 175.29609322547913 and batch: 150, loss is 4.007704558372498 and perplexity is 55.02042931591629
At time: 176.24057936668396 and batch: 200, loss is 3.994809613227844 and perplexity is 54.31549868688709
At time: 177.19129014015198 and batch: 250, loss is 4.036602873802185 and perplexity is 56.633624077856346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5897674560546875 and perplexity of 98.47152854148496
Finished 33 epochs...
Completing Train Step...
At time: 178.79158997535706 and batch: 50, loss is 4.003256764411926 and perplexity is 54.77625320774841
At time: 179.73541641235352 and batch: 100, loss is 3.907367286682129 and perplexity is 49.76775511563609
At time: 180.67896056175232 and batch: 150, loss is 3.9964764976501463 and perplexity is 54.406111845388274
At time: 181.6318016052246 and batch: 200, loss is 3.983182473182678 and perplexity is 53.68762205535232
At time: 182.5785050392151 and batch: 250, loss is 4.025528779029846 and perplexity is 56.00991782264227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.587568283081055 and perplexity of 98.25521056475094
Finished 34 epochs...
Completing Train Step...
At time: 184.1646990776062 and batch: 50, loss is 3.9922080707550047 and perplexity is 54.17437825511041
At time: 185.12656712532043 and batch: 100, loss is 3.8961654472351075 and perplexity is 49.21337554566582
At time: 186.07263708114624 and batch: 150, loss is 3.9857378816604614 and perplexity is 53.82499130242082
At time: 187.02566742897034 and batch: 200, loss is 3.971957187652588 and perplexity is 53.088333056286814
At time: 187.96963214874268 and batch: 250, loss is 4.01440577507019 and perplexity is 55.390371281808946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.584503555297852 and perplexity of 97.95454605386267
Finished 35 epochs...
Completing Train Step...
At time: 189.54662418365479 and batch: 50, loss is 3.9812668991088866 and perplexity is 53.58487787691945
At time: 190.49551725387573 and batch: 100, loss is 3.8856199645996092 and perplexity is 48.69712359563782
At time: 191.44064784049988 and batch: 150, loss is 3.9751639318466188 and perplexity is 53.258847011284246
At time: 192.38863730430603 and batch: 200, loss is 3.961002950668335 and perplexity is 52.50996445171698
At time: 193.333740234375 and batch: 250, loss is 4.003799047470093 and perplexity is 54.80596549736032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.581582260131836 and perplexity of 97.66880947563409
Finished 36 epochs...
Completing Train Step...
At time: 194.89462518692017 and batch: 50, loss is 3.9705315732955935 and perplexity is 53.0127034886006
At time: 195.86786437034607 and batch: 100, loss is 3.8751990604400635 and perplexity is 48.19229051490856
At time: 196.8100790977478 and batch: 150, loss is 3.9648470783233645 and perplexity is 52.71220793406093
At time: 197.75714492797852 and batch: 200, loss is 3.950504198074341 and perplexity is 51.961559149282124
At time: 198.70261693000793 and batch: 250, loss is 3.993428182601929 and perplexity is 54.24051739618905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.578875732421875 and perplexity of 97.40482354016191
Finished 37 epochs...
Completing Train Step...
At time: 200.27360725402832 and batch: 50, loss is 3.9602665519714355 and perplexity is 52.47131041645789
At time: 201.2301366329193 and batch: 100, loss is 3.8650448894500733 and perplexity is 47.70541385441817
At time: 202.1722173690796 and batch: 150, loss is 3.954568657875061 and perplexity is 52.17318459785672
At time: 203.12181854248047 and batch: 200, loss is 3.940486440658569 and perplexity is 51.44361948324356
At time: 204.06526899337769 and batch: 250, loss is 3.983854556083679 and perplexity is 53.72371671607462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.576211547851562 and perplexity of 97.14566448927442
Finished 38 epochs...
Completing Train Step...
At time: 205.64493942260742 and batch: 50, loss is 3.9495629596710207 and perplexity is 51.912673944239664
At time: 206.59012246131897 and batch: 100, loss is 3.855375638008118 and perplexity is 47.2463611373573
At time: 207.53615617752075 and batch: 150, loss is 3.944595832824707 and perplexity is 51.655456452627696
At time: 208.48114562034607 and batch: 200, loss is 3.930705428123474 and perplexity is 50.94290155229935
At time: 209.44291472434998 and batch: 250, loss is 3.974302077293396 and perplexity is 53.212965405963445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5739593505859375 and perplexity of 96.92711948495379
Finished 39 epochs...
Completing Train Step...
At time: 211.00562834739685 and batch: 50, loss is 3.9392683029174806 and perplexity is 51.380992220872955
At time: 211.9668824672699 and batch: 100, loss is 3.8458822441101073 and perplexity is 46.79995512755
At time: 212.91536021232605 and batch: 150, loss is 3.935184288024902 and perplexity is 51.171579397062416
At time: 213.86385798454285 and batch: 200, loss is 3.9209709787368774 and perplexity is 50.449406304728065
At time: 214.8122923374176 and batch: 250, loss is 3.965040965080261 and perplexity is 52.72242912395079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.572500991821289 and perplexity of 96.7858679934307
Finished 40 epochs...
Completing Train Step...
At time: 216.3872103691101 and batch: 50, loss is 3.9292454671859742 and perplexity is 50.868581171617485
At time: 217.3329463005066 and batch: 100, loss is 3.8365576124191283 and perplexity is 46.365591072527664
At time: 218.27926325798035 and batch: 150, loss is 3.92584988117218 and perplexity is 50.69614545468595
At time: 219.224356174469 and batch: 200, loss is 3.9116925477981566 and perplexity is 49.98347984825596
At time: 220.16841864585876 and batch: 250, loss is 3.956057963371277 and perplexity is 52.25094429803503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.571328353881836 and perplexity of 96.6724397307586
Finished 41 epochs...
Completing Train Step...
At time: 221.75694346427917 and batch: 50, loss is 3.919446864128113 and perplexity is 50.37257419292048
At time: 222.70127749443054 and batch: 100, loss is 3.827720255851746 and perplexity is 45.95764704019284
At time: 223.64885234832764 and batch: 150, loss is 3.9168918991088866 and perplexity is 50.244038300205865
At time: 224.60535192489624 and batch: 200, loss is 3.9026835536956788 and perplexity is 49.53520127452231
At time: 225.56024384498596 and batch: 250, loss is 3.9473412322998045 and perplexity is 51.797466163154525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.570515060424805 and perplexity of 96.59384863119584
Finished 42 epochs...
Completing Train Step...
At time: 227.13159847259521 and batch: 50, loss is 3.9097347354888914 and perplexity is 49.88571730772897
At time: 228.09784626960754 and batch: 100, loss is 3.8189015769958496 and perplexity is 45.55414311076606
At time: 229.04573130607605 and batch: 150, loss is 3.908084635734558 and perplexity is 49.803468775658416
At time: 229.99390864372253 and batch: 200, loss is 3.893928771018982 and perplexity is 49.103424167721556
At time: 230.94225883483887 and batch: 250, loss is 3.938986144065857 and perplexity is 51.3664966642336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.570160293579102 and perplexity of 96.55958641411132
Finished 43 epochs...
Completing Train Step...
At time: 232.5195529460907 and batch: 50, loss is 3.9005561208724977 and perplexity is 49.42993047940481
At time: 233.46380281448364 and batch: 100, loss is 3.810506477355957 and perplexity is 45.173312333173584
At time: 234.42264556884766 and batch: 150, loss is 3.8996549034118653 and perplexity is 49.38540343026971
At time: 235.36663460731506 and batch: 200, loss is 3.8854017734527586 and perplexity is 48.68649947347901
At time: 236.31653690338135 and batch: 250, loss is 3.9306471443176267 and perplexity is 50.93993249264086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.57018814086914 and perplexity of 96.56227537436014
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 237.87910079956055 and batch: 50, loss is 3.8978785991668703 and perplexity is 49.29775779421879
At time: 238.83946466445923 and batch: 100, loss is 3.797395739555359 and perplexity is 44.584922420763796
At time: 239.7856240272522 and batch: 150, loss is 3.877629427909851 and perplexity is 48.309557933808804
At time: 240.7291543483734 and batch: 200, loss is 3.8515113735198976 and perplexity is 47.06414100196642
At time: 241.67464637756348 and batch: 250, loss is 3.882414937019348 and perplexity is 48.541297817781754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.540252304077148 and perplexity of 93.71444167264009
Finished 45 epochs...
Completing Train Step...
At time: 243.24457263946533 and batch: 50, loss is 3.8827334356307985 and perplexity is 48.556760616043896
At time: 244.20802235603333 and batch: 100, loss is 3.7865070247650148 and perplexity is 44.10208343518142
At time: 245.15691900253296 and batch: 150, loss is 3.8702443981170656 and perplexity is 47.95410454163158
At time: 246.1063747406006 and batch: 200, loss is 3.847924885749817 and perplexity is 46.895648364853244
At time: 247.05426931381226 and batch: 250, loss is 3.8833361673355102 and perplexity is 48.58603613690129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.538937759399414 and perplexity of 93.59133078722037
Finished 46 epochs...
Completing Train Step...
At time: 248.6595904827118 and batch: 50, loss is 3.878266444206238 and perplexity is 48.34034171332753
At time: 249.60386180877686 and batch: 100, loss is 3.7818899297714235 and perplexity is 43.89892927846174
At time: 250.5473382472992 and batch: 150, loss is 3.866707706451416 and perplexity is 47.78480521599144
At time: 251.49205350875854 and batch: 200, loss is 3.8462794971466066 and perplexity is 46.818550245071364
At time: 252.43905186653137 and batch: 250, loss is 3.8836893367767336 and perplexity is 48.60319827052721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.538260650634766 and perplexity of 93.52798072671023
Finished 47 epochs...
Completing Train Step...
At time: 254.03156757354736 and batch: 50, loss is 3.874943799972534 and perplexity is 48.1799904982212
At time: 255.01051235198975 and batch: 100, loss is 3.7786577463150026 and perplexity is 43.75726894491379
At time: 255.9543218612671 and batch: 150, loss is 3.8641902923583986 and perplexity is 47.66466236201468
At time: 256.89874243736267 and batch: 200, loss is 3.8450448226928713 and perplexity is 46.76078024803144
At time: 257.8420765399933 and batch: 250, loss is 3.8836245727539063 and perplexity is 48.60005063381284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.537818145751953 and perplexity of 93.48660329409222
Finished 48 epochs...
Completing Train Step...
At time: 259.4288353919983 and batch: 50, loss is 3.8721733474731446 and perplexity is 48.04669485300975
At time: 260.37313199043274 and batch: 100, loss is 3.7760270166397096 and perplexity is 43.6423066826101
At time: 261.3197076320648 and batch: 150, loss is 3.862125430107117 and perplexity is 47.56634294294511
At time: 262.2782368659973 and batch: 200, loss is 3.84395788192749 and perplexity is 46.70998166228695
At time: 263.2249119281769 and batch: 250, loss is 3.8833144664764405 and perplexity is 48.584981789618496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.537496948242188 and perplexity of 93.45658045180693
Finished 49 epochs...
Completing Train Step...
At time: 264.8161144256592 and batch: 50, loss is 3.8697539520263673 and perplexity is 47.9305914049605
At time: 265.7586154937744 and batch: 100, loss is 3.773731327056885 and perplexity is 43.54223240747472
At time: 266.7043135166168 and batch: 150, loss is 3.8603093957901002 and perplexity is 47.48003922080529
At time: 267.64789867401123 and batch: 200, loss is 3.842903881072998 and perplexity is 46.66077523806352
At time: 268.59499335289 and batch: 250, loss is 3.8828387022018434 and perplexity is 48.561872288774396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.537251663208008 and perplexity of 93.43365976244243
Finished Training.
Improved accuracyfrom -10000000 to -93.43365976244243
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa769f578d0>
SETTINGS FOR THIS RUN
{'dropout': 0.28662865101683643, 'tune_wordvecs': True, 'lr': 2.043935036858242, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.691206261612701, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2273740768432617 and batch: 50, loss is 6.795486526489258 and perplexity is 893.8040131619882
At time: 2.191856622695923 and batch: 100, loss is 5.979414157867431 and perplexity is 395.2087703732333
At time: 3.1392552852630615 and batch: 150, loss is 5.785394248962402 and perplexity is 325.5103469350845
At time: 4.089116811752319 and batch: 200, loss is 5.609639348983765 and perplexity is 273.0457460194265
At time: 5.045786142349243 and batch: 250, loss is 5.521521301269531 and perplexity is 250.0150963075998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.591485977172852 and perplexity of 268.1337644766527
Finished 1 epochs...
Completing Train Step...
At time: 6.636871337890625 and batch: 50, loss is 5.270991277694702 and perplexity is 194.60877820365027
At time: 7.578896760940552 and batch: 100, loss is 5.124019527435303 and perplexity is 168.00933232857727
At time: 8.523826122283936 and batch: 150, loss is 5.055642890930176 and perplexity is 156.90537096582426
At time: 9.46446943283081 and batch: 200, loss is 4.969169311523437 and perplexity is 143.90729557390614
At time: 10.410739183425903 and batch: 250, loss is 4.959436454772949 and perplexity is 142.51346048235013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.133331298828125 and perplexity of 169.58110345255568
Finished 2 epochs...
Completing Train Step...
At time: 12.001604318618774 and batch: 50, loss is 4.85794264793396 and perplexity is 128.7590267716498
At time: 12.953197240829468 and batch: 100, loss is 4.757834711074829 and perplexity is 116.49341072140002
At time: 13.892425298690796 and batch: 150, loss is 4.760900173187256 and perplexity is 116.85106476570033
At time: 14.834851264953613 and batch: 200, loss is 4.710838823318482 and perplexity is 111.14535212743583
At time: 15.786391973495483 and batch: 250, loss is 4.73324857711792 and perplexity is 113.66421018632987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.945207977294922 and perplexity of 140.50006863723436
Finished 3 epochs...
Completing Train Step...
At time: 17.37906241416931 and batch: 50, loss is 4.655148582458496 and perplexity is 105.12483854550999
At time: 18.34836196899414 and batch: 100, loss is 4.565168313980102 and perplexity is 96.07876405485008
At time: 19.290759563446045 and batch: 150, loss is 4.594346532821655 and perplexity is 98.92347118036385
At time: 20.235269784927368 and batch: 200, loss is 4.558281707763672 and perplexity is 95.41938050442731
At time: 21.180980920791626 and batch: 250, loss is 4.592073678970337 and perplexity is 98.69888790706518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.845981216430664 and perplexity of 127.22805902904793
Finished 4 epochs...
Completing Train Step...
At time: 22.78179144859314 and batch: 50, loss is 4.523239984512329 and perplexity is 92.13362643636812
At time: 23.72420883178711 and batch: 100, loss is 4.436189994812012 and perplexity is 84.45256322798903
At time: 24.664233684539795 and batch: 150, loss is 4.479463214874268 and perplexity is 88.18732232540414
At time: 25.60576105117798 and batch: 200, loss is 4.450236864089966 and perplexity is 85.64722835259049
At time: 26.555721282958984 and batch: 250, loss is 4.488991432189941 and perplexity is 89.03160616949438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.782061767578125 and perplexity of 119.35016885569814
Finished 5 epochs...
Completing Train Step...
At time: 28.136986255645752 and batch: 50, loss is 4.426348896026611 and perplexity is 83.62553332667919
At time: 29.10753846168518 and batch: 100, loss is 4.340276575088501 and perplexity is 76.72875766687204
At time: 30.053606271743774 and batch: 150, loss is 4.391841554641724 and perplexity is 80.789059541211
At time: 30.99527072906494 and batch: 200, loss is 4.366857986450196 and perplexity is 78.7956652986333
At time: 31.937045335769653 and batch: 250, loss is 4.4083989810943605 and perplexity is 82.13785392060286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.735531234741211 and perplexity of 113.92396301280442
Finished 6 epochs...
Completing Train Step...
At time: 33.49979543685913 and batch: 50, loss is 4.349207763671875 and perplexity is 77.41710598009381
At time: 34.45518350601196 and batch: 100, loss is 4.263380146026611 and perplexity is 71.04973651039134
At time: 35.39733529090881 and batch: 150, loss is 4.319657783508301 and perplexity is 75.16290190568125
At time: 36.34243154525757 and batch: 200, loss is 4.298892135620117 and perplexity is 73.61818953487148
At time: 37.28643870353699 and batch: 250, loss is 4.340848779678344 and perplexity is 76.77267477776998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.697642135620117 and perplexity of 109.68823731745816
Finished 7 epochs...
Completing Train Step...
At time: 38.87523317337036 and batch: 50, loss is 4.283373126983642 and perplexity is 72.48452762264968
At time: 39.82317781448364 and batch: 100, loss is 4.198664126396179 and perplexity is 66.59730600788694
At time: 40.7825083732605 and batch: 150, loss is 4.259454870223999 and perplexity is 70.7713933431225
At time: 41.72211194038391 and batch: 200, loss is 4.241575231552124 and perplexity is 69.51727143491117
At time: 42.6708505153656 and batch: 250, loss is 4.282197589874268 and perplexity is 72.39936943368599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.67107925415039 and perplexity of 106.81295857637554
Finished 8 epochs...
Completing Train Step...
At time: 44.230254888534546 and batch: 50, loss is 4.227700366973877 and perplexity is 68.55938931715897
At time: 45.185370206832886 and batch: 100, loss is 4.142322044372559 and perplexity is 62.94882183141007
At time: 46.126381635665894 and batch: 150, loss is 4.207595739364624 and perplexity is 67.19479165319427
At time: 47.06859564781189 and batch: 200, loss is 4.191443710327149 and perplexity is 66.11817758430578
At time: 48.01410698890686 and batch: 250, loss is 4.231012029647827 and perplexity is 68.78681125222951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.648276519775391 and perplexity of 104.4048906622724
Finished 9 epochs...
Completing Train Step...
At time: 49.61571550369263 and batch: 50, loss is 4.179816522598267 and perplexity is 65.35386115726948
At time: 50.5611207485199 and batch: 100, loss is 4.092122602462768 and perplexity is 59.86683041795565
At time: 51.50416946411133 and batch: 150, loss is 4.161677522659302 and perplexity is 64.17909423261335
At time: 52.44935703277588 and batch: 200, loss is 4.146601629257202 and perplexity is 63.218793930040825
At time: 53.399964570999146 and batch: 250, loss is 4.185473484992981 and perplexity is 65.72461316856906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629764175415039 and perplexity of 102.48989162421722
Finished 10 epochs...
Completing Train Step...
At time: 55.08671545982361 and batch: 50, loss is 4.137283840179443 and perplexity is 62.63247040402664
At time: 56.03409814834595 and batch: 100, loss is 4.047228832244873 and perplexity is 57.23861924799469
At time: 56.98139762878418 and batch: 150, loss is 4.120651092529297 and perplexity is 61.59933607815096
At time: 57.929107904434204 and batch: 200, loss is 4.106393203735352 and perplexity is 60.727291127461626
At time: 58.876479625701904 and batch: 250, loss is 4.145267457962036 and perplexity is 63.13450546998916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.616666412353515 and perplexity of 101.15625619349365
Finished 11 epochs...
Completing Train Step...
At time: 60.47469139099121 and batch: 50, loss is 4.098703813552857 and perplexity is 60.26212600130555
At time: 61.4345064163208 and batch: 100, loss is 4.0075275468826295 and perplexity is 55.01069092967843
At time: 62.377548933029175 and batch: 150, loss is 4.083689522743225 and perplexity is 59.364091462758324
At time: 63.32188010215759 and batch: 200, loss is 4.069972462654114 and perplexity is 58.55535011076127
At time: 64.26516890525818 and batch: 250, loss is 4.109588284492492 and perplexity is 60.92163002568692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6029003143310545 and perplexity of 99.77327025535553
Finished 12 epochs...
Completing Train Step...
At time: 65.88824963569641 and batch: 50, loss is 4.063081488609314 and perplexity is 58.15323379075001
At time: 66.8302264213562 and batch: 100, loss is 3.9720579242706298 and perplexity is 53.09368126479208
At time: 67.771155834198 and batch: 150, loss is 4.05008400440216 and perplexity is 57.402278887035955
At time: 68.71788358688354 and batch: 200, loss is 4.036412682533264 and perplexity is 56.62285388126055
At time: 69.66040515899658 and batch: 250, loss is 4.077559542655945 and perplexity is 59.00130384066988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.592101287841797 and perplexity of 98.70161290959162
Finished 13 epochs...
Completing Train Step...
At time: 71.24873232841492 and batch: 50, loss is 4.030621047019959 and perplexity is 56.29586277054435
At time: 72.20709490776062 and batch: 100, loss is 3.940008397102356 and perplexity is 51.419033069597845
At time: 73.15476703643799 and batch: 150, loss is 4.0189510393142704 and perplexity is 55.64270819039867
At time: 74.10233736038208 and batch: 200, loss is 4.005206718444824 and perplexity is 54.88316858976084
At time: 75.04378008842468 and batch: 250, loss is 4.04858154296875 and perplexity is 57.31609893405799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.583811569213867 and perplexity of 97.88678631823126
Finished 14 epochs...
Completing Train Step...
At time: 76.67277479171753 and batch: 50, loss is 4.0005695104599 and perplexity is 54.62925310660145
At time: 77.64476776123047 and batch: 100, loss is 3.910887565612793 and perplexity is 49.943260227626524
At time: 78.59201693534851 and batch: 150, loss is 3.989675178527832 and perplexity is 54.03733402591535
At time: 79.53443741798401 and batch: 200, loss is 3.9759780883789064 and perplexity is 53.30222570560814
At time: 80.47854399681091 and batch: 250, loss is 4.02219395160675 and perplexity is 55.82344551202385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.577420806884765 and perplexity of 97.26320981864633
Finished 15 epochs...
Completing Train Step...
At time: 82.0570240020752 and batch: 50, loss is 3.9726505041122437 and perplexity is 53.12515283381443
At time: 83.00684452056885 and batch: 100, loss is 3.8841524600982664 and perplexity is 48.62571275823713
At time: 83.95089268684387 and batch: 150, loss is 3.962337827682495 and perplexity is 52.58010560076641
At time: 84.89566254615784 and batch: 200, loss is 3.948453941345215 and perplexity is 51.85513374995638
At time: 85.84421133995056 and batch: 250, loss is 3.9982324743270876 and perplexity is 54.50173163736418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.571771621704102 and perplexity of 96.71530101140259
Finished 16 epochs...
Completing Train Step...
At time: 87.42543339729309 and batch: 50, loss is 3.9468168020248413 and perplexity is 51.770309125341
At time: 88.3839979171753 and batch: 100, loss is 3.85940083026886 and perplexity is 47.43692008546817
At time: 89.32947373390198 and batch: 150, loss is 3.9367480945587157 and perplexity is 51.25166444971279
At time: 90.28902745246887 and batch: 200, loss is 3.922840046882629 and perplexity is 50.54378785832585
At time: 91.24255180358887 and batch: 250, loss is 3.9757449769973756 and perplexity is 53.2898017982686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.566001510620117 and perplexity of 96.15884991723998
Finished 17 epochs...
Completing Train Step...
At time: 92.8431568145752 and batch: 50, loss is 3.922834749221802 and perplexity is 50.543520095190104
At time: 93.7886745929718 and batch: 100, loss is 3.836695375442505 and perplexity is 46.37197897653284
At time: 94.73702144622803 and batch: 150, loss is 3.9126614141464233 and perplexity is 50.031930627224774
At time: 95.68249559402466 and batch: 200, loss is 3.89904100894928 and perplexity is 49.35509530851723
At time: 96.63350296020508 and batch: 250, loss is 3.954663782119751 and perplexity is 52.17814776868983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5623634338378904 and perplexity of 95.80965222710832
Finished 18 epochs...
Completing Train Step...
At time: 98.23181986808777 and batch: 50, loss is 3.9003944110870363 and perplexity is 49.42193782221447
At time: 99.18035507202148 and batch: 100, loss is 3.8152732849121094 and perplexity is 45.38915886045498
At time: 100.12217259407043 and batch: 150, loss is 3.8897067499160767 and perplexity is 48.89654550500928
At time: 101.074214220047 and batch: 200, loss is 3.8769329261779784 and perplexity is 48.27592195815933
At time: 102.0239372253418 and batch: 250, loss is 3.934946961402893 and perplexity is 51.15943645995935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.558765029907226 and perplexity of 95.4655099507428
Finished 19 epochs...
Completing Train Step...
At time: 103.6120855808258 and batch: 50, loss is 3.879031567573547 and perplexity is 48.37734219151629
At time: 104.574875831604 and batch: 100, loss is 3.794804563522339 and perplexity is 44.46954458505013
At time: 105.5187463760376 and batch: 150, loss is 3.867876567840576 and perplexity is 47.84069168520213
At time: 106.46157813072205 and batch: 200, loss is 3.8565221643447876 and perplexity is 47.30056139978996
At time: 107.4088819026947 and batch: 250, loss is 3.9164527320861815 and perplexity is 50.22197762001307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.556426620483398 and perplexity of 95.2425333095689
Finished 20 epochs...
Completing Train Step...
At time: 109.09385442733765 and batch: 50, loss is 3.858589324951172 and perplexity is 47.39844038791538
At time: 110.0379536151886 and batch: 100, loss is 3.7751238536834717 and perplexity is 43.6029083621195
At time: 110.98037648200989 and batch: 150, loss is 3.8472614431381227 and perplexity is 46.86454611184577
At time: 111.92239093780518 and batch: 200, loss is 3.837355823516846 and perplexity is 46.402615376513346
At time: 112.87298440933228 and batch: 250, loss is 3.89906005859375 and perplexity is 49.3560355144909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.555356216430664 and perplexity of 95.14063985923161
Finished 21 epochs...
Completing Train Step...
At time: 114.47793078422546 and batch: 50, loss is 3.839245376586914 and perplexity is 46.490378471257266
At time: 115.4415295124054 and batch: 100, loss is 3.7566454648971557 and perplexity is 42.80459535352198
At time: 116.38570022583008 and batch: 150, loss is 3.827763385772705 and perplexity is 45.959629232622746
At time: 117.33341479301453 and batch: 200, loss is 3.8189023542404175 and perplexity is 45.554178517490094
At time: 118.28486180305481 and batch: 250, loss is 3.8819971084594727 and perplexity is 48.521020113816995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.555482482910156 and perplexity of 95.1526536913394
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 119.926677942276 and batch: 50, loss is 3.830140781402588 and perplexity is 46.06902343944129
At time: 120.88357877731323 and batch: 100, loss is 3.7303696155548094 and perplexity is 41.6945162580775
At time: 121.82432317733765 and batch: 150, loss is 3.7850202035903933 and perplexity is 44.03656024638799
At time: 122.76906728744507 and batch: 200, loss is 3.7516220808029175 and perplexity is 42.59011060074301
At time: 123.71263194084167 and batch: 250, loss is 3.786655168533325 and perplexity is 44.10861736798061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5116321563720705 and perplexity of 91.07033830676707
Finished 23 epochs...
Completing Train Step...
At time: 125.29346752166748 and batch: 50, loss is 3.8014316511154176 and perplexity is 44.7652268261412
At time: 126.23804664611816 and batch: 100, loss is 3.7077751398086547 and perplexity is 40.76301355746733
At time: 127.18397951126099 and batch: 150, loss is 3.769159688949585 and perplexity is 43.343627399536636
At time: 128.13284850120544 and batch: 200, loss is 3.743413472175598 and perplexity is 42.24193602089176
At time: 129.07551455497742 and batch: 250, loss is 3.7869021224975588 and perplexity is 44.11951151101703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.509313583374023 and perplexity of 90.85942967745568
Finished 24 epochs...
Completing Train Step...
At time: 130.64771103858948 and batch: 50, loss is 3.7914164066314697 and perplexity is 44.31912974955306
At time: 131.60334396362305 and batch: 100, loss is 3.697980971336365 and perplexity is 40.365722478989696
At time: 132.54670310020447 and batch: 150, loss is 3.761575951576233 and perplexity is 43.01616398026646
At time: 133.49223232269287 and batch: 200, loss is 3.739438772201538 and perplexity is 42.074370231358586
At time: 134.43323683738708 and batch: 250, loss is 3.7864199256896973 and perplexity is 44.09824235177447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5084278106689455 and perplexity of 90.77898450797586
Finished 25 epochs...
Completing Train Step...
At time: 136.01339626312256 and batch: 50, loss is 3.7840485906600954 and perplexity is 43.99379453427054
At time: 136.95751976966858 and batch: 100, loss is 3.6910868501663208 and perplexity is 40.08839336578662
At time: 137.90127038955688 and batch: 150, loss is 3.756192102432251 and perplexity is 42.785193754973854
At time: 138.85480189323425 and batch: 200, loss is 3.7364011812210083 and perplexity is 41.946759416683804
At time: 139.82964777946472 and batch: 250, loss is 3.7853171586990357 and perplexity is 44.0496390697358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.507985687255859 and perplexity of 90.73885786462657
Finished 26 epochs...
Completing Train Step...
At time: 141.41555786132812 and batch: 50, loss is 3.777945485115051 and perplexity is 43.726113436775435
At time: 142.36286306381226 and batch: 100, loss is 3.685488934516907 and perplexity is 39.86460886896077
At time: 143.30538845062256 and batch: 150, loss is 3.751778483390808 and perplexity is 42.59677232520141
At time: 144.24513459205627 and batch: 200, loss is 3.7337098693847657 and perplexity is 41.83401938391546
At time: 145.1917371749878 and batch: 250, loss is 3.7838480138778685 and perplexity is 43.984971285423924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.507763671875 and perplexity of 90.71871467867031
Finished 27 epochs...
Completing Train Step...
At time: 146.75036334991455 and batch: 50, loss is 3.77261474609375 and perplexity is 43.493641112781894
At time: 147.7363953590393 and batch: 100, loss is 3.680629301071167 and perplexity is 39.67135144285666
At time: 148.6758589744568 and batch: 150, loss is 3.7478999996185305 and perplexity is 42.43188140506514
At time: 149.63373589515686 and batch: 200, loss is 3.7311857891082765 and perplexity is 41.728560110528136
At time: 150.58721446990967 and batch: 250, loss is 3.7821565866470337 and perplexity is 43.91063679066071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.507676315307617 and perplexity of 90.71079014929352
Finished 28 epochs...
Completing Train Step...
At time: 152.193204164505 and batch: 50, loss is 3.767812190055847 and perplexity is 43.2852612425627
At time: 153.13530826568604 and batch: 100, loss is 3.676250042915344 and perplexity is 39.4980002054508
At time: 154.08003759384155 and batch: 150, loss is 3.7443589258193968 and perplexity is 42.28189269884082
At time: 155.02052235603333 and batch: 200, loss is 3.7287588357925414 and perplexity is 41.62740963659497
At time: 155.96298599243164 and batch: 250, loss is 3.7803234338760374 and perplexity is 43.83021561981872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.50767822265625 and perplexity of 90.71096316656009
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 157.54147481918335 and batch: 50, loss is 3.768860449790955 and perplexity is 43.33065922933056
At time: 158.49884271621704 and batch: 100, loss is 3.6747678089141846 and perplexity is 39.439498294040405
At time: 159.4452886581421 and batch: 150, loss is 3.739187364578247 and perplexity is 42.06379374349779
At time: 160.3973047733307 and batch: 200, loss is 3.7172351360321043 and perplexity is 41.150461249191444
At time: 161.34533309936523 and batch: 250, loss is 3.7626026916503905 and perplexity is 43.060353081139894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.502758026123047 and perplexity of 90.26574358216062
Finished 30 epochs...
Completing Train Step...
At time: 162.92270708084106 and batch: 50, loss is 3.7641137218475342 and perplexity is 43.12546775767721
At time: 163.87971925735474 and batch: 100, loss is 3.6702674198150635 and perplexity is 39.26240400135346
At time: 164.82274341583252 and batch: 150, loss is 3.736290464401245 and perplexity is 41.942115461968505
At time: 165.76653122901917 and batch: 200, loss is 3.716326584815979 and perplexity is 41.11309092657607
At time: 166.71064853668213 and batch: 250, loss is 3.7638655376434325 and perplexity is 43.11476602584059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.502217864990234 and perplexity of 90.21699870208312
Finished 31 epochs...
Completing Train Step...
At time: 168.33525323867798 and batch: 50, loss is 3.7623181104660035 and perplexity is 43.048100658347124
At time: 169.27973294258118 and batch: 100, loss is 3.6684062004089357 and perplexity is 39.18939601611019
At time: 170.22435426712036 and batch: 150, loss is 3.7350087451934812 and perplexity is 41.88839188359259
At time: 171.17186164855957 and batch: 200, loss is 3.716009831428528 and perplexity is 41.10007027803262
At time: 172.11728405952454 and batch: 250, loss is 3.7646012544631957 and perplexity is 43.14649795581194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.502047348022461 and perplexity of 90.20161648452465
Finished 32 epochs...
Completing Train Step...
At time: 173.68502378463745 and batch: 50, loss is 3.7610023069381713 and perplexity is 42.99149506472124
At time: 174.64834427833557 and batch: 100, loss is 3.6671064949035643 and perplexity is 39.13849442805837
At time: 175.59588503837585 and batch: 150, loss is 3.734138569831848 and perplexity is 41.85195749149124
At time: 176.54115962982178 and batch: 200, loss is 3.715785975456238 and perplexity is 41.09087081155551
At time: 177.4878957271576 and batch: 250, loss is 3.765020294189453 and perplexity is 43.16458184117287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501974868774414 and perplexity of 90.19507897610896
Finished 33 epochs...
Completing Train Step...
At time: 179.08949255943298 and batch: 50, loss is 3.759906768798828 and perplexity is 42.944422032069866
At time: 180.0356662273407 and batch: 100, loss is 3.6660510301589966 and perplexity is 39.09720691962175
At time: 180.9755504131317 and batch: 150, loss is 3.733439998626709 and perplexity is 41.82273112864528
At time: 181.92038559913635 and batch: 200, loss is 3.715576615333557 and perplexity is 41.082268922279056
At time: 182.87044143676758 and batch: 250, loss is 3.7652533769607546 and perplexity is 43.17464393413518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5019371032714846 and perplexity of 90.19167277790847
Finished 34 epochs...
Completing Train Step...
At time: 184.4642848968506 and batch: 50, loss is 3.758939938545227 and perplexity is 42.90292213053872
At time: 185.40741801261902 and batch: 100, loss is 3.6651336765289306 and perplexity is 39.061357400785774
At time: 186.34791445732117 and batch: 150, loss is 3.7328303146362303 and perplexity is 41.79724025051781
At time: 187.29019856452942 and batch: 200, loss is 3.715361876487732 and perplexity is 41.07344791040766
At time: 188.23596286773682 and batch: 250, loss is 3.765369029045105 and perplexity is 43.17963746044756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.50191650390625 and perplexity of 90.18981490583533
Finished 35 epochs...
Completing Train Step...
At time: 189.80476474761963 and batch: 50, loss is 3.7580579328536987 and perplexity is 42.86509819195298
At time: 190.76170873641968 and batch: 100, loss is 3.66430534362793 and perplexity is 39.02901499028358
At time: 191.70109939575195 and batch: 150, loss is 3.7322734498977663 and perplexity is 41.77397132068176
At time: 192.6454107761383 and batch: 200, loss is 3.7151371526718138 and perplexity is 41.06421876550349
At time: 193.5904552936554 and batch: 250, loss is 3.7654058933258057 and perplexity is 43.18122927606383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5019073486328125 and perplexity of 90.18898919719842
Finished 36 epochs...
Completing Train Step...
At time: 195.1614260673523 and batch: 50, loss is 3.7572364473342894 and perplexity is 42.82989959404892
At time: 196.1037826538086 and batch: 100, loss is 3.663539056777954 and perplexity is 38.99911902523436
At time: 197.043381690979 and batch: 150, loss is 3.731750922203064 and perplexity is 41.752148965637964
At time: 197.983980178833 and batch: 200, loss is 3.7149022722244265 and perplexity is 41.0545747160719
At time: 198.93523931503296 and batch: 250, loss is 3.7653872108459474 and perplexity is 43.18042255115346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501904296875 and perplexity of 90.188713962666
Finished 37 epochs...
Completing Train Step...
At time: 200.51813101768494 and batch: 50, loss is 3.756460747718811 and perplexity is 42.79668933965886
At time: 201.47705674171448 and batch: 100, loss is 3.662818331718445 and perplexity is 38.971021509362245
At time: 202.42257237434387 and batch: 150, loss is 3.7312523221969602 and perplexity is 41.73133653287957
At time: 203.36967611312866 and batch: 200, loss is 3.714657950401306 and perplexity is 41.04454541276856
At time: 204.31462788581848 and batch: 250, loss is 3.7653280782699583 and perplexity is 43.177869257027886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501905059814453 and perplexity of 90.18878277122033
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 205.89137387275696 and batch: 50, loss is 3.7571635150909426 and perplexity is 42.826776027294954
At time: 206.85137724876404 and batch: 100, loss is 3.6624491786956788 and perplexity is 38.95663789401267
At time: 207.79764413833618 and batch: 150, loss is 3.730844302177429 and perplexity is 41.71431278538343
At time: 208.7444896697998 and batch: 200, loss is 3.71249849319458 and perplexity is 40.9560071051298
At time: 209.69186878204346 and batch: 250, loss is 3.761200075149536 and perplexity is 42.9999982566069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501379013061523 and perplexity of 90.1413517314589
Finished 39 epochs...
Completing Train Step...
At time: 211.29134678840637 and batch: 50, loss is 3.7565996742248533 and perplexity is 42.80263534719846
At time: 212.23446035385132 and batch: 100, loss is 3.661762866973877 and perplexity is 38.92991066943688
At time: 213.20603799819946 and batch: 150, loss is 3.7303481101989746 and perplexity is 41.69361961231039
At time: 214.14952778816223 and batch: 200, loss is 3.712310719490051 and perplexity is 40.948317365940966
At time: 215.0974006652832 and batch: 250, loss is 3.7615395307540895 and perplexity is 43.01459732473846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501226425170898 and perplexity of 90.12759830207018
Finished 40 epochs...
Completing Train Step...
At time: 216.66506266593933 and batch: 50, loss is 3.7562817430496214 and perplexity is 42.78902921806043
At time: 217.6221489906311 and batch: 100, loss is 3.661391854286194 and perplexity is 38.91546985767599
At time: 218.57019639015198 and batch: 150, loss is 3.7300861358642576 and perplexity is 41.68269838465365
At time: 219.51520419120789 and batch: 200, loss is 3.7122445154190062 and perplexity is 40.94560651036473
At time: 220.47040510177612 and batch: 250, loss is 3.761792402267456 and perplexity is 43.0254758664395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501164245605469 and perplexity of 90.121994381401
Finished 41 epochs...
Completing Train Step...
At time: 222.1023232936859 and batch: 50, loss is 3.7560394954681398 and perplexity is 42.778664934630385
At time: 223.0455367565155 and batch: 100, loss is 3.6611363220214845 and perplexity is 38.90552696994944
At time: 224.01108932495117 and batch: 150, loss is 3.7299118280410766 and perplexity is 41.67543339742436
At time: 224.96280479431152 and batch: 200, loss is 3.7122158718109133 and perplexity is 40.9444336972556
At time: 225.91402339935303 and batch: 250, loss is 3.761974210739136 and perplexity is 43.0332989735821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501131439208985 and perplexity of 90.11903785201821
Finished 42 epochs...
Completing Train Step...
At time: 227.5185170173645 and batch: 50, loss is 3.755834946632385 and perplexity is 42.76991550339636
At time: 228.4604480266571 and batch: 100, loss is 3.660936312675476 and perplexity is 38.897746279075456
At time: 229.40397381782532 and batch: 150, loss is 3.7297792482376098 and perplexity is 41.66990844291192
At time: 230.34690165519714 and batch: 200, loss is 3.7121997976303103 and perplexity is 40.94377555432323
At time: 231.29095196723938 and batch: 250, loss is 3.762106671333313 and perplexity is 43.03899956747724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.50111083984375 and perplexity of 90.11718147616303
Finished 43 epochs...
Completing Train Step...
At time: 232.85916924476624 and batch: 50, loss is 3.755654797554016 and perplexity is 42.762211236515626
At time: 233.82082343101501 and batch: 100, loss is 3.66076696395874 and perplexity is 38.89115955340168
At time: 234.76463174819946 and batch: 150, loss is 3.7296697902679443 and perplexity is 41.66534758895307
At time: 235.71565508842468 and batch: 200, loss is 3.7121872997283933 and perplexity is 40.94326384622989
At time: 236.6570749282837 and batch: 250, loss is 3.7622050428390503 and perplexity is 43.04323358692014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501097106933594 and perplexity of 90.11594391350401
Finished 44 epochs...
Completing Train Step...
At time: 238.2447099685669 and batch: 50, loss is 3.7554915523529053 and perplexity is 42.75523108049637
At time: 239.18697905540466 and batch: 100, loss is 3.6606166315078736 and perplexity is 38.88531338951406
At time: 240.12818551063538 and batch: 150, loss is 3.729574317932129 and perplexity is 41.661369890779284
At time: 241.0783166885376 and batch: 200, loss is 3.712174835205078 and perplexity is 40.94275351114362
At time: 242.02738690376282 and batch: 250, loss is 3.7622796440124513 and perplexity is 43.04644478243068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.50108757019043 and perplexity of 90.11508450498991
Finished 45 epochs...
Completing Train Step...
At time: 243.59242415428162 and batch: 50, loss is 3.7553406715393067 and perplexity is 42.748780623082716
At time: 244.55629754066467 and batch: 100, loss is 3.6604787826538088 and perplexity is 38.87995346306171
At time: 245.49864435195923 and batch: 150, loss is 3.7294881916046143 and perplexity is 41.65778190450362
At time: 246.43985891342163 and batch: 200, loss is 3.7121611261367797 and perplexity is 40.94219222798676
At time: 247.38595271110535 and batch: 250, loss is 3.762336926460266 and perplexity is 43.04891065878259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501080322265625 and perplexity of 90.11443136000058
Finished 46 epochs...
Completing Train Step...
At time: 248.9554181098938 and batch: 50, loss is 3.7551994132995605 and perplexity is 42.74274243206276
At time: 249.92569756507874 and batch: 100, loss is 3.6603498458862305 and perplexity is 38.87494073070829
At time: 250.8732349872589 and batch: 150, loss is 3.729408206939697 and perplexity is 41.654450054027045
At time: 251.81394505500793 and batch: 200, loss is 3.712145686149597 and perplexity is 40.94156008594368
At time: 252.76222157478333 and batch: 250, loss is 3.7623815965652465 and perplexity is 43.05083370109194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.50107421875 and perplexity of 90.11388134683929
Finished 47 epochs...
Completing Train Step...
At time: 254.35478329658508 and batch: 50, loss is 3.755065894126892 and perplexity is 42.737035837434036
At time: 255.29926443099976 and batch: 100, loss is 3.660227417945862 and perplexity is 38.87018164311126
At time: 256.2678327560425 and batch: 150, loss is 3.729332709312439 and perplexity is 41.651305360593156
At time: 257.21487617492676 and batch: 200, loss is 3.712128701210022 and perplexity is 40.940864701925044
At time: 258.1585056781769 and batch: 250, loss is 3.7624165058135985 and perplexity is 43.05233659956975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501070785522461 and perplexity of 90.11357196591125
Finished 48 epochs...
Completing Train Step...
At time: 259.7396581172943 and batch: 50, loss is 3.7549381351470945 and perplexity is 42.73157614610559
At time: 260.69686627388 and batch: 100, loss is 3.6601097631454467 and perplexity is 38.86560864867059
At time: 261.6447012424469 and batch: 150, loss is 3.7292605209350587 and perplexity is 41.648298728966644
At time: 262.6036467552185 and batch: 200, loss is 3.7121099996566773 and perplexity is 40.9400990513193
At time: 263.5486145019531 and batch: 250, loss is 3.762443952560425 and perplexity is 43.053518262369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501069259643555 and perplexity of 90.11343446361755
Finished 49 epochs...
Completing Train Step...
At time: 265.1354694366455 and batch: 50, loss is 3.7548152446746825 and perplexity is 42.726325165180455
At time: 266.0815441608429 and batch: 100, loss is 3.659996089935303 and perplexity is 38.861190921264686
At time: 267.0259473323822 and batch: 150, loss is 3.729190859794617 and perplexity is 41.64539756203019
At time: 267.97355341911316 and batch: 200, loss is 3.7120898866653445 and perplexity is 40.93927563174265
At time: 268.91906332969666 and batch: 250, loss is 3.762465419769287 and perplexity is 43.05444251115828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.501067733764648 and perplexity of 90.11329696153359
Finished Training.
Improved accuracyfrom -93.43365976244243 to -90.11329696153359
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa75b581898>
SETTINGS FOR THIS RUN
{'dropout': 0.9126267719419398, 'tune_wordvecs': True, 'lr': 10.437363813840214, 'data': 'ptb', 'num_layers': 1, 'anneal': 2.0215821760184216, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2223334312438965 and batch: 50, loss is 6.82145848274231 and perplexity is 917.3219331131637
At time: 2.1923561096191406 and batch: 100, loss is 6.263275508880615 and perplexity is 524.935558257033
At time: 3.1460518836975098 and batch: 150, loss is 6.166358366012573 and perplexity is 476.4478941996987
At time: 4.0970728397369385 and batch: 200, loss is 6.138126125335694 and perplexity is 463.18480686159967
At time: 5.047366142272949 and batch: 250, loss is 6.1134821319580075 and perplexity is 451.90958740457614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.265319442749023 and perplexity of 526.009589073355
Finished 1 epochs...
Completing Train Step...
At time: 6.62964129447937 and batch: 50, loss is 5.750689144134522 and perplexity is 314.40725756115035
At time: 7.578666925430298 and batch: 100, loss is 5.414016838073731 and perplexity is 224.5316861210168
At time: 8.523236274719238 and batch: 150, loss is 5.2690775680541995 and perplexity is 194.2367096378279
At time: 9.471395254135132 and batch: 200, loss is 5.175088033676148 and perplexity is 176.8121790174182
At time: 10.42016339302063 and batch: 250, loss is 5.161698341369629 and perplexity is 174.46049762423021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3494621276855465 and perplexity of 210.4950479537845
Finished 2 epochs...
Completing Train Step...
At time: 12.02624773979187 and batch: 50, loss is 5.071253547668457 and perplexity is 159.37398506430694
At time: 12.97763967514038 and batch: 100, loss is 4.967085437774658 and perplexity is 143.60772318233174
At time: 13.922975063323975 and batch: 150, loss is 4.976749277114868 and perplexity is 145.00225254816883
At time: 14.86970043182373 and batch: 200, loss is 4.934666271209717 and perplexity is 139.02673756422794
At time: 15.818871974945068 and batch: 250, loss is 4.954883432388305 and perplexity is 141.86606842030608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.190382766723633 and perplexity of 179.53726067051235
Finished 3 epochs...
Completing Train Step...
At time: 17.398250579833984 and batch: 50, loss is 4.894872407913208 and perplexity is 133.60295884666897
At time: 18.373803853988647 and batch: 100, loss is 4.811679830551148 and perplexity is 122.9379590988871
At time: 19.319144248962402 and batch: 150, loss is 4.837405242919922 and perplexity is 126.14161986047317
At time: 20.294549465179443 and batch: 200, loss is 4.815193719863892 and perplexity is 123.3707093524619
At time: 21.236998558044434 and batch: 250, loss is 4.851207447052002 and perplexity is 127.89472275882981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.153886795043945 and perplexity of 173.10300034437662
Finished 4 epochs...
Completing Train Step...
At time: 22.80595588684082 and batch: 50, loss is 4.809955291748047 and perplexity is 122.72613052384443
At time: 23.75593852996826 and batch: 100, loss is 4.71513783454895 and perplexity is 111.6241957840961
At time: 24.710039377212524 and batch: 150, loss is 4.7533032417297365 and perplexity is 115.96671864757117
At time: 25.657429695129395 and batch: 200, loss is 4.731415300369263 and perplexity is 113.45602312319993
At time: 26.6053946018219 and batch: 250, loss is 4.7759214210510255 and perplexity is 118.61956284345511
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1146289825439455 and perplexity of 166.4390177454553
Finished 5 epochs...
Completing Train Step...
At time: 28.175912141799927 and batch: 50, loss is 4.739880638122559 and perplexity is 114.42054341415698
At time: 29.132859468460083 and batch: 100, loss is 4.653815069198608 and perplexity is 104.98474660735451
At time: 30.08128571510315 and batch: 150, loss is 4.679821701049804 and perplexity is 107.75085899352979
At time: 31.02228546142578 and batch: 200, loss is 4.662161207199096 and perplexity is 105.86463049910337
At time: 31.96874189376831 and batch: 250, loss is 4.712671689987182 and perplexity is 111.34925354370378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.090465545654297 and perplexity of 162.46547954840239
Finished 6 epochs...
Completing Train Step...
At time: 33.551653146743774 and batch: 50, loss is 4.680747089385986 and perplexity is 107.85061653176821
At time: 34.50927734375 and batch: 100, loss is 4.585107955932617 and perplexity is 98.01376773868493
At time: 35.450849533081055 and batch: 150, loss is 4.625949106216431 and perplexity is 102.09963050544587
At time: 36.398558139801025 and batch: 200, loss is 4.616692771911621 and perplexity is 101.15892266284989
At time: 37.342591285705566 and batch: 250, loss is 4.672147035598755 and perplexity is 106.92707238545019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.080848693847656 and perplexity of 160.910561798389
Finished 7 epochs...
Completing Train Step...
At time: 38.91295576095581 and batch: 50, loss is 4.628608484268188 and perplexity is 102.37151338134477
At time: 39.857383012771606 and batch: 100, loss is 4.54105917930603 and perplexity is 93.79008804870365
At time: 40.80012583732605 and batch: 150, loss is 4.582140026092529 and perplexity is 97.72330100832933
At time: 41.744590759277344 and batch: 200, loss is 4.567303619384766 and perplexity is 96.28414075196035
At time: 42.68590474128723 and batch: 250, loss is 4.615993366241455 and perplexity is 101.08819627485312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0741321563720705 and perplexity of 159.83342135624142
Finished 8 epochs...
Completing Train Step...
At time: 44.255398750305176 and batch: 50, loss is 4.585922403335571 and perplexity is 98.0936273135707
At time: 45.21159601211548 and batch: 100, loss is 4.508048858642578 and perplexity is 90.74459014516331
At time: 46.158305406570435 and batch: 150, loss is 4.545666742324829 and perplexity is 94.22322888549253
At time: 47.103848934173584 and batch: 200, loss is 4.538088359832764 and perplexity is 93.51186810396939
At time: 48.04804730415344 and batch: 250, loss is 4.5806381893157955 and perplexity is 97.5766467139027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.079154968261719 and perplexity of 160.6382541351493
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 49.620588302612305 and batch: 50, loss is 4.484503955841064 and perplexity is 88.63297403759061
At time: 50.565651416778564 and batch: 100, loss is 4.31462399482727 and perplexity is 74.78549842244553
At time: 51.508323669433594 and batch: 150, loss is 4.332481470108032 and perplexity is 76.13297406054606
At time: 52.45194101333618 and batch: 200, loss is 4.324975786209106 and perplexity is 75.56368315435157
At time: 53.39948606491089 and batch: 250, loss is 4.392286643981934 and perplexity is 80.82502589394473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.937751388549804 and perplexity of 139.45631366864387
Finished 10 epochs...
Completing Train Step...
At time: 54.980664014816284 and batch: 50, loss is 4.36309362411499 and perplexity is 78.49960744812175
At time: 55.9240779876709 and batch: 100, loss is 4.258825664520264 and perplexity is 70.72687758502278
At time: 56.869503021240234 and batch: 150, loss is 4.306509294509888 and perplexity is 74.18109211926482
At time: 57.814074993133545 and batch: 200, loss is 4.299925041198731 and perplexity is 73.69426945845412
At time: 58.75889563560486 and batch: 250, loss is 4.350944700241089 and perplexity is 77.55169143191341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.935822296142578 and perplexity of 139.18754887221834
Finished 11 epochs...
Completing Train Step...
At time: 60.33181548118591 and batch: 50, loss is 4.321344604492188 and perplexity is 75.2897952589229
At time: 61.292471408843994 and batch: 100, loss is 4.22854905128479 and perplexity is 68.61759929269084
At time: 62.23942255973816 and batch: 150, loss is 4.282885408401489 and perplexity is 72.44918419113424
At time: 63.18672490119934 and batch: 200, loss is 4.27539984703064 and perplexity is 71.90888611670368
At time: 64.13598680496216 and batch: 250, loss is 4.320345783233643 and perplexity is 75.21463175456867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.923299407958984 and perplexity of 137.45538723121652
Finished 12 epochs...
Completing Train Step...
At time: 65.72404074668884 and batch: 50, loss is 4.285604372024536 and perplexity is 72.64643893017788
At time: 66.6735110282898 and batch: 100, loss is 4.203334121704102 and perplexity is 66.90904245177816
At time: 67.6191394329071 and batch: 150, loss is 4.2594218349456785 and perplexity is 70.76905542906329
At time: 68.56403803825378 and batch: 200, loss is 4.251418104171753 and perplexity is 70.2048996494061
At time: 69.53724503517151 and batch: 250, loss is 4.296477947235108 and perplexity is 73.44067571888438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9198963165283205 and perplexity of 136.98840901628623
Finished 13 epochs...
Completing Train Step...
At time: 71.11774253845215 and batch: 50, loss is 4.258763027191162 and perplexity is 70.72244758105843
At time: 72.07795906066895 and batch: 100, loss is 4.177497596740722 and perplexity is 65.2024859804813
At time: 73.0205602645874 and batch: 150, loss is 4.232105741500854 and perplexity is 68.86208535961615
At time: 73.97002339363098 and batch: 200, loss is 4.228292160034179 and perplexity is 68.59997429574533
At time: 74.91685175895691 and batch: 250, loss is 4.271469707489014 and perplexity is 71.62682878537223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.917919921875 and perplexity of 136.71793322863275
Finished 14 epochs...
Completing Train Step...
At time: 76.48940229415894 and batch: 50, loss is 4.229397106170654 and perplexity is 68.67581546479279
At time: 77.44991135597229 and batch: 100, loss is 4.151681909561157 and perplexity is 63.54078032172671
At time: 78.39626932144165 and batch: 150, loss is 4.211985063552857 and perplexity is 67.49037961865372
At time: 79.34241342544556 and batch: 200, loss is 4.208422641754151 and perplexity is 67.25037816611888
At time: 80.29512786865234 and batch: 250, loss is 4.249359130859375 and perplexity is 70.06049834486018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.913572311401367 and perplexity of 136.12482714153927
Finished 15 epochs...
Completing Train Step...
At time: 81.87830138206482 and batch: 50, loss is 4.20856556892395 and perplexity is 67.25999075927207
At time: 82.82466840744019 and batch: 100, loss is 4.125597276687622 and perplexity is 61.904772488088625
At time: 83.7736325263977 and batch: 150, loss is 4.190008125305176 and perplexity is 66.02332741791447
At time: 84.71779870986938 and batch: 200, loss is 4.190183887481689 and perplexity is 66.03493284150949
At time: 85.66399264335632 and batch: 250, loss is 4.22884274482727 and perplexity is 68.63775479812963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.915256118774414 and perplexity of 136.35422820853893
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 87.2363760471344 and batch: 50, loss is 4.162269496917725 and perplexity is 64.21709785180032
At time: 88.19600343704224 and batch: 100, loss is 4.037806367874145 and perplexity is 56.701823339179406
At time: 89.14137530326843 and batch: 150, loss is 4.078425874710083 and perplexity is 59.052440708978764
At time: 90.0871958732605 and batch: 200, loss is 4.08087532043457 and perplexity is 59.19726375307807
At time: 91.03544354438782 and batch: 250, loss is 4.146416130065918 and perplexity is 63.207067982503375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.844428634643554 and perplexity of 127.03068032472952
Finished 17 epochs...
Completing Train Step...
At time: 92.60744619369507 and batch: 50, loss is 4.110469365119934 and perplexity is 60.97533054746548
At time: 93.55359292030334 and batch: 100, loss is 4.008686127662659 and perplexity is 55.074462193832794
At time: 94.52728033065796 and batch: 150, loss is 4.065852174758911 and perplexity is 58.31458156897728
At time: 95.47436285018921 and batch: 200, loss is 4.071318283081054 and perplexity is 58.634208149524156
At time: 96.42294383049011 and batch: 250, loss is 4.1294990921020505 and perplexity is 62.146785321274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.839627838134765 and perplexity of 126.42229341747773
Finished 18 epochs...
Completing Train Step...
At time: 98.00479578971863 and batch: 50, loss is 4.090230412483216 and perplexity is 59.7536581068544
At time: 98.95017051696777 and batch: 100, loss is 3.995416941642761 and perplexity is 54.34849605171441
At time: 99.89452481269836 and batch: 150, loss is 4.053848338127136 and perplexity is 57.61876743326813
At time: 100.83890080451965 and batch: 200, loss is 4.05851007938385 and perplexity is 57.887998273256855
At time: 101.78671598434448 and batch: 250, loss is 4.114976148605347 and perplexity is 61.25075332943894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.840183258056641 and perplexity of 126.49253038150988
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 103.34546160697937 and batch: 50, loss is 4.0642776727676395 and perplexity is 58.22283738882718
At time: 104.31602716445923 and batch: 100, loss is 3.951485486030579 and perplexity is 52.01257342721734
At time: 105.26452779769897 and batch: 150, loss is 3.9988042593002318 and perplexity is 54.53290381956816
At time: 106.2107264995575 and batch: 200, loss is 4.004335398674011 and perplexity is 54.835368627433986
At time: 107.16123938560486 and batch: 250, loss is 4.066477618217468 and perplexity is 58.35106545067136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812448883056641 and perplexity of 123.03254120899506
Finished 20 epochs...
Completing Train Step...
At time: 108.76107168197632 and batch: 50, loss is 4.0421438837051396 and perplexity is 56.948302562887655
At time: 109.71233940124512 and batch: 100, loss is 3.936856999397278 and perplexity is 51.2572463078959
At time: 110.66208672523499 and batch: 150, loss is 3.992989754676819 and perplexity is 54.21674205095863
At time: 111.61181426048279 and batch: 200, loss is 3.9992848205566407 and perplexity is 54.55911651824256
At time: 112.5575623512268 and batch: 250, loss is 4.0568799829483035 and perplexity is 57.793712122248266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.810841751098633 and perplexity of 122.83497048378139
Finished 21 epochs...
Completing Train Step...
At time: 114.14055919647217 and batch: 50, loss is 4.029822659492493 and perplexity is 56.2509347932153
At time: 115.12020444869995 and batch: 100, loss is 3.9282420682907104 and perplexity is 50.81756529238495
At time: 116.0665602684021 and batch: 150, loss is 3.9866057443618774 and perplexity is 53.87172428074742
At time: 117.01260662078857 and batch: 200, loss is 3.9936121797561643 and perplexity is 54.2504984152456
At time: 117.95843935012817 and batch: 250, loss is 4.0495623064041135 and perplexity is 57.37234004325372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.810367202758789 and perplexity of 122.77669318125572
Finished 22 epochs...
Completing Train Step...
At time: 119.61485576629639 and batch: 50, loss is 4.019741706848144 and perplexity is 55.68672047050924
At time: 120.58759808540344 and batch: 100, loss is 3.9212494850158692 and perplexity is 50.46345873790995
At time: 121.53800177574158 and batch: 150, loss is 3.980609073638916 and perplexity is 53.54963997091352
At time: 122.48154091835022 and batch: 200, loss is 3.987858924865723 and perplexity is 53.93927759473179
At time: 123.42678737640381 and batch: 250, loss is 4.042040238380432 and perplexity is 56.9424004434458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.810491943359375 and perplexity of 122.79200937495693
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 125.04214072227478 and batch: 50, loss is 4.007874178886413 and perplexity is 55.02976270095711
At time: 125.99338006973267 and batch: 100, loss is 3.8988615703582763 and perplexity is 49.3462398942815
At time: 126.94048523902893 and batch: 150, loss is 3.9498017406463624 and perplexity is 51.92507118321017
At time: 127.88754081726074 and batch: 200, loss is 3.957963061332703 and perplexity is 52.350582345484824
At time: 128.83698296546936 and batch: 250, loss is 4.01819863319397 and perplexity is 55.600858022341754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.794654083251953 and perplexity of 120.86256616526157
Finished 24 epochs...
Completing Train Step...
At time: 130.44580459594727 and batch: 50, loss is 3.99669047832489 and perplexity is 54.41775494756613
At time: 131.42199063301086 and batch: 100, loss is 3.8912435722351075 and perplexity is 48.97174857954523
At time: 132.36908650398254 and batch: 150, loss is 3.9467932033538817 and perplexity is 51.76908742926574
At time: 133.3126871585846 and batch: 200, loss is 3.9562661170959474 and perplexity is 52.26182165875051
At time: 134.26569414138794 and batch: 250, loss is 4.013355822563171 and perplexity is 55.332244543114314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.793678665161133 and perplexity of 120.7447321097984
Finished 25 epochs...
Completing Train Step...
At time: 135.86984658241272 and batch: 50, loss is 3.9908793067932127 and perplexity is 54.10244109796793
At time: 136.81907844543457 and batch: 100, loss is 3.8870910739898683 and perplexity is 48.76881511153975
At time: 137.7694547176361 and batch: 150, loss is 3.9429596853256226 and perplexity is 51.571009609322935
At time: 138.71600914001465 and batch: 200, loss is 3.9530280303955077 and perplexity is 52.09286704157295
At time: 139.6658434867859 and batch: 250, loss is 4.008770589828491 and perplexity is 55.07911409864393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7934211730957035 and perplexity of 120.71364530181266
Finished 26 epochs...
Completing Train Step...
At time: 141.25694012641907 and batch: 50, loss is 3.98587851524353 and perplexity is 53.83256143610136
At time: 142.20029973983765 and batch: 100, loss is 3.883266372680664 and perplexity is 48.58264520961445
At time: 143.1624517440796 and batch: 150, loss is 3.9398681831359865 and perplexity is 51.411823908448625
At time: 144.11050987243652 and batch: 200, loss is 3.949848871231079 and perplexity is 51.92751849984777
At time: 145.06001710891724 and batch: 250, loss is 4.005444259643554 and perplexity is 54.89620715195432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.793548965454102 and perplexity of 120.72907256895908
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 146.62067437171936 and batch: 50, loss is 3.98056583404541 and perplexity is 53.54732455630793
At time: 147.59484100341797 and batch: 100, loss is 3.8716949605941773 and perplexity is 48.023715441577025
At time: 148.54315948486328 and batch: 150, loss is 3.9237544202804564 and perplexity is 50.59002488910291
At time: 149.49298691749573 and batch: 200, loss is 3.9314177322387693 and perplexity is 50.979201317421364
At time: 150.44175601005554 and batch: 250, loss is 3.993017625808716 and perplexity is 54.21825315398534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.789510345458984 and perplexity of 120.24247697122796
Finished 28 epochs...
Completing Train Step...
At time: 152.02447867393494 and batch: 50, loss is 3.973525433540344 and perplexity is 53.17165393302474
At time: 152.974613904953 and batch: 100, loss is 3.8664406871795656 and perplexity is 47.77204745545692
At time: 153.9230306148529 and batch: 150, loss is 3.9226201438903807 and perplexity is 50.53267435012787
At time: 154.86895561218262 and batch: 200, loss is 3.931566472053528 and perplexity is 50.98678451832983
At time: 155.81855487823486 and batch: 250, loss is 3.9908759069442747 and perplexity is 54.10225715815371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.788422012329102 and perplexity of 120.1116842858374
Finished 29 epochs...
Completing Train Step...
At time: 157.37952399253845 and batch: 50, loss is 3.9688383102416993 and perplexity is 52.92301499093516
At time: 158.34347414970398 and batch: 100, loss is 3.8634276151657105 and perplexity is 47.628323470317405
At time: 159.29941606521606 and batch: 150, loss is 3.9215111923217774 and perplexity is 50.47666712203298
At time: 160.24728536605835 and batch: 200, loss is 3.930693817138672 and perplexity is 50.94231005847757
At time: 161.19169330596924 and batch: 250, loss is 3.9891919231414796 and perplexity is 54.01122650199206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.788084030151367 and perplexity of 120.07109553673438
Finished 30 epochs...
Completing Train Step...
At time: 162.76897501945496 and batch: 50, loss is 3.9648582315444947 and perplexity is 52.71279584825084
At time: 163.73934173583984 and batch: 100, loss is 3.8609664058685302 and perplexity is 47.51124433501237
At time: 164.6854465007782 and batch: 150, loss is 3.920452289581299 and perplexity is 50.42324553001403
At time: 165.6368317604065 and batch: 200, loss is 3.9297564792633057 and perplexity is 50.89458227382571
At time: 166.58319473266602 and batch: 250, loss is 3.987488241195679 and perplexity is 53.919286890696135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787760162353516 and perplexity of 120.0322146719072
Finished 31 epochs...
Completing Train Step...
At time: 168.16526317596436 and batch: 50, loss is 3.96135018825531 and perplexity is 52.528201051098485
At time: 169.11412906646729 and batch: 100, loss is 3.858631386756897 and perplexity is 47.400434093835784
At time: 170.06168055534363 and batch: 150, loss is 3.919133243560791 and perplexity is 50.356778794635
At time: 171.0199956893921 and batch: 200, loss is 3.928557105064392 and perplexity is 50.833577216241
At time: 171.97047686576843 and batch: 250, loss is 3.985799446105957 and perplexity is 53.82830511016949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.78758544921875 and perplexity of 120.01124529927473
Finished 32 epochs...
Completing Train Step...
At time: 173.5496323108673 and batch: 50, loss is 3.9581222772598266 and perplexity is 52.35891805555972
At time: 174.5153796672821 and batch: 100, loss is 3.856484251022339 and perplexity is 47.29876811234857
At time: 175.46270298957825 and batch: 150, loss is 3.917857065200806 and perplexity is 50.29255555213119
At time: 176.4090130329132 and batch: 200, loss is 3.9274531602859497 and perplexity is 50.77749071799805
At time: 177.35703253746033 and batch: 250, loss is 3.9841592359542846 and perplexity is 53.74008774496696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787344741821289 and perplexity of 119.98236118120256
Finished 33 epochs...
Completing Train Step...
At time: 178.9360544681549 and batch: 50, loss is 3.955101714134216 and perplexity is 52.201003254262396
At time: 179.88757395744324 and batch: 100, loss is 3.8543527030944826 and perplexity is 47.198055895778936
At time: 180.83977556228638 and batch: 150, loss is 3.916438274383545 and perplexity is 50.22125153084363
At time: 181.78525400161743 and batch: 200, loss is 3.926211109161377 and perplexity is 50.71446162932689
At time: 182.73433709144592 and batch: 250, loss is 3.9825238370895386 and perplexity is 53.65227309204125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787307357788086 and perplexity of 119.97787584046894
Finished 34 epochs...
Completing Train Step...
At time: 184.31068801879883 and batch: 50, loss is 3.9522218465805055 and perplexity is 52.05088753915833
At time: 185.25768518447876 and batch: 100, loss is 3.8523162174224854 and perplexity is 47.1020355364293
At time: 186.2087562084198 and batch: 150, loss is 3.9150377559661864 and perplexity is 50.150964973429595
At time: 187.15494513511658 and batch: 200, loss is 3.9249772119522093 and perplexity is 50.65192378722693
At time: 188.10884404182434 and batch: 250, loss is 3.980808482170105 and perplexity is 53.56031929070415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787202453613281 and perplexity of 119.96529032055741
Finished 35 epochs...
Completing Train Step...
At time: 189.69168877601624 and batch: 50, loss is 3.949413948059082 and perplexity is 51.90493892933303
At time: 190.66830778121948 and batch: 100, loss is 3.850338864326477 and perplexity is 47.0089902026891
At time: 191.61910343170166 and batch: 150, loss is 3.9136051893234254 and perplexity is 50.07917181042974
At time: 192.59601831436157 and batch: 200, loss is 3.9237678050994873 and perplexity is 50.59070203196252
At time: 193.5465989112854 and batch: 250, loss is 3.9791550636291504 and perplexity is 53.47183483680597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.78712158203125 and perplexity of 119.95558893002905
Finished 36 epochs...
Completing Train Step...
At time: 195.14341855049133 and batch: 50, loss is 3.946734976768494 and perplexity is 51.76607317983166
At time: 196.09101271629333 and batch: 100, loss is 3.8483983373641966 and perplexity is 46.91785644208913
At time: 197.03536200523376 and batch: 150, loss is 3.9121500396728517 and perplexity is 50.006352115694845
At time: 197.98671507835388 and batch: 200, loss is 3.922497296333313 and perplexity is 50.526466915823725
At time: 198.93805527687073 and batch: 250, loss is 3.977476930618286 and perplexity is 53.382177235345814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.786926651000977 and perplexity of 119.93220814238649
Finished 37 epochs...
Completing Train Step...
At time: 200.51448106765747 and batch: 50, loss is 3.9441123485565184 and perplexity is 51.630487888507105
At time: 201.4767770767212 and batch: 100, loss is 3.8465556573867796 and perplexity is 46.8314814526121
At time: 202.42806577682495 and batch: 150, loss is 3.9106729888916014 and perplexity is 49.93254471629203
At time: 203.37421798706055 and batch: 200, loss is 3.9212641096115113 and perplexity is 50.46419675098525
At time: 204.31935667991638 and batch: 250, loss is 3.975847935676575 and perplexity is 53.29528872833522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.786914443969726 and perplexity of 119.93074413510934
Finished 38 epochs...
Completing Train Step...
At time: 205.8985035419464 and batch: 50, loss is 3.941556911468506 and perplexity is 51.498717861599246
At time: 206.86011385917664 and batch: 100, loss is 3.8445595407485964 and perplexity is 46.73809359083464
At time: 207.81147289276123 and batch: 150, loss is 3.9091286849975586 and perplexity is 49.85549320383574
At time: 208.7566487789154 and batch: 200, loss is 3.9199952602386476 and perplexity is 50.40020589255132
At time: 209.7000060081482 and batch: 250, loss is 3.9741817331314087 and perplexity is 53.20656192155352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.786824035644531 and perplexity of 119.91990188751478
Finished 39 epochs...
Completing Train Step...
At time: 211.25839734077454 and batch: 50, loss is 3.9390707540512087 and perplexity is 51.370842966631635
At time: 212.20646452903748 and batch: 100, loss is 3.8427772331237793 and perplexity is 46.654866120767366
At time: 213.14987707138062 and batch: 150, loss is 3.9076405000686645 and perplexity is 49.78135419019152
At time: 214.0995888710022 and batch: 200, loss is 3.9187203645706177 and perplexity is 50.33599183020362
At time: 215.04825568199158 and batch: 250, loss is 3.9725705432891845 and perplexity is 53.120905072698186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.786764526367188 and perplexity of 119.91276575314959
Finished 40 epochs...
Completing Train Step...
At time: 216.60980010032654 and batch: 50, loss is 3.9365655851364134 and perplexity is 51.242311391578625
At time: 217.5766294002533 and batch: 100, loss is 3.8407880544662474 and perplexity is 46.5621534983655
At time: 218.52319884300232 and batch: 150, loss is 3.906110014915466 and perplexity is 49.70522284050826
At time: 219.46979403495789 and batch: 200, loss is 3.917433753013611 and perplexity is 50.27127060584723
At time: 220.4224841594696 and batch: 250, loss is 3.97097647190094 and perplexity is 53.03629401375508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.786771392822265 and perplexity of 119.91358913159569
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 222.01179909706116 and batch: 50, loss is 3.9330816316604613 and perplexity is 51.064096189689785
At time: 222.96477127075195 and batch: 100, loss is 3.8341534662246706 and perplexity is 46.25425530055908
At time: 223.91630935668945 and batch: 150, loss is 3.8973400926589967 and perplexity is 49.27121777745026
At time: 224.8696231842041 and batch: 200, loss is 3.9076178312301635 and perplexity is 49.780225717503654
At time: 225.8206217288971 and batch: 250, loss is 3.964814748764038 and perplexity is 52.71050379915441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.785456848144531 and perplexity of 119.75606092283341
Finished 42 epochs...
Completing Train Step...
At time: 227.40265941619873 and batch: 50, loss is 3.929822964668274 and perplexity is 50.897966133226255
At time: 228.35159277915955 and batch: 100, loss is 3.832326736450195 and perplexity is 46.16983840211081
At time: 229.29707670211792 and batch: 150, loss is 3.897663617134094 and perplexity is 49.28716080115925
At time: 230.24029922485352 and batch: 200, loss is 3.907990355491638 and perplexity is 49.798773513862685
At time: 231.18892431259155 and batch: 250, loss is 3.963939971923828 and perplexity is 52.664414033264116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784950637817383 and perplexity of 119.69545450914613
Finished 43 epochs...
Completing Train Step...
At time: 232.75302481651306 and batch: 50, loss is 3.927716817855835 and perplexity is 50.790880352867354
At time: 233.72684860229492 and batch: 100, loss is 3.831063961982727 and perplexity is 46.11157310471693
At time: 234.67081689834595 and batch: 150, loss is 3.8976002979278563 and perplexity is 49.284040076061565
At time: 235.6204698085785 and batch: 200, loss is 3.9078110790252687 and perplexity is 49.78984656593733
At time: 236.56820678710938 and batch: 250, loss is 3.962936592102051 and perplexity is 52.611598124537096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784788513183594 and perplexity of 119.67605050039403
Finished 44 epochs...
Completing Train Step...
At time: 238.15031790733337 and batch: 50, loss is 3.9258858585357665 and perplexity is 50.6979694011536
At time: 239.09469199180603 and batch: 100, loss is 3.8299020099639893 and perplexity is 46.05802478557427
At time: 240.04175853729248 and batch: 150, loss is 3.897406344413757 and perplexity is 49.27448219022255
At time: 240.98900651931763 and batch: 200, loss is 3.907476487159729 and perplexity is 49.77319007500848
At time: 241.94918179512024 and batch: 250, loss is 3.961945695877075 and perplexity is 52.559491311051474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784663391113281 and perplexity of 119.66107732194564
Finished 45 epochs...
Completing Train Step...
At time: 243.5114939212799 and batch: 50, loss is 3.9242079210281373 and perplexity is 50.61297270624814
At time: 244.47455048561096 and batch: 100, loss is 3.8288110637664796 and perplexity is 46.0078053568978
At time: 245.42538452148438 and batch: 150, loss is 3.8971444034576415 and perplexity is 49.26157687553456
At time: 246.37085628509521 and batch: 200, loss is 3.9070708656311037 and perplexity is 49.753005091574366
At time: 247.31703281402588 and batch: 250, loss is 3.9609687185287474 and perplexity is 52.50816695405041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7845512390136715 and perplexity of 119.64765783340837
Finished 46 epochs...
Completing Train Step...
At time: 248.88797736167908 and batch: 50, loss is 3.9226123571395872 and perplexity is 50.532280866317755
At time: 249.87289762496948 and batch: 100, loss is 3.827754874229431 and perplexity is 45.959238046914486
At time: 250.82735562324524 and batch: 150, loss is 3.8968280029296873 and perplexity is 49.245992952114094
At time: 251.77421402931213 and batch: 200, loss is 3.906640067100525 and perplexity is 49.73157618619093
At time: 252.72070336341858 and batch: 250, loss is 3.9600067138671875 and perplexity is 52.45767814180077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784533309936523 and perplexity of 119.64551268055084
Finished 47 epochs...
Completing Train Step...
At time: 254.31209588050842 and batch: 50, loss is 3.9210951471328737 and perplexity is 50.455670915513174
At time: 255.2606906890869 and batch: 100, loss is 3.8267052030563353 and perplexity is 45.911021269906776
At time: 256.2071158885956 and batch: 150, loss is 3.8964561223983765 and perplexity is 49.22768273090854
At time: 257.1573212146759 and batch: 200, loss is 3.906172528266907 and perplexity is 49.70833017769615
At time: 258.102614402771 and batch: 250, loss is 3.959047961235046 and perplexity is 52.407408306825296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784523391723633 and perplexity of 119.64432601676953
Finished 48 epochs...
Completing Train Step...
At time: 259.68108654022217 and batch: 50, loss is 3.919627799987793 and perplexity is 50.38168922252938
At time: 260.6460943222046 and batch: 100, loss is 3.8256837463378908 and perplexity is 45.86414909180603
At time: 261.60261726379395 and batch: 150, loss is 3.8960553884506224 and perplexity is 49.207959479421156
At time: 262.5547225475311 and batch: 200, loss is 3.905696668624878 and perplexity is 49.684681616636084
At time: 263.50094175338745 and batch: 250, loss is 3.958063049316406 and perplexity is 52.355817036357976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784466552734375 and perplexity of 119.63752574747029
Finished 49 epochs...
Completing Train Step...
At time: 265.09474992752075 and batch: 50, loss is 3.918193655014038 and perplexity is 50.30948636322089
At time: 266.0414972305298 and batch: 100, loss is 3.8246869802474976 and perplexity is 45.81845603965222
At time: 266.99982142448425 and batch: 150, loss is 3.895645170211792 and perplexity is 49.187777616714165
At time: 267.94643688201904 and batch: 200, loss is 3.9052161598205566 and perplexity is 49.660813424576844
At time: 268.8921573162079 and batch: 250, loss is 3.9570727157592773 and perplexity is 52.30399297962278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.784383010864258 and perplexity of 119.62753142231132
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa75b581898>
SETTINGS FOR THIS RUN
{'dropout': 0.8942611318164297, 'tune_wordvecs': True, 'lr': 10.333996825618843, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.0250139781457053, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.26072096824646 and batch: 50, loss is 6.757041816711426 and perplexity is 860.094112768097
At time: 2.214200258255005 and batch: 100, loss is 6.190311574935913 and perplexity is 487.99813051597084
At time: 3.1661205291748047 and batch: 150, loss is 6.099801845550537 and perplexity is 445.76943013431355
At time: 4.12130069732666 and batch: 200, loss is 6.07581582069397 and perplexity is 435.2044064885618
At time: 5.078124046325684 and batch: 250, loss is 6.0619401931762695 and perplexity is 429.20737475790435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 6.217173004150391 and perplexity of 501.2840989565047
Finished 1 epochs...
Completing Train Step...
At time: 6.736607313156128 and batch: 50, loss is 5.700294275283813 and perplexity is 298.95536319822276
At time: 7.684730052947998 and batch: 100, loss is 5.371161432266235 and perplexity is 215.1125613447602
At time: 8.63100790977478 and batch: 150, loss is 5.246719045639038 and perplexity is 189.94205379893822
At time: 9.576454877853394 and batch: 200, loss is 5.140123233795166 and perplexity is 170.73680756969443
At time: 10.607949256896973 and batch: 250, loss is 5.132693519592285 and perplexity is 169.47298262825618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.314156723022461 and perplexity of 203.1930928044426
Finished 2 epochs...
Completing Train Step...
At time: 12.228999614715576 and batch: 50, loss is 5.051503009796143 and perplexity is 156.25714409763455
At time: 13.180933475494385 and batch: 100, loss is 4.936814947128296 and perplexity is 139.32578212714364
At time: 14.134198665618896 and batch: 150, loss is 4.958846178054809 and perplexity is 142.42936292746202
At time: 15.08011245727539 and batch: 200, loss is 4.908541717529297 and perplexity is 135.44175798257527
At time: 16.02531385421753 and batch: 250, loss is 4.939464321136475 and perplexity is 139.6953976416492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.195527648925781 and perplexity of 180.46333896667136
Finished 3 epochs...
Completing Train Step...
At time: 17.631012201309204 and batch: 50, loss is 4.900138530731201 and perplexity is 134.30838423492932
At time: 18.58621072769165 and batch: 100, loss is 4.803090705871582 and perplexity is 121.88655143986058
At time: 19.53389263153076 and batch: 150, loss is 4.8379693126678465 and perplexity is 126.21279260349917
At time: 20.480026245117188 and batch: 200, loss is 4.80186731338501 and perplexity is 121.73752752457094
At time: 21.426674604415894 and batch: 250, loss is 4.847721328735352 and perplexity is 127.44964287454117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.130200576782227 and perplexity of 169.0510223546609
Finished 4 epochs...
Completing Train Step...
At time: 23.03687357902527 and batch: 50, loss is 4.807729568481445 and perplexity is 122.45327987737191
At time: 24.016526222229004 and batch: 100, loss is 4.715560874938965 and perplexity is 111.67142731713436
At time: 24.965819120407104 and batch: 150, loss is 4.749741439819336 and perplexity is 115.5544028969647
At time: 25.916045904159546 and batch: 200, loss is 4.726618146896362 and perplexity is 112.91306054650981
At time: 26.863802671432495 and batch: 250, loss is 4.771177587509155 and perplexity is 118.05818397922928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.089163589477539 and perplexity of 162.25409425085107
Finished 5 epochs...
Completing Train Step...
At time: 28.449684858322144 and batch: 50, loss is 4.736047830581665 and perplexity is 113.98283086235875
At time: 29.415800094604492 and batch: 100, loss is 4.642075366973877 and perplexity is 103.7594632473482
At time: 30.367364645004272 and batch: 150, loss is 4.687417030334473 and perplexity is 108.57237815317414
At time: 31.313786029815674 and batch: 200, loss is 4.672149505615234 and perplexity is 106.92733649740725
At time: 32.25841426849365 and batch: 250, loss is 4.710541687011719 and perplexity is 111.1123317140156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.082916641235352 and perplexity of 161.24366067107894
Finished 6 epochs...
Completing Train Step...
At time: 33.85758328437805 and batch: 50, loss is 4.67751482963562 and perplexity is 107.50257810321354
At time: 34.803499698638916 and batch: 100, loss is 4.588598690032959 and perplexity is 98.35650559536714
At time: 35.74775791168213 and batch: 150, loss is 4.603673973083496 and perplexity is 99.85049058638911
At time: 36.6965765953064 and batch: 200, loss is 4.59706745147705 and perplexity is 99.19300041583143
At time: 37.645567417144775 and batch: 250, loss is 4.6619863605499265 and perplexity is 105.84612204131308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.068999862670898 and perplexity of 159.01521073917746
Finished 7 epochs...
Completing Train Step...
At time: 39.21786594390869 and batch: 50, loss is 4.636714963912964 and perplexity is 103.204758751034
At time: 40.186798334121704 and batch: 100, loss is 4.548790283203125 and perplexity is 94.51799911637374
At time: 41.132551431655884 and batch: 150, loss is 4.586362743377686 and perplexity is 98.13683137709462
At time: 42.0781569480896 and batch: 200, loss is 4.575771179199219 and perplexity is 97.10289400200438
At time: 43.04849672317505 and batch: 250, loss is 4.621120138168335 and perplexity is 101.60778316533685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066896057128906 and perplexity of 158.68102531139948
Finished 8 epochs...
Completing Train Step...
At time: 44.66813087463379 and batch: 50, loss is 4.59477988243103 and perplexity is 98.96634891781228
At time: 45.6429705619812 and batch: 100, loss is 4.522805643081665 and perplexity is 92.09361767460992
At time: 46.602660179138184 and batch: 150, loss is 4.549407596588135 and perplexity is 94.57636435532076
At time: 47.549076795578 and batch: 200, loss is 4.54229995727539 and perplexity is 93.90653294989366
At time: 48.49689769744873 and batch: 250, loss is 4.581891946792602 and perplexity is 97.69906088708913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.071446228027344 and perplexity of 159.40469625957337
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 50.109930992126465 and batch: 50, loss is 4.495703468322754 and perplexity is 89.63119952214727
At time: 51.06304478645325 and batch: 100, loss is 4.3083515644073485 and perplexity is 74.31787967334591
At time: 52.013994455337524 and batch: 150, loss is 4.299691915512085 and perplexity is 73.67709143368197
At time: 52.96102333068848 and batch: 200, loss is 4.273059568405151 and perplexity is 71.74079605305141
At time: 53.91430902481079 and batch: 250, loss is 4.35926703453064 and perplexity is 78.19979566205404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.902315139770508 and perplexity of 134.60103946057353
Finished 10 epochs...
Completing Train Step...
At time: 55.49162244796753 and batch: 50, loss is 4.347356996536255 and perplexity is 77.27395745276701
At time: 56.46332883834839 and batch: 100, loss is 4.233623046875 and perplexity is 68.96664947959698
At time: 57.410120487213135 and batch: 150, loss is 4.268771228790283 and perplexity is 71.43380586488621
At time: 58.359249114990234 and batch: 200, loss is 4.26232837677002 and perplexity is 70.97504786633579
At time: 59.31178641319275 and batch: 250, loss is 4.322119283676147 and perplexity is 75.3481432936605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8891864776611325 and perplexity of 132.8454573324056
Finished 11 epochs...
Completing Train Step...
At time: 60.889095067977905 and batch: 50, loss is 4.303827743530274 and perplexity is 73.98243820835691
At time: 61.853400230407715 and batch: 100, loss is 4.208035144805908 and perplexity is 67.22432389811203
At time: 62.803396701812744 and batch: 150, loss is 4.249183959960938 and perplexity is 70.04822685925511
At time: 63.75236892700195 and batch: 200, loss is 4.240978384017945 and perplexity is 69.47579260237313
At time: 64.69890141487122 and batch: 250, loss is 4.295866460800171 and perplexity is 73.39578146942084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.888261413574218 and perplexity of 132.72262359401725
Finished 12 epochs...
Completing Train Step...
At time: 66.3007595539093 and batch: 50, loss is 4.273921709060669 and perplexity is 71.80267337963471
At time: 67.24583053588867 and batch: 100, loss is 4.184189500808716 and perplexity is 65.64027795886778
At time: 68.20997619628906 and batch: 150, loss is 4.229060039520264 and perplexity is 68.6526710385375
At time: 69.15687727928162 and batch: 200, loss is 4.22121732711792 and perplexity is 68.11635372414322
At time: 70.10527300834656 and batch: 250, loss is 4.2724838542938235 and perplexity is 71.69950575125095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.887830352783203 and perplexity of 132.6654244039549
Finished 13 epochs...
Completing Train Step...
At time: 71.67980670928955 and batch: 50, loss is 4.249690971374512 and perplexity is 70.08375111461385
At time: 72.64045977592468 and batch: 100, loss is 4.164218330383301 and perplexity is 64.3423683071363
At time: 73.58473992347717 and batch: 150, loss is 4.212552785873413 and perplexity is 67.5287062920106
At time: 74.52809834480286 and batch: 200, loss is 4.203097133636475 and perplexity is 66.89318768587012
At time: 75.47361135482788 and batch: 250, loss is 4.25089958190918 and perplexity is 70.16850628219713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.887671661376953 and perplexity of 132.6443732115614
Finished 14 epochs...
Completing Train Step...
At time: 77.05490136146545 and batch: 50, loss is 4.2298122215271 and perplexity is 68.70432976836183
At time: 78.02027893066406 and batch: 100, loss is 4.147314395904541 and perplexity is 63.26387024037606
At time: 78.98115754127502 and batch: 150, loss is 4.1943510723114015 and perplexity is 66.31068677162618
At time: 79.939129114151 and batch: 200, loss is 4.1849737644195555 and perplexity is 65.69177743221812
At time: 80.8901629447937 and batch: 250, loss is 4.231856098175049 and perplexity is 68.84489654523075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.888576889038086 and perplexity of 132.7645009305501
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 82.47772145271301 and batch: 50, loss is 4.193207244873047 and perplexity is 66.23488215060827
At time: 83.43016409873962 and batch: 100, loss is 4.075229320526123 and perplexity is 58.86397775904851
At time: 84.38358855247498 and batch: 150, loss is 4.100448780059814 and perplexity is 60.36737319252203
At time: 85.33585119247437 and batch: 200, loss is 4.082717924118042 and perplexity is 59.30644140400194
At time: 86.2848379611969 and batch: 250, loss is 4.153721623420715 and perplexity is 63.67051760047859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.834403228759766 and perplexity of 125.76350876286577
Finished 16 epochs...
Completing Train Step...
At time: 87.8646092414856 and batch: 50, loss is 4.15596300125122 and perplexity is 63.81338733980618
At time: 88.82736849784851 and batch: 100, loss is 4.048603420257568 and perplexity is 57.31735286862458
At time: 89.77505373954773 and batch: 150, loss is 4.090192718505859 and perplexity is 59.7514057962683
At time: 90.72131514549255 and batch: 200, loss is 4.083271160125732 and perplexity is 59.33926094051217
At time: 91.669593334198 and batch: 250, loss is 4.145801734924317 and perplexity is 63.168245794323155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.830615234375 and perplexity of 125.28801844375651
Finished 17 epochs...
Completing Train Step...
At time: 93.2577588558197 and batch: 50, loss is 4.138876667022705 and perplexity is 62.73231257868481
At time: 94.20746350288391 and batch: 100, loss is 4.037899355888367 and perplexity is 56.707096174285915
At time: 95.1612479686737 and batch: 150, loss is 4.083306107521057 and perplexity is 59.34133472935911
At time: 96.10948514938354 and batch: 200, loss is 4.075669755935669 and perplexity is 58.88990924935441
At time: 97.05772829055786 and batch: 250, loss is 4.137636198997497 and perplexity is 62.65454339584827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.831056213378906 and perplexity of 125.34328001303153
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 98.69200825691223 and batch: 50, loss is 4.125173497200012 and perplexity is 61.87854407324763
At time: 99.6415102481842 and batch: 100, loss is 4.017002577781677 and perplexity is 55.53439606918849
At time: 100.5897068977356 and batch: 150, loss is 4.048429980278015 and perplexity is 57.30741261015902
At time: 101.53682112693787 and batch: 200, loss is 4.037408485412597 and perplexity is 56.67926716578543
At time: 102.49099493026733 and batch: 250, loss is 4.1094186401367185 and perplexity is 60.91129589159708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.814717102050781 and perplexity of 123.31192268526763
Finished 19 epochs...
Completing Train Step...
At time: 104.10066771507263 and batch: 50, loss is 4.110488233566284 and perplexity is 60.976481068072786
At time: 105.06592679023743 and batch: 100, loss is 4.0061905431747435 and perplexity is 54.93719057800242
At time: 106.0155189037323 and batch: 150, loss is 4.04466187953949 and perplexity is 57.091878837885744
At time: 106.9633367061615 and batch: 200, loss is 4.03898063659668 and perplexity is 56.768445625415275
At time: 107.91360664367676 and batch: 250, loss is 4.106052265167237 and perplexity is 60.70659038082109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.812693023681641 and perplexity of 123.06258211745504
Finished 20 epochs...
Completing Train Step...
At time: 109.52324390411377 and batch: 50, loss is 4.102454414367676 and perplexity is 60.488569564446486
At time: 110.47094631195068 and batch: 100, loss is 4.001807336807251 and perplexity is 54.69691650456173
At time: 111.42237854003906 and batch: 150, loss is 4.043055157661438 and perplexity is 57.00022172056386
At time: 112.37213039398193 and batch: 200, loss is 4.03737587928772 and perplexity is 56.677419104651484
At time: 113.32054829597473 and batch: 250, loss is 4.102960371971131 and perplexity is 60.51918195977376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.81219253540039 and perplexity of 123.0010061475558
Finished 21 epochs...
Completing Train Step...
At time: 114.89819025993347 and batch: 50, loss is 4.097079076766968 and perplexity is 60.164295404401
At time: 115.86162066459656 and batch: 100, loss is 3.9977244424819944 and perplexity is 54.47405005423793
At time: 116.80941319465637 and batch: 150, loss is 4.041259245872498 and perplexity is 56.89794621678423
At time: 117.77815270423889 and batch: 200, loss is 4.035638132095337 and perplexity is 56.57901360543431
At time: 118.71464920043945 and batch: 250, loss is 4.099366579055786 and perplexity is 60.302078897790715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.811288070678711 and perplexity of 122.88980637248174
Finished 22 epochs...
Completing Train Step...
At time: 120.33174085617065 and batch: 50, loss is 4.091367030143738 and perplexity is 59.821613782433985
At time: 121.32311797142029 and batch: 100, loss is 3.9940529108047484 and perplexity is 54.27441356398554
At time: 122.27419209480286 and batch: 150, loss is 4.039215073585511 and perplexity is 56.78175580900702
At time: 123.2217857837677 and batch: 200, loss is 4.033715229034424 and perplexity is 56.47032218201545
At time: 124.16802167892456 and batch: 250, loss is 4.096141352653503 and perplexity is 60.10790433759443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8108375549316404 and perplexity of 122.83445504881414
Finished 23 epochs...
Completing Train Step...
At time: 125.77054166793823 and batch: 50, loss is 4.086806426048279 and perplexity is 59.54941225910746
At time: 126.72623991966248 and batch: 100, loss is 3.990567851066589 and perplexity is 54.085593206685964
At time: 127.68673348426819 and batch: 150, loss is 4.036947994232178 and perplexity is 56.653172871700214
At time: 128.63718175888062 and batch: 200, loss is 4.031412301063537 and perplexity is 56.340424727187525
At time: 129.58904457092285 and batch: 250, loss is 4.093126449584961 and perplexity is 59.926957737654696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.810513305664062 and perplexity of 122.79463252329248
Finished 24 epochs...
Completing Train Step...
At time: 131.1690845489502 and batch: 50, loss is 4.082455201148987 and perplexity is 59.290862286217816
At time: 132.13927555084229 and batch: 100, loss is 3.9871445989608763 and perplexity is 53.900761129749306
At time: 133.08720088005066 and batch: 150, loss is 4.0348413372039795 and perplexity is 56.53394969216759
At time: 134.033686876297 and batch: 200, loss is 4.029328231811523 and perplexity is 56.223129648356206
At time: 134.98278045654297 and batch: 250, loss is 4.089914741516114 and perplexity is 59.73479858866971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8102764129638675 and perplexity of 122.76554681645644
Finished 25 epochs...
Completing Train Step...
At time: 136.5735728740692 and batch: 50, loss is 4.078344693183899 and perplexity is 59.04764693630264
At time: 137.5198323726654 and batch: 100, loss is 3.9843362188339233 and perplexity is 53.749599662146515
At time: 138.46769738197327 and batch: 150, loss is 4.032534999847412 and perplexity is 56.403713574019704
At time: 139.4198911190033 and batch: 200, loss is 4.0269494104385375 and perplexity is 56.089543817302086
At time: 140.36619234085083 and batch: 250, loss is 4.086744828224182 and perplexity is 59.545744257857685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.80976448059082 and perplexity of 122.7027152428767
Finished 26 epochs...
Completing Train Step...
At time: 141.99899625778198 and batch: 50, loss is 4.0741123819351195 and perplexity is 58.798267014933884
At time: 142.95111227035522 and batch: 100, loss is 3.9810890531539918 and perplexity is 53.57534887051824
At time: 143.90240120887756 and batch: 150, loss is 4.030186586380005 and perplexity is 56.27140974630122
At time: 144.8476448059082 and batch: 200, loss is 4.0247736740112305 and perplexity is 55.96764041655171
At time: 145.79824566841125 and batch: 250, loss is 4.083961782455444 and perplexity is 59.38025611360784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.809500885009766 and perplexity of 122.6703756118345
Finished 27 epochs...
Completing Train Step...
At time: 147.3845295906067 and batch: 50, loss is 4.0705286693573 and perplexity is 58.58792804819397
At time: 148.35811281204224 and batch: 100, loss is 3.977716784477234 and perplexity is 53.39498269221247
At time: 149.30971121788025 and batch: 150, loss is 4.027673726081848 and perplexity is 56.13018506808515
At time: 150.2706422805786 and batch: 200, loss is 4.022761378288269 and perplexity is 55.85513021298391
At time: 151.2255494594574 and batch: 250, loss is 4.081165103912354 and perplexity is 59.21442062781306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.808952331542969 and perplexity of 122.60310280507196
Finished 28 epochs...
Completing Train Step...
At time: 152.83708500862122 and batch: 50, loss is 4.066661829948425 and perplexity is 58.36181539154344
At time: 153.7859010696411 and batch: 100, loss is 3.974222493171692 and perplexity is 53.20873066735957
At time: 154.74174904823303 and batch: 150, loss is 4.025146694183349 and perplexity is 55.98852136967922
At time: 155.68732571601868 and batch: 200, loss is 4.020320897102356 and perplexity is 55.718983018469544
At time: 156.63535237312317 and batch: 250, loss is 4.078261394500732 and perplexity is 59.042728549919126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8085380554199215 and perplexity of 122.55232178637635
Finished 29 epochs...
Completing Train Step...
At time: 158.21172857284546 and batch: 50, loss is 4.062858505249023 and perplexity is 58.14026803289541
At time: 159.1833372116089 and batch: 100, loss is 3.971040616035461 and perplexity is 53.039696090043265
At time: 160.13205575942993 and batch: 150, loss is 4.022527198791504 and perplexity is 55.842051618128046
At time: 161.08311581611633 and batch: 200, loss is 4.018297381401062 and perplexity is 55.60634877848095
At time: 162.03264021873474 and batch: 250, loss is 4.0754526281356815 and perplexity is 58.87712400098428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8082122802734375 and perplexity of 122.51240378829331
Finished 30 epochs...
Completing Train Step...
At time: 163.61039209365845 and batch: 50, loss is 4.059183530807495 and perplexity is 57.9269961582226
At time: 164.57938241958618 and batch: 100, loss is 3.9680506372451783 and perplexity is 52.88134537430263
At time: 165.52767062187195 and batch: 150, loss is 4.020185976028443 and perplexity is 55.71146586056625
At time: 166.49812865257263 and batch: 200, loss is 4.0160426950454715 and perplexity is 55.48111513695054
At time: 167.44808435440063 and batch: 250, loss is 4.072480034828186 and perplexity is 58.702366126942344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807799530029297 and perplexity of 122.46184719805969
Finished 31 epochs...
Completing Train Step...
At time: 169.03919672966003 and batch: 50, loss is 4.055513968467713 and perplexity is 57.71481897146322
At time: 169.9869887828827 and batch: 100, loss is 3.964909973144531 and perplexity is 52.71552336321282
At time: 170.93819999694824 and batch: 150, loss is 4.017585520744324 and perplexity is 55.566778892314225
At time: 171.88510704040527 and batch: 200, loss is 4.013387212753296 and perplexity is 55.333981460051476
At time: 172.83519768714905 and batch: 250, loss is 4.06939347743988 and perplexity is 58.52145724151475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807423400878906 and perplexity of 122.4157943889627
Finished 32 epochs...
Completing Train Step...
At time: 174.4194540977478 and batch: 50, loss is 4.051900143623352 and perplexity is 57.50662414118209
At time: 175.38626837730408 and batch: 100, loss is 3.9620895624160766 and perplexity is 52.56705340711132
At time: 176.33798480033875 and batch: 150, loss is 4.0152938222885135 and perplexity is 55.43958239460946
At time: 177.2900116443634 and batch: 200, loss is 4.011171770095825 and perplexity is 55.21152789163212
At time: 178.2449233531952 and batch: 250, loss is 4.066294956207275 and perplexity is 58.34040790115358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.807086181640625 and perplexity of 122.37452038761008
Finished 33 epochs...
Completing Train Step...
At time: 179.83748602867126 and batch: 50, loss is 4.0483618831634525 and perplexity is 57.30351027358767
At time: 180.78952479362488 and batch: 100, loss is 3.9592296171188353 and perplexity is 52.41692928564309
At time: 181.7405025959015 and batch: 150, loss is 4.012966294288635 and perplexity is 55.310695266669924
At time: 182.69536662101746 and batch: 200, loss is 4.00888503074646 and perplexity is 55.08541776371401
At time: 183.6499466896057 and batch: 250, loss is 4.0634540891647335 and perplexity is 58.17490575520213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.806673812866211 and perplexity of 122.32406735995983
Finished 34 epochs...
Completing Train Step...
At time: 185.2452745437622 and batch: 50, loss is 4.044835205078125 and perplexity is 57.10177517615633
At time: 186.1988868713379 and batch: 100, loss is 3.9562735414505004 and perplexity is 52.26220967048446
At time: 187.1549072265625 and batch: 150, loss is 4.01077410697937 and perplexity is 55.18957666827169
At time: 188.1160023212433 and batch: 200, loss is 4.006453056335449 and perplexity is 54.95161420665466
At time: 189.07109761238098 and batch: 250, loss is 4.060726895332336 and perplexity is 58.01646765493093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.806335830688477 and perplexity of 122.28273099115572
Finished 35 epochs...
Completing Train Step...
At time: 190.65258932113647 and batch: 50, loss is 4.0413986015319825 and perplexity is 56.90587582010732
At time: 191.6238603591919 and batch: 100, loss is 3.9532620525360107 and perplexity is 52.10505935240266
At time: 192.57733988761902 and batch: 150, loss is 4.008373923301697 and perplexity is 55.05727039037589
At time: 193.52700114250183 and batch: 200, loss is 4.0039807891845705 and perplexity is 54.815926932670166
At time: 194.47682571411133 and batch: 250, loss is 4.057941098213195 and perplexity is 57.85507046077348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.805825042724609 and perplexity of 122.22028639330873
Finished 36 epochs...
Completing Train Step...
At time: 196.0999903678894 and batch: 50, loss is 4.038390526771545 and perplexity is 56.73495589017684
At time: 197.05801725387573 and batch: 100, loss is 3.9500103902816774 and perplexity is 51.9359064607253
At time: 198.00474214553833 and batch: 150, loss is 4.0055812644958495 and perplexity is 54.90372871394021
At time: 198.95586156845093 and batch: 200, loss is 4.001472182273865 and perplexity is 54.67858765670279
At time: 199.9027063846588 and batch: 250, loss is 4.0552287101745605 and perplexity is 57.698357688684176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.805489349365234 and perplexity of 122.17926474051902
Finished 37 epochs...
Completing Train Step...
At time: 201.48425102233887 and batch: 50, loss is 4.035024924278259 and perplexity is 56.54432954736352
At time: 202.44992017745972 and batch: 100, loss is 3.947020716667175 and perplexity is 51.78086692581828
At time: 203.40097379684448 and batch: 150, loss is 4.002941274642945 and perplexity is 54.75897458603573
At time: 204.35130643844604 and batch: 200, loss is 3.998786506652832 and perplexity is 54.53193572474812
At time: 205.30022811889648 and batch: 250, loss is 4.052307424545288 and perplexity is 57.53005026226198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.805145263671875 and perplexity of 122.13723183537202
Finished 38 epochs...
Completing Train Step...
At time: 206.90436840057373 and batch: 50, loss is 4.031659193038941 and perplexity is 56.35433644321836
At time: 207.8729531764984 and batch: 100, loss is 3.9435952520370483 and perplexity is 51.60379684443915
At time: 208.81873512268066 and batch: 150, loss is 3.999617004394531 and perplexity is 54.57724318548576
At time: 209.76651334762573 and batch: 200, loss is 3.996106963157654 and perplexity is 54.38601062473492
At time: 210.71677541732788 and batch: 250, loss is 4.050029759407043 and perplexity is 57.39916518515015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.804807662963867 and perplexity of 122.09600517889606
Finished 39 epochs...
Completing Train Step...
At time: 212.32567071914673 and batch: 50, loss is 4.029362840652466 and perplexity is 56.22507549937912
At time: 213.27548360824585 and batch: 100, loss is 3.94118257522583 and perplexity is 51.47944363279858
At time: 214.2231481075287 and batch: 150, loss is 3.997171764373779 and perplexity is 54.443951757408136
At time: 215.17274808883667 and batch: 200, loss is 3.9936665391921995 and perplexity is 54.25344752189924
At time: 216.13883709907532 and batch: 250, loss is 4.047362642288208 and perplexity is 57.24627886257198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.804419708251953 and perplexity of 122.04864664545813
Finished 40 epochs...
Completing Train Step...
At time: 217.71837282180786 and batch: 50, loss is 4.025508856773376 and perplexity is 56.00880198980952
At time: 218.68645763397217 and batch: 100, loss is 3.938205132484436 and perplexity is 51.326394497608554
At time: 219.639812707901 and batch: 150, loss is 3.9947534465789794 and perplexity is 54.31244805301717
At time: 220.59603476524353 and batch: 200, loss is 3.9911342096328735 and perplexity is 54.11623372165117
At time: 221.54971313476562 and batch: 250, loss is 4.044821615219116 and perplexity is 57.100999176355415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.804043197631836 and perplexity of 122.00270268356276
Finished 41 epochs...
Completing Train Step...
At time: 223.1698603630066 and batch: 50, loss is 4.0220940923690796 and perplexity is 55.817871303633595
At time: 224.12539172172546 and batch: 100, loss is 3.93532488822937 and perplexity is 51.17877463740287
At time: 225.082102060318 and batch: 150, loss is 3.992089729309082 and perplexity is 54.16796756018868
At time: 226.04869079589844 and batch: 200, loss is 3.9885634851455687 and perplexity is 53.97729445826198
At time: 227.004727602005 and batch: 250, loss is 4.041960754394531 and perplexity is 56.93787461435966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.80369873046875 and perplexity of 121.96068399612503
Finished 42 epochs...
Completing Train Step...
At time: 228.60113310813904 and batch: 50, loss is 4.018557271957397 and perplexity is 55.62080222147607
At time: 229.55026483535767 and batch: 100, loss is 3.932329421043396 and perplexity is 51.025699677327644
At time: 230.50098848342896 and batch: 150, loss is 3.9894509887695313 and perplexity is 54.02522076694589
At time: 231.4540877342224 and batch: 200, loss is 3.9859078550338745 and perplexity is 53.83414089533798
At time: 232.4072916507721 and batch: 250, loss is 4.039097476005554 and perplexity is 56.77507880454567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.803153991699219 and perplexity of 121.8942653752351
Finished 43 epochs...
Completing Train Step...
At time: 233.98154497146606 and batch: 50, loss is 4.015177249908447 and perplexity is 55.433120047212775
At time: 234.94615268707275 and batch: 100, loss is 3.9296234464645385 and perplexity is 50.887812075442945
At time: 235.8962197303772 and batch: 150, loss is 3.986856026649475 and perplexity is 53.88520910657337
At time: 236.8434796333313 and batch: 200, loss is 3.9834479761123656 and perplexity is 53.70187816873216
At time: 237.7961666584015 and batch: 250, loss is 4.035965819358825 and perplexity is 56.597556865602705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.802604675292969 and perplexity of 121.82732524277644
Finished 44 epochs...
Completing Train Step...
At time: 239.4037971496582 and batch: 50, loss is 4.011873955726624 and perplexity is 55.25031024778514
At time: 240.3538691997528 and batch: 100, loss is 3.9267167949676516 and perplexity is 50.74011369814259
At time: 241.31684184074402 and batch: 150, loss is 3.984315609931946 and perplexity is 53.748491953330124
At time: 242.26667428016663 and batch: 200, loss is 3.9805805015563966 and perplexity is 53.548109968039164
At time: 243.2143359184265 and batch: 250, loss is 4.032778067588806 and perplexity is 56.41742516363943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.801605987548828 and perplexity of 121.70571851983425
Finished 45 epochs...
Completing Train Step...
At time: 244.80145359039307 and batch: 50, loss is 4.008047232627868 and perplexity is 55.03928663133602
At time: 245.7642698287964 and batch: 100, loss is 3.923395118713379 and perplexity is 50.5718510790165
At time: 246.71300387382507 and batch: 150, loss is 3.981559953689575 and perplexity is 53.60058347202281
At time: 247.668785572052 and batch: 200, loss is 3.9774054479599 and perplexity is 53.37836147178867
At time: 248.62324380874634 and batch: 250, loss is 4.0292520999908445 and perplexity is 56.218849442063835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.800751495361328 and perplexity of 121.60176635366702
Finished 46 epochs...
Completing Train Step...
At time: 250.20410251617432 and batch: 50, loss is 4.004086799621582 and perplexity is 54.82173830106692
At time: 251.17117714881897 and batch: 100, loss is 3.9201441955566407 and perplexity is 50.40771282225245
At time: 252.1264727115631 and batch: 150, loss is 3.9781704902648927 and perplexity is 53.41921380137344
At time: 253.07794642448425 and batch: 200, loss is 3.974032402038574 and perplexity is 53.19861712073347
At time: 254.024671792984 and batch: 250, loss is 4.02549825668335 and perplexity is 56.008208294612785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.800028610229492 and perplexity of 121.51389400939712
Finished 47 epochs...
Completing Train Step...
At time: 255.64220309257507 and batch: 50, loss is 4.000109467506409 and perplexity is 54.60412708362334
At time: 256.59669947624207 and batch: 100, loss is 3.917189373970032 and perplexity is 50.258986861823004
At time: 257.5505311489105 and batch: 150, loss is 3.9751365089416506 and perplexity is 53.25738651900951
At time: 258.4968023300171 and batch: 200, loss is 3.9705192375183107 and perplexity is 53.012049539730704
At time: 259.4465231895447 and batch: 250, loss is 4.021481266021729 and perplexity is 55.783675120676854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.799407577514648 and perplexity of 121.43845333389764
Finished 48 epochs...
Completing Train Step...
At time: 261.05640029907227 and batch: 50, loss is 3.995668315887451 and perplexity is 54.36215958111719
At time: 262.03806138038635 and batch: 100, loss is 3.9137742424011233 and perplexity is 50.08763856419808
At time: 262.98638582229614 and batch: 150, loss is 3.97149453163147 and perplexity is 53.063777100265675
At time: 263.93514466285706 and batch: 200, loss is 3.96693208694458 and perplexity is 52.822227998581106
At time: 264.88353991508484 and batch: 250, loss is 4.0174593210220335 and perplexity is 55.55976682271938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.798161315917969 and perplexity of 121.28720352114038
Finished 49 epochs...
Completing Train Step...
At time: 266.4929962158203 and batch: 50, loss is 3.9912712955474854 and perplexity is 54.123652803560454
At time: 267.4419195652008 and batch: 100, loss is 3.910267367362976 and perplexity is 49.91229510829218
At time: 268.39198660850525 and batch: 150, loss is 3.967570700645447 and perplexity is 52.855971770562064
At time: 269.345321893692 and batch: 200, loss is 3.963499011993408 and perplexity is 52.64119625634727
At time: 270.2996644973755 and batch: 250, loss is 4.013490295410156 and perplexity is 55.33968572787552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.796936416625977 and perplexity of 121.13872986262318
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa75b581898>
SETTINGS FOR THIS RUN
{'dropout': 0.1957311340224973, 'tune_wordvecs': True, 'lr': 25.2925964544902, 'data': 'ptb', 'num_layers': 1, 'anneal': 5.681526621650348, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2513720989227295 and batch: 50, loss is 6.77038348197937 and perplexity is 871.646090475598
At time: 2.2213363647460938 and batch: 100, loss is 5.745081434249878 and perplexity is 312.64908713729653
At time: 3.1720592975616455 and batch: 150, loss is 5.607879505157471 and perplexity is 272.56565071928
At time: 4.12335205078125 and batch: 200, loss is 5.619573602676391 and perplexity is 275.77176981696425
At time: 5.071627140045166 and batch: 250, loss is 5.720179500579834 and perplexity is 304.9596584797558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.8019264221191404 and perplexity of 330.9364695692593
Finished 1 epochs...
Completing Train Step...
At time: 6.691122055053711 and batch: 50, loss is 5.585452184677124 and perplexity is 266.52077210497106
At time: 7.638661623001099 and batch: 100, loss is 5.59421480178833 and perplexity is 268.86645372896794
At time: 8.584709167480469 and batch: 150, loss is 5.620298681259155 and perplexity is 275.9717985304938
At time: 9.535074234008789 and batch: 200, loss is 5.622978258132934 and perplexity is 276.71227782218267
At time: 10.483497858047485 and batch: 250, loss is 5.700284051895141 and perplexity is 298.95230687697233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.815327835083008 and perplexity of 335.40133680439794
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 12.128118515014648 and batch: 50, loss is 5.461083307266235 and perplexity is 235.3522451129899
At time: 13.076820373535156 and batch: 100, loss is 5.286687526702881 and perplexity is 197.68750506811716
At time: 14.034148216247559 and batch: 150, loss is 5.202953271865844 and perplexity is 181.8083793773661
At time: 14.983628988265991 and batch: 200, loss is 5.138250713348389 and perplexity is 170.41739854979247
At time: 15.923572063446045 and batch: 250, loss is 5.209556369781494 and perplexity is 183.0128501511773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.467852783203125 and perplexity of 236.95086126494388
Finished 3 epochs...
Completing Train Step...
At time: 17.522220849990845 and batch: 50, loss is 5.187234878540039 and perplexity is 178.97298605179427
At time: 18.48241400718689 and batch: 100, loss is 5.102530813217163 and perplexity is 164.4375418356434
At time: 19.428789615631104 and batch: 150, loss is 5.094915781021118 and perplexity is 163.19010034201656
At time: 20.370398998260498 and batch: 200, loss is 5.089611692428589 and perplexity is 162.3268170817455
At time: 21.316537141799927 and batch: 250, loss is 5.144613409042359 and perplexity is 171.5051695057468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.420920181274414 and perplexity of 226.08706789989844
Finished 4 epochs...
Completing Train Step...
At time: 22.9275221824646 and batch: 50, loss is 5.111304244995117 and perplexity is 165.88657057338554
At time: 23.87251091003418 and batch: 100, loss is 5.057128915786743 and perplexity is 157.1387095777575
At time: 24.816635847091675 and batch: 150, loss is 5.070223178863525 and perplexity is 159.20985565321243
At time: 25.76586103439331 and batch: 200, loss is 5.051876258850098 and perplexity is 156.31547781466782
At time: 26.7182457447052 and batch: 250, loss is 5.100375032424926 and perplexity is 164.08343236976444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.392453002929687 and perplexity of 219.7417521383642
Finished 5 epochs...
Completing Train Step...
At time: 28.315956592559814 and batch: 50, loss is 5.070630149841309 and perplexity is 159.2746626302253
At time: 29.287076473236084 and batch: 100, loss is 5.024898500442505 and perplexity is 152.15481168282483
At time: 30.232044219970703 and batch: 150, loss is 5.028769264221191 and perplexity is 152.74490834225176
At time: 31.17737126350403 and batch: 200, loss is 5.018021869659424 and perplexity is 151.11208854106576
At time: 32.12812066078186 and batch: 250, loss is 5.070223598480225 and perplexity is 159.2099224603406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.3727153778076175 and perplexity of 215.4470944061191
Finished 6 epochs...
Completing Train Step...
At time: 33.74040389060974 and batch: 50, loss is 5.037043466567993 and perplexity is 154.01399371552594
At time: 34.722065925598145 and batch: 100, loss is 4.991850471496582 and perplexity is 147.20857688757044
At time: 35.66893696784973 and batch: 150, loss is 5.000592470169067 and perplexity is 148.50111552524737
At time: 36.6272611618042 and batch: 200, loss is 4.989201002120971 and perplexity is 146.8190684944115
At time: 37.579694747924805 and batch: 250, loss is 5.038792819976806 and perplexity is 154.28365441760258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.354667663574219 and perplexity of 211.59364439111008
Finished 7 epochs...
Completing Train Step...
At time: 39.20477604866028 and batch: 50, loss is 5.006435108184815 and perplexity is 149.3712933799082
At time: 40.15216612815857 and batch: 100, loss is 4.960392093658447 and perplexity is 142.64971698252924
At time: 41.1060836315155 and batch: 150, loss is 4.973273782730103 and perplexity is 144.4991727655334
At time: 42.052053928375244 and batch: 200, loss is 4.964970512390137 and perplexity is 143.30432450902754
At time: 42.99638915061951 and batch: 250, loss is 5.0148670101165775 and perplexity is 150.63610235606788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.337202072143555 and perplexity of 207.93012216838244
Finished 8 epochs...
Completing Train Step...
At time: 44.60891056060791 and batch: 50, loss is 4.981231956481934 and perplexity is 145.65371020084572
At time: 45.58149266242981 and batch: 100, loss is 4.937101726531982 and perplexity is 139.3657436216531
At time: 46.52795600891113 and batch: 150, loss is 4.954196214675903 and perplexity is 141.7686090370416
At time: 47.55902814865112 and batch: 200, loss is 4.943273344039917 and perplexity is 140.2285152948792
At time: 48.50777244567871 and batch: 250, loss is 4.992835397720337 and perplexity is 147.35363790077994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.332312774658203 and perplexity of 206.91597120809757
Finished 9 epochs...
Completing Train Step...
At time: 50.14884948730469 and batch: 50, loss is 4.963618268966675 and perplexity is 143.11067314006027
At time: 51.10004281997681 and batch: 100, loss is 4.91777777671814 and perplexity is 136.6985008177126
At time: 52.044572591781616 and batch: 150, loss is 4.933280639648437 and perplexity is 138.83423113109257
At time: 52.994863986968994 and batch: 200, loss is 4.9254732418060305 and perplexity is 137.75451741630283
At time: 53.94011092185974 and batch: 250, loss is 4.972034702301025 and perplexity is 144.32023754899774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.320513916015625 and perplexity of 204.4889451377639
Finished 10 epochs...
Completing Train Step...
At time: 55.58829212188721 and batch: 50, loss is 4.944156579971313 and perplexity is 140.35242487083363
At time: 56.54616737365723 and batch: 100, loss is 4.903966779708862 and perplexity is 134.82353560427
At time: 57.49489116668701 and batch: 150, loss is 4.916380910873413 and perplexity is 136.50768465421302
At time: 58.44127702713013 and batch: 200, loss is 4.904955806732178 and perplexity is 134.956945686575
At time: 59.39853549003601 and batch: 250, loss is 4.948441734313965 and perplexity is 140.95514713043193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.307153701782227 and perplexity of 201.77509817804693
Finished 11 epochs...
Completing Train Step...
At time: 61.027745723724365 and batch: 50, loss is 4.9242414855957035 and perplexity is 137.58494189326683
At time: 61.99151825904846 and batch: 100, loss is 4.878180990219116 and perplexity is 131.39144405832147
At time: 62.94313645362854 and batch: 150, loss is 4.898299131393433 and perplexity is 134.06156455148255
At time: 63.88885140419006 and batch: 200, loss is 4.887051553726196 and perplexity is 132.5621449187347
At time: 64.84150981903076 and batch: 250, loss is 4.931317663192749 and perplexity is 138.56197011254795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.291470718383789 and perplexity of 198.63534734629488
Finished 12 epochs...
Completing Train Step...
At time: 66.48548293113708 and batch: 50, loss is 4.904802284240723 and perplexity is 134.93622835036322
At time: 67.43637251853943 and batch: 100, loss is 4.861372671127319 and perplexity is 129.20143151535368
At time: 68.3834547996521 and batch: 150, loss is 4.880265970230102 and perplexity is 131.66567837999256
At time: 69.33008337020874 and batch: 200, loss is 4.865120058059692 and perplexity is 129.68650758766904
At time: 70.27633261680603 and batch: 250, loss is 4.909818048477173 and perplexity is 135.61473685558545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.28455924987793 and perplexity of 197.26721873397338
Finished 13 epochs...
Completing Train Step...
At time: 71.870032787323 and batch: 50, loss is 4.881343030929566 and perplexity is 131.80756670509663
At time: 72.85101699829102 and batch: 100, loss is 4.833628253936768 and perplexity is 125.6660829660685
At time: 73.79681277275085 and batch: 150, loss is 4.854591789245606 and perplexity is 128.3282955308781
At time: 74.74643516540527 and batch: 200, loss is 4.850172557830811 and perplexity is 127.762434352529
At time: 75.69290924072266 and batch: 250, loss is 4.89092324256897 and perplexity is 133.0763791312594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.287139129638672 and perplexity of 197.77680148752006
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 77.3022129535675 and batch: 50, loss is 4.836530971527099 and perplexity is 126.03138604494204
At time: 78.27788949012756 and batch: 100, loss is 4.734568738937378 and perplexity is 113.81436442899808
At time: 79.22509050369263 and batch: 150, loss is 4.723223228454589 and perplexity is 112.53037986733887
At time: 80.17589139938354 and batch: 200, loss is 4.7025351810455325 and perplexity is 110.22626206159893
At time: 81.12063980102539 and batch: 250, loss is 4.765664930343628 and perplexity is 117.40916025163708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.17035026550293 and perplexity of 175.9764651754866
Finished 15 epochs...
Completing Train Step...
At time: 82.72831583023071 and batch: 50, loss is 4.757281990051269 and perplexity is 116.42904015530947
At time: 83.67453575134277 and batch: 100, loss is 4.685348443984985 and perplexity is 108.34801894699439
At time: 84.62200212478638 and batch: 150, loss is 4.704134340286255 and perplexity is 110.40267242368071
At time: 85.5666275024414 and batch: 200, loss is 4.706904554367066 and perplexity is 110.70893547263587
At time: 86.51499843597412 and batch: 250, loss is 4.759968347549439 and perplexity is 116.74223066282411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.163782119750977 and perplexity of 174.8244136660881
Finished 16 epochs...
Completing Train Step...
At time: 88.1054835319519 and batch: 50, loss is 4.737574577331543 and perplexity is 114.15698669101332
At time: 89.07339787483215 and batch: 100, loss is 4.674638166427612 and perplexity is 107.193773768004
At time: 90.02070713043213 and batch: 150, loss is 4.702702779769897 and perplexity is 110.24473739068948
At time: 90.96976137161255 and batch: 200, loss is 4.699851741790772 and perplexity is 109.93087308951304
At time: 91.91462278366089 and batch: 250, loss is 4.749099950790406 and perplexity is 115.48029978596925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.161927795410156 and perplexity of 174.50053288327277
Finished 17 epochs...
Completing Train Step...
At time: 93.54567527770996 and batch: 50, loss is 4.731466379165649 and perplexity is 113.4618184683122
At time: 94.49008393287659 and batch: 100, loss is 4.667947158813477 and perplexity is 106.47893357903082
At time: 95.43679523468018 and batch: 150, loss is 4.695467643737793 and perplexity is 109.44998027371015
At time: 96.38399481773376 and batch: 200, loss is 4.692992362976074 and perplexity is 109.17939586751194
At time: 97.3549177646637 and batch: 250, loss is 4.7423773193359375 and perplexity is 114.706571947779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.157056427001953 and perplexity of 173.6525436111603
Finished 18 epochs...
Completing Train Step...
At time: 98.96410512924194 and batch: 50, loss is 4.723572263717651 and perplexity is 112.56966379341722
At time: 99.91541194915771 and batch: 100, loss is 4.662052612304688 and perplexity is 105.85313476493306
At time: 100.86712145805359 and batch: 150, loss is 4.690304403305054 and perplexity is 108.88632011886922
At time: 101.81473207473755 and batch: 200, loss is 4.686378774642944 and perplexity is 108.45971076251465
At time: 102.7666244506836 and batch: 250, loss is 4.736733322143555 and perplexity is 114.0609919174264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.154018783569336 and perplexity of 173.12584946201034
Finished 19 epochs...
Completing Train Step...
At time: 104.35480785369873 and batch: 50, loss is 4.718129997253418 and perplexity is 111.95869372620666
At time: 105.32541823387146 and batch: 100, loss is 4.655948305130005 and perplexity is 105.20894288782576
At time: 106.27587532997131 and batch: 150, loss is 4.686425981521606 and perplexity is 108.46483092777291
At time: 107.21855425834656 and batch: 200, loss is 4.680349044799804 and perplexity is 107.80769572050637
At time: 108.16938138008118 and batch: 250, loss is 4.729120903015136 and perplexity is 113.19600832663241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.149549865722657 and perplexity of 172.3538904590575
Finished 20 epochs...
Completing Train Step...
At time: 109.79843330383301 and batch: 50, loss is 4.712078485488892 and perplexity is 111.28322025317922
At time: 110.760990858078 and batch: 100, loss is 4.650841073989868 and perplexity is 104.67298629058176
At time: 111.71305847167969 and batch: 150, loss is 4.6809146690368655 and perplexity is 107.8686916148995
At time: 112.66561603546143 and batch: 200, loss is 4.673780155181885 and perplexity is 107.1018397504896
At time: 113.62728524208069 and batch: 250, loss is 4.722911567687988 and perplexity is 112.49531402749056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.147705078125 and perplexity of 172.03622724020434
Finished 21 epochs...
Completing Train Step...
At time: 115.2553083896637 and batch: 50, loss is 4.705605983734131 and perplexity is 110.56526540328314
At time: 116.24093866348267 and batch: 100, loss is 4.64446515083313 and perplexity is 104.0077224626422
At time: 117.19218683242798 and batch: 150, loss is 4.675715770721435 and perplexity is 107.30934849961697
At time: 118.13850927352905 and batch: 200, loss is 4.668556575775146 and perplexity is 106.54384342378566
At time: 119.08694672584534 and batch: 250, loss is 4.716708383560181 and perplexity is 111.7996447939848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.145127487182617 and perplexity of 171.59335923059373
Finished 22 epochs...
Completing Train Step...
At time: 120.71063280105591 and batch: 50, loss is 4.699891939163208 and perplexity is 109.93529211057675
At time: 121.67965412139893 and batch: 100, loss is 4.639593458175659 and perplexity is 103.50226103086588
At time: 122.63019371032715 and batch: 150, loss is 4.67135684967041 and perplexity is 106.84261349100167
At time: 123.57858848571777 and batch: 200, loss is 4.6622070217132565 and perplexity is 105.86948074682147
At time: 124.52889108657837 and batch: 250, loss is 4.710652933120728 and perplexity is 111.12469321615347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.142781066894531 and perplexity of 171.19120109171533
Finished 23 epochs...
Completing Train Step...
At time: 126.13910245895386 and batch: 50, loss is 4.694673366546631 and perplexity is 109.36308116636366
At time: 127.09273838996887 and batch: 100, loss is 4.633911933898926 and perplexity is 102.91587777483089
At time: 128.04484701156616 and batch: 150, loss is 4.664990749359131 and perplexity is 106.1646031268161
At time: 128.99067878723145 and batch: 200, loss is 4.656018161773682 and perplexity is 105.2162926881739
At time: 129.9369764328003 and batch: 250, loss is 4.70457685470581 and perplexity is 110.45153800925137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.137540817260742 and perplexity of 170.29646283626886
Finished 24 epochs...
Completing Train Step...
At time: 131.52252578735352 and batch: 50, loss is 4.688627891540527 and perplexity is 108.70392385965276
At time: 132.483234167099 and batch: 100, loss is 4.626531915664673 and perplexity is 102.15915247806569
At time: 133.4322965145111 and batch: 150, loss is 4.6595934009552 and perplexity is 105.59313935740036
At time: 134.38230419158936 and batch: 200, loss is 4.651329708099365 and perplexity is 104.7241455800953
At time: 135.33196330070496 and batch: 250, loss is 4.699705047607422 and perplexity is 109.91474805261372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.135529708862305 and perplexity of 169.95432234575884
Finished 25 epochs...
Completing Train Step...
At time: 136.94434142112732 and batch: 50, loss is 4.683449497222901 and perplexity is 108.14246705505487
At time: 137.89142537117004 and batch: 100, loss is 4.6206239986419675 and perplexity is 101.557384031457
At time: 138.84151339530945 and batch: 150, loss is 4.654957942962646 and perplexity is 105.10479950946491
At time: 139.7831163406372 and batch: 200, loss is 4.647031555175781 and perplexity is 104.27499114629919
At time: 140.76005601882935 and batch: 250, loss is 4.695671195983887 and perplexity is 109.47226133063249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.13335952758789 and perplexity of 169.58589058435285
Finished 26 epochs...
Completing Train Step...
At time: 142.44564032554626 and batch: 50, loss is 4.678335638046264 and perplexity is 107.59085334706178
At time: 143.39450669288635 and batch: 100, loss is 4.616396503448486 and perplexity is 101.12895690347412
At time: 144.33584237098694 and batch: 150, loss is 4.651239557266235 and perplexity is 104.71470503666522
At time: 145.28447270393372 and batch: 200, loss is 4.6429569530487065 and perplexity is 103.85097647774664
At time: 146.23222994804382 and batch: 250, loss is 4.691691045761108 and perplexity is 109.03741124370427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.131113052368164 and perplexity of 169.20534768367702
Finished 27 epochs...
Completing Train Step...
At time: 147.82108521461487 and batch: 50, loss is 4.673187046051026 and perplexity is 107.03833550573708
At time: 148.79416799545288 and batch: 100, loss is 4.611135988235474 and perplexity is 100.59836330863946
At time: 149.742858171463 and batch: 150, loss is 4.648184213638306 and perplexity is 104.3952538948958
At time: 150.69268345832825 and batch: 200, loss is 4.638940210342407 and perplexity is 103.43467048220415
At time: 151.64144134521484 and batch: 250, loss is 4.687159538269043 and perplexity is 108.54442522625776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.129312133789062 and perplexity of 168.90089685721063
Finished 28 epochs...
Completing Train Step...
At time: 153.26069951057434 and batch: 50, loss is 4.668785924911499 and perplexity is 106.5682819646306
At time: 154.20787858963013 and batch: 100, loss is 4.6073228454589845 and perplexity is 100.21549781057372
At time: 155.15902590751648 and batch: 150, loss is 4.644751987457275 and perplexity is 104.03755996567816
At time: 156.10233330726624 and batch: 200, loss is 4.635490255355835 and perplexity is 103.0784403672468
At time: 157.04874110221863 and batch: 250, loss is 4.683169898986816 and perplexity is 108.11223483865459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.127878189086914 and perplexity of 168.6588758747972
Finished 29 epochs...
Completing Train Step...
At time: 158.637286901474 and batch: 50, loss is 4.665082168579102 and perplexity is 106.17430905566995
At time: 159.60042881965637 and batch: 100, loss is 4.604754915237427 and perplexity is 99.95848154622995
At time: 160.54519271850586 and batch: 150, loss is 4.6423330497741695 and perplexity is 103.78620372152676
At time: 161.4927089214325 and batch: 200, loss is 4.632535705566406 and perplexity is 102.77433944484049
At time: 162.44009637832642 and batch: 250, loss is 4.679520616531372 and perplexity is 107.71842176145827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.122700500488281 and perplexity of 167.78786958165915
Finished 30 epochs...
Completing Train Step...
At time: 164.05654788017273 and batch: 50, loss is 4.6616452693939205 and perplexity is 105.81002502172429
At time: 165.0255217552185 and batch: 100, loss is 4.600713033676147 and perplexity is 99.55527660500665
At time: 165.9772276878357 and batch: 150, loss is 4.637782697677612 and perplexity is 103.3150128071394
At time: 166.9249017238617 and batch: 200, loss is 4.627609415054321 and perplexity is 102.2692882274535
At time: 167.86816096305847 and batch: 250, loss is 4.67521713256836 and perplexity is 107.25585330275489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1203765869140625 and perplexity of 167.39839779861177
Finished 31 epochs...
Completing Train Step...
At time: 169.48344898223877 and batch: 50, loss is 4.656006336212158 and perplexity is 105.2150484537883
At time: 170.43280148506165 and batch: 100, loss is 4.59209267616272 and perplexity is 98.70076292663671
At time: 171.39078330993652 and batch: 150, loss is 4.632322998046875 and perplexity is 102.75248089484711
At time: 172.3428897857666 and batch: 200, loss is 4.623670206069947 and perplexity is 101.86722056264027
At time: 173.29207015037537 and batch: 250, loss is 4.672043380737304 and perplexity is 106.9159894489868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.118582153320313 and perplexity of 167.09828183967235
Finished 32 epochs...
Completing Train Step...
At time: 174.87452864646912 and batch: 50, loss is 4.649401750564575 and perplexity is 104.5224363804151
At time: 175.84486603736877 and batch: 100, loss is 4.587559404373169 and perplexity is 98.25433818931091
At time: 176.80794739723206 and batch: 150, loss is 4.628894233703614 and perplexity is 102.40077016335255
At time: 177.7645013332367 and batch: 200, loss is 4.620148067474365 and perplexity is 101.5090612071775
At time: 178.70905470848083 and batch: 250, loss is 4.666950979232788 and perplexity is 106.37291425553606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.114175033569336 and perplexity of 166.363480070437
Finished 33 epochs...
Completing Train Step...
At time: 180.433753490448 and batch: 50, loss is 4.644568519592285 and perplexity is 104.01847416754133
At time: 181.38939833641052 and batch: 100, loss is 4.58375039100647 and perplexity is 97.88079796329674
At time: 182.35094714164734 and batch: 150, loss is 4.6247890853881835 and perplexity is 101.98126147604657
At time: 183.29946875572205 and batch: 200, loss is 4.615682849884033 and perplexity is 101.05681160934563
At time: 184.24649286270142 and batch: 250, loss is 4.662816581726074 and perplexity is 105.93403422147206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1133369445800785 and perplexity of 166.2241110795414
Finished 34 epochs...
Completing Train Step...
At time: 185.88812565803528 and batch: 50, loss is 4.640571756362915 and perplexity is 103.60356665068171
At time: 186.8370943069458 and batch: 100, loss is 4.578997707366943 and perplexity is 97.41670521278103
At time: 187.78720045089722 and batch: 150, loss is 4.622670831680298 and perplexity is 101.76546792422454
At time: 188.73622393608093 and batch: 200, loss is 4.612286319732666 and perplexity is 100.71415135907192
At time: 189.6910698413849 and batch: 250, loss is 4.659872350692749 and perplexity is 105.62259864455025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.111306762695312 and perplexity of 165.88698822656238
Finished 35 epochs...
Completing Train Step...
At time: 191.28740811347961 and batch: 50, loss is 4.6368956375122075 and perplexity is 103.22340681081191
At time: 192.32840180397034 and batch: 100, loss is 4.57540997505188 and perplexity is 97.067826367641
At time: 193.2845277786255 and batch: 150, loss is 4.618314361572265 and perplexity is 101.3230939992179
At time: 194.2399411201477 and batch: 200, loss is 4.608163356781006 and perplexity is 100.29976548012971
At time: 195.19459986686707 and batch: 250, loss is 4.656296844482422 and perplexity is 105.2456187357652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.110076904296875 and perplexity of 165.68309612580651
Finished 36 epochs...
Completing Train Step...
At time: 196.78875017166138 and batch: 50, loss is 4.63292405128479 and perplexity is 102.81425917034916
At time: 197.7372007369995 and batch: 100, loss is 4.57240891456604 and perplexity is 96.77695662663238
At time: 198.68611025810242 and batch: 150, loss is 4.614739084243775 and perplexity is 100.9614826540067
At time: 199.63666653633118 and batch: 200, loss is 4.604628868103028 and perplexity is 99.94588286010296
At time: 200.58943605422974 and batch: 250, loss is 4.653087368011475 and perplexity is 104.90837687316066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.110497283935547 and perplexity of 165.7527605676204
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 202.19416117668152 and batch: 50, loss is 4.626938209533692 and perplexity is 102.2006675484699
At time: 203.15206742286682 and batch: 100, loss is 4.55553243637085 and perplexity is 95.157407014407
At time: 204.09767889976501 and batch: 150, loss is 4.588858232498169 and perplexity is 98.38203659834524
At time: 205.04318857192993 and batch: 200, loss is 4.575733404159546 and perplexity is 97.09922600561094
At time: 205.98736572265625 and batch: 250, loss is 4.631136951446533 and perplexity is 102.63068390693452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.094443130493164 and perplexity of 163.11298668027501
Finished 38 epochs...
Completing Train Step...
At time: 207.55750632286072 and batch: 50, loss is 4.6121149921417235 and perplexity is 100.69689772419979
At time: 208.53288507461548 and batch: 100, loss is 4.54660626411438 and perplexity is 94.31179526061712
At time: 209.48192954063416 and batch: 150, loss is 4.585352115631103 and perplexity is 98.03770167239558
At time: 210.43245458602905 and batch: 200, loss is 4.576811218261719 and perplexity is 97.20393734024964
At time: 211.38515949249268 and batch: 250, loss is 4.630029335021972 and perplexity is 102.51707140692596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.091763305664062 and perplexity of 162.67645762056972
Finished 39 epochs...
Completing Train Step...
At time: 212.99870085716248 and batch: 50, loss is 4.608319854736328 and perplexity is 100.31546341666207
At time: 213.9492404460907 and batch: 100, loss is 4.545365104675293 and perplexity is 94.19481189823189
At time: 214.89817905426025 and batch: 150, loss is 4.586536588668824 and perplexity is 98.15389348615739
At time: 215.85842776298523 and batch: 200, loss is 4.577014932632446 and perplexity is 97.22374119627364
At time: 216.81356167793274 and batch: 250, loss is 4.628093671798706 and perplexity is 102.31882481326593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.090878677368164 and perplexity of 162.53261305694838
Finished 40 epochs...
Completing Train Step...
At time: 218.4105257987976 and batch: 50, loss is 4.606067895889282 and perplexity is 100.08981129633261
At time: 219.38395762443542 and batch: 100, loss is 4.545096702575684 and perplexity is 94.169533205525
At time: 220.32587122917175 and batch: 150, loss is 4.58663119316101 and perplexity is 98.16317972465981
At time: 221.28469562530518 and batch: 200, loss is 4.576974983215332 and perplexity is 97.21985724206458
At time: 222.2318835258484 and batch: 250, loss is 4.626781005859375 and perplexity is 102.18460249078974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.090467834472657 and perplexity of 162.46585140280035
Finished 41 epochs...
Completing Train Step...
At time: 223.8432478904724 and batch: 50, loss is 4.6042587852478025 and perplexity is 99.90890144592186
At time: 224.78496384620667 and batch: 100, loss is 4.54474289894104 and perplexity is 94.13622157563952
At time: 225.72955465316772 and batch: 150, loss is 4.586791200637817 and perplexity is 98.17888782403607
At time: 226.67148399353027 and batch: 200, loss is 4.576701889038086 and perplexity is 97.19331069015841
At time: 227.61623525619507 and batch: 250, loss is 4.6255756187438966 and perplexity is 102.06150469270163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.090000152587891 and perplexity of 162.38988683221757
Finished 42 epochs...
Completing Train Step...
At time: 229.2168996334076 and batch: 50, loss is 4.602634773254395 and perplexity is 99.7467798710483
At time: 230.16240310668945 and batch: 100, loss is 4.544412384033203 and perplexity is 94.10511329220022
At time: 231.10565876960754 and batch: 150, loss is 4.586868124008179 and perplexity is 98.18644036546557
At time: 232.04880094528198 and batch: 200, loss is 4.576316204071045 and perplexity is 97.15583191929211
At time: 232.99324536323547 and batch: 250, loss is 4.624346914291382 and perplexity is 101.93617827778468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.089546585083008 and perplexity of 162.31624875761025
Finished 43 epochs...
Completing Train Step...
At time: 234.56821727752686 and batch: 50, loss is 4.6012905502319335 and perplexity is 99.61278803076374
At time: 235.52740573883057 and batch: 100, loss is 4.544089002609253 and perplexity is 94.07468636667922
At time: 236.47691559791565 and batch: 150, loss is 4.586981859207153 and perplexity is 98.19760825487616
At time: 237.43967580795288 and batch: 200, loss is 4.575801286697388 and perplexity is 97.10581757121822
At time: 238.38618302345276 and batch: 250, loss is 4.623007297515869 and perplexity is 101.79971428846461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.089007186889648 and perplexity of 162.22871927502206
Finished 44 epochs...
Completing Train Step...
At time: 240.02761125564575 and batch: 50, loss is 4.600128698348999 and perplexity is 99.49711993303609
At time: 240.9711434841156 and batch: 100, loss is 4.543797874450684 and perplexity is 94.04730256276095
At time: 241.9216320514679 and batch: 150, loss is 4.587054519653321 and perplexity is 98.20474359612992
At time: 242.86685991287231 and batch: 200, loss is 4.5754741668701175 and perplexity is 97.07405752790054
At time: 243.80984616279602 and batch: 250, loss is 4.621840314865112 and perplexity is 101.68098507897606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.088426208496093 and perplexity of 162.13449526801563
Finished 45 epochs...
Completing Train Step...
At time: 245.39014053344727 and batch: 50, loss is 4.5990573596954345 and perplexity is 99.390581901879
At time: 246.3605215549469 and batch: 100, loss is 4.543533601760864 and perplexity is 94.0224517129875
At time: 247.30823683738708 and batch: 150, loss is 4.58707221031189 and perplexity is 98.20648091808589
At time: 248.25239157676697 and batch: 200, loss is 4.575040998458863 and perplexity is 97.03201721855147
At time: 249.19680094718933 and batch: 250, loss is 4.620751190185547 and perplexity is 101.57030209341053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.088035964965821 and perplexity of 162.07123567432964
Finished 46 epochs...
Completing Train Step...
At time: 250.76667952537537 and batch: 50, loss is 4.598014268875122 and perplexity is 99.28696254986426
At time: 251.72577214241028 and batch: 100, loss is 4.543260154724121 and perplexity is 93.99674506704247
At time: 252.67143893241882 and batch: 150, loss is 4.587011213302612 and perplexity is 98.20049079914976
At time: 253.6188108921051 and batch: 200, loss is 4.574592189788818 and perplexity is 96.98847817903196
At time: 254.56674933433533 and batch: 250, loss is 4.619747810363769 and perplexity is 101.46843961372254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.087698364257813 and perplexity of 162.01652954535183
Finished 47 epochs...
Completing Train Step...
At time: 256.21687293052673 and batch: 50, loss is 4.597123870849609 and perplexity is 99.19859698055296
At time: 257.1648428440094 and batch: 100, loss is 4.542926521301269 and perplexity is 93.96538984211523
At time: 258.11572527885437 and batch: 150, loss is 4.586971378326416 and perplexity is 98.19657906284876
At time: 259.0702528953552 and batch: 200, loss is 4.574107751846314 and perplexity is 96.9415046590126
At time: 260.0188901424408 and batch: 250, loss is 4.61866418838501 and perplexity is 101.35854573487136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0872962951660154 and perplexity of 161.9514008004664
Finished 48 epochs...
Completing Train Step...
At time: 261.60739254951477 and batch: 50, loss is 4.596362581253052 and perplexity is 99.12310685924653
At time: 262.56405305862427 and batch: 100, loss is 4.542605266571045 and perplexity is 93.93520786446221
At time: 263.5128629207611 and batch: 150, loss is 4.586856908798218 and perplexity is 98.18533919009654
At time: 264.4582734107971 and batch: 200, loss is 4.573768253326416 and perplexity is 96.90859874773435
At time: 265.4075710773468 and batch: 250, loss is 4.6177719211578365 and perplexity is 101.2681471621576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.087088775634766 and perplexity of 161.91779620861223
Finished 49 epochs...
Completing Train Step...
At time: 266.9958701133728 and batch: 50, loss is 4.595530948638916 and perplexity is 99.04070711867278
At time: 267.9416525363922 and batch: 100, loss is 4.5421989631652835 and perplexity is 93.89704942206345
At time: 268.89016675949097 and batch: 150, loss is 4.58675555229187 and perplexity is 98.17538797146055
At time: 269.8376805782318 and batch: 200, loss is 4.5733600521087645 and perplexity is 96.86904861248047
At time: 270.8001358509064 and batch: 250, loss is 4.616858997344971 and perplexity is 101.1757392462399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.086792755126953 and perplexity of 161.8698723139324
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa75b581898>
SETTINGS FOR THIS RUN
{'dropout': 0.0, 'tune_wordvecs': True, 'lr': 2.0113506699736963, 'data': 'ptb', 'num_layers': 1, 'anneal': 2.0, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2190117835998535 and batch: 50, loss is 6.748381681442261 and perplexity is 852.6777411490202
At time: 2.18442964553833 and batch: 100, loss is 5.952188024520874 and perplexity is 384.5939200474964
At time: 3.127756118774414 and batch: 150, loss is 5.770426816940308 and perplexity is 320.6745727824247
At time: 4.069941759109497 and batch: 200, loss is 5.634642305374146 and perplexity is 279.9587596547515
At time: 5.01045036315918 and batch: 250, loss is 5.550039281845093 and perplexity is 257.2476608700675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.677333068847656 and perplexity of 292.16919484090465
Finished 1 epochs...
Completing Train Step...
At time: 6.595609426498413 and batch: 50, loss is 5.399261608123779 and perplexity is 221.24299184815237
At time: 7.53899884223938 and batch: 100, loss is 5.264558258056641 and perplexity is 193.36087431042756
At time: 8.483916521072388 and batch: 150, loss is 5.183450746536255 and perplexity is 178.2970084483513
At time: 9.426309585571289 and batch: 200, loss is 5.0851702308654785 and perplexity is 161.60744747180917
At time: 10.367250680923462 and batch: 250, loss is 5.067693405151367 and perplexity is 158.80759976837413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.245421981811523 and perplexity of 189.69584653941465
Finished 2 epochs...
Completing Train Step...
At time: 11.947930097579956 and batch: 50, loss is 4.962947845458984 and perplexity is 143.0147605352107
At time: 12.918763160705566 and batch: 100, loss is 4.855795669555664 and perplexity is 128.48288047130367
At time: 13.893689632415771 and batch: 150, loss is 4.846555910110474 and perplexity is 127.30119720447703
At time: 14.84430193901062 and batch: 200, loss is 4.792539625167847 and perplexity is 120.60727732905427
At time: 15.790230512619019 and batch: 250, loss is 4.8085649299621585 and perplexity is 122.555615368181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.013905715942383 and perplexity of 150.49136632654634
Finished 3 epochs...
Completing Train Step...
At time: 17.368910551071167 and batch: 50, loss is 4.7359499263763425 and perplexity is 113.97167201014096
At time: 18.34220862388611 and batch: 100, loss is 4.639928379058838 and perplexity is 103.53693190521722
At time: 19.28103232383728 and batch: 150, loss is 4.6619863700866695 and perplexity is 105.84612305074033
At time: 20.224589824676514 and batch: 200, loss is 4.62366810798645 and perplexity is 101.8670068369302
At time: 21.175875902175903 and batch: 250, loss is 4.65320689201355 and perplexity is 104.92091669160548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.8942138671875 and perplexity of 133.51500482102588
Finished 4 epochs...
Completing Train Step...
At time: 22.783506393432617 and batch: 50, loss is 4.590972118377685 and perplexity is 98.59022496200525
At time: 23.72457218170166 and batch: 100, loss is 4.499857454299927 and perplexity is 90.00430066002006
At time: 24.689023733139038 and batch: 150, loss is 4.536379737854004 and perplexity is 93.35222809190631
At time: 25.631144046783447 and batch: 200, loss is 4.506125545501709 and perplexity is 90.57022761309811
At time: 26.580424308776855 and batch: 250, loss is 4.543082580566407 and perplexity is 93.98005515610161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.816374588012695 and perplexity of 123.51647994461699
Finished 5 epochs...
Completing Train Step...
At time: 28.169588565826416 and batch: 50, loss is 4.482683181762695 and perplexity is 88.47174024567244
At time: 29.142794132232666 and batch: 100, loss is 4.394616470336914 and perplexity is 81.0135537026643
At time: 30.09109330177307 and batch: 150, loss is 4.441467981338501 and perplexity is 84.89948109458564
At time: 31.036311864852905 and batch: 200, loss is 4.415726099014282 and perplexity is 82.74189791031637
At time: 31.98147201538086 and batch: 250, loss is 4.456657810211182 and perplexity is 86.19893393167254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.763779830932617 and perplexity of 117.18804079435962
Finished 6 epochs...
Completing Train Step...
At time: 33.57292675971985 and batch: 50, loss is 4.397198066711426 and perplexity is 81.22296819473014
At time: 34.539371967315674 and batch: 100, loss is 4.311037168502808 and perplexity is 74.51773632315948
At time: 35.48133134841919 and batch: 150, loss is 4.366266107559204 and perplexity is 78.74904160679198
At time: 36.43448758125305 and batch: 200, loss is 4.3429033565521244 and perplexity is 76.93057229065161
At time: 37.38246250152588 and batch: 250, loss is 4.386511163711548 and perplexity is 80.35956796690184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.727090835571289 and perplexity of 112.96644588781062
Finished 7 epochs...
Completing Train Step...
At time: 39.016377449035645 and batch: 50, loss is 4.3265156745910645 and perplexity is 75.68013242857025
At time: 39.9659788608551 and batch: 100, loss is 4.241924476623535 and perplexity is 69.54155423941583
At time: 40.914238691329956 and batch: 150, loss is 4.304425954818726 and perplexity is 74.02670857823763
At time: 41.86051869392395 and batch: 200, loss is 4.281137447357178 and perplexity is 72.32265645446913
At time: 42.80928945541382 and batch: 250, loss is 4.3270535564422605 and perplexity is 75.72085034803294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.697899627685547 and perplexity of 109.71648480483422
Finished 8 epochs...
Completing Train Step...
At time: 44.411460876464844 and batch: 50, loss is 4.266299180984497 and perplexity is 71.25743616881562
At time: 45.37824583053589 and batch: 100, loss is 4.182256264686584 and perplexity is 65.51350238549412
At time: 46.330491065979004 and batch: 150, loss is 4.250830163955689 and perplexity is 70.16363549715341
At time: 47.27460074424744 and batch: 200, loss is 4.227220401763916 and perplexity is 68.52649109110415
At time: 48.224204540252686 and batch: 250, loss is 4.274272947311402 and perplexity is 71.82789765463627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6720634460449215 and perplexity of 106.91813477272751
Finished 9 epochs...
Completing Train Step...
At time: 49.80629301071167 and batch: 50, loss is 4.213930492401123 and perplexity is 67.62180514820214
At time: 50.75878119468689 and batch: 100, loss is 4.129906139373779 and perplexity is 62.17208714985654
At time: 51.70781183242798 and batch: 150, loss is 4.20253469467163 and perplexity is 66.85557492906446
At time: 52.65648794174194 and batch: 200, loss is 4.179336996078491 and perplexity is 65.32252976039305
At time: 53.611738443374634 and batch: 250, loss is 4.2266746997833256 and perplexity is 68.48910625061129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.649298477172851 and perplexity of 104.51164255126615
Finished 10 epochs...
Completing Train Step...
At time: 55.20940375328064 and batch: 50, loss is 4.167863683700562 and perplexity is 64.57734700285762
At time: 56.15253663063049 and batch: 100, loss is 4.083003630638123 and perplexity is 59.323388061763865
At time: 57.09929037094116 and batch: 150, loss is 4.158530158996582 and perplexity is 63.97741682607279
At time: 58.041096925735474 and batch: 200, loss is 4.136310949325561 and perplexity is 62.57156547814115
At time: 58.989434003829956 and batch: 250, loss is 4.183731617927552 and perplexity is 65.61022927918854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.630892181396485 and perplexity of 102.60556606347146
Finished 11 epochs...
Completing Train Step...
At time: 60.56806826591492 and batch: 50, loss is 4.1270276021957395 and perplexity is 61.993379816807476
At time: 61.53307890892029 and batch: 100, loss is 4.041197953224182 and perplexity is 56.894458897851464
At time: 62.47583770751953 and batch: 150, loss is 4.118636054992676 and perplexity is 61.47533607800719
At time: 63.42108702659607 and batch: 200, loss is 4.097497730255127 and perplexity is 60.189488669790954
At time: 64.36815285682678 and batch: 250, loss is 4.145124354362488 and perplexity is 63.125471341424294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.616572570800781 and perplexity of 101.14676397873276
Finished 12 epochs...
Completing Train Step...
At time: 65.96064138412476 and batch: 50, loss is 4.09035430431366 and perplexity is 59.76106155553695
At time: 66.90954637527466 and batch: 100, loss is 4.003784294128418 and perplexity is 54.805156932190066
At time: 67.85867285728455 and batch: 150, loss is 4.081941175460815 and perplexity is 59.26039309157412
At time: 68.80256366729736 and batch: 200, loss is 4.062010040283203 and perplexity is 58.090958973828776
At time: 69.746333360672 and batch: 250, loss is 4.110449571609497 and perplexity is 60.97412364356831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.604102706909179 and perplexity of 99.89330904741999
Finished 13 epochs...
Completing Train Step...
At time: 71.3448965549469 and batch: 50, loss is 4.057255611419678 and perplexity is 57.815425163750795
At time: 72.30523800849915 and batch: 100, loss is 3.9697689056396483 and perplexity is 52.97228782811318
At time: 73.24965858459473 and batch: 150, loss is 4.0481015491485595 and perplexity is 57.28859416236049
At time: 74.20894432067871 and batch: 200, loss is 4.029197163581848 and perplexity is 56.21576106519054
At time: 75.15280413627625 and batch: 250, loss is 4.0790501308441165 and perplexity is 59.08931606595736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.592937088012695 and perplexity of 98.78414221873037
Finished 14 epochs...
Completing Train Step...
At time: 76.75147652626038 and batch: 50, loss is 4.02716477394104 and perplexity is 56.10162475876259
At time: 77.72405576705933 and batch: 100, loss is 3.9386889839172365 and perplexity is 51.351234856163636
At time: 78.68245673179626 and batch: 150, loss is 4.017038373947144 and perplexity is 55.536384023199616
At time: 79.63141942024231 and batch: 200, loss is 3.999146294593811 and perplexity is 54.551559187551085
At time: 80.5755672454834 and batch: 250, loss is 4.05016872882843 and perplexity is 57.40714246821037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.582122421264648 and perplexity of 97.72158062157874
Finished 15 epochs...
Completing Train Step...
At time: 82.17731642723083 and batch: 50, loss is 3.9991038179397584 and perplexity is 54.5492420690555
At time: 83.12022805213928 and batch: 100, loss is 3.910788588523865 and perplexity is 49.938317233743234
At time: 84.06737613677979 and batch: 150, loss is 3.9883064937591555 and perplexity is 53.96342454082531
At time: 85.01175165176392 and batch: 200, loss is 3.9718007850646972 and perplexity is 53.08003055289345
At time: 85.9609854221344 and batch: 250, loss is 4.022873210906982 and perplexity is 55.86137698775427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.574394226074219 and perplexity of 96.96927987996426
Finished 16 epochs...
Completing Train Step...
At time: 87.5391252040863 and batch: 50, loss is 3.9730499458312987 and perplexity is 53.14637747490878
At time: 88.49787378311157 and batch: 100, loss is 3.8852773237228395 and perplexity is 48.680440828775
At time: 89.44175362586975 and batch: 150, loss is 3.961741623878479 and perplexity is 52.54876648497
At time: 90.4229998588562 and batch: 200, loss is 3.9465942001342773 and perplexity is 51.758786239210316
At time: 91.3869788646698 and batch: 250, loss is 3.997304048538208 and perplexity is 54.45115430645565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.568698883056641 and perplexity of 96.41857628042432
Finished 17 epochs...
Completing Train Step...
At time: 92.98597812652588 and batch: 50, loss is 3.949108600616455 and perplexity is 51.889092308456945
At time: 93.93183493614197 and batch: 100, loss is 3.8612560272216796 and perplexity is 47.525006598713006
At time: 94.87659430503845 and batch: 150, loss is 3.936785454750061 and perplexity is 51.253579257471976
At time: 95.83280539512634 and batch: 200, loss is 3.923479266166687 and perplexity is 50.57610675054332
At time: 96.78524017333984 and batch: 250, loss is 3.9730181217193605 and perplexity is 53.144686165555264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.565695953369141 and perplexity of 96.12947237189182
Finished 18 epochs...
Completing Train Step...
At time: 98.39367628097534 and batch: 50, loss is 3.9268596982955932 and perplexity is 50.74736514736597
At time: 99.33917546272278 and batch: 100, loss is 3.8385930633544922 and perplexity is 46.46006207116963
At time: 100.29569125175476 and batch: 150, loss is 3.913370623588562 and perplexity is 50.067426330290125
At time: 101.23987984657288 and batch: 200, loss is 3.9021689987182615 and perplexity is 49.5097192466635
At time: 102.18654751777649 and batch: 250, loss is 3.950365810394287 and perplexity is 51.95436880719832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.563222885131836 and perplexity of 95.89203135203931
Finished 19 epochs...
Completing Train Step...
At time: 103.77565550804138 and batch: 50, loss is 3.905490074157715 and perplexity is 49.674418096541075
At time: 104.75178647041321 and batch: 100, loss is 3.8175840854644774 and perplexity is 45.4941654317165
At time: 105.70294213294983 and batch: 150, loss is 3.8914150953292848 and perplexity is 48.98014908580868
At time: 106.65002298355103 and batch: 200, loss is 3.882649779319763 and perplexity is 48.55269870647948
At time: 107.5939245223999 and batch: 250, loss is 3.929442048072815 and perplexity is 50.87858194536481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.562123107910156 and perplexity of 95.78662945014686
Finished 20 epochs...
Completing Train Step...
At time: 109.19831991195679 and batch: 50, loss is 3.885501494407654 and perplexity is 48.69135477978089
At time: 110.14382767677307 and batch: 100, loss is 3.797581715583801 and perplexity is 44.59321491864302
At time: 111.08898544311523 and batch: 150, loss is 3.8708591270446777 and perplexity is 47.98359237947593
At time: 112.03400588035583 and batch: 200, loss is 3.8642513751983643 and perplexity is 47.6675739438807
At time: 112.9800157546997 and batch: 250, loss is 3.9098928022384642 and perplexity is 49.8936032041465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.562722778320312 and perplexity of 95.84408708361617
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 114.56791734695435 and batch: 50, loss is 3.8626963758468627 and perplexity is 47.59350849809472
At time: 115.53200340270996 and batch: 100, loss is 3.7637993049621583 and perplexity is 43.11191051384934
At time: 116.47911882400513 and batch: 150, loss is 3.82928334236145 and perplexity is 46.02953899033039
At time: 117.42582321166992 and batch: 200, loss is 3.815281476974487 and perplexity is 45.38953069279867
At time: 118.37669062614441 and batch: 250, loss is 3.851144561767578 and perplexity is 47.04688048780733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.533077239990234 and perplexity of 93.04444107078422
Finished 22 epochs...
Completing Train Step...
At time: 119.96084380149841 and batch: 50, loss is 3.836499180793762 and perplexity is 46.36288193483066
At time: 120.93085098266602 and batch: 100, loss is 3.743506517410278 and perplexity is 42.245866614600835
At time: 121.87491941452026 and batch: 150, loss is 3.8148097658157347 and perplexity is 45.36812499373081
At time: 122.83735156059265 and batch: 200, loss is 3.805142149925232 and perplexity is 44.93163668787283
At time: 123.78703594207764 and batch: 250, loss is 3.844437823295593 and perplexity is 46.73240509532615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5326885223388675 and perplexity of 93.00828008284093
Finished 23 epochs...
Completing Train Step...
At time: 125.39172339439392 and batch: 50, loss is 3.8214703130722047 and perplexity is 45.67131010271545
At time: 126.33637261390686 and batch: 100, loss is 3.730105576515198 and perplexity is 41.683508731319996
At time: 127.2823555469513 and batch: 150, loss is 3.8038505125045776 and perplexity is 44.87363876876015
At time: 128.2300579547882 and batch: 200, loss is 3.796111044883728 and perplexity is 44.527681185122745
At time: 129.17728805541992 and batch: 250, loss is 3.8365033531188963 and perplexity is 46.36307537625181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5326988220214846 and perplexity of 93.00923804353987
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 130.76489925384521 and batch: 50, loss is 3.8105876064300537 and perplexity is 45.17697735084479
At time: 131.72554302215576 and batch: 100, loss is 3.7121363258361817 and perplexity is 40.94117686190311
At time: 132.67102146148682 and batch: 150, loss is 3.7825157165527346 and perplexity is 43.9264092455212
At time: 133.61330270767212 and batch: 200, loss is 3.7713212966918945 and perplexity is 43.43742065569648
At time: 134.55889320373535 and batch: 250, loss is 3.8077124071121218 and perplexity is 45.047271091408064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.51904296875 and perplexity of 91.74775048413278
Finished 25 epochs...
Completing Train Step...
At time: 136.15354347229004 and batch: 50, loss is 3.7983760023117066 and perplexity is 44.62864878787064
At time: 137.09770011901855 and batch: 100, loss is 3.701388988494873 and perplexity is 40.50352423581376
At time: 138.0475435256958 and batch: 150, loss is 3.7748776865005493 and perplexity is 43.592176078023044
At time: 138.99581575393677 and batch: 200, loss is 3.766846432685852 and perplexity is 43.24347836181557
At time: 139.94231939315796 and batch: 250, loss is 3.804996690750122 and perplexity is 44.925101444380964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.518506240844727 and perplexity of 91.6985201190391
Finished 26 epochs...
Completing Train Step...
At time: 141.52706146240234 and batch: 50, loss is 3.7909720134735108 and perplexity is 44.29943900706534
At time: 142.4775824546814 and batch: 100, loss is 3.6941370391845703 and perplexity is 40.2108572169905
At time: 143.4230489730835 and batch: 150, loss is 3.7689871263504027 and perplexity is 43.33614855583749
At time: 144.37343215942383 and batch: 200, loss is 3.762527379989624 and perplexity is 43.05711025654895
At time: 145.31656956672668 and batch: 250, loss is 3.801415147781372 and perplexity is 44.764488056745364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.518515777587891 and perplexity of 91.69939462844398
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 146.90011835098267 and batch: 50, loss is 3.787330722808838 and perplexity is 44.138425200301164
At time: 147.8671646118164 and batch: 100, loss is 3.686303038597107 and perplexity is 39.897076023724985
At time: 148.8153853416443 and batch: 150, loss is 3.757932577133179 and perplexity is 42.85972514346219
At time: 149.76490116119385 and batch: 200, loss is 3.7494697427749633 and perplexity is 42.498540865943646
At time: 150.7132625579834 and batch: 250, loss is 3.786666054725647 and perplexity is 44.10909754548597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.511008453369141 and perplexity of 91.01355517303358
Finished 28 epochs...
Completing Train Step...
At time: 152.31317281723022 and batch: 50, loss is 3.7807347106933595 and perplexity is 43.84824567881988
At time: 153.25949716567993 and batch: 100, loss is 3.6804996395111083 and perplexity is 39.6662079270043
At time: 154.20972323417664 and batch: 150, loss is 3.7539589643478393 and perplexity is 42.689755112845866
At time: 155.1610827445984 and batch: 200, loss is 3.7472920846939086 and perplexity is 42.406094270065495
At time: 156.1071162223816 and batch: 250, loss is 3.785976095199585 and perplexity is 44.07867454997314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.510781097412109 and perplexity of 90.99286505119471
Finished 29 epochs...
Completing Train Step...
At time: 157.69018578529358 and batch: 50, loss is 3.7767675065994264 and perplexity is 43.674635340573545
At time: 158.65136432647705 and batch: 100, loss is 3.676618843078613 and perplexity is 39.51256976083744
At time: 159.59899497032166 and batch: 150, loss is 3.7508615159988405 and perplexity is 42.557730376808564
At time: 160.5418517589569 and batch: 200, loss is 3.7450920438766477 and perplexity is 42.31290168310739
At time: 161.48636174201965 and batch: 250, loss is 3.784586958885193 and perplexity is 44.017485772081876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.510712814331055 and perplexity of 90.98665199014094
Finished 30 epochs...
Completing Train Step...
At time: 163.07441425323486 and batch: 50, loss is 3.773329758644104 and perplexity is 43.52475073259705
At time: 164.04418301582336 and batch: 100, loss is 3.673181734085083 and perplexity is 39.376993879965944
At time: 164.99565529823303 and batch: 150, loss is 3.748004341125488 and perplexity is 42.43630904250316
At time: 165.94362664222717 and batch: 200, loss is 3.7428845977783203 and perplexity is 42.219601249103945
At time: 166.89852738380432 and batch: 250, loss is 3.782976927757263 and perplexity is 43.94667327027837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.510732269287109 and perplexity of 90.98842214867605
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 168.4974126815796 and batch: 50, loss is 3.7726562547683717 and perplexity is 43.49544651364861
At time: 169.4428482055664 and batch: 100, loss is 3.6699194049835206 and perplexity is 39.248742479782905
At time: 170.39402031898499 and batch: 150, loss is 3.743116102218628 and perplexity is 42.22937640571355
At time: 171.3416130542755 and batch: 200, loss is 3.7360676908493042 and perplexity is 41.93277290860667
At time: 172.326025724411 and batch: 250, loss is 3.775248293876648 and perplexity is 43.60833465407653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.506490707397461 and perplexity of 90.60330644830954
Finished 32 epochs...
Completing Train Step...
At time: 173.91874051094055 and batch: 50, loss is 3.768729510307312 and perplexity is 43.32498590662424
At time: 174.88210678100586 and batch: 100, loss is 3.666661787033081 and perplexity is 39.12109310108765
At time: 175.8311960697174 and batch: 150, loss is 3.7412643909454344 and perplexity is 42.15125214747094
At time: 176.78092122077942 and batch: 200, loss is 3.735133647918701 and perplexity is 41.89362418465146
At time: 177.72716450691223 and batch: 250, loss is 3.7751838064193723 and perplexity is 43.605522554132236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.506437683105469 and perplexity of 90.59850239949978
Finished 33 epochs...
Completing Train Step...
At time: 179.3483111858368 and batch: 50, loss is 3.7665745782852174 and perplexity is 43.231724029729996
At time: 180.29285311698914 and batch: 100, loss is 3.664570336341858 and perplexity is 39.03935876533978
At time: 181.24569869041443 and batch: 150, loss is 3.739805703163147 and perplexity is 42.08981145324461
At time: 182.19215154647827 and batch: 200, loss is 3.7341090202331544 and perplexity is 41.85072080121476
At time: 183.14120841026306 and batch: 250, loss is 3.7746513414382936 and perplexity is 43.58231032078964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.506428146362305 and perplexity of 90.59763838897128
Finished 34 epochs...
Completing Train Step...
At time: 184.76992225646973 and batch: 50, loss is 3.764728002548218 and perplexity is 43.1519670383938
At time: 185.7216079235077 and batch: 100, loss is 3.6627377605438234 and perplexity is 38.96788169487401
At time: 186.67287278175354 and batch: 150, loss is 3.738441572189331 and perplexity is 42.03243458144066
At time: 187.61749911308289 and batch: 200, loss is 3.733038215637207 and perplexity is 41.80593084196398
At time: 188.56057810783386 and batch: 250, loss is 3.773961567878723 and perplexity is 43.55225876104254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.506442260742188 and perplexity of 90.59891712748023
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 190.16796326637268 and batch: 50, loss is 3.764943232536316 and perplexity is 43.16125563530233
At time: 191.1283254623413 and batch: 100, loss is 3.6615184497833253 and perplexity is 38.92039669277971
At time: 192.0711977481842 and batch: 150, loss is 3.736313891410828 and perplexity is 41.943098051818886
At time: 193.01551222801208 and batch: 200, loss is 3.729626417160034 and perplexity is 41.66354047252639
At time: 193.95775175094604 and batch: 250, loss is 3.7694922161102293 and perplexity is 43.35804272949937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.504293441772461 and perplexity of 90.40444547274022
Finished 36 epochs...
Completing Train Step...
At time: 195.55456137657166 and batch: 50, loss is 3.762441692352295 and perplexity is 43.053420952566974
At time: 196.50255799293518 and batch: 100, loss is 3.6596754741668702 and perplexity is 38.848733407819594
At time: 197.47463941574097 and batch: 150, loss is 3.7354112434387208 and perplexity is 41.90525528133798
At time: 198.42317152023315 and batch: 200, loss is 3.7292644071578978 and perplexity is 41.64846058385088
At time: 199.37140154838562 and batch: 250, loss is 3.76959641456604 and perplexity is 43.362560805982966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.504268646240234 and perplexity of 90.40220387419001
Finished 37 epochs...
Completing Train Step...
At time: 200.94952654838562 and batch: 50, loss is 3.7612180662155152 and perplexity is 43.00077187937177
At time: 201.9303913116455 and batch: 100, loss is 3.6585446405410766 and perplexity is 38.80482678398479
At time: 202.87483096122742 and batch: 150, loss is 3.7347095823287964 and perplexity is 41.875862306565175
At time: 203.81894183158875 and batch: 200, loss is 3.7288407850265504 and perplexity is 41.6308211107104
At time: 204.7621726989746 and batch: 250, loss is 3.769395360946655 and perplexity is 43.3538434825412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.504278945922851 and perplexity of 90.40313499299289
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 206.33114051818848 and batch: 50, loss is 3.761350212097168 and perplexity is 43.00645462975128
At time: 207.29195499420166 and batch: 100, loss is 3.65811776638031 and perplexity is 38.788265541152484
At time: 208.23831295967102 and batch: 150, loss is 3.733677453994751 and perplexity is 41.83266333983971
At time: 209.1838297843933 and batch: 200, loss is 3.727167935371399 and perplexity is 41.561237223889336
At time: 210.1288628578186 and batch: 250, loss is 3.7666008996963503 and perplexity is 43.232861964688134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5035346984863285 and perplexity of 90.33587772265075
Finished 39 epochs...
Completing Train Step...
At time: 211.7288920879364 and batch: 50, loss is 3.759912805557251 and perplexity is 42.944681277953784
At time: 212.67333102226257 and batch: 100, loss is 3.656998085975647 and perplexity is 38.74485938534103
At time: 213.61544799804688 and batch: 150, loss is 3.73316171169281 and perplexity is 41.81109402833379
At time: 214.56169176101685 and batch: 200, loss is 3.727014408111572 and perplexity is 41.554856930810324
At time: 215.50987720489502 and batch: 250, loss is 3.7668043088912966 and perplexity is 43.241656820782566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503502655029297 and perplexity of 90.3329830952117
Finished 40 epochs...
Completing Train Step...
At time: 217.07426977157593 and batch: 50, loss is 3.7591908407211303 and perplexity is 42.91368791757709
At time: 218.05483889579773 and batch: 100, loss is 3.6563551568984987 and perplexity is 38.7199571946812
At time: 219.00213813781738 and batch: 150, loss is 3.7327932119369507 and perplexity is 41.795689488850975
At time: 219.95020961761475 and batch: 200, loss is 3.726846308708191 and perplexity is 41.54787217123607
At time: 220.9024031162262 and batch: 250, loss is 3.7667760705947875 and perplexity is 43.240435767296034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503502273559571 and perplexity of 90.33294863591996
Finished 41 epochs...
Completing Train Step...
At time: 222.4980857372284 and batch: 50, loss is 3.7585967540740968 and perplexity is 42.888201040066704
At time: 223.44355416297913 and batch: 100, loss is 3.6558090114593504 and perplexity is 38.69881624019873
At time: 224.39395380020142 and batch: 150, loss is 3.732452335357666 and perplexity is 41.78144474517685
At time: 225.34298086166382 and batch: 200, loss is 3.726652536392212 and perplexity is 41.53982212378481
At time: 226.28745794296265 and batch: 250, loss is 3.7666729164123534 and perplexity is 43.23597556554455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503509902954102 and perplexity of 90.33363782425332
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 227.87981605529785 and batch: 50, loss is 3.7586654806137085 and perplexity is 42.891148699004404
At time: 228.82730078697205 and batch: 100, loss is 3.6556625509262086 and perplexity is 38.69314880597808
At time: 229.78240180015564 and batch: 150, loss is 3.731950397491455 and perplexity is 41.760478318316224
At time: 230.73109245300293 and batch: 200, loss is 3.725508689880371 and perplexity is 41.49233410782445
At time: 231.67773628234863 and batch: 250, loss is 3.7649456119537352 and perplexity is 43.161358334068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503280258178711 and perplexity of 90.31289555805257
Finished 43 epochs...
Completing Train Step...
At time: 233.27341628074646 and batch: 50, loss is 3.7580047130584715 and perplexity is 42.86281698090814
At time: 234.23746848106384 and batch: 100, loss is 3.6550199842453 and perplexity is 38.66829386410988
At time: 235.18193531036377 and batch: 150, loss is 3.7316611194610596 and perplexity is 41.74839967652701
At time: 236.1278371810913 and batch: 200, loss is 3.7254465198516846 and perplexity is 41.4897546084073
At time: 237.06989192962646 and batch: 250, loss is 3.7651355934143065 and perplexity is 43.16955897092436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503258895874024 and perplexity of 90.3109662870674
Finished 44 epochs...
Completing Train Step...
At time: 238.6685664653778 and batch: 50, loss is 3.7576029109954834 and perplexity is 42.84559807214792
At time: 239.64685130119324 and batch: 100, loss is 3.6546593952178954 and perplexity is 38.654353015243636
At time: 240.60320138931274 and batch: 150, loss is 3.731467785835266 and perplexity is 41.74032908722975
At time: 241.5601692199707 and batch: 200, loss is 3.725367240905762 and perplexity is 41.4864654747766
At time: 242.5190761089325 and batch: 250, loss is 3.7651737356185913 and perplexity is 43.171205584464055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503256607055664 and perplexity of 90.31075958190624
Finished 45 epochs...
Completing Train Step...
At time: 244.11483907699585 and batch: 50, loss is 3.757273736000061 and perplexity is 42.83149669363658
At time: 245.08684468269348 and batch: 100, loss is 3.654361672401428 and perplexity is 38.642846445364405
At time: 246.04424405097961 and batch: 150, loss is 3.731294107437134 and perplexity is 41.733080323231455
At time: 247.0197672843933 and batch: 200, loss is 3.7252761363983153 and perplexity is 41.48268604293808
At time: 247.96421122550964 and batch: 250, loss is 3.765157322883606 and perplexity is 43.17049703272245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503258132934571 and perplexity of 90.31089738529448
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 249.55102229118347 and batch: 50, loss is 3.7572789001464844 and perplexity is 42.831717882328164
At time: 250.51324009895325 and batch: 100, loss is 3.654254937171936 and perplexity is 38.63872211239056
At time: 251.46073365211487 and batch: 150, loss is 3.731035342216492 and perplexity is 41.722282650584866
At time: 252.4079191684723 and batch: 200, loss is 3.724485697746277 and perplexity is 41.44990948013697
At time: 253.350519657135 and batch: 250, loss is 3.7640811252593993 and perplexity is 43.12406203747755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503229141235352 and perplexity of 90.3082791568749
Finished 47 epochs...
Completing Train Step...
At time: 254.95134615898132 and batch: 50, loss is 3.756997060775757 and perplexity is 42.819647918888386
At time: 255.90514516830444 and batch: 100, loss is 3.6539204120635986 and perplexity is 38.62579865142151
At time: 256.84823393821716 and batch: 150, loss is 3.7308731746673582 and perplexity is 41.71551719884635
At time: 257.79294514656067 and batch: 200, loss is 3.724479775428772 and perplexity is 41.44966400133938
At time: 258.73768520355225 and batch: 250, loss is 3.7642135429382324 and perplexity is 43.12977280376927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.5032188415527346 and perplexity of 90.30734901505198
Finished 48 epochs...
Completing Train Step...
At time: 260.32324743270874 and batch: 50, loss is 3.756782302856445 and perplexity is 42.81045304776669
At time: 261.2831618785858 and batch: 100, loss is 3.6537147998809814 and perplexity is 38.61785753307877
At time: 262.2278482913971 and batch: 150, loss is 3.7307680082321166 and perplexity is 41.71113035728658
At time: 263.175035238266 and batch: 200, loss is 3.7244514656066894 and perplexity is 41.4484905853358
At time: 264.120370388031 and batch: 250, loss is 3.7642641735076903 and perplexity is 43.13195654400845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503216171264649 and perplexity of 90.30710786873583
Finished 49 epochs...
Completing Train Step...
At time: 265.73042702674866 and batch: 50, loss is 3.7565987348556518 and perplexity is 42.80259513973996
At time: 266.6736743450165 and batch: 100, loss is 3.653547101020813 and perplexity is 38.6113819053812
At time: 267.6179940700531 and batch: 150, loss is 3.73067711353302 and perplexity is 41.70733920894402
At time: 268.5622160434723 and batch: 200, loss is 3.724412226676941 and perplexity is 41.4468642228341
At time: 269.5064709186554 and batch: 250, loss is 3.7642808151245117 and perplexity is 43.13267433547461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.503217315673828 and perplexity of 90.30721121707813
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa75b581898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -93.43365976244243, 'params': {'dropout': 0.6280757312144768, 'tune_wordvecs': True, 'lr': 1.0233030793280484, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.650782445341421, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}, {'best_accuracy': -90.11329696153359, 'params': {'dropout': 0.28662865101683643, 'tune_wordvecs': True, 'lr': 2.043935036858242, 'data': 'ptb', 'num_layers': 1, 'anneal': 7.691206261612701, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}, {'best_accuracy': -119.62753142231132, 'params': {'dropout': 0.9126267719419398, 'tune_wordvecs': True, 'lr': 10.437363813840214, 'data': 'ptb', 'num_layers': 1, 'anneal': 2.0215821760184216, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}, {'best_accuracy': -121.13872986262318, 'params': {'dropout': 0.8942611318164297, 'tune_wordvecs': True, 'lr': 10.333996825618843, 'data': 'ptb', 'num_layers': 1, 'anneal': 3.0250139781457053, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}, {'best_accuracy': -161.8698723139324, 'params': {'dropout': 0.1957311340224973, 'tune_wordvecs': True, 'lr': 25.2925964544902, 'data': 'ptb', 'num_layers': 1, 'anneal': 5.681526621650348, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}, {'best_accuracy': -90.30710786873583, 'params': {'dropout': 0.0, 'tune_wordvecs': True, 'lr': 2.0113506699736963, 'data': 'ptb', 'num_layers': 1, 'anneal': 2.0, 'batch_size': 80, 'wordvec_source': 'glove', 'seq_len': 50, 'wordvec_dim': 200}}]
