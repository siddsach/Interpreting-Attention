Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 7.981558640577813, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 23.983799453362604, 'dropout': 0.08787475654303023, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 3.1851305961608887 and batch: 50, loss is 7.880955858230591 and perplexity is 2646.40093524115
At time: 5.616040945053101 and batch: 100, loss is 6.6118387031555175 and perplexity is 743.8494804598337
At time: 8.05609655380249 and batch: 150, loss is 6.207411012649536 and perplexity is 496.41437561414625
At time: 10.496322870254517 and batch: 200, loss is 6.075734806060791 and perplexity is 435.16914999137765
At time: 12.937232255935669 and batch: 250, loss is 6.037756929397583 and perplexity is 418.95224069231745
At time: 15.39117169380188 and batch: 300, loss is 6.090331382751465 and perplexity is 441.5677148276713
At time: 17.841976404190063 and batch: 350, loss is 6.101339302062988 and perplexity is 446.45530836659754
At time: 20.291232109069824 and batch: 400, loss is 6.1653049945831295 and perplexity is 475.94628183877717
At time: 22.729539394378662 and batch: 450, loss is 6.187955913543701 and perplexity is 486.84992508300957
At time: 25.176954984664917 and batch: 500, loss is 6.223281393051147 and perplexity is 504.35550831397484
At time: 27.613417387008667 and batch: 550, loss is 6.224053058624268 and perplexity is 504.74485229870885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.855009460449219 and perplexity of 348.97820152892285
Finished 1 epochs...
Completing Train Step...
At time: 31.604816675186157 and batch: 50, loss is 6.201163711547852 and perplexity is 493.32279261845815
At time: 34.02567648887634 and batch: 100, loss is 6.233357982635498 and perplexity is 509.463383536929
At time: 36.452221393585205 and batch: 150, loss is 6.294052038192749 and perplexity is 541.3424311331554
At time: 38.883219957351685 and batch: 200, loss is 6.331230592727661 and perplexity is 561.8475741469284
At time: 41.30527353286743 and batch: 250, loss is 6.2798119735717775 and perplexity is 533.6883068848506
At time: 43.73371386528015 and batch: 300, loss is 6.3095481491088865 and perplexity is 549.7964666544358
At time: 46.16271090507507 and batch: 350, loss is 6.316251821517945 and perplexity is 553.4945034231738
At time: 48.596630811691284 and batch: 400, loss is 6.3010366344451905 and perplexity is 545.136724828457
At time: 51.02611994743347 and batch: 450, loss is 6.263156728744507 and perplexity is 524.8732100429131
At time: 53.45624589920044 and batch: 500, loss is 6.285213394165039 and perplexity is 536.5787812022472
At time: 55.88754916191101 and batch: 550, loss is 6.282447071075439 and perplexity is 535.0964821347659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.919594319661458 and perplexity of 372.2606644069915
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.90320181846619 and batch: 50, loss is 6.203929853439331 and perplexity is 494.6892825423403
At time: 62.37039041519165 and batch: 100, loss is 6.122276802062988 and perplexity is 455.90151123969844
At time: 64.8054518699646 and batch: 150, loss is 6.083966541290283 and perplexity is 438.7661316029031
At time: 67.2345724105835 and batch: 200, loss is 6.029358863830566 and perplexity is 415.44858486320805
At time: 69.70362043380737 and batch: 250, loss is 5.921159553527832 and perplexity is 372.84379565537546
At time: 72.13840293884277 and batch: 300, loss is 5.870124883651734 and perplexity is 354.2932229365797
At time: 74.57149076461792 and batch: 350, loss is 5.760141849517822 and perplexity is 317.39334781816837
At time: 77.00424838066101 and batch: 400, loss is 5.699197568893433 and perplexity is 298.62767666176825
At time: 79.43502402305603 and batch: 450, loss is 5.613374357223511 and perplexity is 274.06748103720054
At time: 81.86308574676514 and batch: 500, loss is 5.6046419525146485 and perplexity is 271.68463201821424
At time: 84.29384803771973 and batch: 550, loss is 5.669176988601684 and perplexity is 289.79593086420465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.405620320638021 and perplexity of 222.6542947083943
Finished 3 epochs...
Completing Train Step...
At time: 88.29303860664368 and batch: 50, loss is 5.796998119354248 and perplexity is 329.3095267775603
At time: 90.72361397743225 and batch: 100, loss is 5.8023193359375 and perplexity is 331.0665246296942
At time: 93.15091276168823 and batch: 150, loss is 5.80687705039978 and perplexity is 332.57887512987554
At time: 95.57500553131104 and batch: 200, loss is 5.773807516098023 and perplexity is 321.7605116225143
At time: 98.00976514816284 and batch: 250, loss is 5.692502107620239 and perplexity is 296.634905323759
At time: 100.44026112556458 and batch: 300, loss is 5.66126202583313 and perplexity is 287.51126032927823
At time: 102.86478662490845 and batch: 350, loss is 5.588812170028686 and perplexity is 267.41778412485274
At time: 105.30015397071838 and batch: 400, loss is 5.578316640853882 and perplexity is 264.6257704300429
At time: 107.73017692565918 and batch: 450, loss is 5.542053136825562 and perplexity is 255.201425386558
At time: 110.16674304008484 and batch: 500, loss is 5.547959966659546 and perplexity is 256.7133176289034
At time: 112.59764122962952 and batch: 550, loss is 5.583516445159912 and perplexity is 266.005356330533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.341806030273437 and perplexity of 208.8896308187669
Finished 4 epochs...
Completing Train Step...
At time: 116.59560823440552 and batch: 50, loss is 5.689594783782959 and perplexity is 295.7737440364937
At time: 119.02418351173401 and batch: 100, loss is 5.6846462059021 and perplexity is 294.3137001788983
At time: 121.49682879447937 and batch: 150, loss is 5.690956430435181 and perplexity is 296.1767576836995
At time: 123.93626999855042 and batch: 200, loss is 5.673260097503662 and perplexity is 290.9816182072533
At time: 126.36673760414124 and batch: 250, loss is 5.602731170654297 and perplexity is 271.1659976080275
At time: 128.82568740844727 and batch: 300, loss is 5.582114925384522 and perplexity is 265.6328056928012
At time: 131.2877700328827 and batch: 350, loss is 5.527866144180297 and perplexity is 251.6064459120391
At time: 133.74792528152466 and batch: 400, loss is 5.54022852897644 and perplexity is 254.73620741194145
At time: 136.21274328231812 and batch: 450, loss is 5.5159312438964845 and perplexity is 248.62139663495617
At time: 138.67540216445923 and batch: 500, loss is 5.516448850631714 and perplexity is 248.7501180550397
At time: 141.1337230205536 and batch: 550, loss is 5.530663375854492 and perplexity is 252.3112326983864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.312841796875 and perplexity of 202.92608448031623
Finished 5 epochs...
Completing Train Step...
At time: 145.15745615959167 and batch: 50, loss is 5.623016967773437 and perplexity is 276.7229894623004
At time: 147.6446795463562 and batch: 100, loss is 5.61301082611084 and perplexity is 273.9678670883542
At time: 150.10726356506348 and batch: 150, loss is 5.624859733581543 and perplexity is 277.2333952603391
At time: 152.57331085205078 and batch: 200, loss is 5.608180503845215 and perplexity is 272.64770497096214
At time: 155.0324878692627 and batch: 250, loss is 5.54854248046875 and perplexity is 256.8629002441687
At time: 157.49158763885498 and batch: 300, loss is 5.537697649002075 and perplexity is 254.09231579575138
At time: 159.95738220214844 and batch: 350, loss is 5.494377565383911 and perplexity is 243.32002820843152
At time: 162.41843724250793 and batch: 400, loss is 5.514054708480835 and perplexity is 248.15528725124463
At time: 164.879497051239 and batch: 450, loss is 5.490642614364624 and perplexity is 242.41293485017812
At time: 167.33438229560852 and batch: 500, loss is 5.489549551010132 and perplexity is 242.14810691766036
At time: 169.79932618141174 and batch: 550, loss is 5.491224346160888 and perplexity is 242.5539951878699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.292877197265625 and perplexity of 198.91492032818383
Finished 6 epochs...
Completing Train Step...
At time: 173.85458421707153 and batch: 50, loss is 5.571486120223999 and perplexity is 262.82439780608996
At time: 176.3437430858612 and batch: 100, loss is 5.557395486831665 and perplexity is 259.1470048123893
At time: 178.8034725189209 and batch: 150, loss is 5.572752780914307 and perplexity is 263.1575180698774
At time: 181.28911566734314 and batch: 200, loss is 5.567969570159912 and perplexity is 261.9017858095401
At time: 183.75345921516418 and batch: 250, loss is 5.5177684497833255 and perplexity is 249.07858517459007
At time: 186.21087098121643 and batch: 300, loss is 5.509973850250244 and perplexity is 247.1446642120004
At time: 188.67231369018555 and batch: 350, loss is 5.4706005859375 and perplexity is 237.60285084858776
At time: 191.1389467716217 and batch: 400, loss is 5.494624862670898 and perplexity is 243.3802080321239
At time: 193.60687160491943 and batch: 450, loss is 5.469172945022583 and perplexity is 237.26388131817342
At time: 196.07492804527283 and batch: 500, loss is 5.465417222976685 and perplexity is 236.37445539216955
At time: 198.53686380386353 and batch: 550, loss is 5.462595539093018 and perplexity is 235.70842151132055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.279868570963542 and perplexity of 196.34406834422367
Finished 7 epochs...
Completing Train Step...
At time: 202.61209201812744 and batch: 50, loss is 5.5343239307403564 and perplexity is 253.23652432116822
At time: 205.0765461921692 and batch: 100, loss is 5.518667478561401 and perplexity is 249.30261468006125
At time: 207.53403759002686 and batch: 150, loss is 5.541463613510132 and perplexity is 255.05102253351185
At time: 210.00428104400635 and batch: 200, loss is 5.541981353759765 and perplexity is 255.18310690333445
At time: 212.46029257774353 and batch: 250, loss is 5.4957741355896 and perplexity is 243.6600791074819
At time: 214.9175124168396 and batch: 300, loss is 5.493553829193115 and perplexity is 243.11967922393416
At time: 217.37468934059143 and batch: 350, loss is 5.453079080581665 and perplexity is 233.4759515493531
At time: 219.82958149909973 and batch: 400, loss is 5.476221513748169 and perplexity is 238.94215987422322
At time: 222.29641008377075 and batch: 450, loss is 5.449901847839356 and perplexity is 232.73532131191217
At time: 224.7588448524475 and batch: 500, loss is 5.44459527015686 and perplexity is 231.50356434741667
At time: 227.2246196269989 and batch: 550, loss is 5.442116661071777 and perplexity is 230.93046804384997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.271337890625 and perplexity of 194.6762438140762
Finished 8 epochs...
Completing Train Step...
At time: 231.43972849845886 and batch: 50, loss is 5.5067055225372314 and perplexity is 246.33823301540315
At time: 234.06810092926025 and batch: 100, loss is 5.493057832717896 and perplexity is 242.99912262027937
At time: 236.53846168518066 and batch: 150, loss is 5.519738502502442 and perplexity is 249.5697667865613
At time: 239.00280594825745 and batch: 200, loss is 5.523580417633057 and perplexity is 250.53043687454712
At time: 241.46191215515137 and batch: 250, loss is 5.478751926422119 and perplexity is 239.54754776147703
At time: 243.98701810836792 and batch: 300, loss is 5.476697244644165 and perplexity is 239.05585908500075
At time: 246.4615559577942 and batch: 350, loss is 5.435206670761108 and perplexity is 229.34024130042573
At time: 248.9297046661377 and batch: 400, loss is 5.459490346908569 and perplexity is 234.97763676377696
At time: 251.40253162384033 and batch: 450, loss is 5.432729997634888 and perplexity is 228.7729432838917
At time: 253.88029599189758 and batch: 500, loss is 5.428911924362183 and perplexity is 227.9011367932282
At time: 256.35540080070496 and batch: 550, loss is 5.425805196762085 and perplexity is 227.19420872651855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.261470540364583 and perplexity of 192.76475132123738
Finished 9 epochs...
Completing Train Step...
At time: 260.51596188545227 and batch: 50, loss is 5.486825323104858 and perplexity is 241.48933801331873
At time: 262.9840497970581 and batch: 100, loss is 5.47401083946228 and perplexity is 238.41452002038787
At time: 265.45141649246216 and batch: 150, loss is 5.497958059310913 and perplexity is 244.19279562857176
At time: 267.9192125797272 and batch: 200, loss is 5.499709138870239 and perplexity is 244.620771241848
At time: 270.3830099105835 and batch: 250, loss is 5.455966205596924 and perplexity is 234.15099981499492
At time: 272.8445222377777 and batch: 300, loss is 5.455965929031372 and perplexity is 234.15093505690342
At time: 275.3032400608063 and batch: 350, loss is 5.412396364212036 and perplexity is 224.16813303622666
At time: 277.7538743019104 and batch: 400, loss is 5.432175512313843 and perplexity is 228.64612720706427
At time: 280.20871663093567 and batch: 450, loss is 5.403119516372681 and perplexity is 222.0981755608911
At time: 282.6608462333679 and batch: 500, loss is 5.398743352890015 and perplexity is 221.12836121622155
At time: 285.11485147476196 and batch: 550, loss is 5.39797269821167 and perplexity is 220.95801325829188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.244529215494792 and perplexity of 189.52656805150986
Finished 10 epochs...
Completing Train Step...
At time: 289.1250238418579 and batch: 50, loss is 5.461077785491943 and perplexity is 235.35094555460117
At time: 291.6293692588806 and batch: 100, loss is 5.444040603637696 and perplexity is 231.3751926762311
At time: 294.0867748260498 and batch: 150, loss is 5.455990591049194 and perplexity is 234.15670976264462
At time: 296.54077339172363 and batch: 200, loss is 5.464575262069702 and perplexity is 236.17552110052034
At time: 299.0031876564026 and batch: 250, loss is 5.429904747009277 and perplexity is 228.12751456102748
At time: 301.4634635448456 and batch: 300, loss is 5.434014167785644 and perplexity is 229.06691538365652
At time: 303.9243092536926 and batch: 350, loss is 5.387095499038696 and perplexity is 218.56763282903313
At time: 306.4204590320587 and batch: 400, loss is 5.403574361801147 and perplexity is 222.1992188785192
At time: 308.88518142700195 and batch: 450, loss is 5.374674463272095 and perplexity is 215.86958739193983
At time: 311.3485007286072 and batch: 500, loss is 5.3741146850585935 and perplexity is 215.74878211519578
At time: 313.8103425502777 and batch: 550, loss is 5.373459424972534 and perplexity is 215.60745685711683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.229301452636719 and perplexity of 186.66236546367352
Finished 11 epochs...
Completing Train Step...
At time: 317.86915040016174 and batch: 50, loss is 5.4334984874725345 and perplexity is 228.9488205372191
At time: 320.33869981765747 and batch: 100, loss is 5.415725469589233 and perplexity is 224.9156559742341
At time: 322.80204916000366 and batch: 150, loss is 5.424627561569213 and perplexity is 226.9268143081467
At time: 325.26673698425293 and batch: 200, loss is 5.433234605789185 and perplexity is 228.88841310761242
At time: 327.73176074028015 and batch: 250, loss is 5.403829307556152 and perplexity is 222.25587484793027
At time: 330.19541573524475 and batch: 300, loss is 5.4065884017944335 and perplexity is 222.86994650287306
At time: 332.66166257858276 and batch: 350, loss is 5.355736923217774 and perplexity is 211.82001393823316
At time: 335.1241490840912 and batch: 400, loss is 5.369728479385376 and perplexity is 214.804535925935
At time: 337.587607383728 and batch: 450, loss is 5.342738227844238 and perplexity is 209.08444801513244
At time: 340.0545566082001 and batch: 500, loss is 5.342422637939453 and perplexity is 209.0184734850867
At time: 342.5193238258362 and batch: 550, loss is 5.341872329711914 and perplexity is 208.90348054310326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.199776713053385 and perplexity of 181.2317706682711
Finished 12 epochs...
Completing Train Step...
At time: 346.5816605091095 and batch: 50, loss is 5.398100786209106 and perplexity is 220.9863171403827
At time: 349.07393050193787 and batch: 100, loss is 5.384503726959228 and perplexity is 218.0018887972728
At time: 351.53769993782043 and batch: 150, loss is 5.390445766448974 and perplexity is 219.30112085067572
At time: 353.9986960887909 and batch: 200, loss is 5.3946465969085695 and perplexity is 220.22430539222202
At time: 356.45715498924255 and batch: 250, loss is 5.368621168136596 and perplexity is 214.56681208844515
At time: 358.91995549201965 and batch: 300, loss is 5.371086158752441 and perplexity is 215.09636967581616
At time: 361.3836317062378 and batch: 350, loss is 5.321632280349731 and perplexity is 204.71776620953807
At time: 363.8462646007538 and batch: 400, loss is 5.337706289291382 and perplexity is 208.03499053752276
At time: 366.3137001991272 and batch: 450, loss is 5.3093899059295655 and perplexity is 202.22681336487193
At time: 368.81102871894836 and batch: 500, loss is 5.307710971832275 and perplexity is 201.88757273354702
At time: 371.27562832832336 and batch: 550, loss is 5.3068893527984615 and perplexity is 201.72176618533408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.181257120768229 and perplexity of 177.90632020482198
Finished 13 epochs...
Completing Train Step...
At time: 375.3996350765228 and batch: 50, loss is 5.365962753295898 and perplexity is 213.99716200946438
At time: 377.86736607551575 and batch: 100, loss is 5.353950424194336 and perplexity is 211.44193550911572
At time: 380.3341164588928 and batch: 150, loss is 5.360844221115112 and perplexity is 212.9046091679102
At time: 382.79973459243774 and batch: 200, loss is 5.367998743057251 and perplexity is 214.43330187776823
At time: 385.2664749622345 and batch: 250, loss is 5.344087600708008 and perplexity is 209.3667713323938
At time: 387.7318046092987 and batch: 300, loss is 5.3473186016082765 and perplexity is 210.04432956524548
At time: 390.1993372440338 and batch: 350, loss is 5.300132541656494 and perplexity is 200.36336470720778
At time: 392.66492533683777 and batch: 400, loss is 5.31946551322937 and perplexity is 204.27467070050824
At time: 395.1287622451782 and batch: 450, loss is 5.288405132293701 and perplexity is 198.02734600485493
At time: 397.59292554855347 and batch: 500, loss is 5.286126661300659 and perplexity is 197.5766600735398
At time: 400.05704855918884 and batch: 550, loss is 5.283993034362793 and perplexity is 197.15555459006706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.165448506673177 and perplexity of 175.11598164776848
Finished 14 epochs...
Completing Train Step...
At time: 404.0956151485443 and batch: 50, loss is 5.342162837982178 and perplexity is 208.964177547952
At time: 406.58798718452454 and batch: 100, loss is 5.327692308425903 and perplexity is 205.96212824674623
At time: 409.0522840023041 and batch: 150, loss is 5.337393417358398 and perplexity is 207.96991240899584
At time: 411.51656675338745 and batch: 200, loss is 5.347192745208741 and perplexity is 210.01789580564727
At time: 413.9777603149414 and batch: 250, loss is 5.325310277938843 and perplexity is 205.47210403608975
At time: 416.4413206577301 and batch: 300, loss is 5.326798429489136 and perplexity is 205.77810529790676
At time: 418.90474700927734 and batch: 350, loss is 5.28160210609436 and perplexity is 196.68473287608094
At time: 421.36908054351807 and batch: 400, loss is 5.298913555145264 and perplexity is 200.11927327059448
At time: 423.8311417102814 and batch: 450, loss is 5.268179798126221 and perplexity is 194.0624080140908
At time: 426.29526925086975 and batch: 500, loss is 5.265711889266968 and perplexity is 193.58407016798824
At time: 428.7574360370636 and batch: 550, loss is 5.265818014144897 and perplexity is 193.60461534396168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.160078430175782 and perplexity of 174.17811588966106
Finished 15 epochs...
Completing Train Step...
At time: 432.83549404144287 and batch: 50, loss is 5.320348844528199 and perplexity is 204.45519262929272
At time: 435.297030210495 and batch: 100, loss is 5.307667922973633 and perplexity is 201.87888189103356
At time: 437.76525115966797 and batch: 150, loss is 5.321408195495605 and perplexity is 204.67189719822736
At time: 440.22829937934875 and batch: 200, loss is 5.328432188034058 and perplexity is 206.11457181347708
At time: 442.69304609298706 and batch: 250, loss is 5.309300146102905 and perplexity is 202.20866233578704
At time: 445.1570029258728 and batch: 300, loss is 5.309833717346192 and perplexity is 202.3165838525013
At time: 447.62322187423706 and batch: 350, loss is 5.267179880142212 and perplexity is 193.86845850525816
At time: 450.09164786338806 and batch: 400, loss is 5.283659048080445 and perplexity is 197.08971833416032
At time: 452.5583839416504 and batch: 450, loss is 5.250491085052491 and perplexity is 190.6598756872531
At time: 455.02319145202637 and batch: 500, loss is 5.251456918716431 and perplexity is 190.8441103692618
At time: 457.4873094558716 and batch: 550, loss is 5.248506851196289 and perplexity is 190.28193699027557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.153475952148438 and perplexity of 173.03189681368443
Finished 16 epochs...
Completing Train Step...
At time: 461.5298316478729 and batch: 50, loss is 5.303266077041626 and perplexity is 200.9921951170294
At time: 464.02224922180176 and batch: 100, loss is 5.29163966178894 and perplexity is 198.66890831313106
At time: 466.48725390434265 and batch: 150, loss is 5.308045415878296 and perplexity is 201.95510412231934
At time: 468.9490418434143 and batch: 200, loss is 5.315946960449219 and perplexity is 203.55718249029022
At time: 471.41564893722534 and batch: 250, loss is 5.295050621032715 and perplexity is 199.34771689846065
At time: 473.87808084487915 and batch: 300, loss is 5.293268527984619 and perplexity is 198.9927770798502
At time: 476.34229612350464 and batch: 350, loss is 5.252311038970947 and perplexity is 191.00718382163882
At time: 478.806058883667 and batch: 400, loss is 5.269194822311402 and perplexity is 194.2594860542277
At time: 481.27047204971313 and batch: 450, loss is 5.236755151748657 and perplexity is 188.05888873788575
At time: 483.73414635658264 and batch: 500, loss is 5.237756023406982 and perplexity is 188.24720577459163
At time: 486.19433975219727 and batch: 550, loss is 5.23411940574646 and perplexity is 187.56386593748428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.151689656575521 and perplexity of 172.72308659770763
Finished 17 epochs...
Completing Train Step...
At time: 490.27824211120605 and batch: 50, loss is 5.28955117225647 and perplexity is 198.254423352101
At time: 492.7447247505188 and batch: 100, loss is 5.277566165924072 and perplexity is 195.89252478934802
At time: 495.2370755672455 and batch: 150, loss is 5.29467393875122 and perplexity is 199.27264028655105
At time: 497.7043924331665 and batch: 200, loss is 5.302725601196289 and perplexity is 200.8835930415103
At time: 500.1741895675659 and batch: 250, loss is 5.281923522949219 and perplexity is 196.7479608250406
At time: 502.64194345474243 and batch: 300, loss is 5.280310745239258 and perplexity is 196.43090583767295
At time: 505.10972118377686 and batch: 350, loss is 5.239870958328247 and perplexity is 188.6457576710181
At time: 507.5767750740051 and batch: 400, loss is 5.257530326843262 and perplexity is 192.00671144187012
At time: 510.04513478279114 and batch: 450, loss is 5.222180852890014 and perplexity is 185.33793842487623
At time: 512.5064153671265 and batch: 500, loss is 5.224406251907348 and perplexity is 185.75084856542415
At time: 514.9603779315948 and batch: 550, loss is 5.2208044147491455 and perplexity is 185.08300770590958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.144481404622396 and perplexity of 171.48253155951292
Finished 18 epochs...
Completing Train Step...
At time: 519.0156426429749 and batch: 50, loss is 5.27595778465271 and perplexity is 195.57770816172723
At time: 521.5048308372498 and batch: 100, loss is 5.262590379714966 and perplexity is 192.98073778763435
At time: 523.9679064750671 and batch: 150, loss is 5.280149097442627 and perplexity is 196.3991557807866
At time: 526.4352552890778 and batch: 200, loss is 5.28515175819397 and perplexity is 197.3841358353123
At time: 528.8988540172577 and batch: 250, loss is 5.266803407669068 and perplexity is 193.79548610413204
At time: 531.3627753257751 and batch: 300, loss is 5.2673719692230225 and perplexity is 193.90570209617962
At time: 533.828474521637 and batch: 350, loss is 5.227700395584106 and perplexity is 186.36374748299198
At time: 536.2926862239838 and batch: 400, loss is 5.245631818771362 and perplexity is 189.73565591573976
At time: 538.7571339607239 and batch: 450, loss is 5.210079355239868 and perplexity is 183.1085882431347
At time: 541.2200658321381 and batch: 500, loss is 5.211848659515381 and perplexity is 183.4328498253082
At time: 543.6815648078918 and batch: 550, loss is 5.205965223312378 and perplexity is 182.35680288651326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.140547688802084 and perplexity of 170.80929304485562
Finished 19 epochs...
Completing Train Step...
At time: 547.7259848117828 and batch: 50, loss is 5.260136070251465 and perplexity is 192.507684084158
At time: 550.2186799049377 and batch: 100, loss is 5.249507551193237 and perplexity is 190.47244743004111
At time: 552.6826634407043 and batch: 150, loss is 5.26557225227356 and perplexity is 193.55704055766896
At time: 555.2067594528198 and batch: 200, loss is 5.270522689819336 and perplexity is 194.5176082519793
At time: 557.6678311824799 and batch: 250, loss is 5.253050746917725 and perplexity is 191.14852562278531
At time: 560.1329369544983 and batch: 300, loss is 5.253076009750366 and perplexity is 191.15335463699472
At time: 562.5928790569305 and batch: 350, loss is 5.215689535140991 and perplexity is 184.13874735163773
At time: 565.0571875572205 and batch: 400, loss is 5.23032693862915 and perplexity is 186.85388328740973
At time: 567.5166771411896 and batch: 450, loss is 5.197542390823364 and perplexity is 180.8272925297986
At time: 569.969498872757 and batch: 500, loss is 5.197944259643554 and perplexity is 180.89997598413456
At time: 572.4291658401489 and batch: 550, loss is 5.194093761444091 and perplexity is 180.20476027470173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.1346277872721355 and perplexity of 169.80110597805393
Finished 20 epochs...
Completing Train Step...
At time: 576.4756300449371 and batch: 50, loss is 5.245576705932617 and perplexity is 189.72519933327968
At time: 578.9373803138733 and batch: 100, loss is 5.236671113967896 and perplexity is 188.04308535027397
At time: 581.3955059051514 and batch: 150, loss is 5.249439172744751 and perplexity is 190.4594236648839
At time: 583.8578462600708 and batch: 200, loss is 5.259644575119019 and perplexity is 192.41309074245984
At time: 586.3145365715027 and batch: 250, loss is 5.239092121124267 and perplexity is 188.49889053678908
At time: 588.765171289444 and batch: 300, loss is 5.239983005523682 and perplexity is 188.6668960833236
At time: 591.214358329773 and batch: 350, loss is 5.198929948806763 and perplexity is 181.07837503865
At time: 593.6758329868317 and batch: 400, loss is 5.221726417541504 and perplexity is 185.2537334485439
At time: 596.1319041252136 and batch: 450, loss is 5.186372270584107 and perplexity is 178.81866909721424
At time: 598.5908753871918 and batch: 500, loss is 5.189493646621704 and perplexity is 179.37770142718182
At time: 601.0491452217102 and batch: 550, loss is 5.183981056213379 and perplexity is 178.39158615285328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.128475443522135 and perplexity of 168.7596382238375
Finished 21 epochs...
Completing Train Step...
At time: 605.0682144165039 and batch: 50, loss is 5.236412057876587 and perplexity is 187.9943779528318
At time: 607.5599241256714 and batch: 100, loss is 5.225508069992065 and perplexity is 185.95562500209903
At time: 610.012773513794 and batch: 150, loss is 5.240099544525147 and perplexity is 188.68888441622693
At time: 612.4641036987305 and batch: 200, loss is 5.247881431579589 and perplexity is 190.1629681407822
At time: 614.9252429008484 and batch: 250, loss is 5.227378702163696 and perplexity is 186.3038051336703
At time: 617.4253880977631 and batch: 300, loss is 5.231106653213501 and perplexity is 186.99963279947744
At time: 619.8802943229675 and batch: 350, loss is 5.189704666137695 and perplexity is 179.41555761697347
At time: 622.3447141647339 and batch: 400, loss is 5.2091308689117435 and perplexity is 182.9349945892417
At time: 624.804181098938 and batch: 450, loss is 5.174290628433227 and perplexity is 176.67124425738365
At time: 627.2580997943878 and batch: 500, loss is 5.176553440093994 and perplexity is 177.07147065671933
At time: 629.7108631134033 and batch: 550, loss is 5.174366626739502 and perplexity is 176.6846714829314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.130328369140625 and perplexity of 169.07262716393262
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 633.7480621337891 and batch: 50, loss is 5.223930034637451 and perplexity is 185.66241186265316
At time: 636.2146546840668 and batch: 100, loss is 5.204892807006836 and perplexity is 182.16134530235388
At time: 638.6888611316681 and batch: 150, loss is 5.211476354598999 and perplexity is 183.36456958481568
At time: 641.1571135520935 and batch: 200, loss is 5.202662343978882 and perplexity is 181.75549394301862
At time: 643.6288039684296 and batch: 250, loss is 5.166122283935547 and perplexity is 175.23401057261808
At time: 646.091894865036 and batch: 300, loss is 5.159225072860718 and perplexity is 174.02954312218765
At time: 648.5466470718384 and batch: 350, loss is 5.093431224822998 and perplexity is 162.9480152060376
At time: 651.0089395046234 and batch: 400, loss is 5.093269758224487 and perplexity is 162.92170666832035
At time: 653.4679148197174 and batch: 450, loss is 5.0506524658203125 and perplexity is 156.12429702918746
At time: 655.9303297996521 and batch: 500, loss is 5.051146831512451 and perplexity is 156.20149860668127
At time: 658.397607088089 and batch: 550, loss is 5.093292751312256 and perplexity is 162.92545278448844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0578765869140625 and perplexity of 157.2562415860625
Finished 23 epochs...
Completing Train Step...
At time: 662.4638864994049 and batch: 50, loss is 5.177759323120117 and perplexity is 177.28512693394322
At time: 664.9440772533417 and batch: 100, loss is 5.166999216079712 and perplexity is 175.3877463072919
At time: 667.40740275383 and batch: 150, loss is 5.175963325500488 and perplexity is 176.9670090229899
At time: 669.8695373535156 and batch: 200, loss is 5.1699839782714845 and perplexity is 175.91201904687546
At time: 672.3375947475433 and batch: 250, loss is 5.141624307632446 and perplexity is 170.9932885749022
At time: 674.8055391311646 and batch: 300, loss is 5.137857484817505 and perplexity is 170.3503987405053
At time: 677.2724740505219 and batch: 350, loss is 5.075843410491943 and perplexity is 160.1071711179306
At time: 679.765216588974 and batch: 400, loss is 5.086350889205932 and perplexity is 161.79836333357113
At time: 682.2312846183777 and batch: 450, loss is 5.052443237304687 and perplexity is 156.40413045240044
At time: 684.6956777572632 and batch: 500, loss is 5.055720052719116 and perplexity is 156.9174785320571
At time: 687.1598815917969 and batch: 550, loss is 5.086887693405151 and perplexity is 161.8852406904141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.050764973958334 and perplexity of 156.14186327130014
Finished 24 epochs...
Completing Train Step...
At time: 691.23011302948 and batch: 50, loss is 5.161357088088989 and perplexity is 174.40097256421257
At time: 693.6918001174927 and batch: 100, loss is 5.150163354873658 and perplexity is 172.45966014194605
At time: 696.1471273899078 and batch: 150, loss is 5.1602928256988525 and perplexity is 174.2154629012977
At time: 698.609828710556 and batch: 200, loss is 5.1565474510192875 and perplexity is 173.56418112623237
At time: 701.0748469829559 and batch: 250, loss is 5.131511116027832 and perplexity is 169.27271559107976
At time: 703.536054611206 and batch: 300, loss is 5.12928876876831 and perplexity is 168.8969505303537
At time: 705.997428894043 and batch: 350, loss is 5.0708314037323 and perplexity is 159.30672050159345
At time: 708.4517996311188 and batch: 400, loss is 5.086948184967041 and perplexity is 161.89503367766397
At time: 710.9102053642273 and batch: 450, loss is 5.055339317321778 and perplexity is 156.8577458654107
At time: 713.3736352920532 and batch: 500, loss is 5.058002471923828 and perplexity is 157.27603903564517
At time: 715.8298003673553 and batch: 550, loss is 5.081942195892334 and perplexity is 161.08661406594027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.047477722167969 and perplexity of 155.62942836444176
Finished 25 epochs...
Completing Train Step...
At time: 719.8588943481445 and batch: 50, loss is 5.151301641464233 and perplexity is 172.65608043058273
At time: 722.3449246883392 and batch: 100, loss is 5.139595184326172 and perplexity is 170.64667388873502
At time: 724.8119602203369 and batch: 150, loss is 5.150717735290527 and perplexity is 172.55529490679982
At time: 727.2700791358948 and batch: 200, loss is 5.1487464427948 and perplexity is 172.21547300308185
At time: 729.7358927726746 and batch: 250, loss is 5.126274442672729 and perplexity is 168.38860658702086
At time: 732.197381734848 and batch: 300, loss is 5.12580569267273 and perplexity is 168.30969292451786
At time: 734.6581997871399 and batch: 350, loss is 5.068695011138916 and perplexity is 158.96674209682055
At time: 737.1252198219299 and batch: 400, loss is 5.087404499053955 and perplexity is 161.96892551979454
At time: 739.5859568119049 and batch: 450, loss is 5.056357421875 and perplexity is 157.01752477269957
At time: 742.068009853363 and batch: 500, loss is 5.058031234741211 and perplexity is 157.2805628026925
At time: 744.519348859787 and batch: 550, loss is 5.0772401809692385 and perplexity is 160.3309603424585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.044320170084635 and perplexity of 155.13879534597612
Finished 26 epochs...
Completing Train Step...
At time: 748.5384261608124 and batch: 50, loss is 5.143636302947998 and perplexity is 171.33767260385648
At time: 750.9952704906464 and batch: 100, loss is 5.131729297637939 and perplexity is 169.30965181397033
At time: 753.4541306495667 and batch: 150, loss is 5.143571910858154 and perplexity is 171.32664016825308
At time: 755.9216458797455 and batch: 200, loss is 5.143322191238403 and perplexity is 171.28386188632757
At time: 758.3788299560547 and batch: 250, loss is 5.122526845932007 and perplexity is 167.75873498336983
At time: 760.8415076732635 and batch: 300, loss is 5.12347978591919 and perplexity is 167.91867518473805
At time: 763.3014893531799 and batch: 350, loss is 5.066912164688111 and perplexity is 158.68358129599463
At time: 765.7593841552734 and batch: 400, loss is 5.0872594928741455 and perplexity is 161.94544072741823
At time: 768.2241933345795 and batch: 450, loss is 5.056567363739013 and perplexity is 157.05049278509503
At time: 770.6784267425537 and batch: 500, loss is 5.057142724990845 and perplexity is 157.14087955325317
At time: 773.1392061710358 and batch: 550, loss is 5.073860712051392 and perplexity is 159.7900413696515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.042874654134114 and perplexity of 154.91470174721522
Finished 27 epochs...
Completing Train Step...
At time: 777.1286177635193 and batch: 50, loss is 5.137955045700073 and perplexity is 170.3670190864869
At time: 779.6308636665344 and batch: 100, loss is 5.126410789489746 and perplexity is 168.41156740283247
At time: 782.090060710907 and batch: 150, loss is 5.138741912841797 and perplexity is 170.50112805188917
At time: 784.5570094585419 and batch: 200, loss is 5.1396270179748536 and perplexity is 170.65210628146625
At time: 787.0237925052643 and batch: 250, loss is 5.120155239105225 and perplexity is 167.36134863058348
At time: 789.4909880161285 and batch: 300, loss is 5.1219107532501225 and perplexity is 167.65541188599468
At time: 791.9607884883881 and batch: 350, loss is 5.066096286773682 and perplexity is 158.55416766667256
At time: 794.4320981502533 and batch: 400, loss is 5.087031688690185 and perplexity is 161.90855308018698
At time: 796.8957440853119 and batch: 450, loss is 5.056398878097534 and perplexity is 157.02403426107674
At time: 799.3594686985016 and batch: 500, loss is 5.055615110397339 and perplexity is 156.9010121115598
At time: 801.8231809139252 and batch: 550, loss is 5.070496454238891 and perplexity is 159.2533697316703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.041297403971354 and perplexity of 154.67055509945448
Finished 28 epochs...
Completing Train Step...
At time: 805.8630707263947 and batch: 50, loss is 5.1331393337249756 and perplexity is 169.54855292292206
At time: 808.3206424713135 and batch: 100, loss is 5.121725044250488 and perplexity is 167.624279658027
At time: 810.776668548584 and batch: 150, loss is 5.135004272460938 and perplexity is 169.86504561490477
At time: 813.2306106090546 and batch: 200, loss is 5.136840810775757 and perplexity is 170.17729592158605
At time: 815.6870641708374 and batch: 250, loss is 5.118118515014649 and perplexity is 167.0208266323888
At time: 818.1504638195038 and batch: 300, loss is 5.120092344284058 and perplexity is 167.35082279950464
At time: 820.6048903465271 and batch: 350, loss is 5.06486946105957 and perplexity is 158.35976860796396
At time: 823.0583698749542 and batch: 400, loss is 5.085903120040894 and perplexity is 161.7259312331574
At time: 825.5101227760315 and batch: 450, loss is 5.055466814041138 and perplexity is 156.87774598836268
At time: 827.9784817695618 and batch: 500, loss is 5.053616800308228 and perplexity is 156.58778829944902
At time: 830.4389445781708 and batch: 550, loss is 5.067275514602661 and perplexity is 158.7412494379219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.039652506510417 and perplexity of 154.4163470265138
Finished 29 epochs...
Completing Train Step...
At time: 834.4641897678375 and batch: 50, loss is 5.12878228187561 and perplexity is 168.8114280985167
At time: 836.9704899787903 and batch: 100, loss is 5.117831516265869 and perplexity is 166.9728987420764
At time: 839.4366147518158 and batch: 150, loss is 5.130805101394653 and perplexity is 169.1532487545043
At time: 841.8946487903595 and batch: 200, loss is 5.13323205947876 and perplexity is 169.5642751692122
At time: 844.3609600067139 and batch: 250, loss is 5.1146823596954345 and perplexity is 166.4479020232258
At time: 846.8256163597107 and batch: 300, loss is 5.117542953491211 and perplexity is 166.92472353024337
At time: 849.2830154895782 and batch: 350, loss is 5.062145843505859 and perplexity is 157.9290439932054
At time: 851.7471394538879 and batch: 400, loss is 5.083489303588867 and perplexity is 161.336025289634
At time: 854.2171070575714 and batch: 450, loss is 5.053512191772461 and perplexity is 156.57140873693197
At time: 856.6825513839722 and batch: 500, loss is 5.050840539932251 and perplexity is 156.15366272908065
At time: 859.1497271060944 and batch: 550, loss is 5.063985795974731 and perplexity is 158.2198934201138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.036313883463541 and perplexity of 153.90166868806557
Finished 30 epochs...
Completing Train Step...
At time: 863.2672772407532 and batch: 50, loss is 5.1244793891906735 and perplexity is 168.08661116248874
At time: 865.7331240177155 and batch: 100, loss is 5.113831357955933 and perplexity is 166.30631482308922
At time: 868.252815246582 and batch: 150, loss is 5.127646694183349 and perplexity is 168.61983672344786
At time: 870.7161355018616 and batch: 200, loss is 5.13079345703125 and perplexity is 169.15127908407274
At time: 873.1791186332703 and batch: 250, loss is 5.112516622543335 and perplexity is 166.0878096913444
At time: 875.6400501728058 and batch: 300, loss is 5.11555419921875 and perplexity is 166.5930811600414
At time: 878.0963037014008 and batch: 350, loss is 5.060446929931641 and perplexity is 157.6609639834659
At time: 880.5599040985107 and batch: 400, loss is 5.081642265319824 and perplexity is 161.03830651037833
At time: 883.0176179409027 and batch: 450, loss is 5.051772966384887 and perplexity is 156.29933243747604
At time: 885.472772359848 and batch: 500, loss is 5.048558168411255 and perplexity is 155.7976684664895
At time: 887.9347913265228 and batch: 550, loss is 5.060531082153321 and perplexity is 157.67423206211862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.033918762207032 and perplexity of 153.5334966143632
Finished 31 epochs...
Completing Train Step...
At time: 891.9481778144836 and batch: 50, loss is 5.120961675643921 and perplexity is 167.49636937279712
At time: 894.4342231750488 and batch: 100, loss is 5.110590991973877 and perplexity is 165.76829366143747
At time: 896.8988502025604 and batch: 150, loss is 5.124962787628174 and perplexity is 168.16788360959384
At time: 899.3586297035217 and batch: 200, loss is 5.1288817024230955 and perplexity is 168.82821225745136
At time: 901.8165509700775 and batch: 250, loss is 5.1103219795227055 and perplexity is 165.72370592403033
At time: 904.2711985111237 and batch: 300, loss is 5.113183898925781 and perplexity is 166.19867314833328
At time: 906.7227849960327 and batch: 350, loss is 5.0580545425415036 and perplexity is 157.28422870936214
At time: 909.1718716621399 and batch: 400, loss is 5.079595861434936 and perplexity is 160.70909406002716
At time: 911.6353538036346 and batch: 450, loss is 5.049450016021728 and perplexity is 155.9366782233745
At time: 914.0999708175659 and batch: 500, loss is 5.045940637588501 and perplexity is 155.39039652313198
At time: 916.5580458641052 and batch: 550, loss is 5.057087621688843 and perplexity is 157.1322208104751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.031340026855469 and perplexity of 153.13808440983001
Finished 32 epochs...
Completing Train Step...
At time: 920.5930299758911 and batch: 50, loss is 5.1170604705810545 and perplexity is 166.84420462991616
At time: 923.0505602359772 and batch: 100, loss is 5.106835174560547 and perplexity is 165.1468659325651
At time: 925.5129089355469 and batch: 150, loss is 5.12197660446167 and perplexity is 167.66645256150684
At time: 927.9708561897278 and batch: 200, loss is 5.126280145645142 and perplexity is 168.38956690533715
At time: 930.4764189720154 and batch: 250, loss is 5.10729061126709 and perplexity is 165.22209700747752
At time: 932.9401154518127 and batch: 300, loss is 5.110285167694092 and perplexity is 165.7176054436562
At time: 935.4008536338806 and batch: 350, loss is 5.055766143798828 and perplexity is 156.92471119474766
At time: 937.8641469478607 and batch: 400, loss is 5.0770890998840335 and perplexity is 160.30673919670258
At time: 940.3244042396545 and batch: 450, loss is 5.046857500076294 and perplexity is 155.53293348207762
At time: 942.787606716156 and batch: 500, loss is 5.042930793762207 and perplexity is 154.9233988450811
At time: 945.2504937648773 and batch: 550, loss is 5.05334623336792 and perplexity is 156.54542655178196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.029086303710938 and perplexity of 152.79334218736903
Finished 33 epochs...
Completing Train Step...
At time: 949.2401971817017 and batch: 50, loss is 5.1135702896118165 and perplexity is 166.2629031758123
At time: 951.7279546260834 and batch: 100, loss is 5.10310453414917 and perplexity is 164.53191016335637
At time: 954.1940042972565 and batch: 150, loss is 5.118714656829834 and perplexity is 167.12042441540314
At time: 956.6507811546326 and batch: 200, loss is 5.123646411895752 and perplexity is 167.94665712916841
At time: 959.1115684509277 and batch: 250, loss is 5.104915056228638 and perplexity is 164.83006864929837
At time: 961.5671286582947 and batch: 300, loss is 5.108046655654907 and perplexity is 165.3470594792987
At time: 964.0230512619019 and batch: 350, loss is 5.053633060455322 and perplexity is 156.5903344606204
At time: 966.4814696311951 and batch: 400, loss is 5.07464900970459 and perplexity is 159.9160531451356
At time: 968.9375185966492 and batch: 450, loss is 5.044412117004395 and perplexity is 155.15306053615353
At time: 971.3975594043732 and batch: 500, loss is 5.0399611377716065 and perplexity is 154.46401209353138
At time: 973.8585369586945 and batch: 550, loss is 5.0502527236938475 and perplexity is 156.06190004288305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.026694742838542 and perplexity of 152.42836421610835
Finished 34 epochs...
Completing Train Step...
At time: 977.841267824173 and batch: 50, loss is 5.110539464950562 and perplexity is 165.75975233476157
At time: 980.3002247810364 and batch: 100, loss is 5.099793119430542 and perplexity is 163.98797786410745
At time: 982.7689006328583 and batch: 150, loss is 5.115689744949341 and perplexity is 166.61566367138553
At time: 985.226722240448 and batch: 200, loss is 5.1210799980163575 and perplexity is 167.51618911313207
At time: 987.683943271637 and batch: 250, loss is 5.102719793319702 and perplexity is 164.46862019566413
At time: 990.150132894516 and batch: 300, loss is 5.105831212997437 and perplexity is 164.98114802802345
At time: 992.6525490283966 and batch: 350, loss is 5.051669788360596 and perplexity is 156.28320661308672
At time: 995.1119570732117 and batch: 400, loss is 5.072031373977661 and perplexity is 159.4979985671228
At time: 997.5747520923615 and batch: 450, loss is 5.0421818828582765 and perplexity is 154.80741845732132
At time: 1000.0367076396942 and batch: 500, loss is 5.037285051345825 and perplexity is 154.05120564671785
At time: 1002.4986808300018 and batch: 550, loss is 5.047170543670655 and perplexity is 155.5816296922369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.024757893880208 and perplexity of 152.1334192218127
Finished 35 epochs...
Completing Train Step...
At time: 1006.4920644760132 and batch: 50, loss is 5.107367753982544 and perplexity is 165.23484318032453
At time: 1008.9765951633453 and batch: 100, loss is 5.096773891448975 and perplexity is 163.493607456729
At time: 1011.4405555725098 and batch: 150, loss is 5.112883520126343 and perplexity is 166.1487580875131
At time: 1013.9011061191559 and batch: 200, loss is 5.118449239730835 and perplexity is 167.07607368315308
At time: 1016.3647613525391 and batch: 250, loss is 5.100816917419434 and perplexity is 164.15595439839393
At time: 1018.8196048736572 and batch: 300, loss is 5.10379997253418 and perplexity is 164.6463717651255
At time: 1021.2820920944214 and batch: 350, loss is 5.049819355010986 and perplexity is 155.99428235556894
At time: 1023.7383186817169 and batch: 400, loss is 5.06957592010498 and perplexity is 159.10683902249895
At time: 1026.1997826099396 and batch: 450, loss is 5.039731407165528 and perplexity is 154.4285310581117
At time: 1028.670082807541 and batch: 500, loss is 5.034645586013794 and perplexity is 153.64512897845356
At time: 1031.1258828639984 and batch: 550, loss is 5.0443658351898195 and perplexity is 155.14587993714184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.022808837890625 and perplexity of 151.83719144589938
Finished 36 epochs...
Completing Train Step...
At time: 1035.1506717205048 and batch: 50, loss is 5.104571924209595 and perplexity is 164.77351987744575
At time: 1037.6087007522583 and batch: 100, loss is 5.094015598297119 and perplexity is 163.043265531956
At time: 1040.0661675930023 and batch: 150, loss is 5.110096168518067 and perplexity is 165.68628791236154
At time: 1042.528978586197 and batch: 200, loss is 5.115873565673828 and perplexity is 166.64629389854156
At time: 1044.985026359558 and batch: 250, loss is 5.098817882537841 and perplexity is 163.82812869620713
At time: 1047.4519901275635 and batch: 300, loss is 5.1020669174194335 and perplexity is 164.36127764168415
At time: 1049.9121203422546 and batch: 350, loss is 5.047891550064087 and perplexity is 155.69384549123856
At time: 1052.3695974349976 and batch: 400, loss is 5.067082567214966 and perplexity is 158.710623683198
At time: 1054.8861470222473 and batch: 450, loss is 5.036821861267089 and perplexity is 153.97986717955558
At time: 1057.338588476181 and batch: 500, loss is 5.031977691650391 and perplexity is 153.23576631586627
At time: 1059.8070983886719 and batch: 550, loss is 5.041910514831543 and perplexity is 154.76541437319136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.021818033854166 and perplexity of 151.68682504784846
Finished 37 epochs...
Completing Train Step...
At time: 1063.8240313529968 and batch: 50, loss is 5.101786165237427 and perplexity is 164.31513933136756
At time: 1066.31995511055 and batch: 100, loss is 5.091322193145752 and perplexity is 162.6047148231652
At time: 1068.7867300510406 and batch: 150, loss is 5.107363443374634 and perplexity is 165.23413091923769
At time: 1071.2547976970673 and batch: 200, loss is 5.113286771774292 and perplexity is 166.2157713587145
At time: 1073.7241792678833 and batch: 250, loss is 5.09679986000061 and perplexity is 163.497853204044
At time: 1076.1850197315216 and batch: 300, loss is 5.100452680587768 and perplexity is 164.09617364150247
At time: 1078.637578010559 and batch: 350, loss is 5.045824928283691 and perplexity is 155.37241744856937
At time: 1081.1000537872314 and batch: 400, loss is 5.064583196640014 and perplexity is 158.3144423286825
At time: 1083.557454109192 and batch: 450, loss is 5.03368896484375 and perplexity is 153.49821907516386
At time: 1086.010213136673 and batch: 500, loss is 5.029454727172851 and perplexity is 152.84964521049554
At time: 1088.469496011734 and batch: 550, loss is 5.039075927734375 and perplexity is 154.3273395005354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.019101969401041 and perplexity of 151.27539284544665
Finished 38 epochs...
Completing Train Step...
At time: 1092.4885783195496 and batch: 50, loss is 5.098699407577515 and perplexity is 163.80872031488576
At time: 1094.9603595733643 and batch: 100, loss is 5.089083471298218 and perplexity is 162.24109526896663
At time: 1097.419599056244 and batch: 150, loss is 5.1054184532165525 and perplexity is 164.91306449755155
At time: 1099.8724927902222 and batch: 200, loss is 5.111352062225341 and perplexity is 165.89450299937357
At time: 1102.3333802223206 and batch: 250, loss is 5.094713754653931 and perplexity is 163.15713496893292
At time: 1104.8003985881805 and batch: 300, loss is 5.09827166557312 and perplexity is 163.73866742787013
At time: 1107.2660233974457 and batch: 350, loss is 5.042969369888306 and perplexity is 154.92937530492418
At time: 1109.736411333084 and batch: 400, loss is 5.061292629241944 and perplexity is 157.79435414800335
At time: 1112.2088720798492 and batch: 450, loss is 5.030601320266723 and perplexity is 153.02500207036547
At time: 1114.6795806884766 and batch: 500, loss is 5.026814060211182 and perplexity is 152.44655265311908
At time: 1117.2014708518982 and batch: 550, loss is 5.036225891113281 and perplexity is 153.8881271143149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.017412821451823 and perplexity of 151.02008201544191
Finished 39 epochs...
Completing Train Step...
At time: 1121.2394416332245 and batch: 50, loss is 5.096413307189941 and perplexity is 163.43466486295185
At time: 1123.7351808547974 and batch: 100, loss is 5.087280769348144 and perplexity is 161.94888639203273
At time: 1126.2015707492828 and batch: 150, loss is 5.103507213592529 and perplexity is 164.5981771226314
At time: 1128.6681118011475 and batch: 200, loss is 5.108921890258789 and perplexity is 165.49184029675354
At time: 1131.136915922165 and batch: 250, loss is 5.092364625930786 and perplexity is 162.77430768812175
At time: 1133.6037294864655 and batch: 300, loss is 5.096098413467407 and perplexity is 163.38320841502355
At time: 1136.0671560764313 and batch: 350, loss is 5.039828519821167 and perplexity is 154.4435287510904
At time: 1138.531193971634 and batch: 400, loss is 5.058785457611084 and perplexity is 157.3992321460856
At time: 1140.9895951747894 and batch: 450, loss is 5.0291733551025395 and perplexity is 152.8066436393805
At time: 1143.448312997818 and batch: 500, loss is 5.024530668258667 and perplexity is 152.09885453821232
At time: 1145.906436920166 and batch: 550, loss is 5.03414665222168 and perplexity is 153.568489352252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.017198689778646 and perplexity of 150.9877472946641
Finished 40 epochs...
Completing Train Step...
At time: 1149.934462070465 and batch: 50, loss is 5.095029220581055 and perplexity is 163.20861360523375
At time: 1152.3934495449066 and batch: 100, loss is 5.084619073867798 and perplexity is 161.51840093782212
At time: 1154.851155281067 and batch: 150, loss is 5.099544553756714 and perplexity is 163.94722114746023
At time: 1157.3080217838287 and batch: 200, loss is 5.106014051437378 and perplexity is 165.0113156815679
At time: 1159.770902633667 and batch: 250, loss is 5.090234432220459 and perplexity is 162.427935932128
At time: 1162.2258381843567 and batch: 300, loss is 5.094119329452514 and perplexity is 163.06017907548457
At time: 1164.6856052875519 and batch: 350, loss is 5.038137540817261 and perplexity is 154.18258867095938
At time: 1167.147384405136 and batch: 400, loss is 5.056764888763428 and perplexity is 157.08151725147002
At time: 1169.6172497272491 and batch: 450, loss is 5.0264827919006345 and perplexity is 152.39606030489216
At time: 1172.0737235546112 and batch: 500, loss is 5.023029193878174 and perplexity is 151.87065336679908
At time: 1174.5252656936646 and batch: 550, loss is 5.031714153289795 and perplexity is 153.19538813406038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.014930216471354 and perplexity of 150.64562381590082
Finished 41 epochs...
Completing Train Step...
At time: 1178.5350217819214 and batch: 50, loss is 5.0915173816680905 and perplexity is 162.6364564948811
At time: 1181.024582862854 and batch: 100, loss is 5.082230577468872 and perplexity is 161.1330751766059
At time: 1183.4851732254028 and batch: 150, loss is 5.097826061248779 and perplexity is 163.66572102342613
At time: 1185.95605301857 and batch: 200, loss is 5.104566879272461 and perplexity is 164.77268860749348
At time: 1188.4232339859009 and batch: 250, loss is 5.089070863723755 and perplexity is 162.23904981517126
At time: 1190.8900921344757 and batch: 300, loss is 5.092604513168335 and perplexity is 162.81335985100745
At time: 1193.3552088737488 and batch: 350, loss is 5.036284265518188 and perplexity is 153.89711050435497
At time: 1195.8267586231232 and batch: 400, loss is 5.0542137145996096 and perplexity is 156.68128569032913
At time: 1198.2995069026947 and batch: 450, loss is 5.023478727340699 and perplexity is 151.93893965479873
At time: 1200.7671852111816 and batch: 500, loss is 5.0197759532928465 and perplexity is 151.37738438991917
At time: 1203.2402982711792 and batch: 550, loss is 5.028943061828613 and perplexity is 152.77145734887765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.014234415690104 and perplexity of 150.54084093139164
Finished 42 epochs...
Completing Train Step...
At time: 1207.3071503639221 and batch: 50, loss is 5.089688692092896 and perplexity is 162.3393166733948
At time: 1209.7719721794128 and batch: 100, loss is 5.080807466506958 and perplexity is 160.90392802058318
At time: 1212.2385745048523 and batch: 150, loss is 5.095950298309326 and perplexity is 163.3590106773514
At time: 1214.7050108909607 and batch: 200, loss is 5.103066568374634 and perplexity is 164.52566370052767
At time: 1217.1700241565704 and batch: 250, loss is 5.087601079940796 and perplexity is 162.00076864458615
At time: 1219.6376159191132 and batch: 300, loss is 5.090254755020141 and perplexity is 162.43123695607565
At time: 1222.1030712127686 and batch: 350, loss is 5.034171924591065 and perplexity is 153.5723704408827
At time: 1224.5646317005157 and batch: 400, loss is 5.052072076797486 and perplexity is 156.34609018780662
At time: 1227.0214672088623 and batch: 450, loss is 5.021117734909057 and perplexity is 151.58063611062065
At time: 1229.4864468574524 and batch: 500, loss is 5.017582101821899 and perplexity is 151.04564891474044
At time: 1231.95303440094 and batch: 550, loss is 5.0272079277038575 and perplexity is 152.5066082207712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.013105773925782 and perplexity of 150.37103009690608
Finished 43 epochs...
Completing Train Step...
At time: 1236.0004794597626 and batch: 50, loss is 5.088063840866089 and perplexity is 162.07575361890224
At time: 1238.491954088211 and batch: 100, loss is 5.078708782196045 and perplexity is 160.56659557225493
At time: 1240.9641530513763 and batch: 150, loss is 5.093953371047974 and perplexity is 163.03312011371426
At time: 1243.4766619205475 and batch: 200, loss is 5.1012914180755615 and perplexity is 164.2338649893147
At time: 1245.9444425106049 and batch: 250, loss is 5.085940217971801 and perplexity is 161.73193104186984
At time: 1248.4129378795624 and batch: 300, loss is 5.0884393787384035 and perplexity is 162.13663063266674
At time: 1250.8783388137817 and batch: 350, loss is 5.032788763046264 and perplexity is 153.36010187844298
At time: 1253.3436901569366 and batch: 400, loss is 5.0502916431427005 and perplexity is 156.06797400421652
At time: 1255.8100204467773 and batch: 450, loss is 5.018817472457886 and perplexity is 151.23236158002942
At time: 1258.2780356407166 and batch: 500, loss is 5.015355453491211 and perplexity is 150.7096975343198
At time: 1260.745992898941 and batch: 550, loss is 5.025173358917236 and perplexity is 152.1966384702472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.012560017903646 and perplexity of 150.28898659158094
Finished 44 epochs...
Completing Train Step...
At time: 1264.8027470111847 and batch: 50, loss is 5.08612829208374 and perplexity is 161.7623514917325
At time: 1267.2961082458496 and batch: 100, loss is 5.076784763336182 and perplexity is 160.25795942020898
At time: 1269.763811826706 and batch: 150, loss is 5.092346200942993 and perplexity is 162.7713086011187
At time: 1272.2289280891418 and batch: 200, loss is 5.099735431671142 and perplexity is 163.97851803795672
At time: 1274.6956338882446 and batch: 250, loss is 5.084377164840698 and perplexity is 161.47933290423808
At time: 1277.1681683063507 and batch: 300, loss is 5.086735010147095 and perplexity is 161.86052541128694
At time: 1279.6317212581635 and batch: 350, loss is 5.031394710540772 and perplexity is 153.14645879361473
At time: 1282.0937736034393 and batch: 400, loss is 5.048609180450439 and perplexity is 155.8056162259722
At time: 1284.560703754425 and batch: 450, loss is 5.016301279067993 and perplexity is 150.85231005356934
At time: 1287.032793521881 and batch: 500, loss is 5.013063516616821 and perplexity is 150.36467595608397
At time: 1289.48663687706 and batch: 550, loss is 5.02301703453064 and perplexity is 151.8688067299715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.0114390055338545 and perplexity of 150.120605175027
Finished 45 epochs...
Completing Train Step...
At time: 1293.555948972702 and batch: 50, loss is 5.0842024612426755 and perplexity is 161.45112434791326
At time: 1296.0239067077637 and batch: 100, loss is 5.074541711807251 and perplexity is 159.8988954093934
At time: 1298.490082502365 and batch: 150, loss is 5.090459480285644 and perplexity is 162.46449413836405
At time: 1300.9474415779114 and batch: 200, loss is 5.098041505813598 and perplexity is 163.70098571212424
At time: 1303.443396806717 and batch: 250, loss is 5.082925910949707 and perplexity is 161.24515536068262
At time: 1305.9101040363312 and batch: 300, loss is 5.085040197372437 and perplexity is 161.5864344571403
At time: 1308.37442111969 and batch: 350, loss is 5.029961061477661 and perplexity is 152.9270578260212
At time: 1310.837159395218 and batch: 400, loss is 5.046844635009766 and perplexity is 155.53093255341207
At time: 1313.302258014679 and batch: 450, loss is 5.014476547241211 and perplexity is 150.57729603199823
At time: 1315.77241563797 and batch: 500, loss is 5.011505174636841 and perplexity is 150.13053884945873
At time: 1318.239230632782 and batch: 550, loss is 5.021648473739624 and perplexity is 151.66110719284168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.012136840820313 and perplexity of 150.22540119146896
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1322.3412818908691 and batch: 50, loss is 5.083814706802368 and perplexity is 161.3885330933574
At time: 1324.8431460857391 and batch: 100, loss is 5.072445011138916 and perplexity is 159.56398651306833
At time: 1327.3167886734009 and batch: 150, loss is 5.086406269073486 and perplexity is 161.8073239536197
At time: 1329.7891955375671 and batch: 200, loss is 5.09149655342102 and perplexity is 162.63306909785948
At time: 1332.2574048042297 and batch: 250, loss is 5.071806421279907 and perplexity is 159.46212309735193
At time: 1334.7288467884064 and batch: 300, loss is 5.0711605262756345 and perplexity is 159.35916056374515
At time: 1337.1930372714996 and batch: 350, loss is 5.012350959777832 and perplexity is 150.25757074169735
At time: 1339.6622524261475 and batch: 400, loss is 5.023964881896973 and perplexity is 152.01282342059685
At time: 1342.119699716568 and batch: 450, loss is 4.991429538726806 and perplexity is 147.14662501326822
At time: 1344.577266216278 and batch: 500, loss is 4.992191047668457 and perplexity is 147.2587211596414
At time: 1347.0379927158356 and batch: 550, loss is 5.010410976409912 and perplexity is 149.96635612064816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.002708435058594 and perplexity of 148.81567134893004
Finished 47 epochs...
Completing Train Step...
At time: 1351.132483959198 and batch: 50, loss is 5.078606662750244 and perplexity is 160.5501994376973
At time: 1353.5908880233765 and batch: 100, loss is 5.06834059715271 and perplexity is 158.91041204274882
At time: 1356.0529172420502 and batch: 150, loss is 5.083071203231811 and perplexity is 161.26858473929593
At time: 1358.5129544734955 and batch: 200, loss is 5.08860333442688 and perplexity is 162.16321603492378
At time: 1360.9743437767029 and batch: 250, loss is 5.0693410205841065 and perplexity is 159.06946929148262
At time: 1363.4397866725922 and batch: 300, loss is 5.069550657272339 and perplexity is 159.1028195838241
At time: 1365.94211435318 and batch: 350, loss is 5.011420431137085 and perplexity is 150.11781680123954
At time: 1368.3953585624695 and batch: 400, loss is 5.0241100883483885 and perplexity is 152.03489826592164
At time: 1370.8586056232452 and batch: 450, loss is 4.992462186813355 and perplexity is 147.29865417683283
At time: 1373.321778535843 and batch: 500, loss is 4.9932041835784915 and perplexity is 147.40798986008636
At time: 1375.7874748706818 and batch: 550, loss is 5.009885864257813 and perplexity is 149.88762763709386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.001690673828125 and perplexity of 148.66428957646167
Finished 48 epochs...
Completing Train Step...
At time: 1379.8913164138794 and batch: 50, loss is 5.07661075592041 and perplexity is 160.23007577288453
At time: 1382.3783910274506 and batch: 100, loss is 5.066445579528809 and perplexity is 158.60955916209454
At time: 1384.84641623497 and batch: 150, loss is 5.081430940628052 and perplexity is 161.00427873547267
At time: 1387.3159420490265 and batch: 200, loss is 5.087243776321412 and perplexity is 161.94289552336
At time: 1389.7825605869293 and batch: 250, loss is 5.0682958889007566 and perplexity is 158.90330759482418
At time: 1392.2510159015656 and batch: 300, loss is 5.069054651260376 and perplexity is 159.02392319694883
At time: 1394.7216873168945 and batch: 350, loss is 5.011400423049927 and perplexity is 150.11481326092456
At time: 1397.1893634796143 and batch: 400, loss is 5.024646005630493 and perplexity is 152.11639823205493
At time: 1399.6574034690857 and batch: 450, loss is 4.993312730789184 and perplexity is 147.423991454668
At time: 1402.1284353733063 and batch: 500, loss is 4.993792858123779 and perplexity is 147.49479073771593
At time: 1404.598018169403 and batch: 550, loss is 5.009345149993896 and perplexity is 149.80660316636352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.001201883951823 and perplexity of 148.59164173296068
Finished 49 epochs...
Completing Train Step...
At time: 1408.6851711273193 and batch: 50, loss is 5.0751799201965335 and perplexity is 160.00097679701472
At time: 1411.150280714035 and batch: 100, loss is 5.06512752532959 and perplexity is 158.40064087966
At time: 1413.6150856018066 and batch: 150, loss is 5.080312566757202 and perplexity is 160.82431640837953
At time: 1416.0833096504211 and batch: 200, loss is 5.086369848251342 and perplexity is 161.80143090516788
At time: 1418.5485863685608 and batch: 250, loss is 5.067663917541504 and perplexity is 158.80291698087132
At time: 1421.0132603645325 and batch: 300, loss is 5.068848066329956 and perplexity is 158.9910746439649
At time: 1423.4792284965515 and batch: 350, loss is 5.011537046432495 and perplexity is 150.13532385556735
At time: 1425.9522399902344 and batch: 400, loss is 5.02513708114624 and perplexity is 152.1911172156004
At time: 1428.4489636421204 and batch: 450, loss is 4.9939351272583 and perplexity is 147.51577618669603
At time: 1430.9136137962341 and batch: 500, loss is 4.9941346549987795 and perplexity is 147.5452126127977
At time: 1433.3801352977753 and batch: 550, loss is 5.008827600479126 and perplexity is 149.72909089153572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.000899251302084 and perplexity of 148.54667985450328
Finished Training.
Improved accuracyfrom -10000000 to -148.54667985450328
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fae90a11898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 4.1902268708621895, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 22.029776370308223, 'dropout': 0.855069310926333, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9752042293548584 and batch: 50, loss is 7.296015520095825 and perplexity is 1474.4134373597997
At time: 5.4260032176971436 and batch: 100, loss is 6.665533361434936 and perplexity is 784.8819791400493
At time: 7.880547761917114 and batch: 150, loss is 6.637634429931641 and perplexity is 763.287246223955
At time: 10.336653709411621 and batch: 200, loss is 6.6418423748016355 and perplexity is 766.5058840519906
At time: 12.795049667358398 and batch: 250, loss is 6.586915817260742 and perplexity is 725.5397185386149
At time: 15.24660611152649 and batch: 300, loss is 6.605504846572876 and perplexity is 739.1529338590555
At time: 17.706164598464966 and batch: 350, loss is 6.58204701423645 and perplexity is 722.0157941708475
At time: 20.159583806991577 and batch: 400, loss is 6.622165031433106 and perplexity is 751.5705107211028
At time: 22.61928629875183 and batch: 450, loss is 6.639384880065918 and perplexity is 764.624512554156
At time: 25.071472883224487 and batch: 500, loss is 6.626691045761109 and perplexity is 754.9798391358074
At time: 27.52923274040222 and batch: 550, loss is 6.614438276290894 and perplexity is 745.7856871510788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.1645863850911455 and perplexity of 475.6043851827774
Finished 1 epochs...
Completing Train Step...
At time: 31.598063468933105 and batch: 50, loss is 6.563066263198852 and perplexity is 708.4406330696493
At time: 34.06112265586853 and batch: 100, loss is 6.677010402679444 and perplexity is 793.94199361621
At time: 36.500821352005005 and batch: 150, loss is 6.850607643127441 and perplexity is 944.4546237075946
At time: 38.96151113510132 and batch: 200, loss is 6.904376096725464 and perplexity is 996.626520754043
At time: 41.42523956298828 and batch: 250, loss is 6.913503141403198 and perplexity is 1005.7644130773705
At time: 43.88290476799011 and batch: 300, loss is 6.9675661277771 and perplexity is 1061.6357178771189
At time: 46.34177875518799 and batch: 350, loss is 6.962641124725342 and perplexity is 1056.420012950583
At time: 48.8001971244812 and batch: 400, loss is 7.045609455108643 and perplexity is 1147.8081602377897
At time: 51.26411151885986 and batch: 450, loss is 7.12466911315918 and perplexity is 1242.237059147597
At time: 53.72333264350891 and batch: 500, loss is 7.154620094299316 and perplexity is 1280.006063868
At time: 56.16624903678894 and batch: 550, loss is 7.034255952835083 and perplexity is 1134.8502158979745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.462747192382812 and perplexity of 640.8190939641136
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 60.15296387672424 and batch: 50, loss is 7.018911571502685 and perplexity is 1117.5695609691086
At time: 62.58866596221924 and batch: 100, loss is 6.9990590381622315 and perplexity is 1095.6017538086405
At time: 65.02898073196411 and batch: 150, loss is 6.947792892456055 and perplexity is 1040.8499234375613
At time: 67.46985626220703 and batch: 200, loss is 6.928513908386231 and perplexity is 1020.9755884080572
At time: 69.90800929069519 and batch: 250, loss is 6.793831415176392 and perplexity is 892.3258915936133
At time: 72.37481570243835 and batch: 300, loss is 6.746289539337158 and perplexity is 850.8956829541842
At time: 74.81557488441467 and batch: 350, loss is 6.657228193283081 and perplexity is 778.3903964801871
At time: 77.25359511375427 and batch: 400, loss is 6.609281883239746 and perplexity is 741.9500206170183
At time: 79.6931221485138 and batch: 450, loss is 6.598613109588623 and perplexity is 734.0763994107883
At time: 82.13154745101929 and batch: 500, loss is 6.60836893081665 and perplexity is 741.2729646548463
At time: 84.59118294715881 and batch: 550, loss is 6.636929311752319 and perplexity is 762.7492282160289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.229919942220052 and perplexity of 507.7148353461506
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 88.67471218109131 and batch: 50, loss is 6.717387056350708 and perplexity is 826.6546849309253
At time: 91.14247632026672 and batch: 100, loss is 6.710949382781982 and perplexity is 821.3500450056885
At time: 93.61065292358398 and batch: 150, loss is 6.677341632843017 and perplexity is 794.2050147104787
At time: 96.07553768157959 and batch: 200, loss is 6.657182455062866 and perplexity is 778.3547951029977
At time: 98.54414510726929 and batch: 250, loss is 6.541484870910645 and perplexity is 693.3152978070652
At time: 101.0082848072052 and batch: 300, loss is 6.496349363327027 and perplexity is 662.7178694380234
At time: 103.47415065765381 and batch: 350, loss is 6.413902235031128 and perplexity is 610.2704594368124
At time: 105.9371600151062 and batch: 400, loss is 6.367685031890869 and perplexity is 582.7073174421158
At time: 108.4038200378418 and batch: 450, loss is 6.342326574325561 and perplexity is 568.1165403373477
At time: 110.87153697013855 and batch: 500, loss is 6.337679119110107 and perplexity is 565.4823699945705
At time: 113.33629322052002 and batch: 550, loss is 6.382397556304932 and perplexity is 591.3437894429891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.977081298828125 and perplexity of 394.28787859385295
Finished 4 epochs...
Completing Train Step...
At time: 117.36652064323425 and batch: 50, loss is 6.487704906463623 and perplexity is 657.0137235455394
At time: 119.84783625602722 and batch: 100, loss is 6.492730579376221 and perplexity is 660.3239707617041
At time: 122.31452536582947 and batch: 150, loss is 6.466681060791015 and perplexity is 643.3449569020623
At time: 124.79741835594177 and batch: 200, loss is 6.4502268981933595 and perplexity is 632.8458681072984
At time: 127.26403498649597 and batch: 250, loss is 6.350299768447876 and perplexity is 572.6643499894614
At time: 129.72937417030334 and batch: 300, loss is 6.313459005355835 and perplexity is 551.9508515996062
At time: 132.1890640258789 and batch: 350, loss is 6.25866325378418 and perplexity is 522.5199964290028
At time: 134.64022183418274 and batch: 400, loss is 6.2390687084198 and perplexity is 512.3811124628504
At time: 137.1001172065735 and batch: 450, loss is 6.2312469577789305 and perplexity is 508.38902806530626
At time: 139.56177139282227 and batch: 500, loss is 6.233097248077392 and perplexity is 509.33056614254616
At time: 142.03198504447937 and batch: 550, loss is 6.275062236785889 and perplexity is 531.1594383855808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.878369140625 and perplexity of 357.22618072701493
Finished 5 epochs...
Completing Train Step...
At time: 146.12882256507874 and batch: 50, loss is 6.377751626968384 and perplexity is 588.6028200888359
At time: 148.60286688804626 and batch: 100, loss is 6.37790919303894 and perplexity is 588.6955712293402
At time: 151.07305526733398 and batch: 150, loss is 6.366984806060791 and perplexity is 582.299433549152
At time: 153.54345846176147 and batch: 200, loss is 6.366431303024292 and perplexity is 581.9772182263372
At time: 156.01878762245178 and batch: 250, loss is 6.27570650100708 and perplexity is 531.5017556670282
At time: 158.49011516571045 and batch: 300, loss is 6.250246028900147 and perplexity is 518.1402864728581
At time: 160.9609694480896 and batch: 350, loss is 6.207538003921509 and perplexity is 496.47741991008405
At time: 163.4347357749939 and batch: 400, loss is 6.2021802711486815 and perplexity is 493.824539624262
At time: 165.91579246520996 and batch: 450, loss is 6.196128482818604 and perplexity is 490.8450427738463
At time: 168.38717031478882 and batch: 500, loss is 6.199690284729004 and perplexity is 492.59645282113996
At time: 170.86470580101013 and batch: 550, loss is 6.234800891876221 and perplexity is 510.199023564108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.850836181640625 and perplexity of 347.524852919621
Finished 6 epochs...
Completing Train Step...
At time: 174.9611840248108 and batch: 50, loss is 6.329846906661987 and perplexity is 561.0706910926277
At time: 177.4520194530487 and batch: 100, loss is 6.327598886489868 and perplexity is 559.8108095112491
At time: 179.9277229309082 and batch: 150, loss is 6.321561231613159 and perplexity is 556.4410480122073
At time: 182.40041065216064 and batch: 200, loss is 6.322673902511597 and perplexity is 557.0605283479862
At time: 184.87148666381836 and batch: 250, loss is 6.239333906173706 and perplexity is 512.5170128024555
At time: 187.36239457130432 and batch: 300, loss is 6.219461784362793 and perplexity is 502.4327420773794
At time: 189.83071541786194 and batch: 350, loss is 6.177796964645386 and perplexity is 481.9290792047909
At time: 192.30063676834106 and batch: 400, loss is 6.178030300140381 and perplexity is 482.0415434854837
At time: 194.76961851119995 and batch: 450, loss is 6.173123369216919 and perplexity is 479.68199274005156
At time: 197.23505806922913 and batch: 500, loss is 6.173781709671021 and perplexity is 479.9978907737945
At time: 199.69987630844116 and batch: 550, loss is 6.202913846969604 and perplexity is 494.18693027056537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.830687967936198 and perplexity of 340.59291549409215
Finished 7 epochs...
Completing Train Step...
At time: 203.76609182357788 and batch: 50, loss is 6.292426710128784 and perplexity is 540.4632867301905
At time: 206.23398542404175 and batch: 100, loss is 6.289470500946045 and perplexity is 538.8679234746245
At time: 208.69820380210876 and batch: 150, loss is 6.288828916549683 and perplexity is 538.5223051067546
At time: 211.16005778312683 and batch: 200, loss is 6.291807622909546 and perplexity is 540.128796366946
At time: 213.61122012138367 and batch: 250, loss is 6.209963274002075 and perplexity is 497.68297304733056
At time: 216.07773327827454 and batch: 300, loss is 6.19213942527771 and perplexity is 488.8909337724252
At time: 218.55212306976318 and batch: 350, loss is 6.148621339797973 and perplexity is 468.07163001712087
At time: 221.01951003074646 and batch: 400, loss is 6.151749429702758 and perplexity is 469.53809257528445
At time: 223.49078798294067 and batch: 450, loss is 6.144241571426392 and perplexity is 466.0260675112138
At time: 225.95978665351868 and batch: 500, loss is 6.141464319229126 and perplexity is 464.733591188342
At time: 228.42475724220276 and batch: 550, loss is 6.162402410507202 and perplexity is 474.56681072406315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.802613321940104 and perplexity of 331.16386786193885
Finished 8 epochs...
Completing Train Step...
At time: 232.49171543121338 and batch: 50, loss is 6.2503832244873045 and perplexity is 518.2113779102949
At time: 234.97257947921753 and batch: 100, loss is 6.24382513999939 and perplexity is 514.8240233305263
At time: 237.42556762695312 and batch: 150, loss is 6.248436212539673 and perplexity is 517.2033957611748
At time: 239.88663387298584 and batch: 200, loss is 6.249865217208862 and perplexity is 517.943010158991
At time: 242.34290862083435 and batch: 250, loss is 6.174655027389527 and perplexity is 480.4172645332661
At time: 244.8075692653656 and batch: 300, loss is 6.162100677490234 and perplexity is 474.423639849287
At time: 247.2779679298401 and batch: 350, loss is 6.1233960723876955 and perplexity is 456.4120739477611
At time: 249.76575589179993 and batch: 400, loss is 6.134051513671875 and perplexity is 461.3013484317375
At time: 252.22864031791687 and batch: 450, loss is 6.127470932006836 and perplexity is 458.2756834724897
At time: 254.6852161884308 and batch: 500, loss is 6.122065668106079 and perplexity is 455.80526511044025
At time: 257.14816999435425 and batch: 550, loss is 6.142093000411987 and perplexity is 465.02585231207394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.789791870117187 and perplexity of 326.94497027582105
Finished 9 epochs...
Completing Train Step...
At time: 261.1639635562897 and batch: 50, loss is 6.226600179672241 and perplexity is 506.0321372752109
At time: 263.6287338733673 and batch: 100, loss is 6.217713413238525 and perplexity is 501.5550706505063
At time: 266.08849358558655 and batch: 150, loss is 6.224026699066162 and perplexity is 504.7315476227994
At time: 268.55448746681213 and batch: 200, loss is 6.224832153320312 and perplexity is 505.1382495629525
At time: 271.0204110145569 and batch: 250, loss is 6.153484086990357 and perplexity is 470.3532870865127
At time: 273.47928071022034 and batch: 300, loss is 6.142224273681641 and perplexity is 465.0869017831746
At time: 275.9426438808441 and batch: 350, loss is 6.1046531200408936 and perplexity is 447.9372340536525
At time: 278.40049147605896 and batch: 400, loss is 6.115745782852173 and perplexity is 452.9337116387448
At time: 280.8667938709259 and batch: 450, loss is 6.1064509010314945 and perplexity is 448.7432514022618
At time: 283.3336749076843 and batch: 500, loss is 6.100370101928711 and perplexity is 446.02281344270415
At time: 285.7998502254486 and batch: 550, loss is 6.113727254867554 and perplexity is 452.02037437515014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.766907755533854 and perplexity of 319.5480825260263
Finished 10 epochs...
Completing Train Step...
At time: 289.8374328613281 and batch: 50, loss is 6.192194318771362 and perplexity is 488.91777144039474
At time: 292.32701683044434 and batch: 100, loss is 6.183257026672363 and perplexity is 484.5676386638886
At time: 294.7919180393219 and batch: 150, loss is 6.193466730117798 and perplexity is 489.5402719145958
At time: 297.25501251220703 and batch: 200, loss is 6.198915624618531 and perplexity is 492.21500576356397
At time: 299.7089240550995 and batch: 250, loss is 6.130200424194336 and perplexity is 459.52825203064646
At time: 302.1730229854584 and batch: 300, loss is 6.121988916397095 and perplexity is 455.7702826198793
At time: 304.640438079834 and batch: 350, loss is 6.084913263320923 and perplexity is 439.18171785728117
At time: 307.1070601940155 and batch: 400, loss is 6.099069528579712 and perplexity is 445.44310511680874
At time: 309.572062253952 and batch: 450, loss is 6.089435501098633 and perplexity is 441.17229956256153
At time: 312.0684323310852 and batch: 500, loss is 6.083383903503418 and perplexity is 438.5105643338385
At time: 314.53308749198914 and batch: 550, loss is 6.091404418945313 and perplexity is 442.04178727067347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.755955505371094 and perplexity of 316.0674073922673
Finished 11 epochs...
Completing Train Step...
At time: 318.59368896484375 and batch: 50, loss is 6.169477748870849 and perplexity is 477.9364380557925
At time: 321.0606598854065 and batch: 100, loss is 6.1573351383209225 and perplexity is 472.1681340300134
At time: 323.524046421051 and batch: 150, loss is 6.171083707809448 and perplexity is 478.7046010045493
At time: 325.9831714630127 and batch: 200, loss is 6.17504056930542 and perplexity is 480.6025212356796
At time: 328.4476339817047 and batch: 250, loss is 6.110477056503296 and perplexity is 450.5536034332243
At time: 330.9124722480774 and batch: 300, loss is 6.102525043487549 and perplexity is 446.98500289825523
At time: 333.37426376342773 and batch: 350, loss is 6.0671884727478025 and perplexity is 431.4658965475423
At time: 335.8289132118225 and batch: 400, loss is 6.083396253585815 and perplexity is 438.5159800088818
At time: 338.28470945358276 and batch: 450, loss is 6.07368878364563 and perplexity is 434.2796943894452
At time: 340.7395815849304 and batch: 500, loss is 6.067258501052857 and perplexity is 431.49611243093744
At time: 343.20110416412354 and batch: 550, loss is 6.071145095825195 and perplexity is 433.17642619997014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.742290751139323 and perplexity of 311.77779892040735
Finished 12 epochs...
Completing Train Step...
At time: 347.2230587005615 and batch: 50, loss is 6.145940170288086 and perplexity is 466.81833153790615
At time: 349.70069122314453 and batch: 100, loss is 6.131327438354492 and perplexity is 460.0464388247556
At time: 352.16095447540283 and batch: 150, loss is 6.147444496154785 and perplexity is 467.5211068981327
At time: 354.6229157447815 and batch: 200, loss is 6.1555992603302006 and perplexity is 471.3492187323857
At time: 357.08667969703674 and batch: 250, loss is 6.095779867172241 and perplexity is 443.9801557480425
At time: 359.55166840553284 and batch: 300, loss is 6.088516750335693 and perplexity is 440.7671583161637
At time: 362.01749420166016 and batch: 350, loss is 6.05544282913208 and perplexity is 426.4276982575598
At time: 364.4826090335846 and batch: 400, loss is 6.072246809005737 and perplexity is 433.6539253634777
At time: 366.94997334480286 and batch: 450, loss is 6.062344036102295 and perplexity is 429.3807421242343
At time: 369.4181101322174 and batch: 500, loss is 6.056257171630859 and perplexity is 426.7750978868506
At time: 371.88682436943054 and batch: 550, loss is 6.056703624725341 and perplexity is 426.96567548876465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.733870442708334 and perplexity of 309.16355550564606
Finished 13 epochs...
Completing Train Step...
At time: 375.9353017807007 and batch: 50, loss is 6.128332834243775 and perplexity is 458.6708425790054
At time: 378.39551758766174 and batch: 100, loss is 6.113547315597534 and perplexity is 451.93904547630206
At time: 380.86346435546875 and batch: 150, loss is 6.130181341171265 and perplexity is 459.51948292608193
At time: 383.33674669265747 and batch: 200, loss is 6.139749402999878 and perplexity is 463.9372949963228
At time: 385.803245306015 and batch: 250, loss is 6.08368371963501 and perplexity is 438.64205658566334
At time: 388.27312207221985 and batch: 300, loss is 6.079150648117065 and perplexity is 436.65816074046904
At time: 390.7449288368225 and batch: 350, loss is 6.04517894744873 and perplexity is 422.0732796618819
At time: 393.2140488624573 and batch: 400, loss is 6.064794492721558 and perplexity is 430.43421121896444
At time: 395.68109488487244 and batch: 450, loss is 6.054430866241455 and perplexity is 425.9963875234252
At time: 398.1421627998352 and batch: 500, loss is 6.046311063766479 and perplexity is 422.55138629416786
At time: 400.60351848602295 and batch: 550, loss is 6.038443832397461 and perplexity is 419.2401191042079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.7141977945963545 and perplexity of 303.1409244549842
Finished 14 epochs...
Completing Train Step...
At time: 404.6734480857849 and batch: 50, loss is 6.099303693771362 and perplexity is 445.547424600407
At time: 407.1832609176636 and batch: 100, loss is 6.082462015151978 and perplexity is 438.1064928356169
At time: 409.6506905555725 and batch: 150, loss is 6.102923612594605 and perplexity is 447.16319281986796
At time: 412.125607252121 and batch: 200, loss is 6.114390020370483 and perplexity is 452.3200571846515
At time: 414.59348368644714 and batch: 250, loss is 6.060251998901367 and perplexity is 428.48340060154896
At time: 417.05998253822327 and batch: 300, loss is 6.055271100997925 and perplexity is 426.3544749120204
At time: 419.5343105792999 and batch: 350, loss is 6.025088062286377 and perplexity is 413.67806985570155
At time: 422.00310921669006 and batch: 400, loss is 6.046607599258423 and perplexity is 422.6767063573791
At time: 424.4786012172699 and batch: 450, loss is 6.035938282012939 and perplexity is 418.1910067133979
At time: 426.9512782096863 and batch: 500, loss is 6.02559642791748 and perplexity is 413.88842303238977
At time: 429.4188997745514 and batch: 550, loss is 6.018684873580932 and perplexity is 411.03767361034875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.706185404459635 and perplexity of 300.72174574592765
Finished 15 epochs...
Completing Train Step...
At time: 433.5062081813812 and batch: 50, loss is 6.084714784622192 and perplexity is 439.0945582913593
At time: 435.9699170589447 and batch: 100, loss is 6.068213930130005 and perplexity is 431.9085733706418
At time: 438.4587080478668 and batch: 150, loss is 6.090828857421875 and perplexity is 441.7874382299509
At time: 440.9256429672241 and batch: 200, loss is 6.103430347442627 and perplexity is 447.38984341344974
At time: 443.3941955566406 and batch: 250, loss is 6.050185804367065 and perplexity is 424.1918394192121
At time: 445.8627691268921 and batch: 300, loss is 6.0465108680725095 and perplexity is 422.6358223157278
At time: 448.33472633361816 and batch: 350, loss is 6.014161319732666 and perplexity is 409.182521663396
At time: 450.8000109195709 and batch: 400, loss is 6.037337837219238 and perplexity is 418.77669787202404
At time: 453.26382851600647 and batch: 450, loss is 6.027299394607544 and perplexity is 414.59386172903146
At time: 455.73158955574036 and batch: 500, loss is 6.015356836318969 and perplexity is 409.6719986855203
At time: 458.1962568759918 and batch: 550, loss is 6.007507419586181 and perplexity is 406.4689001179473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.698016357421875 and perplexity of 298.27514247407385
Finished 16 epochs...
Completing Train Step...
At time: 462.2648215293884 and batch: 50, loss is 6.06603889465332 and perplexity is 430.9701777926581
At time: 464.75938844680786 and batch: 100, loss is 6.049417953491211 and perplexity is 423.86624836247023
At time: 467.23036670684814 and batch: 150, loss is 6.075358619689942 and perplexity is 435.00547607601504
At time: 469.6985731124878 and batch: 200, loss is 6.088181591033935 and perplexity is 440.6194558564538
At time: 472.16840744018555 and batch: 250, loss is 6.035170297622681 and perplexity is 417.8699658410642
At time: 474.635249376297 and batch: 300, loss is 6.017065696716308 and perplexity is 410.37266944371936
At time: 477.1004681587219 and batch: 350, loss is 5.977935791015625 and perplexity is 394.6249384927536
At time: 479.5640802383423 and batch: 400, loss is 5.9984579753875735 and perplexity is 402.80717576190364
At time: 482.02542638778687 and batch: 450, loss is 5.988089952468872 and perplexity is 398.6524371623444
At time: 484.4956741333008 and batch: 500, loss is 5.972735147476197 and perplexity is 392.5779622663869
At time: 486.9501826763153 and batch: 550, loss is 5.958900308609008 and perplexity is 387.1841070077707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.655658976236979 and perplexity of 285.9048251489177
Finished 17 epochs...
Completing Train Step...
At time: 490.9983503818512 and batch: 50, loss is 6.019971303939819 and perplexity is 411.566785211972
At time: 493.46022725105286 and batch: 100, loss is 6.004587230682373 and perplexity is 405.28366554276454
At time: 495.9186224937439 and batch: 150, loss is 6.032531490325928 and perplexity is 416.7687411250303
At time: 498.37484192848206 and batch: 200, loss is 6.043969306945801 and perplexity is 421.56303139829936
At time: 500.8716425895691 and batch: 250, loss is 5.994056062698364 and perplexity is 401.0379505878159
At time: 503.3356590270996 and batch: 300, loss is 5.987912511825561 and perplexity is 398.5817062928879
At time: 505.7958824634552 and batch: 350, loss is 5.9583087825775145 and perplexity is 386.9551452545897
At time: 508.2515788078308 and batch: 400, loss is 5.982571811676025 and perplexity is 396.45867519701216
At time: 510.7188506126404 and batch: 450, loss is 5.970164766311646 and perplexity is 391.5701830098237
At time: 513.1757328510284 and batch: 500, loss is 5.955759887695312 and perplexity is 385.9700931955969
At time: 515.624890089035 and batch: 550, loss is 5.940438423156738 and perplexity is 380.10153832921253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.631741333007812 and perplexity of 279.1477839071368
Finished 18 epochs...
Completing Train Step...
At time: 519.6027193069458 and batch: 50, loss is 5.997699642181397 and perplexity is 402.5018294965763
At time: 522.0911040306091 and batch: 100, loss is 5.981576566696167 and perplexity is 396.0642979743296
At time: 524.5459096431732 and batch: 150, loss is 6.013861618041992 and perplexity is 409.05990734463654
At time: 527.0066659450531 and batch: 200, loss is 6.027903518676758 and perplexity is 414.84440353139684
At time: 529.4607899188995 and batch: 250, loss is 5.978592824935913 and perplexity is 394.88430566031786
At time: 531.9290781021118 and batch: 300, loss is 5.974892015457153 and perplexity is 393.42561491204145
At time: 534.4002220630646 and batch: 350, loss is 5.947282667160034 and perplexity is 382.7119690257132
At time: 536.8713643550873 and batch: 400, loss is 5.967936906814575 and perplexity is 390.6987906890837
At time: 539.3354887962341 and batch: 450, loss is 5.957008934020996 and perplexity is 386.45248892691825
At time: 541.8047423362732 and batch: 500, loss is 5.9424627494812015 and perplexity is 380.8717672134979
At time: 544.2667887210846 and batch: 550, loss is 5.926125574111938 and perplexity is 374.6999506566208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.634477233886718 and perplexity of 279.91255025990864
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 548.3587689399719 and batch: 50, loss is 5.98339882850647 and perplexity is 396.7866888116703
At time: 550.8311805725098 and batch: 100, loss is 5.961874933242798 and perplexity is 388.3375490683624
At time: 553.3012120723724 and batch: 150, loss is 5.992676210403443 and perplexity is 400.48495906240026
At time: 555.7700908184052 and batch: 200, loss is 5.999240407943725 and perplexity is 403.1224685416539
At time: 558.2464792728424 and batch: 250, loss is 5.94551836013794 and perplexity is 382.0373429101149
At time: 560.7208359241486 and batch: 300, loss is 5.941634912490844 and perplexity is 380.55659794840295
At time: 563.2097251415253 and batch: 350, loss is 5.901763153076172 and perplexity is 365.6816525309191
At time: 565.6781618595123 and batch: 400, loss is 5.9152572727203365 and perplexity is 370.649648482233
At time: 568.1464068889618 and batch: 450, loss is 5.905818071365356 and perplexity is 367.1674721545041
At time: 570.6139397621155 and batch: 500, loss is 5.899504356384277 and perplexity is 364.85658420550027
At time: 573.0797486305237 and batch: 550, loss is 5.896695318222046 and perplexity is 363.83312627589953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.594350179036458 and perplexity of 268.90285459345654
Finished 20 epochs...
Completing Train Step...
At time: 577.1180381774902 and batch: 50, loss is 5.96162317276001 and perplexity is 388.23979332555734
At time: 579.6285462379456 and batch: 100, loss is 5.944568252563476 and perplexity is 381.6745387156867
At time: 582.093029499054 and batch: 150, loss is 5.978646383285523 and perplexity is 394.90545557838806
At time: 584.5584955215454 and batch: 200, loss is 5.987828235626221 and perplexity is 398.54811675697414
At time: 587.0236587524414 and batch: 250, loss is 5.9373895454406735 and perplexity is 378.9444200715415
At time: 589.4899849891663 and batch: 300, loss is 5.936458683013916 and perplexity is 378.591839076727
At time: 591.9590575695038 and batch: 350, loss is 5.89914867401123 and perplexity is 364.7268342260665
At time: 594.4252774715424 and batch: 400, loss is 5.915500555038452 and perplexity is 370.7398319575022
At time: 596.8878209590912 and batch: 450, loss is 5.908167428970337 and perplexity is 368.0310939284745
At time: 599.3595871925354 and batch: 500, loss is 5.9000270652771 and perplexity is 365.0473478392499
At time: 601.8275966644287 and batch: 550, loss is 5.89320294380188 and perplexity is 362.56470096974533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.592038981119791 and perplexity of 268.2820845137192
Finished 21 epochs...
Completing Train Step...
At time: 605.8889183998108 and batch: 50, loss is 5.954242362976074 and perplexity is 385.38481823521397
At time: 608.3584566116333 and batch: 100, loss is 5.937433271408081 and perplexity is 378.96099014517154
At time: 610.8280866146088 and batch: 150, loss is 5.972828845977784 and perplexity is 392.61474795656244
At time: 613.2942547798157 and batch: 200, loss is 5.983360137939453 and perplexity is 396.7713372066785
At time: 615.764986038208 and batch: 250, loss is 5.934792079925537 and perplexity is 377.96140223840104
At time: 618.2354621887207 and batch: 300, loss is 5.934775266647339 and perplexity is 377.955047521619
At time: 620.7067115306854 and batch: 350, loss is 5.89864520072937 and perplexity is 364.5432502285531
At time: 623.174348115921 and batch: 400, loss is 5.915744218826294 and perplexity is 370.8301788359451
At time: 625.6626846790314 and batch: 450, loss is 5.909233808517456 and perplexity is 368.4237640902473
At time: 628.1200261116028 and batch: 500, loss is 5.899786138534546 and perplexity is 364.9594087647199
At time: 630.5864391326904 and batch: 550, loss is 5.890432987213135 and perplexity is 361.5618021218254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5897705078125 and perplexity of 267.67418353043917
Finished 22 epochs...
Completing Train Step...
At time: 634.5903282165527 and batch: 50, loss is 5.949891014099121 and perplexity is 383.7115176367531
At time: 637.0943365097046 and batch: 100, loss is 5.933810243606567 and perplexity is 377.5904881247854
At time: 639.5584955215454 and batch: 150, loss is 5.969708089828491 and perplexity is 391.3914029411722
At time: 642.016449213028 and batch: 200, loss is 5.980589418411255 and perplexity is 395.67351669313865
At time: 644.4782202243805 and batch: 250, loss is 5.93315354347229 and perplexity is 377.34260580161794
At time: 646.9304444789886 and batch: 300, loss is 5.933604745864868 and perplexity is 377.51290210433785
At time: 649.388932466507 and batch: 350, loss is 5.897825555801392 and perplexity is 364.2445766223489
At time: 651.8504672050476 and batch: 400, loss is 5.9155500125885006 and perplexity is 370.75816829472757
At time: 654.3062686920166 and batch: 450, loss is 5.909217691421508 and perplexity is 368.41782621694284
At time: 656.7693457603455 and batch: 500, loss is 5.898316173553467 and perplexity is 364.4233253227943
At time: 659.2281985282898 and batch: 550, loss is 5.888743858337403 and perplexity is 360.9515931473843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.589396667480469 and perplexity of 267.5741348270774
Finished 23 epochs...
Completing Train Step...
At time: 663.2312896251678 and batch: 50, loss is 5.947371349334717 and perplexity is 382.74591026037257
At time: 665.6772861480713 and batch: 100, loss is 5.9313336372375485 and perplexity is 376.6565021522085
At time: 668.1397712230682 and batch: 150, loss is 5.967278108596802 and perplexity is 390.44148378806176
At time: 670.5976316928864 and batch: 200, loss is 5.979088220596314 and perplexity is 395.079978095338
At time: 673.0592908859253 and batch: 250, loss is 5.932268733978272 and perplexity is 377.0088771464209
At time: 675.5282990932465 and batch: 300, loss is 5.932532062530518 and perplexity is 377.1081674206322
At time: 677.9818117618561 and batch: 350, loss is 5.897185373306274 and perplexity is 364.01146824436665
At time: 680.4463999271393 and batch: 400, loss is 5.914712295532227 and perplexity is 370.4477079105298
At time: 682.9100399017334 and batch: 450, loss is 5.908453702926636 and perplexity is 368.1364667277987
At time: 685.3650178909302 and batch: 500, loss is 5.897243337631226 and perplexity is 364.0325685349241
At time: 687.8513035774231 and batch: 550, loss is 5.887029781341552 and perplexity is 360.3334242708317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.589541117350261 and perplexity of 267.61278866771676
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 691.838693857193 and batch: 50, loss is 5.944461755752563 and perplexity is 381.63389375882434
At time: 694.3467769622803 and batch: 100, loss is 5.9264852333068845 and perplexity is 374.8347391767327
At time: 696.8059582710266 and batch: 150, loss is 5.96206205368042 and perplexity is 388.41022175955
At time: 699.266844034195 and batch: 200, loss is 5.9726721382141115 and perplexity is 392.55322699795727
At time: 701.7335691452026 and batch: 250, loss is 5.923291339874267 and perplexity is 373.63946676738504
At time: 704.2030453681946 and batch: 300, loss is 5.922180137634277 and perplexity is 373.2245083490247
At time: 706.6664502620697 and batch: 350, loss is 5.8858080577850345 and perplexity is 359.8934652470747
At time: 709.133207321167 and batch: 400, loss is 5.90100326538086 and perplexity is 365.4038810936741
At time: 711.600583076477 and batch: 450, loss is 5.893893375396728 and perplexity is 362.81511353089604
At time: 714.0653920173645 and batch: 500, loss is 5.886399803161621 and perplexity is 360.10649356424295
At time: 716.526661157608 and batch: 550, loss is 5.87916296005249 and perplexity is 357.50986639198317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.58016357421875 and perplexity of 265.11496821344696
Finished 25 epochs...
Completing Train Step...
At time: 720.5845425128937 and batch: 50, loss is 5.940383815765381 and perplexity is 380.08078254246834
At time: 723.049387216568 and batch: 100, loss is 5.9236120510101316 and perplexity is 373.759316322692
At time: 725.5140473842621 and batch: 150, loss is 5.959713907241821 and perplexity is 387.4992476494929
At time: 727.9761545658112 and batch: 200, loss is 5.970716352462769 and perplexity is 391.78622727800024
At time: 730.4440631866455 and batch: 250, loss is 5.921953411102295 and perplexity is 373.1398980426589
At time: 732.9028391838074 and batch: 300, loss is 5.921110162734985 and perplexity is 372.8253810594596
At time: 735.3643460273743 and batch: 350, loss is 5.885281791687012 and perplexity is 359.70411534600225
At time: 737.828268289566 and batch: 400, loss is 5.901174879074096 and perplexity is 365.46659478434174
At time: 740.2950387001038 and batch: 450, loss is 5.8948820686340335 and perplexity is 363.1740037669759
At time: 742.7627217769623 and batch: 500, loss is 5.886989879608154 and perplexity is 360.3190466294502
At time: 745.2258410453796 and batch: 550, loss is 5.878432588577271 and perplexity is 357.24884671572727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.580098470052083 and perplexity of 265.09770868621024
Finished 26 epochs...
Completing Train Step...
At time: 749.2352659702301 and batch: 50, loss is 5.938950977325439 and perplexity is 379.53657815854405
At time: 751.760657787323 and batch: 100, loss is 5.922377223968506 and perplexity is 373.2980730482798
At time: 754.2278735637665 and batch: 150, loss is 5.958761529922485 and perplexity is 387.13037783427586
At time: 756.69464635849 and batch: 200, loss is 5.969983148574829 and perplexity is 391.4990733769613
At time: 759.1662147045135 and batch: 250, loss is 5.9214186668396 and perplexity is 372.94041696343277
At time: 761.6379277706146 and batch: 300, loss is 5.920661373138428 and perplexity is 372.6580984472632
At time: 764.1087670326233 and batch: 350, loss is 5.885103197097778 and perplexity is 359.63987987350094
At time: 766.5815343856812 and batch: 400, loss is 5.901290102005005 and perplexity is 365.5087073426615
At time: 769.0491714477539 and batch: 450, loss is 5.895295972824097 and perplexity is 363.3243541220316
At time: 771.5295875072479 and batch: 500, loss is 5.88716456413269 and perplexity is 360.3819942886229
At time: 774.0002956390381 and batch: 550, loss is 5.877802171707153 and perplexity is 357.02370199090063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.57978261311849 and perplexity of 265.0139889592662
Finished 27 epochs...
Completing Train Step...
At time: 778.087744474411 and batch: 50, loss is 5.937907781600952 and perplexity is 379.1408536678584
At time: 780.5604906082153 and batch: 100, loss is 5.921492662429809 and perplexity is 372.9680139307134
At time: 783.0218541622162 and batch: 150, loss is 5.95811318397522 and perplexity is 386.8794647707669
At time: 785.4905786514282 and batch: 200, loss is 5.969532566070557 and perplexity is 391.32271048006055
At time: 787.9595155715942 and batch: 250, loss is 5.921089172363281 and perplexity is 372.8175553982625
At time: 790.4271442890167 and batch: 300, loss is 5.920344257354737 and perplexity is 372.539941418045
At time: 792.8982088565826 and batch: 350, loss is 5.884937429428101 and perplexity is 359.58026814967616
At time: 795.372243642807 and batch: 400, loss is 5.901288013458252 and perplexity is 365.507943961435
At time: 797.8418543338776 and batch: 450, loss is 5.895471277236939 and perplexity is 363.3880520677045
At time: 800.3097445964813 and batch: 500, loss is 5.887149772644043 and perplexity is 360.37666374186915
At time: 802.7803919315338 and batch: 550, loss is 5.877200431823731 and perplexity is 356.80893121463697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.579463195800781 and perplexity of 264.92935241966546
Finished 28 epochs...
Completing Train Step...
At time: 806.8684446811676 and batch: 50, loss is 5.937022943496704 and perplexity is 378.8055237718691
At time: 809.3614206314087 and batch: 100, loss is 5.920771970748901 and perplexity is 372.6993158217046
At time: 811.8300416469574 and batch: 150, loss is 5.957604732513428 and perplexity is 386.6828053414907
At time: 814.3571574687958 and batch: 200, loss is 5.969169244766236 and perplexity is 391.1805604271135
At time: 816.8219740390778 and batch: 250, loss is 5.920818405151367 and perplexity is 372.7166222935391
At time: 819.2740199565887 and batch: 300, loss is 5.920062627792358 and perplexity is 372.43503793003003
At time: 821.7348098754883 and batch: 350, loss is 5.884743747711181 and perplexity is 359.5106307699318
At time: 824.1915626525879 and batch: 400, loss is 5.901212310791015 and perplexity is 365.48027508249794
At time: 826.6528165340424 and batch: 450, loss is 5.895523805618286 and perplexity is 363.40714075522493
At time: 829.1157722473145 and batch: 500, loss is 5.887029180526733 and perplexity is 360.33320777723554
At time: 831.5880525112152 and batch: 550, loss is 5.876613245010376 and perplexity is 356.5994792150848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.579149373372396 and perplexity of 264.8462246912936
Finished 29 epochs...
Completing Train Step...
At time: 835.7089474201202 and batch: 50, loss is 5.936235094070435 and perplexity is 378.50719958999673
At time: 838.1868236064911 and batch: 100, loss is 5.920153932571411 and perplexity is 372.46904458134105
At time: 840.659592628479 and batch: 150, loss is 5.95718469619751 and perplexity is 386.52041862705255
At time: 843.125764131546 and batch: 200, loss is 5.968857898712158 and perplexity is 391.0587868610359
At time: 845.5986502170563 and batch: 250, loss is 5.9205867958068845 and perplexity is 372.6303076369982
At time: 848.0724232196808 and batch: 300, loss is 5.919811449050903 and perplexity is 372.34150191355025
At time: 850.5467474460602 and batch: 350, loss is 5.884555177688599 and perplexity is 359.44284423362404
At time: 853.01300573349 and batch: 400, loss is 5.901112279891968 and perplexity is 365.4437175904672
At time: 855.4818913936615 and batch: 450, loss is 5.895529670715332 and perplexity is 363.40927217962326
At time: 857.9422495365143 and batch: 500, loss is 5.886858367919922 and perplexity is 360.27166357910664
At time: 860.4014670848846 and batch: 550, loss is 5.876060705184937 and perplexity is 356.4024982260197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.578880310058594 and perplexity of 264.7749738743504
Finished 30 epochs...
Completing Train Step...
At time: 864.4594600200653 and batch: 50, loss is 5.935539293289184 and perplexity is 378.2439255885628
At time: 866.979819059372 and batch: 100, loss is 5.9196269321441655 and perplexity is 372.2728049494376
At time: 869.4476499557495 and batch: 150, loss is 5.956845684051514 and perplexity is 386.3894057192006
At time: 871.9177956581116 and batch: 200, loss is 5.968592786788941 and perplexity is 390.9551262553986
At time: 874.3861310482025 and batch: 250, loss is 5.9203886222839355 and perplexity is 372.55646949280043
At time: 876.8959183692932 and batch: 300, loss is 5.919573135375977 and perplexity is 372.2527784143331
At time: 879.3596434593201 and batch: 350, loss is 5.884372835159302 and perplexity is 359.37730849142685
At time: 881.8239841461182 and batch: 400, loss is 5.900994653701782 and perplexity is 365.40073436626585
At time: 884.285079240799 and batch: 450, loss is 5.895499572753907 and perplexity is 363.39833446596987
At time: 886.7529001235962 and batch: 500, loss is 5.886652870178223 and perplexity is 360.19763617233576
At time: 889.216335773468 and batch: 550, loss is 5.8755338001251225 and perplexity is 356.21475741151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.578644816080729 and perplexity of 264.71262830380505
Finished 31 epochs...
Completing Train Step...
At time: 893.2846291065216 and batch: 50, loss is 5.934908246994018 and perplexity is 378.0053114568513
At time: 895.7404544353485 and batch: 100, loss is 5.919156255722046 and perplexity is 372.09762614705176
At time: 898.2033276557922 and batch: 150, loss is 5.956553344726562 and perplexity is 386.2764654104155
At time: 900.6607437133789 and batch: 200, loss is 5.9683386421203615 and perplexity is 390.85577971913807
At time: 903.1220595836639 and batch: 250, loss is 5.9201974678039555 and perplexity is 372.485260460791
At time: 905.5833523273468 and batch: 300, loss is 5.919331607818603 and perplexity is 372.1628799669503
At time: 908.0539314746857 and batch: 350, loss is 5.884186162948608 and perplexity is 359.3102289959113
At time: 910.5067272186279 and batch: 400, loss is 5.9008615112304685 and perplexity is 365.35208724804306
At time: 912.9622724056244 and batch: 450, loss is 5.895443258285522 and perplexity is 363.3778704581679
At time: 915.425473690033 and batch: 500, loss is 5.886423606872558 and perplexity is 360.1150655371444
At time: 917.8862552642822 and batch: 550, loss is 5.8750248050689695 and perplexity is 356.03349199657225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.578433227539063 and perplexity of 264.6566240699576
Finished 32 epochs...
Completing Train Step...
At time: 921.898759841919 and batch: 50, loss is 5.934323358535766 and perplexity is 377.7842851571891
At time: 924.3869853019714 and batch: 100, loss is 5.918723754882812 and perplexity is 371.93672840817777
At time: 926.8411569595337 and batch: 150, loss is 5.956289052963257 and perplexity is 386.1743892117922
At time: 929.3048114776611 and batch: 200, loss is 5.968085823059082 and perplexity is 390.7569764180189
At time: 931.7765793800354 and batch: 250, loss is 5.920003423690796 and perplexity is 372.4129889009219
At time: 934.254091501236 and batch: 300, loss is 5.919086513519287 and perplexity is 372.07167614387834
At time: 936.7215003967285 and batch: 350, loss is 5.883998765945434 and perplexity is 359.24290164445677
At time: 939.2363610267639 and batch: 400, loss is 5.900719184875488 and perplexity is 365.3000917174354
At time: 941.7003474235535 and batch: 450, loss is 5.895370292663574 and perplexity is 363.35135733313285
At time: 944.1717464923859 and batch: 500, loss is 5.886178359985352 and perplexity is 360.0267592671469
At time: 946.638486623764 and batch: 550, loss is 5.874531345367432 and perplexity is 355.8578471562636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.57823740641276 and perplexity of 264.60480378566757
Finished 33 epochs...
Completing Train Step...
At time: 950.7069981098175 and batch: 50, loss is 5.933775939941406 and perplexity is 377.57753560927347
At time: 953.169260263443 and batch: 100, loss is 5.918319463729858 and perplexity is 371.78638807210547
At time: 955.6308724880219 and batch: 150, loss is 5.956044435501099 and perplexity is 386.0799357657048
At time: 958.0889885425568 and batch: 200, loss is 5.967830905914306 and perplexity is 390.65737846044226
At time: 960.545022726059 and batch: 250, loss is 5.91980284690857 and perplexity is 372.33829899273013
At time: 962.996239900589 and batch: 300, loss is 5.918842344284058 and perplexity is 371.98083877756045
At time: 965.4665579795837 and batch: 350, loss is 5.88381404876709 and perplexity is 359.17654943770964
At time: 967.922933101654 and batch: 400, loss is 5.900575704574585 and perplexity is 365.24768211031903
At time: 970.3839602470398 and batch: 450, loss is 5.895286378860473 and perplexity is 363.3208684181155
At time: 972.8525664806366 and batch: 500, loss is 5.8859228515625 and perplexity is 359.93478114879457
At time: 975.3088533878326 and batch: 550, loss is 5.874052963256836 and perplexity is 355.6876518407118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.578051249186198 and perplexity of 264.5555502738507
Finished 34 epochs...
Completing Train Step...
At time: 979.3218677043915 and batch: 50, loss is 5.933260650634765 and perplexity is 377.3830240619103
At time: 981.8084070682526 and batch: 100, loss is 5.917935209274292 and perplexity is 371.643554939861
At time: 984.2710485458374 and batch: 150, loss is 5.955815258026123 and perplexity is 385.99146507901895
At time: 986.7335004806519 and batch: 200, loss is 5.967573070526123 and perplexity is 390.5566661477774
At time: 989.1883239746094 and batch: 250, loss is 5.91959506034851 and perplexity is 372.2609401357478
At time: 991.6483776569366 and batch: 300, loss is 5.918603572845459 and perplexity is 371.89203098035904
At time: 994.1108615398407 and batch: 350, loss is 5.883635520935059 and perplexity is 359.1124321505524
At time: 996.5632667541504 and batch: 400, loss is 5.900434494018555 and perplexity is 365.1961089234643
At time: 999.0269978046417 and batch: 450, loss is 5.895194683074951 and perplexity is 363.2875549530649
At time: 1001.5342650413513 and batch: 500, loss is 5.885662231445313 and perplexity is 359.840987126787
At time: 1003.9913971424103 and batch: 550, loss is 5.87358850479126 and perplexity is 355.52248805854117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.577874755859375 and perplexity of 264.5088621048496
Finished 35 epochs...
Completing Train Step...
At time: 1008.0296213626862 and batch: 50, loss is 5.932770290374756 and perplexity is 377.19801578816794
At time: 1010.4860799312592 and batch: 100, loss is 5.917562246322632 and perplexity is 371.5049715074946
At time: 1012.9536402225494 and batch: 150, loss is 5.955594396591186 and perplexity is 385.90622386376316
At time: 1015.4240562915802 and batch: 200, loss is 5.967309055328369 and perplexity is 390.4535668628179
At time: 1017.8814070224762 and batch: 250, loss is 5.919380836486816 and perplexity is 372.18120150085804
At time: 1020.3465209007263 and batch: 300, loss is 5.9183740234375 and perplexity is 371.80667318211357
At time: 1022.8171122074127 and batch: 350, loss is 5.8834627246856686 and perplexity is 359.0503842301458
At time: 1025.279014825821 and batch: 400, loss is 5.900293378829956 and perplexity is 365.14457784167286
At time: 1027.7416560649872 and batch: 450, loss is 5.895096035003662 and perplexity is 363.25171910404305
At time: 1030.2142128944397 and batch: 500, loss is 5.885399961471558 and perplexity is 359.7466240153837
At time: 1032.6732635498047 and batch: 550, loss is 5.8731366062164305 and perplexity is 355.36186424843646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.57770741780599 and perplexity of 264.4646034099472
Finished 36 epochs...
Completing Train Step...
At time: 1036.6606948375702 and batch: 50, loss is 5.932297449111939 and perplexity is 377.0197031621564
At time: 1039.151938199997 and batch: 100, loss is 5.9171945762634275 and perplexity is 371.36840535980593
At time: 1041.6206324100494 and batch: 150, loss is 5.955376644134521 and perplexity is 385.8222009839007
At time: 1044.0762271881104 and batch: 200, loss is 5.9670387840271 and perplexity is 390.3480527285802
At time: 1046.5399923324585 and batch: 250, loss is 5.9191648960113525 and perplexity is 372.10084119208125
At time: 1049.0009446144104 and batch: 300, loss is 5.918156957626342 and perplexity is 371.7259754236846
At time: 1051.4534776210785 and batch: 350, loss is 5.883296709060669 and perplexity is 358.9907812038554
At time: 1053.9166679382324 and batch: 400, loss is 5.900152807235718 and perplexity is 365.09325249376553
At time: 1056.387378692627 and batch: 450, loss is 5.894992132186889 and perplexity is 363.21397818795833
At time: 1058.856861114502 and batch: 500, loss is 5.885138778686524 and perplexity is 359.6526766594668
At time: 1061.3221724033356 and batch: 550, loss is 5.872695388793946 and perplexity is 355.2051069872038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.577546183268229 and perplexity of 264.4219660192646
Finished 37 epochs...
Completing Train Step...
At time: 1065.6677312850952 and batch: 50, loss is 5.931837501525879 and perplexity is 376.84633373327165
At time: 1068.1397695541382 and batch: 100, loss is 5.916826887130737 and perplexity is 371.2318823334853
At time: 1070.6030814647675 and batch: 150, loss is 5.955156307220459 and perplexity is 385.73719947558834
At time: 1073.070883512497 and batch: 200, loss is 5.966766195297241 and perplexity is 390.2416627496976
At time: 1075.5290780067444 and batch: 250, loss is 5.9189535331726075 and perplexity is 372.0222012130649
At time: 1077.9905128479004 and batch: 300, loss is 5.917955198287964 and perplexity is 371.65098380220934
At time: 1080.457367658615 and batch: 350, loss is 5.8831398391723635 and perplexity is 358.93447077692576
At time: 1082.9233932495117 and batch: 400, loss is 5.900015001296997 and perplexity is 365.04294394187326
At time: 1085.3946313858032 and batch: 450, loss is 5.894884309768677 and perplexity is 363.17481768972925
At time: 1087.8529591560364 and batch: 500, loss is 5.884880657196045 and perplexity is 359.5598545547182
At time: 1090.310188293457 and batch: 550, loss is 5.872260799407959 and perplexity is 355.0507721564277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.577392578125 and perplexity of 264.3813525645989
Finished 38 epochs...
Completing Train Step...
At time: 1094.4530589580536 and batch: 50, loss is 5.931384267807007 and perplexity is 376.6755729681819
At time: 1096.937715768814 and batch: 100, loss is 5.916454668045044 and perplexity is 371.0937284550132
At time: 1099.3986976146698 and batch: 150, loss is 5.954928140640259 and perplexity is 385.64919717790144
At time: 1101.8585228919983 and batch: 200, loss is 5.96649697303772 and perplexity is 390.1366151487035
At time: 1104.3141605854034 and batch: 250, loss is 5.918751983642578 and perplexity is 371.9472278689249
At time: 1106.7703771591187 and batch: 300, loss is 5.917772750854493 and perplexity is 371.5831832192768
At time: 1109.2329750061035 and batch: 350, loss is 5.882995128631592 and perplexity is 358.8825329336254
At time: 1111.6988101005554 and batch: 400, loss is 5.89987735748291 and perplexity is 364.9927014966238
At time: 1114.1556525230408 and batch: 450, loss is 5.894773397445679 and perplexity is 363.1345393607678
At time: 1116.6134283542633 and batch: 500, loss is 5.884626617431641 and perplexity is 359.4685236553092
At time: 1119.0625596046448 and batch: 550, loss is 5.87182861328125 and perplexity is 354.8973572927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.577241007486979 and perplexity of 264.3412831510598
Finished 39 epochs...
Completing Train Step...
At time: 1123.0875778198242 and batch: 50, loss is 5.930928554534912 and perplexity is 376.5039560173373
At time: 1125.551826953888 and batch: 100, loss is 5.916070356369018 and perplexity is 370.951140203188
At time: 1128.0370287895203 and batch: 150, loss is 5.9546858596801755 and perplexity is 385.5557730380558
At time: 1130.4979419708252 and batch: 200, loss is 5.966234951019287 and perplexity is 390.0344041566878
At time: 1132.9645659923553 and batch: 250, loss is 5.918563823699952 and perplexity is 371.87724888369627
At time: 1135.4183869361877 and batch: 300, loss is 5.917611675262451 and perplexity is 371.52333505821554
At time: 1137.8699741363525 and batch: 350, loss is 5.882864046096802 and perplexity is 358.83549278465483
At time: 1140.3310782909393 and batch: 400, loss is 5.899738712310791 and perplexity is 364.94210052857363
At time: 1142.792219877243 and batch: 450, loss is 5.894658460617065 and perplexity is 363.09280422695144
At time: 1145.2491228580475 and batch: 500, loss is 5.88437741279602 and perplexity is 359.3789535939554
At time: 1147.708811044693 and batch: 550, loss is 5.871395635604858 and perplexity is 354.7437279210262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.577089945475261 and perplexity of 264.30135424099353
Finished 40 epochs...
Completing Train Step...
At time: 1151.7152938842773 and batch: 50, loss is 5.930462160110474 and perplexity is 376.32839761438436
At time: 1154.2040209770203 and batch: 100, loss is 5.915666637420654 and perplexity is 370.8014104253785
At time: 1156.6719195842743 and batch: 150, loss is 5.954424152374267 and perplexity is 385.45488347776006
At time: 1159.1342988014221 and batch: 200, loss is 5.96598337173462 and perplexity is 389.9362919223146
At time: 1161.5949873924255 and batch: 250, loss is 5.918390960693359 and perplexity is 371.8129706201979
At time: 1164.0562584400177 and batch: 300, loss is 5.917473258972168 and perplexity is 371.4719137352797
At time: 1166.5219564437866 and batch: 350, loss is 5.882745580673218 and perplexity is 358.7929857038646
At time: 1168.9850511550903 and batch: 400, loss is 5.899599256515503 and perplexity is 364.8912107862277
At time: 1171.451898097992 and batch: 450, loss is 5.894538288116455 and perplexity is 363.04917307839855
At time: 1173.9198276996613 and batch: 500, loss is 5.884132471084595 and perplexity is 359.2909374778608
At time: 1176.3872044086456 and batch: 550, loss is 5.8709595584869385 and perplexity is 354.58906602326255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5769399007161455 and perplexity of 264.2617001829788
Finished 41 epochs...
Completing Train Step...
At time: 1180.4568910598755 and batch: 50, loss is 5.929977722167969 and perplexity is 376.14613401100496
At time: 1182.9189693927765 and batch: 100, loss is 5.91523515701294 and perplexity is 370.64145139370294
At time: 1185.3753814697266 and batch: 150, loss is 5.954140100479126 and perplexity is 385.3454098364505
At time: 1187.829400062561 and batch: 200, loss is 5.965741930007934 and perplexity is 389.8421563952749
At time: 1190.3083338737488 and batch: 250, loss is 5.91823166847229 and perplexity is 371.75374842322856
At time: 1192.775460243225 and batch: 300, loss is 5.9173555660247805 and perplexity is 371.4281966835252
At time: 1195.2387826442719 and batch: 350, loss is 5.88263560295105 and perplexity is 358.7535286383058
At time: 1197.6952252388 and batch: 400, loss is 5.899457511901855 and perplexity is 364.8394930879703
At time: 1200.149938583374 and batch: 450, loss is 5.894408884048462 and perplexity is 363.0021960780939
At time: 1202.6042921543121 and batch: 500, loss is 5.883887615203857 and perplexity is 359.2029737485814
At time: 1205.0585808753967 and batch: 550, loss is 5.870516939163208 and perplexity is 354.4321527796357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5767822265625 and perplexity of 264.2200362278123
Finished 42 epochs...
Completing Train Step...
At time: 1209.0324170589447 and batch: 50, loss is 5.929462900161743 and perplexity is 375.9525355423114
At time: 1211.5105466842651 and batch: 100, loss is 5.914764823913575 and perplexity is 370.46716744008467
At time: 1213.960820198059 and batch: 150, loss is 5.953824224472046 and perplexity is 385.2237076894508
At time: 1216.413052558899 and batch: 200, loss is 5.965498199462891 and perplexity is 389.74715153227964
At time: 1218.872216463089 and batch: 250, loss is 5.918070726394653 and perplexity is 371.69392241697784
At time: 1221.3259093761444 and batch: 300, loss is 5.917246570587158 and perplexity is 371.3877149108868
At time: 1223.7859153747559 and batch: 350, loss is 5.882520475387573 and perplexity is 358.7122285960974
At time: 1226.2454833984375 and batch: 400, loss is 5.8993062400817875 and perplexity is 364.7843073279488
At time: 1228.699048280716 and batch: 450, loss is 5.894255933761596 and perplexity is 362.9466790338529
At time: 1231.160658121109 and batch: 500, loss is 5.88361780166626 and perplexity is 359.1060689972135
At time: 1233.6179690361023 and batch: 550, loss is 5.870050067901611 and perplexity is 354.26671721487753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.576595052083333 and perplexity of 264.1705856082373
Finished 43 epochs...
Completing Train Step...
At time: 1237.6696257591248 and batch: 50, loss is 5.9288684844970705 and perplexity is 375.729129870505
At time: 1240.1267046928406 and batch: 100, loss is 5.914207925796509 and perplexity is 370.2609124089525
At time: 1242.586219072342 and batch: 150, loss is 5.953423433303833 and perplexity is 385.06934436541724
At time: 1245.0430617332458 and batch: 200, loss is 5.965181188583374 and perplexity is 389.6236170269089
At time: 1247.49560880661 and batch: 250, loss is 5.917813158035278 and perplexity is 371.59819815149143
At time: 1249.9592881202698 and batch: 300, loss is 5.917083625793457 and perplexity is 371.3272041463883
At time: 1252.4505269527435 and batch: 350, loss is 5.882371597290039 and perplexity is 358.6588281771167
At time: 1254.9066603183746 and batch: 400, loss is 5.899127960205078 and perplexity is 364.7192794233685
At time: 1257.3597185611725 and batch: 450, loss is 5.894034357070923 and perplexity is 362.8662674188203
At time: 1259.8118317127228 and batch: 500, loss is 5.883228569030762 and perplexity is 358.96632039467727
At time: 1262.2646696567535 and batch: 550, loss is 5.869510765075684 and perplexity is 354.07571168269254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.576329040527344 and perplexity of 264.1003225255232
Finished 44 epochs...
Completing Train Step...
At time: 1266.2421584129333 and batch: 50, loss is 5.928100652694702 and perplexity is 375.4407438256712
At time: 1268.723388671875 and batch: 100, loss is 5.913531217575073 and perplexity is 370.0104385638626
At time: 1271.1848940849304 and batch: 150, loss is 5.952955675125122 and perplexity is 384.8892671497982
At time: 1273.6367764472961 and batch: 200, loss is 5.9648597717285154 and perplexity is 389.49840555296254
At time: 1276.0960927009583 and batch: 250, loss is 5.917479133605957 and perplexity is 371.4740960031456
At time: 1278.5530905723572 and batch: 300, loss is 5.916870517730713 and perplexity is 371.24807975659064
At time: 1281.0081930160522 and batch: 350, loss is 5.882214317321777 and perplexity is 358.6024227638413
At time: 1283.4776074886322 and batch: 400, loss is 5.8989730548858645 and perplexity is 364.66278684257964
At time: 1285.9334297180176 and batch: 450, loss is 5.893855724334717 and perplexity is 362.80145341371843
At time: 1288.3890912532806 and batch: 500, loss is 5.882897472381591 and perplexity is 358.847487522498
At time: 1290.84987449646 and batch: 550, loss is 5.869063262939453 and perplexity is 353.91729749331483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.576127115885416 and perplexity of 264.0469995462573
Finished 45 epochs...
Completing Train Step...
At time: 1294.8770875930786 and batch: 50, loss is 5.92755973815918 and perplexity is 375.23771738503507
At time: 1297.334403514862 and batch: 100, loss is 5.913013010025025 and perplexity is 369.8187460335499
At time: 1299.7894668579102 and batch: 150, loss is 5.952610120773316 and perplexity is 384.75628996531657
At time: 1302.2510750293732 and batch: 200, loss is 5.964586973190308 and perplexity is 389.3921654490248
At time: 1304.7152609825134 and batch: 250, loss is 5.9172283554077145 and perplexity is 371.38095007862796
At time: 1307.1788094043732 and batch: 300, loss is 5.91672080039978 and perplexity is 371.1925016455826
At time: 1309.6381986141205 and batch: 350, loss is 5.882092876434326 and perplexity is 358.5588764115861
At time: 1312.1004197597504 and batch: 400, loss is 5.898848133087158 and perplexity is 364.61723535657205
At time: 1314.5930697917938 and batch: 450, loss is 5.893715267181396 and perplexity is 362.7504989328958
At time: 1317.0456805229187 and batch: 500, loss is 5.882620649337769 and perplexity is 358.7481640168857
At time: 1319.5074307918549 and batch: 550, loss is 5.868663864135742 and perplexity is 353.775971572664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.575970458984375 and perplexity of 264.00563800144937
Finished 46 epochs...
Completing Train Step...
At time: 1323.4882578849792 and batch: 50, loss is 5.92709659576416 and perplexity is 375.06396912810146
At time: 1325.9734654426575 and batch: 100, loss is 5.912579679489136 and perplexity is 369.6585269945577
At time: 1328.4328236579895 and batch: 150, loss is 5.952327728271484 and perplexity is 384.6476530138501
At time: 1330.8855502605438 and batch: 200, loss is 5.964345645904541 and perplexity is 389.29820583260386
At time: 1333.3459131717682 and batch: 250, loss is 5.917005863189697 and perplexity is 371.29832989882993
At time: 1335.8078725337982 and batch: 300, loss is 5.916575946807861 and perplexity is 371.13873697252296
At time: 1338.2594890594482 and batch: 350, loss is 5.881977767944336 and perplexity is 358.5176056161048
At time: 1340.710967540741 and batch: 400, loss is 5.8987235164642335 and perplexity is 364.5718008190499
At time: 1343.168741941452 and batch: 450, loss is 5.893571519851685 and perplexity is 362.6983582649534
At time: 1345.6349034309387 and batch: 500, loss is 5.882357950210571 and perplexity is 358.6539335649855
At time: 1348.0973653793335 and batch: 550, loss is 5.868293647766113 and perplexity is 353.6450221581104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.57582753499349 and perplexity of 263.9679079583785
Finished 47 epochs...
Completing Train Step...
At time: 1352.113065481186 and batch: 50, loss is 5.926679515838623 and perplexity is 374.907570093502
At time: 1354.580374956131 and batch: 100, loss is 5.912186241149902 and perplexity is 369.51311776427195
At time: 1357.038144826889 and batch: 150, loss is 5.952074556350708 and perplexity is 384.55028335486645
At time: 1359.4926381111145 and batch: 200, loss is 5.964118909835816 and perplexity is 389.2099478938586
At time: 1361.9475507736206 and batch: 250, loss is 5.916796436309815 and perplexity is 371.22057819002674
At time: 1364.4034757614136 and batch: 300, loss is 5.91642596244812 and perplexity is 371.0830761409143
At time: 1366.8632595539093 and batch: 350, loss is 5.881857852935791 and perplexity is 358.47461655193257
At time: 1369.316602230072 and batch: 400, loss is 5.898594789505005 and perplexity is 364.52487362017246
At time: 1371.7849562168121 and batch: 450, loss is 5.893423938751221 and perplexity is 362.64483479172793
At time: 1374.2419764995575 and batch: 500, loss is 5.882104454040527 and perplexity is 358.5630276890879
At time: 1376.7534465789795 and batch: 550, loss is 5.867946567535401 and perplexity is 353.5223002606337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5756891886393225 and perplexity of 263.9313914867139
Finished 48 epochs...
Completing Train Step...
At time: 1380.7954804897308 and batch: 50, loss is 5.926297264099121 and perplexity is 374.76428840926235
At time: 1383.2830460071564 and batch: 100, loss is 5.911818294525147 and perplexity is 369.377181669935
At time: 1385.7395887374878 and batch: 150, loss is 5.951836996078491 and perplexity is 384.4589403350373
At time: 1388.193948507309 and batch: 200, loss is 5.963900556564331 and perplexity is 389.1249719061718
At time: 1390.6676754951477 and batch: 250, loss is 5.9165936470031735 and perplexity is 371.14530625879394
At time: 1393.1406061649323 and batch: 300, loss is 5.916273002624512 and perplexity is 371.0263196798836
At time: 1395.6106624603271 and batch: 350, loss is 5.881732301712036 and perplexity is 358.4296124503584
At time: 1398.0633022785187 and batch: 400, loss is 5.898463582992553 and perplexity is 364.4770487203408
At time: 1400.5315680503845 and batch: 450, loss is 5.8932759475708005 and perplexity is 362.59117052557065
At time: 1402.99130320549 and batch: 500, loss is 5.881859645843506 and perplexity is 358.47525926441426
At time: 1405.444664478302 and batch: 550, loss is 5.867618417739868 and perplexity is 353.40631102205015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.575553894042969 and perplexity of 263.8956854111116
Finished 49 epochs...
Completing Train Step...
At time: 1409.5130140781403 and batch: 50, loss is 5.925941257476807 and perplexity is 374.6308935869107
At time: 1411.9676458835602 and batch: 100, loss is 5.911468572616577 and perplexity is 369.2480249627609
At time: 1414.4354529380798 and batch: 150, loss is 5.951608057022095 and perplexity is 384.370932742586
At time: 1416.890512228012 and batch: 200, loss is 5.963688354492188 and perplexity is 389.04240754128506
At time: 1419.349497795105 and batch: 250, loss is 5.916395568847657 and perplexity is 371.07179776155556
At time: 1421.8106532096863 and batch: 300, loss is 5.91611876487732 and perplexity is 370.9690978291854
At time: 1424.2646107673645 and batch: 350, loss is 5.881602449417114 and perplexity is 358.3830725643346
At time: 1426.7255589962006 and batch: 400, loss is 5.898331251144409 and perplexity is 364.4288199900463
At time: 1429.1897847652435 and batch: 450, loss is 5.893129873275757 and perplexity is 362.5382091441894
At time: 1431.6634001731873 and batch: 500, loss is 5.881622791290283 and perplexity is 358.3903628214911
At time: 1434.1180543899536 and batch: 550, loss is 5.867305536270141 and perplexity is 353.29575403256547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.57541758219401 and perplexity of 263.859715753902
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fae90a11898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 5.575509407299329, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 15.470220075807763, 'dropout': 0.7870846207383437, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9832139015197754 and batch: 50, loss is 7.287358522415161 and perplexity is 1461.7045334986888
At time: 5.41716194152832 and batch: 100, loss is 6.547458639144898 and perplexity is 697.4693981722661
At time: 7.889734983444214 and batch: 150, loss is 6.534618139266968 and perplexity is 688.570795965748
At time: 10.331490993499756 and batch: 200, loss is 6.532016000747681 and perplexity is 686.7813685530112
At time: 12.76987600326538 and batch: 250, loss is 6.502267379760742 and perplexity is 666.651472764323
At time: 15.21443796157837 and batch: 300, loss is 6.524638891220093 and perplexity is 681.7335492262872
At time: 17.649503707885742 and batch: 350, loss is 6.5292846298217775 and perplexity is 684.9080733870949
At time: 20.090068817138672 and batch: 400, loss is 6.566746187210083 and perplexity is 711.0524434502876
At time: 22.533533096313477 and batch: 450, loss is 6.586559648513794 and perplexity is 725.2813499803361
At time: 24.970832109451294 and batch: 500, loss is 6.594753198623657 and perplexity is 731.2483913085077
At time: 27.414302587509155 and batch: 550, loss is 6.579277019500733 and perplexity is 720.0185816330265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.227384440104166 and perplexity of 506.42915391963174
Finished 1 epochs...
Completing Train Step...
At time: 31.411037921905518 and batch: 50, loss is 6.586988620758056 and perplexity is 725.5925422904186
At time: 33.83425164222717 and batch: 100, loss is 6.6380438709259035 and perplexity is 763.5998313011625
At time: 36.28164744377136 and batch: 150, loss is 6.743000679016113 and perplexity is 848.1018027643738
At time: 38.75971484184265 and batch: 200, loss is 6.782613697052002 and perplexity is 882.3719658143929
At time: 41.21414518356323 and batch: 250, loss is 6.726064920425415 and perplexity is 833.8594979039661
At time: 43.66369962692261 and batch: 300, loss is 6.758827772140503 and perplexity is 861.6315750291864
At time: 46.117356300354004 and batch: 350, loss is 6.771716060638428 and perplexity is 872.808401717596
At time: 48.57785964012146 and batch: 400, loss is 6.804756174087524 and perplexity is 902.1277809805918
At time: 51.03939461708069 and batch: 450, loss is 6.79116660118103 and perplexity is 889.9511745641431
At time: 53.49758315086365 and batch: 500, loss is 6.802294082641602 and perplexity is 899.9093919468788
At time: 55.96495008468628 and batch: 550, loss is 6.752529001235962 and perplexity is 856.22141171073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.3749445597330725 and perplexity of 586.9528892148043
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.97639727592468 and batch: 50, loss is 6.7397477912902835 and perplexity is 845.3475049593349
At time: 62.42252159118652 and batch: 100, loss is 6.6684709072113035 and perplexity is 787.1909956432486
At time: 64.85292768478394 and batch: 150, loss is 6.640943851470947 and perplexity is 765.8174699564386
At time: 67.28818798065186 and batch: 200, loss is 6.593314952850342 and perplexity is 730.1974323502683
At time: 69.72271704673767 and batch: 250, loss is 6.486479444503784 and perplexity is 656.2090713562659
At time: 72.14774179458618 and batch: 300, loss is 6.45683198928833 and perplexity is 637.0397078147529
At time: 74.58577632904053 and batch: 350, loss is 6.366757736206055 and perplexity is 582.1672259121157
At time: 77.0294771194458 and batch: 400, loss is 6.319650115966797 and perplexity is 555.3786403343994
At time: 79.4866828918457 and batch: 450, loss is 6.277946176528931 and perplexity is 532.693481180091
At time: 81.91399669647217 and batch: 500, loss is 6.277940855026245 and perplexity is 532.6906464578428
At time: 84.34125757217407 and batch: 550, loss is 6.316289691925049 and perplexity is 553.5154648822555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.932609049479167 and perplexity of 377.13720094533943
Finished 3 epochs...
Completing Train Step...
At time: 88.28059983253479 and batch: 50, loss is 6.406566762924195 and perplexity is 605.8102164979017
At time: 90.74240803718567 and batch: 100, loss is 6.407820806503296 and perplexity is 606.5704054654775
At time: 93.17220973968506 and batch: 150, loss is 6.415571203231812 and perplexity is 611.2898318408746
At time: 95.59603905677795 and batch: 200, loss is 6.3941198921203615 and perplexity is 598.3165083564397
At time: 98.02969074249268 and batch: 250, loss is 6.322645244598388 and perplexity is 557.0445643844607
At time: 100.47447371482849 and batch: 300, loss is 6.297587251663208 and perplexity is 543.259578954563
At time: 102.94674301147461 and batch: 350, loss is 6.245379686355591 and perplexity is 515.6249635281154
At time: 105.40375566482544 and batch: 400, loss is 6.241555852890015 and perplexity is 513.6570643940087
At time: 107.88010907173157 and batch: 450, loss is 6.225095596313476 and perplexity is 505.2713422257741
At time: 110.35584568977356 and batch: 500, loss is 6.223529386520386 and perplexity is 504.48060069661693
At time: 112.81739258766174 and batch: 550, loss is 6.237748003005981 and perplexity is 511.70485462067654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.886176554361979 and perplexity of 360.02610919500245
Finished 4 epochs...
Completing Train Step...
At time: 116.90473937988281 and batch: 50, loss is 6.31313946723938 and perplexity is 551.7745104394719
At time: 119.3654305934906 and batch: 100, loss is 6.312854309082031 and perplexity is 551.6171898684908
At time: 121.83665633201599 and batch: 150, loss is 6.334026603698731 and perplexity is 563.4207043481107
At time: 124.30710864067078 and batch: 200, loss is 6.325212774276733 and perplexity is 558.4766304850793
At time: 126.76723456382751 and batch: 250, loss is 6.262552251815796 and perplexity is 524.556032169955
At time: 129.23968935012817 and batch: 300, loss is 6.243145093917847 and perplexity is 514.4740382872427
At time: 131.75131225585938 and batch: 350, loss is 6.204131078720093 and perplexity is 494.78883654816514
At time: 134.21187233924866 and batch: 400, loss is 6.221584253311157 and perplexity is 503.5002724703776
At time: 136.68155717849731 and batch: 450, loss is 6.20728271484375 and perplexity is 496.3506908243909
At time: 139.14044165611267 and batch: 500, loss is 6.19917784690857 and perplexity is 492.34409243352826
At time: 141.60026049613953 and batch: 550, loss is 6.191071405410766 and perplexity is 488.3690672738881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.857211303710938 and perplexity of 349.74744339467435
Finished 5 epochs...
Completing Train Step...
At time: 145.6105670928955 and batch: 50, loss is 6.258278970718384 and perplexity is 522.3192394190655
At time: 148.11521577835083 and batch: 100, loss is 6.253028812408448 and perplexity is 519.5841667884359
At time: 150.57436966896057 and batch: 150, loss is 6.287894830703736 and perplexity is 538.0195139054576
At time: 153.03405928611755 and batch: 200, loss is 6.283582239151001 and perplexity is 535.7042514735416
At time: 155.49706959724426 and batch: 250, loss is 6.22653491973877 and perplexity is 505.99911472913425
At time: 157.9502305984497 and batch: 300, loss is 6.219556608200073 and perplexity is 502.48038693685675
At time: 160.41820693016052 and batch: 350, loss is 6.190520706176758 and perplexity is 488.1001968427924
At time: 162.8876302242279 and batch: 400, loss is 6.216404619216919 and perplexity is 500.89906775250967
At time: 165.3600161075592 and batch: 450, loss is 6.200866451263428 and perplexity is 493.1761691385599
At time: 167.81806445121765 and batch: 500, loss is 6.193193597793579 and perplexity is 489.40658090082866
At time: 170.27632188796997 and batch: 550, loss is 6.173523597717285 and perplexity is 479.8740135681994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.848929341634115 and perplexity of 346.86281003235007
Finished 6 epochs...
Completing Train Step...
At time: 174.35301995277405 and batch: 50, loss is 6.2339794921875 and perplexity is 509.78011831284135
At time: 176.8237955570221 and batch: 100, loss is 6.226028175354004 and perplexity is 505.74276747579995
At time: 179.2838945388794 and batch: 150, loss is 6.26176251411438 and perplexity is 524.1419340308233
At time: 181.76267647743225 and batch: 200, loss is 6.264998569488525 and perplexity is 525.8408337374411
At time: 184.2336187362671 and batch: 250, loss is 6.214617223739624 and perplexity is 500.00456267953393
At time: 186.70353651046753 and batch: 300, loss is 6.214975500106812 and perplexity is 500.1837345924427
At time: 189.17036819458008 and batch: 350, loss is 6.186297578811645 and perplexity is 486.04323400972993
At time: 191.64019012451172 and batch: 400, loss is 6.213186120986938 and perplexity is 499.2895165478094
At time: 194.12549138069153 and batch: 450, loss is 6.194183826446533 and perplexity is 489.89144534389357
At time: 196.59098029136658 and batch: 500, loss is 6.186347494125366 and perplexity is 486.0674956157451
At time: 199.05162477493286 and batch: 550, loss is 6.162594661712647 and perplexity is 474.6580555361706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.8482610066731775 and perplexity of 346.63106693938806
Finished 7 epochs...
Completing Train Step...
At time: 203.07914447784424 and batch: 50, loss is 6.212831268310547 and perplexity is 499.1123737582235
At time: 205.5832874774933 and batch: 100, loss is 6.202944803237915 and perplexity is 494.2022286905642
At time: 208.04314732551575 and batch: 150, loss is 6.247430143356323 and perplexity is 516.6833150256437
At time: 210.5155792236328 and batch: 200, loss is 6.255351963043213 and perplexity is 520.7926422670977
At time: 212.97206091880798 and batch: 250, loss is 6.207311315536499 and perplexity is 496.3648870010043
At time: 215.4390892982483 and batch: 300, loss is 6.210719223022461 and perplexity is 498.0593382417953
At time: 217.90734887123108 and batch: 350, loss is 6.180984592437744 and perplexity is 483.4677407689344
At time: 220.3664674758911 and batch: 400, loss is 6.205741548538208 and perplexity is 495.58632102574336
At time: 222.8235239982605 and batch: 450, loss is 6.189173555374145 and perplexity is 487.4430949778889
At time: 225.27407670021057 and batch: 500, loss is 6.179205169677735 and perplexity is 482.60821222640266
At time: 227.74032711982727 and batch: 550, loss is 6.148941907882691 and perplexity is 468.2217028960685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.839769999186198 and perplexity of 343.70028022917614
Finished 8 epochs...
Completing Train Step...
At time: 231.79271030426025 and batch: 50, loss is 6.196270380020142 and perplexity is 490.91469725357575
At time: 234.26022958755493 and batch: 100, loss is 6.18792314529419 and perplexity is 486.8339721245666
At time: 236.7187578678131 and batch: 150, loss is 6.232479410171509 and perplexity is 509.01597960390944
At time: 239.18074321746826 and batch: 200, loss is 6.2423020362854 and perplexity is 514.0404898014275
At time: 241.65602326393127 and batch: 250, loss is 6.195259475708008 and perplexity is 490.41868022435034
At time: 244.1169309616089 and batch: 300, loss is 6.199639701843262 and perplexity is 492.57153650122484
At time: 246.5858690738678 and batch: 350, loss is 6.170052185058593 and perplexity is 478.21106091035443
At time: 249.05080914497375 and batch: 400, loss is 6.194958562850952 and perplexity is 490.2711291392531
At time: 251.5111129283905 and batch: 450, loss is 6.177452802658081 and perplexity is 481.7632460735188
At time: 253.98045659065247 and batch: 500, loss is 6.167582015991211 and perplexity is 477.0312564981005
At time: 256.47049951553345 and batch: 550, loss is 6.139470262527466 and perplexity is 463.80780939380475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.834708658854167 and perplexity of 341.9650910344667
Finished 9 epochs...
Completing Train Step...
At time: 260.49945759773254 and batch: 50, loss is 6.187105674743652 and perplexity is 486.4361623104107
At time: 262.9967908859253 and batch: 100, loss is 6.177589673995971 and perplexity is 481.8291901663804
At time: 265.4551064968109 and batch: 150, loss is 6.2241987609863285 and perplexity is 504.8184001738462
At time: 267.91946840286255 and batch: 200, loss is 6.2372865390777585 and perplexity is 511.46877576350056
At time: 270.3881506919861 and batch: 250, loss is 6.189305534362793 and perplexity is 487.50743147002675
At time: 272.8624413013458 and batch: 300, loss is 6.1928641223907475 and perplexity is 489.24536003105186
At time: 275.33015847206116 and batch: 350, loss is 6.160554008483887 and perplexity is 473.69043067177284
At time: 277.78938126564026 and batch: 400, loss is 6.184109573364258 and perplexity is 484.98093135185457
At time: 280.2613275051117 and batch: 450, loss is 6.166507167816162 and perplexity is 476.5187957806826
At time: 282.7295515537262 and batch: 500, loss is 6.153692979812622 and perplexity is 470.451550775046
At time: 285.1987679004669 and batch: 550, loss is 6.124008512496948 and perplexity is 456.6916846218499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.821444193522136 and perplexity of 337.45905806674574
Finished 10 epochs...
Completing Train Step...
At time: 289.2951579093933 and batch: 50, loss is 6.16959358215332 and perplexity is 477.9918022086687
At time: 291.76591062545776 and batch: 100, loss is 6.160004281997681 and perplexity is 473.43010205709356
At time: 294.233460187912 and batch: 150, loss is 6.205048656463623 and perplexity is 495.24305212951396
At time: 296.70254254341125 and batch: 200, loss is 6.216989116668701 and perplexity is 501.1919275607791
At time: 299.1763870716095 and batch: 250, loss is 6.170540857315063 and perplexity is 478.44480649639826
At time: 301.6501941680908 and batch: 300, loss is 6.1747089385986325 and perplexity is 480.44316510703175
At time: 304.12493920326233 and batch: 350, loss is 6.14124550819397 and perplexity is 464.631913474691
At time: 306.5893986225128 and batch: 400, loss is 6.166806678771973 and perplexity is 476.66153975629396
At time: 309.06124567985535 and batch: 450, loss is 6.152456789016724 and perplexity is 469.87034221441394
At time: 311.5269663333893 and batch: 500, loss is 6.140208692550659 and perplexity is 464.15042548868956
At time: 313.994473695755 and batch: 550, loss is 6.112836475372315 and perplexity is 451.6179031773847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.816195170084636 and perplexity of 335.6923683160649
Finished 11 epochs...
Completing Train Step...
At time: 318.08992552757263 and batch: 50, loss is 6.160479249954224 and perplexity is 473.655019595317
At time: 320.59627866744995 and batch: 100, loss is 6.153139181137085 and perplexity is 470.19108745809956
At time: 323.0696802139282 and batch: 150, loss is 6.197648324966431 and perplexity is 491.59161695164636
At time: 325.5421950817108 and batch: 200, loss is 6.209547805786133 and perplexity is 497.4762445379497
At time: 328.0181076526642 and batch: 250, loss is 6.162504634857178 and perplexity is 474.6153254874624
At time: 330.4905459880829 and batch: 300, loss is 6.167273406982422 and perplexity is 476.88406306864835
At time: 332.964022397995 and batch: 350, loss is 6.13569296836853 and perplexity is 462.0591754959351
At time: 335.4387285709381 and batch: 400, loss is 6.1613444805145265 and perplexity is 474.0650177391956
At time: 337.9069330692291 and batch: 450, loss is 6.144004135131836 and perplexity is 465.91542914387946
At time: 340.3742206096649 and batch: 500, loss is 6.133168687820435 and perplexity is 460.89427938801373
At time: 342.8445131778717 and batch: 550, loss is 6.10601848602295 and perplexity is 448.54925003294704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.809663391113281 and perplexity of 333.5068454104138
Finished 12 epochs...
Completing Train Step...
At time: 346.93832302093506 and batch: 50, loss is 6.155010557174682 and perplexity is 471.07181562202175
At time: 349.40362906455994 and batch: 100, loss is 6.146711931228638 and perplexity is 467.1787427502799
At time: 351.8691909313202 and batch: 150, loss is 6.190880584716797 and perplexity is 488.27588524037054
At time: 354.33598041534424 and batch: 200, loss is 6.201290340423584 and perplexity is 493.3852654844112
At time: 356.80187940597534 and batch: 250, loss is 6.155573682785034 and perplexity is 471.33716293063407
At time: 359.27400302886963 and batch: 300, loss is 6.159042530059814 and perplexity is 472.97499862238044
At time: 361.7420291900635 and batch: 350, loss is 6.124974956512451 and perplexity is 457.1332649143282
At time: 364.2020983695984 and batch: 400, loss is 6.1469689464569095 and perplexity is 467.2988302329863
At time: 366.6747648715973 and batch: 450, loss is 6.121468238830566 and perplexity is 455.5330350283378
At time: 369.1447443962097 and batch: 500, loss is 6.092848558425903 and perplexity is 442.68061843737064
At time: 371.6185235977173 and batch: 550, loss is 6.052764568328858 and perplexity is 425.2871417036398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.755869547526042 and perplexity of 316.04024008667506
Finished 13 epochs...
Completing Train Step...
At time: 375.74276065826416 and batch: 50, loss is 6.09340684890747 and perplexity is 442.92783181503535
At time: 378.2688443660736 and batch: 100, loss is 6.081349802017212 and perplexity is 437.6194959122483
At time: 380.7406928539276 and batch: 150, loss is 6.124254550933838 and perplexity is 456.80406215410414
At time: 383.24206829071045 and batch: 200, loss is 6.1329672908782955 and perplexity is 460.8014660359718
At time: 385.70657539367676 and batch: 250, loss is 6.081015348434448 and perplexity is 437.47315697710786
At time: 388.1667230129242 and batch: 300, loss is 6.0739827156066895 and perplexity is 434.4073618335173
At time: 390.6283140182495 and batch: 350, loss is 6.039451675415039 and perplexity is 419.6628603235455
At time: 393.0944621562958 and batch: 400, loss is 6.058282680511475 and perplexity is 427.6404106911886
At time: 395.5577952861786 and batch: 450, loss is 6.039460773468018 and perplexity is 419.6666784558506
At time: 398.0126769542694 and batch: 500, loss is 6.023644914627075 and perplexity is 413.0815018888554
At time: 400.48044657707214 and batch: 550, loss is 5.988779020309448 and perplexity is 398.92723040107916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.707243855794271 and perplexity of 301.040213590692
Finished 14 epochs...
Completing Train Step...
At time: 404.56201672554016 and batch: 50, loss is 6.039456043243408 and perplexity is 419.6646933428955
At time: 407.03509402275085 and batch: 100, loss is 6.02962947845459 and perplexity is 415.56102653929656
At time: 409.5070960521698 and batch: 150, loss is 6.066927042007446 and perplexity is 431.35311284197553
At time: 411.979603767395 and batch: 200, loss is 6.079410352706909 and perplexity is 436.7715775958077
At time: 414.44666862487793 and batch: 250, loss is 6.030069379806519 and perplexity is 415.7438726108487
At time: 416.9167535305023 and batch: 300, loss is 6.030669269561767 and perplexity is 415.9933479221955
At time: 419.3817665576935 and batch: 350, loss is 6.000593500137329 and perplexity is 403.6682996035001
At time: 421.85002756118774 and batch: 400, loss is 6.020593881607056 and perplexity is 411.82309727982704
At time: 424.3255774974823 and batch: 450, loss is 6.0030362415313725 and perplexity is 404.6555621910843
At time: 426.8046190738678 and batch: 500, loss is 5.985946254730225 and perplexity is 397.79876217178423
At time: 429.2703261375427 and batch: 550, loss is 5.956100931167603 and perplexity is 386.1017482251487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.675432840983073 and perplexity of 291.61453395363236
Finished 15 epochs...
Completing Train Step...
At time: 433.34610509872437 and batch: 50, loss is 6.008777847290039 and perplexity is 406.9856176260346
At time: 435.8396301269531 and batch: 100, loss is 5.998263750076294 and perplexity is 402.72894800995635
At time: 438.30691957473755 and batch: 150, loss is 6.035203924179077 and perplexity is 417.8840176052918
At time: 440.777291059494 and batch: 200, loss is 6.051298952102661 and perplexity is 424.66429050986613
At time: 443.24718737602234 and batch: 250, loss is 6.005300264358521 and perplexity is 405.57274949573826
At time: 445.75102615356445 and batch: 300, loss is 6.007813043594361 and perplexity is 406.5931457576597
At time: 448.219765663147 and batch: 350, loss is 5.976276330947876 and perplexity is 393.9706172257501
At time: 450.69218707084656 and batch: 400, loss is 5.99605092048645 and perplexity is 401.8387627544912
At time: 453.1574864387512 and batch: 450, loss is 5.977663326263428 and perplexity is 394.5174317532399
At time: 455.6252634525299 and batch: 500, loss is 5.963605012893677 and perplexity is 389.009985476224
At time: 458.0941870212555 and batch: 550, loss is 5.933963842391968 and perplexity is 377.64849001953655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.657966105143229 and perplexity of 286.56520593432947
Finished 16 epochs...
Completing Train Step...
At time: 462.21728467941284 and batch: 50, loss is 5.989409484863281 and perplexity is 399.1788191799758
At time: 464.68658208847046 and batch: 100, loss is 5.978926095962525 and perplexity is 395.0159310905108
At time: 467.1521120071411 and batch: 150, loss is 6.0154288864135745 and perplexity is 409.7015166551561
At time: 469.6209280490875 and batch: 200, loss is 6.034646005630493 and perplexity is 417.6509373866474
At time: 472.0961883068085 and batch: 250, loss is 5.988640727996827 and perplexity is 398.87206564633783
At time: 474.56467938423157 and batch: 300, loss is 5.991219539642334 and perplexity is 399.9020090184918
At time: 477.03569316864014 and batch: 350, loss is 5.9607690238952635 and perplexity is 387.9083203306327
At time: 479.5049521923065 and batch: 400, loss is 5.98183045387268 and perplexity is 396.16486638663497
At time: 481.9762499332428 and batch: 450, loss is 5.963239288330078 and perplexity is 388.8677409817884
At time: 484.442494392395 and batch: 500, loss is 5.949956827163696 and perplexity is 383.736771698656
At time: 486.9040560722351 and batch: 550, loss is 5.922770280838012 and perplexity is 373.4448292601441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.649767557779948 and perplexity of 284.22539216391453
Finished 17 epochs...
Completing Train Step...
At time: 490.9843237400055 and batch: 50, loss is 5.9790357494354245 and perplexity is 395.05924833410535
At time: 493.47472739219666 and batch: 100, loss is 5.967330780029297 and perplexity is 390.46204944192465
At time: 495.9414312839508 and batch: 150, loss is 6.004917240142822 and perplexity is 405.41743505794796
At time: 498.4087390899658 and batch: 200, loss is 6.023184642791748 and perplexity is 412.89141585682063
At time: 500.8750033378601 and batch: 250, loss is 5.9784471130371095 and perplexity is 394.8267705102121
At time: 503.3521361351013 and batch: 300, loss is 5.9796460914611815 and perplexity is 395.30044319422
At time: 505.827219247818 and batch: 350, loss is 5.950012407302856 and perplexity is 383.75810043454953
At time: 508.32932114601135 and batch: 400, loss is 5.973626689910889 and perplexity is 392.9281182449358
At time: 510.80439615249634 and batch: 450, loss is 5.954956188201904 and perplexity is 385.6600138492225
At time: 513.2727818489075 and batch: 500, loss is 5.940474071502686 and perplexity is 380.1150885618662
At time: 515.7482931613922 and batch: 550, loss is 5.91328857421875 and perplexity is 369.92066888062396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.646076965332031 and perplexity of 283.17836534114696
Finished 18 epochs...
Completing Train Step...
At time: 519.8623135089874 and batch: 50, loss is 5.969941158294677 and perplexity is 391.4826345663286
At time: 522.3374886512756 and batch: 100, loss is 5.955890445709229 and perplexity is 386.0204879740451
At time: 524.8140919208527 and batch: 150, loss is 5.993128519058228 and perplexity is 400.66614284790376
At time: 527.2895171642303 and batch: 200, loss is 6.01395022392273 and perplexity is 409.0961540638137
At time: 529.7663776874542 and batch: 250, loss is 5.968162651062012 and perplexity is 390.7869986494074
At time: 532.2482264041901 and batch: 300, loss is 5.972623338699341 and perplexity is 392.53407105836226
At time: 534.7294182777405 and batch: 350, loss is 5.941365604400635 and perplexity is 380.4541247768392
At time: 537.2017068862915 and batch: 400, loss is 5.962383165359497 and perplexity is 388.5349648451889
At time: 539.683191537857 and batch: 450, loss is 5.944373188018798 and perplexity is 381.6000948064967
At time: 542.15882396698 and batch: 500, loss is 5.927959375381469 and perplexity is 375.38770631269216
At time: 544.631833076477 and batch: 550, loss is 5.90269344329834 and perplexity is 366.022000883521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.640420532226562 and perplexity of 281.58110751043233
Finished 19 epochs...
Completing Train Step...
At time: 548.6777741909027 and batch: 50, loss is 5.9584206199646 and perplexity is 386.99842372698475
At time: 551.1776897907257 and batch: 100, loss is 5.944110641479492 and perplexity is 381.49992017303333
At time: 553.6410057544708 and batch: 150, loss is 5.980957088470459 and perplexity is 395.8190207455491
At time: 556.1017973423004 and batch: 200, loss is 5.997310447692871 and perplexity is 402.34520848290924
At time: 558.5661461353302 and batch: 250, loss is 5.944816389083862 and perplexity is 381.7692578587969
At time: 561.0346267223358 and batch: 300, loss is 5.940112342834473 and perplexity is 379.977614902694
At time: 563.5075871944427 and batch: 350, loss is 5.898558540344238 and perplexity is 364.5116601389155
At time: 565.9813244342804 and batch: 400, loss is 5.9096779537200925 and perplexity is 368.58743408153543
At time: 568.4563539028168 and batch: 450, loss is 5.878347024917603 and perplexity is 357.21828050468724
At time: 570.9783043861389 and batch: 500, loss is 5.858151273727417 and perplexity is 350.07635006179135
At time: 573.4478640556335 and batch: 550, loss is 5.832268629074097 and perplexity is 341.1317031876621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.581172688802083 and perplexity of 265.3826346244566
Finished 20 epochs...
Completing Train Step...
At time: 577.5293409824371 and batch: 50, loss is 5.884148168563843 and perplexity is 359.2965774841627
At time: 579.9999387264252 and batch: 100, loss is 5.87020977973938 and perplexity is 354.3233023218795
At time: 582.4709701538086 and batch: 150, loss is 5.911783256530762 and perplexity is 369.36423966105
At time: 584.9454593658447 and batch: 200, loss is 5.930125379562378 and perplexity is 376.2016788697738
At time: 587.4213721752167 and batch: 250, loss is 5.881174020767212 and perplexity is 358.2295638745164
At time: 589.890692949295 and batch: 300, loss is 5.884911642074585 and perplexity is 359.5709956457413
At time: 592.3581929206848 and batch: 350, loss is 5.849656095504761 and perplexity is 347.1149855457902
At time: 594.8261022567749 and batch: 400, loss is 5.871279468536377 and perplexity is 354.70252077559456
At time: 597.2955465316772 and batch: 450, loss is 5.849299650192261 and perplexity is 346.9912800847196
At time: 599.7665505409241 and batch: 500, loss is 5.834624376296997 and perplexity is 341.93627055668037
At time: 602.2404038906097 and batch: 550, loss is 5.810224065780639 and perplexity is 333.69388667977705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.570742797851563 and perplexity of 262.6291071421304
Finished 21 epochs...
Completing Train Step...
At time: 606.3216016292572 and batch: 50, loss is 5.8648089504241945 and perplexity is 352.41482097368464
At time: 608.8139328956604 and batch: 100, loss is 5.852814111709595 and perplexity is 348.2129130189854
At time: 611.2810559272766 and batch: 150, loss is 5.892460680007934 and perplexity is 362.2956821730606
At time: 613.7474277019501 and batch: 200, loss is 5.909704780578613 and perplexity is 368.5973222571157
At time: 616.2184898853302 and batch: 250, loss is 5.860732946395874 and perplexity is 350.9813002465714
At time: 618.6851170063019 and batch: 300, loss is 5.863063716888428 and perplexity is 351.8003111971302
At time: 621.15127825737 and batch: 350, loss is 5.821565084457397 and perplexity is 337.49985627390487
At time: 623.6210215091705 and batch: 400, loss is 5.837790927886963 and perplexity is 343.02074551465637
At time: 626.0890851020813 and batch: 450, loss is 5.809331979751587 and perplexity is 333.3963357657294
At time: 628.5518035888672 and batch: 500, loss is 5.794779682159424 and perplexity is 328.5797840182707
At time: 631.0118563175201 and batch: 550, loss is 5.773581171035767 and perplexity is 321.6876909610891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.548106892903646 and perplexity of 256.7510383234852
Finished 22 epochs...
Completing Train Step...
At time: 635.103408575058 and batch: 50, loss is 5.829238548278808 and perplexity is 340.0996110161729
At time: 637.5731370449066 and batch: 100, loss is 5.819755525588989 and perplexity is 336.88968265494094
At time: 640.0435969829559 and batch: 150, loss is 5.858482503890992 and perplexity is 350.19232511464344
At time: 642.5117344856262 and batch: 200, loss is 5.877487077713012 and perplexity is 356.91122368818674
At time: 644.9815866947174 and batch: 250, loss is 5.827224950790406 and perplexity is 339.4154763107426
At time: 647.4502823352814 and batch: 300, loss is 5.833078012466431 and perplexity is 341.4079212909143
At time: 649.9160277843475 and batch: 350, loss is 5.796993131637573 and perplexity is 329.3078842790387
At time: 652.3904407024384 and batch: 400, loss is 5.810868167877198 and perplexity is 333.908888846036
At time: 654.8605983257294 and batch: 450, loss is 5.779696397781372 and perplexity is 323.66091132836044
At time: 657.3293805122375 and batch: 500, loss is 5.770807638168335 and perplexity is 320.7967157228648
At time: 659.7972302436829 and batch: 550, loss is 5.75321551322937 and perplexity is 315.2025705439093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.527461751302083 and perplexity of 251.50471862748054
Finished 23 epochs...
Completing Train Step...
At time: 663.894613981247 and batch: 50, loss is 5.809389839172363 and perplexity is 333.41562644267407
At time: 666.3888964653015 and batch: 100, loss is 5.800411939620972 and perplexity is 330.4356514139818
At time: 668.8549702167511 and batch: 150, loss is 5.835974264144897 and perplexity is 342.3981578509465
At time: 671.3229672908783 and batch: 200, loss is 5.857919101715088 and perplexity is 349.9950815656288
At time: 673.7965319156647 and batch: 250, loss is 5.8077026462554935 and perplexity is 332.85356424637604
At time: 676.2643609046936 and batch: 300, loss is 5.811197090148926 and perplexity is 334.01873698106414
At time: 678.731858253479 and batch: 350, loss is 5.776844635009765 and perplexity is 322.73922203505714
At time: 681.202803850174 and batch: 400, loss is 5.79117712020874 and perplexity is 327.39818466098916
At time: 683.6733334064484 and batch: 450, loss is 5.758485155105591 and perplexity is 316.86795935658955
At time: 686.143716096878 and batch: 500, loss is 5.750934791564942 and perplexity is 314.48450038293754
At time: 688.6145920753479 and batch: 550, loss is 5.733887014389038 and perplexity is 309.1686789078248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.516139221191406 and perplexity of 248.67310961786478
Finished 24 epochs...
Completing Train Step...
At time: 692.7230570316315 and batch: 50, loss is 5.7900745391845705 and perplexity is 327.0374005686314
At time: 695.1911315917969 and batch: 100, loss is 5.780558290481568 and perplexity is 323.9399925569334
At time: 697.7024435997009 and batch: 150, loss is 5.814682855606079 and perplexity is 335.18507957385907
At time: 700.1645476818085 and batch: 200, loss is 5.8349072360992436 and perplexity is 342.03300426294453
At time: 702.6152625083923 and batch: 250, loss is 5.786912126541138 and perplexity is 326.00480696230386
At time: 705.067195892334 and batch: 300, loss is 5.791454582214356 and perplexity is 327.48903782150734
At time: 707.5355522632599 and batch: 350, loss is 5.75757643699646 and perplexity is 316.5801464939045
At time: 710.0025725364685 and batch: 400, loss is 5.770348329544067 and perplexity is 320.64940485786167
At time: 712.4763705730438 and batch: 450, loss is 5.738147382736206 and perplexity is 310.48866117015035
At time: 714.9411358833313 and batch: 500, loss is 5.726929121017456 and perplexity is 307.02498265953795
At time: 717.4066903591156 and batch: 550, loss is 5.70910080909729 and perplexity is 301.5997505658914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.501313781738281 and perplexity of 245.013615320727
Finished 25 epochs...
Completing Train Step...
At time: 721.4247672557831 and batch: 50, loss is 5.766230564117432 and perplexity is 319.3317605613613
At time: 723.9096641540527 and batch: 100, loss is 5.758111906051636 and perplexity is 316.7497107599923
At time: 726.3719012737274 and batch: 150, loss is 5.789838676452637 and perplexity is 326.9602737299197
At time: 728.8298397064209 and batch: 200, loss is 5.812694158554077 and perplexity is 334.5191603694861
At time: 731.2944984436035 and batch: 250, loss is 5.763351831436157 and perplexity is 318.4138116847077
At time: 733.7494449615479 and batch: 300, loss is 5.768383865356445 and perplexity is 320.0201188924817
At time: 736.2096798419952 and batch: 350, loss is 5.736577425003052 and perplexity is 310.0015895364718
At time: 738.6742105484009 and batch: 400, loss is 5.75169075012207 and perplexity is 314.722327514488
At time: 741.1364328861237 and batch: 450, loss is 5.72186240196228 and perplexity is 305.47330759978075
At time: 743.5927460193634 and batch: 500, loss is 5.709105539321899 and perplexity is 301.60117720382783
At time: 746.053777217865 and batch: 550, loss is 5.691871404647827 and perplexity is 296.44787579349037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4822265625 and perplexity of 240.3813360298292
Finished 26 epochs...
Completing Train Step...
At time: 750.098197221756 and batch: 50, loss is 5.750436944961548 and perplexity is 314.327974308822
At time: 752.5670120716095 and batch: 100, loss is 5.740493535995483 and perplexity is 311.21797035554835
At time: 755.0246210098267 and batch: 150, loss is 5.7752441310882565 and perplexity is 322.2230897905836
At time: 757.4802124500275 and batch: 200, loss is 5.7975719547271725 and perplexity is 329.49855046171757
At time: 759.9697179794312 and batch: 250, loss is 5.748162879943847 and perplexity is 313.61398419602324
At time: 762.4314117431641 and batch: 300, loss is 5.755407190322876 and perplexity is 315.894150380597
At time: 764.8917510509491 and batch: 350, loss is 5.721695413589478 and perplexity is 305.42230136805244
At time: 767.3566813468933 and batch: 400, loss is 5.737468872070313 and perplexity is 310.2780627566219
At time: 769.8165228366852 and batch: 450, loss is 5.709297618865967 and perplexity is 301.6591141845117
At time: 772.2761359214783 and batch: 500, loss is 5.697779378890991 and perplexity is 298.2044660437402
At time: 774.735725402832 and batch: 550, loss is 5.679746513366699 and perplexity is 292.87518056883897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.472016398111979 and perplexity of 237.93949011025907
Finished 27 epochs...
Completing Train Step...
At time: 778.7199161052704 and batch: 50, loss is 5.734372072219848 and perplexity is 309.31867997322513
At time: 781.2193555831909 and batch: 100, loss is 5.725710544586182 and perplexity is 306.65107711436775
At time: 783.6897311210632 and batch: 150, loss is 5.759003829956055 and perplexity is 317.03235342787536
At time: 786.1576473712921 and batch: 200, loss is 5.7796416759490965 and perplexity is 323.6432004948457
At time: 788.6241919994354 and batch: 250, loss is 5.72918321609497 and perplexity is 307.71782673647795
At time: 791.0892164707184 and batch: 300, loss is 5.739582529067993 and perplexity is 310.9345777345153
At time: 793.5554578304291 and batch: 350, loss is 5.706954412460327 and perplexity is 300.95309211659225
At time: 796.0222074985504 and batch: 400, loss is 5.720562200546265 and perplexity is 305.0763888657514
At time: 798.4815001487732 and batch: 450, loss is 5.693819351196289 and perplexity is 297.0259032104202
At time: 800.9447116851807 and batch: 500, loss is 5.680827112197876 and perplexity is 293.1918322025222
At time: 803.4122648239136 and batch: 550, loss is 5.663912076950073 and perplexity is 288.2741903210958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4616750081380205 and perplexity of 235.4915444493051
Finished 28 epochs...
Completing Train Step...
At time: 807.481119632721 and batch: 50, loss is 5.721367206573486 and perplexity is 305.32207607412516
At time: 809.9493436813354 and batch: 100, loss is 5.71459921836853 and perplexity is 303.2626368558709
At time: 812.4168484210968 and batch: 150, loss is 5.745313081741333 and perplexity is 312.7215199031481
At time: 814.8751683235168 and batch: 200, loss is 5.768612365722657 and perplexity is 320.09325196199205
At time: 817.3297417163849 and batch: 250, loss is 5.721370582580566 and perplexity is 305.32310684535565
At time: 819.7833549976349 and batch: 300, loss is 5.729761686325073 and perplexity is 307.8958838339168
At time: 822.2775473594666 and batch: 350, loss is 5.700615854263305 and perplexity is 299.0515164184324
At time: 824.7388775348663 and batch: 400, loss is 5.7144753360748295 and perplexity is 303.22507031178634
At time: 827.203369140625 and batch: 450, loss is 5.687424230575561 and perplexity is 295.13244762348273
At time: 829.6622233390808 and batch: 500, loss is 5.677851886749267 and perplexity is 292.320816778174
At time: 832.12073969841 and batch: 550, loss is 5.660225791931152 and perplexity is 287.2134857228412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.45754140218099 and perplexity of 234.5201243155614
Finished 29 epochs...
Completing Train Step...
At time: 836.1362361907959 and batch: 50, loss is 5.7134127521514895 and perplexity is 302.90303934968097
At time: 838.6334388256073 and batch: 100, loss is 5.705889377593994 and perplexity is 300.6327372052141
At time: 841.0971746444702 and batch: 150, loss is 5.737628307342529 and perplexity is 310.327535967803
At time: 843.5606462955475 and batch: 200, loss is 5.7582923030853275 and perplexity is 316.80685662255314
At time: 846.0288321971893 and batch: 250, loss is 5.7102541255950925 and perplexity is 301.9477911958739
At time: 848.490786075592 and batch: 300, loss is 5.7193518733978275 and perplexity is 304.7073699917883
At time: 850.9491717815399 and batch: 350, loss is 5.687851848602295 and perplexity is 295.25867856575036
At time: 853.4117176532745 and batch: 400, loss is 5.702084312438965 and perplexity is 299.4909836533162
At time: 855.8774955272675 and batch: 450, loss is 5.673317031860352 and perplexity is 290.9981855301149
At time: 858.3472828865051 and batch: 500, loss is 5.662154006958008 and perplexity is 287.7678293570355
At time: 860.8160264492035 and batch: 550, loss is 5.644172105789185 and perplexity is 282.63946375846035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.44468027750651 and perplexity of 231.52324468833135
Finished 30 epochs...
Completing Train Step...
At time: 864.9047484397888 and batch: 50, loss is 5.699407300949097 and perplexity is 298.69031502667957
At time: 867.3729856014252 and batch: 100, loss is 5.688104839324951 and perplexity is 295.33338572192133
At time: 869.8370988368988 and batch: 150, loss is 5.718674154281616 and perplexity is 304.50093394302405
At time: 872.3029749393463 and batch: 200, loss is 5.738074417114258 and perplexity is 310.4660069983782
At time: 874.7738943099976 and batch: 250, loss is 5.693082857131958 and perplexity is 296.80722593289613
At time: 877.2402908802032 and batch: 300, loss is 5.705697746276855 and perplexity is 300.57513207745785
At time: 879.7045726776123 and batch: 350, loss is 5.670605049133301 and perplexity is 290.2100726346715
At time: 882.1733431816101 and batch: 400, loss is 5.682820901870728 and perplexity is 293.7769781849014
At time: 884.6634545326233 and batch: 450, loss is 5.651052646636963 and perplexity is 284.59088184132173
At time: 887.128808259964 and batch: 500, loss is 5.642931156158447 and perplexity is 282.2889399564513
At time: 889.5951421260834 and batch: 550, loss is 5.62261266708374 and perplexity is 276.6111327801964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.425710042317708 and perplexity of 227.17259121633884
Finished 31 epochs...
Completing Train Step...
At time: 893.6403138637543 and batch: 50, loss is 5.67847544670105 and perplexity is 292.5031531755188
At time: 896.1304948329926 and batch: 100, loss is 5.669423589706421 and perplexity is 289.8674036731785
At time: 898.5935130119324 and batch: 150, loss is 5.698626909255982 and perplexity is 298.4573105152366
At time: 901.0623018741608 and batch: 200, loss is 5.718397397994995 and perplexity is 304.4166730556762
At time: 903.5320258140564 and batch: 250, loss is 5.67366244316101 and perplexity is 291.09871695321345
At time: 905.9970846176147 and batch: 300, loss is 5.686141519546509 and perplexity is 294.7541206718349
At time: 908.4630064964294 and batch: 350, loss is 5.652124767303467 and perplexity is 284.89616122616945
At time: 910.9320480823517 and batch: 400, loss is 5.663139905929565 and perplexity is 288.0516792647203
At time: 913.3955330848694 and batch: 450, loss is 5.634247617721558 and perplexity is 279.8482851919692
At time: 915.8680770397186 and batch: 500, loss is 5.624946098327637 and perplexity is 277.2573394860832
At time: 918.333859205246 and batch: 550, loss is 5.606530466079712 and perplexity is 272.19819691570257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.418862915039062 and perplexity of 225.62242472009842
Finished 32 epochs...
Completing Train Step...
At time: 922.4115302562714 and batch: 50, loss is 5.664292039871216 and perplexity is 288.3837446365112
At time: 924.8748526573181 and batch: 100, loss is 5.6550039863586425 and perplexity is 285.71762169715333
At time: 927.3310165405273 and batch: 150, loss is 5.685085000991822 and perplexity is 294.44287192326567
At time: 929.7884683609009 and batch: 200, loss is 5.705807037353516 and perplexity is 300.6079840524412
At time: 932.2616033554077 and batch: 250, loss is 5.662214889526367 and perplexity is 287.78534993492144
At time: 934.7304399013519 and batch: 300, loss is 5.675859031677246 and perplexity is 291.7388438422661
At time: 937.1980669498444 and batch: 350, loss is 5.642444639205933 and perplexity is 282.1516350049569
At time: 939.6659028530121 and batch: 400, loss is 5.65498049736023 and perplexity is 285.71091055521003
At time: 942.1338536739349 and batch: 450, loss is 5.625885486602783 and perplexity is 277.51791415122517
At time: 944.5992293357849 and batch: 500, loss is 5.614304027557373 and perplexity is 274.3223919169789
At time: 947.0929017066956 and batch: 550, loss is 5.597555027008057 and perplexity is 269.7660297953955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.415316263834636 and perplexity of 224.82363802193217
Finished 33 epochs...
Completing Train Step...
At time: 951.1458656787872 and batch: 50, loss is 5.655590591430664 and perplexity is 285.88527427132436
At time: 953.6543347835541 and batch: 100, loss is 5.645516147613526 and perplexity is 283.0195984201787
At time: 956.1193404197693 and batch: 150, loss is 5.675518035888672 and perplexity is 291.639379084648
At time: 958.5921912193298 and batch: 200, loss is 5.6982299327850345 and perplexity is 298.33885349925964
At time: 961.0633051395416 and batch: 250, loss is 5.656390886306763 and perplexity is 286.11415836664423
At time: 963.5321464538574 and batch: 300, loss is 5.6675630569458 and perplexity is 289.32859726132597
At time: 966.0036172866821 and batch: 350, loss is 5.635882930755615 and perplexity is 280.3062991363185
At time: 968.47971367836 and batch: 400, loss is 5.64586109161377 and perplexity is 283.11724117227783
At time: 970.9539232254028 and batch: 450, loss is 5.614759464263916 and perplexity is 274.44735685833507
At time: 973.4267523288727 and batch: 500, loss is 5.605994834899902 and perplexity is 272.05243811431075
At time: 975.8966822624207 and batch: 550, loss is 5.590068626403808 and perplexity is 267.7539940768756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.410967000325521 and perplexity of 223.8479440901298
Finished 34 epochs...
Completing Train Step...
At time: 980.0134088993073 and batch: 50, loss is 5.650194616317749 and perplexity is 284.34679896635845
At time: 982.4891138076782 and batch: 100, loss is 5.639804077148438 and perplexity is 281.4075788985959
At time: 984.9610505104065 and batch: 150, loss is 5.664270467758179 and perplexity is 288.3775236568739
At time: 987.434889793396 and batch: 200, loss is 5.690394697189331 and perplexity is 296.010432071947
At time: 989.908442735672 and batch: 250, loss is 5.649942541122437 and perplexity is 284.2751312246822
At time: 992.3780555725098 and batch: 300, loss is 5.661947631835938 and perplexity is 287.70844736381804
At time: 994.8451216220856 and batch: 350, loss is 5.6293776321411135 and perplexity is 278.48874124507796
At time: 997.3107011318207 and batch: 400, loss is 5.642637786865234 and perplexity is 282.206137196142
At time: 999.7764241695404 and batch: 450, loss is 5.611240444183349 and perplexity is 273.48326841668415
At time: 1002.2469401359558 and batch: 500, loss is 5.598906011581421 and perplexity is 270.1307258339587
At time: 1004.7202124595642 and batch: 550, loss is 5.582408561706543 and perplexity is 265.71081658572825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.405626424153646 and perplexity of 222.6556536865084
Finished 35 epochs...
Completing Train Step...
At time: 1008.7784316539764 and batch: 50, loss is 5.643262739181519 and perplexity is 282.3825576967107
At time: 1011.2738060951233 and batch: 100, loss is 5.632989292144775 and perplexity is 279.4963663977385
At time: 1013.7479598522186 and batch: 150, loss is 5.65916449546814 and perplexity is 286.9088287605882
At time: 1016.21826338768 and batch: 200, loss is 5.684787712097168 and perplexity is 294.35535033757526
At time: 1018.6901609897614 and batch: 250, loss is 5.641235609054565 and perplexity is 281.8107113060227
At time: 1021.1698973178864 and batch: 300, loss is 5.650415658950806 and perplexity is 284.4096586785855
At time: 1023.6483244895935 and batch: 350, loss is 5.619489116668701 and perplexity is 275.7484719452847
At time: 1026.128446817398 and batch: 400, loss is 5.634207878112793 and perplexity is 279.8371643515726
At time: 1028.6005370616913 and batch: 450, loss is 5.603897495269775 and perplexity is 271.4824496929737
At time: 1031.0724177360535 and batch: 500, loss is 5.593660459518433 and perplexity is 268.7174509917821
At time: 1033.5387909412384 and batch: 550, loss is 5.578262500762939 and perplexity is 264.61144395458786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.407968139648437 and perplexity of 223.17766083832558
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1037.6716301441193 and batch: 50, loss is 5.6312792682647705 and perplexity is 279.0188293530659
At time: 1040.1427521705627 and batch: 100, loss is 5.610562572479248 and perplexity is 273.2979446674387
At time: 1042.611465215683 and batch: 150, loss is 5.629024543762207 and perplexity is 278.3904274646361
At time: 1045.0767254829407 and batch: 200, loss is 5.639981622695923 and perplexity is 281.4575459968446
At time: 1047.544190645218 and batch: 250, loss is 5.584179372787475 and perplexity is 266.181757094334
At time: 1050.0102999210358 and batch: 300, loss is 5.589356431961059 and perplexity is 267.56336905936666
At time: 1052.4863369464874 and batch: 350, loss is 5.533779449462891 and perplexity is 253.0986793053255
At time: 1054.962298631668 and batch: 400, loss is 5.533396348953247 and perplexity is 253.0017356430624
At time: 1057.4422051906586 and batch: 450, loss is 5.495861968994141 and perplexity is 243.68148154169114
At time: 1059.9227192401886 and batch: 500, loss is 5.487054605484008 and perplexity is 241.5447136113597
At time: 1062.3989822864532 and batch: 550, loss is 5.509327335357666 and perplexity is 246.9849331457864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.34856211344401 and perplexity of 210.305684640475
Finished 37 epochs...
Completing Train Step...
At time: 1066.4791502952576 and batch: 50, loss is 5.5877820587158205 and perplexity is 267.1424558738839
At time: 1069.0283756256104 and batch: 100, loss is 5.575760374069214 and perplexity is 263.9501802250129
At time: 1071.499840259552 and batch: 150, loss is 5.597602663040161 and perplexity is 269.77888068473163
At time: 1074.0436871051788 and batch: 200, loss is 5.612217416763306 and perplexity is 273.75058463004723
At time: 1076.5067374706268 and batch: 250, loss is 5.563529205322266 and perplexity is 260.7414244493604
At time: 1078.9801652431488 and batch: 300, loss is 5.569616365432739 and perplexity is 262.33343975764546
At time: 1081.4500682353973 and batch: 350, loss is 5.521639394760132 and perplexity is 250.0446232064586
At time: 1083.9168586730957 and batch: 400, loss is 5.530125970840454 and perplexity is 252.17567580457586
At time: 1086.383151292801 and batch: 450, loss is 5.499024829864502 and perplexity is 244.45343230739687
At time: 1088.8555297851562 and batch: 500, loss is 5.492798557281494 and perplexity is 242.93612708366734
At time: 1091.3289580345154 and batch: 550, loss is 5.505234279632568 and perplexity is 245.97607611364376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.341622416178385 and perplexity of 208.85127925928958
Finished 38 epochs...
Completing Train Step...
At time: 1095.4695026874542 and batch: 50, loss is 5.574916839599609 and perplexity is 263.72762303026417
At time: 1097.9370369911194 and batch: 100, loss is 5.562733325958252 and perplexity is 260.5339882883272
At time: 1100.4102177619934 and batch: 150, loss is 5.584275245666504 and perplexity is 266.2072779290899
At time: 1102.8847970962524 and batch: 200, loss is 5.600049171447754 and perplexity is 270.4397050110448
At time: 1105.3553018569946 and batch: 250, loss is 5.555439262390137 and perplexity is 258.6405506381523
At time: 1107.832727432251 and batch: 300, loss is 5.563369379043579 and perplexity is 260.6997544478606
At time: 1110.305391550064 and batch: 350, loss is 5.518696327209472 and perplexity is 249.30980682719678
At time: 1112.7774879932404 and batch: 400, loss is 5.530727062225342 and perplexity is 252.32730199681382
At time: 1115.24951338768 and batch: 450, loss is 5.5013188457489015 and perplexity is 245.01485607541878
At time: 1117.7246499061584 and batch: 500, loss is 5.494155769348144 and perplexity is 243.26606677518942
At time: 1120.2011232376099 and batch: 550, loss is 5.501135091781617 and perplexity is 244.96983775984492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.337623596191406 and perplexity of 208.01778819051933
Finished 39 epochs...
Completing Train Step...
At time: 1124.286987543106 and batch: 50, loss is 5.566027479171753 and perplexity is 261.3936423017148
At time: 1126.786217212677 and batch: 100, loss is 5.553939771652222 and perplexity is 258.2530121558177
At time: 1129.25195646286 and batch: 150, loss is 5.576515159606934 and perplexity is 264.14948120901363
At time: 1131.726991891861 and batch: 200, loss is 5.592752056121826 and perplexity is 268.47345798539004
At time: 1134.1976110935211 and batch: 250, loss is 5.550865821838379 and perplexity is 257.46037424590196
At time: 1136.6978940963745 and batch: 300, loss is 5.559129209518432 and perplexity is 259.59668355031835
At time: 1139.1691899299622 and batch: 350, loss is 5.516124086380005 and perplexity is 248.6693460257308
At time: 1141.6401679515839 and batch: 400, loss is 5.529350910186768 and perplexity is 251.98030008424018
At time: 1144.1106495857239 and batch: 450, loss is 5.5001951026916505 and perplexity is 244.73967697623976
At time: 1146.5799732208252 and batch: 500, loss is 5.493630228042602 and perplexity is 243.13825399725107
At time: 1149.04989528656 and batch: 550, loss is 5.49813081741333 and perplexity is 244.23498555678935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.3342641194661455 and perplexity of 207.32012981234735
Finished 40 epochs...
Completing Train Step...
At time: 1153.112541437149 and batch: 50, loss is 5.56111361503601 and perplexity is 260.1123401081443
At time: 1155.5790269374847 and batch: 100, loss is 5.548060207366944 and perplexity is 256.73905204325735
At time: 1158.0479516983032 and batch: 150, loss is 5.571304244995117 and perplexity is 262.77660090525114
At time: 1160.5222733020782 and batch: 200, loss is 5.588705186843872 and perplexity is 267.3891764489286
At time: 1162.996838569641 and batch: 250, loss is 5.548997955322266 and perplexity is 256.97992148412396
At time: 1165.471756219864 and batch: 300, loss is 5.557580738067627 and perplexity is 259.19501656230733
At time: 1167.9418270587921 and batch: 350, loss is 5.51511248588562 and perplexity is 248.41791918555185
At time: 1170.41033244133 and batch: 400, loss is 5.528569469451904 and perplexity is 251.78346932917236
At time: 1172.8770520687103 and batch: 450, loss is 5.499298305511474 and perplexity is 244.52029350999038
At time: 1175.3442463874817 and batch: 500, loss is 5.4920347213745115 and perplexity is 242.75063459862648
At time: 1177.812905550003 and batch: 550, loss is 5.494644050598144 and perplexity is 243.38487803865252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.332120259602864 and perplexity of 206.87614060256854
Finished 41 epochs...
Completing Train Step...
At time: 1181.855107307434 and batch: 50, loss is 5.556586675643921 and perplexity is 258.9374885565941
At time: 1184.3559131622314 and batch: 100, loss is 5.5436202335357665 and perplexity is 255.60166422529596
At time: 1186.830818414688 and batch: 150, loss is 5.567677097320557 and perplexity is 261.825197851107
At time: 1189.2987117767334 and batch: 200, loss is 5.58623779296875 and perplexity is 266.73023530066575
At time: 1191.7712111473083 and batch: 250, loss is 5.54691367149353 and perplexity is 256.44486019296596
At time: 1194.2430348396301 and batch: 300, loss is 5.5556673908233645 and perplexity is 258.69956063241136
At time: 1196.7161729335785 and batch: 350, loss is 5.513606100082398 and perplexity is 248.04398767209673
At time: 1199.2379879951477 and batch: 400, loss is 5.527693634033203 and perplexity is 251.56304499070222
At time: 1201.7148325443268 and batch: 450, loss is 5.498467054367065 and perplexity is 244.31712019190456
At time: 1204.1897225379944 and batch: 500, loss is 5.491225671768189 and perplexity is 242.55431671942978
At time: 1206.660898923874 and batch: 550, loss is 5.491978282928467 and perplexity is 242.7369345166427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.330546061197917 and perplexity of 206.55073270750447
Finished 42 epochs...
Completing Train Step...
At time: 1210.7673349380493 and batch: 50, loss is 5.553305139541626 and perplexity is 258.08916849736806
At time: 1213.2480227947235 and batch: 100, loss is 5.540507459640503 and perplexity is 254.80727106189053
At time: 1215.7230241298676 and batch: 150, loss is 5.564715938568115 and perplexity is 261.0510386447174
At time: 1218.1949462890625 and batch: 200, loss is 5.583707799911499 and perplexity is 266.0562625898365
At time: 1220.6669552326202 and batch: 250, loss is 5.544883165359497 and perplexity is 255.92467562900168
At time: 1223.137279033661 and batch: 300, loss is 5.553821620941162 and perplexity is 258.2225011812803
At time: 1225.6110758781433 and batch: 350, loss is 5.5119583034515385 and perplexity is 247.63559818871923
At time: 1228.078558921814 and batch: 400, loss is 5.526583099365235 and perplexity is 251.28383057540034
At time: 1230.551383972168 and batch: 450, loss is 5.49774338722229 and perplexity is 244.140379877408
At time: 1233.0225653648376 and batch: 500, loss is 5.490042238235474 and perplexity is 242.26743959100736
At time: 1235.4929659366608 and batch: 550, loss is 5.489420700073242 and perplexity is 242.11690791727128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.328603108723958 and perplexity of 206.14980406916487
Finished 43 epochs...
Completing Train Step...
At time: 1239.5744721889496 and batch: 50, loss is 5.550709619522094 and perplexity is 257.4201614798387
At time: 1242.072514295578 and batch: 100, loss is 5.538054151535034 and perplexity is 254.1829164986685
At time: 1244.5352337360382 and batch: 150, loss is 5.5620826148986815 and perplexity is 260.3645110870786
At time: 1247.0018646717072 and batch: 200, loss is 5.5816286277771 and perplexity is 265.5036604989845
At time: 1249.471342086792 and batch: 250, loss is 5.543048028945923 and perplexity is 255.4554496161792
At time: 1251.9404108524323 and batch: 300, loss is 5.551504096984863 and perplexity is 257.62475725920297
At time: 1254.4133398532867 and batch: 350, loss is 5.50979063987732 and perplexity is 247.09938889348453
At time: 1256.8826863765717 and batch: 400, loss is 5.524681787490845 and perplexity is 250.80651555063804
At time: 1259.3473722934723 and batch: 450, loss is 5.495797567367553 and perplexity is 243.66578856324276
At time: 1261.8685812950134 and batch: 500, loss is 5.487642288208008 and perplexity is 241.6867069860624
At time: 1264.3358325958252 and batch: 550, loss is 5.486241426467895 and perplexity is 241.3483743590972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.326490275065104 and perplexity of 205.71470363364364
Finished 44 epochs...
Completing Train Step...
At time: 1268.4238781929016 and batch: 50, loss is 5.547333879470825 and perplexity is 256.5526430129697
At time: 1270.8929345607758 and batch: 100, loss is 5.534635152816772 and perplexity is 253.31534938350532
At time: 1273.36123585701 and batch: 150, loss is 5.558846950531006 and perplexity is 259.5234203933588
At time: 1275.836107492447 and batch: 200, loss is 5.579142341613769 and perplexity is 264.8443623631161
At time: 1278.3014969825745 and batch: 250, loss is 5.540186796188355 and perplexity is 254.72557678160425
At time: 1280.7705047130585 and batch: 300, loss is 5.548188219070434 and perplexity is 256.7719197503427
At time: 1283.2374594211578 and batch: 350, loss is 5.506950206756592 and perplexity is 246.3985154684289
At time: 1285.7014241218567 and batch: 400, loss is 5.522342290878296 and perplexity is 250.22044038484557
At time: 1288.1742205619812 and batch: 450, loss is 5.49339732170105 and perplexity is 243.08163215007073
At time: 1290.6497383117676 and batch: 500, loss is 5.48497314453125 and perplexity is 241.04247060256807
At time: 1293.1192524433136 and batch: 550, loss is 5.482965784072876 and perplexity is 240.55909679334005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.323788960774739 and perplexity of 205.1597534498714
Finished 45 epochs...
Completing Train Step...
At time: 1297.2094695568085 and batch: 50, loss is 5.543950452804565 and perplexity is 255.68608275750603
At time: 1299.703869819641 and batch: 100, loss is 5.531107158660888 and perplexity is 252.4232289344825
At time: 1302.1755545139313 and batch: 150, loss is 5.554980354309082 and perplexity is 258.52188562962425
At time: 1304.638617515564 and batch: 200, loss is 5.576089715957641 and perplexity is 264.03712439221124
At time: 1307.1038672924042 and batch: 250, loss is 5.537831649780274 and perplexity is 254.12636664517157
At time: 1309.5687792301178 and batch: 300, loss is 5.546470260620117 and perplexity is 256.33117496002234
At time: 1312.0418899059296 and batch: 350, loss is 5.505465955734253 and perplexity is 246.03306949381252
At time: 1314.507761001587 and batch: 400, loss is 5.520764436721802 and perplexity is 249.82594033656142
At time: 1316.9617328643799 and batch: 450, loss is 5.491978626251221 and perplexity is 242.7370178537699
At time: 1319.4126188755035 and batch: 500, loss is 5.482991704940796 and perplexity is 240.56533237473036
At time: 1321.8766915798187 and batch: 550, loss is 5.480811834335327 and perplexity is 240.0415022263928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.323785400390625 and perplexity of 205.1590230036446
Finished 46 epochs...
Completing Train Step...
At time: 1325.9235212802887 and batch: 50, loss is 5.542016372680664 and perplexity is 255.19204329684038
At time: 1328.389526128769 and batch: 100, loss is 5.528790512084961 and perplexity is 251.83913036169704
At time: 1330.8574965000153 and batch: 150, loss is 5.552784633636475 and perplexity is 257.9548665166288
At time: 1333.329425573349 and batch: 200, loss is 5.5738468265533445 and perplexity is 263.4455819536868
At time: 1335.798397064209 and batch: 250, loss is 5.535983219146728 and perplexity is 253.65706555315202
At time: 1338.26726770401 and batch: 300, loss is 5.544882020950317 and perplexity is 255.92438274662115
At time: 1340.7362031936646 and batch: 350, loss is 5.50404408454895 and perplexity is 245.68349074853606
At time: 1343.2035567760468 and batch: 400, loss is 5.519541215896607 and perplexity is 249.52053487091138
At time: 1345.6635403633118 and batch: 450, loss is 5.49077335357666 and perplexity is 242.44462979810888
At time: 1348.1366457939148 and batch: 500, loss is 5.4815857791900635 and perplexity is 240.22735302182062
At time: 1350.6080420017242 and batch: 550, loss is 5.479195623397827 and perplexity is 239.65385786694912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.323509216308594 and perplexity of 205.10236917100949
Finished 47 epochs...
Completing Train Step...
At time: 1354.6706874370575 and batch: 50, loss is 5.5398594093322755 and perplexity is 254.6421966253871
At time: 1357.1651735305786 and batch: 100, loss is 5.526953048706055 and perplexity is 251.37681006066967
At time: 1359.6298372745514 and batch: 150, loss is 5.550926942825317 and perplexity is 257.4761109589902
At time: 1362.0955111980438 and batch: 200, loss is 5.572055349349975 and perplexity is 262.97404769683584
At time: 1364.5595479011536 and batch: 250, loss is 5.534522571563721 and perplexity is 253.28683242932186
At time: 1367.0177607536316 and batch: 300, loss is 5.543743495941162 and perplexity is 255.6331722430886
At time: 1369.4776995182037 and batch: 350, loss is 5.5024145889282225 and perplexity is 245.28347657545945
At time: 1371.9304645061493 and batch: 400, loss is 5.517762145996094 and perplexity is 249.07701504113405
At time: 1374.3918612003326 and batch: 450, loss is 5.488719415664673 and perplexity is 241.94717462730912
At time: 1376.846402168274 and batch: 500, loss is 5.479874849319458 and perplexity is 239.81669227384592
At time: 1379.3084688186646 and batch: 550, loss is 5.476952743530274 and perplexity is 239.11694539412434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.322096761067709 and perplexity of 204.81287575113396
Finished 48 epochs...
Completing Train Step...
At time: 1383.3770368099213 and batch: 50, loss is 5.536962013244629 and perplexity is 253.9054651379988
At time: 1385.845763206482 and batch: 100, loss is 5.524499540328979 and perplexity is 250.7608109398958
At time: 1388.3367049694061 and batch: 150, loss is 5.547998390197754 and perplexity is 256.72318165237596
At time: 1390.798496723175 and batch: 200, loss is 5.569245796203614 and perplexity is 262.23624506689373
At time: 1393.2716310024261 and batch: 250, loss is 5.532358675003052 and perplexity is 252.7393384975177
At time: 1395.7363555431366 and batch: 300, loss is 5.5415959072113035 and perplexity is 255.0847664092723
At time: 1398.2161338329315 and batch: 350, loss is 5.499573621749878 and perplexity is 244.5876231854641
At time: 1400.690194606781 and batch: 400, loss is 5.513620491027832 and perplexity is 248.04755728527365
At time: 1403.160549879074 and batch: 450, loss is 5.484833984375 and perplexity is 241.0089294285478
At time: 1405.6403110027313 and batch: 500, loss is 5.477482957839966 and perplexity is 239.2437622373437
At time: 1408.110068321228 and batch: 550, loss is 5.473938264846802 and perplexity is 238.39721780613135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.319089253743489 and perplexity of 204.1978248758238
Finished 49 epochs...
Completing Train Step...
At time: 1412.2052102088928 and batch: 50, loss is 5.533493337631225 and perplexity is 253.02627513693582
At time: 1414.7299389839172 and batch: 100, loss is 5.521560096740723 and perplexity is 250.02479594921604
At time: 1417.2078156471252 and batch: 150, loss is 5.544858665466308 and perplexity is 255.91840557859248
At time: 1419.682153224945 and batch: 200, loss is 5.56661135673523 and perplexity is 261.54630874960986
At time: 1422.1577806472778 and batch: 250, loss is 5.529950733184815 and perplexity is 252.13148900204743
At time: 1424.6314220428467 and batch: 300, loss is 5.539121074676514 and perplexity is 254.45425485729743
At time: 1427.1112995147705 and batch: 350, loss is 5.496858825683594 and perplexity is 243.92451817288173
At time: 1429.5846099853516 and batch: 400, loss is 5.511651906967163 and perplexity is 247.55973513470806
At time: 1432.0661573410034 and batch: 450, loss is 5.4836256885528565 and perplexity is 240.7178952091443
At time: 1434.5492429733276 and batch: 500, loss is 5.475722856521607 and perplexity is 238.82303934209696
At time: 1437.0216805934906 and batch: 550, loss is 5.471126546859741 and perplexity is 237.7278535335292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.31637929280599 and perplexity of 203.64520587304102
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fae90a11898>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 4.005848223882902, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 8.63971982266622, 'dropout': 0.09810637284736412, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9901654720306396 and batch: 50, loss is 7.399228839874268 and perplexity is 1634.7233103622802
At time: 5.435528516769409 and batch: 100, loss is 6.50966796875 and perplexity is 671.6033872544477
At time: 7.883825778961182 and batch: 150, loss is 6.074135370254517 and perplexity is 434.47368119820254
At time: 10.326903343200684 and batch: 200, loss is 5.788628520965577 and perplexity is 326.5648402768549
At time: 12.802989721298218 and batch: 250, loss is 5.589852123260498 and perplexity is 267.69603077036953
At time: 15.249551773071289 and batch: 300, loss is 5.504815740585327 and perplexity is 245.87314706251905
At time: 17.7069993019104 and batch: 350, loss is 5.409634790420532 and perplexity is 223.5499301944092
At time: 20.16672921180725 and batch: 400, loss is 5.375649299621582 and perplexity is 216.0801275168885
At time: 22.621263027191162 and batch: 450, loss is 5.287543020248413 and perplexity is 197.8566978140674
At time: 25.077341079711914 and batch: 500, loss is 5.262308263778687 and perplexity is 192.92630252499822
At time: 27.5295090675354 and batch: 550, loss is 5.241151390075683 and perplexity is 188.88746039705845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.95093994140625 and perplexity of 141.30772249846
Finished 1 epochs...
Completing Train Step...
At time: 31.6127610206604 and batch: 50, loss is 5.181431636810303 and perplexity is 177.93737042098576
At time: 34.07571077346802 and batch: 100, loss is 5.129491386413574 and perplexity is 168.93117549993667
At time: 36.5356240272522 and batch: 150, loss is 5.076067781448364 and perplexity is 160.14309854742922
At time: 39.00240349769592 and batch: 200, loss is 5.040015735626221 and perplexity is 154.47244572743386
At time: 41.468175649642944 and batch: 250, loss is 4.994114608764648 and perplexity is 147.54225491656615
At time: 43.94771480560303 and batch: 300, loss is 4.969464931488037 and perplexity is 143.94984373226208
At time: 46.40550208091736 and batch: 350, loss is 4.903858470916748 and perplexity is 134.80893382074512
At time: 48.87200975418091 and batch: 400, loss is 4.881126623153687 and perplexity is 131.77904560894677
At time: 51.34243083000183 and batch: 450, loss is 4.814730758666992 and perplexity is 123.3136067203691
At time: 53.812172651290894 and batch: 500, loss is 4.82149260520935 and perplexity is 124.15025987790193
At time: 56.28014278411865 and batch: 550, loss is 4.811247158050537 and perplexity is 122.88477873037496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.738242594401042 and perplexity of 114.23327098349343
Finished 2 epochs...
Completing Train Step...
At time: 60.392313957214355 and batch: 50, loss is 4.8421632766723635 and perplexity is 126.74323606546874
At time: 62.86197876930237 and batch: 100, loss is 4.8176625537872315 and perplexity is 123.67566743539103
At time: 65.34095573425293 and batch: 150, loss is 4.787185621261597 and perplexity is 119.96327103959553
At time: 67.81292796134949 and batch: 200, loss is 4.76782530784607 and perplexity is 117.66308254527105
At time: 70.28749370574951 and batch: 250, loss is 4.754097099304199 and perplexity is 116.05881625691079
At time: 72.7478666305542 and batch: 300, loss is 4.743096866607666 and perplexity is 114.78913845036179
At time: 75.21010184288025 and batch: 350, loss is 4.6912490749359135 and perplexity is 108.98923053709741
At time: 77.68476033210754 and batch: 400, loss is 4.67077338218689 and perplexity is 106.78029248308421
At time: 80.16769576072693 and batch: 450, loss is 4.616499605178833 and perplexity is 101.13938401143625
At time: 82.64109706878662 and batch: 500, loss is 4.623029565811157 and perplexity is 101.80198121980285
At time: 85.1023941040039 and batch: 550, loss is 4.6211686706542965 and perplexity is 101.61271456331245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.6957758585611975 and perplexity of 109.48371957926183
Finished 3 epochs...
Completing Train Step...
At time: 89.19415998458862 and batch: 50, loss is 4.661983871459961 and perplexity is 105.84585858112072
At time: 91.70037722587585 and batch: 100, loss is 4.6516838550567625 and perplexity is 104.76123988564852
At time: 94.17880821228027 and batch: 150, loss is 4.632888984680176 and perplexity is 102.81065388658708
At time: 96.65984201431274 and batch: 200, loss is 4.617615089416504 and perplexity is 101.25226634763712
At time: 99.1353051662445 and batch: 250, loss is 4.605152378082275 and perplexity is 99.99821922527437
At time: 101.61251044273376 and batch: 300, loss is 4.612226419448852 and perplexity is 100.70811873350124
At time: 104.08222246170044 and batch: 350, loss is 4.562686958312988 and perplexity is 95.84065400920164
At time: 106.55408215522766 and batch: 400, loss is 4.539475593566895 and perplexity is 93.64168094150065
At time: 109.02792167663574 and batch: 450, loss is 4.482592782974243 and perplexity is 88.463742869024
At time: 111.49928712844849 and batch: 500, loss is 4.490597848892212 and perplexity is 89.17474296656454
At time: 113.9747166633606 and batch: 550, loss is 4.498990726470947 and perplexity is 89.92632522453151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.666765340169271 and perplexity of 106.35316912014103
Finished 4 epochs...
Completing Train Step...
At time: 118.08260893821716 and batch: 50, loss is 4.557839336395264 and perplexity is 95.37717903754948
At time: 120.5478196144104 and batch: 100, loss is 4.543797903060913 and perplexity is 94.04730525347584
At time: 123.01352906227112 and batch: 150, loss is 4.528391284942627 and perplexity is 92.60945895177568
At time: 125.48372411727905 and batch: 200, loss is 4.516368370056153 and perplexity is 91.50268993592184
At time: 127.95070099830627 and batch: 250, loss is 4.502364683151245 and perplexity is 90.2302451683116
At time: 130.41675353050232 and batch: 300, loss is 4.509286918640137 and perplexity is 90.85700696724284
At time: 132.89158391952515 and batch: 350, loss is 4.470285472869873 and perplexity is 87.38166454775863
At time: 135.36345434188843 and batch: 400, loss is 4.448987312316895 and perplexity is 85.5402745426216
At time: 137.84166026115417 and batch: 450, loss is 4.392904062271118 and perplexity is 80.87494415179233
At time: 140.3105652332306 and batch: 500, loss is 4.3984055519104 and perplexity is 81.3211029628585
At time: 142.78378224372864 and batch: 550, loss is 4.404894533157349 and perplexity is 81.85050987308388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.646026611328125 and perplexity of 104.17025327213813
Finished 5 epochs...
Completing Train Step...
At time: 146.868066072464 and batch: 50, loss is 4.468013324737549 and perplexity is 87.18334585190523
At time: 149.36607694625854 and batch: 100, loss is 4.457904253005982 and perplexity is 86.30644295979884
At time: 151.83582067489624 and batch: 150, loss is 4.4448271751403805 and perplexity is 85.18515445271582
At time: 154.30990743637085 and batch: 200, loss is 4.438868999481201 and perplexity is 84.67911537082774
At time: 156.78464221954346 and batch: 250, loss is 4.42714451789856 and perplexity is 83.69209410514169
At time: 159.25391268730164 and batch: 300, loss is 4.434866247177124 and perplexity is 84.34084330795964
At time: 161.72318196296692 and batch: 350, loss is 4.392051134109497 and perplexity is 80.80599304370679
At time: 164.19645762443542 and batch: 400, loss is 4.3806476974487305 and perplexity is 79.88976104575474
At time: 166.67614126205444 and batch: 450, loss is 4.310084600448608 and perplexity is 74.44678690550599
At time: 169.14774370193481 and batch: 500, loss is 4.327123184204101 and perplexity is 75.7261228049199
At time: 171.6207983493805 and batch: 550, loss is 4.333913202285767 and perplexity is 76.24205415740427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.644510396321615 and perplexity of 104.01242844931274
Finished 6 epochs...
Completing Train Step...
At time: 175.69487500190735 and batch: 50, loss is 4.406418447494507 and perplexity is 81.97533812820943
At time: 178.16156029701233 and batch: 100, loss is 4.391102027893067 and perplexity is 80.72933595699733
At time: 180.63110089302063 and batch: 150, loss is 4.384137048721313 and perplexity is 80.1690114030849
At time: 183.09676885604858 and batch: 200, loss is 4.3786460876464846 and perplexity is 79.73001284706181
At time: 185.5658473968506 and batch: 250, loss is 4.36678638458252 and perplexity is 78.79002358381423
At time: 188.03851175308228 and batch: 300, loss is 4.380884504318237 and perplexity is 79.9086817301592
At time: 190.50731587409973 and batch: 350, loss is 4.341896858215332 and perplexity is 76.85318075135375
At time: 192.96920800209045 and batch: 400, loss is 4.331203870773315 and perplexity is 76.03576873149018
At time: 195.43130946159363 and batch: 450, loss is 4.265195322036743 and perplexity is 71.17882140809293
At time: 197.89787125587463 and batch: 500, loss is 4.2714829254150395 and perplexity is 71.6277755497537
At time: 200.35960340499878 and batch: 550, loss is 4.284674596786499 and perplexity is 72.5789254611686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.650371805826823 and perplexity of 104.6238781139378
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 204.41937041282654 and batch: 50, loss is 4.339100160598755 and perplexity is 76.63854591818075
At time: 206.90875601768494 and batch: 100, loss is 4.290975675582886 and perplexity is 73.03769484256887
At time: 209.3772234916687 and batch: 150, loss is 4.260686130523681 and perplexity is 70.85858501690703
At time: 211.8484766483307 and batch: 200, loss is 4.229526815414428 and perplexity is 68.68472393062501
At time: 214.3199508190155 and batch: 250, loss is 4.204157028198242 and perplexity is 66.96412499811262
At time: 216.7848777770996 and batch: 300, loss is 4.187099132537842 and perplexity is 65.83154511788457
At time: 219.2612340450287 and batch: 350, loss is 4.115173397064209 and perplexity is 61.26283613775576
At time: 221.73317313194275 and batch: 400, loss is 4.0709833240509035 and perplexity is 58.6145713809688
At time: 224.20092940330505 and batch: 450, loss is 3.9952489042282107 and perplexity is 54.339364238217826
At time: 226.66950726509094 and batch: 500, loss is 3.979712657928467 and perplexity is 53.50165874113177
At time: 229.13747692108154 and batch: 550, loss is 4.023528237342834 and perplexity is 55.897979652972275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.502617899576823 and perplexity of 90.25309584143291
Finished 8 epochs...
Completing Train Step...
At time: 233.17524814605713 and batch: 50, loss is 4.207515144348145 and perplexity is 67.18937630608158
At time: 235.63744187355042 and batch: 100, loss is 4.17895706653595 and perplexity is 65.29771651548442
At time: 238.1035988330841 and batch: 150, loss is 4.165774297714234 and perplexity is 64.44256085817099
At time: 240.56523823738098 and batch: 200, loss is 4.143945107460022 and perplexity is 63.051074699504454
At time: 243.02942490577698 and batch: 250, loss is 4.130022368431091 and perplexity is 62.17931377290074
At time: 245.5031406879425 and batch: 300, loss is 4.1250354671478275 and perplexity is 61.870003564017125
At time: 247.96751236915588 and batch: 350, loss is 4.06798508644104 and perplexity is 58.43909416122164
At time: 250.43376564979553 and batch: 400, loss is 4.0366150712966915 and perplexity is 56.634314870387904
At time: 252.9093098640442 and batch: 450, loss is 3.9709498119354247 and perplexity is 53.034880086833326
At time: 255.37465000152588 and batch: 500, loss is 3.967410197257996 and perplexity is 52.847488888829716
At time: 257.84047532081604 and batch: 550, loss is 4.001611661911011 and perplexity is 54.68621473816771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.495707702636719 and perplexity of 89.63157904959064
Finished 9 epochs...
Completing Train Step...
At time: 261.9196684360504 and batch: 50, loss is 4.156802482604981 and perplexity is 63.86697998046214
At time: 264.4080729484558 and batch: 100, loss is 4.130347127914429 and perplexity is 62.19951037405728
At time: 266.8671872615814 and batch: 150, loss is 4.123419976234436 and perplexity is 61.77013382649478
At time: 269.35242795944214 and batch: 200, loss is 4.104906497001648 and perplexity is 60.63707453424824
At time: 271.81586718559265 and batch: 250, loss is 4.095585823059082 and perplexity is 60.07452189120324
At time: 274.2817575931549 and batch: 300, loss is 4.094130291938781 and perplexity is 59.9871451602551
At time: 276.7562415599823 and batch: 350, loss is 4.042428350448608 and perplexity is 56.96450476544842
At time: 279.23181319236755 and batch: 400, loss is 4.015657858848572 and perplexity is 55.459768103420544
At time: 281.7006616592407 and batch: 450, loss is 3.953530592918396 and perplexity is 52.11905354388403
At time: 284.16726088523865 and batch: 500, loss is 3.953847231864929 and perplexity is 52.13555907910221
At time: 286.6312894821167 and batch: 550, loss is 3.980844326019287 and perplexity is 53.56223913311801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.49540049235026 and perplexity of 89.60404753571373
Finished 10 epochs...
Completing Train Step...
At time: 290.71150493621826 and batch: 50, loss is 4.121029314994812 and perplexity is 61.62263873743335
At time: 293.1798462867737 and batch: 100, loss is 4.095769248008728 and perplexity is 60.08554206801293
At time: 295.65699338912964 and batch: 150, loss is 4.093263306617737 and perplexity is 59.93515972451094
At time: 298.1320185661316 and batch: 200, loss is 4.076606702804566 and perplexity is 58.945111822335754
At time: 300.6088526248932 and batch: 250, loss is 4.070331015586853 and perplexity is 58.576349067664864
At time: 303.0804181098938 and batch: 300, loss is 4.07003237247467 and perplexity is 58.55885825636414
At time: 305.5505037307739 and batch: 350, loss is 4.022303128242493 and perplexity is 55.829540460707356
At time: 308.0199935436249 and batch: 400, loss is 3.997569785118103 and perplexity is 54.46562589270214
At time: 310.48769211769104 and batch: 450, loss is 3.9377654790878296 and perplexity is 51.303833633775746
At time: 312.95200514793396 and batch: 500, loss is 3.939224376678467 and perplexity is 51.37873529669736
At time: 315.4116270542145 and batch: 550, loss is 3.9615419816970827 and perplexity is 52.538276581747674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.495839436848958 and perplexity of 89.64338737281149
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 319.47622060775757 and batch: 50, loss is 4.106611123085022 and perplexity is 60.74052622129051
At time: 321.9686541557312 and batch: 100, loss is 4.084335503578186 and perplexity is 59.402451916853224
At time: 324.43271923065186 and batch: 150, loss is 4.070726771354675 and perplexity is 58.59953558346239
At time: 326.90058064460754 and batch: 200, loss is 4.048734412193299 and perplexity is 57.324861471400254
At time: 329.36669087409973 and batch: 250, loss is 4.033720760345459 and perplexity is 56.47063453779555
At time: 331.85769724845886 and batch: 300, loss is 4.0222464227676396 and perplexity is 55.82637470986323
At time: 334.32281732559204 and batch: 350, loss is 3.962155385017395 and perplexity is 52.570513621189114
At time: 336.78795886039734 and batch: 400, loss is 3.926806993484497 and perplexity is 50.74469058755395
At time: 339.25403022766113 and batch: 450, loss is 3.8592823266983034 and perplexity is 47.4312989741293
At time: 341.7180516719818 and batch: 500, loss is 3.8479827213287354 and perplexity is 46.89836068025859
At time: 344.1814272403717 and batch: 550, loss is 3.890378041267395 and perplexity is 48.929380352757754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.467318725585938 and perplexity of 87.12280940055966
Finished 12 epochs...
Completing Train Step...
At time: 348.2546970844269 and batch: 50, loss is 4.0769405174255375 and perplexity is 58.96479184705412
At time: 350.7296826839447 and batch: 100, loss is 4.052215614318848 and perplexity is 57.52476865777666
At time: 353.2029802799225 and batch: 150, loss is 4.043529658317566 and perplexity is 57.027274781009574
At time: 355.68186926841736 and batch: 200, loss is 4.025950498580933 and perplexity is 56.033543281351605
At time: 358.15137934684753 and batch: 250, loss is 4.015821647644043 and perplexity is 55.468852535979146
At time: 360.6275939941406 and batch: 300, loss is 4.008351922035217 and perplexity is 55.05605907402368
At time: 363.10110807418823 and batch: 350, loss is 3.9513209581375124 and perplexity is 52.00401661203509
At time: 365.5672607421875 and batch: 400, loss is 3.9217511558532716 and perplexity is 50.48878113473614
At time: 368.0319187641144 and batch: 450, loss is 3.859276270866394 and perplexity is 47.4310117390252
At time: 370.50572443008423 and batch: 500, loss is 3.851379494667053 and perplexity is 47.0579346462935
At time: 372.96804022789 and batch: 550, loss is 3.8911563634872435 and perplexity is 48.96747800088951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.464928690592448 and perplexity of 86.9148314736249
Finished 13 epochs...
Completing Train Step...
At time: 377.0108160972595 and batch: 50, loss is 4.063041400909424 and perplexity is 58.150902608092345
At time: 379.5017638206482 and batch: 100, loss is 4.037642903327942 and perplexity is 56.69255535887212
At time: 381.96629762649536 and batch: 150, loss is 4.0304387664794925 and perplexity is 56.28560206544438
At time: 384.4242060184479 and batch: 200, loss is 4.014471907615661 and perplexity is 55.39403450918434
At time: 386.89125657081604 and batch: 250, loss is 4.006808505058289 and perplexity is 54.971150159551485
At time: 389.35876059532166 and batch: 300, loss is 4.000518846511841 and perplexity is 54.62648544307053
At time: 391.8202362060547 and batch: 350, loss is 3.944801468849182 and perplexity is 51.666079767565755
At time: 394.3267056941986 and batch: 400, loss is 3.918557767868042 and perplexity is 50.3278080292587
At time: 396.7921733856201 and batch: 450, loss is 3.858175711631775 and perplexity is 47.37883981546064
At time: 399.2591361999512 and batch: 500, loss is 3.8513006734848023 and perplexity is 47.05422563042683
At time: 401.7254922389984 and batch: 550, loss is 3.8887643671035765 and perplexity is 48.85048794626918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.463969421386719 and perplexity of 86.83149672888871
Finished 14 epochs...
Completing Train Step...
At time: 405.80869936943054 and batch: 50, loss is 4.051983861923218 and perplexity is 57.511438699516724
At time: 408.2769470214844 and batch: 100, loss is 4.026793351173401 and perplexity is 56.080791207292826
At time: 410.74170303344727 and batch: 150, loss is 4.0208318901062015 and perplexity is 55.74746230471169
At time: 413.2074909210205 and batch: 200, loss is 4.005950651168823 and perplexity is 54.924013165796154
At time: 415.66324400901794 and batch: 250, loss is 3.999797234535217 and perplexity is 54.587080536170255
At time: 418.12745809555054 and batch: 300, loss is 3.994268898963928 and perplexity is 54.286137460727645
At time: 420.5906240940094 and batch: 350, loss is 3.9394389820098876 and perplexity is 51.389762630433665
At time: 423.0538160800934 and batch: 400, loss is 3.9150995302200315 and perplexity is 50.15406310756192
At time: 425.5195505619049 and batch: 450, loss is 3.856267628669739 and perplexity is 47.288523251597965
At time: 427.9830358028412 and batch: 500, loss is 3.8497275924682617 and perplexity is 46.980263710637644
At time: 430.4388642311096 and batch: 550, loss is 3.8854581356048583 and perplexity is 48.689243626700005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.463614908854167 and perplexity of 86.80071933088706
Finished 15 epochs...
Completing Train Step...
At time: 434.4825105667114 and batch: 50, loss is 4.0425133752822875 and perplexity is 56.969348368902104
At time: 436.9711003303528 and batch: 100, loss is 4.017696342468262 and perplexity is 55.572937239777794
At time: 439.4434292316437 and batch: 150, loss is 4.012875456809997 and perplexity is 55.30567121075994
At time: 441.9037706851959 and batch: 200, loss is 3.9987463617324828 and perplexity is 54.5297465884736
At time: 444.37017273902893 and batch: 250, loss is 3.9936550760269167 and perplexity is 54.252825609227685
At time: 446.839154958725 and batch: 300, loss is 3.9887533617019653 and perplexity is 53.98754445414348
At time: 449.29644203186035 and batch: 350, loss is 3.934512982368469 and perplexity is 51.13723915405386
At time: 451.76596784591675 and batch: 400, loss is 3.911529293060303 and perplexity is 49.975320474402665
At time: 454.2340261936188 and batch: 450, loss is 3.85406138420105 and perplexity is 47.1843082129403
At time: 456.763201713562 and batch: 500, loss is 3.847536063194275 and perplexity is 46.87741782346475
At time: 459.2371027469635 and batch: 550, loss is 3.8815486907958983 and perplexity is 48.4992673088788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.46338857014974 and perplexity of 86.78107519172876
Finished 16 epochs...
Completing Train Step...
At time: 463.34963965415955 and batch: 50, loss is 4.03397008895874 and perplexity is 56.48471603818348
At time: 465.81207489967346 and batch: 100, loss is 4.0096314954757695 and perplexity is 55.12655243605489
At time: 468.27834272384644 and batch: 150, loss is 4.005804138183594 and perplexity is 54.9159666741386
At time: 470.74137711524963 and batch: 200, loss is 3.992276291847229 and perplexity is 54.17807421643538
At time: 473.20710706710815 and batch: 250, loss is 3.9880912590026854 and perplexity is 53.95181098615111
At time: 475.67183232307434 and batch: 300, loss is 3.983648347854614 and perplexity is 53.71263958572877
At time: 478.1387188434601 and batch: 350, loss is 3.929935584068298 and perplexity is 50.90369855441949
At time: 480.6067171096802 and batch: 400, loss is 3.9080382061004637 and perplexity is 49.80115647250664
At time: 483.077663898468 and batch: 450, loss is 3.8513951301574707 and perplexity is 47.05867042593186
At time: 485.5398778915405 and batch: 500, loss is 3.8449309062957764 and perplexity is 46.75545373181476
At time: 487.99818778038025 and batch: 550, loss is 3.877624077796936 and perplexity is 48.30929947291039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.463261413574219 and perplexity of 86.77004110893027
Finished 17 epochs...
Completing Train Step...
At time: 492.00918102264404 and batch: 50, loss is 4.0261881113052365 and perplexity is 56.04685914616987
At time: 494.49812364578247 and batch: 100, loss is 4.002264504432678 and perplexity is 54.72192788075842
At time: 496.97410345077515 and batch: 150, loss is 3.999295320510864 and perplexity is 54.55968938947603
At time: 499.4437029361725 and batch: 200, loss is 3.9863379621505737 and perplexity is 53.85730032261867
At time: 501.91590666770935 and batch: 250, loss is 3.983018479347229 and perplexity is 53.67881833819272
At time: 504.38198494911194 and batch: 300, loss is 3.978803334236145 and perplexity is 53.45303052800386
At time: 506.8436782360077 and batch: 350, loss is 3.9255092477798463 and perplexity is 50.67887959551251
At time: 509.31167697906494 and batch: 400, loss is 3.904501214027405 and perplexity is 49.62532132391029
At time: 511.7746744155884 and batch: 450, loss is 3.848594517707825 and perplexity is 46.92706170620466
At time: 514.2413041591644 and batch: 500, loss is 3.8420708274841306 and perplexity is 46.621920498079454
At time: 516.7175302505493 and batch: 550, loss is 3.8734662675857545 and perplexity is 48.10885556688902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4631708780924475 and perplexity of 86.76218569705729
Finished 18 epochs...
Completing Train Step...
At time: 520.7926931381226 and batch: 50, loss is 4.01889093875885 and perplexity is 55.63936413322219
At time: 523.3023715019226 and batch: 100, loss is 3.995405716896057 and perplexity is 54.34788600703629
At time: 525.7744867801666 and batch: 150, loss is 3.9932428073883055 and perplexity is 54.23046348059186
At time: 528.2461800575256 and batch: 200, loss is 3.98076632976532 and perplexity is 53.558061642027994
At time: 530.7242956161499 and batch: 250, loss is 3.9780324792861936 and perplexity is 53.411841872110664
At time: 533.2006206512451 and batch: 300, loss is 3.974147243499756 and perplexity is 53.20472687847649
At time: 535.6711368560791 and batch: 350, loss is 3.9213667201995848 and perplexity is 50.469375177566675
At time: 538.1444170475006 and batch: 400, loss is 3.9008648586273194 and perplexity is 49.44519372121027
At time: 540.6149351596832 and batch: 450, loss is 3.845705819129944 and perplexity is 46.79169917469456
At time: 543.0857591629028 and batch: 500, loss is 3.8391823101043703 and perplexity is 46.487446579067985
At time: 545.5563762187958 and batch: 550, loss is 3.869369468688965 and perplexity is 47.912166433487684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.463258870442709 and perplexity of 86.76982044158521
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 549.6011214256287 and batch: 50, loss is 4.018753499984741 and perplexity is 55.631717652697
At time: 552.071088552475 and batch: 100, loss is 4.003773102760315 and perplexity is 54.804543590936944
At time: 554.5455598831177 and batch: 150, loss is 3.9980517482757567 and perplexity is 54.49188264462625
At time: 557.0190887451172 and batch: 200, loss is 3.9801628255844115 and perplexity is 53.52574887933011
At time: 559.4972867965698 and batch: 250, loss is 3.9731760168075563 and perplexity is 53.15307811297071
At time: 561.9758439064026 and batch: 300, loss is 3.965888986587524 and perplexity is 52.76715784053936
At time: 564.4540629386902 and batch: 350, loss is 3.9095344829559324 and perplexity is 49.87572856664835
At time: 566.9329826831818 and batch: 400, loss is 3.8815250492095945 and perplexity is 48.498120722818655
At time: 569.4070293903351 and batch: 450, loss is 3.8223827266693116 and perplexity is 45.71300024348425
At time: 571.8809130191803 and batch: 500, loss is 3.814349179267883 and perplexity is 45.34723385711502
At time: 574.3570201396942 and batch: 550, loss is 3.8504814100265503 and perplexity is 47.015691609724314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.450411478678386 and perplexity of 85.66218491389746
Finished 20 epochs...
Completing Train Step...
At time: 578.4619424343109 and batch: 50, loss is 4.010659499168396 and perplexity is 55.18325187414349
At time: 580.9566037654877 and batch: 100, loss is 3.9936695528030395 and perplexity is 54.25361102092316
At time: 583.4291770458221 and batch: 150, loss is 3.9886288928985594 and perplexity is 53.98082510726966
At time: 585.9085655212402 and batch: 200, loss is 3.971993799209595 and perplexity is 53.090276738399304
At time: 588.385106086731 and batch: 250, loss is 3.9672532939910887 and perplexity is 52.83919759565772
At time: 590.8523333072662 and batch: 300, loss is 3.9618027639389037 and perplexity is 52.55197941794659
At time: 593.3212740421295 and batch: 350, loss is 3.9071304988861084 and perplexity is 49.75597211367996
At time: 595.7917003631592 and batch: 400, loss is 3.8803644704818727 and perplexity is 48.44186748504299
At time: 598.2557964324951 and batch: 450, loss is 3.822913718223572 and perplexity is 45.73727990611087
At time: 600.7257318496704 and batch: 500, loss is 3.8170790004730226 and perplexity is 45.47119281360888
At time: 603.1897222995758 and batch: 550, loss is 3.8520754098892214 and perplexity is 47.09069437701317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.449572245279948 and perplexity of 85.5903245053797
Finished 21 epochs...
Completing Train Step...
At time: 607.2507588863373 and batch: 50, loss is 4.007314457893371 and perplexity is 54.998970005994074
At time: 609.7187707424164 and batch: 100, loss is 3.990070605278015 and perplexity is 54.058706058555984
At time: 612.1867969036102 and batch: 150, loss is 3.9849461698532105 and perplexity is 53.782394285787035
At time: 614.660049200058 and batch: 200, loss is 3.96861120223999 and perplexity is 52.910997115485635
At time: 617.1355338096619 and batch: 250, loss is 3.964736542701721 and perplexity is 52.70638167939902
At time: 619.6067185401917 and batch: 300, loss is 3.9600801610946657 and perplexity is 52.46153115431504
At time: 622.0843975543976 and batch: 350, loss is 3.9061597871780394 and perplexity is 49.70769684347859
At time: 624.5557506084442 and batch: 400, loss is 3.879668140411377 and perplexity is 48.4081476974557
At time: 627.0262417793274 and batch: 450, loss is 3.823001322746277 and perplexity is 45.74128687419857
At time: 629.4984686374664 and batch: 500, loss is 3.818103051185608 and perplexity is 45.51778147151454
At time: 631.9772651195526 and batch: 550, loss is 3.852158269882202 and perplexity is 47.09459647328031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.449276733398437 and perplexity of 85.56503528436266
Finished 22 epochs...
Completing Train Step...
At time: 636.0903213024139 and batch: 50, loss is 4.004455213546753 and perplexity is 54.84193911376095
At time: 638.590085029602 and batch: 100, loss is 3.987410821914673 and perplexity is 53.91511265985776
At time: 641.0571899414062 and batch: 150, loss is 3.9822365760803224 and perplexity is 53.636863099377074
At time: 643.5530226230621 and batch: 200, loss is 3.9660639333724976 and perplexity is 52.77639009270862
At time: 646.0271091461182 and batch: 250, loss is 3.9628359842300416 and perplexity is 52.606305249863674
At time: 648.4865517616272 and batch: 300, loss is 3.958746085166931 and perplexity is 52.39159015214681
At time: 650.9452321529388 and batch: 350, loss is 3.9053753662109374 and perplexity is 49.66872037282719
At time: 653.4063658714294 and batch: 400, loss is 3.878941135406494 and perplexity is 48.37296752143408
At time: 655.8845703601837 and batch: 450, loss is 3.822764458656311 and perplexity is 45.730453688956175
At time: 658.3565168380737 and batch: 500, loss is 3.8184904718399046 and perplexity is 45.53541941662587
At time: 660.8252422809601 and batch: 550, loss is 3.851795234680176 and perplexity is 47.07750257996553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.449120585123698 and perplexity of 85.55167549480616
Finished 23 epochs...
Completing Train Step...
At time: 664.9376113414764 and batch: 50, loss is 4.001885328292847 and perplexity is 54.701182564693454
At time: 667.4087319374084 and batch: 100, loss is 3.9851584005355836 and perplexity is 53.793809771341266
At time: 669.8758835792542 and batch: 150, loss is 3.979944486618042 and perplexity is 53.51406339838998
At time: 672.3400187492371 and batch: 200, loss is 3.9638875913619995 and perplexity is 52.66165551391572
At time: 674.8033957481384 and batch: 250, loss is 3.961185073852539 and perplexity is 52.51952860454593
At time: 677.2688946723938 and batch: 300, loss is 3.9575527048110963 and perplexity is 52.32910434972867
At time: 679.7316970825195 and batch: 350, loss is 3.9045931625366213 and perplexity is 49.62988450801118
At time: 682.1937367916107 and batch: 400, loss is 3.878127512931824 and perplexity is 48.33362619455627
At time: 684.6593728065491 and batch: 450, loss is 3.8223509931564332 and perplexity is 45.71154963241894
At time: 687.1170651912689 and batch: 500, loss is 3.8185553312301637 and perplexity is 45.53837291194434
At time: 689.587512254715 and batch: 550, loss is 3.8512891244888308 and perplexity is 47.05368220450259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4490234375 and perplexity of 85.54336475651928
Finished 24 epochs...
Completing Train Step...
At time: 693.6243517398834 and batch: 50, loss is 3.999509563446045 and perplexity is 54.571379669708655
At time: 696.1382040977478 and batch: 100, loss is 3.983169198036194 and perplexity is 53.686909349035794
At time: 698.6155514717102 and batch: 150, loss is 3.977922420501709 and perplexity is 53.40596375319238
At time: 701.0861687660217 and batch: 200, loss is 3.961906223297119 and perplexity is 52.557416693273765
At time: 703.5531718730927 and batch: 250, loss is 3.9596745443344115 and perplexity is 52.44025619304237
At time: 706.0620818138123 and batch: 300, loss is 3.9564135313034057 and perplexity is 52.26952636164805
At time: 708.532059431076 and batch: 350, loss is 3.9037845087051393 and perplexity is 49.58976733438697
At time: 711.0058200359344 and batch: 400, loss is 3.8772592973709106 and perplexity is 48.29168039980954
At time: 713.4710433483124 and batch: 450, loss is 3.8218338918685912 and perplexity is 45.68791824167263
At time: 715.9404966831207 and batch: 500, loss is 3.8184504556655883 and perplexity is 45.533597299802246
At time: 718.4154496192932 and batch: 550, loss is 3.8507140636444093 and perplexity is 47.02663125299791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.448971048990885 and perplexity of 85.53888338456224
Finished 25 epochs...
Completing Train Step...
At time: 722.5268094539642 and batch: 50, loss is 3.997306160926819 and perplexity is 54.45126932857536
At time: 724.9949195384979 and batch: 100, loss is 3.9813411474227904 and perplexity is 53.58885661145789
At time: 727.462316274643 and batch: 150, loss is 3.9761035108566283 and perplexity is 53.30891142208502
At time: 729.9336400032043 and batch: 200, loss is 3.9600555896759033 and perplexity is 52.46024211590095
At time: 732.4020347595215 and batch: 250, loss is 3.9582546234130858 and perplexity is 52.365848015519845
At time: 734.8690936565399 and batch: 300, loss is 3.955320439338684 and perplexity is 52.21242217813031
At time: 737.3382709026337 and batch: 350, loss is 3.9029419898986815 and perplexity is 49.54800461820717
At time: 739.8095650672913 and batch: 400, loss is 3.8763614892959595 and perplexity is 48.24834319635027
At time: 742.2772989273071 and batch: 450, loss is 3.8212557554244997 and perplexity is 45.661512025016044
At time: 744.7501771450043 and batch: 500, loss is 3.8182478380203246 and perplexity is 45.524372324139954
At time: 747.2246174812317 and batch: 550, loss is 3.85004702091217 and perplexity is 46.99527294022895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.448929850260416 and perplexity of 85.53535936375415
Finished 26 epochs...
Completing Train Step...
At time: 751.3344449996948 and batch: 50, loss is 3.9951837682724 and perplexity is 54.33582490706013
At time: 753.8370027542114 and batch: 100, loss is 3.979383692741394 and perplexity is 53.48406145256163
At time: 756.3135840892792 and batch: 150, loss is 3.974334759712219 and perplexity is 53.214704562805444
At time: 758.7940044403076 and batch: 200, loss is 3.958304748535156 and perplexity is 52.36847292583037
At time: 761.2687346935272 and batch: 250, loss is 3.9568898725509642 and perplexity is 52.29443042398962
At time: 763.745539188385 and batch: 300, loss is 3.9542520380020143 and perplexity is 52.156668145629375
At time: 766.2234210968018 and batch: 350, loss is 3.902092943191528 and perplexity is 49.50595390207722
At time: 768.744217634201 and batch: 400, loss is 3.875455765724182 and perplexity is 48.2046633185516
At time: 771.2186443805695 and batch: 450, loss is 3.8206315994262696 and perplexity is 45.63302101074105
At time: 773.6877040863037 and batch: 500, loss is 3.817991271018982 and perplexity is 45.51269377067453
At time: 776.1531012058258 and batch: 550, loss is 3.8493195724487306 and perplexity is 46.96109873263409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.448904418945313 and perplexity of 85.53318411473752
Finished 27 epochs...
Completing Train Step...
At time: 780.2210831642151 and batch: 50, loss is 3.993131608963013 and perplexity is 54.22443347371978
At time: 782.6967561244965 and batch: 100, loss is 3.977714891433716 and perplexity is 53.394881613282266
At time: 785.1740889549255 and batch: 150, loss is 3.972640748023987 and perplexity is 53.124634542662974
At time: 787.6452436447144 and batch: 200, loss is 3.956633810997009 and perplexity is 52.281041545133775
At time: 790.1166880130768 and batch: 250, loss is 3.9555631351470946 and perplexity is 52.22509545195292
At time: 792.5853078365326 and batch: 300, loss is 3.9532103300094605 and perplexity is 52.102364416781946
At time: 795.0550439357758 and batch: 350, loss is 3.9012327671051024 and perplexity is 49.46338837394419
At time: 797.5266494750977 and batch: 400, loss is 3.8745426511764527 and perplexity is 48.16066702908995
At time: 799.9974911212921 and batch: 450, loss is 3.8199670553207397 and perplexity is 45.60270592958072
At time: 802.470201253891 and batch: 500, loss is 3.817678632736206 and perplexity is 45.49846698428447
At time: 804.9428095817566 and batch: 550, loss is 3.848588328361511 and perplexity is 46.92677125926711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.448907470703125 and perplexity of 85.5334451416986
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 809.0511796474457 and batch: 50, loss is 3.993812608718872 and perplexity is 54.26137287611139
At time: 811.5535769462585 and batch: 100, loss is 3.9826887607574464 and perplexity is 53.661122351417276
At time: 814.0287437438965 and batch: 150, loss is 3.9790625047683714 and perplexity is 53.46688577373307
At time: 816.5005540847778 and batch: 200, loss is 3.9616881084442137 and perplexity is 52.545954390157355
At time: 818.9719727039337 and batch: 250, loss is 3.95725700378418 and perplexity is 52.31363286741417
At time: 821.4449927806854 and batch: 300, loss is 3.9511896657943724 and perplexity is 51.99718933103607
At time: 823.9147071838379 and batch: 350, loss is 3.8973978567123413 and perplexity is 49.27406396490518
At time: 826.388997554779 and batch: 400, loss is 3.868312830924988 and perplexity is 47.86156736623051
At time: 828.8627364635468 and batch: 450, loss is 3.811802086830139 and perplexity is 45.23187723504122
At time: 831.3629772663116 and batch: 500, loss is 3.808235878944397 and perplexity is 45.070858242011134
At time: 833.8342752456665 and batch: 550, loss is 3.8430694246292116 and perplexity is 46.66850026812896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.443583170572917 and perplexity of 85.0792496182032
Finished 29 epochs...
Completing Train Step...
At time: 837.915244102478 and batch: 50, loss is 3.9912781000137327 and perplexity is 54.124021087382125
At time: 840.3802735805511 and batch: 100, loss is 3.9784056043624876 and perplexity is 53.431774888207016
At time: 842.8469526767731 and batch: 150, loss is 3.974706597328186 and perplexity is 53.23449547095847
At time: 845.3200335502625 and batch: 200, loss is 3.957559971809387 and perplexity is 52.32948462662226
At time: 847.7870337963104 and batch: 250, loss is 3.954171142578125 and perplexity is 52.15244908050491
At time: 850.2488672733307 and batch: 300, loss is 3.9493362140655517 and perplexity is 51.900904307961746
At time: 852.7191753387451 and batch: 350, loss is 3.89681303024292 and perplexity is 49.245255612807064
At time: 855.1928534507751 and batch: 400, loss is 3.8682490587234497 and perplexity is 47.85851522603238
At time: 857.6670553684235 and batch: 450, loss is 3.8124273157119752 and perplexity is 45.2601663537337
At time: 860.1284115314484 and batch: 500, loss is 3.809844079017639 and perplexity is 45.143399514342256
At time: 862.590692281723 and batch: 550, loss is 3.8444223833084106 and perplexity is 46.731683553160785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.443004862467448 and perplexity of 85.03006182276305
Finished 30 epochs...
Completing Train Step...
At time: 866.639969587326 and batch: 50, loss is 3.990332398414612 and perplexity is 54.07286010941127
At time: 869.130865573883 and batch: 100, loss is 3.976869206428528 and perplexity is 53.349745450726104
At time: 871.5937650203705 and batch: 150, loss is 3.973276963233948 and perplexity is 53.15844399708679
At time: 874.0570864677429 and batch: 200, loss is 3.956275143623352 and perplexity is 52.26229340364504
At time: 876.5195384025574 and batch: 250, loss is 3.953153510093689 and perplexity is 52.09940404892901
At time: 878.9799296855927 and batch: 300, loss is 3.9484498929977416 and perplexity is 51.85492382278162
At time: 881.4274430274963 and batch: 350, loss is 3.8965450143814087 and perplexity is 49.23205887174561
At time: 883.8946719169617 and batch: 400, loss is 3.8682301092147826 and perplexity is 47.85760833927586
At time: 886.3636615276337 and batch: 450, loss is 3.812728695869446 and perplexity is 45.2738089254935
At time: 888.8372774124146 and batch: 500, loss is 3.8105315494537355 and perplexity is 45.1744449370757
At time: 891.3098936080933 and batch: 550, loss is 3.8448608684539796 and perplexity is 46.75217919541522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4428349812825525 and perplexity of 85.01561804200689
Finished 31 epochs...
Completing Train Step...
At time: 895.4034464359283 and batch: 50, loss is 3.9895109844207766 and perplexity is 54.02846214248272
At time: 897.8759565353394 and batch: 100, loss is 3.9758979845047 and perplexity is 53.29795616183106
At time: 900.3450410366058 and batch: 150, loss is 3.9724252653121948 and perplexity is 53.113188335622915
At time: 902.8142302036285 and batch: 200, loss is 3.9555466890335085 and perplexity is 52.22423655916381
At time: 905.2873599529266 and batch: 250, loss is 3.9525593090057374 and perplexity is 52.06845572203685
At time: 907.7567644119263 and batch: 300, loss is 3.947910814285278 and perplexity is 51.82697747053092
At time: 910.23486328125 and batch: 350, loss is 3.8963241672515867 and perplexity is 49.22118731336898
At time: 912.7152905464172 and batch: 400, loss is 3.868158416748047 and perplexity is 47.854177432268514
At time: 915.1903512477875 and batch: 450, loss is 3.81284508228302 and perplexity is 45.27907848838999
At time: 917.6679885387421 and batch: 500, loss is 3.8108650779724123 and perplexity is 45.18951441568834
At time: 920.1429946422577 and batch: 550, loss is 3.8449893951416017 and perplexity is 46.75818848431505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.442768859863281 and perplexity of 85.00999687452355
Finished 32 epochs...
Completing Train Step...
At time: 924.2455751895905 and batch: 50, loss is 3.9887559604644776 and perplexity is 53.98768475513245
At time: 926.7468454837799 and batch: 100, loss is 3.97513277053833 and perplexity is 53.257187421791045
At time: 929.2163481712341 and batch: 150, loss is 3.971787371635437 and perplexity is 53.07931857243356
At time: 931.6865468025208 and batch: 200, loss is 3.9550141429901124 and perplexity is 52.196432152835264
At time: 934.1643085479736 and batch: 250, loss is 3.952095241546631 and perplexity is 52.044298051918176
At time: 936.6351194381714 and batch: 300, loss is 3.9474914121627807 and perplexity is 51.805245683674464
At time: 939.1100790500641 and batch: 350, loss is 3.896113395690918 and perplexity is 49.21081398014119
At time: 941.5760586261749 and batch: 400, loss is 3.868051300048828 and perplexity is 47.84905172526744
At time: 944.0470542907715 and batch: 450, loss is 3.8128585481643675 and perplexity is 45.27968821519368
At time: 946.5156321525574 and batch: 500, loss is 3.811037893295288 and perplexity is 45.197324531047066
At time: 948.9800801277161 and batch: 550, loss is 3.8449901819229124 and perplexity is 46.75822527279835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.44273681640625 and perplexity of 85.00727290398446
Finished 33 epochs...
Completing Train Step...
At time: 953.0763349533081 and batch: 50, loss is 3.988042407035828 and perplexity is 53.94917539844626
At time: 955.5509548187256 and batch: 100, loss is 3.974485936164856 and perplexity is 53.22274998119295
At time: 958.0568904876709 and batch: 150, loss is 3.971245999336243 and perplexity is 53.050590676643466
At time: 960.5312461853027 and batch: 200, loss is 3.9545651292800903 and perplexity is 52.17300050014473
At time: 963.0116250514984 and batch: 250, loss is 3.951692571640015 and perplexity is 52.02334559802614
At time: 965.4816534519196 and batch: 300, loss is 3.947110137939453 and perplexity is 51.78549744384876
At time: 967.9507632255554 and batch: 350, loss is 3.8959056091308595 and perplexity is 49.20058969665771
At time: 970.4077639579773 and batch: 400, loss is 3.867923755645752 and perplexity is 47.84294923570486
At time: 972.8766827583313 and batch: 450, loss is 3.8128104162216188 and perplexity is 45.277508868281345
At time: 975.3473336696625 and batch: 500, loss is 3.8111288213729857 and perplexity is 45.20143442373323
At time: 977.8164184093475 and batch: 550, loss is 3.8449325561523438 and perplexity is 46.75553087167079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4427129109700525 and perplexity of 85.00524079233507
Finished 34 epochs...
Completing Train Step...
At time: 981.8867380619049 and batch: 50, loss is 3.987359251976013 and perplexity is 53.91233233249633
At time: 984.3775410652161 and batch: 100, loss is 3.9738719129562377 and perplexity is 53.19008000856516
At time: 986.8430979251862 and batch: 150, loss is 3.9707539749145506 and perplexity is 53.02449491084886
At time: 989.3140425682068 and batch: 200, loss is 3.9541590118408205 and perplexity is 52.15181643668256
At time: 991.7779433727264 and batch: 250, loss is 3.9513271188735963 and perplexity is 52.00433699604364
At time: 994.2460718154907 and batch: 300, loss is 3.946743893623352 and perplexity is 51.76653477245075
At time: 996.7142939567566 and batch: 350, loss is 3.8956978273391725 and perplexity is 49.19036777198017
At time: 999.1781313419342 and batch: 400, loss is 3.8677753353118898 and perplexity is 47.83584889613676
At time: 1001.6422708034515 and batch: 450, loss is 3.8127399396896364 and perplexity is 45.274317978922234
At time: 1004.1151299476624 and batch: 500, loss is 3.811168284416199 and perplexity is 45.203218245090476
At time: 1006.5832080841064 and batch: 550, loss is 3.8448404026031495 and perplexity is 46.75122238208086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.442698160807292 and perplexity of 85.00398696044502
Finished 35 epochs...
Completing Train Step...
At time: 1010.6903786659241 and batch: 50, loss is 3.98670241355896 and perplexity is 53.87693226880106
At time: 1013.1587963104248 and batch: 100, loss is 3.973318090438843 and perplexity is 53.16063030026292
At time: 1015.6323437690735 and batch: 150, loss is 3.9702987384796145 and perplexity is 53.000361722391474
At time: 1018.1027095317841 and batch: 200, loss is 3.9537785196304323 and perplexity is 52.13197685141398
At time: 1020.60977435112 and batch: 250, loss is 3.9509764671325684 and perplexity is 51.98610478150068
At time: 1023.0851747989655 and batch: 300, loss is 3.946393141746521 and perplexity is 51.7483807471876
At time: 1025.5533800125122 and batch: 350, loss is 3.8954873132705687 and perplexity is 49.18001359741233
At time: 1028.0119836330414 and batch: 400, loss is 3.8676111125946044 and perplexity is 47.827993808056995
At time: 1030.4721353054047 and batch: 450, loss is 3.812635555267334 and perplexity is 45.269592292043214
At time: 1032.9309499263763 and batch: 500, loss is 3.8111754655838013 and perplexity is 45.203542858142406
At time: 1035.3906710147858 and batch: 550, loss is 3.8447279596328734 and perplexity is 46.745965831308816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.44268798828125 and perplexity of 85.0031222595721
Finished 36 epochs...
Completing Train Step...
At time: 1039.3905601501465 and batch: 50, loss is 3.9860679149627685 and perplexity is 53.842758273731846
At time: 1041.8765180110931 and batch: 100, loss is 3.9727382183074953 and perplexity is 53.12981286821546
At time: 1044.342381477356 and batch: 150, loss is 3.9698647451400757 and perplexity is 52.97736490900368
At time: 1046.7975769042969 and batch: 200, loss is 3.953415937423706 and perplexity is 52.11307815057956
At time: 1049.2576353549957 and batch: 250, loss is 3.9506393384933474 and perplexity is 51.9685817306644
At time: 1051.7258467674255 and batch: 300, loss is 3.946051850318909 and perplexity is 51.730722481924474
At time: 1054.2006196975708 and batch: 350, loss is 3.8952755069732667 and perplexity is 49.16959806390773
At time: 1056.6711659431458 and batch: 400, loss is 3.867436089515686 and perplexity is 47.819623537838815
At time: 1059.1400866508484 and batch: 450, loss is 3.8125151586532593 and perplexity is 45.26414231449671
At time: 1061.6114931106567 and batch: 500, loss is 3.811159358024597 and perplexity is 45.20281474526366
At time: 1064.0856685638428 and batch: 550, loss is 3.8446009731292725 and perplexity is 46.74003010143721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.442681884765625 and perplexity of 85.00260344327049
Finished 37 epochs...
Completing Train Step...
At time: 1068.1729300022125 and batch: 50, loss is 3.985449342727661 and perplexity is 53.809462937249116
At time: 1070.6900382041931 and batch: 100, loss is 3.972238974571228 and perplexity is 53.103294761981964
At time: 1073.1612377166748 and batch: 150, loss is 3.96945228099823 and perplexity is 52.95551815146102
At time: 1075.6285803318024 and batch: 200, loss is 3.9530656814575194 and perplexity is 52.09482843026426
At time: 1078.100759267807 and batch: 250, loss is 3.9503068590164183 and perplexity is 51.95130611584667
At time: 1080.5949957370758 and batch: 300, loss is 3.9457211542129516 and perplexity is 51.71361816176326
At time: 1083.0616755485535 and batch: 350, loss is 3.895060544013977 and perplexity is 49.15902955756022
At time: 1085.5315747261047 and batch: 400, loss is 3.8672525310516357 and perplexity is 47.81084664675171
At time: 1088.001309633255 and batch: 450, loss is 3.8123803472518922 and perplexity is 45.25804060333897
At time: 1090.4800879955292 and batch: 500, loss is 3.811128787994385 and perplexity is 45.20143291497263
At time: 1092.9548072814941 and batch: 550, loss is 3.8444658374786376 and perplexity is 46.733714283814436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.442681376139323 and perplexity of 85.00256020872163
Finished 38 epochs...
Completing Train Step...
At time: 1097.047547340393 and batch: 50, loss is 3.984846591949463 and perplexity is 53.77703901434331
At time: 1099.5174353122711 and batch: 100, loss is 3.971758556365967 and perplexity is 53.077789099601794
At time: 1101.9915947914124 and batch: 150, loss is 3.969051036834717 and perplexity is 52.93427432114342
At time: 1104.460256099701 and batch: 200, loss is 3.952724299430847 and perplexity is 52.07704722741981
At time: 1106.9321126937866 and batch: 250, loss is 3.949983239173889 and perplexity is 51.93449636247385
At time: 1109.404582977295 and batch: 300, loss is 3.945396728515625 and perplexity is 51.696843656317284
At time: 1111.8769261837006 and batch: 350, loss is 3.8948434925079347 and perplexity is 49.14836067404967
At time: 1114.3530039787292 and batch: 400, loss is 3.8670621156692504 and perplexity is 47.80174359281356
At time: 1116.827712059021 and batch: 450, loss is 3.812239227294922 and perplexity is 45.25165424122864
At time: 1119.298898935318 and batch: 500, loss is 3.811087923049927 and perplexity is 45.19958579866854
At time: 1121.7725052833557 and batch: 550, loss is 3.8443235254287718 and perplexity is 46.72706398635682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.442679850260417 and perplexity of 85.00243050520702
Finished 39 epochs...
Completing Train Step...
At time: 1125.8318524360657 and batch: 50, loss is 3.984254140853882 and perplexity is 53.745188184623274
At time: 1128.3269176483154 and batch: 100, loss is 3.9712911462783813 and perplexity is 53.05298580265706
At time: 1130.7933423519135 and batch: 150, loss is 3.968663659095764 and perplexity is 52.91377273282962
At time: 1133.2567341327667 and batch: 200, loss is 3.952388005256653 and perplexity is 52.059536964292754
At time: 1135.7187418937683 and batch: 250, loss is 3.949661469459534 and perplexity is 51.91778810266426
At time: 1138.1851522922516 and batch: 300, loss is 3.945077295303345 and perplexity is 51.68033260471293
At time: 1140.6512496471405 and batch: 350, loss is 3.8946242237091067 and perplexity is 49.137585153451205
At time: 1143.1520268917084 and batch: 400, loss is 3.866866307258606 and perplexity is 47.792384525696555
At time: 1145.6110882759094 and batch: 450, loss is 3.812087755203247 and perplexity is 45.24480039760509
At time: 1148.0714905261993 and batch: 500, loss is 3.8110385370254516 and perplexity is 45.19735362593754
At time: 1150.5371446609497 and batch: 550, loss is 3.8441760969161987 and perplexity is 46.7201755926017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4426823933919275 and perplexity of 85.0026466778414
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1154.554592847824 and batch: 50, loss is 3.984351844787598 and perplexity is 53.75043955746293
At time: 1157.0120241641998 and batch: 100, loss is 3.972151126861572 and perplexity is 53.09862996406084
At time: 1159.4803535938263 and batch: 150, loss is 3.970139389038086 and perplexity is 52.99191681721353
At time: 1161.9430770874023 and batch: 200, loss is 3.9539661741256715 and perplexity is 52.14176056916646
At time: 1164.4060735702515 and batch: 250, loss is 3.9509267473220824 and perplexity is 51.983520106478345
At time: 1166.8830394744873 and batch: 300, loss is 3.9442594766616823 and perplexity is 51.638084743200544
At time: 1169.3482048511505 and batch: 350, loss is 3.8920717239379883 and perplexity is 49.01232141443673
At time: 1171.8136100769043 and batch: 400, loss is 3.8639808225631715 and perplexity is 47.65467910058251
At time: 1174.285394668579 and batch: 450, loss is 3.8088104248046877 and perplexity is 45.09676095747025
At time: 1176.7631044387817 and batch: 500, loss is 3.8074216842651367 and perplexity is 45.03417672401557
At time: 1179.2393589019775 and batch: 550, loss is 3.842053575515747 and perplexity is 46.62111618511904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.44212646484375 and perplexity of 84.95540441276118
Finished 41 epochs...
Completing Train Step...
At time: 1183.302464723587 and batch: 50, loss is 3.983928108215332 and perplexity is 53.72766835528336
At time: 1185.785514831543 and batch: 100, loss is 3.9712053966522216 and perplexity is 53.04843672400151
At time: 1188.257835149765 and batch: 150, loss is 3.969062943458557 and perplexity is 52.93490459338821
At time: 1190.7218127250671 and batch: 200, loss is 3.9529661464691164 and perplexity is 52.08964343016934
At time: 1193.1828880310059 and batch: 250, loss is 3.950109057426453 and perplexity is 51.941031081138775
At time: 1195.6551373004913 and batch: 300, loss is 3.943892450332642 and perplexity is 51.61913568413308
At time: 1198.1283423900604 and batch: 350, loss is 3.8920437908172607 and perplexity is 49.0109523664665
At time: 1200.6015937328339 and batch: 400, loss is 3.864085702896118 and perplexity is 47.65967740130017
At time: 1203.0702738761902 and batch: 450, loss is 3.809024257659912 and perplexity is 45.106405157713844
At time: 1205.5628361701965 and batch: 500, loss is 3.8079501390457153 and perplexity is 45.057981539325034
At time: 1208.030281305313 and batch: 550, loss is 3.842490577697754 and perplexity is 46.64149416690649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441769917805989 and perplexity of 84.92511921434577
Finished 42 epochs...
Completing Train Step...
At time: 1212.103806734085 and batch: 50, loss is 3.983669681549072 and perplexity is 53.713785486993345
At time: 1214.5738689899445 and batch: 100, loss is 3.9706462478637694 and perplexity is 53.018783046059646
At time: 1217.0462696552277 and batch: 150, loss is 3.9684642219543456 and perplexity is 52.9032208135114
At time: 1219.52112865448 and batch: 200, loss is 3.952427878379822 and perplexity is 52.06161278200665
At time: 1221.99569606781 and batch: 250, loss is 3.949662656784058 and perplexity is 51.9178497459639
At time: 1224.466425895691 and batch: 300, loss is 3.9436967849731444 and perplexity is 51.60903659544562
At time: 1226.9389414787292 and batch: 350, loss is 3.8920442390441896 and perplexity is 49.010974334500084
At time: 1229.4046337604523 and batch: 400, loss is 3.864171404838562 and perplexity is 47.663762103260666
At time: 1231.8749668598175 and batch: 450, loss is 3.809188709259033 and perplexity is 45.11382358814229
At time: 1234.3477051258087 and batch: 500, loss is 3.8082867240905762 and perplexity is 45.0731499346471
At time: 1236.8272397518158 and batch: 550, loss is 3.8427443313598633 and perplexity is 46.653331118629005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441595458984375 and perplexity of 84.91030457043244
Finished 43 epochs...
Completing Train Step...
At time: 1240.8826534748077 and batch: 50, loss is 3.983453073501587 and perplexity is 53.70215190880478
At time: 1243.3724792003632 and batch: 100, loss is 3.970260682106018 and perplexity is 52.998344759204414
At time: 1245.8417356014252 and batch: 150, loss is 3.968054161071777 and perplexity is 52.88153171932238
At time: 1248.3062949180603 and batch: 200, loss is 3.952083501815796 and perplexity is 52.04368706945394
At time: 1250.7741196155548 and batch: 250, loss is 3.9493730545043944 and perplexity is 51.90281639527363
At time: 1253.232965707779 and batch: 300, loss is 3.9435577154159547 and perplexity is 51.60185984862438
At time: 1255.6972634792328 and batch: 350, loss is 3.8920340394973754 and perplexity is 49.01047444732226
At time: 1258.1568155288696 and batch: 400, loss is 3.864222960472107 and perplexity is 47.66621950205887
At time: 1260.616149187088 and batch: 450, loss is 3.809303402900696 and perplexity is 45.11899815359815
At time: 1263.072253227234 and batch: 500, loss is 3.8085099124908446 and perplexity is 45.0832108615754
At time: 1265.5186567306519 and batch: 550, loss is 3.8428952503204346 and perplexity is 46.66037252219591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441501871744792 and perplexity of 84.90235842125037
Finished 44 epochs...
Completing Train Step...
At time: 1269.5399441719055 and batch: 50, loss is 3.983254690170288 and perplexity is 53.691499353690816
At time: 1272.0014834403992 and batch: 100, loss is 3.9699578619003297 and perplexity is 52.98229821927418
At time: 1274.4608488082886 and batch: 150, loss is 3.9677436208724974 and perplexity is 52.86511242748107
At time: 1276.9140872955322 and batch: 200, loss is 3.9518368577957155 and perplexity is 52.03085238811908
At time: 1279.3624000549316 and batch: 250, loss is 3.949167790412903 and perplexity is 51.892163704165284
At time: 1281.823447227478 and batch: 300, loss is 3.943445296287537 and perplexity is 51.59605913857695
At time: 1284.2780303955078 and batch: 350, loss is 3.8920106792449953 and perplexity is 49.009329563642346
At time: 1286.7403311729431 and batch: 400, loss is 3.8642491912841797 and perplexity is 47.667469842103486
At time: 1289.2024726867676 and batch: 450, loss is 3.8093801498413087 and perplexity is 45.1224610315509
At time: 1291.6608617305756 and batch: 500, loss is 3.808664894104004 and perplexity is 45.0901984717827
At time: 1294.11847448349 and batch: 550, loss is 3.8429871559143067 and perplexity is 46.66466106851053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441446940104167 and perplexity of 84.89769472350284
Finished 45 epochs...
Completing Train Step...
At time: 1298.160097360611 and batch: 50, loss is 3.983067498207092 and perplexity is 53.681449677158554
At time: 1300.6552362442017 and batch: 100, loss is 3.9697035503387452 and perplexity is 52.968825921430856
At time: 1303.1294918060303 and batch: 150, loss is 3.9674971294403076 and perplexity is 52.85208323606394
At time: 1305.598445892334 and batch: 200, loss is 3.951644768714905 and perplexity is 52.02085878937139
At time: 1308.065824508667 and batch: 250, loss is 3.949010691642761 and perplexity is 51.88401214938376
At time: 1310.5284316539764 and batch: 300, loss is 3.943347897529602 and perplexity is 51.59103399122804
At time: 1312.991617679596 and batch: 350, loss is 3.891976685523987 and perplexity is 49.00766358248296
At time: 1315.4574859142303 and batch: 400, loss is 3.8642574310302735 and perplexity is 47.66786261157009
At time: 1317.9206495285034 and batch: 450, loss is 3.8094314193725585 and perplexity is 45.1247744982815
At time: 1320.383864402771 and batch: 500, loss is 3.808777208328247 and perplexity is 45.09526302685049
At time: 1322.856377363205 and batch: 550, loss is 3.843043384552002 and perplexity is 46.6672850326012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441413370768229 and perplexity of 84.89484481210341
Finished 46 epochs...
Completing Train Step...
At time: 1326.8730664253235 and batch: 50, loss is 3.982888603210449 and perplexity is 53.67184719334253
At time: 1329.3311607837677 and batch: 100, loss is 3.9694818449020386 and perplexity is 52.95708374644822
At time: 1331.8073363304138 and batch: 150, loss is 3.967293128967285 and perplexity is 52.841302485760025
At time: 1334.2661924362183 and batch: 200, loss is 3.9514853239059446 and perplexity is 52.012564994698586
At time: 1336.717621564865 and batch: 250, loss is 3.948881549835205 and perplexity is 51.87731218690346
At time: 1339.165427684784 and batch: 300, loss is 3.9432586526870725 and perplexity is 51.58642996296951
At time: 1341.6241037845612 and batch: 350, loss is 3.8919349670410157 and perplexity is 49.00561909975099
At time: 1344.0974912643433 and batch: 400, loss is 3.864252557754517 and perplexity is 47.66763031349687
At time: 1346.558134317398 and batch: 450, loss is 3.809461727142334 and perplexity is 45.1261421502833
At time: 1349.011384010315 and batch: 500, loss is 3.808861241340637 and perplexity is 45.09905267687277
At time: 1351.4610786437988 and batch: 550, loss is 3.8430764198303224 and perplexity is 46.66882672481569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441392008463541 and perplexity of 84.89303128193278
Finished 47 epochs...
Completing Train Step...
At time: 1355.447632074356 and batch: 50, loss is 3.9827159786224366 and perplexity is 53.66258291247725
At time: 1357.9474260807037 and batch: 100, loss is 3.969284143447876 and perplexity is 52.94661508885184
At time: 1360.4143998622894 and batch: 150, loss is 3.9671144819259645 and perplexity is 52.83186338657009
At time: 1362.8830292224884 and batch: 200, loss is 3.9513466501235963 and perplexity is 52.0053527156697
At time: 1365.340458393097 and batch: 250, loss is 3.9487697076797486 and perplexity is 51.871510440935126
At time: 1367.8051731586456 and batch: 300, loss is 3.943173842430115 and perplexity is 51.58205509010849
At time: 1370.2773849964142 and batch: 350, loss is 3.891887755393982 and perplexity is 49.003305518373814
At time: 1372.7410154342651 and batch: 400, loss is 3.864237985610962 and perplexity is 47.66693569900604
At time: 1375.2029249668121 and batch: 450, loss is 3.809474377632141 and perplexity is 45.12671302169549
At time: 1377.671336889267 and batch: 500, loss is 3.8089252042770387 and perplexity is 45.1019374369688
At time: 1380.1387734413147 and batch: 550, loss is 3.8430940389633177 and perplexity is 46.66964899632432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.44137929280599 and perplexity of 84.89195181808152
Finished 48 epochs...
Completing Train Step...
At time: 1384.1683297157288 and batch: 50, loss is 3.98254873752594 and perplexity is 53.65360907368835
At time: 1386.6255338191986 and batch: 100, loss is 3.969103674888611 and perplexity is 52.937060751663466
At time: 1389.0875055789948 and batch: 150, loss is 3.9669526386260987 and perplexity is 52.82331359534343
At time: 1391.5373435020447 and batch: 200, loss is 3.951221957206726 and perplexity is 51.99886842082795
At time: 1394.0197985172272 and batch: 250, loss is 3.9486689233779906 and perplexity is 51.866282870407154
At time: 1396.479284286499 and batch: 300, loss is 3.9430921411514284 and perplexity is 51.57784094240332
At time: 1398.939864397049 and batch: 350, loss is 3.891836404800415 and perplexity is 49.00078923415559
At time: 1401.3893065452576 and batch: 400, loss is 3.8642162609100343 and perplexity is 47.66590016033227
At time: 1403.8393940925598 and batch: 450, loss is 3.8094742918014526 and perplexity is 45.12670914843881
At time: 1406.3044700622559 and batch: 500, loss is 3.8089746713638304 and perplexity is 45.10416855360543
At time: 1408.7650537490845 and batch: 550, loss is 3.8431010484695434 and perplexity is 46.66997612866602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441371154785156 and perplexity of 84.89126096842013
Finished 49 epochs...
Completing Train Step...
At time: 1412.7217512130737 and batch: 50, loss is 3.9823855352401734 and perplexity is 53.6448533965406
At time: 1415.2075686454773 and batch: 100, loss is 3.9689378547668457 and perplexity is 52.9282834495505
At time: 1417.6675479412079 and batch: 150, loss is 3.9668032932281494 and perplexity is 52.81542526561106
At time: 1420.1311297416687 and batch: 200, loss is 3.9511070394515992 and perplexity is 51.992893170937606
At time: 1422.596159696579 and batch: 250, loss is 3.9485758113861085 and perplexity is 51.86145372232686
At time: 1425.048894405365 and batch: 300, loss is 3.943012166023254 and perplexity is 51.57371616290509
At time: 1427.493314743042 and batch: 350, loss is 3.891782240867615 and perplexity is 48.998135230576636
At time: 1429.9413177967072 and batch: 400, loss is 3.864189138412476 and perplexity is 47.66460735960362
At time: 1432.4003818035126 and batch: 450, loss is 3.809474883079529 and perplexity is 45.12673583088048
At time: 1434.8602631092072 and batch: 500, loss is 3.809014205932617 and perplexity is 45.10595176270865
At time: 1437.3177206516266 and batch: 550, loss is 3.843100543022156 and perplexity is 46.66995253945448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.441365559895833 and perplexity of 84.89078601253915
Finished Training.
Improved accuracyfrom -148.54667985450328 to -84.89078601253915
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fae881507f0>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 5.936322633091923, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 26.7776888998906, 'dropout': 0.9561181916555967, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.950922727584839 and batch: 50, loss is 7.852738351821899 and perplexity is 2572.769828532595
At time: 5.391416072845459 and batch: 100, loss is 7.179385871887207 and perplexity is 1312.1022119079462
At time: 7.83639669418335 and batch: 150, loss is 7.062486982345581 and perplexity is 1167.3447244235654
At time: 10.277528762817383 and batch: 200, loss is 6.9923098754882815 and perplexity is 1088.2322562892111
At time: 12.721909046173096 and batch: 250, loss is 6.904247522354126 and perplexity is 996.4983883631254
At time: 15.201931953430176 and batch: 300, loss is 6.895994234085083 and perplexity is 988.3078458512631
At time: 17.648405075073242 and batch: 350, loss is 6.858534460067749 and perplexity is 951.970893308084
At time: 20.102425575256348 and batch: 400, loss is 6.860268650054931 and perplexity is 953.6232240129261
At time: 22.552636861801147 and batch: 450, loss is 6.857696666717529 and perplexity is 951.1736724238759
At time: 24.990519046783447 and batch: 500, loss is 6.84791337966919 and perplexity is 941.9134389747621
At time: 27.439868927001953 and batch: 550, loss is 6.823671674728393 and perplexity is 919.3543909442616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.50130615234375 and perplexity of 666.0109769714979
Finished 1 epochs...
Completing Train Step...
At time: 31.436144828796387 and batch: 50, loss is 6.736698484420776 and perplexity is 842.7737071493352
At time: 33.858962059020996 and batch: 100, loss is 6.907776756286621 and perplexity is 1000.0214775351229
At time: 36.267497062683105 and batch: 150, loss is 7.259726333618164 and perplexity is 1421.8673666544778
At time: 38.69765758514404 and batch: 200, loss is 7.130680913925171 and perplexity is 1249.72763421359
At time: 41.1269474029541 and batch: 250, loss is 7.206195135116577 and perplexity is 1347.7544755951435
At time: 43.57854723930359 and batch: 300, loss is 6.910870923995971 and perplexity is 1003.1205036804101
At time: 46.048295974731445 and batch: 350, loss is 6.966696071624756 and perplexity is 1060.7124369006222
At time: 48.50344753265381 and batch: 400, loss is 7.146709022521972 and perplexity is 1269.9197932308891
At time: 50.952736139297485 and batch: 450, loss is 7.516998882293701 and perplexity is 1839.0398289220698
At time: 53.401880502700806 and batch: 500, loss is 7.565175895690918 and perplexity is 1929.80819415012
At time: 55.86462187767029 and batch: 550, loss is 7.475289688110352 and perplexity is 1763.9125986882552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.662590535481771 and perplexity of 782.5756033773677
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 59.86676263809204 and batch: 50, loss is 6.705996446609497 and perplexity is 817.2920085511123
At time: 62.29041576385498 and batch: 100, loss is 6.661654853820801 and perplexity is 781.8437042026811
At time: 64.71773338317871 and batch: 150, loss is 6.692770919799805 and perplexity is 806.5540552435888
At time: 67.15294337272644 and batch: 200, loss is 6.697586221694946 and perplexity is 810.4472223792684
At time: 69.58861589431763 and batch: 250, loss is 6.624456224441528 and perplexity is 753.2944780373609
At time: 72.02318811416626 and batch: 300, loss is 6.594687900543213 and perplexity is 731.2006437511563
At time: 74.45540475845337 and batch: 350, loss is 6.534094963073731 and perplexity is 688.210646337011
At time: 76.89360451698303 and batch: 400, loss is 6.531641798019409 and perplexity is 686.524421169377
At time: 79.32654070854187 and batch: 450, loss is 6.510890207290649 and perplexity is 672.4247486459049
At time: 81.77041506767273 and batch: 500, loss is 6.4773727130889895 and perplexity is 650.2602797275933
At time: 84.20407629013062 and batch: 550, loss is 6.443131675720215 and perplexity is 628.3715776952545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.1688232421875 and perplexity of 477.623727809505
Finished 3 epochs...
Completing Train Step...
At time: 88.15380859375 and batch: 50, loss is 6.469686403274536 and perplexity is 645.2813371180004
At time: 90.6144597530365 and batch: 100, loss is 6.459544334411621 and perplexity is 638.7699247716095
At time: 93.04569625854492 and batch: 150, loss is 6.481113586425781 and perplexity is 652.6973766633866
At time: 95.47524785995483 and batch: 200, loss is 6.482801942825318 and perplexity is 653.8002932523843
At time: 97.93548941612244 and batch: 250, loss is 6.423276128768921 and perplexity is 616.017966046998
At time: 100.39954471588135 and batch: 300, loss is 6.42656476020813 and perplexity is 618.0471568988492
At time: 102.85944890975952 and batch: 350, loss is 6.403790578842163 and perplexity is 604.1307082089089
At time: 105.32453203201294 and batch: 400, loss is 6.421885509490966 and perplexity is 615.1619149463825
At time: 107.77824640274048 and batch: 450, loss is 6.414467554092408 and perplexity is 610.6155544952845
At time: 110.23228597640991 and batch: 500, loss is 6.400367403030396 and perplexity is 602.0661981878224
At time: 112.68067836761475 and batch: 550, loss is 6.365806770324707 and perplexity is 581.6138678970324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 6.082658386230468 and perplexity of 438.19253272770544
Finished 4 epochs...
Completing Train Step...
At time: 116.63346123695374 and batch: 50, loss is 6.402067861557007 and perplexity is 603.0908577367431
At time: 119.09705758094788 and batch: 100, loss is 6.372503442764282 and perplexity is 585.5218159762978
At time: 121.5657548904419 and batch: 150, loss is 6.349572124481202 and perplexity is 572.2478057965753
At time: 124.02369236946106 and batch: 200, loss is 6.29711145401001 and perplexity is 543.0011588045451
At time: 126.49192333221436 and batch: 250, loss is 6.229637022018433 and perplexity is 507.57121288037587
At time: 128.95677185058594 and batch: 300, loss is 6.225740194320679 and perplexity is 505.5971441204142
At time: 131.4279501438141 and batch: 350, loss is 6.203204374313355 and perplexity is 494.330525944958
At time: 133.89632058143616 and batch: 400, loss is 6.214687948226929 and perplexity is 500.03992649640867
At time: 136.3694076538086 and batch: 450, loss is 6.198126487731933 and perplexity is 491.8267339663271
At time: 138.84920644760132 and batch: 500, loss is 6.188085250854492 and perplexity is 486.91289701530104
At time: 141.3103003501892 and batch: 550, loss is 6.154510536193848 and perplexity is 470.8363287098357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.840437316894532 and perplexity of 343.92971405664605
Finished 5 epochs...
Completing Train Step...
At time: 145.458655834198 and batch: 50, loss is 6.201152877807617 and perplexity is 493.3174481164216
At time: 147.96725749969482 and batch: 100, loss is 6.185796937942505 and perplexity is 485.7999618037009
At time: 150.43775081634521 and batch: 150, loss is 6.2385447978973385 and perplexity is 512.1127409139986
At time: 152.90182065963745 and batch: 200, loss is 6.247084665298462 and perplexity is 516.5048431082802
At time: 155.35685014724731 and batch: 250, loss is 6.1927509689331055 and perplexity is 489.19000335888757
At time: 157.82167053222656 and batch: 300, loss is 6.199018154144287 and perplexity is 492.2654749219204
At time: 160.2876398563385 and batch: 350, loss is 6.177640943527222 and perplexity is 481.85389395637355
At time: 162.74338817596436 and batch: 400, loss is 6.192835855484009 and perplexity is 489.23153077354357
At time: 165.20593976974487 and batch: 450, loss is 6.172833061218261 and perplexity is 479.54275743228686
At time: 167.65852093696594 and batch: 500, loss is 6.1646364974975585 and perplexity is 475.62821946021097
At time: 170.1275930404663 and batch: 550, loss is 6.125044565200806 and perplexity is 457.1650864688177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.823904418945313 and perplexity of 338.2903055294784
Finished 6 epochs...
Completing Train Step...
At time: 174.21005654335022 and batch: 50, loss is 6.179072036743164 and perplexity is 482.54396545563645
At time: 176.67141318321228 and batch: 100, loss is 6.158585500717163 and perplexity is 472.7588845586657
At time: 179.13670539855957 and batch: 150, loss is 6.202522802352905 and perplexity is 493.993718911433
At time: 181.61107397079468 and batch: 200, loss is 6.208882818222046 and perplexity is 497.14553899162786
At time: 184.08255696296692 and batch: 250, loss is 6.160381622314453 and perplexity is 473.6087800308549
At time: 186.5550239086151 and batch: 300, loss is 6.152702894210815 and perplexity is 469.9859939768484
At time: 189.02712512016296 and batch: 350, loss is 6.103861999511719 and perplexity is 447.58300185067026
At time: 191.49746084213257 and batch: 400, loss is 6.102856140136719 and perplexity is 447.133022638012
At time: 193.9711468219757 and batch: 450, loss is 6.06984712600708 and perplexity is 432.6145410067396
At time: 196.43550825119019 and batch: 500, loss is 6.052115287780762 and perplexity is 425.01110065891413
At time: 198.90320324897766 and batch: 550, loss is 6.000579166412353 and perplexity is 403.66251357457986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.696777852376302 and perplexity of 297.90595587207645
Finished 7 epochs...
Completing Train Step...
At time: 202.95643258094788 and batch: 50, loss is 6.036996021270752 and perplexity is 418.63357777959556
At time: 205.44527411460876 and batch: 100, loss is 6.015685873031616 and perplexity is 409.8068179922657
At time: 207.9163224697113 and batch: 150, loss is 6.053855600357056 and perplexity is 425.75139680896183
At time: 210.4137361049652 and batch: 200, loss is 6.06438099861145 and perplexity is 430.256266000022
At time: 212.8731300830841 and batch: 250, loss is 6.015664434432983 and perplexity is 409.79803240255325
At time: 215.33299207687378 and batch: 300, loss is 6.017373065948487 and perplexity is 410.4988247631719
At time: 217.79704022407532 and batch: 350, loss is 5.993163204193115 and perplexity is 400.6800402481283
At time: 220.2656180858612 and batch: 400, loss is 6.001958522796631 and perplexity is 404.2196922253953
At time: 222.73292088508606 and batch: 450, loss is 5.97887110710144 and perplexity is 394.9942102115587
At time: 225.19829034805298 and batch: 500, loss is 5.969428453445435 and perplexity is 391.2819709661883
At time: 227.66841459274292 and batch: 550, loss is 5.933959455490112 and perplexity is 377.6468333163087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.65390879313151 and perplexity of 285.40487698220517
Finished 8 epochs...
Completing Train Step...
At time: 231.72679376602173 and batch: 50, loss is 5.987578258514405 and perplexity is 398.44850130113804
At time: 234.1930046081543 and batch: 100, loss is 5.9708174705505375 and perplexity is 391.8258459551654
At time: 236.65969944000244 and batch: 150, loss is 6.009155216217041 and perplexity is 407.1392303343719
At time: 239.12640738487244 and batch: 200, loss is 6.016500425338745 and perplexity is 410.1407630707413
At time: 241.5867931842804 and batch: 250, loss is 5.971563186645508 and perplexity is 392.1181457677114
At time: 244.05447483062744 and batch: 300, loss is 5.9761647033691405 and perplexity is 393.92664169414314
At time: 246.51440954208374 and batch: 350, loss is 5.94933747291565 and perplexity is 383.49917628443256
At time: 248.97848081588745 and batch: 400, loss is 5.96245756149292 and perplexity is 388.56387141952854
At time: 251.4549376964569 and batch: 450, loss is 5.939338073730469 and perplexity is 379.6835238427646
At time: 253.9302260875702 and batch: 500, loss is 5.931861181259155 and perplexity is 376.85525745959563
At time: 256.4059829711914 and batch: 550, loss is 5.895107479095459 and perplexity is 363.255876213849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.6323298136393225 and perplexity of 279.31210531653636
Finished 9 epochs...
Completing Train Step...
At time: 260.46411871910095 and batch: 50, loss is 5.946814775466919 and perplexity is 382.532943160174
At time: 262.9720194339752 and batch: 100, loss is 5.930758905410767 and perplexity is 376.4400878687027
At time: 265.4408552646637 and batch: 150, loss is 5.967586040496826 and perplexity is 390.56173168914495
At time: 267.9045171737671 and batch: 200, loss is 5.983116140365601 and perplexity is 396.6745377729197
At time: 270.375794172287 and batch: 250, loss is 5.938760919570923 and perplexity is 379.4644511431087
At time: 272.8935248851776 and batch: 300, loss is 5.944154863357544 and perplexity is 381.51679118901126
At time: 275.3731641769409 and batch: 350, loss is 5.9206322574615475 and perplexity is 372.64724841243594
At time: 277.85078477859497 and batch: 400, loss is 5.938467473983764 and perplexity is 379.3531153107406
At time: 280.32673621177673 and batch: 450, loss is 5.913229598999023 and perplexity is 369.8988533711886
At time: 282.7961015701294 and batch: 500, loss is 5.905082654953003 and perplexity is 366.8975504339964
At time: 285.2640554904938 and batch: 550, loss is 5.871326732635498 and perplexity is 354.71928586688523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.622725423177084 and perplexity of 276.6423241293782
Finished 10 epochs...
Completing Train Step...
At time: 289.34911274909973 and batch: 50, loss is 5.927378368377686 and perplexity is 375.16966677357345
At time: 291.80380296707153 and batch: 100, loss is 5.910475740432739 and perplexity is 368.8816055663426
At time: 294.26134848594666 and batch: 150, loss is 5.949295682907104 and perplexity is 383.4831501854463
At time: 296.73090386390686 and batch: 200, loss is 5.963589658737183 and perplexity is 389.0040126018837
At time: 299.20564699172974 and batch: 250, loss is 5.922547988891601 and perplexity is 373.36182470813424
At time: 301.6655149459839 and batch: 300, loss is 5.929927949905395 and perplexity is 376.12741283275847
At time: 304.13083028793335 and batch: 350, loss is 5.906324949264526 and perplexity is 367.3536284066926
At time: 306.5984036922455 and batch: 400, loss is 5.921535882949829 and perplexity is 372.9841341505859
At time: 309.0560157299042 and batch: 450, loss is 5.895643939971924 and perplexity is 363.4508010596859
At time: 311.5328619480133 and batch: 500, loss is 5.885366735458374 and perplexity is 359.7346712678833
At time: 314.00580501556396 and batch: 550, loss is 5.850207967758179 and perplexity is 347.3066015439715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5981903076171875 and perplexity of 269.937461370939
Finished 11 epochs...
Completing Train Step...
At time: 318.0790123939514 and batch: 50, loss is 5.903096389770508 and perplexity is 366.169517876243
At time: 320.5830991268158 and batch: 100, loss is 5.8890696620941165 and perplexity is 361.0692116916649
At time: 323.0656361579895 and batch: 150, loss is 5.92801875114441 and perplexity is 375.4099959058771
At time: 325.5394492149353 and batch: 200, loss is 5.943843393325806 and perplexity is 381.39797864618487
At time: 328.0192701816559 and batch: 250, loss is 5.899705314636231 and perplexity is 364.9299125145937
At time: 330.4925787448883 and batch: 300, loss is 5.905347766876221 and perplexity is 366.99483224393
At time: 332.9655976295471 and batch: 350, loss is 5.877654333114624 and perplexity is 356.9709240107058
At time: 335.48793959617615 and batch: 400, loss is 5.894569311141968 and perplexity is 363.0604361368636
At time: 337.96145963668823 and batch: 450, loss is 5.86649598121643 and perplexity is 353.00985740957606
At time: 340.4379367828369 and batch: 500, loss is 5.85148404121399 and perplexity is 347.75007317023733
At time: 342.91247177124023 and batch: 550, loss is 5.822811918258667 and perplexity is 337.9209249494103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.5893300374348955 and perplexity of 267.5563069442225
Finished 12 epochs...
Completing Train Step...
At time: 346.9836902618408 and batch: 50, loss is 5.873355808258057 and perplexity is 355.43976883270597
At time: 349.4344506263733 and batch: 100, loss is 5.862344913482666 and perplexity is 351.5475267973212
At time: 351.9052503108978 and batch: 150, loss is 5.89799355506897 and perplexity is 364.3057745849033
At time: 354.36364126205444 and batch: 200, loss is 5.9120284271240235 and perplexity is 369.454808012712
At time: 356.82559084892273 and batch: 250, loss is 5.86667142868042 and perplexity is 353.07179752728143
At time: 359.2799301147461 and batch: 300, loss is 5.873698329925537 and perplexity is 355.5615355076869
At time: 361.7357861995697 and batch: 350, loss is 5.839372901916504 and perplexity is 343.56382488112024
At time: 364.1976079940796 and batch: 400, loss is 5.854702243804931 and perplexity is 348.87100608387516
At time: 366.6658761501312 and batch: 450, loss is 5.828629179000854 and perplexity is 339.8924278937016
At time: 369.1271662712097 and batch: 500, loss is 5.816976022720337 and perplexity is 335.95459695428696
At time: 371.59701681137085 and batch: 550, loss is 5.784022798538208 and perplexity is 325.06423161414796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.54424082438151 and perplexity of 255.76033750877207
Finished 13 epochs...
Completing Train Step...
At time: 375.6625597476959 and batch: 50, loss is 5.833935527801514 and perplexity is 341.7008093790061
At time: 378.1604325771332 and batch: 100, loss is 5.817199573516846 and perplexity is 336.02970826731104
At time: 380.63510751724243 and batch: 150, loss is 5.85647234916687 and perplexity is 349.48909139904845
At time: 383.10926246643066 and batch: 200, loss is 5.869822254180908 and perplexity is 354.18601958826986
At time: 385.5826108455658 and batch: 250, loss is 5.830077495574951 and perplexity is 340.3850563853223
At time: 388.0613257884979 and batch: 300, loss is 5.838668489456177 and perplexity is 343.3218994596006
At time: 390.53448486328125 and batch: 350, loss is 5.805273942947387 and perplexity is 332.04614258449465
At time: 393.00068640708923 and batch: 400, loss is 5.823748321533203 and perplexity is 338.2375034094878
At time: 395.46634674072266 and batch: 450, loss is 5.793592271804809 and perplexity is 328.18985652819447
At time: 397.9693102836609 and batch: 500, loss is 5.784906101226807 and perplexity is 325.3514885726448
At time: 400.427814245224 and batch: 550, loss is 5.75602520942688 and perplexity is 316.08943934031345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.532807413736979 and perplexity of 252.8527778787072
Finished 14 epochs...
Completing Train Step...
At time: 404.49253487586975 and batch: 50, loss is 5.806650657653808 and perplexity is 332.5035902073842
At time: 406.95279264450073 and batch: 100, loss is 5.79414852142334 and perplexity is 328.3724627933182
At time: 409.4140667915344 and batch: 150, loss is 5.836128339767456 and perplexity is 342.4509171246351
At time: 411.88023805618286 and batch: 200, loss is 5.853980016708374 and perplexity is 348.6191329558255
At time: 414.34573006629944 and batch: 250, loss is 5.814345464706421 and perplexity is 335.0720102536623
At time: 416.8119287490845 and batch: 300, loss is 5.820678043365478 and perplexity is 337.2006127731355
At time: 419.2844784259796 and batch: 350, loss is 5.792755279541016 and perplexity is 327.91527908285343
At time: 421.75391125679016 and batch: 400, loss is 5.811400785446167 and perplexity is 334.0867819569631
At time: 424.21636843681335 and batch: 450, loss is 5.781009531021118 and perplexity is 324.08620039892855
At time: 426.6803448200226 and batch: 500, loss is 5.773178997039795 and perplexity is 321.55834254898775
At time: 429.14067459106445 and batch: 550, loss is 5.746039047241211 and perplexity is 312.9486273637833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.522765604654948 and perplexity of 250.3263845666943
Finished 15 epochs...
Completing Train Step...
At time: 433.205260515213 and batch: 50, loss is 5.7937490844726565 and perplexity is 328.2413248904972
At time: 435.6954412460327 and batch: 100, loss is 5.7764027976989745 and perplexity is 322.5966553030765
At time: 438.1623396873474 and batch: 150, loss is 5.8170977973937985 and perplexity is 335.99551020667735
At time: 440.6268701553345 and batch: 200, loss is 5.836611423492432 and perplexity is 342.6163895546018
At time: 443.0885851383209 and batch: 250, loss is 5.796290254592895 and perplexity is 329.07650265249026
At time: 445.56164145469666 and batch: 300, loss is 5.8047727108001705 and perplexity is 331.87975208708866
At time: 448.0265872478485 and batch: 350, loss is 5.778121147155762 and perplexity is 323.1514656329756
At time: 450.48440623283386 and batch: 400, loss is 5.793615913391113 and perplexity is 328.197615548729
At time: 452.9515652656555 and batch: 450, loss is 5.759442596435547 and perplexity is 317.1714871188418
At time: 455.41388416290283 and batch: 500, loss is 5.751459856033325 and perplexity is 314.649668378074
At time: 457.88211846351624 and batch: 550, loss is 5.723880910873413 and perplexity is 306.0905309191026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.516239929199219 and perplexity of 248.6981542524073
Finished 16 epochs...
Completing Train Step...
At time: 461.93512201309204 and batch: 50, loss is 5.773047695159912 and perplexity is 321.51612410586
At time: 464.4070768356323 and batch: 100, loss is 5.7597700881958005 and perplexity is 317.2753751777737
At time: 466.87731099128723 and batch: 150, loss is 5.799843769073487 and perplexity is 330.2479609341599
At time: 469.3529841899872 and batch: 200, loss is 5.812750759124756 and perplexity is 334.5380948807131
At time: 471.8309028148651 and batch: 250, loss is 5.772400703430176 and perplexity is 321.30817311112753
At time: 474.3045437335968 and batch: 300, loss is 5.783167848587036 and perplexity is 324.7864367324828
At time: 476.77076840400696 and batch: 350, loss is 5.749595584869385 and perplexity is 314.06362251850584
At time: 479.2391335964203 and batch: 400, loss is 5.768374729156494 and perplexity is 320.0171951380431
At time: 481.70890617370605 and batch: 450, loss is 5.737853031158448 and perplexity is 310.39728179235067
At time: 484.180055141449 and batch: 500, loss is 5.731044359207154 and perplexity is 308.2910669257512
At time: 486.64169120788574 and batch: 550, loss is 5.700617542266846 and perplexity is 299.052021218877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.49315185546875 and perplexity of 243.02197114036684
Finished 17 epochs...
Completing Train Step...
At time: 490.65718126296997 and batch: 50, loss is 5.751273603439331 and perplexity is 314.59106951840346
At time: 493.15348505973816 and batch: 100, loss is 5.738785829544067 and perplexity is 310.68695495803394
At time: 495.61950612068176 and batch: 150, loss is 5.7784872722625735 and perplexity is 323.26980115932855
At time: 498.0838198661804 and batch: 200, loss is 5.791532211303711 and perplexity is 327.5144614840823
At time: 500.55951166152954 and batch: 250, loss is 5.74936692237854 and perplexity is 313.9918161583204
At time: 503.0277738571167 and batch: 300, loss is 5.761195917129516 and perplexity is 317.7280782495317
At time: 505.4994993209839 and batch: 350, loss is 5.732924861907959 and perplexity is 308.87135455496076
At time: 507.97180223464966 and batch: 400, loss is 5.748054609298706 and perplexity is 313.58003084573784
At time: 510.4389810562134 and batch: 450, loss is 5.717211532592773 and perplexity is 304.0558898177
At time: 512.9130883216858 and batch: 500, loss is 5.7112274265289305 and perplexity is 302.2418203292187
At time: 515.3832232952118 and batch: 550, loss is 5.6829843330383305 and perplexity is 293.824994423029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.485707092285156 and perplexity of 241.2194481206442
Finished 18 epochs...
Completing Train Step...
At time: 519.4698948860168 and batch: 50, loss is 5.73347375869751 and perplexity is 309.04093958794255
At time: 521.9707000255585 and batch: 100, loss is 5.72027626991272 and perplexity is 304.98917065037733
At time: 524.446174621582 and batch: 150, loss is 5.762388353347778 and perplexity is 318.1071746968423
At time: 526.9232931137085 and batch: 200, loss is 5.779332771301269 and perplexity is 323.5432410457392
At time: 529.4020817279816 and batch: 250, loss is 5.73645281791687 and perplexity is 309.9629635482738
At time: 531.8788945674896 and batch: 300, loss is 5.747669696807861 and perplexity is 313.459353201642
At time: 534.3554139137268 and batch: 350, loss is 5.718749208450317 and perplexity is 304.52378886515777
At time: 536.8301811218262 and batch: 400, loss is 5.737869491577149 and perplexity is 310.40239110362324
At time: 539.3005146980286 and batch: 450, loss is 5.706015625 and perplexity is 300.67069370435104
At time: 541.7731363773346 and batch: 500, loss is 5.700296297073364 and perplexity is 298.9559676236633
At time: 544.2394127845764 and batch: 550, loss is 5.673106517791748 and perplexity is 290.9369327656332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.483243815104166 and perplexity of 240.62598898572944
Finished 19 epochs...
Completing Train Step...
At time: 548.3070101737976 and batch: 50, loss is 5.725247497558594 and perplexity is 306.50911611444934
At time: 550.7765321731567 and batch: 100, loss is 5.710807905197144 and perplexity is 302.1150500315139
At time: 553.2427334785461 and batch: 150, loss is 5.751282815933227 and perplexity is 314.59396770006106
At time: 555.7081401348114 and batch: 200, loss is 5.768006448745727 and perplexity is 319.89936077334124
At time: 558.175173997879 and batch: 250, loss is 5.727130651473999 and perplexity is 307.0868637797193
At time: 560.6409151554108 and batch: 300, loss is 5.739599685668946 and perplexity is 310.9399123607497
At time: 563.0955634117126 and batch: 350, loss is 5.709940719604492 and perplexity is 301.8531737768719
At time: 565.5617623329163 and batch: 400, loss is 5.729062805175781 and perplexity is 307.68077638078813
At time: 568.0254471302032 and batch: 450, loss is 5.698637838363648 and perplexity is 298.4605724051417
At time: 570.4889640808105 and batch: 500, loss is 5.691770420074463 and perplexity is 296.41794064274836
At time: 572.9567639827728 and batch: 550, loss is 5.664706563949585 and perplexity is 288.50331142241686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.479158528645834 and perplexity of 239.6449681314095
Finished 20 epochs...
Completing Train Step...
At time: 577.0318837165833 and batch: 50, loss is 5.715305824279785 and perplexity is 303.47699975386223
At time: 579.5306465625763 and batch: 100, loss is 5.702224454879761 and perplexity is 299.53295799188624
At time: 581.9938063621521 and batch: 150, loss is 5.7426511001586915 and perplexity is 311.89016798933517
At time: 584.4835822582245 and batch: 200, loss is 5.757830324172974 and perplexity is 316.66053233747397
At time: 586.9486021995544 and batch: 250, loss is 5.717103757858276 and perplexity is 304.0231220407037
At time: 589.4157350063324 and batch: 300, loss is 5.732856750488281 and perplexity is 308.85031760494064
At time: 591.8803043365479 and batch: 350, loss is 5.702678813934326 and perplexity is 299.6690844262877
At time: 594.3484282493591 and batch: 400, loss is 5.719986610412597 and perplexity is 304.90084043312487
At time: 596.815661907196 and batch: 450, loss is 5.688775911331176 and perplexity is 295.5316422043306
At time: 599.2815701961517 and batch: 500, loss is 5.683283452987671 and perplexity is 293.9128964864507
At time: 601.7490689754486 and batch: 550, loss is 5.654232807159424 and perplexity is 285.49736714922904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.468430582682291 and perplexity of 237.0878109100775
Finished 21 epochs...
Completing Train Step...
At time: 605.9208817481995 and batch: 50, loss is 5.702992286682129 and perplexity is 299.7630372426688
At time: 608.3954210281372 and batch: 100, loss is 5.688013248443603 and perplexity is 295.3063371155534
At time: 610.8722493648529 and batch: 150, loss is 5.728844709396363 and perplexity is 307.61367981905573
At time: 613.3414373397827 and batch: 200, loss is 5.74431040763855 and perplexity is 312.4081192793594
At time: 615.8175880908966 and batch: 250, loss is 5.70415397644043 and perplexity is 300.11147123986876
At time: 618.2902264595032 and batch: 300, loss is 5.719041366577148 and perplexity is 304.6127709626765
At time: 620.7613170146942 and batch: 350, loss is 5.687216262817383 and perplexity is 295.0710759718685
At time: 623.2255568504333 and batch: 400, loss is 5.709248771667481 and perplexity is 301.6443793417668
At time: 625.6902258396149 and batch: 450, loss is 5.674541292190551 and perplexity is 291.3546612298242
At time: 628.1553156375885 and batch: 500, loss is 5.6698611545562745 and perplexity is 289.9942672136331
At time: 630.618554353714 and batch: 550, loss is 5.642434225082398 and perplexity is 282.1486966582745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4614618937174475 and perplexity of 235.44136315262884
Finished 22 epochs...
Completing Train Step...
At time: 634.6458659172058 and batch: 50, loss is 5.689981880187989 and perplexity is 295.8882591522773
At time: 637.1295554637909 and batch: 100, loss is 5.677788925170899 and perplexity is 292.30241237755075
At time: 639.5935831069946 and batch: 150, loss is 5.718796453475952 and perplexity is 304.5381764392371
At time: 642.0602602958679 and batch: 200, loss is 5.733637962341309 and perplexity is 309.09168940284405
At time: 644.5237984657288 and batch: 250, loss is 5.695086469650269 and perplexity is 297.40250876522276
At time: 647.0113716125488 and batch: 300, loss is 5.708961791992188 and perplexity is 301.5578259558547
At time: 649.4669613838196 and batch: 350, loss is 5.676084785461426 and perplexity is 291.804712425017
At time: 651.9368481636047 and batch: 400, loss is 5.697119016647338 and perplexity is 298.0076080794136
At time: 654.4076037406921 and batch: 450, loss is 5.665809392929077 and perplexity is 288.8216567428904
At time: 656.8761043548584 and batch: 500, loss is 5.655882158279419 and perplexity is 285.96864109275936
At time: 659.3365726470947 and batch: 550, loss is 5.628113555908203 and perplexity is 278.1369326494249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.4578404744466145 and perplexity of 234.5902732697555
Finished 23 epochs...
Completing Train Step...
At time: 663.3798146247864 and batch: 50, loss is 5.67747317314148 and perplexity is 292.2101318672844
At time: 665.846601486206 and batch: 100, loss is 5.66486499786377 and perplexity is 288.54902375239703
At time: 668.315890789032 and batch: 150, loss is 5.705182571411132 and perplexity is 300.4203232043944
At time: 670.7836811542511 and batch: 200, loss is 5.7193474769592285 and perplexity is 304.7060303674903
At time: 673.2521376609802 and batch: 250, loss is 5.682389125823975 and perplexity is 293.6501597031953
At time: 675.7167258262634 and batch: 300, loss is 5.697815523147583 and perplexity is 298.2152446172684
At time: 678.1872458457947 and batch: 350, loss is 5.664302358627319 and perplexity is 288.3867204133893
At time: 680.6588847637177 and batch: 400, loss is 5.68268126487732 and perplexity is 293.73595891490555
At time: 683.1241416931152 and batch: 450, loss is 5.65071626663208 and perplexity is 284.4951672582369
At time: 685.5939180850983 and batch: 500, loss is 5.644555234909058 and perplexity is 282.7477719141309
At time: 688.0549974441528 and batch: 550, loss is 5.617255420684814 and perplexity is 275.1332210884325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.449097696940104 and perplexity of 232.5482422239813
Finished 24 epochs...
Completing Train Step...
At time: 692.0398881435394 and batch: 50, loss is 5.667445545196533 and perplexity is 289.2945997493414
At time: 694.5309262275696 and batch: 100, loss is 5.65509033203125 and perplexity is 285.74229324249984
At time: 696.9884784221649 and batch: 150, loss is 5.69427188873291 and perplexity is 297.16034899956026
At time: 699.4461829662323 and batch: 200, loss is 5.711231861114502 and perplexity is 302.2431606494061
At time: 701.8998687267303 and batch: 250, loss is 5.67106855392456 and perplexity is 290.34461757251233
At time: 704.3659880161285 and batch: 300, loss is 5.687888116836548 and perplexity is 295.26938727086133
At time: 706.8278965950012 and batch: 350, loss is 5.652918138504028 and perplexity is 285.12227932156867
At time: 709.3310825824738 and batch: 400, loss is 5.673913526535034 and perplexity is 291.171816177856
At time: 711.7814762592316 and batch: 450, loss is 5.64213716506958 and perplexity is 282.0648940106528
At time: 714.2343368530273 and batch: 500, loss is 5.635875520706176 and perplexity is 280.30422206047945
At time: 716.6921243667603 and batch: 550, loss is 5.606876459121704 and perplexity is 272.29239189234175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.440356953938802 and perplexity of 230.52445538846533
Finished 25 epochs...
Completing Train Step...
At time: 720.726800441742 and batch: 50, loss is 5.658791627883911 and perplexity is 286.80186970073663
At time: 723.1925501823425 and batch: 100, loss is 5.645279703140258 and perplexity is 282.9526879109277
At time: 725.6569354534149 and batch: 150, loss is 5.684409723281861 and perplexity is 294.2441083328631
At time: 728.112823009491 and batch: 200, loss is 5.699619035720826 and perplexity is 298.7535648482064
At time: 730.5722327232361 and batch: 250, loss is 5.665015697479248 and perplexity is 288.5925112560158
At time: 733.0252463817596 and batch: 300, loss is 5.67814606666565 and perplexity is 292.40682434183935
At time: 735.4798245429993 and batch: 350, loss is 5.643237867355347 and perplexity is 282.375534414163
At time: 737.9439458847046 and batch: 400, loss is 5.662335033416748 and perplexity is 287.81992766356717
At time: 740.4169454574585 and batch: 450, loss is 5.632770566940308 and perplexity is 279.4352401830158
At time: 742.8847861289978 and batch: 500, loss is 5.626512956619263 and perplexity is 277.6921029648048
At time: 745.347594499588 and batch: 550, loss is 5.595326223373413 and perplexity is 269.1654438303958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.435078938802083 and perplexity of 229.31094909293654
Finished 26 epochs...
Completing Train Step...
At time: 749.3237361907959 and batch: 50, loss is 5.645641222000122 and perplexity is 283.0549991366611
At time: 751.8069884777069 and batch: 100, loss is 5.633570156097412 and perplexity is 279.6587629224494
At time: 754.2732739448547 and batch: 150, loss is 5.673103713989258 and perplexity is 290.9361170370803
At time: 756.7395391464233 and batch: 200, loss is 5.688008375167847 and perplexity is 295.3048980098466
At time: 759.2012422084808 and batch: 250, loss is 5.64841703414917 and perplexity is 283.84179814003045
At time: 761.6560716629028 and batch: 300, loss is 5.661084585189819 and perplexity is 287.4602486721907
At time: 764.1083872318268 and batch: 350, loss is 5.627094411849976 and perplexity is 277.85361544218836
At time: 766.563202381134 and batch: 400, loss is 5.646574325561524 and perplexity is 283.3192420282471
At time: 769.0160856246948 and batch: 450, loss is 5.613466119766235 and perplexity is 274.09263132004793
At time: 771.5263812541962 and batch: 500, loss is 5.605807943344116 and perplexity is 272.0015985617857
At time: 773.9832427501678 and batch: 550, loss is 5.578802814483643 and perplexity is 264.75445578055843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.42210693359375 and perplexity of 226.3555365234726
Finished 27 epochs...
Completing Train Step...
At time: 777.9810004234314 and batch: 50, loss is 5.628112211227417 and perplexity is 278.1365586442871
At time: 780.4453780651093 and batch: 100, loss is 5.615145854949951 and perplexity is 274.5534212506592
At time: 782.9134421348572 and batch: 150, loss is 5.659323978424072 and perplexity is 286.9545894776122
At time: 785.3722155094147 and batch: 200, loss is 5.671462440490723 and perplexity is 290.4590029428832
At time: 787.8271400928497 and batch: 250, loss is 5.6337532043457035 and perplexity is 279.70995865462334
At time: 790.2817249298096 and batch: 300, loss is 5.646851491928101 and perplexity is 283.39777947660826
At time: 792.7409236431122 and batch: 350, loss is 5.615147180557251 and perplexity is 274.55378520092
At time: 795.2038671970367 and batch: 400, loss is 5.632493934631348 and perplexity is 279.35795005828487
At time: 797.6654994487762 and batch: 450, loss is 5.601768121719361 and perplexity is 270.9049771907314
At time: 800.1301040649414 and batch: 500, loss is 5.594374961853028 and perplexity is 268.9095188461561
At time: 802.5856897830963 and batch: 550, loss is 5.568387575149536 and perplexity is 262.0112849467986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.413963317871094 and perplexity of 224.51966946124713
Finished 28 epochs...
Completing Train Step...
At time: 806.5343554019928 and batch: 50, loss is 5.617718458175659 and perplexity is 275.26064758411906
At time: 809.0061073303223 and batch: 100, loss is 5.60595700263977 and perplexity is 272.04214595039167
At time: 811.4591026306152 and batch: 150, loss is 5.643368425369263 and perplexity is 282.4124032098207
At time: 813.9098019599915 and batch: 200, loss is 5.660746603012085 and perplexity is 287.36310864805574
At time: 816.3593623638153 and batch: 250, loss is 5.625904331207275 and perplexity is 277.5231439158331
At time: 818.8073453903198 and batch: 300, loss is 5.637861680984497 and perplexity is 280.86150441502105
At time: 821.2565338611603 and batch: 350, loss is 5.602946615219116 and perplexity is 271.22442514209706
At time: 823.7059495449066 and batch: 400, loss is 5.622970046997071 and perplexity is 276.71000570940265
At time: 826.1540257930756 and batch: 450, loss is 5.593574199676514 and perplexity is 268.69427246664094
At time: 828.6016631126404 and batch: 500, loss is 5.5852109146118165 and perplexity is 266.45647637750847
At time: 831.0476806163788 and batch: 550, loss is 5.559494132995606 and perplexity is 259.691433761979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.402003987630208 and perplexity of 221.85055680100228
Finished 29 epochs...
Completing Train Step...
At time: 835.0307149887085 and batch: 50, loss is 5.608048734664917 and perplexity is 272.6117807732708
At time: 837.4815368652344 and batch: 100, loss is 5.596889142990112 and perplexity is 269.58645670163224
At time: 839.9293918609619 and batch: 150, loss is 5.634647035598755 and perplexity is 279.96008392569803
At time: 842.3782842159271 and batch: 200, loss is 5.6460889625549315 and perplexity is 283.18176271549606
At time: 844.8263947963715 and batch: 250, loss is 5.612480812072754 and perplexity is 273.8226987468405
At time: 847.2756922245026 and batch: 300, loss is 5.625866556167603 and perplexity is 277.5126606660655
At time: 849.7254905700684 and batch: 350, loss is 5.591237659454346 and perplexity is 268.06719037810973
At time: 852.1758599281311 and batch: 400, loss is 5.612022905349732 and perplexity is 273.6973421951626
At time: 854.6243500709534 and batch: 450, loss is 5.579067678451538 and perplexity is 264.82458898370356
At time: 857.0748767852783 and batch: 500, loss is 5.5708772087097165 and perplexity is 262.66440971826916
At time: 859.5222201347351 and batch: 550, loss is 5.544918909072876 and perplexity is 255.93382349074216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.402354939778646 and perplexity of 221.92842939452137
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 863.4680514335632 and batch: 50, loss is 5.591097278594971 and perplexity is 268.0295615168019
At time: 865.9543142318726 and batch: 100, loss is 5.55783088684082 and perplexity is 259.25986198788183
At time: 868.4100875854492 and batch: 150, loss is 5.5827588367462155 and perplexity is 265.80390475482864
At time: 870.8650321960449 and batch: 200, loss is 5.5836506271362305 and perplexity is 266.04105184975083
At time: 873.3317909240723 and batch: 250, loss is 5.534292449951172 and perplexity is 253.22855236101472
At time: 875.8026392459869 and batch: 300, loss is 5.530236339569091 and perplexity is 252.2035096492724
At time: 878.2729661464691 and batch: 350, loss is 5.470228672027588 and perplexity is 237.5144994738933
At time: 880.7371890544891 and batch: 400, loss is 5.467309894561768 and perplexity is 236.82225824552117
At time: 883.2071883678436 and batch: 450, loss is 5.428713703155518 and perplexity is 227.85596643190246
At time: 885.6743013858795 and batch: 500, loss is 5.418894662857055 and perplexity is 225.62958785347996
At time: 888.1431603431702 and batch: 550, loss is 5.435976524353027 and perplexity is 229.51686768846855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.293021647135417 and perplexity of 198.9436556378808
Finished 31 epochs...
Completing Train Step...
At time: 892.1657016277313 and batch: 50, loss is 5.5136917781829835 and perplexity is 248.06524052026103
At time: 894.6296820640564 and batch: 100, loss is 5.498523120880127 and perplexity is 244.33081858492196
At time: 897.115579366684 and batch: 150, loss is 5.532724657058716 and perplexity is 252.8318534885488
At time: 899.5770950317383 and batch: 200, loss is 5.5395551109313965 and perplexity is 254.56472120057947
At time: 902.0408310890198 and batch: 250, loss is 5.496649618148804 and perplexity is 243.87349266340655
At time: 904.5065240859985 and batch: 300, loss is 5.495580472946167 and perplexity is 243.6128958214303
At time: 906.9763808250427 and batch: 350, loss is 5.445843725204468 and perplexity is 231.79276663132745
At time: 909.44122838974 and batch: 400, loss is 5.45840859413147 and perplexity is 234.72358648722147
At time: 911.9121301174164 and batch: 450, loss is 5.432391357421875 and perplexity is 228.69548468168554
At time: 914.3807299137115 and batch: 500, loss is 5.426089687347412 and perplexity is 227.25885253478344
At time: 916.8513596057892 and batch: 550, loss is 5.429773378372192 and perplexity is 228.09754772875206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.285224405924479 and perplexity of 197.39847586569834
Finished 32 epochs...
Completing Train Step...
At time: 920.9027140140533 and batch: 50, loss is 5.493828830718994 and perplexity is 243.18654670059945
At time: 923.3993058204651 and batch: 100, loss is 5.477467365264893 and perplexity is 239.24003184010363
At time: 925.8530633449554 and batch: 150, loss is 5.514397068023682 and perplexity is 248.24026012675043
At time: 928.3178634643555 and batch: 200, loss is 5.521223802566528 and perplexity is 249.94072820347284
At time: 930.7947347164154 and batch: 250, loss is 5.482863960266113 and perplexity is 240.5346033973796
At time: 933.2736275196075 and batch: 300, loss is 5.484946517944336 and perplexity is 241.0360525497204
At time: 935.7406346797943 and batch: 350, loss is 5.440611972808838 and perplexity is 230.58325097126692
At time: 938.2103052139282 and batch: 400, loss is 5.459453105926514 and perplexity is 234.96888612876492
At time: 940.6730673313141 and batch: 450, loss is 5.435083417892456 and perplexity is 229.31197619970135
At time: 943.1363663673401 and batch: 500, loss is 5.4271400260925295 and perplexity is 227.49767671395986
At time: 945.5919859409332 and batch: 550, loss is 5.425202779769897 and perplexity is 227.05738429147516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.280864969889323 and perplexity of 196.53980286164284
Finished 33 epochs...
Completing Train Step...
At time: 949.5976841449738 and batch: 50, loss is 5.482425212860107 and perplexity is 240.4290926120687
At time: 952.0658931732178 and batch: 100, loss is 5.464918708801269 and perplexity is 236.2566487420327
At time: 954.5261015892029 and batch: 150, loss is 5.502948684692383 and perplexity is 245.41451643211806
At time: 956.9849667549133 and batch: 200, loss is 5.5131278991699215 and perplexity is 247.92540116720497
At time: 959.4682765007019 and batch: 250, loss is 5.476604270935058 and perplexity is 239.03363420827856
At time: 961.9402456283569 and batch: 300, loss is 5.480838832855224 and perplexity is 240.0479830791531
At time: 964.4096558094025 and batch: 350, loss is 5.438317499160767 and perplexity is 230.0547902796372
At time: 966.8672468662262 and batch: 400, loss is 5.459409875869751 and perplexity is 234.95872863003632
At time: 969.3295607566833 and batch: 450, loss is 5.434612598419189 and perplexity is 229.20403706776972
At time: 971.7892615795135 and batch: 500, loss is 5.426006956100464 and perplexity is 227.24005190424376
At time: 974.2645153999329 and batch: 550, loss is 5.420939826965332 and perplexity is 226.09150958018486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.277831522623698 and perplexity of 195.94451308062315
Finished 34 epochs...
Completing Train Step...
At time: 978.2778232097626 and batch: 50, loss is 5.474295949935913 and perplexity is 238.48250418815041
At time: 980.7636349201202 and batch: 100, loss is 5.457706060409546 and perplexity is 234.5587431631602
At time: 983.2252333164215 and batch: 150, loss is 5.495029859542846 and perplexity is 243.47879621767683
At time: 985.6832184791565 and batch: 200, loss is 5.505745544433593 and perplexity is 246.1018671767863
At time: 988.1488087177277 and batch: 250, loss is 5.4714897346496585 and perplexity is 237.8142090679444
At time: 990.6187648773193 and batch: 300, loss is 5.476300621032715 and perplexity is 238.9610626873197
At time: 993.0866904258728 and batch: 350, loss is 5.435019683837891 and perplexity is 229.29736168342384
At time: 995.5534362792969 and batch: 400, loss is 5.456258363723755 and perplexity is 234.2194189266097
At time: 998.0170817375183 and batch: 450, loss is 5.431199369430542 and perplexity is 228.42304481505514
At time: 1000.4854938983917 and batch: 500, loss is 5.422282238006591 and perplexity is 226.39522112623754
At time: 1002.9509572982788 and batch: 550, loss is 5.415005884170532 and perplexity is 224.75386816481293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.27292226155599 and perplexity of 194.98492766601802
Finished 35 epochs...
Completing Train Step...
At time: 1006.9498932361603 and batch: 50, loss is 5.466777801513672 and perplexity is 236.6962802872445
At time: 1009.410920381546 and batch: 100, loss is 5.44971118927002 and perplexity is 232.69095255829288
At time: 1011.8766751289368 and batch: 150, loss is 5.4874907493591305 and perplexity is 241.65008483560612
At time: 1014.343935251236 and batch: 200, loss is 5.498272228240967 and perplexity is 244.26952547033616
At time: 1016.799158334732 and batch: 250, loss is 5.467630195617676 and perplexity is 236.89812481432207
At time: 1019.2596333026886 and batch: 300, loss is 5.472717418670654 and perplexity is 238.10634906349205
At time: 1021.7609589099884 and batch: 350, loss is 5.432140665054321 and perplexity is 228.6381596549554
At time: 1024.2227482795715 and batch: 400, loss is 5.452673263549805 and perplexity is 233.38122225436547
At time: 1026.6791574954987 and batch: 450, loss is 5.42763689994812 and perplexity is 227.61074244910435
At time: 1029.1363921165466 and batch: 500, loss is 5.419061822891235 and perplexity is 225.667307255598
At time: 1031.5974102020264 and batch: 550, loss is 5.410208778381348 and perplexity is 223.67828199565233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.270021057128906 and perplexity of 194.42005633047796
Finished 36 epochs...
Completing Train Step...
At time: 1035.589993238449 and batch: 50, loss is 5.460903072357178 and perplexity is 235.30983024493037
At time: 1038.0762724876404 and batch: 100, loss is 5.444161624908447 and perplexity is 231.40319569051513
At time: 1040.5338723659515 and batch: 150, loss is 5.48223557472229 and perplexity is 240.38350240962586
At time: 1042.996172428131 and batch: 200, loss is 5.493496942520141 and perplexity is 243.10584934762016
At time: 1045.4679381847382 and batch: 250, loss is 5.463358592987061 and perplexity is 235.8883483784316
At time: 1047.9305448532104 and batch: 300, loss is 5.467865467071533 and perplexity is 236.9538667375478
At time: 1050.3886675834656 and batch: 350, loss is 5.426004667282104 and perplexity is 227.2395317936361
At time: 1052.8455440998077 and batch: 400, loss is 5.447314205169678 and perplexity is 232.13386397768323
At time: 1055.3176758289337 and batch: 450, loss is 5.422418689727783 and perplexity is 226.4261152515601
At time: 1057.7799186706543 and batch: 500, loss is 5.413175783157349 and perplexity is 224.34292203413008
At time: 1060.2351877689362 and batch: 550, loss is 5.403608484268188 and perplexity is 222.20680099340146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.262416585286458 and perplexity of 192.94720172487249
Finished 37 epochs...
Completing Train Step...
At time: 1064.2133255004883 and batch: 50, loss is 5.454154806137085 and perplexity is 233.72724273295233
At time: 1066.7138411998749 and batch: 100, loss is 5.4368917274475095 and perplexity is 229.72701838662917
At time: 1069.1749520301819 and batch: 150, loss is 5.4750762557983395 and perplexity is 238.6686661064269
At time: 1071.6338596343994 and batch: 200, loss is 5.48832329750061 and perplexity is 241.85135393610665
At time: 1074.097645521164 and batch: 250, loss is 5.457177972793579 and perplexity is 234.4349082963866
At time: 1076.5759291648865 and batch: 300, loss is 5.462989225387573 and perplexity is 235.8012349548724
At time: 1079.0472514629364 and batch: 350, loss is 5.423047828674316 and perplexity is 226.56861356009472
At time: 1081.5584661960602 and batch: 400, loss is 5.443961572647095 and perplexity is 231.3569075881073
At time: 1084.0315170288086 and batch: 450, loss is 5.419011850357055 and perplexity is 225.65603037014247
At time: 1086.5036177635193 and batch: 500, loss is 5.409594421386719 and perplexity is 223.54090588187077
At time: 1088.98437666893 and batch: 550, loss is 5.3987819194793705 and perplexity is 221.13688954737677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.258833312988282 and perplexity of 192.25705658933992
Finished 38 epochs...
Completing Train Step...
At time: 1093.0564875602722 and batch: 50, loss is 5.448653154373169 and perplexity is 232.44488760591244
At time: 1095.5143139362335 and batch: 100, loss is 5.431384859085083 and perplexity is 228.46541885657874
At time: 1097.970825433731 and batch: 150, loss is 5.4700014019012455 and perplexity is 237.46052565714012
At time: 1100.4293971061707 and batch: 200, loss is 5.481476812362671 and perplexity is 240.2011776354593
At time: 1102.8961417675018 and batch: 250, loss is 5.452272090911865 and perplexity is 233.28761487140392
At time: 1105.3575801849365 and batch: 300, loss is 5.458804655075073 and perplexity is 234.81656974467444
At time: 1107.8134744167328 and batch: 350, loss is 5.419643793106079 and perplexity is 225.79867712985396
At time: 1110.268634557724 and batch: 400, loss is 5.440094976425171 and perplexity is 230.46407107482304
At time: 1112.7377235889435 and batch: 450, loss is 5.4143573188781735 and perplexity is 224.60814786626983
At time: 1115.1985425949097 and batch: 500, loss is 5.405135288238525 and perplexity is 222.54632634776706
At time: 1117.6535732746124 and batch: 550, loss is 5.394449348449707 and perplexity is 220.1808707712262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.257704671223959 and perplexity of 192.04018965135015
Finished 39 epochs...
Completing Train Step...
At time: 1121.6490142345428 and batch: 50, loss is 5.44454360961914 and perplexity is 231.49160505771277
At time: 1124.1390838623047 and batch: 100, loss is 5.42745849609375 and perplexity is 227.5701394373299
At time: 1126.6031424999237 and batch: 150, loss is 5.464760589599609 and perplexity is 236.2192949825979
At time: 1129.0655071735382 and batch: 200, loss is 5.477678489685059 and perplexity is 239.29054658536685
At time: 1131.5358855724335 and batch: 250, loss is 5.448634691238404 and perplexity is 232.44059598424565
At time: 1134.0057017803192 and batch: 300, loss is 5.455457677841187 and perplexity is 234.0319578032044
At time: 1136.467934370041 and batch: 350, loss is 5.4154413795471195 and perplexity is 224.85176875134792
At time: 1138.9318222999573 and batch: 400, loss is 5.436086759567261 and perplexity is 229.5421699241216
At time: 1141.398344039917 and batch: 450, loss is 5.411233062744141 and perplexity is 223.90750953924976
At time: 1143.8677344322205 and batch: 500, loss is 5.401105108261109 and perplexity is 221.65122951148584
At time: 1146.3823359012604 and batch: 550, loss is 5.388984413146972 and perplexity is 218.98087848419897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.254731750488281 and perplexity of 191.4701171994293
Finished 40 epochs...
Completing Train Step...
At time: 1150.46204662323 and batch: 50, loss is 5.43991382598877 and perplexity is 230.42232618893925
At time: 1152.9192218780518 and batch: 100, loss is 5.423278064727783 and perplexity is 226.6207838290282
At time: 1155.3861050605774 and batch: 150, loss is 5.460059013366699 and perplexity is 235.11129866513318
At time: 1157.8524658679962 and batch: 200, loss is 5.471842441558838 and perplexity is 237.8981025766241
At time: 1160.3082013130188 and batch: 250, loss is 5.443074703216553 and perplexity is 231.15181517775997
At time: 1162.765763759613 and batch: 300, loss is 5.449016752243042 and perplexity is 232.52941943877488
At time: 1165.2274765968323 and batch: 350, loss is 5.407372159957886 and perplexity is 223.04469111264493
At time: 1167.6925661563873 and batch: 400, loss is 5.426751632690429 and perplexity is 227.40933527405775
At time: 1170.1510634422302 and batch: 450, loss is 5.402786436080933 and perplexity is 222.02421135447216
At time: 1172.6052877902985 and batch: 500, loss is 5.393443374633789 and perplexity is 219.95948595279947
At time: 1175.0606653690338 and batch: 550, loss is 5.382697658538818 and perplexity is 217.60851780471742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.2483159383138025 and perplexity of 190.24561318465112
Finished 41 epochs...
Completing Train Step...
At time: 1179.036877632141 and batch: 50, loss is 5.433000173568725 and perplexity is 228.83476057787675
At time: 1181.5178883075714 and batch: 100, loss is 5.416707000732422 and perplexity is 225.13652607287145
At time: 1183.9878571033478 and batch: 150, loss is 5.451571960449218 and perplexity is 233.12434026910512
At time: 1186.4537105560303 and batch: 200, loss is 5.4627072143554685 and perplexity is 235.7347457810122
At time: 1188.916714668274 and batch: 250, loss is 5.4366320037841795 and perplexity is 229.66736059145384
At time: 1191.37562251091 and batch: 300, loss is 5.4416068744659425 and perplexity is 230.81277278664382
At time: 1193.8314208984375 and batch: 350, loss is 5.400543756484986 and perplexity is 221.52684011649
At time: 1196.284913778305 and batch: 400, loss is 5.420174064636231 and perplexity is 225.91844349142133
At time: 1198.7384779453278 and batch: 450, loss is 5.396681213378907 and perplexity is 220.67283352777793
At time: 1201.2040371894836 and batch: 500, loss is 5.387593555450439 and perplexity is 218.67651895342848
At time: 1203.670330286026 and batch: 550, loss is 5.375841388702392 and perplexity is 216.1216381367054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.244014485677083 and perplexity of 189.42903817860602
Finished 42 epochs...
Completing Train Step...
At time: 1207.6710906028748 and batch: 50, loss is 5.425395288467407 and perplexity is 227.10109902038292
At time: 1210.1320641040802 and batch: 100, loss is 5.408400993347168 and perplexity is 223.27428502485324
At time: 1212.5877287387848 and batch: 150, loss is 5.44460883140564 and perplexity is 231.50670384613392
At time: 1215.0431785583496 and batch: 200, loss is 5.45682463645935 and perplexity is 234.35208855774704
At time: 1217.4999310970306 and batch: 250, loss is 5.428969993591308 and perplexity is 227.91437122081143
At time: 1219.9678208827972 and batch: 300, loss is 5.433437738418579 and perplexity is 228.93491253542052
At time: 1222.4390079975128 and batch: 350, loss is 5.392116117477417 and perplexity is 219.66773680685657
At time: 1224.898252248764 and batch: 400, loss is 5.41121208190918 and perplexity is 223.9028118220268
At time: 1227.3616707324982 and batch: 450, loss is 5.386818637847901 and perplexity is 218.5071283100018
At time: 1229.8285863399506 and batch: 500, loss is 5.379023160934448 and perplexity is 216.81038309741825
At time: 1232.2905213832855 and batch: 550, loss is 5.366584596633911 and perplexity is 214.13027610278013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.236442057291667 and perplexity of 188.00001775880375
Finished 43 epochs...
Completing Train Step...
At time: 1236.2695395946503 and batch: 50, loss is 5.415462579727173 and perplexity is 224.85653569986064
At time: 1238.763563632965 and batch: 100, loss is 5.398238105773926 and perplexity is 221.0166649689111
At time: 1241.2210726737976 and batch: 150, loss is 5.434333486557007 and perplexity is 229.14007242922392
At time: 1243.676192522049 and batch: 200, loss is 5.44599778175354 and perplexity is 231.82847857581345
At time: 1246.1409072875977 and batch: 250, loss is 5.418308048248291 and perplexity is 225.49726905494023
At time: 1248.6121854782104 and batch: 300, loss is 5.424539833068848 and perplexity is 226.90690723225643
At time: 1251.0795814990997 and batch: 350, loss is 5.383730936050415 and perplexity is 217.83348399873577
At time: 1253.5401072502136 and batch: 400, loss is 5.4039631175994876 and perplexity is 222.28561690602388
At time: 1256.0024571418762 and batch: 450, loss is 5.378518857955933 and perplexity is 216.70107254058553
At time: 1258.4723098278046 and batch: 500, loss is 5.369249296188355 and perplexity is 214.70162985907345
At time: 1260.9371192455292 and batch: 550, loss is 5.357509860992431 and perplexity is 212.1958907470112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.234773763020834 and perplexity of 187.6866398822012
Finished 44 epochs...
Completing Train Step...
At time: 1264.9607276916504 and batch: 50, loss is 5.4053231143951415 and perplexity is 222.5881302947285
At time: 1267.4276661872864 and batch: 100, loss is 5.3883496761322025 and perplexity is 218.84192731848745
At time: 1269.9364008903503 and batch: 150, loss is 5.424342927932739 and perplexity is 226.86223249529073
At time: 1272.397377729416 and batch: 200, loss is 5.43492000579834 and perplexity is 229.27450691101535
At time: 1274.8632907867432 and batch: 250, loss is 5.409137411117554 and perplexity is 223.43876873294494
At time: 1277.3321950435638 and batch: 300, loss is 5.414914569854736 and perplexity is 224.73334585612352
At time: 1279.7955226898193 and batch: 350, loss is 5.37303427696228 and perplexity is 215.5158112586905
At time: 1282.2523217201233 and batch: 400, loss is 5.39209924697876 and perplexity is 219.6640309338579
At time: 1284.7194871902466 and batch: 450, loss is 5.366044111251831 and perplexity is 214.0145730893963
At time: 1287.1877048015594 and batch: 500, loss is 5.357289924621582 and perplexity is 212.14922628468543
At time: 1289.64608168602 and batch: 550, loss is 5.347804851531983 and perplexity is 210.14648843983113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.227915954589844 and perplexity of 186.40392419717563
Finished 45 epochs...
Completing Train Step...
At time: 1293.6162922382355 and batch: 50, loss is 5.39533429145813 and perplexity is 220.3758045333171
At time: 1296.102989912033 and batch: 100, loss is 5.37827073097229 and perplexity is 216.64730982736975
At time: 1298.577900648117 and batch: 150, loss is 5.41472469329834 and perplexity is 224.6906783132162
At time: 1301.0409095287323 and batch: 200, loss is 5.428146448135376 and perplexity is 227.72675064369182
At time: 1303.499314069748 and batch: 250, loss is 5.399329929351807 and perplexity is 221.25810795741768
At time: 1305.9526822566986 and batch: 300, loss is 5.404089803695679 and perplexity is 222.31377918691624
At time: 1308.4099061489105 and batch: 350, loss is 5.362094879150391 and perplexity is 213.1710466044454
At time: 1310.8677072525024 and batch: 400, loss is 5.382436227798462 and perplexity is 217.55163568449126
At time: 1313.3284363746643 and batch: 450, loss is 5.356362209320069 and perplexity is 211.95250346675036
At time: 1315.7868692874908 and batch: 500, loss is 5.3483327293395995 and perplexity is 210.25744939176246
At time: 1318.2537655830383 and batch: 550, loss is 5.339614648818969 and perplexity is 208.43237514954106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.221089172363281 and perplexity of 185.13571900621682
Finished 46 epochs...
Completing Train Step...
At time: 1322.2918968200684 and batch: 50, loss is 5.387471370697021 and perplexity is 218.6498016491387
At time: 1324.7548072338104 and batch: 100, loss is 5.36959056854248 and perplexity is 214.77491409396046
At time: 1327.2229130268097 and batch: 150, loss is 5.406584119796753 and perplexity is 222.86899217632222
At time: 1329.7020192146301 and batch: 200, loss is 5.420155868530274 and perplexity is 225.91433269288626
At time: 1332.2196252346039 and batch: 250, loss is 5.391928977966309 and perplexity is 219.62663214025793
At time: 1334.682949066162 and batch: 300, loss is 5.395698623657227 and perplexity is 220.45610916270329
At time: 1337.1510496139526 and batch: 350, loss is 5.355292043685913 and perplexity is 211.72580050796086
At time: 1339.619831085205 and batch: 400, loss is 5.375450277328492 and perplexity is 216.03712703358818
At time: 1342.0928745269775 and batch: 450, loss is 5.347931642532348 and perplexity is 210.17313481254814
At time: 1344.557853460312 and batch: 500, loss is 5.338680896759033 and perplexity is 208.23784182696753
At time: 1347.0195038318634 and batch: 550, loss is 5.33170407295227 and perplexity is 206.79005942868358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.218004862467448 and perplexity of 184.5655827664109
Finished 47 epochs...
Completing Train Step...
At time: 1351.0148632526398 and batch: 50, loss is 5.380019006729126 and perplexity is 217.0264003477512
At time: 1353.5154600143433 and batch: 100, loss is 5.361334228515625 and perplexity is 213.0089595661516
At time: 1355.988481760025 and batch: 150, loss is 5.396967372894287 and perplexity is 220.73599019488753
At time: 1358.4591856002808 and batch: 200, loss is 5.410629453659058 and perplexity is 223.7723977137386
At time: 1360.9335134029388 and batch: 250, loss is 5.382946462631225 and perplexity is 217.66266643041033
At time: 1363.40012383461 and batch: 300, loss is 5.386367645263672 and perplexity is 218.40860543374603
At time: 1365.8595621585846 and batch: 350, loss is 5.3469416046142575 and perplexity is 209.9651584089703
At time: 1368.3124265670776 and batch: 400, loss is 5.368161563873291 and perplexity is 214.4682189255
At time: 1370.7674219608307 and batch: 450, loss is 5.338917331695557 and perplexity is 208.28708234874208
At time: 1373.2222139835358 and batch: 500, loss is 5.331468572616577 and perplexity is 206.7413660341506
At time: 1375.6853613853455 and batch: 550, loss is 5.325050392150879 and perplexity is 205.41871169468365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.208731079101563 and perplexity of 182.86187365995463
Finished 48 epochs...
Completing Train Step...
At time: 1379.7102355957031 and batch: 50, loss is 5.371590452194214 and perplexity is 215.20486871977153
At time: 1382.169831752777 and batch: 100, loss is 5.353822727203369 and perplexity is 211.41493673405478
At time: 1384.6369152069092 and batch: 150, loss is 5.390272493362427 and perplexity is 219.26312516049296
At time: 1387.1108186244965 and batch: 200, loss is 5.402811317443848 and perplexity is 222.02973568817694
At time: 1389.5865590572357 and batch: 250, loss is 5.37615963935852 and perplexity is 216.19042993578282
At time: 1392.0580427646637 and batch: 300, loss is 5.379855070114136 and perplexity is 216.9908246904709
At time: 1394.5724256038666 and batch: 350, loss is 5.34115571975708 and perplexity is 208.7538318556203
At time: 1397.0499255657196 and batch: 400, loss is 5.361675224304199 and perplexity is 213.0816071098414
At time: 1399.5216903686523 and batch: 450, loss is 5.33159462928772 and perplexity is 206.76742880519885
At time: 1401.9926002025604 and batch: 500, loss is 5.324226264953613 and perplexity is 205.24949028710867
At time: 1404.4597473144531 and batch: 550, loss is 5.31917371749878 and perplexity is 204.21507291934196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.210109456380208 and perplexity of 183.11410010340316
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1408.483684539795 and batch: 50, loss is 5.366449384689331 and perplexity is 214.10132508906048
At time: 1410.9727430343628 and batch: 100, loss is 5.347030220031738 and perplexity is 209.98376538355927
At time: 1413.4341158866882 and batch: 150, loss is 5.380895662307739 and perplexity is 217.21674117180496
At time: 1415.9005060195923 and batch: 200, loss is 5.386583080291748 and perplexity is 218.45566336657134
At time: 1418.3659641742706 and batch: 250, loss is 5.354263620376587 and perplexity is 211.50816868753284
At time: 1420.8243503570557 and batch: 300, loss is 5.357510366439819 and perplexity is 212.195998000897
At time: 1423.2813646793365 and batch: 350, loss is 5.309300279617309 and perplexity is 202.20868933355786
At time: 1425.7437863349915 and batch: 400, loss is 5.321277093887329 and perplexity is 204.6450661421715
At time: 1428.2067227363586 and batch: 450, loss is 5.290095052719116 and perplexity is 198.3622793873319
At time: 1430.6615455150604 and batch: 500, loss is 5.285668754577637 and perplexity is 197.4862091032141
At time: 1433.1177945137024 and batch: 550, loss is 5.29583345413208 and perplexity is 199.50383398837167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 5.183367411295573 and perplexity of 178.28215064333747
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fae881507f0>
SETTINGS FOR THIS RUN
{'batch_size': 80, 'anneal': 4.082427181902782, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 8.431778148042385, 'dropout': 0.07097106708434686, 'wordvec_source': 'glove'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 550 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.9573333263397217 and batch: 50, loss is 7.460912666320801 and perplexity is 1738.7342175824517
At time: 5.385543346405029 and batch: 100, loss is 6.446492166519165 and perplexity is 630.4867666463743
At time: 7.81412935256958 and batch: 150, loss is 6.043807773590088 and perplexity is 421.4949404068244
At time: 10.246054410934448 and batch: 200, loss is 5.743450603485107 and perplexity is 312.1396249236556
At time: 12.70288348197937 and batch: 250, loss is 5.552627143859863 and perplexity is 257.91424446118833
At time: 15.143877267837524 and batch: 300, loss is 5.460187244415283 and perplexity is 235.14144916656883
At time: 17.584322929382324 and batch: 350, loss is 5.365325555801392 and perplexity is 213.86084698840764
At time: 20.02355432510376 and batch: 400, loss is 5.331932668685913 and perplexity is 206.83733615745209
At time: 22.458244800567627 and batch: 450, loss is 5.246099891662598 and perplexity is 189.82448682082222
At time: 24.906522512435913 and batch: 500, loss is 5.225319709777832 and perplexity is 185.9206016593415
At time: 27.347145557403564 and batch: 550, loss is 5.196674327850342 and perplexity is 180.67039116262782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.944099426269531 and perplexity of 140.34440343942762
Finished 1 epochs...
Completing Train Step...
At time: 31.362471103668213 and batch: 50, loss is 5.16560564994812 and perplexity is 175.14350210889063
At time: 33.79075002670288 and batch: 100, loss is 5.108466682434082 and perplexity is 165.41652425965052
At time: 36.23237228393555 and batch: 150, loss is 5.060757751464844 and perplexity is 157.7099760226215
At time: 38.696847677230835 and batch: 200, loss is 5.020879011154175 and perplexity is 151.54445453087402
At time: 41.17745757102966 and batch: 250, loss is 4.978993225097656 and perplexity is 145.32799539859562
At time: 43.67364048957825 and batch: 300, loss is 4.954276428222657 and perplexity is 141.77998125608764
At time: 46.13032627105713 and batch: 350, loss is 4.889496393203736 and perplexity is 132.8866345848028
At time: 48.59351181983948 and batch: 400, loss is 4.873099670410157 and perplexity is 130.7254954926077
At time: 51.06228017807007 and batch: 450, loss is 4.80818024635315 and perplexity is 122.50847929857481
At time: 53.520262241363525 and batch: 500, loss is 4.803045711517334 and perplexity is 121.88106735656437
At time: 55.973122358322144 and batch: 550, loss is 4.79652585029602 and perplexity is 121.08900458734824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.7513575236002605 and perplexity of 115.74129947288655
Finished 2 epochs...
Completing Train Step...
At time: 59.98224329948425 and batch: 50, loss is 4.837025623321534 and perplexity is 126.09374311745164
At time: 62.441752672195435 and batch: 100, loss is 4.812937459945679 and perplexity is 123.0926667520087
At time: 64.89600110054016 and batch: 150, loss is 4.790304222106934 and perplexity is 120.33797256663318
At time: 67.3497884273529 and batch: 200, loss is 4.770766839981079 and perplexity is 118.00970182963393
At time: 69.80500435829163 and batch: 250, loss is 4.753874425888061 and perplexity is 116.03297592090665
At time: 72.2611129283905 and batch: 300, loss is 4.74156626701355 and perplexity is 114.61357663333821
At time: 74.72183537483215 and batch: 350, loss is 4.68590934753418 and perplexity is 108.40880878239342
At time: 77.1846559047699 and batch: 400, loss is 4.670268335342407 and perplexity is 106.72637704936858
At time: 79.64271187782288 and batch: 450, loss is 4.618746490478515 and perplexity is 101.36688809867232
At time: 82.10032558441162 and batch: 500, loss is 4.611627511978149 and perplexity is 100.64782194673163
At time: 84.55209064483643 and batch: 550, loss is 4.617019319534302 and perplexity is 101.19196126260292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.685986328125 and perplexity of 108.41715447776775
Finished 3 epochs...
Completing Train Step...
At time: 88.55350589752197 and batch: 50, loss is 4.6630620098114015 and perplexity is 105.96003659938847
At time: 91.0457820892334 and batch: 100, loss is 4.646911582946777 and perplexity is 104.26248179358448
At time: 93.50565075874329 and batch: 150, loss is 4.631203556060791 and perplexity is 102.63751981169611
At time: 95.96938824653625 and batch: 200, loss is 4.613466863632202 and perplexity is 100.8331190455271
At time: 98.43111300468445 and batch: 250, loss is 4.606674842834472 and perplexity is 100.15057894104625
At time: 100.88770174980164 and batch: 300, loss is 4.611770601272583 and perplexity is 100.66222460296865
At time: 103.34243702888489 and batch: 350, loss is 4.5520021343231205 and perplexity is 94.8220649033197
At time: 105.79845213890076 and batch: 400, loss is 4.5424489402771 and perplexity is 93.92052446927607
At time: 108.26703929901123 and batch: 450, loss is 4.490373773574829 and perplexity is 89.15476334628525
At time: 110.73158860206604 and batch: 500, loss is 4.485661401748657 and perplexity is 88.7356213035376
At time: 113.19057655334473 and batch: 550, loss is 4.502237033843994 and perplexity is 90.21872807511294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.662545776367187 and perplexity of 105.90535060133072
Finished 4 epochs...
Completing Train Step...
At time: 117.22654128074646 and batch: 50, loss is 4.542423982620239 and perplexity is 93.91818046230473
At time: 119.68598699569702 and batch: 100, loss is 4.531625509262085 and perplexity is 92.90946359577798
At time: 122.14545035362244 and batch: 150, loss is 4.518263444900513 and perplexity is 91.67625879285302
At time: 124.59613418579102 and batch: 200, loss is 4.507364730834961 and perplexity is 90.6825304784466
At time: 127.054842710495 and batch: 250, loss is 4.503124122619629 and perplexity is 90.29879560439304
At time: 129.509845495224 and batch: 300, loss is 4.515422534942627 and perplexity is 91.41618439523636
At time: 131.96986365318298 and batch: 350, loss is 4.469733943939209 and perplexity is 87.33348431936724
At time: 134.42587089538574 and batch: 400, loss is 4.456524829864502 and perplexity is 86.18747192968232
At time: 136.89400148391724 and batch: 450, loss is 4.401862659454346 and perplexity is 81.60272528022611
At time: 139.3592402935028 and batch: 500, loss is 4.402892665863037 and perplexity is 81.6868199118014
At time: 141.8171546459198 and batch: 550, loss is 4.430146780014038 and perplexity is 83.94373726899023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.649051920572917 and perplexity of 104.48587769240363
Finished 5 epochs...
Completing Train Step...
At time: 145.8161928653717 and batch: 50, loss is 4.466830806732178 and perplexity is 87.0803109080116
At time: 148.29332208633423 and batch: 100, loss is 4.452160358428955 and perplexity is 85.81212885322455
At time: 150.75038957595825 and batch: 150, loss is 4.438331546783448 and perplexity is 84.63361657964757
At time: 153.2100965976715 and batch: 200, loss is 4.435215320587158 and perplexity is 84.37028959289687
At time: 155.67456126213074 and batch: 250, loss is 4.430547037124634 and perplexity is 83.97734307175425
At time: 158.1340126991272 and batch: 300, loss is 4.44487642288208 and perplexity is 85.18934973250215
At time: 160.58754014968872 and batch: 350, loss is 4.398861560821533 and perplexity is 81.35819456688097
At time: 163.05233216285706 and batch: 400, loss is 4.37956057548523 and perplexity is 79.80295832298515
At time: 165.51903009414673 and batch: 450, loss is 4.317536821365357 and perplexity is 75.00365317614688
At time: 167.96927952766418 and batch: 500, loss is 4.320702543258667 and perplexity is 75.24147011561814
At time: 170.4247853755951 and batch: 550, loss is 4.3448766040802 and perplexity is 77.082525223327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.657482401529948 and perplexity of 105.37046741377027
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 174.42672061920166 and batch: 50, loss is 4.375376014709473 and perplexity is 79.46971571703786
At time: 176.88300108909607 and batch: 100, loss is 4.337545423507691 and perplexity is 76.51948570588337
At time: 179.33995938301086 and batch: 150, loss is 4.309641523361206 and perplexity is 74.4138085465146
At time: 181.7939956188202 and batch: 200, loss is 4.2871974658966066 and perplexity is 72.762263762284
At time: 184.2543134689331 and batch: 250, loss is 4.257473745346069 and perplexity is 70.6313251672109
At time: 186.71720623970032 and batch: 300, loss is 4.243678436279297 and perplexity is 69.6636343504485
At time: 189.16656160354614 and batch: 350, loss is 4.175697374343872 and perplexity is 65.0852125956475
At time: 191.61516904830933 and batch: 400, loss is 4.1310869359970095 and perplexity is 62.245543100156446
At time: 194.0751757621765 and batch: 450, loss is 4.03777319431305 and perplexity is 56.69994236897806
At time: 196.53427910804749 and batch: 500, loss is 4.010412673950196 and perplexity is 55.169632936776395
At time: 198.99283599853516 and batch: 550, loss is 4.07691915512085 and perplexity is 58.96353223665897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.491923522949219 and perplexity of 89.29303800286151
Finished 7 epochs...
Completing Train Step...
At time: 202.98922562599182 and batch: 50, loss is 4.241267232894898 and perplexity is 69.49586350563114
At time: 205.47621893882751 and batch: 100, loss is 4.223473730087281 and perplexity is 68.27022519969108
At time: 207.93825888633728 and batch: 150, loss is 4.208117475509644 and perplexity is 67.22985875184777
At time: 210.39311933517456 and batch: 200, loss is 4.196121764183045 and perplexity is 66.42820658067562
At time: 212.84373712539673 and batch: 250, loss is 4.182700347900391 and perplexity is 65.5426022930932
At time: 215.29230523109436 and batch: 300, loss is 4.179766464233398 and perplexity is 65.3505897317239
At time: 217.7452380657196 and batch: 350, loss is 4.121013193130493 and perplexity is 61.62164527362096
At time: 220.20395278930664 and batch: 400, loss is 4.090509524345398 and perplexity is 59.770338389364895
At time: 222.65840911865234 and batch: 450, loss is 4.012302632331848 and perplexity is 55.27399984044408
At time: 225.10956811904907 and batch: 500, loss is 4.0014786577224735 and perplexity is 54.678941726233525
At time: 227.55999779701233 and batch: 550, loss is 4.0582012176513675 and perplexity is 57.87012164666529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.482125854492187 and perplexity of 88.42244626987865
Finished 8 epochs...
Completing Train Step...
At time: 231.56067872047424 and batch: 50, loss is 4.190867500305176 and perplexity is 66.08009060185142
At time: 234.01903295516968 and batch: 100, loss is 4.174042048454285 and perplexity is 64.97756447914635
At time: 236.48087739944458 and batch: 150, loss is 4.162196884155273 and perplexity is 64.21243504022053
At time: 238.9388918876648 and batch: 200, loss is 4.152563052177429 and perplexity is 63.59679348530802
At time: 241.40955877304077 and batch: 250, loss is 4.144611530303955 and perplexity is 63.09310738023242
At time: 243.87376475334167 and batch: 300, loss is 4.145553531646729 and perplexity is 63.15256917425226
At time: 246.3392150402069 and batch: 350, loss is 4.0909654235839845 and perplexity is 59.79759385352726
At time: 248.79622530937195 and batch: 400, loss is 4.0663515043258665 and perplexity is 58.34370703473726
At time: 251.26000547409058 and batch: 450, loss is 3.994340376853943 and perplexity is 54.29001785797004
At time: 253.73139548301697 and batch: 500, loss is 3.9879534912109373 and perplexity is 53.944378676268954
At time: 256.19722509384155 and batch: 550, loss is 4.036838641166687 and perplexity is 56.64697801229652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.479688517252604 and perplexity of 88.20719337727671
Finished 9 epochs...
Completing Train Step...
At time: 260.2489950656891 and batch: 50, loss is 4.154947290420532 and perplexity is 63.748604297130825
At time: 262.73051404953003 and batch: 100, loss is 4.13862313747406 and perplexity is 62.716410099750576
At time: 265.1884756088257 and batch: 150, loss is 4.129435014724732 and perplexity is 62.14280324584365
At time: 267.6747603416443 and batch: 200, loss is 4.121724820137024 and perplexity is 61.66551250728734
At time: 270.1293206214905 and batch: 250, loss is 4.117432622909546 and perplexity is 61.401399184188065
At time: 272.5792100429535 and batch: 300, loss is 4.120164756774902 and perplexity is 61.56938540220182
At time: 275.0312135219574 and batch: 350, loss is 4.067721171379089 and perplexity is 58.42367323906204
At time: 277.495148897171 and batch: 400, loss is 4.045447511672974 and perplexity is 57.13674967614003
At time: 279.95424222946167 and batch: 450, loss is 3.976936354637146 and perplexity is 53.35332791083991
At time: 282.41052412986755 and batch: 500, loss is 3.9728377628326417 and perplexity is 53.13510191345192
At time: 284.86185359954834 and batch: 550, loss is 4.016387171745301 and perplexity is 55.50023038058475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.478399658203125 and perplexity of 88.09357996942522
Finished 10 epochs...
Completing Train Step...
At time: 288.8750557899475 and batch: 50, loss is 4.125848889350891 and perplexity is 61.9203504724903
At time: 291.33867740631104 and batch: 100, loss is 4.109651741981506 and perplexity is 60.925496082018796
At time: 293.79247760772705 and batch: 150, loss is 4.104576144218445 and perplexity is 60.61704621629802
At time: 296.2469606399536 and batch: 200, loss is 4.096270670890808 and perplexity is 60.11567788845159
At time: 298.7066333293915 and batch: 250, loss is 4.094186587333679 and perplexity is 59.99052225533725
At time: 301.1616635322571 and batch: 300, loss is 4.098418622016907 and perplexity is 60.24494220348497
At time: 303.6251492500305 and batch: 350, loss is 4.046902194023132 and perplexity is 57.2199259803227
At time: 306.08799290657043 and batch: 400, loss is 4.027510728836059 and perplexity is 56.12103674810918
At time: 308.54403948783875 and batch: 450, loss is 3.9614789485931396 and perplexity is 52.534965035468524
At time: 311.0005099773407 and batch: 500, loss is 3.9572209548950195 and perplexity is 52.311747053052315
At time: 313.4555735588074 and batch: 550, loss is 3.9971440124511717 and perplexity is 54.442440854037855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4785888671875 and perplexity of 88.11024964319743
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 317.47169828414917 and batch: 50, loss is 4.113265571594238 and perplexity is 61.146068759915735
At time: 319.96173667907715 and batch: 100, loss is 4.093364934921265 and perplexity is 59.94125114264044
At time: 322.42226457595825 and batch: 150, loss is 4.084027600288391 and perplexity is 59.38416452200517
At time: 324.88137578964233 and batch: 200, loss is 4.067931180000305 and perplexity is 58.435944002563296
At time: 327.3445293903351 and batch: 250, loss is 4.055735411643982 and perplexity is 57.727600939481654
At time: 329.8443179130554 and batch: 300, loss is 4.051020436286926 and perplexity is 57.45605738726526
At time: 332.2991154193878 and batch: 350, loss is 3.9892763757705687 and perplexity is 54.01578808468659
At time: 334.74823689460754 and batch: 400, loss is 3.9616174030303957 and perplexity is 52.542239238049966
At time: 337.1970331668854 and batch: 450, loss is 3.8826857805252075 and perplexity is 48.554446693625124
At time: 339.64561653137207 and batch: 500, loss is 3.863755602836609 and perplexity is 47.643947535311504
At time: 342.09938883781433 and batch: 550, loss is 3.922615990638733 and perplexity is 50.53246447565068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.449555460611979 and perplexity of 85.58888791225789
Finished 12 epochs...
Completing Train Step...
At time: 346.0935444831848 and batch: 50, loss is 4.086371097564697 and perplexity is 59.52349434558286
At time: 348.5613045692444 and batch: 100, loss is 4.064562058448791 and perplexity is 58.23939748471205
At time: 351.0201246738434 and batch: 150, loss is 4.057815170288086 and perplexity is 57.84778535050231
At time: 353.4768786430359 and batch: 200, loss is 4.045712246894836 and perplexity is 57.1518777886258
At time: 355.93081164360046 and batch: 250, loss is 4.037607069015503 and perplexity is 56.69052385652938
At time: 358.39585185050964 and batch: 300, loss is 4.03710825920105 and perplexity is 56.66225311829027
At time: 360.8545639514923 and batch: 350, loss is 3.9793110275268555 and perplexity is 53.48017516296254
At time: 363.3055934906006 and batch: 400, loss is 3.956411747932434 and perplexity is 52.269433145775146
At time: 365.75871562957764 and batch: 450, loss is 3.881893849372864 and perplexity is 48.51601013626602
At time: 368.2137794494629 and batch: 500, loss is 3.8674699306488036 and perplexity is 47.82124183546695
At time: 370.66270565986633 and batch: 550, loss is 3.9235588932037353 and perplexity is 50.58013413641159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.447056070963542 and perplexity of 85.3752350438957
Finished 13 epochs...
Completing Train Step...
At time: 374.6483323574066 and batch: 50, loss is 4.073469309806824 and perplexity is 58.76046764338829
At time: 377.1346197128296 and batch: 100, loss is 4.050511679649353 and perplexity is 57.42683367121081
At time: 379.59015798568726 and batch: 150, loss is 4.045246376991272 and perplexity is 57.12525864984113
At time: 382.04273796081543 and batch: 200, loss is 4.034604382514954 and perplexity is 56.5205552946942
At time: 384.49767446517944 and batch: 250, loss is 4.028163905143738 and perplexity is 56.15770565400198
At time: 386.9574637413025 and batch: 300, loss is 4.0296097135543825 and perplexity is 56.23895766042232
At time: 389.4236035346985 and batch: 350, loss is 3.9737146377563475 and perplexity is 53.18171518590646
At time: 391.9084165096283 and batch: 400, loss is 3.9528695440292356 and perplexity is 52.084611686564735
At time: 394.3636100292206 and batch: 450, loss is 3.8805658531188967 and perplexity is 48.45162381840459
At time: 396.82532811164856 and batch: 500, loss is 3.8676931190490724 and perplexity is 47.831916173080934
At time: 399.2850573062897 and batch: 550, loss is 3.9215090036392213 and perplexity is 50.47655664475306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.446054077148437 and perplexity of 85.28973243011738
Finished 14 epochs...
Completing Train Step...
At time: 403.293568611145 and batch: 50, loss is 4.063269619941711 and perplexity is 58.16417526529108
At time: 405.7532923221588 and batch: 100, loss is 4.039955644607544 and perplexity is 56.82382230663133
At time: 408.2091679573059 and batch: 150, loss is 4.035911703109742 and perplexity is 56.594494100991255
At time: 410.67654156684875 and batch: 200, loss is 4.026269607543945 and perplexity is 56.051426940508215
At time: 413.1322977542877 and batch: 250, loss is 4.021047654151917 and perplexity is 55.75949190044738
At time: 415.58534955978394 and batch: 300, loss is 4.023505721092224 and perplexity is 55.89672105422334
At time: 418.0389368534088 and batch: 350, loss is 3.9689885807037353 and perplexity is 52.93096835441302
At time: 420.49769473075867 and batch: 400, loss is 3.949506039619446 and perplexity is 51.90971915625551
At time: 422.9603588581085 and batch: 450, loss is 3.8784198856353758 and perplexity is 48.34775969354322
At time: 425.4185016155243 and batch: 500, loss is 3.866406183242798 and perplexity is 47.77039916018876
At time: 427.87630820274353 and batch: 550, loss is 3.918290295600891 and perplexity is 50.31434853644516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.445636494954427 and perplexity of 85.25412439168151
Finished 15 epochs...
Completing Train Step...
At time: 431.91752648353577 and batch: 50, loss is 4.0545245885849 and perplexity is 57.65774532904514
At time: 434.41525769233704 and batch: 100, loss is 4.03100465297699 and perplexity is 56.317462341456526
At time: 436.8858153820038 and batch: 150, loss is 4.028218231201172 and perplexity is 56.16075656361589
At time: 439.3659224510193 and batch: 200, loss is 4.019247212409973 and perplexity is 55.65919050422518
At time: 441.8360357284546 and batch: 250, loss is 4.014712262153625 and perplexity is 55.40735031694806
At time: 444.3106324672699 and batch: 300, loss is 4.018100419044495 and perplexity is 55.59539749951568
At time: 446.77686882019043 and batch: 350, loss is 3.9645649099349978 and perplexity is 52.697336313550196
At time: 449.241503238678 and batch: 400, loss is 3.9460836124420164 and perplexity is 51.732365585594486
At time: 451.6965699195862 and batch: 450, loss is 3.875851426124573 and perplexity is 48.223739768589994
At time: 454.18629813194275 and batch: 500, loss is 3.8644182300567627 and perplexity is 47.675528173763794
At time: 456.6521472930908 and batch: 550, loss is 3.9146998643875124 and perplexity is 50.13402224726614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.445447285970052 and perplexity of 85.23799507134584
Finished 16 epochs...
Completing Train Step...
At time: 460.854455947876 and batch: 50, loss is 4.046694421768189 and perplexity is 57.20803850226091
At time: 463.3196153640747 and batch: 100, loss is 4.023065614700317 and perplexity is 55.872125962626235
At time: 465.7920615673065 and batch: 150, loss is 4.0213434028625485 and perplexity is 55.77598513708893
At time: 468.255006313324 and batch: 200, loss is 4.0128866386413575 and perplexity is 55.306289632906235
At time: 470.7239751815796 and batch: 250, loss is 4.008856816291809 and perplexity is 55.083863580617916
At time: 473.188688993454 and batch: 300, loss is 4.013119249343872 and perplexity is 55.319155964156394
At time: 475.65463066101074 and batch: 350, loss is 3.9602717781066894 and perplexity is 52.47158463933964
At time: 478.124449968338 and batch: 400, loss is 3.9425916624069215 and perplexity is 51.55203378782893
At time: 480.58401465415955 and batch: 450, loss is 3.87323504447937 and perplexity is 48.09773297381007
At time: 483.0387010574341 and batch: 500, loss is 3.8619953203201294 and perplexity is 47.56015449879434
At time: 485.48884654045105 and batch: 550, loss is 3.9109748888015745 and perplexity is 49.94762162279045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.445433044433594 and perplexity of 85.23678115997545
Finished 17 epochs...
Completing Train Step...
At time: 489.5077705383301 and batch: 50, loss is 4.039428815841675 and perplexity is 56.79389376675365
At time: 491.96767711639404 and batch: 100, loss is 4.015803422927856 and perplexity is 55.46784164109613
At time: 494.42646861076355 and batch: 150, loss is 4.0150448608398435 and perplexity is 55.4257817938429
At time: 496.88602662086487 and batch: 200, loss is 4.0070645093917845 and perplexity is 54.98522481371892
At time: 499.36181449890137 and batch: 250, loss is 4.003518719673156 and perplexity is 54.790604015019326
At time: 501.8289837837219 and batch: 300, loss is 4.0083921432495115 and perplexity is 55.05827354010785
At time: 504.28838753700256 and batch: 350, loss is 3.9560311937332155 and perplexity is 52.249545577889506
At time: 506.7458794116974 and batch: 400, loss is 3.939195041656494 and perplexity is 51.377228122475096
At time: 509.1988670825958 and batch: 450, loss is 3.870362024307251 and perplexity is 47.95974553201017
At time: 511.6543478965759 and batch: 500, loss is 3.8593685817718506 and perplexity is 47.43539034075878
At time: 514.110445022583 and batch: 550, loss is 3.9071043014526365 and perplexity is 49.75466865198443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.445404052734375 and perplexity of 85.23431003667486
Finished 18 epochs...
Completing Train Step...
At time: 518.1244874000549 and batch: 50, loss is 4.032696919441223 and perplexity is 56.41284717984597
At time: 520.5772387981415 and batch: 100, loss is 4.009036364555359 and perplexity is 55.093754680611795
At time: 523.0362315177917 and batch: 150, loss is 4.009187979698181 and perplexity is 55.102108361352535
At time: 525.4955854415894 and batch: 200, loss is 4.001517639160157 and perplexity is 54.681073231537304
At time: 527.9691791534424 and batch: 250, loss is 3.998369646072388 and perplexity is 54.50920824779289
At time: 530.4238178730011 and batch: 300, loss is 4.003811116218567 and perplexity is 54.80662694076419
At time: 532.8865184783936 and batch: 350, loss is 3.9519002485275267 and perplexity is 52.03415076647093
At time: 535.3469769954681 and batch: 400, loss is 3.93580885887146 and perplexity is 51.20354965653288
At time: 537.808835029602 and batch: 450, loss is 3.867346487045288 and perplexity is 47.81533897339297
At time: 540.2722117900848 and batch: 500, loss is 3.856557598114014 and perplexity is 47.30223746666138
At time: 542.7368862628937 and batch: 550, loss is 3.903204288482666 and perplexity is 49.56100269427173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.445445760091146 and perplexity of 85.23786500858641
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 546.7280011177063 and batch: 50, loss is 4.034468879699707 and perplexity is 56.51289711919636
At time: 549.206253528595 and batch: 100, loss is 4.013050346374512 and perplexity is 55.31534444136208
At time: 551.6582245826721 and batch: 150, loss is 4.01048131942749 and perplexity is 55.173420212549686
At time: 554.1210660934448 and batch: 200, loss is 3.999031138420105 and perplexity is 54.54527760041406
At time: 556.5831305980682 and batch: 250, loss is 3.99745325088501 and perplexity is 54.45927915257223
At time: 559.0405027866364 and batch: 300, loss is 3.994966721534729 and perplexity is 54.32403277329378
At time: 561.4943587779999 and batch: 350, loss is 3.939981598854065 and perplexity is 51.41765514804578
At time: 563.9514067173004 and batch: 400, loss is 3.918250632286072 and perplexity is 50.31235294217544
At time: 566.4362239837646 and batch: 450, loss is 3.846237554550171 and perplexity is 46.81658659469332
At time: 568.8944532871246 and batch: 500, loss is 3.8296439838409424 and perplexity is 46.046142145085405
At time: 571.3521265983582 and batch: 550, loss is 3.882224678993225 and perplexity is 48.53206332476956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.434366353352865 and perplexity of 84.29869237762988
Finished 20 epochs...
Completing Train Step...
At time: 575.3551120758057 and batch: 50, loss is 4.027909164428711 and perplexity is 56.14340182187096
At time: 577.8130757808685 and batch: 100, loss is 4.004116516113282 and perplexity is 54.823367435004045
At time: 580.2752549648285 and batch: 150, loss is 4.001822032928467 and perplexity is 54.69772034298345
At time: 582.746636390686 and batch: 200, loss is 3.9926354598999025 and perplexity is 54.19753674479928
At time: 585.2092275619507 and batch: 250, loss is 3.992415509223938 and perplexity is 54.18561727085287
At time: 587.6727480888367 and batch: 300, loss is 3.991210789680481 and perplexity is 54.1203781040924
At time: 590.1413104534149 and batch: 350, loss is 3.9376224184036257 and perplexity is 51.29649459721013
At time: 592.6102488040924 and batch: 400, loss is 3.9175898456573486 and perplexity is 50.27911819383959
At time: 595.0757875442505 and batch: 450, loss is 3.8466461372375487 and perplexity is 46.835718949766395
At time: 597.5450360774994 and batch: 500, loss is 3.8315354108810427 and perplexity is 46.13331748033496
At time: 599.992397069931 and batch: 550, loss is 3.8831984901428225 and perplexity is 48.5793474082954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.433674112955729 and perplexity of 84.24035761051101
Finished 21 epochs...
Completing Train Step...
At time: 603.9764955043793 and batch: 50, loss is 4.0248404264450075 and perplexity is 55.9713765174578
At time: 606.4544582366943 and batch: 100, loss is 4.000443277359008 and perplexity is 54.622357521817115
At time: 608.9180135726929 and batch: 150, loss is 3.9981255435943606 and perplexity is 54.495904038845545
At time: 611.3782818317413 and batch: 200, loss is 3.9897774267196655 and perplexity is 54.04285952809248
At time: 613.830888748169 and batch: 250, loss is 3.990612058639526 and perplexity is 54.08798425234326
At time: 616.2836735248566 and batch: 300, loss is 3.9895656156539916 and perplexity is 54.03141386462565
At time: 618.7370181083679 and batch: 350, loss is 3.9364901781082153 and perplexity is 51.23844750684245
At time: 621.1933212280273 and batch: 400, loss is 3.9171845960617064 and perplexity is 50.25874672956492
At time: 623.6575057506561 and batch: 450, loss is 3.846741027832031 and perplexity is 46.84016342984689
At time: 626.1474821567535 and batch: 500, loss is 3.832163052558899 and perplexity is 46.16228176176766
At time: 628.5999782085419 and batch: 550, loss is 3.8829718446731567 and perplexity is 48.56833836690779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.433390808105469 and perplexity of 84.21649528892763
Finished 22 epochs...
Completing Train Step...
At time: 632.6420726776123 and batch: 50, loss is 4.022263870239258 and perplexity is 55.827348747448774
At time: 635.098878622055 and batch: 100, loss is 3.9976636219024657 and perplexity is 54.47073701169595
At time: 637.5490210056305 and batch: 150, loss is 3.9953383016586304 and perplexity is 54.344222254895236
At time: 639.9992821216583 and batch: 200, loss is 3.9876385879516603 and perplexity is 53.92739408999527
At time: 642.4546134471893 and batch: 250, loss is 3.989298677444458 and perplexity is 54.016992740610206
At time: 644.9323048591614 and batch: 300, loss is 3.988226156234741 and perplexity is 53.959089427027266
At time: 647.404937505722 and batch: 350, loss is 3.9355245208740235 and perplexity is 51.1889926114205
At time: 649.8803696632385 and batch: 400, loss is 3.9166721391677854 and perplexity is 50.23299788647309
At time: 652.3584246635437 and batch: 450, loss is 3.8465476274490356 and perplexity is 46.83110540024141
At time: 654.8299443721771 and batch: 500, loss is 3.8322934770584105 and perplexity is 46.16830284690272
At time: 657.3053200244904 and batch: 550, loss is 3.88240683555603 and perplexity is 48.54090456383105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.433218892415365 and perplexity of 84.20201839646002
Finished 23 epochs...
Completing Train Step...
At time: 661.3805558681488 and batch: 50, loss is 4.019980235099792 and perplexity is 55.700004910877276
At time: 663.8803210258484 and batch: 100, loss is 3.995280089378357 and perplexity is 54.341058845873604
At time: 666.3402376174927 and batch: 150, loss is 3.992964186668396 and perplexity is 54.215355854562425
At time: 668.7996792793274 and batch: 200, loss is 3.985764255523682 and perplexity is 53.82641089409925
At time: 671.2540085315704 and batch: 250, loss is 3.9882191133499147 and perplexity is 53.95870940071334
At time: 673.721765756607 and batch: 300, loss is 3.986985273361206 and perplexity is 53.892174042749176
At time: 676.1949303150177 and batch: 350, loss is 3.9345745515823363 and perplexity is 51.14038773059461
At time: 678.6688938140869 and batch: 400, loss is 3.916075496673584 and perplexity is 50.20303568457316
At time: 681.1272714138031 and batch: 450, loss is 3.8462127351760866 and perplexity is 46.815424650736695
At time: 683.5900900363922 and batch: 500, loss is 3.832197332382202 and perplexity is 46.163864223752825
At time: 686.0588343143463 and batch: 550, loss is 3.881737871170044 and perplexity is 48.50844328634414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.433113606770833 and perplexity of 84.19315359935841
Finished 24 epochs...
Completing Train Step...
At time: 690.0441360473633 and batch: 50, loss is 4.017881422042847 and perplexity is 55.58322360722948
At time: 692.5087594985962 and batch: 100, loss is 3.993093662261963 and perplexity is 54.22237587439295
At time: 694.9638383388519 and batch: 150, loss is 3.9908381843566896 and perplexity is 54.100216319512604
At time: 697.4253134727478 and batch: 200, loss is 3.9840940284729003 and perplexity is 53.7365836034451
At time: 699.883228302002 and batch: 250, loss is 3.9872305250167845 and perplexity is 53.90539280855174
At time: 702.3431859016418 and batch: 300, loss is 3.985799956321716 and perplexity is 53.82833257422604
At time: 704.794463634491 and batch: 350, loss is 3.933640704154968 and perplexity is 51.0926527031631
At time: 707.254070520401 and batch: 400, loss is 3.9154251766204835 and perplexity is 50.17039825727794
At time: 709.7247498035431 and batch: 450, loss is 3.8457584047317503 and perplexity is 46.7941598090516
At time: 712.2100346088409 and batch: 500, loss is 3.8319353675842285 and perplexity is 46.15177250025987
At time: 714.6861634254456 and batch: 550, loss is 3.880973467826843 and perplexity is 48.47137743855641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4330388387044275 and perplexity of 84.18685887538429
Finished 25 epochs...
Completing Train Step...
At time: 718.7890367507935 and batch: 50, loss is 4.0159053611755375 and perplexity is 55.47349622387992
At time: 721.2926177978516 and batch: 100, loss is 3.991059350967407 and perplexity is 54.11218280423973
At time: 723.7588739395142 and batch: 150, loss is 3.9888668918609618 and perplexity is 53.99367401658771
At time: 726.2308399677277 and batch: 200, loss is 3.9825421285629274 and perplexity is 53.65325448014225
At time: 728.688558101654 and batch: 250, loss is 3.9863065338134764 and perplexity is 53.855607703827225
At time: 731.1499130725861 and batch: 300, loss is 3.984680380821228 and perplexity is 53.76810141480043
At time: 733.615984916687 and batch: 350, loss is 3.9327102756500243 and perplexity is 51.04513675122081
At time: 736.0768411159515 and batch: 400, loss is 3.914743494987488 and perplexity is 50.13620967245497
At time: 738.5363826751709 and batch: 450, loss is 3.8452495527267456 and perplexity is 46.7703545641972
At time: 741.0085601806641 and batch: 500, loss is 3.831573848724365 and perplexity is 46.1350907796449
At time: 743.4792296886444 and batch: 550, loss is 3.8801658821105955 and perplexity is 48.43224844862351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432985941569011 and perplexity of 84.18240574948989
Finished 26 epochs...
Completing Train Step...
At time: 747.5394780635834 and batch: 50, loss is 4.014030981063843 and perplexity is 55.369615192515056
At time: 750.0016605854034 and batch: 100, loss is 3.989134225845337 and perplexity is 54.008110290160914
At time: 752.4908783435822 and batch: 150, loss is 3.987004556655884 and perplexity is 53.89321327144192
At time: 754.9435155391693 and batch: 200, loss is 3.981072397232056 and perplexity is 53.57445653112116
At time: 757.4106843471527 and batch: 250, loss is 3.98538526058197 and perplexity is 53.80601482188813
At time: 759.8733944892883 and batch: 300, loss is 3.983572773933411 and perplexity is 53.708580464320896
At time: 762.326562166214 and batch: 350, loss is 3.9317929267883303 and perplexity is 50.99833202454073
At time: 764.792986869812 and batch: 400, loss is 3.9140320682525633 and perplexity is 50.10055411716769
At time: 767.268404006958 and batch: 450, loss is 3.844695792198181 and perplexity is 46.74446215769059
At time: 769.7346963882446 and batch: 500, loss is 3.8311433172225953 and perplexity is 46.11523244485334
At time: 772.1919097900391 and batch: 550, loss is 3.8793197441101075 and perplexity is 48.39128541539552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432944742838542 and perplexity of 84.1789376126871
Finished 27 epochs...
Completing Train Step...
At time: 776.186571598053 and batch: 50, loss is 4.0122335290908815 and perplexity is 55.27018035988466
At time: 778.6758413314819 and batch: 100, loss is 3.987303614616394 and perplexity is 53.90933287611613
At time: 781.1410393714905 and batch: 150, loss is 3.9852315950393677 and perplexity is 53.797747326656086
At time: 783.6144306659698 and batch: 200, loss is 3.9796501207351684 and perplexity is 53.49831300217494
At time: 786.0780267715454 and batch: 250, loss is 3.9844999885559083 and perplexity is 53.7584029399755
At time: 788.546224117279 and batch: 300, loss is 3.9824740648269654 and perplexity is 53.64960276347196
At time: 791.0094549655914 and batch: 350, loss is 3.9308670377731323 and perplexity is 50.95113508206279
At time: 793.4724025726318 and batch: 400, loss is 3.913301510810852 and perplexity is 50.063966150956325
At time: 795.9279887676239 and batch: 450, loss is 3.844107151031494 and perplexity is 46.71695453980236
At time: 798.3934936523438 and batch: 500, loss is 3.83066388130188 and perplexity is 46.093128445076275
At time: 800.8546805381775 and batch: 550, loss is 3.878473148345947 and perplexity is 48.350334894855045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432920837402344 and perplexity of 84.17692530251747
Finished 28 epochs...
Completing Train Step...
At time: 804.8963720798492 and batch: 50, loss is 4.010509004592896 and perplexity is 55.1749477189588
At time: 807.3614671230316 and batch: 100, loss is 3.9855335569381714 and perplexity is 53.81399464950297
At time: 809.828768491745 and batch: 150, loss is 3.9835304832458496 and perplexity is 53.706309139553404
At time: 812.290730714798 and batch: 200, loss is 3.9782795190811155 and perplexity is 53.42503835253395
At time: 814.8078553676605 and batch: 250, loss is 3.983628888130188 and perplexity is 53.71159436273415
At time: 817.2793872356415 and batch: 300, loss is 3.9813926887512205 and perplexity is 53.59161872349755
At time: 819.745566368103 and batch: 350, loss is 3.9299585914611814 and perplexity is 50.90486972928414
At time: 822.2102844715118 and batch: 400, loss is 3.912553367614746 and perplexity is 50.02652514267458
At time: 824.6829605102539 and batch: 450, loss is 3.8434856748580932 and perplexity is 46.68793008559957
At time: 827.1529049873352 and batch: 500, loss is 3.830145926475525 and perplexity is 46.06926046853678
At time: 829.6253383159637 and batch: 550, loss is 3.877602987289429 and perplexity is 48.30828061601133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432909138997396 and perplexity of 84.17594057251792
Finished 29 epochs...
Completing Train Step...
At time: 833.6762652397156 and batch: 50, loss is 4.008829140663147 and perplexity is 55.08233912115949
At time: 836.1755666732788 and batch: 100, loss is 3.9838442754745484 and perplexity is 53.72316440638106
At time: 838.647524356842 and batch: 150, loss is 3.9818953704833984 and perplexity is 53.618565023366614
At time: 841.1192219257355 and batch: 200, loss is 3.9769662046432495 and perplexity is 53.35492053177343
At time: 843.5933611392975 and batch: 250, loss is 3.9827741384506226 and perplexity is 53.665704009839814
At time: 846.0670628547668 and batch: 300, loss is 3.980347170829773 and perplexity is 53.53561700618362
At time: 848.5456802845001 and batch: 350, loss is 3.9290509033203125 and perplexity is 50.858684946579125
At time: 851.0185813903809 and batch: 400, loss is 3.911794605255127 and perplexity is 49.988581295415734
At time: 853.4902079105377 and batch: 450, loss is 3.8428509712219237 and perplexity is 46.658306488705826
At time: 855.9642925262451 and batch: 500, loss is 3.829594945907593 and perplexity is 46.043884192799
At time: 858.439147233963 and batch: 550, loss is 3.87672315120697 and perplexity is 48.26579594015987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432903035481771 and perplexity of 84.17542680491725
Finished 30 epochs...
Completing Train Step...
At time: 862.5458686351776 and batch: 50, loss is 4.007210268974304 and perplexity is 54.99324002126497
At time: 865.0141036510468 and batch: 100, loss is 3.982193431854248 and perplexity is 53.634549028349355
At time: 867.4787428379059 and batch: 150, loss is 3.9802902698516847 and perplexity is 53.53257086387846
At time: 869.9422833919525 and batch: 200, loss is 3.975660972595215 and perplexity is 53.285325408348136
At time: 872.4074015617371 and batch: 250, loss is 3.981926317214966 and perplexity is 53.62022436838095
At time: 874.8573541641235 and batch: 300, loss is 3.9792888736724854 and perplexity is 53.47899038407405
At time: 877.3404858112335 and batch: 350, loss is 3.9281350660324095 and perplexity is 50.81212798904436
At time: 879.8107001781464 and batch: 400, loss is 3.911031584739685 and perplexity is 49.950453530332815
At time: 882.2789442539215 and batch: 450, loss is 3.842199749946594 and perplexity is 46.62793149834299
At time: 884.7442951202393 and batch: 500, loss is 3.8290276670455934 and perplexity is 46.01777187775451
At time: 887.2180707454681 and batch: 550, loss is 3.8758346700668334 and perplexity is 48.22293173559176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432895914713542 and perplexity of 84.17482741334643
Finished 31 epochs...
Completing Train Step...
At time: 891.2635469436646 and batch: 50, loss is 4.005613169670105 and perplexity is 54.905480454916756
At time: 893.762291431427 and batch: 100, loss is 3.9805663347244264 and perplexity is 53.54735136633643
At time: 896.2293481826782 and batch: 150, loss is 3.9787325048446656 and perplexity is 53.449244616457314
At time: 898.70170545578 and batch: 200, loss is 3.9743958806991575 and perplexity is 53.21795719746906
At time: 901.1671061515808 and batch: 250, loss is 3.981093611717224 and perplexity is 53.57559309769042
At time: 903.6414227485657 and batch: 300, loss is 3.97824800491333 and perplexity is 53.423354733440426
At time: 906.1058917045593 and batch: 350, loss is 3.927241849899292 and perplexity is 50.766762040381344
At time: 908.577883720398 and batch: 400, loss is 3.910254158973694 and perplexity is 49.91163585162228
At time: 911.046671628952 and batch: 450, loss is 3.841531739234924 and perplexity is 46.5967939419076
At time: 913.5159003734589 and batch: 500, loss is 3.8284453392028808 and perplexity is 45.99098224886094
At time: 915.982741355896 and batch: 550, loss is 3.8749334478378294 and perplexity is 48.17949173505114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.432901000976562 and perplexity of 84.17525554974718
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 920.0332229137421 and batch: 50, loss is 4.007190356254577 and perplexity is 54.99214496719237
At time: 922.5223813056946 and batch: 100, loss is 3.9845884370803835 and perplexity is 53.76315800167968
At time: 924.9818806648254 and batch: 150, loss is 3.981543917655945 and perplexity is 53.59972393815542
At time: 927.444803237915 and batch: 200, loss is 3.975659351348877 and perplexity is 53.285239019779475
At time: 929.9051053524017 and batch: 250, loss is 3.9832406187057496 and perplexity is 53.69074384097724
At time: 932.3650932312012 and batch: 300, loss is 3.9786525201797485 and perplexity is 53.444969667504054
At time: 934.8256771564484 and batch: 350, loss is 3.9256301736831665 and perplexity is 50.68500835536229
At time: 937.2833685874939 and batch: 400, loss is 3.903561067581177 and perplexity is 49.57868817885232
At time: 939.7970764636993 and batch: 450, loss is 3.8350448560714723 and perplexity is 46.29550425582404
At time: 942.2615203857422 and batch: 500, loss is 3.8198532629013062 and perplexity is 45.5975169825773
At time: 944.715188741684 and batch: 550, loss is 3.8679308319091796 and perplexity is 47.84328778621464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.430264282226562 and perplexity of 83.95360142336554
Finished 33 epochs...
Completing Train Step...
At time: 948.7265357971191 and batch: 50, loss is 4.004755721092224 and perplexity is 54.85842200676575
At time: 951.1980986595154 and batch: 100, loss is 3.981529040336609 and perplexity is 53.59892652387778
At time: 953.6629118919373 and batch: 150, loss is 3.9787581634521483 and perplexity is 53.45061606723986
At time: 956.1171853542328 and batch: 200, loss is 3.9731795501708986 and perplexity is 53.15326592244025
At time: 958.5850548744202 and batch: 250, loss is 3.980857548713684 and perplexity is 53.56294737491972
At time: 961.047607421875 and batch: 300, loss is 3.9769135093688965 and perplexity is 53.35210905367439
At time: 963.5198485851288 and batch: 350, loss is 3.9242833614349366 and perplexity is 50.616791113527704
At time: 965.9921786785126 and batch: 400, loss is 3.903533992767334 and perplexity is 49.57734586327086
At time: 968.463312625885 and batch: 450, loss is 3.8357994079589846 and perplexity is 46.33044979839814
At time: 970.9350905418396 and batch: 500, loss is 3.8211917018890382 and perplexity is 45.65858733740562
At time: 973.3880236148834 and batch: 550, loss is 3.8687867498397828 and perplexity is 47.88425524397965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.429860432942708 and perplexity of 83.9197036668068
Finished 34 epochs...
Completing Train Step...
At time: 977.4171915054321 and batch: 50, loss is 4.003812265396118 and perplexity is 54.80668992334572
At time: 979.915027141571 and batch: 100, loss is 3.9802587366104127 and perplexity is 53.53088283502015
At time: 982.3820052146912 and batch: 150, loss is 3.9775559520721435 and perplexity is 53.38639573927498
At time: 984.8434689044952 and batch: 200, loss is 3.972184910774231 and perplexity is 53.100423873840334
At time: 987.300704240799 and batch: 250, loss is 3.980092754364014 and perplexity is 53.52199839618544
At time: 989.7623517513275 and batch: 300, loss is 3.976434555053711 and perplexity is 53.32656194925519
At time: 992.234176158905 and batch: 350, loss is 3.92389564037323 and perplexity is 50.59716972159624
At time: 994.7007999420166 and batch: 400, loss is 3.9035818147659302 and perplexity is 49.579716807726335
At time: 997.1559116840363 and batch: 450, loss is 3.8361337661743162 and perplexity is 46.34594335496287
At time: 999.6112806797028 and batch: 500, loss is 3.821722888946533 and perplexity is 45.68284703070987
At time: 1002.1033489704132 and batch: 550, loss is 3.868999843597412 and perplexity is 47.89446016712496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.429732259114584 and perplexity of 83.90894804644199
Finished 35 epochs...
Completing Train Step...
At time: 1006.1708652973175 and batch: 50, loss is 4.003083367347717 and perplexity is 54.76675598967246
At time: 1008.6375663280487 and batch: 100, loss is 3.9794018936157225 and perplexity is 53.48503491810165
At time: 1011.110342502594 and batch: 150, loss is 3.9767382097244264 and perplexity is 53.34275726763188
At time: 1013.5844173431396 and batch: 200, loss is 3.971537823677063 and perplexity is 53.066074389450065
At time: 1016.0550963878632 and batch: 250, loss is 3.979671678543091 and perplexity is 53.49946632096228
At time: 1018.520848274231 and batch: 300, loss is 3.9761816930770872 and perplexity is 53.31307939407878
At time: 1020.9925186634064 and batch: 350, loss is 3.9236815404891967 and perplexity is 50.58633803299945
At time: 1023.4662756919861 and batch: 400, loss is 3.9035416269302368 and perplexity is 49.57772434625016
At time: 1025.9424698352814 and batch: 450, loss is 3.836264410018921 and perplexity is 46.351998562713796
At time: 1028.4199757575989 and batch: 500, loss is 3.821945061683655 and perplexity is 45.692997641426935
At time: 1030.8895807266235 and batch: 550, loss is 3.8689901351928713 and perplexity is 47.893995190587496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.4297739664713545 and perplexity of 83.91244773985541
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1034.9867541790009 and batch: 50, loss is 4.003313055038452 and perplexity is 54.77933668414482
At time: 1037.4874014854431 and batch: 100, loss is 3.9805382537841796 and perplexity is 53.54584772747423
At time: 1039.9502291679382 and batch: 150, loss is 3.9778604650497438 and perplexity is 53.40265506506711
At time: 1042.4180738925934 and batch: 200, loss is 3.9720283937454224 and perplexity is 53.09211340364914
At time: 1044.8761410713196 and batch: 250, loss is 3.9793315744400024 and perplexity is 53.48127402676579
At time: 1047.3330216407776 and batch: 300, loss is 3.975310778617859 and perplexity is 53.26666847527214
At time: 1049.8067951202393 and batch: 350, loss is 3.9225674438476563 and perplexity is 50.53001134620146
At time: 1052.2831284999847 and batch: 400, loss is 3.9008465433120727 and perplexity is 49.444288125193005
At time: 1054.7562899589539 and batch: 450, loss is 3.8331924629211427 and perplexity is 46.20982616010769
At time: 1057.2281649112701 and batch: 500, loss is 3.8187056016921996 and perplexity is 45.54521649846376
At time: 1059.6993808746338 and batch: 550, loss is 3.866640582084656 and perplexity is 47.781597798849695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.42997080485026 and perplexity of 83.92896655575368
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1063.8175547122955 and batch: 50, loss is 4.002943143844605 and perplexity is 54.75907694169759
At time: 1066.2892508506775 and batch: 100, loss is 3.98030460357666 and perplexity is 53.53333819052576
At time: 1068.762769460678 and batch: 150, loss is 3.9777646970748903 and perplexity is 53.39754104582327
At time: 1071.233519077301 and batch: 200, loss is 3.97185959815979 and perplexity is 53.083152445581064
At time: 1073.703748703003 and batch: 250, loss is 3.9788946962356566 and perplexity is 53.457914326846264
At time: 1076.177274465561 and batch: 300, loss is 3.97489164352417 and perplexity is 53.24434722332704
At time: 1078.6493990421295 and batch: 350, loss is 3.922115297317505 and perplexity is 50.50716954121335
At time: 1081.1196687221527 and batch: 400, loss is 3.900141940116882 and perplexity is 49.40946179260952
At time: 1083.5905759334564 and batch: 450, loss is 3.832421636581421 and perplexity is 46.174220133744015
At time: 1086.0551011562347 and batch: 500, loss is 3.817926540374756 and perplexity is 45.50974780003461
At time: 1088.5145835876465 and batch: 550, loss is 3.8660570669174192 and perplexity is 47.75372464481421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.430043538411458 and perplexity of 83.9350712303836
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1092.5098299980164 and batch: 50, loss is 4.002840037345886 and perplexity is 54.753431216061585
At time: 1095.003960609436 and batch: 100, loss is 3.980222840309143 and perplexity is 53.52896130881072
At time: 1097.4655528068542 and batch: 150, loss is 3.9777145195007324 and perplexity is 53.394861753968335
At time: 1099.9302277565002 and batch: 200, loss is 3.9717906522750854 and perplexity is 53.079492706836206
At time: 1102.3937766551971 and batch: 250, loss is 3.978761715888977 and perplexity is 53.45080594751416
At time: 1104.8620264530182 and batch: 300, loss is 3.974780211448669 and perplexity is 53.23841442576538
At time: 1107.3301839828491 and batch: 350, loss is 3.9219954586029053 and perplexity is 50.50111718959772
At time: 1109.7882158756256 and batch: 400, loss is 3.8999639320373536 and perplexity is 49.40066729197458
At time: 1112.2514071464539 and batch: 450, loss is 3.8322314977645875 and perplexity is 46.16544145676965
At time: 1114.7091245651245 and batch: 500, loss is 3.8177314949035646 and perplexity is 45.50087219543253
At time: 1117.1647078990936 and batch: 550, loss is 3.865914306640625 and perplexity is 47.746907796465116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.430058288574219 and perplexity of 83.93630929547636
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1121.1962933540344 and batch: 50, loss is 4.0028145170211795 and perplexity is 54.752033908548086
At time: 1123.652442932129 and batch: 100, loss is 3.9802021312713625 and perplexity is 53.52785278700687
At time: 1126.1322140693665 and batch: 150, loss is 3.977701106071472 and perplexity is 53.39414555057073
At time: 1128.5877511501312 and batch: 200, loss is 3.9717747354507447 and perplexity is 53.07864785659838
At time: 1131.0565390586853 and batch: 250, loss is 3.978727240562439 and perplexity is 53.44896324528946
At time: 1133.5275402069092 and batch: 300, loss is 3.974751591682434 and perplexity is 53.23689077659313
At time: 1135.9968373775482 and batch: 350, loss is 3.9219654989242554 and perplexity is 50.49960421501949
At time: 1138.45951628685 and batch: 400, loss is 3.8999197626113893 and perplexity is 49.398485341046154
At time: 1140.9207017421722 and batch: 450, loss is 3.832184200286865 and perplexity is 46.16325799946722
At time: 1143.3899903297424 and batch: 500, loss is 3.817682595252991 and perplexity is 45.498647273080785
At time: 1145.8555793762207 and batch: 550, loss is 3.8658790493011477 and perplexity is 47.74522439720421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 61 batches
Done Evaluating: Achieved loss of 4.43006337483724 and perplexity of 83.93673621870822
Annealing...
Model not improving. Stopping early with 83.90894804644199loss at 39 epochs.
Finished Training.
Improved accuracyfrom -84.89078601253915 to -83.90894804644199
<pretraining.langmodel.trainer.TrainLangModel object at 0x7faee1ec8828>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -148.54667985450328, 'params': {'batch_size': 80, 'anneal': 7.981558640577813, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 23.983799453362604, 'dropout': 0.08787475654303023, 'wordvec_source': 'glove'}}, {'best_accuracy': -263.859715753902, 'params': {'batch_size': 80, 'anneal': 4.1902268708621895, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 22.029776370308223, 'dropout': 0.855069310926333, 'wordvec_source': 'glove'}}, {'best_accuracy': -203.64520587304102, 'params': {'batch_size': 80, 'anneal': 5.575509407299329, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 15.470220075807763, 'dropout': 0.7870846207383437, 'wordvec_source': 'glove'}}, {'best_accuracy': -84.89078601253915, 'params': {'batch_size': 80, 'anneal': 4.005848223882902, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 8.63971982266622, 'dropout': 0.09810637284736412, 'wordvec_source': 'glove'}}, {'best_accuracy': -178.28215064333747, 'params': {'batch_size': 80, 'anneal': 5.936322633091923, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 26.7776888998906, 'dropout': 0.9561181916555967, 'wordvec_source': 'glove'}}, {'best_accuracy': -83.90894804644199, 'params': {'batch_size': 80, 'anneal': 4.082427181902782, 'num_layers': 1, 'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'tune_wordvecs': True, 'lr': 8.431778148042385, 'dropout': 0.07097106708434686, 'wordvec_source': 'glove'}}]
