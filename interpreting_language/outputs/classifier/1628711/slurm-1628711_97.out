ROOT_PATH: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language
Building Bayesian Optimizer for 
 data:MPQA 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [5e-05, 0.005]}, {'name': 'rnn_dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'l2', 'type': 'continuous', 'domain': [0, 1]}]
SETTINGS FOR THIS RUN
{'attn_type': 'similarity', 'l2': 0.6611883000033582, 'batch_size': 32, 'tune_wordvecs': False, 'tune_attn': False, 'num_layers': 1, 'wordvec_dim': 300, 'wordvec_source': 'glove', 'data': 'MPQA', 'dropout': 0.9784656943110627, 'lr': 0.002087584479647842, 'rnn_dropout': 0.39135735269631344}
Using CUDA!
here
similarity
Building RNN Classifier...

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/mpqa/mpqa_subj_labels.pickle...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Getting GloVe Vectors with 300 dims
Building Vocab...
[<torchtext.vocab.Vectors object at 0x7f839c55ea90>]
Getting Batches...
Created Iterator with 261 batches
Getting Batches...
Created Iterator with 87 batches
Building model...
similarity
Using Attention model with following args:
{'attn_type': 'similarity', 'batch_size': 32, 'vocab_size': 15284, 'tune_attn': False, 'num_classes': 2, 'train_word_vecs': False, 'cuda': True, 'input_size': 300, 'hidden_size': 300, 'dropout': 0.9784656943110627, 'vectors': 
 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
          ...             â‹±             ...          
-0.4786 -0.1089 -0.0076  ...   0.7458  0.7174  0.6185
 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000
-0.0479  0.9828  0.9182  ...   0.0424  0.4554 -0.3210
[torch.FloatTensor of size 15284x300]
, 'attention_dim': 350, 'rnn_dropout': 0.39135735269631344, 'num_layers': 1}
Not Tuning Word Vectors!
Begin Training...
Completing Train Step at 0th epoch...
Traceback (most recent call last):
  File "tune_models.py", line 186, in <module>
    batch_size = args.batch_size, seq_len = args.seq_len)
  File "tune_models.py", line 67, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 120, in getError
    trainer.train(optimizer)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/trainer.py", line 614, in train
    optimizer = self.train_step(optimizer, start_time)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/trainer.py", line 490, in train_step
    output, h, A = self.model(data, lengths = lengths)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File "/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/classifier/attention_rnn/model.py", line 217, in forward
    weighted_seq = torch.bmm(out, attn_params)
RuntimeError: Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'mat2'
