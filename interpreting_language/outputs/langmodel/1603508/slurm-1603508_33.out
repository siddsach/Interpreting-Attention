Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 0.6015664662908425, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 23.662790968094207, 'anneal': 4.35545433808363, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5394954681396484 and batch: 50, loss is 6.357159023284912 and perplexity is 576.605903307499
At time: 2.523339033126831 and batch: 100, loss is 5.807819662094116 and perplexity is 332.89251566425673
At time: 3.4949026107788086 and batch: 150, loss is 5.74891806602478 and perplexity is 313.85091056216083
At time: 4.459462881088257 and batch: 200, loss is 5.703121585845947 and perplexity is 299.8017988585962
At time: 5.428696393966675 and batch: 250, loss is 5.700692052841187 and perplexity is 299.07430458689873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.669417190551758 and perplexity of 289.8655487727655
Finished 1 epochs...
Completing Train Step...
At time: 7.037054777145386 and batch: 50, loss is 5.366872978210449 and perplexity is 214.1920362342013
At time: 8.000763177871704 and batch: 100, loss is 5.266055412292481 and perplexity is 193.6505821770126
At time: 8.961127758026123 and batch: 150, loss is 5.26319899559021 and perplexity is 193.09822467683472
At time: 9.922911405563354 and batch: 200, loss is 5.221553506851197 and perplexity is 185.22170386681967
At time: 10.884933233261108 and batch: 250, loss is 5.210841875076294 and perplexity is 183.24826542043152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.446066284179688 and perplexity of 231.8443599329955
Finished 2 epochs...
Completing Train Step...
At time: 12.49826955795288 and batch: 50, loss is 5.123152408599854 and perplexity is 167.86371141642604
At time: 13.463975191116333 and batch: 100, loss is 5.0577194881439205 and perplexity is 157.2315387643544
At time: 14.429334878921509 and batch: 150, loss is 5.092591018676758 and perplexity is 162.81116278231843
At time: 15.394682884216309 and batch: 200, loss is 5.068547220230102 and perplexity is 158.94324999353725
At time: 16.362916946411133 and batch: 250, loss is 5.1060884571075436 and perplexity is 165.02359391587558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.382048797607422 and perplexity of 217.467365938093
Finished 3 epochs...
Completing Train Step...
At time: 17.96409583091736 and batch: 50, loss is 5.039359836578369 and perplexity is 154.3711606173967
At time: 18.94627618789673 and batch: 100, loss is 4.997794437408447 and perplexity is 148.0861853039023
At time: 19.905890226364136 and batch: 150, loss is 4.987599487304688 and perplexity is 146.5841237648539
At time: 20.86668848991394 and batch: 200, loss is 4.983360481262207 and perplexity is 145.964067917216
At time: 21.828840494155884 and batch: 250, loss is 4.964087791442871 and perplexity is 143.17788259462318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.362425994873047 and perplexity of 213.24164257666442
Finished 4 epochs...
Completing Train Step...
At time: 23.418533086776733 and batch: 50, loss is 5.00216402053833 and perplexity is 148.73467598613115
At time: 24.396238327026367 and batch: 100, loss is 4.97873104095459 and perplexity is 145.28989769718333
At time: 25.357924222946167 and batch: 150, loss is 4.991774110794068 and perplexity is 147.19733636639572
At time: 26.321771144866943 and batch: 200, loss is 4.945473489761352 and perplexity is 140.53737810983768
At time: 27.284776210784912 and batch: 250, loss is 4.979649953842163 and perplexity is 145.42346781686473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.380731582641602 and perplexity of 217.18110324524352
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 28.904189825057983 and batch: 50, loss is 4.882948894500732 and perplexity is 132.0194017184146
At time: 29.86720037460327 and batch: 100, loss is 4.775031070709229 and perplexity is 118.51399687745437
At time: 30.828418731689453 and batch: 150, loss is 4.8139213180541995 and perplexity is 123.21383206525631
At time: 31.797838926315308 and batch: 200, loss is 4.76190821647644 and perplexity is 116.96891508650447
At time: 32.75934720039368 and batch: 250, loss is 4.777918291091919 and perplexity is 118.85666734941573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1888378143310545 and perplexity of 179.26009830654058
Finished 6 epochs...
Completing Train Step...
At time: 34.341686487197876 and batch: 50, loss is 4.741070203781128 and perplexity is 114.55673515168421
At time: 35.323803663253784 and batch: 100, loss is 4.694181966781616 and perplexity is 109.30935337597012
At time: 36.32375645637512 and batch: 150, loss is 4.7551801395416256 and perplexity is 116.18458071651695
At time: 37.311028718948364 and batch: 200, loss is 4.7413770198822025 and perplexity is 114.59188839503008
At time: 38.29041528701782 and batch: 250, loss is 4.755694761276245 and perplexity is 116.24438721452326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.195254898071289 and perplexity of 180.4141241487606
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 39.930654525756836 and batch: 50, loss is 4.690601453781128 and perplexity is 108.91866965658346
At time: 40.906010150909424 and batch: 100, loss is 4.5897194766998295 and perplexity is 98.46680405441347
At time: 41.867862939834595 and batch: 150, loss is 4.61105694770813 and perplexity is 100.59041227518472
At time: 42.83203196525574 and batch: 200, loss is 4.578854036331177 and perplexity is 97.402710259201
At time: 43.79409694671631 and batch: 250, loss is 4.629539909362793 and perplexity is 102.46690919801293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.109602355957032 and perplexity of 165.60449014024502
Finished 8 epochs...
Completing Train Step...
At time: 45.410961389541626 and batch: 50, loss is 4.629819736480713 and perplexity is 102.49558623001444
At time: 46.37389016151428 and batch: 100, loss is 4.549337472915649 and perplexity is 94.56973254584796
At time: 47.34453797340393 and batch: 150, loss is 4.588679056167603 and perplexity is 98.36441044517579
At time: 48.314337968826294 and batch: 200, loss is 4.581383905410767 and perplexity is 97.64943832741622
At time: 49.28845930099487 and batch: 250, loss is 4.622470951080322 and perplexity is 101.74512901418346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.103789520263672 and perplexity of 164.64465084570347
Finished 9 epochs...
Completing Train Step...
At time: 50.90732526779175 and batch: 50, loss is 4.606825761795044 and perplexity is 100.1656947029195
At time: 51.87043499946594 and batch: 100, loss is 4.5373748016357425 and perplexity is 93.44516574480281
At time: 52.834877729415894 and batch: 150, loss is 4.580654649734497 and perplexity is 97.57825287958218
At time: 53.80198931694031 and batch: 200, loss is 4.572376394271851 and perplexity is 96.77380946270578
At time: 54.770450830459595 and batch: 250, loss is 4.610847826004028 and perplexity is 100.56937883610439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.1031440734863285 and perplexity of 164.5384157746385
Finished 10 epochs...
Completing Train Step...
At time: 56.36422419548035 and batch: 50, loss is 4.593094310760498 and perplexity is 98.79967455399559
At time: 57.34415364265442 and batch: 100, loss is 4.526005554199219 and perplexity is 92.38878106210646
At time: 58.315141677856445 and batch: 150, loss is 4.570601072311401 and perplexity is 96.60215720766324
At time: 59.29787015914917 and batch: 200, loss is 4.564469089508057 and perplexity is 96.01160691347359
At time: 60.2667396068573 and batch: 250, loss is 4.600552129745483 and perplexity is 99.53925905836024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.100941467285156 and perplexity of 164.17640127377314
Finished 11 epochs...
Completing Train Step...
At time: 61.883397817611694 and batch: 50, loss is 4.580799369812012 and perplexity is 97.59237543378659
At time: 62.8552987575531 and batch: 100, loss is 4.516167249679565 and perplexity is 91.4842887309538
At time: 63.82587552070618 and batch: 150, loss is 4.5611552715301515 and perplexity is 95.693968512993
At time: 64.78537487983704 and batch: 200, loss is 4.555294418334961 and perplexity is 95.13476053053188
At time: 65.75152587890625 and batch: 250, loss is 4.591571578979492 and perplexity is 98.64934363548065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.100217056274414 and perplexity of 164.05751314811917
Finished 12 epochs...
Completing Train Step...
At time: 67.34905362129211 and batch: 50, loss is 4.569881219863891 and perplexity is 96.5326429313753
At time: 68.342600107193 and batch: 100, loss is 4.505065670013428 and perplexity is 90.47428530131654
At time: 69.3102343082428 and batch: 150, loss is 4.549500169754029 and perplexity is 94.5851199940502
At time: 70.2761926651001 and batch: 200, loss is 4.545941457748413 and perplexity is 94.2491170154988
At time: 71.24242067337036 and batch: 250, loss is 4.583582277297974 and perplexity is 97.86434424244732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.099005889892578 and perplexity of 163.85893248489347
Finished 13 epochs...
Completing Train Step...
At time: 72.8496356010437 and batch: 50, loss is 4.560948009490967 and perplexity is 95.67413684118868
At time: 73.83770775794983 and batch: 100, loss is 4.496613807678223 and perplexity is 89.71283148130526
At time: 74.80961513519287 and batch: 150, loss is 4.5406607723236085 and perplexity is 93.75272886531702
At time: 75.78155851364136 and batch: 200, loss is 4.538482532501221 and perplexity is 93.54873519207426
At time: 76.75155663490295 and batch: 250, loss is 4.576102771759033 and perplexity is 97.13509793818965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.098539733886719 and perplexity of 163.7825664600254
Finished 14 epochs...
Completing Train Step...
At time: 78.34746289253235 and batch: 50, loss is 4.551298046112061 and perplexity is 94.75532530331286
At time: 79.31263756752014 and batch: 100, loss is 4.487915258407593 and perplexity is 88.9358442266228
At time: 80.28122544288635 and batch: 150, loss is 4.532443561553955 and perplexity is 92.98549949183811
At time: 81.24504160881042 and batch: 200, loss is 4.531013603210449 and perplexity is 92.85262912320172
At time: 82.21230983734131 and batch: 250, loss is 4.56802191734314 and perplexity is 96.35332629863008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.097658157348633 and perplexity of 163.63824321742604
Finished 15 epochs...
Completing Train Step...
At time: 83.80481815338135 and batch: 50, loss is 4.541953725814819 and perplexity is 93.874025181764
At time: 84.78868579864502 and batch: 100, loss is 4.479820261001587 and perplexity is 88.21881488913345
At time: 85.75650835037231 and batch: 150, loss is 4.524058723449707 and perplexity is 92.2090907123285
At time: 86.72337532043457 and batch: 200, loss is 4.524067354202271 and perplexity is 92.20988654960887
At time: 87.69348978996277 and batch: 250, loss is 4.560686483383178 and perplexity is 95.64911882813897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.097532272338867 and perplexity of 163.6176449121167
Finished 16 epochs...
Completing Train Step...
At time: 89.31176424026489 and batch: 50, loss is 4.5341383934021 and perplexity is 93.143227901603
At time: 90.27684593200684 and batch: 100, loss is 4.472319459915161 and perplexity is 87.55957859754331
At time: 91.24119186401367 and batch: 150, loss is 4.516376962661743 and perplexity is 91.5034761858249
At time: 92.20641732215881 and batch: 200, loss is 4.51686508178711 and perplexity is 91.5481516851845
At time: 93.17826986312866 and batch: 250, loss is 4.553584699630737 and perplexity is 94.9722458178407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.097497940063477 and perplexity of 163.6120276425002
Finished 17 epochs...
Completing Train Step...
At time: 94.7885115146637 and batch: 50, loss is 4.5261547565460205 and perplexity is 92.4025667134593
At time: 95.75960111618042 and batch: 100, loss is 4.465337333679199 and perplexity is 86.95035587654922
At time: 96.72588634490967 and batch: 150, loss is 4.508872976303101 and perplexity is 90.81940518846373
At time: 97.69599747657776 and batch: 200, loss is 4.510494537353516 and perplexity is 90.96679386611264
At time: 98.67028260231018 and batch: 250, loss is 4.547394409179687 and perplexity is 94.38615593630261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.09747200012207 and perplexity of 163.60778361113498
Finished 18 epochs...
Completing Train Step...
At time: 100.27768182754517 and batch: 50, loss is 4.517761297225952 and perplexity is 91.63023532894253
At time: 101.26749753952026 and batch: 100, loss is 4.456842679977417 and perplexity is 86.21487098153214
At time: 102.23962569236755 and batch: 150, loss is 4.501306972503662 and perplexity is 90.13485813209785
At time: 103.20581293106079 and batch: 200, loss is 4.5047281455993655 and perplexity is 90.44375317414153
At time: 104.17319965362549 and batch: 250, loss is 4.541783828735351 and perplexity is 93.85807761380867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.097533798217773 and perplexity of 163.6178945730202
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 105.79054880142212 and batch: 50, loss is 4.502591552734375 and perplexity is 90.250717988661
At time: 106.75921702384949 and batch: 100, loss is 4.4247510051727295 and perplexity is 83.49201555364844
At time: 107.72901248931885 and batch: 150, loss is 4.454871759414673 and perplexity is 86.04511566157734
At time: 108.69465613365173 and batch: 200, loss is 4.454905109405518 and perplexity is 86.04798531324803
At time: 109.67245149612427 and batch: 250, loss is 4.505345821380615 and perplexity is 90.49963534679816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.077179718017578 and perplexity of 160.32126655241416
Finished 20 epochs...
Completing Train Step...
At time: 111.27517604827881 and batch: 50, loss is 4.486310682296753 and perplexity is 88.7932543243482
At time: 112.26452112197876 and batch: 100, loss is 4.4145659160614015 and perplexity is 82.64595783565285
At time: 113.23415565490723 and batch: 150, loss is 4.451489410400391 and perplexity is 85.75457268530143
At time: 114.20617938041687 and batch: 200, loss is 4.457422513961792 and perplexity is 86.26487578952828
At time: 115.18004822731018 and batch: 250, loss is 4.506122131347656 and perplexity is 90.5699183929163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.074480438232422 and perplexity of 159.88909813262057
Finished 21 epochs...
Completing Train Step...
At time: 116.79800224304199 and batch: 50, loss is 4.4812964248657225 and perplexity is 88.3491364801565
At time: 117.82596850395203 and batch: 100, loss is 4.411220846176147 and perplexity is 82.36996319898589
At time: 118.84298586845398 and batch: 150, loss is 4.449436864852905 and perplexity is 85.57873803500576
At time: 119.84746360778809 and batch: 200, loss is 4.457029037475586 and perplexity is 86.2309392663694
At time: 120.81691884994507 and batch: 250, loss is 4.505472192764282 and perplexity is 90.51107263359593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.075056076049805 and perplexity of 159.98116283951416
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 122.42620491981506 and batch: 50, loss is 4.47681734085083 and perplexity is 87.95429819226051
At time: 123.39801287651062 and batch: 100, loss is 4.4023895263671875 and perplexity is 81.64573038416233
At time: 124.37208914756775 and batch: 150, loss is 4.437684783935547 and perplexity is 84.57889639815802
At time: 125.34543085098267 and batch: 200, loss is 4.445847597122192 and perplexity is 85.27212362192293
At time: 126.32004690170288 and batch: 250, loss is 4.497291059494018 and perplexity is 89.77361023826475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.068550491333008 and perplexity of 158.9437699141145
Finished 23 epochs...
Completing Train Step...
At time: 127.92089247703552 and batch: 50, loss is 4.472722549438476 and perplexity is 87.59488006068888
At time: 128.8958601951599 and batch: 100, loss is 4.400692367553711 and perplexity is 81.50728213082643
At time: 129.87259721755981 and batch: 150, loss is 4.438568773269654 and perplexity is 84.65369629674986
At time: 130.8392095565796 and batch: 200, loss is 4.4460658264160156 and perplexity is 85.29073452789234
At time: 131.8083267211914 and batch: 250, loss is 4.495761089324951 and perplexity is 89.63636431049059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.067363739013672 and perplexity of 158.75525490895978
Finished 24 epochs...
Completing Train Step...
At time: 133.41601514816284 and batch: 50, loss is 4.471152505874634 and perplexity is 87.45746018884546
At time: 134.38218903541565 and batch: 100, loss is 4.400638628005981 and perplexity is 81.50290208403993
At time: 135.36617922782898 and batch: 150, loss is 4.439298028945923 and perplexity is 84.715453000768
At time: 136.34198665618896 and batch: 200, loss is 4.4459328365325925 and perplexity is 85.27939247725612
At time: 137.3166024684906 and batch: 250, loss is 4.494634389877319 and perplexity is 89.53542794148767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.067089080810547 and perplexity of 158.71165746338193
Finished 25 epochs...
Completing Train Step...
At time: 138.94447207450867 and batch: 50, loss is 4.469693565368653 and perplexity is 87.32995798930327
At time: 139.91269659996033 and batch: 100, loss is 4.400459966659546 and perplexity is 81.48834196651903
At time: 140.87972831726074 and batch: 150, loss is 4.439939193725586 and perplexity is 84.76978698218151
At time: 141.84518909454346 and batch: 200, loss is 4.445734958648682 and perplexity is 85.26251924100674
At time: 142.81093645095825 and batch: 250, loss is 4.493465147018433 and perplexity is 89.43080046113678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066865539550781 and perplexity of 158.67618282470346
Finished 26 epochs...
Completing Train Step...
At time: 144.40199661254883 and batch: 50, loss is 4.468490676879883 and perplexity is 87.22497294342371
At time: 145.38302159309387 and batch: 100, loss is 4.400381631851197 and perplexity is 81.48195884288204
At time: 146.34757375717163 and batch: 150, loss is 4.440512981414795 and perplexity is 84.8184407995146
At time: 147.3277621269226 and batch: 200, loss is 4.445457201004029 and perplexity is 85.2388402131529
At time: 148.2981662750244 and batch: 250, loss is 4.492359933853149 and perplexity is 89.33201496265752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066823959350586 and perplexity of 158.6695851744222
Finished 27 epochs...
Completing Train Step...
At time: 149.91831755638123 and batch: 50, loss is 4.467429113388062 and perplexity is 87.13242722683984
At time: 150.88762378692627 and batch: 100, loss is 4.400352516174316 and perplexity is 81.47958647503351
At time: 151.85957264900208 and batch: 150, loss is 4.440967473983765 and perplexity is 84.8569989120937
At time: 152.82539677619934 and batch: 200, loss is 4.445096158981324 and perplexity is 85.20807096469939
At time: 153.80019664764404 and batch: 250, loss is 4.491267194747925 and perplexity is 89.23445169186817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066770553588867 and perplexity of 158.66111153063707
Finished 28 epochs...
Completing Train Step...
At time: 155.40576601028442 and batch: 50, loss is 4.46644998550415 and perplexity is 87.04715519067578
At time: 156.38260221481323 and batch: 100, loss is 4.40027738571167 and perplexity is 81.47346510595891
At time: 157.35096979141235 and batch: 150, loss is 4.441313896179199 and perplexity is 84.88640035231651
At time: 158.31834411621094 and batch: 200, loss is 4.444702377319336 and perplexity is 85.17452419438513
At time: 159.2870054244995 and batch: 250, loss is 4.490236616134643 and perplexity is 89.14253594572295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066672134399414 and perplexity of 158.64549700103973
Finished 29 epochs...
Completing Train Step...
At time: 160.85908818244934 and batch: 50, loss is 4.4655310153961185 and perplexity is 86.96719820173475
At time: 161.84012746810913 and batch: 100, loss is 4.400200777053833 and perplexity is 81.46722377222098
At time: 162.79699420928955 and batch: 150, loss is 4.441614599227905 and perplexity is 84.91192978989359
At time: 163.7518916130066 and batch: 200, loss is 4.444313955307007 and perplexity is 85.14144695867748
At time: 164.70795154571533 and batch: 250, loss is 4.48924072265625 and perplexity is 89.05380366680988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0666969299316404 and perplexity of 158.6494307493427
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 166.30673623085022 and batch: 50, loss is 4.463960485458374 and perplexity is 86.83072081232264
At time: 167.26594591140747 and batch: 100, loss is 4.3979078388214115 and perplexity is 81.28063845619549
At time: 168.2222294807434 and batch: 150, loss is 4.439385566711426 and perplexity is 84.72286912681807
At time: 169.18027424812317 and batch: 200, loss is 4.440983266830444 and perplexity is 84.8583390562495
At time: 170.13328194618225 and batch: 250, loss is 4.486501045227051 and perplexity is 88.81015887738117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066719436645508 and perplexity of 158.6530014668683
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 171.70460057258606 and batch: 50, loss is 4.463521738052368 and perplexity is 86.79263241500833
At time: 172.67778539657593 and batch: 100, loss is 4.397865180969238 and perplexity is 81.27717127268751
At time: 173.63177466392517 and batch: 150, loss is 4.438549137115478 and perplexity is 84.65203404003806
At time: 174.58746337890625 and batch: 200, loss is 4.440197811126709 and perplexity is 84.79171275925619
At time: 175.54321360588074 and batch: 250, loss is 4.486005983352661 and perplexity is 88.76620323494157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066250228881836 and perplexity of 158.57857770831822
Finished 32 epochs...
Completing Train Step...
At time: 177.17001104354858 and batch: 50, loss is 4.463340005874634 and perplexity is 86.77686083405358
At time: 178.12897491455078 and batch: 100, loss is 4.39745512008667 and perplexity is 81.24384951654409
At time: 179.09703612327576 and batch: 150, loss is 4.438397188186645 and perplexity is 84.63917223133592
At time: 180.0703001022339 and batch: 200, loss is 4.440261945724488 and perplexity is 84.79715101603728
At time: 181.02905201911926 and batch: 250, loss is 4.486013164520264 and perplexity is 88.76684068221327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066189193725586 and perplexity of 158.5688991354195
Finished 33 epochs...
Completing Train Step...
At time: 182.61684370040894 and batch: 50, loss is 4.463277778625488 and perplexity is 86.77146111672091
At time: 183.56969738006592 and batch: 100, loss is 4.397617092132569 and perplexity is 81.25700981483838
At time: 184.53732752799988 and batch: 150, loss is 4.438193273544312 and perplexity is 84.62191482438175
At time: 185.49341583251953 and batch: 200, loss is 4.440164976119995 and perplexity is 84.78892866850647
At time: 186.44580245018005 and batch: 250, loss is 4.4859284400939945 and perplexity is 88.75932028115005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066091537475586 and perplexity of 158.55341464745408
Finished 34 epochs...
Completing Train Step...
At time: 188.02057456970215 and batch: 50, loss is 4.463169107437134 and perplexity is 86.76203207126815
At time: 188.99074387550354 and batch: 100, loss is 4.39756100654602 and perplexity is 81.25245259558011
At time: 189.9426302909851 and batch: 150, loss is 4.438085498809815 and perplexity is 84.6127952114197
At time: 190.8990433216095 and batch: 200, loss is 4.440161151885986 and perplexity is 84.78860441642192
At time: 191.854576587677 and batch: 250, loss is 4.485895986557007 and perplexity is 88.75643977400789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066057586669922 and perplexity of 158.5480317226639
Finished 35 epochs...
Completing Train Step...
At time: 193.44756412506104 and batch: 50, loss is 4.463105373382568 and perplexity is 86.75650255139314
At time: 194.40222787857056 and batch: 100, loss is 4.397568550109863 and perplexity is 81.25306553095551
At time: 195.35659837722778 and batch: 150, loss is 4.437907609939575 and perplexity is 84.59774487555504
At time: 196.3140845298767 and batch: 200, loss is 4.440116329193115 and perplexity is 84.7848040480192
At time: 197.26973867416382 and batch: 250, loss is 4.485824127197265 and perplexity is 88.7500620222261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.066016769409179 and perplexity of 158.5415603583854
Finished 36 epochs...
Completing Train Step...
At time: 198.84535551071167 and batch: 50, loss is 4.463045072555542 and perplexity is 86.7512712202678
At time: 199.81553840637207 and batch: 100, loss is 4.3975945091247555 and perplexity is 81.25517480787093
At time: 200.77458786964417 and batch: 150, loss is 4.437784214019775 and perplexity is 84.58730650305262
At time: 201.73205018043518 and batch: 200, loss is 4.44005654335022 and perplexity is 84.77973526856664
At time: 202.68431758880615 and batch: 250, loss is 4.485745830535889 and perplexity is 88.74311346070093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.06600341796875 and perplexity of 158.53944361431746
Finished 37 epochs...
Completing Train Step...
At time: 204.25664854049683 and batch: 50, loss is 4.462973003387451 and perplexity is 86.74501935360622
At time: 205.2273404598236 and batch: 100, loss is 4.397608280181885 and perplexity is 81.25629378523
At time: 206.1796133518219 and batch: 150, loss is 4.4376539039611815 and perplexity is 84.57628464433107
At time: 207.13461470603943 and batch: 200, loss is 4.440014524459839 and perplexity is 84.77617299300579
At time: 208.09364128112793 and batch: 250, loss is 4.485678043365478 and perplexity is 88.73709802003319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.06599006652832 and perplexity of 158.5373268985109
Finished 38 epochs...
Completing Train Step...
At time: 209.68528628349304 and batch: 50, loss is 4.4629130363464355 and perplexity is 86.73981766743918
At time: 210.64089632034302 and batch: 100, loss is 4.397618751525879 and perplexity is 81.2571446522888
At time: 211.59382486343384 and batch: 150, loss is 4.437523202896118 and perplexity is 84.56523115621603
At time: 212.552631855011 and batch: 200, loss is 4.439962167739868 and perplexity is 84.77173450684947
At time: 213.51099705696106 and batch: 250, loss is 4.485610246658325 and perplexity is 88.7310821409158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065987014770508 and perplexity of 158.53684308172333
Finished 39 epochs...
Completing Train Step...
At time: 215.09420228004456 and batch: 50, loss is 4.462850170135498 and perplexity is 86.73436483516628
At time: 216.0644679069519 and batch: 100, loss is 4.397633190155029 and perplexity is 81.25831790253628
At time: 217.01932001113892 and batch: 150, loss is 4.437394008636475 and perplexity is 84.5543065195006
At time: 217.97961330413818 and batch: 200, loss is 4.439912815093994 and perplexity is 84.7675509006931
At time: 218.93610334396362 and batch: 250, loss is 4.485544118881226 and perplexity is 88.72521474569523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065982818603516 and perplexity of 158.53617783605105
Finished 40 epochs...
Completing Train Step...
At time: 220.54370617866516 and batch: 50, loss is 4.462792091369629 and perplexity is 86.72932755657905
At time: 221.49913215637207 and batch: 100, loss is 4.397641239166259 and perplexity is 81.25897195428183
At time: 222.45539379119873 and batch: 150, loss is 4.437268362045288 and perplexity is 84.54368322652054
At time: 223.41325902938843 and batch: 200, loss is 4.439859113693237 and perplexity is 84.76299888669689
At time: 224.3704800605774 and batch: 250, loss is 4.4854755783081055 and perplexity is 88.7191336770287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.06597900390625 and perplexity of 158.53557306968045
Finished 41 epochs...
Completing Train Step...
At time: 225.9642777442932 and batch: 50, loss is 4.4627342033386235 and perplexity is 86.72430711188963
At time: 226.91942691802979 and batch: 100, loss is 4.397656202316284 and perplexity is 81.26018785356686
At time: 227.87682366371155 and batch: 150, loss is 4.437143707275391 and perplexity is 84.53314510996856
At time: 228.83770108222961 and batch: 200, loss is 4.439806661605835 and perplexity is 84.75855300706965
At time: 229.8038513660431 and batch: 250, loss is 4.485407600402832 and perplexity is 88.71310294114436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065972137451172 and perplexity of 158.53448449602698
Finished 42 epochs...
Completing Train Step...
At time: 231.40340757369995 and batch: 50, loss is 4.462676582336425 and perplexity is 86.7193101143663
At time: 232.37865447998047 and batch: 100, loss is 4.3976653289794925 and perplexity is 81.26092949131798
At time: 233.34333086013794 and batch: 150, loss is 4.437021894454956 and perplexity is 84.52284851628376
At time: 234.32422375679016 and batch: 200, loss is 4.439752626419067 and perplexity is 84.7539731865646
At time: 235.28610920906067 and batch: 250, loss is 4.4853408241271975 and perplexity is 88.70717920831459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065968704223633 and perplexity of 158.5339402120033
Finished 43 epochs...
Completing Train Step...
At time: 236.8870255947113 and batch: 50, loss is 4.46261999130249 and perplexity is 86.71440271780338
At time: 237.84575653076172 and batch: 100, loss is 4.397675676345825 and perplexity is 81.2617703322742
At time: 238.804762840271 and batch: 150, loss is 4.43690128326416 and perplexity is 84.5126547296295
At time: 239.76658749580383 and batch: 200, loss is 4.4396975708007815 and perplexity is 84.74930713261607
At time: 240.72350907325745 and batch: 250, loss is 4.485273389816284 and perplexity is 88.70119750250001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065963363647461 and perplexity of 158.5330935516805
Finished 44 epochs...
Completing Train Step...
At time: 242.30709409713745 and batch: 50, loss is 4.4625638961792 and perplexity is 86.70953859911982
At time: 243.28122878074646 and batch: 100, loss is 4.397682971954346 and perplexity is 81.26236318850087
At time: 244.242413520813 and batch: 150, loss is 4.4367837142944335 and perplexity is 84.50271924794768
At time: 245.19805669784546 and batch: 200, loss is 4.439639921188355 and perplexity is 84.74442150873493
At time: 246.15560722351074 and batch: 250, loss is 4.485205469131469 and perplexity is 88.69517306101612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065962219238282 and perplexity of 158.5329121250569
Finished 45 epochs...
Completing Train Step...
At time: 247.73307967185974 and batch: 50, loss is 4.462508478164673 and perplexity is 86.70473346179695
At time: 248.70431780815125 and batch: 100, loss is 4.397689580917358 and perplexity is 81.2629002502282
At time: 249.66246056556702 and batch: 150, loss is 4.436668329238891 and perplexity is 84.49296945949455
At time: 250.6179964542389 and batch: 200, loss is 4.439581279754639 and perplexity is 84.73945212006569
At time: 251.57307720184326 and batch: 250, loss is 4.485137434005737 and perplexity is 88.68913887903557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0659629821777346 and perplexity of 158.53303307611628
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 253.16804385185242 and batch: 50, loss is 4.462421846389771 and perplexity is 86.69722240219764
At time: 254.12532448768616 and batch: 100, loss is 4.3975741481781006 and perplexity is 81.25352039243403
At time: 255.0834095478058 and batch: 150, loss is 4.436442737579346 and perplexity is 84.47391070012364
At time: 256.0389070510864 and batch: 200, loss is 4.439404306411743 and perplexity is 84.72445682287191
At time: 256.998651266098 and batch: 250, loss is 4.485038595199585 and perplexity is 88.6803733836228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.065962600708008 and perplexity of 158.532972600575
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 258.6223819255829 and batch: 50, loss is 4.462397937774658 and perplexity is 86.69514961645471
At time: 259.5955810546875 and batch: 100, loss is 4.397539501190185 and perplexity is 81.25070525146329
At time: 260.554226398468 and batch: 150, loss is 4.436393966674805 and perplexity is 84.46979093155194
At time: 261.51243710517883 and batch: 200, loss is 4.439370250701904 and perplexity is 84.72157152048489
At time: 262.47044229507446 and batch: 250, loss is 4.485018510818481 and perplexity is 88.67859231109324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0659629821777346 and perplexity of 158.53303307611628
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 264.0792109966278 and batch: 50, loss is 4.462393712997437 and perplexity is 86.6947833495351
At time: 265.0375452041626 and batch: 100, loss is 4.39753119468689 and perplexity is 81.25003034501539
At time: 265.9953188896179 and batch: 150, loss is 4.4363827133178715 and perplexity is 84.46884036819301
At time: 266.95388865470886 and batch: 200, loss is 4.439362478256226 and perplexity is 84.72091302923153
At time: 267.9095633029938 and batch: 250, loss is 4.485014190673828 and perplexity is 88.6782092075744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0659629821777346 and perplexity of 158.53303307611628
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 269.506715297699 and batch: 50, loss is 4.462393274307251 and perplexity is 86.69474531739286
At time: 270.4661452770233 and batch: 100, loss is 4.397529640197754 and perplexity is 81.24990404282411
At time: 271.4254686832428 and batch: 150, loss is 4.4363810825347905 and perplexity is 84.46870261794959
At time: 272.3804030418396 and batch: 200, loss is 4.439361276626587 and perplexity is 84.7208112261326
At time: 273.3379182815552 and batch: 250, loss is 4.485013580322265 and perplexity is 88.67815508270732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.0659629821777346 and perplexity of 158.53303307611628
Annealing...
Model not improving. Stopping early with 158.5329121250569loss at 49 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -158.5329121250569
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6da3525898>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 0.6585271305567568, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 6.407833584085658, 'anneal': 7.763844023126149, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.269681692123413 and batch: 50, loss is 6.517582654953003 and perplexity is 676.9400082997831
At time: 2.250521183013916 and batch: 100, loss is 5.797925930023194 and perplexity is 329.6152054539305
At time: 3.218231678009033 and batch: 150, loss is 5.670437107086181 and perplexity is 290.1613382533689
At time: 4.179151296615601 and batch: 200, loss is 5.603299398422241 and perplexity is 271.3201254433013
At time: 5.153815746307373 and batch: 250, loss is 5.596948070526123 and perplexity is 269.6023432353401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.521281814575195 and perplexity of 249.95522818774617
Finished 1 epochs...
Completing Train Step...
At time: 6.773821592330933 and batch: 50, loss is 5.147835206985474 and perplexity is 172.05861557506796
At time: 7.737033843994141 and batch: 100, loss is 4.909708070755005 and perplexity is 135.59982307584042
At time: 8.698848485946655 and batch: 150, loss is 4.842076759338379 and perplexity is 126.73227105292248
At time: 9.654173135757446 and batch: 200, loss is 4.762225074768066 and perplexity is 117.00598352952375
At time: 10.61116909980774 and batch: 250, loss is 4.7561751556396485 and perplexity is 116.30024377843345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.932401275634765 and perplexity of 138.71219896758038
Finished 2 epochs...
Completing Train Step...
At time: 12.210529565811157 and batch: 50, loss is 4.605418186187745 and perplexity is 100.0248030954245
At time: 13.165623664855957 and batch: 100, loss is 4.463428363800049 and perplexity is 86.78452859619972
At time: 14.125584125518799 and batch: 150, loss is 4.494344091415405 and perplexity is 89.50943971682274
At time: 15.081629514694214 and batch: 200, loss is 4.456928625106811 and perplexity is 86.2222810481994
At time: 16.039449453353882 and batch: 250, loss is 4.473150882720947 and perplexity is 87.63240789983044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.787042617797852 and perplexity of 119.94611710288007
Finished 3 epochs...
Completing Train Step...
At time: 17.619386196136475 and batch: 50, loss is 4.3694557762146 and perplexity is 79.00062597853871
At time: 18.588699340820312 and batch: 100, loss is 4.243151769638062 and perplexity is 69.6269544980041
At time: 19.54788327217102 and batch: 150, loss is 4.296119604110718 and perplexity is 73.41436347237786
At time: 20.504914045333862 and batch: 200, loss is 4.27590256690979 and perplexity is 71.94504523143345
At time: 21.464975118637085 and batch: 250, loss is 4.299667587280274 and perplexity is 73.67529902212557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.728607940673828 and perplexity of 113.13795792728169
Finished 4 epochs...
Completing Train Step...
At time: 23.06458282470703 and batch: 50, loss is 4.209352760314942 and perplexity is 67.31295808993411
At time: 24.02547335624695 and batch: 100, loss is 4.099261074066162 and perplexity is 60.29571706319085
At time: 24.982349395751953 and batch: 150, loss is 4.154198064804077 and perplexity is 63.7008600976002
At time: 25.942350149154663 and batch: 200, loss is 4.139560561180115 and perplexity is 62.77522951438625
At time: 26.903621673583984 and batch: 250, loss is 4.169711813926697 and perplexity is 64.69680470246844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.703780746459961 and perplexity of 110.36364162123735
Finished 5 epochs...
Completing Train Step...
At time: 28.488895654678345 and batch: 50, loss is 4.085230555534363 and perplexity is 59.45564399893058
At time: 29.45983099937439 and batch: 100, loss is 3.9852586507797243 and perplexity is 53.79920288423003
At time: 30.421257495880127 and batch: 150, loss is 4.0395305585861205 and perplexity is 56.799672427336624
At time: 31.378113508224487 and batch: 200, loss is 4.029599075317383 and perplexity is 56.23835938024444
At time: 32.33501863479614 and batch: 250, loss is 4.061207113265991 and perplexity is 58.044334893784516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.695051193237305 and perplexity of 109.40440926434022
Finished 6 epochs...
Completing Train Step...
At time: 33.9702033996582 and batch: 50, loss is 3.9830962896347044 and perplexity is 53.68299526498079
At time: 34.9431836605072 and batch: 100, loss is 3.8901800775527953 and perplexity is 48.91969506956901
At time: 35.89920687675476 and batch: 150, loss is 3.9452122974395754 and perplexity is 51.6873100309888
At time: 36.87373638153076 and batch: 200, loss is 3.938785042762756 and perplexity is 51.35616783342419
At time: 37.83407282829285 and batch: 250, loss is 3.973285336494446 and perplexity is 53.15888910844956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.696504211425781 and perplexity of 109.56349140753589
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 39.4369170665741 and batch: 50, loss is 3.891736693382263 and perplexity is 48.9959035395543
At time: 40.39688849449158 and batch: 100, loss is 3.763364858627319 and perplexity is 43.09318477029854
At time: 41.35665845870972 and batch: 150, loss is 3.7728921365737915 and perplexity is 43.50570750824318
At time: 42.31427836418152 and batch: 200, loss is 3.693746681213379 and perplexity is 40.195163651600865
At time: 43.275795459747314 and batch: 250, loss is 3.6353220558166504 and perplexity is 37.91406137512964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612775802612305 and perplexity of 100.76346127909581
Finished 8 epochs...
Completing Train Step...
At time: 44.868218421936035 and batch: 50, loss is 3.7992193794250486 and perplexity is 44.66630344516386
At time: 45.84230017662048 and batch: 100, loss is 3.6832178354263307 and perplexity is 39.774175122882156
At time: 46.80108642578125 and batch: 150, loss is 3.7080301094055175 and perplexity is 40.77340821170508
At time: 47.76036548614502 and batch: 200, loss is 3.648655571937561 and perplexity is 38.422974383951406
At time: 48.721792221069336 and batch: 250, loss is 3.628066034317017 and perplexity is 37.63995180576875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.607816314697265 and perplexity of 100.26496327978215
Finished 9 epochs...
Completing Train Step...
At time: 50.32731485366821 and batch: 50, loss is 3.7572149229049683 and perplexity is 42.82897771482377
At time: 51.292073488235474 and batch: 100, loss is 3.643242244720459 and perplexity is 38.215540212036956
At time: 52.25393033027649 and batch: 150, loss is 3.6735883569717407 and perplexity is 39.39300872266544
At time: 53.21399235725403 and batch: 200, loss is 3.624545006752014 and perplexity is 37.50765354750444
At time: 54.17761278152466 and batch: 250, loss is 3.62145459651947 and perplexity is 37.39191843777818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.607441711425781 and perplexity of 100.22741073061533
Finished 10 epochs...
Completing Train Step...
At time: 55.790035247802734 and batch: 50, loss is 3.7268930530548094 and perplexity is 41.54981434476656
At time: 56.74962615966797 and batch: 100, loss is 3.6144929790496825 and perplexity is 37.13251418937556
At time: 57.70835900306702 and batch: 150, loss is 3.648853087425232 and perplexity is 38.43056426600956
At time: 58.66591668128967 and batch: 200, loss is 3.6064558029174805 and perplexity is 36.83526973568849
At time: 59.62636208534241 and batch: 250, loss is 3.613204312324524 and perplexity is 37.08469357295464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.608877182006836 and perplexity of 100.37138754265051
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 61.21475601196289 and batch: 50, loss is 3.70746928691864 and perplexity is 40.75054797837895
At time: 62.18769812583923 and batch: 100, loss is 3.590987606048584 and perplexity is 36.269878595582526
At time: 63.145225286483765 and batch: 150, loss is 3.615265293121338 and perplexity is 37.1612032296487
At time: 64.10254836082458 and batch: 200, loss is 3.556325225830078 and perplexity is 35.03421746724494
At time: 65.06684708595276 and batch: 250, loss is 3.544665160179138 and perplexity is 34.62808853787044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.603640365600586 and perplexity of 99.84713491912123
Finished 12 epochs...
Completing Train Step...
At time: 66.66805362701416 and batch: 50, loss is 3.695824670791626 and perplexity is 40.278775625082915
At time: 67.62376308441162 and batch: 100, loss is 3.580076575279236 and perplexity is 35.87628797865014
At time: 68.58220291137695 and batch: 150, loss is 3.6072552394866944 and perplexity is 36.86472897117819
At time: 69.53929829597473 and batch: 200, loss is 3.55232310295105 and perplexity is 34.89428642136331
At time: 70.50509476661682 and batch: 250, loss is 3.5472941827774047 and perplexity is 34.71924634062794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602899551391602 and perplexity of 99.77319413442034
Finished 13 epochs...
Completing Train Step...
At time: 72.08578205108643 and batch: 50, loss is 3.689049792289734 and perplexity is 40.00681410736674
At time: 73.06261491775513 and batch: 100, loss is 3.5733860158920288 and perplexity is 35.637056731174056
At time: 74.02008414268494 and batch: 150, loss is 3.602219796180725 and perplexity is 36.679565300283336
At time: 74.97734665870667 and batch: 200, loss is 3.5499621438980102 and perplexity is 34.811999616049505
At time: 75.94074845314026 and batch: 250, loss is 3.548859186172485 and perplexity is 34.773624619022044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602853012084961 and perplexity of 99.76855086719208
Finished 14 epochs...
Completing Train Step...
At time: 77.52751922607422 and batch: 50, loss is 3.68362576007843 and perplexity is 39.79040329914324
At time: 78.50277924537659 and batch: 100, loss is 3.5681824398040773 and perplexity is 35.452098235072924
At time: 79.46244049072266 and batch: 150, loss is 3.598260989189148 and perplexity is 36.53464502569038
At time: 80.42004799842834 and batch: 200, loss is 3.5480321073532104 and perplexity is 34.74487598096386
At time: 81.38162994384766 and batch: 250, loss is 3.549617233276367 and perplexity is 34.79999465805907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.603043746948242 and perplexity of 99.78758202299625
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 82.98287415504456 and batch: 50, loss is 3.6798181676864625 and perplexity is 39.63918573238941
At time: 83.94251680374146 and batch: 100, loss is 3.5634908533096312 and perplexity is 35.286161208120426
At time: 84.89993929862976 and batch: 150, loss is 3.5924553775787356 and perplexity is 36.3231535789747
At time: 85.85654830932617 and batch: 200, loss is 3.5396465730667113 and perplexity is 34.454739805696
At time: 86.8462221622467 and batch: 250, loss is 3.538760142326355 and perplexity is 34.42421159775123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602986145019531 and perplexity of 99.78183423135383
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 88.47385096549988 and batch: 50, loss is 3.678899335861206 and perplexity is 39.60278071461694
At time: 89.46282696723938 and batch: 100, loss is 3.5625587701797485 and perplexity is 35.25328689571606
At time: 90.42039132118225 and batch: 150, loss is 3.591550669670105 and perplexity is 36.29030659536875
At time: 91.36945819854736 and batch: 200, loss is 3.5384951877593993 and perplexity is 34.41509195387355
At time: 92.32711243629456 and batch: 250, loss is 3.537346210479736 and perplexity is 34.37557250296426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602966690063477 and perplexity of 99.7798929990372
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 93.90636658668518 and batch: 50, loss is 3.6787799406051636 and perplexity is 39.59805261273562
At time: 94.86611342430115 and batch: 100, loss is 3.562437934875488 and perplexity is 35.249027311427106
At time: 95.82623672485352 and batch: 150, loss is 3.591430311203003 and perplexity is 36.28593901253932
At time: 96.78718256950378 and batch: 200, loss is 3.538346872329712 and perplexity is 34.409988043226384
At time: 97.7476544380188 and batch: 250, loss is 3.5371631813049316 and perplexity is 34.3692813460458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602963638305664 and perplexity of 99.77958849543383
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 99.35081648826599 and batch: 50, loss is 3.6787650346755982 and perplexity is 39.59746237135151
At time: 100.3098213672638 and batch: 100, loss is 3.5624227714538574 and perplexity is 35.24849281961628
At time: 101.27016353607178 and batch: 150, loss is 3.591415066719055 and perplexity is 36.285385856340824
At time: 102.23341202735901 and batch: 200, loss is 3.538327860832214 and perplexity is 34.40933386404329
At time: 103.1907696723938 and batch: 250, loss is 3.5371393632888792 and perplexity is 34.36846274769973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.602963638305664 and perplexity of 99.77958849543383
Annealing...
Model not improving. Stopping early with 99.76855086719208loss at 18 epochs.
Finished Training.
Improved accuracyfrom -158.5329121250569 to -99.76855086719208
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6d9509c780>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 0.6290737966847788, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 5.025658666082009, 'anneal': 7.726592178557317, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.264901876449585 and batch: 50, loss is 6.5852157020568844 and perplexity is 724.307265385153
At time: 2.2470192909240723 and batch: 100, loss is 5.785466203689575 and perplexity is 325.53376978597254
At time: 3.21756649017334 and batch: 150, loss is 5.6438487529754635 and perplexity is 282.5480862669661
At time: 4.186795711517334 and batch: 200, loss is 5.566577224731446 and perplexity is 261.5373818023581
At time: 5.154247999191284 and batch: 250, loss is 5.55539231300354 and perplexity is 258.6284079079999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.496955871582031 and perplexity of 243.94819119556502
Finished 1 epochs...
Completing Train Step...
At time: 6.781327724456787 and batch: 50, loss is 5.13675784111023 and perplexity is 170.1631769539942
At time: 7.741193056106567 and batch: 100, loss is 4.9060718536376955 and perplexity is 135.10764804799624
At time: 8.70210075378418 and batch: 150, loss is 4.84411150932312 and perplexity is 126.99040206649192
At time: 9.661744594573975 and batch: 200, loss is 4.763026876449585 and perplexity is 117.09983674467024
At time: 10.620540142059326 and batch: 250, loss is 4.756294574737549 and perplexity is 116.31413307793835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.941474151611328 and perplexity of 139.9764440422312
Finished 2 epochs...
Completing Train Step...
At time: 12.22268557548523 and batch: 50, loss is 4.610312509536743 and perplexity is 100.5155567987055
At time: 13.182267427444458 and batch: 100, loss is 4.472473411560059 and perplexity is 87.57305957637779
At time: 14.139005661010742 and batch: 150, loss is 4.503197813034058 and perplexity is 90.30545000524334
At time: 15.098403453826904 and batch: 200, loss is 4.462565059661865 and perplexity is 86.70963948422363
At time: 16.057578325271606 and batch: 250, loss is 4.4811555290222165 and perplexity is 88.33668933094548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7931255340576175 and perplexity of 120.67796291064558
Finished 3 epochs...
Completing Train Step...
At time: 17.65363883972168 and batch: 50, loss is 4.379743452072144 and perplexity is 79.81755375016915
At time: 18.63250708580017 and batch: 100, loss is 4.259393796920777 and perplexity is 70.7670712323415
At time: 19.589686632156372 and batch: 150, loss is 4.3129681205749515 and perplexity is 74.66176551250297
At time: 20.549740314483643 and batch: 200, loss is 4.283860721588135 and perplexity is 72.5198793051721
At time: 21.50735640525818 and batch: 250, loss is 4.312888898849487 and perplexity is 74.65585091289836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7243400573730465 and perplexity of 112.65612725586917
Finished 4 epochs...
Completing Train Step...
At time: 23.111828804016113 and batch: 50, loss is 4.224554691314697 and perplexity is 68.34406256659456
At time: 24.07053256034851 and batch: 100, loss is 4.113569712638855 and perplexity is 61.16466861748926
At time: 25.027774333953857 and batch: 150, loss is 4.176989536285401 and perplexity is 65.16936758955073
At time: 25.98728656768799 and batch: 200, loss is 4.156459197998047 and perplexity is 63.845059192093395
At time: 26.945306062698364 and batch: 250, loss is 4.188727221488953 and perplexity is 65.93881202546494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.691737365722656 and perplexity of 109.04246196937419
Finished 5 epochs...
Completing Train Step...
At time: 28.535991430282593 and batch: 50, loss is 4.10705349445343 and perplexity is 60.7674020350998
At time: 29.513338804244995 and batch: 100, loss is 4.0019231081008915 and perplexity is 54.70324920390893
At time: 30.468979358673096 and batch: 150, loss is 4.069304008483886 and perplexity is 58.5162216220483
At time: 31.42791223526001 and batch: 200, loss is 4.053180036544799 and perplexity is 57.58027358400306
At time: 32.38634967803955 and batch: 250, loss is 4.088654856681824 and perplexity is 59.65958701076037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.679013442993164 and perplexity of 107.66380367995696
Finished 6 epochs...
Completing Train Step...
At time: 33.977290630340576 and batch: 50, loss is 4.01046058177948 and perplexity is 55.17227605744537
At time: 34.95593810081482 and batch: 100, loss is 3.9111555004119873 and perplexity is 49.956643557876625
At time: 35.912352323532104 and batch: 150, loss is 3.9775395584106445 and perplexity is 53.385520547948396
At time: 36.884310722351074 and batch: 200, loss is 3.965710091590881 and perplexity is 52.75771890432902
At time: 37.843536615371704 and batch: 250, loss is 4.002730612754822 and perplexity is 54.747440172031126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.676629257202149 and perplexity of 107.40741892490972
Finished 7 epochs...
Completing Train Step...
At time: 39.44737792015076 and batch: 50, loss is 3.9283264541625975 and perplexity is 50.82185375787973
At time: 40.409292221069336 and batch: 100, loss is 3.835129041671753 and perplexity is 46.299401834697825
At time: 41.36920952796936 and batch: 150, loss is 3.9002646350860597 and perplexity is 49.41552445692284
At time: 42.32622003555298 and batch: 200, loss is 3.887334842681885 and perplexity is 48.78070487092736
At time: 43.28552746772766 and batch: 250, loss is 3.928832688331604 and perplexity is 50.84758803001858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.679477691650391 and perplexity of 107.7137980602613
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 44.87967038154602 and batch: 50, loss is 3.860815200805664 and perplexity is 47.50406093742258
At time: 45.85472846031189 and batch: 100, loss is 3.7389968252182006 and perplexity is 42.05577969867652
At time: 46.81178307533264 and batch: 150, loss is 3.764343008995056 and perplexity is 43.13535700686045
At time: 47.76775765419006 and batch: 200, loss is 3.688878545761108 and perplexity is 39.99996366590338
At time: 48.72442030906677 and batch: 250, loss is 3.6461351203918455 and perplexity is 38.326253080656834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.620814514160156 and perplexity of 101.57673413229038
Finished 9 epochs...
Completing Train Step...
At time: 50.328845739364624 and batch: 50, loss is 3.785628385543823 and perplexity is 44.063350633510176
At time: 51.28961706161499 and batch: 100, loss is 3.6719087600708007 and perplexity is 39.32689988094033
At time: 52.24878478050232 and batch: 150, loss is 3.7115286350250245 and perplexity is 40.91630484294027
At time: 53.20858097076416 and batch: 200, loss is 3.653408637046814 and perplexity is 38.60603599011787
At time: 54.16586422920227 and batch: 250, loss is 3.6423401689529418 and perplexity is 38.18108244336481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.617715835571289 and perplexity of 101.26246763799665
Finished 10 epochs...
Completing Train Step...
At time: 55.77202224731445 and batch: 50, loss is 3.752533783912659 and perplexity is 42.62895784290686
At time: 56.73023343086243 and batch: 100, loss is 3.6392923736572267 and perplexity is 38.064891472997076
At time: 57.68777012825012 and batch: 150, loss is 3.684186406135559 and perplexity is 39.8127178865727
At time: 58.64418935775757 and batch: 200, loss is 3.635067186355591 and perplexity is 37.904399470054955
At time: 59.60120105743408 and batch: 250, loss is 3.6385521602630617 and perplexity is 38.036725756087826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618373107910156 and perplexity of 101.32904653477728
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 61.19574737548828 and batch: 50, loss is 3.7349772548675535 and perplexity is 41.88707282524848
At time: 62.17945694923401 and batch: 100, loss is 3.618996877670288 and perplexity is 37.300132453300506
At time: 63.14929270744324 and batch: 150, loss is 3.6555757904052735 and perplexity is 38.68979191385601
At time: 64.10794067382812 and batch: 200, loss is 3.594512453079224 and perplexity is 36.39794995282112
At time: 65.07398009300232 and batch: 250, loss is 3.5819653415679933 and perplexity is 35.94411393547661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.613615036010742 and perplexity of 100.84806083556465
Finished 12 epochs...
Completing Train Step...
At time: 66.67915153503418 and batch: 50, loss is 3.7250091934204104 and perplexity is 41.47161400906087
At time: 67.63738226890564 and batch: 100, loss is 3.6097147417068483 and perplexity is 36.95550944555525
At time: 68.60073447227478 and batch: 150, loss is 3.6486821365356445 and perplexity is 38.42399508838032
At time: 69.55777740478516 and batch: 200, loss is 3.5912792825698854 and perplexity is 36.28045921058279
At time: 70.51716780662537 and batch: 250, loss is 3.584377613067627 and perplexity is 36.03092556161173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612928009033203 and perplexity of 100.77879929213682
Finished 13 epochs...
Completing Train Step...
At time: 72.12510395050049 and batch: 50, loss is 3.7188744497299195 and perplexity is 41.21797508704881
At time: 73.09609365463257 and batch: 100, loss is 3.6037202405929567 and perplexity is 36.73464225870543
At time: 74.05832505226135 and batch: 150, loss is 3.644227695465088 and perplexity is 38.2532183064993
At time: 75.01628994941711 and batch: 200, loss is 3.589294228553772 and perplexity is 36.20851197252263
At time: 75.97569346427917 and batch: 250, loss is 3.585930323600769 and perplexity is 36.08691461539818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612759399414062 and perplexity of 100.76180844962074
Finished 14 epochs...
Completing Train Step...
At time: 77.56948494911194 and batch: 50, loss is 3.7139425754547117 and perplexity is 41.0151936732872
At time: 78.54071831703186 and batch: 100, loss is 3.5989822912216187 and perplexity is 36.56100704574936
At time: 79.50299835205078 and batch: 150, loss is 3.6406417894363403 and perplexity is 38.11629151039212
At time: 80.46256041526794 and batch: 200, loss is 3.587623620033264 and perplexity is 36.14807222354317
At time: 81.41952252388 and batch: 250, loss is 3.5868331146240235 and perplexity is 36.119508268384834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612826156616211 and perplexity of 100.76853525056497
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 83.02333903312683 and batch: 50, loss is 3.710639934539795 and perplexity is 40.879958655801765
At time: 83.98403286933899 and batch: 100, loss is 3.595235595703125 and perplexity is 36.42428038103467
At time: 84.9461145401001 and batch: 150, loss is 3.6358492040634154 and perplexity is 37.934052974916725
At time: 85.90235328674316 and batch: 200, loss is 3.580630412101746 and perplexity is 35.89616309126427
At time: 86.87319016456604 and batch: 250, loss is 3.5779205322265626 and perplexity is 35.799020483153136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612776565551758 and perplexity of 100.76353815554516
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 88.45907855033875 and batch: 50, loss is 3.709875636100769 and perplexity is 40.84872610422929
At time: 89.43443727493286 and batch: 100, loss is 3.59451235294342 and perplexity is 36.397946308083334
At time: 90.39118266105652 and batch: 150, loss is 3.635104217529297 and perplexity is 37.90580314044558
At time: 91.35340428352356 and batch: 200, loss is 3.579689860343933 and perplexity is 35.86241676454128
At time: 92.31631875038147 and batch: 250, loss is 3.5767462921142577 and perplexity is 35.757008508220984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.612768936157226 and perplexity of 100.7627693936908
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 93.93688344955444 and batch: 50, loss is 3.709777021408081 and perplexity is 40.84469801827531
At time: 94.90122294425964 and batch: 100, loss is 3.5944165992736816 and perplexity is 36.39446123801018
At time: 95.86127376556396 and batch: 150, loss is 3.635004744529724 and perplexity is 37.9020327240364
At time: 96.82200241088867 and batch: 200, loss is 3.5795680332183837 and perplexity is 35.85804801551307
At time: 97.78252649307251 and batch: 250, loss is 3.57659375667572 and perplexity is 35.75155471320648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6127685546875 and perplexity of 100.76273095575209
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 99.40226221084595 and batch: 50, loss is 3.709764757156372 and perplexity is 40.844197091689594
At time: 100.36357975006104 and batch: 100, loss is 3.594404559135437 and perplexity is 36.39402304630348
At time: 101.32390713691711 and batch: 150, loss is 3.6349920988082887 and perplexity is 37.901553428519264
At time: 102.28408551216125 and batch: 200, loss is 3.579552264213562 and perplexity is 35.85748257423926
At time: 103.24164867401123 and batch: 250, loss is 3.5765736722946166 and perplexity is 35.7508366725673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6127685546875 and perplexity of 100.76273095575209
Annealing...
Model not improving. Stopping early with 100.76180844962074loss at 18 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6d9509c780>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 0.2600879040070142, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 7.534100279776419, 'anneal': 7.011583710499144, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2746152877807617 and batch: 50, loss is 6.303447751998902 and perplexity is 546.4526994020273
At time: 2.2451963424682617 and batch: 100, loss is 5.346434888839721 and perplexity is 209.85879270196816
At time: 3.2160398960113525 and batch: 150, loss is 5.201172084808349 and perplexity is 181.48483287903366
At time: 4.200838088989258 and batch: 200, loss is 5.104746513366699 and perplexity is 164.80229005880196
At time: 5.170391082763672 and batch: 250, loss is 5.099769983291626 and perplexity is 163.98418385936054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.13040771484375 and perplexity of 169.08604288264695
Finished 1 epochs...
Completing Train Step...
At time: 6.787085771560669 and batch: 50, loss is 4.8248319339752195 and perplexity is 124.5655313901606
At time: 7.748986005783081 and batch: 100, loss is 4.653367128372192 and perplexity is 104.93773018427198
At time: 8.706695795059204 and batch: 150, loss is 4.656752977371216 and perplexity is 105.29363567409193
At time: 9.66768193244934 and batch: 200, loss is 4.605964317321777 and perplexity is 100.07944467394589
At time: 10.631611824035645 and batch: 250, loss is 4.612965145111084 and perplexity is 100.7825418909684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.862815856933594 and perplexity of 129.38802780163624
Finished 2 epochs...
Completing Train Step...
At time: 12.263031005859375 and batch: 50, loss is 4.481535882949829 and perplexity is 88.37029492829011
At time: 13.221303462982178 and batch: 100, loss is 4.346447114944458 and perplexity is 77.20367927861243
At time: 14.180366039276123 and batch: 150, loss is 4.394708318710327 and perplexity is 81.0209950075269
At time: 15.136894226074219 and batch: 200, loss is 4.368334894180298 and perplexity is 78.91212520491023
At time: 16.099671602249146 and batch: 250, loss is 4.3914824485778805 and perplexity is 80.7600529085781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.774303817749024 and perplexity of 118.42783855564419
Finished 3 epochs...
Completing Train Step...
At time: 17.72333550453186 and batch: 50, loss is 4.286414346694946 and perplexity is 72.70530454221591
At time: 18.681803226470947 and batch: 100, loss is 4.1730759334564205 and perplexity is 64.91481899413584
At time: 19.64119553565979 and batch: 150, loss is 4.230886688232422 and perplexity is 68.77818995625996
At time: 20.60029149055481 and batch: 200, loss is 4.20980899810791 and perplexity is 67.3436758121313
At time: 21.56401300430298 and batch: 250, loss is 4.237086267471313 and perplexity is 69.20591026783927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.731864547729492 and perplexity of 113.50700439283305
Finished 4 epochs...
Completing Train Step...
At time: 23.181881427764893 and batch: 50, loss is 4.1480006980896 and perplexity is 63.30730327514588
At time: 24.140459299087524 and batch: 100, loss is 4.03849889755249 and perplexity is 56.74110463481963
At time: 25.098990201950073 and batch: 150, loss is 4.101896605491638 and perplexity is 60.45483791222674
At time: 26.056740522384644 and batch: 200, loss is 4.086221489906311 and perplexity is 59.5145898410825
At time: 27.019888877868652 and batch: 250, loss is 4.119225964546204 and perplexity is 61.511611664671555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.718486022949219 and perplexity of 111.99856099450638
Finished 5 epochs...
Completing Train Step...
At time: 28.645353317260742 and batch: 50, loss is 4.034206767082214 and perplexity is 56.49808631694418
At time: 29.60502576828003 and batch: 100, loss is 3.9345082235336304 and perplexity is 51.13699580095766
At time: 30.564813375473022 and batch: 150, loss is 3.996729779243469 and perplexity is 54.419893657348936
At time: 31.52456498146057 and batch: 200, loss is 3.9854870080947875 and perplexity is 53.81148972859521
At time: 32.48784017562866 and batch: 250, loss is 4.019503769874572 and perplexity is 55.673472116971425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7274120330810545 and perplexity of 113.0027362567929
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 34.100459575653076 and batch: 50, loss is 3.927467098236084 and perplexity is 50.77819845706384
At time: 35.058446407318115 and batch: 100, loss is 3.7838399314880373 and perplexity is 43.98461578317594
At time: 36.01276516914368 and batch: 150, loss is 3.794361643791199 and perplexity is 44.44985250764533
At time: 36.994815826416016 and batch: 200, loss is 3.7101139307022093 and perplexity is 40.85846129501127
At time: 37.97486400604248 and batch: 250, loss is 3.646833453178406 and perplexity is 38.35302690720145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.632713317871094 and perplexity of 102.7925950532944
Finished 7 epochs...
Completing Train Step...
At time: 39.592209339141846 and batch: 50, loss is 3.817195539474487 and perplexity is 45.476492289806615
At time: 40.55594205856323 and batch: 100, loss is 3.690613613128662 and perplexity is 40.06942654152154
At time: 41.51464056968689 and batch: 150, loss is 3.718861041069031 and perplexity is 41.21742241290367
At time: 42.47683262825012 and batch: 200, loss is 3.656742901802063 and perplexity is 38.73497357182339
At time: 43.43959641456604 and batch: 250, loss is 3.6346436834335325 and perplexity is 37.88835024480729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.627205276489258 and perplexity of 102.22796561465238
Finished 8 epochs...
Completing Train Step...
At time: 45.03133988380432 and batch: 50, loss is 3.7665133237838746 and perplexity is 43.22907597313589
At time: 46.007789850234985 and batch: 100, loss is 3.6437183475494384 and perplexity is 38.233739070763185
At time: 46.96747183799744 and batch: 150, loss is 3.678695168495178 and perplexity is 39.59469594454225
At time: 47.93266224861145 and batch: 200, loss is 3.6280050754547117 and perplexity is 37.637657387062745
At time: 48.8930938243866 and batch: 250, loss is 3.6238349199295046 and perplexity is 37.481029310855625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.627901458740235 and perplexity of 102.29915968901675
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 50.49464297294617 and batch: 50, loss is 3.73560610294342 and perplexity is 41.9134217142541
At time: 51.47048473358154 and batch: 100, loss is 3.6080795335769653 and perplexity is 36.8951288769186
At time: 52.43230152130127 and batch: 150, loss is 3.633994474411011 and perplexity is 37.86376076869843
At time: 53.39878177642822 and batch: 200, loss is 3.5646714115142824 and perplexity is 35.32784317439788
At time: 54.36293601989746 and batch: 250, loss is 3.5407755613327025 and perplexity is 34.49366076916584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.619731521606445 and perplexity of 101.4667868323945
Finished 10 epochs...
Completing Train Step...
At time: 55.971986532211304 and batch: 50, loss is 3.719600601196289 and perplexity is 41.24791644976561
At time: 56.93230700492859 and batch: 100, loss is 3.5929847526550294 and perplexity is 36.34238724163345
At time: 57.894606828689575 and batch: 150, loss is 3.6225647783279418 and perplexity is 37.4334533167792
At time: 58.8603777885437 and batch: 200, loss is 3.5588524961471557 and perplexity is 35.12287038288686
At time: 59.820597410202026 and batch: 250, loss is 3.5440795946121217 and perplexity is 34.607817457173724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618449401855469 and perplexity of 101.33677762242601
Finished 11 epochs...
Completing Train Step...
At time: 61.42872166633606 and batch: 50, loss is 3.7101917171478274 and perplexity is 40.86163965310382
At time: 62.405449628829956 and batch: 100, loss is 3.583793897628784 and perplexity is 36.00989989118685
At time: 63.362948179244995 and batch: 150, loss is 3.615407581329346 and perplexity is 37.16649120686315
At time: 64.3274245262146 and batch: 200, loss is 3.555316710472107 and perplexity is 34.99890273159459
At time: 65.28936457633972 and batch: 250, loss is 3.54564058303833 and perplexity is 34.661882045801065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618299865722657 and perplexity of 101.32162524553085
Finished 12 epochs...
Completing Train Step...
At time: 66.90048837661743 and batch: 50, loss is 3.70257896900177 and perplexity is 40.55175132907442
At time: 67.86066794395447 and batch: 100, loss is 3.5765092182159424 and perplexity is 35.74853245958649
At time: 68.82201647758484 and batch: 150, loss is 3.609693741798401 and perplexity is 36.95473339138883
At time: 69.78761577606201 and batch: 200, loss is 3.552299337387085 and perplexity is 34.89345714882146
At time: 70.74708032608032 and batch: 250, loss is 3.5460136461257936 and perplexity is 34.674815526886555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618562698364258 and perplexity of 101.34825937595156
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 72.3607931137085 and batch: 50, loss is 3.6971176719665526 and perplexity is 40.330889813883836
At time: 73.32208013534546 and batch: 100, loss is 3.5695579147338865 and perplexity is 35.50089525925272
At time: 74.28678107261658 and batch: 150, loss is 3.601378402709961 and perplexity is 36.6487163334088
At time: 75.25147128105164 and batch: 200, loss is 3.5401835346221926 and perplexity is 34.473245644393096
At time: 76.2131814956665 and batch: 250, loss is 3.5310624027252198 and perplexity is 34.160240275618726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618262863159179 and perplexity of 101.31787615502449
Finished 14 epochs...
Completing Train Step...
At time: 77.8124270439148 and batch: 50, loss is 3.69538010597229 and perplexity is 40.26087307819021
At time: 78.78580331802368 and batch: 100, loss is 3.568044466972351 and perplexity is 35.44720714611525
At time: 79.75186347961426 and batch: 150, loss is 3.600250954627991 and perplexity is 36.60742009253026
At time: 80.74279737472534 and batch: 200, loss is 3.5398104095458987 and perplexity is 34.460385211406155
At time: 81.70393657684326 and batch: 250, loss is 3.531548342704773 and perplexity is 34.17684413599284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618192291259765 and perplexity of 101.3107262123551
Finished 15 epochs...
Completing Train Step...
At time: 83.34073281288147 and batch: 50, loss is 3.69400936126709 and perplexity is 40.20572350621865
At time: 84.2972993850708 and batch: 100, loss is 3.5667784595489502 and perplexity is 35.40235911369054
At time: 85.26310467720032 and batch: 150, loss is 3.5993213081359863 and perplexity is 36.57340394680502
At time: 86.22582960128784 and batch: 200, loss is 3.5394917011260985 and perplexity is 34.449404146461596
At time: 87.18671035766602 and batch: 250, loss is 3.5319103002548218 and perplexity is 34.18921694184487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6181682586669925 and perplexity of 101.3082914821851
Finished 16 epochs...
Completing Train Step...
At time: 88.80446290969849 and batch: 50, loss is 3.692758402824402 and perplexity is 40.1554592627489
At time: 89.79364371299744 and batch: 100, loss is 3.565621461868286 and perplexity is 35.361422352741336
At time: 90.75757074356079 and batch: 150, loss is 3.5984744358062746 and perplexity is 36.542444054387566
At time: 91.71806502342224 and batch: 200, loss is 3.5391734647750854 and perplexity is 34.43844283802736
At time: 92.67801451683044 and batch: 250, loss is 3.532185354232788 and perplexity is 34.19862211537418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6181694030761715 and perplexity of 101.30840742039011
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 94.28248119354248 and batch: 50, loss is 3.6917605638504027 and perplexity is 40.11541056487648
At time: 95.25900053977966 and batch: 100, loss is 3.5643862819671632 and perplexity is 35.31777159839306
At time: 96.22300863265991 and batch: 150, loss is 3.5971580791473388 and perplexity is 36.49437281121478
At time: 97.18612408638 and batch: 200, loss is 3.5372844886779786 and perplexity is 34.373450846169845
At time: 98.14774584770203 and batch: 250, loss is 3.5299138879776 and perplexity is 34.121029257406505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618143463134766 and perplexity of 101.30577952032168
Finished 18 epochs...
Completing Train Step...
At time: 99.77585411071777 and batch: 50, loss is 3.691562452316284 and perplexity is 40.10746402652412
At time: 100.73881220817566 and batch: 100, loss is 3.564215908050537 and perplexity is 35.31175488387949
At time: 101.70363974571228 and batch: 150, loss is 3.5970251035690306 and perplexity is 36.489520273526864
At time: 102.66172122955322 and batch: 200, loss is 3.5372440004348755 and perplexity is 34.37205915370947
At time: 103.62074828147888 and batch: 250, loss is 3.5299662351608276 and perplexity is 34.12281544392747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6181293487548825 and perplexity of 101.30434966215594
Finished 19 epochs...
Completing Train Step...
At time: 105.21676731109619 and batch: 50, loss is 3.691374087333679 and perplexity is 40.09990989624957
At time: 106.19732069969177 and batch: 100, loss is 3.5640502977371216 and perplexity is 35.30590737730299
At time: 107.1566309928894 and batch: 150, loss is 3.5968989324569702 and perplexity is 36.484916640604254
At time: 108.11686539649963 and batch: 200, loss is 3.5372030782699584 and perplexity is 34.370652603415984
At time: 109.07879948616028 and batch: 250, loss is 3.5300144481658937 and perplexity is 34.12446064706111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618121337890625 and perplexity of 101.30353813001267
Finished 20 epochs...
Completing Train Step...
At time: 110.71112632751465 and batch: 50, loss is 3.6911911869049074 and perplexity is 40.09257627621741
At time: 111.67205619812012 and batch: 100, loss is 3.5638873291015627 and perplexity is 35.30015409056592
At time: 112.63248491287231 and batch: 150, loss is 3.5967766094207763 and perplexity is 36.4804539677749
At time: 113.59122610092163 and batch: 200, loss is 3.53716166973114 and perplexity is 34.36922939438015
At time: 114.54971814155579 and batch: 250, loss is 3.5300595712661744 and perplexity is 34.12600048326175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618115997314453 and perplexity of 101.30299711219541
Finished 21 epochs...
Completing Train Step...
At time: 116.1557834148407 and batch: 50, loss is 3.691011972427368 and perplexity is 40.08539174991165
At time: 117.1193859577179 and batch: 100, loss is 3.5637264156341555 and perplexity is 35.29447427736267
At time: 118.09248781204224 and batch: 150, loss is 3.596657099723816 and perplexity is 36.476094460283136
At time: 119.06923651695251 and batch: 200, loss is 3.5371195316314696 and perplexity is 34.36778117087924
At time: 120.04769134521484 and batch: 250, loss is 3.530101819038391 and perplexity is 34.127442261212565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618113327026367 and perplexity of 101.30272660437035
Finished 22 epochs...
Completing Train Step...
At time: 121.64375972747803 and batch: 50, loss is 3.6908356142044068 and perplexity is 40.07832298479168
At time: 122.62062883377075 and batch: 100, loss is 3.563567223548889 and perplexity is 35.28885612359874
At time: 123.57990646362305 and batch: 150, loss is 3.5965395164489746 and perplexity is 36.47180573378927
At time: 124.54256439208984 and batch: 200, loss is 3.5370769453048707 and perplexity is 34.36631760449
At time: 125.50531458854675 and batch: 250, loss is 3.5301420211791994 and perplexity is 34.12881428503074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111801147461 and perplexity of 101.30257202879464
Finished 23 epochs...
Completing Train Step...
At time: 127.11212587356567 and batch: 50, loss is 3.690661544799805 and perplexity is 40.07134718212679
At time: 128.0760350227356 and batch: 100, loss is 3.5634094715118407 and perplexity is 35.28328967373108
At time: 129.03760600090027 and batch: 150, loss is 3.5964235162734983 and perplexity is 36.46757524329777
At time: 129.9954526424408 and batch: 200, loss is 3.5370339345932007 and perplexity is 34.36483951649939
At time: 130.9720597267151 and batch: 250, loss is 3.5301799535751344 and perplexity is 34.13010889728071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618112182617187 and perplexity of 101.3026106726664
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 132.58521819114685 and batch: 50, loss is 3.690513458251953 and perplexity is 40.06541359400796
At time: 133.56156492233276 and batch: 100, loss is 3.563227710723877 and perplexity is 35.276877137989466
At time: 134.52402687072754 and batch: 150, loss is 3.596231460571289 and perplexity is 36.46057211004392
At time: 135.48423266410828 and batch: 200, loss is 3.5367582035064697 and perplexity is 34.3553653681728
At time: 136.44319343566895 and batch: 250, loss is 3.529848690032959 and perplexity is 34.11880470894867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618110656738281 and perplexity of 101.30245609726761
Finished 25 epochs...
Completing Train Step...
At time: 138.03775811195374 and batch: 50, loss is 3.6904886865615847 and perplexity is 40.06442111828063
At time: 139.01531195640564 and batch: 100, loss is 3.5632056188583374 and perplexity is 35.276097814571465
At time: 139.97388553619385 and batch: 150, loss is 3.5962151527404784 and perplexity is 36.45997752205092
At time: 140.93411993980408 and batch: 200, loss is 3.5367527055740355 and perplexity is 34.355176485214486
At time: 141.89257955551147 and batch: 250, loss is 3.5298548889160157 and perplexity is 34.11901620808463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111038208008 and perplexity of 101.30249474109524
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 143.50750613212585 and batch: 50, loss is 3.6904675245285032 and perplexity is 40.06357328264653
At time: 144.46840929985046 and batch: 100, loss is 3.563179965019226 and perplexity is 35.2751928588415
At time: 145.43000984191895 and batch: 150, loss is 3.5961876773834227 and perplexity is 36.458975784911864
At time: 146.38901615142822 and batch: 200, loss is 3.5367135858535765 and perplexity is 34.35383254660149
At time: 147.34793639183044 and batch: 250, loss is 3.5298076820373536 and perplexity is 34.117405593842754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111038208008 and perplexity of 101.30249474109524
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 148.94591450691223 and batch: 50, loss is 3.6904650497436524 and perplexity is 40.06347413404499
At time: 149.9215304851532 and batch: 100, loss is 3.5631768465042115 and perplexity is 35.27508285279445
At time: 150.87899017333984 and batch: 150, loss is 3.596184196472168 and perplexity is 36.4588488746736
At time: 151.83998203277588 and batch: 200, loss is 3.5367082929611207 and perplexity is 34.353650715941576
At time: 152.80068135261536 and batch: 250, loss is 3.5298009967803954 and perplexity is 34.11717751098201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111038208008 and perplexity of 101.30249474109524
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 154.4187548160553 and batch: 50, loss is 3.6904648971557616 and perplexity is 40.06346802084444
At time: 155.37713599205017 and batch: 100, loss is 3.5631765699386597 and perplexity is 35.275073096923045
At time: 156.35239481925964 and batch: 150, loss is 3.596184015274048 and perplexity is 36.45884226839932
At time: 157.3134527206421 and batch: 200, loss is 3.5367079687118532 and perplexity is 34.353639576797306
At time: 158.27505588531494 and batch: 250, loss is 3.529800591468811 and perplexity is 34.11716368289754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111419677734 and perplexity of 101.30253338493752
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 159.90619707107544 and batch: 50, loss is 3.69046489238739 and perplexity is 40.06346782980694
At time: 160.8677797317505 and batch: 100, loss is 3.5631766271591188 and perplexity is 35.27507511537898
At time: 161.8286702632904 and batch: 150, loss is 3.5961840295791627 and perplexity is 36.458842789947255
At time: 162.78884053230286 and batch: 200, loss is 3.536707949638367 and perplexity is 34.35363892155363
At time: 163.74929809570312 and batch: 250, loss is 3.5298005628585813 and perplexity is 34.117162706797664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.618111038208008 and perplexity of 101.30249474109524
Annealing...
Model not improving. Stopping early with 101.30245609726761loss at 29 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6d9509c780>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 0.5042748982115124, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 7.153931264864485, 'anneal': 4.224639763492165, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2405190467834473 and batch: 50, loss is 6.414978094100952 and perplexity is 610.9273777580466
At time: 2.2266759872436523 and batch: 100, loss is 5.595131406784057 and perplexity is 269.11301104421045
At time: 3.1951303482055664 and batch: 150, loss is 5.451733570098877 and perplexity is 233.16201845656036
At time: 4.165802717208862 and batch: 200, loss is 5.37559061050415 and perplexity is 216.06744633703727
At time: 5.1383116245269775 and batch: 250, loss is 5.3770669651031495 and perplexity is 216.38667409383143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 5.335135650634766 and perplexity of 207.5008945269608
Finished 1 epochs...
Completing Train Step...
At time: 6.745825529098511 and batch: 50, loss is 5.002243185043335 and perplexity is 148.74645095920656
At time: 7.703711986541748 and batch: 100, loss is 4.797071475982666 and perplexity is 121.15509188644585
At time: 8.660223722457886 and batch: 150, loss is 4.7702099609375 and perplexity is 117.94400299459598
At time: 9.620863199234009 and batch: 200, loss is 4.706816415786744 and perplexity is 110.69917817423786
At time: 10.578726768493652 and batch: 250, loss is 4.70726993560791 and perplexity is 110.74939383176407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.9023384094238285 and perplexity of 134.60417161654036
Finished 2 epochs...
Completing Train Step...
At time: 12.17353367805481 and batch: 50, loss is 4.560021085739136 and perplexity is 95.58549529963642
At time: 13.129700183868408 and batch: 100, loss is 4.42349702835083 and perplexity is 83.38738411773897
At time: 14.088381052017212 and batch: 150, loss is 4.46485466003418 and perplexity is 86.90839735829269
At time: 15.052383184432983 and batch: 200, loss is 4.434076375961304 and perplexity is 84.27425120656699
At time: 16.011918544769287 and batch: 250, loss is 4.452500238418579 and perplexity is 85.84129963569058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.7876838684082035 and perplexity of 120.02305729001611
Finished 3 epochs...
Completing Train Step...
At time: 17.59325861930847 and batch: 50, loss is 4.344859046936035 and perplexity is 77.08117188619944
At time: 18.565825939178467 and batch: 100, loss is 4.224648675918579 and perplexity is 68.35048615809717
At time: 19.527875185012817 and batch: 150, loss is 4.281347408294677 and perplexity is 72.33784298145368
At time: 20.50140690803528 and batch: 200, loss is 4.260909547805786 and perplexity is 70.87441781798015
At time: 21.45990538597107 and batch: 250, loss is 4.287446374893189 and perplexity is 72.7803771985512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.74140510559082 and perplexity of 114.59510683461333
Finished 4 epochs...
Completing Train Step...
At time: 23.063156127929688 and batch: 50, loss is 4.190305910110474 and perplexity is 66.04299108924069
At time: 24.021140575408936 and batch: 100, loss is 4.084988045692444 and perplexity is 59.44122716828552
At time: 24.979007482528687 and batch: 150, loss is 4.145253968238831 and perplexity is 63.133653808730024
At time: 25.942493438720703 and batch: 200, loss is 4.128572430610657 and perplexity is 62.0892229630323
At time: 26.902143001556396 and batch: 250, loss is 4.156749401092529 and perplexity is 63.86358991454479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.729531478881836 and perplexity of 113.24249341803339
Finished 5 epochs...
Completing Train Step...
At time: 28.489805698394775 and batch: 50, loss is 4.070696392059326 and perplexity is 58.59775539790414
At time: 29.471657514572144 and batch: 100, loss is 3.9747057962417602 and perplexity is 53.234452825543855
At time: 30.428983211517334 and batch: 150, loss is 4.0362504911422725 and perplexity is 56.613670886549116
At time: 31.39191246032715 and batch: 200, loss is 4.023256120681762 and perplexity is 55.88277095075293
At time: 32.34806847572327 and batch: 250, loss is 4.0549882841110225 and perplexity is 57.68448716715674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.723287963867188 and perplexity of 112.53766480370356
Finished 6 epochs...
Completing Train Step...
At time: 33.93043518066406 and batch: 50, loss is 3.9712422990798952 and perplexity is 53.05039437622176
At time: 34.91229271888733 and batch: 100, loss is 3.879913287162781 and perplexity is 48.42001625231462
At time: 35.87133049964905 and batch: 150, loss is 3.9388502264022827 and perplexity is 51.359515524461855
At time: 36.8318030834198 and batch: 200, loss is 3.9313614988327026 and perplexity is 50.97633466389432
At time: 37.790019512176514 and batch: 250, loss is 3.9637551212310793 and perplexity is 52.654679879557115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.731272506713867 and perplexity of 113.43982347956216
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 39.3902907371521 and batch: 50, loss is 3.8689600896835326 and perplexity is 47.892556212725246
At time: 40.34693670272827 and batch: 100, loss is 3.7310141801834105 and perplexity is 41.721399731601394
At time: 41.30586051940918 and batch: 150, loss is 3.7456305742263796 and perplexity is 42.3356946016373
At time: 42.269601583480835 and batch: 200, loss is 3.676634511947632 and perplexity is 39.513188882968066
At time: 43.226969957351685 and batch: 250, loss is 3.646234197616577 and perplexity is 38.33005052756357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.640549087524414 and perplexity of 103.60121810478073
Finished 8 epochs...
Completing Train Step...
At time: 44.80917429924011 and batch: 50, loss is 3.7589164066314695 and perplexity is 42.90191255455387
At time: 45.78137707710266 and batch: 100, loss is 3.6393224716186525 and perplexity is 38.06603716587374
At time: 46.74092149734497 and batch: 150, loss is 3.6741500377655028 and perplexity is 39.41514123419491
At time: 47.69669532775879 and batch: 200, loss is 3.6283882904052733 and perplexity is 37.652083464045226
At time: 48.653562784194946 and batch: 250, loss is 3.6304635047912597 and perplexity is 37.730300740050055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.638973236083984 and perplexity of 103.43808654531031
Finished 9 epochs...
Completing Train Step...
At time: 50.24311089515686 and batch: 50, loss is 3.705715227127075 and perplexity is 40.6791317331786
At time: 51.200040340423584 and batch: 100, loss is 3.5916877222061157 and perplexity is 36.29528061476345
At time: 52.16071963310242 and batch: 150, loss is 3.633881702423096 and perplexity is 37.85949103788414
At time: 53.123943567276 and batch: 200, loss is 3.5981373023986816 and perplexity is 36.53012645215601
At time: 54.09177017211914 and batch: 250, loss is 3.6118631553649903 and perplexity is 37.034990515328005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6426959991455075 and perplexity of 103.82387969574302
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 55.68912363052368 and batch: 50, loss is 3.671591281890869 and perplexity is 39.31441643006026
At time: 56.64562273025513 and batch: 100, loss is 3.550446619987488 and perplexity is 34.82886928363572
At time: 57.607147455215454 and batch: 150, loss is 3.5754629707336427 and perplexity is 35.711150206458846
At time: 58.563868045806885 and batch: 200, loss is 3.516855993270874 and perplexity is 33.67837679435888
At time: 59.52945947647095 and batch: 250, loss is 3.512274994850159 and perplexity is 33.52444904403032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.63104019165039 and perplexity of 102.62075386330396
Finished 11 epochs...
Completing Train Step...
At time: 61.109575271606445 and batch: 50, loss is 3.6438389015197754 and perplexity is 38.23834857765059
At time: 62.08419728279114 and batch: 100, loss is 3.524610342979431 and perplexity is 33.94054586729315
At time: 63.045095920562744 and batch: 150, loss is 3.5570349502563476 and perplexity is 35.059090932745626
At time: 64.00383925437927 and batch: 200, loss is 3.5083381795883177 and perplexity is 33.39272893038794
At time: 64.96440601348877 and batch: 250, loss is 3.514948959350586 and perplexity is 33.614212188917364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.631530380249023 and perplexity of 102.67106971795248
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 66.56454610824585 and batch: 50, loss is 3.631419467926025 and perplexity is 37.7663867623265
At time: 67.5238687992096 and batch: 100, loss is 3.5109362602233887 and perplexity is 33.47959873128347
At time: 68.4841821193695 and batch: 150, loss is 3.5394467735290527 and perplexity is 34.44785645228099
At time: 69.44301772117615 and batch: 200, loss is 3.483732542991638 and perplexity is 32.58110577802679
At time: 70.41832375526428 and batch: 250, loss is 3.484930739402771 and perplexity is 32.62016773931831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629956436157227 and perplexity of 102.50959830119713
Finished 13 epochs...
Completing Train Step...
At time: 71.9995710849762 and batch: 50, loss is 3.6243226385116576 and perplexity is 37.49931396384882
At time: 72.97175145149231 and batch: 100, loss is 3.504193549156189 and perplexity is 33.25461482352627
At time: 73.93084335327148 and batch: 150, loss is 3.534789090156555 and perplexity is 34.28778232076282
At time: 74.88820743560791 and batch: 200, loss is 3.4823087644577027 and perplexity is 32.534750506659044
At time: 75.85134744644165 and batch: 250, loss is 3.4869026565551757 and perplexity is 32.684555470369354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.630002975463867 and perplexity of 102.5143691378409
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 77.43152523040771 and batch: 50, loss is 3.620322136878967 and perplexity is 37.349597567128306
At time: 78.40375971794128 and batch: 100, loss is 3.4996100425720216 and perplexity is 33.102540859773235
At time: 79.363534450531 and batch: 150, loss is 3.529737753868103 and perplexity is 34.11501990954424
At time: 80.32246279716492 and batch: 200, loss is 3.4756423234939575 and perplexity is 32.31858085482082
At time: 81.28388786315918 and batch: 250, loss is 3.4789713430404663 and perplexity is 32.426349324345225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629779434204101 and perplexity of 102.491455507786
Finished 15 epochs...
Completing Train Step...
At time: 82.88795781135559 and batch: 50, loss is 3.6187038135528566 and perplexity is 37.289202724536885
At time: 83.84515833854675 and batch: 100, loss is 3.498167634010315 and perplexity is 33.05482789049296
At time: 84.80077242851257 and batch: 150, loss is 3.5287095880508423 and perplexity is 34.07996203795905
At time: 85.75848340988159 and batch: 200, loss is 3.475446105003357 and perplexity is 32.31223997378654
At time: 86.71727252006531 and batch: 250, loss is 3.479527769088745 and perplexity is 32.4443972104512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629742813110352 and perplexity of 102.48770222731036
Finished 16 epochs...
Completing Train Step...
At time: 88.31026911735535 and batch: 50, loss is 3.617361445426941 and perplexity is 37.23918046901083
At time: 89.28150177001953 and batch: 100, loss is 3.496902766227722 and perplexity is 33.01304433454276
At time: 90.24010753631592 and batch: 150, loss is 3.527837677001953 and perplexity is 34.050260293033894
At time: 91.20054626464844 and batch: 200, loss is 3.475261998176575 and perplexity is 32.30629161740205
At time: 92.16159892082214 and batch: 250, loss is 3.4799619579315184 and perplexity is 32.45848726438123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629756164550781 and perplexity of 102.48907059489625
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 93.75397634506226 and batch: 50, loss is 3.6163339900970457 and perplexity is 37.20093852386642
At time: 94.71087431907654 and batch: 100, loss is 3.495727653503418 and perplexity is 32.974273070841456
At time: 95.6934015750885 and batch: 150, loss is 3.526549582481384 and perplexity is 34.00642857508477
At time: 96.65426635742188 and batch: 200, loss is 3.4735862016677856 and perplexity is 32.25219818415017
At time: 97.61226391792297 and batch: 250, loss is 3.477949514389038 and perplexity is 32.39323207450334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629736328125 and perplexity of 102.48703759821778
Finished 18 epochs...
Completing Train Step...
At time: 99.21221470832825 and batch: 50, loss is 3.6160021305084227 and perplexity is 37.188595083969226
At time: 100.16960668563843 and batch: 100, loss is 3.4954328632354734 and perplexity is 32.96455400866048
At time: 101.12734723091125 and batch: 150, loss is 3.526338815689087 and perplexity is 33.99926190449113
At time: 102.0887804031372 and batch: 200, loss is 3.4735497856140136 and perplexity is 32.251023707751855
At time: 103.05062055587769 and batch: 250, loss is 3.4780646991729736 and perplexity is 32.39696349683822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629732894897461 and perplexity of 102.48668573750186
Finished 19 epochs...
Completing Train Step...
At time: 104.63438558578491 and batch: 50, loss is 3.615686869621277 and perplexity is 37.17687282237374
At time: 105.6081759929657 and batch: 100, loss is 3.4951461362838745 and perplexity is 32.955103537495894
At time: 106.56508755683899 and batch: 150, loss is 3.5261381578445437 and perplexity is 33.992440370301374
At time: 107.52518796920776 and batch: 200, loss is 3.4735133123397826 and perplexity is 32.24984742877143
At time: 108.48401474952698 and batch: 250, loss is 3.478170313835144 and perplexity is 32.400385271884915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629734802246094 and perplexity of 102.4868812155282
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 110.0813000202179 and batch: 50, loss is 3.61542848110199 and perplexity is 37.16726798619658
At time: 111.03953886032104 and batch: 100, loss is 3.4948603820800783 and perplexity is 32.94568782347752
At time: 111.99690866470337 and batch: 150, loss is 3.5258262872695925 and perplexity is 33.98184078131496
At time: 112.95398211479187 and batch: 200, loss is 3.473110876083374 and perplexity is 32.236871532063226
At time: 113.9306378364563 and batch: 250, loss is 3.4776832914352416 and perplexity is 32.38460940040525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.6297344207763675 and perplexity of 102.48684211989317
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 115.54429912567139 and batch: 50, loss is 3.6153654432296753 and perplexity is 37.16492511454858
At time: 116.52089262008667 and batch: 100, loss is 3.4947922086715697 and perplexity is 32.9434418802006
At time: 117.48949575424194 and batch: 150, loss is 3.525751633644104 and perplexity is 33.979304008390685
At time: 118.45103693008423 and batch: 200, loss is 3.4730156898498534 and perplexity is 32.23380317171675
At time: 119.40918707847595 and batch: 250, loss is 3.4775679111480713 and perplexity is 32.3808730704263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629734802246094 and perplexity of 102.4868812155282
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 120.98942542076111 and batch: 50, loss is 3.6153508281707762 and perplexity is 37.16438195094825
At time: 121.96283650398254 and batch: 100, loss is 3.494776363372803 and perplexity is 32.9429198856572
At time: 122.92177224159241 and batch: 150, loss is 3.525734190940857 and perplexity is 33.97871132264336
At time: 123.88323140144348 and batch: 200, loss is 3.4729934120178223 and perplexity is 32.23308508046276
At time: 124.84160137176514 and batch: 250, loss is 3.47754065990448 and perplexity is 32.37999066338995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629734802246094 and perplexity of 102.4868812155282
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 126.44204640388489 and batch: 50, loss is 3.6153478527069094 and perplexity is 37.16427136983713
At time: 127.39911413192749 and batch: 100, loss is 3.4947730922698974 and perplexity is 32.94281212615249
At time: 128.3613305091858 and batch: 150, loss is 3.5257304191589354 and perplexity is 33.978583162595974
At time: 129.32083535194397 and batch: 200, loss is 3.472988095283508 and perplexity is 32.23291370616883
At time: 130.28025150299072 and batch: 250, loss is 3.4775341176986694 and perplexity is 32.37977882751982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 4.629734802246094 and perplexity of 102.4868812155282
Annealing...
Model not improving. Stopping early with 102.48668573750186loss at 23 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6d9509c780>
SETTINGS FOR THIS RUN
{'data': 'ptb', 'dropout': 1.0, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 0.0, 'anneal': 2.0, 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 261 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.2493727207183838 and batch: 50, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 2.2345454692840576 and batch: 100, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 3.201561212539673 and batch: 150, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 4.167110919952393 and batch: 200, loss is 9.169513702392578 and perplexity is 9599.955119496755
At time: 5.135259628295898 and batch: 250, loss is 9.169513702392578 and perplexity is 9599.955119496755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 1 epochs...
Completing Train Step...
At time: 6.745187282562256 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 7.708547592163086 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 8.667937278747559 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 9.628386497497559 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 10.586840391159058 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 2 epochs...
Completing Train Step...
At time: 12.192610502243042 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 13.151254653930664 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 14.112170219421387 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 15.067139863967896 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 16.02776789665222 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 3 epochs...
Completing Train Step...
At time: 17.61885094642639 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 18.59113097190857 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 19.55029582977295 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 20.511494159698486 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 21.468909978866577 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 4 epochs...
Completing Train Step...
At time: 23.08631467819214 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 24.045074701309204 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 25.007598400115967 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 25.96447205543518 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 26.92332363128662 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 5 epochs...
Completing Train Step...
At time: 28.51599359512329 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 29.489311456680298 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 30.44901418685913 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 31.407325506210327 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 32.36510753631592 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 6 epochs...
Completing Train Step...
At time: 33.95226836204529 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 34.92758250236511 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 35.88913369178772 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 36.86266303062439 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 37.82184290885925 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 7 epochs...
Completing Train Step...
At time: 39.432377099990845 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 40.39150357246399 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 41.34954261779785 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 42.30920958518982 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 43.268792390823364 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 8 epochs...
Completing Train Step...
At time: 44.853530168533325 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 45.82751131057739 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 46.78605031967163 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 47.74419164657593 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 48.70421266555786 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 9 epochs...
Completing Train Step...
At time: 50.30530834197998 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 51.260756731033325 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 52.21914052963257 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 53.17889475822449 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 54.13874888420105 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 10 epochs...
Completing Train Step...
At time: 55.73976755142212 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 56.69389247894287 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 57.64627480506897 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 58.62389779090881 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 59.59844779968262 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 11 epochs...
Completing Train Step...
At time: 61.182578325271606 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 62.158610105514526 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 63.11643123626709 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 64.07504343986511 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 65.03667616844177 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 12 epochs...
Completing Train Step...
At time: 66.64192271232605 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 67.60212516784668 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 68.55951738357544 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 69.51773428916931 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 70.47702383995056 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 13 epochs...
Completing Train Step...
At time: 72.0651798248291 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 73.03904342651367 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 73.9965283870697 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 74.95532774925232 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 75.91631555557251 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 14 epochs...
Completing Train Step...
At time: 77.50860524177551 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 78.4854781627655 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 79.44313883781433 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 80.40137553215027 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 81.36428928375244 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 15 epochs...
Completing Train Step...
At time: 82.98010873794556 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 83.93673038482666 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 84.8953218460083 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 85.85313057899475 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 86.82963156700134 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 16 epochs...
Completing Train Step...
At time: 88.42261385917664 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 89.39527893066406 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 90.35287761688232 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 91.3116466999054 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 92.27734375 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 17 epochs...
Completing Train Step...
At time: 93.87715697288513 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 94.83260345458984 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 95.79473066329956 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 96.75241994857788 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 97.71752858161926 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 18 epochs...
Completing Train Step...
At time: 99.3226912021637 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 100.28062701225281 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 101.23885321617126 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 102.19647693634033 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 103.1595184803009 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 19 epochs...
Completing Train Step...
At time: 104.75050592422485 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 105.72437047958374 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 106.68157362937927 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 107.63959169387817 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 108.60254240036011 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 20 epochs...
Completing Train Step...
At time: 110.20425963401794 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 111.16068363189697 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 112.1352174282074 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 113.09611773490906 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 114.0549840927124 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 21 epochs...
Completing Train Step...
At time: 115.64340257644653 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 116.61611604690552 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 117.57245659828186 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 118.53295493125916 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 119.4902126789093 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 22 epochs...
Completing Train Step...
At time: 121.07719087600708 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 122.05125141143799 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 123.00734829902649 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 123.97187685966492 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 124.93133425712585 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 23 epochs...
Completing Train Step...
At time: 126.5362696647644 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 127.49510669708252 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 128.4521837234497 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 129.41381525993347 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 130.37188696861267 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 24 epochs...
Completing Train Step...
At time: 131.9604196548462 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 132.93214440345764 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 133.89343452453613 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 134.85659456253052 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 135.8118360042572 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 25 epochs...
Completing Train Step...
At time: 137.41230821609497 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 138.37221908569336 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 139.33341002464294 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 140.30006098747253 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 141.25751447677612 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 26 epochs...
Completing Train Step...
At time: 142.86333084106445 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 143.82003593444824 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 144.78024649620056 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 145.7403907775879 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 146.69997692108154 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 27 epochs...
Completing Train Step...
At time: 148.2996244430542 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 149.2729651927948 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 150.23427319526672 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 151.1954846382141 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 152.1527042388916 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 28 epochs...
Completing Train Step...
At time: 153.75630807876587 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 154.71297979354858 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 155.67321276664734 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 156.63395380973816 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 157.593003988266 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 29 epochs...
Completing Train Step...
At time: 159.17660975456238 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 160.14832973480225 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 161.11067152023315 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 162.08235144615173 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 163.0404453277588 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 30 epochs...
Completing Train Step...
At time: 164.62784838676453 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 165.60078954696655 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 166.56062388420105 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 167.5206069946289 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 168.4791762828827 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 31 epochs...
Completing Train Step...
At time: 170.08178901672363 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 171.03816938400269 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 172.00019097328186 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 172.9594213962555 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 173.91884088516235 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 32 epochs...
Completing Train Step...
At time: 175.5059359073639 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 176.47795367240906 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 177.43882179260254 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 178.39714217185974 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 179.35629749298096 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 33 epochs...
Completing Train Step...
At time: 180.95522928237915 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 181.91413831710815 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 182.8707139492035 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 183.83195209503174 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 184.79142808914185 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 34 epochs...
Completing Train Step...
At time: 186.38706064224243 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 187.3440179824829 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 188.3084864616394 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 189.2796139717102 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 190.24589490890503 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 35 epochs...
Completing Train Step...
At time: 191.8328242301941 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 192.8063564300537 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 193.7667863368988 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 194.72551798820496 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 195.68922066688538 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 36 epochs...
Completing Train Step...
At time: 197.29644131660461 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 198.25627970695496 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 199.21308541297913 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 200.1708950996399 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 201.13054394721985 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 37 epochs...
Completing Train Step...
At time: 202.71576261520386 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 203.69057202339172 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 204.64806747436523 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 205.60894656181335 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 206.5716197490692 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 38 epochs...
Completing Train Step...
At time: 208.15648674964905 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 209.13014030456543 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 210.08940267562866 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 211.07364058494568 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 212.03361582756042 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 39 epochs...
Completing Train Step...
At time: 213.64216375350952 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 214.60086750984192 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 215.55720210075378 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 216.51976108551025 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 217.48182773590088 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 40 epochs...
Completing Train Step...
At time: 219.07487177848816 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 220.0482738018036 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 221.00798559188843 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 221.97198915481567 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 222.928852558136 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 41 epochs...
Completing Train Step...
At time: 224.54430770874023 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 225.5160095691681 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 226.47660040855408 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 227.44328474998474 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 228.40599131584167 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 42 epochs...
Completing Train Step...
At time: 230.01010251045227 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 230.9660289287567 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 231.9251971244812 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 232.88830518722534 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 233.84887886047363 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 43 epochs...
Completing Train Step...
At time: 235.43766403198242 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 236.40933656692505 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 237.3685507774353 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 238.33253002166748 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 239.28975176811218 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 44 epochs...
Completing Train Step...
At time: 240.8896782398224 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 241.84613513946533 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 242.8047001361847 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 243.76500868797302 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 244.72522020339966 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 45 epochs...
Completing Train Step...
At time: 246.31484818458557 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 247.28828263282776 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 248.24866676330566 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 249.2136251926422 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 250.174498796463 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 46 epochs...
Completing Train Step...
At time: 251.7583749294281 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 252.7309365272522 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 253.690274477005 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 254.64966368675232 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 255.60953974723816 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 47 epochs...
Completing Train Step...
At time: 257.21086835861206 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 258.16645550727844 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 259.13171219825745 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 260.09422302246094 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 261.0702052116394 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 48 epochs...
Completing Train Step...
At time: 262.65694785118103 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 263.633229970932 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 264.5948836803436 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 265.5564422607422 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 266.51428985595703 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished 49 epochs...
Completing Train Step...
At time: 268.10680747032166 and batch: 50, loss is 9.181751594543456 and perplexity is 9718.160149854793
At time: 269.06375908851624 and batch: 100, loss is 9.180827426910401 and perplexity is 9709.183089584652
At time: 270.02045369148254 and batch: 150, loss is 9.182317714691163 and perplexity is 9723.66335370457
At time: 270.97673773765564 and batch: 200, loss is 9.181509494781494 and perplexity is 9715.807670374654
At time: 271.9395580291748 and batch: 250, loss is 9.181962814331055 and perplexity is 9720.213034374774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 21 batches
Done Evaluating: Achieved loss of 9.640282440185548 and perplexity of 15371.684700468142
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f6d9509c780>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'data': 'ptb', 'dropout': 0.6015664662908425, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 23.662790968094207, 'anneal': 4.35545433808363, 'seq_len': 50}, 'best_accuracy': -158.5329121250569}, {'params': {'data': 'ptb', 'dropout': 0.6585271305567568, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 6.407833584085658, 'anneal': 7.763844023126149, 'seq_len': 50}, 'best_accuracy': -99.76855086719208}, {'params': {'data': 'ptb', 'dropout': 0.6290737966847788, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 5.025658666082009, 'anneal': 7.726592178557317, 'seq_len': 50}, 'best_accuracy': -100.76180844962074}, {'params': {'data': 'ptb', 'dropout': 0.2600879040070142, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 7.534100279776419, 'anneal': 7.011583710499144, 'seq_len': 50}, 'best_accuracy': -101.30245609726761}, {'params': {'data': 'ptb', 'dropout': 0.5042748982115124, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 7.153931264864485, 'anneal': 4.224639763492165, 'seq_len': 50}, 'best_accuracy': -102.48668573750186}, {'params': {'data': 'ptb', 'dropout': 1.0, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'tune_wordvecs': True, 'batch_size': 80, 'lr': 0.0, 'anneal': 2.0, 'seq_len': 50}, 'best_accuracy': -15371.684700468142}]
