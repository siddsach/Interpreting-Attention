Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'anneal': 4.437270328169882, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.10716540860374768, 'batch_size': 50, 'num_layers': 1, 'lr': 3.8680240822552636, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.8887588977813721 and batch: 50, loss is 6.660686178207397 and perplexity is 781.0867179691709
At time: 1.3624722957611084 and batch: 100, loss is 5.849458522796631 and perplexity is 347.0464118724297
At time: 1.806016206741333 and batch: 150, loss is 5.619383850097656 and perplexity is 275.71944637691
At time: 2.2356278896331787 and batch: 200, loss is 5.462705497741699 and perplexity is 235.73434111584902
At time: 2.662418842315674 and batch: 250, loss is 5.370330028533935 and perplexity is 214.93379028415478
At time: 3.088890790939331 and batch: 300, loss is 5.1903651905059816 and perplexity is 179.5341051122738
At time: 3.5174546241760254 and batch: 350, loss is 5.188357629776001 and perplexity is 179.17404103933526
At time: 3.9464597702026367 and batch: 400, loss is 5.089604749679565 and perplexity is 162.32569009130688
At time: 4.375231504440308 and batch: 450, loss is 5.06450400352478 and perplexity is 158.3019054112332
At time: 4.804429769515991 and batch: 500, loss is 5.032433309555054 and perplexity is 153.30559918197858
At time: 5.2335896492004395 and batch: 550, loss is 5.0499428939819335 and perplexity is 156.01355491911664
At time: 5.662971258163452 and batch: 600, loss is 5.052531566619873 and perplexity is 156.41794613229132
At time: 6.092395305633545 and batch: 650, loss is 5.004339895248413 and perplexity is 149.0586563486386
At time: 6.521095037460327 and batch: 700, loss is 4.968687038421631 and perplexity is 143.83790968891506
At time: 6.951190710067749 and batch: 750, loss is 4.886817436218262 and perplexity is 132.53111343236884
At time: 7.380528450012207 and batch: 800, loss is 4.915340766906739 and perplexity is 136.36577082781508
At time: 7.810593843460083 and batch: 850, loss is 4.864175901412964 and perplexity is 129.56412099468844
At time: 8.239538192749023 and batch: 900, loss is 4.9955786609649655 and perplexity is 147.7584226813503
At time: 8.666536331176758 and batch: 950, loss is 4.90129301071167 and perplexity is 134.46353011491198
At time: 9.094820499420166 and batch: 1000, loss is 4.854226837158203 and perplexity is 128.2814703965362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.876676978134528 and perplexity of 131.19397827133437
Finished 1 epochs...
Completing Train Step...
At time: 10.343971967697144 and batch: 50, loss is 4.839101152420044 and perplexity is 126.35572613307014
At time: 10.778136253356934 and batch: 100, loss is 4.724131479263305 and perplexity is 112.63263210416773
At time: 11.200823068618774 and batch: 150, loss is 4.725128374099731 and perplexity is 112.74497097924417
At time: 11.623331069946289 and batch: 200, loss is 4.724986934661866 and perplexity is 112.72902552161153
At time: 12.04596996307373 and batch: 250, loss is 4.700385875701905 and perplexity is 109.98960658108899
At time: 12.469089031219482 and batch: 300, loss is 4.566554012298584 and perplexity is 96.21199252255623
At time: 12.891127109527588 and batch: 350, loss is 4.614784097671508 and perplexity is 100.96602737869608
At time: 13.323489904403687 and batch: 400, loss is 4.5326026248931885 and perplexity is 93.00029125226979
At time: 13.75369930267334 and batch: 450, loss is 4.578045177459717 and perplexity is 97.32395706730622
At time: 14.175700187683105 and batch: 500, loss is 4.529132204055786 and perplexity is 92.6781004957482
At time: 14.597214221954346 and batch: 550, loss is 4.568925790786743 and perplexity is 96.44045688305286
At time: 15.018984079360962 and batch: 600, loss is 4.57990532875061 and perplexity is 97.5051628345006
At time: 15.461031913757324 and batch: 650, loss is 4.552079553604126 and perplexity is 94.82940624358496
At time: 15.881975889205933 and batch: 700, loss is 4.515418663024902 and perplexity is 91.41583043997687
At time: 16.303478717803955 and batch: 750, loss is 4.461502304077149 and perplexity is 86.61753728034749
At time: 16.725403547286987 and batch: 800, loss is 4.480534143447876 and perplexity is 88.28181523726103
At time: 17.14850091934204 and batch: 850, loss is 4.452101993560791 and perplexity is 85.80712058579199
At time: 17.571288585662842 and batch: 900, loss is 4.612181329727173 and perplexity is 100.70357793482916
At time: 17.993133068084717 and batch: 950, loss is 4.519487504959106 and perplexity is 91.78854474791893
At time: 18.416136264801025 and batch: 1000, loss is 4.470812740325928 and perplexity is 87.42775020439639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.666951993616616 and perplexity of 106.3730221585547
Finished 2 epochs...
Completing Train Step...
At time: 19.686119079589844 and batch: 50, loss is 4.518609571456909 and perplexity is 91.7079958728185
At time: 20.110494375228882 and batch: 100, loss is 4.395185480117798 and perplexity is 81.05966432455854
At time: 20.535754203796387 and batch: 150, loss is 4.423125600814819 and perplexity is 83.35641749840525
At time: 20.960275888442993 and batch: 200, loss is 4.445805578231812 and perplexity is 85.26854065718459
At time: 21.38623023033142 and batch: 250, loss is 4.425075092315674 and perplexity is 83.51907862758817
At time: 21.811453104019165 and batch: 300, loss is 4.30867109298706 and perplexity is 74.34163015416175
At time: 22.236608028411865 and batch: 350, loss is 4.370904417037964 and perplexity is 79.11515244423093
At time: 22.66071081161499 and batch: 400, loss is 4.284337573051452 and perplexity is 72.55446876209952
At time: 23.084359884262085 and batch: 450, loss is 4.351438331604004 and perplexity is 77.58998282919103
At time: 23.508009910583496 and batch: 500, loss is 4.292191004753112 and perplexity is 73.1265136447454
At time: 23.932731866836548 and batch: 550, loss is 4.343893718719483 and perplexity is 77.0067991588343
At time: 24.358317852020264 and batch: 600, loss is 4.354084863662719 and perplexity is 77.79559917129062
At time: 24.782981395721436 and batch: 650, loss is 4.332733011245727 and perplexity is 76.15212704423803
At time: 25.208666563034058 and batch: 700, loss is 4.298498272895813 and perplexity is 73.5891997835489
At time: 25.634177446365356 and batch: 750, loss is 4.257525606155395 and perplexity is 70.63498825988249
At time: 26.073802709579468 and batch: 800, loss is 4.264331836700439 and perplexity is 71.11738606762664
At time: 26.497983932495117 and batch: 850, loss is 4.248616070747375 and perplexity is 70.00845851986682
At time: 26.923700094223022 and batch: 900, loss is 4.420934314727783 and perplexity is 83.1739597220787
At time: 27.348572969436646 and batch: 950, loss is 4.333614387512207 and perplexity is 76.21927530875325
At time: 27.773115634918213 and batch: 1000, loss is 4.280279111862183 and perplexity is 72.26060598521161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.583704878644245 and perplexity of 97.87634327833739
Finished 3 epochs...
Completing Train Step...
At time: 29.049957275390625 and batch: 50, loss is 4.336836252212525 and perplexity is 76.4652395202922
At time: 29.474075078964233 and batch: 100, loss is 4.212623882293701 and perplexity is 67.53350751196739
At time: 29.89828085899353 and batch: 150, loss is 4.2458471584320066 and perplexity is 69.81487936242662
At time: 30.320709228515625 and batch: 200, loss is 4.275213422775269 and perplexity is 71.89548180563803
At time: 30.743560791015625 and batch: 250, loss is 4.25710165977478 and perplexity is 70.60504915900006
At time: 31.167471170425415 and batch: 300, loss is 4.153048276901245 and perplexity is 63.62765970978452
At time: 31.59326410293579 and batch: 350, loss is 4.218041181564331 and perplexity is 67.9003494821777
At time: 32.020774364471436 and batch: 400, loss is 4.129103336334229 and perplexity is 62.122195238685975
At time: 32.447559118270874 and batch: 450, loss is 4.205021390914917 and perplexity is 67.02203131354051
At time: 32.8718159198761 and batch: 500, loss is 4.141049737930298 and perplexity is 62.86878256789358
At time: 33.29679036140442 and batch: 550, loss is 4.200492510795593 and perplexity is 66.719182868157
At time: 33.72156834602356 and batch: 600, loss is 4.207867522239685 and perplexity is 67.21305652878664
At time: 34.148555517196655 and batch: 650, loss is 4.187672486305237 and perplexity is 65.86930070491108
At time: 34.575035572052 and batch: 700, loss is 4.155155053138733 and perplexity is 63.76185025640173
At time: 35.00215125083923 and batch: 750, loss is 4.120207195281982 and perplexity is 61.571998370444966
At time: 35.428302526474 and batch: 800, loss is 4.121139483451843 and perplexity is 61.629427982434734
At time: 35.85503530502319 and batch: 850, loss is 4.109849367141724 and perplexity is 60.9375376827658
At time: 36.28028726577759 and batch: 900, loss is 4.287796397209167 and perplexity is 72.80585641362282
At time: 36.727989196777344 and batch: 950, loss is 4.205681900978089 and perplexity is 67.06631466286629
At time: 37.154541015625 and batch: 1000, loss is 4.145783658027649 and perplexity is 63.16710391879207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.54417270567359 and perplexity of 94.08256103576706
Finished 4 epochs...
Completing Train Step...
At time: 38.41658163070679 and batch: 50, loss is 4.207004165649414 and perplexity is 67.15505273606153
At time: 38.85672664642334 and batch: 100, loss is 4.082400712966919 and perplexity is 59.28763172293318
At time: 39.2843451499939 and batch: 150, loss is 4.121145963668823 and perplexity is 61.62982735579442
At time: 39.71115803718567 and batch: 200, loss is 4.150131416320801 and perplexity is 63.44233710884069
At time: 40.13801193237305 and batch: 250, loss is 4.133710098266602 and perplexity is 62.4090376028222
At time: 40.566396713256836 and batch: 300, loss is 4.038660144805908 and perplexity is 56.75025471979111
At time: 40.99380850791931 and batch: 350, loss is 4.10678731918335 and perplexity is 60.75122940792323
At time: 41.421292304992676 and batch: 400, loss is 4.015176916122437 and perplexity is 55.43310154441588
At time: 41.84929871559143 and batch: 450, loss is 4.093314142227173 and perplexity is 59.93820664232747
At time: 42.2765839099884 and batch: 500, loss is 4.024669766426086 and perplexity is 55.961825256314675
At time: 42.70581889152527 and batch: 550, loss is 4.089901247024536 and perplexity is 59.73399250337214
At time: 43.135546922683716 and batch: 600, loss is 4.09821159362793 and perplexity is 60.232471081139295
At time: 43.56229329109192 and batch: 650, loss is 4.074297218322754 and perplexity is 58.809136078676396
At time: 43.993671894073486 and batch: 700, loss is 4.045088815689087 and perplexity is 57.116258628746756
At time: 44.43425512313843 and batch: 750, loss is 4.013423199653626 and perplexity is 55.33597279435789
At time: 44.86136841773987 and batch: 800, loss is 4.011019425392151 and perplexity is 55.203117348443186
At time: 45.28615760803223 and batch: 850, loss is 4.0021841430664065 and perplexity is 54.717530528559884
At time: 45.71424221992493 and batch: 900, loss is 4.180881090164185 and perplexity is 65.42347180420934
At time: 46.14019298553467 and batch: 950, loss is 4.106343326568603 and perplexity is 60.72426229777121
At time: 46.56758666038513 and batch: 1000, loss is 4.0433532333374025 and perplexity is 57.017214632644375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.525293861947408 and perplexity of 92.32305207465023
Finished 5 epochs...
Completing Train Step...
At time: 47.857046127319336 and batch: 50, loss is 4.106503319740296 and perplexity is 60.73397854234059
At time: 48.2973153591156 and batch: 100, loss is 3.9824356126785276 and perplexity is 53.64753986064465
At time: 48.72934913635254 and batch: 150, loss is 4.0195924663543705 and perplexity is 55.67841037696624
At time: 49.16121697425842 and batch: 200, loss is 4.050655145645141 and perplexity is 57.43507306011035
At time: 49.59269881248474 and batch: 250, loss is 4.0344610595703125 and perplexity is 56.51245518275642
At time: 50.02496647834778 and batch: 300, loss is 3.9452950525283814 and perplexity is 51.691587595913255
At time: 50.45740866661072 and batch: 350, loss is 4.015119271278381 and perplexity is 55.42990620402017
At time: 50.888551235198975 and batch: 400, loss is 3.9238320207595825 and perplexity is 50.593950851599615
At time: 51.320104360580444 and batch: 450, loss is 4.005035610198974 and perplexity is 54.873778430446535
At time: 51.75131845474243 and batch: 500, loss is 3.9322682666778563 and perplexity is 51.022579328450114
At time: 52.18305969238281 and batch: 550, loss is 3.9993136882781983 and perplexity is 54.56069153836015
At time: 52.61489534378052 and batch: 600, loss is 4.008468656539917 and perplexity is 55.062486390947946
At time: 53.04618453979492 and batch: 650, loss is 3.9833900213241575 and perplexity is 53.69876597794057
At time: 53.4781334400177 and batch: 700, loss is 3.955226836204529 and perplexity is 52.207535160496256
At time: 53.91012406349182 and batch: 750, loss is 3.926705060005188 and perplexity is 50.73951826830663
At time: 54.34253239631653 and batch: 800, loss is 3.920764045715332 and perplexity is 50.43896773672572
At time: 54.77431583404541 and batch: 850, loss is 3.9156056880950927 and perplexity is 50.179455407284244
At time: 55.206249952316284 and batch: 900, loss is 4.093709836006164 and perplexity is 59.961928510808136
At time: 55.63815975189209 and batch: 950, loss is 4.021848378181457 and perplexity is 55.804157745608286
At time: 56.07085108757019 and batch: 1000, loss is 3.957485990524292 and perplexity is 52.32561336730336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.52351528260766 and perplexity of 92.158994139922
Finished 6 epochs...
Completing Train Step...
At time: 57.351524114608765 and batch: 50, loss is 4.022556614875794 and perplexity is 55.8436942967858
At time: 57.7946572303772 and batch: 100, loss is 3.898677735328674 and perplexity is 49.33716916059463
At time: 58.22346591949463 and batch: 150, loss is 3.9375588655471803 and perplexity is 51.293234662043034
At time: 58.666383028030396 and batch: 200, loss is 3.9653752183914186 and perplexity is 52.74005471599988
At time: 59.09640455245972 and batch: 250, loss is 3.951921968460083 and perplexity is 52.03528095698998
At time: 59.527787923812866 and batch: 300, loss is 3.866107258796692 and perplexity is 47.7561215541486
At time: 59.95831537246704 and batch: 350, loss is 3.940923390388489 and perplexity is 51.46610267053644
At time: 60.3881049156189 and batch: 400, loss is 3.8470111513137817 and perplexity is 46.85281776691818
At time: 60.819347858428955 and batch: 450, loss is 3.9306842136383056 and perplexity is 50.94182083633338
At time: 61.25001120567322 and batch: 500, loss is 3.851338529586792 and perplexity is 47.0560069537081
At time: 61.68199634552002 and batch: 550, loss is 3.923796310424805 and perplexity is 50.59214415693601
At time: 62.11314558982849 and batch: 600, loss is 3.9304795694351196 and perplexity is 50.93139695462934
At time: 62.54446887969971 and batch: 650, loss is 3.904175081253052 and perplexity is 49.609139519040646
At time: 62.97402095794678 and batch: 700, loss is 3.8796359205245974 and perplexity is 48.40658801754418
At time: 63.40293264389038 and batch: 750, loss is 3.853065528869629 and perplexity is 47.137342857278355
At time: 63.83184885978699 and batch: 800, loss is 3.8463368463516234 and perplexity is 46.82123532870093
At time: 64.2603919506073 and batch: 850, loss is 3.8413224267959594 and perplexity is 46.587041673990775
At time: 64.68734908103943 and batch: 900, loss is 4.02169445514679 and perplexity is 55.795568861331546
At time: 65.11451411247253 and batch: 950, loss is 3.9502215909957887 and perplexity is 51.946876519659185
At time: 65.54213190078735 and batch: 1000, loss is 3.8823267126083376 and perplexity is 48.53701547927827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.525214218511814 and perplexity of 92.3156994423968
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 66.81371784210205 and batch: 50, loss is 3.964041213989258 and perplexity is 52.669746157228566
At time: 67.25573205947876 and batch: 100, loss is 3.8336758279800414 and perplexity is 46.23216777459417
At time: 67.68272566795349 and batch: 150, loss is 3.8701621103286743 and perplexity is 47.95015866677545
At time: 68.10994529724121 and batch: 200, loss is 3.8871150493621824 and perplexity is 48.769984376056094
At time: 68.53677296638489 and batch: 250, loss is 3.863512053489685 and perplexity is 47.63234529592068
At time: 68.9643075466156 and batch: 300, loss is 3.7743462133407593 and perplexity is 43.56901416197429
At time: 69.39118909835815 and batch: 350, loss is 3.8484771394729616 and perplexity is 46.92155381379393
At time: 69.83479142189026 and batch: 400, loss is 3.7389699602127076 and perplexity is 42.05464988510019
At time: 70.26271224021912 and batch: 450, loss is 3.8113502931594847 and perplexity is 45.211446374808794
At time: 70.68996977806091 and batch: 500, loss is 3.720534629821777 and perplexity is 41.28646118261113
At time: 71.11764335632324 and batch: 550, loss is 3.787181172370911 and perplexity is 44.13182477304374
At time: 71.545081615448 and batch: 600, loss is 3.784864001274109 and perplexity is 44.029682170875965
At time: 71.9724109172821 and batch: 650, loss is 3.7468937635421753 and perplexity is 42.38920638937866
At time: 72.40018129348755 and batch: 700, loss is 3.708261976242065 and perplexity is 40.78286330900179
At time: 72.82702612876892 and batch: 750, loss is 3.6753822994232177 and perplexity is 39.46374093909793
At time: 73.25355339050293 and batch: 800, loss is 3.65025447845459 and perplexity is 38.48445826849074
At time: 73.6798198223114 and batch: 850, loss is 3.6370206308364867 and perplexity is 37.97851597767408
At time: 74.10644745826721 and batch: 900, loss is 3.801305742263794 and perplexity is 44.75959084265641
At time: 74.53352975845337 and batch: 950, loss is 3.7178487825393676 and perplexity is 41.17572083544507
At time: 74.9600841999054 and batch: 1000, loss is 3.6351253175735474 and perplexity is 37.90660296300731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.464046943478468 and perplexity of 86.83822834906614
Finished 8 epochs...
Completing Train Step...
At time: 76.23955631256104 and batch: 50, loss is 3.8782157945632934 and perplexity is 48.33789335428471
At time: 76.66664004325867 and batch: 100, loss is 3.7479412269592287 and perplexity is 42.43363079475739
At time: 77.09484148025513 and batch: 150, loss is 3.7890319776535035 and perplexity is 44.213579820463444
At time: 77.52216362953186 and batch: 200, loss is 3.8108834552764894 and perplexity is 45.19034488476673
At time: 77.9495759010315 and batch: 250, loss is 3.7924902772903444 and perplexity is 44.36674832613794
At time: 78.37714838981628 and batch: 300, loss is 3.7099227476119996 and perplexity is 40.85065059478037
At time: 78.8034086227417 and batch: 350, loss is 3.7851354026794435 and perplexity is 44.04163351022536
At time: 79.22989964485168 and batch: 400, loss is 3.6796384191513063 and perplexity is 39.632061287142704
At time: 79.65590238571167 and batch: 450, loss is 3.7583568048477174 and perplexity is 42.87791128396522
At time: 80.08308291435242 and batch: 500, loss is 3.671006760597229 and perplexity is 39.29144303138432
At time: 80.52187538146973 and batch: 550, loss is 3.73898775100708 and perplexity is 42.05539807738415
At time: 80.94827771186829 and batch: 600, loss is 3.740322332382202 and perplexity is 42.11156189763965
At time: 81.3758819103241 and batch: 650, loss is 3.7068728971481324 and perplexity is 40.72625201407448
At time: 81.80382370948792 and batch: 700, loss is 3.67235848903656 and perplexity is 39.34459030460012
At time: 82.2317750453949 and batch: 750, loss is 3.644992356300354 and perplexity is 38.28248023065903
At time: 82.65909433364868 and batch: 800, loss is 3.6244790983200073 and perplexity is 37.50518155833423
At time: 83.08599495887756 and batch: 850, loss is 3.6170899772644045 and perplexity is 37.22907258916041
At time: 83.5144350528717 and batch: 900, loss is 3.787096037864685 and perplexity is 44.128067791859486
At time: 83.94252586364746 and batch: 950, loss is 3.709901132583618 and perplexity is 40.8497676163512
At time: 84.37050318717957 and batch: 1000, loss is 3.6320010566711427 and perplexity is 37.78835765620423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.463377045422066 and perplexity of 86.78007506923296
Finished 9 epochs...
Completing Train Step...
At time: 85.6318666934967 and batch: 50, loss is 3.841398448944092 and perplexity is 46.590583455599194
At time: 86.0715582370758 and batch: 100, loss is 3.710683002471924 and perplexity is 40.88171930900017
At time: 86.49837756156921 and batch: 150, loss is 3.7516477251052858 and perplexity is 42.59120280842155
At time: 86.92540740966797 and batch: 200, loss is 3.775504217147827 and perplexity is 43.61949646995517
At time: 87.35253882408142 and batch: 250, loss is 3.758097357749939 and perplexity is 42.86678817731505
At time: 87.77980184555054 and batch: 300, loss is 3.676955041885376 and perplexity is 39.525856072939156
At time: 88.20609498023987 and batch: 350, loss is 3.752484164237976 and perplexity is 42.62684266036439
At time: 88.64456820487976 and batch: 400, loss is 3.6486676263809206 and perplexity is 38.42343755431144
At time: 89.0773549079895 and batch: 450, loss is 3.728956480026245 and perplexity is 41.63563786717804
At time: 89.50549411773682 and batch: 500, loss is 3.6426514959335328 and perplexity is 38.19297109501058
At time: 89.93302392959595 and batch: 550, loss is 3.7117806673049927 and perplexity is 40.92661837215425
At time: 90.36097168922424 and batch: 600, loss is 3.714404988288879 and perplexity is 41.03416401096337
At time: 90.78942251205444 and batch: 650, loss is 3.6822029733657837 and perplexity is 39.73383029723958
At time: 91.21811580657959 and batch: 700, loss is 3.6497819232940674 and perplexity is 38.46627653541029
At time: 91.65805459022522 and batch: 750, loss is 3.6246553659439087 and perplexity is 37.5117930902539
At time: 92.08586549758911 and batch: 800, loss is 3.605614700317383 and perplexity is 36.80430052050799
At time: 92.51326203346252 and batch: 850, loss is 3.6005953454971316 and perplexity is 36.62002952491498
At time: 92.93889880180359 and batch: 900, loss is 3.7730427742004395 and perplexity is 43.51226159840176
At time: 93.36585402488708 and batch: 950, loss is 3.69823233127594 and perplexity is 40.375870079849655
At time: 93.7926881313324 and batch: 1000, loss is 3.621735248565674 and perplexity is 37.4024140289349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.465490201624428 and perplexity of 86.96364881479523
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 95.04181218147278 and batch: 50, loss is 3.8219404220581055 and perplexity is 45.69278564351944
At time: 95.47971057891846 and batch: 100, loss is 3.6945235443115236 and perplexity is 40.226401923324886
At time: 95.90456676483154 and batch: 150, loss is 3.738516297340393 and perplexity is 42.03557557881856
At time: 96.33000135421753 and batch: 200, loss is 3.7634756708145143 and perplexity is 43.097960284943866
At time: 96.75632214546204 and batch: 250, loss is 3.7442949533462526 and perplexity is 42.279187908112654
At time: 97.18151211738586 and batch: 300, loss is 3.6610779428482054 and perplexity is 38.90325576374516
At time: 97.60771083831787 and batch: 350, loss is 3.7317783784866334 and perplexity is 41.753295340217115
At time: 98.03319835662842 and batch: 400, loss is 3.626980037689209 and perplexity is 37.59909713307053
At time: 98.45713996887207 and batch: 450, loss is 3.7027953338623045 and perplexity is 40.56052625235347
At time: 98.88279676437378 and batch: 500, loss is 3.6129922533035277 and perplexity is 37.076830262913994
At time: 99.3086485862732 and batch: 550, loss is 3.677407693862915 and perplexity is 39.543751579767225
At time: 99.73440074920654 and batch: 600, loss is 3.6776523780822754 and perplexity is 39.55342849559911
At time: 100.156991481781 and batch: 650, loss is 3.6382731914520265 and perplexity is 38.02611617586777
At time: 100.58239889144897 and batch: 700, loss is 3.6012796640396116 and perplexity is 36.645097866532275
At time: 101.00803780555725 and batch: 750, loss is 3.5738511657714844 and perplexity is 35.65363715970993
At time: 101.43355894088745 and batch: 800, loss is 3.5493495941162108 and perplexity is 34.79068206297849
At time: 101.85899138450623 and batch: 850, loss is 3.53836669921875 and perplexity is 34.41067029300454
At time: 102.30488228797913 and batch: 900, loss is 3.706603078842163 and perplexity is 40.71526480808887
At time: 102.72925925254822 and batch: 950, loss is 3.6287672567367553 and perplexity is 37.66635504004085
At time: 103.15387845039368 and batch: 1000, loss is 3.548917555809021 and perplexity is 34.77565440209036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.453475207817264 and perplexity of 85.92503308812277
Finished 11 epochs...
Completing Train Step...
At time: 104.42491626739502 and batch: 50, loss is 3.8042168760299684 and perplexity is 44.89008184514438
At time: 104.85416579246521 and batch: 100, loss is 3.671196756362915 and perplexity is 39.29890894841183
At time: 105.29863810539246 and batch: 150, loss is 3.7152101230621337 and perplexity is 41.06721534690452
At time: 105.7381899356842 and batch: 200, loss is 3.741172161102295 and perplexity is 42.147364723368256
At time: 106.17288208007812 and batch: 250, loss is 3.723339796066284 and perplexity is 41.402439162580855
At time: 106.60535335540771 and batch: 300, loss is 3.6407552909851075 and perplexity is 38.1206180140396
At time: 107.05375790596008 and batch: 350, loss is 3.712890100479126 and perplexity is 40.97204891669822
At time: 107.49758505821228 and batch: 400, loss is 3.609398965835571 and perplexity is 36.94384162966591
At time: 107.92797613143921 and batch: 450, loss is 3.68739426612854 and perplexity is 39.94063657441927
At time: 108.35214829444885 and batch: 500, loss is 3.598761830329895 and perplexity is 36.55294766195586
At time: 108.785400390625 and batch: 550, loss is 3.664336953163147 and perplexity is 39.030248698805785
At time: 109.22575449943542 and batch: 600, loss is 3.6660989475250245 and perplexity is 39.09908039968214
At time: 109.6542661190033 and batch: 650, loss is 3.6288426542282104 and perplexity is 37.66919509578832
At time: 110.079092502594 and batch: 700, loss is 3.5939035511016844 and perplexity is 36.37579391522857
At time: 110.50421953201294 and batch: 750, loss is 3.5682763624191285 and perplexity is 35.455428145222704
At time: 110.92964696884155 and batch: 800, loss is 3.5459304094314574 and perplexity is 34.67192942998163
At time: 111.35498070716858 and batch: 850, loss is 3.5371778774261475 and perplexity is 34.36978644488206
At time: 111.7800133228302 and batch: 900, loss is 3.7078269958496093 and perplexity is 40.76512742077562
At time: 112.20577764511108 and batch: 950, loss is 3.6318614149093627 and perplexity is 37.7830811917823
At time: 112.63171410560608 and batch: 1000, loss is 3.553666968345642 and perplexity is 34.941211168563854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.453117928853849 and perplexity of 85.89433936480695
Finished 12 epochs...
Completing Train Step...
At time: 113.88360381126404 and batch: 50, loss is 3.7940212297439575 and perplexity is 44.43472372862447
At time: 114.32126593589783 and batch: 100, loss is 3.6600408792495727 and perplexity is 38.862931526337725
At time: 114.7476499080658 and batch: 150, loss is 3.703427815437317 and perplexity is 40.58618815236488
At time: 115.17470860481262 and batch: 200, loss is 3.7299925756454466 and perplexity is 41.67879872470151
At time: 115.60054636001587 and batch: 250, loss is 3.7122526359558106 and perplexity is 40.945939012019416
At time: 116.02694463729858 and batch: 300, loss is 3.629908699989319 and perplexity is 37.70937359379821
At time: 116.452303647995 and batch: 350, loss is 3.702583222389221 and perplexity is 40.551923811751465
At time: 116.87930345535278 and batch: 400, loss is 3.599771728515625 and perplexity is 36.58988106383151
At time: 117.30606317520142 and batch: 450, loss is 3.67856912612915 and perplexity is 39.58970564988413
At time: 117.7327229976654 and batch: 500, loss is 3.59055383682251 and perplexity is 36.25414925011396
At time: 118.15938067436218 and batch: 550, loss is 3.65663827419281 and perplexity is 38.73092103615134
At time: 118.58709621429443 and batch: 600, loss is 3.6591750383377075 and perplexity is 38.82929697345444
At time: 119.01408982276917 and batch: 650, loss is 3.622789225578308 and perplexity is 37.44185609540287
At time: 119.4411096572876 and batch: 700, loss is 3.588877110481262 and perplexity is 36.19341189727627
At time: 119.86921501159668 and batch: 750, loss is 3.56423837184906 and perplexity is 35.31254812893631
At time: 120.29639339447021 and batch: 800, loss is 3.5427536296844484 and perplexity is 34.56195911499782
At time: 120.72297382354736 and batch: 850, loss is 3.535047516822815 and perplexity is 34.29664434308354
At time: 121.14839148521423 and batch: 900, loss is 3.7068373012542724 and perplexity is 40.724802352531626
At time: 121.58734726905823 and batch: 950, loss is 3.6318005180358885 and perplexity is 37.780780390324026
At time: 122.02451062202454 and batch: 1000, loss is 3.5543070936203005 and perplexity is 34.96358508124953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.453682131883575 and perplexity of 85.94281488504468
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 123.27277541160583 and batch: 50, loss is 3.7886081075668336 and perplexity is 44.194842977827044
At time: 123.71092009544373 and batch: 100, loss is 3.655134429931641 and perplexity is 38.67271953678552
At time: 124.13703489303589 and batch: 150, loss is 3.69924108505249 and perplexity is 40.41661994110689
At time: 124.57545685768127 and batch: 200, loss is 3.727792353630066 and perplexity is 41.587196923276395
At time: 125.00114369392395 and batch: 250, loss is 3.710727834701538 and perplexity is 40.883552168712534
At time: 125.42695498466492 and batch: 300, loss is 3.62687566280365 and perplexity is 37.59517293640749
At time: 125.8538727760315 and batch: 350, loss is 3.6990134716033936 and perplexity is 40.40742162171158
At time: 126.28124856948853 and batch: 400, loss is 3.5946786642074584 and perplexity is 36.40400019994348
At time: 126.70706415176392 and batch: 450, loss is 3.6728813791275026 and perplexity is 39.365168580622026
At time: 127.13347053527832 and batch: 500, loss is 3.5829195976257324 and perplexity is 35.978430194584625
At time: 127.55931448936462 and batch: 550, loss is 3.6477806329727174 and perplexity is 38.38937132897241
At time: 127.9840362071991 and batch: 600, loss is 3.649645280838013 and perplexity is 38.46102076799788
At time: 128.40923762321472 and batch: 650, loss is 3.6114118671417237 and perplexity is 37.01828083098512
At time: 128.83546328544617 and batch: 700, loss is 3.575839023590088 and perplexity is 35.72458201187703
At time: 129.2616844177246 and batch: 750, loss is 3.5503534269332886 and perplexity is 34.82562362617158
At time: 129.68787384033203 and batch: 800, loss is 3.525978298187256 and perplexity is 33.98700678475054
At time: 130.11346411705017 and batch: 850, loss is 3.515570001602173 and perplexity is 33.63509451867385
At time: 130.53956007957458 and batch: 900, loss is 3.686341977119446 and perplexity is 39.89862958715753
At time: 130.9654097557068 and batch: 950, loss is 3.610234761238098 and perplexity is 36.974732029880684
At time: 131.39275813102722 and batch: 1000, loss is 3.532725830078125 and perplexity is 34.21711064042188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.451069529463605 and perplexity of 85.7185735331964
Finished 14 epochs...
Completing Train Step...
At time: 132.65470170974731 and batch: 50, loss is 3.7844841289520263 and perplexity is 44.012959689674126
At time: 133.08141422271729 and batch: 100, loss is 3.650049114227295 and perplexity is 38.47655574893074
At time: 133.50740242004395 and batch: 150, loss is 3.6935139751434325 and perplexity is 40.18581108127879
At time: 133.93200540542603 and batch: 200, loss is 3.721677703857422 and perplexity is 41.333681647514716
At time: 134.3566951751709 and batch: 250, loss is 3.7048999738693236 and perplexity is 40.64598145328354
At time: 134.78125619888306 and batch: 300, loss is 3.6218714618682863 and perplexity is 37.4075090822746
At time: 135.2271614074707 and batch: 350, loss is 3.693705759048462 and perplexity is 40.19351881214047
At time: 135.65061116218567 and batch: 400, loss is 3.589959492683411 and perplexity is 36.23260821101841
At time: 136.07443022727966 and batch: 450, loss is 3.668581976890564 and perplexity is 39.19628519571918
At time: 136.49982213974 and batch: 500, loss is 3.579433979988098 and perplexity is 35.85324145051984
At time: 136.92458200454712 and batch: 550, loss is 3.64450110912323 and perplexity is 38.26367868879271
At time: 137.34914827346802 and batch: 600, loss is 3.6468236684799193 and perplexity is 38.35265163623308
At time: 137.77336597442627 and batch: 650, loss is 3.608963828086853 and perplexity is 36.92776946664619
At time: 138.1978268623352 and batch: 700, loss is 3.5740338706970216 and perplexity is 35.6601518499472
At time: 138.62206864356995 and batch: 750, loss is 3.5493576669692994 and perplexity is 34.790962924177315
At time: 139.0470643043518 and batch: 800, loss is 3.5257678604125977 and perplexity is 33.979855387163944
At time: 139.47280049324036 and batch: 850, loss is 3.5162209129333495 and perplexity is 33.65699510971944
At time: 139.89844942092896 and batch: 900, loss is 3.6874248361587525 and perplexity is 39.941857579549044
At time: 140.32320475578308 and batch: 950, loss is 3.6119999361038206 and perplexity is 37.04005653515233
At time: 140.74959588050842 and batch: 1000, loss is 3.534969840049744 and perplexity is 34.29398039388863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450825761004192 and perplexity of 85.69768059520628
Finished 15 epochs...
Completing Train Step...
At time: 142.00166416168213 and batch: 50, loss is 3.7819197463989256 and perplexity is 43.90023821599773
At time: 142.43968319892883 and batch: 100, loss is 3.64696635723114 and perplexity is 38.35812451865115
At time: 142.86401677131653 and batch: 150, loss is 3.6900672483444215 and perplexity is 40.04753999749744
At time: 143.2882833480835 and batch: 200, loss is 3.7182127094268798 and perplexity is 41.19070851441405
At time: 143.71338820457458 and batch: 250, loss is 3.701576061248779 and perplexity is 40.51110205041467
At time: 144.13834714889526 and batch: 300, loss is 3.618662061691284 and perplexity is 37.28764586340773
At time: 144.56248140335083 and batch: 350, loss is 3.690573410987854 and perplexity is 40.067815697173536
At time: 144.98740816116333 and batch: 400, loss is 3.5870321130752565 and perplexity is 36.12669670981017
At time: 145.41377973556519 and batch: 450, loss is 3.665921287536621 and perplexity is 39.092134674518746
At time: 145.83875060081482 and batch: 500, loss is 3.5770543909072874 and perplexity is 35.76802689667365
At time: 146.27619051933289 and batch: 550, loss is 3.642379069328308 and perplexity is 38.182567730692675
At time: 146.69972252845764 and batch: 600, loss is 3.645005073547363 and perplexity is 38.28296708151195
At time: 147.12533950805664 and batch: 650, loss is 3.6074400901794434 and perplexity is 36.871544071735144
At time: 147.54951643943787 and batch: 700, loss is 3.5728659057617187 and perplexity is 35.618526356289316
At time: 147.97544503211975 and batch: 750, loss is 3.548651347160339 and perplexity is 34.766398054239545
At time: 148.40090918540955 and batch: 800, loss is 3.5254909658432005 and perplexity is 33.97044785224611
At time: 148.8400273323059 and batch: 850, loss is 3.516409878730774 and perplexity is 33.66335573159032
At time: 149.27147817611694 and batch: 900, loss is 3.6879500102996827 and perplexity is 39.962839519394414
At time: 149.70010423660278 and batch: 950, loss is 3.6128815412521362 and perplexity is 37.072725638196395
At time: 150.13032603263855 and batch: 1000, loss is 3.536090087890625 and perplexity is 34.33241967812273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450821667182736 and perplexity of 85.69732976492084
Finished 16 epochs...
Completing Train Step...
At time: 151.4136860370636 and batch: 50, loss is 3.779570517539978 and perplexity is 43.797227554650505
At time: 151.85808968544006 and batch: 100, loss is 3.644342927932739 and perplexity is 38.2576265732232
At time: 152.28768968582153 and batch: 150, loss is 3.6872454261779786 and perplexity is 39.934692254433266
At time: 152.71709966659546 and batch: 200, loss is 3.715467848777771 and perplexity is 41.07780078838068
At time: 153.1455430984497 and batch: 250, loss is 3.6989063119888304 and perplexity is 40.40309180997973
At time: 153.5737977027893 and batch: 300, loss is 3.616060791015625 and perplexity is 37.19077664980425
At time: 154.00175046920776 and batch: 350, loss is 3.688081169128418 and perplexity is 39.96808134236687
At time: 154.4309356212616 and batch: 400, loss is 3.5846898555755615 and perplexity is 36.04217770477958
At time: 154.86021184921265 and batch: 450, loss is 3.6637949705123902 and perplexity is 39.009100712594794
At time: 155.29032135009766 and batch: 500, loss is 3.5750962114334106 and perplexity is 35.69805521150102
At time: 155.71875858306885 and batch: 550, loss is 3.640629153251648 and perplexity is 38.11580986893586
At time: 156.1472852230072 and batch: 600, loss is 3.643479027748108 and perplexity is 38.22459007473627
At time: 156.57502794265747 and batch: 650, loss is 3.606156816482544 and perplexity is 36.82425813595181
At time: 157.02819800376892 and batch: 700, loss is 3.5718414306640627 and perplexity is 35.58205474833904
At time: 157.45668649673462 and batch: 750, loss is 3.5479565143585203 and perplexity is 34.74224961100716
At time: 157.8882200717926 and batch: 800, loss is 3.5250667476654054 and perplexity is 33.956040027006104
At time: 158.31997346878052 and batch: 850, loss is 3.5163090133666994 and perplexity is 33.65996043619522
At time: 158.7499017715454 and batch: 900, loss is 3.688127107620239 and perplexity is 39.9699174579186
At time: 159.18163967132568 and batch: 950, loss is 3.613308744430542 and perplexity is 37.088566607836576
At time: 159.61529397964478 and batch: 1000, loss is 3.536673469543457 and perplexity is 34.35245442524401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450922151891197 and perplexity of 85.70594146878315
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 160.9143989086151 and batch: 50, loss is 3.778128995895386 and perplexity is 43.73413838628307
At time: 161.34412336349487 and batch: 100, loss is 3.643051266670227 and perplexity is 38.2082425795447
At time: 161.77402138710022 and batch: 150, loss is 3.6860179233551027 and perplexity is 39.885702380715756
At time: 162.20267415046692 and batch: 200, loss is 3.714741311073303 and perplexity is 41.04796705626926
At time: 162.63204741477966 and batch: 250, loss is 3.6987676858901977 and perplexity is 40.39749127518852
At time: 163.0601360797882 and batch: 300, loss is 3.6155230712890627 and perplexity is 37.17078381130703
At time: 163.48928356170654 and batch: 350, loss is 3.6876896142959597 and perplexity is 39.95243471043027
At time: 163.91802835464478 and batch: 400, loss is 3.5831282329559326 and perplexity is 35.98593734934998
At time: 164.35942840576172 and batch: 450, loss is 3.6624010467529295 and perplexity is 38.95476288047224
At time: 164.79518961906433 and batch: 500, loss is 3.572698531150818 and perplexity is 35.61256521818507
At time: 165.2217152118683 and batch: 550, loss is 3.6384942865371706 and perplexity is 38.03452449274589
At time: 165.64723682403564 and batch: 600, loss is 3.6405060386657713 and perplexity is 38.11111754564114
At time: 166.0735466480255 and batch: 650, loss is 3.6027834939956667 and perplexity is 36.700247319753046
At time: 166.5010542869568 and batch: 700, loss is 3.5684997177124025 and perplexity is 35.46334818723296
At time: 166.92864847183228 and batch: 750, loss is 3.544078345298767 and perplexity is 34.60777422119221
At time: 167.35552716255188 and batch: 800, loss is 3.5203446102142335 and perplexity is 33.79607292926586
At time: 167.78181552886963 and batch: 850, loss is 3.5111139535903932 and perplexity is 33.48554836249734
At time: 168.22109508514404 and batch: 900, loss is 3.682671837806702 and perplexity is 39.75246444547081
At time: 168.64789462089539 and batch: 950, loss is 3.6075670528411865 and perplexity is 36.876225678301395
At time: 169.089528799057 and batch: 1000, loss is 3.5307296228408815 and perplexity is 34.14887432609541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450937782845846 and perplexity of 85.70728114493757
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 170.40309357643127 and batch: 50, loss is 3.777664499282837 and perplexity is 43.713828744394995
At time: 170.8459026813507 and batch: 100, loss is 3.6421166706085204 and perplexity is 38.17254998818082
At time: 171.2733347415924 and batch: 150, loss is 3.685251030921936 and perplexity is 39.85512606324026
At time: 171.6995894908905 and batch: 200, loss is 3.713713812828064 and perplexity is 41.00581200297871
At time: 172.12613010406494 and batch: 250, loss is 3.6976908302307128 and perplexity is 40.354012422505704
At time: 172.56449937820435 and batch: 300, loss is 3.6148277282714845 and perplexity is 37.14494635031601
At time: 172.99669337272644 and batch: 350, loss is 3.686959295272827 and perplexity is 39.923267339380146
At time: 173.43953800201416 and batch: 400, loss is 3.58232524394989 and perplexity is 35.95705263589204
At time: 173.88648200035095 and batch: 450, loss is 3.661733479499817 and perplexity is 38.928766634507156
At time: 174.32707142829895 and batch: 500, loss is 3.572054162025452 and perplexity is 35.589624972470176
At time: 174.7624866962433 and batch: 550, loss is 3.6378358697891233 and perplexity is 38.00949016722984
At time: 175.18860864639282 and batch: 600, loss is 3.6397833013534546 and perplexity is 38.083583170235606
At time: 175.63470911979675 and batch: 650, loss is 3.602029447555542 and perplexity is 36.67258405991194
At time: 176.0714385509491 and batch: 700, loss is 3.5677118349075316 and perplexity is 35.43541822920474
At time: 176.50082111358643 and batch: 750, loss is 3.5431942462921144 and perplexity is 34.5771910436513
At time: 176.92853450775146 and batch: 800, loss is 3.5192166662216184 and perplexity is 33.75797434240667
At time: 177.35760951042175 and batch: 850, loss is 3.509970245361328 and perplexity is 33.44727255762918
At time: 177.78700828552246 and batch: 900, loss is 3.6813735485076906 and perplexity is 39.700887734262665
At time: 178.21487832069397 and batch: 950, loss is 3.6062212371826172 and perplexity is 36.82663045685306
At time: 178.64376854896545 and batch: 1000, loss is 3.5293311166763304 and perplexity is 34.10115029379287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450972766411014 and perplexity of 85.71027954363994
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 179.95026302337646 and batch: 50, loss is 3.7775594568252564 and perplexity is 43.70923717755238
At time: 180.39331102371216 and batch: 100, loss is 3.641918992996216 and perplexity is 38.16500487541809
At time: 180.82446837425232 and batch: 150, loss is 3.685086770057678 and perplexity is 39.84857996343673
At time: 181.25552082061768 and batch: 200, loss is 3.7135069084167482 and perplexity is 40.99732859724302
At time: 181.6870584487915 and batch: 250, loss is 3.6974542760849 and perplexity is 40.34446764254012
At time: 182.11573004722595 and batch: 300, loss is 3.6146730852127074 and perplexity is 37.139202586322355
At time: 182.54481172561646 and batch: 350, loss is 3.686787428855896 and perplexity is 39.91640646006461
At time: 182.9742293357849 and batch: 400, loss is 3.5821150445938112 and perplexity is 35.94949528088489
At time: 183.40317058563232 and batch: 450, loss is 3.661572527885437 and perplexity is 38.92250149087753
At time: 183.8316831588745 and batch: 500, loss is 3.5719047451019286 and perplexity is 35.58430767745426
At time: 184.26097083091736 and batch: 550, loss is 3.6376657676696778 and perplexity is 38.00302522225936
At time: 184.6903727054596 and batch: 600, loss is 3.6395894479751587 and perplexity is 38.07620125450821
At time: 185.11939024925232 and batch: 650, loss is 3.6018308496475218 and perplexity is 36.66530168459214
At time: 185.54818201065063 and batch: 700, loss is 3.5675244617462156 and perplexity is 35.4287792048755
At time: 185.97540497779846 and batch: 750, loss is 3.542979054450989 and perplexity is 34.56975111478437
At time: 186.40275239944458 and batch: 800, loss is 3.518966107368469 and perplexity is 33.749517042640704
At time: 186.83314728736877 and batch: 850, loss is 3.509707431793213 and perplexity is 33.4384833156012
At time: 187.26243090629578 and batch: 900, loss is 3.6810800504684447 and perplexity is 39.689237311328114
At time: 187.69265484809875 and batch: 950, loss is 3.6059146785736083 and perplexity is 36.815342666518774
At time: 188.12271547317505 and batch: 1000, loss is 3.5290138578414916 and perplexity is 34.09033311859693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.450979837557164 and perplexity of 85.71088561569599
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 189.41580271720886 and batch: 50, loss is 3.7775348138809206 and perplexity is 43.7081600665253
At time: 189.84717869758606 and batch: 100, loss is 3.641874632835388 and perplexity is 38.16331190721427
At time: 190.28798580169678 and batch: 150, loss is 3.685046901702881 and perplexity is 39.846991297781535
At time: 190.71506762504578 and batch: 200, loss is 3.713457713127136 and perplexity is 40.995311771398924
At time: 191.14339661598206 and batch: 250, loss is 3.6974010372161867 and perplexity is 40.342319805898704
At time: 191.57040786743164 and batch: 300, loss is 3.614636936187744 and perplexity is 37.137860064626516
At time: 191.99804496765137 and batch: 350, loss is 3.6867481517791747 and perplexity is 39.91483869109453
At time: 192.42476272583008 and batch: 400, loss is 3.582068696022034 and perplexity is 35.94782911173508
At time: 192.8536012172699 and batch: 450, loss is 3.661535577774048 and perplexity is 38.92106332668222
At time: 193.28168201446533 and batch: 500, loss is 3.5718707513809203 and perplexity is 35.583098054986706
At time: 193.71007561683655 and batch: 550, loss is 3.6376268005371095 and perplexity is 38.00154438218976
At time: 194.13859343528748 and batch: 600, loss is 3.639544711112976 and perplexity is 38.07449788284228
At time: 194.5667634010315 and batch: 650, loss is 3.6017846298217773 and perplexity is 36.663607059900336
At time: 194.99502110481262 and batch: 700, loss is 3.5674811887741087 and perplexity is 35.42724612947178
At time: 195.42408084869385 and batch: 750, loss is 3.5429293584823607 and perplexity is 34.56803318020505
At time: 195.85198140144348 and batch: 800, loss is 3.5189091873168947 and perplexity is 33.74759607306139
At time: 196.27937364578247 and batch: 850, loss is 3.5096473932266234 and perplexity is 33.436475777259474
At time: 196.7075424194336 and batch: 900, loss is 3.6810132884979248 and perplexity is 39.68658766808546
At time: 197.13628578186035 and batch: 950, loss is 3.605844883918762 and perplexity is 36.812773242051406
At time: 197.56445503234863 and batch: 1000, loss is 3.528941721916199 and perplexity is 34.08787406956784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4509828148818595 and perplexity of 85.71114080521228
Annealing...
Model not improving. Stopping early with 85.69732976492084loss at 20 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -85.69732976492084
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
SETTINGS FOR THIS RUN
{'anneal': 4.243624028933469, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.7357042626312168, 'batch_size': 50, 'num_layers': 1, 'lr': 7.1830387212582885, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6637890338897705 and batch: 50, loss is 6.875475769042969 and perplexity is 968.2359126925105
At time: 1.1084017753601074 and batch: 100, loss is 6.190589056015015 and perplexity is 488.1335595524546
At time: 1.5496153831481934 and batch: 150, loss is 6.071423416137695 and perplexity is 433.297004777233
At time: 1.9951732158660889 and batch: 200, loss is 5.997155046463012 and perplexity is 402.28268840065795
At time: 2.4312832355499268 and batch: 250, loss is 5.960607271194458 and perplexity is 387.84558018648517
At time: 2.8634331226348877 and batch: 300, loss is 5.830549468994141 and perplexity is 340.5457470020926
At time: 3.295667886734009 and batch: 350, loss is 5.860558681488037 and perplexity is 350.9201418516671
At time: 3.7287545204162598 and batch: 400, loss is 5.7854479312896725 and perplexity is 325.5278215570936
At time: 4.189276456832886 and batch: 450, loss is 5.749536161422729 and perplexity is 314.04496033007786
At time: 4.653399705886841 and batch: 500, loss is 5.747209634780884 and perplexity is 313.3151756240902
At time: 5.095060348510742 and batch: 550, loss is 5.799044723510742 and perplexity is 329.9841831656324
At time: 5.549543619155884 and batch: 600, loss is 5.822591953277588 and perplexity is 337.84660235403123
At time: 5.980407238006592 and batch: 650, loss is 5.772268409729004 and perplexity is 321.26566887527326
At time: 6.411334037780762 and batch: 700, loss is 5.765749464035034 and perplexity is 319.17816697494897
At time: 6.842016935348511 and batch: 750, loss is 5.6643862056732175 and perplexity is 288.4109018017271
At time: 7.273879051208496 and batch: 800, loss is 5.726415328979492 and perplexity is 306.8672761856261
At time: 7.705024480819702 and batch: 850, loss is 5.6772364902496335 and perplexity is 292.1409789122308
At time: 8.134725332260132 and batch: 900, loss is 5.766209087371826 and perplexity is 319.3249024280213
At time: 8.5635244846344 and batch: 950, loss is 5.748332471847534 and perplexity is 313.66717509887155
At time: 8.992117166519165 and batch: 1000, loss is 5.687311544418335 and perplexity is 295.0991921558378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4530751298113564 and perplexity of 233.47502914131817
Finished 1 epochs...
Completing Train Step...
At time: 10.258127689361572 and batch: 50, loss is 5.349516134262085 and perplexity is 210.50641637768493
At time: 10.68208122253418 and batch: 100, loss is 5.199697904586792 and perplexity is 181.21748863310677
At time: 11.107511281967163 and batch: 150, loss is 5.126535453796387 and perplexity is 168.43256362282537
At time: 11.533297538757324 and batch: 200, loss is 5.06787371635437 and perplexity is 158.83623713947625
At time: 11.958599328994751 and batch: 250, loss is 5.024086065292359 and perplexity is 152.03124596691197
At time: 12.383578538894653 and batch: 300, loss is 4.868359622955322 and perplexity is 130.10731669626998
At time: 12.810106039047241 and batch: 350, loss is 4.910167760848999 and perplexity is 135.66217130058854
At time: 13.23762321472168 and batch: 400, loss is 4.812948932647705 and perplexity is 123.09407896559695
At time: 13.664159536361694 and batch: 450, loss is 4.83002236366272 and perplexity is 125.21376086176946
At time: 14.089356184005737 and batch: 500, loss is 4.788779249191284 and perplexity is 120.15460027217287
At time: 14.514896392822266 and batch: 550, loss is 4.828112297058105 and perplexity is 124.97482250544097
At time: 14.941020965576172 and batch: 600, loss is 4.825216026306152 and perplexity is 124.61338524502712
At time: 15.36567997932434 and batch: 650, loss is 4.793964147567749 and perplexity is 120.77920752733574
At time: 15.803029537200928 and batch: 700, loss is 4.7585459899902345 and perplexity is 116.57629950325666
At time: 16.22761631011963 and batch: 750, loss is 4.693133144378662 and perplexity is 109.19476737798189
At time: 16.652767658233643 and batch: 800, loss is 4.717630977630615 and perplexity is 111.90283807878511
At time: 17.077435970306396 and batch: 850, loss is 4.677735776901245 and perplexity is 107.5263331281003
At time: 17.50339961051941 and batch: 900, loss is 4.828472461700439 and perplexity is 125.01984212444025
At time: 17.929375410079956 and batch: 950, loss is 4.731250238418579 and perplexity is 113.43729739620143
At time: 18.356794595718384 and batch: 1000, loss is 4.6875583267211915 and perplexity is 108.58772012176149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8107121165205795 and perplexity of 122.81904785629635
Finished 2 epochs...
Completing Train Step...
At time: 19.61086678504944 and batch: 50, loss is 4.693111562728882 and perplexity is 109.19241080018402
At time: 20.051113843917847 and batch: 100, loss is 4.574403162002564 and perplexity is 96.97014639437246
At time: 20.47937560081482 and batch: 150, loss is 4.5967448043823245 and perplexity is 99.16100124492748
At time: 20.90680193901062 and batch: 200, loss is 4.602280302047729 and perplexity is 99.71142877546933
At time: 21.334983348846436 and batch: 250, loss is 4.580712213516235 and perplexity is 97.5838700145035
At time: 21.762673139572144 and batch: 300, loss is 4.465603094100953 and perplexity is 86.97346691066167
At time: 22.1897394657135 and batch: 350, loss is 4.531174077987671 and perplexity is 92.86753082381624
At time: 22.6156325340271 and batch: 400, loss is 4.435213785171509 and perplexity is 84.37016004953338
At time: 23.044434309005737 and batch: 450, loss is 4.4926551246643065 and perplexity is 89.35838884508904
At time: 23.472464561462402 and batch: 500, loss is 4.43919358253479 and perplexity is 84.70660523780111
At time: 23.899524927139282 and batch: 550, loss is 4.495463209152222 and perplexity is 89.60966739124584
At time: 24.326661109924316 and batch: 600, loss is 4.500400457382202 and perplexity is 90.05318654408818
At time: 24.754424333572388 and batch: 650, loss is 4.476223754882812 and perplexity is 87.9021052470592
At time: 25.18258833885193 and batch: 700, loss is 4.4431592464447025 and perplexity is 85.04319011526637
At time: 25.610670804977417 and batch: 750, loss is 4.396109266281128 and perplexity is 81.13458071890233
At time: 26.038777828216553 and batch: 800, loss is 4.419544506072998 and perplexity is 83.05844412388711
At time: 26.480036735534668 and batch: 850, loss is 4.397603330612182 and perplexity is 81.25589160253546
At time: 26.908366441726685 and batch: 900, loss is 4.563724422454834 and perplexity is 95.94013684708273
At time: 27.337055444717407 and batch: 950, loss is 4.477301425933838 and perplexity is 87.9968858631983
At time: 27.765774488449097 and batch: 1000, loss is 4.423084669113159 and perplexity is 83.35300564821965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.694246617759147 and perplexity of 109.31642056096686
Finished 3 epochs...
Completing Train Step...
At time: 29.017090559005737 and batch: 50, loss is 4.4631149864196775 and perplexity is 86.75733654888026
At time: 29.454936981201172 and batch: 100, loss is 4.335255327224732 and perplexity is 76.3444492177986
At time: 29.88070297241211 and batch: 150, loss is 4.368261938095093 and perplexity is 78.90636829518341
At time: 30.30471110343933 and batch: 200, loss is 4.386520586013794 and perplexity is 80.36032514260674
At time: 30.728948831558228 and batch: 250, loss is 4.373163318634033 and perplexity is 79.29406778836035
At time: 31.15375590324402 and batch: 300, loss is 4.268849515914917 and perplexity is 71.43939843105917
At time: 31.57768726348877 and batch: 350, loss is 4.339335021972656 and perplexity is 76.65654746622027
At time: 32.002450704574585 and batch: 400, loss is 4.249005279541016 and perplexity is 70.0357117308026
At time: 32.42785859107971 and batch: 450, loss is 4.316721639633179 and perplexity is 74.94253648222173
At time: 32.85218954086304 and batch: 500, loss is 4.252463755607605 and perplexity is 70.27834789746906
At time: 33.277135372161865 and batch: 550, loss is 4.316715636253357 and perplexity is 74.94208657506091
At time: 33.703131675720215 and batch: 600, loss is 4.31905460357666 and perplexity is 75.11757882200925
At time: 34.128411293029785 and batch: 650, loss is 4.294797477722168 and perplexity is 73.31736454168502
At time: 34.55359172821045 and batch: 700, loss is 4.2618579053878785 and perplexity is 70.9416639911633
At time: 34.97950315475464 and batch: 750, loss is 4.226407036781311 and perplexity is 68.47077670400715
At time: 35.405640602111816 and batch: 800, loss is 4.23767427444458 and perplexity is 69.24661579206125
At time: 35.8320107460022 and batch: 850, loss is 4.219121050834656 and perplexity is 67.97371258717999
At time: 36.25558805465698 and batch: 900, loss is 4.386902761459351 and perplexity is 80.39104275505794
At time: 36.67960786819458 and batch: 950, loss is 4.313770332336426 and perplexity is 74.72168408940975
At time: 37.11690545082092 and batch: 1000, loss is 4.2576712799072265 and perplexity is 70.64527867313622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6425282548113564 and perplexity of 103.80646528879937
Finished 4 epochs...
Completing Train Step...
At time: 38.406144857406616 and batch: 50, loss is 4.309945244789123 and perplexity is 74.43641304726495
At time: 38.83490967750549 and batch: 100, loss is 4.179541049003601 and perplexity is 65.3358603736953
At time: 39.2630078792572 and batch: 150, loss is 4.216762480735778 and perplexity is 67.81358073649211
At time: 39.69300317764282 and batch: 200, loss is 4.237850046157837 and perplexity is 69.25878845813008
At time: 40.121779680252075 and batch: 250, loss is 4.222518939971923 and perplexity is 68.20507257198922
At time: 40.55006194114685 and batch: 300, loss is 4.126789164543152 and perplexity is 61.97860002294769
At time: 40.978761434555054 and batch: 350, loss is 4.201987996101379 and perplexity is 66.8190350708916
At time: 41.40835976600647 and batch: 400, loss is 4.113972268104553 and perplexity is 61.18929574570883
At time: 41.83764028549194 and batch: 450, loss is 4.178539085388183 and perplexity is 65.27042900421868
At time: 42.26665234565735 and batch: 500, loss is 4.115818467140198 and perplexity is 61.30236770906449
At time: 42.69282579421997 and batch: 550, loss is 4.177280879020691 and perplexity is 65.18835697743717
At time: 43.120264768600464 and batch: 600, loss is 4.185320219993591 and perplexity is 65.71454065767331
At time: 43.548547983169556 and batch: 650, loss is 4.1531719207763675 and perplexity is 63.63552736657977
At time: 43.971930503845215 and batch: 700, loss is 4.127275962829589 and perplexity is 62.00877844404053
At time: 44.39584970474243 and batch: 750, loss is 4.093411998748779 and perplexity is 59.944072273731294
At time: 44.820351123809814 and batch: 800, loss is 4.100332536697388 and perplexity is 60.360356293923125
At time: 45.2445330619812 and batch: 850, loss is 4.08758544921875 and perplexity is 59.5958207053257
At time: 45.66902732849121 and batch: 900, loss is 4.256647253036499 and perplexity is 70.57297303726043
At time: 46.094730615615845 and batch: 950, loss is 4.1936729669570925 and perplexity is 66.26573638215076
At time: 46.51907563209534 and batch: 1000, loss is 4.130842814445495 and perplexity is 62.23034947622098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.627781937762005 and perplexity of 102.28693352403475
Finished 5 epochs...
Completing Train Step...
At time: 47.7715539932251 and batch: 50, loss is 4.186329364776611 and perplexity is 65.78088961576834
At time: 48.21079730987549 and batch: 100, loss is 4.0485223865509035 and perplexity is 57.312708419246434
At time: 48.63756847381592 and batch: 150, loss is 4.1002402067184445 and perplexity is 60.35478348077032
At time: 49.06574821472168 and batch: 200, loss is 4.119919109344482 and perplexity is 61.554262898363376
At time: 49.49262809753418 and batch: 250, loss is 4.105807824134827 and perplexity is 60.69175301269898
At time: 49.919126987457275 and batch: 300, loss is 4.016844816207886 and perplexity is 55.525635566518574
At time: 50.346967458724976 and batch: 350, loss is 4.0943847751617435 and perplexity is 60.002412824891636
At time: 50.77493381500244 and batch: 400, loss is 4.007093191146851 and perplexity is 54.98680190908611
At time: 51.204938650131226 and batch: 450, loss is 4.069020090103149 and perplexity is 58.499610149421294
At time: 51.631978273391724 and batch: 500, loss is 4.003538203239441 and perplexity is 54.79167154178401
At time: 52.059847831726074 and batch: 550, loss is 4.066808261871338 and perplexity is 58.37036205013244
At time: 52.4871301651001 and batch: 600, loss is 4.07473138332367 and perplexity is 58.83467449083704
At time: 52.915374517440796 and batch: 650, loss is 4.042966198921204 and perplexity is 56.99515127817993
At time: 53.34306836128235 and batch: 700, loss is 4.026840348243713 and perplexity is 56.08342690211482
At time: 53.769808530807495 and batch: 750, loss is 3.985579824447632 and perplexity is 53.81648454660976
At time: 54.197592973709106 and batch: 800, loss is 3.99072135925293 and perplexity is 54.09389642529599
At time: 54.626197814941406 and batch: 850, loss is 3.983207583427429 and perplexity is 53.68897018160805
At time: 55.052485942840576 and batch: 900, loss is 4.154331059455871 and perplexity is 63.709332534689814
At time: 55.48039484024048 and batch: 950, loss is 4.088394956588745 and perplexity is 59.6440834933133
At time: 55.90879225730896 and batch: 1000, loss is 4.026939754486084 and perplexity is 56.089002221948654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6173791652772485 and perplexity of 101.22838131149044
Finished 6 epochs...
Completing Train Step...
At time: 57.16017484664917 and batch: 50, loss is 4.086509504318237 and perplexity is 59.5317333693497
At time: 57.59862470626831 and batch: 100, loss is 3.95530743598938 and perplexity is 52.21174324618093
At time: 58.02458596229553 and batch: 150, loss is 4.0081169271469115 and perplexity is 55.04312270162124
At time: 58.450963258743286 and batch: 200, loss is 4.023089556694031 and perplexity is 55.87346366872842
At time: 58.877514600753784 and batch: 250, loss is 4.010170764923096 and perplexity is 55.15628851867988
At time: 59.329755783081055 and batch: 300, loss is 3.928583197593689 and perplexity is 50.8349036101482
At time: 59.758819818496704 and batch: 350, loss is 4.006660990715027 and perplexity is 54.96304172450726
At time: 60.18758273124695 and batch: 400, loss is 3.914041199684143 and perplexity is 50.101011609038494
At time: 60.61762571334839 and batch: 450, loss is 3.976875038146973 and perplexity is 53.35005657232787
At time: 61.0469434261322 and batch: 500, loss is 3.9122985124588014 and perplexity is 50.013777249310145
At time: 61.47467017173767 and batch: 550, loss is 3.9786554384231567 and perplexity is 53.445125633162064
At time: 61.9026734828949 and batch: 600, loss is 3.9857294273376467 and perplexity is 53.82453625049243
At time: 62.331056118011475 and batch: 650, loss is 3.9522445440292358 and perplexity is 52.05206897491736
At time: 62.76042175292969 and batch: 700, loss is 3.934796919822693 and perplexity is 51.15176099310475
At time: 63.18952918052673 and batch: 750, loss is 3.9002925729751587 and perplexity is 49.41690504165011
At time: 63.61906456947327 and batch: 800, loss is 3.9019754838943483 and perplexity is 49.500139309021364
At time: 64.04896926879883 and batch: 850, loss is 3.8964257860183715 and perplexity is 49.22618936387025
At time: 64.48122358322144 and batch: 900, loss is 4.062614493370056 and perplexity is 58.12608284759628
At time: 64.9071774482727 and batch: 950, loss is 4.002923951148987 and perplexity is 54.758025977486994
At time: 65.33346939086914 and batch: 1000, loss is 3.9389625501632692 and perplexity is 51.36528474241208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.618727149032965 and perplexity of 101.3649275354857
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 66.59744811058044 and batch: 50, loss is 4.001240115165711 and perplexity is 54.665900027235026
At time: 67.02260160446167 and batch: 100, loss is 3.860942873954773 and perplexity is 47.51012631766278
At time: 67.44796752929688 and batch: 150, loss is 3.8995138120651247 and perplexity is 49.378436068719175
At time: 67.8726418018341 and batch: 200, loss is 3.908912196159363 and perplexity is 49.84470121424916
At time: 68.2979941368103 and batch: 250, loss is 3.8826596403121947 and perplexity is 48.553177486634574
At time: 68.72503662109375 and batch: 300, loss is 3.791877770423889 and perplexity is 44.33958170886502
At time: 69.15195560455322 and batch: 350, loss is 3.86344051361084 and perplexity is 47.628937805596266
At time: 69.57767701148987 and batch: 400, loss is 3.7522589778900146 and perplexity is 42.61724475803942
At time: 70.0156466960907 and batch: 450, loss is 3.809126515388489 and perplexity is 45.11101787208836
At time: 70.44200277328491 and batch: 500, loss is 3.723984122276306 and perplexity is 41.42912443537927
At time: 70.86747741699219 and batch: 550, loss is 3.7753161668777464 and perplexity is 43.61129458307091
At time: 71.29255533218384 and batch: 600, loss is 3.7751106882095335 and perplexity is 43.602334312944656
At time: 71.71943783760071 and batch: 650, loss is 3.7279397439956665 and perplexity is 41.59332692717587
At time: 72.14565920829773 and batch: 700, loss is 3.693582019805908 and perplexity is 40.18854560426392
At time: 72.5729615688324 and batch: 750, loss is 3.6458502435684204 and perplexity is 38.31533637445755
At time: 73.00035977363586 and batch: 800, loss is 3.6302960872650147 and perplexity is 37.72398455517045
At time: 73.4273693561554 and batch: 850, loss is 3.6103059434890747 and perplexity is 36.97736406821193
At time: 73.85556530952454 and batch: 900, loss is 3.7583254957199097 and perplexity is 42.87656883497626
At time: 74.28364205360413 and batch: 950, loss is 3.6794843339920043 and perplexity is 39.62595504511853
At time: 74.71210145950317 and batch: 1000, loss is 3.5933862924575806 and perplexity is 36.356983086840984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.51942592713891 and perplexity of 91.78289278343394
Finished 8 epochs...
Completing Train Step...
At time: 75.97385263442993 and batch: 50, loss is 3.866275682449341 and perplexity is 47.764165491952774
At time: 76.41724371910095 and batch: 100, loss is 3.7305841493606566 and perplexity is 41.70346210089185
At time: 76.84461855888367 and batch: 150, loss is 3.7813284397125244 and perplexity is 43.874287384812135
At time: 77.27251672744751 and batch: 200, loss is 3.7979998207092285 and perplexity is 44.61186346861528
At time: 77.700021982193 and batch: 250, loss is 3.778045039176941 and perplexity is 43.73046676567091
At time: 78.12855887413025 and batch: 300, loss is 3.6968020153045655 and perplexity is 40.31816110888617
At time: 78.55510306358337 and batch: 350, loss is 3.7711787700653074 and perplexity is 43.43123010783221
At time: 78.98365473747253 and batch: 400, loss is 3.665653986930847 and perplexity is 39.08168671967378
At time: 79.41103792190552 and batch: 450, loss is 3.72538996219635 and perplexity is 41.487408111523045
At time: 79.83779621124268 and batch: 500, loss is 3.6484548807144166 and perplexity is 38.415264003954114
At time: 80.26554822921753 and batch: 550, loss is 3.7018091583251955 and perplexity is 40.52054617052066
At time: 80.69424319267273 and batch: 600, loss is 3.7077659368515015 and perplexity is 40.76263841892632
At time: 81.1585693359375 and batch: 650, loss is 3.6657215547561646 and perplexity is 39.08432747346916
At time: 81.61617112159729 and batch: 700, loss is 3.6385318279266357 and perplexity is 38.035952388445416
At time: 82.05801653862 and batch: 750, loss is 3.5969569730758666 and perplexity is 36.48703430920126
At time: 82.4898407459259 and batch: 800, loss is 3.5871207189559935 and perplexity is 36.12989788940984
At time: 82.91730332374573 and batch: 850, loss is 3.575401153564453 and perplexity is 35.70894271247579
At time: 83.34413933753967 and batch: 900, loss is 3.7313781785964966 and perplexity is 41.736589019166004
At time: 83.77190017700195 and batch: 950, loss is 3.661562056541443 and perplexity is 38.922093922109205
At time: 84.20033478736877 and batch: 1000, loss is 3.582691149711609 and perplexity is 35.97021193601099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.518453086294779 and perplexity of 91.69364605501183
Finished 9 epochs...
Completing Train Step...
At time: 85.46609997749329 and batch: 50, loss is 3.8114644718170165 and perplexity is 45.21660885177768
At time: 85.90843844413757 and batch: 100, loss is 3.6759113025665284 and perplexity is 39.48462290492732
At time: 86.33752536773682 and batch: 150, loss is 3.7280787658691406 and perplexity is 41.59910971136674
At time: 86.76672291755676 and batch: 200, loss is 3.7461857175827027 and perplexity is 42.35920350603301
At time: 87.19632506370544 and batch: 250, loss is 3.727561078071594 and perplexity is 41.577579933212924
At time: 87.62591290473938 and batch: 300, loss is 3.648178791999817 and perplexity is 38.40465944706086
At time: 88.0551130771637 and batch: 350, loss is 3.723081974983215 and perplexity is 41.39176611680149
At time: 88.48562455177307 and batch: 400, loss is 3.6196732044219972 and perplexity is 37.32536806351817
At time: 88.91574144363403 and batch: 450, loss is 3.680849151611328 and perplexity is 39.68007416971327
At time: 89.34591555595398 and batch: 500, loss is 3.6060145902633667 and perplexity is 36.81902113337148
At time: 89.77598667144775 and batch: 550, loss is 3.6607840967178347 and perplexity is 38.89182587197714
At time: 90.20636129379272 and batch: 600, loss is 3.669314675331116 and perplexity is 39.22501477653175
At time: 90.6443543434143 and batch: 650, loss is 3.627098321914673 and perplexity is 37.60354477619087
At time: 91.08779287338257 and batch: 700, loss is 3.604320788383484 and perplexity is 36.756709792590826
At time: 91.5177047252655 and batch: 750, loss is 3.564310531616211 and perplexity is 35.31509636612579
At time: 91.95984172821045 and batch: 800, loss is 3.556207528114319 and perplexity is 35.0300942625263
At time: 92.3885588645935 and batch: 850, loss is 3.5497367572784424 and perplexity is 34.80415434127884
At time: 92.81855082511902 and batch: 900, loss is 3.7079519557952882 and perplexity is 40.77022174717046
At time: 93.2469425201416 and batch: 950, loss is 3.6411809635162355 and perplexity is 38.136848368160685
At time: 93.67619895935059 and batch: 1000, loss is 3.5633814477920533 and perplexity is 35.282300918562456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.521808531226181 and perplexity of 92.00183580261287
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 94.95544290542603 and batch: 50, loss is 3.7825797319412233 and perplexity is 43.92922130168044
At time: 95.37990498542786 and batch: 100, loss is 3.6532057332992554 and perplexity is 38.59820347538725
At time: 95.80502462387085 and batch: 150, loss is 3.7021581411361693 and perplexity is 40.53468961239095
At time: 96.229816198349 and batch: 200, loss is 3.718151135444641 and perplexity is 41.188172316542286
At time: 96.65526413917542 and batch: 250, loss is 3.696310863494873 and perplexity is 40.29836363327503
At time: 97.07998037338257 and batch: 300, loss is 3.613737564086914 and perplexity is 37.104474324751756
At time: 97.50526285171509 and batch: 350, loss is 3.687400908470154 and perplexity is 39.94090187465279
At time: 97.93056321144104 and batch: 400, loss is 3.581571021080017 and perplexity is 35.92994322901326
At time: 98.35628986358643 and batch: 450, loss is 3.6368693685531617 and perplexity is 37.972771695087594
At time: 98.78067302703857 and batch: 500, loss is 3.5540767908096313 and perplexity is 34.9555337964866
At time: 99.20507073402405 and batch: 550, loss is 3.6035180711746215 and perplexity is 36.7272163881145
At time: 99.62901949882507 and batch: 600, loss is 3.607081241607666 and perplexity is 36.85831514453854
At time: 100.05278420448303 and batch: 650, loss is 3.559177489280701 and perplexity is 35.134286929640865
At time: 100.4763834476471 and batch: 700, loss is 3.533558015823364 and perplexity is 34.24559748366854
At time: 100.9010226726532 and batch: 750, loss is 3.489399781227112 and perplexity is 32.766274869525255
At time: 101.32501673698425 and batch: 800, loss is 3.471789584159851 and perplexity is 32.19430534144291
At time: 101.74930000305176 and batch: 850, loss is 3.458540530204773 and perplexity is 31.770574467898506
At time: 102.17346668243408 and batch: 900, loss is 3.611111488342285 and perplexity is 37.00716299409667
At time: 102.59842157363892 and batch: 950, loss is 3.537017707824707 and perplexity is 34.36428189072847
At time: 103.04478645324707 and batch: 1000, loss is 3.4535733556747434 and perplexity is 31.613155766343116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5017149390243905 and perplexity of 90.17163763845986
Finished 11 epochs...
Completing Train Step...
At time: 104.29291868209839 and batch: 50, loss is 3.753215250968933 and perplexity is 42.65801797395646
At time: 104.73208975791931 and batch: 100, loss is 3.6182261419296267 and perplexity is 37.271394984007266
At time: 105.157297372818 and batch: 150, loss is 3.6672304582595827 and perplexity is 39.14334646790854
At time: 105.5824453830719 and batch: 200, loss is 3.6855363702774047 and perplexity is 39.86649992185083
At time: 106.00884461402893 and batch: 250, loss is 3.664906244277954 and perplexity is 39.05247459850206
At time: 106.4342749118805 and batch: 300, loss is 3.5843344354629516 and perplexity is 36.02936986613759
At time: 106.85823965072632 and batch: 350, loss is 3.6594542360305784 and perplexity is 38.84013953712417
At time: 107.28235054016113 and batch: 400, loss is 3.5552175045013428 and perplexity is 34.99543080369426
At time: 107.70755863189697 and batch: 450, loss is 3.613239269256592 and perplexity is 37.085989962727396
At time: 108.13298726081848 and batch: 500, loss is 3.532346053123474 and perplexity is 34.204118237609656
At time: 108.55766654014587 and batch: 550, loss is 3.5843629169464113 and perplexity is 36.03039605065306
At time: 108.98234510421753 and batch: 600, loss is 3.5897499561309814 and perplexity is 36.225016950559386
At time: 109.40806484222412 and batch: 650, loss is 3.5439469480514525 and perplexity is 34.603227153666474
At time: 109.83335852622986 and batch: 700, loss is 3.5208255672454833 and perplexity is 33.81233129764471
At time: 110.25905156135559 and batch: 750, loss is 3.479583616256714 and perplexity is 32.44620918874832
At time: 110.68330693244934 and batch: 800, loss is 3.4648127508163453 and perplexity is 31.970472767690286
At time: 111.10879707336426 and batch: 850, loss is 3.4551801776885984 and perplexity is 31.6639933134628
At time: 111.53358054161072 and batch: 900, loss is 3.6108425569534304 and perplexity is 36.99721194448986
At time: 111.9584231376648 and batch: 950, loss is 3.5399300956726076 and perplexity is 34.46450988826538
At time: 112.3833396434784 and batch: 1000, loss is 3.4586129140853883 and perplexity is 31.77287422859966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.502104596393864 and perplexity of 90.20678052798148
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 113.63542318344116 and batch: 50, loss is 3.7424237298965455 and perplexity is 42.20014807392303
At time: 114.07356643676758 and batch: 100, loss is 3.6092828035354616 and perplexity is 36.93955039729186
At time: 114.49963092803955 and batch: 150, loss is 3.6580568170547485 and perplexity is 38.7859014945723
At time: 114.92561507225037 and batch: 200, loss is 3.676771893501282 and perplexity is 39.51861763914342
At time: 115.35166001319885 and batch: 250, loss is 3.655736708641052 and perplexity is 38.69601830787017
At time: 115.77877688407898 and batch: 300, loss is 3.575749382972717 and perplexity is 35.72137978181726
At time: 116.20557832717896 and batch: 350, loss is 3.6500893211364747 and perplexity is 38.478102803414224
At time: 116.63292837142944 and batch: 400, loss is 3.5443806934356687 and perplexity is 34.61823939923411
At time: 117.0613865852356 and batch: 450, loss is 3.6020171213150025 and perplexity is 36.67213202760555
At time: 117.48873782157898 and batch: 500, loss is 3.518947081565857 and perplexity is 33.7488749370995
At time: 117.91643452644348 and batch: 550, loss is 3.5695134258270262 and perplexity is 35.499315898362354
At time: 118.344642162323 and batch: 600, loss is 3.572270565032959 and perplexity is 35.597327507746115
At time: 118.77178883552551 and batch: 650, loss is 3.5263224601745606 and perplexity is 33.99870583361659
At time: 119.19979214668274 and batch: 700, loss is 3.5017849969863892 and perplexity is 33.174615728507874
At time: 119.62711715698242 and batch: 750, loss is 3.4588395500183107 and perplexity is 31.780075919642243
At time: 120.05508804321289 and batch: 800, loss is 3.4406398916244507 and perplexity is 31.206920827921643
At time: 120.48181200027466 and batch: 850, loss is 3.42801905632019 and perplexity is 30.815538401486325
At time: 120.90788102149963 and batch: 900, loss is 3.581129469871521 and perplexity is 35.91408182122888
At time: 121.33486652374268 and batch: 950, loss is 3.5080790233612063 and perplexity is 33.38407611800891
At time: 121.76200318336487 and batch: 1000, loss is 3.426230673789978 and perplexity is 30.760477680440943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4985850264386436 and perplexity of 89.8898495110823
Finished 13 epochs...
Completing Train Step...
At time: 123.02100419998169 and batch: 50, loss is 3.7342498445510866 and perplexity is 41.856614815427086
At time: 123.44656014442444 and batch: 100, loss is 3.599349241256714 and perplexity is 36.574425570381386
At time: 123.87285780906677 and batch: 150, loss is 3.6488578796386717 and perplexity is 38.43074843391743
At time: 124.2977945804596 and batch: 200, loss is 3.6670414781570435 and perplexity is 39.135949853207826
At time: 124.7522943019867 and batch: 250, loss is 3.646638822555542 and perplexity is 38.3455629600657
At time: 125.1775860786438 and batch: 300, loss is 3.5669150018692015 and perplexity is 35.40719336397866
At time: 125.60331559181213 and batch: 350, loss is 3.6412421369552614 and perplexity is 38.139181401688084
At time: 126.02845287322998 and batch: 400, loss is 3.5363919925689697 and perplexity is 34.34278636103875
At time: 126.4548408985138 and batch: 450, loss is 3.5948151636123655 and perplexity is 36.4089696634637
At time: 126.8806824684143 and batch: 500, loss is 3.5120472288131714 and perplexity is 33.516814182643444
At time: 127.30616188049316 and batch: 550, loss is 3.5635888671875 and perplexity is 35.289619911113206
At time: 127.74197602272034 and batch: 600, loss is 3.5674650382995607 and perplexity is 35.42667396725522
At time: 128.1780378818512 and batch: 650, loss is 3.5219559621810914 and perplexity is 33.850574196422016
At time: 128.60372066497803 and batch: 700, loss is 3.498479700088501 and perplexity is 33.06514479069143
At time: 129.0295650959015 and batch: 750, loss is 3.45705801486969 and perplexity is 31.723509000304414
At time: 129.45479273796082 and batch: 800, loss is 3.440284295082092 and perplexity is 31.195825727584147
At time: 129.88045716285706 and batch: 850, loss is 3.428572449684143 and perplexity is 30.83259623535506
At time: 130.3059492111206 and batch: 900, loss is 3.583066873550415 and perplexity is 35.98372934136896
At time: 130.73092436790466 and batch: 950, loss is 3.511028552055359 and perplexity is 33.482688767374356
At time: 131.1565647125244 and batch: 1000, loss is 3.4298495626449585 and perplexity is 30.871998098582495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.498333442501906 and perplexity of 89.86723751339652
Finished 14 epochs...
Completing Train Step...
At time: 132.41668677330017 and batch: 50, loss is 3.729308829307556 and perplexity is 41.6503107390936
At time: 132.85311627388 and batch: 100, loss is 3.5936769390106202 and perplexity is 36.36755165443857
At time: 133.29270267486572 and batch: 150, loss is 3.6430132818222045 and perplexity is 38.206791272821114
At time: 133.72584581375122 and batch: 200, loss is 3.661113772392273 and perplexity is 38.90464967463337
At time: 134.1512815952301 and batch: 250, loss is 3.640988140106201 and perplexity is 38.12949539994535
At time: 134.5748255252838 and batch: 300, loss is 3.5614272928237916 and perplexity is 35.21342115771356
At time: 134.99877786636353 and batch: 350, loss is 3.63608247756958 and perplexity is 37.94290301665722
At time: 135.42290592193604 and batch: 400, loss is 3.531588840484619 and perplexity is 34.178228250329084
At time: 135.86051845550537 and batch: 450, loss is 3.590265140533447 and perplexity is 36.2436843224279
At time: 136.28439617156982 and batch: 500, loss is 3.5078024578094484 and perplexity is 33.37484450920875
At time: 136.70873379707336 and batch: 550, loss is 3.5598999071121216 and perplexity is 35.1596777352942
At time: 137.13315057754517 and batch: 600, loss is 3.564387307167053 and perplexity is 35.31780780618717
At time: 137.55783486366272 and batch: 650, loss is 3.5193871593475343 and perplexity is 33.76373033564159
At time: 137.98276448249817 and batch: 700, loss is 3.496436562538147 and perplexity is 32.99765711853702
At time: 138.40703511238098 and batch: 750, loss is 3.4556883716583253 and perplexity is 31.680088853403937
At time: 138.8318374156952 and batch: 800, loss is 3.4396172046661375 and perplexity is 31.175022230901593
At time: 139.2752811908722 and batch: 850, loss is 3.428490886688232 and perplexity is 30.830081538988868
At time: 139.7235507965088 and batch: 900, loss is 3.5837574768066407 and perplexity is 36.00858840491035
At time: 140.15925121307373 and batch: 950, loss is 3.512353653907776 and perplexity is 33.52708614931401
At time: 140.5853750705719 and batch: 1000, loss is 3.4315429973602294 and perplexity is 30.924322103030974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.49844062619093 and perplexity of 89.87687033166686
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 141.83425974845886 and batch: 50, loss is 3.7264314556121825 and perplexity is 41.53063948259753
At time: 142.27535152435303 and batch: 100, loss is 3.591626973152161 and perplexity is 36.29307577777464
At time: 142.711571931839 and batch: 150, loss is 3.6410386848449705 and perplexity is 38.13142269403663
At time: 143.13854479789734 and batch: 200, loss is 3.6591159439086915 and perplexity is 38.82700244611826
At time: 143.56257557868958 and batch: 250, loss is 3.639339737892151 and perplexity is 38.06669443015668
At time: 143.98757767677307 and batch: 300, loss is 3.560005259513855 and perplexity is 35.16338208691554
At time: 144.41377305984497 and batch: 350, loss is 3.635714807510376 and perplexity is 37.92895511152939
At time: 144.83919429779053 and batch: 400, loss is 3.5295960092544556 and perplexity is 34.11018463192296
At time: 145.2645766735077 and batch: 450, loss is 3.5873013305664063 and perplexity is 36.13642395777581
At time: 145.69122982025146 and batch: 500, loss is 3.5037147569656373 and perplexity is 33.23869658471727
At time: 146.11710166931152 and batch: 550, loss is 3.556349697113037 and perplexity is 35.03507480998393
At time: 146.56528663635254 and batch: 600, loss is 3.5603354740142823 and perplexity is 35.174995462911745
At time: 146.99254751205444 and batch: 650, loss is 3.5149525928497316 and perplexity is 33.61433432635052
At time: 147.41899156570435 and batch: 700, loss is 3.49114520072937 and perplexity is 32.82351570480201
At time: 147.84607338905334 and batch: 750, loss is 3.448431959152222 and perplexity is 31.45103711248136
At time: 148.27182936668396 and batch: 800, loss is 3.431223850250244 and perplexity is 30.914454269732534
At time: 148.69643568992615 and batch: 850, loss is 3.419974617958069 and perplexity is 30.5686391167234
At time: 149.12040662765503 and batch: 900, loss is 3.574689350128174 and perplexity is 35.68353400842139
At time: 149.5448513031006 and batch: 950, loss is 3.5029342937469483 and perplexity is 33.21276512518721
At time: 149.96927428245544 and batch: 1000, loss is 3.422141947746277 and perplexity is 30.63496328607787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497688479539825 and perplexity of 89.80929516104
Finished 16 epochs...
Completing Train Step...
At time: 151.23568677902222 and batch: 50, loss is 3.7244009494781496 and perplexity is 41.44639682094249
At time: 151.66336703300476 and batch: 100, loss is 3.5892334508895876 and perplexity is 36.206311370615744
At time: 152.0919098854065 and batch: 150, loss is 3.639145460128784 and perplexity is 38.059299636249285
At time: 152.5195701122284 and batch: 200, loss is 3.656911725997925 and perplexity is 38.74151352462398
At time: 152.94735050201416 and batch: 250, loss is 3.6371226358413695 and perplexity is 37.98239017397216
At time: 153.37558245658875 and batch: 300, loss is 3.557857666015625 and perplexity is 35.08794646768601
At time: 153.80612182617188 and batch: 350, loss is 3.633296694755554 and perplexity is 37.83734942247661
At time: 154.23718857765198 and batch: 400, loss is 3.526995849609375 and perplexity is 34.02160791306606
At time: 154.66710662841797 and batch: 450, loss is 3.5853983545303345 and perplexity is 36.06772259820774
At time: 155.09740018844604 and batch: 500, loss is 3.5022277879714965 and perplexity is 33.18930840193847
At time: 155.52643370628357 and batch: 550, loss is 3.554910545349121 and perplexity is 34.9846902844578
At time: 155.9557909965515 and batch: 600, loss is 3.559121460914612 and perplexity is 35.13231846809585
At time: 156.3860685825348 and batch: 650, loss is 3.5140236616134644 and perplexity is 33.58312341984579
At time: 156.81549263000488 and batch: 700, loss is 3.490557098388672 and perplexity is 32.804217793515676
At time: 157.25876235961914 and batch: 750, loss is 3.4484317970275877 and perplexity is 31.451032013493887
At time: 157.6883509159088 and batch: 800, loss is 3.4315389823913574 and perplexity is 30.92419794308959
At time: 158.1170494556427 and batch: 850, loss is 3.420476803779602 and perplexity is 30.5839941090784
At time: 158.54553627967834 and batch: 900, loss is 3.575556001663208 and perplexity is 35.714472602498795
At time: 158.97489261627197 and batch: 950, loss is 3.5039828205108643 and perplexity is 33.24760786190341
At time: 159.40259194374084 and batch: 1000, loss is 3.4232227325439455 and perplexity is 30.668090987407894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497534775152439 and perplexity of 89.7954921391654
Finished 17 epochs...
Completing Train Step...
At time: 160.6498200893402 and batch: 50, loss is 3.722974033355713 and perplexity is 41.38729846332889
At time: 161.088232755661 and batch: 100, loss is 3.5875982904434203 and perplexity is 36.14715661929609
At time: 161.5129270553589 and batch: 150, loss is 3.6376503038406374 and perplexity is 38.00243755451811
At time: 161.9392466545105 and batch: 200, loss is 3.65532958984375 and perplexity is 38.680267637850754
At time: 162.36458325386047 and batch: 250, loss is 3.635526866912842 and perplexity is 37.92182739085691
At time: 162.7883334159851 and batch: 300, loss is 3.556312575340271 and perplexity is 35.03377427003733
At time: 163.21247696876526 and batch: 350, loss is 3.631845164299011 and perplexity is 37.782467198640866
At time: 163.63757491111755 and batch: 400, loss is 3.525709619522095 and perplexity is 33.97787642775578
At time: 164.06301355361938 and batch: 450, loss is 3.5842632389068605 and perplexity is 36.02680479039836
At time: 164.4871642589569 and batch: 500, loss is 3.501147255897522 and perplexity is 33.15346565780181
At time: 164.9117636680603 and batch: 550, loss is 3.553911175727844 and perplexity is 34.94974511225791
At time: 165.3360583782196 and batch: 600, loss is 3.558295588493347 and perplexity is 35.103315633147204
At time: 165.76093769073486 and batch: 650, loss is 3.51336651802063 and perplexity is 33.56106173510438
At time: 166.18944334983826 and batch: 700, loss is 3.4900831890106203 and perplexity is 32.78867525023309
At time: 166.6268515586853 and batch: 750, loss is 3.4483015584945678 and perplexity is 31.446936143948296
At time: 167.06539177894592 and batch: 800, loss is 3.4316009044647218 and perplexity is 30.926112892831576
At time: 167.4929964542389 and batch: 850, loss is 3.4207344484329223 and perplexity is 30.591874926819468
At time: 167.91899991035461 and batch: 900, loss is 3.5760644817352296 and perplexity is 35.73263731790645
At time: 168.35706758499146 and batch: 950, loss is 3.504655795097351 and perplexity is 33.269990187579396
At time: 168.7825710773468 and batch: 1000, loss is 3.423936138153076 and perplexity is 30.68997758162091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497489370950839 and perplexity of 89.7914151390947
Finished 18 epochs...
Completing Train Step...
At time: 170.0298194885254 and batch: 50, loss is 3.7217271280288697 and perplexity is 41.33572458096777
At time: 170.46752882003784 and batch: 100, loss is 3.5862056589126587 and perplexity is 36.09685198528058
At time: 170.89377284049988 and batch: 150, loss is 3.6363041162490846 and perplexity is 37.95131356359523
At time: 171.31941294670105 and batch: 200, loss is 3.653948411941528 and perplexity is 38.62688018421
At time: 171.74516367912292 and batch: 250, loss is 3.6341567993164063 and perplexity is 37.86990749895333
At time: 172.1711242198944 and batch: 300, loss is 3.554998755455017 and perplexity is 34.987776423804874
At time: 172.5977816581726 and batch: 350, loss is 3.6306648588180543 and perplexity is 37.737898652945745
At time: 173.02412724494934 and batch: 400, loss is 3.5246450805664065 and perplexity is 33.941724900435474
At time: 173.45017528533936 and batch: 450, loss is 3.5832521677017213 and perplexity is 35.99039753372754
At time: 173.87695384025574 and batch: 500, loss is 3.500190191268921 and perplexity is 33.121750827497166
At time: 174.30398106575012 and batch: 550, loss is 3.5530619478225707 and perplexity is 34.920077412528634
At time: 174.7323932647705 and batch: 600, loss is 3.5575992918014525 and perplexity is 35.07888181817677
At time: 175.1619987487793 and batch: 650, loss is 3.5127965021133423 and perplexity is 33.541936847312876
At time: 175.58929443359375 and batch: 700, loss is 3.489643387794495 and perplexity is 32.77425792159461
At time: 176.0141429901123 and batch: 750, loss is 3.4481089067459108 and perplexity is 31.440878420244527
At time: 176.4385552406311 and batch: 800, loss is 3.431561675071716 and perplexity is 30.924899703991255
At time: 176.8627257347107 and batch: 850, loss is 3.420869760513306 and perplexity is 30.596014657130095
At time: 177.28705215454102 and batch: 900, loss is 3.5764027881622313 and perplexity is 35.74472794381773
At time: 177.71072435379028 and batch: 950, loss is 3.5051450157165527 and perplexity is 33.28627053479586
At time: 178.13505482673645 and batch: 1000, loss is 3.4244644689559935 and perplexity is 30.70619632617203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497490487447599 and perplexity of 89.79151539097474
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 179.40264749526978 and batch: 50, loss is 3.7209156703948976 and perplexity is 41.30219599705324
At time: 179.8294780254364 and batch: 100, loss is 3.5855933237075805 and perplexity is 36.07475537797321
At time: 180.25675988197327 and batch: 150, loss is 3.635829219818115 and perplexity is 37.93329489907165
At time: 180.68368864059448 and batch: 200, loss is 3.6534438228607176 and perplexity is 38.60739439881398
At time: 181.1104576587677 and batch: 250, loss is 3.63365581035614 and perplexity is 37.85093984505937
At time: 181.53795957565308 and batch: 300, loss is 3.554481153488159 and perplexity is 34.9696713679224
At time: 181.96416997909546 and batch: 350, loss is 3.63056884765625 and perplexity is 37.73427556738312
At time: 182.3906307220459 and batch: 400, loss is 3.5245752477645875 and perplexity is 33.93935473744563
At time: 182.8173656463623 and batch: 450, loss is 3.5832246923446656 and perplexity is 35.98940869828909
At time: 183.24410200119019 and batch: 500, loss is 3.499659652709961 and perplexity is 33.10418312212753
At time: 183.67036938667297 and batch: 550, loss is 3.5526994562149046 and perplexity is 34.90742147150233
At time: 184.09608674049377 and batch: 600, loss is 3.556945457458496 and perplexity is 35.05595353699719
At time: 184.52340078353882 and batch: 650, loss is 3.5118026781082152 and perplexity is 33.50861862426283
At time: 184.95665955543518 and batch: 700, loss is 3.4883565759658812 and perplexity is 32.732110742387356
At time: 185.3961582183838 and batch: 750, loss is 3.4459723901748656 and perplexity is 31.373776170550823
At time: 185.83382487297058 and batch: 800, loss is 3.428535695075989 and perplexity is 30.831463016187687
At time: 186.2847752571106 and batch: 850, loss is 3.4183426475524903 and perplexity is 30.51879268735798
At time: 186.73245573043823 and batch: 900, loss is 3.573660888671875 and perplexity is 35.646853734426365
At time: 187.16466975212097 and batch: 950, loss is 3.5025099754333495 and perplexity is 33.198675330189765
At time: 187.60286045074463 and batch: 1000, loss is 3.4220150899887085 and perplexity is 30.63107724982434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497351669683689 and perplexity of 89.779051598708
Finished 20 epochs...
Completing Train Step...
At time: 188.91271829605103 and batch: 50, loss is 3.720488042831421 and perplexity is 41.284537815444466
At time: 189.35607862472534 and batch: 100, loss is 3.585215439796448 and perplexity is 36.06112588366333
At time: 189.79332876205444 and batch: 150, loss is 3.635404095649719 and perplexity is 37.91717196599025
At time: 190.239727973938 and batch: 200, loss is 3.6529755544662477 and perplexity is 38.58932000838735
At time: 190.664936542511 and batch: 250, loss is 3.633192987442017 and perplexity is 37.833425616083865
At time: 191.08957505226135 and batch: 300, loss is 3.5540336322784425 and perplexity is 34.95402519954567
At time: 191.51456546783447 and batch: 350, loss is 3.629815168380737 and perplexity is 37.705846740366084
At time: 191.93936133384705 and batch: 400, loss is 3.5237978219985964 and perplexity is 33.91297966225155
At time: 192.3649446964264 and batch: 450, loss is 3.582511739730835 and perplexity is 35.96375909984803
At time: 192.7900047302246 and batch: 500, loss is 3.4990845012664793 and perplexity is 33.08514867778834
At time: 193.21665954589844 and batch: 550, loss is 3.5521078586578367 and perplexity is 34.88677643361537
At time: 193.65927052497864 and batch: 600, loss is 3.5564943599700927 and perplexity is 35.040143450616284
At time: 194.09215092658997 and batch: 650, loss is 3.5115217065811155 and perplexity is 33.49920497906203
At time: 194.5172815322876 and batch: 700, loss is 3.488159170150757 and perplexity is 32.725649871113276
At time: 194.94373536109924 and batch: 750, loss is 3.4459694814682007 and perplexity is 31.373684913571694
At time: 195.3694760799408 and batch: 800, loss is 3.4288196992874145 and perplexity is 30.840220525054534
At time: 195.79580354690552 and batch: 850, loss is 3.418509383201599 and perplexity is 30.523881682313373
At time: 196.22283816337585 and batch: 900, loss is 3.573927946090698 and perplexity is 35.65637476244784
At time: 196.64897441864014 and batch: 950, loss is 3.502903547286987 and perplexity is 33.21174396593268
At time: 197.07424640655518 and batch: 1000, loss is 3.422420334815979 and perplexity is 30.643492850942714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497282446884528 and perplexity of 89.77283705654678
Finished 21 epochs...
Completing Train Step...
At time: 198.31830978393555 and batch: 50, loss is 3.7201558685302736 and perplexity is 41.270826430358376
At time: 198.7546410560608 and batch: 100, loss is 3.5849080228805543 and perplexity is 36.050041787367164
At time: 199.1796944141388 and batch: 150, loss is 3.6350421476364136 and perplexity is 37.90345040432279
At time: 199.60415983200073 and batch: 200, loss is 3.6525601053237917 and perplexity is 38.57329143824031
At time: 200.02975225448608 and batch: 250, loss is 3.6328124475479124 and perplexity is 37.81903122729961
At time: 200.4550426006317 and batch: 300, loss is 3.553650040626526 and perplexity is 34.94061969856175
At time: 200.89570474624634 and batch: 350, loss is 3.629261698722839 and perplexity is 37.684983472395416
At time: 201.33684515953064 and batch: 400, loss is 3.523296570777893 and perplexity is 33.895984999451734
At time: 201.76244568824768 and batch: 450, loss is 3.582087869644165 and perplexity is 35.94851836843466
At time: 202.18737268447876 and batch: 500, loss is 3.498732557296753 and perplexity is 33.073506608021994
At time: 202.61236429214478 and batch: 550, loss is 3.551775531768799 and perplexity is 34.875184545989285
At time: 203.03686237335205 and batch: 600, loss is 3.5562478733062743 and perplexity is 35.03150758691376
At time: 203.4614326953888 and batch: 650, loss is 3.5113610219955445 and perplexity is 33.49382260563681
At time: 203.88643622398376 and batch: 700, loss is 3.4880393028259276 and perplexity is 32.72172737010447
At time: 204.3107397556305 and batch: 750, loss is 3.445975031852722 and perplexity is 31.373859050070077
At time: 204.73559498786926 and batch: 800, loss is 3.428946795463562 and perplexity is 30.84414044825316
At time: 205.1608715057373 and batch: 850, loss is 3.4186286973953246 and perplexity is 30.52752383192142
At time: 205.58480095863342 and batch: 900, loss is 3.5741153144836426 and perplexity is 35.66305626601706
At time: 206.00831127166748 and batch: 950, loss is 3.5031610107421876 and perplexity is 33.22029587714245
At time: 206.4329433441162 and batch: 1000, loss is 3.4226852083206176 and perplexity is 30.6516105753261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497248951981708 and perplexity of 89.76983017445157
Finished 22 epochs...
Completing Train Step...
At time: 207.7012436389923 and batch: 50, loss is 3.7198464012145998 and perplexity is 41.258056434537416
At time: 208.12813425064087 and batch: 100, loss is 3.584598627090454 and perplexity is 36.038889781486006
At time: 208.55499601364136 and batch: 150, loss is 3.6347087717056272 and perplexity is 37.890816412315665
At time: 208.98184609413147 and batch: 200, loss is 3.6521946573257447 and perplexity is 38.5591974815671
At time: 209.41009044647217 and batch: 250, loss is 3.6324628734588624 and perplexity is 37.805812984421785
At time: 209.83725929260254 and batch: 300, loss is 3.5533042526245118 and perplexity is 34.928539740159756
At time: 210.2633764743805 and batch: 350, loss is 3.6288690614700316 and perplexity is 37.6701898484667
At time: 210.6904284954071 and batch: 400, loss is 3.5229616069793703 and perplexity is 33.88463297292675
At time: 211.11785054206848 and batch: 450, loss is 3.5817996215820314 and perplexity is 35.93815777096119
At time: 211.54380655288696 and batch: 500, loss is 3.4984767389297486 and perplexity is 33.06504687969349
At time: 211.98380708694458 and batch: 550, loss is 3.5515327501296996 and perplexity is 34.86671851926083
At time: 212.41082668304443 and batch: 600, loss is 3.5560570621490477 and perplexity is 35.02482382210044
At time: 212.83672261238098 and batch: 650, loss is 3.5112258100509646 and perplexity is 33.48929414690864
At time: 213.26286149024963 and batch: 700, loss is 3.4879350566864016 and perplexity is 32.718316434139055
At time: 213.69087958335876 and batch: 750, loss is 3.4459640693664553 and perplexity is 31.373515116456293
At time: 214.1183831691742 and batch: 800, loss is 3.4290024137496946 and perplexity is 30.84585599418955
At time: 214.5458345413208 and batch: 850, loss is 3.418706784248352 and perplexity is 30.529907723262244
At time: 214.97359228134155 and batch: 900, loss is 3.5742477226257323 and perplexity is 35.66777865767313
At time: 215.40081405639648 and batch: 950, loss is 3.5033397579193117 and perplexity is 33.22623444198876
At time: 215.82805466651917 and batch: 1000, loss is 3.422873115539551 and perplexity is 30.657370775399773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497232948861471 and perplexity of 89.76839358856058
Finished 23 epochs...
Completing Train Step...
At time: 217.09903407096863 and batch: 50, loss is 3.7195462369918824 and perplexity is 41.245674100556705
At time: 217.54248523712158 and batch: 100, loss is 3.58428382396698 and perplexity is 36.02754641197402
At time: 217.98574304580688 and batch: 150, loss is 3.6343887710571288 and perplexity is 37.878693266302335
At time: 218.43597292900085 and batch: 200, loss is 3.651856656074524 and perplexity is 38.54616662691891
At time: 218.88317680358887 and batch: 250, loss is 3.6321298694610595 and perplexity is 37.793225593499855
At time: 219.3172755241394 and batch: 300, loss is 3.552980589866638 and perplexity is 34.91723650197625
At time: 219.75911021232605 and batch: 350, loss is 3.6285524368286133 and perplexity is 37.65826442615468
At time: 220.19694185256958 and batch: 400, loss is 3.5226910638809206 and perplexity is 33.87546695928919
At time: 220.62593483924866 and batch: 450, loss is 3.581557993888855 and perplexity is 35.92947516582315
At time: 221.05470085144043 and batch: 500, loss is 3.498253426551819 and perplexity is 33.05766386983785
At time: 221.4826681613922 and batch: 550, loss is 3.551321930885315 and perplexity is 34.859368718775336
At time: 221.9128806591034 and batch: 600, loss is 3.555886993408203 and perplexity is 35.018867700904075
At time: 222.3415606021881 and batch: 650, loss is 3.511099739074707 and perplexity is 33.48507238502778
At time: 222.76985836029053 and batch: 700, loss is 3.4878358936309812 and perplexity is 32.71507214677262
At time: 223.2256805896759 and batch: 750, loss is 3.44594199180603 and perplexity is 31.37282247342652
At time: 223.6555941104889 and batch: 800, loss is 3.429027075767517 and perplexity is 30.846616724620365
At time: 224.08430123329163 and batch: 850, loss is 3.418762640953064 and perplexity is 30.53161307092994
At time: 224.5129919052124 and batch: 900, loss is 3.57435142993927 and perplexity is 35.67147785899138
At time: 224.9416425228119 and batch: 950, loss is 3.5034811162948607 and perplexity is 33.2309315804972
At time: 225.37068247795105 and batch: 1000, loss is 3.4230242013931274 and perplexity is 30.662003020356316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497224389052972 and perplexity of 89.76762519159092
Finished 24 epochs...
Completing Train Step...
At time: 226.6244592666626 and batch: 50, loss is 3.719253945350647 and perplexity is 41.23362009650812
At time: 227.06052565574646 and batch: 100, loss is 3.583971390724182 and perplexity is 36.016291967041376
At time: 227.4849636554718 and batch: 150, loss is 3.6340781688690185 and perplexity is 37.866929888251114
At time: 227.90956687927246 and batch: 200, loss is 3.65153489112854 and perplexity is 38.53376581687629
At time: 228.3337984085083 and batch: 250, loss is 3.6318094873428346 and perplexity is 37.78111925925972
At time: 228.75856280326843 and batch: 300, loss is 3.5526718616485597 and perplexity is 34.90645822963479
At time: 229.1831078529358 and batch: 350, loss is 3.628270635604858 and perplexity is 37.64765377627217
At time: 229.6076488494873 and batch: 400, loss is 3.5224467277526856 and perplexity is 33.86719096995298
At time: 230.03207778930664 and batch: 450, loss is 3.5813348150253295 and perplexity is 35.921457361124126
At time: 230.45696091651917 and batch: 500, loss is 3.498043460845947 and perplexity is 33.050723622741394
At time: 230.88167691230774 and batch: 550, loss is 3.55112642288208 and perplexity is 34.85255409938112
At time: 231.30662870407104 and batch: 600, loss is 3.5557273244857788 and perplexity is 35.013276722398395
At time: 231.73336124420166 and batch: 650, loss is 3.510977954864502 and perplexity is 33.48099468023879
At time: 232.15816497802734 and batch: 700, loss is 3.4877390336990355 and perplexity is 32.71190352056982
At time: 232.58313941955566 and batch: 750, loss is 3.4459121942520143 and perplexity is 31.37188765398198
At time: 233.0074486732483 and batch: 800, loss is 3.4290354156494143 and perplexity is 30.84687398283352
At time: 233.4306788444519 and batch: 850, loss is 3.4188049983978273 and perplexity is 30.532906339433705
At time: 233.86758017539978 and batch: 900, loss is 3.5744380950927734 and perplexity is 35.67456946706113
At time: 234.2906153202057 and batch: 950, loss is 3.503602590560913 and perplexity is 33.23496852870887
At time: 234.714115858078 and batch: 1000, loss is 3.423155360221863 and perplexity is 30.66602487650436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.49721992306593 and perplexity of 89.7672242914352
Finished 25 epochs...
Completing Train Step...
At time: 235.97346210479736 and batch: 50, loss is 3.7189689540863036 and perplexity is 41.22187054932205
At time: 236.39764976501465 and batch: 100, loss is 3.583665466308594 and perplexity is 36.0052753891757
At time: 236.82268500328064 and batch: 150, loss is 3.6337753629684446 and perplexity is 37.85546529430528
At time: 237.2474353313446 and batch: 200, loss is 3.651224341392517 and perplexity is 38.521801024001675
At time: 237.6719822883606 and batch: 250, loss is 3.631499400138855 and perplexity is 37.769405633841934
At time: 238.09681701660156 and batch: 300, loss is 3.5523748254776 and perplexity is 34.89609128869493
At time: 238.521564245224 and batch: 350, loss is 3.628006896972656 and perplexity is 37.6377259447935
At time: 238.9472873210907 and batch: 400, loss is 3.5222148847579957 and perplexity is 33.85934000910707
At time: 239.37241125106812 and batch: 450, loss is 3.5811204528808593 and perplexity is 35.9137579857485
At time: 239.79603004455566 and batch: 500, loss is 3.4978402757644655 and perplexity is 33.04400889095884
At time: 240.22002601623535 and batch: 550, loss is 3.5509398651123045 and perplexity is 34.84605269108034
At time: 240.64431071281433 and batch: 600, loss is 3.5555741214752197 and perplexity is 35.00791299387515
At time: 241.07060718536377 and batch: 650, loss is 3.510858669281006 and perplexity is 33.47700111844423
At time: 241.4948697090149 and batch: 700, loss is 3.4876432037353515 and perplexity is 32.708768890241565
At time: 241.91853737831116 and batch: 750, loss is 3.445876507759094 and perplexity is 31.370768121311535
At time: 242.34339809417725 and batch: 800, loss is 3.429033803939819 and perplexity is 30.846824266670808
At time: 242.76884031295776 and batch: 850, loss is 3.4188380241394043 and perplexity is 30.533914727959363
At time: 243.19381070137024 and batch: 900, loss is 3.5745132637023924 and perplexity is 35.67725117563561
At time: 243.61869406700134 and batch: 950, loss is 3.5037114095687865 and perplexity is 33.23858532179522
At time: 244.04387187957764 and batch: 1000, loss is 3.423273730278015 and perplexity is 30.66965503043747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497220667397103 and perplexity of 89.76729110800348
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 245.31010389328003 and batch: 50, loss is 3.7187678146362306 and perplexity is 41.2135800387511
At time: 245.75093030929565 and batch: 100, loss is 3.5835481691360473 and perplexity is 36.001052319857685
At time: 246.17769598960876 and batch: 150, loss is 3.6336808156967164 and perplexity is 37.85188633253491
At time: 246.60405087471008 and batch: 200, loss is 3.6511001539230348 and perplexity is 38.51701739605208
At time: 247.02913451194763 and batch: 250, loss is 3.6313815402984617 and perplexity is 37.76495440003818
At time: 247.45542240142822 and batch: 300, loss is 3.5522232103347777 and perplexity is 34.890800913890864
At time: 247.88100838661194 and batch: 350, loss is 3.627838053703308 and perplexity is 37.631371604553046
At time: 248.30709147453308 and batch: 400, loss is 3.5220115089416506 and perplexity is 33.85245453838466
At time: 248.73359560966492 and batch: 450, loss is 3.5810362815856935 and perplexity is 35.910735205442016
At time: 249.1600377559662 and batch: 500, loss is 3.4977219676971436 and perplexity is 33.04009974937644
At time: 249.5879135131836 and batch: 550, loss is 3.5509008836746214 and perplexity is 34.844694368323715
At time: 250.01496171951294 and batch: 600, loss is 3.555444369316101 and perplexity is 35.003370936255216
At time: 250.44270968437195 and batch: 650, loss is 3.5106286001205445 and perplexity is 33.46929997883347
At time: 250.86979866027832 and batch: 700, loss is 3.4873529434204102 and perplexity is 32.69927621042291
At time: 251.29744958877563 and batch: 750, loss is 3.44531090259552 and perplexity is 31.35302966982961
At time: 251.72559118270874 and batch: 800, loss is 3.428184552192688 and perplexity is 30.820638667925447
At time: 252.15458607673645 and batch: 850, loss is 3.4181457996368407 and perplexity is 30.51278571787844
At time: 252.58296370506287 and batch: 900, loss is 3.573746643066406 and perplexity is 35.64991073985925
At time: 253.01200342178345 and batch: 950, loss is 3.5029332304000853 and perplexity is 33.21272980851638
At time: 253.4411177635193 and batch: 1000, loss is 3.422570295333862 and perplexity is 30.648088509575448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497209874595084 and perplexity of 89.76632227263099
Finished 27 epochs...
Completing Train Step...
At time: 254.70615220069885 and batch: 50, loss is 3.7186934757232666 and perplexity is 41.21051637988761
At time: 255.14469170570374 and batch: 100, loss is 3.583485064506531 and perplexity is 35.998780558468916
At time: 255.5703263282776 and batch: 150, loss is 3.633600854873657 and perplexity is 37.848859785553614
At time: 256.00827193260193 and batch: 200, loss is 3.651011471748352 and perplexity is 38.51360177464171
At time: 256.43386816978455 and batch: 250, loss is 3.631303753852844 and perplexity is 37.76201691271632
At time: 256.85914945602417 and batch: 300, loss is 3.5521510648727417 and perplexity is 34.88828379173869
At time: 257.2840826511383 and batch: 350, loss is 3.627728986740112 and perplexity is 37.62726748894701
At time: 257.7109797000885 and batch: 400, loss is 3.5219173049926757 and perplexity is 33.84926565368978
At time: 258.1387674808502 and batch: 450, loss is 3.580954465866089 and perplexity is 35.90779726298623
At time: 258.5653541088104 and batch: 500, loss is 3.4976530265808106 and perplexity is 33.037822006531925
At time: 258.9909300804138 and batch: 550, loss is 3.550828609466553 and perplexity is 34.84217608663736
At time: 259.4175446033478 and batch: 600, loss is 3.555383114814758 and perplexity is 35.001226887890176
At time: 259.84371614456177 and batch: 650, loss is 3.510588402748108 and perplexity is 33.467954627957
At time: 260.2696542739868 and batch: 700, loss is 3.4873230171203615 and perplexity is 32.698297656713976
At time: 260.7179925441742 and batch: 750, loss is 3.4453140258789063 and perplexity is 31.353127594379206
At time: 261.1508505344391 and batch: 800, loss is 3.428210129737854 and perplexity is 30.821426994284714
At time: 261.57598996162415 and batch: 850, loss is 3.418172626495361 and perplexity is 30.51360429104378
At time: 262.0016202926636 and batch: 900, loss is 3.5737801456451415 and perplexity is 35.65110512380808
At time: 262.4278528690338 and batch: 950, loss is 3.5029830884933473 and perplexity is 33.21438577317793
At time: 262.85402250289917 and batch: 1000, loss is 3.4226314926147463 and perplexity is 30.64996414664789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497202431283346 and perplexity of 89.76565411639734
Finished 28 epochs...
Completing Train Step...
At time: 264.1111514568329 and batch: 50, loss is 3.7186214542388916 and perplexity is 41.20754844420492
At time: 264.53642320632935 and batch: 100, loss is 3.5834211492538453 and perplexity is 35.996479760841964
At time: 264.96135330200195 and batch: 150, loss is 3.63352388381958 and perplexity is 37.84594663103603
At time: 265.3868508338928 and batch: 200, loss is 3.6509270811080934 and perplexity is 38.510351724268126
At time: 265.81244015693665 and batch: 250, loss is 3.6312277507781983 and perplexity is 37.75914699238889
At time: 266.23782992362976 and batch: 300, loss is 3.5520790815353394 and perplexity is 34.88577250702153
At time: 266.67577266693115 and batch: 350, loss is 3.6276313781738283 and perplexity is 37.62359492455403
At time: 267.1005778312683 and batch: 400, loss is 3.521833510398865 and perplexity is 33.8464293870571
At time: 267.52531719207764 and batch: 450, loss is 3.5808808517456057 and perplexity is 35.905154039362706
At time: 267.94893646240234 and batch: 500, loss is 3.4975894784927366 and perplexity is 33.03572258281717
At time: 268.3733127117157 and batch: 550, loss is 3.5507639598846437 and perplexity is 34.83992362733161
At time: 268.79692125320435 and batch: 600, loss is 3.5553292655944824 and perplexity is 34.999342149859864
At time: 269.2204661369324 and batch: 650, loss is 3.510552158355713 and perplexity is 33.466741624259235
At time: 269.6447868347168 and batch: 700, loss is 3.4872949838638307 and perplexity is 32.69738102979572
At time: 270.06987714767456 and batch: 750, loss is 3.4453141689300537 and perplexity is 31.353132079480403
At time: 270.4939727783203 and batch: 800, loss is 3.4282299089431763 and perplexity is 30.822036623646525
At time: 270.9199435710907 and batch: 850, loss is 3.4181948709487915 and perplexity is 30.514283057042785
At time: 271.34469413757324 and batch: 900, loss is 3.5738107919692994 and perplexity is 35.65219771587417
At time: 271.76965379714966 and batch: 950, loss is 3.503027958869934 and perplexity is 33.215876148612274
At time: 272.1949772834778 and batch: 1000, loss is 3.4226841259002687 and perplexity is 30.651577397417043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497196848799542 and perplexity of 89.76515300248587
Finished 29 epochs...
Completing Train Step...
At time: 273.44985365867615 and batch: 50, loss is 3.718551015853882 and perplexity is 41.20464595326689
At time: 273.8871593475342 and batch: 100, loss is 3.583356008529663 and perplexity is 35.99413500045285
At time: 274.31217074394226 and batch: 150, loss is 3.6334487533569337 and perplexity is 37.843103354366036
At time: 274.7366313934326 and batch: 200, loss is 3.6508456754684446 and perplexity is 38.50721689205116
At time: 275.1616368293762 and batch: 250, loss is 3.6311529064178467 and perplexity is 37.75632103893948
At time: 275.58561515808105 and batch: 300, loss is 3.552007250785828 and perplexity is 34.88326672583221
At time: 276.0106256008148 and batch: 350, loss is 3.627542757987976 and perplexity is 37.62026086231417
At time: 276.4351599216461 and batch: 400, loss is 3.5217575597763062 and perplexity is 33.843858827292806
At time: 276.8596589565277 and batch: 450, loss is 3.5808134365081785 and perplexity is 35.902733566467575
At time: 277.2834758758545 and batch: 500, loss is 3.4975298547744753 and perplexity is 33.03375292892097
At time: 277.7205722332001 and batch: 550, loss is 3.5507048892974855 and perplexity is 34.83786567336927
At time: 278.14672327041626 and batch: 600, loss is 3.555280647277832 and perplexity is 34.99764058212468
At time: 278.5721230506897 and batch: 650, loss is 3.5105187702178955 and perplexity is 33.465624250731196
At time: 278.9973781108856 and batch: 700, loss is 3.4872686958312986 and perplexity is 32.69652149127734
At time: 279.42270946502686 and batch: 750, loss is 3.445311985015869 and perplexity is 31.35306360700529
At time: 279.8474748134613 and batch: 800, loss is 3.428245348930359 and perplexity is 30.822512519170836
At time: 280.2736008167267 and batch: 850, loss is 3.4182136821746827 and perplexity is 30.51485707351324
At time: 280.69965052604675 and batch: 900, loss is 3.573838768005371 and perplexity is 35.65319513699539
At time: 281.12465715408325 and batch: 950, loss is 3.5030687999725343 and perplexity is 33.21723274932037
At time: 281.55008602142334 and batch: 1000, loss is 3.422730121612549 and perplexity is 30.65298727097577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497193127143674 and perplexity of 89.76481892809905
Finished 30 epochs...
Completing Train Step...
At time: 282.8018958568573 and batch: 50, loss is 3.7184814167022706 and perplexity is 41.20177824466229
At time: 283.24170660972595 and batch: 100, loss is 3.583289632797241 and perplexity is 35.99174594266791
At time: 283.66935896873474 and batch: 150, loss is 3.6333749151229857 and perplexity is 37.84030918960661
At time: 284.09511256217957 and batch: 200, loss is 3.6507666444778444 and perplexity is 38.504173748807816
At time: 284.5221564769745 and batch: 250, loss is 3.6310787200927734 and perplexity is 37.7535201401288
At time: 284.9482989311218 and batch: 300, loss is 3.551935806274414 and perplexity is 34.880774596909994
At time: 285.37542366981506 and batch: 350, loss is 3.62746054649353 and perplexity is 37.617168171576346
At time: 285.80304479599 and batch: 400, loss is 3.521687250137329 and perplexity is 33.84147936144774
At time: 286.2304663658142 and batch: 450, loss is 3.580750513076782 and perplexity is 35.90047451434946
At time: 286.6577835083008 and batch: 500, loss is 3.4974732398986816 and perplexity is 33.03188278004152
At time: 287.08467960357666 and batch: 550, loss is 3.5506499862670897 and perplexity is 34.83595302147695
At time: 287.51161313056946 and batch: 600, loss is 3.5552356815338135 and perplexity is 34.99606692255767
At time: 287.9394247531891 and batch: 650, loss is 3.5104872608184814 and perplexity is 33.4645697856229
At time: 288.37883019447327 and batch: 700, loss is 3.487243399620056 and perplexity is 32.69569440362393
At time: 288.8046624660492 and batch: 750, loss is 3.445308222770691 and perplexity is 31.35294564931481
At time: 289.2307200431824 and batch: 800, loss is 3.428257212638855 and perplexity is 30.822878190643582
At time: 289.6573076248169 and batch: 850, loss is 3.418229761123657 and perplexity is 30.51534772428765
At time: 290.0842635631561 and batch: 900, loss is 3.5738643169403077 and perplexity is 35.6541060497946
At time: 290.51092886924744 and batch: 950, loss is 3.503106031417847 and perplexity is 33.21846949792773
At time: 290.93755745887756 and batch: 1000, loss is 3.4227711963653564 and perplexity is 30.65424636070896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497191266315739 and perplexity of 89.76465189137187
Finished 31 epochs...
Completing Train Step...
At time: 292.2059979438782 and batch: 50, loss is 3.7184125661849974 and perplexity is 41.198941578571656
At time: 292.64093112945557 and batch: 100, loss is 3.583222222328186 and perplexity is 35.98931980396631
At time: 293.0657813549042 and batch: 150, loss is 3.633301787376404 and perplexity is 37.83754211424184
At time: 293.4902837276459 and batch: 200, loss is 3.6506892585754396 and perplexity is 38.501194183865564
At time: 293.9153730869293 and batch: 250, loss is 3.6310051250457764 and perplexity is 37.75074177027817
At time: 294.3409585952759 and batch: 300, loss is 3.551864786148071 and perplexity is 34.87829744785594
At time: 294.7660632133484 and batch: 350, loss is 3.6273835563659667 and perplexity is 37.61427213248488
At time: 295.1915531158447 and batch: 400, loss is 3.5216212606430055 and perplexity is 33.83924625301917
At time: 295.61556911468506 and batch: 450, loss is 3.580691194534302 and perplexity is 35.898345013686985
At time: 296.0394558906555 and batch: 500, loss is 3.497419171333313 and perplexity is 33.03009684181018
At time: 296.46407437324524 and batch: 550, loss is 3.5505980968475344 and perplexity is 34.83414545099231
At time: 296.8931043148041 and batch: 600, loss is 3.555193347930908 and perplexity is 34.994585444315696
At time: 297.3301956653595 and batch: 650, loss is 3.5104570102691652 and perplexity is 33.463557479315746
At time: 297.7571518421173 and batch: 700, loss is 3.487218828201294 and perplexity is 32.69489103389503
At time: 298.1818404197693 and batch: 750, loss is 3.4453031063079833 and perplexity is 31.352785233547998
At time: 298.6063439846039 and batch: 800, loss is 3.428266053199768 and perplexity is 30.823150683380238
At time: 299.0311768054962 and batch: 850, loss is 3.4182437753677366 and perplexity is 30.515775376815434
At time: 299.46819949150085 and batch: 900, loss is 3.573887996673584 and perplexity is 35.65495033951231
At time: 299.89202189445496 and batch: 950, loss is 3.503140382766724 and perplexity is 33.21961061676199
At time: 300.3173477649689 and batch: 1000, loss is 3.422808575630188 and perplexity is 30.655392215317352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497189777653392 and perplexity of 89.76451826221395
Finished 32 epochs...
Completing Train Step...
At time: 301.5647931098938 and batch: 50, loss is 3.718344221115112 and perplexity is 41.19612593024921
At time: 302.00225853919983 and batch: 100, loss is 3.5831541919708254 and perplexity is 35.9868715209586
At time: 302.42773389816284 and batch: 150, loss is 3.6332292127609254 and perplexity is 37.83479616881641
At time: 302.85291361808777 and batch: 200, loss is 3.650613260269165 and perplexity is 38.49826826950172
At time: 303.27656602859497 and batch: 250, loss is 3.6309319925308228 and perplexity is 37.747981064541044
At time: 303.70095205307007 and batch: 300, loss is 3.551794376373291 and perplexity is 34.875841761241105
At time: 304.12471985816956 and batch: 350, loss is 3.6273106145858764 and perplexity is 37.61152858057973
At time: 304.54844975471497 and batch: 400, loss is 3.5215583562850954 and perplexity is 33.8371176839103
At time: 304.97333788871765 and batch: 450, loss is 3.5806344318389893 and perplexity is 35.896307384697955
At time: 305.4066617488861 and batch: 500, loss is 3.497366986274719 and perplexity is 33.028373209245444
At time: 305.8300862312317 and batch: 550, loss is 3.5505486965179442 and perplexity is 34.83242467522984
At time: 306.2546479701996 and batch: 600, loss is 3.555152902603149 and perplexity is 34.993170105459726
At time: 306.6786756515503 and batch: 650, loss is 3.510427656173706 and perplexity is 33.4625752012721
At time: 307.10310983657837 and batch: 700, loss is 3.4871947717666627 and perplexity is 32.69410452084649
At time: 307.5286943912506 and batch: 750, loss is 3.4452971506118772 and perplexity is 31.352598506443115
At time: 307.96925473213196 and batch: 800, loss is 3.4282727909088133 and perplexity is 30.82335836150104
At time: 308.39930057525635 and batch: 850, loss is 3.4182562255859374 and perplexity is 30.51615530724255
At time: 308.8236291408539 and batch: 900, loss is 3.57391001701355 and perplexity is 35.65573548228477
At time: 309.2496521472931 and batch: 950, loss is 3.5031724262237547 and perplexity is 33.22067510498222
At time: 309.6751446723938 and batch: 1000, loss is 3.422842926979065 and perplexity is 30.65644528747742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497187916825458 and perplexity of 89.76435122604627
Finished 33 epochs...
Completing Train Step...
At time: 310.9336905479431 and batch: 50, loss is 3.7182762575149537 and perplexity is 41.193326188359755
At time: 311.3736596107483 and batch: 100, loss is 3.583085675239563 and perplexity is 35.98440590262264
At time: 311.79956555366516 and batch: 150, loss is 3.63315731048584 and perplexity is 37.8320758586939
At time: 312.2264037132263 and batch: 200, loss is 3.6505383348464964 and perplexity is 38.49538387853807
At time: 312.65342688560486 and batch: 250, loss is 3.630859365463257 and perplexity is 37.74523963892186
At time: 313.09409976005554 and batch: 300, loss is 3.551724519729614 and perplexity is 34.87340553708447
At time: 313.52935218811035 and batch: 350, loss is 3.6272405624389648 and perplexity is 37.60889390453747
At time: 313.95584058761597 and batch: 400, loss is 3.5214979887008666 and perplexity is 33.835075080512595
At time: 314.38248014450073 and batch: 450, loss is 3.580579571723938 and perplexity is 35.89433816316127
At time: 314.81013536453247 and batch: 500, loss is 3.497316098213196 and perplexity is 33.02669250212188
At time: 315.2375953197479 and batch: 550, loss is 3.550501093864441 and perplexity is 34.830766598852065
At time: 315.6648528575897 and batch: 600, loss is 3.5551139402389524 and perplexity is 34.9918067153823
At time: 316.0916073322296 and batch: 650, loss is 3.51039888381958 and perplexity is 33.46161241805929
At time: 316.5189027786255 and batch: 700, loss is 3.487171001434326 and perplexity is 32.693327380353054
At time: 316.94428300857544 and batch: 750, loss is 3.445290517807007 and perplexity is 31.352390551464712
At time: 317.37047600746155 and batch: 800, loss is 3.428277711868286 and perplexity is 30.82351004237156
At time: 317.797646522522 and batch: 850, loss is 3.4182673263549805 and perplexity is 30.516494061914912
At time: 318.2243478298187 and batch: 900, loss is 3.5739305877685545 and perplexity is 35.65646895522791
At time: 318.6501772403717 and batch: 950, loss is 3.5032023286819456 and perplexity is 33.22166849968302
At time: 319.07646799087524 and batch: 1000, loss is 3.4228748178482054 and perplexity is 30.657422963751788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497187172494283 and perplexity of 89.76428441166617
Finished 34 epochs...
Completing Train Step...
At time: 320.3380608558655 and batch: 50, loss is 3.7182088184356687 and perplexity is 41.190548242041054
At time: 320.7640538215637 and batch: 100, loss is 3.58301682472229 and perplexity is 35.98192844295066
At time: 321.20265007019043 and batch: 150, loss is 3.6330858707427978 and perplexity is 37.82937324145409
At time: 321.6287045478821 and batch: 200, loss is 3.6504644632339476 and perplexity is 38.492540267487634
At time: 322.0543749332428 and batch: 250, loss is 3.630787105560303 and perplexity is 37.74251227010946
At time: 322.48033809661865 and batch: 300, loss is 3.5516551399230956 and perplexity is 34.8709861108863
At time: 322.90726494789124 and batch: 350, loss is 3.6271730184555055 and perplexity is 37.60635373581718
At time: 323.3337163925171 and batch: 400, loss is 3.521439623832703 and perplexity is 33.833100358443964
At time: 323.75978231430054 and batch: 450, loss is 3.5805263900756836 and perplexity is 35.89242929385361
At time: 324.1845910549164 and batch: 500, loss is 3.497266583442688 and perplexity is 33.025057233507326
At time: 324.62069272994995 and batch: 550, loss is 3.5504549980163573 and perplexity is 34.829161082130405
At time: 325.05405712127686 and batch: 600, loss is 3.555076169967651 and perplexity is 34.99048509030857
At time: 325.4805474281311 and batch: 650, loss is 3.5103707361221312 and perplexity is 33.460670563972364
At time: 325.90578150749207 and batch: 700, loss is 3.4871477556228636 and perplexity is 32.69256740626184
At time: 326.33192133903503 and batch: 750, loss is 3.4452831840515135 and perplexity is 31.352160621541394
At time: 326.75730299949646 and batch: 800, loss is 3.4282810211181642 and perplexity is 30.823612045237187
At time: 327.1826288700104 and batch: 850, loss is 3.41827721118927 and perplexity is 30.51679571389269
At time: 327.60903882980347 and batch: 900, loss is 3.5739499378204345 and perplexity is 35.65715891642742
At time: 328.03558802604675 and batch: 950, loss is 3.503230676651001 and perplexity is 33.22261027986232
At time: 328.4609041213989 and batch: 1000, loss is 3.4229050064086914 and perplexity is 30.658348481189226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4971864281631095 and perplexity of 89.76421759733587
Finished 35 epochs...
Completing Train Step...
At time: 329.7176003456116 and batch: 50, loss is 3.7181416511535645 and perplexity is 41.1877816777796
At time: 330.15505743026733 and batch: 100, loss is 3.582947697639465 and perplexity is 35.97944120317179
At time: 330.58031702041626 and batch: 150, loss is 3.6330147647857665 and perplexity is 37.826683443297334
At time: 331.00527787208557 and batch: 200, loss is 3.650391173362732 and perplexity is 38.489719257545666
At time: 331.4302933216095 and batch: 250, loss is 3.6307154369354246 and perplexity is 37.73980741308346
At time: 331.86755657196045 and batch: 300, loss is 3.5515864372253416 and perplexity is 34.86859046236184
At time: 332.291624546051 and batch: 350, loss is 3.627107582092285 and perplexity is 37.60389299330663
At time: 332.7152621746063 and batch: 400, loss is 3.5213827228546144 and perplexity is 33.831175276711846
At time: 333.1466226577759 and batch: 450, loss is 3.5804744577407837 and perplexity is 35.890565364594664
At time: 333.598162651062 and batch: 500, loss is 3.4972180223464964 and perplexity is 33.02345353946515
At time: 334.03657126426697 and batch: 550, loss is 3.550409936904907 and perplexity is 34.82759167678101
At time: 334.46482157707214 and batch: 600, loss is 3.555039234161377 and perplexity is 34.98919271239748
At time: 334.8903601169586 and batch: 650, loss is 3.510342926979065 and perplexity is 33.459740064335826
At time: 335.3148744106293 and batch: 700, loss is 3.487124619483948 and perplexity is 32.69181103523059
At time: 335.74088072776794 and batch: 750, loss is 3.445275378227234 and perplexity is 31.35191589303996
At time: 336.16696763038635 and batch: 800, loss is 3.4282831716537476 and perplexity is 30.823678332582976
At time: 336.59253668785095 and batch: 850, loss is 3.418286180496216 and perplexity is 30.517069429627977
At time: 337.0179567337036 and batch: 900, loss is 3.5739683151245116 and perplexity is 35.65781420490055
At time: 337.442750453949 and batch: 950, loss is 3.503257646560669 and perplexity is 33.22350630274329
At time: 337.8760380744934 and batch: 1000, loss is 3.4229336738586427 and perplexity is 30.659227390457882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497186800328697 and perplexity of 89.76425100449484
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 339.13764214515686 and batch: 50, loss is 3.718093056678772 and perplexity is 41.18578022779119
At time: 339.5741493701935 and batch: 100, loss is 3.582923641204834 and perplexity is 35.978575676507205
At time: 339.9984517097473 and batch: 150, loss is 3.6329940700531007 and perplexity is 37.82590063829584
At time: 340.42355275154114 and batch: 200, loss is 3.6503611469268797 and perplexity is 38.488563565810146
At time: 340.8485631942749 and batch: 250, loss is 3.63069224357605 and perplexity is 37.73893211031804
At time: 341.27357172966003 and batch: 300, loss is 3.551552052497864 and perplexity is 34.867391535993754
At time: 341.69941806793213 and batch: 350, loss is 3.6270464849472046 and perplexity is 37.60159557298445
At time: 342.1243567466736 and batch: 400, loss is 3.5213085317611696 and perplexity is 33.82866539793202
At time: 342.5497748851776 and batch: 450, loss is 3.5804356050491335 and perplexity is 35.88917094661402
At time: 342.98708033561707 and batch: 500, loss is 3.497180643081665 and perplexity is 33.022219170119705
At time: 343.41156125068665 and batch: 550, loss is 3.550397963523865 and perplexity is 34.82717467525154
At time: 343.8376133441925 and batch: 600, loss is 3.55500714302063 and perplexity is 34.98806988730599
At time: 344.26323080062866 and batch: 650, loss is 3.510286793708801 and perplexity is 33.45786191241773
At time: 344.6891419887543 and batch: 700, loss is 3.487056884765625 and perplexity is 32.68959673961186
At time: 345.115177154541 and batch: 750, loss is 3.4451354265213014 and perplexity is 31.347528445948985
At time: 345.5395984649658 and batch: 800, loss is 3.4280759859085084 and perplexity is 30.817292767338515
At time: 345.96716928482056 and batch: 850, loss is 3.4181161451339723 and perplexity is 30.511880889803024
At time: 346.4059739112854 and batch: 900, loss is 3.573783965110779 and perplexity is 35.651241292239085
At time: 346.83405089378357 and batch: 950, loss is 3.503068265914917 and perplexity is 33.21721500940893
At time: 347.25904536247253 and batch: 1000, loss is 3.422758378982544 and perplexity is 30.653853456016538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497187172494283 and perplexity of 89.76428441166617
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 348.5188670158386 and batch: 50, loss is 3.718081274032593 and perplexity is 41.18529495317407
At time: 348.94530844688416 and batch: 100, loss is 3.5829183530807494 and perplexity is 35.97838541783769
At time: 349.3730766773224 and batch: 150, loss is 3.6329883193969725 and perplexity is 37.82568311517398
At time: 349.80104207992554 and batch: 200, loss is 3.6503531455993654 and perplexity is 38.488255607439534
At time: 350.2285912036896 and batch: 250, loss is 3.630686888694763 and perplexity is 37.73873002335777
At time: 350.6550974845886 and batch: 300, loss is 3.551544108390808 and perplexity is 34.86711454680285
At time: 351.0830523967743 and batch: 350, loss is 3.6270294713974 and perplexity is 37.600955841807504
At time: 351.5097725391388 and batch: 400, loss is 3.521288733482361 and perplexity is 33.82799565521264
At time: 351.9360601902008 and batch: 450, loss is 3.5804245853424073 and perplexity is 35.88877546065462
At time: 352.3617112636566 and batch: 500, loss is 3.497170810699463 and perplexity is 33.02189448463588
At time: 352.7872760295868 and batch: 550, loss is 3.5503942728042603 and perplexity is 34.8270461381524
At time: 353.21280813217163 and batch: 600, loss is 3.5549985647201536 and perplexity is 34.987769750416746
At time: 353.6516869068146 and batch: 650, loss is 3.510273127555847 and perplexity is 33.457404675283655
At time: 354.0786783695221 and batch: 700, loss is 3.487041187286377 and perplexity is 32.68908359937293
At time: 354.50560998916626 and batch: 750, loss is 3.445102777481079 and perplexity is 31.3465049959393
At time: 354.9323823451996 and batch: 800, loss is 3.428027391433716 and perplexity is 30.8157952535677
At time: 355.35944271087646 and batch: 850, loss is 3.4180760908126833 and perplexity is 30.510658781598252
At time: 355.7863128185272 and batch: 900, loss is 3.5737400817871094 and perplexity is 35.649676831605355
At time: 356.21256375312805 and batch: 950, loss is 3.503023099899292 and perplexity is 33.215714754037336
At time: 356.6390538215637 and batch: 1000, loss is 3.422716727256775 and perplexity is 30.652576696708426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497188288991044 and perplexity of 89.76438463325489
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 357.88872838020325 and batch: 50, loss is 3.718079090118408 and perplexity is 41.18520500812244
At time: 358.3245384693146 and batch: 100, loss is 3.5829178380966185 and perplexity is 35.978366889544915
At time: 358.7486927509308 and batch: 150, loss is 3.6329876375198364 and perplexity is 37.8256573227143
At time: 359.17262411117554 and batch: 200, loss is 3.650351872444153 and perplexity is 38.48820660594749
At time: 359.5963611602783 and batch: 250, loss is 3.6306863594055176 and perplexity is 37.73871004865912
At time: 360.0201599597931 and batch: 300, loss is 3.5515428304672243 and perplexity is 34.86706998932335
At time: 360.4435758590698 and batch: 350, loss is 3.6270258617401123 and perplexity is 37.60082011548818
At time: 360.86739921569824 and batch: 400, loss is 3.5212844705581663 and perplexity is 33.82785144933888
At time: 361.2909049987793 and batch: 450, loss is 3.580422430038452 and perplexity is 35.88869810951827
At time: 361.7315719127655 and batch: 500, loss is 3.497168869972229 and perplexity is 33.02183039820814
At time: 362.1604537963867 and batch: 550, loss is 3.550393795967102 and perplexity is 34.827029531326644
At time: 362.5848569869995 and batch: 600, loss is 3.5549971628189088 and perplexity is 34.98772070105316
At time: 363.01089334487915 and batch: 650, loss is 3.510270457267761 and perplexity is 33.457315334493856
At time: 363.4357588291168 and batch: 700, loss is 3.4870379066467283 and perplexity is 32.688976358445096
At time: 363.8602514266968 and batch: 750, loss is 3.4450953674316405 and perplexity is 31.34627271764815
At time: 364.28524518013 and batch: 800, loss is 3.428016104698181 and perplexity is 30.815447445799194
At time: 364.7292785644531 and batch: 850, loss is 3.418066854476929 and perplexity is 30.51037697621108
At time: 365.15576457977295 and batch: 900, loss is 3.573730049133301 and perplexity is 35.64931917253345
At time: 365.58288168907166 and batch: 950, loss is 3.503012709617615 and perplexity is 33.215369635197874
At time: 366.010320186615 and batch: 1000, loss is 3.422707190513611 and perplexity is 30.65228437235107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497188288991044 and perplexity of 89.76438463325489
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 367.2667906284332 and batch: 50, loss is 3.71807900428772 and perplexity is 41.1852014731681
At time: 367.70579528808594 and batch: 100, loss is 3.582918062210083 and perplexity is 35.978374952782275
At time: 368.13263034820557 and batch: 150, loss is 3.6329878425598143 and perplexity is 37.825665078487035
At time: 368.5597791671753 and batch: 200, loss is 3.6503517723083494 and perplexity is 38.48820275190018
At time: 368.9863338470459 and batch: 250, loss is 3.630686478614807 and perplexity is 37.738714547464205
At time: 369.41262316703796 and batch: 300, loss is 3.5515428495407106 and perplexity is 34.867070654359935
At time: 369.83936619758606 and batch: 350, loss is 3.6270252227783204 and perplexity is 37.600796090008465
At time: 370.2672679424286 and batch: 400, loss is 3.52128381729126 and perplexity is 33.827829350730234
At time: 370.69399404525757 and batch: 450, loss is 3.5804224395751953 and perplexity is 35.88869845177958
At time: 371.1209645271301 and batch: 500, loss is 3.4971689414978027 and perplexity is 33.021832760113575
At time: 371.54759073257446 and batch: 550, loss is 3.5503940105438234 and perplexity is 34.827037004397255
At time: 371.9744465351105 and batch: 600, loss is 3.554997191429138 and perplexity is 34.987721702059886
At time: 372.40176701545715 and batch: 650, loss is 3.510270256996155 and perplexity is 33.45730863394424
At time: 372.828729391098 and batch: 700, loss is 3.487037615776062 and perplexity is 32.68896685018215
At time: 373.2557737827301 and batch: 750, loss is 3.4450942516326903 and perplexity is 31.346237741529478
At time: 373.6909132003784 and batch: 800, loss is 3.4280141401290893 and perplexity is 30.815386906783058
At time: 374.1263563632965 and batch: 850, loss is 3.418065323829651 and perplexity is 30.51033027562136
At time: 374.55370903015137 and batch: 900, loss is 3.573728518486023 and perplexity is 35.64926460604187
At time: 374.9793174266815 and batch: 950, loss is 3.5030110549926756 and perplexity is 33.21531467626437
At time: 375.4189569950104 and batch: 1000, loss is 3.4227055358886718 and perplexity is 30.65223365435886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497188288991044 and perplexity of 89.76438463325489
Annealing...
Model not improving. Stopping early with 89.76421759733587loss at 39 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
SETTINGS FOR THIS RUN
{'anneal': 5.841560043068629, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.8712656972912644, 'batch_size': 50, 'num_layers': 1, 'lr': 28.644506215470557, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6641886234283447 and batch: 50, loss is 7.3554346561431885 and perplexity is 1564.6769440574683
At time: 1.0930020809173584 and batch: 100, loss is 6.856259965896607 and perplexity is 949.8081016211022
At time: 1.522085428237915 and batch: 150, loss is 6.7323810005187985 and perplexity is 839.1428888755034
At time: 1.9507217407226562 and batch: 200, loss is 6.662511768341065 and perplexity is 782.5139645622919
At time: 2.3790104389190674 and batch: 250, loss is 6.609456281661988 and perplexity is 742.0794268137861
At time: 2.8078701496124268 and batch: 300, loss is 6.4845156955719 and perplexity is 654.9217059382361
At time: 3.237583875656128 and batch: 350, loss is 6.513916559219361 and perplexity is 674.4628249934967
At time: 3.667611837387085 and batch: 400, loss is 6.428781805038452 and perplexity is 619.4189152159072
At time: 4.098268270492554 and batch: 450, loss is 6.363208885192871 and perplexity is 580.1048628367485
At time: 4.527904987335205 and batch: 500, loss is 6.3422269535064695 and perplexity is 568.0599469212478
At time: 4.970404863357544 and batch: 550, loss is 6.402966938018799 and perplexity is 603.6333263550977
At time: 5.399789333343506 and batch: 600, loss is 6.403277215957641 and perplexity is 603.8206495190354
At time: 5.8295347690582275 and batch: 650, loss is 6.359103870391846 and perplexity is 577.7284048234853
At time: 6.258974075317383 and batch: 700, loss is 6.326241998672486 and perplexity is 559.0517241570843
At time: 6.688213586807251 and batch: 750, loss is 6.223399248123169 and perplexity is 504.4149526715724
At time: 7.117257356643677 and batch: 800, loss is 6.2805205345153805 and perplexity is 534.0665915782198
At time: 7.557677745819092 and batch: 850, loss is 6.231365060806274 and perplexity is 508.44907389431637
At time: 7.999057769775391 and batch: 900, loss is 6.298353500366211 and perplexity is 543.6760104269939
At time: 8.428923845291138 and batch: 950, loss is 6.303594999313354 and perplexity is 546.5331690188117
At time: 8.860254764556885 and batch: 1000, loss is 6.232575273513794 and perplexity is 509.0647779159379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.987503796088986 and perplexity of 398.41883296392865
Finished 1 epochs...
Completing Train Step...
At time: 10.130484819412231 and batch: 50, loss is 5.900045928955078 and perplexity is 365.0542340398155
At time: 10.557869672775269 and batch: 100, loss is 5.773639526367187 and perplexity is 321.7064637006484
At time: 10.985673666000366 and batch: 150, loss is 5.681423921585083 and perplexity is 293.3668640653672
At time: 11.425973653793335 and batch: 200, loss is 5.602206726074218 and perplexity is 271.0238233547972
At time: 11.853533506393433 and batch: 250, loss is 5.574498834609986 and perplexity is 263.6174066050486
At time: 12.279924392700195 and batch: 300, loss is 5.44225998878479 and perplexity is 230.96356915179604
At time: 12.70606541633606 and batch: 350, loss is 5.478012504577637 and perplexity is 239.37048654140426
At time: 13.132589340209961 and batch: 400, loss is 5.407297925949097 and perplexity is 223.0281342256342
At time: 13.558120012283325 and batch: 450, loss is 5.390715923309326 and perplexity is 219.36037455649355
At time: 13.98436164855957 and batch: 500, loss is 5.383514060974121 and perplexity is 217.78624646778087
At time: 14.409754276275635 and batch: 550, loss is 5.4080350303649904 and perplexity is 223.19258985127834
At time: 14.836055278778076 and batch: 600, loss is 5.419050827026367 and perplexity is 225.66482586202474
At time: 15.262983798980713 and batch: 650, loss is 5.392082033157348 and perplexity is 219.66024970900355
At time: 15.689931392669678 and batch: 700, loss is 5.381897706985473 and perplexity is 217.43451114060824
At time: 16.1172513961792 and batch: 750, loss is 5.319288368225098 and perplexity is 204.2384876680106
At time: 16.572346448898315 and batch: 800, loss is 5.348309164047241 and perplexity is 210.25249467187697
At time: 17.021631002426147 and batch: 850, loss is 5.331879787445068 and perplexity is 206.82639863166054
At time: 17.460163354873657 and batch: 900, loss is 5.470521669387818 and perplexity is 237.5841007912587
At time: 17.888124704360962 and batch: 950, loss is 5.386046762466431 and perplexity is 218.33853311259543
At time: 18.315694332122803 and batch: 1000, loss is 5.33060926437378 and perplexity is 206.5637877823451
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.454924048447028 and perplexity of 233.90710478693052
Finished 2 epochs...
Completing Train Step...
At time: 19.571027755737305 and batch: 50, loss is 5.395414419174195 and perplexity is 220.39346345068523
At time: 20.01013422012329 and batch: 100, loss is 5.401904945373535 and perplexity is 221.8285853092835
At time: 20.43579077720642 and batch: 150, loss is 5.402505140304566 and perplexity is 221.96176566483302
At time: 20.86139941215515 and batch: 200, loss is 5.418099308013916 and perplexity is 225.45020361454323
At time: 21.287709951400757 and batch: 250, loss is 5.3661236000061034 and perplexity is 214.03158551734663
At time: 21.714304208755493 and batch: 300, loss is 5.254335660934448 and perplexity is 191.39429290373243
At time: 22.141294240951538 and batch: 350, loss is 5.315393476486206 and perplexity is 203.44454802778552
At time: 22.569032430648804 and batch: 400, loss is 5.261742458343506 and perplexity is 192.817174649924
At time: 22.995592832565308 and batch: 450, loss is 5.292680921554566 and perplexity is 198.87588199202006
At time: 23.422579288482666 and batch: 500, loss is 5.289181652069092 and perplexity is 198.18117787411032
At time: 23.84968400001526 and batch: 550, loss is 5.359034061431885 and perplexity is 212.519566427582
At time: 24.277923822402954 and batch: 600, loss is 5.403847560882569 and perplexity is 222.25993179398813
At time: 24.70574951171875 and batch: 650, loss is 5.338486766815185 and perplexity is 208.19742055004363
At time: 25.13253903388977 and batch: 700, loss is 5.321855630874634 and perplexity is 204.76349513667768
At time: 25.55977511405945 and batch: 750, loss is 5.220139770507813 and perplexity is 184.96003422200522
At time: 25.986818313598633 and batch: 800, loss is 5.285471353530884 and perplexity is 197.4472289663043
At time: 26.41306185722351 and batch: 850, loss is 5.280800104141235 and perplexity is 196.52705457376933
At time: 26.837968587875366 and batch: 900, loss is 5.366961660385132 and perplexity is 214.21103209205884
At time: 27.263890266418457 and batch: 950, loss is 5.357974405288696 and perplexity is 212.2944880373799
At time: 27.69036340713501 and batch: 1000, loss is 5.331925497055054 and perplexity is 206.83585280174836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.5047361792587655 and perplexity of 245.85358584694131
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 28.95521640777588 and batch: 50, loss is 5.330208158493042 and perplexity is 206.48095044669992
At time: 29.38120985031128 and batch: 100, loss is 5.215148391723633 and perplexity is 184.03912883701054
At time: 29.807981491088867 and batch: 150, loss is 5.2044078540802 and perplexity is 182.07302704166003
At time: 30.233971118927002 and batch: 200, loss is 5.192175712585449 and perplexity is 179.85945000678137
At time: 30.67395520210266 and batch: 250, loss is 5.161628179550171 and perplexity is 174.44825758768994
At time: 31.100791215896606 and batch: 300, loss is 5.054088449478149 and perplexity is 156.6616602192657
At time: 31.527421474456787 and batch: 350, loss is 5.120625047683716 and perplexity is 167.4399949007776
At time: 31.953989505767822 and batch: 400, loss is 5.054780282974243 and perplexity is 156.77008150377276
At time: 32.37995004653931 and batch: 450, loss is 5.080163984298706 and perplexity is 160.80042251121262
At time: 32.80690813064575 and batch: 500, loss is 5.029684286117554 and perplexity is 152.88473724143896
At time: 33.2334463596344 and batch: 550, loss is 5.079708223342895 and perplexity is 160.72715265499224
At time: 33.66886067390442 and batch: 600, loss is 5.087569065093994 and perplexity is 161.99558229781684
At time: 34.1032452583313 and batch: 650, loss is 5.059745273590088 and perplexity is 157.5503789691468
At time: 34.52896857261658 and batch: 700, loss is 5.027421894073487 and perplexity is 152.5392429973245
At time: 34.95519733428955 and batch: 750, loss is 4.964658393859863 and perplexity is 143.25960355336244
At time: 35.380919456481934 and batch: 800, loss is 4.9936485099792485 and perplexity is 147.47350167490035
At time: 35.80716347694397 and batch: 850, loss is 4.958635492324829 and perplexity is 142.39935825405226
At time: 36.23438858985901 and batch: 900, loss is 5.044369382858276 and perplexity is 155.14643034426263
At time: 36.660057067871094 and batch: 950, loss is 4.962270059585571 and perplexity is 142.91785999344918
At time: 37.0863835811615 and batch: 1000, loss is 4.938196430206299 and perplexity is 139.51839135004002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.202075120879383 and perplexity of 181.64879424984275
Finished 4 epochs...
Completing Train Step...
At time: 38.35071110725403 and batch: 50, loss is 5.108487596511841 and perplexity is 165.4199838298783
At time: 38.7787868976593 and batch: 100, loss is 5.042137994766235 and perplexity is 154.80062440418146
At time: 39.206878900527954 and batch: 150, loss is 5.063785486221313 and perplexity is 158.18820360627237
At time: 39.6346321105957 and batch: 200, loss is 5.069625835418702 and perplexity is 159.1147810884977
At time: 40.06341814994812 and batch: 250, loss is 5.02688196182251 and perplexity is 152.4569043711309
At time: 40.49013900756836 and batch: 300, loss is 4.942711133956909 and perplexity is 140.14969956722953
At time: 40.9162118434906 and batch: 350, loss is 5.005726280212403 and perplexity is 149.26545234485982
At time: 41.342726707458496 and batch: 400, loss is 4.94018627166748 and perplexity is 139.79628722241674
At time: 41.78191781044006 and batch: 450, loss is 4.978897075653077 and perplexity is 145.31402286429267
At time: 42.20869016647339 and batch: 500, loss is 4.929388999938965 and perplexity is 138.29498827420915
At time: 42.63578701019287 and batch: 550, loss is 4.98081789970398 and perplexity is 145.59341377883547
At time: 43.07170867919922 and batch: 600, loss is 4.999665594100952 and perplexity is 148.36353716407322
At time: 43.49797248840332 and batch: 650, loss is 4.977872276306153 and perplexity is 145.16518142789215
At time: 43.92414212226868 and batch: 700, loss is 4.949353704452514 and perplexity is 141.08375264877256
At time: 44.351253509521484 and batch: 750, loss is 4.892053880691528 and perplexity is 133.22692544938297
At time: 44.7939236164093 and batch: 800, loss is 4.926509561538697 and perplexity is 137.89734913784804
At time: 45.22596526145935 and batch: 850, loss is 4.899168558120728 and perplexity is 134.17817194225236
At time: 45.65328931808472 and batch: 900, loss is 5.001611404418945 and perplexity is 148.65250551312485
At time: 46.079923152923584 and batch: 950, loss is 4.927538299560547 and perplexity is 138.03928237756134
At time: 46.506826400756836 and batch: 1000, loss is 4.897157430648804 and perplexity is 133.90859370348974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.160316281202363 and perplexity of 174.21954926062293
Finished 5 epochs...
Completing Train Step...
At time: 47.75916051864624 and batch: 50, loss is 5.019173793792724 and perplexity is 151.28625849871918
At time: 48.19588804244995 and batch: 100, loss is 4.963325319290161 and perplexity is 143.06875505490268
At time: 48.62138557434082 and batch: 150, loss is 4.992796354293823 and perplexity is 147.34788482215768
At time: 49.07679796218872 and batch: 200, loss is 5.01510142326355 and perplexity is 150.67141757788326
At time: 49.52442979812622 and batch: 250, loss is 4.976222534179687 and perplexity is 144.92589374854842
At time: 49.96103024482727 and batch: 300, loss is 4.893083009719849 and perplexity is 133.36410372068013
At time: 50.387054443359375 and batch: 350, loss is 4.9516650390625 and perplexity is 141.41022155327502
At time: 50.81324362754822 and batch: 400, loss is 4.8738289737701415 and perplexity is 130.8208688095191
At time: 51.23980116844177 and batch: 450, loss is 4.9217190933227535 and perplexity is 137.23833602072054
At time: 51.666972160339355 and batch: 500, loss is 4.875409030914307 and perplexity is 131.02773664635075
At time: 52.09335017204285 and batch: 550, loss is 4.91797981262207 and perplexity is 136.72612161299347
At time: 52.53237748146057 and batch: 600, loss is 4.939901609420776 and perplexity is 139.7564981607056
At time: 52.95879364013672 and batch: 650, loss is 4.9184794425964355 and perplexity is 136.79445114995104
At time: 53.38482165336609 and batch: 700, loss is 4.893131256103516 and perplexity is 133.37053821161484
At time: 53.81180143356323 and batch: 750, loss is 4.839653644561768 and perplexity is 126.42555596726149
At time: 54.23775863647461 and batch: 800, loss is 4.8834942436218265 and perplexity is 132.09141801833724
At time: 54.66382575035095 and batch: 850, loss is 4.853183174133301 and perplexity is 128.14765760897873
At time: 55.08814859390259 and batch: 900, loss is 4.946952238082885 and perplexity is 140.7453512539013
At time: 55.51314115524292 and batch: 950, loss is 4.88614408493042 and perplexity is 132.4419034746506
At time: 55.937649965286255 and batch: 1000, loss is 4.867530555725097 and perplexity is 129.99949368602293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.167700790777439 and perplexity of 175.51083708603156
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 57.204498052597046 and batch: 50, loss is 4.968185443878173 and perplexity is 143.7657794698472
At time: 57.63019680976868 and batch: 100, loss is 4.887256927490235 and perplexity is 132.58937250122554
At time: 58.05569314956665 and batch: 150, loss is 4.898308782577515 and perplexity is 134.062858410564
At time: 58.48170876502991 and batch: 200, loss is 4.90998462677002 and perplexity is 135.63732920858126
At time: 58.90992307662964 and batch: 250, loss is 4.867287940979004 and perplexity is 129.96795771757021
At time: 59.33571720123291 and batch: 300, loss is 4.777673950195313 and perplexity is 118.82762935247749
At time: 59.76194524765015 and batch: 350, loss is 4.840207347869873 and perplexity is 126.49557759964536
At time: 60.18835997581482 and batch: 400, loss is 4.759730234146118 and perplexity is 116.71443608223213
At time: 60.614776372909546 and batch: 450, loss is 4.801523418426513 and perplexity is 121.6956698003381
At time: 61.04162526130676 and batch: 500, loss is 4.752305879592895 and perplexity is 115.85111549194635
At time: 61.46710443496704 and batch: 550, loss is 4.795131101608276 and perplexity is 120.92023358102911
At time: 61.89243006706238 and batch: 600, loss is 4.812370510101318 and perplexity is 123.02289916298238
At time: 62.31826591491699 and batch: 650, loss is 4.767864837646484 and perplexity is 117.66773383537189
At time: 62.743510723114014 and batch: 700, loss is 4.7417566680908205 and perplexity is 114.6354012594493
At time: 63.169111013412476 and batch: 750, loss is 4.685023393630981 and perplexity is 108.31280610836293
At time: 63.60890531539917 and batch: 800, loss is 4.720918226242065 and perplexity is 112.27129580215042
At time: 64.03405213356018 and batch: 850, loss is 4.693699684143066 and perplexity is 109.25664808305125
At time: 64.45998167991638 and batch: 900, loss is 4.788386240005493 and perplexity is 120.10738768864799
At time: 64.88643646240234 and batch: 950, loss is 4.71033408164978 and perplexity is 111.08926659247854
At time: 65.31378245353699 and batch: 1000, loss is 4.688592338562012 and perplexity is 108.70005918008412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.068962283250762 and perplexity of 159.0092351520454
Finished 7 epochs...
Completing Train Step...
At time: 66.58079528808594 and batch: 50, loss is 4.890718183517456 and perplexity is 133.0490934128632
At time: 67.0082221031189 and batch: 100, loss is 4.828720607757568 and perplexity is 125.05086915478473
At time: 67.43637800216675 and batch: 150, loss is 4.853645391464234 and perplexity is 128.20690336839763
At time: 67.86399102210999 and batch: 200, loss is 4.8669586277008055 and perplexity is 129.92516458991275
At time: 68.29158067703247 and batch: 250, loss is 4.830376434326172 and perplexity is 125.25810323085383
At time: 68.71783137321472 and batch: 300, loss is 4.748790559768676 and perplexity is 115.44457674450797
At time: 69.14359307289124 and batch: 350, loss is 4.81147985458374 and perplexity is 122.91337691958965
At time: 69.57006859779358 and batch: 400, loss is 4.733702402114869 and perplexity is 113.71580555291882
At time: 69.99652051925659 and batch: 450, loss is 4.779241514205933 and perplexity is 119.01404533904085
At time: 70.42346477508545 and batch: 500, loss is 4.73208532333374 and perplexity is 113.53206673678955
At time: 70.85009694099426 and batch: 550, loss is 4.774845914840698 and perplexity is 118.49205534679375
At time: 71.27769327163696 and batch: 600, loss is 4.794005613327027 and perplexity is 120.7842158327167
At time: 71.70397400856018 and batch: 650, loss is 4.7532023906707765 and perplexity is 115.95502387091666
At time: 72.13051676750183 and batch: 700, loss is 4.729465703964234 and perplexity is 113.23504514731712
At time: 72.55780601501465 and batch: 750, loss is 4.674311790466309 and perplexity is 107.15879400563203
At time: 72.98512053489685 and batch: 800, loss is 4.713006372451782 and perplexity is 111.38652642325283
At time: 73.42230820655823 and batch: 850, loss is 4.688754453659057 and perplexity is 108.7176825291938
At time: 73.8572986125946 and batch: 900, loss is 4.788718738555908 and perplexity is 120.14732986093784
At time: 74.30594253540039 and batch: 950, loss is 4.716029090881348 and perplexity is 111.7237259022731
At time: 74.73325085639954 and batch: 1000, loss is 4.69489652633667 and perplexity is 109.38748933195403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.063808906369093 and perplexity of 158.1919084407616
Finished 8 epochs...
Completing Train Step...
At time: 75.98797488212585 and batch: 50, loss is 4.871296701431274 and perplexity is 130.49001382634512
At time: 76.42690086364746 and batch: 100, loss is 4.810330200195312 and perplexity is 122.77215021292412
At time: 76.85313439369202 and batch: 150, loss is 4.835718746185303 and perplexity is 125.92906172016478
At time: 77.27902007102966 and batch: 200, loss is 4.84842116355896 and perplexity is 127.5388677905913
At time: 77.7046115398407 and batch: 250, loss is 4.813331785202027 and perplexity is 123.14121487062584
At time: 78.13054704666138 and batch: 300, loss is 4.733664846420288 and perplexity is 113.71153495704955
At time: 78.55613350868225 and batch: 350, loss is 4.79675555229187 and perplexity is 121.11682216813203
At time: 78.98240208625793 and batch: 400, loss is 4.719655447006225 and perplexity is 112.12961141790748
At time: 79.40925168991089 and batch: 450, loss is 4.765484819412231 and perplexity is 117.38801548268881
At time: 79.8351571559906 and batch: 500, loss is 4.7208366870880125 and perplexity is 112.26214166888153
At time: 80.2614049911499 and batch: 550, loss is 4.763267364501953 and perplexity is 117.1280012428185
At time: 80.68827366828918 and batch: 600, loss is 4.783587245941162 and perplexity is 119.5323738955014
At time: 81.11483693122864 and batch: 650, loss is 4.745022859573364 and perplexity is 115.0104345625262
At time: 81.54116106033325 and batch: 700, loss is 4.721600170135498 and perplexity is 112.34788463840698
At time: 81.96821236610413 and batch: 750, loss is 4.667028284072876 and perplexity is 106.38113771451472
At time: 82.39397883415222 and batch: 800, loss is 4.708011236190796 and perplexity is 110.83152285935276
At time: 82.81972026824951 and batch: 850, loss is 4.684507026672363 and perplexity is 108.25689139159186
At time: 83.24479651451111 and batch: 900, loss is 4.78640606880188 and perplexity is 119.8697898180731
At time: 83.67125248908997 and batch: 950, loss is 4.715834035873413 and perplexity is 111.70193575523847
At time: 84.09752249717712 and batch: 1000, loss is 4.694419422149658 and perplexity is 109.33531255065718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060558784298781 and perplexity of 157.6786000371314
Finished 9 epochs...
Completing Train Step...
At time: 85.39747405052185 and batch: 50, loss is 4.8570179271697995 and perplexity is 128.64001566052434
At time: 85.82353949546814 and batch: 100, loss is 4.7973770236969 and perplexity is 121.19211620389929
At time: 86.25076127052307 and batch: 150, loss is 4.822926187515259 and perplexity is 124.32836712893548
At time: 86.67748880386353 and batch: 200, loss is 4.8354221439361575 and perplexity is 125.89171641585023
At time: 87.10425615310669 and batch: 250, loss is 4.8017471981048585 and perplexity is 121.72290586550729
At time: 87.53994226455688 and batch: 300, loss is 4.722474889755249 and perplexity is 112.44620053048884
At time: 87.97658276557922 and batch: 350, loss is 4.785678129196167 and perplexity is 119.78256360209643
At time: 88.40365529060364 and batch: 400, loss is 4.70854022026062 and perplexity is 110.89016647878243
At time: 88.83000159263611 and batch: 450, loss is 4.755424184799194 and perplexity is 116.21293847259062
At time: 89.2565336227417 and batch: 500, loss is 4.71146788597107 and perplexity is 111.21529151328936
At time: 89.68079042434692 and batch: 550, loss is 4.7539326858520505 and perplexity is 116.0397361948301
At time: 90.10588192939758 and batch: 600, loss is 4.775597887039185 and perplexity is 118.5811915879434
At time: 90.53189969062805 and batch: 650, loss is 4.737975463867188 and perplexity is 114.20275986423894
At time: 90.95722436904907 and batch: 700, loss is 4.714803085327149 and perplexity is 111.58683592486393
At time: 91.39498448371887 and batch: 750, loss is 4.659802350997925 and perplexity is 105.61520535364572
At time: 91.8433108329773 and batch: 800, loss is 4.701787242889404 and perplexity is 110.14385045762904
At time: 92.2890477180481 and batch: 850, loss is 4.679941806793213 and perplexity is 107.7638012677573
At time: 92.7223973274231 and batch: 900, loss is 4.782638320922851 and perplexity is 119.41900043537755
At time: 93.14821720123291 and batch: 950, loss is 4.713254566192627 and perplexity is 111.41417529292178
At time: 93.57494950294495 and batch: 1000, loss is 4.691104984283447 and perplexity is 108.97352733912668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.058372683641387 and perplexity of 157.3342752473863
Finished 10 epochs...
Completing Train Step...
At time: 94.8402726650238 and batch: 50, loss is 4.845881338119507 and perplexity is 127.21535233995225
At time: 95.2665364742279 and batch: 100, loss is 4.786840763092041 and perplexity is 119.92190785815632
At time: 95.69333696365356 and batch: 150, loss is 4.811882152557373 and perplexity is 122.96283466976605
At time: 96.12017273902893 and batch: 200, loss is 4.824253406524658 and perplexity is 124.49348765250694
At time: 96.56039261817932 and batch: 250, loss is 4.7914323806762695 and perplexity is 120.47380948998818
At time: 96.98616075515747 and batch: 300, loss is 4.7132523727416995 and perplexity is 111.41393091166367
At time: 97.41215109825134 and batch: 350, loss is 4.775981044769287 and perplexity is 118.62663559370012
At time: 97.83817601203918 and batch: 400, loss is 4.699757823944092 and perplexity is 109.92054910343967
At time: 98.26373624801636 and batch: 450, loss is 4.746812782287598 and perplexity is 115.21647869825212
At time: 98.68949174880981 and batch: 500, loss is 4.70309422492981 and perplexity is 110.28790060701185
At time: 99.1159315109253 and batch: 550, loss is 4.74591362953186 and perplexity is 115.11292804482844
At time: 99.5417845249176 and batch: 600, loss is 4.768832998275757 and perplexity is 117.7817102674513
At time: 99.98451352119446 and batch: 650, loss is 4.731351175308228 and perplexity is 113.44874798205424
At time: 100.42176914215088 and batch: 700, loss is 4.708439607620239 and perplexity is 110.87901008758712
At time: 100.84727311134338 and batch: 750, loss is 4.6532644844055175 and perplexity is 104.92695951217374
At time: 101.2744619846344 and batch: 800, loss is 4.696359281539917 and perplexity is 109.54761353384833
At time: 101.70270037651062 and batch: 850, loss is 4.675097389221191 and perplexity is 107.24301089678869
At time: 102.12975525856018 and batch: 900, loss is 4.778468656539917 and perplexity is 118.92209995666046
At time: 102.55666089057922 and batch: 950, loss is 4.710099620819092 and perplexity is 111.06322356390709
At time: 102.98040413856506 and batch: 1000, loss is 4.687486038208008 and perplexity is 108.57987076063665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057030282369474 and perplexity of 157.12321121418742
Finished 11 epochs...
Completing Train Step...
At time: 104.2237777709961 and batch: 50, loss is 4.836262264251709 and perplexity is 125.9975250441366
At time: 104.6600661277771 and batch: 100, loss is 4.777564620971679 and perplexity is 118.81463873015446
At time: 105.08260798454285 and batch: 150, loss is 4.802583360671997 and perplexity is 121.82472856718861
At time: 105.50627493858337 and batch: 200, loss is 4.814739227294922 and perplexity is 123.31465102184495
At time: 105.92950057983398 and batch: 250, loss is 4.7822557640075685 and perplexity is 119.37332460830365
At time: 106.35511493682861 and batch: 300, loss is 4.705149631500245 and perplexity is 110.5148202086902
At time: 106.78283286094666 and batch: 350, loss is 4.767590799331665 and perplexity is 117.6354927857263
At time: 107.22364354133606 and batch: 400, loss is 4.691524400711059 and perplexity is 109.01924221278193
At time: 107.64939475059509 and batch: 450, loss is 4.7387950992584225 and perplexity is 114.29640285931433
At time: 108.08763122558594 and batch: 500, loss is 4.695862741470337 and perplexity is 109.49323225656082
At time: 108.52625799179077 and batch: 550, loss is 4.739004459381103 and perplexity is 114.32033447331435
At time: 108.95557427406311 and batch: 600, loss is 4.762429552078247 and perplexity is 117.02991104454452
At time: 109.3827428817749 and batch: 650, loss is 4.72507137298584 and perplexity is 112.7385445734706
At time: 109.81069803237915 and batch: 700, loss is 4.702677364349365 and perplexity is 110.24193550993293
At time: 110.23778748512268 and batch: 750, loss is 4.647537889480591 and perplexity is 104.32780252042724
At time: 110.66395974159241 and batch: 800, loss is 4.69061164855957 and perplexity is 108.91978006394896
At time: 111.0895094871521 and batch: 850, loss is 4.6701461029052735 and perplexity is 106.71333242144995
At time: 111.51547813415527 and batch: 900, loss is 4.774366579055786 and perplexity is 118.4352714747964
At time: 111.94101691246033 and batch: 950, loss is 4.7062530612945555 and perplexity is 110.63683285783777
At time: 112.3676381111145 and batch: 1000, loss is 4.683004903793335 and perplexity is 108.09439831106081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.055681554282584 and perplexity of 156.9114375707536
Finished 12 epochs...
Completing Train Step...
At time: 113.64066529273987 and batch: 50, loss is 4.827301139831543 and perplexity is 124.87348937913245
At time: 114.06618785858154 and batch: 100, loss is 4.768903360366822 and perplexity is 117.78999792644
At time: 114.49164867401123 and batch: 150, loss is 4.79369481086731 and perplexity is 120.74668163450359
At time: 114.9162859916687 and batch: 200, loss is 4.805775814056396 and perplexity is 122.21426979947664
At time: 115.34279370307922 and batch: 250, loss is 4.774359064102173 and perplexity is 118.43438144256939
At time: 115.76821660995483 and batch: 300, loss is 4.697438554763794 and perplexity is 109.66590916504582
At time: 116.19407057762146 and batch: 350, loss is 4.760062885284424 and perplexity is 116.75326773058958
At time: 116.62015104293823 and batch: 400, loss is 4.6838119602203365 and perplexity is 108.18167180251967
At time: 117.046875 and batch: 450, loss is 4.732058067321777 and perplexity is 113.52897234759092
At time: 117.47339224815369 and batch: 500, loss is 4.688501510620117 and perplexity is 108.69018662578364
At time: 117.89816117286682 and batch: 550, loss is 4.732251129150391 and perplexity is 113.55089257450445
At time: 118.3351821899414 and batch: 600, loss is 4.755753488540649 and perplexity is 116.2512141298478
At time: 118.7612886428833 and batch: 650, loss is 4.7191709041595455 and perplexity is 112.07529297766254
At time: 119.18677067756653 and batch: 700, loss is 4.696883916854858 and perplexity is 109.6051011592764
At time: 119.61216735839844 and batch: 750, loss is 4.641900157928466 and perplexity is 103.74128524336216
At time: 120.03745126724243 and batch: 800, loss is 4.684834518432617 and perplexity is 108.29235043746911
At time: 120.46298241615295 and batch: 850, loss is 4.66471420288086 and perplexity is 106.135247738956
At time: 120.88815784454346 and batch: 900, loss is 4.769059658050537 and perplexity is 117.80840966909963
At time: 121.31422019004822 and batch: 950, loss is 4.702463426589966 and perplexity is 110.21835311992986
At time: 121.74032235145569 and batch: 1000, loss is 4.678584289550781 and perplexity is 107.61760930093504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.054226014672256 and perplexity of 156.68321289337783
Finished 13 epochs...
Completing Train Step...
At time: 122.99644351005554 and batch: 50, loss is 4.819619235992431 and perplexity is 123.91789832022515
At time: 123.41955375671387 and batch: 100, loss is 4.761364192962646 and perplexity is 116.90529855232866
At time: 123.84615159034729 and batch: 150, loss is 4.785860471725464 and perplexity is 119.80440704914338
At time: 124.2789397239685 and batch: 200, loss is 4.7978910446167 and perplexity is 121.25442750022587
At time: 124.70420980453491 and batch: 250, loss is 4.766686735153198 and perplexity is 117.52919080972701
At time: 125.12924194335938 and batch: 300, loss is 4.690115604400635 and perplexity is 108.86576444143034
At time: 125.55477380752563 and batch: 350, loss is 4.753093700408936 and perplexity is 115.94242137390717
At time: 125.98030805587769 and batch: 400, loss is 4.677398681640625 and perplexity is 107.49009261940603
At time: 126.40587782859802 and batch: 450, loss is 4.725626707077026 and perplexity is 112.80116951793683
At time: 126.83163714408875 and batch: 500, loss is 4.682177429199219 and perplexity is 108.00498993937178
At time: 127.2573561668396 and batch: 550, loss is 4.726084156036377 and perplexity is 112.85278209970721
At time: 127.68297743797302 and batch: 600, loss is 4.750204849243164 and perplexity is 115.60796430570424
At time: 128.10895943641663 and batch: 650, loss is 4.713780612945556 and perplexity is 111.47279977632259
At time: 128.53491139411926 and batch: 700, loss is 4.692010221481323 and perplexity is 109.072218892551
At time: 128.97338485717773 and batch: 750, loss is 4.636295557022095 and perplexity is 103.16148303974343
At time: 129.39930152893066 and batch: 800, loss is 4.679660081863403 and perplexity is 107.73344579455666
At time: 129.82776951789856 and batch: 850, loss is 4.660647039413452 and perplexity is 105.70445498286878
At time: 130.25569653511047 and batch: 900, loss is 4.7646723556518555 and perplexity is 117.29268070747587
At time: 130.6846342086792 and batch: 950, loss is 4.698152503967285 and perplexity is 109.74423300984067
At time: 131.11390376091003 and batch: 1000, loss is 4.674232778549194 and perplexity is 107.15032751836304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052836348370808 and perplexity of 156.46562673351062
Finished 14 epochs...
Completing Train Step...
At time: 132.38213777542114 and batch: 50, loss is 4.811365690231323 and perplexity is 122.89934539447547
At time: 132.82335424423218 and batch: 100, loss is 4.754190120697022 and perplexity is 116.06961271179117
At time: 133.25454258918762 and batch: 150, loss is 4.778180284500122 and perplexity is 118.88781109233176
At time: 133.68848991394043 and batch: 200, loss is 4.790584812164306 and perplexity is 120.37174294285413
At time: 134.13512253761292 and batch: 250, loss is 4.7597993469238284 and perplexity is 116.72250281986281
At time: 134.57514190673828 and batch: 300, loss is 4.683360872268676 and perplexity is 108.13288335854602
At time: 135.0018892288208 and batch: 350, loss is 4.746022539138794 and perplexity is 115.1254656312942
At time: 135.42849564552307 and batch: 400, loss is 4.67077377319336 and perplexity is 106.78033423487759
At time: 135.85500621795654 and batch: 450, loss is 4.718992586135864 and perplexity is 112.0553097146562
At time: 136.28306341171265 and batch: 500, loss is 4.676523923873901 and perplexity is 107.39610593982356
At time: 136.7120325565338 and batch: 550, loss is 4.719628458023071 and perplexity is 112.12658519455138
At time: 137.13997673988342 and batch: 600, loss is 4.744507741928101 and perplexity is 114.95120591446417
At time: 137.5679235458374 and batch: 650, loss is 4.708274173736572 and perplexity is 110.86066845953643
At time: 137.99500250816345 and batch: 700, loss is 4.686707563400269 and perplexity is 108.49537695903699
At time: 138.4220633506775 and batch: 750, loss is 4.630554103851319 and perplexity is 102.57088328864037
At time: 138.84890246391296 and batch: 800, loss is 4.67453824043274 and perplexity is 107.18306285867415
At time: 139.2761640548706 and batch: 850, loss is 4.656162433624267 and perplexity is 105.23147353248966
At time: 139.70363759994507 and batch: 900, loss is 4.759857902526855 and perplexity is 116.72933777651278
At time: 140.1618092060089 and batch: 950, loss is 4.694227161407471 and perplexity is 109.31429368293523
At time: 140.59813332557678 and batch: 1000, loss is 4.670354499816894 and perplexity is 106.73557346775752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.052992285751715 and perplexity of 156.49002747599
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 141.8801465034485 and batch: 50, loss is 4.804511022567749 and perplexity is 122.05979194297524
At time: 142.30603790283203 and batch: 100, loss is 4.743584547042847 and perplexity is 114.84513251985162
At time: 142.73216891288757 and batch: 150, loss is 4.768124418258667 and perplexity is 117.69828206244726
At time: 143.15854048728943 and batch: 200, loss is 4.77717583656311 and perplexity is 118.76845442955612
At time: 143.5844190120697 and batch: 250, loss is 4.743637065887452 and perplexity is 114.8511642119074
At time: 144.01157569885254 and batch: 300, loss is 4.667226896286011 and perplexity is 106.40226840604932
At time: 144.4456250667572 and batch: 350, loss is 4.728101949691773 and perplexity is 113.08072562157872
At time: 144.88276600837708 and batch: 400, loss is 4.648039817810059 and perplexity is 104.38018074402082
At time: 145.3112895488739 and batch: 450, loss is 4.695619506835937 and perplexity is 109.46660294895993
At time: 145.7366030216217 and batch: 500, loss is 4.6505908203125 and perplexity is 104.6467947682405
At time: 146.16200470924377 and batch: 550, loss is 4.692197074890137 and perplexity is 109.09260131266103
At time: 146.58703136444092 and batch: 600, loss is 4.716228122711182 and perplexity is 111.7459646929155
At time: 147.01194667816162 and batch: 650, loss is 4.677231569290161 and perplexity is 107.47213119820623
At time: 147.43651461601257 and batch: 700, loss is 4.655664987564087 and perplexity is 105.17913956831529
At time: 147.86292123794556 and batch: 750, loss is 4.594277467727661 and perplexity is 98.91663925745496
At time: 148.2897708415985 and batch: 800, loss is 4.635582151412964 and perplexity is 103.08791330474772
At time: 148.7160403728485 and batch: 850, loss is 4.613460798263549 and perplexity is 100.83250745734244
At time: 149.14491486549377 and batch: 900, loss is 4.713525810241699 and perplexity is 111.44439982387905
At time: 149.57209658622742 and batch: 950, loss is 4.648157148361206 and perplexity is 104.3924284466572
At time: 149.999037027359 and batch: 1000, loss is 4.623659009933472 and perplexity is 101.86608004972122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.040378105349657 and perplexity of 154.52843200812444
Finished 16 epochs...
Completing Train Step...
At time: 151.27379083633423 and batch: 50, loss is 4.792806100845337 and perplexity is 120.63942051748953
At time: 151.70002675056458 and batch: 100, loss is 4.732939567565918 and perplexity is 113.62909228582427
At time: 152.12802934646606 and batch: 150, loss is 4.7586166954040525 and perplexity is 116.58454237015864
At time: 152.5536413192749 and batch: 200, loss is 4.768620624542236 and perplexity is 117.75669918184416
At time: 152.9774088859558 and batch: 250, loss is 4.737169237136841 and perplexity is 114.11072365256928
At time: 153.40176963806152 and batch: 300, loss is 4.660365171432495 and perplexity is 105.67466448025628
At time: 153.8270227909088 and batch: 350, loss is 4.72106050491333 and perplexity is 112.28727074935854
At time: 154.252911567688 and batch: 400, loss is 4.643093204498291 and perplexity is 103.86512728784477
At time: 154.6779420375824 and batch: 450, loss is 4.690746002197265 and perplexity is 108.93441481571156
At time: 155.10399532318115 and batch: 500, loss is 4.64580394744873 and perplexity is 104.14706090157442
At time: 155.5299937725067 and batch: 550, loss is 4.688223180770874 and perplexity is 108.65993911211405
At time: 155.95605516433716 and batch: 600, loss is 4.713743810653686 and perplexity is 111.4686973972986
At time: 156.38412189483643 and batch: 650, loss is 4.674803028106689 and perplexity is 107.21144737034386
At time: 156.81040978431702 and batch: 700, loss is 4.653513603210449 and perplexity is 104.95310204709583
At time: 157.23836183547974 and batch: 750, loss is 4.593157749176026 and perplexity is 98.80594244761447
At time: 157.66434836387634 and batch: 800, loss is 4.635215826034546 and perplexity is 103.0501565019558
At time: 158.09116387367249 and batch: 850, loss is 4.613953533172608 and perplexity is 100.88220339619107
At time: 158.5183973312378 and batch: 900, loss is 4.715535125732422 and perplexity is 111.66855190350738
At time: 158.94512248039246 and batch: 950, loss is 4.651244640350342 and perplexity is 104.71523731167092
At time: 159.37171578407288 and batch: 1000, loss is 4.626282701492309 and perplexity is 102.13369614160318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.039005186499619 and perplexity of 154.31642258012525
Finished 17 epochs...
Completing Train Step...
At time: 160.640930891037 and batch: 50, loss is 4.78889368057251 and perplexity is 120.16835051575733
At time: 161.0796959400177 and batch: 100, loss is 4.729060659408569 and perplexity is 113.18918919624835
At time: 161.50529885292053 and batch: 150, loss is 4.75466757774353 and perplexity is 116.12504419828977
At time: 161.94402146339417 and batch: 200, loss is 4.765023717880249 and perplexity is 117.33390016619984
At time: 162.3705747127533 and batch: 250, loss is 4.734216690063477 and perplexity is 113.77430326232506
At time: 162.79816818237305 and batch: 300, loss is 4.657164087295532 and perplexity is 105.33693183181518
At time: 163.22414207458496 and batch: 350, loss is 4.717318153381347 and perplexity is 111.86783763225178
At time: 163.6657373905182 and batch: 400, loss is 4.640578556060791 and perplexity is 103.6042711260289
At time: 164.09282040596008 and batch: 450, loss is 4.6881655502319335 and perplexity is 108.65367716170333
At time: 164.51939058303833 and batch: 500, loss is 4.643030309677124 and perplexity is 103.85859491466697
At time: 164.94569492340088 and batch: 550, loss is 4.686159219741821 and perplexity is 108.43590051536626
At time: 165.37201404571533 and batch: 600, loss is 4.712616472244263 and perplexity is 111.3431052589961
At time: 165.82546639442444 and batch: 650, loss is 4.673676834106446 and perplexity is 107.09077444487453
At time: 166.2732765674591 and batch: 700, loss is 4.652527408599854 and perplexity is 104.84964888435219
At time: 166.7050302028656 and batch: 750, loss is 4.59277904510498 and perplexity is 98.7685313192865
At time: 167.1303675174713 and batch: 800, loss is 4.634988374710083 and perplexity is 103.02672027277504
At time: 167.5546100139618 and batch: 850, loss is 4.614210338592529 and perplexity is 100.908113819623
At time: 167.97940373420715 and batch: 900, loss is 4.716617259979248 and perplexity is 111.78945767415468
At time: 168.404944896698 and batch: 950, loss is 4.65281460762024 and perplexity is 104.87976592538493
At time: 168.83011603355408 and batch: 1000, loss is 4.627164602279663 and perplexity is 102.22380765751387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0382802079363564 and perplexity of 154.20458702588337
Finished 18 epochs...
Completing Train Step...
At time: 170.11168217658997 and batch: 50, loss is 4.78625657081604 and perplexity is 119.8518708653892
At time: 170.5392165184021 and batch: 100, loss is 4.726287498474121 and perplexity is 112.8757321928104
At time: 170.9666566848755 and batch: 150, loss is 4.751885223388672 and perplexity is 115.80239225003392
At time: 171.39385437965393 and batch: 200, loss is 4.762489109039307 and perplexity is 117.03688119795794
At time: 171.82012462615967 and batch: 250, loss is 4.731986627578736 and perplexity is 113.52086215667711
At time: 172.247656583786 and batch: 300, loss is 4.655047941207886 and perplexity is 105.1142591826576
At time: 172.6755359172821 and batch: 350, loss is 4.714640321731568 and perplexity is 111.56867512822767
At time: 173.11590147018433 and batch: 400, loss is 4.6386263561248775 and perplexity is 103.40221216849028
At time: 173.5429666042328 and batch: 450, loss is 4.686188516616821 and perplexity is 108.43907739492523
At time: 173.96838331222534 and batch: 500, loss is 4.640831890106202 and perplexity is 103.63052094000048
At time: 174.3943226337433 and batch: 550, loss is 4.684659185409546 and perplexity is 108.27336487673797
At time: 174.820392370224 and batch: 600, loss is 4.711757316589355 and perplexity is 111.24748528258138
At time: 175.24617457389832 and batch: 650, loss is 4.673127326965332 and perplexity is 107.03194346507183
At time: 175.67438435554504 and batch: 700, loss is 4.651764526367187 and perplexity is 104.76969145304673
At time: 176.10031628608704 and batch: 750, loss is 4.59251669883728 and perplexity is 98.7426231623314
At time: 176.52586555480957 and batch: 800, loss is 4.634709320068359 and perplexity is 102.99797419931048
At time: 176.9529595375061 and batch: 850, loss is 4.614164628982544 and perplexity is 100.90350145451148
At time: 177.37840008735657 and batch: 900, loss is 4.716921157836914 and perplexity is 111.8234354134711
At time: 177.80477356910706 and batch: 950, loss is 4.653588981628418 and perplexity is 104.96101354406338
At time: 178.23145294189453 and batch: 1000, loss is 4.627460479736328 and perplexity is 102.25405785268892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.037684370831745 and perplexity of 154.1127335787956
Finished 19 epochs...
Completing Train Step...
At time: 179.51760745048523 and batch: 50, loss is 4.784182577133179 and perplexity is 119.60355643262714
At time: 179.95010662078857 and batch: 100, loss is 4.723979406356811 and perplexity is 112.61550503475218
At time: 180.38724279403687 and batch: 150, loss is 4.749479198455811 and perplexity is 115.52410372580931
At time: 180.81215381622314 and batch: 200, loss is 4.760333194732666 and perplexity is 116.78483150777166
At time: 181.23619842529297 and batch: 250, loss is 4.730171136856079 and perplexity is 113.31495305419277
At time: 181.6601526737213 and batch: 300, loss is 4.65297459602356 and perplexity is 104.89654681401392
At time: 182.0847098827362 and batch: 350, loss is 4.71249716758728 and perplexity is 111.32982230039093
At time: 182.50951099395752 and batch: 400, loss is 4.637102975845337 and perplexity is 103.24481119881835
At time: 182.93427419662476 and batch: 450, loss is 4.684551477432251 and perplexity is 108.2617035996297
At time: 183.3586974143982 and batch: 500, loss is 4.63907299041748 and perplexity is 103.44840545736125
At time: 183.79662466049194 and batch: 550, loss is 4.683197994232177 and perplexity is 108.11527232108814
At time: 184.22309470176697 and batch: 600, loss is 4.710660648345947 and perplexity is 111.12555057149643
At time: 184.64932322502136 and batch: 650, loss is 4.672101926803589 and perplexity is 106.92224914283042
At time: 185.07416033744812 and batch: 700, loss is 4.651216011047364 and perplexity is 104.71223943032933
At time: 185.50116181373596 and batch: 750, loss is 4.591934976577758 and perplexity is 98.68519908452579
At time: 185.92814302444458 and batch: 800, loss is 4.634440078735351 and perplexity is 102.9702466203126
At time: 186.35572409629822 and batch: 850, loss is 4.613805780410766 and perplexity is 100.86729887313797
At time: 186.78111720085144 and batch: 900, loss is 4.717089538574219 and perplexity is 111.84226591127599
At time: 187.20691537857056 and batch: 950, loss is 4.653927536010742 and perplexity is 104.99655457111767
At time: 187.63268113136292 and batch: 1000, loss is 4.627231092453003 and perplexity is 102.23060476217229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.037352026962653 and perplexity of 154.06152366676312
Finished 20 epochs...
Completing Train Step...
At time: 188.88525938987732 and batch: 50, loss is 4.782217636108398 and perplexity is 119.36877324098694
At time: 189.32391953468323 and batch: 100, loss is 4.7217902565002445 and perplexity is 112.36924246923674
At time: 189.7496931552887 and batch: 150, loss is 4.747416543960571 and perplexity is 115.28606299623739
At time: 190.17503476142883 and batch: 200, loss is 4.758540687561035 and perplexity is 116.575681367321
At time: 190.5999312400818 and batch: 250, loss is 4.728598556518555 and perplexity is 113.13689622809878
At time: 191.02581524848938 and batch: 300, loss is 4.651256952285767 and perplexity is 104.7165265668473
At time: 191.4516317844391 and batch: 350, loss is 4.710697660446167 and perplexity is 111.12966363762726
At time: 191.87747049331665 and batch: 400, loss is 4.635634136199951 and perplexity is 103.09327244725755
At time: 192.30459141731262 and batch: 450, loss is 4.682854042053223 and perplexity is 108.07809223204792
At time: 192.73041486740112 and batch: 500, loss is 4.637233638763428 and perplexity is 103.2583023485047
At time: 193.1566503047943 and batch: 550, loss is 4.682141532897949 and perplexity is 108.00111302929811
At time: 193.5834743976593 and batch: 600, loss is 4.710089130401611 and perplexity is 111.06205847043634
At time: 194.0116560459137 and batch: 650, loss is 4.671458892822265 and perplexity is 106.85351660431934
At time: 194.43825721740723 and batch: 700, loss is 4.650571613311768 and perplexity is 104.64478483647922
At time: 194.88469743728638 and batch: 750, loss is 4.591402502059936 and perplexity is 98.63266571830773
At time: 195.30948209762573 and batch: 800, loss is 4.634092931747436 and perplexity is 102.93450701316091
At time: 195.7351429462433 and batch: 850, loss is 4.613379344940186 and perplexity is 100.82429464899178
At time: 196.16112804412842 and batch: 900, loss is 4.717003231048584 and perplexity is 111.83261349858772
At time: 196.58754062652588 and batch: 950, loss is 4.65408579826355 and perplexity is 105.01317287737176
At time: 197.01315903663635 and batch: 1000, loss is 4.626992034912109 and perplexity is 102.20616868612429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.037052433665206 and perplexity of 154.01537478017184
Finished 21 epochs...
Completing Train Step...
At time: 198.268394947052 and batch: 50, loss is 4.78043417930603 and perplexity is 119.15607391676635
At time: 198.7071192264557 and batch: 100, loss is 4.719929780960083 and perplexity is 112.16037659732598
At time: 199.13330054283142 and batch: 150, loss is 4.745535650253296 and perplexity is 115.06942596529287
At time: 199.56273937225342 and batch: 200, loss is 4.756833267211914 and perplexity is 116.37680750569545
At time: 199.99107027053833 and batch: 250, loss is 4.727231378555298 and perplexity is 112.98232364493612
At time: 200.41905069351196 and batch: 300, loss is 4.649694395065308 and perplexity is 104.55302877275439
At time: 200.84598922729492 and batch: 350, loss is 4.709118194580078 and perplexity is 110.95427667252578
At time: 201.2733302116394 and batch: 400, loss is 4.634219055175781 and perplexity is 102.94749028481054
At time: 201.6996672153473 and batch: 450, loss is 4.6815056800842285 and perplexity is 107.93246204596691
At time: 202.13448929786682 and batch: 500, loss is 4.63571536064148 and perplexity is 103.10164648082089
At time: 202.56437158584595 and batch: 550, loss is 4.681058130264282 and perplexity is 107.88416769988095
At time: 202.9911675453186 and batch: 600, loss is 4.709312086105347 and perplexity is 110.97579185220293
At time: 203.41718459129333 and batch: 650, loss is 4.670622501373291 and perplexity is 106.76418260104468
At time: 203.84412717819214 and batch: 700, loss is 4.649974298477173 and perplexity is 104.58229761826249
At time: 204.27545928955078 and batch: 750, loss is 4.590946168899536 and perplexity is 98.5876666303107
At time: 204.71307563781738 and batch: 800, loss is 4.633631811141968 and perplexity is 102.8870527328775
At time: 205.14112448692322 and batch: 850, loss is 4.613030405044555 and perplexity is 100.78911916756127
At time: 205.58040142059326 and batch: 900, loss is 4.716784601211548 and perplexity is 111.80816622507228
At time: 206.00750088691711 and batch: 950, loss is 4.654077024459839 and perplexity is 105.01225151644778
At time: 206.44481897354126 and batch: 1000, loss is 4.626635265350342 and perplexity is 102.16971113997032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036771448647103 and perplexity of 153.97210484668736
Finished 22 epochs...
Completing Train Step...
At time: 207.7281892299652 and batch: 50, loss is 4.7787800312042235 and perplexity is 118.95913505120895
At time: 208.1543254852295 and batch: 100, loss is 4.718128662109375 and perplexity is 111.95854424532352
At time: 208.58046698570251 and batch: 150, loss is 4.743918285369873 and perplexity is 114.8834671387559
At time: 209.00534391403198 and batch: 200, loss is 4.755328865051269 and perplexity is 116.20186161251993
At time: 209.43060421943665 and batch: 250, loss is 4.726073007583619 and perplexity is 112.85152397281043
At time: 209.85489201545715 and batch: 300, loss is 4.6482600975036625 and perplexity is 104.40317611086662
At time: 210.2805244922638 and batch: 350, loss is 4.707643957138061 and perplexity is 110.79082423695709
At time: 210.70564723014832 and batch: 400, loss is 4.632960166931152 and perplexity is 102.81797244082755
At time: 211.13081192970276 and batch: 450, loss is 4.68029447555542 and perplexity is 107.80181289652417
At time: 211.5561192035675 and batch: 500, loss is 4.634201383590698 and perplexity is 102.94567105555124
At time: 211.98076367378235 and batch: 550, loss is 4.679988441467285 and perplexity is 107.76882691468994
At time: 212.40674352645874 and batch: 600, loss is 4.708531398773193 and perplexity is 110.88918826688773
At time: 212.8327832221985 and batch: 650, loss is 4.669999513626099 and perplexity is 106.69769053746622
At time: 213.26093411445618 and batch: 700, loss is 4.649304141998291 and perplexity is 104.51223459315453
At time: 213.68808841705322 and batch: 750, loss is 4.590417385101318 and perplexity is 98.53554885022577
At time: 214.11388993263245 and batch: 800, loss is 4.63313551902771 and perplexity is 102.83600336869317
At time: 214.54071855545044 and batch: 850, loss is 4.612552604675293 and perplexity is 100.74097359210818
At time: 214.96745133399963 and batch: 900, loss is 4.716382141113281 and perplexity is 111.76317695330698
At time: 215.39380884170532 and batch: 950, loss is 4.653949127197266 and perplexity is 104.99882159578557
At time: 215.81920266151428 and batch: 1000, loss is 4.626097660064698 and perplexity is 102.11479892509976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0365183760480186 and perplexity of 153.93314365614006
Finished 23 epochs...
Completing Train Step...
At time: 217.07192540168762 and batch: 50, loss is 4.777330350875855 and perplexity is 118.78680727352007
At time: 217.50978899002075 and batch: 100, loss is 4.716506729125976 and perplexity is 111.77710217285578
At time: 217.93493819236755 and batch: 150, loss is 4.742473726272583 and perplexity is 114.7176309900692
At time: 218.36154627799988 and batch: 200, loss is 4.753890914916992 and perplexity is 116.03488920777761
At time: 218.78627157211304 and batch: 250, loss is 4.724913005828857 and perplexity is 112.72069190435965
At time: 219.21375179290771 and batch: 300, loss is 4.64688268661499 and perplexity is 104.25946903384671
At time: 219.6396770477295 and batch: 350, loss is 4.706207904815674 and perplexity is 110.63183700082982
At time: 220.0669140815735 and batch: 400, loss is 4.631615743637085 and perplexity is 102.67983444240754
At time: 220.49268651008606 and batch: 450, loss is 4.678946542739868 and perplexity is 107.65660118514697
At time: 220.9182140827179 and batch: 500, loss is 4.632815494537353 and perplexity is 102.80309859457239
At time: 221.3444766998291 and batch: 550, loss is 4.679084157943725 and perplexity is 107.67141738970977
At time: 221.77114915847778 and batch: 600, loss is 4.707960596084595 and perplexity is 110.82591048137166
At time: 222.19790124893188 and batch: 650, loss is 4.6693172264099125 and perplexity is 106.62491689630153
At time: 222.62491917610168 and batch: 700, loss is 4.648654651641846 and perplexity is 104.44437694348568
At time: 223.04951095581055 and batch: 750, loss is 4.589773693084717 and perplexity is 98.4721427132808
At time: 223.4805736541748 and batch: 800, loss is 4.63250916481018 and perplexity is 102.77161177234845
At time: 223.90917325019836 and batch: 850, loss is 4.6119631576538085 and perplexity is 100.68160962295909
At time: 224.3347294330597 and batch: 900, loss is 4.7159665584564205 and perplexity is 111.71673976520287
At time: 224.76077151298523 and batch: 950, loss is 4.653794040679932 and perplexity is 104.98253895686162
At time: 225.1880486011505 and batch: 1000, loss is 4.625550451278687 and perplexity is 102.05893609565571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036390723251715 and perplexity of 153.91349491404372
Finished 24 epochs...
Completing Train Step...
At time: 226.45207834243774 and batch: 50, loss is 4.776044607162476 and perplexity is 118.63417602619582
At time: 226.90539455413818 and batch: 100, loss is 4.714788465499878 and perplexity is 111.58520455652223
At time: 227.34103894233704 and batch: 150, loss is 4.741080408096313 and perplexity is 114.55790413068065
At time: 227.7803978919983 and batch: 200, loss is 4.752554340362549 and perplexity is 115.87990352546709
At time: 228.20720267295837 and batch: 250, loss is 4.723829469680786 and perplexity is 112.59862110605008
At time: 228.63352298736572 and batch: 300, loss is 4.645622978210449 and perplexity is 104.12821519259208
At time: 229.0599615573883 and batch: 350, loss is 4.704909801483154 and perplexity is 110.48831861553661
At time: 229.48651671409607 and batch: 400, loss is 4.630455131530762 and perplexity is 102.56073211265074
At time: 229.91136860847473 and batch: 450, loss is 4.677849006652832 and perplexity is 107.53850899741182
At time: 230.33681082725525 and batch: 500, loss is 4.631633729934692 and perplexity is 102.68168128907696
At time: 230.76063990592957 and batch: 550, loss is 4.677981357574463 and perplexity is 107.5527427600935
At time: 231.18550086021423 and batch: 600, loss is 4.707088298797608 and perplexity is 110.72927949193307
At time: 231.61094045639038 and batch: 650, loss is 4.668725824356079 and perplexity is 106.56187734415688
At time: 232.0373978614807 and batch: 700, loss is 4.647934799194336 and perplexity is 104.36921945750983
At time: 232.46371364593506 and batch: 750, loss is 4.5892236328125 and perplexity is 98.41799199409915
At time: 232.8904972076416 and batch: 800, loss is 4.631965646743774 and perplexity is 102.71576872186463
At time: 233.31622290611267 and batch: 850, loss is 4.611477479934693 and perplexity is 100.6327226810507
At time: 233.74269771575928 and batch: 900, loss is 4.7156330013275145 and perplexity is 111.67948206436735
At time: 234.1683897972107 and batch: 950, loss is 4.653502674102783 and perplexity is 104.95195500961174
At time: 234.59492182731628 and batch: 1000, loss is 4.624990758895874 and perplexity is 102.00183046880785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03617114555545 and perplexity of 153.87970265355546
Finished 25 epochs...
Completing Train Step...
At time: 235.86447477340698 and batch: 50, loss is 4.774847745895386 and perplexity is 118.49227231242575
At time: 236.2917251586914 and batch: 100, loss is 4.713370323181152 and perplexity is 111.4270730088187
At time: 236.71789383888245 and batch: 150, loss is 4.739754896163941 and perplexity is 114.40615685543797
At time: 237.14427781105042 and batch: 200, loss is 4.751179914474488 and perplexity is 115.72074458729303
At time: 237.56998252868652 and batch: 250, loss is 4.722698249816895 and perplexity is 112.47131932593454
At time: 237.995543718338 and batch: 300, loss is 4.644432830810547 and perplexity is 104.00436098502514
At time: 238.4341526031494 and batch: 350, loss is 4.703613653182983 and perplexity is 110.34520213929932
At time: 238.87107634544373 and batch: 400, loss is 4.629471607208252 and perplexity is 102.4599107263536
At time: 239.31377911567688 and batch: 450, loss is 4.676642150878906 and perplexity is 107.4088038103788
At time: 239.74391508102417 and batch: 500, loss is 4.630416793823242 and perplexity is 102.55680024466989
At time: 240.17114353179932 and batch: 550, loss is 4.677180814743042 and perplexity is 107.46667663728242
At time: 240.5977029800415 and batch: 600, loss is 4.706525373458862 and perplexity is 110.66696471569483
At time: 241.02577543258667 and batch: 650, loss is 4.667929430007934 and perplexity is 106.47704585145664
At time: 241.45284795761108 and batch: 700, loss is 4.647442445755005 and perplexity is 104.31784556144468
At time: 241.88058757781982 and batch: 750, loss is 4.588425350189209 and perplexity is 98.33945797161437
At time: 242.30748915672302 and batch: 800, loss is 4.631289768218994 and perplexity is 102.64636879523114
At time: 242.73487997055054 and batch: 850, loss is 4.610734815597534 and perplexity is 100.55801409190053
At time: 243.16233158111572 and batch: 900, loss is 4.715167379379272 and perplexity is 111.62749375073905
At time: 243.5897138118744 and batch: 950, loss is 4.653148908615112 and perplexity is 104.91483319666101
At time: 244.01650404930115 and batch: 1000, loss is 4.624268484115601 and perplexity is 101.92818371891529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036052052567645 and perplexity of 153.8613777512093
Finished 26 epochs...
Completing Train Step...
At time: 245.2663857936859 and batch: 50, loss is 4.773575420379639 and perplexity is 118.34160743865029
At time: 245.70376348495483 and batch: 100, loss is 4.711971178054809 and perplexity is 111.27127937704127
At time: 246.12937998771667 and batch: 150, loss is 4.73834532737732 and perplexity is 114.24500711024481
At time: 246.555091381073 and batch: 200, loss is 4.749865045547486 and perplexity is 115.56868696585387
At time: 246.98015832901 and batch: 250, loss is 4.721655578613281 and perplexity is 112.35410983613968
At time: 247.40603065490723 and batch: 300, loss is 4.64323694229126 and perplexity is 103.88005770501451
At time: 247.83079051971436 and batch: 350, loss is 4.702440366744995 and perplexity is 110.21581153109845
At time: 248.2556130886078 and batch: 400, loss is 4.6282179737091065 and perplexity is 102.33154402915517
At time: 248.68161010742188 and batch: 450, loss is 4.67559588432312 and perplexity is 107.29648433945411
At time: 249.10753655433655 and batch: 500, loss is 4.6293912124633785 and perplexity is 102.45167381907743
At time: 249.5457694530487 and batch: 550, loss is 4.676240730285644 and perplexity is 107.36569635732025
At time: 249.9714171886444 and batch: 600, loss is 4.70600604057312 and perplexity is 110.60950664277756
At time: 250.39693665504456 and batch: 650, loss is 4.667210092544556 and perplexity is 106.40048046486287
At time: 250.82212090492249 and batch: 700, loss is 4.646692867279053 and perplexity is 104.23968044885696
At time: 251.24619889259338 and batch: 750, loss is 4.587870416641235 and perplexity is 98.28490124637494
At time: 251.67077660560608 and batch: 800, loss is 4.630522384643554 and perplexity is 102.56762987308093
At time: 252.0950403213501 and batch: 850, loss is 4.6101427745819095 and perplexity is 100.49849724305462
At time: 252.5196509361267 and batch: 900, loss is 4.714771499633789 and perplexity is 111.5833114329435
At time: 252.94447946548462 and batch: 950, loss is 4.652837533950805 and perplexity is 104.88217046113166
At time: 253.36921048164368 and batch: 1000, loss is 4.623684349060059 and perplexity is 101.86866127992145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035924771936928 and perplexity of 153.84179542425804
Finished 27 epochs...
Completing Train Step...
At time: 254.6166217327118 and batch: 50, loss is 4.772310266494751 and perplexity is 118.19198176396493
At time: 255.0678186416626 and batch: 100, loss is 4.710605764389038 and perplexity is 111.11945172893321
At time: 255.51552534103394 and batch: 150, loss is 4.737114200592041 and perplexity is 114.10444356543357
At time: 255.9472529888153 and batch: 200, loss is 4.7486606788635255 and perplexity is 115.42958367206361
At time: 256.39414715766907 and batch: 250, loss is 4.720533189773559 and perplexity is 112.22807558011496
At time: 256.83732652664185 and batch: 300, loss is 4.642102909088135 and perplexity is 103.76232104169493
At time: 257.27196192741394 and batch: 350, loss is 4.701211519241333 and perplexity is 110.08045628874315
At time: 257.7062876224518 and batch: 400, loss is 4.627391815185547 and perplexity is 102.24703686479015
At time: 258.1395802497864 and batch: 450, loss is 4.67432053565979 and perplexity is 107.15973113411653
At time: 258.5731039047241 and batch: 500, loss is 4.6280536937713626 and perplexity is 102.31473439025385
At time: 259.00503301620483 and batch: 550, loss is 4.675313367843628 and perplexity is 107.26617559599795
At time: 259.43365716934204 and batch: 600, loss is 4.705358839035034 and perplexity is 110.53794316044727
At time: 259.8563508987427 and batch: 650, loss is 4.666466579437256 and perplexity is 106.32139971544419
At time: 260.29214668273926 and batch: 700, loss is 4.646075048446655 and perplexity is 104.17529910124492
At time: 260.7193748950958 and batch: 750, loss is 4.587172927856446 and perplexity is 98.2163725318246
At time: 261.16157364845276 and batch: 800, loss is 4.629855947494507 and perplexity is 102.49929776630005
At time: 261.5978853702545 and batch: 850, loss is 4.60953052520752 and perplexity is 100.43698593304244
At time: 262.0355224609375 and batch: 900, loss is 4.714202909469605 and perplexity is 111.51988429330827
At time: 262.4735326766968 and batch: 950, loss is 4.6525389671325685 and perplexity is 104.85086079945295
At time: 262.9045286178589 and batch: 1000, loss is 4.623086957931519 and perplexity is 101.80782401902582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035791908822408 and perplexity of 153.82135688196888
Finished 28 epochs...
Completing Train Step...
At time: 264.18375968933105 and batch: 50, loss is 4.77113377571106 and perplexity is 118.05301175121244
At time: 264.6221776008606 and batch: 100, loss is 4.709269704818726 and perplexity is 110.97108865502499
At time: 265.0547091960907 and batch: 150, loss is 4.735890951156616 and perplexity is 113.96495070393878
At time: 265.4816663265228 and batch: 200, loss is 4.747605533599853 and perplexity is 115.30785292666799
At time: 265.9090414047241 and batch: 250, loss is 4.7195937538146975 and perplexity is 112.1226939976954
At time: 266.33540630340576 and batch: 300, loss is 4.640956764221191 and perplexity is 103.643462517606
At time: 266.76195454597473 and batch: 350, loss is 4.700037899017334 and perplexity is 109.95133942087963
At time: 267.1881172657013 and batch: 400, loss is 4.6262445068359375 and perplexity is 102.12979525467205
At time: 267.61472487449646 and batch: 450, loss is 4.6732499885559085 and perplexity is 107.04507297872678
At time: 268.04224610328674 and batch: 500, loss is 4.62700704574585 and perplexity is 102.20770289744455
At time: 268.468731880188 and batch: 550, loss is 4.674230966567993 and perplexity is 107.15013336415984
At time: 268.8974554538727 and batch: 600, loss is 4.704777317047119 and perplexity is 110.47368160256599
At time: 269.32552576065063 and batch: 650, loss is 4.665958890914917 and perplexity is 106.26743526085579
At time: 269.7535982131958 and batch: 700, loss is 4.645419988632202 and perplexity is 104.10708039525092
At time: 270.18436884880066 and batch: 750, loss is 4.58651252746582 and perplexity is 98.15153181381311
At time: 270.620863199234 and batch: 800, loss is 4.629112339019775 and perplexity is 102.42310675147996
At time: 271.05895948410034 and batch: 850, loss is 4.608882789611816 and perplexity is 100.37195038732129
At time: 271.52528190612793 and batch: 900, loss is 4.713692617416382 and perplexity is 111.4629910998839
At time: 271.9776437282562 and batch: 950, loss is 4.652112016677856 and perplexity is 104.80610423185597
At time: 272.4073963165283 and batch: 1000, loss is 4.622485361099243 and perplexity is 101.74659517398136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035640437428544 and perplexity of 153.79805911115335
Finished 29 epochs...
Completing Train Step...
At time: 273.67968368530273 and batch: 50, loss is 4.769974021911621 and perplexity is 117.91617868397067
At time: 274.11867237091064 and batch: 100, loss is 4.708112878799438 and perplexity is 110.84278863698783
At time: 274.546404838562 and batch: 150, loss is 4.734653310775757 and perplexity is 113.8239903260659
At time: 274.9739272594452 and batch: 200, loss is 4.746444225311279 and perplexity is 115.17402268545148
At time: 275.401814699173 and batch: 250, loss is 4.718552627563477 and perplexity is 112.00602086388693
At time: 275.82982420921326 and batch: 300, loss is 4.639891624450684 and perplexity is 103.53312651578875
At time: 276.2591931819916 and batch: 350, loss is 4.698912210464478 and perplexity is 109.82763809436852
At time: 276.687433719635 and batch: 400, loss is 4.625340881347657 and perplexity is 102.03754985249215
At time: 277.1163170337677 and batch: 450, loss is 4.672189016342163 and perplexity is 106.93156135766388
At time: 277.5443892478943 and batch: 500, loss is 4.625895376205444 and perplexity is 102.09414483855092
At time: 277.97291684150696 and batch: 550, loss is 4.673614587783813 and perplexity is 107.0841086454404
At time: 278.4025237560272 and batch: 600, loss is 4.704100704193115 and perplexity is 110.39895897156151
At time: 278.83062291145325 and batch: 650, loss is 4.6651326942443845 and perplexity is 106.17967371879637
At time: 279.2584192752838 and batch: 700, loss is 4.6448304557800295 and perplexity is 104.04572393881455
At time: 279.6852159500122 and batch: 750, loss is 4.585831127166748 and perplexity is 98.08467411169676
At time: 280.1113111972809 and batch: 800, loss is 4.628433961868286 and perplexity is 102.3536488180764
At time: 280.5368161201477 and batch: 850, loss is 4.608285322189331 and perplexity is 100.31199932801934
At time: 280.9614553451538 and batch: 900, loss is 4.713254699707031 and perplexity is 111.41419016831999
At time: 281.38580226898193 and batch: 950, loss is 4.651714973449707 and perplexity is 104.76449993780014
At time: 281.811888217926 and batch: 1000, loss is 4.621882791519165 and perplexity is 101.68530423873408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035531765077172 and perplexity of 153.78134642255333
Finished 30 epochs...
Completing Train Step...
At time: 283.11166429519653 and batch: 50, loss is 4.768827686309814 and perplexity is 117.78108461667942
At time: 283.55316734313965 and batch: 100, loss is 4.70690221786499 and perplexity is 110.70867680128057
At time: 283.98016905784607 and batch: 150, loss is 4.733490905761719 and perplexity is 113.6917576178638
At time: 284.4072527885437 and batch: 200, loss is 4.7454763031005855 and perplexity is 115.06259712513592
At time: 284.8362007141113 and batch: 250, loss is 4.717634401321411 and perplexity is 111.9032212001577
At time: 285.2630729675293 and batch: 300, loss is 4.638852043151855 and perplexity is 103.42555133991256
At time: 285.69003438949585 and batch: 350, loss is 4.697881546020508 and perplexity is 109.71450096604234
At time: 286.11547446250916 and batch: 400, loss is 4.624209289550781 and perplexity is 101.92215030301166
At time: 286.54270815849304 and batch: 450, loss is 4.671271905899048 and perplexity is 106.83353826191716
At time: 286.96940445899963 and batch: 500, loss is 4.624877796173096 and perplexity is 101.99030871508639
At time: 287.395391702652 and batch: 550, loss is 4.672694292068481 and perplexity is 106.98560493229868
At time: 287.82300782203674 and batch: 600, loss is 4.703454303741455 and perplexity is 110.32762009384487
At time: 288.2496404647827 and batch: 650, loss is 4.664423418045044 and perplexity is 106.10438970510957
At time: 288.67812395095825 and batch: 700, loss is 4.644246435165405 and perplexity is 103.9849768316788
At time: 289.1049268245697 and batch: 750, loss is 4.5852174472808835 and perplexity is 98.02449998579667
At time: 289.5319950580597 and batch: 800, loss is 4.627800321578979 and perplexity is 102.28881396558434
At time: 289.95912075042725 and batch: 850, loss is 4.607575359344483 and perplexity is 100.2408068106155
At time: 290.3886775970459 and batch: 900, loss is 4.712717571258545 and perplexity is 111.3543625062236
At time: 290.81594681739807 and batch: 950, loss is 4.651211385726929 and perplexity is 104.71175510378858
At time: 291.2437279224396 and batch: 1000, loss is 4.621226873397827 and perplexity is 101.6186288741905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03543388552782 and perplexity of 153.76629511028662
Finished 31 epochs...
Completing Train Step...
At time: 292.53734374046326 and batch: 50, loss is 4.767674465179443 and perplexity is 117.64533527069317
At time: 292.9638640880585 and batch: 100, loss is 4.705892190933228 and perplexity is 110.59691450709626
At time: 293.40207052230835 and batch: 150, loss is 4.73223918914795 and perplexity is 113.54953678466401
At time: 293.82789635658264 and batch: 200, loss is 4.74431453704834 and perplexity is 114.92899892586549
At time: 294.2539846897125 and batch: 250, loss is 4.716691045761109 and perplexity is 111.7977064510104
At time: 294.6807858943939 and batch: 300, loss is 4.63787260055542 and perplexity is 103.32430154164726
At time: 295.10741996765137 and batch: 350, loss is 4.696776838302612 and perplexity is 109.593365432059
At time: 295.5352551937103 and batch: 400, loss is 4.6233449554443355 and perplexity is 101.8340935730021
At time: 295.9626703262329 and batch: 450, loss is 4.670326414108277 and perplexity is 106.73257576563847
At time: 296.3888235092163 and batch: 500, loss is 4.623894844055176 and perplexity is 101.8901063802441
At time: 296.8156044483185 and batch: 550, loss is 4.671966590881348 and perplexity is 106.90777970077428
At time: 297.2412631511688 and batch: 600, loss is 4.702809762954712 and perplexity is 110.2565323547248
At time: 297.66777873039246 and batch: 650, loss is 4.663638124465942 and perplexity is 106.02109931714338
At time: 298.0937759876251 and batch: 700, loss is 4.643617162704468 and perplexity is 103.91956253327426
At time: 298.52040433883667 and batch: 750, loss is 4.584592704772949 and perplexity is 97.96327903949151
At time: 298.948611497879 and batch: 800, loss is 4.627530632019043 and perplexity is 102.26123145988362
At time: 299.3752393722534 and batch: 850, loss is 4.607175521850586 and perplexity is 100.20073478931626
At time: 299.8027973175049 and batch: 900, loss is 4.712364454269409 and perplexity is 111.31504833066839
At time: 300.2279443740845 and batch: 950, loss is 4.650834579467773 and perplexity is 104.672306491767
At time: 300.6527283191681 and batch: 1000, loss is 4.6205951118469235 and perplexity is 101.554450406491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035418626738758 and perplexity of 153.76394884072528
Finished 32 epochs...
Completing Train Step...
At time: 301.9069049358368 and batch: 50, loss is 4.766738252639771 and perplexity is 117.53524577420339
At time: 302.34490847587585 and batch: 100, loss is 4.704700002670288 and perplexity is 110.46514072888704
At time: 302.7701280117035 and batch: 150, loss is 4.73123462677002 and perplexity is 113.43552646680452
At time: 303.2194426059723 and batch: 200, loss is 4.74338586807251 and perplexity is 114.82231747368435
At time: 303.66771125793457 and batch: 250, loss is 4.7158252716064455 and perplexity is 111.70095677394278
At time: 304.1257598400116 and batch: 300, loss is 4.636781120300293 and perplexity is 103.2115866308796
At time: 304.56252098083496 and batch: 350, loss is 4.695829248428344 and perplexity is 109.48956505654816
At time: 304.997727394104 and batch: 400, loss is 4.622271356582641 and perplexity is 101.72482327279096
At time: 305.4343194961548 and batch: 450, loss is 4.669391164779663 and perplexity is 106.63280086029174
At time: 305.86048769950867 and batch: 500, loss is 4.622909631729126 and perplexity is 101.78977242477626
At time: 306.2877268791199 and batch: 550, loss is 4.671084260940551 and perplexity is 106.81349336778442
At time: 306.7139804363251 and batch: 600, loss is 4.7022272491455075 and perplexity is 110.19232510469362
At time: 307.13981342315674 and batch: 650, loss is 4.663093252182007 and perplexity is 105.96334709383483
At time: 307.5653989315033 and batch: 700, loss is 4.6429636096954345 and perplexity is 103.85166777931029
At time: 307.9902811050415 and batch: 750, loss is 4.583950128555298 and perplexity is 97.90035038657078
At time: 308.4148554801941 and batch: 800, loss is 4.6268188095092775 and perplexity is 102.18846551474552
At time: 308.84097123146057 and batch: 850, loss is 4.606582117080689 and perplexity is 100.14129283365938
At time: 309.26792573928833 and batch: 900, loss is 4.711838979721069 and perplexity is 111.25657047158228
At time: 309.693683385849 and batch: 950, loss is 4.650334415435791 and perplexity is 104.61996625935717
At time: 310.1200964450836 and batch: 1000, loss is 4.619977483749389 and perplexity is 101.49174689021032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035365779225419 and perplexity of 153.75582301310467
Finished 33 epochs...
Completing Train Step...
At time: 311.38475275039673 and batch: 50, loss is 4.765743789672851 and perplexity is 117.41841942434044
At time: 311.8224675655365 and batch: 100, loss is 4.703616819381714 and perplexity is 110.34555151469138
At time: 312.2468247413635 and batch: 150, loss is 4.730177574157715 and perplexity is 113.31568249907323
At time: 312.67135667800903 and batch: 200, loss is 4.7423221874237065 and perplexity is 114.70024812944574
At time: 313.09648036956787 and batch: 250, loss is 4.714887084960938 and perplexity is 111.59620957190326
At time: 313.5217559337616 and batch: 300, loss is 4.635907726287842 and perplexity is 103.12148160342412
At time: 313.94598627090454 and batch: 350, loss is 4.694855442047119 and perplexity is 109.38299531698641
At time: 314.37001729011536 and batch: 400, loss is 4.621485099792481 and perplexity is 101.64487287465533
At time: 314.7942614555359 and batch: 450, loss is 4.668435220718384 and perplexity is 106.53091457412576
At time: 315.23105216026306 and batch: 500, loss is 4.621962242126465 and perplexity is 101.69338351885688
At time: 315.6565260887146 and batch: 550, loss is 4.670212688446045 and perplexity is 106.72043822296598
At time: 316.08052468299866 and batch: 600, loss is 4.701678667068482 and perplexity is 110.13189214784944
At time: 316.5057044029236 and batch: 650, loss is 4.662391052246094 and perplexity is 105.88896575663792
At time: 316.93034052848816 and batch: 700, loss is 4.642329950332641 and perplexity is 103.78588204275539
At time: 317.35550832748413 and batch: 750, loss is 4.58323881149292 and perplexity is 97.83073695847281
At time: 317.78074645996094 and batch: 800, loss is 4.626117076873779 and perplexity is 102.11678168790428
At time: 318.20672130584717 and batch: 850, loss is 4.6058620166778566 and perplexity is 100.06920700598148
At time: 318.6320540904999 and batch: 900, loss is 4.711335515975952 and perplexity is 111.20057091999864
At time: 319.0577018260956 and batch: 950, loss is 4.649806814193726 and perplexity is 104.56478319382097
At time: 319.4831380844116 and batch: 1000, loss is 4.619329557418824 and perplexity is 101.42600901401525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035274598656631 and perplexity of 153.74180410884344
Finished 34 epochs...
Completing Train Step...
At time: 320.74425053596497 and batch: 50, loss is 4.764714517593384 and perplexity is 117.29762609887447
At time: 321.16869592666626 and batch: 100, loss is 4.702539186477662 and perplexity is 110.22670356629467
At time: 321.5939486026764 and batch: 150, loss is 4.7291755104064945 and perplexity is 113.20218983413574
At time: 322.0181179046631 and batch: 200, loss is 4.741384553909302 and perplexity is 114.5927517366748
At time: 322.4429771900177 and batch: 250, loss is 4.714118785858155 and perplexity is 111.5105032324829
At time: 322.86743092536926 and batch: 300, loss is 4.63493896484375 and perplexity is 103.02162986205687
At time: 323.2923402786255 and batch: 350, loss is 4.693872509002685 and perplexity is 109.27553197967502
At time: 323.7180757522583 and batch: 400, loss is 4.6204205417633055 and perplexity is 101.53672358492314
At time: 324.14405727386475 and batch: 450, loss is 4.667535877227783 and perplexity is 106.43514975874213
At time: 324.5716118812561 and batch: 500, loss is 4.621018772125244 and perplexity is 101.59748410840629
At time: 324.9980568885803 and batch: 550, loss is 4.669284296035767 and perplexity is 106.62140575570677
At time: 325.42572259902954 and batch: 600, loss is 4.701004104614258 and perplexity is 110.05762635966619
At time: 325.8639090061188 and batch: 650, loss is 4.66179536819458 and perplexity is 105.82590817156888
At time: 326.2895829677582 and batch: 700, loss is 4.641606492996216 and perplexity is 103.71082453869886
At time: 326.715913772583 and batch: 750, loss is 4.582619466781616 and perplexity is 97.77016476840367
At time: 327.1421937942505 and batch: 800, loss is 4.625475234985352 and perplexity is 102.05125988947239
At time: 327.5675880908966 and batch: 850, loss is 4.6052141189575195 and perplexity is 100.00439339344952
At time: 327.9935038089752 and batch: 900, loss is 4.710697841644287 and perplexity is 111.12968377411526
At time: 328.4186792373657 and batch: 950, loss is 4.64920259475708 and perplexity is 104.50162220289737
At time: 328.84442496299744 and batch: 1000, loss is 4.618701839447022 and perplexity is 101.36236206360624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0351741139481705 and perplexity of 153.72635618463244
Finished 35 epochs...
Completing Train Step...
At time: 330.0941734313965 and batch: 50, loss is 4.763731079101563 and perplexity is 117.18232780205372
At time: 330.5325367450714 and batch: 100, loss is 4.70145661354065 and perplexity is 110.10743968764922
At time: 330.9594359397888 and batch: 150, loss is 4.728122549057007 and perplexity is 113.08305503673888
At time: 331.38587069511414 and batch: 200, loss is 4.740398035049439 and perplexity is 114.47975956949881
At time: 331.81219935417175 and batch: 250, loss is 4.713257188796997 and perplexity is 111.41446748860797
At time: 332.2381446361542 and batch: 300, loss is 4.634040498733521 and perplexity is 102.92910998821468
At time: 332.66366958618164 and batch: 350, loss is 4.692914810180664 and perplexity is 109.17092902847895
At time: 333.08974528312683 and batch: 400, loss is 4.619543895721436 and perplexity is 101.44775082259599
At time: 333.517374753952 and batch: 450, loss is 4.666669101715088 and perplexity is 106.34293434804461
At time: 333.944463968277 and batch: 500, loss is 4.620061159133911 and perplexity is 101.50023960646783
At time: 334.3722577095032 and batch: 550, loss is 4.668481369018554 and perplexity is 106.53583090818833
At time: 334.7991781234741 and batch: 600, loss is 4.7005102252960205 and perplexity is 110.0032845944344
At time: 335.2250304222107 and batch: 650, loss is 4.661152210235596 and perplexity is 105.75786727932768
At time: 335.6514608860016 and batch: 700, loss is 4.640866403579712 and perplexity is 103.6340976509618
At time: 336.07798171043396 and batch: 750, loss is 4.581970643997193 and perplexity is 97.70674983261733
At time: 336.50379490852356 and batch: 800, loss is 4.624688577651978 and perplexity is 101.97101208540036
At time: 336.95102167129517 and batch: 850, loss is 4.6045833015441895 and perplexity is 99.9413287739088
At time: 337.377179145813 and batch: 900, loss is 4.710132999420166 and perplexity is 111.06693076081062
At time: 337.80329275131226 and batch: 950, loss is 4.648676414489746 and perplexity is 104.44664997530882
At time: 338.2302587032318 and batch: 1000, loss is 4.618009490966797 and perplexity is 101.29220827451644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035065069431212 and perplexity of 153.7095940823028
Finished 36 epochs...
Completing Train Step...
At time: 339.48762583732605 and batch: 50, loss is 4.762643089294434 and perplexity is 117.05490395432275
At time: 339.92761516571045 and batch: 100, loss is 4.7004170417785645 and perplexity is 109.99303457901784
At time: 340.35467052459717 and batch: 150, loss is 4.727020750045776 and perplexity is 112.95852885252327
At time: 340.780574798584 and batch: 200, loss is 4.739347476959228 and perplexity is 114.35955508387066
At time: 341.2066447734833 and batch: 250, loss is 4.712397480010987 and perplexity is 111.3187246533946
At time: 341.63336849212646 and batch: 300, loss is 4.633160161972046 and perplexity is 102.83853758182501
At time: 342.0582046508789 and batch: 350, loss is 4.691976995468139 and perplexity is 109.06859491777358
At time: 342.48413133621216 and batch: 400, loss is 4.618640899658203 and perplexity is 101.35618525087666
At time: 342.91311264038086 and batch: 450, loss is 4.665718975067139 and perplexity is 106.24194307714573
At time: 343.33944845199585 and batch: 500, loss is 4.61906436920166 and perplexity is 101.39911559757701
At time: 343.76162695884705 and batch: 550, loss is 4.667538948059082 and perplexity is 106.43547660363322
At time: 344.18337392807007 and batch: 600, loss is 4.699796991348267 and perplexity is 109.92485449032841
At time: 344.606064081192 and batch: 650, loss is 4.660472593307495 and perplexity is 105.6860168605908
At time: 345.02898621559143 and batch: 700, loss is 4.640116367340088 and perplexity is 103.55639746468319
At time: 345.45224118232727 and batch: 750, loss is 4.5813640689849855 and perplexity is 97.64750133079188
At time: 345.8755416870117 and batch: 800, loss is 4.624142904281616 and perplexity is 101.91538439821058
At time: 346.29910469055176 and batch: 850, loss is 4.603916835784912 and perplexity is 99.87474349121445
At time: 346.7225704193115 and batch: 900, loss is 4.709504976272583 and perplexity is 110.99720005590208
At time: 347.1461646556854 and batch: 950, loss is 4.6481743335723875 and perplexity is 104.39422246800106
At time: 347.5810754299164 and batch: 1000, loss is 4.6173188018798825 and perplexity is 101.22227100690584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0350494384765625 and perplexity of 153.70719147338608
Finished 37 epochs...
Completing Train Step...
At time: 348.83761978149414 and batch: 50, loss is 4.761694097518921 and perplexity is 116.94387250548536
At time: 349.25914478302 and batch: 100, loss is 4.699313049316406 and perplexity is 109.8716701030126
At time: 349.6818811893463 and batch: 150, loss is 4.725974674224854 and perplexity is 112.84042744900461
At time: 350.1036467552185 and batch: 200, loss is 4.738246850967407 and perplexity is 114.23375722602734
At time: 350.52589654922485 and batch: 250, loss is 4.711543550491333 and perplexity is 111.22370688333613
At time: 350.94776248931885 and batch: 300, loss is 4.6323082065582275 and perplexity is 102.75096104393293
At time: 351.3715100288391 and batch: 350, loss is 4.691099367141724 and perplexity is 108.9729152210987
At time: 351.7965042591095 and batch: 400, loss is 4.617785501480102 and perplexity is 101.26952242556956
At time: 352.21940541267395 and batch: 450, loss is 4.664853610992432 and perplexity is 106.15004488481162
At time: 352.6420693397522 and batch: 500, loss is 4.618121490478516 and perplexity is 101.30355358770706
At time: 353.0647563934326 and batch: 550, loss is 4.666728096008301 and perplexity is 106.3492081593524
At time: 353.4878041744232 and batch: 600, loss is 4.699252738952636 and perplexity is 109.865043902437
At time: 353.91170358657837 and batch: 650, loss is 4.65979528427124 and perplexity is 105.61445900249286
At time: 354.3356671333313 and batch: 700, loss is 4.639437170028686 and perplexity is 103.48608611828838
At time: 354.7597327232361 and batch: 750, loss is 4.5806184673309325 and perplexity is 97.57472232772966
At time: 355.1829528808594 and batch: 800, loss is 4.623382787704468 and perplexity is 101.83794625979799
At time: 355.60669898986816 and batch: 850, loss is 4.603237400054931 and perplexity is 99.80690806947807
At time: 356.02939653396606 and batch: 900, loss is 4.708914747238159 and perplexity is 110.93170561595079
At time: 356.4515345096588 and batch: 950, loss is 4.647735242843628 and perplexity is 104.34839399494442
At time: 356.8741466999054 and batch: 1000, loss is 4.616639747619629 and perplexity is 101.15355892480241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0349790991806405 and perplexity of 153.69638019799288
Finished 38 epochs...
Completing Train Step...
At time: 358.11422085762024 and batch: 50, loss is 4.760770168304443 and perplexity is 116.8358745441418
At time: 358.5585837364197 and batch: 100, loss is 4.698308048248291 and perplexity is 109.76130442530479
At time: 359.00568556785583 and batch: 150, loss is 4.725017499923706 and perplexity is 112.73247116645189
At time: 359.44067573547363 and batch: 200, loss is 4.737239112854004 and perplexity is 114.11869749980646
At time: 359.86569142341614 and batch: 250, loss is 4.710651035308838 and perplexity is 111.12448232258956
At time: 360.3128151893616 and batch: 300, loss is 4.631403675079346 and perplexity is 102.65806158675908
At time: 360.7419738769531 and batch: 350, loss is 4.690142097473145 and perplexity is 108.8686486682274
At time: 361.17519092559814 and batch: 400, loss is 4.616911792755127 and perplexity is 101.18108100190015
At time: 361.61139392852783 and batch: 450, loss is 4.663960418701172 and perplexity is 106.05527481325286
At time: 362.04066824913025 and batch: 500, loss is 4.617135763168335 and perplexity is 101.20374510837084
At time: 362.4663372039795 and batch: 550, loss is 4.665863695144654 and perplexity is 106.2573195319972
At time: 362.90928530693054 and batch: 600, loss is 4.698610906600952 and perplexity is 109.79455158748526
At time: 363.3398938179016 and batch: 650, loss is 4.659142179489136 and perplexity is 105.54550421406103
At time: 363.76454424858093 and batch: 700, loss is 4.638857746124268 and perplexity is 103.42614117466051
At time: 364.18951392173767 and batch: 750, loss is 4.5799358940124515 and perplexity is 97.50814315088037
At time: 364.62977504730225 and batch: 800, loss is 4.622781057357788 and perplexity is 101.77668571010418
At time: 365.06287240982056 and batch: 850, loss is 4.602551212310791 and perplexity is 99.73844528422867
At time: 365.488712310791 and batch: 900, loss is 4.7083802318573 and perplexity is 110.87242675721629
At time: 365.9139931201935 and batch: 950, loss is 4.647245054244995 and perplexity is 104.297256136544
At time: 366.339914560318 and batch: 1000, loss is 4.615942058563232 and perplexity is 101.08300980726041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035015199242569 and perplexity of 153.70192874698705
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 367.6090168952942 and batch: 50, loss is 4.76064884185791 and perplexity is 116.82170012253928
At time: 368.05140137672424 and batch: 100, loss is 4.696852235794068 and perplexity is 109.60162880840777
At time: 368.48073744773865 and batch: 150, loss is 4.722808809280395 and perplexity is 112.48375478207444
At time: 368.91048288345337 and batch: 200, loss is 4.735486154556274 and perplexity is 113.91882741524076
At time: 369.3400225639343 and batch: 250, loss is 4.708679084777832 and perplexity is 110.90556625743076
At time: 369.7829773426056 and batch: 300, loss is 4.62938738822937 and perplexity is 102.45128202065133
At time: 370.2109363079071 and batch: 350, loss is 4.686279678344727 and perplexity is 108.44896333919613
At time: 370.639333486557 and batch: 400, loss is 4.612375621795654 and perplexity is 100.72314574216288
At time: 371.0662214756012 and batch: 450, loss is 4.659465255737305 and perplexity is 105.57960896849505
At time: 371.49444341659546 and batch: 500, loss is 4.612144317626953 and perplexity is 100.69985075288594
At time: 371.9224147796631 and batch: 550, loss is 4.660628280639648 and perplexity is 105.70247211550586
At time: 372.35213565826416 and batch: 600, loss is 4.6927650260925295 and perplexity is 109.1545781850023
At time: 372.7806100845337 and batch: 650, loss is 4.651645135879517 and perplexity is 104.7571836951596
At time: 373.2086446285248 and batch: 700, loss is 4.632653274536133 and perplexity is 102.78642322836845
At time: 373.6368079185486 and batch: 750, loss is 4.57212794303894 and perplexity is 96.74976887701156
At time: 374.0661141872406 and batch: 800, loss is 4.614320001602173 and perplexity is 100.91918031386338
At time: 374.4952073097229 and batch: 850, loss is 4.5927495193481445 and perplexity is 98.76561514669903
At time: 374.9247145652771 and batch: 900, loss is 4.698580026626587 and perplexity is 109.79116118689483
At time: 375.3543267250061 and batch: 950, loss is 4.6366055393218994 and perplexity is 103.19346623036274
At time: 375.7827250957489 and batch: 1000, loss is 4.60560338973999 and perplexity is 100.0433297598195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.033251878691883 and perplexity of 153.43114178961395
Finished 40 epochs...
Completing Train Step...
At time: 377.06650948524475 and batch: 50, loss is 4.758699951171875 and perplexity is 116.59424910981528
At time: 377.49301862716675 and batch: 100, loss is 4.695454149246216 and perplexity is 109.44850331183873
At time: 377.9199573993683 and batch: 150, loss is 4.721807632446289 and perplexity is 112.37119500809452
At time: 378.34722113609314 and batch: 200, loss is 4.734393796920776 and perplexity is 113.7944552560928
At time: 378.7743122577667 and batch: 250, loss is 4.707279109954834 and perplexity is 110.75040988978553
At time: 379.20068430900574 and batch: 300, loss is 4.628388175964355 and perplexity is 102.34896257102753
At time: 379.6285116672516 and batch: 350, loss is 4.6853846740722656 and perplexity is 108.35194447628821
At time: 380.0555307865143 and batch: 400, loss is 4.61161955833435 and perplexity is 100.64702143299026
At time: 380.49646186828613 and batch: 450, loss is 4.6586502838134765 and perplexity is 105.49359960382608
At time: 380.9241964817047 and batch: 500, loss is 4.611528177261352 and perplexity is 100.63782462039133
At time: 381.3523542881012 and batch: 550, loss is 4.660017852783203 and perplexity is 105.63796807156702
At time: 381.78073167800903 and batch: 600, loss is 4.692225847244263 and perplexity is 109.09574020877501
At time: 382.210736989975 and batch: 650, loss is 4.651300773620606 and perplexity is 104.7211154853667
At time: 382.63941979408264 and batch: 700, loss is 4.632009830474853 and perplexity is 102.72030718802988
At time: 383.07078790664673 and batch: 750, loss is 4.571980276107788 and perplexity is 96.73548319033952
At time: 383.50157618522644 and batch: 800, loss is 4.614164266586304 and perplexity is 100.90346488746856
At time: 383.9302508831024 and batch: 850, loss is 4.592825393676758 and perplexity is 98.77310920573817
At time: 384.3562662601471 and batch: 900, loss is 4.698895015716553 and perplexity is 109.82574965205198
At time: 384.78077697753906 and batch: 950, loss is 4.637112321853638 and perplexity is 103.24577613019002
At time: 385.2048885822296 and batch: 1000, loss is 4.606134719848633 and perplexity is 100.0964999172917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032955262719131 and perplexity of 153.3856384110897
Finished 41 epochs...
Completing Train Step...
At time: 386.4542980194092 and batch: 50, loss is 4.7579092502594 and perplexity is 116.502094368875
At time: 386.89259672164917 and batch: 100, loss is 4.6948261642456055 and perplexity is 109.37979287024106
At time: 387.31799387931824 and batch: 150, loss is 4.721151371002197 and perplexity is 112.2974743180532
At time: 387.7433888912201 and batch: 200, loss is 4.733834638595581 and perplexity is 113.73084392513528
At time: 388.16912961006165 and batch: 250, loss is 4.706544580459595 and perplexity is 110.6690903165804
At time: 388.595157623291 and batch: 300, loss is 4.627782878875732 and perplexity is 102.28702978771733
At time: 389.01993107795715 and batch: 350, loss is 4.684889612197876 and perplexity is 108.29831683515754
At time: 389.4451494216919 and batch: 400, loss is 4.611318778991699 and perplexity is 100.61675344026553
At time: 389.8718819618225 and batch: 450, loss is 4.658269538879394 and perplexity is 105.45344109575822
At time: 390.29826164245605 and batch: 500, loss is 4.611229181289673 and perplexity is 100.60773881422283
At time: 390.72467732429504 and batch: 550, loss is 4.659851484298706 and perplexity is 105.62039470478132
At time: 391.14941334724426 and batch: 600, loss is 4.691935930252075 and perplexity is 109.06411608432025
At time: 391.5950915813446 and batch: 650, loss is 4.65109130859375 and perplexity is 104.69918237128952
At time: 392.01921558380127 and batch: 700, loss is 4.631710796356201 and perplexity is 102.68959490374154
At time: 392.44413137435913 and batch: 750, loss is 4.57193925857544 and perplexity is 96.73151542090314
At time: 392.8690218925476 and batch: 800, loss is 4.614061298370362 and perplexity is 100.89307557260051
At time: 393.2937068939209 and batch: 850, loss is 4.5928617858886716 and perplexity is 98.77670384306772
At time: 393.7187762260437 and batch: 900, loss is 4.6990709972381595 and perplexity is 109.84507865531116
At time: 394.1444637775421 and batch: 950, loss is 4.637427768707275 and perplexity is 103.27834982278594
At time: 394.5706708431244 and batch: 1000, loss is 4.606459274291992 and perplexity is 100.12899195353667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032784810880336 and perplexity of 153.3594957750712
Finished 42 epochs...
Completing Train Step...
At time: 395.82695627212524 and batch: 50, loss is 4.757368259429931 and perplexity is 116.43908484953043
At time: 396.2669367790222 and batch: 100, loss is 4.6943745613098145 and perplexity is 109.33040778672832
At time: 396.6931326389313 and batch: 150, loss is 4.720592384338379 and perplexity is 112.23471906883674
At time: 397.12146949768066 and batch: 200, loss is 4.733442258834839 and perplexity is 113.68622699776907
At time: 397.5489864349365 and batch: 250, loss is 4.706020927429199 and perplexity is 110.61115328284049
At time: 397.9762511253357 and batch: 300, loss is 4.627286005020141 and perplexity is 102.23621866125532
At time: 398.4019157886505 and batch: 350, loss is 4.684511632919311 and perplexity is 108.25739005071594
At time: 398.82725381851196 and batch: 400, loss is 4.611077899932861 and perplexity is 100.59251989018804
At time: 399.2524471282959 and batch: 450, loss is 4.658006992340088 and perplexity is 105.4257582939115
At time: 399.67878580093384 and batch: 500, loss is 4.611003656387329 and perplexity is 100.58505182208867
At time: 400.10517144203186 and batch: 550, loss is 4.659705715179443 and perplexity is 105.60499963495921
At time: 400.5310814380646 and batch: 600, loss is 4.691661233901978 and perplexity is 109.03416068421315
At time: 400.957528591156 and batch: 650, loss is 4.650931625366211 and perplexity is 104.68246500270503
At time: 401.38329124450684 and batch: 700, loss is 4.631483640670776 and perplexity is 102.66627102760097
At time: 401.80831813812256 and batch: 750, loss is 4.571924524307251 and perplexity is 96.73009016331272
At time: 402.24768447875977 and batch: 800, loss is 4.613946113586426 and perplexity is 100.88145489476554
At time: 402.6745903491974 and batch: 850, loss is 4.592860851287842 and perplexity is 98.7766115263215
At time: 403.1175880432129 and batch: 900, loss is 4.699171295166016 and perplexity is 109.85609644160684
At time: 403.55215549468994 and batch: 950, loss is 4.637623510360718 and perplexity is 103.29856767641856
At time: 403.9800269603729 and batch: 1000, loss is 4.606669998168945 and perplexity is 100.15009374616422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032654552924924 and perplexity of 153.33952078168767
Finished 43 epochs...
Completing Train Step...
At time: 405.25171518325806 and batch: 50, loss is 4.756941785812378 and perplexity is 116.38943723924054
At time: 405.67807269096375 and batch: 100, loss is 4.694018878936768 and perplexity is 109.29152780271293
At time: 406.10409021377563 and batch: 150, loss is 4.720122146606445 and perplexity is 112.1819544760211
At time: 406.5305333137512 and batch: 200, loss is 4.733141527175904 and perplexity is 113.65204309047513
At time: 406.95729207992554 and batch: 250, loss is 4.7055991268157955 and perplexity is 110.56450726888677
At time: 407.38273215293884 and batch: 300, loss is 4.62688419342041 and perplexity is 102.19514721472895
At time: 407.8105037212372 and batch: 350, loss is 4.6841909790039065 and perplexity is 108.22268245958456
At time: 408.2375738620758 and batch: 400, loss is 4.610858564376831 and perplexity is 100.57045879338533
At time: 408.66439986228943 and batch: 450, loss is 4.6577843570709225 and perplexity is 105.40228941443388
At time: 409.09198784828186 and batch: 500, loss is 4.610815124511719 and perplexity is 100.56609012110904
At time: 409.51895356178284 and batch: 550, loss is 4.659621353149414 and perplexity is 105.59609095859106
At time: 409.94612646102905 and batch: 600, loss is 4.691498575210571 and perplexity is 109.01642677264415
At time: 410.3739995956421 and batch: 650, loss is 4.65080415725708 and perplexity is 104.6691221772424
At time: 410.8008728027344 and batch: 700, loss is 4.631297044754028 and perplexity is 102.64711570784726
At time: 411.227942943573 and batch: 750, loss is 4.571903772354126 and perplexity is 96.72808284584384
At time: 411.65413641929626 and batch: 800, loss is 4.613811225891113 and perplexity is 100.86784814552713
At time: 412.08043813705444 and batch: 850, loss is 4.5928248596191406 and perplexity is 98.77305645522087
At time: 412.505868434906 and batch: 900, loss is 4.699222660064697 and perplexity is 109.86173933379212
At time: 412.9309992790222 and batch: 950, loss is 4.637741012573242 and perplexity is 103.31070619980885
At time: 413.3702640533447 and batch: 1000, loss is 4.606813488006591 and perplexity is 100.16446529791729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032536204268292 and perplexity of 153.32137432922
Finished 44 epochs...
Completing Train Step...
At time: 414.62495017051697 and batch: 50, loss is 4.756587066650391 and perplexity is 116.34815899712339
At time: 415.06192445755005 and batch: 100, loss is 4.693742141723633 and perplexity is 109.26128695446663
At time: 415.487154006958 and batch: 150, loss is 4.719711637496948 and perplexity is 112.13591221281831
At time: 415.91312313079834 and batch: 200, loss is 4.732876634597778 and perplexity is 113.6219414947923
At time: 416.33913564682007 and batch: 250, loss is 4.705232467651367 and perplexity is 110.52397521021558
At time: 416.76479601860046 and batch: 300, loss is 4.626546812057495 and perplexity is 102.16067429226615
At time: 417.19020771980286 and batch: 350, loss is 4.683897218704224 and perplexity is 108.19089560103988
At time: 417.61569261550903 and batch: 400, loss is 4.610659818649292 and perplexity is 100.55047283051171
At time: 418.0411193370819 and batch: 450, loss is 4.657591304779053 and perplexity is 105.38194322489655
At time: 418.4787893295288 and batch: 500, loss is 4.610637817382813 and perplexity is 100.54826061710014
At time: 418.91459369659424 and batch: 550, loss is 4.659556684494018 and perplexity is 105.58926242217221
At time: 419.3395240306854 and batch: 600, loss is 4.691353492736816 and perplexity is 109.00061154705183
At time: 419.7639043331146 and batch: 650, loss is 4.650679092407227 and perplexity is 104.65603256773512
At time: 420.18843817710876 and batch: 700, loss is 4.631121883392334 and perplexity is 102.62913747387782
At time: 420.61391854286194 and batch: 750, loss is 4.571856365203858 and perplexity is 96.72349735177865
At time: 421.0388901233673 and batch: 800, loss is 4.613666343688965 and perplexity is 100.85323524816178
At time: 421.4653344154358 and batch: 850, loss is 4.5927755165100095 and perplexity is 98.76818280575849
At time: 421.89142298698425 and batch: 900, loss is 4.699249801635742 and perplexity is 109.86472119446137
At time: 422.3182210922241 and batch: 950, loss is 4.637822885513305 and perplexity is 103.31916489732991
At time: 422.7439157962799 and batch: 1000, loss is 4.606911029815674 and perplexity is 100.17423599758631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032425671088986 and perplexity of 153.30442816683362
Finished 45 epochs...
Completing Train Step...
At time: 423.9983563423157 and batch: 50, loss is 4.756295833587647 and perplexity is 116.31427950008833
At time: 424.4382793903351 and batch: 100, loss is 4.693505229949952 and perplexity is 109.23540473520181
At time: 424.86419224739075 and batch: 150, loss is 4.719360971450806 and perplexity is 112.0965968495365
At time: 425.2918870449066 and batch: 200, loss is 4.732635564804077 and perplexity is 113.59455397808043
At time: 425.71904611587524 and batch: 250, loss is 4.704913072586059 and perplexity is 110.48868003478772
At time: 426.14541602134705 and batch: 300, loss is 4.626242542266846 and perplexity is 102.12959461383006
At time: 426.5705056190491 and batch: 350, loss is 4.683641185760498 and perplexity is 108.16319871336401
At time: 426.994749546051 and batch: 400, loss is 4.610465250015259 and perplexity is 100.53091076550548
At time: 427.42050313949585 and batch: 450, loss is 4.657425737380981 and perplexity is 105.36449685506796
At time: 427.8464641571045 and batch: 500, loss is 4.610497713088989 and perplexity is 100.53417436084675
At time: 428.27190685272217 and batch: 550, loss is 4.659488277435303 and perplexity is 105.58203961834619
At time: 428.6982071399689 and batch: 600, loss is 4.69121452331543 and perplexity is 108.98546484762278
At time: 429.1245038509369 and batch: 650, loss is 4.650550651550293 and perplexity is 104.64259132044992
At time: 429.5524787902832 and batch: 700, loss is 4.630984811782837 and perplexity is 102.61507089690923
At time: 429.97912096977234 and batch: 750, loss is 4.571810150146485 and perplexity is 96.71902737309017
At time: 430.4058084487915 and batch: 800, loss is 4.613535814285278 and perplexity is 100.84007179463252
At time: 430.8320209980011 and batch: 850, loss is 4.5927223777771 and perplexity is 98.76293452911699
At time: 431.25892090797424 and batch: 900, loss is 4.699258642196655 and perplexity is 109.86569246451461
At time: 431.68559741973877 and batch: 950, loss is 4.637879838943482 and perplexity is 103.3250494457448
At time: 432.1126070022583 and batch: 1000, loss is 4.606995458602905 and perplexity is 100.1826939438855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032356820455411 and perplexity of 153.29387342317906
Finished 46 epochs...
Completing Train Step...
At time: 433.3779501914978 and batch: 50, loss is 4.756038980484009 and perplexity is 116.2844076529039
At time: 433.8043780326843 and batch: 100, loss is 4.693273849487305 and perplexity is 109.21013272055477
At time: 434.23105669021606 and batch: 150, loss is 4.719046106338501 and perplexity is 112.06130709802783
At time: 434.6569633483887 and batch: 200, loss is 4.732419099807739 and perplexity is 113.56996739453288
At time: 435.0956883430481 and batch: 250, loss is 4.704622220993042 and perplexity is 110.45654889911175
At time: 435.5222918987274 and batch: 300, loss is 4.625960102081299 and perplexity is 102.10075318535843
At time: 435.9494585990906 and batch: 350, loss is 4.683401737213135 and perplexity is 108.13730229310775
At time: 436.37742471694946 and batch: 400, loss is 4.610285177230835 and perplexity is 100.51280951430351
At time: 436.80438923835754 and batch: 450, loss is 4.657269477844238 and perplexity is 105.34803393387796
At time: 437.23121762275696 and batch: 500, loss is 4.610369577407837 and perplexity is 100.52129317122352
At time: 437.65872025489807 and batch: 550, loss is 4.659418640136718 and perplexity is 105.57468742632449
At time: 438.09321546554565 and batch: 600, loss is 4.691080083847046 and perplexity is 108.97081388452304
At time: 438.5332136154175 and batch: 650, loss is 4.650420446395874 and perplexity is 104.62896720267265
At time: 438.9634349346161 and batch: 700, loss is 4.630852193832397 and perplexity is 102.60146319885509
At time: 439.39202332496643 and batch: 750, loss is 4.571761913299561 and perplexity is 96.71436206469293
At time: 439.820255279541 and batch: 800, loss is 4.613403968811035 and perplexity is 100.82677736396856
At time: 440.2466666698456 and batch: 850, loss is 4.59265627861023 and perplexity is 98.75640659717482
At time: 440.67315769195557 and batch: 900, loss is 4.699256763458252 and perplexity is 109.8654860558129
At time: 441.0991036891937 and batch: 950, loss is 4.6379117584228515 and perplexity is 103.32834758016605
At time: 441.5250084400177 and batch: 1000, loss is 4.607057657241821 and perplexity is 100.18892536488266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032294668802401 and perplexity of 153.2843462516173
Finished 47 epochs...
Completing Train Step...
At time: 442.7795412540436 and batch: 50, loss is 4.755791883468628 and perplexity is 116.25567767252996
At time: 443.217976808548 and batch: 100, loss is 4.693053369522095 and perplexity is 109.1860567285264
At time: 443.65181732177734 and batch: 150, loss is 4.718731880187988 and perplexity is 112.02610003665633
At time: 444.08808994293213 and batch: 200, loss is 4.732219228744507 and perplexity is 113.54727031271895
At time: 444.5164144039154 and batch: 250, loss is 4.704352760314942 and perplexity is 110.42678921225755
At time: 444.94227743148804 and batch: 300, loss is 4.625691366195679 and perplexity is 102.07331873550437
At time: 445.36761832237244 and batch: 350, loss is 4.6831761074066165 and perplexity is 108.1129060468776
At time: 445.7927174568176 and batch: 400, loss is 4.610115489959717 and perplexity is 100.49575521693416
At time: 446.23102855682373 and batch: 450, loss is 4.657129468917847 and perplexity is 105.33328530124365
At time: 446.65694546699524 and batch: 500, loss is 4.610234870910644 and perplexity is 100.5077532119079
At time: 447.0826909542084 and batch: 550, loss is 4.659342031478882 and perplexity is 105.5665998010143
At time: 447.5074768066406 and batch: 600, loss is 4.6909435272216795 and perplexity is 108.95593421389745
At time: 447.9315378665924 and batch: 650, loss is 4.6502919769287105 and perplexity is 104.6155264383887
At time: 448.35579228401184 and batch: 700, loss is 4.630723381042481 and perplexity is 102.58824766931383
At time: 448.7811360359192 and batch: 750, loss is 4.571696443557739 and perplexity is 96.70803040764636
At time: 449.205828666687 and batch: 800, loss is 4.613280096054077 and perplexity is 100.8142884466156
At time: 449.6309506893158 and batch: 850, loss is 4.592580509185791 and perplexity is 98.7489241645607
At time: 450.05515241622925 and batch: 900, loss is 4.699235076904297 and perplexity is 109.86310347785677
At time: 450.4805519580841 and batch: 950, loss is 4.637921543121338 and perplexity is 103.32935862183855
At time: 450.906201839447 and batch: 1000, loss is 4.607099075317382 and perplexity is 100.19307508329992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032242565620236 and perplexity of 153.27635985746159
Finished 48 epochs...
Completing Train Step...
At time: 452.17034220695496 and batch: 50, loss is 4.755556917190551 and perplexity is 116.22836471757927
At time: 452.60916209220886 and batch: 100, loss is 4.692845869064331 and perplexity is 109.16340292219267
At time: 453.03617000579834 and batch: 150, loss is 4.718449010848999 and perplexity is 111.99441576925454
At time: 453.46270060539246 and batch: 200, loss is 4.732042484283447 and perplexity is 113.52720323504738
At time: 453.88896107673645 and batch: 250, loss is 4.7040846729278565 and perplexity is 110.39718915075227
At time: 454.31442737579346 and batch: 300, loss is 4.62542908668518 and perplexity is 102.04655050596399
At time: 454.73888206481934 and batch: 350, loss is 4.6829499244689945 and perplexity is 108.08845551744359
At time: 455.1642096042633 and batch: 400, loss is 4.60995038986206 and perplexity is 100.47916472751719
At time: 455.5897104740143 and batch: 450, loss is 4.656986246109009 and perplexity is 105.31820025254594
At time: 456.0153660774231 and batch: 500, loss is 4.610106134414673 and perplexity is 100.49481502876753
At time: 456.44062399864197 and batch: 550, loss is 4.659228296279907 and perplexity is 105.55459384554361
At time: 456.880047082901 and batch: 600, loss is 4.69080605506897 and perplexity is 108.94095683658033
At time: 457.3064720630646 and batch: 650, loss is 4.650146245956421 and perplexity is 104.60028182683726
At time: 457.7326354980469 and batch: 700, loss is 4.6305999660491945 and perplexity is 102.5755875226585
At time: 458.15802478790283 and batch: 750, loss is 4.571606798171997 and perplexity is 96.69936136753162
At time: 458.58382415771484 and batch: 800, loss is 4.613148260116577 and perplexity is 100.80099837645851
At time: 459.008828163147 and batch: 850, loss is 4.592485246658325 and perplexity is 98.7395175405167
At time: 459.43462920188904 and batch: 900, loss is 4.6991852855682374 and perplexity is 109.85763338333375
At time: 459.8620014190674 and batch: 950, loss is 4.6379251956939695 and perplexity is 103.3297360405152
At time: 460.2879717350006 and batch: 1000, loss is 4.607107763290405 and perplexity is 100.19394556181464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032189718106898 and perplexity of 153.26825979702554
Finished 49 epochs...
Completing Train Step...
At time: 461.55441069602966 and batch: 50, loss is 4.755317878723145 and perplexity is 116.2005849877523
At time: 461.9799339771271 and batch: 100, loss is 4.692654581069946 and perplexity is 109.14252327086444
At time: 462.4056725502014 and batch: 150, loss is 4.718137016296387 and perplexity is 111.95947957184663
At time: 462.8312039375305 and batch: 200, loss is 4.731850709915161 and perplexity is 113.50543371484842
At time: 463.26701188087463 and batch: 250, loss is 4.703824110031128 and perplexity is 110.36842748663047
At time: 463.69440627098083 and batch: 300, loss is 4.625178356170654 and perplexity is 102.02096752920046
At time: 464.1213128566742 and batch: 350, loss is 4.682719383239746 and perplexity is 108.06353954423095
At time: 464.5477797985077 and batch: 400, loss is 4.609780931472779 and perplexity is 100.46213913271177
At time: 464.97503089904785 and batch: 450, loss is 4.656833772659302 and perplexity is 105.30214324740109
At time: 465.4019000530243 and batch: 500, loss is 4.6099772453308105 and perplexity is 100.4818631788195
At time: 465.8287618160248 and batch: 550, loss is 4.6590814113616945 and perplexity is 105.53909060628354
At time: 466.2563705444336 and batch: 600, loss is 4.690631656646729 and perplexity is 108.92195936220351
At time: 466.6836688518524 and batch: 650, loss is 4.649989795684815 and perplexity is 104.5839183644028
At time: 467.1109993457794 and batch: 700, loss is 4.630451946258545 and perplexity is 102.56040542932051
At time: 467.5520884990692 and batch: 750, loss is 4.571510238647461 and perplexity is 96.69002457396046
At time: 467.9790861606598 and batch: 800, loss is 4.61298264503479 and perplexity is 100.78430559319465
At time: 468.4052937030792 and batch: 850, loss is 4.592407350540161 and perplexity is 98.73182641494924
At time: 468.8327362537384 and batch: 900, loss is 4.699144201278687 and perplexity is 109.85312005322857
At time: 469.25905418395996 and batch: 950, loss is 4.637911539077759 and perplexity is 103.32832491560261
At time: 469.68607544898987 and batch: 1000, loss is 4.6071189022064205 and perplexity is 100.19506161997533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.032104120021913 and perplexity of 153.25514088898257
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
SETTINGS FOR THIS RUN
{'anneal': 6.288900331060056, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.8327629516169606, 'batch_size': 50, 'num_layers': 1, 'lr': 14.360885070025613, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6595487594604492 and batch: 50, loss is 7.069974670410156 and perplexity is 1176.118243279168
At time: 1.100050449371338 and batch: 100, loss is 6.4754571437835695 and perplexity is 649.0158533682072
At time: 1.5281364917755127 and batch: 150, loss is 6.371535940170288 and perplexity is 584.9555960542903
At time: 1.9558594226837158 and batch: 200, loss is 6.3096840190887455 and perplexity is 549.871172564317
At time: 2.3835535049438477 and batch: 250, loss is 6.28069769859314 and perplexity is 534.1612173752773
At time: 2.8121073246002197 and batch: 300, loss is 6.167870445251465 and perplexity is 477.168866114896
At time: 3.2412381172180176 and batch: 350, loss is 6.199473161697387 and perplexity is 492.4895103961915
At time: 3.6688787937164307 and batch: 400, loss is 6.135956621170044 and perplexity is 462.1810147529446
At time: 4.096947908401489 and batch: 450, loss is 6.088734464645386 and perplexity is 440.86313008060574
At time: 4.54305362701416 and batch: 500, loss is 6.076407804489135 and perplexity is 435.4621167174011
At time: 4.97577166557312 and batch: 550, loss is 6.1340605163574216 and perplexity is 461.30550140141344
At time: 5.4070093631744385 and batch: 600, loss is 6.156754703521728 and perplexity is 471.894150736363
At time: 5.836375713348389 and batch: 650, loss is 6.10240517616272 and perplexity is 446.9314272127705
At time: 6.264895439147949 and batch: 700, loss is 6.098655300140381 and perplexity is 445.25862812501157
At time: 6.691918849945068 and batch: 750, loss is 5.9935393810272215 and perplexity is 400.83079515063173
At time: 7.119033575057983 and batch: 800, loss is 6.048399667739869 and perplexity is 423.4348510814164
At time: 7.546831369400024 and batch: 850, loss is 6.014990797042847 and perplexity is 409.52207008522595
At time: 7.99336838722229 and batch: 900, loss is 6.0683531570434575 and perplexity is 431.96871085448686
At time: 8.44447660446167 and batch: 950, loss is 6.083391227722168 and perplexity is 438.51377609289756
At time: 8.88230037689209 and batch: 1000, loss is 6.001332674026489 and perplexity is 403.966790975372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.7863922119140625 and perplexity of 325.83535634841076
Finished 1 epochs...
Completing Train Step...
At time: 10.14813494682312 and batch: 50, loss is 5.609914541244507 and perplexity is 273.1208964354917
At time: 10.572949886322021 and batch: 100, loss is 5.424601211547851 and perplexity is 226.92083486052167
At time: 10.997921466827393 and batch: 150, loss is 5.340975894927978 and perplexity is 208.71629610851326
At time: 11.422863006591797 and batch: 200, loss is 5.277849721908569 and perplexity is 195.94807916308562
At time: 11.847470760345459 and batch: 250, loss is 5.234970064163208 and perplexity is 187.72348660042655
At time: 12.272308349609375 and batch: 300, loss is 5.104230184555053 and perplexity is 164.7172198522087
At time: 12.697831869125366 and batch: 350, loss is 5.137070941925049 and perplexity is 170.21646352494975
At time: 13.122305631637573 and batch: 400, loss is 5.074981842041016 and perplexity is 159.9692872372581
At time: 13.546837091445923 and batch: 450, loss is 5.081568222045899 and perplexity is 161.02638314836472
At time: 13.969539165496826 and batch: 500, loss is 5.051123313903808 and perplexity is 156.19782516416308
At time: 14.393470764160156 and batch: 550, loss is 5.090335779190063 and perplexity is 162.44439834540754
At time: 14.817021131515503 and batch: 600, loss is 5.100927877426147 and perplexity is 164.1741701547876
At time: 15.240617275238037 and batch: 650, loss is 5.0777610206604 and perplexity is 160.41448882091214
At time: 15.664335012435913 and batch: 700, loss is 5.042938184738159 and perplexity is 154.92454388442783
At time: 16.088045120239258 and batch: 750, loss is 4.976622161865234 and perplexity is 144.98382172208304
At time: 16.51294255256653 and batch: 800, loss is 5.012783937454223 and perplexity is 150.32264300193873
At time: 16.937475442886353 and batch: 850, loss is 4.99743950843811 and perplexity is 148.0336345530749
At time: 17.363319396972656 and batch: 900, loss is 5.127362279891968 and perplexity is 168.57188565138162
At time: 17.78869318962097 and batch: 950, loss is 5.0496122264862064 and perplexity is 155.96197483601048
At time: 18.21483325958252 and batch: 1000, loss is 5.0131370735168455 and perplexity is 150.37573672231332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.156714090486852 and perplexity of 173.59310617892422
Finished 2 epochs...
Completing Train Step...
At time: 19.461254358291626 and batch: 50, loss is 5.068784818649292 and perplexity is 158.98101914524432
At time: 19.899572610855103 and batch: 100, loss is 4.977512273788452 and perplexity is 145.11293100280227
At time: 20.325920581817627 and batch: 150, loss is 5.012078704833985 and perplexity is 150.2166679434596
At time: 20.763792276382446 and batch: 200, loss is 5.01750153541565 and perplexity is 151.03348019982914
At time: 21.188323736190796 and batch: 250, loss is 5.002246656417847 and perplexity is 148.7469673147413
At time: 21.612793684005737 and batch: 300, loss is 4.89869607925415 and perplexity is 134.11479056601308
At time: 22.03756046295166 and batch: 350, loss is 4.952123804092407 and perplexity is 141.47511050104703
At time: 22.462816953659058 and batch: 400, loss is 4.890320100784302 and perplexity is 132.99613940685072
At time: 22.88774824142456 and batch: 450, loss is 4.931193656921387 and perplexity is 138.54478862460994
At time: 23.312253952026367 and batch: 500, loss is 4.896891107559204 and perplexity is 133.8729355016027
At time: 23.737980842590332 and batch: 550, loss is 4.940595417022705 and perplexity is 139.85349592655123
At time: 24.163777828216553 and batch: 600, loss is 4.9499722099304195 and perplexity is 141.1710407139198
At time: 24.58888292312622 and batch: 650, loss is 4.94272964477539 and perplexity is 140.1522938768898
At time: 25.014941692352295 and batch: 700, loss is 4.905973844528198 and perplexity is 135.0944069166111
At time: 25.441619157791138 and batch: 750, loss is 4.854576549530029 and perplexity is 128.32633985905576
At time: 25.8670175075531 and batch: 800, loss is 4.9048978710174564 and perplexity is 134.9491270859603
At time: 26.29285979270935 and batch: 850, loss is 4.885775108337402 and perplexity is 132.3930445267921
At time: 26.71913480758667 and batch: 900, loss is 5.003120651245117 and perplexity is 148.8770282227479
At time: 27.145657777786255 and batch: 950, loss is 4.94232292175293 and perplexity is 140.09530230299765
At time: 27.57127809524536 and batch: 1000, loss is 4.906114082336426 and perplexity is 135.11335358862982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.107612051614901 and perplexity of 165.2752145924158
Finished 3 epochs...
Completing Train Step...
At time: 28.83394980430603 and batch: 50, loss is 4.973326358795166 and perplexity is 144.5067701631612
At time: 29.283803939819336 and batch: 100, loss is 4.892874765396118 and perplexity is 133.3363342946039
At time: 29.70991611480713 and batch: 150, loss is 4.930471258163452 and perplexity is 138.444740183175
At time: 30.137156009674072 and batch: 200, loss is 4.9284634685516355 and perplexity is 138.16705113590223
At time: 30.563663482666016 and batch: 250, loss is 4.90194076538086 and perplexity is 134.5506577099865
At time: 30.99084973335266 and batch: 300, loss is 4.822902221679687 and perplexity is 124.32538753143633
At time: 31.418766021728516 and batch: 350, loss is 4.877487773895264 and perplexity is 131.30039292719712
At time: 31.859694719314575 and batch: 400, loss is 4.808539695739746 and perplexity is 122.55252281154424
At time: 32.28688836097717 and batch: 450, loss is 4.865942907333374 and perplexity is 129.79326395241895
At time: 32.71480131149292 and batch: 500, loss is 4.817434034347534 and perplexity is 123.64740837016035
At time: 33.14257264137268 and batch: 550, loss is 4.86851655960083 and perplexity is 130.12773690440594
At time: 33.57154178619385 and batch: 600, loss is 4.89801212310791 and perplexity is 134.02309329273524
At time: 33.99914240837097 and batch: 650, loss is 4.891306552886963 and perplexity is 133.12739845795403
At time: 34.426886320114136 and batch: 700, loss is 4.820751428604126 and perplexity is 124.05827670184784
At time: 34.87150597572327 and batch: 750, loss is 4.786247129440308 and perplexity is 119.85073930418595
At time: 35.30469465255737 and batch: 800, loss is 4.841483039855957 and perplexity is 126.65704996686851
At time: 35.73079252243042 and batch: 850, loss is 4.82498251914978 and perplexity is 124.58429052483758
At time: 36.15843057632446 and batch: 900, loss is 4.934275321960449 and perplexity is 138.97239578867934
At time: 36.58582353591919 and batch: 950, loss is 4.885333938598633 and perplexity is 132.3346496039069
At time: 37.013667821884155 and batch: 1000, loss is 4.86280312538147 and perplexity is 129.3863805017025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.115953026748285 and perplexity of 166.65953631823038
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 38.27702355384827 and batch: 50, loss is 4.898620061874389 and perplexity is 134.10459589853798
At time: 38.70377445220947 and batch: 100, loss is 4.7853885269165035 and perplexity is 119.7478793211791
At time: 39.130226373672485 and batch: 150, loss is 4.791213054656982 and perplexity is 120.44738934634272
At time: 39.55617713928223 and batch: 200, loss is 4.785345392227173 and perplexity is 119.74271414500652
At time: 39.98324012756348 and batch: 250, loss is 4.754896535873413 and perplexity is 116.15163501521768
At time: 40.41101050376892 and batch: 300, loss is 4.65539855003357 and perplexity is 105.15111963105397
At time: 40.837422370910645 and batch: 350, loss is 4.706145629882813 and perplexity is 110.62494762512821
At time: 41.26374912261963 and batch: 400, loss is 4.626295537948608 and perplexity is 102.1350071847449
At time: 41.69000816345215 and batch: 450, loss is 4.659238538742065 and perplexity is 105.5556749900135
At time: 42.11561369895935 and batch: 500, loss is 4.6100754737854 and perplexity is 100.4917338417359
At time: 42.561318159103394 and batch: 550, loss is 4.648083162307739 and perplexity is 104.38470514857632
At time: 42.98697328567505 and batch: 600, loss is 4.651790657043457 and perplexity is 104.77242919170631
At time: 43.41234874725342 and batch: 650, loss is 4.648458480834961 and perplexity is 104.42389001532038
At time: 43.83768939971924 and batch: 700, loss is 4.590720195770263 and perplexity is 98.56539098371812
At time: 44.26582455635071 and batch: 750, loss is 4.52577127456665 and perplexity is 92.36713878769682
At time: 44.691009283065796 and batch: 800, loss is 4.555489339828491 and perplexity is 95.15330614755207
At time: 45.11687469482422 and batch: 850, loss is 4.541250705718994 and perplexity is 93.80805304817022
At time: 45.543837547302246 and batch: 900, loss is 4.642706384658814 and perplexity is 103.8249579656263
At time: 45.96980333328247 and batch: 950, loss is 4.545801601409912 and perplexity is 94.23593660078978
At time: 46.39650368690491 and batch: 1000, loss is 4.508924474716187 and perplexity is 90.82408236414088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.90046245295827 and perplexity of 134.35189675282007
Finished 5 epochs...
Completing Train Step...
At time: 47.65264105796814 and batch: 50, loss is 4.729496803283691 and perplexity is 113.23856673491915
At time: 48.1060528755188 and batch: 100, loss is 4.641990985870361 and perplexity is 103.75070827872129
At time: 48.53511619567871 and batch: 150, loss is 4.6774214744567875 and perplexity is 107.49254264924782
At time: 48.96268105506897 and batch: 200, loss is 4.6845505332946775 and perplexity is 108.26160138573582
At time: 49.38994026184082 and batch: 250, loss is 4.656076145172119 and perplexity is 105.22239366327085
At time: 49.817426919937134 and batch: 300, loss is 4.570232019424439 and perplexity is 96.56651248045827
At time: 50.2443630695343 and batch: 350, loss is 4.623071641921997 and perplexity is 101.80626474136471
At time: 50.672229528427124 and batch: 400, loss is 4.543688621520996 and perplexity is 94.03702818069011
At time: 51.10004210472107 and batch: 450, loss is 4.5873816776275635 and perplexity is 98.23687731722082
At time: 51.52708458900452 and batch: 500, loss is 4.539051427841186 and perplexity is 93.60196977260132
At time: 51.95487356185913 and batch: 550, loss is 4.577565116882324 and perplexity is 97.27724688503797
At time: 52.383605003356934 and batch: 600, loss is 4.58848349571228 and perplexity is 98.3451761370779
At time: 52.81260299682617 and batch: 650, loss is 4.582141971588134 and perplexity is 97.72349112876694
At time: 53.24102020263672 and batch: 700, loss is 4.530554037094117 and perplexity is 92.80996700482719
At time: 53.685521602630615 and batch: 750, loss is 4.478313035964966 and perplexity is 88.08594943685249
At time: 54.11382031440735 and batch: 800, loss is 4.509863080978394 and perplexity is 90.90937043630497
At time: 54.542789936065674 and batch: 850, loss is 4.503589305877686 and perplexity is 90.34081086397092
At time: 54.97181034088135 and batch: 900, loss is 4.609866161346435 and perplexity is 100.47070187303275
At time: 55.4001739025116 and batch: 950, loss is 4.518572025299072 and perplexity is 91.70455265457076
At time: 55.82857370376587 and batch: 1000, loss is 4.487711458206177 and perplexity is 88.91772093048472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.880985911299542 and perplexity of 131.76050403964163
Finished 6 epochs...
Completing Train Step...
At time: 57.10347056388855 and batch: 50, loss is 4.667557487487793 and perplexity is 106.43744987485557
At time: 57.55320596694946 and batch: 100, loss is 4.584265308380127 and perplexity is 97.93121146499551
At time: 57.98807406425476 and batch: 150, loss is 4.623990182876587 and perplexity is 101.89982092597914
At time: 58.41566348075867 and batch: 200, loss is 4.63177306175232 and perplexity is 102.6959891111123
At time: 58.84350919723511 and batch: 250, loss is 4.603038444519043 and perplexity is 99.78705290781036
At time: 59.275055170059204 and batch: 300, loss is 4.522072639465332 and perplexity is 92.0261374544573
At time: 59.704909801483154 and batch: 350, loss is 4.573727531433105 and perplexity is 96.9046525264646
At time: 60.134259939193726 and batch: 400, loss is 4.496087760925293 and perplexity is 89.66565074833204
At time: 60.56393504142761 and batch: 450, loss is 4.543040208816528 and perplexity is 93.97607314107404
At time: 60.99451684951782 and batch: 500, loss is 4.492808218002319 and perplexity is 89.37207006634199
At time: 61.42442584037781 and batch: 550, loss is 4.534544687271119 and perplexity is 93.18107911287689
At time: 61.85419249534607 and batch: 600, loss is 4.54928768157959 and perplexity is 94.56502390973932
At time: 62.283771991729736 and batch: 650, loss is 4.54024221420288 and perplexity is 93.71349611047687
At time: 62.712082624435425 and batch: 700, loss is 4.491400651931762 and perplexity is 89.24636146519697
At time: 63.14153170585632 and batch: 750, loss is 4.444639348983765 and perplexity is 85.16915595506947
At time: 63.56986331939697 and batch: 800, loss is 4.479727563858032 and perplexity is 88.2106376359953
At time: 64.0054714679718 and batch: 850, loss is 4.473918542861939 and perplexity is 87.6997056340234
At time: 64.46992468833923 and batch: 900, loss is 4.583427925109863 and perplexity is 97.84923983250481
At time: 64.89700150489807 and batch: 950, loss is 4.493437213897705 and perplexity is 89.42830241468002
At time: 65.32447481155396 and batch: 1000, loss is 4.462508382797242 and perplexity is 86.70472519298963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.873015613090701 and perplexity of 130.7145075196884
Finished 7 epochs...
Completing Train Step...
At time: 66.59062814712524 and batch: 50, loss is 4.624728097915649 and perplexity is 101.97504208631763
At time: 67.01878190040588 and batch: 100, loss is 4.544949159622193 and perplexity is 94.15564017940123
At time: 67.4472861289978 and batch: 150, loss is 4.585663957595825 and perplexity is 98.06827870925576
At time: 67.87486100196838 and batch: 200, loss is 4.594834842681885 and perplexity is 98.971788282648
At time: 68.30223941802979 and batch: 250, loss is 4.56681305885315 and perplexity is 96.23691913616436
At time: 68.72868657112122 and batch: 300, loss is 4.486821031570434 and perplexity is 88.83858146255264
At time: 69.15668821334839 and batch: 350, loss is 4.539318590164185 and perplexity is 93.62698003303389
At time: 69.58429527282715 and batch: 400, loss is 4.462333736419677 and perplexity is 86.68958384904589
At time: 70.00951361656189 and batch: 450, loss is 4.510648784637451 and perplexity is 90.98082632920159
At time: 70.43591094017029 and batch: 500, loss is 4.458821125030518 and perplexity is 86.38561121091716
At time: 70.86222219467163 and batch: 550, loss is 4.500843715667725 and perplexity is 90.09311221319965
At time: 71.28804016113281 and batch: 600, loss is 4.5187031936645505 and perplexity is 91.71658217977873
At time: 71.71426558494568 and batch: 650, loss is 4.506886129379272 and perplexity is 90.63914007153825
At time: 72.13879132270813 and batch: 700, loss is 4.46056453704834 and perplexity is 86.53634828390119
At time: 72.56449770927429 and batch: 750, loss is 4.417681884765625 and perplexity is 82.90388168643543
At time: 72.99118041992188 and batch: 800, loss is 4.4535255622863765 and perplexity is 85.92935990648381
At time: 73.41893219947815 and batch: 850, loss is 4.4486000442504885 and perplexity is 85.50715393958951
At time: 73.84504556655884 and batch: 900, loss is 4.557515802383423 and perplexity is 95.34632626740678
At time: 74.27230596542358 and batch: 950, loss is 4.470674448013305 and perplexity is 87.41566045461222
At time: 74.70034861564636 and batch: 1000, loss is 4.439370040893555 and perplexity is 84.72155374519367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.866458799780869 and perplexity of 129.86024059191345
Finished 8 epochs...
Completing Train Step...
At time: 75.96407103538513 and batch: 50, loss is 4.589862937927246 and perplexity is 98.48093123631013
At time: 76.40269732475281 and batch: 100, loss is 4.51173963546753 and perplexity is 91.08012699038144
At time: 76.82984709739685 and batch: 150, loss is 4.553538122177124 and perplexity is 94.96782235548419
At time: 77.25636553764343 and batch: 200, loss is 4.5628729343414305 and perplexity is 95.85847973092466
At time: 77.68506002426147 and batch: 250, loss is 4.534725952148437 and perplexity is 93.1979711006665
At time: 78.11269879341125 and batch: 300, loss is 4.4574164772033695 and perplexity is 86.26435503088467
At time: 78.54091882705688 and batch: 350, loss is 4.509338512420654 and perplexity is 90.8616947446478
At time: 78.96864676475525 and batch: 400, loss is 4.433773221969605 and perplexity is 84.24870700302553
At time: 79.39767289161682 and batch: 450, loss is 4.482581758499146 and perplexity is 88.46276760806961
At time: 79.8262197971344 and batch: 500, loss is 4.429748773574829 and perplexity is 83.91033376887094
At time: 80.2599949836731 and batch: 550, loss is 4.472935314178467 and perplexity is 87.6135191453692
At time: 80.69286918640137 and batch: 600, loss is 4.4911898994445805 and perplexity is 89.22755455441747
At time: 81.12129735946655 and batch: 650, loss is 4.479899988174439 and perplexity is 88.2258486062224
At time: 81.54920935630798 and batch: 700, loss is 4.435168981552124 and perplexity is 84.36638004567462
At time: 81.97867488861084 and batch: 750, loss is 4.3939620304107665 and perplexity is 80.96055254349115
At time: 82.4069504737854 and batch: 800, loss is 4.4311988735198975 and perplexity is 84.03210040481844
At time: 82.83522176742554 and batch: 850, loss is 4.427374401092529 and perplexity is 83.71133572262208
At time: 83.26374673843384 and batch: 900, loss is 4.5349326419830325 and perplexity is 93.21723616477577
At time: 83.69164490699768 and batch: 950, loss is 4.448145694732666 and perplexity is 85.46831262986007
At time: 84.1198661327362 and batch: 1000, loss is 4.4181862068176265 and perplexity is 82.94570248685595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.86617744259718 and perplexity of 129.8237086198547
Finished 9 epochs...
Completing Train Step...
At time: 85.41059684753418 and batch: 50, loss is 4.5633715152740475 and perplexity is 95.90628485752015
At time: 85.8734130859375 and batch: 100, loss is 4.484661502838135 and perplexity is 88.64693899653146
At time: 86.32565450668335 and batch: 150, loss is 4.52750319480896 and perplexity is 92.5272499149169
At time: 86.77777743339539 and batch: 200, loss is 4.535856742858886 and perplexity is 93.3034181086322
At time: 87.20626497268677 and batch: 250, loss is 4.508571557998657 and perplexity is 90.79203468253421
At time: 87.63471126556396 and batch: 300, loss is 4.432595081329346 and perplexity is 84.14950862371113
At time: 88.06324410438538 and batch: 350, loss is 4.484635753631592 and perplexity is 88.644656437577
At time: 88.49271154403687 and batch: 400, loss is 4.408401136398315 and perplexity is 82.13803095283504
At time: 88.92052006721497 and batch: 450, loss is 4.4580341148376466 and perplexity is 86.31765160033764
At time: 89.34713840484619 and batch: 500, loss is 4.405930070877075 and perplexity is 81.93531306430978
At time: 89.77302956581116 and batch: 550, loss is 4.449367828369141 and perplexity is 85.57283018377782
At time: 90.20016503334045 and batch: 600, loss is 4.467349786758422 and perplexity is 87.12551557919814
At time: 90.62652063369751 and batch: 650, loss is 4.4554993724823 and perplexity is 86.0991356506243
At time: 91.05210447311401 and batch: 700, loss is 4.411686897277832 and perplexity is 82.40836075799785
At time: 91.47677254676819 and batch: 750, loss is 4.372786684036255 and perplexity is 79.26420852240197
At time: 91.90196871757507 and batch: 800, loss is 4.4113192367553715 and perplexity is 82.3780680260885
At time: 92.32870125770569 and batch: 850, loss is 4.407979001998902 and perplexity is 82.10336498183395
At time: 92.75479125976562 and batch: 900, loss is 4.5152441596984865 and perplexity is 91.3998794652676
At time: 93.18052840232849 and batch: 950, loss is 4.427525520324707 and perplexity is 83.72398707130803
At time: 93.60699725151062 and batch: 1000, loss is 4.398081102371216 and perplexity is 81.2947226482481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.863794838509908 and perplexity of 129.51475832034959
Finished 10 epochs...
Completing Train Step...
At time: 94.87364172935486 and batch: 50, loss is 4.537600135803222 and perplexity is 93.46622450597103
At time: 95.30103135108948 and batch: 100, loss is 4.4601491355895995 and perplexity is 86.50040842384196
At time: 95.72959280014038 and batch: 150, loss is 4.5039394664764405 and perplexity is 90.37245019549539
At time: 96.16283202171326 and batch: 200, loss is 4.511607027053833 and perplexity is 91.06804980000815
At time: 96.58995938301086 and batch: 250, loss is 4.484868211746216 and perplexity is 88.66526500250525
At time: 97.01777124404907 and batch: 300, loss is 4.411188688278198 and perplexity is 82.36731439670545
At time: 97.46717262268066 and batch: 350, loss is 4.462356548309327 and perplexity is 86.69156142482242
At time: 97.89448881149292 and batch: 400, loss is 4.385291805267334 and perplexity is 80.26164056556338
At time: 98.32104802131653 and batch: 450, loss is 4.435142641067505 and perplexity is 84.36415782360596
At time: 98.74724507331848 and batch: 500, loss is 4.384487991333008 and perplexity is 80.19715106273769
At time: 99.17456197738647 and batch: 550, loss is 4.428479471206665 and perplexity is 83.80389375007935
At time: 99.6013650894165 and batch: 600, loss is 4.447372846603393 and perplexity is 85.40228412262263
At time: 100.029794216156 and batch: 650, loss is 4.432557182312012 and perplexity is 84.14631950045784
At time: 100.45718002319336 and batch: 700, loss is 4.389840421676635 and perplexity is 80.62755154432993
At time: 100.88457918167114 and batch: 750, loss is 4.353698892593384 and perplexity is 77.76557811469122
At time: 101.31249284744263 and batch: 800, loss is 4.392768964767456 and perplexity is 80.8640188867333
At time: 101.73937320709229 and batch: 850, loss is 4.389723863601684 and perplexity is 80.61815429980692
At time: 102.1674153804779 and batch: 900, loss is 4.496638708114624 and perplexity is 89.71506539777252
At time: 102.59461307525635 and batch: 950, loss is 4.409787254333496 and perplexity is 82.25196289401106
At time: 103.02217435836792 and batch: 1000, loss is 4.379763221740722 and perplexity is 79.8191317323516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8636109887099845 and perplexity of 129.49094924665903
Finished 11 epochs...
Completing Train Step...
At time: 104.2952470779419 and batch: 50, loss is 4.514814872741699 and perplexity is 91.3606511098743
At time: 104.73875451087952 and batch: 100, loss is 4.440000839233399 and perplexity is 84.77501281982028
At time: 105.16460680961609 and batch: 150, loss is 4.48325044631958 and perplexity is 88.52194136551157
At time: 105.59009194374084 and batch: 200, loss is 4.489364013671875 and perplexity is 89.064783877576
At time: 106.01528024673462 and batch: 250, loss is 4.464355316162109 and perplexity is 86.86501101589394
At time: 106.4404616355896 and batch: 300, loss is 4.391501140594483 and perplexity is 80.76156249093637
At time: 106.87405514717102 and batch: 350, loss is 4.441907997131348 and perplexity is 84.93684642713706
At time: 107.31249332427979 and batch: 400, loss is 4.364839859008789 and perplexity is 78.6368059573456
At time: 107.7386040687561 and batch: 450, loss is 4.415677185058594 and perplexity is 82.73785077576989
At time: 108.16407656669617 and batch: 500, loss is 4.3645779991149904 and perplexity is 78.61621682754026
At time: 108.60289907455444 and batch: 550, loss is 4.410153865814209 and perplexity is 82.2821229360902
At time: 109.03714990615845 and batch: 600, loss is 4.428043661117553 and perplexity is 83.76737912497516
At time: 109.47456336021423 and batch: 650, loss is 4.413970775604248 and perplexity is 82.59678651589688
At time: 109.90290522575378 and batch: 700, loss is 4.3725649356842045 and perplexity is 79.24663376344449
At time: 110.33040165901184 and batch: 750, loss is 4.3354345798492435 and perplexity is 76.35813538729205
At time: 110.75727105140686 and batch: 800, loss is 4.376378145217895 and perplexity is 79.5493946613358
At time: 111.18374800682068 and batch: 850, loss is 4.374422254562378 and perplexity is 79.39395680294543
At time: 111.61009383201599 and batch: 900, loss is 4.479624309539795 and perplexity is 88.20152997695561
At time: 112.03542232513428 and batch: 950, loss is 4.393119287490845 and perplexity is 80.89235235269255
At time: 112.46178650856018 and batch: 1000, loss is 4.364591398239136 and perplexity is 78.61727022304663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.864435707650533 and perplexity of 129.5977869346092
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 113.70955610275269 and batch: 50, loss is 4.494284658432007 and perplexity is 89.50412006186116
At time: 114.14878392219543 and batch: 100, loss is 4.408971738815308 and perplexity is 82.18491248590976
At time: 114.57507157325745 and batch: 150, loss is 4.45053635597229 and perplexity is 85.67288284368982
At time: 115.0010027885437 and batch: 200, loss is 4.449737434387207 and perplexity is 85.6044642625095
At time: 115.4266619682312 and batch: 250, loss is 4.416299676895141 and perplexity is 82.78937044607474
At time: 115.85382580757141 and batch: 300, loss is 4.342389421463013 and perplexity is 76.89104512819391
At time: 116.28734874725342 and batch: 350, loss is 4.39148063659668 and perplexity is 80.75990657301304
At time: 116.72440528869629 and batch: 400, loss is 4.306791286468506 and perplexity is 74.20201354042162
At time: 117.15196752548218 and batch: 450, loss is 4.355678749084473 and perplexity is 77.91969531393075
At time: 117.58082437515259 and batch: 500, loss is 4.2961716461181645 and perplexity is 73.41818420264674
At time: 118.02057814598083 and batch: 550, loss is 4.340822095870972 and perplexity is 76.77062621783655
At time: 118.4546914100647 and batch: 600, loss is 4.3536866188049315 and perplexity is 77.76462364229408
At time: 118.8804976940155 and batch: 650, loss is 4.332828321456909 and perplexity is 76.1593854654439
At time: 119.32717680931091 and batch: 700, loss is 4.287857627868652 and perplexity is 72.81031450070981
At time: 119.75219058990479 and batch: 750, loss is 4.246580805778503 and perplexity is 69.86611765653791
At time: 120.17845606803894 and batch: 800, loss is 4.281477193832398 and perplexity is 72.34723199656858
At time: 120.61616349220276 and batch: 850, loss is 4.27124559879303 and perplexity is 71.61077838875957
At time: 121.04247522354126 and batch: 900, loss is 4.3670378684997555 and perplexity is 78.8098404992975
At time: 121.46839904785156 and batch: 950, loss is 4.2735557365417485 and perplexity is 71.77640038228347
At time: 121.89455223083496 and batch: 1000, loss is 4.248406162261963 and perplexity is 69.99376469260633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82386928651391 and perplexity of 124.44567639590983
Finished 13 epochs...
Completing Train Step...
At time: 123.16515159606934 and batch: 50, loss is 4.4574727439880375 and perplexity is 86.26920898533064
At time: 123.59219813346863 and batch: 100, loss is 4.377946195602417 and perplexity is 79.6742299686702
At time: 124.0200731754303 and batch: 150, loss is 4.423507585525512 and perplexity is 83.38826445756632
At time: 124.4475965499878 and batch: 200, loss is 4.4235330581665036 and perplexity is 83.39038860394342
At time: 124.87657952308655 and batch: 250, loss is 4.396026096343994 and perplexity is 81.1278330415304
At time: 125.30438208580017 and batch: 300, loss is 4.324170904159546 and perplexity is 75.50288777202007
At time: 125.73188900947571 and batch: 350, loss is 4.373631935119629 and perplexity is 79.33123500364071
At time: 126.15797901153564 and batch: 400, loss is 4.291897983551025 and perplexity is 73.10508916487949
At time: 126.58504891395569 and batch: 450, loss is 4.3413686847686765 and perplexity is 76.81259965986077
At time: 127.01120376586914 and batch: 500, loss is 4.282936239242554 and perplexity is 72.45286693769893
At time: 127.43867492675781 and batch: 550, loss is 4.3281215572357175 and perplexity is 75.80176347623747
At time: 127.86478853225708 and batch: 600, loss is 4.343087644577026 and perplexity is 76.94475098031484
At time: 128.2920184135437 and batch: 650, loss is 4.323937263488769 and perplexity is 75.48524928728938
At time: 128.71892189979553 and batch: 700, loss is 4.281295075416565 and perplexity is 72.33405743298918
At time: 129.14663791656494 and batch: 750, loss is 4.242723278999328 and perplexity is 69.59712639078691
At time: 129.5735800266266 and batch: 800, loss is 4.278161182403564 and perplexity is 72.10772507174025
At time: 130.00091648101807 and batch: 850, loss is 4.269432048797608 and perplexity is 71.48102635340452
At time: 130.44122004508972 and batch: 900, loss is 4.367848997116089 and perplexity is 78.87379134885093
At time: 130.86895370483398 and batch: 950, loss is 4.279273471832275 and perplexity is 72.18797435400651
At time: 131.297132730484 and batch: 1000, loss is 4.254751443862915 and perplexity is 70.43930689035706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.82056147877763 and perplexity of 124.03471409164061
Finished 14 epochs...
Completing Train Step...
At time: 132.55054378509521 and batch: 50, loss is 4.446861457824707 and perplexity is 85.35862151808273
At time: 132.98867535591125 and batch: 100, loss is 4.367523946762085 and perplexity is 78.8481575614129
At time: 133.4155912399292 and batch: 150, loss is 4.413254461288452 and perplexity is 82.53764244067959
At time: 133.84205269813538 and batch: 200, loss is 4.412856206893921 and perplexity is 82.50477800650042
At time: 134.26816391944885 and batch: 250, loss is 4.386369752883911 and perplexity is 80.34820505732525
At time: 134.69427227973938 and batch: 300, loss is 4.315701684951782 and perplexity is 74.86613745969451
At time: 135.12085843086243 and batch: 350, loss is 4.364506378173828 and perplexity is 78.61058646172887
At time: 135.54729795455933 and batch: 400, loss is 4.284092359542846 and perplexity is 72.53667960740898
At time: 135.97486090660095 and batch: 450, loss is 4.33368763923645 and perplexity is 76.22485870658441
At time: 136.40187120437622 and batch: 500, loss is 4.2755743551254275 and perplexity is 71.92143589440529
At time: 136.82948851585388 and batch: 550, loss is 4.321241064071655 and perplexity is 75.28200012542264
At time: 137.25882267951965 and batch: 600, loss is 4.337605123519897 and perplexity is 76.524054056478
At time: 137.6883625984192 and batch: 650, loss is 4.318818821907043 and perplexity is 75.09986956169853
At time: 138.11713814735413 and batch: 700, loss is 4.277903079986572 and perplexity is 72.08911629520385
At time: 138.54470658302307 and batch: 750, loss is 4.240994577407837 and perplexity is 69.47691766008003
At time: 138.9719636440277 and batch: 800, loss is 4.275918703079224 and perplexity is 71.9462061582397
At time: 139.3985893726349 and batch: 850, loss is 4.267759857177734 and perplexity is 71.361596262984
At time: 139.82440614700317 and batch: 900, loss is 4.367816963195801 and perplexity is 78.87126475257466
At time: 140.2500970363617 and batch: 950, loss is 4.2815819835662845 and perplexity is 72.35481364098925
At time: 140.6750831604004 and batch: 1000, loss is 4.256724967956543 and perplexity is 70.57845782333933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.819282717821075 and perplexity of 123.87620471140606
Finished 15 epochs...
Completing Train Step...
At time: 141.9419162273407 and batch: 50, loss is 4.4398055839538575 and perplexity is 84.75846166689662
At time: 142.3808946609497 and batch: 100, loss is 4.360105113983154 and perplexity is 78.26536077453842
At time: 142.80647563934326 and batch: 150, loss is 4.406004991531372 and perplexity is 81.94145194153603
At time: 143.2331826686859 and batch: 200, loss is 4.405112543106079 and perplexity is 81.86835604379553
At time: 143.6585783958435 and batch: 250, loss is 4.37953896522522 and perplexity is 79.80123377894022
At time: 144.08488035202026 and batch: 300, loss is 4.309229946136474 and perplexity is 74.38318781954047
At time: 144.5246331691742 and batch: 350, loss is 4.35811411857605 and perplexity is 78.10968982220062
At time: 144.96017861366272 and batch: 400, loss is 4.27879846572876 and perplexity is 72.15369276821374
At time: 145.3862488269806 and batch: 450, loss is 4.3282490062713626 and perplexity is 75.81142495355262
At time: 145.81221652030945 and batch: 500, loss is 4.2701260852813725 and perplexity is 71.53065401330193
At time: 146.23841261863708 and batch: 550, loss is 4.3162196922302245 and perplexity is 74.90492871001508
At time: 146.6648371219635 and batch: 600, loss is 4.33319616317749 and perplexity is 76.18740521793295
At time: 147.09038472175598 and batch: 650, loss is 4.314941158294678 and perplexity is 74.80922141227097
At time: 147.53205013275146 and batch: 700, loss is 4.275434293746948 and perplexity is 71.911363184367
At time: 147.97923064231873 and batch: 750, loss is 4.239206809997558 and perplexity is 69.35282005280222
At time: 148.42143893241882 and batch: 800, loss is 4.273760499954224 and perplexity is 71.7910990677861
At time: 148.8566927909851 and batch: 850, loss is 4.266115307807922 and perplexity is 71.2443350421825
At time: 149.28230094909668 and batch: 900, loss is 4.366663217544556 and perplexity is 78.78031984759076
At time: 149.70800518989563 and batch: 950, loss is 4.281833658218384 and perplexity is 72.37302580521396
At time: 150.1361813545227 and batch: 1000, loss is 4.256790895462036 and perplexity is 70.58311103839092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.818463953529916 and perplexity of 123.77482080888737
Finished 16 epochs...
Completing Train Step...
At time: 151.41509437561035 and batch: 50, loss is 4.433388566970825 and perplexity is 84.21630654863392
At time: 151.84200930595398 and batch: 100, loss is 4.353989000320435 and perplexity is 77.78814178258983
At time: 152.28388667106628 and batch: 150, loss is 4.4000950050354 and perplexity is 81.45860727522664
At time: 152.7105839252472 and batch: 200, loss is 4.3989186859130855 and perplexity is 81.36284229394377
At time: 153.13842582702637 and batch: 250, loss is 4.374028034210205 and perplexity is 79.36266425781851
At time: 153.5660581588745 and batch: 300, loss is 4.303734693527222 and perplexity is 73.97555446252707
At time: 153.99254751205444 and batch: 350, loss is 4.352983531951904 and perplexity is 77.70996757401359
At time: 154.41901111602783 and batch: 400, loss is 4.273893394470215 and perplexity is 71.80064034512684
At time: 154.8461675643921 and batch: 450, loss is 4.323380107879639 and perplexity is 75.44320397122183
At time: 155.27226448059082 and batch: 500, loss is 4.265136203765869 and perplexity is 71.17461356362989
At time: 155.69893765449524 and batch: 550, loss is 4.311681356430054 and perplexity is 74.56575521419892
At time: 156.1255762577057 and batch: 600, loss is 4.329764957427979 and perplexity is 75.92643852634889
At time: 156.55168795585632 and batch: 650, loss is 4.310965509414673 and perplexity is 74.5123966414453
At time: 156.97822737693787 and batch: 700, loss is 4.272693781852722 and perplexity is 71.71455903346168
At time: 157.40582084655762 and batch: 750, loss is 4.23754430770874 and perplexity is 69.23761662024795
At time: 157.83258771896362 and batch: 800, loss is 4.2711729812622075 and perplexity is 71.60557837966091
At time: 158.25946283340454 and batch: 850, loss is 4.263912801742554 and perplexity is 71.08759163964028
At time: 158.68591332435608 and batch: 900, loss is 4.364815244674682 and perplexity is 78.63487038855213
At time: 159.11270356178284 and batch: 950, loss is 4.2810562896728515 and perplexity is 72.3167871533196
At time: 159.53995084762573 and batch: 1000, loss is 4.255923147201538 and perplexity is 70.52188923296603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.817559218988186 and perplexity of 123.6628880954364
Finished 17 epochs...
Completing Train Step...
At time: 160.7948830127716 and batch: 50, loss is 4.428074159622192 and perplexity is 83.7699339437349
At time: 161.2345769405365 and batch: 100, loss is 4.3486214447021485 and perplexity is 77.37172816651278
At time: 161.6611783504486 and batch: 150, loss is 4.394568004608154 and perplexity is 81.00962741689057
At time: 162.0891308784485 and batch: 200, loss is 4.393304862976074 and perplexity is 80.90736538321387
At time: 162.5156970024109 and batch: 250, loss is 4.369092035293579 and perplexity is 78.97189544363646
At time: 162.95551013946533 and batch: 300, loss is 4.298716878890991 and perplexity is 73.60528858229179
At time: 163.38278532028198 and batch: 350, loss is 4.348061666488648 and perplexity is 77.32842927876193
At time: 163.81035995483398 and batch: 400, loss is 4.269370660781861 and perplexity is 71.47663840971806
At time: 164.2378797531128 and batch: 450, loss is 4.318909816741943 and perplexity is 75.10670357285545
At time: 164.66481065750122 and batch: 500, loss is 4.2607350301742555 and perplexity is 70.86205006167359
At time: 165.0928294658661 and batch: 550, loss is 4.30778805732727 and perplexity is 74.27601281921582
At time: 165.52020525932312 and batch: 600, loss is 4.326157932281494 and perplexity is 75.65306328538425
At time: 165.94879579544067 and batch: 650, loss is 4.307636590003967 and perplexity is 74.26476328235857
At time: 166.37678718566895 and batch: 700, loss is 4.270377039909363 and perplexity is 71.548607214595
At time: 166.80500531196594 and batch: 750, loss is 4.235219769477844 and perplexity is 69.07685805050666
At time: 167.24231243133545 and batch: 800, loss is 4.2688978481292725 and perplexity is 71.44285133882022
At time: 167.67992663383484 and batch: 850, loss is 4.26133707523346 and perplexity is 70.90472505364333
At time: 168.1058964729309 and batch: 900, loss is 4.363011283874512 and perplexity is 78.49314403767005
At time: 168.53398513793945 and batch: 950, loss is 4.280045585632324 and perplexity is 72.24373320852324
At time: 168.9611005783081 and batch: 1000, loss is 4.25422854423523 and perplexity is 70.40248383123065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.817179610089558 and perplexity of 123.61595347164936
Finished 18 epochs...
Completing Train Step...
At time: 170.2916705608368 and batch: 50, loss is 4.422946462631225 and perplexity is 83.34148651857795
At time: 170.7300670146942 and batch: 100, loss is 4.343740701675415 and perplexity is 76.99501670753483
At time: 171.15547919273376 and batch: 150, loss is 4.389773139953613 and perplexity is 80.62212696622848
At time: 171.58192443847656 and batch: 200, loss is 4.387937202453613 and perplexity is 80.47424557203823
At time: 172.00792407989502 and batch: 250, loss is 4.364824094772339 and perplexity is 78.6355663179138
At time: 172.4348874092102 and batch: 300, loss is 4.294376373291016 and perplexity is 73.28649677432908
At time: 172.86055159568787 and batch: 350, loss is 4.343945379257202 and perplexity is 77.01077747424698
At time: 173.2872769832611 and batch: 400, loss is 4.265247955322265 and perplexity is 71.18256788191697
At time: 173.71386432647705 and batch: 450, loss is 4.315012998580933 and perplexity is 74.81459592120257
At time: 174.1536726951599 and batch: 500, loss is 4.256525139808655 and perplexity is 70.56435566988227
At time: 174.59380650520325 and batch: 550, loss is 4.304142217636109 and perplexity is 74.00570742805174
At time: 175.0234363079071 and batch: 600, loss is 4.323427753448486 and perplexity is 75.44679859122391
At time: 175.46780490875244 and batch: 650, loss is 4.304742617607117 and perplexity is 74.05015379410922
At time: 175.91251373291016 and batch: 700, loss is 4.268048801422119 and perplexity is 71.38221876472744
At time: 176.35118222236633 and batch: 750, loss is 4.233545355796814 and perplexity is 68.96129159437267
At time: 176.78462147712708 and batch: 800, loss is 4.266332683563232 and perplexity is 71.25982351667227
At time: 177.2127034664154 and batch: 850, loss is 4.25882257938385 and perplexity is 70.72665938329389
At time: 177.65271162986755 and batch: 900, loss is 4.361037034988403 and perplexity is 78.33833190461087
At time: 178.0832016468048 and batch: 950, loss is 4.278426027297973 and perplexity is 72.12682496371576
At time: 178.50859141349792 and batch: 1000, loss is 4.252586116790772 and perplexity is 70.2869477654031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.816615779225419 and perplexity of 123.54627462716958
Finished 19 epochs...
Completing Train Step...
At time: 179.77552962303162 and batch: 50, loss is 4.4183406639099125 and perplexity is 82.95851502834817
At time: 180.20201301574707 and batch: 100, loss is 4.339403657913208 and perplexity is 76.66180904101962
At time: 180.62831568717957 and batch: 150, loss is 4.385310506820678 and perplexity is 80.26314159695167
At time: 181.05559468269348 and batch: 200, loss is 4.383193378448486 and perplexity is 80.093393974793
At time: 181.49785256385803 and batch: 250, loss is 4.360477485656738 and perplexity is 78.29451000475069
At time: 181.93306970596313 and batch: 300, loss is 4.2902860260009765 and perplexity is 72.98734179183297
At time: 182.35993027687073 and batch: 350, loss is 4.339961137771606 and perplexity is 76.70455837030622
At time: 182.78535962104797 and batch: 400, loss is 4.261711235046387 and perplexity is 70.93125971609767
At time: 183.21275973320007 and batch: 450, loss is 4.310970935821533 and perplexity is 74.5128009771227
At time: 183.63740253448486 and batch: 500, loss is 4.252983961105347 and perplexity is 70.31491659121066
At time: 184.06429839134216 and batch: 550, loss is 4.300339403152466 and perplexity is 73.72481188729927
At time: 184.4911494255066 and batch: 600, loss is 4.320262842178344 and perplexity is 75.20839363233885
At time: 184.93004751205444 and batch: 650, loss is 4.301574001312256 and perplexity is 73.81588861439748
At time: 185.35617399215698 and batch: 700, loss is 4.265476570129395 and perplexity is 71.19884313125489
At time: 185.78347539901733 and batch: 750, loss is 4.23107458114624 and perplexity is 68.79111410491792
At time: 186.20938396453857 and batch: 800, loss is 4.263981342315674 and perplexity is 71.09246419089482
At time: 186.63642072677612 and batch: 850, loss is 4.256235752105713 and perplexity is 70.54393816751491
At time: 187.06361269950867 and batch: 900, loss is 4.359141569137574 and perplexity is 78.18998490942145
At time: 187.49029850959778 and batch: 950, loss is 4.276763038635254 and perplexity is 72.00697855075609
At time: 187.91742205619812 and batch: 1000, loss is 4.250731868743896 and perplexity is 70.15673908668971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.816494081078506 and perplexity of 123.53124018933964
Finished 20 epochs...
Completing Train Step...
At time: 189.17159175872803 and batch: 50, loss is 4.4141342735290525 and perplexity is 82.61029202311884
At time: 189.61081504821777 and batch: 100, loss is 4.335157136917115 and perplexity is 76.33695330086427
At time: 190.03733038902283 and batch: 150, loss is 4.381478977203369 and perplexity is 79.9561993973174
At time: 190.4640669822693 and batch: 200, loss is 4.3787209415435795 and perplexity is 79.73598117261227
At time: 190.89009428024292 and batch: 250, loss is 4.356649560928345 and perplexity is 77.9953774076048
At time: 191.31733226776123 and batch: 300, loss is 4.28655821800232 and perplexity is 72.71576550193132
At time: 191.74419379234314 and batch: 350, loss is 4.33639965057373 and perplexity is 76.43186195828618
At time: 192.1715793609619 and batch: 400, loss is 4.25848084449768 and perplexity is 70.70249374575627
At time: 192.59925985336304 and batch: 450, loss is 4.307644510269165 and perplexity is 74.265351481308
At time: 193.02673268318176 and batch: 500, loss is 4.249529266357422 and perplexity is 70.0724191366857
At time: 193.45379519462585 and batch: 550, loss is 4.297130575180054 and perplexity is 73.48862079960227
At time: 193.88149619102478 and batch: 600, loss is 4.317477283477783 and perplexity is 74.99918775000879
At time: 194.30979800224304 and batch: 650, loss is 4.298937306404114 and perplexity is 73.6215150013156
At time: 194.73663997650146 and batch: 700, loss is 4.262935242652893 and perplexity is 71.01813327363072
At time: 195.16434001922607 and batch: 750, loss is 4.228892154693604 and perplexity is 68.64114626420489
At time: 195.59362626075745 and batch: 800, loss is 4.261888370513916 and perplexity is 70.94382527081979
At time: 196.04179120063782 and batch: 850, loss is 4.253992404937744 and perplexity is 70.38586100088648
At time: 196.46938037872314 and batch: 900, loss is 4.357303075790405 and perplexity is 78.04636520473963
At time: 196.89683651924133 and batch: 950, loss is 4.275380811691284 and perplexity is 71.90751731968159
At time: 197.3234086036682 and batch: 1000, loss is 4.24888879776001 and perplexity is 70.0275543214702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.816381314905678 and perplexity of 123.51731082955386
Finished 21 epochs...
Completing Train Step...
At time: 198.5784146785736 and batch: 50, loss is 4.410019731521606 and perplexity is 82.27108682191354
At time: 199.01648020744324 and batch: 100, loss is 4.331598577499389 and perplexity is 76.06578648454553
At time: 199.4440393447876 and batch: 150, loss is 4.377638244628907 and perplexity is 79.649697989505
At time: 199.87038850784302 and batch: 200, loss is 4.3746300888061525 and perplexity is 79.41045930075047
At time: 200.29824471473694 and batch: 250, loss is 4.353140020370484 and perplexity is 77.7221292355019
At time: 200.73194766044617 and batch: 300, loss is 4.282888822555542 and perplexity is 72.44943154423233
At time: 201.1586308479309 and batch: 350, loss is 4.332821931838989 and perplexity is 76.15889883762443
At time: 201.58536767959595 and batch: 400, loss is 4.2549107646942135 and perplexity is 70.45053023331934
At time: 202.0135326385498 and batch: 450, loss is 4.304037761688233 and perplexity is 73.99797749545984
At time: 202.44047164916992 and batch: 500, loss is 4.24616307258606 and perplexity is 69.8369383551713
At time: 202.86605548858643 and batch: 550, loss is 4.2939379787445064 and perplexity is 73.25437541523924
At time: 203.29130482673645 and batch: 600, loss is 4.314940052032471 and perplexity is 74.80913865370236
At time: 203.7167820930481 and batch: 650, loss is 4.2961110496521 and perplexity is 73.41373545492975
At time: 204.14289569854736 and batch: 700, loss is 4.260876631736755 and perplexity is 70.87208494914535
At time: 204.56960916519165 and batch: 750, loss is 4.2267537021636965 and perplexity is 68.49451726677333
At time: 204.9957869052887 and batch: 800, loss is 4.259763956069946 and perplexity is 70.7932711599904
At time: 205.4218511581421 and batch: 850, loss is 4.251647787094116 and perplexity is 70.22102636786681
At time: 205.84818840026855 and batch: 900, loss is 4.354999856948853 and perplexity is 77.86681419789015
At time: 206.27518701553345 and batch: 950, loss is 4.2737108325958255 and perplexity is 71.78753348208622
At time: 206.71429538726807 and batch: 1000, loss is 4.246922445297241 and perplexity is 69.88999076111597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.816395829363567 and perplexity of 123.51910362937117
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 207.9780547618866 and batch: 50, loss is 4.407056484222412 and perplexity is 82.02765809397266
At time: 208.40591549873352 and batch: 100, loss is 4.327046623229981 and perplexity is 75.72032536112339
At time: 208.85058164596558 and batch: 150, loss is 4.3734908771514895 and perplexity is 79.32004549002467
At time: 209.29961800575256 and batch: 200, loss is 4.3685469150543215 and perplexity is 78.9288579964551
At time: 209.73262357711792 and batch: 250, loss is 4.346870174407959 and perplexity is 77.23634793565337
At time: 210.16337323188782 and batch: 300, loss is 4.276484031677246 and perplexity is 71.98689090514202
At time: 210.59022212028503 and batch: 350, loss is 4.323618240356446 and perplexity is 75.46117158749333
At time: 211.01530766487122 and batch: 400, loss is 4.2430304908752445 and perplexity is 69.6185107391385
At time: 211.44178676605225 and batch: 450, loss is 4.29241639137268 and perplexity is 73.1429972399777
At time: 211.86757016181946 and batch: 500, loss is 4.232138600349426 and perplexity is 68.86434812562706
At time: 212.29446053504944 and batch: 550, loss is 4.279660186767578 and perplexity is 72.21589592033385
At time: 212.72035217285156 and batch: 600, loss is 4.299547643661499 and perplexity is 73.66646267008915
At time: 213.14778017997742 and batch: 650, loss is 4.2790212821960445 and perplexity is 72.16977159038457
At time: 213.57453560829163 and batch: 700, loss is 4.242939901351929 and perplexity is 69.6122043170889
At time: 214.0007975101471 and batch: 750, loss is 4.208461155891419 and perplexity is 67.25296830629307
At time: 214.42642736434937 and batch: 800, loss is 4.237446708679199 and perplexity is 69.23085942581126
At time: 214.85279536247253 and batch: 850, loss is 4.2270118570327755 and perplexity is 68.5122017424795
At time: 215.27917671203613 and batch: 900, loss is 4.330245943069458 and perplexity is 75.962966837181
At time: 215.7045328617096 and batch: 950, loss is 4.246647439002991 and perplexity is 69.87077321634554
At time: 216.13104581832886 and batch: 1000, loss is 4.219971752166748 and perplexity is 68.03156251804296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.812048935308689 and perplexity of 122.983344459934
Finished 23 epochs...
Completing Train Step...
At time: 217.38519048690796 and batch: 50, loss is 4.402187986373901 and perplexity is 81.62927716225568
At time: 217.82268071174622 and batch: 100, loss is 4.322658977508545 and perplexity is 75.38881919715851
At time: 218.2488784790039 and batch: 150, loss is 4.369401369094849 and perplexity is 78.99632789894484
At time: 218.6748309135437 and batch: 200, loss is 4.364989328384399 and perplexity is 78.64856063009192
At time: 219.11381435394287 and batch: 250, loss is 4.3434072971344 and perplexity is 76.96935049818212
At time: 219.54654908180237 and batch: 300, loss is 4.272937507629394 and perplexity is 71.73203985023707
At time: 219.9734857082367 and batch: 350, loss is 4.320790271759034 and perplexity is 75.24807122650529
At time: 220.39916610717773 and batch: 400, loss is 4.240559616088867 and perplexity is 69.44670445959977
At time: 220.8258090019226 and batch: 450, loss is 4.2900048446655275 and perplexity is 72.96682199862369
At time: 221.25192046165466 and batch: 500, loss is 4.230183439254761 and perplexity is 68.72983876794243
At time: 221.6788182258606 and batch: 550, loss is 4.277885541915894 and perplexity is 72.08785200227379
At time: 222.10579323768616 and batch: 600, loss is 4.297823715209961 and perplexity is 73.53957636205105
At time: 222.53258728981018 and batch: 650, loss is 4.277510013580322 and perplexity is 72.06078605352985
At time: 222.95877718925476 and batch: 700, loss is 4.241789646148682 and perplexity is 69.53217855071418
At time: 223.38569140434265 and batch: 750, loss is 4.207571845054627 and perplexity is 67.1931860991942
At time: 223.81197881698608 and batch: 800, loss is 4.236822929382324 and perplexity is 69.18768811508133
At time: 224.23840928077698 and batch: 850, loss is 4.226907958984375 and perplexity is 68.50508382820196
At time: 224.66482210159302 and batch: 900, loss is 4.33086968421936 and perplexity is 76.0103628453497
At time: 225.09040117263794 and batch: 950, loss is 4.247971959114075 and perplexity is 69.96337977673471
At time: 225.5160732269287 and batch: 1000, loss is 4.221121120452881 and perplexity is 68.10980079213974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.811214167897294 and perplexity of 122.88072480956352
Finished 24 epochs...
Completing Train Step...
At time: 226.78311276435852 and batch: 50, loss is 4.400037746429444 and perplexity is 81.45394320246136
At time: 227.23558139801025 and batch: 100, loss is 4.320374183654785 and perplexity is 75.21676791212028
At time: 227.67080950737 and batch: 150, loss is 4.367009410858154 and perplexity is 78.80759778901317
At time: 228.0985746383667 and batch: 200, loss is 4.363107109069825 and perplexity is 78.5006660189201
At time: 228.5263340473175 and batch: 250, loss is 4.341534032821655 and perplexity is 76.82530152374403
At time: 228.96657967567444 and batch: 300, loss is 4.270928354263305 and perplexity is 71.58806386426626
At time: 229.39416337013245 and batch: 350, loss is 4.319078502655029 and perplexity is 75.11937408436485
At time: 229.82171893119812 and batch: 400, loss is 4.2390982532501225 and perplexity is 69.34529174486349
At time: 230.24965858459473 and batch: 450, loss is 4.288514890670776 and perplexity is 72.85818574231196
At time: 230.67710065841675 and batch: 500, loss is 4.228985261917114 and perplexity is 68.64753754828487
At time: 231.10352873802185 and batch: 550, loss is 4.276911869049072 and perplexity is 72.01769617670614
At time: 231.52948093414307 and batch: 600, loss is 4.2967955589294435 and perplexity is 73.46400504096927
At time: 231.95607256889343 and batch: 650, loss is 4.276584672927856 and perplexity is 71.99413612044793
At time: 232.38364672660828 and batch: 700, loss is 4.241089768409729 and perplexity is 69.48353155226701
At time: 232.81079244613647 and batch: 750, loss is 4.2070005941390995 and perplexity is 67.15481289152635
At time: 233.238347530365 and batch: 800, loss is 4.236481266021729 and perplexity is 69.1640532548609
At time: 233.665141582489 and batch: 850, loss is 4.22688766002655 and perplexity is 68.50369326050813
At time: 234.0919280052185 and batch: 900, loss is 4.3313456153869625 and perplexity is 76.04654715602616
At time: 234.5191788673401 and batch: 950, loss is 4.248862409591675 and perplexity is 70.02570644695982
At time: 234.94730377197266 and batch: 1000, loss is 4.221750135421753 and perplexity is 68.15265635334663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.810741517601944 and perplexity of 122.8226589221998
Finished 25 epochs...
Completing Train Step...
At time: 236.20981740951538 and batch: 50, loss is 4.398532228469849 and perplexity is 81.3314050928978
At time: 236.6361563205719 and batch: 100, loss is 4.31872407913208 and perplexity is 75.09275472870172
At time: 237.062495470047 and batch: 150, loss is 4.365268621444702 and perplexity is 78.67052969503962
At time: 237.48827409744263 and batch: 200, loss is 4.361747817993164 and perplexity is 78.39403325299081
At time: 237.9148154258728 and batch: 250, loss is 4.340267963409424 and perplexity is 76.72809690628021
At time: 238.3387999534607 and batch: 300, loss is 4.269580411911011 and perplexity is 71.4916322877688
At time: 238.76410388946533 and batch: 350, loss is 4.317868061065674 and perplexity is 75.02850147889265
At time: 239.18919706344604 and batch: 400, loss is 4.23811559677124 and perplexity is 69.27718261407033
At time: 239.62731552124023 and batch: 450, loss is 4.2874283027648925 and perplexity is 72.77906191412205
At time: 240.05182266235352 and batch: 500, loss is 4.228068876266479 and perplexity is 68.58465874494102
At time: 240.47782468795776 and batch: 550, loss is 4.27617841720581 and perplexity is 71.9648940309805
At time: 240.9038803577423 and batch: 600, loss is 4.296059360504151 and perplexity is 73.40994085956685
At time: 241.32979106903076 and batch: 650, loss is 4.275882396697998 and perplexity is 71.94359409926867
At time: 241.7552924156189 and batch: 700, loss is 4.240583281517029 and perplexity is 69.4483479650423
At time: 242.18101835250854 and batch: 750, loss is 4.206595268249512 and perplexity is 67.12759882290685
At time: 242.60706520080566 and batch: 800, loss is 4.236171855926513 and perplexity is 69.14265650891315
At time: 243.04109907150269 and batch: 850, loss is 4.226807131767273 and perplexity is 68.49817699944619
At time: 243.48411440849304 and batch: 900, loss is 4.331648559570312 and perplexity is 76.06958850509604
At time: 243.91432976722717 and batch: 950, loss is 4.249445214271545 and perplexity is 70.06652965121003
At time: 244.34130859375 and batch: 1000, loss is 4.222056798934936 and perplexity is 68.17355949132777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.810415872713414 and perplexity of 122.78266886275883
Finished 26 epochs...
Completing Train Step...
At time: 245.59513401985168 and batch: 50, loss is 4.397292728424072 and perplexity is 81.23065726393015
At time: 246.03278493881226 and batch: 100, loss is 4.317352933883667 and perplexity is 74.98986221127855
At time: 246.45819067955017 and batch: 150, loss is 4.363879184722901 and perplexity is 78.56129787508165
At time: 246.88362002372742 and batch: 200, loss is 4.3606422996521 and perplexity is 78.30741509920033
At time: 247.30977964401245 and batch: 250, loss is 4.339274549484253 and perplexity is 76.65191199420332
At time: 247.73561930656433 and batch: 300, loss is 4.2684900760650635 and perplexity is 71.41372487873878
At time: 248.16221594810486 and batch: 350, loss is 4.316880521774292 and perplexity is 74.95444445883582
At time: 248.5883891582489 and batch: 400, loss is 4.23729362487793 and perplexity is 69.22026211384433
At time: 249.0150499343872 and batch: 450, loss is 4.286557149887085 and perplexity is 72.71568783315588
At time: 249.4419174194336 and batch: 500, loss is 4.227283325195312 and perplexity is 68.53080314872844
At time: 249.86788058280945 and batch: 550, loss is 4.275521850585937 and perplexity is 71.9176597916663
At time: 250.294358253479 and batch: 600, loss is 4.295460996627807 and perplexity is 73.36602814199414
At time: 250.73424768447876 and batch: 650, loss is 4.275274596214294 and perplexity is 71.89988003403647
At time: 251.16153025627136 and batch: 700, loss is 4.2401618385314945 and perplexity is 69.41908561257517
At time: 251.58892703056335 and batch: 750, loss is 4.206211152076722 and perplexity is 67.10181897810304
At time: 252.01489996910095 and batch: 800, loss is 4.235826568603516 and perplexity is 69.11878654737696
At time: 252.4408402442932 and batch: 850, loss is 4.226668601036072 and perplexity is 68.48868855413633
At time: 252.86611008644104 and batch: 900, loss is 4.331827583312989 and perplexity is 76.08320798660299
At time: 253.29198336601257 and batch: 950, loss is 4.249796509742737 and perplexity is 70.09114802966788
At time: 253.7171607017517 and batch: 1000, loss is 4.222207536697388 and perplexity is 68.18383659569825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.81015312380907 and perplexity of 122.75041208894426
Finished 27 epochs...
Completing Train Step...
At time: 254.9704933166504 and batch: 50, loss is 4.3962532424926755 and perplexity is 81.14626300942551
At time: 255.4103400707245 and batch: 100, loss is 4.3161561489105225 and perplexity is 74.90016915340343
At time: 255.83674693107605 and batch: 150, loss is 4.362704582214356 and perplexity is 78.46907375146989
At time: 256.26486372947693 and batch: 200, loss is 4.3596653175354 and perplexity is 78.23094751485696
At time: 256.6929054260254 and batch: 250, loss is 4.338417367935181 and perplexity is 76.5862355418837
At time: 257.12051033973694 and batch: 300, loss is 4.267558727264404 and perplexity is 71.34724475461977
At time: 257.54876685142517 and batch: 350, loss is 4.316031646728516 and perplexity is 74.89084449939308
At time: 257.97703862190247 and batch: 400, loss is 4.236560869216919 and perplexity is 69.16955915363295
At time: 258.4054958820343 and batch: 450, loss is 4.285796508789063 and perplexity is 72.66039832292167
At time: 258.83327651023865 and batch: 500, loss is 4.226589021682739 and perplexity is 68.48323848544993
At time: 259.25925064086914 and batch: 550, loss is 4.2749282741546635 and perplexity is 71.87498383079691
At time: 259.68619084358215 and batch: 600, loss is 4.294954805374146 and perplexity is 73.32890029792051
At time: 260.13625955581665 and batch: 650, loss is 4.274731750488281 and perplexity is 71.8608600833241
At time: 260.5821077823639 and batch: 700, loss is 4.239768033027649 and perplexity is 69.39175337673468
At time: 261.01460886001587 and batch: 750, loss is 4.2058380651474 and perplexity is 67.07678883600839
At time: 261.4631552696228 and batch: 800, loss is 4.235457715988159 and perplexity is 69.09329660349388
At time: 261.8893880844116 and batch: 850, loss is 4.226477708816528 and perplexity is 68.47561584414362
At time: 262.317507982254 and batch: 900, loss is 4.331890592575073 and perplexity is 76.08800208442993
At time: 262.74481439590454 and batch: 950, loss is 4.2499588680267335 and perplexity is 70.1025288320441
At time: 263.17257738113403 and batch: 1000, loss is 4.222258186340332 and perplexity is 68.18729017013678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809920148151677 and perplexity of 122.7218175620362
Finished 28 epochs...
Completing Train Step...
At time: 264.4774851799011 and batch: 50, loss is 4.395339498519897 and perplexity is 81.07214996601712
At time: 264.9043118953705 and batch: 100, loss is 4.315075426101685 and perplexity is 74.81926655672856
At time: 265.33114981651306 and batch: 150, loss is 4.36161542892456 and perplexity is 78.38365542691487
At time: 265.75709223747253 and batch: 200, loss is 4.358789644241333 and perplexity is 78.16247274848628
At time: 266.1835856437683 and batch: 250, loss is 4.337632255554199 and perplexity is 76.52613033790432
At time: 266.60893082618713 and batch: 300, loss is 4.266746597290039 and perplexity is 71.2893250409174
At time: 267.0352976322174 and batch: 350, loss is 4.315255155563355 and perplexity is 74.83271499173313
At time: 267.4624660015106 and batch: 400, loss is 4.235875253677368 and perplexity is 69.12215168251986
At time: 267.88895416259766 and batch: 450, loss is 4.284979705810547 and perplexity is 72.6010733248672
At time: 268.3155746459961 and batch: 500, loss is 4.225879340171814 and perplexity is 68.43465443892744
At time: 268.7534658908844 and batch: 550, loss is 4.274282846450806 and perplexity is 71.82860869252758
At time: 269.18841457366943 and batch: 600, loss is 4.2944663143157955 and perplexity is 73.29308853338183
At time: 269.61643648147583 and batch: 650, loss is 4.274286246299743 and perplexity is 71.82885289936166
At time: 270.042272567749 and batch: 700, loss is 4.23939245223999 and perplexity is 69.36569606096432
At time: 270.4673318862915 and batch: 750, loss is 4.205510725975037 and perplexity is 67.05483556874421
At time: 270.8906350135803 and batch: 800, loss is 4.235069522857666 and perplexity is 69.06648026568512
At time: 271.31296610832214 and batch: 850, loss is 4.226290760040283 and perplexity is 68.46281560809042
At time: 271.7365298271179 and batch: 900, loss is 4.331812372207642 and perplexity is 76.0820506857131
At time: 272.16038250923157 and batch: 950, loss is 4.24995831489563 and perplexity is 70.10249005616569
At time: 272.5959391593933 and batch: 1000, loss is 4.222295227050782 and perplexity is 68.1898159225859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809722156059451 and perplexity of 122.69752201785758
Finished 29 epochs...
Completing Train Step...
At time: 273.8356432914734 and batch: 50, loss is 4.394480266571045 and perplexity is 81.0025201029895
At time: 274.2701561450958 and batch: 100, loss is 4.314151973724365 and perplexity is 74.7502064189393
At time: 274.6926381587982 and batch: 150, loss is 4.360652694702148 and perplexity is 78.30822911293028
At time: 275.11608505249023 and batch: 200, loss is 4.3579920959472656 and perplexity is 78.10015925399931
At time: 275.5390021800995 and batch: 250, loss is 4.3368724346160885 and perplexity is 76.4680062665006
At time: 275.961966753006 and batch: 300, loss is 4.266007051467896 and perplexity is 71.23662280867983
At time: 276.3851544857025 and batch: 350, loss is 4.314548273086547 and perplexity is 74.7798357487206
At time: 276.8090891838074 and batch: 400, loss is 4.235297431945801 and perplexity is 69.08222293810408
At time: 277.23204469680786 and batch: 450, loss is 4.2843521785736085 and perplexity is 72.5555284657393
At time: 277.6552586555481 and batch: 500, loss is 4.2253059387207035 and perplexity is 68.39542515690437
At time: 278.0784456729889 and batch: 550, loss is 4.273823280334472 and perplexity is 71.79560628176483
At time: 278.50192975997925 and batch: 600, loss is 4.294031686782837 and perplexity is 73.26124026069911
At time: 278.92600560188293 and batch: 650, loss is 4.2738268756866455 and perplexity is 71.79586441271795
At time: 279.35024309158325 and batch: 700, loss is 4.239039950370788 and perplexity is 69.34124883254435
At time: 279.7746567726135 and batch: 750, loss is 4.205179190635681 and perplexity is 67.03260820586021
At time: 280.1983730792999 and batch: 800, loss is 4.234680891036987 and perplexity is 69.03964404875329
At time: 280.62133955955505 and batch: 850, loss is 4.2260728406906125 and perplexity is 68.44789786132839
At time: 281.04418992996216 and batch: 900, loss is 4.331745805740357 and perplexity is 76.0769863409348
At time: 281.4666702747345 and batch: 950, loss is 4.249955587387085 and perplexity is 70.10229885128578
At time: 281.8893520832062 and batch: 1000, loss is 4.22221941947937 and perplexity is 68.18464681417706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809623532178925 and perplexity of 122.68542170880475
Finished 30 epochs...
Completing Train Step...
At time: 283.1278853416443 and batch: 50, loss is 4.393705911636353 and perplexity is 80.93981968114883
At time: 283.5645546913147 and batch: 100, loss is 4.3132671546936034 and perplexity is 74.68409526626272
At time: 283.9909317493439 and batch: 150, loss is 4.359843530654907 and perplexity is 78.24489053843352
At time: 284.41762828826904 and batch: 200, loss is 4.3572354507446285 and perplexity is 78.04108749417469
At time: 284.84391474723816 and batch: 250, loss is 4.3361853504180905 and perplexity is 76.41548435329702
At time: 285.2706849575043 and batch: 300, loss is 4.265345773696899 and perplexity is 71.18953118557343
At time: 285.69759345054626 and batch: 350, loss is 4.313896484375 and perplexity is 74.73111097678137
At time: 286.1251678466797 and batch: 400, loss is 4.23471061706543 and perplexity is 69.04169635367921
At time: 286.5515778064728 and batch: 450, loss is 4.283762283325196 and perplexity is 72.51274092557534
At time: 286.97787380218506 and batch: 500, loss is 4.2247829294204715 and perplexity is 68.35966306622218
At time: 287.4116690158844 and batch: 550, loss is 4.2733619499206545 and perplexity is 71.76249242381077
At time: 287.84358286857605 and batch: 600, loss is 4.293624925613403 and perplexity is 73.2314464928212
At time: 288.26967573165894 and batch: 650, loss is 4.2733505868911745 and perplexity is 71.76167698912671
At time: 288.69538736343384 and batch: 700, loss is 4.238660154342651 and perplexity is 69.31491830208363
At time: 289.12180852890015 and batch: 750, loss is 4.204858269691467 and perplexity is 67.01109948942508
At time: 289.54797983169556 and batch: 800, loss is 4.234274759292602 and perplexity is 69.0116105507161
At time: 289.9745626449585 and batch: 850, loss is 4.2257691621780396 and perplexity is 68.4271148613521
At time: 290.4011013507843 and batch: 900, loss is 4.331611251831054 and perplexity is 76.06675057366137
At time: 290.82738304138184 and batch: 950, loss is 4.24987229347229 and perplexity is 70.09645999955212
At time: 291.2548761367798 and batch: 1000, loss is 4.222083854675293 and perplexity is 68.17540400240475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809528257788681 and perplexity of 122.67373348686259
Finished 31 epochs...
Completing Train Step...
At time: 292.5193655490875 and batch: 50, loss is 4.392985734939575 and perplexity is 80.88154969403502
At time: 292.9466257095337 and batch: 100, loss is 4.3124159240722655 and perplexity is 74.62054892757693
At time: 293.37457489967346 and batch: 150, loss is 4.3590575695037845 and perplexity is 78.18341725516723
At time: 293.8014545440674 and batch: 200, loss is 4.356519050598145 and perplexity is 77.98519886936192
At time: 294.24115920066833 and batch: 250, loss is 4.33554822921753 and perplexity is 76.36681393428866
At time: 294.6675658226013 and batch: 300, loss is 4.264695892333984 and perplexity is 71.14328146606441
At time: 295.10539412498474 and batch: 350, loss is 4.313257694244385 and perplexity is 74.68338872451412
At time: 295.5554449558258 and batch: 400, loss is 4.234145040512085 and perplexity is 69.0026590293567
At time: 295.9916365146637 and batch: 450, loss is 4.28317886352539 and perplexity is 72.47044789527648
At time: 296.4331820011139 and batch: 500, loss is 4.224238319396973 and perplexity is 68.32244384441388
At time: 296.867573261261 and batch: 550, loss is 4.2728976631164555 and perplexity is 71.72918177898674
At time: 297.2954363822937 and batch: 600, loss is 4.293209600448608 and perplexity is 73.20103794539301
At time: 297.7236931324005 and batch: 650, loss is 4.272894496917725 and perplexity is 71.72895467050198
At time: 298.1509039402008 and batch: 700, loss is 4.238328685760498 and perplexity is 69.2919463918356
At time: 298.57922077178955 and batch: 750, loss is 4.204480185508728 and perplexity is 66.98576844157616
At time: 299.0067946910858 and batch: 800, loss is 4.2338778686523435 and perplexity is 68.98422592313
At time: 299.4345951080322 and batch: 850, loss is 4.22546872138977 and perplexity is 68.40655965299028
At time: 299.86189699172974 and batch: 900, loss is 4.331461629867554 and perplexity is 76.05537016848393
At time: 300.28904914855957 and batch: 950, loss is 4.24977753162384 and perplexity is 70.08981784414921
At time: 300.71677327156067 and batch: 1000, loss is 4.221916780471802 and perplexity is 68.16401460254733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80945308034013 and perplexity of 122.66451153522048
Finished 32 epochs...
Completing Train Step...
At time: 301.96850085258484 and batch: 50, loss is 4.3923012638092045 and perplexity is 80.82620755050023
At time: 302.4054961204529 and batch: 100, loss is 4.311613779067994 and perplexity is 74.5607164274174
At time: 302.83050560951233 and batch: 150, loss is 4.3582883548736575 and perplexity is 78.12330055106959
At time: 303.2648193836212 and batch: 200, loss is 4.355820264816284 and perplexity is 77.93072295690969
At time: 303.7005865573883 and batch: 250, loss is 4.334903507232666 and perplexity is 76.31759443858165
At time: 304.1257846355438 and batch: 300, loss is 4.264077348709106 and perplexity is 71.09928984963021
At time: 304.55187487602234 and batch: 350, loss is 4.31264401435852 and perplexity is 74.63757109116361
At time: 304.9783990383148 and batch: 400, loss is 4.2336008644104 and perplexity is 68.96511964630416
At time: 305.4167709350586 and batch: 450, loss is 4.2825796413421635 and perplexity is 72.42703500355213
At time: 305.84370160102844 and batch: 500, loss is 4.223704314231872 and perplexity is 68.28596904624077
At time: 306.2698426246643 and batch: 550, loss is 4.272448978424072 and perplexity is 71.69700521223163
At time: 306.69557189941406 and batch: 600, loss is 4.292754716873169 and perplexity is 73.16774756772361
At time: 307.12204790115356 and batch: 650, loss is 4.272456994056702 and perplexity is 71.69757991138934
At time: 307.5653831958771 and batch: 700, loss is 4.237986168861389 and perplexity is 69.26821679335046
At time: 307.99726581573486 and batch: 750, loss is 4.204127349853516 and perplexity is 66.96213764321746
At time: 308.4223096370697 and batch: 800, loss is 4.233459734916687 and perplexity is 68.95538732065988
At time: 308.847238779068 and batch: 850, loss is 4.225168271064758 and perplexity is 68.38600996714406
At time: 309.272513628006 and batch: 900, loss is 4.331310243606567 and perplexity is 76.04385730183309
At time: 309.69689178466797 and batch: 950, loss is 4.249664950370788 and perplexity is 70.08192748879154
At time: 310.1230194568634 and batch: 1000, loss is 4.221723432540894 and perplexity is 68.15083650538148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80938720703125 and perplexity of 122.65643148409632
Finished 33 epochs...
Completing Train Step...
At time: 311.46505212783813 and batch: 50, loss is 4.391644372940063 and perplexity is 80.77313098743637
At time: 311.91007137298584 and batch: 100, loss is 4.3108759689331055 and perplexity is 74.50572506425989
At time: 312.3492965698242 and batch: 150, loss is 4.3575279426574705 and perplexity is 78.06391721972841
At time: 312.7870569229126 and batch: 200, loss is 4.3551719284057615 and perplexity is 77.8802140068833
At time: 313.2139308452606 and batch: 250, loss is 4.334285411834717 and perplexity is 76.27043745993367
At time: 313.64081382751465 and batch: 300, loss is 4.2634401035308835 and perplexity is 71.05399660298272
At time: 314.0671548843384 and batch: 350, loss is 4.312068338394165 and perplexity is 74.5946164006263
At time: 314.49433636665344 and batch: 400, loss is 4.233159723281861 and perplexity is 68.9347030051025
At time: 314.9206733703613 and batch: 450, loss is 4.282008018493652 and perplexity is 72.38564588610379
At time: 315.3459224700928 and batch: 500, loss is 4.223185510635376 and perplexity is 68.25055122814943
At time: 315.77182602882385 and batch: 550, loss is 4.272042350769043 and perplexity is 71.66785715373453
At time: 316.2099885940552 and batch: 600, loss is 4.292337789535522 and perplexity is 73.13724829196128
At time: 316.635945558548 and batch: 650, loss is 4.272000632286072 and perplexity is 71.66486734182192
At time: 317.0603804588318 and batch: 700, loss is 4.2376578903198245 and perplexity is 69.24548125616343
At time: 317.48583245277405 and batch: 750, loss is 4.203771257400513 and perplexity is 66.93829717632286
At time: 317.9115343093872 and batch: 800, loss is 4.233056306838989 and perplexity is 68.92757439194158
At time: 318.33802032470703 and batch: 850, loss is 4.2248861026763915 and perplexity is 68.3667163190816
At time: 318.766348361969 and batch: 900, loss is 4.3311324787139895 and perplexity is 76.03034057514397
At time: 319.1935739517212 and batch: 950, loss is 4.2495540380477905 and perplexity is 70.07415497045558
At time: 319.62036895751953 and batch: 1000, loss is 4.221528940200805 and perplexity is 68.13758297860707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8093150069073936 and perplexity of 122.64757599423896
Finished 34 epochs...
Completing Train Step...
At time: 320.8923590183258 and batch: 50, loss is 4.39100848197937 and perplexity is 80.72178441071681
At time: 321.32148361206055 and batch: 100, loss is 4.310199279785156 and perplexity is 74.45532490319303
At time: 321.7525825500488 and batch: 150, loss is 4.356801261901856 and perplexity is 78.00721027979333
At time: 322.1822578907013 and batch: 200, loss is 4.354557886123657 and perplexity is 77.83240694182551
At time: 322.61120772361755 and batch: 250, loss is 4.333714246749878 and perplexity is 76.22688688751832
At time: 323.04804134368896 and batch: 300, loss is 4.2628844165802 and perplexity is 71.0145237925551
At time: 323.4923310279846 and batch: 350, loss is 4.311531572341919 and perplexity is 74.55458728695788
At time: 323.92692947387695 and batch: 400, loss is 4.232695913314819 and perplexity is 68.90273781623331
At time: 324.3552920818329 and batch: 450, loss is 4.281496486663818 and perplexity is 72.3486277929836
At time: 324.7846643924713 and batch: 500, loss is 4.222650947570801 and perplexity is 68.2140767541479
At time: 325.21449279785156 and batch: 550, loss is 4.27163402557373 and perplexity is 71.63859933572323
At time: 325.64394211769104 and batch: 600, loss is 4.291881561279297 and perplexity is 73.10388862309834
At time: 326.07395935058594 and batch: 650, loss is 4.271509671211243 and perplexity is 71.62969131726031
At time: 326.50381207466125 and batch: 700, loss is 4.237336626052857 and perplexity is 69.2232387304425
At time: 326.9560236930847 and batch: 750, loss is 4.203427839279175 and perplexity is 66.91531329882689
At time: 327.38622784614563 and batch: 800, loss is 4.2326633262634275 and perplexity is 68.90049251575913
At time: 327.8163118362427 and batch: 850, loss is 4.224603357315064 and perplexity is 68.34738867970212
At time: 328.2524747848511 and batch: 900, loss is 4.330930299758911 and perplexity is 76.01497039414818
At time: 328.69982647895813 and batch: 950, loss is 4.2493790435791015 and perplexity is 70.06189345381777
At time: 329.15262031555176 and batch: 1000, loss is 4.221314449310302 and perplexity is 68.12296965502568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809222709841844 and perplexity of 122.63625650526377
Finished 35 epochs...
Completing Train Step...
At time: 330.45602560043335 and batch: 50, loss is 4.390364818572998 and perplexity is 80.6698434700292
At time: 330.90591049194336 and batch: 100, loss is 4.309513502120971 and perplexity is 74.40428260822812
At time: 331.33430099487305 and batch: 150, loss is 4.35610613822937 and perplexity is 77.95300446335902
At time: 331.76367473602295 and batch: 200, loss is 4.353970260620117 and perplexity is 77.78668406978313
At time: 332.1906027793884 and batch: 250, loss is 4.333152008056641 and perplexity is 76.1840412281176
At time: 332.6167850494385 and batch: 300, loss is 4.262383184432983 and perplexity is 70.97893794944028
At time: 333.0456838607788 and batch: 350, loss is 4.311002597808838 and perplexity is 74.51516023783053
At time: 333.4750020503998 and batch: 400, loss is 4.232148199081421 and perplexity is 68.86500913922117
At time: 333.9133062362671 and batch: 450, loss is 4.281010847091675 and perplexity is 72.31350096651595
At time: 334.35264015197754 and batch: 500, loss is 4.222117161750793 and perplexity is 68.1776747635491
At time: 334.78592920303345 and batch: 550, loss is 4.271258935928345 and perplexity is 71.61173347777004
At time: 335.2188723087311 and batch: 600, loss is 4.291430578231812 and perplexity is 73.07092744164977
At time: 335.65371227264404 and batch: 650, loss is 4.2709823417663575 and perplexity is 71.5919288294116
At time: 336.0865800380707 and batch: 700, loss is 4.237028069496155 and perplexity is 69.20188274119079
At time: 336.5130093097687 and batch: 750, loss is 4.203075284957886 and perplexity is 66.89172617407868
At time: 336.937824010849 and batch: 800, loss is 4.23225802898407 and perplexity is 68.87257299183185
At time: 337.36286759376526 and batch: 850, loss is 4.224294610023499 and perplexity is 68.32628986583015
At time: 337.7886574268341 and batch: 900, loss is 4.330643005371094 and perplexity is 75.99313485652773
At time: 338.2278914451599 and batch: 950, loss is 4.249173064231872 and perplexity is 70.04746363691166
At time: 338.65367794036865 and batch: 1000, loss is 4.2210878658294675 and perplexity is 68.10753586402352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8091237137957314 and perplexity of 122.62411660167085
Finished 36 epochs...
Completing Train Step...
At time: 339.9063265323639 and batch: 50, loss is 4.389667110443115 and perplexity is 80.61357909474229
At time: 340.3451042175293 and batch: 100, loss is 4.308866968154907 and perplexity is 74.35619325967504
At time: 340.78656911849976 and batch: 150, loss is 4.35544864654541 and perplexity is 77.90176785685182
At time: 341.217581987381 and batch: 200, loss is 4.353392601013184 and perplexity is 77.74176282028355
At time: 341.6444709300995 and batch: 250, loss is 4.332590656280518 and perplexity is 76.1412871824144
At time: 342.07112669944763 and batch: 300, loss is 4.261875505447388 and perplexity is 70.94291257965881
At time: 342.4975984096527 and batch: 350, loss is 4.310480089187622 and perplexity is 74.47623559431143
At time: 342.9235360622406 and batch: 400, loss is 4.23162582397461 and perplexity is 68.82904516687624
At time: 343.3487720489502 and batch: 450, loss is 4.28042459487915 and perplexity is 72.27111944092434
At time: 343.77387285232544 and batch: 500, loss is 4.221611342430115 and perplexity is 68.14319789868205
At time: 344.2002966403961 and batch: 550, loss is 4.270927610397339 and perplexity is 71.58801061236178
At time: 344.6428689956665 and batch: 600, loss is 4.290975942611694 and perplexity is 73.03771434574006
At time: 345.08941173553467 and batch: 650, loss is 4.270452098846436 and perplexity is 71.55397777855345
At time: 345.5370788574219 and batch: 700, loss is 4.236699237823486 and perplexity is 69.17913071133619
At time: 345.9717698097229 and batch: 750, loss is 4.2027294731140135 and perplexity is 66.86859822210096
At time: 346.40606713294983 and batch: 800, loss is 4.231844086647033 and perplexity is 68.8440696177935
At time: 346.84340167045593 and batch: 850, loss is 4.223973374366761 and perplexity is 68.30434455023027
At time: 347.2710540294647 and batch: 900, loss is 4.330287313461303 and perplexity is 75.96610951989136
At time: 347.7032690048218 and batch: 950, loss is 4.2489512920379635 and perplexity is 70.03193077966475
At time: 348.134152173996 and batch: 1000, loss is 4.220849313735962 and perplexity is 68.09129060650679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.809087613733803 and perplexity of 122.61968994336944
Finished 37 epochs...
Completing Train Step...
At time: 349.42185616493225 and batch: 50, loss is 4.389107151031494 and perplexity is 80.56845139844202
At time: 349.85487937927246 and batch: 100, loss is 4.308201570510864 and perplexity is 74.30673328096154
At time: 350.28130626678467 and batch: 150, loss is 4.354748764038086 and perplexity is 77.84726484731546
At time: 350.70651626586914 and batch: 200, loss is 4.35285005569458 and perplexity is 77.69959583059116
At time: 351.13278365135193 and batch: 250, loss is 4.332048320770264 and perplexity is 76.1000042541898
At time: 351.5593798160553 and batch: 300, loss is 4.261390323638916 and perplexity is 70.90850071771492
At time: 351.9863772392273 and batch: 350, loss is 4.309947061538696 and perplexity is 74.43654827970941
At time: 352.41436767578125 and batch: 400, loss is 4.23110071182251 and perplexity is 68.79291168673676
At time: 352.843079328537 and batch: 450, loss is 4.279829874038696 and perplexity is 72.22815107838666
At time: 353.2706377506256 and batch: 500, loss is 4.221123852729797 and perplexity is 68.10998688723048
At time: 353.69927525520325 and batch: 550, loss is 4.270634756088257 and perplexity is 71.56704882450009
At time: 354.12759137153625 and batch: 600, loss is 4.290536336898803 and perplexity is 73.00561360560854
At time: 354.5565323829651 and batch: 650, loss is 4.2699620819091795 and perplexity is 71.51892370675978
At time: 354.9849090576172 and batch: 700, loss is 4.236379270553589 and perplexity is 69.15699919461792
At time: 355.4138901233673 and batch: 750, loss is 4.202377786636353 and perplexity is 66.84508557510846
At time: 355.8423500061035 and batch: 800, loss is 4.231407370567322 and perplexity is 68.81401086964568
At time: 356.2696053981781 and batch: 850, loss is 4.223664889335632 and perplexity is 68.28327693206496
At time: 356.6961362361908 and batch: 900, loss is 4.3299777221679685 and perplexity is 75.94259471397288
At time: 357.12207794189453 and batch: 950, loss is 4.2487059545516965 and perplexity is 70.01475142926434
At time: 357.54837799072266 and batch: 1000, loss is 4.220600662231445 and perplexity is 68.07436170943805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8090388600419205 and perplexity of 122.61371192651356
Finished 38 epochs...
Completing Train Step...
At time: 358.8124113082886 and batch: 50, loss is 4.3885348033905025 and perplexity is 80.52235142920856
At time: 359.25324296951294 and batch: 100, loss is 4.3075572681427 and perplexity is 74.25887269673767
At time: 359.6801414489746 and batch: 150, loss is 4.354049034118653 and perplexity is 77.7928118403766
At time: 360.1203429698944 and batch: 200, loss is 4.352285289764405 and perplexity is 77.65572613529837
At time: 360.54749488830566 and batch: 250, loss is 4.3315184211730955 and perplexity is 76.05968957490047
At time: 360.9760193824768 and batch: 300, loss is 4.260907726287842 and perplexity is 70.87428871907386
At time: 361.40342807769775 and batch: 350, loss is 4.309421262741089 and perplexity is 74.39741991984965
At time: 361.83153891563416 and batch: 400, loss is 4.230601320266723 and perplexity is 68.75856566431302
At time: 362.2603209018707 and batch: 450, loss is 4.279320211410522 and perplexity is 72.19134846833404
At time: 362.6890273094177 and batch: 500, loss is 4.2207126760482785 and perplexity is 68.08198740560533
At time: 363.1173791885376 and batch: 550, loss is 4.270299959182739 and perplexity is 71.54309240850733
At time: 363.5456461906433 and batch: 600, loss is 4.290114555358887 and perplexity is 72.9748276784037
At time: 363.9735162258148 and batch: 650, loss is 4.269512367248535 and perplexity is 71.48676782928206
At time: 364.40118622779846 and batch: 700, loss is 4.236035189628601 and perplexity is 69.13320768370293
At time: 364.82801032066345 and batch: 750, loss is 4.202045683860779 and perplexity is 66.82288982249501
At time: 365.2537806034088 and batch: 800, loss is 4.230973391532898 and perplexity is 68.78415351085614
At time: 365.68101596832275 and batch: 850, loss is 4.223390469551086 and perplexity is 68.26454122076369
At time: 366.1095657348633 and batch: 900, loss is 4.329688968658448 and perplexity is 75.92066918891561
At time: 366.5366847515106 and batch: 950, loss is 4.248464241027832 and perplexity is 69.99782996212996
At time: 366.9638249874115 and batch: 1000, loss is 4.220296115875244 and perplexity is 68.05363306720518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808969265077172 and perplexity of 122.60517892648471
Finished 39 epochs...
Completing Train Step...
At time: 368.2175703048706 and batch: 50, loss is 4.388002805709839 and perplexity is 80.47952511776586
At time: 368.65660405158997 and batch: 100, loss is 4.3069642066955565 and perplexity is 74.21484567888677
At time: 369.0832767486572 and batch: 150, loss is 4.3533045673370365 and perplexity is 77.73491922835004
At time: 369.50997591018677 and batch: 200, loss is 4.351672344207763 and perplexity is 77.60814198775168
At time: 369.93728041648865 and batch: 250, loss is 4.33103162765503 and perplexity is 76.02267322142075
At time: 370.3629364967346 and batch: 300, loss is 4.260371246337891 and perplexity is 70.836276281576
At time: 370.7887530326843 and batch: 350, loss is 4.3089025020599365 and perplexity is 74.35883547252844
At time: 371.22763204574585 and batch: 400, loss is 4.230118293762207 and perplexity is 68.72536147458203
At time: 371.6522512435913 and batch: 450, loss is 4.278777523040771 and perplexity is 72.15218169176197
At time: 372.0770072937012 and batch: 500, loss is 4.220277543067932 and perplexity is 68.05236913192881
At time: 372.50194025039673 and batch: 550, loss is 4.2699266004562375 and perplexity is 71.51638615645207
At time: 372.92813372612 and batch: 600, loss is 4.289644107818604 and perplexity is 72.94050492438045
At time: 373.35332441329956 and batch: 650, loss is 4.2690292263031 and perplexity is 71.452237986749
At time: 373.77878642082214 and batch: 700, loss is 4.2357426309585575 and perplexity is 69.11298512269417
At time: 374.2051932811737 and batch: 750, loss is 4.201655387878418 and perplexity is 66.79681420600274
At time: 374.6317973136902 and batch: 800, loss is 4.230524768829346 and perplexity is 68.75330229874108
At time: 375.058696269989 and batch: 850, loss is 4.223074655532837 and perplexity is 68.24298572563977
At time: 375.48478078842163 and batch: 900, loss is 4.329353446960449 and perplexity is 75.89520042997572
At time: 375.91118025779724 and batch: 950, loss is 4.248172316551209 and perplexity is 69.97739886456733
At time: 376.3374400138855 and batch: 1000, loss is 4.219979581832885 and perplexity is 68.03209518454959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808969265077172 and perplexity of 122.60517892648471
Finished 40 epochs...
Completing Train Step...
At time: 377.61444330215454 and batch: 50, loss is 4.387436447143554 and perplexity is 80.43395775425627
At time: 378.0447437763214 and batch: 100, loss is 4.306378431320191 and perplexity is 74.17138518008925
At time: 378.46996116638184 and batch: 150, loss is 4.352653684616089 and perplexity is 77.68433937517197
At time: 378.89585852622986 and batch: 200, loss is 4.351151943206787 and perplexity is 77.56776513996458
At time: 379.32153511047363 and batch: 250, loss is 4.33057333946228 and perplexity is 75.98784091013025
At time: 379.7466037273407 and batch: 300, loss is 4.259888467788696 and perplexity is 70.8020863006412
At time: 380.1720087528229 and batch: 350, loss is 4.308410100936889 and perplexity is 74.32223011143336
At time: 380.59950399398804 and batch: 400, loss is 4.229670972824096 and perplexity is 68.69462605622586
At time: 381.02582454681396 and batch: 450, loss is 4.278278064727783 and perplexity is 72.11615368280903
At time: 381.4685208797455 and batch: 500, loss is 4.219892821311951 and perplexity is 68.02619294057568
At time: 381.9204668998718 and batch: 550, loss is 4.269605660438538 and perplexity is 71.49343736900232
At time: 382.34807872772217 and batch: 600, loss is 4.2892496871948245 and perplexity is 72.91174135777423
At time: 382.77560448646545 and batch: 650, loss is 4.268590717315674 and perplexity is 71.42091240699618
At time: 383.2021014690399 and batch: 700, loss is 4.235413498878479 and perplexity is 69.09024156516283
At time: 383.62938594818115 and batch: 750, loss is 4.2013184309005736 and perplexity is 66.77431034498768
At time: 384.0557942390442 and batch: 800, loss is 4.230124921798706 and perplexity is 68.7258169902959
At time: 384.48240780830383 and batch: 850, loss is 4.222790050506592 and perplexity is 68.22356619247569
At time: 384.9179608821869 and batch: 900, loss is 4.329099817276001 and perplexity is 75.87595359512702
At time: 385.35460209846497 and batch: 950, loss is 4.247951288223266 and perplexity is 69.9619335862978
At time: 385.7808783054352 and batch: 1000, loss is 4.219658164978028 and perplexity is 68.01023203626639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808897064953316 and perplexity of 122.59632713693478
Finished 41 epochs...
Completing Train Step...
At time: 387.03270077705383 and batch: 50, loss is 4.386896200180054 and perplexity is 80.3905152887039
At time: 387.47116327285767 and batch: 100, loss is 4.305756521224976 and perplexity is 74.1252715876103
At time: 387.89938259124756 and batch: 150, loss is 4.352015838623047 and perplexity is 77.63480453006007
At time: 388.32738280296326 and batch: 200, loss is 4.350626277923584 and perplexity is 77.52700117377442
At time: 388.7682058811188 and batch: 250, loss is 4.330138893127441 and perplexity is 75.95483544122617
At time: 389.20163106918335 and batch: 300, loss is 4.259385013580323 and perplexity is 70.76644966379165
At time: 389.62999200820923 and batch: 350, loss is 4.30793080329895 and perplexity is 74.28661617761352
At time: 390.0571594238281 and batch: 400, loss is 4.229212989807129 and perplexity is 68.66317228733095
At time: 390.4848282337189 and batch: 450, loss is 4.277791175842285 and perplexity is 72.08104967568494
At time: 390.91323471069336 and batch: 500, loss is 4.219513630867004 and perplexity is 68.0004029481749
At time: 391.3417954444885 and batch: 550, loss is 4.269271793365479 and perplexity is 71.46957204846903
At time: 391.76979804039 and batch: 600, loss is 4.288859634399414 and perplexity is 72.88330747495765
At time: 392.197500705719 and batch: 650, loss is 4.268136086463929 and perplexity is 71.38844963660377
At time: 392.62367701530457 and batch: 700, loss is 4.23511960029602 and perplexity is 69.06993902469513
At time: 393.07178831100464 and batch: 750, loss is 4.200962219238281 and perplexity is 66.75052879278586
At time: 393.49776554107666 and batch: 800, loss is 4.229727649688721 and perplexity is 68.6985195625821
At time: 393.923406124115 and batch: 850, loss is 4.222479429244995 and perplexity is 68.20237779322838
At time: 394.3499164581299 and batch: 900, loss is 4.328871192932129 and perplexity is 75.85860848785313
At time: 394.77636909484863 and batch: 950, loss is 4.247733907699585 and perplexity is 69.94672687741812
At time: 395.20392370224 and batch: 1000, loss is 4.2193766117095945 and perplexity is 67.99108622855854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808866175209603 and perplexity of 122.59254022629811
Finished 42 epochs...
Completing Train Step...
At time: 396.4589877128601 and batch: 50, loss is 4.386354198455811 and perplexity is 80.34695529666638
At time: 396.89848613739014 and batch: 100, loss is 4.305157537460327 and perplexity is 74.08088504811451
At time: 397.3251905441284 and batch: 150, loss is 4.351410493850708 and perplexity is 77.58782292845437
At time: 397.7520275115967 and batch: 200, loss is 4.350117168426514 and perplexity is 77.48754148670051
At time: 398.1785795688629 and batch: 250, loss is 4.329701023101807 and perplexity is 75.92158437583815
At time: 398.6048812866211 and batch: 300, loss is 4.258930311203003 and perplexity is 70.73427930541857
At time: 399.04338479042053 and batch: 350, loss is 4.307446012496948 and perplexity is 74.25061143746736
At time: 399.4804801940918 and batch: 400, loss is 4.228788299560547 and perplexity is 68.6340178989915
At time: 399.90638518333435 and batch: 450, loss is 4.277309694290161 and perplexity is 72.04635233374368
At time: 400.3331549167633 and batch: 500, loss is 4.219125270843506 and perplexity is 67.97399943745393
At time: 400.7589259147644 and batch: 550, loss is 4.268990912437439 and perplexity is 71.44950042774514
At time: 401.1853446960449 and batch: 600, loss is 4.288483438491821 and perplexity is 72.85589422965234
At time: 401.6113369464874 and batch: 650, loss is 4.267733926773071 and perplexity is 71.35974585190658
At time: 402.038391828537 and batch: 700, loss is 4.234782214164734 and perplexity is 69.04663971583227
At time: 402.4654951095581 and batch: 750, loss is 4.2006391906738285 and perplexity is 66.72896994754234
At time: 402.8925178050995 and batch: 800, loss is 4.229345426559449 and perplexity is 68.67226641705784
At time: 403.3190453052521 and batch: 850, loss is 4.222210779190063 and perplexity is 68.18405768164742
At time: 403.75933957099915 and batch: 900, loss is 4.328619537353515 and perplexity is 75.83952064772274
At time: 404.18822264671326 and batch: 950, loss is 4.247518396377563 and perplexity is 69.93165419006338
At time: 404.62821316719055 and batch: 1000, loss is 4.219061079025269 and perplexity is 67.96963620288068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808797324576029 and perplexity of 122.58409994279485
Finished 43 epochs...
Completing Train Step...
At time: 405.90666937828064 and batch: 50, loss is 4.385851345062256 and perplexity is 80.30656271415837
At time: 406.3330490589142 and batch: 100, loss is 4.3045840549469 and perplexity is 74.03841313557378
At time: 406.7586088180542 and batch: 150, loss is 4.3507905769348145 and perplexity is 77.5397398298566
At time: 407.1845397949219 and batch: 200, loss is 4.349600696563721 and perplexity is 77.4475316846635
At time: 407.61096572875977 and batch: 250, loss is 4.329282302856445 and perplexity is 75.88980112600949
At time: 408.0505881309509 and batch: 300, loss is 4.258460578918457 and perplexity is 70.70106093328639
At time: 408.47965717315674 and batch: 350, loss is 4.306936435699463 and perplexity is 74.21278468731535
At time: 408.90769839286804 and batch: 400, loss is 4.228384056091309 and perplexity is 68.60627865257061
At time: 409.33912229537964 and batch: 450, loss is 4.276861276626587 and perplexity is 72.01405271916121
At time: 409.77368927001953 and batch: 500, loss is 4.218726906776428 and perplexity is 67.94692643139693
At time: 410.21024227142334 and batch: 550, loss is 4.268680295944214 and perplexity is 71.42731048093972
At time: 410.6540734767914 and batch: 600, loss is 4.288109569549561 and perplexity is 72.82866076472988
At time: 411.0873951911926 and batch: 650, loss is 4.267323055267334 and perplexity is 71.33043218816634
At time: 411.51935839653015 and batch: 700, loss is 4.234459533691406 and perplexity is 69.0243633077213
At time: 411.9573805332184 and batch: 750, loss is 4.200288009643555 and perplexity is 66.7055401134245
At time: 412.3878903388977 and batch: 800, loss is 4.2289551162719725 and perplexity is 68.64546815516984
At time: 412.8162913322449 and batch: 850, loss is 4.221889185905456 and perplexity is 68.16213367207584
At time: 413.2440893650055 and batch: 900, loss is 4.328365154266358 and perplexity is 75.82023080994009
At time: 413.67031359672546 and batch: 950, loss is 4.2472819805145265 and perplexity is 69.91512319185657
At time: 414.0954236984253 and batch: 1000, loss is 4.218788146972656 and perplexity is 67.95108764192004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80879955756955 and perplexity of 122.58437367260139
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 415.362589597702 and batch: 50, loss is 4.385542287826538 and perplexity is 80.28174722477672
At time: 415.80388355255127 and batch: 100, loss is 4.304362897872925 and perplexity is 74.0220408272556
At time: 416.23095321655273 and batch: 150, loss is 4.349927520751953 and perplexity is 77.4728475480298
At time: 416.6596784591675 and batch: 200, loss is 4.348830976486206 and perplexity is 77.38794170131725
At time: 417.0956745147705 and batch: 250, loss is 4.328143005371094 and perplexity is 75.80338930015765
At time: 417.53374099731445 and batch: 300, loss is 4.257597799301148 and perplexity is 70.64008780595907
At time: 417.96387791633606 and batch: 350, loss is 4.30548475265503 and perplexity is 74.10512940568404
At time: 418.40155124664307 and batch: 400, loss is 4.226532783508301 and perplexity is 68.47938722143293
At time: 418.8393225669861 and batch: 450, loss is 4.2747194194793705 and perplexity is 71.8599739718814
At time: 419.287273645401 and batch: 500, loss is 4.2166124534606935 and perplexity is 67.80340761290256
At time: 419.73788261413574 and batch: 550, loss is 4.266382274627685 and perplexity is 71.26335745479835
At time: 420.17649483680725 and batch: 600, loss is 4.285607948303222 and perplexity is 72.64669873455364
At time: 420.604612827301 and batch: 650, loss is 4.263807244300843 and perplexity is 71.08008821135772
At time: 421.03137588500977 and batch: 700, loss is 4.231246438026428 and perplexity is 68.80293734709632
At time: 421.4590389728546 and batch: 750, loss is 4.197029986381531 and perplexity is 66.48856555803036
At time: 421.8853850364685 and batch: 800, loss is 4.2247161245346065 and perplexity is 68.35509645927081
At time: 422.3123209476471 and batch: 850, loss is 4.217446804046631 and perplexity is 67.86000303270058
At time: 422.7403492927551 and batch: 900, loss is 4.3234218502044675 and perplexity is 75.44635321167597
At time: 423.1704969406128 and batch: 950, loss is 4.242124166488647 and perplexity is 69.55544236961485
At time: 423.5983669757843 and batch: 1000, loss is 4.2135787773132325 and perplexity is 67.59802572109791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808285596894055 and perplexity of 122.52138631300292
Finished 45 epochs...
Completing Train Step...
At time: 424.8650732040405 and batch: 50, loss is 4.384961156845093 and perplexity is 80.2351065676979
At time: 425.30757331848145 and batch: 100, loss is 4.304092931747436 and perplexity is 74.00206008087642
At time: 425.73752903938293 and batch: 150, loss is 4.349714469909668 and perplexity is 77.45634365075254
At time: 426.1808261871338 and batch: 200, loss is 4.348171625137329 and perplexity is 77.33693267585254
At time: 426.6092736721039 and batch: 250, loss is 4.327649259567261 and perplexity is 75.76597093311543
At time: 427.037624835968 and batch: 300, loss is 4.257270927429199 and perplexity is 70.61700132159008
At time: 427.46478033065796 and batch: 350, loss is 4.30504584312439 and perplexity is 74.07261109493885
At time: 427.8916025161743 and batch: 400, loss is 4.226080141067505 and perplexity is 68.44839755860424
At time: 428.31915736198425 and batch: 450, loss is 4.274452648162842 and perplexity is 71.84080634881889
At time: 428.74636578559875 and batch: 500, loss is 4.216394534111023 and perplexity is 67.78863354824587
At time: 429.1728250980377 and batch: 550, loss is 4.266162605285644 and perplexity is 71.24770479922185
At time: 429.61587262153625 and batch: 600, loss is 4.285403938293457 and perplexity is 72.63187959251343
At time: 430.05032300949097 and batch: 650, loss is 4.263663945198059 and perplexity is 71.06990322825813
At time: 430.47835302352905 and batch: 700, loss is 4.231034507751465 and perplexity is 68.78835746667968
At time: 430.9058620929718 and batch: 750, loss is 4.196928720474244 and perplexity is 66.48183287401632
At time: 431.33387517929077 and batch: 800, loss is 4.224750046730041 and perplexity is 68.35741525354094
At time: 431.76145792007446 and batch: 850, loss is 4.217410593032837 and perplexity is 67.85754579768447
At time: 432.19000911712646 and batch: 900, loss is 4.323570585250854 and perplexity is 75.45757556307841
At time: 432.6180160045624 and batch: 950, loss is 4.242302188873291 and perplexity is 69.56782589757032
At time: 433.0516679286957 and batch: 1000, loss is 4.213736095428467 and perplexity is 67.60866095163318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80818325135766 and perplexity of 122.50884743766076
Finished 46 epochs...
Completing Train Step...
At time: 434.3582332134247 and batch: 50, loss is 4.3846841812133786 and perplexity is 80.21288647572469
At time: 434.79873752593994 and batch: 100, loss is 4.303908948898315 and perplexity is 73.98844622341846
At time: 435.2382810115814 and batch: 150, loss is 4.349496059417724 and perplexity is 77.43942821995263
At time: 435.6700224876404 and batch: 200, loss is 4.347855567932129 and perplexity is 77.31249364332184
At time: 436.1032540798187 and batch: 250, loss is 4.327386093139649 and perplexity is 75.74603449662582
At time: 436.53232049942017 and batch: 300, loss is 4.2570512580871585 and perplexity is 70.60149063504639
At time: 436.97220611572266 and batch: 350, loss is 4.304809198379517 and perplexity is 74.05508427468098
At time: 437.3990144729614 and batch: 400, loss is 4.22587248802185 and perplexity is 68.43418551601907
At time: 437.82604241371155 and batch: 450, loss is 4.274300184249878 and perplexity is 71.8298540533086
At time: 438.25381779670715 and batch: 500, loss is 4.2162526512145995 and perplexity is 67.77901618285941
At time: 438.681081533432 and batch: 550, loss is 4.266034455299377 and perplexity is 71.23857499183511
At time: 439.1093020439148 and batch: 600, loss is 4.285332670211792 and perplexity is 72.62670344223652
At time: 439.53762459754944 and batch: 650, loss is 4.263584499359131 and perplexity is 71.06425724445157
At time: 439.9658010005951 and batch: 700, loss is 4.230929679870606 and perplexity is 68.78114690687912
At time: 440.3932681083679 and batch: 750, loss is 4.196912569999695 and perplexity is 66.48075916953698
At time: 440.8196716308594 and batch: 800, loss is 4.224772253036499 and perplexity is 68.35893323610705
At time: 441.2458984851837 and batch: 850, loss is 4.2174110603332515 and perplexity is 67.85757750755114
At time: 441.67050647735596 and batch: 900, loss is 4.323673505783081 and perplexity is 75.4653420965769
At time: 442.10565662384033 and batch: 950, loss is 4.2424085903167725 and perplexity is 69.57522840847764
At time: 442.54245471954346 and batch: 1000, loss is 4.213823385238648 and perplexity is 67.6145627563942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808125193526105 and perplexity of 122.50173504609927
Finished 47 epochs...
Completing Train Step...
At time: 443.7965953350067 and batch: 50, loss is 4.384458179473877 and perplexity is 80.19476027220466
At time: 444.2347903251648 and batch: 100, loss is 4.30376091003418 and perplexity is 73.9774938685875
At time: 444.660560131073 and batch: 150, loss is 4.349300804138184 and perplexity is 77.42430923882658
At time: 445.08686876296997 and batch: 200, loss is 4.347610664367676 and perplexity is 77.29356185637752
At time: 445.5128915309906 and batch: 250, loss is 4.327176513671875 and perplexity is 75.73016134643137
At time: 445.9504826068878 and batch: 300, loss is 4.256867828369141 and perplexity is 70.58854141120014
At time: 446.38587641716003 and batch: 350, loss is 4.304620313644409 and perplexity is 74.04109772066921
At time: 446.81274366378784 and batch: 400, loss is 4.225726356506348 and perplexity is 68.42418585542801
At time: 447.2396116256714 and batch: 450, loss is 4.274181108474732 and perplexity is 71.82130136697756
At time: 447.6656630039215 and batch: 500, loss is 4.216142053604126 and perplexity is 67.77152040014481
At time: 448.10516381263733 and batch: 550, loss is 4.265933895111084 and perplexity is 71.2314115875029
At time: 448.53053617477417 and batch: 600, loss is 4.285289087295532 and perplexity is 72.6235382276773
At time: 448.98190784454346 and batch: 650, loss is 4.263513083457947 and perplexity is 71.05918230769623
At time: 449.4308123588562 and batch: 700, loss is 4.2308495998382565 and perplexity is 68.77563913094417
At time: 449.8672981262207 and batch: 750, loss is 4.196913022994995 and perplexity is 66.48078928501523
At time: 450.29401302337646 and batch: 800, loss is 4.224780316352844 and perplexity is 68.35948443803301
At time: 450.72036242485046 and batch: 850, loss is 4.217414131164551 and perplexity is 67.85778588704402
At time: 451.14630365371704 and batch: 900, loss is 4.3237435913085935 and perplexity is 75.47063131008224
At time: 451.5807890892029 and batch: 950, loss is 4.242473640441895 and perplexity is 69.57975443299864
At time: 452.015504360199 and batch: 1000, loss is 4.213871660232544 and perplexity is 67.61782692778688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808083883145961 and perplexity of 122.49667455738226
Finished 48 epochs...
Completing Train Step...
At time: 453.2683775424957 and batch: 50, loss is 4.38426254272461 and perplexity is 80.17907276457326
At time: 453.7278399467468 and batch: 100, loss is 4.303640460968017 and perplexity is 73.96858388514433
At time: 454.1598105430603 and batch: 150, loss is 4.349122705459595 and perplexity is 77.41052129950383
At time: 454.58806014060974 and batch: 200, loss is 4.347402315139771 and perplexity is 77.27745947995982
At time: 455.0152838230133 and batch: 250, loss is 4.3269933986663816 and perplexity is 75.71629528710088
At time: 455.4438054561615 and batch: 300, loss is 4.256704092025757 and perplexity is 70.57698444771795
At time: 455.8714964389801 and batch: 350, loss is 4.304458675384521 and perplexity is 74.02913081365453
At time: 456.29831862449646 and batch: 400, loss is 4.2256044578552245 and perplexity is 68.41584554781447
At time: 456.7256295681 and batch: 450, loss is 4.274077768325806 and perplexity is 71.81387972648156
At time: 457.15200901031494 and batch: 500, loss is 4.216043381690979 and perplexity is 67.76483358447594
At time: 457.57947850227356 and batch: 550, loss is 4.265846204757691 and perplexity is 71.22516555371051
At time: 458.00677466392517 and batch: 600, loss is 4.285258808135986 and perplexity is 72.62133928126781
At time: 458.4338207244873 and batch: 650, loss is 4.26343867778778 and perplexity is 71.05389529830933
At time: 458.88219141960144 and batch: 700, loss is 4.230780248641968 and perplexity is 68.77086962348235
At time: 459.30933141708374 and batch: 750, loss is 4.196910562515259 and perplexity is 66.48062571058163
At time: 459.7362411022186 and batch: 800, loss is 4.224779725074768 and perplexity is 68.3594440185805
At time: 460.1634871959686 and batch: 850, loss is 4.217413578033447 and perplexity is 67.8577483528024
At time: 460.59127044677734 and batch: 900, loss is 4.3237992286682125 and perplexity is 75.47483041354955
At time: 461.01937341690063 and batch: 950, loss is 4.242514595985413 and perplexity is 69.58260416801514
At time: 461.4469048976898 and batch: 1000, loss is 4.2138948822021485 and perplexity is 67.61939716514043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808048155249619 and perplexity of 122.49229808707295
Finished 49 epochs...
Completing Train Step...
At time: 462.714035987854 and batch: 50, loss is 4.384088830947876 and perplexity is 80.16514592504952
At time: 463.13915395736694 and batch: 100, loss is 4.303545179367066 and perplexity is 73.96153637580596
At time: 463.5646963119507 and batch: 150, loss is 4.3489586353302006 and perplexity is 77.39782158710793
At time: 463.9896168708801 and batch: 200, loss is 4.3472170066833495 and perplexity is 77.26314063996946
At time: 464.41536498069763 and batch: 250, loss is 4.326826639175415 and perplexity is 75.70366992896936
At time: 464.84171438217163 and batch: 300, loss is 4.256554107666016 and perplexity is 70.56639979767894
At time: 465.26700949668884 and batch: 350, loss is 4.304311094284057 and perplexity is 74.01820631920695
At time: 465.69285440444946 and batch: 400, loss is 4.225492658615113 and perplexity is 68.40819713582191
At time: 466.1197557449341 and batch: 450, loss is 4.273980512619018 and perplexity is 71.80689575647257
At time: 466.546338558197 and batch: 500, loss is 4.215952076911926 and perplexity is 67.75864661377224
At time: 466.97632360458374 and batch: 550, loss is 4.265774874687195 and perplexity is 71.22008523882228
At time: 467.4138889312744 and batch: 600, loss is 4.285233831405639 and perplexity is 72.61952546031088
At time: 467.84032678604126 and batch: 650, loss is 4.263369569778442 and perplexity is 71.04898507471938
At time: 468.27958488464355 and batch: 700, loss is 4.230714011192322 and perplexity is 68.76631456732787
At time: 468.71359157562256 and batch: 750, loss is 4.196906938552856 and perplexity is 66.48038478773007
At time: 469.142169713974 and batch: 800, loss is 4.224772353172302 and perplexity is 68.35894008128405
At time: 469.5707712173462 and batch: 850, loss is 4.217418184280396 and perplexity is 67.85806092306855
At time: 470.03190183639526 and batch: 900, loss is 4.323836545944214 and perplexity is 75.47764698118027
At time: 470.46895956993103 and batch: 950, loss is 4.242541542053223 and perplexity is 69.5844791708473
At time: 470.91678380966187 and batch: 1000, loss is 4.213911113739013 and perplexity is 67.62049474078594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.808025825314406 and perplexity of 122.48956287253128
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
SETTINGS FOR THIS RUN
{'anneal': 3.710547618739717, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.010499086771258437, 'batch_size': 50, 'num_layers': 1, 'lr': 24.298905683244797, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6574177742004395 and batch: 50, loss is 6.277679166793823 and perplexity is 532.5512658220749
At time: 1.0938730239868164 and batch: 100, loss is 5.4613379287719725 and perplexity is 235.41217848585828
At time: 1.5322742462158203 and batch: 150, loss is 5.387010087966919 and perplexity is 218.54896553046578
At time: 1.9622914791107178 and batch: 200, loss is 5.345860357284546 and perplexity is 209.73825683257576
At time: 2.392688274383545 and batch: 250, loss is 5.305813217163086 and perplexity is 201.50480296617522
At time: 2.8525638580322266 and batch: 300, loss is 5.184604320526123 and perplexity is 178.50280591825114
At time: 3.281862497329712 and batch: 350, loss is 5.225264024734497 and perplexity is 185.91024895082947
At time: 3.7108824253082275 and batch: 400, loss is 5.160635662078858 and perplexity is 174.2752005394724
At time: 4.1402201652526855 and batch: 450, loss is 5.170298767089844 and perplexity is 175.96740290013798
At time: 4.58197283744812 and batch: 500, loss is 5.161511936187744 and perplexity is 174.42798031423058
At time: 5.0118937492370605 and batch: 550, loss is 5.197309675216675 and perplexity is 180.78521609282325
At time: 5.441550970077515 and batch: 600, loss is 5.2153457832336425 and perplexity is 184.07546018418424
At time: 5.871252775192261 and batch: 650, loss is 5.2066143608093265 and perplexity is 182.47521595411357
At time: 6.302634000778198 and batch: 700, loss is 5.176400871276855 and perplexity is 177.04445713265528
At time: 6.736307859420776 and batch: 750, loss is 5.099560565948487 and perplexity is 163.94984632282313
At time: 7.19490647315979 and batch: 800, loss is 5.155968532562256 and perplexity is 173.46373069740795
At time: 7.650286674499512 and batch: 850, loss is 5.1394500732421875 and perplexity is 170.62191296149462
At time: 8.109673738479614 and batch: 900, loss is 5.260274744033813 and perplexity is 192.53438170392818
At time: 8.567327499389648 and batch: 950, loss is 5.189541654586792 and perplexity is 179.38631319232448
At time: 9.007817506790161 and batch: 1000, loss is 5.160179481506348 and perplexity is 174.19571770935593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.292964656178544 and perplexity of 198.93231797165748
Finished 1 epochs...
Completing Train Step...
At time: 10.384514570236206 and batch: 50, loss is 5.228303031921387 and perplexity is 186.47609089688873
At time: 10.841259479522705 and batch: 100, loss is 5.17454797744751 and perplexity is 176.71671627878436
At time: 11.290640592575073 and batch: 150, loss is 5.2116655921936035 and perplexity is 183.39927233832879
At time: 11.72852349281311 and batch: 200, loss is 5.217716960906983 and perplexity is 184.51245369546243
At time: 12.17334794998169 and batch: 250, loss is 5.1830972480773925 and perplexity is 178.23399186943305
At time: 12.605350494384766 and batch: 300, loss is 5.099637136459351 and perplexity is 163.962400526947
At time: 13.036316156387329 and batch: 350, loss is 5.156399402618408 and perplexity is 173.53848712879127
At time: 13.466777563095093 and batch: 400, loss is 5.092373113632203 and perplexity is 162.7756892737069
At time: 13.89416241645813 and batch: 450, loss is 5.12113881111145 and perplexity is 167.52604154841524
At time: 14.32162594795227 and batch: 500, loss is 5.087525415420532 and perplexity is 161.98851139786953
At time: 14.75058364868164 and batch: 550, loss is 5.156516237258911 and perplexity is 173.55876362002357
At time: 15.179749727249146 and batch: 600, loss is 5.170699253082275 and perplexity is 176.03788949362854
At time: 15.609796285629272 and batch: 650, loss is 5.15141845703125 and perplexity is 172.6762505265848
At time: 16.0452983379364 and batch: 700, loss is 5.136057634353637 and perplexity is 170.04406925264283
At time: 16.480418920516968 and batch: 750, loss is 5.047923793792725 and perplexity is 155.69886572227827
At time: 16.916988372802734 and batch: 800, loss is 5.105403623580933 and perplexity is 164.9106189150297
At time: 17.3610258102417 and batch: 850, loss is 5.084361534118653 and perplexity is 161.4768088853955
At time: 17.799203872680664 and batch: 900, loss is 5.222977619171143 and perplexity is 185.48566829009937
At time: 18.22882318496704 and batch: 950, loss is 5.158249855041504 and perplexity is 173.85990913915097
At time: 18.657044887542725 and batch: 1000, loss is 5.124589052200317 and perplexity is 168.10504505688934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.311310558784299 and perplexity of 202.6155941081723
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 19.958360195159912 and batch: 50, loss is 5.120657949447632 and perplexity is 167.44550406259
At time: 20.41337299346924 and batch: 100, loss is 5.019230737686157 and perplexity is 151.29487357258654
At time: 20.841283798217773 and batch: 150, loss is 5.031940240859985 and perplexity is 153.23002762275928
At time: 21.268030881881714 and batch: 200, loss is 5.035462255477905 and perplexity is 153.77065751428398
At time: 21.693767786026 and batch: 250, loss is 5.000107536315918 and perplexity is 148.42911976510024
At time: 22.12089252471924 and batch: 300, loss is 4.887995624542237 and perplexity is 132.68735206397068
At time: 22.54834818840027 and batch: 350, loss is 4.944435691833496 and perplexity is 140.39160436497758
At time: 22.9885675907135 and batch: 400, loss is 4.874068994522094 and perplexity is 130.852272301416
At time: 23.449162483215332 and batch: 450, loss is 4.912086410522461 and perplexity is 135.92270934194474
At time: 23.904701709747314 and batch: 500, loss is 4.864507255554199 and perplexity is 129.60705971630418
At time: 24.34386706352234 and batch: 550, loss is 4.904104127883911 and perplexity is 134.84205464258775
At time: 24.781747341156006 and batch: 600, loss is 4.922624750137329 and perplexity is 137.3626831543739
At time: 25.23652720451355 and batch: 650, loss is 4.886395692825317 and perplexity is 132.47523109575053
At time: 25.684508085250854 and batch: 700, loss is 4.850640687942505 and perplexity is 127.82225779667809
At time: 26.114686965942383 and batch: 750, loss is 4.785810203552246 and perplexity is 119.79838485182128
At time: 26.55238938331604 and batch: 800, loss is 4.825548267364502 and perplexity is 124.65479380645668
At time: 26.995558261871338 and batch: 850, loss is 4.801856889724731 and perplexity is 121.73625858055436
At time: 27.43578052520752 and batch: 900, loss is 4.903441371917725 and perplexity is 134.75271687422023
At time: 27.88715100288391 and batch: 950, loss is 4.833248090744019 and perplexity is 125.61831842648306
At time: 28.338411569595337 and batch: 1000, loss is 4.796705236434937 and perplexity is 121.11072822474787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.062492184522675 and perplexity of 157.98375077194063
Finished 3 epochs...
Completing Train Step...
At time: 29.754916667938232 and batch: 50, loss is 4.926755819320679 and perplexity is 137.93131161477842
At time: 30.200706958770752 and batch: 100, loss is 4.855826692581177 and perplexity is 128.48686646081092
At time: 30.67401361465454 and batch: 150, loss is 4.889301862716675 and perplexity is 132.86078659724546
At time: 31.1175217628479 and batch: 200, loss is 4.9080980777740475 and perplexity is 135.3816839608108
At time: 31.57564616203308 and batch: 250, loss is 4.878262720108032 and perplexity is 131.40218310529346
At time: 32.0190315246582 and batch: 300, loss is 4.775710525512696 and perplexity is 118.59454914462425
At time: 32.475017070770264 and batch: 350, loss is 4.827615404129029 and perplexity is 124.91273882557358
At time: 32.91916513442993 and batch: 400, loss is 4.760110816955566 and perplexity is 116.7588640439425
At time: 33.35497808456421 and batch: 450, loss is 4.808199052810669 and perplexity is 122.51078327075119
At time: 33.81356596946716 and batch: 500, loss is 4.767293949127197 and perplexity is 117.6005778481497
At time: 34.252289056777954 and batch: 550, loss is 4.806202192306518 and perplexity is 122.26639041673283
At time: 34.67961239814758 and batch: 600, loss is 4.827516069412232 and perplexity is 124.9003312702987
At time: 35.104844093322754 and batch: 650, loss is 4.803702621459961 and perplexity is 121.96115854499777
At time: 35.53074502944946 and batch: 700, loss is 4.766707525253296 and perplexity is 117.53163427876832
At time: 35.95564794540405 and batch: 750, loss is 4.709288949966431 and perplexity is 110.97322433056773
At time: 36.41456866264343 and batch: 800, loss is 4.750854597091675 and perplexity is 115.68310474030697
At time: 36.875200033187866 and batch: 850, loss is 4.729174041748047 and perplexity is 113.20202357890547
At time: 37.3102548122406 and batch: 900, loss is 4.835963268280029 and perplexity is 125.95985792314606
At time: 37.752986431121826 and batch: 950, loss is 4.7766607475280765 and perplexity is 118.70729385389477
At time: 38.22264885902405 and batch: 1000, loss is 4.745000743865967 and perplexity is 115.00789105353367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.045197277534299 and perplexity of 155.2749284320504
Finished 4 epochs...
Completing Train Step...
At time: 39.55990648269653 and batch: 50, loss is 4.839768304824829 and perplexity is 126.44005278585497
At time: 39.99496650695801 and batch: 100, loss is 4.778527555465698 and perplexity is 118.92910454687892
At time: 40.429523229599 and batch: 150, loss is 4.8162140464782714 and perplexity is 123.4966520110461
At time: 40.872984409332275 and batch: 200, loss is 4.839809226989746 and perplexity is 126.44522709241839
At time: 41.31856727600098 and batch: 250, loss is 4.800037803649903 and perplexity is 121.51501114284562
At time: 41.76108503341675 and batch: 300, loss is 4.704586019515991 and perplexity is 110.45255028127005
At time: 42.23906898498535 and batch: 350, loss is 4.773500328063965 and perplexity is 118.3327212269544
At time: 42.667747020721436 and batch: 400, loss is 4.680866718292236 and perplexity is 107.86351935482226
At time: 43.10260772705078 and batch: 450, loss is 4.74451696395874 and perplexity is 114.95226600289524
At time: 43.53659224510193 and batch: 500, loss is 4.697389078140259 and perplexity is 109.66048340036879
At time: 43.974719762802124 and batch: 550, loss is 4.739225416183472 and perplexity is 114.34559711973928
At time: 44.42353177070618 and batch: 600, loss is 4.763769102096558 and perplexity is 117.18678350982641
At time: 44.86738729476929 and batch: 650, loss is 4.744264783859253 and perplexity is 114.92328098389447
At time: 45.312137603759766 and batch: 700, loss is 4.714050521850586 and perplexity is 111.50289133845843
At time: 45.74902939796448 and batch: 750, loss is 4.659455995559693 and perplexity is 105.57863128709053
At time: 46.1744270324707 and batch: 800, loss is 4.696769399642944 and perplexity is 109.5925502073438
At time: 46.611889123916626 and batch: 850, loss is 4.6778258228302 and perplexity is 107.53601587259327
At time: 47.04692220687866 and batch: 900, loss is 4.779723472595215 and perplexity is 119.07141898137796
At time: 47.48663687705994 and batch: 950, loss is 4.721636590957641 and perplexity is 112.35197651524585
At time: 47.926016092300415 and batch: 1000, loss is 4.706447381973266 and perplexity is 110.65833397127693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.046786796755907 and perplexity of 155.52193717599306
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 49.22876453399658 and batch: 50, loss is 4.778908452987671 and perplexity is 118.9744129764778
At time: 49.67963361740112 and batch: 100, loss is 4.682993974685669 and perplexity is 108.09321694219925
At time: 50.12098431587219 and batch: 150, loss is 4.70396505355835 and perplexity is 110.38398429838456
At time: 50.57158374786377 and batch: 200, loss is 4.722388105392456 and perplexity is 112.43644238206078
At time: 51.00950479507446 and batch: 250, loss is 4.677618551254272 and perplexity is 107.5137290229094
At time: 51.442726850509644 and batch: 300, loss is 4.582002639770508 and perplexity is 97.70987608564953
At time: 51.889195919036865 and batch: 350, loss is 4.6734778881073 and perplexity is 107.06947128291299
At time: 52.34030222892761 and batch: 400, loss is 4.566906423568725 and perplexity is 96.24590468820762
At time: 52.78276085853577 and batch: 450, loss is 4.6123262214660645 and perplexity is 100.7181701084659
At time: 53.24084758758545 and batch: 500, loss is 4.563103828430176 and perplexity is 95.8806154426548
At time: 53.678396701812744 and batch: 550, loss is 4.604111995697021 and perplexity is 99.89423693948406
At time: 54.11740469932556 and batch: 600, loss is 4.617628526687622 and perplexity is 101.25362691093245
At time: 54.54528450965881 and batch: 650, loss is 4.588392105102539 and perplexity is 98.33618872215455
At time: 54.989797830581665 and batch: 700, loss is 4.551021013259888 and perplexity is 94.72907860105275
At time: 55.42517709732056 and batch: 750, loss is 4.494429950714111 and perplexity is 89.51712526447757
At time: 55.86895775794983 and batch: 800, loss is 4.528096723556518 and perplexity is 92.58218379848077
At time: 56.31036376953125 and batch: 850, loss is 4.508896379470825 and perplexity is 90.82153067510748
At time: 56.74374222755432 and batch: 900, loss is 4.6120819568634035 and perplexity is 100.69357122910365
At time: 57.180543422698975 and batch: 950, loss is 4.530558156967163 and perplexity is 92.81034937089632
At time: 57.62028455734253 and batch: 1000, loss is 4.513536615371704 and perplexity is 91.24394329147123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.938252146651105 and perplexity of 139.52616503535018
Finished 6 epochs...
Completing Train Step...
At time: 59.010449171066284 and batch: 50, loss is 4.689411392211914 and perplexity is 108.78912683091316
At time: 59.447235107421875 and batch: 100, loss is 4.619419851303101 and perplexity is 101.43516757581068
At time: 59.88076043128967 and batch: 150, loss is 4.654225721359253 and perplexity is 105.02786767365706
At time: 60.31232213973999 and batch: 200, loss is 4.675630712509156 and perplexity is 107.30022134644786
At time: 60.75492286682129 and batch: 250, loss is 4.6369407367706295 and perplexity is 103.22806221488771
At time: 61.19233727455139 and batch: 300, loss is 4.547892799377442 and perplexity is 94.43320879559295
At time: 61.62996459007263 and batch: 350, loss is 4.637138166427612 and perplexity is 103.24844450777015
At time: 62.06285285949707 and batch: 400, loss is 4.535150871276856 and perplexity is 93.23758111624763
At time: 62.51725912094116 and batch: 450, loss is 4.584366436004639 and perplexity is 97.94111551655469
At time: 62.95776176452637 and batch: 500, loss is 4.535999212265015 and perplexity is 93.31671193815941
At time: 63.393004179000854 and batch: 550, loss is 4.576256885528564 and perplexity is 97.15006894787639
At time: 63.8429000377655 and batch: 600, loss is 4.591628580093384 and perplexity is 98.65496691821777
At time: 64.29923510551453 and batch: 650, loss is 4.5653095626831055 and perplexity is 96.09233601414724
At time: 64.75091910362244 and batch: 700, loss is 4.532433700561524 and perplexity is 92.98458256705229
At time: 65.18477749824524 and batch: 750, loss is 4.478823776245117 and perplexity is 88.13094997015952
At time: 65.61923789978027 and batch: 800, loss is 4.514950275421143 and perplexity is 91.37302242436597
At time: 66.05937838554382 and batch: 850, loss is 4.4940785312652585 and perplexity is 89.48567273249365
At time: 66.50358939170837 and batch: 900, loss is 4.603222618103027 and perplexity is 99.80543273946742
At time: 66.94127678871155 and batch: 950, loss is 4.525208597183227 and perplexity is 92.3151805069751
At time: 67.38186883926392 and batch: 1000, loss is 4.507428102493286 and perplexity is 90.68827736287695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.932433709865663 and perplexity of 138.716698064032
Finished 7 epochs...
Completing Train Step...
At time: 68.70132493972778 and batch: 50, loss is 4.661354455947876 and perplexity is 105.77925851759505
At time: 69.14781427383423 and batch: 100, loss is 4.593982086181641 and perplexity is 98.88742542245028
At time: 69.5903992652893 and batch: 150, loss is 4.629795742034912 and perplexity is 102.49312693473064
At time: 70.02638530731201 and batch: 200, loss is 4.6524386310577395 and perplexity is 104.84034100340422
At time: 70.45926260948181 and batch: 250, loss is 4.615271377563476 and perplexity is 101.01523808232906
At time: 70.8959891796112 and batch: 300, loss is 4.5257903099060055 and perplexity is 92.36889704426338
At time: 71.34156727790833 and batch: 350, loss is 4.612471380233765 and perplexity is 100.7327912950949
At time: 71.78055477142334 and batch: 400, loss is 4.514441719055176 and perplexity is 91.32656590601478
At time: 72.21826672554016 and batch: 450, loss is 4.564152765274048 and perplexity is 95.98124091846395
At time: 72.64922404289246 and batch: 500, loss is 4.516530179977417 and perplexity is 91.5174971769228
At time: 73.07607388496399 and batch: 550, loss is 4.55885682106018 and perplexity is 95.47427324215869
At time: 73.5118625164032 and batch: 600, loss is 4.5742533493041995 and perplexity is 96.95562012321747
At time: 73.94641852378845 and batch: 650, loss is 4.54905743598938 and perplexity is 94.54325323639299
At time: 74.38091206550598 and batch: 700, loss is 4.516390333175659 and perplexity is 91.50469964250566
At time: 74.82065033912659 and batch: 750, loss is 4.46561188697815 and perplexity is 86.97423166103779
At time: 75.25742363929749 and batch: 800, loss is 4.502914609909058 and perplexity is 90.2798788406851
At time: 75.73446297645569 and batch: 850, loss is 4.481204710006714 and perplexity is 88.3410339231288
At time: 76.17620182037354 and batch: 900, loss is 4.592765598297119 and perplexity is 98.76720320675258
At time: 76.62311053276062 and batch: 950, loss is 4.514721927642822 and perplexity is 91.35215997973401
At time: 77.05790543556213 and batch: 1000, loss is 4.496457910537719 and perplexity is 89.69884659754086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.93048095703125 and perplexity of 138.44608294691898
Finished 8 epochs...
Completing Train Step...
At time: 78.34003162384033 and batch: 50, loss is 4.640426855087281 and perplexity is 103.58855544931114
At time: 78.78934216499329 and batch: 100, loss is 4.5736045646667485 and perplexity is 96.89273720730762
At time: 79.22453570365906 and batch: 150, loss is 4.610275907516479 and perplexity is 100.51187779358868
At time: 79.67533421516418 and batch: 200, loss is 4.6336539936065675 and perplexity is 102.88933504659606
At time: 80.13425612449646 and batch: 250, loss is 4.5981267642974855 and perplexity is 99.2981325069242
At time: 80.56476283073425 and batch: 300, loss is 4.508974676132202 and perplexity is 90.82864197613239
At time: 80.99808764457703 and batch: 350, loss is 4.594116554260254 and perplexity is 98.90072351861053
At time: 81.42821502685547 and batch: 400, loss is 4.498317422866822 and perplexity is 89.86579788457672
At time: 81.85628914833069 and batch: 450, loss is 4.548798980712891 and perplexity is 94.5188211911691
At time: 82.29409861564636 and batch: 500, loss is 4.500167140960693 and perplexity is 90.0321781077601
At time: 82.72905683517456 and batch: 550, loss is 4.543120260238648 and perplexity is 93.98359636049238
At time: 83.16056394577026 and batch: 600, loss is 4.5596420955657955 and perplexity is 95.54927619997895
At time: 83.60204148292542 and batch: 650, loss is 4.534964017868042 and perplexity is 93.22016098394272
At time: 84.0386004447937 and batch: 700, loss is 4.503265790939331 and perplexity is 90.31158898922469
At time: 84.48268294334412 and batch: 750, loss is 4.452844924926758 and perplexity is 85.87089307345595
At time: 84.92490863800049 and batch: 800, loss is 4.490607872009277 and perplexity is 89.17563677993193
At time: 85.36074018478394 and batch: 850, loss is 4.467651596069336 and perplexity is 87.15181483949928
At time: 85.80423045158386 and batch: 900, loss is 4.580910654067993 and perplexity is 97.60323653299962
At time: 86.24420142173767 and batch: 950, loss is 4.504713306427002 and perplexity is 90.4424110736568
At time: 86.68110513687134 and batch: 1000, loss is 4.484543876647949 and perplexity is 88.63651240805765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.9288441727801064 and perplexity of 138.21966193044557
Finished 9 epochs...
Completing Train Step...
At time: 88.01676988601685 and batch: 50, loss is 4.622758865356445 and perplexity is 101.77442710681984
At time: 88.44737887382507 and batch: 100, loss is 4.556418752670288 and perplexity is 95.24178396206452
At time: 88.8743941783905 and batch: 150, loss is 4.593730583190918 and perplexity is 98.86255806644988
At time: 89.3103699684143 and batch: 200, loss is 4.615479335784912 and perplexity is 101.03624721601392
At time: 89.73877358436584 and batch: 250, loss is 4.58351466178894 and perplexity is 97.85772731870098
At time: 90.17275738716125 and batch: 300, loss is 4.494300165176392 and perplexity is 89.50550799013341
At time: 90.61829733848572 and batch: 350, loss is 4.577310371398926 and perplexity is 97.25246910190474
At time: 91.06919741630554 and batch: 400, loss is 4.4833505344390865 and perplexity is 88.53080180356277
At time: 91.50517272949219 and batch: 450, loss is 4.5344030475616455 and perplexity is 93.16788190654924
At time: 91.93674969673157 and batch: 500, loss is 4.486172904968262 and perplexity is 88.78102146970373
At time: 92.36781430244446 and batch: 550, loss is 4.528667230606079 and perplexity is 92.63501765661273
At time: 92.8084785938263 and batch: 600, loss is 4.545647792816162 and perplexity is 94.22144341851704
At time: 93.25150728225708 and batch: 650, loss is 4.522575874328613 and perplexity is 92.07245986970705
At time: 93.68928623199463 and batch: 700, loss is 4.490549068450928 and perplexity is 89.1703930893466
At time: 94.12574529647827 and batch: 750, loss is 4.441442384719848 and perplexity is 84.89730798275649
At time: 94.57428646087646 and batch: 800, loss is 4.480782814025879 and perplexity is 88.3037710570533
At time: 95.00384783744812 and batch: 850, loss is 4.455531778335572 and perplexity is 86.10192581178957
At time: 95.44604134559631 and batch: 900, loss is 4.569780941009522 and perplexity is 96.52296323387547
At time: 95.88879656791687 and batch: 950, loss is 4.493934240341186 and perplexity is 89.47276169357914
At time: 96.3266875743866 and batch: 1000, loss is 4.4741108608245845 and perplexity is 87.71657348467892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.927959907345656 and perplexity of 138.09749308384374
Finished 10 epochs...
Completing Train Step...
At time: 97.69826602935791 and batch: 50, loss is 4.607087106704712 and perplexity is 100.19187591836814
At time: 98.13865518569946 and batch: 100, loss is 4.542279949188233 and perplexity is 93.90465407859408
At time: 98.58921456336975 and batch: 150, loss is 4.578786449432373 and perplexity is 97.39612733454173
At time: 99.02784705162048 and batch: 200, loss is 4.600555772781372 and perplexity is 99.53962168411388
At time: 99.47105431556702 and batch: 250, loss is 4.569838895797729 and perplexity is 96.52855736386861
At time: 99.89837145805359 and batch: 300, loss is 4.481065626144409 and perplexity is 88.32874796533983
At time: 100.32831025123596 and batch: 350, loss is 4.563014268875122 and perplexity is 95.87202880191107
At time: 100.76714849472046 and batch: 400, loss is 4.470092220306396 and perplexity is 87.36477944868085
At time: 101.20057320594788 and batch: 450, loss is 4.521595897674561 and perplexity is 91.98227520520392
At time: 101.64238119125366 and batch: 500, loss is 4.473795413970947 and perplexity is 87.68890793129663
At time: 102.0772659778595 and batch: 550, loss is 4.516252880096435 and perplexity is 91.49212290415188
At time: 102.51933813095093 and batch: 600, loss is 4.533375234603882 and perplexity is 93.07217194467344
At time: 102.95717763900757 and batch: 650, loss is 4.509833154678344 and perplexity is 90.90664989591602
At time: 103.40233874320984 and batch: 700, loss is 4.4796271324157715 and perplexity is 88.2017789592871
At time: 103.8400399684906 and batch: 750, loss is 4.430734605789184 and perplexity is 83.9930960671847
At time: 104.29032683372498 and batch: 800, loss is 4.470601902008057 and perplexity is 87.40931902767547
At time: 104.73695969581604 and batch: 850, loss is 4.445528144836426 and perplexity is 85.24488759765649
At time: 105.16255331039429 and batch: 900, loss is 4.560264215469361 and perplexity is 95.60873780067905
At time: 105.59645533561707 and batch: 950, loss is 4.484233808517456 and perplexity is 88.60903331077819
At time: 106.03168272972107 and batch: 1000, loss is 4.462759103775024 and perplexity is 86.7264666118694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.928183206697789 and perplexity of 138.12833360778697
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 107.37438201904297 and batch: 50, loss is 4.592292747497559 and perplexity is 98.72051209558047
At time: 107.8367018699646 and batch: 100, loss is 4.516954965591431 and perplexity is 91.55638075116087
At time: 108.28855776786804 and batch: 150, loss is 4.551218929290772 and perplexity is 94.74782885972596
At time: 108.72607684135437 and batch: 200, loss is 4.570463991165161 and perplexity is 96.58891578081963
At time: 109.17393589019775 and batch: 250, loss is 4.534186964035034 and perplexity is 93.14775203700489
At time: 109.6265299320221 and batch: 300, loss is 4.441632785797119 and perplexity is 84.91347406062428
At time: 110.06005620956421 and batch: 350, loss is 4.524919195175171 and perplexity is 92.28846817385013
At time: 110.51282000541687 and batch: 400, loss is 4.427789154052735 and perplexity is 83.74606244792405
At time: 110.94535851478577 and batch: 450, loss is 4.477421550750733 and perplexity is 88.00745710792161
At time: 111.38294720649719 and batch: 500, loss is 4.425285911560058 and perplexity is 83.53668791275899
At time: 111.82306551933289 and batch: 550, loss is 4.4664436626434325 and perplexity is 87.04660480537764
At time: 112.2851049900055 and batch: 600, loss is 4.481027841567993 and perplexity is 88.32541056406413
At time: 112.73009657859802 and batch: 650, loss is 4.45344428062439 and perplexity is 85.92237570914457
At time: 113.16250228881836 and batch: 700, loss is 4.425610485076905 and perplexity is 83.5638061100268
At time: 113.58834052085876 and batch: 750, loss is 4.369774045944214 and perplexity is 79.02577348804154
At time: 114.03377342224121 and batch: 800, loss is 4.404949979782105 and perplexity is 81.85504833341085
At time: 114.47176265716553 and batch: 850, loss is 4.372078104019165 and perplexity is 79.20806338218391
At time: 114.91455960273743 and batch: 900, loss is 4.485983753204346 and perplexity is 88.76422997101135
At time: 115.358065366745 and batch: 950, loss is 4.410239124298096 and perplexity is 82.28913848420599
At time: 115.80794739723206 and batch: 1000, loss is 4.389570426940918 and perplexity is 80.60578546835438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.89751117985423 and perplexity of 133.95597214166042
Finished 12 epochs...
Completing Train Step...
At time: 117.14321613311768 and batch: 50, loss is 4.566358699798584 and perplexity is 96.19320295274525
At time: 117.58408069610596 and batch: 100, loss is 4.494770441055298 and perplexity is 89.54761017061446
At time: 118.02262449264526 and batch: 150, loss is 4.532366228103638 and perplexity is 92.97830888037393
At time: 118.459397315979 and batch: 200, loss is 4.552548408508301 and perplexity is 94.87387790032474
At time: 118.89507555961609 and batch: 250, loss is 4.5197525978088375 and perplexity is 91.81288046028799
At time: 119.32963800430298 and batch: 300, loss is 4.428875551223755 and perplexity is 83.83709337216752
At time: 119.75514602661133 and batch: 350, loss is 4.5112212371826175 and perplexity is 91.03292344493566
At time: 120.19384288787842 and batch: 400, loss is 4.417339725494385 and perplexity is 82.87552020704284
At time: 120.63753914833069 and batch: 450, loss is 4.4680289459228515 and perplexity is 87.18470776974345
At time: 121.08835244178772 and batch: 500, loss is 4.416145114898682 and perplexity is 82.77657534453667
At time: 121.5350968837738 and batch: 550, loss is 4.458134670257568 and perplexity is 86.32633174445138
At time: 121.98058223724365 and batch: 600, loss is 4.474198179244995 and perplexity is 87.72423309172697
At time: 122.41897630691528 and batch: 650, loss is 4.448526086807251 and perplexity is 85.50083028294928
At time: 122.86355113983154 and batch: 700, loss is 4.422781162261963 and perplexity is 83.32771127863901
At time: 123.31739044189453 and batch: 750, loss is 4.368145427703857 and perplexity is 78.89717541888558
At time: 123.7550208568573 and batch: 800, loss is 4.402930746078491 and perplexity is 81.68993062273127
At time: 124.18839168548584 and batch: 850, loss is 4.372718963623047 and perplexity is 79.25884089919913
At time: 124.62989211082458 and batch: 900, loss is 4.488332691192627 and perplexity is 88.97297671338717
At time: 125.0716061592102 and batch: 950, loss is 4.4147323513031 and perplexity is 82.65971418035917
At time: 125.5161361694336 and batch: 1000, loss is 4.392732248306275 and perplexity is 80.86104990062852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.895750836628239 and perplexity of 133.72037108372325
Finished 13 epochs...
Completing Train Step...
At time: 126.8265917301178 and batch: 50, loss is 4.55790267944336 and perplexity is 95.38322071013545
At time: 127.2669882774353 and batch: 100, loss is 4.487038488388062 and perplexity is 88.85790211838805
At time: 127.712238073349 and batch: 150, loss is 4.524716081619263 and perplexity is 92.26972503846694
At time: 128.150062084198 and batch: 200, loss is 4.5455364227294925 and perplexity is 94.21095055250396
At time: 128.58223962783813 and batch: 250, loss is 4.513068218231201 and perplexity is 91.20121489705535
At time: 129.01681280136108 and batch: 300, loss is 4.421955986022949 and perplexity is 83.25897959301747
At time: 129.47356796264648 and batch: 350, loss is 4.504436731338501 and perplexity is 90.41740041463193
At time: 129.9102168083191 and batch: 400, loss is 4.411631498336792 and perplexity is 82.40379554853408
At time: 130.34468412399292 and batch: 450, loss is 4.462547407150269 and perplexity is 86.70810885481687
At time: 130.78305864334106 and batch: 500, loss is 4.411170110702515 and perplexity is 82.3657842259019
At time: 131.23078560829163 and batch: 550, loss is 4.4533625411987305 and perplexity is 85.91535275053306
At time: 131.67481565475464 and batch: 600, loss is 4.4705517387390135 and perplexity is 87.40493440046271
At time: 132.13152050971985 and batch: 650, loss is 4.445571737289429 and perplexity is 85.24860371240952
At time: 132.5705599784851 and batch: 700, loss is 4.420535497665405 and perplexity is 83.14079514153586
At time: 133.01635551452637 and batch: 750, loss is 4.366133470535278 and perplexity is 78.73859726094503
At time: 133.46514987945557 and batch: 800, loss is 4.400461854934693 and perplexity is 81.48849583907521
At time: 133.9056429862976 and batch: 850, loss is 4.371624212265015 and perplexity is 79.17211965325072
At time: 134.34742665290833 and batch: 900, loss is 4.488529348373413 and perplexity is 88.99047560873909
At time: 134.79455733299255 and batch: 950, loss is 4.4154771709442135 and perplexity is 82.7213036927015
At time: 135.23464608192444 and batch: 1000, loss is 4.39278136253357 and perplexity is 80.86502142614106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.894795115401105 and perplexity of 133.59263273742584
Finished 14 epochs...
Completing Train Step...
At time: 136.522287607193 and batch: 50, loss is 4.551565418243408 and perplexity is 94.78066362382512
At time: 136.97326564788818 and batch: 100, loss is 4.480960502624511 and perplexity is 88.31946302448692
At time: 137.41177201271057 and batch: 150, loss is 4.518568248748779 and perplexity is 91.70420632836952
At time: 137.85493111610413 and batch: 200, loss is 4.539955291748047 and perplexity is 93.68661146121104
At time: 138.30685544013977 and batch: 250, loss is 4.50776364326477 and perplexity is 90.71871208318709
At time: 138.7413251399994 and batch: 300, loss is 4.416303987503052 and perplexity is 82.78972731935907
At time: 139.1756238937378 and batch: 350, loss is 4.498766279220581 and perplexity is 89.90614377301965
At time: 139.65860056877136 and batch: 400, loss is 4.40678204536438 and perplexity is 82.00514960590141
At time: 140.11276006698608 and batch: 450, loss is 4.457983503341675 and perplexity is 86.31328304541184
At time: 140.55078196525574 and batch: 500, loss is 4.406698808670044 and perplexity is 81.99832405240194
At time: 140.9796862602234 and batch: 550, loss is 4.449569530487061 and perplexity is 85.5900921456909
At time: 141.42056107521057 and batch: 600, loss is 4.466956701278686 and perplexity is 87.0912745343791
At time: 141.85224676132202 and batch: 650, loss is 4.44281626701355 and perplexity is 85.01402705174856
At time: 142.28867983818054 and batch: 700, loss is 4.418761396408081 and perplexity is 82.99342571513569
At time: 142.72178316116333 and batch: 750, loss is 4.364430418014527 and perplexity is 78.60461541584215
At time: 143.15223097801208 and batch: 800, loss is 4.3981418037414555 and perplexity is 81.2996574990807
At time: 143.62246179580688 and batch: 850, loss is 4.370050649642945 and perplexity is 79.04763533267753
At time: 144.07571864128113 and batch: 900, loss is 4.487652177810669 and perplexity is 88.912450009053
At time: 144.51552867889404 and batch: 950, loss is 4.4151254081726075 and perplexity is 82.69221053488859
At time: 144.957275390625 and batch: 1000, loss is 4.391896085739136 and perplexity is 80.79346517740765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.894339956888339 and perplexity of 133.53184074943752
Finished 15 epochs...
Completing Train Step...
At time: 146.31534910202026 and batch: 50, loss is 4.546744375228882 and perplexity is 94.32482166729633
At time: 146.75523209571838 and batch: 100, loss is 4.475826835632324 and perplexity is 87.86722213267595
At time: 147.19340348243713 and batch: 150, loss is 4.513647680282593 and perplexity is 91.25407785468879
At time: 147.64090204238892 and batch: 200, loss is 4.535185489654541 and perplexity is 93.24080890591529
At time: 148.0949203968048 and batch: 250, loss is 4.503230094909668 and perplexity is 90.30836528160236
At time: 148.5365343093872 and batch: 300, loss is 4.411439266204834 and perplexity is 82.38795641367844
At time: 148.97939896583557 and batch: 350, loss is 4.494093723297119 and perplexity is 89.48703221201151
At time: 149.42127656936646 and batch: 400, loss is 4.402666683197022 and perplexity is 81.6683621921003
At time: 149.8571093082428 and batch: 450, loss is 4.45404161453247 and perplexity is 85.97371538955794
At time: 150.29396843910217 and batch: 500, loss is 4.402937898635864 and perplexity is 81.69051491673645
At time: 150.728741645813 and batch: 550, loss is 4.445745878219604 and perplexity is 85.26345027621585
At time: 151.16894030570984 and batch: 600, loss is 4.463577995300293 and perplexity is 86.7975152669948
At time: 151.60859513282776 and batch: 650, loss is 4.440039024353028 and perplexity is 84.7782500256325
At time: 152.06232333183289 and batch: 700, loss is 4.416934223175049 and perplexity is 82.841920804161
At time: 152.50274991989136 and batch: 750, loss is 4.362688045501709 and perplexity is 78.46777614167478
At time: 152.9450147151947 and batch: 800, loss is 4.39568398475647 and perplexity is 81.10008301684934
At time: 153.38840651512146 and batch: 850, loss is 4.368227510452271 and perplexity is 78.90365178168122
At time: 153.83303308486938 and batch: 900, loss is 4.486105079650879 and perplexity is 88.77500007294886
At time: 154.27351117134094 and batch: 950, loss is 4.414191608428955 and perplexity is 82.61502861172751
At time: 154.73233890533447 and batch: 1000, loss is 4.390590991973877 and perplexity is 80.68809090633978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.894262918611852 and perplexity of 133.52155408280868
Finished 16 epochs...
Completing Train Step...
At time: 156.181405544281 and batch: 50, loss is 4.542348804473877 and perplexity is 93.91112013298249
At time: 156.626211643219 and batch: 100, loss is 4.471042385101319 and perplexity is 87.44782983595798
At time: 157.0705862045288 and batch: 150, loss is 4.509138069152832 and perplexity is 90.84348395480943
At time: 157.53006172180176 and batch: 200, loss is 4.530713224411011 and perplexity is 92.82474235044836
At time: 157.97827672958374 and batch: 250, loss is 4.498981018066406 and perplexity is 89.9254521876252
At time: 158.42100548744202 and batch: 300, loss is 4.407332181930542 and perplexity is 82.05027604902834
At time: 158.86695885658264 and batch: 350, loss is 4.489523963928223 and perplexity is 89.07903095196998
At time: 159.30431699752808 and batch: 400, loss is 4.398965167999267 and perplexity is 81.36662429648828
At time: 159.7443675994873 and batch: 450, loss is 4.450673294067383 and perplexity is 85.68461552837495
At time: 160.19952297210693 and batch: 500, loss is 4.399164056777954 and perplexity is 81.3828088144266
At time: 160.6559820175171 and batch: 550, loss is 4.442435884475708 and perplexity is 84.98169534998374
At time: 161.11187505722046 and batch: 600, loss is 4.460643968582153 and perplexity is 86.54322227177809
At time: 161.55104112625122 and batch: 650, loss is 4.437669725418091 and perplexity is 84.57762277495965
At time: 161.99695992469788 and batch: 700, loss is 4.415221567153931 and perplexity is 82.70016251593806
At time: 162.44855999946594 and batch: 750, loss is 4.361019029617309 and perplexity is 78.33692140657237
At time: 162.89233231544495 and batch: 800, loss is 4.393238229751587 and perplexity is 80.90197444418342
At time: 163.34611320495605 and batch: 850, loss is 4.366441555023194 and perplexity is 78.76285913852354
At time: 163.78655815124512 and batch: 900, loss is 4.484513664245606 and perplexity is 88.63383452653528
At time: 164.2317283153534 and batch: 950, loss is 4.412921333312989 and perplexity is 82.5101514222217
At time: 164.6785707473755 and batch: 1000, loss is 4.389558897018433 and perplexity is 80.60485609525388
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.894089861613948 and perplexity of 133.49844924278847
Finished 17 epochs...
Completing Train Step...
At time: 166.02942657470703 and batch: 50, loss is 4.538506422042847 and perplexity is 93.5509700551725
At time: 166.4991478919983 and batch: 100, loss is 4.466883726119995 and perplexity is 87.0849192666905
At time: 166.9335904121399 and batch: 150, loss is 4.505195608139038 and perplexity is 90.48604212417776
At time: 167.38897728919983 and batch: 200, loss is 4.526821870803833 and perplexity is 92.46423034923066
At time: 167.82949304580688 and batch: 250, loss is 4.4950778102874756 and perplexity is 89.57513858127243
At time: 168.26998090744019 and batch: 300, loss is 4.402934551239014 and perplexity is 81.69024146662179
At time: 168.71351408958435 and batch: 350, loss is 4.485120687484741 and perplexity is 88.68765365694543
At time: 169.1554036140442 and batch: 400, loss is 4.395749540328979 and perplexity is 81.105399753491
At time: 169.60467433929443 and batch: 450, loss is 4.4472194480896 and perplexity is 85.3891845439173
At time: 170.03972172737122 and batch: 500, loss is 4.396060419082642 and perplexity is 81.13061761872781
At time: 170.48954439163208 and batch: 550, loss is 4.439252262115478 and perplexity is 84.71157593171573
At time: 170.93257689476013 and batch: 600, loss is 4.45800009727478 and perplexity is 86.31471533414046
At time: 171.3622531890869 and batch: 650, loss is 4.435254755020142 and perplexity is 84.37361675302957
At time: 171.79850435256958 and batch: 700, loss is 4.41317198753357 and perplexity is 82.53083553208789
At time: 172.24785375595093 and batch: 750, loss is 4.3587069416046145 and perplexity is 78.15600877319514
At time: 172.68937516212463 and batch: 800, loss is 4.390306577682495 and perplexity is 80.66514532332228
At time: 173.13126254081726 and batch: 850, loss is 4.3641448402404786 and perplexity is 78.58217088972323
At time: 173.5875256061554 and batch: 900, loss is 4.482556438446045 and perplexity is 88.46052775445301
At time: 174.02500987052917 and batch: 950, loss is 4.411372699737549 and perplexity is 82.3824723210035
At time: 174.479914188385 and batch: 1000, loss is 4.3876082229614255 and perplexity is 80.44777554988173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893989004739901 and perplexity of 133.4849856854653
Finished 18 epochs...
Completing Train Step...
At time: 175.9053406715393 and batch: 50, loss is 4.534596014022827 and perplexity is 93.18586191773029
At time: 176.3430438041687 and batch: 100, loss is 4.462954568862915 and perplexity is 86.74342026515782
At time: 176.78656673431396 and batch: 150, loss is 4.501008062362671 and perplexity is 90.10791993519608
At time: 177.24414920806885 and batch: 200, loss is 4.523290452957153 and perplexity is 92.13827639454746
At time: 177.68670535087585 and batch: 250, loss is 4.491678190231323 and perplexity is 89.27113418613042
At time: 178.14295649528503 and batch: 300, loss is 4.39911958694458 and perplexity is 81.37918981494786
At time: 178.59702849388123 and batch: 350, loss is 4.481356315612793 and perplexity is 88.35442793439628
At time: 179.04446053504944 and batch: 400, loss is 4.392250061035156 and perplexity is 80.82206913040804
At time: 179.4826204776764 and batch: 450, loss is 4.443881845474243 and perplexity is 85.10466444988505
At time: 179.91961693763733 and batch: 500, loss is 4.392693185806275 and perplexity is 80.85789132755806
At time: 180.3536398410797 and batch: 550, loss is 4.436420078277588 and perplexity is 84.47199660197658
At time: 180.8021059036255 and batch: 600, loss is 4.455417575836182 and perplexity is 86.09209331811797
At time: 181.24332118034363 and batch: 650, loss is 4.432789726257324 and perplexity is 84.16588949293143
At time: 181.69030594825745 and batch: 700, loss is 4.410996885299682 and perplexity is 82.3515176154551
At time: 182.14155673980713 and batch: 750, loss is 4.356677675247193 and perplexity is 77.99757022533856
At time: 182.57912921905518 and batch: 800, loss is 4.387532386779785 and perplexity is 80.44167492908939
At time: 183.02133202552795 and batch: 850, loss is 4.362109413146973 and perplexity is 78.42238528117169
At time: 183.46484303474426 and batch: 900, loss is 4.480817594528198 and perplexity is 88.30684235997775
At time: 183.90613770484924 and batch: 950, loss is 4.409758834838867 and perplexity is 82.2496253680092
At time: 184.35687470436096 and batch: 1000, loss is 4.385259523391723 and perplexity is 80.25904961108698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893766449718941 and perplexity of 133.45528123723778
Finished 19 epochs...
Completing Train Step...
At time: 185.67287158966064 and batch: 50, loss is 4.530955781936646 and perplexity is 92.8472604211239
At time: 186.1235818862915 and batch: 100, loss is 4.458774976730346 and perplexity is 86.38162475378533
At time: 186.5710220336914 and batch: 150, loss is 4.496858968734741 and perplexity is 89.73482827012225
At time: 187.01697158813477 and batch: 200, loss is 4.519440031051635 and perplexity is 91.78418729047226
At time: 187.4603831768036 and batch: 250, loss is 4.4876973342895505 and perplexity is 88.91646507287652
At time: 187.89856624603271 and batch: 300, loss is 4.395260238647461 and perplexity is 81.06572445239877
At time: 188.34482336044312 and batch: 350, loss is 4.477389631271362 and perplexity is 88.00464800054284
At time: 188.78672575950623 and batch: 400, loss is 4.388596076965332 and perplexity is 80.52728547269597
At time: 189.24426198005676 and batch: 450, loss is 4.440376634597778 and perplexity is 84.80687686345816
At time: 189.67977118492126 and batch: 500, loss is 4.389330797195434 and perplexity is 80.58647223860265
At time: 190.11817026138306 and batch: 550, loss is 4.433156204223633 and perplexity is 84.19674008963206
At time: 190.56032395362854 and batch: 600, loss is 4.451891441345214 and perplexity is 85.78905560831767
At time: 191.02653336524963 and batch: 650, loss is 4.429921846389771 and perplexity is 83.92485762334496
At time: 191.46674871444702 and batch: 700, loss is 4.408778142929077 and perplexity is 82.16900336496217
At time: 191.91146063804626 and batch: 750, loss is 4.35412899017334 and perplexity is 77.79903209536461
At time: 192.36023926734924 and batch: 800, loss is 4.384866123199463 and perplexity is 80.22748189531919
At time: 192.79863238334656 and batch: 850, loss is 4.359082307815552 and perplexity is 78.185351404842
At time: 193.25701141357422 and batch: 900, loss is 4.478416366577148 and perplexity is 88.09505188220497
At time: 193.6938087940216 and batch: 950, loss is 4.407621440887451 and perplexity is 82.07401325922719
At time: 194.13410449028015 and batch: 1000, loss is 4.382869424819947 and perplexity is 80.06745163147784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893495513171684 and perplexity of 133.419128221934
Finished 20 epochs...
Completing Train Step...
At time: 195.569482088089 and batch: 50, loss is 4.527127914428711 and perplexity is 92.49253276812483
At time: 196.0362091064453 and batch: 100, loss is 4.455287790298462 and perplexity is 86.08092053454175
At time: 196.48271536827087 and batch: 150, loss is 4.493567180633545 and perplexity is 89.43992587455178
At time: 196.93182802200317 and batch: 200, loss is 4.516009664535522 and perplexity is 91.46987330199501
At time: 197.37399172782898 and batch: 250, loss is 4.484074487686157 and perplexity is 88.59491717045795
At time: 197.81941103935242 and batch: 300, loss is 4.391703701019287 and perplexity is 80.77792324430702
At time: 198.26712584495544 and batch: 350, loss is 4.47396068572998 and perplexity is 87.70340162902458
At time: 198.71242356300354 and batch: 400, loss is 4.38540675163269 and perplexity is 80.27086687967936
At time: 199.157044172287 and batch: 450, loss is 4.437197885513306 and perplexity is 84.53772509088252
At time: 199.60363912582397 and batch: 500, loss is 4.386114597320557 and perplexity is 80.32770638108619
At time: 200.062335729599 and batch: 550, loss is 4.430370531082153 and perplexity is 83.96252187132481
At time: 200.508460521698 and batch: 600, loss is 4.449340810775757 and perplexity is 85.57051824307888
At time: 200.97915744781494 and batch: 650, loss is 4.427603178024292 and perplexity is 83.73048913600847
At time: 201.42856168746948 and batch: 700, loss is 4.406815538406372 and perplexity is 82.00789625381728
At time: 201.88270688056946 and batch: 750, loss is 4.351954441070557 and perplexity is 77.63003808939926
At time: 202.33034372329712 and batch: 800, loss is 4.3822054767608645 and perplexity is 80.01430864641459
At time: 202.77119541168213 and batch: 850, loss is 4.357003297805786 and perplexity is 78.02297212921142
At time: 203.21594071388245 and batch: 900, loss is 4.47643663406372 and perplexity is 87.92081976711945
At time: 203.65342831611633 and batch: 950, loss is 4.405690498352051 and perplexity is 81.91568596562098
At time: 204.09920167922974 and batch: 1000, loss is 4.380889263153076 and perplexity is 79.90906200328259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893417730564025 and perplexity of 133.40875093782054
Finished 21 epochs...
Completing Train Step...
At time: 205.41303062438965 and batch: 50, loss is 4.523455324172974 and perplexity is 92.15346859654429
At time: 205.88427186012268 and batch: 100, loss is 4.451902017593384 and perplexity is 85.78996293945809
At time: 206.33273339271545 and batch: 150, loss is 4.490166692733765 and perplexity is 89.13630301436312
At time: 206.7890100479126 and batch: 200, loss is 4.512941913604736 and perplexity is 91.1896964891041
At time: 207.23592567443848 and batch: 250, loss is 4.4806202793121335 and perplexity is 88.28941979522257
At time: 207.6793031692505 and batch: 300, loss is 4.388345241546631 and perplexity is 80.50708891044006
At time: 208.11808562278748 and batch: 350, loss is 4.470369939804077 and perplexity is 87.3890457207968
At time: 208.5648319721222 and batch: 400, loss is 4.382549562454224 and perplexity is 80.04184516247277
At time: 209.00546050071716 and batch: 450, loss is 4.434124193191528 and perplexity is 84.27828106418646
At time: 209.44399905204773 and batch: 500, loss is 4.383330049514771 and perplexity is 80.10434117241545
At time: 209.8786997795105 and batch: 550, loss is 4.427656002044678 and perplexity is 83.73491223389543
At time: 210.3267936706543 and batch: 600, loss is 4.446462125778198 and perplexity is 85.32454189006138
At time: 210.76263999938965 and batch: 650, loss is 4.425290994644165 and perplexity is 83.53711253784884
At time: 211.21261763572693 and batch: 700, loss is 4.404555835723877 and perplexity is 81.82279200971105
At time: 211.66541385650635 and batch: 750, loss is 4.349908103942871 and perplexity is 77.47134328714398
At time: 212.11405110359192 and batch: 800, loss is 4.3794568824768065 and perplexity is 79.794683743171
At time: 212.55754733085632 and batch: 850, loss is 4.354841461181641 and perplexity is 77.85448140087215
At time: 213.0064001083374 and batch: 900, loss is 4.4744558429718015 and perplexity is 87.7468393568388
At time: 213.45065593719482 and batch: 950, loss is 4.403990888595581 and perplexity is 81.7765795133737
At time: 213.891263961792 and batch: 1000, loss is 4.379043378829956 and perplexity is 79.76169517136135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893635447432355 and perplexity of 133.4377994353418
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 215.2736988067627 and batch: 50, loss is 4.521288185119629 and perplexity is 91.95397545860905
At time: 215.72308707237244 and batch: 100, loss is 4.447149686813354 and perplexity is 85.38322789320014
At time: 216.16600489616394 and batch: 150, loss is 4.484683074951172 and perplexity is 88.64885131894623
At time: 216.61348390579224 and batch: 200, loss is 4.505951089859009 and perplexity is 90.55442850399233
At time: 217.0716028213501 and batch: 250, loss is 4.471409072875977 and perplexity is 87.47990176591216
At time: 217.50864052772522 and batch: 300, loss is 4.379950675964356 and perplexity is 79.83409556819487
At time: 217.9635467529297 and batch: 350, loss is 4.45983136177063 and perplexity is 86.47292522566637
At time: 218.41361904144287 and batch: 400, loss is 4.37026873588562 and perplexity is 79.06487641441265
At time: 218.8519642353058 and batch: 450, loss is 4.420239515304566 and perplexity is 83.1161905741464
At time: 219.3032591342926 and batch: 500, loss is 4.369078598022461 and perplexity is 78.97083428399624
At time: 219.7570321559906 and batch: 550, loss is 4.411175680160523 and perplexity is 82.36624295995587
At time: 220.19521617889404 and batch: 600, loss is 4.430315847396851 and perplexity is 83.95793061673609
At time: 220.63634538650513 and batch: 650, loss is 4.4076198387145995 and perplexity is 82.07388176257669
At time: 221.08818101882935 and batch: 700, loss is 4.385134916305542 and perplexity is 80.24904938783757
At time: 221.54430961608887 and batch: 750, loss is 4.330164794921875 and perplexity is 75.9568028332394
At time: 221.9841296672821 and batch: 800, loss is 4.356112756729126 and perplexity is 77.95352039700741
At time: 222.42922377586365 and batch: 850, loss is 4.330328817367554 and perplexity is 75.96926247560863
At time: 222.86580061912537 and batch: 900, loss is 4.448331251144409 and perplexity is 85.48417329474788
At time: 223.29875993728638 and batch: 950, loss is 4.3777175140380855 and perplexity is 79.6560120242575
At time: 223.762366771698 and batch: 1000, loss is 4.35290997505188 and perplexity is 77.70425167992215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.886217442954459 and perplexity of 132.45161950735962
Finished 23 epochs...
Completing Train Step...
At time: 225.05300068855286 and batch: 50, loss is 4.514362878799439 and perplexity is 91.31936598002888
At time: 225.51554346084595 and batch: 100, loss is 4.441159706115723 and perplexity is 84.87331272187514
At time: 225.96908569335938 and batch: 150, loss is 4.478911380767823 and perplexity is 88.13867097815991
At time: 226.4151895046234 and batch: 200, loss is 4.500466575622559 and perplexity is 90.05914089916453
At time: 226.86242747306824 and batch: 250, loss is 4.467577524185181 and perplexity is 87.14535957944615
At time: 227.31999158859253 and batch: 300, loss is 4.375858898162842 and perplexity is 79.50809959452535
At time: 227.76723957061768 and batch: 350, loss is 4.455762491226197 and perplexity is 86.12179292769373
At time: 228.21026992797852 and batch: 400, loss is 4.367217226028442 and perplexity is 78.82397690522316
At time: 228.65004181861877 and batch: 450, loss is 4.417362689971924 and perplexity is 82.87742342191827
At time: 229.10372686386108 and batch: 500, loss is 4.366326179504394 and perplexity is 78.75377235699446
At time: 229.54874420166016 and batch: 550, loss is 4.409026327133179 and perplexity is 82.18939894448978
At time: 229.99904131889343 and batch: 600, loss is 4.429133672714233 and perplexity is 83.85873632080099
At time: 230.43599128723145 and batch: 650, loss is 4.405821619033813 and perplexity is 81.92642751041573
At time: 230.88603711128235 and batch: 700, loss is 4.383856086730957 and perplexity is 80.14649012203635
At time: 231.32901191711426 and batch: 750, loss is 4.32923583984375 and perplexity is 75.88627513913097
At time: 231.7715926170349 and batch: 800, loss is 4.3563294601440425 and perplexity is 77.97041502157751
At time: 232.21370935440063 and batch: 850, loss is 4.331125640869141 and perplexity is 76.02982069324878
At time: 232.6560730934143 and batch: 900, loss is 4.4494763946533205 and perplexity is 85.58212101230384
At time: 233.09699273109436 and batch: 950, loss is 4.379185218811035 and perplexity is 79.77300937107923
At time: 233.54290342330933 and batch: 1000, loss is 4.353848562240601 and perplexity is 77.77721813239023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.885321268221227 and perplexity of 132.3329728845749
Finished 24 epochs...
Completing Train Step...
At time: 234.94676208496094 and batch: 50, loss is 4.51198166847229 and perplexity is 91.10217405514196
At time: 235.3968460559845 and batch: 100, loss is 4.438455123901367 and perplexity is 84.64407600432301
At time: 235.83839082717896 and batch: 150, loss is 4.476282949447632 and perplexity is 87.9073087279334
At time: 236.2869930267334 and batch: 200, loss is 4.497837772369385 and perplexity is 89.82270404573566
At time: 236.7258334159851 and batch: 250, loss is 4.465486783981323 and perplexity is 86.96335160458831
At time: 237.18546295166016 and batch: 300, loss is 4.373679609298706 and perplexity is 79.33501714529916
At time: 237.62847638130188 and batch: 350, loss is 4.453546237945557 and perplexity is 85.93113657100962
At time: 238.0786259174347 and batch: 400, loss is 4.365409984588623 and perplexity is 78.68165159454601
At time: 238.53056812286377 and batch: 450, loss is 4.415668830871582 and perplexity is 82.73715957117881
At time: 238.96792483329773 and batch: 500, loss is 4.364678220748901 and perplexity is 78.62409626808122
At time: 239.4257528781891 and batch: 550, loss is 4.407895946502686 and perplexity is 82.0965461292899
At time: 239.87903571128845 and batch: 600, loss is 4.42855525970459 and perplexity is 83.81024536199342
At time: 240.31579852104187 and batch: 650, loss is 4.404609503746033 and perplexity is 81.82718339496293
At time: 240.7642524242401 and batch: 700, loss is 4.383125677108764 and perplexity is 80.0879717282668
At time: 241.2065305709839 and batch: 750, loss is 4.328591651916504 and perplexity is 75.8374058590328
At time: 241.6440908908844 and batch: 800, loss is 4.356364450454712 and perplexity is 77.97314327835313
At time: 242.09845304489136 and batch: 850, loss is 4.331486673355102 and perplexity is 76.05727488405002
At time: 242.56614804267883 and batch: 900, loss is 4.449980716705323 and perplexity is 85.62529284852695
At time: 243.0015254020691 and batch: 950, loss is 4.379877347946167 and perplexity is 79.82824170681158
At time: 243.44230914115906 and batch: 1000, loss is 4.35409631729126 and perplexity is 77.79649021828848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884852711747333 and perplexity of 132.27098193768404
Finished 25 epochs...
Completing Train Step...
At time: 244.79473614692688 and batch: 50, loss is 4.5101896667480466 and perplexity is 90.93906499166322
At time: 245.23887872695923 and batch: 100, loss is 4.436397457122803 and perplexity is 84.47008576947916
At time: 245.68453359603882 and batch: 150, loss is 4.474395475387573 and perplexity is 87.74154245200543
At time: 246.12052631378174 and batch: 200, loss is 4.495899343490601 and perplexity is 89.64875776795405
At time: 246.57007670402527 and batch: 250, loss is 4.463883991241455 and perplexity is 86.82407901836226
At time: 247.0089395046234 and batch: 300, loss is 4.372090072631836 and perplexity is 79.20901139848813
At time: 247.4507384300232 and batch: 350, loss is 4.4518818855285645 and perplexity is 85.78823582774855
At time: 247.89063596725464 and batch: 400, loss is 4.364046602249146 and perplexity is 78.57445151427464
At time: 248.33583641052246 and batch: 450, loss is 4.414275321960449 and perplexity is 82.6219448970164
At time: 248.80334854125977 and batch: 500, loss is 4.363400735855103 and perplexity is 78.5237193014904
At time: 249.24053072929382 and batch: 550, loss is 4.407057104110717 and perplexity is 82.02770894197437
At time: 249.68165373802185 and batch: 600, loss is 4.428108139038086 and perplexity is 83.7727804455207
At time: 250.12898468971252 and batch: 650, loss is 4.4038013124465945 and perplexity is 81.76107809374838
At time: 250.57070875167847 and batch: 700, loss is 4.382574129104614 and perplexity is 80.04381154665312
At time: 251.01967692375183 and batch: 750, loss is 4.328085956573486 and perplexity is 75.79906493129468
At time: 251.46199369430542 and batch: 800, loss is 4.356306085586548 and perplexity is 77.96859251892887
At time: 251.91803860664368 and batch: 850, loss is 4.331612691879273 and perplexity is 76.06686011352889
At time: 252.36320185661316 and batch: 900, loss is 4.4502479839324955 and perplexity is 85.64818074157806
At time: 252.80604314804077 and batch: 950, loss is 4.380145502090454 and perplexity is 79.84965085100355
At time: 253.24191522598267 and batch: 1000, loss is 4.354019021987915 and perplexity is 77.79047714737192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884585869021532 and perplexity of 132.2356910970809
Finished 26 epochs...
Completing Train Step...
At time: 254.57574343681335 and batch: 50, loss is 4.508785562515259 and perplexity is 90.81146666721982
At time: 255.03443956375122 and batch: 100, loss is 4.434707059860229 and perplexity is 84.327418383976
At time: 255.49289774894714 and batch: 150, loss is 4.472709484100342 and perplexity is 87.59373561143839
At time: 255.9348075389862 and batch: 200, loss is 4.494380712509155 and perplexity is 89.51271771042765
At time: 256.3830146789551 and batch: 250, loss is 4.462618284225464 and perplexity is 86.714254689765
At time: 256.83403849601746 and batch: 300, loss is 4.370813598632813 and perplexity is 79.107967658523
At time: 257.2742404937744 and batch: 350, loss is 4.45051194190979 and perplexity is 85.67079124610578
At time: 257.7200229167938 and batch: 400, loss is 4.36286527633667 and perplexity is 78.48168428359727
At time: 258.18599247932434 and batch: 450, loss is 4.4130306148529055 and perplexity is 82.51916875133273
At time: 258.6271517276764 and batch: 500, loss is 4.3622567844390865 and perplexity is 78.4339433410634
At time: 259.0761387348175 and batch: 550, loss is 4.406247396469116 and perplexity is 81.96131736173325
At time: 259.5290012359619 and batch: 600, loss is 4.427624530792237 and perplexity is 83.73227703280111
At time: 259.97493529319763 and batch: 650, loss is 4.402981581687928 and perplexity is 81.69408348569507
At time: 260.4184744358063 and batch: 700, loss is 4.38186276435852 and perplexity is 79.98689144885164
At time: 260.8729839324951 and batch: 750, loss is 4.327547149658203 and perplexity is 75.75823487168475
At time: 261.33650279045105 and batch: 800, loss is 4.356169090270996 and perplexity is 77.95791191860647
At time: 261.7729983329773 and batch: 850, loss is 4.331643676757812 and perplexity is 76.06921707246524
At time: 262.2148721218109 and batch: 900, loss is 4.45029543876648 and perplexity is 85.65224525821591
At time: 262.6485311985016 and batch: 950, loss is 4.380213232040405 and perplexity is 79.8550592470124
At time: 263.11141657829285 and batch: 1000, loss is 4.353851079940796 and perplexity is 77.777413952354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884397925400153 and perplexity of 132.21084057774203
Finished 27 epochs...
Completing Train Step...
At time: 264.4664809703827 and batch: 50, loss is 4.5075356388092045 and perplexity is 90.69803017050266
At time: 264.92291140556335 and batch: 100, loss is 4.4331553936004635 and perplexity is 84.1966718378314
At time: 265.38967728614807 and batch: 150, loss is 4.471264324188232 and perplexity is 87.46724008133076
At time: 265.81838274002075 and batch: 200, loss is 4.492935380935669 and perplexity is 89.38343560355335
At time: 266.2607355117798 and batch: 250, loss is 4.461521196365356 and perplexity is 86.61917369928344
At time: 266.7036521434784 and batch: 300, loss is 4.369572038650513 and perplexity is 79.00981131769825
At time: 267.13761925697327 and batch: 350, loss is 4.449285364151001 and perplexity is 85.56577377819714
At time: 267.5902056694031 and batch: 400, loss is 4.361879997253418 and perplexity is 78.40439600316972
At time: 268.02644085884094 and batch: 450, loss is 4.411973152160645 and perplexity is 82.4319539303187
At time: 268.46745324134827 and batch: 500, loss is 4.361153964996338 and perplexity is 78.34749254194834
At time: 268.90894293785095 and batch: 550, loss is 4.405481615066528 and perplexity is 81.8985769349581
At time: 269.3757529258728 and batch: 600, loss is 4.427174787521363 and perplexity is 83.69462747160367
At time: 269.8128249645233 and batch: 650, loss is 4.402276883125305 and perplexity is 81.63653406236747
At time: 270.28290033340454 and batch: 700, loss is 4.381315965652465 and perplexity is 79.94316667552022
At time: 270.7184114456177 and batch: 750, loss is 4.3270290184021 and perplexity is 75.71899232956225
At time: 271.16066455841064 and batch: 800, loss is 4.356084718704223 and perplexity is 77.95133476490182
At time: 271.6166150569916 and batch: 850, loss is 4.331480808258057 and perplexity is 76.05682880205998
At time: 272.06829380989075 and batch: 900, loss is 4.450285129547119 and perplexity is 85.65136225498235
At time: 272.5336058139801 and batch: 950, loss is 4.380126953125 and perplexity is 79.84816973632499
At time: 272.9956045150757 and batch: 1000, loss is 4.353578929901123 and perplexity is 77.75624970611706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884242360184833 and perplexity of 132.19027476956353
Finished 28 epochs...
Completing Train Step...
At time: 274.3467845916748 and batch: 50, loss is 4.506420154571533 and perplexity is 90.59691435449398
At time: 274.7937424182892 and batch: 100, loss is 4.43183162689209 and perplexity is 84.08528882552368
At time: 275.2322907447815 and batch: 150, loss is 4.469969272613525 and perplexity is 87.3540388108913
At time: 275.66452503204346 and batch: 200, loss is 4.491690225601197 and perplexity is 89.27220860371484
At time: 276.1023020744324 and batch: 250, loss is 4.46058723449707 and perplexity is 86.53831246052049
At time: 276.550390958786 and batch: 300, loss is 4.368454818725586 and perplexity is 78.92158927311883
At time: 276.9912157058716 and batch: 350, loss is 4.448153266906738 and perplexity is 85.4689598132512
At time: 277.45306754112244 and batch: 400, loss is 4.360909948348999 and perplexity is 78.32837678186753
At time: 277.89934515953064 and batch: 450, loss is 4.410903215408325 and perplexity is 82.34380411901378
At time: 278.3490262031555 and batch: 500, loss is 4.360187244415283 and perplexity is 78.27178900641269
At time: 278.7912919521332 and batch: 550, loss is 4.40476728439331 and perplexity is 81.84009515951035
At time: 279.24249935150146 and batch: 600, loss is 4.426673383712768 and perplexity is 83.6526731855289
At time: 279.6871247291565 and batch: 650, loss is 4.401570830345154 and perplexity is 81.57891470407856
At time: 280.1447682380676 and batch: 700, loss is 4.380715618133545 and perplexity is 79.89518739731328
At time: 280.62905383110046 and batch: 750, loss is 4.3264658164978025 and perplexity is 75.67635925553193
At time: 281.07844638824463 and batch: 800, loss is 4.355774269104004 and perplexity is 77.92713856023288
At time: 281.51287817955017 and batch: 850, loss is 4.331377744674683 and perplexity is 76.04899051667138
At time: 281.95085763931274 and batch: 900, loss is 4.450224294662475 and perplexity is 85.64615182272962
At time: 282.3843939304352 and batch: 950, loss is 4.379958086013794 and perplexity is 79.83468714498167
At time: 282.84471464157104 and batch: 1000, loss is 4.353169994354248 and perplexity is 77.72445891225641
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884151551781631 and perplexity of 132.17827132680796
Finished 29 epochs...
Completing Train Step...
At time: 284.16851472854614 and batch: 50, loss is 4.505422201156616 and perplexity is 90.50654795266239
At time: 284.61428117752075 and batch: 100, loss is 4.430681552886963 and perplexity is 83.98864010787358
At time: 285.09166979789734 and batch: 150, loss is 4.468717947006225 and perplexity is 87.24479882687494
At time: 285.53393030166626 and batch: 200, loss is 4.490577268600464 and perplexity is 89.1729077432225
At time: 285.97580218315125 and batch: 250, loss is 4.45964204788208 and perplexity is 86.4565562494238
At time: 286.41735553741455 and batch: 300, loss is 4.367358150482178 and perplexity is 78.83508591385693
At time: 286.87675404548645 and batch: 350, loss is 4.4471242713928225 and perplexity is 85.38105787013302
At time: 287.3161141872406 and batch: 400, loss is 4.3600930404663085 and perplexity is 78.264415842091
At time: 287.7673189640045 and batch: 450, loss is 4.4099777317047115 and perplexity is 82.26763152389276
At time: 288.20740246772766 and batch: 500, loss is 4.3592892837524415 and perplexity is 78.20153556600826
At time: 288.6421208381653 and batch: 550, loss is 4.404053506851196 and perplexity is 81.78170038046116
At time: 289.08390712738037 and batch: 600, loss is 4.426158494949341 and perplexity is 83.60961245077057
At time: 289.51150608062744 and batch: 650, loss is 4.4007451581954955 and perplexity is 81.51158506613656
At time: 289.95569133758545 and batch: 700, loss is 4.380286626815796 and perplexity is 79.86092040623625
At time: 290.39994263648987 and batch: 750, loss is 4.3259345817565915 and perplexity is 75.6361680208423
At time: 290.8369975090027 and batch: 800, loss is 4.355529088973999 and perplexity is 77.90803471630687
At time: 291.27123403549194 and batch: 850, loss is 4.331097679138184 and perplexity is 76.02769479757981
At time: 291.71209168434143 and batch: 900, loss is 4.450044775009156 and perplexity is 85.63077803523603
At time: 292.1699287891388 and batch: 950, loss is 4.379606685638428 and perplexity is 79.80663813445675
At time: 292.61038160324097 and batch: 1000, loss is 4.352700567245483 and perplexity is 77.68798150664028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.884092005287728 and perplexity of 132.1704008085135
Finished 30 epochs...
Completing Train Step...
At time: 294.01666283607483 and batch: 50, loss is 4.504482221603394 and perplexity is 90.42151361968239
At time: 294.48920917510986 and batch: 100, loss is 4.429555549621582 and perplexity is 83.8941218487779
At time: 294.9418752193451 and batch: 150, loss is 4.467547798156739 and perplexity is 87.14276913251072
At time: 295.3793168067932 and batch: 200, loss is 4.489511518478394 and perplexity is 89.07792233025812
At time: 295.82264971733093 and batch: 250, loss is 4.458745851516723 and perplexity is 86.37910890714872
At time: 296.2623426914215 and batch: 300, loss is 4.366289196014404 and perplexity is 78.75085982150098
At time: 296.701735496521 and batch: 350, loss is 4.446063690185547 and perplexity is 85.29055232742112
At time: 297.14367055892944 and batch: 400, loss is 4.359264822006225 and perplexity is 78.19962264328835
At time: 297.5842649936676 and batch: 450, loss is 4.408935499191284 and perplexity is 82.18193418954738
At time: 298.03067994117737 and batch: 500, loss is 4.358346834182739 and perplexity is 78.12786928129576
At time: 298.4761872291565 and batch: 550, loss is 4.403381605148315 and perplexity is 81.72676957282918
At time: 298.9258449077606 and batch: 600, loss is 4.425670919418335 and perplexity is 83.56885638621996
At time: 299.37317943573 and batch: 650, loss is 4.4002008056640625 and perplexity is 81.467226103017
At time: 299.82051062583923 and batch: 700, loss is 4.379392070770264 and perplexity is 79.789512281132
At time: 300.28050661087036 and batch: 750, loss is 4.325397500991821 and perplexity is 75.59555619676836
At time: 300.7255072593689 and batch: 800, loss is 4.355172529220581 and perplexity is 77.88026079848406
At time: 301.16023778915405 and batch: 850, loss is 4.3308063507080075 and perplexity is 76.00554899461216
At time: 301.6018695831299 and batch: 900, loss is 4.449794359207154 and perplexity is 85.60933741992476
At time: 302.0315315723419 and batch: 950, loss is 4.379359083175659 and perplexity is 79.78688026045945
At time: 302.46792793273926 and batch: 1000, loss is 4.352212924957275 and perplexity is 77.6501067969777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.88398296077077 and perplexity of 132.15598913677235
Finished 31 epochs...
Completing Train Step...
At time: 303.87337827682495 and batch: 50, loss is 4.5035568046569825 and perplexity is 90.33787472505283
At time: 304.3172173500061 and batch: 100, loss is 4.428437948226929 and perplexity is 83.80041403494191
At time: 304.7680387496948 and batch: 150, loss is 4.466479225158691 and perplexity is 87.04970045663352
At time: 305.21829557418823 and batch: 200, loss is 4.488475828170777 and perplexity is 88.98571294790219
At time: 305.6659092903137 and batch: 250, loss is 4.457824869155884 and perplexity is 86.29959189400402
At time: 306.1089129447937 and batch: 300, loss is 4.365388402938843 and perplexity is 78.67995353302072
At time: 306.54771518707275 and batch: 350, loss is 4.445054397583008 and perplexity is 85.20451263080915
At time: 306.9956796169281 and batch: 400, loss is 4.358357944488525 and perplexity is 78.12873731063596
At time: 307.451762676239 and batch: 450, loss is 4.408082551956177 and perplexity is 82.11186722196568
At time: 307.89654517173767 and batch: 500, loss is 4.357518758773804 and perplexity is 78.06320029308618
At time: 308.3430483341217 and batch: 550, loss is 4.402707033157348 and perplexity is 81.67165757375851
At time: 308.7819550037384 and batch: 600, loss is 4.4251636791229245 and perplexity is 83.52647764383109
At time: 309.2363772392273 and batch: 650, loss is 4.399586176872253 and perplexity is 81.41716938499111
At time: 309.6946144104004 and batch: 700, loss is 4.37878867149353 and perplexity is 79.74138186951876
At time: 310.1444752216339 and batch: 750, loss is 4.324888772964478 and perplexity is 75.55710839915379
At time: 310.58123564720154 and batch: 800, loss is 4.354889831542969 and perplexity is 77.85824734134772
At time: 311.02620124816895 and batch: 850, loss is 4.330502300262451 and perplexity is 75.98244298644966
At time: 311.46733593940735 and batch: 900, loss is 4.4495704555511475 and perplexity is 85.59017132204795
At time: 311.90642046928406 and batch: 950, loss is 4.379065465927124 and perplexity is 79.7634568951285
At time: 312.34998846054077 and batch: 1000, loss is 4.3517805862426755 and perplexity is 77.6165429056242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883908155487805 and perplexity of 132.14610354036142
Finished 32 epochs...
Completing Train Step...
At time: 313.66937160491943 and batch: 50, loss is 4.5026937675476075 and perplexity is 90.25994342042412
At time: 314.12859559059143 and batch: 100, loss is 4.427459192276001 and perplexity is 83.71843400677972
At time: 314.5822298526764 and batch: 150, loss is 4.465585489273071 and perplexity is 86.9719357712242
At time: 315.05532145500183 and batch: 200, loss is 4.487475328445434 and perplexity is 88.89672728902494
At time: 315.49690341949463 and batch: 250, loss is 4.457027063369751 and perplexity is 86.23076903753713
At time: 315.93313360214233 and batch: 300, loss is 4.364423475265503 and perplexity is 78.60406968561965
At time: 316.37024664878845 and batch: 350, loss is 4.444095306396484 and perplexity is 85.1228329091046
At time: 316.8144941329956 and batch: 400, loss is 4.357534046173096 and perplexity is 78.064393685521
At time: 317.266051530838 and batch: 450, loss is 4.407280769348144 and perplexity is 82.04605774088822
At time: 317.7106771469116 and batch: 500, loss is 4.356735439300537 and perplexity is 78.00207581127498
At time: 318.1569666862488 and batch: 550, loss is 4.402073917388916 and perplexity is 81.61996632451365
At time: 318.5934810638428 and batch: 600, loss is 4.424673843383789 and perplexity is 83.48557340891297
At time: 319.03399538993835 and batch: 650, loss is 4.3988783645629885 and perplexity is 81.35956170043417
At time: 319.49461674690247 and batch: 700, loss is 4.37815505027771 and perplexity is 79.69087204192644
At time: 319.93684005737305 and batch: 750, loss is 4.32425332069397 and perplexity is 75.50911071480239
At time: 320.3730890750885 and batch: 800, loss is 4.3544887351989745 and perplexity is 77.8270249450024
At time: 320.81690406799316 and batch: 850, loss is 4.330227499008179 and perplexity is 75.9615657844863
At time: 321.2563097476959 and batch: 900, loss is 4.449336423873901 and perplexity is 85.57014285443698
At time: 321.7021641731262 and batch: 950, loss is 4.378652906417846 and perplexity is 79.73055650964363
At time: 322.1389672756195 and batch: 1000, loss is 4.351141700744629 and perplexity is 77.56697065913414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883747007788681 and perplexity of 132.124810215559
Finished 33 epochs...
Completing Train Step...
At time: 323.4761300086975 and batch: 50, loss is 4.50181791305542 and perplexity is 90.18092345356268
At time: 323.92396068573 and batch: 100, loss is 4.426421976089477 and perplexity is 83.63164490922661
At time: 324.36517429351807 and batch: 150, loss is 4.464492244720459 and perplexity is 86.87690613099528
At time: 324.8062198162079 and batch: 200, loss is 4.486465969085693 and perplexity is 88.8070438143269
At time: 325.2690660953522 and batch: 250, loss is 4.456068086624145 and perplexity is 86.14811537307976
At time: 325.71627831459045 and batch: 300, loss is 4.363441820144653 and perplexity is 78.52694545898254
At time: 326.17131996154785 and batch: 350, loss is 4.443109674453735 and perplexity is 85.03897445950425
At time: 326.62569189071655 and batch: 400, loss is 4.356604013442993 and perplexity is 77.9918249951972
At time: 327.0664668083191 and batch: 450, loss is 4.406383848190307 and perplexity is 81.97250188761494
At time: 327.5101981163025 and batch: 500, loss is 4.355868711471557 and perplexity is 77.93449853123622
At time: 327.94686460494995 and batch: 550, loss is 4.401432399749756 and perplexity is 81.56762246795765
At time: 328.39225935935974 and batch: 600, loss is 4.424094486236572 and perplexity is 83.43721945372631
At time: 328.8453757762909 and batch: 650, loss is 4.398178381919861 and perplexity is 81.30263134684562
At time: 329.2954022884369 and batch: 700, loss is 4.377626371383667 and perplexity is 79.64875229472177
At time: 329.7406129837036 and batch: 750, loss is 4.323797054290772 and perplexity is 75.47466630295938
At time: 330.1818563938141 and batch: 800, loss is 4.354126796722412 and perplexity is 77.79886144719265
At time: 330.6447739601135 and batch: 850, loss is 4.3299307346344 and perplexity is 75.93902644258725
At time: 331.0858778953552 and batch: 900, loss is 4.448983249664306 and perplexity is 85.53992702290974
At time: 331.5278694629669 and batch: 950, loss is 4.378291835784912 and perplexity is 79.70177334383118
At time: 331.96569871902466 and batch: 1000, loss is 4.350652303695679 and perplexity is 77.52901890009448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883704208746189 and perplexity of 132.11915552120092
Finished 34 epochs...
Completing Train Step...
At time: 333.3308012485504 and batch: 50, loss is 4.500905055999755 and perplexity is 90.09863872411375
At time: 333.76694226264954 and batch: 100, loss is 4.425409660339356 and perplexity is 83.54702611557155
At time: 334.2075369358063 and batch: 150, loss is 4.463518085479737 and perplexity is 86.79231539919357
At time: 334.6537461280823 and batch: 200, loss is 4.485557174682617 and perplexity is 88.72637313203914
At time: 335.1088273525238 and batch: 250, loss is 4.455252695083618 and perplexity is 86.07789955915285
At time: 335.5461173057556 and batch: 300, loss is 4.362567520141601 and perplexity is 78.45831935460119
At time: 336.00446939468384 and batch: 350, loss is 4.442232236862183 and perplexity is 84.96439079260809
At time: 336.451131105423 and batch: 400, loss is 4.355731077194214 and perplexity is 77.92377281098108
At time: 336.9017550945282 and batch: 450, loss is 4.405547332763672 and perplexity is 81.90395929768992
At time: 337.3505337238312 and batch: 500, loss is 4.355004072189331 and perplexity is 77.86714242592906
At time: 337.81877613067627 and batch: 550, loss is 4.400855655670166 and perplexity is 81.52059238807676
At time: 338.2650179862976 and batch: 600, loss is 4.423659982681275 and perplexity is 83.40097356028458
At time: 338.7130422592163 and batch: 650, loss is 4.397621531486511 and perplexity is 81.25737054426594
At time: 339.15384435653687 and batch: 700, loss is 4.377079935073852 and perplexity is 79.60524121353647
At time: 339.6017508506775 and batch: 750, loss is 4.323266677856445 and perplexity is 75.43464693216731
At time: 340.03879284858704 and batch: 800, loss is 4.353693246841431 and perplexity is 77.76513907076607
At time: 340.48181319236755 and batch: 850, loss is 4.3294285297393795 and perplexity is 75.90089906646352
At time: 340.92381978034973 and batch: 900, loss is 4.448637056350708 and perplexity is 85.51031879750926
At time: 341.3696310520172 and batch: 950, loss is 4.3779922580719 and perplexity is 79.67790004498247
At time: 341.8195159435272 and batch: 1000, loss is 4.350126962661744 and perplexity is 77.48830042162581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.88363945193407 and perplexity of 132.11060018288075
Finished 35 epochs...
Completing Train Step...
At time: 343.2162175178528 and batch: 50, loss is 4.500115566253662 and perplexity is 90.0275348442893
At time: 343.67499470710754 and batch: 100, loss is 4.4244367790222165 and perplexity is 83.46578430049114
At time: 344.14207887649536 and batch: 150, loss is 4.462569284439087 and perplexity is 86.71000581390723
At time: 344.5769455432892 and batch: 200, loss is 4.484607362747193 and perplexity is 88.64213977310882
At time: 345.02438735961914 and batch: 250, loss is 4.454497241973877 and perplexity is 86.01289629880003
At time: 345.46229815483093 and batch: 300, loss is 4.361721792221069 and perplexity is 78.39199301429728
At time: 345.9006702899933 and batch: 350, loss is 4.441323471069336 and perplexity is 84.88721313416514
At time: 346.3657021522522 and batch: 400, loss is 4.354899349212647 and perplexity is 77.85898837395408
At time: 346.81174874305725 and batch: 450, loss is 4.404716205596924 and perplexity is 81.83591497271382
At time: 347.25228929519653 and batch: 500, loss is 4.354181938171386 and perplexity is 77.80315150742035
At time: 347.704062461853 and batch: 550, loss is 4.400199737548828 and perplexity is 81.46713908667815
At time: 348.14945435523987 and batch: 600, loss is 4.4231578540802 and perplexity is 83.35910605841724
At time: 348.59220242500305 and batch: 650, loss is 4.397030282020569 and perplexity is 81.20934136733955
At time: 349.0360367298126 and batch: 700, loss is 4.376528358459472 and perplexity is 79.56134493129528
At time: 349.4967517852783 and batch: 750, loss is 4.32282639503479 and perplexity is 75.40144166335658
At time: 349.9394519329071 and batch: 800, loss is 4.353337678909302 and perplexity is 77.73749319635944
At time: 350.3916451931 and batch: 850, loss is 4.329132223129273 and perplexity is 75.87841245998668
At time: 350.82455015182495 and batch: 900, loss is 4.44829381942749 and perplexity is 85.48097353525867
At time: 351.27585792541504 and batch: 950, loss is 4.377586107254029 and perplexity is 79.64554537159626
At time: 351.729820728302 and batch: 1000, loss is 4.3495941162109375 and perplexity is 77.44702205425958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8836215879859 and perplexity of 132.1082401870458
Finished 36 epochs...
Completing Train Step...
At time: 353.1126706600189 and batch: 50, loss is 4.49934663772583 and perplexity is 89.95833671207562
At time: 353.55208253860474 and batch: 100, loss is 4.423567514419556 and perplexity is 83.3932619737778
At time: 354.00136280059814 and batch: 150, loss is 4.461656322479248 and perplexity is 86.6308790024422
At time: 354.44682812690735 and batch: 200, loss is 4.483741207122803 and perplexity is 88.56539512638805
At time: 354.88965582847595 and batch: 250, loss is 4.453757486343384 and perplexity is 85.94929130344525
At time: 355.33335757255554 and batch: 300, loss is 4.36084475517273 and perplexity is 78.3232704726432
At time: 355.78629994392395 and batch: 350, loss is 4.440490837097168 and perplexity is 84.81656257381702
At time: 356.2266004085541 and batch: 400, loss is 4.354153728485107 and perplexity is 77.80095673588184
At time: 356.67718029022217 and batch: 450, loss is 4.403879690170288 and perplexity is 81.76748659207456
At time: 357.13881945610046 and batch: 500, loss is 4.35337236404419 and perplexity is 77.74018957855871
At time: 357.59563755989075 and batch: 550, loss is 4.399643516540527 and perplexity is 81.42183795232123
At time: 358.04825258255005 and batch: 600, loss is 4.4225828742980955 and perplexity is 83.31119003447193
At time: 358.4916398525238 and batch: 650, loss is 4.3963882970809935 and perplexity is 81.15722292464656
At time: 358.9317409992218 and batch: 700, loss is 4.375972013473511 and perplexity is 79.51709368658688
At time: 359.3714873790741 and batch: 750, loss is 4.322225732803345 and perplexity is 75.35616446466581
At time: 359.81637835502625 and batch: 800, loss is 4.352788829803467 and perplexity is 77.69483874922712
At time: 360.2526741027832 and batch: 850, loss is 4.328821983337402 and perplexity is 75.85487560832038
At time: 360.7076344490051 and batch: 900, loss is 4.447871208190918 and perplexity is 85.44485594771628
At time: 361.1543221473694 and batch: 950, loss is 4.3772329998016355 and perplexity is 79.61742690068859
At time: 361.5968062877655 and batch: 1000, loss is 4.349007015228271 and perplexity is 77.40156617640791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.883640940596417 and perplexity of 132.1107968511033
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 362.9653470516205 and batch: 50, loss is 4.499090986251831 and perplexity is 89.93534167018008
At time: 363.3994324207306 and batch: 100, loss is 4.422301168441773 and perplexity is 83.2877240897442
At time: 363.8348870277405 and batch: 150, loss is 4.459861469268799 and perplexity is 86.47552874829687
At time: 364.28180623054504 and batch: 200, loss is 4.481987380981446 and perplexity is 88.41020295102254
At time: 364.73347067832947 and batch: 250, loss is 4.451738595962524 and perplexity is 85.77594414932112
At time: 365.17290019989014 and batch: 300, loss is 4.358694400787353 and perplexity is 78.1550286391171
At time: 365.6255600452423 and batch: 350, loss is 4.437290935516358 and perplexity is 84.54559169244821
At time: 366.0881817340851 and batch: 400, loss is 4.350393285751343 and perplexity is 77.5089400934905
At time: 366.5260407924652 and batch: 450, loss is 4.399524555206299 and perplexity is 81.41215247795303
At time: 366.9849383831024 and batch: 500, loss is 4.3487769222259525 and perplexity is 77.38375866643247
At time: 367.43366861343384 and batch: 550, loss is 4.3952086067199705 and perplexity is 81.06153898084476
At time: 367.88719153404236 and batch: 600, loss is 4.417283401489258 and perplexity is 82.87085245727225
At time: 368.3332347869873 and batch: 650, loss is 4.390062646865845 and perplexity is 80.64547100823175
At time: 368.7779800891876 and batch: 700, loss is 4.370124082565308 and perplexity is 79.05344024467914
At time: 369.2379252910614 and batch: 750, loss is 4.315970096588135 and perplexity is 74.88623509925698
At time: 369.69145035743713 and batch: 800, loss is 4.345924377441406 and perplexity is 77.16333256636848
At time: 370.15022110939026 and batch: 850, loss is 4.320920267105103 and perplexity is 75.25785376139363
At time: 370.59405970573425 and batch: 900, loss is 4.439987907409668 and perplexity is 84.77391653138622
At time: 371.0411915779114 and batch: 950, loss is 4.368585042953491 and perplexity is 78.93186744536597
At time: 371.4768371582031 and batch: 1000, loss is 4.340698652267456 and perplexity is 76.76114995999578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882252762957317 and perplexity of 131.92753082935758
Finished 38 epochs...
Completing Train Step...
At time: 372.8415675163269 and batch: 50, loss is 4.497328996658325 and perplexity is 89.77701605906996
At time: 373.2995071411133 and batch: 100, loss is 4.420971002578735 and perplexity is 83.17701125189276
At time: 373.74676871299744 and batch: 150, loss is 4.458748273849487 and perplexity is 86.37931814634776
At time: 374.1851794719696 and batch: 200, loss is 4.480788478851318 and perplexity is 88.30427128391878
At time: 374.6160478591919 and batch: 250, loss is 4.450495300292968 and perplexity is 85.66936555748798
At time: 375.0656843185425 and batch: 300, loss is 4.357744016647339 and perplexity is 78.08078662424097
At time: 375.5109369754791 and batch: 350, loss is 4.43660722732544 and perplexity is 84.48780693510903
At time: 375.9475028514862 and batch: 400, loss is 4.349734573364258 and perplexity is 77.45790080649188
At time: 376.3895604610443 and batch: 450, loss is 4.398826637268066 and perplexity is 81.35535329923691
At time: 376.835058927536 and batch: 500, loss is 4.348373174667358 and perplexity is 77.35252146919147
At time: 377.2750005722046 and batch: 550, loss is 4.394857320785523 and perplexity is 81.03306820336071
At time: 377.72485518455505 and batch: 600, loss is 4.416851701736451 and perplexity is 82.83508485174022
At time: 378.1752440929413 and batch: 650, loss is 4.389765410423279 and perplexity is 80.62150379746092
At time: 378.61826944351196 and batch: 700, loss is 4.3696972846984865 and perplexity is 79.01970760403938
At time: 379.0586950778961 and batch: 750, loss is 4.315786657333374 and perplexity is 74.87249928398086
At time: 379.50905442237854 and batch: 800, loss is 4.345733041763306 and perplexity is 77.14856988016673
At time: 379.9463324546814 and batch: 850, loss is 4.320959243774414 and perplexity is 75.26078711903868
At time: 380.38365411758423 and batch: 900, loss is 4.440285110473633 and perplexity is 84.7991153435203
At time: 380.83569145202637 and batch: 950, loss is 4.369016284942627 and perplexity is 78.96591352141049
At time: 381.2951292991638 and batch: 1000, loss is 4.3411416244506835 and perplexity is 76.79516054649636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881965451124238 and perplexity of 131.88963193330272
Finished 39 epochs...
Completing Train Step...
At time: 382.6899678707123 and batch: 50, loss is 4.4966110992431645 and perplexity is 89.71258850025633
At time: 383.14431834220886 and batch: 100, loss is 4.420321989059448 and perplexity is 83.12304576115719
At time: 383.58864164352417 and batch: 150, loss is 4.458036365509034 and perplexity is 86.31784587322491
At time: 384.05021119117737 and batch: 200, loss is 4.480190334320068 and perplexity is 88.25146836042853
At time: 384.4957437515259 and batch: 250, loss is 4.449807844161987 and perplexity is 85.61049186575701
At time: 384.9390184879303 and batch: 300, loss is 4.357125110626221 and perplexity is 78.0324769063953
At time: 385.37931180000305 and batch: 350, loss is 4.436179838180542 and perplexity is 84.45170547878354
At time: 385.82349920272827 and batch: 400, loss is 4.349408721923828 and perplexity is 77.43266514970223
At time: 386.2658636569977 and batch: 450, loss is 4.398438653945923 and perplexity is 81.32379490145158
At time: 386.7106864452362 and batch: 500, loss is 4.348089599609375 and perplexity is 77.33058933328104
At time: 387.1633667945862 and batch: 550, loss is 4.3946169662475585 and perplexity is 81.01359387815785
At time: 387.60426568984985 and batch: 600, loss is 4.416556024551392 and perplexity is 82.81059602759886
At time: 388.04346227645874 and batch: 650, loss is 4.389556927680969 and perplexity is 80.60469735724732
At time: 388.4921226501465 and batch: 700, loss is 4.369496440887451 and perplexity is 79.00383857846839
At time: 388.9529592990875 and batch: 750, loss is 4.315729675292968 and perplexity is 74.86823301775286
At time: 389.38902163505554 and batch: 800, loss is 4.345623426437378 and perplexity is 77.14011367800757
At time: 389.8314142227173 and batch: 850, loss is 4.3209916114807125 and perplexity is 75.26322317751652
At time: 390.2671480178833 and batch: 900, loss is 4.440464448928833 and perplexity is 84.81432444961638
At time: 390.70307779312134 and batch: 950, loss is 4.369268980026245 and perplexity is 78.98587034091963
At time: 391.1578016281128 and batch: 1000, loss is 4.341395435333252 and perplexity is 76.81465446775165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881803931259528 and perplexity of 131.8683308581159
Finished 40 epochs...
Completing Train Step...
At time: 392.54725217819214 and batch: 50, loss is 4.496099758148193 and perplexity is 89.66672649358354
At time: 392.9851288795471 and batch: 100, loss is 4.419824533462524 and perplexity is 83.08170602000524
At time: 393.43502497673035 and batch: 150, loss is 4.457494201660157 and perplexity is 86.2710601415917
At time: 393.89746475219727 and batch: 200, loss is 4.479757070541382 and perplexity is 88.21324047774856
At time: 394.3592257499695 and batch: 250, loss is 4.44930401802063 and perplexity is 85.56736992587307
At time: 394.82429480552673 and batch: 300, loss is 4.35665343284607 and perplexity is 77.99567939987371
At time: 395.29161977767944 and batch: 350, loss is 4.4358506011962895 and perplexity is 84.42390543060986
At time: 395.7347195148468 and batch: 400, loss is 4.3491678810119625 and perplexity is 77.41401844155465
At time: 396.1885759830475 and batch: 450, loss is 4.3981789875030515 and perplexity is 81.30268058236743
At time: 396.6275472640991 and batch: 500, loss is 4.347856283187866 and perplexity is 77.31254894154627
At time: 397.07524013519287 and batch: 550, loss is 4.394410572052002 and perplexity is 80.99687486803289
At time: 397.53028416633606 and batch: 600, loss is 4.416290521621704 and perplexity is 82.78861249022026
At time: 397.9784128665924 and batch: 650, loss is 4.389372358322143 and perplexity is 80.58982157278697
At time: 398.4230899810791 and batch: 700, loss is 4.369375524520874 and perplexity is 78.99428629888695
At time: 398.8680100440979 and batch: 750, loss is 4.315684356689453 and perplexity is 74.86484017086495
At time: 399.33595418930054 and batch: 800, loss is 4.345502700805664 and perplexity is 77.13080145117686
At time: 399.7813913822174 and batch: 850, loss is 4.321019802093506 and perplexity is 75.26534492380523
At time: 400.2376983165741 and batch: 900, loss is 4.440573749542236 and perplexity is 84.82359521394494
At time: 400.69478940963745 and batch: 950, loss is 4.36943039894104 and perplexity is 78.99862118348008
At time: 401.140926361084 and batch: 1000, loss is 4.341541757583618 and perplexity is 76.82589498320323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881704935213414 and perplexity of 131.8552770609012
Finished 41 epochs...
Completing Train Step...
At time: 402.47752356529236 and batch: 50, loss is 4.4956865406036375 and perplexity is 89.62968228321944
At time: 402.9448413848877 and batch: 100, loss is 4.419403848648071 and perplexity is 83.04676215861504
At time: 403.40852522850037 and batch: 150, loss is 4.457036628723144 and perplexity is 86.2315938692612
At time: 403.84793519973755 and batch: 200, loss is 4.479392471313477 and perplexity is 88.18108386087444
At time: 404.28746795654297 and batch: 250, loss is 4.448893346786499 and perplexity is 85.53223708298201
At time: 404.7523458003998 and batch: 300, loss is 4.356272277832031 and perplexity is 77.96595662045013
At time: 405.1934542655945 and batch: 350, loss is 4.435557060241699 and perplexity is 84.39912719371108
At time: 405.6400854587555 and batch: 400, loss is 4.348950939178467 and perplexity is 77.3972259240213
At time: 406.0793113708496 and batch: 450, loss is 4.397915410995483 and perplexity is 81.28125392966878
At time: 406.53574323654175 and batch: 500, loss is 4.347641639709472 and perplexity is 77.29595608795557
At time: 406.99208211898804 and batch: 550, loss is 4.394248151779175 and perplexity is 80.98372040182363
At time: 407.4288601875305 and batch: 600, loss is 4.416075248718261 and perplexity is 82.7707922634123
At time: 407.87243819236755 and batch: 650, loss is 4.389183092117309 and perplexity is 80.57457008645063
At time: 408.3094091415405 and batch: 700, loss is 4.369207983016968 and perplexity is 78.98105258598949
At time: 408.7558147907257 and batch: 750, loss is 4.315611028671265 and perplexity is 74.8593506817724
At time: 409.19830775260925 and batch: 800, loss is 4.345379886627197 and perplexity is 77.1213292768327
At time: 409.65556025505066 and batch: 850, loss is 4.321001253128052 and perplexity is 75.26394884247031
At time: 410.11247754096985 and batch: 900, loss is 4.440630025863648 and perplexity is 84.82836890817423
At time: 410.57341718673706 and batch: 950, loss is 4.369528188705444 and perplexity is 79.0063468177717
At time: 411.01609659194946 and batch: 1000, loss is 4.341632947921753 and perplexity is 76.83290108198366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8816375732421875 and perplexity of 131.84639532867058
Finished 42 epochs...
Completing Train Step...
At time: 412.4026608467102 and batch: 50, loss is 4.495327768325805 and perplexity is 89.59753140571196
At time: 412.8504512310028 and batch: 100, loss is 4.419028606414795 and perplexity is 83.0156053521567
At time: 413.29688358306885 and batch: 150, loss is 4.456637125015259 and perplexity is 86.19715090827702
At time: 413.7396774291992 and batch: 200, loss is 4.479066486358643 and perplexity is 88.15234283905968
At time: 414.1852912902832 and batch: 250, loss is 4.448545093536377 and perplexity is 85.50245538951441
At time: 414.6507785320282 and batch: 300, loss is 4.355940456390381 and perplexity is 77.94009013608913
At time: 415.08703875541687 and batch: 350, loss is 4.43528377532959 and perplexity is 84.37606533702613
At time: 415.53206062316895 and batch: 400, loss is 4.348750905990601 and perplexity is 77.38174545854277
At time: 415.9757626056671 and batch: 450, loss is 4.397671251296997 and perplexity is 81.26141074576823
At time: 416.4123480319977 and batch: 500, loss is 4.34743088722229 and perplexity is 77.27966748945254
At time: 416.8651509284973 and batch: 550, loss is 4.394076642990112 and perplexity is 80.96983217301437
At time: 417.30985713005066 and batch: 600, loss is 4.415864191055298 and perplexity is 82.75332469683228
At time: 417.75058674812317 and batch: 650, loss is 4.3889886665344235 and perplexity is 80.55890585150915
At time: 418.20339488983154 and batch: 700, loss is 4.36904128074646 and perplexity is 78.96788736256312
At time: 418.6544282436371 and batch: 750, loss is 4.315500793457031 and perplexity is 74.85109900003387
At time: 419.09525847435 and batch: 800, loss is 4.345241069793701 and perplexity is 77.1106242811415
At time: 419.5368492603302 and batch: 850, loss is 4.320918893814087 and perplexity is 75.25775041053014
At time: 420.014422416687 and batch: 900, loss is 4.44066330909729 and perplexity is 84.83119231758191
At time: 420.45685362815857 and batch: 950, loss is 4.369576816558838 and perplexity is 79.01018882023534
At time: 420.9065306186676 and batch: 1000, loss is 4.341665554046631 and perplexity is 76.83540634599429
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881572072098895 and perplexity of 131.83775952186826
Finished 43 epochs...
Completing Train Step...
At time: 422.2736737728119 and batch: 50, loss is 4.494982643127441 and perplexity is 89.56661437534265
At time: 422.7238166332245 and batch: 100, loss is 4.418689746856689 and perplexity is 82.98747948643953
At time: 423.1731803417206 and batch: 150, loss is 4.4562599468231205 and perplexity is 86.16464535330852
At time: 423.62525844573975 and batch: 200, loss is 4.478715782165527 and perplexity is 88.12143286323895
At time: 424.0821704864502 and batch: 250, loss is 4.448236742019653 and perplexity is 85.47609464210855
At time: 424.5352659225464 and batch: 300, loss is 4.355634574890137 and perplexity is 77.91625335019089
At time: 424.98438239097595 and batch: 350, loss is 4.435016469955444 and perplexity is 84.35351417546991
At time: 425.4306390285492 and batch: 400, loss is 4.348556737899781 and perplexity is 77.36672185136253
At time: 425.8606598377228 and batch: 450, loss is 4.397436780929565 and perplexity is 81.24235958648607
At time: 426.31254863739014 and batch: 500, loss is 4.347205142974854 and perplexity is 77.26222401802873
At time: 426.7496314048767 and batch: 550, loss is 4.39387957572937 and perplexity is 80.9538772421342
At time: 427.19242238998413 and batch: 600, loss is 4.415669927597046 and perplexity is 82.73725031117823
At time: 427.62523674964905 and batch: 650, loss is 4.388788661956787 and perplexity is 80.54279531271392
At time: 428.0613691806793 and batch: 700, loss is 4.368868598937988 and perplexity is 78.95425222226638
At time: 428.49773359298706 and batch: 750, loss is 4.3154030036926265 and perplexity is 74.84377968658005
At time: 428.94563937187195 and batch: 800, loss is 4.345108995437622 and perplexity is 77.1004406176079
At time: 429.397034406662 and batch: 850, loss is 4.3208308601379395 and perplexity is 75.25112548571546
At time: 429.8746192455292 and batch: 900, loss is 4.440695781707763 and perplexity is 84.83394705257251
At time: 430.3120105266571 and batch: 950, loss is 4.3696129035949705 and perplexity is 79.01304011522123
At time: 430.7476592063904 and batch: 1000, loss is 4.341671838760376 and perplexity is 76.83588923604607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881544159679878 and perplexity of 131.83407966243934
Finished 44 epochs...
Completing Train Step...
At time: 432.1305491924286 and batch: 50, loss is 4.494690065383911 and perplexity is 89.54041301056806
At time: 432.6008059978485 and batch: 100, loss is 4.418369455337524 and perplexity is 82.9609035568128
At time: 433.0446083545685 and batch: 150, loss is 4.455916957855225 and perplexity is 86.13509689821642
At time: 433.48843932151794 and batch: 200, loss is 4.478426742553711 and perplexity is 88.09596595914078
At time: 433.9299736022949 and batch: 250, loss is 4.44794189453125 and perplexity is 85.45089594535358
At time: 434.3654098510742 and batch: 300, loss is 4.355349941253662 and perplexity is 77.89407891960346
At time: 434.8131604194641 and batch: 350, loss is 4.4347647857666015 and perplexity is 84.33228640113825
At time: 435.25921607017517 and batch: 400, loss is 4.3483764266967775 and perplexity is 77.35277302227595
At time: 435.6995711326599 and batch: 450, loss is 4.397230348587036 and perplexity is 81.22559026680855
At time: 436.1397616863251 and batch: 500, loss is 4.347014493942261 and perplexity is 77.24749545380159
At time: 436.5710473060608 and batch: 550, loss is 4.393709630966186 and perplexity is 80.94012072359473
At time: 437.0105516910553 and batch: 600, loss is 4.415473613739014 and perplexity is 82.72100943657323
At time: 437.4494822025299 and batch: 650, loss is 4.38861487865448 and perplexity is 80.52879953591884
At time: 437.8892207145691 and batch: 700, loss is 4.368705425262451 and perplexity is 78.94137001777892
At time: 438.3311278820038 and batch: 750, loss is 4.315311260223389 and perplexity is 74.83691357354608
At time: 438.7845814228058 and batch: 800, loss is 4.344996299743652 and perplexity is 77.09175221952886
At time: 439.23120641708374 and batch: 850, loss is 4.320746955871582 and perplexity is 75.24481186011275
At time: 439.67399191856384 and batch: 900, loss is 4.440699052810669 and perplexity is 84.8342245535971
At time: 440.13231682777405 and batch: 950, loss is 4.369622764587402 and perplexity is 79.01381926605339
At time: 440.58018469810486 and batch: 1000, loss is 4.341665525436401 and perplexity is 76.83540414771569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881528900890816 and perplexity of 131.83206804937396
Finished 45 epochs...
Completing Train Step...
At time: 441.9245140552521 and batch: 50, loss is 4.494414129257202 and perplexity is 89.51570898434142
At time: 442.3770263195038 and batch: 100, loss is 4.418065280914306 and perplexity is 82.93567280929234
At time: 442.8180685043335 and batch: 150, loss is 4.4555940914154055 and perplexity is 86.10729125513345
At time: 443.25839710235596 and batch: 200, loss is 4.478157300949096 and perplexity is 88.07223243825524
At time: 443.69852662086487 and batch: 250, loss is 4.447650299072266 and perplexity is 85.42598248463233
At time: 444.13785457611084 and batch: 300, loss is 4.355091667175293 and perplexity is 77.87396349591813
At time: 444.58238697052 and batch: 350, loss is 4.434525098800659 and perplexity is 84.31207547352449
At time: 445.0366635322571 and batch: 400, loss is 4.348193416595459 and perplexity is 77.33861797874535
At time: 445.47802567481995 and batch: 450, loss is 4.397015256881714 and perplexity is 81.20812119487584
At time: 445.9263734817505 and batch: 500, loss is 4.346820068359375 and perplexity is 77.23247802440505
At time: 446.37264752388 and batch: 550, loss is 4.393518266677856 and perplexity is 80.92463315692595
At time: 446.80920219421387 and batch: 600, loss is 4.415263748168945 and perplexity is 82.70365096630749
At time: 447.25404024124146 and batch: 650, loss is 4.388385243415833 and perplexity is 80.51030940889304
At time: 447.69991850852966 and batch: 700, loss is 4.368573579788208 and perplexity is 78.93096264150932
At time: 448.15454506874084 and batch: 750, loss is 4.31522126197815 and perplexity is 74.83017868571392
At time: 448.5944106578827 and batch: 800, loss is 4.344859085083008 and perplexity is 77.08117482661285
At time: 449.03542256355286 and batch: 850, loss is 4.320613069534302 and perplexity is 75.23473828222575
At time: 449.47879791259766 and batch: 900, loss is 4.440710477828979 and perplexity is 84.83519379170271
At time: 449.93666768074036 and batch: 950, loss is 4.369619951248169 and perplexity is 79.01359697368841
At time: 450.3742325305939 and batch: 1000, loss is 4.341648416519165 and perplexity is 76.83408958839073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881509920445884 and perplexity of 131.82956584181275
Finished 46 epochs...
Completing Train Step...
At time: 451.7619013786316 and batch: 50, loss is 4.494144811630249 and perplexity is 89.49160407210728
At time: 452.19150829315186 and batch: 100, loss is 4.417778635025025 and perplexity is 82.9119030465219
At time: 452.65591526031494 and batch: 150, loss is 4.455270261764526 and perplexity is 86.07941167542904
At time: 453.0980432033539 and batch: 200, loss is 4.477884454727173 and perplexity is 88.04820554035132
At time: 453.56027150154114 and batch: 250, loss is 4.4473686790466305 and perplexity is 85.40192820449754
At time: 454.0152168273926 and batch: 300, loss is 4.3548583984375 and perplexity is 77.85580005331056
At time: 454.45159006118774 and batch: 350, loss is 4.4342857837677006 and perplexity is 84.29190074055848
At time: 454.89772868156433 and batch: 400, loss is 4.348011665344238 and perplexity is 77.32456286546594
At time: 455.35666823387146 and batch: 450, loss is 4.396775016784668 and perplexity is 81.18861409124712
At time: 455.80708599090576 and batch: 500, loss is 4.346623516082763 and perplexity is 77.21729929677657
At time: 456.2510116100311 and batch: 550, loss is 4.393325366973877 and perplexity is 80.90902432466332
At time: 456.7057218551636 and batch: 600, loss is 4.415083475112915 and perplexity is 82.68874307018936
At time: 457.14106130599976 and batch: 650, loss is 4.388195080757141 and perplexity is 80.4950008100118
At time: 457.59276962280273 and batch: 700, loss is 4.368427658081055 and perplexity is 78.91944574099676
At time: 458.0298743247986 and batch: 750, loss is 4.3151148986816406 and perplexity is 74.82221992449809
At time: 458.48054909706116 and batch: 800, loss is 4.34473879814148 and perplexity is 77.07190352546296
At time: 458.9325678348541 and batch: 850, loss is 4.320533609390258 and perplexity is 75.22876035659135
At time: 459.39232063293457 and batch: 900, loss is 4.440678081512451 and perplexity is 84.83244548842961
At time: 459.835750579834 and batch: 950, loss is 4.369602880477905 and perplexity is 79.0122481622394
At time: 460.28830075263977 and batch: 1000, loss is 4.341619329452515 and perplexity is 76.83185474260856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881500988471799 and perplexity of 131.8283883488056
Finished 47 epochs...
Completing Train Step...
At time: 461.6968038082123 and batch: 50, loss is 4.493895816802978 and perplexity is 89.46932389954816
At time: 462.15661334991455 and batch: 100, loss is 4.417501802444458 and perplexity is 82.8889535071785
At time: 462.598867893219 and batch: 150, loss is 4.45496618270874 and perplexity is 86.05324070842731
At time: 463.05030941963196 and batch: 200, loss is 4.477640886306762 and perplexity is 88.02676238955107
At time: 463.4874258041382 and batch: 250, loss is 4.4471300506591795 and perplexity is 85.38155131143417
At time: 463.95943689346313 and batch: 300, loss is 4.354604730606079 and perplexity is 77.83605304604363
At time: 464.40502405166626 and batch: 350, loss is 4.4340602397918705 and perplexity is 84.27289135394207
At time: 464.86028695106506 and batch: 400, loss is 4.347844858169555 and perplexity is 77.31166564930476
At time: 465.3148272037506 and batch: 450, loss is 4.3965478515625 and perplexity is 81.1701729563611
At time: 465.76424527168274 and batch: 500, loss is 4.346434621810913 and perplexity is 77.20271476876196
At time: 466.2152829170227 and batch: 550, loss is 4.393162260055542 and perplexity is 80.89582857922817
At time: 466.6605668067932 and batch: 600, loss is 4.414901790618896 and perplexity is 82.67372117240784
At time: 467.1135139465332 and batch: 650, loss is 4.3880239105224605 and perplexity is 80.48122364098681
At time: 467.5523624420166 and batch: 700, loss is 4.368313436508179 and perplexity is 78.91043195256805
At time: 468.023521900177 and batch: 750, loss is 4.315024662017822 and perplexity is 74.81546852160923
At time: 468.48231744766235 and batch: 800, loss is 4.344623661041259 and perplexity is 77.06303020081683
At time: 468.91006422042847 and batch: 850, loss is 4.320457038879394 and perplexity is 75.22300027250826
At time: 469.3458821773529 and batch: 900, loss is 4.440644111633301 and perplexity is 84.82956378945408
At time: 469.79238843917847 and batch: 950, loss is 4.369585170745849 and perplexity is 79.01084888888572
At time: 470.24078011512756 and batch: 1000, loss is 4.341587114334106 and perplexity is 76.82937963517864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881492056497714 and perplexity of 131.82721086631585
Finished 48 epochs...
Completing Train Step...
At time: 471.5632746219635 and batch: 50, loss is 4.493656339645386 and perplexity is 89.44790060546617
At time: 472.0302104949951 and batch: 100, loss is 4.417243757247925 and perplexity is 82.86756717031972
At time: 472.4798674583435 and batch: 150, loss is 4.454687032699585 and perplexity is 86.02922229802142
At time: 472.9307506084442 and batch: 200, loss is 4.477415723800659 and perplexity is 88.00694429435701
At time: 473.39934253692627 and batch: 250, loss is 4.4469254779815675 and perplexity is 85.36408636535
At time: 473.85106205940247 and batch: 300, loss is 4.354370975494385 and perplexity is 77.81786059714184
At time: 474.2894854545593 and batch: 350, loss is 4.433842754364013 and perplexity is 84.25456522101487
At time: 474.73227310180664 and batch: 400, loss is 4.347680711746216 and perplexity is 77.29897625739369
At time: 475.1768386363983 and batch: 450, loss is 4.396324396133423 and perplexity is 81.15203706689185
At time: 475.64508271217346 and batch: 500, loss is 4.3462443447113035 and perplexity is 77.18802625760172
At time: 476.10928416252136 and batch: 550, loss is 4.392996711730957 and perplexity is 80.88243751880536
At time: 476.54501581192017 and batch: 600, loss is 4.414699811935424 and perplexity is 82.65702452928748
At time: 477.001816034317 and batch: 650, loss is 4.387840490341187 and perplexity is 80.4664631140881
At time: 477.45154190063477 and batch: 700, loss is 4.368144245147705 and perplexity is 78.89708211860054
At time: 477.8868854045868 and batch: 750, loss is 4.314908428192139 and perplexity is 74.80677293885287
At time: 478.33931827545166 and batch: 800, loss is 4.344496059417724 and perplexity is 77.05319746039885
At time: 478.77711486816406 and batch: 850, loss is 4.320365781784058 and perplexity is 75.21613595321459
At time: 479.2480170726776 and batch: 900, loss is 4.440583019256592 and perplexity is 84.82438150808758
At time: 479.70270109176636 and batch: 950, loss is 4.36956226348877 and perplexity is 79.00903898778816
At time: 480.15175008773804 and batch: 1000, loss is 4.341536312103272 and perplexity is 76.82547663044105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881481263695694 and perplexity of 131.825788089006
Finished 49 epochs...
Completing Train Step...
At time: 481.48872470855713 and batch: 50, loss is 4.493413095474243 and perplexity is 89.42614557102283
At time: 481.9335091114044 and batch: 100, loss is 4.41698884010315 and perplexity is 82.84644549895465
At time: 482.3983814716339 and batch: 150, loss is 4.454417724609375 and perplexity is 86.00605705189615
At time: 482.8380968570709 and batch: 200, loss is 4.47717360496521 and perplexity is 87.98563873483573
At time: 483.289146900177 and batch: 250, loss is 4.446709146499634 and perplexity is 85.34562142338883
At time: 483.7317717075348 and batch: 300, loss is 4.354114456176758 and perplexity is 77.79790137271502
At time: 484.1738700866699 and batch: 350, loss is 4.433629941940308 and perplexity is 84.23663671055448
At time: 484.6133234500885 and batch: 400, loss is 4.347521467208862 and perplexity is 77.28666779773513
At time: 485.05668091773987 and batch: 450, loss is 4.396110219955444 and perplexity is 81.13465809490502
At time: 485.49239563941956 and batch: 500, loss is 4.346043491363526 and perplexity is 77.17252434097756
At time: 485.95181250572205 and batch: 550, loss is 4.392830514907837 and perplexity is 80.86899623162411
At time: 486.3933961391449 and batch: 600, loss is 4.414523229598999 and perplexity is 82.64243004737594
At time: 486.8625910282135 and batch: 650, loss is 4.387677202224731 and perplexity is 80.45332496956898
At time: 487.3173499107361 and batch: 700, loss is 4.368036499023438 and perplexity is 78.88858172173691
At time: 487.75626015663147 and batch: 750, loss is 4.314809942245484 and perplexity is 74.79940588578535
At time: 488.20528650283813 and batch: 800, loss is 4.344380922317505 and perplexity is 77.04432628939088
At time: 488.64544224739075 and batch: 850, loss is 4.3202943515777585 and perplexity is 75.21076344098853
At time: 489.1011085510254 and batch: 900, loss is 4.440545024871827 and perplexity is 84.8211587191234
At time: 489.5371024608612 and batch: 950, loss is 4.369534549713134 and perplexity is 79.00684937934979
At time: 489.9783582687378 and batch: 1000, loss is 4.3414990425109865 and perplexity is 76.82261342960541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.881465632741044 and perplexity of 131.82372754219492
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
SETTINGS FOR THIS RUN
{'anneal': 2.0, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 1.0, 'batch_size': 50, 'num_layers': 1, 'lr': 3.4334501782790112, 'tune_wordvecs': True, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.6786484718322754 and batch: 50, loss is 7.595937175750732 and perplexity is 1990.0940457062566
At time: 1.1404423713684082 and batch: 100, loss is 6.630442705154419 and perplexity is 757.8175861427404
At time: 1.5813603401184082 and batch: 150, loss is 6.470799837112427 and perplexity is 646.0002153311028
At time: 2.0168583393096924 and batch: 200, loss is 6.432899770736694 and perplexity is 621.9749202200909
At time: 2.4578185081481934 and batch: 250, loss is 6.435041131973267 and perplexity is 623.3082202333742
At time: 2.896547794342041 and batch: 300, loss is 6.3566335010528565 and perplexity is 576.3029636938995
At time: 3.3285107612609863 and batch: 350, loss is 6.408639068603516 and perplexity is 607.0669421602593
At time: 3.7609901428222656 and batch: 400, loss is 6.370448637008667 and perplexity is 584.3199176354962
At time: 4.204691171646118 and batch: 450, loss is 6.354620580673218 and perplexity is 575.1440784765718
At time: 4.652990818023682 and batch: 500, loss is 6.355650873184204 and perplexity is 575.7369504767105
At time: 5.112863302230835 and batch: 550, loss is 6.410805931091309 and perplexity is 608.3837989533466
At time: 5.580693244934082 and batch: 600, loss is 6.428414077758789 and perplexity is 619.1911798580612
At time: 6.027235746383667 and batch: 650, loss is 6.402335376739502 and perplexity is 603.2522152794455
At time: 6.4707190990448 and batch: 700, loss is 6.402590274810791 and perplexity is 603.4060027048477
At time: 6.924072265625 and batch: 750, loss is 6.340391521453857 and perplexity is 567.0182677449727
At time: 7.388173341751099 and batch: 800, loss is 6.410787715911865 and perplexity is 608.372717234206
At time: 7.827571392059326 and batch: 850, loss is 6.38095591545105 and perplexity is 590.491898285529
At time: 8.282503843307495 and batch: 900, loss is 6.422286510467529 and perplexity is 615.4086449411984
At time: 8.7192542552948 and batch: 950, loss is 6.4247619152069095 and perplexity is 616.9339174721089
At time: 9.145870685577393 and batch: 1000, loss is 6.3733811378479 and perplexity is 586.0359511895006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.461892476895961 and perplexity of 640.2716099650885
Finished 1 epochs...
Completing Train Step...
At time: 10.551310539245605 and batch: 50, loss is 5.798937082290649 and perplexity is 329.9486651771792
At time: 11.004549503326416 and batch: 100, loss is 5.462108573913574 and perplexity is 235.59366766037544
At time: 11.454711198806763 and batch: 150, loss is 5.3319266986846925 and perplexity is 206.8361013419887
At time: 11.895961999893188 and batch: 200, loss is 5.231548128128051 and perplexity is 187.0822066721942
At time: 12.341567516326904 and batch: 250, loss is 5.163951759338379 and perplexity is 174.85407332314512
At time: 12.780678033828735 and batch: 300, loss is 4.990183429718018 and perplexity is 146.96337847451963
At time: 13.235676765441895 and batch: 350, loss is 5.0163528347015385 and perplexity is 150.86008754047086
At time: 13.67157793045044 and batch: 400, loss is 4.913572788238525 and perplexity is 136.1248920509305
At time: 14.120290517807007 and batch: 450, loss is 4.9057450771331785 and perplexity is 135.06350525584008
At time: 14.570151567459106 and batch: 500, loss is 4.863791122436523 and perplexity is 129.51427703489748
At time: 15.034225940704346 and batch: 550, loss is 4.890527315139771 and perplexity is 133.0237009716351
At time: 15.481584548950195 and batch: 600, loss is 4.886902933120727 and perplexity is 132.54244491644405
At time: 15.927597522735596 and batch: 650, loss is 4.843817234039307 and perplexity is 126.95303742789665
At time: 16.365471839904785 and batch: 700, loss is 4.801996469497681 and perplexity is 121.75325168580606
At time: 16.80829691886902 and batch: 750, loss is 4.7203077220916745 and perplexity is 112.20277462845218
At time: 17.24893593788147 and batch: 800, loss is 4.744379997253418 and perplexity is 114.93652244794752
At time: 17.695858001708984 and batch: 850, loss is 4.693284778594971 and perplexity is 109.21132629637583
At time: 18.152113676071167 and batch: 900, loss is 4.83791805267334 and perplexity is 126.20632310225832
At time: 18.591115474700928 and batch: 950, loss is 4.735541315078735 and perplexity is 113.92511141059174
At time: 19.04318928718567 and batch: 1000, loss is 4.689222621917724 and perplexity is 108.76859261362273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.800594050709794 and perplexity of 121.5826223130405
Finished 2 epochs...
Completing Train Step...
At time: 20.36417579650879 and batch: 50, loss is 4.721248893737793 and perplexity is 112.30842640897951
At time: 20.8309485912323 and batch: 100, loss is 4.5904113388061525 and perplexity is 98.5349530770142
At time: 21.28111696243286 and batch: 150, loss is 4.610367469787597 and perplexity is 100.52108131073481
At time: 21.719996452331543 and batch: 200, loss is 4.623721446990967 and perplexity is 101.87244046657885
At time: 22.17165780067444 and batch: 250, loss is 4.6011114597320555 and perplexity is 99.59494992412647
At time: 22.606897115707397 and batch: 300, loss is 4.477880010604858 and perplexity is 88.0478142442258
At time: 23.062368631362915 and batch: 350, loss is 4.539730272293091 and perplexity is 93.66553252263792
At time: 23.523484468460083 and batch: 400, loss is 4.452859344482422 and perplexity is 85.87213130250588
At time: 23.96475648880005 and batch: 450, loss is 4.4978969860076905 and perplexity is 89.8280229323184
At time: 24.413583278656006 and batch: 500, loss is 4.447116861343384 and perplexity is 85.38042519461713
At time: 24.85474157333374 and batch: 550, loss is 4.499280233383178 and perplexity is 89.95236328619308
At time: 25.29819130897522 and batch: 600, loss is 4.506654748916626 and perplexity is 90.61817037145762
At time: 25.748016357421875 and batch: 650, loss is 4.480965366363526 and perplexity is 88.31989258834959
At time: 26.19286608695984 and batch: 700, loss is 4.449444341659546 and perplexity is 85.57937789307464
At time: 26.63896131515503 and batch: 750, loss is 4.394142208099365 and perplexity is 80.97514114294668
At time: 27.082584619522095 and batch: 800, loss is 4.413971853256226 and perplexity is 82.59687552653516
At time: 27.527817726135254 and batch: 850, loss is 4.386832075119019 and perplexity is 80.38536040728465
At time: 27.985426664352417 and batch: 900, loss is 4.553669691085815 and perplexity is 94.98031799023278
At time: 28.423059225082397 and batch: 950, loss is 4.461809453964233 and perplexity is 86.64414593335587
At time: 28.87310266494751 and batch: 1000, loss is 4.4092905139923095 and perplexity is 82.21111517209542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.64622944157298 and perplexity of 104.19138429304854
Finished 3 epochs...
Completing Train Step...
At time: 30.20187783241272 and batch: 50, loss is 4.463356142044067 and perplexity is 86.77826109148026
At time: 30.668681144714355 and batch: 100, loss is 4.328795223236084 and perplexity is 75.85284575132336
At time: 31.113304376602173 and batch: 150, loss is 4.366196613311768 and perplexity is 78.74356919156213
At time: 31.570030450820923 and batch: 200, loss is 4.395570907592774 and perplexity is 81.09091296795756
At time: 32.008934020996094 and batch: 250, loss is 4.373967037200928 and perplexity is 79.35782352028725
At time: 32.443193197250366 and batch: 300, loss is 4.266018342971802 and perplexity is 71.23742718182582
At time: 32.89055156707764 and batch: 350, loss is 4.33242374420166 and perplexity is 76.12857934245936
At time: 33.3527295589447 and batch: 400, loss is 4.244548373222351 and perplexity is 69.7242636875838
At time: 33.792240858078 and batch: 450, loss is 4.306523933410644 and perplexity is 74.1821780568588
At time: 34.234286308288574 and batch: 500, loss is 4.245157904624939 and perplexity is 69.76677577073237
At time: 34.67378568649292 and batch: 550, loss is 4.306930351257324 and perplexity is 74.21233314529468
At time: 35.12146592140198 and batch: 600, loss is 4.312712802886963 and perplexity is 74.64270547643686
At time: 35.573872327804565 and batch: 650, loss is 4.291484036445618 and perplexity is 73.07483378732414
At time: 36.00840425491333 and batch: 700, loss is 4.263228402137757 and perplexity is 71.03895596503281
At time: 36.44765305519104 and batch: 750, loss is 4.218488411903381 and perplexity is 67.93072337005407
At time: 36.90299463272095 and batch: 800, loss is 4.230182967185974 and perplexity is 68.72980632273848
At time: 37.339014768600464 and batch: 850, loss is 4.212748336791992 and perplexity is 67.54191288379495
At time: 37.76651668548584 and batch: 900, loss is 4.388772678375244 and perplexity is 80.54150796066564
At time: 38.21464967727661 and batch: 950, loss is 4.300884428024292 and perplexity is 73.76500469549367
At time: 38.65143537521362 and batch: 1000, loss is 4.24423318862915 and perplexity is 69.70229113678384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.581725702053163 and perplexity of 97.68282028216345
Finished 4 epochs...
Completing Train Step...
At time: 39.93187189102173 and batch: 50, loss is 4.304397602081298 and perplexity is 74.02460974816064
At time: 40.38550138473511 and batch: 100, loss is 4.16889949798584 and perplexity is 64.64427179623337
At time: 40.82400155067444 and batch: 150, loss is 4.2124244546890255 and perplexity is 67.52004080918972
At time: 41.26536154747009 and batch: 200, loss is 4.249979243278504 and perplexity is 70.10395720327045
At time: 41.726511001586914 and batch: 250, loss is 4.2266701269149785 and perplexity is 68.48879305966129
At time: 42.18128967285156 and batch: 300, loss is 4.127487411499024 and perplexity is 62.021891504056455
At time: 42.62527561187744 and batch: 350, loss is 4.19754620552063 and perplexity is 66.52289708863684
At time: 43.06801891326904 and batch: 400, loss is 4.108892164230347 and perplexity is 60.8792360020045
At time: 43.517494916915894 and batch: 450, loss is 4.17775475025177 and perplexity is 65.2192551847177
At time: 43.95842432975769 and batch: 500, loss is 4.109161357879639 and perplexity is 60.8956265117216
At time: 44.409268856048584 and batch: 550, loss is 4.174560694694519 and perplexity is 65.01127358945908
At time: 44.86524939537048 and batch: 600, loss is 4.18150022983551 and perplexity is 65.46399061314585
At time: 45.30080485343933 and batch: 650, loss is 4.161113619804382 and perplexity is 64.14291366027221
At time: 45.741949796676636 and batch: 700, loss is 4.1331164741516115 and perplexity is 62.37200108707988
At time: 46.18933343887329 and batch: 750, loss is 4.092742819786071 and perplexity is 59.903972380125666
At time: 46.64654803276062 and batch: 800, loss is 4.103173999786377 and perplexity is 60.532111921446884
At time: 47.091997385025024 and batch: 850, loss is 4.090927858352661 and perplexity is 59.7953475852726
At time: 47.53325939178467 and batch: 900, loss is 4.267309045791626 and perplexity is 71.32943289320914
At time: 47.98046660423279 and batch: 950, loss is 4.185235414505005 and perplexity is 65.70896794024746
At time: 48.4202823638916 and batch: 1000, loss is 4.122762684822082 and perplexity is 61.72954618840495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.546136251310023 and perplexity of 94.26747792491624
Finished 5 epochs...
Completing Train Step...
At time: 49.69670271873474 and batch: 50, loss is 4.184136099815369 and perplexity is 65.6367727964116
At time: 50.15308666229248 and batch: 100, loss is 4.055006899833679 and perplexity is 57.68556101556664
At time: 50.599695920944214 and batch: 150, loss is 4.0981759405136104 and perplexity is 60.230323644243725
At time: 51.05000638961792 and batch: 200, loss is 4.142065782546997 and perplexity is 62.932692518162455
At time: 51.49984931945801 and batch: 250, loss is 4.113606348037719 and perplexity is 61.16690945056715
At time: 51.94091320037842 and batch: 300, loss is 4.024409027099609 and perplexity is 55.947235709805824
At time: 52.40000367164612 and batch: 350, loss is 4.093599786758423 and perplexity is 59.955330108763654
At time: 52.845513105392456 and batch: 400, loss is 4.006382608413697 and perplexity is 54.947743115993624
At time: 53.3151593208313 and batch: 450, loss is 4.078779153823852 and perplexity is 59.07330638838568
At time: 53.75946116447449 and batch: 500, loss is 4.007189803123474 and perplexity is 54.99211454933494
At time: 54.18648433685303 and batch: 550, loss is 4.0745005035400395 and perplexity is 58.82109232190509
At time: 54.641393423080444 and batch: 600, loss is 4.0774443292617795 and perplexity is 58.994506491774544
At time: 55.093167781829834 and batch: 650, loss is 4.060475821495056 and perplexity is 58.001903066241475
At time: 55.54697012901306 and batch: 700, loss is 4.031764698028565 and perplexity is 56.360282420559685
At time: 55.9953887462616 and batch: 750, loss is 3.996243133544922 and perplexity is 54.39341689310944
At time: 56.429022550582886 and batch: 800, loss is 4.002159152030945 and perplexity is 54.7161630979009
At time: 56.887654066085815 and batch: 850, loss is 3.992774796485901 and perplexity is 54.205088970677295
At time: 57.33009338378906 and batch: 900, loss is 4.169246554374695 and perplexity is 64.6667108973546
At time: 57.783045053482056 and batch: 950, loss is 4.092044067382813 and perplexity is 59.862128956259326
At time: 58.23077845573425 and batch: 1000, loss is 4.024397459030151 and perplexity is 55.94658851204058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.52720493223609 and perplexity of 92.49965661452562
Finished 6 epochs...
Completing Train Step...
At time: 59.61009669303894 and batch: 50, loss is 4.0890462732315065 and perplexity is 59.68294333119057
At time: 60.09313082695007 and batch: 100, loss is 3.961917824745178 and perplexity is 52.55802643895061
At time: 60.53517508506775 and batch: 150, loss is 4.004268393516541 and perplexity is 54.831694498018344
At time: 60.979960918426514 and batch: 200, loss is 4.05116865158081 and perplexity is 57.46457388481835
At time: 61.41860795021057 and batch: 250, loss is 4.024421377182007 and perplexity is 55.94792666704344
At time: 61.863625049591064 and batch: 300, loss is 3.9422270345687864 and perplexity is 51.53323990779156
At time: 62.29555368423462 and batch: 350, loss is 4.011560001373291 and perplexity is 55.23296689501288
At time: 62.741076707839966 and batch: 400, loss is 3.920238881111145 and perplexity is 50.41248593046094
At time: 63.18099117279053 and batch: 450, loss is 3.9963799285888673 and perplexity is 54.40085815191572
At time: 63.643938302993774 and batch: 500, loss is 3.9219056367874146 and perplexity is 50.4965812912818
At time: 64.1143069267273 and batch: 550, loss is 3.9927169370651243 and perplexity is 54.201952786356074
At time: 64.5658552646637 and batch: 600, loss is 3.9943128490447997 and perplexity is 54.28852339328982
At time: 65.01420474052429 and batch: 650, loss is 3.97701003074646 and perplexity is 53.357258921268325
At time: 65.46013927459717 and batch: 700, loss is 3.9503484630584715 and perplexity is 51.95346754513282
At time: 65.90293979644775 and batch: 750, loss is 3.9152859544754026 and perplexity is 50.16341391301197
At time: 66.34678220748901 and batch: 800, loss is 3.9194463682174683 and perplexity is 50.37254921263094
At time: 66.78655195236206 and batch: 850, loss is 3.910878648757935 and perplexity is 49.94281489280944
At time: 67.23724293708801 and batch: 900, loss is 4.088205943107605 and perplexity is 59.6328110228208
At time: 67.6892626285553 and batch: 950, loss is 4.01250837802887 and perplexity is 55.285373398058574
At time: 68.12905263900757 and batch: 1000, loss is 3.943433918952942 and perplexity is 51.59547211628773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.517165393364139 and perplexity of 91.57564878363303
Finished 7 epochs...
Completing Train Step...
At time: 69.50292873382568 and batch: 50, loss is 4.011601963043213 and perplexity is 55.23528461116582
At time: 69.96224999427795 and batch: 100, loss is 3.887104563713074 and perplexity is 48.76947299379399
At time: 70.40821647644043 and batch: 150, loss is 3.9275145053863527 and perplexity is 50.78060576380978
At time: 70.8527991771698 and batch: 200, loss is 3.9739169025421144 and perplexity is 53.19247306206834
At time: 71.30777168273926 and batch: 250, loss is 3.9494216442108154 and perplexity is 51.90533839915592
At time: 71.7401385307312 and batch: 300, loss is 3.8719779539108274 and perplexity is 48.03730775526395
At time: 72.17554497718811 and batch: 350, loss is 3.9417880535125733 and perplexity is 51.51062275632048
At time: 72.61528897285461 and batch: 400, loss is 3.846195969581604 and perplexity is 46.814639768891
At time: 73.05487585067749 and batch: 450, loss is 3.9266349983215334 and perplexity is 50.73596349675702
At time: 73.50377249717712 and batch: 500, loss is 3.8494759798049927 and perplexity is 46.96844436837478
At time: 73.96118378639221 and batch: 550, loss is 3.9226718282699586 and perplexity is 50.53528616754457
At time: 74.41240167617798 and batch: 600, loss is 3.9224294805526734 and perplexity is 50.52304054020936
At time: 74.88059163093567 and batch: 650, loss is 3.9051154327392577 and perplexity is 49.65581148770514
At time: 75.33003354072571 and batch: 700, loss is 3.880593671798706 and perplexity is 48.45297169736187
At time: 75.78270959854126 and batch: 750, loss is 3.8453521299362183 and perplexity is 46.775152382723654
At time: 76.2184796333313 and batch: 800, loss is 3.8482340955734253 and perplexity is 46.91015120210646
At time: 76.66373872756958 and batch: 850, loss is 3.839853649139404 and perplexity is 46.51866589479568
At time: 77.10825085639954 and batch: 900, loss is 4.018992176055908 and perplexity is 55.64499719719036
At time: 77.55676364898682 and batch: 950, loss is 3.943388333320618 and perplexity is 51.59312015767442
At time: 78.00852251052856 and batch: 1000, loss is 3.872589111328125 and perplexity is 48.06667508532229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5162785227705795 and perplexity of 91.4944690369267
Finished 8 epochs...
Completing Train Step...
At time: 79.3213152885437 and batch: 50, loss is 3.9415600395202635 and perplexity is 51.49887895250612
At time: 79.79044842720032 and batch: 100, loss is 3.820852584838867 and perplexity is 45.64310635703333
At time: 80.2380907535553 and batch: 150, loss is 3.8615306711196897 and perplexity is 47.53806084433063
At time: 80.682288646698 and batch: 200, loss is 3.9076650094985963 and perplexity is 49.78257431775621
At time: 81.1316442489624 and batch: 250, loss is 3.88192503452301 and perplexity is 48.5175231389181
At time: 81.57274746894836 and batch: 300, loss is 3.8104173707962037 and perplexity is 45.16928727405116
At time: 82.01709127426147 and batch: 350, loss is 3.8815147399902346 and perplexity is 48.49762074763076
At time: 82.4889497756958 and batch: 400, loss is 3.783093671798706 and perplexity is 43.95180408201393
At time: 82.95287227630615 and batch: 450, loss is 3.8655229139328005 and perplexity is 47.728223661588
At time: 83.41168284416199 and batch: 500, loss is 3.787118010520935 and perplexity is 44.1290374133766
At time: 83.87282943725586 and batch: 550, loss is 3.861429147720337 and perplexity is 47.533234863774204
At time: 84.31380009651184 and batch: 600, loss is 3.8558303117752075 and perplexity is 47.26784770267554
At time: 84.7557201385498 and batch: 650, loss is 3.8416545581817627 and perplexity is 46.60251726252451
At time: 85.19739007949829 and batch: 700, loss is 3.820021839141846 and perplexity is 45.60520428849853
At time: 85.66057109832764 and batch: 750, loss is 3.7830967569351195 and perplexity is 43.95193967953431
At time: 86.10705256462097 and batch: 800, loss is 3.7847827672958374 and perplexity is 44.026105609902295
At time: 86.55614995956421 and batch: 850, loss is 3.779468936920166 and perplexity is 43.79277883108532
At time: 86.99884629249573 and batch: 900, loss is 3.9556671667098997 and perplexity is 52.230528792865
At time: 87.45376181602478 and batch: 950, loss is 3.8834890222549436 and perplexity is 48.5934633191668
At time: 87.89060497283936 and batch: 1000, loss is 3.8086308574676515 and perplexity is 45.08866377921229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.519386105421113 and perplexity of 91.7792379037512
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 89.25150537490845 and batch: 50, loss is 3.885251593589783 and perplexity is 48.67918829066927
At time: 89.71536779403687 and batch: 100, loss is 3.7611221361160276 and perplexity is 42.99664700890012
At time: 90.15524053573608 and batch: 150, loss is 3.794969344139099 and perplexity is 44.47687290780008
At time: 90.61045265197754 and batch: 200, loss is 3.8327294826507567 and perplexity is 46.188436874088374
At time: 91.05912256240845 and batch: 250, loss is 3.80425856590271 and perplexity is 44.89195334595491
At time: 91.50319170951843 and batch: 300, loss is 3.726781907081604 and perplexity is 41.5451965068465
At time: 91.95652365684509 and batch: 350, loss is 3.7947314262390135 and perplexity is 44.46629232230073
At time: 92.39652109146118 and batch: 400, loss is 3.689452428817749 and perplexity is 40.022925555406964
At time: 92.84112191200256 and batch: 450, loss is 3.7664695835113524 and perplexity is 43.22718516292452
At time: 93.29100322723389 and batch: 500, loss is 3.6818601989746096 and perplexity is 39.7202128917327
At time: 93.71861338615417 and batch: 550, loss is 3.7522361612319948 and perplexity is 42.61627238603321
At time: 94.16995882987976 and batch: 600, loss is 3.739260582923889 and perplexity is 42.06687369764032
At time: 94.6312313079834 and batch: 650, loss is 3.7209590768814085 and perplexity is 41.303988819176425
At time: 95.07751607894897 and batch: 700, loss is 3.693710970878601 and perplexity is 40.1937282944791
At time: 95.53460001945496 and batch: 750, loss is 3.6565040493011476 and perplexity is 38.72572273135003
At time: 95.97726702690125 and batch: 800, loss is 3.65029682636261 and perplexity is 38.48608803929814
At time: 96.41408205032349 and batch: 850, loss is 3.6433534908294676 and perplexity is 38.21979177866995
At time: 96.85808753967285 and batch: 900, loss is 3.8112303018569946 and perplexity is 45.206021719933034
At time: 97.28960633277893 and batch: 950, loss is 3.736942434310913 and perplexity is 41.969469375192766
At time: 97.7241530418396 and batch: 1000, loss is 3.655316710472107 and perplexity is 38.67976946351669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.493152525366806 and perplexity of 89.40284682627377
Finished 10 epochs...
Completing Train Step...
At time: 99.10444951057434 and batch: 50, loss is 3.81324266910553 and perplexity is 45.29708443255823
At time: 99.55321049690247 and batch: 100, loss is 3.6904558801651 and perplexity is 40.06310677055612
At time: 99.98836159706116 and batch: 150, loss is 3.726654806137085 and perplexity is 41.5399164086901
At time: 100.42492294311523 and batch: 200, loss is 3.7699877262115478 and perplexity is 43.37953240138028
At time: 100.86368155479431 and batch: 250, loss is 3.7429068422317506 and perplexity is 42.220540411503315
At time: 101.30260729789734 and batch: 300, loss is 3.6705160999298094 and perplexity is 39.272168994615214
At time: 101.74033331871033 and batch: 350, loss is 3.742054624557495 and perplexity is 42.18457464825432
At time: 102.17498588562012 and batch: 400, loss is 3.639256420135498 and perplexity is 38.06352293069651
At time: 102.62585353851318 and batch: 450, loss is 3.71822265625 and perplexity is 41.191118233143534
At time: 103.06688690185547 and batch: 500, loss is 3.6354535150527956 and perplexity is 37.91904585629805
At time: 103.511892080307 and batch: 550, loss is 3.7080855417251586 and perplexity is 40.77566843894617
At time: 103.9559690952301 and batch: 600, loss is 3.6976509857177735 and perplexity is 40.35240456856788
At time: 104.39771628379822 and batch: 650, loss is 3.6822068309783935 and perplexity is 39.73398357526001
At time: 104.84509468078613 and batch: 700, loss is 3.658153576850891 and perplexity is 38.789654592065666
At time: 105.28049683570862 and batch: 750, loss is 3.6229718208312987 and perplexity is 37.448693424801995
At time: 105.72024035453796 and batch: 800, loss is 3.6186009359359743 and perplexity is 37.28536669754918
At time: 106.15396165847778 and batch: 850, loss is 3.613376622200012 and perplexity is 37.091084182453464
At time: 106.59171080589294 and batch: 900, loss is 3.7821280193328857 and perplexity is 43.90938239962245
At time: 107.04733204841614 and batch: 950, loss is 3.7115390491485596 and perplexity is 40.916730952612276
At time: 107.49727535247803 and batch: 1000, loss is 3.6301073741912844 and perplexity is 37.71686621777448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4997331572742 and perplexity of 89.99311408838857
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 108.80476093292236 and batch: 50, loss is 3.779095230102539 and perplexity is 43.77641622867179
At time: 109.25830602645874 and batch: 100, loss is 3.6587893676757814 and perplexity is 38.814324540188274
At time: 109.69443488121033 and batch: 150, loss is 3.6942870378494264 and perplexity is 40.21688924427247
At time: 110.15921092033386 and batch: 200, loss is 3.732946720123291 and perplexity is 41.80210596181921
At time: 110.59436917304993 and batch: 250, loss is 3.7054940605163575 and perplexity is 40.670135862316044
At time: 111.03571510314941 and batch: 300, loss is 3.6287125539779663 and perplexity is 37.66429464286185
At time: 111.49281883239746 and batch: 350, loss is 3.699985375404358 and perplexity is 40.44671283892171
At time: 111.93673038482666 and batch: 400, loss is 3.595372543334961 and perplexity is 36.42926894155216
At time: 112.38429379463196 and batch: 450, loss is 3.667975001335144 and perplexity is 39.17250122761594
At time: 112.83735179901123 and batch: 500, loss is 3.581853833198547 and perplexity is 35.940106089398824
At time: 113.28827977180481 and batch: 550, loss is 3.6510697650909423 and perplexity is 38.51584692666242
At time: 113.73486733436584 and batch: 600, loss is 3.6396341276168824 and perplexity is 38.077902523544225
At time: 114.18693494796753 and batch: 650, loss is 3.6205272340774535 and perplexity is 37.35725865056044
At time: 114.63803267478943 and batch: 700, loss is 3.5947861433029176 and perplexity is 36.40791307922866
At time: 115.09233593940735 and batch: 750, loss is 3.5572673654556275 and perplexity is 35.06724014531508
At time: 115.53789401054382 and batch: 800, loss is 3.547747540473938 and perplexity is 34.73499014669246
At time: 115.99066853523254 and batch: 850, loss is 3.5406674671173097 and perplexity is 34.48993240547988
At time: 116.43785738945007 and batch: 900, loss is 3.7075973892211915 and perplexity is 40.75576855178181
At time: 116.87873363494873 and batch: 950, loss is 3.6348725605010985 and perplexity is 37.89702301176711
At time: 117.33839917182922 and batch: 1000, loss is 3.5486376428604127 and perplexity is 34.765921608357935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.490256704935214 and perplexity of 89.14432673033721
Finished 12 epochs...
Completing Train Step...
At time: 118.65342831611633 and batch: 50, loss is 3.7428344297409057 and perplexity is 42.217483227697784
At time: 119.12930297851562 and batch: 100, loss is 3.6192155981063845 and perplexity is 37.308291646795794
At time: 119.58407211303711 and batch: 150, loss is 3.654653763771057 and perplexity is 38.6541353359225
At time: 120.02021312713623 and batch: 200, loss is 3.695384078025818 and perplexity is 40.26103299685075
At time: 120.4649076461792 and batch: 250, loss is 3.6702986192703246 and perplexity is 39.26362898607987
At time: 120.92653393745422 and batch: 300, loss is 3.595073518753052 and perplexity is 36.41837732314964
At time: 121.36202120780945 and batch: 350, loss is 3.6686706924438477 and perplexity is 39.199762670097726
At time: 121.83555197715759 and batch: 400, loss is 3.566351580619812 and perplexity is 35.387249817692606
At time: 122.27343678474426 and batch: 450, loss is 3.6411446809768675 and perplexity is 38.135464691560195
At time: 122.70959067344666 and batch: 500, loss is 3.5556415033340456 and perplexity is 35.010271971601554
At time: 123.15734767913818 and batch: 550, loss is 3.6266265630722048 and perplexity is 37.5858091552315
At time: 123.59945964813232 and batch: 600, loss is 3.6171051454544068 and perplexity is 37.229637291089794
At time: 124.04943799972534 and batch: 650, loss is 3.5997488737106322 and perplexity is 36.58904481879123
At time: 124.50209903717041 and batch: 700, loss is 3.5773670053482056 and perplexity is 35.77921024635194
At time: 124.94188642501831 and batch: 750, loss is 3.541188440322876 and perplexity is 34.50790541744724
At time: 125.39828944206238 and batch: 800, loss is 3.5336733436584473 and perplexity is 34.249547182038164
At time: 125.84489154815674 and batch: 850, loss is 3.528070497512817 and perplexity is 34.05818881495582
At time: 126.29120707511902 and batch: 900, loss is 3.6960408878326416 and perplexity is 40.2874855243447
At time: 126.74213790893555 and batch: 950, loss is 3.62491539478302 and perplexity is 37.52154850655397
At time: 127.20130896568298 and batch: 1000, loss is 3.5389958477020262 and perplexity is 34.43232652580657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.494813128215511 and perplexity of 89.5514327853842
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 128.56960320472717 and batch: 50, loss is 3.724738817214966 and perplexity is 41.4604025871506
At time: 129.02337050437927 and batch: 100, loss is 3.605354223251343 and perplexity is 36.794715092737
At time: 129.46256732940674 and batch: 150, loss is 3.6410812664031984 and perplexity is 38.13304642400262
At time: 129.91699171066284 and batch: 200, loss is 3.6797730779647826 and perplexity is 39.637398452831384
At time: 130.35266399383545 and batch: 250, loss is 3.65293381690979 and perplexity is 38.58770941807612
At time: 130.80714869499207 and batch: 300, loss is 3.5759293937683108 and perplexity is 35.72781059460198
At time: 131.24413084983826 and batch: 350, loss is 3.6492662382125856 and perplexity is 38.446445164271424
At time: 131.6958827972412 and batch: 400, loss is 3.5459014797210693 and perplexity is 34.67092639561344
At time: 132.1447148323059 and batch: 450, loss is 3.617662811279297 and perplexity is 37.25040477760062
At time: 132.58468294143677 and batch: 500, loss is 3.5289828395843506 and perplexity is 34.08927571227777
At time: 133.06319403648376 and batch: 550, loss is 3.598250832557678 and perplexity is 36.534273958649365
At time: 133.51540184020996 and batch: 600, loss is 3.586980924606323 and perplexity is 36.12484748684783
At time: 133.9651370048523 and batch: 650, loss is 3.5683255338668824 and perplexity is 35.45717158281866
At time: 134.4054250717163 and batch: 700, loss is 3.5442380714416504 and perplexity is 34.61330242897023
At time: 134.84429144859314 and batch: 750, loss is 3.505501194000244 and perplexity is 33.2981284931537
At time: 135.29609632492065 and batch: 800, loss is 3.494543151855469 and perplexity is 32.93523811309898
At time: 135.74273800849915 and batch: 850, loss is 3.488534688949585 and perplexity is 32.73794127552601
At time: 136.1916310787201 and batch: 900, loss is 3.6553266525268553 and perplexity is 38.680154021814
At time: 136.63833928108215 and batch: 950, loss is 3.5835527420043944 and perplexity is 36.00121694830671
At time: 137.08386635780334 and batch: 1000, loss is 3.4959677600860597 and perplexity is 32.98219136144239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.490004004501715 and perplexity of 89.12180276635613
Finished 14 epochs...
Completing Train Step...
At time: 138.45881867408752 and batch: 50, loss is 3.7064609241485598 and perplexity is 40.70947735346177
At time: 138.92015552520752 and batch: 100, loss is 3.582533531188965 and perplexity is 35.964542811137726
At time: 139.36714220046997 and batch: 150, loss is 3.617636547088623 and perplexity is 37.24942643871456
At time: 139.8191328048706 and batch: 200, loss is 3.6573366785049437 and perplexity is 38.75798032647853
At time: 140.28029894828796 and batch: 250, loss is 3.6317497158050536 and perplexity is 37.77886109115026
At time: 140.71505069732666 and batch: 300, loss is 3.556261157989502 and perplexity is 35.03197297248629
At time: 141.16282534599304 and batch: 350, loss is 3.6307509660720827 and perplexity is 37.74114829967862
At time: 141.6023919582367 and batch: 400, loss is 3.528932976722717 and perplexity is 34.08757596581721
At time: 142.05337715148926 and batch: 450, loss is 3.602118740081787 and perplexity is 36.67585879378903
At time: 142.4925980567932 and batch: 500, loss is 3.5140202617645264 and perplexity is 33.58300924249339
At time: 142.9546902179718 and batch: 550, loss is 3.584580464363098 and perplexity is 36.038235222901
At time: 143.38919043540955 and batch: 600, loss is 3.5750176429748537 and perplexity is 35.695250580508784
At time: 143.84689497947693 and batch: 650, loss is 3.5573551988601686 and perplexity is 35.070320355675605
At time: 144.28903722763062 and batch: 700, loss is 3.5353804302215575 and perplexity is 34.308064056299564
At time: 144.75104784965515 and batch: 750, loss is 3.4978958320617677 and perplexity is 33.04584474473699
At time: 145.20663332939148 and batch: 800, loss is 3.4884840965271 and perplexity is 32.73628502566691
At time: 145.63389444351196 and batch: 850, loss is 3.483552923202515 and perplexity is 32.57525409223255
At time: 146.0687050819397 and batch: 900, loss is 3.6512724590301513 and perplexity is 38.523654646659985
At time: 146.51765704154968 and batch: 950, loss is 3.580315046310425 and perplexity is 35.88484445423488
At time: 146.97000360488892 and batch: 1000, loss is 3.4931024646759035 and perplexity is 32.8878229012386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.492456947884908 and perplexity of 89.34068184199187
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 148.3592014312744 and batch: 50, loss is 3.6974380207061768 and perplexity is 40.34381183326944
At time: 148.82371520996094 and batch: 100, loss is 3.577220335006714 and perplexity is 35.773962882192556
At time: 149.26545405387878 and batch: 150, loss is 3.61227801322937 and perplexity is 37.05035795983271
At time: 149.7017047405243 and batch: 200, loss is 3.653321475982666 and perplexity is 38.602671193580186
At time: 150.14553666114807 and batch: 250, loss is 3.624947156906128 and perplexity is 37.522740289523504
At time: 150.59729981422424 and batch: 300, loss is 3.547183928489685 and perplexity is 34.71541860586846
At time: 151.0403368473053 and batch: 350, loss is 3.6218655729293823 and perplexity is 37.407288792387696
At time: 151.48445749282837 and batch: 400, loss is 3.519066877365112 and perplexity is 33.75291815272184
At time: 151.94534420967102 and batch: 450, loss is 3.590170645713806 and perplexity is 36.240259643824466
At time: 152.39772987365723 and batch: 500, loss is 3.5005146217346192 and perplexity is 33.13249827584844
At time: 152.8449318408966 and batch: 550, loss is 3.5691808080673217 and perplexity is 35.48751015894534
At time: 153.31022191047668 and batch: 600, loss is 3.5586856269836424 and perplexity is 35.11700994786243
At time: 153.7619731426239 and batch: 650, loss is 3.539419469833374 and perplexity is 34.44691591133182
At time: 154.19369745254517 and batch: 700, loss is 3.5166868352890015 and perplexity is 33.6726803099241
At time: 154.63139128684998 and batch: 750, loss is 3.4782802867889404 and perplexity is 32.40394863389053
At time: 155.07873249053955 and batch: 800, loss is 3.467055540084839 and perplexity is 32.04225626846505
At time: 155.52023220062256 and batch: 850, loss is 3.461074595451355 and perplexity is 31.851185269900707
At time: 155.9944224357605 and batch: 900, loss is 3.628168935775757 and perplexity is 37.64382521100258
At time: 156.44692540168762 and batch: 950, loss is 3.5569786739349367 and perplexity is 35.057117991591376
At time: 156.88446831703186 and batch: 1000, loss is 3.470386576652527 and perplexity is 32.14916816066166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.490603191096608 and perplexity of 89.17521935754093
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 158.28293824195862 and batch: 50, loss is 3.6904994821548462 and perplexity is 40.06485363980994
At time: 158.72332453727722 and batch: 100, loss is 3.570372672080994 and perplexity is 35.529831660966295
At time: 159.17222380638123 and batch: 150, loss is 3.6054959726333617 and perplexity is 36.799931090536454
At time: 159.61662793159485 and batch: 200, loss is 3.6489823055267334 and perplexity is 38.43553051142114
At time: 160.0656497478485 and batch: 250, loss is 3.6194918060302737 and perplexity is 37.318597915846475
At time: 160.52014183998108 and batch: 300, loss is 3.539707884788513 and perplexity is 34.45685234987892
At time: 160.96534276008606 and batch: 350, loss is 3.614176468849182 and perplexity is 37.12076322961182
At time: 161.4061734676361 and batch: 400, loss is 3.5108157205581665 and perplexity is 33.475563354876975
At time: 161.8502779006958 and batch: 450, loss is 3.581609001159668 and perplexity is 35.93130787703351
At time: 162.30076813697815 and batch: 500, loss is 3.4917198038101196 and perplexity is 32.84238161773379
At time: 162.73526620864868 and batch: 550, loss is 3.5596919107437133 and perplexity is 35.152365410505716
At time: 163.20001673698425 and batch: 600, loss is 3.5488659143447876 and perplexity is 34.77385858274713
At time: 163.65533900260925 and batch: 650, loss is 3.528711562156677 and perplexity is 34.08002931547737
At time: 164.09164476394653 and batch: 700, loss is 3.505890769958496 and perplexity is 33.31110317061652
At time: 164.53700375556946 and batch: 750, loss is 3.467520971298218 and perplexity is 32.05717320581432
At time: 164.9888243675232 and batch: 800, loss is 3.455618600845337 and perplexity is 31.6778785849561
At time: 165.44224190711975 and batch: 850, loss is 3.448721671104431 and perplexity is 31.46015017385988
At time: 165.88462018966675 and batch: 900, loss is 3.6156414699554444 and perplexity is 37.1751850430835
At time: 166.334547996521 and batch: 950, loss is 3.545434985160828 and perplexity is 34.654756368959326
At time: 166.77581095695496 and batch: 1000, loss is 3.460518412590027 and perplexity is 31.833475112040286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.490081414943788 and perplexity of 89.12870199153915
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 168.1616656780243 and batch: 50, loss is 3.684864058494568 and perplexity is 39.839706212085545
At time: 168.6153585910797 and batch: 100, loss is 3.563593292236328 and perplexity is 35.28977606974995
At time: 169.052255153656 and batch: 150, loss is 3.59959171295166 and perplexity is 36.583294908579404
At time: 169.49819922447205 and batch: 200, loss is 3.6453173923492432 and perplexity is 38.29492543923604
At time: 169.93865633010864 and batch: 250, loss is 3.616154851913452 and perplexity is 37.19427501217379
At time: 170.3960084915161 and batch: 300, loss is 3.5361621046066283 and perplexity is 34.33489227527349
At time: 170.84802889823914 and batch: 350, loss is 3.6100061464309694 and perplexity is 36.966280024812605
At time: 171.2867476940155 and batch: 400, loss is 3.5058540439605714 and perplexity is 33.30987980957533
At time: 171.7412085533142 and batch: 450, loss is 3.5768946170806886 and perplexity is 35.76231255865947
At time: 172.18740057945251 and batch: 500, loss is 3.4867787742614746 and perplexity is 32.68050668346076
At time: 172.64682865142822 and batch: 550, loss is 3.5544452238082886 and perplexity is 34.96841494139662
At time: 173.09143328666687 and batch: 600, loss is 3.543796820640564 and perplexity is 34.5980326506939
At time: 173.55884385108948 and batch: 650, loss is 3.5230573129653933 and perplexity is 33.88787609032685
At time: 174.00352597236633 and batch: 700, loss is 3.4999886322021485 and perplexity is 33.11507551106838
At time: 174.45191979408264 and batch: 750, loss is 3.4617641592025756 and perplexity is 31.873156267027248
At time: 174.88987398147583 and batch: 800, loss is 3.4492870378494263 and perplexity is 31.47794172546264
At time: 175.34604573249817 and batch: 850, loss is 3.441645550727844 and perplexity is 31.238320137795714
At time: 175.78044199943542 and batch: 900, loss is 3.608734622001648 and perplexity is 36.9193063671054
At time: 176.22125959396362 and batch: 950, loss is 3.5393183612823487 and perplexity is 34.44343320964516
At time: 176.6559910774231 and batch: 1000, loss is 3.4562827253341677 and perplexity is 31.698923627369084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.49034081435785 and perplexity of 89.15182492351975
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 178.00853991508484 and batch: 50, loss is 3.681619415283203 and perplexity is 39.7106500635819
At time: 178.47418355941772 and batch: 100, loss is 3.5583519744873047 and perplexity is 35.10529502429483
At time: 178.91137838363647 and batch: 150, loss is 3.5949143028259276 and perplexity is 36.412579399012955
At time: 179.36303615570068 and batch: 200, loss is 3.6413098573684692 and perplexity is 38.14176429026817
At time: 179.80554580688477 and batch: 250, loss is 3.612791075706482 and perplexity is 37.069371985539945
At time: 180.24949669837952 and batch: 300, loss is 3.5336629056930544 and perplexity is 34.24918968831572
At time: 180.6930410861969 and batch: 350, loss is 3.607338318824768 and perplexity is 36.86779179568633
At time: 181.14309191703796 and batch: 400, loss is 3.5026527500152587 and perplexity is 33.20341559556662
At time: 181.58898305892944 and batch: 450, loss is 3.574140439033508 and perplexity is 35.663952295508686
At time: 182.0322847366333 and batch: 500, loss is 3.4841004180908204 and perplexity is 32.59309376045126
At time: 182.47000789642334 and batch: 550, loss is 3.551705012321472 and perplexity is 34.87272525396465
At time: 182.92271971702576 and batch: 600, loss is 3.5413091659545897 and perplexity is 34.51207165760881
At time: 183.36982202529907 and batch: 650, loss is 3.5203259563446045 and perplexity is 33.79544250760739
At time: 183.83801746368408 and batch: 700, loss is 3.49704562664032 and perplexity is 33.01776092857677
At time: 184.27250123023987 and batch: 750, loss is 3.458847098350525 and perplexity is 31.78031580711846
At time: 184.71391320228577 and batch: 800, loss is 3.4460564613342286 and perplexity is 31.37641391116448
At time: 185.15041995048523 and batch: 850, loss is 3.437848219871521 and perplexity is 31.119922839964243
At time: 185.60402703285217 and batch: 900, loss is 3.604741277694702 and perplexity is 36.77216884612977
At time: 186.0582127571106 and batch: 950, loss is 3.5355755519866943 and perplexity is 34.31475895945607
At time: 186.5109784603119 and batch: 1000, loss is 3.4536955165863037 and perplexity is 31.617017894164555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.490784063571837 and perplexity of 89.19135015896025
Annealing...
Model not improving. Stopping early with 89.12180276635613loss at 18 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f0d70489898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -85.69732976492084, 'params': {'anneal': 4.437270328169882, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.10716540860374768, 'batch_size': 50, 'num_layers': 1, 'lr': 3.8680240822552636, 'tune_wordvecs': True, 'wordvec_dim': 200}}, {'best_accuracy': -89.76421759733587, 'params': {'anneal': 4.243624028933469, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.7357042626312168, 'batch_size': 50, 'num_layers': 1, 'lr': 7.1830387212582885, 'tune_wordvecs': True, 'wordvec_dim': 200}}, {'best_accuracy': -153.25514088898257, 'params': {'anneal': 5.841560043068629, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.8712656972912644, 'batch_size': 50, 'num_layers': 1, 'lr': 28.644506215470557, 'tune_wordvecs': True, 'wordvec_dim': 200}}, {'best_accuracy': -122.48956287253128, 'params': {'anneal': 6.288900331060056, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.8327629516169606, 'batch_size': 50, 'num_layers': 1, 'lr': 14.360885070025613, 'tune_wordvecs': True, 'wordvec_dim': 200}}, {'best_accuracy': -131.82372754219492, 'params': {'anneal': 3.710547618739717, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 0.010499086771258437, 'batch_size': 50, 'num_layers': 1, 'lr': 24.298905683244797, 'tune_wordvecs': True, 'wordvec_dim': 200}}, {'best_accuracy': -89.12180276635613, 'params': {'anneal': 2.0, 'wordvec_source': '', 'seq_len': 20, 'data': 'ptb', 'dropout': 1.0, 'batch_size': 50, 'num_layers': 1, 'lr': 3.4334501782790112, 'tune_wordvecs': True, 'wordvec_dim': 200}}]
