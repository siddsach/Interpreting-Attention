Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'lr': 2.3182911126552996, 'seq_len': 35, 'dropout': 0.08390891661702704, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.668310697536721, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7597551345825195 and batch: 50, loss is 6.818829078674316 and perplexity is 914.9130913875393
At time: 2.443650722503662 and batch: 100, loss is 5.909296960830688 and perplexity is 368.4470316378913
At time: 3.1124775409698486 and batch: 150, loss is 5.698864583969116 and perplexity is 298.5282547013818
At time: 3.781146287918091 and batch: 200, loss is 5.476448163986206 and perplexity is 238.9963223093684
At time: 4.450106143951416 and batch: 250, loss is 5.348758172988892 and perplexity is 210.34692111956224
At time: 5.120743989944458 and batch: 300, loss is 5.263195533752441 and perplexity is 193.09755620326442
At time: 5.791207313537598 and batch: 350, loss is 5.256501264572144 and perplexity is 191.80922620905253
At time: 6.461148738861084 and batch: 400, loss is 5.213459577560425 and perplexity is 183.72858325034733
At time: 7.130111217498779 and batch: 450, loss is 5.100535745620728 and perplexity is 164.10980486170558
At time: 7.801502466201782 and batch: 500, loss is 5.088541049957275 and perplexity is 162.15311609956143
At time: 8.470468759536743 and batch: 550, loss is 5.10084397315979 and perplexity is 164.1603958193568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0970822598071805 and perplexity of 163.54403148622558
Finished 1 epochs...
Completing Train Step...
At time: 10.275894403457642 and batch: 50, loss is 5.016200571060181 and perplexity is 150.83711878290413
At time: 10.951215982437134 and batch: 100, loss is 4.95866548538208 and perplexity is 142.4036293102076
At time: 11.613452911376953 and batch: 150, loss is 4.916143827438354 and perplexity is 136.47532477956992
At time: 12.291774272918701 and batch: 200, loss is 4.817917089462281 and perplexity is 123.70715131160372
At time: 12.959293603897095 and batch: 250, loss is 4.77382604598999 and perplexity is 118.37127059326241
At time: 13.620792865753174 and batch: 300, loss is 4.7445493412017825 and perplexity is 114.95598790060194
At time: 14.28229022026062 and batch: 350, loss is 4.775403728485108 and perplexity is 118.55817027020915
At time: 14.943873405456543 and batch: 400, loss is 4.76443489074707 and perplexity is 117.26483111900048
At time: 15.605518579483032 and batch: 450, loss is 4.681107711791992 and perplexity is 107.88951689434077
At time: 16.266725540161133 and batch: 500, loss is 4.698121089935302 and perplexity is 109.74078555514448
At time: 16.927846670150757 and batch: 550, loss is 4.728555402755737 and perplexity is 113.13201405065598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849468799347573 and perplexity of 127.67255208692747
Finished 2 epochs...
Completing Train Step...
At time: 18.724458694458008 and batch: 50, loss is 4.697744140625 and perplexity is 109.69942663731491
At time: 19.414160013198853 and batch: 100, loss is 4.660666904449463 and perplexity is 105.70655482653018
At time: 20.07947015762329 and batch: 150, loss is 4.648097295761108 and perplexity is 104.38618047536463
At time: 20.74411129951477 and batch: 200, loss is 4.566634426116943 and perplexity is 96.21972960732697
At time: 21.410504817962646 and batch: 250, loss is 4.540352306365967 and perplexity is 93.72381379991201
At time: 22.077203512191772 and batch: 300, loss is 4.519705743789673 and perplexity is 91.80857875860411
At time: 22.743229150772095 and batch: 350, loss is 4.559594554901123 and perplexity is 95.54473383185388
At time: 23.40855360031128 and batch: 400, loss is 4.55387095451355 and perplexity is 94.99943597841018
At time: 24.0771381855011 and batch: 450, loss is 4.482199039459228 and perplexity is 88.42891770049745
At time: 24.75738763809204 and batch: 500, loss is 4.5081605052948 and perplexity is 90.75472204044586
At time: 25.422173738479614 and batch: 550, loss is 4.54987642288208 and perplexity is 94.62071463719305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.736398899808843 and perplexity of 114.02285375169251
Finished 3 epochs...
Completing Train Step...
At time: 27.238733291625977 and batch: 50, loss is 4.52080228805542 and perplexity is 91.90930614511437
At time: 27.89856743812561 and batch: 100, loss is 4.490222234725952 and perplexity is 89.14125395970012
At time: 28.55891251564026 and batch: 150, loss is 4.488603172302246 and perplexity is 88.99704547778097
At time: 29.218801259994507 and batch: 200, loss is 4.417458992004395 and perplexity is 82.8854050705581
At time: 29.879132747650146 and batch: 250, loss is 4.3944783020019536 and perplexity is 81.00236096809849
At time: 30.544572353363037 and batch: 300, loss is 4.375866842269898 and perplexity is 79.50873121788916
At time: 31.22337508201599 and batch: 350, loss is 4.420074806213379 and perplexity is 83.10250170930524
At time: 31.91678810119629 and batch: 400, loss is 4.415506019592285 and perplexity is 82.72369012490223
At time: 32.624916076660156 and batch: 450, loss is 4.349486331939698 and perplexity is 77.43867493328003
At time: 33.33503603935242 and batch: 500, loss is 4.379197826385498 and perplexity is 79.77401512157508
At time: 34.04643392562866 and batch: 550, loss is 4.428246927261353 and perplexity is 83.78440792773699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6704065850440495 and perplexity of 106.74113295912956
Finished 4 epochs...
Completing Train Step...
At time: 35.98005652427673 and batch: 50, loss is 4.396096277236938 and perplexity is 81.13352686509235
At time: 36.68681287765503 and batch: 100, loss is 4.369153327941895 and perplexity is 78.97673598859411
At time: 37.39307880401611 and batch: 150, loss is 4.372910413742066 and perplexity is 79.27401646635828
At time: 38.099002838134766 and batch: 200, loss is 4.309414329528809 and perplexity is 74.3969041085324
At time: 38.80556106567383 and batch: 250, loss is 4.28789119720459 and perplexity is 72.81275873564245
At time: 39.511855602264404 and batch: 300, loss is 4.268294916152954 and perplexity is 71.39978914236083
At time: 40.218095541000366 and batch: 350, loss is 4.317096433639526 and perplexity is 74.97062975998297
At time: 40.92496919631958 and batch: 400, loss is 4.311018667221069 and perplexity is 74.51635766227878
At time: 41.646148443222046 and batch: 450, loss is 4.24920841217041 and perplexity is 70.0499397141129
At time: 42.353052616119385 and batch: 500, loss is 4.281380639076233 and perplexity is 72.34024686445325
At time: 43.058573484420776 and batch: 550, loss is 4.334610919952393 and perplexity is 76.29526814754153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.628931897751828 and perplexity of 102.40462706352996
Finished 5 epochs...
Completing Train Step...
At time: 44.9697003364563 and batch: 50, loss is 4.298202848434448 and perplexity is 73.5674629447956
At time: 45.67557215690613 and batch: 100, loss is 4.274371395111084 and perplexity is 71.83496930120447
At time: 46.38240647315979 and batch: 150, loss is 4.281379776000977 and perplexity is 72.34018442940307
At time: 47.088109493255615 and batch: 200, loss is 4.223544435501099 and perplexity is 68.27505244486944
At time: 47.79411840438843 and batch: 250, loss is 4.203202247619629 and perplexity is 66.90021946483633
At time: 48.50021982192993 and batch: 300, loss is 4.1826583814620975 and perplexity is 65.53985176123389
At time: 49.20611619949341 and batch: 350, loss is 4.233990130424499 and perplexity is 68.99197064929312
At time: 49.91259407997131 and batch: 400, loss is 4.228188247680664 and perplexity is 68.59284628131594
At time: 50.62007546424866 and batch: 450, loss is 4.167658104896545 and perplexity is 64.56407263360545
At time: 51.33158326148987 and batch: 500, loss is 4.2014626073837285 and perplexity is 66.78393832426573
At time: 52.046836614608765 and batch: 550, loss is 4.258177428245545 and perplexity is 70.68104471424422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.602607564723238 and perplexity of 99.74406594460152
Finished 6 epochs...
Completing Train Step...
At time: 53.94518232345581 and batch: 50, loss is 4.2177925968170165 and perplexity is 67.88347258871516
At time: 54.66592001914978 and batch: 100, loss is 4.1965499687194825 and perplexity is 66.45665753105918
At time: 55.37111306190491 and batch: 150, loss is 4.205202598571777 and perplexity is 67.03417731923423
At time: 56.07576251029968 and batch: 200, loss is 4.1521403598785405 and perplexity is 63.569917291048085
At time: 56.779420614242554 and batch: 250, loss is 4.1321183204650875 and perplexity is 62.30977530487224
At time: 57.48409390449524 and batch: 300, loss is 4.111068549156189 and perplexity is 61.011876940048985
At time: 58.189005613327026 and batch: 350, loss is 4.163785910606384 and perplexity is 64.31455140930842
At time: 58.89675712585449 and batch: 400, loss is 4.15935357093811 and perplexity is 64.03011828960506
At time: 59.60487914085388 and batch: 450, loss is 4.098624105453491 and perplexity is 60.25732281320732
At time: 60.329394578933716 and batch: 500, loss is 4.133415746688843 and perplexity is 62.39067010752061
At time: 61.038745403289795 and batch: 550, loss is 4.1920120859146115 and perplexity is 66.15576822412125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.585578106819315 and perplexity of 98.05985983276629
Finished 7 epochs...
Completing Train Step...
At time: 62.923832416534424 and batch: 50, loss is 4.149656510353088 and perplexity is 63.4122151174649
At time: 63.63679814338684 and batch: 100, loss is 4.129841675758362 and perplexity is 62.1680794415179
At time: 64.3373167514801 and batch: 150, loss is 4.139734129905701 and perplexity is 62.786126276612634
At time: 65.0400037765503 and batch: 200, loss is 4.090760140419007 and perplexity is 59.78531967408725
At time: 65.75869631767273 and batch: 250, loss is 4.07068281173706 and perplexity is 58.5969596269052
At time: 66.47071051597595 and batch: 300, loss is 4.048678631782532 and perplexity is 57.32166395626041
At time: 67.17535018920898 and batch: 350, loss is 4.102980842590332 and perplexity is 60.5204208375823
At time: 67.89703607559204 and batch: 400, loss is 4.099748272895813 and perplexity is 60.325100223116316
At time: 68.60455536842346 and batch: 450, loss is 4.039105315208435 and perplexity is 56.77552387765149
At time: 69.30848956108093 and batch: 500, loss is 4.074566149711609 and perplexity is 58.82495382816874
At time: 70.01249861717224 and batch: 550, loss is 4.133457374572754 and perplexity is 62.39326735315155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.573592165683178 and perplexity of 96.89153584329875
Finished 8 epochs...
Completing Train Step...
At time: 71.91311264038086 and batch: 50, loss is 4.090058236122132 and perplexity is 59.74337082507889
At time: 72.6289234161377 and batch: 100, loss is 4.071535382270813 and perplexity is 58.64693897048102
At time: 73.3299171924591 and batch: 150, loss is 4.082441682815552 and perplexity is 59.290060777989325
At time: 74.02992630004883 and batch: 200, loss is 4.036659803390503 and perplexity is 56.63684829853597
At time: 74.72953748703003 and batch: 250, loss is 4.016406173706055 and perplexity is 55.50128500380422
At time: 75.4335629940033 and batch: 300, loss is 3.9934226751327513 and perplexity is 54.24021866903393
At time: 76.14235854148865 and batch: 350, loss is 4.0490463972091675 and perplexity is 57.3427487593539
At time: 76.8505961894989 and batch: 400, loss is 4.046437735557556 and perplexity is 57.193355872135
At time: 77.56069946289062 and batch: 450, loss is 3.9863277959823606 and perplexity is 53.856752803027184
At time: 78.28681015968323 and batch: 500, loss is 4.0223024559021 and perplexity is 55.829502924264794
At time: 78.99560236930847 and batch: 550, loss is 4.0813614988327025 and perplexity is 59.226051181289186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.564873877992022 and perplexity of 96.05047917326989
Finished 9 epochs...
Completing Train Step...
At time: 80.89520192146301 and batch: 50, loss is 4.037075343132019 and perplexity is 56.66038805035073
At time: 81.62111306190491 and batch: 100, loss is 4.019352102279663 and perplexity is 55.66502889565294
At time: 82.33235955238342 and batch: 150, loss is 4.03136616230011 and perplexity is 56.33782530962704
At time: 83.04307794570923 and batch: 200, loss is 3.988122615814209 and perplexity is 53.953502769443894
At time: 83.75423121452332 and batch: 250, loss is 3.9673964309692384 and perplexity is 52.84676138004513
At time: 84.46589756011963 and batch: 300, loss is 3.9437455797195433 and perplexity is 51.61155490673787
At time: 85.17773962020874 and batch: 350, loss is 4.000504217147827 and perplexity is 54.62568629817571
At time: 85.88990449905396 and batch: 400, loss is 3.998918924331665 and perplexity is 54.53915719521369
At time: 86.60101270675659 and batch: 450, loss is 3.9389712238311767 and perplexity is 51.36573026976608
At time: 87.31314969062805 and batch: 500, loss is 3.9752949047088624 and perplexity is 53.26582293173527
At time: 88.02328372001648 and batch: 550, loss is 4.034372329711914 and perplexity is 56.50744106306466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.560859842503325 and perplexity of 95.66570191259471
Finished 10 epochs...
Completing Train Step...
At time: 89.91573071479797 and batch: 50, loss is 3.989412727355957 and perplexity is 54.02315372517491
At time: 90.63043260574341 and batch: 100, loss is 3.972247395515442 and perplexity is 53.10374194374757
At time: 91.32899308204651 and batch: 150, loss is 3.9852223205566406 and perplexity is 53.79724838269147
At time: 92.02812147140503 and batch: 200, loss is 3.9438873672485353 and perplexity is 51.61887330039175
At time: 92.72708082199097 and batch: 250, loss is 3.9227610301971434 and perplexity is 50.53979421352175
At time: 93.4263846874237 and batch: 300, loss is 3.899027256965637 and perplexity is 49.35441658272077
At time: 94.12682747840881 and batch: 350, loss is 3.9561443233489992 and perplexity is 52.25545688327115
At time: 94.82558488845825 and batch: 400, loss is 3.9552118730545045 and perplexity is 52.20675397715975
At time: 95.52623128890991 and batch: 450, loss is 3.8957705068588258 and perplexity is 49.193943034204004
At time: 96.24696278572083 and batch: 500, loss is 3.9321459770202636 and perplexity is 51.01634017619421
At time: 96.95064377784729 and batch: 550, loss is 3.9914688158035276 and perplexity is 54.13434437718657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.559979702563996 and perplexity of 95.58153975019074
Finished 11 epochs...
Completing Train Step...
At time: 98.834557056427 and batch: 50, loss is 3.94584406375885 and perplexity is 51.719974649716804
At time: 99.54620170593262 and batch: 100, loss is 3.928875403404236 and perplexity is 50.84976003482277
At time: 100.2430350780487 and batch: 150, loss is 3.9430884456634523 and perplexity is 51.577650337464476
At time: 100.94555330276489 and batch: 200, loss is 3.903323049545288 and perplexity is 49.566888961139064
At time: 101.64945793151855 and batch: 250, loss is 3.881904516220093 and perplexity is 48.516527651894435
At time: 102.35792350769043 and batch: 300, loss is 3.857928113937378 and perplexity is 47.36711037611012
At time: 103.06782078742981 and batch: 350, loss is 3.9153489160537718 and perplexity is 50.166572380158314
At time: 103.77770209312439 and batch: 400, loss is 3.9146405839920044 and perplexity is 50.1310503706868
At time: 104.48710656166077 and batch: 450, loss is 3.8562053298950194 and perplexity is 47.28557732630582
At time: 105.19750738143921 and batch: 500, loss is 3.8919198656082155 and perplexity is 49.004879050275235
At time: 105.90682816505432 and batch: 550, loss is 3.951515693664551 and perplexity is 52.01414462772838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.562012043405087 and perplexity of 95.77599154631243
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 107.81997966766357 and batch: 50, loss is 3.9100149631500245 and perplexity is 49.89969862449842
At time: 108.54118371009827 and batch: 100, loss is 3.8879806089401243 and perplexity is 48.81221597749578
At time: 109.24651551246643 and batch: 150, loss is 3.894126505851746 and perplexity is 49.113134585099736
At time: 109.95213460922241 and batch: 200, loss is 3.849947381019592 and perplexity is 46.99059056956084
At time: 110.6575288772583 and batch: 250, loss is 3.8195390415191652 and perplexity is 45.58319151857025
At time: 111.3631603717804 and batch: 300, loss is 3.784730305671692 and perplexity is 44.02379598948096
At time: 112.07371068000793 and batch: 350, loss is 3.8337558841705324 and perplexity is 46.23586909397913
At time: 112.78789520263672 and batch: 400, loss is 3.825296411514282 and perplexity is 45.846387749722794
At time: 113.50251483917236 and batch: 450, loss is 3.7594462537765505 and perplexity is 42.92465003360165
At time: 114.23267769813538 and batch: 500, loss is 3.787049593925476 and perplexity is 44.1260183581538
At time: 114.94729351997375 and batch: 550, loss is 3.8384830236434935 and perplexity is 46.454949900642376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.536551617561503 and perplexity of 93.36827482457946
Finished 13 epochs...
Completing Train Step...
At time: 116.85114526748657 and batch: 50, loss is 3.8640940093994143 and perplexity is 47.66007328821183
At time: 117.57090640068054 and batch: 100, loss is 3.843115472793579 and perplexity is 46.67064931637957
At time: 118.27493739128113 and batch: 150, loss is 3.8546321201324463 and perplexity is 47.211245679394295
At time: 118.97836995124817 and batch: 200, loss is 3.815334177017212 and perplexity is 45.391922786036595
At time: 119.68207812309265 and batch: 250, loss is 3.7886301279067993 and perplexity is 44.19581617400916
At time: 120.38676166534424 and batch: 300, loss is 3.7572838497161865 and perplexity is 42.831929881425935
At time: 121.0913815498352 and batch: 350, loss is 3.8093240594863893 and perplexity is 45.119930167675996
At time: 121.7969241142273 and batch: 400, loss is 3.8048357915878297 and perplexity is 44.91787361468413
At time: 122.50326371192932 and batch: 450, loss is 3.7431572675704956 and perplexity is 42.2311148286336
At time: 123.20757365226746 and batch: 500, loss is 3.774535870552063 and perplexity is 43.577278123334665
At time: 123.91102647781372 and batch: 550, loss is 3.830250687599182 and perplexity is 46.07408698884094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.537361307347075 and perplexity of 93.4439047772696
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 125.78483271598816 and batch: 50, loss is 3.847347836494446 and perplexity is 46.86859507217605
At time: 126.49564123153687 and batch: 100, loss is 3.828191776275635 and perplexity is 45.97932211912579
At time: 127.19497442245483 and batch: 150, loss is 3.8370287656784057 and perplexity is 46.38744151893012
At time: 127.89873218536377 and batch: 200, loss is 3.7956262254714965 and perplexity is 44.506098533169606
At time: 128.60790848731995 and batch: 250, loss is 3.765185446739197 and perplexity is 43.17171117061983
At time: 129.31835174560547 and batch: 300, loss is 3.7300936222076415 and perplexity is 41.68301043681498
At time: 130.0272080898285 and batch: 350, loss is 3.7776434421539307 and perplexity is 43.71290826635948
At time: 130.7367525100708 and batch: 400, loss is 3.7682459449768064 and perplexity is 43.30404051013709
At time: 131.44517707824707 and batch: 450, loss is 3.703676633834839 and perplexity is 40.59628799912423
At time: 132.17271375656128 and batch: 500, loss is 3.729460983276367 and perplexity is 41.656648481318854
At time: 132.88400983810425 and batch: 550, loss is 3.783638401031494 and perplexity is 43.97575243662341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.524826374459774 and perplexity of 92.27990228976329
Finished 15 epochs...
Completing Train Step...
At time: 134.78478956222534 and batch: 50, loss is 3.828021774291992 and perplexity is 45.97150620753804
At time: 135.50618267059326 and batch: 100, loss is 3.8071216487884523 and perplexity is 45.020666900149656
At time: 136.21290493011475 and batch: 150, loss is 3.8179337930679322 and perplexity is 45.51007786946888
At time: 136.91884875297546 and batch: 200, loss is 3.7788308382034304 and perplexity is 43.76484362876716
At time: 137.62508034706116 and batch: 250, loss is 3.7505134248733523 and perplexity is 42.54291898654978
At time: 138.3314607143402 and batch: 300, loss is 3.717848801612854 and perplexity is 41.17572162080963
At time: 139.0405821800232 and batch: 350, loss is 3.767214608192444 and perplexity is 43.25940248262397
At time: 139.75037336349487 and batch: 400, loss is 3.760308918952942 and perplexity is 42.96169561106182
At time: 140.45935320854187 and batch: 450, loss is 3.698365445137024 and perplexity is 40.38124502554256
At time: 141.1709144115448 and batch: 500, loss is 3.7262862873077394 and perplexity is 41.524610987665184
At time: 141.8826162815094 and batch: 550, loss is 3.7829210138320923 and perplexity is 43.94421610797306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.525185767640459 and perplexity of 92.31307301767016
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 143.80235171318054 and batch: 50, loss is 3.821192865371704 and perplexity is 45.65864046041143
At time: 144.52750658988953 and batch: 100, loss is 3.8019237613677976 and perplexity is 44.78726167455519
At time: 145.23794198036194 and batch: 150, loss is 3.8120271873474123 and perplexity is 45.24206010004515
At time: 145.94984436035156 and batch: 200, loss is 3.7720227241516113 and perplexity is 43.46789954343841
At time: 146.6599988937378 and batch: 250, loss is 3.74219621181488 and perplexity is 42.19054786933874
At time: 147.37140655517578 and batch: 300, loss is 3.707734560966492 and perplexity is 40.761359475134704
At time: 148.08236050605774 and batch: 350, loss is 3.7553804063797 and perplexity is 42.75047927280534
At time: 148.7945749759674 and batch: 400, loss is 3.7448254203796387 and perplexity is 42.30162157313005
At time: 149.50624656677246 and batch: 450, loss is 3.6814470052719117 and perplexity is 39.70380414012589
At time: 150.23304271697998 and batch: 500, loss is 3.7060801124572755 and perplexity is 40.69397765995906
At time: 150.9450089931488 and batch: 550, loss is 3.762429323196411 and perplexity is 43.05288842138536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.521411327605552 and perplexity of 91.96529959696448
Finished 17 epochs...
Completing Train Step...
At time: 152.84439992904663 and batch: 50, loss is 3.813120946884155 and perplexity is 45.29157110637326
At time: 153.55867624282837 and batch: 100, loss is 3.7928334188461306 and perplexity is 44.381975013488926
At time: 154.2584629058838 and batch: 150, loss is 3.8031169033050536 and perplexity is 44.840731126693605
At time: 154.95856881141663 and batch: 200, loss is 3.764033737182617 and perplexity is 43.12201851953414
At time: 155.65876960754395 and batch: 250, loss is 3.735463733673096 and perplexity is 41.907454955739446
At time: 156.36378502845764 and batch: 300, loss is 3.702257614135742 and perplexity is 40.538721920102994
At time: 157.07003903388977 and batch: 350, loss is 3.7508667421340944 and perplexity is 42.55795278984479
At time: 157.7783579826355 and batch: 400, loss is 3.7420320892333985 and perplexity is 42.183624015904186
At time: 158.486989736557 and batch: 450, loss is 3.679912815093994 and perplexity is 39.64293765610776
At time: 159.19602394104004 and batch: 500, loss is 3.705822296142578 and perplexity is 40.68348744094129
At time: 159.90329790115356 and batch: 550, loss is 3.7633453607559204 and perplexity is 43.09234455311498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.521535995158743 and perplexity of 91.9767654005357
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 161.79222631454468 and batch: 50, loss is 3.8102339220047 and perplexity is 45.161001782892555
At time: 162.5416338443756 and batch: 100, loss is 3.7907091903686525 and perplexity is 44.28779762084167
At time: 163.2707405090332 and batch: 150, loss is 3.800660824775696 and perplexity is 44.73073390593361
At time: 164.01049137115479 and batch: 200, loss is 3.7612876081466675 and perplexity is 43.0037623400693
At time: 164.74485969543457 and batch: 250, loss is 3.732246980667114 and perplexity is 41.77286561043333
At time: 165.4737672805786 and batch: 300, loss is 3.6978670501708986 and perplexity is 40.36112423076386
At time: 166.1989254951477 and batch: 350, loss is 3.745921230316162 and perplexity is 42.34800151754183
At time: 166.90490555763245 and batch: 400, loss is 3.7353295850753785 and perplexity is 41.90183350648643
At time: 167.61200881004333 and batch: 450, loss is 3.672698698043823 and perplexity is 39.357977965781146
At time: 168.35986733436584 and batch: 500, loss is 3.697207727432251 and perplexity is 40.33452199449446
At time: 169.081960439682 and batch: 550, loss is 3.7536200141906737 and perplexity is 42.67528786561738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520800326732879 and perplexity of 91.90912588149729
Finished 19 epochs...
Completing Train Step...
At time: 170.9808430671692 and batch: 50, loss is 3.8068011713027956 and perplexity is 45.00624110171419
At time: 171.7007758617401 and batch: 100, loss is 3.7869771242141725 and perplexity is 44.12282067421143
At time: 172.40611863136292 and batch: 150, loss is 3.7967723560333253 and perplexity is 44.55713757600251
At time: 173.11311197280884 and batch: 200, loss is 3.7577413272857667 and perplexity is 42.85152901134787
At time: 173.82006764411926 and batch: 250, loss is 3.729199194908142 and perplexity is 41.64574468259332
At time: 174.53082370758057 and batch: 300, loss is 3.695556683540344 and perplexity is 40.267982872942746
At time: 175.24326181411743 and batch: 350, loss is 3.744054045677185 and perplexity is 42.26900375427327
At time: 175.95804047584534 and batch: 400, loss is 3.734320740699768 and perplexity is 41.859582393408026
At time: 176.67395567893982 and batch: 450, loss is 3.6722927331924438 and perplexity is 39.342003252911034
At time: 177.39093542099 and batch: 500, loss is 3.6974780702590944 and perplexity is 40.34542761725185
At time: 178.1065697669983 and batch: 550, loss is 3.7545531511306764 and perplexity is 42.71512833856764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520815260866855 and perplexity of 91.91049847494602
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 180.00897121429443 and batch: 50, loss is 3.805506730079651 and perplexity is 44.94802085740484
At time: 180.7292721271515 and batch: 100, loss is 3.7859801626205445 and perplexity is 44.07885383686249
At time: 181.4327838420868 and batch: 150, loss is 3.795657138824463 and perplexity is 44.507474387168735
At time: 182.14066433906555 and batch: 200, loss is 3.7564016485214236 and perplexity is 42.7941601644049
At time: 182.84835124015808 and batch: 250, loss is 3.7274990034103395 and perplexity is 41.574999099125804
At time: 183.5590193271637 and batch: 300, loss is 3.693593349456787 and perplexity is 40.18900092903427
At time: 184.26750802993774 and batch: 350, loss is 3.7418561935424806 and perplexity is 42.176204750740006
At time: 184.97726368904114 and batch: 400, loss is 3.73138295173645 and perplexity is 41.73678823422202
At time: 185.68738341331482 and batch: 450, loss is 3.669131579399109 and perplexity is 39.21783349334518
At time: 186.4143967628479 and batch: 500, loss is 3.693864941596985 and perplexity is 40.19991742815959
At time: 187.1282114982605 and batch: 550, loss is 3.7502568769454956 and perplexity is 42.53200608873932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520702930206948 and perplexity of 91.9001746878497
Finished 21 epochs...
Completing Train Step...
At time: 189.02802419662476 and batch: 50, loss is 3.804175319671631 and perplexity is 44.88821641557792
At time: 189.7517900466919 and batch: 100, loss is 3.7845430040359496 and perplexity is 44.01555103265159
At time: 190.45212388038635 and batch: 150, loss is 3.7941266822814943 and perplexity is 44.439409730067425
At time: 191.15285634994507 and batch: 200, loss is 3.7550411462783813 and perplexity is 42.73597820083249
At time: 191.85268354415894 and batch: 250, loss is 3.726314344406128 and perplexity is 41.52577606410547
At time: 192.5549783706665 and batch: 300, loss is 3.6927494621276855 and perplexity is 40.15510024657106
At time: 193.25904035568237 and batch: 350, loss is 3.741171541213989 and perplexity is 42.147338596717844
At time: 193.96301102638245 and batch: 400, loss is 3.7310266494750977 and perplexity is 41.72191997114775
At time: 194.66872477531433 and batch: 450, loss is 3.669077544212341 and perplexity is 39.21571440764086
At time: 195.37258100509644 and batch: 500, loss is 3.6940955305099488 and perplexity is 40.2091881522426
At time: 196.0830693244934 and batch: 550, loss is 3.7507820081710816 and perplexity is 42.55434683862262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520685723487367 and perplexity of 91.89859340091884
Finished 22 epochs...
Completing Train Step...
At time: 197.9843990802765 and batch: 50, loss is 3.8031952381134033 and perplexity is 44.844243854355305
At time: 198.70765113830566 and batch: 100, loss is 3.7834658288955687 and perplexity is 43.9681641018829
At time: 199.41382718086243 and batch: 150, loss is 3.7930302476882933 and perplexity is 44.39071152601435
At time: 200.12049889564514 and batch: 200, loss is 3.7540301847457886 and perplexity is 42.69279560246452
At time: 200.82655501365662 and batch: 250, loss is 3.7254380989074707 and perplexity is 41.48940522696935
At time: 201.53332471847534 and batch: 300, loss is 3.6920715379714966 and perplexity is 40.12788735929875
At time: 202.2393937110901 and batch: 350, loss is 3.740623421669006 and perplexity is 42.124243146775946
At time: 202.94618892669678 and batch: 400, loss is 3.7307202339172365 and perplexity is 41.70913768421055
At time: 203.65398836135864 and batch: 450, loss is 3.668973069190979 and perplexity is 39.21161755905329
At time: 204.3742015361786 and batch: 500, loss is 3.69419207572937 and perplexity is 40.21307034453604
At time: 205.08204746246338 and batch: 550, loss is 3.751088328361511 and perplexity is 42.567384090934695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520709423308677 and perplexity of 91.90077140697015
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 206.9755654335022 and batch: 50, loss is 3.802684688568115 and perplexity is 44.82135448962125
At time: 207.69787502288818 and batch: 100, loss is 3.783091802597046 and perplexity is 43.951721927305556
At time: 208.40435814857483 and batch: 150, loss is 3.7926214361190795 and perplexity is 44.37256779851243
At time: 209.110200881958 and batch: 200, loss is 3.7535225105285646 and perplexity is 42.67112707161854
At time: 209.81591129302979 and batch: 250, loss is 3.7246178722381593 and perplexity is 41.455388462943986
At time: 210.52214670181274 and batch: 300, loss is 3.6913335704803467 and perplexity is 40.098285206995534
At time: 211.23001909255981 and batch: 350, loss is 3.7397636699676515 and perplexity is 42.088042321152834
At time: 211.9412612915039 and batch: 400, loss is 3.729470353126526 and perplexity is 41.65703879970185
At time: 212.65419507026672 and batch: 450, loss is 3.6675905084609983 and perplexity is 39.15744257518924
At time: 213.368638753891 and batch: 500, loss is 3.692637529373169 and perplexity is 40.15060582713363
At time: 214.08376955986023 and batch: 550, loss is 3.7492981576919555 and perplexity is 42.491249375854814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520683775556848 and perplexity of 91.89841438901844
Finished 24 epochs...
Completing Train Step...
At time: 215.98073172569275 and batch: 50, loss is 3.802233266830444 and perplexity is 44.80112572208899
At time: 216.69575142860413 and batch: 100, loss is 3.7825629615783694 and perplexity is 43.92848459887672
At time: 217.39542818069458 and batch: 150, loss is 3.792072196006775 and perplexity is 44.34820329598507
At time: 218.09799337387085 and batch: 200, loss is 3.7530464458465578 and perplexity is 42.650817689751605
At time: 218.80197715759277 and batch: 250, loss is 3.724235076904297 and perplexity is 41.43952257056544
At time: 219.5068817138672 and batch: 300, loss is 3.691022539138794 and perplexity is 40.085815322916545
At time: 220.2105734348297 and batch: 350, loss is 3.7395241594314577 and perplexity is 42.07796299866929
At time: 220.91502952575684 and batch: 400, loss is 3.729361238479614 and perplexity is 41.6524936545973
At time: 221.62416243553162 and batch: 450, loss is 3.667590789794922 and perplexity is 39.15745359150775
At time: 222.34902787208557 and batch: 500, loss is 3.692731637954712 and perplexity is 40.154384521497114
At time: 223.06040143966675 and batch: 550, loss is 3.7495010757446288 and perplexity is 42.49987249229722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520681178316157 and perplexity of 91.89817570702712
Finished 25 epochs...
Completing Train Step...
At time: 224.96297097206116 and batch: 50, loss is 3.801854519844055 and perplexity is 44.78416064367387
At time: 225.68381643295288 and batch: 100, loss is 3.782139415740967 and perplexity is 43.90988281171431
At time: 226.39203023910522 and batch: 150, loss is 3.7916381263732912 and perplexity is 44.32895726499572
At time: 227.09743475914001 and batch: 200, loss is 3.752657399177551 and perplexity is 42.63422775853699
At time: 227.80430793762207 and batch: 250, loss is 3.7239124250411986 and perplexity is 41.42615418818486
At time: 228.51087951660156 and batch: 300, loss is 3.690768208503723 and perplexity is 40.075621568395114
At time: 229.216379404068 and batch: 350, loss is 3.7393228673934935 and perplexity is 42.06949389215436
At time: 229.9232349395752 and batch: 400, loss is 3.7292549085617064 and perplexity is 41.64806498382062
At time: 230.63294887542725 and batch: 450, loss is 3.6675763082504274 and perplexity is 39.15688653520721
At time: 231.3431613445282 and batch: 500, loss is 3.6927965450286866 and perplexity is 40.156990909689256
At time: 232.0524537563324 and batch: 550, loss is 3.7496468305587767 and perplexity is 42.506067504779125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.52068539883228 and perplexity of 91.89856356557787
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 233.95706224441528 and batch: 50, loss is 3.8016381883621215 and perplexity is 44.77447346769303
At time: 234.6910982131958 and batch: 100, loss is 3.7819753217697145 and perplexity is 43.90267805581119
At time: 235.40337014198303 and batch: 150, loss is 3.791453175544739 and perplexity is 44.32075934575005
At time: 236.11243104934692 and batch: 200, loss is 3.752434883117676 and perplexity is 42.62474201356477
At time: 236.81850385665894 and batch: 250, loss is 3.723538179397583 and perplexity is 41.41065353115579
At time: 237.52406644821167 and batch: 300, loss is 3.6904679584503173 and perplexity is 40.0635906671087
At time: 238.23327541351318 and batch: 350, loss is 3.738995227813721 and perplexity is 42.05571251863928
At time: 238.94505834579468 and batch: 400, loss is 3.728764605522156 and perplexity is 41.62764981618601
At time: 239.65509510040283 and batch: 450, loss is 3.667031650543213 and perplexity is 39.13556524209567
At time: 240.38191056251526 and batch: 500, loss is 3.6921781396865843 and perplexity is 40.13216528892736
At time: 241.09676337242126 and batch: 550, loss is 3.7489164876937866 and perplexity is 42.475034835282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520676633144947 and perplexity of 91.89775801503387
Finished 27 epochs...
Completing Train Step...
At time: 242.99942803382874 and batch: 50, loss is 3.801479935646057 and perplexity is 44.76738834629105
At time: 243.71946954727173 and batch: 100, loss is 3.7817934465408327 and perplexity is 43.89469397226678
At time: 244.424706697464 and batch: 150, loss is 3.7912732696533205 and perplexity is 44.31278649723432
At time: 245.13013648986816 and batch: 200, loss is 3.7522772645950315 and perplexity is 42.618024094148666
At time: 245.83938837051392 and batch: 250, loss is 3.723411922454834 and perplexity is 41.4054254786896
At time: 246.54805541038513 and batch: 300, loss is 3.69036274433136 and perplexity is 40.05937563345881
At time: 247.25650572776794 and batch: 350, loss is 3.7389045190811157 and perplexity is 42.051897871271436
At time: 247.96595239639282 and batch: 400, loss is 3.7287262201309206 and perplexity is 41.6260519532291
At time: 248.67517018318176 and batch: 450, loss is 3.6670337915420532 and perplexity is 39.13564903138517
At time: 249.38996529579163 and batch: 500, loss is 3.692212986946106 and perplexity is 40.133563809273504
At time: 250.1033706665039 and batch: 550, loss is 3.748993730545044 and perplexity is 42.47831585479596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.52067273728391 and perplexity of 91.8973999948364
Finished 28 epochs...
Completing Train Step...
At time: 252.00073504447937 and batch: 50, loss is 3.801334581375122 and perplexity is 44.76088168809304
At time: 252.71758580207825 and batch: 100, loss is 3.781628427505493 and perplexity is 43.887451109832476
At time: 253.42041611671448 and batch: 150, loss is 3.7911081218719485 and perplexity is 44.30546894311344
At time: 254.12649869918823 and batch: 200, loss is 3.752129535675049 and perplexity is 42.61172864449888
At time: 254.83226609230042 and batch: 250, loss is 3.7232938766479493 and perplexity is 41.40053803030664
At time: 255.53691959381104 and batch: 300, loss is 3.690265212059021 and perplexity is 40.05546874205189
At time: 256.2414915561676 and batch: 350, loss is 3.738826131820679 and perplexity is 42.04860166739302
At time: 256.94881868362427 and batch: 400, loss is 3.728689765930176 and perplexity is 41.624534536433266
At time: 257.6518111228943 and batch: 450, loss is 3.667033591270447 and perplexity is 39.13564119362666
At time: 258.3816738128662 and batch: 500, loss is 3.6922422361373903 and perplexity is 40.1347377007259
At time: 259.11341285705566 and batch: 550, loss is 3.749058012962341 and perplexity is 42.481046551388765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.52067176331865 and perplexity of 91.89731049000495
Finished 29 epochs...
Completing Train Step...
At time: 261.0946202278137 and batch: 50, loss is 3.8011957168579102 and perplexity is 44.75466642141744
At time: 261.84401535987854 and batch: 100, loss is 3.781474208831787 and perplexity is 43.88068336719954
At time: 262.5702152252197 and batch: 150, loss is 3.79095251083374 and perplexity is 44.298575059488996
At time: 263.282502412796 and batch: 200, loss is 3.7519894313812254 and perplexity is 42.60575897654637
At time: 263.9901578426361 and batch: 250, loss is 3.723179702758789 and perplexity is 41.39581143969814
At time: 264.69205236434937 and batch: 300, loss is 3.6901727056503297 and perplexity is 40.05176352587088
At time: 265.3939759731293 and batch: 350, loss is 3.7387526512145994 and perplexity is 42.04551202417353
At time: 266.09848046302795 and batch: 400, loss is 3.728652696609497 and perplexity is 41.62299157181293
At time: 266.80445098876953 and batch: 450, loss is 3.6670304584503173 and perplexity is 39.13551858889419
At time: 267.51053833961487 and batch: 500, loss is 3.6922669267654418 and perplexity is 40.13572866484012
At time: 268.21752762794495 and batch: 550, loss is 3.7491136169433594 and perplexity is 42.483408732367565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.52067273728391 and perplexity of 91.8973999948364
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 270.1211383342743 and batch: 50, loss is 3.80111234664917 and perplexity is 44.75093537106718
At time: 270.8477637767792 and batch: 100, loss is 3.7814090633392334 and perplexity is 43.87782483157935
At time: 271.5587913990021 and batch: 150, loss is 3.790881519317627 and perplexity is 44.2954303481091
At time: 272.26927614212036 and batch: 200, loss is 3.7519040727615356 and perplexity is 42.60212236297966
At time: 272.98076581954956 and batch: 250, loss is 3.7230292415618895 and perplexity is 41.389583444909825
At time: 273.6923382282257 and batch: 300, loss is 3.6900531959533693 and perplexity is 40.04697723775879
At time: 274.4036362171173 and batch: 350, loss is 3.738620719909668 and perplexity is 42.03996527080892
At time: 275.1150321960449 and batch: 400, loss is 3.728458971977234 and perplexity is 41.61492895406604
At time: 275.8259778022766 and batch: 450, loss is 3.666815094947815 and perplexity is 39.12709113405432
At time: 276.5533893108368 and batch: 500, loss is 3.692026481628418 and perplexity is 40.12607938416954
At time: 277.26498103141785 and batch: 550, loss is 3.748827586174011 and perplexity is 42.47125890797795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520671438663564 and perplexity of 91.89728065508056
Finished 31 epochs...
Completing Train Step...
At time: 279.1675863265991 and batch: 50, loss is 3.801057372093201 and perplexity is 44.748475275887884
At time: 279.8899428844452 and batch: 100, loss is 3.7813457536697386 and perplexity is 43.87504702892292
At time: 280.5965402126312 and batch: 150, loss is 3.790820446014404 and perplexity is 44.29272516246822
At time: 281.30345606803894 and batch: 200, loss is 3.751850099563599 and perplexity is 42.59982305224797
At time: 282.0091166496277 and batch: 250, loss is 3.7229866123199464 and perplexity is 41.387819075950354
At time: 282.7159249782562 and batch: 300, loss is 3.6900177240371703 and perplexity is 40.045556719932584
At time: 283.4223358631134 and batch: 350, loss is 3.7385908794403075 and perplexity is 42.03871079723048
At time: 284.12914180755615 and batch: 400, loss is 3.7284458351135252 and perplexity is 41.61438226800699
At time: 284.83514857292175 and batch: 450, loss is 3.666816635131836 and perplexity is 39.12715139702127
At time: 285.54109168052673 and batch: 500, loss is 3.6920386600494384 and perplexity is 40.12656805943383
At time: 286.2492847442627 and batch: 550, loss is 3.7488521671295167 and perplexity is 42.47230290493461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520671114008477 and perplexity of 91.89725082016578
Finished 32 epochs...
Completing Train Step...
At time: 288.14735889434814 and batch: 50, loss is 3.8010040378570555 and perplexity is 44.74608871378368
At time: 288.8688917160034 and batch: 100, loss is 3.7812851762771604 and perplexity is 43.87238927347543
At time: 289.5756320953369 and batch: 150, loss is 3.7907609653472902 and perplexity is 44.290090679978434
At time: 290.2827727794647 and batch: 200, loss is 3.7517972087860105 and perplexity is 42.59756997406567
At time: 290.9889588356018 and batch: 250, loss is 3.7229444360733033 and perplexity is 41.38607352989553
At time: 291.6962208747864 and batch: 300, loss is 3.6899828195571898 and perplexity is 40.044158974993664
At time: 292.4017987251282 and batch: 350, loss is 3.7385619831085206 and perplexity is 42.03749605024633
At time: 293.10813093185425 and batch: 400, loss is 3.7284327650070193 and perplexity is 41.613838367153
At time: 293.8165156841278 and batch: 450, loss is 3.6668172454833985 and perplexity is 39.127175278346556
At time: 294.5421531200409 and batch: 500, loss is 3.6920495891571044 and perplexity is 40.127006609412895
At time: 295.2544951438904 and batch: 550, loss is 3.7488753747940065 and perplexity is 42.47328859932832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670140043218 and perplexity of 91.89716131547962
Finished 33 epochs...
Completing Train Step...
At time: 297.1598711013794 and batch: 50, loss is 3.8009516286849974 and perplexity is 44.74374366977279
At time: 297.8878753185272 and batch: 100, loss is 3.781226568222046 and perplexity is 43.869818073414145
At time: 298.59951663017273 and batch: 150, loss is 3.790702862739563 and perplexity is 44.28751738497174
At time: 299.31078124046326 and batch: 200, loss is 3.7517451095581054 and perplexity is 42.5953507313703
At time: 300.0225553512573 and batch: 250, loss is 3.7229025745391846 and perplexity is 41.38434108162814
At time: 300.7359037399292 and batch: 300, loss is 3.689948477745056 and perplexity is 40.042783809622065
At time: 301.447802066803 and batch: 350, loss is 3.7385341501235962 and perplexity is 42.03632603753506
At time: 302.159259557724 and batch: 400, loss is 3.728419418334961 and perplexity is 41.613282964605624
At time: 302.8716366291046 and batch: 450, loss is 3.6668172883987427 and perplexity is 39.12717695750278
At time: 303.5838956832886 and batch: 500, loss is 3.6920597314834596 and perplexity is 40.127413592673456
At time: 304.29566979408264 and batch: 550, loss is 3.7488972330093384 and perplexity is 42.47421699976293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670464698305 and perplexity of 91.89719115036534
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 306.19752955436707 and batch: 50, loss is 3.8009193658828737 and perplexity is 44.74230013451087
At time: 306.916246175766 and batch: 100, loss is 3.781199893951416 and perplexity is 43.86864789362128
At time: 307.619873046875 and batch: 150, loss is 3.790675387382507 and perplexity is 44.28630058633455
At time: 308.3274884223938 and batch: 200, loss is 3.75171275138855 and perplexity is 42.59397244608858
At time: 309.0362343788147 and batch: 250, loss is 3.7228452253341673 and perplexity is 41.38196779062078
At time: 309.7440984249115 and batch: 300, loss is 3.6899026679992675 and perplexity is 40.04094950188998
At time: 310.4545407295227 and batch: 350, loss is 3.738482437133789 and perplexity is 42.03415226964165
At time: 311.16343903541565 and batch: 400, loss is 3.728344464302063 and perplexity is 41.61016399811631
At time: 311.87254214286804 and batch: 450, loss is 3.6667337465286254 and perplexity is 39.1239083365026
At time: 312.59790563583374 and batch: 500, loss is 3.6919683456420898 and perplexity is 40.12374668277467
At time: 313.3072476387024 and batch: 550, loss is 3.748789014816284 and perplexity is 42.469620765450394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 315.200190782547 and batch: 50, loss is 3.800906472206116 and perplexity is 44.74172324547465
At time: 315.9159162044525 and batch: 100, loss is 3.78118860244751 and perplexity is 43.868152553408805
At time: 316.6160023212433 and batch: 150, loss is 3.7906644916534424 and perplexity is 44.28581805743084
At time: 317.3166117668152 and batch: 200, loss is 3.751700310707092 and perplexity is 42.59344255134148
At time: 318.01699662208557 and batch: 250, loss is 3.7228236961364747 and perplexity is 41.38107687964563
At time: 318.7259261608124 and batch: 300, loss is 3.6898852014541625 and perplexity is 40.04025013094728
At time: 319.4328770637512 and batch: 350, loss is 3.738462553024292 and perplexity is 42.03331646626493
At time: 320.142418384552 and batch: 400, loss is 3.728316240310669 and perplexity is 41.60898960977878
At time: 320.8510911464691 and batch: 450, loss is 3.6667023849487306 and perplexity is 39.122681368165445
At time: 321.55999398231506 and batch: 500, loss is 3.6919339513778686 and perplexity is 40.12236667976195
At time: 322.26954770088196 and batch: 550, loss is 3.748748531341553 and perplexity is 42.4679014824328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 324.18283557891846 and batch: 50, loss is 3.800901608467102 and perplexity is 44.741505633938964
At time: 324.90376591682434 and batch: 100, loss is 3.781184139251709 and perplexity is 43.86795676169146
At time: 325.61092162132263 and batch: 150, loss is 3.7906602478027343 and perplexity is 44.285630115429306
At time: 326.31826663017273 and batch: 200, loss is 3.7516956186294554 and perplexity is 42.59324270007108
At time: 327.0251386165619 and batch: 250, loss is 3.722815613746643 and perplexity is 41.380742423002246
At time: 327.73220658302307 and batch: 300, loss is 3.6898786401748658 and perplexity is 40.03998741654493
At time: 328.439190864563 and batch: 350, loss is 3.738455057144165 and perplexity is 42.03300139074425
At time: 329.1491467952728 and batch: 400, loss is 3.728305540084839 and perplexity is 41.60854438657538
At time: 329.85874605178833 and batch: 450, loss is 3.6666906023025514 and perplexity is 39.122220402169006
At time: 330.5850827693939 and batch: 500, loss is 3.691921215057373 and perplexity is 40.121855671695066
At time: 331.29456543922424 and batch: 550, loss is 3.7487334394454956 and perplexity is 42.46726056611419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670140043218 and perplexity of 91.89716131547962
Finished 37 epochs...
Completing Train Step...
At time: 333.1911358833313 and batch: 50, loss is 3.800898847579956 and perplexity is 44.74138210786169
At time: 333.9111235141754 and batch: 100, loss is 3.7811810064315794 and perplexity is 43.86781933148875
At time: 334.61556243896484 and batch: 150, loss is 3.7906571578979493 and perplexity is 44.28549327726032
At time: 335.3208863735199 and batch: 200, loss is 3.751692795753479 and perplexity is 42.5931224647992
At time: 336.02713441848755 and batch: 250, loss is 3.7228135585784914 and perplexity is 41.38065737870572
At time: 336.7352783679962 and batch: 300, loss is 3.689876842498779 and perplexity is 40.039915437681735
At time: 337.44585633277893 and batch: 350, loss is 3.7384536457061768 and perplexity is 42.0329420638112
At time: 338.15656089782715 and batch: 400, loss is 3.7283049631118774 and perplexity is 41.608520379577236
At time: 338.8660092353821 and batch: 450, loss is 3.6666907978057863 and perplexity is 39.1222280506904
At time: 339.5808050632477 and batch: 500, loss is 3.6919217252731324 and perplexity is 40.121876142503346
At time: 340.2944531440735 and batch: 550, loss is 3.748734631538391 and perplexity is 42.46731119106398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 342.197637796402 and batch: 50, loss is 3.8008972454071044 and perplexity is 44.74131042449135
At time: 342.9186975955963 and batch: 100, loss is 3.781179552078247 and perplexity is 43.86775553222591
At time: 343.6241807937622 and batch: 150, loss is 3.790655798912048 and perplexity is 44.28543309394023
At time: 344.3305079936981 and batch: 200, loss is 3.7516912269592284 and perplexity is 42.59305564500598
At time: 345.03622221946716 and batch: 250, loss is 3.722810697555542 and perplexity is 41.38053898786466
At time: 345.7430076599121 and batch: 300, loss is 3.6898745584487913 and perplexity is 40.03982398461781
At time: 346.4483287334442 and batch: 350, loss is 3.7384508657455444 and perplexity is 42.03282521404942
At time: 347.1537094116211 and batch: 400, loss is 3.728300814628601 and perplexity is 41.60834776768432
At time: 347.86129903793335 and batch: 450, loss is 3.6666862201690673 and perplexity is 39.12204896375265
At time: 348.58598375320435 and batch: 500, loss is 3.6919167137145994 and perplexity is 40.121675069876446
At time: 349.29509115219116 and batch: 550, loss is 3.748728632926941 and perplexity is 42.467056446928865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 351.19313073158264 and batch: 50, loss is 3.80089684009552 and perplexity is 44.741292290323614
At time: 351.9144186973572 and batch: 100, loss is 3.7811792278289795 and perplexity is 43.86774130814062
At time: 352.621808052063 and batch: 150, loss is 3.7906555557250976 and perplexity is 44.285422324302104
At time: 353.32840037345886 and batch: 200, loss is 3.751690969467163 and perplexity is 42.59304467763352
At time: 354.03398609161377 and batch: 250, loss is 3.7228098821640017 and perplexity is 41.380505246536984
At time: 354.73983812332153 and batch: 300, loss is 3.6898738813400267 and perplexity is 40.03979687331124
At time: 355.45301699638367 and batch: 350, loss is 3.7384500694274903 and perplexity is 42.03279174256516
At time: 356.18263363838196 and batch: 400, loss is 3.728299503326416 and perplexity is 41.60829320660275
At time: 356.8923044204712 and batch: 450, loss is 3.6666845893859863 and perplexity is 39.12198516422912
At time: 357.6056652069092 and batch: 500, loss is 3.691915040016174 and perplexity is 40.12160791834826
At time: 358.32994747161865 and batch: 550, loss is 3.7487264013290407 and perplexity is 42.46696167764061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 360.23053312301636 and batch: 50, loss is 3.800896854400635 and perplexity is 44.74129293035294
At time: 360.9569556713104 and batch: 100, loss is 3.7811792135238647 and perplexity is 43.86774068060755
At time: 361.6688301563263 and batch: 150, loss is 3.790655589103699 and perplexity is 44.28542380248758
At time: 362.380836725235 and batch: 200, loss is 3.751690835952759 and perplexity is 42.59303899084893
At time: 363.0928781032562 and batch: 250, loss is 3.722809648513794 and perplexity is 41.38049557797447
At time: 363.8056254386902 and batch: 300, loss is 3.6898736476898195 and perplexity is 40.0397875180055
At time: 364.51728224754333 and batch: 350, loss is 3.738449854850769 and perplexity is 42.032782723307484
At time: 365.23094749450684 and batch: 400, loss is 3.7282991361618043 and perplexity is 41.60827792951274
At time: 365.94311141967773 and batch: 450, loss is 3.6666843366622923 and perplexity is 39.12197527717777
At time: 366.6704957485199 and batch: 500, loss is 3.6919146251678465 and perplexity is 40.12159127396977
At time: 367.3833873271942 and batch: 550, loss is 3.7487259197235105 and perplexity is 42.466941225321946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670789353391 and perplexity of 91.89722098526069
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 369.27904081344604 and batch: 50, loss is 3.800896878242493 and perplexity is 44.7412939970685
At time: 369.9984085559845 and batch: 100, loss is 3.7811791467666627 and perplexity is 43.86773775212002
At time: 370.7022988796234 and batch: 150, loss is 3.7906555700302125 and perplexity is 44.28542295781016
At time: 371.4068343639374 and batch: 200, loss is 3.7516908740997312 and perplexity is 42.59304061564443
At time: 372.11251759529114 and batch: 250, loss is 3.722809624671936 and perplexity is 41.380494591386594
At time: 372.8174669742584 and batch: 300, loss is 3.689873666763306 and perplexity is 40.03978828170384
At time: 373.5249412059784 and batch: 350, loss is 3.7384498214721678 and perplexity is 42.03278132031202
At time: 374.23536229133606 and batch: 400, loss is 3.7282991218566894 and perplexity is 41.60827733430155
At time: 374.9445536136627 and batch: 450, loss is 3.6666842317581176 and perplexity is 39.121971173119455
At time: 375.6545741558075 and batch: 500, loss is 3.691914610862732 and perplexity is 40.12159070002582
At time: 376.36673521995544 and batch: 550, loss is 3.7487257957458495 and perplexity is 42.46693596037023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.520670464698305 and perplexity of 91.89719115036534
Annealing...
Model not improving. Stopping early with 91.89716131547962loss at 41 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -91.89716131547962
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23d1748898>
SETTINGS FOR THIS RUN
{'lr': 2.8093823593666523, 'seq_len': 35, 'dropout': 0.4612155562702229, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 6.351047942086454, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.959991455078125 and batch: 50, loss is 6.86280463218689 and perplexity is 956.0446645368635
At time: 1.6466090679168701 and batch: 100, loss is 5.940772008895874 and perplexity is 380.22835593291876
At time: 2.3174028396606445 and batch: 150, loss is 5.76932538986206 and perplexity is 320.3215675650963
At time: 2.9873383045196533 and batch: 200, loss is 5.606883764266968 and perplexity is 272.29438103508426
At time: 3.6557869911193848 and batch: 250, loss is 5.496465072631836 and perplexity is 243.82849105617845
At time: 4.325122594833374 and batch: 300, loss is 5.456733827590942 and perplexity is 234.33080827600983
At time: 4.99560284614563 and batch: 350, loss is 5.4641568470001225 and perplexity is 236.07672237430546
At time: 5.666551351547241 and batch: 400, loss is 5.443496656417847 and perplexity is 231.24937100670888
At time: 6.338850975036621 and batch: 450, loss is 5.334952898025513 and perplexity is 207.4629766619636
At time: 7.009267807006836 and batch: 500, loss is 5.33208701133728 and perplexity is 206.8692624440468
At time: 7.6797099113464355 and batch: 550, loss is 5.355767126083374 and perplexity is 211.8264116062591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.225621324904422 and perplexity of 185.9766865827521
Finished 1 epochs...
Completing Train Step...
At time: 9.497313499450684 and batch: 50, loss is 5.125412368774414 and perplexity is 168.24350571733103
At time: 10.174064874649048 and batch: 100, loss is 5.037993278503418 and perplexity is 154.16034753828058
At time: 10.834895133972168 and batch: 150, loss is 4.973694620132446 and perplexity is 144.5599962195353
At time: 11.496649742126465 and batch: 200, loss is 4.861869993209839 and perplexity is 129.2657022206353
At time: 12.158106803894043 and batch: 250, loss is 4.807176656723023 and perplexity is 122.38559273330364
At time: 12.819088697433472 and batch: 300, loss is 4.776764078140259 and perplexity is 118.71956058499269
At time: 13.480725526809692 and batch: 350, loss is 4.796807889938354 and perplexity is 121.12316130344024
At time: 14.141604661941528 and batch: 400, loss is 4.7840650844573975 and perplexity is 119.58950471625195
At time: 14.808500051498413 and batch: 450, loss is 4.69602518081665 and perplexity is 109.51101971028639
At time: 15.489804983139038 and batch: 500, loss is 4.70425407409668 and perplexity is 110.41589214773924
At time: 16.18542981147766 and batch: 550, loss is 4.732565755844116 and perplexity is 113.58662433719319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.818677861639794 and perplexity of 123.80130007882147
Finished 2 epochs...
Completing Train Step...
At time: 18.085288047790527 and batch: 50, loss is 4.663172578811645 and perplexity is 105.97175314243233
At time: 18.8122820854187 and batch: 100, loss is 4.620760173797607 and perplexity is 101.57121456570005
At time: 19.524152994155884 and batch: 150, loss is 4.607246427536011 and perplexity is 100.207839842988
At time: 20.236817359924316 and batch: 200, loss is 4.530289297103882 and perplexity is 92.78539974717988
At time: 20.949082374572754 and batch: 250, loss is 4.500724382400513 and perplexity is 90.08236174922273
At time: 21.661524772644043 and batch: 300, loss is 4.4805734252929685 and perplexity is 88.28528317796476
At time: 22.373530387878418 and batch: 350, loss is 4.520424203872681 and perplexity is 91.87456325849617
At time: 23.088470697402954 and batch: 400, loss is 4.515438919067383 and perplexity is 91.41768218167608
At time: 23.803974390029907 and batch: 450, loss is 4.44214186668396 and perplexity is 84.95671289240136
At time: 24.51869559288025 and batch: 500, loss is 4.4648135375976565 and perplexity is 86.90482354672139
At time: 25.233370304107666 and batch: 550, loss is 4.5079429149627686 and perplexity is 90.73497683860407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6883557908078455 and perplexity of 108.67434946611473
Finished 3 epochs...
Completing Train Step...
At time: 27.148324251174927 and batch: 50, loss is 4.454761953353882 and perplexity is 86.03566790509613
At time: 27.862502336502075 and batch: 100, loss is 4.420112819671631 and perplexity is 83.10566078282783
At time: 28.56170630455017 and batch: 150, loss is 4.420731563568115 and perplexity is 83.15709781473423
At time: 29.260754346847534 and batch: 200, loss is 4.354119176864624 and perplexity is 77.79826863319092
At time: 29.960789918899536 and batch: 250, loss is 4.336361885070801 and perplexity is 76.42897552508364
At time: 30.66656255722046 and batch: 300, loss is 4.312631330490112 and perplexity is 74.63662440403743
At time: 31.37104558944702 and batch: 350, loss is 4.357776956558228 and perplexity is 78.08335864075526
At time: 32.074761390686035 and batch: 400, loss is 4.353835582733154 and perplexity is 77.77620862895975
At time: 32.77842879295349 and batch: 450, loss is 4.287749891281128 and perplexity is 72.8024705884347
At time: 33.482197284698486 and batch: 500, loss is 4.315714502334595 and perplexity is 74.86709705378779
At time: 34.18646025657654 and batch: 550, loss is 4.366997194290161 and perplexity is 78.80663503651722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.6220407688871346 and perplexity of 101.70136948439809
Finished 4 epochs...
Completing Train Step...
At time: 36.08212447166443 and batch: 50, loss is 4.3154892730712895 and perplexity is 74.8502366914673
At time: 36.803138256073 and batch: 100, loss is 4.284891786575318 and perplexity is 72.59469057461105
At time: 37.50988411903381 and batch: 150, loss is 4.290060415267944 and perplexity is 72.97087692154479
At time: 38.21659588813782 and batch: 200, loss is 4.231851377487183 and perplexity is 68.84457155073007
At time: 38.92317342758179 and batch: 250, loss is 4.2186612701416015 and perplexity is 67.94246677015933
At time: 39.629071950912476 and batch: 300, loss is 4.192788643836975 and perplexity is 66.2071619625704
At time: 40.334725856781006 and batch: 350, loss is 4.240177936553955 and perplexity is 69.42020313156947
At time: 41.04090738296509 and batch: 400, loss is 4.23682665348053 and perplexity is 69.18794577730628
At time: 41.74853491783142 and batch: 450, loss is 4.174343156814575 and perplexity is 64.99713271297387
At time: 42.455167293548584 and batch: 500, loss is 4.2054996681213375 and perplexity is 67.05409409028225
At time: 43.162906646728516 and batch: 550, loss is 4.261787123680115 and perplexity is 70.93664279674087
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.58197021484375 and perplexity of 97.70670790143828
Finished 5 epochs...
Completing Train Step...
At time: 45.079662561416626 and batch: 50, loss is 4.2094086122512815 and perplexity is 67.31671775397548
At time: 45.80721068382263 and batch: 100, loss is 4.181015968322754 and perplexity is 65.432296596736
At time: 46.51893329620361 and batch: 150, loss is 4.189629955291748 and perplexity is 65.99836409578573
At time: 47.23079490661621 and batch: 200, loss is 4.137538781166077 and perplexity is 62.6484400233955
At time: 47.94308614730835 and batch: 250, loss is 4.126368460655212 and perplexity is 61.95253086903067
At time: 48.65605354309082 and batch: 300, loss is 4.098304071426392 and perplexity is 60.238041505027084
At time: 49.367762327194214 and batch: 350, loss is 4.147267804145813 and perplexity is 63.26092273406293
At time: 50.07951378822327 and batch: 400, loss is 4.145044002532959 and perplexity is 63.120399298088856
At time: 50.79408836364746 and batch: 450, loss is 4.084522414207458 and perplexity is 59.41355590421565
At time: 51.50965642929077 and batch: 500, loss is 4.1168443775177 and perplexity is 61.365290715426475
At time: 52.22460699081421 and batch: 550, loss is 4.177056984901428 and perplexity is 65.17376332144339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.55779477383228 and perplexity of 95.37292888070107
Finished 6 epochs...
Completing Train Step...
At time: 54.128421783447266 and batch: 50, loss is 4.124717712402344 and perplexity is 61.850347199898444
At time: 54.84834814071655 and batch: 100, loss is 4.097709841728211 and perplexity is 60.20225690499357
At time: 55.5529625415802 and batch: 150, loss is 4.107583088874817 and perplexity is 60.799592635448576
At time: 56.2579128742218 and batch: 200, loss is 4.060992045402527 and perplexity is 58.03185276499322
At time: 56.962238788604736 and batch: 250, loss is 4.0502112770080565 and perplexity is 57.409585089584134
At time: 57.6672797203064 and batch: 300, loss is 4.021281027793885 and perplexity is 55.772506214686345
At time: 58.373154640197754 and batch: 350, loss is 4.070266132354736 and perplexity is 58.57254856810743
At time: 59.08210229873657 and batch: 400, loss is 4.068138318061829 and perplexity is 58.44804956444532
At time: 59.79204535484314 and batch: 450, loss is 4.010141344070434 and perplexity is 55.15466579751305
At time: 60.502577781677246 and batch: 500, loss is 4.04213173866272 and perplexity is 56.94761092753727
At time: 61.21308779716492 and batch: 550, loss is 4.105041794776916 and perplexity is 60.64527915055217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.543288859915226 and perplexity of 93.99944330029933
Finished 7 epochs...
Completing Train Step...
At time: 63.1359703540802 and batch: 50, loss is 4.053000407218933 and perplexity is 57.56993140718291
At time: 63.85192394256592 and batch: 100, loss is 4.026685390472412 and perplexity is 56.07473701257551
At time: 64.55174660682678 and batch: 150, loss is 4.0380760669708256 and perplexity is 56.717117832076866
At time: 65.25136852264404 and batch: 200, loss is 3.9949119234085084 and perplexity is 54.3210559996506
At time: 65.95269393920898 and batch: 250, loss is 3.9847511434555054 and perplexity is 53.771906321917676
At time: 66.66659092903137 and batch: 300, loss is 3.954980745315552 and perplexity is 52.19468894249075
At time: 67.40296411514282 and batch: 350, loss is 4.003530111312866 and perplexity is 54.79122817339483
At time: 68.13670063018799 and batch: 400, loss is 4.00157247543335 and perplexity is 54.68407182002249
At time: 68.88438034057617 and batch: 450, loss is 3.9449501180648805 and perplexity is 51.67376046065177
At time: 69.624356508255 and batch: 500, loss is 3.9768645668029787 and perplexity is 53.34949792845828
At time: 70.35780167579651 and batch: 550, loss is 4.0417432832717894 and perplexity is 56.92549361714421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.535239686357214 and perplexity of 93.24586238725234
Finished 8 epochs...
Completing Train Step...
At time: 72.28376889228821 and batch: 50, loss is 3.9909095668792727 and perplexity is 54.104078267261926
At time: 73.02677702903748 and batch: 100, loss is 3.9657542848587037 and perplexity is 52.76005049185012
At time: 73.74869394302368 and batch: 150, loss is 3.977641839981079 and perplexity is 53.390981182084865
At time: 74.46663355827332 and batch: 200, loss is 3.9371181964874267 and perplexity is 51.270636300117935
At time: 75.17488622665405 and batch: 250, loss is 3.927434663772583 and perplexity is 50.776551520148224
At time: 75.88557958602905 and batch: 300, loss is 3.8966177797317503 and perplexity is 49.23564139009747
At time: 76.59677124023438 and batch: 350, loss is 3.9445124435424805 and perplexity is 51.65114912078618
At time: 77.30615258216858 and batch: 400, loss is 3.9427442264556887 and perplexity is 51.559899374813426
At time: 78.01638793945312 and batch: 450, loss is 3.8883053970336916 and perplexity is 48.82807217887923
At time: 78.7266366481781 and batch: 500, loss is 3.9183898067474363 and perplexity is 50.31935562408208
At time: 79.43669128417969 and batch: 550, loss is 3.9866006994247436 and perplexity is 53.87145250197069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.5327112725440495 and perplexity of 93.01039606436585
Finished 9 epochs...
Completing Train Step...
At time: 81.34399604797363 and batch: 50, loss is 3.93531409740448 and perplexity is 51.17822237918733
At time: 82.05957388877869 and batch: 100, loss is 3.910741534233093 and perplexity is 49.93596747692697
At time: 82.76126956939697 and batch: 150, loss is 3.92336537361145 and perplexity is 50.57034683652347
At time: 83.46309471130371 and batch: 200, loss is 3.885746088027954 and perplexity is 48.70326583114799
At time: 84.16266512870789 and batch: 250, loss is 3.8760422754287718 and perplexity is 48.232944114064225
At time: 84.86542987823486 and batch: 300, loss is 3.84419921875 and perplexity is 46.72125586122577
At time: 85.56701064109802 and batch: 350, loss is 3.8921599054336546 and perplexity is 49.016643584810055
At time: 86.27097535133362 and batch: 400, loss is 3.89037184715271 and perplexity is 48.92907727950301
At time: 86.97810816764832 and batch: 450, loss is 3.837168321609497 and perplexity is 46.39391561326086
At time: 87.68376159667969 and batch: 500, loss is 3.86680748462677 and perplexity is 47.78957333453867
At time: 88.39425873756409 and batch: 550, loss is 3.936464099884033 and perplexity is 51.23711131654442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.534402400889295 and perplexity of 93.16782165747556
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 90.29127144813538 and batch: 50, loss is 3.8993350315093993 and perplexity is 49.36960895355975
At time: 91.0120279788971 and batch: 100, loss is 3.8793450498580935 and perplexity is 48.39251000856352
At time: 91.7172839641571 and batch: 150, loss is 3.8759261322021485 and perplexity is 48.22734250960578
At time: 92.4241623878479 and batch: 200, loss is 3.8323290491104127 and perplexity is 46.16994517738281
At time: 93.13340997695923 and batch: 250, loss is 3.8061827516555784 and perplexity is 44.97841696235348
At time: 93.84380602836609 and batch: 300, loss is 3.756915488243103 and perplexity is 42.81615515421906
At time: 94.55158758163452 and batch: 350, loss is 3.7979526042938234 and perplexity is 44.60975710606559
At time: 95.26113748550415 and batch: 400, loss is 3.772038292884827 and perplexity is 43.468576288837866
At time: 95.97116160392761 and batch: 450, loss is 3.702752251625061 and perplexity is 40.55877885177998
At time: 96.67865896224976 and batch: 500, loss is 3.708754382133484 and perplexity is 40.80294997615483
At time: 97.38983345031738 and batch: 550, loss is 3.7569525337219236 and perplexity is 42.81774132856812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.491364174700798 and perplexity of 89.2431060644314
Finished 11 epochs...
Completing Train Step...
At time: 99.3092155456543 and batch: 50, loss is 3.844237208366394 and perplexity is 46.72303081752813
At time: 100.03425765037537 and batch: 100, loss is 3.8234149742126466 and perplexity is 45.7602117384668
At time: 100.74464344978333 and batch: 150, loss is 3.8265492486953736 and perplexity is 45.90386180421299
At time: 101.45479726791382 and batch: 200, loss is 3.7875744581222532 and perplexity is 44.149184604371115
At time: 102.16383624076843 and batch: 250, loss is 3.7670136737823485 and perplexity is 43.25071105333789
At time: 102.8754358291626 and batch: 300, loss is 3.7221533203125 and perplexity is 41.353345302463495
At time: 103.585853099823 and batch: 350, loss is 3.7661423254013062 and perplexity is 43.21304103052274
At time: 104.29599690437317 and batch: 400, loss is 3.7460528898239134 and perplexity is 42.3535774016269
At time: 105.00559997558594 and batch: 450, loss is 3.6853819608688356 and perplexity is 39.86034463440556
At time: 105.71497225761414 and batch: 500, loss is 3.6987555122375486 and perplexity is 40.39699949315524
At time: 106.42542171478271 and batch: 550, loss is 3.756357355117798 and perplexity is 42.79226470737434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.489571104658411 and perplexity of 89.08323030151291
Finished 12 epochs...
Completing Train Step...
At time: 108.327388048172 and batch: 50, loss is 3.8215913867950437 and perplexity is 45.67684003301439
At time: 109.05387949943542 and batch: 100, loss is 3.7998339128494263 and perplexity is 44.69376081744671
At time: 109.76596903800964 and batch: 150, loss is 3.8040268087387084 and perplexity is 44.88155051967228
At time: 110.47870373725891 and batch: 200, loss is 3.7660836601257324 and perplexity is 43.21050599992216
At time: 111.19093704223633 and batch: 250, loss is 3.7471918869018555 and perplexity is 42.401845485912986
At time: 111.90480899810791 and batch: 300, loss is 3.7040136766433718 and perplexity is 40.60997299213214
At time: 112.6152572631836 and batch: 350, loss is 3.7494898414611817 and perplexity is 42.499395039365105
At time: 113.32751560211182 and batch: 400, loss is 3.7320059823989866 and perplexity is 41.76279963515651
At time: 114.04067754745483 and batch: 450, loss is 3.675156831741333 and perplexity is 39.45484414391748
At time: 114.75246095657349 and batch: 500, loss is 3.6915670251846313 and perplexity is 40.10764743309619
At time: 115.46515893936157 and batch: 550, loss is 3.7532281160354612 and perplexity is 42.65856677572583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.490194117769282 and perplexity of 89.13874761416064
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 117.37393927574158 and batch: 50, loss is 3.81101601600647 and perplexity is 45.196335746940484
At time: 118.09000635147095 and batch: 100, loss is 3.793935933113098 and perplexity is 44.43093375803541
At time: 118.78823852539062 and batch: 150, loss is 3.7982889652252196 and perplexity is 44.62476460934254
At time: 119.48795104026794 and batch: 200, loss is 3.7575372838974 and perplexity is 42.842786332144975
At time: 120.18877100944519 and batch: 250, loss is 3.7363966035842897 and perplexity is 41.946567400097166
At time: 120.89042901992798 and batch: 300, loss is 3.6858462476730347 and perplexity is 39.87885556328763
At time: 121.5918276309967 and batch: 350, loss is 3.731160101890564 and perplexity is 41.72748823368657
At time: 122.29384970664978 and batch: 400, loss is 3.7083967781066893 and perplexity is 40.788361285580976
At time: 123.00054001808167 and batch: 450, loss is 3.6457988929748537 and perplexity is 38.31336890970769
At time: 123.71146392822266 and batch: 500, loss is 3.654984641075134 and perplexity is 38.66692722817057
At time: 124.4215178489685 and batch: 550, loss is 3.712165312767029 and perplexity is 40.94236363816602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4850476853390955 and perplexity of 88.68117950425257
Finished 14 epochs...
Completing Train Step...
At time: 126.29278135299683 and batch: 50, loss is 3.8021410131454467 and perplexity is 44.79699284378865
At time: 127.01274800300598 and batch: 100, loss is 3.78266592502594 and perplexity is 43.933007859958856
At time: 127.7139356136322 and batch: 150, loss is 3.7879955625534056 and perplexity is 44.167779936653396
At time: 128.4137725830078 and batch: 200, loss is 3.7482045698165893 and perplexity is 42.44480685984294
At time: 129.11732506752014 and batch: 250, loss is 3.728539614677429 and perplexity is 41.61828502962499
At time: 129.82523727416992 and batch: 300, loss is 3.6794589042663572 and perplexity is 39.6249473807656
At time: 130.5333287715912 and batch: 350, loss is 3.7250945663452146 and perplexity is 41.47515471318316
At time: 131.2418782711029 and batch: 400, loss is 3.704258761405945 and perplexity is 40.61992709747096
At time: 131.94941186904907 and batch: 450, loss is 3.643781657218933 and perplexity is 38.23615971277152
At time: 132.65691137313843 and batch: 500, loss is 3.6554041957855223 and perplexity is 38.683153523296646
At time: 133.36406064033508 and batch: 550, loss is 3.7147207069396972 and perplexity is 41.04712130718477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.484421101022274 and perplexity of 88.62563067280871
Finished 15 epochs...
Completing Train Step...
At time: 135.25893354415894 and batch: 50, loss is 3.7974047899246215 and perplexity is 44.5853259326012
At time: 135.9771625995636 and batch: 100, loss is 3.7771963357925413 and perplexity is 43.69336831554243
At time: 136.68083810806274 and batch: 150, loss is 3.7825003623962403 and perplexity is 43.92573479773721
At time: 137.38492035865784 and batch: 200, loss is 3.743113317489624 and perplexity is 42.229258808508
At time: 138.09047079086304 and batch: 250, loss is 3.72404381275177 and perplexity is 41.43159743332134
At time: 138.7975914478302 and batch: 300, loss is 3.6755969285964967 and perplexity is 39.47221191821729
At time: 139.50533962249756 and batch: 350, loss is 3.7216516065597536 and perplexity is 41.33260296419646
At time: 140.21322679519653 and batch: 400, loss is 3.7017707586288453 and perplexity is 40.5189902237259
At time: 140.92250442504883 and batch: 450, loss is 3.6425499296188355 and perplexity is 38.189092172676375
At time: 141.63434743881226 and batch: 500, loss is 3.6554873323440553 and perplexity is 38.68636964124041
At time: 142.34359574317932 and batch: 550, loss is 3.7160178136825563 and perplexity is 41.10039835054354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.484319159325133 and perplexity of 88.61659648609567
Finished 16 epochs...
Completing Train Step...
At time: 144.2469506263733 and batch: 50, loss is 3.79349139213562 and perplexity is 44.411186776808705
At time: 144.97474670410156 and batch: 100, loss is 3.7729106903076173 and perplexity is 43.50651470904845
At time: 145.6855685710907 and batch: 150, loss is 3.7783263158798217 and perplexity is 43.742768857242964
At time: 146.39855527877808 and batch: 200, loss is 3.7391912841796877 and perplexity is 42.06395861712747
At time: 147.11098337173462 and batch: 250, loss is 3.7205402278900146 and perplexity is 41.28669230768504
At time: 147.8226442337036 and batch: 300, loss is 3.672528791427612 and perplexity is 39.35129135299003
At time: 148.53455710411072 and batch: 350, loss is 3.7189827585220336 and perplexity is 41.222439597912405
At time: 149.2468090057373 and batch: 400, loss is 3.6997218179702758 and perplexity is 40.43605421171073
At time: 149.9583876132965 and batch: 450, loss is 3.641412601470947 and perplexity is 38.14568333293291
At time: 150.670316696167 and batch: 500, loss is 3.6552133655548094 and perplexity is 38.67577231248657
At time: 151.38139081001282 and batch: 550, loss is 3.716585536003113 and perplexity is 41.12373858883113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.484397076545878 and perplexity of 88.62350151401253
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 153.28866624832153 and batch: 50, loss is 3.7913882446289064 and perplexity is 44.31788165168201
At time: 153.99961042404175 and batch: 100, loss is 3.7711440038681032 and perplexity is 43.42972019536857
At time: 154.6937825679779 and batch: 150, loss is 3.777795925140381 and perplexity is 43.71957424936995
At time: 155.39541292190552 and batch: 200, loss is 3.737398748397827 and perplexity is 41.988625005448554
At time: 156.10028672218323 and batch: 250, loss is 3.717740569114685 and perplexity is 41.171265310757846
At time: 156.80512499809265 and batch: 300, loss is 3.669195499420166 and perplexity is 39.220340378207105
At time: 157.50914669036865 and batch: 350, loss is 3.71552460193634 and perplexity is 41.08013214947792
At time: 158.213693857193 and batch: 400, loss is 3.6949047946929934 and perplexity is 40.24174117826591
At time: 158.92408514022827 and batch: 450, loss is 3.6347456312179567 and perplexity is 37.89221307507039
At time: 159.63320994377136 and batch: 500, loss is 3.646818733215332 and perplexity is 38.352462356216705
At time: 160.34080362319946 and batch: 550, loss is 3.707540454864502 and perplexity is 40.75344821437201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.484011061648105 and perplexity of 88.5892981240691
Finished 18 epochs...
Completing Train Step...
At time: 162.24007177352905 and batch: 50, loss is 3.790224075317383 and perplexity is 44.266318154055774
At time: 162.96110320091248 and batch: 100, loss is 3.769818377494812 and perplexity is 43.372186755240946
At time: 163.66750645637512 and batch: 150, loss is 3.776058349609375 and perplexity is 43.64367414710476
At time: 164.3751392364502 and batch: 200, loss is 3.7362003564834594 and perplexity is 41.93833631554477
At time: 165.08144116401672 and batch: 250, loss is 3.7166019535064696 and perplexity is 41.12441374348962
At time: 165.78796529769897 and batch: 300, loss is 3.668305764198303 and perplexity is 39.18546017933092
At time: 166.497376203537 and batch: 350, loss is 3.7147440767288207 and perplexity is 41.04808058096281
At time: 167.20720648765564 and batch: 400, loss is 3.6943466472625732 and perplexity is 40.21928662089132
At time: 167.91692805290222 and batch: 450, loss is 3.634624009132385 and perplexity is 37.88760482532743
At time: 168.62593245506287 and batch: 500, loss is 3.6470687198638916 and perplexity is 38.362051158231445
At time: 169.33477091789246 and batch: 550, loss is 3.70808783531189 and perplexity is 40.77576196158551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.48389126392121 and perplexity of 88.57868596319568
Finished 19 epochs...
Completing Train Step...
At time: 171.25005459785461 and batch: 50, loss is 3.789402265548706 and perplexity is 44.22995460537975
At time: 171.97761797904968 and batch: 100, loss is 3.768833622932434 and perplexity is 43.329496819457766
At time: 172.68897938728333 and batch: 150, loss is 3.774918613433838 and perplexity is 43.59396020861454
At time: 173.40200853347778 and batch: 200, loss is 3.7352920961380005 and perplexity is 41.90026268071856
At time: 174.11430549621582 and batch: 250, loss is 3.7158253526687623 and perplexity is 41.092488887365725
At time: 174.82780694961548 and batch: 300, loss is 3.6676434326171874 and perplexity is 39.15951500463636
At time: 175.5409767627716 and batch: 350, loss is 3.714161353111267 and perplexity is 41.0241678628825
At time: 176.25212788581848 and batch: 400, loss is 3.6939655256271364 and perplexity is 40.20396110122733
At time: 176.96418261528015 and batch: 450, loss is 3.6345418977737425 and perplexity is 37.884493950340335
At time: 177.6759021282196 and batch: 500, loss is 3.647262358665466 and perplexity is 38.36948025910155
At time: 178.3885989189148 and batch: 550, loss is 3.7085080766677856 and perplexity is 40.79290122414116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4838269822140955 and perplexity of 88.57299215705375
Finished 20 epochs...
Completing Train Step...
At time: 180.29090023040771 and batch: 50, loss is 3.7886701583862306 and perplexity is 44.19758538913049
At time: 181.0110912322998 and batch: 100, loss is 3.76798059463501 and perplexity is 43.292551292587866
At time: 181.71325135231018 and batch: 150, loss is 3.7739925813674926 and perplexity is 43.553609489475804
At time: 182.42725706100464 and batch: 200, loss is 3.7345047855377196 and perplexity is 41.86728714245424
At time: 183.15989017486572 and batch: 250, loss is 3.715171427726746 and perplexity is 41.065626267978395
At time: 183.88822650909424 and batch: 300, loss is 3.667070302963257 and perplexity is 39.13707795563691
At time: 184.62677025794983 and batch: 350, loss is 3.7136610412597655 and perplexity is 41.00364811906624
At time: 185.36186385154724 and batch: 400, loss is 3.6936442756652834 and perplexity is 40.19104765459039
At time: 186.09572219848633 and batch: 450, loss is 3.6344523191452027 and perplexity is 37.881100461323655
At time: 186.8142433166504 and batch: 500, loss is 3.64739586353302 and perplexity is 38.37460311343704
At time: 187.53417563438416 and batch: 550, loss is 3.708833360671997 and perplexity is 40.806172660770756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483790620844415 and perplexity of 88.56977158029486
Finished 21 epochs...
Completing Train Step...
At time: 189.43496704101562 and batch: 50, loss is 3.7879885911941527 and perplexity is 44.16747202826532
At time: 190.15069246292114 and batch: 100, loss is 3.767206802368164 and perplexity is 43.25906480864765
At time: 190.85039615631104 and batch: 150, loss is 3.773181886672974 and perplexity is 43.51831511774853
At time: 191.55448651313782 and batch: 200, loss is 3.7337916135787963 and perplexity is 41.8374392118867
At time: 192.2594931125641 and batch: 250, loss is 3.7145786237716676 and perplexity is 41.041289616453305
At time: 192.96483778953552 and batch: 300, loss is 3.6665471458435057 and perplexity is 39.116608469503724
At time: 193.66909646987915 and batch: 350, loss is 3.7132079076766966 and perplexity is 40.98507219807983
At time: 194.3743016719818 and batch: 400, loss is 3.6933487606048585 and perplexity is 40.179172349466384
At time: 195.07916045188904 and batch: 450, loss is 3.6343500232696533 and perplexity is 37.8772255791808
At time: 195.78547883033752 and batch: 500, loss is 3.647482843399048 and perplexity is 38.3779410764404
At time: 196.48943758010864 and batch: 550, loss is 3.7090885829925537 and perplexity is 40.81658863598842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483772440159575 and perplexity of 88.5681613358291
Finished 22 epochs...
Completing Train Step...
At time: 198.3929295539856 and batch: 50, loss is 3.787342209815979 and perplexity is 44.13893222161871
At time: 199.1094834804535 and batch: 100, loss is 3.7664877223968505 and perplexity is 43.227969262997924
At time: 199.80973100662231 and batch: 150, loss is 3.772444944381714 and perplexity is 43.486256445040624
At time: 200.51019072532654 and batch: 200, loss is 3.733130464553833 and perplexity is 41.80978757167952
At time: 201.2105209827423 and batch: 250, loss is 3.714023389816284 and perplexity is 41.0185084239166
At time: 201.91612696647644 and batch: 300, loss is 3.666056742668152 and perplexity is 39.09743026341216
At time: 202.6218729019165 and batch: 350, loss is 3.712785563468933 and perplexity is 40.967766045066185
At time: 203.33230066299438 and batch: 400, loss is 3.6930659294128416 and perplexity is 40.16781003314101
At time: 204.04419136047363 and batch: 450, loss is 3.634236035346985 and perplexity is 37.87290827898531
At time: 204.757150888443 and batch: 500, loss is 3.647534646987915 and perplexity is 38.379929243018125
At time: 205.46738600730896 and batch: 550, loss is 3.7092908906936644 and perplexity is 40.82484698153783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4837659470578455 and perplexity of 88.56758625561461
Finished 23 epochs...
Completing Train Step...
At time: 207.38813591003418 and batch: 50, loss is 3.7867226362228394 and perplexity is 44.111593374873124
At time: 208.1152045726776 and batch: 100, loss is 3.765808844566345 and perplexity is 43.19863271210123
At time: 208.82689380645752 and batch: 150, loss is 3.771760096549988 and perplexity is 43.4564851721638
At time: 209.53938055038452 and batch: 200, loss is 3.7325080490112303 and perplexity is 41.783772606961044
At time: 210.25105261802673 and batch: 250, loss is 3.7134940958023073 and perplexity is 40.9968033176437
At time: 210.96246552467346 and batch: 300, loss is 3.665589942932129 and perplexity is 39.07918385232728
At time: 211.67416644096375 and batch: 350, loss is 3.712385754585266 and perplexity is 40.95139004211124
At time: 212.38692688941956 and batch: 400, loss is 3.6927898931503296 and perplexity is 40.15672379115896
At time: 213.09887647628784 and batch: 450, loss is 3.6341118669509886 and perplexity is 37.86820595265879
At time: 213.81086540222168 and batch: 500, loss is 3.647559413909912 and perplexity is 38.38087980750316
At time: 214.52255249023438 and batch: 550, loss is 3.709452357292175 and perplexity is 40.83143936292504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4837682196434505 and perplexity of 88.56778753326492
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 216.41263389587402 and batch: 50, loss is 3.786354546546936 and perplexity is 44.09535934073765
At time: 217.12205481529236 and batch: 100, loss is 3.7654957723617555 and perplexity is 43.18511053774171
At time: 217.81716108322144 and batch: 150, loss is 3.7714763641357423 and perplexity is 43.444156907758085
At time: 218.51795625686646 and batch: 200, loss is 3.732112169265747 and perplexity is 41.767234531456616
At time: 219.22283673286438 and batch: 250, loss is 3.7128325700759888 and perplexity is 40.96969184600894
At time: 219.9287257194519 and batch: 300, loss is 3.6650423622131347 and perplexity is 39.05779070250844
At time: 220.6379942893982 and batch: 350, loss is 3.7116922092437745 and perplexity is 40.922998242958
At time: 221.34634518623352 and batch: 400, loss is 3.6919500970840455 and perplexity is 40.12301448893514
At time: 222.05504059791565 and batch: 450, loss is 3.632953624725342 and perplexity is 37.8243707882846
At time: 222.76504611968994 and batch: 500, loss is 3.646064839363098 and perplexity is 38.323559566814865
At time: 223.47683191299438 and batch: 550, loss is 3.707809443473816 and perplexity is 40.764411902219415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.48375555809508 and perplexity of 88.56666613503832
Finished 25 epochs...
Completing Train Step...
At time: 225.39297342300415 and batch: 50, loss is 3.786225757598877 and perplexity is 44.08968071147407
At time: 226.11360359191895 and batch: 100, loss is 3.765366644859314 and perplexity is 43.179534512292186
At time: 226.82054090499878 and batch: 150, loss is 3.7713263273239135 and perplexity is 43.43763917392529
At time: 227.52738308906555 and batch: 200, loss is 3.7319961404800415 and perplexity is 41.762388611090216
At time: 228.23580884933472 and batch: 250, loss is 3.712722969055176 and perplexity is 40.96520177202308
At time: 228.946946144104 and batch: 300, loss is 3.6649489116668703 and perplexity is 39.05414090117203
At time: 229.6560595035553 and batch: 350, loss is 3.711608638763428 and perplexity is 40.91957843123726
At time: 230.36554193496704 and batch: 400, loss is 3.691890335083008 and perplexity is 40.1206167289498
At time: 231.07461428642273 and batch: 450, loss is 3.632933874130249 and perplexity is 37.82362374182985
At time: 231.7830879688263 and batch: 500, loss is 3.6460809564590453 and perplexity is 38.32417723627894
At time: 232.49181580543518 and batch: 550, loss is 3.7078570127487183 and perplexity is 40.766351081857735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483743870511968 and perplexity of 88.56563101081598
Finished 26 epochs...
Completing Train Step...
At time: 234.3911726474762 and batch: 50, loss is 3.7861074113845827 and perplexity is 44.08446317341646
At time: 235.11160159111023 and batch: 100, loss is 3.7652406454086305 and perplexity is 43.17409425740462
At time: 235.81709623336792 and batch: 150, loss is 3.7711853408813476 and perplexity is 43.43151548739323
At time: 236.52309465408325 and batch: 200, loss is 3.7318824100494385 and perplexity is 41.757639226730355
At time: 237.22796988487244 and batch: 250, loss is 3.712622537612915 and perplexity is 40.96108778431692
At time: 237.94000005722046 and batch: 300, loss is 3.6648602724075316 and perplexity is 39.05067932446651
At time: 238.65092396736145 and batch: 350, loss is 3.7115362548828124 and perplexity is 40.916616620552226
At time: 239.36229348182678 and batch: 400, loss is 3.691838788986206 and perplexity is 40.11854872105546
At time: 240.07263135910034 and batch: 450, loss is 3.6329185009002685 and perplexity is 37.82304227503289
At time: 240.78524899482727 and batch: 500, loss is 3.6460986852645876 and perplexity is 38.324856684187615
At time: 241.49946880340576 and batch: 550, loss is 3.7079015588760376 and perplexity is 40.76816710537148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483733806204288 and perplexity of 88.56473966354105
Finished 27 epochs...
Completing Train Step...
At time: 243.41680240631104 and batch: 50, loss is 3.785994501113892 and perplexity is 44.07948586574615
At time: 244.13715147972107 and batch: 100, loss is 3.765118498802185 and perplexity is 43.168821010365896
At time: 244.84202909469604 and batch: 150, loss is 3.771050877571106 and perplexity is 43.425675934663566
At time: 245.54729223251343 and batch: 200, loss is 3.7317714405059816 and perplexity is 41.753005657666804
At time: 246.2565667629242 and batch: 250, loss is 3.712527322769165 and perplexity is 40.95718786641168
At time: 246.96703386306763 and batch: 300, loss is 3.664775128364563 and perplexity is 39.047354533293245
At time: 247.67611002922058 and batch: 350, loss is 3.711468758583069 and perplexity is 40.913854993533164
At time: 248.38590908050537 and batch: 400, loss is 3.6917910528182984 and perplexity is 40.116633660986686
At time: 249.09646821022034 and batch: 450, loss is 3.632904472351074 and perplexity is 37.822511676345435
At time: 249.81061673164368 and batch: 500, loss is 3.6461159229278564 and perplexity is 38.32551732085586
At time: 250.52498984336853 and batch: 550, loss is 3.7079434490203855 and perplexity is 40.769874925546496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483726663792387 and perplexity of 88.56410709994948
Finished 28 epochs...
Completing Train Step...
At time: 252.4244363307953 and batch: 50, loss is 3.785885601043701 and perplexity is 44.07468586800618
At time: 253.1401698589325 and batch: 100, loss is 3.764999761581421 and perplexity is 43.16369556883183
At time: 253.83969521522522 and batch: 150, loss is 3.770921950340271 and perplexity is 43.42007754341847
At time: 254.53944897651672 and batch: 200, loss is 3.731663146018982 and perplexity is 41.74848428216285
At time: 255.24572348594666 and batch: 250, loss is 3.7124356603622437 and perplexity is 40.95343380404695
At time: 255.95232701301575 and batch: 300, loss is 3.6646927213668823 and perplexity is 39.04413689061874
At time: 256.65792202949524 and batch: 350, loss is 3.711403737068176 and perplexity is 40.91119479918725
At time: 257.36259269714355 and batch: 400, loss is 3.691745328903198 and perplexity is 40.114799413369866
At time: 258.0707883834839 and batch: 450, loss is 3.6328904914855955 and perplexity is 37.821982888594086
At time: 258.78094696998596 and batch: 500, loss is 3.646131839752197 and perplexity is 38.32612734623765
At time: 259.4928877353668 and batch: 550, loss is 3.707982792854309 and perplexity is 40.77147900028967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483720820000831 and perplexity of 88.56358955128043
Finished 29 epochs...
Completing Train Step...
At time: 261.4244830608368 and batch: 50, loss is 3.785779428482056 and perplexity is 44.070006594113515
At time: 262.1474401950836 and batch: 100, loss is 3.764884171485901 and perplexity is 43.1587065614835
At time: 262.85412073135376 and batch: 150, loss is 3.770797247886658 and perplexity is 43.41466329080505
At time: 263.55990266799927 and batch: 200, loss is 3.7315574073791504 and perplexity is 41.744070087599404
At time: 264.26660442352295 and batch: 250, loss is 3.7123463344573975 and perplexity is 40.949775764897105
At time: 264.973450422287 and batch: 300, loss is 3.6646127128601074 and perplexity is 39.04101315249228
At time: 265.6800458431244 and batch: 350, loss is 3.7113401794433596 and perplexity is 40.90859466344751
At time: 266.38760781288147 and batch: 400, loss is 3.6917010068893434 and perplexity is 40.11302148407549
At time: 267.09456062316895 and batch: 450, loss is 3.632876238822937 and perplexity is 37.821443828472425
At time: 267.8025050163269 and batch: 500, loss is 3.6461464977264404 and perplexity is 38.326689133742455
At time: 268.5085027217865 and batch: 550, loss is 3.7080201053619386 and perplexity is 40.77300031479279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4837165994847075 and perplexity of 88.56321576801157
Finished 30 epochs...
Completing Train Step...
At time: 270.4010841846466 and batch: 50, loss is 3.785675435066223 and perplexity is 44.065423841884176
At time: 271.12197184562683 and batch: 100, loss is 3.7647711181640626 and perplexity is 43.15382760213696
At time: 271.8280391693115 and batch: 150, loss is 3.770676403045654 and perplexity is 43.409417169712164
At time: 272.5336763858795 and batch: 200, loss is 3.731453847885132 and perplexity is 41.73974731665871
At time: 273.23883605003357 and batch: 250, loss is 3.7122590923309327 and perplexity is 40.94620337521482
At time: 273.9449963569641 and batch: 300, loss is 3.6645345306396484 and perplexity is 39.03796095871025
At time: 274.65101385116577 and batch: 350, loss is 3.711277527809143 and perplexity is 40.90603175342443
At time: 275.3623480796814 and batch: 400, loss is 3.6916573095321654 and perplexity is 40.11126868934474
At time: 276.07819628715515 and batch: 450, loss is 3.6328615140914917 and perplexity is 37.82088692196934
At time: 276.79274702072144 and batch: 500, loss is 3.6461596584320066 and perplexity is 38.32719354333266
At time: 277.5084068775177 and batch: 550, loss is 3.708055362701416 and perplexity is 40.77443788764874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.48371367758893 and perplexity of 88.5629569959034
Finished 31 epochs...
Completing Train Step...
At time: 279.4336488246918 and batch: 50, loss is 3.785573205947876 and perplexity is 44.06091930270669
At time: 280.1664369106293 and batch: 100, loss is 3.764660096168518 and perplexity is 43.14903684402587
At time: 280.8646924495697 and batch: 150, loss is 3.770558500289917 and perplexity is 43.4042993815095
At time: 281.56540608406067 and batch: 200, loss is 3.731352062225342 and perplexity is 41.73549902514986
At time: 282.2821342945099 and batch: 250, loss is 3.7121733236312866 and perplexity is 40.94269162319724
At time: 283.01105189323425 and batch: 300, loss is 3.664457902908325 and perplexity is 39.0349696829353
At time: 283.7459714412689 and batch: 350, loss is 3.711215624809265 and perplexity is 40.903499625719746
At time: 284.47864723205566 and batch: 400, loss is 3.6916143703460693 and perplexity is 40.109546381091455
At time: 285.2044641971588 and batch: 450, loss is 3.6328463840484617 and perplexity is 37.8203146946517
At time: 285.92971754074097 and batch: 500, loss is 3.6461715269088746 and perplexity is 38.32764843144205
At time: 286.66081643104553 and batch: 550, loss is 3.708088765144348 and perplexity is 40.77579987623012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483712054313497 and perplexity of 88.56281323394778
Finished 32 epochs...
Completing Train Step...
At time: 288.6330180168152 and batch: 50, loss is 3.785472469329834 and perplexity is 44.05648097826297
At time: 289.3774678707123 and batch: 100, loss is 3.7645511484146117 and perplexity is 43.14433610945037
At time: 290.10489535331726 and batch: 150, loss is 3.7704433822631835 and perplexity is 43.399303051802285
At time: 290.83155703544617 and batch: 200, loss is 3.7312519264221193 and perplexity is 41.731320016669756
At time: 291.56327199935913 and batch: 250, loss is 3.712088928222656 and perplexity is 40.93923639381207
At time: 292.2954297065735 and batch: 300, loss is 3.664382576942444 and perplexity is 39.03202944688024
At time: 293.0249047279358 and batch: 350, loss is 3.711154246330261 and perplexity is 40.90098910817345
At time: 293.7386019229889 and batch: 400, loss is 3.691571774482727 and perplexity is 40.1078379167221
At time: 294.44113421440125 and batch: 450, loss is 3.632830801010132 and perplexity is 37.819725343830115
At time: 295.1450798511505 and batch: 500, loss is 3.6461819982528687 and perplexity is 38.328049775534566
At time: 295.84917402267456 and batch: 550, loss is 3.7081205081939697 and perplexity is 40.77709424501245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483709781727892 and perplexity of 88.56261196760198
Finished 33 epochs...
Completing Train Step...
At time: 297.7342748641968 and batch: 50, loss is 3.7853728771209716 and perplexity is 44.05209351448986
At time: 298.4496719837189 and batch: 100, loss is 3.76444344997406 and perplexity is 43.13968978193836
At time: 299.1501476764679 and batch: 150, loss is 3.7703306770324705 and perplexity is 43.39441199896784
At time: 299.8517987728119 and batch: 200, loss is 3.7311533212661745 and perplexity is 41.727205296221385
At time: 300.55293226242065 and batch: 250, loss is 3.7120056200027465 and perplexity is 40.93582596096415
At time: 301.25387167930603 and batch: 300, loss is 3.664308576583862 and perplexity is 39.02914116957309
At time: 301.9586777687073 and batch: 350, loss is 3.7110932731628417 and perplexity is 40.89849532134475
At time: 302.6653742790222 and batch: 400, loss is 3.691529531478882 and perplexity is 40.1061436769559
At time: 303.3726763725281 and batch: 450, loss is 3.632814784049988 and perplexity is 37.81911959164779
At time: 304.08079981803894 and batch: 500, loss is 3.646191372871399 and perplexity is 38.32840908806443
At time: 304.79141306877136 and batch: 550, loss is 3.708150806427002 and perplexity is 40.77832973763279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.48370913241772 and perplexity of 88.56255446301579
Finished 34 epochs...
Completing Train Step...
At time: 306.67248272895813 and batch: 50, loss is 3.7852745342254637 and perplexity is 44.04776151707454
At time: 307.39417457580566 and batch: 100, loss is 3.7643372011184693 and perplexity is 43.13510648275794
At time: 308.1014769077301 and batch: 150, loss is 3.77021990776062 and perplexity is 43.38960549775957
At time: 308.8236355781555 and batch: 200, loss is 3.731056098937988 and perplexity is 41.72314867737394
At time: 309.53934621810913 and batch: 250, loss is 3.7119232273101805 and perplexity is 40.932453286984575
At time: 310.2574100494385 and batch: 300, loss is 3.664235200881958 and perplexity is 39.02627748400881
At time: 310.974924325943 and batch: 350, loss is 3.711032929420471 and perplexity is 40.896027427541455
At time: 311.6913537979126 and batch: 400, loss is 3.691487603187561 and perplexity is 40.10446213013251
At time: 312.4116427898407 and batch: 450, loss is 3.6327983570098876 and perplexity is 37.81849834055638
At time: 313.13141775131226 and batch: 500, loss is 3.6461996126174925 and perplexity is 38.328724905724606
At time: 313.8501889705658 and batch: 550, loss is 3.708179497718811 and perplexity is 40.7794997373751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.4837094570728055 and perplexity of 88.56258321530417
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 315.8698077201843 and batch: 50, loss is 3.785213031768799 and perplexity is 44.04505255483542
At time: 316.57884311676025 and batch: 100, loss is 3.7642883253097534 and perplexity is 43.13299827106525
At time: 317.27440786361694 and batch: 150, loss is 3.770166902542114 and perplexity is 43.3873056831909
At time: 317.97062969207764 and batch: 200, loss is 3.7309917974472047 and perplexity is 41.72046590296792
At time: 318.66648054122925 and batch: 250, loss is 3.711804599761963 and perplexity is 40.9275978584081
At time: 319.3626127243042 and batch: 300, loss is 3.6641448450088503 and perplexity is 39.02275138993665
At time: 320.0585298538208 and batch: 350, loss is 3.7109038496017455 and perplexity is 40.89074891641647
At time: 320.7544620037079 and batch: 400, loss is 3.6913439512252806 and perplexity is 40.09870145922705
At time: 321.4498972892761 and batch: 450, loss is 3.6326030015945436 and perplexity is 37.81111101370602
At time: 322.1475079059601 and batch: 500, loss is 3.645957183837891 and perplexity is 38.31943404595342
At time: 322.8458733558655 and batch: 550, loss is 3.7079156541824343 and perplexity is 40.76874174922794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708807762633 and perplexity of 88.56252571073664
Finished 36 epochs...
Completing Train Step...
At time: 324.69803762435913 and batch: 50, loss is 3.7851965713500975 and perplexity is 44.044327560795516
At time: 325.40711283683777 and batch: 100, loss is 3.7642696285247803 and perplexity is 43.132191830210274
At time: 326.10289430618286 and batch: 150, loss is 3.7701481199264526 and perplexity is 43.386490763756846
At time: 326.79829931259155 and batch: 200, loss is 3.7309756755828856 and perplexity is 41.71979329669916
At time: 327.49372601509094 and batch: 250, loss is 3.711791262626648 and perplexity is 40.927052005137405
At time: 328.18942618370056 and batch: 300, loss is 3.6641327857971193 and perplexity is 39.02228080915274
At time: 328.88484263420105 and batch: 350, loss is 3.7108942604064943 and perplexity is 40.89035680892114
At time: 329.58076214790344 and batch: 400, loss is 3.6913370752334593 and perplexity is 40.09842574183169
At time: 330.2777497768402 and batch: 450, loss is 3.6326013946533204 and perplexity is 37.811050253521856
At time: 330.9791328907013 and batch: 500, loss is 3.645959315299988 and perplexity is 38.319515722461716
At time: 331.67917037010193 and batch: 550, loss is 3.7079201412200926 and perplexity is 40.76892468051787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708483107547 and perplexity of 88.56249695846691
Finished 37 epochs...
Completing Train Step...
At time: 333.5392029285431 and batch: 50, loss is 3.785180253982544 and perplexity is 44.04360887917757
At time: 334.25425910949707 and batch: 100, loss is 3.7642512416839597 and perplexity is 43.13139877275579
At time: 334.9543459415436 and batch: 150, loss is 3.770129461288452 and perplexity is 43.38568123848392
At time: 335.6556031703949 and batch: 200, loss is 3.7309596014022826 and perplexity is 41.71912269059672
At time: 336.36244773864746 and batch: 250, loss is 3.711778092384338 and perplexity is 40.92651298949497
At time: 337.06894397735596 and batch: 300, loss is 3.6641206312179566 and perplexity is 39.02180651263398
At time: 337.7751476764679 and batch: 350, loss is 3.7108846426010134 and perplexity is 40.88996353531453
At time: 338.48192596435547 and batch: 400, loss is 3.6913302755355835 and perplexity is 40.09815308557834
At time: 339.1882517337799 and batch: 450, loss is 3.632599701881409 and perplexity is 37.81098624809221
At time: 339.89405727386475 and batch: 500, loss is 3.6459611892700194 and perplexity is 38.31958753215309
At time: 340.6002838611603 and batch: 550, loss is 3.7079246473312377 and perplexity is 40.76910839023765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483707833797373 and perplexity of 88.5624394539553
Finished 38 epochs...
Completing Train Step...
At time: 342.4493238925934 and batch: 50, loss is 3.7851637315750124 and perplexity is 44.042881178734206
At time: 343.1607143878937 and batch: 100, loss is 3.76423291683197 and perplexity is 43.13060840349889
At time: 343.85872745513916 and batch: 150, loss is 3.7701109313964842 and perplexity is 43.38487731394596
At time: 344.55643248558044 and batch: 200, loss is 3.7309434986114502 and perplexity is 41.71845090169918
At time: 345.2543194293976 and batch: 250, loss is 3.7117648363113402 and perplexity is 40.92597046824709
At time: 345.95209288597107 and batch: 300, loss is 3.66410870552063 and perplexity is 39.02134115315525
At time: 346.6502614021301 and batch: 350, loss is 3.7108749103546144 and perplexity is 40.88956558605063
At time: 347.34886264801025 and batch: 400, loss is 3.691323471069336 and perplexity is 40.09788023997737
At time: 348.0471568107605 and batch: 450, loss is 3.632597632408142 and perplexity is 37.81090799934795
At time: 348.7451403141022 and batch: 500, loss is 3.6459630346298217 and perplexity is 38.31965824564481
At time: 349.4442894458771 and batch: 550, loss is 3.707929139137268 and perplexity is 40.76929151757585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708807762633 and perplexity of 88.56252571073664
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 351.29182863235474 and batch: 50, loss is 3.785153212547302 and perplexity is 44.042417892883314
At time: 351.9956109523773 and batch: 100, loss is 3.7642238664627077 and perplexity is 43.13021805733271
At time: 352.68554162979126 and batch: 150, loss is 3.770101714134216 and perplexity is 43.384477425996224
At time: 353.3761377334595 and batch: 200, loss is 3.7309330463409425 and perplexity is 41.71801485144405
At time: 354.0667915344238 and batch: 250, loss is 3.711745777130127 and perplexity is 40.92519046019278
At time: 354.7611548900604 and batch: 300, loss is 3.6640944910049438 and perplexity is 39.02078648763149
At time: 355.45933055877686 and batch: 350, loss is 3.710854229927063 and perplexity is 40.88871998109569
At time: 356.1601228713989 and batch: 400, loss is 3.691300482749939 and perplexity is 40.096958467694314
At time: 356.8602397441864 and batch: 450, loss is 3.632566728591919 and perplexity is 37.8097395160513
At time: 357.5613236427307 and batch: 500, loss is 3.6459247064590454 and perplexity is 38.31818955138584
At time: 358.26117610931396 and batch: 550, loss is 3.7078877544403075 and perplexity is 40.767604327713265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708483107547 and perplexity of 88.56249695846691
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 360.10841369628906 and batch: 50, loss is 3.7851518964767457 and perplexity is 44.04235992999203
At time: 360.82337832450867 and batch: 100, loss is 3.7642226600646973 and perplexity is 43.13016602515485
At time: 361.52368664741516 and batch: 150, loss is 3.77010046005249 and perplexity is 43.384423018350006
At time: 362.22534680366516 and batch: 200, loss is 3.7309316396713257 and perplexity is 41.71795616802136
At time: 362.9268128871918 and batch: 250, loss is 3.7117430543899537 and perplexity is 40.92507903168431
At time: 363.62728667259216 and batch: 300, loss is 3.6640923261642455 and perplexity is 39.02070201393626
At time: 364.32841753959656 and batch: 350, loss is 3.7108511114120484 and perplexity is 40.88859246920732
At time: 365.0303370952606 and batch: 400, loss is 3.6912968587875366 and perplexity is 40.09681315808767
At time: 365.7317690849304 and batch: 450, loss is 3.632561807632446 and perplexity is 37.80955345631326
At time: 366.43406987190247 and batch: 500, loss is 3.6459186697006225 and perplexity is 38.31795823443052
At time: 367.13908219337463 and batch: 550, loss is 3.707880973815918 and perplexity is 40.76732789883824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708483107547 and perplexity of 88.56249695846691
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 369.00241923332214 and batch: 50, loss is 3.7851518106460573 and perplexity is 44.042356149806125
At time: 369.71721386909485 and batch: 100, loss is 3.764222583770752 and perplexity is 43.130162734584445
At time: 370.4174828529358 and batch: 150, loss is 3.770100464820862 and perplexity is 43.384423225223074
At time: 371.1188495159149 and batch: 200, loss is 3.7309315490722654 and perplexity is 41.717952388413906
At time: 371.82356238365173 and batch: 250, loss is 3.711742835044861 and perplexity is 40.92507005497004
At time: 372.52886986732483 and batch: 300, loss is 3.664092230796814 and perplexity is 39.0206982926323
At time: 373.234899520874 and batch: 350, loss is 3.710850868225098 and perplexity is 40.888582525636416
At time: 373.94143557548523 and batch: 400, loss is 3.691296615600586 and perplexity is 40.096803407067135
At time: 374.64699959754944 and batch: 450, loss is 3.632561373710632 and perplexity is 37.809537049926796
At time: 375.353319644928 and batch: 500, loss is 3.6459180688858033 and perplexity is 38.31793521244029
At time: 376.0589473247528 and batch: 550, loss is 3.7078804922103883 and perplexity is 40.76730826507242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.483708807762633 and perplexity of 88.56252571073664
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 377.9078965187073 and batch: 50, loss is 3.785151786804199 and perplexity is 44.04235509975452
At time: 378.6205475330353 and batch: 100, loss is 3.7642226362228395 and perplexity is 43.130164996851576
At time: 379.318922996521 and batch: 150, loss is 3.770100431442261 and perplexity is 43.38442177711173
At time: 380.017920255661 and batch: 200, loss is 3.7309315252304076 and perplexity is 41.71795139378043
At time: 380.7167465686798 and batch: 250, loss is 3.7117429256439207 and perplexity is 40.925073762743075
At time: 381.4153616428375 and batch: 300, loss is 3.664092230796814 and perplexity is 39.0206982926323
At time: 382.11352944374084 and batch: 350, loss is 3.710850839614868 and perplexity is 40.88858135580469
At time: 382.81189703941345 and batch: 400, loss is 3.6912966203689574 and perplexity is 40.09680359826359
At time: 383.51138377189636 and batch: 450, loss is 3.632561402320862 and perplexity is 37.80953813166635
At time: 384.2120099067688 and batch: 500, loss is 3.6459181118011474 and perplexity is 38.3179368568677
At time: 384.9119143486023 and batch: 550, loss is 3.707880401611328 and perplexity is 40.76730457159277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.48370913241772 and perplexity of 88.56255446301579
Annealing...
Model not improving. Stopping early with 88.5624394539553loss at 42 epochs.
Finished Training.
Improved accuracyfrom -91.89716131547962 to -88.5624394539553
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23cfa37a20>
SETTINGS FOR THIS RUN
{'lr': 21.983735712465226, 'seq_len': 35, 'dropout': 0.18839212092800361, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.6444654761443784, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.919623851776123 and batch: 50, loss is 6.39034631729126 and perplexity is 596.0629708610268
At time: 1.5973269939422607 and batch: 100, loss is 5.525476760864258 and perplexity is 251.0059793260697
At time: 2.2629990577697754 and batch: 150, loss is 5.420643100738525 and perplexity is 226.0244322519274
At time: 2.940859794616699 and batch: 200, loss is 5.313978233337402 and perplexity is 203.1568281698297
At time: 3.604508638381958 and batch: 250, loss is 5.276630620956421 and perplexity is 195.70934422376973
At time: 4.2698938846588135 and batch: 300, loss is 5.26167251586914 and perplexity is 192.80368901124368
At time: 4.934698820114136 and batch: 350, loss is 5.28794264793396 and perplexity is 197.93578262947315
At time: 5.6010801792144775 and batch: 400, loss is 5.287702579498291 and perplexity is 197.88827019911992
At time: 6.269465446472168 and batch: 450, loss is 5.209556283950806 and perplexity is 183.0128344430591
At time: 6.9377360343933105 and batch: 500, loss is 5.225480737686158 and perplexity is 185.95054247553014
At time: 7.604992866516113 and batch: 550, loss is 5.264608707427978 and perplexity is 193.37062949104705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2144950704371675 and perplexity of 183.91893142463806
Finished 1 epochs...
Completing Train Step...
At time: 9.382442712783813 and batch: 50, loss is 5.148408632278443 and perplexity is 172.15730663038133
At time: 10.064429998397827 and batch: 100, loss is 5.155658006668091 and perplexity is 173.40987407969783
At time: 10.729503154754639 and batch: 150, loss is 5.122329788208008 and perplexity is 167.72568008584147
At time: 11.394219636917114 and batch: 200, loss is 5.065899209976196 and perplexity is 158.52292339794212
At time: 12.058907508850098 and batch: 250, loss is 5.047813901901245 and perplexity is 155.68175661951582
At time: 12.724231719970703 and batch: 300, loss is 5.031935815811157 and perplexity is 153.22934957390535
At time: 13.390129327774048 and batch: 350, loss is 5.094031476974488 and perplexity is 163.04585446392085
At time: 14.062485218048096 and batch: 400, loss is 5.091002292633057 and perplexity is 162.55270581082507
At time: 14.75187349319458 and batch: 450, loss is 5.0450413799285885 and perplexity is 155.25072332928883
At time: 15.45267653465271 and batch: 500, loss is 5.096797189712524 and perplexity is 163.49741661825743
At time: 16.168806076049805 and batch: 550, loss is 5.079829301834106 and perplexity is 160.74661443431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.2187136386303195 and perplexity of 184.69644482243677
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 18.070419549942017 and batch: 50, loss is 4.999357080459594 and perplexity is 148.31777204892222
At time: 18.800475120544434 and batch: 100, loss is 4.935384874343872 and perplexity is 139.12667851821362
At time: 19.516125917434692 and batch: 150, loss is 4.909388074874878 and perplexity is 135.55643863290192
At time: 20.246520280838013 and batch: 200, loss is 4.8456839656829835 and perplexity is 127.19024601362763
At time: 20.961772680282593 and batch: 250, loss is 4.824191904067993 and perplexity is 124.48583123262421
At time: 21.818028450012207 and batch: 300, loss is 4.795429668426514 and perplexity is 120.95634174051087
At time: 22.528608322143555 and batch: 350, loss is 4.842108421325683 and perplexity is 126.73628371200361
At time: 23.23872423171997 and batch: 400, loss is 4.812013721466064 and perplexity is 122.97901382004639
At time: 23.949345588684082 and batch: 450, loss is 4.743903493881225 and perplexity is 114.88176785382338
At time: 24.659844398498535 and batch: 500, loss is 4.777142000198364 and perplexity is 118.76443580479997
At time: 25.371537685394287 and batch: 550, loss is 4.811991529464722 and perplexity is 122.97628469988904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.045026089282746 and perplexity of 155.2483494636106
Finished 3 epochs...
Completing Train Step...
At time: 27.254692316055298 and batch: 50, loss is 4.839218339920044 and perplexity is 126.37053431237615
At time: 27.95242691040039 and batch: 100, loss is 4.827576351165772 and perplexity is 124.90786070822698
At time: 28.663078784942627 and batch: 150, loss is 4.795964050292969 and perplexity is 121.02099588963358
At time: 29.361312866210938 and batch: 200, loss is 4.74301965713501 and perplexity is 114.78027598365297
At time: 30.05829930305481 and batch: 250, loss is 4.717098541259766 and perplexity is 111.8432727965592
At time: 30.755771160125732 and batch: 300, loss is 4.690200138092041 and perplexity is 108.87496765535164
At time: 31.453465700149536 and batch: 350, loss is 4.746899404525757 and perplexity is 115.22645943978064
At time: 32.148457288742065 and batch: 400, loss is 4.736315689086914 and perplexity is 114.01336622245299
At time: 32.841827630996704 and batch: 450, loss is 4.6779068565368656 and perplexity is 107.54473026763458
At time: 33.538458585739136 and batch: 500, loss is 4.736352872848511 and perplexity is 114.01760574710171
At time: 34.23939609527588 and batch: 550, loss is 4.768679904937744 and perplexity is 117.7636800524577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0273284912109375 and perplexity of 152.52499606074014
Finished 4 epochs...
Completing Train Step...
At time: 36.108144760131836 and batch: 50, loss is 4.757093877792358 and perplexity is 116.40714048542577
At time: 36.81138300895691 and batch: 100, loss is 4.7508230400085445 and perplexity is 115.67945417655476
At time: 37.51389288902283 and batch: 150, loss is 4.7265846729278564 and perplexity is 112.90928096153642
At time: 38.23059940338135 and batch: 200, loss is 4.685457191467285 and perplexity is 108.35980216195294
At time: 38.93334245681763 and batch: 250, loss is 4.663254823684692 and perplexity is 105.98046913423401
At time: 39.63567352294922 and batch: 300, loss is 4.633074111938477 and perplexity is 102.82968870294253
At time: 40.3385648727417 and batch: 350, loss is 4.688257570266724 and perplexity is 108.6636759368926
At time: 41.041393518447876 and batch: 400, loss is 4.668430757522583 and perplexity is 106.53043910685626
At time: 41.74536848068237 and batch: 450, loss is 4.617483148574829 and perplexity is 101.23890791967416
At time: 42.44839262962341 and batch: 500, loss is 4.675879764556885 and perplexity is 107.32694801432349
At time: 43.15129470825195 and batch: 550, loss is 4.708086214065552 and perplexity is 110.83983308293028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.027446016352227 and perplexity of 152.5429226358434
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 45.012020111083984 and batch: 50, loss is 4.662365217208862 and perplexity is 105.88623014660256
At time: 45.70413780212402 and batch: 100, loss is 4.6234307289123535 and perplexity is 101.84282861098221
At time: 46.39715886116028 and batch: 150, loss is 4.584547395706177 and perplexity is 97.95884051529374
At time: 47.09275245666504 and batch: 200, loss is 4.5339178943634035 and perplexity is 93.12269217352699
At time: 47.78980851173401 and batch: 250, loss is 4.493267288208008 and perplexity is 89.41310753975013
At time: 48.49607014656067 and batch: 300, loss is 4.473501615524292 and perplexity is 87.66314885053174
At time: 49.201441526412964 and batch: 350, loss is 4.5076141357421875 and perplexity is 90.7051499671361
At time: 49.906349420547485 and batch: 400, loss is 4.485820388793945 and perplexity is 88.74973023931973
At time: 50.612387895584106 and batch: 450, loss is 4.43157925605774 and perplexity is 84.0640708285406
At time: 51.31940698623657 and batch: 500, loss is 4.471521768569946 and perplexity is 87.48976092968228
At time: 52.04086661338806 and batch: 550, loss is 4.512393140792847 and perplexity is 91.13966779139652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.931211430975732 and perplexity of 138.54725114909658
Finished 6 epochs...
Completing Train Step...
At time: 53.89751696586609 and batch: 50, loss is 4.575114707946778 and perplexity is 97.03916966245028
At time: 54.61137247085571 and batch: 100, loss is 4.565279560089111 and perplexity is 96.08945303805245
At time: 55.31232953071594 and batch: 150, loss is 4.539483852386475 and perplexity is 93.6424543144423
At time: 56.01228332519531 and batch: 200, loss is 4.501154232025146 and perplexity is 90.12109194208979
At time: 56.71159553527832 and batch: 250, loss is 4.464619531631469 and perplexity is 86.88796512783269
At time: 57.41217565536499 and batch: 300, loss is 4.442619285583496 and perplexity is 84.99728251636071
At time: 58.11258006095886 and batch: 350, loss is 4.476656188964844 and perplexity is 87.94012533324842
At time: 58.81362009048462 and batch: 400, loss is 4.460037889480591 and perplexity is 86.49078612520874
At time: 59.5136194229126 and batch: 450, loss is 4.4087663173675535 and perplexity is 82.16803167610291
At time: 60.213401794433594 and batch: 500, loss is 4.450742101669311 and perplexity is 85.69051148413256
At time: 60.92488765716553 and batch: 550, loss is 4.497370691299438 and perplexity is 89.78075935757195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.928963194502161 and perplexity of 138.23611405169075
Finished 7 epochs...
Completing Train Step...
At time: 62.7934992313385 and batch: 50, loss is 4.537550773620605 and perplexity is 93.46161092299765
At time: 63.52410554885864 and batch: 100, loss is 4.533283996582031 and perplexity is 93.063680611188
At time: 64.23133563995361 and batch: 150, loss is 4.509330921173095 and perplexity is 90.86100499364744
At time: 64.9392409324646 and batch: 200, loss is 4.471559791564942 and perplexity is 87.49308761566913
At time: 65.64961886405945 and batch: 250, loss is 4.440059823989868 and perplexity is 84.78001340078372
At time: 66.35644197463989 and batch: 300, loss is 4.416807622909546 and perplexity is 82.83143365884412
At time: 67.06328272819519 and batch: 350, loss is 4.451729593276977 and perplexity is 85.77517193894441
At time: 67.7706253528595 and batch: 400, loss is 4.437279376983643 and perplexity is 84.54461447510835
At time: 68.47962641716003 and batch: 450, loss is 4.388748359680176 and perplexity is 80.53954932010909
At time: 69.18844532966614 and batch: 500, loss is 4.42488127708435 and perplexity is 83.50289292661239
At time: 69.93229866027832 and batch: 550, loss is 4.475607395172119 and perplexity is 87.84794262444369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9271509698096745 and perplexity of 137.98582600999646
Finished 8 epochs...
Completing Train Step...
At time: 71.86342859268188 and batch: 50, loss is 4.509060564041138 and perplexity is 90.83644339328212
At time: 72.59776902198792 and batch: 100, loss is 4.50623197555542 and perplexity is 90.57986752026645
At time: 73.30967426300049 and batch: 150, loss is 4.484442777633667 and perplexity is 88.62755179698591
At time: 74.02669858932495 and batch: 200, loss is 4.445844449996948 and perplexity is 85.27185526029237
At time: 74.7478380203247 and batch: 250, loss is 4.416874170303345 and perplexity is 82.83694605829463
At time: 75.445974111557 and batch: 300, loss is 4.393622608184814 and perplexity is 80.93307739563421
At time: 76.14972758293152 and batch: 350, loss is 4.431397790908814 and perplexity is 84.04881751342278
At time: 76.85575675964355 and batch: 400, loss is 4.420114412307739 and perplexity is 83.1057931400094
At time: 77.56306433677673 and batch: 450, loss is 4.367545804977417 and perplexity is 78.84988106025564
At time: 78.27038073539734 and batch: 500, loss is 4.407510995864868 and perplexity is 82.06494909353452
At time: 78.97939276695251 and batch: 550, loss is 4.457586507797242 and perplexity is 86.27902385743987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.931277660613365 and perplexity of 138.55642738720132
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 80.85179281234741 and batch: 50, loss is 4.4725336170196535 and perplexity is 87.57833211139379
At time: 81.56598114967346 and batch: 100, loss is 4.451385908126831 and perplexity is 85.74569735137689
At time: 82.26490092277527 and batch: 150, loss is 4.417703876495361 and perplexity is 82.90570490624334
At time: 82.96569061279297 and batch: 200, loss is 4.3783721828460695 and perplexity is 79.70817740435865
At time: 83.6655023097992 and batch: 250, loss is 4.3405419540405275 and perplexity is 76.74912256626031
At time: 84.36570525169373 and batch: 300, loss is 4.312615003585815 and perplexity is 74.63540582896154
At time: 85.07151246070862 and batch: 350, loss is 4.344406270980835 and perplexity is 77.0462792848323
At time: 85.77141070365906 and batch: 400, loss is 4.330267343521118 and perplexity is 75.96459249637564
At time: 86.46991872787476 and batch: 450, loss is 4.272193212509155 and perplexity is 71.6786699069697
At time: 87.17141556739807 and batch: 500, loss is 4.311217451095581 and perplexity is 74.531171784925
At time: 87.8940749168396 and batch: 550, loss is 4.3584678363800045 and perplexity is 78.13732349712538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8816093282496675 and perplexity of 131.84267138081242
Finished 10 epochs...
Completing Train Step...
At time: 89.7608916759491 and batch: 50, loss is 4.433230037689209 and perplexity is 84.20295685624298
At time: 90.47917938232422 and batch: 100, loss is 4.422583427429199 and perplexity is 83.31123611649515
At time: 91.1784017086029 and batch: 150, loss is 4.392760419845581 and perplexity is 80.86332791296158
At time: 91.87757873535156 and batch: 200, loss is 4.357803440093994 and perplexity is 78.08542659155978
At time: 92.57573962211609 and batch: 250, loss is 4.323820056915284 and perplexity is 75.47640243833625
At time: 93.27450370788574 and batch: 300, loss is 4.299084911346435 and perplexity is 73.63238270283446
At time: 93.97273755073547 and batch: 350, loss is 4.334929819107056 and perplexity is 76.31960252395835
At time: 94.67040920257568 and batch: 400, loss is 4.328802118301391 and perplexity is 75.85336876345164
At time: 95.36915898323059 and batch: 450, loss is 4.271204242706299 and perplexity is 71.60781690843568
At time: 96.06965255737305 and batch: 500, loss is 4.308166151046753 and perplexity is 74.30410142289867
At time: 96.77085947990417 and batch: 550, loss is 4.354862051010132 and perplexity is 77.85608442779443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8799380038646945 and perplexity of 131.62250354633898
Finished 11 epochs...
Completing Train Step...
At time: 98.63387060165405 and batch: 50, loss is 4.418577632904053 and perplexity is 82.97817595363264
At time: 99.35755062103271 and batch: 100, loss is 4.407745504379273 and perplexity is 82.08419627955745
At time: 100.06063318252563 and batch: 150, loss is 4.3792018699646 and perplexity is 79.77433769476765
At time: 100.76317739486694 and batch: 200, loss is 4.3465009212493895 and perplexity is 77.20783343508043
At time: 101.46980118751526 and batch: 250, loss is 4.312874383926392 and perplexity is 74.65476729682804
At time: 102.17696332931519 and batch: 300, loss is 4.28979998588562 and perplexity is 72.95187563549445
At time: 102.88472604751587 and batch: 350, loss is 4.328784713745117 and perplexity is 75.85204858071502
At time: 103.59104657173157 and batch: 400, loss is 4.326370716094971 and perplexity is 75.66916274548282
At time: 104.29819536209106 and batch: 450, loss is 4.267954092025757 and perplexity is 71.37545851801175
At time: 105.00744080543518 and batch: 500, loss is 4.3025569820404055 and perplexity is 73.88848388436692
At time: 105.7319803237915 and batch: 550, loss is 4.348915185928345 and perplexity is 77.39445877111172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.879501667428524 and perplexity of 131.56508438016994
Finished 12 epochs...
Completing Train Step...
At time: 107.61097931861877 and batch: 50, loss is 4.407021379470825 and perplexity is 82.02477858395451
At time: 108.33585166931152 and batch: 100, loss is 4.396103229522705 and perplexity is 81.13409093051715
At time: 109.04447197914124 and batch: 150, loss is 4.367910432815552 and perplexity is 78.87863716424323
At time: 109.7540910243988 and batch: 200, loss is 4.336478290557861 and perplexity is 76.43787279504062
At time: 110.46337127685547 and batch: 250, loss is 4.303690128326416 and perplexity is 73.97225780054646
At time: 111.17524290084839 and batch: 300, loss is 4.281589365005493 and perplexity is 72.35534772561871
At time: 111.88497996330261 and batch: 350, loss is 4.3223404216766355 and perplexity is 75.36480747388279
At time: 112.59500861167908 and batch: 400, loss is 4.322867469787598 and perplexity is 75.40453882254135
At time: 113.303884267807 and batch: 450, loss is 4.264456262588501 and perplexity is 71.12623546208023
At time: 114.0137677192688 and batch: 500, loss is 4.296410722732544 and perplexity is 73.43573887193163
At time: 114.72462391853333 and batch: 550, loss is 4.3425592517852785 and perplexity is 76.90410466808892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.880021764876995 and perplexity of 131.63352884221604
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 116.60844278335571 and batch: 50, loss is 4.394538059234619 and perplexity is 81.00720158965893
At time: 117.32706260681152 and batch: 100, loss is 4.3784770488739015 and perplexity is 79.71653652259488
At time: 118.02844429016113 and batch: 150, loss is 4.34267562866211 and perplexity is 76.91305504840426
At time: 118.73238205909729 and batch: 200, loss is 4.309618434906006 and perplexity is 74.41209046646368
At time: 119.43407320976257 and batch: 250, loss is 4.270973625183106 and perplexity is 71.59130479512353
At time: 120.13582801818848 and batch: 300, loss is 4.245974712371826 and perplexity is 69.82378509331576
At time: 120.83709335327148 and batch: 350, loss is 4.287064323425293 and perplexity is 72.752576659565
At time: 121.54171681404114 and batch: 400, loss is 4.2852292728424075 and perplexity is 72.61919442036675
At time: 122.24917888641357 and batch: 450, loss is 4.219555597305298 and perplexity is 68.00325674277057
At time: 122.95320630073547 and batch: 500, loss is 4.2494595336914065 and perplexity is 70.0675329704498
At time: 123.67071175575256 and batch: 550, loss is 4.299565620422364 and perplexity is 73.66778696637557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.861337539997507 and perplexity of 129.1968926028202
Finished 14 epochs...
Completing Train Step...
At time: 125.52549958229065 and batch: 50, loss is 4.378313055038452 and perplexity is 79.70346457391061
At time: 126.23729062080383 and batch: 100, loss is 4.366441926956177 and perplexity is 78.76288843303414
At time: 126.9342851638794 and batch: 150, loss is 4.333903579711914 and perplexity is 76.24132051613725
At time: 127.63122391700745 and batch: 200, loss is 4.301746587753296 and perplexity is 73.82862933531192
At time: 128.33512544631958 and batch: 250, loss is 4.2647679424285885 and perplexity is 71.148407530889
At time: 129.03951478004456 and batch: 300, loss is 4.241762857437134 and perplexity is 69.53031589818885
At time: 129.74569630622864 and batch: 350, loss is 4.284157733917237 and perplexity is 72.54142180246599
At time: 130.451016664505 and batch: 400, loss is 4.280817575454712 and perplexity is 72.29952616831919
At time: 131.1557912826538 and batch: 450, loss is 4.217505578994751 and perplexity is 67.86399161807154
At time: 131.86069345474243 and batch: 500, loss is 4.249715528488159 and perplexity is 70.08547219038704
At time: 132.56605410575867 and batch: 550, loss is 4.300079765319825 and perplexity is 73.70567262167519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.860301890271775 and perplexity of 129.06315913889654
Finished 15 epochs...
Completing Train Step...
At time: 134.43884778022766 and batch: 50, loss is 4.372477607727051 and perplexity is 79.23971361897206
At time: 135.15633463859558 and batch: 100, loss is 4.360918769836426 and perplexity is 78.32906775770618
At time: 135.85990691184998 and batch: 150, loss is 4.3286397171020505 and perplexity is 75.84105108562031
At time: 136.56344556808472 and batch: 200, loss is 4.296840696334839 and perplexity is 73.46732109038516
At time: 137.26669478416443 and batch: 250, loss is 4.260671644210816 and perplexity is 70.85755854471017
At time: 137.97007036209106 and batch: 300, loss is 4.238874197006226 and perplexity is 69.32975623974166
At time: 138.6733057498932 and batch: 350, loss is 4.281915102005005 and perplexity is 72.37892037851971
At time: 139.3764832019806 and batch: 400, loss is 4.277321643829346 and perplexity is 72.04721325959784
At time: 140.08198595046997 and batch: 450, loss is 4.215450172424316 and perplexity is 67.72464677801122
At time: 140.78830432891846 and batch: 500, loss is 4.24886923789978 and perplexity is 70.02618460569118
At time: 141.5123586654663 and batch: 550, loss is 4.299088296890258 and perplexity is 73.63263198891488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8597142645653255 and perplexity of 128.98734058747752
Finished 16 epochs...
Completing Train Step...
At time: 143.38050651550293 and batch: 50, loss is 4.367709341049195 and perplexity is 78.86277691450432
At time: 144.0951211452484 and batch: 100, loss is 4.356219816207886 and perplexity is 77.96186650702572
At time: 144.79628443717957 and batch: 150, loss is 4.324205484390259 and perplexity is 75.5054987244421
At time: 145.49763417243958 and batch: 200, loss is 4.293095865249634 and perplexity is 73.19271288421237
At time: 146.19804739952087 and batch: 250, loss is 4.257025194168091 and perplexity is 70.59965050748902
At time: 146.89887356758118 and batch: 300, loss is 4.236311531066894 and perplexity is 69.15231469365578
At time: 147.59952402114868 and batch: 350, loss is 4.279574213027954 and perplexity is 72.20968751658492
At time: 148.29994225502014 and batch: 400, loss is 4.273993701934814 and perplexity is 71.80784284654275
At time: 149.0015652179718 and batch: 450, loss is 4.21325514793396 and perplexity is 67.57615255358449
At time: 149.70242428779602 and batch: 500, loss is 4.24727499961853 and perplexity is 69.91463512335658
At time: 150.40360760688782 and batch: 550, loss is 4.297572231292724 and perplexity is 73.52108466659507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.859889253656915 and perplexity of 129.0099139400211
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 152.26451420783997 and batch: 50, loss is 4.36238205909729 and perplexity is 78.44376974199366
At time: 152.97737622261047 and batch: 100, loss is 4.348282566070557 and perplexity is 77.34551298328142
At time: 153.67616319656372 and batch: 150, loss is 4.315811491012573 and perplexity is 74.8743586666965
At time: 154.37525534629822 and batch: 200, loss is 4.283667135238647 and perplexity is 72.50584180525016
At time: 155.0742700099945 and batch: 250, loss is 4.243957977294922 and perplexity is 69.68311091566954
At time: 155.77278065681458 and batch: 300, loss is 4.223486127853394 and perplexity is 68.27107160322235
At time: 156.47146558761597 and batch: 350, loss is 4.265763254165649 and perplexity is 71.21925762909589
At time: 157.1712052822113 and batch: 400, loss is 4.255261182785034 and perplexity is 70.47522169953658
At time: 157.8725197315216 and batch: 450, loss is 4.194432802200318 and perplexity is 66.31610655816624
At time: 158.57355761528015 and batch: 500, loss is 4.226724801063537 and perplexity is 68.492537728475
At time: 159.28895020484924 and batch: 550, loss is 4.278815994262695 and perplexity is 72.15495752775067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.854403232006317 and perplexity of 128.3041005828881
Finished 18 epochs...
Completing Train Step...
At time: 161.1472578048706 and batch: 50, loss is 4.355219898223877 and perplexity is 77.88394999619062
At time: 161.86425232887268 and batch: 100, loss is 4.343000049591065 and perplexity is 76.93801130111827
At time: 162.56739020347595 and batch: 150, loss is 4.3124753284454345 and perplexity is 74.62498184617758
At time: 163.27138805389404 and batch: 200, loss is 4.280240659713745 and perplexity is 72.25782746308448
At time: 163.97399759292603 and batch: 250, loss is 4.241487774848938 and perplexity is 69.51119194938673
At time: 164.67647647857666 and batch: 300, loss is 4.221275124549866 and perplexity is 68.12029078823703
At time: 165.3787145614624 and batch: 350, loss is 4.264848518371582 and perplexity is 71.15414061188939
At time: 166.08168840408325 and batch: 400, loss is 4.254114742279053 and perplexity is 70.3944723467209
At time: 166.78473567962646 and batch: 450, loss is 4.1943322277069095 and perplexity is 66.30943718473439
At time: 167.49063372612 and batch: 500, loss is 4.2277216053009035 and perplexity is 68.56084541935257
At time: 168.19643259048462 and batch: 550, loss is 4.279534759521485 and perplexity is 72.20683864741066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.853517572930518 and perplexity of 128.19051719724737
Finished 19 epochs...
Completing Train Step...
At time: 170.0582413673401 and batch: 50, loss is 4.351615209579467 and perplexity is 77.60370800207473
At time: 170.7749810218811 and batch: 100, loss is 4.339769983291626 and perplexity is 76.6898973516432
At time: 171.47851276397705 and batch: 150, loss is 4.310562181472778 and perplexity is 74.48234976964109
At time: 172.18118333816528 and batch: 200, loss is 4.278166265487671 and perplexity is 72.10809160230308
At time: 172.8833131790161 and batch: 250, loss is 4.239877939224243 and perplexity is 69.39938037954309
At time: 173.58846354484558 and batch: 300, loss is 4.219970331192017 and perplexity is 68.03146584698041
At time: 174.2938277721405 and batch: 350, loss is 4.264226198196411 and perplexity is 71.10987373015512
At time: 174.9991490840912 and batch: 400, loss is 4.253153142929077 and perplexity is 70.32681360338572
At time: 175.70588970184326 and batch: 450, loss is 4.193973035812378 and perplexity is 66.28562364943713
At time: 176.4137260913849 and batch: 500, loss is 4.228135299682617 and perplexity is 68.58921452357303
At time: 177.13553261756897 and batch: 550, loss is 4.279629926681519 and perplexity is 72.2137106941712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.853083509079953 and perplexity of 128.1348864022788
Finished 20 epochs...
Completing Train Step...
At time: 178.99614071846008 and batch: 50, loss is 4.3487834453582765 and perplexity is 77.38426345257638
At time: 179.71152424812317 and batch: 100, loss is 4.337332391738892 and perplexity is 76.50318636069669
At time: 180.41362166404724 and batch: 150, loss is 4.309053106307983 and perplexity is 74.37003507236291
At time: 181.1138470172882 and batch: 200, loss is 4.276447639465332 and perplexity is 71.98427119062211
At time: 181.81384944915771 and batch: 250, loss is 4.238541693687439 and perplexity is 69.30670769777196
At time: 182.51661276817322 and batch: 300, loss is 4.218898429870605 and perplexity is 67.95858189802053
At time: 183.21752834320068 and batch: 350, loss is 4.263738713264465 and perplexity is 71.07521718615692
At time: 183.91913318634033 and batch: 400, loss is 4.252101058959961 and perplexity is 70.25286279824412
At time: 184.62022066116333 and batch: 450, loss is 4.193420906066894 and perplexity is 66.24903548655836
At time: 185.3209388256073 and batch: 500, loss is 4.228159923553466 and perplexity is 68.5909034763273
At time: 186.02172565460205 and batch: 550, loss is 4.279316501617432 and perplexity is 72.19108065386506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.852927349983378 and perplexity of 128.1148785364242
Finished 21 epochs...
Completing Train Step...
At time: 187.88613557815552 and batch: 50, loss is 4.346328630447387 and perplexity is 77.19453238139457
At time: 188.59883499145508 and batch: 100, loss is 4.335146951675415 and perplexity is 76.33617579450382
At time: 189.2969241142273 and batch: 150, loss is 4.30767894744873 and perplexity is 74.26790901458928
At time: 189.99536275863647 and batch: 200, loss is 4.274883680343628 and perplexity is 71.87177872281427
At time: 190.69398999214172 and batch: 250, loss is 4.2371988773345945 and perplexity is 69.2137039747487
At time: 191.39457821846008 and batch: 300, loss is 4.217821521759033 and perplexity is 67.88543614262139
At time: 192.09568762779236 and batch: 350, loss is 4.2632829952239994 and perplexity is 71.04283430674677
At time: 192.7968738079071 and batch: 400, loss is 4.251126127243042 and perplexity is 70.18440443063712
At time: 193.4984200000763 and batch: 450, loss is 4.192951269149781 and perplexity is 66.21792979853214
At time: 194.2025134563446 and batch: 500, loss is 4.228118810653687 and perplexity is 68.58808356335467
At time: 194.9196846485138 and batch: 550, loss is 4.278893384933472 and perplexity is 72.1605418643986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.852783203125 and perplexity of 128.09641251011354
Finished 22 epochs...
Completing Train Step...
At time: 196.7817690372467 and batch: 50, loss is 4.3441670131683345 and perplexity is 77.02784756564363
At time: 197.49873065948486 and batch: 100, loss is 4.333281564712524 and perplexity is 76.19391201713019
At time: 198.20224380493164 and batch: 150, loss is 4.306495923995971 and perplexity is 74.18010028657096
At time: 198.9048454761505 and batch: 200, loss is 4.273559093475342 and perplexity is 71.77664133129754
At time: 199.6080448627472 and batch: 250, loss is 4.235876812934875 and perplexity is 69.1222594618378
At time: 200.31390643119812 and batch: 300, loss is 4.216759243011475 and perplexity is 67.81336117516908
At time: 201.01979970932007 and batch: 350, loss is 4.262600150108337 and perplexity is 70.99433961340418
At time: 201.72580671310425 and batch: 400, loss is 4.250042667388916 and perplexity is 70.10840362539616
At time: 202.431884765625 and batch: 450, loss is 4.192342586517334 and perplexity is 66.17763635890972
At time: 203.13750839233398 and batch: 500, loss is 4.227862615585327 and perplexity is 68.57051388532601
At time: 203.84309244155884 and batch: 550, loss is 4.278368396759033 and perplexity is 72.12266837569541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.852803981050532 and perplexity of 128.09907411548485
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 205.6963882446289 and batch: 50, loss is 4.341947870254517 and perplexity is 76.85710128881817
At time: 206.40805792808533 and batch: 100, loss is 4.3300132560729985 and perplexity is 75.94529329886653
At time: 207.10728430747986 and batch: 150, loss is 4.303319492340088 and perplexity is 73.94484610001079
At time: 207.80752325057983 and batch: 200, loss is 4.268994359970093 and perplexity is 71.44974675265559
At time: 208.50762605667114 and batch: 250, loss is 4.230803260803222 and perplexity is 68.77245220803303
At time: 209.20764827728271 and batch: 300, loss is 4.211415395736695 and perplexity is 67.45194347044216
At time: 209.90780329704285 and batch: 350, loss is 4.2563379812240605 and perplexity is 70.55115018075114
At time: 210.6105329990387 and batch: 400, loss is 4.242541227340698 and perplexity is 69.58445727174364
At time: 211.31369304656982 and batch: 450, loss is 4.184376821517945 and perplexity is 65.65257489398833
At time: 212.01647782325745 and batch: 500, loss is 4.219880647659302 and perplexity is 68.02536481837247
At time: 212.7335169315338 and batch: 550, loss is 4.2702045822143555 and perplexity is 71.53626917063967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.851094347365359 and perplexity of 127.88025872371686
Finished 24 epochs...
Completing Train Step...
At time: 214.58895349502563 and batch: 50, loss is 4.3396954917907715 and perplexity is 76.68418481885928
At time: 215.30148577690125 and batch: 100, loss is 4.328552227020264 and perplexity is 75.83441603611281
At time: 215.99998474121094 and batch: 150, loss is 4.301901617050171 and perplexity is 73.84007582305453
At time: 216.69908046722412 and batch: 200, loss is 4.267872896194458 and perplexity is 71.36966336359738
At time: 217.39740252494812 and batch: 250, loss is 4.22992235660553 and perplexity is 68.71189694180735
At time: 218.09562706947327 and batch: 300, loss is 4.210894327163697 and perplexity is 67.41680553794329
At time: 218.79563212394714 and batch: 350, loss is 4.255815558433532 and perplexity is 70.5143022779289
At time: 219.49670004844666 and batch: 400, loss is 4.242036781311035 and perplexity is 69.54936452049033
At time: 220.19855642318726 and batch: 450, loss is 4.184324884414673 and perplexity is 65.64916517797218
At time: 220.8997220993042 and batch: 500, loss is 4.220056042671204 and perplexity is 68.0372971744517
At time: 221.60079550743103 and batch: 550, loss is 4.270452842712403 and perplexity is 71.55403100514212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.85082098778258 and perplexity of 127.84530620706676
Finished 25 epochs...
Completing Train Step...
At time: 223.4602255821228 and batch: 50, loss is 4.338543272018432 and perplexity is 76.59587866870106
At time: 224.17952394485474 and batch: 100, loss is 4.3275946998596195 and perplexity is 75.76183727665871
At time: 224.88561415672302 and batch: 150, loss is 4.3009558773040775 and perplexity is 73.77027534024216
At time: 225.59120273590088 and batch: 200, loss is 4.267088270187378 and perplexity is 71.31368683279844
At time: 226.29657363891602 and batch: 250, loss is 4.2293587493896485 and perplexity is 68.67318133209739
At time: 227.00226497650146 and batch: 300, loss is 4.210520148277283 and perplexity is 67.39158431164097
At time: 227.70808959007263 and batch: 350, loss is 4.25540530204773 and perplexity is 70.4853792684593
At time: 228.41370820999146 and batch: 400, loss is 4.24159948348999 and perplexity is 69.5189573839023
At time: 229.118980884552 and batch: 450, loss is 4.184230756759644 and perplexity is 65.64298606681652
At time: 229.82474637031555 and batch: 500, loss is 4.220078530311585 and perplexity is 68.0388271899262
At time: 230.54373288154602 and batch: 550, loss is 4.270507154464721 and perplexity is 71.55791733548722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850657686274102 and perplexity of 127.82443058026844
Finished 26 epochs...
Completing Train Step...
At time: 232.39725947380066 and batch: 50, loss is 4.337560968399048 and perplexity is 76.52067520222064
At time: 233.10734915733337 and batch: 100, loss is 4.326805477142334 and perplexity is 75.70206790235306
At time: 233.80372881889343 and batch: 150, loss is 4.3001680660247805 and perplexity is 73.71218117187749
At time: 234.50076413154602 and batch: 200, loss is 4.26641396522522 and perplexity is 71.26561586896348
At time: 235.20064187049866 and batch: 250, loss is 4.228806180953979 and perplexity is 68.63524518184114
At time: 235.90100741386414 and batch: 300, loss is 4.2101793336868285 and perplexity is 67.36862018990989
At time: 236.60141015052795 and batch: 350, loss is 4.255032830238342 and perplexity is 70.45913034050412
At time: 237.30287504196167 and batch: 400, loss is 4.2410520362854 and perplexity is 69.48090984047735
At time: 238.00801467895508 and batch: 450, loss is 4.184010210037232 and perplexity is 65.62851031773835
At time: 238.71336388587952 and batch: 500, loss is 4.220039010047913 and perplexity is 68.03613833066827
At time: 239.41813945770264 and batch: 550, loss is 4.270392007827759 and perplexity is 71.549678156324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850409974443152 and perplexity of 127.79277087793251
Finished 27 epochs...
Completing Train Step...
At time: 241.27152037620544 and batch: 50, loss is 4.3365845870971675 and perplexity is 76.44599830824001
At time: 241.98549962043762 and batch: 100, loss is 4.325943145751953 and perplexity is 75.63681577140808
At time: 242.68586421012878 and batch: 150, loss is 4.29939380645752 and perplexity is 73.65513089909129
At time: 243.38411498069763 and batch: 200, loss is 4.265783243179321 and perplexity is 71.22068124603864
At time: 244.08224415779114 and batch: 250, loss is 4.228178534507752 and perplexity is 68.59218003037518
At time: 244.7801969051361 and batch: 300, loss is 4.209776811599731 and perplexity is 67.34150828924166
At time: 245.47954940795898 and batch: 350, loss is 4.254694132804871 and perplexity is 70.43527005483004
At time: 246.18509674072266 and batch: 400, loss is 4.240600538253784 and perplexity is 69.44954642724196
At time: 246.89481616020203 and batch: 450, loss is 4.183859386444092 and perplexity is 65.61861273641294
At time: 247.60417199134827 and batch: 500, loss is 4.219970960617065 and perplexity is 68.03150866770254
At time: 248.32802534103394 and batch: 550, loss is 4.270287818908692 and perplexity is 71.54222386103152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850322317569814 and perplexity of 127.78156945415026
Finished 28 epochs...
Completing Train Step...
At time: 250.1978256702423 and batch: 50, loss is 4.3357900047302245 and perplexity is 76.38527979208666
At time: 250.92078351974487 and batch: 100, loss is 4.325309772491455 and perplexity is 75.588924602883
At time: 251.62250566482544 and batch: 150, loss is 4.298741865158081 and perplexity is 73.60712772666812
At time: 252.32270908355713 and batch: 200, loss is 4.265225963592529 and perplexity is 71.1810024713353
At time: 253.02328062057495 and batch: 250, loss is 4.22770628452301 and perplexity is 68.5597950219142
At time: 253.7247016429901 and batch: 300, loss is 4.209456906318665 and perplexity is 67.31996883058194
At time: 254.42668557167053 and batch: 350, loss is 4.254348340034485 and perplexity is 70.41091825824518
At time: 255.12834811210632 and batch: 400, loss is 4.240168418884277 and perplexity is 69.41954241615132
At time: 255.82959914207458 and batch: 450, loss is 4.183683671951294 and perplexity is 65.607083608105
At time: 256.52956104278564 and batch: 500, loss is 4.219878540039063 and perplexity is 68.02522144688788
At time: 257.22956585884094 and batch: 550, loss is 4.270184803009033 and perplexity is 71.53485425407675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850272645341589 and perplexity of 127.77522241650645
Finished 29 epochs...
Completing Train Step...
At time: 259.0855646133423 and batch: 50, loss is 4.335056467056274 and perplexity is 76.32926885720092
At time: 259.80310320854187 and batch: 100, loss is 4.324703283309937 and perplexity is 75.54309463696285
At time: 260.50667095184326 and batch: 150, loss is 4.298127899169922 and perplexity is 73.5619493241783
At time: 261.21050357818604 and batch: 200, loss is 4.2647009944915775 and perplexity is 71.14364445122415
At time: 261.9131202697754 and batch: 250, loss is 4.227256121635437 and perplexity is 68.52893889227903
At time: 262.6165542602539 and batch: 300, loss is 4.209132075309753 and perplexity is 67.29810476844176
At time: 263.319278717041 and batch: 350, loss is 4.254001913070678 and perplexity is 70.38653024219118
At time: 264.023845911026 and batch: 400, loss is 4.239732446670533 and perplexity is 69.38928402095605
At time: 264.72966504096985 and batch: 450, loss is 4.183499913215638 and perplexity is 65.59502884098937
At time: 265.435777425766 and batch: 500, loss is 4.219763698577881 and perplexity is 68.01740977961995
At time: 266.1574172973633 and batch: 550, loss is 4.270053434371948 and perplexity is 71.52545743500649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.85023433604139 and perplexity of 127.77032753091325
Finished 30 epochs...
Completing Train Step...
At time: 268.0185458660126 and batch: 50, loss is 4.33436017036438 and perplexity is 76.27613953883177
At time: 268.7327661514282 and batch: 100, loss is 4.324128437042236 and perplexity is 75.49968145010993
At time: 269.43279552459717 and batch: 150, loss is 4.297526588439942 and perplexity is 73.51772903113219
At time: 270.1331400871277 and batch: 200, loss is 4.264179725646972 and perplexity is 71.10656914981934
At time: 270.8334324359894 and batch: 250, loss is 4.226806678771973 and perplexity is 68.49814597010099
At time: 271.53482246398926 and batch: 300, loss is 4.208824400901794 and perplexity is 67.27740204891217
At time: 272.2352304458618 and batch: 350, loss is 4.25365038394928 and perplexity is 70.36179167547493
At time: 272.9366502761841 and batch: 400, loss is 4.2392966938018795 and perplexity is 69.35905402827149
At time: 273.637642621994 and batch: 450, loss is 4.183307375907898 and perplexity is 65.58240056648121
At time: 274.3386392593384 and batch: 500, loss is 4.219630784988404 and perplexity is 68.00836994231109
At time: 275.03955030441284 and batch: 550, loss is 4.269897060394287 and perplexity is 71.51427358917738
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.85019700070645 and perplexity of 127.76555727198964
Finished 31 epochs...
Completing Train Step...
At time: 276.8857660293579 and batch: 50, loss is 4.333684062957763 and perplexity is 76.22458610573429
At time: 277.59293603897095 and batch: 100, loss is 4.323537940979004 and perplexity is 75.45511234567377
At time: 278.29189467430115 and batch: 150, loss is 4.296949939727783 and perplexity is 73.47534734821143
At time: 278.9931495189667 and batch: 200, loss is 4.263680953979492 and perplexity is 71.07111205098889
At time: 279.69461822509766 and batch: 250, loss is 4.226359333992004 and perplexity is 68.46751053487566
At time: 280.3961691856384 and batch: 300, loss is 4.2085135507583615 and perplexity is 67.25649210893276
At time: 281.09773898124695 and batch: 350, loss is 4.253292384147644 and perplexity is 70.33660667639371
At time: 281.8027627468109 and batch: 400, loss is 4.2388483333587645 and perplexity is 69.32796314255583
At time: 282.5071213245392 and batch: 450, loss is 4.183090538978576 and perplexity is 65.5681814217984
At time: 283.2114279270172 and batch: 500, loss is 4.2194656658172605 and perplexity is 67.99714138368607
At time: 283.9295349121094 and batch: 550, loss is 4.269715785980225 and perplexity is 71.50131105605868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850153821579953 and perplexity of 127.76004058593398
Finished 32 epochs...
Completing Train Step...
At time: 285.7917671203613 and batch: 50, loss is 4.3330169296264645 and perplexity is 76.17375110242666
At time: 286.511198759079 and batch: 100, loss is 4.322970266342163 and perplexity is 75.41229054774941
At time: 287.21759843826294 and batch: 150, loss is 4.296347274780273 and perplexity is 73.4310796724866
At time: 287.9239637851715 and batch: 200, loss is 4.2631484889984135 and perplexity is 71.03327924587123
At time: 288.6301386356354 and batch: 250, loss is 4.225892086029052 and perplexity is 68.43552670282192
At time: 289.33744263648987 and batch: 300, loss is 4.208198657035828 and perplexity is 67.23531679592956
At time: 290.045946598053 and batch: 350, loss is 4.25292444229126 and perplexity is 70.31073165530498
At time: 290.7543144226074 and batch: 400, loss is 4.238398017883301 and perplexity is 69.29675071611716
At time: 291.46247267723083 and batch: 450, loss is 4.182788543701172 and perplexity is 65.54838313030673
At time: 292.170108795166 and batch: 500, loss is 4.219235563278199 and perplexity is 67.98149686879506
At time: 292.8775670528412 and batch: 550, loss is 4.269482345581054 and perplexity is 71.48462170952422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.85013044641373 and perplexity of 127.75705420865229
Finished 33 epochs...
Completing Train Step...
At time: 294.7426793575287 and batch: 50, loss is 4.332349681854248 and perplexity is 76.12294128995444
At time: 295.46195363998413 and batch: 100, loss is 4.322384471893311 and perplexity is 75.36812738310253
At time: 296.1677243709564 and batch: 150, loss is 4.295740528106689 and perplexity is 73.38653912293924
At time: 296.87295150756836 and batch: 200, loss is 4.262599658966065 and perplexity is 70.99430474509144
At time: 297.5791485309601 and batch: 250, loss is 4.225454225540161 and perplexity is 68.40556804897633
At time: 298.28409790992737 and batch: 300, loss is 4.207891659736633 and perplexity is 67.21467890331347
At time: 298.98985052108765 and batch: 350, loss is 4.252571806907654 and perplexity is 70.28594197459228
At time: 299.6944420337677 and batch: 400, loss is 4.237967195510865 and perplexity is 69.26690255566082
At time: 300.3997070789337 and batch: 450, loss is 4.182564458847046 and perplexity is 65.53369637603585
At time: 301.10511541366577 and batch: 500, loss is 4.2190433645248415 and perplexity is 67.96843216539561
At time: 301.82425260543823 and batch: 550, loss is 4.269303464889527 and perplexity is 71.47183563458428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850111941073803 and perplexity of 127.75469004281099
Finished 34 epochs...
Completing Train Step...
At time: 303.67772912979126 and batch: 50, loss is 4.331733388900757 and perplexity is 76.07604171106281
At time: 304.3842351436615 and batch: 100, loss is 4.321831636428833 and perplexity is 75.326472724541
At time: 305.07666850090027 and batch: 150, loss is 4.295256023406982 and perplexity is 73.35099161200237
At time: 305.76987624168396 and batch: 200, loss is 4.262092523574829 and perplexity is 70.9583101484174
At time: 306.4670684337616 and batch: 250, loss is 4.225016236305237 and perplexity is 68.37561370688056
At time: 307.1644866466522 and batch: 300, loss is 4.20757878780365 and perplexity is 67.19365260624073
At time: 307.8665962219238 and batch: 350, loss is 4.252226767539978 and perplexity is 70.26169474098245
At time: 308.5700123310089 and batch: 400, loss is 4.237526607513428 and perplexity is 69.23639111175673
At time: 309.27349877357483 and batch: 450, loss is 4.1823467445373534 and perplexity is 65.51943030558793
At time: 309.97711205482483 and batch: 500, loss is 4.218868079185486 and perplexity is 67.9565193398004
At time: 310.68035197257996 and batch: 550, loss is 4.26913519859314 and perplexity is 71.45981034525988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850091487803358 and perplexity of 127.75207706830692
Finished 35 epochs...
Completing Train Step...
At time: 312.55021047592163 and batch: 50, loss is 4.331126260757446 and perplexity is 76.02986782326008
At time: 313.2694981098175 and batch: 100, loss is 4.321300745010376 and perplexity is 75.28649315993182
At time: 313.97464776039124 and batch: 150, loss is 4.294727001190186 and perplexity is 73.31219757017533
At time: 314.6830141544342 and batch: 200, loss is 4.261638603210449 and perplexity is 70.92610803556938
At time: 315.3867497444153 and batch: 250, loss is 4.224604215621948 and perplexity is 68.34744734276153
At time: 316.09209656715393 and batch: 300, loss is 4.207266368865967 and perplexity is 67.17266331557113
At time: 316.79747223854065 and batch: 350, loss is 4.251878089904785 and perplexity is 70.23720032999039
At time: 317.5035948753357 and batch: 400, loss is 4.2371057033538815 and perplexity is 69.20725535885575
At time: 318.21074414253235 and batch: 450, loss is 4.1821246910095216 and perplexity is 65.50488310013597
At time: 318.9156939983368 and batch: 500, loss is 4.218681964874268 and perplexity is 67.94387283589484
At time: 319.6426341533661 and batch: 550, loss is 4.268958597183228 and perplexity is 71.44719155628164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.850061944190492 and perplexity of 127.7483028661513
Finished 36 epochs...
Completing Train Step...
At time: 321.5083603858948 and batch: 50, loss is 4.330528211593628 and perplexity is 75.98441181820091
At time: 322.2263605594635 and batch: 100, loss is 4.320781917572021 and perplexity is 75.24744259267182
At time: 322.9280593395233 and batch: 150, loss is 4.294271717071533 and perplexity is 73.27882728797428
At time: 323.6296281814575 and batch: 200, loss is 4.261190729141235 and perplexity is 70.89434918346404
At time: 324.33141231536865 and batch: 250, loss is 4.224189257621765 and perplexity is 68.31909190625903
At time: 325.03288292884827 and batch: 300, loss is 4.206917133331299 and perplexity is 67.14920833047856
At time: 325.7414286136627 and batch: 350, loss is 4.251528172492981 and perplexity is 70.21262741013552
At time: 326.4478394985199 and batch: 400, loss is 4.236683320999146 and perplexity is 69.1780296080277
At time: 327.1551365852356 and batch: 450, loss is 4.181910133361816 and perplexity is 65.49083003415508
At time: 327.86471223831177 and batch: 500, loss is 4.218508081436157 and perplexity is 67.93205954878485
At time: 328.57553482055664 and batch: 550, loss is 4.268759908676148 and perplexity is 71.43299723062765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849973313351895 and perplexity of 127.73698092868273
Finished 37 epochs...
Completing Train Step...
At time: 330.44507241249084 and batch: 50, loss is 4.32992075920105 and perplexity is 75.93826892166864
At time: 331.16262555122375 and batch: 100, loss is 4.320201711654663 and perplexity is 75.20379624437268
At time: 331.86540484428406 and batch: 150, loss is 4.293747339248657 and perplexity is 73.24041156911392
At time: 332.56808257102966 and batch: 200, loss is 4.260737390518188 and perplexity is 70.8622173206809
At time: 333.27104020118713 and batch: 250, loss is 4.223709697723389 and perplexity is 68.28633666416536
At time: 333.97652196884155 and batch: 300, loss is 4.206604213714599 and perplexity is 67.12819931318437
At time: 334.69018387794495 and batch: 350, loss is 4.251181316375733 and perplexity is 70.18827795393327
At time: 335.4047417640686 and batch: 400, loss is 4.236241855621338 and perplexity is 69.14749664317011
At time: 336.1205542087555 and batch: 450, loss is 4.181678342819214 and perplexity is 65.47565163829825
At time: 336.83541917800903 and batch: 500, loss is 4.218320565223694 and perplexity is 67.91932238052352
At time: 337.56477761268616 and batch: 550, loss is 4.268583698272705 and perplexity is 71.42041110230245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.84996552162982 and perplexity of 127.73598564150618
Finished 38 epochs...
Completing Train Step...
At time: 339.4471480846405 and batch: 50, loss is 4.329323749542237 and perplexity is 75.89294657193527
At time: 340.174076795578 and batch: 100, loss is 4.319695816040039 and perplexity is 75.16576059549477
At time: 340.8837490081787 and batch: 150, loss is 4.293326539993286 and perplexity is 73.20959854196666
At time: 341.5931067466736 and batch: 200, loss is 4.260320854187012 and perplexity is 70.83270677919215
At time: 342.3043293952942 and batch: 250, loss is 4.223308343887329 and perplexity is 68.25893518020789
At time: 343.01488876342773 and batch: 300, loss is 4.206271543502807 and perplexity is 67.10587147501087
At time: 343.72609519958496 and batch: 350, loss is 4.2508792352676394 and perplexity is 70.16707860327666
At time: 344.4359166622162 and batch: 400, loss is 4.2358106517791745 and perplexity is 69.11768640454821
At time: 345.1460802555084 and batch: 450, loss is 4.181449689865112 and perplexity is 65.46068214860368
At time: 345.85580110549927 and batch: 500, loss is 4.218133039474488 and perplexity is 67.90658695285616
At time: 346.56661438941956 and batch: 550, loss is 4.2684123325347905 and perplexity is 71.40817313946546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849914875436337 and perplexity of 127.72951646388357
Finished 39 epochs...
Completing Train Step...
At time: 348.4442265033722 and batch: 50, loss is 4.32874701499939 and perplexity is 75.84918910752234
At time: 349.16810965538025 and batch: 100, loss is 4.319200477600098 and perplexity is 75.1285373247239
At time: 349.8793053627014 and batch: 150, loss is 4.292828121185303 and perplexity is 73.17311859302997
At time: 350.5876808166504 and batch: 200, loss is 4.259891223907471 and perplexity is 70.80228143986947
At time: 351.29916644096375 and batch: 250, loss is 4.222933778762817 and perplexity is 68.23337255138658
At time: 352.0093514919281 and batch: 300, loss is 4.205962743759155 and perplexity is 67.08515239828927
At time: 352.7185266017914 and batch: 350, loss is 4.2505919456481935 and perplexity is 70.14692322532116
At time: 353.4280574321747 and batch: 400, loss is 4.235358991622925 and perplexity is 69.08647574834262
At time: 354.13887310028076 and batch: 450, loss is 4.181184902191162 and perplexity is 65.44335126144624
At time: 354.8506305217743 and batch: 500, loss is 4.217942805290222 and perplexity is 67.89367002734026
At time: 355.5763511657715 and batch: 550, loss is 4.268222227096557 and perplexity is 71.39459934768405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849865852518285 and perplexity of 127.72325494374537
Finished 40 epochs...
Completing Train Step...
At time: 357.44433212280273 and batch: 50, loss is 4.328194313049316 and perplexity is 75.8072786958411
At time: 358.161669254303 and batch: 100, loss is 4.318686933517456 and perplexity is 75.08996541397921
At time: 358.8624656200409 and batch: 150, loss is 4.292405347824097 and perplexity is 73.14218948619441
At time: 359.5629622936249 and batch: 200, loss is 4.259478454589844 and perplexity is 70.77306246123688
At time: 360.2634937763214 and batch: 250, loss is 4.222583017349243 and perplexity is 68.20944311418448
At time: 360.96543431282043 and batch: 300, loss is 4.205637679100037 and perplexity is 67.06334893005351
At time: 361.665470123291 and batch: 350, loss is 4.250288743972778 and perplexity is 70.12565778469552
At time: 362.37161588668823 and batch: 400, loss is 4.234955682754516 and perplexity is 69.05861817796642
At time: 363.08236050605774 and batch: 450, loss is 4.180968704223633 and perplexity is 65.42920407126724
At time: 363.7936120033264 and batch: 500, loss is 4.217773404121399 and perplexity is 67.88216973439098
At time: 364.5075912475586 and batch: 550, loss is 4.268061208724975 and perplexity is 71.38310443102856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.84978696133228 and perplexity of 127.7131791021353
Finished 41 epochs...
Completing Train Step...
At time: 366.3804979324341 and batch: 50, loss is 4.327620525360107 and perplexity is 75.76379388928937
At time: 367.09697008132935 and batch: 100, loss is 4.318208208084107 and perplexity is 75.05402654085891
At time: 367.799254655838 and batch: 150, loss is 4.291917219161987 and perplexity is 73.10649539945884
At time: 368.5004982948303 and batch: 200, loss is 4.259047956466675 and perplexity is 70.74260134787289
At time: 369.2033371925354 and batch: 250, loss is 4.222226214408875 and perplexity is 68.18511012561953
At time: 369.90534114837646 and batch: 300, loss is 4.2053597545623775 and perplexity is 67.04471296962353
At time: 370.6066379547119 and batch: 350, loss is 4.250007996559143 and perplexity is 70.10597295100537
At time: 371.3075215816498 and batch: 400, loss is 4.234531354904175 and perplexity is 69.02932089923267
At time: 372.011510848999 and batch: 450, loss is 4.180737271308899 and perplexity is 65.41406335195825
At time: 372.71516394615173 and batch: 500, loss is 4.217586259841919 and perplexity is 67.86946716329037
At time: 373.4327185153961 and batch: 550, loss is 4.267865266799927 and perplexity is 71.36911885835515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849739237034575 and perplexity of 127.7070842257929
Finished 42 epochs...
Completing Train Step...
At time: 375.27780270576477 and batch: 50, loss is 4.327090873718261 and perplexity is 75.7236760966287
At time: 375.98434233665466 and batch: 100, loss is 4.317754716873169 and perplexity is 75.01999791590147
At time: 376.6791453361511 and batch: 150, loss is 4.291521205902099 and perplexity is 73.07754998965807
At time: 377.3777322769165 and batch: 200, loss is 4.2586183261871335 and perplexity is 70.71221471225714
At time: 378.07841086387634 and batch: 250, loss is 4.221844959259033 and perplexity is 68.15911915615192
At time: 378.77988958358765 and batch: 300, loss is 4.205040292739868 and perplexity is 67.02329816421715
At time: 379.4809572696686 and batch: 350, loss is 4.2497180509567265 and perplexity is 70.08564897901064
At time: 380.1831052303314 and batch: 400, loss is 4.234124326705933 and perplexity is 69.0012297364567
At time: 380.8839282989502 and batch: 450, loss is 4.180517325401306 and perplexity is 65.39967737855096
At time: 381.5855529308319 and batch: 500, loss is 4.217407426834106 and perplexity is 67.85733094754922
At time: 382.2878394126892 and batch: 550, loss is 4.26771710395813 and perplexity is 71.35854539020548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849707096181017 and perplexity of 127.70297967706276
Finished 43 epochs...
Completing Train Step...
At time: 384.15167212486267 and batch: 50, loss is 4.3265671539306645 and perplexity is 75.68402849209104
At time: 384.8688774108887 and batch: 100, loss is 4.31726975440979 and perplexity is 74.9836248534074
At time: 385.5726845264435 and batch: 150, loss is 4.291057529449463 and perplexity is 73.04367350498259
At time: 386.2755846977234 and batch: 200, loss is 4.2581998729705814 and perplexity is 70.68263114866161
At time: 386.979088306427 and batch: 250, loss is 4.221472401618957 and perplexity is 68.13373068519752
At time: 387.6854591369629 and batch: 300, loss is 4.204765362739563 and perplexity is 67.00487398162872
At time: 388.39174461364746 and batch: 350, loss is 4.2494230508804325 and perplexity is 70.06497675651825
At time: 389.09819078445435 and batch: 400, loss is 4.233717002868652 and perplexity is 68.9731296140967
At time: 389.80500841140747 and batch: 450, loss is 4.180281782150269 and perplexity is 65.38427473998944
At time: 390.51073002815247 and batch: 500, loss is 4.217220935821533 and perplexity is 67.84467734511854
At time: 391.23792695999146 and batch: 550, loss is 4.267515001296997 and perplexity is 71.34412509552654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.8496879415309175 and perplexity of 127.70053359459735
Finished 44 epochs...
Completing Train Step...
At time: 393.0924320220947 and batch: 50, loss is 4.326044797897339 and perplexity is 75.64450480679824
At time: 393.80432057380676 and batch: 100, loss is 4.316822052001953 and perplexity is 74.95006201765398
At time: 394.5021529197693 and batch: 150, loss is 4.290671167373657 and perplexity is 73.01545765078168
At time: 395.20033502578735 and batch: 200, loss is 4.257794008255005 and perplexity is 70.65394938352622
At time: 395.89834451675415 and batch: 250, loss is 4.221089906692505 and perplexity is 68.1076748623179
At time: 396.59869384765625 and batch: 300, loss is 4.204448318481445 and perplexity is 66.98363383827758
At time: 397.29876351356506 and batch: 350, loss is 4.249112567901611 and perplexity is 70.04322615059503
At time: 397.99936079978943 and batch: 400, loss is 4.233319568634033 and perplexity is 68.94572277768751
At time: 398.69998955726624 and batch: 450, loss is 4.180048575401306 and perplexity is 65.36902846368041
At time: 399.40302181243896 and batch: 500, loss is 4.2170390701293945 and perplexity is 67.83233984783298
At time: 400.1057925224304 and batch: 550, loss is 4.267347764968872 and perplexity is 71.33219476363257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.84965677464262 and perplexity of 127.69655362835314
Finished 45 epochs...
Completing Train Step...
At time: 401.9602704048157 and batch: 50, loss is 4.325499324798584 and perplexity is 75.60325401597868
At time: 402.67329478263855 and batch: 100, loss is 4.316306648254394 and perplexity is 74.91144242800583
At time: 403.3715488910675 and batch: 150, loss is 4.290201988220215 and perplexity is 72.98120835532889
At time: 404.07031893730164 and batch: 200, loss is 4.257393770217895 and perplexity is 70.62567664380124
At time: 404.7712004184723 and batch: 250, loss is 4.2207177734375 and perplexity is 68.08233444687863
At time: 405.4745399951935 and batch: 300, loss is 4.204171028137207 and perplexity is 66.96506249833787
At time: 406.18388295173645 and batch: 350, loss is 4.248789987564087 and perplexity is 70.02063522695185
At time: 406.89433002471924 and batch: 400, loss is 4.232910165786743 and perplexity is 68.91750197970555
At time: 407.60502099990845 and batch: 450, loss is 4.179794120788574 and perplexity is 65.35239712890765
At time: 408.3258183002472 and batch: 500, loss is 4.216843175888061 and perplexity is 67.81905318451355
At time: 409.0503787994385 and batch: 550, loss is 4.267126045227051 and perplexity is 71.3163807610291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.84962755568484 and perplexity of 127.69282252265386
Finished 46 epochs...
Completing Train Step...
At time: 410.9246220588684 and batch: 50, loss is 4.324986848831177 and perplexity is 75.56451909144441
At time: 411.6490979194641 and batch: 100, loss is 4.3158696842193605 and perplexity is 74.87871597251504
At time: 412.3585596084595 and batch: 150, loss is 4.2898175907135006 and perplexity is 72.95315995201364
At time: 413.068412065506 and batch: 200, loss is 4.256980991363525 and perplexity is 70.59652987390609
At time: 413.7782852649689 and batch: 250, loss is 4.220349683761596 and perplexity is 68.05727865412952
At time: 414.4888970851898 and batch: 300, loss is 4.203872442245483 and perplexity is 66.94507066022642
At time: 415.19908142089844 and batch: 350, loss is 4.248456926345825 and perplexity is 69.99731795213522
At time: 415.90912556648254 and batch: 400, loss is 4.232493600845337 and perplexity is 68.88879934320262
At time: 416.6191442012787 and batch: 450, loss is 4.179550786018371 and perplexity is 65.33649655303003
At time: 417.32924795150757 and batch: 500, loss is 4.216639776229858 and perplexity is 67.80526021506633
At time: 418.039404630661 and batch: 550, loss is 4.266945152282715 and perplexity is 71.30348129768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849554508290392 and perplexity of 127.68349523535005
Finished 47 epochs...
Completing Train Step...
At time: 419.90317463874817 and batch: 50, loss is 4.324455890655518 and perplexity is 75.5244081418037
At time: 420.6198580265045 and batch: 100, loss is 4.315387754440308 and perplexity is 74.84263838360116
At time: 421.32253551483154 and batch: 150, loss is 4.289356451034546 and perplexity is 72.91952611080006
At time: 422.0250062942505 and batch: 200, loss is 4.25654673576355 and perplexity is 70.56587959097969
At time: 422.7278027534485 and batch: 250, loss is 4.2199956941604615 and perplexity is 68.03319134878376
At time: 423.4322307109833 and batch: 300, loss is 4.2035506200790405 and perplexity is 66.92352971891599
At time: 424.13381123542786 and batch: 350, loss is 4.248143000602722 and perplexity is 69.97534744081686
At time: 424.83570075035095 and batch: 400, loss is 4.232073516845703 and perplexity is 68.85986633841621
At time: 425.53794598579407 and batch: 450, loss is 4.179288740158081 and perplexity is 65.31937763765087
At time: 426.2394223213196 and batch: 500, loss is 4.216451358795166 and perplexity is 67.79248572538408
At time: 426.9553680419922 and batch: 550, loss is 4.2667210578918455 and perplexity is 71.28750437770765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849503862096908 and perplexity of 127.67702871609927
Finished 48 epochs...
Completing Train Step...
At time: 428.79386496543884 and batch: 50, loss is 4.323971462249756 and perplexity is 75.48783083343037
At time: 429.49775195121765 and batch: 100, loss is 4.314936265945435 and perplexity is 74.80885542032851
At time: 430.1893844604492 and batch: 150, loss is 4.289001722335815 and perplexity is 72.89366404946954
At time: 430.88170170783997 and batch: 200, loss is 4.256185035705567 and perplexity is 70.54036052364229
At time: 431.57743215560913 and batch: 250, loss is 4.219632987976074 and perplexity is 68.0085197640766
At time: 432.27398896217346 and batch: 300, loss is 4.203248252868653 and perplexity is 66.90329729689022
At time: 432.96936440467834 and batch: 350, loss is 4.247873501777649 and perplexity is 69.95649170781056
At time: 433.66700434684753 and batch: 400, loss is 4.2316426658630375 and perplexity is 68.83020438773725
At time: 434.3685266971588 and batch: 450, loss is 4.179010744094849 and perplexity is 65.30122163158084
At time: 435.0709671974182 and batch: 500, loss is 4.216246261596679 and perplexity is 67.7785831022266
At time: 435.77438402175903 and batch: 550, loss is 4.2665122127532955 and perplexity is 71.27261788351908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849461981590758 and perplexity of 127.67168164948261
Finished 49 epochs...
Completing Train Step...
At time: 437.62541103363037 and batch: 50, loss is 4.323485450744629 and perplexity is 75.45115179308802
At time: 438.33676648139954 and batch: 100, loss is 4.314500684738159 and perplexity is 74.77627718451835
At time: 439.0367796421051 and batch: 150, loss is 4.288603458404541 and perplexity is 72.86463891247648
At time: 439.7367277145386 and batch: 200, loss is 4.255828218460083 and perplexity is 70.51519499651886
At time: 440.437477350235 and batch: 250, loss is 4.219273858070373 and perplexity is 67.98410025593715
At time: 441.13788890838623 and batch: 300, loss is 4.20294997215271 and perplexity is 66.88334430941718
At time: 441.83835196495056 and batch: 350, loss is 4.24757043838501 and perplexity is 69.93529366843356
At time: 442.54505729675293 and batch: 400, loss is 4.2312198257446285 and perplexity is 68.80110636830237
At time: 443.2527484893799 and batch: 450, loss is 4.178734602928162 and perplexity is 65.28319176556212
At time: 443.95370531082153 and batch: 500, loss is 4.216062150001526 and perplexity is 67.76610542785193
At time: 444.66864943504333 and batch: 550, loss is 4.266281366348267 and perplexity is 71.25616675481868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.849425620221077 and perplexity of 127.66703941666789
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23cfa37a20>
SETTINGS FOR THIS RUN
{'lr': 28.64800965039765, 'seq_len': 35, 'dropout': 0.43537702910137677, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 4.822065407101446, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.960773229598999 and batch: 50, loss is 6.473177585601807 and perplexity is 647.5380689572685
At time: 1.681438684463501 and batch: 100, loss is 5.815123043060303 and perplexity is 335.3326563190635
At time: 2.3667287826538086 and batch: 150, loss is 5.735363569259643 and perplexity is 309.62552061943103
At time: 3.0349864959716797 and batch: 200, loss is 5.637563352584839 and perplexity is 280.7777279489541
At time: 3.7034311294555664 and batch: 250, loss is 5.586973819732666 and perplexity is 266.92662815892663
At time: 4.373470306396484 and batch: 300, loss is 5.576763515472412 and perplexity is 264.2150924291634
At time: 5.044400691986084 and batch: 350, loss is 5.610968561172485 and perplexity is 273.40892306932244
At time: 5.714719295501709 and batch: 400, loss is 5.608615083694458 and perplexity is 272.7662179192892
At time: 6.384720802307129 and batch: 450, loss is 5.530223712921143 and perplexity is 252.2003251844493
At time: 7.057116746902466 and batch: 500, loss is 5.538337688446045 and perplexity is 254.25499695589656
At time: 7.727031230926514 and batch: 550, loss is 5.580077152252198 and perplexity is 265.09205744654264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.442861354097407 and perplexity of 231.10250440199522
Finished 1 epochs...
Completing Train Step...
At time: 9.50341796875 and batch: 50, loss is 5.422047576904297 and perplexity is 226.34210120692703
At time: 10.176441431045532 and batch: 100, loss is 5.4519621276855466 and perplexity is 233.21531549529095
At time: 10.832475900650024 and batch: 150, loss is 5.39322003364563 and perplexity is 219.91036546936454
At time: 11.488443613052368 and batch: 200, loss is 5.3271036624908445 and perplexity is 205.84092515353157
At time: 12.144209623336792 and batch: 250, loss is 5.299996500015259 and perplexity is 200.3361088002407
At time: 12.799986362457275 and batch: 300, loss is 5.23267448425293 and perplexity is 187.29304657965503
At time: 13.455269813537598 and batch: 350, loss is 5.323280992507935 and perplexity is 205.0555652698784
At time: 14.112258911132812 and batch: 400, loss is 5.3313422203063965 and perplexity is 206.715245435176
At time: 14.777275323867798 and batch: 450, loss is 5.241967306137085 and perplexity is 189.0416395998845
At time: 15.456344842910767 and batch: 500, loss is 5.274260883331299 and perplexity is 195.2461135115772
At time: 16.150076866149902 and batch: 550, loss is 5.335003080368042 and perplexity is 207.47338790134864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.388904003386802 and perplexity of 218.96327099219477
Finished 2 epochs...
Completing Train Step...
At time: 18.010958909988403 and batch: 50, loss is 5.275740280151367 and perplexity is 195.53517375571974
At time: 18.736196756362915 and batch: 100, loss is 5.308693103790283 and perplexity is 202.08595037121611
At time: 19.43519687652588 and batch: 150, loss is 5.263024740219116 and perplexity is 193.06457920557318
At time: 20.136701345443726 and batch: 200, loss is 5.195664796829224 and perplexity is 180.48809083255338
At time: 20.8387508392334 and batch: 250, loss is 5.203410482406616 and perplexity is 181.89152309046062
At time: 21.542730808258057 and batch: 300, loss is 5.197247943878174 and perplexity is 180.77405632390986
At time: 22.24590492248535 and batch: 350, loss is 5.2709841060638425 and perplexity is 194.60738254633551
At time: 22.949806451797485 and batch: 400, loss is 5.244077863693238 and perplexity is 189.44104419570255
At time: 23.653581619262695 and batch: 450, loss is 5.198476667404175 and perplexity is 180.99631417857012
At time: 24.3571298122406 and batch: 500, loss is 5.226787166595459 and perplexity is 186.19363239519515
At time: 25.06076455116272 and batch: 550, loss is 5.266001214981079 and perplexity is 193.64008712051177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.453218500664893 and perplexity of 233.5085050552035
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 26.928714275360107 and batch: 50, loss is 5.19568395614624 and perplexity is 180.49154889423028
At time: 27.65197992324829 and batch: 100, loss is 5.1233068943023685 and perplexity is 167.88964596301773
At time: 28.356884241104126 and batch: 150, loss is 5.086195936203003 and perplexity is 161.77329413362764
At time: 29.06242084503174 and batch: 200, loss is 5.013850908279419 and perplexity is 150.48311847248664
At time: 29.768136262893677 and batch: 250, loss is 4.988022842407227 and perplexity is 146.64619403955766
At time: 30.474919080734253 and batch: 300, loss is 4.9461236000061035 and perplexity is 140.62877260414697
At time: 31.179981470108032 and batch: 350, loss is 5.003920822143555 and perplexity is 148.99620296189795
At time: 31.885003089904785 and batch: 400, loss is 4.978948783874512 and perplexity is 145.3215369882339
At time: 32.590747356414795 and batch: 450, loss is 4.902679271697998 and perplexity is 134.65006092111236
At time: 33.29629611968994 and batch: 500, loss is 4.92912522315979 and perplexity is 138.2585140783606
At time: 34.002196073532104 and batch: 550, loss is 4.967088603973389 and perplexity is 143.60817787364243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.193896679168052 and perplexity of 180.1692486098685
Finished 4 epochs...
Completing Train Step...
At time: 35.869874477386475 and batch: 50, loss is 5.017244577407837 and perplexity is 150.99467592339224
At time: 36.5894079208374 and batch: 100, loss is 5.004732971191406 and perplexity is 149.11725923748247
At time: 37.29097032546997 and batch: 150, loss is 4.991366415023804 and perplexity is 147.13733686656727
At time: 37.99319243431091 and batch: 200, loss is 4.93892128944397 and perplexity is 139.61955920663317
At time: 38.695494174957275 and batch: 250, loss is 4.920111970901489 and perplexity is 137.01795435143987
At time: 39.398001194000244 and batch: 300, loss is 4.884273414611816 and perplexity is 132.19437992654065
At time: 40.09977579116821 and batch: 350, loss is 4.927849836349488 and perplexity is 138.0822933917496
At time: 40.80128288269043 and batch: 400, loss is 4.920545377731323 and perplexity is 137.0773517393527
At time: 41.512638330459595 and batch: 450, loss is 4.855294895172119 and perplexity is 128.4185556434969
At time: 42.21417999267578 and batch: 500, loss is 4.871212968826294 and perplexity is 130.47908801499324
At time: 42.915159463882446 and batch: 550, loss is 4.9147384738922115 and perplexity is 136.28366340547043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.162049963119182 and perplexity of 174.52185251585783
Finished 5 epochs...
Completing Train Step...
At time: 44.76659607887268 and batch: 50, loss is 4.949279870986938 and perplexity is 141.07333633091918
At time: 45.47512769699097 and batch: 100, loss is 4.943546352386474 and perplexity is 140.26680407632065
At time: 46.166287660598755 and batch: 150, loss is 4.922675609588623 and perplexity is 137.36966952272735
At time: 46.85764288902283 and batch: 200, loss is 4.881791687011718 and perplexity is 131.8667162394796
At time: 47.54978895187378 and batch: 250, loss is 4.86638768196106 and perplexity is 129.8510055431148
At time: 48.24157404899597 and batch: 300, loss is 4.82332552909851 and perplexity is 124.37802653077992
At time: 48.932583808898926 and batch: 350, loss is 4.879743776321411 and perplexity is 131.59694131335957
At time: 49.626220703125 and batch: 400, loss is 4.877666301727295 and perplexity is 131.32383579423114
At time: 50.322340965270996 and batch: 450, loss is 4.8197307205200195 and perplexity is 123.93171401868311
At time: 51.01766347885132 and batch: 500, loss is 4.827489252090454 and perplexity is 124.89698182283672
At time: 51.713841915130615 and batch: 550, loss is 4.878511571884156 and perplexity is 131.4348868409683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.145758770881815 and perplexity of 171.7017175199789
Finished 6 epochs...
Completing Train Step...
At time: 53.55917811393738 and batch: 50, loss is 4.884556150436401 and perplexity is 132.23176129783204
At time: 54.26904368400574 and batch: 100, loss is 4.897218809127808 and perplexity is 133.91681306154013
At time: 54.96164536476135 and batch: 150, loss is 4.883672389984131 and perplexity is 132.1149517201083
At time: 55.65657663345337 and batch: 200, loss is 4.8437145328521725 and perplexity is 126.93999986974032
At time: 56.35694479942322 and batch: 250, loss is 4.826971025466919 and perplexity is 124.83227364985406
At time: 57.05754089355469 and batch: 300, loss is 4.787772827148437 and perplexity is 120.0337348649164
At time: 57.75754928588867 and batch: 350, loss is 4.8437462902069095 and perplexity is 126.94403121235855
At time: 58.4571590423584 and batch: 400, loss is 4.842537717819214 and perplexity is 126.79070283433772
At time: 59.15792751312256 and batch: 450, loss is 4.789527568817139 and perplexity is 120.24454796830896
At time: 59.85993051528931 and batch: 500, loss is 4.804622678756714 and perplexity is 122.07342143506386
At time: 60.56266665458679 and batch: 550, loss is 4.847778577804565 and perplexity is 127.45693945682657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.150021167511635 and perplexity of 172.4351403010623
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 62.42676639556885 and batch: 50, loss is 4.832724399566651 and perplexity is 125.55255044396648
At time: 63.14856576919556 and batch: 100, loss is 4.805456295013427 and perplexity is 122.17522625086399
At time: 63.85084939002991 and batch: 150, loss is 4.759823293685913 and perplexity is 116.72529797933511
At time: 64.55311489105225 and batch: 200, loss is 4.717093877792358 and perplexity is 111.84275122031792
At time: 65.25558876991272 and batch: 250, loss is 4.685409355163574 and perplexity is 108.35461875352519
At time: 65.95812106132507 and batch: 300, loss is 4.646357698440552 and perplexity is 104.20474841062754
At time: 66.66095900535583 and batch: 350, loss is 4.6921193981170655 and perplexity is 109.08412768053155
At time: 67.36395478248596 and batch: 400, loss is 4.6721345329284665 and perplexity is 106.92573551987647
At time: 68.0661678314209 and batch: 450, loss is 4.6164914417266845 and perplexity is 101.13855836828455
At time: 68.76827931404114 and batch: 500, loss is 4.618979768753052 and perplexity is 101.39053754976743
At time: 69.47059988975525 and batch: 550, loss is 4.6656532478332515 and perplexity is 106.23496031758569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.055931253636137 and perplexity of 156.95062314738306
Finished 8 epochs...
Completing Train Step...
At time: 71.3434693813324 and batch: 50, loss is 4.760348300933838 and perplexity is 116.78659569625539
At time: 72.0986258983612 and batch: 100, loss is 4.757782039642334 and perplexity is 116.48727500817085
At time: 72.79193234443665 and batch: 150, loss is 4.725434865951538 and perplexity is 112.77953169019908
At time: 73.48910403251648 and batch: 200, loss is 4.689335699081421 and perplexity is 108.78089255298276
At time: 74.18393969535828 and batch: 250, loss is 4.660400562286377 and perplexity is 105.67840446304606
At time: 74.87860941886902 and batch: 300, loss is 4.626800680160523 and perplexity is 102.18661292120869
At time: 75.57964992523193 and batch: 350, loss is 4.676998291015625 and perplexity is 107.44706320890198
At time: 76.2774269580841 and batch: 400, loss is 4.664236888885498 and perplexity is 106.08459998821077
At time: 76.9776828289032 and batch: 450, loss is 4.611960706710815 and perplexity is 100.68136285837582
At time: 77.67753601074219 and batch: 500, loss is 4.619848566055298 and perplexity is 101.4786636515805
At time: 78.3764750957489 and batch: 550, loss is 4.669307527542114 and perplexity is 106.62388276034349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0513906275972404 and perplexity of 156.2395845628947
Finished 9 epochs...
Completing Train Step...
At time: 80.22904133796692 and batch: 50, loss is 4.741398687362671 and perplexity is 114.59437133943321
At time: 80.93480825424194 and batch: 100, loss is 4.739069023132324 and perplexity is 114.3277156612248
At time: 81.62514281272888 and batch: 150, loss is 4.7084317398071285 and perplexity is 110.87813771568973
At time: 82.3181962966919 and batch: 200, loss is 4.674133596420288 and perplexity is 107.1397006477733
At time: 83.01520657539368 and batch: 250, loss is 4.647548942565918 and perplexity is 104.32895567090341
At time: 83.71202301979065 and batch: 300, loss is 4.61677996635437 and perplexity is 101.16774354330083
At time: 84.41768097877502 and batch: 350, loss is 4.669268541336059 and perplexity is 106.61972598070892
At time: 85.12201285362244 and batch: 400, loss is 4.659363031387329 and perplexity is 105.56881671322257
At time: 85.82517743110657 and batch: 450, loss is 4.608106966018677 and perplexity is 100.29410965936239
At time: 86.52902269363403 and batch: 500, loss is 4.616663589477539 and perplexity is 101.15597064233121
At time: 87.23351836204529 and batch: 550, loss is 4.6651542854309085 and perplexity is 106.18196628868625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0491323268159904 and perplexity of 155.88714669215733
Finished 10 epochs...
Completing Train Step...
At time: 89.10512566566467 and batch: 50, loss is 4.727226181030273 and perplexity is 112.9817364180077
At time: 89.82643032073975 and batch: 100, loss is 4.725460271835328 and perplexity is 112.78239699027263
At time: 90.53316879272461 and batch: 150, loss is 4.695950708389282 and perplexity is 109.50286446249933
At time: 91.2398042678833 and batch: 200, loss is 4.663208770751953 and perplexity is 105.97558853520111
At time: 91.94563364982605 and batch: 250, loss is 4.6368953895568845 and perplexity is 103.2233812160219
At time: 92.65197014808655 and batch: 300, loss is 4.6080419921875 and perplexity is 100.28759337850953
At time: 93.35783457756042 and batch: 350, loss is 4.66198130607605 and perplexity is 105.84558704620632
At time: 94.0656361579895 and batch: 400, loss is 4.654288177490234 and perplexity is 105.03442751276616
At time: 94.77003884315491 and batch: 450, loss is 4.60369532585144 and perplexity is 99.85262269350682
At time: 95.47840523719788 and batch: 500, loss is 4.612228860855103 and perplexity is 100.70836460323191
At time: 96.18463921546936 and batch: 550, loss is 4.660055503845215 and perplexity is 105.64194552813099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0471016092503325 and perplexity of 155.5709051324312
Finished 11 epochs...
Completing Train Step...
At time: 98.05024790763855 and batch: 50, loss is 4.71483172416687 and perplexity is 111.59003168813422
At time: 98.76643753051758 and batch: 100, loss is 4.713420372009278 and perplexity is 111.43264994280258
At time: 99.4706552028656 and batch: 150, loss is 4.68528374671936 and perplexity is 108.34100935318587
At time: 100.17397904396057 and batch: 200, loss is 4.653655281066895 and perplexity is 104.96797263101256
At time: 100.87857508659363 and batch: 250, loss is 4.628083877563476 and perplexity is 102.31782268353481
At time: 101.58158874511719 and batch: 300, loss is 4.600878591537476 and perplexity is 99.57176012813612
At time: 102.28184986114502 and batch: 350, loss is 4.6556203746795655 and perplexity is 105.1744473281756
At time: 102.98581147193909 and batch: 400, loss is 4.649648370742798 and perplexity is 104.54821690117122
At time: 103.69118976593018 and batch: 450, loss is 4.599582262039185 and perplexity is 99.44276594583134
At time: 104.3969292640686 and batch: 500, loss is 4.608176431655884 and perplexity is 100.30107689558693
At time: 105.10477900505066 and batch: 550, loss is 4.656335401535034 and perplexity is 105.24967677485648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.04843366906998 and perplexity of 155.77827296678183
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 106.97983193397522 and batch: 50, loss is 4.7031356143951415 and perplexity is 110.29246545871824
At time: 107.69684720039368 and batch: 100, loss is 4.695407238006592 and perplexity is 109.44336906730116
At time: 108.3986337184906 and batch: 150, loss is 4.658443717956543 and perplexity is 105.4718104785451
At time: 109.10382604598999 and batch: 200, loss is 4.626006660461425 and perplexity is 102.10550694169916
At time: 109.806725025177 and batch: 250, loss is 4.592959213256836 and perplexity is 98.7863278661731
At time: 110.50958561897278 and batch: 300, loss is 4.560903434753418 and perplexity is 95.66987228669517
At time: 111.211678981781 and batch: 350, loss is 4.6145292663574216 and perplexity is 100.94030135129894
At time: 111.91428565979004 and batch: 400, loss is 4.604075593948364 and perplexity is 99.89060068076218
At time: 112.61760020256042 and batch: 450, loss is 4.5461381149291995 and perplexity is 94.2676536037595
At time: 113.32093095779419 and batch: 500, loss is 4.550197486877441 and perplexity is 94.65109881925086
At time: 114.02280688285828 and batch: 550, loss is 4.602242670059204 and perplexity is 99.70767650672892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.026616522606383 and perplexity of 152.4164417003688
Finished 13 epochs...
Completing Train Step...
At time: 115.87603616714478 and batch: 50, loss is 4.684217624664306 and perplexity is 108.22556616284785
At time: 116.58552956581116 and batch: 100, loss is 4.681773586273193 and perplexity is 107.96138169424265
At time: 117.27973246574402 and batch: 150, loss is 4.649485559463501 and perplexity is 104.53119665781065
At time: 117.97106146812439 and batch: 200, loss is 4.617224054336548 and perplexity is 101.21268089972392
At time: 118.66242837905884 and batch: 250, loss is 4.586399841308594 and perplexity is 98.1404721180161
At time: 119.35356402397156 and batch: 300, loss is 4.5561855602264405 and perplexity is 95.21957688706772
At time: 120.05322194099426 and batch: 350, loss is 4.612273950576782 and perplexity is 100.7129056177384
At time: 120.75892901420593 and batch: 400, loss is 4.601522436141968 and perplexity is 99.63588951111771
At time: 121.46369886398315 and batch: 450, loss is 4.545679273605347 and perplexity is 94.22440963060313
At time: 122.16819071769714 and batch: 500, loss is 4.553098659515381 and perplexity is 94.9260967125945
At time: 122.87258696556091 and batch: 550, loss is 4.604802875518799 and perplexity is 99.96327569809395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.024808193774933 and perplexity of 152.14107170924174
Finished 14 epochs...
Completing Train Step...
At time: 124.74627709388733 and batch: 50, loss is 4.678499822616577 and perplexity is 107.60851955530786
At time: 125.46781849861145 and batch: 100, loss is 4.6764196681976316 and perplexity is 107.38490986980712
At time: 126.17468547821045 and batch: 150, loss is 4.645731077194214 and perplexity is 104.13947195524702
At time: 126.88192892074585 and batch: 200, loss is 4.6133002662658695 and perplexity is 100.81632191267292
At time: 127.58915972709656 and batch: 250, loss is 4.583211231231689 and perplexity is 97.82803879839928
At time: 128.29803609848022 and batch: 300, loss is 4.5541469097137455 and perplexity is 95.0256551842808
At time: 129.0043523311615 and batch: 350, loss is 4.611515264511109 and perplexity is 100.63652511768707
At time: 129.71130347251892 and batch: 400, loss is 4.600465784072876 and perplexity is 99.53066464513654
At time: 130.41773128509521 and batch: 450, loss is 4.545428943634033 and perplexity is 94.2008253888877
At time: 131.12580704689026 and batch: 500, loss is 4.554488306045532 and perplexity is 95.05810213270577
At time: 131.83373022079468 and batch: 550, loss is 4.6053712272644045 and perplexity is 100.02010614864649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0238916924659245 and perplexity of 152.00169809567174
Finished 15 epochs...
Completing Train Step...
At time: 133.69251656532288 and batch: 50, loss is 4.674498291015625 and perplexity is 107.17878104331695
At time: 134.39999794960022 and batch: 100, loss is 4.672699756622315 and perplexity is 106.9861895624936
At time: 135.0914306640625 and batch: 150, loss is 4.64285641670227 and perplexity is 103.84053620481995
At time: 135.78940892219543 and batch: 200, loss is 4.6102840614318845 and perplexity is 100.51269736227876
At time: 136.48898458480835 and batch: 250, loss is 4.580699949264527 and perplexity is 97.58267322869783
At time: 137.1894063949585 and batch: 300, loss is 4.552581119537353 and perplexity is 94.8769813732597
At time: 137.8907413482666 and batch: 350, loss is 4.610949239730835 and perplexity is 100.57957846879741
At time: 138.58993768692017 and batch: 400, loss is 4.599534215927124 and perplexity is 99.43798822233154
At time: 139.29102444648743 and batch: 450, loss is 4.544985637664795 and perplexity is 94.15907485549971
At time: 139.99166417121887 and batch: 500, loss is 4.554729700088501 and perplexity is 95.08105136208871
At time: 140.69124960899353 and batch: 550, loss is 4.605185251235962 and perplexity is 100.00150653613524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.023299521588265 and perplexity of 151.911713762385
Finished 16 epochs...
Completing Train Step...
At time: 142.55394172668457 and batch: 50, loss is 4.671132364273071 and perplexity is 106.81863157635499
At time: 143.26661801338196 and batch: 100, loss is 4.669475736618042 and perplexity is 106.6418193736428
At time: 143.96288323402405 and batch: 150, loss is 4.640635042190552 and perplexity is 103.61012349561884
At time: 144.66092467308044 and batch: 200, loss is 4.607841157913208 and perplexity is 100.26745421485772
At time: 145.35907125473022 and batch: 250, loss is 4.57870587348938 and perplexity is 97.38827986590069
At time: 146.05761814117432 and batch: 300, loss is 4.551193828582764 and perplexity is 94.74545065198681
At time: 146.75479173660278 and batch: 350, loss is 4.610418071746826 and perplexity is 100.52616800309018
At time: 147.4518382549286 and batch: 400, loss is 4.598660554885864 and perplexity is 99.35115106464464
At time: 148.14798045158386 and batch: 450, loss is 4.5442938709259035 and perplexity is 94.09396126365503
At time: 148.84598422050476 and batch: 500, loss is 4.554756956100464 and perplexity is 95.0836429276798
At time: 149.54355573654175 and batch: 550, loss is 4.604554967880249 and perplexity is 99.93849711000145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.022901494452294 and perplexity of 151.85126080979168
Finished 17 epochs...
Completing Train Step...
At time: 151.4147243499756 and batch: 50, loss is 4.668084268569946 and perplexity is 106.49353388058198
At time: 152.129864692688 and batch: 100, loss is 4.666785001754761 and perplexity is 106.3552602126248
At time: 152.83189368247986 and batch: 150, loss is 4.638700351715088 and perplexity is 103.40986375929728
At time: 153.53472423553467 and batch: 200, loss is 4.605647535324096 and perplexity is 100.04774632853298
At time: 154.2365608215332 and batch: 250, loss is 4.576787652969361 and perplexity is 97.20164672803742
At time: 154.93828105926514 and batch: 300, loss is 4.549838457107544 and perplexity is 94.61712235666701
At time: 155.6431474685669 and batch: 350, loss is 4.60971965789795 and perplexity is 100.45598364689836
At time: 156.3484501838684 and batch: 400, loss is 4.597696332931519 and perplexity is 99.2554006733318
At time: 157.0569076538086 and batch: 450, loss is 4.543426609039306 and perplexity is 94.01239253313028
At time: 157.7663815021515 and batch: 500, loss is 4.554363842010498 and perplexity is 95.04627155400772
At time: 158.47526001930237 and batch: 550, loss is 4.603926525115967 and perplexity is 99.87571121535643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.022623589698305 and perplexity of 151.8090664857865
Finished 18 epochs...
Completing Train Step...
At time: 160.34711980819702 and batch: 50, loss is 4.665363092422485 and perplexity is 106.20414014057376
At time: 161.0634377002716 and batch: 100, loss is 4.664326581954956 and perplexity is 106.09411546833584
At time: 161.76346445083618 and batch: 150, loss is 4.636950988769531 and perplexity is 103.22912051429296
At time: 162.46394729614258 and batch: 200, loss is 4.603609619140625 and perplexity is 99.8440650203797
At time: 163.16435503959656 and batch: 250, loss is 4.574957857131958 and perplexity is 97.02395018324432
At time: 163.8656129837036 and batch: 300, loss is 4.548640422821045 and perplexity is 94.5038356742086
At time: 164.5665156841278 and batch: 350, loss is 4.609190044403076 and perplexity is 100.40279488830355
At time: 165.26660466194153 and batch: 400, loss is 4.596790218353272 and perplexity is 99.16550464201492
At time: 165.96860265731812 and batch: 450, loss is 4.542520112991333 and perplexity is 93.9272092858102
At time: 166.67252707481384 and batch: 500, loss is 4.5540119075775145 and perplexity is 95.01282738374388
At time: 167.37743020057678 and batch: 550, loss is 4.603010787963867 and perplexity is 99.78429317983824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.022388539415725 and perplexity of 151.77338791510417
Finished 19 epochs...
Completing Train Step...
At time: 169.23782992362976 and batch: 50, loss is 4.662785005569458 and perplexity is 105.93068928462696
At time: 169.94856762886047 and batch: 100, loss is 4.662265501022339 and perplexity is 105.87567210193976
At time: 170.6462960243225 and batch: 150, loss is 4.635204010009765 and perplexity is 103.04893886594672
At time: 171.34261202812195 and batch: 200, loss is 4.601746788024903 and perplexity is 99.65824551824977
At time: 172.04053854942322 and batch: 250, loss is 4.573318758010864 and perplexity is 96.86504857509311
At time: 172.73687195777893 and batch: 300, loss is 4.547373466491699 and perplexity is 94.38417925718699
At time: 173.43527507781982 and batch: 350, loss is 4.608537998199463 and perplexity is 100.33734896626576
At time: 174.13551902770996 and batch: 400, loss is 4.595833129882813 and perplexity is 99.07063988507885
At time: 174.83608984947205 and batch: 450, loss is 4.541651582717895 and perplexity is 93.84566607754438
At time: 175.53766298294067 and batch: 500, loss is 4.553635559082031 and perplexity is 94.97707617698482
At time: 176.23757195472717 and batch: 550, loss is 4.602169427871704 and perplexity is 99.70037396582134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.022262248587101 and perplexity of 151.75422153847555
Finished 20 epochs...
Completing Train Step...
At time: 178.09946465492249 and batch: 50, loss is 4.660548601150513 and perplexity is 105.69405013206146
At time: 178.81625628471375 and batch: 100, loss is 4.66013385772705 and perplexity is 105.65022330894163
At time: 179.51658987998962 and batch: 150, loss is 4.633741579055786 and perplexity is 102.89834704988034
At time: 180.21944117546082 and batch: 200, loss is 4.59993763923645 and perplexity is 99.47811191748565
At time: 180.92307496070862 and batch: 250, loss is 4.571669216156006 and perplexity is 96.70539733509764
At time: 181.63015389442444 and batch: 300, loss is 4.546201486587524 and perplexity is 94.27362769058662
At time: 182.33591604232788 and batch: 350, loss is 4.607846975326538 and perplexity is 100.26803751377906
At time: 183.04418683052063 and batch: 400, loss is 4.594852952957154 and perplexity is 98.97358070520824
At time: 183.75110125541687 and batch: 450, loss is 4.540692253112793 and perplexity is 93.75568032166673
At time: 184.4584949016571 and batch: 500, loss is 4.5530594539642335 and perplexity is 94.9223751556079
At time: 185.16678190231323 and batch: 550, loss is 4.601176233291626 and perplexity is 99.60140125248377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.022299259266955 and perplexity of 151.75983816932245
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 187.0400197505951 and batch: 50, loss is 4.657670135498047 and perplexity is 105.39025088669516
At time: 187.7611427307129 and batch: 100, loss is 4.6557011318206785 and perplexity is 105.18294125882812
At time: 188.46714639663696 and batch: 150, loss is 4.629684429168702 and perplexity is 102.4817187659544
At time: 189.17409348487854 and batch: 200, loss is 4.593005247116089 and perplexity is 98.79087548675771
At time: 189.8821084499359 and batch: 250, loss is 4.562500352859497 and perplexity is 95.82277128905581
At time: 190.5894649028778 and batch: 300, loss is 4.537533464431763 and perplexity is 93.45999319232553
At time: 191.29563641548157 and batch: 350, loss is 4.59748010635376 and perplexity is 99.23394133784304
At time: 192.0020568370819 and batch: 400, loss is 4.582375392913819 and perplexity is 97.74630453808125
At time: 192.7081823348999 and batch: 450, loss is 4.5266125297546385 and perplexity is 92.44487581615624
At time: 193.4164969921112 and batch: 500, loss is 4.538555583953857 and perplexity is 93.55556931269051
At time: 194.1219265460968 and batch: 550, loss is 4.587340602874756 and perplexity is 98.23284234463671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.01946112450133 and perplexity of 151.32973393219035
Finished 22 epochs...
Completing Train Step...
At time: 195.97755002975464 and batch: 50, loss is 4.6546626663208 and perplexity is 105.07376909872707
At time: 196.68401956558228 and batch: 100, loss is 4.6536899757385255 and perplexity is 104.9716145235315
At time: 197.37618207931519 and batch: 150, loss is 4.627492046356201 and perplexity is 102.25728571860974
At time: 198.06719732284546 and batch: 200, loss is 4.591415462493896 and perplexity is 98.6339440487419
At time: 198.75772500038147 and batch: 250, loss is 4.561582298278808 and perplexity is 95.73484112346105
At time: 199.45146679878235 and batch: 300, loss is 4.53687726020813 and perplexity is 93.398684467774
At time: 200.1473937034607 and batch: 350, loss is 4.5965258979797365 and perplexity is 99.13929664259281
At time: 200.84395146369934 and batch: 400, loss is 4.581745290756226 and perplexity is 97.68473378066574
At time: 201.53891038894653 and batch: 450, loss is 4.526508674621582 and perplexity is 92.43527543981081
At time: 202.23356008529663 and batch: 500, loss is 4.539097766876221 and perplexity is 93.60630729805554
At time: 202.9359188079834 and batch: 550, loss is 4.588086528778076 and perplexity is 98.30614410173926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.018956285841922 and perplexity of 151.2533561130435
Finished 23 epochs...
Completing Train Step...
At time: 204.79907846450806 and batch: 50, loss is 4.653525915145874 and perplexity is 104.9543942308655
At time: 205.51006245613098 and batch: 100, loss is 4.652655696868896 and perplexity is 104.86310072715565
At time: 206.20495438575745 and batch: 150, loss is 4.6264576244354245 and perplexity is 102.15156323095987
At time: 206.90006113052368 and batch: 200, loss is 4.590486783981323 and perplexity is 98.54238734424575
At time: 207.59677124023438 and batch: 250, loss is 4.561183481216431 and perplexity is 95.69666804789992
At time: 208.29415798187256 and batch: 300, loss is 4.536551294326782 and perplexity is 93.36824464471609
At time: 208.98966884613037 and batch: 350, loss is 4.5959384155273435 and perplexity is 99.08107115037483
At time: 209.68836569786072 and batch: 400, loss is 4.581342420578003 and perplexity is 97.6453874408235
At time: 210.38980913162231 and batch: 450, loss is 4.526426830291748 and perplexity is 92.42771044621954
At time: 211.09526681900024 and batch: 500, loss is 4.539402208328247 and perplexity is 93.63480927653987
At time: 211.80054354667664 and batch: 550, loss is 4.588459196090699 and perplexity is 98.34278641554818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.018670264710772 and perplexity of 151.21010064332188
Finished 24 epochs...
Completing Train Step...
At time: 213.6808557510376 and batch: 50, loss is 4.652722616195678 and perplexity is 104.87011833006461
At time: 214.40546798706055 and batch: 100, loss is 4.651935729980469 and perplexity is 104.78762993830884
At time: 215.114839553833 and batch: 150, loss is 4.62570387840271 and perplexity is 102.07459590599086
At time: 215.82326889038086 and batch: 200, loss is 4.589822549819946 and perplexity is 98.47695385821224
At time: 216.53101563453674 and batch: 250, loss is 4.560840530395508 and perplexity is 95.6638544240845
At time: 217.239910364151 and batch: 300, loss is 4.536309108734131 and perplexity is 93.34563493903505
At time: 217.95098066329956 and batch: 350, loss is 4.5954325485229495 and perplexity is 99.0309619810761
At time: 218.65975642204285 and batch: 400, loss is 4.581001100540161 and perplexity is 97.612064800653
At time: 219.36959218978882 and batch: 450, loss is 4.526317501068116 and perplexity is 92.41760594876281
At time: 220.07893347740173 and batch: 500, loss is 4.539566068649292 and perplexity is 93.65015356357449
At time: 220.78706526756287 and batch: 550, loss is 4.588653974533081 and perplexity is 98.3619433359227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.018473848383477 and perplexity of 151.18040342730825
Finished 25 epochs...
Completing Train Step...
At time: 222.6725549697876 and batch: 50, loss is 4.652033958435059 and perplexity is 104.79792357081334
At time: 223.39459443092346 and batch: 100, loss is 4.651309032440185 and perplexity is 104.72198036173707
At time: 224.10023641586304 and batch: 150, loss is 4.625002403259277 and perplexity is 102.00301822210493
At time: 224.80598664283752 and batch: 200, loss is 4.589251222610474 and perplexity is 98.42070736407338
At time: 225.51293015480042 and batch: 250, loss is 4.5605463123321535 and perplexity is 95.63571253023261
At time: 226.2201223373413 and batch: 300, loss is 4.536075916290283 and perplexity is 93.32386998011168
At time: 226.92677927017212 and batch: 350, loss is 4.5949644947052 and perplexity is 98.98462100712639
At time: 227.63282680511475 and batch: 400, loss is 4.5806561470031735 and perplexity is 97.57839898055308
At time: 228.33841633796692 and batch: 450, loss is 4.526177253723144 and perplexity is 92.40464553375305
At time: 229.04493594169617 and batch: 500, loss is 4.5396393775939945 and perplexity is 93.65701920915699
At time: 229.75163316726685 and batch: 550, loss is 4.588752632141113 and perplexity is 98.3716479686837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.01832872755984 and perplexity of 151.15846559450176
Finished 26 epochs...
Completing Train Step...
At time: 231.60789680480957 and batch: 50, loss is 4.651442317962647 and perplexity is 104.73593921583871
At time: 232.31469011306763 and batch: 100, loss is 4.650778036117554 and perplexity is 104.66638813620624
At time: 233.0053586959839 and batch: 150, loss is 4.624399471282959 and perplexity is 101.9415358774362
At time: 233.69849395751953 and batch: 200, loss is 4.588766555786133 and perplexity is 98.37301767012566
At time: 234.39007210731506 and batch: 250, loss is 4.560266227722168 and perplexity is 95.60893018982364
At time: 235.0833613872528 and batch: 300, loss is 4.535874195098877 and perplexity is 93.30504647648631
At time: 235.7785723209381 and batch: 350, loss is 4.594519805908203 and perplexity is 98.94061344065061
At time: 236.4765169620514 and batch: 400, loss is 4.580335245132447 and perplexity is 97.54709091345501
At time: 237.17700004577637 and batch: 450, loss is 4.526015539169311 and perplexity is 92.38970356592783
At time: 237.8764762878418 and batch: 500, loss is 4.539655971527099 and perplexity is 93.65857336036325
At time: 238.57886791229248 and batch: 550, loss is 4.588798494338989 and perplexity is 98.37615961212437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0182248379321805 and perplexity of 151.14276261349607
Finished 27 epochs...
Completing Train Step...
At time: 240.4436583518982 and batch: 50, loss is 4.650877676010132 and perplexity is 104.67681760346349
At time: 241.16164231300354 and batch: 100, loss is 4.650294971466065 and perplexity is 104.6158397139595
At time: 241.8635492324829 and batch: 150, loss is 4.623817481994629 and perplexity is 101.88222425655778
At time: 242.5657114982605 and batch: 200, loss is 4.588274526596069 and perplexity is 98.32462717966042
At time: 243.26767659187317 and batch: 250, loss is 4.559998416900635 and perplexity is 95.58332851203984
At time: 243.96775603294373 and batch: 300, loss is 4.535677394866943 and perplexity is 93.28668582844828
At time: 244.67040395736694 and batch: 350, loss is 4.59410608291626 and perplexity is 98.89968790053541
At time: 245.37158727645874 and batch: 400, loss is 4.579993152618409 and perplexity is 97.51372649107226
At time: 246.0726671218872 and batch: 450, loss is 4.525813198089599 and perplexity is 92.37101122473206
At time: 246.7735517024994 and batch: 500, loss is 4.539637022018432 and perplexity is 93.65679859323114
At time: 247.47577357292175 and batch: 550, loss is 4.588821592330933 and perplexity is 98.37843193010946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.01816023156998 and perplexity of 151.13299814485748
Finished 28 epochs...
Completing Train Step...
At time: 249.33553981781006 and batch: 50, loss is 4.650334119796753 and perplexity is 104.61993532961559
At time: 250.05154633522034 and batch: 100, loss is 4.649859552383423 and perplexity is 104.57029789660912
At time: 250.75330185890198 and batch: 150, loss is 4.623306188583374 and perplexity is 101.83014586137611
At time: 251.45518255233765 and batch: 200, loss is 4.587859611511231 and perplexity is 98.28383927097694
At time: 252.15545678138733 and batch: 250, loss is 4.559721336364746 and perplexity is 95.55684790095407
At time: 252.85565161705017 and batch: 300, loss is 4.535504407882691 and perplexity is 93.27054984169398
At time: 253.55622124671936 and batch: 350, loss is 4.59371654510498 and perplexity is 98.86117023510504
At time: 254.25750970840454 and batch: 400, loss is 4.579670381546021 and perplexity is 97.48225696000056
At time: 254.96003079414368 and batch: 450, loss is 4.525617399215698 and perplexity is 92.35292685526053
At time: 255.66996335983276 and batch: 500, loss is 4.539587173461914 and perplexity is 93.65213005337412
At time: 256.37853240966797 and batch: 550, loss is 4.588781366348266 and perplexity is 98.37447464060534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.01811835106383 and perplexity of 151.12666875093922
Finished 29 epochs...
Completing Train Step...
At time: 258.25365018844604 and batch: 50, loss is 4.649844493865967 and perplexity is 104.56872323480893
At time: 258.97058057785034 and batch: 100, loss is 4.649455213546753 and perplexity is 104.52802461094895
At time: 259.67070055007935 and batch: 150, loss is 4.622773141860962 and perplexity is 101.77588010025988
At time: 260.37198424339294 and batch: 200, loss is 4.5874410820007325 and perplexity is 98.24271319067641
At time: 261.0728051662445 and batch: 250, loss is 4.559462728500367 and perplexity is 95.53213934364287
At time: 261.77544140815735 and batch: 300, loss is 4.535329132080078 and perplexity is 93.25420320383726
At time: 262.4776473045349 and batch: 350, loss is 4.59334506034851 and perplexity is 98.82445163797775
At time: 263.1795771121979 and batch: 400, loss is 4.579354791641236 and perplexity is 97.45149739776727
At time: 263.87971234321594 and batch: 450, loss is 4.525415210723877 and perplexity is 92.33425604383962
At time: 264.5806999206543 and batch: 500, loss is 4.539531412124634 and perplexity is 93.64690803095804
At time: 265.282753944397 and batch: 550, loss is 4.588729400634765 and perplexity is 98.36936267366498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.018066081594913 and perplexity of 151.11876964666726
Finished 30 epochs...
Completing Train Step...
At time: 267.141973733902 and batch: 50, loss is 4.649419364929199 and perplexity is 104.52427749293584
At time: 267.8523323535919 and batch: 100, loss is 4.649092063903809 and perplexity is 104.49007218775535
At time: 268.5475392341614 and batch: 150, loss is 4.62231554031372 and perplexity is 101.72931795432169
At time: 269.24475049972534 and batch: 200, loss is 4.58707839012146 and perplexity is 98.20708781731174
At time: 269.9436283111572 and batch: 250, loss is 4.559211988449096 and perplexity is 95.50818861295508
At time: 270.6436574459076 and batch: 300, loss is 4.5351673412323 and perplexity is 93.23911674770017
At time: 271.3447251319885 and batch: 350, loss is 4.592976779937744 and perplexity is 98.78806322931506
At time: 272.04642844200134 and batch: 400, loss is 4.579056596755981 and perplexity is 97.42244219195504
At time: 272.7474446296692 and batch: 450, loss is 4.525190696716309 and perplexity is 92.31352803693039
At time: 273.4504840373993 and batch: 500, loss is 4.539472408294678 and perplexity is 93.64138266773107
At time: 274.1509008407593 and batch: 550, loss is 4.588676033020019 and perplexity is 98.36411307549551
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0180089422997005 and perplexity of 151.11013507336529
Finished 31 epochs...
Completing Train Step...
At time: 276.01319670677185 and batch: 50, loss is 4.648993654251099 and perplexity is 104.47978986198802
At time: 276.7295880317688 and batch: 100, loss is 4.648721675872803 and perplexity is 104.45137748212836
At time: 277.432674407959 and batch: 150, loss is 4.621818189620972 and perplexity is 101.67873538724429
At time: 278.13366651535034 and batch: 200, loss is 4.586689462661743 and perplexity is 98.1688998107844
At time: 278.83401441574097 and batch: 250, loss is 4.5589695739746094 and perplexity is 95.48503885163527
At time: 279.5363869667053 and batch: 300, loss is 4.534988708496094 and perplexity is 93.22246267667956
At time: 280.2393832206726 and batch: 350, loss is 4.592620191574096 and perplexity is 98.7528428354647
At time: 280.945463180542 and batch: 400, loss is 4.5787679481506345 and perplexity is 97.39432539801878
At time: 281.65118741989136 and batch: 450, loss is 4.524956693649292 and perplexity is 92.29192891547152
At time: 282.3580484390259 and batch: 500, loss is 4.5394046592712405 and perplexity is 93.63503877040084
At time: 283.06309819221497 and batch: 550, loss is 4.58858136177063 and perplexity is 98.35480126280302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0179196621509305 and perplexity of 151.09664454025267
Finished 32 epochs...
Completing Train Step...
At time: 284.9374647140503 and batch: 50, loss is 4.648568735122681 and perplexity is 104.43540383164714
At time: 285.65368127822876 and batch: 100, loss is 4.648361501693725 and perplexity is 104.41376356717727
At time: 286.3544681072235 and batch: 150, loss is 4.621389846801758 and perplexity is 101.63519135762645
At time: 287.05636620521545 and batch: 200, loss is 4.586347951889038 and perplexity is 98.13537979800287
At time: 287.7608242034912 and batch: 250, loss is 4.558713483810425 and perplexity is 95.46058920314942
At time: 288.4651083946228 and batch: 300, loss is 4.5348171138763425 and perplexity is 93.2064675760201
At time: 289.16960620880127 and batch: 350, loss is 4.592271614074707 and perplexity is 98.71842581529943
At time: 289.87363386154175 and batch: 400, loss is 4.578475179672242 and perplexity is 97.36581558315909
At time: 290.58302879333496 and batch: 450, loss is 4.52472526550293 and perplexity is 92.27057243677886
At time: 291.2938280105591 and batch: 500, loss is 4.539342069625855 and perplexity is 93.62917836993064
At time: 292.0046212673187 and batch: 550, loss is 4.5885024261474605 and perplexity is 98.34703787168174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017858951649767 and perplexity of 151.08747166568628
Finished 33 epochs...
Completing Train Step...
At time: 293.88040709495544 and batch: 50, loss is 4.648171167373658 and perplexity is 104.39389193566974
At time: 294.59196949005127 and batch: 100, loss is 4.648005619049072 and perplexity is 104.3766111322063
At time: 295.2887101173401 and batch: 150, loss is 4.620934085845947 and perplexity is 101.58888055979766
At time: 295.9883885383606 and batch: 200, loss is 4.5859896850585935 and perplexity is 98.10022744386545
At time: 296.68853640556335 and batch: 250, loss is 4.558475408554077 and perplexity is 95.437865104034
At time: 297.38850355148315 and batch: 300, loss is 4.534633045196533 and perplexity is 93.18931276346397
At time: 298.0895001888275 and batch: 350, loss is 4.5919344615936275 and perplexity is 98.6851482632274
At time: 298.7915029525757 and batch: 400, loss is 4.578166179656982 and perplexity is 97.33573419247259
At time: 299.4932370185852 and batch: 450, loss is 4.524485549926758 and perplexity is 92.2484563942293
At time: 300.19826102256775 and batch: 500, loss is 4.539256267547607 and perplexity is 93.62114513648086
At time: 300.902535200119 and batch: 550, loss is 4.5884132575988765 and perplexity is 98.33826880002569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017797916493517 and perplexity of 151.07825029966236
Finished 34 epochs...
Completing Train Step...
At time: 302.7726619243622 and batch: 50, loss is 4.647782430648804 and perplexity is 104.35331808280823
At time: 303.48993039131165 and batch: 100, loss is 4.647656888961792 and perplexity is 104.34021821351783
At time: 304.1907196044922 and batch: 150, loss is 4.62055380821228 and perplexity is 101.55025592519928
At time: 304.89209246635437 and batch: 200, loss is 4.585669384002686 and perplexity is 98.06881086908002
At time: 305.5949721336365 and batch: 250, loss is 4.558220119476318 and perplexity is 95.41350396916643
At time: 306.29853796958923 and batch: 300, loss is 4.534443950653076 and perplexity is 93.17169283888016
At time: 307.0006561279297 and batch: 350, loss is 4.591590843200684 and perplexity is 98.65124405656185
At time: 307.70197796821594 and batch: 400, loss is 4.577847681045532 and perplexity is 97.30473783269862
At time: 308.40320348739624 and batch: 450, loss is 4.524242563247681 and perplexity is 92.22604397123045
At time: 309.1065490245819 and batch: 500, loss is 4.539151763916015 and perplexity is 93.61136189802119
At time: 309.8107476234436 and batch: 550, loss is 4.588310966491699 and perplexity is 98.32821018409446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017753763401762 and perplexity of 151.07157987507568
Finished 35 epochs...
Completing Train Step...
At time: 311.6777527332306 and batch: 50, loss is 4.647405986785889 and perplexity is 104.31404230966679
At time: 312.3950583934784 and batch: 100, loss is 4.647315216064453 and perplexity is 104.3045740785161
At time: 313.0956552028656 and batch: 150, loss is 4.620098810195923 and perplexity is 101.5040612702279
At time: 313.7977833747864 and batch: 200, loss is 4.585317192077636 and perplexity is 98.03427790726484
At time: 314.49908804893494 and batch: 250, loss is 4.55796591758728 and perplexity is 95.38925275870008
At time: 315.1999704837799 and batch: 300, loss is 4.534242067337036 and perplexity is 93.15288492713451
At time: 315.90071201324463 and batch: 350, loss is 4.591254205703735 and perplexity is 98.61803993788108
At time: 316.60249185562134 and batch: 400, loss is 4.577489738464355 and perplexity is 97.26991455641657
At time: 317.3051059246063 and batch: 450, loss is 4.52396089553833 and perplexity is 92.2000705307941
At time: 318.0057997703552 and batch: 500, loss is 4.539052305221557 and perplexity is 93.60205189716831
At time: 318.70680475234985 and batch: 550, loss is 4.588182067871093 and perplexity is 98.31553663025446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017688507729388 and perplexity of 151.06172191920157
Finished 36 epochs...
Completing Train Step...
At time: 320.5701973438263 and batch: 50, loss is 4.64701621055603 and perplexity is 104.27339109848661
At time: 321.28100514411926 and batch: 100, loss is 4.64702220916748 and perplexity is 104.27401659592047
At time: 321.97648668289185 and batch: 150, loss is 4.619672832489013 and perplexity is 101.46083201097001
At time: 322.6724064350128 and batch: 200, loss is 4.5849936580657955 and perplexity is 98.00256561431488
At time: 323.369416475296 and batch: 250, loss is 4.557683124542236 and perplexity is 95.36228115531863
At time: 324.06854224205017 and batch: 300, loss is 4.534048433303833 and perplexity is 93.13484910455244
At time: 324.7682554721832 and batch: 350, loss is 4.590922842025757 and perplexity is 98.5853669150777
At time: 325.46829986572266 and batch: 400, loss is 4.577167959213257 and perplexity is 97.2386201513713
At time: 326.1675021648407 and batch: 450, loss is 4.523682050704956 and perplexity is 92.17436460163914
At time: 326.8683617115021 and batch: 500, loss is 4.53894492149353 and perplexity is 93.59200109954034
At time: 327.56823444366455 and batch: 550, loss is 4.588031339645386 and perplexity is 98.3007188206178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017648899808843 and perplexity of 151.05573879701268
Finished 37 epochs...
Completing Train Step...
At time: 329.43544840812683 and batch: 50, loss is 4.646628789901733 and perplexity is 104.23300125751597
At time: 330.1505591869354 and batch: 100, loss is 4.646719741821289 and perplexity is 104.24248188019527
At time: 330.8510670661926 and batch: 150, loss is 4.619217977523804 and perplexity is 101.41469254193434
At time: 331.55134749412537 and batch: 200, loss is 4.584642095565796 and perplexity is 97.9681176430034
At time: 332.251469373703 and batch: 250, loss is 4.557403621673584 and perplexity is 95.33563084876654
At time: 332.95154452323914 and batch: 300, loss is 4.5338499641418455 and perplexity is 93.11636654326871
At time: 333.653608083725 and batch: 350, loss is 4.5906096458435055 and perplexity is 98.55449518923976
At time: 334.3585524559021 and batch: 400, loss is 4.57685625076294 and perplexity is 97.20831477523923
At time: 335.06280064582825 and batch: 450, loss is 4.523433685302734 and perplexity is 92.15147452117004
At time: 335.76696038246155 and batch: 500, loss is 4.538842716217041 and perplexity is 93.58243599200134
At time: 336.47035241127014 and batch: 550, loss is 4.587902059555054 and perplexity is 98.28801131624036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017624875332447 and perplexity of 151.052109805574
Finished 38 epochs...
Completing Train Step...
At time: 338.3374435901642 and batch: 50, loss is 4.646289691925049 and perplexity is 104.1976620497514
At time: 339.0538113117218 and batch: 100, loss is 4.64643253326416 and perplexity is 104.21254684638762
At time: 339.75340509414673 and batch: 150, loss is 4.618869085311889 and perplexity is 101.379315917206
At time: 340.45267367362976 and batch: 200, loss is 4.584308404922485 and perplexity is 97.93543205254429
At time: 341.1526257991791 and batch: 250, loss is 4.5571318531036376 and perplexity is 95.30972514104388
At time: 341.8531641960144 and batch: 300, loss is 4.5336581897735595 and perplexity is 93.09851092307765
At time: 342.554404258728 and batch: 350, loss is 4.590288877487183 and perplexity is 98.52288709551883
At time: 343.2544279098511 and batch: 400, loss is 4.576564464569092 and perplexity is 97.17995486877642
At time: 343.9581730365753 and batch: 450, loss is 4.523206062316895 and perplexity is 92.13050111449529
At time: 344.6621832847595 and batch: 500, loss is 4.538717813491822 and perplexity is 93.57074802065819
At time: 345.366539478302 and batch: 550, loss is 4.58777195930481 and perplexity is 98.27522485315119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017592085168717 and perplexity of 151.04715686336604
Finished 39 epochs...
Completing Train Step...
At time: 347.230357170105 and batch: 50, loss is 4.645931758880615 and perplexity is 104.16037293725194
At time: 347.94647884368896 and batch: 100, loss is 4.6461295223236085 and perplexity is 104.1809740882376
At time: 348.6477870941162 and batch: 150, loss is 4.618458709716797 and perplexity is 101.33772085548927
At time: 349.34835743904114 and batch: 200, loss is 4.583953199386596 and perplexity is 97.90065102249248
At time: 350.05063128471375 and batch: 250, loss is 4.556875944137573 and perplexity is 95.28533764844927
At time: 350.7506387233734 and batch: 300, loss is 4.533455247879028 and perplexity is 93.07961925191351
At time: 351.4505410194397 and batch: 350, loss is 4.589969177246093 and perplexity is 98.49139433915072
At time: 352.15117502212524 and batch: 400, loss is 4.576278514862061 and perplexity is 97.15217026184182
At time: 352.8513038158417 and batch: 450, loss is 4.522977905273438 and perplexity is 92.10948328952236
At time: 353.5538647174835 and batch: 500, loss is 4.5385863304138185 and perplexity is 93.55844585947811
At time: 354.2537169456482 and batch: 550, loss is 4.587642230987549 and perplexity is 98.2624766005251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017556697764295 and perplexity of 151.04181179111401
Finished 40 epochs...
Completing Train Step...
At time: 356.09931683540344 and batch: 50, loss is 4.645600986480713 and perplexity is 104.12592525820558
At time: 356.8054850101471 and batch: 100, loss is 4.645848169326782 and perplexity is 104.15166658203624
At time: 357.5028431415558 and batch: 150, loss is 4.618102073669434 and perplexity is 101.3015866150439
At time: 358.20373725891113 and batch: 200, loss is 4.5836356830596925 and perplexity is 97.86957090186232
At time: 358.9060091972351 and batch: 250, loss is 4.556593446731568 and perplexity is 95.25842358948813
At time: 359.6109721660614 and batch: 300, loss is 4.533272085189819 and perplexity is 93.0625720997895
At time: 360.31471729278564 and batch: 350, loss is 4.589661512374878 and perplexity is 98.46109665800063
At time: 361.01943492889404 and batch: 400, loss is 4.575980129241944 and perplexity is 97.1231857757645
At time: 361.72277450561523 and batch: 450, loss is 4.522752819061279 and perplexity is 92.08875304795812
At time: 362.42650413513184 and batch: 500, loss is 4.5384360027313235 and perplexity is 93.54438249221742
At time: 363.130646944046 and batch: 550, loss is 4.587466230392456 and perplexity is 98.24518386797838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017523258290392 and perplexity of 151.0367611168368
Finished 41 epochs...
Completing Train Step...
At time: 365.0006263256073 and batch: 50, loss is 4.645214424133301 and perplexity is 104.08568187490081
At time: 365.72447443008423 and batch: 100, loss is 4.645547647476196 and perplexity is 104.12037143312695
At time: 366.42478251457214 and batch: 150, loss is 4.617708358764649 and perplexity is 101.26171052093659
At time: 367.12645864486694 and batch: 200, loss is 4.583306674957275 and perplexity is 97.83737631648565
At time: 367.828729391098 and batch: 250, loss is 4.556349658966065 and perplexity is 95.23520358174783
At time: 368.5297133922577 and batch: 300, loss is 4.533083858489991 and perplexity is 93.04505688743265
At time: 369.2300865650177 and batch: 350, loss is 4.589345941543579 and perplexity is 98.43003010998363
At time: 369.931410074234 and batch: 400, loss is 4.575685176849365 and perplexity is 97.09454328403848
At time: 370.63311672210693 and batch: 450, loss is 4.522516603469849 and perplexity is 92.06700281766572
At time: 371.3383364677429 and batch: 500, loss is 4.538281869888306 and perplexity is 93.5299653417023
At time: 372.0423424243927 and batch: 550, loss is 4.587315940856934 and perplexity is 98.23041975440124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017492740712267 and perplexity of 151.03215191101094
Finished 42 epochs...
Completing Train Step...
At time: 373.9095163345337 and batch: 50, loss is 4.644885721206665 and perplexity is 104.05147422903232
At time: 374.6244282722473 and batch: 100, loss is 4.6452649974822995 and perplexity is 104.09094596952635
At time: 375.3256480693817 and batch: 150, loss is 4.617363595962525 and perplexity is 101.22680526723182
At time: 376.0256841182709 and batch: 200, loss is 4.582994985580444 and perplexity is 97.80688619760069
At time: 376.72570633888245 and batch: 250, loss is 4.556042928695678 and perplexity is 95.20599654157354
At time: 377.4264934062958 and batch: 300, loss is 4.532842960357666 and perplexity is 93.02264520658083
At time: 378.12656593322754 and batch: 350, loss is 4.588945951461792 and perplexity is 98.39066694715183
At time: 378.82858443260193 and batch: 400, loss is 4.575373134613037 and perplexity is 97.0642504121904
At time: 379.5335762500763 and batch: 450, loss is 4.522241182327271 and perplexity is 92.04164911018977
At time: 380.23813223838806 and batch: 500, loss is 4.538069372177124 and perplexity is 93.51009254967646
At time: 380.94214367866516 and batch: 550, loss is 4.587067899703979 and perplexity is 98.20605758936478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.01732943920379 and perplexity of 151.00749014648176
Finished 43 epochs...
Completing Train Step...
At time: 382.8021137714386 and batch: 50, loss is 4.64450138092041 and perplexity is 104.01149073976687
At time: 383.51400780677795 and batch: 100, loss is 4.644900894165039 and perplexity is 104.05305300969684
At time: 384.2088737487793 and batch: 150, loss is 4.616943159103394 and perplexity is 101.18425473269872
At time: 384.9046766757965 and batch: 200, loss is 4.582636003494263 and perplexity is 97.77178157889216
At time: 385.60043382644653 and batch: 250, loss is 4.5557314395904545 and perplexity is 95.17634552912227
At time: 386.2974088191986 and batch: 300, loss is 4.5326278781890865 and perplexity is 93.00263984579813
At time: 386.9967486858368 and batch: 350, loss is 4.588625030517578 and perplexity is 98.35909638751119
At time: 387.69527220726013 and batch: 400, loss is 4.575060405731201 and perplexity is 97.03390036360848
At time: 388.3953263759613 and batch: 450, loss is 4.522003107070923 and perplexity is 92.01973887922856
At time: 389.0943489074707 and batch: 500, loss is 4.537913732528686 and perplexity is 93.49553980426812
At time: 389.79456424713135 and batch: 550, loss is 4.586921348571777 and perplexity is 98.1916664349817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017300869556183 and perplexity of 151.00317597732973
Finished 44 epochs...
Completing Train Step...
At time: 391.6556670665741 and batch: 50, loss is 4.644175090789795 and perplexity is 103.97755835307044
At time: 392.37149024009705 and batch: 100, loss is 4.6446139430999756 and perplexity is 104.02319915881463
At time: 393.07363986968994 and batch: 150, loss is 4.616606121063232 and perplexity is 101.15015753613736
At time: 393.7736585140228 and batch: 200, loss is 4.582319297790527 and perplexity is 97.74082160086137
At time: 394.47509145736694 and batch: 250, loss is 4.555459327697754 and perplexity is 95.1504504369411
At time: 395.1762726306915 and batch: 300, loss is 4.532438564300537 and perplexity is 92.98503482089399
At time: 395.8777587413788 and batch: 350, loss is 4.588316869735718 and perplexity is 98.3287906412264
At time: 396.5796582698822 and batch: 400, loss is 4.574760417938233 and perplexity is 97.0047957437289
At time: 397.28058886528015 and batch: 450, loss is 4.521773471832275 and perplexity is 91.99861033055325
At time: 397.98178911209106 and batch: 500, loss is 4.537776441574096 and perplexity is 93.48270459345783
At time: 398.681923866272 and batch: 550, loss is 4.586784782409668 and perplexity is 98.17825769155677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017263209566157 and perplexity of 150.99748930630918
Finished 45 epochs...
Completing Train Step...
At time: 400.5749821662903 and batch: 50, loss is 4.643843383789062 and perplexity is 103.94307398871435
At time: 401.30466771125793 and batch: 100, loss is 4.644311332702637 and perplexity is 103.99172541956531
At time: 402.02138209342957 and batch: 150, loss is 4.616224460601806 and perplexity is 101.11155988640574
At time: 402.71400022506714 and batch: 200, loss is 4.581971006393433 and perplexity is 97.70678524118253
At time: 403.4061794281006 and batch: 250, loss is 4.555200634002685 and perplexity is 95.12583879890514
At time: 404.10205078125 and batch: 300, loss is 4.532247867584228 and perplexity is 92.96730457069178
At time: 404.79793763160706 and batch: 350, loss is 4.588009691238403 and perplexity is 98.29859078968441
At time: 405.50695300102234 and batch: 400, loss is 4.57448715209961 and perplexity is 96.9782912684183
At time: 406.2076177597046 and batch: 450, loss is 4.521526527404785 and perplexity is 91.97589459127337
At time: 406.90826201438904 and batch: 500, loss is 4.537651195526123 and perplexity is 93.4709969873346
At time: 407.60948610305786 and batch: 550, loss is 4.586645832061768 and perplexity is 98.16461673622395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017227822161735 and perplexity of 150.992145991632
Finished 46 epochs...
Completing Train Step...
At time: 409.46061730384827 and batch: 50, loss is 4.643532247543335 and perplexity is 103.91073856152668
At time: 410.18201661109924 and batch: 100, loss is 4.6440110206604 and perplexity is 103.96050014102711
At time: 410.8818287849426 and batch: 150, loss is 4.615907373428345 and perplexity is 101.07950379023416
At time: 411.58376908302307 and batch: 200, loss is 4.581650743484497 and perplexity is 97.67549839219447
At time: 412.28521966934204 and batch: 250, loss is 4.5549443817138675 and perplexity is 95.10146570795077
At time: 412.9858272075653 and batch: 300, loss is 4.532079162597657 and perplexity is 92.95162184573671
At time: 413.6877405643463 and batch: 350, loss is 4.587711744308471 and perplexity is 98.26930738900832
At time: 414.3910686969757 and batch: 400, loss is 4.574211645126343 and perplexity is 96.95157675310493
At time: 415.0922272205353 and batch: 450, loss is 4.521279306411743 and perplexity is 91.9531590297464
At time: 415.7949950695038 and batch: 500, loss is 4.537518329620362 and perplexity is 93.45857870365901
At time: 416.49680042266846 and batch: 550, loss is 4.58649920463562 and perplexity is 98.15022416633168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0171872402759305 and perplexity of 150.98601856993795
Finished 47 epochs...
Completing Train Step...
At time: 418.3539021015167 and batch: 50, loss is 4.643199901580811 and perplexity is 103.8762099851372
At time: 419.05782532691956 and batch: 100, loss is 4.643714056015015 and perplexity is 103.92963213154923
At time: 419.7525579929352 and batch: 150, loss is 4.615507564544678 and perplexity is 101.03909938422056
At time: 420.44952869415283 and batch: 200, loss is 4.581313190460205 and perplexity is 97.64253329635984
At time: 421.1463141441345 and batch: 250, loss is 4.554663372039795 and perplexity is 95.0747450306286
At time: 421.84175634384155 and batch: 300, loss is 4.531888465881348 and perplexity is 92.93389796667167
At time: 422.5396363735199 and batch: 350, loss is 4.587367343902588 and perplexity is 98.23546922693048
At time: 423.2402665615082 and batch: 400, loss is 4.57392484664917 and perplexity is 96.92377517544831
At time: 423.94056940078735 and batch: 450, loss is 4.5210502815246585 and perplexity is 91.93210187928035
At time: 424.6421549320221 and batch: 500, loss is 4.537392349243164 and perplexity is 93.44680549827356
At time: 425.3429157733917 and batch: 550, loss is 4.586260347366333 and perplexity is 98.1267830714568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017112894261137 and perplexity of 150.97479377843217
Finished 48 epochs...
Completing Train Step...
At time: 427.2055821418762 and batch: 50, loss is 4.642878513336182 and perplexity is 103.84283075648455
At time: 427.9202265739441 and batch: 100, loss is 4.643435134887695 and perplexity is 103.90064800372386
At time: 428.6130561828613 and batch: 150, loss is 4.6151935005187985 and perplexity is 101.00737162043215
At time: 429.3053047657013 and batch: 200, loss is 4.580993928909302 and perplexity is 97.61136476546707
At time: 429.99721026420593 and batch: 250, loss is 4.554363746643066 and perplexity is 95.04626248968928
At time: 430.6909146308899 and batch: 300, loss is 4.531696119308472 and perplexity is 92.9160241689307
At time: 431.38690996170044 and batch: 350, loss is 4.587038555145264 and perplexity is 98.203175818224
At time: 432.08460807800293 and batch: 400, loss is 4.573616533279419 and perplexity is 96.89389688588972
At time: 432.7808756828308 and batch: 450, loss is 4.520809211730957 and perplexity is 91.90994249753189
At time: 433.47787284851074 and batch: 500, loss is 4.537236957550049 and perplexity is 93.43228576910288
At time: 434.17696142196655 and batch: 550, loss is 4.586106204986573 and perplexity is 98.11165874127599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.0170706890999 and perplexity of 150.96842199738
Finished 49 epochs...
Completing Train Step...
At time: 436.03680539131165 and batch: 50, loss is 4.642545747756958 and perplexity is 103.80828118553238
At time: 436.7536532878876 and batch: 100, loss is 4.643115863800049 and perplexity is 103.86748082577083
At time: 437.45621061325073 and batch: 150, loss is 4.614799861907959 and perplexity is 100.9676190435708
At time: 438.15911388397217 and batch: 200, loss is 4.58064712524414 and perplexity is 97.57751865572168
At time: 438.86180329322815 and batch: 250, loss is 4.554135446548462 and perplexity is 95.02456589573258
At time: 439.5646436214447 and batch: 300, loss is 4.531478862762452 and perplexity is 92.89583974712812
At time: 440.26708602905273 and batch: 350, loss is 4.586765193939209 and perplexity is 98.17633454849204
At time: 440.9712290763855 and batch: 400, loss is 4.573351049423218 and perplexity is 96.86817653482217
At time: 441.6739294528961 and batch: 450, loss is 4.520563497543335 and perplexity is 91.88736169500207
At time: 442.37686824798584 and batch: 500, loss is 4.53707200050354 and perplexity is 93.41687472630878
At time: 443.0797564983368 and batch: 550, loss is 4.585961828231811 and perplexity is 98.09749472088492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.017045366003158 and perplexity of 150.9645990578293
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23cfa37a20>
SETTINGS FOR THIS RUN
{'lr': 26.571065701167413, 'seq_len': 35, 'dropout': 0.2664040303606525, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.7148639865914515, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9224278926849365 and batch: 50, loss is 6.305460119247437 and perplexity is 547.5534701231759
At time: 1.5986287593841553 and batch: 100, loss is 5.627324714660644 and perplexity is 277.9176132799371
At time: 2.2610268592834473 and batch: 150, loss is 5.554070072174072 and perplexity is 258.2866648505053
At time: 2.923535108566284 and batch: 200, loss is 5.446051816940308 and perplexity is 231.841005809404
At time: 3.5866456031799316 and batch: 250, loss is 5.4056771373748775 and perplexity is 222.66694555825336
At time: 4.26286506652832 and batch: 300, loss is 5.395068683624268 and perplexity is 220.3172787660346
At time: 4.9254310131073 and batch: 350, loss is 5.420953044891357 and perplexity is 226.09449806078405
At time: 5.587875843048096 and batch: 400, loss is 5.424063377380371 and perplexity is 226.7988218965321
At time: 6.251291751861572 and batch: 450, loss is 5.335542573928833 and perplexity is 207.58534865649446
At time: 6.914007663726807 and batch: 500, loss is 5.36096794128418 and perplexity is 212.93095139165032
At time: 7.576460123062134 and batch: 550, loss is 5.405797262191772 and perplexity is 222.6936949909202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.320431648416722 and perplexity of 204.4721230152125
Finished 1 epochs...
Completing Train Step...
At time: 9.350886106491089 and batch: 50, loss is 5.26920371055603 and perplexity is 194.26121268773448
At time: 10.021267414093018 and batch: 100, loss is 5.293513317108154 and perplexity is 199.0414943098221
At time: 10.677686929702759 and batch: 150, loss is 5.2734084320068355 and perplexity is 195.07974662342957
At time: 11.33384656906128 and batch: 200, loss is 5.151537141799927 and perplexity is 172.6967457836479
At time: 11.989736080169678 and batch: 250, loss is 5.182227411270142 and perplexity is 178.07902479080713
At time: 12.646429300308228 and batch: 300, loss is 5.164992742538452 and perplexity is 175.03618824878222
At time: 13.302189350128174 and batch: 350, loss is 5.220760154724121 and perplexity is 185.07481610863837
At time: 13.964694738388062 and batch: 400, loss is 5.226736392974853 and perplexity is 186.1841789103406
At time: 14.63917851448059 and batch: 450, loss is 5.173513059616089 and perplexity is 176.53392360201264
At time: 15.329040050506592 and batch: 500, loss is 5.206324052810669 and perplexity is 182.4222496280114
At time: 16.03035616874695 and batch: 550, loss is 5.231357126235962 and perplexity is 187.04647702906692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.3464835958277925 and perplexity of 209.8690145406164
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 17.901727437973022 and batch: 50, loss is 5.129900817871094 and perplexity is 169.0003553985891
At time: 18.625726461410522 and batch: 100, loss is 5.090039577484131 and perplexity is 162.39628916286117
At time: 19.336432695388794 and batch: 150, loss is 5.080212602615356 and perplexity is 160.80824054712002
At time: 20.04741096496582 and batch: 200, loss is 4.990033416748047 and perplexity is 146.941333715179
At time: 20.75827646255493 and batch: 250, loss is 4.967439727783203 and perplexity is 143.65861097779157
At time: 21.481964588165283 and batch: 300, loss is 4.949568662643433 and perplexity is 141.11408301677366
At time: 22.19235348701477 and batch: 350, loss is 4.991439247131348 and perplexity is 147.14805357916532
At time: 22.90315532684326 and batch: 400, loss is 4.977929468154907 and perplexity is 145.17348393040828
At time: 23.613386631011963 and batch: 450, loss is 4.8925942611694335 and perplexity is 133.29893813439935
At time: 24.32403039932251 and batch: 500, loss is 4.91931866645813 and perplexity is 136.90930050292158
At time: 25.033879041671753 and batch: 550, loss is 4.988596611022949 and perplexity is 146.7303591666579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.17434465124252 and perplexity of 176.68078879212862
Finished 3 epochs...
Completing Train Step...
At time: 26.896262884140015 and batch: 50, loss is 4.97636962890625 and perplexity is 144.94721315120864
At time: 27.610299348831177 and batch: 100, loss is 4.95029806137085 and perplexity is 141.21704899640895
At time: 28.310259342193604 and batch: 150, loss is 4.9740486907958985 and perplexity is 144.61118973583174
At time: 29.011176109313965 and batch: 200, loss is 4.8937865829467775 and perplexity is 133.45796814986898
At time: 29.71239995956421 and batch: 250, loss is 4.880463485717773 and perplexity is 131.69168695913137
At time: 30.41342043876648 and batch: 300, loss is 4.85940242767334 and perplexity is 128.9471238469408
At time: 31.11427354812622 and batch: 350, loss is 4.902859239578247 and perplexity is 134.6742957878351
At time: 31.815739154815674 and batch: 400, loss is 4.901017322540283 and perplexity is 134.42646521958775
At time: 32.518797159194946 and batch: 450, loss is 4.8210521411895755 and perplexity is 124.09558819671815
At time: 33.223262310028076 and batch: 500, loss is 4.859668521881104 and perplexity is 128.9814404952325
At time: 33.928893089294434 and batch: 550, loss is 4.927287559509278 and perplexity is 138.00467473976252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.160747771567487 and perplexity of 174.2947395383188
Finished 4 epochs...
Completing Train Step...
At time: 35.79258990287781 and batch: 50, loss is 4.936272602081299 and perplexity is 139.25023996609815
At time: 36.5055570602417 and batch: 100, loss is 4.909400062561035 and perplexity is 135.55806365068497
At time: 37.20216202735901 and batch: 150, loss is 4.899849710464477 and perplexity is 134.2695988528431
At time: 37.899402141571045 and batch: 200, loss is 4.822271137237549 and perplexity is 124.24695246578048
At time: 38.60268974304199 and batch: 250, loss is 4.797517004013062 and perplexity is 121.20908190204608
At time: 39.32248902320862 and batch: 300, loss is 4.779279766082763 and perplexity is 119.0185979367164
At time: 40.0271360874176 and batch: 350, loss is 4.838713188171386 and perplexity is 126.30671413678323
At time: 40.73321318626404 and batch: 400, loss is 4.853309898376465 and perplexity is 128.16389805290973
At time: 41.437779664993286 and batch: 450, loss is 4.785338096618652 and perplexity is 119.74184055222759
At time: 42.14405274391174 and batch: 500, loss is 4.81691689491272 and perplexity is 123.5834819501406
At time: 42.8480019569397 and batch: 550, loss is 4.875346488952637 and perplexity is 131.01954217091966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.1412668431058846 and perplexity of 170.9321754634921
Finished 5 epochs...
Completing Train Step...
At time: 44.716808795928955 and batch: 50, loss is 4.8869907188415525 and perplexity is 132.554080761234
At time: 45.42795920372009 and batch: 100, loss is 4.869712657928467 and perplexity is 130.2834755937221
At time: 46.125876665115356 and batch: 150, loss is 4.873821067810058 and perplexity is 130.81983454904068
At time: 46.8221652507782 and batch: 200, loss is 4.8209804439544675 and perplexity is 124.086691205104
At time: 47.51786494255066 and batch: 250, loss is 4.79535216331482 and perplexity is 120.94696736901972
At time: 48.215463399887085 and batch: 300, loss is 4.764657783508301 and perplexity is 117.290971514148
At time: 48.910584449768066 and batch: 350, loss is 4.804809417724609 and perplexity is 122.09621942836068
At time: 49.608848333358765 and batch: 400, loss is 4.814594049453735 and perplexity is 123.29674976648712
At time: 50.30457615852356 and batch: 450, loss is 4.751257991790771 and perplexity is 115.72978010519863
At time: 51.003220081329346 and batch: 500, loss is 4.776868553161621 and perplexity is 118.73196446155725
At time: 51.706512689590454 and batch: 550, loss is 4.832916269302368 and perplexity is 125.57664248983212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.13836118008228 and perplexity of 170.43622504304005
Finished 6 epochs...
Completing Train Step...
At time: 53.57445812225342 and batch: 50, loss is 4.855690240859985 and perplexity is 128.46933540283055
At time: 54.290406227111816 and batch: 100, loss is 4.844062986373902 and perplexity is 126.98424026715675
At time: 54.991315603256226 and batch: 150, loss is 4.855415229797363 and perplexity is 128.43400977207935
At time: 55.69231963157654 and batch: 200, loss is 4.779540348052978 and perplexity is 119.04961607866774
At time: 56.39371061325073 and batch: 250, loss is 4.766902770996094 and perplexity is 117.55458407035692
At time: 57.11026334762573 and batch: 300, loss is 4.7455133438110355 and perplexity is 115.0668592044144
At time: 57.81163692474365 and batch: 350, loss is 4.768133945465088 and perplexity is 117.6994034036175
At time: 58.51317596435547 and batch: 400, loss is 4.782419757843018 and perplexity is 119.39290270295602
At time: 59.21469497680664 and batch: 450, loss is 4.73249831199646 and perplexity is 113.578963876534
At time: 59.916133642196655 and batch: 500, loss is 4.764431095123291 and perplexity is 117.26438602666366
At time: 60.618040561676025 and batch: 550, loss is 4.8087877368927 and perplexity is 122.5829246508976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.123730273956948 and perplexity of 167.96074207259414
Finished 7 epochs...
Completing Train Step...
At time: 62.458622217178345 and batch: 50, loss is 4.808380041122437 and perplexity is 122.5329582972385
At time: 63.15918970108032 and batch: 100, loss is 4.799483156204223 and perplexity is 121.44763183991002
At time: 63.850866079330444 and batch: 150, loss is 4.814528102874756 and perplexity is 123.2886190357407
At time: 64.54489731788635 and batch: 200, loss is 4.750640897750855 and perplexity is 115.65838597836556
At time: 65.23976397514343 and batch: 250, loss is 4.736492118835449 and perplexity is 114.03348334656266
At time: 65.93871355056763 and batch: 300, loss is 4.728252353668213 and perplexity is 113.09773469145479
At time: 66.6395833492279 and batch: 350, loss is 4.740095272064209 and perplexity is 114.44510458213156
At time: 67.34030103683472 and batch: 400, loss is 4.7400227069854735 and perplexity is 114.43680016541566
At time: 68.03958630561829 and batch: 450, loss is 4.6964594268798825 and perplexity is 109.55858476619782
At time: 68.74300384521484 and batch: 500, loss is 4.732249965667725 and perplexity is 113.55076046008611
At time: 69.44692826271057 and batch: 550, loss is 4.783127822875977 and perplexity is 119.47747057878813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.138209241501828 and perplexity of 170.4103311721382
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 71.30610060691833 and batch: 50, loss is 4.765851583480835 and perplexity is 117.43107708509409
At time: 72.016530752182 and batch: 100, loss is 4.71693772315979 and perplexity is 111.82528782012665
At time: 72.71155071258545 and batch: 150, loss is 4.689665594100952 and perplexity is 108.81678474765877
At time: 73.40964365005493 and batch: 200, loss is 4.619872894287109 and perplexity is 101.4811324780647
At time: 74.10535454750061 and batch: 250, loss is 4.608003444671631 and perplexity is 100.28372761542056
At time: 74.81603956222534 and batch: 300, loss is 4.60313045501709 and perplexity is 99.79623478665523
At time: 75.51318454742432 and batch: 350, loss is 4.626226997375488 and perplexity is 102.12800703271677
At time: 76.2129898071289 and batch: 400, loss is 4.608278293609619 and perplexity is 100.31129427961375
At time: 76.91209697723389 and batch: 450, loss is 4.548419599533081 and perplexity is 94.48296933046196
At time: 77.61208319664001 and batch: 500, loss is 4.573315782546997 and perplexity is 96.86476035706985
At time: 78.31145334243774 and batch: 550, loss is 4.624539041519165 and perplexity is 101.95576487462702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.045455283307015 and perplexity of 155.31499542847988
Finished 9 epochs...
Completing Train Step...
At time: 80.16733694076538 and batch: 50, loss is 4.69267086982727 and perplexity is 109.14430108141889
At time: 80.88389587402344 and batch: 100, loss is 4.681630258560181 and perplexity is 107.94590894517407
At time: 81.58410215377808 and batch: 150, loss is 4.6565431880950925 and perplexity is 105.27154851538897
At time: 82.28562593460083 and batch: 200, loss is 4.594812726974487 and perplexity is 98.9695994757413
At time: 82.9852876663208 and batch: 250, loss is 4.585554790496826 and perplexity is 98.05757346410618
At time: 83.68497943878174 and batch: 300, loss is 4.581343898773193 and perplexity is 97.64553177987224
At time: 84.38549089431763 and batch: 350, loss is 4.606187467575073 and perplexity is 100.10177991933976
At time: 85.09014225006104 and batch: 400, loss is 4.594377698898316 and perplexity is 98.92655428489408
At time: 85.79470348358154 and batch: 450, loss is 4.533904275894165 and perplexity is 93.12142399364356
At time: 86.49819350242615 and batch: 500, loss is 4.5623862743377686 and perplexity is 95.81184059244993
At time: 87.20037007331848 and batch: 550, loss is 4.617421703338623 and perplexity is 101.23268746217452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.043140167885638 and perplexity of 154.95583919187928
Finished 10 epochs...
Completing Train Step...
At time: 89.06288385391235 and batch: 50, loss is 4.662889652252197 and perplexity is 105.94177515990076
At time: 89.77879118919373 and batch: 100, loss is 4.655837707519531 and perplexity is 105.19730767356715
At time: 90.47909498214722 and batch: 150, loss is 4.631571788787841 and perplexity is 102.67532126495298
At time: 91.17878246307373 and batch: 200, loss is 4.574147024154663 and perplexity is 96.94531185043351
At time: 91.88003039360046 and batch: 250, loss is 4.566828746795654 and perplexity is 96.23842890726111
At time: 92.59463834762573 and batch: 300, loss is 4.56316689491272 and perplexity is 95.8866624864958
At time: 93.30202913284302 and batch: 350, loss is 4.589815826416015 and perplexity is 98.47629176009939
At time: 94.00458335876465 and batch: 400, loss is 4.5807844829559325 and perplexity is 97.59092260095314
At time: 94.70855188369751 and batch: 450, loss is 4.525465888977051 and perplexity is 92.33893550121635
At time: 95.41160416603088 and batch: 500, loss is 4.552325344085693 and perplexity is 94.85271727370957
At time: 96.1180465221405 and batch: 550, loss is 4.611314172744751 and perplexity is 100.61628997571945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 5.043721625145445 and perplexity of 155.04596558931172
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 97.99011707305908 and batch: 50, loss is 4.627171058654785 and perplexity is 102.22446765489306
At time: 98.70531010627747 and batch: 100, loss is 4.599810342788697 and perplexity is 99.46544951316598
At time: 99.40426445007324 and batch: 150, loss is 4.570804634094238 and perplexity is 96.62182371661724
At time: 100.10501623153687 and batch: 200, loss is 4.507094230651855 and perplexity is 90.65800415468475
At time: 100.80531072616577 and batch: 250, loss is 4.491113357543945 and perplexity is 89.2207251691732
At time: 101.50612831115723 and batch: 300, loss is 4.476750154495239 and perplexity is 87.9483890620151
At time: 102.20617961883545 and batch: 350, loss is 4.500472841262817 and perplexity is 90.05970517911115
At time: 102.907057762146 and batch: 400, loss is 4.485330257415772 and perplexity is 88.70624187010279
At time: 103.60794591903687 and batch: 450, loss is 4.4249233150482175 and perplexity is 83.50640329199183
At time: 104.30704426765442 and batch: 500, loss is 4.450965433120728 and perplexity is 85.70965100758416
At time: 105.00725150108337 and batch: 550, loss is 4.517337980270386 and perplexity is 91.59145490546977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9826341994265295 and perplexity of 145.85809535365834
Finished 12 epochs...
Completing Train Step...
At time: 106.85884642601013 and batch: 50, loss is 4.580447454452514 and perplexity is 97.55803722032832
At time: 107.57046723365784 and batch: 100, loss is 4.571889324188232 and perplexity is 96.7266853125522
At time: 108.26969170570374 and batch: 150, loss is 4.549260997772217 and perplexity is 94.56250058852319
At time: 108.96911787986755 and batch: 200, loss is 4.486826333999634 and perplexity is 88.83905252408994
At time: 109.66889691352844 and batch: 250, loss is 4.476384162902832 and perplexity is 87.9162065806707
At time: 110.38397431373596 and batch: 300, loss is 4.464187068939209 and perplexity is 86.85039744840385
At time: 111.08474087715149 and batch: 350, loss is 4.49115138053894 and perplexity is 89.22411767285595
At time: 111.78500747680664 and batch: 400, loss is 4.4831809806823735 and perplexity is 88.51579234602308
At time: 112.48380661010742 and batch: 450, loss is 4.426813297271728 and perplexity is 83.66437814757315
At time: 113.18307852745056 and batch: 500, loss is 4.4528309345245365 and perplexity is 85.86969171352649
At time: 113.88302874565125 and batch: 550, loss is 4.515299348831177 and perplexity is 91.4049238845406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.981344994078291 and perplexity of 145.67017547672737
Finished 13 epochs...
Completing Train Step...
At time: 115.73607754707336 and batch: 50, loss is 4.566657419204712 and perplexity is 96.22194202144995
At time: 116.4466803073883 and batch: 100, loss is 4.559833240509033 and perplexity is 95.56754170657861
At time: 117.14282584190369 and batch: 150, loss is 4.538577728271484 and perplexity is 93.55764105987176
At time: 117.84369826316833 and batch: 200, loss is 4.4767239379882815 and perplexity is 87.9460833926848
At time: 118.54474067687988 and batch: 250, loss is 4.4674837684631346 and perplexity is 87.13718958633358
At time: 119.24992799758911 and batch: 300, loss is 4.455895490646363 and perplexity is 86.13324783794809
At time: 119.9589593410492 and batch: 350, loss is 4.48484203338623 and perplexity is 88.66294392166097
At time: 120.66899871826172 and batch: 400, loss is 4.4797111511230465 and perplexity is 88.20918987005783
At time: 121.37923312187195 and batch: 450, loss is 4.424119329452514 and perplexity is 83.43929232833514
At time: 122.0902783870697 and batch: 500, loss is 4.44998682975769 and perplexity is 85.625816282026
At time: 122.80009055137634 and batch: 550, loss is 4.510753698348999 and perplexity is 90.99037196609687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.980944369701629 and perplexity of 145.61182814194154
Finished 14 epochs...
Completing Train Step...
At time: 124.67134809494019 and batch: 50, loss is 4.556343822479248 and perplexity is 95.23464774435972
At time: 125.38794159889221 and batch: 100, loss is 4.551110115051269 and perplexity is 94.73751950769639
At time: 126.09130144119263 and batch: 150, loss is 4.530451898574829 and perplexity is 92.80048801631534
At time: 126.79350543022156 and batch: 200, loss is 4.4686455821990965 and perplexity is 87.23848560226524
At time: 127.49808192253113 and batch: 250, loss is 4.460541114807129 and perplexity is 86.534321432415
At time: 128.21679830551147 and batch: 300, loss is 4.448791246414185 and perplexity is 85.52350465553062
At time: 128.91739201545715 and batch: 350, loss is 4.478664722442627 and perplexity is 88.11693352216389
At time: 129.62160897254944 and batch: 400, loss is 4.47534439086914 and perplexity is 87.82484127553587
At time: 130.32369136810303 and batch: 450, loss is 4.420378818511963 and perplexity is 83.12776973256823
At time: 131.02592206001282 and batch: 500, loss is 4.446118116378784 and perplexity is 85.29519449383002
At time: 131.72958493232727 and batch: 550, loss is 4.505945825576783 and perplexity is 90.5539518011786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.981466739735705 and perplexity of 145.68791126761226
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 133.58748841285706 and batch: 50, loss is 4.544375143051147 and perplexity is 94.10160879062072
At time: 134.2967607975006 and batch: 100, loss is 4.531987752914429 and perplexity is 92.94312555575577
At time: 134.99099206924438 and batch: 150, loss is 4.504398174285889 and perplexity is 90.41391425337558
At time: 135.68614721298218 and batch: 200, loss is 4.441799898147583 and perplexity is 84.92766533659628
At time: 136.38188767433167 and batch: 250, loss is 4.429203252792359 and perplexity is 83.86457142122674
At time: 137.07665848731995 and batch: 300, loss is 4.4117629337310795 and perplexity is 82.41462703569758
At time: 137.77135133743286 and batch: 350, loss is 4.438756771087647 and perplexity is 84.66961250299784
At time: 138.46626019477844 and batch: 400, loss is 4.4358272457122805 and perplexity is 84.42193369246213
At time: 139.16043782234192 and batch: 450, loss is 4.374896650314331 and perplexity is 79.43162989405411
At time: 139.8575940132141 and batch: 500, loss is 4.400326452255249 and perplexity is 81.47746282536143
At time: 140.55873155593872 and batch: 550, loss is 4.463250579833985 and perplexity is 86.7691010699369
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.958765232816655 and perplexity of 142.41783441535512
Finished 16 epochs...
Completing Train Step...
At time: 142.4139747619629 and batch: 50, loss is 4.52656623840332 and perplexity is 92.44059651698029
At time: 143.13009071350098 and batch: 100, loss is 4.519033212661743 and perplexity is 91.74685538934797
At time: 143.8310010433197 and batch: 150, loss is 4.494802389144898 and perplexity is 89.55047109138785
At time: 144.5300714969635 and batch: 200, loss is 4.433936719894409 and perplexity is 84.2624826178995
At time: 145.2309217453003 and batch: 250, loss is 4.42434100151062 and perplexity is 83.45779053818484
At time: 145.94559454917908 and batch: 300, loss is 4.407138071060181 and perplexity is 82.03435074421847
At time: 146.64587903022766 and batch: 350, loss is 4.437797899246216 and perplexity is 84.58846410741717
At time: 147.34741258621216 and batch: 400, loss is 4.4359861087799075 and perplexity is 84.43534628517813
At time: 148.0457067489624 and batch: 450, loss is 4.375481252670288 and perplexity is 79.47807938794658
At time: 148.74598217010498 and batch: 500, loss is 4.402038040161133 and perplexity is 81.61703807891922
At time: 149.44606590270996 and batch: 550, loss is 4.463390207290649 and perplexity is 86.78121726469338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.957786722386137 and perplexity of 142.27854523795824
Finished 17 epochs...
Completing Train Step...
At time: 151.2950804233551 and batch: 50, loss is 4.520321922302246 and perplexity is 91.86516666444048
At time: 152.00498151779175 and batch: 100, loss is 4.513209791183471 and perplexity is 91.21412743631058
At time: 152.69977641105652 and batch: 150, loss is 4.48993878364563 and perplexity is 89.11599035563107
At time: 153.39850735664368 and batch: 200, loss is 4.430175561904907 and perplexity is 83.94615336324516
At time: 154.10227370262146 and batch: 250, loss is 4.42132061958313 and perplexity is 83.20609643344194
At time: 154.80853247642517 and batch: 300, loss is 4.404510631561279 and perplexity is 81.81909336251458
At time: 155.5161955356598 and batch: 350, loss is 4.437273578643799 and perplexity is 84.54412425812288
At time: 156.22294116020203 and batch: 400, loss is 4.4356896114349365 and perplexity is 84.41031514020025
At time: 156.92899250984192 and batch: 450, loss is 4.375221033096313 and perplexity is 79.45740032665286
At time: 157.63554859161377 and batch: 500, loss is 4.402262287139893 and perplexity is 81.63534250540317
At time: 158.34329509735107 and batch: 550, loss is 4.462331476211548 and perplexity is 86.6893879127652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.9576052401928195 and perplexity of 142.25272655839257
Finished 18 epochs...
Completing Train Step...
At time: 160.20729112625122 and batch: 50, loss is 4.515678157806397 and perplexity is 91.43955544904694
At time: 160.91747331619263 and batch: 100, loss is 4.509015254974365 and perplexity is 90.83232777204117
At time: 161.61173510551453 and batch: 150, loss is 4.4861734580993655 and perplexity is 88.7810705772617
At time: 162.3070363998413 and batch: 200, loss is 4.427493801116944 and perplexity is 83.72133145488458
At time: 163.00198101997375 and batch: 250, loss is 4.418706607818604 and perplexity is 82.98887874696695
At time: 163.7124993801117 and batch: 300, loss is 4.402052888870239 and perplexity is 81.61824999557342
At time: 164.40794038772583 and batch: 350, loss is 4.436339807510376 and perplexity is 84.4652162421374
At time: 165.1074686050415 and batch: 400, loss is 4.435049810409546 and perplexity is 84.35632660682118
At time: 165.81092858314514 and batch: 450, loss is 4.37437255859375 and perplexity is 79.39001134139644
At time: 166.51346397399902 and batch: 500, loss is 4.40167200088501 and perplexity is 81.5871685044337
At time: 167.2159562110901 and batch: 550, loss is 4.460997314453125 and perplexity is 86.57380736526842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.957480572639628 and perplexity of 142.2349933644376
Finished 19 epochs...
Completing Train Step...
At time: 169.08763456344604 and batch: 50, loss is 4.511772108078003 and perplexity is 91.08308464788826
At time: 169.802232503891 and batch: 100, loss is 4.505449275970459 and perplexity is 90.50899843377282
At time: 170.50216436386108 and batch: 150, loss is 4.48302375793457 and perplexity is 88.50187674387973
At time: 171.20209670066833 and batch: 200, loss is 4.425258703231812 and perplexity is 83.53441505005418
At time: 171.90262699127197 and batch: 250, loss is 4.416305065155029 and perplexity is 82.7898165379205
At time: 172.60354781150818 and batch: 300, loss is 4.399817934036255 and perplexity is 81.43604058395216
At time: 173.30417203903198 and batch: 350, loss is 4.435484485626221 and perplexity is 84.39300218177118
At time: 174.00423979759216 and batch: 400, loss is 4.434185266494751 and perplexity is 84.283428374381
At time: 174.70401310920715 and batch: 450, loss is 4.373331756591797 and perplexity is 79.30742504410426
At time: 175.40574955940247 and batch: 500, loss is 4.40066593170166 and perplexity is 81.5051274448555
At time: 176.10617446899414 and batch: 550, loss is 4.45909613609314 and perplexity is 86.40937147666976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.957331555954953 and perplexity of 142.21379955643644
Finished 20 epochs...
Completing Train Step...
At time: 177.96330213546753 and batch: 50, loss is 4.508029365539551 and perplexity is 90.74282126875899
At time: 178.6761429309845 and batch: 100, loss is 4.502075538635254 and perplexity is 90.20415935921075
At time: 179.37413954734802 and batch: 150, loss is 4.4798275661468505 and perplexity is 88.21945934274514
At time: 180.0737109184265 and batch: 200, loss is 4.422652053833008 and perplexity is 83.31695366321178
At time: 180.77378582954407 and batch: 250, loss is 4.4139078330993655 and perplexity is 82.59158783086914
At time: 181.48801159858704 and batch: 300, loss is 4.397681941986084 and perplexity is 81.26227949088901
At time: 182.1881411075592 and batch: 350, loss is 4.434166402816772 and perplexity is 84.28183849392474
At time: 182.88870120048523 and batch: 400, loss is 4.433091068267823 and perplexity is 84.19125603309678
At time: 183.5922863483429 and batch: 450, loss is 4.37208927154541 and perplexity is 79.20894794524973
At time: 184.29540014266968 and batch: 500, loss is 4.3995758819580075 and perplexity is 81.41633120652868
At time: 184.99901294708252 and batch: 550, loss is 4.457352247238159 and perplexity is 86.25881445229962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.957348762674535 and perplexity of 142.21624661045888
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 186.85358381271362 and batch: 50, loss is 4.502611474990845 and perplexity is 90.25251600452151
At time: 187.56453442573547 and batch: 100, loss is 4.4948398208618165 and perplexity is 89.55382318200857
At time: 188.2595465183258 and batch: 150, loss is 4.470264549255371 and perplexity is 87.37983622662274
At time: 188.95741438865662 and batch: 200, loss is 4.412781114578247 and perplexity is 82.4985827642762
At time: 189.6548240184784 and batch: 250, loss is 4.40056866645813 and perplexity is 81.49720021431393
At time: 190.35455203056335 and batch: 300, loss is 4.383315629959107 and perplexity is 80.10318611173672
At time: 191.0549795627594 and batch: 350, loss is 4.420601434707642 and perplexity is 83.14627738039603
At time: 191.75345087051392 and batch: 400, loss is 4.414806451797485 and perplexity is 82.66583953298868
At time: 192.4507007598877 and batch: 450, loss is 4.3517485618591305 and perplexity is 77.61405732348459
At time: 193.15006351470947 and batch: 500, loss is 4.379705057144165 and perplexity is 79.81448921977311
At time: 193.84832310676575 and batch: 550, loss is 4.439706192016602 and perplexity is 84.750037777833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.954047669755652 and perplexity of 141.74755159434483
Finished 22 epochs...
Completing Train Step...
At time: 195.6971538066864 and batch: 50, loss is 4.496461172103881 and perplexity is 89.6991391567408
At time: 196.40877413749695 and batch: 100, loss is 4.490508108139038 and perplexity is 89.16674071703599
At time: 197.1086461544037 and batch: 150, loss is 4.467161512374878 and perplexity is 87.1091136205442
At time: 197.80629086494446 and batch: 200, loss is 4.4096370029449465 and perplexity is 82.23960535076756
At time: 198.50606560707092 and batch: 250, loss is 4.398906526565551 and perplexity is 81.36185298088266
At time: 199.21973705291748 and batch: 300, loss is 4.38207857131958 and perplexity is 80.00415503955435
At time: 199.91916871070862 and batch: 350, loss is 4.419961204528809 and perplexity is 83.09306166133187
At time: 200.61916255950928 and batch: 400, loss is 4.413883943557739 and perplexity is 82.58961477926138
At time: 201.31985521316528 and batch: 450, loss is 4.352070751190186 and perplexity is 77.63906777352787
At time: 202.02501368522644 and batch: 500, loss is 4.380859804153443 and perplexity is 79.90670799692784
At time: 202.72809052467346 and batch: 550, loss is 4.440615606307984 and perplexity is 84.82714572960572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.953272393409242 and perplexity of 141.63770065835058
Finished 23 epochs...
Completing Train Step...
At time: 204.5923194885254 and batch: 50, loss is 4.493564214706421 and perplexity is 89.43966060264302
At time: 205.31280541419983 and batch: 100, loss is 4.488128061294556 and perplexity is 88.95477204490443
At time: 206.0171835422516 and batch: 150, loss is 4.465831632614136 and perplexity is 86.99334596895453
At time: 206.7208104133606 and batch: 200, loss is 4.4080939292907715 and perplexity is 82.1128014414677
At time: 207.42429852485657 and batch: 250, loss is 4.397833290100098 and perplexity is 81.2745793143845
At time: 208.12938261032104 and batch: 300, loss is 4.381363344192505 and perplexity is 79.94695435577013
At time: 208.83510565757751 and batch: 350, loss is 4.419658937454224 and perplexity is 83.06794916019751
At time: 209.54029512405396 and batch: 400, loss is 4.413100509643555 and perplexity is 82.52493661292655
At time: 210.24426341056824 and batch: 450, loss is 4.352124977111816 and perplexity is 77.64327793768139
At time: 210.94817876815796 and batch: 500, loss is 4.381319379806518 and perplexity is 79.94343961427265
At time: 211.6532232761383 and batch: 550, loss is 4.44090744972229 and perplexity is 84.85190558626611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.952990592794215 and perplexity of 141.5977926905031
Finished 24 epochs...
Completing Train Step...
At time: 213.50888848304749 and batch: 50, loss is 4.491239719390869 and perplexity is 89.23199997712724
At time: 214.2186677455902 and batch: 100, loss is 4.486182737350464 and perplexity is 88.78189440293059
At time: 214.91301894187927 and batch: 150, loss is 4.464843921661377 and perplexity is 86.9074641085329
At time: 215.61048650741577 and batch: 200, loss is 4.406890602111816 and perplexity is 82.01405230143077
At time: 216.31057858467102 and batch: 250, loss is 4.39694808959961 and perplexity is 81.20266684926922
At time: 217.02501606941223 and batch: 300, loss is 4.380749368667603 and perplexity is 79.89788394806136
At time: 217.7240846157074 and batch: 350, loss is 4.419467506408691 and perplexity is 83.05204889779002
At time: 218.42284202575684 and batch: 400, loss is 4.412298059463501 and perplexity is 82.45874102556513
At time: 219.12260055541992 and batch: 450, loss is 4.352000827789307 and perplexity is 77.63363917566328
At time: 219.82275462150574 and batch: 500, loss is 4.381657447814941 and perplexity is 79.97047050257137
At time: 220.5232708454132 and batch: 550, loss is 4.440993690490723 and perplexity is 84.85922359535772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.952906507126828 and perplexity of 141.58588684616663
Finished 25 epochs...
Completing Train Step...
At time: 222.38573288917542 and batch: 50, loss is 4.4894509029388425 and perplexity is 89.07252298757766
At time: 223.09989523887634 and batch: 100, loss is 4.484722967147827 and perplexity is 88.6523877868946
At time: 223.80108618736267 and batch: 150, loss is 4.463906726837158 and perplexity is 86.82605303796026
At time: 224.50008606910706 and batch: 200, loss is 4.405938186645508 and perplexity is 81.93597803503542
At time: 225.20002341270447 and batch: 250, loss is 4.396150140762329 and perplexity is 81.13789712057434
At time: 225.90090417861938 and batch: 300, loss is 4.38019645690918 and perplexity is 79.85371967915026
At time: 226.6001226902008 and batch: 350, loss is 4.419196157455445 and perplexity is 83.02951586855183
At time: 227.30213451385498 and batch: 400, loss is 4.411628370285034 and perplexity is 82.4035377855997
At time: 228.00283908843994 and batch: 450, loss is 4.351780300140381 and perplexity is 77.61652069935634
At time: 228.70347476005554 and batch: 500, loss is 4.381617765426636 and perplexity is 79.96729714627155
At time: 229.40311574935913 and batch: 550, loss is 4.4408440589904785 and perplexity is 84.84652693235562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.952704247007978 and perplexity of 141.55725256374996
Finished 26 epochs...
Completing Train Step...
At time: 231.2572066783905 and batch: 50, loss is 4.487656621932984 and perplexity is 88.91284514773415
At time: 231.964745759964 and batch: 100, loss is 4.4832109928131105 and perplexity is 88.51844893341998
At time: 232.65783500671387 and batch: 150, loss is 4.463188009262085 and perplexity is 86.76367204751001
At time: 233.35532999038696 and batch: 200, loss is 4.404954252243042 and perplexity is 81.85539805665444
At time: 234.05496978759766 and batch: 250, loss is 4.395383024215699 and perplexity is 81.07567876455066
At time: 234.76887965202332 and batch: 300, loss is 4.379648313522339 and perplexity is 79.80996038507303
At time: 235.46850538253784 and batch: 350, loss is 4.419059829711914 and perplexity is 83.0181974135343
At time: 236.17364144325256 and batch: 400, loss is 4.411021671295166 and perplexity is 82.35355880509513
At time: 236.8821096420288 and batch: 450, loss is 4.3514750289916995 and perplexity is 77.59283023111801
At time: 237.58671736717224 and batch: 500, loss is 4.381645317077637 and perplexity is 79.96950040768559
At time: 238.2941551208496 and batch: 550, loss is 4.440644311904907 and perplexity is 84.8295807784088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.95275424389129 and perplexity of 141.56433016211577
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 240.17074370384216 and batch: 50, loss is 4.485694618225097 and perplexity is 88.73856883716513
At time: 240.8911509513855 and batch: 100, loss is 4.4800482177734375 and perplexity is 88.2389272576803
At time: 241.5967583656311 and batch: 150, loss is 4.460072317123413 and perplexity is 86.49376385035859
At time: 242.3022437095642 and batch: 200, loss is 4.400532817840576 and perplexity is 81.49427870471804
At time: 243.00759291648865 and batch: 250, loss is 4.389940366744995 and perplexity is 80.63561027318913
At time: 243.71260929107666 and batch: 300, loss is 4.374682817459107 and perplexity is 79.41464661769469
At time: 244.41827964782715 and batch: 350, loss is 4.411920413970948 and perplexity is 82.42760673292788
At time: 245.12419056892395 and batch: 400, loss is 4.403217754364014 and perplexity is 81.71337967453887
At time: 245.8295702934265 and batch: 450, loss is 4.343783254623413 and perplexity is 76.99829314218738
At time: 246.53462958335876 and batch: 500, loss is 4.373676786422729 and perplexity is 79.33479319270127
At time: 247.2416079044342 and batch: 550, loss is 4.43328351020813 and perplexity is 84.20745952083003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949907343438331 and perplexity of 141.16188374071774
Finished 28 epochs...
Completing Train Step...
At time: 249.09863114356995 and batch: 50, loss is 4.483703413009644 and perplexity is 88.56204793907506
At time: 249.8060486316681 and batch: 100, loss is 4.4790721607208255 and perplexity is 88.1528430487994
At time: 250.49954175949097 and batch: 150, loss is 4.458677616119385 and perplexity is 86.37321499541967
At time: 251.1929910182953 and batch: 200, loss is 4.399753732681274 and perplexity is 81.43081244763091
At time: 251.88661909103394 and batch: 250, loss is 4.389656391143799 and perplexity is 80.61271497829036
At time: 252.5967378616333 and batch: 300, loss is 4.3744717216491695 and perplexity is 79.39788428783712
At time: 253.2897355556488 and batch: 350, loss is 4.411188526153564 and perplexity is 82.36730104293586
At time: 253.98728895187378 and batch: 400, loss is 4.402837266921997 and perplexity is 81.68229467382922
At time: 254.68455862998962 and batch: 450, loss is 4.34348466873169 and perplexity is 76.97530597016174
At time: 255.38374257087708 and batch: 500, loss is 4.37363941192627 and perplexity is 79.3318281501628
At time: 256.0827739238739 and batch: 550, loss is 4.4333540630340575 and perplexity is 84.21340080464809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949642100232713 and perplexity of 141.1244464753729
Finished 29 epochs...
Completing Train Step...
At time: 257.9314160346985 and batch: 50, loss is 4.483011837005615 and perplexity is 88.5008217255831
At time: 258.6419906616211 and batch: 100, loss is 4.478645362854004 and perplexity is 88.11522763109292
At time: 259.33620715141296 and batch: 150, loss is 4.457864789962769 and perplexity is 86.30303711211378
At time: 260.03367829322815 and batch: 200, loss is 4.399298906326294 and perplexity is 81.39378398941994
At time: 260.73199677467346 and batch: 250, loss is 4.389489116668702 and perplexity is 80.59923165644544
At time: 261.43039560317993 and batch: 300, loss is 4.374373521804809 and perplexity is 79.39008781077021
At time: 262.12932229042053 and batch: 350, loss is 4.410642538070679 and perplexity is 82.32234175288214
At time: 262.8272511959076 and batch: 400, loss is 4.402483959197998 and perplexity is 81.65344078565786
At time: 263.5273380279541 and batch: 450, loss is 4.343215026855469 and perplexity is 76.95455300229762
At time: 264.22687339782715 and batch: 500, loss is 4.37354001045227 and perplexity is 79.32394284142175
At time: 264.9258613586426 and batch: 550, loss is 4.433343095779419 and perplexity is 84.21247721990208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949566455597573 and perplexity of 141.11377157186465
Finished 30 epochs...
Completing Train Step...
At time: 266.78102374076843 and batch: 50, loss is 4.482507200241089 and perplexity is 88.45617222408706
At time: 267.49414014816284 and batch: 100, loss is 4.478336782455444 and perplexity is 88.08804119384787
At time: 268.1940689086914 and batch: 150, loss is 4.457183599472046 and perplexity is 86.24426832255529
At time: 268.89375352859497 and batch: 200, loss is 4.398896608352661 and perplexity is 81.36104602070547
At time: 269.59392952919006 and batch: 250, loss is 4.389314413070679 and perplexity is 80.58515191060413
At time: 270.31021642684937 and batch: 300, loss is 4.374271764755249 and perplexity is 79.3820097206785
At time: 271.0101535320282 and batch: 350, loss is 4.410160570144654 and perplexity is 82.28267458448127
At time: 271.7101378440857 and batch: 400, loss is 4.40215500831604 and perplexity is 81.6265852316178
At time: 272.41326785087585 and batch: 450, loss is 4.342960977554322 and perplexity is 76.93500523504073
At time: 273.1173207759857 and batch: 500, loss is 4.373441190719604 and perplexity is 79.316104457896
At time: 273.82304978370667 and batch: 550, loss is 4.433290538787841 and perplexity is 84.2080513817515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949530418882978 and perplexity of 141.10868638678036
Finished 31 epochs...
Completing Train Step...
At time: 275.6886372566223 and batch: 50, loss is 4.482061223983765 and perplexity is 88.41673166689111
At time: 276.40196466445923 and batch: 100, loss is 4.4780791473388675 and perplexity is 88.06534954429429
At time: 277.10166358947754 and batch: 150, loss is 4.45659478187561 and perplexity is 86.19350112755092
At time: 277.8018002510071 and batch: 200, loss is 4.398590059280395 and perplexity is 81.33610868998231
At time: 278.5009400844574 and batch: 250, loss is 4.389146375656128 and perplexity is 80.5716117276864
At time: 279.19998478889465 and batch: 300, loss is 4.3741999435424805 and perplexity is 79.37630861320106
At time: 279.8989737033844 and batch: 350, loss is 4.409746856689453 and perplexity is 82.24864017560765
At time: 280.5984220504761 and batch: 400, loss is 4.401823873519898 and perplexity is 81.59956030365127
At time: 281.29839396476746 and batch: 450, loss is 4.342676572799682 and perplexity is 76.91312766494363
At time: 282.0003981590271 and batch: 500, loss is 4.37329909324646 and perplexity is 79.30483464059812
At time: 282.70425963401794 and batch: 550, loss is 4.433217515945435 and perplexity is 84.20190249499342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949515809404089 and perplexity of 141.10662487746438
Finished 32 epochs...
Completing Train Step...
At time: 284.5688285827637 and batch: 50, loss is 4.481674423217774 and perplexity is 88.38253862073047
At time: 285.28506541252136 and batch: 100, loss is 4.4778313732147215 and perplexity is 88.04353193247479
At time: 285.98436164855957 and batch: 150, loss is 4.456043119430542 and perplexity is 86.14596452325515
At time: 286.68404150009155 and batch: 200, loss is 4.398296728134155 and perplexity is 81.3122537748554
At time: 287.3840835094452 and batch: 250, loss is 4.388998870849609 and perplexity is 80.55972790416972
At time: 288.1001470088959 and batch: 300, loss is 4.374120321273804 and perplexity is 79.36998874303461
At time: 288.80168437957764 and batch: 350, loss is 4.409345598220825 and perplexity is 82.2156438326778
At time: 289.50150442123413 and batch: 400, loss is 4.401493349075317 and perplexity is 81.57259411104235
At time: 290.20194578170776 and batch: 450, loss is 4.342369146347046 and perplexity is 76.88948616914124
At time: 290.9019842147827 and batch: 500, loss is 4.373142585754395 and perplexity is 79.29242381103913
At time: 291.6045174598694 and batch: 550, loss is 4.433128709793091 and perplexity is 84.1944251800335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949502173890459 and perplexity of 141.1047008292753
Finished 33 epochs...
Completing Train Step...
At time: 293.4613070487976 and batch: 50, loss is 4.481307077407837 and perplexity is 88.3500776280664
At time: 294.1713297367096 and batch: 100, loss is 4.477574281692505 and perplexity is 88.02089959624448
At time: 294.8665156364441 and batch: 150, loss is 4.455516347885132 and perplexity is 86.10059723054087
At time: 295.5605511665344 and batch: 200, loss is 4.3980383205413816 and perplexity is 81.29124478565254
At time: 296.25566959381104 and batch: 250, loss is 4.388821840286255 and perplexity is 80.54546763244437
At time: 296.9499866962433 and batch: 300, loss is 4.374028081893921 and perplexity is 79.3626680421253
At time: 297.644371509552 and batch: 350, loss is 4.408981046676636 and perplexity is 82.18567745523855
At time: 298.34102606773376 and batch: 400, loss is 4.401151809692383 and perplexity is 81.54473861473058
At time: 299.0447509288788 and batch: 450, loss is 4.342053022384643 and perplexity is 76.8651834016523
At time: 299.7498347759247 and batch: 500, loss is 4.372957448959351 and perplexity is 79.27774522463865
At time: 300.4517390727997 and batch: 550, loss is 4.433028516769409 and perplexity is 84.18598990858227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949486915101397 and perplexity of 141.10254775883627
Finished 34 epochs...
Completing Train Step...
At time: 302.3165452480316 and batch: 50, loss is 4.480964603424073 and perplexity is 88.31982520564476
At time: 303.031857252121 and batch: 100, loss is 4.477324132919311 and perplexity is 87.99888402989338
At time: 303.7309286594391 and batch: 150, loss is 4.455022344589233 and perplexity is 86.05807375596324
At time: 304.4305467605591 and batch: 200, loss is 4.39776912689209 and perplexity is 81.26936464394295
At time: 305.12971019744873 and batch: 250, loss is 4.388642358779907 and perplexity is 80.53101250783679
At time: 305.8460159301758 and batch: 300, loss is 4.373927745819092 and perplexity is 79.35470550299765
At time: 306.54642057418823 and batch: 350, loss is 4.408598346710205 and perplexity is 82.15423101689458
At time: 307.24899077415466 and batch: 400, loss is 4.400812015533448 and perplexity is 81.51703489590486
At time: 307.9493329524994 and batch: 450, loss is 4.341745405197144 and perplexity is 76.84154198655688
At time: 308.65045142173767 and batch: 500, loss is 4.372801351547241 and perplexity is 79.26537113957762
At time: 309.35174012184143 and batch: 550, loss is 4.432924461364746 and perplexity is 84.17723035708265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.949490810962433 and perplexity of 141.10309747582505
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 311.2095491886139 and batch: 50, loss is 4.4807026576995845 and perplexity is 88.29669323483729
At time: 311.91380977630615 and batch: 100, loss is 4.476538496017456 and perplexity is 87.92977600973742
At time: 312.60438418388367 and batch: 150, loss is 4.453656787872315 and perplexity is 85.94063677697761
At time: 313.2942099571228 and batch: 200, loss is 4.396526918411255 and perplexity is 81.1684738266382
At time: 313.98792839050293 and batch: 250, loss is 4.3869138526916505 and perplexity is 80.39193439573263
At time: 314.6829249858856 and batch: 300, loss is 4.371705598831177 and perplexity is 79.17856346242127
At time: 315.38099002838135 and batch: 350, loss is 4.405589656829834 and perplexity is 81.90742587964156
At time: 316.08124828338623 and batch: 400, loss is 4.397544546127319 and perplexity is 81.25111515719733
At time: 316.78058218955994 and batch: 450, loss is 4.338550891876221 and perplexity is 76.59646232062737
At time: 317.4794728755951 and batch: 500, loss is 4.369005708694458 and perplexity is 78.96507836272863
At time: 318.1788139343262 and batch: 550, loss is 4.429899015426636 and perplexity is 83.92294155988732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.94860385326629 and perplexity of 140.9780004836403
Finished 36 epochs...
Completing Train Step...
At time: 320.02753019332886 and batch: 50, loss is 4.479906282424927 and perplexity is 88.22640392356068
At time: 320.74202609062195 and batch: 100, loss is 4.476101875305176 and perplexity is 87.89139242844848
At time: 321.44316935539246 and batch: 150, loss is 4.453175191879272 and perplexity is 85.89925807537453
At time: 322.1431927680969 and batch: 200, loss is 4.396063451766968 and perplexity is 81.13086366265331
At time: 322.84428310394287 and batch: 250, loss is 4.386526679992675 and perplexity is 80.36081485822322
At time: 323.5594027042389 and batch: 300, loss is 4.371416501998901 and perplexity is 79.15567649897383
At time: 324.26119780540466 and batch: 350, loss is 4.405545015335083 and perplexity is 81.90376949133298
At time: 324.96423387527466 and batch: 400, loss is 4.397646102905274 and perplexity is 81.2593671776751
At time: 325.6641376018524 and batch: 450, loss is 4.3388079643249515 and perplexity is 76.6161556919634
At time: 326.3648211956024 and batch: 500, loss is 4.369086532592774 and perplexity is 78.97146088611943
At time: 327.06600975990295 and batch: 550, loss is 4.430062513351441 and perplexity is 83.93666390843309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948441525723072 and perplexity of 140.95511772847496
Finished 37 epochs...
Completing Train Step...
At time: 328.92484879493713 and batch: 50, loss is 4.479600028991699 and perplexity is 88.19938842146406
At time: 329.63507294654846 and batch: 100, loss is 4.47566967010498 and perplexity is 87.85341351952067
At time: 330.33075523376465 and batch: 150, loss is 4.452841215133667 and perplexity is 85.87057451080096
At time: 331.0291180610657 and batch: 200, loss is 4.395746145248413 and perplexity is 81.10512439459193
At time: 331.7311611175537 and batch: 250, loss is 4.386273307800293 and perplexity is 80.34045624164352
At time: 332.43582940101624 and batch: 300, loss is 4.37120512008667 and perplexity is 79.1389461890163
At time: 333.1446261405945 and batch: 350, loss is 4.405502882003784 and perplexity is 81.9003186853759
At time: 333.8535451889038 and batch: 400, loss is 4.397723302841187 and perplexity is 81.26564063776581
At time: 334.5629551410675 and batch: 450, loss is 4.339011096954346 and perplexity is 76.63172051393128
At time: 335.27057003974915 and batch: 500, loss is 4.369129810333252 and perplexity is 78.97487866646523
At time: 335.98040437698364 and batch: 550, loss is 4.430164375305176 and perplexity is 83.94521429648101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948344129197141 and perplexity of 140.94138985823025
Finished 38 epochs...
Completing Train Step...
At time: 337.8493790626526 and batch: 50, loss is 4.479368600845337 and perplexity is 88.17897896224416
At time: 338.570326089859 and batch: 100, loss is 4.475293083190918 and perplexity is 87.82033530243628
At time: 339.2779448032379 and batch: 150, loss is 4.452560710906982 and perplexity is 85.84649082964796
At time: 339.98401618003845 and batch: 200, loss is 4.395480270385742 and perplexity is 81.08356344716559
At time: 340.689151763916 and batch: 250, loss is 4.3860424041748045 and perplexity is 80.32190748059443
At time: 341.4093236923218 and batch: 300, loss is 4.371029081344605 and perplexity is 79.12501589465253
At time: 342.1143214702606 and batch: 350, loss is 4.405459899902343 and perplexity is 81.89679851322288
At time: 342.82014989852905 and batch: 400, loss is 4.3977870178222656 and perplexity is 81.27081864147786
At time: 343.52532148361206 and batch: 450, loss is 4.339172849655151 and perplexity is 76.64411690423954
At time: 344.22979950904846 and batch: 500, loss is 4.369146528244019 and perplexity is 78.97619897247594
At time: 344.93321228027344 and batch: 550, loss is 4.4302338600158695 and perplexity is 83.95104740806421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948281470765459 and perplexity of 140.93255896845048
Finished 39 epochs...
Completing Train Step...
At time: 346.79270100593567 and batch: 50, loss is 4.479178190231323 and perplexity is 88.16219034713282
At time: 347.5019853115082 and batch: 100, loss is 4.474964027404785 and perplexity is 87.79144226693585
At time: 348.196799993515 and batch: 150, loss is 4.452308597564698 and perplexity is 85.82485051194253
At time: 348.89095163345337 and batch: 200, loss is 4.395256586074829 and perplexity is 81.06542835449301
At time: 349.5869891643524 and batch: 250, loss is 4.38582028388977 and perplexity is 80.30406833690154
At time: 350.28444385528564 and batch: 300, loss is 4.370880336761474 and perplexity is 79.11324735242324
At time: 350.9838035106659 and batch: 350, loss is 4.405417957305908 and perplexity is 81.89336362088817
At time: 351.68303513526917 and batch: 400, loss is 4.397850999832153 and perplexity is 81.27601867815237
At time: 352.3813774585724 and batch: 450, loss is 4.339317140579223 and perplexity is 76.65517675259102
At time: 353.08096051216125 and batch: 500, loss is 4.369151058197022 and perplexity is 78.97655673175596
At time: 353.7814197540283 and batch: 550, loss is 4.430290088653565 and perplexity is 83.95576799380787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948231473882148 and perplexity of 140.92551295588572
Finished 40 epochs...
Completing Train Step...
At time: 355.6308274269104 and batch: 50, loss is 4.4790120124816895 and perplexity is 88.14754096997213
At time: 356.3416521549225 and batch: 100, loss is 4.474659557342529 and perplexity is 87.76471646985233
At time: 357.03738832473755 and batch: 150, loss is 4.45206353187561 and perplexity is 85.80382036280022
At time: 357.7360770702362 and batch: 200, loss is 4.395021772384643 and perplexity is 81.04639531681093
At time: 358.43570852279663 and batch: 250, loss is 4.385642747879029 and perplexity is 80.28981273844117
At time: 359.1482949256897 and batch: 300, loss is 4.370730457305908 and perplexity is 79.10139079053167
At time: 359.8476791381836 and batch: 350, loss is 4.405373287200928 and perplexity is 81.88970551744256
At time: 360.54603457450867 and batch: 400, loss is 4.397896852493286 and perplexity is 81.2797454853364
At time: 361.2476348876953 and batch: 450, loss is 4.33942873954773 and perplexity is 76.66373186860946
At time: 361.9461159706116 and batch: 500, loss is 4.369140586853027 and perplexity is 78.97572974539275
At time: 362.6443393230438 and batch: 550, loss is 4.430326309204101 and perplexity is 83.95880897301797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.94819478785738 and perplexity of 140.92034305385923
Finished 41 epochs...
Completing Train Step...
At time: 364.5095844268799 and batch: 50, loss is 4.478860397338867 and perplexity is 88.13417748103774
At time: 365.23008251190186 and batch: 100, loss is 4.474374294281006 and perplexity is 87.73968400872639
At time: 365.935218334198 and batch: 150, loss is 4.451843309402466 and perplexity is 85.78492651377613
At time: 366.6406581401825 and batch: 200, loss is 4.394828538894654 and perplexity is 81.03073595199838
At time: 367.34718465805054 and batch: 250, loss is 4.385450401306152 and perplexity is 80.27437075327812
At time: 368.05307173728943 and batch: 300, loss is 4.3706026458740235 and perplexity is 79.09128137457392
At time: 368.7590284347534 and batch: 350, loss is 4.405325794219971 and perplexity is 81.88581642357093
At time: 369.46471548080444 and batch: 400, loss is 4.397938814163208 and perplexity is 81.28315619074675
At time: 370.1717734336853 and batch: 450, loss is 4.339532136917114 and perplexity is 76.6716591066326
At time: 370.87983775138855 and batch: 500, loss is 4.369129810333252 and perplexity is 78.97487866646523
At time: 371.58723640441895 and batch: 550, loss is 4.430359172821045 and perplexity is 83.96156820849406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948163296313996 and perplexity of 140.915905324638
Finished 42 epochs...
Completing Train Step...
At time: 373.4529881477356 and batch: 50, loss is 4.478722639083863 and perplexity is 88.1220371067793
At time: 374.1697084903717 and batch: 100, loss is 4.474103927612305 and perplexity is 87.71596532916273
At time: 374.87038135528564 and batch: 150, loss is 4.45162335395813 and perplexity is 85.76605972714978
At time: 375.5719072818756 and batch: 200, loss is 4.394619188308716 and perplexity is 81.01377389551803
At time: 376.27207374572754 and batch: 250, loss is 4.385281705856324 and perplexity is 80.26082997436018
At time: 376.9881625175476 and batch: 300, loss is 4.370470085144043 and perplexity is 79.08079767145884
At time: 377.6880762577057 and batch: 350, loss is 4.405273952484131 and perplexity is 81.88157143074169
At time: 378.38914704322815 and batch: 400, loss is 4.39796724319458 and perplexity is 81.28546702499139
At time: 379.09010004997253 and batch: 450, loss is 4.3396150016784665 and perplexity is 76.67801274860983
At time: 379.7891821861267 and batch: 500, loss is 4.3691075420379635 and perplexity is 78.97312005012748
At time: 380.4889576435089 and batch: 550, loss is 4.430379142761231 and perplexity is 83.96324493273109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948136025286735 and perplexity of 140.9120624555422
Finished 43 epochs...
Completing Train Step...
At time: 382.34390449523926 and batch: 50, loss is 4.47859130859375 and perplexity is 88.11046475637424
At time: 383.0543339252472 and batch: 100, loss is 4.473848114013672 and perplexity is 87.69352926226216
At time: 383.75013613700867 and batch: 150, loss is 4.451419048309326 and perplexity is 85.74853902652184
At time: 384.44759249687195 and batch: 200, loss is 4.394437580108643 and perplexity is 80.99906246575831
At time: 385.1468005180359 and batch: 250, loss is 4.3851065254211425 and perplexity is 80.24677107869496
At time: 385.84699606895447 and batch: 300, loss is 4.370350904464722 and perplexity is 79.07137332988198
At time: 386.5464482307434 and batch: 350, loss is 4.405217761993408 and perplexity is 81.87697059432469
At time: 387.246125459671 and batch: 400, loss is 4.397992496490478 and perplexity is 81.28751977686164
At time: 387.9457812309265 and batch: 450, loss is 4.339690198898316 and perplexity is 76.68377893879011
At time: 388.64413595199585 and batch: 500, loss is 4.369081974029541 and perplexity is 78.97110089054193
At time: 389.34495735168457 and batch: 550, loss is 4.430395784378052 and perplexity is 83.96464222850693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948108754259475 and perplexity of 140.90821969124397
Finished 44 epochs...
Completing Train Step...
At time: 391.2075037956238 and batch: 50, loss is 4.478466596603393 and perplexity is 88.0994770101093
At time: 391.91835379600525 and batch: 100, loss is 4.473600158691406 and perplexity is 87.67178788051038
At time: 392.6145088672638 and batch: 150, loss is 4.451207752227783 and perplexity is 85.73042261025883
At time: 393.3102421760559 and batch: 200, loss is 4.394242515563965 and perplexity is 80.98326396143322
At time: 394.00804924964905 and batch: 250, loss is 4.3849572086334225 and perplexity is 80.23478978313914
At time: 394.7217938899994 and batch: 300, loss is 4.370219812393189 and perplexity is 79.06100837914946
At time: 395.4217195510864 and batch: 350, loss is 4.405157051086426 and perplexity is 81.87199992006752
At time: 396.1238374710083 and batch: 400, loss is 4.39800178527832 and perplexity is 81.28827484289387
At time: 396.82914686203003 and batch: 450, loss is 4.339742355346679 and perplexity is 76.68777859664974
At time: 397.53766679763794 and batch: 500, loss is 4.369042415618896 and perplexity is 78.96797698109268
At time: 398.2455015182495 and batch: 550, loss is 4.430398864746094 and perplexity is 83.96490087090584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948082457197473 and perplexity of 140.90451426777528
Finished 45 epochs...
Completing Train Step...
At time: 400.11507844924927 and batch: 50, loss is 4.478346824645996 and perplexity is 88.08892579518457
At time: 400.8355634212494 and batch: 100, loss is 4.473359689712525 and perplexity is 87.6507080698239
At time: 401.54141998291016 and batch: 150, loss is 4.4510062885284425 and perplexity is 85.71315278185388
At time: 402.24652433395386 and batch: 200, loss is 4.3940754318237305 and perplexity is 80.96973410513512
At time: 402.95116448402405 and batch: 250, loss is 4.384788122177124 and perplexity is 80.22122431376373
At time: 403.6562075614929 and batch: 300, loss is 4.370089654922485 and perplexity is 79.05071866792348
At time: 404.3598589897156 and batch: 350, loss is 4.405097846984863 and perplexity is 81.86715290535211
At time: 405.0670733451843 and batch: 400, loss is 4.398006782531739 and perplexity is 81.28868106201817
At time: 405.7717020511627 and batch: 450, loss is 4.339790573120117 and perplexity is 76.69147639973276
At time: 406.4760482311249 and batch: 500, loss is 4.369001054763794 and perplexity is 78.96471086558421
At time: 407.18096470832825 and batch: 550, loss is 4.43040433883667 and perplexity is 83.96536050363648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948058432721077 and perplexity of 140.9011291512612
Finished 46 epochs...
Completing Train Step...
At time: 409.04581475257874 and batch: 50, loss is 4.47823618888855 and perplexity is 88.07918054925287
At time: 409.75494623184204 and batch: 100, loss is 4.4731126880645755 and perplexity is 87.62906087404332
At time: 410.4495425224304 and batch: 150, loss is 4.450803518295288 and perplexity is 85.69577446784184
At time: 411.1461100578308 and batch: 200, loss is 4.393895301818848 and perplexity is 80.95515034006168
At time: 411.8421380519867 and batch: 250, loss is 4.384618263244629 and perplexity is 80.20759917944653
At time: 412.5568914413452 and batch: 300, loss is 4.369946413040161 and perplexity is 79.03939610513436
At time: 413.25583243370056 and batch: 350, loss is 4.405037984848023 and perplexity is 81.86225230932371
At time: 413.9560432434082 and batch: 400, loss is 4.398011093139648 and perplexity is 81.28903146640496
At time: 414.65582609176636 and batch: 450, loss is 4.339836196899414 and perplexity is 76.69497543454496
At time: 415.35409474372864 and batch: 500, loss is 4.368957691192627 and perplexity is 78.96128674796638
At time: 416.05464148521423 and batch: 550, loss is 4.430411968231201 and perplexity is 83.96600111094243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948048693068484 and perplexity of 140.8997568298963
Finished 47 epochs...
Completing Train Step...
At time: 417.9054729938507 and batch: 50, loss is 4.478122787475586 and perplexity is 88.06919281204833
At time: 418.61749172210693 and batch: 100, loss is 4.472880687713623 and perplexity is 87.60873325926495
At time: 419.3135278224945 and batch: 150, loss is 4.450615539550781 and perplexity is 85.67966699772624
At time: 420.00780057907104 and batch: 200, loss is 4.393733434677124 and perplexity is 80.94204742176287
At time: 420.702556848526 and batch: 250, loss is 4.384445695877075 and perplexity is 80.19375915940057
At time: 421.39827513694763 and batch: 300, loss is 4.369823865890503 and perplexity is 79.0297106459058
At time: 422.0941526889801 and batch: 350, loss is 4.4049734783172605 and perplexity is 81.85697182974138
At time: 422.79164242744446 and batch: 400, loss is 4.398022956848145 and perplexity is 81.28999586149888
At time: 423.491553068161 and batch: 450, loss is 4.3398887825012205 and perplexity is 76.69900859202578
At time: 424.1924126148224 and batch: 500, loss is 4.368919916152954 and perplexity is 78.95830403856324
At time: 424.89545726776123 and batch: 550, loss is 4.430424718856812 and perplexity is 83.96707173681217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948039278070977 and perplexity of 140.8984302652818
Finished 48 epochs...
Completing Train Step...
At time: 426.7590148448944 and batch: 50, loss is 4.478010215759277 and perplexity is 88.05927926986226
At time: 427.4747884273529 and batch: 100, loss is 4.472664279937744 and perplexity is 87.58977609946514
At time: 428.1766812801361 and batch: 150, loss is 4.4504310894012455 and perplexity is 85.66386482773748
At time: 428.8778603076935 and batch: 200, loss is 4.393564739227295 and perplexity is 80.92839401832889
At time: 429.57923460006714 and batch: 250, loss is 4.384297227859497 and perplexity is 80.18185383475783
At time: 430.3004322052002 and batch: 300, loss is 4.3697043800354 and perplexity is 79.02026827747673
At time: 431.00517749786377 and batch: 350, loss is 4.404915294647217 and perplexity is 81.85220922925579
At time: 431.7110421657562 and batch: 400, loss is 4.398021640777588 and perplexity is 81.28988887819918
At time: 432.41636323928833 and batch: 450, loss is 4.339926042556763 and perplexity is 76.70186645458769
At time: 433.12201976776123 and batch: 500, loss is 4.368878040313721 and perplexity is 78.95499766254629
At time: 433.8275692462921 and batch: 550, loss is 4.430422143936157 and perplexity is 83.96685552854322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948032784969248 and perplexity of 140.89751540041078
Finished 49 epochs...
Completing Train Step...
At time: 435.6810357570648 and batch: 50, loss is 4.477901659011841 and perplexity is 88.0497203597746
At time: 436.3896381855011 and batch: 100, loss is 4.472456693649292 and perplexity is 87.57159555001998
At time: 437.0837233066559 and batch: 150, loss is 4.450262136459351 and perplexity is 85.64939288833357
At time: 437.77854323387146 and batch: 200, loss is 4.3934235191345214 and perplexity is 80.91696610996175
At time: 438.47259044647217 and batch: 250, loss is 4.384135293960571 and perplexity is 80.16887072577438
At time: 439.16574263572693 and batch: 300, loss is 4.369600658416748 and perplexity is 79.01207259238687
At time: 439.8588922023773 and batch: 350, loss is 4.4048588085174565 and perplexity is 81.84758584532386
At time: 440.5541265010834 and batch: 400, loss is 4.398020973205567 and perplexity is 81.28983461136184
At time: 441.2489721775055 and batch: 450, loss is 4.33996615409851 and perplexity is 76.70494314641113
At time: 441.9440634250641 and batch: 500, loss is 4.368840427398681 and perplexity is 78.95202799077663
At time: 442.64294052124023 and batch: 550, loss is 4.430423021316528 and perplexity is 83.9669291994464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 4.948026616522607 and perplexity of 140.89664628428565
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23cfa37a20>
SETTINGS FOR THIS RUN
{'lr': 0.0, 'seq_len': 35, 'dropout': 1.0, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 8.0, 'tune_wordvecs': True, 'data': 'ptb'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 596 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.9319055080413818 and batch: 50, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 1.5917422771453857 and batch: 100, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 2.2515573501586914 and batch: 150, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 2.912767171859741 and batch: 200, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 3.573277473449707 and batch: 250, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 4.236325979232788 and batch: 300, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 4.896986961364746 and batch: 350, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 5.556532382965088 and batch: 400, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 6.2158801555633545 and batch: 450, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 6.874911785125732 and batch: 500, loss is 9.169524192810059 and perplexity is 9600.055827561986
At time: 7.536022663116455 and batch: 550, loss is 9.169524192810059 and perplexity is 9600.055827561986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 1 epochs...
Completing Train Step...
At time: 9.321978092193604 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 9.983435153961182 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 10.645565509796143 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 11.306910514831543 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 11.969679594039917 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 12.63141393661499 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 13.29383134841919 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 13.955517768859863 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 14.617576599121094 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 15.280117988586426 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 15.94162893295288 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 2 epochs...
Completing Train Step...
At time: 17.728041410446167 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 18.406359672546387 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 19.09771752357483 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 19.807124137878418 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 20.50556969642639 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 21.207451343536377 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 21.90968084335327 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 22.612486839294434 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 23.315479278564453 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 24.02237105369568 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 24.729219913482666 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 3 epochs...
Completing Train Step...
At time: 26.605947971343994 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 27.29952359199524 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 27.992400407791138 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 28.686042308807373 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 29.38328504562378 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 30.08510136604309 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 30.78518557548523 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 31.487421989440918 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 32.189165115356445 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 32.88962650299072 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 33.592307329177856 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 4 epochs...
Completing Train Step...
At time: 35.463749408721924 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 36.17880320549011 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 36.87991642951965 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 37.57969284057617 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 38.279144287109375 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 38.97901487350464 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 39.6794638633728 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 40.38028597831726 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 41.0821475982666 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 41.78340935707092 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 42.4842472076416 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 5 epochs...
Completing Train Step...
At time: 44.346940755844116 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 45.06576752662659 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 45.76936626434326 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 46.47335410118103 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 47.17790961265564 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 47.8833384513855 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 48.58706307411194 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 49.291133880615234 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 49.99482989311218 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 50.700084924697876 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 51.405372858047485 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 6 epochs...
Completing Train Step...
At time: 53.2707154750824 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 53.97857618331909 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 54.670589208602905 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 55.36405634880066 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 56.056374073028564 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 56.74907422065735 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 57.441248655319214 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 58.13971924781799 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 58.842976331710815 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 59.54734563827515 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 60.251235246658325 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 7 epochs...
Completing Train Step...
At time: 62.121013879776 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 62.8365740776062 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 63.538331270217896 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 64.24050569534302 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 64.94217371940613 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 65.6435034275055 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 66.346027135849 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 67.04810619354248 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 67.74840927124023 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 68.45216727256775 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 69.15289735794067 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 8 epochs...
Completing Train Step...
At time: 71.03164029121399 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 71.74497961997986 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 72.44483685493469 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 73.14889931678772 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 73.85197043418884 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 74.5552031993866 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 75.25707244873047 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 75.95959568023682 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 76.66340661048889 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 77.3669695854187 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 78.06984376907349 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 9 epochs...
Completing Train Step...
At time: 79.92607760429382 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 80.64186787605286 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 81.34079098701477 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 82.03998184204102 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 82.73928785324097 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 83.43898224830627 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 84.1388897895813 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 84.83946895599365 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 85.5404405593872 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 86.2413718700409 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 86.94237923622131 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 10 epochs...
Completing Train Step...
At time: 88.8151216506958 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 89.52347493171692 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 90.21773433685303 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 90.91530179977417 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 91.61387729644775 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 92.31098651885986 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 93.008540391922 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 93.70597815513611 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 94.40419101715088 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 95.10711646080017 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 95.80855679512024 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 11 epochs...
Completing Train Step...
At time: 97.6704330444336 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 98.38701677322388 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 99.0891981124878 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 99.79028391838074 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 100.49164271354675 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 101.1942253112793 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 101.89553570747375 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 102.598557472229 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 103.3009922504425 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 104.00339984893799 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 104.70631408691406 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 12 epochs...
Completing Train Step...
At time: 106.57876205444336 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 107.2928216457367 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 107.9915816783905 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 108.69098401069641 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 109.39089727401733 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 110.09226155281067 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 110.79415345191956 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 111.49711990356445 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 112.19917416572571 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 112.9030191898346 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 113.605064868927 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 13 epochs...
Completing Train Step...
At time: 115.45171403884888 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 116.15528893470764 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 116.84931659698486 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 117.54243302345276 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 118.23422145843506 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 118.93127489089966 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 119.63288927078247 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 120.33620500564575 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 121.03823590278625 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 121.73927187919617 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 122.43960618972778 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 14 epochs...
Completing Train Step...
At time: 124.32285070419312 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 125.03753137588501 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 125.73721814155579 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 126.43800139427185 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 127.13689756393433 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 127.83803749084473 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 128.538019657135 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 129.23810458183289 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 129.93784928321838 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 130.63770866394043 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 131.3404791355133 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 15 epochs...
Completing Train Step...
At time: 133.20766401290894 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 133.92757606506348 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 134.63432717323303 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 135.33955311775208 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 136.04379725456238 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 136.74801754951477 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 137.4524884223938 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 138.15753531455994 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 138.8627951145172 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 139.5667757987976 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 140.27129817008972 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 16 epochs...
Completing Train Step...
At time: 142.1446545124054 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 142.8585901260376 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 143.55691123008728 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 144.2554657459259 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 144.95400547981262 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 145.6546425819397 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 146.3567943572998 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 147.05904150009155 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 147.76090741157532 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 148.46717047691345 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 149.17352151870728 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 17 epochs...
Completing Train Step...
At time: 151.03188610076904 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 151.74657368659973 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 152.4452805519104 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 153.14601349830627 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 153.84610080718994 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 154.54486298561096 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 155.2439239025116 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 155.94340586662292 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 156.64423871040344 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 157.34462809562683 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 158.04453039169312 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 18 epochs...
Completing Train Step...
At time: 159.90496277809143 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 160.6144678592682 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 161.3096125125885 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 162.00565147399902 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 162.7016191482544 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 163.40124130249023 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 164.10447549819946 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 164.8065435886383 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 165.5090892314911 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 166.21251893043518 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 166.9133915901184 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 19 epochs...
Completing Train Step...
At time: 168.7782826423645 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 169.4973292350769 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 170.20327758789062 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 170.9089720249176 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 171.61372995376587 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 172.3188066482544 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 173.02342224121094 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 173.72903108596802 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 174.4339873790741 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 175.13937711715698 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 175.84316873550415 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 20 epochs...
Completing Train Step...
At time: 177.70775604248047 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 178.4132797718048 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 179.1041386127472 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 179.7972514629364 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 180.49139094352722 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 181.1920669078827 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 181.89423727989197 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 182.5962302684784 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 183.29834866523743 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 183.99947881698608 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 184.70132660865784 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 21 epochs...
Completing Train Step...
At time: 186.5600655078888 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 187.28083968162537 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 187.98129868507385 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 188.6807563304901 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 189.38218641281128 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 190.08205485343933 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 190.7817029953003 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 191.48131465911865 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 192.18189144134521 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 192.88449692726135 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 193.58757972717285 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 22 epochs...
Completing Train Step...
At time: 195.47539353370667 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 196.200581073761 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 196.90922355651855 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 197.61667108535767 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 198.324285030365 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 199.03332018852234 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 199.74163389205933 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 200.4496705532074 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 201.15784287452698 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 201.86508870124817 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 202.5725929737091 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 23 epochs...
Completing Train Step...
At time: 204.4474539756775 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 205.1667821407318 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 205.8720440864563 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 206.57580280303955 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 207.28078937530518 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 207.98432230949402 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 208.68803000450134 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 209.39207243919373 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 210.09570050239563 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 210.80076265335083 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 211.50433111190796 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 24 epochs...
Completing Train Step...
At time: 213.3706386089325 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 214.078768491745 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 214.77363920211792 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 215.4663348197937 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 216.1588578224182 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 216.8529441356659 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 217.54572916030884 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 218.2412621974945 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 218.93798518180847 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 219.638249874115 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 220.33898377418518 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 25 epochs...
Completing Train Step...
At time: 222.1987862586975 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 222.9132969379425 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 223.61258673667908 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 224.315447807312 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 225.01824307441711 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 225.71943497657776 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 226.42131900787354 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 227.1232590675354 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 227.82636642456055 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 228.52824068069458 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 229.23111271858215 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 26 epochs...
Completing Train Step...
At time: 231.10694813728333 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 231.82642602920532 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 232.53168988227844 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 233.23559284210205 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 233.93915915489197 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 234.6436984539032 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 235.34852290153503 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 236.05458235740662 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 236.75961995124817 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 237.46435928344727 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 238.16822862625122 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 27 epochs...
Completing Train Step...
At time: 240.02011466026306 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 240.72752118110657 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 241.420156955719 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 242.11779260635376 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 242.81520795822144 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 243.51286435127258 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 244.20871353149414 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 244.90617489814758 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 245.60216307640076 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 246.29970622062683 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 246.99805688858032 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 28 epochs...
Completing Train Step...
At time: 248.85880303382874 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 249.5720043182373 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 250.27104449272156 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 250.96881580352783 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 251.66581559181213 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 252.36367535591125 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 253.06191110610962 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 253.76096725463867 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 254.46390962600708 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 255.16538333892822 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 255.86691093444824 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 29 epochs...
Completing Train Step...
At time: 257.73733401298523 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 258.45629262924194 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 259.16025376319885 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 259.8652226924896 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 260.571252822876 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 261.2802481651306 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 261.98716139793396 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 262.6947889328003 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 263.40172719955444 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 264.11041474342346 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 264.81836318969727 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 30 epochs...
Completing Train Step...
At time: 266.7042236328125 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 267.42268228530884 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 268.12607741355896 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 268.83187890052795 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 269.53677439689636 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 270.24037885665894 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 270.9434189796448 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 271.64765548706055 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 272.3525381088257 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 273.05659341812134 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 273.7621500492096 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 31 epochs...
Completing Train Step...
At time: 275.6271479129791 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 276.34103512763977 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 277.04038882255554 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 277.73983931541443 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 278.4400734901428 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 279.13900446891785 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 279.84009432792664 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 280.53936672210693 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 281.2374973297119 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 281.93764567375183 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 282.6361689567566 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 32 epochs...
Completing Train Step...
At time: 284.501930475235 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 285.2106273174286 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 285.9047853946686 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 286.59915471076965 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 287.2929091453552 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 287.99029660224915 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 288.6908824443817 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 289.3951542377472 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 290.0990900993347 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 290.80444717407227 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 291.50824332237244 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 33 epochs...
Completing Train Step...
At time: 293.37485575675964 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 294.09755778312683 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 294.8047833442688 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 295.51130199432373 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 296.2185699939728 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 296.9268596172333 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 297.63536500930786 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 298.34254455566406 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 299.0497570037842 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 299.7569751739502 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 300.463002204895 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 34 epochs...
Completing Train Step...
At time: 302.34090089797974 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 303.05537962913513 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 303.75474667549133 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 304.4538164138794 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 305.15476417541504 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 305.85413360595703 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 306.55105328559875 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 307.25092482566833 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 307.9501190185547 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 308.6510241031647 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 309.3499596118927 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 35 epochs...
Completing Train Step...
At time: 311.1931035518646 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 311.89763879776 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 312.58752489089966 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 313.2770187854767 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 313.969069480896 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 314.6646635532379 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 315.3655071258545 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 316.06884002685547 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 316.7710659503937 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 317.4722430706024 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 318.1736080646515 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 36 epochs...
Completing Train Step...
At time: 320.0557789802551 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 320.7705981731415 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 321.4690840244293 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 322.16887974739075 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 322.86915397644043 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 323.56878876686096 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 324.26979398727417 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 324.9720969200134 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 325.67929887771606 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 326.38701725006104 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 327.094131231308 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 37 epochs...
Completing Train Step...
At time: 328.9656262397766 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 329.6835193634033 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 330.38748836517334 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 331.0914523601532 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 331.79541397094727 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 332.49968123435974 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 333.20353078842163 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 333.90904545783997 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 334.6120789051056 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 335.31517267227173 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 336.018089056015 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 38 epochs...
Completing Train Step...
At time: 337.8860981464386 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 338.59455370903015 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 339.28880167007446 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 339.9900951385498 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 340.69120264053345 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 341.3946361541748 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 342.09678983688354 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 342.7975273132324 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 343.498170375824 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 344.2013771533966 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 344.90329480171204 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 39 epochs...
Completing Train Step...
At time: 346.75759649276733 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 347.47287130355835 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 348.1714563369751 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 348.87274503707886 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 349.5717420578003 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 350.2708623409271 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 350.96994400024414 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 351.6695771217346 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 352.3678615093231 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 353.0714204311371 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 353.77419328689575 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 40 epochs...
Completing Train Step...
At time: 355.6566970348358 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 356.37799882888794 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 357.0835392475128 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 357.7874138355255 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 358.49085092544556 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 359.1961717605591 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 359.9005711078644 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 360.6044592857361 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 361.3071849346161 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 362.0118956565857 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 362.71534490585327 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 41 epochs...
Completing Train Step...
At time: 364.5895972251892 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 365.3034174442291 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 365.99731945991516 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 366.69123911857605 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 367.38516306877136 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 368.08093643188477 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 368.778445482254 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 369.4786355495453 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 370.18101620674133 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 370.8834147453308 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 371.5843892097473 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 42 epochs...
Completing Train Step...
At time: 373.46573185920715 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 374.1764509677887 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 374.86932134628296 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 375.56228947639465 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 376.2592089176178 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 376.9621386528015 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 377.66519927978516 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 378.36678767204285 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 379.0688145160675 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 379.7711179256439 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 380.4734764099121 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 43 epochs...
Completing Train Step...
At time: 382.3490951061249 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 383.0663278102875 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 383.7710316181183 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 384.474618434906 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 385.1796119213104 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 385.8829770088196 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 386.58609342575073 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 387.29090452194214 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 387.9960312843323 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 388.7010576725006 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 389.4044940471649 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 44 epochs...
Completing Train Step...
At time: 391.2958233356476 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 392.0106990337372 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 392.70875334739685 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 393.4072527885437 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 394.10505270957947 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 394.80358958244324 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 395.5030884742737 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 396.2003917694092 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 396.9003846645355 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 397.6027615070343 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 398.3062369823456 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 45 epochs...
Completing Train Step...
At time: 400.1581304073334 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 400.86679458618164 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 401.5608870983124 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 402.25831747055054 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 402.95746541023254 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 403.65674233436584 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 404.35854864120483 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 405.0603361129761 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 405.7623641490936 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 406.46543645858765 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 407.1672360897064 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 46 epochs...
Completing Train Step...
At time: 409.04872155189514 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 409.7628183364868 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 410.46498012542725 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 411.16519117355347 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 411.8647270202637 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 412.56433033943176 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 413.2647068500519 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 413.9653136730194 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 414.66448044776917 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 415.366051197052 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 416.0663754940033 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 47 epochs...
Completing Train Step...
At time: 417.9356472492218 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 418.6495273113251 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 419.34789848327637 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 420.04787278175354 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 420.747239112854 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 421.44778776168823 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 422.1475901603699 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 422.8503804206848 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 423.5537350177765 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 424.25735330581665 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 424.9655020236969 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 48 epochs...
Completing Train Step...
At time: 426.84186601638794 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 427.55637788772583 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 428.25627088546753 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 428.9556543827057 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 429.65408873558044 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 430.3528628349304 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 431.0523955821991 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 431.7530941963196 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 432.45322155952454 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 433.1579921245575 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 433.86341881752014 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished 49 epochs...
Completing Train Step...
At time: 435.7270197868347 and batch: 50, loss is 9.186528835296631 and perplexity is 9764.697211436302
At time: 436.443608045578 and batch: 100, loss is 9.186711311340332 and perplexity is 9766.479197331308
At time: 437.14579129219055 and batch: 150, loss is 9.186595859527587 and perplexity is 9765.351704690629
At time: 437.8476357460022 and batch: 200, loss is 9.188070487976074 and perplexity is 9779.762592865265
At time: 438.55202889442444 and batch: 250, loss is 9.18723648071289 and perplexity is 9771.609600130767
At time: 439.25807094573975 and batch: 300, loss is 9.188269996643067 and perplexity is 9781.713934912032
At time: 439.9633004665375 and batch: 350, loss is 9.185982875823974 and perplexity is 9759.367537521459
At time: 440.66828083992004 and batch: 400, loss is 9.187341537475586 and perplexity is 9772.636227727822
At time: 441.3732442855835 and batch: 450, loss is 9.188080368041993 and perplexity is 9779.859218041687
At time: 442.07642817497253 and batch: 500, loss is 9.187896766662597 and perplexity is 9778.063787225788
At time: 442.7764205932617 and batch: 550, loss is 9.187230167388917 and perplexity is 9771.547908988354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 48 batches
Done Evaluating: Achieved loss of 9.382323569439826 and perplexity of 11876.578773815883
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23cfa37a20>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'lr': 2.3182911126552996, 'seq_len': 35, 'dropout': 0.08390891661702704, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.668310697536721, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -91.89716131547962}, {'params': {'lr': 2.8093823593666523, 'seq_len': 35, 'dropout': 0.4612155562702229, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 6.351047942086454, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -88.5624394539553}, {'params': {'lr': 21.983735712465226, 'seq_len': 35, 'dropout': 0.18839212092800361, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.6444654761443784, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -127.66703941666789}, {'params': {'lr': 28.64800965039765, 'seq_len': 35, 'dropout': 0.43537702910137677, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 4.822065407101446, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -150.9645990578293}, {'params': {'lr': 26.571065701167413, 'seq_len': 35, 'dropout': 0.2664040303606525, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 2.7148639865914515, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -140.89664628428565}, {'params': {'lr': 0.0, 'seq_len': 35, 'dropout': 1.0, 'batch_size': 50, 'num_layers': 1, 'wordvec_dim': 200, 'wordvec_source': '', 'anneal': 8.0, 'tune_wordvecs': True, 'data': 'ptb'}, 'best_accuracy': -11876.578773815883}]
