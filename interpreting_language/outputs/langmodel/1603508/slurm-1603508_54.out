Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 6.21590689787748, 'num_layers': 1, 'dropout': 0.6665903176202587, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 29.37138080686878, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.616351842880249 and batch: 50, loss is 7.469650812149048 and perplexity is 1753.9941051963829
At time: 2.426264524459839 and batch: 100, loss is 6.578723392486572 and perplexity is 719.6200702190578
At time: 3.2462854385375977 and batch: 150, loss is 6.509691753387451 and perplexity is 671.6193612874924
At time: 4.055463552474976 and batch: 200, loss is 6.912864274978638 and perplexity is 1005.1220691706633
At time: 4.86506986618042 and batch: 250, loss is 7.291742792129517 and perplexity is 1468.1271092806912
At time: 5.6750853061676025 and batch: 300, loss is 7.165278558731079 and perplexity is 1293.7219281514426
At time: 6.4870264530181885 and batch: 350, loss is 7.189900360107422 and perplexity is 1325.9710794088896
At time: 7.299174785614014 and batch: 400, loss is 7.366039438247681 and perplexity is 1581.358296835092
At time: 8.111541032791138 and batch: 450, loss is 7.350982275009155 and perplexity is 1557.725891800814
At time: 8.9259614944458 and batch: 500, loss is 7.448798789978027 and perplexity is 1717.798468832614
At time: 9.736372232437134 and batch: 550, loss is 7.575218458175659 and perplexity is 1949.2860536340347
At time: 10.547839879989624 and batch: 600, loss is 7.366422595977784 and perplexity is 1581.9643225848981
At time: 11.358752489089966 and batch: 650, loss is 7.423565626144409 and perplexity is 1674.9952796644322
At time: 12.168213605880737 and batch: 700, loss is 7.437159271240234 and perplexity is 1697.920033501514
At time: 12.978793144226074 and batch: 750, loss is 7.051385860443116 and perplexity is 1154.4575517186504
At time: 13.787655115127563 and batch: 800, loss is 7.098636636734009 and perplexity is 1210.3158489695027
At time: 14.60184359550476 and batch: 850, loss is 7.042729988098144 and perplexity is 1144.5078383703121
At time: 15.41029667854309 and batch: 900, loss is 7.229070234298706 and perplexity is 1378.9398169342614
At time: 16.22046732902527 and batch: 950, loss is 7.014451923370362 and perplexity is 1112.5966908311946
At time: 17.031618118286133 and batch: 1000, loss is 7.148092241287231 and perplexity is 1271.6775855446524
At time: 17.8431556224823 and batch: 1050, loss is 7.461019487380981 and perplexity is 1738.9199609354202
At time: 18.65421175956726 and batch: 1100, loss is 7.285461368560791 and perplexity is 1458.9340839245044
At time: 19.465190172195435 and batch: 1150, loss is 6.882929925918579 and perplexity is 975.480261791649
At time: 20.281684398651123 and batch: 1200, loss is 7.162271766662598 and perplexity is 1289.8378176017416
At time: 21.092697858810425 and batch: 1250, loss is 7.234271411895752 and perplexity is 1386.1306119080455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 8.760908809021442 and perplexity of 6379.907061191968
Finished 1 epochs...
Completing Train Step...
At time: 23.329061031341553 and batch: 50, loss is 7.1323889636993405 and perplexity is 1251.8640552543143
At time: 24.162251234054565 and batch: 100, loss is 7.080427131652832 and perplexity is 1188.4760458486958
At time: 24.968069553375244 and batch: 150, loss is 6.957887964248657 and perplexity is 1051.4105938160546
At time: 25.774017810821533 and batch: 200, loss is 6.816604242324829 and perplexity is 912.8798221695374
At time: 26.578005075454712 and batch: 250, loss is 6.974095687866211 and perplexity is 1068.5904129298447
At time: 27.384001970291138 and batch: 300, loss is 6.964156255722046 and perplexity is 1058.0218408408548
At time: 28.19072723388672 and batch: 350, loss is 7.187687902450562 and perplexity is 1323.0406674430285
At time: 28.997736930847168 and batch: 400, loss is 7.098505239486695 and perplexity is 1210.156827246305
At time: 29.832016706466675 and batch: 450, loss is 6.933398065567016 and perplexity is 1025.9743911916466
At time: 30.643619060516357 and batch: 500, loss is 7.184003734588623 and perplexity is 1318.1753313964844
At time: 31.45858097076416 and batch: 550, loss is 7.103895397186279 and perplexity is 1216.6973748422438
At time: 32.26838755607605 and batch: 600, loss is 7.105420293807984 and perplexity is 1218.554127877199
At time: 33.078857421875 and batch: 650, loss is 6.728486251831055 and perplexity is 835.880994463041
At time: 33.89012050628662 and batch: 700, loss is 6.952496328353882 and perplexity is 1045.757025405117
At time: 34.701385259628296 and batch: 750, loss is 6.783662366867065 and perplexity is 883.297768006164
At time: 35.511831760406494 and batch: 800, loss is 6.798788194656372 and perplexity is 896.7599344703173
At time: 36.32343864440918 and batch: 850, loss is 6.851489810943604 and perplexity is 945.2881587853088
At time: 37.1553008556366 and batch: 900, loss is 6.958165721893311 and perplexity is 1051.7026717077124
At time: 37.97261905670166 and batch: 950, loss is 7.171625490188599 and perplexity is 1301.9592056200495
At time: 38.784539222717285 and batch: 1000, loss is 7.008951091766358 and perplexity is 1106.493286082937
At time: 39.59613585472107 and batch: 1050, loss is 6.816483106613159 and perplexity is 912.7692465200746
At time: 40.40753793716431 and batch: 1100, loss is 6.847065525054932 and perplexity is 941.1151717743805
At time: 41.21992087364197 and batch: 1150, loss is 7.1317819976806645 and perplexity is 1251.1044468643336
At time: 42.03180813789368 and batch: 1200, loss is 7.028384618759155 and perplexity is 1128.2066535264091
At time: 42.86071419715881 and batch: 1250, loss is 6.953692646026611 and perplexity is 1047.008831645658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.420467181797445 and perplexity of 614.2900322202828
Finished 2 epochs...
Completing Train Step...
At time: 45.17974877357483 and batch: 50, loss is 6.931891117095947 and perplexity is 1024.4294650059248
At time: 46.004998207092285 and batch: 100, loss is 7.146743202209473 and perplexity is 1269.9631994343738
At time: 46.8140549659729 and batch: 150, loss is 6.777689609527588 and perplexity is 878.0377687659126
At time: 47.619149684906006 and batch: 200, loss is 6.869986400604248 and perplexity is 962.9354703824238
At time: 48.42056727409363 and batch: 250, loss is 6.787654123306274 and perplexity is 886.8307242184857
At time: 49.22758388519287 and batch: 300, loss is 6.943668985366822 and perplexity is 1036.5663935697557
At time: 50.036344051361084 and batch: 350, loss is 7.359872999191285 and perplexity is 1571.636951118756
At time: 50.84452176094055 and batch: 400, loss is 7.508547410964966 and perplexity is 1823.5627307850334
At time: 51.654463052749634 and batch: 450, loss is 7.165140657424927 and perplexity is 1293.5435345083938
At time: 52.4629328250885 and batch: 500, loss is 7.3724719905853275 and perplexity is 1591.563253615214
At time: 53.272629499435425 and batch: 550, loss is 7.301302223205567 and perplexity is 1482.2288641818554
At time: 54.07858347892761 and batch: 600, loss is 6.857872047424316 and perplexity is 951.3405045639649
At time: 54.88558840751648 and batch: 650, loss is 6.771878261566162 and perplexity is 872.9499835321287
At time: 55.69054841995239 and batch: 700, loss is 6.834462442398071 and perplexity is 929.3286487484461
At time: 56.49680733680725 and batch: 750, loss is 6.924663181304932 and perplexity is 1017.0516499172462
At time: 57.30168342590332 and batch: 800, loss is 6.947266216278076 and perplexity is 1040.3018769122293
At time: 58.10678219795227 and batch: 850, loss is 6.6953816413879395 and perplexity is 808.6624944038574
At time: 58.915207386016846 and batch: 900, loss is 6.715093278884888 and perplexity is 824.7606960675259
At time: 59.719398975372314 and batch: 950, loss is 6.688809671401978 and perplexity is 803.3654139541368
At time: 60.525623083114624 and batch: 1000, loss is 6.822842836380005 and perplexity is 918.5927104678864
At time: 61.38600397109985 and batch: 1050, loss is 6.613550653457642 and perplexity is 745.1240044522305
At time: 62.19301152229309 and batch: 1100, loss is 6.982162780761719 and perplexity is 1077.245695603902
At time: 62.99833059310913 and batch: 1150, loss is 6.946134815216064 and perplexity is 1039.1255438416026
At time: 63.805299282073975 and batch: 1200, loss is 6.868211097717285 and perplexity is 961.2274848063452
At time: 64.61152219772339 and batch: 1250, loss is 6.779777984619141 and perplexity is 879.8733570028916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.041923133126141 and perplexity of 420.7013220643077
Finished 3 epochs...
Completing Train Step...
At time: 66.8834240436554 and batch: 50, loss is 6.574945745468139 and perplexity is 716.9067278579854
At time: 67.69977951049805 and batch: 100, loss is 6.707728147506714 and perplexity is 818.7085400060146
At time: 68.50746393203735 and batch: 150, loss is 6.717425966262818 and perplexity is 826.6868506178394
At time: 69.31183695793152 and batch: 200, loss is 6.698927135467529 and perplexity is 811.5346911596971
At time: 70.11679482460022 and batch: 250, loss is 6.729201049804687 and perplexity is 836.4786940958753
At time: 70.92303395271301 and batch: 300, loss is 6.845013351440429 and perplexity is 939.1858204097477
At time: 71.7324948310852 and batch: 350, loss is 6.763920278549194 and perplexity is 866.0306309502382
At time: 72.53817200660706 and batch: 400, loss is 6.8284760761260985 and perplexity is 923.781965872149
At time: 73.3428122997284 and batch: 450, loss is 6.508994016647339 and perplexity is 671.1509112301643
At time: 74.15005445480347 and batch: 500, loss is 6.680779132843018 and perplexity is 796.9397921543417
At time: 74.95721769332886 and batch: 550, loss is 6.795301914215088 and perplexity is 893.6390212006953
At time: 75.76654243469238 and batch: 600, loss is 6.927330989837646 and perplexity is 1019.7685714889759
At time: 76.57759976387024 and batch: 650, loss is 6.941353397369385 and perplexity is 1034.1689097339215
At time: 77.38792681694031 and batch: 700, loss is 6.858423070907593 and perplexity is 951.8648599753769
At time: 78.19571375846863 and batch: 750, loss is 6.7220876598358155 and perplexity is 830.5496078963824
At time: 79.00408411026001 and batch: 800, loss is 6.6211308670043945 and perplexity is 750.7936649955907
At time: 79.81188201904297 and batch: 850, loss is 6.724827871322632 and perplexity is 832.8286105208563
At time: 80.61853694915771 and batch: 900, loss is 6.563380260467529 and perplexity is 708.6631164212072
At time: 81.4295301437378 and batch: 950, loss is 6.702240009307861 and perplexity is 814.2276614810519
At time: 82.29020524024963 and batch: 1000, loss is 6.809938316345215 and perplexity is 906.8148695670103
At time: 83.09881472587585 and batch: 1050, loss is 7.0191249084472656 and perplexity is 1117.8080052781875
At time: 83.90803742408752 and batch: 1100, loss is 6.998960266113281 and perplexity is 1095.4935443227073
At time: 84.71751713752747 and batch: 1150, loss is 6.9547078323364255 and perplexity is 1048.0722803857675
At time: 85.52530121803284 and batch: 1200, loss is 6.975904512405395 and perplexity is 1070.5250546773352
At time: 86.33550977706909 and batch: 1250, loss is 6.951641187667847 and perplexity is 1044.8631382790838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.8020055172217155 and perplexity of 899.6497466794566
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 88.56911182403564 and batch: 50, loss is 6.30213846206665 and perplexity is 545.7377025554746
At time: 89.40579581260681 and batch: 100, loss is 6.167723054885864 and perplexity is 477.09854120400064
At time: 90.21730470657349 and batch: 150, loss is 6.055742797851562 and perplexity is 426.55563241532144
At time: 91.03053116798401 and batch: 200, loss is 6.090750226974487 and perplexity is 441.7527016517268
At time: 91.8423924446106 and batch: 250, loss is 6.111187982559204 and perplexity is 450.8740276149425
At time: 92.6573235988617 and batch: 300, loss is 6.113741760253906 and perplexity is 452.02693115287383
At time: 93.4642767906189 and batch: 350, loss is 6.135969171524048 and perplexity is 462.18681532469327
At time: 94.27324652671814 and batch: 400, loss is 6.127345037460327 and perplexity is 458.21799269469636
At time: 95.08383989334106 and batch: 450, loss is 6.0542588043212895 and perplexity is 425.92309607251883
At time: 95.90018248558044 and batch: 500, loss is 6.030275545120239 and perplexity is 415.82959341279724
At time: 96.71369194984436 and batch: 550, loss is 6.03447829246521 and perplexity is 417.580897699408
At time: 97.5273118019104 and batch: 600, loss is 6.056787424087524 and perplexity is 427.00145643923395
At time: 98.37053298950195 and batch: 650, loss is 6.0205983066558835 and perplexity is 411.82491962117285
At time: 99.20783710479736 and batch: 700, loss is 6.070293397903442 and perplexity is 432.8076478042121
At time: 100.01991558074951 and batch: 750, loss is 6.014847574234008 and perplexity is 409.4634213840831
At time: 100.82974553108215 and batch: 800, loss is 6.031022958755493 and perplexity is 416.1405062966466
At time: 101.64308023452759 and batch: 850, loss is 6.0571956443786625 and perplexity is 427.17580268151386
At time: 102.45534420013428 and batch: 900, loss is 5.997195930480957 and perplexity is 402.2991356695222
At time: 103.31399345397949 and batch: 950, loss is 5.949004459381103 and perplexity is 383.37148713053074
At time: 104.12528705596924 and batch: 1000, loss is 5.945284109115601 and perplexity is 381.94786075301914
At time: 104.93613767623901 and batch: 1050, loss is 5.955864973068238 and perplexity is 386.0106551379744
At time: 105.75253057479858 and batch: 1100, loss is 5.945487442016602 and perplexity is 382.02553121578984
At time: 106.60644149780273 and batch: 1150, loss is 5.97214259147644 and perplexity is 392.3454067473624
At time: 107.44014525413513 and batch: 1200, loss is 5.968146905899048 and perplexity is 390.7808456928694
At time: 108.26050043106079 and batch: 1250, loss is 5.957351760864258 and perplexity is 386.584997926291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.630978772239963 and perplexity of 278.9349979001389
Finished 5 epochs...
Completing Train Step...
At time: 110.51888680458069 and batch: 50, loss is 5.975051383972168 and perplexity is 393.48831956449976
At time: 111.32979106903076 and batch: 100, loss is 5.973585186004638 and perplexity is 392.9118105315722
At time: 112.163813829422 and batch: 150, loss is 5.880646915435791 and perplexity is 358.0407889180393
At time: 112.97549510002136 and batch: 200, loss is 5.9273452472686765 and perplexity is 375.15724094392306
At time: 113.78701424598694 and batch: 250, loss is 5.946618404388428 and perplexity is 382.4578321286256
At time: 114.60215497016907 and batch: 300, loss is 5.953546447753906 and perplexity is 385.1167163727155
At time: 115.41269826889038 and batch: 350, loss is 5.977404222488404 and perplexity is 394.415224039134
At time: 116.22362804412842 and batch: 400, loss is 5.96477352142334 and perplexity is 389.4648126453384
At time: 117.05423903465271 and batch: 450, loss is 5.905257663726807 and perplexity is 366.96176634341924
At time: 117.86335229873657 and batch: 500, loss is 5.910427885055542 and perplexity is 368.8639530203552
At time: 118.67344737052917 and batch: 550, loss is 5.91145396232605 and perplexity is 369.2426301812492
At time: 119.48807835578918 and batch: 600, loss is 5.920846338272095 and perplexity is 372.72703357735594
At time: 120.30162596702576 and batch: 650, loss is 5.887440319061279 and perplexity is 360.4813851028464
At time: 121.11384177207947 and batch: 700, loss is 5.9152788639068605 and perplexity is 370.65765133432365
At time: 121.92630648612976 and batch: 750, loss is 5.857970705032349 and perplexity is 350.0131429388721
At time: 122.73869109153748 and batch: 800, loss is 5.8716500377655025 and perplexity is 354.8339869724464
At time: 123.55066442489624 and batch: 850, loss is 5.8981111621856686 and perplexity is 364.3486220561833
At time: 124.40960335731506 and batch: 900, loss is 5.865020847320556 and perplexity is 352.4895044928044
At time: 125.22018623352051 and batch: 950, loss is 5.832844657897949 and perplexity is 341.3282614876162
At time: 126.03288626670837 and batch: 1000, loss is 5.8364323806762695 and perplexity is 342.5550520425369
At time: 126.84942865371704 and batch: 1050, loss is 5.848216829299926 and perplexity is 346.6157540276081
At time: 127.66356825828552 and batch: 1100, loss is 5.8307273387908936 and perplexity is 340.6063251922523
At time: 128.4777798652649 and batch: 1150, loss is 5.855806150436401 and perplexity is 349.2563397480814
At time: 129.28872537612915 and batch: 1200, loss is 5.8525117492675784 and perplexity is 348.10764242800184
At time: 130.1002025604248 and batch: 1250, loss is 5.8368159103393555 and perplexity is 342.6864572635245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.529620316776916 and perplexity of 252.04819438284557
Finished 6 epochs...
Completing Train Step...
At time: 132.31874895095825 and batch: 50, loss is 5.831163320541382 and perplexity is 340.7548557100819
At time: 133.15336728096008 and batch: 100, loss is 5.8345033645629885 and perplexity is 341.8948947591889
At time: 133.96446585655212 and batch: 150, loss is 5.747413253784179 and perplexity is 313.3789790434421
At time: 134.77661991119385 and batch: 200, loss is 5.794270696640015 and perplexity is 328.41258422098224
At time: 135.5887315273285 and batch: 250, loss is 5.814430112838745 and perplexity is 335.10037467400497
At time: 136.40071487426758 and batch: 300, loss is 5.804364767074585 and perplexity is 331.7443914362171
At time: 137.2169213294983 and batch: 350, loss is 5.821836719512939 and perplexity is 337.59154551855204
At time: 138.02876353263855 and batch: 400, loss is 5.79769832611084 and perplexity is 329.5401922805676
At time: 138.838228225708 and batch: 450, loss is 5.737189884185791 and perplexity is 310.19151101028575
At time: 139.6492726802826 and batch: 500, loss is 5.743503665924072 and perplexity is 312.15618825289295
At time: 140.45815992355347 and batch: 550, loss is 5.7479632949829105 and perplexity is 313.551397807097
At time: 141.26864552497864 and batch: 600, loss is 5.759165668487549 and perplexity is 317.08366563042404
At time: 142.08112978935242 and batch: 650, loss is 5.737160234451294 and perplexity is 310.1823140506855
At time: 142.8941011428833 and batch: 700, loss is 5.7696988487243654 and perplexity is 320.44121683394025
At time: 143.70934796333313 and batch: 750, loss is 5.720844783782959 and perplexity is 305.16261052097946
At time: 144.56755137443542 and batch: 800, loss is 5.732160367965698 and perplexity is 308.63531451255307
At time: 145.37948870658875 and batch: 850, loss is 5.75873384475708 and perplexity is 316.9467709383762
At time: 146.1910846233368 and batch: 900, loss is 5.736257963180542 and perplexity is 309.90257168075186
At time: 147.00242614746094 and batch: 950, loss is 5.715927667617798 and perplexity is 303.66577359219326
At time: 147.81434035301208 and batch: 1000, loss is 5.721003227233886 and perplexity is 305.21096536873773
At time: 148.62594389915466 and batch: 1050, loss is 5.724575071334839 and perplexity is 306.30308062655
At time: 149.4361982345581 and batch: 1100, loss is 5.708817110061646 and perplexity is 301.5141991435072
At time: 150.2771532535553 and batch: 1150, loss is 5.736634225845337 and perplexity is 310.0191983879611
At time: 151.08905386924744 and batch: 1200, loss is 5.7372378158569335 and perplexity is 310.2063793641123
At time: 151.90113234519958 and batch: 1250, loss is 5.71815149307251 and perplexity is 304.341824700452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.453973700530337 and perplexity of 233.68491725165038
Finished 7 epochs...
Completing Train Step...
At time: 154.12834072113037 and batch: 50, loss is 5.712470302581787 and perplexity is 302.6177029892692
At time: 154.93693256378174 and batch: 100, loss is 5.723603897094726 and perplexity is 306.0057513676064
At time: 155.74918699264526 and batch: 150, loss is 5.643299560546875 and perplexity is 282.39295559946936
At time: 156.58581376075745 and batch: 200, loss is 5.690874214172363 and perplexity is 296.15240813852745
At time: 157.39649415016174 and batch: 250, loss is 5.715923767089844 and perplexity is 303.6645891376647
At time: 158.20935034751892 and batch: 300, loss is 5.7075175952911374 and perplexity is 301.12263146729623
At time: 159.02058792114258 and batch: 350, loss is 5.732517976760864 and perplexity is 308.7457049526411
At time: 159.8306155204773 and batch: 400, loss is 5.708604345321655 and perplexity is 301.4500543774371
At time: 160.64233541488647 and batch: 450, loss is 5.660194253921508 and perplexity is 287.204427723995
At time: 161.45643782615662 and batch: 500, loss is 5.673685712814331 and perplexity is 291.10549079825125
At time: 162.2650966644287 and batch: 550, loss is 5.669952621459961 and perplexity is 290.02079330445326
At time: 163.07630109786987 and batch: 600, loss is 5.692682304382324 and perplexity is 296.6883627895063
At time: 163.88641119003296 and batch: 650, loss is 5.67863881111145 and perplexity is 292.55094168404196
At time: 164.69504070281982 and batch: 700, loss is 5.7085137367248535 and perplexity is 301.42274164840694
At time: 165.53312182426453 and batch: 750, loss is 5.671406688690186 and perplexity is 290.4428097818901
At time: 166.3455171585083 and batch: 800, loss is 5.683443164825439 and perplexity is 293.95984160403805
At time: 167.1562943458557 and batch: 850, loss is 5.70554741859436 and perplexity is 300.5299507105272
At time: 167.97091960906982 and batch: 900, loss is 5.68350471496582 and perplexity is 293.9779354303882
At time: 168.78128123283386 and batch: 950, loss is 5.669130668640137 and perplexity is 289.7825078387099
At time: 169.59128832817078 and batch: 1000, loss is 5.66886887550354 and perplexity is 289.706654696401
At time: 170.40246081352234 and batch: 1050, loss is 5.670432682037354 and perplexity is 290.16005427812007
At time: 171.2139856815338 and batch: 1100, loss is 5.653481283187866 and perplexity is 285.28288963659037
At time: 172.04825592041016 and batch: 1150, loss is 5.684523668289184 and perplexity is 294.2776378901686
At time: 172.85751366615295 and batch: 1200, loss is 5.68313702583313 and perplexity is 293.86986280806167
At time: 173.69763827323914 and batch: 1250, loss is 5.669707717895508 and perplexity is 289.9497748750957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4106690344149175 and perplexity of 223.78125496966788
Finished 8 epochs...
Completing Train Step...
At time: 175.92075276374817 and batch: 50, loss is 5.654718246459961 and perplexity is 285.6359924358019
At time: 176.7829511165619 and batch: 100, loss is 5.669069967269897 and perplexity is 289.7649181772763
At time: 177.59341168403625 and batch: 150, loss is 5.591742925643921 and perplexity is 268.20266988966813
At time: 178.40427803993225 and batch: 200, loss is 5.635113410949707 and perplexity is 280.09068085931955
At time: 179.2195761203766 and batch: 250, loss is 5.658477983474731 and perplexity is 286.71193000304174
At time: 180.03101658821106 and batch: 300, loss is 5.650403337478638 and perplexity is 284.4061543544811
At time: 180.8703875541687 and batch: 350, loss is 5.6749543285369874 and perplexity is 291.47502615044806
At time: 181.7038381099701 and batch: 400, loss is 5.654028224945068 and perplexity is 285.4389654398441
At time: 182.53277492523193 and batch: 450, loss is 5.606893939971924 and perplexity is 272.29715183646425
At time: 183.3494725227356 and batch: 500, loss is 5.6204740428924564 and perplexity is 276.02019763937864
At time: 184.15795707702637 and batch: 550, loss is 5.616809005737305 and perplexity is 275.0104249170447
At time: 184.96967267990112 and batch: 600, loss is 5.637635383605957 and perplexity is 280.7979533838262
At time: 185.7803988456726 and batch: 650, loss is 5.6232325553894045 and perplexity is 276.78265394311245
At time: 186.62039709091187 and batch: 700, loss is 5.65612940788269 and perplexity is 286.03935546750785
At time: 187.43611526489258 and batch: 750, loss is 5.618256168365479 and perplexity is 275.4086978396797
At time: 188.25379085540771 and batch: 800, loss is 5.635278024673462 and perplexity is 280.13679142439673
At time: 189.06688523292542 and batch: 850, loss is 5.656596364974976 and perplexity is 286.172954763358
At time: 189.87800669670105 and batch: 900, loss is 5.640449810028076 and perplexity is 281.5893517068934
At time: 190.68860411643982 and batch: 950, loss is 5.623510618209838 and perplexity is 276.8596276097765
At time: 191.50005054473877 and batch: 1000, loss is 5.622247848510742 and perplexity is 276.5102383066681
At time: 192.31240224838257 and batch: 1050, loss is 5.629280576705932 and perplexity is 278.46171371070915
At time: 193.12417149543762 and batch: 1100, loss is 5.613820781707764 and perplexity is 274.1898587852523
At time: 193.9366579055786 and batch: 1150, loss is 5.637666654586792 and perplexity is 280.80673434853895
At time: 194.75129461288452 and batch: 1200, loss is 5.641157054901123 and perplexity is 281.78857477363493
At time: 195.56348323822021 and batch: 1250, loss is 5.629220685958862 and perplexity is 278.44503693004197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3779991873859485 and perplexity of 216.58848862650925
Finished 9 epochs...
Completing Train Step...
At time: 197.77318835258484 and batch: 50, loss is 5.60819257736206 and perplexity is 272.65099680749285
At time: 198.62013149261475 and batch: 100, loss is 5.62611951828003 and perplexity is 277.58286973467557
At time: 199.43423199653625 and batch: 150, loss is 5.548070907592773 and perplexity is 256.74179922379125
At time: 200.24465465545654 and batch: 200, loss is 5.596078481674194 and perplexity is 269.3680019482612
At time: 201.06800079345703 and batch: 250, loss is 5.617875061035156 and perplexity is 275.3037575641227
At time: 201.89765000343323 and batch: 300, loss is 5.611105890274048 and perplexity is 273.44647264935276
At time: 202.70936036109924 and batch: 350, loss is 5.639166288375854 and perplexity is 281.2281575268047
At time: 203.5197651386261 and batch: 400, loss is 5.615016937255859 and perplexity is 274.51802873809623
At time: 204.32985734939575 and batch: 450, loss is 5.568620500564575 and perplexity is 262.07232114225417
At time: 205.14522194862366 and batch: 500, loss is 5.582880001068116 and perplexity is 265.8361126558915
At time: 205.9565167427063 and batch: 550, loss is 5.575877571105957 and perplexity is 263.9811162167511
At time: 206.76724123954773 and batch: 600, loss is 5.595352764129639 and perplexity is 269.17258777962735
At time: 207.60403490066528 and batch: 650, loss is 5.582981548309326 and perplexity is 265.86310895042226
At time: 208.4160282611847 and batch: 700, loss is 5.614780387878418 and perplexity is 274.4530993491078
At time: 209.22683835029602 and batch: 750, loss is 5.578370504379272 and perplexity is 264.6400244908309
At time: 210.03753352165222 and batch: 800, loss is 5.600785827636718 and perplexity is 270.6389994902261
At time: 210.85427236557007 and batch: 850, loss is 5.623091363906861 and perplexity is 276.7435773485659
At time: 211.66500663757324 and batch: 900, loss is 5.608336048126221 and perplexity is 272.69011706059314
At time: 212.4732117652893 and batch: 950, loss is 5.58816855430603 and perplexity is 267.245725210323
At time: 213.2842001914978 and batch: 1000, loss is 5.5830058574676515 and perplexity is 265.86957193738516
At time: 214.0947825908661 and batch: 1050, loss is 5.590956592559815 and perplexity is 267.9918561528588
At time: 214.9060401916504 and batch: 1100, loss is 5.575262088775634 and perplexity is 263.8186904943893
At time: 215.7170069217682 and batch: 1150, loss is 5.600948610305786 and perplexity is 270.68305841483436
At time: 216.5291929244995 and batch: 1200, loss is 5.599431266784668 and perplexity is 270.2726506733772
At time: 217.3448851108551 and batch: 1250, loss is 5.5889805316925045 and perplexity is 267.4628108182008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.348775821880703 and perplexity of 210.35063354237585
Finished 10 epochs...
Completing Train Step...
At time: 219.5989863872528 and batch: 50, loss is 5.565348253250122 and perplexity is 261.2161572471624
At time: 220.42664289474487 and batch: 100, loss is 5.5841030025482175 and perplexity is 266.16142950607986
At time: 221.25374746322632 and batch: 150, loss is 5.504061031341553 and perplexity is 245.68765433097911
At time: 222.078120470047 and batch: 200, loss is 5.552769813537598 and perplexity is 257.95104362832905
At time: 222.90284729003906 and batch: 250, loss is 5.578218584060669 and perplexity is 264.59982334775765
At time: 223.7306501865387 and batch: 300, loss is 5.569988641738892 and perplexity is 262.4311184621843
At time: 224.5525622367859 and batch: 350, loss is 5.5971441745758055 and perplexity is 269.6552185310128
At time: 225.36422872543335 and batch: 400, loss is 5.5691452312469485 and perplexity is 262.20987461626635
At time: 226.1780710220337 and batch: 450, loss is 5.532074842453003 and perplexity is 252.66761302606753
At time: 226.98822593688965 and batch: 500, loss is 5.543924312591553 and perplexity is 255.6793991561943
At time: 227.82607316970825 and batch: 550, loss is 5.539033670425415 and perplexity is 254.43201544563638
At time: 228.6377353668213 and batch: 600, loss is 5.558301830291748 and perplexity is 259.381987476816
At time: 229.47664427757263 and batch: 650, loss is 5.546954498291016 and perplexity is 256.4553302290669
At time: 230.28790712356567 and batch: 700, loss is 5.5767128372192385 and perplexity is 264.20170280910116
At time: 231.09885358810425 and batch: 750, loss is 5.541228199005127 and perplexity is 254.9909868901991
At time: 231.9105200767517 and batch: 800, loss is 5.565847797393799 and perplexity is 261.346678846685
At time: 232.72265529632568 and batch: 850, loss is 5.591652679443359 and perplexity is 268.1784667098691
At time: 233.53499841690063 and batch: 900, loss is 5.5720917510986325 and perplexity is 262.98362058625736
At time: 234.34673047065735 and batch: 950, loss is 5.554944324493408 and perplexity is 258.51257130153505
At time: 235.15852522850037 and batch: 1000, loss is 5.549452028274536 and perplexity is 257.09663561210544
At time: 235.97463750839233 and batch: 1050, loss is 5.556662111282349 and perplexity is 258.95702240812136
At time: 236.78341150283813 and batch: 1100, loss is 5.542286376953125 and perplexity is 255.26095554171022
At time: 237.5943865776062 and batch: 1150, loss is 5.566343383789063 and perplexity is 261.47623080461113
At time: 238.40563678741455 and batch: 1200, loss is 5.565269088745117 and perplexity is 261.1954790178762
At time: 239.21627688407898 and batch: 1250, loss is 5.545311059951782 and perplexity is 256.03420784618606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.325616766936587 and perplexity of 205.53508862686925
Finished 11 epochs...
Completing Train Step...
At time: 241.4766354560852 and batch: 50, loss is 5.524947290420532 and perplexity is 250.8731142559858
At time: 242.3438858985901 and batch: 100, loss is 5.542953977584839 and perplexity is 255.43142481324688
At time: 243.17087078094482 and batch: 150, loss is 5.46535436630249 and perplexity is 236.35959814698248
At time: 243.9974400997162 and batch: 200, loss is 5.5135993480682375 and perplexity is 248.04231288123373
At time: 244.82699513435364 and batch: 250, loss is 5.532259693145752 and perplexity is 252.71432312643486
At time: 245.65627002716064 and batch: 300, loss is 5.527043237686157 and perplexity is 251.39948250117712
At time: 246.48624968528748 and batch: 350, loss is 5.55225832939148 and perplexity is 257.8191394953506
At time: 247.30813789367676 and batch: 400, loss is 5.529058237075805 and perplexity is 251.90656301686806
At time: 248.13769555091858 and batch: 450, loss is 5.487647876739502 and perplexity is 241.68805766361035
At time: 249.0339343547821 and batch: 500, loss is 5.500738468170166 and perplexity is 244.87269620368696
At time: 249.84754586219788 and batch: 550, loss is 5.493513984680176 and perplexity is 243.10999243171347
At time: 250.66323924064636 and batch: 600, loss is 5.518471946716309 and perplexity is 249.25387284527233
At time: 251.47682309150696 and batch: 650, loss is 5.503269309997559 and perplexity is 245.4932151520701
At time: 252.28889679908752 and batch: 700, loss is 5.5286008644104 and perplexity is 251.7913741848276
At time: 253.10578083992004 and batch: 750, loss is 5.495693998336792 and perplexity is 243.64055364049239
At time: 253.9192225933075 and batch: 800, loss is 5.524442558288574 and perplexity is 250.7465224843313
At time: 254.73185300827026 and batch: 850, loss is 5.546814212799072 and perplexity is 256.419355790309
At time: 255.54511308670044 and batch: 900, loss is 5.5303874206542964 and perplexity is 252.24161570768257
At time: 256.3639371395111 and batch: 950, loss is 5.51307487487793 and perplexity is 247.9122554468656
At time: 257.17576909065247 and batch: 1000, loss is 5.506565446853638 and perplexity is 246.30372943563094
At time: 257.9929277896881 and batch: 1050, loss is 5.509931621551513 and perplexity is 247.13422783479135
At time: 258.80351638793945 and batch: 1100, loss is 5.4969479370117185 and perplexity is 243.94625557916845
At time: 259.6160943508148 and batch: 1150, loss is 5.524087781906128 and perplexity is 250.6575793185736
At time: 260.42703795433044 and batch: 1200, loss is 5.527881946563721 and perplexity is 251.6104219249843
At time: 261.23873925209045 and batch: 1250, loss is 5.509325942993164 and perplexity is 246.9845892529724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.302363708941606 and perplexity of 200.81090797796963
Finished 12 epochs...
Completing Train Step...
At time: 263.5262484550476 and batch: 50, loss is 5.49001727104187 and perplexity is 242.2613909284485
At time: 264.340131521225 and batch: 100, loss is 5.512299365997315 and perplexity is 247.72007182083945
At time: 265.15279030799866 and batch: 150, loss is 5.432002487182618 and perplexity is 228.60656910327185
At time: 265.99048042297363 and batch: 200, loss is 5.480371646881103 and perplexity is 239.93586222102797
At time: 266.80980110168457 and batch: 250, loss is 5.500897045135498 and perplexity is 244.9115304517709
At time: 267.6239228248596 and batch: 300, loss is 5.492246446609497 and perplexity is 242.80203647512485
At time: 268.43728518486023 and batch: 350, loss is 5.520399141311645 and perplexity is 249.7346967336678
At time: 269.2481858730316 and batch: 400, loss is 5.495834617614746 and perplexity is 243.67481660818612
At time: 270.11100912094116 and batch: 450, loss is 5.45858850479126 and perplexity is 234.76581956151097
At time: 270.9377188682556 and batch: 500, loss is 5.471515035629272 and perplexity is 237.82022607651766
At time: 271.7490565776825 and batch: 550, loss is 5.4641108798980715 and perplexity is 236.06587086092443
At time: 272.5655553340912 and batch: 600, loss is 5.490166816711426 and perplexity is 242.29762277945318
At time: 273.37779903411865 and batch: 650, loss is 5.477173557281494 and perplexity is 239.1697515337636
At time: 274.19098448753357 and batch: 700, loss is 5.507252655029297 and perplexity is 246.47304954461248
At time: 275.00541377067566 and batch: 750, loss is 5.467956333160401 and perplexity is 236.975398786912
At time: 275.8168799877167 and batch: 800, loss is 5.4959678459167485 and perplexity is 243.70728315292678
At time: 276.62948727607727 and batch: 850, loss is 5.523689641952514 and perplexity is 250.55780238548056
At time: 277.4459810256958 and batch: 900, loss is 5.506651201248169 and perplexity is 246.3248519684815
At time: 278.2918930053711 and batch: 950, loss is 5.489930820465088 and perplexity is 242.24044819673946
At time: 279.1232113838196 and batch: 1000, loss is 5.481367950439453 and perplexity is 240.17503029655325
At time: 279.9602437019348 and batch: 1050, loss is 5.486561985015869 and perplexity is 241.42575304507452
At time: 280.7811667919159 and batch: 1100, loss is 5.47184235572815 and perplexity is 237.8980821576671
At time: 281.59690117836 and batch: 1150, loss is 5.500481338500976 and perplexity is 244.80974026258556
At time: 282.41370725631714 and batch: 1200, loss is 5.501843385696411 and perplexity is 245.14340986801585
At time: 283.2255189418793 and batch: 1250, loss is 5.483844947814942 and perplexity is 240.7706806238526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.290240879476506 and perplexity of 198.39120802455676
Finished 13 epochs...
Completing Train Step...
At time: 285.45816135406494 and batch: 50, loss is 5.464788036346436 and perplexity is 236.22577852275842
At time: 286.29859590530396 and batch: 100, loss is 5.485843324661255 and perplexity is 241.25231225775192
At time: 287.1112132072449 and batch: 150, loss is 5.406466751098633 and perplexity is 222.84283586785523
At time: 287.9272804260254 and batch: 200, loss is 5.455912780761719 and perplexity is 234.13849067056927
At time: 288.74047565460205 and batch: 250, loss is 5.472198648452759 and perplexity is 237.982858615259
At time: 289.56093764305115 and batch: 300, loss is 5.469800987243652 and perplexity is 237.41293985580396
At time: 290.3736617565155 and batch: 350, loss is 5.497557106018067 and perplexity is 244.09490534910148
At time: 291.2338581085205 and batch: 400, loss is 5.473251638412475 and perplexity is 238.23358415853858
At time: 292.0482335090637 and batch: 450, loss is 5.431924047470093 and perplexity is 228.58863797297573
At time: 292.86067509651184 and batch: 500, loss is 5.439540100097656 and perplexity is 230.33622748941065
At time: 293.70250391960144 and batch: 550, loss is 5.436335687637329 and perplexity is 229.5993165258816
At time: 294.5203495025635 and batch: 600, loss is 5.4606075191497805 and perplexity is 235.2402939462135
At time: 295.3332517147064 and batch: 650, loss is 5.443737850189209 and perplexity is 231.3051536415688
At time: 296.14599442481995 and batch: 700, loss is 5.474923086166382 and perplexity is 238.63211211423186
At time: 296.9559426307678 and batch: 750, loss is 5.438174047470093 and perplexity is 230.02179089798912
At time: 297.7663688659668 and batch: 800, loss is 5.468487119674682 and perplexity is 237.1012155207635
At time: 298.5778748989105 and batch: 850, loss is 5.49479305267334 and perplexity is 243.4211455924548
At time: 299.3894968032837 and batch: 900, loss is 5.481656904220581 and perplexity is 240.24443980727733
At time: 300.2017002105713 and batch: 950, loss is 5.459552555084229 and perplexity is 234.9922547485552
At time: 301.01322293281555 and batch: 1000, loss is 5.452101488113403 and perplexity is 233.2478187462219
At time: 301.8236930370331 and batch: 1050, loss is 5.456539669036865 and perplexity is 234.28531536166142
At time: 302.63423228263855 and batch: 1100, loss is 5.439863500595092 and perplexity is 230.4107303864459
At time: 303.45263719558716 and batch: 1150, loss is 5.470719585418701 and perplexity is 237.63112714696956
At time: 304.2659001350403 and batch: 1200, loss is 5.47121955871582 and perplexity is 237.74996607077605
At time: 305.085729598999 and batch: 1250, loss is 5.457883377075195 and perplexity is 234.60033802501886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.274912590528056 and perplexity of 195.37339828058657
Finished 14 epochs...
Completing Train Step...
At time: 307.4359884262085 and batch: 50, loss is 5.440169486999512 and perplexity is 230.48124372488778
At time: 308.2759552001953 and batch: 100, loss is 5.455691556930542 and perplexity is 234.0866993855817
At time: 309.0877757072449 and batch: 150, loss is 5.379011993408203 and perplexity is 216.80796187529438
At time: 309.90079951286316 and batch: 200, loss is 5.4250149822235105 and perplexity is 227.01474747548627
At time: 310.71851921081543 and batch: 250, loss is 5.445269079208374 and perplexity is 231.65960610980378
At time: 311.580078125 and batch: 300, loss is 5.437543058395386 and perplexity is 229.87669544262567
At time: 312.3926646709442 and batch: 350, loss is 5.467555131912231 and perplexity is 236.88034303065234
At time: 313.20468258857727 and batch: 400, loss is 5.443223218917847 and perplexity is 231.18614740107057
At time: 314.0161521434784 and batch: 450, loss is 5.40249116897583 and perplexity is 221.9586645857013
At time: 314.82880449295044 and batch: 500, loss is 5.410846757888794 and perplexity is 223.82102968603516
At time: 315.6452045440674 and batch: 550, loss is 5.40964708328247 and perplexity is 223.55267827972824
At time: 316.457998752594 and batch: 600, loss is 5.43600640296936 and perplexity is 229.5237254373497
At time: 317.2691013813019 and batch: 650, loss is 5.425981960296631 and perplexity is 227.23437192747144
At time: 318.08112049102783 and batch: 700, loss is 5.449240207672119 and perplexity is 232.5813852057687
At time: 318.89425587654114 and batch: 750, loss is 5.425385656356812 and perplexity is 227.09891156801572
At time: 319.70528864860535 and batch: 800, loss is 5.44742522239685 and perplexity is 232.15963626615232
At time: 320.52084255218506 and batch: 850, loss is 5.47573034286499 and perplexity is 238.82482726006984
At time: 321.3340811729431 and batch: 900, loss is 5.458778505325317 and perplexity is 234.8104294304213
At time: 322.1472864151001 and batch: 950, loss is 5.438253917694092 and perplexity is 230.0401635236559
At time: 322.9596848487854 and batch: 1000, loss is 5.4289172172546385 and perplexity is 227.90234305262814
At time: 323.77126264572144 and batch: 1050, loss is 5.430288305282593 and perplexity is 228.21503153946597
At time: 324.58342719078064 and batch: 1100, loss is 5.412168073654175 and perplexity is 224.11696340907469
At time: 325.4149901866913 and batch: 1150, loss is 5.444784116744995 and perplexity is 231.5472871340044
At time: 326.2575225830078 and batch: 1200, loss is 5.445261163711548 and perplexity is 231.6577724161842
At time: 327.06993436813354 and batch: 1250, loss is 5.43174256324768 and perplexity is 228.54715650599056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.258342213874315 and perplexity of 192.16266249951923
Finished 15 epochs...
Completing Train Step...
At time: 329.2988648414612 and batch: 50, loss is 5.4081486797332765 and perplexity is 223.2179569895734
At time: 330.11111330986023 and batch: 100, loss is 5.428701677322388 and perplexity is 227.85322629054872
At time: 330.9293293952942 and batch: 150, loss is 5.342240362167359 and perplexity is 208.98037795350186
At time: 331.73895239830017 and batch: 200, loss is 5.384918985366821 and perplexity is 218.09243471314218
At time: 332.5764989852905 and batch: 250, loss is 5.408781433105469 and perplexity is 223.359243599668
At time: 333.39009857177734 and batch: 300, loss is 5.400209789276123 and perplexity is 221.45286976853097
At time: 334.2022776603699 and batch: 350, loss is 5.427669076919556 and perplexity is 227.61806639129313
At time: 335.01429176330566 and batch: 400, loss is 5.400398263931274 and perplexity is 221.49461195534144
At time: 335.8274097442627 and batch: 450, loss is 5.357316989898681 and perplexity is 212.1549682399847
At time: 336.6459250450134 and batch: 500, loss is 5.364556446075439 and perplexity is 213.6964277673043
At time: 337.45803141593933 and batch: 550, loss is 5.361533813476562 and perplexity is 213.05147719382418
At time: 338.2731909751892 and batch: 600, loss is 5.382728433609008 and perplexity is 217.6152148251769
At time: 339.0849220752716 and batch: 650, loss is 5.369551639556885 and perplexity is 214.76655328716345
At time: 339.8983426094055 and batch: 700, loss is 5.3978244972229 and perplexity is 220.92526948863983
At time: 340.7449948787689 and batch: 750, loss is 5.378170900344848 and perplexity is 216.62568287003037
At time: 341.5689058303833 and batch: 800, loss is 5.405609769821167 and perplexity is 222.65194553610195
At time: 342.4079909324646 and batch: 850, loss is 5.430021677017212 and perplexity is 228.15419107272723
At time: 343.2184364795685 and batch: 900, loss is 5.4014365673065186 and perplexity is 221.72470999364612
At time: 344.0368142127991 and batch: 950, loss is 5.38245397567749 and perplexity is 217.55549679886687
At time: 344.85003328323364 and batch: 1000, loss is 5.381085472106934 and perplexity is 217.25797495099235
At time: 345.66614723205566 and batch: 1050, loss is 5.377353162765503 and perplexity is 216.44861231698602
At time: 346.51372361183167 and batch: 1100, loss is 5.3584233665466305 and perplexity is 212.38982143668173
At time: 347.34307312965393 and batch: 1150, loss is 5.390934171676636 and perplexity is 219.40825482479818
At time: 348.1561133861542 and batch: 1200, loss is 5.399677572250366 and perplexity is 221.33504013908617
At time: 348.9686243534088 and batch: 1250, loss is 5.3922679996490475 and perplexity is 219.70110295355727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.225609104128649 and perplexity of 185.97441381725395
Finished 16 epochs...
Completing Train Step...
At time: 351.2101192474365 and batch: 50, loss is 5.37193006515503 and perplexity is 215.27796749436246
At time: 352.0444893836975 and batch: 100, loss is 5.3797166061401365 and perplexity is 216.96078135857042
At time: 352.8549416065216 and batch: 150, loss is 5.290884218215942 and perplexity is 198.51888183859435
At time: 353.71314120292664 and batch: 200, loss is 5.342213220596314 and perplexity is 208.9747059747002
At time: 354.52410984039307 and batch: 250, loss is 5.36467999458313 and perplexity is 213.72283127307716
At time: 355.33510398864746 and batch: 300, loss is 5.361658430099487 and perplexity is 213.07802860376037
At time: 356.14356112480164 and batch: 350, loss is 5.394083986282348 and perplexity is 220.10043970519934
At time: 356.9520390033722 and batch: 400, loss is 5.3672480869293215 and perplexity is 214.27239660550143
At time: 357.76129126548767 and batch: 450, loss is 5.310411128997803 and perplexity is 202.4334375384252
At time: 358.572137594223 and batch: 500, loss is 5.323079710006714 and perplexity is 205.0142953264096
At time: 359.3825032711029 and batch: 550, loss is 5.316851081848145 and perplexity is 203.74130611739795
At time: 360.19281554222107 and batch: 600, loss is 5.356800317764282 and perplexity is 212.0453819922389
At time: 361.00413513183594 and batch: 650, loss is 5.340968780517578 and perplexity is 208.71481122040757
At time: 361.8145155906677 and batch: 700, loss is 5.361072931289673 and perplexity is 212.9533081870061
At time: 362.6262557506561 and batch: 750, loss is 5.3456042385101314 and perplexity is 209.68454580578273
At time: 363.43933939933777 and batch: 800, loss is 5.375861349105835 and perplexity is 216.125952054849
At time: 364.25198197364807 and batch: 850, loss is 5.393451061248779 and perplexity is 219.96117670317943
At time: 365.06313610076904 and batch: 900, loss is 5.365340776443482 and perplexity is 213.86410211258917
At time: 365.87523007392883 and batch: 950, loss is 5.346400108337402 and perplexity is 209.8514938346718
At time: 366.6877329349518 and batch: 1000, loss is 5.343808164596558 and perplexity is 209.30827486933856
At time: 367.51217913627625 and batch: 1050, loss is 5.343642482757568 and perplexity is 209.27359916208925
At time: 368.3331460952759 and batch: 1100, loss is 5.324627227783203 and perplexity is 205.331804204814
At time: 369.14677691459656 and batch: 1150, loss is 5.359080410003662 and perplexity is 212.52941663423033
At time: 369.9585430622101 and batch: 1200, loss is 5.373933792114258 and perplexity is 215.7097582124429
At time: 370.77263736724854 and batch: 1250, loss is 5.367099065780639 and perplexity is 214.24046786591617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203150394189096 and perplexity of 181.84422140001098
Finished 17 epochs...
Completing Train Step...
At time: 373.0240309238434 and batch: 50, loss is 5.342688875198364 and perplexity is 209.07412939903938
At time: 373.83536291122437 and batch: 100, loss is 5.346406717300415 and perplexity is 209.85288074001565
At time: 374.67506885528564 and batch: 150, loss is 5.257143907546997 and perplexity is 191.93253067691916
At time: 375.48630237579346 and batch: 200, loss is 5.3108514881134035 and perplexity is 202.52260057838546
At time: 376.29886174201965 and batch: 250, loss is 5.339839773178101 and perplexity is 208.47930363659341
At time: 377.1114094257355 and batch: 300, loss is 5.328422355651855 and perplexity is 206.1125452261926
At time: 377.9241886138916 and batch: 350, loss is 5.356352119445801 and perplexity is 211.9503649034286
At time: 378.73690724372864 and batch: 400, loss is 5.334984912872314 and perplexity is 207.46961866369912
At time: 379.5527968406677 and batch: 450, loss is 5.287832155227661 and perplexity is 197.91391337739566
At time: 380.36245250701904 and batch: 500, loss is 5.300936813354492 and perplexity is 200.52457611095213
At time: 381.1744818687439 and batch: 550, loss is 5.29416639328003 and perplexity is 199.17152602265537
At time: 381.9869978427887 and batch: 600, loss is 5.326161832809448 and perplexity is 205.64714932681238
At time: 382.7992424964905 and batch: 650, loss is 5.320073070526123 and perplexity is 204.3988169764033
At time: 383.6115520000458 and batch: 700, loss is 5.343943758010864 and perplexity is 209.33665761718404
At time: 384.4287602901459 and batch: 750, loss is 5.330323104858398 and perplexity is 206.50468604560345
At time: 385.2419307231903 and batch: 800, loss is 5.351830615997314 and perplexity is 210.99419389186835
At time: 386.0551347732544 and batch: 850, loss is 5.38175443649292 and perplexity is 217.40336142257004
At time: 386.8677031993866 and batch: 900, loss is 5.35494550704956 and perplexity is 211.6524424725813
At time: 387.6797938346863 and batch: 950, loss is 5.337432222366333 and perplexity is 207.97798283968265
At time: 388.4900336265564 and batch: 1000, loss is 5.330284852981567 and perplexity is 206.49678700486524
At time: 389.30418276786804 and batch: 1050, loss is 5.326833972930908 and perplexity is 205.7854194899954
At time: 390.1198890209198 and batch: 1100, loss is 5.306755743026733 and perplexity is 201.6948159866465
At time: 390.9326522350311 and batch: 1150, loss is 5.339877386093139 and perplexity is 208.48714529840134
At time: 391.7452552318573 and batch: 1200, loss is 5.3535872650146485 and perplexity is 211.3651623705435
At time: 392.5552957057953 and batch: 1250, loss is 5.347679738998413 and perplexity is 210.12019812489987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203147275604471 and perplexity of 181.8436543043022
Finished 18 epochs...
Completing Train Step...
At time: 394.7861952781677 and batch: 50, loss is 5.324950428009033 and perplexity is 205.39817821577287
At time: 395.65050292015076 and batch: 100, loss is 5.325910110473632 and perplexity is 205.59538986081017
At time: 396.48514342308044 and batch: 150, loss is 5.239813766479492 and perplexity is 188.6349689798927
At time: 397.3167746067047 and batch: 200, loss is 5.296507434844971 and perplexity is 199.63834104711754
At time: 398.1441731452942 and batch: 250, loss is 5.323679466247558 and perplexity is 205.13729080946135
At time: 398.95652413368225 and batch: 300, loss is 5.314323692321778 and perplexity is 203.22702264531617
At time: 399.76850605010986 and batch: 350, loss is 5.342625226974487 and perplexity is 209.06082262552522
At time: 400.58519077301025 and batch: 400, loss is 5.325227584838867 and perplexity is 205.45511361335355
At time: 401.39755606651306 and batch: 450, loss is 5.280928239822388 and perplexity is 196.55223831520553
At time: 402.21048617362976 and batch: 500, loss is 5.290552816390991 and perplexity is 198.4531032190451
At time: 403.02250599861145 and batch: 550, loss is 5.283681840896606 and perplexity is 197.0942106150733
At time: 403.8341224193573 and batch: 600, loss is 5.312096090316772 and perplexity is 202.77481757565482
At time: 404.6456608772278 and batch: 650, loss is 5.301517181396484 and perplexity is 200.64098794414716
At time: 405.45982336997986 and batch: 700, loss is 5.3270952510833744 and perplexity is 205.83919374891786
At time: 406.2792911529541 and batch: 750, loss is 5.3177415466308595 and perplexity is 203.92281137530642
At time: 407.09182596206665 and batch: 800, loss is 5.341604022979737 and perplexity is 208.84743785155013
At time: 407.9037685394287 and batch: 850, loss is 5.372030992507934 and perplexity is 215.29969602624348
At time: 408.7201683521271 and batch: 900, loss is 5.337656135559082 and perplexity is 208.0245570679393
At time: 409.5521638393402 and batch: 950, loss is 5.324595727920532 and perplexity is 205.3253363830478
At time: 410.37934160232544 and batch: 1000, loss is 5.318230152130127 and perplexity is 204.0224735281257
At time: 411.22727847099304 and batch: 1050, loss is 5.308159475326538 and perplexity is 201.9781403237884
At time: 412.044650554657 and batch: 1100, loss is 5.284179315567017 and perplexity is 197.1922843851245
At time: 412.8569178581238 and batch: 1150, loss is 5.32374984741211 and perplexity is 205.15172911896803
At time: 413.66842222213745 and batch: 1200, loss is 5.337240247726441 and perplexity is 207.9380601735138
At time: 414.4810039997101 and batch: 1250, loss is 5.327055549621582 and perplexity is 205.83102179425214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.19595359189667 and perplexity of 180.5402224302304
Finished 19 epochs...
Completing Train Step...
At time: 416.7333827018738 and batch: 50, loss is 5.306872844696045 and perplexity is 201.71843616924443
At time: 417.5737729072571 and batch: 100, loss is 5.310666666030884 and perplexity is 202.48517338838147
At time: 418.39208149909973 and batch: 150, loss is 5.225563402175903 and perplexity is 185.96591461759823
At time: 419.2073986530304 and batch: 200, loss is 5.2843379878997805 and perplexity is 197.22357582736828
At time: 420.019464969635 and batch: 250, loss is 5.297992210388184 and perplexity is 199.93497933953648
At time: 420.8386883735657 and batch: 300, loss is 5.300731515884399 and perplexity is 200.48341314825566
At time: 421.6730659008026 and batch: 350, loss is 5.323851842880249 and perplexity is 205.17265473275978
At time: 422.4867115020752 and batch: 400, loss is 5.301470212936401 and perplexity is 200.63156436722107
At time: 423.3001663684845 and batch: 450, loss is 5.2730349349975585 and perplexity is 195.0068985266152
At time: 424.1135346889496 and batch: 500, loss is 5.273138265609742 and perplexity is 195.02704974992108
At time: 424.93200612068176 and batch: 550, loss is 5.260011940002442 and perplexity is 192.4837895404422
At time: 425.745304107666 and batch: 600, loss is 5.29716124534607 and perplexity is 199.76890936973518
At time: 426.5590841770172 and batch: 650, loss is 5.286206159591675 and perplexity is 197.59236770471693
At time: 427.3705520629883 and batch: 700, loss is 5.314645862579345 and perplexity is 203.292506895519
At time: 428.1866750717163 and batch: 750, loss is 5.297826709747315 and perplexity is 199.9018927103283
At time: 429.0051965713501 and batch: 800, loss is 5.325111684799194 and perplexity is 205.43130273740212
At time: 429.8210115432739 and batch: 850, loss is 5.356839122772217 and perplexity is 212.05361057462375
At time: 430.6380591392517 and batch: 900, loss is 5.329790668487549 and perplexity is 206.39476470565836
At time: 431.45594930648804 and batch: 950, loss is 5.302675123214722 and perplexity is 200.87345309912766
At time: 432.27307081222534 and batch: 1000, loss is 5.300787506103515 and perplexity is 200.49463857274105
At time: 433.0898804664612 and batch: 1050, loss is 5.306658754348755 and perplexity is 201.67525482170987
At time: 433.9076478481293 and batch: 1100, loss is 5.277835102081299 and perplexity is 195.94521445695506
At time: 434.7206745147705 and batch: 1150, loss is 5.311342458724976 and perplexity is 202.6220576366972
At time: 435.5375077724457 and batch: 1200, loss is 5.329525136947632 and perplexity is 206.33996766144907
At time: 436.3993787765503 and batch: 1250, loss is 5.318307991027832 and perplexity is 204.03835503066352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.198096059534672 and perplexity of 180.92743866515536
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 438.6307978630066 and batch: 50, loss is 5.3011132335662845 and perplexity is 200.55995581989526
At time: 439.44424533843994 and batch: 100, loss is 5.288907442092896 and perplexity is 198.12684206809334
At time: 440.2583043575287 and batch: 150, loss is 5.211433849334717 and perplexity is 183.35677579096523
At time: 441.0719861984253 and batch: 200, loss is 5.256338357925415 and perplexity is 191.77798175623278
At time: 441.88543820381165 and batch: 250, loss is 5.266485643386841 and perplexity is 193.7339146037213
At time: 442.69880723953247 and batch: 300, loss is 5.258266258239746 and perplexity is 192.14806721685252
At time: 443.5164043903351 and batch: 350, loss is 5.273269863128662 and perplexity is 195.05271651459472
At time: 444.3295941352844 and batch: 400, loss is 5.241839771270752 and perplexity is 189.01753173697585
At time: 445.1440031528473 and batch: 450, loss is 5.19292742729187 and perplexity is 179.99470383021537
At time: 445.9559168815613 and batch: 500, loss is 5.194068851470948 and perplexity is 180.20027143487158
At time: 446.79888367652893 and batch: 550, loss is 5.191409244537353 and perplexity is 179.72164630301734
At time: 447.61337208747864 and batch: 600, loss is 5.22591703414917 and perplexity is 186.03168974035248
At time: 448.4261395931244 and batch: 650, loss is 5.212374887466431 and perplexity is 183.52940272021493
At time: 449.2454180717468 and batch: 700, loss is 5.22477767944336 and perplexity is 185.81985435995162
At time: 450.06119990348816 and batch: 750, loss is 5.201168947219848 and perplexity is 181.48426345520224
At time: 450.87891602516174 and batch: 800, loss is 5.211287117004394 and perplexity is 183.32987339774667
At time: 451.69128155708313 and batch: 850, loss is 5.238280868530273 and perplexity is 188.346032334566
At time: 452.5033948421478 and batch: 900, loss is 5.195632514953613 and perplexity is 180.48226443249996
At time: 453.3191056251526 and batch: 950, loss is 5.174145517349243 and perplexity is 176.6456091616338
At time: 454.1327238082886 and batch: 1000, loss is 5.161402854919434 and perplexity is 174.40895452660646
At time: 454.9488208293915 and batch: 1050, loss is 5.150007801055908 and perplexity is 172.4328354697974
At time: 455.76244592666626 and batch: 1100, loss is 5.112452592849731 and perplexity is 166.07717548023479
At time: 456.57648158073425 and batch: 1150, loss is 5.135735292434692 and perplexity is 169.98926575426162
At time: 457.44324922561646 and batch: 1200, loss is 5.167668685913086 and perplexity is 175.50520242486496
At time: 458.2591555118561 and batch: 1250, loss is 5.1915514087677 and perplexity is 179.74719810877406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.077818487682482 and perplexity of 160.4237076287696
Finished 21 epochs...
Completing Train Step...
At time: 460.4872622489929 and batch: 50, loss is 5.220274887084961 and perplexity is 184.985027077175
At time: 461.3280053138733 and batch: 100, loss is 5.217855968475342 and perplexity is 184.53810410574187
At time: 462.1398627758026 and batch: 150, loss is 5.151230974197388 and perplexity is 172.64387972837469
At time: 462.9524042606354 and batch: 200, loss is 5.201383199691772 and perplexity is 181.52315107299816
At time: 463.76455783843994 and batch: 250, loss is 5.210372591018677 and perplexity is 183.16229010588378
At time: 464.5772776603699 and batch: 300, loss is 5.211522617340088 and perplexity is 183.37305272864845
At time: 465.3910984992981 and batch: 350, loss is 5.225845136642456 and perplexity is 186.0183150065012
At time: 466.24132442474365 and batch: 400, loss is 5.200133562088013 and perplexity is 181.29645459118922
At time: 467.07167768478394 and batch: 450, loss is 5.154908514022827 and perplexity is 173.27995334785723
At time: 467.884113073349 and batch: 500, loss is 5.157931118011475 and perplexity is 173.80450237863306
At time: 468.69727969169617 and batch: 550, loss is 5.156358375549316 and perplexity is 173.53136749933944
At time: 469.5088884830475 and batch: 600, loss is 5.195826625823974 and perplexity is 180.51730140235208
At time: 470.32192873954773 and batch: 650, loss is 5.185356216430664 and perplexity is 178.6370719176721
At time: 471.136919260025 and batch: 700, loss is 5.198049058914185 and perplexity is 180.91893516311146
At time: 471.95336532592773 and batch: 750, loss is 5.176219930648804 and perplexity is 177.01242549538836
At time: 472.7663993835449 and batch: 800, loss is 5.18809344291687 and perplexity is 179.12671186434355
At time: 473.57970690727234 and batch: 850, loss is 5.221707677841186 and perplexity is 185.25026188162448
At time: 474.391921043396 and batch: 900, loss is 5.1819546604156494 and perplexity is 178.0304602079451
At time: 475.2041668891907 and batch: 950, loss is 5.161188077926636 and perplexity is 174.37149951821723
At time: 476.0157186985016 and batch: 1000, loss is 5.1558936595916744 and perplexity is 173.45074343880583
At time: 476.82894492149353 and batch: 1050, loss is 5.147474784851074 and perplexity is 171.99661301581907
At time: 477.64612102508545 and batch: 1100, loss is 5.117861490249634 and perplexity is 166.97790366004085
At time: 478.50417828559875 and batch: 1150, loss is 5.147566776275635 and perplexity is 172.01243595704642
At time: 479.31642150878906 and batch: 1200, loss is 5.181037092208863 and perplexity is 177.8671800396202
At time: 480.1280333995819 and batch: 1250, loss is 5.1982834434509275 and perplexity is 180.9613447337973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.073650722085994 and perplexity of 159.7564905871805
Finished 22 epochs...
Completing Train Step...
At time: 482.3589949607849 and batch: 50, loss is 5.20524320602417 and perplexity is 182.22518564288714
At time: 483.1749758720398 and batch: 100, loss is 5.201320590972901 and perplexity is 181.51178649682845
At time: 483.9897789955139 and batch: 150, loss is 5.13656436920166 and perplexity is 170.13025834389651
At time: 484.8039491176605 and batch: 200, loss is 5.182138910293579 and perplexity is 178.06326532058242
At time: 485.6169216632843 and batch: 250, loss is 5.19480936050415 and perplexity is 180.33376078258866
At time: 486.44920921325684 and batch: 300, loss is 5.1949072265625 and perplexity is 180.35141020056923
At time: 487.272705078125 and batch: 350, loss is 5.210416717529297 and perplexity is 183.170372596948
At time: 488.0981276035309 and batch: 400, loss is 5.185698156356811 and perplexity is 178.69816550942465
At time: 488.92214465141296 and batch: 450, loss is 5.140535469055176 and perplexity is 170.8072058112779
At time: 489.76107931137085 and batch: 500, loss is 5.145198392868042 and perplexity is 171.60552660670723
At time: 490.59318590164185 and batch: 550, loss is 5.145063571929931 and perplexity is 171.58239214816493
At time: 491.40860199928284 and batch: 600, loss is 5.185285558700562 and perplexity is 178.62445027357188
At time: 492.2542083263397 and batch: 650, loss is 5.176973695755005 and perplexity is 177.14590158358004
At time: 493.0732009410858 and batch: 700, loss is 5.190527105331421 and perplexity is 179.56317669906048
At time: 493.8865885734558 and batch: 750, loss is 5.168648700714112 and perplexity is 175.67728442858092
At time: 494.70040559768677 and batch: 800, loss is 5.1815657138824465 and perplexity is 177.9612293420681
At time: 495.513236284256 and batch: 850, loss is 5.2172558307647705 and perplexity is 184.42738905589192
At time: 496.33101177215576 and batch: 900, loss is 5.1769191265106205 and perplexity is 177.13623512933276
At time: 497.1446487903595 and batch: 950, loss is 5.160199184417724 and perplexity is 174.1991499059561
At time: 497.9574177265167 and batch: 1000, loss is 5.153293352127076 and perplexity is 173.00030407013554
At time: 498.77287006378174 and batch: 1050, loss is 5.149192686080933 and perplexity is 172.29234015112644
At time: 499.63218879699707 and batch: 1100, loss is 5.122705535888672 and perplexity is 167.7887144628904
At time: 500.4435112476349 and batch: 1150, loss is 5.1525226402282716 and perplexity is 172.86702204491075
At time: 501.25667929649353 and batch: 1200, loss is 5.1858281230926515 and perplexity is 178.72139183598875
At time: 502.06739258766174 and batch: 1250, loss is 5.200063915252685 and perplexity is 181.28382830656636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.072130634836907 and perplexity of 159.51383126127897
Finished 23 epochs...
Completing Train Step...
At time: 504.2577648162842 and batch: 50, loss is 5.196392822265625 and perplexity is 180.61953859649896
At time: 505.0968725681305 and batch: 100, loss is 5.192163410186768 and perplexity is 179.85723731773155
At time: 505.90783643722534 and batch: 150, loss is 5.127999019622803 and perplexity is 168.67925624841249
At time: 506.7199845314026 and batch: 200, loss is 5.1736363697052 and perplexity is 176.55569335805083
At time: 507.5320484638214 and batch: 250, loss is 5.186420249938965 and perplexity is 178.82724890741932
At time: 508.3442351818085 and batch: 300, loss is 5.187282342910766 and perplexity is 178.98148109355867
At time: 509.16122603416443 and batch: 350, loss is 5.206499824523926 and perplexity is 182.45431711756083
At time: 509.9729323387146 and batch: 400, loss is 5.177458810806274 and perplexity is 177.2318585745382
At time: 510.7868137359619 and batch: 450, loss is 5.133431091308593 and perplexity is 169.59802721592789
At time: 511.6001310348511 and batch: 500, loss is 5.140027074813843 and perplexity is 170.720390481542
At time: 512.4115822315216 and batch: 550, loss is 5.139369478225708 and perplexity is 170.60816223973504
At time: 513.2247354984283 and batch: 600, loss is 5.178371114730835 and perplexity is 177.39362167200764
At time: 514.0367019176483 and batch: 650, loss is 5.171160507202148 and perplexity is 176.11910642479336
At time: 514.8652596473694 and batch: 700, loss is 5.185055847167969 and perplexity is 178.583422889753
At time: 515.70516705513 and batch: 750, loss is 5.162636919021606 and perplexity is 174.62431921606944
At time: 516.5487127304077 and batch: 800, loss is 5.177770214080811 and perplexity is 177.28705774980637
At time: 517.3895292282104 and batch: 850, loss is 5.216595096588135 and perplexity is 184.30557182567046
At time: 518.2252566814423 and batch: 900, loss is 5.175405864715576 and perplexity is 176.86838434748498
At time: 519.0418145656586 and batch: 950, loss is 5.158117294311523 and perplexity is 173.83686367017697
At time: 519.899665594101 and batch: 1000, loss is 5.152658843994141 and perplexity is 172.89056878784905
At time: 520.7152633666992 and batch: 1050, loss is 5.149623012542724 and perplexity is 172.36649805916818
At time: 521.5292046070099 and batch: 1100, loss is 5.124269933700561 and perplexity is 168.05140818582345
At time: 522.3423895835876 and batch: 1150, loss is 5.15440936088562 and perplexity is 173.19348169862155
At time: 523.1559660434723 and batch: 1200, loss is 5.187269687652588 and perplexity is 178.9792160510387
At time: 523.9654729366302 and batch: 1250, loss is 5.199263610839844 and perplexity is 181.13880409829326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.071614286325274 and perplexity of 159.43148779271556
Finished 24 epochs...
Completing Train Step...
At time: 526.2190179824829 and batch: 50, loss is 5.188973455429077 and perplexity is 179.28441499224027
At time: 527.0355861186981 and batch: 100, loss is 5.18543478012085 and perplexity is 178.65110685655696
At time: 527.849009513855 and batch: 150, loss is 5.122280025482178 and perplexity is 167.71733380647728
At time: 528.6663744449615 and batch: 200, loss is 5.165852069854736 and perplexity is 175.18666627236084
At time: 529.5170674324036 and batch: 250, loss is 5.1794900226593015 and perplexity is 177.5922198875952
At time: 530.3299949169159 and batch: 300, loss is 5.180887384414673 and perplexity is 177.8405539295555
At time: 531.1446306705475 and batch: 350, loss is 5.200881242752075 and perplexity is 181.43205713213197
At time: 531.965206861496 and batch: 400, loss is 5.170483026504517 and perplexity is 175.99982953816277
At time: 532.7773652076721 and batch: 450, loss is 5.127021818161011 and perplexity is 168.51450314423022
At time: 533.6068632602692 and batch: 500, loss is 5.133840341567993 and perplexity is 169.6674494571258
At time: 534.4356529712677 and batch: 550, loss is 5.134816541671753 and perplexity is 169.8331597089133
At time: 535.2640345096588 and batch: 600, loss is 5.176129302978516 and perplexity is 176.99638399856636
At time: 536.1058576107025 and batch: 650, loss is 5.1674377059936525 and perplexity is 175.4646689287407
At time: 536.935061454773 and batch: 700, loss is 5.18167971611023 and perplexity is 177.98151847515334
At time: 537.7913990020752 and batch: 750, loss is 5.158907661437988 and perplexity is 173.97431292311998
At time: 538.6055152416229 and batch: 800, loss is 5.174159708023072 and perplexity is 176.64811589964285
At time: 539.4181342124939 and batch: 850, loss is 5.211864051818847 and perplexity is 183.43567330112828
At time: 540.2308232784271 and batch: 900, loss is 5.172357358932495 and perplexity is 176.33002107366914
At time: 541.0903873443604 and batch: 950, loss is 5.15710298538208 and perplexity is 173.66062878051054
At time: 541.9037029743195 and batch: 1000, loss is 5.150898466110229 and perplexity is 172.5864837850031
At time: 542.7171006202698 and batch: 1050, loss is 5.148892278671265 and perplexity is 172.24059002896527
At time: 543.5307464599609 and batch: 1100, loss is 5.1236756324768065 and perplexity is 167.95156469977655
At time: 544.3444745540619 and batch: 1150, loss is 5.155468444824219 and perplexity is 173.37700529965343
At time: 545.1612956523895 and batch: 1200, loss is 5.187440824508667 and perplexity is 179.00984861248256
At time: 545.9727997779846 and batch: 1250, loss is 5.197849321365356 and perplexity is 180.88280246711352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0699097570711675 and perplexity of 159.15996363392404
Finished 25 epochs...
Completing Train Step...
At time: 548.2432506084442 and batch: 50, loss is 5.183754358291626 and perplexity is 178.3511497346134
At time: 549.0524406433105 and batch: 100, loss is 5.18014349937439 and perplexity is 177.70831019509782
At time: 549.8678243160248 and batch: 150, loss is 5.117203855514527 and perplexity is 166.86812929027417
At time: 550.6826796531677 and batch: 200, loss is 5.160121841430664 and perplexity is 174.1856773443698
At time: 551.501825094223 and batch: 250, loss is 5.174379930496216 and perplexity is 176.68702206845168
At time: 552.3161933422089 and batch: 300, loss is 5.174856662750244 and perplexity is 176.7712745520824
At time: 553.1286227703094 and batch: 350, loss is 5.195327291488647 and perplexity is 180.42718541651666
At time: 553.9408349990845 and batch: 400, loss is 5.1652277565002445 and perplexity is 175.0773290309794
At time: 554.753669500351 and batch: 450, loss is 5.122144680023194 and perplexity is 167.69463556304035
At time: 555.566065788269 and batch: 500, loss is 5.129182786941528 and perplexity is 168.8790514715028
At time: 556.3804347515106 and batch: 550, loss is 5.130655841827393 and perplexity is 169.12800289793415
At time: 557.1985263824463 and batch: 600, loss is 5.171380586624146 and perplexity is 176.15787088141118
At time: 558.0104310512543 and batch: 650, loss is 5.164657278060913 and perplexity is 174.97747967321234
At time: 558.8231732845306 and batch: 700, loss is 5.178988275527954 and perplexity is 177.50313585141564
At time: 559.6352853775024 and batch: 750, loss is 5.155690774917603 and perplexity is 173.4155565108207
At time: 560.4662349224091 and batch: 800, loss is 5.171424627304077 and perplexity is 176.16562916465887
At time: 561.2937486171722 and batch: 850, loss is 5.209102888107299 and perplexity is 182.92987599254369
At time: 562.1532380580902 and batch: 900, loss is 5.17052921295166 and perplexity is 176.0079585327102
At time: 562.9678463935852 and batch: 950, loss is 5.155204467773437 and perplexity is 173.33124378938103
At time: 563.783239364624 and batch: 1000, loss is 5.149703207015992 and perplexity is 172.3803214539615
At time: 564.5994141101837 and batch: 1050, loss is 5.148080806732178 and perplexity is 172.10087831711843
At time: 565.4110724925995 and batch: 1100, loss is 5.123654460906982 and perplexity is 167.94800893913805
At time: 566.2230024337769 and batch: 1150, loss is 5.15666618347168 and perplexity is 173.58479005055483
At time: 567.0323808193207 and batch: 1200, loss is 5.187047033309937 and perplexity is 178.9393699874548
At time: 567.8432655334473 and batch: 1250, loss is 5.196179628372192 and perplexity is 180.5810357182707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.069927577554744 and perplexity of 159.1627999667144
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 570.0686185359955 and batch: 50, loss is 5.182689781188965 and perplexity is 178.16138221337394
At time: 570.9106845855713 and batch: 100, loss is 5.187967309951782 and perplexity is 179.10411950590023
At time: 571.7227399349213 and batch: 150, loss is 5.121099433898926 and perplexity is 167.5194449697521
At time: 572.5356667041779 and batch: 200, loss is 5.163343305587769 and perplexity is 174.7477150667435
At time: 573.3496332168579 and batch: 250, loss is 5.175957498550415 and perplexity is 176.96597784806798
At time: 574.1685345172882 and batch: 300, loss is 5.175266208648682 and perplexity is 176.84368532929312
At time: 574.9814851284027 and batch: 350, loss is 5.189725065231324 and perplexity is 179.41921756906146
At time: 575.7954137325287 and batch: 400, loss is 5.158885145187378 and perplexity is 173.9703957179911
At time: 576.6087808609009 and batch: 450, loss is 5.118642950057984 and perplexity is 167.1084411789028
At time: 577.4222676753998 and batch: 500, loss is 5.121180534362793 and perplexity is 167.5330314253724
At time: 578.2551648616791 and batch: 550, loss is 5.1212602233886715 and perplexity is 167.54638250140923
At time: 579.084775686264 and batch: 600, loss is 5.160820550918579 and perplexity is 174.30742505797755
At time: 579.8997764587402 and batch: 650, loss is 5.1542935562133785 and perplexity is 173.173426245519
At time: 580.7162308692932 and batch: 700, loss is 5.168655729293823 and perplexity is 175.67851919471727
At time: 581.5295538902283 and batch: 750, loss is 5.137185268402099 and perplexity is 170.2359248860138
At time: 582.3420441150665 and batch: 800, loss is 5.1505790710449215 and perplexity is 172.53136931585664
At time: 583.2030735015869 and batch: 850, loss is 5.179221935272217 and perplexity is 177.5446160346804
At time: 584.0148451328278 and batch: 900, loss is 5.139926719665527 and perplexity is 170.70325867108178
At time: 584.8256034851074 and batch: 950, loss is 5.123207025527954 and perplexity is 167.87287986705715
At time: 585.6364996433258 and batch: 1000, loss is 5.116175384521484 and perplexity is 166.69659848198597
At time: 586.477658033371 and batch: 1050, loss is 5.106905498504639 and perplexity is 165.15848011990488
At time: 587.3068280220032 and batch: 1100, loss is 5.075962009429932 and perplexity is 160.1261607844444
At time: 588.1576614379883 and batch: 1150, loss is 5.107872505187988 and perplexity is 165.31826671889579
At time: 588.9816484451294 and batch: 1200, loss is 5.1483969974517825 and perplexity is 172.15530362161272
At time: 589.7982246875763 and batch: 1250, loss is 5.169409370422363 and perplexity is 175.8109676552181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.056175064866561 and perplexity of 156.98889413718734
Finished 27 epochs...
Completing Train Step...
At time: 592.0728762149811 and batch: 50, loss is 5.1694859790802 and perplexity is 175.82443681340368
At time: 592.8873910903931 and batch: 100, loss is 5.173538417816162 and perplexity is 176.53840024132654
At time: 593.7046971321106 and batch: 150, loss is 5.1096243476867675 and perplexity is 165.6081321094817
At time: 594.5181245803833 and batch: 200, loss is 5.150107021331787 and perplexity is 172.44994515210297
At time: 595.3310148715973 and batch: 250, loss is 5.164131832122803 and perplexity is 174.88556261809376
At time: 596.1420557498932 and batch: 300, loss is 5.166052436828613 and perplexity is 175.22177141138314
At time: 596.954906463623 and batch: 350, loss is 5.181672744750976 and perplexity is 177.9802777063725
At time: 597.7730503082275 and batch: 400, loss is 5.150288286209107 and perplexity is 172.48120710351603
At time: 598.5872149467468 and batch: 450, loss is 5.110279741287232 and perplexity is 165.71670619494506
At time: 599.4024922847748 and batch: 500, loss is 5.114232778549194 and perplexity is 166.3730870036162
At time: 600.2190413475037 and batch: 550, loss is 5.1140839385986325 and perplexity is 166.34832588433937
At time: 601.0342199802399 and batch: 600, loss is 5.1546734428405765 and perplexity is 173.23922501158012
At time: 601.8465647697449 and batch: 650, loss is 5.149474945068359 and perplexity is 172.34097807652174
At time: 602.6606712341309 and batch: 700, loss is 5.16386155128479 and perplexity is 174.83830078894408
At time: 603.4788119792938 and batch: 750, loss is 5.134269666671753 and perplexity is 169.74030759126262
At time: 604.3390698432922 and batch: 800, loss is 5.14835641860962 and perplexity is 172.14831790045676
At time: 605.1534538269043 and batch: 850, loss is 5.1787081146240235 and perplexity is 177.45341337789682
At time: 605.9668033123016 and batch: 900, loss is 5.139215087890625 and perplexity is 170.58182402163433
At time: 606.7800171375275 and batch: 950, loss is 5.1240364360809325 and perplexity is 168.0121731628573
At time: 607.5942227840424 and batch: 1000, loss is 5.117416667938232 and perplexity is 166.9036446802317
At time: 608.432112455368 and batch: 1050, loss is 5.109459733963012 and perplexity is 165.58087298184583
At time: 609.2856721878052 and batch: 1100, loss is 5.080814924240112 and perplexity is 160.90512800361645
At time: 610.1214411258698 and batch: 1150, loss is 5.11324215888977 and perplexity is 166.20835615910914
At time: 610.9439215660095 and batch: 1200, loss is 5.154084138870239 and perplexity is 173.13716452374155
At time: 611.7599680423737 and batch: 1250, loss is 5.17139235496521 and perplexity is 176.15994397951533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.055362005303376 and perplexity of 156.8613046914336
Finished 28 epochs...
Completing Train Step...
At time: 614.0877621173859 and batch: 50, loss is 5.165932464599609 and perplexity is 175.2007509258594
At time: 614.9456539154053 and batch: 100, loss is 5.169464473724365 and perplexity is 175.82065568698292
At time: 615.7572391033173 and batch: 150, loss is 5.105954971313476 and perplexity is 165.0015670805694
At time: 616.585910320282 and batch: 200, loss is 5.1453608131408695 and perplexity is 171.63340108678648
At time: 617.4234764575958 and batch: 250, loss is 5.15976616859436 and perplexity is 174.123735246679
At time: 618.2906315326691 and batch: 300, loss is 5.162226839065552 and perplexity is 174.5527239638115
At time: 619.1215229034424 and batch: 350, loss is 5.177699508666993 and perplexity is 177.2745230381648
At time: 619.9327700138092 and batch: 400, loss is 5.146635875701905 and perplexity is 171.8523839895175
At time: 620.7521772384644 and batch: 450, loss is 5.106336975097657 and perplexity is 165.06461034420585
At time: 621.5987055301666 and batch: 500, loss is 5.111139039993287 and perplexity is 165.85916754581123
At time: 622.4268095493317 and batch: 550, loss is 5.110695705413819 and perplexity is 165.78565273854858
At time: 623.2681760787964 and batch: 600, loss is 5.151992130279541 and perplexity is 172.77533869152433
At time: 624.1148467063904 and batch: 650, loss is 5.146979532241821 and perplexity is 171.9114523342059
At time: 624.938093662262 and batch: 700, loss is 5.161697330474854 and perplexity is 174.46032126311385
At time: 625.8057489395142 and batch: 750, loss is 5.132808933258056 and perplexity is 169.49254325519055
At time: 626.6169610023499 and batch: 800, loss is 5.147461538314819 and perplexity is 171.9943346715392
At time: 627.4485342502594 and batch: 850, loss is 5.178558359146118 and perplexity is 177.42684074691866
At time: 628.2764372825623 and batch: 900, loss is 5.1399688816070555 and perplexity is 170.71045600361825
At time: 629.106457233429 and batch: 950, loss is 5.125126438140869 and perplexity is 168.1954066219815
At time: 629.9296743869781 and batch: 1000, loss is 5.1186299133300786 and perplexity is 167.106262645825
At time: 630.7427296638489 and batch: 1050, loss is 5.111457481384277 and perplexity is 165.9119923801935
At time: 631.575014591217 and batch: 1100, loss is 5.083027811050415 and perplexity is 161.261587095436
At time: 632.4355878829956 and batch: 1150, loss is 5.1163608264923095 and perplexity is 166.72751389415723
At time: 633.2680580615997 and batch: 1200, loss is 5.156989688873291 and perplexity is 173.64095475207606
At time: 634.0991332530975 and batch: 1250, loss is 5.17213191986084 and perplexity is 176.29027387786718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.054970845688868 and perplexity of 156.7999588827429
Finished 29 epochs...
Completing Train Step...
At time: 636.6766586303711 and batch: 50, loss is 5.163287887573242 and perplexity is 174.73803116366526
At time: 637.5161521434784 and batch: 100, loss is 5.166825752258301 and perplexity is 175.35732551711834
At time: 638.3489992618561 and batch: 150, loss is 5.103413286209107 and perplexity is 164.58271757258711
At time: 639.2036998271942 and batch: 200, loss is 5.142399082183838 and perplexity is 171.12582115815553
At time: 640.0277016162872 and batch: 250, loss is 5.1568690776824955 and perplexity is 173.62001297268438
At time: 640.8429608345032 and batch: 300, loss is 5.159736862182617 and perplexity is 174.1186323795734
At time: 641.658727645874 and batch: 350, loss is 5.174868659973145 and perplexity is 176.77339532918728
At time: 642.4724974632263 and batch: 400, loss is 5.144202508926392 and perplexity is 171.43471248809763
At time: 643.2867414951324 and batch: 450, loss is 5.103730411529541 and perplexity is 164.63491919642607
At time: 644.0991241931915 and batch: 500, loss is 5.1090811920166015 and perplexity is 165.51820553777532
At time: 644.9209396839142 and batch: 550, loss is 5.10859335899353 and perplexity is 165.43747998309107
At time: 645.7678737640381 and batch: 600, loss is 5.150288066864014 and perplexity is 172.48116927061372
At time: 646.6632215976715 and batch: 650, loss is 5.145641317367554 and perplexity is 171.68155173414587
At time: 647.47793841362 and batch: 700, loss is 5.160246715545655 and perplexity is 174.2074299848148
At time: 648.2925004959106 and batch: 750, loss is 5.132014780044556 and perplexity is 169.35799364087583
At time: 649.106427192688 and batch: 800, loss is 5.147152118682861 and perplexity is 171.94112448036967
At time: 649.9374096393585 and batch: 850, loss is 5.178543424606323 and perplexity is 177.4241909784914
At time: 650.7682280540466 and batch: 900, loss is 5.14026328086853 and perplexity is 170.76072043433433
At time: 651.5983543395996 and batch: 950, loss is 5.1259463596343995 and perplexity is 168.3333702029075
At time: 652.4255604743958 and batch: 1000, loss is 5.1195403003692626 and perplexity is 167.2584632917399
At time: 653.2517349720001 and batch: 1050, loss is 5.112948837280274 and perplexity is 166.15961080595102
At time: 654.0961670875549 and batch: 1100, loss is 5.084550638198852 and perplexity is 161.50734769622917
At time: 654.9157774448395 and batch: 1150, loss is 5.118458576202393 and perplexity is 167.077633591445
At time: 655.7295989990234 and batch: 1200, loss is 5.15885069847107 and perplexity is 173.96440311233707
At time: 656.5502092838287 and batch: 1250, loss is 5.1724050331115725 and perplexity is 176.33842766305753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.05459617002167 and perplexity of 156.741220758092
Finished 30 epochs...
Completing Train Step...
At time: 658.9471933841705 and batch: 50, loss is 5.161118831634521 and perplexity is 174.35942535647544
At time: 659.8412845134735 and batch: 100, loss is 5.16476071357727 and perplexity is 174.99557949523933
At time: 660.6708025932312 and batch: 150, loss is 5.1012630367279055 and perplexity is 164.2292038770402
At time: 661.5070419311523 and batch: 200, loss is 5.140195865631103 and perplexity is 170.74920894785217
At time: 662.3626313209534 and batch: 250, loss is 5.1546125316619875 and perplexity is 173.22867312757393
At time: 663.1780760288239 and batch: 300, loss is 5.15791410446167 and perplexity is 173.80154537223024
At time: 663.9903376102448 and batch: 350, loss is 5.172720308303833 and perplexity is 176.39403155954594
At time: 664.8193528652191 and batch: 400, loss is 5.142368764877319 and perplexity is 171.12063316282564
At time: 665.6825082302094 and batch: 450, loss is 5.101727972030639 and perplexity is 164.30557758470317
At time: 666.5166466236115 and batch: 500, loss is 5.107489185333252 and perplexity is 165.25490908880647
At time: 667.3368628025055 and batch: 550, loss is 5.107081661224365 and perplexity is 165.1875774498058
At time: 668.2098321914673 and batch: 600, loss is 5.14905632019043 and perplexity is 172.2688469546066
At time: 669.0230467319489 and batch: 650, loss is 5.144551544189453 and perplexity is 171.49455969185348
At time: 669.8365433216095 and batch: 700, loss is 5.159461145401001 and perplexity is 174.07063156825294
At time: 670.6492927074432 and batch: 750, loss is 5.131489362716675 and perplexity is 169.26903338907718
At time: 671.4644751548767 and batch: 800, loss is 5.146964044570923 and perplexity is 171.90878984682638
At time: 672.2789316177368 and batch: 850, loss is 5.1785793876647945 and perplexity is 177.43057180978224
At time: 673.0979228019714 and batch: 900, loss is 5.140645742416382 and perplexity is 170.82604233454518
At time: 673.9107372760773 and batch: 950, loss is 5.12665566444397 and perplexity is 168.45281222739706
At time: 674.7245304584503 and batch: 1000, loss is 5.120330018997192 and perplexity is 167.3906025854437
At time: 675.5368025302887 and batch: 1050, loss is 5.113971071243286 and perplexity is 166.32955164824963
At time: 676.3510136604309 and batch: 1100, loss is 5.085751647949219 and perplexity is 161.70143612328505
At time: 677.1737756729126 and batch: 1150, loss is 5.119963159561157 and perplexity is 167.32920502620723
At time: 678.0017755031586 and batch: 1200, loss is 5.1601438808441165 and perplexity is 174.1895163368347
At time: 678.8314249515533 and batch: 1250, loss is 5.1724374485015865 and perplexity is 176.34414383461015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.054350692860401 and perplexity of 156.70274909031767
Finished 31 epochs...
Completing Train Step...
At time: 681.2703199386597 and batch: 50, loss is 5.159315910339355 and perplexity is 174.0453522451129
At time: 682.0976493358612 and batch: 100, loss is 5.163100023269653 and perplexity is 174.70520720845144
At time: 682.9250457286835 and batch: 150, loss is 5.099484395980835 and perplexity is 163.93735874392755
At time: 683.7550845146179 and batch: 200, loss is 5.138233966827393 and perplexity is 170.41454467514592
At time: 684.5845532417297 and batch: 250, loss is 5.152777900695801 and perplexity is 172.9111537940842
At time: 685.4130687713623 and batch: 300, loss is 5.156315259933471 and perplexity is 173.52388574885262
At time: 686.2405314445496 and batch: 350, loss is 5.170960550308227 and perplexity is 176.08389371594183
At time: 687.069629907608 and batch: 400, loss is 5.140954837799073 and perplexity is 170.87885203667122
At time: 687.8946957588196 and batch: 450, loss is 5.100059299468994 and perplexity is 164.0316340002993
At time: 688.7238993644714 and batch: 500, loss is 5.1061782550811765 and perplexity is 165.03841336557915
At time: 689.7208609580994 and batch: 550, loss is 5.105885028839111 and perplexity is 164.99002686627395
At time: 690.5498628616333 and batch: 600, loss is 5.148079681396484 and perplexity is 172.10068464596608
At time: 691.382374048233 and batch: 650, loss is 5.143747291564941 and perplexity is 171.35669019051736
At time: 692.2116363048553 and batch: 700, loss is 5.15869520187378 and perplexity is 173.93735434265386
At time: 693.0421919822693 and batch: 750, loss is 5.130976295471191 and perplexity is 169.1822092675816
At time: 693.8708174228668 and batch: 800, loss is 5.146777954101562 and perplexity is 171.8768022358237
At time: 694.6988549232483 and batch: 850, loss is 5.178545274734497 and perplexity is 177.42451923628957
At time: 695.5270533561707 and batch: 900, loss is 5.140952911376953 and perplexity is 170.87852285218796
At time: 696.3566875457764 and batch: 950, loss is 5.127081336975098 and perplexity is 168.52453322610015
At time: 697.1834437847137 and batch: 1000, loss is 5.120832948684693 and perplexity is 167.47480946218667
At time: 698.010082244873 and batch: 1050, loss is 5.11473967552185 and perplexity is 166.45744239568995
At time: 698.8382406234741 and batch: 1100, loss is 5.086493902206421 and perplexity is 161.82150425767261
At time: 699.6621544361115 and batch: 1150, loss is 5.121156969070435 and perplexity is 167.5290835070243
At time: 700.4810845851898 and batch: 1200, loss is 5.161056232452393 and perplexity is 174.348510940672
At time: 701.2986528873444 and batch: 1250, loss is 5.1724395656585695 and perplexity is 176.34451718324092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.05413818359375 and perplexity of 156.66945184213841
Finished 32 epochs...
Completing Train Step...
At time: 703.7419090270996 and batch: 50, loss is 5.157699031829834 and perplexity is 173.76416943586437
At time: 704.5716578960419 and batch: 100, loss is 5.161663007736206 and perplexity is 174.45433340986312
At time: 705.3958756923676 and batch: 150, loss is 5.0979024696350095 and perplexity is 163.678226934823
At time: 706.2189857959747 and batch: 200, loss is 5.13667649269104 and perplexity is 170.14933501156247
At time: 707.0462696552277 and batch: 250, loss is 5.151269369125366 and perplexity is 172.65050850495766
At time: 707.872880935669 and batch: 300, loss is 5.15510552406311 and perplexity is 173.31409460142103
At time: 708.689991235733 and batch: 350, loss is 5.169543991088867 and perplexity is 175.83463703802087
At time: 709.5084292888641 and batch: 400, loss is 5.1397502231597905 and perplexity is 170.67313280104204
At time: 710.3213016986847 and batch: 450, loss is 5.098659162521362 and perplexity is 163.80212795639423
At time: 711.177170753479 and batch: 500, loss is 5.105061645507813 and perplexity is 164.85423274130315
At time: 711.9902286529541 and batch: 550, loss is 5.1050498580932615 and perplexity is 164.85228954757395
At time: 712.8039724826813 and batch: 600, loss is 5.14733853340149 and perplexity is 171.9731798244115
At time: 713.6197729110718 and batch: 650, loss is 5.143112506866455 and perplexity is 171.24795010252453
At time: 714.4415965080261 and batch: 700, loss is 5.157992000579834 and perplexity is 173.81508436525627
At time: 715.2537612915039 and batch: 750, loss is 5.130568618774414 and perplexity is 169.1132516805097
At time: 716.069210767746 and batch: 800, loss is 5.146636219024658 and perplexity is 171.85244299036128
At time: 716.8859503269196 and batch: 850, loss is 5.178509788513184 and perplexity is 177.41822322224505
At time: 717.6999480724335 and batch: 900, loss is 5.141298503875732 and perplexity is 170.93758739343147
At time: 718.514235496521 and batch: 950, loss is 5.127449808120727 and perplexity is 168.58664109570083
At time: 719.3282012939453 and batch: 1000, loss is 5.1211954116821286 and perplexity is 167.5355238863207
At time: 720.1470787525177 and batch: 1050, loss is 5.115276708602905 and perplexity is 166.54685955668194
At time: 720.9609808921814 and batch: 1100, loss is 5.086932249069214 and perplexity is 161.89245375550533
At time: 721.7742207050323 and batch: 1150, loss is 5.1219706535339355 and perplexity is 167.66545479353297
At time: 722.5870835781097 and batch: 1200, loss is 5.161721830368042 and perplexity is 174.46459557471022
At time: 723.3998851776123 and batch: 1250, loss is 5.172274169921875 and perplexity is 176.31535296379454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053908299355611 and perplexity of 156.63344014398157
Finished 33 epochs...
Completing Train Step...
At time: 725.6652269363403 and batch: 50, loss is 5.156341896057129 and perplexity is 173.5285078140877
At time: 726.5205018520355 and batch: 100, loss is 5.160403261184692 and perplexity is 174.2347035329887
At time: 727.3452136516571 and batch: 150, loss is 5.09647644996643 and perplexity is 163.4449849072806
At time: 728.1681697368622 and batch: 200, loss is 5.135328035354615 and perplexity is 169.92005051740034
At time: 728.9785313606262 and batch: 250, loss is 5.149986810684204 and perplexity is 172.42921607847325
At time: 729.7925541400909 and batch: 300, loss is 5.154063758850097 and perplexity is 173.1336360207969
At time: 730.6063113212585 and batch: 350, loss is 5.168214864730835 and perplexity is 175.60108583119333
At time: 731.4235453605652 and batch: 400, loss is 5.13867259979248 and perplexity is 170.48931050835162
At time: 732.3444619178772 and batch: 450, loss is 5.097451028823852 and perplexity is 163.6043525794908
At time: 733.1567730903625 and batch: 500, loss is 5.104063158035278 and perplexity is 164.68971000573066
At time: 733.9700255393982 and batch: 550, loss is 5.104365520477295 and perplexity is 164.73951351759928
At time: 734.8007094860077 and batch: 600, loss is 5.146618881225586 and perplexity is 171.84946347306388
At time: 735.6153099536896 and batch: 650, loss is 5.142555379867554 and perplexity is 171.15256981794894
At time: 736.4325110912323 and batch: 700, loss is 5.157370948791504 and perplexity is 173.7071697100359
At time: 737.2451591491699 and batch: 750, loss is 5.130256023406982 and perplexity is 169.0603959231259
At time: 738.0595445632935 and batch: 800, loss is 5.146453685760498 and perplexity is 171.82107706573782
At time: 738.8734848499298 and batch: 850, loss is 5.178466310501099 and perplexity is 177.41050959827945
At time: 739.686233997345 and batch: 900, loss is 5.141484184265137 and perplexity is 170.96933009813023
At time: 740.4974176883698 and batch: 950, loss is 5.12788348197937 and perplexity is 168.65976857045138
At time: 741.310307264328 and batch: 1000, loss is 5.121951551437378 and perplexity is 167.6622520624156
At time: 742.1262085437775 and batch: 1050, loss is 5.117646341323852 and perplexity is 166.94198240778707
At time: 742.9391191005707 and batch: 1100, loss is 5.088988485336304 and perplexity is 162.22568537434398
At time: 743.7656877040863 and batch: 1150, loss is 5.122489519119263 and perplexity is 167.7524732013499
At time: 744.5945491790771 and batch: 1200, loss is 5.1616324615478515 and perplexity is 174.44900457632366
At time: 745.4235875606537 and batch: 1250, loss is 5.171986026763916 and perplexity is 176.26455621991352
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.05377197265625 and perplexity of 156.6120882795248
Finished 34 epochs...
Completing Train Step...
At time: 747.8045170307159 and batch: 50, loss is 5.154857196807861 and perplexity is 173.2710613314008
At time: 748.6191091537476 and batch: 100, loss is 5.159166421890259 and perplexity is 174.01933641991442
At time: 749.4312131404877 and batch: 150, loss is 5.094956388473511 and perplexity is 163.19672721079635
At time: 750.2436645030975 and batch: 200, loss is 5.134114446640015 and perplexity is 169.71396254002383
At time: 751.0565371513367 and batch: 250, loss is 5.148772974014282 and perplexity is 172.22004215020652
At time: 751.8692510128021 and batch: 300, loss is 5.152932672500611 and perplexity is 172.9379176365193
At time: 752.7321243286133 and batch: 350, loss is 5.1667232418060305 and perplexity is 175.33935047970095
At time: 753.5486767292023 and batch: 400, loss is 5.137594575881958 and perplexity is 170.30561798539077
At time: 754.3618094921112 and batch: 450, loss is 5.096125841140747 and perplexity is 163.3876896977484
At time: 755.1742691993713 and batch: 500, loss is 5.103052730560303 and perplexity is 164.52338704069277
At time: 755.984424829483 and batch: 550, loss is 5.103612565994263 and perplexity is 164.61551884939254
At time: 756.8084394931793 and batch: 600, loss is 5.1458424377441405 and perplexity is 171.7160838649248
At time: 757.6400804519653 and batch: 650, loss is 5.141877126693726 and perplexity is 171.0365244027953
At time: 758.4686336517334 and batch: 700, loss is 5.156653957366943 and perplexity is 173.58266779770446
At time: 759.2982306480408 and batch: 750, loss is 5.129849758148193 and perplexity is 168.99172650756864
At time: 760.1279337406158 and batch: 800, loss is 5.146426315307617 and perplexity is 171.81637430940262
At time: 760.9589042663574 and batch: 850, loss is 5.178921175003052 and perplexity is 177.49122569742156
At time: 761.7878186702728 and batch: 900, loss is 5.143149280548096 and perplexity is 171.25424763591434
At time: 762.6132929325104 and batch: 950, loss is 5.128810548782349 and perplexity is 168.81619994285353
At time: 763.4653406143188 and batch: 1000, loss is 5.121795520782471 and perplexity is 167.6360936522332
At time: 764.3171441555023 and batch: 1050, loss is 5.11595027923584 and perplexity is 166.65907841970932
At time: 765.1325879096985 and batch: 1100, loss is 5.087268075942993 and perplexity is 161.94683072225078
At time: 765.9507966041565 and batch: 1150, loss is 5.123078193664551 and perplexity is 167.85125388421676
At time: 766.7871623039246 and batch: 1200, loss is 5.1623099899291995 and perplexity is 174.56723877701245
At time: 767.6005010604858 and batch: 1250, loss is 5.171791105270386 and perplexity is 176.23020181767322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053551889684078 and perplexity of 156.57762441825182
Finished 35 epochs...
Completing Train Step...
At time: 769.8198237419128 and batch: 50, loss is 5.153773040771484 and perplexity is 173.08331025844853
At time: 770.6894526481628 and batch: 100, loss is 5.1581219291687015 and perplexity is 173.83766938107954
At time: 771.5197720527649 and batch: 150, loss is 5.094168710708618 and perplexity is 163.0682313907629
At time: 772.3500325679779 and batch: 200, loss is 5.133076858520508 and perplexity is 169.53796067329094
At time: 773.1979532241821 and batch: 250, loss is 5.147735967636108 and perplexity is 172.0415414372327
At time: 774.0649020671844 and batch: 300, loss is 5.152100276947022 and perplexity is 172.79402477902752
At time: 774.8957686424255 and batch: 350, loss is 5.165702457427979 and perplexity is 175.16045813066495
At time: 775.7548456192017 and batch: 400, loss is 5.136674270629883 and perplexity is 170.14895692975426
At time: 776.5833015441895 and batch: 450, loss is 5.095142593383789 and perplexity is 163.22711807212036
At time: 777.3977770805359 and batch: 500, loss is 5.102251472473145 and perplexity is 164.39161414539882
At time: 778.2108993530273 and batch: 550, loss is 5.1030678462982175 and perplexity is 164.5258739518878
At time: 779.0227906703949 and batch: 600, loss is 5.145277910232544 and perplexity is 171.61917276846316
At time: 779.8359763622284 and batch: 650, loss is 5.141454582214355 and perplexity is 170.9642691302465
At time: 780.6498370170593 and batch: 700, loss is 5.156163730621338 and perplexity is 173.49759378585821
At time: 781.4666187763214 and batch: 750, loss is 5.129518375396729 and perplexity is 168.9357348421123
At time: 782.2796339988708 and batch: 800, loss is 5.14623908996582 and perplexity is 171.78420894117616
At time: 783.0919761657715 and batch: 850, loss is 5.178731784820557 and perplexity is 177.45761378477906
At time: 783.9048516750336 and batch: 900, loss is 5.142920694351196 and perplexity is 171.21510575256187
At time: 784.7173585891724 and batch: 950, loss is 5.128705644607544 and perplexity is 168.7984913475739
At time: 785.5290553569794 and batch: 1000, loss is 5.121772842407227 and perplexity is 167.63229198110488
At time: 786.3435969352722 and batch: 1050, loss is 5.115960607528686 and perplexity is 166.6607997323659
At time: 787.1587781906128 and batch: 1100, loss is 5.087385034561157 and perplexity is 161.96577290749235
At time: 787.9805533885956 and batch: 1150, loss is 5.123657121658325 and perplexity is 167.94845580762285
At time: 788.7909107208252 and batch: 1200, loss is 5.162654056549072 and perplexity is 174.62731187077944
At time: 789.6041879653931 and batch: 1250, loss is 5.171594944000244 and perplexity is 176.19563566782804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053407989279197 and perplexity of 156.55509445577698
Finished 36 epochs...
Completing Train Step...
At time: 791.9065477848053 and batch: 50, loss is 5.152775068283081 and perplexity is 172.9106640390263
At time: 792.7291676998138 and batch: 100, loss is 5.157254886627197 and perplexity is 173.6870100498733
At time: 793.5477027893066 and batch: 150, loss is 5.093339633941651 and perplexity is 162.93309133716662
At time: 794.3602454662323 and batch: 200, loss is 5.132217521667481 and perplexity is 169.39233303625662
At time: 795.1999776363373 and batch: 250, loss is 5.146784029006958 and perplexity is 171.87784637430852
At time: 796.0161652565002 and batch: 300, loss is 5.151315307617187 and perplexity is 172.6584399911094
At time: 796.8276336193085 and batch: 350, loss is 5.16483772277832 and perplexity is 175.0090562839152
At time: 797.640349149704 and batch: 400, loss is 5.13585244178772 and perplexity is 170.00918105327585
At time: 798.4526042938232 and batch: 450, loss is 5.09416802406311 and perplexity is 163.06811942073284
At time: 799.2660040855408 and batch: 500, loss is 5.10148868560791 and perplexity is 164.26626619433696
At time: 800.0945310592651 and batch: 550, loss is 5.102521314620971 and perplexity is 164.43597991725284
At time: 800.9293677806854 and batch: 600, loss is 5.144712228775024 and perplexity is 171.52211843817878
At time: 801.7642509937286 and batch: 650, loss is 5.140997114181519 and perplexity is 170.88607632907932
At time: 802.5991108417511 and batch: 700, loss is 5.155691738128662 and perplexity is 173.41572354668304
At time: 803.4118402004242 and batch: 750, loss is 5.12915904045105 and perplexity is 168.87504123432984
At time: 804.2244727611542 and batch: 800, loss is 5.146019821166992 and perplexity is 171.74654615331204
At time: 805.036947965622 and batch: 850, loss is 5.178535642623902 and perplexity is 177.42281027192843
At time: 805.8606352806091 and batch: 900, loss is 5.142724475860596 and perplexity is 171.1815134787635
At time: 806.6890799999237 and batch: 950, loss is 5.128654174804687 and perplexity is 168.7898035460833
At time: 807.518036365509 and batch: 1000, loss is 5.121743631362915 and perplexity is 167.62739533831416
At time: 808.3476889133453 and batch: 1050, loss is 5.1160326671600345 and perplexity is 166.6728096808658
At time: 809.1830956935883 and batch: 1100, loss is 5.087428455352783 and perplexity is 161.97280574225286
At time: 810.0142283439636 and batch: 1150, loss is 5.124091548919678 and perplexity is 168.02143304583092
At time: 810.8265242576599 and batch: 1200, loss is 5.162957639694214 and perplexity is 174.68033382722373
At time: 811.6386661529541 and batch: 1250, loss is 5.171452503204346 and perplexity is 176.17054000861506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053227556882984 and perplexity of 156.5268493931837
Finished 37 epochs...
Completing Train Step...
At time: 813.852481842041 and batch: 50, loss is 5.15178524017334 and perplexity is 172.73959688079458
At time: 814.7209959030151 and batch: 100, loss is 5.156413536071778 and perplexity is 173.54093984423955
At time: 815.5322291851044 and batch: 150, loss is 5.09245698928833 and perplexity is 162.789342764036
At time: 816.3709006309509 and batch: 200, loss is 5.131341543197632 and perplexity is 169.24401397119743
At time: 817.1825265884399 and batch: 250, loss is 5.1458376693725585 and perplexity is 171.71526506078249
At time: 818.0023124217987 and batch: 300, loss is 5.150643262863159 and perplexity is 172.54244477362914
At time: 818.8138809204102 and batch: 350, loss is 5.163987350463867 and perplexity is 174.86029668715858
At time: 819.6244871616364 and batch: 400, loss is 5.135047216415405 and perplexity is 169.87234044832257
At time: 820.436226606369 and batch: 450, loss is 5.093258981704712 and perplexity is 162.91995094878692
At time: 821.2488124370575 and batch: 500, loss is 5.1006921195983885 and perplexity is 164.13546937124337
At time: 822.0608382225037 and batch: 550, loss is 5.10200611114502 and perplexity is 164.35128374858164
At time: 822.8715124130249 and batch: 600, loss is 5.1441504096984865 and perplexity is 171.42578110460195
At time: 823.6828308105469 and batch: 650, loss is 5.140539236068726 and perplexity is 170.80784924554854
At time: 824.4981682300568 and batch: 700, loss is 5.15520528793335 and perplexity is 173.33138594877704
At time: 825.3098089694977 and batch: 750, loss is 5.1287783432006835 and perplexity is 168.8107632064869
At time: 826.1426274776459 and batch: 800, loss is 5.145811691284179 and perplexity is 171.71080428439217
At time: 826.9669106006622 and batch: 850, loss is 5.178284769058227 and perplexity is 177.3783051616948
At time: 827.780276298523 and batch: 900, loss is 5.142536153793335 and perplexity is 171.14927925757124
At time: 828.5927112102509 and batch: 950, loss is 5.128597860336304 and perplexity is 168.7802985056662
At time: 829.4047472476959 and batch: 1000, loss is 5.121696834564209 and perplexity is 167.61955109638097
At time: 830.2172563076019 and batch: 1050, loss is 5.116009140014649 and perplexity is 166.66888839156917
At time: 831.0294094085693 and batch: 1100, loss is 5.08743971824646 and perplexity is 161.97463003501593
At time: 831.846173286438 and batch: 1150, loss is 5.124438924789429 and perplexity is 168.07980977601872
At time: 832.6589291095734 and batch: 1200, loss is 5.163069543838501 and perplexity is 174.69988237426574
At time: 833.4709219932556 and batch: 1250, loss is 5.17125937461853 and perplexity is 176.13651972661094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.053062717409899 and perplexity of 156.50104971626715
Finished 38 epochs...
Completing Train Step...
At time: 835.7185657024384 and batch: 50, loss is 5.150843105316162 and perplexity is 172.57692952468366
At time: 836.5281281471252 and batch: 100, loss is 5.155658445358276 and perplexity is 173.40995015292435
At time: 837.3730056285858 and batch: 150, loss is 5.091711549758911 and perplexity is 162.66803837113363
At time: 838.1854751110077 and batch: 200, loss is 5.130447568893433 and perplexity is 169.09278178048592
At time: 839.1447896957397 and batch: 250, loss is 5.1449345779418945 and perplexity is 171.56026047858165
At time: 839.9568250179291 and batch: 300, loss is 5.149958200454712 and perplexity is 172.42428290959998
At time: 840.768411397934 and batch: 350, loss is 5.163198585510254 and perplexity is 174.72242739373468
At time: 841.5804808139801 and batch: 400, loss is 5.134288187026978 and perplexity is 169.7434512711661
At time: 842.396032333374 and batch: 450, loss is 5.092446231842041 and perplexity is 162.7875915758441
At time: 843.2088265419006 and batch: 500, loss is 5.099929103851318 and perplexity is 164.01027919057367
At time: 844.0206890106201 and batch: 550, loss is 5.101490068435669 and perplexity is 164.2664933464468
At time: 844.833220243454 and batch: 600, loss is 5.143607587814331 and perplexity is 171.33275269032373
At time: 845.6446576118469 and batch: 650, loss is 5.140104074478149 and perplexity is 170.73353640040847
At time: 846.4558026790619 and batch: 700, loss is 5.154720592498779 and perplexity is 173.24739337439294
At time: 847.26864361763 and batch: 750, loss is 5.128431386947632 and perplexity is 168.75220341604023
At time: 848.1030802726746 and batch: 800, loss is 5.145567140579224 and perplexity is 171.66881742032376
At time: 848.9294617176056 and batch: 850, loss is 5.178021469116211 and perplexity is 177.33160761223158
At time: 849.7418217658997 and batch: 900, loss is 5.142194471359253 and perplexity is 171.090810544678
At time: 850.5557036399841 and batch: 950, loss is 5.12857798576355 and perplexity is 168.7769441026779
At time: 851.3687214851379 and batch: 1000, loss is 5.121700315475464 and perplexity is 167.6201345661785
At time: 852.1806848049164 and batch: 1050, loss is 5.115959310531617 and perplexity is 166.66058357393717
At time: 852.9936418533325 and batch: 1100, loss is 5.0873795509338375 and perplexity is 161.9648847499904
At time: 853.8115684986115 and batch: 1150, loss is 5.124651355743408 and perplexity is 168.11551892308393
At time: 854.624570608139 and batch: 1200, loss is 5.1630430507659915 and perplexity is 174.69525409892356
At time: 855.4368607997894 and batch: 1250, loss is 5.170898141860962 and perplexity is 176.07290493644652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052941092609489 and perplexity of 156.48201646481252
Finished 39 epochs...
Completing Train Step...
At time: 857.6945226192474 and batch: 50, loss is 5.149928941726684 and perplexity is 172.4192380682041
At time: 858.5112402439117 and batch: 100, loss is 5.154928293228149 and perplexity is 173.28338072152803
At time: 859.3586332798004 and batch: 150, loss is 5.0910321235656735 and perplexity is 162.55755498196604
At time: 860.1843421459198 and batch: 200, loss is 5.129743547439575 and perplexity is 168.97377872968562
At time: 861.01926445961 and batch: 250, loss is 5.144109296798706 and perplexity is 171.41873343851958
At time: 861.8602814674377 and batch: 300, loss is 5.1493252086639405 and perplexity is 172.31517429006186
At time: 862.6831555366516 and batch: 350, loss is 5.162539501190185 and perplexity is 174.60730852216392
At time: 863.5053598880768 and batch: 400, loss is 5.133525981903076 and perplexity is 169.614121237127
At time: 864.3220496177673 and batch: 450, loss is 5.091615791320801 and perplexity is 162.65246227963172
At time: 865.133131980896 and batch: 500, loss is 5.099236602783203 and perplexity is 163.89674121417468
At time: 865.948079586029 and batch: 550, loss is 5.100994014739991 and perplexity is 164.18502855247274
At time: 866.7702322006226 and batch: 600, loss is 5.143104810714721 and perplexity is 171.24663215738804
At time: 867.6072454452515 and batch: 650, loss is 5.1396733665466305 and perplexity is 170.66001594616256
At time: 868.4382517337799 and batch: 700, loss is 5.154207096099854 and perplexity is 173.15845429867383
At time: 869.2702209949493 and batch: 750, loss is 5.128122100830078 and perplexity is 168.70001877262584
At time: 870.1121363639832 and batch: 800, loss is 5.145319738388062 and perplexity is 171.62635143204565
At time: 870.9303722381592 and batch: 850, loss is 5.177715435028076 and perplexity is 177.27734639871244
At time: 871.7460670471191 and batch: 900, loss is 5.142046842575073 and perplexity is 171.0655544806399
At time: 872.5605854988098 and batch: 950, loss is 5.128490324020386 and perplexity is 168.7621494700231
At time: 873.3727848529816 and batch: 1000, loss is 5.121624593734741 and perplexity is 167.60744255834544
At time: 874.189227104187 and batch: 1050, loss is 5.115952024459839 and perplexity is 166.65936927738645
At time: 875.0046877861023 and batch: 1100, loss is 5.087282915115356 and perplexity is 161.949233897016
At time: 875.8176879882812 and batch: 1150, loss is 5.12479419708252 and perplexity is 168.13953448409896
At time: 876.6292068958282 and batch: 1200, loss is 5.163004198074341 and perplexity is 174.68846684993554
At time: 877.4425806999207 and batch: 1250, loss is 5.170538139343262 and perplexity is 176.00952965568536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0527869454265515 and perplexity of 156.4578970618109
Finished 40 epochs...
Completing Train Step...
At time: 879.6969981193542 and batch: 50, loss is 5.149027376174927 and perplexity is 172.2638608745887
At time: 880.5478975772858 and batch: 100, loss is 5.154255151748657 and perplexity is 173.16677574048546
At time: 881.3600287437439 and batch: 150, loss is 5.09036768913269 and perplexity is 162.44958201954373
At time: 882.1715633869171 and batch: 200, loss is 5.129035091400146 and perplexity is 168.8541106304385
At time: 882.9938542842865 and batch: 250, loss is 5.1433292007446285 and perplexity is 171.2850625058316
At time: 883.8219559192657 and batch: 300, loss is 5.148710670471192 and perplexity is 172.2093125656385
At time: 884.650475025177 and batch: 350, loss is 5.161587162017822 and perplexity is 174.4411022973884
At time: 885.4870071411133 and batch: 400, loss is 5.132797269821167 and perplexity is 169.49056640113758
At time: 886.3129262924194 and batch: 450, loss is 5.090837106704712 and perplexity is 162.52585660882698
At time: 887.1551439762115 and batch: 500, loss is 5.098625440597534 and perplexity is 163.79660432664656
At time: 887.9685559272766 and batch: 550, loss is 5.100530061721802 and perplexity is 164.10887208081294
At time: 888.7825818061829 and batch: 600, loss is 5.142635488510132 and perplexity is 171.16628116718226
At time: 889.5978088378906 and batch: 650, loss is 5.139237985610962 and perplexity is 170.5857300012543
At time: 890.4151129722595 and batch: 700, loss is 5.153709402084351 and perplexity is 173.0722958142951
At time: 891.227564573288 and batch: 750, loss is 5.127810163497925 and perplexity is 168.6474031456515
At time: 892.0409319400787 and batch: 800, loss is 5.14505069732666 and perplexity is 171.58018310715792
At time: 892.857923746109 and batch: 850, loss is 5.177444925308228 and perplexity is 177.22939763899788
At time: 893.670351266861 and batch: 900, loss is 5.141882905960083 and perplexity is 171.0375128712829
At time: 894.5336127281189 and batch: 950, loss is 5.128397846221924 and perplexity is 168.74654343959324
At time: 895.3461446762085 and batch: 1000, loss is 5.121669034957886 and perplexity is 167.61489140361763
At time: 896.1861000061035 and batch: 1050, loss is 5.115860023498535 and perplexity is 166.64403716049824
At time: 897.0044975280762 and batch: 1100, loss is 5.087145090103149 and perplexity is 161.92691477997892
At time: 897.8223559856415 and batch: 1150, loss is 5.124964513778687 and perplexity is 168.16817389292282
At time: 898.6558134555817 and batch: 1200, loss is 5.162941007614136 and perplexity is 174.67742855408397
At time: 899.4928936958313 and batch: 1250, loss is 5.170160779953003 and perplexity is 175.94312333720663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052633689267792 and perplexity of 156.43392076280435
Finished 41 epochs...
Completing Train Step...
At time: 901.8741221427917 and batch: 50, loss is 5.148157148361206 and perplexity is 172.1140172800449
At time: 902.6897876262665 and batch: 100, loss is 5.153612947463989 and perplexity is 173.0556029967698
At time: 903.5049996376038 and batch: 150, loss is 5.089742946624756 and perplexity is 162.348124555955
At time: 904.3154737949371 and batch: 200, loss is 5.12834620475769 and perplexity is 168.73782934601184
At time: 905.1280074119568 and batch: 250, loss is 5.142586622238159 and perplexity is 171.1579171134959
At time: 905.9409534931183 and batch: 300, loss is 5.1481873989105225 and perplexity is 172.1192239023638
At time: 906.7537505626678 and batch: 350, loss is 5.160931539535523 and perplexity is 174.32677227164805
At time: 907.5651924610138 and batch: 400, loss is 5.132102794647217 and perplexity is 169.3729002733847
At time: 908.380811214447 and batch: 450, loss is 5.090079097747803 and perplexity is 162.40270723384936
At time: 909.1903879642487 and batch: 500, loss is 5.098047065734863 and perplexity is 163.701895879246
At time: 910.0176351070404 and batch: 550, loss is 5.100083560943603 and perplexity is 164.03561369789912
At time: 910.837345123291 and batch: 600, loss is 5.142168130874634 and perplexity is 171.08630398916705
At time: 911.6823263168335 and batch: 650, loss is 5.138818483352662 and perplexity is 170.51418391020752
At time: 912.4961843490601 and batch: 700, loss is 5.153237924575806 and perplexity is 172.99071535265406
At time: 913.3092300891876 and batch: 750, loss is 5.127491941452027 and perplexity is 168.59374436214384
At time: 914.122095823288 and batch: 800, loss is 5.14475193977356 and perplexity is 171.52892988801676
At time: 914.9339823722839 and batch: 850, loss is 5.1771641540527344 and perplexity is 177.17964370357674
At time: 915.7931563854218 and batch: 900, loss is 5.1418075847625735 and perplexity is 171.02463060615432
At time: 916.6069304943085 and batch: 950, loss is 5.128290767669678 and perplexity is 168.72847527139908
At time: 917.4202582836151 and batch: 1000, loss is 5.121638526916504 and perplexity is 167.60977787957663
At time: 918.2319524288177 and batch: 1050, loss is 5.115740985870361 and perplexity is 166.62420143018787
At time: 919.0436134338379 and batch: 1100, loss is 5.0871165752410885 and perplexity is 161.92229752217057
At time: 919.8564252853394 and batch: 1150, loss is 5.125090293884277 and perplexity is 168.18932743391147
At time: 920.6687180995941 and batch: 1200, loss is 5.162847557067871 and perplexity is 174.6611056156711
At time: 921.4801135063171 and batch: 1250, loss is 5.169777040481567 and perplexity is 175.87561996873632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052561516309306 and perplexity of 156.42263087135265
Finished 42 epochs...
Completing Train Step...
At time: 923.6906640529633 and batch: 50, loss is 5.147362298965454 and perplexity is 171.97726691258262
At time: 924.5276067256927 and batch: 100, loss is 5.152993440628052 and perplexity is 172.94842706925368
At time: 925.3419075012207 and batch: 150, loss is 5.089155883789062 and perplexity is 162.25284397616383
At time: 926.1547782421112 and batch: 200, loss is 5.1277062129974365 and perplexity is 168.62987307483408
At time: 926.9668917655945 and batch: 250, loss is 5.141872272491455 and perplexity is 171.0356941589253
At time: 927.7818710803986 and batch: 300, loss is 5.147736434936523 and perplexity is 172.04162183233524
At time: 928.5939660072327 and batch: 350, loss is 5.160352735519409 and perplexity is 174.22590043107053
At time: 929.4110279083252 and batch: 400, loss is 5.131452007293701 and perplexity is 169.26271039083846
At time: 930.2457208633423 and batch: 450, loss is 5.08938419342041 and perplexity is 162.28989209221712
At time: 931.0686931610107 and batch: 500, loss is 5.09743914604187 and perplexity is 163.60240851618818
At time: 931.8866579532623 and batch: 550, loss is 5.09964277267456 and perplexity is 163.96332465692686
At time: 932.7088437080383 and batch: 600, loss is 5.141737146377563 and perplexity is 171.01258433164253
At time: 933.5250113010406 and batch: 650, loss is 5.138420352935791 and perplexity is 170.44631053920824
At time: 934.3683202266693 and batch: 700, loss is 5.152791252136231 and perplexity is 172.91346242246544
At time: 935.185049533844 and batch: 750, loss is 5.127171287536621 and perplexity is 168.53969278428937
At time: 935.9994428157806 and batch: 800, loss is 5.144433536529541 and perplexity is 171.47432321422974
At time: 936.8639883995056 and batch: 850, loss is 5.176805200576783 and perplexity is 177.11605586782156
At time: 937.680890083313 and batch: 900, loss is 5.141544704437256 and perplexity is 170.9796775045251
At time: 938.4976603984833 and batch: 950, loss is 5.128119983673096 and perplexity is 168.6996616085813
At time: 939.3126113414764 and batch: 1000, loss is 5.121457099914551 and perplexity is 167.57937169841242
At time: 940.128663778305 and batch: 1050, loss is 5.115651168823242 and perplexity is 166.60923640850396
At time: 940.9432792663574 and batch: 1100, loss is 5.086981935501099 and perplexity is 161.90049781372008
At time: 941.7587454319 and batch: 1150, loss is 5.125107622146606 and perplexity is 168.1922418879493
At time: 942.5733342170715 and batch: 1200, loss is 5.162766160964966 and perplexity is 174.64688946092255
At time: 943.3904411792755 and batch: 1250, loss is 5.16938419342041 and perplexity is 175.80654131786335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052474195939781 and perplexity of 156.4089725857548
Finished 43 epochs...
Completing Train Step...
At time: 945.6630697250366 and batch: 50, loss is 5.146584491729737 and perplexity is 171.84355375826973
At time: 946.479487657547 and batch: 100, loss is 5.152393178939819 and perplexity is 172.84464390608855
At time: 947.2971065044403 and batch: 150, loss is 5.088642978668213 and perplexity is 162.16964500003053
At time: 948.1118369102478 and batch: 200, loss is 5.127082252502442 and perplexity is 168.52468751498904
At time: 948.9271657466888 and batch: 250, loss is 5.14111219406128 and perplexity is 170.90574300979395
At time: 949.7416195869446 and batch: 300, loss is 5.147291669845581 and perplexity is 171.96512073852415
At time: 950.556391954422 and batch: 350, loss is 5.159796590805054 and perplexity is 174.12903255621697
At time: 951.3712449073792 and batch: 400, loss is 5.130796661376953 and perplexity is 169.15182110411553
At time: 952.186199426651 and batch: 450, loss is 5.08871054649353 and perplexity is 162.1806028204695
At time: 953.002934217453 and batch: 500, loss is 5.096871709823608 and perplexity is 163.50960091788778
At time: 953.818564414978 and batch: 550, loss is 5.099186544418335 and perplexity is 163.8885370166483
At time: 954.6337327957153 and batch: 600, loss is 5.14134801864624 and perplexity is 170.94605153839078
At time: 955.4473621845245 and batch: 650, loss is 5.1380555629730225 and perplexity is 170.38414477534536
At time: 956.2618937492371 and batch: 700, loss is 5.152350978851318 and perplexity is 172.83735000072159
At time: 957.1039967536926 and batch: 750, loss is 5.126791076660156 and perplexity is 168.4756243405082
At time: 957.9178326129913 and batch: 800, loss is 5.1441320705413816 and perplexity is 171.42263732909763
At time: 958.7377517223358 and batch: 850, loss is 5.176481161117554 and perplexity is 177.0586725745852
At time: 959.5538067817688 and batch: 900, loss is 5.141251478195191 and perplexity is 170.92954912605813
At time: 960.3697040081024 and batch: 950, loss is 5.127975654602051 and perplexity is 168.67531510013248
At time: 961.1855292320251 and batch: 1000, loss is 5.1212848281860355 and perplexity is 167.55050499691612
At time: 962.0009255409241 and batch: 1050, loss is 5.115641765594482 and perplexity is 166.6076697511063
At time: 962.8165426254272 and batch: 1100, loss is 5.086815929412841 and perplexity is 161.87362357609638
At time: 963.6325197219849 and batch: 1150, loss is 5.125143966674805 and perplexity is 168.1983548667134
At time: 964.4476048946381 and batch: 1200, loss is 5.162682094573975 and perplexity is 174.63220814433896
At time: 965.2653012275696 and batch: 1250, loss is 5.169024038314819 and perplexity is 175.74323509512536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0524046960538325 and perplexity of 156.398102557736
Finished 44 epochs...
Completing Train Step...
At time: 967.5431616306305 and batch: 50, loss is 5.145830574035645 and perplexity is 171.71404668744609
At time: 968.4041745662689 and batch: 100, loss is 5.151869020462036 and perplexity is 172.75406966034882
At time: 969.2275660037994 and batch: 150, loss is 5.088115377426147 and perplexity is 162.08410666095435
At time: 970.0435762405396 and batch: 200, loss is 5.126493082046509 and perplexity is 168.42542699156513
At time: 970.860374212265 and batch: 250, loss is 5.140403938293457 and perplexity is 170.7847408868372
At time: 971.6742146015167 and batch: 300, loss is 5.146880264282227 and perplexity is 171.89438788209245
At time: 972.4898557662964 and batch: 350, loss is 5.159205570220947 and perplexity is 174.02614911979478
At time: 973.3053317070007 and batch: 400, loss is 5.130148105621338 and perplexity is 169.04215228398192
At time: 974.1181871891022 and batch: 450, loss is 5.088120145797729 and perplexity is 162.0848795400451
At time: 974.9339027404785 and batch: 500, loss is 5.096420783996582 and perplexity is 163.43588683690763
At time: 975.7486310005188 and batch: 550, loss is 5.098744592666626 and perplexity is 163.8161221937364
At time: 976.563529253006 and batch: 600, loss is 5.14093599319458 and perplexity is 170.87563192262937
At time: 977.3758325576782 and batch: 650, loss is 5.137620897293091 and perplexity is 170.31010072857575
At time: 978.2176394462585 and batch: 700, loss is 5.151895370483398 and perplexity is 172.75862179374892
At time: 979.0328788757324 and batch: 750, loss is 5.126424427032471 and perplexity is 168.4138641384391
At time: 979.8472571372986 and batch: 800, loss is 5.143838396072388 and perplexity is 171.37230226852938
At time: 980.661559343338 and batch: 850, loss is 5.176146211624146 and perplexity is 176.9993767930032
At time: 981.4785623550415 and batch: 900, loss is 5.140981693267822 and perplexity is 170.88344112996285
At time: 982.3266172409058 and batch: 950, loss is 5.127824850082398 and perplexity is 168.64988001817233
At time: 983.1655735969543 and batch: 1000, loss is 5.1212139987945555 and perplexity is 167.538637916879
At time: 983.9876430034637 and batch: 1050, loss is 5.115410108566284 and perplexity is 166.56907838361005
At time: 984.8025579452515 and batch: 1100, loss is 5.086640748977661 and perplexity is 161.84526896793295
At time: 985.6174776554108 and batch: 1150, loss is 5.125134372711182 and perplexity is 168.19674118555622
At time: 986.4335060119629 and batch: 1200, loss is 5.162574853897095 and perplexity is 174.61348147228094
At time: 987.2758266925812 and batch: 1250, loss is 5.168659181594848 and perplexity is 175.67912569089614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052312029539234 and perplexity of 156.38361036216315
Finished 45 epochs...
Completing Train Step...
At time: 989.5357131958008 and batch: 50, loss is 5.145107479095459 and perplexity is 171.5899260100526
At time: 990.3781220912933 and batch: 100, loss is 5.151340789794922 and perplexity is 172.6628397602223
At time: 991.1924350261688 and batch: 150, loss is 5.08751859664917 and perplexity is 161.98740683901295
At time: 992.0048260688782 and batch: 200, loss is 5.12589861869812 and perplexity is 168.3253340020363
At time: 992.8174793720245 and batch: 250, loss is 5.139824237823486 and perplexity is 170.68576558306802
At time: 993.6309576034546 and batch: 300, loss is 5.146422605514527 and perplexity is 171.81573690738668
At time: 994.4463090896606 and batch: 350, loss is 5.15863881111145 and perplexity is 173.92754615919293
At time: 995.2622230052948 and batch: 400, loss is 5.12953405380249 and perplexity is 168.9383835058741
At time: 996.0778474807739 and batch: 450, loss is 5.08752254486084 and perplexity is 161.98804640084552
At time: 996.8940327167511 and batch: 500, loss is 5.095933303833008 and perplexity is 163.35623450010303
At time: 997.7111339569092 and batch: 550, loss is 5.098307828903199 and perplexity is 163.7445888704159
At time: 998.526880979538 and batch: 600, loss is 5.140539226531982 and perplexity is 170.80784761659783
At time: 999.367002248764 and batch: 650, loss is 5.137171859741211 and perplexity is 170.23364226552948
At time: 1000.1828162670135 and batch: 700, loss is 5.151471138000488 and perplexity is 172.68534751844408
At time: 1000.9979691505432 and batch: 750, loss is 5.126072053909302 and perplexity is 168.35453007362645
At time: 1001.8145246505737 and batch: 800, loss is 5.143549699783325 and perplexity is 171.32283486168822
At time: 1002.6331632137299 and batch: 850, loss is 5.175830516815186 and perplexity is 176.94350782779563
At time: 1003.448614358902 and batch: 900, loss is 5.140734300613404 and perplexity is 170.84117105073472
At time: 1004.2675893306732 and batch: 950, loss is 5.127688636779785 and perplexity is 168.62690922552915
At time: 1005.1117277145386 and batch: 1000, loss is 5.120996417999268 and perplexity is 167.5021886922691
At time: 1005.9433605670929 and batch: 1050, loss is 5.115172529220581 and perplexity is 166.52950971148914
At time: 1006.7599914073944 and batch: 1100, loss is 5.086469945907592 and perplexity is 161.81762765979443
At time: 1007.5749242305756 and batch: 1150, loss is 5.125058145523071 and perplexity is 168.18392050957448
At time: 1008.3907282352448 and batch: 1200, loss is 5.162474594116211 and perplexity is 174.5959756404693
At time: 1009.205463886261 and batch: 1250, loss is 5.168328475952149 and perplexity is 175.62103721834785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052210898294936 and perplexity of 156.36779589274136
Finished 46 epochs...
Completing Train Step...
At time: 1011.4786930084229 and batch: 50, loss is 5.1443868827819825 and perplexity is 171.46632348105194
At time: 1012.2974181175232 and batch: 100, loss is 5.15081750869751 and perplexity is 172.57251219536502
At time: 1013.1175446510315 and batch: 150, loss is 5.086974573135376 and perplexity is 161.89930584743237
At time: 1013.9355499744415 and batch: 200, loss is 5.125346269607544 and perplexity is 168.23238532929994
At time: 1014.7606418132782 and batch: 250, loss is 5.139239625930786 and perplexity is 170.58600981663838
At time: 1015.5788140296936 and batch: 300, loss is 5.145951452255249 and perplexity is 171.73480443024496
At time: 1016.3983187675476 and batch: 350, loss is 5.158048667907715 and perplexity is 173.82493428071416
At time: 1017.2170553207397 and batch: 400, loss is 5.128938522338867 and perplexity is 168.83780533478802
At time: 1018.0344486236572 and batch: 450, loss is 5.08689024925232 and perplexity is 161.88565444487688
At time: 1018.8527450561523 and batch: 500, loss is 5.095405693054199 and perplexity is 163.2700687229904
At time: 1019.6999218463898 and batch: 550, loss is 5.097870607376098 and perplexity is 163.6730118598607
At time: 1020.5160005092621 and batch: 600, loss is 5.140147542953491 and perplexity is 170.74095808822906
At time: 1021.3631277084351 and batch: 650, loss is 5.136779947280884 and perplexity is 170.1669386518012
At time: 1022.2172031402588 and batch: 700, loss is 5.151061029434204 and perplexity is 172.61454229806105
At time: 1023.0452630519867 and batch: 750, loss is 5.125738582611084 and perplexity is 168.29839802965492
At time: 1023.8712074756622 and batch: 800, loss is 5.143252868652343 and perplexity is 171.27198845762442
At time: 1024.6854379177094 and batch: 850, loss is 5.175466423034668 and perplexity is 176.8790955238637
At time: 1025.504237651825 and batch: 900, loss is 5.140550994873047 and perplexity is 170.80985775343316
At time: 1026.320097208023 and batch: 950, loss is 5.127533693313598 and perplexity is 168.60078361177017
At time: 1027.1372294425964 and batch: 1000, loss is 5.120783834457398 and perplexity is 167.46658426831723
At time: 1027.9624283313751 and batch: 1050, loss is 5.114995765686035 and perplexity is 166.50007596822914
At time: 1028.7762265205383 and batch: 1100, loss is 5.086337423324585 and perplexity is 161.7961845906776
At time: 1029.596854686737 and batch: 1150, loss is 5.124993696212768 and perplexity is 168.17308152118002
At time: 1030.4419825077057 and batch: 1200, loss is 5.162361927032471 and perplexity is 174.576305529169
At time: 1031.2635786533356 and batch: 1250, loss is 5.167998628616333 and perplexity is 175.56311863977777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052130706118842 and perplexity of 156.35525692168807
Finished 47 epochs...
Completing Train Step...
At time: 1033.526272058487 and batch: 50, loss is 5.14370964050293 and perplexity is 171.35023855060527
At time: 1034.3747146129608 and batch: 100, loss is 5.150300989151001 and perplexity is 172.48339813618392
At time: 1035.1946976184845 and batch: 150, loss is 5.086455183029175 and perplexity is 161.81523878346488
At time: 1036.0151147842407 and batch: 200, loss is 5.124824781417846 and perplexity is 168.14467699864306
At time: 1036.8310611248016 and batch: 250, loss is 5.138673286437989 and perplexity is 170.4894275741111
At time: 1037.6477088928223 and batch: 300, loss is 5.145492925643921 and perplexity is 171.6560775028973
At time: 1038.4636237621307 and batch: 350, loss is 5.157508811950684 and perplexity is 173.7311191800611
At time: 1039.2792551517487 and batch: 400, loss is 5.128361196517944 and perplexity is 168.74035904205738
At time: 1040.0948565006256 and batch: 450, loss is 5.0862794113159175 and perplexity is 161.78679874126345
At time: 1040.9596695899963 and batch: 500, loss is 5.09490556716919 and perplexity is 163.18843355100762
At time: 1041.775959968567 and batch: 550, loss is 5.097455940246582 and perplexity is 163.6051561116
At time: 1042.589682340622 and batch: 600, loss is 5.139770584106445 and perplexity is 170.6766079029725
At time: 1043.4030227661133 and batch: 650, loss is 5.136357431411743 and perplexity is 170.09505560675072
At time: 1044.222937822342 and batch: 700, loss is 5.150675573348999 and perplexity is 172.54801979391308
At time: 1045.0436697006226 and batch: 750, loss is 5.125394706726074 and perplexity is 168.24053421864156
At time: 1045.863532781601 and batch: 800, loss is 5.14295823097229 and perplexity is 171.22153270973445
At time: 1046.6825332641602 and batch: 850, loss is 5.17509877204895 and perplexity is 176.8140777027069
At time: 1047.5002753734589 and batch: 900, loss is 5.140343961715698 and perplexity is 170.77449810971171
At time: 1048.3220131397247 and batch: 950, loss is 5.127380352020264 and perplexity is 168.57493213165435
At time: 1049.1425879001617 and batch: 1000, loss is 5.120605325698852 and perplexity is 167.4366926842958
At time: 1049.9629726409912 and batch: 1050, loss is 5.114852733612061 and perplexity is 166.47626282010916
At time: 1050.7827773094177 and batch: 1100, loss is 5.086196603775025 and perplexity is 161.7734021289887
At time: 1051.6009631156921 and batch: 1150, loss is 5.124918041229248 and perplexity is 168.16035887074094
At time: 1052.422215461731 and batch: 1200, loss is 5.162268257141113 and perplexity is 174.55995375144298
At time: 1053.246128320694 and batch: 1250, loss is 5.167661714553833 and perplexity is 175.50397891931277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.052030020386633 and perplexity of 156.3395149706685
Finished 48 epochs...
Completing Train Step...
At time: 1055.5222010612488 and batch: 50, loss is 5.143054676055908 and perplexity is 171.23804698112122
At time: 1056.3659400939941 and batch: 100, loss is 5.149777936935425 and perplexity is 172.39320390284178
At time: 1057.182232618332 and batch: 150, loss is 5.085954504013062 and perplexity is 161.73424156741387
At time: 1057.9989387989044 and batch: 200, loss is 5.12428653717041 and perplexity is 168.05419844547626
At time: 1058.817979812622 and batch: 250, loss is 5.138160667419434 and perplexity is 170.40205384770428
At time: 1059.634804725647 and batch: 300, loss is 5.145024642944336 and perplexity is 171.57571274970456
At time: 1060.452632188797 and batch: 350, loss is 5.1570086097717285 and perplexity is 173.64424022602745
At time: 1061.2695496082306 and batch: 400, loss is 5.127797689437866 and perplexity is 168.64529944093684
At time: 1062.1372411251068 and batch: 450, loss is 5.085679445266724 and perplexity is 161.68976126731667
At time: 1062.9570395946503 and batch: 500, loss is 5.094348611831665 and perplexity is 163.0975701876856
At time: 1063.7744090557098 and batch: 550, loss is 5.097040243148804 and perplexity is 163.53716005688335
At time: 1064.5909345149994 and batch: 600, loss is 5.1394038772583 and perplexity is 170.61403109640932
At time: 1065.4068369865417 and batch: 650, loss is 5.1359585094451905 and perplexity is 170.02721448522368
At time: 1066.2248904705048 and batch: 700, loss is 5.1503017616271975 and perplexity is 172.48353137555478
At time: 1067.0434975624084 and batch: 750, loss is 5.125050201416015 and perplexity is 168.1825844438118
At time: 1067.861397743225 and batch: 800, loss is 5.142648191452026 and perplexity is 171.16845549631628
At time: 1068.6799757480621 and batch: 850, loss is 5.174731092453003 and perplexity is 176.7490787241907
At time: 1069.5006861686707 and batch: 900, loss is 5.140087327957153 and perplexity is 170.7306772315971
At time: 1070.3292865753174 and batch: 950, loss is 5.1272225761413575 and perplexity is 168.5483371716524
At time: 1071.141844511032 and batch: 1000, loss is 5.12044054031372 and perplexity is 167.4091038375884
At time: 1071.9561433792114 and batch: 1050, loss is 5.114690828323364 and perplexity is 166.44931161454633
At time: 1072.7718768119812 and batch: 1100, loss is 5.086037168502807 and perplexity is 161.74761179858154
At time: 1073.590351343155 and batch: 1150, loss is 5.124815473556518 and perplexity is 168.1431119385902
At time: 1074.408846616745 and batch: 1200, loss is 5.162186050415039 and perplexity is 174.54560433895873
At time: 1075.2295203208923 and batch: 1250, loss is 5.167292060852051 and perplexity is 175.439115213121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051921760948905 and perplexity of 156.32259065880783
Finished 49 epochs...
Completing Train Step...
At time: 1077.5105440616608 and batch: 50, loss is 5.142406063079834 and perplexity is 171.12701577388506
At time: 1078.3808963298798 and batch: 100, loss is 5.149267082214355 and perplexity is 172.30515851186436
At time: 1079.1998176574707 and batch: 150, loss is 5.085462608337402 and perplexity is 161.65470475689116
At time: 1080.0209527015686 and batch: 200, loss is 5.123758249282837 and perplexity is 167.96544089481537
At time: 1080.85249710083 and batch: 250, loss is 5.137703952789306 and perplexity is 170.32424650593643
At time: 1081.6697733402252 and batch: 300, loss is 5.144540414810181 and perplexity is 171.4926510744764
At time: 1082.4871702194214 and batch: 350, loss is 5.156507987976074 and perplexity is 173.55733189059896
At time: 1083.3543629646301 and batch: 400, loss is 5.1272479438781735 and perplexity is 168.5526129157433
At time: 1084.1723124980927 and batch: 450, loss is 5.085086431503296 and perplexity is 161.59390543820186
At time: 1084.9885611534119 and batch: 500, loss is 5.093842391967773 and perplexity is 163.01502785195171
At time: 1085.805270433426 and batch: 550, loss is 5.096643009185791 and perplexity is 163.47221044364426
At time: 1086.6225152015686 and batch: 600, loss is 5.138950700759888 and perplexity is 170.5367303439821
At time: 1087.43732380867 and batch: 650, loss is 5.135546102523803 and perplexity is 169.9571085422275
At time: 1088.2581536769867 and batch: 700, loss is 5.149911785125733 and perplexity is 172.41627996551574
At time: 1089.0748081207275 and batch: 750, loss is 5.124699382781983 and perplexity is 168.12359320748666
At time: 1089.8905022144318 and batch: 800, loss is 5.142323799133301 and perplexity is 171.11293876923202
At time: 1090.7049112319946 and batch: 850, loss is 5.174371633529663 and perplexity is 176.68555610822077
At time: 1091.5300643444061 and batch: 900, loss is 5.139870462417602 and perplexity is 170.69365564565953
At time: 1092.366848230362 and batch: 950, loss is 5.127057189941406 and perplexity is 168.52046390764966
At time: 1093.1819837093353 and batch: 1000, loss is 5.120123472213745 and perplexity is 167.35603216522782
At time: 1093.9979648590088 and batch: 1050, loss is 5.114435062408448 and perplexity is 166.4067449978509
At time: 1094.8156261444092 and batch: 1100, loss is 5.085859642028809 and perplexity is 161.71889986402093
At time: 1095.6330335140228 and batch: 1150, loss is 5.124707899093628 and perplexity is 168.12502500649813
At time: 1096.4494512081146 and batch: 1200, loss is 5.162075300216674 and perplexity is 174.52627444906875
At time: 1097.2659330368042 and batch: 1250, loss is 5.1669336032867434 and perplexity is 175.37623900492147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.051861171304744 and perplexity of 156.3131194155981
Finished Training.
Improved accuracyfrom -10000000 to -156.3131194155981
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f1814fb8898>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 6.113648271593582, 'num_layers': 1, 'dropout': 0.5372937996772553, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 3.8840390778038114, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.349114179611206 and batch: 50, loss is 7.396531343460083 and perplexity is 1630.3195922715727
At time: 2.158409595489502 and batch: 100, loss is 6.666603918075562 and perplexity is 785.7226896887611
At time: 2.957627773284912 and batch: 150, loss is 6.383773822784423 and perplexity is 592.1581963701842
At time: 3.755030870437622 and batch: 200, loss is 6.218604373931885 and perplexity is 502.00213563310075
At time: 4.578850746154785 and batch: 250, loss is 6.172094612121582 and perplexity is 479.1887702330917
At time: 5.376297950744629 and batch: 300, loss is 6.098705396652222 and perplexity is 445.2809345878811
At time: 6.173393487930298 and batch: 350, loss is 6.071093578338623 and perplexity is 433.15411061407616
At time: 6.96906042098999 and batch: 400, loss is 5.984938659667969 and perplexity is 397.39814396760454
At time: 7.76532244682312 and batch: 450, loss is 5.917713422775268 and perplexity is 371.56113855668445
At time: 8.563087463378906 and batch: 500, loss is 5.885979433059692 and perplexity is 359.9551473737736
At time: 9.361082315444946 and batch: 550, loss is 5.870227966308594 and perplexity is 354.3297463057382
At time: 10.158610582351685 and batch: 600, loss is 5.867633190155029 and perplexity is 353.4115317253583
At time: 10.95886754989624 and batch: 650, loss is 5.8161403179168705 and perplexity is 335.67395536696023
At time: 11.757721424102783 and batch: 700, loss is 5.808168811798096 and perplexity is 333.0087652806366
At time: 12.556352376937866 and batch: 750, loss is 5.763407049179077 and perplexity is 318.43139426213395
At time: 13.356613874435425 and batch: 800, loss is 5.746518316268921 and perplexity is 313.0986498958482
At time: 14.155012369155884 and batch: 850, loss is 5.79351508140564 and perplexity is 328.1645243998768
At time: 14.949360132217407 and batch: 900, loss is 5.75004548072815 and perplexity is 314.20495023065797
At time: 15.742847681045532 and batch: 950, loss is 5.7129027462005615 and perplexity is 302.74859638382196
At time: 16.538053035736084 and batch: 1000, loss is 5.694470596313477 and perplexity is 297.21940288058295
At time: 17.33132767677307 and batch: 1050, loss is 5.6675244808197025 and perplexity is 289.317436300149
At time: 18.125481605529785 and batch: 1100, loss is 5.642298593521118 and perplexity is 282.1104309851088
At time: 18.935079336166382 and batch: 1150, loss is 5.671045837402343 and perplexity is 290.3380220274708
At time: 19.743279695510864 and batch: 1200, loss is 5.654786777496338 and perplexity is 285.6555680371506
At time: 20.538508415222168 and batch: 1250, loss is 5.634062747955323 and perplexity is 279.79655448677346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.068081820968294 and perplexity of 158.8692951329244
Finished 1 epochs...
Completing Train Step...
At time: 22.739184617996216 and batch: 50, loss is 5.308229465484619 and perplexity is 201.99227730047735
At time: 23.528263092041016 and batch: 100, loss is 5.278239793777466 and perplexity is 196.02452790581415
At time: 24.339648485183716 and batch: 150, loss is 5.12879490852356 and perplexity is 168.8135596344462
At time: 25.160331964492798 and batch: 200, loss is 5.135959815979004 and perplexity is 170.02743663167377
At time: 25.951321363449097 and batch: 250, loss is 5.13568922996521 and perplexity is 169.9814358092302
At time: 26.74215030670166 and batch: 300, loss is 5.114550352096558 and perplexity is 166.42593108553888
At time: 27.532118797302246 and batch: 350, loss is 5.099991188049317 and perplexity is 164.0204619533118
At time: 28.346721649169922 and batch: 400, loss is 5.074246282577515 and perplexity is 159.851663579062
At time: 29.135058164596558 and batch: 450, loss is 5.00865496635437 and perplexity is 149.7032447720044
At time: 29.925077438354492 and batch: 500, loss is 4.996343841552735 and perplexity is 147.87152782547722
At time: 30.717242002487183 and batch: 550, loss is 4.98977593421936 and perplexity is 146.90350375949077
At time: 31.510446548461914 and batch: 600, loss is 4.988506059646607 and perplexity is 146.71707313222763
At time: 32.30473804473877 and batch: 650, loss is 4.963312845230103 and perplexity is 143.0669704177906
At time: 33.09636735916138 and batch: 700, loss is 4.959549179077149 and perplexity is 142.52952611849886
At time: 33.884939193725586 and batch: 750, loss is 4.948149328231811 and perplexity is 140.91393701343816
At time: 34.68167424201965 and batch: 800, loss is 4.949622163772583 and perplexity is 141.12163298151805
At time: 35.47222661972046 and batch: 850, loss is 4.985199308395385 and perplexity is 146.23271753017164
At time: 36.26144623756409 and batch: 900, loss is 4.955067672729492 and perplexity is 141.89220828108918
At time: 37.0529351234436 and batch: 950, loss is 4.922280216217041 and perplexity is 137.31536520243333
At time: 37.84243321418762 and batch: 1000, loss is 4.89933144569397 and perplexity is 134.2000296791906
At time: 38.63305330276489 and batch: 1050, loss is 4.874135417938232 and perplexity is 130.86096424502279
At time: 39.42344045639038 and batch: 1100, loss is 4.845812511444092 and perplexity is 127.20659683149867
At time: 40.21776723861694 and batch: 1150, loss is 4.863266954421997 and perplexity is 129.4464075825033
At time: 41.012585401535034 and batch: 1200, loss is 4.859094772338867 and perplexity is 128.90745867833763
At time: 41.8084557056427 and batch: 1250, loss is 4.8471877384185795 and perplexity is 127.38165511964418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.662590528056569 and perplexity of 105.91009015073533
Finished 2 epochs...
Completing Train Step...
At time: 44.03499674797058 and batch: 50, loss is 4.769770278930664 and perplexity is 117.89215653747745
At time: 44.82871913909912 and batch: 100, loss is 4.784399337768555 and perplexity is 119.62948458551264
At time: 45.623461961746216 and batch: 150, loss is 4.6678510189056395 and perplexity is 106.46869719624024
At time: 46.41831016540527 and batch: 200, loss is 4.719188795089722 and perplexity is 112.07729812684065
At time: 47.23996639251709 and batch: 250, loss is 4.732903099060058 and perplexity is 113.62494847816612
At time: 48.03359794616699 and batch: 300, loss is 4.729708967208862 and perplexity is 113.26259442253246
At time: 48.82533097267151 and batch: 350, loss is 4.720802087783813 and perplexity is 112.25825754408623
At time: 49.616687536239624 and batch: 400, loss is 4.716183605194092 and perplexity is 111.74099015075055
At time: 50.41100597381592 and batch: 450, loss is 4.650821208953857 and perplexity is 104.67090697859261
At time: 51.20218300819397 and batch: 500, loss is 4.660830430984497 and perplexity is 105.72384206659406
At time: 51.99585294723511 and batch: 550, loss is 4.658000221252442 and perplexity is 105.42504444927869
At time: 52.789040327072144 and batch: 600, loss is 4.669904079437256 and perplexity is 106.68750841578733
At time: 53.58302855491638 and batch: 650, loss is 4.6652649974823 and perplexity is 106.19372256276345
At time: 54.37958574295044 and batch: 700, loss is 4.6575760173797605 and perplexity is 105.38033222136633
At time: 55.172033071517944 and batch: 750, loss is 4.6518031597137455 and perplexity is 104.7737391350327
At time: 55.96561312675476 and batch: 800, loss is 4.666653280258179 and perplexity is 106.34125186120096
At time: 56.758999824523926 and batch: 850, loss is 4.70676547050476 and perplexity is 110.69353871704355
At time: 57.552000999450684 and batch: 900, loss is 4.674619598388672 and perplexity is 107.19178340831712
At time: 58.34546113014221 and batch: 950, loss is 4.651777000427246 and perplexity is 104.77099836462148
At time: 59.13932943344116 and batch: 1000, loss is 4.635258169174194 and perplexity is 103.05452006150605
At time: 59.93195176124573 and batch: 1050, loss is 4.614878759384156 and perplexity is 100.9755854481514
At time: 60.72578048706055 and batch: 1100, loss is 4.593762264251709 and perplexity is 98.86569018677619
At time: 61.516881465911865 and batch: 1150, loss is 4.609100322723389 and perplexity is 100.39378698500909
At time: 62.31233024597168 and batch: 1200, loss is 4.611026105880737 and perplexity is 100.58730993089313
At time: 63.10635256767273 and batch: 1250, loss is 4.612685022354126 and perplexity is 100.75431436125193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.546218315180201 and perplexity of 94.27521419641648
Finished 3 epochs...
Completing Train Step...
At time: 65.32638907432556 and batch: 50, loss is 4.546195859909058 and perplexity is 94.27309724468809
At time: 66.20949196815491 and batch: 100, loss is 4.563573083877563 and perplexity is 95.92561850188967
At time: 67.01008677482605 and batch: 150, loss is 4.461894817352295 and perplexity is 86.65154248690145
At time: 67.83900952339172 and batch: 200, loss is 4.517787933349609 and perplexity is 91.63267603572682
At time: 68.633305311203 and batch: 250, loss is 4.53700855255127 and perplexity is 93.41094780492745
At time: 69.42897534370422 and batch: 300, loss is 4.535250797271728 and perplexity is 93.24689843981383
At time: 70.23750591278076 and batch: 350, loss is 4.526357288360596 and perplexity is 92.42128306823184
At time: 71.03433108329773 and batch: 400, loss is 4.528307008743286 and perplexity is 92.60165450742127
At time: 71.83040308952332 and batch: 450, loss is 4.460983972549439 and perplexity is 86.57265231357414
At time: 72.62679386138916 and batch: 500, loss is 4.48369194984436 and perplexity is 88.56103274350036
At time: 73.4444305896759 and batch: 550, loss is 4.476171531677246 and perplexity is 87.89751483721118
At time: 74.25804471969604 and batch: 600, loss is 4.495617437362671 and perplexity is 89.62348879568435
At time: 75.05421757698059 and batch: 650, loss is 4.499326572418213 and perplexity is 89.956531688486
At time: 75.85375785827637 and batch: 700, loss is 4.486530494689942 and perplexity is 88.81277432737093
At time: 76.67629837989807 and batch: 750, loss is 4.480838394165039 and perplexity is 88.30867912933135
At time: 77.48924994468689 and batch: 800, loss is 4.504367122650146 and perplexity is 90.4111067970324
At time: 78.28667283058167 and batch: 850, loss is 4.548388462066651 and perplexity is 94.48002741597833
At time: 79.08514046669006 and batch: 900, loss is 4.509776620864868 and perplexity is 90.90151074159645
At time: 79.91234469413757 and batch: 950, loss is 4.4947683715820315 and perplexity is 89.54742485442088
At time: 80.71134901046753 and batch: 1000, loss is 4.4786481857299805 and perplexity is 88.11547636980325
At time: 81.51716709136963 and batch: 1050, loss is 4.463956890106201 and perplexity is 86.83040862586307
At time: 82.31568503379822 and batch: 1100, loss is 4.440378475189209 and perplexity is 84.8070329584126
At time: 83.11167168617249 and batch: 1150, loss is 4.4526528549194335 and perplexity is 85.85440143421968
At time: 83.90563488006592 and batch: 1200, loss is 4.458744373321533 and perplexity is 86.37898122205978
At time: 84.72268152236938 and batch: 1250, loss is 4.4701301383972165 and perplexity is 87.36809221712899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4914626518305205 and perplexity of 89.25189490210683
Finished 4 epochs...
Completing Train Step...
At time: 86.97662711143494 and batch: 50, loss is 4.408667554855347 and perplexity is 82.15991695559319
At time: 87.77622580528259 and batch: 100, loss is 4.423719158172608 and perplexity is 83.40590899989974
At time: 88.62000870704651 and batch: 150, loss is 4.331123723983764 and perplexity is 76.02967495293699
At time: 89.41759872436523 and batch: 200, loss is 4.386111364364624 and perplexity is 80.3274466855711
At time: 90.21553444862366 and batch: 250, loss is 4.407980012893677 and perplexity is 82.10344797973856
At time: 91.01474237442017 and batch: 300, loss is 4.40549015045166 and perplexity is 81.89927597383729
At time: 91.81146717071533 and batch: 350, loss is 4.396447219848633 and perplexity is 81.16200507372255
At time: 92.60935926437378 and batch: 400, loss is 4.404677495956421 and perplexity is 81.83274719517718
At time: 93.4066321849823 and batch: 450, loss is 4.334885320663452 and perplexity is 76.31620649598914
At time: 94.20125246047974 and batch: 500, loss is 4.364517984390258 and perplexity is 78.61149883850369
At time: 94.99836730957031 and batch: 550, loss is 4.3529356002807615 and perplexity is 77.70624289466912
At time: 95.8392014503479 and batch: 600, loss is 4.378027572631836 and perplexity is 79.68071388464364
At time: 96.6335461139679 and batch: 650, loss is 4.384721803665161 and perplexity is 80.21590433794779
At time: 97.4300479888916 and batch: 700, loss is 4.36867172241211 and perplexity is 78.93870951343298
At time: 98.2284791469574 and batch: 750, loss is 4.363806524276733 and perplexity is 78.55558978350531
At time: 99.02485585212708 and batch: 800, loss is 4.392800006866455 and perplexity is 80.86652911457419
At time: 99.82349133491516 and batch: 850, loss is 4.438771905899048 and perplexity is 84.67089397131187
At time: 100.62209343910217 and batch: 900, loss is 4.396591529846192 and perplexity is 81.17371840763188
At time: 101.42019701004028 and batch: 950, loss is 4.385601787567139 and perplexity is 80.28652411002192
At time: 102.21622133255005 and batch: 1000, loss is 4.370073289871216 and perplexity is 79.049425009445
At time: 103.01494479179382 and batch: 1050, loss is 4.357047367095947 and perplexity is 78.02641062197482
At time: 103.81092810630798 and batch: 1100, loss is 4.3309281539916995 and perplexity is 76.0148072838921
At time: 104.60784864425659 and batch: 1150, loss is 4.340367345809937 and perplexity is 76.73572270766648
At time: 105.40452861785889 and batch: 1200, loss is 4.350255508422851 and perplexity is 77.4982618544167
At time: 106.20149660110474 and batch: 1250, loss is 4.366316156387329 and perplexity is 78.7529830026707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.466490808194571 and perplexity of 87.05070876227684
Finished 5 epochs...
Completing Train Step...
At time: 108.42664694786072 and batch: 50, loss is 4.308325099945068 and perplexity is 74.31591291664724
At time: 109.25248765945435 and batch: 100, loss is 4.322524347305298 and perplexity is 75.37867026829896
At time: 110.0596444606781 and batch: 150, loss is 4.235641522407532 and perplexity is 69.10599756216835
At time: 110.8683168888092 and batch: 200, loss is 4.291541090011597 and perplexity is 73.07900308611059
At time: 111.66163945198059 and batch: 250, loss is 4.31394326210022 and perplexity is 74.73460680991892
At time: 112.45912289619446 and batch: 300, loss is 4.308337450027466 and perplexity is 74.31683072996275
At time: 113.25330829620361 and batch: 350, loss is 4.300433349609375 and perplexity is 73.73173839751782
At time: 114.0532078742981 and batch: 400, loss is 4.3123453426361085 and perplexity is 74.61528228793249
At time: 114.8523576259613 and batch: 450, loss is 4.241062321662903 and perplexity is 69.48162448153946
At time: 115.64994597434998 and batch: 500, loss is 4.27442494392395 and perplexity is 71.8388160815272
At time: 116.44835066795349 and batch: 550, loss is 4.261800928115845 and perplexity is 70.9376220438262
At time: 117.24619889259338 and batch: 600, loss is 4.2903735542297365 and perplexity is 72.99373052417505
At time: 118.04366040229797 and batch: 650, loss is 4.300589532852173 and perplexity is 73.74325495884138
At time: 118.84097409248352 and batch: 700, loss is 4.279655027389526 and perplexity is 72.21552333218659
At time: 119.63614177703857 and batch: 750, loss is 4.2742593431472775 and perplexity is 71.82692050277457
At time: 120.45285606384277 and batch: 800, loss is 4.307390804290772 and perplexity is 74.24651230755623
At time: 121.26171708106995 and batch: 850, loss is 4.354845705032349 and perplexity is 77.85481180436926
At time: 122.08584260940552 and batch: 900, loss is 4.312132630348206 and perplexity is 74.59941238844567
At time: 122.89432692527771 and batch: 950, loss is 4.303465843200684 and perplexity is 73.95566878380937
At time: 123.70339274406433 and batch: 1000, loss is 4.288427038192749 and perplexity is 72.85178525130353
At time: 124.53283762931824 and batch: 1050, loss is 4.276396532058715 and perplexity is 71.98059235521302
At time: 125.35591840744019 and batch: 1100, loss is 4.24822235584259 and perplexity is 69.98090057162999
At time: 126.17572522163391 and batch: 1150, loss is 4.256132173538208 and perplexity is 70.5366317058563
At time: 126.97642397880554 and batch: 1200, loss is 4.268405284881592 and perplexity is 71.40766988119901
At time: 127.77089715003967 and batch: 1250, loss is 4.2854953861236575 and perplexity is 72.63852192401434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.446904760207573 and perplexity of 85.36231782982159
Finished 6 epochs...
Completing Train Step...
At time: 130.14349961280823 and batch: 50, loss is 4.228727788925171 and perplexity is 68.62986493661047
At time: 130.9804482460022 and batch: 100, loss is 4.243397054672241 and perplexity is 69.64403504263326
At time: 131.77701377868652 and batch: 150, loss is 4.16053005695343 and perplexity is 64.10549315837757
At time: 132.57160782814026 and batch: 200, loss is 4.215664167404174 and perplexity is 67.7391410632312
At time: 133.369975566864 and batch: 250, loss is 4.240196561813354 and perplexity is 69.42149611290134
At time: 134.16637682914734 and batch: 300, loss is 4.233763875961304 and perplexity is 68.97636267376275
At time: 134.96359086036682 and batch: 350, loss is 4.225742921829224 and perplexity is 68.42531933354742
At time: 135.75898694992065 and batch: 400, loss is 4.240186452865601 and perplexity is 69.42079433817126
At time: 136.5562551021576 and batch: 450, loss is 4.165990347862244 and perplexity is 64.45648518710506
At time: 137.36648297309875 and batch: 500, loss is 4.203360648155212 and perplexity is 66.91081733476217
At time: 138.20358109474182 and batch: 550, loss is 4.190447306632995 and perplexity is 66.05232999874674
At time: 139.0140516757965 and batch: 600, loss is 4.221933627128601 and perplexity is 68.16516294798026
At time: 139.83055353164673 and batch: 650, loss is 4.233290224075318 and perplexity is 68.94369962556168
At time: 140.64347195625305 and batch: 700, loss is 4.207764568328858 and perplexity is 67.20613703795881
At time: 141.45194673538208 and batch: 750, loss is 4.2044567155838015 and perplexity is 66.9841963090687
At time: 142.25581669807434 and batch: 800, loss is 4.238987822532653 and perplexity is 69.3376343173584
At time: 143.05322432518005 and batch: 850, loss is 4.288046369552612 and perplexity is 72.82405813903625
At time: 143.85161471366882 and batch: 900, loss is 4.2447154712677 and perplexity is 69.73591544922557
At time: 144.65498042106628 and batch: 950, loss is 4.237301664352417 and perplexity is 69.22081861061254
At time: 145.4541826248169 and batch: 1000, loss is 4.222473945617676 and perplexity is 68.2020037978317
At time: 146.28688621520996 and batch: 1050, loss is 4.21223970413208 and perplexity is 67.50756759629724
At time: 147.12411665916443 and batch: 1100, loss is 4.1824934911727905 and perplexity is 65.52904576704209
At time: 147.9636538028717 and batch: 1150, loss is 4.189473676681518 and perplexity is 65.98805076906459
At time: 148.7739555835724 and batch: 1200, loss is 4.2034735679626465 and perplexity is 66.91837331797294
At time: 149.6447412967682 and batch: 1250, loss is 4.221417617797852 and perplexity is 68.12999816132631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.443241759808394 and perplexity of 85.05020760445622
Finished 7 epochs...
Completing Train Step...
At time: 151.8812279701233 and batch: 50, loss is 4.165087375640869 and perplexity is 64.3983090411684
At time: 152.69136476516724 and batch: 100, loss is 4.180845737457275 and perplexity is 65.42115894826875
At time: 153.490709066391 and batch: 150, loss is 4.099591870307922 and perplexity is 60.315665959117545
At time: 154.28781080245972 and batch: 200, loss is 4.1567714357376095 and perplexity is 63.86499714158592
At time: 155.08805918693542 and batch: 250, loss is 4.179251351356506 and perplexity is 65.31693547005653
At time: 155.8862166404724 and batch: 300, loss is 4.174827976226807 and perplexity is 65.02865222467068
At time: 156.68531560897827 and batch: 350, loss is 4.165164170265197 and perplexity is 64.40325467501519
At time: 157.51536202430725 and batch: 400, loss is 4.180737075805664 and perplexity is 65.4140505632985
At time: 158.31217694282532 and batch: 450, loss is 4.105538239479065 and perplexity is 60.675393652570804
At time: 159.11164665222168 and batch: 500, loss is 4.144494113922119 and perplexity is 63.08569965074795
At time: 159.90930271148682 and batch: 550, loss is 4.132477993965149 and perplexity is 62.332190510680306
At time: 160.72752213478088 and batch: 600, loss is 4.166230492591858 and perplexity is 64.47196593104731
At time: 161.57125687599182 and batch: 650, loss is 4.179318509101868 and perplexity is 65.32132215547492
At time: 162.40623378753662 and batch: 700, loss is 4.151633863449097 and perplexity is 63.537727507613475
At time: 163.2025351524353 and batch: 750, loss is 4.14846667766571 and perplexity is 63.33681005973627
At time: 164.01337480545044 and batch: 800, loss is 4.18286657333374 and perplexity is 65.55349804611276
At time: 164.8260293006897 and batch: 850, loss is 4.233000297546386 and perplexity is 68.92371391537134
At time: 165.62226629257202 and batch: 900, loss is 4.188690733909607 and perplexity is 65.93640612172229
At time: 166.4185483455658 and batch: 950, loss is 4.1824126625061036 and perplexity is 65.52374935569729
At time: 167.21414875984192 and batch: 1000, loss is 4.168789381980896 and perplexity is 64.63715381918945
At time: 168.01081156730652 and batch: 1050, loss is 4.159540510177612 and perplexity is 64.04208915010022
At time: 168.81100916862488 and batch: 1100, loss is 4.126923513412476 and perplexity is 61.98692733715314
At time: 169.60858941078186 and batch: 1150, loss is 4.133772969245911 and perplexity is 62.412961443480576
At time: 170.49838733673096 and batch: 1200, loss is 4.147791919708252 and perplexity is 63.294087458476966
At time: 171.30520939826965 and batch: 1250, loss is 4.168574271202087 and perplexity is 64.6232511660503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.440199803261861 and perplexity of 84.79188167597601
Finished 8 epochs...
Completing Train Step...
At time: 173.50803971290588 and batch: 50, loss is 4.113985185623169 and perplexity is 61.19008616468082
At time: 174.35563611984253 and batch: 100, loss is 4.126599678993225 and perplexity is 61.96685708643216
At time: 175.15533423423767 and batch: 150, loss is 4.050155739784241 and perplexity is 57.40639680914281
At time: 175.9516351222992 and batch: 200, loss is 4.105085997581482 and perplexity is 60.64795990122223
At time: 176.772625207901 and batch: 250, loss is 4.127094736099243 and perplexity is 61.99754181408998
At time: 177.5692069530487 and batch: 300, loss is 4.124178681373596 and perplexity is 61.81701692745144
At time: 178.36721634864807 and batch: 350, loss is 4.115481114387512 and perplexity is 61.281690674495046
At time: 179.16431856155396 and batch: 400, loss is 4.131464729309082 and perplexity is 62.269063492689746
At time: 179.96107411384583 and batch: 450, loss is 4.054860692024231 and perplexity is 57.677127552587876
At time: 180.7601342201233 and batch: 500, loss is 4.094862213134766 and perplexity is 60.03106709502122
At time: 181.5554678440094 and batch: 550, loss is 4.083438181877137 and perplexity is 59.34917271551947
At time: 182.36430859565735 and batch: 600, loss is 4.1186774587631225 and perplexity is 61.47788144140376
At time: 183.16062998771667 and batch: 650, loss is 4.133981432914734 and perplexity is 62.42597363464263
At time: 183.95603919029236 and batch: 700, loss is 4.103870792388916 and perplexity is 60.574304947436865
At time: 184.75208711624146 and batch: 750, loss is 4.100541391372681 and perplexity is 60.372964153096795
At time: 185.5474817752838 and batch: 800, loss is 4.135285067558288 and perplexity is 62.50740736491355
At time: 186.34363555908203 and batch: 850, loss is 4.185215721130371 and perplexity is 65.70767392166682
At time: 187.13963174819946 and batch: 900, loss is 4.139905209541321 and perplexity is 62.79686862309018
At time: 187.9378411769867 and batch: 950, loss is 4.135770926475525 and perplexity is 62.53778452508462
At time: 188.7349545955658 and batch: 1000, loss is 4.123197937011719 and perplexity is 61.75641995655769
At time: 189.5300235748291 and batch: 1050, loss is 4.114475483894348 and perplexity is 61.22009491415852
At time: 190.32622861862183 and batch: 1100, loss is 4.080119018554687 and perplexity is 59.152509677146824
At time: 191.16840934753418 and batch: 1150, loss is 4.086763153076172 and perplexity is 59.54683543480562
At time: 191.9648027420044 and batch: 1200, loss is 4.099880781173706 and perplexity is 60.33309432789146
At time: 192.76146936416626 and batch: 1250, loss is 4.123003096580505 and perplexity is 61.74438848121011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.445699204493613 and perplexity of 85.2594708061697
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 194.97807955741882 and batch: 50, loss is 4.1050445127487185 and perplexity is 60.64544398293485
At time: 195.78021574020386 and batch: 100, loss is 4.131302599906921 and perplexity is 62.25896866500888
At time: 196.5887520313263 and batch: 150, loss is 4.052806544303894 and perplexity is 57.55877181421226
At time: 197.3948690891266 and batch: 200, loss is 4.1043634557724 and perplexity is 60.60415504188494
At time: 198.22541522979736 and batch: 250, loss is 4.116860585212708 and perplexity is 61.36628531340248
At time: 199.02717280387878 and batch: 300, loss is 4.1036896276474 and perplexity is 60.56333201312297
At time: 199.82648468017578 and batch: 350, loss is 4.084516005516052 and perplexity is 59.41317514229062
At time: 200.6366057395935 and batch: 400, loss is 4.096153726577759 and perplexity is 60.10864811285158
At time: 201.44002485275269 and batch: 450, loss is 4.014155726432801 and perplexity is 55.37652272642379
At time: 202.23963499069214 and batch: 500, loss is 4.046950650215149 and perplexity is 57.22269870722055
At time: 203.04124546051025 and batch: 550, loss is 4.021688041687011 and perplexity is 55.79521101984235
At time: 203.85283136367798 and batch: 600, loss is 4.046539759635925 and perplexity is 57.199191269227114
At time: 204.6592493057251 and batch: 650, loss is 4.057953667640686 and perplexity is 57.85579767045691
At time: 205.46732759475708 and batch: 700, loss is 4.021297998428345 and perplexity is 55.77345271753361
At time: 206.29420852661133 and batch: 750, loss is 4.008492078781128 and perplexity is 55.06377609288964
At time: 207.09867811203003 and batch: 800, loss is 4.031205725669861 and perplexity is 56.328787383805405
At time: 207.9207935333252 and batch: 850, loss is 4.061753182411194 and perplexity is 58.0760397698633
At time: 208.71778106689453 and batch: 900, loss is 4.005967860221863 and perplexity is 54.924958364184825
At time: 209.5163540840149 and batch: 950, loss is 3.9927227306365967 and perplexity is 54.20226681015314
At time: 210.316556930542 and batch: 1000, loss is 3.9658848905563353 and perplexity is 52.76694170505775
At time: 211.11371445655823 and batch: 1050, loss is 3.957073926925659 and perplexity is 52.30405632849907
At time: 211.95674920082092 and batch: 1100, loss is 3.9007211208343504 and perplexity is 49.43808708894992
At time: 212.75281381607056 and batch: 1150, loss is 3.8981254148483275 and perplexity is 49.30992675558655
At time: 213.54921007156372 and batch: 1200, loss is 3.9044113683700563 and perplexity is 49.620862904582616
At time: 214.34832215309143 and batch: 1250, loss is 3.9198310136795045 and perplexity is 50.39192851193761
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.352235223255018 and perplexity of 77.65183828148336
Finished 10 epochs...
Completing Train Step...
At time: 216.54805660247803 and batch: 50, loss is 4.036572012901306 and perplexity is 56.63187634016582
At time: 217.37621974945068 and batch: 100, loss is 4.056478757858276 and perplexity is 57.77052848613824
At time: 218.20775890350342 and batch: 150, loss is 3.9773228406906127 and perplexity is 53.37395221322937
At time: 219.02253985404968 and batch: 200, loss is 4.029150748252869 and perplexity is 56.21315185270107
At time: 219.83015155792236 and batch: 250, loss is 4.04649263381958 and perplexity is 57.196495774158464
At time: 220.6377341747284 and batch: 300, loss is 4.039366478919983 and perplexity is 56.790353520590486
At time: 221.44238948822021 and batch: 350, loss is 4.023420324325562 and perplexity is 55.8919478587889
At time: 222.24237060546875 and batch: 400, loss is 4.03875584602356 and perplexity is 56.75568604815807
At time: 223.042165517807 and batch: 450, loss is 3.959329752922058 and perplexity is 52.42217835976532
At time: 223.8396234512329 and batch: 500, loss is 3.9958517789840697 and perplexity is 54.3721339461888
At time: 224.63735103607178 and batch: 550, loss is 3.9715643548965454 and perplexity is 53.067482315793676
At time: 225.43799924850464 and batch: 600, loss is 4.001318211555481 and perplexity is 54.67016940337996
At time: 226.23649764060974 and batch: 650, loss is 4.017781572341919 and perplexity is 55.5776739160478
At time: 227.05830931663513 and batch: 700, loss is 3.983385705947876 and perplexity is 53.69853424805951
At time: 227.85381388664246 and batch: 750, loss is 3.974242148399353 and perplexity is 53.209776507352466
At time: 228.65130352973938 and batch: 800, loss is 4.001382756233215 and perplexity is 54.6736981857266
At time: 229.44834971427917 and batch: 850, loss is 4.036630563735962 and perplexity is 56.63519228086823
At time: 230.2458679676056 and batch: 900, loss is 3.984695706367493 and perplexity is 53.76892544664059
At time: 231.0429573059082 and batch: 950, loss is 3.9770593070983886 and perplexity is 53.35988823711789
At time: 231.8893084526062 and batch: 1000, loss is 3.953363575935364 and perplexity is 52.110349503684134
At time: 232.68681621551514 and batch: 1050, loss is 3.9500367069244384 and perplexity is 51.937273257406765
At time: 233.48506546020508 and batch: 1100, loss is 3.8975061559677124 and perplexity is 49.27940059831319
At time: 234.31247425079346 and batch: 1150, loss is 3.9001554584503175 and perplexity is 49.41012973070359
At time: 235.12321829795837 and batch: 1200, loss is 3.9122382545471193 and perplexity is 50.010763614336355
At time: 235.9451403617859 and batch: 1250, loss is 3.932317337989807 and perplexity is 51.0250831347889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.347670951898951 and perplexity of 77.29822183487316
Finished 11 epochs...
Completing Train Step...
At time: 238.23164820671082 and batch: 50, loss is 4.011240429878235 and perplexity is 55.2153188332648
At time: 239.02699661254883 and batch: 100, loss is 4.028528614044189 and perplexity is 56.178190604377725
At time: 239.8234739303589 and batch: 150, loss is 3.949596629142761 and perplexity is 51.9144218459733
At time: 240.62122559547424 and batch: 200, loss is 4.000455002784729 and perplexity is 54.62299799596785
At time: 241.41765546798706 and batch: 250, loss is 4.01863691329956 and perplexity is 55.625232113216086
At time: 242.2157461643219 and batch: 300, loss is 4.013053140640259 and perplexity is 55.315499007350276
At time: 243.01561498641968 and batch: 350, loss is 3.997604532241821 and perplexity is 54.46751844942369
At time: 243.81525254249573 and batch: 400, loss is 4.013567585945129 and perplexity is 55.34396312708977
At time: 244.6121265888214 and batch: 450, loss is 3.934992833137512 and perplexity is 51.16178328587799
At time: 245.4092299938202 and batch: 500, loss is 3.972850465774536 and perplexity is 53.135776889851165
At time: 246.21023321151733 and batch: 550, loss is 3.9490640354156494 and perplexity is 51.88677991216518
At time: 247.00637412071228 and batch: 600, loss is 3.9819307565689086 and perplexity is 53.62046240806378
At time: 247.79835414886475 and batch: 650, loss is 3.99878276348114 and perplexity is 54.53173160273202
At time: 248.59125638008118 and batch: 700, loss is 3.9652892398834227 and perplexity is 52.73552039971345
At time: 249.38539266586304 and batch: 750, loss is 3.95774320602417 and perplexity is 52.33907405717687
At time: 250.18433451652527 and batch: 800, loss is 3.987333827018738 and perplexity is 53.910961631175255
At time: 250.98960971832275 and batch: 850, loss is 4.023261952400207 and perplexity is 55.8830968442893
At time: 251.7953338623047 and batch: 900, loss is 3.9729789686203003 and perplexity is 53.14260542712724
At time: 252.691326379776 and batch: 950, loss is 3.9679783630371093 and perplexity is 52.87752355505523
At time: 253.52236604690552 and batch: 1000, loss is 3.9455734395980837 and perplexity is 51.705979868730715
At time: 254.35924649238586 and batch: 1050, loss is 3.9444816160202025 and perplexity is 51.6495568683787
At time: 255.1764371395111 and batch: 1100, loss is 3.8934395027160646 and perplexity is 49.07940529502728
At time: 256.0018661022186 and batch: 1150, loss is 3.8979875946044924 and perplexity is 49.30313131774282
At time: 256.81632137298584 and batch: 1200, loss is 3.9123445558547973 and perplexity is 50.016080106476565
At time: 257.62783312797546 and batch: 1250, loss is 3.9338232612609865 and perplexity is 51.10198088141609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346480543596031 and perplexity of 77.20626013664426
Finished 12 epochs...
Completing Train Step...
At time: 259.88882851600647 and batch: 50, loss is 3.992046284675598 and perplexity is 54.165614303809996
At time: 260.68869638442993 and batch: 100, loss is 4.008365521430969 and perplexity is 55.05680780825071
At time: 261.4860301017761 and batch: 150, loss is 3.9303039598464964 and perplexity is 50.92245369824591
At time: 262.2816698551178 and batch: 200, loss is 3.9808708810806275 and perplexity is 53.56366150054917
At time: 263.0840449333191 and batch: 250, loss is 3.9993710708618164 and perplexity is 54.56382246163399
At time: 263.8862977027893 and batch: 300, loss is 3.9944444561004637 and perplexity is 54.29566861618053
At time: 264.68468952178955 and batch: 350, loss is 3.9794696235656737 and perplexity is 53.488657579519746
At time: 265.48385787010193 and batch: 400, loss is 3.9958288717269896 and perplexity is 54.370888444004066
At time: 266.28474521636963 and batch: 450, loss is 3.917850332260132 and perplexity is 50.29221693647826
At time: 267.08468794822693 and batch: 500, loss is 3.9566446590423583 and perplexity is 52.28160869531958
At time: 267.8836622238159 and batch: 550, loss is 3.9335466051101684 and perplexity is 51.087845159543626
At time: 268.7007064819336 and batch: 600, loss is 3.966859073638916 and perplexity is 52.81837141389512
At time: 269.49813079833984 and batch: 650, loss is 3.9854030227661132 and perplexity is 53.80697054271921
At time: 270.2950007915497 and batch: 700, loss is 3.9523061323165893 and perplexity is 52.05527487142056
At time: 271.0936396121979 and batch: 750, loss is 3.9456842708587647 and perplexity is 51.71171082524302
At time: 271.8913719654083 and batch: 800, loss is 3.9765428113937378 and perplexity is 53.33233520016791
At time: 272.6907813549042 and batch: 850, loss is 4.012837677001953 and perplexity is 55.30358181258665
At time: 273.57482385635376 and batch: 900, loss is 3.963288607597351 and perplexity is 52.63012148237265
At time: 274.3877182006836 and batch: 950, loss is 3.959686532020569 and perplexity is 52.4408848341436
At time: 275.1852557659149 and batch: 1000, loss is 3.9383665895462037 and perplexity is 51.334682175489334
At time: 275.9853277206421 and batch: 1050, loss is 3.9386656141281127 and perplexity is 51.35003480265635
At time: 276.78234910964966 and batch: 1100, loss is 3.888444662094116 and perplexity is 48.83487269682792
At time: 277.5817391872406 and batch: 1150, loss is 3.894119391441345 and perplexity is 49.112785175347156
At time: 278.37938690185547 and batch: 1200, loss is 3.9096601152420045 and perplexity is 49.88199496207024
At time: 279.1783788204193 and batch: 1250, loss is 3.9317689180374145 and perplexity is 50.99710763298814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346170021669708 and perplexity of 77.18228962190022
Finished 13 epochs...
Completing Train Step...
At time: 281.3713068962097 and batch: 50, loss is 3.9762750387191774 and perplexity is 53.31805616998318
At time: 282.19315671920776 and batch: 100, loss is 3.992083053588867 and perplexity is 54.16760595119964
At time: 282.9889142513275 and batch: 150, loss is 3.9150511264801025 and perplexity is 50.15163552208745
At time: 283.78145027160645 and batch: 200, loss is 3.9655211305618288 and perplexity is 52.74775069330577
At time: 284.5850214958191 and batch: 250, loss is 3.98398184299469 and perplexity is 53.730555477256594
At time: 285.42031383514404 and batch: 300, loss is 3.9796136379241944 and perplexity is 53.49636126893682
At time: 286.2274284362793 and batch: 350, loss is 3.9650789070129395 and perplexity is 52.72442955275708
At time: 287.0338728427887 and batch: 400, loss is 3.981555366516113 and perplexity is 53.6003375974127
At time: 287.84215021133423 and batch: 450, loss is 3.9040942287445066 and perplexity is 49.60512865781004
At time: 288.6517996788025 and batch: 500, loss is 3.943502178192139 and perplexity is 51.59899410416313
At time: 289.4586296081543 and batch: 550, loss is 3.920855150222778 and perplexity is 50.44356316336633
At time: 290.2729666233063 and batch: 600, loss is 3.9546155548095703 and perplexity is 52.17563141765166
At time: 291.10939168930054 and batch: 650, loss is 3.9743079710006715 and perplexity is 53.213279028529016
At time: 291.9137849807739 and batch: 700, loss is 3.941486134529114 and perplexity is 51.49507306895155
At time: 292.7104969024658 and batch: 750, loss is 3.9354151916503906 and perplexity is 51.18339646451652
At time: 293.5086097717285 and batch: 800, loss is 3.966776576042175 and perplexity is 52.814014204921804
At time: 294.3620254993439 and batch: 850, loss is 4.003628182411194 and perplexity is 54.796601872818506
At time: 295.17591738700867 and batch: 900, loss is 3.954601626396179 and perplexity is 52.17490469894936
At time: 295.9726676940918 and batch: 950, loss is 3.9518109226226805 and perplexity is 52.02950297645793
At time: 296.7694013118744 and batch: 1000, loss is 3.931368799209595 and perplexity is 50.97670681170837
At time: 297.5685968399048 and batch: 1050, loss is 3.932626886367798 and perplexity is 51.04088031137917
At time: 298.3665759563446 and batch: 1100, loss is 3.8829344367980956 and perplexity is 48.56652156255584
At time: 299.16733956336975 and batch: 1150, loss is 3.889282536506653 and perplexity is 48.87580733374773
At time: 299.96603536605835 and batch: 1200, loss is 3.9054823207855223 and perplexity is 49.674032953782195
At time: 300.76407384872437 and batch: 1250, loss is 3.9279786682128908 and perplexity is 50.804181704428764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346452921846487 and perplexity of 77.20412759411597
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 303.0000629425049 and batch: 50, loss is 3.97942307472229 and perplexity is 53.486167802323834
At time: 303.80824613571167 and batch: 100, loss is 4.008659014701843 and perplexity is 55.07296898234015
At time: 304.62235736846924 and batch: 150, loss is 3.9366833400726318 and perplexity is 51.248345781970876
At time: 305.42160511016846 and batch: 200, loss is 3.9882081747055054 and perplexity is 53.95811916880659
At time: 306.21535778045654 and batch: 250, loss is 4.005648350715637 and perplexity is 54.90741212110341
At time: 307.0111699104309 and batch: 300, loss is 3.998897614479065 and perplexity is 54.53799498619621
At time: 307.80922627449036 and batch: 350, loss is 3.9829820156097413 and perplexity is 53.67686104353754
At time: 308.60735630989075 and batch: 400, loss is 3.9928099584579466 and perplexity is 54.20699496200942
At time: 309.4073305130005 and batch: 450, loss is 3.9142313814163208 and perplexity is 50.11054081232163
At time: 310.20555996894836 and batch: 500, loss is 3.948999972343445 and perplexity is 51.883455992108615
At time: 311.0031638145447 and batch: 550, loss is 3.9243362236022947 and perplexity is 50.619466897533925
At time: 311.8013348579407 and batch: 600, loss is 3.956045985221863 and perplexity is 52.250318432165564
At time: 312.5995271205902 and batch: 650, loss is 3.976575574874878 and perplexity is 53.334082581751396
At time: 313.39735317230225 and batch: 700, loss is 3.9411926889419555 and perplexity is 51.47996428391063
At time: 314.201735496521 and batch: 750, loss is 3.9302255964279174 and perplexity is 50.91846339704055
At time: 315.0360987186432 and batch: 800, loss is 3.960670189857483 and perplexity is 52.492494100253595
At time: 315.8515763282776 and batch: 850, loss is 3.9903735065460206 and perplexity is 54.0750829893392
At time: 316.65826439857483 and batch: 900, loss is 3.9332328987121583 and perplexity is 51.07182108921463
At time: 317.4660687446594 and batch: 950, loss is 3.924710063934326 and perplexity is 50.63839403348911
At time: 318.2866349220276 and batch: 1000, loss is 3.9003429508209226 and perplexity is 49.419394621579734
At time: 319.0847306251526 and batch: 1050, loss is 3.902530241012573 and perplexity is 49.52760748203396
At time: 319.9046723842621 and batch: 1100, loss is 3.8497019481658934 and perplexity is 46.9790589499974
At time: 320.7015001773834 and batch: 1150, loss is 3.8494007110595705 and perplexity is 46.96490924553654
At time: 321.5024518966675 and batch: 1200, loss is 3.868604636192322 and perplexity is 47.8755356616028
At time: 322.3003759384155 and batch: 1250, loss is 3.890610432624817 and perplexity is 48.94075243921188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.330355233519617 and perplexity of 75.97126931770471
Finished 15 epochs...
Completing Train Step...
At time: 324.5834369659424 and batch: 50, loss is 3.971546621322632 and perplexity is 53.066541248017906
At time: 325.42593574523926 and batch: 100, loss is 3.991164836883545 and perplexity is 54.117891178488314
At time: 326.25130796432495 and batch: 150, loss is 3.916597328186035 and perplexity is 50.229240047150526
At time: 327.05397510528564 and batch: 200, loss is 3.968016619682312 and perplexity is 52.879546510408574
At time: 327.8790717124939 and batch: 250, loss is 3.985814480781555 and perplexity is 53.829114407358546
At time: 328.6782314777374 and batch: 300, loss is 3.981124691963196 and perplexity is 53.57725826617878
At time: 329.5034210681915 and batch: 350, loss is 3.9663311529159544 and perplexity is 52.7904948600253
At time: 330.3080997467041 and batch: 400, loss is 3.977593698501587 and perplexity is 53.38841092312775
At time: 331.1078109741211 and batch: 450, loss is 3.9002781534194946 and perplexity is 49.41619247697456
At time: 331.9142107963562 and batch: 500, loss is 3.9358561658859252 and perplexity is 51.205972000893645
At time: 332.71185636520386 and batch: 550, loss is 3.912190718650818 and perplexity is 50.008386364366025
At time: 333.5367548465729 and batch: 600, loss is 3.944866738319397 and perplexity is 51.66945209527408
At time: 334.3455150127411 and batch: 650, loss is 3.967075433731079 and perplexity is 52.82980043794988
At time: 335.1435298919678 and batch: 700, loss is 3.9323093461990357 and perplexity is 51.02467535462984
At time: 335.9847707748413 and batch: 750, loss is 3.922642478942871 and perplexity is 50.53380301266627
At time: 336.78290843963623 and batch: 800, loss is 3.9541435718536375 and perplexity is 52.15101121952149
At time: 337.59686946868896 and batch: 850, loss is 3.9856828451156616 and perplexity is 53.82202904239268
At time: 338.40124702453613 and batch: 900, loss is 3.9299837446212766 and perplexity is 50.906150163725485
At time: 339.19800329208374 and batch: 950, loss is 3.9232209014892576 and perplexity is 50.56304135892761
At time: 339.99427676200867 and batch: 1000, loss is 3.9008804321289063 and perplexity is 49.44596376200926
At time: 340.7915618419647 and batch: 1050, loss is 3.904398937225342 and perplexity is 49.62024606428901
At time: 341.58754205703735 and batch: 1100, loss is 3.8530379152297973 and perplexity is 47.13604124164133
At time: 342.3867144584656 and batch: 1150, loss is 3.8546000051498415 and perplexity is 47.20972951540647
At time: 343.18367433547974 and batch: 1200, loss is 3.8747326612472532 and perplexity is 48.16981891028921
At time: 343.981614112854 and batch: 1250, loss is 3.8967989444732667 and perplexity is 49.24456196036531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.329232543054288 and perplexity of 75.88602495847118
Finished 16 epochs...
Completing Train Step...
At time: 346.23009037971497 and batch: 50, loss is 3.966873092651367 and perplexity is 52.8191118804919
At time: 347.0330812931061 and batch: 100, loss is 3.985379209518433 and perplexity is 53.80568923925881
At time: 347.83360743522644 and batch: 150, loss is 3.910005793571472 and perplexity is 49.899241067389944
At time: 348.63120126724243 and batch: 200, loss is 3.9610065269470214 and perplexity is 52.51015224231948
At time: 349.4290289878845 and batch: 250, loss is 3.9785359477996827 and perplexity is 53.4387398233083
At time: 350.25430607795715 and batch: 300, loss is 3.974165692329407 and perplexity is 53.20570845247372
At time: 351.0570487976074 and batch: 350, loss is 3.9597719144821166 and perplexity is 52.445362557133265
At time: 351.8720180988312 and batch: 400, loss is 3.97122980594635 and perplexity is 53.04973161470019
At time: 352.6843738555908 and batch: 450, loss is 3.8942453908920287 and perplexity is 49.11897374917106
At time: 353.48539566993713 and batch: 500, loss is 3.929934763908386 and perplexity is 50.90365680526369
At time: 354.2829780578613 and batch: 550, loss is 3.906666564941406 and perplexity is 49.732893983042786
At time: 355.08082723617554 and batch: 600, loss is 3.9399004364013672 and perplexity is 51.4134821343903
At time: 355.9510052204132 and batch: 650, loss is 3.962766103744507 and perplexity is 52.602629224153354
At time: 356.74419474601746 and batch: 700, loss is 3.9283027410507203 and perplexity is 50.820648627864514
At time: 357.5355751514435 and batch: 750, loss is 3.9192831897735596 and perplexity is 50.36433016903832
At time: 358.3344328403473 and batch: 800, loss is 3.9513618421554564 and perplexity is 52.00614278864644
At time: 359.1295711994171 and batch: 850, loss is 3.9836499881744385 and perplexity is 53.71272769170857
At time: 359.9373528957367 and batch: 900, loss is 3.9285356426239013 and perplexity is 50.83248621532288
At time: 360.7527048587799 and batch: 950, loss is 3.9227105474472044 and perplexity is 50.53724289012795
At time: 361.5710418224335 and batch: 1000, loss is 3.901230840682983 and perplexity is 49.463293086670525
At time: 362.3984031677246 and batch: 1050, loss is 3.905435800552368 and perplexity is 49.67172215993723
At time: 363.20950150489807 and batch: 1100, loss is 3.85460738658905 and perplexity is 47.21007799244108
At time: 364.0220835208893 and batch: 1150, loss is 3.856885290145874 and perplexity is 47.31774057295184
At time: 364.8208062648773 and batch: 1200, loss is 3.8772588968276978 and perplexity is 48.291661056908595
At time: 365.61933970451355 and batch: 1250, loss is 3.899282808303833 and perplexity is 49.36703078164343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328803514912181 and perplexity of 75.85347470115887
Finished 17 epochs...
Completing Train Step...
At time: 367.83893966674805 and batch: 50, loss is 3.9628136014938353 and perplexity is 52.60512778998791
At time: 368.673748254776 and batch: 100, loss is 3.980939702987671 and perplexity is 53.56734798073572
At time: 369.4724552631378 and batch: 150, loss is 3.9051430892944334 and perplexity is 49.65718481538601
At time: 370.268794298172 and batch: 200, loss is 3.9559344339370726 and perplexity is 52.24449016709503
At time: 371.07200264930725 and batch: 250, loss is 3.9734349060058594 and perplexity is 53.166840652160225
At time: 371.8885705471039 and batch: 300, loss is 3.969356598854065 and perplexity is 52.950451496336996
At time: 372.7005958557129 and batch: 350, loss is 3.955241460800171 and perplexity is 52.20829868017052
At time: 373.5056290626526 and batch: 400, loss is 3.9667555046081544 and perplexity is 52.812901349630884
At time: 374.31316614151 and batch: 450, loss is 3.889961814880371 and perplexity is 48.909018891336935
At time: 375.12324118614197 and batch: 500, loss is 3.925785994529724 and perplexity is 50.692906751623426
At time: 375.9354581832886 and batch: 550, loss is 3.9028036117553713 and perplexity is 49.54114873168675
At time: 376.7864944934845 and batch: 600, loss is 3.9364143085479735 and perplexity is 51.234560215828026
At time: 377.59215354919434 and batch: 650, loss is 3.95975914478302 and perplexity is 52.444692849910375
At time: 378.39022302627563 and batch: 700, loss is 3.925484747886658 and perplexity is 50.67763798358526
At time: 379.18459582328796 and batch: 750, loss is 3.916836290359497 and perplexity is 50.24124436975591
At time: 379.97943329811096 and batch: 800, loss is 3.949340333938599 and perplexity is 51.90111813353899
At time: 380.7765552997589 and batch: 850, loss is 3.982079949378967 and perplexity is 53.62846279231214
At time: 381.5826852321625 and batch: 900, loss is 3.927294487953186 and perplexity is 50.769434374270276
At time: 382.3979754447937 and batch: 950, loss is 3.92201584815979 and perplexity is 50.502146895496985
At time: 383.195689201355 and batch: 1000, loss is 3.9010534954071043 and perplexity is 49.454521783109804
At time: 383.9953022003174 and batch: 1050, loss is 3.905753092765808 and perplexity is 49.68748511120534
At time: 384.8109984397888 and batch: 1100, loss is 3.855170192718506 and perplexity is 47.23665559202374
At time: 385.61051654815674 and batch: 1150, loss is 3.8578643083572386 and perplexity is 47.36408818657035
At time: 386.40805411338806 and batch: 1200, loss is 3.878391885757446 and perplexity is 48.34640598112561
At time: 387.20782351493835 and batch: 1250, loss is 3.9004090881347655 and perplexity is 49.42266319567791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328583877452099 and perplexity of 75.83681626611222
Finished 18 epochs...
Completing Train Step...
At time: 389.4463937282562 and batch: 50, loss is 3.9591420555114745 and perplexity is 52.41233977599728
At time: 390.2456204891205 and batch: 100, loss is 3.97714608669281 and perplexity is 53.364518987501896
At time: 391.04426169395447 and batch: 150, loss is 3.9010941934585572 and perplexity is 49.456534526739006
At time: 391.8430371284485 and batch: 200, loss is 3.95177592754364 and perplexity is 52.027682231747576
At time: 392.6559805870056 and batch: 250, loss is 3.9693206548690796 and perplexity is 52.94854828030824
At time: 393.4653377532959 and batch: 300, loss is 3.96548460483551 and perplexity is 52.745824078585734
At time: 394.2661027908325 and batch: 350, loss is 3.951597170829773 and perplexity is 52.01838276543749
At time: 395.0632309913635 and batch: 400, loss is 3.963133120536804 and perplexity is 52.62193881565301
At time: 395.8599593639374 and batch: 450, loss is 3.886475839614868 and perplexity is 48.73882008798602
At time: 396.68491888046265 and batch: 500, loss is 3.9224679613113405 and perplexity is 50.52498474254646
At time: 397.52542090415955 and batch: 550, loss is 3.8996939611434938 and perplexity is 49.38733234977262
At time: 398.3328869342804 and batch: 600, loss is 3.933579206466675 and perplexity is 51.08951071974643
At time: 399.13152623176575 and batch: 650, loss is 3.957309603691101 and perplexity is 52.31638463200435
At time: 399.9299738407135 and batch: 700, loss is 3.923164110183716 and perplexity is 50.56016989933441
At time: 400.7303099632263 and batch: 750, loss is 3.914792580604553 and perplexity is 50.138670699644074
At time: 401.5254068374634 and batch: 800, loss is 3.9476253986358643 and perplexity is 51.812187350865216
At time: 402.3223571777344 and batch: 850, loss is 3.9805932092666625 and perplexity is 53.548790446229575
At time: 403.1205017566681 and batch: 900, loss is 3.926058864593506 and perplexity is 50.70674121574174
At time: 403.9218144416809 and batch: 950, loss is 3.9211739110946655 and perplexity is 50.45964516056094
At time: 404.7222526073456 and batch: 1000, loss is 3.9005756473541258 and perplexity is 49.43089568145766
At time: 405.5218584537506 and batch: 1050, loss is 3.905662932395935 and perplexity is 49.683005471115685
At time: 406.32961773872375 and batch: 1100, loss is 3.855235176086426 and perplexity is 47.239725288731925
At time: 407.14379239082336 and batch: 1150, loss is 3.8582239532470703 and perplexity is 47.381125502356326
At time: 407.9416527748108 and batch: 1200, loss is 3.878855562210083 and perplexity is 48.36882826909043
At time: 408.7401702404022 and batch: 1250, loss is 3.900880274772644 and perplexity is 49.44595598137783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328439531535127 and perplexity of 75.82587032134833
Finished 19 epochs...
Completing Train Step...
At time: 410.99871301651 and batch: 50, loss is 3.9558166217803956 and perplexity is 52.23833549358876
At time: 411.7980375289917 and batch: 100, loss is 3.973787202835083 and perplexity is 53.18557446127884
At time: 412.5959930419922 and batch: 150, loss is 3.8975592184066774 and perplexity is 49.282015552876985
At time: 413.39266562461853 and batch: 200, loss is 3.948134036064148 and perplexity is 51.83854767194777
At time: 414.1948802471161 and batch: 250, loss is 3.965743074417114 and perplexity is 52.759459031701084
At time: 414.9970471858978 and batch: 300, loss is 3.9621355867385866 and perplexity is 52.56947282580636
At time: 415.798597574234 and batch: 350, loss is 3.948437461853027 and perplexity is 51.85427921072606
At time: 416.60065627098083 and batch: 400, loss is 3.95999089717865 and perplexity is 52.45684844160577
At time: 417.4005661010742 and batch: 450, loss is 3.8834350728988647 and perplexity is 48.59084180382626
At time: 418.27139616012573 and batch: 500, loss is 3.9195491743087767 and perplexity is 50.37772808372992
At time: 419.07057452201843 and batch: 550, loss is 3.896982526779175 and perplexity is 49.25360322048573
At time: 419.868545293808 and batch: 600, loss is 3.931106934547424 and perplexity is 50.963359561263424
At time: 420.66700887680054 and batch: 650, loss is 3.955129885673523 and perplexity is 52.2024738575919
At time: 421.46517276763916 and batch: 700, loss is 3.9211166143417358 and perplexity is 50.4567540695651
At time: 422.27093410491943 and batch: 750, loss is 3.91289813041687 and perplexity is 50.043775401116136
At time: 423.1044340133667 and batch: 800, loss is 3.9459968996047974 and perplexity is 51.72787991988371
At time: 423.90627908706665 and batch: 850, loss is 3.9791814613342287 and perplexity is 53.473246389162775
At time: 424.7033612728119 and batch: 900, loss is 3.9248269033432006 and perplexity is 50.64431093917149
At time: 425.50133895874023 and batch: 950, loss is 3.9202226972579957 and perplexity is 50.41167006879366
At time: 426.29883193969727 and batch: 1000, loss is 3.8999143409729005 and perplexity is 49.39821752104275
At time: 427.09785771369934 and batch: 1050, loss is 3.905295104980469 and perplexity is 49.66473406018959
At time: 427.8984889984131 and batch: 1100, loss is 3.8549895238876344 and perplexity is 47.22812217156841
At time: 428.6968924999237 and batch: 1150, loss is 3.858192038536072 and perplexity is 47.379613371558875
At time: 429.4938611984253 and batch: 1200, loss is 3.8788929319381715 and perplexity is 48.370635832824675
At time: 430.2890865802765 and batch: 1250, loss is 3.9009680223464964 and perplexity is 49.450294934415524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328353102189781 and perplexity of 75.81931702421882
Finished 20 epochs...
Completing Train Step...
At time: 432.4887135028839 and batch: 50, loss is 3.9527168321609496 and perplexity is 52.076658355504605
At time: 433.3133234977722 and batch: 100, loss is 3.9706858730316164 and perplexity is 53.020883965861266
At time: 434.1110143661499 and batch: 150, loss is 3.894353995323181 and perplexity is 49.1243085770616
At time: 434.90885972976685 and batch: 200, loss is 3.944859504699707 and perplexity is 51.669078339459844
At time: 435.7056863307953 and batch: 250, loss is 3.9625780296325686 and perplexity is 52.592736961644796
At time: 436.5044615268707 and batch: 300, loss is 3.9591555547714234 and perplexity is 52.413047308572025
At time: 437.3005745410919 and batch: 350, loss is 3.945603036880493 and perplexity is 51.707510247866566
At time: 438.0968961715698 and batch: 400, loss is 3.957163543701172 and perplexity is 52.308743859411045
At time: 438.922331571579 and batch: 450, loss is 3.8806796216964723 and perplexity is 48.457136404301224
At time: 439.7141463756561 and batch: 500, loss is 3.9169065380096435 and perplexity is 50.244773823079775
At time: 440.5116662979126 and batch: 550, loss is 3.8945192813873293 and perplexity is 49.13242881174264
At time: 441.3086724281311 and batch: 600, loss is 3.9288396406173707 and perplexity is 50.847941538209945
At time: 442.1125416755676 and batch: 650, loss is 3.953128790855408 and perplexity is 52.09811620726332
At time: 442.9160017967224 and batch: 700, loss is 3.91922221660614 and perplexity is 50.36125938992147
At time: 443.71409010887146 and batch: 750, loss is 3.911129503250122 and perplexity is 49.95534484380931
At time: 444.5141427516937 and batch: 800, loss is 3.944470753669739 and perplexity is 51.64899583583778
At time: 445.3484468460083 and batch: 850, loss is 3.977767643928528 and perplexity is 53.39769840079319
At time: 446.1566836833954 and batch: 900, loss is 3.9235771179199217 and perplexity is 50.58105595340079
At time: 446.9547326564789 and batch: 950, loss is 3.919189715385437 and perplexity is 50.359622614113896
At time: 447.7496347427368 and batch: 1000, loss is 3.899118709564209 and perplexity is 49.3589303787643
At time: 448.5508017539978 and batch: 1050, loss is 3.9047451257705688 and perplexity is 49.63742699883759
At time: 449.3470141887665 and batch: 1100, loss is 3.854539546966553 and perplexity is 47.206875387204995
At time: 450.1450629234314 and batch: 1150, loss is 3.8579184770584107 and perplexity is 47.36665390719987
At time: 450.94416546821594 and batch: 1200, loss is 3.878686203956604 and perplexity is 48.360637302435435
At time: 451.7417531013489 and batch: 1250, loss is 3.900819182395935 and perplexity is 49.44293530267944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328312560589644 and perplexity of 75.81624325009368
Finished 21 epochs...
Completing Train Step...
At time: 453.9782657623291 and batch: 50, loss is 3.9498075246810913 and perplexity is 51.925371520493776
At time: 454.7773926258087 and batch: 100, loss is 3.96779860496521 and perplexity is 52.86801924763775
At time: 455.57808327674866 and batch: 150, loss is 3.8914017724990844 and perplexity is 48.97949653594613
At time: 456.3867619037628 and batch: 200, loss is 3.9418474769592287 and perplexity is 51.51368378601159
At time: 457.19469928741455 and batch: 250, loss is 3.959650573730469 and perplexity is 52.438999183496215
At time: 458.0033633708954 and batch: 300, loss is 3.956354413032532 and perplexity is 52.26643636896857
At time: 458.8598530292511 and batch: 350, loss is 3.9429643869400026 and perplexity is 51.5712520768933
At time: 459.6700143814087 and batch: 400, loss is 3.954542121887207 and perplexity is 52.17180014923285
At time: 460.46534538269043 and batch: 450, loss is 3.8781601715087892 and perplexity is 48.33520472778355
At time: 461.27047395706177 and batch: 500, loss is 3.914470510482788 and perplexity is 50.12252513200874
At time: 462.07337951660156 and batch: 550, loss is 3.8922513246536257 and perplexity is 49.021124852966075
At time: 462.8744149208069 and batch: 600, loss is 3.926740412712097 and perplexity is 50.74131207933253
At time: 463.67093801498413 and batch: 650, loss is 3.9512472772598266 and perplexity is 52.00018505160603
At time: 464.46775817871094 and batch: 700, loss is 3.9174383974075315 and perplexity is 50.27150408597299
At time: 465.26953315734863 and batch: 750, loss is 3.9094263172149657 and perplexity is 49.870334013269876
At time: 466.071044921875 and batch: 800, loss is 3.942961778640747 and perplexity is 51.57111756381032
At time: 466.8695418834686 and batch: 850, loss is 3.976389417648315 and perplexity is 53.3241549809327
At time: 467.6729815006256 and batch: 900, loss is 3.922334003448486 and perplexity is 50.51821697687743
At time: 468.4823136329651 and batch: 950, loss is 3.9181371116638184 and perplexity is 50.30664177673606
At time: 469.2806315422058 and batch: 1000, loss is 3.8982488489151 and perplexity is 49.31601365603597
At time: 470.07832431793213 and batch: 1050, loss is 3.9040743494033814 and perplexity is 49.60414255033751
At time: 470.8752193450928 and batch: 1100, loss is 3.8539564752578737 and perplexity is 47.17935841667467
At time: 471.67306756973267 and batch: 1150, loss is 3.8574896097183227 and perplexity is 47.34634425171507
At time: 472.4715955257416 and batch: 1200, loss is 3.878313579559326 and perplexity is 48.342620306103306
At time: 473.2694399356842 and batch: 1250, loss is 3.900519061088562 and perplexity is 49.42809865080507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328300086251141 and perplexity of 75.81529749851013
Finished 22 epochs...
Completing Train Step...
At time: 475.53262519836426 and batch: 50, loss is 3.947048616409302 and perplexity is 51.7823116188058
At time: 476.3552277088165 and batch: 100, loss is 3.9650774145126344 and perplexity is 52.724350861588604
At time: 477.1517219543457 and batch: 150, loss is 3.888625659942627 and perplexity is 48.84371250368722
At time: 477.9498643875122 and batch: 200, loss is 3.9390277528762816 and perplexity is 51.36863400752134
At time: 478.74607610702515 and batch: 250, loss is 3.956885046958923 and perplexity is 52.294178073011246
At time: 479.58867835998535 and batch: 300, loss is 3.953755774497986 and perplexity is 52.13079111618071
At time: 480.4109773635864 and batch: 350, loss is 3.940497803688049 and perplexity is 51.44420404192948
At time: 481.2060308456421 and batch: 400, loss is 3.9520942306518556 and perplexity is 52.04424544063578
At time: 482.0212550163269 and batch: 450, loss is 3.875755114555359 and perplexity is 48.21909548819207
At time: 482.8407552242279 and batch: 500, loss is 3.9121727085113527 and perplexity is 50.00748571446362
At time: 483.64649510383606 and batch: 550, loss is 3.8901043128967285 and perplexity is 48.915988826100076
At time: 484.4409384727478 and batch: 600, loss is 3.9247412157058714 and perplexity is 50.639971533742305
At time: 485.23885464668274 and batch: 650, loss is 3.949445323944092 and perplexity is 51.906567518277384
At time: 486.03644919395447 and batch: 700, loss is 3.9157241058349608 and perplexity is 50.185397896822536
At time: 486.8335003852844 and batch: 750, loss is 3.907806224822998 and perplexity is 49.78960487653767
At time: 487.63079476356506 and batch: 800, loss is 3.9415043640136718 and perplexity is 51.49601180614719
At time: 488.4286231994629 and batch: 850, loss is 3.9750072717666627 and perplexity is 53.25050412956841
At time: 489.2262673377991 and batch: 900, loss is 3.9210740232467653 and perplexity is 50.45460510692429
At time: 490.0240306854248 and batch: 950, loss is 3.9170277166366576 and perplexity is 50.25086278470485
At time: 490.8213698863983 and batch: 1000, loss is 3.8973205137252807 and perplexity is 49.27025310898691
At time: 491.6236934661865 and batch: 1050, loss is 3.9033190536499025 and perplexity is 49.566690897431904
At time: 492.4214496612549 and batch: 1100, loss is 3.853274116516113 and perplexity is 47.14717615020253
At time: 493.2178566455841 and batch: 1150, loss is 3.8569496631622315 and perplexity is 47.320786656681484
At time: 494.0146141052246 and batch: 1200, loss is 3.877830457687378 and perplexity is 48.31927056972449
At time: 494.8096704483032 and batch: 1250, loss is 3.900107908248901 and perplexity is 49.40778032494068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328305432396213 and perplexity of 75.81570281917271
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 497.0336136817932 and batch: 50, loss is 3.9490239000320435 and perplexity is 51.884697458139655
At time: 497.82978105545044 and batch: 100, loss is 3.9725173139572143 and perplexity is 53.11807755766156
At time: 498.62469720840454 and batch: 150, loss is 3.8972655820846556 and perplexity is 49.26754668748438
At time: 499.46508288383484 and batch: 200, loss is 3.949672269821167 and perplexity is 51.918348836579014
At time: 500.33270955085754 and batch: 250, loss is 3.9688259601593017 and perplexity is 52.9223613913753
At time: 501.13564801216125 and batch: 300, loss is 3.9655510282516477 and perplexity is 52.749327752769744
At time: 501.9332981109619 and batch: 350, loss is 3.952918915748596 and perplexity is 52.08718325687682
At time: 502.72948694229126 and batch: 400, loss is 3.961479229927063 and perplexity is 52.534979815338424
At time: 503.5303735733032 and batch: 450, loss is 3.885013556480408 and perplexity is 48.66760221641436
At time: 504.32649660110474 and batch: 500, loss is 3.9179940891265868 and perplexity is 50.29944730768742
At time: 505.1240589618683 and batch: 550, loss is 3.8940700006484987 and perplexity is 49.11035951585157
At time: 505.92668199539185 and batch: 600, loss is 3.9270749855041505 and perplexity is 50.75829158206879
At time: 506.73726534843445 and batch: 650, loss is 3.951520848274231 and perplexity is 52.014412741032785
At time: 507.53141713142395 and batch: 700, loss is 3.9193192148208618 and perplexity is 50.36614457909691
At time: 508.3292534351349 and batch: 750, loss is 3.90941819190979 and perplexity is 49.86992880323303
At time: 509.1274631023407 and batch: 800, loss is 3.944413175582886 and perplexity is 51.646022071082335
At time: 509.9226098060608 and batch: 850, loss is 3.9769303369522095 and perplexity is 53.35300684828825
At time: 510.7568917274475 and batch: 900, loss is 3.9201224184036256 and perplexity is 50.406615097729876
At time: 511.5637319087982 and batch: 950, loss is 3.9145866203308106 and perplexity is 50.128345188660724
At time: 512.3632276058197 and batch: 1000, loss is 3.890694432258606 and perplexity is 48.944863617160436
At time: 513.1594803333282 and batch: 1050, loss is 3.8947526836395263 and perplexity is 49.14389776967145
At time: 513.9548513889313 and batch: 1100, loss is 3.842165961265564 and perplexity is 46.626356028655174
At time: 514.7533962726593 and batch: 1150, loss is 3.844812512397766 and perplexity is 46.74991849906957
At time: 515.5744886398315 and batch: 1200, loss is 3.867200813293457 and perplexity is 47.80837404088589
At time: 516.3838088512421 and batch: 1250, loss is 3.8911388683319093 and perplexity is 48.96662131474951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326112621892108 and perplexity of 75.64963549328641
Finished 24 epochs...
Completing Train Step...
At time: 518.5816049575806 and batch: 50, loss is 3.948405079841614 and perplexity is 51.8526000920516
At time: 519.4067511558533 and batch: 100, loss is 3.968442463874817 and perplexity is 52.90206975354809
At time: 520.203727722168 and batch: 150, loss is 3.8925882625579833 and perplexity is 49.03764471097016
At time: 521.0283539295197 and batch: 200, loss is 3.943594036102295 and perplexity is 51.60373409762731
At time: 521.8411722183228 and batch: 250, loss is 3.9627258920669557 and perplexity is 52.600514026716745
At time: 522.6513605117798 and batch: 300, loss is 3.9589630365371704 and perplexity is 52.402957812489504
At time: 523.4511578083038 and batch: 350, loss is 3.947020468711853 and perplexity is 51.780854086478335
At time: 524.2492527961731 and batch: 400, loss is 3.957039546966553 and perplexity is 52.30225814809227
At time: 525.0466849803925 and batch: 450, loss is 3.880936932563782 and perplexity is 48.469606556380775
At time: 525.8762402534485 and batch: 500, loss is 3.9148462915420534 and perplexity is 50.14136376697531
At time: 526.6844608783722 and batch: 550, loss is 3.89043523311615 and perplexity is 48.93217879450184
At time: 527.4828011989594 and batch: 600, loss is 3.923789520263672 and perplexity is 50.59180062929143
At time: 528.2993440628052 and batch: 650, loss is 3.948660664558411 and perplexity is 51.8658545179035
At time: 529.1067388057709 and batch: 700, loss is 3.916269164085388 and perplexity is 50.212759318103984
At time: 529.9048545360565 and batch: 750, loss is 3.9072838544845583 and perplexity is 49.76360305566879
At time: 530.7024776935577 and batch: 800, loss is 3.9425643920898437 and perplexity is 51.550627966690215
At time: 531.5165722370148 and batch: 850, loss is 3.975088267326355 and perplexity is 53.25481735862812
At time: 532.3422100543976 and batch: 900, loss is 3.9192394113540647 and perplexity is 50.362125346526774
At time: 533.1449973583221 and batch: 950, loss is 3.914115796089172 and perplexity is 50.104749103792955
At time: 533.9695448875427 and batch: 1000, loss is 3.89076105594635 and perplexity is 48.948124613099324
At time: 534.765721321106 and batch: 1050, loss is 3.8954612016677856 and perplexity is 49.17872944519811
At time: 535.5630280971527 and batch: 1100, loss is 3.8437595844268797 and perplexity is 46.70072010797164
At time: 536.3617095947266 and batch: 1150, loss is 3.8472801303863524 and perplexity is 46.86542188943505
At time: 537.1594741344452 and batch: 1200, loss is 3.8699467897415163 and perplexity is 47.93983512193276
At time: 537.9573004245758 and batch: 1250, loss is 3.893796248435974 and perplexity is 49.09691728628022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325751311587592 and perplexity of 75.62230743770112
Finished 25 epochs...
Completing Train Step...
At time: 540.1935210227966 and batch: 50, loss is 3.9480052947998048 and perplexity is 51.83187434135449
At time: 541.0068233013153 and batch: 100, loss is 3.9672918844223024 and perplexity is 52.84123672242306
At time: 541.8378896713257 and batch: 150, loss is 3.8909948873519897 and perplexity is 48.959571560156725
At time: 542.6466245651245 and batch: 200, loss is 3.941472063064575 and perplexity is 51.494348462955074
At time: 543.4406073093414 and batch: 250, loss is 3.9604999351501466 and perplexity is 52.4835577667812
At time: 544.2335543632507 and batch: 300, loss is 3.956660604476929 and perplexity is 52.28244235493679
At time: 545.0293128490448 and batch: 350, loss is 3.9449723815917968 and perplexity is 51.67491091361517
At time: 545.8297138214111 and batch: 400, loss is 3.955282573699951 and perplexity is 52.21044515884553
At time: 546.6435446739197 and batch: 450, loss is 3.879231743812561 and perplexity is 48.38702715524715
At time: 547.4525601863861 and batch: 500, loss is 3.91349326133728 and perplexity is 50.07356686326225
At time: 548.280862569809 and batch: 550, loss is 3.8890375661849976 and perplexity is 48.86383567791407
At time: 549.0829622745514 and batch: 600, loss is 3.9224760484695436 and perplexity is 50.52539334774351
At time: 549.893860578537 and batch: 650, loss is 3.9475575256347657 and perplexity is 51.80867082155628
At time: 550.707795381546 and batch: 700, loss is 3.9152266216278075 and perplexity is 50.16043766311498
At time: 551.5023078918457 and batch: 750, loss is 3.906536273956299 and perplexity is 49.726414657401534
At time: 552.3035266399384 and batch: 800, loss is 3.9419012022018434 and perplexity is 51.51645144551689
At time: 553.1016623973846 and batch: 850, loss is 3.9744448900222777 and perplexity is 53.220565437442666
At time: 553.9036183357239 and batch: 900, loss is 3.919012498855591 and perplexity is 50.350698847292726
At time: 554.7034783363342 and batch: 950, loss is 3.9140892696380614 and perplexity is 50.10342002024345
At time: 555.5020980834961 and batch: 1000, loss is 3.891016893386841 and perplexity is 48.96064897804959
At time: 556.2986936569214 and batch: 1050, loss is 3.8960020685195924 and perplexity is 49.20533578436383
At time: 557.0980966091156 and batch: 1100, loss is 3.844655404090881 and perplexity is 46.74257427546154
At time: 557.8970062732697 and batch: 1150, loss is 3.8485623741149904 and perplexity is 46.92555332608276
At time: 558.6952226161957 and batch: 1200, loss is 3.8712128973007203 and perplexity is 48.00057055024669
At time: 559.4924833774567 and batch: 1250, loss is 3.8949463462829588 and perplexity is 49.153416028457926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3256140938640515 and perplexity of 75.61193142872813
Finished 26 epochs...
Completing Train Step...
At time: 561.7707257270813 and batch: 50, loss is 3.9474028015136717 and perplexity is 51.80065539060461
At time: 562.5934388637543 and batch: 100, loss is 3.966403760910034 and perplexity is 52.79432801112059
At time: 563.4039468765259 and batch: 150, loss is 3.8898823118209838 and perplexity is 48.90513062926985
At time: 564.2084782123566 and batch: 200, loss is 3.9401375579833986 and perplexity is 51.4256748261298
At time: 565.0044560432434 and batch: 250, loss is 3.9591273784637453 and perplexity is 52.41157052322998
At time: 565.8010895252228 and batch: 300, loss is 3.9552772426605225 and perplexity is 52.210166823645714
At time: 566.6016652584076 and batch: 350, loss is 3.9437377405166627 and perplexity is 51.61115031487382
At time: 567.4063217639923 and batch: 400, loss is 3.954144449234009 and perplexity is 52.15105697581516
At time: 568.2165277004242 and batch: 450, loss is 3.8781311559677123 and perplexity is 48.33380227601185
At time: 569.0280408859253 and batch: 500, loss is 3.912580428123474 and perplexity is 50.02787890421149
At time: 569.8250460624695 and batch: 550, loss is 3.888156304359436 and perplexity is 48.82079281368283
At time: 570.6226012706757 and batch: 600, loss is 3.9216670417785644 and perplexity is 50.48453449623144
At time: 571.4203462600708 and batch: 650, loss is 3.94686589717865 and perplexity is 51.77285085902312
At time: 572.2156414985657 and batch: 700, loss is 3.914612398147583 and perplexity is 50.12963740461328
At time: 573.0150127410889 and batch: 750, loss is 3.9060945558547973 and perplexity is 49.70445445039212
At time: 573.8109292984009 and batch: 800, loss is 3.9415210103988647 and perplexity is 51.49686903573048
At time: 574.6081652641296 and batch: 850, loss is 3.9740850067138673 and perplexity is 53.201415690321426
At time: 575.433572769165 and batch: 900, loss is 3.9188764905929565 and perplexity is 50.3438512018988
At time: 576.2713055610657 and batch: 950, loss is 3.914084334373474 and perplexity is 50.1031727472191
At time: 577.0782203674316 and batch: 1000, loss is 3.8911903285980225 and perplexity is 48.96914121494984
At time: 577.8764879703522 and batch: 1050, loss is 3.8963518047332766 and perplexity is 49.2225476818306
At time: 578.6816003322601 and batch: 1100, loss is 3.8451706409454345 and perplexity is 46.76666397782327
At time: 579.4878478050232 and batch: 1150, loss is 3.849286479949951 and perplexity is 46.95954469824518
At time: 580.3128705024719 and batch: 1200, loss is 3.8718992328643798 and perplexity is 48.03352635696869
At time: 581.1294240951538 and batch: 1250, loss is 3.895554237365723 and perplexity is 49.18330503545902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325541475393476 and perplexity of 75.60644080527327
Finished 27 epochs...
Completing Train Step...
At time: 583.3533580303192 and batch: 50, loss is 3.9467636728286744 and perplexity is 51.76755868349689
At time: 584.1954727172852 and batch: 100, loss is 3.965618443489075 and perplexity is 52.75288398109496
At time: 584.9901793003082 and batch: 150, loss is 3.8889661264419555 and perplexity is 48.86034498273786
At time: 585.786721944809 and batch: 200, loss is 3.9391089200973513 and perplexity is 51.372803626009755
At time: 586.5837845802307 and batch: 250, loss is 3.9580876350402834 and perplexity is 52.357104257842565
At time: 587.3823857307434 and batch: 300, loss is 3.9542529249191283 and perplexity is 52.15671440429148
At time: 588.1766490936279 and batch: 350, loss is 3.9428148460388184 and perplexity is 51.56354064198436
At time: 588.9735019207001 and batch: 400, loss is 3.9532703876495363 and perplexity is 52.1054936557976
At time: 589.779070854187 and batch: 450, loss is 3.8772947072982786 and perplexity is 48.293390434980914
At time: 590.5957970619202 and batch: 500, loss is 3.9118572902679443 and perplexity is 49.99171492848985
At time: 591.3950707912445 and batch: 550, loss is 3.8874765253067016 and perplexity is 48.78761673886812
At time: 592.2067778110504 and batch: 600, loss is 3.92105571269989 and perplexity is 50.453681263970466
At time: 593.0044231414795 and batch: 650, loss is 3.9463308668136596 and perplexity is 51.745158220593254
At time: 593.7993791103363 and batch: 700, loss is 3.9141443014144897 and perplexity is 50.1061773763227
At time: 594.5994520187378 and batch: 750, loss is 3.905752491950989 and perplexity is 49.68745525823692
At time: 595.396381855011 and batch: 800, loss is 3.9412330293655398 and perplexity is 51.48204104936447
At time: 596.2059576511383 and batch: 850, loss is 3.97381422996521 and perplexity is 53.18701193414597
At time: 597.0109379291534 and batch: 900, loss is 3.918746929168701 and perplexity is 50.33732900335637
At time: 597.8109765052795 and batch: 950, loss is 3.914054684638977 and perplexity is 50.101687223472496
At time: 598.6108174324036 and batch: 1000, loss is 3.891291971206665 and perplexity is 48.97411881916997
At time: 599.4116947650909 and batch: 1050, loss is 3.896570949554443 and perplexity is 49.233335730269
At time: 600.2172384262085 and batch: 1100, loss is 3.8454789972305297 and perplexity is 46.78108699619367
At time: 601.0422921180725 and batch: 1150, loss is 3.849723997116089 and perplexity is 46.98009480034808
At time: 601.8596076965332 and batch: 1200, loss is 3.872313675880432 and perplexity is 48.05343764226527
At time: 602.7202517986298 and batch: 1250, loss is 3.8959089660644532 and perplexity is 49.20075486004732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325495587648266 and perplexity of 75.60297147578183
Finished 28 epochs...
Completing Train Step...
At time: 605.0128200054169 and batch: 50, loss is 3.9461303520202637 and perplexity is 51.734783591051524
At time: 605.8318009376526 and batch: 100, loss is 3.9649003887176515 and perplexity is 52.71501811755496
At time: 606.6468412876129 and batch: 150, loss is 3.888164801597595 and perplexity is 48.821207657349
At time: 607.4558112621307 and batch: 200, loss is 3.9382396602630614 and perplexity is 51.3281667145906
At time: 608.2654733657837 and batch: 250, loss is 3.9572212982177732 and perplexity is 52.31176501286845
At time: 609.0725328922272 and batch: 300, loss is 3.9534132289886474 and perplexity is 52.112937005882834
At time: 609.8899767398834 and batch: 350, loss is 3.942051863670349 and perplexity is 51.52421357445607
At time: 610.7183971405029 and batch: 400, loss is 3.952539176940918 and perplexity is 52.06740748706282
At time: 611.5356068611145 and batch: 450, loss is 3.876599225997925 and perplexity is 48.259814961912895
At time: 612.3716876506805 and batch: 500, loss is 3.911237850189209 and perplexity is 49.96075764573914
At time: 613.1955406665802 and batch: 550, loss is 3.8868975591659547 and perplexity is 48.759378535959044
At time: 614.0006449222565 and batch: 600, loss is 3.9205428981781005 and perplexity is 50.42781451652929
At time: 614.803804397583 and batch: 650, loss is 3.9458757305145262 and perplexity is 51.72161247944986
At time: 615.604983329773 and batch: 700, loss is 3.9137449741363524 and perplexity is 50.08617260738394
At time: 616.4075243473053 and batch: 750, loss is 3.905452938079834 and perplexity is 49.672573417734235
At time: 617.2085552215576 and batch: 800, loss is 3.9409827995300293 and perplexity is 51.46916031833975
At time: 618.0081965923309 and batch: 850, loss is 3.9735757446289064 and perplexity is 53.174329124110095
At time: 618.8039286136627 and batch: 900, loss is 3.918607635498047 and perplexity is 50.33031782034661
At time: 619.6008212566376 and batch: 950, loss is 3.9139967250823973 and perplexity is 50.09878343604904
At time: 620.3997371196747 and batch: 1000, loss is 3.8913376331329346 and perplexity is 48.97635512282918
At time: 621.2009415626526 and batch: 1050, loss is 3.8967028427124024 and perplexity is 49.23982969864091
At time: 621.9990146160126 and batch: 1100, loss is 3.8456649589538574 and perplexity is 46.78978729668699
At time: 622.7964007854462 and batch: 1150, loss is 3.8499979877471926 and perplexity is 46.99296866975101
At time: 623.6395800113678 and batch: 1200, loss is 3.8725769186019896 and perplexity is 48.06608902508958
At time: 624.447452545166 and batch: 1250, loss is 3.896130013465881 and perplexity is 49.2116317611685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325468856922901 and perplexity of 75.60095058052467
Finished 29 epochs...
Completing Train Step...
At time: 626.6511552333832 and batch: 50, loss is 3.945515446662903 and perplexity is 51.702981374138325
At time: 627.4766318798065 and batch: 100, loss is 3.9642330408096313 and perplexity is 52.67985059628401
At time: 628.2721972465515 and batch: 150, loss is 3.8874384117126466 and perplexity is 48.78575730288394
At time: 629.0683057308197 and batch: 200, loss is 3.9374688720703124 and perplexity is 51.28861881321725
At time: 629.8639304637909 and batch: 250, loss is 3.9564609909057618 and perplexity is 52.27200711145174
At time: 630.6591513156891 and batch: 300, loss is 3.9526819133758546 and perplexity is 52.07483993361175
At time: 631.4582023620605 and batch: 350, loss is 3.941383595466614 and perplexity is 51.489793083142715
At time: 632.2603492736816 and batch: 400, loss is 3.951895332336426 and perplexity is 52.033894957270796
At time: 633.0610661506653 and batch: 450, loss is 3.875987854003906 and perplexity is 48.230319279944325
At time: 633.8594677448273 and batch: 500, loss is 3.9106812715530395 and perplexity is 49.93295829236741
At time: 634.6573669910431 and batch: 550, loss is 3.8863791036605835 and perplexity is 48.7341055197519
At time: 635.4574885368347 and batch: 600, loss is 3.92008487701416 and perplexity is 50.40472279888084
At time: 636.2676439285278 and batch: 650, loss is 3.9454645681381226 and perplexity is 51.70035086963792
At time: 637.0783767700195 and batch: 700, loss is 3.9133812475204466 and perplexity is 50.0679582460426
At time: 637.8761994838715 and batch: 750, loss is 3.9051702499389647 and perplexity is 49.65853355484744
At time: 638.6734657287598 and batch: 800, loss is 3.9407472038269042 and perplexity is 51.45703583361963
At time: 639.4699442386627 and batch: 850, loss is 3.973347725868225 and perplexity is 53.16220576171271
At time: 640.2666711807251 and batch: 900, loss is 3.9184561920166017 and perplexity is 50.32269619893066
At time: 641.0621211528778 and batch: 950, loss is 3.913914928436279 and perplexity is 50.09468569118255
At time: 641.8602194786072 and batch: 1000, loss is 3.8913403749465942 and perplexity is 48.97648940705275
At time: 642.6759827136993 and batch: 1050, loss is 3.8967741537094116 and perplexity is 49.243341165190884
At time: 643.4715676307678 and batch: 1100, loss is 3.845772361755371 and perplexity is 46.79481292080311
At time: 644.3119757175446 and batch: 1150, loss is 3.850171947479248 and perplexity is 47.00114426508085
At time: 645.1064970493317 and batch: 1200, loss is 3.8727486991882323 and perplexity is 48.07434655526208
At time: 646.1314356327057 and batch: 1250, loss is 3.896269211769104 and perplexity is 49.21848241359701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325450145415146 and perplexity of 75.59953598598618
Finished 30 epochs...
Completing Train Step...
At time: 648.3582394123077 and batch: 50, loss is 3.9449201679229735 and perplexity is 51.67221284736884
At time: 649.1926717758179 and batch: 100, loss is 3.9636044692993164 and perplexity is 52.64674794781232
At time: 650.0214488506317 and batch: 150, loss is 3.8867656278610228 and perplexity is 48.75294607185215
At time: 650.8193507194519 and batch: 200, loss is 3.9367643642425536 and perplexity is 51.252498304872816
At time: 651.6157186031342 and batch: 250, loss is 3.9557712697982788 and perplexity is 52.235966435252784
At time: 652.413293838501 and batch: 300, loss is 3.952021932601929 and perplexity is 52.040482879195075
At time: 653.211776971817 and batch: 350, loss is 3.9407776308059694 and perplexity is 51.45860153959143
At time: 654.0415141582489 and batch: 400, loss is 3.951309666633606 and perplexity is 52.003429411793554
At time: 654.8394572734833 and batch: 450, loss is 3.8754313802719116 and perplexity is 48.20348784036738
At time: 655.6384890079498 and batch: 500, loss is 3.9101669549942017 and perplexity is 49.90728354812491
At time: 656.4353077411652 and batch: 550, loss is 3.8859009838104246 and perplexity is 48.71081034590946
At time: 657.2333536148071 and batch: 600, loss is 3.919660840034485 and perplexity is 50.38335386339347
At time: 658.0311830043793 and batch: 650, loss is 3.9450805377960205 and perplexity is 51.68050017808456
At time: 658.8291563987732 and batch: 700, loss is 3.9130397033691406 and perplexity is 50.050860747677326
At time: 659.6244478225708 and batch: 750, loss is 3.9048961544036866 and perplexity is 49.64492423772334
At time: 660.4214630126953 and batch: 800, loss is 3.940518002510071 and perplexity is 51.44524316474546
At time: 661.2192232608795 and batch: 850, loss is 3.9731232023239134 and perplexity is 53.150270934726635
At time: 662.0344774723053 and batch: 900, loss is 3.9182939767837524 and perplexity is 50.31453375310357
At time: 662.839015007019 and batch: 950, loss is 3.9138141012191774 and perplexity is 50.08963503805865
At time: 663.6393091678619 and batch: 1000, loss is 3.8913101625442503 and perplexity is 48.97500973200177
At time: 664.4766366481781 and batch: 1050, loss is 3.896802806854248 and perplexity is 49.24475216199217
At time: 665.2908809185028 and batch: 1100, loss is 3.8458257722854614 and perplexity is 46.7973123233133
At time: 666.102041721344 and batch: 1150, loss is 3.8502798748016356 and perplexity is 47.00621724648226
At time: 666.8997673988342 and batch: 1200, loss is 3.872860007286072 and perplexity is 48.079697917151464
At time: 667.6954095363617 and batch: 1250, loss is 3.896354808807373 and perplexity is 49.22269555023316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325435443516195 and perplexity of 75.59842453741761
Finished 31 epochs...
Completing Train Step...
At time: 669.9148392677307 and batch: 50, loss is 3.944343786239624 and perplexity is 51.64243851186058
At time: 670.7312386035919 and batch: 100, loss is 3.963007035255432 and perplexity is 52.61530438195211
At time: 671.5397534370422 and batch: 150, loss is 3.8861331701278687 and perplexity is 48.72212164269675
At time: 672.3403396606445 and batch: 200, loss is 3.9361078691482545 and perplexity is 51.21886233329831
At time: 673.1377356052399 and batch: 250, loss is 3.9551315355300902 and perplexity is 52.20255998425728
At time: 673.9350872039795 and batch: 300, loss is 3.9514123487472532 and perplexity is 52.00876950800396
At time: 674.7381906509399 and batch: 350, loss is 3.940215616226196 and perplexity is 51.429689180616094
At time: 675.5432786941528 and batch: 400, loss is 3.9507653522491455 and perplexity is 51.97513089946579
At time: 676.3424098491669 and batch: 450, loss is 3.8749119949340822 and perplexity is 48.17845815613903
At time: 677.1434726715088 and batch: 500, loss is 3.9096829795837404 and perplexity is 49.88313549408823
At time: 677.944352388382 and batch: 550, loss is 3.8854505443572998 and perplexity is 48.6888740160011
At time: 678.7463359832764 and batch: 600, loss is 3.9192589998245237 and perplexity is 50.363111873193624
At time: 679.5456523895264 and batch: 650, loss is 3.944715723991394 and perplexity is 51.66164985682735
At time: 680.3891623020172 and batch: 700, loss is 3.9127136945724486 and perplexity is 50.034546386248735
At time: 681.1872208118439 and batch: 750, loss is 3.904627537727356 and perplexity is 49.63159057408029
At time: 681.9815201759338 and batch: 800, loss is 3.940292272567749 and perplexity is 51.433631743545185
At time: 682.7796385288239 and batch: 850, loss is 3.972899389266968 and perplexity is 53.13837654122119
At time: 683.5771763324738 and batch: 900, loss is 3.918123459815979 and perplexity is 50.30595500280509
At time: 684.3796598911285 and batch: 950, loss is 3.9136980533599854 and perplexity is 50.083822580412935
At time: 685.1744387149811 and batch: 1000, loss is 3.891254334449768 and perplexity is 48.972275626851825
At time: 685.9736289978027 and batch: 1050, loss is 3.8968001985549927 and perplexity is 49.24462371710929
At time: 686.7799627780914 and batch: 1100, loss is 3.845841474533081 and perplexity is 46.79804715206854
At time: 687.5815827846527 and batch: 1150, loss is 3.850341968536377 and perplexity is 47.009136128688375
At time: 688.3840425014496 and batch: 1200, loss is 3.872929792404175 and perplexity is 48.083053281624856
At time: 689.1864762306213 and batch: 1250, loss is 3.8964034795761107 and perplexity is 49.225091314966306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3254269787864965 and perplexity of 75.59778461989664
Finished 32 epochs...
Completing Train Step...
At time: 691.4289634227753 and batch: 50, loss is 3.9437845134735108 and perplexity is 51.613564377436354
At time: 692.2580406665802 and batch: 100, loss is 3.96243528842926 and perplexity is 52.585230346849976
At time: 693.0614323616028 and batch: 150, loss is 3.8855327939987183 and perplexity is 48.69287882312471
At time: 693.8742380142212 and batch: 200, loss is 3.9354881191253663 and perplexity is 51.18712927649198
At time: 694.6887385845184 and batch: 250, loss is 3.954529447555542 and perplexity is 52.17113891072458
At time: 695.4922211170197 and batch: 300, loss is 3.9508401918411256 and perplexity is 51.97902084261448
At time: 696.2903180122375 and batch: 350, loss is 3.939685916900635 and perplexity is 51.402454122775985
At time: 697.0890347957611 and batch: 400, loss is 3.950251407623291 and perplexity is 51.948425423417696
At time: 697.8911261558533 and batch: 450, loss is 3.874419541358948 and perplexity is 48.1547383431083
At time: 698.6908190250397 and batch: 500, loss is 3.9092225408554078 and perplexity is 49.86017265351233
At time: 699.4885141849518 and batch: 550, loss is 3.885019383430481 and perplexity is 48.667885800928865
At time: 700.2862904071808 and batch: 600, loss is 3.9188728761672973 and perplexity is 50.34366923812008
At time: 701.1287724971771 and batch: 650, loss is 3.9443651962280275 and perplexity is 51.64354418770646
At time: 701.9270598888397 and batch: 700, loss is 3.9123982524871828 and perplexity is 50.018765873651084
At time: 702.729193687439 and batch: 750, loss is 3.9043618059158325 and perplexity is 49.61840363378061
At time: 703.5325033664703 and batch: 800, loss is 3.9400680685043334 and perplexity is 51.422101406934516
At time: 704.3320987224579 and batch: 850, loss is 3.9726744747161864 and perplexity is 53.12642629107511
At time: 705.1320357322693 and batch: 900, loss is 3.917945747375488 and perplexity is 50.297015803097345
At time: 705.9283566474915 and batch: 950, loss is 3.913569097518921 and perplexity is 50.07736439536764
At time: 706.7266888618469 and batch: 1000, loss is 3.891178693771362 and perplexity is 48.968571470794544
At time: 707.5214974880219 and batch: 1050, loss is 3.8967741250991823 and perplexity is 49.24333975632762
At time: 708.3192279338837 and batch: 1100, loss is 3.8458297061920166 and perplexity is 46.79749641992912
At time: 709.118510723114 and batch: 1150, loss is 3.8503714084625242 and perplexity is 47.01052009455607
At time: 709.914510011673 and batch: 1200, loss is 3.8729692602157595 and perplexity is 48.08495105196236
At time: 710.7131879329681 and batch: 1250, loss is 3.8964259433746338 and perplexity is 49.226197109920015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325420741617244 and perplexity of 75.59731310518934
Finished 33 epochs...
Completing Train Step...
At time: 712.9472296237946 and batch: 50, loss is 3.9432410478591917 and perplexity is 51.58552180074307
At time: 713.7466385364532 and batch: 100, loss is 3.9618853998184203 and perplexity is 52.55632227642162
At time: 714.5699963569641 and batch: 150, loss is 3.884957752227783 and perplexity is 48.664886433022474
At time: 715.3692722320557 and batch: 200, loss is 3.934896445274353 and perplexity is 51.156852148566365
At time: 716.1674818992615 and batch: 250, loss is 3.953956446647644 and perplexity is 52.14125336380312
At time: 716.9667453765869 and batch: 300, loss is 3.9502965879440306 and perplexity is 51.95077252296121
At time: 717.7673008441925 and batch: 350, loss is 3.9391814661026 and perplexity is 51.37653065288008
At time: 718.5645298957825 and batch: 400, loss is 3.9497608757019043 and perplexity is 51.922949311415664
At time: 719.3913085460663 and batch: 450, loss is 3.8739491176605223 and perplexity is 48.13209054044942
At time: 720.2012963294983 and batch: 500, loss is 3.908780031204224 and perplexity is 49.838113926863116
At time: 721.0693349838257 and batch: 550, loss is 3.884606146812439 and perplexity is 48.64777860319456
At time: 721.879168510437 and batch: 600, loss is 3.918500695228577 and perplexity is 50.324935770380456
At time: 722.6789035797119 and batch: 650, loss is 3.944024486541748 and perplexity is 51.62595172909861
At time: 723.475996017456 and batch: 700, loss is 3.9120902395248414 and perplexity is 50.00336181784789
At time: 724.2741403579712 and batch: 750, loss is 3.904098391532898 and perplexity is 49.60533515389357
At time: 725.0753419399261 and batch: 800, loss is 3.9398437261581423 and perplexity is 51.410566545986065
At time: 725.8735511302948 and batch: 850, loss is 3.9724482440948488 and perplexity is 53.114408826056376
At time: 726.6760444641113 and batch: 900, loss is 3.917762961387634 and perplexity is 50.28782305355616
At time: 727.477908372879 and batch: 950, loss is 3.9134319734573366 and perplexity is 50.07049805454935
At time: 728.3007121086121 and batch: 1000, loss is 3.891086902618408 and perplexity is 48.9640767954496
At time: 729.1123566627502 and batch: 1050, loss is 3.8967290496826172 and perplexity is 49.241120142300446
At time: 729.9132881164551 and batch: 1100, loss is 3.845796766281128 and perplexity is 46.79595493995548
At time: 730.7133493423462 and batch: 1150, loss is 3.850374994277954 and perplexity is 47.01068866590663
At time: 731.5116145610809 and batch: 1200, loss is 3.872984414100647 and perplexity is 48.08567973129657
At time: 732.3084480762482 and batch: 1250, loss is 3.8964276027679445 and perplexity is 49.226278795609986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325415395472172 and perplexity of 75.59690895206673
Finished 34 epochs...
Completing Train Step...
At time: 734.5223114490509 and batch: 50, loss is 3.942710862159729 and perplexity is 51.558179143768335
At time: 735.3664281368256 and batch: 100, loss is 3.9613526105880736 and perplexity is 52.52832829203502
At time: 736.1637978553772 and batch: 150, loss is 3.8844042158126832 and perplexity is 48.637956100392614
At time: 736.9623246192932 and batch: 200, loss is 3.93432806968689 and perplexity is 51.12778410423958
At time: 737.7603344917297 and batch: 250, loss is 3.953406777381897 and perplexity is 52.112600794791206
At time: 738.558025598526 and batch: 300, loss is 3.9497759771347045 and perplexity is 51.92373342826611
At time: 739.3549559116364 and batch: 350, loss is 3.9386971044540404 and perplexity is 51.35165185744935
At time: 740.1518850326538 and batch: 400, loss is 3.949289469718933 and perplexity is 51.89847829080268
At time: 740.9533624649048 and batch: 450, loss is 3.8734951782226563 and perplexity is 48.11024644464956
At time: 741.8010020256042 and batch: 500, loss is 3.908352704048157 and perplexity is 49.816821297158484
At time: 742.6206171512604 and batch: 550, loss is 3.8842049837112427 and perplexity is 48.62826682342851
At time: 743.418496131897 and batch: 600, loss is 3.918137593269348 and perplexity is 50.306666004698755
At time: 744.2236759662628 and batch: 650, loss is 3.9436922311782836 and perplexity is 51.608801579015115
At time: 745.0346417427063 and batch: 700, loss is 3.9117878580093386 and perplexity is 49.98824401130899
At time: 745.8327531814575 and batch: 750, loss is 3.9038363695144653 and perplexity is 49.59233916654329
At time: 746.6307234764099 and batch: 800, loss is 3.939619584083557 and perplexity is 51.399044566273304
At time: 747.447598695755 and batch: 850, loss is 3.9722203493118284 and perplexity is 53.10230570855276
At time: 748.245748758316 and batch: 900, loss is 3.9175752353668214 and perplexity is 50.27838360668161
At time: 749.0437433719635 and batch: 950, loss is 3.913286037445068 and perplexity is 50.063191498888735
At time: 749.8426396846771 and batch: 1000, loss is 3.890982542037964 and perplexity is 48.95896714260212
At time: 750.6396012306213 and batch: 1050, loss is 3.896668782234192 and perplexity is 49.23815259505602
At time: 751.4366979598999 and batch: 1100, loss is 3.8457471132278442 and perplexity is 46.7936314355964
At time: 752.247624874115 and batch: 1150, loss is 3.850358934402466 and perplexity is 47.0099336861625
At time: 753.052711725235 and batch: 1200, loss is 3.872981085777283 and perplexity is 48.085519686871585
At time: 753.8503429889679 and batch: 1250, loss is 3.8964144802093506 and perplexity is 49.225632825120535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325412276887546 and perplexity of 75.5966731970763
Finished 35 epochs...
Completing Train Step...
At time: 756.0837407112122 and batch: 50, loss is 3.942193379402161 and perplexity is 51.53150557720046
At time: 756.8833074569702 and batch: 100, loss is 3.9608357620239256 and perplexity is 52.50118611578181
At time: 757.7016952037811 and batch: 150, loss is 3.883868055343628 and perplexity is 48.61188534071585
At time: 758.5079216957092 and batch: 200, loss is 3.9337778282165528 and perplexity is 51.09965921558863
At time: 759.305953502655 and batch: 250, loss is 3.9528759288787843 and perplexity is 52.0849442400358
At time: 760.1058554649353 and batch: 300, loss is 3.9492733001708986 and perplexity is 51.89763912264955
At time: 760.9053514003754 and batch: 350, loss is 3.938228716850281 and perplexity is 51.32760501234846
At time: 761.7036874294281 and batch: 400, loss is 3.948832859992981 and perplexity is 51.87478635024988
At time: 762.5327942371368 and batch: 450, loss is 3.873054690361023 and perplexity is 48.089059131790144
At time: 763.330317735672 and batch: 500, loss is 3.9079368686676026 and perplexity is 49.79611000685846
At time: 764.131530046463 and batch: 550, loss is 3.883814935684204 and perplexity is 48.60930316250541
At time: 764.9294593334198 and batch: 600, loss is 3.917783007621765 and perplexity is 50.28883114513521
At time: 765.7277216911316 and batch: 650, loss is 3.9433663749694823 and perplexity is 51.59198727026403
At time: 766.5258874893188 and batch: 700, loss is 3.9114903736114504 and perplexity is 49.97337550032247
At time: 767.3247454166412 and batch: 750, loss is 3.9035748434066773 and perplexity is 49.57937117091359
At time: 768.1501312255859 and batch: 800, loss is 3.93939471244812 and perplexity is 51.38748767851862
At time: 768.9552659988403 and batch: 850, loss is 3.971991376876831 and perplexity is 53.090148136238284
At time: 769.7517356872559 and batch: 900, loss is 3.9173840427398683 and perplexity is 50.26877166933594
At time: 770.5616555213928 and batch: 950, loss is 3.913133964538574 and perplexity is 50.055578822704696
At time: 771.3808126449585 and batch: 1000, loss is 3.8908671712875367 and perplexity is 48.95331903564217
At time: 772.1952395439148 and batch: 1050, loss is 3.8965958309173585 and perplexity is 49.2345607380027
At time: 773.0014233589172 and batch: 1100, loss is 3.8456838369369506 and perplexity is 46.790670601837995
At time: 773.8076167106628 and batch: 1150, loss is 3.8503262424468994 and perplexity is 47.00839686462023
At time: 774.6184685230255 and batch: 1200, loss is 3.872962112426758 and perplexity is 48.084607352106445
At time: 775.4189193248749 and batch: 1250, loss is 3.896390075683594 and perplexity is 49.22443151155517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325411385863367 and perplexity of 75.59660583864265
Finished 36 epochs...
Completing Train Step...
At time: 777.6497557163239 and batch: 50, loss is 3.9416884183883667 and perplexity is 51.50549074469274
At time: 778.4810044765472 and batch: 100, loss is 3.9603331327438354 and perplexity is 52.47480411313947
At time: 779.2784349918365 and batch: 150, loss is 3.8833476543426513 and perplexity is 48.58659424825086
At time: 780.0753047466278 and batch: 200, loss is 3.9332422637939453 and perplexity is 51.072299383235766
At time: 780.8730194568634 and batch: 250, loss is 3.952360820770264 and perplexity is 52.05812177175446
At time: 781.6701357364655 and batch: 300, loss is 3.9487850952148436 and perplexity is 51.872308621763466
At time: 782.4680893421173 and batch: 350, loss is 3.9377724742889404 and perplexity is 51.304192515664994
At time: 783.312876701355 and batch: 400, loss is 3.948388090133667 and perplexity is 51.85171913900335
At time: 784.1124751567841 and batch: 450, loss is 3.8726238059997558 and perplexity is 48.068342771760506
At time: 784.9136514663696 and batch: 500, loss is 3.9075307703018187 and perplexity is 49.77589199349123
At time: 785.7135798931122 and batch: 550, loss is 3.8834321880340577 and perplexity is 48.59070162601899
At time: 786.5152957439423 and batch: 600, loss is 3.917433485984802 and perplexity is 50.271257181971514
At time: 787.3163735866547 and batch: 650, loss is 3.9430461978912352 and perplexity is 51.57547134267069
At time: 788.1197431087494 and batch: 700, loss is 3.911196537017822 and perplexity is 49.958693651031275
At time: 788.9194478988647 and batch: 750, loss is 3.9033135747909546 and perplexity is 49.56641932926791
At time: 789.7204396724701 and batch: 800, loss is 3.9391687059402467 and perplexity is 51.37587508419038
At time: 790.5188634395599 and batch: 850, loss is 3.971760630607605 and perplexity is 53.07789919587618
At time: 791.3168573379517 and batch: 900, loss is 3.917189655303955 and perplexity is 50.25900100138295
At time: 792.1188473701477 and batch: 950, loss is 3.912974786758423 and perplexity is 50.04761172089313
At time: 792.9166066646576 and batch: 1000, loss is 3.8907424306869505 and perplexity is 48.94721295007128
At time: 793.7173352241516 and batch: 1050, loss is 3.8965125465393067 and perplexity is 49.23046043898078
At time: 794.5162973403931 and batch: 1100, loss is 3.8456092739105223 and perplexity is 46.78718187789584
At time: 795.3161396980286 and batch: 1150, loss is 3.8502810335159303 and perplexity is 47.00627171328968
At time: 796.1134293079376 and batch: 1200, loss is 3.87293167591095 and perplexity is 48.08314384646676
At time: 796.9117345809937 and batch: 1250, loss is 3.8963525247573854 and perplexity is 49.222583123264386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325410494839188 and perplexity of 75.59653848026902
Finished 37 epochs...
Completing Train Step...
At time: 799.1235632896423 and batch: 50, loss is 3.941188793182373 and perplexity is 51.47976373073712
At time: 799.9667751789093 and batch: 100, loss is 3.9598387336730956 and perplexity is 52.44886703091168
At time: 800.7728099822998 and batch: 150, loss is 3.882839159965515 and perplexity is 48.56189451864046
At time: 801.5916414260864 and batch: 200, loss is 3.932722029685974 and perplexity is 51.0457367411194
At time: 802.4068846702576 and batch: 250, loss is 3.951859016418457 and perplexity is 52.03200533292186
At time: 803.2576913833618 and batch: 300, loss is 3.948312268257141 and perplexity is 51.8477877934
At time: 804.0843207836151 and batch: 350, loss is 3.9373294067382814 and perplexity is 51.28146632773853
At time: 804.898647069931 and batch: 400, loss is 3.9479559898376464 and perplexity is 51.829318835751565
At time: 805.7083678245544 and batch: 450, loss is 3.872205443382263 and perplexity is 48.04823698010904
At time: 806.5070035457611 and batch: 500, loss is 3.907133979797363 and perplexity is 49.75614531010472
At time: 807.336728811264 and batch: 550, loss is 3.8830598735809327 and perplexity is 48.572613972872425
At time: 808.1412169933319 and batch: 600, loss is 3.9170917320251464 and perplexity is 50.254079716173365
At time: 808.9407138824463 and batch: 650, loss is 3.9427309703826903 and perplexity is 51.55921589755365
At time: 809.7415072917938 and batch: 700, loss is 3.9109060335159302 and perplexity is 49.94418258343574
At time: 810.5410680770874 and batch: 750, loss is 3.903053650856018 and perplexity is 49.553537504734905
At time: 811.3404350280762 and batch: 800, loss is 3.9389435768127443 and perplexity is 51.364310180105235
At time: 812.1396064758301 and batch: 850, loss is 3.9715300369262696 and perplexity is 53.06566117876199
At time: 812.9394481182098 and batch: 900, loss is 3.9169927406311036 and perplexity is 50.2491052409851
At time: 813.7388405799866 and batch: 950, loss is 3.91281222820282 and perplexity is 50.039476714645744
At time: 814.5375866889954 and batch: 1000, loss is 3.8906113576889036 and perplexity is 48.94079771256527
At time: 815.3363087177277 and batch: 1050, loss is 3.896420679092407 and perplexity is 49.22593797000758
At time: 816.1340963840485 and batch: 1100, loss is 3.84552547454834 and perplexity is 46.78326130616916
At time: 816.9320318698883 and batch: 1150, loss is 3.850224771499634 and perplexity is 47.00362712006028
At time: 817.7304193973541 and batch: 1200, loss is 3.872888741493225 and perplexity is 48.08107946900009
At time: 818.5289690494537 and batch: 1250, loss is 3.89630331993103 and perplexity is 49.22016119419484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.32540871279083 and perplexity of 75.5964037637018
Finished 38 epochs...
Completing Train Step...
At time: 820.7241775989532 and batch: 50, loss is 3.9406952905654906 and perplexity is 51.45436460040364
At time: 821.5243787765503 and batch: 100, loss is 3.9593526029586794 and perplexity is 52.42337622214617
At time: 822.3240377902985 and batch: 150, loss is 3.8823422622680663 and perplexity is 48.53777021922118
At time: 823.1232342720032 and batch: 200, loss is 3.9322157382965086 and perplexity is 51.01989926533611
At time: 823.9520645141602 and batch: 250, loss is 3.9513698863983153 and perplexity is 52.00656114037185
At time: 824.7502076625824 and batch: 300, loss is 3.9478521013259886 and perplexity is 51.823934644640126
At time: 825.5494508743286 and batch: 350, loss is 3.9368974113464357 and perplexity is 51.25931775498299
At time: 826.3445630073547 and batch: 400, loss is 3.947534694671631 and perplexity is 51.8074879932053
At time: 827.1390297412872 and batch: 450, loss is 3.8717962884902954 and perplexity is 48.02858183017279
At time: 827.935524225235 and batch: 500, loss is 3.906746144294739 and perplexity is 49.73685185206556
At time: 828.730836391449 and batch: 550, loss is 3.8826951932907106 and perplexity is 48.554903727396955
At time: 829.5240693092346 and batch: 600, loss is 3.916755542755127 and perplexity is 50.237187673418944
At time: 830.3428404331207 and batch: 650, loss is 3.942420325279236 and perplexity is 51.54320176708151
At time: 831.1419732570648 and batch: 700, loss is 3.910618472099304 and perplexity is 49.92982262832328
At time: 831.9357380867004 and batch: 750, loss is 3.902795448303223 and perplexity is 49.54074430654045
At time: 832.7305562496185 and batch: 800, loss is 3.9387192583084105 and perplexity is 51.352789507067875
At time: 833.5248234272003 and batch: 850, loss is 3.9712995147705077 and perplexity is 53.053429778008734
At time: 834.3199939727783 and batch: 900, loss is 3.916793851852417 and perplexity is 50.23911225159329
At time: 835.1138792037964 and batch: 950, loss is 3.912645797729492 and perplexity is 50.031149313836906
At time: 835.9094214439392 and batch: 1000, loss is 3.8904749870300295 and perplexity is 48.934124078789644
At time: 836.7030053138733 and batch: 1050, loss is 3.8963215494155885 and perplexity is 49.22105846054161
At time: 837.4976987838745 and batch: 1100, loss is 3.845433831214905 and perplexity is 46.77897412860271
At time: 838.2930929660797 and batch: 1150, loss is 3.8501590585708616 and perplexity is 47.000538475542356
At time: 839.0935337543488 and batch: 1200, loss is 3.8728353452682494 and perplexity is 48.07851218940581
At time: 839.8876700401306 and batch: 1250, loss is 3.896246271133423 and perplexity is 49.21735332327427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325410049327099 and perplexity of 75.5965048011047
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 842.0882256031036 and batch: 50, loss is 3.941125373840332 and perplexity is 51.476499021516844
At time: 842.9298729896545 and batch: 100, loss is 3.960850100517273 and perplexity is 52.50193890908661
At time: 843.7750101089478 and batch: 150, loss is 3.8843748569488525 and perplexity is 48.63652816622382
At time: 844.6554610729218 and batch: 200, loss is 3.9342636489868164 and perplexity is 51.124490522682926
At time: 845.4662976264954 and batch: 250, loss is 3.9539972352981567 and perplexity is 52.14338017853853
At time: 846.2968702316284 and batch: 300, loss is 3.9500925970077514 and perplexity is 51.94017611705595
At time: 847.0986268520355 and batch: 350, loss is 3.9392273664474486 and perplexity is 51.378888907476096
At time: 847.8982126712799 and batch: 400, loss is 3.949638319015503 and perplexity is 51.91658619672897
At time: 848.6994316577911 and batch: 450, loss is 3.8736002588272096 and perplexity is 48.11530216405554
At time: 849.4998018741608 and batch: 500, loss is 3.908443765640259 and perplexity is 49.82135790277139
At time: 850.3002741336823 and batch: 550, loss is 3.8839372062683104 and perplexity is 48.61524701376777
At time: 851.1113202571869 and batch: 600, loss is 3.917240481376648 and perplexity is 50.261555533939195
At time: 851.926727771759 and batch: 650, loss is 3.9420013189315797 and perplexity is 51.52160936235581
At time: 852.7262194156647 and batch: 700, loss is 3.9107664918899534 and perplexity is 49.93721377722054
At time: 853.5284268856049 and batch: 750, loss is 3.9025035524368286 and perplexity is 49.526285678368836
At time: 854.3285126686096 and batch: 800, loss is 3.9374021768569945 and perplexity is 51.28519822191452
At time: 855.1273312568665 and batch: 850, loss is 3.9693862056732176 and perplexity is 52.952019213985956
At time: 855.9259326457977 and batch: 900, loss is 3.914260754585266 and perplexity is 50.11201273932031
At time: 856.7281725406647 and batch: 950, loss is 3.9100492858886717 and perplexity is 49.9014113482054
At time: 857.5294880867004 and batch: 1000, loss is 3.887781844139099 and perplexity is 48.80251479125853
At time: 858.3312623500824 and batch: 1050, loss is 3.8933086013793945 and perplexity is 49.072981155744564
At time: 859.1331315040588 and batch: 1100, loss is 3.8422834873199463 and perplexity is 46.63183616233238
At time: 859.9347724914551 and batch: 1150, loss is 3.846706485748291 and perplexity is 46.83854550094278
At time: 860.7606036663055 and batch: 1200, loss is 3.869325942993164 and perplexity is 47.91008106849015
At time: 861.5625586509705 and batch: 1250, loss is 3.8934362840652468 and perplexity is 49.079247325813505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.325076360772126 and perplexity of 75.57128332095031
Finished 40 epochs...
Completing Train Step...
At time: 863.8028552532196 and batch: 50, loss is 3.9409151792526247 and perplexity is 51.46568007711001
At time: 864.6116251945496 and batch: 100, loss is 3.9602511167526244 and perplexity is 52.470500516550814
At time: 865.4371600151062 and batch: 150, loss is 3.8837059926986695 and perplexity is 48.604007808345216
At time: 866.2572038173676 and batch: 200, loss is 3.93360830783844 and perplexity is 51.090997516224995
At time: 867.0605919361115 and batch: 250, loss is 3.9532076644897463 and perplexity is 52.10222553708753
At time: 867.8598477840424 and batch: 300, loss is 3.9493745088577272 and perplexity is 51.90289188036253
At time: 868.6607530117035 and batch: 350, loss is 3.938464741706848 and perplexity is 51.33972103274417
At time: 869.4589793682098 and batch: 400, loss is 3.9490493011474608 and perplexity is 51.88601540406676
At time: 870.2598514556885 and batch: 450, loss is 3.8731578683853147 and perplexity is 48.094021121881184
At time: 871.0623767375946 and batch: 500, loss is 3.908058090209961 and perplexity is 49.80214673400023
At time: 871.8646168708801 and batch: 550, loss is 3.8834353923797607 and perplexity is 48.59085732767441
At time: 872.7092854976654 and batch: 600, loss is 3.9167377281188966 and perplexity is 50.23629272416694
At time: 873.5209782123566 and batch: 650, loss is 3.941590929031372 and perplexity is 51.500469752268984
At time: 874.3473982810974 and batch: 700, loss is 3.910333127975464 and perplexity is 49.91557747931335
At time: 875.1459653377533 and batch: 750, loss is 3.902150106430054 and perplexity is 49.50878390361383
At time: 875.9456057548523 and batch: 800, loss is 3.9373682737350464 and perplexity is 51.2834595230589
At time: 876.7459683418274 and batch: 850, loss is 3.969453954696655 and perplexity is 52.95560678310253
At time: 877.5468204021454 and batch: 900, loss is 3.914387793540955 and perplexity is 50.1183793214796
At time: 878.3463222980499 and batch: 950, loss is 3.9103060960769653 and perplexity is 49.914228184726504
At time: 879.147420167923 and batch: 1000, loss is 3.888034701347351 and perplexity is 48.81485641917336
At time: 879.9493775367737 and batch: 1050, loss is 3.8935953855514525 and perplexity is 49.08705652821628
At time: 880.7496523857117 and batch: 1100, loss is 3.842559604644775 and perplexity is 46.64471379797272
At time: 881.5497088432312 and batch: 1150, loss is 3.8471213436126708 and perplexity is 46.85798087107928
At time: 882.3506393432617 and batch: 1200, loss is 3.869788103103638 and perplexity is 47.93222831424214
At time: 883.1530866622925 and batch: 1250, loss is 3.8938028383255006 and perplexity is 49.097240830607284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3249226591012775 and perplexity of 75.55966878104567
Finished 41 epochs...
Completing Train Step...
At time: 885.4511375427246 and batch: 50, loss is 3.9407630491256715 and perplexity is 51.45785119218588
At time: 886.2828576564789 and batch: 100, loss is 3.9599126529693605 and perplexity is 52.45274415754796
At time: 887.0969595909119 and batch: 150, loss is 3.883322653770447 and perplexity is 48.58537957077707
At time: 887.9097104072571 and batch: 200, loss is 3.9331696128845213 and perplexity is 51.06858906901969
At time: 888.7117261886597 and batch: 250, loss is 3.9526967096328733 and perplexity is 52.075610452027995
At time: 889.538251876831 and batch: 300, loss is 3.948910336494446 and perplexity is 51.87880558290655
At time: 890.3566772937775 and batch: 350, loss is 3.937974705696106 and perplexity is 51.31456888388934
At time: 891.1627743244171 and batch: 400, loss is 3.9486731481552124 and perplexity is 51.866501994360476
At time: 891.9664595127106 and batch: 450, loss is 3.8728488874435425 and perplexity is 48.0791632814543
At time: 892.7671709060669 and batch: 500, loss is 3.907806005477905 and perplexity is 49.789593955433375
At time: 893.5656383037567 and batch: 550, loss is 3.8831263399124145 and perplexity is 48.57584252362746
At time: 894.3800375461578 and batch: 600, loss is 3.916444487571716 and perplexity is 50.22156356589894
At time: 895.1990718841553 and batch: 650, loss is 3.9413594675064085 and perplexity is 51.48855075445171
At time: 896.0132608413696 and batch: 700, loss is 3.910075044631958 and perplexity is 49.9026967624052
At time: 896.812025308609 and batch: 750, loss is 3.901958451271057 and perplexity is 49.499296198975856
At time: 897.6127116680145 and batch: 800, loss is 3.9373627424240114 and perplexity is 51.28317585907784
At time: 898.4116086959839 and batch: 850, loss is 3.969518880844116 and perplexity is 52.959045098254485
At time: 899.2115442752838 and batch: 900, loss is 3.9144926261901856 and perplexity is 50.12363363936624
At time: 900.0108435153961 and batch: 950, loss is 3.9104951429367065 and perplexity is 49.923665204812636
At time: 900.8086779117584 and batch: 1000, loss is 3.8882207250595093 and perplexity is 48.82393798464
At time: 901.6108210086823 and batch: 1050, loss is 3.893814878463745 and perplexity is 49.097831971733015
At time: 902.4120695590973 and batch: 1100, loss is 3.8427754926681517 and perplexity is 46.654784920113734
At time: 903.2337651252747 and batch: 1150, loss is 3.8474348735809327 and perplexity is 46.87267455566947
At time: 904.0533063411713 and batch: 1200, loss is 3.870117664337158 and perplexity is 47.94802752179093
At time: 904.8528323173523 and batch: 1250, loss is 3.8940564346313478 and perplexity is 49.10969328839113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324845139997719 and perplexity of 75.55381169027774
Finished 42 epochs...
Completing Train Step...
At time: 907.0641360282898 and batch: 50, loss is 3.9406284141540526 and perplexity is 51.450923632207434
At time: 907.9114689826965 and batch: 100, loss is 3.95966157913208 and perplexity is 52.43957629891801
At time: 908.7103321552277 and batch: 150, loss is 3.8830422306060792 and perplexity is 48.5717570150252
At time: 909.5097727775574 and batch: 200, loss is 3.93282687664032 and perplexity is 51.05108901172876
At time: 910.3073756694794 and batch: 250, loss is 3.9523119068145753 and perplexity is 52.055575465368356
At time: 911.1066002845764 and batch: 300, loss is 3.9485573244094847 and perplexity is 51.860494969706465
At time: 911.9081516265869 and batch: 350, loss is 3.9376110649108886 and perplexity is 51.29591220613737
At time: 912.7366456985474 and batch: 400, loss is 3.9483949184417724 and perplexity is 51.85207319972623
At time: 913.5470123291016 and batch: 450, loss is 3.872603917121887 and perplexity is 48.067386755868846
At time: 914.341276884079 and batch: 500, loss is 3.907607922554016 and perplexity is 49.779732463812216
At time: 915.1369643211365 and batch: 550, loss is 3.882906699180603 and perplexity is 48.56517446164056
At time: 915.9293332099915 and batch: 600, loss is 3.9162476158142088 and perplexity is 50.21167733160705
At time: 916.7235848903656 and batch: 650, loss is 3.941207280158997 and perplexity is 51.48071544472294
At time: 917.5176565647125 and batch: 700, loss is 3.90990403175354 and perplexity is 49.89416348826171
At time: 918.3415598869324 and batch: 750, loss is 3.901842451095581 and perplexity is 49.49355460495023
At time: 919.1364736557007 and batch: 800, loss is 3.937364616394043 and perplexity is 51.28327196230258
At time: 919.9301571846008 and batch: 850, loss is 3.9695676660537718 and perplexity is 52.961628779394964
At time: 920.7251679897308 and batch: 900, loss is 3.914568657875061 and perplexity is 50.12744476856538
At time: 921.5222632884979 and batch: 950, loss is 3.910627031326294 and perplexity is 49.93024999083766
At time: 922.3386311531067 and batch: 1000, loss is 3.8883541679382323 and perplexity is 48.830453626198576
At time: 923.1392560005188 and batch: 1050, loss is 3.8939786386489867 and perplexity is 49.105872900165664
At time: 923.9428355693817 and batch: 1100, loss is 3.842943506240845 and perplexity is 46.66262421574698
At time: 924.7590131759644 and batch: 1150, loss is 3.847672905921936 and perplexity is 46.8838330961171
At time: 925.5910384654999 and batch: 1200, loss is 3.870354862213135 and perplexity is 47.959402041028945
At time: 926.437132358551 and batch: 1250, loss is 3.8942349052429197 and perplexity is 49.11845870754801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324803707373404 and perplexity of 75.55068136243166
Finished 43 epochs...
Completing Train Step...
At time: 928.7122149467468 and batch: 50, loss is 3.940502462387085 and perplexity is 51.444443705551514
At time: 929.5086164474487 and batch: 100, loss is 3.9594564390182496 and perplexity is 52.428819941584734
At time: 930.3076598644257 and batch: 150, loss is 3.8828159952163697 and perplexity is 48.56076960756509
At time: 931.1259658336639 and batch: 200, loss is 3.932542862892151 and perplexity is 51.03659185938317
At time: 931.9237637519836 and batch: 250, loss is 3.9520040130615235 and perplexity is 52.039550346014714
At time: 932.7183306217194 and batch: 300, loss is 3.948271532058716 and perplexity is 51.845675754647054
At time: 933.5180571079254 and batch: 350, loss is 3.9373231649398805 and perplexity is 51.281146240162975
At time: 934.3179340362549 and batch: 400, loss is 3.9481727027893068 and perplexity is 51.84055213757604
At time: 935.1163403987885 and batch: 450, loss is 3.8723995542526244 and perplexity is 48.057564570472636
At time: 935.917222738266 and batch: 500, loss is 3.9074424266815186 and perplexity is 49.77149480522348
At time: 936.7163653373718 and batch: 550, loss is 3.882736554145813 and perplexity is 48.55691204126705
At time: 937.5158052444458 and batch: 600, loss is 3.91610143661499 and perplexity is 50.204337965267705
At time: 938.3150367736816 and batch: 650, loss is 3.9410955905914307 and perplexity is 51.47496590696459
At time: 939.1127569675446 and batch: 700, loss is 3.909780626296997 and perplexity is 49.88800665613877
At time: 939.9095978736877 and batch: 750, loss is 3.9017652463912964 and perplexity is 49.489733617203974
At time: 940.7088503837585 and batch: 800, loss is 3.9373661994934084 and perplexity is 51.28335314888214
At time: 941.5095884799957 and batch: 850, loss is 3.969599905014038 and perplexity is 52.963336234763965
At time: 942.3113150596619 and batch: 900, loss is 3.914621195793152 and perplexity is 50.13007842933565
At time: 943.1132040023804 and batch: 950, loss is 3.9107187366485596 and perplexity is 49.93482907046366
At time: 943.9256408214569 and batch: 1000, loss is 3.8884516525268555 and perplexity is 48.835214074914035
At time: 944.7244110107422 and batch: 1050, loss is 3.8941019916534425 and perplexity is 49.111930630736275
At time: 945.5232002735138 and batch: 1100, loss is 3.8430757999420164 and perplexity is 46.66879779536471
At time: 946.323757648468 and batch: 1150, loss is 3.8478574705123902 and perplexity is 46.89248699014806
At time: 947.1666433811188 and batch: 1200, loss is 3.870529360771179 and perplexity is 47.967771617748056
At time: 947.994916677475 and batch: 1250, loss is 3.894363036155701 and perplexity is 49.124752703715735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324778313184306 and perplexity of 75.5487628385025
Finished 44 epochs...
Completing Train Step...
At time: 950.2619752883911 and batch: 50, loss is 3.9403818702697753 and perplexity is 51.438240285210554
At time: 951.1027374267578 and batch: 100, loss is 3.959279432296753 and perplexity is 52.41954050934011
At time: 951.9086389541626 and batch: 150, loss is 3.8826232719421387 and perplexity is 48.55141171881759
At time: 952.7378907203674 and batch: 200, loss is 3.9323003768920897 and perplexity is 51.02421770070719
At time: 953.5459380149841 and batch: 250, loss is 3.951747708320618 and perplexity is 52.0262140716946
At time: 954.3436801433563 and batch: 300, loss is 3.948031601905823 and perplexity is 51.83323790590354
At time: 955.1406946182251 and batch: 350, loss is 3.9370856714248657 and perplexity is 51.26896874658352
At time: 955.9413406848907 and batch: 400, loss is 3.947985973358154 and perplexity is 51.83087288449359
At time: 956.7426800727844 and batch: 450, loss is 3.8722236585617065 and perplexity is 48.04911219533865
At time: 957.5620613098145 and batch: 500, loss is 3.907299084663391 and perplexity is 49.76436097001426
At time: 958.3621025085449 and batch: 550, loss is 3.8825961637496946 and perplexity is 48.55009559564423
At time: 959.162045955658 and batch: 600, loss is 3.91598415851593 and perplexity is 50.19845044119213
At time: 959.9879298210144 and batch: 650, loss is 3.941006245613098 and perplexity is 51.47036708269495
At time: 960.7993502616882 and batch: 700, loss is 3.90968542098999 and perplexity is 49.883257279235636
At time: 961.5992245674133 and batch: 750, loss is 3.90170804977417 and perplexity is 49.4869030528087
At time: 962.411926984787 and batch: 800, loss is 3.9373644256591795 and perplexity is 51.28326218079564
At time: 963.225150346756 and batch: 850, loss is 3.9696184492111204 and perplexity is 52.964318406416005
At time: 964.0229728221893 and batch: 900, loss is 3.9146561765670778 and perplexity is 50.13183204894738
At time: 964.8207767009735 and batch: 950, loss is 3.910782904624939 and perplexity is 49.93803339020223
At time: 965.6205043792725 and batch: 1000, loss is 3.8885239934921265 and perplexity is 48.8387469892251
At time: 966.4183123111725 and batch: 1050, loss is 3.8941961526870728 and perplexity is 49.11655527861544
At time: 967.2166750431061 and batch: 1100, loss is 3.843180732727051 and perplexity is 46.67369513923306
At time: 968.0596432685852 and batch: 1150, loss is 3.8480031156539916 and perplexity is 46.89931715043353
At time: 968.8591787815094 and batch: 1200, loss is 3.870660452842712 and perplexity is 47.974060224480496
At time: 969.6667258739471 and batch: 1250, loss is 3.894457049369812 and perplexity is 49.1293712967108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324762720261177 and perplexity of 75.54758482163541
Finished 45 epochs...
Completing Train Step...
At time: 971.8789846897125 and batch: 50, loss is 3.9402656316757203 and perplexity is 51.43226152396727
At time: 972.6773233413696 and batch: 100, loss is 3.95912193775177 and perplexity is 52.41128536774632
At time: 973.4776775836945 and batch: 150, loss is 3.8824533319473264 and perplexity is 48.543161593194974
At time: 974.2919821739197 and batch: 200, loss is 3.9320882463455202 and perplexity is 51.01339505346568
At time: 975.1008198261261 and batch: 250, loss is 3.951528367996216 and perplexity is 52.01480387642643
At time: 975.90789103508 and batch: 300, loss is 3.947824673652649 and perplexity is 51.82251325418232
At time: 976.7055957317352 and batch: 350, loss is 3.936883683204651 and perplexity is 51.25861406463125
At time: 977.5053746700287 and batch: 400, loss is 3.947823796272278 and perplexity is 51.82246778614636
At time: 978.3042380809784 and batch: 450, loss is 3.8720684099197387 and perplexity is 48.04165321493584
At time: 979.1062638759613 and batch: 500, loss is 3.9071719217300416 and perplexity is 49.758033190235096
At time: 979.9183306694031 and batch: 550, loss is 3.8824752521514894 and perplexity is 48.54422568087028
At time: 980.7169778347015 and batch: 600, loss is 3.9158842897415163 and perplexity is 50.19343743379471
At time: 981.5157160758972 and batch: 650, loss is 3.9409297275543214 and perplexity is 51.466428820797255
At time: 982.3140313625336 and batch: 700, loss is 3.9096072053909303 and perplexity is 49.87935578296542
At time: 983.1151452064514 and batch: 750, loss is 3.9016618013381956 and perplexity is 49.48461441386469
At time: 983.9171159267426 and batch: 800, loss is 3.937358465194702 and perplexity is 51.28295650964409
At time: 984.7275586128235 and batch: 850, loss is 3.9696263217926027 and perplexity is 52.96473537396962
At time: 985.53449177742 and batch: 900, loss is 3.914677691459656 and perplexity is 50.132910641531524
At time: 986.3447153568268 and batch: 950, loss is 3.910827808380127 and perplexity is 49.940275845775105
At time: 987.1867280006409 and batch: 1000, loss is 3.88857816696167 and perplexity is 48.84139282526408
At time: 988.0213689804077 and batch: 1050, loss is 3.8942689418792726 and perplexity is 49.120130563117264
At time: 988.8670170307159 and batch: 1100, loss is 3.843265109062195 and perplexity is 46.67763346072477
At time: 989.6853702068329 and batch: 1150, loss is 3.848119688034058 and perplexity is 46.90478463412984
At time: 990.4862785339355 and batch: 1200, loss is 3.870760970115662 and perplexity is 47.978882688552986
At time: 991.2850434780121 and batch: 1250, loss is 3.894527177810669 and perplexity is 49.13281678373203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324748463874315 and perplexity of 75.54650779371704
Finished 46 epochs...
Completing Train Step...
At time: 993.4962439537048 and batch: 50, loss is 3.9401530408859253 and perplexity is 51.42647105100443
At time: 994.3380837440491 and batch: 100, loss is 3.95897825717926 and perplexity is 52.40375542522489
At time: 995.1386725902557 and batch: 150, loss is 3.8822996282577513 and perplexity is 48.53570090353691
At time: 995.940197467804 and batch: 200, loss is 3.9318995237350465 and perplexity is 51.00376858077716
At time: 996.7387483119965 and batch: 250, loss is 3.951335973739624 and perplexity is 52.00479748951939
At time: 997.5366923809052 and batch: 300, loss is 3.9476422834396363 and perplexity is 51.81306219686741
At time: 998.3398385047913 and batch: 350, loss is 3.9367075204849242 and perplexity is 51.24958500308357
At time: 999.1394817829132 and batch: 400, loss is 3.94767915725708 and perplexity is 51.814972777489054
At time: 999.9382538795471 and batch: 450, loss is 3.8719287872314454 and perplexity is 48.03494597841601
At time: 1000.7404441833496 and batch: 500, loss is 3.9070568561553953 and perplexity is 49.752308082940495
At time: 1001.5416972637177 and batch: 550, loss is 3.8823675203323362 and perplexity is 48.53899620482384
At time: 1002.3444261550903 and batch: 600, loss is 3.915795760154724 and perplexity is 50.18899402620845
At time: 1003.1671533584595 and batch: 650, loss is 3.940861077308655 and perplexity is 51.46289575908931
At time: 1003.9717626571655 and batch: 700, loss is 3.909539804458618 and perplexity is 49.87599398117807
At time: 1004.7715933322906 and batch: 750, loss is 3.9016215085983275 and perplexity is 49.48262058333736
At time: 1005.5706925392151 and batch: 800, loss is 3.937348484992981 and perplexity is 51.28244469794726
At time: 1006.3722677230835 and batch: 850, loss is 3.9696259450912477 and perplexity is 52.964715422085796
At time: 1007.173150062561 and batch: 900, loss is 3.9146891260147094 and perplexity is 50.13348389233567
At time: 1008.0129487514496 and batch: 950, loss is 3.9108582448959353 and perplexity is 49.94179587690246
At time: 1008.862380027771 and batch: 1000, loss is 3.888618755340576 and perplexity is 48.84337525845397
At time: 1009.6589367389679 and batch: 1050, loss is 3.8943257331848145 and perplexity is 49.12292023867423
At time: 1010.4672446250916 and batch: 1100, loss is 3.8433332061767578 and perplexity is 46.68081218110768
At time: 1011.285653591156 and batch: 1150, loss is 3.848214039802551 and perplexity is 46.909210392296636
At time: 1012.1120524406433 and batch: 1200, loss is 3.870839114189148 and perplexity is 47.98263210038284
At time: 1012.9135961532593 and batch: 1250, loss is 3.8945806503295897 and perplexity is 49.13544410945137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324741781192975 and perplexity of 75.54600294216594
Finished 47 epochs...
Completing Train Step...
At time: 1015.1606545448303 and batch: 50, loss is 3.940043511390686 and perplexity is 51.420838644051244
At time: 1015.9815714359283 and batch: 100, loss is 3.9588446712493894 and perplexity is 52.396755488384635
At time: 1016.7964396476746 and batch: 150, loss is 3.8821578550338747 and perplexity is 48.52882032849888
At time: 1017.608350276947 and batch: 200, loss is 3.9317285871505736 and perplexity is 50.99505091588588
At time: 1018.4195144176483 and batch: 250, loss is 3.9511638927459716 and perplexity is 51.99584922222815
At time: 1019.2324976921082 and batch: 300, loss is 3.9474784755706787 and perplexity is 51.80457550467724
At time: 1020.0598886013031 and batch: 350, loss is 3.936550679206848 and perplexity is 51.24154758298691
At time: 1020.8666579723358 and batch: 400, loss is 3.947547364234924 and perplexity is 51.80814437561152
At time: 1021.6648180484772 and batch: 450, loss is 3.871801266670227 and perplexity is 48.02882092569012
At time: 1022.4750752449036 and batch: 500, loss is 3.9069510650634767 and perplexity is 49.74704501034093
At time: 1023.3104057312012 and batch: 550, loss is 3.882269287109375 and perplexity is 48.534228296974646
At time: 1024.1402611732483 and batch: 600, loss is 3.9157148599624634 and perplexity is 50.18493389117742
At time: 1024.9397718906403 and batch: 650, loss is 3.940797505378723 and perplexity is 51.45962426747464
At time: 1025.7393779754639 and batch: 700, loss is 3.9094793605804443 and perplexity is 49.87297937378227
At time: 1026.5651643276215 and batch: 750, loss is 3.9015841245651246 and perplexity is 49.480770757983684
At time: 1027.3774886131287 and batch: 800, loss is 3.937334818840027 and perplexity is 51.28174386900299
At time: 1028.1906440258026 and batch: 850, loss is 3.9696186876296995 and perplexity is 52.964331034095046
At time: 1028.992308139801 and batch: 900, loss is 3.9146923351287843 and perplexity is 50.1336447766626
At time: 1029.8343665599823 and batch: 950, loss is 3.910878105163574 and perplexity is 49.94278774418429
At time: 1030.6359868049622 and batch: 1000, loss is 3.888648719787598 and perplexity is 48.844838845111944
At time: 1031.4377295970917 and batch: 1050, loss is 3.89437002658844 and perplexity is 49.1250961081956
At time: 1032.240193605423 and batch: 1100, loss is 3.8433884716033937 and perplexity is 46.683392087397735
At time: 1033.0377814769745 and batch: 1150, loss is 3.848291263580322 and perplexity is 46.912833038610806
At time: 1033.8361628055573 and batch: 1200, loss is 3.8709008264541627 and perplexity is 47.98559330866161
At time: 1034.6365177631378 and batch: 1250, loss is 3.8946217107772827 and perplexity is 49.13746167420487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324733761975365 and perplexity of 75.54539712475787
Finished 48 epochs...
Completing Train Step...
At time: 1036.859946489334 and batch: 50, loss is 3.9399368381500244 and perplexity is 51.415353709108636
At time: 1037.6610701084137 and batch: 100, loss is 3.9587189388275146 and perplexity is 52.39016793156211
At time: 1038.458974123001 and batch: 150, loss is 3.882025294303894 and perplexity is 48.522387739014846
At time: 1039.2834680080414 and batch: 200, loss is 3.9315717220306396 and perplexity is 50.9870521984842
At time: 1040.132481098175 and batch: 250, loss is 3.9510071897506713 and perplexity is 51.98770195527909
At time: 1040.9683904647827 and batch: 300, loss is 3.947329273223877 and perplexity is 51.79684671702785
At time: 1041.77006649971 and batch: 350, loss is 3.936408829689026 and perplexity is 51.23427950966835
At time: 1042.5714814662933 and batch: 400, loss is 3.9474257326126097 and perplexity is 51.80184325017783
At time: 1043.37029504776 and batch: 450, loss is 3.8716831254959105 and perplexity is 48.023147079548956
At time: 1044.1996431350708 and batch: 500, loss is 3.9068526792526246 and perplexity is 49.74215084774212
At time: 1045.036943912506 and batch: 550, loss is 3.8821781158447264 and perplexity is 48.52980357170903
At time: 1045.842566728592 and batch: 600, loss is 3.9156391525268557 and perplexity is 50.181134662343126
At time: 1046.6596672534943 and batch: 650, loss is 3.9407371139526366 and perplexity is 51.45651664121718
At time: 1047.4980330467224 and batch: 700, loss is 3.909423518180847 and perplexity is 49.870194424698816
At time: 1048.325831413269 and batch: 750, loss is 3.9015482091903686 and perplexity is 49.478993669471286
At time: 1049.1258511543274 and batch: 800, loss is 3.937317757606506 and perplexity is 51.28086894665914
At time: 1049.9235491752625 and batch: 850, loss is 3.969606385231018 and perplexity is 52.9636794497868
At time: 1050.7515366077423 and batch: 900, loss is 3.9146894121170046 and perplexity is 50.13349823564253
At time: 1051.5513343811035 and batch: 950, loss is 3.9108896923065184 and perplexity is 49.94336644175763
At time: 1052.3533742427826 and batch: 1000, loss is 3.8886703205108644 and perplexity is 48.8458939403542
At time: 1053.1676967144012 and batch: 1050, loss is 3.89440438747406 and perplexity is 49.12678411900467
At time: 1053.9791376590729 and batch: 1100, loss is 3.843433656692505 and perplexity is 46.685501528286494
At time: 1054.7796504497528 and batch: 1150, loss is 3.848354907035828 and perplexity is 46.91581882842493
At time: 1055.5879681110382 and batch: 1200, loss is 3.8709501886367796 and perplexity is 47.98796204074391
At time: 1056.4004662036896 and batch: 1250, loss is 3.8946539688110353 and perplexity is 49.1390467776681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.324728861342382 and perplexity of 75.54502690540016
Finished 49 epochs...
Completing Train Step...
At time: 1058.7576787471771 and batch: 50, loss is 3.9398328733444212 and perplexity is 51.410008599711695
At time: 1059.5873675346375 and batch: 100, loss is 3.9585993671417237 and perplexity is 52.383903925370014
At time: 1060.3856155872345 and batch: 150, loss is 3.8819000625610354 and perplexity is 48.516311576302776
At time: 1061.2047979831696 and batch: 200, loss is 3.9314257860183717 and perplexity is 50.97961189432642
At time: 1062.0046923160553 and batch: 250, loss is 3.950862512588501 and perplexity is 51.980181066156
At time: 1062.8057644367218 and batch: 300, loss is 3.947191367149353 and perplexity is 51.78970410974015
At time: 1063.6066558361053 and batch: 350, loss is 3.936278395652771 and perplexity is 51.227597251603704
At time: 1064.4063851833344 and batch: 400, loss is 3.9473118591308594 and perplexity is 51.79594472977475
At time: 1065.2178483009338 and batch: 450, loss is 3.871572527885437 and perplexity is 48.017836127929236
At time: 1066.0163748264313 and batch: 500, loss is 3.9067599534988404 and perplexity is 49.73753868314644
At time: 1066.8156785964966 and batch: 550, loss is 3.882092399597168 and perplexity is 48.52564395732793
At time: 1067.6184570789337 and batch: 600, loss is 3.915567331314087 and perplexity is 50.17753072181481
At time: 1068.4350428581238 and batch: 650, loss is 3.9406789445877077 and perplexity is 51.45352353537709
At time: 1069.2406504154205 and batch: 700, loss is 3.9093707609176636 and perplexity is 49.8675634791279
At time: 1070.0541770458221 and batch: 750, loss is 3.9015129137039186 and perplexity is 49.47724731514006
At time: 1070.8941571712494 and batch: 800, loss is 3.937297987937927 and perplexity is 51.27985515089686
At time: 1071.7393646240234 and batch: 850, loss is 3.969589877128601 and perplexity is 52.96280512715878
At time: 1072.551346063614 and batch: 900, loss is 3.914681372642517 and perplexity is 50.13309519028264
At time: 1073.3529930114746 and batch: 950, loss is 3.9108946657180788 and perplexity is 49.94361483129133
At time: 1074.1795446872711 and batch: 1000, loss is 3.8886853790283205 and perplexity is 48.84662949263891
At time: 1074.9954743385315 and batch: 1050, loss is 3.8944311857223513 and perplexity is 49.128100648403496
At time: 1075.79599070549 and batch: 1100, loss is 3.843470516204834 and perplexity is 46.687222364820066
At time: 1076.5958364009857 and batch: 1150, loss is 3.8484075689315795 and perplexity is 46.91828956944156
At time: 1077.3966629505157 and batch: 1200, loss is 3.870989818572998 and perplexity is 47.98986383830265
At time: 1078.1953296661377 and batch: 1250, loss is 3.8946790361404418 and perplexity is 49.1402785777793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3247235151973085 and perplexity of 75.54462303180635
Finished Training.
Improved accuracyfrom -156.3131194155981 to -75.54462303180635
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f17ffca5e80>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 2.8037930852815878, 'num_layers': 1, 'dropout': 0.8024002944436802, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 16.941056461945752, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5206091403961182 and batch: 50, loss is 7.392888031005859 and perplexity is 1624.390635675665
At time: 2.3412859439849854 and batch: 100, loss is 6.715309925079346 and perplexity is 824.9393966903734
At time: 3.1456167697906494 and batch: 150, loss is 6.568406114578247 and perplexity is 712.2337190059459
At time: 3.940598249435425 and batch: 200, loss is 6.506750030517578 and perplexity is 669.6465464125455
At time: 4.737373352050781 and batch: 250, loss is 6.535026226043701 and perplexity is 688.8518499457324
At time: 5.532118082046509 and batch: 300, loss is 6.544906215667725 and perplexity is 695.6914309336902
At time: 6.326101541519165 and batch: 350, loss is 6.555079545974731 and perplexity is 702.8050529124698
At time: 7.126297950744629 and batch: 400, loss is 6.518518114089966 and perplexity is 677.5735542977188
At time: 7.923663377761841 and batch: 450, loss is 6.519051532745362 and perplexity is 677.9350810859784
At time: 8.775523662567139 and batch: 500, loss is 6.5436277675628665 and perplexity is 694.8025938293645
At time: 9.574157476425171 and batch: 550, loss is 6.546754636764526 and perplexity is 696.9785508548295
At time: 10.398745775222778 and batch: 600, loss is 6.580577049255371 and perplexity is 720.9552359203456
At time: 11.213266372680664 and batch: 650, loss is 6.568298225402832 and perplexity is 712.1568808423773
At time: 12.022195100784302 and batch: 700, loss is 6.583352108001709 and perplexity is 722.9587076436487
At time: 12.832364797592163 and batch: 750, loss is 6.511168155670166 and perplexity is 672.611673991733
At time: 13.641484022140503 and batch: 800, loss is 6.525924663543702 and perplexity is 682.6106671220771
At time: 14.450969219207764 and batch: 850, loss is 6.589507417678833 and perplexity is 727.4224661875458
At time: 15.261205196380615 and batch: 900, loss is 6.588941812515259 and perplexity is 727.0111486172062
At time: 16.071839094161987 and batch: 950, loss is 6.573136949539185 and perplexity is 715.6111619475979
At time: 16.89725923538208 and batch: 1000, loss is 6.5756520175933835 and perplexity is 717.4132379421935
At time: 17.718597173690796 and batch: 1050, loss is 6.5881757640838625 and perplexity is 726.4544361287893
At time: 18.553853750228882 and batch: 1100, loss is 6.600843925476074 and perplexity is 735.7158166443168
At time: 19.384475469589233 and batch: 1150, loss is 6.61818868637085 and perplexity is 748.5879408252617
At time: 20.207321166992188 and batch: 1200, loss is 6.593774318695068 and perplexity is 730.5329371644232
At time: 21.003307342529297 and batch: 1250, loss is 6.6124145030975345 and perplexity is 744.2779122812237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.164995847827327 and perplexity of 475.7991673309684
Finished 1 epochs...
Completing Train Step...
At time: 23.247414350509644 and batch: 50, loss is 6.552011232376099 and perplexity is 700.6519315264669
At time: 24.040119171142578 and batch: 100, loss is 6.68195125579834 and perplexity is 797.8744512394113
At time: 24.82885193824768 and batch: 150, loss is 6.725605144500732 and perplexity is 833.4761975051616
At time: 25.63865351676941 and batch: 200, loss is 6.753685264587403 and perplexity is 857.2120017309148
At time: 26.429797887802124 and batch: 250, loss is 6.803008527755737 and perplexity is 900.5525575410202
At time: 27.223663330078125 and batch: 300, loss is 6.887066535949707 and perplexity is 979.5238007336872
At time: 28.019498825073242 and batch: 350, loss is 6.921065120697022 and perplexity is 1013.3988119464885
At time: 28.809827089309692 and batch: 400, loss is 6.855659084320068 and perplexity is 949.2375508654768
At time: 29.604681253433228 and batch: 450, loss is 6.728314085006714 and perplexity is 835.7370958743343
At time: 30.3996422290802 and batch: 500, loss is 6.743507404327392 and perplexity is 848.5316663165491
At time: 31.192092418670654 and batch: 550, loss is 6.7860339069366455 and perplexity is 885.3950299437702
At time: 32.004066944122314 and batch: 600, loss is 6.844653644561768 and perplexity is 938.8480495626952
At time: 32.823736906051636 and batch: 650, loss is 6.845752687454223 and perplexity is 939.8804510615083
At time: 33.62011909484863 and batch: 700, loss is 6.874622259140015 and perplexity is 967.4098663221571
At time: 34.4118070602417 and batch: 750, loss is 6.814570121765136 and perplexity is 911.0248018618249
At time: 35.203359603881836 and batch: 800, loss is 6.846555404663086 and perplexity is 940.6352121631833
At time: 36.03874087333679 and batch: 850, loss is 6.91014347076416 and perplexity is 1002.391045783532
At time: 36.82868003845215 and batch: 900, loss is 6.866406545639038 and perplexity is 959.494463884508
At time: 37.63395023345947 and batch: 950, loss is 6.81008547782898 and perplexity is 906.948327608417
At time: 38.43632745742798 and batch: 1000, loss is 6.783744382858276 and perplexity is 883.3702155190281
At time: 39.24312138557434 and batch: 1050, loss is 6.786501760482788 and perplexity is 885.8093620640847
At time: 40.035627126693726 and batch: 1100, loss is 6.803073568344116 and perplexity is 900.6111319140641
At time: 40.82669758796692 and batch: 1150, loss is 6.863858461380005 and perplexity is 957.0527033713595
At time: 41.61751961708069 and batch: 1200, loss is 6.885482349395752 and perplexity is 977.9732807799888
At time: 42.40857696533203 and batch: 1250, loss is 6.898027038574218 and perplexity is 990.3189258506517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.469885610315922 and perplexity of 645.4098945083729
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 44.64281225204468 and batch: 50, loss is 6.788079452514649 and perplexity is 887.207999455781
At time: 45.43617248535156 and batch: 100, loss is 6.706244459152222 and perplexity is 817.4947323582863
At time: 46.228413581848145 and batch: 150, loss is 6.561338157653808 and perplexity is 707.217430099634
At time: 47.03454637527466 and batch: 200, loss is 6.548473491668701 and perplexity is 698.1775860436193
At time: 47.85955286026001 and batch: 250, loss is 6.545267534255982 and perplexity is 695.942842596496
At time: 48.68066215515137 and batch: 300, loss is 6.53519079208374 and perplexity is 688.9652208950995
At time: 49.47406363487244 and batch: 350, loss is 6.568616724014282 and perplexity is 712.3837379449784
At time: 50.26602220535278 and batch: 400, loss is 6.5488862133026124 and perplexity is 698.4657985093603
At time: 51.060617208480835 and batch: 450, loss is 6.576272258758545 and perplexity is 717.8583451874457
At time: 51.889567375183105 and batch: 500, loss is 6.491255378723144 and perplexity is 659.3505785597265
At time: 52.69068145751953 and batch: 550, loss is 6.468147668838501 and perplexity is 644.2891840294684
At time: 53.485525608062744 and batch: 600, loss is 6.4979541015625 and perplexity is 663.7822119095653
At time: 54.27852416038513 and batch: 650, loss is 6.449914741516113 and perplexity is 632.6483518735105
At time: 55.07136678695679 and batch: 700, loss is 6.470287275314331 and perplexity is 645.6691851431241
At time: 55.864413022994995 and batch: 750, loss is 6.402703924179077 and perplexity is 603.4745833128727
At time: 56.69397449493408 and batch: 800, loss is 6.416120328903198 and perplexity is 611.6255989609314
At time: 57.48790431022644 and batch: 850, loss is 6.454623413085938 and perplexity is 635.6343096118793
At time: 58.28221035003662 and batch: 900, loss is 6.436465253829956 and perplexity is 624.1965194663073
At time: 59.082637548446655 and batch: 950, loss is 6.399226016998291 and perplexity is 601.3794002641481
At time: 59.884581565856934 and batch: 1000, loss is 6.390930118560791 and perplexity is 596.4110547761657
At time: 60.6772358417511 and batch: 1050, loss is 6.390094900131226 and perplexity is 595.9131292388582
At time: 61.4697802066803 and batch: 1100, loss is 6.353307914733887 and perplexity is 574.389601730847
At time: 62.2632155418396 and batch: 1150, loss is 6.365040512084961 and perplexity is 581.1683721825842
At time: 63.09254026412964 and batch: 1200, loss is 6.333763980865479 and perplexity is 563.2727566344963
At time: 63.89019966125488 and batch: 1250, loss is 6.336297254562378 and perplexity is 564.7014896148265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.959413403142108 and perplexity of 387.3828200313028
Finished 3 epochs...
Completing Train Step...
At time: 66.14212036132812 and batch: 50, loss is 6.355333442687988 and perplexity is 575.5542230140001
At time: 66.97123289108276 and batch: 100, loss is 6.364098463058472 and perplexity is 580.6211408831703
At time: 67.79258012771606 and batch: 150, loss is 6.2943880748748775 and perplexity is 541.5243726154026
At time: 68.58636212348938 and batch: 200, loss is 6.3013183116912845 and perplexity is 545.2902990680204
At time: 69.37813448905945 and batch: 250, loss is 6.328013343811035 and perplexity is 560.0428752870773
At time: 70.17086148262024 and batch: 300, loss is 6.332975769042969 and perplexity is 562.8289533169072
At time: 70.96335983276367 and batch: 350, loss is 6.373009958267212 and perplexity is 585.8184669762163
At time: 71.75671434402466 and batch: 400, loss is 6.346818542480468 and perplexity is 570.6742420013047
At time: 72.55084490776062 and batch: 450, loss is 6.33599271774292 and perplexity is 564.5295434025411
At time: 73.34417915344238 and batch: 500, loss is 6.322022581100464 and perplexity is 556.6978210309193
At time: 74.13976669311523 and batch: 550, loss is 6.3206338119506835 and perplexity is 555.9252328687968
At time: 74.93674039840698 and batch: 600, loss is 6.338662729263306 and perplexity is 566.0388578338301
At time: 75.73056745529175 and batch: 650, loss is 6.312416067123413 and perplexity is 551.375501033758
At time: 76.52496194839478 and batch: 700, loss is 6.337835111618042 and perplexity is 565.5705878881452
At time: 77.35091853141785 and batch: 750, loss is 6.262063455581665 and perplexity is 524.2996938105557
At time: 78.14501166343689 and batch: 800, loss is 6.287689304351806 and perplexity is 537.9089480799823
At time: 78.9390161037445 and batch: 850, loss is 6.3245023822784425 and perplexity is 558.0800340416993
At time: 79.73464846611023 and batch: 900, loss is 6.313986101150513 and perplexity is 552.2418592600726
At time: 80.52985906600952 and batch: 950, loss is 6.286548862457275 and perplexity is 537.2958438514735
At time: 81.3263807296753 and batch: 1000, loss is 6.279864892959595 and perplexity is 533.7165500906364
At time: 82.12095808982849 and batch: 1050, loss is 6.283304882049561 and perplexity is 535.5556906982689
At time: 82.915030002594 and batch: 1100, loss is 6.277184638977051 and perplexity is 532.2879695163082
At time: 83.710369348526 and batch: 1150, loss is 6.3001735973358155 and perplexity is 544.6664545647808
At time: 84.52520227432251 and batch: 1200, loss is 6.282162141799927 and perplexity is 534.9440392005408
At time: 85.34502458572388 and batch: 1250, loss is 6.278205032348633 and perplexity is 532.8313898362708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.875090438954151 and perplexity of 356.05686062478543
Finished 4 epochs...
Completing Train Step...
At time: 87.57361125946045 and batch: 50, loss is 6.267060852050781 and perplexity is 526.9263850914555
At time: 88.36798524856567 and batch: 100, loss is 6.276531400680542 and perplexity is 531.9403721741578
At time: 89.16349339485168 and batch: 150, loss is 6.218680782318115 and perplexity is 502.04049427161095
At time: 89.96358251571655 and batch: 200, loss is 6.238899383544922 and perplexity is 512.2943609398956
At time: 90.76576972007751 and batch: 250, loss is 6.2654118251800535 and perplexity is 526.058185362626
At time: 91.58339500427246 and batch: 300, loss is 6.277861938476563 and perplexity is 532.6486100086837
At time: 92.37799668312073 and batch: 350, loss is 6.313462266921997 and perplexity is 551.9526518267626
At time: 93.20102047920227 and batch: 400, loss is 6.272270402908325 and perplexity is 529.6785975637425
At time: 93.99391913414001 and batch: 450, loss is 6.253338584899902 and perplexity is 519.7451446022643
At time: 94.78538179397583 and batch: 500, loss is 6.255552787780761 and perplexity is 520.8972408154343
At time: 95.57773876190186 and batch: 550, loss is 6.254627151489258 and perplexity is 520.4153025093951
At time: 96.39130783081055 and batch: 600, loss is 6.276420822143555 and perplexity is 531.8815542380987
At time: 97.22032189369202 and batch: 650, loss is 6.252516851425171 and perplexity is 519.3182280484501
At time: 98.01455760002136 and batch: 700, loss is 6.281348237991333 and perplexity is 534.5088233455971
At time: 98.81100511550903 and batch: 750, loss is 6.213911733627319 and perplexity is 499.6519388054102
At time: 99.60797548294067 and batch: 800, loss is 6.239598369598388 and perplexity is 512.6525727314006
At time: 100.40405178070068 and batch: 850, loss is 6.271803359985352 and perplexity is 529.4312726834632
At time: 101.20332527160645 and batch: 900, loss is 6.26458046913147 and perplexity is 525.6210254512649
At time: 102.02451753616333 and batch: 950, loss is 6.238895034790039 and perplexity is 512.292133102136
At time: 102.8445315361023 and batch: 1000, loss is 6.234431600570678 and perplexity is 510.01064628579593
At time: 103.67182445526123 and batch: 1050, loss is 6.240942831039429 and perplexity is 513.3422778851894
At time: 104.46679139137268 and batch: 1100, loss is 6.243313484191894 and perplexity is 514.5606780059784
At time: 105.26450085639954 and batch: 1150, loss is 6.276483383178711 and perplexity is 531.914830339595
At time: 106.06869626045227 and batch: 1200, loss is 6.26117730140686 and perplexity is 523.8352892454488
At time: 106.87228226661682 and batch: 1250, loss is 6.248509531021118 and perplexity is 517.2413177189235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.859773287807938 and perplexity of 350.6446395930656
Finished 5 epochs...
Completing Train Step...
At time: 109.0700011253357 and batch: 50, loss is 6.232927980422974 and perplexity is 509.24436024843226
At time: 109.88557934761047 and batch: 100, loss is 6.248610620498657 and perplexity is 517.2936080164482
At time: 110.67792010307312 and batch: 150, loss is 6.184435195922852 and perplexity is 485.13887779765804
At time: 111.47153496742249 and batch: 200, loss is 6.19500545501709 and perplexity is 490.2941195535241
At time: 112.26615762710571 and batch: 250, loss is 6.188718843460083 and perplexity is 487.22149918011786
At time: 113.05915641784668 and batch: 300, loss is 6.185494899749756 and perplexity is 485.65325381802296
At time: 113.8508448600769 and batch: 350, loss is 6.2230771541595455 and perplexity is 504.2525098224905
At time: 114.64151883125305 and batch: 400, loss is 6.18474458694458 and perplexity is 485.2889986325618
At time: 115.432204246521 and batch: 450, loss is 6.161315240859985 and perplexity is 474.05115644449756
At time: 116.22469830513 and batch: 500, loss is 6.1552307987213135 and perplexity is 471.1755766330921
At time: 117.01869416236877 and batch: 550, loss is 6.152041501998902 and perplexity is 469.67525167332093
At time: 117.83468699455261 and batch: 600, loss is 6.174043979644775 and perplexity is 480.1237963179102
At time: 118.62840723991394 and batch: 650, loss is 6.15290675163269 and perplexity is 470.0818138763648
At time: 119.43984627723694 and batch: 700, loss is 6.182137832641602 and perplexity is 484.02561682557666
At time: 120.26488256454468 and batch: 750, loss is 6.103650636672974 and perplexity is 447.4884094338381
At time: 121.06993460655212 and batch: 800, loss is 6.123748264312744 and perplexity is 456.5728469045069
At time: 121.86293411254883 and batch: 850, loss is 6.150961570739746 and perplexity is 469.1683084685624
At time: 122.66239833831787 and batch: 900, loss is 6.142262706756592 and perplexity is 465.1047768464242
At time: 123.45594215393066 and batch: 950, loss is 6.115488576889038 and perplexity is 452.81722936781597
At time: 124.26804828643799 and batch: 1000, loss is 6.1064816761016845 and perplexity is 448.75706171982665
At time: 125.10932683944702 and batch: 1050, loss is 6.1166329765319825 and perplexity is 453.3357298727321
At time: 125.91600465774536 and batch: 1100, loss is 6.108436660766602 and perplexity is 449.6352330197795
At time: 126.7081651687622 and batch: 1150, loss is 6.137975568771362 and perplexity is 463.1150765977344
At time: 127.50066494941711 and batch: 1200, loss is 6.121471710205078 and perplexity is 455.53461635684977
At time: 128.29528069496155 and batch: 1250, loss is 6.1061286354064945 and perplexity is 448.59866017752626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.748516890254334 and perplexity of 313.72502643383024
Finished 6 epochs...
Completing Train Step...
At time: 130.49794840812683 and batch: 50, loss is 6.088143568038941 and perplexity is 440.6027025035977
At time: 131.2917308807373 and batch: 100, loss is 6.103297605514526 and perplexity is 447.3304599644462
At time: 132.12370204925537 and batch: 150, loss is 6.041611566543579 and perplexity is 420.5702660085166
At time: 132.92646622657776 and batch: 200, loss is 6.0618698692321775 and perplexity is 429.17719226376647
At time: 133.7546660900116 and batch: 250, loss is 6.087367753982544 and perplexity is 440.2610092960442
At time: 134.55237412452698 and batch: 300, loss is 6.088903255462647 and perplexity is 440.93755000915587
At time: 135.37004590034485 and batch: 350, loss is 6.1282983493804934 and perplexity is 458.6550256504318
At time: 136.17765974998474 and batch: 400, loss is 6.0831702613830565 and perplexity is 438.4168900138196
At time: 136.98051404953003 and batch: 450, loss is 6.06261251449585 and perplexity is 429.49603705250934
At time: 137.78083491325378 and batch: 500, loss is 6.049262199401856 and perplexity is 423.80023460203796
At time: 138.60818529129028 and batch: 550, loss is 6.047308731079101 and perplexity is 422.9731623612901
At time: 139.40747475624084 and batch: 600, loss is 6.0800823402404784 and perplexity is 437.06518128893174
At time: 140.19915318489075 and batch: 650, loss is 6.066630449295044 and perplexity is 431.22519562282974
At time: 140.99183535575867 and batch: 700, loss is 6.094482269287109 and perplexity is 443.4044216533584
At time: 141.78578877449036 and batch: 750, loss is 6.031979303359986 and perplexity is 416.53867038524675
At time: 142.58107089996338 and batch: 800, loss is 6.05728705406189 and perplexity is 427.21485247105653
At time: 143.37769889831543 and batch: 850, loss is 6.087810449600219 and perplexity is 440.455954062906
At time: 144.17282009124756 and batch: 900, loss is 6.08095046043396 and perplexity is 437.4447711396444
At time: 144.97150373458862 and batch: 950, loss is 6.055059862136841 and perplexity is 426.2644217900163
At time: 145.8049192428589 and batch: 1000, loss is 6.04674129486084 and perplexity is 422.73322015200165
At time: 146.61388754844666 and batch: 1050, loss is 6.066401996612549 and perplexity is 431.1266923222303
At time: 147.4080102443695 and batch: 1100, loss is 6.058767404556274 and perplexity is 427.8477485275362
At time: 148.20031023025513 and batch: 1150, loss is 6.08178970336914 and perplexity is 437.8120476689142
At time: 148.99195861816406 and batch: 1200, loss is 6.0642479801177975 and perplexity is 430.19903776592514
At time: 149.78735780715942 and batch: 1250, loss is 6.050675344467163 and perplexity is 424.3995491717205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.715464348340556 and perplexity of 303.5251119735854
Finished 7 epochs...
Completing Train Step...
At time: 151.9816119670868 and batch: 50, loss is 6.037039289474487 and perplexity is 418.65169169440486
At time: 152.77645826339722 and batch: 100, loss is 6.06183198928833 and perplexity is 429.1609353637301
At time: 153.57211303710938 and batch: 150, loss is 5.992040767669677 and perplexity is 400.23055464346197
At time: 154.36588716506958 and batch: 200, loss is 6.016924810409546 and perplexity is 410.3148576264676
At time: 155.16041254997253 and batch: 250, loss is 6.0435976219177245 and perplexity is 421.40637184694657
At time: 155.97113156318665 and batch: 300, loss is 6.045207481384278 and perplexity is 422.0853232454648
At time: 156.77670097351074 and batch: 350, loss is 6.092241754531861 and perplexity is 442.4120795977325
At time: 157.57437658309937 and batch: 400, loss is 6.051880712509155 and perplexity is 424.9114152568637
At time: 158.3706817626953 and batch: 450, loss is 6.026508083343506 and perplexity is 414.26591870586077
At time: 159.1943747997284 and batch: 500, loss is 6.012047185897827 and perplexity is 408.3183688389504
At time: 159.98749136924744 and batch: 550, loss is 6.012611684799194 and perplexity is 408.5489291789748
At time: 160.79462051391602 and batch: 600, loss is 6.040873460769653 and perplexity is 420.25995520202133
At time: 161.59948539733887 and batch: 650, loss is 6.021459131240845 and perplexity is 412.17958126551304
At time: 162.3953218460083 and batch: 700, loss is 6.048863496780395 and perplexity is 423.6312980174941
At time: 163.1895661354065 and batch: 750, loss is 5.9914106750488285 and perplexity is 399.9784517567679
At time: 163.98342561721802 and batch: 800, loss is 6.011358318328857 and perplexity is 408.037188415999
At time: 164.77491664886475 and batch: 850, loss is 6.041967716217041 and perplexity is 420.72007864770114
At time: 165.56739020347595 and batch: 900, loss is 6.032406387329101 and perplexity is 416.71660536776955
At time: 166.36306071281433 and batch: 950, loss is 6.00885419845581 and perplexity is 407.01669263868405
At time: 167.16153001785278 and batch: 1000, loss is 5.995855388641357 and perplexity is 401.7601981609702
At time: 167.9577353000641 and batch: 1050, loss is 6.017309532165528 and perplexity is 410.47274504841465
At time: 168.75258588790894 and batch: 1100, loss is 6.006690664291382 and perplexity is 406.13705003021056
At time: 169.54583954811096 and batch: 1150, loss is 6.028530836105347 and perplexity is 415.10472429922504
At time: 170.3391978740692 and batch: 1200, loss is 6.0131637001037594 and perplexity is 408.77451669869873
At time: 171.13419723510742 and batch: 1250, loss is 6.004627799987793 and perplexity is 405.3001079531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.669005902144161 and perplexity of 289.74635494599335
Finished 8 epochs...
Completing Train Step...
At time: 173.31065678596497 and batch: 50, loss is 5.985572729110718 and perplexity is 397.65020188996795
At time: 174.128662109375 and batch: 100, loss is 6.014099960327148 and perplexity is 409.1574152373842
At time: 174.92181062698364 and batch: 150, loss is 5.9431852531433105 and perplexity is 381.1470478937784
At time: 175.71770906448364 and batch: 200, loss is 5.9747356510162355 and perplexity is 393.3641019450686
At time: 176.51274299621582 and batch: 250, loss is 5.99782769203186 and perplexity is 402.55337309565925
At time: 177.30953001976013 and batch: 300, loss is 6.001073522567749 and perplexity is 403.8621159561359
At time: 178.10194849967957 and batch: 350, loss is 6.045071792602539 and perplexity is 422.0280548875886
At time: 178.92354488372803 and batch: 400, loss is 6.00588544845581 and perplexity is 405.8101536748375
At time: 179.72758674621582 and batch: 450, loss is 5.9824685382843015 and perplexity is 396.4177336790674
At time: 180.52613759040833 and batch: 500, loss is 5.968508720397949 and perplexity is 390.92226145032686
At time: 181.32421231269836 and batch: 550, loss is 5.969802312850952 and perplexity is 391.4282827595592
At time: 182.1193459033966 and batch: 600, loss is 6.002474575042725 and perplexity is 404.4283445386005
At time: 182.91341853141785 and batch: 650, loss is 5.990211572647095 and perplexity is 399.49912407354594
At time: 183.7073953151703 and batch: 700, loss is 6.014297637939453 and perplexity is 409.23830449301926
At time: 184.5006239414215 and batch: 750, loss is 5.962772188186645 and perplexity is 388.6861432196717
At time: 185.29584383964539 and batch: 800, loss is 5.987278060913086 and perplexity is 398.32890596881253
At time: 186.09210467338562 and batch: 850, loss is 6.017046413421631 and perplexity is 410.364756182904
At time: 186.88617825508118 and batch: 900, loss is 6.008536720275879 and perplexity is 406.88749422982653
At time: 187.67889523506165 and batch: 950, loss is 5.9853990650177 and perplexity is 397.58115032438053
At time: 188.47323083877563 and batch: 1000, loss is 5.976625080108643 and perplexity is 394.10803810919055
At time: 189.26705193519592 and batch: 1050, loss is 5.995303812026978 and perplexity is 401.5386577349521
At time: 190.0627977848053 and batch: 1100, loss is 5.987085762023926 and perplexity is 398.2523151270774
At time: 190.85526633262634 and batch: 1150, loss is 6.00955602645874 and perplexity is 407.30244861528143
At time: 191.64866471290588 and batch: 1200, loss is 5.994224557876587 and perplexity is 401.1055292419529
At time: 192.44326758384705 and batch: 1250, loss is 5.985681858062744 and perplexity is 397.6935994076929
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.663082818915374 and perplexity of 288.035235723779
Finished 9 epochs...
Completing Train Step...
At time: 194.6429488658905 and batch: 50, loss is 5.96736707687378 and perplexity is 390.4762222394221
At time: 195.43900156021118 and batch: 100, loss is 5.994773483276367 and perplexity is 401.3257666963823
At time: 196.2330002784729 and batch: 150, loss is 5.925090293884278 and perplexity is 374.3122319397912
At time: 197.02681136131287 and batch: 200, loss is 5.954422569274902 and perplexity is 385.4542732648616
At time: 197.82154488563538 and batch: 250, loss is 5.977967519760131 and perplexity is 394.6374596452421
At time: 198.63313102722168 and batch: 300, loss is 5.975876150131225 and perplexity is 393.8129892843552
At time: 199.46527910232544 and batch: 350, loss is 6.024984521865845 and perplexity is 413.6352396717495
At time: 200.2735719680786 and batch: 400, loss is 5.988051242828369 and perplexity is 398.6370057684901
At time: 201.0820653438568 and batch: 450, loss is 5.969375896453857 and perplexity is 391.2614069033332
At time: 201.87803077697754 and batch: 500, loss is 5.953825521469116 and perplexity is 385.22420732379493
At time: 202.69133019447327 and batch: 550, loss is 5.958810634613037 and perplexity is 387.1493882184284
At time: 203.49459171295166 and batch: 600, loss is 5.990577116012573 and perplexity is 399.6451850220444
At time: 204.2891595363617 and batch: 650, loss is 5.979126081466675 and perplexity is 395.09493645033746
At time: 205.10681772232056 and batch: 700, loss is 6.004398069381714 and perplexity is 405.2070088079271
At time: 205.93478894233704 and batch: 750, loss is 5.9535087203979495 and perplexity is 385.10218721134714
At time: 206.7389271259308 and batch: 800, loss is 5.975387620925903 and perplexity is 393.6206471238614
At time: 207.57201981544495 and batch: 850, loss is 6.0041857433319095 and perplexity is 405.1209819375898
At time: 208.37898015975952 and batch: 900, loss is 5.994615859985352 and perplexity is 401.2625133934944
At time: 209.19206714630127 and batch: 950, loss is 5.975341138839721 and perplexity is 393.6023512402375
At time: 209.98740124702454 and batch: 1000, loss is 5.963638458251953 and perplexity is 389.02299627213563
At time: 210.78285217285156 and batch: 1050, loss is 5.985254316329956 and perplexity is 397.523605139495
At time: 211.59726643562317 and batch: 1100, loss is 5.975990505218506 and perplexity is 393.8580263781783
At time: 212.40381383895874 and batch: 1150, loss is 5.998123006820679 and perplexity is 402.672270615257
At time: 213.2086992263794 and batch: 1200, loss is 5.9830581569671635 and perplexity is 396.65153790195785
At time: 214.00766158103943 and batch: 1250, loss is 5.975261707305908 and perplexity is 393.5710880434242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.658131397553604 and perplexity of 286.6125769028709
Finished 10 epochs...
Completing Train Step...
At time: 216.21255040168762 and batch: 50, loss is 5.960398721694946 and perplexity is 387.7647036185262
At time: 217.0759584903717 and batch: 100, loss is 5.981672849655151 and perplexity is 396.10243405278436
At time: 217.91256093978882 and batch: 150, loss is 5.915792665481567 and perplexity is 370.84814475297486
At time: 218.71970081329346 and batch: 200, loss is 5.945499668121338 and perplexity is 382.03020192849846
At time: 219.52069568634033 and batch: 250, loss is 5.969152393341065 and perplexity is 391.17396853271276
At time: 220.37928915023804 and batch: 300, loss is 5.967185640335083 and perplexity is 390.40538201191293
At time: 221.19988584518433 and batch: 350, loss is 6.014572534561157 and perplexity is 409.3508181845046
At time: 222.00853180885315 and batch: 400, loss is 5.979138450622559 and perplexity is 395.0998234714193
At time: 222.80467462539673 and batch: 450, loss is 5.9525723552703855 and perplexity is 384.74175972489314
At time: 223.61069679260254 and batch: 500, loss is 5.942155609130859 and perplexity is 380.7548040884475
At time: 224.42837500572205 and batch: 550, loss is 5.947654781341552 and perplexity is 382.85440807697626
At time: 225.2268669605255 and batch: 600, loss is 5.980567750930786 and perplexity is 395.6649435378231
At time: 226.02945685386658 and batch: 650, loss is 5.970870456695557 and perplexity is 391.84660784630285
At time: 226.8437259197235 and batch: 700, loss is 5.995858764648437 and perplexity is 401.7615545085332
At time: 227.64117741584778 and batch: 750, loss is 5.946184282302856 and perplexity is 382.29183477106545
At time: 228.45398259162903 and batch: 800, loss is 5.965776872634888 and perplexity is 389.8557787423156
At time: 229.2603154182434 and batch: 850, loss is 5.995331563949585 and perplexity is 401.54980135933306
At time: 230.05634379386902 and batch: 900, loss is 5.985623092651367 and perplexity is 397.6702294664005
At time: 230.86369037628174 and batch: 950, loss is 5.966951627731323 and perplexity is 390.31403292078244
At time: 231.661274433136 and batch: 1000, loss is 5.956914949417114 and perplexity is 386.41617004956174
At time: 232.45820236206055 and batch: 1050, loss is 5.977593231201172 and perplexity is 394.4897789984722
At time: 233.25268650054932 and batch: 1100, loss is 5.968394374847412 and perplexity is 390.87756378466224
At time: 234.04522156715393 and batch: 1150, loss is 5.9907803630828855 and perplexity is 399.72641999016963
At time: 234.83932638168335 and batch: 1200, loss is 5.974263153076172 and perplexity is 393.1782821204038
At time: 235.63578581809998 and batch: 1250, loss is 5.9671886444091795 and perplexity is 390.40655482036965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.650397753193431 and perplexity of 284.4045661538351
Finished 11 epochs...
Completing Train Step...
At time: 237.83366346359253 and batch: 50, loss is 5.950960559844971 and perplexity is 384.1221342055342
At time: 238.6299684047699 and batch: 100, loss is 5.974784078598023 and perplexity is 393.38315207856
At time: 239.42926263809204 and batch: 150, loss is 5.907284498214722 and perplexity is 367.7062913668082
At time: 240.2364091873169 and batch: 200, loss is 5.935228776931763 and perplexity is 378.1264928959018
At time: 241.07662749290466 and batch: 250, loss is 5.9592436408996585 and perplexity is 387.3170626368073
At time: 241.87422227859497 and batch: 300, loss is 5.956573505401611 and perplexity is 386.2842530832156
At time: 242.6719045639038 and batch: 350, loss is 6.008033037185669 and perplexity is 406.6826034836987
At time: 243.46978759765625 and batch: 400, loss is 5.973217220306396 and perplexity is 392.76725905948507
At time: 244.26586961746216 and batch: 450, loss is 5.949087629318237 and perplexity is 383.403373438987
At time: 245.06212949752808 and batch: 500, loss is 5.934458208084107 and perplexity is 377.8352326324574
At time: 245.85932564735413 and batch: 550, loss is 5.937153091430664 and perplexity is 378.85482773649585
At time: 246.65703964233398 and batch: 600, loss is 5.971795024871827 and perplexity is 392.20906428191995
At time: 247.45100116729736 and batch: 650, loss is 5.9602851390838625 and perplexity is 387.7206627921865
At time: 248.24549508094788 and batch: 700, loss is 5.987391223907471 and perplexity is 398.373984611131
At time: 249.0429229736328 and batch: 750, loss is 5.938554306030273 and perplexity is 379.38605674825794
At time: 249.83903217315674 and batch: 800, loss is 5.959945850372314 and perplexity is 387.58913586213015
At time: 250.63712573051453 and batch: 850, loss is 5.9891582870483395 and perplexity is 399.0785589259323
At time: 251.43204069137573 and batch: 900, loss is 5.979998054504395 and perplexity is 395.439598828574
At time: 252.24338221549988 and batch: 950, loss is 5.959054412841797 and perplexity is 387.2437783152116
At time: 253.04300808906555 and batch: 1000, loss is 5.9489922618865965 and perplexity is 383.36681098744106
At time: 253.83929705619812 and batch: 1050, loss is 5.97257550239563 and perplexity is 392.51529412843524
At time: 254.63292980194092 and batch: 1100, loss is 5.96194149017334 and perplexity is 388.3633964837954
At time: 255.4246551990509 and batch: 1150, loss is 5.983764600753784 and perplexity is 396.93184891669193
At time: 256.2560706138611 and batch: 1200, loss is 5.9649381256103515 and perplexity is 389.52892546066755
At time: 257.0687634944916 and batch: 1250, loss is 5.958470935821533 and perplexity is 387.0178963741938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.644001090613595 and perplexity of 282.59113225376535
Finished 12 epochs...
Completing Train Step...
At time: 259.2939221858978 and batch: 50, loss is 5.944070959091187 and perplexity is 381.4847816454311
At time: 260.1147253513336 and batch: 100, loss is 5.962061796188355 and perplexity is 388.4101217470128
At time: 260.9083614349365 and batch: 150, loss is 5.896885194778442 and perplexity is 363.90221621609203
At time: 261.76273560523987 and batch: 200, loss is 5.926474123001099 and perplexity is 374.8305746712959
At time: 262.5670042037964 and batch: 250, loss is 5.948852882385254 and perplexity is 383.313381236087
At time: 263.3605420589447 and batch: 300, loss is 5.9499225902557376 and perplexity is 383.7236339630221
At time: 264.1556439399719 and batch: 350, loss is 6.000349521636963 and perplexity is 403.5698252304208
At time: 264.949590921402 and batch: 400, loss is 5.964123640060425 and perplexity is 389.2117889486866
At time: 265.7451021671295 and batch: 450, loss is 5.937843990325928 and perplexity is 379.11666856079677
At time: 266.5400469303131 and batch: 500, loss is 5.9253878402709965 and perplexity is 374.423623763205
At time: 267.3356263637543 and batch: 550, loss is 5.928868150711059 and perplexity is 375.72900445739833
At time: 268.13176107406616 and batch: 600, loss is 5.964716882705688 and perplexity is 389.4427544824585
At time: 268.9254472255707 and batch: 650, loss is 5.952792501449585 and perplexity is 384.82646847708685
At time: 269.74501967430115 and batch: 700, loss is 5.979389057159424 and perplexity is 395.19885047778496
At time: 270.5639898777008 and batch: 750, loss is 5.931250553131104 and perplexity is 376.62520928327865
At time: 271.4029121398926 and batch: 800, loss is 5.948122024536133 and perplexity is 383.0333359918354
At time: 272.2050738334656 and batch: 850, loss is 5.981210975646973 and perplexity is 395.91952687720595
At time: 273.0008943080902 and batch: 900, loss is 5.967798318862915 and perplexity is 390.64464829579424
At time: 273.7947998046875 and batch: 950, loss is 5.947929449081421 and perplexity is 382.9595802749853
At time: 274.59211921691895 and batch: 1000, loss is 5.939337711334229 and perplexity is 379.68338624690807
At time: 275.4070739746094 and batch: 1050, loss is 5.962420644760132 and perplexity is 388.5495271756892
At time: 276.2095413208008 and batch: 1100, loss is 5.9526376533508305 and perplexity is 384.7668834435265
At time: 277.00027227401733 and batch: 1150, loss is 5.9746841621398925 and perplexity is 393.34384859088146
At time: 277.79730892181396 and batch: 1200, loss is 5.955445594787598 and perplexity is 385.8488045937877
At time: 278.59428238868713 and batch: 1250, loss is 5.947999601364136 and perplexity is 382.9864467060887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.643025419137774 and perplexity of 282.3155506071426
Finished 13 epochs...
Completing Train Step...
At time: 280.807137966156 and batch: 50, loss is 5.93404860496521 and perplexity is 377.6805018340143
At time: 281.62665724754333 and batch: 100, loss is 5.95106614112854 and perplexity is 384.1626924545693
At time: 282.4216306209564 and batch: 150, loss is 5.887433290481567 and perplexity is 360.47885143960053
At time: 283.21701431274414 and batch: 200, loss is 5.919066076278686 and perplexity is 372.0640721032155
At time: 284.0540146827698 and batch: 250, loss is 5.94021954536438 and perplexity is 380.01835164782176
At time: 284.8613398075104 and batch: 300, loss is 5.939416942596435 and perplexity is 379.7134702326195
At time: 285.6861455440521 and batch: 350, loss is 5.988378467559815 and perplexity is 398.76747100020793
At time: 286.5152130126953 and batch: 400, loss is 5.955100708007812 and perplexity is 385.7157533872071
At time: 287.32283878326416 and batch: 450, loss is 5.928411483764648 and perplexity is 375.557460612438
At time: 288.12017035484314 and batch: 500, loss is 5.916065273284912 and perplexity is 370.9492546321352
At time: 288.9196443557739 and batch: 550, loss is 5.919858474731445 and perplexity is 372.3590119377788
At time: 289.71746373176575 and batch: 600, loss is 5.953646459579468 and perplexity is 385.15523452467744
At time: 290.5418722629547 and batch: 650, loss is 5.940188179016113 and perplexity is 380.0064320467942
At time: 291.3411455154419 and batch: 700, loss is 5.964905815124512 and perplexity is 389.5163397951629
At time: 292.1378152370453 and batch: 750, loss is 5.917878742218018 and perplexity is 371.6225699148171
At time: 292.93814873695374 and batch: 800, loss is 5.937993106842041 and perplexity is 379.17320533279127
At time: 293.73562359809875 and batch: 850, loss is 5.970424728393555 and perplexity is 391.6719896421712
At time: 294.5312876701355 and batch: 900, loss is 5.9565318775177 and perplexity is 386.2681732218594
At time: 295.3267066478729 and batch: 950, loss is 5.935708713531494 and perplexity is 378.30801319480383
At time: 296.12439036369324 and batch: 1000, loss is 5.925865268707275 and perplexity is 374.60242692786534
At time: 296.92284631729126 and batch: 1050, loss is 5.952763919830322 and perplexity is 384.8154696706653
At time: 297.7209994792938 and batch: 1100, loss is 5.944155883789063 and perplexity is 381.5171805009686
At time: 298.514675617218 and batch: 1150, loss is 5.963189630508423 and perplexity is 388.8484311363059
At time: 299.3158836364746 and batch: 1200, loss is 5.944720821380615 and perplexity is 381.73277479098226
At time: 300.11407566070557 and batch: 1250, loss is 5.940831823348999 and perplexity is 380.2510997642895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.643699478929061 and perplexity of 282.5059123186419
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 302.36429834365845 and batch: 50, loss is 5.912823410034179 and perplexity is 369.74863504941857
At time: 303.18554520606995 and batch: 100, loss is 5.902710676193237 and perplexity is 366.0283085565421
At time: 303.9856550693512 and batch: 150, loss is 5.812478246688843 and perplexity is 334.44694151034906
At time: 304.780722618103 and batch: 200, loss is 5.8465363311767575 and perplexity is 346.033756063714
At time: 305.57574105262756 and batch: 250, loss is 5.85695219039917 and perplexity is 349.6568309162907
At time: 306.38591265678406 and batch: 300, loss is 5.857195901870727 and perplexity is 349.74205668191996
At time: 307.18306970596313 and batch: 350, loss is 5.890689296722412 and perplexity is 361.6544857272379
At time: 307.9769639968872 and batch: 400, loss is 5.870770606994629 and perplexity is 354.52207221956974
At time: 308.7750177383423 and batch: 450, loss is 5.839421949386597 and perplexity is 343.58067623080063
At time: 309.570565700531 and batch: 500, loss is 5.819316797256469 and perplexity is 336.7419120241782
At time: 310.3647789955139 and batch: 550, loss is 5.831902141571045 and perplexity is 341.0067055879772
At time: 311.1711802482605 and batch: 600, loss is 5.862040452957153 and perplexity is 351.44051074447833
At time: 311.97342920303345 and batch: 650, loss is 5.836261157989502 and perplexity is 342.4964038672527
At time: 312.78129410743713 and batch: 700, loss is 5.8523938274383545 and perplexity is 348.0665953582602
At time: 313.61752343177795 and batch: 750, loss is 5.819635429382324 and perplexity is 336.84922591138894
At time: 314.43050742149353 and batch: 800, loss is 5.825770721435547 and perplexity is 338.922247082732
At time: 315.2266335487366 and batch: 850, loss is 5.85418514251709 and perplexity is 348.6906510722618
At time: 316.0234498977661 and batch: 900, loss is 5.842450160980224 and perplexity is 344.62268813790166
At time: 316.819256067276 and batch: 950, loss is 5.821868505477905 and perplexity is 337.6022763621349
At time: 317.6149935722351 and batch: 1000, loss is 5.812601480484009 and perplexity is 334.48815921588795
At time: 318.40884137153625 and batch: 1050, loss is 5.826081275939941 and perplexity is 339.0275172584205
At time: 319.2059051990509 and batch: 1100, loss is 5.796251726150513 and perplexity is 329.06382409166264
At time: 320.0030355453491 and batch: 1150, loss is 5.836287317276001 and perplexity is 342.50536344599385
At time: 320.80043506622314 and batch: 1200, loss is 5.832696733474731 and perplexity is 341.2777744356337
At time: 321.6220088005066 and batch: 1250, loss is 5.827382678985596 and perplexity is 339.4690159234829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.51165771484375 and perplexity of 247.56117293527285
Finished 15 epochs...
Completing Train Step...
At time: 323.800199508667 and batch: 50, loss is 5.833298454284668 and perplexity is 341.4831901697436
At time: 324.6627974510193 and batch: 100, loss is 5.8438406181335445 and perplexity is 345.10220451605164
At time: 325.4679629802704 and batch: 150, loss is 5.768288335800171 and perplexity is 319.98954897271375
At time: 326.2649247646332 and batch: 200, loss is 5.8055268669128415 and perplexity is 332.13013563307055
At time: 327.08032393455505 and batch: 250, loss is 5.820756921768188 and perplexity is 337.2272116678892
At time: 327.9066050052643 and batch: 300, loss is 5.823087368011475 and perplexity is 338.01401800529715
At time: 328.71302342414856 and batch: 350, loss is 5.855348281860351 and perplexity is 349.09646284922155
At time: 329.53317856788635 and batch: 400, loss is 5.828464670181274 and perplexity is 339.83651719063255
At time: 330.33252596855164 and batch: 450, loss is 5.795835485458374 and perplexity is 328.9268828400028
At time: 331.1269190311432 and batch: 500, loss is 5.791172103881836 and perplexity is 327.3965423287864
At time: 331.9277274608612 and batch: 550, loss is 5.8062747287750245 and perplexity is 332.3786159977051
At time: 332.7340941429138 and batch: 600, loss is 5.838465852737427 and perplexity is 343.25233688462293
At time: 333.5468051433563 and batch: 650, loss is 5.818287372589111 and perplexity is 336.39543995745004
At time: 334.35422015190125 and batch: 700, loss is 5.833345527648926 and perplexity is 341.499265310695
At time: 335.14967465400696 and batch: 750, loss is 5.797930974960327 and perplexity is 329.6168683461147
At time: 335.97302293777466 and batch: 800, loss is 5.812096738815308 and perplexity is 334.31937170487305
At time: 336.7770149707794 and batch: 850, loss is 5.8417846202850345 and perplexity is 344.3934040218786
At time: 337.5875794887543 and batch: 900, loss is 5.8330009651184085 and perplexity is 341.38161772930783
At time: 338.38507866859436 and batch: 950, loss is 5.816866807937622 and perplexity is 335.9179077495166
At time: 339.1798675060272 and batch: 1000, loss is 5.805279474258423 and perplexity is 332.0479792400671
At time: 339.97293043136597 and batch: 1050, loss is 5.818036394119263 and perplexity is 336.31102253858586
At time: 340.76987075805664 and batch: 1100, loss is 5.793478126525879 and perplexity is 328.15239734341424
At time: 341.56658267974854 and batch: 1150, loss is 5.838632984161377 and perplexity is 343.30970993074664
At time: 342.36452746391296 and batch: 1200, loss is 5.835379610061645 and perplexity is 342.19460991458635
At time: 343.18890738487244 and batch: 1250, loss is 5.825062303543091 and perplexity is 338.6822335237346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.507672163691834 and perplexity of 246.57646881510155
Finished 16 epochs...
Completing Train Step...
At time: 345.3870048522949 and batch: 50, loss is 5.821601238250732 and perplexity is 337.5120583945344
At time: 346.2208490371704 and batch: 100, loss is 5.833493375778199 and perplexity is 341.5497590708312
At time: 347.0295023918152 and batch: 150, loss is 5.758546524047851 and perplexity is 316.88740580478935
At time: 347.83600425720215 and batch: 200, loss is 5.795227098464966 and perplexity is 328.72682886398957
At time: 348.6455810070038 and batch: 250, loss is 5.811276302337647 and perplexity is 334.0451963842341
At time: 349.4537138938904 and batch: 300, loss is 5.814980344772339 and perplexity is 335.28480833710216
At time: 350.2606084346771 and batch: 350, loss is 5.848410291671753 and perplexity is 346.68281762042636
At time: 351.0826983451843 and batch: 400, loss is 5.821748685836792 and perplexity is 337.5618274018806
At time: 351.9005694389343 and batch: 450, loss is 5.790207357406616 and perplexity is 327.0808399794261
At time: 352.7015585899353 and batch: 500, loss is 5.785799102783203 and perplexity is 325.6421577230017
At time: 353.5180141925812 and batch: 550, loss is 5.80082576751709 and perplexity is 330.5724232024476
At time: 354.34456753730774 and batch: 600, loss is 5.833561058044434 and perplexity is 341.57287671487563
At time: 355.1640899181366 and batch: 650, loss is 5.814448099136353 and perplexity is 335.1064019432763
At time: 355.9635498523712 and batch: 700, loss is 5.828210897445679 and perplexity is 339.75028688992234
At time: 356.77156138420105 and batch: 750, loss is 5.792002286911011 and perplexity is 327.66845423474615
At time: 357.5992810726166 and batch: 800, loss is 5.808467864990234 and perplexity is 333.1083675073436
At time: 358.3964011669159 and batch: 850, loss is 5.838726816177368 and perplexity is 343.34192488431205
At time: 359.2006595134735 and batch: 900, loss is 5.830336437225342 and perplexity is 340.4732076661177
At time: 360.03280687332153 and batch: 950, loss is 5.815087871551514 and perplexity is 335.3208623710012
At time: 360.8422601222992 and batch: 1000, loss is 5.804207344055175 and perplexity is 331.69217134287527
At time: 361.63852286338806 and batch: 1050, loss is 5.8151397705078125 and perplexity is 335.33826562538496
At time: 362.4325830936432 and batch: 1100, loss is 5.791563510894775 and perplexity is 327.5247127132233
At time: 363.2280089855194 and batch: 1150, loss is 5.837984609603882 and perplexity is 343.0871887958031
At time: 364.0561902523041 and batch: 1200, loss is 5.834758262634278 and perplexity is 341.9820542163635
At time: 364.8498158454895 and batch: 1250, loss is 5.82224287033081 and perplexity is 337.728686448936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.506899200216697 and perplexity of 246.38594785323764
Finished 17 epochs...
Completing Train Step...
At time: 367.02871227264404 and batch: 50, loss is 5.816283664703369 and perplexity is 335.7220765987029
At time: 367.8530492782593 and batch: 100, loss is 5.826812992095947 and perplexity is 339.2756799513825
At time: 368.6490263938904 and batch: 150, loss is 5.75427414894104 and perplexity is 315.53643192910954
At time: 369.4469916820526 and batch: 200, loss is 5.790130710601806 and perplexity is 327.05577123885894
At time: 370.24514293670654 and batch: 250, loss is 5.805681352615356 and perplexity is 332.1814489538853
At time: 371.0446631908417 and batch: 300, loss is 5.809695539474487 and perplexity is 333.51756728128913
At time: 371.8398175239563 and batch: 350, loss is 5.844120960235596 and perplexity is 345.19896475583914
At time: 372.63690543174744 and batch: 400, loss is 5.818631896972656 and perplexity is 336.51135635589486
At time: 373.4338891506195 and batch: 450, loss is 5.786682977676391 and perplexity is 325.930111889359
At time: 374.22945642471313 and batch: 500, loss is 5.7819721889495845 and perplexity is 324.3983347644552
At time: 375.0246138572693 and batch: 550, loss is 5.797171173095703 and perplexity is 329.366519954565
At time: 375.81957840919495 and batch: 600, loss is 5.829891529083252 and perplexity is 340.32176205605407
At time: 376.61720180511475 and batch: 650, loss is 5.81069317817688 and perplexity is 333.8504633417233
At time: 377.41468477249146 and batch: 700, loss is 5.825561027526856 and perplexity is 338.85118460294376
At time: 378.21112728118896 and batch: 750, loss is 5.78956524848938 and perplexity is 326.8708858693351
At time: 379.00689029693604 and batch: 800, loss is 5.805487642288208 and perplexity is 332.1171082086704
At time: 379.8046054840088 and batch: 850, loss is 5.836551837921142 and perplexity is 342.59597516950447
At time: 380.632652759552 and batch: 900, loss is 5.828200359344482 and perplexity is 339.7467065858822
At time: 381.44494915008545 and batch: 950, loss is 5.8137581920623775 and perplexity is 334.8752893982979
At time: 382.25379371643066 and batch: 1000, loss is 5.801953125 and perplexity is 330.9453066443654
At time: 383.0589497089386 and batch: 1050, loss is 5.8131349086761475 and perplexity is 334.6666322269861
At time: 383.8903396129608 and batch: 1100, loss is 5.78931188583374 and perplexity is 326.7880794841058
At time: 384.6919801235199 and batch: 1150, loss is 5.837267971038818 and perplexity is 342.841407363878
At time: 385.5126781463623 and batch: 1200, loss is 5.833368492126465 and perplexity is 341.5071077529513
At time: 386.3129403591156 and batch: 1250, loss is 5.82012674331665 and perplexity is 337.0147652924185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.505475789091013 and perplexity of 246.0354888366547
Finished 18 epochs...
Completing Train Step...
At time: 388.4816517829895 and batch: 50, loss is 5.812985124588013 and perplexity is 334.6165082446302
At time: 389.3142738342285 and batch: 100, loss is 5.822707529067993 and perplexity is 337.8856514986586
At time: 390.11568331718445 and batch: 150, loss is 5.748971328735352 and perplexity is 313.86762755756513
At time: 390.9141309261322 and batch: 200, loss is 5.785680618286133 and perplexity is 325.6035764614047
At time: 391.7127068042755 and batch: 250, loss is 5.803337659835815 and perplexity is 331.4038292972105
At time: 392.52874994277954 and batch: 300, loss is 5.807152557373047 and perplexity is 332.6705155523429
At time: 393.3495547771454 and batch: 350, loss is 5.84003324508667 and perplexity is 343.79076982892303
At time: 394.14666223526 and batch: 400, loss is 5.815262756347656 and perplexity is 335.3795100198009
At time: 394.946790933609 and batch: 450, loss is 5.783733358383179 and perplexity is 324.97015858746755
At time: 395.76949071884155 and batch: 500, loss is 5.778917093276977 and perplexity is 323.40877917888594
At time: 396.59662318229675 and batch: 550, loss is 5.794204244613647 and perplexity is 328.39076126437396
At time: 397.4088442325592 and batch: 600, loss is 5.825661735534668 and perplexity is 338.88531134907936
At time: 398.20332050323486 and batch: 650, loss is 5.807601327896118 and perplexity is 332.81984177771585
At time: 398.999463558197 and batch: 700, loss is 5.821808767318726 and perplexity is 337.58210922599
At time: 399.79514360427856 and batch: 750, loss is 5.783478326797486 and perplexity is 324.8872914999315
At time: 400.59294295310974 and batch: 800, loss is 5.801411361694336 and perplexity is 330.7660611797356
At time: 401.3909878730774 and batch: 850, loss is 5.830683946609497 and perplexity is 340.5915458614608
At time: 402.2048387527466 and batch: 900, loss is 5.823070087432861 and perplexity is 338.0081769779549
At time: 403.0093867778778 and batch: 950, loss is 5.80758677482605 and perplexity is 332.81499826248245
At time: 403.8071041107178 and batch: 1000, loss is 5.794486770629883 and perplexity is 328.4835533053881
At time: 404.63279938697815 and batch: 1050, loss is 5.805195951461792 and perplexity is 332.02024682238596
At time: 405.4476499557495 and batch: 1100, loss is 5.783036594390869 and perplexity is 324.74380994733696
At time: 406.24717903137207 and batch: 1150, loss is 5.832921028137207 and perplexity is 341.3543298040206
At time: 407.0399503707886 and batch: 1200, loss is 5.827668647766114 and perplexity is 339.56610734588634
At time: 407.83576464653015 and batch: 1250, loss is 5.814604749679566 and perplexity is 335.1589006550433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.501051854043111 and perplexity of 244.94944787316356
Finished 19 epochs...
Completing Train Step...
At time: 410.0381691455841 and batch: 50, loss is 5.806317234039307 and perplexity is 332.39274413887796
At time: 410.83579659461975 and batch: 100, loss is 5.814870977401734 and perplexity is 335.24814112434535
At time: 411.6327168941498 and batch: 150, loss is 5.740631513595581 and perplexity is 311.260914426797
At time: 412.4291377067566 and batch: 200, loss is 5.777650833129883 and perplexity is 322.9995187003927
At time: 413.22616696357727 and batch: 250, loss is 5.7967063999176025 and perplexity is 329.2134747987226
At time: 414.0227544307709 and batch: 300, loss is 5.800767135620117 and perplexity is 330.5530416823818
At time: 414.82026958465576 and batch: 350, loss is 5.832902326583862 and perplexity is 341.34794600750587
At time: 415.6151223182678 and batch: 400, loss is 5.807936067581177 and perplexity is 332.9312684351554
At time: 416.41167545318604 and batch: 450, loss is 5.775634307861328 and perplexity is 322.3488382864429
At time: 417.2071957588196 and batch: 500, loss is 5.772939004898071 and perplexity is 321.48118033320327
At time: 418.00192761421204 and batch: 550, loss is 5.787033452987671 and perplexity is 326.044362366595
At time: 418.8193292617798 and batch: 600, loss is 5.820422983169555 and perplexity is 337.1146172862553
At time: 419.6325807571411 and batch: 650, loss is 5.8018824577331545 and perplexity is 330.92192047039777
At time: 420.42997217178345 and batch: 700, loss is 5.814781742095947 and perplexity is 335.21822648869846
At time: 421.224613904953 and batch: 750, loss is 5.777204637527466 and perplexity is 322.8554298838544
At time: 422.0180974006653 and batch: 800, loss is 5.796163854598999 and perplexity is 329.0349100132736
At time: 422.8156476020813 and batch: 850, loss is 5.827111883163452 and perplexity is 339.3771015777957
At time: 423.6129126548767 and batch: 900, loss is 5.818791131973267 and perplexity is 336.56494500841114
At time: 424.4087688922882 and batch: 950, loss is 5.802126455307007 and perplexity is 331.00267446761615
At time: 425.23103189468384 and batch: 1000, loss is 5.791482391357422 and perplexity is 327.49814513764557
At time: 426.0272789001465 and batch: 1050, loss is 5.802240447998047 and perplexity is 331.0404085038815
At time: 426.83514952659607 and batch: 1100, loss is 5.780856885910034 and perplexity is 324.036734000366
At time: 427.6326422691345 and batch: 1150, loss is 5.829801187515259 and perplexity is 340.29101824319093
At time: 428.42828488349915 and batch: 1200, loss is 5.8252670955657955 and perplexity is 338.7516000460048
At time: 429.220867395401 and batch: 1250, loss is 5.811008729934692 and perplexity is 333.9558270652571
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.497545228387318 and perplexity of 244.09200609716615
Finished 20 epochs...
Completing Train Step...
At time: 431.39170598983765 and batch: 50, loss is 5.8025009727478025 and perplexity is 331.12666395881854
At time: 432.21583437919617 and batch: 100, loss is 5.811560773849488 and perplexity is 334.140236243698
At time: 433.01317024230957 and batch: 150, loss is 5.73866039276123 and perplexity is 310.6479858300666
At time: 433.8103322982788 and batch: 200, loss is 5.773050575256348 and perplexity is 321.5170501046366
At time: 434.6079385280609 and batch: 250, loss is 5.792010765075684 and perplexity is 327.6712322736358
At time: 435.41839361190796 and batch: 300, loss is 5.797292900085449 and perplexity is 329.406615189849
At time: 436.22481322288513 and batch: 350, loss is 5.830956268310547 and perplexity is 340.68430896072215
At time: 437.034175157547 and batch: 400, loss is 5.806459150314331 and perplexity is 332.43991942636256
At time: 437.8425974845886 and batch: 450, loss is 5.773237257003784 and perplexity is 321.57707707217605
At time: 438.6499819755554 and batch: 500, loss is 5.770231504440307 and perplexity is 320.6119471459118
At time: 439.46994256973267 and batch: 550, loss is 5.785014972686768 and perplexity is 325.38691199247097
At time: 440.27029943466187 and batch: 600, loss is 5.816977882385254 and perplexity is 335.9552217178456
At time: 441.065470457077 and batch: 650, loss is 5.798744611740112 and perplexity is 329.88516588701117
At time: 441.86237597465515 and batch: 700, loss is 5.8149875831604 and perplexity is 335.28723526743954
At time: 442.6598792076111 and batch: 750, loss is 5.776942367553711 and perplexity is 322.77076570165394
At time: 443.45749855041504 and batch: 800, loss is 5.795682764053344 and perplexity is 328.87665250002505
At time: 444.25678992271423 and batch: 850, loss is 5.826326789855957 and perplexity is 339.110763450451
At time: 445.05937910079956 and batch: 900, loss is 5.816980285644531 and perplexity is 335.95602910631897
At time: 445.9065992832184 and batch: 950, loss is 5.80177734375 and perplexity is 330.8871377773299
At time: 446.70496225357056 and batch: 1000, loss is 5.788564300537109 and perplexity is 326.5438688162952
At time: 447.50195026397705 and batch: 1050, loss is 5.798337306976318 and perplexity is 329.75082944723357
At time: 448.2973165512085 and batch: 1100, loss is 5.777565488815307 and perplexity is 322.9719537041356
At time: 449.0951569080353 and batch: 1150, loss is 5.825019254684448 and perplexity is 338.6676539539579
At time: 449.89276337623596 and batch: 1200, loss is 5.819327325820923 and perplexity is 336.74545745176715
At time: 450.68898153305054 and batch: 1250, loss is 5.802869110107422 and perplexity is 331.2485864953313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.493729417341469 and perplexity of 243.16237190629226
Finished 21 epochs...
Completing Train Step...
At time: 452.88283467292786 and batch: 50, loss is 5.792923517227173 and perplexity is 327.970451431566
At time: 453.68342685699463 and batch: 100, loss is 5.804486265182495 and perplexity is 331.7847002007562
At time: 454.48673939704895 and batch: 150, loss is 5.73286169052124 and perplexity is 308.8518433394575
At time: 455.29444456100464 and batch: 200, loss is 5.764645338058472 and perplexity is 318.8259485521777
At time: 456.10159182548523 and batch: 250, loss is 5.785596914291382 and perplexity is 325.57632328196576
At time: 456.91492891311646 and batch: 300, loss is 5.789354209899902 and perplexity is 326.80191077709907
At time: 457.70998764038086 and batch: 350, loss is 5.824367589950562 and perplexity is 338.44702808222587
At time: 458.506370306015 and batch: 400, loss is 5.798632507324219 and perplexity is 329.84818637599943
At time: 459.34241223335266 and batch: 450, loss is 5.764482774734497 and perplexity is 318.7741233587637
At time: 460.1509997844696 and batch: 500, loss is 5.759864234924317 and perplexity is 317.3052470225316
At time: 460.9768304824829 and batch: 550, loss is 5.7769381618499756 and perplexity is 322.76940822629354
At time: 461.789847612381 and batch: 600, loss is 5.808915348052978 and perplexity is 333.25746121582813
At time: 462.58779096603394 and batch: 650, loss is 5.794154167175293 and perplexity is 328.3743167080247
At time: 463.3961968421936 and batch: 700, loss is 5.808694314956665 and perplexity is 333.18380842745796
At time: 464.20407819747925 and batch: 750, loss is 5.772413167953491 and perplexity is 321.3121780893028
At time: 465.0099878311157 and batch: 800, loss is 5.792036409378052 and perplexity is 327.6796352815376
At time: 465.8377501964569 and batch: 850, loss is 5.822542304992676 and perplexity is 337.8298292660421
At time: 466.6392855644226 and batch: 900, loss is 5.814253892898559 and perplexity is 335.04132850873236
At time: 467.44844102859497 and batch: 950, loss is 5.796615133285522 and perplexity is 329.1834299647085
At time: 468.24205565452576 and batch: 1000, loss is 5.782687692642212 and perplexity is 324.63052602780886
At time: 469.0648374557495 and batch: 1050, loss is 5.790916576385498 and perplexity is 327.3128941976692
At time: 469.89331698417664 and batch: 1100, loss is 5.771631221771241 and perplexity is 321.0610274642684
At time: 470.69466280937195 and batch: 1150, loss is 5.82026385307312 and perplexity is 337.060976472744
At time: 471.4909555912018 and batch: 1200, loss is 5.8170445728302 and perplexity is 335.9776274681803
At time: 472.3053345680237 and batch: 1250, loss is 5.800945014953613 and perplexity is 330.6118454669552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.487601844063641 and perplexity of 241.67693237165778
Finished 22 epochs...
Completing Train Step...
At time: 474.4946713447571 and batch: 50, loss is 5.789034175872803 and perplexity is 326.6973397795604
At time: 475.3426969051361 and batch: 100, loss is 5.800555658340454 and perplexity is 330.4831446154237
At time: 476.1448917388916 and batch: 150, loss is 5.728287124633789 and perplexity is 307.4422069282077
At time: 476.94053626060486 and batch: 200, loss is 5.7602857494354245 and perplexity is 317.4390239810861
At time: 477.73798847198486 and batch: 250, loss is 5.778646793365478 and perplexity is 323.3213736278845
At time: 478.5454225540161 and batch: 300, loss is 5.783746948242188 and perplexity is 324.9745749161134
At time: 479.372638463974 and batch: 350, loss is 5.818087577819824 and perplexity is 336.3282366217955
At time: 480.18026542663574 and batch: 400, loss is 5.7926665210723876 and perplexity is 327.8861751164731
At time: 480.9863340854645 and batch: 450, loss is 5.75823392868042 and perplexity is 316.78836375058773
At time: 481.7939236164093 and batch: 500, loss is 5.750854892730713 and perplexity is 314.459374441754
At time: 482.5990250110626 and batch: 550, loss is 5.768141374588013 and perplexity is 319.9425263760524
At time: 483.4161756038666 and batch: 600, loss is 5.801863145828247 and perplexity is 330.9155297994462
At time: 484.21078658103943 and batch: 650, loss is 5.786511039733886 and perplexity is 325.87407695392415
At time: 485.0049431324005 and batch: 700, loss is 5.802702693939209 and perplexity is 331.1934659614421
At time: 485.7999665737152 and batch: 750, loss is 5.768241262435913 and perplexity is 319.9744863426433
At time: 486.6238374710083 and batch: 800, loss is 5.782981090545654 and perplexity is 324.72578591738124
At time: 487.4226768016815 and batch: 850, loss is 5.812729091644287 and perplexity is 334.5308463616049
At time: 488.24464106559753 and batch: 900, loss is 5.80393720626831 and perplexity is 331.6025808552211
At time: 489.04045271873474 and batch: 950, loss is 5.78905990600586 and perplexity is 326.7057458537266
At time: 489.83683252334595 and batch: 1000, loss is 5.773574743270874 and perplexity is 321.68562323488817
At time: 490.631285905838 and batch: 1050, loss is 5.785866022109985 and perplexity is 325.6639502061294
At time: 491.4243485927582 and batch: 1100, loss is 5.766776962280273 and perplexity is 319.50629052575266
At time: 492.2177758216858 and batch: 1150, loss is 5.815673894882202 and perplexity is 335.51742580937236
At time: 493.0337836742401 and batch: 1200, loss is 5.8112709426879885 and perplexity is 334.04340602380927
At time: 493.832750082016 and batch: 1250, loss is 5.797051982879639 and perplexity is 329.32726502733544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.485879939838047 and perplexity of 241.26114591553866
Finished 23 epochs...
Completing Train Step...
At time: 496.03382992744446 and batch: 50, loss is 5.781765279769897 and perplexity is 324.3312207146131
At time: 496.8702929019928 and batch: 100, loss is 5.795170688629151 and perplexity is 328.7082859605514
At time: 497.6946909427643 and batch: 150, loss is 5.721619625091552 and perplexity is 305.3991547477337
At time: 498.49213767051697 and batch: 200, loss is 5.754146022796631 and perplexity is 315.4960060525271
At time: 499.28767490386963 and batch: 250, loss is 5.774290113449097 and perplexity is 321.9158298681761
At time: 500.084979057312 and batch: 300, loss is 5.7793177795410156 and perplexity is 323.5383905993962
At time: 500.88036918640137 and batch: 350, loss is 5.814253196716309 and perplexity is 335.0410952589874
At time: 501.69004130363464 and batch: 400, loss is 5.789481582641602 and perplexity is 326.8435390835669
At time: 502.49723291397095 and batch: 450, loss is 5.75460841178894 and perplexity is 315.6419216651301
At time: 503.2972295284271 and batch: 500, loss is 5.748818120956421 and perplexity is 313.8195442789289
At time: 504.0920057296753 and batch: 550, loss is 5.767040977478027 and perplexity is 319.5906561786466
At time: 504.8897020816803 and batch: 600, loss is 5.800265197753906 and perplexity is 330.38716622703873
At time: 505.68779277801514 and batch: 650, loss is 5.783426933288574 and perplexity is 324.8705948310744
At time: 506.50595355033875 and batch: 700, loss is 5.796815214157104 and perplexity is 329.24929986171963
At time: 507.32995772361755 and batch: 750, loss is 5.760825576782227 and perplexity is 317.6104325085161
At time: 508.126784324646 and batch: 800, loss is 5.778225717544555 and perplexity is 323.1852594742628
At time: 508.9256896972656 and batch: 850, loss is 5.809363603591919 and perplexity is 333.40687920493014
At time: 509.7276358604431 and batch: 900, loss is 5.798615865707397 and perplexity is 329.84269721454694
At time: 510.52249026298523 and batch: 950, loss is 5.7856738471984865 and perplexity is 325.6013717785147
At time: 511.3196544647217 and batch: 1000, loss is 5.770484972000122 and perplexity is 320.6932221736587
At time: 512.1149671077728 and batch: 1050, loss is 5.7830119323730464 and perplexity is 324.7358012084645
At time: 512.9107394218445 and batch: 1100, loss is 5.762164478302002 and perplexity is 318.03596640972194
At time: 513.7085630893707 and batch: 1150, loss is 5.811235628128052 and perplexity is 334.0316096362191
At time: 514.5037693977356 and batch: 1200, loss is 5.806030750274658 and perplexity is 332.29753265311876
At time: 515.2978813648224 and batch: 1250, loss is 5.792682037353516 and perplexity is 327.89126273001466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.482454842894617 and perplexity of 240.43621663992195
Finished 24 epochs...
Completing Train Step...
At time: 517.4993958473206 and batch: 50, loss is 5.777691993713379 and perplexity is 323.012813822667
At time: 518.2986724376678 and batch: 100, loss is 5.790228977203369 and perplexity is 327.0879114771501
At time: 519.1014356613159 and batch: 150, loss is 5.716152000427246 and perplexity is 303.73390342990956
At time: 519.9037590026855 and batch: 200, loss is 5.748912296295166 and perplexity is 313.84909973249177
At time: 520.7049398422241 and batch: 250, loss is 5.771339836120606 and perplexity is 320.96748851650267
At time: 521.5069401264191 and batch: 300, loss is 5.777016706466675 and perplexity is 322.7947610213947
At time: 522.3076889514923 and batch: 350, loss is 5.811547451019287 and perplexity is 334.1357845797217
At time: 523.109480381012 and batch: 400, loss is 5.786345481872559 and perplexity is 325.8201304044424
At time: 523.911860704422 and batch: 450, loss is 5.752436304092408 and perplexity is 314.9570574863786
At time: 524.7125449180603 and batch: 500, loss is 5.747801446914673 and perplexity is 313.5006542255549
At time: 525.5065307617188 and batch: 550, loss is 5.766436157226562 and perplexity is 319.3974197201631
At time: 526.3047139644623 and batch: 600, loss is 5.797390069961548 and perplexity is 329.43862514500944
At time: 527.0996248722076 and batch: 650, loss is 5.781488018035889 and perplexity is 324.241308543143
At time: 527.9228098392487 and batch: 700, loss is 5.793995065689087 and perplexity is 328.32207602210013
At time: 528.7191536426544 and batch: 750, loss is 5.759161825180054 and perplexity is 317.0824469827371
At time: 529.7032940387726 and batch: 800, loss is 5.775921812057495 and perplexity is 322.44152825381883
At time: 530.5005729198456 and batch: 850, loss is 5.806324434280396 and perplexity is 332.3951374553881
At time: 531.3450906276703 and batch: 900, loss is 5.795740976333618 and perplexity is 328.89579771713375
At time: 532.1566913127899 and batch: 950, loss is 5.78149866104126 and perplexity is 324.2447594634954
At time: 532.9661440849304 and batch: 1000, loss is 5.767766580581665 and perplexity is 319.8226363032347
At time: 533.7679152488708 and batch: 1050, loss is 5.779905815124511 and perplexity is 323.7286986341599
At time: 534.578861951828 and batch: 1100, loss is 5.7599668788909915 and perplexity is 317.33781816331964
At time: 535.386969089508 and batch: 1150, loss is 5.8074415588378905 and perplexity is 332.7666717126142
At time: 536.2043154239655 and batch: 1200, loss is 5.804678544998169 and perplexity is 331.84850183544097
At time: 537.008975982666 and batch: 1250, loss is 5.789792261123657 and perplexity is 326.9450981134447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.479319774321396 and perplexity of 239.68361296176306
Finished 25 epochs...
Completing Train Step...
At time: 539.2335722446442 and batch: 50, loss is 5.77389163017273 and perplexity is 321.787577348515
At time: 540.0611982345581 and batch: 100, loss is 5.78735993385315 and perplexity is 326.15082699059076
At time: 540.8615925312042 and batch: 150, loss is 5.714766960144043 and perplexity is 303.31351093575887
At time: 541.6600685119629 and batch: 200, loss is 5.746741018295288 and perplexity is 313.1683853644596
At time: 542.475688457489 and batch: 250, loss is 5.768900060653687 and perplexity is 320.1853544161078
At time: 543.2871854305267 and batch: 300, loss is 5.7730233860015865 and perplexity is 321.50830841449175
At time: 544.0911254882812 and batch: 350, loss is 5.808069667816162 and perplexity is 332.9757511022335
At time: 544.9200809001923 and batch: 400, loss is 5.784408197402954 and perplexity is 325.18953514434986
At time: 545.7274076938629 and batch: 450, loss is 5.750917892456055 and perplexity is 314.4791859200274
At time: 546.5304107666016 and batch: 500, loss is 5.744436092376709 and perplexity is 312.44738667963463
At time: 547.327262878418 and batch: 550, loss is 5.762176933288575 and perplexity is 318.0399275680813
At time: 548.1570608615875 and batch: 600, loss is 5.792978525161743 and perplexity is 327.98849290490676
At time: 548.9531936645508 and batch: 650, loss is 5.776550674438477 and perplexity is 322.64436337200704
At time: 549.748429775238 and batch: 700, loss is 5.789985675811767 and perplexity is 327.00834021340376
At time: 550.5456874370575 and batch: 750, loss is 5.755299415588379 and perplexity is 315.86010680696216
At time: 551.3423273563385 and batch: 800, loss is 5.7682765865325925 and perplexity is 319.985789351967
At time: 552.141193151474 and batch: 850, loss is 5.801209659576416 and perplexity is 330.6993516926082
At time: 552.9390239715576 and batch: 900, loss is 5.792534608840942 and perplexity is 327.8429257720748
At time: 553.7326786518097 and batch: 950, loss is 5.779066133499145 and perplexity is 323.456983687302
At time: 554.5260484218597 and batch: 1000, loss is 5.765154161453247 and perplexity is 318.9882159328813
At time: 555.323207616806 and batch: 1050, loss is 5.77662239074707 and perplexity is 322.6675030644731
At time: 556.1195850372314 and batch: 1100, loss is 5.754686861038208 and perplexity is 315.66668450822283
At time: 556.9176297187805 and batch: 1150, loss is 5.80241904258728 and perplexity is 331.0995358094091
At time: 557.7160613536835 and batch: 1200, loss is 5.800194215774536 and perplexity is 330.36371552432007
At time: 558.5112717151642 and batch: 1250, loss is 5.786936740875245 and perplexity is 326.0128314523012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.479313537152144 and perplexity of 239.68211801916436
Finished 26 epochs...
Completing Train Step...
At time: 560.6952831745148 and batch: 50, loss is 5.77065110206604 and perplexity is 320.74650338548093
At time: 561.5157804489136 and batch: 100, loss is 5.7840117168426515 and perplexity is 325.06062937125637
At time: 562.304755449295 and batch: 150, loss is 5.70984938621521 and perplexity is 301.8256057624047
At time: 563.0936238765717 and batch: 200, loss is 5.743469438552856 and perplexity is 312.1455041500058
At time: 563.919267654419 and batch: 250, loss is 5.763346300125122 and perplexity is 318.41205044374846
At time: 564.714540719986 and batch: 300, loss is 5.766866121292114 and perplexity is 319.5347786608611
At time: 565.5129675865173 and batch: 350, loss is 5.8036257266998295 and perplexity is 331.49930951072105
At time: 566.3276867866516 and batch: 400, loss is 5.778467521667481 and perplexity is 323.2634164514313
At time: 567.1221175193787 and batch: 450, loss is 5.745813627243042 and perplexity is 312.87809043529177
At time: 567.9167861938477 and batch: 500, loss is 5.7391446018218994 and perplexity is 310.79844082237986
At time: 568.7123427391052 and batch: 550, loss is 5.756917200088501 and perplexity is 316.37151395369995
At time: 569.5129010677338 and batch: 600, loss is 5.786975860595703 and perplexity is 326.02558523259404
At time: 570.3080277442932 and batch: 650, loss is 5.771544456481934 and perplexity is 321.0331717198086
At time: 571.1029286384583 and batch: 700, loss is 5.785586309432984 and perplexity is 325.57287060946703
At time: 571.8990752696991 and batch: 750, loss is 5.751719150543213 and perplexity is 314.7312658880586
At time: 572.6938545703888 and batch: 800, loss is 5.76892879486084 and perplexity is 320.1945548205916
At time: 573.4912569522858 and batch: 850, loss is 5.80015926361084 and perplexity is 330.3521687994483
At time: 574.2882199287415 and batch: 900, loss is 5.787583427429199 and perplexity is 326.22372775136876
At time: 575.0847208499908 and batch: 950, loss is 5.770636081695557 and perplexity is 320.7416856903507
At time: 575.8818881511688 and batch: 1000, loss is 5.756745100021362 and perplexity is 316.3170710798533
At time: 576.6993231773376 and batch: 1050, loss is 5.768733720779419 and perplexity is 320.13209925386326
At time: 577.4967904090881 and batch: 1100, loss is 5.744528932571411 and perplexity is 312.4763957024289
At time: 578.2930915355682 and batch: 1150, loss is 5.791989774703979 and perplexity is 327.66435440485816
At time: 579.0891461372375 and batch: 1200, loss is 5.791391878128052 and perplexity is 327.46850356441394
At time: 579.8961887359619 and batch: 1250, loss is 5.778663835525513 and perplexity is 323.3268837694286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.469813437357436 and perplexity of 237.41589569231905
Finished 27 epochs...
Completing Train Step...
At time: 582.1470711231232 and batch: 50, loss is 5.7636676788330075 and perplexity is 318.51439774233546
At time: 582.9850168228149 and batch: 100, loss is 5.776939353942871 and perplexity is 322.7697929976413
At time: 583.7822670936584 and batch: 150, loss is 5.701233043670654 and perplexity is 299.2361448161023
At time: 584.6078617572784 and batch: 200, loss is 5.734449367523194 and perplexity is 309.34258977846997
At time: 585.4026763439178 and batch: 250, loss is 5.756278676986694 and perplexity is 316.16956791375424
At time: 586.1986227035522 and batch: 300, loss is 5.762110023498535 and perplexity is 318.0186482952074
At time: 586.9951636791229 and batch: 350, loss is 5.798514080047608 and perplexity is 329.8091256665642
At time: 587.7918417453766 and batch: 400, loss is 5.770398950576782 and perplexity is 320.66563687271224
At time: 588.5862658023834 and batch: 450, loss is 5.738781967163086 and perplexity is 310.68575496896534
At time: 589.3794407844543 and batch: 500, loss is 5.732652683258056 and perplexity is 308.7872978064294
At time: 590.1686024665833 and batch: 550, loss is 5.7513900566101075 and perplexity is 314.6277067791681
At time: 590.9880673885345 and batch: 600, loss is 5.780160751342773 and perplexity is 323.81123932519137
At time: 591.7832322120667 and batch: 650, loss is 5.763683805465698 and perplexity is 318.51953434845245
At time: 592.5791938304901 and batch: 700, loss is 5.7788991355896 and perplexity is 323.40297155728035
At time: 593.3789286613464 and batch: 750, loss is 5.743975219726562 and perplexity is 312.3034214018818
At time: 594.1778314113617 and batch: 800, loss is 5.75997260093689 and perplexity is 317.3396339900756
At time: 594.9773442745209 and batch: 850, loss is 5.791882514953613 and perplexity is 327.62921109276596
At time: 595.7756667137146 and batch: 900, loss is 5.779621315002442 and perplexity is 323.6366108799908
At time: 596.5730941295624 and batch: 950, loss is 5.76321403503418 and perplexity is 318.36993842997526
At time: 597.3846318721771 and batch: 1000, loss is 5.750774641036987 and perplexity is 314.4341395569317
At time: 598.1852459907532 and batch: 1050, loss is 5.763536214828491 and perplexity is 318.4725273163994
At time: 598.977756023407 and batch: 1100, loss is 5.740391645431519 and perplexity is 311.1862617964618
At time: 599.7799699306488 and batch: 1150, loss is 5.789333353042602 and perplexity is 326.79509478736094
At time: 600.5720155239105 and batch: 1200, loss is 5.787528305053711 and perplexity is 326.20574602015694
At time: 601.3688282966614 and batch: 1250, loss is 5.776083068847656 and perplexity is 322.49352833224987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.469142941662865 and perplexity of 237.25676271138562
Finished 28 epochs...
Completing Train Step...
At time: 603.5736587047577 and batch: 50, loss is 5.760714406967163 and perplexity is 317.57512577802953
At time: 604.3706955909729 and batch: 100, loss is 5.775301542282104 and perplexity is 322.24158953389485
At time: 605.1942713260651 and batch: 150, loss is 5.697765684127807 and perplexity is 298.20038223216085
At time: 605.9887688159943 and batch: 200, loss is 5.730097999572754 and perplexity is 307.9994507130371
At time: 606.7801027297974 and batch: 250, loss is 5.750229940414429 and perplexity is 314.2629137230043
At time: 607.5700566768646 and batch: 300, loss is 5.75849723815918 and perplexity is 316.87178811225454
At time: 608.365175485611 and batch: 350, loss is 5.794173460006714 and perplexity is 328.3806520394731
At time: 609.1579251289368 and batch: 400, loss is 5.765996055603027 and perplexity is 319.25688332460777
At time: 609.9536142349243 and batch: 450, loss is 5.735069599151611 and perplexity is 309.53451334904895
At time: 610.7493858337402 and batch: 500, loss is 5.7287286853790285 and perplexity is 307.57799131453805
At time: 611.5461928844452 and batch: 550, loss is 5.747661991119385 and perplexity is 313.45693779082234
At time: 612.3590025901794 and batch: 600, loss is 5.775692882537842 and perplexity is 322.36772031836944
At time: 613.1615128517151 and batch: 650, loss is 5.7596272945404055 and perplexity is 317.2300735016581
At time: 613.9632315635681 and batch: 700, loss is 5.7730508804321286 and perplexity is 321.51714822386833
At time: 614.7641656398773 and batch: 750, loss is 5.7376549243927 and perplexity is 310.33579608132675
At time: 615.5591464042664 and batch: 800, loss is 5.753120613098145 and perplexity is 315.1726591979199
At time: 616.3571248054504 and batch: 850, loss is 5.785346651077271 and perplexity is 325.4948536997077
At time: 617.1758575439453 and batch: 900, loss is 5.774405450820923 and perplexity is 321.95296093520085
At time: 617.9715826511383 and batch: 950, loss is 5.757539367675781 and perplexity is 316.5684113004428
At time: 618.7686331272125 and batch: 1000, loss is 5.745010643005371 and perplexity is 312.62695510271067
At time: 619.5637676715851 and batch: 1050, loss is 5.757615375518799 and perplexity is 316.5924738970147
At time: 620.3668015003204 and batch: 1100, loss is 5.73325436592102 and perplexity is 308.9731456751766
At time: 621.1839926242828 and batch: 1150, loss is 5.7801746654510495 and perplexity is 323.81574490118186
At time: 621.9803292751312 and batch: 1200, loss is 5.779820070266724 and perplexity is 323.70094175295577
At time: 622.7774136066437 and batch: 1250, loss is 5.7693466091156 and perplexity is 320.3283646217668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.462672324076186 and perplexity of 235.72652107337728
Finished 29 epochs...
Completing Train Step...
At time: 624.9752020835876 and batch: 50, loss is 5.754248781204224 and perplexity is 315.52842758547496
At time: 625.7711074352264 and batch: 100, loss is 5.766915884017944 and perplexity is 319.5506799780878
At time: 626.5689063072205 and batch: 150, loss is 5.687775659561157 and perplexity is 295.2361839470753
At time: 627.36492228508 and batch: 200, loss is 5.722488842010498 and perplexity is 305.66472826384796
At time: 628.1628861427307 and batch: 250, loss is 5.744537630081177 and perplexity is 312.479113480751
At time: 628.9676613807678 and batch: 300, loss is 5.751957902908325 and perplexity is 314.8064176931427
At time: 629.772260427475 and batch: 350, loss is 5.788405113220215 and perplexity is 326.4918913111595
At time: 630.5708453655243 and batch: 400, loss is 5.7572918796539305 and perplexity is 316.49007410470836
At time: 631.3699805736542 and batch: 450, loss is 5.725163440704346 and perplexity is 306.4833530051487
At time: 632.1641986370087 and batch: 500, loss is 5.7200110816955565 and perplexity is 304.9083018391603
At time: 632.9578018188477 and batch: 550, loss is 5.738554372787475 and perplexity is 310.6150526845784
At time: 633.7543976306915 and batch: 600, loss is 5.767355813980102 and perplexity is 319.6912908238324
At time: 634.548410654068 and batch: 650, loss is 5.753041677474975 and perplexity is 315.14778182952836
At time: 635.3445026874542 and batch: 700, loss is 5.7639445781707765 and perplexity is 318.6026063800333
At time: 636.1419134140015 and batch: 750, loss is 5.730063066482544 and perplexity is 307.98869152836767
At time: 636.9399065971375 and batch: 800, loss is 5.744629898071289 and perplexity is 312.5079466306719
At time: 637.7483592033386 and batch: 850, loss is 5.776995029449463 and perplexity is 322.787763869643
At time: 638.5445675849915 and batch: 900, loss is 5.767160310745239 and perplexity is 319.62879625146326
At time: 639.3414890766144 and batch: 950, loss is 5.7481153869628905 and perplexity is 313.59909008672963
At time: 640.1393485069275 and batch: 1000, loss is 5.738092336654663 and perplexity is 310.47157045638227
At time: 640.9350531101227 and batch: 1050, loss is 5.7492210388183596 and perplexity is 313.9460132553382
At time: 641.7298762798309 and batch: 1100, loss is 5.726012077331543 and perplexity is 306.74355639758335
At time: 642.525666475296 and batch: 1150, loss is 5.771642370223999 and perplexity is 321.0646068179177
At time: 643.3220386505127 and batch: 1200, loss is 5.7727405548095705 and perplexity is 321.4173886944523
At time: 644.1169254779816 and batch: 1250, loss is 5.763198118209839 and perplexity is 318.3648710319186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.454954272639142 and perplexity of 233.9141745470409
Finished 30 epochs...
Completing Train Step...
At time: 646.2975282669067 and batch: 50, loss is 5.748498163223267 and perplexity is 313.71915135052524
At time: 647.1266984939575 and batch: 100, loss is 5.761363220214844 and perplexity is 317.78123958422174
At time: 647.9260387420654 and batch: 150, loss is 5.680911760330201 and perplexity is 293.2166513939654
At time: 648.7273168563843 and batch: 200, loss is 5.714634885787964 and perplexity is 303.2734536444406
At time: 649.5273461341858 and batch: 250, loss is 5.735224094390869 and perplexity is 309.58233865204306
At time: 650.3292315006256 and batch: 300, loss is 5.743892498016358 and perplexity is 312.2775881972589
At time: 651.131121635437 and batch: 350, loss is 5.779937877655029 and perplexity is 323.7390783618386
At time: 651.9317350387573 and batch: 400, loss is 5.748580160140992 and perplexity is 313.7448764086406
At time: 652.7344155311584 and batch: 450, loss is 5.719726095199585 and perplexity is 304.8214194713653
At time: 653.5350570678711 and batch: 500, loss is 5.711265773773193 and perplexity is 302.253410692357
At time: 654.334139585495 and batch: 550, loss is 5.730264253616333 and perplexity is 308.05066112398936
At time: 655.1288175582886 and batch: 600, loss is 5.760223007202148 and perplexity is 317.4191077725927
At time: 655.9241690635681 and batch: 650, loss is 5.743877820968628 and perplexity is 312.27300491782654
At time: 656.7184925079346 and batch: 700, loss is 5.754491806030273 and perplexity is 315.60511814518026
At time: 657.5268323421478 and batch: 750, loss is 5.719140186309814 and perplexity is 304.6428742026423
At time: 658.360559463501 and batch: 800, loss is 5.736553182601929 and perplexity is 309.99407444468176
At time: 659.1669328212738 and batch: 850, loss is 5.767083234786988 and perplexity is 319.60416150509303
At time: 659.9700794219971 and batch: 900, loss is 5.7572302150726316 and perplexity is 316.47055847852107
At time: 660.7723190784454 and batch: 950, loss is 5.738475427627564 and perplexity is 310.59053209747685
At time: 661.5857918262482 and batch: 1000, loss is 5.727160491943359 and perplexity is 307.09602753259355
At time: 662.3901023864746 and batch: 1050, loss is 5.738525972366333 and perplexity is 310.6062312115365
At time: 663.2071688175201 and batch: 1100, loss is 5.7147406959533695 and perplexity is 303.30554475648677
At time: 664.0046625137329 and batch: 1150, loss is 5.7622658634185795 and perplexity is 318.06821215784413
At time: 664.7987458705902 and batch: 1200, loss is 5.7589683341979985 and perplexity is 317.0211003238819
At time: 665.655720949173 and batch: 1250, loss is 5.750348720550537 and perplexity is 314.3002441316817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4452438911382295 and perplexity of 231.65377112488179
Finished 31 epochs...
Completing Train Step...
At time: 667.8694653511047 and batch: 50, loss is 5.735548610687256 and perplexity is 309.6828194689587
At time: 668.6671183109283 and batch: 100, loss is 5.747154903411865 and perplexity is 313.29802792485697
At time: 669.4657390117645 and batch: 150, loss is 5.667098007202148 and perplexity is 289.1940763532197
At time: 670.2631943225861 and batch: 200, loss is 5.702400608062744 and perplexity is 299.58572632336325
At time: 671.0618052482605 and batch: 250, loss is 5.723923435211182 and perplexity is 306.10354749298574
At time: 671.860298871994 and batch: 300, loss is 5.733806009292603 and perplexity is 309.1436356835033
At time: 672.6600971221924 and batch: 350, loss is 5.769212913513184 and perplexity is 320.28554099081697
At time: 673.4623193740845 and batch: 400, loss is 5.737223377227783 and perplexity is 310.2019004415755
At time: 674.2637021541595 and batch: 450, loss is 5.706328372955323 and perplexity is 300.7647425550588
At time: 675.0657758712769 and batch: 500, loss is 5.701449651718139 and perplexity is 299.30096879361207
At time: 675.8702073097229 and batch: 550, loss is 5.719448156356812 and perplexity is 304.73670953142164
At time: 676.6738772392273 and batch: 600, loss is 5.748761854171753 and perplexity is 313.8018871589657
At time: 677.4753861427307 and batch: 650, loss is 5.733825483322144 and perplexity is 309.14965601441685
At time: 678.2771618366241 and batch: 700, loss is 5.744197702407837 and perplexity is 312.37291123430197
At time: 679.0788159370422 and batch: 750, loss is 5.708532247543335 and perplexity is 301.42832128170545
At time: 679.878927230835 and batch: 800, loss is 5.727330627441407 and perplexity is 307.1482799130525
At time: 680.6749703884125 and batch: 850, loss is 5.755993585586548 and perplexity is 316.079443536407
At time: 681.4747581481934 and batch: 900, loss is 5.746036605834961 and perplexity is 312.9478633299813
At time: 682.2730832099915 and batch: 950, loss is 5.729080085754394 and perplexity is 307.68609332857204
At time: 683.0705406665802 and batch: 1000, loss is 5.718659248352051 and perplexity is 304.49639510737796
At time: 683.8649365901947 and batch: 1050, loss is 5.727992839813233 and perplexity is 307.3517446649986
At time: 684.6626601219177 and batch: 1100, loss is 5.703841886520386 and perplexity is 300.01782408873135
At time: 685.4601273536682 and batch: 1150, loss is 5.747320241928101 and perplexity is 313.34983243845625
At time: 686.328804731369 and batch: 1200, loss is 5.747041568756104 and perplexity is 313.2625224127518
At time: 687.1473972797394 and batch: 1250, loss is 5.737162656784058 and perplexity is 310.18306541637753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.433298820996806 and perplexity of 228.90311169651372
Finished 32 epochs...
Completing Train Step...
At time: 689.3829846382141 and batch: 50, loss is 5.724421005249024 and perplexity is 306.255893344918
At time: 690.2139286994934 and batch: 100, loss is 5.736551780700683 and perplexity is 309.9936398639073
At time: 691.0115287303925 and batch: 150, loss is 5.654856557846069 and perplexity is 285.675501878078
At time: 691.8090822696686 and batch: 200, loss is 5.689810180664063 and perplexity is 295.8374596402964
At time: 692.6081523895264 and batch: 250, loss is 5.710988817214965 and perplexity is 302.16971121911314
At time: 693.4287612438202 and batch: 300, loss is 5.720453367233277 and perplexity is 305.0431881983387
At time: 694.2265753746033 and batch: 350, loss is 5.756330394744873 and perplexity is 316.185919917852
At time: 695.0226595401764 and batch: 400, loss is 5.722008085250854 and perplexity is 305.51781319758567
At time: 695.8196549415588 and batch: 450, loss is 5.692900485992432 and perplexity is 296.75310179637347
At time: 696.6158668994904 and batch: 500, loss is 5.685723781585693 and perplexity is 294.63101640097716
At time: 697.4133925437927 and batch: 550, loss is 5.703833732604981 and perplexity is 300.0153777787472
At time: 698.2301468849182 and batch: 600, loss is 5.736196632385254 and perplexity is 309.8835656924011
At time: 699.0263199806213 and batch: 650, loss is 5.7220416831970216 and perplexity is 305.5280781410662
At time: 699.8232309818268 and batch: 700, loss is 5.734231967926025 and perplexity is 309.27534613370045
At time: 700.6200540065765 and batch: 750, loss is 5.700655660629272 and perplexity is 299.06342080947167
At time: 701.4185674190521 and batch: 800, loss is 5.715566110610962 and perplexity is 303.55600094974454
At time: 702.2377619743347 and batch: 850, loss is 5.743236865997314 and perplexity is 312.07291611372443
At time: 703.0337009429932 and batch: 900, loss is 5.732007989883423 and perplexity is 308.58828883813715
At time: 703.8304135799408 and batch: 950, loss is 5.71380355834961 and perplexity is 303.02143886901433
At time: 704.6266899108887 and batch: 1000, loss is 5.70467360496521 and perplexity is 300.26745824507765
At time: 705.4230194091797 and batch: 1050, loss is 5.71499641418457 and perplexity is 303.3831154316005
At time: 706.2201347351074 and batch: 1100, loss is 5.6933713722229005 and perplexity is 296.8928716511256
At time: 707.04612159729 and batch: 1150, loss is 5.735841360092163 and perplexity is 309.77349220161483
At time: 707.8520092964172 and batch: 1200, loss is 5.736127786636352 and perplexity is 309.86223226061514
At time: 708.6670877933502 and batch: 1250, loss is 5.72710108757019 and perplexity is 307.077785227417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.428854392392792 and perplexity of 227.8880255691631
Finished 33 epochs...
Completing Train Step...
At time: 710.9375050067902 and batch: 50, loss is 5.714270372390747 and perplexity is 303.16292655309337
At time: 711.7354383468628 and batch: 100, loss is 5.725118350982666 and perplexity is 306.4695340676105
At time: 712.5329868793488 and batch: 150, loss is 5.645380916595459 and perplexity is 282.9813279794853
At time: 713.3279049396515 and batch: 200, loss is 5.681654691696167 and perplexity is 293.4345721813803
At time: 714.1325669288635 and batch: 250, loss is 5.7038711833953855 and perplexity is 300.02661380217626
At time: 714.9428865909576 and batch: 300, loss is 5.712409744262695 and perplexity is 302.59937752473405
At time: 715.7396223545074 and batch: 350, loss is 5.745371799468995 and perplexity is 312.73988273929433
At time: 716.5354781150818 and batch: 400, loss is 5.714167222976685 and perplexity is 303.13165708759544
At time: 717.332533121109 and batch: 450, loss is 5.6846839046478275 and perplexity is 294.3247956453868
At time: 718.1586465835571 and batch: 500, loss is 5.678634309768677 and perplexity is 292.5496248149387
At time: 718.9939289093018 and batch: 550, loss is 5.696592893600464 and perplexity is 297.85086064649323
At time: 719.7897744178772 and batch: 600, loss is 5.728934335708618 and perplexity is 307.6412513343254
At time: 720.6079199314117 and batch: 650, loss is 5.713707342147827 and perplexity is 302.9922846996818
At time: 721.4039978981018 and batch: 700, loss is 5.724782819747925 and perplexity is 306.3667212158603
At time: 722.2107992172241 and batch: 750, loss is 5.691411409378052 and perplexity is 296.3115425316222
At time: 723.0279140472412 and batch: 800, loss is 5.70859658241272 and perplexity is 301.44771425719955
At time: 723.8259692192078 and batch: 850, loss is 5.737861328125 and perplexity is 310.3998571588994
At time: 724.6232795715332 and batch: 900, loss is 5.726205911636352 and perplexity is 306.80301958440907
At time: 725.4192593097687 and batch: 950, loss is 5.707322416305542 and perplexity is 301.0638643927821
At time: 726.2141950130463 and batch: 1000, loss is 5.6985407733917235 and perplexity is 298.43160374400463
At time: 727.0067324638367 and batch: 1050, loss is 5.707834091186523 and perplexity is 301.2179506274292
At time: 727.8302018642426 and batch: 1100, loss is 5.686040878295898 and perplexity is 294.7244577411901
At time: 728.6276528835297 and batch: 1150, loss is 5.730746698379517 and perplexity is 308.1993144078597
At time: 729.4467737674713 and batch: 1200, loss is 5.729292583465576 and perplexity is 307.7514828664844
At time: 730.2495908737183 and batch: 1250, loss is 5.718339986801148 and perplexity is 304.39919663272514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.42732272182938 and perplexity of 227.5392433665483
Finished 34 epochs...
Completing Train Step...
At time: 732.4254684448242 and batch: 50, loss is 5.708148727416992 and perplexity is 301.3127396192077
At time: 733.2493424415588 and batch: 100, loss is 5.7184442615509035 and perplexity is 304.4309394377376
At time: 734.0488820075989 and batch: 150, loss is 5.638249015808105 and perplexity is 280.97031292759453
At time: 734.886314868927 and batch: 200, loss is 5.676079587936401 and perplexity is 291.8031957666633
At time: 735.6884157657623 and batch: 250, loss is 5.69583945274353 and perplexity is 297.6265321585455
At time: 736.4866008758545 and batch: 300, loss is 5.703678321838379 and perplexity is 299.9687557817683
At time: 737.2836363315582 and batch: 350, loss is 5.737440004348755 and perplexity is 310.26910586518386
At time: 738.0828642845154 and batch: 400, loss is 5.703589820861817 and perplexity is 299.9422094286499
At time: 738.8809590339661 and batch: 450, loss is 5.673204984664917 and perplexity is 290.96558182616053
At time: 739.6819019317627 and batch: 500, loss is 5.66900712966919 and perplexity is 289.74671061711445
At time: 740.4798724651337 and batch: 550, loss is 5.686366424560547 and perplexity is 294.82041980670687
At time: 741.2761254310608 and batch: 600, loss is 5.720530185699463 and perplexity is 305.06662204824136
At time: 742.0733041763306 and batch: 650, loss is 5.707942161560059 and perplexity is 301.2505051229257
At time: 742.8743379116058 and batch: 700, loss is 5.714486236572266 and perplexity is 303.22837563390266
At time: 743.672509431839 and batch: 750, loss is 5.682905578613282 and perplexity is 293.80185531569373
At time: 744.4696011543274 and batch: 800, loss is 5.700995168685913 and perplexity is 299.1649724881141
At time: 745.2677998542786 and batch: 850, loss is 5.72892056465149 and perplexity is 307.63701481824893
At time: 746.0790627002716 and batch: 900, loss is 5.716337022781372 and perplexity is 303.79010619097323
At time: 746.890796661377 and batch: 950, loss is 5.6984807395935055 and perplexity is 298.4136882990951
At time: 747.7357277870178 and batch: 1000, loss is 5.693331823348999 and perplexity is 296.8811301045665
At time: 748.5377864837646 and batch: 1050, loss is 5.698617038726806 and perplexity is 298.4543645981845
At time: 749.3362815380096 and batch: 1100, loss is 5.677418031692505 and perplexity is 292.194019421444
At time: 750.1336526870728 and batch: 1150, loss is 5.720972328186035 and perplexity is 305.2015347862129
At time: 750.9299449920654 and batch: 1200, loss is 5.719980659484864 and perplexity is 304.8990259956563
At time: 751.7261874675751 and batch: 1250, loss is 5.709656944274903 and perplexity is 301.7675274457285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.420152204750228 and perplexity of 225.9135049939784
Finished 35 epochs...
Completing Train Step...
At time: 753.9099447727203 and batch: 50, loss is 5.698601598739624 and perplexity is 298.4497565021952
At time: 754.7346189022064 and batch: 100, loss is 5.709656267166138 and perplexity is 301.76732311635993
At time: 755.5329566001892 and batch: 150, loss is 5.627914371490479 and perplexity is 278.08153762354374
At time: 756.3298268318176 and batch: 200, loss is 5.665278978347779 and perplexity is 288.6685021460655
At time: 757.1254839897156 and batch: 250, loss is 5.68793550491333 and perplexity is 295.28337985079503
At time: 757.9572169780731 and batch: 300, loss is 5.695449752807617 and perplexity is 297.51056971478334
At time: 758.7713851928711 and batch: 350, loss is 5.7286631202697755 and perplexity is 307.5578255910249
At time: 759.5741431713104 and batch: 400, loss is 5.697166757583618 and perplexity is 298.02183558125625
At time: 760.3717341423035 and batch: 450, loss is 5.664555425643921 and perplexity is 288.45971081569405
At time: 761.1694805622101 and batch: 500, loss is 5.661314144134521 and perplexity is 287.5262453182901
At time: 761.9661948680878 and batch: 550, loss is 5.677195158004761 and perplexity is 292.12890431928975
At time: 762.7674715518951 and batch: 600, loss is 5.711742420196533 and perplexity is 302.3975130397118
At time: 763.5655252933502 and batch: 650, loss is 5.69798773765564 and perplexity is 298.26660603137856
At time: 764.3616940975189 and batch: 700, loss is 5.705166931152344 and perplexity is 300.41562458953797
At time: 765.1603896617889 and batch: 750, loss is 5.676051969528198 and perplexity is 291.79513673817695
At time: 765.9599833488464 and batch: 800, loss is 5.6943662738800045 and perplexity is 297.1883978464877
At time: 766.7704076766968 and batch: 850, loss is 5.722982788085938 and perplexity is 305.81574745137124
At time: 767.5867917537689 and batch: 900, loss is 5.710361280441284 and perplexity is 301.9801480985659
At time: 768.4348425865173 and batch: 950, loss is 5.692946195602417 and perplexity is 296.76666657493644
At time: 769.230498790741 and batch: 1000, loss is 5.6848729228973385 and perplexity is 294.3804336611819
At time: 770.0286560058594 and batch: 1050, loss is 5.691401100158691 and perplexity is 296.308487806677
At time: 770.8263771533966 and batch: 1100, loss is 5.673629179000854 and perplexity is 291.0890339599207
At time: 771.6190872192383 and batch: 1150, loss is 5.717497234344482 and perplexity is 304.1427715285759
At time: 772.4146437644958 and batch: 1200, loss is 5.715461883544922 and perplexity is 303.52436384713656
At time: 773.2091951370239 and batch: 1250, loss is 5.70537633895874 and perplexity is 300.47854055380765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.419699564467382 and perplexity of 225.8112705806315
Finished 36 epochs...
Completing Train Step...
At time: 775.4119935035706 and batch: 50, loss is 5.693533411026001 and perplexity is 296.9409837146024
At time: 776.2229652404785 and batch: 100, loss is 5.70581823348999 and perplexity is 300.61134971929704
At time: 777.0386612415314 and batch: 150, loss is 5.6223682689666745 and perplexity is 276.54353780056476
At time: 777.8372797966003 and batch: 200, loss is 5.660250425338745 and perplexity is 287.2205608568432
At time: 778.6661269664764 and batch: 250, loss is 5.680204420089722 and perplexity is 293.0093207925392
At time: 779.4726264476776 and batch: 300, loss is 5.688538732528687 and perplexity is 295.4615566750643
At time: 780.2795062065125 and batch: 350, loss is 5.719100980758667 and perplexity is 304.63093074498266
At time: 781.0801451206207 and batch: 400, loss is 5.686930589675903 and perplexity is 294.98679412973814
At time: 781.8801765441895 and batch: 450, loss is 5.656325197219848 and perplexity is 286.0953644061135
At time: 782.678492307663 and batch: 500, loss is 5.651343669891357 and perplexity is 284.6737164587416
At time: 783.4752213954926 and batch: 550, loss is 5.667563819885254 and perplexity is 289.32881800161215
At time: 784.2713236808777 and batch: 600, loss is 5.701055631637574 and perplexity is 299.18306143223407
At time: 785.0684537887573 and batch: 650, loss is 5.688904466629029 and perplexity is 295.5696368047702
At time: 785.8635318279266 and batch: 700, loss is 5.693708734512329 and perplexity is 296.99304900710183
At time: 786.6588053703308 and batch: 750, loss is 5.664032115936279 and perplexity is 288.30879653965746
At time: 787.4566774368286 and batch: 800, loss is 5.681119709014893 and perplexity is 293.2776317511254
At time: 788.2508156299591 and batch: 850, loss is 5.709008378982544 and perplexity is 301.57187495463205
At time: 789.0749802589417 and batch: 900, loss is 5.697389097213745 and perplexity is 298.0881050128365
At time: 789.8781657218933 and batch: 950, loss is 5.680260524749756 and perplexity is 293.02576044203533
At time: 790.6919643878937 and batch: 1000, loss is 5.672296085357666 and perplexity is 290.70124355703723
At time: 791.5142178535461 and batch: 1050, loss is 5.677426586151123 and perplexity is 292.1965189937828
At time: 792.3327789306641 and batch: 1100, loss is 5.6576884078979495 and perplexity is 286.4856386143726
At time: 793.1727163791656 and batch: 1150, loss is 5.702187128067017 and perplexity is 299.5217775899184
At time: 793.9862003326416 and batch: 1200, loss is 5.7000409030914305 and perplexity is 298.87962581770296
At time: 794.7857987880707 and batch: 1250, loss is 5.691329908370972 and perplexity is 296.28739382658165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.406887082287865 and perplexity of 222.93652335057348
Finished 37 epochs...
Completing Train Step...
At time: 796.968820810318 and batch: 50, loss is 5.680189685821533 and perplexity is 293.0050035464307
At time: 797.7909324169159 and batch: 100, loss is 5.690607147216797 and perplexity is 296.0733261770616
At time: 798.5879416465759 and batch: 150, loss is 5.606526718139649 and perplexity is 272.19717673508706
At time: 799.3857865333557 and batch: 200, loss is 5.645739498138428 and perplexity is 283.0828180558501
At time: 800.1831228733063 and batch: 250, loss is 5.667822799682617 and perplexity is 289.40375802382647
At time: 801.0090892314911 and batch: 300, loss is 5.671856288909912 and perplexity is 290.57342229249247
At time: 801.8051886558533 and batch: 350, loss is 5.706464853286743 and perplexity is 300.80579382808423
At time: 802.6022231578827 and batch: 400, loss is 5.672990942001343 and perplexity is 290.9033094427294
At time: 803.4009282588959 and batch: 450, loss is 5.641585140228272 and perplexity is 281.9092301514094
At time: 804.1997094154358 and batch: 500, loss is 5.638234481811524 and perplexity is 280.9662293357025
At time: 804.995706319809 and batch: 550, loss is 5.65517110824585 and perplexity is 285.7653753555294
At time: 805.7922689914703 and batch: 600, loss is 5.687490978240967 and perplexity is 295.1521476828062
At time: 806.5896785259247 and batch: 650, loss is 5.6755272483825685 and perplexity is 291.6420658230236
At time: 807.3858652114868 and batch: 700, loss is 5.683130750656128 and perplexity is 293.86801872844296
At time: 808.1823630332947 and batch: 750, loss is 5.649508752822876 and perplexity is 284.1518427413848
At time: 809.0079615116119 and batch: 800, loss is 5.668712310791015 and perplexity is 289.66130040782525
At time: 809.8095693588257 and batch: 850, loss is 5.697515382766723 and perplexity is 298.12575161107503
At time: 810.6099636554718 and batch: 900, loss is 5.685098905563354 and perplexity is 294.44696605370405
At time: 811.4125783443451 and batch: 950, loss is 5.66728175163269 and perplexity is 289.2472190362818
At time: 812.2096674442291 and batch: 1000, loss is 5.659842472076416 and perplexity is 287.103412189249
At time: 813.0096199512482 and batch: 1050, loss is 5.667190198898315 and perplexity is 289.2207388746528
At time: 813.8079726696014 and batch: 1100, loss is 5.647203636169434 and perplexity is 283.4975939461714
At time: 814.6030058860779 and batch: 1150, loss is 5.692070150375367 and perplexity is 296.5067993974558
At time: 815.3989291191101 and batch: 1200, loss is 5.687483329772949 and perplexity is 295.1498902296772
At time: 816.1937384605408 and batch: 1250, loss is 5.678231458663941 and perplexity is 292.43179461099925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.400343846230611 and perplexity of 221.4825590557981
Finished 38 epochs...
Completing Train Step...
At time: 818.3985018730164 and batch: 50, loss is 5.669523935317994 and perplexity is 289.89649205449643
At time: 819.1955831050873 and batch: 100, loss is 5.679030075073242 and perplexity is 292.66542872037627
At time: 819.9932315349579 and batch: 150, loss is 5.59393741607666 and perplexity is 268.7918843590731
At time: 820.790619134903 and batch: 200, loss is 5.628756408691406 and perplexity is 278.3157912344002
At time: 821.5881123542786 and batch: 250, loss is 5.650696258544922 and perplexity is 284.4894751110791
At time: 822.3860683441162 and batch: 300, loss is 5.656461696624756 and perplexity is 286.1344189185001
At time: 823.1839773654938 and batch: 350, loss is 5.690785694122314 and perplexity is 296.126193872798
At time: 823.9791934490204 and batch: 400, loss is 5.655983943939209 and perplexity is 285.9977500809857
At time: 824.7760133743286 and batch: 450, loss is 5.626483507156372 and perplexity is 277.68392520193953
At time: 825.5755345821381 and batch: 500, loss is 5.624039678573609 and perplexity is 277.006141819086
At time: 826.3715779781342 and batch: 550, loss is 5.639052772521973 and perplexity is 281.19623548423084
At time: 827.1682884693146 and batch: 600, loss is 5.670392408370971 and perplexity is 290.1483687042086
At time: 827.9650657176971 and batch: 650, loss is 5.659908447265625 and perplexity is 287.1223545160456
At time: 828.7637376785278 and batch: 700, loss is 5.666157817840576 and perplexity is 288.9223069365791
At time: 829.5917210578918 and batch: 750, loss is 5.633105583190918 and perplexity is 279.5288712125529
At time: 830.387672662735 and batch: 800, loss is 5.6539293479919435 and perplexity is 285.41074349991106
At time: 831.1835403442383 and batch: 850, loss is 5.682015552520752 and perplexity is 293.54048033096115
At time: 831.9832184314728 and batch: 900, loss is 5.673461847305298 and perplexity is 291.0403296133143
At time: 832.7813460826874 and batch: 950, loss is 5.657415323257446 and perplexity is 286.40741446813473
At time: 833.5752017498016 and batch: 1000, loss is 5.647241716384888 and perplexity is 283.50838980118243
At time: 834.3697197437286 and batch: 1050, loss is 5.65462098121643 and perplexity is 285.6082113325263
At time: 835.1650922298431 and batch: 1100, loss is 5.633242597579956 and perplexity is 279.5671733139701
At time: 835.9610257148743 and batch: 1150, loss is 5.677769165039063 and perplexity is 292.29663650041255
At time: 836.7580618858337 and batch: 1200, loss is 5.675578832626343 and perplexity is 291.65711034646864
At time: 837.5554203987122 and batch: 1250, loss is 5.665895290374756 and perplexity is 288.8464668510086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.392393682994982 and perplexity of 219.72871745858723
Finished 39 epochs...
Completing Train Step...
At time: 839.7466146945953 and batch: 50, loss is 5.658339471817016 and perplexity is 286.67221980855624
At time: 840.5915660858154 and batch: 100, loss is 5.667407608032226 and perplexity is 289.2836249407555
At time: 841.4082524776459 and batch: 150, loss is 5.581423053741455 and perplexity is 265.4490854498159
At time: 842.2108681201935 and batch: 200, loss is 5.618736162185669 and perplexity is 275.5409240440111
At time: 843.007383108139 and batch: 250, loss is 5.639942741394043 and perplexity is 281.44660277377693
At time: 843.8316855430603 and batch: 300, loss is 5.644825353622436 and perplexity is 282.82415769464086
At time: 844.644832611084 and batch: 350, loss is 5.679343433380127 and perplexity is 292.75715223401573
At time: 845.4493274688721 and batch: 400, loss is 5.6470395374298095 and perplexity is 283.45107616517726
At time: 846.2645630836487 and batch: 450, loss is 5.614630546569824 and perplexity is 274.4119780184676
At time: 847.0816130638123 and batch: 500, loss is 5.614846849441529 and perplexity is 274.47134053725335
At time: 847.8959419727325 and batch: 550, loss is 5.628817663192749 and perplexity is 278.33283985155515
At time: 848.6937005519867 and batch: 600, loss is 5.660000867843628 and perplexity is 287.14889175628224
At time: 849.4912106990814 and batch: 650, loss is 5.646670789718628 and perplexity is 283.3465734983531
At time: 850.3187923431396 and batch: 700, loss is 5.654007339477539 and perplexity is 285.43300397585415
At time: 851.1166923046112 and batch: 750, loss is 5.62175482749939 and perplexity is 276.37394654943955
At time: 851.9350986480713 and batch: 800, loss is 5.63895245552063 and perplexity is 281.16802813596007
At time: 852.7595834732056 and batch: 850, loss is 5.666842355728149 and perplexity is 289.1201529110492
At time: 853.5866746902466 and batch: 900, loss is 5.657053165435791 and perplexity is 286.3037085628909
At time: 854.3791389465332 and batch: 950, loss is 5.640335855484008 and perplexity is 281.5572651489446
At time: 855.1719136238098 and batch: 1000, loss is 5.631922359466553 and perplexity is 279.198321616116
At time: 855.9771945476532 and batch: 1050, loss is 5.638512868881225 and perplexity is 281.04445758933423
At time: 856.785710811615 and batch: 1100, loss is 5.617114381790161 and perplexity is 275.0944193393901
At time: 857.5927495956421 and batch: 1150, loss is 5.661191968917847 and perplexity is 287.4911188828024
At time: 858.3846254348755 and batch: 1200, loss is 5.658923473358154 and perplexity is 286.839685722142
At time: 859.1771860122681 and batch: 1250, loss is 5.64792010307312 and perplexity is 283.70078337006527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.378931644189096 and perplexity of 216.79054222469864
Finished 40 epochs...
Completing Train Step...
At time: 861.3183555603027 and batch: 50, loss is 5.640044956207276 and perplexity is 281.47537225602315
At time: 862.141832113266 and batch: 100, loss is 5.647753219604493 and perplexity is 283.65344234961094
At time: 862.9385614395142 and batch: 150, loss is 5.565248508453369 and perplexity is 261.1901035940288
At time: 863.7348854541779 and batch: 200, loss is 5.60172869682312 and perplexity is 270.8942970006486
At time: 864.5348093509674 and batch: 250, loss is 5.622297248840332 and perplexity is 276.52389834097767
At time: 865.3329548835754 and batch: 300, loss is 5.625524950027466 and perplexity is 277.4178768275086
At time: 866.1309368610382 and batch: 350, loss is 5.6609223937988284 and perplexity is 287.4136288753711
At time: 866.926319360733 and batch: 400, loss is 5.627659091949463 and perplexity is 278.01055815644247
At time: 867.7271237373352 and batch: 450, loss is 5.59710470199585 and perplexity is 269.6445747539089
At time: 868.5230779647827 and batch: 500, loss is 5.59781005859375 and perplexity is 269.83483742742726
At time: 869.3313636779785 and batch: 550, loss is 5.611658010482788 and perplexity is 273.5974896588464
At time: 870.1397955417633 and batch: 600, loss is 5.64229154586792 and perplexity is 282.1084427756338
At time: 870.962646484375 and batch: 650, loss is 5.629897222518921 and perplexity is 278.6334789142505
At time: 871.7633697986603 and batch: 700, loss is 5.6386949157714845 and perplexity is 281.0956255162027
At time: 872.5599412918091 and batch: 750, loss is 5.603436203002929 and perplexity is 271.35724581835575
At time: 873.3644962310791 and batch: 800, loss is 5.627804470062256 and perplexity is 278.05097774471443
At time: 874.1818299293518 and batch: 850, loss is 5.655059671401977 and perplexity is 285.7335323382873
At time: 875.0006742477417 and batch: 900, loss is 5.64648962020874 and perplexity is 283.29524438827934
At time: 875.802864074707 and batch: 950, loss is 5.628345155715943 and perplexity is 278.20135656954767
At time: 876.5985598564148 and batch: 1000, loss is 5.6205433177948 and perplexity is 276.03931957394235
At time: 877.3947689533234 and batch: 1050, loss is 5.627794771194458 and perplexity is 278.04828097811804
At time: 878.1958212852478 and batch: 1100, loss is 5.606626501083374 and perplexity is 272.2243387257846
At time: 879.0028486251831 and batch: 1150, loss is 5.65087438583374 and perplexity is 284.5401549635762
At time: 879.8463394641876 and batch: 1200, loss is 5.648717412948608 and perplexity is 283.92707100502537
At time: 880.6528837680817 and batch: 1250, loss is 5.638118276596069 and perplexity is 280.9335814914487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.373665245780109 and perplexity of 215.65183792512332
Finished 41 epochs...
Completing Train Step...
At time: 882.8625867366791 and batch: 50, loss is 5.631839084625244 and perplexity is 279.175072388241
At time: 883.6768243312836 and batch: 100, loss is 5.638245649337769 and perplexity is 280.96936705096283
At time: 884.4747784137726 and batch: 150, loss is 5.555763196945191 and perplexity is 258.72434682134906
At time: 885.2730050086975 and batch: 200, loss is 5.591949243545532 and perplexity is 268.2580106104207
At time: 886.0849761962891 and batch: 250, loss is 5.611638174057007 and perplexity is 273.59206251637664
At time: 886.8973245620728 and batch: 300, loss is 5.6155962467193605 and perplexity is 274.6771057029926
At time: 887.698736667633 and batch: 350, loss is 5.651146335601807 and perplexity is 284.6175461155001
At time: 888.5009651184082 and batch: 400, loss is 5.616929235458374 and perplexity is 275.0434913314668
At time: 889.3036675453186 and batch: 450, loss is 5.58784478187561 and perplexity is 267.1592124183365
At time: 890.1043565273285 and batch: 500, loss is 5.589795656204224 and perplexity is 267.68091519030577
At time: 890.9328105449677 and batch: 550, loss is 5.603931293487549 and perplexity is 271.49162547099183
At time: 891.734697341919 and batch: 600, loss is 5.633987894058228 and perplexity is 279.77561140812173
At time: 892.5673315525055 and batch: 650, loss is 5.621004829406738 and perplexity is 276.16674432701717
At time: 893.399742603302 and batch: 700, loss is 5.62871208190918 and perplexity is 278.3034546643543
At time: 894.2079284191132 and batch: 750, loss is 5.596042690277099 and perplexity is 269.3583610636702
At time: 895.007004737854 and batch: 800, loss is 5.61872896194458 and perplexity is 275.53894009007064
At time: 895.8067774772644 and batch: 850, loss is 5.647953405380249 and perplexity is 283.7102314180058
At time: 896.6047685146332 and batch: 900, loss is 5.639493808746338 and perplexity is 281.3202805624103
At time: 897.4004018306732 and batch: 950, loss is 5.621437969207764 and perplexity is 276.2863890452791
At time: 898.1950099468231 and batch: 1000, loss is 5.6140240287780765 and perplexity is 274.2455927344489
At time: 898.9876570701599 and batch: 1050, loss is 5.620438098907471 and perplexity is 276.01027655184174
At time: 899.7866973876953 and batch: 1100, loss is 5.599750003814697 and perplexity is 270.35881030575416
At time: 900.5775375366211 and batch: 1150, loss is 5.644783554077148 and perplexity is 282.81233602052475
At time: 901.366822719574 and batch: 1200, loss is 5.640946130752564 and perplexity is 281.7291450262318
At time: 902.1596009731293 and batch: 1250, loss is 5.632457933425903 and perplexity is 279.3478930163662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.368504433736314 and perplexity of 214.5417662222011
Finished 42 epochs...
Completing Train Step...
At time: 904.3043158054352 and batch: 50, loss is 5.626504869461059 and perplexity is 277.68985723391705
At time: 905.1259353160858 and batch: 100, loss is 5.633222446441651 and perplexity is 279.56153977395627
At time: 905.920220375061 and batch: 150, loss is 5.548472480773926 and perplexity is 256.84492054883384
At time: 906.7261009216309 and batch: 200, loss is 5.584592695236206 and perplexity is 266.29179872975624
At time: 907.5532515048981 and batch: 250, loss is 5.604955768585205 and perplexity is 271.769904401087
At time: 908.3564009666443 and batch: 300, loss is 5.610731477737427 and perplexity is 273.34411002601865
At time: 909.1516618728638 and batch: 350, loss is 5.645245943069458 and perplexity is 282.94313556939545
At time: 909.9457750320435 and batch: 400, loss is 5.6098401069641115 and perplexity is 273.1005676346938
At time: 910.7409961223602 and batch: 450, loss is 5.583045501708984 and perplexity is 265.88011234378985
At time: 911.5619633197784 and batch: 500, loss is 5.58418438911438 and perplexity is 266.18309235239275
At time: 912.3584792613983 and batch: 550, loss is 5.597151393890381 and perplexity is 269.6571652638893
At time: 913.1525819301605 and batch: 600, loss is 5.626642265319824 and perplexity is 277.72801329150417
At time: 913.9459352493286 and batch: 650, loss is 5.613165616989136 and perplexity is 274.01027809745955
At time: 914.7409281730652 and batch: 700, loss is 5.621987342834473 and perplexity is 276.4382152016637
At time: 915.5386574268341 and batch: 750, loss is 5.590315494537354 and perplexity is 267.8201021652971
At time: 916.3331489562988 and batch: 800, loss is 5.616098861694336 and perplexity is 274.8151972301296
At time: 917.1271281242371 and batch: 850, loss is 5.642392711639404 and perplexity is 282.1369839375595
At time: 917.9226915836334 and batch: 900, loss is 5.633205051422119 and perplexity is 279.55667683780723
At time: 918.717612028122 and batch: 950, loss is 5.614102573394775 and perplexity is 274.2671340953794
At time: 919.5126435756683 and batch: 1000, loss is 5.606808643341065 and perplexity is 272.273926797343
At time: 920.3063614368439 and batch: 1050, loss is 5.614116239547729 and perplexity is 274.27088229759596
At time: 921.1287038326263 and batch: 1100, loss is 5.592599010467529 and perplexity is 268.432372433414
At time: 921.9591550827026 and batch: 1150, loss is 5.6396017742156985 and perplexity is 281.35065507821173
At time: 922.7932095527649 and batch: 1200, loss is 5.636478652954102 and perplexity is 280.47333356920956
At time: 923.5909452438354 and batch: 1250, loss is 5.6268517112731935 and perplexity is 277.78618839208235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.364844106409672 and perplexity of 213.75790859551404
Finished 43 epochs...
Completing Train Step...
At time: 925.8054754734039 and batch: 50, loss is 5.620701627731323 and perplexity is 276.0830228003383
At time: 926.6021032333374 and batch: 100, loss is 5.627784233093262 and perplexity is 278.0453508926345
At time: 927.3986263275146 and batch: 150, loss is 5.541481637954712 and perplexity is 255.0556197279635
At time: 928.1945939064026 and batch: 200, loss is 5.5792547130584715 and perplexity is 264.8741249789388
At time: 928.9920561313629 and batch: 250, loss is 5.598504095077515 and perplexity is 270.02217765214806
At time: 929.8238394260406 and batch: 300, loss is 5.604640798568726 and perplexity is 271.68431850902175
At time: 930.6211760044098 and batch: 350, loss is 5.639898271560669 and perplexity is 281.43408716853344
At time: 931.4246737957001 and batch: 400, loss is 5.604645481109619 and perplexity is 271.6855906849318
At time: 932.2589285373688 and batch: 450, loss is 5.578510675430298 and perplexity is 264.67712196114223
At time: 933.0603148937225 and batch: 500, loss is 5.578045053482056 and perplexity is 264.55391117100964
At time: 933.8738541603088 and batch: 550, loss is 5.590067577362061 and perplexity is 267.75371319190504
At time: 934.6855742931366 and batch: 600, loss is 5.621986827850342 and perplexity is 276.43807284040633
At time: 935.4786605834961 and batch: 650, loss is 5.608654432296753 and perplexity is 272.776951099884
At time: 936.275829076767 and batch: 700, loss is 5.61743763923645 and perplexity is 275.1833600334704
At time: 937.0720067024231 and batch: 750, loss is 5.586495370864868 and perplexity is 266.7989479625571
At time: 937.8663055896759 and batch: 800, loss is 5.608147649765015 and perplexity is 272.63874752854167
At time: 938.6620111465454 and batch: 850, loss is 5.637318916320801 and perplexity is 280.70910407753234
At time: 939.4590032100677 and batch: 900, loss is 5.62894681930542 and perplexity is 278.3687905607559
At time: 940.2569763660431 and batch: 950, loss is 5.610302925109863 and perplexity is 273.22699278668074
At time: 941.0546615123749 and batch: 1000, loss is 5.6033973884582515 and perplexity is 271.3467134148208
At time: 941.8488335609436 and batch: 1050, loss is 5.609949502944946 and perplexity is 273.13044537337925
At time: 942.6448228359222 and batch: 1100, loss is 5.586753206253052 and perplexity is 266.86774704190964
At time: 943.4397969245911 and batch: 1150, loss is 5.633932638168335 and perplexity is 279.76015258484244
At time: 944.2345638275146 and batch: 1200, loss is 5.631676540374756 and perplexity is 279.1296977731361
At time: 945.0306265354156 and batch: 1250, loss is 5.622794332504273 and perplexity is 276.6613880226385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.366006447450958 and perplexity of 214.00651263892593
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 947.2185468673706 and batch: 50, loss is 5.6154406642913814 and perplexity is 274.63437409620684
At time: 948.0505084991455 and batch: 100, loss is 5.626014280319214 and perplexity is 277.5536590165724
At time: 948.8460414409637 and batch: 150, loss is 5.534445285797119 and perplexity is 253.26725771874047
At time: 949.642674446106 and batch: 200, loss is 5.568179149627685 and perplexity is 261.9566807986493
At time: 950.4671013355255 and batch: 250, loss is 5.5853118991851805 and perplexity is 266.48338572978736
At time: 951.2848036289215 and batch: 300, loss is 5.58730640411377 and perplexity is 267.01541855068325
At time: 952.0900974273682 and batch: 350, loss is 5.620743207931518 and perplexity is 276.0945026263619
At time: 952.9160108566284 and batch: 400, loss is 5.578364267349243 and perplexity is 264.6383739281985
At time: 953.7116343975067 and batch: 450, loss is 5.553677616119384 and perplexity is 258.18531857332044
At time: 954.5103228092194 and batch: 500, loss is 5.546097497940064 and perplexity is 256.2356420709172
At time: 955.3062086105347 and batch: 550, loss is 5.55991647720337 and perplexity is 259.801136099277
At time: 956.1026847362518 and batch: 600, loss is 5.59071120262146 and perplexity is 267.9261017158664
At time: 956.8988382816315 and batch: 650, loss is 5.583446836471557 and perplexity is 265.9868406910157
At time: 957.6984148025513 and batch: 700, loss is 5.582568655014038 and perplexity is 265.7533585144409
At time: 958.4956390857697 and batch: 750, loss is 5.552148427963257 and perplexity is 257.7908063606514
At time: 959.3053357601166 and batch: 800, loss is 5.566302671432495 and perplexity is 261.4655857077636
At time: 960.1199624538422 and batch: 850, loss is 5.593907308578491 and perplexity is 268.7837918297306
At time: 960.9152510166168 and batch: 900, loss is 5.5835950756073 and perplexity is 266.0262732730523
At time: 961.710205078125 and batch: 950, loss is 5.561027774810791 and perplexity is 260.0900129645842
At time: 962.5270545482635 and batch: 1000, loss is 5.553491458892823 and perplexity is 258.1372599838415
At time: 963.3353314399719 and batch: 1050, loss is 5.551389131546021 and perplexity is 257.59514101838266
At time: 964.1328420639038 and batch: 1100, loss is 5.534263525009155 and perplexity is 253.22122784575183
At time: 964.9262561798096 and batch: 1150, loss is 5.575299940109253 and perplexity is 263.82867657265
At time: 965.7422895431519 and batch: 1200, loss is 5.5822292137146 and perplexity is 265.66316615746814
At time: 966.5471920967102 and batch: 1250, loss is 5.5827741622924805 and perplexity is 265.8079783760835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.305953199846031 and perplexity of 201.53301212347137
Finished 45 epochs...
Completing Train Step...
At time: 968.7116260528564 and batch: 50, loss is 5.585805864334106 and perplexity is 266.61505175164143
At time: 969.5356092453003 and batch: 100, loss is 5.5999870872497555 and perplexity is 270.42291550003927
At time: 970.331945180893 and batch: 150, loss is 5.510143661499024 and perplexity is 247.1866357195757
At time: 971.1297159194946 and batch: 200, loss is 5.542885961532593 and perplexity is 255.41405196693455
At time: 971.9309346675873 and batch: 250, loss is 5.565306940078735 and perplexity is 261.205365802205
At time: 972.7549331188202 and batch: 300, loss is 5.569024267196656 and perplexity is 262.17815856609565
At time: 973.5512773990631 and batch: 350, loss is 5.6034716033935545 and perplexity is 271.3668521408894
At time: 974.3490860462189 and batch: 400, loss is 5.562075128555298 and perplexity is 260.3625619162398
At time: 975.1479229927063 and batch: 450, loss is 5.538835487365723 and perplexity is 254.38159632660444
At time: 975.9436764717102 and batch: 500, loss is 5.534499635696411 and perplexity is 253.28102314276282
At time: 976.7417981624603 and batch: 550, loss is 5.548288555145263 and perplexity is 256.7976845294436
At time: 977.5421073436737 and batch: 600, loss is 5.580985870361328 and perplexity is 265.333060885241
At time: 978.3649790287018 and batch: 650, loss is 5.574727516174317 and perplexity is 263.67769793947156
At time: 979.1656491756439 and batch: 700, loss is 5.575127229690552 and perplexity is 263.78311454608547
At time: 979.9615776538849 and batch: 750, loss is 5.546975584030151 and perplexity is 256.46073783627145
At time: 980.7598423957825 and batch: 800, loss is 5.563018045425415 and perplexity is 260.6081779477694
At time: 981.5601444244385 and batch: 850, loss is 5.591190481185913 and perplexity is 268.05454373057
At time: 982.3568754196167 and batch: 900, loss is 5.580927238464356 and perplexity is 265.31750436061066
At time: 983.155773639679 and batch: 950, loss is 5.559148797988891 and perplexity is 259.60176870209034
At time: 983.9761316776276 and batch: 1000, loss is 5.5543926811218265 and perplexity is 258.37000388190745
At time: 984.7843773365021 and batch: 1050, loss is 5.554465341567993 and perplexity is 258.38877784371954
At time: 985.5920741558075 and batch: 1100, loss is 5.5398908042907715 and perplexity is 254.65019123207603
At time: 986.3998501300812 and batch: 1150, loss is 5.580542268753052 and perplexity is 265.2153848152803
At time: 987.2164309024811 and batch: 1200, loss is 5.58664722442627 and perplexity is 266.8394654092649
At time: 988.0260660648346 and batch: 1250, loss is 5.582860946655273 and perplexity is 265.83104735311093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.30448924712021 and perplexity of 201.2381931741861
Finished 46 epochs...
Completing Train Step...
At time: 990.225576877594 and batch: 50, loss is 5.579945812225342 and perplexity is 265.057242534944
At time: 991.0266356468201 and batch: 100, loss is 5.592761659622193 and perplexity is 268.47603628272253
At time: 991.8247940540314 and batch: 150, loss is 5.503418865203858 and perplexity is 245.52993268608793
At time: 992.6223726272583 and batch: 200, loss is 5.535419940948486 and perplexity is 253.51422629119367
At time: 993.4492955207825 and batch: 250, loss is 5.559058256149292 and perplexity is 259.5782649444416
At time: 994.2460117340088 and batch: 300, loss is 5.563349752426148 and perplexity is 260.6946378437265
At time: 995.0443396568298 and batch: 350, loss is 5.597502288818359 and perplexity is 269.7518031984879
At time: 995.8427851200104 and batch: 400, loss is 5.556379041671753 and perplexity is 258.8837299185548
At time: 996.6423325538635 and batch: 450, loss is 5.5340152359008785 and perplexity is 253.15836357747273
At time: 997.4413690567017 and batch: 500, loss is 5.530535650253296 and perplexity is 252.27900815249504
At time: 998.239999294281 and batch: 550, loss is 5.545146131515503 and perplexity is 255.99198400670375
At time: 999.0367968082428 and batch: 600, loss is 5.578488550186157 and perplexity is 264.6712659799831
At time: 999.8357012271881 and batch: 650, loss is 5.572717761993408 and perplexity is 263.1483027389246
At time: 1000.6329860687256 and batch: 700, loss is 5.572812166213989 and perplexity is 263.1731462219883
At time: 1001.4309661388397 and batch: 750, loss is 5.544876966476441 and perplexity is 255.92308918678324
At time: 1002.2284362316132 and batch: 800, loss is 5.561524085998535 and perplexity is 260.2191305864488
At time: 1003.025993347168 and batch: 850, loss is 5.590097284317016 and perplexity is 267.7616674575499
At time: 1003.8233644962311 and batch: 900, loss is 5.580257711410522 and perplexity is 265.13992656678766
At time: 1004.6216702461243 and batch: 950, loss is 5.558580341339112 and perplexity is 259.45423828667265
At time: 1005.4204218387604 and batch: 1000, loss is 5.554748239517212 and perplexity is 258.4618858396363
At time: 1006.2188982963562 and batch: 1050, loss is 5.555589714050293 and perplexity is 258.67946646578156
At time: 1007.0169229507446 and batch: 1100, loss is 5.541192064285278 and perplexity is 254.98177302879492
At time: 1007.8130633831024 and batch: 1150, loss is 5.581548528671265 and perplexity is 265.48239474487787
At time: 1008.6067073345184 and batch: 1200, loss is 5.587793407440185 and perplexity is 267.1454876171849
At time: 1009.4309945106506 and batch: 1250, loss is 5.582145366668701 and perplexity is 265.6408920196052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.303465014826642 and perplexity of 201.03218403665315
Finished 47 epochs...
Completing Train Step...
At time: 1011.609719991684 and batch: 50, loss is 5.575705833435059 and perplexity is 263.9357846073748
At time: 1012.4320735931396 and batch: 100, loss is 5.58890248298645 and perplexity is 267.44193650651584
At time: 1013.2307298183441 and batch: 150, loss is 5.499812211990356 and perplexity is 244.6459863674639
At time: 1014.0544033050537 and batch: 200, loss is 5.531870794296265 and perplexity is 252.61606192499457
At time: 1014.8513634204865 and batch: 250, loss is 5.555754709243774 and perplexity is 258.7221508556635
At time: 1015.672660112381 and batch: 300, loss is 5.560175647735596 and perplexity is 259.8684776240853
At time: 1016.4696533679962 and batch: 350, loss is 5.59436520576477 and perplexity is 268.90689535395455
At time: 1017.2672674655914 and batch: 400, loss is 5.55297306060791 and perplexity is 258.0034767504893
At time: 1018.065158367157 and batch: 450, loss is 5.530970783233642 and perplexity is 252.3888069559991
At time: 1018.8643536567688 and batch: 500, loss is 5.528643274307251 and perplexity is 251.80205285747388
At time: 1019.668463230133 and batch: 550, loss is 5.543755168914795 and perplexity is 255.63615625978406
At time: 1020.465823173523 and batch: 600, loss is 5.576988325119019 and perplexity is 264.2744972078541
At time: 1021.2950029373169 and batch: 650, loss is 5.571349573135376 and perplexity is 262.7885123498335
At time: 1022.128561258316 and batch: 700, loss is 5.571360168457031 and perplexity is 262.79129669339954
At time: 1022.9389412403107 and batch: 750, loss is 5.5436285209655765 and perplexity is 255.60378251492511
At time: 1023.7629764080048 and batch: 800, loss is 5.5610791778564455 and perplexity is 260.1033827270153
At time: 1024.570743560791 and batch: 850, loss is 5.591080799102783 and perplexity is 268.02514456213146
At time: 1025.367327928543 and batch: 900, loss is 5.579729042053223 and perplexity is 264.99979225783653
At time: 1026.1634759902954 and batch: 950, loss is 5.557936506271362 and perplexity is 259.28724631300037
At time: 1026.9961256980896 and batch: 1000, loss is 5.554793062210083 and perplexity is 258.47347105700254
At time: 1027.820362329483 and batch: 1050, loss is 5.555969972610473 and perplexity is 258.7778502517009
At time: 1028.638677597046 and batch: 1100, loss is 5.541583576202393 and perplexity is 255.0816209761379
At time: 1029.4677777290344 and batch: 1150, loss is 5.581590757369995 and perplexity is 265.49360595765967
At time: 1030.2666583061218 and batch: 1200, loss is 5.5873628616333 and perplexity is 267.03049400444837
At time: 1031.0620019435883 and batch: 1250, loss is 5.580963821411133 and perplexity is 265.32721063429256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3032471594149175 and perplexity of 200.9883928576758
Finished 48 epochs...
Completing Train Step...
At time: 1033.3129630088806 and batch: 50, loss is 5.57305679321289 and perplexity is 263.2375333540343
At time: 1034.1096804141998 and batch: 100, loss is 5.586339092254638 and perplexity is 266.757256251605
At time: 1034.9326586723328 and batch: 150, loss is 5.497556066513061 and perplexity is 244.09465161135742
At time: 1035.7315714359283 and batch: 200, loss is 5.529171733856201 and perplexity is 251.93515522326214
At time: 1036.5612726211548 and batch: 250, loss is 5.5534576797485355 and perplexity is 258.12854047536024
At time: 1037.359275817871 and batch: 300, loss is 5.557776193618775 and perplexity is 259.2456826184439
At time: 1038.1828253269196 and batch: 350, loss is 5.5918857288360595 and perplexity is 268.24097282189393
At time: 1039.0112237930298 and batch: 400, loss is 5.550395927429199 and perplexity is 257.33942347472964
At time: 1039.8199543952942 and batch: 450, loss is 5.529253177642822 and perplexity is 251.95567461186337
At time: 1040.6160700321198 and batch: 500, loss is 5.526844043731689 and perplexity is 251.3494102313188
At time: 1041.4111905097961 and batch: 550, loss is 5.542344923019409 and perplexity is 255.27590050401315
At time: 1042.2116296291351 and batch: 600, loss is 5.57548342704773 and perplexity is 263.87709013028865
At time: 1043.0100526809692 and batch: 650, loss is 5.570411720275879 and perplexity is 262.54217092614374
At time: 1043.8079545497894 and batch: 700, loss is 5.570311613082886 and perplexity is 262.5158898818528
At time: 1044.605699300766 and batch: 750, loss is 5.543194065093994 and perplexity is 255.49275807017008
At time: 1045.4026172161102 and batch: 800, loss is 5.560416917800904 and perplexity is 259.9311836728952
At time: 1046.200974225998 and batch: 850, loss is 5.589753503799439 and perplexity is 267.6696320338231
At time: 1046.998058795929 and batch: 900, loss is 5.578471574783325 and perplexity is 264.6667731167593
At time: 1047.7959475517273 and batch: 950, loss is 5.5575480651855464 and perplexity is 259.1865480524413
At time: 1048.5946474075317 and batch: 1000, loss is 5.55436595916748 and perplexity is 258.3630998227047
At time: 1049.3945355415344 and batch: 1050, loss is 5.556043300628662 and perplexity is 258.79682661435214
At time: 1050.1879632472992 and batch: 1100, loss is 5.541362981796265 and perplexity is 255.02535760336545
At time: 1050.9873535633087 and batch: 1150, loss is 5.581193304061889 and perplexity is 265.38810561281014
At time: 1051.7838439941406 and batch: 1200, loss is 5.586545219421387 and perplexity is 266.8122478364808
At time: 1052.5789380073547 and batch: 1250, loss is 5.5797654819488525 and perplexity is 265.0094489985526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.303012820055885 and perplexity of 200.94129888472182
Finished 49 epochs...
Completing Train Step...
At time: 1054.7596690654755 and batch: 50, loss is 5.570787668228149 and perplexity is 262.6408916734567
At time: 1055.6091051101685 and batch: 100, loss is 5.584483003616333 and perplexity is 266.2625903529809
At time: 1056.4167747497559 and batch: 150, loss is 5.49570182800293 and perplexity is 243.642461272153
At time: 1057.2229068279266 and batch: 200, loss is 5.527120323181152 and perplexity is 251.4188625016761
At time: 1058.0577507019043 and batch: 250, loss is 5.551853408813477 and perplexity is 257.7147643536135
At time: 1058.8623611927032 and batch: 300, loss is 5.555770072937012 and perplexity is 258.72612581395794
At time: 1059.667367219925 and batch: 350, loss is 5.590126552581787 and perplexity is 267.7695044916162
At time: 1060.486914396286 and batch: 400, loss is 5.548453979492187 and perplexity is 256.840168632554
At time: 1061.2903048992157 and batch: 450, loss is 5.527422075271606 and perplexity is 251.49474011655434
At time: 1062.1018908023834 and batch: 500, loss is 5.525308990478516 and perplexity is 250.96387148841742
At time: 1062.8991544246674 and batch: 550, loss is 5.540945777893066 and perplexity is 254.91898222041408
At time: 1063.6968355178833 and batch: 600, loss is 5.574284086227417 and perplexity is 263.56080127153257
At time: 1064.4935274124146 and batch: 650, loss is 5.569147729873658 and perplexity is 262.210529781681
At time: 1065.28994846344 and batch: 700, loss is 5.569581632614136 and perplexity is 262.3243283361023
At time: 1066.086350440979 and batch: 750, loss is 5.542161302566528 and perplexity is 255.22903093079083
At time: 1066.8837232589722 and batch: 800, loss is 5.558610973358154 and perplexity is 259.46218601556734
At time: 1067.6807689666748 and batch: 850, loss is 5.588599147796631 and perplexity is 267.36082425866164
At time: 1068.4773061275482 and batch: 900, loss is 5.5774663543701175 and perplexity is 264.4008583475931
At time: 1069.3112094402313 and batch: 950, loss is 5.556917715072632 and perplexity is 259.0232212645481
At time: 1070.117752313614 and batch: 1000, loss is 5.5535275268554685 and perplexity is 258.14657063679925
At time: 1070.9130592346191 and batch: 1050, loss is 5.555324840545654 and perplexity is 258.6109582023331
At time: 1071.7105045318604 and batch: 1100, loss is 5.540990076065063 and perplexity is 254.93027491545487
At time: 1072.5061836242676 and batch: 1150, loss is 5.580549688339233 and perplexity is 265.2173526109845
At time: 1073.3024351596832 and batch: 1200, loss is 5.585995244979858 and perplexity is 266.66554826368946
At time: 1074.10085272789 and batch: 1250, loss is 5.578925333023071 and perplexity is 264.7868950969561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.302752640995666 and perplexity of 200.88902496700308
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f17ffca5e80>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 7.0423698389972795, 'num_layers': 1, 'dropout': 0.05605163182934092, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 19.32387795030508, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4390435218811035 and batch: 50, loss is 7.476552963256836 and perplexity is 1766.1423137100066
At time: 2.2360191345214844 and batch: 100, loss is 6.4230112457275395 and perplexity is 615.8548149435398
At time: 3.034264087677002 and batch: 150, loss is 6.008243379592895 and perplexity is 406.76815507874124
At time: 3.832042932510376 and batch: 200, loss is 5.965670108795166 and perplexity is 389.8141584642492
At time: 4.633010149002075 and batch: 250, loss is 5.976285076141357 and perplexity is 393.9740625900889
At time: 5.431273460388184 and batch: 300, loss is 5.996803464889527 and perplexity is 402.1412780801722
At time: 6.229442358016968 and batch: 350, loss is 6.0364730930328365 and perplexity is 418.4147196889603
At time: 7.026421785354614 and batch: 400, loss is 6.020528631210327 and perplexity is 411.79622653602047
At time: 7.8374598026275635 and batch: 450, loss is 6.019689121246338 and perplexity is 411.45066457236146
At time: 8.649468660354614 and batch: 500, loss is 6.038869047164917 and perplexity is 419.4184241002332
At time: 9.444473266601562 and batch: 550, loss is 6.0690109825134275 and perplexity is 432.25296435906415
At time: 10.240988731384277 and batch: 600, loss is 6.104120845794678 and perplexity is 447.69887204265217
At time: 11.056444883346558 and batch: 650, loss is 6.1025364112854 and perplexity is 446.99008416229213
At time: 11.866382122039795 and batch: 700, loss is 6.13883222579956 and perplexity is 463.51197736260855
At time: 12.682576894760132 and batch: 750, loss is 6.1248283863067625 and perplexity is 457.06626770767963
At time: 13.506017446517944 and batch: 800, loss is 6.140549001693725 and perplexity is 464.30840700199354
At time: 14.3174729347229 and batch: 850, loss is 6.17649206161499 and perplexity is 481.30061861813994
At time: 15.12702488899231 and batch: 900, loss is 6.1522988605499265 and perplexity is 469.7961421709782
At time: 15.92441701889038 and batch: 950, loss is 6.133005971908569 and perplexity is 460.819290656165
At time: 16.722745418548584 and batch: 1000, loss is 6.132116632461548 and perplexity is 460.4096480656931
At time: 17.5231032371521 and batch: 1050, loss is 6.145600080490112 and perplexity is 466.6595983791368
At time: 18.3192241191864 and batch: 1100, loss is 6.141805629730225 and perplexity is 464.8922367153802
At time: 19.159109115600586 and batch: 1150, loss is 6.146544742584228 and perplexity is 467.1006422985187
At time: 19.966108560562134 and batch: 1200, loss is 6.16641752243042 and perplexity is 476.47607998408705
At time: 20.761586666107178 and batch: 1250, loss is 6.1529402256011965 and perplexity is 470.09754964356574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.744890421846487 and perplexity of 312.5893729869979
Finished 1 epochs...
Completing Train Step...
At time: 22.9861581325531 and batch: 50, loss is 6.108413419723511 and perplexity is 449.6247831493871
At time: 23.778759002685547 and batch: 100, loss is 6.117576370239258 and perplexity is 453.7636057435452
At time: 24.601335287094116 and batch: 150, loss is 6.048464059829712 and perplexity is 423.46211781426155
At time: 25.395326614379883 and batch: 200, loss is 6.081527347564697 and perplexity is 437.6972002030616
At time: 26.188464164733887 and batch: 250, loss is 6.07174822807312 and perplexity is 433.4377676754655
At time: 26.981452465057373 and batch: 300, loss is 6.057005672454834 and perplexity is 427.0946589802221
At time: 27.77641463279724 and batch: 350, loss is 6.114783697128296 and perplexity is 452.4981601334529
At time: 28.582154512405396 and batch: 400, loss is 6.087060203552246 and perplexity is 440.12562765260026
At time: 29.421064138412476 and batch: 450, loss is 6.086792030334473 and perplexity is 440.0076135716232
At time: 30.225638151168823 and batch: 500, loss is 6.062390556335449 and perplexity is 429.4007174810955
At time: 31.018433332443237 and batch: 550, loss is 6.009710283279419 and perplexity is 407.3652826422235
At time: 31.813287019729614 and batch: 600, loss is 5.995300884246826 and perplexity is 401.5374821197608
At time: 32.60709881782532 and batch: 650, loss is 6.027095737457276 and perplexity is 414.50943532194555
At time: 33.39992356300354 and batch: 700, loss is 6.088955602645874 and perplexity is 440.9606324520231
At time: 34.193238735198975 and batch: 750, loss is 6.058137664794922 and perplexity is 427.5784006069322
At time: 34.985591888427734 and batch: 800, loss is 6.100967330932617 and perplexity is 446.28927076339613
At time: 35.77722215652466 and batch: 850, loss is 6.118957748413086 and perplexity is 454.3908580211561
At time: 36.5707802772522 and batch: 900, loss is 6.109380903244019 and perplexity is 450.05999821525063
At time: 37.36769127845764 and batch: 950, loss is 6.0759624671936034 and perplexity is 435.2682323712051
At time: 38.162211418151855 and batch: 1000, loss is 6.085834045410156 and perplexity is 439.5862947519973
At time: 38.95738387107849 and batch: 1050, loss is 6.132326993942261 and perplexity is 460.50651070872203
At time: 39.75124478340149 and batch: 1100, loss is 6.096544942855835 and perplexity is 444.319964142198
At time: 40.54658317565918 and batch: 1150, loss is 6.126945171356201 and perplexity is 458.03480347913694
At time: 41.342084646224976 and batch: 1200, loss is 6.089626159667969 and perplexity is 441.2564208609951
At time: 42.13627600669861 and batch: 1250, loss is 6.121756572723388 and perplexity is 455.66439957915264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.730118577497719 and perplexity of 308.00578876783493
Finished 2 epochs...
Completing Train Step...
At time: 44.3309543132782 and batch: 50, loss is 6.078446159362793 and perplexity is 436.35064830900933
At time: 45.17965078353882 and batch: 100, loss is 6.086371936798096 and perplexity is 439.8228080377281
At time: 46.019779443740845 and batch: 150, loss is 6.0130471420288085 and perplexity is 408.72687350459677
At time: 46.82270646095276 and batch: 200, loss is 6.047760124206543 and perplexity is 423.1641326379649
At time: 47.64560890197754 and batch: 250, loss is 6.042868309020996 and perplexity is 421.09914679041054
At time: 48.439316272735596 and batch: 300, loss is 6.058166990280151 and perplexity is 427.59093973486074
At time: 49.23330879211426 and batch: 350, loss is 6.107160234451294 and perplexity is 449.06167290754746
At time: 50.02981686592102 and batch: 400, loss is 6.069488773345947 and perplexity is 432.4595402088581
At time: 50.82501745223999 and batch: 450, loss is 6.078261842727661 and perplexity is 436.270229037308
At time: 51.62103533744812 and batch: 500, loss is 6.046076984405517 and perplexity is 422.4524873112474
At time: 52.41548013687134 and batch: 550, loss is 6.03172739982605 and perplexity is 416.433756036858
At time: 53.20942544937134 and batch: 600, loss is 6.0603382110595705 and perplexity is 428.52034267267413
At time: 54.004737854003906 and batch: 650, loss is 6.036100149154663 and perplexity is 418.25870357515026
At time: 54.798967361450195 and batch: 700, loss is 6.100841999053955 and perplexity is 446.233339995692
At time: 55.595245599746704 and batch: 750, loss is 6.043796710968017 and perplexity is 421.49027759338543
At time: 56.38930034637451 and batch: 800, loss is 6.0704365062713626 and perplexity is 432.86959063247565
At time: 57.18070864677429 and batch: 850, loss is 6.099525518417359 and perplexity is 445.6462689627962
At time: 58.00556039810181 and batch: 900, loss is 6.074712772369384 and perplexity is 434.72461965980807
At time: 58.829182624816895 and batch: 950, loss is 6.05694899559021 and perplexity is 427.0704532800113
At time: 59.62504768371582 and batch: 1000, loss is 6.0169820213317875 and perplexity is 410.33833278939323
At time: 60.42137908935547 and batch: 1050, loss is 6.045061178207398 and perplexity is 422.02357533882713
At time: 61.22567701339722 and batch: 1100, loss is 6.04232494354248 and perplexity is 420.87039820368307
At time: 62.09943437576294 and batch: 1150, loss is 6.050383415222168 and perplexity is 424.2756726142291
At time: 62.91566610336304 and batch: 1200, loss is 6.0467581462860105 and perplexity is 422.7403438692505
At time: 63.736764907836914 and batch: 1250, loss is 6.07594409942627 and perplexity is 435.260237539009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.7652503243328015 and perplexity of 319.0188922332064
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 65.97216725349426 and batch: 50, loss is 5.973307771682739 and perplexity is 392.8028262856817
At time: 66.76932859420776 and batch: 100, loss is 5.911129331588745 and perplexity is 369.1227821282024
At time: 67.58954215049744 and batch: 150, loss is 5.745089330673218 and perplexity is 312.65155595659286
At time: 68.40997505187988 and batch: 200, loss is 5.757345190048218 and perplexity is 316.50694676508704
At time: 69.23451972007751 and batch: 250, loss is 5.7500763034820555 and perplexity is 314.21463504177035
At time: 70.0260398387909 and batch: 300, loss is 5.735875177383423 and perplexity is 309.7839680791571
At time: 70.84140610694885 and batch: 350, loss is 5.757631416320801 and perplexity is 316.5975523349349
At time: 71.64641165733337 and batch: 400, loss is 5.738694410324097 and perplexity is 310.65855349719624
At time: 72.44377756118774 and batch: 450, loss is 5.753342380523682 and perplexity is 315.24256197794324
At time: 73.24459719657898 and batch: 500, loss is 5.6856678771972655 and perplexity is 294.61454569459005
At time: 74.05182123184204 and batch: 550, loss is 5.671428585052491 and perplexity is 290.4491694925089
At time: 74.86010432243347 and batch: 600, loss is 5.689153709411621 and perplexity is 295.6433145849433
At time: 75.68663001060486 and batch: 650, loss is 5.655457801818848 and perplexity is 285.8473141971376
At time: 76.48436665534973 and batch: 700, loss is 5.680650815963745 and perplexity is 293.1401481426136
At time: 77.28264284133911 and batch: 750, loss is 5.630227317810059 and perplexity is 278.72546969561995
At time: 78.08412384986877 and batch: 800, loss is 5.6288009452819825 and perplexity is 278.3281867468702
At time: 78.89136362075806 and batch: 850, loss is 5.654043054580688 and perplexity is 285.44319842708006
At time: 79.69794178009033 and batch: 900, loss is 5.628002443313599 and perplexity is 278.1060298500599
At time: 80.50201749801636 and batch: 950, loss is 5.596089763641357 and perplexity is 269.37104096635704
At time: 81.30233263969421 and batch: 1000, loss is 5.577277517318725 and perplexity is 264.3509343830126
At time: 82.09952402114868 and batch: 1050, loss is 5.581980648040772 and perplexity is 265.5971396198663
At time: 82.94796752929688 and batch: 1100, loss is 5.493028783798218 and perplexity is 242.99206386080976
At time: 83.74175095558167 and batch: 1150, loss is 5.5010013771057125 and perplexity is 244.9370838872681
At time: 84.53624629974365 and batch: 1200, loss is 5.494512329101562 and perplexity is 243.3528211296102
At time: 85.3346357345581 and batch: 1250, loss is 5.5230164718627925 and perplexity is 250.389191125564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298880249914462 and perplexity of 200.11260836299908
Finished 4 epochs...
Completing Train Step...
At time: 87.53973984718323 and batch: 50, loss is 5.601933708190918 and perplexity is 270.9498391041923
At time: 88.36651349067688 and batch: 100, loss is 5.627247047424317 and perplexity is 277.89602902519243
At time: 89.16560649871826 and batch: 150, loss is 5.521012649536133 and perplexity is 249.8879580327804
At time: 89.962242603302 and batch: 200, loss is 5.549180850982666 and perplexity is 257.02692629493043
At time: 90.76329135894775 and batch: 250, loss is 5.569075365066528 and perplexity is 262.1915556538038
At time: 91.56060147285461 and batch: 300, loss is 5.5660120964050295 and perplexity is 261.38962137521884
At time: 92.355947971344 and batch: 350, loss is 5.594562425613403 and perplexity is 268.9599343611531
At time: 93.1505298614502 and batch: 400, loss is 5.584464626312256 and perplexity is 266.25769720935523
At time: 93.94429230690002 and batch: 450, loss is 5.599071865081787 and perplexity is 270.1755316758949
At time: 94.73856782913208 and batch: 500, loss is 5.5502471828460695 and perplexity is 257.301148476132
At time: 95.55878233909607 and batch: 550, loss is 5.533168563842773 and perplexity is 252.94411217789164
At time: 96.35452628135681 and batch: 600, loss is 5.5508487510681155 and perplexity is 257.45597923651457
At time: 97.14970684051514 and batch: 650, loss is 5.53555850982666 and perplexity is 253.54935790714998
At time: 97.96058702468872 and batch: 700, loss is 5.558392305374145 and perplexity is 259.405456145154
At time: 98.78309655189514 and batch: 750, loss is 5.5176967430114745 and perplexity is 249.06072519365844
At time: 99.57717251777649 and batch: 800, loss is 5.52679859161377 and perplexity is 251.33798612791287
At time: 100.37334108352661 and batch: 850, loss is 5.557860031127929 and perplexity is 259.26741804184286
At time: 101.19487285614014 and batch: 900, loss is 5.54541862487793 and perplexity is 256.0617496280826
At time: 102.00569605827332 and batch: 950, loss is 5.522108678817749 and perplexity is 250.16199269948424
At time: 102.80339527130127 and batch: 1000, loss is 5.5020826625823975 and perplexity is 245.2020740379593
At time: 103.64825367927551 and batch: 1050, loss is 5.508832902908325 and perplexity is 246.86284596475994
At time: 104.44243288040161 and batch: 1100, loss is 5.455390872955323 and perplexity is 234.01632384719292
At time: 105.23707127571106 and batch: 1150, loss is 5.4922448348999025 and perplexity is 242.80164514906852
At time: 106.03435850143433 and batch: 1200, loss is 5.4909803009033205 and perplexity is 242.4948082580787
At time: 106.83004188537598 and batch: 1250, loss is 5.509390077590942 and perplexity is 247.00043001822672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.281149314267792 and perplexity of 196.59569579577627
Finished 5 epochs...
Completing Train Step...
At time: 109.0617048740387 and batch: 50, loss is 5.547602710723877 and perplexity is 256.6216216528563
At time: 109.85543441772461 and batch: 100, loss is 5.566556491851807 and perplexity is 261.53195943551094
At time: 110.65345096588135 and batch: 150, loss is 5.467719850540161 and perplexity is 236.9193648494618
At time: 111.45117807388306 and batch: 200, loss is 5.4986183643341064 and perplexity is 244.35409060423382
At time: 112.2475197315216 and batch: 250, loss is 5.520871419906616 and perplexity is 249.85266894103793
At time: 113.06941604614258 and batch: 300, loss is 5.5196719741821285 and perplexity is 249.55316388146386
At time: 113.86474585533142 and batch: 350, loss is 5.546063175201416 and perplexity is 256.2268475128697
At time: 114.66126108169556 and batch: 400, loss is 5.528943195343017 and perplexity is 251.87758491623535
At time: 115.45745992660522 and batch: 450, loss is 5.536807699203491 and perplexity is 253.86628698255376
At time: 116.25331568717957 and batch: 500, loss is 5.499968013763428 and perplexity is 244.68410561536083
At time: 117.05799531936646 and batch: 550, loss is 5.491522636413574 and perplexity is 242.62635747232997
At time: 117.85054588317871 and batch: 600, loss is 5.504939413070678 and perplexity is 245.90355668607552
At time: 118.6501350402832 and batch: 650, loss is 5.493207530975342 and perplexity is 243.0355018883851
At time: 119.45895671844482 and batch: 700, loss is 5.513140926361084 and perplexity is 247.92863095983745
At time: 120.28503942489624 and batch: 750, loss is 5.477815561294555 and perplexity is 239.32334877379765
At time: 121.08705687522888 and batch: 800, loss is 5.487914199829102 and perplexity is 241.75243334583138
At time: 121.91652631759644 and batch: 850, loss is 5.52501748085022 and perplexity is 250.8907237656749
At time: 122.7263548374176 and batch: 900, loss is 5.516425619125366 and perplexity is 248.7443392822181
At time: 123.52773523330688 and batch: 950, loss is 5.492084131240845 and perplexity is 242.76262917135705
At time: 124.37318158149719 and batch: 1000, loss is 5.4745971298217775 and perplexity is 238.55434113892184
At time: 125.16957926750183 and batch: 1050, loss is 5.482414751052857 and perplexity is 240.42657730240168
At time: 125.96682906150818 and batch: 1100, loss is 5.446542482376099 and perplexity is 231.95479009027778
At time: 126.76144623756409 and batch: 1150, loss is 5.487500524520874 and perplexity is 241.652447015816
At time: 127.5907711982727 and batch: 1200, loss is 5.486946983337402 and perplexity is 241.51871944957833
At time: 128.4190592765808 and batch: 1250, loss is 5.501336336135864 and perplexity is 245.01914151754008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.271324881671989 and perplexity of 194.67371129644064
Finished 6 epochs...
Completing Train Step...
At time: 130.67227005958557 and batch: 50, loss is 5.516928615570069 and perplexity is 248.869488272627
At time: 131.5149393081665 and batch: 100, loss is 5.532267370223999 and perplexity is 252.7162632415148
At time: 132.31207966804504 and batch: 150, loss is 5.440504856109619 and perplexity is 230.5585529773353
At time: 133.10719299316406 and batch: 200, loss is 5.47261212348938 and perplexity is 238.08127893221015
At time: 133.90262699127197 and batch: 250, loss is 5.496686029434204 and perplexity is 243.8823725724129
At time: 134.70627856254578 and batch: 300, loss is 5.498093566894531 and perplexity is 244.2258878463169
At time: 135.50932908058167 and batch: 350, loss is 5.516430616378784 and perplexity is 248.74558232382373
At time: 136.32444071769714 and batch: 400, loss is 5.495250844955445 and perplexity is 243.53260742544492
At time: 137.12605905532837 and batch: 450, loss is 5.492497234344483 and perplexity is 242.86293588399565
At time: 137.93327403068542 and batch: 500, loss is 5.4703141212463375 and perplexity is 237.534795769454
At time: 138.7368381023407 and batch: 550, loss is 5.464513540267944 and perplexity is 236.16094437168258
At time: 139.53276801109314 and batch: 600, loss is 5.479315633773804 and perplexity is 239.68262054241117
At time: 140.32730436325073 and batch: 650, loss is 5.4693574047088624 and perplexity is 237.3076509760313
At time: 141.12144351005554 and batch: 700, loss is 5.489509372711182 and perplexity is 242.13837801407712
At time: 141.9182915687561 and batch: 750, loss is 5.459624691009521 and perplexity is 235.00920674370468
At time: 142.7403061389923 and batch: 800, loss is 5.470885353088379 and perplexity is 237.67052197026348
At time: 143.54533171653748 and batch: 850, loss is 5.508338041305542 and perplexity is 246.74071324302795
At time: 144.34411573410034 and batch: 900, loss is 5.500139541625977 and perplexity is 244.72607935672696
At time: 145.20289254188538 and batch: 950, loss is 5.477710542678833 and perplexity is 239.2982166866909
At time: 146.03553295135498 and batch: 1000, loss is 5.461050148010254 and perplexity is 235.3444411370362
At time: 146.83221244812012 and batch: 1050, loss is 5.467623205184936 and perplexity is 236.8964687997026
At time: 147.63628959655762 and batch: 1100, loss is 5.439745140075684 and perplexity is 230.38346046659282
At time: 148.43261456489563 and batch: 1150, loss is 5.481116743087768 and perplexity is 240.1147041407558
At time: 149.22725677490234 and batch: 1200, loss is 5.481862592697143 and perplexity is 240.2938604025531
At time: 150.02335739135742 and batch: 1250, loss is 5.492549991607666 and perplexity is 242.87574900581112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.264244358034899 and perplexity of 193.30018785301596
Finished 7 epochs...
Completing Train Step...
At time: 152.25817251205444 and batch: 50, loss is 5.493759336471558 and perplexity is 243.16964722176508
At time: 153.0563313961029 and batch: 100, loss is 5.510179166793823 and perplexity is 247.19541230975418
At time: 153.85143995285034 and batch: 150, loss is 5.4237410259246825 and perplexity is 226.7257247482644
At time: 154.6463406085968 and batch: 200, loss is 5.455288257598877 and perplexity is 233.99231141075063
At time: 155.44277596473694 and batch: 250, loss is 5.4796901512146 and perplexity is 239.7724026754979
At time: 156.23830032348633 and batch: 300, loss is 5.479821252822876 and perplexity is 239.8038392837588
At time: 157.0453338623047 and batch: 350, loss is 5.4944095802307125 and perplexity is 243.32781818655508
At time: 157.84345078468323 and batch: 400, loss is 5.469575595855713 and perplexity is 237.3594350537624
At time: 158.63927459716797 and batch: 450, loss is 5.457433881759644 and perplexity is 234.4949099685338
At time: 159.43435406684875 and batch: 500, loss is 5.4428520774841305 and perplexity is 231.1003605633784
At time: 160.23021125793457 and batch: 550, loss is 5.440873851776123 and perplexity is 230.643643782363
At time: 161.05353617668152 and batch: 600, loss is 5.456834850311279 and perplexity is 234.3544822075029
At time: 161.85749077796936 and batch: 650, loss is 5.448482370376587 and perplexity is 232.40519312870924
At time: 162.6756558418274 and batch: 700, loss is 5.468220901489258 and perplexity is 237.0381032665933
At time: 163.4729299545288 and batch: 750, loss is 5.439017181396484 and perplexity is 230.21581185503646
At time: 164.26517724990845 and batch: 800, loss is 5.451084461212158 and perplexity is 233.01072002822434
At time: 165.06159448623657 and batch: 850, loss is 5.484746408462525 and perplexity is 240.9878237758252
At time: 165.95007944107056 and batch: 900, loss is 5.476449747085571 and perplexity is 238.99670066459385
At time: 166.75906443595886 and batch: 950, loss is 5.453944444656372 and perplexity is 233.6780806951689
At time: 167.5829997062683 and batch: 1000, loss is 5.438526525497436 and perplexity is 230.10288281581086
At time: 168.38009548187256 and batch: 1050, loss is 5.441661558151245 and perplexity is 230.82539482478134
At time: 169.17568564414978 and batch: 1100, loss is 5.417653408050537 and perplexity is 225.34969778644543
At time: 169.99282455444336 and batch: 1150, loss is 5.458852090835571 and perplexity is 234.82770871142844
At time: 170.78904819488525 and batch: 1200, loss is 5.458243560791016 and perplexity is 234.68485246595054
At time: 171.5839056968689 and batch: 1250, loss is 5.4666251277923585 and perplexity is 236.6601457437798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.244043503364507 and perplexity of 189.4345350509777
Finished 8 epochs...
Completing Train Step...
At time: 173.89713191986084 and batch: 50, loss is 5.460175933837891 and perplexity is 235.13878959605066
At time: 174.75996708869934 and batch: 100, loss is 5.4790646266937255 and perplexity is 239.62246605759992
At time: 175.58495211601257 and batch: 150, loss is 5.393559494018555 and perplexity is 219.98502899597415
At time: 176.377347946167 and batch: 200, loss is 5.425582447052002 and perplexity is 227.1436069183704
At time: 177.17311596870422 and batch: 250, loss is 5.450070428848266 and perplexity is 232.77455937449125
At time: 177.968416929245 and batch: 300, loss is 5.446835870742798 and perplexity is 232.02285291122206
At time: 178.76413321495056 and batch: 350, loss is 5.468461313247681 and perplexity is 237.09509686450386
At time: 179.56027507781982 and batch: 400, loss is 5.442721433639527 and perplexity is 231.07017069588818
At time: 180.35701704025269 and batch: 450, loss is 5.4264817047119145 and perplexity is 227.34795941579517
At time: 181.1632194519043 and batch: 500, loss is 5.4142224311828615 and perplexity is 224.57785303410182
At time: 181.99401903152466 and batch: 550, loss is 5.416183271408081 and perplexity is 225.01864634341686
At time: 182.80044198036194 and batch: 600, loss is 5.436707067489624 and perplexity is 229.68460092161288
At time: 183.62327432632446 and batch: 650, loss is 5.425725641250611 and perplexity is 227.1761348939853
At time: 184.41957354545593 and batch: 700, loss is 5.444681587219239 and perplexity is 231.52354791747044
At time: 185.2141568660736 and batch: 750, loss is 5.4163656902313235 and perplexity is 225.05969772424888
At time: 186.0110330581665 and batch: 800, loss is 5.432765588760376 and perplexity is 228.7810857153228
At time: 186.83765363693237 and batch: 850, loss is 5.464859571456909 and perplexity is 236.2426775643519
At time: 187.66580986976624 and batch: 900, loss is 5.4561302280426025 and perplexity is 234.1894089845396
At time: 188.46054244041443 and batch: 950, loss is 5.431326322555542 and perplexity is 228.4520456752524
At time: 189.26682353019714 and batch: 1000, loss is 5.416354303359985 and perplexity is 225.05713501301813
At time: 190.07497119903564 and batch: 1050, loss is 5.4214632701873775 and perplexity is 226.20988662756525
At time: 190.8732442855835 and batch: 1100, loss is 5.399504985809326 and perplexity is 221.29684400839312
At time: 191.66668510437012 and batch: 1150, loss is 5.439249782562256 and perplexity is 230.2693665494538
At time: 192.46044826507568 and batch: 1200, loss is 5.441129131317139 and perplexity is 230.70252990177684
At time: 193.25762391090393 and batch: 1250, loss is 5.447663688659668 and perplexity is 232.21500510852798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.232309605953467 and perplexity of 187.22471987754136
Finished 9 epochs...
Completing Train Step...
At time: 195.50007820129395 and batch: 50, loss is 5.43570990562439 and perplexity is 229.45568234994573
At time: 196.2996609210968 and batch: 100, loss is 5.456652593612671 and perplexity is 234.31177342537086
At time: 197.0940797328949 and batch: 150, loss is 5.370157108306885 and perplexity is 214.89662709756402
At time: 197.8871991634369 and batch: 200, loss is 5.402399406433106 and perplexity is 221.93829802871704
At time: 198.67788171768188 and batch: 250, loss is 5.427257242202759 and perplexity is 227.52434466963706
At time: 199.472065448761 and batch: 300, loss is 5.422359218597412 and perplexity is 226.41264983494645
At time: 200.2671275138855 and batch: 350, loss is 5.442781763076782 and perplexity is 231.08411144976753
At time: 201.060711145401 and batch: 400, loss is 5.420411376953125 and perplexity is 225.97206308271853
At time: 201.854186296463 and batch: 450, loss is 5.401482429504394 and perplexity is 221.73487900935473
At time: 202.64716482162476 and batch: 500, loss is 5.392562789916992 and perplexity is 219.76587824765875
At time: 203.44124245643616 and batch: 550, loss is 5.394498090744019 and perplexity is 220.19160315358954
At time: 204.23417568206787 and batch: 600, loss is 5.416797666549683 and perplexity is 225.15693918537494
At time: 205.0659101009369 and batch: 650, loss is 5.409124135971069 and perplexity is 223.43580257024792
At time: 205.8809130191803 and batch: 700, loss is 5.425349950790405 and perplexity is 227.0908030175089
At time: 206.6751856803894 and batch: 750, loss is 5.398693552017212 and perplexity is 221.11734910504026
At time: 207.5116410255432 and batch: 800, loss is 5.41732424736023 and perplexity is 225.27553373097703
At time: 208.3272306919098 and batch: 850, loss is 5.44668758392334 and perplexity is 231.98844953116918
At time: 209.1272795200348 and batch: 900, loss is 5.439036779403686 and perplexity is 230.22032367038636
At time: 209.93993592262268 and batch: 950, loss is 5.413776388168335 and perplexity is 224.47770398859015
At time: 210.7436339855194 and batch: 1000, loss is 5.398696260452271 and perplexity is 221.11794798783163
At time: 211.5456576347351 and batch: 1050, loss is 5.4047064685821535 and perplexity is 222.4509145672614
At time: 212.33939146995544 and batch: 1100, loss is 5.384683246612549 and perplexity is 218.0410279337899
At time: 213.14456987380981 and batch: 1150, loss is 5.422795448303223 and perplexity is 226.5114393044597
At time: 213.9401249885559 and batch: 1200, loss is 5.424072427749634 and perplexity is 226.80087451890716
At time: 214.73238229751587 and batch: 1250, loss is 5.42727725982666 and perplexity is 227.5288992119823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.217169323106752 and perplexity of 184.41143536445847
Finished 10 epochs...
Completing Train Step...
At time: 216.9570803642273 and batch: 50, loss is 5.410336475372315 and perplexity is 223.70684686299288
At time: 217.80583810806274 and batch: 100, loss is 5.432966871261597 and perplexity is 228.8271399792907
At time: 218.6024305820465 and batch: 150, loss is 5.349539079666138 and perplexity is 210.51124658787967
At time: 219.3975899219513 and batch: 200, loss is 5.38261191368103 and perplexity is 217.58985979323046
At time: 220.19636249542236 and batch: 250, loss is 5.405993127822876 and perplexity is 222.73731730394903
At time: 221.00702238082886 and batch: 300, loss is 5.403614263534546 and perplexity is 222.2080851894018
At time: 221.8023726940155 and batch: 350, loss is 5.425486392974854 and perplexity is 227.12178989665145
At time: 222.600430727005 and batch: 400, loss is 5.401639080047607 and perplexity is 221.76961661936244
At time: 223.41166949272156 and batch: 450, loss is 5.378186044692993 and perplexity is 216.62896354963058
At time: 224.22040438652039 and batch: 500, loss is 5.374214115142823 and perplexity is 215.7702351012923
At time: 225.01773834228516 and batch: 550, loss is 5.374329404830933 and perplexity is 215.79511261843356
At time: 225.81343531608582 and batch: 600, loss is 5.399231796264648 and perplexity is 221.23639628155928
At time: 226.63665747642517 and batch: 650, loss is 5.392032852172852 and perplexity is 219.6494468673175
At time: 227.43504428863525 and batch: 700, loss is 5.4067908382415775 and perplexity is 222.91506806998905
At time: 228.28033924102783 and batch: 750, loss is 5.380388927459717 and perplexity is 217.10669776324912
At time: 229.07529044151306 and batch: 800, loss is 5.401844806671143 and perplexity is 221.81524522714113
At time: 229.8713595867157 and batch: 850, loss is 5.4301733303070066 and perplexity is 228.18879403014364
At time: 230.66600918769836 and batch: 900, loss is 5.420598745346069 and perplexity is 226.0144070718673
At time: 231.46203899383545 and batch: 950, loss is 5.3959402656555175 and perplexity is 220.50938705428567
At time: 232.2582266330719 and batch: 1000, loss is 5.381245565414429 and perplexity is 217.29275928307706
At time: 233.05413365364075 and batch: 1050, loss is 5.38869740486145 and perplexity is 218.9180381759789
At time: 233.84926319122314 and batch: 1100, loss is 5.370261878967285 and perplexity is 214.91914313859237
At time: 234.6509246826172 and batch: 1150, loss is 5.402689542770386 and perplexity is 222.00269973579515
At time: 235.47759652137756 and batch: 1200, loss is 5.406892261505127 and perplexity is 222.93767799025363
At time: 236.28305840492249 and batch: 1250, loss is 5.409258031845093 and perplexity is 223.46572170530092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.20552051850479 and perplexity of 182.27572596855697
Finished 11 epochs...
Completing Train Step...
At time: 238.51656460762024 and batch: 50, loss is 5.389143028259277 and perplexity is 219.01561491562103
At time: 239.3125557899475 and batch: 100, loss is 5.4094424819946285 and perplexity is 223.50694379267873
At time: 240.13638639450073 and batch: 150, loss is 5.3290900611877445 and perplexity is 206.25021366953249
At time: 240.93140196800232 and batch: 200, loss is 5.362683687210083 and perplexity is 213.2966003946936
At time: 241.7266879081726 and batch: 250, loss is 5.386833868026733 and perplexity is 218.51045623798458
At time: 242.53899264335632 and batch: 300, loss is 5.3844273567199705 and perplexity is 217.9852405765891
At time: 243.33103847503662 and batch: 350, loss is 5.40564227104187 and perplexity is 222.65918211372198
At time: 244.12694907188416 and batch: 400, loss is 5.38274580001831 and perplexity is 217.61899405288355
At time: 244.9219470024109 and batch: 450, loss is 5.358076009750366 and perplexity is 212.31605920039706
At time: 245.71532464027405 and batch: 500, loss is 5.355683250427246 and perplexity is 211.80864527209224
At time: 246.511332988739 and batch: 550, loss is 5.355498304367066 and perplexity is 211.76947571987586
At time: 247.3192596435547 and batch: 600, loss is 5.38078574180603 and perplexity is 217.19286591085017
At time: 248.136474609375 and batch: 650, loss is 5.376221675872802 and perplexity is 216.20384205249334
At time: 248.9930498600006 and batch: 700, loss is 5.388485231399536 and perplexity is 218.8715945051752
At time: 249.80297684669495 and batch: 750, loss is 5.366365947723389 and perplexity is 214.08346186932755
At time: 250.62126874923706 and batch: 800, loss is 5.388233118057251 and perplexity is 218.8164210112324
At time: 251.41730499267578 and batch: 850, loss is 5.411790132522583 and perplexity is 224.03227639467607
At time: 252.21434044837952 and batch: 900, loss is 5.401200742721557 and perplexity is 221.6724280208658
At time: 253.0088987350464 and batch: 950, loss is 5.377962350845337 and perplexity is 216.58051040279796
At time: 253.8043990135193 and batch: 1000, loss is 5.361701507568359 and perplexity is 213.08720766360875
At time: 254.60658812522888 and batch: 1050, loss is 5.371852741241455 and perplexity is 215.26132200296504
At time: 255.42922925949097 and batch: 1100, loss is 5.354959211349487 and perplexity is 211.65534304100828
At time: 256.236665725708 and batch: 1150, loss is 5.389904470443725 and perplexity is 219.18244615198049
At time: 257.0344648361206 and batch: 1200, loss is 5.391294269561768 and perplexity is 219.4872775004363
At time: 257.8546562194824 and batch: 1250, loss is 5.394290800094605 and perplexity is 220.14596422360137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.19630465542313 and perplexity of 180.60361464408058
Finished 12 epochs...
Completing Train Step...
At time: 260.0932309627533 and batch: 50, loss is 5.371087980270386 and perplexity is 215.09676147807016
At time: 260.93637323379517 and batch: 100, loss is 5.385667190551758 and perplexity is 218.25567366408964
At time: 261.7401795387268 and batch: 150, loss is 5.307245407104492 and perplexity is 201.79360287692725
At time: 262.54581332206726 and batch: 200, loss is 5.3448866271972655 and perplexity is 209.53412778087252
At time: 263.341402053833 and batch: 250, loss is 5.366809587478638 and perplexity is 214.17845887462022
At time: 264.13756942749023 and batch: 300, loss is 5.367425889968872 and perplexity is 214.31049827610565
At time: 264.932119846344 and batch: 350, loss is 5.386853847503662 and perplexity is 218.51482200621646
At time: 265.7368369102478 and batch: 400, loss is 5.366738166809082 and perplexity is 214.1631626519226
At time: 266.5533094406128 and batch: 450, loss is 5.339454307556152 and perplexity is 208.39895751848175
At time: 267.36245465278625 and batch: 500, loss is 5.338809900283813 and perplexity is 208.26470697536791
At time: 268.17411398887634 and batch: 550, loss is 5.336309328079223 and perplexity is 207.7445766206873
At time: 268.9863576889038 and batch: 600, loss is 5.364938459396362 and perplexity is 213.77807824413705
At time: 269.8346652984619 and batch: 650, loss is 5.359150867462159 and perplexity is 212.54439144431953
At time: 270.6470341682434 and batch: 700, loss is 5.374054470062256 and perplexity is 215.73579119419895
At time: 271.4623210430145 and batch: 750, loss is 5.355330257415772 and perplexity is 211.733891495098
At time: 272.27407789230347 and batch: 800, loss is 5.373893823623657 and perplexity is 215.70113679129304
At time: 273.0994691848755 and batch: 850, loss is 5.399980382919312 and perplexity is 221.4020728992571
At time: 273.89554142951965 and batch: 900, loss is 5.384273710250855 and perplexity is 217.95175048693807
At time: 274.69273233413696 and batch: 950, loss is 5.362440605163574 and perplexity is 213.24475812177434
At time: 275.4898769855499 and batch: 1000, loss is 5.347969617843628 and perplexity is 210.18111635431507
At time: 276.28748512268066 and batch: 1050, loss is 5.356309947967529 and perplexity is 211.94142683168747
At time: 277.0850365161896 and batch: 1100, loss is 5.343812646865845 and perplexity is 209.3092130474932
At time: 277.8876349925995 and batch: 1150, loss is 5.372870378494262 and perplexity is 215.48049144190256
At time: 278.68498611450195 and batch: 1200, loss is 5.374755992889404 and perplexity is 215.88718787425907
At time: 279.5000329017639 and batch: 1250, loss is 5.3797581768035885 and perplexity is 216.96980074966427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.187495990391195 and perplexity of 179.01972412115552
Finished 13 epochs...
Completing Train Step...
At time: 281.9052104949951 and batch: 50, loss is 5.3547530841827395 and perplexity is 211.61171962096066
At time: 282.72742104530334 and batch: 100, loss is 5.365425128936767 and perplexity is 213.88214284370622
At time: 283.5385365486145 and batch: 150, loss is 5.28988127708435 and perplexity is 198.31987889739958
At time: 284.3375279903412 and batch: 200, loss is 5.331995153427124 and perplexity is 206.85026073866507
At time: 285.13262248039246 and batch: 250, loss is 5.352600193023681 and perplexity is 211.15663267274434
At time: 285.92965960502625 and batch: 300, loss is 5.350544414520264 and perplexity is 210.7229872985844
At time: 286.7250106334686 and batch: 350, loss is 5.370641679763794 and perplexity is 215.0007851032016
At time: 287.51855635643005 and batch: 400, loss is 5.353927526473999 and perplexity is 211.43709402623864
At time: 288.3167042732239 and batch: 450, loss is 5.32194091796875 and perplexity is 204.7809595648935
At time: 289.11473059654236 and batch: 500, loss is 5.323157720565796 and perplexity is 205.03028923004638
At time: 289.91180539131165 and batch: 550, loss is 5.323487539291381 and perplexity is 205.0979232116104
At time: 290.7624158859253 and batch: 600, loss is 5.352204866409302 and perplexity is 211.07317333398197
At time: 291.55956292152405 and batch: 650, loss is 5.343774003982544 and perplexity is 209.30112489227534
At time: 292.35536789894104 and batch: 700, loss is 5.35818657875061 and perplexity is 212.33953607268177
At time: 293.15119099617004 and batch: 750, loss is 5.341266679763794 and perplexity is 208.77699646735346
At time: 293.9485249519348 and batch: 800, loss is 5.3630724620819095 and perplexity is 213.3795408747156
At time: 294.7440299987793 and batch: 850, loss is 5.388313474655152 and perplexity is 218.83400506087747
At time: 295.5401711463928 and batch: 900, loss is 5.3687476062774655 and perplexity is 214.5939432324274
At time: 296.3367156982422 and batch: 950, loss is 5.348745536804199 and perplexity is 210.3442631538108
At time: 297.13325786590576 and batch: 1000, loss is 5.3345319938659665 and perplexity is 207.37567300665944
At time: 297.92942571640015 and batch: 1050, loss is 5.344681949615478 and perplexity is 209.49124523099445
At time: 298.72440242767334 and batch: 1100, loss is 5.32882134437561 and perplexity is 206.19479821547873
At time: 299.54153203964233 and batch: 1150, loss is 5.361651935577393 and perplexity is 213.07664476828947
At time: 300.33926582336426 and batch: 1200, loss is 5.3639559841156 and perplexity is 213.56814970837823
At time: 301.13628125190735 and batch: 1250, loss is 5.36914831161499 and perplexity is 214.67994940129572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.183421336821396 and perplexity of 178.29176486127923
Finished 14 epochs...
Completing Train Step...
At time: 303.4017572402954 and batch: 50, loss is 5.34251989364624 and perplexity is 209.03880271300764
At time: 304.2242007255554 and batch: 100, loss is 5.3508836460113525 and perplexity is 210.794483297934
At time: 305.04106307029724 and batch: 150, loss is 5.274428787231446 and perplexity is 195.27889884784022
At time: 305.8628194332123 and batch: 200, loss is 5.317170743942261 and perplexity is 203.80644490061457
At time: 306.6696095466614 and batch: 250, loss is 5.337483758926392 and perplexity is 207.98870158568744
At time: 307.49602150917053 and batch: 300, loss is 5.337995290756226 and perplexity is 208.09512164309714
At time: 308.3152663707733 and batch: 350, loss is 5.355258197784424 and perplexity is 211.71863457864345
At time: 309.10893726348877 and batch: 400, loss is 5.34047565460205 and perplexity is 208.61191391079052
At time: 309.90320086479187 and batch: 450, loss is 5.307653932571411 and perplexity is 201.87605754403276
At time: 310.6991617679596 and batch: 500, loss is 5.310370893478393 and perplexity is 202.42529268777722
At time: 311.5448160171509 and batch: 550, loss is 5.309668035507202 and perplexity is 202.28306644552
At time: 312.34243273735046 and batch: 600, loss is 5.33875467300415 and perplexity is 208.25320539975507
At time: 313.14469146728516 and batch: 650, loss is 5.33142086982727 and perplexity is 206.73150412954766
At time: 313.9512450695038 and batch: 700, loss is 5.3488359451293945 and perplexity is 210.36328088602465
At time: 314.77136754989624 and batch: 750, loss is 5.333472757339478 and perplexity is 207.15612941392888
At time: 315.59781789779663 and batch: 800, loss is 5.351701164245606 and perplexity is 210.9668820916871
At time: 316.42723965644836 and batch: 850, loss is 5.3796055889129635 and perplexity is 216.936696311171
At time: 317.2243995666504 and batch: 900, loss is 5.355927114486694 and perplexity is 211.86030408676262
At time: 318.02046155929565 and batch: 950, loss is 5.334785747528076 and perplexity is 207.42830202023734
At time: 318.81781244277954 and batch: 1000, loss is 5.321403818130493 and perplexity is 204.67100127656602
At time: 319.615704536438 and batch: 1050, loss is 5.330440979003907 and perplexity is 206.5290290436914
At time: 320.41603994369507 and batch: 1100, loss is 5.3180536365509035 and perplexity is 203.98646356128984
At time: 321.21230959892273 and batch: 1150, loss is 5.350688791275024 and perplexity is 210.75341299597287
At time: 322.00838589668274 and batch: 1200, loss is 5.354201965332031 and perplexity is 211.4951285439679
At time: 322.8054790496826 and batch: 1250, loss is 5.357684679031372 and perplexity is 212.23298965918795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.181577362283303 and perplexity of 177.96330231787775
Finished 15 epochs...
Completing Train Step...
At time: 325.0525770187378 and batch: 50, loss is 5.331034259796143 and perplexity is 206.6515951041118
At time: 325.8542392253876 and batch: 100, loss is 5.336995019912719 and perplexity is 207.8870742294693
At time: 326.65203189849854 and batch: 150, loss is 5.261586923599243 and perplexity is 192.7871872120799
At time: 327.44710421562195 and batch: 200, loss is 5.305828580856323 and perplexity is 201.50789884793576
At time: 328.2510635852814 and batch: 250, loss is 5.3240610694885255 and perplexity is 205.21558680252974
At time: 329.0495812892914 and batch: 300, loss is 5.326478538513183 and perplexity is 205.71228926651182
At time: 329.850772857666 and batch: 350, loss is 5.341752271652222 and perplexity is 208.87840150206696
At time: 330.68327283859253 and batch: 400, loss is 5.330163116455078 and perplexity is 206.4716503333408
At time: 331.50027561187744 and batch: 450, loss is 5.297652158737183 and perplexity is 199.86700267816215
At time: 332.34657287597656 and batch: 500, loss is 5.301940870285034 and perplexity is 200.72601531263052
At time: 333.16196727752686 and batch: 550, loss is 5.299757785797119 and perplexity is 200.2882914302335
At time: 334.2851984500885 and batch: 600, loss is 5.3287820148468015 and perplexity is 206.18668883069228
At time: 335.0811057090759 and batch: 650, loss is 5.319909505844116 and perplexity is 204.36538728293496
At time: 335.87629890441895 and batch: 700, loss is 5.338374261856079 and perplexity is 208.17399862532469
At time: 336.6870174407959 and batch: 750, loss is 5.322280836105347 and perplexity is 204.85058015906034
At time: 337.49643898010254 and batch: 800, loss is 5.345709028244019 and perplexity is 209.70651974483954
At time: 338.3070378303528 and batch: 850, loss is 5.368737401962281 and perplexity is 214.5917534593665
At time: 339.10813164711 and batch: 900, loss is 5.3457230281829835 and perplexity is 209.70945564386759
At time: 339.9064438343048 and batch: 950, loss is 5.324843244552612 and perplexity is 205.37616410888432
At time: 340.7049741744995 and batch: 1000, loss is 5.310873117446899 and perplexity is 202.52698105462704
At time: 341.5022315979004 and batch: 1050, loss is 5.319616594314575 and perplexity is 204.30553507089016
At time: 342.31169986724854 and batch: 1100, loss is 5.3077601146698 and perplexity is 201.89749430551723
At time: 343.12349700927734 and batch: 1150, loss is 5.340915374755859 and perplexity is 208.7036649445727
At time: 343.92483592033386 and batch: 1200, loss is 5.344187774658203 and perplexity is 209.38774547943186
At time: 344.7216079235077 and batch: 1250, loss is 5.348733730316162 and perplexity is 210.34177974144444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.175958563811587 and perplexity of 176.96616636315346
Finished 16 epochs...
Completing Train Step...
At time: 346.9775948524475 and batch: 50, loss is 5.320851573944092 and perplexity is 204.55800410987197
At time: 347.7753999233246 and batch: 100, loss is 5.326046924591065 and perplexity is 205.62352013688775
At time: 348.57141184806824 and batch: 150, loss is 5.250435009002685 and perplexity is 190.6491845343298
At time: 349.36781334877014 and batch: 200, loss is 5.29523814201355 and perplexity is 199.3851022830231
At time: 350.1644492149353 and batch: 250, loss is 5.314296684265137 and perplexity is 203.2215339524974
At time: 350.96155309677124 and batch: 300, loss is 5.3148108577728275 and perplexity is 203.3260519493379
At time: 351.75716185569763 and batch: 350, loss is 5.330812511444091 and perplexity is 206.60577553384354
At time: 352.6009566783905 and batch: 400, loss is 5.320130805969239 and perplexity is 204.41061837334982
At time: 353.39296865463257 and batch: 450, loss is 5.285589256286621 and perplexity is 197.47050991112891
At time: 354.18736815452576 and batch: 500, loss is 5.288358106613159 and perplexity is 198.0180338530999
At time: 354.99690318107605 and batch: 550, loss is 5.2869282054901126 and perplexity is 197.7350899831979
At time: 355.79624605178833 and batch: 600, loss is 5.315996971130371 and perplexity is 203.56736277819934
At time: 356.59206104278564 and batch: 650, loss is 5.309061212539673 and perplexity is 202.16035367108918
At time: 357.38846039772034 and batch: 700, loss is 5.32678713798523 and perplexity is 205.77578176674513
At time: 358.1845624446869 and batch: 750, loss is 5.311570243835449 and perplexity is 202.66821718150925
At time: 358.9892563819885 and batch: 800, loss is 5.332928352355957 and perplexity is 207.04338327727294
At time: 359.8142695426941 and batch: 850, loss is 5.358082265853882 and perplexity is 212.3173874757963
At time: 360.609402179718 and batch: 900, loss is 5.3321192169189455 and perplexity is 206.875924896256
At time: 361.40602850914 and batch: 950, loss is 5.3097130489349365 and perplexity is 202.2921721046503
At time: 362.2009060382843 and batch: 1000, loss is 5.295175809860229 and perplexity is 199.37267456758488
At time: 363.0094907283783 and batch: 1050, loss is 5.302739696502686 and perplexity is 200.88642457725993
At time: 363.8056285381317 and batch: 1100, loss is 5.2880489540100095 and perplexity is 197.95682552430827
At time: 364.60268092155457 and batch: 1150, loss is 5.320331268310547 and perplexity is 204.4515991119073
At time: 365.3999147415161 and batch: 1200, loss is 5.325492687225342 and perplexity is 205.50958747454223
At time: 366.1972830295563 and batch: 1250, loss is 5.331525688171387 and perplexity is 206.75317451919454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.150025305086679 and perplexity of 172.43585376587143
Finished 17 epochs...
Completing Train Step...
At time: 368.4311122894287 and batch: 50, loss is 5.302105150222778 and perplexity is 200.7589932786646
At time: 369.2267861366272 and batch: 100, loss is 5.304291009902954 and perplexity is 201.1983042286331
At time: 370.02899265289307 and batch: 150, loss is 5.229213218688965 and perplexity is 186.64589623285497
At time: 370.82327032089233 and batch: 200, loss is 5.272265319824219 and perplexity is 194.85687599581615
At time: 371.6160442829132 and batch: 250, loss is 5.289975214004516 and perplexity is 198.33850933105998
At time: 372.4127082824707 and batch: 300, loss is 5.288490257263184 and perplexity is 198.04420379413946
At time: 373.21056485176086 and batch: 350, loss is 5.306027011871338 and perplexity is 201.54788823227352
At time: 374.01270866394043 and batch: 400, loss is 5.301202955245972 and perplexity is 200.57795120328544
At time: 374.80999660491943 and batch: 450, loss is 5.262482528686523 and perplexity is 192.95992573891502
At time: 375.6067678928375 and batch: 500, loss is 5.267940292358398 and perplexity is 194.01593451361168
At time: 376.3984375 and batch: 550, loss is 5.269858388900757 and perplexity is 194.38843293652857
At time: 377.1978096961975 and batch: 600, loss is 5.299145975112915 and perplexity is 200.16579039115885
At time: 377.9933075904846 and batch: 650, loss is 5.294644708633423 and perplexity is 199.26681560893013
At time: 378.79040002822876 and batch: 700, loss is 5.310938177108764 and perplexity is 202.54015782016629
At time: 379.58765625953674 and batch: 750, loss is 5.294398441314697 and perplexity is 199.21774874656933
At time: 380.3842628002167 and batch: 800, loss is 5.320831413269043 and perplexity is 204.55388012399385
At time: 381.181764125824 and batch: 850, loss is 5.344105892181396 and perplexity is 209.370600994145
At time: 381.9776220321655 and batch: 900, loss is 5.3106005668640135 and perplexity is 202.47178972944607
At time: 382.7735381126404 and batch: 950, loss is 5.292726716995239 and perplexity is 198.88498980922185
At time: 383.5706422328949 and batch: 1000, loss is 5.2810538864135745 and perplexity is 196.57693598549648
At time: 384.3684289455414 and batch: 1050, loss is 5.287245807647705 and perplexity is 197.79790104834515
At time: 385.1647536754608 and batch: 1100, loss is 5.2718661403656 and perplexity is 194.77910865614297
At time: 385.9657824039459 and batch: 1150, loss is 5.3011808013916015 and perplexity is 200.57350767778533
At time: 386.76552081108093 and batch: 1200, loss is 5.311755418777466 and perplexity is 202.70574973181107
At time: 387.5692663192749 and batch: 1250, loss is 5.316538372039795 and perplexity is 203.6776041732395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.143622850849681 and perplexity of 171.33536776814168
Finished 18 epochs...
Completing Train Step...
At time: 389.77417159080505 and batch: 50, loss is 5.288726825714111 and perplexity is 198.0910603468187
At time: 390.59455490112305 and batch: 100, loss is 5.290128812789917 and perplexity is 198.3689762249705
At time: 391.3883738517761 and batch: 150, loss is 5.2142398643493655 and perplexity is 183.87200018250053
At time: 392.1862518787384 and batch: 200, loss is 5.257497491836548 and perplexity is 192.0004070037146
At time: 392.9806396961212 and batch: 250, loss is 5.272040433883667 and perplexity is 194.81306035093084
At time: 393.7772991657257 and batch: 300, loss is 5.276271276473999 and perplexity is 195.6390297850738
At time: 394.5724287033081 and batch: 350, loss is 5.292834959030151 and perplexity is 198.9065186903762
At time: 395.3685505390167 and batch: 400, loss is 5.287526655197143 and perplexity is 197.85345990555797
At time: 396.1636004447937 and batch: 450, loss is 5.247838325500489 and perplexity is 190.15477113750708
At time: 396.96020770072937 and batch: 500, loss is 5.257148056030274 and perplexity is 191.93332690746453
At time: 397.76798844337463 and batch: 550, loss is 5.258893327713013 and perplexity is 192.26859518990827
At time: 398.5704164505005 and batch: 600, loss is 5.284290418624878 and perplexity is 197.2141942680113
At time: 399.3903934955597 and batch: 650, loss is 5.281162815093994 and perplexity is 196.5983500180143
At time: 400.19246888160706 and batch: 700, loss is 5.3005568885803225 and perplexity is 200.44840632696787
At time: 400.9933202266693 and batch: 750, loss is 5.286699714660645 and perplexity is 197.68991448976232
At time: 401.8202819824219 and batch: 800, loss is 5.31089153289795 and perplexity is 202.5307107146746
At time: 402.6186501979828 and batch: 850, loss is 5.3326682853698735 and perplexity is 206.9895451296614
At time: 403.4111707210541 and batch: 900, loss is 5.301469316482544 and perplexity is 200.631384510362
At time: 404.2057559490204 and batch: 950, loss is 5.279907369613648 and perplexity is 196.35168637681548
At time: 405.00904059410095 and batch: 1000, loss is 5.272838268280029 and perplexity is 194.96855093095792
At time: 405.83416628837585 and batch: 1050, loss is 5.275862054824829 and perplexity is 195.55898643751559
At time: 406.62856435775757 and batch: 1100, loss is 5.2597041416168215 and perplexity is 192.4245524577705
At time: 407.4250521659851 and batch: 1150, loss is 5.291069145202637 and perplexity is 198.5555967318974
At time: 408.2219235897064 and batch: 1200, loss is 5.300199861526489 and perplexity is 200.3768535969017
At time: 409.0669264793396 and batch: 1250, loss is 5.307430191040039 and perplexity is 201.83089453837889
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1368893811302465 and perplexity of 170.18556169390914
Finished 19 epochs...
Completing Train Step...
At time: 411.30931305885315 and batch: 50, loss is 5.280253448486328 and perplexity is 196.41965130702053
At time: 412.11029839515686 and batch: 100, loss is 5.280551500320435 and perplexity is 196.47820326967366
At time: 412.9495487213135 and batch: 150, loss is 5.20398398399353 and perplexity is 181.99586808574952
At time: 413.77063393592834 and batch: 200, loss is 5.244025135040284 and perplexity is 189.43105548797575
At time: 414.567822933197 and batch: 250, loss is 5.260169839859008 and perplexity is 192.51418510286638
At time: 415.39024901390076 and batch: 300, loss is 5.261352281570435 and perplexity is 192.74195654205994
At time: 416.19874334335327 and batch: 350, loss is 5.28099681854248 and perplexity is 196.5657180783476
At time: 417.00535583496094 and batch: 400, loss is 5.277593774795532 and perplexity is 195.89793323554514
At time: 417.8030152320862 and batch: 450, loss is 5.240244073867798 and perplexity is 188.71615746748782
At time: 418.6015329360962 and batch: 500, loss is 5.246748418807983 and perplexity is 189.94763308091441
At time: 419.397732257843 and batch: 550, loss is 5.247138433456421 and perplexity is 190.02172988873298
At time: 420.19809317588806 and batch: 600, loss is 5.274985132217407 and perplexity is 195.38757151102084
At time: 420.99290561676025 and batch: 650, loss is 5.274201955795288 and perplexity is 195.23460847817725
At time: 421.7912561893463 and batch: 700, loss is 5.289476985931397 and perplexity is 198.2397161305471
At time: 422.5865216255188 and batch: 750, loss is 5.27491024017334 and perplexity is 195.37293908433816
At time: 423.4004728794098 and batch: 800, loss is 5.300665159225463 and perplexity is 200.47011018016227
At time: 424.20152831077576 and batch: 850, loss is 5.3202755165100095 and perplexity is 204.44020088487292
At time: 424.9979531764984 and batch: 900, loss is 5.286371717453003 and perplexity is 197.62508338262293
At time: 425.7947676181793 and batch: 950, loss is 5.271057987213135 and perplexity is 194.62176089455676
At time: 426.59168553352356 and batch: 1000, loss is 5.259548234939575 and perplexity is 192.39455452367602
At time: 427.3904941082001 and batch: 1050, loss is 5.269119148254394 and perplexity is 194.24478620701137
At time: 428.1899173259735 and batch: 1100, loss is 5.248742980957031 and perplexity is 190.32687352374822
At time: 428.9865379333496 and batch: 1150, loss is 5.281685190200806 and perplexity is 196.7010749302365
At time: 429.8314514160156 and batch: 1200, loss is 5.291029357910157 and perplexity is 198.54769689945417
At time: 430.6273183822632 and batch: 1250, loss is 5.2973255443572995 and perplexity is 199.8017339004642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.131945087961907 and perplexity of 169.34619114086095
Finished 20 epochs...
Completing Train Step...
At time: 432.81319975852966 and batch: 50, loss is 5.270035123825073 and perplexity is 194.42279119757436
At time: 433.6364040374756 and batch: 100, loss is 5.271053066253662 and perplexity is 194.62080317111534
At time: 434.43130230903625 and batch: 150, loss is 5.19504545211792 and perplexity is 180.37634109741887
At time: 435.2275993824005 and batch: 200, loss is 5.237224817276001 and perplexity is 188.14723425983567
At time: 436.0235974788666 and batch: 250, loss is 5.253095598220825 and perplexity is 191.15709907550897
At time: 436.8188245296478 and batch: 300, loss is 5.253515958786011 and perplexity is 191.23747087309926
At time: 437.6159288883209 and batch: 350, loss is 5.27408200263977 and perplexity is 195.21119087535976
At time: 438.4144539833069 and batch: 400, loss is 5.270668058395386 and perplexity is 194.54588705508252
At time: 439.20898270606995 and batch: 450, loss is 5.233888721466064 and perplexity is 187.5206028922863
At time: 440.00446915626526 and batch: 500, loss is 5.236639194488525 and perplexity is 188.03708320868344
At time: 440.7986681461334 and batch: 550, loss is 5.240079746246338 and perplexity is 188.68514873806535
At time: 441.59363865852356 and batch: 600, loss is 5.2697379016876225 and perplexity is 194.36501302690658
At time: 442.3885188102722 and batch: 650, loss is 5.2621126556396485 and perplexity is 192.88856826067467
At time: 443.18324875831604 and batch: 700, loss is 5.2849985694885255 and perplexity is 197.3539011309352
At time: 443.97768926620483 and batch: 750, loss is 5.273423414230347 and perplexity is 195.08266937369066
At time: 444.77496671676636 and batch: 800, loss is 5.2929897880554195 and perplexity is 198.9373175770038
At time: 445.5722324848175 and batch: 850, loss is 5.313809652328491 and perplexity is 203.12258267342082
At time: 446.3662676811218 and batch: 900, loss is 5.280330581665039 and perplexity is 196.43480236340415
At time: 447.16619539260864 and batch: 950, loss is 5.263996658325195 and perplexity is 193.25231338209005
At time: 447.98059940338135 and batch: 1000, loss is 5.250944499969482 and perplexity is 190.74634332033744
At time: 448.7775423526764 and batch: 1050, loss is 5.259996070861816 and perplexity is 192.48073501235427
At time: 449.62322211265564 and batch: 1100, loss is 5.239069604873658 and perplexity is 188.49464629631245
At time: 450.419326543808 and batch: 1150, loss is 5.271931943893432 and perplexity is 194.7919262303567
At time: 451.2151572704315 and batch: 1200, loss is 5.283103771209717 and perplexity is 196.98030935106124
At time: 452.01389479637146 and batch: 1250, loss is 5.2902798748016355 and perplexity is 198.3989445050589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1315552648836675 and perplexity of 169.28018895276617
Finished 21 epochs...
Completing Train Step...
At time: 454.23993396759033 and batch: 50, loss is 5.263173809051514 and perplexity is 193.09336126217323
At time: 455.06343960762024 and batch: 100, loss is 5.262033948898315 and perplexity is 192.8733872274591
At time: 455.8597435951233 and batch: 150, loss is 5.1874203109741215 and perplexity is 179.0061765254329
At time: 456.6777603626251 and batch: 200, loss is 5.22807222366333 and perplexity is 186.43305564181668
At time: 457.4809904098511 and batch: 250, loss is 5.241887731552124 and perplexity is 189.0265972883737
At time: 458.27732586860657 and batch: 300, loss is 5.237783908843994 and perplexity is 188.25245520338188
At time: 459.0746157169342 and batch: 350, loss is 5.261225414276123 and perplexity is 192.7175054425883
At time: 459.87026715278625 and batch: 400, loss is 5.257688646316528 and perplexity is 192.03711224974566
At time: 460.6670069694519 and batch: 450, loss is 5.221469202041626 and perplexity is 185.2060894445415
At time: 461.4627730846405 and batch: 500, loss is 5.2253594493865965 and perplexity is 185.92799021812093
At time: 462.2570333480835 and batch: 550, loss is 5.226716775894165 and perplexity is 186.18052655610433
At time: 463.06340050697327 and batch: 600, loss is 5.2548075294494625 and perplexity is 191.48462715577574
At time: 463.87573742866516 and batch: 650, loss is 5.250413417816162 and perplexity is 190.64506823666406
At time: 464.66973328590393 and batch: 700, loss is 5.269960880279541 and perplexity is 194.4083570960497
At time: 465.49111104011536 and batch: 750, loss is 5.256301298141479 and perplexity is 191.7708746373602
At time: 466.3020615577698 and batch: 800, loss is 5.281455707550049 and perplexity is 196.65594062512287
At time: 467.1134943962097 and batch: 850, loss is 5.3013719463348385 and perplexity is 200.6118499538746
At time: 467.9236068725586 and batch: 900, loss is 5.2652942562103275 and perplexity is 193.5032399409093
At time: 468.7380533218384 and batch: 950, loss is 5.249641733169556 and perplexity is 190.49800711425763
At time: 469.53126883506775 and batch: 1000, loss is 5.238829202651978 and perplexity is 188.4493372109887
At time: 470.3532016277313 and batch: 1050, loss is 5.244021663665771 and perplexity is 189.43039790297925
At time: 471.1482229232788 and batch: 1100, loss is 5.2225925159454345 and perplexity is 185.41425091333184
At time: 471.9431688785553 and batch: 1150, loss is 5.252713069915772 and perplexity is 191.08399005842642
At time: 472.7381136417389 and batch: 1200, loss is 5.262182664871216 and perplexity is 192.90207271382943
At time: 473.5425069332123 and batch: 1250, loss is 5.272570934295654 and perplexity is 194.91643617774236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.121324525262318 and perplexity of 167.55715639411372
Finished 22 epochs...
Completing Train Step...
At time: 475.8278069496155 and batch: 50, loss is 5.247678756713867 and perplexity is 190.12443079215683
At time: 476.61984634399414 and batch: 100, loss is 5.2463079452514645 and perplexity is 189.8639845952444
At time: 477.41137981414795 and batch: 150, loss is 5.167441024780273 and perplexity is 175.46525125950265
At time: 478.2044427394867 and batch: 200, loss is 5.209066476821899 and perplexity is 182.92321540188178
At time: 478.999160528183 and batch: 250, loss is 5.22028151512146 and perplexity is 184.98625316874958
At time: 479.8200263977051 and batch: 300, loss is 5.216461553573608 and perplexity is 184.2809607473702
At time: 480.62156867980957 and batch: 350, loss is 5.236588697433472 and perplexity is 188.02758812947943
At time: 481.4182198047638 and batch: 400, loss is 5.238303575515747 and perplexity is 188.35030915374298
At time: 482.21381640434265 and batch: 450, loss is 5.197910661697388 and perplexity is 180.8938982185807
At time: 483.00971508026123 and batch: 500, loss is 5.203706359863281 and perplexity is 181.94534865419436
At time: 483.80544233322144 and batch: 550, loss is 5.202514219284057 and perplexity is 181.72857345978903
At time: 484.6024146080017 and batch: 600, loss is 5.227722244262695 and perplexity is 186.36781932909344
At time: 485.39741563796997 and batch: 650, loss is 5.219226360321045 and perplexity is 184.79116697687363
At time: 486.21449279785156 and batch: 700, loss is 5.235565786361694 and perplexity is 187.83535096530613
At time: 487.0541615486145 and batch: 750, loss is 5.225514650344849 and perplexity is 185.95684865973965
At time: 487.87540221214294 and batch: 800, loss is 5.2469691562652585 and perplexity is 189.98956626639827
At time: 488.67225646972656 and batch: 850, loss is 5.27438720703125 and perplexity is 195.27077928093988
At time: 489.4688811302185 and batch: 900, loss is 5.2412613487243656 and perplexity is 188.9082313489087
At time: 490.26931977272034 and batch: 950, loss is 5.229733419418335 and perplexity is 186.74301482259972
At time: 491.1426877975464 and batch: 1000, loss is 5.220359773635864 and perplexity is 185.00073048458694
At time: 491.9712977409363 and batch: 1050, loss is 5.219331521987915 and perplexity is 184.8106009458521
At time: 492.77209520339966 and batch: 1100, loss is 5.191095657348633 and perplexity is 179.66529673291444
At time: 493.5663220882416 and batch: 1150, loss is 5.224887208938599 and perplexity is 185.8402082294592
At time: 494.36231207847595 and batch: 1200, loss is 5.237587461471557 and perplexity is 188.21547713544348
At time: 495.157772064209 and batch: 1250, loss is 5.247120981216431 and perplexity is 190.01841361283795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104153152799954 and perplexity of 164.70453188436292
Finished 23 epochs...
Completing Train Step...
At time: 497.3722732067108 and batch: 50, loss is 5.221805181503296 and perplexity is 185.26832534117713
At time: 498.1962809562683 and batch: 100, loss is 5.2182473468780515 and perplexity is 184.61034246951147
At time: 498.9947531223297 and batch: 150, loss is 5.1454560089111325 and perplexity is 171.64974063832156
At time: 499.79107785224915 and batch: 200, loss is 5.193282260894775 and perplexity is 180.05858333210588
At time: 500.5889902114868 and batch: 250, loss is 5.200605707168579 and perplexity is 181.38207303092065
At time: 501.3849802017212 and batch: 300, loss is 5.195772142410278 and perplexity is 180.5074664714632
At time: 502.1822371482849 and batch: 350, loss is 5.212323427200317 and perplexity is 183.5199584913147
At time: 502.98016357421875 and batch: 400, loss is 5.211781320571899 and perplexity is 183.42049806688297
At time: 503.77679443359375 and batch: 450, loss is 5.171749658584595 and perplexity is 176.22289781123607
At time: 504.5746924877167 and batch: 500, loss is 5.1815456295013425 and perplexity is 177.95765513680922
At time: 505.36774706840515 and batch: 550, loss is 5.17491925239563 and perplexity is 176.78233894972573
At time: 506.17042684555054 and batch: 600, loss is 5.203980522155762 and perplexity is 181.99523804667024
At time: 506.97429871559143 and batch: 650, loss is 5.188186426162719 and perplexity is 179.14336842180944
At time: 507.7806165218353 and batch: 700, loss is 5.205750799179077 and perplexity is 182.31770537897705
At time: 508.58836698532104 and batch: 750, loss is 5.200977668762207 and perplexity is 181.4495527450138
At time: 509.3833155632019 and batch: 800, loss is 5.227270393371582 and perplexity is 186.28362788627646
At time: 510.1824128627777 and batch: 850, loss is 5.257283554077149 and perplexity is 191.95933526039144
At time: 510.9801187515259 and batch: 900, loss is 5.225365705490113 and perplexity is 185.9291534065128
At time: 511.8436162471771 and batch: 950, loss is 5.214505910873413 and perplexity is 183.92092519689518
At time: 512.6571843624115 and batch: 1000, loss is 5.202556924819946 and perplexity is 181.73633444162223
At time: 513.4544234275818 and batch: 1050, loss is 5.199547300338745 and perplexity is 181.19019856454995
At time: 514.2503793239594 and batch: 1100, loss is 5.1633469772338865 and perplexity is 174.748356679691
At time: 515.046151638031 and batch: 1150, loss is 5.194824895858765 and perplexity is 180.33656235327297
At time: 515.8593909740448 and batch: 1200, loss is 5.200519247055054 and perplexity is 181.36639139422272
At time: 516.6984615325928 and batch: 1250, loss is 5.222045888900757 and perplexity is 185.31292616526082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.090379255531478 and perplexity of 162.45146098705874
Finished 24 epochs...
Completing Train Step...
At time: 519.0216724872589 and batch: 50, loss is 5.194839191436768 and perplexity is 180.33914038709412
At time: 519.8184063434601 and batch: 100, loss is 5.1992516040802 and perplexity is 181.136629221267
At time: 520.61638879776 and batch: 150, loss is 5.123798503875732 and perplexity is 167.97220241134926
At time: 521.4294455051422 and batch: 200, loss is 5.178399143218994 and perplexity is 177.39859381671266
At time: 522.2275853157043 and batch: 250, loss is 5.188057508468628 and perplexity is 179.1202751604377
At time: 523.0255377292633 and batch: 300, loss is 5.178648538589478 and perplexity is 177.44284172212298
At time: 523.8257460594177 and batch: 350, loss is 5.19837854385376 and perplexity is 180.9785550489196
At time: 524.6420094966888 and batch: 400, loss is 5.198467073440551 and perplexity is 180.9945777148457
At time: 525.4383838176727 and batch: 450, loss is 5.157542152404785 and perplexity is 173.73691155102574
At time: 526.2344350814819 and batch: 500, loss is 5.162428903579712 and perplexity is 174.58799843891438
At time: 527.0325639247894 and batch: 550, loss is 5.158687152862549 and perplexity is 173.93595432456968
At time: 527.82959151268 and batch: 600, loss is 5.186443557739258 and perplexity is 178.83141702579852
At time: 528.6260187625885 and batch: 650, loss is 5.171641750335693 and perplexity is 176.20388293286643
At time: 529.4217381477356 and batch: 700, loss is 5.188275041580201 and perplexity is 179.1592439895908
At time: 530.2165911197662 and batch: 750, loss is 5.190821285247803 and perplexity is 179.61600834999132
At time: 531.012642621994 and batch: 800, loss is 5.215014581680298 and perplexity is 184.014504200754
At time: 531.8537497520447 and batch: 850, loss is 5.244048547744751 and perplexity is 189.43549063321404
At time: 532.6523098945618 and batch: 900, loss is 5.213533248901367 and perplexity is 183.7421192800474
At time: 533.4492983818054 and batch: 950, loss is 5.206656284332276 and perplexity is 182.48286611837707
At time: 534.2464444637299 and batch: 1000, loss is 5.1906740188598635 and perplexity is 179.58955889683105
At time: 535.0429062843323 and batch: 1050, loss is 5.1833200359344485 and perplexity is 178.2737046621359
At time: 535.8399577140808 and batch: 1100, loss is 5.149156665802002 and perplexity is 172.28613424474642
At time: 536.6366281509399 and batch: 1150, loss is 5.180849075317383 and perplexity is 177.8337411489695
At time: 537.4486663341522 and batch: 1200, loss is 5.186100606918335 and perplexity is 178.77009715997283
At time: 538.2559678554535 and batch: 1250, loss is 5.207271957397461 and perplexity is 182.5952504963713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.093773612140739 and perplexity of 163.00381609210785
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 540.4631485939026 and batch: 50, loss is 5.188689193725586 and perplexity is 179.23345854185192
At time: 541.2956848144531 and batch: 100, loss is 5.196995344161987 and perplexity is 180.72839861539342
At time: 542.0938906669617 and batch: 150, loss is 5.1194791507720945 and perplexity is 167.24823581679283
At time: 542.8913207054138 and batch: 200, loss is 5.165958766937256 and perplexity is 175.2053591757698
At time: 543.6924700737 and batch: 250, loss is 5.179592151641845 and perplexity is 177.6103581265242
At time: 544.4902920722961 and batch: 300, loss is 5.165960607528686 and perplexity is 175.20568165754923
At time: 545.2932789325714 and batch: 350, loss is 5.183493385314941 and perplexity is 178.30461097711512
At time: 546.1143171787262 and batch: 400, loss is 5.15975775718689 and perplexity is 174.12227062715138
At time: 546.9401977062225 and batch: 450, loss is 5.1301312923431395 and perplexity is 169.03931015514118
At time: 547.737711429596 and batch: 500, loss is 5.121384363174439 and perplexity is 167.56718296449486
At time: 548.5368275642395 and batch: 550, loss is 5.12097599029541 and perplexity is 167.49876704211124
At time: 549.3434426784515 and batch: 600, loss is 5.144695644378662 and perplexity is 171.51927387096953
At time: 550.1472690105438 and batch: 650, loss is 5.132825355529786 and perplexity is 169.4953267306474
At time: 550.9435393810272 and batch: 700, loss is 5.139934320449829 and perplexity is 170.70455615466145
At time: 551.7686314582825 and batch: 750, loss is 5.129726209640503 and perplexity is 168.97084912165812
At time: 552.6299893856049 and batch: 800, loss is 5.141902856826782 and perplexity is 171.0409252519426
At time: 553.4503235816956 and batch: 850, loss is 5.167434215545654 and perplexity is 175.46405647950715
At time: 554.2714767456055 and batch: 900, loss is 5.132867450714111 and perplexity is 169.50246181784385
At time: 555.0692863464355 and batch: 950, loss is 5.11596004486084 and perplexity is 166.660705957719
At time: 555.8700449466705 and batch: 1000, loss is 5.100645360946655 and perplexity is 164.12779479742167
At time: 556.6660933494568 and batch: 1050, loss is 5.079931421279907 and perplexity is 160.7630306276824
At time: 557.4616606235504 and batch: 1100, loss is 5.033407163619995 and perplexity is 153.45496918340666
At time: 558.2593624591827 and batch: 1150, loss is 5.049580526351929 and perplexity is 155.95703089882815
At time: 559.0749950408936 and batch: 1200, loss is 5.074437379837036 and perplexity is 159.88221371283237
At time: 559.8907308578491 and batch: 1250, loss is 5.123176498413086 and perplexity is 167.86775527059018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.013986406535127 and perplexity of 150.50351005403382
Finished 26 epochs...
Completing Train Step...
At time: 562.1054072380066 and batch: 50, loss is 5.138474292755127 and perplexity is 170.45550463036687
At time: 562.9517743587494 and batch: 100, loss is 5.154762516021728 and perplexity is 173.2546566677153
At time: 563.7733221054077 and batch: 150, loss is 5.077796869277954 and perplexity is 160.42023956164937
At time: 564.5706086158752 and batch: 200, loss is 5.124334049224854 and perplexity is 168.06218323538766
At time: 565.366941690445 and batch: 250, loss is 5.142657527923584 and perplexity is 171.17005361319298
At time: 566.1634376049042 and batch: 300, loss is 5.133111495971679 and perplexity is 169.54383313782844
At time: 566.9595685005188 and batch: 350, loss is 5.150174283981324 and perplexity is 172.4615449824396
At time: 567.7556009292603 and batch: 400, loss is 5.13071117401123 and perplexity is 169.13736137859277
At time: 568.567848443985 and batch: 450, loss is 5.101176452636719 and perplexity is 164.21498485625557
At time: 569.3643982410431 and batch: 500, loss is 5.095223350524902 and perplexity is 163.24030035980275
At time: 570.1611578464508 and batch: 550, loss is 5.0939094352722165 and perplexity is 163.02595728446136
At time: 570.9582479000092 and batch: 600, loss is 5.11989933013916 and perplexity is 167.31852484062574
At time: 571.7546198368073 and batch: 650, loss is 5.110343818664551 and perplexity is 165.72732522707219
At time: 572.551427602768 and batch: 700, loss is 5.118772096633911 and perplexity is 167.13002405553698
At time: 573.3987786769867 and batch: 750, loss is 5.110589227676392 and perplexity is 165.76800119711186
At time: 574.1947982311249 and batch: 800, loss is 5.127242736816406 and perplexity is 168.5517352541618
At time: 574.9928114414215 and batch: 850, loss is 5.155539350509644 and perplexity is 173.38929915089787
At time: 575.7864594459534 and batch: 900, loss is 5.122063913345337 and perplexity is 167.68109197137434
At time: 576.5818090438843 and batch: 950, loss is 5.108033971786499 and perplexity is 165.34496225225513
At time: 577.3864443302155 and batch: 1000, loss is 5.095770406723022 and perplexity is 163.3296264088542
At time: 578.1798787117004 and batch: 1050, loss is 5.079160089492798 and perplexity is 160.63907680287542
At time: 578.9713332653046 and batch: 1100, loss is 5.037049798965454 and perplexity is 154.01496899643672
At time: 579.7679543495178 and batch: 1150, loss is 5.058590650558472 and perplexity is 157.36857265202767
At time: 580.5615186691284 and batch: 1200, loss is 5.084326848983765 and perplexity is 161.47120813763
At time: 581.3536767959595 and batch: 1250, loss is 5.126483983993531 and perplexity is 168.42389465507807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.008762331774635 and perplexity of 149.71931858666494
Finished 27 epochs...
Completing Train Step...
At time: 583.5423383712769 and batch: 50, loss is 5.127170038223267 and perplexity is 168.53948222553203
At time: 584.3719811439514 and batch: 100, loss is 5.143193674087525 and perplexity is 171.26185038686432
At time: 585.1866185665131 and batch: 150, loss is 5.064379272460937 and perplexity is 158.28216147752946
At time: 585.997326374054 and batch: 200, loss is 5.108770217895508 and perplexity is 165.46674166166474
At time: 586.7949984073639 and batch: 250, loss is 5.128416786193847 and perplexity is 168.74973952465493
At time: 587.5921261310577 and batch: 300, loss is 5.119457635879517 and perplexity is 167.24463752767394
At time: 588.3937003612518 and batch: 350, loss is 5.136588125228882 and perplexity is 170.13430001095176
At time: 589.1901805400848 and batch: 400, loss is 5.118105783462524 and perplexity is 167.018700211565
At time: 589.9860908985138 and batch: 450, loss is 5.0888314628601075 and perplexity is 162.2002142953421
At time: 590.7760457992554 and batch: 500, loss is 5.083498220443726 and perplexity is 161.33746390596897
At time: 591.5661542415619 and batch: 550, loss is 5.082928409576416 and perplexity is 161.24555825263786
At time: 592.3568913936615 and batch: 600, loss is 5.110279951095581 and perplexity is 165.7167409636973
At time: 593.1481533050537 and batch: 650, loss is 5.101692504882813 and perplexity is 164.29975023783453
At time: 593.9683709144592 and batch: 700, loss is 5.1100516033172605 and perplexity is 165.67890423419868
At time: 594.759580373764 and batch: 750, loss is 5.102519769668579 and perplexity is 164.43572587168848
At time: 595.5487382411957 and batch: 800, loss is 5.121523008346558 and perplexity is 167.5904169560216
At time: 596.346839427948 and batch: 850, loss is 5.150309505462647 and perplexity is 172.48486706481114
At time: 597.1381499767303 and batch: 900, loss is 5.118399562835694 and perplexity is 167.06777406871132
At time: 597.9283273220062 and batch: 950, loss is 5.105110397338867 and perplexity is 164.8622698829175
At time: 598.7200937271118 and batch: 1000, loss is 5.09475980758667 and perplexity is 163.164649006516
At time: 599.5125434398651 and batch: 1050, loss is 5.080086784362793 and perplexity is 160.78800920805918
At time: 600.3084347248077 and batch: 1100, loss is 5.038501596450805 and perplexity is 154.23872992961014
At time: 601.1043827533722 and batch: 1150, loss is 5.062501306533814 and perplexity is 157.9851919080479
At time: 601.9254863262177 and batch: 1200, loss is 5.087227907180786 and perplexity is 161.94032564916853
At time: 602.7484254837036 and batch: 1250, loss is 5.125958642959595 and perplexity is 168.33543790913404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.005483362796533 and perplexity of 149.22919757053396
Finished 28 epochs...
Completing Train Step...
At time: 604.9535908699036 and batch: 50, loss is 5.119087104797363 and perplexity is 167.18267967051125
At time: 605.7775349617004 and batch: 100, loss is 5.135302267074585 and perplexity is 169.91567202636926
At time: 606.5754692554474 and batch: 150, loss is 5.055085039138794 and perplexity is 156.81786543338183
At time: 607.3967192173004 and batch: 200, loss is 5.099104919433594 and perplexity is 163.87516016326526
At time: 608.2136693000793 and batch: 250, loss is 5.119035053253174 and perplexity is 167.17397778034803
At time: 609.0230522155762 and batch: 300, loss is 5.110542192459106 and perplexity is 165.76020444651894
At time: 609.8264317512512 and batch: 350, loss is 5.127844209671021 and perplexity is 168.65314504207544
At time: 610.629106760025 and batch: 400, loss is 5.1095468616485595 and perplexity is 165.5953002885794
At time: 611.4303917884827 and batch: 450, loss is 5.080307941436768 and perplexity is 160.82357254610284
At time: 612.2295551300049 and batch: 500, loss is 5.07575626373291 and perplexity is 160.0932189048236
At time: 613.0226194858551 and batch: 550, loss is 5.075850896835327 and perplexity is 160.10836973967835
At time: 613.8440852165222 and batch: 600, loss is 5.104218893051147 and perplexity is 164.71535995757785
At time: 614.6399779319763 and batch: 650, loss is 5.096112833023072 and perplexity is 163.38556434527754
At time: 615.4385793209076 and batch: 700, loss is 5.104537811279297 and perplexity is 164.76789906571886
At time: 616.2333567142487 and batch: 750, loss is 5.097074365615844 and perplexity is 163.542740443445
At time: 617.0303997993469 and batch: 800, loss is 5.117228746414185 and perplexity is 166.87228283982898
At time: 617.8526382446289 and batch: 850, loss is 5.1462387847900395 and perplexity is 171.7841565168041
At time: 618.6496691703796 and batch: 900, loss is 5.115426559448242 and perplexity is 166.57181861439594
At time: 619.4487645626068 and batch: 950, loss is 5.102298946380615 and perplexity is 164.39941864293112
At time: 620.2460882663727 and batch: 1000, loss is 5.092902507781982 and perplexity is 162.86188458496844
At time: 621.0437512397766 and batch: 1050, loss is 5.079892635345459 and perplexity is 160.75679540423505
At time: 621.8821277618408 and batch: 1100, loss is 5.038124313354492 and perplexity is 154.18054923999645
At time: 622.6896011829376 and batch: 1150, loss is 5.063473091125489 and perplexity is 158.1387941052932
At time: 623.5127830505371 and batch: 1200, loss is 5.087874431610107 and perplexity is 162.04505787811706
At time: 624.3165752887726 and batch: 1250, loss is 5.124087982177734 and perplexity is 168.02083375780703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.00282276459854 and perplexity of 148.83268634881222
Finished 29 epochs...
Completing Train Step...
At time: 626.5915904045105 and batch: 50, loss is 5.112400751113892 and perplexity is 166.0685659743424
At time: 627.4243009090424 and batch: 100, loss is 5.128199224472046 and perplexity is 168.71303003420513
At time: 628.2404069900513 and batch: 150, loss is 5.047464628219604 and perplexity is 155.6273905740841
At time: 629.0415930747986 and batch: 200, loss is 5.091160373687744 and perplexity is 162.5784043451741
At time: 629.8440148830414 and batch: 250, loss is 5.111739377975464 and perplexity is 165.95876899805737
At time: 630.6445770263672 and batch: 300, loss is 5.10365514755249 and perplexity is 164.62252858393515
At time: 631.4454963207245 and batch: 350, loss is 5.1206621742248535 and perplexity is 167.44621148403573
At time: 632.2424457073212 and batch: 400, loss is 5.102716999053955 and perplexity is 164.4681606272744
At time: 633.0384321212769 and batch: 450, loss is 5.073841857910156 and perplexity is 159.78702869404424
At time: 633.8328967094421 and batch: 500, loss is 5.069346389770508 and perplexity is 159.07032336740684
At time: 634.6552174091339 and batch: 550, loss is 5.0698843097686765 and perplexity is 159.1559134937178
At time: 635.4509315490723 and batch: 600, loss is 5.098932390213013 and perplexity is 163.84688934845062
At time: 636.2478039264679 and batch: 650, loss is 5.091889581680298 and perplexity is 162.69700105261745
At time: 637.0459551811218 and batch: 700, loss is 5.100446367263794 and perplexity is 164.0951376524657
At time: 637.8467922210693 and batch: 750, loss is 5.093323907852173 and perplexity is 162.93052905694074
At time: 638.6442732810974 and batch: 800, loss is 5.113573131561279 and perplexity is 166.26337568725202
At time: 639.4407091140747 and batch: 850, loss is 5.143093738555908 and perplexity is 171.24473609797735
At time: 640.2368385791779 and batch: 900, loss is 5.112979068756103 and perplexity is 166.1646341321398
At time: 641.031231880188 and batch: 950, loss is 5.099960050582886 and perplexity is 164.01535485119527
At time: 641.8273892402649 and batch: 1000, loss is 5.09136399269104 and perplexity is 162.61151176836015
At time: 642.6255049705505 and batch: 1050, loss is 5.07876277923584 and perplexity is 160.57526592719043
At time: 643.4220604896545 and batch: 1100, loss is 5.036929006576538 and perplexity is 153.99636628395845
At time: 644.2165324687958 and batch: 1150, loss is 5.063069705963135 and perplexity is 158.07501612656853
At time: 645.0128040313721 and batch: 1200, loss is 5.08685718536377 and perplexity is 161.8803019641279
At time: 645.8085737228394 and batch: 1250, loss is 5.12172911643982 and perplexity is 167.62496225722018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.000007128193431 and perplexity of 148.41421702405293
Finished 30 epochs...
Completing Train Step...
At time: 648.0211887359619 and batch: 50, loss is 5.10658221244812 and perplexity is 165.10509531591262
At time: 648.8484246730804 and batch: 100, loss is 5.122697277069092 and perplexity is 167.7873287318923
At time: 649.6445410251617 and batch: 150, loss is 5.041157188415528 and perplexity is 154.6488694019728
At time: 650.4410429000854 and batch: 200, loss is 5.085338039398193 and perplexity is 161.63456885598137
At time: 651.2376282215118 and batch: 250, loss is 5.106175451278687 and perplexity is 165.03795063111357
At time: 652.0335476398468 and batch: 300, loss is 5.09783444404602 and perplexity is 163.66709300573135
At time: 652.8306956291199 and batch: 350, loss is 5.115383424758911 and perplexity is 166.56463374570842
At time: 653.6275632381439 and batch: 400, loss is 5.097419672012329 and perplexity is 163.5992225490737
At time: 654.4246742725372 and batch: 450, loss is 5.068650970458984 and perplexity is 158.9597412475742
At time: 655.2949798107147 and batch: 500, loss is 5.064282321929932 and perplexity is 158.26681668178065
At time: 656.0915801525116 and batch: 550, loss is 5.065475034713745 and perplexity is 158.45569615438006
At time: 656.8866243362427 and batch: 600, loss is 5.095215492248535 and perplexity is 163.23901757744846
At time: 657.6823065280914 and batch: 650, loss is 5.08805100440979 and perplexity is 162.07367315392673
At time: 658.4803802967072 and batch: 700, loss is 5.097462644577027 and perplexity is 163.6062529783059
At time: 659.2787640094757 and batch: 750, loss is 5.0899225616455075 and perplexity is 162.37728733667447
At time: 660.1005730628967 and batch: 800, loss is 5.110409336090088 and perplexity is 165.73818361046503
At time: 660.9015579223633 and batch: 850, loss is 5.140217390060425 and perplexity is 170.75288426668632
At time: 661.7262473106384 and batch: 900, loss is 5.110387210845947 and perplexity is 165.7345166532555
At time: 662.552220582962 and batch: 950, loss is 5.09793454170227 and perplexity is 163.68347651810836
At time: 663.3546159267426 and batch: 1000, loss is 5.089945001602173 and perplexity is 162.3809311168487
At time: 664.1513001918793 and batch: 1050, loss is 5.077520475387574 and perplexity is 160.37590651451512
At time: 664.947643995285 and batch: 1100, loss is 5.03553505897522 and perplexity is 153.78185296346433
At time: 665.7451541423798 and batch: 1150, loss is 5.062292385101318 and perplexity is 157.95218886308317
At time: 666.5427587032318 and batch: 1200, loss is 5.085607137680054 and perplexity is 161.6780702935683
At time: 667.3409349918365 and batch: 1250, loss is 5.118933992385864 and perplexity is 167.15708388683214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.997737244097856 and perplexity of 148.0777160069078
Finished 31 epochs...
Completing Train Step...
At time: 669.5597560405731 and batch: 50, loss is 5.101363983154297 and perplexity is 164.24578306507146
At time: 670.3850102424622 and batch: 100, loss is 5.1182757759094235 and perplexity is 167.04709454243445
At time: 671.1800017356873 and batch: 150, loss is 5.035972976684571 and perplexity is 153.84921150792712
At time: 671.9773118495941 and batch: 200, loss is 5.080083742141723 and perplexity is 160.78752005613387
At time: 672.7727334499359 and batch: 250, loss is 5.101377086639404 and perplexity is 164.24793527134452
At time: 673.5693032741547 and batch: 300, loss is 5.0929610061645505 and perplexity is 162.87141202046573
At time: 674.3661713600159 and batch: 350, loss is 5.110445852279663 and perplexity is 165.74423584789926
At time: 675.2204248905182 and batch: 400, loss is 5.092645559310913 and perplexity is 162.8200428485442
At time: 676.0491864681244 and batch: 450, loss is 5.064042692184448 and perplexity is 158.2288957884491
At time: 676.8698964118958 and batch: 500, loss is 5.060102624893188 and perplexity is 157.60668986316185
At time: 677.7012083530426 and batch: 550, loss is 5.061528978347778 and perplexity is 157.83165311012647
At time: 678.4966113567352 and batch: 600, loss is 5.091618509292602 and perplexity is 162.6529043650384
At time: 679.2959976196289 and batch: 650, loss is 5.084714241027832 and perplexity is 161.53377291677484
At time: 680.1099574565887 and batch: 700, loss is 5.094595823287964 and perplexity is 163.13789475967317
At time: 680.9060974121094 and batch: 750, loss is 5.086663551330567 and perplexity is 161.84895946294785
At time: 681.7021610736847 and batch: 800, loss is 5.107699670791626 and perplexity is 165.28969650508878
At time: 682.4984681606293 and batch: 850, loss is 5.13774356842041 and perplexity is 170.33099414210906
At time: 683.2950901985168 and batch: 900, loss is 5.107940092086792 and perplexity is 165.32944044545343
At time: 684.0927958488464 and batch: 950, loss is 5.096160116195679 and perplexity is 163.39328991576136
At time: 684.913327217102 and batch: 1000, loss is 5.0882010173797605 and perplexity is 162.09798813072592
At time: 685.7142522335052 and batch: 1050, loss is 5.075702514648437 and perplexity is 160.08461427212478
At time: 686.512291431427 and batch: 1100, loss is 5.034130849838257 and perplexity is 153.56606262327563
At time: 687.306964635849 and batch: 1150, loss is 5.061444244384766 and perplexity is 157.818279975257
At time: 688.1049056053162 and batch: 1200, loss is 5.0848567676544185 and perplexity is 161.5567974212719
At time: 688.9043350219727 and batch: 1250, loss is 5.11636342048645 and perplexity is 166.72794638491226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.995710609603102 and perplexity of 147.7779204903227
Finished 32 epochs...
Completing Train Step...
At time: 691.1299204826355 and batch: 50, loss is 5.097185630798339 and perplexity is 163.56093806866846
At time: 691.9274950027466 and batch: 100, loss is 5.113922348022461 and perplexity is 166.32144773420094
At time: 692.724237203598 and batch: 150, loss is 5.031457376480103 and perplexity is 153.15605616102113
At time: 693.5213758945465 and batch: 200, loss is 5.076117506027222 and perplexity is 160.1510617935444
At time: 694.3169448375702 and batch: 250, loss is 5.096645154953003 and perplexity is 163.47256121732983
At time: 695.1113517284393 and batch: 300, loss is 5.088662738800049 and perplexity is 162.1728495252563
At time: 695.9361972808838 and batch: 350, loss is 5.106468963623047 and perplexity is 165.0863984165754
At time: 696.734757900238 and batch: 400, loss is 5.088944692611694 and perplexity is 162.21858122513586
At time: 697.5476570129395 and batch: 450, loss is 5.060041885375977 and perplexity is 157.5971171996327
At time: 698.3614785671234 and batch: 500, loss is 5.056493339538574 and perplexity is 157.03886767821288
At time: 699.156140089035 and batch: 550, loss is 5.057742776870728 and perplexity is 157.23520052934495
At time: 699.9536566734314 and batch: 600, loss is 5.088701963424683 and perplexity is 162.17921081916356
At time: 700.7669551372528 and batch: 650, loss is 5.081756839752197 and perplexity is 161.05675843997653
At time: 701.5849974155426 and batch: 700, loss is 5.0922932624816895 and perplexity is 162.7626919665747
At time: 702.3887557983398 and batch: 750, loss is 5.0837360286712645 and perplexity is 161.37583584469652
At time: 703.1943986415863 and batch: 800, loss is 5.1050216007232665 and perplexity is 164.8476313212486
At time: 703.9882702827454 and batch: 850, loss is 5.134876537322998 and perplexity is 169.84334926559458
At time: 704.7849128246307 and batch: 900, loss is 5.1055206680297855 and perplexity is 164.92992191716317
At time: 705.5952517986298 and batch: 950, loss is 5.093639802932739 and perplexity is 162.9820061397945
At time: 706.394424200058 and batch: 1000, loss is 5.086600646972657 and perplexity is 161.8387787782825
At time: 707.1863243579865 and batch: 1050, loss is 5.073685398101807 and perplexity is 159.76203040182307
At time: 707.9994883537292 and batch: 1100, loss is 5.032237577438354 and perplexity is 153.27559528900733
At time: 708.8083503246307 and batch: 1150, loss is 5.059864768981933 and perplexity is 157.56920663830456
At time: 709.6444327831268 and batch: 1200, loss is 5.08274624824524 and perplexity is 161.2161882222234
At time: 710.4595372676849 and batch: 1250, loss is 5.113944511413575 and perplexity is 166.3251340223478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.99435447080292 and perplexity of 147.5776489472341
Finished 33 epochs...
Completing Train Step...
At time: 712.6926472187042 and batch: 50, loss is 5.0930784225463865 and perplexity is 162.8905369151357
At time: 713.5389897823334 and batch: 100, loss is 5.110038633346558 and perplexity is 165.6767553975999
At time: 714.336731672287 and batch: 150, loss is 5.027535171508789 and perplexity is 152.5565232302659
At time: 715.1342241764069 and batch: 200, loss is 5.072208757400513 and perplexity is 159.52629337748758
At time: 715.9700255393982 and batch: 250, loss is 5.092918939590454 and perplexity is 162.86456072225016
At time: 716.8282859325409 and batch: 300, loss is 5.085110883712769 and perplexity is 161.59785681453698
At time: 717.6556425094604 and batch: 350, loss is 5.102692050933838 and perplexity is 164.46405750703036
At time: 718.454549074173 and batch: 400, loss is 5.085470600128174 and perplexity is 161.65599667263115
At time: 719.2557182312012 and batch: 450, loss is 5.056510982513427 and perplexity is 157.0416383354476
At time: 720.0754096508026 and batch: 500, loss is 5.053146276473999 and perplexity is 156.51412734388163
At time: 720.8812210559845 and batch: 550, loss is 5.054558572769165 and perplexity is 156.73532782961686
At time: 721.7102553844452 and batch: 600, loss is 5.085929460525513 and perplexity is 161.7301912286665
At time: 722.5242986679077 and batch: 650, loss is 5.079464273452759 and perplexity is 160.68794806593212
At time: 723.3623380661011 and batch: 700, loss is 5.089867134094238 and perplexity is 162.3682874106798
At time: 724.174485206604 and batch: 750, loss is 5.081439313888549 and perplexity is 161.00562687188383
At time: 724.9865531921387 and batch: 800, loss is 5.102433538436889 and perplexity is 164.4215469878529
At time: 725.825291633606 and batch: 850, loss is 5.132650365829468 and perplexity is 169.4656693891579
At time: 726.6262466907501 and batch: 900, loss is 5.103559656143188 and perplexity is 164.60680929721823
At time: 727.431985616684 and batch: 950, loss is 5.091498079299927 and perplexity is 162.6333172564185
At time: 728.2288160324097 and batch: 1000, loss is 5.084610576629639 and perplexity is 161.5170284833333
At time: 729.0259401798248 and batch: 1050, loss is 5.072005701065064 and perplexity is 159.4939038415081
At time: 729.8234267234802 and batch: 1100, loss is 5.030888671875 and perplexity is 153.0689803691318
At time: 730.6225838661194 and batch: 1150, loss is 5.058801517486573 and perplexity is 157.40175997845424
At time: 731.4189896583557 and batch: 1200, loss is 5.081075439453125 and perplexity is 160.9470516979561
At time: 732.2145745754242 and batch: 1250, loss is 5.111923189163208 and perplexity is 165.98927688026248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.993233116873859 and perplexity of 147.41225492069734
Finished 34 epochs...
Completing Train Step...
At time: 734.4426414966583 and batch: 50, loss is 5.08955810546875 and perplexity is 162.31811871417946
At time: 735.2621874809265 and batch: 100, loss is 5.106637887954712 and perplexity is 165.1142878816332
At time: 736.0994501113892 and batch: 150, loss is 5.024682998657227 and perplexity is 152.12202558212704
At time: 736.9072232246399 and batch: 200, loss is 5.069133977890015 and perplexity is 159.03653852916733
At time: 737.7580320835114 and batch: 250, loss is 5.089993362426758 and perplexity is 162.3887841824632
At time: 738.5537393093109 and batch: 300, loss is 5.081916208267212 and perplexity is 161.08242786179673
At time: 739.3499896526337 and batch: 350, loss is 5.099601640701294 and perplexity is 163.9565806605383
At time: 740.1456468105316 and batch: 400, loss is 5.082839431762696 and perplexity is 161.23121161366814
At time: 740.9422347545624 and batch: 450, loss is 5.0535155868530275 and perplexity is 156.57194031038148
At time: 741.7362744808197 and batch: 500, loss is 5.050643825531006 and perplexity is 156.12294807592107
At time: 742.5360136032104 and batch: 550, loss is 5.052018060684204 and perplexity is 156.33764520777243
At time: 743.3581116199493 and batch: 600, loss is 5.083432998657226 and perplexity is 161.32694153149146
At time: 744.1757698059082 and batch: 650, loss is 5.07740364074707 and perplexity is 160.3571701476817
At time: 744.9820020198822 and batch: 700, loss is 5.08784966468811 and perplexity is 162.04104457050747
At time: 745.8040492534637 and batch: 750, loss is 5.079167470932007 and perplexity is 160.64026255483174
At time: 746.6090037822723 and batch: 800, loss is 5.100154333114624 and perplexity is 164.0472232652175
At time: 747.4168343544006 and batch: 850, loss is 5.130770998001099 and perplexity is 169.147480153056
At time: 748.2234961986542 and batch: 900, loss is 5.10163064956665 and perplexity is 164.28958773914368
At time: 749.030366897583 and batch: 950, loss is 5.089136438369751 and perplexity is 162.24968893225395
At time: 749.8262526988983 and batch: 1000, loss is 5.083024291992188 and perplexity is 161.26101960751973
At time: 750.6205615997314 and batch: 1050, loss is 5.070277414321899 and perplexity is 159.2184907068724
At time: 751.4150700569153 and batch: 1100, loss is 5.029380006790161 and perplexity is 152.83822465319088
At time: 752.2103679180145 and batch: 1150, loss is 5.057647113800049 and perplexity is 157.22015964668344
At time: 753.0056102275848 and batch: 1200, loss is 5.079574804306031 and perplexity is 160.70571002354643
At time: 753.8016395568848 and batch: 1250, loss is 5.109890823364258 and perplexity is 165.65226852906343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.992265464615648 and perplexity of 147.2696801118782
Finished 35 epochs...
Completing Train Step...
At time: 756.0075769424438 and batch: 50, loss is 5.086647262573242 and perplexity is 161.84632316599502
At time: 756.8417410850525 and batch: 100, loss is 5.103801574707031 and perplexity is 164.64663555728373
At time: 757.6495008468628 and batch: 150, loss is 5.021544790267944 and perplexity is 151.6453832578992
At time: 758.4758231639862 and batch: 200, loss is 5.066575708389283 and perplexity is 158.6302001862559
At time: 759.2718782424927 and batch: 250, loss is 5.087112894058228 and perplexity is 161.92170145768125
At time: 760.0665729045868 and batch: 300, loss is 5.079268827438354 and perplexity is 160.6565453157909
At time: 760.8796455860138 and batch: 350, loss is 5.096799125671387 and perplexity is 163.49773314283652
At time: 761.6847615242004 and batch: 400, loss is 5.080006160736084 and perplexity is 160.7750464181861
At time: 762.4896173477173 and batch: 450, loss is 5.050916023254395 and perplexity is 156.16545017118042
At time: 763.2850270271301 and batch: 500, loss is 5.048246774673462 and perplexity is 155.7491616009254
At time: 764.082231760025 and batch: 550, loss is 5.049815969467163 and perplexity is 155.99375423098388
At time: 764.8779671192169 and batch: 600, loss is 5.081266298294067 and perplexity is 160.9777727972997
At time: 765.6733667850494 and batch: 650, loss is 5.07547441482544 and perplexity is 160.04810316419156
At time: 766.4704072475433 and batch: 700, loss is 5.086099681854248 and perplexity is 161.75772349993738
At time: 767.2658560276031 and batch: 750, loss is 5.077069330215454 and perplexity is 160.3035700169245
At time: 768.063286781311 and batch: 800, loss is 5.097806673049927 and perplexity is 163.6625478706427
At time: 768.8603086471558 and batch: 850, loss is 5.128707027435302 and perplexity is 168.79872476697466
At time: 769.6575794219971 and batch: 900, loss is 5.099724388122558 and perplexity is 163.97670714322538
At time: 770.4514150619507 and batch: 950, loss is 5.086959638595581 and perplexity is 161.89688797386137
At time: 771.2482919692993 and batch: 1000, loss is 5.081461935043335 and perplexity is 161.00926904628562
At time: 772.0434215068817 and batch: 1050, loss is 5.068772592544556 and perplexity is 158.97907543853512
At time: 772.8414468765259 and batch: 1100, loss is 5.027819805145263 and perplexity is 152.59995212861028
At time: 773.6386632919312 and batch: 1150, loss is 5.056220045089722 and perplexity is 156.9959556914934
At time: 774.4326128959656 and batch: 1200, loss is 5.077933959960937 and perplexity is 160.4422331893815
At time: 775.2473175525665 and batch: 1250, loss is 5.107787075042725 and perplexity is 165.30414415861014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.991465770415146 and perplexity of 147.15195648050934
Finished 36 epochs...
Completing Train Step...
At time: 777.5515718460083 and batch: 50, loss is 5.083983860015869 and perplexity is 161.41583479137225
At time: 778.3763515949249 and batch: 100, loss is 5.101469669342041 and perplexity is 164.26314249304644
At time: 779.172000169754 and batch: 150, loss is 5.019256534576416 and perplexity is 151.29877656017916
At time: 779.9717302322388 and batch: 200, loss is 5.064263772964478 and perplexity is 158.26388102329224
At time: 780.7675533294678 and batch: 250, loss is 5.08451018333435 and perplexity is 161.50081407052141
At time: 781.5630321502686 and batch: 300, loss is 5.0762872219085695 and perplexity is 160.17824427872992
At time: 782.3597683906555 and batch: 350, loss is 5.094375028610229 and perplexity is 163.10187875699054
At time: 783.1568567752838 and batch: 400, loss is 5.077568597793579 and perplexity is 160.38362437470147
At time: 783.9539532661438 and batch: 450, loss is 5.048393144607544 and perplexity is 155.77196026392087
At time: 784.7485356330872 and batch: 500, loss is 5.046174001693726 and perplexity is 155.426663295494
At time: 785.548003911972 and batch: 550, loss is 5.0476904296875 and perplexity is 155.66253543505184
At time: 786.3512661457062 and batch: 600, loss is 5.078725290298462 and perplexity is 160.56924624393818
At time: 787.1551911830902 and batch: 650, loss is 5.07342758178711 and perplexity is 159.7208464530914
At time: 787.9576110839844 and batch: 700, loss is 5.084116230010986 and perplexity is 161.43720281884703
At time: 788.7595746517181 and batch: 750, loss is 5.0746891307830815 and perplexity is 159.92246927836612
At time: 789.5627546310425 and batch: 800, loss is 5.095336208343506 and perplexity is 163.25872434363527
At time: 790.3631711006165 and batch: 850, loss is 5.1268223285675045 and perplexity is 168.4808896073849
At time: 791.1651623249054 and batch: 900, loss is 5.097867193222046 and perplexity is 163.672453055938
At time: 791.964736700058 and batch: 950, loss is 5.084892492294312 and perplexity is 161.56256908277678
At time: 792.7614214420319 and batch: 1000, loss is 5.079711456298828 and perplexity is 160.72767227963385
At time: 793.5567011833191 and batch: 1050, loss is 5.066981458663941 and perplexity is 158.6945774932223
At time: 794.3553142547607 and batch: 1100, loss is 5.026076345443726 and perplexity is 152.33413205224636
At time: 795.1591963768005 and batch: 1150, loss is 5.0545275783538814 and perplexity is 156.73046998505998
At time: 795.9655973911285 and batch: 1200, loss is 5.076289701461792 and perplexity is 160.17864144970406
At time: 796.7632577419281 and batch: 1250, loss is 5.105483064651489 and perplexity is 164.92372011152207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.990448666315009 and perplexity of 147.00236371083574
Finished 37 epochs...
Completing Train Step...
At time: 798.9942004680634 and batch: 50, loss is 5.080925827026367 and perplexity is 160.92297382019186
At time: 799.7910943031311 and batch: 100, loss is 5.0987206554412845 and perplexity is 163.81220093723707
At time: 800.5861957073212 and batch: 150, loss is 5.016468639373779 and perplexity is 150.87755885507312
At time: 801.3844213485718 and batch: 200, loss is 5.0618446159362795 and perplexity is 157.88147857548458
At time: 802.1812491416931 and batch: 250, loss is 5.082023286819458 and perplexity is 161.0996772584682
At time: 802.9784471988678 and batch: 300, loss is 5.073531141281128 and perplexity is 159.73738791963245
At time: 803.7762920856476 and batch: 350, loss is 5.091388235092163 and perplexity is 162.61545390963894
At time: 804.5997204780579 and batch: 400, loss is 5.074795713424683 and perplexity is 159.9395151459739
At time: 805.4286377429962 and batch: 450, loss is 5.045400810241699 and perplexity is 155.30653517501952
At time: 806.2375111579895 and batch: 500, loss is 5.043923845291138 and perplexity is 155.07732217742227
At time: 807.0504996776581 and batch: 550, loss is 5.045520210266114 and perplexity is 155.32507988620856
At time: 807.8467082977295 and batch: 600, loss is 5.076447820663452 and perplexity is 160.20397077108632
At time: 808.6480479240417 and batch: 650, loss is 5.071312694549561 and perplexity is 159.38341181723135
At time: 809.4485263824463 and batch: 700, loss is 5.081323766708374 and perplexity is 160.9870242004703
At time: 810.2452020645142 and batch: 750, loss is 5.072260046005249 and perplexity is 159.53447546831592
At time: 811.0432779788971 and batch: 800, loss is 5.092801561355591 and perplexity is 162.84544508949236
At time: 811.8421945571899 and batch: 850, loss is 5.12410346031189 and perplexity is 168.0234344269396
At time: 812.6393158435822 and batch: 900, loss is 5.095924673080444 and perplexity is 163.35482461894742
At time: 813.4361138343811 and batch: 950, loss is 5.082955913543701 and perplexity is 161.24999320618602
At time: 814.232488155365 and batch: 1000, loss is 5.078399000167846 and perplexity is 160.51686263020122
At time: 815.0304436683655 and batch: 1050, loss is 5.065012273788452 and perplexity is 158.38238601365225
At time: 815.8272130489349 and batch: 1100, loss is 5.023967742919922 and perplexity is 152.01325833339538
At time: 816.6224036216736 and batch: 1150, loss is 5.053021669387817 and perplexity is 156.4946257895682
At time: 817.4323642253876 and batch: 1200, loss is 5.074370584487915 and perplexity is 159.87153468120795
At time: 818.2295508384705 and batch: 1250, loss is 5.103538818359375 and perplexity is 164.6033792918489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.989755895015967 and perplexity of 146.9005599597955
Finished 38 epochs...
Completing Train Step...
At time: 820.4716413021088 and batch: 50, loss is 5.078020992279053 and perplexity is 160.45619745652215
At time: 821.2913835048676 and batch: 100, loss is 5.095998973846435 and perplexity is 163.3669624584648
At time: 822.0943059921265 and batch: 150, loss is 5.0139439582824705 and perplexity is 150.4971215786041
At time: 822.8971092700958 and batch: 200, loss is 5.059536323547364 and perplexity is 157.5174622498247
At time: 823.7006866931915 and batch: 250, loss is 5.079144153594971 and perplexity is 160.63651689535772
At time: 824.5390644073486 and batch: 300, loss is 5.070759611129761 and perplexity is 159.2952838680644
At time: 825.3736763000488 and batch: 350, loss is 5.0884698867797855 and perplexity is 162.14157717915796
At time: 826.1941938400269 and batch: 400, loss is 5.071998491287231 and perplexity is 159.49275393004106
At time: 827.0031983852386 and batch: 450, loss is 5.042959241867066 and perplexity is 154.9278061848664
At time: 827.8127596378326 and batch: 500, loss is 5.041499633789062 and perplexity is 154.70183726060327
At time: 828.6233701705933 and batch: 550, loss is 5.043233633041382 and perplexity is 154.97032284037044
At time: 829.4275372028351 and batch: 600, loss is 5.074099397659301 and perplexity is 159.82818550486064
At time: 830.2344143390656 and batch: 650, loss is 5.069026613235474 and perplexity is 159.01946454273647
At time: 831.0346124172211 and batch: 700, loss is 5.078908405303955 and perplexity is 160.5986515745426
At time: 831.8389284610748 and batch: 750, loss is 5.069968070983887 and perplexity is 159.16924514476975
At time: 832.6356160640717 and batch: 800, loss is 5.090516290664673 and perplexity is 162.47372407003022
At time: 833.4318134784698 and batch: 850, loss is 5.122410402297974 and perplexity is 167.73920168391388
At time: 834.24196600914 and batch: 900, loss is 5.094137153625488 and perplexity is 163.06308551422393
At time: 835.0502126216888 and batch: 950, loss is 5.081365280151367 and perplexity is 160.99370746484368
At time: 835.8692982196808 and batch: 1000, loss is 5.075899953842163 and perplexity is 160.11622436972797
At time: 836.6839287281036 and batch: 1050, loss is 5.06272325515747 and perplexity is 158.02026039550734
At time: 837.4785454273224 and batch: 1100, loss is 5.021789703369141 and perplexity is 151.68252774739545
At time: 838.274477481842 and batch: 1150, loss is 5.050983848571778 and perplexity is 156.17604250161256
At time: 839.0728802680969 and batch: 1200, loss is 5.072410564422608 and perplexity is 159.5584901523583
At time: 839.9264497756958 and batch: 1250, loss is 5.10148853302002 and perplexity is 164.26624112929596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.988918777799954 and perplexity of 146.77763842905432
Finished 39 epochs...
Completing Train Step...
At time: 842.2284345626831 and batch: 50, loss is 5.0752395820617675 and perplexity is 160.01052303850014
At time: 843.0249490737915 and batch: 100, loss is 5.093486738204956 and perplexity is 162.95706125253105
At time: 843.8205347061157 and batch: 150, loss is 5.011689405441285 and perplexity is 150.15820006734853
At time: 844.64515376091 and batch: 200, loss is 5.057296600341797 and perplexity is 157.16506152170004
At time: 845.4523293972015 and batch: 250, loss is 5.0766934967041015 and perplexity is 160.24333388341054
At time: 846.2583000659943 and batch: 300, loss is 5.068168210983276 and perplexity is 158.8830204465677
At time: 847.0757908821106 and batch: 350, loss is 5.08585485458374 and perplexity is 161.71812564552448
At time: 847.9076335430145 and batch: 400, loss is 5.069115295410156 and perplexity is 159.0335673599939
At time: 848.7221660614014 and batch: 450, loss is 5.040480365753174 and perplexity is 154.54423495593068
At time: 849.5161318778992 and batch: 500, loss is 5.039180631637573 and perplexity is 154.343499021319
At time: 850.3110983371735 and batch: 550, loss is 5.041174583435058 and perplexity is 154.6515595454739
At time: 851.1055986881256 and batch: 600, loss is 5.071400127410889 and perplexity is 159.39734777419616
At time: 851.9159371852875 and batch: 650, loss is 5.066735019683838 and perplexity is 158.65547378193435
At time: 852.7204148769379 and batch: 700, loss is 5.07659954071045 and perplexity is 160.22827876901962
At time: 853.5203022956848 and batch: 750, loss is 5.067649793624878 and perplexity is 158.8006740775513
At time: 854.3157677650452 and batch: 800, loss is 5.088130788803101 and perplexity is 162.08660461946866
At time: 855.1419610977173 and batch: 850, loss is 5.120452156066895 and perplexity is 167.41104843171166
At time: 855.9657144546509 and batch: 900, loss is 5.091615333557129 and perplexity is 162.65238782326034
At time: 856.7708115577698 and batch: 950, loss is 5.079734554290772 and perplexity is 160.73138480898916
At time: 857.5815961360931 and batch: 1000, loss is 5.073597898483277 and perplexity is 159.74805189667313
At time: 858.3861808776855 and batch: 1050, loss is 5.060303955078125 and perplexity is 157.6384240415961
At time: 859.2001304626465 and batch: 1100, loss is 5.019650230407715 and perplexity is 151.35835398471374
At time: 859.9949622154236 and batch: 1150, loss is 5.048772478103638 and perplexity is 155.83106099493605
At time: 860.8442687988281 and batch: 1200, loss is 5.069941654205322 and perplexity is 159.16504046160378
At time: 861.6402773857117 and batch: 1250, loss is 5.099315233230591 and perplexity is 163.9096289949411
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.987355475878194 and perplexity of 146.54835992728266
Finished 40 epochs...
Completing Train Step...
At time: 863.830126285553 and batch: 50, loss is 5.072296075820923 and perplexity is 159.54022356961144
At time: 864.656022310257 and batch: 100, loss is 5.091016597747803 and perplexity is 162.55503116256617
At time: 865.4547960758209 and batch: 150, loss is 5.0093096828460695 and perplexity is 149.80129004764476
At time: 866.2521965503693 and batch: 200, loss is 5.055074472427368 and perplexity is 156.81620839300612
At time: 867.0466840267181 and batch: 250, loss is 5.074427499771118 and perplexity is 159.8806340738252
At time: 867.8423693180084 and batch: 300, loss is 5.065861835479736 and perplexity is 158.51699879421795
At time: 868.662034034729 and batch: 350, loss is 5.083139772415161 and perplexity is 161.2796431735811
At time: 869.4683287143707 and batch: 400, loss is 5.066606903076172 and perplexity is 158.63514868286495
At time: 870.2850725650787 and batch: 450, loss is 5.037828464508056 and perplexity is 154.13494184914052
At time: 871.0955593585968 and batch: 500, loss is 5.036679801940918 and perplexity is 153.95799445702892
At time: 871.8928253650665 and batch: 550, loss is 5.0387262439727785 and perplexity is 154.27338317031771
At time: 872.6891498565674 and batch: 600, loss is 5.0689896392822265 and perplexity is 159.0135850731833
At time: 873.4842796325684 and batch: 650, loss is 5.064245128631592 and perplexity is 158.26093032631766
At time: 874.2825179100037 and batch: 700, loss is 5.074303245544433 and perplexity is 159.86076946343297
At time: 875.0953547954559 and batch: 750, loss is 5.0654566192626955 and perplexity is 158.45277814813232
At time: 875.909184217453 and batch: 800, loss is 5.085484619140625 and perplexity is 161.65826294594675
At time: 876.7054624557495 and batch: 850, loss is 5.118246431350708 and perplexity is 167.04219269108208
At time: 877.5274651050568 and batch: 900, loss is 5.088976278305053 and perplexity is 162.22370509241966
At time: 878.3263700008392 and batch: 950, loss is 5.077043981552124 and perplexity is 160.299506587199
At time: 879.1222493648529 and batch: 1000, loss is 5.071067848205566 and perplexity is 159.3443921486611
At time: 879.9221217632294 and batch: 1050, loss is 5.058187351226807 and perplexity is 157.30511880815854
At time: 880.8015413284302 and batch: 1100, loss is 5.017430658340454 and perplexity is 151.02277576784985
At time: 881.6086103916168 and batch: 1150, loss is 5.046732378005982 and perplexity is 155.5134740968632
At time: 882.4143536090851 and batch: 1200, loss is 5.067806034088135 and perplexity is 158.82548710678378
At time: 883.2273366451263 and batch: 1250, loss is 5.097108030319214 and perplexity is 163.54824615396387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.986016266537408 and perplexity of 146.35223235204424
Finished 41 epochs...
Completing Train Step...
At time: 885.4235348701477 and batch: 50, loss is 5.069756021499634 and perplexity is 159.13549696669625
At time: 886.2629902362823 and batch: 100, loss is 5.088379878997802 and perplexity is 162.12698383219606
At time: 887.0588235855103 and batch: 150, loss is 5.0070039176940915 and perplexity is 149.45628136069004
At time: 887.8530404567719 and batch: 200, loss is 5.05267406463623 and perplexity is 156.44023696750003
At time: 888.6494102478027 and batch: 250, loss is 5.072166805267334 and perplexity is 159.51960104956197
At time: 889.4699997901917 and batch: 300, loss is 5.063333921432495 and perplexity is 158.11678750922826
At time: 890.2712104320526 and batch: 350, loss is 5.080598421096802 and perplexity is 160.87029530845902
At time: 891.0714581012726 and batch: 400, loss is 5.0641640853881835 and perplexity is 158.2481048669346
At time: 891.8639900684357 and batch: 450, loss is 5.035625057220459 and perplexity is 153.7956936831916
At time: 892.685788154602 and batch: 500, loss is 5.03437502861023 and perplexity is 153.60356477429193
At time: 893.4805681705475 and batch: 550, loss is 5.036489782333374 and perplexity is 153.92874219868327
At time: 894.2734365463257 and batch: 600, loss is 5.066381931304932 and perplexity is 158.59946426662842
At time: 895.0671374797821 and batch: 650, loss is 5.062226371765137 and perplexity is 157.9417622562905
At time: 895.8659772872925 and batch: 700, loss is 5.072645120620727 and perplexity is 159.59591997471293
At time: 896.6608283519745 and batch: 750, loss is 5.063513593673706 and perplexity is 158.14519925913814
At time: 897.4563724994659 and batch: 800, loss is 5.0832236385345455 and perplexity is 161.29316963858787
At time: 898.2484476566315 and batch: 850, loss is 5.116031341552734 and perplexity is 166.67258873831898
At time: 899.042319059372 and batch: 900, loss is 5.086386499404907 and perplexity is 161.80412510807156
At time: 899.8420491218567 and batch: 950, loss is 5.074445943832398 and perplexity is 159.88358294923208
At time: 900.6661760807037 and batch: 1000, loss is 5.068565378189087 and perplexity is 158.94613610475434
At time: 901.5729281902313 and batch: 1050, loss is 5.056082935333252 and perplexity is 156.9744314898645
At time: 902.3941984176636 and batch: 1100, loss is 5.015106105804444 and perplexity is 150.67212310460943
At time: 903.1996386051178 and batch: 1150, loss is 5.044490022659302 and perplexity is 155.16514830779215
At time: 904.0069398880005 and batch: 1200, loss is 5.065765762329102 and perplexity is 158.50177029825124
At time: 904.8137536048889 and batch: 1250, loss is 5.0949922370910645 and perplexity is 163.20257769272135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.984781307025547 and perplexity of 146.1716048274154
Finished 42 epochs...
Completing Train Step...
At time: 907.4392433166504 and batch: 50, loss is 5.067311344146728 and perplexity is 158.7469371664076
At time: 908.263192653656 and batch: 100, loss is 5.085774669647217 and perplexity is 161.70515880776438
At time: 909.0767862796783 and batch: 150, loss is 5.004262542724609 and perplexity is 149.04712673130345
At time: 909.8916885852814 and batch: 200, loss is 5.050147037506104 and perplexity is 156.04540732715697
At time: 910.7082149982452 and batch: 250, loss is 5.069842576980591 and perplexity is 159.14927161230045
At time: 911.5214116573334 and batch: 300, loss is 5.0609643840789795 and perplexity is 157.7425674143486
At time: 912.3273565769196 and batch: 350, loss is 5.07794529914856 and perplexity is 160.4440524842809
At time: 913.1378951072693 and batch: 400, loss is 5.061663646697998 and perplexity is 157.85290946971057
At time: 913.9558434486389 and batch: 450, loss is 5.033273792266845 and perplexity is 153.4345040512806
At time: 914.754555940628 and batch: 500, loss is 5.031896381378174 and perplexity is 153.2233071805285
At time: 915.552396774292 and batch: 550, loss is 5.034210748672486 and perplexity is 153.57833286283872
At time: 916.357812166214 and batch: 600, loss is 5.064472303390503 and perplexity is 158.29688729911325
At time: 917.1639225482941 and batch: 650, loss is 5.060279588699341 and perplexity is 157.63458301084117
At time: 917.9645035266876 and batch: 700, loss is 5.070606708526611 and perplexity is 159.27092906649526
At time: 918.7609899044037 and batch: 750, loss is 5.06155086517334 and perplexity is 157.83510758178977
At time: 919.5653290748596 and batch: 800, loss is 5.080745992660522 and perplexity is 160.89403694124564
At time: 920.3714597225189 and batch: 850, loss is 5.113625984191895 and perplexity is 166.2721633762564
At time: 921.1789920330048 and batch: 900, loss is 5.084207563400269 and perplexity is 161.45194809909475
At time: 921.9769430160522 and batch: 950, loss is 5.072162666320801 and perplexity is 159.51894080782859
At time: 922.9313769340515 and batch: 1000, loss is 5.0664733791351315 and perplexity is 158.613968506687
At time: 923.7245597839355 and batch: 1050, loss is 5.054101915359497 and perplexity is 156.66376982079777
At time: 924.5218560695648 and batch: 1100, loss is 5.012847700119019 and perplexity is 150.33222827982377
At time: 925.3184432983398 and batch: 1150, loss is 5.042437925338745 and perplexity is 154.84706080758428
At time: 926.114688873291 and batch: 1200, loss is 5.063613529205322 and perplexity is 158.16100437343175
At time: 926.9098126888275 and batch: 1250, loss is 5.092881870269776 and perplexity is 162.85852355551907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.983414475935219 and perplexity of 145.9719494121121
Finished 43 epochs...
Completing Train Step...
At time: 929.1369667053223 and batch: 50, loss is 5.064594821929932 and perplexity is 158.3162827906706
At time: 929.961639881134 and batch: 100, loss is 5.083379898071289 and perplexity is 161.3183752038092
At time: 930.7575461864471 and batch: 150, loss is 5.001718254089355 and perplexity is 148.66838983294662
At time: 931.5541598796844 and batch: 200, loss is 5.04801160812378 and perplexity is 155.71253891435998
At time: 932.3495688438416 and batch: 250, loss is 5.067286949157715 and perplexity is 158.7430645838556
At time: 933.1476883888245 and batch: 300, loss is 5.0584579181671145 and perplexity is 157.3476861312447
At time: 933.9402477741241 and batch: 350, loss is 5.075650148391723 and perplexity is 160.076231459608
At time: 934.7413146495819 and batch: 400, loss is 5.059328641891479 and perplexity is 157.48475215919441
At time: 935.5617604255676 and batch: 450, loss is 5.030746641159058 and perplexity is 153.0472414160975
At time: 936.3572859764099 and batch: 500, loss is 5.029438438415528 and perplexity is 152.84715549999493
At time: 937.1633322238922 and batch: 550, loss is 5.031889295578003 and perplexity is 153.22222147463876
At time: 937.9605398178101 and batch: 600, loss is 5.0626154804229735 and perplexity is 158.00323072159887
At time: 938.7535572052002 and batch: 650, loss is 5.0583278179168705 and perplexity is 157.32721648948532
At time: 939.5511872768402 and batch: 700, loss is 5.0685734367370605 and perplexity is 158.9474169849784
At time: 940.3483707904816 and batch: 750, loss is 5.059148845672607 and perplexity is 157.4564395415532
At time: 941.1427531242371 and batch: 800, loss is 5.078587036132813 and perplexity is 160.5470484112766
At time: 941.9414141178131 and batch: 850, loss is 5.1113778018951415 and perplexity is 165.89877312405798
At time: 942.7397155761719 and batch: 900, loss is 5.082344961166382 and perplexity is 161.15150722767876
At time: 943.5860290527344 and batch: 950, loss is 5.070202512741089 and perplexity is 159.20656543684052
At time: 944.380975484848 and batch: 1000, loss is 5.06434853553772 and perplexity is 158.2772964456539
At time: 945.1780960559845 and batch: 1050, loss is 5.051970977783203 and perplexity is 156.33028455118227
At time: 945.97412276268 and batch: 1100, loss is 5.010569877624512 and perplexity is 149.99018785018004
At time: 946.7695243358612 and batch: 1150, loss is 5.040323934555054 and perplexity is 154.52006130689963
At time: 947.5645308494568 and batch: 1200, loss is 5.061575756072998 and perplexity is 157.83903628850953
At time: 948.3601107597351 and batch: 1250, loss is 5.090875940322876 and perplexity is 162.53216819843442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.982145657504562 and perplexity of 145.7868549627954
Finished 44 epochs...
Completing Train Step...
At time: 950.5927081108093 and batch: 50, loss is 5.062074470520019 and perplexity is 157.91777252802865
At time: 951.3873019218445 and batch: 100, loss is 5.0808687591552735 and perplexity is 160.9137905507032
At time: 952.185652256012 and batch: 150, loss is 4.999351100921631 and perplexity is 148.31688517982514
At time: 952.9823632240295 and batch: 200, loss is 5.045936136245728 and perplexity is 155.38969705926783
At time: 953.7798917293549 and batch: 250, loss is 5.064784154891968 and perplexity is 158.3462601191886
At time: 954.5779752731323 and batch: 300, loss is 5.055959234237671 and perplexity is 156.95501478166977
At time: 955.3884201049805 and batch: 350, loss is 5.073371438980103 and perplexity is 159.71187952815052
At time: 956.1838450431824 and batch: 400, loss is 5.05711763381958 and perplexity is 157.13693675399648
At time: 956.9797239303589 and batch: 450, loss is 5.028537359237671 and perplexity is 152.70949014378692
At time: 957.7762336730957 and batch: 500, loss is 5.027262849807739 and perplexity is 152.51498443456808
At time: 958.5746552944183 and batch: 550, loss is 5.029745044708252 and perplexity is 152.89402658481387
At time: 959.3702654838562 and batch: 600, loss is 5.060367288589478 and perplexity is 157.6484081526758
At time: 960.1665453910828 and batch: 650, loss is 5.056066799163818 and perplexity is 156.9718985442773
At time: 960.9635481834412 and batch: 700, loss is 5.066474103927613 and perplexity is 158.61408346894044
At time: 961.7913103103638 and batch: 750, loss is 5.056898164749145 and perplexity is 157.10245384066104
At time: 962.5956466197968 and batch: 800, loss is 5.076388711929321 and perplexity is 160.19450159702959
At time: 963.4542095661163 and batch: 850, loss is 5.109449710845947 and perplexity is 165.5792133536895
At time: 964.262401342392 and batch: 900, loss is 5.080528993606567 and perplexity is 160.859126875305
At time: 965.068642616272 and batch: 950, loss is 5.068104696273804 and perplexity is 158.8729293581536
At time: 965.8663859367371 and batch: 1000, loss is 5.062182655334473 and perplexity is 157.934857757113
At time: 966.6646454334259 and batch: 1050, loss is 5.050211734771729 and perplexity is 156.05550336491385
At time: 967.4875078201294 and batch: 1100, loss is 5.008657073974609 and perplexity is 149.70356028992546
At time: 968.3117249011993 and batch: 1150, loss is 5.038363780975342 and perplexity is 154.21747491037894
At time: 969.1326050758362 and batch: 1200, loss is 5.0596199798583985 and perplexity is 157.53064013083886
At time: 969.9294338226318 and batch: 1250, loss is 5.08918194770813 and perplexity is 162.2570729762696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.981052370837135 and perplexity of 145.6275552339806
Finished 45 epochs...
Completing Train Step...
At time: 972.1375341415405 and batch: 50, loss is 5.0595514106750485 and perplexity is 157.5198387538175
At time: 972.9622476100922 and batch: 100, loss is 5.078904542922974 and perplexity is 160.59803128256306
At time: 973.7575578689575 and batch: 150, loss is 4.997425699234009 and perplexity is 148.03159034051603
At time: 974.55357670784 and batch: 200, loss is 5.043883361816406 and perplexity is 155.07104423564576
At time: 975.3535318374634 and batch: 250, loss is 5.062607479095459 and perplexity is 158.0019664910593
At time: 976.1633658409119 and batch: 300, loss is 5.0539522552490235 and perplexity is 156.6403252581009
At time: 976.984347820282 and batch: 350, loss is 5.071333045959473 and perplexity is 159.38665552738536
At time: 977.7919597625732 and batch: 400, loss is 5.05497067451477 and perplexity is 156.7999320426537
At time: 978.6056258678436 and batch: 450, loss is 5.026433773040772 and perplexity is 152.38859020685777
At time: 979.4217252731323 and batch: 500, loss is 5.025177040100098 and perplexity is 152.19719873493554
At time: 980.2175941467285 and batch: 550, loss is 5.027621450424195 and perplexity is 152.56968620946392
At time: 981.0135447978973 and batch: 600, loss is 5.058279628753662 and perplexity is 157.31963520524215
At time: 981.8101232051849 and batch: 650, loss is 5.053799257278443 and perplexity is 156.61636143947985
At time: 982.6064105033875 and batch: 700, loss is 5.06452714920044 and perplexity is 158.30556945819544
At time: 983.4035720825195 and batch: 750, loss is 5.054941940307617 and perplexity is 156.79542658565543
At time: 984.2506773471832 and batch: 800, loss is 5.074575185775757 and perplexity is 159.90424794956726
At time: 985.0447313785553 and batch: 850, loss is 5.107798938751221 and perplexity is 165.3061052904227
At time: 985.8377861976624 and batch: 900, loss is 5.0790026569366455 and perplexity is 160.61378897301248
At time: 986.6350121498108 and batch: 950, loss is 5.066400575637817 and perplexity is 158.60242127540133
At time: 987.4333047866821 and batch: 1000, loss is 5.060326862335205 and perplexity is 157.64203514686133
At time: 988.2306220531464 and batch: 1050, loss is 5.048423681259155 and perplexity is 155.77671709063077
At time: 989.0293202400208 and batch: 1100, loss is 5.006942262649536 and perplexity is 149.44706691106526
At time: 989.8362221717834 and batch: 1150, loss is 5.0366589450836186 and perplexity is 153.95478341059476
At time: 990.6668190956116 and batch: 1200, loss is 5.057910356521607 and perplexity is 157.26155215729213
At time: 991.4659984111786 and batch: 1250, loss is 5.087432594299316 and perplexity is 161.973476140423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.980006308451186 and perplexity of 145.47529937454013
Finished 46 epochs...
Completing Train Step...
At time: 993.6993594169617 and batch: 50, loss is 5.057465848922729 and perplexity is 157.1916637364648
At time: 994.5440211296082 and batch: 100, loss is 5.076668434143066 and perplexity is 160.23931782540114
At time: 995.3406538963318 and batch: 150, loss is 4.995266580581665 and perplexity is 147.7123173708198
At time: 996.1472375392914 and batch: 200, loss is 5.042017889022827 and perplexity is 154.78203307659237
At time: 996.9760551452637 and batch: 250, loss is 5.060569467544556 and perplexity is 157.68028456536717
At time: 997.7785043716431 and batch: 300, loss is 5.051657238006592 and perplexity is 156.28124521582697
At time: 998.5763096809387 and batch: 350, loss is 5.068951377868652 and perplexity is 159.00750110503222
At time: 999.3718991279602 and batch: 400, loss is 5.052759914398194 and perplexity is 156.45366790111817
At time: 1000.1680643558502 and batch: 450, loss is 5.02437195777893 and perplexity is 152.07471677155016
At time: 1000.9635920524597 and batch: 500, loss is 5.023301038742066 and perplexity is 151.91194423599376
At time: 1001.7605175971985 and batch: 550, loss is 5.0256674861907955 and perplexity is 152.27186156361986
At time: 1002.556459903717 and batch: 600, loss is 5.0562028884887695 and perplexity is 156.99326219763614
At time: 1003.3523275852203 and batch: 650, loss is 5.051915760040283 and perplexity is 156.32165258404046
At time: 1004.1469566822052 and batch: 700, loss is 5.062635793685913 and perplexity is 158.0064403153683
At time: 1004.972795009613 and batch: 750, loss is 5.053053741455078 and perplexity is 156.49964497621986
At time: 1005.7684481143951 and batch: 800, loss is 5.072643117904663 and perplexity is 159.59560034972029
At time: 1006.5654008388519 and batch: 850, loss is 5.10612681388855 and perplexity is 165.02992381112472
At time: 1007.3593389987946 and batch: 900, loss is 5.077215662002564 and perplexity is 160.3270292411784
At time: 1008.1665697097778 and batch: 950, loss is 5.064440670013428 and perplexity is 158.29187991318818
At time: 1008.9717409610748 and batch: 1000, loss is 5.058715047836304 and perplexity is 157.3881500917468
At time: 1009.7672383785248 and batch: 1050, loss is 5.046550426483154 and perplexity is 155.48518075751727
At time: 1010.5634388923645 and batch: 1100, loss is 5.005183382034302 and perplexity is 149.18443839588085
At time: 1011.3594546318054 and batch: 1150, loss is 5.034708480834961 and perplexity is 153.65479276526176
At time: 1012.1572701931 and batch: 1200, loss is 5.055910882949829 and perplexity is 156.94742598803728
At time: 1012.9539096355438 and batch: 1250, loss is 5.08566876411438 and perplexity is 161.68803424356744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.979088999059078 and perplexity of 145.3419147029633
Finished 47 epochs...
Completing Train Step...
At time: 1015.2038888931274 and batch: 50, loss is 5.055317020416259 and perplexity is 156.8542484620621
At time: 1016.0146098136902 and batch: 100, loss is 5.074812774658203 and perplexity is 159.94224393466908
At time: 1016.8206043243408 and batch: 150, loss is 4.993543491363526 and perplexity is 147.45801502510614
At time: 1017.6298775672913 and batch: 200, loss is 5.040183753967285 and perplexity is 154.4984021120184
At time: 1018.4374973773956 and batch: 250, loss is 5.058651256561279 and perplexity is 157.37811042120373
At time: 1019.2435154914856 and batch: 300, loss is 5.049417343139648 and perplexity is 155.93158340590662
At time: 1020.0610585212708 and batch: 350, loss is 5.067281293869018 and perplexity is 158.74216684853522
At time: 1020.8623449802399 and batch: 400, loss is 5.050847768783569 and perplexity is 156.15479154477134
At time: 1021.6620011329651 and batch: 450, loss is 5.022680883407593 and perplexity is 151.81776443947567
At time: 1022.4654583930969 and batch: 500, loss is 5.02150818824768 and perplexity is 151.63983283208734
At time: 1023.2579123973846 and batch: 550, loss is 5.023796405792236 and perplexity is 151.98721504949683
At time: 1024.057209968567 and batch: 600, loss is 5.054556264877319 and perplexity is 156.73496610184924
At time: 1024.852468252182 and batch: 650, loss is 5.050088348388672 and perplexity is 156.0362494286588
At time: 1025.7013227939606 and batch: 700, loss is 5.060971021652222 and perplexity is 157.7436144456681
At time: 1026.4975864887238 and batch: 750, loss is 5.051398143768311 and perplexity is 156.24075889075345
At time: 1027.3011083602905 and batch: 800, loss is 5.070741291046143 and perplexity is 159.29236559187555
At time: 1028.1388757228851 and batch: 850, loss is 5.104547395706176 and perplexity is 164.76947827916752
At time: 1028.947841644287 and batch: 900, loss is 5.075713834762573 and perplexity is 160.08642645848678
At time: 1029.7762598991394 and batch: 950, loss is 5.062698554992676 and perplexity is 158.01635731723826
At time: 1030.5719003677368 and batch: 1000, loss is 5.057010488510132 and perplexity is 157.1201011702253
At time: 1031.3695640563965 and batch: 1050, loss is 5.044893941879272 and perplexity is 155.22783515282177
At time: 1032.1672804355621 and batch: 1100, loss is 5.003560056686402 and perplexity is 148.9424599735121
At time: 1032.962209701538 and batch: 1150, loss is 5.032985019683838 and perplexity is 153.3902027700288
At time: 1033.7589311599731 and batch: 1200, loss is 5.054264345169067 and perplexity is 156.6892187538731
At time: 1034.555824995041 and batch: 1250, loss is 5.08399115562439 and perplexity is 161.4170124224077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.978260346572765 and perplexity of 145.2215266507974
Finished 48 epochs...
Completing Train Step...
At time: 1036.7776806354523 and batch: 50, loss is 5.053175420761108 and perplexity is 156.51868890301694
At time: 1037.6370167732239 and batch: 100, loss is 5.072925815582275 and perplexity is 159.64072403317599
At time: 1038.4403326511383 and batch: 150, loss is 4.9916015625 and perplexity is 147.17193990823512
At time: 1039.2420217990875 and batch: 200, loss is 5.038593978881836 and perplexity is 154.25297953663662
At time: 1040.0382928848267 and batch: 250, loss is 5.056758966445923 and perplexity is 157.08058696760543
At time: 1040.838012456894 and batch: 300, loss is 5.047548379898071 and perplexity is 155.6404251750874
At time: 1041.6343972682953 and batch: 350, loss is 5.065391521453858 and perplexity is 158.44246355520298
At time: 1042.4301223754883 and batch: 400, loss is 5.049116582870483 and perplexity is 155.88469243273445
At time: 1043.2241036891937 and batch: 450, loss is 5.021043081283569 and perplexity is 151.5693204889625
At time: 1044.0202462673187 and batch: 500, loss is 5.019835405349731 and perplexity is 151.38638435431753
At time: 1044.813557624817 and batch: 550, loss is 5.0219050312042235 and perplexity is 151.7000219737067
At time: 1045.6548986434937 and batch: 600, loss is 5.052682590484619 and perplexity is 156.4415707589282
At time: 1046.4512395858765 and batch: 650, loss is 5.0485262966156 and perplexity is 155.7927029941664
At time: 1047.2856550216675 and batch: 700, loss is 5.0588742542266845 and perplexity is 157.41320928575055
At time: 1048.0843749046326 and batch: 750, loss is 5.049707307815551 and perplexity is 155.97680461291145
At time: 1048.8803205490112 and batch: 800, loss is 5.06895320892334 and perplexity is 159.00779225672903
At time: 1049.678428888321 and batch: 850, loss is 5.102942199707031 and perplexity is 164.50520313529978
At time: 1050.4747285842896 and batch: 900, loss is 5.074197902679443 and perplexity is 159.843930158944
At time: 1051.277509689331 and batch: 950, loss is 5.06099606513977 and perplexity is 157.74756494537928
At time: 1052.0855515003204 and batch: 1000, loss is 5.055464735031128 and perplexity is 156.87741983829753
At time: 1052.8919496536255 and batch: 1050, loss is 5.04336199760437 and perplexity is 154.9902168149513
At time: 1053.688045501709 and batch: 1100, loss is 5.001884593963623 and perplexity is 148.69312137108244
At time: 1054.4842884540558 and batch: 1150, loss is 5.031265907287597 and perplexity is 153.12673430182662
At time: 1055.2793083190918 and batch: 1200, loss is 5.0526762962341305 and perplexity is 156.4405860795939
At time: 1056.0732688903809 and batch: 1250, loss is 5.082261152267456 and perplexity is 161.13800186324056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.977437485743613 and perplexity of 145.1020786962282
Finished 49 epochs...
Completing Train Step...
At time: 1058.342048883438 and batch: 50, loss is 5.051078968048095 and perplexity is 156.19089859152922
At time: 1059.1379816532135 and batch: 100, loss is 5.07107439994812 and perplexity is 159.3454361355158
At time: 1059.9328157901764 and batch: 150, loss is 4.989858913421631 and perplexity is 146.915694200813
At time: 1060.7378196716309 and batch: 200, loss is 5.037136144638062 and perplexity is 154.02826809667795
At time: 1061.534006357193 and batch: 250, loss is 5.054878482818603 and perplexity is 156.78547705728485
At time: 1062.3302869796753 and batch: 300, loss is 5.04556471824646 and perplexity is 155.33199324566016
At time: 1063.126564025879 and batch: 350, loss is 5.063616828918457 and perplexity is 158.16152626023631
At time: 1063.9496395587921 and batch: 400, loss is 5.047351751327515 and perplexity is 155.60982482931414
At time: 1064.7467277050018 and batch: 450, loss is 5.019544544219971 and perplexity is 151.34235834257208
At time: 1065.5418734550476 and batch: 500, loss is 5.01809832572937 and perplexity is 151.1236424191534
At time: 1066.417650461197 and batch: 550, loss is 5.020143632888794 and perplexity is 151.43305299892592
At time: 1067.2109949588776 and batch: 600, loss is 5.050673265457153 and perplexity is 156.1275443916395
At time: 1068.0059342384338 and batch: 650, loss is 5.046960039138794 and perplexity is 155.54888250094942
At time: 1068.7988305091858 and batch: 700, loss is 5.056860580444336 and perplexity is 157.09654936510805
At time: 1069.5899424552917 and batch: 750, loss is 5.048046245574951 and perplexity is 155.71793249323275
At time: 1070.3808805942535 and batch: 800, loss is 5.067254219055176 and perplexity is 158.73786899210094
At time: 1071.1715524196625 and batch: 850, loss is 5.101329278945923 and perplexity is 164.24008314409767
At time: 1071.9650280475616 and batch: 900, loss is 5.072664775848389 and perplexity is 159.59905689968224
At time: 1072.7580120563507 and batch: 950, loss is 5.059373569488526 and perplexity is 157.49182772962334
At time: 1073.5494599342346 and batch: 1000, loss is 5.053824682235717 and perplexity is 156.620343454399
At time: 1074.3406538963318 and batch: 1050, loss is 5.041790771484375 and perplexity is 154.74688335395368
At time: 1075.1329500675201 and batch: 1100, loss is 5.000346775054932 and perplexity is 148.46463400856783
At time: 1075.9241080284119 and batch: 1150, loss is 5.029746723175049 and perplexity is 152.89428321257628
At time: 1076.7161712646484 and batch: 1200, loss is 5.0510511302948 and perplexity is 156.186550648346
At time: 1077.5092370510101 and batch: 1250, loss is 5.080730457305908 and perplexity is 160.891537414742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.976796393846944 and perplexity of 145.00908474140658
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f17ffca5e80>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 7.542203663166058, 'num_layers': 1, 'dropout': 0.455943817025638, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 28.082956472718813, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4933397769927979 and batch: 50, loss is 7.6491896915435795 and perplexity is 2098.944107990622
At time: 2.2881903648376465 and batch: 100, loss is 6.509663372039795 and perplexity is 671.6003000953996
At time: 3.0840976238250732 and batch: 150, loss is 6.351082134246826 and perplexity is 573.1125582997354
At time: 3.888566255569458 and batch: 200, loss is 6.729113626480102 and perplexity is 836.4055695439354
At time: 4.691049575805664 and batch: 250, loss is 6.760050601959229 and perplexity is 862.685848278899
At time: 5.496289491653442 and batch: 300, loss is 7.122131414413452 and perplexity is 1239.0886322897795
At time: 6.302465915679932 and batch: 350, loss is 6.868847236633301 and perplexity is 961.8391535490927
At time: 7.159104347229004 and batch: 400, loss is 6.776444654464722 and perplexity is 876.9453313591388
At time: 7.951915502548218 and batch: 450, loss is 6.7474424934387205 and perplexity is 851.8772923884217
At time: 8.748061895370483 and batch: 500, loss is 6.720548877716064 and perplexity is 829.2725558146287
At time: 9.541288614273071 and batch: 550, loss is 6.977773141860962 and perplexity is 1072.5273395090073
At time: 10.334639549255371 and batch: 600, loss is 7.109903898239136 and perplexity is 1224.0299090096562
At time: 11.12668776512146 and batch: 650, loss is 6.823151426315308 and perplexity is 918.8762226752362
At time: 11.94086742401123 and batch: 700, loss is 6.723793048858642 and perplexity is 831.9672265326545
At time: 12.73654294013977 and batch: 750, loss is 6.79466835975647 and perplexity is 893.0730315259913
At time: 13.536579608917236 and batch: 800, loss is 6.884770727157592 and perplexity is 977.2775808122029
At time: 14.341954231262207 and batch: 850, loss is 6.864512596130371 and perplexity is 957.6789496049481
At time: 15.147293090820312 and batch: 900, loss is 6.919701700210571 and perplexity is 1012.0180647287488
At time: 15.941635370254517 and batch: 950, loss is 7.053672590255737 and perplexity is 1157.100504927824
At time: 16.742940425872803 and batch: 1000, loss is 6.8113319873809814 and perplexity is 908.079552256644
At time: 17.5369393825531 and batch: 1050, loss is 6.715925045013428 and perplexity is 825.446989456897
At time: 18.359873294830322 and batch: 1100, loss is 6.700183992385864 and perplexity is 812.5553154057167
At time: 19.17905044555664 and batch: 1150, loss is 6.794847297668457 and perplexity is 893.2328504479102
At time: 19.995964765548706 and batch: 1200, loss is 6.716906137466431 and perplexity is 826.2572266624202
At time: 20.7914457321167 and batch: 1250, loss is 6.745969667434692 and perplexity is 850.6235488596233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.6037521919194795 and perplexity of 737.8585886313798
Finished 1 epochs...
Completing Train Step...
At time: 23.155701398849487 and batch: 50, loss is 6.740545682907104 and perplexity is 846.0222698056514
At time: 23.99192786216736 and batch: 100, loss is 6.6635865879058835 and perplexity is 783.3554780377087
At time: 24.800511121749878 and batch: 150, loss is 6.547077283859253 and perplexity is 697.2034652413852
At time: 25.58927893638611 and batch: 200, loss is 6.533571758270264 and perplexity is 687.8506654011396
At time: 26.38541007041931 and batch: 250, loss is 6.550421361923218 and perplexity is 699.5388707684403
At time: 27.179994583129883 and batch: 300, loss is 6.582948942184448 and perplexity is 722.667294153362
At time: 27.973805904388428 and batch: 350, loss is 6.629940185546875 and perplexity is 757.4368636150417
At time: 28.764492511749268 and batch: 400, loss is 6.588585414886475 and perplexity is 726.7520897344673
At time: 29.55968928337097 and batch: 450, loss is 6.593339834213257 and perplexity is 730.2156008836101
At time: 30.35352349281311 and batch: 500, loss is 6.6327956104278565 and perplexity is 759.6027584846044
At time: 31.145210027694702 and batch: 550, loss is 6.661161460876465 and perplexity is 781.4580431844901
At time: 31.936213731765747 and batch: 600, loss is 6.794317197799683 and perplexity is 892.7594733107674
At time: 32.79900527000427 and batch: 650, loss is 6.797343130111694 and perplexity is 895.4649943453003
At time: 33.62322449684143 and batch: 700, loss is 6.895125970840454 and perplexity is 987.4501068998211
At time: 34.41878342628479 and batch: 750, loss is 6.96033802986145 and perplexity is 1053.989777050768
At time: 35.21323370933533 and batch: 800, loss is 6.7598410987854 and perplexity is 862.5051317866678
At time: 36.00936150550842 and batch: 850, loss is 6.912477474212647 and perplexity is 1004.733362365285
At time: 36.80473351478577 and batch: 900, loss is 6.9333131217956545 and perplexity is 1025.887244758864
At time: 37.59730339050293 and batch: 950, loss is 6.757870140075684 and perplexity is 860.8068439623626
At time: 38.39071583747864 and batch: 1000, loss is 6.602832136154174 and perplexity is 737.180029786785
At time: 39.18418002128601 and batch: 1050, loss is 6.770567359924317 and perplexity is 871.8063817039948
At time: 39.978278160095215 and batch: 1100, loss is 6.770073261260986 and perplexity is 871.3757297371537
At time: 40.76723885536194 and batch: 1150, loss is 6.54116415977478 and perplexity is 693.0929795222738
At time: 41.55944085121155 and batch: 1200, loss is 6.747601547241211 and perplexity is 852.0127974870462
At time: 42.35135293006897 and batch: 1250, loss is 6.818786010742188 and perplexity is 914.8736888211159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.562811412950502 and perplexity of 708.260109802547
Finished 2 epochs...
Completing Train Step...
At time: 44.56919550895691 and batch: 50, loss is 6.696960773468017 and perplexity is 809.9404880855636
At time: 45.394463300704956 and batch: 100, loss is 6.485171670913696 and perplexity is 655.3514593665546
At time: 46.18227744102478 and batch: 150, loss is 6.564322929382325 and perplexity is 709.3314660788503
At time: 46.97340488433838 and batch: 200, loss is 6.655536632537842 and perplexity is 777.0748148478515
At time: 47.76390027999878 and batch: 250, loss is 6.453986234664917 and perplexity is 635.2294261513349
At time: 48.55507802963257 and batch: 300, loss is 6.503327436447144 and perplexity is 667.358535812771
At time: 49.34463143348694 and batch: 350, loss is 6.596037445068359 and perplexity is 732.1880977334034
At time: 50.13693714141846 and batch: 400, loss is 6.487059879302978 and perplexity is 656.5900684981311
At time: 50.92554688453674 and batch: 450, loss is 6.557131156921387 and perplexity is 704.2484155554167
At time: 51.71726059913635 and batch: 500, loss is 6.523402166366577 and perplexity is 680.8909535393925
At time: 52.558186292648315 and batch: 550, loss is 6.550174865722656 and perplexity is 699.3664583450276
At time: 53.34975290298462 and batch: 600, loss is 6.595991716384888 and perplexity is 732.1546165011731
At time: 54.164052963256836 and batch: 650, loss is 6.634947338104248 and perplexity is 761.2389764795731
At time: 54.97244477272034 and batch: 700, loss is 6.6737085056304934 and perplexity is 791.3248021167276
At time: 55.77408218383789 and batch: 750, loss is 6.4994428348541256 and perplexity is 664.7711425311443
At time: 56.59464502334595 and batch: 800, loss is 6.563519763946533 and perplexity is 708.7619842874351
At time: 57.38538408279419 and batch: 850, loss is 6.605497121810913 and perplexity is 739.1472241006408
At time: 58.17773938179016 and batch: 900, loss is 6.6761006832122805 and perplexity is 793.2200575582508
At time: 58.97016906738281 and batch: 950, loss is 6.52174331665039 and perplexity is 679.7623940884176
At time: 59.760457038879395 and batch: 1000, loss is 6.448301725387573 and perplexity is 631.6287024548019
At time: 60.551676511764526 and batch: 1050, loss is 6.402878026962281 and perplexity is 603.5796590641451
At time: 61.354568004608154 and batch: 1100, loss is 6.416454544067383 and perplexity is 611.8300476739347
At time: 62.168768882751465 and batch: 1150, loss is 6.562474670410157 and perplexity is 708.0216486462078
At time: 62.9615523815155 and batch: 1200, loss is 6.533676595687866 and perplexity is 687.9227816687719
At time: 63.766032218933105 and batch: 1250, loss is 6.611216239929199 and perplexity is 743.3866055885696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.298256978501368 and perplexity of 543.6235363370863
Finished 3 epochs...
Completing Train Step...
At time: 66.00583052635193 and batch: 50, loss is 6.571391830444336 and perplexity is 714.3634242864641
At time: 66.79768800735474 and batch: 100, loss is 6.562165641784668 and perplexity is 707.802883493399
At time: 67.58859014511108 and batch: 150, loss is 6.513022336959839 and perplexity is 673.8599749033242
At time: 68.37848663330078 and batch: 200, loss is 6.603715944290161 and perplexity is 737.8318434914964
At time: 69.16957831382751 and batch: 250, loss is 6.682598905563355 and perplexity is 798.3913618107048
At time: 69.96148538589478 and batch: 300, loss is 6.581283531188965 and perplexity is 721.4647577322378
At time: 70.75259923934937 and batch: 350, loss is 6.530503711700439 and perplexity is 685.7435415564931
At time: 71.54225039482117 and batch: 400, loss is 6.4839558601379395 and perplexity is 654.5551601730093
At time: 72.40762257575989 and batch: 450, loss is 6.645272064208984 and perplexity is 769.1392744374194
At time: 73.19626021385193 and batch: 500, loss is 6.592485589981079 and perplexity is 729.5920847737459
At time: 73.98683428764343 and batch: 550, loss is 6.614503965377808 and perplexity is 745.8346787409901
At time: 74.77850365638733 and batch: 600, loss is 6.800495271682739 and perplexity is 898.2920801258725
At time: 75.57070016860962 and batch: 650, loss is 6.550881986618042 and perplexity is 699.8611698710699
At time: 76.36053228378296 and batch: 700, loss is 6.544599714279175 and perplexity is 695.478233218474
At time: 77.14734649658203 and batch: 750, loss is 6.544363765716553 and perplexity is 695.3141554867246
At time: 77.94001507759094 and batch: 800, loss is 6.428934841156006 and perplexity is 619.5137159356136
At time: 78.73101019859314 and batch: 850, loss is 6.607171964645386 and perplexity is 740.3862168019454
At time: 79.52258205413818 and batch: 900, loss is 6.589958362579345 and perplexity is 727.7505676114631
At time: 80.31567096710205 and batch: 950, loss is 6.605158224105835 and perplexity is 738.8967712440242
At time: 81.10744333267212 and batch: 1000, loss is 6.652206745147705 and perplexity is 774.4915466038436
At time: 81.89871001243591 and batch: 1050, loss is 6.725349073410034 and perplexity is 833.2627956703819
At time: 82.69077587127686 and batch: 1100, loss is 6.609021577835083 and perplexity is 741.7569121513407
At time: 83.4821298122406 and batch: 1150, loss is 6.75261999130249 and perplexity is 856.299322898463
At time: 84.27453947067261 and batch: 1200, loss is 6.74453652381897 and perplexity is 849.4053562821533
At time: 85.06528878211975 and batch: 1250, loss is 6.718264722824097 and perplexity is 827.3805305115318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 7.251403363081661 and perplexity of 1410.0823177931406
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 87.25549745559692 and batch: 50, loss is 6.258327922821045 and perplexity is 522.3448086699246
At time: 88.07583093643188 and batch: 100, loss is 6.004527931213379 and perplexity is 405.2596331491671
At time: 88.8706955909729 and batch: 150, loss is 5.85191837310791 and perplexity is 347.9011449233938
At time: 89.66507029533386 and batch: 200, loss is 5.8791334915161135 and perplexity is 357.4993312547088
At time: 90.45934391021729 and batch: 250, loss is 5.878712739944458 and perplexity is 357.3489444891701
At time: 91.25360584259033 and batch: 300, loss is 5.881004915237427 and perplexity is 358.1689903961323
At time: 92.09165000915527 and batch: 350, loss is 5.898080091476441 and perplexity is 364.3373016619571
At time: 92.9215247631073 and batch: 400, loss is 5.882048835754395 and perplexity is 358.5430855825955
At time: 93.72081542015076 and batch: 450, loss is 5.883118886947631 and perplexity is 358.9269503800148
At time: 94.5167624950409 and batch: 500, loss is 5.857187490463257 and perplexity is 349.7391148713442
At time: 95.3099160194397 and batch: 550, loss is 5.872702112197876 and perplexity is 355.20749518264466
At time: 96.10504531860352 and batch: 600, loss is 5.903053226470948 and perplexity is 366.15371313274795
At time: 96.8985948562622 and batch: 650, loss is 5.88953293800354 and perplexity is 361.2365251122221
At time: 97.69259357452393 and batch: 700, loss is 5.946947336196899 and perplexity is 382.58365536751023
At time: 98.48747491836548 and batch: 750, loss is 5.897507762908935 and perplexity is 364.12884067578716
At time: 99.28140616416931 and batch: 800, loss is 5.907337589263916 and perplexity is 367.72581379784083
At time: 100.08380246162415 and batch: 850, loss is 5.886304416656494 and perplexity is 360.07214590252624
At time: 100.87870931625366 and batch: 900, loss is 5.830808162689209 and perplexity is 340.6338554357811
At time: 101.67544198036194 and batch: 950, loss is 5.790999526977539 and perplexity is 327.3400461221442
At time: 102.49505805969238 and batch: 1000, loss is 5.785499839782715 and perplexity is 325.54471965432765
At time: 103.3046224117279 and batch: 1050, loss is 5.803092021942138 and perplexity is 331.3224339559248
At time: 104.10175442695618 and batch: 1100, loss is 5.785255870819092 and perplexity is 325.4653065340228
At time: 104.897141456604 and batch: 1150, loss is 5.817532758712769 and perplexity is 336.1416870452956
At time: 105.69396042823792 and batch: 1200, loss is 5.821959924697876 and perplexity is 337.6331411096944
At time: 106.48987293243408 and batch: 1250, loss is 5.828207788467407 and perplexity is 339.7492306153044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.511524506729015 and perplexity of 247.52819797445946
Finished 5 epochs...
Completing Train Step...
At time: 108.72737503051758 and batch: 50, loss is 5.8703978157043455 and perplexity is 354.38993411033147
At time: 109.52231287956238 and batch: 100, loss is 5.862197904586792 and perplexity is 351.4958499821285
At time: 110.31637382507324 and batch: 150, loss is 5.743612689971924 and perplexity is 312.1902226393479
At time: 111.10883045196533 and batch: 200, loss is 5.7840233993530275 and perplexity is 325.0644269176142
At time: 111.90493583679199 and batch: 250, loss is 5.788925104141235 and perplexity is 326.6617082782423
At time: 112.70040965080261 and batch: 300, loss is 5.795949440002442 and perplexity is 328.96436768871223
At time: 113.52423238754272 and batch: 350, loss is 5.817157745361328 and perplexity is 336.0156530583688
At time: 114.31779170036316 and batch: 400, loss is 5.797945098876953 and perplexity is 329.62152386015885
At time: 115.11082863807678 and batch: 450, loss is 5.783646697998047 and perplexity is 324.9419977685993
At time: 115.90502262115479 and batch: 500, loss is 5.771017875671387 and perplexity is 320.86416631343906
At time: 116.70238280296326 and batch: 550, loss is 5.792984046936035 and perplexity is 327.9903039883352
At time: 117.49370455741882 and batch: 600, loss is 5.821837854385376 and perplexity is 337.5919286421094
At time: 118.28948092460632 and batch: 650, loss is 5.805218429565429 and perplexity is 332.02771009178343
At time: 119.08468723297119 and batch: 700, loss is 5.838491086959839 and perplexity is 343.260998699722
At time: 119.88051438331604 and batch: 750, loss is 5.790758247375488 and perplexity is 327.26107517350323
At time: 120.67446684837341 and batch: 800, loss is 5.812679176330566 and perplexity is 334.5141485662009
At time: 121.4677722454071 and batch: 850, loss is 5.804893379211426 and perplexity is 331.91980190582615
At time: 122.2638430595398 and batch: 900, loss is 5.778796234130859 and perplexity is 323.36969463189746
At time: 123.05740594863892 and batch: 950, loss is 5.752212333679199 and perplexity is 314.8865243230361
At time: 123.88045358657837 and batch: 1000, loss is 5.7448790073394775 and perplexity is 312.5858049537724
At time: 124.67685914039612 and batch: 1050, loss is 5.758808603286743 and perplexity is 316.97046629865724
At time: 125.47123503684998 and batch: 1100, loss is 5.740479469299316 and perplexity is 311.21359257770814
At time: 126.26623916625977 and batch: 1150, loss is 5.768842420578003 and perplexity is 320.1668994399246
At time: 127.06082034111023 and batch: 1200, loss is 5.777995576858521 and perplexity is 323.11088995498545
At time: 127.85526990890503 and batch: 1250, loss is 5.7702521800994875 and perplexity is 320.61857607778865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.474549676380018 and perplexity of 238.54302118297576
Finished 6 epochs...
Completing Train Step...
At time: 130.03885340690613 and batch: 50, loss is 5.792982940673828 and perplexity is 327.98994114525823
At time: 130.8611261844635 and batch: 100, loss is 5.798008852005005 and perplexity is 329.64253893325923
At time: 131.65459823608398 and batch: 150, loss is 5.688402509689331 and perplexity is 295.4213108041591
At time: 132.46051692962646 and batch: 200, loss is 5.733424949645996 and perplexity is 309.025855960914
At time: 133.25904941558838 and batch: 250, loss is 5.744764575958252 and perplexity is 312.5500373748657
At time: 134.08318448066711 and batch: 300, loss is 5.751792945861816 and perplexity is 314.7544924390941
At time: 134.9026927947998 and batch: 350, loss is 5.775271816253662 and perplexity is 322.23201071360995
At time: 135.70645689964294 and batch: 400, loss is 5.752817058563233 and perplexity is 315.07700162731936
At time: 136.5006628036499 and batch: 450, loss is 5.735421380996704 and perplexity is 309.64342112604106
At time: 137.2964723110199 and batch: 500, loss is 5.725562791824341 and perplexity is 306.6057719178691
At time: 138.08880519866943 and batch: 550, loss is 5.7478773021698 and perplexity is 313.52443579963096
At time: 138.88320350646973 and batch: 600, loss is 5.774222507476806 and perplexity is 321.8940671711546
At time: 139.67352271080017 and batch: 650, loss is 5.756434736251831 and perplexity is 316.2189129544566
At time: 140.46873593330383 and batch: 700, loss is 5.783108358383179 and perplexity is 324.76711569586337
At time: 141.262291431427 and batch: 750, loss is 5.736536073684692 and perplexity is 309.98877082708793
At time: 142.05568623542786 and batch: 800, loss is 5.7515937614440915 and perplexity is 314.69180449222966
At time: 142.87441325187683 and batch: 850, loss is 5.754315996170044 and perplexity is 315.5496365307219
At time: 143.68186116218567 and batch: 900, loss is 5.741192131042481 and perplexity is 311.43546164850005
At time: 144.47483086585999 and batch: 950, loss is 5.720845947265625 and perplexity is 305.1629655725936
At time: 145.26923155784607 and batch: 1000, loss is 5.716377029418945 and perplexity is 303.80226005476595
At time: 146.06633758544922 and batch: 1050, loss is 5.725067348480224 and perplexity is 306.4539037530468
At time: 146.86216163635254 and batch: 1100, loss is 5.70709608078003 and perplexity is 300.99573065565556
At time: 147.6573405265808 and batch: 1150, loss is 5.734828033447266 and perplexity is 309.45974945687226
At time: 148.45220732688904 and batch: 1200, loss is 5.744288730621338 and perplexity is 312.4013472765792
At time: 149.24787068367004 and batch: 1250, loss is 5.7309752368927 and perplexity is 308.269757870154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.451162519246123 and perplexity of 233.02890909681463
Finished 7 epochs...
Completing Train Step...
At time: 151.43489789962769 and batch: 50, loss is 5.742726793289185 and perplexity is 311.91377682602246
At time: 152.31305599212646 and batch: 100, loss is 5.7487891483306885 and perplexity is 313.81045223443584
At time: 153.11855673789978 and batch: 150, loss is 5.651957607269287 and perplexity is 284.84854195425163
At time: 154.0028657913208 and batch: 200, loss is 5.694590911865235 and perplexity is 297.25516514836903
At time: 154.8283772468567 and batch: 250, loss is 5.710188341140747 and perplexity is 301.9279283785288
At time: 155.63691473007202 and batch: 300, loss is 5.713934488296509 and perplexity is 303.0611160473236
At time: 156.44905853271484 and batch: 350, loss is 5.740907392501831 and perplexity is 311.3467965934219
At time: 157.25703525543213 and batch: 400, loss is 5.719389610290527 and perplexity is 304.71886891808
At time: 158.0627303123474 and batch: 450, loss is 5.701940727233887 and perplexity is 299.4479842661212
At time: 158.8697748184204 and batch: 500, loss is 5.6920979213714595 and perplexity is 296.51503380096165
At time: 159.67698764801025 and batch: 550, loss is 5.711377801895142 and perplexity is 302.2872734710659
At time: 160.48120784759521 and batch: 600, loss is 5.740455160140991 and perplexity is 311.206027329166
At time: 161.29979491233826 and batch: 650, loss is 5.725233659744263 and perplexity is 306.50487472755566
At time: 162.10856366157532 and batch: 700, loss is 5.748539791107178 and perplexity is 313.7322110867612
At time: 162.90367913246155 and batch: 750, loss is 5.700467596054077 and perplexity is 299.0071828626292
At time: 163.69961857795715 and batch: 800, loss is 5.7146814250946045 and perplexity is 303.2875681091318
At time: 164.49605417251587 and batch: 850, loss is 5.721410913467407 and perplexity is 305.33542104534735
At time: 165.31518268585205 and batch: 900, loss is 5.712355756759644 and perplexity is 302.58304138089443
At time: 166.11078763008118 and batch: 950, loss is 5.69263708114624 and perplexity is 296.67494588501273
At time: 166.90618705749512 and batch: 1000, loss is 5.689888420104981 and perplexity is 295.8606067032361
At time: 167.70196080207825 and batch: 1050, loss is 5.6980562591552735 and perplexity is 298.2870444067405
At time: 168.4979498386383 and batch: 1100, loss is 5.681594362258911 and perplexity is 293.4168699727568
At time: 169.29435396194458 and batch: 1150, loss is 5.70744366645813 and perplexity is 301.1003706454282
At time: 170.11794686317444 and batch: 1200, loss is 5.717123374938965 and perplexity is 304.02908614531896
At time: 170.93129467964172 and batch: 1250, loss is 5.704159851074219 and perplexity is 300.11323429003687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.433704682510265 and perplexity of 228.9960335152817
Finished 8 epochs...
Completing Train Step...
At time: 173.19572639465332 and batch: 50, loss is 5.705585327148437 and perplexity is 300.5413435823578
At time: 174.00084733963013 and batch: 100, loss is 5.720098667144775 and perplexity is 304.93500853928947
At time: 174.82316970825195 and batch: 150, loss is 5.629561939239502 and perplexity is 278.54007342720814
At time: 175.61840963363647 and batch: 200, loss is 5.672379693984985 and perplexity is 290.7255497050587
At time: 176.41460180282593 and batch: 250, loss is 5.688328895568848 and perplexity is 295.39956442462244
At time: 177.2112500667572 and batch: 300, loss is 5.69212155342102 and perplexity is 296.5220411417345
At time: 178.02515769004822 and batch: 350, loss is 5.719952335357666 and perplexity is 304.89039011916407
At time: 178.838449716568 and batch: 400, loss is 5.69800498008728 and perplexity is 298.2717489172813
At time: 179.6574306488037 and batch: 450, loss is 5.678498296737671 and perplexity is 292.50983695964266
At time: 180.4506528377533 and batch: 500, loss is 5.670202713012696 and perplexity is 290.09333412552127
At time: 181.24650311470032 and batch: 550, loss is 5.685205726623535 and perplexity is 294.4784208707736
At time: 182.0493667125702 and batch: 600, loss is 5.714227409362793 and perplexity is 303.14990203559364
At time: 182.84468507766724 and batch: 650, loss is 5.698835592269898 and perplexity is 298.5195999854714
At time: 183.6393973827362 and batch: 700, loss is 5.723455476760864 and perplexity is 305.9603372620964
At time: 184.43102645874023 and batch: 750, loss is 5.674149475097656 and perplexity is 291.2405258550153
At time: 185.2252902984619 and batch: 800, loss is 5.6763367843627925 and perplexity is 291.87825615804246
At time: 186.02188801765442 and batch: 850, loss is 5.6872486400604245 and perplexity is 295.0806297144708
At time: 186.82124161720276 and batch: 900, loss is 5.675419549942017 and perplexity is 291.610658118646
At time: 187.63321781158447 and batch: 950, loss is 5.6527793502807615 and perplexity is 285.08271045291787
At time: 188.44185590744019 and batch: 1000, loss is 5.652118215560913 and perplexity is 284.89429466598114
At time: 189.25848865509033 and batch: 1050, loss is 5.658330297470092 and perplexity is 286.66958979022263
At time: 190.05425477027893 and batch: 1100, loss is 5.641584224700928 and perplexity is 281.9089720559189
At time: 190.8466215133667 and batch: 1150, loss is 5.665061655044556 and perplexity is 288.60577456997174
At time: 191.6595265865326 and batch: 1200, loss is 5.674139757156372 and perplexity is 291.2376956104374
At time: 192.4877028465271 and batch: 1250, loss is 5.664166574478149 and perplexity is 288.3475647263471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.402406121692518 and perplexity of 221.93978840696602
Finished 9 epochs...
Completing Train Step...
At time: 194.71119332313538 and batch: 50, loss is 5.657424659729004 and perplexity is 286.4100885152969
At time: 195.53231525421143 and batch: 100, loss is 5.67546462059021 and perplexity is 291.62380149621544
At time: 196.32308316230774 and batch: 150, loss is 5.5898165607452395 and perplexity is 267.68651099546514
At time: 197.11483478546143 and batch: 200, loss is 5.628860950469971 and perplexity is 278.34488838312575
At time: 197.91202092170715 and batch: 250, loss is 5.646935024261475 and perplexity is 283.4214533431538
At time: 198.73379755020142 and batch: 300, loss is 5.650597829818725 and perplexity is 284.46147455247973
At time: 199.55509614944458 and batch: 350, loss is 5.674233455657959 and perplexity is 291.2649854246091
At time: 200.39227628707886 and batch: 400, loss is 5.655160570144654 and perplexity is 285.7623639469528
At time: 201.20331120491028 and batch: 450, loss is 5.6336843776702885 and perplexity is 279.69070781058207
At time: 201.9988465309143 and batch: 500, loss is 5.622710256576538 and perplexity is 276.6381284375715
At time: 202.79438614845276 and batch: 550, loss is 5.642532958984375 and perplexity is 282.17655567532535
At time: 203.59071159362793 and batch: 600, loss is 5.673536491394043 and perplexity is 291.06205486432725
At time: 204.39921760559082 and batch: 650, loss is 5.658768510818481 and perplexity is 286.7952397597821
At time: 205.2286696434021 and batch: 700, loss is 5.6824932956695555 and perplexity is 293.6807507882929
At time: 206.04568648338318 and batch: 750, loss is 5.6400227451324465 and perplexity is 281.4691204548973
At time: 206.83811807632446 and batch: 800, loss is 5.646875343322754 and perplexity is 283.40453898950193
At time: 207.63488006591797 and batch: 850, loss is 5.6617064285278325 and perplexity is 287.63905950317115
At time: 208.42928791046143 and batch: 900, loss is 5.652418088912964 and perplexity is 284.97973968380455
At time: 209.22455549240112 and batch: 950, loss is 5.628385725021363 and perplexity is 278.2126432342951
At time: 210.02145147323608 and batch: 1000, loss is 5.630257453918457 and perplexity is 278.7338695231564
At time: 210.81840586662292 and batch: 1050, loss is 5.6348453140258785 and perplexity is 280.0155994743821
At time: 211.64054012298584 and batch: 1100, loss is 5.616332168579102 and perplexity is 274.8793209876477
At time: 212.43776726722717 and batch: 1150, loss is 5.64153223991394 and perplexity is 281.894317458968
At time: 213.2357349395752 and batch: 1200, loss is 5.65141339302063 and perplexity is 284.6935654930352
At time: 214.07334756851196 and batch: 1250, loss is 5.640852766036987 and perplexity is 281.70284269261083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.386858640796077 and perplexity of 218.5158694141655
Finished 10 epochs...
Completing Train Step...
At time: 216.33966183662415 and batch: 50, loss is 5.632291145324707 and perplexity is 279.30130499698856
At time: 217.13465476036072 and batch: 100, loss is 5.653933849334717 and perplexity is 285.4120282343902
At time: 217.9289586544037 and batch: 150, loss is 5.571987152099609 and perplexity is 262.95611420137976
At time: 218.72507119178772 and batch: 200, loss is 5.610121126174927 and perplexity is 273.1773249253152
At time: 219.52097535133362 and batch: 250, loss is 5.628330383300781 and perplexity is 278.1972468939649
At time: 220.31571006774902 and batch: 300, loss is 5.6306530952453615 and perplexity is 278.84416997941844
At time: 221.11193442344666 and batch: 350, loss is 5.652759323120117 and perplexity is 285.07700111284976
At time: 221.90761876106262 and batch: 400, loss is 5.632518758773804 and perplexity is 279.3648849659096
At time: 222.70222353935242 and batch: 450, loss is 5.61449348449707 and perplexity is 274.37436912140754
At time: 223.49734163284302 and batch: 500, loss is 5.606961517333985 and perplexity is 272.3155535814457
At time: 224.2928192615509 and batch: 550, loss is 5.624306383132935 and perplexity is 277.08003047285297
At time: 225.088205575943 and batch: 600, loss is 5.65431658744812 and perplexity is 285.52128720307127
At time: 225.8825933933258 and batch: 650, loss is 5.641784324645996 and perplexity is 281.9653876699338
At time: 226.68044543266296 and batch: 700, loss is 5.6645534801483155 and perplexity is 288.4591496191402
At time: 227.47442626953125 and batch: 750, loss is 5.621516857147217 and perplexity is 276.3081855689399
At time: 228.26877689361572 and batch: 800, loss is 5.63191011428833 and perplexity is 279.1949028038404
At time: 229.06296014785767 and batch: 850, loss is 5.646625118255615 and perplexity is 283.33363294131124
At time: 229.87024784088135 and batch: 900, loss is 5.63677981376648 and perplexity is 280.5578138666366
At time: 230.68706703186035 and batch: 950, loss is 5.612612218856811 and perplexity is 273.85868327133835
At time: 231.479829788208 and batch: 1000, loss is 5.613653354644775 and perplexity is 274.1439558253007
At time: 232.27513670921326 and batch: 1050, loss is 5.617239713668823 and perplexity is 275.1288996004681
At time: 233.07308077812195 and batch: 1100, loss is 5.599825353622436 and perplexity is 270.37918255764424
At time: 233.89049243927002 and batch: 1150, loss is 5.624975929260254 and perplexity is 277.2656104544596
At time: 234.70686030387878 and batch: 1200, loss is 5.634442310333252 and perplexity is 279.90277488969014
At time: 235.50338768959045 and batch: 1250, loss is 5.6235636043548585 and perplexity is 276.8742977228082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.377560803489963 and perplexity of 216.4935605300343
Finished 11 epochs...
Completing Train Step...
At time: 237.71972584724426 and batch: 50, loss is 5.615153226852417 and perplexity is 274.55544523916274
At time: 238.54284238815308 and batch: 100, loss is 5.636368007659912 and perplexity is 280.4423022314718
At time: 239.33787655830383 and batch: 150, loss is 5.553287696838379 and perplexity is 258.084666763852
At time: 240.13351106643677 and batch: 200, loss is 5.593649024963379 and perplexity is 268.714378344862
At time: 240.93537211418152 and batch: 250, loss is 5.609463682174683 and perplexity is 272.99778515717895
At time: 241.73857259750366 and batch: 300, loss is 5.611243371963501 and perplexity is 273.48406911674135
At time: 242.5382513999939 and batch: 350, loss is 5.636524095535278 and perplexity is 280.4860792910351
At time: 243.36709237098694 and batch: 400, loss is 5.616383619308472 and perplexity is 274.89346409303477
At time: 244.19350695610046 and batch: 450, loss is 5.595967855453491 and perplexity is 269.33820443245105
At time: 244.99149703979492 and batch: 500, loss is 5.589299163818359 and perplexity is 267.54804664091347
At time: 245.79885363578796 and batch: 550, loss is 5.603733110427856 and perplexity is 271.43782576124664
At time: 246.60886406898499 and batch: 600, loss is 5.63652961730957 and perplexity is 280.487628076133
At time: 247.43552017211914 and batch: 650, loss is 5.6272557258605955 and perplexity is 277.8984407386374
At time: 248.2374243736267 and batch: 700, loss is 5.651228933334351 and perplexity is 284.6410558503636
At time: 249.03202033042908 and batch: 750, loss is 5.607706651687622 and perplexity is 272.51854087245914
At time: 249.8258662223816 and batch: 800, loss is 5.617971363067627 and perplexity is 275.3302711521588
At time: 250.62068605422974 and batch: 850, loss is 5.63209418296814 and perplexity is 279.2462985710394
At time: 251.42482614517212 and batch: 900, loss is 5.622916173934937 and perplexity is 276.69509889561346
At time: 252.2273416519165 and batch: 950, loss is 5.5990396499633786 and perplexity is 270.16682807934535
At time: 253.02468609809875 and batch: 1000, loss is 5.603895759582519 and perplexity is 271.48197848475445
At time: 253.84748315811157 and batch: 1050, loss is 5.607946224212647 and perplexity is 272.5838366486366
At time: 254.64337515830994 and batch: 1100, loss is 5.586338176727295 and perplexity is 266.7570120281545
At time: 255.44113206863403 and batch: 1150, loss is 5.613434963226318 and perplexity is 274.08409167507284
At time: 256.2369918823242 and batch: 1200, loss is 5.6180921649932865 and perplexity is 275.3635335881489
At time: 257.0819685459137 and batch: 1250, loss is 5.6083229064941404 and perplexity is 272.68653349094984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.367802306683394 and perplexity of 214.39118351443793
Finished 12 epochs...
Completing Train Step...
At time: 259.29649019241333 and batch: 50, loss is 5.594076795578003 and perplexity is 268.8293510488637
At time: 260.12289547920227 and batch: 100, loss is 5.613109569549561 and perplexity is 273.99492095332334
At time: 260.91703486442566 and batch: 150, loss is 5.53264196395874 and perplexity is 252.81094690323837
At time: 261.7175979614258 and batch: 200, loss is 5.571553001403808 and perplexity is 262.8419763997299
At time: 262.52085995674133 and batch: 250, loss is 5.5824153614044185 and perplexity is 265.7126233451461
At time: 263.32655692100525 and batch: 300, loss is 5.587145500183105 and perplexity is 266.9724581766296
At time: 264.14670634269714 and batch: 350, loss is 5.6085142993927 and perplexity is 272.7387287517347
At time: 264.95690059661865 and batch: 400, loss is 5.586965961456299 and perplexity is 266.9245305839544
At time: 265.7662835121155 and batch: 450, loss is 5.566685066223145 and perplexity is 261.5655879046135
At time: 266.5769581794739 and batch: 500, loss is 5.560642328262329 and perplexity is 259.9897814849775
At time: 267.371080160141 and batch: 550, loss is 5.569305591583252 and perplexity is 262.25192605154285
At time: 268.16457653045654 and batch: 600, loss is 5.599255065917969 and perplexity is 270.2250325933802
At time: 268.95466589927673 and batch: 650, loss is 5.587607545852661 and perplexity is 267.09584014667826
At time: 269.7510709762573 and batch: 700, loss is 5.609745769500733 and perplexity is 273.07480523509463
At time: 270.55020666122437 and batch: 750, loss is 5.572285642623902 and perplexity is 263.03461582518617
At time: 271.3405718803406 and batch: 800, loss is 5.581841564178466 and perplexity is 265.56020191264975
At time: 272.1324498653412 and batch: 850, loss is 5.600717887878418 and perplexity is 270.62061296660926
At time: 272.92699217796326 and batch: 900, loss is 5.589233751296997 and perplexity is 267.5305462209767
At time: 273.72006344795227 and batch: 950, loss is 5.56701847076416 and perplexity is 261.6528095986407
At time: 274.51624274253845 and batch: 1000, loss is 5.5685460662841795 and perplexity is 262.0528147036013
At time: 275.3176245689392 and batch: 1050, loss is 5.570085315704346 and perplexity is 262.45648994542586
At time: 276.1138246059418 and batch: 1100, loss is 5.546757974624634 and perplexity is 256.404935639334
At time: 276.97468852996826 and batch: 1150, loss is 5.571421947479248 and perplexity is 262.80753218425764
At time: 277.78480339050293 and batch: 1200, loss is 5.5812294292449955 and perplexity is 265.39769297989125
At time: 278.5822536945343 and batch: 1250, loss is 5.572702760696411 and perplexity is 263.1443552026902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.344167890339873 and perplexity of 209.38358198824045
Finished 13 epochs...
Completing Train Step...
At time: 280.8128559589386 and batch: 50, loss is 5.55902172088623 and perplexity is 259.5687813574904
At time: 281.60603189468384 and batch: 100, loss is 5.579539394378662 and perplexity is 264.9495404286976
At time: 282.41836762428284 and batch: 150, loss is 5.4968415641784665 and perplexity is 243.92030770490018
At time: 283.21062326431274 and batch: 200, loss is 5.5402802467346195 and perplexity is 254.74938213819658
At time: 284.00630402565 and batch: 250, loss is 5.55178614616394 and perplexity is 257.69743035871016
At time: 284.80465483665466 and batch: 300, loss is 5.553899068832397 and perplexity is 258.24250074391824
At time: 285.5990993976593 and batch: 350, loss is 5.580515146255493 and perplexity is 265.2081916092023
At time: 286.3934006690979 and batch: 400, loss is 5.560395994186401 and perplexity is 259.92574502990914
At time: 287.1851599216461 and batch: 450, loss is 5.539038228988647 and perplexity is 254.4331752927108
At time: 287.97765612602234 and batch: 500, loss is 5.533160905838013 and perplexity is 252.9421751380935
At time: 288.7707848548889 and batch: 550, loss is 5.539077568054199 and perplexity is 254.44318465295035
At time: 289.561532497406 and batch: 600, loss is 5.570413570404053 and perplexity is 262.5426566632604
At time: 290.37540650367737 and batch: 650, loss is 5.556962347030639 and perplexity is 259.0347822360751
At time: 291.1689417362213 and batch: 700, loss is 5.5826108932495115 and perplexity is 265.7645837044402
At time: 291.96037578582764 and batch: 750, loss is 5.548201732635498 and perplexity is 256.7753896778322
At time: 292.75376415252686 and batch: 800, loss is 5.557157020568848 and perplexity is 259.08521436239306
At time: 293.54861974716187 and batch: 850, loss is 5.574777240753174 and perplexity is 263.690809527937
At time: 294.3415424823761 and batch: 900, loss is 5.566580324172974 and perplexity is 261.53819242343667
At time: 295.13646817207336 and batch: 950, loss is 5.545547456741333 and perplexity is 256.0947406655363
At time: 295.92782330513 and batch: 1000, loss is 5.54599817276001 and perplexity is 256.21019268353564
At time: 296.72434258461 and batch: 1050, loss is 5.55025387763977 and perplexity is 257.30287106000617
At time: 297.5692403316498 and batch: 1100, loss is 5.523792428970337 and perplexity is 250.58355779841827
At time: 298.3630197048187 and batch: 1150, loss is 5.552287263870239 and perplexity is 257.82659946569083
At time: 299.15569615364075 and batch: 1200, loss is 5.563076686859131 and perplexity is 260.6234608330632
At time: 299.9508135318756 and batch: 1250, loss is 5.554066505432129 and perplexity is 258.2857436102672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.335851516166743 and perplexity of 207.64949044620965
Finished 14 epochs...
Completing Train Step...
At time: 302.1368215084076 and batch: 50, loss is 5.543478164672852 and perplexity is 255.5653537668542
At time: 302.98034834861755 and batch: 100, loss is 5.560809030532837 and perplexity is 260.03312598457273
At time: 303.78417444229126 and batch: 150, loss is 5.479113063812256 and perplexity is 239.63407296049107
At time: 304.5998911857605 and batch: 200, loss is 5.521644735336304 and perplexity is 250.04595859238117
At time: 305.4382474422455 and batch: 250, loss is 5.533086185455322 and perplexity is 252.92327590805613
At time: 306.2664384841919 and batch: 300, loss is 5.539008378982544 and perplexity is 254.4255805742272
At time: 307.0676600933075 and batch: 350, loss is 5.562993831634522 and perplexity is 260.6018677122412
At time: 307.8654091358185 and batch: 400, loss is 5.545662183761596 and perplexity is 256.12412333749893
At time: 308.66923213005066 and batch: 450, loss is 5.525120573043823 and perplexity is 250.91658997402175
At time: 309.4879219532013 and batch: 500, loss is 5.5179911136627195 and perplexity is 249.13405215365623
At time: 310.29092836380005 and batch: 550, loss is 5.526381978988647 and perplexity is 251.2332973585648
At time: 311.0895025730133 and batch: 600, loss is 5.554478425979614 and perplexity is 258.3921587309655
At time: 311.9050784111023 and batch: 650, loss is 5.540719614028931 and perplexity is 254.86133527742712
At time: 312.7000548839569 and batch: 700, loss is 5.5686156272888185 and perplexity is 262.0710439946771
At time: 313.49600625038147 and batch: 750, loss is 5.535306329727173 and perplexity is 253.48542586638112
At time: 314.29134130477905 and batch: 800, loss is 5.542652416229248 and perplexity is 255.35440817972358
At time: 315.088659286499 and batch: 850, loss is 5.559804611206054 and perplexity is 259.7720748115989
At time: 315.8843162059784 and batch: 900, loss is 5.549703245162964 and perplexity is 257.1612307422832
At time: 316.67979431152344 and batch: 950, loss is 5.527593545913696 and perplexity is 251.53786777858258
At time: 317.4784200191498 and batch: 1000, loss is 5.529962682723999 and perplexity is 252.1345018751562
At time: 318.3445963859558 and batch: 1050, loss is 5.5286413764953615 and perplexity is 251.80157498499756
At time: 319.14258432388306 and batch: 1100, loss is 5.5041365146636965 and perplexity is 245.70620035128655
At time: 319.9434790611267 and batch: 1150, loss is 5.532821416854858 and perplexity is 252.8563186307527
At time: 320.74097323417664 and batch: 1200, loss is 5.545111961364746 and perplexity is 255.98323687146402
At time: 321.5362317562103 and batch: 1250, loss is 5.537043685913086 and perplexity is 253.92620312172406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.322791329265511 and perplexity of 204.95518167599514
Finished 15 epochs...
Completing Train Step...
At time: 323.75124311447144 and batch: 50, loss is 5.519488554000855 and perplexity is 249.5073949925046
At time: 324.55796933174133 and batch: 100, loss is 5.539571008682251 and perplexity is 254.56876823926277
At time: 325.36550641059875 and batch: 150, loss is 5.459184322357178 and perplexity is 234.90573883972542
At time: 326.1974482536316 and batch: 200, loss is 5.501332998275757 and perplexity is 245.01832367928696
At time: 326.9938235282898 and batch: 250, loss is 5.5117582321167 and perplexity is 247.58605835995107
At time: 327.7896852493286 and batch: 300, loss is 5.517915058135986 and perplexity is 249.11510485262508
At time: 328.5858099460602 and batch: 350, loss is 5.541922588348388 and perplexity is 255.16811140369344
At time: 329.40254974365234 and batch: 400, loss is 5.525232229232788 and perplexity is 250.94460792836622
At time: 330.2075004577637 and batch: 450, loss is 5.502475519180297 and perplexity is 245.29842221483318
At time: 331.00158190727234 and batch: 500, loss is 5.495836820602417 and perplexity is 243.675353421394
At time: 331.79871368408203 and batch: 550, loss is 5.505516166687012 and perplexity is 246.04542335878912
At time: 332.5956447124481 and batch: 600, loss is 5.5341955661773685 and perplexity is 253.20401981164935
At time: 333.4118149280548 and batch: 650, loss is 5.521773052215576 and perplexity is 250.0780457680815
At time: 334.2078227996826 and batch: 700, loss is 5.54524899482727 and perplexity is 256.01831754431873
At time: 335.00284337997437 and batch: 750, loss is 5.516283845901489 and perplexity is 248.70907649503548
At time: 335.7984952926636 and batch: 800, loss is 5.523080205917358 and perplexity is 250.40514995248878
At time: 336.6227753162384 and batch: 850, loss is 5.540168886184692 and perplexity is 254.72101468644487
At time: 337.4204103946686 and batch: 900, loss is 5.531113271713257 and perplexity is 252.4247720156164
At time: 338.2748785018921 and batch: 950, loss is 5.508717088699341 and perplexity is 246.83425739504028
At time: 339.0858705043793 and batch: 1000, loss is 5.50939341545105 and perplexity is 247.00125447248453
At time: 339.8894987106323 and batch: 1050, loss is 5.511197729110718 and perplexity is 247.44732451400205
At time: 340.68321347236633 and batch: 1100, loss is 5.48600697517395 and perplexity is 241.2917965530662
At time: 341.47905349731445 and batch: 1150, loss is 5.51532506942749 and perplexity is 248.47073436029592
At time: 342.27615785598755 and batch: 1200, loss is 5.525816583633423 and perplexity is 251.0912913677074
At time: 343.0735614299774 and batch: 1250, loss is 5.519274663925171 and perplexity is 249.45403354385155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.312365900861086 and perplexity of 202.82953574099508
Finished 16 epochs...
Completing Train Step...
At time: 345.32944345474243 and batch: 50, loss is 5.498933687210083 and perplexity is 244.43115318799894
At time: 346.16494488716125 and batch: 100, loss is 5.522263278961182 and perplexity is 250.20067076917758
At time: 346.96193647384644 and batch: 150, loss is 5.443188629150391 and perplexity is 231.17815086429275
At time: 347.7597086429596 and batch: 200, loss is 5.484606666564941 and perplexity is 240.95415003290748
At time: 348.55682706832886 and batch: 250, loss is 5.492301321029663 and perplexity is 242.81536046166121
At time: 349.35082364082336 and batch: 300, loss is 5.489815301895142 and perplexity is 242.2124665428121
At time: 350.1686315536499 and batch: 350, loss is 5.51166708946228 and perplexity is 247.56349373771036
At time: 350.9883041381836 and batch: 400, loss is 5.4925631904602055 and perplexity is 242.8789547081635
At time: 351.7900586128235 and batch: 450, loss is 5.471341199874878 and perplexity is 237.7788880112285
At time: 352.58782935142517 and batch: 500, loss is 5.465282526016235 and perplexity is 236.34261861570698
At time: 353.3891866207123 and batch: 550, loss is 5.471052017211914 and perplexity is 237.71013642055988
At time: 354.18719720840454 and batch: 600, loss is 5.4990354251861575 and perplexity is 244.4560223838633
At time: 354.99080657958984 and batch: 650, loss is 5.487305488586426 and perplexity is 241.60532070081243
At time: 355.8090708255768 and batch: 700, loss is 5.507674961090088 and perplexity is 246.57715858864978
At time: 356.6198363304138 and batch: 750, loss is 5.48032977104187 and perplexity is 239.92581491580637
At time: 357.41591787338257 and batch: 800, loss is 5.489380350112915 and perplexity is 242.10713870673732
At time: 358.21124053001404 and batch: 850, loss is 5.508070077896118 and perplexity is 246.67460461800545
At time: 359.05581307411194 and batch: 900, loss is 5.499299249649048 and perplexity is 244.52052437089594
At time: 359.85250425338745 and batch: 950, loss is 5.477395229339599 and perplexity is 239.22277466147833
At time: 360.64906334877014 and batch: 1000, loss is 5.4763685321807865 and perplexity is 238.97729135847874
At time: 361.44465136528015 and batch: 1050, loss is 5.477991800308228 and perplexity is 239.36553060166705
At time: 362.2419412136078 and batch: 1100, loss is 5.454523210525513 and perplexity is 233.81336473775843
At time: 363.0372405052185 and batch: 1150, loss is 5.4807861804962155 and perplexity is 240.03534431930186
At time: 363.83455181121826 and batch: 1200, loss is 5.491669197082519 and perplexity is 242.6619195595228
At time: 364.6310238838196 and batch: 1250, loss is 5.483534030914306 and perplexity is 240.69583258643206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.285976437756615 and perplexity of 197.54698163669892
Finished 17 epochs...
Completing Train Step...
At time: 366.8458800315857 and batch: 50, loss is 5.462408323287963 and perplexity is 235.66429729993303
At time: 367.670547246933 and batch: 100, loss is 5.487542695999146 and perplexity is 241.66263807161914
At time: 368.46543860435486 and batch: 150, loss is 5.410242938995362 and perplexity is 223.6859231136188
At time: 369.26166558265686 and batch: 200, loss is 5.453159074783326 and perplexity is 233.494629018739
At time: 370.05714416503906 and batch: 250, loss is 5.463434715270996 and perplexity is 235.9063054217207
At time: 370.861403465271 and batch: 300, loss is 5.464741296768189 and perplexity is 236.21473768752324
At time: 371.6747920513153 and batch: 350, loss is 5.4886288070678715 and perplexity is 241.92525312646924
At time: 372.47089862823486 and batch: 400, loss is 5.470009756088257 and perplexity is 237.4625094550658
At time: 373.26447772979736 and batch: 450, loss is 5.448259153366089 and perplexity is 232.3533221257354
At time: 374.0604598522186 and batch: 500, loss is 5.444502115249634 and perplexity is 231.4819996588012
At time: 374.85634422302246 and batch: 550, loss is 5.4535290813446045 and perplexity is 233.581039548745
At time: 375.64977836608887 and batch: 600, loss is 5.4801533412933345 and perplexity is 239.88348859853423
At time: 376.44525718688965 and batch: 650, loss is 5.467251567840576 and perplexity is 236.8084455825201
At time: 377.24017238616943 and batch: 700, loss is 5.4913802814483645 and perplexity is 242.59182086394023
At time: 378.03483390808105 and batch: 750, loss is 5.46546407699585 and perplexity is 236.38553074489298
At time: 378.835675239563 and batch: 800, loss is 5.472694158554077 and perplexity is 238.10081074646635
At time: 379.6808259487152 and batch: 850, loss is 5.493416271209717 and perplexity is 243.08623847120958
At time: 380.477499961853 and batch: 900, loss is 5.484922714233399 and perplexity is 241.030315065487
At time: 381.27281188964844 and batch: 950, loss is 5.460092840194702 and perplexity is 235.1192518691097
At time: 382.0769441127777 and batch: 1000, loss is 5.459627103805542 and perplexity is 235.0097737736676
At time: 382.91656470298767 and batch: 1050, loss is 5.463870601654053 and perplexity is 236.00915618192582
At time: 383.7307834625244 and batch: 1100, loss is 5.437451438903809 and perplexity is 229.85563522144184
At time: 384.52851939201355 and batch: 1150, loss is 5.466746158599854 and perplexity is 236.68879064574395
At time: 385.32413935661316 and batch: 1200, loss is 5.477421531677246 and perplexity is 239.22906686241964
At time: 386.1265389919281 and batch: 1250, loss is 5.47237491607666 and perplexity is 238.02481098559008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.279394887659672 and perplexity of 196.25108546119333
Finished 18 epochs...
Completing Train Step...
At time: 388.37449979782104 and batch: 50, loss is 5.448881950378418 and perplexity is 232.49807615206868
At time: 389.17288041114807 and batch: 100, loss is 5.473047876358033 and perplexity is 238.1850461392644
At time: 389.97167205810547 and batch: 150, loss is 5.396560974121094 and perplexity is 220.64630158517767
At time: 390.76765275001526 and batch: 200, loss is 5.440673341751099 and perplexity is 230.59740205569628
At time: 391.5645561218262 and batch: 250, loss is 5.452910041809082 and perplexity is 233.43648839657098
At time: 392.3604702949524 and batch: 300, loss is 5.453478689193726 and perplexity is 233.56926919432686
At time: 393.1573021411896 and batch: 350, loss is 5.47544997215271 and perplexity is 238.757877159003
At time: 393.95113134384155 and batch: 400, loss is 5.459712362289428 and perplexity is 235.02981120484637
At time: 394.77335691452026 and batch: 450, loss is 5.437493944168091 and perplexity is 229.8654055036065
At time: 395.5703115463257 and batch: 500, loss is 5.434325342178345 and perplexity is 229.1382062333069
At time: 396.3652081489563 and batch: 550, loss is 5.442052507400513 and perplexity is 230.91565348172753
At time: 397.1597213745117 and batch: 600, loss is 5.4684819412231445 and perplexity is 237.09998770678845
At time: 397.9638683795929 and batch: 650, loss is 5.455456809997559 and perplexity is 234.03175470014918
At time: 398.7612943649292 and batch: 700, loss is 5.479575405120849 and perplexity is 239.74489130734295
At time: 399.557000875473 and batch: 750, loss is 5.452845239639283 and perplexity is 233.42136169573942
At time: 400.4014251232147 and batch: 800, loss is 5.463057069778443 and perplexity is 235.81723328871672
At time: 401.19760751724243 and batch: 850, loss is 5.481788110733032 and perplexity is 240.2759635103726
At time: 401.9946858882904 and batch: 900, loss is 5.471710605621338 and perplexity is 237.86674112457004
At time: 402.7910797595978 and batch: 950, loss is 5.446617097854614 and perplexity is 231.97209815365088
At time: 403.59627652168274 and batch: 1000, loss is 5.448177080154419 and perplexity is 232.33425292489244
At time: 404.41534900665283 and batch: 1050, loss is 5.44923698425293 and perplexity is 232.58063549967684
At time: 405.212527513504 and batch: 1100, loss is 5.423594388961792 and perplexity is 226.69248081403256
At time: 406.0085029602051 and batch: 1150, loss is 5.452955560684204 and perplexity is 233.44711440477548
At time: 406.8060426712036 and batch: 1200, loss is 5.462333354949951 and perplexity is 235.64663060146543
At time: 407.6012456417084 and batch: 1250, loss is 5.459546670913697 and perplexity is 234.99087201812264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.27445995024521 and perplexity of 195.28498442167478
Finished 19 epochs...
Completing Train Step...
At time: 409.8136463165283 and batch: 50, loss is 5.43681342124939 and perplexity is 229.70903004152262
At time: 410.6641230583191 and batch: 100, loss is 5.461131429672241 and perplexity is 235.36357110179833
At time: 411.4663348197937 and batch: 150, loss is 5.3814749336242675 and perplexity is 217.34260505058342
At time: 412.26105976104736 and batch: 200, loss is 5.425330238342285 and perplexity is 227.08632654595715
At time: 413.05668783187866 and batch: 250, loss is 5.439272384643555 and perplexity is 230.27457117521467
At time: 413.85229563713074 and batch: 300, loss is 5.438889951705932 and perplexity is 230.18652343175202
At time: 414.6484065055847 and batch: 350, loss is 5.463009958267212 and perplexity is 235.80612384417563
At time: 415.45367670059204 and batch: 400, loss is 5.446022386550903 and perplexity is 231.83418273872638
At time: 416.2507469654083 and batch: 450, loss is 5.42099663734436 and perplexity is 226.104354289392
At time: 417.0520634651184 and batch: 500, loss is 5.418086900711059 and perplexity is 225.44740640294074
At time: 417.8518486022949 and batch: 550, loss is 5.42415418624878 and perplexity is 226.81941817605326
At time: 418.6496858596802 and batch: 600, loss is 5.452261877059937 and perplexity is 233.28523211841738
At time: 419.44670248031616 and batch: 650, loss is 5.440674114227295 and perplexity is 230.59758018676902
At time: 420.2952370643616 and batch: 700, loss is 5.466004285812378 and perplexity is 236.51326279058344
At time: 421.0944483280182 and batch: 750, loss is 5.441177043914795 and perplexity is 230.7135837240769
At time: 421.89294505119324 and batch: 800, loss is 5.453426294326782 and perplexity is 233.55703168413945
At time: 422.7030565738678 and batch: 850, loss is 5.4697946929931645 and perplexity is 237.4114455239943
At time: 423.5080416202545 and batch: 900, loss is 5.459657192230225 and perplexity is 235.01684495392524
At time: 424.31827783584595 and batch: 950, loss is 5.434096689224243 and perplexity is 229.0858190950194
At time: 425.1168303489685 and batch: 1000, loss is 5.4371750831604 and perplexity is 229.7921220730095
At time: 425.92193818092346 and batch: 1050, loss is 5.440284214019775 and perplexity is 230.50768766809537
At time: 426.7222752571106 and batch: 1100, loss is 5.414458017349244 and perplexity is 224.63076670217183
At time: 427.5175635814667 and batch: 1150, loss is 5.4436425113677975 and perplexity is 231.2831023320238
At time: 428.3142261505127 and batch: 1200, loss is 5.453400268554687 and perplexity is 233.5509532611599
At time: 429.1091995239258 and batch: 1250, loss is 5.452082853317261 and perplexity is 233.2434722611671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.269865384067062 and perplexity of 194.38979272070065
Finished 20 epochs...
Completing Train Step...
At time: 431.3358714580536 and batch: 50, loss is 5.428416528701782 and perplexity is 227.78826351983386
At time: 432.1438422203064 and batch: 100, loss is 5.447866764068603 and perplexity is 232.2621670542049
At time: 432.9435405731201 and batch: 150, loss is 5.371307773590088 and perplexity is 215.1440435052791
At time: 433.74471497535706 and batch: 200, loss is 5.417760190963745 and perplexity is 225.37376256849646
At time: 434.5640678405762 and batch: 250, loss is 5.432361259460449 and perplexity is 228.68860151739435
At time: 435.38344621658325 and batch: 300, loss is 5.429708395004273 and perplexity is 228.08272566348552
At time: 436.183913230896 and batch: 350, loss is 5.451800184249878 and perplexity is 233.17755086379873
At time: 436.9796521663666 and batch: 400, loss is 5.438047752380371 and perplexity is 229.99274210966806
At time: 437.7761776447296 and batch: 450, loss is 5.4131150054931645 and perplexity is 224.32928740969712
At time: 438.57253885269165 and batch: 500, loss is 5.410800857543945 and perplexity is 223.81075645936235
At time: 439.3691086769104 and batch: 550, loss is 5.415776767730713 and perplexity is 224.92719402531299
At time: 440.16378831863403 and batch: 600, loss is 5.443753309249878 and perplexity is 231.30872942961108
At time: 440.98678636550903 and batch: 650, loss is 5.428673477172851 and perplexity is 227.84680088609406
At time: 441.7840750217438 and batch: 700, loss is 5.457781639099121 and perplexity is 234.57647147552973
At time: 442.57703828811646 and batch: 750, loss is 5.432920961380005 and perplexity is 228.8166347935371
At time: 443.369389295578 and batch: 800, loss is 5.444049501419068 and perplexity is 231.37725141126953
At time: 444.16640996932983 and batch: 850, loss is 5.462408285140992 and perplexity is 235.66428831005388
At time: 444.96025228500366 and batch: 900, loss is 5.4519562339782714 and perplexity is 233.2139409965398
At time: 445.75551557540894 and batch: 950, loss is 5.424678573608398 and perplexity is 226.93839060295295
At time: 446.5695734024048 and batch: 1000, loss is 5.426881999969482 and perplexity is 227.4389839428948
At time: 447.37470293045044 and batch: 1050, loss is 5.428967514038086 and perplexity is 227.9138060956985
At time: 448.1699914932251 and batch: 1100, loss is 5.404419145584106 and perplexity is 222.38700848485286
At time: 448.9640591144562 and batch: 1150, loss is 5.435369100570679 and perplexity is 229.37749601770344
At time: 449.75841331481934 and batch: 1200, loss is 5.446661949157715 and perplexity is 231.9825026378617
At time: 450.55515456199646 and batch: 1250, loss is 5.441602878570556 and perplexity is 230.81185048479267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.268392075587363 and perplexity of 194.1036074620696
Finished 21 epochs...
Completing Train Step...
At time: 452.90435218811035 and batch: 50, loss is 5.420134162902832 and perplexity is 225.9094291337646
At time: 453.7270760536194 and batch: 100, loss is 5.440249977111816 and perplexity is 230.49979593270385
At time: 454.52334785461426 and batch: 150, loss is 5.364102582931519 and perplexity is 213.5994608412748
At time: 455.3194100856781 and batch: 200, loss is 5.40864854812622 and perplexity is 223.32956448321528
At time: 456.1176438331604 and batch: 250, loss is 5.423675203323365 and perplexity is 226.71080156242263
At time: 456.9151756763458 and batch: 300, loss is 5.422898406982422 and perplexity is 226.5347618236813
At time: 457.71019220352173 and batch: 350, loss is 5.44395694732666 and perplexity is 231.3558374907496
At time: 458.5046606063843 and batch: 400, loss is 5.429539785385132 and perplexity is 228.0442719639018
At time: 459.30037093162537 and batch: 450, loss is 5.402945775985717 and perplexity is 222.05959148982802
At time: 460.0949921607971 and batch: 500, loss is 5.401631603240967 and perplexity is 221.76795849701898
At time: 460.89186000823975 and batch: 550, loss is 5.404404907226563 and perplexity is 222.38384208165522
At time: 461.7419185638428 and batch: 600, loss is 5.43265658378601 and perplexity is 228.75614879808768
At time: 462.5429947376251 and batch: 650, loss is 5.419856882095337 and perplexity is 225.8467974685143
At time: 463.3464424610138 and batch: 700, loss is 5.448104286193848 and perplexity is 232.31734100999574
At time: 464.1469657421112 and batch: 750, loss is 5.4261754035949705 and perplexity is 227.2783331457377
At time: 464.95357275009155 and batch: 800, loss is 5.43515869140625 and perplexity is 229.32923796757368
At time: 465.7528157234192 and batch: 850, loss is 5.453770141601563 and perplexity is 233.63735344140727
At time: 466.55106115341187 and batch: 900, loss is 5.444754734039306 and perplexity is 231.5404837481649
At time: 467.3498296737671 and batch: 950, loss is 5.415180864334107 and perplexity is 224.79319907439256
At time: 468.1462163925171 and batch: 1000, loss is 5.419640045166016 and perplexity is 225.79783085153164
At time: 468.94242000579834 and batch: 1050, loss is 5.421131553649903 and perplexity is 226.1348615114544
At time: 469.7440416812897 and batch: 1100, loss is 5.395489120483399 and perplexity is 220.40992774588958
At time: 470.57808208465576 and batch: 1150, loss is 5.428349552154541 and perplexity is 227.77300755934263
At time: 471.3857011795044 and batch: 1200, loss is 5.43831992149353 and perplexity is 230.05534754956807
At time: 472.184002161026 and batch: 1250, loss is 5.434207601547241 and perplexity is 229.11122894448775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.266394844890511 and perplexity of 193.7163246541698
Finished 22 epochs...
Completing Train Step...
At time: 474.37912249565125 and batch: 50, loss is 5.41175313949585 and perplexity is 224.02398891597667
At time: 475.2090644836426 and batch: 100, loss is 5.429317378997803 and perplexity is 227.9935591008646
At time: 476.0107319355011 and batch: 150, loss is 5.355506286621094 and perplexity is 211.7711661243731
At time: 476.81251215934753 and batch: 200, loss is 5.402300024032593 and perplexity is 221.91624236388387
At time: 477.61444211006165 and batch: 250, loss is 5.416621961593628 and perplexity is 225.11738147061493
At time: 478.40915608406067 and batch: 300, loss is 5.414108667373657 and perplexity is 224.5523056552889
At time: 479.2053518295288 and batch: 350, loss is 5.436439905166626 and perplexity is 229.6232460462943
At time: 480.00583243370056 and batch: 400, loss is 5.420760879516601 and perplexity is 226.0510547011224
At time: 480.80918431282043 and batch: 450, loss is 5.3933587074279785 and perplexity is 219.94086338610373
At time: 481.6053292751312 and batch: 500, loss is 5.391973600387574 and perplexity is 219.6364326310174
At time: 482.45550322532654 and batch: 550, loss is 5.396851167678833 and perplexity is 220.71034101190182
At time: 483.2502443790436 and batch: 600, loss is 5.426832675933838 and perplexity is 227.42776601100303
At time: 484.04696321487427 and batch: 650, loss is 5.4132304382324214 and perplexity is 224.35518384845804
At time: 484.8430998325348 and batch: 700, loss is 5.439025650024414 and perplexity is 230.21776147534587
At time: 485.63848066329956 and batch: 750, loss is 5.416789503097534 and perplexity is 225.1551011349783
At time: 486.4611825942993 and batch: 800, loss is 5.428531646728516 and perplexity is 227.81448756464448
At time: 487.26232051849365 and batch: 850, loss is 5.44667103767395 and perplexity is 231.98461102418435
At time: 488.0657539367676 and batch: 900, loss is 5.437036151885986 and perplexity is 229.76019897824912
At time: 488.8615095615387 and batch: 950, loss is 5.408728828430176 and perplexity is 223.34749416822496
At time: 489.6549985408783 and batch: 1000, loss is 5.412745094299316 and perplexity is 224.24632084125346
At time: 490.4505729675293 and batch: 1050, loss is 5.413827180862427 and perplexity is 224.48910610550888
At time: 491.24502897262573 and batch: 1100, loss is 5.388687353134156 and perplexity is 218.91583768261881
At time: 492.0454170703888 and batch: 1150, loss is 5.42122519493103 and perplexity is 226.15603806107808
At time: 492.858425617218 and batch: 1200, loss is 5.431328687667847 and perplexity is 228.45258599063573
At time: 493.6542224884033 and batch: 1250, loss is 5.4271963024139405 and perplexity is 227.51047980658697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.266009476933166 and perplexity of 193.6416869722426
Finished 23 epochs...
Completing Train Step...
At time: 495.8898901939392 and batch: 50, loss is 5.405606279373169 and perplexity is 222.65116838242068
At time: 496.6909120082855 and batch: 100, loss is 5.421648321151733 and perplexity is 226.25175085862114
At time: 497.5096278190613 and batch: 150, loss is 5.348701848983764 and perplexity is 210.3350738721439
At time: 498.30652689933777 and batch: 200, loss is 5.395653190612793 and perplexity is 220.44609339802517
At time: 499.1031587123871 and batch: 250, loss is 5.410075569152832 and perplexity is 223.6484879687358
At time: 499.8990590572357 and batch: 300, loss is 5.408238668441772 and perplexity is 223.2380449890689
At time: 500.69574189186096 and batch: 350, loss is 5.430316667556763 and perplexity is 228.2215043285513
At time: 501.49537563323975 and batch: 400, loss is 5.414851970672608 and perplexity is 224.71927817279322
At time: 502.31906914711 and batch: 450, loss is 5.388189277648926 and perplexity is 218.80682822026452
At time: 503.1162419319153 and batch: 500, loss is 5.3871503353118895 and perplexity is 218.57961859208245
At time: 503.91754388809204 and batch: 550, loss is 5.3892128276824955 and perplexity is 219.03090261274792
At time: 504.7211401462555 and batch: 600, loss is 5.419456901550293 and perplexity is 225.75648120694444
At time: 505.51604676246643 and batch: 650, loss is 5.405934743881225 and perplexity is 222.72431340102528
At time: 506.3109884262085 and batch: 700, loss is 5.431603832244873 and perplexity is 228.51545212902056
At time: 507.10571098327637 and batch: 750, loss is 5.410006055831909 and perplexity is 223.63294195995118
At time: 507.90184259414673 and batch: 800, loss is 5.420438814163208 and perplexity is 225.97826321074285
At time: 508.69701313972473 and batch: 850, loss is 5.439767332077026 and perplexity is 230.38857319338734
At time: 509.49154448509216 and batch: 900, loss is 5.428567142486572 and perplexity is 227.8225741560958
At time: 510.28498220443726 and batch: 950, loss is 5.400188474655152 and perplexity is 221.44814963485297
At time: 511.0815074443817 and batch: 1000, loss is 5.404552812576294 and perplexity is 222.41673627414644
At time: 511.87841176986694 and batch: 1050, loss is 5.405151300430298 and perplexity is 222.5498898307523
At time: 512.6726267337799 and batch: 1100, loss is 5.380668001174927 and perplexity is 217.1672949911443
At time: 513.4688045978546 and batch: 1150, loss is 5.413382606506348 and perplexity is 224.38932618715518
At time: 514.2664787769318 and batch: 1200, loss is 5.423262090682983 and perplexity is 226.617163807388
At time: 515.063622713089 and batch: 1250, loss is 5.4176515769958495 and perplexity is 225.34928515920276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.263571634779882 and perplexity of 193.17019405127718
Finished 24 epochs...
Completing Train Step...
At time: 517.2770886421204 and batch: 50, loss is 5.395451040267944 and perplexity is 220.40153464815916
At time: 518.1002240180969 and batch: 100, loss is 5.4133065319061275 and perplexity is 224.37225650816455
At time: 518.9005038738251 and batch: 150, loss is 5.338485116958618 and perplexity is 208.1970770544455
At time: 519.7018322944641 and batch: 200, loss is 5.387947912216187 and perplexity is 218.7540221885166
At time: 520.504332780838 and batch: 250, loss is 5.402732839584351 and perplexity is 222.01231195347307
At time: 521.3049285411835 and batch: 300, loss is 5.4003771209716795 and perplexity is 221.48992895321686
At time: 522.1072087287903 and batch: 350, loss is 5.417611198425293 and perplexity is 225.34018606089762
At time: 522.9362404346466 and batch: 400, loss is 5.4068521213531495 and perplexity is 222.92872941757736
At time: 523.731082201004 and batch: 450, loss is 5.378995771408081 and perplexity is 216.80444484503712
At time: 524.5269174575806 and batch: 500, loss is 5.378968086242676 and perplexity is 216.79844266120705
At time: 525.322606086731 and batch: 550, loss is 5.38287917137146 and perplexity is 217.64802012817196
At time: 526.1171970367432 and batch: 600, loss is 5.4117359447479245 and perplexity is 224.02013691307528
At time: 526.9133868217468 and batch: 650, loss is 5.397636222839355 and perplexity is 220.88367883506757
At time: 527.7106781005859 and batch: 700, loss is 5.423538703918457 and perplexity is 226.6798577848748
At time: 528.5044288635254 and batch: 750, loss is 5.401805982589722 and perplexity is 221.80663362117
At time: 529.3006036281586 and batch: 800, loss is 5.413708219528198 and perplexity is 224.46240217032613
At time: 530.0985336303711 and batch: 850, loss is 5.433504781723022 and perplexity is 228.95026160297968
At time: 530.9098052978516 and batch: 900, loss is 5.421793146133423 and perplexity is 226.2845201371443
At time: 531.7169055938721 and batch: 950, loss is 5.39378396987915 and perplexity is 220.03441586754906
At time: 532.5110032558441 and batch: 1000, loss is 5.398352575302124 and perplexity is 221.0419660903502
At time: 533.3088788986206 and batch: 1050, loss is 5.398662977218628 and perplexity is 221.11058858997905
At time: 534.1044523715973 and batch: 1100, loss is 5.373106775283813 and perplexity is 215.5314363596605
At time: 534.9008963108063 and batch: 1150, loss is 5.403679351806641 and perplexity is 222.22254880041288
At time: 535.6971497535706 and batch: 1200, loss is 5.417103424072265 and perplexity is 225.22579313906323
At time: 536.4916532039642 and batch: 1250, loss is 5.414518356323242 and perplexity is 224.64432110108828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.263274032704151 and perplexity of 193.1127147539616
Finished 25 epochs...
Completing Train Step...
At time: 538.6979126930237 and batch: 50, loss is 5.388947381973266 and perplexity is 218.97276951539274
At time: 539.4967923164368 and batch: 100, loss is 5.406339073181153 and perplexity is 222.81438557492194
At time: 540.2911684513092 and batch: 150, loss is 5.331714220046997 and perplexity is 206.79215775765118
At time: 541.0885608196259 and batch: 200, loss is 5.380874214172363 and perplexity is 217.21208232769627
At time: 541.8868808746338 and batch: 250, loss is 5.394681186676025 and perplexity is 220.23192303147903
At time: 542.6878671646118 and batch: 300, loss is 5.392941474914551 and perplexity is 219.84911604818487
At time: 543.5138382911682 and batch: 350, loss is 5.4079172706604 and perplexity is 223.16630830531466
At time: 544.311306476593 and batch: 400, loss is 5.397207374572754 and perplexity is 220.78897356082643
At time: 545.1054089069366 and batch: 450, loss is 5.370825634002686 and perplexity is 215.04033904693276
At time: 545.9160521030426 and batch: 500, loss is 5.372083520889282 and perplexity is 215.31100566781637
At time: 546.7215151786804 and batch: 550, loss is 5.375507669448853 and perplexity is 216.0495262181897
At time: 547.51700091362 and batch: 600, loss is 5.403891639709473 and perplexity is 222.26972896697174
At time: 548.31272315979 and batch: 650, loss is 5.389994230270386 and perplexity is 219.20212081333793
At time: 549.1094675064087 and batch: 700, loss is 5.419009408950806 and perplexity is 225.65547945277217
At time: 549.9058601856232 and batch: 750, loss is 5.393914003372192 and perplexity is 220.0630295715633
At time: 550.7009921073914 and batch: 800, loss is 5.407692337036133 and perplexity is 223.11611634391477
At time: 551.507631778717 and batch: 850, loss is 5.424779472351074 and perplexity is 226.961289556448
At time: 552.302059173584 and batch: 900, loss is 5.414636783599853 and perplexity is 224.67092669162514
At time: 553.0975739955902 and batch: 950, loss is 5.383196392059326 and perplexity is 217.71707353483524
At time: 553.8951950073242 and batch: 1000, loss is 5.390682535171509 and perplexity is 219.35305064434283
At time: 554.691351890564 and batch: 1050, loss is 5.390232849121094 and perplexity is 219.2544328125455
At time: 555.5269682407379 and batch: 1100, loss is 5.3681857204437256 and perplexity is 214.47339980471236
At time: 556.3498330116272 and batch: 1150, loss is 5.40220235824585 and perplexity is 221.8945697978336
At time: 557.1498000621796 and batch: 1200, loss is 5.412267065048217 and perplexity is 224.13915015784167
At time: 557.9678258895874 and batch: 1250, loss is 5.408055286407471 and perplexity is 223.19711089564882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.259209625912408 and perplexity of 192.32941901909388
Finished 26 epochs...
Completing Train Step...
At time: 560.1755588054657 and batch: 50, loss is 5.384876613616943 and perplexity is 218.08319395082333
At time: 561.0015046596527 and batch: 100, loss is 5.401113576889038 and perplexity is 221.65310660122688
At time: 561.7959399223328 and batch: 150, loss is 5.3269264507293705 and perplexity is 205.80445095252585
At time: 562.5873475074768 and batch: 200, loss is 5.375523853302002 and perplexity is 216.05302276028863
At time: 563.3826382160187 and batch: 250, loss is 5.392871341705322 and perplexity is 219.8336978648
At time: 564.2073774337769 and batch: 300, loss is 5.390023784637451 and perplexity is 219.20859928901118
At time: 565.0035402774811 and batch: 350, loss is 5.407943191528321 and perplexity is 223.17209304468895
At time: 565.7985482215881 and batch: 400, loss is 5.394820575714111 and perplexity is 220.26262308696215
At time: 566.5936331748962 and batch: 450, loss is 5.369813594818115 and perplexity is 214.8228198850772
At time: 567.3970618247986 and batch: 500, loss is 5.3684954166412355 and perplexity is 214.53983168741868
At time: 568.228898525238 and batch: 550, loss is 5.370216188430786 and perplexity is 214.90932359197032
At time: 569.0300717353821 and batch: 600, loss is 5.39987928390503 and perplexity is 221.37969049936532
At time: 569.8498411178589 and batch: 650, loss is 5.389789457321167 and perplexity is 219.1572387440577
At time: 570.6441059112549 and batch: 700, loss is 5.414278650283814 and perplexity is 224.5904789539988
At time: 571.4406926631927 and batch: 750, loss is 5.389336528778077 and perplexity is 219.0579986512375
At time: 572.2348544597626 and batch: 800, loss is 5.402439022064209 and perplexity is 221.94709042861658
At time: 573.0503730773926 and batch: 850, loss is 5.422000150680542 and perplexity is 226.33136691033573
At time: 573.8587129116058 and batch: 900, loss is 5.411844539642334 and perplexity is 224.0444656771548
At time: 574.6811270713806 and batch: 950, loss is 5.3838176441192624 and perplexity is 217.8523727383549
At time: 575.4899673461914 and batch: 1000, loss is 5.388761510848999 and perplexity is 218.93207258284852
At time: 576.3152410984039 and batch: 1050, loss is 5.390306520462036 and perplexity is 219.27058617563102
At time: 577.1114544868469 and batch: 1100, loss is 5.363754663467407 and perplexity is 213.52515835771405
At time: 577.906082868576 and batch: 1150, loss is 5.3963822174072265 and perplexity is 220.60686310243108
At time: 578.7025647163391 and batch: 1200, loss is 5.407419967651367 and perplexity is 223.0553546197671
At time: 579.498450756073 and batch: 1250, loss is 5.40167067527771 and perplexity is 221.77662359212232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.256186380873632 and perplexity of 191.74883811860667
Finished 27 epochs...
Completing Train Step...
At time: 581.7052316665649 and batch: 50, loss is 5.381335172653198 and perplexity is 217.31223115963854
At time: 582.5519556999207 and batch: 100, loss is 5.397088966369629 and perplexity is 220.76283188292234
At time: 583.3458275794983 and batch: 150, loss is 5.321878032684326 and perplexity is 204.76808226090748
At time: 584.1901969909668 and batch: 200, loss is 5.369970846176147 and perplexity is 214.85660372144784
At time: 584.98823595047 and batch: 250, loss is 5.385103960037231 and perplexity is 218.13278002068714
At time: 585.809360742569 and batch: 300, loss is 5.383398160934449 and perplexity is 217.7610064958603
At time: 586.6065011024475 and batch: 350, loss is 5.400718364715576 and perplexity is 221.5655239032266
At time: 587.4195382595062 and batch: 400, loss is 5.385660552978516 and perplexity is 218.25422498087806
At time: 588.237877368927 and batch: 450, loss is 5.362028675079346 and perplexity is 213.15693428048309
At time: 589.0661780834198 and batch: 500, loss is 5.363459615707398 and perplexity is 213.462167531143
At time: 589.8818471431732 and batch: 550, loss is 5.3628961372375485 and perplexity is 213.34192007721916
At time: 590.6863842010498 and batch: 600, loss is 5.3913838577270505 and perplexity is 219.50694184376354
At time: 591.4813089370728 and batch: 650, loss is 5.380760679244995 and perplexity is 217.18742256960417
At time: 592.2776479721069 and batch: 700, loss is 5.408636226654052 and perplexity is 223.32681275115496
At time: 593.0724651813507 and batch: 750, loss is 5.384219341278076 and perplexity is 217.93990099627393
At time: 593.8668208122253 and batch: 800, loss is 5.398522825241089 and perplexity is 221.07960167522276
At time: 594.663645029068 and batch: 850, loss is 5.4138508224487305 and perplexity is 224.49441344682185
At time: 595.4591467380524 and batch: 900, loss is 5.402292060852051 and perplexity is 221.9144752118167
At time: 596.2895216941833 and batch: 950, loss is 5.3759669494628906 and perplexity is 216.1487762376546
At time: 597.0856659412384 and batch: 1000, loss is 5.380072450637817 and perplexity is 217.03799939682153
At time: 597.8820471763611 and batch: 1050, loss is 5.381164464950562 and perplexity is 217.27513745408314
At time: 598.6781330108643 and batch: 1100, loss is 5.353945951461792 and perplexity is 211.44098978800469
At time: 599.4747519493103 and batch: 1150, loss is 5.386998271942138 and perplexity is 218.5463831657299
At time: 600.2698311805725 and batch: 1200, loss is 5.4012578582763675 and perplexity is 221.68508932615362
At time: 601.0667564868927 and batch: 1250, loss is 5.392800502777099 and perplexity is 219.8181256328226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.253707106096031 and perplexity of 191.27402889510418
Finished 28 epochs...
Completing Train Step...
At time: 603.3124566078186 and batch: 50, loss is 5.367877645492554 and perplexity is 214.40733609934875
At time: 604.1189613342285 and batch: 100, loss is 5.38387282371521 and perplexity is 217.86439407592204
At time: 604.945515871048 and batch: 150, loss is 5.314100732803345 and perplexity is 203.18171629714314
At time: 605.7414407730103 and batch: 200, loss is 5.363232879638672 and perplexity is 213.4137734450051
At time: 606.5450372695923 and batch: 250, loss is 5.37638412475586 and perplexity is 216.2389669780727
At time: 607.3358442783356 and batch: 300, loss is 5.373777618408203 and perplexity is 215.6760726505387
At time: 608.1272006034851 and batch: 350, loss is 5.389088621139527 and perplexity is 219.003699230985
At time: 608.917882680893 and batch: 400, loss is 5.376166982650757 and perplexity is 216.1920174911175
At time: 609.7059710025787 and batch: 450, loss is 5.352554426193238 and perplexity is 211.14696892408102
At time: 610.5081510543823 and batch: 500, loss is 5.35332106590271 and perplexity is 211.30890464023688
At time: 611.3159666061401 and batch: 550, loss is 5.352667360305786 and perplexity is 211.17081596618075
At time: 612.1310994625092 and batch: 600, loss is 5.383072938919067 and perplexity is 217.690197337429
At time: 612.920553445816 and batch: 650, loss is 5.3726802158355715 and perplexity is 215.4395189945922
At time: 613.7112994194031 and batch: 700, loss is 5.39788589477539 and perplexity is 220.9388341758847
At time: 614.4998002052307 and batch: 750, loss is 5.374290342330933 and perplexity is 215.7866832864834
At time: 615.2918167114258 and batch: 800, loss is 5.3879218006134035 and perplexity is 218.74831024495631
At time: 616.0823602676392 and batch: 850, loss is 5.405862474441529 and perplexity is 222.70821782130525
At time: 616.8735210895538 and batch: 900, loss is 5.393408365249634 and perplexity is 219.95178544145304
At time: 617.6679730415344 and batch: 950, loss is 5.3643584728240965 and perplexity is 213.65412577816824
At time: 618.4657964706421 and batch: 1000, loss is 5.370643215179443 and perplexity is 215.0011152190252
At time: 619.2567703723907 and batch: 1050, loss is 5.368816947937011 and perplexity is 214.60882404850392
At time: 620.0501828193665 and batch: 1100, loss is 5.343673181533814 and perplexity is 209.28002370409632
At time: 620.8425762653351 and batch: 1150, loss is 5.380672283172608 and perplexity is 217.16822490298875
At time: 621.6349530220032 and batch: 1200, loss is 5.39197883605957 and perplexity is 219.6375825783475
At time: 622.4265308380127 and batch: 1250, loss is 5.3840208339691165 and perplexity is 217.8966426267056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.250531941434763 and perplexity of 190.6676655191492
Finished 29 epochs...
Completing Train Step...
At time: 624.593231678009 and batch: 50, loss is 5.362412853240967 and perplexity is 213.2388402518671
At time: 625.4150562286377 and batch: 100, loss is 5.37541579246521 and perplexity is 216.02967715125354
At time: 626.2088270187378 and batch: 150, loss is 5.302970304489135 and perplexity is 200.9327559331218
At time: 627.0088415145874 and batch: 200, loss is 5.3486032295227055 and perplexity is 210.31433176332146
At time: 627.8002519607544 and batch: 250, loss is 5.363538284301757 and perplexity is 213.47896096036072
At time: 628.5929009914398 and batch: 300, loss is 5.360629692077636 and perplexity is 212.85893984590425
At time: 629.386280298233 and batch: 350, loss is 5.375497970581055 and perplexity is 216.04743079255877
At time: 630.1936187744141 and batch: 400, loss is 5.364216928482056 and perplexity is 213.62388638566858
At time: 630.9940657615662 and batch: 450, loss is 5.338609266281128 and perplexity is 208.22292618505307
At time: 631.7854316234589 and batch: 500, loss is 5.3382573986053465 and perplexity is 208.14967215659016
At time: 632.5935156345367 and batch: 550, loss is 5.339816589355468 and perplexity is 208.47447034542267
At time: 633.4093089103699 and batch: 600, loss is 5.367607755661011 and perplexity is 214.34947754759594
At time: 634.2200167179108 and batch: 650, loss is 5.35719596862793 and perplexity is 212.12929452969593
At time: 635.0173425674438 and batch: 700, loss is 5.381691541671753 and perplexity is 217.38968830702083
At time: 635.8120813369751 and batch: 750, loss is 5.3582128810882566 and perplexity is 212.3451211723055
At time: 636.6112039089203 and batch: 800, loss is 5.373287839889526 and perplexity is 215.57046500745048
At time: 637.4136548042297 and batch: 850, loss is 5.388290920257568 and perplexity is 218.82906944738266
At time: 638.2320718765259 and batch: 900, loss is 5.3772432899475096 and perplexity is 216.42483180443995
At time: 639.0352964401245 and batch: 950, loss is 5.348884706497192 and perplexity is 210.3735387374265
At time: 639.8556616306305 and batch: 1000, loss is 5.351794881820679 and perplexity is 210.9866543227857
At time: 640.6646265983582 and batch: 1050, loss is 5.3508830261230464 and perplexity is 210.79435262893932
At time: 641.4606046676636 and batch: 1100, loss is 5.3237768650054935 and perplexity is 205.15727189984332
At time: 642.2628891468048 and batch: 1150, loss is 5.361475162506103 and perplexity is 213.03898188436364
At time: 643.0597202777863 and batch: 1200, loss is 5.370516271591186 and perplexity is 214.97382393824319
At time: 643.8548953533173 and batch: 1250, loss is 5.367715864181519 and perplexity is 214.37265180513006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.238145368812728 and perplexity of 188.3205132293388
Finished 30 epochs...
Completing Train Step...
At time: 646.0791594982147 and batch: 50, loss is 5.342788858413696 and perplexity is 209.09503434779398
At time: 646.8749957084656 and batch: 100, loss is 5.359795751571656 and perplexity is 212.68150215043073
At time: 647.67129945755 and batch: 150, loss is 5.2864754772186275 and perplexity is 197.6455899788177
At time: 648.4672377109528 and batch: 200, loss is 5.333986396789551 and perplexity is 207.2625603055327
At time: 649.2647132873535 and batch: 250, loss is 5.349233255386353 and perplexity is 210.44687698090138
At time: 650.0620610713959 and batch: 300, loss is 5.344513549804687 and perplexity is 209.4559699151968
At time: 650.8573694229126 and batch: 350, loss is 5.357153749465942 and perplexity is 212.12033879770092
At time: 651.6522345542908 and batch: 400, loss is 5.34617247581482 and perplexity is 209.8037302462418
At time: 652.4490513801575 and batch: 450, loss is 5.3197089958190915 and perplexity is 204.32441408192278
At time: 653.2574136257172 and batch: 500, loss is 5.320800638198852 and perplexity is 204.54758506084121
At time: 654.0630927085876 and batch: 550, loss is 5.322832536697388 and perplexity is 204.96362752669648
At time: 654.8590178489685 and batch: 600, loss is 5.351080236434936 and perplexity is 210.8359275483325
At time: 655.6544682979584 and batch: 650, loss is 5.344457902908325 and perplexity is 209.44431466483877
At time: 656.451212644577 and batch: 700, loss is 5.370344524383545 and perplexity is 214.93690595463684
At time: 657.2506098747253 and batch: 750, loss is 5.343046531677246 and perplexity is 209.1489194897686
At time: 658.0691757202148 and batch: 800, loss is 5.36234073638916 and perplexity is 213.22346269252245
At time: 658.8902897834778 and batch: 850, loss is 5.3767124271392825 and perplexity is 216.30997040097924
At time: 659.705807685852 and batch: 900, loss is 5.362377891540527 and perplexity is 213.2313851897337
At time: 660.5168251991272 and batch: 950, loss is 5.335908489227295 and perplexity is 207.66132121021664
At time: 661.3128023147583 and batch: 1000, loss is 5.3405252456665036 and perplexity is 208.6222594541801
At time: 662.1104409694672 and batch: 1050, loss is 5.340870523452759 and perplexity is 208.6943045231533
At time: 662.9064660072327 and batch: 1100, loss is 5.312260332107544 and perplexity is 202.80812440992912
At time: 663.7032878398895 and batch: 1150, loss is 5.350401096343994 and perplexity is 210.69278902837883
At time: 664.5058453083038 and batch: 1200, loss is 5.3603287696838375 and perplexity is 212.79489546086245
At time: 665.3041703701019 and batch: 1250, loss is 5.358368692398071 and perplexity is 212.37820952147064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.231439520842837 and perplexity of 187.06188928494376
Finished 31 epochs...
Completing Train Step...
At time: 667.5718696117401 and batch: 50, loss is 5.333769083023071 and perplexity is 207.2175241915635
At time: 668.4055290222168 and batch: 100, loss is 5.349573469161987 and perplexity is 210.51848608800108
At time: 669.2022626399994 and batch: 150, loss is 5.275275688171387 and perplexity is 195.44435078163522
At time: 669.9967784881592 and batch: 200, loss is 5.323503065109253 and perplexity is 205.10110754933163
At time: 670.7976942062378 and batch: 250, loss is 5.337778472900391 and perplexity is 208.0500077959334
At time: 671.6237230300903 and batch: 300, loss is 5.333156099319458 and perplexity is 207.0905421490685
At time: 672.4367411136627 and batch: 350, loss is 5.347435140609742 and perplexity is 210.06880934807342
At time: 673.2327890396118 and batch: 400, loss is 5.335309677124023 and perplexity is 207.53700832144037
At time: 674.0285835266113 and batch: 450, loss is 5.310487937927246 and perplexity is 202.4489868312007
At time: 674.8245604038239 and batch: 500, loss is 5.3094582271575925 and perplexity is 202.2406302210879
At time: 675.6377403736115 and batch: 550, loss is 5.312943449020386 and perplexity is 202.9467134006276
At time: 676.4370441436768 and batch: 600, loss is 5.339502153396606 and perplexity is 208.40892878029376
At time: 677.2570493221283 and batch: 650, loss is 5.333883533477783 and perplexity is 207.24124168864475
At time: 678.0710606575012 and batch: 700, loss is 5.360184898376465 and perplexity is 212.76428258326047
At time: 678.8653898239136 and batch: 750, loss is 5.333616819381714 and perplexity is 207.18597489874307
At time: 679.660612821579 and batch: 800, loss is 5.352215452194214 and perplexity is 211.07540772102186
At time: 680.4576508998871 and batch: 850, loss is 5.366526584625245 and perplexity is 214.11785433565646
At time: 681.2525534629822 and batch: 900, loss is 5.35008376121521 and perplexity is 210.62593941246533
At time: 682.048798084259 and batch: 950, loss is 5.324246006011963 and perplexity is 205.25354216926678
At time: 682.8449041843414 and batch: 1000, loss is 5.327996025085449 and perplexity is 206.02469187665767
At time: 683.6409547328949 and batch: 1050, loss is 5.33454306602478 and perplexity is 207.3779691157565
At time: 684.4391303062439 and batch: 1100, loss is 5.30346305847168 and perplexity is 201.031790746727
At time: 685.2563412189484 and batch: 1150, loss is 5.342485256195069 and perplexity is 209.0315622670818
At time: 686.0604741573334 and batch: 1200, loss is 5.348849468231201 and perplexity is 210.3661256693235
At time: 686.9328441619873 and batch: 1250, loss is 5.347697525024414 and perplexity is 210.12393536144234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.227543072108805 and perplexity of 186.33443039676118
Finished 32 epochs...
Completing Train Step...
At time: 689.1670670509338 and batch: 50, loss is 5.323884334564209 and perplexity is 205.17932124611715
At time: 690.1348628997803 and batch: 100, loss is 5.337591104507446 and perplexity is 208.01102945208933
At time: 690.9425890445709 and batch: 150, loss is 5.266539545059204 and perplexity is 193.7443574671532
At time: 691.7744898796082 and batch: 200, loss is 5.31407434463501 and perplexity is 203.1763547745516
At time: 692.584347486496 and batch: 250, loss is 5.3284400272369385 and perplexity is 206.1161875937554
At time: 693.3927111625671 and batch: 300, loss is 5.324092149734497 and perplexity is 205.22196505256304
At time: 694.1923410892487 and batch: 350, loss is 5.337494792938233 and perplexity is 207.99099654814484
At time: 694.9896559715271 and batch: 400, loss is 5.32786563873291 and perplexity is 205.9978308197466
At time: 695.8117711544037 and batch: 450, loss is 5.30031270980835 and perplexity is 200.39946705647571
At time: 696.6079640388489 and batch: 500, loss is 5.3011095905303955 and perplexity is 200.55922517410923
At time: 697.4056611061096 and batch: 550, loss is 5.3025711631774906 and perplexity is 200.85257137291603
At time: 698.2009720802307 and batch: 600, loss is 5.330873365402222 and perplexity is 206.61834869561682
At time: 698.9978179931641 and batch: 650, loss is 5.3240218830108645 and perplexity is 205.20754528408224
At time: 699.7945230007172 and batch: 700, loss is 5.349525823593139 and perplexity is 210.50845605392374
At time: 700.5922996997833 and batch: 750, loss is 5.3226846408843995 and perplexity is 204.93331650586217
At time: 701.3866558074951 and batch: 800, loss is 5.3427748870849605 and perplexity is 209.0921130327395
At time: 702.1894462108612 and batch: 850, loss is 5.358204860687255 and perplexity is 212.34341808611282
At time: 702.9843971729279 and batch: 900, loss is 5.342379941940307 and perplexity is 209.00954942303352
At time: 703.7816619873047 and batch: 950, loss is 5.316265888214112 and perplexity is 203.62211288103148
At time: 704.5778884887695 and batch: 1000, loss is 5.31994963645935 and perplexity is 204.37358875622323
At time: 705.3746540546417 and batch: 1050, loss is 5.32491394996643 and perplexity is 205.39068582893208
At time: 706.1678123474121 and batch: 1100, loss is 5.295082960128784 and perplexity is 199.35416372767043
At time: 707.0228440761566 and batch: 1150, loss is 5.334530687332153 and perplexity is 207.37540206350752
At time: 707.8315269947052 and batch: 1200, loss is 5.340934991836548 and perplexity is 208.7077591413659
At time: 708.636969089508 and batch: 1250, loss is 5.340436038970947 and perplexity is 208.60364978186067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.224812974024863 and perplexity of 185.82641290968627
Finished 33 epochs...
Completing Train Step...
At time: 711.0539960861206 and batch: 50, loss is 5.315239162445068 and perplexity is 203.41315609960176
At time: 711.8720304965973 and batch: 100, loss is 5.330936307907105 and perplexity is 206.6313541813331
At time: 712.6862287521362 and batch: 150, loss is 5.257141523361206 and perplexity is 191.93207307465218
At time: 713.4835350513458 and batch: 200, loss is 5.3055070114135745 and perplexity is 201.44311048273136
At time: 714.2804098129272 and batch: 250, loss is 5.32239541053772 and perplexity is 204.87405214262222
At time: 715.0837268829346 and batch: 300, loss is 5.313999042510987 and perplexity is 203.16105573951788
At time: 715.8787951469421 and batch: 350, loss is 5.329691762924194 and perplexity is 206.3743521246573
At time: 716.6739225387573 and batch: 400, loss is 5.318313751220703 and perplexity is 204.0395303343266
At time: 717.4795219898224 and batch: 450, loss is 5.2919192886352535 and perplexity is 198.7244692412247
At time: 718.2848882675171 and batch: 500, loss is 5.29146969795227 and perplexity is 198.63514465262912
At time: 719.0915744304657 and batch: 550, loss is 5.293359069824219 and perplexity is 199.0107950676327
At time: 719.8939926624298 and batch: 600, loss is 5.32243537902832 and perplexity is 204.88224081289292
At time: 720.6975660324097 and batch: 650, loss is 5.312015542984009 and perplexity is 202.75848526271844
At time: 721.5095920562744 and batch: 700, loss is 5.340057907104492 and perplexity is 208.52478500600145
At time: 722.3151965141296 and batch: 750, loss is 5.31487325668335 and perplexity is 203.33873966930616
At time: 723.1125264167786 and batch: 800, loss is 5.334421949386597 and perplexity is 207.35285371428105
At time: 723.9084596633911 and batch: 850, loss is 5.350146598815918 and perplexity is 210.63917505698873
At time: 724.7049350738525 and batch: 900, loss is 5.335255603790284 and perplexity is 207.525786406932
At time: 725.5015139579773 and batch: 950, loss is 5.310834035873413 and perplexity is 202.51906613619883
At time: 726.3074312210083 and batch: 1000, loss is 5.311740198135376 and perplexity is 202.70266444362497
At time: 727.1095082759857 and batch: 1050, loss is 5.315767726898193 and perplexity is 203.52070148304358
At time: 728.0434606075287 and batch: 1100, loss is 5.287211866378784 and perplexity is 197.791187650525
At time: 728.8384320735931 and batch: 1150, loss is 5.327246522903442 and perplexity is 205.87033377364224
At time: 729.6336624622345 and batch: 1200, loss is 5.33351921081543 and perplexity is 207.16575275972215
At time: 730.4269406795502 and batch: 1250, loss is 5.333847856521606 and perplexity is 207.2338480838384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.22254932709854 and perplexity of 185.40624325855904
Finished 34 epochs...
Completing Train Step...
At time: 732.6920092105865 and batch: 50, loss is 5.306133909225464 and perplexity is 201.56943431984456
At time: 733.5325133800507 and batch: 100, loss is 5.32530782699585 and perplexity is 205.47160043629333
At time: 734.3529419898987 and batch: 150, loss is 5.250902490615845 and perplexity is 190.73833035805671
At time: 735.1768414974213 and batch: 200, loss is 5.296647634506225 and perplexity is 199.6663322370376
At time: 735.9748306274414 and batch: 250, loss is 5.311246109008789 and perplexity is 202.60253599941794
At time: 736.7712364196777 and batch: 300, loss is 5.306580877304077 and perplexity is 201.6595495604273
At time: 737.5674369335175 and batch: 350, loss is 5.322220525741577 and perplexity is 204.83822591860064
At time: 738.3998708724976 and batch: 400, loss is 5.310941228866577 and perplexity is 202.54077592461846
At time: 739.1938321590424 and batch: 450, loss is 5.283123617172241 and perplexity is 196.9842186536904
At time: 739.997079372406 and batch: 500, loss is 5.282668142318726 and perplexity is 196.89451772536364
At time: 740.824315071106 and batch: 550, loss is 5.286225929260254 and perplexity is 197.59627407895385
At time: 741.6350252628326 and batch: 600, loss is 5.31240439414978 and perplexity is 202.83734346714166
At time: 742.4432814121246 and batch: 650, loss is 5.304774541854858 and perplexity is 201.29561356158644
At time: 743.2403521537781 and batch: 700, loss is 5.332809028625488 and perplexity is 207.01867956231044
At time: 744.0367367267609 and batch: 750, loss is 5.307001686096191 and perplexity is 201.7444275293414
At time: 744.8324794769287 and batch: 800, loss is 5.327477655410767 and perplexity is 205.91792259952155
At time: 745.6282448768616 and batch: 850, loss is 5.3450777721405025 and perplexity is 209.57418299788756
At time: 746.424706697464 and batch: 900, loss is 5.328275117874146 and perplexity is 206.0821999071189
At time: 747.2189819812775 and batch: 950, loss is 5.302373218536377 and perplexity is 200.81281761741008
At time: 748.0142736434937 and batch: 1000, loss is 5.302712383270264 and perplexity is 200.88093779458632
At time: 748.9027874469757 and batch: 1050, loss is 5.31021071434021 and perplexity is 202.39287097555822
At time: 749.6949231624603 and batch: 1100, loss is 5.280475215911865 and perplexity is 196.46321561780977
At time: 750.4892477989197 and batch: 1150, loss is 5.3198007965087895 and perplexity is 204.34317206504235
At time: 751.2829630374908 and batch: 1200, loss is 5.325304117202759 and perplexity is 205.47083818058348
At time: 752.0888774394989 and batch: 1250, loss is 5.3265259647369385 and perplexity is 205.7220456549243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2174397489450275 and perplexity of 184.46131172507882
Finished 35 epochs...
Completing Train Step...
At time: 754.3590548038483 and batch: 50, loss is 5.300604305267334 and perplexity is 200.4579111516545
At time: 755.1549274921417 and batch: 100, loss is 5.317460842132569 and perplexity is 203.8655773581481
At time: 755.9533722400665 and batch: 150, loss is 5.245177412033081 and perplexity is 189.64945834106211
At time: 756.7487096786499 and batch: 200, loss is 5.289060831069946 and perplexity is 198.15723487262545
At time: 757.542578458786 and batch: 250, loss is 5.305010595321655 and perplexity is 201.34313569768057
At time: 758.3369715213776 and batch: 300, loss is 5.29971983909607 and perplexity is 200.28069129451578
At time: 759.1322736740112 and batch: 350, loss is 5.314405994415283 and perplexity is 203.24374934304848
At time: 759.9277634620667 and batch: 400, loss is 5.305128402709961 and perplexity is 201.3668568038838
At time: 760.7226483821869 and batch: 450, loss is 5.275557441711426 and perplexity is 195.4994256777576
At time: 761.5182406902313 and batch: 500, loss is 5.275611124038696 and perplexity is 195.50992082360747
At time: 762.3175249099731 and batch: 550, loss is 5.2788426876068115 and perplexity is 196.1427455168422
At time: 763.1421813964844 and batch: 600, loss is 5.305356254577637 and perplexity is 201.4127438458201
At time: 763.9585275650024 and batch: 650, loss is 5.29733362197876 and perplexity is 199.80334782975618
At time: 764.7542276382446 and batch: 700, loss is 5.328066692352295 and perplexity is 206.03925159297705
At time: 765.5496735572815 and batch: 750, loss is 5.301394367218018 and perplexity is 200.61634789915058
At time: 766.346239566803 and batch: 800, loss is 5.321321821212768 and perplexity is 204.654219573347
At time: 767.1481423377991 and batch: 850, loss is 5.339632139205933 and perplexity is 208.4360207443195
At time: 767.9443781375885 and batch: 900, loss is 5.318455591201782 and perplexity is 204.06847335003837
At time: 768.7892289161682 and batch: 950, loss is 5.294705448150634 and perplexity is 199.27891934669034
At time: 769.5855617523193 and batch: 1000, loss is 5.299452524185181 and perplexity is 200.227160434487
At time: 770.4162125587463 and batch: 1050, loss is 5.301863079071045 and perplexity is 200.71040119954836
At time: 771.2295832633972 and batch: 1100, loss is 5.271957540512085 and perplexity is 194.79691230882221
At time: 772.0356707572937 and batch: 1150, loss is 5.313999252319336 and perplexity is 203.16109836440813
At time: 772.8687152862549 and batch: 1200, loss is 5.320178337097168 and perplexity is 204.42033447150862
At time: 773.6804258823395 and batch: 1250, loss is 5.320922336578369 and perplexity is 204.57247968526423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.214436551950274 and perplexity of 183.9081690819611
Finished 36 epochs...
Completing Train Step...
At time: 775.9407997131348 and batch: 50, loss is 5.292724561691284 and perplexity is 198.8845611520787
At time: 776.773472070694 and batch: 100, loss is 5.3101646518707275 and perplexity is 202.38354847482583
At time: 777.5730743408203 and batch: 150, loss is 5.237396259307861 and perplexity is 188.17949336917061
At time: 778.3688714504242 and batch: 200, loss is 5.2812612342834475 and perplexity is 196.6177000204611
At time: 779.1914305686951 and batch: 250, loss is 5.296686477661133 and perplexity is 199.67408805793983
At time: 779.9865086078644 and batch: 300, loss is 5.291416225433349 and perplexity is 198.62452341507296
At time: 780.7831478118896 and batch: 350, loss is 5.306974906921386 and perplexity is 201.73902505238797
At time: 781.5796649456024 and batch: 400, loss is 5.29642882347107 and perplexity is 199.62264781968565
At time: 782.3752808570862 and batch: 450, loss is 5.267513475418091 and perplexity is 193.93314289576108
At time: 783.1710367202759 and batch: 500, loss is 5.27025876045227 and perplexity is 194.4662761170772
At time: 783.9664471149445 and batch: 550, loss is 5.272031764984131 and perplexity is 194.81137154340243
At time: 784.7614662647247 and batch: 600, loss is 5.296521615982056 and perplexity is 199.64117216587354
At time: 785.5575339794159 and batch: 650, loss is 5.290369567871093 and perplexity is 198.41674031343715
At time: 786.352550983429 and batch: 700, loss is 5.319160385131836 and perplexity is 204.2123502672041
At time: 787.146731376648 and batch: 750, loss is 5.2922538471221925 and perplexity is 198.7909653217656
At time: 787.9460234642029 and batch: 800, loss is 5.31314395904541 and perplexity is 202.98741033114513
At time: 788.741694688797 and batch: 850, loss is 5.332056360244751 and perplexity is 206.86292177231698
At time: 789.5877068042755 and batch: 900, loss is 5.310306873321533 and perplexity is 202.4123338036062
At time: 790.386825799942 and batch: 950, loss is 5.286158027648926 and perplexity is 197.58285742906284
At time: 791.182051897049 and batch: 1000, loss is 5.288381614685059 and perplexity is 198.02268892999288
At time: 791.9758756160736 and batch: 1050, loss is 5.292002429962158 and perplexity is 198.74099214414443
At time: 792.7769875526428 and batch: 1100, loss is 5.265597372055054 and perplexity is 193.56190272930246
At time: 793.577632188797 and batch: 1150, loss is 5.306784782409668 and perplexity is 201.70067316468788
At time: 794.3686516284943 and batch: 1200, loss is 5.312418851852417 and perplexity is 202.8402760503363
At time: 795.1686203479767 and batch: 1250, loss is 5.31291109085083 and perplexity is 202.94014652271116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.215367672217154 and perplexity of 184.07948945299515
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 797.4470872879028 and batch: 50, loss is 5.295148725509644 and perplexity is 199.3672747612952
At time: 798.2768805027008 and batch: 100, loss is 5.312785730361939 and perplexity is 202.91470744128864
At time: 799.0736856460571 and batch: 150, loss is 5.231773929595947 and perplexity is 187.124454878752
At time: 799.8696870803833 and batch: 200, loss is 5.271703662872315 and perplexity is 194.74746400566553
At time: 800.6668908596039 and batch: 250, loss is 5.284189586639404 and perplexity is 197.19430977175307
At time: 801.4685022830963 and batch: 300, loss is 5.27230128288269 and perplexity is 194.86388377105095
At time: 802.2662491798401 and batch: 350, loss is 5.2854666328430175 and perplexity is 197.4462968817663
At time: 803.0624399185181 and batch: 400, loss is 5.255043478012085 and perplexity is 191.52981300887632
At time: 803.8586146831512 and batch: 450, loss is 5.227933692932129 and perplexity is 186.40723072311235
At time: 804.6538105010986 and batch: 500, loss is 5.216528387069702 and perplexity is 184.293277299815
At time: 805.4507477283478 and batch: 550, loss is 5.223133869171143 and perplexity is 185.51465269011695
At time: 806.2468340396881 and batch: 600, loss is 5.244282512664795 and perplexity is 189.47981707784788
At time: 807.0437819957733 and batch: 650, loss is 5.241863183975219 and perplexity is 189.02195720039145
At time: 807.8502082824707 and batch: 700, loss is 5.259651966094971 and perplexity is 192.41451286824184
At time: 808.6481337547302 and batch: 750, loss is 5.222309093475342 and perplexity is 185.36170779464965
At time: 809.4432163238525 and batch: 800, loss is 5.224567527770996 and perplexity is 185.78080810976059
At time: 810.2907025814056 and batch: 850, loss is 5.240493459701538 and perplexity is 188.7632264726857
At time: 811.0882213115692 and batch: 900, loss is 5.210896005630493 and perplexity is 183.258185019069
At time: 811.8859367370605 and batch: 950, loss is 5.1862502288818355 and perplexity is 178.79684709406405
At time: 812.6820411682129 and batch: 1000, loss is 5.179747486114502 and perplexity is 177.63794928071542
At time: 813.5036523342133 and batch: 1050, loss is 5.164857082366943 and perplexity is 175.0124444200465
At time: 814.3012902736664 and batch: 1100, loss is 5.120683126449585 and perplexity is 167.44971989144346
At time: 815.0952568054199 and batch: 1150, loss is 5.155708675384521 and perplexity is 173.41866075803682
At time: 815.8915762901306 and batch: 1200, loss is 5.173321266174316 and perplexity is 176.50006879988365
At time: 816.6885366439819 and batch: 1250, loss is 5.2086766242980955 and perplexity is 182.85191622368038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.130501183280109 and perplexity of 169.1018478293056
Finished 38 epochs...
Completing Train Step...
At time: 818.9452934265137 and batch: 50, loss is 5.2280988693237305 and perplexity is 186.4380233398883
At time: 819.7428312301636 and batch: 100, loss is 5.254459161758422 and perplexity is 191.41793171628646
At time: 820.5390582084656 and batch: 150, loss is 5.178131618499756 and perplexity is 177.35114165530476
At time: 821.3356573581696 and batch: 200, loss is 5.218057689666748 and perplexity is 184.57533310677485
At time: 822.1534080505371 and batch: 250, loss is 5.238091526031494 and perplexity is 188.31037380211293
At time: 822.9540088176727 and batch: 300, loss is 5.228842716217041 and perplexity is 186.57675627597965
At time: 823.7505738735199 and batch: 350, loss is 5.242528162002563 and perplexity is 189.1476944502361
At time: 824.5625715255737 and batch: 400, loss is 5.217993488311768 and perplexity is 184.563483500678
At time: 825.3591496944427 and batch: 450, loss is 5.192542400360107 and perplexity is 179.92541436167775
At time: 826.155033826828 and batch: 500, loss is 5.184366359710693 and perplexity is 178.4603342984893
At time: 826.9512357711792 and batch: 550, loss is 5.190301990509033 and perplexity is 179.52275891592208
At time: 827.7486808300018 and batch: 600, loss is 5.214587669372559 and perplexity is 183.9359629104229
At time: 828.5451624393463 and batch: 650, loss is 5.2145631313323975 and perplexity is 183.9314495377527
At time: 829.341964006424 and batch: 700, loss is 5.233017892837524 and perplexity is 187.3573756646332
At time: 830.1612679958344 and batch: 750, loss is 5.19886996269226 and perplexity is 181.06751317629477
At time: 831.0315201282501 and batch: 800, loss is 5.207217998504639 and perplexity is 182.58539812463383
At time: 831.8277943134308 and batch: 850, loss is 5.22744668006897 and perplexity is 186.3164701065521
At time: 832.6258363723755 and batch: 900, loss is 5.198301458358765 and perplexity is 180.96460476510933
At time: 833.4224846363068 and batch: 950, loss is 5.176293392181396 and perplexity is 177.02542957709716
At time: 834.2217383384705 and batch: 1000, loss is 5.1734762954711915 and perplexity is 176.52743360256642
At time: 835.0181505680084 and batch: 1050, loss is 5.1637278747558595 and perplexity is 174.8149305738299
At time: 835.813939332962 and batch: 1100, loss is 5.126834421157837 and perplexity is 168.4829269900804
At time: 836.6075522899628 and batch: 1150, loss is 5.168806829452515 and perplexity is 175.70506625242814
At time: 837.4038274288177 and batch: 1200, loss is 5.188669481277466 and perplexity is 179.22992544642213
At time: 838.2014062404633 and batch: 1250, loss is 5.214679498672485 and perplexity is 183.95285439668297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.126736160612454 and perplexity of 168.46637257912266
Finished 39 epochs...
Completing Train Step...
At time: 840.4125220775604 and batch: 50, loss is 5.216827411651611 and perplexity is 184.3483937601858
At time: 841.2384777069092 and batch: 100, loss is 5.241197347640991 and perplexity is 188.89614140433295
At time: 842.0347149372101 and batch: 150, loss is 5.163826360702514 and perplexity is 174.83214823559183
At time: 842.8299820423126 and batch: 200, loss is 5.201331739425659 and perplexity is 181.5138100836852
At time: 843.6251313686371 and batch: 250, loss is 5.223257923126221 and perplexity is 185.53766794404524
At time: 844.4219739437103 and batch: 300, loss is 5.214984817504883 and perplexity is 184.00902724228104
At time: 845.2172257900238 and batch: 350, loss is 5.228292970657349 and perplexity is 186.47421472113982
At time: 846.0175273418427 and batch: 400, loss is 5.204364900588989 and perplexity is 182.0652065374534
At time: 846.8152568340302 and batch: 450, loss is 5.178890743255615 and perplexity is 177.4858244114546
At time: 847.6113708019257 and batch: 500, loss is 5.172817611694336 and perplexity is 176.4111961319532
At time: 848.4072761535645 and batch: 550, loss is 5.179167337417603 and perplexity is 177.53492274416544
At time: 849.2028150558472 and batch: 600, loss is 5.204521198272705 and perplexity is 182.09366513146864
At time: 850.0005776882172 and batch: 650, loss is 5.205834665298462 and perplexity is 182.3329962986085
At time: 850.8466980457306 and batch: 700, loss is 5.224339866638184 and perplexity is 185.7385178546379
At time: 851.6447064876556 and batch: 750, loss is 5.19186336517334 and perplexity is 179.80328014574044
At time: 852.4580023288727 and batch: 800, loss is 5.2010640621185305 and perplexity is 181.46522945805094
At time: 853.2634222507477 and batch: 850, loss is 5.2239762306213375 and perplexity is 185.67098891855107
At time: 854.0575745105743 and batch: 900, loss is 5.195790128707886 and perplexity is 180.51071316167344
At time: 854.857195854187 and batch: 950, loss is 5.174793558120728 and perplexity is 176.760119818254
At time: 855.6577446460724 and batch: 1000, loss is 5.173621654510498 and perplexity is 176.55309532576268
At time: 856.4667332172394 and batch: 1050, loss is 5.165738925933838 and perplexity is 175.16684608733757
At time: 857.2998821735382 and batch: 1100, loss is 5.131815204620361 and perplexity is 169.3241973200239
At time: 858.1119921207428 and batch: 1150, loss is 5.17549575805664 and perplexity is 176.88428435212418
At time: 858.9065010547638 and batch: 1200, loss is 5.195221729278565 and perplexity is 180.40814012931412
At time: 859.7032527923584 and batch: 1250, loss is 5.215860271453858 and perplexity is 184.1701892064732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.124815112482892 and perplexity of 168.14305122655804
Finished 40 epochs...
Completing Train Step...
At time: 862.0311617851257 and batch: 50, loss is 5.209297313690185 and perplexity is 182.96544569802907
At time: 862.8267812728882 and batch: 100, loss is 5.233001375198365 and perplexity is 187.35428098866652
At time: 863.6215162277222 and batch: 150, loss is 5.1551683807373045 and perplexity is 173.32498889138452
At time: 864.4176287651062 and batch: 200, loss is 5.19187560081482 and perplexity is 179.80548016767247
At time: 865.2124238014221 and batch: 250, loss is 5.21565094947815 and perplexity is 184.13164237309235
At time: 866.0058584213257 and batch: 300, loss is 5.207618732452392 and perplexity is 182.6585809544692
At time: 866.802250623703 and batch: 350, loss is 5.2205040168762205 and perplexity is 185.02741751408587
At time: 867.597243309021 and batch: 400, loss is 5.1967236328125 and perplexity is 180.67929932903368
At time: 868.4124090671539 and batch: 450, loss is 5.17129451751709 and perplexity is 176.14270978322406
At time: 869.2154486179352 and batch: 500, loss is 5.166502943038941 and perplexity is 175.30072769140045
At time: 870.010066986084 and batch: 550, loss is 5.173360986709595 and perplexity is 176.50707961632887
At time: 870.8071436882019 and batch: 600, loss is 5.199027633666992 and perplexity is 181.0960645183893
At time: 871.6318626403809 and batch: 650, loss is 5.201093769073486 and perplexity is 181.47062031752105
At time: 872.428361415863 and batch: 700, loss is 5.219251384735108 and perplexity is 184.79579132541176
At time: 873.224650144577 and batch: 750, loss is 5.187082223892212 and perplexity is 178.94566707887506
At time: 874.0206999778748 and batch: 800, loss is 5.1980313873291015 and perplexity is 180.91573806700453
At time: 874.8157608509064 and batch: 850, loss is 5.222103414535522 and perplexity is 185.32358671559314
At time: 875.6419851779938 and batch: 900, loss is 5.194861183166504 and perplexity is 180.3431064003399
At time: 876.4399404525757 and batch: 950, loss is 5.174509506225586 and perplexity is 176.70991790154656
At time: 877.237628698349 and batch: 1000, loss is 5.1739614391326905 and perplexity is 176.6130955455536
At time: 878.0347151756287 and batch: 1050, loss is 5.167690076828003 and perplexity is 175.50895668187084
At time: 878.8294885158539 and batch: 1100, loss is 5.1347877788543705 and perplexity is 169.82827489900598
At time: 879.6256487369537 and batch: 1150, loss is 5.179412832260132 and perplexity is 177.57851200231806
At time: 880.4189331531525 and batch: 1200, loss is 5.198812379837036 and perplexity is 181.0570870920824
At time: 881.2126932144165 and batch: 1250, loss is 5.215681467056275 and perplexity is 184.1372617106176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.123806473112454 and perplexity of 167.97354102692685
Finished 41 epochs...
Completing Train Step...
At time: 883.428014755249 and batch: 50, loss is 5.203894243240357 and perplexity is 181.97953637229415
At time: 884.2705090045929 and batch: 100, loss is 5.227344198226929 and perplexity is 186.29737702985653
At time: 885.0628890991211 and batch: 150, loss is 5.149804763793945 and perplexity is 172.3978287329673
At time: 885.8565344810486 and batch: 200, loss is 5.186446046829223 and perplexity is 178.83186215383807
At time: 886.6506242752075 and batch: 250, loss is 5.209977054595948 and perplexity is 183.0898570747717
At time: 887.445662021637 and batch: 300, loss is 5.202254028320312 and perplexity is 181.68129547803903
At time: 888.2712731361389 and batch: 350, loss is 5.215359077453614 and perplexity is 184.0779073401097
At time: 889.0787708759308 and batch: 400, loss is 5.191518955230713 and perplexity is 179.74136477108857
At time: 889.8895487785339 and batch: 450, loss is 5.166021909713745 and perplexity is 175.21642247788407
At time: 890.6884982585907 and batch: 500, loss is 5.162057847976684 and perplexity is 174.523228601221
At time: 891.4829208850861 and batch: 550, loss is 5.169279041290284 and perplexity is 175.78805585746335
At time: 892.3256688117981 and batch: 600, loss is 5.195582094192505 and perplexity is 180.47316460877278
At time: 893.1195120811462 and batch: 650, loss is 5.197843141555786 and perplexity is 180.88168464929376
At time: 893.9135875701904 and batch: 700, loss is 5.216041040420532 and perplexity is 184.2034844705529
At time: 894.7067635059357 and batch: 750, loss is 5.184022216796875 and perplexity is 178.39892900574716
At time: 895.5007441043854 and batch: 800, loss is 5.196440124511719 and perplexity is 180.6280825084346
At time: 896.2942237854004 and batch: 850, loss is 5.220917749404907 and perplexity is 185.1039852135921
At time: 897.089277267456 and batch: 900, loss is 5.194114427566529 and perplexity is 180.20848444682335
At time: 897.9078454971313 and batch: 950, loss is 5.1743757438659665 and perplexity is 176.68628234676885
At time: 898.7344284057617 and batch: 1000, loss is 5.174251108169556 and perplexity is 176.66426230119274
At time: 899.5538339614868 and batch: 1050, loss is 5.168474884033203 and perplexity is 175.64675143973872
At time: 900.3685119152069 and batch: 1100, loss is 5.136426124572754 and perplexity is 170.10674037511993
At time: 901.1718502044678 and batch: 1150, loss is 5.179948263168335 and perplexity is 177.67361848547992
At time: 901.9615426063538 and batch: 1200, loss is 5.20026047706604 and perplexity is 181.3194652869102
At time: 902.7627623081207 and batch: 1250, loss is 5.215186767578125 and perplexity is 184.04619163135905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.123773505217837 and perplexity of 167.96800338421056
Finished 42 epochs...
Completing Train Step...
At time: 904.9886424541473 and batch: 50, loss is 5.199769229888916 and perplexity is 181.23041448619838
At time: 905.8913705348969 and batch: 100, loss is 5.223216409683228 and perplexity is 185.52996579651634
At time: 906.697099685669 and batch: 150, loss is 5.145583667755127 and perplexity is 171.671654644512
At time: 907.4936678409576 and batch: 200, loss is 5.182419366836548 and perplexity is 178.11321133191942
At time: 908.2895481586456 and batch: 250, loss is 5.205758399963379 and perplexity is 182.3190911417966
At time: 909.0839455127716 and batch: 300, loss is 5.199377536773682 and perplexity is 181.15944168126006
At time: 909.8798415660858 and batch: 350, loss is 5.2111827087402345 and perplexity is 183.31073324310915
At time: 910.676435470581 and batch: 400, loss is 5.188173112869262 and perplexity is 179.14098344945072
At time: 911.4762778282166 and batch: 450, loss is 5.162019100189209 and perplexity is 174.51646634326175
At time: 912.2846636772156 and batch: 500, loss is 5.1589916229248045 and perplexity is 173.9889206783367
At time: 913.1281409263611 and batch: 550, loss is 5.166076736450195 and perplexity is 175.2260292858536
At time: 913.9250066280365 and batch: 600, loss is 5.193348388671875 and perplexity is 180.0704905996657
At time: 914.720648765564 and batch: 650, loss is 5.195309591293335 and perplexity is 180.42399184835867
At time: 915.5168063640594 and batch: 700, loss is 5.214271621704102 and perplexity is 183.87783956355753
At time: 916.3139917850494 and batch: 750, loss is 5.181942129135132 and perplexity is 178.0282292722859
At time: 917.1087806224823 and batch: 800, loss is 5.19457236289978 and perplexity is 180.2910271773772
At time: 917.9057712554932 and batch: 850, loss is 5.219658899307251 and perplexity is 184.87111364967757
At time: 918.7021653652191 and batch: 900, loss is 5.193231439590454 and perplexity is 180.04943275257085
At time: 919.4976103305817 and batch: 950, loss is 5.173882026672363 and perplexity is 176.59907082198652
At time: 920.3113715648651 and batch: 1000, loss is 5.174279012680054 and perplexity is 176.6691920997362
At time: 921.1077647209167 and batch: 1050, loss is 5.169501686096192 and perplexity is 175.8271985123334
At time: 921.9054300785065 and batch: 1100, loss is 5.136833801269531 and perplexity is 170.1761030669514
At time: 922.7028369903564 and batch: 1150, loss is 5.180394315719605 and perplexity is 177.7528879341544
At time: 923.4983677864075 and batch: 1200, loss is 5.200441007614136 and perplexity is 181.35220194425327
At time: 924.2948007583618 and batch: 1250, loss is 5.2137323665618895 and perplexity is 183.778709223709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.122436968949589 and perplexity of 167.74365801204186
Finished 43 epochs...
Completing Train Step...
At time: 926.5304517745972 and batch: 50, loss is 5.195209503173828 and perplexity is 180.40593445398102
At time: 927.3263657093048 and batch: 100, loss is 5.218509778976441 and perplexity is 184.65879650673935
At time: 928.1226260662079 and batch: 150, loss is 5.141680335998535 and perplexity is 171.0028693178674
At time: 928.9213681221008 and batch: 200, loss is 5.17849404335022 and perplexity is 177.41542976539947
At time: 929.7262632846832 and batch: 250, loss is 5.202210626602173 and perplexity is 181.67341036877636
At time: 930.5485990047455 and batch: 300, loss is 5.196081104278565 and perplexity is 180.56324501182124
At time: 931.3472361564636 and batch: 350, loss is 5.207538242340088 and perplexity is 182.64387933645023
At time: 932.1649534702301 and batch: 400, loss is 5.184152584075928 and perplexity is 178.42218790477435
At time: 933.0156791210175 and batch: 450, loss is 5.158739252090454 and perplexity is 173.9450164895588
At time: 933.813230752945 and batch: 500, loss is 5.1561852645874025 and perplexity is 173.5013299173818
At time: 934.6095445156097 and batch: 550, loss is 5.163748989105224 and perplexity is 174.81862171631616
At time: 935.4425556659698 and batch: 600, loss is 5.191399459838867 and perplexity is 179.71988778950012
At time: 936.2625350952148 and batch: 650, loss is 5.193678188323974 and perplexity is 180.12988757883193
At time: 937.0832989215851 and batch: 700, loss is 5.212538614273071 and perplexity is 183.55945386326866
At time: 937.8916628360748 and batch: 750, loss is 5.179997987747193 and perplexity is 177.68245345098893
At time: 938.6909046173096 and batch: 800, loss is 5.192183465957641 and perplexity is 179.8608445294457
At time: 939.4888186454773 and batch: 850, loss is 5.21856725692749 and perplexity is 184.66941062104164
At time: 940.2843379974365 and batch: 900, loss is 5.191919364929199 and perplexity is 179.81334936746563
At time: 941.0820181369781 and batch: 950, loss is 5.173076839447021 and perplexity is 176.4569327377176
At time: 941.8826966285706 and batch: 1000, loss is 5.173874082565308 and perplexity is 176.59766790563458
At time: 942.6811828613281 and batch: 1050, loss is 5.1693590641021725 and perplexity is 175.8021234748472
At time: 943.4835188388824 and batch: 1100, loss is 5.136581478118896 and perplexity is 170.1331691133058
At time: 944.2848975658417 and batch: 1150, loss is 5.180127277374267 and perplexity is 177.70542743425017
At time: 945.0808446407318 and batch: 1200, loss is 5.199755172729493 and perplexity is 181.22786691927544
At time: 945.8751697540283 and batch: 1250, loss is 5.21178671836853 and perplexity is 183.42148813610157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.120725757014142 and perplexity of 167.45685851969824
Finished 44 epochs...
Completing Train Step...
At time: 948.1062455177307 and batch: 50, loss is 5.191081466674805 and perplexity is 179.66274717938026
At time: 948.9395530223846 and batch: 100, loss is 5.215494203567505 and perplexity is 184.10278275300286
At time: 949.736020565033 and batch: 150, loss is 5.1384647178649905 and perplexity is 170.45387254545037
At time: 950.552757024765 and batch: 200, loss is 5.174620780944824 and perplexity is 176.7295823421049
At time: 951.3625659942627 and batch: 250, loss is 5.198995819091797 and perplexity is 181.090303115676
At time: 952.1662323474884 and batch: 300, loss is 5.192252063751221 and perplexity is 179.87318300972322
At time: 952.9654548168182 and batch: 350, loss is 5.204665842056275 and perplexity is 182.12000575311507
At time: 953.811304807663 and batch: 400, loss is 5.181513013839722 and perplexity is 177.9518510247995
At time: 954.6059634685516 and batch: 450, loss is 5.155756931304932 and perplexity is 173.4270294370456
At time: 955.4035491943359 and batch: 500, loss is 5.153258361816406 and perplexity is 172.99425084165304
At time: 956.1988618373871 and batch: 550, loss is 5.161279830932617 and perplexity is 174.38749936146115
At time: 957.022554397583 and batch: 600, loss is 5.189012603759766 and perplexity is 179.29143381519236
At time: 957.818380355835 and batch: 650, loss is 5.19122820854187 and perplexity is 179.6891131607927
At time: 958.6156008243561 and batch: 700, loss is 5.209527387619018 and perplexity is 183.00754611988052
At time: 959.4095335006714 and batch: 750, loss is 5.17766622543335 and perplexity is 177.26862286698451
At time: 960.2104322910309 and batch: 800, loss is 5.189819803237915 and perplexity is 179.43621619327334
At time: 961.0073788166046 and batch: 850, loss is 5.217344150543213 and perplexity is 184.44367836135547
At time: 961.8103642463684 and batch: 900, loss is 5.190906667709351 and perplexity is 179.63134506165147
At time: 962.6116857528687 and batch: 950, loss is 5.172706212997436 and perplexity is 176.39154524914724
At time: 963.4166593551636 and batch: 1000, loss is 5.173467473983765 and perplexity is 176.52587637489896
At time: 964.2369563579559 and batch: 1050, loss is 5.168993682861328 and perplexity is 175.7379004104948
At time: 965.032068490982 and batch: 1100, loss is 5.136762685775757 and perplexity is 170.16400133966854
At time: 965.8320531845093 and batch: 1150, loss is 5.180100440979004 and perplexity is 177.70065852514955
At time: 966.6687219142914 and batch: 1200, loss is 5.199961605072022 and perplexity is 181.26528207409135
At time: 967.4750571250916 and batch: 1250, loss is 5.210839862823486 and perplexity is 183.24789667896587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.120778772952783 and perplexity of 167.46573663757331
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 969.7660353183746 and batch: 50, loss is 5.191812038421631 and perplexity is 179.7940516642599
At time: 970.5628199577332 and batch: 100, loss is 5.220677518844605 and perplexity is 185.0595229203247
At time: 971.3598892688751 and batch: 150, loss is 5.142628374099732 and perplexity is 171.16506342434312
At time: 972.1815750598907 and batch: 200, loss is 5.1800692653656 and perplexity is 177.69511868447222
At time: 972.9911458492279 and batch: 250, loss is 5.202290897369385 and perplexity is 181.6879940181212
At time: 973.7881648540497 and batch: 300, loss is 5.1969586277008055 and perplexity is 180.7217630299796
At time: 974.6333801746368 and batch: 350, loss is 5.204694547653198 and perplexity is 182.1252336916271
At time: 975.4287009239197 and batch: 400, loss is 5.177530632019043 and perplexity is 177.24458803867964
At time: 976.2256879806519 and batch: 450, loss is 5.153335552215577 and perplexity is 173.00760485232374
At time: 977.0225172042847 and batch: 500, loss is 5.146853752136231 and perplexity is 171.88983065339625
At time: 977.8309922218323 and batch: 550, loss is 5.154317941665649 and perplexity is 173.17764920932854
At time: 978.6368851661682 and batch: 600, loss is 5.181515312194824 and perplexity is 177.9522600218142
At time: 979.4429445266724 and batch: 650, loss is 5.183824443817139 and perplexity is 178.3636500067073
At time: 980.2492432594299 and batch: 700, loss is 5.201747303009033 and perplexity is 181.58925628829726
At time: 981.0570027828217 and batch: 750, loss is 5.16531810760498 and perplexity is 175.09314817569935
At time: 981.86341381073 and batch: 800, loss is 5.175406398773194 and perplexity is 176.8684788054182
At time: 982.672360420227 and batch: 850, loss is 5.196007242202759 and perplexity is 180.54990872825928
At time: 983.4828500747681 and batch: 900, loss is 5.167843656539917 and perplexity is 175.53591336682334
At time: 984.2836182117462 and batch: 950, loss is 5.149507255554199 and perplexity is 172.34654658721362
At time: 985.0798242092133 and batch: 1000, loss is 5.149723310470581 and perplexity is 172.38378692875978
At time: 985.8760633468628 and batch: 1050, loss is 5.13979139328003 and perplexity is 170.68015957908673
At time: 986.6720969676971 and batch: 1100, loss is 5.104712171554565 and perplexity is 164.7966305466971
At time: 987.4670643806458 and batch: 1150, loss is 5.143933696746826 and perplexity is 171.38863494276066
At time: 988.2618315219879 and batch: 1200, loss is 5.172023096084595 and perplexity is 176.27109034837943
At time: 989.0590584278107 and batch: 1250, loss is 5.192813224792481 and perplexity is 179.97414915888083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.111918428518476 and perplexity of 165.98848666616695
Finished 46 epochs...
Completing Train Step...
At time: 991.3199350833893 and batch: 50, loss is 5.1847965526580815 and perplexity is 178.53712319152896
At time: 992.1458177566528 and batch: 100, loss is 5.211553077697754 and perplexity is 183.37863842249163
At time: 992.9428396224976 and batch: 150, loss is 5.135084705352783 and perplexity is 169.8787089012285
At time: 993.7402448654175 and batch: 200, loss is 5.171574096679688 and perplexity is 176.19196249921868
At time: 994.537624835968 and batch: 250, loss is 5.1946204662322994 and perplexity is 180.29969998520153
At time: 995.3832240104675 and batch: 300, loss is 5.189960260391235 and perplexity is 179.46142106346304
At time: 996.1807243824005 and batch: 350, loss is 5.198041219711303 and perplexity is 180.91751690843262
At time: 996.9776790142059 and batch: 400, loss is 5.171907758712768 and perplexity is 176.2507608764861
At time: 997.7719511985779 and batch: 450, loss is 5.147194232940674 and perplexity is 171.94836580569506
At time: 998.5790643692017 and batch: 500, loss is 5.142388095855713 and perplexity is 171.12394112406105
At time: 999.3998501300812 and batch: 550, loss is 5.149693965911865 and perplexity is 172.37872847682203
At time: 1000.1964271068573 and batch: 600, loss is 5.177107439041138 and perplexity is 177.16959524298144
At time: 1000.9916698932648 and batch: 650, loss is 5.1803060817718505 and perplexity is 177.73720478703038
At time: 1001.7907602787018 and batch: 700, loss is 5.198412504196167 and perplexity is 180.98470124698022
At time: 1002.5888421535492 and batch: 750, loss is 5.162174711227417 and perplexity is 174.54362514482366
At time: 1003.3875126838684 and batch: 800, loss is 5.173123445510864 and perplexity is 176.46515689243654
At time: 1004.1821186542511 and batch: 850, loss is 5.194922637939453 and perplexity is 180.35418968555368
At time: 1004.9771444797516 and batch: 900, loss is 5.166942548751831 and perplexity is 175.37780783396164
At time: 1005.7772288322449 and batch: 950, loss is 5.149066858291626 and perplexity is 172.27066235071416
At time: 1006.5790529251099 and batch: 1000, loss is 5.14972412109375 and perplexity is 172.38392666710814
At time: 1007.3858919143677 and batch: 1050, loss is 5.14066409111023 and perplexity is 170.8291767980538
At time: 1008.2128558158875 and batch: 1100, loss is 5.106985006332398 and perplexity is 165.17161203393323
At time: 1009.0111575126648 and batch: 1150, loss is 5.146679611206054 and perplexity is 171.85990020453215
At time: 1009.8094646930695 and batch: 1200, loss is 5.175252199172974 and perplexity is 176.8412078593334
At time: 1010.617527961731 and batch: 1250, loss is 5.19354061126709 and perplexity is 180.1051075436631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.111057699161725 and perplexity of 165.84567697187558
Finished 47 epochs...
Completing Train Step...
At time: 1012.8255484104156 and batch: 50, loss is 5.181960678100586 and perplexity is 178.03153154238714
At time: 1013.6505551338196 and batch: 100, loss is 5.208652629852295 and perplexity is 182.8475288459235
At time: 1014.4458751678467 and batch: 150, loss is 5.131972932815552 and perplexity is 169.35090662641932
At time: 1015.2704956531525 and batch: 200, loss is 5.1679646682739255 and perplexity is 175.55715655739237
At time: 1016.0662376880646 and batch: 250, loss is 5.191590909957886 and perplexity is 179.75429847726744
At time: 1016.8617300987244 and batch: 300, loss is 5.186787214279175 and perplexity is 178.89288417299062
At time: 1017.6587002277374 and batch: 350, loss is 5.195065746307373 and perplexity is 180.38000172620247
At time: 1018.455525636673 and batch: 400, loss is 5.169160375595093 and perplexity is 175.7671970832437
At time: 1019.2508437633514 and batch: 450, loss is 5.144775381088257 and perplexity is 171.5329507987695
At time: 1020.0464136600494 and batch: 500, loss is 5.140577516555786 and perplexity is 170.8143879783635
At time: 1020.8428204059601 and batch: 550, loss is 5.1478265857696535 and perplexity is 172.05713222700348
At time: 1021.6382005214691 and batch: 600, loss is 5.175373458862305 and perplexity is 176.86265286944075
At time: 1022.4339771270752 and batch: 650, loss is 5.1789158344268795 and perplexity is 177.49027779454195
At time: 1023.230197429657 and batch: 700, loss is 5.196793174743652 and perplexity is 180.69186455332817
At time: 1024.026126384735 and batch: 750, loss is 5.161186809539795 and perplexity is 174.3712783478421
At time: 1024.822564125061 and batch: 800, loss is 5.172465343475341 and perplexity is 176.34906301848417
At time: 1025.6177875995636 and batch: 850, loss is 5.1949022769927975 and perplexity is 180.35051754090262
At time: 1026.4136233329773 and batch: 900, loss is 5.167325868606567 and perplexity is 175.44504651592206
At time: 1027.2099013328552 and batch: 950, loss is 5.149638080596924 and perplexity is 172.36909530647085
At time: 1028.0064344406128 and batch: 1000, loss is 5.150643720626831 and perplexity is 172.54252375731022
At time: 1028.8025319576263 and batch: 1050, loss is 5.141973953247071 and perplexity is 171.05308608174153
At time: 1029.5970602035522 and batch: 1100, loss is 5.10875244140625 and perplexity is 165.46380027005299
At time: 1030.4170100688934 and batch: 1150, loss is 5.148945026397705 and perplexity is 172.24967556810864
At time: 1031.2161376476288 and batch: 1200, loss is 5.177579202651978 and perplexity is 177.25319712957764
At time: 1032.0234813690186 and batch: 1250, loss is 5.194183731079102 and perplexity is 180.22097396056958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.110828705947765 and perplexity of 165.8077037852514
Finished 48 epochs...
Completing Train Step...
At time: 1034.316811800003 and batch: 50, loss is 5.180319547653198 and perplexity is 177.73959819125574
At time: 1035.1134440898895 and batch: 100, loss is 5.206816930770874 and perplexity is 182.51218369574326
At time: 1035.9365165233612 and batch: 150, loss is 5.130379524230957 and perplexity is 169.08127631066986
At time: 1036.733053445816 and batch: 200, loss is 5.165771636962891 and perplexity is 175.17257606884527
At time: 1037.5282125473022 and batch: 250, loss is 5.189634475708008 and perplexity is 179.40296480384444
At time: 1038.324187040329 and batch: 300, loss is 5.184807138442993 and perplexity is 178.5390131571172
At time: 1039.11975979805 and batch: 350, loss is 5.1929982566833495 and perplexity is 180.00745319706675
At time: 1039.9145419597626 and batch: 400, loss is 5.167316226959229 and perplexity is 175.44335494481106
At time: 1040.7093830108643 and batch: 450, loss is 5.14310733795166 and perplexity is 171.24706493874933
At time: 1041.5413575172424 and batch: 500, loss is 5.139438953399658 and perplexity is 170.62001568323916
At time: 1042.3359484672546 and batch: 550, loss is 5.146792402267456 and perplexity is 171.87928555831542
At time: 1043.1310517787933 and batch: 600, loss is 5.1743771362304685 and perplexity is 176.68652835864765
At time: 1043.925793170929 and batch: 650, loss is 5.177947120666504 and perplexity is 177.31842377222668
At time: 1044.7197742462158 and batch: 700, loss is 5.1957165622711186 and perplexity is 180.4974341201597
At time: 1045.5135390758514 and batch: 750, loss is 5.160854063034058 and perplexity is 174.3132665664149
At time: 1046.3097653388977 and batch: 800, loss is 5.172041234970092 and perplexity is 176.27428773850227
At time: 1047.1055624485016 and batch: 850, loss is 5.195187196731568 and perplexity is 180.40191028430328
At time: 1047.902319431305 and batch: 900, loss is 5.167807474136352 and perplexity is 175.52956217046724
At time: 1048.713722705841 and batch: 950, loss is 5.150349826812744 and perplexity is 172.4918220277383
At time: 1049.5219972133636 and batch: 1000, loss is 5.151507415771484 and perplexity is 172.6916122715707
At time: 1050.3214790821075 and batch: 1050, loss is 5.142994995117188 and perplexity is 171.22782763868565
At time: 1051.1176586151123 and batch: 1100, loss is 5.110036478042603 and perplexity is 165.67639831421857
At time: 1051.912089586258 and batch: 1150, loss is 5.150469417572022 and perplexity is 172.51245168923765
At time: 1052.7088797092438 and batch: 1200, loss is 5.179119367599487 and perplexity is 177.52640663047202
At time: 1053.4994277954102 and batch: 1250, loss is 5.194549760818481 and perplexity is 180.28695227097433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.110737375969435 and perplexity of 165.79256126275135
Finished 49 epochs...
Completing Train Step...
At time: 1055.7138721942902 and batch: 50, loss is 5.179062118530274 and perplexity is 177.51624369984359
At time: 1056.5668215751648 and batch: 100, loss is 5.205425691604614 and perplexity is 182.2584421459847
At time: 1057.3794071674347 and batch: 150, loss is 5.129135837554932 and perplexity is 168.87112288974976
At time: 1058.1847569942474 and batch: 200, loss is 5.164127645492553 and perplexity is 174.88483043843976
At time: 1058.9805388450623 and batch: 250, loss is 5.188159780502319 and perplexity is 179.13859509204613
At time: 1059.7848706245422 and batch: 300, loss is 5.183272657394409 and perplexity is 178.26525851436622
At time: 1060.5887660980225 and batch: 350, loss is 5.191409311294556 and perplexity is 179.72165830073206
At time: 1061.3936669826508 and batch: 400, loss is 5.16592059135437 and perplexity is 175.19867073672685
At time: 1062.195868730545 and batch: 450, loss is 5.141848516464234 and perplexity is 171.03163107858046
At time: 1062.992546081543 and batch: 500, loss is 5.138593378067017 and perplexity is 170.47580458598839
At time: 1063.7949059009552 and batch: 550, loss is 5.146080226898193 and perplexity is 171.75692094235887
At time: 1064.6029987335205 and batch: 600, loss is 5.173670511245728 and perplexity is 176.56172134431287
At time: 1065.4091987609863 and batch: 650, loss is 5.1772160148620605 and perplexity is 177.18883262156564
At time: 1066.2139055728912 and batch: 700, loss is 5.194955730438233 and perplexity is 180.36015815511095
At time: 1067.0120282173157 and batch: 750, loss is 5.160723667144776 and perplexity is 174.29053831487485
At time: 1067.807894706726 and batch: 800, loss is 5.171817636489868 and perplexity is 176.2348774818622
At time: 1068.603828907013 and batch: 850, loss is 5.195491161346435 and perplexity is 180.45675441639972
At time: 1069.400503873825 and batch: 900, loss is 5.168293600082397 and perplexity is 175.61491238873325
At time: 1070.2247099876404 and batch: 950, loss is 5.1509644889831545 and perplexity is 172.59787881665287
At time: 1071.0288207530975 and batch: 1000, loss is 5.152233057022094 and perplexity is 172.81696990590393
At time: 1071.8251266479492 and batch: 1050, loss is 5.1437834358215335 and perplexity is 171.36288386262854
At time: 1072.6200602054596 and batch: 1100, loss is 5.111020956039429 and perplexity is 165.83958339583356
At time: 1073.4176089763641 and batch: 1150, loss is 5.151596431732178 and perplexity is 172.70698526555148
At time: 1074.230070590973 and batch: 1200, loss is 5.180233535766601 and perplexity is 177.7243111305362
At time: 1075.0263361930847 and batch: 1250, loss is 5.19475546836853 and perplexity is 180.3240424729683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.110679904909899 and perplexity of 165.78303326238725
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f17ffca5e80>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'anneal': 6.850360280317041, 'num_layers': 1, 'dropout': 0.45900980452330353, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 1.840699063396855, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.463388442993164 and batch: 50, loss is 7.619092626571655 and perplexity is 2036.7132316599398
At time: 2.2913265228271484 and batch: 100, loss is 6.689397268295288 and perplexity is 803.8376076917696
At time: 3.0889933109283447 and batch: 150, loss is 6.461786317825317 and perplexity is 640.2036429337375
At time: 3.886302947998047 and batch: 200, loss is 6.362303657531738 and perplexity is 579.5799734765776
At time: 4.682039737701416 and batch: 250, loss is 6.334349374771119 and perplexity is 563.6025896051293
At time: 5.479264974594116 and batch: 300, loss is 6.268565883636475 and perplexity is 527.7200230196213
At time: 6.275373220443726 and batch: 350, loss is 6.242155828475952 and perplexity is 513.965338561429
At time: 7.073875427246094 and batch: 400, loss is 6.158836469650269 and perplexity is 472.8775472412402
At time: 7.871092319488525 and batch: 450, loss is 6.102147083282471 and perplexity is 446.8160922776504
At time: 8.668471336364746 and batch: 500, loss is 6.070764541625977 and perplexity is 433.0116104546282
At time: 9.46679401397705 and batch: 550, loss is 6.049982118606567 and perplexity is 424.1054463806175
At time: 10.270187616348267 and batch: 600, loss is 6.043272895812988 and perplexity is 421.2695524129288
At time: 11.098233222961426 and batch: 650, loss is 5.992076568603515 and perplexity is 400.2448835275609
At time: 11.89626955986023 and batch: 700, loss is 5.977682247161865 and perplexity is 394.52489644811703
At time: 12.696660041809082 and batch: 750, loss is 5.909766159057617 and perplexity is 368.6199468944467
At time: 13.493469476699829 and batch: 800, loss is 5.8948897361755375 and perplexity is 363.17678842939864
At time: 14.293201208114624 and batch: 850, loss is 5.931394157409668 and perplexity is 376.67929815835066
At time: 15.092433452606201 and batch: 900, loss is 5.901411008834839 and perplexity is 365.5529025134383
At time: 15.890642881393433 and batch: 950, loss is 5.860325107574463 and perplexity is 350.83818563257284
At time: 16.713305234909058 and batch: 1000, loss is 5.833781538009643 and perplexity is 341.64819499363085
At time: 17.512502670288086 and batch: 1050, loss is 5.801667518615723 and perplexity is 330.8508000484273
At time: 18.31284999847412 and batch: 1100, loss is 5.7754953098297115 and perplexity is 322.3040355462458
At time: 19.144542455673218 and batch: 1150, loss is 5.803622102737426 and perplexity is 331.4981081718635
At time: 19.94576358795166 and batch: 1200, loss is 5.787561225891113 and perplexity is 326.2164851632511
At time: 20.743562936782837 and batch: 1250, loss is 5.7640841197967525 and perplexity is 318.64706780780557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.243943708656478 and perplexity of 189.41563143011808
Finished 1 epochs...
Completing Train Step...
At time: 23.04964804649353 and batch: 50, loss is 5.510970401763916 and perplexity is 247.39107936351968
At time: 23.860477685928345 and batch: 100, loss is 5.50268931388855 and perplexity is 245.3508713259168
At time: 24.664807081222534 and batch: 150, loss is 5.372798185348511 and perplexity is 215.46493578888987
At time: 25.496315717697144 and batch: 200, loss is 5.3766231346130375 and perplexity is 216.290656399581
At time: 26.289634227752686 and batch: 250, loss is 5.387979345321655 and perplexity is 218.76089841483773
At time: 27.089019298553467 and batch: 300, loss is 5.3698937129974365 and perplexity is 214.84003178776697
At time: 27.906012535095215 and batch: 350, loss is 5.3694957447052 and perplexity is 214.75454927800484
At time: 28.699843168258667 and batch: 400, loss is 5.330587434768677 and perplexity is 206.55927862564593
At time: 29.492828130722046 and batch: 450, loss is 5.281749620437622 and perplexity is 196.7137488353632
At time: 30.28669261932373 and batch: 500, loss is 5.270280714035034 and perplexity is 194.47054539542756
At time: 31.080838918685913 and batch: 550, loss is 5.26234733581543 and perplexity is 192.93384069584403
At time: 31.874354124069214 and batch: 600, loss is 5.268718404769897 and perplexity is 194.16695946986184
At time: 32.66739797592163 and batch: 650, loss is 5.234173927307129 and perplexity is 187.5740924909616
At time: 33.46286725997925 and batch: 700, loss is 5.230275726318359 and perplexity is 186.84431431329298
At time: 34.25761938095093 and batch: 750, loss is 5.212630825042725 and perplexity is 183.57638080219763
At time: 35.052043199539185 and batch: 800, loss is 5.207867326736451 and perplexity is 182.70399447816604
At time: 35.846221685409546 and batch: 850, loss is 5.24360918045044 and perplexity is 189.35227715622673
At time: 36.64117503166199 and batch: 900, loss is 5.217624702453613 and perplexity is 184.49543164708308
At time: 37.43614435195923 and batch: 950, loss is 5.183871412277222 and perplexity is 178.37202766942434
At time: 38.230469703674316 and batch: 1000, loss is 5.15943305015564 and perplexity is 174.0657410798489
At time: 39.037272691726685 and batch: 1050, loss is 5.135413436889649 and perplexity is 169.93456257021455
At time: 39.85221481323242 and batch: 1100, loss is 5.108597793579102 and perplexity is 165.4382136313795
At time: 40.64825701713562 and batch: 1150, loss is 5.131889696121216 and perplexity is 169.33681100341357
At time: 41.44463109970093 and batch: 1200, loss is 5.12821590423584 and perplexity is 168.71584415116445
At time: 42.25981020927429 and batch: 1250, loss is 5.115875034332276 and perplexity is 166.64653864520855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.850064688355383 and perplexity of 127.7486534290399
Finished 2 epochs...
Completing Train Step...
At time: 44.532049894332886 and batch: 50, loss is 5.063146390914917 and perplexity is 158.0871385663566
At time: 45.33666443824768 and batch: 100, loss is 5.091118841171265 and perplexity is 162.57165219513433
At time: 46.133121490478516 and batch: 150, loss is 4.9828455352783205 and perplexity is 145.88892365592852
At time: 46.93456196784973 and batch: 200, loss is 5.01754467010498 and perplexity is 151.03999512258466
At time: 47.746580362319946 and batch: 250, loss is 5.040930261611939 and perplexity is 154.6137794099412
At time: 48.54108381271362 and batch: 300, loss is 5.035799131393433 and perplexity is 153.8224678716562
At time: 49.33533072471619 and batch: 350, loss is 5.041222476959229 and perplexity is 154.65896653105094
At time: 50.13478398323059 and batch: 400, loss is 5.026404285430909 and perplexity is 152.3840966978139
At time: 50.9299259185791 and batch: 450, loss is 4.972462959289551 and perplexity is 144.38205693566115
At time: 51.72636365890503 and batch: 500, loss is 4.976324529647827 and perplexity is 144.94067628679002
At time: 52.52367067337036 and batch: 550, loss is 4.97705771446228 and perplexity is 145.0469835563157
At time: 53.31856179237366 and batch: 600, loss is 4.986363439559937 and perplexity is 146.40305071977642
At time: 54.11559224128723 and batch: 650, loss is 4.968309659957885 and perplexity is 143.7836386005423
At time: 54.91042923927307 and batch: 700, loss is 4.970684633255005 and perplexity is 144.12552672966837
At time: 55.70922899246216 and batch: 750, loss is 4.967190313339233 and perplexity is 143.62278491316778
At time: 56.504135608673096 and batch: 800, loss is 4.970744867324829 and perplexity is 144.134208258168
At time: 57.295220136642456 and batch: 850, loss is 5.007335767745972 and perplexity is 149.50588666571974
At time: 58.11044979095459 and batch: 900, loss is 4.983218631744385 and perplexity is 145.94336445298543
At time: 58.91681432723999 and batch: 950, loss is 4.956488084793091 and perplexity is 142.09389689202024
At time: 59.73168158531189 and batch: 1000, loss is 4.936694984436035 and perplexity is 139.30906923370023
At time: 60.548134326934814 and batch: 1050, loss is 4.9170436191558835 and perplexity is 136.5981794099543
At time: 61.36269497871399 and batch: 1100, loss is 4.892781505584717 and perplexity is 133.3238999530358
At time: 62.16315007209778 and batch: 1150, loss is 4.913385381698609 and perplexity is 136.0993837462009
At time: 63.009337186813354 and batch: 1200, loss is 4.920578308105469 and perplexity is 137.0818658221572
At time: 63.804731607437134 and batch: 1250, loss is 4.911602582931518 and perplexity is 135.85696209137893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.714382644987454 and perplexity of 111.53993017888709
Finished 3 epochs...
Completing Train Step...
At time: 66.03768944740295 and batch: 50, loss is 4.8666857147216795 and perplexity is 129.88971116425117
At time: 66.90174889564514 and batch: 100, loss is 4.895848655700684 and perplexity is 133.73345212616692
At time: 67.69642782211304 and batch: 150, loss is 4.794295349121094 and perplexity is 120.81921641361745
At time: 68.49644136428833 and batch: 200, loss is 4.835591993331909 and perplexity is 125.91310086382997
At time: 69.29344034194946 and batch: 250, loss is 4.862163810729981 and perplexity is 129.3036883289199
At time: 70.09000205993652 and batch: 300, loss is 4.862126684188842 and perplexity is 129.29888781932954
At time: 70.88585805892944 and batch: 350, loss is 4.866908588409424 and perplexity is 129.91866338940315
At time: 71.68315887451172 and batch: 400, loss is 4.858143014907837 and perplexity is 128.7848284130154
At time: 72.4802086353302 and batch: 450, loss is 4.800173921585083 and perplexity is 121.53155264102753
At time: 73.27723002433777 and batch: 500, loss is 4.80849437713623 and perplexity is 122.54696902819884
At time: 74.07594513893127 and batch: 550, loss is 4.811076965332031 and perplexity is 122.8638664154322
At time: 74.87450623512268 and batch: 600, loss is 4.826058483123779 and perplexity is 124.71841087456149
At time: 75.67616891860962 and batch: 650, loss is 4.814638967514038 and perplexity is 123.30228814171394
At time: 76.47649312019348 and batch: 700, loss is 4.815087003707886 and perplexity is 123.35754440706472
At time: 77.2745885848999 and batch: 750, loss is 4.8156625938415525 and perplexity is 123.42856823088404
At time: 78.07307314872742 and batch: 800, loss is 4.821652593612671 and perplexity is 124.1701240687294
At time: 78.88524961471558 and batch: 850, loss is 4.859728879928589 and perplexity is 128.98922579809312
At time: 79.69865107536316 and batch: 900, loss is 4.833125715255737 and perplexity is 125.60294676400505
At time: 80.50651574134827 and batch: 950, loss is 4.81075761795044 and perplexity is 122.82463642573089
At time: 81.3134617805481 and batch: 1000, loss is 4.792932024002075 and perplexity is 120.65461277065634
At time: 82.11962294578552 and batch: 1050, loss is 4.77747088432312 and perplexity is 118.8035019660897
At time: 82.92635416984558 and batch: 1100, loss is 4.755743837356567 and perplexity is 116.25009217339458
At time: 83.77102136611938 and batch: 1150, loss is 4.771646356582641 and perplexity is 118.11353897809282
At time: 84.56631255149841 and batch: 1200, loss is 4.784110841751098 and perplexity is 119.59497693353875
At time: 85.36162972450256 and batch: 1250, loss is 4.776300687789917 and perplexity is 118.66455983061435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6309449133211675 and perplexity of 102.61097679511104
Finished 4 epochs...
Completing Train Step...
At time: 87.62849521636963 and batch: 50, loss is 4.734091958999634 and perplexity is 113.7601129574455
At time: 88.42297267913818 and batch: 100, loss is 4.76308985710144 and perplexity is 117.10721200096738
At time: 89.21749019622803 and batch: 150, loss is 4.664384593963623 and perplexity is 106.10027037960957
At time: 90.01285433769226 and batch: 200, loss is 4.709482555389404 and perplexity is 110.99471142854522
At time: 90.80544352531433 and batch: 250, loss is 4.7383630657196045 and perplexity is 114.2470336452589
At time: 91.60113501548767 and batch: 300, loss is 4.739130325317383 and perplexity is 114.33472441483138
At time: 92.40073347091675 and batch: 350, loss is 4.742289972305298 and perplexity is 114.69655310688889
At time: 93.21937441825867 and batch: 400, loss is 4.739582681655884 and perplexity is 114.38645615183863
At time: 94.02043581008911 and batch: 450, loss is 4.675459852218628 and perplexity is 107.28188956558624
At time: 94.81403756141663 and batch: 500, loss is 4.688290691375732 and perplexity is 108.66727505795167
At time: 95.60883045196533 and batch: 550, loss is 4.690445156097412 and perplexity is 108.90164725111859
At time: 96.40461277961731 and batch: 600, loss is 4.709559593200684 and perplexity is 111.0032625475528
At time: 97.20008134841919 and batch: 650, loss is 4.703533744812011 and perplexity is 110.33638498623868
At time: 97.9976658821106 and batch: 700, loss is 4.701901865005493 and perplexity is 110.15647610241795
At time: 98.79331016540527 and batch: 750, loss is 4.703227615356445 and perplexity is 110.30261293834675
At time: 99.59609389305115 and batch: 800, loss is 4.712400188446045 and perplexity is 111.31902615333941
At time: 100.40122771263123 and batch: 850, loss is 4.751304063796997 and perplexity is 115.7351121311758
At time: 101.22305274009705 and batch: 900, loss is 4.721650943756104 and perplexity is 112.35358909209411
At time: 102.03302597999573 and batch: 950, loss is 4.703175945281982 and perplexity is 110.29691374136304
At time: 102.82797956466675 and batch: 1000, loss is 4.686545619964599 and perplexity is 108.47780826754352
At time: 103.67102313041687 and batch: 1050, loss is 4.67488655090332 and perplexity is 107.22040234422612
At time: 104.46848201751709 and batch: 1100, loss is 4.6538134765625 and perplexity is 104.98457940498939
At time: 105.27063345909119 and batch: 1150, loss is 4.6666168785095214 and perplexity is 106.3373809241337
At time: 106.07231569290161 and batch: 1200, loss is 4.680853481292725 and perplexity is 107.86209157491902
At time: 106.8747308254242 and batch: 1250, loss is 4.675299882888794 and perplexity is 107.26472912621767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.580738819428604 and perplexity of 97.58646635693664
Finished 5 epochs...
Completing Train Step...
At time: 109.1657931804657 and batch: 50, loss is 4.6332799911499025 and perplexity is 102.85086137759583
At time: 109.96216082572937 and batch: 100, loss is 4.6606946849823 and perplexity is 105.70949145173793
At time: 110.75788927078247 and batch: 150, loss is 4.565716886520386 and perplexity is 96.13148468573807
At time: 111.5531256198883 and batch: 200, loss is 4.612433404922485 and perplexity is 100.72896600862309
At time: 112.34945821762085 and batch: 250, loss is 4.641775960922241 and perplexity is 103.72840168637914
At time: 113.14331221580505 and batch: 300, loss is 4.644327802658081 and perplexity is 103.99343817275395
At time: 113.93569445610046 and batch: 350, loss is 4.646851110458374 and perplexity is 104.25617697249935
At time: 114.72837805747986 and batch: 400, loss is 4.64520004272461 and perplexity is 104.08418498693455
At time: 115.5237193107605 and batch: 450, loss is 4.578549909591675 and perplexity is 97.37309199459258
At time: 116.34123277664185 and batch: 500, loss is 4.594748964309693 and perplexity is 98.96328911153006
At time: 117.14029431343079 and batch: 550, loss is 4.597477569580078 and perplexity is 99.2336896041116
At time: 117.93481206893921 and batch: 600, loss is 4.6177705383300784 and perplexity is 101.26800712584952
At time: 118.73100090026855 and batch: 650, loss is 4.616434097290039 and perplexity is 101.13275880091985
At time: 119.52650427818298 and batch: 700, loss is 4.613176259994507 and perplexity is 100.80382083162229
At time: 120.32225751876831 and batch: 750, loss is 4.614761896133423 and perplexity is 100.96378580247723
At time: 121.11625218391418 and batch: 800, loss is 4.627086753845215 and perplexity is 102.21585000387387
At time: 121.91043639183044 and batch: 850, loss is 4.666810665130615 and perplexity is 106.3579896826653
At time: 122.70779800415039 and batch: 900, loss is 4.634392118453979 and perplexity is 102.96530825673533
At time: 123.50220823287964 and batch: 950, loss is 4.617553167343139 and perplexity is 101.24599679148594
At time: 124.3258650302887 and batch: 1000, loss is 4.601282262802124 and perplexity is 99.61196250019562
At time: 125.12220811843872 and batch: 1050, loss is 4.5941374969482425 and perplexity is 98.90279478729389
At time: 125.91755104064941 and batch: 1100, loss is 4.571688537597656 and perplexity is 96.70726584084056
At time: 126.71639347076416 and batch: 1150, loss is 4.581645936965942 and perplexity is 97.67502891422738
At time: 127.51186037063599 and batch: 1200, loss is 4.598677234649658 and perplexity is 99.35280823219762
At time: 128.30501222610474 and batch: 1250, loss is 4.595661382675171 and perplexity is 99.05362624038413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.544316869582573 and perplexity of 94.09612532324996
Finished 6 epochs...
Completing Train Step...
At time: 130.49567699432373 and batch: 50, loss is 4.551675939559937 and perplexity is 94.7911394864427
At time: 131.32817912101746 and batch: 100, loss is 4.577503576278686 and perplexity is 97.27126056874708
At time: 132.12679719924927 and batch: 150, loss is 4.4868631935119625 and perplexity is 88.84232714859186
At time: 132.92857313156128 and batch: 200, loss is 4.534811992645263 and perplexity is 93.20599024538383
At time: 133.72921705245972 and batch: 250, loss is 4.562831697463989 and perplexity is 95.85452690804584
At time: 134.52867221832275 and batch: 300, loss is 4.567413368225098 and perplexity is 96.29470840463347
At time: 135.3298215866089 and batch: 350, loss is 4.568222618103027 and perplexity is 96.37266642515998
At time: 136.13709044456482 and batch: 400, loss is 4.568716917037964 and perplexity is 96.42031510690713
At time: 136.93658638000488 and batch: 450, loss is 4.500763063430786 and perplexity is 90.08584629517709
At time: 137.73416924476624 and batch: 500, loss is 4.519207630157471 and perplexity is 91.76285904172381
At time: 138.53364491462708 and batch: 550, loss is 4.521126613616944 and perplexity is 91.93911951679705
At time: 139.33460545539856 and batch: 600, loss is 4.542974376678467 and perplexity is 93.96988669888819
At time: 140.13149070739746 and batch: 650, loss is 4.545544033050537 and perplexity is 94.21166753081178
At time: 140.92643523216248 and batch: 700, loss is 4.5402015018463135 and perplexity is 93.70968089087191
At time: 141.72338771820068 and batch: 750, loss is 4.541570310592651 and perplexity is 93.83803935074168
At time: 142.51769828796387 and batch: 800, loss is 4.555860815048217 and perplexity is 95.18865980895892
At time: 143.31263542175293 and batch: 850, loss is 4.597295818328857 and perplexity is 99.21565539578242
At time: 144.10943222045898 and batch: 900, loss is 4.562294368743896 and perplexity is 95.80303535297186
At time: 144.95233702659607 and batch: 950, loss is 4.547108249664307 and perplexity is 94.3591503038049
At time: 145.7494411468506 and batch: 1000, loss is 4.530654106140137 and perplexity is 92.8192548743929
At time: 146.54725980758667 and batch: 1050, loss is 4.526525011062622 and perplexity is 92.43678551557264
At time: 147.34184098243713 and batch: 1100, loss is 4.503669281005859 and perplexity is 90.34803617081761
At time: 148.13377833366394 and batch: 1150, loss is 4.511475143432617 and perplexity is 91.05604020777514
At time: 148.94459891319275 and batch: 1200, loss is 4.529934148788453 and perplexity is 92.75245301959696
At time: 149.75193810462952 and batch: 1250, loss is 4.529276361465454 and perplexity is 92.69146169368334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.516664379704608 and perplexity of 91.5297796242092
Finished 7 epochs...
Completing Train Step...
At time: 151.95757627487183 and batch: 50, loss is 4.483610324859619 and perplexity is 88.55380424557106
At time: 152.75443196296692 and batch: 100, loss is 4.507796154022217 and perplexity is 90.72166146517459
At time: 153.5521423816681 and batch: 150, loss is 4.42084753036499 and perplexity is 83.16674183618761
At time: 154.3465964794159 and batch: 200, loss is 4.4694513320922855 and perplexity is 87.30880632937588
At time: 155.15348768234253 and batch: 250, loss is 4.497053031921387 and perplexity is 89.75224418668893
At time: 155.9660861492157 and batch: 300, loss is 4.5027198982238765 and perplexity is 90.26230200460127
At time: 156.763601064682 and batch: 350, loss is 4.502700834274292 and perplexity is 90.26058126502859
At time: 157.5603530406952 and batch: 400, loss is 4.503774662017822 and perplexity is 90.35755763998036
At time: 158.3560869693756 and batch: 450, loss is 4.434944343566895 and perplexity is 84.34743028053828
At time: 159.15266060829163 and batch: 500, loss is 4.456381874084473 and perplexity is 86.17515181304074
At time: 159.9498269557953 and batch: 550, loss is 4.457424364089966 and perplexity is 86.26503539075306
At time: 160.74469447135925 and batch: 600, loss is 4.479927463531494 and perplexity is 88.2282726762153
At time: 161.5411069393158 and batch: 650, loss is 4.485186910629272 and perplexity is 88.693527026726
At time: 162.33980751037598 and batch: 700, loss is 4.477485475540161 and perplexity is 88.01308314590506
At time: 163.14360737800598 and batch: 750, loss is 4.478990201950073 and perplexity is 88.14561844620862
At time: 163.94235706329346 and batch: 800, loss is 4.494718904495239 and perplexity is 89.54299531374274
At time: 164.74229311943054 and batch: 850, loss is 4.538463468551636 and perplexity is 93.5469518007021
At time: 165.57277417182922 and batch: 900, loss is 4.500805549621582 and perplexity is 90.08967378093786
At time: 166.38522362709045 and batch: 950, loss is 4.488005466461182 and perplexity is 88.94386731789304
At time: 167.18228459358215 and batch: 1000, loss is 4.471483726501464 and perplexity is 87.48643270151226
At time: 167.98058986663818 and batch: 1050, loss is 4.469090213775635 and perplexity is 87.27728321233549
At time: 168.77771997451782 and batch: 1100, loss is 4.4453268814086915 and perplexity is 85.22773264577187
At time: 169.57733392715454 and batch: 1150, loss is 4.452000379562378 and perplexity is 85.79840182415859
At time: 170.37427830696106 and batch: 1200, loss is 4.470725269317627 and perplexity is 87.42010314538535
At time: 171.17080926895142 and batch: 1250, loss is 4.472626705169677 and perplexity is 87.5864849957748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.495940493841241 and perplexity of 89.65244692167511
Finished 8 epochs...
Completing Train Step...
At time: 173.38392639160156 and batch: 50, loss is 4.4256356334686275 and perplexity is 83.56590763178154
At time: 174.20563459396362 and batch: 100, loss is 4.448361091613769 and perplexity is 85.4867242206621
At time: 175.00110459327698 and batch: 150, loss is 4.364241514205933 and perplexity is 78.58976810701779
At time: 175.7976360321045 and batch: 200, loss is 4.414319849014282 and perplexity is 82.62562389071135
At time: 176.59471130371094 and batch: 250, loss is 4.4408932876586915 and perplexity is 84.85070391669184
At time: 177.39035439491272 and batch: 300, loss is 4.446438512802124 and perplexity is 85.32252714748233
At time: 178.18634462356567 and batch: 350, loss is 4.445072774887085 and perplexity is 85.20607847443442
At time: 178.98246479034424 and batch: 400, loss is 4.447359285354614 and perplexity is 85.40112596885437
At time: 179.77936625480652 and batch: 450, loss is 4.378370199203491 and perplexity is 79.70801929198095
At time: 180.5747528076172 and batch: 500, loss is 4.403516321182251 and perplexity is 81.73778022073121
At time: 181.37077736854553 and batch: 550, loss is 4.402840518951416 and perplexity is 81.68256030748644
At time: 182.16863012313843 and batch: 600, loss is 4.424790992736816 and perplexity is 83.49535426272409
At time: 182.9659605026245 and batch: 650, loss is 4.432901296615601 and perplexity is 84.17528043524248
At time: 183.7612988948822 and batch: 700, loss is 4.423754863739013 and perplexity is 83.40888710828936
At time: 184.57344818115234 and batch: 750, loss is 4.424822368621826 and perplexity is 83.49797404445708
At time: 185.4526515007019 and batch: 800, loss is 4.442194900512695 and perplexity is 84.96121859163901
At time: 186.26011896133423 and batch: 850, loss is 4.487194032669067 and perplexity is 88.87172453185553
At time: 187.0763533115387 and batch: 900, loss is 4.44754732131958 and perplexity is 85.41718596186591
At time: 187.873220205307 and batch: 950, loss is 4.437143630981446 and perplexity is 84.53313866060164
At time: 188.66873288154602 and batch: 1000, loss is 4.420629234313965 and perplexity is 83.14858884630318
At time: 189.46468997001648 and batch: 1050, loss is 4.419156923294067 and perplexity is 83.02625833903079
At time: 190.26094698905945 and batch: 1100, loss is 4.394189729690551 and perplexity is 80.97898930193494
At time: 191.05829548835754 and batch: 1150, loss is 4.40031831741333 and perplexity is 81.47680002177727
At time: 191.85476183891296 and batch: 1200, loss is 4.41922854423523 and perplexity is 83.03220497074338
At time: 192.65192294120789 and batch: 1250, loss is 4.4227357006073 and perplexity is 83.32392314911297
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.479380809477646 and perplexity of 88.18005551354726
Finished 9 epochs...
Completing Train Step...
At time: 194.89388990402222 and batch: 50, loss is 4.375230340957642 and perplexity is 79.45813990855859
At time: 195.689297914505 and batch: 100, loss is 4.396713161468506 and perplexity is 81.18359229916766
At time: 196.48845601081848 and batch: 150, loss is 4.315183992385864 and perplexity is 74.82738984743344
At time: 197.28340244293213 and batch: 200, loss is 4.365696249008178 and perplexity is 78.7041785760522
At time: 198.0796823501587 and batch: 250, loss is 4.390268125534058 and perplexity is 80.66204363481422
At time: 198.87429666519165 and batch: 300, loss is 4.397508573532105 and perplexity is 81.24819239629102
At time: 199.69952082633972 and batch: 350, loss is 4.394536304473877 and perplexity is 81.00705944152648
At time: 200.49793982505798 and batch: 400, loss is 4.398226842880249 and perplexity is 81.30657144591252
At time: 201.2965362071991 and batch: 450, loss is 4.33136028289795 and perplexity is 76.04766257777233
At time: 202.098375082016 and batch: 500, loss is 4.3559691429138185 and perplexity is 77.94232599838094
At time: 202.89978122711182 and batch: 550, loss is 4.354794039726257 and perplexity is 77.85078951559395
At time: 203.70223808288574 and batch: 600, loss is 4.3765212917327885 and perplexity is 79.56078269500264
At time: 204.5025143623352 and batch: 650, loss is 4.387288236618042 and perplexity is 80.42203747848549
At time: 205.30515336990356 and batch: 700, loss is 4.376023035049439 and perplexity is 79.52115087752122
At time: 206.1369707584381 and batch: 750, loss is 4.378205976486206 and perplexity is 79.69493049923136
At time: 206.9365155696869 and batch: 800, loss is 4.396254835128784 and perplexity is 81.14639224599695
At time: 207.73830437660217 and batch: 850, loss is 4.442445249557495 and perplexity is 84.98249121423771
At time: 208.5363404750824 and batch: 900, loss is 4.400829620361328 and perplexity is 81.51847000190592
At time: 209.3382167816162 and batch: 950, loss is 4.391913089752197 and perplexity is 80.79483900222506
At time: 210.17174530029297 and batch: 1000, loss is 4.3760151863098145 and perplexity is 79.5205267391627
At time: 210.9894380569458 and batch: 1050, loss is 4.375470247268677 and perplexity is 79.47720470457675
At time: 211.8027937412262 and batch: 1100, loss is 4.348621854782104 and perplexity is 77.37175989511415
At time: 212.59950065612793 and batch: 1150, loss is 4.354386177062988 and perplexity is 77.81904355967968
At time: 213.39434337615967 and batch: 1200, loss is 4.374994201660156 and perplexity is 79.43937893440989
At time: 214.1911814212799 and batch: 1250, loss is 4.37929328918457 and perplexity is 79.78163093585948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.466168702953923 and perplexity of 87.02267378813083
Finished 10 epochs...
Completing Train Step...
At time: 216.40128874778748 and batch: 50, loss is 4.331469621658325 and perplexity is 76.05597798951815
At time: 217.2237844467163 and batch: 100, loss is 4.351047849655151 and perplexity is 77.5596912560256
At time: 218.02004075050354 and batch: 150, loss is 4.272023391723633 and perplexity is 71.66649841245656
At time: 218.81704568862915 and batch: 200, loss is 4.323326826095581 and perplexity is 75.439184329807
At time: 219.62031483650208 and batch: 250, loss is 4.3462505054473874 and perplexity is 77.18850179412516
At time: 220.419260263443 and batch: 300, loss is 4.354758462905884 and perplexity is 77.84801988130711
At time: 221.21528124809265 and batch: 350, loss is 4.350425701141358 and perplexity is 77.51145261673526
At time: 222.01205825805664 and batch: 400, loss is 4.355520362854004 and perplexity is 77.90735488441355
At time: 222.8142375946045 and batch: 450, loss is 4.286497602462768 and perplexity is 72.71135793015655
At time: 223.61113119125366 and batch: 500, loss is 4.313739128112793 and perplexity is 74.71935249364722
At time: 224.41046333312988 and batch: 550, loss is 4.311799335479736 and perplexity is 74.57455293010082
At time: 225.20744800567627 and batch: 600, loss is 4.333868255615235 and perplexity is 76.23862740792644
At time: 226.00401973724365 and batch: 650, loss is 4.346332607269287 and perplexity is 77.19483937091188
At time: 226.83580994606018 and batch: 700, loss is 4.334710464477539 and perplexity is 76.30286330180193
At time: 227.63409090042114 and batch: 750, loss is 4.336516704559326 and perplexity is 76.44080913599609
At time: 228.42988181114197 and batch: 800, loss is 4.35478554725647 and perplexity is 77.85012837292346
At time: 229.22935700416565 and batch: 850, loss is 4.402518787384033 and perplexity is 81.65628467640748
At time: 230.0290777683258 and batch: 900, loss is 4.359697093963623 and perplexity is 78.2334334544424
At time: 230.82692956924438 and batch: 950, loss is 4.351792669296264 and perplexity is 77.61748075613755
At time: 231.62257361412048 and batch: 1000, loss is 4.335626211166382 and perplexity is 76.37276939947326
At time: 232.41838145256042 and batch: 1050, loss is 4.335576820373535 and perplexity is 76.36899738099298
At time: 233.2158019542694 and batch: 1100, loss is 4.307690544128418 and perplexity is 74.2687702807351
At time: 234.01041626930237 and batch: 1150, loss is 4.313316736221314 and perplexity is 74.68779830960219
At time: 234.80719876289368 and batch: 1200, loss is 4.33526351928711 and perplexity is 76.34507463885053
At time: 235.60458707809448 and batch: 1250, loss is 4.340030097961426 and perplexity is 76.70984811360023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.456145126454151 and perplexity of 86.15475246490038
Finished 11 epochs...
Completing Train Step...
At time: 237.793452501297 and batch: 50, loss is 4.292737035751343 and perplexity is 73.16645389130197
At time: 238.63472366333008 and batch: 100, loss is 4.309987154006958 and perplexity is 74.4395326844845
At time: 239.42962408065796 and batch: 150, loss is 4.232970600128174 and perplexity is 68.92166708940731
At time: 240.22738909721375 and batch: 200, loss is 4.284707641601562 and perplexity is 72.581323857965
At time: 241.02501940727234 and batch: 250, loss is 4.306569814682007 and perplexity is 74.18558170758182
At time: 241.82265400886536 and batch: 300, loss is 4.3168794059753415 and perplexity is 74.95436082479205
At time: 242.61904978752136 and batch: 350, loss is 4.3104242420196535 and perplexity is 74.47207642361258
At time: 243.41366052627563 and batch: 400, loss is 4.316662015914917 and perplexity is 74.93806826274798
At time: 244.21083641052246 and batch: 450, loss is 4.247005977630615 and perplexity is 69.89582907896448
At time: 245.00661659240723 and batch: 500, loss is 4.27635238647461 and perplexity is 71.97741480005726
At time: 245.80278873443604 and batch: 550, loss is 4.273732032775879 and perplexity is 71.7890554068541
At time: 246.60010647773743 and batch: 600, loss is 4.296043720245361 and perplexity is 73.40879271807273
At time: 247.44674229621887 and batch: 650, loss is 4.310154142379761 and perplexity is 74.45196425885503
At time: 248.2453715801239 and batch: 700, loss is 4.2962579345703125 and perplexity is 73.42451961745374
At time: 249.04096865653992 and batch: 750, loss is 4.298155069351196 and perplexity is 73.56394804282901
At time: 249.83535432815552 and batch: 800, loss is 4.317679114341736 and perplexity is 75.01432642854307
At time: 250.63267922401428 and batch: 850, loss is 4.36620774269104 and perplexity is 78.74444556348564
At time: 251.42763257026672 and batch: 900, loss is 4.32258369922638 and perplexity is 75.38314426995696
At time: 252.22456192970276 and batch: 950, loss is 4.315369491577148 and perplexity is 74.841271555219
At time: 253.02162408828735 and batch: 1000, loss is 4.2989083194732665 and perplexity is 73.619380970481
At time: 253.81773447990417 and batch: 1050, loss is 4.299673748016358 and perplexity is 73.67575291759692
At time: 254.60980367660522 and batch: 1100, loss is 4.271535778045655 and perplexity is 71.6315613661609
At time: 255.40496110916138 and batch: 1150, loss is 4.276149616241455 and perplexity is 71.96282140247972
At time: 256.20020174980164 and batch: 1200, loss is 4.298486375808716 and perplexity is 73.5883242916376
At time: 256.99766063690186 and batch: 1250, loss is 4.304982852935791 and perplexity is 74.06794539414423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.448022104527828 and perplexity of 85.45775023631259
Finished 12 epochs...
Completing Train Step...
At time: 259.2269628047943 and batch: 50, loss is 4.2574386882781985 and perplexity is 70.62884908345308
At time: 260.03547859191895 and batch: 100, loss is 4.272870187759399 and perplexity is 71.72721102117978
At time: 260.8319728374481 and batch: 150, loss is 4.1979458093643185 and perplexity is 66.5494852060099
At time: 261.63125467300415 and batch: 200, loss is 4.249475440979004 and perplexity is 70.06864756371306
At time: 262.43268060684204 and batch: 250, loss is 4.270665545463562 and perplexity is 71.5692523631484
At time: 263.2401599884033 and batch: 300, loss is 4.280812091827393 and perplexity is 72.29912970574934
At time: 264.0418517589569 and batch: 350, loss is 4.274403123855591 and perplexity is 71.83724857075111
At time: 264.84077501296997 and batch: 400, loss is 4.2814237499237064 and perplexity is 72.34336558102677
At time: 265.6359136104584 and batch: 450, loss is 4.21106635093689 and perplexity is 67.42840382877084
At time: 266.4306592941284 and batch: 500, loss is 4.24248480796814 and perplexity is 69.58053147107118
At time: 267.2546739578247 and batch: 550, loss is 4.239720373153687 and perplexity is 69.38844625332392
At time: 268.0506076812744 and batch: 600, loss is 4.262619676589966 and perplexity is 70.99572589660697
At time: 268.8473825454712 and batch: 650, loss is 4.277261791229248 and perplexity is 72.04290117560042
At time: 269.64297223091125 and batch: 700, loss is 4.261866312026978 and perplexity is 70.94226037463638
At time: 270.4378733634949 and batch: 750, loss is 4.263167381286621 and perplexity is 71.0346212397317
At time: 271.23238587379456 and batch: 800, loss is 4.2845691251754765 and perplexity is 72.57127084865317
At time: 272.02751874923706 and batch: 850, loss is 4.33321439743042 and perplexity is 76.18879445101554
At time: 272.8248817920685 and batch: 900, loss is 4.288500666618347 and perplexity is 72.8571494110285
At time: 273.6203248500824 and batch: 950, loss is 4.282314224243164 and perplexity is 72.4078141809127
At time: 274.4166190624237 and batch: 1000, loss is 4.265833826065063 and perplexity is 71.22428388473575
At time: 275.21362948417664 and batch: 1050, loss is 4.267670593261719 and perplexity is 71.35522653174627
At time: 276.01059007644653 and batch: 1100, loss is 4.238109321594238 and perplexity is 69.2767478888512
At time: 276.8066782951355 and batch: 1150, loss is 4.242803287506104 and perplexity is 69.60269497570941
At time: 277.60330390930176 and batch: 1200, loss is 4.266709318161011 and perplexity is 71.28666748650683
At time: 278.39759039878845 and batch: 1250, loss is 4.27327374458313 and perplexity is 71.75616286809931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4391555229242705 and perplexity of 84.70338139874679
Finished 13 epochs...
Completing Train Step...
At time: 280.59045791625977 and batch: 50, loss is 4.225225076675415 and perplexity is 68.38989478653937
At time: 281.4139726161957 and batch: 100, loss is 4.239526824951172 and perplexity is 69.37501754386949
At time: 282.20829796791077 and batch: 150, loss is 4.166085667610169 and perplexity is 64.46262945585565
At time: 283.0073661804199 and batch: 200, loss is 4.217332553863526 and perplexity is 67.85225045780355
At time: 283.8028016090393 and batch: 250, loss is 4.237876505851745 and perplexity is 69.26062104871797
At time: 284.60172033309937 and batch: 300, loss is 4.248475761413574 and perplexity is 69.9986363687773
At time: 285.4034893512726 and batch: 350, loss is 4.24093258857727 and perplexity is 69.4726110006867
At time: 286.20606660842896 and batch: 400, loss is 4.248991947174073 and perplexity is 70.03477799521916
At time: 287.015634059906 and batch: 450, loss is 4.178372106552124 and perplexity is 65.25953113383663
At time: 287.8642590045929 and batch: 500, loss is 4.2116827869415285 and perplexity is 67.46998193843149
At time: 288.65941762924194 and batch: 550, loss is 4.208588614463806 and perplexity is 67.26154081993076
At time: 289.46124029159546 and batch: 600, loss is 4.231451625823975 and perplexity is 68.81705631874611
At time: 290.2737355232239 and batch: 650, loss is 4.247160882949829 and perplexity is 69.90665715332116
At time: 291.095662355423 and batch: 700, loss is 4.230553808212281 and perplexity is 68.75529888120418
At time: 291.9048480987549 and batch: 750, loss is 4.231318435668945 and perplexity is 68.80789117471338
At time: 292.6992826461792 and batch: 800, loss is 4.257858080863953 and perplexity is 70.65847651142657
At time: 293.49518489837646 and batch: 850, loss is 4.303299655914307 and perplexity is 73.94337931310719
At time: 294.2880048751831 and batch: 900, loss is 4.257317643165589 and perplexity is 70.6203003238655
At time: 295.08059453964233 and batch: 950, loss is 4.252291350364685 and perplexity is 70.26623258622944
At time: 295.87435841560364 and batch: 1000, loss is 4.235365743637085 and perplexity is 69.08694222277997
At time: 296.67244505882263 and batch: 1050, loss is 4.237232365608215 and perplexity is 69.2160218610165
At time: 297.46961736679077 and batch: 1100, loss is 4.207160387039185 and perplexity is 67.16554461123644
At time: 298.26661467552185 and batch: 1150, loss is 4.213497734069824 and perplexity is 67.59254757983162
At time: 299.06170129776 and batch: 1200, loss is 4.236139793395996 and perplexity is 69.1404396559179
At time: 299.85772466659546 and batch: 1250, loss is 4.243336114883423 and perplexity is 69.63979107915978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.435345058023494 and perplexity of 84.38123628805432
Finished 14 epochs...
Completing Train Step...
At time: 302.11033391952515 and batch: 50, loss is 4.195715398788452 and perplexity is 66.40121794029251
At time: 302.9081678390503 and batch: 100, loss is 4.208962512016297 and perplexity is 67.28669444756694
At time: 303.70728969573975 and batch: 150, loss is 4.137278413772583 and perplexity is 62.63213053568167
At time: 304.51637864112854 and batch: 200, loss is 4.1872785282135006 and perplexity is 65.84335607178599
At time: 305.3134379386902 and batch: 250, loss is 4.208358693122864 and perplexity is 67.24607773398644
At time: 306.11104130744934 and batch: 300, loss is 4.218810157775879 and perplexity is 67.95258331639934
At time: 306.9086585044861 and batch: 350, loss is 4.210689239501953 and perplexity is 67.40298060064376
At time: 307.70860171318054 and batch: 400, loss is 4.219514312744141 and perplexity is 68.00044931611077
At time: 308.5553460121155 and batch: 450, loss is 4.147976145744324 and perplexity is 63.3057489514586
At time: 309.352650642395 and batch: 500, loss is 4.183487215042114 and perplexity is 65.59419590921925
At time: 310.1508228778839 and batch: 550, loss is 4.179982047080994 and perplexity is 65.3646797166753
At time: 310.94778776168823 and batch: 600, loss is 4.203161029815674 and perplexity is 66.89746204153374
At time: 311.743976354599 and batch: 650, loss is 4.219560890197754 and perplexity is 68.00361667764771
At time: 312.54077339172363 and batch: 700, loss is 4.202545795440674 and perplexity is 66.85631708148024
At time: 313.3374879360199 and batch: 750, loss is 4.202339100837707 and perplexity is 66.8424996696067
At time: 314.1343152523041 and batch: 800, loss is 4.228390498161316 and perplexity is 68.60672062044421
At time: 314.9315838813782 and batch: 850, loss is 4.277058095932007 and perplexity is 72.02822786992377
At time: 315.7266149520874 and batch: 900, loss is 4.229350228309631 and perplexity is 68.67259616491732
At time: 316.52216362953186 and batch: 950, loss is 4.225644097328186 and perplexity is 68.41855756962542
At time: 317.3186228275299 and batch: 1000, loss is 4.207881927490234 and perplexity is 67.21402475667992
At time: 318.11548829078674 and batch: 1050, loss is 4.209752507209778 and perplexity is 67.33987161485344
At time: 318.91202211380005 and batch: 1100, loss is 4.178895754814148 and perplexity is 65.29371312278383
At time: 319.71956038475037 and batch: 1150, loss is 4.185038781166076 and perplexity is 65.69604863670833
At time: 320.5314345359802 and batch: 1200, loss is 4.207274017333984 and perplexity is 67.1731770855029
At time: 321.3309705257416 and batch: 1250, loss is 4.217008275985718 and perplexity is 67.83025104117544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.430907312100821 and perplexity of 84.0076034577323
Finished 15 epochs...
Completing Train Step...
At time: 323.517466545105 and batch: 50, loss is 4.168490200042725 and perplexity is 64.61781844277388
At time: 324.357923746109 and batch: 100, loss is 4.181237683296204 and perplexity is 65.4468055250026
At time: 325.1561415195465 and batch: 150, loss is 4.109434318542481 and perplexity is 60.91225089109598
At time: 325.95125126838684 and batch: 200, loss is 4.159797334671021 and perplexity is 64.0585388394542
At time: 326.74842619895935 and batch: 250, loss is 4.179784550666809 and perplexity is 65.35177170150219
At time: 327.54755997657776 and batch: 300, loss is 4.190745620727539 and perplexity is 66.07203727909643
At time: 328.3428466320038 and batch: 350, loss is 4.184418001174927 and perplexity is 65.65527850016888
At time: 329.1902222633362 and batch: 400, loss is 4.191668014526368 and perplexity is 66.13300983258824
At time: 330.0161566734314 and batch: 450, loss is 4.121453528404236 and perplexity is 61.64878543260711
At time: 330.8145270347595 and batch: 500, loss is 4.157538108825683 and perplexity is 63.91397949048936
At time: 331.6392979621887 and batch: 550, loss is 4.153272075653076 and perplexity is 63.641901094152075
At time: 332.4365348815918 and batch: 600, loss is 4.176801223754882 and perplexity is 65.15709653646046
At time: 333.2337861061096 and batch: 650, loss is 4.193703880310059 and perplexity is 66.2677849099124
At time: 334.02892446517944 and batch: 700, loss is 4.176213731765747 and perplexity is 65.11882850639121
At time: 334.82549381256104 and batch: 750, loss is 4.175154004096985 and perplexity is 65.04985683411506
At time: 335.62260603904724 and batch: 800, loss is 4.2019898176193236 and perplexity is 66.81915678307386
At time: 336.44159865379333 and batch: 850, loss is 4.254234657287598 and perplexity is 70.40291420661663
At time: 337.2445616722107 and batch: 900, loss is 4.202489848136902 and perplexity is 66.85257675543096
At time: 338.0417354106903 and batch: 950, loss is 4.199356474876404 and perplexity is 66.64343051676293
At time: 338.85507225990295 and batch: 1000, loss is 4.181596655845642 and perplexity is 65.47030334891902
At time: 339.65408754348755 and batch: 1050, loss is 4.183561768531799 and perplexity is 65.59908636772546
At time: 340.4539647102356 and batch: 1100, loss is 4.151955590248108 and perplexity is 63.55817258598943
At time: 341.2507965564728 and batch: 1150, loss is 4.158438730239868 and perplexity is 63.97156771778703
At time: 342.04662919044495 and batch: 1200, loss is 4.182181882858276 and perplexity is 65.50862955263517
At time: 342.84604239463806 and batch: 1250, loss is 4.192621898651123 and perplexity is 66.19612315740362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4278439709739965 and perplexity of 83.7506532750616
Finished 16 epochs...
Completing Train Step...
At time: 345.0558350086212 and batch: 50, loss is 4.143960537910462 and perplexity is 63.052047613494025
At time: 345.88526129722595 and batch: 100, loss is 4.155492396354675 and perplexity is 63.78336351249311
At time: 346.68319368362427 and batch: 150, loss is 4.084716744422913 and perplexity is 59.425102875261786
At time: 347.4816794395447 and batch: 200, loss is 4.134218473434448 and perplexity is 62.440772873817025
At time: 348.277468919754 and batch: 250, loss is 4.154078454971313 and perplexity is 63.693241304027424
At time: 349.120228767395 and batch: 300, loss is 4.16543327331543 and perplexity is 64.42058811942994
At time: 349.9433732032776 and batch: 350, loss is 4.159206352233887 and perplexity is 64.0206925523986
At time: 350.7387912273407 and batch: 400, loss is 4.166259551048279 and perplexity is 64.47383941407986
At time: 351.5345652103424 and batch: 450, loss is 4.095945749282837 and perplexity is 60.09614817871196
At time: 352.3315408229828 and batch: 500, loss is 4.133758244514465 and perplexity is 62.41204243615068
At time: 353.1497495174408 and batch: 550, loss is 4.128371725082397 and perplexity is 62.07676256321568
At time: 353.95046854019165 and batch: 600, loss is 4.152841539382934 and perplexity is 63.61450684496236
At time: 354.74937677383423 and batch: 650, loss is 4.169551610946655 and perplexity is 64.68644091173317
At time: 355.54947113990784 and batch: 700, loss is 4.151256318092346 and perplexity is 63.51374366139429
At time: 356.34891843795776 and batch: 750, loss is 4.150477018356323 and perplexity is 63.464266698920994
At time: 357.1500346660614 and batch: 800, loss is 4.1773483848571775 and perplexity is 65.19275772054038
At time: 357.95249819755554 and batch: 850, loss is 4.229670763015747 and perplexity is 68.69461164352126
At time: 358.751339673996 and batch: 900, loss is 4.177755336761475 and perplexity is 65.21929343645502
At time: 359.54357409477234 and batch: 950, loss is 4.174855322837829 and perplexity is 65.03043056224405
At time: 360.3372983932495 and batch: 1000, loss is 4.157707962989807 and perplexity is 63.924836468076876
At time: 361.1291711330414 and batch: 1050, loss is 4.15940333366394 and perplexity is 64.03330468210748
At time: 361.9225707054138 and batch: 1100, loss is 4.1273029088974 and perplexity is 62.01044935930155
At time: 362.71834564208984 and batch: 1150, loss is 4.133767385482788 and perplexity is 62.41261294526104
At time: 363.5133559703827 and batch: 1200, loss is 4.157382125854492 and perplexity is 63.90401077556289
At time: 364.30767607688904 and batch: 1250, loss is 4.170219488143921 and perplexity is 64.72965794081809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.426020044479927 and perplexity of 83.59803746197902
Finished 17 epochs...
Completing Train Step...
At time: 366.57143354415894 and batch: 50, loss is 4.11892294883728 and perplexity is 61.4929755037233
At time: 367.367538690567 and batch: 100, loss is 4.131579027175904 and perplexity is 62.27618112057304
At time: 368.16433024406433 and batch: 150, loss is 4.06176381111145 and perplexity is 58.076657045962484
At time: 368.985680103302 and batch: 200, loss is 4.111255059242248 and perplexity is 61.023257331713616
At time: 369.83629965782166 and batch: 250, loss is 4.130337839126587 and perplexity is 62.198932618684864
At time: 370.6299707889557 and batch: 300, loss is 4.142327527999878 and perplexity is 62.949167020235635
At time: 371.4256315231323 and batch: 350, loss is 4.134977140426636 and perplexity is 62.48816260140111
At time: 372.2210223674774 and batch: 400, loss is 4.142671222686768 and perplexity is 62.97080603287842
At time: 373.0150201320648 and batch: 450, loss is 4.0742576313018795 and perplexity is 58.806808046259064
At time: 373.81198596954346 and batch: 500, loss is 4.11082706451416 and perplexity is 60.99714528759207
At time: 374.60898089408875 and batch: 550, loss is 4.108935208320617 and perplexity is 60.881856549733726
At time: 375.4066262245178 and batch: 600, loss is 4.129874558448791 and perplexity is 62.1701237288394
At time: 376.2022879123688 and batch: 650, loss is 4.147514595985412 and perplexity is 63.276536940209276
At time: 377.0044276714325 and batch: 700, loss is 4.128419008255005 and perplexity is 62.07969781888843
At time: 377.8113327026367 and batch: 750, loss is 4.127462015151978 and perplexity is 62.02031639457648
At time: 378.60599732398987 and batch: 800, loss is 4.153987736701965 and perplexity is 63.687463425490684
At time: 379.40467381477356 and batch: 850, loss is 4.206500730514526 and perplexity is 67.12125303172222
At time: 380.201452255249 and batch: 900, loss is 4.154943399429321 and perplexity is 63.74835625235436
At time: 380.9975872039795 and batch: 950, loss is 4.152421760559082 and perplexity is 63.58780842620649
At time: 381.79412722587585 and batch: 1000, loss is 4.135543479919433 and perplexity is 62.5235621388465
At time: 382.5904235839844 and batch: 1050, loss is 4.136643352508545 and perplexity is 62.59236792287827
At time: 383.3882534503937 and batch: 1100, loss is 4.103850421905517 and perplexity is 60.57307103213131
At time: 384.183447599411 and batch: 1150, loss is 4.109578824043274 and perplexity is 60.921053682425985
At time: 384.9810960292816 and batch: 1200, loss is 4.134644713401794 and perplexity is 62.46739329974955
At time: 385.7766239643097 and batch: 1250, loss is 4.147843561172485 and perplexity is 63.29735614222974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.426062813640511 and perplexity of 83.60161295632768
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 387.9898397922516 and batch: 50, loss is 4.127987294197083 and perplexity is 62.05290292491083
At time: 388.8179430961609 and batch: 100, loss is 4.149069423675537 and perplexity is 63.37499757680558
At time: 389.6337766647339 and batch: 150, loss is 4.075382876396179 and perplexity is 58.87301736239257
At time: 390.48515009880066 and batch: 200, loss is 4.129601073265076 and perplexity is 62.15312344589852
At time: 391.3013458251953 and batch: 250, loss is 4.146119089126587 and perplexity is 63.18829568386609
At time: 392.09932231903076 and batch: 300, loss is 4.147353343963623 and perplexity is 63.26633429331666
At time: 392.9017107486725 and batch: 350, loss is 4.137512803077698 and perplexity is 62.64681255782307
At time: 393.721168756485 and batch: 400, loss is 4.148404250144958 and perplexity is 63.3328562231273
At time: 394.5178527832031 and batch: 450, loss is 4.078989605903626 and perplexity is 59.08573979684667
At time: 395.3129391670227 and batch: 500, loss is 4.104283323287964 and perplexity is 60.599298874945276
At time: 396.1370539665222 and batch: 550, loss is 4.099229145050049 and perplexity is 60.293791911003424
At time: 396.9323785305023 and batch: 600, loss is 4.110508580207824 and perplexity is 60.97772174729726
At time: 397.7264664173126 and batch: 650, loss is 4.12670783996582 and perplexity is 61.97355984444518
At time: 398.5211663246155 and batch: 700, loss is 4.102479457855225 and perplexity is 60.490084428153864
At time: 399.3379783630371 and batch: 750, loss is 4.094253268241882 and perplexity is 59.99452261121701
At time: 400.13263416290283 and batch: 800, loss is 4.112173304557801 and perplexity is 61.079317386400476
At time: 400.92960476875305 and batch: 850, loss is 4.154649987220764 and perplexity is 63.729654450157184
At time: 401.7244954109192 and batch: 900, loss is 4.099989290237427 and perplexity is 60.33964137066868
At time: 402.52171063423157 and batch: 950, loss is 4.089587697982788 and perplexity is 59.715265903270094
At time: 403.31822299957275 and batch: 1000, loss is 4.066249794960022 and perplexity is 58.337773235060375
At time: 404.1147429943085 and batch: 1050, loss is 4.0618046855926515 and perplexity is 58.07903094770482
At time: 404.9092426300049 and batch: 1100, loss is 4.0161836004257205 and perplexity is 55.48893327537151
At time: 405.72166752815247 and batch: 1150, loss is 4.0114277696609495 and perplexity is 55.225663828081814
At time: 406.5430266857147 and batch: 1200, loss is 4.0333893728256225 and perplexity is 56.451923974667245
At time: 407.36491560935974 and batch: 1250, loss is 4.046978034973145 and perplexity is 57.22426575843306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.367043599595118 and perplexity of 78.81029216730319
Finished 19 epochs...
Completing Train Step...
At time: 409.64931654930115 and batch: 50, loss is 4.101313920021057 and perplexity is 60.41962201744044
At time: 410.45969581604004 and batch: 100, loss is 4.114632210731506 and perplexity is 61.22969049792804
At time: 411.3112304210663 and batch: 150, loss is 4.0383900451660155 and perplexity is 56.734928566314935
At time: 412.11902475357056 and batch: 200, loss is 4.092556691169738 and perplexity is 59.8928235742218
At time: 412.92568349838257 and batch: 250, loss is 4.109070820808411 and perplexity is 60.89011344961862
At time: 413.73050928115845 and batch: 300, loss is 4.113403744697571 and perplexity is 61.15451808571426
At time: 414.5393030643463 and batch: 350, loss is 4.1040212488174435 and perplexity is 60.58341942666859
At time: 415.3388409614563 and batch: 400, loss is 4.117647552490235 and perplexity is 61.41459757948039
At time: 416.13571286201477 and batch: 450, loss is 4.048574395179749 and perplexity is 57.31568925214058
At time: 416.93688225746155 and batch: 500, loss is 4.075482048988342 and perplexity is 58.87885624165648
At time: 417.73278403282166 and batch: 550, loss is 4.0720142650604245 and perplexity is 58.675030706004854
At time: 418.5366442203522 and batch: 600, loss is 4.084954628944397 and perplexity is 59.43924086896166
At time: 419.3354289531708 and batch: 650, loss is 4.104314985275269 and perplexity is 60.60121759955209
At time: 420.1354281902313 and batch: 700, loss is 4.082161660194397 and perplexity is 59.273460544090824
At time: 420.9280879497528 and batch: 750, loss is 4.076430692672729 and perplexity is 58.934737798426234
At time: 421.7292129993439 and batch: 800, loss is 4.096691899299621 and perplexity is 60.141005653805905
At time: 422.5391957759857 and batch: 850, loss is 4.14215922832489 and perplexity is 62.938573587346
At time: 423.34940481185913 and batch: 900, loss is 4.088990073204041 and perplexity is 59.67958924238696
At time: 424.15912556648254 and batch: 950, loss is 4.08182605266571 and perplexity is 59.25357126216397
At time: 424.96288442611694 and batch: 1000, loss is 4.061343612670899 and perplexity is 58.052258451723965
At time: 425.7583751678467 and batch: 1050, loss is 4.059470744132995 and perplexity is 57.94363595289646
At time: 426.56737637519836 and batch: 1100, loss is 4.016446290016174 and perplexity is 55.50351155522566
At time: 427.3655767440796 and batch: 1150, loss is 4.014489612579346 and perplexity is 55.39501526723578
At time: 428.16047859191895 and batch: 1200, loss is 4.040073976516724 and perplexity is 56.83054677586356
At time: 428.95776772499084 and batch: 1250, loss is 4.056489224433899 and perplexity is 57.77113314890779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.363763739592837 and perplexity of 78.55222887932626
Finished 20 epochs...
Completing Train Step...
At time: 431.16643166542053 and batch: 50, loss is 4.090421996116638 and perplexity is 59.76510702646259
At time: 432.0078022480011 and batch: 100, loss is 4.102067756652832 and perplexity is 60.46518571342904
At time: 432.8177251815796 and batch: 150, loss is 4.024797344207764 and perplexity is 55.968965197278735
At time: 433.61854124069214 and batch: 200, loss is 4.078504548072815 and perplexity is 59.05708674582415
At time: 434.4178149700165 and batch: 250, loss is 4.095826778411865 and perplexity is 60.08899891290672
At time: 435.21771788597107 and batch: 300, loss is 4.101264533996582 and perplexity is 60.416638206188594
At time: 436.01900577545166 and batch: 350, loss is 4.091357851028443 and perplexity is 59.82106467546408
At time: 436.8201320171356 and batch: 400, loss is 4.105417432785035 and perplexity is 60.66806410159296
At time: 437.6248571872711 and batch: 450, loss is 4.035818486213684 and perplexity is 56.589218783795395
At time: 438.42059350013733 and batch: 500, loss is 4.064071202278138 and perplexity is 58.210817332027524
At time: 439.23540711402893 and batch: 550, loss is 4.060582356452942 and perplexity is 58.00808262570564
At time: 440.0488693714142 and batch: 600, loss is 4.074277291297912 and perplexity is 58.807964199236906
At time: 440.84982442855835 and batch: 650, loss is 4.0947660446167 and perplexity is 60.02529427384703
At time: 441.65061116218567 and batch: 700, loss is 4.073113055229187 and perplexity is 58.73953768622412
At time: 442.4485468864441 and batch: 750, loss is 4.068359394073486 and perplexity is 58.46097245455004
At time: 443.2459919452667 and batch: 800, loss is 4.089675188064575 and perplexity is 59.72049062532027
At time: 444.03919863700867 and batch: 850, loss is 4.13609881401062 and perplexity is 62.55829324719632
At time: 444.8343930244446 and batch: 900, loss is 4.083586921691895 and perplexity is 59.358000957014454
At time: 445.6343185901642 and batch: 950, loss is 4.0777318477630615 and perplexity is 59.0114709425448
At time: 446.43337893486023 and batch: 1000, loss is 4.058450465202331 and perplexity is 57.884547430480666
At time: 447.2322328090668 and batch: 1050, loss is 4.057849488258362 and perplexity is 57.84977060314524
At time: 448.03149795532227 and batch: 1100, loss is 4.015719223022461 and perplexity is 55.46317145069501
At time: 448.83084321022034 and batch: 1150, loss is 4.01479335308075 and perplexity is 55.41184353253181
At time: 449.63044691085815 and batch: 1200, loss is 4.041861023902893 and perplexity is 56.932196455279026
At time: 450.4284100532532 and batch: 1250, loss is 4.059076981544495 and perplexity is 57.920824408277674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.362295331746123 and perplexity of 78.43696681663246
Finished 21 epochs...
Completing Train Step...
At time: 452.66659140586853 and batch: 50, loss is 4.081507329940796 and perplexity is 59.23468881176324
At time: 453.49503898620605 and batch: 100, loss is 4.092617526054382 and perplexity is 59.896467258065336
At time: 454.2963111400604 and batch: 150, loss is 4.015139212608338 and perplexity is 55.43101156109034
At time: 455.0985381603241 and batch: 200, loss is 4.068816585540771 and perplexity is 58.487706423130966
At time: 455.90926027297974 and batch: 250, loss is 4.08643443107605 and perplexity is 59.527264296868616
At time: 456.7208173274994 and batch: 300, loss is 4.092601284980774 and perplexity is 59.895494483031236
At time: 457.5578396320343 and batch: 350, loss is 4.082604899406433 and perplexity is 59.299738689358996
At time: 458.36733055114746 and batch: 400, loss is 4.09694034576416 and perplexity is 60.155949330309355
At time: 459.1666307449341 and batch: 450, loss is 4.027071366310119 and perplexity is 56.096384683638384
At time: 459.9641623497009 and batch: 500, loss is 4.056289472579956 and perplexity is 57.75959441043748
At time: 460.75806951522827 and batch: 550, loss is 4.052779006958008 and perplexity is 57.55718682022739
At time: 461.56095695495605 and batch: 600, loss is 4.067046313285828 and perplexity is 58.38425885139374
At time: 462.3586926460266 and batch: 650, loss is 4.0881380939483645 and perplexity is 59.62876512398674
At time: 463.15499544143677 and batch: 700, loss is 4.066781129837036 and perplexity is 58.368778364951474
At time: 463.9534389972687 and batch: 750, loss is 4.062726159095764 and perplexity is 58.132573901227374
At time: 464.7496922016144 and batch: 800, loss is 4.084425387382507 and perplexity is 59.40779147518384
At time: 465.5473401546478 and batch: 850, loss is 4.131336183547973 and perplexity is 62.26105958297492
At time: 466.34393668174744 and batch: 900, loss is 4.079373064041138 and perplexity is 59.10840104912582
At time: 467.13994216918945 and batch: 950, loss is 4.074213066101074 and perplexity is 58.80418736744574
At time: 467.93533635139465 and batch: 1000, loss is 4.05570321559906 and perplexity is 57.72574236896801
At time: 468.7340610027313 and batch: 1050, loss is 4.055891404151916 and perplexity is 57.73660671512772
At time: 469.5386326313019 and batch: 1100, loss is 4.014244823455811 and perplexity is 55.38145682954705
At time: 470.33631920814514 and batch: 1150, loss is 4.013808856010437 and perplexity is 55.35731757963788
At time: 471.16468262672424 and batch: 1200, loss is 4.041713309288025 and perplexity is 56.923787358894565
At time: 472.01067423820496 and batch: 1250, loss is 4.059313087463379 and perplexity is 57.934501472301484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.361441285070712 and perplexity of 78.3700065835457
Finished 22 epochs...
Completing Train Step...
At time: 474.2517988681793 and batch: 50, loss is 4.07389835357666 and perplexity is 58.785683864987924
At time: 475.0478653907776 and batch: 100, loss is 4.084909324645996 and perplexity is 59.436548076854606
At time: 475.84535551071167 and batch: 150, loss is 4.00723168849945 and perplexity is 54.99441796296792
At time: 476.6413743495941 and batch: 200, loss is 4.060956878662109 and perplexity is 58.029812009774645
At time: 477.4400577545166 and batch: 250, loss is 4.07895302772522 and perplexity is 59.08357858764191
At time: 478.2356073856354 and batch: 300, loss is 4.085663089752197 and perplexity is 59.48136616186029
At time: 479.041339635849 and batch: 350, loss is 4.075484585762024 and perplexity is 58.87900560417886
At time: 479.8392312526703 and batch: 400, loss is 4.08978741645813 and perplexity is 59.72719333615563
At time: 480.6477553844452 and batch: 450, loss is 4.020002570152283 and perplexity is 55.70124898730387
At time: 481.4490852355957 and batch: 500, loss is 4.0499877977371215 and perplexity is 57.39675667085986
At time: 482.2478201389313 and batch: 550, loss is 4.046441006660461 and perplexity is 57.19354295779353
At time: 483.043653011322 and batch: 600, loss is 4.06120964050293 and perplexity is 58.04448158575711
At time: 483.84846353530884 and batch: 650, loss is 4.082775845527649 and perplexity is 59.309876616172005
At time: 484.6448709964752 and batch: 700, loss is 4.06156129360199 and perplexity is 58.064896696898266
At time: 485.4413380622864 and batch: 750, loss is 4.057775750160217 and perplexity is 57.84550502835247
At time: 486.2429690361023 and batch: 800, loss is 4.0798615550994874 and perplexity is 59.137282027987304
At time: 487.0416114330292 and batch: 850, loss is 4.12715672492981 and perplexity is 62.00138508832383
At time: 487.8414263725281 and batch: 900, loss is 4.075631332397461 and perplexity is 58.887646534147336
At time: 488.64063143730164 and batch: 950, loss is 4.070925726890564 and perplexity is 58.611195445325805
At time: 489.4426074028015 and batch: 1000, loss is 4.052848329544068 and perplexity is 57.56117697156631
At time: 490.2417621612549 and batch: 1050, loss is 4.05373703956604 and perplexity is 57.61235490421992
At time: 491.03648495674133 and batch: 1100, loss is 4.012264161109925 and perplexity is 55.2718734230328
At time: 491.83576941490173 and batch: 1150, loss is 4.012109489440918 and perplexity is 55.26332509123074
At time: 492.68530201911926 and batch: 1200, loss is 4.040605049133301 and perplexity is 56.86073593865063
At time: 493.48702120780945 and batch: 1250, loss is 4.058417043685913 and perplexity is 57.882612873456445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.360857664233577 and perplexity of 78.32428155903354
Finished 23 epochs...
Completing Train Step...
At time: 495.7321219444275 and batch: 50, loss is 4.067197313308716 and perplexity is 58.39307554146006
At time: 496.5618793964386 and batch: 100, loss is 4.0781543159484865 and perplexity is 59.03640667849984
At time: 497.3607804775238 and batch: 150, loss is 4.000421228408814 and perplexity is 54.621153169454004
At time: 498.1567897796631 and batch: 200, loss is 4.054255704879761 and perplexity is 57.64224418494705
At time: 498.9548661708832 and batch: 250, loss is 4.072506456375122 and perplexity is 58.703917154754414
At time: 499.78605604171753 and batch: 300, loss is 4.079680361747742 and perplexity is 59.12656771635385
At time: 500.5828664302826 and batch: 350, loss is 4.06924204826355 and perplexity is 58.51259605638491
At time: 501.3788540363312 and batch: 400, loss is 4.0838496255874634 and perplexity is 59.373596583525256
At time: 502.1759901046753 and batch: 450, loss is 4.0139683771133425 and perplexity is 55.36614894436788
At time: 502.9749836921692 and batch: 500, loss is 4.044544816017151 and perplexity is 57.08519585262657
At time: 503.7697651386261 and batch: 550, loss is 4.040974845886231 and perplexity is 56.88176674249174
At time: 504.56598234176636 and batch: 600, loss is 4.056097254753113 and perplexity is 57.74849305369409
At time: 505.3638844490051 and batch: 650, loss is 4.07787127494812 and perplexity is 59.019699319440875
At time: 506.175372838974 and batch: 700, loss is 4.056761832237243 and perplexity is 57.78688415743861
At time: 507.00848960876465 and batch: 750, loss is 4.053388485908508 and perplexity is 57.59227740644493
At time: 507.8210527896881 and batch: 800, loss is 4.075710945129394 and perplexity is 58.89233492719045
At time: 508.6232006549835 and batch: 850, loss is 4.1232810306549075 and perplexity is 61.761551735688315
At time: 509.42362117767334 and batch: 900, loss is 4.072111916542053 and perplexity is 58.68076068945405
At time: 510.2224631309509 and batch: 950, loss is 4.0677119779586794 and perplexity is 58.42313612814101
At time: 511.01851081848145 and batch: 1000, loss is 4.049938035011292 and perplexity is 57.39390052285957
At time: 511.8196198940277 and batch: 1050, loss is 4.051362953186035 and perplexity is 57.475740428571285
At time: 512.6182539463043 and batch: 1100, loss is 4.010028214454651 and perplexity is 55.14842652429264
At time: 513.4695899486542 and batch: 1150, loss is 4.010062651634216 and perplexity is 55.1503257132608
At time: 514.2702655792236 and batch: 1200, loss is 4.0389825010299685 and perplexity is 56.76855146649371
At time: 515.0712082386017 and batch: 1250, loss is 4.056868448257446 and perplexity is 57.793045493490105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.360445565550867 and perplexity of 78.29201087558863
Finished 24 epochs...
Completing Train Step...
At time: 517.3008778095245 and batch: 50, loss is 4.061125926971435 and perplexity is 58.0396226806007
At time: 518.0969681739807 and batch: 100, loss is 4.072126092910767 and perplexity is 58.68159257545052
At time: 518.8960111141205 and batch: 150, loss is 3.994351415634155 and perplexity is 54.29061715685265
At time: 519.6951947212219 and batch: 200, loss is 4.048276481628418 and perplexity is 57.298616674810816
At time: 520.4916179180145 and batch: 250, loss is 4.066743812561035 and perplexity is 58.36660024178046
At time: 521.2877552509308 and batch: 300, loss is 4.074285655021668 and perplexity is 58.808456054860955
At time: 522.087887763977 and batch: 350, loss is 4.063735008239746 and perplexity is 58.19125049158245
At time: 522.8844451904297 and batch: 400, loss is 4.078358101844787 and perplexity is 59.04843869148483
At time: 523.6804535388947 and batch: 450, loss is 4.008537545204162 and perplexity is 55.06627970274202
At time: 524.4779541492462 and batch: 500, loss is 4.039676494598389 and perplexity is 56.807962149898636
At time: 525.2771546840668 and batch: 550, loss is 4.036007876396179 and perplexity is 56.599937241221376
At time: 526.0746648311615 and batch: 600, loss is 4.05147488117218 and perplexity is 57.48217393248751
At time: 526.8721604347229 and batch: 650, loss is 4.073475260734558 and perplexity is 58.76081732372529
At time: 527.668035030365 and batch: 700, loss is 4.052334108352661 and perplexity is 57.53158540352298
At time: 528.4612140655518 and batch: 750, loss is 4.049377808570862 and perplexity is 57.361755947228566
At time: 529.270565032959 and batch: 800, loss is 4.071825399398803 and perplexity is 58.6639500539202
At time: 530.0687942504883 and batch: 850, loss is 4.119569230079651 and perplexity is 61.53273010527182
At time: 530.8680801391602 and batch: 900, loss is 4.068698177337646 and perplexity is 58.48078140890583
At time: 531.6673862934113 and batch: 950, loss is 4.0645243406295775 and perplexity is 58.23720086307276
At time: 532.4650988578796 and batch: 1000, loss is 4.04697988986969 and perplexity is 57.22437190362439
At time: 533.2693598270416 and batch: 1050, loss is 4.049052405357361 and perplexity is 57.34309328412123
At time: 534.1196067333221 and batch: 1100, loss is 4.007830471992492 and perplexity is 55.02735757351695
At time: 534.9154689311981 and batch: 1150, loss is 4.0079487562179565 and perplexity is 55.03386682685028
At time: 535.7128441333771 and batch: 1200, loss is 4.037041373252869 and perplexity is 56.658463336507374
At time: 536.5100169181824 and batch: 1250, loss is 4.055005512237549 and perplexity is 57.685480971360924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.360114104556342 and perplexity of 78.26606442815562
Finished 25 epochs...
Completing Train Step...
At time: 538.7026417255402 and batch: 50, loss is 4.055457673072815 and perplexity is 57.71156998439013
At time: 539.5289797782898 and batch: 100, loss is 4.066604208946228 and perplexity is 58.35845262213453
At time: 540.3242254257202 and batch: 150, loss is 3.9887588119506834 and perplexity is 53.98783870049029
At time: 541.1224298477173 and batch: 200, loss is 4.042782306671143 and perplexity is 56.984671275194
At time: 541.9180662631989 and batch: 250, loss is 4.061430687904358 and perplexity is 58.05731358576678
At time: 542.7160248756409 and batch: 300, loss is 4.069323406219483 and perplexity is 58.51735671525251
At time: 543.5125904083252 and batch: 350, loss is 4.058567442893982 and perplexity is 57.89131902727648
At time: 544.3085739612579 and batch: 400, loss is 4.073281931877136 and perplexity is 58.749458260103715
At time: 545.105580329895 and batch: 450, loss is 4.003634729385376 and perplexity is 54.79696062593062
At time: 545.9003274440765 and batch: 500, loss is 4.034952831268311 and perplexity is 56.54025324338921
At time: 546.6962015628815 and batch: 550, loss is 4.031303644180298 and perplexity is 56.33430328481074
At time: 547.4926285743713 and batch: 600, loss is 4.047083148956299 and perplexity is 57.23028114508508
At time: 548.3187260627747 and batch: 650, loss is 4.069239764213562 and perplexity is 58.512462410843256
At time: 549.1319370269775 and batch: 700, loss is 4.048165788650513 and perplexity is 57.29227447132644
At time: 549.9407343864441 and batch: 750, loss is 4.045496330261231 and perplexity is 57.13953907968364
At time: 550.7461915016174 and batch: 800, loss is 4.068041415214538 and perplexity is 58.44238605643341
At time: 551.5567500591278 and batch: 850, loss is 4.1159829330444335 and perplexity is 61.31245068753192
At time: 552.3650095462799 and batch: 900, loss is 4.065351033210755 and perplexity is 58.285365030711795
At time: 553.185604095459 and batch: 950, loss is 4.061371259689331 and perplexity is 58.05386344576994
At time: 554.0409801006317 and batch: 1000, loss is 4.0440123796463014 and perplexity is 57.05480970817965
At time: 554.8386013507843 and batch: 1050, loss is 4.046717247962952 and perplexity is 57.209344358993576
At time: 555.6365120410919 and batch: 1100, loss is 4.005232973098755 and perplexity is 54.884609547274636
At time: 556.434463262558 and batch: 1150, loss is 4.005407371520996 and perplexity is 54.89418217128601
At time: 557.2314088344574 and batch: 1200, loss is 4.034906578063965 and perplexity is 56.5376381359812
At time: 558.0495712757111 and batch: 1250, loss is 4.0529462385177615 and perplexity is 57.566813003232745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359949265083257 and perplexity of 78.25316415460122
Finished 26 epochs...
Completing Train Step...
At time: 560.2920789718628 and batch: 50, loss is 4.050259833335876 and perplexity is 57.41237275589662
At time: 561.0872824192047 and batch: 100, loss is 4.061475992202759 and perplexity is 58.05994389120744
At time: 561.8816344738007 and batch: 150, loss is 3.98360613822937 and perplexity is 53.71037244318895
At time: 562.6784551143646 and batch: 200, loss is 4.037771143913269 and perplexity is 56.69982611154785
At time: 563.4756321907043 and batch: 250, loss is 4.0565083360672 and perplexity is 57.772237260170556
At time: 564.270435333252 and batch: 300, loss is 4.064692974090576 and perplexity is 58.24702243191247
At time: 565.0670337677002 and batch: 350, loss is 4.053709664344788 and perplexity is 57.61077777484477
At time: 565.8629829883575 and batch: 400, loss is 4.068569655418396 and perplexity is 58.473265829608586
At time: 566.6590509414673 and batch: 450, loss is 3.998873481750488 and perplexity is 54.536678851447114
At time: 567.4590363502502 and batch: 500, loss is 4.03063551902771 and perplexity is 56.296677490602036
At time: 568.25510597229 and batch: 550, loss is 4.0270113325119015 and perplexity is 56.09301710568479
At time: 569.0514969825745 and batch: 600, loss is 4.043012027740478 and perplexity is 56.99776335852121
At time: 569.8761358261108 and batch: 650, loss is 4.065297474861145 and perplexity is 58.28224344634856
At time: 570.6941161155701 and batch: 700, loss is 4.044237680435181 and perplexity is 57.067665649988804
At time: 571.492372751236 and batch: 750, loss is 4.042017784118652 and perplexity is 56.94112185823477
At time: 572.2947528362274 and batch: 800, loss is 4.064463362693787 and perplexity is 58.23364978704765
At time: 573.0895359516144 and batch: 850, loss is 4.112584819793701 and perplexity is 61.10445762854496
At time: 573.8938097953796 and batch: 900, loss is 4.062157444953918 and perplexity is 58.09952248364299
At time: 574.728376865387 and batch: 950, loss is 4.058253521919251 and perplexity is 57.8731485801704
At time: 575.5336527824402 and batch: 1000, loss is 4.0410046911239625 and perplexity is 56.883464417676414
At time: 576.3411755561829 and batch: 1050, loss is 4.043992433547974 and perplexity is 57.0536716986846
At time: 577.1479859352112 and batch: 1100, loss is 4.002608723640442 and perplexity is 54.740767461707655
At time: 577.9547953605652 and batch: 1150, loss is 4.0028327417373655 and perplexity is 54.7530317579194
At time: 578.7493050098419 and batch: 1200, loss is 4.032645397186279 and perplexity is 56.40994073762531
At time: 579.5474646091461 and batch: 1250, loss is 4.050749917030334 and perplexity is 57.44051651948092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359785316634352 and perplexity of 78.24033572134577
Finished 27 epochs...
Completing Train Step...
At time: 581.8187456130981 and batch: 50, loss is 4.045350832939148 and perplexity is 57.13122603454006
At time: 582.6334390640259 and batch: 100, loss is 4.056642518043518 and perplexity is 57.779989773254606
At time: 583.4588088989258 and batch: 150, loss is 3.9787830829620363 and perplexity is 53.45194804699154
At time: 584.2663085460663 and batch: 200, loss is 4.032972364425659 and perplexity is 56.42838795587557
At time: 585.0603454113007 and batch: 250, loss is 4.051821961402893 and perplexity is 57.50212832136448
At time: 585.8573040962219 and batch: 300, loss is 4.0602999114990235 and perplexity is 57.991700849065495
At time: 586.6517176628113 and batch: 350, loss is 4.049087257385254 and perplexity is 57.345091842034535
At time: 587.4476654529572 and batch: 400, loss is 4.0640394449234005 and perplexity is 58.208968739805336
At time: 588.2454743385315 and batch: 450, loss is 3.9943512678146362 and perplexity is 54.29060913164033
At time: 589.0424892902374 and batch: 500, loss is 4.026523032188416 and perplexity is 56.06563355352957
At time: 589.8537907600403 and batch: 550, loss is 4.022904362678528 and perplexity is 55.86311719571354
At time: 590.6751618385315 and batch: 600, loss is 4.039125695228576 and perplexity is 56.776680975762474
At time: 591.4784777164459 and batch: 650, loss is 4.061436309814453 and perplexity is 58.05763997968158
At time: 592.2746839523315 and batch: 700, loss is 4.040444025993347 and perplexity is 56.851580781525534
At time: 593.0723125934601 and batch: 750, loss is 4.038502106666565 and perplexity is 56.74128672378931
At time: 593.869039773941 and batch: 800, loss is 4.060937561988831 and perplexity is 58.028691077682005
At time: 594.6653280258179 and batch: 850, loss is 4.109165477752685 and perplexity is 60.89587739448871
At time: 595.5093169212341 and batch: 900, loss is 4.059009747505188 and perplexity is 57.91693028820291
At time: 596.3080215454102 and batch: 950, loss is 4.055254225730896 and perplexity is 57.699829913162475
At time: 597.1063704490662 and batch: 1000, loss is 4.038121638298034 and perplexity is 56.71970256530634
At time: 597.903068780899 and batch: 1050, loss is 4.041365213394165 and perplexity is 56.903975870600895
At time: 598.7002460956573 and batch: 1100, loss is 4.000005111694336 and perplexity is 54.598429122911845
At time: 599.4974830150604 and batch: 1150, loss is 4.000207586288452 and perplexity is 54.609485036920944
At time: 600.3145554065704 and batch: 1200, loss is 4.030240836143494 and perplexity is 56.27446253977709
At time: 601.132034778595 and batch: 1250, loss is 4.048485736846924 and perplexity is 57.31060796393939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359677948220803 and perplexity of 78.23193563158404
Finished 28 epochs...
Completing Train Step...
At time: 603.3430516719818 and batch: 50, loss is 4.040678682327271 and perplexity is 56.86492293039815
At time: 604.1676547527313 and batch: 100, loss is 4.052187328338623 and perplexity is 57.523141536320445
At time: 604.9739563465118 and batch: 150, loss is 3.974263963699341 and perplexity is 53.21093730725081
At time: 605.7991592884064 and batch: 200, loss is 4.028486051559448 and perplexity is 56.175799571881846
At time: 606.5965926647186 and batch: 250, loss is 4.047462630271911 and perplexity is 57.252003088742114
At time: 607.3931455612183 and batch: 300, loss is 4.0560941314697265 and perplexity is 57.7483126890668
At time: 608.1888716220856 and batch: 350, loss is 4.044716777801514 and perplexity is 57.09501316884352
At time: 608.9865336418152 and batch: 400, loss is 4.060000042915345 and perplexity is 57.97431356694688
At time: 609.783712387085 and batch: 450, loss is 3.9901898860931397 and perplexity is 54.06515460966581
At time: 610.5806820392609 and batch: 500, loss is 4.022558307647705 and perplexity is 55.84378882750293
At time: 611.3970601558685 and batch: 550, loss is 4.018919095993042 and perplexity is 55.64093080588488
At time: 612.2157244682312 and batch: 600, loss is 4.035382647514343 and perplexity is 56.564560386216215
At time: 613.0276501178741 and batch: 650, loss is 4.057736554145813 and perplexity is 57.84323775953842
At time: 613.8334283828735 and batch: 700, loss is 4.036720809936523 and perplexity is 56.640303622425755
At time: 614.6418821811676 and batch: 750, loss is 4.0349842023849485 and perplexity is 56.542027002090684
At time: 615.4646043777466 and batch: 800, loss is 4.057478542327881 and perplexity is 57.828315445758435
At time: 616.3103535175323 and batch: 850, loss is 4.105944356918335 and perplexity is 60.70003999238313
At time: 617.1063055992126 and batch: 900, loss is 4.0558887434005735 and perplexity is 57.73645309257825
At time: 617.9019396305084 and batch: 950, loss is 4.052193565368652 and perplexity is 57.52350031100043
At time: 618.7008080482483 and batch: 1000, loss is 4.035144047737122 and perplexity is 56.551065704687474
At time: 619.4993686676025 and batch: 1050, loss is 4.038652811050415 and perplexity is 56.749838528824974
At time: 620.2960324287415 and batch: 1100, loss is 3.997307467460632 and perplexity is 54.45134047104638
At time: 621.0960342884064 and batch: 1150, loss is 3.9975455665588377 and perplexity is 54.464306829686514
At time: 621.8959491252899 and batch: 1200, loss is 4.027775764465332 and perplexity is 56.13591279365311
At time: 622.6935904026031 and batch: 1250, loss is 4.046105270385742 and perplexity is 57.17434423377727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359596865020529 and perplexity of 78.2255925930398
Finished 29 epochs...
Completing Train Step...
At time: 624.9294273853302 and batch: 50, loss is 4.036269645690918 and perplexity is 56.614755306247886
At time: 625.7267732620239 and batch: 100, loss is 4.0476720428466795 and perplexity is 57.26399363356117
At time: 626.5509006977081 and batch: 150, loss is 3.9699286365509034 and perplexity is 52.98074981572174
At time: 627.3496952056885 and batch: 200, loss is 4.0241714239120485 and perplexity is 55.93394404739918
At time: 628.1467337608337 and batch: 250, loss is 4.04324254989624 and perplexity is 57.01090412036438
At time: 628.945550441742 and batch: 300, loss is 4.052102093696594 and perplexity is 57.518238780888495
At time: 629.7440915107727 and batch: 350, loss is 4.040792622566223 and perplexity is 56.87140250243983
At time: 630.5402863025665 and batch: 400, loss is 4.055806851387024 and perplexity is 57.731725131773025
At time: 631.3367962837219 and batch: 450, loss is 3.9860406017303465 and perplexity is 53.84128767404438
At time: 632.1324784755707 and batch: 500, loss is 4.01872296333313 and perplexity is 55.63001887225419
At time: 632.9432194232941 and batch: 550, loss is 4.015116772651672 and perplexity is 55.429767705549075
At time: 633.7437481880188 and batch: 600, loss is 4.031746702194214 and perplexity is 56.35926817937937
At time: 634.5397474765778 and batch: 650, loss is 4.054249992370606 and perplexity is 57.641914904039915
At time: 635.3356096744537 and batch: 700, loss is 4.033139543533325 and perplexity is 56.43782239201939
At time: 636.1346080303192 and batch: 750, loss is 4.031827712059021 and perplexity is 56.36383402101183
At time: 636.9616510868073 and batch: 800, loss is 4.05422610282898 and perplexity is 57.64053788156271
At time: 637.787106513977 and batch: 850, loss is 4.102763938903808 and perplexity is 60.50729515874818
At time: 638.6005146503448 and batch: 900, loss is 4.052880477905274 and perplexity is 57.56302749882059
At time: 639.4112875461578 and batch: 950, loss is 4.049183192253113 and perplexity is 57.35059349973898
At time: 640.234010219574 and batch: 1000, loss is 4.032179374694824 and perplexity is 56.383658561018294
At time: 641.0669827461243 and batch: 1050, loss is 4.036034150123596 and perplexity is 56.601424352080144
At time: 641.899582862854 and batch: 1100, loss is 3.9945824718475342 and perplexity is 54.303162790592424
At time: 642.7067515850067 and batch: 1150, loss is 3.994898591041565 and perplexity is 54.320331776227086
At time: 643.509140253067 and batch: 1200, loss is 4.025275549888611 and perplexity is 55.995736274917725
At time: 644.3752908706665 and batch: 1250, loss is 4.043662323951721 and perplexity is 57.03484084244958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359540730497263 and perplexity of 78.22120155993781
Finished 30 epochs...
Completing Train Step...
At time: 646.5418574810028 and batch: 50, loss is 4.031696105003357 and perplexity is 56.356416630871536
At time: 647.3739576339722 and batch: 100, loss is 4.043440775871277 and perplexity is 57.02220628257544
At time: 648.1712174415588 and batch: 150, loss is 3.9656979703903197 and perplexity is 52.757079421312675
At time: 648.9693050384521 and batch: 200, loss is 4.020000724792481 and perplexity is 55.70114619855292
At time: 649.7709572315216 and batch: 250, loss is 4.039190821647644 and perplexity is 56.78037875809131
At time: 650.5631506443024 and batch: 300, loss is 4.048223519325257 and perplexity is 57.29558208846384
At time: 651.3547174930573 and batch: 350, loss is 4.036675162315369 and perplexity is 56.6377181863139
At time: 652.1467742919922 and batch: 400, loss is 4.051746253967285 and perplexity is 57.497775147473185
At time: 652.9368600845337 and batch: 450, loss is 3.9820163011550904 and perplexity is 53.6250495445309
At time: 653.7599453926086 and batch: 500, loss is 4.0150071811676025 and perplexity is 55.4236934078963
At time: 654.5521261692047 and batch: 550, loss is 4.011403141021728 and perplexity is 55.22430371188061
At time: 655.3404741287231 and batch: 600, loss is 4.028231110572815 and perplexity is 56.16147988352937
At time: 656.1356837749481 and batch: 650, loss is 4.050719542503357 and perplexity is 57.4387718174597
At time: 656.9360928535461 and batch: 700, loss is 4.029797854423523 and perplexity is 56.249539502203376
At time: 657.7761700153351 and batch: 750, loss is 4.028529939651489 and perplexity is 56.178265074646625
At time: 658.5686349868774 and batch: 800, loss is 4.050794687271118 and perplexity is 57.44308820280327
At time: 659.3640050888062 and batch: 850, loss is 4.09958692073822 and perplexity is 60.31536742326359
At time: 660.1564300060272 and batch: 900, loss is 4.049820671081543 and perplexity is 57.3871649444153
At time: 660.9478876590729 and batch: 950, loss is 4.046212096214294 and perplexity is 57.18045225671345
At time: 661.7423768043518 and batch: 1000, loss is 4.029223365783691 and perplexity is 56.217234061206504
At time: 662.5343337059021 and batch: 1050, loss is 4.033384261131286 and perplexity is 56.45163541042472
At time: 663.3268887996674 and batch: 1100, loss is 3.9919698095321654 and perplexity is 54.16147213907534
At time: 664.1188106536865 and batch: 1150, loss is 3.9921623945236204 and perplexity is 54.17190383018577
At time: 664.9139034748077 and batch: 1200, loss is 4.022748003005981 and perplexity is 55.85438313984625
At time: 665.7086110115051 and batch: 1250, loss is 4.041250901222229 and perplexity is 56.89747142530304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359445390910127 and perplexity of 78.21374433836573
Finished 31 epochs...
Completing Train Step...
At time: 667.9223763942719 and batch: 50, loss is 4.027796468734741 and perplexity is 56.137075058747
At time: 668.7130632400513 and batch: 100, loss is 4.0394264554977415 and perplexity is 56.79375971378923
At time: 669.5067400932312 and batch: 150, loss is 3.961714186668396 and perplexity is 52.54732471320354
At time: 670.3002450466156 and batch: 200, loss is 4.016001586914062 and perplexity is 55.4788344588563
At time: 671.0988097190857 and batch: 250, loss is 4.035297646522522 and perplexity is 56.55975254681991
At time: 671.8917450904846 and batch: 300, loss is 4.044502778053284 and perplexity is 57.082796157665456
At time: 672.6849377155304 and batch: 350, loss is 4.032687005996704 and perplexity is 56.412287936987276
At time: 673.4806916713715 and batch: 400, loss is 4.048012819290161 and perplexity is 57.28351117902215
At time: 674.2743101119995 and batch: 450, loss is 3.9782497978210447 and perplexity is 53.42345051667114
At time: 675.0686609745026 and batch: 500, loss is 4.011444411277771 and perplexity is 55.22658288006519
At time: 675.8621525764465 and batch: 550, loss is 4.00781569480896 and perplexity is 55.02654443016276
At time: 676.6548523902893 and batch: 600, loss is 4.024786853790284 and perplexity is 55.968378062547515
At time: 677.4755551815033 and batch: 650, loss is 4.047325053215027 and perplexity is 57.24412706844869
At time: 678.2675542831421 and batch: 700, loss is 4.0263085412979125 and perplexity is 56.05360927545721
At time: 679.0671679973602 and batch: 750, loss is 4.025238423347473 and perplexity is 55.99365738550262
At time: 679.8790504932404 and batch: 800, loss is 4.047456045150756 and perplexity is 57.25162607860674
At time: 680.6713583469391 and batch: 850, loss is 4.096434993743896 and perplexity is 60.12555707982377
At time: 681.4655034542084 and batch: 900, loss is 4.046820449829101 and perplexity is 57.215248774760745
At time: 682.2584161758423 and batch: 950, loss is 4.043354177474976 and perplexity is 57.017268464764456
At time: 683.0523297786713 and batch: 1000, loss is 4.0263097381591795 and perplexity is 56.05367636389118
At time: 683.8480658531189 and batch: 1050, loss is 4.030712423324585 and perplexity is 56.301007113482
At time: 684.641270160675 and batch: 1100, loss is 3.989213490486145 and perplexity is 54.01239139329159
At time: 685.4346904754639 and batch: 1150, loss is 3.9895966911315917 and perplexity is 54.03309294270584
At time: 686.2273602485657 and batch: 1200, loss is 4.020170516967774 and perplexity is 55.71060462029256
At time: 687.0201852321625 and batch: 1250, loss is 4.03921630859375 and perplexity is 56.78182593498661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359413759551779 and perplexity of 78.21127037051846
Finished 32 epochs...
Completing Train Step...
At time: 689.1894574165344 and batch: 50, loss is 4.0238546991348265 and perplexity is 55.91623118663063
At time: 690.0276637077332 and batch: 100, loss is 4.0355728816986085 and perplexity is 56.57532192279324
At time: 690.8239018917084 and batch: 150, loss is 3.957787127494812 and perplexity is 52.3413729167658
At time: 691.6166565418243 and batch: 200, loss is 4.012130942344665 and perplexity is 55.26451066274163
At time: 692.4087822437286 and batch: 250, loss is 4.031549258232117 and perplexity is 56.348141480655016
At time: 693.1993043422699 and batch: 300, loss is 4.040869278907776 and perplexity is 56.87576222319262
At time: 693.993456363678 and batch: 350, loss is 4.028913230895996 and perplexity is 56.19980183894425
At time: 694.7849988937378 and batch: 400, loss is 4.044220232963562 and perplexity is 57.06666997219809
At time: 695.5768611431122 and batch: 450, loss is 3.9744587469100954 and perplexity is 53.22130291395708
At time: 696.367614030838 and batch: 500, loss is 4.007941122055054 and perplexity is 55.03344669094945
At time: 697.1939213275909 and batch: 550, loss is 4.004250931739807 and perplexity is 54.830737047570494
At time: 698.0402302742004 and batch: 600, loss is 4.021425495147705 and perplexity is 55.780564103112
At time: 698.8322014808655 and batch: 650, loss is 4.04395631313324 and perplexity is 57.051610933618825
At time: 699.6254775524139 and batch: 700, loss is 4.022921652793884 and perplexity is 55.864083083804154
At time: 700.4203193187714 and batch: 750, loss is 4.022087903022766 and perplexity is 55.81752582856674
At time: 701.2129166126251 and batch: 800, loss is 4.044215531349182 and perplexity is 57.066401667352636
At time: 702.0018463134766 and batch: 850, loss is 4.093417072296143 and perplexity is 59.94437640359265
At time: 702.7958912849426 and batch: 900, loss is 4.043897948265076 and perplexity is 57.04828122103819
At time: 703.5884029865265 and batch: 950, loss is 4.040368967056274 and perplexity is 56.84731372244362
At time: 704.3810646533966 and batch: 1000, loss is 4.023394494056702 and perplexity is 55.89050417339408
At time: 705.1720263957977 and batch: 1050, loss is 4.028071594238281 and perplexity is 56.15252192460576
At time: 705.9666996002197 and batch: 1100, loss is 3.9866324043273926 and perplexity is 53.873160518203925
At time: 706.759654045105 and batch: 1150, loss is 3.9868357467651365 and perplexity is 53.884116331845846
At time: 707.5519769191742 and batch: 1200, loss is 4.017659482955932 and perplexity is 55.57088888616331
At time: 708.346428155899 and batch: 1250, loss is 4.036547427177429 and perplexity is 56.630484021607124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.359475685732208 and perplexity of 78.21611384572641
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 710.5491018295288 and batch: 50, loss is 4.032904324531555 and perplexity is 56.424548704947156
At time: 711.3432066440582 and batch: 100, loss is 4.057473912239074 and perplexity is 57.82804769614224
At time: 712.1333317756653 and batch: 150, loss is 3.9826796197891237 and perplexity is 53.66063183903958
At time: 712.9250473976135 and batch: 200, loss is 4.0458443117141725 and perplexity is 57.15942603945891
At time: 713.7165369987488 and batch: 250, loss is 4.064545331001281 and perplexity is 58.23842329639549
At time: 714.5474851131439 and batch: 300, loss is 4.073193607330322 and perplexity is 58.74426946997945
At time: 715.3433990478516 and batch: 350, loss is 4.05706708908081 and perplexity is 57.80452669191126
At time: 716.1349241733551 and batch: 400, loss is 4.072295327186584 and perplexity is 58.69152435264868
At time: 716.9294686317444 and batch: 450, loss is 3.999285879135132 and perplexity is 54.55917427338037
At time: 717.7200493812561 and batch: 500, loss is 4.0283563566207885 and perplexity is 56.16851432744208
At time: 718.5400912761688 and batch: 550, loss is 4.024099831581116 and perplexity is 55.92993974930678
At time: 719.3350005149841 and batch: 600, loss is 4.043923630714416 and perplexity is 57.04974637944496
At time: 720.1261429786682 and batch: 650, loss is 4.064916553497315 and perplexity is 58.26004672255943
At time: 720.9178278446198 and batch: 700, loss is 4.041024632453919 and perplexity is 56.88459876091954
At time: 721.7090799808502 and batch: 750, loss is 4.034989252090454 and perplexity is 56.54231252339662
At time: 722.5024914741516 and batch: 800, loss is 4.051607894897461 and perplexity is 57.489820359108066
At time: 723.2938606739044 and batch: 850, loss is 4.095022101402282 and perplexity is 60.040666125701954
At time: 724.0864584445953 and batch: 900, loss is 4.044453597068786 and perplexity is 57.07998883858649
At time: 724.8778102397919 and batch: 950, loss is 4.034872703552246 and perplexity is 56.53572298353319
At time: 725.6763734817505 and batch: 1000, loss is 4.016528797149658 and perplexity is 55.50809117978565
At time: 726.4723000526428 and batch: 1050, loss is 4.021695389747619 and perplexity is 55.795621007940866
At time: 727.2680778503418 and batch: 1100, loss is 3.9780695486068725 and perplexity is 53.41382184950305
At time: 728.0648066997528 and batch: 1150, loss is 3.9757003116607668 and perplexity is 53.28742164448904
At time: 728.864422082901 and batch: 1200, loss is 4.0040263652801515 and perplexity is 54.81842528552757
At time: 729.6616179943085 and batch: 1250, loss is 4.022184510231018 and perplexity is 55.8229184643881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.349461464986314 and perplexity of 77.43674929330288
Finished 34 epochs...
Completing Train Step...
At time: 731.8987984657288 and batch: 50, loss is 4.032989959716797 and perplexity is 56.4293808385251
At time: 732.6958477497101 and batch: 100, loss is 4.052936367988586 and perplexity is 57.56624479112975
At time: 733.493242263794 and batch: 150, loss is 3.974841289520264 and perplexity is 53.24166622475906
At time: 734.2881772518158 and batch: 200, loss is 4.034235343933106 and perplexity is 56.499700877401956
At time: 735.0853548049927 and batch: 250, loss is 4.050157294273377 and perplexity is 57.40648604683233
At time: 735.8800413608551 and batch: 300, loss is 4.059069843292236 and perplexity is 57.92041095629769
At time: 736.6762449741364 and batch: 350, loss is 4.043127079010009 and perplexity is 57.00432140080434
At time: 737.4720389842987 and batch: 400, loss is 4.060464220046997 and perplexity is 58.00123016407905
At time: 738.2632603645325 and batch: 450, loss is 3.9882984018325804 and perplexity is 53.96298787452292
At time: 739.0896546840668 and batch: 500, loss is 4.018837695121765 and perplexity is 55.63640176997483
At time: 739.8826222419739 and batch: 550, loss is 4.014829225540161 and perplexity is 55.413831327293195
At time: 740.6756029129028 and batch: 600, loss is 4.034681992530823 and perplexity is 56.52494202611249
At time: 741.4702682495117 and batch: 650, loss is 4.05649167060852 and perplexity is 57.77127446736038
At time: 742.2851519584656 and batch: 700, loss is 4.032948470115661 and perplexity is 56.427039654589485
At time: 743.0986440181732 and batch: 750, loss is 4.028543887138366 and perplexity is 56.17904862572582
At time: 743.8917784690857 and batch: 800, loss is 4.045643067359924 and perplexity is 57.14792418505683
At time: 744.6853303909302 and batch: 850, loss is 4.091667346954345 and perplexity is 59.83958191662183
At time: 745.4781684875488 and batch: 900, loss is 4.0429340887069705 and perplexity is 56.99332118104469
At time: 746.2984583377838 and batch: 950, loss is 4.034482669830322 and perplexity is 56.51367644480256
At time: 747.0921678543091 and batch: 1000, loss is 4.017319526672363 and perplexity is 55.55200042411053
At time: 747.8861627578735 and batch: 1050, loss is 4.023109316825867 and perplexity is 55.874567746644956
At time: 748.6855039596558 and batch: 1100, loss is 3.9807355403900146 and perplexity is 53.55641264815335
At time: 749.4815578460693 and batch: 1150, loss is 3.9796973609924318 and perplexity is 53.50084033593979
At time: 750.2791829109192 and batch: 1200, loss is 4.0084675645828245 and perplexity is 55.062426265108215
At time: 751.0770983695984 and batch: 1250, loss is 4.026562657356262 and perplexity is 56.06785520768597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.349024417626596 and perplexity of 77.40291316101384
Finished 35 epochs...
Completing Train Step...
At time: 753.2843391895294 and batch: 50, loss is 4.0320720291137695 and perplexity is 56.377606349272995
At time: 754.1081309318542 and batch: 100, loss is 4.050554232597351 and perplexity is 57.429277404271375
At time: 754.9041776657104 and batch: 150, loss is 3.9719318103790284 and perplexity is 53.08698583623049
At time: 755.6989872455597 and batch: 200, loss is 4.030348129272461 and perplexity is 56.28050072686613
At time: 756.4944624900818 and batch: 250, loss is 4.046069612503052 and perplexity is 57.17230555406542
At time: 757.2880229949951 and batch: 300, loss is 4.0550333023071286 and perplexity is 57.68708407716597
At time: 758.0822188854218 and batch: 350, loss is 4.039246506690979 and perplexity is 56.78354066397768
At time: 758.8784558773041 and batch: 400, loss is 4.0570811891555785 and perplexity is 57.805341745805706
At time: 759.7185208797455 and batch: 450, loss is 3.9847822284698484 and perplexity is 53.77357784837652
At time: 760.511461019516 and batch: 500, loss is 4.015649075508118 and perplexity is 55.45928098353506
At time: 761.3073177337646 and batch: 550, loss is 4.01166187286377 and perplexity is 55.238593846282136
At time: 762.102668762207 and batch: 600, loss is 4.031561722755432 and perplexity is 56.34884383775557
At time: 762.8995759487152 and batch: 650, loss is 4.0537359380722044 and perplexity is 57.612291444601084
At time: 763.6947596073151 and batch: 700, loss is 4.0301552629470825 and perplexity is 56.26964716017787
At time: 764.4884963035583 and batch: 750, loss is 4.026250262260437 and perplexity is 56.05034262025128
At time: 765.2838268280029 and batch: 800, loss is 4.043828711509705 and perplexity is 57.04433151988079
At time: 766.0784299373627 and batch: 850, loss is 4.090595273971558 and perplexity is 59.7754638932891
At time: 766.8724653720856 and batch: 900, loss is 4.042474131584168 and perplexity is 56.9671127248604
At time: 767.6664168834686 and batch: 950, loss is 4.034469828605652 and perplexity is 56.51295074464584
At time: 768.4615805149078 and batch: 1000, loss is 4.017841520309449 and perplexity is 55.58100578450747
At time: 769.2634422779083 and batch: 1050, loss is 4.023957901000976 and perplexity is 55.92200214381881
At time: 770.0559272766113 and batch: 1100, loss is 3.9823501205444334 and perplexity is 53.64295361401985
At time: 770.8555526733398 and batch: 1150, loss is 3.981357536315918 and perplexity is 53.58973488069863
At time: 771.6505279541016 and batch: 1200, loss is 4.01016857624054 and perplexity is 55.15616779920545
At time: 772.4429397583008 and batch: 1250, loss is 4.028234949111939 and perplexity is 56.161695461980905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348906356922901 and perplexity of 77.39377545803045
Finished 36 epochs...
Completing Train Step...
At time: 774.6698501110077 and batch: 50, loss is 4.030862512588501 and perplexity is 56.30945792436957
At time: 775.4625415802002 and batch: 100, loss is 4.048784875869751 and perplexity is 57.32775436765472
At time: 776.2509603500366 and batch: 150, loss is 3.970009112358093 and perplexity is 52.98501365589436
At time: 777.0403287410736 and batch: 200, loss is 4.027806253433227 and perplexity is 56.13762434578764
At time: 777.8324220180511 and batch: 250, loss is 4.043548474311828 and perplexity is 57.02834781597972
At time: 778.6215260028839 and batch: 300, loss is 4.052657270431519 and perplexity is 57.5501804347046
At time: 779.4399254322052 and batch: 350, loss is 4.036956114768982 and perplexity is 56.65363292774352
At time: 780.2320263385773 and batch: 400, loss is 4.055152535438538 and perplexity is 57.693962698914
At time: 781.0217506885529 and batch: 450, loss is 3.982764148712158 and perplexity is 53.66516790617002
At time: 781.8109104633331 and batch: 500, loss is 4.013829808235169 and perplexity is 55.35847745074727
At time: 782.6002290248871 and batch: 550, loss is 4.009788250923156 and perplexity is 55.13519450077256
At time: 783.3935468196869 and batch: 600, loss is 4.029783554077149 and perplexity is 56.248735120056566
At time: 784.1823046207428 and batch: 650, loss is 4.052184281349182 and perplexity is 57.522966264182585
At time: 784.9728417396545 and batch: 700, loss is 4.028562021255493 and perplexity is 56.180067392410876
At time: 785.7622396945953 and batch: 750, loss is 4.024932284355163 and perplexity is 55.97651816728022
At time: 786.554034948349 and batch: 800, loss is 4.042809739112854 and perplexity is 56.98623452530897
At time: 787.3434352874756 and batch: 850, loss is 4.089958453178406 and perplexity is 59.73740975308148
At time: 788.1323413848877 and batch: 900, loss is 4.042100172042847 and perplexity is 56.94581331232293
At time: 788.9429280757904 and batch: 950, loss is 4.034338817596436 and perplexity is 56.5055474109047
At time: 789.7544233798981 and batch: 1000, loss is 4.018046975135803 and perplexity is 55.59242634356385
At time: 790.5444805622101 and batch: 1050, loss is 4.024365353584289 and perplexity is 55.944792350705384
At time: 791.3358054161072 and batch: 1100, loss is 3.98266622543335 and perplexity is 53.65991309425925
At time: 792.1245293617249 and batch: 1150, loss is 3.9820671701431274 and perplexity is 53.627777465917376
At time: 792.9165301322937 and batch: 1200, loss is 4.010878977775573 and perplexity is 55.19536474661391
At time: 793.7064483165741 and batch: 1250, loss is 4.029009294509888 and perplexity is 56.205200854329235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348843539718294 and perplexity of 77.38891395009689
Finished 37 epochs...
Completing Train Step...
At time: 795.8569982051849 and batch: 50, loss is 4.029669919013977 and perplexity is 56.24234365464204
At time: 796.6755218505859 and batch: 100, loss is 4.047364373207092 and perplexity is 57.24637795132285
At time: 797.4650230407715 and batch: 150, loss is 3.968491177558899 and perplexity is 52.90464687103129
At time: 798.2524635791779 and batch: 200, loss is 4.025917854309082 and perplexity is 56.03171413698766
At time: 799.0438320636749 and batch: 250, loss is 4.041700234413147 and perplexity is 56.923043092362875
At time: 799.8630304336548 and batch: 300, loss is 4.05101863861084 and perplexity is 57.455954099977724
At time: 800.6512794494629 and batch: 350, loss is 4.035424437522888 and perplexity is 56.56692426907111
At time: 801.4403507709503 and batch: 400, loss is 4.0538476467132565 and perplexity is 57.61872759486636
At time: 802.2297582626343 and batch: 450, loss is 3.981307878494263 and perplexity is 53.58707379727373
At time: 803.0206274986267 and batch: 500, loss is 4.012502956390381 and perplexity is 55.2850736615628
At time: 803.8110663890839 and batch: 550, loss is 4.008405222892761 and perplexity is 55.058993687393304
At time: 804.6005065441132 and batch: 600, loss is 4.028535499572754 and perplexity is 56.17857742224554
At time: 805.3888776302338 and batch: 650, loss is 4.051049118041992 and perplexity is 57.457705351463446
At time: 806.1761698722839 and batch: 700, loss is 4.027399482727051 and perplexity is 56.114793848393035
At time: 806.9675240516663 and batch: 750, loss is 4.023962278366088 and perplexity is 55.92224693537577
At time: 807.7572686672211 and batch: 800, loss is 4.042026028633118 and perplexity is 56.94159131207284
At time: 808.5464389324188 and batch: 850, loss is 4.089395670890808 and perplexity is 59.70380005532385
At time: 809.3601658344269 and batch: 900, loss is 4.041706519126892 and perplexity is 56.92340083851837
At time: 810.1563391685486 and batch: 950, loss is 4.03416944026947 and perplexity is 56.495977462814494
At time: 810.9443275928497 and batch: 1000, loss is 4.018084807395935 and perplexity is 55.594529570483296
At time: 811.7324886322021 and batch: 1050, loss is 4.024412722587585 and perplexity is 55.94744246252471
At time: 812.5206072330475 and batch: 1100, loss is 3.9831461763381957 and perplexity is 53.685673399449186
At time: 813.30832862854 and batch: 1150, loss is 3.9824225091934204 and perplexity is 53.6468368955107
At time: 814.1001608371735 and batch: 1200, loss is 4.011212124824524 and perplexity is 55.213755982817915
At time: 814.8884618282318 and batch: 1250, loss is 4.029324731826782 and perplexity is 56.2229328686047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348794087876368 and perplexity of 77.3850870203826
Finished 38 epochs...
Completing Train Step...
At time: 817.1946778297424 and batch: 50, loss is 4.028554425239563 and perplexity is 56.17964064934478
At time: 817.982542514801 and batch: 100, loss is 4.046090135574341 and perplexity is 57.1734789174085
At time: 818.7705297470093 and batch: 150, loss is 3.9671827268600466 and perplexity is 52.83546901663594
At time: 819.5636510848999 and batch: 200, loss is 4.024386758804321 and perplexity is 55.94598987411187
At time: 820.3815877437592 and batch: 250, loss is 4.040219926834107 and perplexity is 56.83884181751979
At time: 821.170473575592 and batch: 300, loss is 4.049593338966369 and perplexity is 57.37412048159352
At time: 821.9598286151886 and batch: 350, loss is 4.034081478118896 and perplexity is 56.49100817369597
At time: 822.7517101764679 and batch: 400, loss is 4.052660088539124 and perplexity is 57.550342617534284
At time: 823.5410163402557 and batch: 450, loss is 3.9801089715957643 and perplexity is 53.52286638187532
At time: 824.3302690982819 and batch: 500, loss is 4.01136878490448 and perplexity is 55.22240645181877
At time: 825.119663476944 and batch: 550, loss is 4.007255239486694 and perplexity is 54.99571315105527
At time: 825.911847114563 and batch: 600, loss is 4.0274746179580685 and perplexity is 56.11901020478879
At time: 826.7018296718597 and batch: 650, loss is 4.050156192779541 and perplexity is 57.40642281397664
At time: 827.4899671077728 and batch: 700, loss is 4.026465272903442 and perplexity is 56.062395336143396
At time: 828.2789363861084 and batch: 750, loss is 4.023124575614929 and perplexity is 55.87542033139285
At time: 829.1183075904846 and batch: 800, loss is 4.041356706619263 and perplexity is 56.90349180334605
At time: 829.9132344722748 and batch: 850, loss is 4.088895630836487 and perplexity is 59.67395322682778
At time: 830.7033247947693 and batch: 900, loss is 4.0413130760192875 and perplexity is 56.90100912401875
At time: 831.4919719696045 and batch: 950, loss is 4.033814697265625 and perplexity is 56.47593946444419
At time: 832.3134579658508 and batch: 1000, loss is 4.017935085296631 and perplexity is 55.586206463898215
At time: 833.1027958393097 and batch: 1050, loss is 4.02438729763031 and perplexity is 55.946020019273284
At time: 833.8918764591217 and batch: 1100, loss is 3.9829297494888305 and perplexity is 53.674055635542665
At time: 834.6815612316132 and batch: 1150, loss is 3.9825114154815675 and perplexity is 53.651606648677294
At time: 835.4700672626495 and batch: 1200, loss is 4.01129011631012 and perplexity is 55.21806235360029
At time: 836.2627155780792 and batch: 1250, loss is 4.029481148719787 and perplexity is 56.23172777289751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348753100764142 and perplexity of 77.38191529413668
Finished 39 epochs...
Completing Train Step...
At time: 838.6045935153961 and batch: 50, loss is 4.027502746582031 and perplexity is 56.12058877752543
At time: 839.4668755531311 and batch: 100, loss is 4.044977841377258 and perplexity is 57.10992054294005
At time: 840.2904121875763 and batch: 150, loss is 3.966039204597473 and perplexity is 52.77508501336798
At time: 841.138439655304 and batch: 200, loss is 4.023049831390381 and perplexity is 55.87124412250456
At time: 841.9384953975677 and batch: 250, loss is 4.03890031337738 and perplexity is 56.76388598423297
At time: 842.728179693222 and batch: 300, loss is 4.0484076023101805 and perplexity is 57.306130201071866
At time: 843.5162358283997 and batch: 350, loss is 4.032970204353332 and perplexity is 56.428266066607925
At time: 844.3055009841919 and batch: 400, loss is 4.0517023229599 and perplexity is 57.4952492677712
At time: 845.0947208404541 and batch: 450, loss is 3.979127025604248 and perplexity is 53.470335613186975
At time: 845.8876008987427 and batch: 500, loss is 4.0104079961776735 and perplexity is 55.169374866386974
At time: 846.6747930049896 and batch: 550, loss is 4.006269941329956 and perplexity is 54.941552662755306
At time: 847.4637410640717 and batch: 600, loss is 4.026600017547607 and perplexity is 56.06994995261463
At time: 848.2514262199402 and batch: 650, loss is 4.049380826950073 and perplexity is 57.36192908702155
At time: 849.0406272411346 and batch: 700, loss is 4.025650215148926 and perplexity is 56.016719862690934
At time: 849.8286752700806 and batch: 750, loss is 4.022461466789245 and perplexity is 55.83838112890056
At time: 850.6174705028534 and batch: 800, loss is 4.040760855674744 and perplexity is 56.86959590346344
At time: 851.4068026542664 and batch: 850, loss is 4.088423252105713 and perplexity is 59.64577117736664
At time: 852.1964030265808 and batch: 900, loss is 4.040927033424378 and perplexity is 56.87904715020497
At time: 852.988335609436 and batch: 950, loss is 4.033475556373596 and perplexity is 56.45678941142262
At time: 853.7777950763702 and batch: 1000, loss is 4.017685575485229 and perplexity is 55.572338890126694
At time: 854.5668225288391 and batch: 1050, loss is 4.024245247840882 and perplexity is 55.93807346332687
At time: 855.3562095165253 and batch: 1100, loss is 3.982806849479675 and perplexity is 53.66745949895458
At time: 856.1468434333801 and batch: 1150, loss is 3.982483263015747 and perplexity is 53.65009624491581
At time: 856.9360842704773 and batch: 1200, loss is 4.0112479877471925 and perplexity is 55.215736144985975
At time: 857.7252464294434 and batch: 1250, loss is 4.029463310241699 and perplexity is 56.23072469340049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348721469405794 and perplexity of 77.37946763775585
Finished 40 epochs...
Completing Train Step...
At time: 859.9469649791718 and batch: 50, loss is 4.026490683555603 and perplexity is 56.063819936270534
At time: 860.7354965209961 and batch: 100, loss is 4.043926067352295 and perplexity is 57.04988538918734
At time: 861.55420088768 and batch: 150, loss is 3.9649860048294068 and perplexity is 52.71953156564648
At time: 862.3423030376434 and batch: 200, loss is 4.021861414909363 and perplexity is 55.804885253971
At time: 863.1305115222931 and batch: 250, loss is 4.037712206840515 and perplexity is 56.696484488244906
At time: 863.9203000068665 and batch: 300, loss is 4.04733829498291 and perplexity is 57.24488508691074
At time: 864.7122807502747 and batch: 350, loss is 4.0319719409942625 and perplexity is 56.37196390304682
At time: 865.5296113491058 and batch: 400, loss is 4.0508444166183475 and perplexity is 57.445944881112226
At time: 866.3222308158875 and batch: 450, loss is 3.978148159980774 and perplexity is 53.41802094847028
At time: 867.1143946647644 and batch: 500, loss is 4.0095078563690185 and perplexity is 55.11973705968564
At time: 867.9032726287842 and batch: 550, loss is 4.005367712974548 and perplexity is 54.89200519098088
At time: 868.692854642868 and batch: 600, loss is 4.025800285339355 and perplexity is 56.02512693331751
At time: 869.4817750453949 and batch: 650, loss is 4.048667521476745 and perplexity is 57.321027098583514
At time: 870.271669626236 and batch: 700, loss is 4.02491792678833 and perplexity is 55.975714486449
At time: 871.0569553375244 and batch: 750, loss is 4.022075505256653 and perplexity is 55.81683382022618
At time: 871.8443334102631 and batch: 800, loss is 4.040268745422363 and perplexity is 56.84161667726727
At time: 872.633709192276 and batch: 850, loss is 4.08798421382904 and perplexity is 59.61959014843558
At time: 873.4183189868927 and batch: 900, loss is 4.040509452819824 and perplexity is 56.855300521720366
At time: 874.2054557800293 and batch: 950, loss is 4.033102045059204 and perplexity is 56.43570609947608
At time: 875.003160238266 and batch: 1000, loss is 4.017425522804261 and perplexity is 55.557889033354044
At time: 875.8074331283569 and batch: 1050, loss is 4.024099631309509 and perplexity is 55.92992854812903
At time: 876.6078538894653 and batch: 1100, loss is 3.9826416301727297 and perplexity is 53.65859333094187
At time: 877.426144361496 and batch: 1150, loss is 3.982384748458862 and perplexity is 53.64481118978911
At time: 878.227486371994 and batch: 1200, loss is 4.01109806060791 and perplexity is 55.207458428165104
At time: 879.0275495052338 and batch: 1250, loss is 4.029353485107422 and perplexity is 56.22454948561329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348681818829836 and perplexity of 77.37639955812269
Finished 41 epochs...
Completing Train Step...
At time: 881.3233034610748 and batch: 50, loss is 4.025527081489563 and perplexity is 56.0098227436312
At time: 882.1129591464996 and batch: 100, loss is 4.042937774658203 and perplexity is 56.99353125603429
At time: 882.899653673172 and batch: 150, loss is 3.963998408317566 and perplexity is 52.66749164161985
At time: 883.6892004013062 and batch: 200, loss is 4.02075080871582 and perplexity is 55.7429424062001
At time: 884.4761486053467 and batch: 250, loss is 4.036633558273316 and perplexity is 56.63536187732101
At time: 885.2636406421661 and batch: 300, loss is 4.04636516571045 and perplexity is 57.189205509641255
At time: 886.0515646934509 and batch: 350, loss is 4.031038751602173 and perplexity is 56.31938272223715
At time: 886.8419380187988 and batch: 400, loss is 4.050003280639649 and perplexity is 57.397645346128385
At time: 887.627866268158 and batch: 450, loss is 3.977280740737915 and perplexity is 53.371705219665394
At time: 888.4130187034607 and batch: 500, loss is 4.00869122505188 and perplexity is 55.07474293051824
At time: 889.2060585021973 and batch: 550, loss is 4.004549531936646 and perplexity is 54.847111961100246
At time: 889.9942901134491 and batch: 600, loss is 4.025084085464478 and perplexity is 55.98501610981582
At time: 890.7808425426483 and batch: 650, loss is 4.048018436431885 and perplexity is 57.28383294952661
At time: 891.568473815918 and batch: 700, loss is 4.02433976650238 and perplexity is 55.943360905034446
At time: 892.3597912788391 and batch: 750, loss is 4.021142702102662 and perplexity is 55.764791977763835
At time: 893.1475546360016 and batch: 800, loss is 4.039603824615479 and perplexity is 56.80383406625573
At time: 893.9352719783783 and batch: 850, loss is 4.087450103759766 and perplexity is 59.587755227444006
At time: 894.7250916957855 and batch: 900, loss is 4.040062184333801 and perplexity is 56.829876623611675
At time: 895.5127432346344 and batch: 950, loss is 4.032729940414429 and perplexity is 56.41471001771733
At time: 896.3026187419891 and batch: 1000, loss is 4.017119402885437 and perplexity is 55.5408842597559
At time: 897.0945105552673 and batch: 1050, loss is 4.023939328193665 and perplexity is 55.92096352489357
At time: 897.884299993515 and batch: 1100, loss is 3.9824710655212403 and perplexity is 53.64944185215255
At time: 898.6722404956818 and batch: 1150, loss is 3.9822584533691407 and perplexity is 53.638036541358225
At time: 899.4619767665863 and batch: 1200, loss is 4.01092532157898 and perplexity is 55.19792276902046
At time: 900.2530000209808 and batch: 1250, loss is 4.029211177825927 and perplexity is 56.216548892107575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348645732350593 and perplexity of 77.3736073666666
Finished 42 epochs...
Completing Train Step...
At time: 902.4273755550385 and batch: 50, loss is 4.0246206521987915 and perplexity is 55.95907680200386
At time: 903.2439403533936 and batch: 100, loss is 4.042034430503845 and perplexity is 56.94206972997184
At time: 904.0323243141174 and batch: 150, loss is 3.963091878890991 and perplexity is 52.61976864504139
At time: 904.8228824138641 and batch: 200, loss is 4.019753174781799 and perplexity is 55.687359085786866
At time: 905.6154301166534 and batch: 250, loss is 4.035632772445679 and perplexity is 56.578710362556016
At time: 906.4048027992249 and batch: 300, loss is 4.045452699661255 and perplexity is 57.13704610169672
At time: 907.193195104599 and batch: 350, loss is 4.030175089836121 and perplexity is 56.27076282328835
At time: 907.9819447994232 and batch: 400, loss is 4.049234752655029 and perplexity is 57.353550595623815
At time: 908.7776250839233 and batch: 450, loss is 3.976472716331482 and perplexity is 53.32859699782809
At time: 909.5707221031189 and batch: 500, loss is 4.007913861274719 and perplexity is 55.03194645669703
At time: 910.3719916343689 and batch: 550, loss is 4.003770899772644 and perplexity is 54.804422857336114
At time: 911.1881453990936 and batch: 600, loss is 4.024383215904236 and perplexity is 55.9457916634107
At time: 911.978301525116 and batch: 650, loss is 4.047381858825684 and perplexity is 57.247378948404986
At time: 912.7708740234375 and batch: 700, loss is 4.023649549484253 and perplexity is 55.904761167916675
At time: 913.5663139820099 and batch: 750, loss is 4.020484180450439 and perplexity is 55.72808174338352
At time: 914.3619532585144 and batch: 800, loss is 4.039065036773682 and perplexity is 56.77323709447176
At time: 915.1581449508667 and batch: 850, loss is 4.08699221611023 and perplexity is 59.56047697592562
At time: 915.9506649971008 and batch: 900, loss is 4.039638390541077 and perplexity is 56.80579757729286
At time: 916.7437653541565 and batch: 950, loss is 4.032336339950562 and perplexity is 56.39250953103298
At time: 917.5367770195007 and batch: 1000, loss is 4.016774873733521 and perplexity is 55.5217521019875
At time: 918.3328609466553 and batch: 1050, loss is 4.023654470443725 and perplexity is 55.905036273657615
At time: 919.1290295124054 and batch: 1100, loss is 3.9821839427947996 and perplexity is 53.63404008933981
At time: 919.9228181838989 and batch: 1150, loss is 3.9820318746566774 and perplexity is 53.625884680828065
At time: 920.716141462326 and batch: 1200, loss is 4.010697178840637 and perplexity is 55.185331200161215
At time: 921.5630736351013 and batch: 1250, loss is 4.029007863998413 and perplexity is 56.20512045220198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348628802891195 and perplexity of 77.37229748441005
Finished 43 epochs...
Completing Train Step...
At time: 923.7806220054626 and batch: 50, loss is 4.0237404727935795 and perplexity is 55.909844444899875
At time: 924.5757067203522 and batch: 100, loss is 4.041149296760559 and perplexity is 56.89169068202811
At time: 925.3686156272888 and batch: 150, loss is 3.962219543457031 and perplexity is 52.57388657151438
At time: 926.164041519165 and batch: 200, loss is 4.018800978660583 and perplexity is 55.634359035690174
At time: 926.9604957103729 and batch: 250, loss is 4.034682893753052 and perplexity is 56.52499296766971
At time: 927.7894699573517 and batch: 300, loss is 4.044591999053955 and perplexity is 57.08788936906708
At time: 928.5802071094513 and batch: 350, loss is 4.029343638420105 and perplexity is 56.22399586278064
At time: 929.3701756000519 and batch: 400, loss is 4.0485048723220824 and perplexity is 57.31170464014706
At time: 930.1630070209503 and batch: 450, loss is 3.97569926738739 and perplexity is 53.28736599788236
At time: 930.9522383213043 and batch: 500, loss is 4.007178883552552 and perplexity is 54.9915140623185
At time: 931.7403531074524 and batch: 550, loss is 4.003046016693116 and perplexity is 54.76471045368716
At time: 932.5303463935852 and batch: 600, loss is 4.023723177909851 and perplexity is 55.90887749900254
At time: 933.319329738617 and batch: 650, loss is 4.0467778587341305 and perplexity is 57.21281196655993
At time: 934.1114506721497 and batch: 700, loss is 4.022998785972595 and perplexity is 55.86839222429489
At time: 934.8992009162903 and batch: 750, loss is 4.019982061386108 and perplexity is 55.700106635126936
At time: 935.6905760765076 and batch: 800, loss is 4.038527693748474 and perplexity is 56.74273858631473
At time: 936.4795117378235 and batch: 850, loss is 4.086502504348755 and perplexity is 59.5313166504914
At time: 937.2672724723816 and batch: 900, loss is 4.039214115142823 and perplexity is 56.78170138697444
At time: 938.0560259819031 and batch: 950, loss is 4.031940832138061 and perplexity is 56.370210263004985
At time: 938.8586580753326 and batch: 1000, loss is 4.016417269706726 and perplexity is 55.50190084951658
At time: 939.6672194004059 and batch: 1050, loss is 4.023436913490295 and perplexity is 55.89287506720999
At time: 940.4562871456146 and batch: 1100, loss is 3.9822477293014527 and perplexity is 53.637461326508024
At time: 941.2455439567566 and batch: 1150, loss is 3.981862020492554 and perplexity is 53.61677687453153
At time: 942.0856773853302 and batch: 1200, loss is 4.0104816198349 and perplexity is 55.173436787056445
At time: 942.8749191761017 and batch: 1250, loss is 4.028784441947937 and perplexity is 56.19256439164591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348603408702099 and perplexity of 77.37033270260405
Finished 44 epochs...
Completing Train Step...
At time: 945.0321698188782 and batch: 50, loss is 4.02288161277771 and perplexity is 55.86184632979408
At time: 945.850435256958 and batch: 100, loss is 4.040291466712952 and perplexity is 56.84290820682987
At time: 946.6388244628906 and batch: 150, loss is 3.9613790369033812 and perplexity is 52.52971644054279
At time: 947.4272162914276 and batch: 200, loss is 4.017902331352234 and perplexity is 55.58438582619917
At time: 948.2179353237152 and batch: 250, loss is 4.033771486282348 and perplexity is 56.47349913629331
At time: 949.0066854953766 and batch: 300, loss is 4.043766160011291 and perplexity is 57.04076342306427
At time: 949.7992308139801 and batch: 350, loss is 4.028551807403565 and perplexity is 56.179493580451606
At time: 950.5869400501251 and batch: 400, loss is 4.047786021232605 and perplexity is 57.270520863101666
At time: 951.3782374858856 and batch: 450, loss is 3.9749596166610717 and perplexity is 53.24796653163657
At time: 952.1678447723389 and batch: 500, loss is 4.0064604330062865 and perplexity is 54.952019568119766
At time: 952.9560039043427 and batch: 550, loss is 4.002325053215027 and perplexity is 54.72524132717094
At time: 953.7469732761383 and batch: 600, loss is 4.0230910158157345 and perplexity is 55.873545194971385
At time: 954.5648772716522 and batch: 650, loss is 4.046195163726806 and perplexity is 57.17948405761811
At time: 955.3636116981506 and batch: 700, loss is 4.022361059188842 and perplexity is 55.83277481250412
At time: 956.1492788791656 and batch: 750, loss is 4.019325513839721 and perplexity is 55.66354886905115
At time: 956.9388604164124 and batch: 800, loss is 4.037939610481263 and perplexity is 56.70937894130232
At time: 957.7258179187775 and batch: 850, loss is 4.0859857940673825 and perplexity is 59.50056415286451
At time: 958.5115432739258 and batch: 900, loss is 4.0387580823898315 and perplexity is 56.755812974802005
At time: 959.30122423172 and batch: 950, loss is 4.031538443565369 and perplexity is 56.34753209757818
At time: 960.089827299118 and batch: 1000, loss is 4.016143703460694 and perplexity is 55.48671947950341
At time: 960.8787312507629 and batch: 1050, loss is 4.02308744430542 and perplexity is 55.873345642384784
At time: 961.6704366207123 and batch: 1100, loss is 3.9816789293289183 and perplexity is 53.60696101508917
At time: 962.528041601181 and batch: 1150, loss is 3.981568069458008 and perplexity is 53.60101848371136
At time: 963.3222367763519 and batch: 1200, loss is 4.0101934337615965 and perplexity is 55.15753886184846
At time: 964.1088578701019 and batch: 1250, loss is 4.02855580329895 and perplexity is 56.179718068279264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348593161924042 and perplexity of 77.36953991003848
Finished 45 epochs...
Completing Train Step...
At time: 966.2846434116364 and batch: 50, loss is 4.022078943252564 and perplexity is 55.817025718602494
At time: 967.0863173007965 and batch: 100, loss is 4.039501399993896 and perplexity is 56.798016252995815
At time: 967.8828840255737 and batch: 150, loss is 3.960595545768738 and perplexity is 52.48857599209914
At time: 968.6716051101685 and batch: 200, loss is 4.017054305076599 and perplexity is 55.53726878757062
At time: 969.4625768661499 and batch: 250, loss is 4.032924132347107 and perplexity is 56.425666363069666
At time: 970.2548863887787 and batch: 300, loss is 4.042992463111878 and perplexity is 56.996648229358605
At time: 971.044766664505 and batch: 350, loss is 4.02780387878418 and perplexity is 56.13749103878976
At time: 971.8353691101074 and batch: 400, loss is 4.047130975723267 and perplexity is 57.23301834986024
At time: 972.6252446174622 and batch: 450, loss is 3.9742384481430055 and perplexity is 53.209579617903465
At time: 973.418568611145 and batch: 500, loss is 4.005792589187622 and perplexity is 54.915332453523
At time: 974.2085564136505 and batch: 550, loss is 4.001659293174743 and perplexity is 54.68881957371974
At time: 974.9999554157257 and batch: 600, loss is 4.022492361068726 and perplexity is 55.84010624210082
At time: 975.7929892539978 and batch: 650, loss is 4.045637288093567 and perplexity is 57.14759391293558
At time: 976.5833258628845 and batch: 700, loss is 4.0217626094818115 and perplexity is 55.79937170081298
At time: 977.3725588321686 and batch: 750, loss is 4.018738512992859 and perplexity is 55.630883906843835
At time: 978.1708693504333 and batch: 800, loss is 4.037415571212769 and perplexity is 56.67966878516933
At time: 978.9627833366394 and batch: 850, loss is 4.085503525733948 and perplexity is 59.47187583324218
At time: 979.7688794136047 and batch: 900, loss is 4.038315916061402 and perplexity is 56.73072301273888
At time: 980.5776743888855 and batch: 950, loss is 4.03112211227417 and perplexity is 56.32407773951452
At time: 981.3722009658813 and batch: 1000, loss is 4.0157481288909915 and perplexity is 55.46477468500859
At time: 982.1620557308197 and batch: 1050, loss is 4.0227626514434816 and perplexity is 55.855201325279346
At time: 983.0035846233368 and batch: 1100, loss is 3.9813581705093384 and perplexity is 53.58976886696667
At time: 983.7937698364258 and batch: 1150, loss is 3.981296892166138 and perplexity is 53.58648507533169
At time: 984.5839657783508 and batch: 1200, loss is 4.009894552230835 and perplexity is 55.14105575557113
At time: 985.3747110366821 and batch: 1250, loss is 4.02828588962555 and perplexity is 56.164556440462235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348581578609717 and perplexity of 77.36864371952893
Finished 46 epochs...
Completing Train Step...
At time: 987.5544993877411 and batch: 50, loss is 4.021280903816223 and perplexity is 55.77249930014185
At time: 988.3882660865784 and batch: 100, loss is 4.038711810112 and perplexity is 56.753186814815145
At time: 989.1810405254364 and batch: 150, loss is 3.9598220682144163 and perplexity is 52.44799295376887
At time: 989.9717209339142 and batch: 200, loss is 4.016235208511352 and perplexity is 55.49179702688732
At time: 990.761438369751 and batch: 250, loss is 4.03210832118988 and perplexity is 56.37965244678189
At time: 991.5500829219818 and batch: 300, loss is 4.042235670089721 and perplexity is 56.95352988158264
At time: 992.3422346115112 and batch: 350, loss is 4.027065620422364 and perplexity is 56.0960623610345
At time: 993.1320428848267 and batch: 400, loss is 4.046478319168091 and perplexity is 57.19567703211508
At time: 993.9219262599945 and batch: 450, loss is 3.9735889053344726 and perplexity is 53.17502894040441
At time: 994.7135992050171 and batch: 500, loss is 4.005134696960449 and perplexity is 54.87921596483049
At time: 995.5054633617401 and batch: 550, loss is 4.00101104259491 and perplexity is 54.65337900314698
At time: 996.2947969436646 and batch: 600, loss is 4.021917657852173 and perplexity is 55.808023973205415
At time: 997.0985085964203 and batch: 650, loss is 4.045101108551026 and perplexity is 57.11696075534412
At time: 997.9190125465393 and batch: 700, loss is 4.021163406372071 and perplexity is 55.76594655899282
At time: 998.7143993377686 and batch: 750, loss is 4.018159909248352 and perplexity is 55.598704979426806
At time: 999.5026426315308 and batch: 800, loss is 4.036896033287048 and perplexity is 56.650229195771956
At time: 1000.2911069393158 and batch: 850, loss is 4.085000495910645 and perplexity is 59.44196722914087
At time: 1001.0796658992767 and batch: 900, loss is 4.0378691816329955 and perplexity is 56.70538510569981
At time: 1001.8683044910431 and batch: 950, loss is 4.0306923198699955 and perplexity is 56.299875280119096
At time: 1002.659857749939 and batch: 1000, loss is 4.0153542470932 and perplexity is 55.44293242175893
At time: 1003.4963262081146 and batch: 1050, loss is 4.022435312271118 and perplexity is 55.83692072204733
At time: 1004.283483505249 and batch: 1100, loss is 3.981020312309265 and perplexity is 53.571666182357355
At time: 1005.071005821228 and batch: 1150, loss is 3.980994620323181 and perplexity is 53.570289837535896
At time: 1005.8826291561127 and batch: 1200, loss is 4.00958324432373 and perplexity is 55.12389258056296
At time: 1006.6789429187775 and batch: 1250, loss is 4.028018708229065 and perplexity is 56.14955232033968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348562421589873 and perplexity of 77.3671615810826
Finished 47 epochs...
Completing Train Step...
At time: 1008.8736228942871 and batch: 50, loss is 4.02050085067749 and perplexity is 55.72901075090267
At time: 1009.6644487380981 and batch: 100, loss is 4.037948493957519 and perplexity is 56.70988271996133
At time: 1010.452071428299 and batch: 150, loss is 3.959074687957764 and perplexity is 52.408809003813396
At time: 1011.2406284809113 and batch: 200, loss is 4.01543206691742 and perplexity is 55.44724714889767
At time: 1012.0297281742096 and batch: 250, loss is 4.031303606033325 and perplexity is 56.33430113582765
At time: 1012.8185358047485 and batch: 300, loss is 4.041495876312256 and perplexity is 56.91141159591531
At time: 1013.6102035045624 and batch: 350, loss is 4.026359324455261 and perplexity is 56.05645592699737
At time: 1014.3993191719055 and batch: 400, loss is 4.045846004486084 and perplexity is 57.159522797411654
At time: 1015.1880421638489 and batch: 450, loss is 3.97286141872406 and perplexity is 53.13635888652063
At time: 1015.9771292209625 and batch: 500, loss is 4.0044780492782595 and perplexity is 54.8431914838572
At time: 1016.7939386367798 and batch: 550, loss is 4.000372023582458 and perplexity is 54.61846561121792
At time: 1017.5900075435638 and batch: 600, loss is 4.021345005035401 and perplexity is 55.776074499929706
At time: 1018.3832597732544 and batch: 650, loss is 4.044566502571106 and perplexity is 57.086433847230346
At time: 1019.175549030304 and batch: 700, loss is 4.020596642494201 and perplexity is 55.734349389780874
At time: 1019.9686019420624 and batch: 750, loss is 4.0177064037323 and perplexity is 55.57349637658553
At time: 1020.7617564201355 and batch: 800, loss is 4.036377115249634 and perplexity is 56.620839995971025
At time: 1021.5625350475311 and batch: 850, loss is 4.084539456367493 and perplexity is 59.41456844817152
At time: 1022.3894865512848 and batch: 900, loss is 4.0374440097808835 and perplexity is 56.681280696710914
At time: 1023.2334449291229 and batch: 950, loss is 4.030284557342529 and perplexity is 56.27692298054072
At time: 1024.0288894176483 and batch: 1000, loss is 4.014952626228332 and perplexity is 55.42066985414396
At time: 1024.8264293670654 and batch: 1050, loss is 4.022092895507813 and perplexity is 55.817804497425406
At time: 1025.6211714744568 and batch: 1100, loss is 3.980676321983337 and perplexity is 53.55324121663345
At time: 1026.4181506633759 and batch: 1150, loss is 3.980697717666626 and perplexity is 53.554387037079366
At time: 1027.2103271484375 and batch: 1200, loss is 4.0092661094665525 and perplexity is 55.10641364449878
At time: 1028.0038723945618 and batch: 1250, loss is 4.027719140052795 and perplexity is 56.132734220562234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348560639541515 and perplexity of 77.3670237091822
Finished 48 epochs...
Completing Train Step...
At time: 1030.2372555732727 and batch: 50, loss is 4.019734892845154 and perplexity is 55.686341022322225
At time: 1031.0317566394806 and batch: 100, loss is 4.03719916343689 and perplexity is 56.6674041912346
At time: 1031.825352191925 and batch: 150, loss is 3.958344969749451 and perplexity is 52.37057929176221
At time: 1032.6357100009918 and batch: 200, loss is 4.0146663904190065 and perplexity is 55.40480874397202
At time: 1033.429854631424 and batch: 250, loss is 4.030526804924011 and perplexity is 56.29055758043373
At time: 1034.2223808765411 and batch: 300, loss is 4.040775489807129 and perplexity is 56.87042814674814
At time: 1035.0193481445312 and batch: 350, loss is 4.0256563091278075 and perplexity is 56.017061228438926
At time: 1035.812138080597 and batch: 400, loss is 4.0452022361755375 and perplexity is 57.122737149976246
At time: 1036.60542178154 and batch: 450, loss is 3.97218581199646 and perplexity is 53.10047172914425
At time: 1037.3976027965546 and batch: 500, loss is 4.003831543922424 and perplexity is 54.807746525743994
At time: 1038.19242811203 and batch: 550, loss is 3.999733290672302 and perplexity is 54.58359013897146
At time: 1038.9817838668823 and batch: 600, loss is 4.020784215927124 and perplexity is 55.74480465356184
At time: 1039.775130033493 and batch: 650, loss is 4.044046535491943 and perplexity is 57.05675849673422
At time: 1040.5700919628143 and batch: 700, loss is 4.020033946037293 and perplexity is 55.702996690704744
At time: 1041.3653025627136 and batch: 750, loss is 4.01712345123291 and perplexity is 55.54110910900948
At time: 1042.1984674930573 and batch: 800, loss is 4.035824069976806 and perplexity is 56.58953476547053
At time: 1043.0079882144928 and batch: 850, loss is 4.084042558670044 and perplexity is 59.385052819645935
At time: 1043.8474900722504 and batch: 900, loss is 4.036989030838012 and perplexity is 56.655497773327326
At time: 1044.6356236934662 and batch: 950, loss is 4.029867386817932 and perplexity is 56.25345080334883
At time: 1045.4242074489594 and batch: 1000, loss is 4.014550185203552 and perplexity is 55.39837079030385
At time: 1046.2151730060577 and batch: 1050, loss is 4.021756167411804 and perplexity is 55.79901223851197
At time: 1047.0054631233215 and batch: 1100, loss is 3.980350522994995 and perplexity is 53.53579646671787
At time: 1047.795305967331 and batch: 1150, loss is 3.980402102470398 and perplexity is 53.538557886230585
At time: 1048.5845510959625 and batch: 1200, loss is 4.008938422203064 and perplexity is 55.088358932922034
At time: 1049.373220205307 and batch: 1250, loss is 4.02742479801178 and perplexity is 56.11621442835793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348549947251368 and perplexity of 77.3661964829394
Finished 49 epochs...
Completing Train Step...
At time: 1051.5296123027802 and batch: 50, loss is 4.018994493484497 and perplexity is 55.645126150647094
At time: 1052.3499279022217 and batch: 100, loss is 4.036482682228089 and perplexity is 56.62681760248036
At time: 1053.148831129074 and batch: 150, loss is 3.9576378154754637 and perplexity is 52.33355830410243
At time: 1053.959686756134 and batch: 200, loss is 4.013918685913086 and perplexity is 55.36339780232758
At time: 1054.7492668628693 and batch: 250, loss is 4.02977677822113 and perplexity is 56.24835398801742
At time: 1055.5425939559937 and batch: 300, loss is 4.04007306098938 and perplexity is 56.83049474596785
At time: 1056.3327593803406 and batch: 350, loss is 4.024973659515381 and perplexity is 55.9788342526017
At time: 1057.1223719120026 and batch: 400, loss is 4.044576339721679 and perplexity is 57.08699541783791
At time: 1057.9155104160309 and batch: 450, loss is 3.971540956497192 and perplexity is 53.066240636176495
At time: 1058.7049565315247 and batch: 500, loss is 4.003217825889587 and perplexity is 54.774120342914806
At time: 1059.494502544403 and batch: 550, loss is 3.9991285514831545 and perplexity is 54.55059128178678
At time: 1060.2852261066437 and batch: 600, loss is 4.020240411758423 and perplexity is 55.71449863742362
At time: 1061.079313993454 and batch: 650, loss is 4.0435353994369505 and perplexity is 57.02760218234208
At time: 1061.8696262836456 and batch: 700, loss is 4.019476547241211 and perplexity is 55.67195655908084
At time: 1062.659718990326 and batch: 750, loss is 4.016563720703125 and perplexity is 55.51002975342656
At time: 1063.4520959854126 and batch: 800, loss is 4.035305371284485 and perplexity is 56.560189459132566
At time: 1064.2893981933594 and batch: 850, loss is 4.08355754852295 and perplexity is 59.356257450030384
At time: 1065.07750248909 and batch: 900, loss is 4.03653892993927 and perplexity is 56.630002820941755
At time: 1065.8660271167755 and batch: 950, loss is 4.0294359588623045 and perplexity is 56.22918672654864
At time: 1066.658433675766 and batch: 1000, loss is 4.014150404930115 and perplexity is 55.376228040893466
At time: 1067.449468612671 and batch: 1050, loss is 4.021448731422424 and perplexity is 55.781860250682264
At time: 1068.2385110855103 and batch: 1100, loss is 3.9800062227249144 and perplexity is 53.51736725030955
At time: 1069.0380432605743 and batch: 1150, loss is 3.980088973045349 and perplexity is 53.52179601283656
At time: 1069.8284401893616 and batch: 1200, loss is 4.008596324920655 and perplexity is 55.06951657818161
At time: 1070.6175413131714 and batch: 1250, loss is 4.02710569858551 and perplexity is 56.09831063322669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348544155594206 and perplexity of 77.36574840575102
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f17ffca5e80>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -156.3131194155981, 'params': {'wordvec_dim': 200, 'anneal': 6.21590689787748, 'num_layers': 1, 'dropout': 0.6665903176202587, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 29.37138080686878, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}, {'best_accuracy': -75.54462303180635, 'params': {'wordvec_dim': 200, 'anneal': 6.113648271593582, 'num_layers': 1, 'dropout': 0.5372937996772553, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 3.8840390778038114, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}, {'best_accuracy': -200.88902496700308, 'params': {'wordvec_dim': 200, 'anneal': 2.8037930852815878, 'num_layers': 1, 'dropout': 0.8024002944436802, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 16.941056461945752, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}, {'best_accuracy': -145.00908474140658, 'params': {'wordvec_dim': 200, 'anneal': 7.0423698389972795, 'num_layers': 1, 'dropout': 0.05605163182934092, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 19.32387795030508, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}, {'best_accuracy': -165.78303326238725, 'params': {'wordvec_dim': 200, 'anneal': 7.542203663166058, 'num_layers': 1, 'dropout': 0.455943817025638, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 28.082956472718813, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}, {'best_accuracy': -77.36574840575102, 'params': {'wordvec_dim': 200, 'anneal': 6.850360280317041, 'num_layers': 1, 'dropout': 0.45900980452330353, 'wordvec_source': 'glove', 'tune_wordvecs': True, 'lr': 1.840699063396855, 'batch_size': 50, 'data': 'wikitext', 'seq_len': 35}}]
