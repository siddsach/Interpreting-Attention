Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'lr': 11.358133663123011, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 3.2445999806025085, 'wordvec_source': '', 'dropout': 0.2746691213793858, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.89060378074646 and batch: 50, loss is 7.01403886795044 and perplexity is 1112.1372216375125
At time: 3.083038568496704 and batch: 100, loss is 6.152014579772949 and perplexity is 469.6626071402813
At time: 4.275468587875366 and batch: 150, loss is 5.857212247848511 and perplexity is 349.74777360453277
At time: 5.468590259552002 and batch: 200, loss is 5.779820404052734 and perplexity is 323.7010497998197
At time: 6.659189701080322 and batch: 250, loss is 5.77106635093689 and perplexity is 320.87972066608916
At time: 7.849918365478516 and batch: 300, loss is 5.75212607383728 and perplexity is 314.8593634326894
At time: 9.041926622390747 and batch: 350, loss is 5.758047933578491 and perplexity is 316.72944814575885
At time: 10.233879566192627 and batch: 400, loss is 5.694965410232544 and perplexity is 297.3665075698675
At time: 11.425193309783936 and batch: 450, loss is 5.667179069519043 and perplexity is 289.2175200452682
At time: 12.616724729537964 and batch: 500, loss is 5.645269498825074 and perplexity is 282.94980058724946
At time: 13.807960271835327 and batch: 550, loss is 5.660652942657471 and perplexity is 287.3361953777696
At time: 15.000368595123291 and batch: 600, loss is 5.6696237945556645 and perplexity is 289.92544234264926
At time: 16.192041635513306 and batch: 650, loss is 5.642488012313843 and perplexity is 282.1638730636683
At time: 17.385038137435913 and batch: 700, loss is 5.643645324707031 and perplexity is 282.49061384498384
At time: 18.57577633857727 and batch: 750, loss is 5.607474269866944 and perplexity is 272.45521987537177
At time: 19.768231868743896 and batch: 800, loss is 5.6117163753509525 and perplexity is 273.61345860626864
At time: 20.960267066955566 and batch: 850, loss is 5.658463373184204 and perplexity is 286.7077410890474
At time: 22.15327525138855 and batch: 900, loss is 5.642720079421997 and perplexity is 282.2293616162913
At time: 23.354503870010376 and batch: 950, loss is 5.607681636810303 and perplexity is 272.51172393985456
At time: 24.54670000076294 and batch: 1000, loss is 5.588759965896607 and perplexity is 267.4038241759166
At time: 25.738426685333252 and batch: 1050, loss is 5.584381885528565 and perplexity is 266.23566775020424
At time: 26.929751873016357 and batch: 1100, loss is 5.561701889038086 and perplexity is 260.2654024523337
At time: 28.12419319152832 and batch: 1150, loss is 5.605541954040527 and perplexity is 271.92925866725477
At time: 29.317504405975342 and batch: 1200, loss is 5.58907039642334 and perplexity is 267.48684737170646
At time: 30.511611223220825 and batch: 1250, loss is 5.569410333633423 and perplexity is 262.27939629455847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.145659732122491 and perplexity of 171.68471323695732
Finished 1 epochs...
Completing Train Step...
At time: 34.310253620147705 and batch: 50, loss is 5.350263805389404 and perplexity is 210.66386479981085
At time: 35.52873206138611 and batch: 100, loss is 5.358168697357177 and perplexity is 212.3357391798429
At time: 36.72424912452698 and batch: 150, loss is 5.23761215209961 and perplexity is 188.22012435115423
At time: 37.918379068374634 and batch: 200, loss is 5.263235063552856 and perplexity is 193.10518946199142
At time: 39.10989737510681 and batch: 250, loss is 5.272790718078613 and perplexity is 194.95928035750188
At time: 40.299360275268555 and batch: 300, loss is 5.26302206993103 and perplexity is 193.06406366821585
At time: 41.48824977874756 and batch: 350, loss is 5.285279369354248 and perplexity is 197.40932586113672
At time: 42.678287744522095 and batch: 400, loss is 5.24411376953125 and perplexity is 189.4478463572662
At time: 43.86893391609192 and batch: 450, loss is 5.2098416900634765 and perplexity is 183.06507487922158
At time: 45.05882811546326 and batch: 500, loss is 5.209727602005005 and perplexity is 183.04419053160464
At time: 46.251091957092285 and batch: 550, loss is 5.217717370986938 and perplexity is 184.51252936033677
At time: 47.44000601768494 and batch: 600, loss is 5.22547643661499 and perplexity is 185.94974269073325
At time: 48.62954640388489 and batch: 650, loss is 5.219384689331054 and perplexity is 184.82042709570126
At time: 49.81811213493347 and batch: 700, loss is 5.226126766204834 and perplexity is 186.07071064088416
At time: 51.00689244270325 and batch: 750, loss is 5.19484130859375 and perplexity is 180.33952219376852
At time: 52.19540214538574 and batch: 800, loss is 5.223432598114013 and perplexity is 185.57007956459566
At time: 53.384501457214355 and batch: 850, loss is 5.266580114364624 and perplexity is 193.7522177006057
At time: 54.57299304008484 and batch: 900, loss is 5.23834641456604 and perplexity is 188.35837807494073
At time: 55.76198101043701 and batch: 950, loss is 5.217145471572876 and perplexity is 184.40703692131578
At time: 56.9529972076416 and batch: 1000, loss is 5.20889986038208 and perplexity is 182.89273992589938
At time: 58.14453363418579 and batch: 1050, loss is 5.2035973930358885 and perplexity is 181.92552372694203
At time: 59.33592367172241 and batch: 1100, loss is 5.187597093582153 and perplexity is 179.03782450149626
At time: 60.52698493003845 and batch: 1150, loss is 5.204236698150635 and perplexity is 182.0418668301715
At time: 61.717164039611816 and batch: 1200, loss is 5.194965867996216 and perplexity is 180.36198657593997
At time: 62.906139850616455 and batch: 1250, loss is 5.200022659301758 and perplexity is 181.27634942411706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.040728715214416 and perplexity of 154.58262069976217
Finished 2 epochs...
Completing Train Step...
At time: 65.84051060676575 and batch: 50, loss is 5.127778539657593 and perplexity is 168.6420699514305
At time: 67.06272196769714 and batch: 100, loss is 5.147841053009033 and perplexity is 172.05962143672838
At time: 68.24965596199036 and batch: 150, loss is 5.052334098815918 and perplexity is 156.38706167341294
At time: 69.43590259552002 and batch: 200, loss is 5.094241838455201 and perplexity is 163.08015663909168
At time: 70.6226282119751 and batch: 250, loss is 5.098212442398071 and perplexity is 163.7289705912484
At time: 71.81018853187561 and batch: 300, loss is 5.09479284286499 and perplexity is 163.17003928514208
At time: 72.99574613571167 and batch: 350, loss is 5.113546619415283 and perplexity is 166.25896774679427
At time: 74.18319272994995 and batch: 400, loss is 5.092183399200439 and perplexity is 162.74481130540218
At time: 75.36898612976074 and batch: 450, loss is 5.050201406478882 and perplexity is 156.05389158629822
At time: 76.55522656440735 and batch: 500, loss is 5.06415904045105 and perplexity is 158.2473065172078
At time: 77.74156260490417 and batch: 550, loss is 5.076589946746826 and perplexity is 160.22674155211567
At time: 78.92947626113892 and batch: 600, loss is 5.085318822860717 and perplexity is 161.6314628290752
At time: 80.11452198028564 and batch: 650, loss is 5.089727821350098 and perplexity is 162.34566901455136
At time: 81.2975389957428 and batch: 700, loss is 5.099212846755981 and perplexity is 163.89284772497592
At time: 82.4809319972992 and batch: 750, loss is 5.061042900085449 and perplexity is 157.7549532170278
At time: 83.67051696777344 and batch: 800, loss is 5.084671564102173 and perplexity is 161.52687929905687
At time: 84.86443185806274 and batch: 850, loss is 5.130463161468506 and perplexity is 169.09541839293584
At time: 86.05577540397644 and batch: 900, loss is 5.087177867889404 and perplexity is 161.93222247276722
At time: 87.2478084564209 and batch: 950, loss is 5.076103754043579 and perplexity is 160.14885941390585
At time: 88.44011306762695 and batch: 1000, loss is 5.06376012802124 and perplexity is 158.1841922890162
At time: 89.67857027053833 and batch: 1050, loss is 5.057604122161865 and perplexity is 157.21340063975617
At time: 90.87073349952698 and batch: 1100, loss is 5.029705743789673 and perplexity is 152.88801782719946
At time: 92.06373810768127 and batch: 1150, loss is 5.068979177474976 and perplexity is 159.01192151240795
At time: 93.25482249259949 and batch: 1200, loss is 5.0815040874481205 and perplexity is 161.01605611721334
At time: 94.45205473899841 and batch: 1250, loss is 5.091670570373535 and perplexity is 162.66137247148396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.000472242814781 and perplexity of 148.48326270223953
Finished 3 epochs...
Completing Train Step...
At time: 97.43183422088623 and batch: 50, loss is 5.034228010177612 and perplexity is 153.58098387889896
At time: 98.61944580078125 and batch: 100, loss is 5.050984449386597 and perplexity is 156.1761363345215
At time: 99.81149768829346 and batch: 150, loss is 4.963003187179566 and perplexity is 143.02267543713737
At time: 101.00182771682739 and batch: 200, loss is 5.005174398422241 and perplexity is 149.18309818678082
At time: 102.19331789016724 and batch: 250, loss is 5.006543550491333 and perplexity is 149.38749242580502
At time: 103.38115572929382 and batch: 300, loss is 5.012516384124756 and perplexity is 150.28242905823635
At time: 104.56834483146667 and batch: 350, loss is 5.008825035095215 and perplexity is 149.72870677942757
At time: 105.75579643249512 and batch: 400, loss is 4.997787294387817 and perplexity is 148.08512752500354
At time: 106.94593381881714 and batch: 450, loss is 4.959608039855957 and perplexity is 142.53791576431752
At time: 108.13365435600281 and batch: 500, loss is 4.972277622222901 and perplexity is 144.35530006834801
At time: 109.32088851928711 and batch: 550, loss is 4.989505777359009 and perplexity is 146.8638221305139
At time: 110.5081434249878 and batch: 600, loss is 5.005025100708008 and perplexity is 149.16082715376783
At time: 111.69569730758667 and batch: 650, loss is 5.0097990989685055 and perplexity is 149.87462315789523
At time: 112.88950300216675 and batch: 700, loss is 5.0088133621215825 and perplexity is 149.72695901038222
At time: 114.07993745803833 and batch: 750, loss is 4.968884048461914 and perplexity is 143.86624999286755
At time: 115.26731634140015 and batch: 800, loss is 4.996554765701294 and perplexity is 147.9027207911392
At time: 116.4546959400177 and batch: 850, loss is 5.041483936309814 and perplexity is 154.69940885078324
At time: 117.642751455307 and batch: 900, loss is 4.9989094734191895 and perplexity is 148.25139882560515
At time: 118.83013844490051 and batch: 950, loss is 4.980098972320556 and perplexity is 145.48878030326588
At time: 120.04568982124329 and batch: 1000, loss is 4.9777115631103515 and perplexity is 145.14185334228162
At time: 121.23069643974304 and batch: 1050, loss is 4.956375494003296 and perplexity is 142.07789932855013
At time: 122.41604948043823 and batch: 1100, loss is 4.932581119537353 and perplexity is 138.73714775415561
At time: 123.60242986679077 and batch: 1150, loss is 4.971530847549438 and perplexity is 144.2475394277434
At time: 124.78898644447327 and batch: 1200, loss is 4.992007627487182 and perplexity is 147.23171341527046
At time: 125.97543096542358 and batch: 1250, loss is 4.989614677429199 and perplexity is 146.8798164819296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.988074086878422 and perplexity of 146.65370903877346
Finished 4 epochs...
Completing Train Step...
At time: 128.89036178588867 and batch: 50, loss is 4.9348078536987305 and perplexity is 139.04642270927476
At time: 130.09606957435608 and batch: 100, loss is 4.940161170959473 and perplexity is 139.79277828066932
At time: 131.28039646148682 and batch: 150, loss is 4.860783596038818 and perplexity is 129.12534458330762
At time: 132.4640874862671 and batch: 200, loss is 4.911106433868408 and perplexity is 135.7895735056945
At time: 133.65025734901428 and batch: 250, loss is 4.906347932815552 and perplexity is 135.1449536057984
At time: 134.83473300933838 and batch: 300, loss is 4.9132781505584715 and perplexity is 136.08479043655245
At time: 136.0187487602234 and batch: 350, loss is 4.9204073333740235 and perplexity is 137.05843029046116
At time: 137.20316219329834 and batch: 400, loss is 4.910882596969604 and perplexity is 135.7591821901469
At time: 138.38998937606812 and batch: 450, loss is 4.855048418045044 and perplexity is 128.38690730729442
At time: 139.57427835464478 and batch: 500, loss is 4.874499950408936 and perplexity is 130.90867601135332
At time: 140.75856566429138 and batch: 550, loss is 4.885278959274292 and perplexity is 132.3273741342868
At time: 141.9494194984436 and batch: 600, loss is 4.916626892089844 and perplexity is 136.54126711069378
At time: 143.13817954063416 and batch: 650, loss is 4.925667200088501 and perplexity is 137.78123863722075
At time: 144.35721826553345 and batch: 700, loss is 4.923727550506592 and perplexity is 137.51425033088768
At time: 145.568115234375 and batch: 750, loss is 4.891732025146484 and perplexity is 133.1840525244923
At time: 146.7807879447937 and batch: 800, loss is 4.908427066802979 and perplexity is 135.4262303767914
At time: 147.99172949790955 and batch: 850, loss is 4.9493435287475585 and perplexity is 141.08231702943576
At time: 149.20375514030457 and batch: 900, loss is 4.925209341049194 and perplexity is 137.71816869133798
At time: 150.45555353164673 and batch: 950, loss is 4.903911476135254 and perplexity is 134.81607958711862
At time: 151.6673800945282 and batch: 1000, loss is 4.926286478042602 and perplexity is 137.86658994616178
At time: 152.88022136688232 and batch: 1050, loss is 4.886016311645508 and perplexity is 132.42498201866172
At time: 154.0934181213379 and batch: 1100, loss is 4.8509651470184325 and perplexity is 127.86373761721399
At time: 155.30661463737488 and batch: 1150, loss is 4.892667284011841 and perplexity is 133.3086723571578
At time: 156.51804947853088 and batch: 1200, loss is 4.915499906539917 and perplexity is 136.38747375341475
At time: 157.729877948761 and batch: 1250, loss is 4.9205726146697994 and perplexity is 137.08108535759442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.011757064039689 and perplexity of 150.16835990440038
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 160.73513627052307 and batch: 50, loss is 4.846524858474732 and perplexity is 127.29724435544347
At time: 161.9238212108612 and batch: 100, loss is 4.830052480697632 and perplexity is 125.21753198576407
At time: 163.1132037639618 and batch: 150, loss is 4.731208324432373 and perplexity is 113.43254288652402
At time: 164.30215883255005 and batch: 200, loss is 4.778044700622559 and perplexity is 118.87169291461605
At time: 165.49231553077698 and batch: 250, loss is 4.763721704483032 and perplexity is 117.18122926758126
At time: 166.68324971199036 and batch: 300, loss is 4.750983963012695 and perplexity is 115.6980711597496
At time: 167.8737998008728 and batch: 350, loss is 4.744225254058838 and perplexity is 114.91873817932279
At time: 169.06692934036255 and batch: 400, loss is 4.729945383071899 and perplexity is 113.28937466205471
At time: 170.26404309272766 and batch: 450, loss is 4.668418483734131 and perplexity is 106.52913158280708
At time: 171.45443868637085 and batch: 500, loss is 4.677609243392944 and perplexity is 107.51272830468609
At time: 172.64595198631287 and batch: 550, loss is 4.672548723220825 and perplexity is 106.97003229454589
At time: 173.83697128295898 and batch: 600, loss is 4.680686197280884 and perplexity is 107.84404948063384
At time: 175.02674508094788 and batch: 650, loss is 4.675958738327027 and perplexity is 107.33542436274622
At time: 176.2160620689392 and batch: 700, loss is 4.666298284530639 and perplexity is 106.30350787100308
At time: 177.40524554252625 and batch: 750, loss is 4.631301536560058 and perplexity is 102.6475767798161
At time: 178.59242725372314 and batch: 800, loss is 4.639141235351563 and perplexity is 103.45546552787691
At time: 179.82734155654907 and batch: 850, loss is 4.671018457412719 and perplexity is 106.80646489435246
At time: 181.01683402061462 and batch: 900, loss is 4.628050727844238 and perplexity is 102.31443093265791
At time: 182.20371842384338 and batch: 950, loss is 4.605919933319091 and perplexity is 100.07500284617849
At time: 183.39063596725464 and batch: 1000, loss is 4.609920701980591 and perplexity is 100.47618175826382
At time: 184.58428168296814 and batch: 1050, loss is 4.575319232940674 and perplexity is 97.05901862776876
At time: 185.7728350162506 and batch: 1100, loss is 4.507518138885498 and perplexity is 90.69644297578228
At time: 186.96249270439148 and batch: 1150, loss is 4.541051530838013 and perplexity is 93.78937070095819
At time: 188.1525297164917 and batch: 1200, loss is 4.5459873008728025 and perplexity is 94.25343778853181
At time: 189.33967232704163 and batch: 1250, loss is 4.572678003311157 and perplexity is 96.80300172051098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.822156920050182 and perplexity of 124.232762138736
Finished 6 epochs...
Completing Train Step...
At time: 192.25170516967773 and batch: 50, loss is 4.6639288139343265 and perplexity is 106.05192301398637
At time: 193.47311115264893 and batch: 100, loss is 4.662375316619873 and perplexity is 105.88729954056139
At time: 194.660986661911 and batch: 150, loss is 4.573611583709717 and perplexity is 96.8934173039802
At time: 195.85148644447327 and batch: 200, loss is 4.629146709442138 and perplexity is 102.426627137417
At time: 197.0435552597046 and batch: 250, loss is 4.622063627243042 and perplexity is 101.70369423706796
At time: 198.23451709747314 and batch: 300, loss is 4.620934600830078 and perplexity is 101.58893287647248
At time: 199.42303156852722 and batch: 350, loss is 4.6209713745117185 and perplexity is 101.59266874423864
At time: 200.61510229110718 and batch: 400, loss is 4.608438405990601 and perplexity is 100.32735664563776
At time: 201.8103485107422 and batch: 450, loss is 4.550915813446045 and perplexity is 94.71911364378339
At time: 203.00094413757324 and batch: 500, loss is 4.562968358993531 and perplexity is 95.86762742945486
At time: 204.19537329673767 and batch: 550, loss is 4.563052940368652 and perplexity is 95.87573638814109
At time: 205.38515400886536 and batch: 600, loss is 4.576803932189941 and perplexity is 97.2032291079652
At time: 206.576824426651 and batch: 650, loss is 4.581757087707519 and perplexity is 97.68588616950956
At time: 207.7696406841278 and batch: 700, loss is 4.569302682876587 and perplexity is 96.47681137882363
At time: 208.9598309993744 and batch: 750, loss is 4.543610773086548 and perplexity is 94.0297078302087
At time: 210.1848828792572 and batch: 800, loss is 4.563748540878296 and perplexity is 95.94245079983453
At time: 211.3760004043579 and batch: 850, loss is 4.599469232559204 and perplexity is 99.43152661690833
At time: 212.56678366661072 and batch: 900, loss is 4.556571540832519 and perplexity is 95.25633689093304
At time: 213.75997805595398 and batch: 950, loss is 4.540812425613403 and perplexity is 93.76694785322756
At time: 214.96186232566833 and batch: 1000, loss is 4.546561126708984 and perplexity is 94.30753836695415
At time: 216.15928053855896 and batch: 1050, loss is 4.514280672073364 and perplexity is 91.31185922248167
At time: 217.3528561592102 and batch: 1100, loss is 4.458428173065186 and perplexity is 86.35167248379689
At time: 218.5430872440338 and batch: 1150, loss is 4.4965473079681395 and perplexity is 89.70686580238115
At time: 219.7342872619629 and batch: 1200, loss is 4.504046792984009 and perplexity is 90.38215007547308
At time: 220.927166223526 and batch: 1250, loss is 4.536239557266235 and perplexity is 93.33914283887351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812452775718522 and perplexity of 123.03302013401047
Finished 7 epochs...
Completing Train Step...
At time: 223.86873650550842 and batch: 50, loss is 4.584101247787475 and perplexity is 97.91514613028367
At time: 225.08522629737854 and batch: 100, loss is 4.5840463542938235 and perplexity is 97.90977137335209
At time: 226.2744743824005 and batch: 150, loss is 4.497464179992676 and perplexity is 89.7891532358024
At time: 227.4652030467987 and batch: 200, loss is 4.5548984146118165 and perplexity is 95.0970942696433
At time: 228.65397310256958 and batch: 250, loss is 4.5492656040191655 and perplexity is 94.56293616775615
At time: 229.84428453445435 and batch: 300, loss is 4.54868447303772 and perplexity is 94.50799868033668
At time: 231.03374791145325 and batch: 350, loss is 4.551759643554687 and perplexity is 94.7990742155643
At time: 232.22070932388306 and batch: 400, loss is 4.54237998008728 and perplexity is 93.91404791539544
At time: 233.41248846054077 and batch: 450, loss is 4.482001466751099 and perplexity is 88.41144828554701
At time: 234.60268664360046 and batch: 500, loss is 4.501415138244629 and perplexity is 90.1446081631148
At time: 235.7927348613739 and batch: 550, loss is 4.501005325317383 and perplexity is 90.10767330607591
At time: 236.98358130455017 and batch: 600, loss is 4.514475049972535 and perplexity is 91.32960995496582
At time: 238.17483258247375 and batch: 650, loss is 4.523240127563477 and perplexity is 92.13363961619008
At time: 239.36646962165833 and batch: 700, loss is 4.508741731643677 and perplexity is 90.8074864087159
At time: 240.59209370613098 and batch: 750, loss is 4.487601652145385 and perplexity is 88.90795776185213
At time: 241.78261637687683 and batch: 800, loss is 4.508936748504639 and perplexity is 90.82519712655535
At time: 242.9716055393219 and batch: 850, loss is 4.549053173065186 and perplexity is 94.54285020653042
At time: 244.16250038146973 and batch: 900, loss is 4.508371601104736 and perplexity is 90.7738820042234
At time: 245.35231566429138 and batch: 950, loss is 4.490005388259887 and perplexity is 89.12192608946434
At time: 246.54213213920593 and batch: 1000, loss is 4.500251388549804 and perplexity is 90.03976342122581
At time: 247.73361945152283 and batch: 1050, loss is 4.471103267669678 and perplexity is 87.45315404651
At time: 248.9279203414917 and batch: 1100, loss is 4.419055309295654 and perplexity is 83.01782213757303
At time: 250.13136911392212 and batch: 1150, loss is 4.4536466312408445 and perplexity is 85.93976391403424
At time: 251.3260691165924 and batch: 1200, loss is 4.463414916992187 and perplexity is 86.78336162916432
At time: 252.51672649383545 and batch: 1250, loss is 4.500007019042969 and perplexity is 90.0177631368518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812861310304516 and perplexity of 123.08329364648964
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 255.4551854133606 and batch: 50, loss is 4.533090944290161 and perplexity is 93.04571618844845
At time: 256.6469542980194 and batch: 100, loss is 4.535112714767456 and perplexity is 93.23402356347965
At time: 257.8392004966736 and batch: 150, loss is 4.453475637435913 and perplexity is 85.92507000312735
At time: 259.03216457366943 and batch: 200, loss is 4.501943197250366 and perplexity is 90.19222240573238
At time: 260.2247393131256 and batch: 250, loss is 4.48994963645935 and perplexity is 89.11695752012214
At time: 261.4189887046814 and batch: 300, loss is 4.485002183914185 and perplexity is 88.67714447602283
At time: 262.6149287223816 and batch: 350, loss is 4.482344026565552 and perplexity is 88.44173968287876
At time: 263.80886816978455 and batch: 400, loss is 4.465747556686401 and perplexity is 86.98603223014445
At time: 265.00274658203125 and batch: 450, loss is 4.403404674530029 and perplexity is 81.72865498062023
At time: 266.1950304508209 and batch: 500, loss is 4.4133635520935055 and perplexity is 82.54664702969066
At time: 267.387934923172 and batch: 550, loss is 4.409475984573365 and perplexity is 82.2263643294869
At time: 268.57899475097656 and batch: 600, loss is 4.425979681015015 and perplexity is 83.59466322362566
At time: 269.77104020118713 and batch: 650, loss is 4.42728289604187 and perplexity is 83.70367606306014
At time: 270.99390482902527 and batch: 700, loss is 4.407937889099121 and perplexity is 82.0999895438053
At time: 272.1915690898895 and batch: 750, loss is 4.386239643096924 and perplexity is 80.33775164954052
At time: 273.39700961112976 and batch: 800, loss is 4.397375774383545 and perplexity is 81.23740342191795
At time: 274.5887539386749 and batch: 850, loss is 4.428437767028808 and perplexity is 83.80039885046577
At time: 275.80182337760925 and batch: 900, loss is 4.3859022521972655 and perplexity is 80.31065099524909
At time: 277.01887702941895 and batch: 950, loss is 4.361506938934326 and perplexity is 78.37515204617728
At time: 278.23622965812683 and batch: 1000, loss is 4.365724077224732 and perplexity is 78.7063688034522
At time: 279.43373012542725 and batch: 1050, loss is 4.333595561981201 and perplexity is 76.21784045392866
At time: 280.63186502456665 and batch: 1100, loss is 4.273301124572754 and perplexity is 71.75812757799085
At time: 281.82332134246826 and batch: 1150, loss is 4.301086368560791 and perplexity is 73.7799023442857
At time: 283.0146448612213 and batch: 1200, loss is 4.309977540969848 and perplexity is 74.43881709793389
At time: 284.2072494029999 and batch: 1250, loss is 4.364424295425415 and perplexity is 78.60413415355299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.760759562471487 and perplexity of 116.83463540894418
Finished 9 epochs...
Completing Train Step...
At time: 287.1337368488312 and batch: 50, loss is 4.466957988739014 and perplexity is 87.09138666101212
At time: 288.3517732620239 and batch: 100, loss is 4.467679777145386 and perplexity is 87.15427090602826
At time: 289.5422031879425 and batch: 150, loss is 4.3917857837677 and perplexity is 80.78455399038924
At time: 290.73100113868713 and batch: 200, loss is 4.4429825496673585 and perplexity is 85.02816458515842
At time: 291.92118763923645 and batch: 250, loss is 4.433527135848999 and perplexity is 84.22797711632644
At time: 293.11103439331055 and batch: 300, loss is 4.433293294906616 and perplexity is 84.20828346946276
At time: 294.2995398044586 and batch: 350, loss is 4.432433042526245 and perplexity is 84.13587424273145
At time: 295.48924565315247 and batch: 400, loss is 4.420721759796143 and perplexity is 83.15628256550568
At time: 296.6811566352844 and batch: 450, loss is 4.359226932525635 and perplexity is 78.1966597563355
At time: 297.87291955947876 and batch: 500, loss is 4.372287912368774 and perplexity is 79.2246836387112
At time: 299.0628888607025 and batch: 550, loss is 4.369527463912964 and perplexity is 79.00628955458639
At time: 300.2981195449829 and batch: 600, loss is 4.389056310653687 and perplexity is 80.56435537205623
At time: 301.4875388145447 and batch: 650, loss is 4.393812646865845 and perplexity is 80.94845927244373
At time: 302.6798207759857 and batch: 700, loss is 4.376580286026001 and perplexity is 79.56547646559663
At time: 303.8697690963745 and batch: 750, loss is 4.358010854721069 and perplexity is 78.10162433096225
At time: 305.0600280761719 and batch: 800, loss is 4.375361108779908 and perplexity is 79.46853115588131
At time: 306.25015664100647 and batch: 850, loss is 4.407341604232788 and perplexity is 82.05104915517086
At time: 307.43936800956726 and batch: 900, loss is 4.365068988800049 and perplexity is 78.6548260566719
At time: 308.6308424472809 and batch: 950, loss is 4.3450353145599365 and perplexity is 77.09475999875232
At time: 309.82082438468933 and batch: 1000, loss is 4.350298223495483 and perplexity is 77.50157226900245
At time: 311.01087975502014 and batch: 1050, loss is 4.324245901107788 and perplexity is 75.50855047052632
At time: 312.2013809680939 and batch: 1100, loss is 4.267467489242554 and perplexity is 71.34073547009551
At time: 313.39212250709534 and batch: 1150, loss is 4.298857870101929 and perplexity is 73.61566701267701
At time: 314.5834605693817 and batch: 1200, loss is 4.3099436378479 and perplexity is 74.43629343242046
At time: 315.7741641998291 and batch: 1250, loss is 4.361912841796875 and perplexity is 78.40697120205189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.757682410469891 and perplexity of 116.4756700554672
Finished 10 epochs...
Completing Train Step...
At time: 318.71897864341736 and batch: 50, loss is 4.4354492282867435 and perplexity is 84.39002676149676
At time: 319.91127943992615 and batch: 100, loss is 4.435323810577392 and perplexity is 84.37944342133126
At time: 321.1094899177551 and batch: 150, loss is 4.360997982025147 and perplexity is 78.33527262035089
At time: 322.30117201805115 and batch: 200, loss is 4.412835922241211 and perplexity is 82.50310444270549
At time: 323.5017294883728 and batch: 250, loss is 4.405688734054565 and perplexity is 81.91554144210966
At time: 324.69616627693176 and batch: 300, loss is 4.405843982696533 and perplexity is 81.92825970589566
At time: 325.8905806541443 and batch: 350, loss is 4.406232109069824 and perplexity is 81.96006439592561
At time: 327.0812966823578 and batch: 400, loss is 4.394944715499878 and perplexity is 81.04015037467732
At time: 328.276428937912 and batch: 450, loss is 4.335021486282349 and perplexity is 76.32659884700291
At time: 329.46690034866333 and batch: 500, loss is 4.348523225784302 and perplexity is 77.36412917228893
At time: 330.7014317512512 and batch: 550, loss is 4.34598666191101 and perplexity is 77.16813879328535
At time: 331.8915476799011 and batch: 600, loss is 4.367820339202881 and perplexity is 78.87153102297235
At time: 333.0839660167694 and batch: 650, loss is 4.374111061096191 and perplexity is 79.36925376624478
At time: 334.27478671073914 and batch: 700, loss is 4.357949066162109 and perplexity is 78.09679869322855
At time: 335.47113847732544 and batch: 750, loss is 4.340695152282715 and perplexity is 76.76088129761236
At time: 336.6627519130707 and batch: 800, loss is 4.358849439620972 and perplexity is 78.16714664295505
At time: 337.85664772987366 and batch: 850, loss is 4.392170743942261 and perplexity is 80.81565881307004
At time: 339.0472135543823 and batch: 900, loss is 4.351904363632202 and perplexity is 77.62615067328906
At time: 340.23883056640625 and batch: 950, loss is 4.332226781845093 and perplexity is 76.11358635464556
At time: 341.4294157028198 and batch: 1000, loss is 4.339191093444824 and perplexity is 76.64551519614302
At time: 342.6188404560089 and batch: 1050, loss is 4.314932584762573 and perplexity is 74.80858003575891
At time: 343.8117311000824 and batch: 1100, loss is 4.258421001434326 and perplexity is 70.69826281854081
At time: 345.0019762516022 and batch: 1150, loss is 4.292332906723022 and perplexity is 73.13689117736297
At time: 346.19202756881714 and batch: 1200, loss is 4.30207968711853 and perplexity is 73.85322570116908
At time: 347.38644433021545 and batch: 1250, loss is 4.3547911453247075 and perplexity is 77.85056418447424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.757113046019617 and perplexity of 116.40937182533449
Finished 11 epochs...
Completing Train Step...
At time: 350.31219244003296 and batch: 50, loss is 4.412045278549194 and perplexity is 82.43789966387635
At time: 351.5299575328827 and batch: 100, loss is 4.410923843383789 and perplexity is 82.34550272250407
At time: 352.71942615509033 and batch: 150, loss is 4.33892900466919 and perplexity is 76.62542989908785
At time: 353.9092607498169 and batch: 200, loss is 4.390527925491333 and perplexity is 80.6830023527234
At time: 355.1015009880066 and batch: 250, loss is 4.385695018768311 and perplexity is 80.29400966804094
At time: 356.2920434474945 and batch: 300, loss is 4.385225973129272 and perplexity is 80.25635694407856
At time: 357.48568987846375 and batch: 350, loss is 4.384710578918457 and perplexity is 80.21500393979336
At time: 358.6775755882263 and batch: 400, loss is 4.376983861923218 and perplexity is 79.59759365457452
At time: 359.8679051399231 and batch: 450, loss is 4.3167939949035645 and perplexity is 74.94795916588981
At time: 361.08661699295044 and batch: 500, loss is 4.3313242530822755 and perplexity is 76.04492264386715
At time: 362.279248714447 and batch: 550, loss is 4.328606128692627 and perplexity is 75.83850374812609
At time: 363.4721658229828 and batch: 600, loss is 4.350915126800537 and perplexity is 77.54939799548967
At time: 364.66898679733276 and batch: 650, loss is 4.359193439483643 and perplexity is 78.19404075618601
At time: 365.8745744228363 and batch: 700, loss is 4.344541187286377 and perplexity is 77.05667478543465
At time: 367.06729793548584 and batch: 750, loss is 4.325447444915771 and perplexity is 75.59933182976222
At time: 368.25875997543335 and batch: 800, loss is 4.346058621406555 and perplexity is 77.17369197342465
At time: 369.4523355960846 and batch: 850, loss is 4.379128465652466 and perplexity is 79.76848212929771
At time: 370.6496067047119 and batch: 900, loss is 4.338259201049805 and perplexity is 76.57412309346476
At time: 371.8397159576416 and batch: 950, loss is 4.319508867263794 and perplexity is 75.15170976196988
At time: 373.0327043533325 and batch: 1000, loss is 4.327678918838501 and perplexity is 75.76821812992304
At time: 374.2231888771057 and batch: 1050, loss is 4.304973793029785 and perplexity is 74.06727434856072
At time: 375.4148819446564 and batch: 1100, loss is 4.24884765625 and perplexity is 70.02467334140744
At time: 376.60730934143066 and batch: 1150, loss is 4.284281482696533 and perplexity is 72.55039927032638
At time: 377.7986924648285 and batch: 1200, loss is 4.293697938919068 and perplexity is 73.23679355800932
At time: 378.98977494239807 and batch: 1250, loss is 4.345498723983765 and perplexity is 77.13049471632645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7574217858975825 and perplexity of 116.44531758924573
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 381.9348087310791 and batch: 50, loss is 4.399002122879028 and perplexity is 81.36963124586607
At time: 383.1545617580414 and batch: 100, loss is 4.400893096923828 and perplexity is 81.52364467839547
At time: 384.3460478782654 and batch: 150, loss is 4.3296755027771 and perplexity is 75.91964685707578
At time: 385.5389132499695 and batch: 200, loss is 4.380143737792968 and perplexity is 79.84950997258959
At time: 386.7293529510498 and batch: 250, loss is 4.374770288467407 and perplexity is 79.4215934007248
At time: 387.92066740989685 and batch: 300, loss is 4.372354497909546 and perplexity is 79.22995903274429
At time: 389.11424493789673 and batch: 350, loss is 4.369041795730591 and perplexity is 78.96792802978244
At time: 390.3417167663574 and batch: 400, loss is 4.357215003967285 and perplexity is 78.0394918217483
At time: 391.5336284637451 and batch: 450, loss is 4.295127296447754 and perplexity is 73.3415499696058
At time: 392.7256691455841 and batch: 500, loss is 4.305575370788574 and perplexity is 74.1118449784686
At time: 393.921284198761 and batch: 550, loss is 4.302226371765137 and perplexity is 73.86405963004793
At time: 395.1117832660675 and batch: 600, loss is 4.324164113998413 and perplexity is 75.50237509698667
At time: 396.3023347854614 and batch: 650, loss is 4.331035623550415 and perplexity is 76.02297700067679
At time: 397.49338269233704 and batch: 700, loss is 4.313949632644653 and perplexity is 74.73508291156881
At time: 398.6847538948059 and batch: 750, loss is 4.291787109375 and perplexity is 73.09698414768194
At time: 399.87725138664246 and batch: 800, loss is 4.3086941003799435 and perplexity is 74.34334058093044
At time: 401.0703091621399 and batch: 850, loss is 4.334948139190674 and perplexity is 76.32100071826578
At time: 402.2620544433594 and batch: 900, loss is 4.293494958877563 and perplexity is 73.22192945922198
At time: 403.453400850296 and batch: 950, loss is 4.27251049041748 and perplexity is 71.7014155735873
At time: 404.64658761024475 and batch: 1000, loss is 4.279028673171997 and perplexity is 72.17030499740207
At time: 405.8407163619995 and batch: 1050, loss is 4.254410638809204 and perplexity is 70.41530490881952
At time: 407.03170680999756 and batch: 1100, loss is 4.192456378936767 and perplexity is 66.18516730073728
At time: 408.22346782684326 and batch: 1150, loss is 4.224217138290405 and perplexity is 68.32099671477583
At time: 409.4161410331726 and batch: 1200, loss is 4.235190925598144 and perplexity is 69.07486563465764
At time: 410.60692024230957 and batch: 1250, loss is 4.292944536209107 and perplexity is 73.18163753922984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.742809880388914 and perplexity of 114.75620027619122
Finished 13 epochs...
Completing Train Step...
At time: 413.55399441719055 and batch: 50, loss is 4.379267387390136 and perplexity is 79.77956447521802
At time: 414.74636912345886 and batch: 100, loss is 4.380175724029541 and perplexity is 79.85206409875397
At time: 415.9397165775299 and batch: 150, loss is 4.309605712890625 and perplexity is 74.41114380072602
At time: 417.13223600387573 and batch: 200, loss is 4.359543218612671 and perplexity is 78.22139618355676
At time: 418.3268938064575 and batch: 250, loss is 4.356501922607422 and perplexity is 77.9838631510383
At time: 419.5199222564697 and batch: 300, loss is 4.355291147232055 and perplexity is 77.88949934807145
At time: 420.73758363723755 and batch: 350, loss is 4.352984189987183 and perplexity is 77.71001870993058
At time: 421.92943811416626 and batch: 400, loss is 4.342622880935669 and perplexity is 76.90899816661342
At time: 423.12305665016174 and batch: 450, loss is 4.281665716171265 and perplexity is 72.36087235167054
At time: 424.31808638572693 and batch: 500, loss is 4.293000679016114 and perplexity is 73.1857462771196
At time: 425.5113353729248 and batch: 550, loss is 4.289638504981995 and perplexity is 72.94009625179406
At time: 426.70467162132263 and batch: 600, loss is 4.312261037826538 and perplexity is 74.60899212591733
At time: 427.89848136901855 and batch: 650, loss is 4.320907430648804 and perplexity is 75.25688772344294
At time: 429.0914008617401 and batch: 700, loss is 4.305063381195068 and perplexity is 74.0739101970194
At time: 430.28398633003235 and batch: 750, loss is 4.284711847305298 and perplexity is 72.58162911415178
At time: 431.47654604911804 and batch: 800, loss is 4.303020787239075 and perplexity is 73.92276169581339
At time: 432.67290472984314 and batch: 850, loss is 4.330036392211914 and perplexity is 75.94705040004848
At time: 433.86769461631775 and batch: 900, loss is 4.2897560405731205 and perplexity is 72.94866981296319
At time: 435.072879076004 and batch: 950, loss is 4.270561552047729 and perplexity is 71.56181001911068
At time: 436.2763614654541 and batch: 1000, loss is 4.278016729354858 and perplexity is 72.0973096433067
At time: 437.49334478378296 and batch: 1050, loss is 4.255218534469605 and perplexity is 70.4722161141436
At time: 438.7124648094177 and batch: 1100, loss is 4.1940361642837525 and perplexity is 66.28980829161588
At time: 439.90657019615173 and batch: 1150, loss is 4.226963396072388 and perplexity is 68.50888165583272
At time: 441.1010329723358 and batch: 1200, loss is 4.239047622680664 and perplexity is 69.34178084213342
At time: 442.2962486743927 and batch: 1250, loss is 4.294872889518738 and perplexity is 73.32289374434582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7421710160526915 and perplexity of 114.68291004622418
Finished 14 epochs...
Completing Train Step...
At time: 445.21995186805725 and batch: 50, loss is 4.369580001831054 and perplexity is 79.01044048959544
At time: 446.4957242012024 and batch: 100, loss is 4.369890975952148 and perplexity is 79.03501451262849
At time: 447.707190990448 and batch: 150, loss is 4.2995596027374265 and perplexity is 73.66734365817744
At time: 448.9213135242462 and batch: 200, loss is 4.348925790786743 and perplexity is 77.39527953273983
At time: 450.13905596733093 and batch: 250, loss is 4.346750316619873 and perplexity is 77.22709111259218
At time: 451.5186231136322 and batch: 300, loss is 4.346078538894654 and perplexity is 77.17522909482382
At time: 452.7394423484802 and batch: 350, loss is 4.344441614151001 and perplexity is 77.04900239271304
At time: 453.95991945266724 and batch: 400, loss is 4.334509992599488 and perplexity is 76.28756825665954
At time: 455.1746709346771 and batch: 450, loss is 4.273851156234741 and perplexity is 71.79760767682052
At time: 456.3862280845642 and batch: 500, loss is 4.285613718032837 and perplexity is 72.64711788757197
At time: 457.59186697006226 and batch: 550, loss is 4.282168035507202 and perplexity is 72.39722974776511
At time: 458.79684233665466 and batch: 600, loss is 4.305277156829834 and perplexity is 74.08974708690164
At time: 459.99095249176025 and batch: 650, loss is 4.3148922348022465 and perplexity is 74.80556157342019
At time: 461.181293964386 and batch: 700, loss is 4.299576826095581 and perplexity is 73.66861246814814
At time: 462.3720393180847 and batch: 750, loss is 4.2801265621185305 and perplexity is 72.24958348905326
At time: 463.56234669685364 and batch: 800, loss is 4.2990121126174925 and perplexity is 73.62702255407304
At time: 464.7548756599426 and batch: 850, loss is 4.326192941665649 and perplexity is 75.65571189890223
At time: 465.9448347091675 and batch: 900, loss is 4.2865903854370115 and perplexity is 72.71810461919067
At time: 467.1338806152344 and batch: 950, loss is 4.268132152557373 and perplexity is 71.38816880166617
At time: 468.3239998817444 and batch: 1000, loss is 4.276025404930115 and perplexity is 71.95388336117996
At time: 469.51391530036926 and batch: 1050, loss is 4.254304151535035 and perplexity is 70.40780697416439
At time: 470.7070300579071 and batch: 1100, loss is 4.193569850921631 and perplexity is 66.25890367441502
At time: 471.90109419822693 and batch: 1150, loss is 4.226791553497314 and perplexity is 68.49710992466484
At time: 473.0912334918976 and batch: 1200, loss is 4.239431705474853 and perplexity is 69.36841894236386
At time: 474.2824604511261 and batch: 1250, loss is 4.294165740013122 and perplexity is 73.27106182490226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.741901035726506 and perplexity of 114.65195209596875
Finished 15 epochs...
Completing Train Step...
At time: 477.3014807701111 and batch: 50, loss is 4.361628694534302 and perplexity is 78.38469524079474
At time: 478.49590253829956 and batch: 100, loss is 4.361316118240357 and perplexity is 78.36019787210216
At time: 479.6864137649536 and batch: 150, loss is 4.291413736343384 and perplexity is 73.06969679960568
At time: 480.87653064727783 and batch: 200, loss is 4.341076402664185 and perplexity is 76.79015199226627
At time: 482.14744448661804 and batch: 250, loss is 4.339401741027832 and perplexity is 76.66166208925976
At time: 483.3709788322449 and batch: 300, loss is 4.338870716094971 and perplexity is 76.62096364219727
At time: 484.5886423587799 and batch: 350, loss is 4.337456178665161 and perplexity is 76.51265704114721
At time: 485.80661964416504 and batch: 400, loss is 4.3277874946594235 and perplexity is 75.77644517302723
At time: 487.02165031433105 and batch: 450, loss is 4.267380456924439 and perplexity is 71.33452679069332
At time: 488.24122977256775 and batch: 500, loss is 4.279281539916992 and perplexity is 72.18855677504864
At time: 489.4620318412781 and batch: 550, loss is 4.275867853164673 and perplexity is 71.94254779281886
At time: 490.6817293167114 and batch: 600, loss is 4.299597854614258 and perplexity is 73.67016162622949
At time: 491.9075503349304 and batch: 650, loss is 4.309557142257691 and perplexity is 74.40752969214473
At time: 493.125456571579 and batch: 700, loss is 4.294695177078247 and perplexity is 73.30986451171734
At time: 494.33964467048645 and batch: 750, loss is 4.276033840179443 and perplexity is 71.95449031268615
At time: 495.55606031417847 and batch: 800, loss is 4.295179271697998 and perplexity is 73.34536201408389
At time: 496.7767062187195 and batch: 850, loss is 4.32235595703125 and perplexity is 75.36597830198693
At time: 497.9977693557739 and batch: 900, loss is 4.283336210250854 and perplexity is 72.48185178010415
At time: 499.2195599079132 and batch: 950, loss is 4.265390424728394 and perplexity is 71.19270994253932
At time: 500.4385917186737 and batch: 1000, loss is 4.273320050239563 and perplexity is 71.7594856612555
At time: 501.65604305267334 and batch: 1050, loss is 4.252402772903443 and perplexity is 70.27406226444631
At time: 502.87768745422363 and batch: 1100, loss is 4.192030739784241 and perplexity is 66.15700229670702
At time: 504.0930676460266 and batch: 1150, loss is 4.225437898635864 and perplexity is 68.40445120693073
At time: 505.2919862270355 and batch: 1200, loss is 4.238397531509399 and perplexity is 69.2967170119943
At time: 506.50036120414734 and batch: 1250, loss is 4.292424106597901 and perplexity is 73.14356155685162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.741743324446852 and perplexity of 114.63387161567424
Finished 16 epochs...
Completing Train Step...
At time: 509.6628589630127 and batch: 50, loss is 4.354491481781006 and perplexity is 77.82723870360421
At time: 510.928103685379 and batch: 100, loss is 4.354068279266357 and perplexity is 77.79430898893722
At time: 512.1382973194122 and batch: 150, loss is 4.2844670963287355 and perplexity is 72.56386686329841
At time: 513.3860628604889 and batch: 200, loss is 4.33455530166626 and perplexity is 76.29102485349057
At time: 514.5873420238495 and batch: 250, loss is 4.333256101608276 and perplexity is 76.19197190830612
At time: 515.7887244224548 and batch: 300, loss is 4.332584123611451 and perplexity is 76.14078977820759
At time: 516.9848790168762 and batch: 350, loss is 4.331501550674439 and perplexity is 76.05840642083342
At time: 518.1850056648254 and batch: 400, loss is 4.3219209480285645 and perplexity is 75.33320055275428
At time: 519.3842175006866 and batch: 450, loss is 4.26171422958374 and perplexity is 70.93147212272243
At time: 520.5883092880249 and batch: 500, loss is 4.27379467010498 and perplexity is 71.79355222237628
At time: 521.7946093082428 and batch: 550, loss is 4.270499749183655 and perplexity is 71.55738743095887
At time: 523.0061705112457 and batch: 600, loss is 4.294452152252197 and perplexity is 73.29205055935066
At time: 524.2166883945465 and batch: 650, loss is 4.304970216751099 and perplexity is 74.06700946381974
At time: 525.4260458946228 and batch: 700, loss is 4.290460615158081 and perplexity is 73.00008570275769
At time: 526.632374048233 and batch: 750, loss is 4.272241382598877 and perplexity is 71.68212275809171
At time: 527.8393666744232 and batch: 800, loss is 4.291716318130494 and perplexity is 73.09180970435926
At time: 529.0428946018219 and batch: 850, loss is 4.318550319671631 and perplexity is 75.07970778570412
At time: 530.2401714324951 and batch: 900, loss is 4.2800804615020756 and perplexity is 72.24625281548946
At time: 531.4379186630249 and batch: 950, loss is 4.262437925338745 and perplexity is 70.98282350714217
At time: 532.6313133239746 and batch: 1000, loss is 4.270523371696473 and perplexity is 71.55907781622615
At time: 533.823492527008 and batch: 1050, loss is 4.2502264356613155 and perplexity is 70.12128850949125
At time: 535.0175943374634 and batch: 1100, loss is 4.190034112930298 and perplexity is 66.02504322969156
At time: 536.2118244171143 and batch: 1150, loss is 4.223690910339355 and perplexity is 68.2850537545855
At time: 537.4053905010223 and batch: 1200, loss is 4.236894903182983 and perplexity is 69.19266799516208
At time: 538.5966646671295 and batch: 1250, loss is 4.290287880897522 and perplexity is 72.9874771759267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.741598533017792 and perplexity of 114.61727481515058
Finished 17 epochs...
Completing Train Step...
At time: 541.6081681251526 and batch: 50, loss is 4.347755470275879 and perplexity is 77.30475523121423
At time: 542.8405215740204 and batch: 100, loss is 4.347298192977905 and perplexity is 77.2694136026996
At time: 544.0333924293518 and batch: 150, loss is 4.278192524909973 and perplexity is 72.10998514399343
At time: 545.2476098537445 and batch: 200, loss is 4.328465509414673 and perplexity is 75.82784014225973
At time: 546.4703199863434 and batch: 250, loss is 4.327642621994019 and perplexity is 75.76546803260312
At time: 547.6874887943268 and batch: 300, loss is 4.3268155670166015 and perplexity is 75.70283173055346
At time: 548.9016304016113 and batch: 350, loss is 4.326047458648682 and perplexity is 75.64470607828375
At time: 550.1157178878784 and batch: 400, loss is 4.316643505096436 and perplexity is 74.93668111060771
At time: 551.3311457633972 and batch: 450, loss is 4.256369771957398 and perplexity is 70.55339308920483
At time: 552.5320181846619 and batch: 500, loss is 4.268689813613892 and perplexity is 71.42799030572205
At time: 553.7275285720825 and batch: 550, loss is 4.265561990737915 and perplexity is 71.20492523952606
At time: 554.9226043224335 and batch: 600, loss is 4.289863920211792 and perplexity is 72.95653991360832
At time: 556.1204690933228 and batch: 650, loss is 4.300646238327026 and perplexity is 73.7474367236944
At time: 557.3136696815491 and batch: 700, loss is 4.2864895439147945 and perplexity is 72.71077198455137
At time: 558.5079801082611 and batch: 750, loss is 4.2685467147827145 and perplexity is 71.41776977508638
At time: 559.7000801563263 and batch: 800, loss is 4.288362336158753 and perplexity is 72.84707174510586
At time: 560.8947257995605 and batch: 850, loss is 4.315054941177368 and perplexity is 74.81773390541389
At time: 562.0862550735474 and batch: 900, loss is 4.2767489528656 and perplexity is 72.00596428418613
At time: 563.2838747501373 and batch: 950, loss is 4.2593937015533445 and perplexity is 70.76706448346795
At time: 564.4756050109863 and batch: 1000, loss is 4.26758071899414 and perplexity is 71.348813821197
At time: 565.6693079471588 and batch: 1050, loss is 4.247755446434021 and perplexity is 69.94823345761786
At time: 566.8613138198853 and batch: 1100, loss is 4.187691240310669 and perplexity is 65.87053602971793
At time: 568.0559504032135 and batch: 1150, loss is 4.221865873336792 and perplexity is 68.16054465617643
At time: 569.2505910396576 and batch: 1200, loss is 4.235083475112915 and perplexity is 69.06744390556938
At time: 570.443874835968 and batch: 1250, loss is 4.288119611740112 and perplexity is 72.8293921276911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.741834654425182 and perplexity of 114.64434160278938
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 573.5177183151245 and batch: 50, loss is 4.344303579330444 and perplexity is 77.0383676814908
At time: 574.7083094120026 and batch: 100, loss is 4.345866556167603 and perplexity is 77.15887101317615
At time: 575.8993330001831 and batch: 150, loss is 4.276638970375061 and perplexity is 71.99804532438195
At time: 577.0893409252167 and batch: 200, loss is 4.327473678588867 and perplexity is 75.75266903762412
At time: 578.2826464176178 and batch: 250, loss is 4.32604489326477 and perplexity is 75.6445120208207
At time: 579.4780809879303 and batch: 300, loss is 4.324331073760987 and perplexity is 75.51498200800074
At time: 580.6695408821106 and batch: 350, loss is 4.321556072235108 and perplexity is 75.30571830553724
At time: 581.8602735996246 and batch: 400, loss is 4.31128475189209 and perplexity is 74.53618796094523
At time: 583.0510196685791 and batch: 450, loss is 4.249483308792114 and perplexity is 70.0691988529057
At time: 584.2426571846008 and batch: 500, loss is 4.260917615890503 and perplexity is 70.8749896410941
At time: 585.4324066638947 and batch: 550, loss is 4.257349743843078 and perplexity is 70.62256731973626
At time: 586.6232399940491 and batch: 600, loss is 4.280433053970337 and perplexity is 72.27173079149958
At time: 587.8135447502136 and batch: 650, loss is 4.290987129211426 and perplexity is 73.03853139398576
At time: 589.0042417049408 and batch: 700, loss is 4.276250648498535 and perplexity is 71.97009233604506
At time: 590.1988232135773 and batch: 750, loss is 4.25630672454834 and perplexity is 70.54894502079138
At time: 591.3901927471161 and batch: 800, loss is 4.275643973350525 and perplexity is 71.92644311141349
At time: 592.5816333293915 and batch: 850, loss is 4.29848648071289 and perplexity is 73.58833201136041
At time: 593.7735812664032 and batch: 900, loss is 4.259349708557129 and perplexity is 70.76395129674763
At time: 594.9651341438293 and batch: 950, loss is 4.24124228477478 and perplexity is 69.49412973610798
At time: 596.1574597358704 and batch: 1000, loss is 4.248547101020813 and perplexity is 70.00363022213078
At time: 597.348085641861 and batch: 1050, loss is 4.227450222969055 and perplexity is 68.54224174171806
At time: 598.5380983352661 and batch: 1100, loss is 4.166537718772888 and perplexity is 64.49177644992305
At time: 599.7302780151367 and batch: 1150, loss is 4.198522901535034 and perplexity is 66.58790147668579
At time: 600.9261310100555 and batch: 1200, loss is 4.21279616355896 and perplexity is 67.54514327237193
At time: 602.1203663349152 and batch: 1250, loss is 4.2687114715576175 and perplexity is 71.42953730586888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.738251846202099 and perplexity of 114.2343278518796
Finished 19 epochs...
Completing Train Step...
At time: 605.0511407852173 and batch: 50, loss is 4.338653516769409 and perplexity is 76.6043234277574
At time: 606.2764945030212 and batch: 100, loss is 4.339865865707398 and perplexity is 76.6972509167997
At time: 607.4692833423615 and batch: 150, loss is 4.270807557106018 and perplexity is 71.57941675193955
At time: 608.6613481044769 and batch: 200, loss is 4.321511888504029 and perplexity is 75.30239109143581
At time: 609.8541195392609 and batch: 250, loss is 4.320475454330444 and perplexity is 75.22438555074287
At time: 611.0462472438812 and batch: 300, loss is 4.3190598297119145 and perplexity is 75.11797139766198
At time: 612.2393386363983 and batch: 350, loss is 4.316881847381592 and perplexity is 74.95454381906045
At time: 613.4359455108643 and batch: 400, loss is 4.306856470108032 and perplexity is 74.20685045536649
At time: 614.626720905304 and batch: 450, loss is 4.245498104095459 and perplexity is 69.7905144286199
At time: 615.8187303543091 and batch: 500, loss is 4.257380132675171 and perplexity is 70.62471348968612
At time: 617.0075514316559 and batch: 550, loss is 4.253855504989624 and perplexity is 70.37622583970867
At time: 618.1981546878815 and batch: 600, loss is 4.277238359451294 and perplexity is 72.04121310211431
At time: 619.3968439102173 and batch: 650, loss is 4.288249435424805 and perplexity is 72.8388477214978
At time: 620.6116044521332 and batch: 700, loss is 4.273863954544067 and perplexity is 71.79852657069256
At time: 621.8025460243225 and batch: 750, loss is 4.254523167610168 and perplexity is 70.42322910449222
At time: 622.9988842010498 and batch: 800, loss is 4.274156560897827 and perplexity is 71.81953834969786
At time: 624.1968622207642 and batch: 850, loss is 4.297572526931763 and perplexity is 73.52110640230107
At time: 625.3913996219635 and batch: 900, loss is 4.258804702758789 and perplexity is 70.72539504062345
At time: 626.5860013961792 and batch: 950, loss is 4.241013774871826 and perplexity is 69.47825145350758
At time: 627.7818956375122 and batch: 1000, loss is 4.248813185691834 and perplexity is 70.02225959343382
At time: 628.9750702381134 and batch: 1050, loss is 4.228440546989441 and perplexity is 68.6101543923402
At time: 630.1658849716187 and batch: 1100, loss is 4.167597570419312 and perplexity is 64.56016439951316
At time: 631.3578195571899 and batch: 1150, loss is 4.19982008934021 and perplexity is 66.6743345382903
At time: 632.5508301258087 and batch: 1200, loss is 4.2145481872558594 and perplexity is 67.66358769241296
At time: 633.7906107902527 and batch: 1250, loss is 4.269681987762451 and perplexity is 71.49889448001471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7381756636348085 and perplexity of 114.22562551899841
Finished 20 epochs...
Completing Train Step...
At time: 636.7846944332123 and batch: 50, loss is 4.3355389499664305 and perplexity is 76.36610531073428
At time: 637.9853303432465 and batch: 100, loss is 4.336548385620117 and perplexity is 76.44323090027906
At time: 639.1861770153046 and batch: 150, loss is 4.2675390625 and perplexity is 71.34584174165576
At time: 640.3809702396393 and batch: 200, loss is 4.318166313171386 and perplexity is 75.05088222483356
At time: 641.5792918205261 and batch: 250, loss is 4.317383079528809 and perplexity is 74.99212286312824
At time: 642.7730758190155 and batch: 300, loss is 4.316039323806763 and perplexity is 74.89141944447323
At time: 643.9666512012482 and batch: 350, loss is 4.313905544281006 and perplexity is 74.73178803668958
At time: 645.1609814167023 and batch: 400, loss is 4.304098281860352 and perplexity is 74.00245600131294
At time: 646.3569643497467 and batch: 450, loss is 4.242928047180175 and perplexity is 69.61137912695376
At time: 647.5512611865997 and batch: 500, loss is 4.2550789260864255 and perplexity is 70.46237828873028
At time: 648.7453498840332 and batch: 550, loss is 4.251687874794007 and perplexity is 70.223841423722
At time: 649.9402258396149 and batch: 600, loss is 4.275187587738037 and perplexity is 71.89362440718183
At time: 651.1369922161102 and batch: 650, loss is 4.286442852020263 and perplexity is 72.70737706011293
At time: 652.3302788734436 and batch: 700, loss is 4.272277956008911 and perplexity is 71.68474446570153
At time: 653.5283563137054 and batch: 750, loss is 4.25326141834259 and perplexity is 70.33442868046507
At time: 654.7207822799683 and batch: 800, loss is 4.273046646118164 and perplexity is 71.73986900388594
At time: 655.9144451618195 and batch: 850, loss is 4.296756048202514 and perplexity is 73.46110248206844
At time: 657.1110906600952 and batch: 900, loss is 4.258255200386047 and perplexity is 70.68654194414752
At time: 658.3066477775574 and batch: 950, loss is 4.240644884109497 and perplexity is 69.45262629509632
At time: 659.5003802776337 and batch: 1000, loss is 4.248687448501587 and perplexity is 70.01345574475503
At time: 660.6945295333862 and batch: 1050, loss is 4.228730788230896 and perplexity is 68.63007077886601
At time: 661.8867719173431 and batch: 1100, loss is 4.1679111671447755 and perplexity is 64.58041343051322
At time: 663.0815024375916 and batch: 1150, loss is 4.200354928970337 and perplexity is 66.71000415262512
At time: 664.3296842575073 and batch: 1200, loss is 4.2152700614929195 and perplexity is 67.71244992722755
At time: 665.5273962020874 and batch: 1250, loss is 4.269795694351196 and perplexity is 71.50702483763396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.738252291714188 and perplexity of 114.23437874466502
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 668.4694979190826 and batch: 50, loss is 4.3339144897460935 and perplexity is 76.24215231608743
At time: 669.6867008209229 and batch: 100, loss is 4.335637102127075 and perplexity is 76.37360117683224
At time: 670.8769855499268 and batch: 150, loss is 4.2671635675430295 and perplexity is 71.31905676700713
At time: 672.0668466091156 and batch: 200, loss is 4.317347631454468 and perplexity is 74.98946458389774
At time: 673.2574038505554 and batch: 250, loss is 4.316771936416626 and perplexity is 74.94630594554533
At time: 674.4486167430878 and batch: 300, loss is 4.3154168128967285 and perplexity is 74.84481322674567
At time: 675.639014005661 and batch: 350, loss is 4.311943788528442 and perplexity is 74.5853262297267
At time: 676.834662437439 and batch: 400, loss is 4.302131175994873 and perplexity is 73.85702841867273
At time: 678.0260627269745 and batch: 450, loss is 4.240128221511841 and perplexity is 69.41675198903476
At time: 679.2176978588104 and batch: 500, loss is 4.2524099445343015 and perplexity is 70.27456624588702
At time: 680.4094123840332 and batch: 550, loss is 4.248508558273316 and perplexity is 70.00093214188327
At time: 681.6003715991974 and batch: 600, loss is 4.271850366592407 and perplexity is 71.65409937986729
At time: 682.7946424484253 and batch: 650, loss is 4.282993850708007 and perplexity is 72.45704117378027
At time: 683.9919126033783 and batch: 700, loss is 4.268944387435913 and perplexity is 71.44617631695644
At time: 685.184387922287 and batch: 750, loss is 4.248950872421265 and perplexity is 70.03190139310331
At time: 686.3773412704468 and batch: 800, loss is 4.268796715736389 and perplexity is 71.4356265176478
At time: 687.5673897266388 and batch: 850, loss is 4.291096668243409 and perplexity is 73.04653240221545
At time: 688.7631845474243 and batch: 900, loss is 4.252291784286499 and perplexity is 70.26626307628715
At time: 689.9619114398956 and batch: 950, loss is 4.234273700714112 and perplexity is 69.01153749654823
At time: 691.1526169776917 and batch: 1000, loss is 4.241794857978821 and perplexity is 69.53254094156233
At time: 692.342936038971 and batch: 1050, loss is 4.221504006385803 and perplexity is 68.13588406989862
At time: 693.5678141117096 and batch: 1100, loss is 4.160687575340271 and perplexity is 64.11559174758337
At time: 694.7578799724579 and batch: 1150, loss is 4.192122573852539 and perplexity is 66.16307804235029
At time: 695.9517266750336 and batch: 1200, loss is 4.207233562469482 and perplexity is 67.17045965869268
At time: 697.1442625522614 and batch: 1250, loss is 4.26357551574707 and perplexity is 71.06361883360074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737764010464188 and perplexity of 114.17861385500156
Finished 22 epochs...
Completing Train Step...
At time: 700.0975849628448 and batch: 50, loss is 4.33229811668396 and perplexity is 76.11901609872642
At time: 701.3160490989685 and batch: 100, loss is 4.333600530624389 and perplexity is 76.21821915412329
At time: 702.5106325149536 and batch: 150, loss is 4.265017905235291 and perplexity is 71.16619420943924
At time: 703.706179857254 and batch: 200, loss is 4.315295162200928 and perplexity is 74.83570885692728
At time: 704.9001774787903 and batch: 250, loss is 4.31489070892334 and perplexity is 74.80544742927876
At time: 706.0940542221069 and batch: 300, loss is 4.313601303100586 and perplexity is 74.70905500761953
At time: 707.2916219234467 and batch: 350, loss is 4.31035945892334 and perplexity is 74.46725204818367
At time: 708.4883880615234 and batch: 400, loss is 4.300693311691284 and perplexity is 73.75090834535622
At time: 709.688383102417 and batch: 450, loss is 4.238762969970703 and perplexity is 69.32204532532064
At time: 710.9055180549622 and batch: 500, loss is 4.251326370239258 and perplexity is 70.19845977326291
At time: 712.1014063358307 and batch: 550, loss is 4.247540550231934 and perplexity is 69.93320346290733
At time: 713.301652431488 and batch: 600, loss is 4.270932397842407 and perplexity is 71.58835333685049
At time: 714.5054984092712 and batch: 650, loss is 4.282177286148071 and perplexity is 72.39789947163513
At time: 715.7006466388702 and batch: 700, loss is 4.268223638534546 and perplexity is 71.39470011680487
At time: 716.8938207626343 and batch: 750, loss is 4.248520607948303 and perplexity is 70.00177563544631
At time: 718.0894770622253 and batch: 800, loss is 4.268479948043823 and perplexity is 71.41300160267903
At time: 719.2860593795776 and batch: 850, loss is 4.290881757736206 and perplexity is 73.03083562164944
At time: 720.4815275669098 and batch: 900, loss is 4.252223691940308 and perplexity is 70.2614786444696
At time: 721.676257610321 and batch: 950, loss is 4.2343130207061765 and perplexity is 69.01425108300374
At time: 722.8728532791138 and batch: 1000, loss is 4.241987462043762 and perplexity is 69.54593448137709
At time: 724.1220688819885 and batch: 1050, loss is 4.222025361061096 and perplexity is 68.17141629325421
At time: 725.3188843727112 and batch: 1100, loss is 4.16125018119812 and perplexity is 64.15167370408835
At time: 726.5144112110138 and batch: 1150, loss is 4.192751026153564 and perplexity is 66.20467144935611
At time: 727.71191573143 and batch: 1200, loss is 4.207966203689575 and perplexity is 67.21968953792857
At time: 728.9056072235107 and batch: 1250, loss is 4.264098796844483 and perplexity is 71.10081481317782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737752427149863 and perplexity of 114.17729129588795
Finished 23 epochs...
Completing Train Step...
At time: 731.8644120693207 and batch: 50, loss is 4.331156053543091 and perplexity is 76.0321329985575
At time: 733.0571908950806 and batch: 100, loss is 4.332375545501709 and perplexity is 76.1249101323322
At time: 734.2488145828247 and batch: 150, loss is 4.263810820579529 and perplexity is 71.08034241401677
At time: 735.4410417079926 and batch: 200, loss is 4.314009418487549 and perplexity is 74.7395511450618
At time: 736.6339106559753 and batch: 250, loss is 4.313713827133179 and perplexity is 74.71746204474817
At time: 737.8248510360718 and batch: 300, loss is 4.312453155517578 and perplexity is 74.62332721018295
At time: 739.0187745094299 and batch: 350, loss is 4.309255619049072 and perplexity is 74.38509747713323
At time: 740.2114839553833 and batch: 400, loss is 4.299680347442627 and perplexity is 73.67623913690053
At time: 741.4021108150482 and batch: 450, loss is 4.237738618850708 and perplexity is 69.25107156778134
At time: 742.5918002128601 and batch: 500, loss is 4.250496559143066 and perplexity is 70.14023247457799
At time: 743.7906956672668 and batch: 550, loss is 4.2468251037597655 and perplexity is 69.8831878930673
At time: 744.9806218147278 and batch: 600, loss is 4.270244760513306 and perplexity is 71.5391434339893
At time: 746.1728849411011 and batch: 650, loss is 4.2815562438964845 and perplexity is 72.35295127594611
At time: 747.3633551597595 and batch: 700, loss is 4.26770058631897 and perplexity is 71.35736672523612
At time: 748.5528178215027 and batch: 750, loss is 4.248175768852234 and perplexity is 69.97764044803021
At time: 749.743283033371 and batch: 800, loss is 4.268224420547486 and perplexity is 71.39475594840602
At time: 750.9335353374481 and batch: 850, loss is 4.2906777286529545 and perplexity is 73.01593672716405
At time: 752.1266169548035 and batch: 900, loss is 4.252152991294861 and perplexity is 70.25651128817913
At time: 753.3167321681976 and batch: 950, loss is 4.234303512573242 and perplexity is 69.01359488944968
At time: 754.5547208786011 and batch: 1000, loss is 4.242085666656494 and perplexity is 69.55276454830644
At time: 755.7449610233307 and batch: 1050, loss is 4.222360644340515 and perplexity is 68.19427686144066
At time: 756.9360229969025 and batch: 1100, loss is 4.16160590171814 and perplexity is 64.17449783008281
At time: 758.1265940666199 and batch: 1150, loss is 4.193140602111816 and perplexity is 66.23046822225176
At time: 759.3180968761444 and batch: 1200, loss is 4.208416509628296 and perplexity is 67.24996577960056
At time: 760.5081026554108 and batch: 1250, loss is 4.2643980789184575 and perplexity is 71.12209719705555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737771584169708 and perplexity of 114.17947861347425
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 763.4067990779877 and batch: 50, loss is 4.330519409179687 and perplexity is 75.98374297489876
At time: 764.6228861808777 and batch: 100, loss is 4.331906871795654 and perplexity is 76.08924074788166
At time: 765.8166818618774 and batch: 150, loss is 4.263575773239136 and perplexity is 71.06363713192111
At time: 767.0071187019348 and batch: 200, loss is 4.313642339706421 and perplexity is 74.71212087656819
At time: 768.1982498168945 and batch: 250, loss is 4.313356227874756 and perplexity is 74.69074791249137
At time: 769.388950586319 and batch: 300, loss is 4.312200508117676 and perplexity is 74.6044762020206
At time: 770.5782661437988 and batch: 350, loss is 4.308564233779907 and perplexity is 74.33368649093934
At time: 771.7691593170166 and batch: 400, loss is 4.2989608001708985 and perplexity is 73.62324466833748
At time: 772.9617912769318 and batch: 450, loss is 4.236861066818237 and perplexity is 69.19032680641895
At time: 774.1543111801147 and batch: 500, loss is 4.249645442962646 and perplexity is 70.08056038536299
At time: 775.3460547924042 and batch: 550, loss is 4.245568389892578 and perplexity is 69.79541988294775
At time: 776.5381028652191 and batch: 600, loss is 4.269180688858032 and perplexity is 71.46306114490098
At time: 777.7306976318359 and batch: 650, loss is 4.280522727966309 and perplexity is 72.27821197698802
At time: 778.9388012886047 and batch: 700, loss is 4.266815958023071 and perplexity is 71.29426989224685
At time: 780.1397273540497 and batch: 750, loss is 4.246753101348877 and perplexity is 69.87815631620342
At time: 781.333925485611 and batch: 800, loss is 4.266908602714539 and perplexity is 71.30087523385463
At time: 782.528383731842 and batch: 850, loss is 4.28875771522522 and perplexity is 72.8758796469727
At time: 783.7218127250671 and batch: 900, loss is 4.250035481452942 and perplexity is 70.10789983270656
At time: 784.9616339206696 and batch: 950, loss is 4.23231611251831 and perplexity is 68.87657347046326
At time: 786.1545860767365 and batch: 1000, loss is 4.239732041358947 and perplexity is 69.38925589668105
At time: 787.3471252918243 and batch: 1050, loss is 4.219822735786438 and perplexity is 68.02142545616258
At time: 788.5380136966705 and batch: 1100, loss is 4.159193391799927 and perplexity is 64.01986282181757
At time: 789.7334275245667 and batch: 1150, loss is 4.190450496673584 and perplexity is 66.05254070869653
At time: 790.9248106479645 and batch: 1200, loss is 4.205750503540039 and perplexity is 67.07091574169382
At time: 792.1173572540283 and batch: 1250, loss is 4.262397747039795 and perplexity is 70.97997159533185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737643722200046 and perplexity of 114.16488033374605
Finished 25 epochs...
Completing Train Step...
At time: 795.0664381980896 and batch: 50, loss is 4.330135917663574 and perplexity is 75.95460944069399
At time: 796.259049654007 and batch: 100, loss is 4.331258144378662 and perplexity is 76.03989557878299
At time: 797.4547915458679 and batch: 150, loss is 4.262862696647644 and perplexity is 71.01298137863841
At time: 798.6490378379822 and batch: 200, loss is 4.31299765586853 and perplexity is 74.6639707022318
At time: 799.8525393009186 and batch: 250, loss is 4.31275276184082 and perplexity is 74.64568818045328
At time: 801.0454359054565 and batch: 300, loss is 4.311551895141601 and perplexity is 74.55610246029694
At time: 802.2363359928131 and batch: 350, loss is 4.308042764663696 and perplexity is 74.2949338741648
At time: 803.4334335327148 and batch: 400, loss is 4.298495321273804 and perplexity is 73.58898257636775
At time: 804.629401922226 and batch: 450, loss is 4.236415538787842 and perplexity is 69.15950744234955
At time: 805.8209881782532 and batch: 500, loss is 4.2493062210083 and perplexity is 70.05679155239032
At time: 807.014600276947 and batch: 550, loss is 4.245261645317077 and perplexity is 69.77401379977164
At time: 808.2053546905518 and batch: 600, loss is 4.268888063430786 and perplexity is 71.44215229548085
At time: 809.401985168457 and batch: 650, loss is 4.2802971076965335 and perplexity is 72.26190638680788
At time: 810.5954959392548 and batch: 700, loss is 4.266646995544433 and perplexity is 71.28222485330167
At time: 811.7851855754852 and batch: 750, loss is 4.246654772758484 and perplexity is 69.87128563339142
At time: 812.9765477180481 and batch: 800, loss is 4.266870279312133 and perplexity is 71.29814279407972
At time: 814.214396238327 and batch: 850, loss is 4.288733921051025 and perplexity is 72.87414564622745
At time: 815.4048080444336 and batch: 900, loss is 4.249978337287903 and perplexity is 70.10389368977292
At time: 816.5998497009277 and batch: 950, loss is 4.23233567237854 and perplexity is 68.87792069978924
At time: 817.7980601787567 and batch: 1000, loss is 4.239801979064941 and perplexity is 69.39410899176426
At time: 818.9958670139313 and batch: 1050, loss is 4.219988698959351 and perplexity is 68.03271544459258
At time: 820.1885869503021 and batch: 1100, loss is 4.159421482086182 and perplexity is 64.03446679610362
At time: 821.3808283805847 and batch: 1150, loss is 4.190686178207398 and perplexity is 66.06810990741727
At time: 822.5723598003387 and batch: 1200, loss is 4.206006679534912 and perplexity is 67.08809990125161
At time: 823.7637181282043 and batch: 1250, loss is 4.262607536315918 and perplexity is 70.9948639942702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737608526744982 and perplexity of 114.1608623195387
Finished 26 epochs...
Completing Train Step...
At time: 826.9444508552551 and batch: 50, loss is 4.329756917953492 and perplexity is 75.92582812013714
At time: 828.1925840377808 and batch: 100, loss is 4.330823831558227 and perplexity is 76.00687764784297
At time: 829.4130771160126 and batch: 150, loss is 4.262430782318115 and perplexity is 70.98231647718035
At time: 830.634913444519 and batch: 200, loss is 4.31253870010376 and perplexity is 74.62971110487858
At time: 831.8558864593506 and batch: 250, loss is 4.3123267555236815 and perplexity is 74.61389541818083
At time: 833.0710048675537 and batch: 300, loss is 4.31113642692566 and perplexity is 74.52513320323665
At time: 834.2866792678833 and batch: 350, loss is 4.307650518417359 and perplexity is 74.26579767988574
At time: 835.5010426044464 and batch: 400, loss is 4.29814040184021 and perplexity is 73.56286905072598
At time: 836.7214608192444 and batch: 450, loss is 4.236076097488404 and perplexity is 69.13603583311695
At time: 837.9366352558136 and batch: 500, loss is 4.249035654067993 and perplexity is 70.03783906472614
At time: 839.1541368961334 and batch: 550, loss is 4.245030446052551 and perplexity is 69.75788396377305
At time: 840.372585773468 and batch: 600, loss is 4.2686898899078365 and perplexity is 71.42799575524539
At time: 841.584513425827 and batch: 650, loss is 4.280139055252075 and perplexity is 72.25048611838666
At time: 842.7804811000824 and batch: 700, loss is 4.266529426574707 and perplexity is 71.27384476819454
At time: 843.9738421440125 and batch: 750, loss is 4.246578106880188 and perplexity is 69.86592909524511
At time: 845.2133843898773 and batch: 800, loss is 4.266826162338257 and perplexity is 71.29499740515965
At time: 846.4062662124634 and batch: 850, loss is 4.288696136474609 and perplexity is 72.87139217952213
At time: 847.6050491333008 and batch: 900, loss is 4.24995325088501 and perplexity is 70.10213505731043
At time: 848.7959039211273 and batch: 950, loss is 4.232360143661499 and perplexity is 68.87960625149998
At time: 849.9860429763794 and batch: 1000, loss is 4.239847221374512 and perplexity is 69.39724861254696
At time: 851.1767868995667 and batch: 1050, loss is 4.220110864639282 and perplexity is 68.04102721522867
At time: 852.3692111968994 and batch: 1100, loss is 4.15958740234375 and perplexity is 64.04509229279584
At time: 853.5600025653839 and batch: 1150, loss is 4.190862722396851 and perplexity is 66.07977487799067
At time: 854.7505922317505 and batch: 1200, loss is 4.206184320449829 and perplexity is 67.1000185512867
At time: 855.9402577877045 and batch: 1250, loss is 4.262737174034118 and perplexity is 71.00406820303579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737603180599908 and perplexity of 114.16025200063847
Finished 27 epochs...
Completing Train Step...
At time: 858.8840217590332 and batch: 50, loss is 4.3294049167633055 and perplexity is 75.89910684150983
At time: 860.1049604415894 and batch: 100, loss is 4.3304351043701175 and perplexity is 75.97733744992894
At time: 861.2976729869843 and batch: 150, loss is 4.262061204910278 and perplexity is 70.95608786370445
At time: 862.4915795326233 and batch: 200, loss is 4.312137298583984 and perplexity is 74.59976063690448
At time: 863.7048027515411 and batch: 250, loss is 4.311954412460327 and perplexity is 74.58611862336137
At time: 864.8999483585358 and batch: 300, loss is 4.310778970718384 and perplexity is 74.49849849243006
At time: 866.0930976867676 and batch: 350, loss is 4.30730128288269 and perplexity is 74.2398659527293
At time: 867.2909064292908 and batch: 400, loss is 4.297824935913086 and perplexity is 73.53966613209654
At time: 868.4916124343872 and batch: 450, loss is 4.235769481658935 and perplexity is 69.11484087966399
At time: 869.6865184307098 and batch: 500, loss is 4.2487874031066895 and perplexity is 70.02045426183702
At time: 870.8819909095764 and batch: 550, loss is 4.244819307327271 and perplexity is 69.74315692785319
At time: 872.0772998332977 and batch: 600, loss is 4.268511600494385 and perplexity is 71.41526203495566
At time: 873.2720260620117 and batch: 650, loss is 4.279993953704834 and perplexity is 72.24000322162266
At time: 874.4675137996674 and batch: 700, loss is 4.266425523757935 and perplexity is 71.2664395996765
At time: 875.7123429775238 and batch: 750, loss is 4.246501126289368 and perplexity is 69.860550981753
At time: 876.9101383686066 and batch: 800, loss is 4.266775183677673 and perplexity is 71.29136297432562
At time: 878.1081621646881 and batch: 850, loss is 4.2886511325836185 and perplexity is 72.86811275712608
At time: 879.3324453830719 and batch: 900, loss is 4.249929270744324 and perplexity is 70.1004540184052
At time: 880.539952993393 and batch: 950, loss is 4.232375783920288 and perplexity is 68.8806835547917
At time: 881.7349503040314 and batch: 1000, loss is 4.2398788404464725 and perplexity is 69.39944292383558
At time: 882.9317195415497 and batch: 1050, loss is 4.220207901000976 and perplexity is 68.04762998930498
At time: 884.1274628639221 and batch: 1100, loss is 4.1597214031219485 and perplexity is 64.0536749600321
At time: 885.3239312171936 and batch: 1150, loss is 4.191007976531982 and perplexity is 66.08937393567479
At time: 886.5236978530884 and batch: 1200, loss is 4.206324863433838 and perplexity is 67.10944965084286
At time: 887.7183575630188 and batch: 1250, loss is 4.262835650444031 and perplexity is 71.01106077305757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737607190208713 and perplexity of 114.16070973950775
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 890.674756526947 and batch: 50, loss is 4.329193811416626 and perplexity is 75.88308582536801
At time: 891.8659138679504 and batch: 100, loss is 4.3302484893798825 and perplexity is 75.9631602627216
At time: 893.0555214881897 and batch: 150, loss is 4.261962285041809 and perplexity is 70.9490692439721
At time: 894.2456603050232 and batch: 200, loss is 4.312029418945312 and perplexity is 74.59171327576296
At time: 895.4357175827026 and batch: 250, loss is 4.3117803192138675 and perplexity is 74.57313481405895
At time: 896.6260848045349 and batch: 300, loss is 4.310606565475464 and perplexity is 74.48565566781724
At time: 897.8242466449738 and batch: 350, loss is 4.3071087455749515 and perplexity is 74.2255733847823
At time: 899.0156247615814 and batch: 400, loss is 4.297548236846924 and perplexity is 73.51932059007798
At time: 900.207248210907 and batch: 450, loss is 4.23555736541748 and perplexity is 69.10018205413027
At time: 901.4010667800903 and batch: 500, loss is 4.2485834407806395 and perplexity is 70.00617418346322
At time: 902.5926368236542 and batch: 550, loss is 4.244330682754517 and perplexity is 69.70908703196882
At time: 903.7843279838562 and batch: 600, loss is 4.268100318908691 and perplexity is 71.38589629195178
At time: 904.9769895076752 and batch: 650, loss is 4.279730262756348 and perplexity is 72.22095669796279
At time: 906.2159719467163 and batch: 700, loss is 4.266326065063477 and perplexity is 71.2593518851085
At time: 907.409185886383 and batch: 750, loss is 4.246214556694031 and perplexity is 69.84053394020286
At time: 908.6015515327454 and batch: 800, loss is 4.266294107437134 and perplexity is 71.25707464175538
At time: 909.7973906993866 and batch: 850, loss is 4.288005504608154 and perplexity is 72.82108224875043
At time: 910.987544298172 and batch: 900, loss is 4.249184584617614 and perplexity is 70.04827061536137
At time: 912.1782712936401 and batch: 950, loss is 4.23167760848999 and perplexity is 68.83260953791314
At time: 913.3739101886749 and batch: 1000, loss is 4.239104237556457 and perplexity is 69.34570672957388
At time: 914.5641658306122 and batch: 1050, loss is 4.219361619949341 and perplexity is 67.99006693013497
At time: 915.7581703662872 and batch: 1100, loss is 4.158937239646912 and perplexity is 64.00346609623797
At time: 916.9497275352478 and batch: 1150, loss is 4.19019121170044 and perplexity is 66.03541649757399
At time: 918.1414501667023 and batch: 1200, loss is 4.205418109893799 and perplexity is 67.04862550022632
At time: 919.3326306343079 and batch: 1250, loss is 4.262067303657532 and perplexity is 70.95652060827007
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737569767193203 and perplexity of 114.1564375814355
Finished 29 epochs...
Completing Train Step...
At time: 922.2631387710571 and batch: 50, loss is 4.329063634872437 and perplexity is 75.87320827041977
At time: 923.494785785675 and batch: 100, loss is 4.330083637237549 and perplexity is 75.95063860515327
At time: 924.6907591819763 and batch: 150, loss is 4.2617377138137815 and perplexity is 70.93313791329076
At time: 925.8868165016174 and batch: 200, loss is 4.311837491989135 and perplexity is 74.5773984890186
At time: 927.0821528434753 and batch: 250, loss is 4.311620540618897 and perplexity is 74.56122057520132
At time: 928.2746367454529 and batch: 300, loss is 4.310467138290405 and perplexity is 74.47527106648492
At time: 929.4671401977539 and batch: 350, loss is 4.306966133117676 and perplexity is 74.21498864814481
At time: 930.6607792377472 and batch: 400, loss is 4.29743275642395 and perplexity is 73.51083103803742
At time: 931.8557605743408 and batch: 450, loss is 4.23543384552002 and perplexity is 69.09164733384318
At time: 933.0519733428955 and batch: 500, loss is 4.248495626449585 and perplexity is 70.00002690802097
At time: 934.2456805706024 and batch: 550, loss is 4.244237651824951 and perplexity is 69.70260223245123
At time: 935.4794533252716 and batch: 600, loss is 4.268011417388916 and perplexity is 71.37955025937107
At time: 936.6771013736725 and batch: 650, loss is 4.27966311454773 and perplexity is 72.21610735291007
At time: 937.8750050067902 and batch: 700, loss is 4.266275815963745 and perplexity is 71.25577125679123
At time: 939.0678629875183 and batch: 750, loss is 4.246183748245239 and perplexity is 69.83838229483402
At time: 940.2661669254303 and batch: 800, loss is 4.266288204193115 and perplexity is 71.2566539950973
At time: 941.459235906601 and batch: 850, loss is 4.287997512817383 and perplexity is 72.82050028022284
At time: 942.6556375026703 and batch: 900, loss is 4.249161100387573 and perplexity is 70.04662560497621
At time: 943.8483068943024 and batch: 950, loss is 4.231669731140137 and perplexity is 68.83206732150212
At time: 945.0421192646027 and batch: 1000, loss is 4.239129862785339 and perplexity is 69.347483751949
At time: 946.2339406013489 and batch: 1050, loss is 4.219406628608704 and perplexity is 67.99312714076497
At time: 947.4278268814087 and batch: 1100, loss is 4.1590179920196535 and perplexity is 64.0086347366761
At time: 948.6312642097473 and batch: 1150, loss is 4.190269651412964 and perplexity is 66.04059649981679
At time: 949.8556871414185 and batch: 1200, loss is 4.205497331619263 and perplexity is 67.05393741843532
At time: 951.069331407547 and batch: 1250, loss is 4.262126531600952 and perplexity is 70.96072334151631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737548382612911 and perplexity of 114.15399642003185
Finished 30 epochs...
Completing Train Step...
At time: 953.9995069503784 and batch: 50, loss is 4.328946056365967 and perplexity is 75.86428773635207
At time: 955.1918842792511 and batch: 100, loss is 4.329943780899048 and perplexity is 75.94001716968597
At time: 956.3838059902191 and batch: 150, loss is 4.261593799591065 and perplexity is 70.92293036040675
At time: 957.5838210582733 and batch: 200, loss is 4.311692457199097 and perplexity is 74.56658295602038
At time: 958.7853443622589 and batch: 250, loss is 4.311487407684326 and perplexity is 74.55129468184727
At time: 959.9764504432678 and batch: 300, loss is 4.310336990356445 and perplexity is 74.46557889454628
At time: 961.1701374053955 and batch: 350, loss is 4.306849641799927 and perplexity is 74.20634374985801
At time: 962.3615601062775 and batch: 400, loss is 4.29732304573059 and perplexity is 73.50276655618285
At time: 963.5521454811096 and batch: 450, loss is 4.235330190658569 and perplexity is 69.08448601987033
At time: 964.741626739502 and batch: 500, loss is 4.2484156608581545 and perplexity is 69.99442953827061
At time: 965.9797222614288 and batch: 550, loss is 4.244157571792602 and perplexity is 69.69702066929848
At time: 967.171190738678 and batch: 600, loss is 4.267943592071533 and perplexity is 71.3747090828991
At time: 968.3668863773346 and batch: 650, loss is 4.279618330001831 and perplexity is 72.21287325975496
At time: 969.5574994087219 and batch: 700, loss is 4.266253566741943 and perplexity is 71.25418588896855
At time: 970.748078584671 and batch: 750, loss is 4.246173977851868 and perplexity is 69.83769994969995
At time: 971.9399518966675 and batch: 800, loss is 4.266278343200684 and perplexity is 71.25595133723601
At time: 973.1307165622711 and batch: 850, loss is 4.287984819412231 and perplexity is 72.81957594597591
At time: 974.3233013153076 and batch: 900, loss is 4.249139771461487 and perplexity is 70.04513160160893
At time: 975.5129141807556 and batch: 950, loss is 4.231666107177734 and perplexity is 68.83181787713
At time: 976.7036924362183 and batch: 1000, loss is 4.239145112037659 and perplexity is 69.34854125728953
At time: 977.8931276798248 and batch: 1050, loss is 4.219446139335632 and perplexity is 67.9958136517171
At time: 979.0839676856995 and batch: 1100, loss is 4.159079117774963 and perplexity is 64.01254743240276
At time: 980.2764666080475 and batch: 1150, loss is 4.190334329605102 and perplexity is 66.04486802434187
At time: 981.4703576564789 and batch: 1200, loss is 4.205559778213501 and perplexity is 67.05812483920107
At time: 982.6608233451843 and batch: 1250, loss is 4.262169775962829 and perplexity is 70.96379205906746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737537690322765 and perplexity of 114.15277585890614
Finished 31 epochs...
Completing Train Step...
At time: 985.6111688613892 and batch: 50, loss is 4.3288321208953855 and perplexity is 75.855644595418
At time: 986.840409040451 and batch: 100, loss is 4.329813423156739 and perplexity is 75.9301184456985
At time: 988.0317904949188 and batch: 150, loss is 4.261466789245605 and perplexity is 70.91392298654762
At time: 989.2234661579132 and batch: 200, loss is 4.311559009552002 and perplexity is 74.55663288489453
At time: 990.4150269031525 and batch: 250, loss is 4.311363868713379 and perplexity is 74.54208526049241
At time: 991.6063733100891 and batch: 300, loss is 4.3102182769775395 and perplexity is 74.45673935875949
At time: 992.797337770462 and batch: 350, loss is 4.306739120483399 and perplexity is 74.19814282024916
At time: 993.99161028862 and batch: 400, loss is 4.297218837738037 and perplexity is 73.49510737951356
At time: 995.1853189468384 and batch: 450, loss is 4.23523328781128 and perplexity is 69.07779186081824
At time: 996.4289681911469 and batch: 500, loss is 4.248339853286743 and perplexity is 69.98912363167149
At time: 997.6468458175659 and batch: 550, loss is 4.244083738327026 and perplexity is 69.69187488668996
At time: 998.8608779907227 and batch: 600, loss is 4.2678831100463865 and perplexity is 71.37039232649393
At time: 1000.0783843994141 and batch: 650, loss is 4.279578742980957 and perplexity is 72.2100146238166
At time: 1001.2891459465027 and batch: 700, loss is 4.266236276626587 and perplexity is 71.25295390652549
At time: 1002.4822227954865 and batch: 750, loss is 4.2461666488647465 and perplexity is 69.83718811197207
At time: 1003.6745641231537 and batch: 800, loss is 4.2662664937973025 and perplexity is 71.25510700172775
At time: 1004.8877067565918 and batch: 850, loss is 4.28797082901001 and perplexity is 72.81855717794532
At time: 1006.1052951812744 and batch: 900, loss is 4.249120779037476 and perplexity is 70.0438012874026
At time: 1007.3208749294281 and batch: 950, loss is 4.231664409637451 and perplexity is 68.83170103244561
At time: 1008.5423636436462 and batch: 1000, loss is 4.239155931472778 and perplexity is 69.34929157339127
At time: 1009.7657315731049 and batch: 1050, loss is 4.219480681419372 and perplexity is 67.99816240937147
At time: 1010.9869523048401 and batch: 1100, loss is 4.1591318464279174 and perplexity is 64.01592281678998
At time: 1012.2041499614716 and batch: 1150, loss is 4.190392065048218 and perplexity is 66.04868126414122
At time: 1013.4230351448059 and batch: 1200, loss is 4.205612525939942 and perplexity is 67.06166209611604
At time: 1014.6395342350006 and batch: 1250, loss is 4.26220458984375 and perplexity is 70.96626262707869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737531898665602 and perplexity of 114.15211472707873
Finished 32 epochs...
Completing Train Step...
At time: 1017.9727087020874 and batch: 50, loss is 4.328721227645874 and perplexity is 75.8472331828881
At time: 1019.2399308681488 and batch: 100, loss is 4.32968879699707 and perplexity is 75.92065615627007
At time: 1020.4523491859436 and batch: 150, loss is 4.261348042488098 and perplexity is 70.90550268808232
At time: 1021.6606032848358 and batch: 200, loss is 4.311432609558105 and perplexity is 74.54720952252194
At time: 1022.8708391189575 and batch: 250, loss is 4.311246156692505 and perplexity is 74.53331127740803
At time: 1024.0825791358948 and batch: 300, loss is 4.310106687545776 and perplexity is 74.44843123708141
At time: 1025.286387205124 and batch: 350, loss is 4.306632804870605 and perplexity is 74.1902548185434
At time: 1026.543571472168 and batch: 400, loss is 4.297119140625 and perplexity is 73.48778049472617
At time: 1027.7462198734283 and batch: 450, loss is 4.235140781402588 and perplexity is 69.07140201792814
At time: 1028.9557042121887 and batch: 500, loss is 4.248266849517822 and perplexity is 69.98401434836364
At time: 1030.1668529510498 and batch: 550, loss is 4.244013123512268 and perplexity is 69.6869537816082
At time: 1031.381935119629 and batch: 600, loss is 4.267826271057129 and perplexity is 71.36633582081612
At time: 1032.5893609523773 and batch: 650, loss is 4.27954140663147 and perplexity is 72.20731861580403
At time: 1033.7860732078552 and batch: 700, loss is 4.26622055053711 and perplexity is 71.2518333850076
At time: 1034.9787583351135 and batch: 750, loss is 4.246159267425537 and perplexity is 69.83667261491604
At time: 1036.170338153839 and batch: 800, loss is 4.266252961158752 and perplexity is 71.25414273864436
At time: 1037.3619542121887 and batch: 850, loss is 4.287955904006958 and perplexity is 72.81747036886759
At time: 1038.55482172966 and batch: 900, loss is 4.249103074073791 and perplexity is 70.04256117542259
At time: 1039.7470009326935 and batch: 950, loss is 4.231662950515747 and perplexity is 68.83160059868996
At time: 1040.9393842220306 and batch: 1000, loss is 4.239164257049561 and perplexity is 69.34986894864657
At time: 1042.1298837661743 and batch: 1050, loss is 4.219511451721192 and perplexity is 68.00025476554303
At time: 1043.3290190696716 and batch: 1100, loss is 4.159179329872131 and perplexity is 64.01896258545862
At time: 1044.5403344631195 and batch: 1150, loss is 4.190445127487183 and perplexity is 66.05218606124527
At time: 1045.7317011356354 and batch: 1200, loss is 4.205659122467041 and perplexity is 67.0647870094757
At time: 1046.9253861904144 and batch: 1250, loss is 4.262233920097351 and perplexity is 70.96834411608381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737528334568887 and perplexity of 114.15170787862657
Finished 33 epochs...
Completing Train Step...
At time: 1050.0463891029358 and batch: 50, loss is 4.328613290786743 and perplexity is 75.83904691257264
At time: 1051.2378015518188 and batch: 100, loss is 4.32956784248352 and perplexity is 75.91147376557329
At time: 1052.4285681247711 and batch: 150, loss is 4.261234555244446 and perplexity is 70.89745627461376
At time: 1053.6176450252533 and batch: 200, loss is 4.3113111400604245 and perplexity is 74.5381548603715
At time: 1054.808634519577 and batch: 250, loss is 4.311132793426514 and perplexity is 74.52486241672075
At time: 1055.9988610744476 and batch: 300, loss is 4.309999856948853 and perplexity is 74.44047829154886
At time: 1057.237497329712 and batch: 350, loss is 4.306529808044433 and perplexity is 74.18261385126864
At time: 1058.4272274971008 and batch: 400, loss is 4.297023029327392 and perplexity is 73.48071782819096
At time: 1059.619408607483 and batch: 450, loss is 4.235051641464233 and perplexity is 69.06524527182022
At time: 1060.8159573078156 and batch: 500, loss is 4.2481958770751955 and perplexity is 69.97904758817415
At time: 1062.0361397266388 and batch: 550, loss is 4.243944473266602 and perplexity is 69.68216991931992
At time: 1063.2531867027283 and batch: 600, loss is 4.267771492004394 and perplexity is 71.36242654761682
At time: 1064.4688503742218 and batch: 650, loss is 4.279505033493042 and perplexity is 72.20469225677323
At time: 1065.6844599246979 and batch: 700, loss is 4.266205739974976 and perplexity is 71.2507781131167
At time: 1066.8994810581207 and batch: 750, loss is 4.246151041984558 and perplexity is 69.83609817984977
At time: 1068.1155304908752 and batch: 800, loss is 4.266238112449646 and perplexity is 71.25308471446135
At time: 1069.3268330097198 and batch: 850, loss is 4.28793963432312 and perplexity is 72.81628566128421
At time: 1070.5286748409271 and batch: 900, loss is 4.249086213111878 and perplexity is 70.04138020042248
At time: 1071.7198915481567 and batch: 950, loss is 4.231661233901978 and perplexity is 68.83148244151802
At time: 1072.910412788391 and batch: 1000, loss is 4.239170832633972 and perplexity is 69.35032496606304
At time: 1074.1006906032562 and batch: 1050, loss is 4.219539279937744 and perplexity is 68.00214711768854
At time: 1075.2949402332306 and batch: 1100, loss is 4.159222965240478 and perplexity is 64.02175613742064
At time: 1076.4868314266205 and batch: 1150, loss is 4.19049451828003 and perplexity is 66.05544851165094
At time: 1077.6796407699585 and batch: 1200, loss is 4.2057013416290285 and perplexity is 67.06761848835303
At time: 1078.8724119663239 and batch: 1250, loss is 4.262259864807129 and perplexity is 70.97018539306092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737529671105155 and perplexity of 114.1518604466262
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1081.8515675067902 and batch: 50, loss is 4.328545379638672 and perplexity is 75.83389677070618
At time: 1083.0680837631226 and batch: 100, loss is 4.329505109786988 and perplexity is 75.90671178349376
At time: 1084.261959552765 and batch: 150, loss is 4.261184587478637 and perplexity is 70.89391377562832
At time: 1085.454960346222 and batch: 200, loss is 4.311269121170044 and perplexity is 74.53502291561415
At time: 1086.6492648124695 and batch: 250, loss is 4.311066932678223 and perplexity is 74.51995431514312
At time: 1087.8731467723846 and batch: 300, loss is 4.30993390083313 and perplexity is 74.43556864866048
At time: 1089.0717861652374 and batch: 350, loss is 4.306478662490845 and perplexity is 74.17881983744086
At time: 1090.2667982578278 and batch: 400, loss is 4.296943712234497 and perplexity is 73.47488978240385
At time: 1091.4616029262543 and batch: 450, loss is 4.235015325546264 and perplexity is 69.06273714958108
At time: 1092.6584177017212 and batch: 500, loss is 4.248179540634156 and perplexity is 69.97790438892713
At time: 1093.854255914688 and batch: 550, loss is 4.243773818016052 and perplexity is 69.67027930578017
At time: 1095.0514283180237 and batch: 600, loss is 4.267595405578613 and perplexity is 71.34986169927308
At time: 1096.2445061206818 and batch: 650, loss is 4.279393663406372 and perplexity is 72.19665126171014
At time: 1097.441968202591 and batch: 700, loss is 4.266180295944213 and perplexity is 71.24896522919018
At time: 1098.6424009799957 and batch: 750, loss is 4.246094365119934 and perplexity is 69.83214020093129
At time: 1099.8355565071106 and batch: 800, loss is 4.2660671043396 and perplexity is 71.24090090090455
At time: 1101.0437459945679 and batch: 850, loss is 4.28772403717041 and perplexity is 72.8005883696312
At time: 1102.256362915039 and batch: 900, loss is 4.248836531639099 and perplexity is 70.02389434849606
At time: 1103.4583406448364 and batch: 950, loss is 4.23141619682312 and perplexity is 68.81461824238853
At time: 1104.6542072296143 and batch: 1000, loss is 4.238888783454895 and perplexity is 69.3307675220478
At time: 1105.8515672683716 and batch: 1050, loss is 4.219293971061706 and perplexity is 67.98546763330735
At time: 1107.0460386276245 and batch: 1100, loss is 4.158918075561523 and perplexity is 64.00223954010147
At time: 1108.2412993907928 and batch: 1150, loss is 4.190201673507691 and perplexity is 66.0361073509869
At time: 1109.437515258789 and batch: 1200, loss is 4.205400972366333 and perplexity is 67.04747646241114
At time: 1110.6356537342072 and batch: 1250, loss is 4.262016153335571 and perplexity is 70.95289125221787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737531007641423 and perplexity of 114.15201301482975
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1113.6014330387115 and batch: 50, loss is 4.328526248931885 and perplexity is 75.83244602853947
At time: 1114.7927017211914 and batch: 100, loss is 4.329482202529907 and perplexity is 75.90497298884839
At time: 1115.985596895218 and batch: 150, loss is 4.261139783859253 and perplexity is 70.89073754285275
At time: 1117.176634311676 and batch: 200, loss is 4.311235790252685 and perplexity is 74.5325386363269
At time: 1118.4088070392609 and batch: 250, loss is 4.311035776138306 and perplexity is 74.5176325673809
At time: 1119.6039807796478 and batch: 300, loss is 4.309916553497314 and perplexity is 74.43427740105437
At time: 1120.7988719940186 and batch: 350, loss is 4.306446733474732 and perplexity is 74.17645141851787
At time: 1121.9901826381683 and batch: 400, loss is 4.296920490264893 and perplexity is 73.47318357055752
At time: 1123.1815221309662 and batch: 450, loss is 4.234995355606079 and perplexity is 69.06135798462215
At time: 1124.372891664505 and batch: 500, loss is 4.248179225921631 and perplexity is 69.97788236600763
At time: 1125.5661458969116 and batch: 550, loss is 4.243722400665283 and perplexity is 69.66669713668453
At time: 1126.7575039863586 and batch: 600, loss is 4.2675302124023435 and perplexity is 71.34521032678302
At time: 1127.9477076530457 and batch: 650, loss is 4.279339895248413 and perplexity is 72.19276948511992
At time: 1129.138004541397 and batch: 700, loss is 4.26614218711853 and perplexity is 71.24625006653028
At time: 1130.3309791088104 and batch: 750, loss is 4.246056656837464 and perplexity is 69.82950700051016
At time: 1131.5222957134247 and batch: 800, loss is 4.266012306213379 and perplexity is 71.23699713998528
At time: 1132.7134189605713 and batch: 850, loss is 4.287661628723145 and perplexity is 72.7960451397205
At time: 1133.9044625759125 and batch: 900, loss is 4.248761224746704 and perplexity is 70.01862126517153
At time: 1135.0950391292572 and batch: 950, loss is 4.231343212127686 and perplexity is 68.80959601170993
At time: 1136.28568816185 and batch: 1000, loss is 4.238803358078003 and perplexity is 69.32484516806527
At time: 1137.478135585785 and batch: 1050, loss is 4.219219055175781 and perplexity is 67.98037463254528
At time: 1138.670287847519 and batch: 1100, loss is 4.158821368217469 and perplexity is 63.99605035277676
At time: 1139.8614559173584 and batch: 1150, loss is 4.190108098983765 and perplexity is 66.029928342783
At time: 1141.0517375469208 and batch: 1200, loss is 4.205307188034058 and perplexity is 67.04118875444922
At time: 1142.2425503730774 and batch: 1250, loss is 4.261941065788269 and perplexity is 70.94756377365593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737531898665602 and perplexity of 114.15211472707873
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1145.171638250351 and batch: 50, loss is 4.328521203994751 and perplexity is 75.83206345958158
At time: 1146.3956966400146 and batch: 100, loss is 4.329475326538086 and perplexity is 75.90445106866929
At time: 1147.5892379283905 and batch: 150, loss is 4.261127057075501 and perplexity is 70.8898353375071
At time: 1148.8248205184937 and batch: 200, loss is 4.311226167678833 and perplexity is 74.5318214449201
At time: 1150.0181987285614 and batch: 250, loss is 4.311026287078858 and perplexity is 74.51692546849037
At time: 1151.211219549179 and batch: 300, loss is 4.309910507202148 and perplexity is 74.43382735080331
At time: 1152.4044332504272 and batch: 350, loss is 4.306436405181885 and perplexity is 74.17568530636161
At time: 1153.6009385585785 and batch: 400, loss is 4.296913814544678 and perplexity is 73.47269308577789
At time: 1154.794777393341 and batch: 450, loss is 4.234988956451416 and perplexity is 69.06091605172517
At time: 1155.9893465042114 and batch: 500, loss is 4.248179388046265 and perplexity is 69.9778937111471
At time: 1157.180857181549 and batch: 550, loss is 4.243707175254822 and perplexity is 69.66563644069993
At time: 1158.3732192516327 and batch: 600, loss is 4.2675102424621585 and perplexity is 71.34378558142639
At time: 1159.5660309791565 and batch: 650, loss is 4.2793224430084225 and perplexity is 72.19150957057545
At time: 1160.7590277194977 and batch: 700, loss is 4.266128578186035 and perplexity is 71.24528048772011
At time: 1161.955982685089 and batch: 750, loss is 4.246043500900268 and perplexity is 69.82858833394464
At time: 1163.1551768779755 and batch: 800, loss is 4.265995488166809 and perplexity is 71.23579908292442
At time: 1164.3491022586823 and batch: 850, loss is 4.2876428031921385 and perplexity is 72.794674728415
At time: 1165.5458581447601 and batch: 900, loss is 4.248738169670105 and perplexity is 70.0170069991035
At time: 1166.7384510040283 and batch: 950, loss is 4.231320905685425 and perplexity is 68.80806113154838
At time: 1167.932522058487 and batch: 1000, loss is 4.238777198791504 and perplexity is 69.32303170329861
At time: 1169.1242308616638 and batch: 1050, loss is 4.219195804595947 and perplexity is 67.97879406779235
At time: 1170.3198301792145 and batch: 1100, loss is 4.158791074752807 and perplexity is 63.994111720051045
At time: 1171.5204989910126 and batch: 1150, loss is 4.190078592300415 and perplexity is 66.02798004733977
At time: 1172.7140429019928 and batch: 1200, loss is 4.205277824401856 and perplexity is 67.03922021054218
At time: 1173.9065945148468 and batch: 1250, loss is 4.26191780090332 and perplexity is 70.94591320594752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7375323441776915 and perplexity of 114.15216558323719
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1176.8325638771057 and batch: 50, loss is 4.328520469665527 and perplexity is 75.8320077739017
At time: 1178.0601861476898 and batch: 100, loss is 4.3294738674163815 and perplexity is 75.90434031491807
At time: 1179.2505090236664 and batch: 150, loss is 4.2611239767074585 and perplexity is 70.88961697106012
At time: 1180.4418652057648 and batch: 200, loss is 4.311223878860473 and perplexity is 74.53165085531401
At time: 1181.6339828968048 and batch: 250, loss is 4.311024103164673 and perplexity is 74.51676273009755
At time: 1182.8290507793427 and batch: 300, loss is 4.309909505844116 and perplexity is 74.43375281592975
At time: 1184.021982908249 and batch: 350, loss is 4.306433811187744 and perplexity is 74.17549289531806
At time: 1185.2144892215729 and batch: 400, loss is 4.296912546157837 and perplexity is 73.47259989403992
At time: 1186.4071140289307 and batch: 450, loss is 4.234987831115722 and perplexity is 69.06083833505501
At time: 1187.6003835201263 and batch: 500, loss is 4.24818000793457 and perplexity is 69.97793708963849
At time: 1188.7937982082367 and batch: 550, loss is 4.243703227043152 and perplexity is 69.66536138656416
At time: 1189.9864528179169 and batch: 600, loss is 4.267504768371582 and perplexity is 71.34339504015098
At time: 1191.1791100502014 and batch: 650, loss is 4.279317636489868 and perplexity is 72.1911625815791
At time: 1192.3703300952911 and batch: 700, loss is 4.26612455368042 and perplexity is 71.2449937612657
At time: 1193.5857360363007 and batch: 750, loss is 4.246039972305298 and perplexity is 69.8283419375738
At time: 1194.7995541095734 and batch: 800, loss is 4.2659908962249755 and perplexity is 71.23547197302958
At time: 1196.0018174648285 and batch: 850, loss is 4.287637586593628 and perplexity is 72.79429498881372
At time: 1197.2147612571716 and batch: 900, loss is 4.248731865882873 and perplexity is 70.01656562817992
At time: 1198.4277431964874 and batch: 950, loss is 4.231314859390259 and perplexity is 68.80764509895872
At time: 1199.6400048732758 and batch: 1000, loss is 4.238769683837891 and perplexity is 69.32251074588852
At time: 1200.84801197052 and batch: 1050, loss is 4.219189085960388 and perplexity is 67.97833734458355
At time: 1202.061341047287 and batch: 1100, loss is 4.158782358169556 and perplexity is 63.99355391247972
At time: 1203.269546508789 and batch: 1150, loss is 4.19007025718689 and perplexity is 66.02742969892388
At time: 1204.4825315475464 and batch: 1200, loss is 4.205269470214843 and perplexity is 67.03866015469879
At time: 1205.6921546459198 and batch: 1250, loss is 4.261911082267761 and perplexity is 70.94543654781353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.737531898665602 and perplexity of 114.15211472707873
Annealing...
Model not improving. Stopping early with 114.15170787862657loss at 37 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -114.15170787862657
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9541e8860>
SETTINGS FOR THIS RUN
{'lr': 16.217378339003858, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 3.178296882833527, 'wordvec_source': '', 'dropout': 0.38907510530416645, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.743696689605713 and batch: 50, loss is 6.957512083053589 and perplexity is 1051.0154626113908
At time: 2.9518959522247314 and batch: 100, loss is 6.210397701263428 and perplexity is 497.89922706825337
At time: 4.201499700546265 and batch: 150, loss is 5.994986248016358 and perplexity is 401.4111637532042
At time: 5.41264796257019 and batch: 200, loss is 5.945707130432129 and perplexity is 382.1094670189576
At time: 6.626888275146484 and batch: 250, loss is 5.9600899791717525 and perplexity is 387.6450026448668
At time: 7.84106183052063 and batch: 300, loss is 5.944279718399048 and perplexity is 381.5644284576243
At time: 9.055323362350464 and batch: 350, loss is 5.947971248626709 and perplexity is 382.97558814586307
At time: 10.271228551864624 and batch: 400, loss is 5.8979912376403805 and perplexity is 364.3049303332642
At time: 11.484479904174805 and batch: 450, loss is 5.875668897628784 and perplexity is 356.26288438684753
At time: 12.698911666870117 and batch: 500, loss is 5.860167760848999 and perplexity is 350.7829867356925
At time: 13.91281795501709 and batch: 550, loss is 5.859341802597046 and perplexity is 350.4933742535281
At time: 15.125807285308838 and batch: 600, loss is 5.878779516220093 and perplexity is 357.3728077175252
At time: 16.335654497146606 and batch: 650, loss is 5.846916828155518 and perplexity is 346.1654459146517
At time: 17.54539680480957 and batch: 700, loss is 5.869187479019165 and perplexity is 353.96126244307203
At time: 18.758745431900024 and batch: 750, loss is 5.826689825057984 and perplexity is 339.2338949441505
At time: 19.97012734413147 and batch: 800, loss is 5.821499042510986 and perplexity is 337.47756786247623
At time: 21.182610034942627 and batch: 850, loss is 5.866529216766358 and perplexity is 353.02159008128666
At time: 22.39539074897766 and batch: 900, loss is 5.841557292938233 and perplexity is 344.3151228811454
At time: 23.6055645942688 and batch: 950, loss is 5.813620986938477 and perplexity is 334.8293459446355
At time: 24.815682649612427 and batch: 1000, loss is 5.807719831466675 and perplexity is 332.85928445432137
At time: 26.02642059326172 and batch: 1050, loss is 5.795558023452759 and perplexity is 328.8356307874633
At time: 27.23701524734497 and batch: 1100, loss is 5.784161386489868 and perplexity is 325.10928472200203
At time: 28.449788570404053 and batch: 1150, loss is 5.819402475357055 and perplexity is 336.77076466759024
At time: 29.660363912582397 and batch: 1200, loss is 5.809867057800293 and perplexity is 333.5747765621359
At time: 30.868204593658447 and batch: 1250, loss is 5.79114296913147 and perplexity is 327.38700385120615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.285065365533759 and perplexity of 197.36708403132008
Finished 1 epochs...
Completing Train Step...
At time: 33.95664882659912 and batch: 50, loss is 5.559685468673706 and perplexity is 259.7411267524241
At time: 35.1576886177063 and batch: 100, loss is 5.599428062438965 and perplexity is 270.2717846277578
At time: 36.35061836242676 and batch: 150, loss is 5.499264726638794 and perplexity is 244.51208293203854
At time: 37.54511594772339 and batch: 200, loss is 5.537817640304565 and perplexity is 254.1228064929492
At time: 38.76668667793274 and batch: 250, loss is 5.536975555419922 and perplexity is 253.9089035935974
At time: 39.97093319892883 and batch: 300, loss is 5.515910234451294 and perplexity is 248.6161732922203
At time: 41.17840027809143 and batch: 350, loss is 5.5465211582183835 and perplexity is 256.34422193321564
At time: 42.38777470588684 and batch: 400, loss is 5.518395013809204 and perplexity is 249.23469775783437
At time: 43.59553790092468 and batch: 450, loss is 5.522824420928955 and perplexity is 250.34110826491238
At time: 44.804821252822876 and batch: 500, loss is 5.506788539886474 and perplexity is 246.35868421141558
At time: 46.013527393341064 and batch: 550, loss is 5.4833802318573 and perplexity is 240.6588166409366
At time: 47.221745014190674 and batch: 600, loss is 5.521058015823364 and perplexity is 249.89929477881114
At time: 48.43107318878174 and batch: 650, loss is 5.527758941650391 and perplexity is 251.57947451022363
At time: 49.63689827919006 and batch: 700, loss is 5.552399463653565 and perplexity is 257.8555291772098
At time: 50.83957886695862 and batch: 750, loss is 5.5062230110168455 and perplexity is 246.21940065135954
At time: 52.04873514175415 and batch: 800, loss is 5.542952146530151 and perplexity is 255.43095710476734
At time: 53.256396770477295 and batch: 850, loss is 5.5597998237609865 and perplexity is 259.77083117003747
At time: 54.4687397480011 and batch: 900, loss is 5.522835559844971 and perplexity is 250.3438968090234
At time: 55.679670572280884 and batch: 950, loss is 5.530119514465332 and perplexity is 252.17404766907202
At time: 56.88427114486694 and batch: 1000, loss is 5.534067211151123 and perplexity is 253.17152188872137
At time: 58.091121435165405 and batch: 1050, loss is 5.537729244232178 and perplexity is 254.10034402776267
At time: 59.29909014701843 and batch: 1100, loss is 5.5091140174865725 and perplexity is 246.93225246472073
At time: 60.503703355789185 and batch: 1150, loss is 5.552993135452271 and perplexity is 258.0086561821175
At time: 61.70832967758179 and batch: 1200, loss is 5.516180696487427 and perplexity is 248.68342362258528
At time: 62.913379430770874 and batch: 1250, loss is 5.504203033447266 and perplexity is 245.7225449724555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.244118794907618 and perplexity of 189.44879840638853
Finished 2 epochs...
Completing Train Step...
At time: 66.01583552360535 and batch: 50, loss is 5.471315946578979 and perplexity is 237.77288338642964
At time: 67.22160792350769 and batch: 100, loss is 5.50200740814209 and perplexity is 245.1836221874158
At time: 68.42541313171387 and batch: 150, loss is 5.410737476348877 and perplexity is 223.7965715156829
At time: 69.63075661659241 and batch: 200, loss is 5.465579700469971 and perplexity is 236.4128640413446
At time: 70.83936882019043 and batch: 250, loss is 5.478135719299316 and perplexity is 239.39998232640178
At time: 72.04788756370544 and batch: 300, loss is 5.44974494934082 and perplexity is 232.69880835393118
At time: 73.25788736343384 and batch: 350, loss is 5.473511838912964 and perplexity is 238.29558072178853
At time: 74.4697904586792 and batch: 400, loss is 5.464973363876343 and perplexity is 236.26956171978304
At time: 75.68043994903564 and batch: 450, loss is 5.460235576629639 and perplexity is 235.15281434814395
At time: 76.8898401260376 and batch: 500, loss is 5.491877059936524 and perplexity is 242.71236520138663
At time: 78.0997724533081 and batch: 550, loss is 5.477525835037231 and perplexity is 239.25402055925417
At time: 79.30871558189392 and batch: 600, loss is 5.481305541992188 and perplexity is 240.1600418135494
At time: 80.51806497573853 and batch: 650, loss is 5.4737889766693115 and perplexity is 238.36163057640874
At time: 81.72754907608032 and batch: 700, loss is 5.475992412567138 and perplexity is 238.8874242134397
At time: 82.93685960769653 and batch: 750, loss is 5.443435258865357 and perplexity is 231.23517329717097
At time: 84.14673089981079 and batch: 800, loss is 5.47638352394104 and perplexity is 238.9808740755924
At time: 85.35804295539856 and batch: 850, loss is 5.505421438217163 and perplexity is 246.02211695622844
At time: 86.5689446926117 and batch: 900, loss is 5.453911933898926 and perplexity is 233.67048376725853
At time: 87.78017592430115 and batch: 950, loss is 5.455491762161255 and perplexity is 234.0399347593042
At time: 88.98998427391052 and batch: 1000, loss is 5.461062383651734 and perplexity is 235.34732074485905
At time: 90.19955468177795 and batch: 1050, loss is 5.460751361846924 and perplexity is 235.274133978334
At time: 91.40913677215576 and batch: 1100, loss is 5.451924257278442 and perplexity is 233.206483703583
At time: 92.62185502052307 and batch: 1150, loss is 5.495344133377075 and perplexity is 243.55532725773932
At time: 93.83539891242981 and batch: 1200, loss is 5.491929769515991 and perplexity is 242.72515880525768
At time: 95.04671907424927 and batch: 1250, loss is 5.488639793395996 and perplexity is 241.92791101128188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3086873075387775 and perplexity of 202.08477903361674
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 98.16315245628357 and batch: 50, loss is 5.416987953186035 and perplexity is 225.19978761859662
At time: 99.40359163284302 and batch: 100, loss is 5.390731582641601 and perplexity is 219.3638096203821
At time: 100.61903238296509 and batch: 150, loss is 5.278419132232666 and perplexity is 196.05968579431658
At time: 101.83407354354858 and batch: 200, loss is 5.323073663711548 and perplexity is 205.01305575321427
At time: 103.04643082618713 and batch: 250, loss is 5.354486236572265 and perplexity is 211.5552590727477
At time: 104.26216745376587 and batch: 300, loss is 5.329337520599365 and perplexity is 206.3012585415603
At time: 105.47625732421875 and batch: 350, loss is 5.34072901725769 and perplexity is 208.6647750755368
At time: 106.69097471237183 and batch: 400, loss is 5.295564308166504 and perplexity is 199.4501455616724
At time: 107.90526413917542 and batch: 450, loss is 5.272927026748658 and perplexity is 194.98585680897946
At time: 109.12012147903442 and batch: 500, loss is 5.269663391113281 and perplexity is 194.35053131768098
At time: 110.33896994590759 and batch: 550, loss is 5.253092527389526 and perplexity is 191.15651206520747
At time: 111.55511212348938 and batch: 600, loss is 5.259946327209473 and perplexity is 192.47116055572528
At time: 112.76924228668213 and batch: 650, loss is 5.25487021446228 and perplexity is 191.49663074830215
At time: 113.9777512550354 and batch: 700, loss is 5.2653117275238035 and perplexity is 193.50662072620622
At time: 115.1872022151947 and batch: 750, loss is 5.223951711654663 and perplexity is 185.66643651357168
At time: 116.3975145816803 and batch: 800, loss is 5.27832688331604 and perplexity is 196.0416003349028
At time: 117.60726165771484 and batch: 850, loss is 5.294930038452148 and perplexity is 199.3236804855424
At time: 118.8041000366211 and batch: 900, loss is 5.238808164596557 and perplexity is 188.44537264509202
At time: 120.00181794166565 and batch: 950, loss is 5.208061637878418 and perplexity is 182.73949934935948
At time: 121.1980938911438 and batch: 1000, loss is 5.193102579116822 and perplexity is 180.02623299218732
At time: 122.39707684516907 and batch: 1050, loss is 5.182026891708374 and perplexity is 178.04332004266595
At time: 123.60852718353271 and batch: 1100, loss is 5.138203535079956 and perplexity is 170.40935874167172
At time: 124.8223614692688 and batch: 1150, loss is 5.184212245941162 and perplexity is 178.43283322286047
At time: 126.03604483604431 and batch: 1200, loss is 5.1952815341949465 and perplexity is 180.418929745682
At time: 127.30051183700562 and batch: 1250, loss is 5.223196029663086 and perplexity is 185.52618473060576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.120298065408303 and perplexity of 167.38525394039937
Finished 4 epochs...
Completing Train Step...
At time: 130.38814210891724 and batch: 50, loss is 5.247650518417358 and perplexity is 190.11906207790852
At time: 131.59847235679626 and batch: 100, loss is 5.246617984771729 and perplexity is 189.9228590601776
At time: 132.80873656272888 and batch: 150, loss is 5.1480014038085935 and perplexity is 172.0872135467471
At time: 134.01973915100098 and batch: 200, loss is 5.193194932937622 and perplexity is 180.04285987041456
At time: 135.23107194900513 and batch: 250, loss is 5.2187389087677 and perplexity is 184.7011121859434
At time: 136.44197177886963 and batch: 300, loss is 5.201822624206543 and perplexity is 181.60293432365242
At time: 137.65235376358032 and batch: 350, loss is 5.220568952560424 and perplexity is 185.03943278614437
At time: 138.8608913421631 and batch: 400, loss is 5.18212721824646 and perplexity is 178.06118340866504
At time: 140.07459831237793 and batch: 450, loss is 5.160584926605225 and perplexity is 174.26635882892663
At time: 141.28593468666077 and batch: 500, loss is 5.175949487686157 and perplexity is 176.96456020331956
At time: 142.49761271476746 and batch: 550, loss is 5.155990839004517 and perplexity is 173.46760009925717
At time: 143.70626950263977 and batch: 600, loss is 5.158268852233887 and perplexity is 173.86321202066517
At time: 144.91338467597961 and batch: 650, loss is 5.161458978652954 and perplexity is 174.41874328298204
At time: 146.122873544693 and batch: 700, loss is 5.176595735549927 and perplexity is 177.07896013368767
At time: 147.3318226337433 and batch: 750, loss is 5.132745332717896 and perplexity is 169.48176378068015
At time: 148.5400996208191 and batch: 800, loss is 5.189545783996582 and perplexity is 179.38705395345184
At time: 149.74820971488953 and batch: 850, loss is 5.210200643539428 and perplexity is 183.1307985193337
At time: 150.95781183242798 and batch: 900, loss is 5.1561758327484135 and perplexity is 173.49969348849092
At time: 152.16684079170227 and batch: 950, loss is 5.132371444702148 and perplexity is 169.41840842495694
At time: 153.3747022151947 and batch: 1000, loss is 5.12538106918335 and perplexity is 168.238239846813
At time: 154.58190965652466 and batch: 1050, loss is 5.119121236801147 and perplexity is 167.1883860477509
At time: 155.79750514030457 and batch: 1100, loss is 5.074965991973877 and perplexity is 159.96675173340924
At time: 157.0071141719818 and batch: 1150, loss is 5.1218602752685545 and perplexity is 167.64694919279538
At time: 158.26733541488647 and batch: 1200, loss is 5.138352699279785 and perplexity is 170.4347796132067
At time: 159.47654819488525 and batch: 1250, loss is 5.1688854694366455 and perplexity is 175.71888423936596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.094283277971031 and perplexity of 163.08691474184963
Finished 5 epochs...
Completing Train Step...
At time: 162.58479261398315 and batch: 50, loss is 5.161987295150757 and perplexity is 174.51091592860527
At time: 163.79678058624268 and batch: 100, loss is 5.169950838088989 and perplexity is 175.9061893870597
At time: 165.00780272483826 and batch: 150, loss is 5.077058181762696 and perplexity is 160.30178289010905
At time: 166.2181293964386 and batch: 200, loss is 5.124403371810913 and perplexity is 168.07383414437916
At time: 167.42887163162231 and batch: 250, loss is 5.147875156402588 and perplexity is 172.06548935377043
At time: 168.64361023902893 and batch: 300, loss is 5.130230255126953 and perplexity is 169.05603958362988
At time: 169.85446214675903 and batch: 350, loss is 5.152429885864258 and perplexity is 172.85098861781876
At time: 171.0656304359436 and batch: 400, loss is 5.113249425888061 and perplexity is 166.209563999338
At time: 172.27770447731018 and batch: 450, loss is 5.092051696777344 and perplexity is 162.72337883079246
At time: 173.48920392990112 and batch: 500, loss is 5.108956508636474 and perplexity is 165.4975694549513
At time: 174.70013070106506 and batch: 550, loss is 5.08870756149292 and perplexity is 162.18011871199363
At time: 175.91225385665894 and batch: 600, loss is 5.098230066299439 and perplexity is 163.73185615990454
At time: 177.12304830551147 and batch: 650, loss is 5.105283584594726 and perplexity is 164.89082439960103
At time: 178.33384728431702 and batch: 700, loss is 5.119240875244141 and perplexity is 167.20838940250547
At time: 179.54413175582886 and batch: 750, loss is 5.077041330337525 and perplexity is 160.2990815993703
At time: 180.75152564048767 and batch: 800, loss is 5.132414989471435 and perplexity is 169.42578587108824
At time: 181.96006631851196 and batch: 850, loss is 5.154964952468872 and perplexity is 173.28973327513438
At time: 183.16833448410034 and batch: 900, loss is 5.104107141494751 and perplexity is 164.69695378821842
At time: 184.37555956840515 and batch: 950, loss is 5.0840543937683105 and perplexity is 161.4272204574357
At time: 185.58509016036987 and batch: 1000, loss is 5.079005365371704 and perplexity is 160.61422398561848
At time: 186.79182124137878 and batch: 1050, loss is 5.079198122024536 and perplexity is 160.64518642984387
At time: 188.04690885543823 and batch: 1100, loss is 5.032904748916626 and perplexity is 153.37789051493337
At time: 189.25418758392334 and batch: 1150, loss is 5.077744331359863 and perplexity is 160.41181163763792
At time: 190.47043824195862 and batch: 1200, loss is 5.097958059310913 and perplexity is 163.68732600731562
At time: 191.68024039268494 and batch: 1250, loss is 5.132313995361328 and perplexity is 169.40867572864138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.087725340014827 and perplexity of 162.02090012283463
Finished 6 epochs...
Completing Train Step...
At time: 194.69477725028992 and batch: 50, loss is 5.115464591979981 and perplexity is 166.5781538828466
At time: 195.92572379112244 and batch: 100, loss is 5.11871072769165 and perplexity is 167.1197677774523
At time: 197.1243381500244 and batch: 150, loss is 5.0296686744689945 and perplexity is 152.88235047728173
At time: 198.3336660861969 and batch: 200, loss is 5.0742983150482175 and perplexity is 159.85998127245716
At time: 199.54407334327698 and batch: 250, loss is 5.094707765579224 and perplexity is 163.1561578115887
At time: 200.75854015350342 and batch: 300, loss is 5.082514591217041 and perplexity is 161.17884568466
At time: 201.97206091880798 and batch: 350, loss is 5.104228439331055 and perplexity is 164.71693238401454
At time: 203.18516302108765 and batch: 400, loss is 5.071586036682129 and perplexity is 159.42698397369531
At time: 204.39661073684692 and batch: 450, loss is 5.057594051361084 and perplexity is 157.2118173828906
At time: 205.60802960395813 and batch: 500, loss is 5.089165496826172 and perplexity is 162.25440372627105
At time: 206.81868886947632 and batch: 550, loss is 5.051537075042725 and perplexity is 156.26246712644303
At time: 208.02822732925415 and batch: 600, loss is 5.054736042022705 and perplexity is 156.76314599961216
At time: 209.23292636871338 and batch: 650, loss is 5.063423957824707 and perplexity is 158.13102441523395
At time: 210.4443175792694 and batch: 700, loss is 5.072794332504272 and perplexity is 159.61973535926344
At time: 211.6571159362793 and batch: 750, loss is 5.040003938674927 and perplexity is 154.47062343426413
At time: 212.87074065208435 and batch: 800, loss is 5.091541023254394 and perplexity is 162.64030152415455
At time: 214.08147192001343 and batch: 850, loss is 5.114925451278687 and perplexity is 166.48836902565222
At time: 215.29137420654297 and batch: 900, loss is 5.066434602737427 and perplexity is 158.60781814760787
At time: 216.5014021396637 and batch: 950, loss is 5.053667144775391 and perplexity is 156.59567182665953
At time: 217.7117886543274 and batch: 1000, loss is 5.046659708023071 and perplexity is 155.5021733459759
At time: 218.96609449386597 and batch: 1050, loss is 5.046918458938599 and perplexity is 155.54241488173832
At time: 220.17598056793213 and batch: 1100, loss is 4.996375713348389 and perplexity is 147.87624083170073
At time: 221.38635730743408 and batch: 1150, loss is 5.045091123580932 and perplexity is 155.25844625937805
At time: 222.59562754631042 and batch: 1200, loss is 5.063442544937134 and perplexity is 158.13396364167875
At time: 223.8042333126068 and batch: 1250, loss is 5.100938482284546 and perplexity is 164.17591120784655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.091145090813185 and perplexity of 162.5759196988007
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 226.90703916549683 and batch: 50, loss is 5.071058731079102 and perplexity is 159.34293939230898
At time: 228.11748385429382 and batch: 100, loss is 5.061437168121338 and perplexity is 157.81716321548546
At time: 229.33054113388062 and batch: 150, loss is 4.9673747539520265 and perplexity is 143.64927723068277
At time: 230.53968262672424 and batch: 200, loss is 5.010625543594361 and perplexity is 149.99853743184616
At time: 231.74969220161438 and batch: 250, loss is 5.023189477920532 and perplexity is 151.89499775999298
At time: 232.96088981628418 and batch: 300, loss is 5.004092359542847 and perplexity is 149.02176357529635
At time: 234.17181634902954 and batch: 350, loss is 5.014646816253662 and perplexity is 150.60293686234866
At time: 235.38290667533875 and batch: 400, loss is 4.982212829589844 and perplexity is 145.7966480987589
At time: 236.59411668777466 and batch: 450, loss is 4.960230741500855 and perplexity is 142.62670199972268
At time: 237.8055396080017 and batch: 500, loss is 4.981124906539917 and perplexity is 145.63811881415762
At time: 239.0164988040924 and batch: 550, loss is 4.951153411865234 and perplexity is 141.3378907427397
At time: 240.22644233703613 and batch: 600, loss is 4.954673709869385 and perplexity is 141.83631903075516
At time: 241.43623089790344 and batch: 650, loss is 4.9597811794281 and perplexity is 142.5625968546422
At time: 242.64677667617798 and batch: 700, loss is 4.965310039520264 and perplexity is 143.35298847596394
At time: 243.85667371749878 and batch: 750, loss is 4.929186239242553 and perplexity is 138.2669503286692
At time: 245.06729531288147 and batch: 800, loss is 4.9665399932861325 and perplexity is 143.52941449967335
At time: 246.2781138420105 and batch: 850, loss is 4.9873723220825195 and perplexity is 146.55082873169252
At time: 247.48889183998108 and batch: 900, loss is 4.944055604934692 and perplexity is 140.33825349508209
At time: 248.6981770992279 and batch: 950, loss is 4.917804155349732 and perplexity is 136.7021067846649
At time: 249.93296003341675 and batch: 1000, loss is 4.908435258865357 and perplexity is 135.42733980146247
At time: 251.14039731025696 and batch: 1050, loss is 4.902209539413452 and perplexity is 134.58682629321737
At time: 252.34873414039612 and batch: 1100, loss is 4.847168397903443 and perplexity is 127.37919151663893
At time: 253.55484247207642 and batch: 1150, loss is 4.896299962997436 and perplexity is 133.79382063027992
At time: 254.76494550704956 and batch: 1200, loss is 4.915776834487915 and perplexity is 136.4252484868522
At time: 255.96905303001404 and batch: 1250, loss is 4.979317502975464 and perplexity is 145.37512969442122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.985241075501825 and perplexity of 146.23882537520223
Finished 8 epochs...
Completing Train Step...
At time: 258.95757937431335 and batch: 50, loss is 4.996527614593506 and perplexity is 147.89870512294004
At time: 260.18561148643494 and batch: 100, loss is 4.999179039001465 and perplexity is 148.29136768712542
At time: 261.3919765949249 and batch: 150, loss is 4.913944797515869 and perplexity is 136.17554119404232
At time: 262.60136580467224 and batch: 200, loss is 4.9581796741485595 and perplexity is 142.3344648291958
At time: 263.80775713920593 and batch: 250, loss is 4.9770114135742185 and perplexity is 145.04026790763785
At time: 265.01364755630493 and batch: 300, loss is 4.9610778427124025 and perplexity is 142.74757243930577
At time: 266.2127661705017 and batch: 350, loss is 4.97390811920166 and perplexity is 144.59086293906444
At time: 267.4162950515747 and batch: 400, loss is 4.944301795959473 and perplexity is 140.3728077668272
At time: 268.61659812927246 and batch: 450, loss is 4.921344623565674 and perplexity is 137.18695403546909
At time: 269.8152165412903 and batch: 500, loss is 4.946714448928833 and perplexity is 140.71188751470459
At time: 271.01150369644165 and batch: 550, loss is 4.921036586761475 and perplexity is 137.14470191250874
At time: 272.2082870006561 and batch: 600, loss is 4.9290571212768555 and perplexity is 138.2490987338251
At time: 273.4050736427307 and batch: 650, loss is 4.937249383926392 and perplexity is 139.38632352357615
At time: 274.60856223106384 and batch: 700, loss is 4.94032057762146 and perplexity is 139.81506395702036
At time: 275.81608176231384 and batch: 750, loss is 4.905677728652954 and perplexity is 135.05440924033212
At time: 277.02564001083374 and batch: 800, loss is 4.951645364761353 and perplexity is 141.4074394333591
At time: 278.2353186607361 and batch: 850, loss is 4.972777509689331 and perplexity is 144.42747951286177
At time: 279.436399936676 and batch: 900, loss is 4.929243021011352 and perplexity is 138.2748015935775
At time: 280.68304562568665 and batch: 950, loss is 4.90567310333252 and perplexity is 135.053784571858
At time: 281.883975982666 and batch: 1000, loss is 4.896912879943848 and perplexity is 133.8758502663784
At time: 283.0912172794342 and batch: 1050, loss is 4.896405410766602 and perplexity is 133.8079296340619
At time: 284.2987184524536 and batch: 1100, loss is 4.8449381065368655 and perplexity is 127.09541537488721
At time: 285.50646567344666 and batch: 1150, loss is 4.897051782608032 and perplexity is 133.8944472702071
At time: 286.71478033065796 and batch: 1200, loss is 4.916294021606445 and perplexity is 136.49582411684207
At time: 287.92257261276245 and batch: 1250, loss is 4.973401203155517 and perplexity is 144.51758608469078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.98335299526688 and perplexity of 145.96297523497546
Finished 9 epochs...
Completing Train Step...
At time: 291.02092695236206 and batch: 50, loss is 4.972265071868897 and perplexity is 144.35348836959855
At time: 292.2322654724121 and batch: 100, loss is 4.976265001296997 and perplexity is 144.93204846416506
At time: 293.44519329071045 and batch: 150, loss is 4.8923111915588375 and perplexity is 133.2612105959005
At time: 294.65625834465027 and batch: 200, loss is 4.9372560501098635 and perplexity is 139.38725270147927
At time: 295.86880588531494 and batch: 250, loss is 4.955881767272949 and perplexity is 142.00776898589712
At time: 297.0805974006653 and batch: 300, loss is 4.939871597290039 and perplexity is 139.75230383335213
At time: 298.2932426929474 and batch: 350, loss is 4.953352212905884 and perplexity is 141.64900655953227
At time: 299.505047082901 and batch: 400, loss is 4.925651502609253 and perplexity is 137.77907583606176
At time: 300.7192099094391 and batch: 450, loss is 4.900367164611817 and perplexity is 134.33909519266507
At time: 301.93080472946167 and batch: 500, loss is 4.9288529014587406 and perplexity is 138.22086841072095
At time: 303.141761302948 and batch: 550, loss is 4.905055322647095 and perplexity is 134.97037671878903
At time: 304.35235619544983 and batch: 600, loss is 4.915170593261719 and perplexity is 136.34256694193826
At time: 305.5627775192261 and batch: 650, loss is 4.924811067581177 and perplexity is 137.66333011978327
At time: 306.77327942848206 and batch: 700, loss is 4.927021484375 and perplexity is 137.9679600120549
At time: 307.9844570159912 and batch: 750, loss is 4.893254737854004 and perplexity is 133.3870080559785
At time: 309.1965174674988 and batch: 800, loss is 4.9401490592956545 and perplexity is 139.79108516778774
At time: 310.43641781806946 and batch: 850, loss is 4.963219938278198 and perplexity is 143.05367911908738
At time: 311.64519715309143 and batch: 900, loss is 4.919217748641968 and perplexity is 136.89548461244897
At time: 312.85361433029175 and batch: 950, loss is 4.896359977722168 and perplexity is 133.8018504705477
At time: 314.06241369247437 and batch: 1000, loss is 4.889468059539795 and perplexity is 132.8828694728963
At time: 315.2708640098572 and batch: 1050, loss is 4.8899571895599365 and perplexity is 132.94788237210216
At time: 316.48069977760315 and batch: 1100, loss is 4.840121812820435 and perplexity is 126.48475825688479
At time: 317.68919134140015 and batch: 1150, loss is 4.891726799011231 and perplexity is 133.183356488439
At time: 318.8986120223999 and batch: 1200, loss is 4.910328645706176 and perplexity is 135.6839990454723
At time: 320.10807704925537 and batch: 1250, loss is 4.964284811019898 and perplexity is 143.20609421952145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.982498948591469 and perplexity of 145.83836925847692
Finished 10 epochs...
Completing Train Step...
At time: 323.17934489250183 and batch: 50, loss is 4.954490928649903 and perplexity is 141.81039638455442
At time: 324.4449143409729 and batch: 100, loss is 4.958106155395508 and perplexity is 142.32400096147526
At time: 325.65464639663696 and batch: 150, loss is 4.875525035858154 and perplexity is 131.04293739324672
At time: 326.8605914115906 and batch: 200, loss is 4.92067554473877 and perplexity is 137.0951958493493
At time: 328.05848240852356 and batch: 250, loss is 4.94001971244812 and perplexity is 139.77300480095238
At time: 329.2685053348541 and batch: 300, loss is 4.9238831996917725 and perplexity is 137.53565597774494
At time: 330.48105812072754 and batch: 350, loss is 4.936436614990234 and perplexity is 139.27308067605935
At time: 331.6924593448639 and batch: 400, loss is 4.911415128707886 and perplexity is 135.83149751682063
At time: 332.90735816955566 and batch: 450, loss is 4.88437665939331 and perplexity is 132.20802901099714
At time: 334.11970829963684 and batch: 500, loss is 4.913644428253174 and perplexity is 136.13464438951573
At time: 335.3321328163147 and batch: 550, loss is 4.892566175460815 and perplexity is 133.29519439183684
At time: 336.54390001296997 and batch: 600, loss is 4.9034281349182125 and perplexity is 134.75093316437815
At time: 337.7547183036804 and batch: 650, loss is 4.913958873748779 and perplexity is 136.1774580461678
At time: 338.9645917415619 and batch: 700, loss is 4.916160831451416 and perplexity is 136.47764542750517
At time: 340.17500615119934 and batch: 750, loss is 4.882743854522705 and perplexity is 131.99233523813714
At time: 341.413302898407 and batch: 800, loss is 4.9306795692443846 and perplexity is 138.4735827606619
At time: 342.618976354599 and batch: 850, loss is 4.953489799499511 and perplexity is 141.66849690460987
At time: 343.82489347457886 and batch: 900, loss is 4.9092052555084225 and perplexity is 135.53165855588014
At time: 345.03182077407837 and batch: 950, loss is 4.886670570373536 and perplexity is 132.51165056769204
At time: 346.2412462234497 and batch: 1000, loss is 4.880889902114868 and perplexity is 131.74785442832564
At time: 347.44906735420227 and batch: 1050, loss is 4.882630920410156 and perplexity is 131.97742964258475
At time: 348.65581130981445 and batch: 1100, loss is 4.8342546939849855 and perplexity is 125.74482989568111
At time: 349.8587589263916 and batch: 1150, loss is 4.88468186378479 and perplexity is 132.24838564023713
At time: 351.057888507843 and batch: 1200, loss is 4.901991243362427 and perplexity is 134.55744972702846
At time: 352.2615032196045 and batch: 1250, loss is 4.955006017684936 and perplexity is 141.88346018035048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.982593842666515 and perplexity of 145.85220911228356
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 355.3071641921997 and batch: 50, loss is 4.943882026672363 and perplexity is 140.31389593893442
At time: 356.54575657844543 and batch: 100, loss is 4.953285589218139 and perplexity is 141.6395696947128
At time: 357.75522780418396 and batch: 150, loss is 4.866746234893799 and perplexity is 129.89757234980507
At time: 358.9648656845093 and batch: 200, loss is 4.9119900035858155 and perplexity is 135.9096060815493
At time: 360.17551922798157 and batch: 250, loss is 4.930010032653809 and perplexity is 138.38090066066644
At time: 361.38089871406555 and batch: 300, loss is 4.907783555984497 and perplexity is 135.33911016684294
At time: 362.58934140205383 and batch: 350, loss is 4.918348398208618 and perplexity is 136.77652617935232
At time: 363.7989172935486 and batch: 400, loss is 4.886739616394043 and perplexity is 132.52080028570677
At time: 365.00869369506836 and batch: 450, loss is 4.858920269012451 and perplexity is 128.88496586059122
At time: 366.2186884880066 and batch: 500, loss is 4.881972179412842 and perplexity is 131.89051932779563
At time: 367.4281840324402 and batch: 550, loss is 4.862941579818726 and perplexity is 129.40429586040523
At time: 368.63731932640076 and batch: 600, loss is 4.8739689922332765 and perplexity is 130.83918742895835
At time: 369.8457751274109 and batch: 650, loss is 4.883971481323242 and perplexity is 132.15447206773715
At time: 371.05566477775574 and batch: 700, loss is 4.881119232177735 and perplexity is 131.778071636789
At time: 372.29172682762146 and batch: 750, loss is 4.8454375648498536 and perplexity is 127.15891009180103
At time: 373.50096368789673 and batch: 800, loss is 4.8856260776519775 and perplexity is 132.37331537078217
At time: 374.71056723594666 and batch: 850, loss is 4.907419700622558 and perplexity is 135.28987526366367
At time: 375.9204022884369 and batch: 900, loss is 4.860836086273193 and perplexity is 129.13212258079633
At time: 377.1306700706482 and batch: 950, loss is 4.833975419998169 and perplexity is 125.70971753892675
At time: 378.34026169776917 and batch: 1000, loss is 4.828206510543823 and perplexity is 124.98659737376316
At time: 379.54977083206177 and batch: 1050, loss is 4.824957752227784 and perplexity is 124.58120499364185
At time: 380.7588346004486 and batch: 1100, loss is 4.770554609298706 and perplexity is 117.98465920758836
At time: 381.9713912010193 and batch: 1150, loss is 4.817838525772094 and perplexity is 123.69743280306018
At time: 383.18203139305115 and batch: 1200, loss is 4.842457046508789 and perplexity is 126.7804748747474
At time: 384.3923370838165 and batch: 1250, loss is 4.908433971405029 and perplexity is 135.42716544424744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.955023772525092 and perplexity of 141.8859793208701
Finished 12 epochs...
Completing Train Step...
At time: 387.47936511039734 and batch: 50, loss is 4.918754215240479 and perplexity is 136.83204368744788
At time: 388.6916103363037 and batch: 100, loss is 4.928861179351807 and perplexity is 138.22201259302489
At time: 389.9049961566925 and batch: 150, loss is 4.843346996307373 and perplexity is 126.89335335346615
At time: 391.11745524406433 and batch: 200, loss is 4.888142385482788 and perplexity is 132.7068268135902
At time: 392.33124113082886 and batch: 250, loss is 4.908945369720459 and perplexity is 135.49644038055217
At time: 393.5451238155365 and batch: 300, loss is 4.888754825592041 and perplexity is 132.78812669021406
At time: 394.75681257247925 and batch: 350, loss is 4.900843238830566 and perplexity is 134.4030657986263
At time: 395.9715220928192 and batch: 400, loss is 4.871249237060547 and perplexity is 130.48382034693864
At time: 397.18487215042114 and batch: 450, loss is 4.843791618347168 and perplexity is 126.94978547962441
At time: 398.3979513645172 and batch: 500, loss is 4.867556686401367 and perplexity is 130.0028907050907
At time: 399.61069655418396 and batch: 550, loss is 4.849315977096557 and perplexity is 127.65304237092197
At time: 400.82380080223083 and batch: 600, loss is 4.861564340591431 and perplexity is 129.22619785789468
At time: 402.0375437736511 and batch: 650, loss is 4.873635997772217 and perplexity is 130.79562595752185
At time: 403.2810046672821 and batch: 700, loss is 4.871414966583252 and perplexity is 130.50544716025675
At time: 404.49562549591064 and batch: 750, loss is 4.837637195587158 and perplexity is 126.1708821392499
At time: 405.7109754085541 and batch: 800, loss is 4.880219469070434 and perplexity is 131.6595559156114
At time: 406.91604375839233 and batch: 850, loss is 4.903334646224976 and perplexity is 134.73833606457657
At time: 408.1279368400574 and batch: 900, loss is 4.857107753753662 and perplexity is 128.65157147268096
At time: 409.3435866832733 and batch: 950, loss is 4.831800746917724 and perplexity is 125.4366370382724
At time: 410.55489468574524 and batch: 1000, loss is 4.82806586265564 and perplexity is 124.9690195089651
At time: 411.767205953598 and batch: 1050, loss is 4.826955251693725 and perplexity is 124.83030458941208
At time: 412.9781560897827 and batch: 1100, loss is 4.774500970840454 and perplexity is 118.45118927188415
At time: 414.1919777393341 and batch: 1150, loss is 4.823472013473511 and perplexity is 124.3962473027567
At time: 415.39938855171204 and batch: 1200, loss is 4.847835073471069 and perplexity is 127.4641404249816
At time: 416.5960147380829 and batch: 1250, loss is 4.909332809448242 and perplexity is 135.5489472554966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.954057456803148 and perplexity of 141.7489388911535
Finished 13 epochs...
Completing Train Step...
At time: 419.61363315582275 and batch: 50, loss is 4.909488019943237 and perplexity is 135.56998750748792
At time: 420.8533055782318 and batch: 100, loss is 4.919230842590332 and perplexity is 136.89727712659138
At time: 422.0577208995819 and batch: 150, loss is 4.834028720855713 and perplexity is 125.7164181532458
At time: 423.2618842124939 and batch: 200, loss is 4.878875026702881 and perplexity is 131.48266616622627
At time: 424.46407294273376 and batch: 250, loss is 4.899872875213623 and perplexity is 134.2727092104436
At time: 425.67139053344727 and batch: 300, loss is 4.880292682647705 and perplexity is 131.66919553555311
At time: 426.88086438179016 and batch: 350, loss is 4.892953014373779 and perplexity is 133.34676813466083
At time: 428.09097170829773 and batch: 400, loss is 4.863502292633057 and perplexity is 129.4768748534078
At time: 429.3002653121948 and batch: 450, loss is 4.8359479904174805 and perplexity is 125.95793354045037
At time: 430.5100727081299 and batch: 500, loss is 4.859963254928589 and perplexity is 129.0194611909612
At time: 431.72031688690186 and batch: 550, loss is 4.84328950881958 and perplexity is 126.88605878303947
At time: 432.97827315330505 and batch: 600, loss is 4.855737199783325 and perplexity is 128.47536832615197
At time: 434.1875035762787 and batch: 650, loss is 4.868573246002197 and perplexity is 130.13511358660585
At time: 435.3967795372009 and batch: 700, loss is 4.86690245628357 and perplexity is 129.9178667142511
At time: 436.6058304309845 and batch: 750, loss is 4.833359289169311 and perplexity is 125.63228776234968
At time: 437.8143904209137 and batch: 800, loss is 4.876950397491455 and perplexity is 131.2298541488622
At time: 439.0240669250488 and batch: 850, loss is 4.900581979751587 and perplexity is 134.3679563639728
At time: 440.23290753364563 and batch: 900, loss is 4.854665126800537 and perplexity is 128.3377071594095
At time: 441.4428780078888 and batch: 950, loss is 4.829969663619995 and perplexity is 125.20716226509694
At time: 442.65277099609375 and batch: 1000, loss is 4.827062358856201 and perplexity is 124.8436755251764
At time: 443.8627812862396 and batch: 1050, loss is 4.82689190864563 and perplexity is 124.82239770785031
At time: 445.0720407962799 and batch: 1100, loss is 4.7750858974456785 and perplexity is 118.52049479125482
At time: 446.2816081047058 and batch: 1150, loss is 4.8248631954193115 and perplexity is 124.56942554942415
At time: 447.49097180366516 and batch: 1200, loss is 4.8490525913238525 and perplexity is 127.61942480310786
At time: 448.6996467113495 and batch: 1250, loss is 4.907804880142212 and perplexity is 135.34199619014407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.953892171817975 and perplexity of 141.72551185601245
Finished 14 epochs...
Completing Train Step...
At time: 451.7866723537445 and batch: 50, loss is 4.902471971511841 and perplexity is 134.62215083140066
At time: 452.9971263408661 and batch: 100, loss is 4.9121349334716795 and perplexity is 135.92930487268382
At time: 454.207795381546 and batch: 150, loss is 4.827362089157105 and perplexity is 124.88110056603664
At time: 455.41765666007996 and batch: 200, loss is 4.872421541213989 and perplexity is 130.6368767682989
At time: 456.6288604736328 and batch: 250, loss is 4.8934702301025395 and perplexity is 133.41575501953253
At time: 457.839204788208 and batch: 300, loss is 4.874225254058838 and perplexity is 130.87272081446366
At time: 459.04954743385315 and batch: 350, loss is 4.88739896774292 and perplexity is 132.60820686678633
At time: 460.2597918510437 and batch: 400, loss is 4.857624711990357 and perplexity is 128.71809615597743
At time: 461.46990275382996 and batch: 450, loss is 4.830020446777343 and perplexity is 125.21352084157235
At time: 462.68083357810974 and batch: 500, loss is 4.854253673553467 and perplexity is 128.28491305497494
At time: 463.9417223930359 and batch: 550, loss is 4.838490533828735 and perplexity is 126.27859452896725
At time: 465.15213441848755 and batch: 600, loss is 4.851526660919189 and perplexity is 127.9355550446437
At time: 466.36203145980835 and batch: 650, loss is 4.864553785324096 and perplexity is 129.61309044327294
At time: 467.572114944458 and batch: 700, loss is 4.863122920989991 and perplexity is 129.42776431481747
At time: 468.78082489967346 and batch: 750, loss is 4.830029449462891 and perplexity is 125.21464810460094
At time: 469.9943542480469 and batch: 800, loss is 4.873496685028076 and perplexity is 130.7774057291326
At time: 471.20198583602905 and batch: 850, loss is 4.898180255889892 and perplexity is 134.04562886269136
At time: 472.41245460510254 and batch: 900, loss is 4.852062253952027 and perplexity is 128.00409478964428
At time: 473.6222457885742 and batch: 950, loss is 4.827966976165771 and perplexity is 124.95666237227056
At time: 474.83309054374695 and batch: 1000, loss is 4.825269079208374 and perplexity is 124.61999652214219
At time: 476.0417501926422 and batch: 1050, loss is 4.825476551055909 and perplexity is 124.64585434535286
At time: 477.25189089775085 and batch: 1100, loss is 4.774368400573731 and perplexity is 118.43548720696513
At time: 478.4625380039215 and batch: 1150, loss is 4.824456205368042 and perplexity is 124.5187373480297
At time: 479.6726624965668 and batch: 1200, loss is 4.84839168548584 and perplexity is 127.53510824593323
At time: 480.8861954212189 and batch: 1250, loss is 4.905701274871826 and perplexity is 135.0575892984508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.953650258753422 and perplexity of 141.69123074980496
Finished 15 epochs...
Completing Train Step...
At time: 483.94882917404175 and batch: 50, loss is 4.896433401107788 and perplexity is 133.81167501608277
At time: 485.18505597114563 and batch: 100, loss is 4.906284770965576 and perplexity is 135.136417870083
At time: 486.39415073394775 and batch: 150, loss is 4.821757192611694 and perplexity is 124.18311281870885
At time: 487.6074974536896 and batch: 200, loss is 4.86726505279541 and perplexity is 129.9649830211355
At time: 488.8187174797058 and batch: 250, loss is 4.888426303863525 and perplexity is 132.74451007020335
At time: 490.0285050868988 and batch: 300, loss is 4.869246730804443 and perplexity is 130.22278712792993
At time: 491.2337303161621 and batch: 350, loss is 4.882554759979248 and perplexity is 131.96737856742504
At time: 492.4367928504944 and batch: 400, loss is 4.8527483463287355 and perplexity is 128.0919475573779
At time: 493.64289116859436 and batch: 450, loss is 4.8249917984008786 and perplexity is 124.58544657911591
At time: 494.8837013244629 and batch: 500, loss is 4.849355964660645 and perplexity is 127.65814700719504
At time: 496.08821082115173 and batch: 550, loss is 4.834542560577392 and perplexity is 125.78103284193534
At time: 497.28691053390503 and batch: 600, loss is 4.847664060592652 and perplexity is 127.44234427919646
At time: 498.4839253425598 and batch: 650, loss is 4.861401853561401 and perplexity is 129.20520198262759
At time: 499.6872773170471 and batch: 700, loss is 4.859652996063232 and perplexity is 128.97943796842407
At time: 500.8944718837738 and batch: 750, loss is 4.826721277236938 and perplexity is 124.80110090329676
At time: 502.1040608882904 and batch: 800, loss is 4.870673198699951 and perplexity is 130.4086783054087
At time: 503.3139934539795 and batch: 850, loss is 4.895301780700684 and perplexity is 133.66033663887367
At time: 504.5236225128174 and batch: 900, loss is 4.849415187835693 and perplexity is 127.66570755185913
At time: 505.7332172393799 and batch: 950, loss is 4.825645780563354 and perplexity is 124.66694988683255
At time: 506.94318175315857 and batch: 1000, loss is 4.823031826019287 and perplexity is 124.34150168539244
At time: 508.15254640579224 and batch: 1050, loss is 4.824041204452515 and perplexity is 124.46707267921389
At time: 509.3627362251282 and batch: 1100, loss is 4.773252201080322 and perplexity is 118.30336332816424
At time: 510.5713975429535 and batch: 1150, loss is 4.823155174255371 and perplexity is 124.356839936251
At time: 511.7822403907776 and batch: 1200, loss is 4.846989622116089 and perplexity is 127.35642123683108
At time: 512.9933624267578 and batch: 1250, loss is 4.903321800231933 and perplexity is 134.7366052279661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.953730005417427 and perplexity of 141.70253060333283
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 516.0437326431274 and batch: 50, loss is 4.893974227905273 and perplexity is 133.48301321447997
At time: 517.2810249328613 and batch: 100, loss is 4.907388515472412 and perplexity is 135.28565629437531
At time: 518.4917821884155 and batch: 150, loss is 4.8219483375549315 and perplexity is 124.20685206150542
At time: 519.7014899253845 and batch: 200, loss is 4.867101421356201 and perplexity is 129.9437184037444
At time: 520.9173548221588 and batch: 250, loss is 4.886107902526856 and perplexity is 132.43711149494348
At time: 522.1279654502869 and batch: 300, loss is 4.866545639038086 and perplexity is 129.87151804840437
At time: 523.3370323181152 and batch: 350, loss is 4.878904085159302 and perplexity is 131.48648690506334
At time: 524.5973100662231 and batch: 400, loss is 4.844982385635376 and perplexity is 127.10104316990075
At time: 525.8085141181946 and batch: 450, loss is 4.817862977981568 and perplexity is 123.70045751557869
At time: 527.018458366394 and batch: 500, loss is 4.8396049499511715 and perplexity is 126.41939987392975
At time: 528.2286145687103 and batch: 550, loss is 4.823283157348633 and perplexity is 124.37275652779991
At time: 529.440970659256 and batch: 600, loss is 4.8368829822540285 and perplexity is 126.0757582540482
At time: 530.657000541687 and batch: 650, loss is 4.85098708152771 and perplexity is 127.86654227631229
At time: 531.8673157691956 and batch: 700, loss is 4.84886456489563 and perplexity is 127.5954312342735
At time: 533.0778415203094 and batch: 750, loss is 4.812692441940308 and perplexity is 123.06251052688525
At time: 534.287999868393 and batch: 800, loss is 4.853880987167359 and perplexity is 128.2371119223052
At time: 535.4990088939667 and batch: 850, loss is 4.873612804412842 and perplexity is 130.79259240274362
At time: 536.7089579105377 and batch: 900, loss is 4.8282511520385745 and perplexity is 124.99217708683626
At time: 537.9196312427521 and batch: 950, loss is 4.8038679885864255 and perplexity is 121.98132857901093
At time: 539.1292736530304 and batch: 1000, loss is 4.7998591232299805 and perplexity is 121.49330072929844
At time: 540.3389160633087 and batch: 1050, loss is 4.799300994873047 and perplexity is 121.4255107924867
At time: 541.5488059520721 and batch: 1100, loss is 4.746032705307007 and perplexity is 115.12663602209261
At time: 542.7599260807037 and batch: 1150, loss is 4.793956708908081 and perplexity is 120.77830909525757
At time: 543.9713411331177 and batch: 1200, loss is 4.822509994506836 and perplexity is 124.27663329816387
At time: 545.1818690299988 and batch: 1250, loss is 4.884659900665283 and perplexity is 132.24548108503538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9466067126197535 and perplexity of 140.69672855190018
Finished 17 epochs...
Completing Train Step...
At time: 548.2583940029144 and batch: 50, loss is 4.885690546035766 and perplexity is 132.38184953956997
At time: 549.4693660736084 and batch: 100, loss is 4.898570146560669 and perplexity is 134.09790219262294
At time: 550.6810426712036 and batch: 150, loss is 4.813958902359008 and perplexity is 123.21846305850309
At time: 551.8922641277313 and batch: 200, loss is 4.858733644485474 and perplexity is 128.86091500911223
At time: 553.1038858890533 and batch: 250, loss is 4.878517818450928 and perplexity is 131.43570786032944
At time: 554.3149650096893 and batch: 300, loss is 4.859978351593018 and perplexity is 129.02140896917402
At time: 555.5544579029083 and batch: 350, loss is 4.872661418914795 and perplexity is 130.66821740074766
At time: 556.7679460048676 and batch: 400, loss is 4.839509696960449 and perplexity is 126.40735862149816
At time: 557.9793136119843 and batch: 450, loss is 4.812559432983399 and perplexity is 123.0461431992502
At time: 559.1908984184265 and batch: 500, loss is 4.834827880859375 and perplexity is 125.81692584195086
At time: 560.4024562835693 and batch: 550, loss is 4.818933782577514 and perplexity is 123.83298747819012
At time: 561.6133859157562 and batch: 600, loss is 4.833122253417969 and perplexity is 125.60251194773275
At time: 562.8248865604401 and batch: 650, loss is 4.847648229598999 and perplexity is 127.4403267562228
At time: 564.0367045402527 and batch: 700, loss is 4.84595832824707 and perplexity is 127.22514704319975
At time: 565.2485110759735 and batch: 750, loss is 4.810465459823608 and perplexity is 122.78875745144614
At time: 566.4594388008118 and batch: 800, loss is 4.8521799468994145 and perplexity is 128.01916085540523
At time: 567.6704909801483 and batch: 850, loss is 4.872970123291015 and perplexity is 130.70856147816838
At time: 568.8821012973785 and batch: 900, loss is 4.828107004165649 and perplexity is 124.97416102889632
At time: 570.0938708782196 and batch: 950, loss is 4.804295711517334 and perplexity is 122.03351395003115
At time: 571.305287361145 and batch: 1000, loss is 4.800693597793579 and perplexity is 121.59472611096959
At time: 572.5176351070404 and batch: 1050, loss is 4.801131420135498 and perplexity is 121.64797465457092
At time: 573.7293879985809 and batch: 1100, loss is 4.748027420043945 and perplexity is 115.35651000987528
At time: 574.9407920837402 and batch: 1150, loss is 4.796413679122924 and perplexity is 121.07542265341965
At time: 576.1535205841064 and batch: 1200, loss is 4.825295248031616 and perplexity is 124.62325772347438
At time: 577.3647000789642 and batch: 1250, loss is 4.885175514221191 and perplexity is 132.31368623002803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.946190158816149 and perplexity of 140.63813299941464
Finished 18 epochs...
Completing Train Step...
At time: 580.395720243454 and batch: 50, loss is 4.881983242034912 and perplexity is 131.89197839083607
At time: 581.6451687812805 and batch: 100, loss is 4.894920740127564 and perplexity is 133.60941632956542
At time: 582.8526728153229 and batch: 150, loss is 4.8101245880126955 and perplexity is 122.74690935815681
At time: 584.0602128505707 and batch: 200, loss is 4.854771337509155 and perplexity is 128.3513387221255
At time: 585.2662234306335 and batch: 250, loss is 4.874726486206055 and perplexity is 130.93833487189335
At time: 586.4988789558411 and batch: 300, loss is 4.856516017913818 and perplexity is 128.57546624630402
At time: 587.7029504776001 and batch: 350, loss is 4.8692608737945555 and perplexity is 130.2246288805446
At time: 588.9087402820587 and batch: 400, loss is 4.8366516590118405 and perplexity is 126.04659737381228
At time: 590.1169035434723 and batch: 450, loss is 4.809589223861694 and perplexity is 122.68121265064045
At time: 591.3289034366608 and batch: 500, loss is 4.831938724517823 and perplexity is 125.45394567849019
At time: 592.5386698246002 and batch: 550, loss is 4.816621160507202 and perplexity is 123.5469394662754
At time: 593.7488808631897 and batch: 600, loss is 4.831243352890015 and perplexity is 125.36673888814668
At time: 594.958643913269 and batch: 650, loss is 4.845878477096558 and perplexity is 127.21498837443036
At time: 596.1685743331909 and batch: 700, loss is 4.8442603206634525 and perplexity is 127.00930108459198
At time: 597.3793866634369 and batch: 750, loss is 4.809177846908569 and perplexity is 122.63075480648318
At time: 598.5891287326813 and batch: 800, loss is 4.8508955764770505 and perplexity is 127.85484237719176
At time: 599.7990057468414 and batch: 850, loss is 4.872448415756225 and perplexity is 130.64038762173726
At time: 601.0080370903015 and batch: 900, loss is 4.828064193725586 and perplexity is 124.96881094458669
At time: 602.217612028122 and batch: 950, loss is 4.804511222839356 and perplexity is 122.05981638808831
At time: 603.4191179275513 and batch: 1000, loss is 4.800950622558593 and perplexity is 121.62598298359836
At time: 604.6132545471191 and batch: 1050, loss is 4.801955270767212 and perplexity is 121.74823570973277
At time: 605.80916929245 and batch: 1100, loss is 4.748901271820069 and perplexity is 115.45735855795881
At time: 607.0132954120636 and batch: 1150, loss is 4.797459621429443 and perplexity is 121.20212681132091
At time: 608.2178134918213 and batch: 1200, loss is 4.826426210403443 and perplexity is 124.76428166997088
At time: 609.4300663471222 and batch: 1250, loss is 4.885009126663208 and perplexity is 132.29167271032716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.946066306455292 and perplexity of 140.62071571322443
Finished 19 epochs...
Completing Train Step...
At time: 612.5549507141113 and batch: 50, loss is 4.879174213409424 and perplexity is 131.5220099173543
At time: 613.767050743103 and batch: 100, loss is 4.89217393875122 and perplexity is 133.24292137575188
At time: 614.9800200462341 and batch: 150, loss is 4.80742579460144 and perplexity is 122.41608741875895
At time: 616.1922442913055 and batch: 200, loss is 4.851936511993408 and perplexity is 127.98800031595076
At time: 617.452089548111 and batch: 250, loss is 4.872058029174805 and perplexity is 130.58939732103033
At time: 618.6663358211517 and batch: 300, loss is 4.854116611480713 and perplexity is 128.26733126381436
At time: 619.880793094635 and batch: 350, loss is 4.86694221496582 and perplexity is 129.92303218011781
At time: 621.092648267746 and batch: 400, loss is 4.834483089447022 and perplexity is 125.7735527241608
At time: 622.3069207668304 and batch: 450, loss is 4.807353944778442 and perplexity is 122.40729216051847
At time: 623.5192461013794 and batch: 500, loss is 4.829668560028076 and perplexity is 125.1694676140872
At time: 624.7307431697845 and batch: 550, loss is 4.814891176223755 and perplexity is 123.333389974623
At time: 625.9444825649261 and batch: 600, loss is 4.829931955337525 and perplexity is 125.20244100707082
At time: 627.1582109928131 and batch: 650, loss is 4.84453405380249 and perplexity is 127.04407249809178
At time: 628.3688976764679 and batch: 700, loss is 4.84302451133728 and perplexity is 126.85243875173158
At time: 629.5816855430603 and batch: 750, loss is 4.8081786251068115 and perplexity is 122.50828068231236
At time: 630.7946290969849 and batch: 800, loss is 4.849768209457397 and perplexity is 127.71078426303379
At time: 632.0072991847992 and batch: 850, loss is 4.871856327056885 and perplexity is 130.56305981926704
At time: 633.2210142612457 and batch: 900, loss is 4.827849473953247 and perplexity is 124.9419805505623
At time: 634.4321355819702 and batch: 950, loss is 4.804373331069947 and perplexity is 122.04298650441069
At time: 635.6458072662354 and batch: 1000, loss is 4.800804090499878 and perplexity is 121.60816218360964
At time: 636.8596494197845 and batch: 1050, loss is 4.802189445495605 and perplexity is 121.77674940822597
At time: 638.0715088844299 and batch: 1100, loss is 4.749093179702759 and perplexity is 115.47951786138513
At time: 639.2831265926361 and batch: 1150, loss is 4.797799892425537 and perplexity is 121.24337539719046
At time: 640.4984080791473 and batch: 1200, loss is 4.8269463157653805 and perplexity is 124.8291891197389
At time: 641.7100858688354 and batch: 1250, loss is 4.884595718383789 and perplexity is 132.23699354071996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945979431597856 and perplexity of 140.60849983922802
Finished 20 epochs...
Completing Train Step...
At time: 644.7807643413544 and batch: 50, loss is 4.8767484760284425 and perplexity is 131.2033586998115
At time: 646.02512550354 and batch: 100, loss is 4.889897594451904 and perplexity is 132.93995956477212
At time: 647.2357170581818 and batch: 150, loss is 4.8052035999298095 and perplexity is 122.14435707225958
At time: 648.5004234313965 and batch: 200, loss is 4.849578247070313 and perplexity is 127.68652632171992
At time: 649.7142403125763 and batch: 250, loss is 4.869750967025757 and perplexity is 130.28846673166626
At time: 650.9227440357208 and batch: 300, loss is 4.852132205963135 and perplexity is 128.01304924669236
At time: 652.1305022239685 and batch: 350, loss is 4.864933748245239 and perplexity is 129.66234796916018
At time: 653.3365190029144 and batch: 400, loss is 4.83263011932373 and perplexity is 125.54071387699734
At time: 654.5351359844208 and batch: 450, loss is 4.80540132522583 and perplexity is 122.16851048921102
At time: 655.7372486591339 and batch: 500, loss is 4.827735977172852 and perplexity is 124.92780084272442
At time: 656.939389705658 and batch: 550, loss is 4.813224620819092 and perplexity is 123.12801922538462
At time: 658.1490445137024 and batch: 600, loss is 4.828812885284424 and perplexity is 125.06240907215077
At time: 659.3573310375214 and batch: 650, loss is 4.843303079605103 and perplexity is 126.88778073821318
At time: 660.5659039020538 and batch: 700, loss is 4.841903076171875 and perplexity is 126.71026170216997
At time: 661.7672913074493 and batch: 750, loss is 4.807051744461059 and perplexity is 122.37030622683953
At time: 662.9706838130951 and batch: 800, loss is 4.848644962310791 and perplexity is 127.56741402419935
At time: 664.1833274364471 and batch: 850, loss is 4.871117334365845 and perplexity is 130.4666103144697
At time: 665.3991749286652 and batch: 900, loss is 4.827306537628174 and perplexity is 124.87416342265189
At time: 666.6151156425476 and batch: 950, loss is 4.804080314636231 and perplexity is 122.0072311424553
At time: 667.8292834758759 and batch: 1000, loss is 4.80035659790039 and perplexity is 121.55375560517369
At time: 669.0440781116486 and batch: 1050, loss is 4.802035102844238 and perplexity is 121.75795551223462
At time: 670.2602143287659 and batch: 1100, loss is 4.749001741409302 and perplexity is 115.46895909408785
At time: 671.4757730960846 and batch: 1150, loss is 4.7977841091156 and perplexity is 121.2414617905203
At time: 672.6906521320343 and batch: 1200, loss is 4.8269665241241455 and perplexity is 124.83171173826588
At time: 673.9057989120483 and batch: 1250, loss is 4.883887004852295 and perplexity is 132.14330859584896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945954482920849 and perplexity of 140.60499188694067
Finished 21 epochs...
Completing Train Step...
At time: 676.969838142395 and batch: 50, loss is 4.874503316879273 and perplexity is 130.9091167122698
At time: 678.2077248096466 and batch: 100, loss is 4.887825336456299 and perplexity is 132.66475891248638
At time: 679.421736240387 and batch: 150, loss is 4.8031684684753415 and perplexity is 121.89603002399876
At time: 680.6349196434021 and batch: 200, loss is 4.847429141998291 and perplexity is 127.41240921910519
At time: 681.8490927219391 and batch: 250, loss is 4.867874002456665 and perplexity is 130.0441492552005
At time: 683.0618326663971 and batch: 300, loss is 4.850297889709473 and perplexity is 127.77844806194202
At time: 684.2753038406372 and batch: 350, loss is 4.863177127838135 and perplexity is 129.4347803761409
At time: 685.4892160892487 and batch: 400, loss is 4.830827684402466 and perplexity is 125.31463871435446
At time: 686.7027144432068 and batch: 450, loss is 4.803694715499878 and perplexity is 121.96019432875816
At time: 687.9183385372162 and batch: 500, loss is 4.825998849868775 and perplexity is 124.7109737315148
At time: 689.1304528713226 and batch: 550, loss is 4.811965856552124 and perplexity is 122.97312758100675
At time: 690.345499753952 and batch: 600, loss is 4.827686061859131 and perplexity is 124.92156518798156
At time: 691.5596573352814 and batch: 650, loss is 4.842230625152588 and perplexity is 126.75177231724507
At time: 692.7721705436707 and batch: 700, loss is 4.840910129547119 and perplexity is 126.58450761936902
At time: 693.9858014583588 and batch: 750, loss is 4.805946521759033 and perplexity is 122.2351344975352
At time: 695.1992893218994 and batch: 800, loss is 4.847679767608643 and perplexity is 127.44434603385673
At time: 696.4132058620453 and batch: 850, loss is 4.870345640182495 and perplexity is 130.36596882738812
At time: 697.627064704895 and batch: 900, loss is 4.826859941482544 and perplexity is 124.81840755368336
At time: 698.8404841423035 and batch: 950, loss is 4.8035758686065675 and perplexity is 121.94570059983901
At time: 700.0521769523621 and batch: 1000, loss is 4.799931583404541 and perplexity is 121.50210447403376
At time: 701.2688798904419 and batch: 1050, loss is 4.801787614822388 and perplexity is 121.72782560522968
At time: 702.4824602603912 and batch: 1100, loss is 4.748745651245117 and perplexity is 115.43939241542458
At time: 703.6975238323212 and batch: 1150, loss is 4.797677364349365 and perplexity is 121.2285205897385
At time: 704.9095623493195 and batch: 1200, loss is 4.826894159317017 and perplexity is 124.82267864236545
At time: 706.122679233551 and batch: 1250, loss is 4.883301029205322 and perplexity is 132.0658985175267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945909486199818 and perplexity of 140.59866526568484
Finished 22 epochs...
Completing Train Step...
At time: 709.2177646160126 and batch: 50, loss is 4.872621212005615 and perplexity is 130.66296374121538
At time: 710.4307582378387 and batch: 100, loss is 4.885986528396606 and perplexity is 132.42103803119412
At time: 711.6457240581512 and batch: 150, loss is 4.801470623016358 and perplexity is 121.68924499713836
At time: 712.85870885849 and batch: 200, loss is 4.845645809173584 and perplexity is 127.18539297039831
At time: 714.0727326869965 and batch: 250, loss is 4.8661636543273925 and perplexity is 129.82191858787624
At time: 715.2858295440674 and batch: 300, loss is 4.8487702751159665 and perplexity is 127.58340085635615
At time: 716.499596118927 and batch: 350, loss is 4.8616506958007815 and perplexity is 129.23735769511237
At time: 717.7125856876373 and batch: 400, loss is 4.8292598533630375 and perplexity is 125.11832047121504
At time: 718.9244554042816 and batch: 450, loss is 4.802140684127807 and perplexity is 121.77081155212903
At time: 720.1375586986542 and batch: 500, loss is 4.824418458938599 and perplexity is 124.51403729900161
At time: 721.3512964248657 and batch: 550, loss is 4.810793313980103 and perplexity is 122.82902085584898
At time: 722.565322637558 and batch: 600, loss is 4.8266287040710445 and perplexity is 124.78954820502227
At time: 723.7789127826691 and batch: 650, loss is 4.841214256286621 and perplexity is 126.62301120763141
At time: 724.9922091960907 and batch: 700, loss is 4.839889640808106 and perplexity is 126.4553954447748
At time: 726.2045276165009 and batch: 750, loss is 4.804912729263306 and perplexity is 122.10883402826448
At time: 727.4186885356903 and batch: 800, loss is 4.846678810119629 and perplexity is 127.31684348422321
At time: 728.6323194503784 and batch: 850, loss is 4.869492130279541 and perplexity is 130.25474765292455
At time: 729.8465204238892 and batch: 900, loss is 4.8262440204620365 and perplexity is 124.7415529433394
At time: 731.0609595775604 and batch: 950, loss is 4.8030627155303955 and perplexity is 121.8831398414459
At time: 732.2743372917175 and batch: 1000, loss is 4.799284734725952 and perplexity is 121.42353641187209
At time: 733.4875273704529 and batch: 1050, loss is 4.8013636589050295 and perplexity is 121.67622931130708
At time: 734.7002415657043 and batch: 1100, loss is 4.748345537185669 and perplexity is 115.39321273069167
At time: 735.9147243499756 and batch: 1150, loss is 4.797338619232177 and perplexity is 121.18746197492008
At time: 737.1283092498779 and batch: 1200, loss is 4.826822090148926 and perplexity is 124.81368309991132
At time: 738.3526136875153 and batch: 1250, loss is 4.8826057243347165 and perplexity is 131.97410437120325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9459063676151915 and perplexity of 140.5982267975326
Finished 23 epochs...
Completing Train Step...
At time: 741.4551360607147 and batch: 50, loss is 4.870834894180298 and perplexity is 130.42976650417603
At time: 742.6929309368134 and batch: 100, loss is 4.884194993972779 and perplexity is 132.18401356526329
At time: 743.9013614654541 and batch: 150, loss is 4.799982795715332 and perplexity is 121.50832703690442
At time: 745.1075658798218 and batch: 200, loss is 4.844123830795288 and perplexity is 126.99196678483638
At time: 746.3149011135101 and batch: 250, loss is 4.864592294692994 and perplexity is 129.61808185769425
At time: 747.5169503688812 and batch: 300, loss is 4.847273616790772 and perplexity is 127.39259491857234
At time: 748.71746301651 and batch: 350, loss is 4.860176830291748 and perplexity is 129.04701951202415
At time: 749.9184086322784 and batch: 400, loss is 4.827771301269531 and perplexity is 124.93221388238223
At time: 751.1233251094818 and batch: 450, loss is 4.800715827941895 and perplexity is 121.59742920981051
At time: 752.3332817554474 and batch: 500, loss is 4.822893905639648 and perplexity is 124.32435364083473
At time: 753.5473742485046 and batch: 550, loss is 4.809724111557006 and perplexity is 122.69776195279648
At time: 754.7611067295074 and batch: 600, loss is 4.825554065704345 and perplexity is 124.65551659940982
At time: 755.9737725257874 and batch: 650, loss is 4.840269441604614 and perplexity is 126.50343242635203
At time: 757.1886742115021 and batch: 700, loss is 4.838897085189819 and perplexity is 126.32994370077932
At time: 758.4020583629608 and batch: 750, loss is 4.803880386352539 and perplexity is 121.98284088436743
At time: 759.6169173717499 and batch: 800, loss is 4.845690689086914 and perplexity is 127.19110116790236
At time: 760.8315885066986 and batch: 850, loss is 4.8686658477783205 and perplexity is 130.14716488723732
At time: 762.0471315383911 and batch: 900, loss is 4.825603647232056 and perplexity is 124.66169736358492
At time: 763.2640066146851 and batch: 950, loss is 4.802433261871338 and perplexity is 121.80644419380558
At time: 764.4734859466553 and batch: 1000, loss is 4.7986900329589846 and perplexity is 121.35134708790336
At time: 765.6851394176483 and batch: 1050, loss is 4.8010107517242435 and perplexity is 121.63329647235132
At time: 766.8971984386444 and batch: 1100, loss is 4.7478972339630126 and perplexity is 115.34149317543968
At time: 768.1122591495514 and batch: 1150, loss is 4.797046813964844 and perplexity is 121.15210399425442
At time: 769.3300457000732 and batch: 1200, loss is 4.8266315746307376 and perplexity is 124.7899064213836
At time: 770.606077671051 and batch: 1250, loss is 4.881876726150512 and perplexity is 131.877930548285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945885428546989 and perplexity of 140.59528283249452
Finished 24 epochs...
Completing Train Step...
At time: 773.7351515293121 and batch: 50, loss is 4.869088125228882 and perplexity is 130.2021347056643
At time: 774.9470434188843 and batch: 100, loss is 4.88267746925354 and perplexity is 131.98357318227366
At time: 776.1557655334473 and batch: 150, loss is 4.79836371421814 and perplexity is 121.31175432940272
At time: 777.3700275421143 and batch: 200, loss is 4.842607316970825 and perplexity is 126.79952766678399
At time: 778.5869116783142 and batch: 250, loss is 4.863029594421387 and perplexity is 129.41568582932246
At time: 779.8019144535065 and batch: 300, loss is 4.845940837860107 and perplexity is 127.22292184560642
At time: 781.0169432163239 and batch: 350, loss is 4.858782997131348 and perplexity is 128.86727479315246
At time: 782.2302248477936 and batch: 400, loss is 4.826357536315918 and perplexity is 124.75571389096704
At time: 783.4490382671356 and batch: 450, loss is 4.799261646270752 and perplexity is 121.42073296235519
At time: 784.6665668487549 and batch: 500, loss is 4.821435718536377 and perplexity is 124.14319758354586
At time: 785.8845372200012 and batch: 550, loss is 4.808609476089478 and perplexity is 122.56107486782575
At time: 787.1047184467316 and batch: 600, loss is 4.824587383270264 and perplexity is 124.53507252616608
At time: 788.3230476379395 and batch: 650, loss is 4.839312744140625 and perplexity is 126.38246478730707
At time: 789.5418062210083 and batch: 700, loss is 4.837974061965943 and perplexity is 126.2133920270982
At time: 790.758722782135 and batch: 750, loss is 4.802761859893799 and perplexity is 121.84647612733761
At time: 791.9771690368652 and batch: 800, loss is 4.844716634750366 and perplexity is 127.06727044295747
At time: 793.1936602592468 and batch: 850, loss is 4.867857599258423 and perplexity is 130.04201613273517
At time: 794.4093835353851 and batch: 900, loss is 4.824891443252564 and perplexity is 124.57294441548531
At time: 795.6266891956329 and batch: 950, loss is 4.80174352645874 and perplexity is 121.72245894289289
At time: 796.8474090099335 and batch: 1000, loss is 4.798016471862793 and perplexity is 121.26963706297069
At time: 798.0674419403076 and batch: 1050, loss is 4.800584774017334 and perplexity is 121.58149443367638
At time: 799.2848610877991 and batch: 1100, loss is 4.747519369125366 and perplexity is 115.29791791414506
At time: 800.5018723011017 and batch: 1150, loss is 4.79668251991272 and perplexity is 121.10797704144821
At time: 801.7716047763824 and batch: 1200, loss is 4.826189403533935 and perplexity is 124.73474012896027
At time: 802.9872133731842 and batch: 1250, loss is 4.881159353256225 and perplexity is 131.78335882120754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945824393390739 and perplexity of 140.5867018393126
Finished 25 epochs...
Completing Train Step...
At time: 806.063197851181 and batch: 50, loss is 4.867473707199097 and perplexity is 129.99210361648534
At time: 807.3031160831451 and batch: 100, loss is 4.881110668182373 and perplexity is 131.77694309482715
At time: 808.5166251659393 and batch: 150, loss is 4.796952314376831 and perplexity is 121.14065571127765
At time: 809.7285394668579 and batch: 200, loss is 4.841245279312134 and perplexity is 126.62693949747202
At time: 810.9423263072968 and batch: 250, loss is 4.8616611289978025 and perplexity is 129.23870606096153
At time: 812.1548337936401 and batch: 300, loss is 4.84461709022522 and perplexity is 127.05462222140063
At time: 813.3671772480011 and batch: 350, loss is 4.857438812255859 and perplexity is 128.69416972010492
At time: 814.5818474292755 and batch: 400, loss is 4.825013475418091 and perplexity is 124.58814724925693
At time: 815.796021938324 and batch: 450, loss is 4.797946882247925 and perplexity is 121.2611982492626
At time: 817.0076215267181 and batch: 500, loss is 4.820037107467652 and perplexity is 123.96969089576878
At time: 818.2204160690308 and batch: 550, loss is 4.807542695999145 and perplexity is 122.43039886697765
At time: 819.433043718338 and batch: 600, loss is 4.823707981109619 and perplexity is 124.42560425468251
At time: 820.6360607147217 and batch: 650, loss is 4.838392467498779 and perplexity is 126.26621145784087
At time: 821.8451590538025 and batch: 700, loss is 4.837123889923095 and perplexity is 126.106134529887
At time: 823.0515348911285 and batch: 750, loss is 4.801759433746338 and perplexity is 121.72439523245497
At time: 824.2597844600677 and batch: 800, loss is 4.843769111633301 and perplexity is 126.94692828928027
At time: 825.4620904922485 and batch: 850, loss is 4.867001390457153 and perplexity is 129.93072066686528
At time: 826.6662969589233 and batch: 900, loss is 4.824149408340454 and perplexity is 124.4805412290598
At time: 827.8740057945251 and batch: 950, loss is 4.801030082702637 and perplexity is 121.63564778570384
At time: 829.0868339538574 and batch: 1000, loss is 4.797288160324097 and perplexity is 121.181347142191
At time: 830.299964427948 and batch: 1050, loss is 4.800076303482055 and perplexity is 121.51968954043683
At time: 831.5664343833923 and batch: 1100, loss is 4.746981163024902 and perplexity is 115.2358805672896
At time: 832.7863540649414 and batch: 1150, loss is 4.796140155792236 and perplexity is 121.04231022926773
At time: 834.006688117981 and batch: 1200, loss is 4.825852365493774 and perplexity is 124.6927068604103
At time: 835.2221410274506 and batch: 1250, loss is 4.880399770736695 and perplexity is 131.68329649309058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945763358234489 and perplexity of 140.578121369857
Finished 26 epochs...
Completing Train Step...
At time: 838.2760989665985 and batch: 50, loss is 4.865832796096802 and perplexity is 129.77897304243572
At time: 839.5384757518768 and batch: 100, loss is 4.879616022109985 and perplexity is 131.58013032375626
At time: 840.7511870861053 and batch: 150, loss is 4.7955610466003415 and perplexity is 120.97223380771416
At time: 841.9662442207336 and batch: 200, loss is 4.839872961044311 and perplexity is 126.45328621623904
At time: 843.178472995758 and batch: 250, loss is 4.86023440361023 and perplexity is 129.05444939105695
At time: 844.395777463913 and batch: 300, loss is 4.843380422592163 and perplexity is 126.8975949977241
At time: 845.6137700080872 and batch: 350, loss is 4.856179084777832 and perplexity is 128.53215220862782
At time: 846.8324415683746 and batch: 400, loss is 4.823730211257935 and perplexity is 124.4283702850639
At time: 848.0547211170197 and batch: 450, loss is 4.79666090965271 and perplexity is 121.10535989485378
At time: 849.277749300003 and batch: 500, loss is 4.818696527481079 and perplexity is 123.80361095580679
At time: 850.4950127601624 and batch: 550, loss is 4.806488590240479 and perplexity is 122.30141227319527
At time: 851.7126648426056 and batch: 600, loss is 4.8228294563293455 and perplexity is 124.31634128018707
At time: 852.927038192749 and batch: 650, loss is 4.837530460357666 and perplexity is 126.15741597985914
At time: 854.1414594650269 and batch: 700, loss is 4.836254224777222 and perplexity is 125.99651209432038
At time: 855.3578021526337 and batch: 750, loss is 4.800647048950196 and perplexity is 121.58906614884097
At time: 856.5743222236633 and batch: 800, loss is 4.84274658203125 and perplexity is 126.81718764034648
At time: 857.7904102802277 and batch: 850, loss is 4.866114406585694 and perplexity is 129.8155253089913
At time: 859.0083734989166 and batch: 900, loss is 4.823402271270752 and perplexity is 124.38757193697798
At time: 860.2249436378479 and batch: 950, loss is 4.800417308807373 and perplexity is 121.56113546794043
At time: 861.4418835639954 and batch: 1000, loss is 4.796503610610962 and perplexity is 121.08631163596753
At time: 862.7086684703827 and batch: 1050, loss is 4.7995648765563965 and perplexity is 121.45755698869237
At time: 863.924649477005 and batch: 1100, loss is 4.746539382934571 and perplexity is 115.18498289318052
At time: 865.142733335495 and batch: 1150, loss is 4.795644359588623 and perplexity is 120.98231278586084
At time: 866.362096786499 and batch: 1200, loss is 4.825224657058715 and perplexity is 124.61446075696247
At time: 867.5781445503235 and batch: 1250, loss is 4.879580974578857 and perplexity is 131.57551884585385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945752665944343 and perplexity of 140.57661827583092
Finished 27 epochs...
Completing Train Step...
At time: 870.6593811511993 and batch: 50, loss is 4.864275674819947 and perplexity is 129.57704869337232
At time: 871.8736605644226 and batch: 100, loss is 4.87818606376648 and perplexity is 131.39211068072711
At time: 873.0874853134155 and batch: 150, loss is 4.794166994094849 and perplexity is 120.80370965513028
At time: 874.3019196987152 and batch: 200, loss is 4.838586921691895 and perplexity is 126.29076683947875
At time: 875.518844127655 and batch: 250, loss is 4.858869676589966 and perplexity is 128.87844542289017
At time: 876.7348792552948 and batch: 300, loss is 4.842128477096558 and perplexity is 126.73882553136019
At time: 877.9497861862183 and batch: 350, loss is 4.854865779876709 and perplexity is 128.3634610988571
At time: 879.1643805503845 and batch: 400, loss is 4.822511730194091 and perplexity is 124.2768490037196
At time: 880.3789808750153 and batch: 450, loss is 4.795353813171387 and perplexity is 120.9471669143327
At time: 881.5926332473755 and batch: 500, loss is 4.817418947219848 and perplexity is 123.64554289999458
At time: 882.808919429779 and batch: 550, loss is 4.805426912307739 and perplexity is 122.17163646488775
At time: 884.0252604484558 and batch: 600, loss is 4.821966247558594 and perplexity is 124.20907662660164
At time: 885.2403423786163 and batch: 650, loss is 4.836585655212402 and perplexity is 126.03827809403437
At time: 886.4608886241913 and batch: 700, loss is 4.835441637039184 and perplexity is 125.89417045996694
At time: 887.6755437850952 and batch: 750, loss is 4.799679098129272 and perplexity is 121.47143085422154
At time: 888.8916239738464 and batch: 800, loss is 4.841816272735596 and perplexity is 126.69926329339906
At time: 890.1075491905212 and batch: 850, loss is 4.865299005508422 and perplexity is 129.7097167338834
At time: 891.3220658302307 and batch: 900, loss is 4.822709827423096 and perplexity is 124.30147034176557
At time: 892.5382866859436 and batch: 950, loss is 4.799859790802002 and perplexity is 121.49338183485389
At time: 893.7820954322815 and batch: 1000, loss is 4.7957282066345215 and perplexity is 120.99245722067839
At time: 894.9947078227997 and batch: 1050, loss is 4.79899642944336 and perplexity is 121.38853441076687
At time: 896.2107019424438 and batch: 1100, loss is 4.745903882980347 and perplexity is 115.11180609621296
At time: 897.4253022670746 and batch: 1150, loss is 4.79506404876709 and perplexity is 120.91212580763828
At time: 898.6433584690094 and batch: 1200, loss is 4.824709167480469 and perplexity is 124.55023985516837
At time: 899.8587703704834 and batch: 1250, loss is 4.878761520385742 and perplexity is 131.46774289998072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945832412608349 and perplexity of 140.58782923918815
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 902.92254114151 and batch: 50, loss is 4.864143800735474 and perplexity is 129.55996196538024
At time: 904.1662158966064 and batch: 100, loss is 4.878834314346314 and perplexity is 131.47731330600345
At time: 905.3824849128723 and batch: 150, loss is 4.795208139419556 and perplexity is 120.929549369993
At time: 906.5954649448395 and batch: 200, loss is 4.838546314239502 and perplexity is 126.28563859729981
At time: 907.812112569809 and batch: 250, loss is 4.859200954437256 and perplexity is 128.92114706951622
At time: 909.0258872509003 and batch: 300, loss is 4.841839542388916 and perplexity is 126.7022115756345
At time: 910.2421724796295 and batch: 350, loss is 4.853565073013305 and perplexity is 128.19660640204447
At time: 911.4597644805908 and batch: 400, loss is 4.820208873748779 and perplexity is 123.99098653743067
At time: 912.6729273796082 and batch: 450, loss is 4.792464094161987 and perplexity is 120.59816808410318
At time: 913.881888628006 and batch: 500, loss is 4.814495487213135 and perplexity is 123.28459796143582
At time: 915.0936918258667 and batch: 550, loss is 4.801575164794922 and perplexity is 121.70196727223559
At time: 916.3066718578339 and batch: 600, loss is 4.817570209503174 and perplexity is 123.6642472217302
At time: 917.5184907913208 and batch: 650, loss is 4.831932535171509 and perplexity is 125.4531692029769
At time: 918.7213315963745 and batch: 700, loss is 4.8309291744232175 and perplexity is 125.32735754504438
At time: 919.9260809421539 and batch: 750, loss is 4.794127607345581 and perplexity is 120.79895168340865
At time: 921.1333403587341 and batch: 800, loss is 4.835180168151855 and perplexity is 125.86125735435962
At time: 922.3386645317078 and batch: 850, loss is 4.856882638931275 and perplexity is 128.6226133565919
At time: 923.5536022186279 and batch: 900, loss is 4.814369249343872 and perplexity is 123.26903575876656
At time: 924.7984535694122 and batch: 950, loss is 4.790966882705688 and perplexity is 120.41774222677344
At time: 926.014092206955 and batch: 1000, loss is 4.7865989208221436 and perplexity is 119.89290917844409
At time: 927.227463722229 and batch: 1050, loss is 4.789156198501587 and perplexity is 120.19990100337843
At time: 928.4420645236969 and batch: 1100, loss is 4.735025444030762 and perplexity is 113.8663559004142
At time: 929.6594789028168 and batch: 1150, loss is 4.783276681900024 and perplexity is 119.49525720227103
At time: 930.8753809928894 and batch: 1200, loss is 4.814440813064575 and perplexity is 123.27785766527342
At time: 932.0911123752594 and batch: 1250, loss is 4.8719071006774906 and perplexity is 130.56968914682747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9447057125342155 and perplexity of 140.42951812288277
Finished 29 epochs...
Completing Train Step...
At time: 935.1829800605774 and batch: 50, loss is 4.862129402160645 and perplexity is 129.29923925053828
At time: 936.3974709510803 and batch: 100, loss is 4.876430263519287 and perplexity is 131.16161479189913
At time: 937.6119067668915 and batch: 150, loss is 4.792828359603882 and perplexity is 120.64210583110757
At time: 938.8248138427734 and batch: 200, loss is 4.835964403152466 and perplexity is 125.9600008715981
At time: 940.0391757488251 and batch: 250, loss is 4.857122192382812 and perplexity is 128.65342903842136
At time: 941.2546405792236 and batch: 300, loss is 4.840045194625855 and perplexity is 126.47506759431054
At time: 942.468936920166 and batch: 350, loss is 4.851763820648193 and perplexity is 127.9658998043433
At time: 943.6843092441559 and batch: 400, loss is 4.818535280227661 and perplexity is 123.78364957297926
At time: 944.8983769416809 and batch: 450, loss is 4.790969486236572 and perplexity is 120.41805573849246
At time: 946.1142213344574 and batch: 500, loss is 4.813298273086548 and perplexity is 123.13708821715969
At time: 947.329021692276 and batch: 550, loss is 4.800606288909912 and perplexity is 121.58411027460822
At time: 948.5438206195831 and batch: 600, loss is 4.816571683883667 and perplexity is 123.54082693207755
At time: 949.7566459178925 and batch: 650, loss is 4.8311280918121335 and perplexity is 125.35228981541584
At time: 950.9693117141724 and batch: 700, loss is 4.830225963592529 and perplexity is 125.23925697010688
At time: 952.1827585697174 and batch: 750, loss is 4.793709869384766 and perplexity is 120.74849991420702
At time: 953.3957860469818 and batch: 800, loss is 4.834889211654663 and perplexity is 125.82464253070631
At time: 954.6371378898621 and batch: 850, loss is 4.856841287612915 and perplexity is 128.61729475192513
At time: 955.848628282547 and batch: 900, loss is 4.814385271072387 and perplexity is 123.27101075761325
At time: 957.0644376277924 and batch: 950, loss is 4.791083812713623 and perplexity is 120.43182349757303
At time: 958.2790606021881 and batch: 1000, loss is 4.78699667930603 and perplexity is 119.94060708572246
At time: 959.4925758838654 and batch: 1050, loss is 4.789863595962524 and perplexity is 120.28496018993238
At time: 960.7080562114716 and batch: 1100, loss is 4.735770931243897 and perplexity is 113.95127346128939
At time: 961.9222984313965 and batch: 1150, loss is 4.784027194976806 and perplexity is 119.58497361787511
At time: 963.1380052566528 and batch: 1200, loss is 4.815332145690918 and perplexity is 123.38778822698616
At time: 964.3498220443726 and batch: 1250, loss is 4.872270669937134 and perplexity is 130.61716890260922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.944727097114507 and perplexity of 140.43252118129783
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 967.3352582454681 and batch: 50, loss is 4.861515779495239 and perplexity is 129.21992264443682
At time: 968.5746817588806 and batch: 100, loss is 4.87596453666687 and perplexity is 131.10054352825168
At time: 969.7888042926788 and batch: 150, loss is 4.7925072860717775 and perplexity is 120.60337706179195
At time: 970.997475862503 and batch: 200, loss is 4.835180377960205 and perplexity is 125.86128376110506
At time: 972.2071154117584 and batch: 250, loss is 4.856738157272339 and perplexity is 128.60403109046814
At time: 973.4169073104858 and batch: 300, loss is 4.839758577346802 and perplexity is 126.43882284900191
At time: 974.6282458305359 and batch: 350, loss is 4.850719261169433 and perplexity is 127.83230159852825
At time: 975.8391809463501 and batch: 400, loss is 4.817433834075928 and perplexity is 123.64738360709772
At time: 977.0514140129089 and batch: 450, loss is 4.789716138839721 and perplexity is 120.26722462343626
At time: 978.2629220485687 and batch: 500, loss is 4.812278242111206 and perplexity is 123.01154861099319
At time: 979.4762105941772 and batch: 550, loss is 4.798968553543091 and perplexity is 121.38515064325084
At time: 980.6905617713928 and batch: 600, loss is 4.814609785079956 and perplexity is 123.29868993332052
At time: 981.9032719135284 and batch: 650, loss is 4.829425935745239 and perplexity is 125.1391021456232
At time: 983.1177024841309 and batch: 700, loss is 4.828668947219849 and perplexity is 125.04440912650759
At time: 984.3331940174103 and batch: 750, loss is 4.791816320419311 and perplexity is 120.5200730540892
At time: 985.5918045043945 and batch: 800, loss is 4.833217716217041 and perplexity is 125.61450288742897
At time: 986.8099579811096 and batch: 850, loss is 4.854031667709351 and perplexity is 128.2564362156941
At time: 988.0221469402313 and batch: 900, loss is 4.81133017539978 and perplexity is 122.89498072243278
At time: 989.231657743454 and batch: 950, loss is 4.788141860961914 and perplexity is 120.0780395462994
At time: 990.442932844162 and batch: 1000, loss is 4.783901414871216 and perplexity is 119.56993315318186
At time: 991.6641085147858 and batch: 1050, loss is 4.786794500350952 and perplexity is 119.91636007031114
At time: 992.8775110244751 and batch: 1100, loss is 4.73237603187561 and perplexity is 113.56507627621116
At time: 994.0865652561188 and batch: 1150, loss is 4.780019483566284 and perplexity is 119.10667064491939
At time: 995.2895636558533 and batch: 1200, loss is 4.811775197982788 and perplexity is 122.94968393537239
At time: 996.4924759864807 and batch: 1250, loss is 4.870293407440186 and perplexity is 130.35915963316546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945251019331661 and perplexity of 140.50611617649867
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 999.4913187026978 and batch: 50, loss is 4.861181545257568 and perplexity is 129.17674013903778
At time: 1000.7297649383545 and batch: 100, loss is 4.875498495101929 and perplexity is 131.03945946074393
At time: 1001.9433438777924 and batch: 150, loss is 4.791867504119873 and perplexity is 120.52624187529024
At time: 1003.1583476066589 and batch: 200, loss is 4.83481728553772 and perplexity is 125.81559277821398
At time: 1004.3711214065552 and batch: 250, loss is 4.856222639083862 and perplexity is 128.5377504592329
At time: 1005.5870110988617 and batch: 300, loss is 4.839311027526856 and perplexity is 126.38224783761403
At time: 1006.7991404533386 and batch: 350, loss is 4.850125932693482 and perplexity is 127.75647755035129
At time: 1008.013542175293 and batch: 400, loss is 4.81674467086792 and perplexity is 123.56219973571584
At time: 1009.2258741855621 and batch: 450, loss is 4.789258880615234 and perplexity is 120.21224401696618
At time: 1010.4365997314453 and batch: 500, loss is 4.811953315734863 and perplexity is 122.97158540715581
At time: 1011.6477658748627 and batch: 550, loss is 4.798286056518554 and perplexity is 121.30233390341785
At time: 1012.8606119155884 and batch: 600, loss is 4.813981313705444 and perplexity is 123.22122458111056
At time: 1014.0747425556183 and batch: 650, loss is 4.829140329360962 and perplexity is 125.10336672250314
At time: 1015.2894587516785 and batch: 700, loss is 4.828419351577759 and perplexity is 125.01320248160525
At time: 1016.5308916568756 and batch: 750, loss is 4.791325988769532 and perplexity is 120.46099273349624
At time: 1017.746102809906 and batch: 800, loss is 4.832664251327515 and perplexity is 125.54499890624638
At time: 1018.9617583751678 and batch: 850, loss is 4.853153715133667 and perplexity is 128.1438825627849
At time: 1020.1777355670929 and batch: 900, loss is 4.8104228496551515 and perplexity is 122.78352551327423
At time: 1021.3912606239319 and batch: 950, loss is 4.78723258972168 and perplexity is 119.96890566202651
At time: 1022.6051695346832 and batch: 1000, loss is 4.782885007858276 and perplexity is 119.4484631765137
At time: 1023.818665266037 and batch: 1050, loss is 4.78573058128357 and perplexity is 119.78884661236883
At time: 1025.0366077423096 and batch: 1100, loss is 4.731349906921387 and perplexity is 113.44860408524644
At time: 1026.256973028183 and batch: 1150, loss is 4.778915910720825 and perplexity is 118.97530025921054
At time: 1027.4713809490204 and batch: 1200, loss is 4.810644073486328 and perplexity is 122.81069115992705
At time: 1028.6861417293549 and batch: 1250, loss is 4.8695768547058105 and perplexity is 130.26578387920068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.944920003849225 and perplexity of 140.4596141735334
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 1031.7563700675964 and batch: 50, loss is 4.861043882369995 and perplexity is 129.15895851994557
At time: 1032.9677066802979 and batch: 100, loss is 4.875329284667969 and perplexity is 131.01728809280905
At time: 1034.1791455745697 and batch: 150, loss is 4.791669816970825 and perplexity is 120.50241774108864
At time: 1035.3908426761627 and batch: 200, loss is 4.834693088531494 and perplexity is 125.79996782856044
At time: 1036.6074273586273 and batch: 250, loss is 4.855975093841553 and perplexity is 128.5059354886344
At time: 1037.817667722702 and batch: 300, loss is 4.83901047706604 and perplexity is 126.34426930230653
At time: 1039.0301489830017 and batch: 350, loss is 4.849918165206909 and perplexity is 127.72993666537924
At time: 1040.2417750358582 and batch: 400, loss is 4.816502819061279 and perplexity is 123.5323196079043
At time: 1041.4578573703766 and batch: 450, loss is 4.789128074645996 and perplexity is 120.19652056625624
At time: 1042.670934677124 and batch: 500, loss is 4.811865434646607 and perplexity is 122.96077900525177
At time: 1043.8824980258942 and batch: 550, loss is 4.798031377792358 and perplexity is 121.27144471311148
At time: 1045.095329284668 and batch: 600, loss is 4.813750562667846 and perplexity is 123.19279443595332
At time: 1046.3088817596436 and batch: 650, loss is 4.829099140167236 and perplexity is 125.09821392181601
At time: 1047.5689356327057 and batch: 700, loss is 4.828534898757934 and perplexity is 125.02764823920386
At time: 1048.781424999237 and batch: 750, loss is 4.791386260986328 and perplexity is 120.46825340337189
At time: 1049.9919030666351 and batch: 800, loss is 4.8324787807464595 and perplexity is 125.5217161615495
At time: 1051.2023961544037 and batch: 850, loss is 4.852808361053467 and perplexity is 128.09963519103414
At time: 1052.4198253154755 and batch: 900, loss is 4.810017776489258 and perplexity is 122.73379927393754
At time: 1053.6383202075958 and batch: 950, loss is 4.786886348724365 and perplexity is 119.92737469875928
At time: 1054.8573014736176 and batch: 1000, loss is 4.78252914428711 and perplexity is 119.40596338235012
At time: 1056.074979543686 and batch: 1050, loss is 4.78540732383728 and perplexity is 119.75013023373488
At time: 1057.2862644195557 and batch: 1100, loss is 4.7310367202758785 and perplexity is 113.4130790607674
At time: 1058.497032403946 and batch: 1150, loss is 4.77863452911377 and perplexity is 118.94182750755287
At time: 1059.696153640747 and batch: 1200, loss is 4.810304956436157 and perplexity is 122.76905102145098
At time: 1060.8994717597961 and batch: 1250, loss is 4.869287796020508 and perplexity is 130.22813486462215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.944850503963275 and perplexity of 140.4498525855864
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1063.9162225723267 and batch: 50, loss is 4.860954809188843 and perplexity is 129.14745443299626
At time: 1065.1624655723572 and batch: 100, loss is 4.875271797180176 and perplexity is 131.00975645454866
At time: 1066.363329410553 and batch: 150, loss is 4.791535224914551 and perplexity is 120.48620016430182
At time: 1067.5672488212585 and batch: 200, loss is 4.834588270187378 and perplexity is 125.78678237529364
At time: 1068.777749300003 and batch: 250, loss is 4.8558886432647705 and perplexity is 128.49482655658514
At time: 1069.9949731826782 and batch: 300, loss is 4.838955640792847 and perplexity is 126.33734124339485
At time: 1071.211547613144 and batch: 350, loss is 4.849823036193848 and perplexity is 127.71778642049541
At time: 1072.4307906627655 and batch: 400, loss is 4.816429452896118 and perplexity is 123.52325684779528
At time: 1073.645695924759 and batch: 450, loss is 4.7890916919708255 and perplexity is 120.1921475748429
At time: 1074.8644716739655 and batch: 500, loss is 4.811864385604858 and perplexity is 122.9606500143288
At time: 1076.0845062732697 and batch: 550, loss is 4.797946939468384 and perplexity is 121.26120518788419
At time: 1077.3533577919006 and batch: 600, loss is 4.813645706176758 and perplexity is 123.17987754902298
At time: 1078.5682277679443 and batch: 650, loss is 4.829040184020996 and perplexity is 125.09083883062722
At time: 1079.7836391925812 and batch: 700, loss is 4.828525886535645 and perplexity is 125.02652146732298
At time: 1081.000064611435 and batch: 750, loss is 4.791394557952881 and perplexity is 120.46925292858758
At time: 1082.213751077652 and batch: 800, loss is 4.832428798675537 and perplexity is 125.51544248301708
At time: 1083.428612947464 and batch: 850, loss is 4.8527114295959475 and perplexity is 128.0872189084613
At time: 1084.6421039104462 and batch: 900, loss is 4.8098760604858395 and perplexity is 122.71640716281941
At time: 1085.8566641807556 and batch: 950, loss is 4.786763544082642 and perplexity is 119.91264796475075
At time: 1087.0723407268524 and batch: 1000, loss is 4.782399396896363 and perplexity is 119.39047177518118
At time: 1088.2885496616364 and batch: 1050, loss is 4.785326652526855 and perplexity is 119.74047022345349
At time: 1089.5046429634094 and batch: 1100, loss is 4.7309001922607425 and perplexity is 113.39759605514884
At time: 1090.7188484668732 and batch: 1150, loss is 4.778510732650757 and perplexity is 118.92710384139082
At time: 1091.9331209659576 and batch: 1200, loss is 4.810191707611084 and perplexity is 122.75514835791246
At time: 1093.1356785297394 and batch: 1250, loss is 4.86920241355896 and perplexity is 130.21701614058335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.944828228358805 and perplexity of 140.44672401506782
Annealing...
Model not improving. Stopping early with 140.42951812288277loss at 33 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9541e8860>
SETTINGS FOR THIS RUN
{'lr': 25.329360970367127, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 2.081824762075887, 'wordvec_source': '', 'dropout': 0.3259448805725369, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.901918649673462 and batch: 50, loss is 6.941042289733887 and perplexity is 1033.8472219320693
At time: 3.113784074783325 and batch: 100, loss is 6.2176776885986325 and perplexity is 501.537153096272
At time: 4.326061725616455 and batch: 150, loss is 6.030641202926636 and perplexity is 415.98167255252713
At time: 5.540638208389282 and batch: 200, loss is 6.0135580348968505 and perplexity is 408.9357424995442
At time: 6.756836652755737 and batch: 250, loss is 6.017294864654541 and perplexity is 410.46672447907065
At time: 7.976654052734375 and batch: 300, loss is 6.005830640792847 and perplexity is 405.78791277819903
At time: 9.192375898361206 and batch: 350, loss is 6.023666276931762 and perplexity is 413.0903263560145
At time: 10.41291856765747 and batch: 400, loss is 5.972710905075073 and perplexity is 392.5684453493102
At time: 11.680721521377563 and batch: 450, loss is 5.9450935363769535 and perplexity is 381.8750788385195
At time: 12.89844536781311 and batch: 500, loss is 5.951053409576416 and perplexity is 384.15780149836115
At time: 14.11324167251587 and batch: 550, loss is 5.955134859085083 and perplexity is 385.7289262206376
At time: 15.32919454574585 and batch: 600, loss is 5.966055526733398 and perplexity is 389.96442879007753
At time: 16.54555034637451 and batch: 650, loss is 5.9559933376312255 and perplexity is 386.06020840740376
At time: 17.761412620544434 and batch: 700, loss is 5.961451778411865 and perplexity is 388.173256921401
At time: 18.96601629257202 and batch: 750, loss is 5.925041646957397 and perplexity is 374.2940232429157
At time: 20.176720142364502 and batch: 800, loss is 5.932538776397705 and perplexity is 377.1106992832829
At time: 21.392064094543457 and batch: 850, loss is 5.969454803466797 and perplexity is 391.29228139032125
At time: 22.60405158996582 and batch: 900, loss is 5.9548046684265135 and perplexity is 385.60158315736135
At time: 23.81040048599243 and batch: 950, loss is 5.936908073425293 and perplexity is 378.76201285340176
At time: 25.017016410827637 and batch: 1000, loss is 5.933397274017334 and perplexity is 377.43458692944824
At time: 26.221089124679565 and batch: 1050, loss is 5.912082719802856 and perplexity is 369.4748672484765
At time: 27.43045949935913 and batch: 1100, loss is 5.929305238723755 and perplexity is 375.8932669972471
At time: 28.6486759185791 and batch: 1150, loss is 5.96593300819397 and perplexity is 389.91665384455115
At time: 29.86736488342285 and batch: 1200, loss is 5.945677556991577 and perplexity is 382.0981668944432
At time: 31.085880517959595 and batch: 1250, loss is 5.934903326034546 and perplexity is 378.0034513126103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.487779157875228 and perplexity of 241.71978882911796
Finished 1 epochs...
Completing Train Step...
At time: 34.184316873550415 and batch: 50, loss is 5.772215776443481 and perplexity is 321.24876005258204
At time: 35.392616987228394 and batch: 100, loss is 5.815273303985595 and perplexity is 335.38304750010076
At time: 36.59892201423645 and batch: 150, loss is 5.716560258865356 and perplexity is 303.8579306747869
At time: 37.805333852767944 and batch: 200, loss is 5.768310623168945 and perplexity is 319.99668077726994
At time: 39.01094603538513 and batch: 250, loss is 5.785646324157715 and perplexity is 325.5924103620072
At time: 40.21976375579834 and batch: 300, loss is 5.774464445114136 and perplexity is 321.97195488284086
At time: 41.42842507362366 and batch: 350, loss is 5.780983734130859 and perplexity is 324.07784009061794
At time: 42.63670372962952 and batch: 400, loss is 5.713713245391846 and perplexity is 302.99407334235366
At time: 43.848742723464966 and batch: 450, loss is 5.723602933883667 and perplexity is 306.00545661962445
At time: 45.058773040771484 and batch: 500, loss is 5.742710647583007 and perplexity is 311.9087407984844
At time: 46.26618456840515 and batch: 550, loss is 5.754158267974853 and perplexity is 315.49986938100324
At time: 47.47369980812073 and batch: 600, loss is 5.754240217208863 and perplexity is 315.5257254130556
At time: 48.67974281311035 and batch: 650, loss is 5.706878728866577 and perplexity is 300.9303157669397
At time: 49.88482713699341 and batch: 700, loss is 5.777823982238769 and perplexity is 323.05545062136775
At time: 51.13817048072815 and batch: 750, loss is 5.7187886428833 and perplexity is 304.5357978248832
At time: 52.34363865852356 and batch: 800, loss is 5.718715677261352 and perplexity is 304.51357799164134
At time: 53.552295446395874 and batch: 850, loss is 5.769905099868774 and perplexity is 320.5073150178067
At time: 54.75855112075806 and batch: 900, loss is 5.754139184951782 and perplexity is 315.49384874716304
At time: 55.96218729019165 and batch: 950, loss is 5.724155874252319 and perplexity is 306.17470617776155
At time: 57.167558670043945 and batch: 1000, loss is 5.715987501144409 and perplexity is 303.6839435299185
At time: 58.37217307090759 and batch: 1050, loss is 5.762196435928344 and perplexity is 318.04613024670516
At time: 59.580305099487305 and batch: 1100, loss is 5.7591584777832034 and perplexity is 317.08138558372923
At time: 60.7887065410614 and batch: 1150, loss is 5.809155673980713 and perplexity is 333.33756124904113
At time: 61.99467420578003 and batch: 1200, loss is 5.787959117889404 and perplexity is 326.34630991881096
At time: 63.20073437690735 and batch: 1250, loss is 5.772741641998291 and perplexity is 321.4177381360018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.602390261462135 and perplexity of 271.0735703823817
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 66.2578763961792 and batch: 50, loss is 5.702635707855225 and perplexity is 299.656167145439
At time: 67.4669439792633 and batch: 100, loss is 5.702099466323853 and perplexity is 299.4955221395953
At time: 68.67381405830383 and batch: 150, loss is 5.6166051197052 and perplexity is 274.9543598483574
At time: 69.8832175731659 and batch: 200, loss is 5.661139478683472 and perplexity is 287.47602880263594
At time: 71.09180569648743 and batch: 250, loss is 5.669612264633178 and perplexity is 289.9220995440434
At time: 72.3012809753418 and batch: 300, loss is 5.644700565338135 and perplexity is 282.7888667552368
At time: 73.51201868057251 and batch: 350, loss is 5.685946683883667 and perplexity is 294.6966976515663
At time: 74.72162246704102 and batch: 400, loss is 5.637361307144165 and perplexity is 280.721003819798
At time: 75.93299078941345 and batch: 450, loss is 5.626100358963012 and perplexity is 277.57755148742285
At time: 77.14244651794434 and batch: 500, loss is 5.641285543441772 and perplexity is 281.8247837025456
At time: 78.35162377357483 and batch: 550, loss is 5.637119550704956 and perplexity is 280.6531459123769
At time: 79.56024408340454 and batch: 600, loss is 5.649517850875855 and perplexity is 284.15442798166436
At time: 80.81843280792236 and batch: 650, loss is 5.6385025215148925 and perplexity is 281.04154953442116
At time: 82.0278217792511 and batch: 700, loss is 5.651993656158448 and perplexity is 284.8588106128539
At time: 83.23765277862549 and batch: 750, loss is 5.616917762756348 and perplexity is 275.0403358575473
At time: 84.44774651527405 and batch: 800, loss is 5.618232545852661 and perplexity is 275.40219207102643
At time: 85.65659666061401 and batch: 850, loss is 5.657666969299316 and perplexity is 286.479496829588
At time: 86.86337566375732 and batch: 900, loss is 5.619976873397827 and perplexity is 275.88300292455034
At time: 88.0688898563385 and batch: 950, loss is 5.629574251174927 and perplexity is 278.5435028157166
At time: 89.27322149276733 and batch: 1000, loss is 5.606957111358643 and perplexity is 272.31435376847463
At time: 90.46960186958313 and batch: 1050, loss is 5.610341520309448 and perplexity is 273.23753824049993
At time: 91.66989922523499 and batch: 1100, loss is 5.585882720947265 and perplexity is 266.63554366899587
At time: 92.86975264549255 and batch: 1150, loss is 5.615069313049316 and perplexity is 274.5324072142084
At time: 94.06934237480164 and batch: 1200, loss is 5.593847150802612 and perplexity is 268.7676228809707
At time: 95.26869058609009 and batch: 1250, loss is 5.586262941360474 and perplexity is 266.736943221454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.370923564381843 and perplexity of 215.06139906006038
Finished 3 epochs...
Completing Train Step...
At time: 98.22428274154663 and batch: 50, loss is 5.568085966110229 and perplexity is 261.93227189098303
At time: 99.44271230697632 and batch: 100, loss is 5.605117864608765 and perplexity is 271.8139607925002
At time: 100.64956545829773 and batch: 150, loss is 5.5272556591033934 and perplexity is 251.4528908078758
At time: 101.85864305496216 and batch: 200, loss is 5.588240556716919 and perplexity is 267.2649682396017
At time: 103.06717658042908 and batch: 250, loss is 5.592857284545898 and perplexity is 268.5017105107373
At time: 104.27446794509888 and batch: 300, loss is 5.551950197219849 and perplexity is 257.73970936212817
At time: 105.47927784919739 and batch: 350, loss is 5.592218627929688 and perplexity is 268.3302848637444
At time: 106.68497896194458 and batch: 400, loss is 5.543346176147461 and perplexity is 255.53162429866984
At time: 107.89110708236694 and batch: 450, loss is 5.551436109542847 and perplexity is 257.60724260635175
At time: 109.0966899394989 and batch: 500, loss is 5.551563711166382 and perplexity is 257.6401158060349
At time: 110.30396032333374 and batch: 550, loss is 5.570250425338745 and perplexity is 262.49982761816165
At time: 111.54072904586792 and batch: 600, loss is 5.586039018630982 and perplexity is 266.6772214438528
At time: 112.74535393714905 and batch: 650, loss is 5.56833927154541 and perplexity is 261.9986291630763
At time: 113.94965291023254 and batch: 700, loss is 5.570265064239502 and perplexity is 262.5036703552134
At time: 115.15529131889343 and batch: 750, loss is 5.544135293960571 and perplexity is 255.73334843680442
At time: 116.36189937591553 and batch: 800, loss is 5.565252103805542 and perplexity is 261.19104266612356
At time: 117.57035040855408 and batch: 850, loss is 5.58173376083374 and perplexity is 265.5315751777139
At time: 118.77854919433594 and batch: 900, loss is 5.557568168640136 and perplexity is 259.19175864981577
At time: 119.98617887496948 and batch: 950, loss is 5.547335052490235 and perplexity is 256.5529439543761
At time: 121.19264841079712 and batch: 1000, loss is 5.529874658584594 and perplexity is 252.1123089293861
At time: 122.40003538131714 and batch: 1050, loss is 5.529221534729004 and perplexity is 251.94770212630266
At time: 123.6080379486084 and batch: 1100, loss is 5.4943813228607175 and perplexity is 243.32094247951164
At time: 124.81581687927246 and batch: 1150, loss is 5.540430068969727 and perplexity is 254.78755211930522
At time: 126.02217650413513 and batch: 1200, loss is 5.543413095474243 and perplexity is 255.54872487511258
At time: 127.23281645774841 and batch: 1250, loss is 5.5539852905273435 and perplexity is 258.26476780997785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.371759345061587 and perplexity of 215.2412183566331
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 130.3615026473999 and batch: 50, loss is 5.487007570266724 and perplexity is 241.53335277045295
At time: 131.57591819763184 and batch: 100, loss is 5.4827043819427494 and perplexity is 240.4962223511315
At time: 132.7906138896942 and batch: 150, loss is 5.380714101791382 and perplexity is 217.17730676809003
At time: 134.00444722175598 and batch: 200, loss is 5.436322669982911 and perplexity is 229.59632770077812
At time: 135.2186152935028 and batch: 250, loss is 5.459764738082885 and perplexity is 235.04212140006956
At time: 136.43285417556763 and batch: 300, loss is 5.42748987197876 and perplexity is 227.5772797638731
At time: 137.64701056480408 and batch: 350, loss is 5.476780662536621 and perplexity is 239.0758014527084
At time: 138.86402368545532 and batch: 400, loss is 5.434674348831177 and perplexity is 229.2181909484873
At time: 140.07965755462646 and batch: 450, loss is 5.404820251464844 and perplexity is 222.47622711361782
At time: 141.2974455356598 and batch: 500, loss is 5.396611204147339 and perplexity is 220.65738493305327
At time: 142.54137682914734 and batch: 550, loss is 5.398544588088989 and perplexity is 221.08441304932236
At time: 143.75984740257263 and batch: 600, loss is 5.418094129562378 and perplexity is 225.4490361346123
At time: 144.9823248386383 and batch: 650, loss is 5.414378986358643 and perplexity is 224.61301461165183
At time: 146.19683170318604 and batch: 700, loss is 5.429498348236084 and perplexity is 228.03482265519244
At time: 147.4090702533722 and batch: 750, loss is 5.384765825271606 and perplexity is 218.05903421295315
At time: 148.6213777065277 and batch: 800, loss is 5.397923984527588 and perplexity is 220.9472498416036
At time: 149.83293104171753 and batch: 850, loss is 5.417284517288208 and perplexity is 225.26658369559112
At time: 151.0441439151764 and batch: 900, loss is 5.3942662048339844 and perplexity is 220.14054974282237
At time: 152.25497817993164 and batch: 950, loss is 5.400147485733032 and perplexity is 221.43907289991813
At time: 153.464182138443 and batch: 1000, loss is 5.374378080368042 and perplexity is 215.80561681709256
At time: 154.666761636734 and batch: 1050, loss is 5.378826723098755 and perplexity is 216.76779751785332
At time: 155.8701183795929 and batch: 1100, loss is 5.344071722030639 and perplexity is 209.36344689137405
At time: 157.08269119262695 and batch: 1150, loss is 5.37304277420044 and perplexity is 215.51764255564635
At time: 158.28917264938354 and batch: 1200, loss is 5.372892255783081 and perplexity is 215.4852056224152
At time: 159.49683570861816 and batch: 1250, loss is 5.395809106826782 and perplexity is 220.4804671979431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.260318951015055 and perplexity of 192.5428932558622
Finished 5 epochs...
Completing Train Step...
At time: 162.56511068344116 and batch: 50, loss is 5.379487581253052 and perplexity is 216.91109762974037
At time: 163.76826357841492 and batch: 100, loss is 5.398774080276489 and perplexity is 221.13515601722978
At time: 164.97271943092346 and batch: 150, loss is 5.3095071983337405 and perplexity is 202.25053442512302
At time: 166.17924857139587 and batch: 200, loss is 5.370628786087036 and perplexity is 214.99801297044743
At time: 167.38595461845398 and batch: 250, loss is 5.397056589126587 and perplexity is 220.7556843067698
At time: 168.5900330543518 and batch: 300, loss is 5.367101182937622 and perplexity is 214.24092144709894
At time: 169.79493141174316 and batch: 350, loss is 5.4088685131073 and perplexity is 223.37869456989253
At time: 170.98995327949524 and batch: 400, loss is 5.369261589050293 and perplexity is 214.7042691727895
At time: 172.18654012680054 and batch: 450, loss is 5.33847674369812 and perplexity is 208.1953337733828
At time: 173.41476845741272 and batch: 500, loss is 5.33702691078186 and perplexity is 207.89370403466424
At time: 174.61259126663208 and batch: 550, loss is 5.326453790664673 and perplexity is 205.70719839293474
At time: 175.80413913726807 and batch: 600, loss is 5.353938493728638 and perplexity is 211.43941292340483
At time: 176.99451971054077 and batch: 650, loss is 5.359469528198242 and perplexity is 212.61213178906846
At time: 178.18359446525574 and batch: 700, loss is 5.3774929618835445 and perplexity is 216.47887375730136
At time: 179.3722243309021 and batch: 750, loss is 5.336059732437134 and perplexity is 207.6927309501909
At time: 180.5719108581543 and batch: 800, loss is 5.34753038406372 and perplexity is 210.08881797987942
At time: 181.77834367752075 and batch: 850, loss is 5.373784036636352 and perplexity is 215.6774569132215
At time: 182.98529624938965 and batch: 900, loss is 5.3587626934051515 and perplexity is 212.4619032365254
At time: 184.19243741035461 and batch: 950, loss is 5.3625551700592045 and perplexity is 213.2691898847168
At time: 185.39869236946106 and batch: 1000, loss is 5.337418947219849 and perplexity is 207.9752219198208
At time: 186.60254073143005 and batch: 1050, loss is 5.347393074035645 and perplexity is 210.05997265880544
At time: 187.8099114894867 and batch: 1100, loss is 5.31021936416626 and perplexity is 202.39462164625743
At time: 189.02112746238708 and batch: 1150, loss is 5.339772891998291 and perplexity is 208.46536076106352
At time: 190.22851967811584 and batch: 1200, loss is 5.343868150711059 and perplexity is 209.32083083606904
At time: 191.43510055541992 and batch: 1250, loss is 5.3635822105407716 and perplexity is 213.48833849418273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.256499575872491 and perplexity of 191.80890230115227
Finished 6 epochs...
Completing Train Step...
At time: 194.443834066391 and batch: 50, loss is 5.33787425994873 and perplexity is 208.0699372465635
At time: 195.69219756126404 and batch: 100, loss is 5.3579749011993405 and perplexity is 212.2945933165023
At time: 196.9010787010193 and batch: 150, loss is 5.271651954650879 and perplexity is 194.73739422102017
At time: 198.10986137390137 and batch: 200, loss is 5.3331278038024905 and perplexity is 207.08468249802058
At time: 199.31674146652222 and batch: 250, loss is 5.359394254684449 and perplexity is 212.59612832915943
At time: 200.52524209022522 and batch: 300, loss is 5.330745258331299 and perplexity is 206.59188111954452
At time: 201.73099637031555 and batch: 350, loss is 5.367798128128052 and perplexity is 214.3902876708843
At time: 202.96586155891418 and batch: 400, loss is 5.336573667526245 and perplexity is 207.79949896594442
At time: 204.1723325252533 and batch: 450, loss is 5.3075204277038575 and perplexity is 201.84910790670196
At time: 205.37891817092896 and batch: 500, loss is 5.303368091583252 and perplexity is 201.01270028957973
At time: 206.5854434967041 and batch: 550, loss is 5.290626029968262 and perplexity is 198.46763321254218
At time: 207.7911515235901 and batch: 600, loss is 5.318185539245605 and perplexity is 204.01337170010527
At time: 208.9967896938324 and batch: 650, loss is 5.321927251815796 and perplexity is 204.7781610161009
At time: 210.20563793182373 and batch: 700, loss is 5.34451717376709 and perplexity is 209.45672897713226
At time: 211.41416001319885 and batch: 750, loss is 5.305453681945801 and perplexity is 201.43236791531294
At time: 212.62133359909058 and batch: 800, loss is 5.316237325668335 and perplexity is 203.61629699816982
At time: 213.82749843597412 and batch: 850, loss is 5.340910749435425 and perplexity is 208.70269962547897
At time: 215.03329420089722 and batch: 900, loss is 5.3244815158844 and perplexity is 205.30188709743072
At time: 216.2409222126007 and batch: 950, loss is 5.330476560592651 and perplexity is 206.5363778054062
At time: 217.44947171211243 and batch: 1000, loss is 5.309394149780274 and perplexity is 202.22767158709811
At time: 218.656884431839 and batch: 1050, loss is 5.322134218215942 and perplexity is 204.82054760106308
At time: 219.864111661911 and batch: 1100, loss is 5.283238582611084 and perplexity is 197.0068663326582
At time: 221.0724959373474 and batch: 1150, loss is 5.312328872680664 and perplexity is 202.82202547139735
At time: 222.28558444976807 and batch: 1200, loss is 5.315401945114136 and perplexity is 203.44627093126238
At time: 223.49985671043396 and batch: 1250, loss is 5.337335147857666 and perplexity is 207.95779445908934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.252270329607664 and perplexity of 190.99940819907437
Finished 7 epochs...
Completing Train Step...
At time: 226.58155155181885 and batch: 50, loss is 5.3026178932189945 and perplexity is 200.86195744121645
At time: 227.7912735939026 and batch: 100, loss is 5.317683582305908 and perplexity is 203.9109914697726
At time: 228.9986457824707 and batch: 150, loss is 5.239191341400146 and perplexity is 188.51759437659575
At time: 230.2104091644287 and batch: 200, loss is 5.300759344100952 and perplexity is 200.48899232172096
At time: 231.42034149169922 and batch: 250, loss is 5.32567419052124 and perplexity is 205.5468915273093
At time: 232.63008165359497 and batch: 300, loss is 5.297027206420898 and perplexity is 199.7421343543278
At time: 233.86727023124695 and batch: 350, loss is 5.330905447006225 and perplexity is 206.62497744998953
At time: 235.07284116744995 and batch: 400, loss is 5.288297643661499 and perplexity is 198.00606146023804
At time: 236.287264585495 and batch: 450, loss is 5.259580793380738 and perplexity is 192.40081869243474
At time: 237.4975643157959 and batch: 500, loss is 5.266581077575683 and perplexity is 193.75240432497446
At time: 238.7070472240448 and batch: 550, loss is 5.255238695144653 and perplexity is 191.5672065595854
At time: 239.91805839538574 and batch: 600, loss is 5.284249429702759 and perplexity is 197.20611083642828
At time: 241.12795066833496 and batch: 650, loss is 5.291187572479248 and perplexity is 198.5791125229023
At time: 242.33864831924438 and batch: 700, loss is 5.3164778995513915 and perplexity is 203.6652876540921
At time: 243.54906916618347 and batch: 750, loss is 5.273223381042481 and perplexity is 195.04365026812653
At time: 244.75996851921082 and batch: 800, loss is 5.28950942993164 and perplexity is 198.24614792428156
At time: 245.96897959709167 and batch: 850, loss is 5.321260232925415 and perplexity is 204.64161565859453
At time: 247.1785430908203 and batch: 900, loss is 5.301270942687989 and perplexity is 200.59158844868816
At time: 248.39596915245056 and batch: 950, loss is 5.301249809265137 and perplexity is 200.5873493066229
At time: 249.60590744018555 and batch: 1000, loss is 5.282658061981201 and perplexity is 196.8925329721717
At time: 250.81523442268372 and batch: 1050, loss is 5.294154920578003 and perplexity is 199.16924100019284
At time: 252.02773094177246 and batch: 1100, loss is 5.258827409744263 and perplexity is 192.2559216523704
At time: 253.23826575279236 and batch: 1150, loss is 5.288745260238647 and perplexity is 198.0947120949901
At time: 254.44828462600708 and batch: 1200, loss is 5.292897052764893 and perplexity is 198.91886992244923
At time: 255.6573703289032 and batch: 1250, loss is 5.3135269927978515 and perplexity is 203.06517625315823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.254950975849681 and perplexity of 191.51209690640974
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 258.6976923942566 and batch: 50, loss is 5.2653985023498535 and perplexity is 193.52341295812022
At time: 259.92876744270325 and batch: 100, loss is 5.264716358184814 and perplexity is 193.3914471061575
At time: 261.136812210083 and batch: 150, loss is 5.173202114105225 and perplexity is 176.47903970434638
At time: 262.33828234672546 and batch: 200, loss is 5.234053792953492 and perplexity is 187.55155975210616
At time: 263.52954602241516 and batch: 250, loss is 5.251299562454224 and perplexity is 190.81408221601083
At time: 264.7485029697418 and batch: 300, loss is 5.233588151931762 and perplexity is 187.46424838165174
At time: 265.93886399269104 and batch: 350, loss is 5.274071617126465 and perplexity is 195.20916351746715
At time: 267.13810992240906 and batch: 400, loss is 5.226280097961426 and perplexity is 186.09924337722845
At time: 268.3460578918457 and batch: 450, loss is 5.194975509643554 and perplexity is 180.36372557099114
At time: 269.55535078048706 and batch: 500, loss is 5.1906742668151855 and perplexity is 179.5896034270235
At time: 270.7633261680603 and batch: 550, loss is 5.176195249557495 and perplexity is 177.00805668946563
At time: 271.96931624412537 and batch: 600, loss is 5.204052476882935 and perplexity is 182.00833393552057
At time: 273.17550921440125 and batch: 650, loss is 5.206816730499267 and perplexity is 182.5121471437387
At time: 274.3805730342865 and batch: 700, loss is 5.226080913543701 and perplexity is 186.06217899924312
At time: 275.5886883735657 and batch: 750, loss is 5.1877254867553715 and perplexity is 179.0608132116753
At time: 276.7957878112793 and batch: 800, loss is 5.196761798858643 and perplexity is 180.68619527510361
At time: 278.00295424461365 and batch: 850, loss is 5.219810180664062 and perplexity is 184.8990833181771
At time: 279.2079391479492 and batch: 900, loss is 5.203999805450439 and perplexity is 181.99874754831265
At time: 280.41358065605164 and batch: 950, loss is 5.193747053146362 and perplexity is 180.14229261867726
At time: 281.62059926986694 and batch: 1000, loss is 5.1775302314758305 and perplexity is 177.2445170445771
At time: 282.8275272846222 and batch: 1050, loss is 5.1848970317840575 and perplexity is 178.555063346912
At time: 284.0337655544281 and batch: 1100, loss is 5.143166484832764 and perplexity is 171.25719396808597
At time: 285.2408359050751 and batch: 1150, loss is 5.177922878265381 and perplexity is 177.31412519997517
At time: 286.4487738609314 and batch: 1200, loss is 5.186648778915405 and perplexity is 178.86812078559814
At time: 287.6541359424591 and batch: 1250, loss is 5.231356763839722 and perplexity is 187.0464092441392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.178432937956204 and perplexity of 177.4045890568754
Finished 9 epochs...
Completing Train Step...
At time: 290.7139804363251 and batch: 50, loss is 5.205225658416748 and perplexity is 182.2219880549223
At time: 291.9229254722595 and batch: 100, loss is 5.219487457275391 and perplexity is 184.83942168706614
At time: 293.1267092227936 and batch: 150, loss is 5.134408369064331 and perplexity is 169.76385261087992
At time: 294.3301408290863 and batch: 200, loss is 5.1949428367614745 and perplexity is 180.35783266452373
At time: 295.5800836086273 and batch: 250, loss is 5.216891670227051 and perplexity is 184.3602401059641
At time: 296.7849931716919 and batch: 300, loss is 5.200539598464966 and perplexity is 181.37008249355765
At time: 297.9924945831299 and batch: 350, loss is 5.236167583465576 and perplexity is 187.94842375555007
At time: 299.19720339775085 and batch: 400, loss is 5.196918973922729 and perplexity is 180.71459687137843
At time: 300.4014103412628 and batch: 450, loss is 5.166010046005249 and perplexity is 175.21434377365475
At time: 301.60462379455566 and batch: 500, loss is 5.167047214508057 and perplexity is 175.39616484550228
At time: 302.8101556301117 and batch: 550, loss is 5.152742080688476 and perplexity is 172.90496022621628
At time: 304.0167398452759 and batch: 600, loss is 5.1824366569519045 and perplexity is 178.1162909565133
At time: 305.22218585014343 and batch: 650, loss is 5.186957197189331 and perplexity is 178.92329549067753
At time: 306.42748856544495 and batch: 700, loss is 5.204315271377563 and perplexity is 182.05617100903982
At time: 307.63815450668335 and batch: 750, loss is 5.16784031867981 and perplexity is 175.53532745347854
At time: 308.84084725379944 and batch: 800, loss is 5.183236875534058 and perplexity is 178.25887996589944
At time: 310.04697585105896 and batch: 850, loss is 5.207198572158814 and perplexity is 182.58185119199928
At time: 311.25408840179443 and batch: 900, loss is 5.191228647232055 and perplexity is 179.68919198866038
At time: 312.4585020542145 and batch: 950, loss is 5.182023916244507 and perplexity is 178.04279028198852
At time: 313.6602051258087 and batch: 1000, loss is 5.165257835388184 and perplexity is 175.08259524154056
At time: 314.8626518249512 and batch: 1050, loss is 5.176011209487915 and perplexity is 176.97548311190988
At time: 316.0658462047577 and batch: 1100, loss is 5.137940292358398 and perplexity is 170.36450562218528
At time: 317.2732584476471 and batch: 1150, loss is 5.172145681381226 and perplexity is 176.29269991675787
At time: 318.4797751903534 and batch: 1200, loss is 5.182519235610962 and perplexity is 178.1310001683019
At time: 319.68393778800964 and batch: 1250, loss is 5.221658735275269 and perplexity is 185.24119548033943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.177834615220118 and perplexity of 177.29847560596747
Finished 10 epochs...
Completing Train Step...
At time: 322.70419049263 and batch: 50, loss is 5.183227701187134 and perplexity is 178.25724456459426
At time: 323.93864011764526 and batch: 100, loss is 5.20088415145874 and perplexity is 181.43258486553327
At time: 325.14295959472656 and batch: 150, loss is 5.118462724685669 and perplexity is 167.07832671165144
At time: 326.3751437664032 and batch: 200, loss is 5.178402481079101 and perplexity is 177.3991859493903
At time: 327.57522344589233 and batch: 250, loss is 5.199576568603516 and perplexity is 181.1955017648628
At time: 328.7813651561737 and batch: 300, loss is 5.183360872268676 and perplexity is 178.28098485537075
At time: 329.9773335456848 and batch: 350, loss is 5.215274648666382 and perplexity is 184.06236652169227
At time: 331.18077516555786 and batch: 400, loss is 5.179735174179077 and perplexity is 177.63576222721832
At time: 332.38066816329956 and batch: 450, loss is 5.150511960983277 and perplexity is 172.51979111353748
At time: 333.5727903842926 and batch: 500, loss is 5.150462617874146 and perplexity is 172.51127866067446
At time: 334.76333117485046 and batch: 550, loss is 5.1373807811737064 and perplexity is 170.2692114373823
At time: 335.954377412796 and batch: 600, loss is 5.167483463287353 and perplexity is 175.4726979008216
At time: 337.15022444725037 and batch: 650, loss is 5.173487634658813 and perplexity is 176.52943529160518
At time: 338.34860372543335 and batch: 700, loss is 5.192735805511474 and perplexity is 179.96021622899886
At time: 339.55647706985474 and batch: 750, loss is 5.155337553024292 and perplexity is 173.354313156504
At time: 340.7644681930542 and batch: 800, loss is 5.171922359466553 and perplexity is 176.25333428923688
At time: 341.971394777298 and batch: 850, loss is 5.197089166641235 and perplexity is 180.745355797292
At time: 343.1759126186371 and batch: 900, loss is 5.180767803192139 and perplexity is 177.81928881018035
At time: 344.3801248073578 and batch: 950, loss is 5.171860809326172 and perplexity is 176.24248620562273
At time: 345.582932472229 and batch: 1000, loss is 5.15382532119751 and perplexity is 173.09235936419026
At time: 346.7894847393036 and batch: 1050, loss is 5.165480794906617 and perplexity is 175.1216359247458
At time: 347.9982326030731 and batch: 1100, loss is 5.129831781387329 and perplexity is 168.98868861101909
At time: 349.2052044868469 and batch: 1150, loss is 5.164561328887939 and perplexity is 174.96069153416553
At time: 350.408677816391 and batch: 1200, loss is 5.174434385299683 and perplexity is 176.69664378748553
At time: 351.6168830394745 and batch: 1250, loss is 5.210607614517212 and perplexity is 183.20534260708064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.177594929716013 and perplexity of 177.2559848238804
Finished 11 epochs...
Completing Train Step...
At time: 354.6865122318268 and batch: 50, loss is 5.166935043334961 and perplexity is 175.37649155534368
At time: 355.91825342178345 and batch: 100, loss is 5.184953746795654 and perplexity is 178.56519038657518
At time: 357.13610887527466 and batch: 150, loss is 5.1046174621582034 and perplexity is 164.78102349637436
At time: 358.3472354412079 and batch: 200, loss is 5.165891971588135 and perplexity is 175.19365666347682
At time: 359.55815076828003 and batch: 250, loss is 5.186516828536988 and perplexity is 178.84452062643288
At time: 360.76947116851807 and batch: 300, loss is 5.169564247131348 and perplexity is 175.83819878797163
At time: 361.9783089160919 and batch: 350, loss is 5.1989170265197755 and perplexity is 181.07603510703925
At time: 363.1857645511627 and batch: 400, loss is 5.16674506187439 and perplexity is 175.34317643805568
At time: 364.3936140537262 and batch: 450, loss is 5.138808994293213 and perplexity is 170.5125658986563
At time: 365.6049211025238 and batch: 500, loss is 5.13881814956665 and perplexity is 170.5141269949677
At time: 366.81314039230347 and batch: 550, loss is 5.127041893005371 and perplexity is 168.51788608060912
At time: 368.02939891815186 and batch: 600, loss is 5.157230110168457 and perplexity is 173.68270675414544
At time: 369.23668217658997 and batch: 650, loss is 5.162601642608642 and perplexity is 174.6181592051234
At time: 370.45033288002014 and batch: 700, loss is 5.183011856079101 and perplexity is 178.21877276250817
At time: 371.6514036655426 and batch: 750, loss is 5.145477647781372 and perplexity is 171.65345498497288
At time: 372.84932923316956 and batch: 800, loss is 5.1609800434112545 and perplexity is 174.3352280008132
At time: 374.05049753189087 and batch: 850, loss is 5.188024168014526 and perplexity is 179.11430330867773
At time: 375.24969482421875 and batch: 900, loss is 5.169864358901978 and perplexity is 175.89097782056265
At time: 376.45015692710876 and batch: 950, loss is 5.161560201644898 and perplexity is 174.43639936361382
At time: 377.6494402885437 and batch: 1000, loss is 5.144075632095337 and perplexity is 171.41296277484088
At time: 378.8549566268921 and batch: 1050, loss is 5.15554913520813 and perplexity is 173.390995721211
At time: 380.06208324432373 and batch: 1100, loss is 5.12164571762085 and perplexity is 167.61098311626807
At time: 381.26866841316223 and batch: 1150, loss is 5.155936365127563 and perplexity is 173.45815090392347
At time: 382.47467374801636 and batch: 1200, loss is 5.165017137527466 and perplexity is 175.04045830675537
At time: 383.68097400665283 and batch: 1250, loss is 5.199331655502319 and perplexity is 181.15113004643203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.178303739450274 and perplexity of 177.38167012960156
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 386.708114862442 and batch: 50, loss is 5.152746829986572 and perplexity is 172.90578140536468
At time: 387.91283321380615 and batch: 100, loss is 5.1676863670349125 and perplexity is 175.50830558116374
At time: 389.11770367622375 and batch: 150, loss is 5.085127878189087 and perplexity is 161.60060310882346
At time: 390.3206579685211 and batch: 200, loss is 5.140211133956909 and perplexity is 170.75181602230833
At time: 391.5380964279175 and batch: 250, loss is 5.161196241378784 and perplexity is 174.37292299741978
At time: 392.7476441860199 and batch: 300, loss is 5.139453029632568 and perplexity is 170.62241738722244
At time: 393.95209670066833 and batch: 350, loss is 5.163178520202637 and perplexity is 174.7189215696542
At time: 395.15570163726807 and batch: 400, loss is 5.129420900344849 and perplexity is 168.91926862512994
At time: 396.3658826351166 and batch: 450, loss is 5.100677337646484 and perplexity is 164.13304314656156
At time: 397.57748651504517 and batch: 500, loss is 5.094628438949585 and perplexity is 163.14321569681871
At time: 398.78560495376587 and batch: 550, loss is 5.086596298217773 and perplexity is 161.8380749826333
At time: 399.990149974823 and batch: 600, loss is 5.116241436004639 and perplexity is 166.70760940319244
At time: 401.19208121299744 and batch: 650, loss is 5.122640829086304 and perplexity is 167.7778577429597
At time: 402.395884513855 and batch: 700, loss is 5.137621669769287 and perplexity is 170.31023228912542
At time: 403.59908175468445 and batch: 750, loss is 5.099780330657959 and perplexity is 163.98588067256247
At time: 404.8061857223511 and batch: 800, loss is 5.109130706787109 and perplexity is 165.52640133664224
At time: 406.0103211402893 and batch: 850, loss is 5.138045072555542 and perplexity is 170.3823573839099
At time: 407.2148780822754 and batch: 900, loss is 5.116364212036133 and perplexity is 166.72807835841755
At time: 408.4071567058563 and batch: 950, loss is 5.101842565536499 and perplexity is 164.32440701571554
At time: 409.60465240478516 and batch: 1000, loss is 5.085685863494873 and perplexity is 161.69079903242414
At time: 410.8058166503906 and batch: 1050, loss is 5.093022012710572 and perplexity is 162.88134854585215
At time: 412.0049774646759 and batch: 1100, loss is 5.057415914535523 and perplexity is 157.1838146630331
At time: 413.19531178474426 and batch: 1150, loss is 5.09436339378357 and perplexity is 163.09998110594282
At time: 414.3857033252716 and batch: 1200, loss is 5.108232593536377 and perplexity is 165.377806619686
At time: 415.5750141143799 and batch: 1250, loss is 5.1492266178131105 and perplexity is 172.29818642785534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.128751211792883 and perplexity of 168.80618319496062
Finished 13 epochs...
Completing Train Step...
At time: 418.54451394081116 and batch: 50, loss is 5.118948469161987 and perplexity is 167.1595038000291
At time: 419.77850699424744 and batch: 100, loss is 5.139197673797607 and perplexity is 170.57885351974812
At time: 420.9863831996918 and batch: 150, loss is 5.0609807205200195 and perplexity is 157.74514438754989
At time: 422.19374918937683 and batch: 200, loss is 5.11534969329834 and perplexity is 166.55901537209115
At time: 423.40022563934326 and batch: 250, loss is 5.1407075786590575 and perplexity is 170.83660590175657
At time: 424.614777803421 and batch: 300, loss is 5.120553007125855 and perplexity is 167.42793286461975
At time: 425.83217787742615 and batch: 350, loss is 5.14577467918396 and perplexity is 171.70444902450674
At time: 427.0488131046295 and batch: 400, loss is 5.114130353927612 and perplexity is 166.35604717580222
At time: 428.2635781764984 and batch: 450, loss is 5.085289993286133 and perplexity is 161.62680312993126
At time: 429.4756088256836 and batch: 500, loss is 5.081164846420288 and perplexity is 160.9614421290151
At time: 430.6867434978485 and batch: 550, loss is 5.073391923904419 and perplexity is 159.71515124742535
At time: 431.89463663101196 and batch: 600, loss is 5.105708341598511 and perplexity is 164.9608778089095
At time: 433.10546040534973 and batch: 650, loss is 5.114236307144165 and perplexity is 166.37367406789005
At time: 434.31392097473145 and batch: 700, loss is 5.129375171661377 and perplexity is 168.9115443459746
At time: 435.5217568874359 and batch: 750, loss is 5.091806879043579 and perplexity is 162.68354613803032
At time: 436.72949862480164 and batch: 800, loss is 5.105144758224487 and perplexity is 164.86793479384104
At time: 437.9505615234375 and batch: 850, loss is 5.134335021972657 and perplexity is 169.7514013826558
At time: 439.1628313064575 and batch: 900, loss is 5.111783027648926 and perplexity is 165.96601320223462
At time: 440.3722605705261 and batch: 950, loss is 5.098114233016968 and perplexity is 163.71289165994187
At time: 441.5817141532898 and batch: 1000, loss is 5.084365606307983 and perplexity is 161.47746645087267
At time: 442.7906799316406 and batch: 1050, loss is 5.093103485107422 and perplexity is 162.89461942031804
At time: 444.0005736351013 and batch: 1100, loss is 5.05850769996643 and perplexity is 157.3555193771534
At time: 445.22073006629944 and batch: 1150, loss is 5.097314500808716 and perplexity is 163.58201752668043
At time: 446.43436574935913 and batch: 1200, loss is 5.1099217414855955 and perplexity is 165.65739026517852
At time: 447.6987028121948 and batch: 1250, loss is 5.145842523574829 and perplexity is 171.7160986034351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.128079825074133 and perplexity of 168.69288700255586
Finished 14 epochs...
Completing Train Step...
At time: 450.7243158817291 and batch: 50, loss is 5.107383480072022 and perplexity is 165.23744169868544
At time: 451.94120812416077 and batch: 100, loss is 5.128372278213501 and perplexity is 168.74222898170544
At time: 453.14779829978943 and batch: 150, loss is 5.052014417648316 and perplexity is 156.3370756651577
At time: 454.35236978530884 and batch: 200, loss is 5.105920715332031 and perplexity is 164.99591488676032
At time: 455.557656288147 and batch: 250, loss is 5.132210950851441 and perplexity is 169.3912199940545
At time: 456.7650556564331 and batch: 300, loss is 5.111866569519043 and perplexity is 165.97987889252818
At time: 457.9733667373657 and batch: 350, loss is 5.136530342102051 and perplexity is 170.1244694031403
At time: 459.1771283149719 and batch: 400, loss is 5.106327991485596 and perplexity is 165.06312747444235
At time: 460.3801951408386 and batch: 450, loss is 5.07793475151062 and perplexity is 160.44236018743055
At time: 461.589435338974 and batch: 500, loss is 5.073606348037719 and perplexity is 159.74940170223738
At time: 462.79438495635986 and batch: 550, loss is 5.0669748497009275 and perplexity is 158.693528690095
At time: 464.000137090683 and batch: 600, loss is 5.100396003723144 and perplexity is 164.0868734484391
At time: 465.20317220687866 and batch: 650, loss is 5.109096355438233 and perplexity is 165.52071537914236
At time: 466.4060595035553 and batch: 700, loss is 5.124267683029175 and perplexity is 168.0510299577533
At time: 467.6106872558594 and batch: 750, loss is 5.0874105644226075 and perplexity is 161.9699079240174
At time: 468.81577587127686 and batch: 800, loss is 5.101835641860962 and perplexity is 164.3232692907772
At time: 470.0273370742798 and batch: 850, loss is 5.131496562957763 and perplexity is 169.27025217131418
At time: 471.23204708099365 and batch: 900, loss is 5.107898969650268 and perplexity is 165.3226418358219
At time: 472.4368815422058 and batch: 950, loss is 5.095051336288452 and perplexity is 163.21222311909023
At time: 473.64145946502686 and batch: 1000, loss is 5.082055196762085 and perplexity is 161.10481802194704
At time: 474.85076332092285 and batch: 1050, loss is 5.091375141143799 and perplexity is 162.61332464522255
At time: 476.0565414428711 and batch: 1100, loss is 5.057435941696167 and perplexity is 157.1869626400625
At time: 477.2612073421478 and batch: 1150, loss is 5.09656343460083 and perplexity is 163.45920272789417
At time: 478.51040053367615 and batch: 1200, loss is 5.107154426574707 and perplexity is 165.1995978190789
At time: 479.70615577697754 and batch: 1250, loss is 5.140872306823731 and perplexity is 170.86474982029017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1286095389484485 and perplexity of 168.78226963682016
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 482.6684956550598 and batch: 50, loss is 5.101747884750366 and perplexity is 164.30884938819426
At time: 483.8999876976013 and batch: 100, loss is 5.125063819885254 and perplexity is 168.1848748487604
At time: 485.0992999076843 and batch: 150, loss is 5.045438833236695 and perplexity is 155.3124405068977
At time: 486.2971429824829 and batch: 200, loss is 5.097522211074829 and perplexity is 163.6159987200714
At time: 487.502379655838 and batch: 250, loss is 5.123843069076538 and perplexity is 167.97968829308323
At time: 488.7030920982361 and batch: 300, loss is 5.1013108253479 and perplexity is 164.23705235158866
At time: 489.9083135128021 and batch: 350, loss is 5.122119588851929 and perplexity is 167.69042796100626
At time: 491.1254427433014 and batch: 400, loss is 5.088976230621338 and perplexity is 162.2236973569908
At time: 492.33341789245605 and batch: 450, loss is 5.059358987808228 and perplexity is 157.48953125088516
At time: 493.5421848297119 and batch: 500, loss is 5.051754255294799 and perplexity is 156.2964079339465
At time: 494.7502021789551 and batch: 550, loss is 5.048775968551635 and perplexity is 155.83160491610008
At time: 495.95663046836853 and batch: 600, loss is 5.080606880187989 and perplexity is 160.8716561307119
At time: 497.159795999527 and batch: 650, loss is 5.0912636947631835 and perplexity is 162.59520298856626
At time: 498.3676438331604 and batch: 700, loss is 5.10204833984375 and perplexity is 164.3582242359719
At time: 499.58109188079834 and batch: 750, loss is 5.065064907073975 and perplexity is 158.39072241838136
At time: 500.7873544692993 and batch: 800, loss is 5.074330139160156 and perplexity is 159.86506875534758
At time: 501.9951114654541 and batch: 850, loss is 5.102770185470581 and perplexity is 164.47690833201463
At time: 503.2033314704895 and batch: 900, loss is 5.078153047561646 and perplexity is 160.47738794414812
At time: 504.4148859977722 and batch: 950, loss is 5.064161338806152 and perplexity is 158.2476702261302
At time: 505.6098279953003 and batch: 1000, loss is 5.052423524856567 and perplexity is 156.40104737448075
At time: 506.8138768672943 and batch: 1050, loss is 5.058665866851807 and perplexity is 157.38040977791678
At time: 508.05003476142883 and batch: 1100, loss is 5.022817192077636 and perplexity is 151.8384599274906
At time: 509.24884247779846 and batch: 1150, loss is 5.060173273086548 and perplexity is 157.61782488439198
At time: 510.4525508880615 and batch: 1200, loss is 5.074668779373169 and perplexity is 159.91921466375769
At time: 511.6547517776489 and batch: 1250, loss is 5.115302591323853 and perplexity is 166.55117029835918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.115037458656478 and perplexity of 166.50701799569114
Finished 16 epochs...
Completing Train Step...
At time: 514.6822044849396 and batch: 50, loss is 5.085697278976441 and perplexity is 161.6926448212955
At time: 515.9172904491425 and batch: 100, loss is 5.110394010543823 and perplexity is 165.73564360172782
At time: 517.1204013824463 and batch: 150, loss is 5.0320987796783445 and perplexity is 153.254322456062
At time: 518.3226714134216 and batch: 200, loss is 5.083284454345703 and perplexity is 161.30297911181614
At time: 519.5266268253326 and batch: 250, loss is 5.1118134498596195 and perplexity is 165.97106233205872
At time: 520.7321286201477 and batch: 300, loss is 5.0908808994293215 and perplexity is 162.5329742147559
At time: 521.9388539791107 and batch: 350, loss is 5.1113945007324215 and perplexity is 165.90154346380598
At time: 523.1450896263123 and batch: 400, loss is 5.079711217880249 and perplexity is 160.7276339591752
At time: 524.3501877784729 and batch: 450, loss is 5.050388135910034 and perplexity is 156.0830341615068
At time: 525.552592754364 and batch: 500, loss is 5.043895206451416 and perplexity is 155.07288100644323
At time: 526.754900932312 and batch: 550, loss is 5.041339025497437 and perplexity is 154.6769928579715
At time: 527.9615848064423 and batch: 600, loss is 5.074526109695435 and perplexity is 159.8964006684095
At time: 529.1678931713104 and batch: 650, loss is 5.08648790359497 and perplexity is 161.82053355625573
At time: 530.3730556964874 and batch: 700, loss is 5.097577886581421 and perplexity is 163.6251083772767
At time: 531.5759749412537 and batch: 750, loss is 5.061078996658325 and perplexity is 157.76064773296872
At time: 532.7792055606842 and batch: 800, loss is 5.071674079895019 and perplexity is 159.44102105551156
At time: 533.9882380962372 and batch: 850, loss is 5.101528186798095 and perplexity is 164.2727550355166
At time: 535.1940498352051 and batch: 900, loss is 5.077151346206665 and perplexity is 160.31671801227967
At time: 536.3987693786621 and batch: 950, loss is 5.064080228805542 and perplexity is 158.23483527802995
At time: 537.6011800765991 and batch: 1000, loss is 5.053487768173218 and perplexity is 156.56758474629015
At time: 538.8343410491943 and batch: 1050, loss is 5.060848274230957 and perplexity is 157.72425301208125
At time: 540.0418727397919 and batch: 1100, loss is 5.025831165313721 and perplexity is 152.29678732822484
At time: 541.2504596710205 and batch: 1150, loss is 5.063167381286621 and perplexity is 158.0904569089832
At time: 542.4584534168243 and batch: 1200, loss is 5.077551727294922 and perplexity is 160.3809186458053
At time: 543.6617794036865 and batch: 1250, loss is 5.114822568893433 and perplexity is 166.47124118622142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.114759459112682 and perplexity of 166.46073555419645
Finished 17 epochs...
Completing Train Step...
At time: 546.6882979869843 and batch: 50, loss is 5.079629669189453 and perplexity is 160.71452736547116
At time: 547.8912620544434 and batch: 100, loss is 5.104623079299927 and perplexity is 164.78194909733625
At time: 549.0941224098206 and batch: 150, loss is 5.02719407081604 and perplexity is 152.5044949684512
At time: 550.2980926036835 and batch: 200, loss is 5.077760696411133 and perplexity is 160.41443680664008
At time: 551.5050668716431 and batch: 250, loss is 5.106959047317505 and perplexity is 165.1673243972483
At time: 552.7093505859375 and batch: 300, loss is 5.086370820999146 and perplexity is 161.8015882972309
At time: 553.91343998909 and batch: 350, loss is 5.106379175186158 and perplexity is 165.07157623235025
At time: 555.118978023529 and batch: 400, loss is 5.074870748519897 and perplexity is 159.95151667298367
At time: 556.326135635376 and batch: 450, loss is 5.045697336196899 and perplexity is 155.35259442225473
At time: 557.5316104888916 and batch: 500, loss is 5.04002254486084 and perplexity is 154.47349757014007
At time: 558.7348916530609 and batch: 550, loss is 5.038027410507202 and perplexity is 154.16560942964506
At time: 559.938720703125 and batch: 600, loss is 5.071645565032959 and perplexity is 159.43647468160935
At time: 561.1336536407471 and batch: 650, loss is 5.084269943237305 and perplexity is 161.46201975943737
At time: 562.3272726535797 and batch: 700, loss is 5.095407638549805 and perplexity is 163.2703863645006
At time: 563.5242638587952 and batch: 750, loss is 5.058587570190429 and perplexity is 157.3680878996522
At time: 564.7207429409027 and batch: 800, loss is 5.069799032211304 and perplexity is 159.14234164486888
At time: 565.9155917167664 and batch: 850, loss is 5.1005293560028075 and perplexity is 164.10875626610564
At time: 567.1052784919739 and batch: 900, loss is 5.076218004226685 and perplexity is 160.16715749567769
At time: 568.2951519489288 and batch: 950, loss is 5.063383264541626 and perplexity is 158.12458967561977
At time: 569.5288369655609 and batch: 1000, loss is 5.053441848754883 and perplexity is 156.56039541893455
At time: 570.7246360778809 and batch: 1050, loss is 5.060957612991333 and perplexity is 157.74149932921517
At time: 571.9306395053864 and batch: 1100, loss is 5.0262995529174805 and perplexity is 152.368137964074
At time: 573.1368567943573 and batch: 1150, loss is 5.0634973430633545 and perplexity is 158.14262932400757
At time: 574.3396453857422 and batch: 1200, loss is 5.077674531936646 and perplexity is 160.40061537645923
At time: 575.5433082580566 and batch: 1250, loss is 5.113281135559082 and perplexity is 166.2148345334959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.114779952668796 and perplexity of 166.464146961577
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 578.5338838100433 and batch: 50, loss is 5.07743932723999 and perplexity is 160.36289283480954
At time: 579.7701342105865 and batch: 100, loss is 5.10489854812622 and perplexity is 164.82734764010303
At time: 580.9786727428436 and batch: 150, loss is 5.026617517471314 and perplexity is 152.41659333421697
At time: 582.186192035675 and batch: 200, loss is 5.076462917327881 and perplexity is 160.2063893349293
At time: 583.392786026001 and batch: 250, loss is 5.1037892246246335 and perplexity is 164.6446021703244
At time: 584.5996882915497 and batch: 300, loss is 5.082233333587647 and perplexity is 161.13351927911168
At time: 585.805627822876 and batch: 350, loss is 5.1001342678070065 and perplexity is 164.0439316402428
At time: 587.0138750076294 and batch: 400, loss is 5.066879491806031 and perplexity is 158.67839673075275
At time: 588.2232849597931 and batch: 450, loss is 5.037422180175781 and perplexity is 154.0723319567815
At time: 589.4330892562866 and batch: 500, loss is 5.029781761169434 and perplexity is 152.89964041546526
At time: 590.6393375396729 and batch: 550, loss is 5.028076725006104 and perplexity is 152.63916312390143
At time: 591.8452236652374 and batch: 600, loss is 5.062247343063355 and perplexity is 157.94507453481913
At time: 593.0522119998932 and batch: 650, loss is 5.075939645767212 and perplexity is 160.1225798170339
At time: 594.2598934173584 and batch: 700, loss is 5.08522479057312 and perplexity is 161.61626496743372
At time: 595.4655666351318 and batch: 750, loss is 5.044583463668824 and perplexity is 155.17964777330445
At time: 596.6753346920013 and batch: 800, loss is 5.056019153594971 and perplexity is 156.96441970704623
At time: 597.8852558135986 and batch: 850, loss is 5.082999305725098 and perplexity is 161.25699034695091
At time: 599.0928916931152 and batch: 900, loss is 5.058866539001465 and perplexity is 157.41199481207335
At time: 600.3274834156036 and batch: 950, loss is 5.046450805664063 and perplexity is 155.46969196796934
At time: 601.5355043411255 and batch: 1000, loss is 5.035502061843872 and perplexity is 153.776778687182
At time: 602.7463235855103 and batch: 1050, loss is 5.041913356781006 and perplexity is 154.76585420930255
At time: 603.956036567688 and batch: 1100, loss is 5.005176420211792 and perplexity is 149.1833998039148
At time: 605.1768479347229 and batch: 1150, loss is 5.040024747848511 and perplexity is 154.4738378737256
At time: 606.3894469738007 and batch: 1200, loss is 5.059961853027343 and perplexity is 157.58450483691055
At time: 607.602454662323 and batch: 1250, loss is 5.099801177978516 and perplexity is 163.98929937441892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108539664832345 and perplexity of 165.4285971948539
Finished 19 epochs...
Completing Train Step...
At time: 610.6807930469513 and batch: 50, loss is 5.069391403198242 and perplexity is 159.0774838290697
At time: 611.8827905654907 and batch: 100, loss is 5.09703109741211 and perplexity is 163.53566439591722
At time: 613.0871531963348 and batch: 150, loss is 5.0193590259552 and perplexity is 151.31428417508198
At time: 614.2912318706512 and batch: 200, loss is 5.068083753585816 and perplexity is 158.86960216680458
At time: 615.4968724250793 and batch: 250, loss is 5.0966273021698 and perplexity is 163.4696428031858
At time: 616.7026567459106 and batch: 300, loss is 5.076019258499145 and perplexity is 160.13532812051423
At time: 617.9065856933594 and batch: 350, loss is 5.0945650005340575 and perplexity is 163.1328664779833
At time: 619.1099057197571 and batch: 400, loss is 5.062153911590576 and perplexity is 157.9303181832517
At time: 620.3121011257172 and batch: 450, loss is 5.032505178451538 and perplexity is 153.3166174821485
At time: 621.5180613994598 and batch: 500, loss is 5.025191125869751 and perplexity is 152.19934256471745
At time: 622.7243373394012 and batch: 550, loss is 5.02411148071289 and perplexity is 152.0351099540644
At time: 623.9292798042297 and batch: 600, loss is 5.059236707687378 and perplexity is 157.47027458934875
At time: 625.1339461803436 and batch: 650, loss is 5.073581027984619 and perplexity is 159.74535689011117
At time: 626.3404626846313 and batch: 700, loss is 5.082865152359009 and perplexity is 161.23535862990678
At time: 627.545000076294 and batch: 750, loss is 5.0434827709198 and perplexity is 155.00893662769948
At time: 628.7510802745819 and batch: 800, loss is 5.055479545593261 and perplexity is 156.87974329827716
At time: 630.0038959980011 and batch: 850, loss is 5.083245048522949 and perplexity is 161.29662296044705
At time: 631.2130644321442 and batch: 900, loss is 5.059186382293701 and perplexity is 157.4623500351924
At time: 632.4197239875793 and batch: 950, loss is 5.047417192459107 and perplexity is 155.62000844554782
At time: 633.6219215393066 and batch: 1000, loss is 5.036592226028443 and perplexity is 153.94451203555784
At time: 634.8260591030121 and batch: 1050, loss is 5.043574962615967 and perplexity is 155.02322782324288
At time: 636.0300004482269 and batch: 1100, loss is 5.007439489364624 and perplexity is 149.52139446251573
At time: 637.2340338230133 and batch: 1150, loss is 5.042855997085571 and perplexity is 154.91181152306626
At time: 638.4471945762634 and batch: 1200, loss is 5.062558155059815 and perplexity is 157.99417338862744
At time: 639.6557853221893 and batch: 1250, loss is 5.100140657424927 and perplexity is 164.04497982163682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.10838284457687 and perplexity of 165.4026566740317
Finished 20 epochs...
Completing Train Step...
At time: 642.683304309845 and batch: 50, loss is 5.066024541854858 and perplexity is 158.54279261883997
At time: 643.9097676277161 and batch: 100, loss is 5.093852252960205 and perplexity is 163.01663534983334
At time: 645.109846830368 and batch: 150, loss is 5.016279535293579 and perplexity is 150.84902999063033
At time: 646.3108103275299 and batch: 200, loss is 5.064513731002807 and perplexity is 158.3034452970294
At time: 647.513209104538 and batch: 250, loss is 5.0932478427886965 and perplexity is 162.91813620724324
At time: 648.7086045742035 and batch: 300, loss is 5.072944393157959 and perplexity is 159.64368979835726
At time: 649.905561208725 and batch: 350, loss is 5.0919379711151125 and perplexity is 162.70487405902804
At time: 651.1105623245239 and batch: 400, loss is 5.059468469619751 and perplexity is 157.5067744339527
At time: 652.3156661987305 and batch: 450, loss is 5.029530324935913 and perplexity is 152.8612007385388
At time: 653.5240828990936 and batch: 500, loss is 5.022717514038086 and perplexity is 151.82332572176318
At time: 654.7369256019592 and batch: 550, loss is 5.0223153591156 and perplexity is 151.762281499416
At time: 655.9474222660065 and batch: 600, loss is 5.05794246673584 and perplexity is 157.26660194049694
At time: 657.1550912857056 and batch: 650, loss is 5.072419738769531 and perplexity is 159.55995400401642
At time: 658.360835313797 and batch: 700, loss is 5.081730375289917 and perplexity is 161.0524962158668
At time: 659.5681910514832 and batch: 750, loss is 5.042993078231811 and perplexity is 154.9330484673151
At time: 660.8217236995697 and batch: 800, loss is 5.055087099075317 and perplexity is 156.8181884685631
At time: 662.0334784984589 and batch: 850, loss is 5.08300817489624 and perplexity is 161.25842056913862
At time: 663.2457258701324 and batch: 900, loss is 5.058993663787842 and perplexity is 157.43200705028576
At time: 664.45245885849 and batch: 950, loss is 5.047581577301026 and perplexity is 155.64559211876204
At time: 665.6583106517792 and batch: 1000, loss is 5.036684169769287 and perplexity is 153.95866692059337
At time: 666.8667724132538 and batch: 1050, loss is 5.044015378952026 and perplexity is 155.09151762211252
At time: 668.0816450119019 and batch: 1100, loss is 5.008067674636841 and perplexity is 149.61535110839927
At time: 669.2961509227753 and batch: 1150, loss is 5.04380425453186 and perplexity is 155.05877747162617
At time: 670.5070593357086 and batch: 1200, loss is 5.063600044250489 and perplexity is 158.15887159381157
At time: 671.7142488956451 and batch: 1250, loss is 5.099704818725586 and perplexity is 163.97349824934727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108358786924042 and perplexity of 165.3986775222053
Finished 21 epochs...
Completing Train Step...
At time: 674.7510395050049 and batch: 50, loss is 5.063404893875122 and perplexity is 158.1280098420916
At time: 675.9870691299438 and batch: 100, loss is 5.091442623138428 and perplexity is 162.62429848698702
At time: 677.1927573680878 and batch: 150, loss is 5.014235944747925 and perplexity is 150.5410711172178
At time: 678.3954718112946 and batch: 200, loss is 5.062174873352051 and perplexity is 157.9336287156083
At time: 679.5975515842438 and batch: 250, loss is 5.090919132232666 and perplexity is 162.53918842478865
At time: 680.8080494403839 and batch: 300, loss is 5.070659704208374 and perplexity is 159.27936996162967
At time: 682.0151426792145 and batch: 350, loss is 5.089744644165039 and perplexity is 162.3484001486703
At time: 683.2287554740906 and batch: 400, loss is 5.057353191375732 and perplexity is 157.17395590669892
At time: 684.4336457252502 and batch: 450, loss is 5.027271757125854 and perplexity is 152.51634294010208
At time: 685.6376886367798 and batch: 500, loss is 5.020696649551391 and perplexity is 151.51682116096217
At time: 686.8465666770935 and batch: 550, loss is 5.020893535614014 and perplexity is 151.54665564820257
At time: 688.0525164604187 and batch: 600, loss is 5.0569552803039555 and perplexity is 157.11142709072783
At time: 689.259624004364 and batch: 650, loss is 5.071570835113525 and perplexity is 159.42456045188183
At time: 690.4619026184082 and batch: 700, loss is 5.080644903182983 and perplexity is 160.8777730691791
At time: 691.6937265396118 and batch: 750, loss is 5.042183427810669 and perplexity is 154.80765762759762
At time: 692.8995630741119 and batch: 800, loss is 5.054716453552246 and perplexity is 156.76007527943315
At time: 694.1120903491974 and batch: 850, loss is 5.082805089950561 and perplexity is 161.22567473676241
At time: 695.3194534778595 and batch: 900, loss is 5.058802223205566 and perplexity is 157.40187105990506
At time: 696.5295090675354 and batch: 950, loss is 5.047555828094483 and perplexity is 155.6415844198608
At time: 697.7328126430511 and batch: 1000, loss is 5.036519918441773 and perplexity is 153.9333810818425
At time: 698.9351124763489 and batch: 1050, loss is 5.044017391204834 and perplexity is 155.09182970576833
At time: 700.1426179409027 and batch: 1100, loss is 5.0082596397399906 and perplexity is 149.64407479159172
At time: 701.356626033783 and batch: 1150, loss is 5.044259519577026 and perplexity is 155.12938638462057
At time: 702.5644080638885 and batch: 1200, loss is 5.063910846710205 and perplexity is 158.20803539984905
At time: 703.7748818397522 and batch: 1250, loss is 5.099135990142822 and perplexity is 163.88025195981893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108304434449133 and perplexity of 165.38968793904053
Finished 22 epochs...
Completing Train Step...
At time: 706.8069627285004 and batch: 50, loss is 5.061322927474976 and perplexity is 157.79913511054343
At time: 708.0132637023926 and batch: 100, loss is 5.089461240768433 and perplexity is 162.3023965797263
At time: 709.216860294342 and batch: 150, loss is 5.012525568008423 and perplexity is 150.2838092409197
At time: 710.421492099762 and batch: 200, loss is 5.060296964645386 and perplexity is 157.63732208464728
At time: 711.6272311210632 and batch: 250, loss is 5.088964595794677 and perplexity is 162.2218099233719
At time: 712.836127281189 and batch: 300, loss is 5.068685054779053 and perplexity is 158.96515937460896
At time: 714.0423185825348 and batch: 350, loss is 5.088048362731934 and perplexity is 162.07324500805882
At time: 715.2478971481323 and batch: 400, loss is 5.0555033874511714 and perplexity is 156.88348364741412
At time: 716.4460396766663 and batch: 450, loss is 5.025360107421875 and perplexity is 152.22506361898698
At time: 717.6473398208618 and batch: 500, loss is 5.019003896713257 and perplexity is 151.26055758855333
At time: 718.845556974411 and batch: 550, loss is 5.019637985229492 and perplexity is 151.35650058604136
At time: 720.0434744358063 and batch: 600, loss is 5.055792760848999 and perplexity is 156.9288881232471
At time: 721.2350206375122 and batch: 650, loss is 5.070863094329834 and perplexity is 159.3117691067535
At time: 722.4730839729309 and batch: 700, loss is 5.079666299819946 and perplexity is 160.72041454776294
At time: 723.6641030311584 and batch: 750, loss is 5.041520338058472 and perplexity is 154.70504028227813
At time: 724.8554515838623 and batch: 800, loss is 5.054061841964722 and perplexity is 156.6574918974925
At time: 726.0509715080261 and batch: 850, loss is 5.082261152267456 and perplexity is 161.13800186324056
At time: 727.2597694396973 and batch: 900, loss is 5.058365211486817 and perplexity is 157.3330996257546
At time: 728.4701755046844 and batch: 950, loss is 5.047129487991333 and perplexity is 155.57524231385986
At time: 729.6794247627258 and batch: 1000, loss is 5.0361407852172855 and perplexity is 153.87503088466488
At time: 730.88516664505 and batch: 1050, loss is 5.043739233016968 and perplexity is 155.04869564278906
At time: 732.0909943580627 and batch: 1100, loss is 5.008248548507691 and perplexity is 149.64241506360017
At time: 733.2978677749634 and batch: 1150, loss is 5.0443383407592775 and perplexity is 155.14161434816225
At time: 734.5030152797699 and batch: 1200, loss is 5.063947792053223 and perplexity is 158.2138805579601
At time: 735.7115037441254 and batch: 1250, loss is 5.098357763290405 and perplexity is 163.75276556026682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.108439870124315 and perplexity of 165.41208912002003
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 738.7499649524689 and batch: 50, loss is 5.06076283454895 and perplexity is 157.7107776777315
At time: 739.9851784706116 and batch: 100, loss is 5.0901368141174315 and perplexity is 162.4120807990302
At time: 741.1972227096558 and batch: 150, loss is 5.012264566421509 and perplexity is 150.244590046579
At time: 742.4063711166382 and batch: 200, loss is 5.060661687850952 and perplexity is 157.69482656004567
At time: 743.615859746933 and batch: 250, loss is 5.087890567779541 and perplexity is 162.04767268572343
At time: 744.8237223625183 and batch: 300, loss is 5.067111663818359 and perplexity is 158.71524169045347
At time: 746.0371203422546 and batch: 350, loss is 5.085667562484741 and perplexity is 161.68783995455004
At time: 747.2422654628754 and batch: 400, loss is 5.0514786911010745 and perplexity is 156.25334417399975
At time: 748.4451751708984 and batch: 450, loss is 5.021307210922242 and perplexity is 151.60935972635897
At time: 749.6543383598328 and batch: 500, loss is 5.0142542743682865 and perplexity is 150.54383050318944
At time: 750.8619334697723 and batch: 550, loss is 5.015267705917358 and perplexity is 150.69647370419298
At time: 752.0939078330994 and batch: 600, loss is 5.051057376861572 and perplexity is 156.18752628111073
At time: 753.3037645816803 and batch: 650, loss is 5.065885553359985 and perplexity is 158.52075852599904
At time: 754.5107984542847 and batch: 700, loss is 5.074857864379883 and perplexity is 159.94945584852334
At time: 755.7155253887177 and batch: 750, loss is 5.034501180648804 and perplexity is 153.62294339942133
At time: 756.9304447174072 and batch: 800, loss is 5.04578197479248 and perplexity is 155.36574380413126
At time: 758.1378593444824 and batch: 850, loss is 5.0723163032531735 and perplexity is 159.5434506913131
At time: 759.344731092453 and batch: 900, loss is 5.0480445289611815 and perplexity is 155.71766518591514
At time: 760.5480823516846 and batch: 950, loss is 5.0370958805084225 and perplexity is 154.02206640737683
At time: 761.7520055770874 and batch: 1000, loss is 5.025658664703369 and perplexity is 152.27051830523243
At time: 762.9565260410309 and batch: 1050, loss is 5.032046604156494 and perplexity is 153.24632654040934
At time: 764.1617543697357 and batch: 1100, loss is 4.995679702758789 and perplexity is 147.77335321172166
At time: 765.3702018260956 and batch: 1150, loss is 5.030921058654785 and perplexity is 153.07393786076912
At time: 766.5805733203888 and batch: 1200, loss is 5.053643398284912 and perplexity is 156.59195327318093
At time: 767.7869355678558 and batch: 1250, loss is 5.090626382827759 and perplexity is 162.49161213840776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106180678318887 and perplexity of 165.0388132933706
Finished 24 epochs...
Completing Train Step...
At time: 770.8432841300964 and batch: 50, loss is 5.05647180557251 and perplexity is 157.03548604497576
At time: 772.0487549304962 and batch: 100, loss is 5.085678825378418 and perplexity is 161.68966103775554
At time: 773.2552194595337 and batch: 150, loss is 5.00833459854126 and perplexity is 149.65529235247735
At time: 774.4589836597443 and batch: 200, loss is 5.05642632484436 and perplexity is 157.0283441191363
At time: 775.6683671474457 and batch: 250, loss is 5.084163913726806 and perplexity is 161.44490092808485
At time: 776.8706245422363 and batch: 300, loss is 5.063789262771606 and perplexity is 158.18880101310708
At time: 778.0765132904053 and batch: 350, loss is 5.082797689437866 and perplexity is 161.22448158852472
At time: 779.2874054908752 and batch: 400, loss is 5.04896671295166 and perplexity is 155.86133175710884
At time: 780.4981305599213 and batch: 450, loss is 5.0189791011810305 and perplexity is 151.25680704902155
At time: 781.7021207809448 and batch: 500, loss is 5.0123024177551265 and perplexity is 150.25027711231192
At time: 782.9312632083893 and batch: 550, loss is 5.013685398101806 and perplexity is 150.45821404584538
At time: 784.1368100643158 and batch: 600, loss is 5.049847602844238 and perplexity is 155.99868891828282
At time: 785.3430464267731 and batch: 650, loss is 5.064852819442749 and perplexity is 158.35713326730323
At time: 786.5527858734131 and batch: 700, loss is 5.07398157119751 and perplexity is 159.80935462467778
At time: 787.7620520591736 and batch: 750, loss is 5.03399772644043 and perplexity is 153.54562074790445
At time: 788.9648923873901 and batch: 800, loss is 5.045528078079224 and perplexity is 155.32630195971598
At time: 790.176427602768 and batch: 850, loss is 5.07287522315979 and perplexity is 159.63264762652406
At time: 791.388067483902 and batch: 900, loss is 5.048975687026978 and perplexity is 155.86273047471522
At time: 792.594181060791 and batch: 950, loss is 5.037886934280396 and perplexity is 154.1439543475768
At time: 793.801970243454 and batch: 1000, loss is 5.026809883117676 and perplexity is 152.445915870944
At time: 794.9998753070831 and batch: 1050, loss is 5.03332854270935 and perplexity is 153.44290488824564
At time: 796.1979489326477 and batch: 1100, loss is 4.997050161361694 and perplexity is 147.97600930908857
At time: 797.3947243690491 and batch: 1150, loss is 5.032443208694458 and perplexity is 153.30711678298778
At time: 798.5923657417297 and batch: 1200, loss is 5.055097541809082 and perplexity is 156.81982608770537
At time: 799.7876167297363 and batch: 1250, loss is 5.090728769302368 and perplexity is 162.50824993345617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106175332173814 and perplexity of 165.03793097429065
Finished 25 epochs...
Completing Train Step...
At time: 802.7275087833405 and batch: 50, loss is 5.054344263076782 and perplexity is 156.70174152878766
At time: 803.9482431411743 and batch: 100, loss is 5.083631181716919 and perplexity is 161.3589169667652
At time: 805.1546061038971 and batch: 150, loss is 5.006242542266846 and perplexity is 149.34253232896828
At time: 806.3615078926086 and batch: 200, loss is 5.054307260513306 and perplexity is 156.69594326992575
At time: 807.5709810256958 and batch: 250, loss is 5.082178506851196 and perplexity is 161.12468509629375
At time: 808.7790486812592 and batch: 300, loss is 5.062043104171753 and perplexity is 157.91281930186082
At time: 809.9841864109039 and batch: 350, loss is 5.081168003082276 and perplexity is 160.9619502306829
At time: 811.1914813518524 and batch: 400, loss is 5.047695407867431 and perplexity is 155.66331035309065
At time: 812.4009251594543 and batch: 450, loss is 5.017548379898071 and perplexity is 151.0405554507543
At time: 813.6462023258209 and batch: 500, loss is 5.011113719940186 and perplexity is 150.0717810461741
At time: 814.8548500537872 and batch: 550, loss is 5.01287317276001 and perplexity is 150.3360576874765
At time: 816.0622463226318 and batch: 600, loss is 5.04930832862854 and perplexity is 155.9145855270999
At time: 817.267810344696 and batch: 650, loss is 5.064257135391236 and perplexity is 158.2628305386767
At time: 818.4838118553162 and batch: 700, loss is 5.073490381240845 and perplexity is 159.7308771499566
At time: 819.6928629875183 and batch: 750, loss is 5.033697853088379 and perplexity is 153.49958341094828
At time: 820.9023313522339 and batch: 800, loss is 5.045382099151611 and perplexity is 155.30362924763523
At time: 822.1118824481964 and batch: 850, loss is 5.073087854385376 and perplexity is 159.66659412094688
At time: 823.331374168396 and batch: 900, loss is 5.049506702423096 and perplexity is 155.94551796304836
At time: 824.5390467643738 and batch: 950, loss is 5.038376684188843 and perplexity is 154.21946482422143
At time: 825.7500648498535 and batch: 1000, loss is 5.027308349609375 and perplexity is 152.5219239939794
At time: 826.9637258052826 and batch: 1050, loss is 5.033903694152832 and perplexity is 153.53118318074422
At time: 828.171377658844 and batch: 1100, loss is 4.997705535888672 and perplexity is 148.0730208021519
At time: 829.385810136795 and batch: 1150, loss is 5.0331754302978515 and perplexity is 153.41941267357151
At time: 830.598185300827 and batch: 1200, loss is 5.055782566070556 and perplexity is 156.92728827615647
At time: 831.8089420795441 and batch: 1250, loss is 5.090513210296631 and perplexity is 162.47322359193382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106163748859489 and perplexity of 165.03601929913233
Finished 26 epochs...
Completing Train Step...
At time: 834.8545467853546 and batch: 50, loss is 5.052732629776001 and perplexity is 156.4493991801343
At time: 836.0819616317749 and batch: 100, loss is 5.0821261215209965 and perplexity is 161.1162447475388
At time: 837.2865884304047 and batch: 150, loss is 5.004699440002441 and perplexity is 149.1122592423134
At time: 838.4903028011322 and batch: 200, loss is 5.0528295135498045 and perplexity is 156.46455732261333
At time: 839.6920020580292 and batch: 250, loss is 5.080724620819092 and perplexity is 160.89059837614548
At time: 840.8942840099335 and batch: 300, loss is 5.060632200241089 and perplexity is 157.69017658508128
At time: 842.0978808403015 and batch: 350, loss is 5.079921503067016 and perplexity is 160.76143615362682
At time: 843.3298816680908 and batch: 400, loss is 5.046529417037964 and perplexity is 155.48191413444928
At time: 844.5353462696075 and batch: 450, loss is 5.016451635360718 and perplexity is 150.87499335290363
At time: 845.7386789321899 and batch: 500, loss is 5.010100183486938 and perplexity is 149.91975488052455
At time: 846.9406244754791 and batch: 550, loss is 5.012251882553101 and perplexity is 150.24268437605548
At time: 848.1453311443329 and batch: 600, loss is 5.0489609432220455 and perplexity is 155.86043248196148
At time: 849.3480486869812 and batch: 650, loss is 5.063682069778443 and perplexity is 158.17184519083045
At time: 850.5502173900604 and batch: 700, loss is 5.073049697875977 and perplexity is 159.66050191727692
At time: 851.7549135684967 and batch: 750, loss is 5.033344736099243 and perplexity is 153.44538966914925
At time: 852.9584319591522 and batch: 800, loss is 5.045109281539917 and perplexity is 155.26126546147262
At time: 854.1599946022034 and batch: 850, loss is 5.073126935958863 and perplexity is 159.67283426461492
At time: 855.3613719940186 and batch: 900, loss is 5.049920883178711 and perplexity is 156.01012097325133
At time: 856.5639884471893 and batch: 950, loss is 5.038619613647461 and perplexity is 154.25693382629743
At time: 857.7684404850006 and batch: 1000, loss is 5.027597217559815 and perplexity is 152.56598905374602
At time: 858.9745547771454 and batch: 1050, loss is 5.034286270141601 and perplexity is 153.58993176213832
At time: 860.182550907135 and batch: 1100, loss is 4.998058576583862 and perplexity is 148.12530583318355
At time: 861.3974561691284 and batch: 1150, loss is 5.033745336532593 and perplexity is 153.5068722729028
At time: 862.6035306453705 and batch: 1200, loss is 5.05606780052185 and perplexity is 156.9720557294274
At time: 863.8128228187561 and batch: 1250, loss is 5.0902587509155275 and perplexity is 162.43188601560277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1062136462135035 and perplexity of 165.0442543652648
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 866.8972742557526 and batch: 50, loss is 5.052280874252319 and perplexity is 156.3787382618098
At time: 868.102705001831 and batch: 100, loss is 5.0821412086486815 and perplexity is 161.11867554723224
At time: 869.3100252151489 and batch: 150, loss is 5.004833316802978 and perplexity is 149.13222325083058
At time: 870.5161936283112 and batch: 200, loss is 5.052535438537598 and perplexity is 156.41855177088195
At time: 871.7247354984283 and batch: 250, loss is 5.080545778274536 and perplexity is 160.8618268649986
At time: 872.9341139793396 and batch: 300, loss is 5.059597988128662 and perplexity is 157.52717579766932
At time: 874.1714541912079 and batch: 350, loss is 5.078290634155273 and perplexity is 160.4994690003034
At time: 875.378668308258 and batch: 400, loss is 5.044164533615112 and perplexity is 155.11465197042605
At time: 876.574952840805 and batch: 450, loss is 5.0140016746521 and perplexity is 150.50580797677253
At time: 877.7747492790222 and batch: 500, loss is 5.007792663574219 and perplexity is 149.57421088897337
At time: 878.9756808280945 and batch: 550, loss is 5.009868822097778 and perplexity is 149.8850732499227
At time: 880.1781766414642 and batch: 600, loss is 5.045897169113159 and perplexity is 155.38364208631594
At time: 881.3715102672577 and batch: 650, loss is 5.060595092773437 and perplexity is 157.6843252105202
At time: 882.5647265911102 and batch: 700, loss is 5.070012807846069 and perplexity is 159.1763660366355
At time: 883.7575964927673 and batch: 750, loss is 5.02968674659729 and perplexity is 152.8851134116996
At time: 884.9511024951935 and batch: 800, loss is 5.0405200099945064 and perplexity is 154.55036186632535
At time: 886.1533308029175 and batch: 850, loss is 5.067911987304687 and perplexity is 158.8423160695389
At time: 887.3633964061737 and batch: 900, loss is 5.044389896392822 and perplexity is 155.14961297856456
At time: 888.5735831260681 and batch: 950, loss is 5.033478698730469 and perplexity is 153.46594699422468
At time: 889.7841804027557 and batch: 1000, loss is 5.022417259216309 and perplexity is 151.77774687913302
At time: 890.992130279541 and batch: 1050, loss is 5.027772455215454 and perplexity is 152.5927267026512
At time: 892.1994361877441 and batch: 1100, loss is 4.991225280761719 and perplexity is 147.11657221243996
At time: 893.4040238857269 and batch: 1150, loss is 5.026556529998779 and perplexity is 152.40729811486608
At time: 894.6086947917938 and batch: 1200, loss is 5.0504114532470705 and perplexity is 156.08667364465134
At time: 895.8148231506348 and batch: 1250, loss is 5.086431379318237 and perplexity is 161.81138702613885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104953292512546 and perplexity of 164.83637125909323
Finished 28 epochs...
Completing Train Step...
At time: 898.7942144870758 and batch: 50, loss is 5.050703039169312 and perplexity is 156.13219295740848
At time: 900.0258619785309 and batch: 100, loss is 5.080015497207642 and perplexity is 160.77654749684154
At time: 901.2288587093353 and batch: 150, loss is 5.002935810089111 and perplexity is 148.84951216387685
At time: 902.4309649467468 and batch: 200, loss is 5.050404510498047 and perplexity is 156.0855899778121
At time: 903.631341457367 and batch: 250, loss is 5.078799114227295 and perplexity is 160.58110053410545
At time: 904.8803169727325 and batch: 300, loss is 5.058084259033203 and perplexity is 157.28890271428617
At time: 906.0866830348969 and batch: 350, loss is 5.076774845123291 and perplexity is 160.2563699555323
At time: 907.2926151752472 and batch: 400, loss is 5.042721805572509 and perplexity is 154.89102506740105
At time: 908.4935863018036 and batch: 450, loss is 5.012814130783081 and perplexity is 150.32718181145415
At time: 909.6972618103027 and batch: 500, loss is 5.006968450546265 and perplexity is 149.4509806666662
At time: 910.8999028205872 and batch: 550, loss is 5.009115285873413 and perplexity is 149.77217196068517
At time: 912.1064877510071 and batch: 600, loss is 5.045216932296753 and perplexity is 155.2779803538761
At time: 913.3108801841736 and batch: 650, loss is 5.0600629234313965 and perplexity is 157.60043277139476
At time: 914.5134847164154 and batch: 700, loss is 5.069561262130737 and perplexity is 159.10450685564308
At time: 915.7161824703217 and batch: 750, loss is 5.029486513137817 and perplexity is 152.8545037611797
At time: 916.9192311763763 and batch: 800, loss is 5.040422458648681 and perplexity is 154.5352860058747
At time: 918.1261200904846 and batch: 850, loss is 5.068207588195801 and perplexity is 158.88927694021118
At time: 919.330328464508 and batch: 900, loss is 5.044893741607666 and perplexity is 155.22780406509696
At time: 920.5334825515747 and batch: 950, loss is 5.0341059017181395 and perplexity is 153.5622314864894
At time: 921.7371878623962 and batch: 1000, loss is 5.023161211013794 and perplexity is 151.8907042189402
At time: 922.9474098682404 and batch: 1050, loss is 5.028606224060058 and perplexity is 152.72000681781287
At time: 924.1625595092773 and batch: 1100, loss is 4.991985254287719 and perplexity is 147.22841940762783
At time: 925.3692212104797 and batch: 1150, loss is 5.027502574920654 and perplexity is 152.5515504891586
At time: 926.5727050304413 and batch: 1200, loss is 5.051350955963135 and perplexity is 156.23338640620938
At time: 927.7765483856201 and batch: 1250, loss is 5.08661449432373 and perplexity is 161.84101983218588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104992497576414 and perplexity of 164.8428338062377
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 930.803352355957 and batch: 50, loss is 5.0503317165374755 and perplexity is 156.07422830306518
At time: 932.0069177150726 and batch: 100, loss is 5.0797105884552005 and perplexity is 160.7275327932083
At time: 933.2108764648438 and batch: 150, loss is 5.002542858123779 and perplexity is 148.79103294604764
At time: 934.4139838218689 and batch: 200, loss is 5.04961254119873 and perplexity is 155.9620239192049
At time: 935.6464807987213 and batch: 250, loss is 5.078520154953003 and perplexity is 160.53631119432666
At time: 936.8544969558716 and batch: 300, loss is 5.05744197845459 and perplexity is 157.18791154264733
At time: 938.0608088970184 and batch: 350, loss is 5.0754695320129395 and perplexity is 160.04732168122072
At time: 939.2647476196289 and batch: 400, loss is 5.041341285705567 and perplexity is 154.67734246056335
At time: 940.4698724746704 and batch: 450, loss is 5.011289873123169 and perplexity is 150.09821899657757
At time: 941.6767659187317 and batch: 500, loss is 5.005825462341309 and perplexity is 149.28025754438903
At time: 942.8857080936432 and batch: 550, loss is 5.00750467300415 and perplexity is 149.53114112885194
At time: 944.0928275585175 and batch: 600, loss is 5.0430329895019534 and perplexity is 154.93923216546523
At time: 945.2980415821075 and batch: 650, loss is 5.058143882751465 and perplexity is 157.298281143093
At time: 946.5024619102478 and batch: 700, loss is 5.067895889282227 and perplexity is 158.83975904294866
At time: 947.7060358524323 and batch: 750, loss is 5.02753607749939 and perplexity is 152.55666144510465
At time: 948.9131739139557 and batch: 800, loss is 5.038307085037231 and perplexity is 154.20873165382176
At time: 950.1212532520294 and batch: 850, loss is 5.06562087059021 and perplexity is 158.47880636481753
At time: 951.3245959281921 and batch: 900, loss is 5.0416860675811765 and perplexity is 154.73068159946703
At time: 952.5284667015076 and batch: 950, loss is 5.03155821800232 and perplexity is 153.17150142961026
At time: 953.725781917572 and batch: 1000, loss is 5.020370531082153 and perplexity is 151.46741678345913
At time: 954.9311518669128 and batch: 1050, loss is 5.025530223846435 and perplexity is 152.2509618053293
At time: 956.1301302909851 and batch: 1100, loss is 4.988764276504517 and perplexity is 146.7549628455244
At time: 957.3265357017517 and batch: 1150, loss is 5.023964319229126 and perplexity is 152.0127378878929
At time: 958.5265445709229 and batch: 1200, loss is 5.048505983352661 and perplexity is 155.7895383681685
At time: 959.7167844772339 and batch: 1250, loss is 5.084985942840576 and perplexity is 161.5776678985987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105595275433394 and perplexity of 164.94222736944803
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 962.6675107479095 and batch: 50, loss is 5.049895029067994 and perplexity is 156.00608752245154
At time: 963.8946113586426 and batch: 100, loss is 5.079073762893676 and perplexity is 160.62520997623562
At time: 965.1011111736298 and batch: 150, loss is 5.001808376312256 and perplexity is 148.68178876247498
At time: 966.3356606960297 and batch: 200, loss is 5.048871498107911 and perplexity is 155.84649215124554
At time: 967.5416495800018 and batch: 250, loss is 5.078074254989624 and perplexity is 160.46474401613102
At time: 968.74782538414 and batch: 300, loss is 5.057073278427124 and perplexity is 157.12996703807084
At time: 969.9494893550873 and batch: 350, loss is 5.074615459442139 and perplexity is 159.91068800958385
At time: 971.1528160572052 and batch: 400, loss is 5.040480175018311 and perplexity is 154.54420547895992
At time: 972.3566155433655 and batch: 450, loss is 5.010357189178467 and perplexity is 149.95829006246984
At time: 973.5711004734039 and batch: 500, loss is 5.005137758255005 and perplexity is 149.1776321932525
At time: 974.775812625885 and batch: 550, loss is 5.006505279541016 and perplexity is 149.38177533390436
At time: 975.9788019657135 and batch: 600, loss is 5.041762628555298 and perplexity is 154.74252838467166
At time: 977.1816992759705 and batch: 650, loss is 5.057240781784057 and perplexity is 157.15628903947479
At time: 978.387989282608 and batch: 700, loss is 5.067147569656372 and perplexity is 158.7209405965232
At time: 979.5978586673737 and batch: 750, loss is 5.026813106536865 and perplexity is 152.44640726882653
At time: 980.8061945438385 and batch: 800, loss is 5.0376881408691405 and perplexity is 154.11331459065948
At time: 982.0120091438293 and batch: 850, loss is 5.0643150520324705 and perplexity is 158.2719968556924
At time: 983.2208595275879 and batch: 900, loss is 5.040035448074341 and perplexity is 154.47549078751894
At time: 984.4359488487244 and batch: 950, loss is 5.030227174758911 and perplexity is 152.9677591625141
At time: 985.6465358734131 and batch: 1000, loss is 5.018740882873535 and perplexity is 151.2207791998662
At time: 986.8533928394318 and batch: 1050, loss is 5.02417483329773 and perplexity is 152.04474207637304
At time: 988.061717748642 and batch: 1100, loss is 4.987499532699585 and perplexity is 146.56947273887957
At time: 989.2640318870544 and batch: 1150, loss is 5.022311887741089 and perplexity is 151.7617546766146
At time: 990.4659421443939 and batch: 1200, loss is 5.0473423671722415 and perplexity is 155.60836456940754
At time: 991.6692419052124 and batch: 1250, loss is 5.08448525428772 and perplexity is 161.4967880593791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106060835566834 and perplexity of 165.0190357729099
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 994.651508808136 and batch: 50, loss is 5.049786977767944 and perplexity is 155.98923177253835
At time: 995.885066986084 and batch: 100, loss is 5.078665237426758 and perplexity is 160.55960388912237
At time: 997.0883512496948 and batch: 150, loss is 5.001334352493286 and perplexity is 148.61132675480894
At time: 998.2914924621582 and batch: 200, loss is 5.048494396209716 and perplexity is 155.7877332229764
At time: 999.4930994510651 and batch: 250, loss is 5.07767611503601 and perplexity is 160.40086930677248
At time: 1000.6977818012238 and batch: 300, loss is 5.056770467758179 and perplexity is 157.08239361087493
At time: 1001.9020609855652 and batch: 350, loss is 5.07409255027771 and perplexity is 159.82709110403232
At time: 1003.1083490848541 and batch: 400, loss is 5.039885768890381 and perplexity is 154.45237075245316
At time: 1004.3100798130035 and batch: 450, loss is 5.009891443252563 and perplexity is 149.88846386171429
At time: 1005.5158307552338 and batch: 500, loss is 5.004836301803589 and perplexity is 149.13266841127253
At time: 1006.7276928424835 and batch: 550, loss is 5.005950136184692 and perplexity is 149.2988700480604
At time: 1007.9304974079132 and batch: 600, loss is 5.041174955368042 and perplexity is 154.65161706550063
At time: 1009.1328938007355 and batch: 650, loss is 5.056949548721313 and perplexity is 157.11052659618005
At time: 1010.3359069824219 and batch: 700, loss is 5.066911344528198 and perplexity is 158.68345115013585
At time: 1011.542153596878 and batch: 750, loss is 5.026563358306885 and perplexity is 152.4083388024082
At time: 1012.7479357719421 and batch: 800, loss is 5.037516422271729 and perplexity is 154.08685274049643
At time: 1013.953145980835 and batch: 850, loss is 5.063679256439209 and perplexity is 158.1714002003987
At time: 1015.1569330692291 and batch: 900, loss is 5.039331798553467 and perplexity is 154.36683241562844
At time: 1016.3595924377441 and batch: 950, loss is 5.0295602893829345 and perplexity is 152.86578120851524
At time: 1017.5619831085205 and batch: 1000, loss is 5.01788800239563 and perplexity is 151.09186093318402
At time: 1018.7646541595459 and batch: 1050, loss is 5.023583927154541 and perplexity is 151.9549244437495
At time: 1019.969720363617 and batch: 1100, loss is 4.9869163227081295 and perplexity is 146.4840168797136
At time: 1021.1765382289886 and batch: 1150, loss is 5.021564626693726 and perplexity is 151.64839139012457
At time: 1022.3876354694366 and batch: 1200, loss is 5.046704940795898 and perplexity is 155.50920729953836
At time: 1023.5959260463715 and batch: 1250, loss is 5.084208898544311 and perplexity is 161.45216366084532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105818922502281 and perplexity of 164.9791203404835
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 1026.664611339569 and batch: 50, loss is 5.04975739479065 and perplexity is 155.98461721489315
At time: 1027.883418083191 and batch: 100, loss is 5.078435611724854 and perplexity is 160.52273951004824
At time: 1029.1016614437103 and batch: 150, loss is 5.001152439117432 and perplexity is 148.58429482547805
At time: 1030.3160622119904 and batch: 200, loss is 5.048311605453491 and perplexity is 155.7592592678771
At time: 1031.5329840183258 and batch: 250, loss is 5.077398891448975 and perplexity is 160.35640856548747
At time: 1032.7464191913605 and batch: 300, loss is 5.056507749557495 and perplexity is 157.04113062757204
At time: 1033.962988615036 and batch: 350, loss is 5.0737619304656985 and perplexity is 159.7742578355611
At time: 1035.1759881973267 and batch: 400, loss is 5.039520406723023 and perplexity is 154.39595000715204
At time: 1036.3913035392761 and batch: 450, loss is 5.0096666431427 and perplexity is 149.85477270560065
At time: 1037.606562614441 and batch: 500, loss is 5.004704551696777 and perplexity is 149.11302146055255
At time: 1038.8192257881165 and batch: 550, loss is 5.005690746307373 and perplexity is 149.2601484546865
At time: 1040.0262439250946 and batch: 600, loss is 5.040924854278565 and perplexity is 154.61294336395213
At time: 1041.2247278690338 and batch: 650, loss is 5.056913433074951 and perplexity is 157.1048525504232
At time: 1042.4240229129791 and batch: 700, loss is 5.06697172164917 and perplexity is 158.69303228930013
At time: 1043.6237378120422 and batch: 750, loss is 5.026559963226318 and perplexity is 152.40782136469727
At time: 1044.8268682956696 and batch: 800, loss is 5.0373622417449955 and perplexity is 154.0630973797323
At time: 1046.041514158249 and batch: 850, loss is 5.063324098587036 and perplexity is 158.1152343600883
At time: 1047.2572283744812 and batch: 900, loss is 5.038996839523316 and perplexity is 154.31513450997855
At time: 1048.4716296195984 and batch: 950, loss is 5.029269361495972 and perplexity is 152.82131475837818
At time: 1049.6883826255798 and batch: 1000, loss is 5.017518672943115 and perplexity is 151.03606856242328
At time: 1050.8965661525726 and batch: 1050, loss is 5.023271627426148 and perplexity is 151.90747637151335
At time: 1052.1092760562897 and batch: 1100, loss is 4.986684713363648 and perplexity is 146.4500937411966
At time: 1053.3239891529083 and batch: 1150, loss is 5.021223630905151 and perplexity is 151.59668874300988
At time: 1054.536099910736 and batch: 1200, loss is 5.046308498382569 and perplexity is 155.44756907291256
At time: 1055.7477929592133 and batch: 1250, loss is 5.08392936706543 and perplexity is 161.4070390059432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105561416514599 and perplexity of 164.93664269851152
Annealing...
Model not improving. Stopping early with 164.83637125909323loss at 32 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9541e8860>
SETTINGS FOR THIS RUN
{'lr': 27.230779683856515, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 7.81897938379199, 'wordvec_source': '', 'dropout': 0.36584214465228326, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7974421977996826 and batch: 50, loss is 6.876115512847901 and perplexity is 968.855533797617
At time: 2.9989681243896484 and batch: 100, loss is 6.25262975692749 and perplexity is 519.3768652440201
At time: 4.242854833602905 and batch: 150, loss is 6.099274091720581 and perplexity is 445.53423567812905
At time: 5.456654071807861 and batch: 200, loss is 6.053685808181763 and perplexity is 425.67911368989127
At time: 6.663280725479126 and batch: 250, loss is 6.0664936065673825 and perplexity is 431.1661896281871
At time: 7.874005556106567 and batch: 300, loss is 6.063726692199707 and perplexity is 429.97483864638684
At time: 9.086938381195068 and batch: 350, loss is 6.076312866210937 and perplexity is 435.42077665622764
At time: 10.298384189605713 and batch: 400, loss is 6.01745683670044 and perplexity is 410.53321399878405
At time: 11.508916854858398 and batch: 450, loss is 6.007805671691894 and perplexity is 406.59014840369383
At time: 12.718570232391357 and batch: 500, loss is 5.996041030883789 and perplexity is 401.8347887484444
At time: 13.926379442214966 and batch: 550, loss is 6.0006583881378175 and perplexity is 403.69449368215334
At time: 15.138458251953125 and batch: 600, loss is 6.0210116100311275 and perplexity is 411.99516342920873
At time: 16.35149908065796 and batch: 650, loss is 6.007544145584107 and perplexity is 406.4838283680556
At time: 17.56110954284668 and batch: 700, loss is 6.021871566772461 and perplexity is 412.34961383155917
At time: 18.772109270095825 and batch: 750, loss is 5.977870092391968 and perplexity is 394.5990130290769
At time: 19.982857704162598 and batch: 800, loss is 5.973838033676148 and perplexity is 393.01116992884045
At time: 21.197643995285034 and batch: 850, loss is 6.020120306015015 and perplexity is 411.6281140860014
At time: 22.411961317062378 and batch: 900, loss is 6.019327325820923 and perplexity is 411.30183052949656
At time: 23.629969358444214 and batch: 950, loss is 5.987564430236817 and perplexity is 398.4429914827533
At time: 24.842992782592773 and batch: 1000, loss is 5.998374862670898 and perplexity is 402.7736987544316
At time: 26.054089546203613 and batch: 1050, loss is 5.976781005859375 and perplexity is 394.1694944920942
At time: 27.269243955612183 and batch: 1100, loss is 5.969232406616211 and perplexity is 391.2052688952924
At time: 28.48963189125061 and batch: 1150, loss is 6.011295223236084 and perplexity is 408.01144408392025
At time: 29.703506469726562 and batch: 1200, loss is 5.994369735717774 and perplexity is 401.1637651039525
At time: 30.915496826171875 and batch: 1250, loss is 5.973516206741333 and perplexity is 392.88470869908093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4851849409785585 and perplexity of 241.09352794818346
Finished 1 epochs...
Completing Train Step...
At time: 34.025736808776855 and batch: 50, loss is 5.808084888458252 and perplexity is 332.9808192455357
At time: 35.23391938209534 and batch: 100, loss is 5.847012453079223 and perplexity is 346.1985495417475
At time: 36.43994855880737 and batch: 150, loss is 5.790056047439575 and perplexity is 327.03135313233
At time: 37.64592671394348 and batch: 200, loss is 5.858210430145264 and perplexity is 350.09705993718893
At time: 38.87907147407532 and batch: 250, loss is 5.846843910217285 and perplexity is 346.1402051643112
At time: 40.08461356163025 and batch: 300, loss is 5.809942626953125 and perplexity is 333.59998547790315
At time: 41.292561531066895 and batch: 350, loss is 5.851260623931885 and perplexity is 347.672388472413
At time: 42.49787712097168 and batch: 400, loss is 5.788548698425293 and perplexity is 326.5387740820855
At time: 43.70339322090149 and batch: 450, loss is 5.770228424072266 and perplexity is 320.6109595446372
At time: 44.90409803390503 and batch: 500, loss is 5.79693039894104 and perplexity is 329.2872265554323
At time: 46.10118746757507 and batch: 550, loss is 5.800286979675293 and perplexity is 330.39436277269766
At time: 47.298383951187134 and batch: 600, loss is 5.78865065574646 and perplexity is 326.5720687980393
At time: 48.496511459350586 and batch: 650, loss is 5.822825784683228 and perplexity is 337.92561073691104
At time: 49.696481466293335 and batch: 700, loss is 5.813529663085937 and perplexity is 334.79876943502404
At time: 50.88781976699829 and batch: 750, loss is 5.7343003463745115 and perplexity is 309.2964946250665
At time: 52.07940435409546 and batch: 800, loss is 5.772469863891602 and perplexity is 321.3303957010933
At time: 53.271246671676636 and batch: 850, loss is 5.796716175079346 and perplexity is 329.2166929294156
At time: 54.46288704872131 and batch: 900, loss is 5.82338851928711 and perplexity is 338.11582668713976
At time: 55.66860389709473 and batch: 950, loss is 5.789909076690674 and perplexity is 326.9832926212773
At time: 56.87671089172363 and batch: 1000, loss is 5.8109814739227295 and perplexity is 333.9467248852846
At time: 58.08314800262451 and batch: 1050, loss is 5.793983125686646 and perplexity is 328.3181558791143
At time: 59.28779125213623 and batch: 1100, loss is 5.767598371505738 and perplexity is 319.76884375744544
At time: 60.49305605888367 and batch: 1150, loss is 5.846688747406006 and perplexity is 346.0865012435145
At time: 61.70107674598694 and batch: 1200, loss is 5.80643783569336 and perplexity is 332.43283367099957
At time: 62.908528089523315 and batch: 1250, loss is 5.804755773544311 and perplexity is 331.8741310024161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.528866510321624 and perplexity of 251.85827041882334
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 65.93752431869507 and batch: 50, loss is 5.721363220214844 and perplexity is 305.32085895325446
At time: 67.13865971565247 and batch: 100, loss is 5.705466527938842 and perplexity is 300.5056416290135
At time: 68.34294867515564 and batch: 150, loss is 5.599686584472656 and perplexity is 270.34166487157137
At time: 69.54883289337158 and batch: 200, loss is 5.633925666809082 and perplexity is 279.7582022831124
At time: 70.75349235534668 and batch: 250, loss is 5.649585847854614 and perplexity is 284.1737502811898
At time: 71.95443558692932 and batch: 300, loss is 5.625334339141846 and perplexity is 277.3650029996433
At time: 73.15607595443726 and batch: 350, loss is 5.652627668380737 and perplexity is 285.0394718450723
At time: 74.35830187797546 and batch: 400, loss is 5.602408323287964 and perplexity is 271.0784665102131
At time: 75.55864453315735 and batch: 450, loss is 5.575598211288452 and perplexity is 263.9073808001357
At time: 76.76485657691956 and batch: 500, loss is 5.5695790195465085 and perplexity is 262.32364286578775
At time: 77.96823763847351 and batch: 550, loss is 5.584153337478638 and perplexity is 266.17482706029426
At time: 79.16952705383301 and batch: 600, loss is 5.574516115188598 and perplexity is 263.6219621057278
At time: 80.38156008720398 and batch: 650, loss is 5.57043574333191 and perplexity is 262.54847806718436
At time: 81.58515310287476 and batch: 700, loss is 5.5972975063323975 and perplexity is 269.6965684093881
At time: 82.7904908657074 and batch: 750, loss is 5.557687120437622 and perplexity is 259.22259180919417
At time: 83.99366688728333 and batch: 800, loss is 5.567481479644775 and perplexity is 261.77398522365326
At time: 85.19716930389404 and batch: 850, loss is 5.586999492645264 and perplexity is 266.9334810308875
At time: 86.40049862861633 and batch: 900, loss is 5.57206524848938 and perplexity is 262.9766509264785
At time: 87.60504603385925 and batch: 950, loss is 5.535784530639648 and perplexity is 253.60667181598097
At time: 88.81226539611816 and batch: 1000, loss is 5.499863700866699 and perplexity is 244.65858323870037
At time: 90.01488041877747 and batch: 1050, loss is 5.496193304061889 and perplexity is 243.76223513939746
At time: 91.22002458572388 and batch: 1100, loss is 5.466364326477051 and perplexity is 236.5984325142845
At time: 92.42292737960815 and batch: 1150, loss is 5.486406383514404 and perplexity is 241.38818975793612
At time: 93.62810611724854 and batch: 1200, loss is 5.466517162322998 and perplexity is 236.63459599933557
At time: 94.83385014533997 and batch: 1250, loss is 5.489046592712402 and perplexity is 242.02634714061836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.266920103643932 and perplexity of 193.81810257690734
Finished 3 epochs...
Completing Train Step...
At time: 97.87845277786255 and batch: 50, loss is 5.541989946365357 and perplexity is 255.18529960054613
At time: 99.10686564445496 and batch: 100, loss is 5.5497591686248775 and perplexity is 257.17561249071116
At time: 100.31304693222046 and batch: 150, loss is 5.458712348937988 and perplexity is 234.7948957345352
At time: 101.518479347229 and batch: 200, loss is 5.505159978866577 and perplexity is 245.9578005817244
At time: 102.72203993797302 and batch: 250, loss is 5.52970817565918 and perplexity is 252.07034002831222
At time: 103.92712092399597 and batch: 300, loss is 5.509658346176147 and perplexity is 247.06670136299522
At time: 105.13111090660095 and batch: 350, loss is 5.540851831436157 and perplexity is 254.89503461014982
At time: 106.33950853347778 and batch: 400, loss is 5.499468183517456 and perplexity is 244.5618356583226
At time: 107.54674029350281 and batch: 450, loss is 5.475772151947021 and perplexity is 238.83481251559954
At time: 108.75137281417847 and batch: 500, loss is 5.476538343429565 and perplexity is 239.01787583650705
At time: 109.95616292953491 and batch: 550, loss is 5.4921635818481445 and perplexity is 242.7819175759019
At time: 111.16077756881714 and batch: 600, loss is 5.492263135910034 and perplexity is 242.80608870509695
At time: 112.36724162101746 and batch: 650, loss is 5.491511735916138 and perplexity is 242.62371273875686
At time: 113.57934999465942 and batch: 700, loss is 5.521922092437745 and perplexity is 250.11532023321044
At time: 114.78122353553772 and batch: 750, loss is 5.485819959640503 and perplexity is 241.2466754583218
At time: 115.9858877658844 and batch: 800, loss is 5.500724239349365 and perplexity is 244.86921197876197
At time: 117.18010306358337 and batch: 850, loss is 5.5203141021728515 and perplexity is 249.7134604131026
At time: 118.37941980361938 and batch: 900, loss is 5.509150505065918 and perplexity is 246.9412625892533
At time: 119.5768690109253 and batch: 950, loss is 5.483787498474121 and perplexity is 240.75684890427723
At time: 120.77390217781067 and batch: 1000, loss is 5.4524028110504155 and perplexity is 233.31811225400955
At time: 121.96799087524414 and batch: 1050, loss is 5.450655517578125 and perplexity is 232.91079299627094
At time: 123.1624367237091 and batch: 1100, loss is 5.423717908859253 and perplexity is 226.72048357543127
At time: 124.35422730445862 and batch: 1150, loss is 5.452960529327393 and perplexity is 233.44827432307193
At time: 125.54540085792542 and batch: 1200, loss is 5.439932670593262 and perplexity is 230.42666844745636
At time: 126.765864610672 and batch: 1250, loss is 5.4619824504852295 and perplexity is 235.5639556530433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.247852631729015 and perplexity of 190.15749155457775
Finished 4 epochs...
Completing Train Step...
At time: 129.77902173995972 and batch: 50, loss is 5.480937328338623 and perplexity is 240.07162788571958
At time: 130.98219108581543 and batch: 100, loss is 5.489834747314453 and perplexity is 242.21717651157994
At time: 132.19078183174133 and batch: 150, loss is 5.402330331802368 and perplexity is 221.92296825218952
At time: 133.39888405799866 and batch: 200, loss is 5.450112800598145 and perplexity is 232.7844226488598
At time: 134.60683941841125 and batch: 250, loss is 5.473136911392212 and perplexity is 238.20625389709107
At time: 135.81801843643188 and batch: 300, loss is 5.456729917526245 and perplexity is 234.3298920291803
At time: 137.02708268165588 and batch: 350, loss is 5.48677056312561 and perplexity is 241.47611442424554
At time: 138.23642134666443 and batch: 400, loss is 5.451348037719726 and perplexity is 233.07214427467392
At time: 139.44555282592773 and batch: 450, loss is 5.425890941619873 and perplexity is 227.21369029684624
At time: 140.6552402973175 and batch: 500, loss is 5.428536338806152 and perplexity is 227.81555649041462
At time: 141.86338758468628 and batch: 550, loss is 5.438516788482666 and perplexity is 230.1006423115501
At time: 143.07255482673645 and batch: 600, loss is 5.445712785720826 and perplexity is 231.76241779312667
At time: 144.28176164627075 and batch: 650, loss is 5.445156784057617 and perplexity is 231.63359331999382
At time: 145.49019527435303 and batch: 700, loss is 5.472465410232544 and perplexity is 238.0463518145844
At time: 146.69822144508362 and batch: 750, loss is 5.441155834197998 and perplexity is 230.708690406198
At time: 147.90691328048706 and batch: 800, loss is 5.458776636123657 and perplexity is 234.809990522787
At time: 149.11391401290894 and batch: 850, loss is 5.478528871536255 and perplexity is 239.49412146927116
At time: 150.31757974624634 and batch: 900, loss is 5.464872131347656 and perplexity is 236.24564476520572
At time: 151.51928091049194 and batch: 950, loss is 5.443137187957763 and perplexity is 231.16625909036907
At time: 152.7244095802307 and batch: 1000, loss is 5.419696874618531 and perplexity is 225.81066318326162
At time: 153.93096113204956 and batch: 1050, loss is 5.418598499298096 and perplexity is 225.56277448606534
At time: 155.13671350479126 and batch: 1100, loss is 5.392928133010864 and perplexity is 219.84618286202013
At time: 156.34109449386597 and batch: 1150, loss is 5.424904117584228 and perplexity is 226.98958096245963
At time: 157.5942084789276 and batch: 1200, loss is 5.416591186523437 and perplexity is 225.11045357400278
At time: 158.79628348350525 and batch: 1250, loss is 5.434600458145142 and perplexity is 229.20125448483765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.236308522468065 and perplexity of 187.97491488568645
Finished 5 epochs...
Completing Train Step...
At time: 161.85770750045776 and batch: 50, loss is 5.438669548034668 and perplexity is 230.13579506747638
At time: 163.06509065628052 and batch: 100, loss is 5.4475515174865725 and perplexity is 232.1889587398477
At time: 164.271635055542 and batch: 150, loss is 5.364714288711548 and perplexity is 213.73016083697848
At time: 165.47381401062012 and batch: 200, loss is 5.416111688613892 and perplexity is 225.00253945646182
At time: 166.67731881141663 and batch: 250, loss is 5.435947370529175 and perplexity is 229.51017649167437
At time: 167.88053107261658 and batch: 300, loss is 5.417392702102661 and perplexity is 225.29095543745302
At time: 169.08419036865234 and batch: 350, loss is 5.447168560028076 and perplexity is 232.10005727014465
At time: 170.2912826538086 and batch: 400, loss is 5.418134889602661 and perplexity is 225.4582256336879
At time: 171.49602484703064 and batch: 450, loss is 5.392246036529541 and perplexity is 219.69627768496662
At time: 172.69979786872864 and batch: 500, loss is 5.3967032146453855 and perplexity is 220.6776886630024
At time: 173.9015166759491 and batch: 550, loss is 5.402193689346314 and perplexity is 221.8926462244381
At time: 175.10614275932312 and batch: 600, loss is 5.415278224945069 and perplexity is 224.81508614304641
At time: 176.31214141845703 and batch: 650, loss is 5.419061651229859 and perplexity is 225.6672685172407
At time: 177.5176112651825 and batch: 700, loss is 5.443961114883423 and perplexity is 231.3568016813439
At time: 178.7201063632965 and batch: 750, loss is 5.410888624191284 and perplexity is 223.83040044112582
At time: 179.9221589565277 and batch: 800, loss is 5.428448514938355 and perplexity is 227.79554972564773
At time: 181.12438201904297 and batch: 850, loss is 5.448560838699341 and perplexity is 232.42343028992377
At time: 182.32841658592224 and batch: 900, loss is 5.4326980018615725 and perplexity is 228.7656236337573
At time: 183.5362093448639 and batch: 950, loss is 5.412853593826294 and perplexity is 224.2706527809694
At time: 184.74033784866333 and batch: 1000, loss is 5.392195568084717 and perplexity is 219.68519023528373
At time: 185.94084429740906 and batch: 1050, loss is 5.3915629577636714 and perplexity is 219.54625906583917
At time: 187.17062425613403 and batch: 1100, loss is 5.365092687606811 and perplexity is 213.8110513972077
At time: 188.37702560424805 and batch: 1150, loss is 5.397438373565674 and perplexity is 220.8399814825292
At time: 189.58186221122742 and batch: 1200, loss is 5.393651475906372 and perplexity is 220.00526456487145
At time: 190.78402519226074 and batch: 1250, loss is 5.412297134399414 and perplexity is 224.14588997799518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.226017193202555 and perplexity of 186.0503234314474
Finished 6 epochs...
Completing Train Step...
At time: 193.7404341697693 and batch: 50, loss is 5.400012187957763 and perplexity is 221.40911471268095
At time: 194.9636025428772 and batch: 100, loss is 5.41192852973938 and perplexity is 224.0632839838343
At time: 196.16430640220642 and batch: 150, loss is 5.3317725944519045 and perplexity is 206.8042294791361
At time: 197.36563324928284 and batch: 200, loss is 5.385702257156372 and perplexity is 218.2633272836952
At time: 198.55806374549866 and batch: 250, loss is 5.4055583286285405 and perplexity is 222.6404923490683
At time: 199.75099682807922 and batch: 300, loss is 5.389479322433472 and perplexity is 219.0892809770141
At time: 200.94381642341614 and batch: 350, loss is 5.416835222244263 and perplexity is 225.16539526940159
At time: 202.13826942443848 and batch: 400, loss is 5.388766326904297 and perplexity is 218.9331269743555
At time: 203.34173774719238 and batch: 450, loss is 5.363647718429565 and perplexity is 213.50232412259902
At time: 204.5503966808319 and batch: 500, loss is 5.366742792129517 and perplexity is 214.16415322746613
At time: 205.7594132423401 and batch: 550, loss is 5.370109758377075 and perplexity is 214.88645199825157
At time: 206.96861290931702 and batch: 600, loss is 5.385770063400269 and perplexity is 218.27812740186332
At time: 208.1769483089447 and batch: 650, loss is 5.391449928283691 and perplexity is 219.5214452687171
At time: 209.38391733169556 and batch: 700, loss is 5.418481826782227 and perplexity is 225.5364590448539
At time: 210.5904049873352 and batch: 750, loss is 5.387169523239136 and perplexity is 218.5838127221397
At time: 211.79585766792297 and batch: 800, loss is 5.404713907241821 and perplexity is 222.4525693100622
At time: 213.00178456306458 and batch: 850, loss is 5.42433795928955 and perplexity is 226.86110530060483
At time: 214.21349382400513 and batch: 900, loss is 5.40743275642395 and perplexity is 223.05820724221152
At time: 215.4202127456665 and batch: 950, loss is 5.387997846603394 and perplexity is 218.76494580929358
At time: 216.63053822517395 and batch: 1000, loss is 5.370662784576416 and perplexity is 215.0053227023672
At time: 217.88325929641724 and batch: 1050, loss is 5.373551549911499 and perplexity is 215.6273205958791
At time: 219.0900363922119 and batch: 1100, loss is 5.347094449996948 and perplexity is 209.9972530666567
At time: 220.3029510974884 and batch: 1150, loss is 5.381555919647217 and perplexity is 217.3602074765495
At time: 221.50738525390625 and batch: 1200, loss is 5.378963270187378 and perplexity is 216.79739855043297
At time: 222.71582627296448 and batch: 1250, loss is 5.3975347137451175 and perplexity is 220.86125827086195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.23094143832687 and perplexity of 186.96874022837116
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 225.75987434387207 and batch: 50, loss is 5.384774961471558 and perplexity is 218.06102645299163
At time: 226.96640300750732 and batch: 100, loss is 5.4048559188842775 and perplexity is 222.48416240803928
At time: 228.17229747772217 and batch: 150, loss is 5.322030515670776 and perplexity is 204.79930829027884
At time: 229.37676215171814 and batch: 200, loss is 5.363047981262207 and perplexity is 213.37431723258712
At time: 230.58488082885742 and batch: 250, loss is 5.3823779201507564 and perplexity is 217.53895113016682
At time: 231.79205513000488 and batch: 300, loss is 5.360060520172119 and perplexity is 212.73782098950073
At time: 232.99742794036865 and batch: 350, loss is 5.383637714385986 and perplexity is 217.81317814527654
At time: 234.20154285430908 and batch: 400, loss is 5.341772098541259 and perplexity is 208.88254295201173
At time: 235.4075644016266 and batch: 450, loss is 5.315815906524659 and perplexity is 203.5305072706367
At time: 236.61756134033203 and batch: 500, loss is 5.3065007781982425 and perplexity is 201.6433974577177
At time: 237.82571268081665 and batch: 550, loss is 5.313733892440796 and perplexity is 203.10719471227168
At time: 239.03080415725708 and batch: 600, loss is 5.3278907108306885 and perplexity is 206.00299568224975
At time: 240.23660039901733 and batch: 650, loss is 5.334100666046143 and perplexity is 207.28624539743134
At time: 241.441223859787 and batch: 700, loss is 5.351550331115723 and perplexity is 210.93506369625396
At time: 242.64443397521973 and batch: 750, loss is 5.315219058990478 and perplexity is 203.40906683355945
At time: 243.8567943572998 and batch: 800, loss is 5.311823711395264 and perplexity is 202.7195935108116
At time: 245.06312036514282 and batch: 850, loss is 5.330318603515625 and perplexity is 206.5037564993194
At time: 246.26770424842834 and batch: 900, loss is 5.30771110534668 and perplexity is 201.88759968844795
At time: 247.47182631492615 and batch: 950, loss is 5.277665243148804 and perplexity is 195.9119342385512
At time: 248.70310640335083 and batch: 1000, loss is 5.25813066482544 and perplexity is 192.12201497066263
At time: 249.9103991985321 and batch: 1050, loss is 5.244756298065186 and perplexity is 189.56961111872113
At time: 251.1168715953827 and batch: 1100, loss is 5.203436231613159 and perplexity is 181.89620671315663
At time: 252.32257986068726 and batch: 1150, loss is 5.229887561798096 and perplexity is 186.7718020539173
At time: 253.53027081489563 and batch: 1200, loss is 5.242305717468262 and perplexity is 189.1056242587454
At time: 254.74080181121826 and batch: 1250, loss is 5.288832473754883 and perplexity is 198.1119893847752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.155610690151688 and perplexity of 173.40166912266307
Finished 8 epochs...
Completing Train Step...
At time: 257.81021428108215 and batch: 50, loss is 5.3285368537902835 and perplexity is 206.13614608002882
At time: 259.04652404785156 and batch: 100, loss is 5.353721361160279 and perplexity is 211.39350752458336
At time: 260.2565186023712 and batch: 150, loss is 5.273707370758057 and perplexity is 195.13807223673243
At time: 261.4686276912689 and batch: 200, loss is 5.315452070236206 and perplexity is 203.45646895601396
At time: 262.68068647384644 and batch: 250, loss is 5.340331954956055 and perplexity is 208.58193860638295
At time: 263.8940830230713 and batch: 300, loss is 5.321630477905273 and perplexity is 204.71739721746752
At time: 265.10605096817017 and batch: 350, loss is 5.346347923278809 and perplexity is 209.84054300790734
At time: 266.31867837905884 and batch: 400, loss is 5.308910007476807 and perplexity is 202.12978831298003
At time: 267.531964302063 and batch: 450, loss is 5.283613996505737 and perplexity is 197.08083933199876
At time: 268.74401783943176 and batch: 500, loss is 5.276647148132324 and perplexity is 195.7125787732564
At time: 269.9541437625885 and batch: 550, loss is 5.284735946655274 and perplexity is 197.30207829549468
At time: 271.1624970436096 and batch: 600, loss is 5.300881938934326 and perplexity is 200.51357274301347
At time: 272.3685052394867 and batch: 650, loss is 5.310255966186523 and perplexity is 202.40202983387664
At time: 273.5743205547333 and batch: 700, loss is 5.327210493087769 and perplexity is 205.86291643708725
At time: 274.78103852272034 and batch: 750, loss is 5.293675565719605 and perplexity is 199.07379113588175
At time: 275.9923255443573 and batch: 800, loss is 5.296167488098145 and perplexity is 199.57048617671148
At time: 277.20012187957764 and batch: 850, loss is 5.318553619384765 and perplexity is 204.08847879221764
At time: 278.40742444992065 and batch: 900, loss is 5.2963849067687985 and perplexity is 199.61388124379573
At time: 279.6597640514374 and batch: 950, loss is 5.2684620761871335 and perplexity is 194.11719530658254
At time: 280.85614228248596 and batch: 1000, loss is 5.253558483123779 and perplexity is 191.2456032928163
At time: 282.0540130138397 and batch: 1050, loss is 5.24514476776123 and perplexity is 189.64326747363637
At time: 283.2535488605499 and batch: 1100, loss is 5.210009422302246 and perplexity is 183.09578336940146
At time: 284.45534324645996 and batch: 1150, loss is 5.243005695343018 and perplexity is 189.23804035047908
At time: 285.6600534915924 and batch: 1200, loss is 5.258074274063111 and perplexity is 192.11118136923864
At time: 286.8539409637451 and batch: 1250, loss is 5.295466585159302 and perplexity is 199.43065564598336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.152226134808394 and perplexity of 172.81577363404767
Finished 9 epochs...
Completing Train Step...
At time: 289.81081557273865 and batch: 50, loss is 5.316083631515503 and perplexity is 203.5850047686778
At time: 291.00921654701233 and batch: 100, loss is 5.340299520492554 and perplexity is 208.57517347282064
At time: 292.21763253211975 and batch: 150, loss is 5.259248943328857 and perplexity is 192.33698106361587
At time: 293.423645734787 and batch: 200, loss is 5.299200448989868 and perplexity is 200.17669449478657
At time: 294.62882447242737 and batch: 250, loss is 5.325565509796142 and perplexity is 205.52455375596114
At time: 295.8318271636963 and batch: 300, loss is 5.307828922271728 and perplexity is 201.9113868658871
At time: 297.0341086387634 and batch: 350, loss is 5.332301864624023 and perplexity is 206.9137137600937
At time: 298.2351689338684 and batch: 400, loss is 5.295254602432251 and perplexity is 199.38838427230078
At time: 299.4413058757782 and batch: 450, loss is 5.26932936668396 and perplexity is 194.285624333232
At time: 300.6463918685913 and batch: 500, loss is 5.26448543548584 and perplexity is 193.3467937871641
At time: 301.8512279987335 and batch: 550, loss is 5.273694038391113 and perplexity is 195.13547060169168
At time: 303.05333495140076 and batch: 600, loss is 5.291021184921265 and perplexity is 198.54607417796413
At time: 304.2589147090912 and batch: 650, loss is 5.301416463851929 and perplexity is 200.62078089412364
At time: 305.4643168449402 and batch: 700, loss is 5.3177347660064695 and perplexity is 203.9214286560058
At time: 306.67158460617065 and batch: 750, loss is 5.284787645339966 and perplexity is 197.31227881710413
At time: 307.87431478500366 and batch: 800, loss is 5.290556230545044 and perplexity is 198.45378076966838
At time: 309.1313488483429 and batch: 850, loss is 5.314171829223633 and perplexity is 203.1961623033634
At time: 310.3352863788605 and batch: 900, loss is 5.292846632003784 and perplexity is 198.9088405344757
At time: 311.54350566864014 and batch: 950, loss is 5.266469984054566 and perplexity is 193.7308808837326
At time: 312.7497639656067 and batch: 1000, loss is 5.253352642059326 and perplexity is 191.20624114557467
At time: 313.95616269111633 and batch: 1050, loss is 5.247145919799805 and perplexity is 190.02315246197836
At time: 315.1593904495239 and batch: 1100, loss is 5.213753185272217 and perplexity is 183.78253529924748
At time: 316.36369347572327 and batch: 1150, loss is 5.249059429168701 and perplexity is 190.38711165312907
At time: 317.5704882144928 and batch: 1200, loss is 5.264555349349975 and perplexity is 193.36031188118153
At time: 318.77407002449036 and batch: 1250, loss is 5.296986932754517 and perplexity is 199.734090168232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.151360950330748 and perplexity of 172.66632077067845
Finished 10 epochs...
Completing Train Step...
At time: 321.82671666145325 and batch: 50, loss is 5.307437992095947 and perplexity is 201.8324690386125
At time: 323.0822229385376 and batch: 100, loss is 5.331739768981934 and perplexity is 206.79744114452728
At time: 324.2857301235199 and batch: 150, loss is 5.250482969284057 and perplexity is 190.65832834213145
At time: 325.48919701576233 and batch: 200, loss is 5.289344959259033 and perplexity is 198.21354492818247
At time: 326.69210600852966 and batch: 250, loss is 5.316858472824097 and perplexity is 203.74281197005678
At time: 327.90220832824707 and batch: 300, loss is 5.299198322296142 and perplexity is 200.17626878071897
At time: 329.11079359054565 and batch: 350, loss is 5.323728799819946 and perplexity is 205.14741121448282
At time: 330.3177099227905 and batch: 400, loss is 5.286552438735962 and perplexity is 197.66080166866672
At time: 331.5235104560852 and batch: 450, loss is 5.260579376220703 and perplexity is 192.5930428082548
At time: 332.7262341976166 and batch: 500, loss is 5.256962699890137 and perplexity is 191.89775418373853
At time: 333.93388652801514 and batch: 550, loss is 5.266798086166382 and perplexity is 193.79445482367626
At time: 335.14251232147217 and batch: 600, loss is 5.284829225540161 and perplexity is 197.3204832717285
At time: 336.34956097602844 and batch: 650, loss is 5.296189250946045 and perplexity is 199.5748294461084
At time: 337.5556626319885 and batch: 700, loss is 5.312098608016968 and perplexity is 202.77532810249542
At time: 338.75902676582336 and batch: 750, loss is 5.279219274520874 and perplexity is 196.21662421809506
At time: 340.00909948349 and batch: 800, loss is 5.2871189785003665 and perplexity is 197.77281609999486
At time: 341.2152864933014 and batch: 850, loss is 5.3115027904510494 and perplexity is 202.65454698540574
At time: 342.4222390651703 and batch: 900, loss is 5.290330018997192 and perplexity is 198.40889330996572
At time: 343.62637996673584 and batch: 950, loss is 5.265251712799072 and perplexity is 193.4950078281056
At time: 344.83246541023254 and batch: 1000, loss is 5.2530146503448485 and perplexity is 191.14162594062813
At time: 346.03620767593384 and batch: 1050, loss is 5.247951784133911 and perplexity is 190.17634706194343
At time: 347.24216532707214 and batch: 1100, loss is 5.215579309463501 and perplexity is 184.11845165203286
At time: 348.4487864971161 and batch: 1150, loss is 5.2515472984313964 and perplexity is 190.8613595850377
At time: 349.6537277698517 and batch: 1200, loss is 5.267525901794434 and perplexity is 193.93555279695323
At time: 350.85791993141174 and batch: 1250, loss is 5.296557559967041 and perplexity is 199.648348194135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.150673525176779 and perplexity of 172.5476663862142
Finished 11 epochs...
Completing Train Step...
At time: 353.84159684181213 and batch: 50, loss is 5.300862874984741 and perplexity is 200.50975019880806
At time: 355.06595277786255 and batch: 100, loss is 5.325047245025635 and perplexity is 205.41806521728776
At time: 356.26058530807495 and batch: 150, loss is 5.2438661479949955 and perplexity is 189.40094079816413
At time: 357.45332860946655 and batch: 200, loss is 5.282260246276856 and perplexity is 196.814221608265
At time: 358.64161920547485 and batch: 250, loss is 5.3102633571624756 and perplexity is 202.40352578794008
At time: 359.829971075058 and batch: 300, loss is 5.292527542114258 and perplexity is 198.84538085973253
At time: 361.0166971683502 and batch: 350, loss is 5.317337980270386 and perplexity is 203.84053159229515
At time: 362.2131004333496 and batch: 400, loss is 5.27993350982666 and perplexity is 196.3568191188078
At time: 363.41835284233093 and batch: 450, loss is 5.2546133708953855 and perplexity is 191.44745238645572
At time: 364.6253778934479 and batch: 500, loss is 5.251777076721192 and perplexity is 190.90522042077208
At time: 365.8280255794525 and batch: 550, loss is 5.2620237350463865 and perplexity is 192.8714172573015
At time: 367.0285849571228 and batch: 600, loss is 5.280503129959106 and perplexity is 196.4686997778338
At time: 368.2298913002014 and batch: 650, loss is 5.292442579269409 and perplexity is 198.82848710817026
At time: 369.4400546550751 and batch: 700, loss is 5.308167343139648 and perplexity is 201.97972945630033
At time: 370.6932098865509 and batch: 750, loss is 5.27514066696167 and perplexity is 195.41796343042648
At time: 371.89885234832764 and batch: 800, loss is 5.284331779479981 and perplexity is 197.2223513844161
At time: 373.1027801036835 and batch: 850, loss is 5.3092196178436275 and perplexity is 202.19237947982208
At time: 374.3055408000946 and batch: 900, loss is 5.288226041793823 and perplexity is 197.9918843639857
At time: 375.5133709907532 and batch: 950, loss is 5.263753776550293 and perplexity is 193.20538161687782
At time: 376.7219970226288 and batch: 1000, loss is 5.252189865112305 and perplexity is 190.98404014637933
At time: 377.93092131614685 and batch: 1050, loss is 5.247835483551025 and perplexity is 190.1542307280252
At time: 379.1378195285797 and batch: 1100, loss is 5.216091899871826 and perplexity is 184.21285319694164
At time: 380.345112323761 and batch: 1150, loss is 5.252717790603637 and perplexity is 191.08489210842873
At time: 381.54738903045654 and batch: 1200, loss is 5.2683710098266605 and perplexity is 194.09951856499126
At time: 382.7516784667969 and batch: 1250, loss is 5.295082540512085 and perplexity is 199.35408007535173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.150112179944115 and perplexity of 172.45083475681045
Finished 12 epochs...
Completing Train Step...
At time: 385.88363790512085 and batch: 50, loss is 5.295276136398315 and perplexity is 199.39267794123106
At time: 387.086229801178 and batch: 100, loss is 5.3197293376922605 and perplexity is 204.32857046551354
At time: 388.29103541374207 and batch: 150, loss is 5.238833332061768 and perplexity is 188.45011539713343
At time: 389.49209213256836 and batch: 200, loss is 5.276916103363037 and perplexity is 195.76522377429072
At time: 390.69420647621155 and batch: 250, loss is 5.304977369308472 and perplexity is 201.33644597913644
At time: 391.8986597061157 and batch: 300, loss is 5.28727991104126 and perplexity is 197.8046467430339
At time: 393.1056945323944 and batch: 350, loss is 5.312087144851684 and perplexity is 202.77300366871665
At time: 394.31269812583923 and batch: 400, loss is 5.274501743316651 and perplexity is 195.29314615153075
At time: 395.5168516635895 and batch: 450, loss is 5.249595575332641 and perplexity is 190.48921434124196
At time: 396.7209975719452 and batch: 500, loss is 5.247012128829956 and perplexity is 189.99773078075037
At time: 397.92811369895935 and batch: 550, loss is 5.258132820129394 and perplexity is 192.12242905244756
At time: 399.13296365737915 and batch: 600, loss is 5.27719560623169 and perplexity is 195.81994836340158
At time: 400.3353934288025 and batch: 650, loss is 5.289413118362427 and perplexity is 198.22705544611253
At time: 401.58411383628845 and batch: 700, loss is 5.304828605651855 and perplexity is 201.3064966609627
At time: 402.78661584854126 and batch: 750, loss is 5.271932039260864 and perplexity is 194.79194480716342
At time: 403.99234223365784 and batch: 800, loss is 5.281703977584839 and perplexity is 196.7047704635856
At time: 405.19922852516174 and batch: 850, loss is 5.307188444137573 and perplexity is 201.78210844198372
At time: 406.4064335823059 and batch: 900, loss is 5.2863682270050045 and perplexity is 197.62439358375008
At time: 407.6094810962677 and batch: 950, loss is 5.2623515319824214 and perplexity is 192.9346502801565
At time: 408.81446146965027 and batch: 1000, loss is 5.251111268997192 and perplexity is 190.77815655520962
At time: 410.0176193714142 and batch: 1050, loss is 5.247357654571533 and perplexity is 190.06339123061085
At time: 411.2246789932251 and batch: 1100, loss is 5.21611478805542 and perplexity is 184.21706954279801
At time: 412.43836545944214 and batch: 1150, loss is 5.252790317535401 and perplexity is 191.09875141194
At time: 413.6404654979706 and batch: 1200, loss is 5.26823618888855 and perplexity is 194.0733516497752
At time: 414.84400844573975 and batch: 1250, loss is 5.293288202285766 and perplexity is 198.99669216218575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1497677990989965 and perplexity of 172.39145621759818
Finished 13 epochs...
Completing Train Step...
At time: 417.90777349472046 and batch: 50, loss is 5.290414113998413 and perplexity is 198.4255792076812
At time: 419.13838386535645 and batch: 100, loss is 5.3151952838897705 and perplexity is 203.40423081999919
At time: 420.34389328956604 and batch: 150, loss is 5.234229593276978 and perplexity is 187.58453427536193
At time: 421.55956649780273 and batch: 200, loss is 5.272455377578735 and perplexity is 194.89391357564818
At time: 422.7739417552948 and batch: 250, loss is 5.300492057800293 and perplexity is 200.43541152166623
At time: 423.99072647094727 and batch: 300, loss is 5.282826547622681 and perplexity is 196.92570933168366
At time: 425.1981451511383 and batch: 350, loss is 5.307576103210449 and perplexity is 201.86034627088765
At time: 426.404878616333 and batch: 400, loss is 5.269963474273681 and perplexity is 194.40886139084293
At time: 427.6110622882843 and batch: 450, loss is 5.245293779373169 and perplexity is 189.6715286281837
At time: 428.80760979652405 and batch: 500, loss is 5.243089857101441 and perplexity is 189.25396762693987
At time: 430.00925302505493 and batch: 550, loss is 5.254335880279541 and perplexity is 191.39433488513606
At time: 431.24499344825745 and batch: 600, loss is 5.2740505504608155 and perplexity is 195.2050511546047
At time: 432.4468240737915 and batch: 650, loss is 5.286349229812622 and perplexity is 197.6206393107861
At time: 433.64140152931213 and batch: 700, loss is 5.301875257492066 and perplexity is 200.7128455502016
At time: 434.83461451530457 and batch: 750, loss is 5.269181833267212 and perplexity is 194.2569628255663
At time: 436.0281627178192 and batch: 800, loss is 5.279362621307373 and perplexity is 196.24475325669
At time: 437.2213315963745 and batch: 850, loss is 5.305092840194702 and perplexity is 201.35969581929814
At time: 438.42312121391296 and batch: 900, loss is 5.284339456558228 and perplexity is 197.22386548165164
At time: 439.6320524215698 and batch: 950, loss is 5.2605899143218995 and perplexity is 192.59507238392348
At time: 440.84311389923096 and batch: 1000, loss is 5.249780626296997 and perplexity is 190.52446781579926
At time: 442.0511920452118 and batch: 1050, loss is 5.246420488357544 and perplexity is 189.88535368025248
At time: 443.26606965065 and batch: 1100, loss is 5.215510883331299 and perplexity is 184.1058535695432
At time: 444.48141169548035 and batch: 1150, loss is 5.25231484413147 and perplexity is 191.00791063601707
At time: 445.68754386901855 and batch: 1200, loss is 5.267690553665161 and perplexity is 193.96748727748556
At time: 446.90241384506226 and batch: 1250, loss is 5.291413373947144 and perplexity is 198.62395704079188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.149543261005931 and perplexity of 172.3527521141937
Finished 14 epochs...
Completing Train Step...
At time: 449.96512818336487 and batch: 50, loss is 5.285855121612549 and perplexity is 197.52301745226558
At time: 451.1670718193054 and batch: 100, loss is 5.310968494415283 and perplexity is 202.54629838529547
At time: 452.36937737464905 and batch: 150, loss is 5.230332136154175 and perplexity is 186.8548544676679
At time: 453.5750777721405 and batch: 200, loss is 5.268520421981812 and perplexity is 194.12852155901984
At time: 454.7856545448303 and batch: 250, loss is 5.296673078536987 and perplexity is 199.67141261796948
At time: 455.9874036312103 and batch: 300, loss is 5.27918511390686 and perplexity is 196.209921452218
At time: 457.1913344860077 and batch: 350, loss is 5.3039094257354735 and perplexity is 201.12154478724068
At time: 458.39898920059204 and batch: 400, loss is 5.266229915618896 and perplexity is 193.68437779640303
At time: 459.6054027080536 and batch: 450, loss is 5.241294097900391 and perplexity is 188.91441803913372
At time: 460.80895924568176 and batch: 500, loss is 5.239437198638916 and perplexity is 188.56394848982225
At time: 462.0397012233734 and batch: 550, loss is 5.251260719299316 and perplexity is 190.80667053900413
At time: 463.2409586906433 and batch: 600, loss is 5.271097774505615 and perplexity is 194.62950452152845
At time: 464.4446346759796 and batch: 650, loss is 5.283614559173584 and perplexity is 197.08095022308143
At time: 465.66109013557434 and batch: 700, loss is 5.299067602157593 and perplexity is 200.15010342133667
At time: 466.8649468421936 and batch: 750, loss is 5.266459426879883 and perplexity is 193.72883564377761
At time: 468.06748127937317 and batch: 800, loss is 5.276814060211182 and perplexity is 195.74524829303098
At time: 469.2724950313568 and batch: 850, loss is 5.3031768798828125 and perplexity is 200.97426798381778
At time: 470.47622299194336 and batch: 900, loss is 5.282402372360229 and perplexity is 196.84219603063482
At time: 471.6821608543396 and batch: 950, loss is 5.2590587711334225 and perplexity is 192.30040739542133
At time: 472.88583302497864 and batch: 1000, loss is 5.248446321487426 and perplexity is 190.27041962860247
At time: 474.09084033966064 and batch: 1050, loss is 5.245496072769165 and perplexity is 189.70990180702316
At time: 475.296514749527 and batch: 1100, loss is 5.214637250900268 and perplexity is 183.94508296255586
At time: 476.50112414360046 and batch: 1150, loss is 5.251656847000122 and perplexity is 190.88226931910006
At time: 477.7072043418884 and batch: 1200, loss is 5.266891365051269 and perplexity is 193.81253259744375
At time: 478.91304087638855 and batch: 1250, loss is 5.28946346282959 and perplexity is 198.2370353328102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1492594698049725 and perplexity of 172.30384685945143
Finished 15 epochs...
Completing Train Step...
At time: 481.94513845443726 and batch: 50, loss is 5.281967210769653 and perplexity is 196.75655650238576
At time: 483.180508852005 and batch: 100, loss is 5.307301054000854 and perplexity is 201.80483237707358
At time: 484.386118888855 and batch: 150, loss is 5.226717185974121 and perplexity is 186.18060290502223
At time: 485.5935273170471 and batch: 200, loss is 5.264837684631348 and perplexity is 193.41491202665387
At time: 486.80202984809875 and batch: 250, loss is 5.292794485092163 and perplexity is 198.89846832318932
At time: 488.00915360450745 and batch: 300, loss is 5.2752954864501955 and perplexity is 195.44822028168812
At time: 489.2148656845093 and batch: 350, loss is 5.300247526168823 and perplexity is 200.3864047155838
At time: 490.4203381538391 and batch: 400, loss is 5.262572898864746 and perplexity is 192.97736434974715
At time: 491.6284873485565 and batch: 450, loss is 5.237605800628662 and perplexity is 188.2189288802992
At time: 492.8666558265686 and batch: 500, loss is 5.236151323318482 and perplexity is 187.94536771137953
At time: 494.073712348938 and batch: 550, loss is 5.248220930099487 and perplexity is 190.2275391472657
At time: 495.2855613231659 and batch: 600, loss is 5.26837191581726 and perplexity is 194.09969441741018
At time: 496.49450874328613 and batch: 650, loss is 5.281061277389527 and perplexity is 196.57838888627225
At time: 497.70441126823425 and batch: 700, loss is 5.296218376159668 and perplexity is 199.5806421902981
At time: 498.92074751853943 and batch: 750, loss is 5.263748149871827 and perplexity is 193.2042945153759
At time: 500.1360228061676 and batch: 800, loss is 5.2745383644104 and perplexity is 195.30029813110045
At time: 501.34741735458374 and batch: 850, loss is 5.30100567817688 and perplexity is 200.53838567576153
At time: 502.5571801662445 and batch: 900, loss is 5.280324182510376 and perplexity is 196.43354535074454
At time: 503.76455450057983 and batch: 950, loss is 5.257063369750977 and perplexity is 191.9170734763667
At time: 504.9700379371643 and batch: 1000, loss is 5.246849813461304 and perplexity is 189.96689373176673
At time: 506.1750137805939 and batch: 1050, loss is 5.244192790985108 and perplexity is 189.4628173930243
At time: 507.38126277923584 and batch: 1100, loss is 5.213521986007691 and perplexity is 183.74004982374817
At time: 508.5878999233246 and batch: 1150, loss is 5.250656366348267 and perplexity is 190.69139080291703
At time: 509.7896444797516 and batch: 1200, loss is 5.265911169052124 and perplexity is 193.6226514040035
At time: 510.9915370941162 and batch: 1250, loss is 5.287334175109863 and perplexity is 197.81538071918675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.149107995694571 and perplexity of 172.27774926413457
Finished 16 epochs...
Completing Train Step...
At time: 513.9600675106049 and batch: 50, loss is 5.278187036514282 and perplexity is 196.0141864610005
At time: 515.2036674022675 and batch: 100, loss is 5.303850841522217 and perplexity is 201.10976258489927
At time: 516.4126193523407 and batch: 150, loss is 5.223439750671386 and perplexity is 185.57140686998326
At time: 517.6207242012024 and batch: 200, loss is 5.261399374008179 and perplexity is 192.75103344437412
At time: 518.8289234638214 and batch: 250, loss is 5.289251804351807 and perplexity is 198.19508122379938
At time: 520.0385355949402 and batch: 300, loss is 5.271954803466797 and perplexity is 194.7963791415809
At time: 521.2479810714722 and batch: 350, loss is 5.296737089157104 and perplexity is 199.68419411798234
At time: 522.490974187851 and batch: 400, loss is 5.259254112243652 and perplexity is 192.33797523965228
At time: 523.7020032405853 and batch: 450, loss is 5.234561138153076 and perplexity is 187.6467372775097
At time: 524.912736415863 and batch: 500, loss is 5.233053798675537 and perplexity is 187.3641030089891
At time: 526.1247825622559 and batch: 550, loss is 5.245319128036499 and perplexity is 189.67633660884388
At time: 527.3332254886627 and batch: 600, loss is 5.265780582427978 and perplexity is 193.59736852643718
At time: 528.5459241867065 and batch: 650, loss is 5.278652753829956 and perplexity is 196.10549492206843
At time: 529.7590703964233 and batch: 700, loss is 5.293711938858032 and perplexity is 199.08103220613341
At time: 530.9680387973785 and batch: 750, loss is 5.261294584274292 and perplexity is 192.7308361731249
At time: 532.1807014942169 and batch: 800, loss is 5.27230019569397 and perplexity is 194.86367191734965
At time: 533.4007167816162 and batch: 850, loss is 5.298833236694336 and perplexity is 200.10320064603744
At time: 534.6138751506805 and batch: 900, loss is 5.278307876586914 and perplexity is 196.0378742607182
At time: 535.821485042572 and batch: 950, loss is 5.255209827423096 and perplexity is 191.561676530627
At time: 537.0301299095154 and batch: 1000, loss is 5.24516453742981 and perplexity is 189.64701669524297
At time: 538.2368834018707 and batch: 1050, loss is 5.242775869369507 and perplexity is 189.19455353102194
At time: 539.443675994873 and batch: 1100, loss is 5.212298107147217 and perplexity is 183.51531181504802
At time: 540.651207447052 and batch: 1150, loss is 5.249627523422241 and perplexity is 190.49530020494484
At time: 541.8558640480042 and batch: 1200, loss is 5.2645894813537595 and perplexity is 193.36691176871145
At time: 543.0599462985992 and batch: 1250, loss is 5.285239191055298 and perplexity is 197.40139444956313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148853608291515 and perplexity of 172.23392954872298
Finished 17 epochs...
Completing Train Step...
At time: 546.1112432479858 and batch: 50, loss is 5.274517650604248 and perplexity is 195.29625276048117
At time: 547.3180503845215 and batch: 100, loss is 5.300394639968872 and perplexity is 200.41588648959453
At time: 548.5240526199341 and batch: 150, loss is 5.220404739379883 and perplexity is 185.0090493671082
At time: 549.7286198139191 and batch: 200, loss is 5.258181762695313 and perplexity is 192.1318322472022
At time: 550.9367597103119 and batch: 250, loss is 5.286122560501099 and perplexity is 197.5758498529203
At time: 552.1390891075134 and batch: 300, loss is 5.2686735534667966 and perplexity is 194.15825102400453
At time: 553.3661823272705 and batch: 350, loss is 5.2933899974823 and perplexity is 199.01695010063682
At time: 554.5716924667358 and batch: 400, loss is 5.256050138473511 and perplexity is 191.72271557622037
At time: 555.7741801738739 and batch: 450, loss is 5.231201810836792 and perplexity is 187.01742808675618
At time: 556.9813420772552 and batch: 500, loss is 5.230040922164917 and perplexity is 186.80044764248495
At time: 558.1887905597687 and batch: 550, loss is 5.242403230667114 and perplexity is 189.12406545220315
At time: 559.3960211277008 and batch: 600, loss is 5.263241147994995 and perplexity is 193.10636440291776
At time: 560.6035296916962 and batch: 650, loss is 5.276045036315918 and perplexity is 195.59477338652457
At time: 561.8072509765625 and batch: 700, loss is 5.291440076828003 and perplexity is 198.629260943467
At time: 563.010665178299 and batch: 750, loss is 5.258821859359741 and perplexity is 192.25485456104005
At time: 564.2133090496063 and batch: 800, loss is 5.270326108932495 and perplexity is 194.47937356627034
At time: 565.4168090820312 and batch: 850, loss is 5.297090406417847 and perplexity is 199.75475845552688
At time: 566.6210148334503 and batch: 900, loss is 5.2764819240570064 and perplexity is 195.68024501463185
At time: 567.8262965679169 and batch: 950, loss is 5.253504514694214 and perplexity is 191.23528234645045
At time: 569.0316281318665 and batch: 1000, loss is 5.243623676300049 and perplexity is 189.35502199825396
At time: 570.237298488617 and batch: 1050, loss is 5.241747026443481 and perplexity is 189.00000215154557
At time: 571.4457898139954 and batch: 1100, loss is 5.210929679870605 and perplexity is 183.2643562030982
At time: 572.6516437530518 and batch: 1150, loss is 5.2481914901733395 and perplexity is 190.22193894499716
At time: 573.8556616306305 and batch: 1200, loss is 5.263359594345093 and perplexity is 193.12923850161198
At time: 575.0602777004242 and batch: 1250, loss is 5.283382997512818 and perplexity is 197.03531911435394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148548878022354 and perplexity of 172.1814526530684
Finished 18 epochs...
Completing Train Step...
At time: 578.0537042617798 and batch: 50, loss is 5.271352787017822 and perplexity is 194.67914380947673
At time: 579.2964298725128 and batch: 100, loss is 5.2971025085449215 and perplexity is 199.75717592762572
At time: 580.4980800151825 and batch: 150, loss is 5.2173840522766115 and perplexity is 184.45103813066922
At time: 581.7011976242065 and batch: 200, loss is 5.255462656021118 and perplexity is 191.61011492378645
At time: 582.9044680595398 and batch: 250, loss is 5.2833013916015625 and perplexity is 197.01924052365112
At time: 584.1401615142822 and batch: 300, loss is 5.265917854309082 and perplexity is 193.62394582550775
At time: 585.3451929092407 and batch: 350, loss is 5.290218477249145 and perplexity is 198.38676366939023
At time: 586.5485942363739 and batch: 400, loss is 5.253051500320435 and perplexity is 191.14866963465678
At time: 587.750960111618 and batch: 450, loss is 5.22819522857666 and perplexity is 186.4559892341112
At time: 588.954030752182 and batch: 500, loss is 5.227313299179077 and perplexity is 186.2916207072219
At time: 590.1589422225952 and batch: 550, loss is 5.239702968597412 and perplexity is 188.6140697826583
At time: 591.3644933700562 and batch: 600, loss is 5.26065809249878 and perplexity is 192.60820361246124
At time: 592.5704703330994 and batch: 650, loss is 5.273652544021607 and perplexity is 195.1273737463588
At time: 593.7755436897278 and batch: 700, loss is 5.289012393951416 and perplexity is 198.14763693960228
At time: 594.9783194065094 and batch: 750, loss is 5.2562737464904785 and perplexity is 191.76559110593544
At time: 596.1805567741394 and batch: 800, loss is 5.26806713104248 and perplexity is 194.04054480017155
At time: 597.3827788829803 and batch: 850, loss is 5.295177841186524 and perplexity is 199.37307955895253
At time: 598.5862386226654 and batch: 900, loss is 5.27450457572937 and perplexity is 195.2936993031053
At time: 599.7764058113098 and batch: 950, loss is 5.251392593383789 and perplexity is 190.83183465320357
At time: 600.9718482494354 and batch: 1000, loss is 5.241978235244751 and perplexity is 189.04370566760713
At time: 602.1706104278564 and batch: 1050, loss is 5.240272541046142 and perplexity is 188.72152976046556
At time: 603.3708341121674 and batch: 1100, loss is 5.209674549102783 and perplexity is 183.03447976365663
At time: 604.5632190704346 and batch: 1150, loss is 5.246864967346191 and perplexity is 189.9697724900189
At time: 605.7527415752411 and batch: 1200, loss is 5.262026700973511 and perplexity is 192.87198930071776
At time: 606.9440906047821 and batch: 1250, loss is 5.2813114738464355 and perplexity is 196.62757825592286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148401859032846 and perplexity of 172.1561405706113
Finished 19 epochs...
Completing Train Step...
At time: 609.9378342628479 and batch: 50, loss is 5.268065404891968 and perplexity is 194.0402098572748
At time: 611.1423871517181 and batch: 100, loss is 5.293993062973023 and perplexity is 199.13700655262443
At time: 612.3476197719574 and batch: 150, loss is 5.214747657775879 and perplexity is 183.96539288560666
At time: 613.5522184371948 and batch: 200, loss is 5.252562084197998 and perplexity is 191.05514128296372
At time: 614.7851409912109 and batch: 250, loss is 5.280498743057251 and perplexity is 196.46783789082076
At time: 615.9860100746155 and batch: 300, loss is 5.26295919418335 and perplexity is 193.05192500248035
At time: 617.1879806518555 and batch: 350, loss is 5.28714659690857 and perplexity is 197.77827834579028
At time: 618.3892025947571 and batch: 400, loss is 5.2500583267211915 and perplexity is 190.57738388839903
At time: 619.5911557674408 and batch: 450, loss is 5.225287418365479 and perplexity is 185.91459811746034
At time: 620.7982747554779 and batch: 500, loss is 5.224319915771485 and perplexity is 185.73481224719245
At time: 622.0021061897278 and batch: 550, loss is 5.236786184310913 and perplexity is 188.06472477761145
At time: 623.2030363082886 and batch: 600, loss is 5.258029994964599 and perplexity is 192.10267504764118
At time: 624.4107182025909 and batch: 650, loss is 5.271299018859863 and perplexity is 194.66867655192618
At time: 625.6204860210419 and batch: 700, loss is 5.286508769989013 and perplexity is 197.6521702575999
At time: 626.8258686065674 and batch: 750, loss is 5.2541748046875 and perplexity is 191.36350841209392
At time: 628.0284788608551 and batch: 800, loss is 5.265925922393799 and perplexity is 193.62550800620775
At time: 629.230973482132 and batch: 850, loss is 5.293165216445923 and perplexity is 198.9722198917763
At time: 630.4356915950775 and batch: 900, loss is 5.272326097488404 and perplexity is 194.86871930149013
At time: 631.6418263912201 and batch: 950, loss is 5.249377126693726 and perplexity is 190.44760677636444
At time: 632.8434798717499 and batch: 1000, loss is 5.240037593841553 and perplexity is 188.67719537292666
At time: 634.0440075397491 and batch: 1050, loss is 5.238814773559571 and perplexity is 188.44661807770538
At time: 635.2508656978607 and batch: 1100, loss is 5.208162832260132 and perplexity is 182.75799249569621
At time: 636.4609286785126 and batch: 1150, loss is 5.245403327941895 and perplexity is 189.69230801082796
At time: 637.6701679229736 and batch: 1200, loss is 5.260388202667237 and perplexity is 192.5562276310444
At time: 638.8754041194916 and batch: 1250, loss is 5.279370622634888 and perplexity is 196.2463234815157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148198260007984 and perplexity of 172.12109331618151
Finished 20 epochs...
Completing Train Step...
At time: 641.8744933605194 and batch: 50, loss is 5.265059728622436 and perplexity is 193.45786341402848
At time: 643.1117942333221 and batch: 100, loss is 5.291232166290283 and perplexity is 198.58796811977257
At time: 644.3199334144592 and batch: 150, loss is 5.212092523574829 and perplexity is 183.47758795949255
At time: 645.5564162731171 and batch: 200, loss is 5.249744262695312 and perplexity is 190.51753978590529
At time: 646.7620360851288 and batch: 250, loss is 5.27776762008667 and perplexity is 195.9319921291851
At time: 647.9671666622162 and batch: 300, loss is 5.260116777420044 and perplexity is 192.5039701016884
At time: 649.1721775531769 and batch: 350, loss is 5.284331464767456 and perplexity is 197.22228931608177
At time: 650.3784594535828 and batch: 400, loss is 5.247281122207641 and perplexity is 190.04884578659642
At time: 651.5837016105652 and batch: 450, loss is 5.222425012588501 and perplexity is 185.38319600485391
At time: 652.7901477813721 and batch: 500, loss is 5.2216330337524415 and perplexity is 185.23643456070693
At time: 653.9943914413452 and batch: 550, loss is 5.234217367172241 and perplexity is 187.5822408612187
At time: 655.2028217315674 and batch: 600, loss is 5.255610113143921 and perplexity is 191.6383712832826
At time: 656.4104404449463 and batch: 650, loss is 5.268962812423706 and perplexity is 194.21442116063704
At time: 657.6156752109528 and batch: 700, loss is 5.284367504119873 and perplexity is 197.22939720775207
At time: 658.8311953544617 and batch: 750, loss is 5.251832361221314 and perplexity is 190.91577481219812
At time: 660.0404326915741 and batch: 800, loss is 5.263735580444336 and perplexity is 193.2018660632672
At time: 661.2495741844177 and batch: 850, loss is 5.29126748085022 and perplexity is 198.59498129030825
At time: 662.4586730003357 and batch: 900, loss is 5.270353240966797 and perplexity is 194.48465025888834
At time: 663.6670482158661 and batch: 950, loss is 5.247583694458008 and perplexity is 190.10635799390587
At time: 664.8763847351074 and batch: 1000, loss is 5.23832745552063 and perplexity is 188.3548070137495
At time: 666.0822825431824 and batch: 1050, loss is 5.237079019546509 and perplexity is 188.119804819894
At time: 667.2884473800659 and batch: 1100, loss is 5.206499176025391 and perplexity is 182.45419879624188
At time: 668.4998319149017 and batch: 1150, loss is 5.243845767974854 and perplexity is 189.39708084250893
At time: 669.7123744487762 and batch: 1200, loss is 5.25877046585083 and perplexity is 192.24497416335535
At time: 670.9219257831573 and batch: 1250, loss is 5.277304172515869 and perplexity is 195.8412089616348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148137224851734 and perplexity of 172.11058819895104
Finished 21 epochs...
Completing Train Step...
At time: 673.9938998222351 and batch: 50, loss is 5.261853246688843 and perplexity is 192.8385377290242
At time: 675.2337076663971 and batch: 100, loss is 5.288186416625977 and perplexity is 197.98403905777232
At time: 676.4481511116028 and batch: 150, loss is 5.209618034362793 and perplexity is 183.02413590991642
At time: 677.6542794704437 and batch: 200, loss is 5.247054882049561 and perplexity is 190.0058539691038
At time: 678.8606703281403 and batch: 250, loss is 5.275129041671753 and perplexity is 195.4156916531516
At time: 680.0675659179688 and batch: 300, loss is 5.257378044128418 and perplexity is 191.97747436479085
At time: 681.2733385562897 and batch: 350, loss is 5.281370468139649 and perplexity is 196.63917850309912
At time: 682.4840002059937 and batch: 400, loss is 5.244683027267456 and perplexity is 189.55572171093937
At time: 683.6945173740387 and batch: 450, loss is 5.219750719070435 and perplexity is 184.88808925088833
At time: 684.9043200016022 and batch: 500, loss is 5.21906343460083 and perplexity is 184.7610621953928
At time: 686.1113467216492 and batch: 550, loss is 5.231756811141968 and perplexity is 187.12125162480015
At time: 687.3173315525055 and batch: 600, loss is 5.2531452655792235 and perplexity is 191.16659357944067
At time: 688.5244374275208 and batch: 650, loss is 5.266771574020385 and perplexity is 193.78931698490447
At time: 689.7310733795166 and batch: 700, loss is 5.281829500198365 and perplexity is 196.7294629101649
At time: 690.9417679309845 and batch: 750, loss is 5.2496232986450195 and perplexity is 190.4944954064398
At time: 692.1579501628876 and batch: 800, loss is 5.261507244110107 and perplexity is 192.7718266394604
At time: 693.368577003479 and batch: 850, loss is 5.289097242355346 and perplexity is 198.16445016361672
At time: 694.5779855251312 and batch: 900, loss is 5.268213701248169 and perplexity is 194.06898744710625
At time: 695.7894926071167 and batch: 950, loss is 5.245632696151733 and perplexity is 189.73582238615296
At time: 697.0032370090485 and batch: 1000, loss is 5.23659948348999 and perplexity is 188.02961621660947
At time: 698.2152764797211 and batch: 1050, loss is 5.235432300567627 and perplexity is 187.8102792877223
At time: 699.4287467002869 and batch: 1100, loss is 5.204864282608032 and perplexity is 182.15614933360013
At time: 700.6332042217255 and batch: 1150, loss is 5.242287311553955 and perplexity is 189.1021436288626
At time: 701.8362445831299 and batch: 1200, loss is 5.257346496582032 and perplexity is 191.97141804204466
At time: 703.0365595817566 and batch: 1250, loss is 5.275140619277954 and perplexity is 195.41795411217194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.148009808394161 and perplexity of 172.08865987453623
Finished 22 epochs...
Completing Train Step...
At time: 706.1018528938293 and batch: 50, loss is 5.258880491256714 and perplexity is 192.26612715832889
At time: 707.3035700321198 and batch: 100, loss is 5.285304098129273 and perplexity is 197.41420761230341
At time: 708.5029168128967 and batch: 150, loss is 5.207163381576538 and perplexity is 182.5754261433941
At time: 709.700142621994 and batch: 200, loss is 5.2445527458190915 and perplexity is 189.5310277255881
At time: 710.9024343490601 and batch: 250, loss is 5.272783679962158 and perplexity is 194.9579082162114
At time: 712.1152799129486 and batch: 300, loss is 5.25473274230957 and perplexity is 191.47030710366192
At time: 713.3283710479736 and batch: 350, loss is 5.278708534240723 and perplexity is 196.11643407222113
At time: 714.5409071445465 and batch: 400, loss is 5.242048645019532 and perplexity is 189.05701666095337
At time: 715.7520146369934 and batch: 450, loss is 5.217089023590088 and perplexity is 184.396627809859
At time: 716.9741315841675 and batch: 500, loss is 5.21643364906311 and perplexity is 184.27581854911205
At time: 718.1809041500092 and batch: 550, loss is 5.2293240928649904 and perplexity is 186.66659159007742
At time: 719.3891890048981 and batch: 600, loss is 5.250860805511475 and perplexity is 190.73037957656408
At time: 720.5983414649963 and batch: 650, loss is 5.26451642036438 and perplexity is 193.35278470689923
At time: 721.8057544231415 and batch: 700, loss is 5.2796063804626465 and perplexity is 196.29259554273136
At time: 723.0129992961884 and batch: 750, loss is 5.247120656967163 and perplexity is 190.0183519995165
At time: 724.2194993495941 and batch: 800, loss is 5.259417896270752 and perplexity is 192.36947970769515
At time: 725.4322950839996 and batch: 850, loss is 5.287060384750366 and perplexity is 197.76122818854432
At time: 726.6428616046906 and batch: 900, loss is 5.266308898925781 and perplexity is 193.699676233206
At time: 727.8528468608856 and batch: 950, loss is 5.243557872772217 and perplexity is 189.34256217974828
At time: 729.0605676174164 and batch: 1000, loss is 5.234601202011109 and perplexity is 187.6542552803514
At time: 730.2665746212006 and batch: 1050, loss is 5.233573169708252 and perplexity is 187.46143977142188
At time: 731.4723103046417 and batch: 1100, loss is 5.20319748878479 and perplexity is 181.85278548175637
At time: 732.6776685714722 and batch: 1150, loss is 5.240512943267822 and perplexity is 188.76690428934901
At time: 733.8880927562714 and batch: 1200, loss is 5.25553524017334 and perplexity is 191.6240232862926
At time: 735.0985949039459 and batch: 1250, loss is 5.273017301559448 and perplexity is 195.00345991485625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147793735030794 and perplexity of 172.05148011592226
Finished 23 epochs...
Completing Train Step...
At time: 738.1182615756989 and batch: 50, loss is 5.256138677597046 and perplexity is 191.7396912889155
At time: 739.3468225002289 and batch: 100, loss is 5.282529945373535 and perplexity is 196.86730938458732
At time: 740.5579164028168 and batch: 150, loss is 5.2046788311004635 and perplexity is 182.1223713332807
At time: 741.7707917690277 and batch: 200, loss is 5.242151336669922 and perplexity is 189.07643223490376
At time: 742.9791045188904 and batch: 250, loss is 5.270284748077392 and perplexity is 194.47132989942745
At time: 744.185558795929 and batch: 300, loss is 5.252124280929565 and perplexity is 190.9715150249194
At time: 745.398163318634 and batch: 350, loss is 5.276282424926758 and perplexity is 195.64121086971306
At time: 746.6061227321625 and batch: 400, loss is 5.239404745101929 and perplexity is 188.55782902204533
At time: 747.8236448764801 and batch: 450, loss is 5.214547481536865 and perplexity is 183.92857107069926
At time: 749.0307641029358 and batch: 500, loss is 5.21382402420044 and perplexity is 183.79555471820956
At time: 750.2384548187256 and batch: 550, loss is 5.2269010162353515 and perplexity is 186.2148316799362
At time: 751.4476289749146 and batch: 600, loss is 5.2484972953796385 and perplexity is 190.28011869966136
At time: 752.6560645103455 and batch: 650, loss is 5.262532014846801 and perplexity is 192.9694748209991
At time: 753.8710932731628 and batch: 700, loss is 5.277365245819092 and perplexity is 195.85316999641947
At time: 755.0856680870056 and batch: 750, loss is 5.245026016235352 and perplexity is 189.6207483833659
At time: 756.2948658466339 and batch: 800, loss is 5.2571048927307125 and perplexity is 191.92504261056953
At time: 757.5040345191956 and batch: 850, loss is 5.285119285583496 and perplexity is 197.37772636122241
At time: 758.7067053318024 and batch: 900, loss is 5.264361400604248 and perplexity is 193.32281352771554
At time: 759.9132173061371 and batch: 950, loss is 5.241760683059693 and perplexity is 189.00258326966352
At time: 761.1200413703918 and batch: 1000, loss is 5.232727375030517 and perplexity is 187.30295291650006
At time: 762.3228993415833 and batch: 1050, loss is 5.231867780685425 and perplexity is 187.1420175368347
At time: 763.5257935523987 and batch: 1100, loss is 5.20138596534729 and perplexity is 181.5236531041968
At time: 764.7290377616882 and batch: 1150, loss is 5.238792686462403 and perplexity is 188.44245588490642
At time: 765.9390470981598 and batch: 1200, loss is 5.253878898620606 and perplexity is 191.30689116607908
At time: 767.1737265586853 and batch: 1250, loss is 5.270912675857544 and perplexity is 194.59348219731135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147599937271898 and perplexity of 172.01814015537025
Finished 24 epochs...
Completing Train Step...
At time: 770.2272691726685 and batch: 50, loss is 5.253225040435791 and perplexity is 191.18184447533494
At time: 771.4399452209473 and batch: 100, loss is 5.279886121749878 and perplexity is 196.34751436725568
At time: 772.6500725746155 and batch: 150, loss is 5.202173919677734 and perplexity is 181.66674181902115
At time: 773.862053155899 and batch: 200, loss is 5.239876203536987 and perplexity is 188.64674715999007
At time: 775.0670864582062 and batch: 250, loss is 5.267689170837403 and perplexity is 193.96721905404544
At time: 776.2756595611572 and batch: 300, loss is 5.249666404724121 and perplexity is 190.50270705421192
At time: 777.4830565452576 and batch: 350, loss is 5.273678359985351 and perplexity is 195.13241121258824
At time: 778.6881589889526 and batch: 400, loss is 5.23726258277893 and perplexity is 188.15433986893487
At time: 779.8920543193817 and batch: 450, loss is 5.211936502456665 and perplexity is 183.4489638141048
At time: 781.0960392951965 and batch: 500, loss is 5.211519641876221 and perplexity is 183.37250710956755
At time: 782.3042023181915 and batch: 550, loss is 5.224476547241211 and perplexity is 185.76390644228778
At time: 783.5152773857117 and batch: 600, loss is 5.246276025772095 and perplexity is 189.85792433242577
At time: 784.7224724292755 and batch: 650, loss is 5.2601206016540525 and perplexity is 192.5047062833253
At time: 785.9252412319183 and batch: 700, loss is 5.275072393417358 and perplexity is 195.40462200887907
At time: 787.1287488937378 and batch: 750, loss is 5.242782945632935 and perplexity is 189.19589232625864
At time: 788.3293738365173 and batch: 800, loss is 5.254959592819214 and perplexity is 191.51374716742404
At time: 789.525276184082 and batch: 850, loss is 5.283200159072876 and perplexity is 196.9992967772279
At time: 790.7211520671844 and batch: 900, loss is 5.262374010086059 and perplexity is 192.9389871339628
At time: 791.9190826416016 and batch: 950, loss is 5.239790830612183 and perplexity is 188.63064252288999
At time: 793.1179902553558 and batch: 1000, loss is 5.23103138923645 and perplexity is 186.9855589930379
At time: 794.3080263137817 and batch: 1050, loss is 5.230202722549438 and perplexity is 186.83067447203268
At time: 795.499960899353 and batch: 1100, loss is 5.199775238037109 and perplexity is 181.23150334865676
At time: 796.695823431015 and batch: 1150, loss is 5.237207069396972 and perplexity is 188.14389507511427
At time: 797.9145784378052 and batch: 1200, loss is 5.252211227416992 and perplexity is 190.9881200492132
At time: 799.1124112606049 and batch: 1250, loss is 5.268979139328003 and perplexity is 194.21759210679025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147492568858349 and perplexity of 171.9996718320351
Finished 25 epochs...
Completing Train Step...
At time: 802.1221356391907 and batch: 50, loss is 5.250609397888184 and perplexity is 190.68243453227268
At time: 803.3571929931641 and batch: 100, loss is 5.277460279464722 and perplexity is 195.87178352161402
At time: 804.5601668357849 and batch: 150, loss is 5.199971847534179 and perplexity is 181.26713868639158
At time: 805.7664675712585 and batch: 200, loss is 5.237593908309936 and perplexity is 188.21669053411622
At time: 806.9729590415955 and batch: 250, loss is 5.265343828201294 and perplexity is 193.51283251953126
At time: 808.1792268753052 and batch: 300, loss is 5.247461814880371 and perplexity is 190.08318932320998
At time: 809.3829770088196 and batch: 350, loss is 5.271362771987915 and perplexity is 194.68108768461025
At time: 810.5870656967163 and batch: 400, loss is 5.234718580245971 and perplexity is 187.67628309836857
At time: 811.7898018360138 and batch: 450, loss is 5.209532251358032 and perplexity is 183.0084362229871
At time: 812.9951465129852 and batch: 500, loss is 5.209065389633179 and perplexity is 182.9230165299334
At time: 814.2012028694153 and batch: 550, loss is 5.222166481018067 and perplexity is 185.3352747908989
At time: 815.4074580669403 and batch: 600, loss is 5.244134359359741 and perplexity is 189.45174709608835
At time: 816.6113827228546 and batch: 650, loss is 5.258122577667236 and perplexity is 192.12046125581577
At time: 817.8163344860077 and batch: 700, loss is 5.27290831565857 and perplexity is 194.98220844517937
At time: 819.0175874233246 and batch: 750, loss is 5.240736808776855 and perplexity is 188.8091674189184
At time: 820.2225542068481 and batch: 800, loss is 5.2530029296875 and perplexity is 191.1393856482543
At time: 821.4245100021362 and batch: 850, loss is 5.281317367553711 and perplexity is 196.6287371247263
At time: 822.6316242218018 and batch: 900, loss is 5.260235357284546 and perplexity is 192.5267985498495
At time: 823.8357455730438 and batch: 950, loss is 5.237951078414917 and perplexity is 188.28392791611006
At time: 825.0394887924194 and batch: 1000, loss is 5.22920482635498 and perplexity is 186.64432984472967
At time: 826.2426559925079 and batch: 1050, loss is 5.228585805892944 and perplexity is 186.52882893783337
At time: 827.4984574317932 and batch: 1100, loss is 5.198062591552734 and perplexity is 180.92138349023386
At time: 828.705198764801 and batch: 1150, loss is 5.2354058742523195 and perplexity is 187.80531621964192
At time: 829.9157490730286 and batch: 1200, loss is 5.250466814041138 and perplexity is 190.6552482354025
At time: 831.1189749240875 and batch: 1250, loss is 5.2668843460083 and perplexity is 193.81117222372376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.147243973112454 and perplexity of 171.95691875965855
Finished 26 epochs...
Completing Train Step...
At time: 834.1287376880646 and batch: 50, loss is 5.247589130401611 and perplexity is 190.10739140415527
At time: 835.3602669239044 and batch: 100, loss is 5.274914112091064 and perplexity is 195.37369555374832
At time: 836.5625321865082 and batch: 150, loss is 5.197707452774048 and perplexity is 180.8571426989362
At time: 837.7652013301849 and batch: 200, loss is 5.23544750213623 and perplexity is 187.81313432026766
At time: 838.9704434871674 and batch: 250, loss is 5.26297194480896 and perplexity is 193.05438655099255
At time: 840.1764674186707 and batch: 300, loss is 5.245012722015381 and perplexity is 189.61822754018218
At time: 841.3877711296082 and batch: 350, loss is 5.268851089477539 and perplexity is 194.1927241653655
At time: 842.5935959815979 and batch: 400, loss is 5.232455158233643 and perplexity is 187.25197284574259
At time: 843.8035848140717 and batch: 450, loss is 5.207207813262939 and perplexity is 182.58353845769366
At time: 845.0083811283112 and batch: 500, loss is 5.206823625564575 and perplexity is 182.51340558125128
At time: 846.2134294509888 and batch: 550, loss is 5.219782476425171 and perplexity is 184.8939609007587
At time: 847.4152717590332 and batch: 600, loss is 5.241959457397461 and perplexity is 189.0401558670998
At time: 848.6172580718994 and batch: 650, loss is 5.255953893661499 and perplexity is 191.70426414744279
At time: 849.8192629814148 and batch: 700, loss is 5.270594034194946 and perplexity is 194.5314864843464
At time: 851.0204076766968 and batch: 750, loss is 5.2385413646698 and perplexity is 188.39510213985426
At time: 852.2255382537842 and batch: 800, loss is 5.250934629440308 and perplexity is 190.7444605622827
At time: 853.4296610355377 and batch: 850, loss is 5.279437398910522 and perplexity is 196.25942851765262
At time: 854.6318306922913 and batch: 900, loss is 5.258247671127319 and perplexity is 192.14449577231605
At time: 855.8348338603973 and batch: 950, loss is 5.235977659225464 and perplexity is 187.91273118353752
At time: 857.0368564128876 and batch: 1000, loss is 5.227437143325806 and perplexity is 186.3146932627021
At time: 858.2695913314819 and batch: 1050, loss is 5.226953401565551 and perplexity is 186.22458686089382
At time: 859.4726462364197 and batch: 1100, loss is 5.196533050537109 and perplexity is 180.6448683381241
At time: 860.6768343448639 and batch: 1150, loss is 5.233757085800171 and perplexity is 187.4959201174583
At time: 861.8825933933258 and batch: 1200, loss is 5.249040994644165 and perplexity is 190.3836019895975
At time: 863.0850870609283 and batch: 1250, loss is 5.265006504058838 and perplexity is 193.4475669776877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1472493192575275 and perplexity of 171.95783806875
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 866.1127529144287 and batch: 50, loss is 5.248635711669922 and perplexity is 190.30645839068563
At time: 867.3180928230286 and batch: 100, loss is 5.279337549209595 and perplexity is 196.23983305072807
At time: 868.5231671333313 and batch: 150, loss is 5.201085453033447 and perplexity is 181.46911120685155
At time: 869.7214498519897 and batch: 200, loss is 5.239420299530029 and perplexity is 188.56076195404955
At time: 870.9188868999481 and batch: 250, loss is 5.266477899551392 and perplexity is 193.73241436597445
At time: 872.1171844005585 and batch: 300, loss is 5.247280807495117 and perplexity is 190.0487859758538
At time: 873.3167653083801 and batch: 350, loss is 5.2683525562286375 and perplexity is 194.09593676354785
At time: 874.5187029838562 and batch: 400, loss is 5.228812274932861 and perplexity is 186.57107672636718
At time: 875.7113018035889 and batch: 450, loss is 5.203151969909668 and perplexity is 181.84450793591725
At time: 876.9036893844604 and batch: 500, loss is 5.199882202148437 and perplexity is 181.250889652158
At time: 878.0970222949982 and batch: 550, loss is 5.213059005737304 and perplexity is 183.65500149516822
At time: 879.2887208461761 and batch: 600, loss is 5.233356657028199 and perplexity is 187.42085638625767
At time: 880.4910185337067 and batch: 650, loss is 5.246992473602295 and perplexity is 189.99399636879718
At time: 881.700352191925 and batch: 700, loss is 5.262065658569336 and perplexity is 192.87950327608513
At time: 882.90824842453 and batch: 750, loss is 5.225307302474976 and perplexity is 185.91829490043983
At time: 884.1136212348938 and batch: 800, loss is 5.235218105316162 and perplexity is 187.77005552574565
At time: 885.3203814029694 and batch: 850, loss is 5.256535472869873 and perplexity is 191.81578778839958
At time: 886.5278632640839 and batch: 900, loss is 5.234763736724854 and perplexity is 187.68475808983197
At time: 887.7384142875671 and batch: 950, loss is 5.212866563796997 and perplexity is 183.61966197084564
At time: 889.0059759616852 and batch: 1000, loss is 5.202792987823487 and perplexity is 181.7792407306774
At time: 890.2117199897766 and batch: 1050, loss is 5.198258037567139 and perplexity is 180.9567473093038
At time: 891.4185667037964 and batch: 1100, loss is 5.1636318111419675 and perplexity is 174.79813802642602
At time: 892.6247012615204 and batch: 1150, loss is 5.196707859039306 and perplexity is 180.67644935722277
At time: 893.8344674110413 and batch: 1200, loss is 5.219884843826294 and perplexity is 184.91288898381225
At time: 895.04461145401 and batch: 1250, loss is 5.245957260131836 and perplexity is 189.7974137944703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142069350193887 and perplexity of 171.06940480226282
Finished 28 epochs...
Completing Train Step...
At time: 898.0513231754303 and batch: 50, loss is 5.243479995727539 and perplexity is 189.32781731472468
At time: 899.2851567268372 and batch: 100, loss is 5.271823844909668 and perplexity is 194.77087055915456
At time: 900.491012096405 and batch: 150, loss is 5.194665851593018 and perplexity is 180.30788313781906
At time: 901.7027928829193 and batch: 200, loss is 5.232205696105957 and perplexity is 187.20526639617052
At time: 902.9081485271454 and batch: 250, loss is 5.260069942474365 and perplexity is 192.4949543998323
At time: 904.1147539615631 and batch: 300, loss is 5.241359996795654 and perplexity is 188.9268677007868
At time: 905.3192999362946 and batch: 350, loss is 5.262952184677124 and perplexity is 193.0505718085527
At time: 906.5261960029602 and batch: 400, loss is 5.223882026672364 and perplexity is 185.653498802017
At time: 907.7363908290863 and batch: 450, loss is 5.198794803619385 and perplexity is 181.05390482127862
At time: 908.9398827552795 and batch: 500, loss is 5.196282577514649 and perplexity is 180.59962733802098
At time: 910.1468884944916 and batch: 550, loss is 5.20909330368042 and perplexity is 182.92812272292517
At time: 911.3496692180634 and batch: 600, loss is 5.229734115600586 and perplexity is 186.74314482981728
At time: 912.5504486560822 and batch: 650, loss is 5.244145488739013 and perplexity is 189.45385558816866
At time: 913.7521557807922 and batch: 700, loss is 5.259195699691772 and perplexity is 192.32674061581983
At time: 914.9565711021423 and batch: 750, loss is 5.222868328094482 and perplexity is 185.46539746943688
At time: 916.1656346321106 and batch: 800, loss is 5.233607835769654 and perplexity is 187.46793843384435
At time: 917.3721091747284 and batch: 850, loss is 5.255822048187256 and perplexity is 191.67899047396816
At time: 918.5815408229828 and batch: 900, loss is 5.234308280944824 and perplexity is 187.59929544564153
At time: 919.812756061554 and batch: 950, loss is 5.212743453979492 and perplexity is 183.59705797918534
At time: 921.0223398208618 and batch: 1000, loss is 5.203307304382324 and perplexity is 181.87275685062122
At time: 922.224606513977 and batch: 1050, loss is 5.199403162002564 and perplexity is 181.16408399289716
At time: 923.4280269145966 and batch: 1100, loss is 5.165613956451416 and perplexity is 175.14495694501045
At time: 924.6327471733093 and batch: 1150, loss is 5.199767913818359 and perplexity is 181.23017597434287
At time: 925.840029001236 and batch: 1200, loss is 5.223425045013427 and perplexity is 185.5686779404123
At time: 927.0508711338043 and batch: 1250, loss is 5.247352972030639 and perplexity is 190.0625012530927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141663488680429 and perplexity of 170.9999884023989
Finished 29 epochs...
Completing Train Step...
At time: 930.0722591876984 and batch: 50, loss is 5.241291809082031 and perplexity is 188.91398564884014
At time: 931.2778754234314 and batch: 100, loss is 5.268882684707641 and perplexity is 194.19885982609802
At time: 932.4865524768829 and batch: 150, loss is 5.191789436340332 and perplexity is 179.78998799041105
At time: 933.694477558136 and batch: 200, loss is 5.228969116210937 and perplexity is 186.60034106736134
At time: 934.9022839069366 and batch: 250, loss is 5.257329320907592 and perplexity is 191.96812083178273
At time: 936.116667509079 and batch: 300, loss is 5.2386172485351565 and perplexity is 188.40939883085613
At time: 937.3230738639832 and batch: 350, loss is 5.2601997184753415 and perplexity is 192.51993724627437
At time: 938.532363653183 and batch: 400, loss is 5.221355838775635 and perplexity is 185.18509506737865
At time: 939.7407565116882 and batch: 450, loss is 5.196568355560303 and perplexity is 180.65124612197386
At time: 940.9470250606537 and batch: 500, loss is 5.194434185028076 and perplexity is 180.2661166680346
At time: 942.153550863266 and batch: 550, loss is 5.2071820163726805 and perplexity is 182.57882843094131
At time: 943.3592376708984 and batch: 600, loss is 5.228092622756958 and perplexity is 186.43685874596397
At time: 944.5675086975098 and batch: 650, loss is 5.242688226699829 and perplexity is 189.17797274186486
At time: 945.7801222801208 and batch: 700, loss is 5.2577945995330815 and perplexity is 192.05746027743658
At time: 946.9889733791351 and batch: 750, loss is 5.222020235061645 and perplexity is 185.30817223824616
At time: 948.1943824291229 and batch: 800, loss is 5.232999696731567 and perplexity is 187.3539665209904
At time: 949.4545130729675 and batch: 850, loss is 5.255810918807984 and perplexity is 191.67685721765557
At time: 950.6524374485016 and batch: 900, loss is 5.234505138397217 and perplexity is 187.6362294002564
At time: 951.8515856266022 and batch: 950, loss is 5.213252773284912 and perplexity is 183.69059132237928
At time: 953.0529050827026 and batch: 1000, loss is 5.2041703224182125 and perplexity is 182.02978406893462
At time: 954.2536990642548 and batch: 1050, loss is 5.200588254928589 and perplexity is 181.3789075350748
At time: 955.4474995136261 and batch: 1100, loss is 5.167165451049804 and perplexity is 175.41690430752635
At time: 956.6407017707825 and batch: 1150, loss is 5.201788463592529 and perplexity is 181.59673076186854
At time: 957.834103345871 and batch: 1200, loss is 5.225596513748169 and perplexity is 185.9720723433644
At time: 959.0259311199188 and batch: 1250, loss is 5.2480944061279295 and perplexity is 190.20347232606017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141521370323905 and perplexity of 170.97568789189674
Finished 30 epochs...
Completing Train Step...
At time: 962.050265789032 and batch: 50, loss is 5.239686889648437 and perplexity is 188.61103709103563
At time: 963.2890584468842 and batch: 100, loss is 5.266895656585693 and perplexity is 193.8133643523839
At time: 964.4924154281616 and batch: 150, loss is 5.189784412384033 and perplexity is 179.4298659047366
At time: 965.6946318149567 and batch: 200, loss is 5.226741638183594 and perplexity is 186.18515548778427
At time: 966.8944442272186 and batch: 250, loss is 5.255476922988891 and perplexity is 191.612848638622
At time: 968.0978486537933 and batch: 300, loss is 5.2367190170288085 and perplexity is 188.05209340540068
At time: 969.3015975952148 and batch: 350, loss is 5.258307332992554 and perplexity is 192.15595981330807
At time: 970.5196530818939 and batch: 400, loss is 5.219651575088501 and perplexity is 184.86975961815932
At time: 971.7319884300232 and batch: 450, loss is 5.195000238418579 and perplexity is 180.36818580013124
At time: 972.9393668174744 and batch: 500, loss is 5.193162994384766 and perplexity is 180.0371096538454
At time: 974.1434843540192 and batch: 550, loss is 5.205906238555908 and perplexity is 182.3460469321261
At time: 975.3508789539337 and batch: 600, loss is 5.226931791305542 and perplexity is 186.22056254263512
At time: 976.5586755275726 and batch: 650, loss is 5.24174651145935 and perplexity is 188.9999048195688
At time: 977.7664539813995 and batch: 700, loss is 5.2569260597229 and perplexity is 191.89072314674294
At time: 978.9735407829285 and batch: 750, loss is 5.221543464660645 and perplexity is 185.21984384451443
At time: 980.2119472026825 and batch: 800, loss is 5.232690715789795 and perplexity is 187.2960866583178
At time: 981.4228644371033 and batch: 850, loss is 5.255893888473511 and perplexity is 191.69276124215492
At time: 982.6317670345306 and batch: 900, loss is 5.234743537902832 and perplexity is 187.68096711709393
At time: 983.8408975601196 and batch: 950, loss is 5.213712711334228 and perplexity is 183.77509704683908
At time: 985.0572063922882 and batch: 1000, loss is 5.204906091690064 and perplexity is 182.16376527419686
At time: 986.2705025672913 and batch: 1050, loss is 5.201523141860962 and perplexity is 181.54855559405792
At time: 987.4798204898834 and batch: 1100, loss is 5.168361558914184 and perplexity is 175.62684737856293
At time: 988.6891059875488 and batch: 1150, loss is 5.203208312988282 and perplexity is 181.85475390396527
At time: 989.9020006656647 and batch: 1200, loss is 5.2270924091339115 and perplexity is 186.25047528718412
At time: 991.1109049320221 and batch: 1250, loss is 5.2485119819641115 and perplexity is 190.2829132852196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141424694200502 and perplexity of 170.9591594241622
Finished 31 epochs...
Completing Train Step...
At time: 994.1572158336639 and batch: 50, loss is 5.238401565551758 and perplexity is 188.3687665116234
At time: 995.3953483104706 and batch: 100, loss is 5.265342054367065 and perplexity is 193.51248926014966
At time: 996.6046857833862 and batch: 150, loss is 5.188178644180298 and perplexity is 179.1419743366899
At time: 997.8141994476318 and batch: 200, loss is 5.224982242584229 and perplexity is 185.85787014117656
At time: 999.0251970291138 and batch: 250, loss is 5.254039239883423 and perplexity is 191.33756801391084
At time: 1000.2443554401398 and batch: 300, loss is 5.235176696777343 and perplexity is 187.76228040309186
At time: 1001.457855463028 and batch: 350, loss is 5.256832704544068 and perplexity is 191.87280999012324
At time: 1002.6683444976807 and batch: 400, loss is 5.218335103988648 and perplexity is 184.62654405064433
At time: 1003.877204656601 and batch: 450, loss is 5.193766975402832 and perplexity is 180.1458814953811
At time: 1005.0860295295715 and batch: 500, loss is 5.192243595123291 and perplexity is 179.87165973711168
At time: 1006.2951126098633 and batch: 550, loss is 5.205018882751465 and perplexity is 182.18431287740214
At time: 1007.5053374767303 and batch: 600, loss is 5.226037406921387 and perplexity is 186.05408423838367
At time: 1008.7156705856323 and batch: 650, loss is 5.2410523891448975 and perplexity is 188.86876128829525
At time: 1009.9246344566345 and batch: 700, loss is 5.2562761402130125 and perplexity is 191.76605014010153
At time: 1011.1829571723938 and batch: 750, loss is 5.221218948364258 and perplexity is 185.15974673854515
At time: 1012.3925371170044 and batch: 800, loss is 5.232450799942017 and perplexity is 187.25115674881573
At time: 1013.6026201248169 and batch: 850, loss is 5.2559488582611085 and perplexity is 191.7032988421465
At time: 1014.8114702701569 and batch: 900, loss is 5.234964427947998 and perplexity is 187.72242855343777
At time: 1016.0213527679443 and batch: 950, loss is 5.2141218090057375 and perplexity is 183.85029439160292
At time: 1017.2305459976196 and batch: 1000, loss is 5.2054664897918705 and perplexity is 182.26587811172232
At time: 1018.446005821228 and batch: 1050, loss is 5.202210998535156 and perplexity is 181.6734779391224
At time: 1019.6547513008118 and batch: 1100, loss is 5.169234695434571 and perplexity is 175.78026055854812
At time: 1020.8663446903229 and batch: 1150, loss is 5.204256706237793 and perplexity is 182.04550917614748
At time: 1022.0835711956024 and batch: 1200, loss is 5.2282242107391355 and perplexity is 186.46139321019476
At time: 1023.3007245063782 and batch: 1250, loss is 5.248764123916626 and perplexity is 190.33089763968562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.14138771669708 and perplexity of 170.95283789813743
Finished 32 epochs...
Completing Train Step...
At time: 1026.3737890720367 and batch: 50, loss is 5.237313938140869 and perplexity is 188.1640028512801
At time: 1027.5755305290222 and batch: 100, loss is 5.2640635108947755 and perplexity is 193.26523322767466
At time: 1028.7745125293732 and batch: 150, loss is 5.186829853057861 and perplexity is 178.90051210970924
At time: 1029.9693775177002 and batch: 200, loss is 5.223476657867431 and perplexity is 185.57825591666577
At time: 1031.1678879261017 and batch: 250, loss is 5.2528644466400145 and perplexity is 191.11291791634363
At time: 1032.3658320903778 and batch: 300, loss is 5.233961772918701 and perplexity is 187.53430204509237
At time: 1033.5639033317566 and batch: 350, loss is 5.255606517791748 and perplexity is 191.63768227708667
At time: 1034.7560503482819 and batch: 400, loss is 5.217243976593018 and perplexity is 184.42520283490413
At time: 1035.9471426010132 and batch: 450, loss is 5.192746210098266 and perplexity is 179.96208865042863
At time: 1037.1376602649689 and batch: 500, loss is 5.191488399505615 and perplexity is 179.73587272726505
At time: 1038.3330903053284 and batch: 550, loss is 5.204272890090943 and perplexity is 182.04845539777514
At time: 1039.5354135036469 and batch: 600, loss is 5.225335941314698 and perplexity is 185.9236194609331
At time: 1040.7417418956757 and batch: 650, loss is 5.240501317977905 and perplexity is 188.76470983211564
At time: 1041.9949522018433 and batch: 700, loss is 5.255777397155762 and perplexity is 191.67043200040143
At time: 1043.1978495121002 and batch: 750, loss is 5.22100193977356 and perplexity is 185.1195698423749
At time: 1044.4008078575134 and batch: 800, loss is 5.232250728607178 and perplexity is 187.21369690738004
At time: 1045.6029887199402 and batch: 850, loss is 5.2559820079803465 and perplexity is 191.70965385801304
At time: 1046.8087594509125 and batch: 900, loss is 5.235122871398926 and perplexity is 187.7521742992815
At time: 1048.0163328647614 and batch: 950, loss is 5.214424638748169 and perplexity is 183.90597815982457
At time: 1049.2278592586517 and batch: 1000, loss is 5.205901412963867 and perplexity is 182.34516700661646
At time: 1050.4338715076447 and batch: 1050, loss is 5.202724123001099 and perplexity is 181.76672296657247
At time: 1051.635256767273 and batch: 1100, loss is 5.169857654571533 and perplexity is 175.8897985932781
At time: 1052.840827703476 and batch: 1150, loss is 5.205067195892334 and perplexity is 182.1931149864014
At time: 1054.0497734546661 and batch: 1200, loss is 5.229088277816772 and perplexity is 186.6225779885196
At time: 1055.2522115707397 and batch: 1250, loss is 5.248931818008423 and perplexity is 190.36281768303272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141378806455292 and perplexity of 170.95131467380358
Finished 33 epochs...
Completing Train Step...
At time: 1058.2662484645844 and batch: 50, loss is 5.236401071548462 and perplexity is 187.9923125962554
At time: 1059.4989371299744 and batch: 100, loss is 5.262969331741333 and perplexity is 193.05388208748386
At time: 1060.703607559204 and batch: 150, loss is 5.185674133300782 and perplexity is 178.6938726849458
At time: 1061.9070796966553 and batch: 200, loss is 5.222222862243652 and perplexity is 185.34572451541715
At time: 1063.1096029281616 and batch: 250, loss is 5.2518832302093506 and perplexity is 190.92548675147927
At time: 1064.3161244392395 and batch: 300, loss is 5.232958812713623 and perplexity is 187.3463068946404
At time: 1065.522484779358 and batch: 350, loss is 5.254528274536133 and perplexity is 191.43116159842268
At time: 1066.7270987033844 and batch: 400, loss is 5.216309928894043 and perplexity is 184.25302132395356
At time: 1067.9300327301025 and batch: 450, loss is 5.191865692138672 and perplexity is 179.80369854222664
At time: 1069.1415300369263 and batch: 500, loss is 5.1908394145965575 and perplexity is 179.61926470076628
At time: 1070.346726179123 and batch: 550, loss is 5.203707895278931 and perplexity is 181.94562801614452
At time: 1071.597127199173 and batch: 600, loss is 5.224757461547852 and perplexity is 185.81609751153076
At time: 1072.798534154892 and batch: 650, loss is 5.24002326965332 and perplexity is 188.6744927446214
At time: 1073.998795747757 and batch: 700, loss is 5.255443925857544 and perplexity is 191.60652606860157
At time: 1075.2031347751617 and batch: 750, loss is 5.220805444717407 and perplexity is 185.08319833563147
At time: 1076.4083161354065 and batch: 800, loss is 5.232055435180664 and perplexity is 187.17713887290878
At time: 1077.6128449440002 and batch: 850, loss is 5.256006298065185 and perplexity is 191.71431055832522
At time: 1078.8159210681915 and batch: 900, loss is 5.2352446269989015 and perplexity is 187.77503556962563
At time: 1080.0175445079803 and batch: 950, loss is 5.214679460525513 and perplexity is 183.95284737943874
At time: 1081.2192776203156 and batch: 1000, loss is 5.2062279415130615 and perplexity is 182.40471763141213
At time: 1082.4252197742462 and batch: 1050, loss is 5.203151473999023 and perplexity is 181.84441775731239
At time: 1083.636885881424 and batch: 1100, loss is 5.17035306930542 and perplexity is 175.9769585794295
At time: 1084.8456449508667 and batch: 1150, loss is 5.205701150894165 and perplexity is 182.30865384229446
At time: 1086.0476696491241 and batch: 1200, loss is 5.229762992858887 and perplexity is 186.7485375377094
At time: 1087.2495245933533 and batch: 1250, loss is 5.249051723480225 and perplexity is 190.38564459500907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141379697479471 and perplexity of 170.95146699562622
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1090.3069732189178 and batch: 50, loss is 5.236428298950195 and perplexity is 187.99743120815614
At time: 1091.512985944748 and batch: 100, loss is 5.26336555480957 and perplexity is 193.1303896450083
At time: 1092.7192113399506 and batch: 150, loss is 5.1858359050750735 and perplexity is 178.7227826481301
At time: 1093.928748846054 and batch: 200, loss is 5.22224048614502 and perplexity is 185.34899105896943
At time: 1095.13716006279 and batch: 250, loss is 5.252423324584961 and perplexity is 191.0286323847153
At time: 1096.3460206985474 and batch: 300, loss is 5.233713569641114 and perplexity is 187.48776119269965
At time: 1097.5517003536224 and batch: 350, loss is 5.253997793197632 and perplexity is 191.32963787018957
At time: 1098.7572858333588 and batch: 400, loss is 5.2155536270141605 and perplexity is 184.1137230999463
At time: 1099.9642074108124 and batch: 450, loss is 5.190549297332764 and perplexity is 179.56716160953522
At time: 1101.1766006946564 and batch: 500, loss is 5.189906997680664 and perplexity is 179.45186271628828
At time: 1102.411984205246 and batch: 550, loss is 5.201324701309204 and perplexity is 181.51253257284728
At time: 1103.616283416748 and batch: 600, loss is 5.222150230407715 and perplexity is 185.3322630040356
At time: 1104.8221950531006 and batch: 650, loss is 5.238015670776367 and perplexity is 188.29609001242224
At time: 1106.0267872810364 and batch: 700, loss is 5.253741607666016 and perplexity is 191.28062826324205
At time: 1107.2380282878876 and batch: 750, loss is 5.218335218429566 and perplexity is 184.62656517947673
At time: 1108.4496195316315 and batch: 800, loss is 5.230061683654785 and perplexity is 186.80432593834558
At time: 1109.65522813797 and batch: 850, loss is 5.25230110168457 and perplexity is 191.00528573798405
At time: 1110.8603339195251 and batch: 900, loss is 5.2309022140502925 and perplexity is 186.96140665862097
At time: 1112.0661528110504 and batch: 950, loss is 5.211017665863037 and perplexity is 183.28048160875053
At time: 1113.2642800807953 and batch: 1000, loss is 5.201720542907715 and perplexity is 181.58439700641847
At time: 1114.4633235931396 and batch: 1050, loss is 5.198776025772094 and perplexity is 181.05050505062283
At time: 1115.6636300086975 and batch: 1100, loss is 5.16568588256836 and perplexity is 175.15755489472107
At time: 1116.8635165691376 and batch: 1150, loss is 5.2001605796813966 and perplexity is 181.3013528512506
At time: 1118.059147119522 and batch: 1200, loss is 5.22510368347168 and perplexity is 185.8804422564274
At time: 1119.2519884109497 and batch: 1250, loss is 5.246940307617187 and perplexity is 189.98408540332198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.142121920620438 and perplexity of 171.07839823023528
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1122.2109322547913 and batch: 50, loss is 5.236518869400024 and perplexity is 188.01445899116268
At time: 1123.4426608085632 and batch: 100, loss is 5.262730054855346 and perplexity is 193.00769428180786
At time: 1124.6545300483704 and batch: 150, loss is 5.18540325164795 and perplexity is 178.64547434876852
At time: 1125.8614156246185 and batch: 200, loss is 5.221885557174683 and perplexity is 185.28321700566892
At time: 1127.0650923252106 and batch: 250, loss is 5.251670045852661 and perplexity is 190.88478876265197
At time: 1128.2702248096466 and batch: 300, loss is 5.232839059829712 and perplexity is 187.32387297738975
At time: 1129.481832742691 and batch: 350, loss is 5.253332948684692 and perplexity is 191.20247568651283
At time: 1130.6985688209534 and batch: 400, loss is 5.214911460876465 and perplexity is 183.9955294555262
At time: 1131.9027893543243 and batch: 450, loss is 5.190253105163574 and perplexity is 179.51398309834096
At time: 1133.1353249549866 and batch: 500, loss is 5.189788122177124 and perplexity is 179.43053155364808
At time: 1134.338553905487 and batch: 550, loss is 5.20105097770691 and perplexity is 181.46285510782735
At time: 1135.54434132576 and batch: 600, loss is 5.221722660064697 and perplexity is 185.2530373632448
At time: 1136.749971151352 and batch: 650, loss is 5.2378474140167235 and perplexity is 188.26441058767665
At time: 1137.961391210556 and batch: 700, loss is 5.253798236846924 and perplexity is 191.29146063525548
At time: 1139.166038274765 and batch: 750, loss is 5.218351831436157 and perplexity is 184.62963240729894
At time: 1140.375102519989 and batch: 800, loss is 5.229702243804931 and perplexity is 186.73719308531233
At time: 1141.5812351703644 and batch: 850, loss is 5.251644096374512 and perplexity is 190.87983546626492
At time: 1142.7905921936035 and batch: 900, loss is 5.230205774307251 and perplexity is 186.83124463487314
At time: 1144.0013077259064 and batch: 950, loss is 5.210503435134887 and perplexity is 183.18625738181004
At time: 1145.2076997756958 and batch: 1000, loss is 5.201095848083496 and perplexity is 181.47099759714948
At time: 1146.4122366905212 and batch: 1050, loss is 5.1982674980163575 and perplexity is 180.95845924952033
At time: 1147.614748954773 and batch: 1100, loss is 5.165133123397827 and perplexity is 175.06076170409136
At time: 1148.8171117305756 and batch: 1150, loss is 5.19950982093811 and perplexity is 181.18340779176472
At time: 1150.0207273960114 and batch: 1200, loss is 5.224409666061401 and perplexity is 185.75148274851918
At time: 1151.2235100269318 and batch: 1250, loss is 5.24644058227539 and perplexity is 189.88916925928433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141885799213047 and perplexity of 171.03800772678989
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1154.231035709381 and batch: 50, loss is 5.236424398422241 and perplexity is 187.99669792035056
At time: 1155.4752659797668 and batch: 100, loss is 5.262656526565552 and perplexity is 192.99350327785663
At time: 1156.688446521759 and batch: 150, loss is 5.185217723846436 and perplexity is 178.6123337210112
At time: 1157.8980927467346 and batch: 200, loss is 5.221736154556274 and perplexity is 185.2555372756647
At time: 1159.1104748249054 and batch: 250, loss is 5.251583242416382 and perplexity is 190.86822002617603
At time: 1160.3311047554016 and batch: 300, loss is 5.232808847427368 and perplexity is 187.31821355866342
At time: 1161.547932624817 and batch: 350, loss is 5.253211164474488 and perplexity is 191.17919166186428
At time: 1162.7858426570892 and batch: 400, loss is 5.214859466552735 and perplexity is 183.98596298110616
At time: 1163.9960825443268 and batch: 450, loss is 5.19022946357727 and perplexity is 179.5097391531838
At time: 1165.2131168842316 and batch: 500, loss is 5.189795837402344 and perplexity is 179.43191590595075
At time: 1166.424536705017 and batch: 550, loss is 5.201001682281494 and perplexity is 181.453910039665
At time: 1167.6372683048248 and batch: 600, loss is 5.221627864837647 and perplexity is 185.23547709183433
At time: 1168.8479166030884 and batch: 650, loss is 5.237757730484009 and perplexity is 188.24752712734616
At time: 1170.0608577728271 and batch: 700, loss is 5.253712339401245 and perplexity is 191.2750298930962
At time: 1171.2758603096008 and batch: 750, loss is 5.218271636962891 and perplexity is 184.61482672485354
At time: 1172.485330581665 and batch: 800, loss is 5.229634380340576 and perplexity is 186.72452088246018
At time: 1173.6969692707062 and batch: 850, loss is 5.251583242416382 and perplexity is 190.86822002617603
At time: 1174.9103717803955 and batch: 900, loss is 5.230094232559204 and perplexity is 186.81040631344982
At time: 1176.1280834674835 and batch: 950, loss is 5.210415830612183 and perplexity is 183.17021014008182
At time: 1177.33757686615 and batch: 1000, loss is 5.20101040840149 and perplexity is 181.4554934351661
At time: 1178.554239988327 and batch: 1050, loss is 5.198219728469849 and perplexity is 180.94981515244902
At time: 1179.7611374855042 and batch: 1100, loss is 5.165030298233032 and perplexity is 175.04276197784824
At time: 1180.9673261642456 and batch: 1150, loss is 5.19939754486084 and perplexity is 181.16306637142026
At time: 1182.1736161708832 and batch: 1200, loss is 5.224312572479248 and perplexity is 185.73344834719538
At time: 1183.3842191696167 and batch: 1250, loss is 5.246379871368408 and perplexity is 189.87764126553347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141859513999772 and perplexity of 171.03351201536424
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1186.4472699165344 and batch: 50, loss is 5.236417455673218 and perplexity is 187.99539271099047
At time: 1187.644834280014 and batch: 100, loss is 5.262650079727173 and perplexity is 192.99225908394342
At time: 1188.835259437561 and batch: 150, loss is 5.185201654434204 and perplexity is 178.60946354885195
At time: 1190.0310454368591 and batch: 200, loss is 5.22172254562378 and perplexity is 185.2530161627185
At time: 1191.2285814285278 and batch: 250, loss is 5.251575193405151 and perplexity is 190.86668373191225
At time: 1192.4285192489624 and batch: 300, loss is 5.232804660797119 and perplexity is 187.31742932820603
At time: 1193.644169807434 and batch: 350, loss is 5.2531990051269535 and perplexity is 191.1768670617644
At time: 1194.8341166973114 and batch: 400, loss is 5.214855165481567 and perplexity is 183.9851716460873
At time: 1196.0238764286041 and batch: 450, loss is 5.1902272987365725 and perplexity is 179.50935054361545
At time: 1197.2139842510223 and batch: 500, loss is 5.189798412322998 and perplexity is 179.43237792949176
At time: 1198.4140853881836 and batch: 550, loss is 5.20099648475647 and perplexity is 181.4529669308777
At time: 1199.6194996833801 and batch: 600, loss is 5.221616430282593 and perplexity is 185.23335901868322
At time: 1200.8241591453552 and batch: 650, loss is 5.237745141983032 and perplexity is 188.24515738808282
At time: 1202.0348494052887 and batch: 700, loss is 5.253696746826172 and perplexity is 191.27204744608508
At time: 1203.23716878891 and batch: 750, loss is 5.218257246017456 and perplexity is 184.6121699620723
At time: 1204.448660850525 and batch: 800, loss is 5.229624500274658 and perplexity is 186.72267604099906
At time: 1205.6558759212494 and batch: 850, loss is 5.251575965881347 and perplexity is 190.86683117193897
At time: 1206.8621847629547 and batch: 900, loss is 5.230080862045288 and perplexity is 186.8079085790105
At time: 1208.067809343338 and batch: 950, loss is 5.2104050827026365 and perplexity is 183.1682414538113
At time: 1209.272498846054 and batch: 1000, loss is 5.201000146865844 and perplexity is 181.45363143270572
At time: 1210.4738764762878 and batch: 1050, loss is 5.1982129859924315 and perplexity is 180.94859510651978
At time: 1211.6759235858917 and batch: 1100, loss is 5.165017347335816 and perplexity is 175.04049503170899
At time: 1212.8789088726044 and batch: 1150, loss is 5.1993833351135255 and perplexity is 181.1604921083142
At time: 1214.0820126533508 and batch: 1200, loss is 5.224300193786621 and perplexity is 185.73114922415778
At time: 1215.284282207489 and batch: 1250, loss is 5.2463719844818115 and perplexity is 189.8761437280151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141855504390967 and perplexity of 171.03282623926336
Annealing...
Model not improving. Stopping early with 170.95131467380358loss at 37 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9541e8860>
SETTINGS FOR THIS RUN
{'lr': 14.633979970439194, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 5.346345943505483, 'wordvec_source': '', 'dropout': 0.17378847536166508, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8662612438201904 and batch: 50, loss is 6.833982639312744 and perplexity is 928.8828609492263
At time: 3.079240322113037 and batch: 100, loss is 6.014769992828369 and perplexity is 409.4316558685169
At time: 4.2890660762786865 and batch: 150, loss is 5.760286512374878 and perplexity is 317.4392661679341
At time: 5.501582145690918 and batch: 200, loss is 5.720685691833496 and perplexity is 305.1140654680343
At time: 6.712230443954468 and batch: 250, loss is 5.718903932571411 and perplexity is 304.57090968601193
At time: 7.971430540084839 and batch: 300, loss is 5.707605447769165 and perplexity is 301.14908699873575
At time: 9.180801630020142 and batch: 350, loss is 5.709108009338379 and perplexity is 301.60192216462576
At time: 10.390626907348633 and batch: 400, loss is 5.656722297668457 and perplexity is 286.2089955636627
At time: 11.603152990341187 and batch: 450, loss is 5.627500095367432 and perplexity is 277.9663589417822
At time: 12.813199996948242 and batch: 500, loss is 5.613539838790894 and perplexity is 274.1128379062908
At time: 14.021678447723389 and batch: 550, loss is 5.624801397323608 and perplexity is 277.217222973205
At time: 15.231447696685791 and batch: 600, loss is 5.638508415222168 and perplexity is 281.0432059159274
At time: 16.442344665527344 and batch: 650, loss is 5.608739948272705 and perplexity is 272.80027888450945
At time: 17.651875495910645 and batch: 700, loss is 5.622165679931641 and perplexity is 276.4875187867075
At time: 18.863735914230347 and batch: 750, loss is 5.571737127304077 and perplexity is 262.8903768710164
At time: 20.068437576293945 and batch: 800, loss is 5.578919315338135 and perplexity is 264.7853016976405
At time: 21.271501302719116 and batch: 850, loss is 5.630046062469482 and perplexity is 278.6749537939596
At time: 22.48151683807373 and batch: 900, loss is 5.607787399291992 and perplexity is 272.5405469802393
At time: 23.6963152885437 and batch: 950, loss is 5.582641716003418 and perplexity is 265.7727754270459
At time: 24.91375708580017 and batch: 1000, loss is 5.570912761688232 and perplexity is 262.6737483863925
At time: 26.119086742401123 and batch: 1050, loss is 5.557687005996704 and perplexity is 259.22256214352444
At time: 27.338781118392944 and batch: 1100, loss is 5.548645629882812 and perplexity is 256.8893968683553
At time: 28.546631336212158 and batch: 1150, loss is 5.5807302379608155 and perplexity is 265.26524182669533
At time: 29.749413013458252 and batch: 1200, loss is 5.57518440246582 and perplexity is 263.79819618993844
At time: 30.95907974243164 and batch: 1250, loss is 5.557096815109253 and perplexity is 259.06961648753133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189275811188413 and perplexity of 179.33863086349967
Finished 1 epochs...
Completing Train Step...
At time: 34.13239145278931 and batch: 50, loss is 5.412311363220215 and perplexity is 224.1490793323872
At time: 35.34068727493286 and batch: 100, loss is 5.430322771072388 and perplexity is 228.22289728631978
At time: 36.5472354888916 and batch: 150, loss is 5.333299283981323 and perplexity is 207.1201964612922
At time: 37.748286962509155 and batch: 200, loss is 5.382343263626098 and perplexity is 217.53141211678167
At time: 38.95760798454285 and batch: 250, loss is 5.389668035507202 and perplexity is 219.1306298900662
At time: 40.16283869743347 and batch: 300, loss is 5.382720947265625 and perplexity is 217.6135856890514
At time: 41.36936974525452 and batch: 350, loss is 5.409838647842407 and perplexity is 223.59550715228164
At time: 42.581183671951294 and batch: 400, loss is 5.3926608276367185 and perplexity is 219.7874246493986
At time: 43.784234046936035 and batch: 450, loss is 5.359149703979492 and perplexity is 212.54414415274803
At time: 45.03803277015686 and batch: 500, loss is 5.364960870742798 and perplexity is 213.7828693523963
At time: 46.244245290756226 and batch: 550, loss is 5.366858377456665 and perplexity is 214.18890889184846
At time: 47.44902539253235 and batch: 600, loss is 5.383775415420533 and perplexity is 217.84317331038022
At time: 48.652873516082764 and batch: 650, loss is 5.385164794921875 and perplexity is 218.14605050684685
At time: 49.85638689994812 and batch: 700, loss is 5.405243091583252 and perplexity is 222.5703188793183
At time: 51.06077551841736 and batch: 750, loss is 5.364100818634033 and perplexity is 213.59908398861552
At time: 52.26546907424927 and batch: 800, loss is 5.376385679244995 and perplexity is 216.23930311945884
At time: 53.470025062561035 and batch: 850, loss is 5.427924728393554 and perplexity is 227.67626472440256
At time: 54.672961473464966 and batch: 900, loss is 5.4133999633789065 and perplexity is 224.39322091789342
At time: 55.87540245056152 and batch: 950, loss is 5.409579086303711 and perplexity is 223.53747788980775
At time: 57.078394651412964 and batch: 1000, loss is 5.3917636299133305 and perplexity is 219.59032030637957
At time: 58.28441023826599 and batch: 1050, loss is 5.375629081726074 and perplexity is 216.07575887561154
At time: 59.488290548324585 and batch: 1100, loss is 5.3738290882110595 and perplexity is 215.68717374116187
At time: 60.69074320793152 and batch: 1150, loss is 5.397696495056152 and perplexity is 220.8969923852598
At time: 61.892600297927856 and batch: 1200, loss is 5.385281524658203 and perplexity is 218.1715161240721
At time: 63.104645013809204 and batch: 1250, loss is 5.393710680007935 and perplexity is 220.0182901644797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2019737967609485 and perplexity of 181.63038977832494
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 66.16550421714783 and batch: 50, loss is 5.35438084602356 and perplexity is 211.53296432276102
At time: 67.37235975265503 and batch: 100, loss is 5.341408882141113 and perplexity is 208.8066871635745
At time: 68.57854771614075 and batch: 150, loss is 5.220439281463623 and perplexity is 185.01544007555762
At time: 69.78431868553162 and batch: 200, loss is 5.264509840011597 and perplexity is 193.35151238155035
At time: 70.98873996734619 and batch: 250, loss is 5.260960597991943 and perplexity is 192.66647746571832
At time: 72.1961817741394 and batch: 300, loss is 5.245954008102417 and perplexity is 189.7967965687006
At time: 73.43097710609436 and batch: 350, loss is 5.25906268119812 and perplexity is 192.30115930392566
At time: 74.63835978507996 and batch: 400, loss is 5.213468799591064 and perplexity is 183.73027760878412
At time: 75.84527707099915 and batch: 450, loss is 5.173691997528076 and perplexity is 176.56551504006558
At time: 77.05943870544434 and batch: 500, loss is 5.15732346534729 and perplexity is 173.69892169115715
At time: 78.26698541641235 and batch: 550, loss is 5.155943698883057 and perplexity is 173.45942300825521
At time: 79.47478532791138 and batch: 600, loss is 5.1530028820037845 and perplexity is 172.95005994804433
At time: 80.68073034286499 and batch: 650, loss is 5.138878173828125 and perplexity is 170.52436228669148
At time: 81.89232301712036 and batch: 700, loss is 5.144553956985473 and perplexity is 171.49497347374373
At time: 83.09198117256165 and batch: 750, loss is 5.1018343257904055 and perplexity is 164.32305302990304
At time: 84.29241299629211 and batch: 800, loss is 5.097566175460815 and perplexity is 163.62319215511891
At time: 85.49325942993164 and batch: 850, loss is 5.110609340667724 and perplexity is 165.7713353210127
At time: 86.69413638114929 and batch: 900, loss is 5.0815937709808345 and perplexity is 161.03049725350598
At time: 87.89816737174988 and batch: 950, loss is 5.055038595199585 and perplexity is 156.81058236310142
At time: 89.10245776176453 and batch: 1000, loss is 5.0256197643280025 and perplexity is 152.2645950401225
At time: 90.30685758590698 and batch: 1050, loss is 4.991991786956787 and perplexity is 147.22938120531077
At time: 91.5083999633789 and batch: 1100, loss is 4.953572635650635 and perplexity is 141.68023266369474
At time: 92.71020364761353 and batch: 1150, loss is 4.963142833709717 and perplexity is 143.04264945211358
At time: 93.89594602584839 and batch: 1200, loss is 4.9371601009368895 and perplexity is 139.37387925145543
At time: 95.08137917518616 and batch: 1250, loss is 4.989741020202636 and perplexity is 146.8983748576395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.951375474024863 and perplexity of 141.3692800250524
Finished 3 epochs...
Completing Train Step...
At time: 98.0016496181488 and batch: 50, loss is 5.1153106784820555 and perplexity is 166.55251722946863
At time: 99.21347832679749 and batch: 100, loss is 5.114552164077759 and perplexity is 166.42623264647065
At time: 100.40265417098999 and batch: 150, loss is 5.021417512893676 and perplexity is 151.6260834599382
At time: 101.59195399284363 and batch: 200, loss is 5.072678308486939 and perplexity is 159.60121671064616
At time: 102.77980637550354 and batch: 250, loss is 5.074874296188354 and perplexity is 159.95208412894056
At time: 103.99460482597351 and batch: 300, loss is 5.072520160675049 and perplexity is 159.57597812321472
At time: 105.18356156349182 and batch: 350, loss is 5.08552303314209 and perplexity is 161.66447300597227
At time: 106.38153648376465 and batch: 400, loss is 5.055535135269165 and perplexity is 156.88846443476325
At time: 107.57148671150208 and batch: 450, loss is 5.013629379272461 and perplexity is 150.44978578890195
At time: 108.7609646320343 and batch: 500, loss is 5.0093497371673585 and perplexity is 149.80729035681418
At time: 109.95911121368408 and batch: 550, loss is 5.0079401588439945 and perplexity is 149.5962740046225
At time: 111.15005612373352 and batch: 600, loss is 5.014018793106079 and perplexity is 150.50838442557227
At time: 112.33904719352722 and batch: 650, loss is 5.015128345489502 and perplexity is 150.67547404243604
At time: 113.52817893028259 and batch: 700, loss is 5.019813709259033 and perplexity is 151.38309989722217
At time: 114.71617937088013 and batch: 750, loss is 4.978416051864624 and perplexity is 145.2441401714655
At time: 115.90565967559814 and batch: 800, loss is 4.991805162429809 and perplexity is 147.2019071554316
At time: 117.09389328956604 and batch: 850, loss is 5.008592739105224 and perplexity is 149.6939294407308
At time: 118.2822105884552 and batch: 900, loss is 4.981697158813477 and perplexity is 145.7214844095667
At time: 119.47172451019287 and batch: 950, loss is 4.962169876098633 and perplexity is 142.9035427010794
At time: 120.66032409667969 and batch: 1000, loss is 4.940483598709107 and perplexity is 139.83785861877084
At time: 121.84963297843933 and batch: 1050, loss is 4.91526611328125 and perplexity is 136.35559100861516
At time: 123.04046559333801 and batch: 1100, loss is 4.884039859771729 and perplexity is 132.16350889445627
At time: 124.23054051399231 and batch: 1150, loss is 4.902092685699463 and perplexity is 134.57110024155273
At time: 125.42000651359558 and batch: 1200, loss is 4.895927104949951 and perplexity is 133.74394382661578
At time: 126.6097891330719 and batch: 1250, loss is 4.949646911621094 and perplexity is 141.12512548152844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.921947618470575 and perplexity of 137.2697020155692
Finished 4 epochs...
Completing Train Step...
At time: 129.55428194999695 and batch: 50, loss is 5.016796417236328 and perplexity is 150.92702128472314
At time: 130.74420189857483 and batch: 100, loss is 5.01354721069336 and perplexity is 150.43742405165767
At time: 131.9306206703186 and batch: 150, loss is 4.9253914928436275 and perplexity is 137.74325658772452
At time: 133.11804246902466 and batch: 200, loss is 4.9848751449584965 and perplexity is 146.1853219122497
At time: 134.33406686782837 and batch: 250, loss is 4.986237382888794 and perplexity is 146.38459680170166
At time: 135.54541110992432 and batch: 300, loss is 4.98585319519043 and perplexity is 146.3283684421941
At time: 136.75870394706726 and batch: 350, loss is 4.994129476547241 and perplexity is 147.54444855904273
At time: 137.9571807384491 and batch: 400, loss is 4.970440101623535 and perplexity is 144.0902877881742
At time: 139.14660143852234 and batch: 450, loss is 4.926935930252075 and perplexity is 137.95615678915863
At time: 140.33594584465027 and batch: 500, loss is 4.925681648254394 and perplexity is 137.78322933779447
At time: 141.52970457077026 and batch: 550, loss is 4.92285228729248 and perplexity is 137.39394182464238
At time: 142.71756386756897 and batch: 600, loss is 4.933461933135987 and perplexity is 138.8594031547386
At time: 143.9052836894989 and batch: 650, loss is 4.937782020568847 and perplexity is 139.46058556252788
At time: 145.0932173728943 and batch: 700, loss is 4.946236352920533 and perplexity is 140.64462980206653
At time: 146.28294920921326 and batch: 750, loss is 4.908071975708008 and perplexity is 135.37815026527412
At time: 147.470397233963 and batch: 800, loss is 4.923304748535156 and perplexity is 137.45612132414257
At time: 148.66058802604675 and batch: 850, loss is 4.944187412261963 and perplexity is 140.3567523243029
At time: 149.850736618042 and batch: 900, loss is 4.916771564483643 and perplexity is 136.56102229163923
At time: 151.04131531715393 and batch: 950, loss is 4.900826778411865 and perplexity is 134.40085348609642
At time: 152.2319450378418 and batch: 1000, loss is 4.887586507797241 and perplexity is 132.63307854925108
At time: 153.42232084274292 and batch: 1050, loss is 4.862341613769531 and perplexity is 129.32668096174928
At time: 154.61359643936157 and batch: 1100, loss is 4.830445299148559 and perplexity is 125.26672940490201
At time: 155.80355858802795 and batch: 1150, loss is 4.85219256401062 and perplexity is 128.02077609758402
At time: 156.99403595924377 and batch: 1200, loss is 4.855129432678223 and perplexity is 128.3973089468317
At time: 158.18460631370544 and batch: 1250, loss is 4.908718271255493 and perplexity is 135.4656728406962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.907444863423814 and perplexity of 135.29327957873775
Finished 5 epochs...
Completing Train Step...
At time: 161.1184527873993 and batch: 50, loss is 4.948380718231201 and perplexity is 140.9465468618886
At time: 162.3103427886963 and batch: 100, loss is 4.941873064041138 and perplexity is 140.0322935244259
At time: 163.52617073059082 and batch: 150, loss is 4.859854850769043 and perplexity is 129.005475702762
At time: 164.71739983558655 and batch: 200, loss is 4.9199081230163575 and perplexity is 136.99002637784614
At time: 165.90879559516907 and batch: 250, loss is 4.920064649581909 and perplexity is 137.01147063444404
At time: 167.10011553764343 and batch: 300, loss is 4.920968494415283 and perplexity is 137.13536372592085
At time: 168.29340839385986 and batch: 350, loss is 4.9260881233215335 and perplexity is 137.8392461691416
At time: 169.48708605766296 and batch: 400, loss is 4.907063837051392 and perplexity is 135.24173909097283
At time: 170.67959022521973 and batch: 450, loss is 4.861782484054565 and perplexity is 129.25439078316242
At time: 171.87011218070984 and batch: 500, loss is 4.861718606948853 and perplexity is 129.2461346504703
At time: 173.06087589263916 and batch: 550, loss is 4.858573598861694 and perplexity is 128.84029303386126
At time: 174.25186967849731 and batch: 600, loss is 4.872780294418335 and perplexity is 130.68375157418586
At time: 175.4428927898407 and batch: 650, loss is 4.880157709121704 and perplexity is 131.651424879277
At time: 176.63593864440918 and batch: 700, loss is 4.891781253814697 and perplexity is 133.19060915941117
At time: 177.82808637619019 and batch: 750, loss is 4.852955493927002 and perplexity is 128.11848424508239
At time: 179.02003931999207 and batch: 800, loss is 4.872631034851074 and perplexity is 130.66424722961912
At time: 180.2161877155304 and batch: 850, loss is 4.894834756851196 and perplexity is 133.5979286480772
At time: 181.4128611087799 and batch: 900, loss is 4.8639146518707275 and perplexity is 129.5302768484646
At time: 182.60654377937317 and batch: 950, loss is 4.85050747871399 and perplexity is 127.80523182636327
At time: 183.79948782920837 and batch: 1000, loss is 4.838306980133057 and perplexity is 126.25541775341284
At time: 184.99331784248352 and batch: 1050, loss is 4.816736574172974 and perplexity is 123.5611992943278
At time: 186.18462705612183 and batch: 1100, loss is 4.787772378921509 and perplexity is 120.03368106257616
At time: 187.37882494926453 and batch: 1150, loss is 4.812604818344116 and perplexity is 123.05172781957326
At time: 188.57151103019714 and batch: 1200, loss is 4.817699556350708 and perplexity is 123.68024383679456
At time: 189.76618123054504 and batch: 1250, loss is 4.870874748229981 and perplexity is 130.43496476215552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.897586126397126 and perplexity of 133.96601205489645
Finished 6 epochs...
Completing Train Step...
At time: 192.6876015663147 and batch: 50, loss is 4.893290309906006 and perplexity is 133.39175298995846
At time: 193.9015018939972 and batch: 100, loss is 4.882752265930176 and perplexity is 131.99344548412125
At time: 195.09252333641052 and batch: 150, loss is 4.80640513420105 and perplexity is 122.29120590760779
At time: 196.28701281547546 and batch: 200, loss is 4.867955732345581 and perplexity is 130.05477818341797
At time: 197.48258471488953 and batch: 250, loss is 4.867345638275147 and perplexity is 129.9754567336501
At time: 198.67506098747253 and batch: 300, loss is 4.870103588104248 and perplexity is 130.33441729241218
At time: 199.86942505836487 and batch: 350, loss is 4.871795873641968 and perplexity is 130.55516707501306
At time: 201.06909203529358 and batch: 400, loss is 4.85982102394104 and perplexity is 129.0011119305307
At time: 202.26832580566406 and batch: 450, loss is 4.807814779281617 and perplexity is 122.46371466390582
At time: 203.46470952033997 and batch: 500, loss is 4.811378231048584 and perplexity is 122.90088666237193
At time: 204.66506505012512 and batch: 550, loss is 4.806517696380615 and perplexity is 122.30497204704514
At time: 205.86034965515137 and batch: 600, loss is 4.823000526428222 and perplexity is 124.33760990814315
At time: 207.05472946166992 and batch: 650, loss is 4.834745140075683 and perplexity is 125.80651608156677
At time: 208.24736499786377 and batch: 700, loss is 4.844434032440185 and perplexity is 127.0313660123584
At time: 209.43778491020203 and batch: 750, loss is 4.806318368911743 and perplexity is 122.28059573605147
At time: 210.62788891792297 and batch: 800, loss is 4.829121675491333 and perplexity is 125.10103308237588
At time: 211.81954503059387 and batch: 850, loss is 4.8550060272216795 and perplexity is 128.3814649959373
At time: 213.01188778877258 and batch: 900, loss is 4.821057682037353 and perplexity is 124.09627579338719
At time: 214.20522546768188 and batch: 950, loss is 4.808272380828857 and perplexity is 122.51976707307331
At time: 215.4005241394043 and batch: 1000, loss is 4.796694536209106 and perplexity is 121.1094323195386
At time: 216.59234046936035 and batch: 1050, loss is 4.778999137878418 and perplexity is 118.9852026473429
At time: 217.78676986694336 and batch: 1100, loss is 4.749397354125977 and perplexity is 115.51464911986838
At time: 218.9797396659851 and batch: 1150, loss is 4.774691314697265 and perplexity is 118.47373787401992
At time: 220.17674803733826 and batch: 1200, loss is 4.784484004974365 and perplexity is 119.6396137085012
At time: 221.36964082717896 and batch: 1250, loss is 4.836769151687622 and perplexity is 126.06140779585189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.890834390682025 and perplexity of 133.06455556914648
Finished 7 epochs...
Completing Train Step...
At time: 224.32130002975464 and batch: 50, loss is 4.845915870666504 and perplexity is 127.21974548593849
At time: 225.51435375213623 and batch: 100, loss is 4.833116035461426 and perplexity is 125.6017309591999
At time: 226.70910096168518 and batch: 150, loss is 4.759448156356812 and perplexity is 116.68151817504547
At time: 227.90472722053528 and batch: 200, loss is 4.823497161865235 and perplexity is 124.39937570764985
At time: 229.10130381584167 and batch: 250, loss is 4.821633491516113 and perplexity is 124.16775218168392
At time: 230.30002284049988 and batch: 300, loss is 4.82250075340271 and perplexity is 124.27548485016156
At time: 231.50014567375183 and batch: 350, loss is 4.823581800460816 and perplexity is 124.40990514169287
At time: 232.6990065574646 and batch: 400, loss is 4.814290866851807 and perplexity is 123.25937400321054
At time: 233.89737248420715 and batch: 450, loss is 4.7592426300048825 and perplexity is 116.6575395124853
At time: 235.09934449195862 and batch: 500, loss is 4.767397403717041 and perplexity is 117.61274479704892
At time: 236.2986867427826 and batch: 550, loss is 4.762878398895264 and perplexity is 117.0824513380023
At time: 237.49670219421387 and batch: 600, loss is 4.778605842590332 and perplexity is 118.93841552896951
At time: 238.69634795188904 and batch: 650, loss is 4.792695484161377 and perplexity is 120.62607652288483
At time: 239.90189170837402 and batch: 700, loss is 4.802281627655029 and perplexity is 121.787975569372
At time: 241.10372614860535 and batch: 750, loss is 4.763783054351807 and perplexity is 117.1884185411479
At time: 242.30293488502502 and batch: 800, loss is 4.788369350433349 and perplexity is 120.10535914338932
At time: 243.50119876861572 and batch: 850, loss is 4.814681797027588 and perplexity is 123.3075692318271
At time: 244.70914506912231 and batch: 900, loss is 4.780205917358399 and perplexity is 119.12887822325105
At time: 245.9122371673584 and batch: 950, loss is 4.770025911331177 and perplexity is 117.92229744478692
At time: 247.10676884651184 and batch: 1000, loss is 4.76059817314148 and perplexity is 116.81578106689958
At time: 248.29970288276672 and batch: 1050, loss is 4.742497482299805 and perplexity is 114.72035625760446
At time: 249.49206137657166 and batch: 1100, loss is 4.715425834655762 and perplexity is 111.65634819413123
At time: 250.6881446838379 and batch: 1150, loss is 4.738768262863159 and perplexity is 114.29333559702734
At time: 251.8832197189331 and batch: 1200, loss is 4.753245830535889 and perplexity is 115.96006105091911
At time: 253.08491110801697 and batch: 1250, loss is 4.807246150970459 and perplexity is 122.39409812350216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.885096194970346 and perplexity of 132.30319162377742
Finished 8 epochs...
Completing Train Step...
At time: 256.0621359348297 and batch: 50, loss is 4.8030304431915285 and perplexity is 121.87920645092495
At time: 257.2812533378601 and batch: 100, loss is 4.788222141265869 and perplexity is 120.08767983477003
At time: 258.4787051677704 and batch: 150, loss is 4.717653474807739 and perplexity is 111.90535560507254
At time: 259.6751937866211 and batch: 200, loss is 4.783807954788208 and perplexity is 119.55875865949682
At time: 260.87528944015503 and batch: 250, loss is 4.782963485717773 and perplexity is 119.4578376040617
At time: 262.0766520500183 and batch: 300, loss is 4.785051593780517 and perplexity is 119.70753908903382
At time: 263.2749481201172 and batch: 350, loss is 4.787111806869507 and perplexity is 119.95441635047635
At time: 264.47824120521545 and batch: 400, loss is 4.7770263671875 and perplexity is 118.7507035094729
At time: 265.67892837524414 and batch: 450, loss is 4.721431379318237 and perplexity is 112.328922947466
At time: 266.8818054199219 and batch: 500, loss is 4.730949831008911 and perplexity is 113.40322510957083
At time: 268.08149218559265 and batch: 550, loss is 4.725917568206787 and perplexity is 112.8339837655044
At time: 269.28188037872314 and batch: 600, loss is 4.745730838775635 and perplexity is 115.0918883886457
At time: 270.4782683849335 and batch: 650, loss is 4.7593271350860595 and perplexity is 116.66739808387554
At time: 271.672730922699 and batch: 700, loss is 4.768839159011841 and perplexity is 117.78243589171898
At time: 272.8656873703003 and batch: 750, loss is 4.729993076324463 and perplexity is 113.29477792966188
At time: 274.0592851638794 and batch: 800, loss is 4.756057043075561 and perplexity is 116.28650806963266
At time: 275.2514591217041 and batch: 850, loss is 4.783868179321289 and perplexity is 119.5659592467367
At time: 276.4446301460266 and batch: 900, loss is 4.747287425994873 and perplexity is 115.27117845526149
At time: 277.63689494132996 and batch: 950, loss is 4.736540670394898 and perplexity is 114.03901998441359
At time: 278.8298177719116 and batch: 1000, loss is 4.729979982376099 and perplexity is 113.29329446340202
At time: 280.0218105316162 and batch: 1050, loss is 4.71234899520874 and perplexity is 111.31332751788399
At time: 281.2136516571045 and batch: 1100, loss is 4.686723985671997 and perplexity is 108.49715871422886
At time: 282.4106991291046 and batch: 1150, loss is 4.711142692565918 and perplexity is 111.17913091385013
At time: 283.6041889190674 and batch: 1200, loss is 4.726572380065918 and perplexity is 112.90789299186511
At time: 284.82066917419434 and batch: 1250, loss is 4.778144636154175 and perplexity is 118.88357301405374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.884067507555885 and perplexity of 132.16716297313687
Finished 9 epochs...
Completing Train Step...
At time: 287.7416672706604 and batch: 50, loss is 4.769360284805298 and perplexity is 117.84383135306821
At time: 288.93588733673096 and batch: 100, loss is 4.751481285095215 and perplexity is 115.75562467557326
At time: 290.14042615890503 and batch: 150, loss is 4.682187471389771 and perplexity is 108.00607455150724
At time: 291.3318259716034 and batch: 200, loss is 4.749478845596314 and perplexity is 115.52406296203935
At time: 292.52669191360474 and batch: 250, loss is 4.747624597549438 and perplexity is 115.3100511707165
At time: 293.7272446155548 and batch: 300, loss is 4.749453983306885 and perplexity is 115.52119080505432
At time: 294.9282615184784 and batch: 350, loss is 4.749870615005493 and perplexity is 115.56933062259533
At time: 296.1234624385834 and batch: 400, loss is 4.742894115447998 and perplexity is 114.76586717864213
At time: 297.3201324939728 and batch: 450, loss is 4.684361314773559 and perplexity is 108.24111822358564
At time: 298.5130157470703 and batch: 500, loss is 4.695787868499756 and perplexity is 109.48503447990304
At time: 299.7052059173584 and batch: 550, loss is 4.693552188873291 and perplexity is 109.24053443264029
At time: 300.897545337677 and batch: 600, loss is 4.711518106460571 and perplexity is 111.22087693991944
At time: 302.08948159217834 and batch: 650, loss is 4.726042861938477 and perplexity is 112.84812204209197
At time: 303.28272914886475 and batch: 700, loss is 4.736579923629761 and perplexity is 114.04349647270637
At time: 304.4783396720886 and batch: 750, loss is 4.696633472442627 and perplexity is 109.57765461119973
At time: 305.67178869247437 and batch: 800, loss is 4.724324560165405 and perplexity is 112.65438141400085
At time: 306.8714964389801 and batch: 850, loss is 4.753042116165161 and perplexity is 115.93644072603405
At time: 308.0659325122833 and batch: 900, loss is 4.7130955219268795 and perplexity is 111.39645691625891
At time: 309.26062536239624 and batch: 950, loss is 4.706622543334961 and perplexity is 110.67771873341935
At time: 310.4550516605377 and batch: 1000, loss is 4.699770965576172 and perplexity is 109.92199364834588
At time: 311.65253376960754 and batch: 1050, loss is 4.682727918624878 and perplexity is 108.06446191209606
At time: 312.84754729270935 and batch: 1100, loss is 4.66019986152649 and perplexity is 105.65719685522937
At time: 314.04484820365906 and batch: 1150, loss is 4.684306106567383 and perplexity is 108.23514259056749
At time: 315.2671785354614 and batch: 1200, loss is 4.702449750900269 and perplexity is 110.21684581824037
At time: 316.4606475830078 and batch: 1250, loss is 4.753187856674194 and perplexity is 115.95333859324298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.879908206689096 and perplexity of 131.61858162591952
Finished 10 epochs...
Completing Train Step...
At time: 319.38758182525635 and batch: 50, loss is 4.737218828201294 and perplexity is 114.11638266513779
At time: 320.62031078338623 and batch: 100, loss is 4.716546325683594 and perplexity is 111.78152824895041
At time: 321.81895208358765 and batch: 150, loss is 4.646932773590088 and perplexity is 104.26469120605617
At time: 323.0205454826355 and batch: 200, loss is 4.719035291671753 and perplexity is 112.06009519888937
At time: 324.2192485332489 and batch: 250, loss is 4.714730567932129 and perplexity is 111.57874423160199
At time: 325.4228699207306 and batch: 300, loss is 4.716700057983399 and perplexity is 111.7987140013326
At time: 326.622123003006 and batch: 350, loss is 4.715855798721313 and perplexity is 111.70436673392894
At time: 327.82262086868286 and batch: 400, loss is 4.7122256565094 and perplexity is 111.29959912348728
At time: 329.0221486091614 and batch: 450, loss is 4.65190598487854 and perplexity is 104.78451306593149
At time: 330.2219166755676 and batch: 500, loss is 4.665440196990967 and perplexity is 106.21232928067629
At time: 331.42959475517273 and batch: 550, loss is 4.6622867679595945 and perplexity is 105.87792377715829
At time: 332.64098358154297 and batch: 600, loss is 4.682725706100464 and perplexity is 108.06422281710026
At time: 333.84589672088623 and batch: 650, loss is 4.697349615097046 and perplexity is 109.65615594936139
At time: 335.04936599731445 and batch: 700, loss is 4.706080894470215 and perplexity is 110.61778650529165
At time: 336.2557783126831 and batch: 750, loss is 4.667979822158814 and perplexity is 106.48241159401093
At time: 337.4601664543152 and batch: 800, loss is 4.696592464447021 and perplexity is 109.57316114335566
At time: 338.6615483760834 and batch: 850, loss is 4.725819835662842 and perplexity is 112.82295675208528
At time: 339.8668305873871 and batch: 900, loss is 4.685619211196899 and perplexity is 108.37736001012074
At time: 341.07076621055603 and batch: 950, loss is 4.677639446258545 and perplexity is 107.51597554620717
At time: 342.2732207775116 and batch: 1000, loss is 4.672702503204346 and perplexity is 106.98648340924298
At time: 343.4840784072876 and batch: 1050, loss is 4.656075277328491 and perplexity is 105.22230234672661
At time: 344.7147183418274 and batch: 1100, loss is 4.634948482513428 and perplexity is 103.02261039256578
At time: 345.9130234718323 and batch: 1150, loss is 4.659704608917236 and perplexity is 105.60488280820384
At time: 347.11446237564087 and batch: 1200, loss is 4.676877994537353 and perplexity is 107.43413848301236
At time: 348.31240797042847 and batch: 1250, loss is 4.72805778503418 and perplexity is 113.07573156033257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.880008892421305 and perplexity of 131.63183440635453
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 351.25613236427307 and batch: 50, loss is 4.715779533386231 and perplexity is 111.69584788782022
At time: 352.4817862510681 and batch: 100, loss is 4.700181217193603 and perplexity is 109.96709857558345
At time: 353.6786048412323 and batch: 150, loss is 4.631586370468139 and perplexity is 102.6768184545779
At time: 354.87645626068115 and batch: 200, loss is 4.7009202003479 and perplexity is 110.04839244265733
At time: 356.07889795303345 and batch: 250, loss is 4.69209098815918 and perplexity is 109.08102864908003
At time: 357.27950406074524 and batch: 300, loss is 4.688373384475708 and perplexity is 108.67626146334385
At time: 358.48198556900024 and batch: 350, loss is 4.6821326065063475 and perplexity is 108.00014897337256
At time: 359.6833417415619 and batch: 400, loss is 4.6677815628051755 and perplexity is 106.46130255251606
At time: 360.8861572742462 and batch: 450, loss is 4.606099615097046 and perplexity is 100.09298611620325
At time: 362.0909330844879 and batch: 500, loss is 4.6097893810272215 and perplexity is 100.46298799661206
At time: 363.29197573661804 and batch: 550, loss is 4.605827779769897 and perplexity is 100.06578100439984
At time: 364.49032068252563 and batch: 600, loss is 4.622776594161987 and perplexity is 101.77623146184163
At time: 365.68981528282166 and batch: 650, loss is 4.6358832740783695 and perplexity is 103.11896008618331
At time: 366.8907814025879 and batch: 700, loss is 4.639085283279419 and perplexity is 103.44967714214366
At time: 368.09058451652527 and batch: 750, loss is 4.591983804702759 and perplexity is 98.69001781540621
At time: 369.2898383140564 and batch: 800, loss is 4.608583698272705 and perplexity is 100.34193449524115
At time: 370.48913049697876 and batch: 850, loss is 4.628954343795776 and perplexity is 102.4069256680867
At time: 371.6838171482086 and batch: 900, loss is 4.5870359516143795 and perplexity is 98.20292014355566
At time: 372.8835783004761 and batch: 950, loss is 4.568968868255615 and perplexity is 96.44461138331428
At time: 374.0822710990906 and batch: 1000, loss is 4.556258344650269 and perplexity is 95.22650764133104
At time: 375.3128581047058 and batch: 1050, loss is 4.531893987655639 and perplexity is 92.93441112809701
At time: 376.51315426826477 and batch: 1100, loss is 4.498160829544068 and perplexity is 89.8517266024472
At time: 377.71120405197144 and batch: 1150, loss is 4.513454389572144 and perplexity is 91.2364409937245
At time: 378.9109606742859 and batch: 1200, loss is 4.536695032119751 and perplexity is 93.38166615470075
At time: 380.1147677898407 and batch: 1250, loss is 4.608184003829956 and perplexity is 100.30183639567639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.827325305799498 and perplexity of 124.87650710433816
Finished 12 epochs...
Completing Train Step...
At time: 383.14797139167786 and batch: 50, loss is 4.67004503250122 and perplexity is 106.70254740685633
At time: 384.34993600845337 and batch: 100, loss is 4.654289855957031 and perplexity is 105.0346038097132
At time: 385.55374097824097 and batch: 150, loss is 4.587762069702149 and perplexity is 98.2742529550318
At time: 386.75881481170654 and batch: 200, loss is 4.656654043197632 and perplexity is 105.28321905055135
At time: 387.962753534317 and batch: 250, loss is 4.651092462539673 and perplexity is 104.69930318855386
At time: 389.1754789352417 and batch: 300, loss is 4.650060291290283 and perplexity is 104.591291330928
At time: 390.3794686794281 and batch: 350, loss is 4.645278491973877 and perplexity is 104.09235063339761
At time: 391.58246064186096 and batch: 400, loss is 4.634595375061036 and perplexity is 102.9862387629959
At time: 392.78566098213196 and batch: 450, loss is 4.573433418273925 and perplexity is 96.87615578380972
At time: 393.9871029853821 and batch: 500, loss is 4.579896631240845 and perplexity is 97.50431478608267
At time: 395.19231367111206 and batch: 550, loss is 4.576018400192261 and perplexity is 97.12690284350938
At time: 396.39510798454285 and batch: 600, loss is 4.595206317901611 and perplexity is 99.00856067904115
At time: 397.5997338294983 and batch: 650, loss is 4.6119593811035156 and perplexity is 100.68122939451474
At time: 398.80672430992126 and batch: 700, loss is 4.6168012619018555 and perplexity is 101.16989798872743
At time: 400.0134291648865 and batch: 750, loss is 4.572220344543457 and perplexity is 96.75870911425673
At time: 401.21785950660706 and batch: 800, loss is 4.593004064559937 and perplexity is 98.79075866106916
At time: 402.42386627197266 and batch: 850, loss is 4.6160086727142335 and perplexity is 101.08974359041953
At time: 403.6381895542145 and batch: 900, loss is 4.575047779083252 and perplexity is 97.03267515844458
At time: 404.8420753479004 and batch: 950, loss is 4.55895357131958 and perplexity is 95.48351084972411
At time: 406.0699634552002 and batch: 1000, loss is 4.549251012802124 and perplexity is 94.56155638949677
At time: 407.27939534187317 and batch: 1050, loss is 4.529360265731811 and perplexity is 92.69923922905393
At time: 408.485800743103 and batch: 1100, loss is 4.499853687286377 and perplexity is 90.00396161323852
At time: 409.69057631492615 and batch: 1150, loss is 4.518530397415161 and perplexity is 91.70073526755421
At time: 410.89637756347656 and batch: 1200, loss is 4.544575653076172 and perplexity is 94.12047899832385
At time: 412.10009026527405 and batch: 1250, loss is 4.612146024703979 and perplexity is 100.70002265543445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.824846476533987 and perplexity of 124.56734290489328
Finished 13 epochs...
Completing Train Step...
At time: 415.0876624584198 and batch: 50, loss is 4.6538674259185795 and perplexity is 104.99024340822984
At time: 416.31317472457886 and batch: 100, loss is 4.63653959274292 and perplexity is 103.18666119867852
At time: 417.51417303085327 and batch: 150, loss is 4.569920644760132 and perplexity is 96.53644879582916
At time: 418.7142744064331 and batch: 200, loss is 4.6376128673553465 and perplexity is 103.2974682750584
At time: 419.91508531570435 and batch: 250, loss is 4.63235182762146 and perplexity is 102.75544324786038
At time: 421.11726784706116 and batch: 300, loss is 4.632825326919556 and perplexity is 102.80410939889869
At time: 422.3171634674072 and batch: 350, loss is 4.627918663024903 and perplexity is 102.30091968802105
At time: 423.5164647102356 and batch: 400, loss is 4.617868165969849 and perplexity is 101.27789416498578
At time: 424.72334241867065 and batch: 450, loss is 4.557163305282593 and perplexity is 95.3127228867178
At time: 425.93257546424866 and batch: 500, loss is 4.564668865203857 and perplexity is 96.03078961510462
At time: 427.13511085510254 and batch: 550, loss is 4.562006969451904 and perplexity is 95.77550558464814
At time: 428.33915758132935 and batch: 600, loss is 4.58192307472229 and perplexity is 97.70210210392017
At time: 429.5381100177765 and batch: 650, loss is 4.599788255691529 and perplexity is 99.46325263437915
At time: 430.7375388145447 and batch: 700, loss is 4.605386819839477 and perplexity is 100.02166573181934
At time: 431.9374647140503 and batch: 750, loss is 4.561964206695556 and perplexity is 95.77141004760762
At time: 433.1385576725006 and batch: 800, loss is 4.584563837051392 and perplexity is 97.9604511036476
At time: 434.3377878665924 and batch: 850, loss is 4.608170394897461 and perplexity is 100.30047140404375
At time: 435.5389950275421 and batch: 900, loss is 4.567846269607544 and perplexity is 96.33640354131407
At time: 436.7709321975708 and batch: 950, loss is 4.553360013961792 and perplexity is 94.95090931235117
At time: 437.974924325943 and batch: 1000, loss is 4.544901952743531 and perplexity is 94.1511954904307
At time: 439.1789345741272 and batch: 1050, loss is 4.5266030979156495 and perplexity is 92.44400389508411
At time: 440.3804519176483 and batch: 1100, loss is 4.498937969207764 and perplexity is 89.92158108286955
At time: 441.5830748081207 and batch: 1150, loss is 4.518665733337403 and perplexity is 91.71314651095643
At time: 442.789338350296 and batch: 1200, loss is 4.545767097473145 and perplexity is 94.23268514608642
At time: 443.98961210250854 and batch: 1250, loss is 4.610446872711182 and perplexity is 100.52906329536505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.823312578410127 and perplexity of 124.37641576014694
Finished 14 epochs...
Completing Train Step...
At time: 447.00762820243835 and batch: 50, loss is 4.640832300186157 and perplexity is 103.63056343680864
At time: 448.2167534828186 and batch: 100, loss is 4.622977199554444 and perplexity is 101.79665037069992
At time: 449.42223048210144 and batch: 150, loss is 4.557138252258301 and perplexity is 95.3103350446674
At time: 450.6268038749695 and batch: 200, loss is 4.624309577941895 and perplexity is 101.93237242405584
At time: 451.8283052444458 and batch: 250, loss is 4.619110279083252 and perplexity is 101.40377092582955
At time: 453.0382900238037 and batch: 300, loss is 4.620178356170654 and perplexity is 101.5121358308659
At time: 454.2437734603882 and batch: 350, loss is 4.615616369247436 and perplexity is 101.0500935114916
At time: 455.44434332847595 and batch: 400, loss is 4.605475931167603 and perplexity is 100.03057919243327
At time: 456.64545345306396 and batch: 450, loss is 4.543580770492554 and perplexity is 94.02688673738155
At time: 457.8558671474457 and batch: 500, loss is 4.553022136688233 and perplexity is 94.91883297722819
At time: 459.0589530467987 and batch: 550, loss is 4.550669002532959 and perplexity is 94.69573881755805
At time: 460.26591539382935 and batch: 600, loss is 4.570603837966919 and perplexity is 96.60242437632182
At time: 461.47769021987915 and batch: 650, loss is 4.58945146560669 and perplexity is 98.44041739473757
At time: 462.68883085250854 and batch: 700, loss is 4.59568000793457 and perplexity is 99.05547115704822
At time: 463.89370942115784 and batch: 750, loss is 4.553107404708863 and perplexity is 94.92692686330659
At time: 465.0936300754547 and batch: 800, loss is 4.576534337997437 and perplexity is 97.17702721400295
At time: 466.3298840522766 and batch: 850, loss is 4.600724325180054 and perplexity is 99.55640074014798
At time: 467.5360987186432 and batch: 900, loss is 4.560804882049561 and perplexity is 95.66044422669167
At time: 468.7358248233795 and batch: 950, loss is 4.546403036117554 and perplexity is 94.29263041087223
At time: 469.93682289123535 and batch: 1000, loss is 4.538963994979858 and perplexity is 93.59378624231812
At time: 471.13789892196655 and batch: 1050, loss is 4.521767597198487 and perplexity is 91.99806987399602
At time: 472.34325194358826 and batch: 1100, loss is 4.494749755859375 and perplexity is 89.54575787991118
At time: 473.54905557632446 and batch: 1150, loss is 4.515722961425781 and perplexity is 91.44365236386356
At time: 474.75435042381287 and batch: 1200, loss is 4.543294515609741 and perplexity is 93.99997493393863
At time: 475.95809721946716 and batch: 1250, loss is 4.60574047088623 and perplexity is 100.05704475414878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.820937107949361 and perplexity of 124.08131390013662
Finished 15 epochs...
Completing Train Step...
At time: 478.97037959098816 and batch: 50, loss is 4.62782730102539 and perplexity is 102.29157369838705
At time: 480.203426361084 and batch: 100, loss is 4.6099973487854005 and perplexity is 100.48388323169797
At time: 481.40745520591736 and batch: 150, loss is 4.545115022659302 and perplexity is 94.17125841505054
At time: 482.6157965660095 and batch: 200, loss is 4.612458581924439 and perplexity is 100.73150209392243
At time: 483.8101131916046 and batch: 250, loss is 4.606991357803345 and perplexity is 100.18228311558059
At time: 485.0045425891876 and batch: 300, loss is 4.608309946060181 and perplexity is 100.31446942814708
At time: 486.1985960006714 and batch: 350, loss is 4.603051595687866 and perplexity is 99.78836523281882
At time: 487.3943085670471 and batch: 400, loss is 4.593664751052857 and perplexity is 98.85604994710226
At time: 488.58992743492126 and batch: 450, loss is 4.532084150314331 and perplexity is 92.9520854632471
At time: 489.8005430698395 and batch: 500, loss is 4.542905178070068 and perplexity is 93.96338433847697
At time: 490.99723744392395 and batch: 550, loss is 4.540928678512573 and perplexity is 93.7778491664042
At time: 492.19261837005615 and batch: 600, loss is 4.5617369556427 and perplexity is 95.74964836661685
At time: 493.3860466480255 and batch: 650, loss is 4.58131404876709 and perplexity is 97.64261710365443
At time: 494.57878279685974 and batch: 700, loss is 4.588447380065918 and perplexity is 98.34162440161208
At time: 495.77688670158386 and batch: 750, loss is 4.546153573989868 and perplexity is 94.2691109043999
At time: 496.9966356754303 and batch: 800, loss is 4.5695969772338865 and perplexity is 96.50520813832077
At time: 498.1897928714752 and batch: 850, loss is 4.594340410232544 and perplexity is 98.92286551445046
At time: 499.39239740371704 and batch: 900, loss is 4.554306697845459 and perplexity is 95.04084036936142
At time: 500.58565759658813 and batch: 950, loss is 4.5406320190429685 and perplexity is 93.75003320554791
At time: 501.7877097129822 and batch: 1000, loss is 4.533649673461914 and perplexity is 93.09771807052103
At time: 502.9933760166168 and batch: 1050, loss is 4.517575826644897 and perplexity is 91.61324219186608
At time: 504.18895840644836 and batch: 1100, loss is 4.490985250473022 and perplexity is 89.20929609549438
At time: 505.390287399292 and batch: 1150, loss is 4.512599515914917 and perplexity is 91.1584786924466
At time: 506.58577036857605 and batch: 1200, loss is 4.540405359268188 and perplexity is 93.7287862521417
At time: 507.7833902835846 and batch: 1250, loss is 4.601628084182739 and perplexity is 99.64641640369801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.820136077212592 and perplexity of 123.98196075161924
Finished 16 epochs...
Completing Train Step...
At time: 510.6973509788513 and batch: 50, loss is 4.6179846668243405 and perplexity is 101.28969381351833
At time: 511.9239273071289 and batch: 100, loss is 4.59979100227356 and perplexity is 99.46352581873674
At time: 513.1189959049225 and batch: 150, loss is 4.536174640655518 and perplexity is 93.33308377474225
At time: 514.3170785903931 and batch: 200, loss is 4.603193349838257 and perplexity is 99.80251165038433
At time: 515.5145909786224 and batch: 250, loss is 4.5981776237487795 and perplexity is 99.30318288388663
At time: 516.708621263504 and batch: 300, loss is 4.5993969535827635 and perplexity is 99.42434006765988
At time: 517.9038181304932 and batch: 350, loss is 4.594649572372436 and perplexity is 98.95345344730895
At time: 519.0998322963715 and batch: 400, loss is 4.584928798675537 and perplexity is 97.99620943379658
At time: 520.2957463264465 and batch: 450, loss is 4.52331163406372 and perplexity is 92.14022800586729
At time: 521.4903485774994 and batch: 500, loss is 4.5352630710601805 and perplexity is 93.24804293954277
At time: 522.6903772354126 and batch: 550, loss is 4.533278360366821 and perplexity is 93.06315608573401
At time: 523.8878507614136 and batch: 600, loss is 4.554781408309936 and perplexity is 95.08596796126005
At time: 525.085521697998 and batch: 650, loss is 4.574647922515869 and perplexity is 96.99388376204587
At time: 526.2828145027161 and batch: 700, loss is 4.5829088306427 and perplexity is 97.79846001439326
At time: 527.5206065177917 and batch: 750, loss is 4.539458322525024 and perplexity is 93.64006366607434
At time: 528.713000535965 and batch: 800, loss is 4.563607044219971 and perplexity is 95.92887622405603
At time: 529.9062883853912 and batch: 850, loss is 4.58901969909668 and perplexity is 98.39792329370012
At time: 531.0993585586548 and batch: 900, loss is 4.54899920463562 and perplexity is 94.53774801505793
At time: 532.2906947135925 and batch: 950, loss is 4.535833082199097 and perplexity is 93.3012105143158
At time: 533.4838547706604 and batch: 1000, loss is 4.529031496047974 and perplexity is 92.6687675388391
At time: 534.6785144805908 and batch: 1050, loss is 4.513672304153443 and perplexity is 91.2563249109819
At time: 535.8734948635101 and batch: 1100, loss is 4.487603454589844 and perplexity is 88.90811801365236
At time: 537.067747592926 and batch: 1150, loss is 4.509781475067139 and perplexity is 90.90195199698725
At time: 538.2648377418518 and batch: 1200, loss is 4.5375304031372075 and perplexity is 93.45970708419513
At time: 539.4637022018433 and batch: 1250, loss is 4.597303562164306 and perplexity is 99.21642370846664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.819596116560219 and perplexity of 123.91503344189236
Finished 17 epochs...
Completing Train Step...
At time: 542.4368672370911 and batch: 50, loss is 4.60942587852478 and perplexity is 100.4264760855601
At time: 543.638295173645 and batch: 100, loss is 4.5912351894378665 and perplexity is 98.61616460884511
At time: 544.8448853492737 and batch: 150, loss is 4.5283002853393555 and perplexity is 92.60103191118637
At time: 546.0497601032257 and batch: 200, loss is 4.595280723571777 and perplexity is 99.01592775142326
At time: 547.2498185634613 and batch: 250, loss is 4.5901275157928465 and perplexity is 98.5069905581068
At time: 548.4499845504761 and batch: 300, loss is 4.5916000175476075 and perplexity is 98.65214912145099
At time: 549.6490485668182 and batch: 350, loss is 4.587049207687378 and perplexity is 98.20422193726205
At time: 550.8488457202911 and batch: 400, loss is 4.577091283798218 and perplexity is 97.23116462564482
At time: 552.0495367050171 and batch: 450, loss is 4.515949773788452 and perplexity is 91.46439526699199
At time: 553.251255273819 and batch: 500, loss is 4.528311929702759 and perplexity is 92.60211019753143
At time: 554.4590816497803 and batch: 550, loss is 4.526264085769653 and perplexity is 92.4126695665982
At time: 555.6625385284424 and batch: 600, loss is 4.548211145401001 and perplexity is 94.46327601774134
At time: 556.8641610145569 and batch: 650, loss is 4.568223657608033 and perplexity is 96.37276660508121
At time: 558.0950496196747 and batch: 700, loss is 4.57640456199646 and perplexity is 97.16441678630744
At time: 559.2952942848206 and batch: 750, loss is 4.533480215072632 and perplexity is 93.08194321779922
At time: 560.4951121807098 and batch: 800, loss is 4.557777395248413 and perplexity is 95.37127144865981
At time: 561.6975362300873 and batch: 850, loss is 4.583669567108155 and perplexity is 97.8728871753298
At time: 562.8951861858368 and batch: 900, loss is 4.54363166809082 and perplexity is 94.03167260188245
At time: 564.0978157520294 and batch: 950, loss is 4.530841541290283 and perplexity is 92.83665409592868
At time: 565.2972178459167 and batch: 1000, loss is 4.5244843292236325 and perplexity is 92.248343786319
At time: 566.4952967166901 and batch: 1050, loss is 4.509799757003784 and perplexity is 90.90361387590573
At time: 567.694384098053 and batch: 1100, loss is 4.483943681716919 and perplexity is 88.58332918435335
At time: 568.8943562507629 and batch: 1150, loss is 4.506346836090088 and perplexity is 90.5902721698103
At time: 570.1000003814697 and batch: 1200, loss is 4.53416561126709 and perplexity is 93.1457630959059
At time: 571.2987468242645 and batch: 1250, loss is 4.592860078811645 and perplexity is 98.77653522376977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.819599235144845 and perplexity of 123.91541988201317
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 574.2310740947723 and batch: 50, loss is 4.605881948471069 and perplexity is 100.07120158460015
At time: 575.4564571380615 and batch: 100, loss is 4.591566257476806 and perplexity is 98.64881867413035
At time: 576.653092622757 and batch: 150, loss is 4.528610153198242 and perplexity is 92.62973044082172
At time: 577.8516874313354 and batch: 200, loss is 4.595180196762085 and perplexity is 99.00597449639054
At time: 579.0497963428497 and batch: 250, loss is 4.589299621582032 and perplexity is 98.42547094036495
At time: 580.2478759288788 and batch: 300, loss is 4.588475084304809 and perplexity is 98.34434891920765
At time: 581.4520590305328 and batch: 350, loss is 4.582444763183593 and perplexity is 97.75308546079104
At time: 582.6491665840149 and batch: 400, loss is 4.569948663711548 and perplexity is 96.53915368379171
At time: 583.8480339050293 and batch: 450, loss is 4.508134841918945 and perplexity is 90.75239299778923
At time: 585.0426254272461 and batch: 500, loss is 4.517180509567261 and perplexity is 91.57703307020363
At time: 586.2397885322571 and batch: 550, loss is 4.514750175476074 and perplexity is 91.35474051676344
At time: 587.4671247005463 and batch: 600, loss is 4.534913568496704 and perplexity is 93.21545820405225
At time: 588.662312746048 and batch: 650, loss is 4.554321088790894 and perplexity is 95.04220810675072
At time: 589.8589091300964 and batch: 700, loss is 4.561114253997803 and perplexity is 95.69004346304246
At time: 591.0568797588348 and batch: 750, loss is 4.515376262664795 and perplexity is 91.4119544580188
At time: 592.2524309158325 and batch: 800, loss is 4.536474180221558 and perplexity is 93.36104491367794
At time: 593.4494442939758 and batch: 850, loss is 4.556926326751709 and perplexity is 95.29013849378737
At time: 594.6496167182922 and batch: 900, loss is 4.516920318603516 and perplexity is 91.55320865329557
At time: 595.8491699695587 and batch: 950, loss is 4.502233171463013 and perplexity is 90.2183796166864
At time: 597.056515455246 and batch: 1000, loss is 4.493034496307373 and perplexity is 89.39229531505691
At time: 598.2494659423828 and batch: 1050, loss is 4.4754717159271244 and perplexity is 87.83602429046763
At time: 599.4437279701233 and batch: 1100, loss is 4.446204748153686 and perplexity is 85.30258408800185
At time: 600.6469686031342 and batch: 1150, loss is 4.465789461135865 and perplexity is 86.9896774083101
At time: 601.8425259590149 and batch: 1200, loss is 4.4958539485931395 and perplexity is 89.64468826415569
At time: 603.0414912700653 and batch: 1250, loss is 4.561268358230591 and perplexity is 95.7047908400632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.813626254562044 and perplexity of 123.17748152480766
Finished 19 epochs...
Completing Train Step...
At time: 605.9959819316864 and batch: 50, loss is 4.597871179580689 and perplexity is 99.2727566648258
At time: 607.1923472881317 and batch: 100, loss is 4.582009105682373 and perplexity is 97.71050787113921
At time: 608.3896405696869 and batch: 150, loss is 4.51977014541626 and perplexity is 91.81449157080614
At time: 609.5862512588501 and batch: 200, loss is 4.585925140380859 and perplexity is 98.09389580063858
At time: 610.7856221199036 and batch: 250, loss is 4.5808642387390135 and perplexity is 97.59870635180222
At time: 611.9857726097107 and batch: 300, loss is 4.58086371421814 and perplexity is 97.5986551592569
At time: 613.1809632778168 and batch: 350, loss is 4.575080413818359 and perplexity is 97.03584184576683
At time: 614.3819808959961 and batch: 400, loss is 4.563011560440064 and perplexity is 95.87176913909877
At time: 615.5826227664948 and batch: 450, loss is 4.501539182662964 and perplexity is 90.15579079215748
At time: 616.7791879177094 and batch: 500, loss is 4.511558160781861 and perplexity is 91.06359975264817
At time: 618.0019626617432 and batch: 550, loss is 4.509044904708862 and perplexity is 90.8350209663694
At time: 619.1986677646637 and batch: 600, loss is 4.529733934402466 and perplexity is 92.73388450307097
At time: 620.3967137336731 and batch: 650, loss is 4.549987297058106 and perplexity is 94.6312062125751
At time: 621.5972304344177 and batch: 700, loss is 4.557039976119995 and perplexity is 95.30096877324664
At time: 622.7958726882935 and batch: 750, loss is 4.512277116775513 and perplexity is 91.12909401441875
At time: 623.9927272796631 and batch: 800, loss is 4.534022722244263 and perplexity is 93.13245453968258
At time: 625.1903083324432 and batch: 850, loss is 4.555450010299682 and perplexity is 95.14956388644785
At time: 626.389636516571 and batch: 900, loss is 4.515773305892944 and perplexity is 91.44825616170418
At time: 627.5994734764099 and batch: 950, loss is 4.501700687408447 and perplexity is 90.17035255606851
At time: 628.8044958114624 and batch: 1000, loss is 4.4934272193908695 and perplexity is 89.42740862736673
At time: 630.0081939697266 and batch: 1050, loss is 4.476866207122803 and perplexity is 87.95859629593245
At time: 631.2090554237366 and batch: 1100, loss is 4.4480799961090085 and perplexity is 85.46269766380372
At time: 632.4049990177155 and batch: 1150, loss is 4.468437547683716 and perplexity is 87.22033887382882
At time: 633.6032481193542 and batch: 1200, loss is 4.499088640213013 and perplexity is 89.93513067862487
At time: 634.8072824478149 and batch: 1250, loss is 4.563150177001953 and perplexity is 95.88505947522816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.813131736142792 and perplexity of 123.11658305030637
Finished 20 epochs...
Completing Train Step...
At time: 637.740053653717 and batch: 50, loss is 4.594100341796875 and perplexity is 98.89912010725
At time: 638.9594206809998 and batch: 100, loss is 4.577714185714722 and perplexity is 97.29174897152771
At time: 640.1533560752869 and batch: 150, loss is 4.515724382400513 and perplexity is 91.44378230307527
At time: 641.3467540740967 and batch: 200, loss is 4.58137354850769 and perplexity is 97.64842698688518
At time: 642.5385870933533 and batch: 250, loss is 4.576657743453979 and perplexity is 97.1890201293921
At time: 643.7321834564209 and batch: 300, loss is 4.57692325592041 and perplexity is 97.21482845190071
At time: 644.9251613616943 and batch: 350, loss is 4.571232299804688 and perplexity is 96.66315439472892
At time: 646.1192209720612 and batch: 400, loss is 4.559259719848633 and perplexity is 95.51274746126404
At time: 647.3119866847992 and batch: 450, loss is 4.497877502441407 and perplexity is 89.82627277912913
At time: 648.5303597450256 and batch: 500, loss is 4.508413686752319 and perplexity is 90.77770236222172
At time: 649.7222032546997 and batch: 550, loss is 4.506116409301757 and perplexity is 90.56940014916896
At time: 650.9140057563782 and batch: 600, loss is 4.527013854980469 and perplexity is 92.4819837224914
At time: 652.1072747707367 and batch: 650, loss is 4.5476531982421875 and perplexity is 94.41058520198743
At time: 653.3005938529968 and batch: 700, loss is 4.554887685775757 and perplexity is 95.09607399398234
At time: 654.4940617084503 and batch: 750, loss is 4.510647735595703 and perplexity is 90.98073088656659
At time: 655.6868960857391 and batch: 800, loss is 4.532618360519409 and perplexity is 93.00175468160532
At time: 656.8816096782684 and batch: 850, loss is 4.5546407127380375 and perplexity is 95.07259072769902
At time: 658.0764200687408 and batch: 900, loss is 4.515284423828125 and perplexity is 91.40355967595308
At time: 659.2712752819061 and batch: 950, loss is 4.501509065628052 and perplexity is 90.1530756079455
At time: 660.4694237709045 and batch: 1000, loss is 4.493762216567993 and perplexity is 89.45737157528566
At time: 661.6665847301483 and batch: 1050, loss is 4.477761287689209 and perplexity is 88.03736157150031
At time: 662.8613011837006 and batch: 1100, loss is 4.449129467010498 and perplexity is 85.552435358478
At time: 664.0566239356995 and batch: 1150, loss is 4.469834680557251 and perplexity is 87.34228244235852
At time: 665.2510163784027 and batch: 1200, loss is 4.500713005065918 and perplexity is 90.08133685788223
At time: 666.4455246925354 and batch: 1250, loss is 4.563802738189697 and perplexity is 95.94765076362782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812989172274179 and perplexity of 123.09903232501563
Finished 21 epochs...
Completing Train Step...
At time: 669.3853826522827 and batch: 50, loss is 4.591245708465576 and perplexity is 98.61720196046917
At time: 670.6070554256439 and batch: 100, loss is 4.5746009349823 and perplexity is 96.98932636574787
At time: 671.80180311203 and batch: 150, loss is 4.512774238586426 and perplexity is 91.17440753689903
At time: 672.9973659515381 and batch: 200, loss is 4.578075046539307 and perplexity is 97.32686408774069
At time: 674.1943833827972 and batch: 250, loss is 4.573588600158692 and perplexity is 96.89119037477107
At time: 675.3919999599457 and batch: 300, loss is 4.573956689834595 and perplexity is 96.92686158633357
At time: 676.5937476158142 and batch: 350, loss is 4.568310852050781 and perplexity is 96.38117014112699
At time: 677.8170704841614 and batch: 400, loss is 4.5564638614654545 and perplexity is 95.2460803010892
At time: 679.0128860473633 and batch: 450, loss is 4.495208034515381 and perplexity is 89.5868041940899
At time: 680.2096540927887 and batch: 500, loss is 4.506094751358032 and perplexity is 90.56743862343865
At time: 681.4128851890564 and batch: 550, loss is 4.503987760543823 and perplexity is 90.37681475408495
At time: 682.6116700172424 and batch: 600, loss is 4.525023946762085 and perplexity is 92.2981360436987
At time: 683.8140139579773 and batch: 650, loss is 4.545883064270019 and perplexity is 94.24361364240282
At time: 685.0116879940033 and batch: 700, loss is 4.553288993835449 and perplexity is 94.94416612622919
At time: 686.2083287239075 and batch: 750, loss is 4.509415979385376 and perplexity is 90.86873379699267
At time: 687.4051284790039 and batch: 800, loss is 4.531469440460205 and perplexity is 92.89496445857216
At time: 688.6050956249237 and batch: 850, loss is 4.553941478729248 and perplexity is 95.00613597537759
At time: 689.7999653816223 and batch: 900, loss is 4.514844627380371 and perplexity is 91.36336955347987
At time: 690.9975214004517 and batch: 950, loss is 4.501245555877685 and perplexity is 90.12932252321941
At time: 692.1943781375885 and batch: 1000, loss is 4.493848161697388 and perplexity is 89.46506033106182
At time: 693.3917243480682 and batch: 1050, loss is 4.478204174041748 and perplexity is 88.07636075291906
At time: 694.5886263847351 and batch: 1100, loss is 4.449632091522217 and perplexity is 85.59544691795293
At time: 695.7857477664948 and batch: 1150, loss is 4.470586280822754 and perplexity is 87.40795360117058
At time: 696.9824512004852 and batch: 1200, loss is 4.501594791412353 and perplexity is 90.16080438233207
At time: 698.1781084537506 and batch: 1250, loss is 4.563902883529663 and perplexity is 95.95725995488236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8129348197992705 and perplexity of 123.0923417697757
Finished 22 epochs...
Completing Train Step...
At time: 701.1390600204468 and batch: 50, loss is 4.588838958740235 and perplexity is 98.38014042505999
At time: 702.3342490196228 and batch: 100, loss is 4.572056350708007 and perplexity is 96.74284258347815
At time: 703.5293111801147 and batch: 150, loss is 4.510321569442749 and perplexity is 90.95106089051765
At time: 704.7257030010223 and batch: 200, loss is 4.575395011901856 and perplexity is 97.06637393805906
At time: 705.9199242591858 and batch: 250, loss is 4.571084222793579 and perplexity is 96.64884186344638
At time: 707.1146235466003 and batch: 300, loss is 4.5715618991851805 and perplexity is 96.695019761648
At time: 708.3343250751495 and batch: 350, loss is 4.565965223312378 and perplexity is 96.15536063476992
At time: 709.5290184020996 and batch: 400, loss is 4.554171304702759 and perplexity is 95.02797336237089
At time: 710.730327129364 and batch: 450, loss is 4.493014974594116 and perplexity is 89.39055024133388
At time: 711.9252061843872 and batch: 500, loss is 4.504174070358276 and perplexity is 90.39365441032058
At time: 713.1204371452332 and batch: 550, loss is 4.502212915420532 and perplexity is 90.21655216786485
At time: 714.3214197158813 and batch: 600, loss is 4.523364524841309 and perplexity is 92.14510150305411
At time: 715.5209822654724 and batch: 650, loss is 4.544353551864624 and perplexity is 94.09957704716716
At time: 716.7169284820557 and batch: 700, loss is 4.551960086822509 and perplexity is 94.8180779563089
At time: 717.9129383563995 and batch: 750, loss is 4.5083209419250485 and perplexity is 90.76928359030093
At time: 719.1142435073853 and batch: 800, loss is 4.530445890426636 and perplexity is 92.79993045890589
At time: 720.3094987869263 and batch: 850, loss is 4.553234548568725 and perplexity is 94.93899700649894
At time: 721.5042872428894 and batch: 900, loss is 4.514362134933472 and perplexity is 91.31929805068565
At time: 722.7087590694427 and batch: 950, loss is 4.500879716873169 and perplexity is 90.0963557322265
At time: 723.906412601471 and batch: 1000, loss is 4.493741245269775 and perplexity is 89.45549555773988
At time: 725.1008455753326 and batch: 1050, loss is 4.478378744125366 and perplexity is 88.0917375927095
At time: 726.3011031150818 and batch: 1100, loss is 4.449833068847656 and perplexity is 85.61265139074087
At time: 727.5008728504181 and batch: 1150, loss is 4.470983295440674 and perplexity is 87.44266272603176
At time: 728.6986820697784 and batch: 1200, loss is 4.502057752609253 and perplexity is 90.20255499995459
At time: 729.8931574821472 and batch: 1250, loss is 4.563749418258667 and perplexity is 95.94253497789454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812909425610173 and perplexity of 123.089215979261
Finished 23 epochs...
Completing Train Step...
At time: 732.8333325386047 and batch: 50, loss is 4.586695680618286 and perplexity is 98.16951022263501
At time: 734.0556952953339 and batch: 100, loss is 4.569800491333008 and perplexity is 96.52485030747695
At time: 735.2542455196381 and batch: 150, loss is 4.508181190490722 and perplexity is 90.75659933906822
At time: 736.4488899707794 and batch: 200, loss is 4.573081188201904 and perplexity is 96.84203909729504
At time: 737.6494407653809 and batch: 250, loss is 4.568919887542725 and perplexity is 96.43988757318306
At time: 738.8768277168274 and batch: 300, loss is 4.56946759223938 and perplexity is 96.49272262023268
At time: 740.0707309246063 and batch: 350, loss is 4.563893556594849 and perplexity is 95.9563649719475
At time: 741.2667825222015 and batch: 400, loss is 4.552136917114257 and perplexity is 94.83484614721525
At time: 742.4620773792267 and batch: 450, loss is 4.49106502532959 and perplexity is 89.21641303816767
At time: 743.6563293933868 and batch: 500, loss is 4.502471799850464 and perplexity is 90.23991085201257
At time: 744.8495671749115 and batch: 550, loss is 4.500655488967896 and perplexity is 90.07615587987782
At time: 746.0448100566864 and batch: 600, loss is 4.521845073699951 and perplexity is 92.00519783871263
At time: 747.2397780418396 and batch: 650, loss is 4.5429863929748535 and perplexity is 93.97101587568241
At time: 748.4342319965363 and batch: 700, loss is 4.550752382278443 and perplexity is 94.70363485333924
At time: 749.6328508853912 and batch: 750, loss is 4.507280282974243 and perplexity is 90.67487285608262
At time: 750.8256168365479 and batch: 800, loss is 4.529461755752563 and perplexity is 92.7086477541946
At time: 752.0187983512878 and batch: 850, loss is 4.552487440109253 and perplexity is 94.86809376820416
At time: 753.2120525836945 and batch: 900, loss is 4.5138343238830565 and perplexity is 91.27111143389142
At time: 754.4048664569855 and batch: 950, loss is 4.500443181991577 and perplexity is 90.05703411349859
At time: 755.5981931686401 and batch: 1000, loss is 4.493546600341797 and perplexity is 89.43808519372429
At time: 756.7916796207428 and batch: 1050, loss is 4.478375844955444 and perplexity is 88.09148220016372
At time: 757.9859046936035 and batch: 1100, loss is 4.449834604263305 and perplexity is 85.61278284184647
At time: 759.1793527603149 and batch: 1150, loss is 4.471146688461304 and perplexity is 87.45695141413066
At time: 760.3720941543579 and batch: 1200, loss is 4.502259578704834 and perplexity is 90.22076206671045
At time: 761.5652649402618 and batch: 1250, loss is 4.563430976867676 and perplexity is 95.91198776760714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812907198049726 and perplexity of 123.0889417908974
Finished 24 epochs...
Completing Train Step...
At time: 764.5020525455475 and batch: 50, loss is 4.5847280406951905 and perplexity is 97.97653788738508
At time: 765.6981823444366 and batch: 100, loss is 4.5677316284179685 and perplexity is 96.32536005444403
At time: 766.8944642543793 and batch: 150, loss is 4.506249923706054 and perplexity is 90.58149327596274
At time: 768.0921838283539 and batch: 200, loss is 4.570984525680542 and perplexity is 96.63920673323952
At time: 769.3132584095001 and batch: 250, loss is 4.566919050216675 and perplexity is 96.24711995903506
At time: 770.5102231502533 and batch: 300, loss is 4.567544984817505 and perplexity is 96.30738322011022
At time: 771.7054133415222 and batch: 350, loss is 4.561992797851563 and perplexity is 95.77414830207798
At time: 772.9022657871246 and batch: 400, loss is 4.550282688140869 and perplexity is 94.65916355601308
At time: 774.0992512702942 and batch: 450, loss is 4.48914080619812 and perplexity is 89.044906170675
At time: 775.2950735092163 and batch: 500, loss is 4.500853242874146 and perplexity is 90.09397055296562
At time: 776.4955139160156 and batch: 550, loss is 4.499201745986938 and perplexity is 89.94530343647135
At time: 777.6916000843048 and batch: 600, loss is 4.52040225982666 and perplexity is 91.87254718097245
At time: 778.8879506587982 and batch: 650, loss is 4.54171950340271 and perplexity is 93.85204035592153
At time: 780.0858578681946 and batch: 700, loss is 4.549583692550659 and perplexity is 94.5930203377165
At time: 781.2813878059387 and batch: 750, loss is 4.506310253143311 and perplexity is 90.5869581713233
At time: 782.482551574707 and batch: 800, loss is 4.528476543426514 and perplexity is 92.61735503043857
At time: 783.677907705307 and batch: 850, loss is 4.5517027473449705 and perplexity is 94.79368066099462
At time: 784.8751566410065 and batch: 900, loss is 4.513261089324951 and perplexity is 91.21880667154176
At time: 786.0792419910431 and batch: 950, loss is 4.499923791885376 and perplexity is 90.01027152605009
At time: 787.2775783538818 and batch: 1000, loss is 4.493362703323364 and perplexity is 89.42163930874379
At time: 788.4739108085632 and batch: 1050, loss is 4.478228940963745 and perplexity is 88.07854216028885
At time: 789.6755256652832 and batch: 1100, loss is 4.449664077758789 and perplexity is 85.5981848379552
At time: 790.8738825321198 and batch: 1150, loss is 4.471153240203858 and perplexity is 87.45752441143793
At time: 792.0772383213043 and batch: 1200, loss is 4.502315311431885 and perplexity is 90.22579045593868
At time: 793.2748339176178 and batch: 1250, loss is 4.563008956909179 and perplexity is 95.87151953431183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812886704493613 and perplexity of 123.0864192866096
Finished 25 epochs...
Completing Train Step...
At time: 796.188244342804 and batch: 50, loss is 4.582892503738403 and perplexity is 97.7968632813311
At time: 797.410502910614 and batch: 100, loss is 4.565747060775757 and perplexity is 96.13438542546976
At time: 798.6033554077148 and batch: 150, loss is 4.504432735443115 and perplexity is 90.41703911688064
At time: 799.8219485282898 and batch: 200, loss is 4.569000663757325 and perplexity is 96.44767793687146
At time: 801.0161664485931 and batch: 250, loss is 4.565009002685547 and perplexity is 96.06345884174823
At time: 802.2179763317108 and batch: 300, loss is 4.565661249160766 and perplexity is 96.12613633253932
At time: 803.4133102893829 and batch: 350, loss is 4.560165815353393 and perplexity is 95.59933035264655
At time: 804.6143374443054 and batch: 400, loss is 4.548534498214722 and perplexity is 94.49382592276768
At time: 805.8147392272949 and batch: 450, loss is 4.487356443405151 and perplexity is 88.88615942621297
At time: 807.0107336044312 and batch: 500, loss is 4.499291543960571 and perplexity is 89.95338070511349
At time: 808.2069509029388 and batch: 550, loss is 4.497811937332154 and perplexity is 89.82038350280821
At time: 809.4014627933502 and batch: 600, loss is 4.519059801101685 and perplexity is 91.74929482753265
At time: 810.5973625183105 and batch: 650, loss is 4.54044921875 and perplexity is 93.73289723828977
At time: 811.7916605472565 and batch: 700, loss is 4.548426837921142 and perplexity is 94.48365323733435
At time: 812.9912095069885 and batch: 750, loss is 4.50531943321228 and perplexity is 90.49724725869515
At time: 814.1867220401764 and batch: 800, loss is 4.527507495880127 and perplexity is 92.52764788205948
At time: 815.3820648193359 and batch: 850, loss is 4.55092490196228 and perplexity is 94.71997450389753
At time: 816.5765156745911 and batch: 900, loss is 4.512662029266357 and perplexity is 91.16417749258555
At time: 817.7774419784546 and batch: 950, loss is 4.499376258850098 and perplexity is 89.96100141861184
At time: 818.9819157123566 and batch: 1000, loss is 4.493006792068481 and perplexity is 89.38981880385754
At time: 820.1849267482758 and batch: 1050, loss is 4.47797924041748 and perplexity is 88.05655164583321
At time: 821.3803050518036 and batch: 1100, loss is 4.44937159538269 and perplexity is 85.57315253839572
At time: 822.5772037506104 and batch: 1150, loss is 4.471034526824951 and perplexity is 87.44714264944345
At time: 823.7735540866852 and batch: 1200, loss is 4.5022779560089115 and perplexity is 90.22242009632407
At time: 824.9689168930054 and batch: 1250, loss is 4.5625529384613035 and perplexity is 95.82781031963985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812847053917655 and perplexity of 123.0815389359472
Finished 26 epochs...
Completing Train Step...
At time: 827.9129173755646 and batch: 50, loss is 4.581146659851075 and perplexity is 97.62627417967089
At time: 829.1394762992859 and batch: 100, loss is 4.5638489913940425 and perplexity is 95.95208875255989
At time: 830.3417868614197 and batch: 150, loss is 4.502710018157959 and perplexity is 90.2614102115131
At time: 831.5426623821259 and batch: 200, loss is 4.567171840667725 and perplexity is 96.27145338740137
At time: 832.7411892414093 and batch: 250, loss is 4.563235979080201 and perplexity is 95.89328696556687
At time: 833.9404640197754 and batch: 300, loss is 4.563890647888184 and perplexity is 95.95608586343509
At time: 835.1416494846344 and batch: 350, loss is 4.558465566635132 and perplexity is 95.43692581692352
At time: 836.3457412719727 and batch: 400, loss is 4.546954698562622 and perplexity is 94.34466246466205
At time: 837.5477573871613 and batch: 450, loss is 4.485710725784302 and perplexity is 88.73999821042824
At time: 838.746013879776 and batch: 500, loss is 4.497837514877319 and perplexity is 89.822680917105
At time: 839.9448392391205 and batch: 550, loss is 4.4965294742584225 and perplexity is 89.70526601044196
At time: 841.1455204486847 and batch: 600, loss is 4.517760801315307 and perplexity is 91.6301898885447
At time: 842.3468811511993 and batch: 650, loss is 4.539283256530762 and perplexity is 93.62367191008727
At time: 843.5443258285522 and batch: 700, loss is 4.547331895828247 and perplexity is 94.38025572578836
At time: 844.7429358959198 and batch: 750, loss is 4.5043074321746825 and perplexity is 90.40571027614254
At time: 845.9442522525787 and batch: 800, loss is 4.5264660835266115 and perplexity is 92.43133860405356
At time: 847.1424489021301 and batch: 850, loss is 4.550117721557617 and perplexity is 94.64354924518297
At time: 848.3408164978027 and batch: 900, loss is 4.512025804519653 and perplexity is 91.1061950337453
At time: 849.5444102287292 and batch: 950, loss is 4.498767910003662 and perplexity is 89.90629039055734
At time: 850.7409706115723 and batch: 1000, loss is 4.492517471313477 and perplexity is 89.34608921000216
At time: 851.9376735687256 and batch: 1050, loss is 4.477603626251221 and perplexity is 88.02348256859884
At time: 853.1374685764313 and batch: 1100, loss is 4.448984146118164 and perplexity is 85.54000370554168
At time: 854.3394930362701 and batch: 1150, loss is 4.470699968338013 and perplexity is 87.41789135911814
At time: 855.5367600917816 and batch: 1200, loss is 4.502063465118408 and perplexity is 90.20307028434765
At time: 856.7344992160797 and batch: 1250, loss is 4.561993560791016 and perplexity is 95.77422137198216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812720082972172 and perplexity of 123.06591214867197
Finished 27 epochs...
Completing Train Step...
At time: 859.6652941703796 and batch: 50, loss is 4.579468030929565 and perplexity is 97.46253336081973
At time: 860.8587760925293 and batch: 100, loss is 4.562046775817871 and perplexity is 95.77931813535551
At time: 862.0544755458832 and batch: 150, loss is 4.501067953109741 and perplexity is 90.11331672744528
At time: 863.2484719753265 and batch: 200, loss is 4.565381937026977 and perplexity is 96.09929088559146
At time: 864.4434170722961 and batch: 250, loss is 4.561519384384155 and perplexity is 95.72881826121483
At time: 865.6432330608368 and batch: 300, loss is 4.562233324050903 and perplexity is 95.79718726459015
At time: 866.8370048999786 and batch: 350, loss is 4.556775512695313 and perplexity is 95.27576848509345
At time: 868.0301773548126 and batch: 400, loss is 4.545378408432007 and perplexity is 94.19606505142892
At time: 869.2241370677948 and batch: 450, loss is 4.484107789993286 and perplexity is 88.59786763472798
At time: 870.4182636737823 and batch: 500, loss is 4.496463994979859 and perplexity is 89.69939236664331
At time: 871.6115820407867 and batch: 550, loss is 4.495214948654175 and perplexity is 89.58742361182956
At time: 872.8045732975006 and batch: 600, loss is 4.51652512550354 and perplexity is 91.517034605293
At time: 873.9994456768036 and batch: 650, loss is 4.538089866638184 and perplexity is 93.51200900826524
At time: 875.1951897144318 and batch: 700, loss is 4.5462025451660155 and perplexity is 94.27372748667403
At time: 876.38831782341 and batch: 750, loss is 4.503254728317261 and perplexity is 90.31058991177339
At time: 877.5827934741974 and batch: 800, loss is 4.525426511764526 and perplexity is 92.33529952291666
At time: 878.7827677726746 and batch: 850, loss is 4.549334011077881 and perplexity is 94.56940516134276
At time: 879.9825212955475 and batch: 900, loss is 4.51135947227478 and perplexity is 91.04550825930933
At time: 881.1770133972168 and batch: 950, loss is 4.498125610351562 and perplexity is 89.84856215291605
At time: 882.3752069473267 and batch: 1000, loss is 4.491970834732055 and perplexity is 89.29726271562286
At time: 883.5807566642761 and batch: 1050, loss is 4.4772016716003415 and perplexity is 87.98810823030998
At time: 884.7791841030121 and batch: 1100, loss is 4.448554372787475 and perplexity is 85.50324879194856
At time: 885.9752831459045 and batch: 1150, loss is 4.470335788726807 and perplexity is 87.38606134170399
At time: 887.1707537174225 and batch: 1200, loss is 4.501750202178955 and perplexity is 90.17481743091965
At time: 888.3659329414368 and batch: 1250, loss is 4.5613822174072265 and perplexity is 95.7156883291261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812675086251141 and perplexity of 123.0603747107388
Finished 28 epochs...
Completing Train Step...
At time: 891.3241949081421 and batch: 50, loss is 4.577836542129517 and perplexity is 97.30365396943256
At time: 892.5428876876831 and batch: 100, loss is 4.560328454971313 and perplexity is 95.61487985565684
At time: 893.7439131736755 and batch: 150, loss is 4.499510192871094 and perplexity is 89.97305106417544
At time: 894.9382607936859 and batch: 200, loss is 4.563738498687744 and perplexity is 95.94148733229925
At time: 896.1403534412384 and batch: 250, loss is 4.559946489334107 and perplexity is 95.57836523125619
At time: 897.3372340202332 and batch: 300, loss is 4.560673818588257 and perplexity is 95.6479074593355
At time: 898.5334131717682 and batch: 350, loss is 4.55522762298584 and perplexity is 95.12840618321148
At time: 899.7294585704803 and batch: 400, loss is 4.5438470458984375 and perplexity is 94.0519271184824
At time: 900.9246690273285 and batch: 450, loss is 4.48264142036438 and perplexity is 88.4680456192353
At time: 902.124710559845 and batch: 500, loss is 4.495150604248047 and perplexity is 89.58165934771182
At time: 903.3267607688904 and batch: 550, loss is 4.4939982700347905 and perplexity is 89.4784907905105
At time: 904.5250387191772 and batch: 600, loss is 4.51536771774292 and perplexity is 91.41117335334675
At time: 905.7247445583344 and batch: 650, loss is 4.536889057159424 and perplexity is 93.39978629400468
At time: 906.9232523441315 and batch: 700, loss is 4.54505241394043 and perplexity is 94.16536265777141
At time: 908.1264729499817 and batch: 750, loss is 4.502240743637085 and perplexity is 90.2190627685478
At time: 909.324649810791 and batch: 800, loss is 4.524392862319946 and perplexity is 92.23990650181467
At time: 910.5216171741486 and batch: 850, loss is 4.548495359420777 and perplexity is 94.49012762075981
At time: 911.7199170589447 and batch: 900, loss is 4.5106312084198 and perplexity is 90.97922724444892
At time: 912.9199419021606 and batch: 950, loss is 4.497395057678222 and perplexity is 89.78294701621452
At time: 914.1253392696381 and batch: 1000, loss is 4.491394004821777 and perplexity is 89.24576823678817
At time: 915.3227560520172 and batch: 1050, loss is 4.476740961074829 and perplexity is 87.94758051921671
At time: 916.5267312526703 and batch: 1100, loss is 4.44800446510315 and perplexity is 85.45624282405912
At time: 917.7223036289215 and batch: 1150, loss is 4.469787845611572 and perplexity is 87.33819186709654
At time: 918.9178411960602 and batch: 1200, loss is 4.501268491744995 and perplexity is 90.13138974110821
At time: 920.1413679122925 and batch: 1250, loss is 4.560686922073364 and perplexity is 95.64916078847786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812538314039689 and perplexity of 123.04354462211802
Finished 29 epochs...
Completing Train Step...
At time: 923.1213455200195 and batch: 50, loss is 4.576129589080811 and perplexity is 97.13770287629552
At time: 924.3154697418213 and batch: 100, loss is 4.558514909744263 and perplexity is 95.44163508775331
At time: 925.5110051631927 and batch: 150, loss is 4.497839288711548 and perplexity is 89.82284024779226
At time: 926.7059869766235 and batch: 200, loss is 4.5617850875854495 and perplexity is 95.75425709412296
At time: 927.9010820388794 and batch: 250, loss is 4.558308362960815 and perplexity is 95.421923960724
At time: 929.0962834358215 and batch: 300, loss is 4.558673028945923 and perplexity is 95.4567274360621
At time: 930.2956132888794 and batch: 350, loss is 4.55333083152771 and perplexity is 94.94813845412948
At time: 931.4928789138794 and batch: 400, loss is 4.541798639297485 and perplexity is 93.85946771499302
At time: 932.6869924068451 and batch: 450, loss is 4.480638208389283 and perplexity is 88.29100275723202
At time: 933.8868124485016 and batch: 500, loss is 4.493347940444946 and perplexity is 89.42031919769904
At time: 935.0910196304321 and batch: 550, loss is 4.492468566894531 and perplexity is 89.34171989826449
At time: 936.2905128002167 and batch: 600, loss is 4.513791618347168 and perplexity is 91.26721373539371
At time: 937.4916136264801 and batch: 650, loss is 4.535376310348511 and perplexity is 93.25860287945218
At time: 938.6858055591583 and batch: 700, loss is 4.5434805393219 and perplexity is 94.0174627847457
At time: 939.8799021244049 and batch: 750, loss is 4.500914535522461 and perplexity is 90.09949282025347
At time: 941.0739960670471 and batch: 800, loss is 4.522917127609253 and perplexity is 92.10388526039802
At time: 942.2674441337585 and batch: 850, loss is 4.54740797996521 and perplexity is 94.38743683927257
At time: 943.4735350608826 and batch: 900, loss is 4.50968843460083 and perplexity is 90.8934948304205
At time: 944.6688647270203 and batch: 950, loss is 4.496544523239136 and perplexity is 89.70661599341793
At time: 945.8629579544067 and batch: 1000, loss is 4.4904594993591305 and perplexity is 89.16240653590185
At time: 947.0568506717682 and batch: 1050, loss is 4.47608380317688 and perplexity is 87.88980405828106
At time: 948.2511293888092 and batch: 1100, loss is 4.447210264205933 and perplexity is 85.38840034318103
At time: 949.4458186626434 and batch: 1150, loss is 4.469203662872315 and perplexity is 87.28718530295758
At time: 950.6716585159302 and batch: 1200, loss is 4.500634889602662 and perplexity is 90.07430038735501
At time: 951.8662683963776 and batch: 1250, loss is 4.559787731170655 and perplexity is 95.56319258994853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812338279111542 and perplexity of 123.01893407707693
Finished 30 epochs...
Completing Train Step...
At time: 954.7788870334625 and batch: 50, loss is 4.574399337768555 and perplexity is 96.96977555854988
At time: 955.9976577758789 and batch: 100, loss is 4.556746520996094 and perplexity is 95.27300631871081
At time: 957.1963710784912 and batch: 150, loss is 4.496213407516479 and perplexity is 89.6769176395035
At time: 958.3912129402161 and batch: 200, loss is 4.560057773590088 and perplexity is 95.58900219037088
At time: 959.584329366684 and batch: 250, loss is 4.556784648895263 and perplexity is 95.27663894754112
At time: 960.7799649238586 and batch: 300, loss is 4.557195978164673 and perplexity is 95.31583707894792
At time: 961.973405122757 and batch: 350, loss is 4.551732149124145 and perplexity is 94.7964678048339
At time: 963.1672122478485 and batch: 400, loss is 4.540223398208618 and perplexity is 93.71173281446089
At time: 964.3671073913574 and batch: 450, loss is 4.479184007644653 and perplexity is 88.16270322452593
At time: 965.562445640564 and batch: 500, loss is 4.491979484558105 and perplexity is 89.2980351247527
At time: 966.7565340995789 and batch: 550, loss is 4.491311531066895 and perplexity is 89.23840810668727
At time: 967.9501295089722 and batch: 600, loss is 4.512581958770752 and perplexity is 91.15687822394419
At time: 969.1438364982605 and batch: 650, loss is 4.534289569854736 and perplexity is 93.15731002880045
At time: 970.3376870155334 and batch: 700, loss is 4.5423823261260985 and perplexity is 93.91426824165589
At time: 971.5306253433228 and batch: 750, loss is 4.49991135597229 and perplexity is 90.00915217309664
At time: 972.7239165306091 and batch: 800, loss is 4.5218993663787845 and perplexity is 92.01019318297399
At time: 973.9180262088776 and batch: 850, loss is 4.546515455245972 and perplexity is 94.30323130205956
At time: 975.1117641925812 and batch: 900, loss is 4.508957824707031 and perplexity is 90.82711139696501
At time: 976.3059527873993 and batch: 950, loss is 4.495829057693482 and perplexity is 89.64245695498501
At time: 977.4992828369141 and batch: 1000, loss is 4.48979944229126 and perplexity is 89.10357367793736
At time: 978.698493719101 and batch: 1050, loss is 4.475576276779175 and perplexity is 87.8452089801793
At time: 979.9209794998169 and batch: 1100, loss is 4.4466252040863035 and perplexity is 85.3384576066376
At time: 981.1156415939331 and batch: 1150, loss is 4.468629312515259 and perplexity is 87.2370662712321
At time: 982.3109004497528 and batch: 1200, loss is 4.500054454803466 and perplexity is 90.02203329918287
At time: 983.508978843689 and batch: 1250, loss is 4.559088764190673 and perplexity is 95.49642041231873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812276798443203 and perplexity of 123.01137102328467
Finished 31 epochs...
Completing Train Step...
At time: 986.4652140140533 and batch: 50, loss is 4.572819757461548 and perplexity is 96.8167249204124
At time: 987.6887741088867 and batch: 100, loss is 4.555239171981811 and perplexity is 95.12950482713534
At time: 988.8912301063538 and batch: 150, loss is 4.494628639221191 and perplexity is 89.53491305551076
At time: 990.0892674922943 and batch: 200, loss is 4.5585136985778805 and perplexity is 95.44151949212342
At time: 991.288654088974 and batch: 250, loss is 4.555353956222534 and perplexity is 95.14042482182693
At time: 992.489456653595 and batch: 300, loss is 4.555737504959106 and perplexity is 95.17692281049554
At time: 993.6934208869934 and batch: 350, loss is 4.550305290222168 and perplexity is 94.66130307430215
At time: 994.8901126384735 and batch: 400, loss is 4.5387340259552005 and perplexity is 93.57226504528113
At time: 996.0938076972961 and batch: 450, loss is 4.4777851581573485 and perplexity is 88.03946308961685
At time: 997.2908940315247 and batch: 500, loss is 4.490719480514526 and perplexity is 89.18559009488457
At time: 998.4954349994659 and batch: 550, loss is 4.490164022445679 and perplexity is 89.13606499507297
At time: 999.6978964805603 and batch: 600, loss is 4.51148681640625 and perplexity is 91.0571031087352
At time: 1000.895593881607 and batch: 650, loss is 4.5332345771789555 and perplexity is 93.05908157328598
At time: 1002.0934748649597 and batch: 700, loss is 4.541360788345337 and perplexity is 93.81838025343436
At time: 1003.2903478145599 and batch: 750, loss is 4.49890944480896 and perplexity is 89.91901616041127
At time: 1004.4856984615326 and batch: 800, loss is 4.520890130996704 and perplexity is 91.91738008351155
At time: 1005.6921675205231 and batch: 850, loss is 4.545614395141602 and perplexity is 94.2182966939601
At time: 1006.8944644927979 and batch: 900, loss is 4.508205709457397 and perplexity is 90.75882462438366
At time: 1008.0951335430145 and batch: 950, loss is 4.495090398788452 and perplexity is 89.57626620508948
At time: 1009.2930424213409 and batch: 1000, loss is 4.489128112792969 and perplexity is 89.0437758947779
At time: 1010.5149624347687 and batch: 1050, loss is 4.4750284004211425 and perplexity is 87.79709384878767
At time: 1011.716641664505 and batch: 1100, loss is 4.446035127639771 and perplexity is 85.28811624690654
At time: 1012.9164402484894 and batch: 1150, loss is 4.468044586181641 and perplexity is 87.18607137179896
At time: 1014.113254070282 and batch: 1200, loss is 4.499486818313598 and perplexity is 89.97094800849936
At time: 1015.3107318878174 and batch: 1250, loss is 4.5583718967437745 and perplexity is 95.42798666912182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812266106153056 and perplexity of 123.01005575704596
Finished 32 epochs...
Completing Train Step...
At time: 1018.2741374969482 and batch: 50, loss is 4.571295719146729 and perplexity is 96.66928490277458
At time: 1019.4702324867249 and batch: 100, loss is 4.553751592636108 and perplexity is 94.98809734408971
At time: 1020.6663291454315 and batch: 150, loss is 4.493160142898559 and perplexity is 89.40352785789216
At time: 1021.8625745773315 and batch: 200, loss is 4.557067699432373 and perplexity is 95.30361086839753
At time: 1023.0667912960052 and batch: 250, loss is 4.55395956993103 and perplexity is 95.00785476610149
At time: 1024.2690875530243 and batch: 300, loss is 4.554297094345093 and perplexity is 95.03992764899877
At time: 1025.4720711708069 and batch: 350, loss is 4.548878202438354 and perplexity is 94.52630943188437
At time: 1026.666799068451 and batch: 400, loss is 4.537272138595581 and perplexity is 93.4355728724242
At time: 1027.8621153831482 and batch: 450, loss is 4.476456604003906 and perplexity is 87.92257555816276
At time: 1029.0573761463165 and batch: 500, loss is 4.489537830352783 and perplexity is 89.0802661681966
At time: 1030.2526032924652 and batch: 550, loss is 4.489051904678345 and perplexity is 89.0369902950601
At time: 1031.4456131458282 and batch: 600, loss is 4.5103957462310795 and perplexity is 90.95780759833121
At time: 1032.6407017707825 and batch: 650, loss is 4.532199964523316 and perplexity is 92.96285125890253
At time: 1033.8395473957062 and batch: 700, loss is 4.540381383895874 and perplexity is 93.72653909653306
At time: 1035.0430052280426 and batch: 750, loss is 4.497929973602295 and perplexity is 89.83098619159814
At time: 1036.2368476390839 and batch: 800, loss is 4.5199046802520755 and perplexity is 91.82684464929618
At time: 1037.4303109645844 and batch: 850, loss is 4.544753522872925 and perplexity is 94.1372216777579
At time: 1038.625153541565 and batch: 900, loss is 4.507431802749633 and perplexity is 90.68861293337176
At time: 1039.8197898864746 and batch: 950, loss is 4.494401321411133 and perplexity is 89.51456248826207
At time: 1041.044843196869 and batch: 1000, loss is 4.488450126647949 and perplexity is 88.98342590895986
At time: 1042.2422223091125 and batch: 1050, loss is 4.474418106079102 and perplexity is 87.74352812625543
At time: 1043.4364831447601 and batch: 1100, loss is 4.445399646759033 and perplexity is 85.23393449723386
At time: 1044.630923986435 and batch: 1150, loss is 4.467407312393188 and perplexity is 87.13052767394672
At time: 1045.8264956474304 and batch: 1200, loss is 4.4988814735412594 and perplexity is 89.91650104671453
At time: 1047.0197463035583 and batch: 1250, loss is 4.557674608230591 and perplexity is 95.36146902387131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812236702355155 and perplexity of 123.00643884740235
Finished 33 epochs...
Completing Train Step...
At time: 1049.937945842743 and batch: 50, loss is 4.569813709259034 and perplexity is 96.52612617424009
At time: 1051.1604063510895 and batch: 100, loss is 4.552231874465942 and perplexity is 94.84385184062445
At time: 1052.3591532707214 and batch: 150, loss is 4.491728572845459 and perplexity is 89.27563201254277
At time: 1053.5556087493896 and batch: 200, loss is 4.555676822662353 and perplexity is 95.17114743145494
At time: 1054.7526226043701 and batch: 250, loss is 4.552602481842041 and perplexity is 94.87900818589227
At time: 1055.9505484104156 and batch: 300, loss is 4.552934103012085 and perplexity is 94.91047729122181
At time: 1057.1482427120209 and batch: 350, loss is 4.54753927230835 and perplexity is 94.39983000056392
At time: 1058.3468520641327 and batch: 400, loss is 4.535881271362305 and perplexity is 93.30570672991037
At time: 1059.5434005260468 and batch: 450, loss is 4.475190715789795 and perplexity is 87.8113458230683
At time: 1060.74032330513 and batch: 500, loss is 4.488364315032959 and perplexity is 88.9757904250865
At time: 1061.9379708766937 and batch: 550, loss is 4.487924356460571 and perplexity is 88.93665337332601
At time: 1063.1367738246918 and batch: 600, loss is 4.509354782104492 and perplexity is 90.86317304772005
At time: 1064.3393516540527 and batch: 650, loss is 4.531155204772949 and perplexity is 92.86577813150585
At time: 1065.5359954833984 and batch: 700, loss is 4.539417638778686 and perplexity is 93.63625411497101
At time: 1066.73349070549 and batch: 750, loss is 4.496968002319336 and perplexity is 89.74461291352921
At time: 1067.931274175644 and batch: 800, loss is 4.518924322128296 and perplexity is 91.73686556923083
At time: 1069.1284308433533 and batch: 850, loss is 4.543919954299927 and perplexity is 94.05878454412453
At time: 1070.3266491889954 and batch: 900, loss is 4.506641082763672 and perplexity is 90.61693197814294
At time: 1071.5499908924103 and batch: 950, loss is 4.493672313690186 and perplexity is 89.44932946164997
At time: 1072.754281282425 and batch: 1000, loss is 4.487752656936646 and perplexity is 88.92138430316588
At time: 1073.9520704746246 and batch: 1050, loss is 4.473783864974975 and perplexity is 87.68789521830006
At time: 1075.1509788036346 and batch: 1100, loss is 4.444808483123779 and perplexity is 85.183562185276
At time: 1076.3490002155304 and batch: 1150, loss is 4.466839036941528 and perplexity is 87.08102760014994
At time: 1077.5478987693787 and batch: 1200, loss is 4.498132286071777 and perplexity is 89.84916195878074
At time: 1078.7461822032928 and batch: 1250, loss is 4.556911602020263 and perplexity is 95.28873538241886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.812045132156706 and perplexity of 122.9828767364731
Finished 34 epochs...
Completing Train Step...
At time: 1081.6804852485657 and batch: 50, loss is 4.56837064743042 and perplexity is 96.38693346209388
At time: 1082.8734312057495 and batch: 100, loss is 4.55074402809143 and perplexity is 94.7028436847677
At time: 1084.0737645626068 and batch: 150, loss is 4.490281467437744 and perplexity is 89.14653419428448
At time: 1085.2710044384003 and batch: 200, loss is 4.554197921752929 and perplexity is 95.03050276036785
At time: 1086.4657821655273 and batch: 250, loss is 4.551345453262329 and perplexity is 94.75981748973828
At time: 1087.661544561386 and batch: 300, loss is 4.551463899612426 and perplexity is 94.77104210900026
At time: 1088.856993675232 and batch: 350, loss is 4.546217594146729 and perplexity is 94.27514622085597
At time: 1090.0519778728485 and batch: 400, loss is 4.534502582550049 and perplexity is 93.17715583212677
At time: 1091.246735572815 and batch: 450, loss is 4.473941507339478 and perplexity is 87.70171963506878
At time: 1092.4416773319244 and batch: 500, loss is 4.487228670120239 and perplexity is 88.87480287518727
At time: 1093.641138792038 and batch: 550, loss is 4.4867324542999265 and perplexity is 88.8307127319914
At time: 1094.8356602191925 and batch: 600, loss is 4.5083669090270995 and perplexity is 90.77345608712088
At time: 1096.0299994945526 and batch: 650, loss is 4.530101137161255 and perplexity is 92.7679428940786
At time: 1097.2244055271149 and batch: 700, loss is 4.538568792343139 and perplexity is 93.55680503923044
At time: 1098.419010400772 and batch: 750, loss is 4.495999097824097 and perplexity is 89.65770106609287
At time: 1099.6194710731506 and batch: 800, loss is 4.517941837310791 and perplexity is 91.6467797528235
At time: 1100.8423063755035 and batch: 850, loss is 4.543037443161011 and perplexity is 93.9758132359883
At time: 1102.0431892871857 and batch: 900, loss is 4.505795488357544 and perplexity is 90.54033919514004
At time: 1103.2440683841705 and batch: 950, loss is 4.492808198928833 and perplexity is 89.372068361705
At time: 1104.4381222724915 and batch: 1000, loss is 4.487061223983765 and perplexity is 88.85992237869145
At time: 1105.6327958106995 and batch: 1050, loss is 4.473031120300293 and perplexity is 87.62191345896495
At time: 1106.8279044628143 and batch: 1100, loss is 4.444105758666992 and perplexity is 85.1237226406304
At time: 1108.0230777263641 and batch: 1150, loss is 4.466340379714966 and perplexity is 87.03761484138316
At time: 1109.217811346054 and batch: 1200, loss is 4.497416954040528 and perplexity is 89.78491295767465
At time: 1110.4119482040405 and batch: 1250, loss is 4.556099100112915 and perplexity is 95.21134454752983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811878065123175 and perplexity of 122.9623320682978
Finished 35 epochs...
Completing Train Step...
At time: 1113.3241801261902 and batch: 50, loss is 4.566972370147705 and perplexity is 96.2522519856516
At time: 1114.5453126430511 and batch: 100, loss is 4.549311351776123 and perplexity is 94.56726230893203
At time: 1115.741177558899 and batch: 150, loss is 4.488863134384156 and perplexity is 89.02018434249061
At time: 1116.9370775222778 and batch: 200, loss is 4.552797031402588 and perplexity is 94.89746865091941
At time: 1118.138323545456 and batch: 250, loss is 4.550147247314453 and perplexity is 94.64634370885818
At time: 1119.334254026413 and batch: 300, loss is 4.550191106796265 and perplexity is 94.65049493948338
At time: 1120.5303370952606 and batch: 350, loss is 4.544929065704346 and perplexity is 94.1537482427109
At time: 1121.7334532737732 and batch: 400, loss is 4.53323899269104 and perplexity is 93.05949247769244
At time: 1122.9289166927338 and batch: 450, loss is 4.47274154663086 and perplexity is 87.59654413328352
At time: 1124.1247351169586 and batch: 500, loss is 4.486087675094605 and perplexity is 88.77345499691005
At time: 1125.3200612068176 and batch: 550, loss is 4.48560094833374 and perplexity is 88.73025709434906
At time: 1126.5167100429535 and batch: 600, loss is 4.507376432418823 and perplexity is 90.68359161389027
At time: 1127.7132275104523 and batch: 650, loss is 4.5290947437286375 and perplexity is 92.67462880880979
At time: 1128.910742521286 and batch: 700, loss is 4.53764554977417 and perplexity is 93.47046927476048
At time: 1130.1127326488495 and batch: 750, loss is 4.4950479221343995 and perplexity is 89.57246138582711
At time: 1131.3354172706604 and batch: 800, loss is 4.516970682144165 and perplexity is 91.55781971315481
At time: 1132.5377337932587 and batch: 850, loss is 4.542225732803344 and perplexity is 93.8995630457357
At time: 1133.7354717254639 and batch: 900, loss is 4.505042543411255 and perplexity is 90.47219296270795
At time: 1134.9349341392517 and batch: 950, loss is 4.4920808124542235 and perplexity is 89.30708396522161
At time: 1136.134776353836 and batch: 1000, loss is 4.486402111053467 and perplexity is 88.80137295232804
At time: 1137.3317914009094 and batch: 1050, loss is 4.4723758888244625 and perplexity is 87.56451962846967
At time: 1138.5296375751495 and batch: 1100, loss is 4.443492593765259 and perplexity is 85.07154376036614
At time: 1139.7269833087921 and batch: 1150, loss is 4.46583550453186 and perplexity is 86.99368280068477
At time: 1140.9275212287903 and batch: 1200, loss is 4.496805429458618 and perplexity is 89.73002406098175
At time: 1142.1251938343048 and batch: 1250, loss is 4.555351343154907 and perplexity is 95.1401762137876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811853561958257 and perplexity of 122.95931913890968
Finished 36 epochs...
Completing Train Step...
At time: 1145.0709567070007 and batch: 50, loss is 4.5655965232849125 and perplexity is 96.11991468552571
At time: 1146.2992084026337 and batch: 100, loss is 4.547929382324218 and perplexity is 94.4366635038356
At time: 1147.4949276447296 and batch: 150, loss is 4.487567367553711 and perplexity is 88.90490964107578
At time: 1148.690076828003 and batch: 200, loss is 4.551514959335327 and perplexity is 94.77588121569
At time: 1149.8854448795319 and batch: 250, loss is 4.548930244445801 and perplexity is 94.53122889879208
At time: 1151.0844600200653 and batch: 300, loss is 4.548954906463623 and perplexity is 94.53356025839187
At time: 1152.2795910835266 and batch: 350, loss is 4.5436918163299564 and perplexity is 94.03732861151026
At time: 1153.4749944210052 and batch: 400, loss is 4.532020854949951 and perplexity is 92.94620221332102
At time: 1154.675518989563 and batch: 450, loss is 4.471545915603638 and perplexity is 87.49187357339406
At time: 1155.8715469837189 and batch: 500, loss is 4.484952373504639 and perplexity is 88.67272754114467
At time: 1157.0704703330994 and batch: 550, loss is 4.484518508911133 and perplexity is 88.63426392885813
At time: 1158.2737340927124 and batch: 600, loss is 4.5063411903381345 and perplexity is 90.58976072104804
At time: 1159.4679288864136 and batch: 650, loss is 4.5280624103546145 and perplexity is 92.57900706181772
At time: 1160.6619334220886 and batch: 700, loss is 4.536685094833374 and perplexity is 93.38073819895251
At time: 1161.8872003555298 and batch: 750, loss is 4.49410382270813 and perplexity is 89.48793598289372
At time: 1163.0846543312073 and batch: 800, loss is 4.516010332107544 and perplexity is 91.46993436474364
At time: 1164.2780168056488 and batch: 850, loss is 4.541419296264649 and perplexity is 93.82386953223778
At time: 1165.4730067253113 and batch: 900, loss is 4.5043089389801025 and perplexity is 90.40584650005941
At time: 1166.6671454906464 and batch: 950, loss is 4.491368894577026 and perplexity is 89.24352728184033
At time: 1167.8630638122559 and batch: 1000, loss is 4.485747137069702 and perplexity is 88.74322940665513
At time: 1169.0574612617493 and batch: 1050, loss is 4.471735048294067 and perplexity is 87.50842271177588
At time: 1170.2516696453094 and batch: 1100, loss is 4.44288366317749 and perplexity is 85.0197568641345
At time: 1171.4478464126587 and batch: 1150, loss is 4.465278205871582 and perplexity is 86.94521484462518
At time: 1172.6429443359375 and batch: 1200, loss is 4.496192169189453 and perplexity is 89.67501307202498
At time: 1173.842607975006 and batch: 1250, loss is 4.554606018066406 and perplexity is 95.0692922726023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811823712648266 and perplexity of 122.95564894285312
Finished 37 epochs...
Completing Train Step...
At time: 1176.777370929718 and batch: 50, loss is 4.56422191619873 and perplexity is 95.98787833951488
At time: 1177.9774186611176 and batch: 100, loss is 4.5465764236450195 and perplexity is 94.30898099437005
At time: 1179.170687675476 and batch: 150, loss is 4.486283578872681 and perplexity is 88.79084775573476
At time: 1180.3643202781677 and batch: 200, loss is 4.550261554718017 and perplexity is 94.65716310502113
At time: 1181.5582792758942 and batch: 250, loss is 4.547718667984009 and perplexity is 94.41676644096569
At time: 1182.752351284027 and batch: 300, loss is 4.547724599838257 and perplexity is 94.41732650912388
At time: 1183.9509649276733 and batch: 350, loss is 4.542453718185425 and perplexity is 93.92097321400385
At time: 1185.1503117084503 and batch: 400, loss is 4.530807008743286 and perplexity is 92.83344826516114
At time: 1186.3476457595825 and batch: 450, loss is 4.4703364658355715 and perplexity is 87.38612051159207
At time: 1187.540756702423 and batch: 500, loss is 4.483804197311401 and perplexity is 88.57097405303735
At time: 1188.7348811626434 and batch: 550, loss is 4.483455781936645 and perplexity is 88.5401199392561
At time: 1189.9286410808563 and batch: 600, loss is 4.505274219512939 and perplexity is 90.4931556358658
At time: 1191.1228938102722 and batch: 650, loss is 4.527011728286743 and perplexity is 92.481787041846
At time: 1192.3400766849518 and batch: 700, loss is 4.535724172592163 and perplexity is 93.29104966946909
At time: 1193.5339744091034 and batch: 750, loss is 4.493159093856812 and perplexity is 89.40343406990824
At time: 1194.7280056476593 and batch: 800, loss is 4.515024309158325 and perplexity is 91.37978736110739
At time: 1195.9237656593323 and batch: 850, loss is 4.5406259918212895 and perplexity is 93.7494681550182
At time: 1197.1185431480408 and batch: 900, loss is 4.503568696975708 and perplexity is 90.33894905824016
At time: 1198.3129975795746 and batch: 950, loss is 4.4906660842895505 and perplexity is 89.18082804819005
At time: 1199.5069541931152 and batch: 1000, loss is 4.485093240737915 and perplexity is 88.68521950277383
At time: 1200.7006289958954 and batch: 1050, loss is 4.471102828979492 and perplexity is 87.45311568167803
At time: 1201.8954322338104 and batch: 1100, loss is 4.442265872955322 and perplexity is 84.96724871083464
At time: 1203.0898604393005 and batch: 1150, loss is 4.464653244018555 and perplexity is 86.89089437792187
At time: 1204.2918639183044 and batch: 1200, loss is 4.4955722618103025 and perplexity is 89.61944009652463
At time: 1205.4861760139465 and batch: 1250, loss is 4.5538509082794185 and perplexity is 94.99753161656191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811793863338275 and perplexity of 122.95197885634775
Finished 38 epochs...
Completing Train Step...
At time: 1208.4245574474335 and batch: 50, loss is 4.5628632640838624 and perplexity is 95.85755275921761
At time: 1209.6478793621063 and batch: 100, loss is 4.545242643356323 and perplexity is 94.18327738360706
At time: 1210.8463671207428 and batch: 150, loss is 4.485048179626465 and perplexity is 88.68122333825032
At time: 1212.0446696281433 and batch: 200, loss is 4.54905403137207 and perplexity is 94.54293135334443
At time: 1213.2414047718048 and batch: 250, loss is 4.546527700424194 and perplexity is 94.30438606900397
At time: 1214.445187330246 and batch: 300, loss is 4.546516275405883 and perplexity is 94.30330864582112
At time: 1215.645673751831 and batch: 350, loss is 4.5412406063079835 and perplexity is 93.8071056468705
At time: 1216.8442840576172 and batch: 400, loss is 4.52961766242981 and perplexity is 92.72310277820608
At time: 1218.0424020290375 and batch: 450, loss is 4.469162950515747 and perplexity is 87.28363170828383
At time: 1219.243580341339 and batch: 500, loss is 4.482701854705811 and perplexity is 88.47339228886956
At time: 1220.441475391388 and batch: 550, loss is 4.482410774230957 and perplexity is 88.44764315954644
At time: 1221.6751110553741 and batch: 600, loss is 4.504235744476318 and perplexity is 90.39922953115148
At time: 1222.8709852695465 and batch: 650, loss is 4.525962829589844 and perplexity is 92.38483387184664
At time: 1224.068253993988 and batch: 700, loss is 4.534786281585693 and perplexity is 93.20359385142336
At time: 1225.2650845050812 and batch: 750, loss is 4.492201166152954 and perplexity is 89.31783304993276
At time: 1226.4650967121124 and batch: 800, loss is 4.51407943725586 and perplexity is 91.2934859458885
At time: 1227.6636040210724 and batch: 850, loss is 4.539835691452026 and perplexity is 93.67540718477785
At time: 1228.8611195087433 and batch: 900, loss is 4.502832727432251 and perplexity is 90.27248680324308
At time: 1230.058863401413 and batch: 950, loss is 4.489982748031617 and perplexity is 89.11990837155462
At time: 1231.2562029361725 and batch: 1000, loss is 4.484442691802979 and perplexity is 88.62754419002246
At time: 1232.4531486034393 and batch: 1050, loss is 4.470470170974732 and perplexity is 87.39780526613421
At time: 1233.6529726982117 and batch: 1100, loss is 4.441645727157593 and perplexity is 84.91457296361183
At time: 1234.8603405952454 and batch: 1150, loss is 4.4640161895751955 and perplexity is 86.8355577756566
At time: 1236.058272600174 and batch: 1200, loss is 4.49495044708252 and perplexity is 89.56373073102395
At time: 1237.2543618679047 and batch: 1250, loss is 4.553074159622192 and perplexity is 94.92377106185342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811782280023951 and perplexity of 122.95055467317822
Finished 39 epochs...
Completing Train Step...
At time: 1240.1997187137604 and batch: 50, loss is 4.561561994552612 and perplexity is 95.73289736919222
At time: 1241.3950579166412 and batch: 100, loss is 4.543928985595703 and perplexity is 94.05963402066403
At time: 1242.5894384384155 and batch: 150, loss is 4.483846187591553 and perplexity is 88.57469325113567
At time: 1243.7837488651276 and batch: 200, loss is 4.5478258800506595 and perplexity is 94.42688960027498
At time: 1244.9780278205872 and batch: 250, loss is 4.545355920791626 and perplexity is 94.19394682800976
At time: 1246.1725294589996 and batch: 300, loss is 4.5453410243988035 and perplexity is 94.1925436884272
At time: 1247.3732960224152 and batch: 350, loss is 4.54005386352539 and perplexity is 93.69584677217907
At time: 1248.5673005580902 and batch: 400, loss is 4.528440160751343 and perplexity is 92.61398542459328
At time: 1249.7636132240295 and batch: 450, loss is 4.468033542633057 and perplexity is 87.1851085335005
At time: 1250.9586486816406 and batch: 500, loss is 4.481627340316773 and perplexity is 88.37837741237601
At time: 1252.1782188415527 and batch: 550, loss is 4.481368284225464 and perplexity is 88.35548542065037
At time: 1253.3727531433105 and batch: 600, loss is 4.503204584121704 and perplexity is 90.3060614734304
At time: 1254.56698346138 and batch: 650, loss is 4.524912281036377 and perplexity is 92.28783008077804
At time: 1255.761668920517 and batch: 700, loss is 4.533832645416259 and perplexity is 93.11475390043348
At time: 1256.956626176834 and batch: 750, loss is 4.491234426498413 and perplexity is 89.23152768299765
At time: 1258.1506893634796 and batch: 800, loss is 4.513162260055542 and perplexity is 91.20979202898464
At time: 1259.345654964447 and batch: 850, loss is 4.5390389919281 and perplexity is 93.60080575387838
At time: 1260.5401685237885 and batch: 900, loss is 4.5020889472961425 and perplexity is 90.20536888430334
At time: 1261.7362082004547 and batch: 950, loss is 4.4893044662475585 and perplexity is 89.05948045700285
At time: 1262.9370715618134 and batch: 1000, loss is 4.4838199710845945 and perplexity is 88.57237116251238
At time: 1264.1351413726807 and batch: 1050, loss is 4.469823846817016 and perplexity is 87.3413362038847
At time: 1265.3303096294403 and batch: 1100, loss is 4.441026811599731 and perplexity is 84.86203427349892
At time: 1266.5249345302582 and batch: 1150, loss is 4.463366384506226 and perplexity is 86.77914991908756
At time: 1267.7227663993835 and batch: 1200, loss is 4.494313821792603 and perplexity is 89.50673034085139
At time: 1268.920562505722 and batch: 1250, loss is 4.552283992767334 and perplexity is 94.84879506989509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811781834511861 and perplexity of 122.95049989723191
Finished 40 epochs...
Completing Train Step...
At time: 1271.8366873264313 and batch: 50, loss is 4.560260343551636 and perplexity is 95.60836761222916
At time: 1273.0640296936035 and batch: 100, loss is 4.542633399963379 and perplexity is 93.93785061769367
At time: 1274.2617242336273 and batch: 150, loss is 4.48266755104065 and perplexity is 88.4703573792994
At time: 1275.4590947628021 and batch: 200, loss is 4.546359281539917 and perplexity is 94.28850476691292
At time: 1276.6564345359802 and batch: 250, loss is 4.544146928787232 and perplexity is 94.08013591153912
At time: 1277.8551077842712 and batch: 300, loss is 4.544172649383545 and perplexity is 94.08255573985556
At time: 1279.0532381534576 and batch: 350, loss is 4.538897056579589 and perplexity is 93.58752143367242
At time: 1280.2508811950684 and batch: 400, loss is 4.5272697162628175 and perplexity is 92.50564930886533
At time: 1281.4517550468445 and batch: 450, loss is 4.4669254112243655 and perplexity is 87.08854948630172
At time: 1282.6757397651672 and batch: 500, loss is 4.480567541122436 and perplexity is 88.28476369383142
At time: 1283.8732481002808 and batch: 550, loss is 4.480343084335328 and perplexity is 88.26494980318635
At time: 1285.0717196464539 and batch: 600, loss is 4.5021833896636965 and perplexity is 90.21388849520663
At time: 1286.267709493637 and batch: 650, loss is 4.523894481658935 and perplexity is 92.19394736976334
At time: 1287.469394683838 and batch: 700, loss is 4.532908964157104 and perplexity is 93.02878525722939
At time: 1288.6696383953094 and batch: 750, loss is 4.490254583358765 and perplexity is 89.14413760403372
At time: 1289.8677070140839 and batch: 800, loss is 4.51222601890564 and perplexity is 91.12443763079781
At time: 1291.0656538009644 and batch: 850, loss is 4.538259220123291 and perplexity is 93.5278469339563
At time: 1292.267412662506 and batch: 900, loss is 4.501365709304809 and perplexity is 90.14015252082261
At time: 1293.4647455215454 and batch: 950, loss is 4.488622770309449 and perplexity is 88.99878965961047
At time: 1294.6616425514221 and batch: 1000, loss is 4.483185710906983 and perplexity is 88.51621104659262
At time: 1295.857589483261 and batch: 1050, loss is 4.469197473526001 and perplexity is 87.28664505401089
At time: 1297.0591735839844 and batch: 1100, loss is 4.440399312973023 and perplexity is 84.8088001674436
At time: 1298.2648029327393 and batch: 1150, loss is 4.462722806930542 and perplexity is 86.723318771927
At time: 1299.4616560935974 and batch: 1200, loss is 4.4936737537384035 and perplexity is 89.44945827309017
At time: 1300.6597673892975 and batch: 1250, loss is 4.551508693695069 and perplexity is 94.77528738597353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811786735144845 and perplexity of 122.9511024339835
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1303.574194431305 and batch: 50, loss is 4.559886045455933 and perplexity is 95.57258827878456
At time: 1304.7940618991852 and batch: 100, loss is 4.542988424301147 and perplexity is 93.9712067616717
At time: 1305.987919807434 and batch: 150, loss is 4.483399820327759 and perplexity is 88.53516523033134
At time: 1307.1838805675507 and batch: 200, loss is 4.546677436828613 and perplexity is 94.3185079259465
At time: 1308.3815846443176 and batch: 250, loss is 4.544461832046509 and perplexity is 94.10976671814485
At time: 1309.5822262763977 and batch: 300, loss is 4.544091033935547 and perplexity is 94.07487746325731
At time: 1310.779051065445 and batch: 350, loss is 4.537871294021606 and perplexity is 93.49157207733298
At time: 1311.9986851215363 and batch: 400, loss is 4.525907897949219 and perplexity is 92.3797591607356
At time: 1313.1909182071686 and batch: 450, loss is 4.465099277496338 and perplexity is 86.92965927030617
At time: 1314.3848812580109 and batch: 500, loss is 4.4786774444580075 and perplexity is 88.11805455427834
At time: 1315.5807795524597 and batch: 550, loss is 4.477210063934326 and perplexity is 87.98884665899945
At time: 1316.7745230197906 and batch: 600, loss is 4.498722076416016 and perplexity is 89.90216975714922
At time: 1317.9683718681335 and batch: 650, loss is 4.5206613349914555 and perplexity is 91.89635215977975
At time: 1319.1618044376373 and batch: 700, loss is 4.529503564834595 and perplexity is 92.7125238986821
At time: 1320.3563661575317 and batch: 750, loss is 4.485860891342163 and perplexity is 88.75332490234413
At time: 1321.5503463745117 and batch: 800, loss is 4.50763897895813 and perplexity is 90.70740340275498
At time: 1322.7439091205597 and batch: 850, loss is 4.5319198513031 and perplexity is 92.93681478202691
At time: 1323.9458918571472 and batch: 900, loss is 4.494839391708374 and perplexity is 89.5537847496853
At time: 1325.1417064666748 and batch: 950, loss is 4.481688661575317 and perplexity is 88.38379705187494
At time: 1326.335965871811 and batch: 1000, loss is 4.475761404037476 and perplexity is 87.86147302828553
At time: 1327.5325274467468 and batch: 1050, loss is 4.461510381698608 and perplexity is 86.61823694685124
At time: 1328.7289109230042 and batch: 1100, loss is 4.431666498184204 and perplexity is 84.07140507676198
At time: 1329.9243326187134 and batch: 1150, loss is 4.453158502578735 and perplexity is 85.89782448880341
At time: 1331.1185216903687 and batch: 1200, loss is 4.484397506713867 and perplexity is 88.62353963701418
At time: 1332.3126492500305 and batch: 1250, loss is 4.544706764221192 and perplexity is 94.13282005110226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811604966212363 and perplexity of 122.92875577437238
Finished 42 epochs...
Completing Train Step...
At time: 1335.2454109191895 and batch: 50, loss is 4.559214782714844 and perplexity is 95.508455488588
At time: 1336.4414207935333 and batch: 100, loss is 4.541677236557007 and perplexity is 93.84807361004437
At time: 1337.636501789093 and batch: 150, loss is 4.481895704269409 and perplexity is 88.40209816582119
At time: 1338.8320503234863 and batch: 200, loss is 4.545239591598511 and perplexity is 94.18298995949314
At time: 1340.0280928611755 and batch: 250, loss is 4.543327035903931 and perplexity is 94.00303189048363
At time: 1341.227306842804 and batch: 300, loss is 4.542916259765625 and perplexity is 93.96442561786525
At time: 1342.4504494667053 and batch: 350, loss is 4.536830835342407 and perplexity is 93.3943485470369
At time: 1343.6456882953644 and batch: 400, loss is 4.525041608810425 and perplexity is 92.29976623223543
At time: 1344.8422207832336 and batch: 450, loss is 4.464256658554077 and perplexity is 86.85644154441414
At time: 1346.038955450058 and batch: 500, loss is 4.477866334915161 and perplexity is 88.0466101378732
At time: 1347.2344081401825 and batch: 550, loss is 4.476545715332032 and perplexity is 87.93041080474238
At time: 1348.4296658039093 and batch: 600, loss is 4.498072156906128 and perplexity is 89.8437595660602
At time: 1349.6342370510101 and batch: 650, loss is 4.520172090530395 and perplexity is 91.85140337486256
At time: 1350.8346457481384 and batch: 700, loss is 4.529082355499267 and perplexity is 92.67348074136257
At time: 1352.031593322754 and batch: 750, loss is 4.4855258178710935 and perplexity is 88.72359099949934
At time: 1353.229011774063 and batch: 800, loss is 4.507441730499267 and perplexity is 90.68951327168477
At time: 1354.4249567985535 and batch: 850, loss is 4.5318100643157955 and perplexity is 92.926612089194
At time: 1355.620451450348 and batch: 900, loss is 4.494691429138183 and perplexity is 89.5405351217721
At time: 1356.8164148330688 and batch: 950, loss is 4.481658945083618 and perplexity is 88.38117063452763
At time: 1358.012669801712 and batch: 1000, loss is 4.475779752731324 and perplexity is 87.8630851863456
At time: 1359.2084987163544 and batch: 1050, loss is 4.461752691268921 and perplexity is 86.63922791768091
At time: 1360.4113743305206 and batch: 1100, loss is 4.432019853591919 and perplexity is 84.10111741157802
At time: 1361.606990814209 and batch: 1150, loss is 4.453637266159058 and perplexity is 85.93895908488513
At time: 1362.809089422226 and batch: 1200, loss is 4.4848369598388675 and perplexity is 88.6624940871568
At time: 1364.0100309848785 and batch: 1250, loss is 4.545091400146484 and perplexity is 94.16903387956623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811530565693431 and perplexity of 122.91961015137483
Finished 43 epochs...
Completing Train Step...
At time: 1366.9435760974884 and batch: 50, loss is 4.558637647628784 and perplexity is 95.45335011106314
At time: 1368.1716449260712 and batch: 100, loss is 4.540985269546509 and perplexity is 93.7831563020155
At time: 1369.36545419693 and batch: 150, loss is 4.481229314804077 and perplexity is 88.3432075631082
At time: 1370.5594444274902 and batch: 200, loss is 4.544411067962646 and perplexity is 94.10498944331289
At time: 1371.761803150177 and batch: 250, loss is 4.542677659988403 and perplexity is 93.9420084013238
At time: 1372.9843685626984 and batch: 300, loss is 4.5423092269897465 and perplexity is 93.90740344066487
At time: 1374.177981376648 and batch: 350, loss is 4.536213016510009 and perplexity is 93.33666558031142
At time: 1375.3744101524353 and batch: 400, loss is 4.524468097686768 and perplexity is 92.24684646607805
At time: 1376.5733170509338 and batch: 450, loss is 4.463704566955567 and perplexity is 86.8085020674699
At time: 1377.7706122398376 and batch: 500, loss is 4.477350978851319 and perplexity is 88.00124647366161
At time: 1378.969969511032 and batch: 550, loss is 4.4761591720581055 and perplexity is 87.89642846411797
At time: 1380.1702513694763 and batch: 600, loss is 4.497722063064575 and perplexity is 89.81231132437328
At time: 1381.3691647052765 and batch: 650, loss is 4.519936113357544 and perplexity is 91.82973109755368
At time: 1382.5740177631378 and batch: 700, loss is 4.52886402130127 and perplexity is 92.65324915997267
At time: 1383.7675802707672 and batch: 750, loss is 4.485348558425903 and perplexity is 88.70786529878912
At time: 1384.9688947200775 and batch: 800, loss is 4.507351007461548 and perplexity is 90.68128601675787
At time: 1386.1765620708466 and batch: 850, loss is 4.531755208969116 and perplexity is 92.92151470748271
At time: 1387.3729197978973 and batch: 900, loss is 4.494628524780273 and perplexity is 89.5349028090537
At time: 1388.5662913322449 and batch: 950, loss is 4.481659069061279 and perplexity is 88.38118159181913
At time: 1389.7607464790344 and batch: 1000, loss is 4.475788173675537 and perplexity is 87.86382507959964
At time: 1390.955410003662 and batch: 1050, loss is 4.461929225921631 and perplexity is 86.65452409380529
At time: 1392.149849653244 and batch: 1100, loss is 4.432244663238525 and perplexity is 84.12002627943079
At time: 1393.3452866077423 and batch: 1150, loss is 4.453947896957398 and perplexity is 85.96565851897272
At time: 1394.5477294921875 and batch: 1200, loss is 4.48509801864624 and perplexity is 88.68564323363466
At time: 1395.7457466125488 and batch: 1250, loss is 4.545281639099121 and perplexity is 94.18695020207952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811518982379106 and perplexity of 122.9181863431399
Finished 44 epochs...
Completing Train Step...
At time: 1398.6971673965454 and batch: 50, loss is 4.558139343261718 and perplexity is 95.40579713876258
At time: 1399.897516965866 and batch: 100, loss is 4.540413150787353 and perplexity is 93.72951654462113
At time: 1401.093270778656 and batch: 150, loss is 4.480699501037598 and perplexity is 88.2964145124621
At time: 1402.2959206104279 and batch: 200, loss is 4.543755931854248 and perplexity is 94.04335805742565
At time: 1403.5274169445038 and batch: 250, loss is 4.542160615921021 and perplexity is 93.89344879801116
At time: 1404.7229173183441 and batch: 300, loss is 4.54182430267334 and perplexity is 93.86187649669911
At time: 1405.9180133342743 and batch: 350, loss is 4.535710830688476 and perplexity is 93.28980499757273
At time: 1407.1139879226685 and batch: 400, loss is 4.52398980140686 and perplexity is 92.20273569243052
At time: 1408.3111534118652 and batch: 450, loss is 4.463248624801635 and perplexity is 86.7689314337032
At time: 1409.5093004703522 and batch: 500, loss is 4.476942844390869 and perplexity is 87.96533746076449
At time: 1410.709966659546 and batch: 550, loss is 4.475849409103393 and perplexity is 87.86920562325976
At time: 1411.9119727611542 and batch: 600, loss is 4.497445182800293 and perplexity is 89.78744751018655
At time: 1413.11017370224 and batch: 650, loss is 4.519756269454956 and perplexity is 91.81321756531304
At time: 1414.3050708770752 and batch: 700, loss is 4.528683280944824 and perplexity is 92.63650449195787
At time: 1415.5056130886078 and batch: 750, loss is 4.485201692581176 and perplexity is 88.69483809986681
At time: 1416.7013924121857 and batch: 800, loss is 4.507280149459839 and perplexity is 90.67486074968181
At time: 1417.8962111473083 and batch: 850, loss is 4.531701564788818 and perplexity is 92.91653014269184
At time: 1419.0915715694427 and batch: 900, loss is 4.494567327499389 and perplexity is 89.52942368411308
At time: 1420.2884774208069 and batch: 950, loss is 4.48164719581604 and perplexity is 88.38013222660531
At time: 1421.4875962734222 and batch: 1000, loss is 4.4757781410217286 and perplexity is 87.86294357668227
At time: 1422.6863446235657 and batch: 1050, loss is 4.462057609558105 and perplexity is 86.66564983089138
At time: 1423.880396604538 and batch: 1100, loss is 4.432396059036255 and perplexity is 84.13276266200747
At time: 1425.0747225284576 and batch: 1150, loss is 4.45416298866272 and perplexity is 85.98415100778189
At time: 1426.2696413993835 and batch: 1200, loss is 4.485263729095459 and perplexity is 88.70034058913329
At time: 1427.4636552333832 and batch: 1250, loss is 4.545392227172852 and perplexity is 94.19736673143424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811522991987911 and perplexity of 122.91867919797023
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1430.3821489810944 and batch: 50, loss is 4.557936944961548 and perplexity is 95.38648912161612
At time: 1431.605134487152 and batch: 100, loss is 4.54031476020813 and perplexity is 93.72029489686705
At time: 1432.8073890209198 and batch: 150, loss is 4.480643472671509 and perplexity is 88.29146754721195
At time: 1434.0282769203186 and batch: 200, loss is 4.543756151199341 and perplexity is 94.04337868537705
At time: 1435.224758863449 and batch: 250, loss is 4.5419754219055175 and perplexity is 93.87606190322292
At time: 1436.4219527244568 and batch: 300, loss is 4.5416176319122314 and perplexity is 93.84247999565834
At time: 1437.6185846328735 and batch: 350, loss is 4.535290355682373 and perplexity is 93.25058721187418
At time: 1438.8156366348267 and batch: 400, loss is 4.523495006561279 and perplexity is 92.15712553882642
At time: 1440.0129425525665 and batch: 450, loss is 4.462865991592407 and perplexity is 86.7357371100377
At time: 1441.2105247974396 and batch: 500, loss is 4.476611852645874 and perplexity is 87.93622647823246
At time: 1442.4073793888092 and batch: 550, loss is 4.475028572082519 and perplexity is 87.79710892015896
At time: 1443.6037621498108 and batch: 600, loss is 4.496614084243775 and perplexity is 89.71285629278742
At time: 1444.80500125885 and batch: 650, loss is 4.51919638633728 and perplexity is 91.76182728243707
At time: 1446.008290052414 and batch: 700, loss is 4.52834644317627 and perplexity is 92.60530627316233
At time: 1447.2050383090973 and batch: 750, loss is 4.484475183486938 and perplexity is 88.63042389496135
At time: 1448.4028632640839 and batch: 800, loss is 4.506419820785522 and perplexity is 90.59688411451643
At time: 1449.6011698246002 and batch: 850, loss is 4.5303929424285885 and perplexity is 92.79501701844872
At time: 1450.8009135723114 and batch: 900, loss is 4.493203525543213 and perplexity is 89.40740650350433
At time: 1451.9987614154816 and batch: 950, loss is 4.4802154922485355 and perplexity is 88.25368861248506
At time: 1453.199185371399 and batch: 1000, loss is 4.47427451133728 and perplexity is 87.73092952155596
At time: 1454.402325630188 and batch: 1050, loss is 4.460443792343139 and perplexity is 86.52590010883213
At time: 1455.6059992313385 and batch: 1100, loss is 4.430652360916138 and perplexity is 83.98618834972795
At time: 1456.8074762821198 and batch: 1150, loss is 4.452365980148316 and perplexity is 85.82977550490659
At time: 1458.0133237838745 and batch: 1200, loss is 4.483396015167236 and perplexity is 88.5348283404567
At time: 1459.2155034542084 and batch: 1250, loss is 4.5438485527038575 and perplexity is 94.05206883654272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811283306483805 and perplexity of 122.88922090288686
Finished 46 epochs...
Completing Train Step...
At time: 1462.1631412506104 and batch: 50, loss is 4.557798881530761 and perplexity is 95.37332064474077
At time: 1463.3829255104065 and batch: 100, loss is 4.540124883651734 and perplexity is 93.70250129935457
At time: 1464.5769851207733 and batch: 150, loss is 4.48038028717041 and perplexity is 88.26823357063982
At time: 1465.7763848304749 and batch: 200, loss is 4.543489398956299 and perplexity is 94.01829574878302
At time: 1466.9711577892303 and batch: 250, loss is 4.541783885955811 and perplexity is 93.85808298441115
At time: 1468.1672871112823 and batch: 300, loss is 4.54143159866333 and perplexity is 93.82502379798667
At time: 1469.361176252365 and batch: 350, loss is 4.535133771896362 and perplexity is 93.23598682500254
At time: 1470.5547852516174 and batch: 400, loss is 4.523360977172851 and perplexity is 92.14477460336384
At time: 1471.7488129138947 and batch: 450, loss is 4.462726707458496 and perplexity is 86.72365703931584
At time: 1472.9439630508423 and batch: 500, loss is 4.476486539840698 and perplexity is 87.92520763343147
At time: 1474.1395173072815 and batch: 550, loss is 4.474905166625977 and perplexity is 87.78627494634905
At time: 1475.3339977264404 and batch: 600, loss is 4.496530485153198 and perplexity is 89.70535669307257
At time: 1476.5302817821503 and batch: 650, loss is 4.519159469604492 and perplexity is 91.75843979810695
At time: 1477.7266006469727 and batch: 700, loss is 4.528307647705078 and perplexity is 92.60171367635928
At time: 1478.9264090061188 and batch: 750, loss is 4.484432420730591 and perplexity is 88.62663389477537
At time: 1480.1234304904938 and batch: 800, loss is 4.5063966941833495 and perplexity is 90.59478894064662
At time: 1481.3200623989105 and batch: 850, loss is 4.5303855228424075 and perplexity is 92.79432852037698
At time: 1482.5182960033417 and batch: 900, loss is 4.493173151016236 and perplexity is 89.40469083706729
At time: 1483.7143907546997 and batch: 950, loss is 4.480207767486572 and perplexity is 88.25300687638126
At time: 1484.9102971553802 and batch: 1000, loss is 4.4742977333068845 and perplexity is 87.73296683018972
At time: 1486.1072993278503 and batch: 1050, loss is 4.460477714538574 and perplexity is 86.5288353071097
At time: 1487.305321931839 and batch: 1100, loss is 4.430738410949707 and perplexity is 83.9934156750061
At time: 1488.5001423358917 and batch: 1150, loss is 4.452474241256714 and perplexity is 85.83906803453701
At time: 1489.6960117816925 and batch: 1200, loss is 4.483488187789917 and perplexity is 88.54298920388177
At time: 1490.8931195735931 and batch: 1250, loss is 4.543915090560913 and perplexity is 94.05832706785705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811226280936359 and perplexity of 122.8822132775984
Finished 47 epochs...
Completing Train Step...
At time: 1493.860779285431 and batch: 50, loss is 4.557687788009644 and perplexity is 95.3627258752457
At time: 1495.0583097934723 and batch: 100, loss is 4.53998574256897 and perplexity is 93.68946433887551
At time: 1496.2574307918549 and batch: 150, loss is 4.480216112136841 and perplexity is 88.25374331993147
At time: 1497.455555677414 and batch: 200, loss is 4.543322267532349 and perplexity is 94.00258365016643
At time: 1498.6524231433868 and batch: 250, loss is 4.541642274856567 and perplexity is 93.84479257916351
At time: 1499.850486755371 and batch: 300, loss is 4.541293544769287 and perplexity is 93.81207178215139
At time: 1501.048598766327 and batch: 350, loss is 4.535016775131226 and perplexity is 93.22507915424325
At time: 1502.2463793754578 and batch: 400, loss is 4.523249225616455 and perplexity is 92.13447785673759
At time: 1503.4435255527496 and batch: 450, loss is 4.462621288299561 and perplexity is 86.71451518620263
At time: 1504.6421630382538 and batch: 500, loss is 4.476391220092774 and perplexity is 87.9168270242288
At time: 1505.8391952514648 and batch: 550, loss is 4.4748211765289305 and perplexity is 87.77890207822527
At time: 1507.0372698307037 and batch: 600, loss is 4.49647087097168 and perplexity is 89.70000914105205
At time: 1508.236773967743 and batch: 650, loss is 4.519142522811889 and perplexity is 91.75688480003426
At time: 1509.437842130661 and batch: 700, loss is 4.528295192718506 and perplexity is 92.60056033044137
At time: 1510.6369924545288 and batch: 750, loss is 4.484417324066162 and perplexity is 88.62529593832336
At time: 1511.8349833488464 and batch: 800, loss is 4.506380825042725 and perplexity is 90.59335129060823
At time: 1513.040126800537 and batch: 850, loss is 4.530379467010498 and perplexity is 92.7937665752228
At time: 1514.2398958206177 and batch: 900, loss is 4.493148069381714 and perplexity is 89.40244844940871
At time: 1515.4382450580597 and batch: 950, loss is 4.480203227996826 and perplexity is 88.25260625367082
At time: 1516.6371335983276 and batch: 1000, loss is 4.47431360244751 and perplexity is 87.73435908802479
At time: 1517.8355059623718 and batch: 1050, loss is 4.460511026382446 and perplexity is 86.53171779017201
At time: 1519.0400397777557 and batch: 1100, loss is 4.430803480148316 and perplexity is 83.99888123707044
At time: 1520.2417786121368 and batch: 1150, loss is 4.4525566673278805 and perplexity is 85.8461437032735
At time: 1521.446583032608 and batch: 1200, loss is 4.483553628921509 and perplexity is 88.54878374688846
At time: 1522.647080898285 and batch: 1250, loss is 4.543957595825195 and perplexity is 94.0623251268757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811207569428604 and perplexity of 122.8799139876234
Finished 48 epochs...
Completing Train Step...
At time: 1525.597172498703 and batch: 50, loss is 4.557583150863647 and perplexity is 95.35274791381745
At time: 1526.83070063591 and batch: 100, loss is 4.539862813949585 and perplexity is 93.67794793023609
At time: 1528.027184009552 and batch: 150, loss is 4.4800812530517575 and perplexity is 88.24184230335032
At time: 1529.2222213745117 and batch: 200, loss is 4.543182077407837 and perplexity is 93.98940633994606
At time: 1530.4177918434143 and batch: 250, loss is 4.541518096923828 and perplexity is 93.83313985034356
At time: 1531.613882780075 and batch: 300, loss is 4.541174983978271 and perplexity is 93.80095000803026
At time: 1532.8146023750305 and batch: 350, loss is 4.534907484054566 and perplexity is 93.21489104171583
At time: 1534.0104882717133 and batch: 400, loss is 4.523144626617432 and perplexity is 92.12484118658006
At time: 1535.210872411728 and batch: 450, loss is 4.462526607513428 and perplexity is 86.70630537639741
At time: 1536.414247751236 and batch: 500, loss is 4.476305847167969 and perplexity is 87.90932162794952
At time: 1537.6119494438171 and batch: 550, loss is 4.47474967956543 and perplexity is 87.77262637761669
At time: 1538.8130259513855 and batch: 600, loss is 4.496418924331665 and perplexity is 89.69534964799139
At time: 1540.0065274238586 and batch: 650, loss is 4.519129953384399 and perplexity is 91.7557314757724
At time: 1541.2018792629242 and batch: 700, loss is 4.528289661407471 and perplexity is 92.60004812935675
At time: 1542.395843744278 and batch: 750, loss is 4.484407567977906 and perplexity is 88.62443130633217
At time: 1543.5915830135345 and batch: 800, loss is 4.506367778778076 and perplexity is 90.59216939348154
At time: 1544.793325662613 and batch: 850, loss is 4.530373287200928 and perplexity is 92.793193129188
At time: 1545.9894390106201 and batch: 900, loss is 4.493124828338623 and perplexity is 89.40037066739687
At time: 1547.1854050159454 and batch: 950, loss is 4.48019832611084 and perplexity is 88.25217365051728
At time: 1548.3857672214508 and batch: 1000, loss is 4.474324626922607 and perplexity is 87.73532631861333
At time: 1549.5844740867615 and batch: 1050, loss is 4.460539512634277 and perplexity is 86.53418278958549
At time: 1550.780441045761 and batch: 1100, loss is 4.4308580303192135 and perplexity is 84.00346351537803
At time: 1551.9782719612122 and batch: 1150, loss is 4.4526268482208256 and perplexity is 85.8521686737109
At time: 1553.1751215457916 and batch: 1200, loss is 4.4836053466796875 and perplexity is 88.55336340989719
At time: 1554.3972928524017 and batch: 1250, loss is 4.543987464904785 and perplexity is 94.06513472391119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811201332259352 and perplexity of 122.8791475671923
Finished 49 epochs...
Completing Train Step...
At time: 1557.3853776454926 and batch: 50, loss is 4.55748309135437 and perplexity is 95.34320744196839
At time: 1558.5858607292175 and batch: 100, loss is 4.539748363494873 and perplexity is 93.66722706001487
At time: 1559.786233663559 and batch: 150, loss is 4.479961729049682 and perplexity is 88.23129591549353
At time: 1560.9896078109741 and batch: 200, loss is 4.543054838180542 and perplexity is 93.97744796131293
At time: 1562.1861431598663 and batch: 250, loss is 4.541403532028198 and perplexity is 93.82239048223187
At time: 1563.3831412792206 and batch: 300, loss is 4.541067523956299 and perplexity is 93.79087069745253
At time: 1564.5845322608948 and batch: 350, loss is 4.534803581237793 and perplexity is 93.20520625511845
At time: 1565.7810018062592 and batch: 400, loss is 4.5230444717407225 and perplexity is 92.11561489650583
At time: 1566.97727227211 and batch: 450, loss is 4.462437782287598 and perplexity is 86.6986040112843
At time: 1568.1804571151733 and batch: 500, loss is 4.476226177215576 and perplexity is 87.90231817546663
At time: 1569.3818640708923 and batch: 550, loss is 4.474683523178101 and perplexity is 87.76681984982072
At time: 1570.5787897109985 and batch: 600, loss is 4.496371002197265 and perplexity is 89.69105135838298
At time: 1571.7750508785248 and batch: 650, loss is 4.519118995666504 and perplexity is 91.75472604786017
At time: 1572.969331741333 and batch: 700, loss is 4.528287029266357 and perplexity is 92.59980439328368
At time: 1574.1650331020355 and batch: 750, loss is 4.4843990516662595 and perplexity is 88.62367655626954
At time: 1575.3616659641266 and batch: 800, loss is 4.506355695724487 and perplexity is 90.59107477005725
At time: 1576.5591254234314 and batch: 850, loss is 4.530366125106812 and perplexity is 92.79252853798542
At time: 1577.7569870948792 and batch: 900, loss is 4.493102674484253 and perplexity is 89.39839012654282
At time: 1578.9568438529968 and batch: 950, loss is 4.480193128585816 and perplexity is 88.25171495882829
At time: 1580.155075788498 and batch: 1000, loss is 4.474332065582275 and perplexity is 87.73597895427402
At time: 1581.3528077602386 and batch: 1050, loss is 4.460563726425171 and perplexity is 86.53627813556078
At time: 1582.550149679184 and batch: 1100, loss is 4.430905342102051 and perplexity is 84.00743796301981
At time: 1583.7460720539093 and batch: 1150, loss is 4.452689361572266 and perplexity is 85.85753574825829
At time: 1584.9688551425934 and batch: 1200, loss is 4.48364821434021 and perplexity is 88.55715956678358
At time: 1586.1652104854584 and batch: 1250, loss is 4.544009828567505 and perplexity is 94.06723838838059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.811195540602189 and perplexity of 122.87843589535802
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9541e8860>
SETTINGS FOR THIS RUN
{'lr': 9.314818014280576, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 7.261082075760348, 'wordvec_source': '', 'dropout': 0.0, 'tune_wordvecs': True, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8629252910614014 and batch: 50, loss is 6.990966682434082 and perplexity is 1086.7715315185064
At time: 3.0506856441497803 and batch: 100, loss is 6.018032064437866 and perplexity is 410.76943202367767
At time: 4.239790201187134 and batch: 150, loss is 5.641889200210572 and perplexity is 281.99496049986305
At time: 5.426767826080322 and batch: 200, loss is 5.552198753356934 and perplexity is 257.80378011091966
At time: 6.613657474517822 and batch: 250, loss is 5.5144469356536865 and perplexity is 248.2526395888594
At time: 7.800809144973755 and batch: 300, loss is 5.482684259414673 and perplexity is 240.49138300783497
At time: 8.988245487213135 and batch: 350, loss is 5.460655946731567 and perplexity is 235.25168634063888
At time: 10.175243377685547 and batch: 400, loss is 5.402585411071778 and perplexity is 221.97958342116755
At time: 11.364341020584106 and batch: 450, loss is 5.353747873306275 and perplexity is 211.39911209441172
At time: 12.55161452293396 and batch: 500, loss is 5.330164413452149 and perplexity is 206.4719181266401
At time: 13.73837423324585 and batch: 550, loss is 5.338958616256714 and perplexity is 208.29568156703783
At time: 14.925377130508423 and batch: 600, loss is 5.331882953643799 and perplexity is 206.82705348617807
At time: 16.112196922302246 and batch: 650, loss is 5.305680732727051 and perplexity is 201.47810848433662
At time: 17.298932313919067 and batch: 700, loss is 5.2831346225738525 and perplexity is 196.98638655605706
At time: 18.49506902694702 and batch: 750, loss is 5.270741872787475 and perplexity is 194.5602478714799
At time: 19.683846712112427 and batch: 800, loss is 5.278632802963257 and perplexity is 196.10158248650865
At time: 20.87242841720581 and batch: 850, loss is 5.328407535552978 and perplexity is 206.10949064052724
At time: 22.061615467071533 and batch: 900, loss is 5.286981496810913 and perplexity is 197.7456278280971
At time: 23.256848335266113 and batch: 950, loss is 5.253102388381958 and perplexity is 191.15839706742017
At time: 24.46681833267212 and batch: 1000, loss is 5.2375056934356685 and perplexity is 188.20008775474253
At time: 25.669883489608765 and batch: 1050, loss is 5.213765888214112 and perplexity is 183.78486989294274
At time: 26.86454439163208 and batch: 1100, loss is 5.18287615776062 and perplexity is 178.19459041551147
At time: 28.06193971633911 and batch: 1150, loss is 5.199213228225708 and perplexity is 181.12967808171956
At time: 29.256009817123413 and batch: 1200, loss is 5.175808582305908 and perplexity is 176.93962670134707
At time: 30.444398403167725 and batch: 1250, loss is 5.169803791046142 and perplexity is 175.88032480379368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0208704593407845 and perplexity of 151.54315855651993
Finished 1 epochs...
Completing Train Step...
At time: 33.557780504226685 and batch: 50, loss is 5.128420629501343 and perplexity is 168.75038808303998
At time: 34.7451708316803 and batch: 100, loss is 5.126412448883056 and perplexity is 168.41184686409258
At time: 35.96057987213135 and batch: 150, loss is 5.010796976089478 and perplexity is 150.024254259669
At time: 37.155380964279175 and batch: 200, loss is 5.0358703994750975 and perplexity is 153.83343089451049
At time: 38.34330892562866 and batch: 250, loss is 5.03582202911377 and perplexity is 153.8259900958323
At time: 39.53152370452881 and batch: 300, loss is 5.037062330245972 and perplexity is 154.01689901330982
At time: 40.71932911872864 and batch: 350, loss is 5.022609243392944 and perplexity is 151.80688860218453
At time: 41.90702176094055 and batch: 400, loss is 5.011471214294434 and perplexity is 150.12544045152586
At time: 43.111469745635986 and batch: 450, loss is 4.955432462692261 and perplexity is 141.9439785765331
At time: 44.30447220802307 and batch: 500, loss is 4.9480444431304935 and perplexity is 140.8991580159393
At time: 45.508408308029175 and batch: 550, loss is 4.9609261322021485 and perplexity is 142.7259177749151
At time: 46.711002349853516 and batch: 600, loss is 4.951862268447876 and perplexity is 141.43811455492718
At time: 47.906466007232666 and batch: 650, loss is 4.958038320541382 and perplexity is 142.31434676108077
At time: 49.096545934677124 and batch: 700, loss is 4.9421842002868654 and perplexity is 140.07586942518327
At time: 50.28330111503601 and batch: 750, loss is 4.943285427093506 and perplexity is 140.23020969378092
At time: 51.47119164466858 and batch: 800, loss is 4.961214561462402 and perplexity is 142.76709004315592
At time: 52.65889930725098 and batch: 850, loss is 5.004670734405518 and perplexity is 149.10797894733776
At time: 53.84708833694458 and batch: 900, loss is 4.963429794311524 and perplexity is 143.0837029469705
At time: 55.03493905067444 and batch: 950, loss is 4.947264814376831 and perplexity is 140.78935179059047
At time: 56.22311520576477 and batch: 1000, loss is 4.940197505950928 and perplexity is 139.79785774235413
At time: 57.411001682281494 and batch: 1050, loss is 4.904010076522827 and perplexity is 134.82937316018194
At time: 58.60265588760376 and batch: 1100, loss is 4.883373470306396 and perplexity is 132.07546586316002
At time: 59.790589332580566 and batch: 1150, loss is 4.9051351070404055 and perplexity is 134.98114567800218
At time: 60.98016929626465 and batch: 1200, loss is 4.897148857116699 and perplexity is 133.907445638784
At time: 62.17514896392822 and batch: 1250, loss is 4.901223859786987 and perplexity is 134.45423215895332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.885587149292883 and perplexity of 132.36816239512598
Finished 2 epochs...
Completing Train Step...
At time: 65.2213728427887 and batch: 50, loss is 4.855890130996704 and perplexity is 128.49501772258404
At time: 66.59495067596436 and batch: 100, loss is 4.855303277969361 and perplexity is 128.41963215472293
At time: 67.79678535461426 and batch: 150, loss is 4.759483127593994 and perplexity is 116.68559874344324
At time: 68.9893753528595 and batch: 200, loss is 4.7925831317901615 and perplexity is 120.61252465846337
At time: 70.18111824989319 and batch: 250, loss is 4.797272500991821 and perplexity is 121.17944953806807
At time: 71.37510776519775 and batch: 300, loss is 4.814931306838989 and perplexity is 123.33833951875509
At time: 72.56687355041504 and batch: 350, loss is 4.793244771957397 and perplexity is 120.69235315540696
At time: 73.76104617118835 and batch: 400, loss is 4.791833019256591 and perplexity is 120.52208561598178
At time: 74.95333981513977 and batch: 450, loss is 4.735518960952759 and perplexity is 113.92256474276381
At time: 76.14711928367615 and batch: 500, loss is 4.742488079071045 and perplexity is 114.71927752092294
At time: 77.34290266036987 and batch: 550, loss is 4.7505502510070805 and perplexity is 115.64790239744485
At time: 78.5415210723877 and batch: 600, loss is 4.743394956588745 and perplexity is 114.82336104292315
At time: 79.74615788459778 and batch: 650, loss is 4.754079542160034 and perplexity is 116.05677861342977
At time: 80.94189524650574 and batch: 700, loss is 4.738416290283203 and perplexity is 114.25311455559253
At time: 82.14773607254028 and batch: 750, loss is 4.750581712722778 and perplexity is 115.65154093610813
At time: 83.34825873374939 and batch: 800, loss is 4.763562221527099 and perplexity is 117.16254234891993
At time: 84.54278182983398 and batch: 850, loss is 4.808311347961426 and perplexity is 122.52454141009963
At time: 85.7371392250061 and batch: 900, loss is 4.770087671279907 and perplexity is 117.9295805447309
At time: 86.93351793289185 and batch: 950, loss is 4.755530710220337 and perplexity is 116.22531876419984
At time: 88.12929654121399 and batch: 1000, loss is 4.75053617477417 and perplexity is 115.64627452209226
At time: 89.33150553703308 and batch: 1050, loss is 4.7231547641754155 and perplexity is 112.52267581972511
At time: 90.52728962898254 and batch: 1100, loss is 4.67470419883728 and perplexity is 107.20085226488986
At time: 91.78519415855408 and batch: 1150, loss is 4.711756687164307 and perplexity is 111.24741526064965
At time: 92.98599481582642 and batch: 1200, loss is 4.695520257949829 and perplexity is 109.45573904967502
At time: 94.19551157951355 and batch: 1250, loss is 4.730049428939819 and perplexity is 113.30116256659824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.844569742244525 and perplexity of 127.04860658401583
Finished 3 epochs...
Completing Train Step...
At time: 97.23474192619324 and batch: 50, loss is 4.680365571975708 and perplexity is 107.80947749198107
At time: 98.42682838439941 and batch: 100, loss is 4.673627309799194 and perplexity is 107.08547097978341
At time: 99.61995935440063 and batch: 150, loss is 4.591705598831177 and perplexity is 98.66256549185931
At time: 100.81264686584473 and batch: 200, loss is 4.631052932739258 and perplexity is 102.6220613717781
At time: 102.01018214225769 and batch: 250, loss is 4.628530616760254 and perplexity is 102.36354227706337
At time: 103.20749545097351 and batch: 300, loss is 4.655803461074829 and perplexity is 105.19370510147508
At time: 104.40208959579468 and batch: 350, loss is 4.630834331512451 and perplexity is 102.59963051506055
At time: 105.59582662582397 and batch: 400, loss is 4.63169529914856 and perplexity is 102.68800351409776
At time: 106.80230665206909 and batch: 450, loss is 4.578617153167724 and perplexity is 97.37963992966013
At time: 107.99676179885864 and batch: 500, loss is 4.587692041397094 and perplexity is 98.26737121662792
At time: 109.1892454624176 and batch: 550, loss is 4.600327749252319 and perplexity is 99.51692689586804
At time: 110.38082265853882 and batch: 600, loss is 4.594914636611938 and perplexity is 98.97968594568812
At time: 111.57305526733398 and batch: 650, loss is 4.607402181625366 and perplexity is 100.22344883937991
At time: 112.76476049423218 and batch: 700, loss is 4.610087404251098 and perplexity is 100.49293276207126
At time: 113.96411991119385 and batch: 750, loss is 4.610787086486816 and perplexity is 100.56327048609853
At time: 115.15587544441223 and batch: 800, loss is 4.632115392684937 and perplexity is 102.73115114302294
At time: 116.34915375709534 and batch: 850, loss is 4.670317306518554 and perplexity is 106.73160369355502
At time: 117.54448199272156 and batch: 900, loss is 4.629865493774414 and perplexity is 102.50027625795727
At time: 118.73899292945862 and batch: 950, loss is 4.633965997695923 and perplexity is 102.92144194836355
At time: 119.93323349952698 and batch: 1000, loss is 4.617820997238159 and perplexity is 101.27311712783403
At time: 121.12864851951599 and batch: 1050, loss is 4.598157215118408 and perplexity is 99.3011562626128
At time: 122.37219047546387 and batch: 1100, loss is 4.555959911346435 and perplexity is 95.19809312017371
At time: 123.57284140586853 and batch: 1150, loss is 4.5807750797271725 and perplexity is 97.59000493549753
At time: 124.77244591712952 and batch: 1200, loss is 4.578943605422974 and perplexity is 97.4114349222217
At time: 125.96651577949524 and batch: 1250, loss is 4.610999383926392 and perplexity is 100.58462207730196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.826715845261177 and perplexity of 124.80042298858282
Finished 4 epochs...
Completing Train Step...
At time: 128.9240117073059 and batch: 50, loss is 4.549486875534058 and perplexity is 94.58386256701729
At time: 130.14888334274292 and batch: 100, loss is 4.542015657424927 and perplexity is 93.87983913132258
At time: 131.34612798690796 and batch: 150, loss is 4.478792381286621 and perplexity is 88.12818314607539
At time: 132.54260683059692 and batch: 200, loss is 4.521464614868164 and perplexity is 91.97020030660687
At time: 133.73760509490967 and batch: 250, loss is 4.5162612819671635 and perplexity is 91.49289161237044
At time: 134.94429850578308 and batch: 300, loss is 4.539081802368164 and perplexity is 93.60481293133695
At time: 136.14181423187256 and batch: 350, loss is 4.510708932876587 and perplexity is 90.98629883027944
At time: 137.3380687236786 and batch: 400, loss is 4.514008378982544 and perplexity is 91.28699901888983
At time: 138.53502893447876 and batch: 450, loss is 4.44958646774292 and perplexity is 85.5915418192573
At time: 139.73152494430542 and batch: 500, loss is 4.472618103027344 and perplexity is 87.58573156760491
At time: 140.9283905029297 and batch: 550, loss is 4.482243537902832 and perplexity is 88.43285273725553
At time: 142.1250524520874 and batch: 600, loss is 4.48208288192749 and perplexity is 88.41864661222677
At time: 143.32259106636047 and batch: 650, loss is 4.496705236434937 and perplexity is 89.72103418892485
At time: 144.51888513565063 and batch: 700, loss is 4.4870922660827635 and perplexity is 88.86268082001266
At time: 145.71576499938965 and batch: 750, loss is 4.494537124633789 and perplexity is 89.5267196797968
At time: 146.9131531715393 and batch: 800, loss is 4.517913026809692 and perplexity is 91.64413940120986
At time: 148.11466360092163 and batch: 850, loss is 4.562561941146851 and perplexity is 95.8286730311662
At time: 149.31838274002075 and batch: 900, loss is 4.51695592880249 and perplexity is 91.55646893932185
At time: 150.51657891273499 and batch: 950, loss is 4.514128732681274 and perplexity is 91.29798640804097
At time: 151.713520526886 and batch: 1000, loss is 4.498924388885498 and perplexity is 89.92035992711165
At time: 152.9729404449463 and batch: 1050, loss is 4.4872727203369145 and perplexity is 88.87871791573944
At time: 154.16823625564575 and batch: 1100, loss is 4.437184543609619 and perplexity is 84.53659720422056
At time: 155.36660194396973 and batch: 1150, loss is 4.461500949859619 and perplexity is 86.6174199814396
At time: 156.56457495689392 and batch: 1200, loss is 4.458167161941528 and perplexity is 86.32913667790814
At time: 157.76287412643433 and batch: 1250, loss is 4.491582975387574 and perplexity is 89.26263465368477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.80361916200958 and perplexity of 121.95098015848497
Finished 5 epochs...
Completing Train Step...
At time: 160.7929229736328 and batch: 50, loss is 4.446536474227905 and perplexity is 85.3308858733026
At time: 161.98697638511658 and batch: 100, loss is 4.4445466041564945 and perplexity is 85.16125732269595
At time: 163.18161153793335 and batch: 150, loss is 4.374468965530395 and perplexity is 79.39766545813916
At time: 164.37667751312256 and batch: 200, loss is 4.415907487869263 and perplexity is 82.75690772970307
At time: 165.57191920280457 and batch: 250, loss is 4.41677342414856 and perplexity is 82.82860097487973
At time: 166.77312350273132 and batch: 300, loss is 4.43567274093628 and perplexity is 84.40889110810417
At time: 167.97604608535767 and batch: 350, loss is 4.401540107727051 and perplexity is 81.57640842473678
At time: 169.17113494873047 and batch: 400, loss is 4.418219833374024 and perplexity is 82.94849171209428
At time: 170.36508512496948 and batch: 450, loss is 4.360037631988526 and perplexity is 78.26007945008217
At time: 171.55920028686523 and batch: 500, loss is 4.382798442840576 and perplexity is 80.06176848698067
At time: 172.75430965423584 and batch: 550, loss is 4.393722095489502 and perplexity is 80.9411296099043
At time: 173.9490146636963 and batch: 600, loss is 4.391857662200928 and perplexity is 80.79036086625113
At time: 175.14337015151978 and batch: 650, loss is 4.405576143264771 and perplexity is 81.90631902579156
At time: 176.3380811214447 and batch: 700, loss is 4.406258935928345 and perplexity is 81.96226315647027
At time: 177.53209805488586 and batch: 750, loss is 4.397152128219605 and perplexity is 81.21923701977296
At time: 178.7370903491974 and batch: 800, loss is 4.422098197937012 and perplexity is 83.27082085383252
At time: 179.9340853691101 and batch: 850, loss is 4.4746150875091555 and perplexity is 87.76081367431378
At time: 181.13010144233704 and batch: 900, loss is 4.426633701324463 and perplexity is 83.64935371353151
At time: 182.32748198509216 and batch: 950, loss is 4.43536153793335 and perplexity is 84.38262689468034
At time: 183.5755639076233 and batch: 1000, loss is 4.409725284576416 and perplexity is 82.24686591778195
At time: 184.76928877830505 and batch: 1050, loss is 4.398243360519409 and perplexity is 81.30791444961257
At time: 185.96494269371033 and batch: 1100, loss is 4.34253306388855 and perplexity is 76.90209073770829
At time: 187.16025733947754 and batch: 1150, loss is 4.368878135681152 and perplexity is 78.95500519228196
At time: 188.3551149368286 and batch: 1200, loss is 4.372805223464966 and perplexity is 79.26567804916729
At time: 189.55882906913757 and batch: 1250, loss is 4.423150129318238 and perplexity is 83.35846213165262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.814872351876141 and perplexity of 123.33106832586952
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 192.58982610702515 and batch: 50, loss is 4.385342025756836 and perplexity is 80.26567144565637
At time: 193.8117015361786 and batch: 100, loss is 4.3856515789031985 and perplexity is 80.29052178284884
At time: 195.00681495666504 and batch: 150, loss is 4.303072566986084 and perplexity is 73.92658949681265
At time: 196.20187211036682 and batch: 200, loss is 4.352201929092407 and perplexity is 77.64925297159068
At time: 197.3975100517273 and batch: 250, loss is 4.342881965637207 and perplexity is 76.92892669292567
At time: 198.59246063232422 and batch: 300, loss is 4.339370794296265 and perplexity is 76.6592896980907
At time: 199.78839755058289 and batch: 350, loss is 4.296511545181274 and perplexity is 73.4431432162048
At time: 200.9826958179474 and batch: 400, loss is 4.290242538452149 and perplexity is 72.98416782025761
At time: 202.18251943588257 and batch: 450, loss is 4.225837507247925 and perplexity is 68.43179167711617
At time: 203.3877148628235 and batch: 500, loss is 4.2261801528930665 and perplexity is 68.45524355013521
At time: 204.58815598487854 and batch: 550, loss is 4.235574297904968 and perplexity is 69.10135210200418
At time: 205.78298807144165 and batch: 600, loss is 4.211315088272094 and perplexity is 67.44517787633576
At time: 206.98349475860596 and batch: 650, loss is 4.219666004180908 and perplexity is 68.01076518436301
At time: 208.18852996826172 and batch: 700, loss is 4.198117237091065 and perplexity is 66.56089461089115
At time: 209.3913218975067 and batch: 750, loss is 4.184593195915222 and perplexity is 65.66678196727878
At time: 210.58779168128967 and batch: 800, loss is 4.172374591827393 and perplexity is 64.86930749065597
At time: 211.7925763130188 and batch: 850, loss is 4.2081834411621095 and perplexity is 67.234293759623
At time: 212.98798298835754 and batch: 900, loss is 4.141757011413574 and perplexity is 62.91326371904482
At time: 214.21095848083496 and batch: 950, loss is 4.12071771144867 and perplexity is 61.60343989604902
At time: 215.40876078605652 and batch: 1000, loss is 4.07477608203888 and perplexity is 58.837304383972565
At time: 216.6068398952484 and batch: 1050, loss is 4.04558723449707 and perplexity is 57.14473354191561
At time: 217.80291533470154 and batch: 1100, loss is 3.973962745666504 and perplexity is 53.1949116271228
At time: 218.99902939796448 and batch: 1150, loss is 3.966576657295227 and perplexity is 52.80345674873252
At time: 220.1955828666687 and batch: 1200, loss is 3.9403168392181396 and perplexity is 51.434895311115284
At time: 221.39192032814026 and batch: 1250, loss is 3.9581814575195313 and perplexity is 52.36201676161832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.622795606181569 and perplexity of 101.77816645194112
Finished 7 epochs...
Completing Train Step...
At time: 224.40378308296204 and batch: 50, loss is 4.216807560920715 and perplexity is 67.81663785416016
At time: 225.59712648391724 and batch: 100, loss is 4.227281723022461 and perplexity is 68.53069335062412
At time: 226.79704546928406 and batch: 150, loss is 4.155198721885681 and perplexity is 63.764634717302044
At time: 227.9910991191864 and batch: 200, loss is 4.201066427230835 and perplexity is 66.75748509383801
At time: 229.1857237815857 and batch: 250, loss is 4.200500979423523 and perplexity is 66.71974789048497
At time: 230.3815712928772 and batch: 300, loss is 4.209563789367675 and perplexity is 67.32716457865475
At time: 231.57607102394104 and batch: 350, loss is 4.170952401161194 and perplexity is 64.77711653911435
At time: 232.77046251296997 and batch: 400, loss is 4.169998111724854 and perplexity is 64.71532990693863
At time: 233.9651939868927 and batch: 450, loss is 4.107767176628113 and perplexity is 60.81078612611882
At time: 235.17181968688965 and batch: 500, loss is 4.120432171821594 and perplexity is 61.58585218390842
At time: 236.36700582504272 and batch: 550, loss is 4.129596872329712 and perplexity is 62.15286234519269
At time: 237.561838388443 and batch: 600, loss is 4.114584245681763 and perplexity is 61.22675368321032
At time: 238.75659155845642 and batch: 650, loss is 4.128377666473389 and perplexity is 62.077131386629226
At time: 239.9517891407013 and batch: 700, loss is 4.107230906486511 and perplexity is 60.77818385982436
At time: 241.14675426483154 and batch: 750, loss is 4.099570102691651 and perplexity is 60.314353045135356
At time: 242.34194231033325 and batch: 800, loss is 4.099330892562866 and perplexity is 60.29992696647675
At time: 243.5444827079773 and batch: 850, loss is 4.140023112297058 and perplexity is 62.80427298344117
At time: 244.78530287742615 and batch: 900, loss is 4.079539079666137 and perplexity is 59.11821478186263
At time: 245.98635482788086 and batch: 950, loss is 4.065001029968261 and perplexity is 58.26496853358941
At time: 247.19283366203308 and batch: 1000, loss is 4.03316113948822 and perplexity is 56.4390412338471
At time: 248.39010977745056 and batch: 1050, loss is 4.015451469421387 and perplexity is 55.448322974767265
At time: 249.5858860015869 and batch: 1100, loss is 3.9514424657821654 and perplexity is 52.01033588151812
At time: 250.78156852722168 and batch: 1150, loss is 3.9533314514160156 and perplexity is 52.10867551064152
At time: 251.97724175453186 and batch: 1200, loss is 3.9418832540512083 and perplexity is 51.51552682878377
At time: 253.17166304588318 and batch: 1250, loss is 3.97625563621521 and perplexity is 53.31702167622272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613502669508439 and perplexity of 100.83672952834614
Finished 8 epochs...
Completing Train Step...
At time: 256.136257648468 and batch: 50, loss is 4.158056602478028 and perplexity is 63.94712707581493
At time: 257.3582775592804 and batch: 100, loss is 4.163249344825744 and perplexity is 64.28005167834026
At time: 258.5566620826721 and batch: 150, loss is 4.0951288270950315 and perplexity is 60.04707434934231
At time: 259.7515618801117 and batch: 200, loss is 4.1402694272995 and perplexity is 62.81974452345242
At time: 260.9459261894226 and batch: 250, loss is 4.139106397628784 and perplexity is 62.74672576638602
At time: 262.14084458351135 and batch: 300, loss is 4.146898822784424 and perplexity is 63.23758493854033
At time: 263.33551359176636 and batch: 350, loss is 4.1112034177780155 and perplexity is 61.02010608272101
At time: 264.52956199645996 and batch: 400, loss is 4.114092268943787 and perplexity is 61.19663895313714
At time: 265.72425055503845 and batch: 450, loss is 4.053347215652466 and perplexity is 57.589900607456215
At time: 266.91903948783875 and batch: 500, loss is 4.069458785057068 and perplexity is 58.52527926324377
At time: 268.1197397708893 and batch: 550, loss is 4.077543749809265 and perplexity is 59.00037204948223
At time: 269.31594252586365 and batch: 600, loss is 4.066028347015381 and perplexity is 58.32485588539558
At time: 270.51250195503235 and batch: 650, loss is 4.083333888053894 and perplexity is 59.34298328615593
At time: 271.7106902599335 and batch: 700, loss is 4.063354229927063 and perplexity is 58.16909674350841
At time: 272.9045498371124 and batch: 750, loss is 4.055959234237671 and perplexity is 57.74052312693615
At time: 274.0980408191681 and batch: 800, loss is 4.061449522972107 and perplexity is 58.058407109499754
At time: 275.3248932361603 and batch: 850, loss is 4.109055671691895 and perplexity is 60.88919102518228
At time: 276.51970958709717 and batch: 900, loss is 4.0468166542053225 and perplexity is 57.21503160761413
At time: 277.7212293148041 and batch: 950, loss is 4.03317241191864 and perplexity is 56.439677442598196
At time: 278.91460704803467 and batch: 1000, loss is 4.006097321510315 and perplexity is 54.932069480360035
At time: 280.1089425086975 and batch: 1050, loss is 3.9934905910491945 and perplexity is 54.2439025682892
At time: 281.30426836013794 and batch: 1100, loss is 3.9344222211837767 and perplexity is 51.13259808826385
At time: 282.49918603897095 and batch: 1150, loss is 3.9396636962890623 and perplexity is 51.40131194149908
At time: 283.6944959163666 and batch: 1200, loss is 3.937747368812561 and perplexity is 51.30290451563961
At time: 284.88937520980835 and batch: 1250, loss is 3.9719078588485717 and perplexity is 53.08571433689961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.612780939923586 and perplexity of 100.76397893369185
Finished 9 epochs...
Completing Train Step...
At time: 287.948673248291 and batch: 50, loss is 4.116062116622925 and perplexity is 61.31730581900425
At time: 289.14584851264954 and batch: 100, loss is 4.121083083152771 and perplexity is 61.625952162284456
At time: 290.347594499588 and batch: 150, loss is 4.051003999710083 and perplexity is 57.45511301412409
At time: 291.54467034339905 and batch: 200, loss is 4.0981618642807005 and perplexity is 60.22947583414687
At time: 292.74808526039124 and batch: 250, loss is 4.09805956363678 and perplexity is 60.22331463513873
At time: 293.9461131095886 and batch: 300, loss is 4.105455965995788 and perplexity is 60.67040188193381
At time: 295.1441423892975 and batch: 350, loss is 4.069990406036377 and perplexity is 58.556400801218274
At time: 296.3418357372284 and batch: 400, loss is 4.076604051589966 and perplexity is 58.944955546401864
At time: 297.5456163883209 and batch: 450, loss is 4.012427854537964 and perplexity is 55.28092180602778
At time: 298.74270272254944 and batch: 500, loss is 4.032981505393982 and perplexity is 56.428903768339886
At time: 299.9410183429718 and batch: 550, loss is 4.041186876296997 and perplexity is 56.89382868556347
At time: 301.13894152641296 and batch: 600, loss is 4.033035097122192 and perplexity is 56.43192797184924
At time: 302.33621549606323 and batch: 650, loss is 4.051180019378662 and perplexity is 57.46522713419089
At time: 303.533650636673 and batch: 700, loss is 4.029658961296081 and perplexity is 56.241727370283016
At time: 304.7338445186615 and batch: 750, loss is 4.025342555046081 and perplexity is 55.99948840374909
At time: 305.9586730003357 and batch: 800, loss is 4.0359492683410645 and perplexity is 56.596620126185826
At time: 307.164901971817 and batch: 850, loss is 4.0794285821914675 and perplexity is 59.111682729316556
At time: 308.36334013938904 and batch: 900, loss is 4.021305346488953 and perplexity is 55.77386254575021
At time: 309.5657904148102 and batch: 950, loss is 4.009514832496643 and perplexity is 55.120121583347256
At time: 310.7645573616028 and batch: 1000, loss is 3.982959713935852 and perplexity is 53.67566397303553
At time: 311.9618089199066 and batch: 1050, loss is 3.9755931282043457 and perplexity is 53.281710420533244
At time: 313.1590745449066 and batch: 1100, loss is 3.916262788772583 and perplexity is 50.21243919707696
At time: 314.3565864562988 and batch: 1150, loss is 3.9229941177368164 and perplexity is 50.55157578282572
At time: 315.5543601512909 and batch: 1200, loss is 3.9278375625610353 and perplexity is 50.797013453004666
At time: 316.7528986930847 and batch: 1250, loss is 3.962343397140503 and perplexity is 52.58039844427208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.614132623602874 and perplexity of 100.9002720513168
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 319.70805072784424 and batch: 50, loss is 4.101704497337341 and perplexity is 60.44322516038496
At time: 320.93040704727173 and batch: 100, loss is 4.1213229417800905 and perplexity is 61.64073545145619
At time: 322.12504386901855 and batch: 150, loss is 4.058905582427979 and perplexity is 57.910897680886684
At time: 323.3197703361511 and batch: 200, loss is 4.112373728752136 and perplexity is 61.091560386233894
At time: 324.5147502422333 and batch: 250, loss is 4.113250217437744 and perplexity is 61.145129920814576
At time: 325.7103657722473 and batch: 300, loss is 4.114163908958435 and perplexity is 61.201023238291434
At time: 326.90587520599365 and batch: 350, loss is 4.078570671081543 and perplexity is 59.06099190719558
At time: 328.1042799949646 and batch: 400, loss is 4.078597593307495 and perplexity is 59.06258198196871
At time: 329.29825592041016 and batch: 450, loss is 4.013097524642944 and perplexity is 55.31795418509168
At time: 330.4993591308594 and batch: 500, loss is 4.020230865478515 and perplexity is 55.71396677376336
At time: 331.6943953037262 and batch: 550, loss is 4.02504346370697 and perplexity is 55.982741946257896
At time: 332.89084362983704 and batch: 600, loss is 4.015014591217041 and perplexity is 55.42410410172617
At time: 334.08553552627563 and batch: 650, loss is 4.034561328887939 and perplexity is 56.5181219321708
At time: 335.2798218727112 and batch: 700, loss is 4.011192760467529 and perplexity is 55.21268681428798
At time: 336.52040219306946 and batch: 750, loss is 3.998083276748657 and perplexity is 54.49360071755547
At time: 337.71431612968445 and batch: 800, loss is 4.002071242332459 and perplexity is 54.71135322792087
At time: 338.9095368385315 and batch: 850, loss is 4.036880736351013 and perplexity is 56.64936262746751
At time: 340.1049153804779 and batch: 900, loss is 3.9686895561218263 and perplexity is 52.91514305992475
At time: 341.3005955219269 and batch: 950, loss is 3.949718351364136 and perplexity is 51.92074136932718
At time: 342.49491810798645 and batch: 1000, loss is 3.921217579841614 and perplexity is 50.461848718149504
At time: 343.68994903564453 and batch: 1050, loss is 3.904758987426758 and perplexity is 49.63811506055359
At time: 344.884948015213 and batch: 1100, loss is 3.837993035316467 and perplexity is 46.43219309319878
At time: 346.08116698265076 and batch: 1150, loss is 3.8356791734695435 and perplexity is 46.32487961529183
At time: 347.27560210227966 and batch: 1200, loss is 3.8376922178268433 and perplexity is 46.418227578075665
At time: 348.47415709495544 and batch: 1250, loss is 3.874978976249695 and perplexity is 48.18168532072917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.592806405394617 and perplexity of 98.77123369187905
Finished 11 epochs...
Completing Train Step...
At time: 351.5229666233063 and batch: 50, loss is 4.083437476158142 and perplexity is 59.34913083169577
At time: 352.7206299304962 and batch: 100, loss is 4.09600938796997 and perplexity is 60.099972740377574
At time: 353.9191699028015 and batch: 150, loss is 4.03008216381073 and perplexity is 56.26553404790189
At time: 355.1179840564728 and batch: 200, loss is 4.080597906112671 and perplexity is 59.18084386195751
At time: 356.31649708747864 and batch: 250, loss is 4.079559621810913 and perplexity is 59.11942920926298
At time: 357.51458263397217 and batch: 300, loss is 4.082222347259521 and perplexity is 59.27705778560292
At time: 358.7131435871124 and batch: 350, loss is 4.046670269966126 and perplexity is 57.206656841723444
At time: 359.91195249557495 and batch: 400, loss is 4.050238423347473 and perplexity is 57.411143570820165
At time: 361.109760761261 and batch: 450, loss is 3.9889652061462404 and perplexity is 53.99898262701006
At time: 362.3087067604065 and batch: 500, loss is 3.9982905530929567 and perplexity is 54.50489712259825
At time: 363.5083634853363 and batch: 550, loss is 4.002792286872864 and perplexity is 54.75081677624227
At time: 364.70563983917236 and batch: 600, loss is 3.995020990371704 and perplexity is 54.326980955368654
At time: 365.90309166908264 and batch: 650, loss is 4.0165450525283815 and perplexity is 55.50899349216367
At time: 367.1404342651367 and batch: 700, loss is 3.993887038230896 and perplexity is 54.265411673917335
At time: 368.3384563922882 and batch: 750, loss is 3.982354736328125 and perplexity is 53.64320121886176
At time: 369.53716111183167 and batch: 800, loss is 3.9870323514938355 and perplexity is 53.894711245389374
At time: 370.73642444610596 and batch: 850, loss is 4.02410014629364 and perplexity is 55.92995735116209
At time: 371.93587136268616 and batch: 900, loss is 3.9614995956420898 and perplexity is 52.53604973866113
At time: 373.135009765625 and batch: 950, loss is 3.944719686508179 and perplexity is 51.661854567387635
At time: 374.33247923851013 and batch: 1000, loss is 3.9189951705932615 and perplexity is 50.34982636473395
At time: 375.52950954437256 and batch: 1050, loss is 3.9048000717163087 and perplexity is 49.64015444913862
At time: 376.72680950164795 and batch: 1100, loss is 3.841156325340271 and perplexity is 46.579304141178035
At time: 377.9308159351349 and batch: 1150, loss is 3.8424560737609865 and perplexity is 46.63988487950456
At time: 379.1291332244873 and batch: 1200, loss is 3.8478313732147216 and perplexity is 46.89126323892502
At time: 380.3264126777649 and batch: 1250, loss is 3.8862720346450805 and perplexity is 48.7288878863809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.591410170506387 and perplexity of 98.6334220805231
Finished 12 epochs...
Completing Train Step...
At time: 383.31048464775085 and batch: 50, loss is 4.074591722488403 and perplexity is 58.82645816481785
At time: 384.53256821632385 and batch: 100, loss is 4.085217809677124 and perplexity is 59.454886190609585
At time: 385.7259736061096 and batch: 150, loss is 4.018657774925232 and perplexity is 55.62639255809066
At time: 386.92140769958496 and batch: 200, loss is 4.06807689666748 and perplexity is 58.44445971399207
At time: 388.1160752773285 and batch: 250, loss is 4.06702043056488 and perplexity is 58.382747727470225
At time: 389.31334948539734 and batch: 300, loss is 4.069981751441955 and perplexity is 58.55589402151154
At time: 390.5119650363922 and batch: 350, loss is 4.034095969200134 and perplexity is 56.491826795415975
At time: 391.70804929733276 and batch: 400, loss is 4.038117842674255 and perplexity is 56.71948727906313
At time: 392.90460729599 and batch: 450, loss is 3.977630167007446 and perplexity is 53.39035795420675
At time: 394.0996935367584 and batch: 500, loss is 3.9882904529571532 and perplexity is 53.96255893115944
At time: 395.2981913089752 and batch: 550, loss is 3.9922685718536375 and perplexity is 54.17765596366408
At time: 396.49876141548157 and batch: 600, loss is 3.9853598070144653 and perplexity is 53.804645284287574
At time: 397.7414255142212 and batch: 650, loss is 4.0077838659286495 and perplexity is 55.024793024739
At time: 398.9444990158081 and batch: 700, loss is 3.9854981660842896 and perplexity is 53.812090159982496
At time: 400.14182710647583 and batch: 750, loss is 3.9750101518630983 and perplexity is 53.250657496376405
At time: 401.34336614608765 and batch: 800, loss is 3.9808153676986695 and perplexity is 53.56068808308222
At time: 402.54235553741455 and batch: 850, loss is 4.018688039779663 and perplexity is 55.628076108240045
At time: 403.74150133132935 and batch: 900, loss is 3.957750368118286 and perplexity is 52.3394489158936
At time: 404.9401273727417 and batch: 950, loss is 3.94218017578125 and perplexity is 51.530825179227726
At time: 406.1360373497009 and batch: 1000, loss is 3.9175505685806273 and perplexity is 50.27714341583882
At time: 407.3320686817169 and batch: 1050, loss is 3.905414366722107 and perplexity is 49.670657516085406
At time: 408.53091979026794 and batch: 1100, loss is 3.8426507091522217 and perplexity is 46.648963535230436
At time: 409.7269380092621 and batch: 1150, loss is 3.8440644359588623 and perplexity is 46.7149590643148
At time: 410.92945885658264 and batch: 1200, loss is 3.8522101163864138 and perplexity is 47.097038226772355
At time: 412.12792921066284 and batch: 1250, loss is 3.8908051204681398 and perplexity is 48.95028153632445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.591230183622263 and perplexity of 98.61567095574509
Finished 13 epochs...
Completing Train Step...
At time: 415.200679063797 and batch: 50, loss is 4.066792545318603 and perplexity is 58.36944467646813
At time: 416.3988256454468 and batch: 100, loss is 4.076535420417786 and perplexity is 58.940910223827814
At time: 417.6016752719879 and batch: 150, loss is 4.009524292945862 and perplexity is 55.12064304692506
At time: 418.7977797985077 and batch: 200, loss is 4.058556070327759 and perplexity is 57.89066065816079
At time: 419.9991137981415 and batch: 250, loss is 4.057827005386352 and perplexity is 57.84846998877782
At time: 421.1994216442108 and batch: 300, loss is 4.061035833358765 and perplexity is 58.03439391685801
At time: 422.43006801605225 and batch: 350, loss is 4.025080122947693 and perplexity is 55.9847942686893
At time: 423.66667890548706 and batch: 400, loss is 4.029362649917602 and perplexity is 56.225064775298044
At time: 424.87819719314575 and batch: 450, loss is 3.9693926906585695 and perplexity is 52.95236260816836
At time: 426.07907366752625 and batch: 500, loss is 3.9809560203552246 and perplexity is 53.568222065972954
At time: 427.28675866127014 and batch: 550, loss is 3.984653797149658 and perplexity is 53.766672080250046
At time: 428.5494291782379 and batch: 600, loss is 3.978194875717163 and perplexity is 53.42051646894492
At time: 429.7524502277374 and batch: 650, loss is 4.001241984367371 and perplexity is 54.66600220892161
At time: 430.96015453338623 and batch: 700, loss is 3.9795006370544432 and perplexity is 53.49031647512482
At time: 432.1642220020294 and batch: 750, loss is 3.96950740814209 and perplexity is 52.95843751839578
At time: 433.36770272254944 and batch: 800, loss is 3.9763774490356445 and perplexity is 53.323516768595
At time: 434.5668487548828 and batch: 850, loss is 4.0146014642715455 and perplexity is 55.401211639963776
At time: 435.7689118385315 and batch: 900, loss is 3.954690890312195 and perplexity is 52.179562243132764
At time: 436.96963453292847 and batch: 950, loss is 3.9400112056732177 and perplexity is 51.419177483798656
At time: 438.17488265037537 and batch: 1000, loss is 3.9160370016098023 and perplexity is 50.20110315270916
At time: 439.37926983833313 and batch: 1050, loss is 3.9053162956237792 and perplexity is 49.66578649900505
At time: 440.5814120769501 and batch: 1100, loss is 3.8430339431762697 and perplexity is 46.666844431308746
At time: 441.7806577682495 and batch: 1150, loss is 3.8449736261367797 and perplexity is 46.75745116002883
At time: 442.9868538379669 and batch: 1200, loss is 3.854339962005615 and perplexity is 47.197454544985334
At time: 444.1862144470215 and batch: 1250, loss is 3.892999300956726 and perplexity is 49.05780520901584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5913731930029655 and perplexity of 98.62977493025237
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 447.26184368133545 and batch: 50, loss is 4.064674859046936 and perplexity is 58.24596729411617
At time: 448.50839591026306 and batch: 100, loss is 4.079756402969361 and perplexity is 59.13106394374028
At time: 449.70784854888916 and batch: 150, loss is 4.014474015235901 and perplexity is 55.39415125889568
At time: 450.90358781814575 and batch: 200, loss is 4.065483708381652 and perplexity is 58.29309856449069
At time: 452.11021733283997 and batch: 250, loss is 4.06584445476532 and perplexity is 58.31413138251902
At time: 453.33650493621826 and batch: 300, loss is 4.07244915008545 and perplexity is 58.70055314746333
At time: 454.5428216457367 and batch: 350, loss is 4.035795574188232 and perplexity is 56.58792222502776
At time: 455.7442021369934 and batch: 400, loss is 4.039707736968994 and perplexity is 56.80973699303046
At time: 456.9441828727722 and batch: 450, loss is 3.9784041023254395 and perplexity is 53.431694631761864
At time: 458.1434979438782 and batch: 500, loss is 3.987949938774109 and perplexity is 53.94418704261185
At time: 459.36604619026184 and batch: 550, loss is 3.986154170036316 and perplexity is 53.8474026851059
At time: 460.5641167163849 and batch: 600, loss is 3.978036651611328 and perplexity is 53.412064724145885
At time: 461.7712700366974 and batch: 650, loss is 4.00106367111206 and perplexity is 54.656255405130906
At time: 462.9735791683197 and batch: 700, loss is 3.9777630138397218 and perplexity is 53.39745116527991
At time: 464.1978871822357 and batch: 750, loss is 3.9657022953033447 and perplexity is 52.757307591586034
At time: 465.3957824707031 and batch: 800, loss is 3.9724565172195434 and perplexity is 53.11484825000138
At time: 466.5897419452667 and batch: 850, loss is 4.009730606079102 and perplexity is 55.13201633268703
At time: 467.7837378978729 and batch: 900, loss is 3.94661452293396 and perplexity is 51.75983813334356
At time: 468.9781355857849 and batch: 950, loss is 3.9309740829467774 and perplexity is 50.95658944709159
At time: 470.17279171943665 and batch: 1000, loss is 3.907631349563599 and perplexity is 49.78089866774196
At time: 471.3670382499695 and batch: 1050, loss is 3.894055666923523 and perplexity is 49.10965558650978
At time: 472.5610463619232 and batch: 1100, loss is 3.8260256624221802 and perplexity is 45.8798334632894
At time: 473.7581753730774 and batch: 1150, loss is 3.8235130643844606 and perplexity is 45.764700585650765
At time: 474.9530072212219 and batch: 1200, loss is 3.834070129394531 and perplexity is 46.25040077815594
At time: 476.1477255821228 and batch: 1250, loss is 3.875079183578491 and perplexity is 48.18651372062855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.58970608676437 and perplexity of 98.46548559908936
Finished 15 epochs...
Completing Train Step...
At time: 479.1965262889862 and batch: 50, loss is 4.063315739631653 and perplexity is 58.16685784087933
At time: 480.3902018070221 and batch: 100, loss is 4.0761097717285155 and perplexity is 58.91582744125212
At time: 481.5847969055176 and batch: 150, loss is 4.0097750520706175 and perplexity is 55.134466784273165
At time: 482.7834644317627 and batch: 200, loss is 4.059994702339172 and perplexity is 57.974003951536
At time: 483.9784014225006 and batch: 250, loss is 4.059966249465942 and perplexity is 57.972354448017605
At time: 485.17650628089905 and batch: 300, loss is 4.066185011863708 and perplexity is 58.33399405589292
At time: 486.374534368515 and batch: 350, loss is 4.029730043411255 and perplexity is 56.24572529331424
At time: 487.57380414009094 and batch: 400, loss is 4.032703585624695 and perplexity is 56.41322323948888
At time: 488.78664350509644 and batch: 450, loss is 3.971428370475769 and perplexity is 53.060266455582315
At time: 490.0380311012268 and batch: 500, loss is 3.98150577545166 and perplexity is 53.597679565524054
At time: 491.24732398986816 and batch: 550, loss is 3.9798461866378783 and perplexity is 53.50880322556112
At time: 492.46059703826904 and batch: 600, loss is 3.9727546119689943 and perplexity is 53.13068386752254
At time: 493.6678102016449 and batch: 650, loss is 3.996875958442688 and perplexity is 54.427849295285576
At time: 494.8774700164795 and batch: 700, loss is 3.9744092988967896 and perplexity is 53.21867129132723
At time: 496.07015204429626 and batch: 750, loss is 3.963258762359619 and perplexity is 52.62855074732475
At time: 497.26495838165283 and batch: 800, loss is 3.9708454990386963 and perplexity is 53.02934815339484
At time: 498.45970582962036 and batch: 850, loss is 4.0079323053359985 and perplexity is 55.03296147865032
At time: 499.6529748439789 and batch: 900, loss is 3.945174217224121 and perplexity is 51.68534180456205
At time: 500.84625697135925 and batch: 950, loss is 3.930009880065918 and perplexity is 50.90748063598116
At time: 502.0398802757263 and batch: 1000, loss is 3.9073730373382567 and perplexity is 49.76804131370493
At time: 503.23356223106384 and batch: 1050, loss is 3.894808125495911 and perplexity is 49.146622474124406
At time: 504.42751455307007 and batch: 1100, loss is 3.8283277702331544 and perplexity is 45.985575454303095
At time: 505.6220266819 and batch: 1150, loss is 3.827238836288452 and perplexity is 45.935527454652565
At time: 506.8251016139984 and batch: 1200, loss is 3.8380655002593995 and perplexity is 46.43555792133607
At time: 508.0214419364929 and batch: 1250, loss is 3.878687062263489 and perplexity is 48.36067881072119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589457491018476 and perplexity of 98.44101054057576
Finished 16 epochs...
Completing Train Step...
At time: 511.1809506416321 and batch: 50, loss is 4.062566165924072 and perplexity is 58.123273830344075
At time: 512.4151964187622 and batch: 100, loss is 4.074371299743652 and perplexity is 58.813492904416826
At time: 513.6110224723816 and batch: 150, loss is 4.007360997200013 and perplexity is 55.00152967948639
At time: 514.8127551078796 and batch: 200, loss is 4.056927275657654 and perplexity is 57.79644540810968
At time: 516.0254328250885 and batch: 250, loss is 4.056581034660339 and perplexity is 57.77643737321118
At time: 517.2231686115265 and batch: 300, loss is 4.0630566692352295 and perplexity is 58.1517904817974
At time: 518.4317378997803 and batch: 350, loss is 4.026589851379395 and perplexity is 56.0693799389692
At time: 519.6344156265259 and batch: 400, loss is 4.029185194969177 and perplexity is 56.215088244546685
At time: 520.8876979351044 and batch: 450, loss is 3.9681741189956665 and perplexity is 52.8878756585748
At time: 522.0846030712128 and batch: 500, loss is 3.978731656074524 and perplexity is 53.44919925035364
At time: 523.2820298671722 and batch: 550, loss is 3.9773711585998535 and perplexity is 53.37653119331319
At time: 524.4792492389679 and batch: 600, loss is 3.9707141494750977 and perplexity is 53.02238322908687
At time: 525.6768267154694 and batch: 650, loss is 3.9951076889038086 and perplexity is 54.33169122905512
At time: 526.8741328716278 and batch: 700, loss is 3.9727972507476808 and perplexity is 53.13294934329164
At time: 528.0788342952728 and batch: 750, loss is 3.9620474100112917 and perplexity is 52.56483762609833
At time: 529.2762787342072 and batch: 800, loss is 3.970103998184204 and perplexity is 52.99004142121466
At time: 530.4787764549255 and batch: 850, loss is 4.007209973335266 and perplexity is 54.99322376311881
At time: 531.6815495491028 and batch: 900, loss is 3.944735736846924 and perplexity is 51.66268376430807
At time: 532.8818967342377 and batch: 950, loss is 3.930000038146973 and perplexity is 50.906979611148564
At time: 534.091964006424 and batch: 1000, loss is 3.9075636863708496 and perplexity is 49.777530447153744
At time: 535.2996912002563 and batch: 1050, loss is 3.8954511213302614 and perplexity is 49.17823370950488
At time: 536.5013136863708 and batch: 1100, loss is 3.8297144746780396 and perplexity is 46.04938809059367
At time: 537.706207036972 and batch: 1150, loss is 3.8291122913360596 and perplexity is 46.021666263826006
At time: 538.9111428260803 and batch: 1200, loss is 3.840161528587341 and perplexity is 46.53299024094287
At time: 540.1203379631042 and batch: 1250, loss is 3.880564556121826 and perplexity is 48.45156097683118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589350122604927 and perplexity of 98.43044165283834
Finished 17 epochs...
Completing Train Step...
At time: 543.1558504104614 and batch: 50, loss is 4.061577029228211 and perplexity is 58.06581039159799
At time: 544.3536405563354 and batch: 100, loss is 4.072988786697388 and perplexity is 58.7322386636461
At time: 545.5718476772308 and batch: 150, loss is 4.005594501495361 and perplexity is 54.90445547938047
At time: 546.8066959381104 and batch: 200, loss is 4.054730343818664 and perplexity is 57.66960993247393
At time: 548.0365719795227 and batch: 250, loss is 4.054281210899353 and perplexity is 57.643714427906474
At time: 549.2353510856628 and batch: 300, loss is 4.06093062877655 and perplexity is 58.02828875384309
At time: 550.4403116703033 and batch: 350, loss is 4.024432988166809 and perplexity is 55.948576281341005
At time: 551.6831107139587 and batch: 400, loss is 4.026937875747681 and perplexity is 56.08889684548517
At time: 552.8886380195618 and batch: 450, loss is 3.9660740232467653 and perplexity is 52.77692260253543
At time: 554.0903875827789 and batch: 500, loss is 3.97682026386261 and perplexity is 53.34713444118796
At time: 555.3046460151672 and batch: 550, loss is 3.9756380462646486 and perplexity is 53.28410378536721
At time: 556.5007770061493 and batch: 600, loss is 3.9693027925491333 and perplexity is 52.94760250484506
At time: 557.6983177661896 and batch: 650, loss is 3.9938563203811643 and perplexity is 54.26374478275769
At time: 558.8948838710785 and batch: 700, loss is 3.9716333293914796 and perplexity is 53.071142744820506
At time: 560.0907101631165 and batch: 750, loss is 3.9611183834075927 and perplexity is 52.516026170605656
At time: 561.2858052253723 and batch: 800, loss is 3.969449033737183 and perplexity is 52.955346191348895
At time: 562.4826502799988 and batch: 850, loss is 4.006708354949951 and perplexity is 54.96564506857987
At time: 563.679652929306 and batch: 900, loss is 3.944514102935791 and perplexity is 51.65123483042861
At time: 564.8796393871307 and batch: 950, loss is 3.9300243711471556 and perplexity is 50.90821834576375
At time: 566.0772557258606 and batch: 1000, loss is 3.907844648361206 and perplexity is 49.79151800607738
At time: 567.2787461280823 and batch: 1050, loss is 3.8958691596984862 and perplexity is 49.198796395773435
At time: 568.4752967357635 and batch: 1100, loss is 3.8305873394012453 and perplexity is 46.08960052443749
At time: 569.6843087673187 and batch: 1150, loss is 3.8302305793762206 and perplexity is 46.07316053014178
At time: 570.8867824077606 and batch: 1200, loss is 3.8414470529556275 and perplexity is 46.59284799988744
At time: 572.0894622802734 and batch: 1250, loss is 3.881716556549072 and perplexity is 48.50740935828049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589278395158531 and perplexity of 98.4233817418087
Finished 18 epochs...
Completing Train Step...
At time: 575.073504447937 and batch: 50, loss is 4.06050048828125 and perplexity is 58.00333380442904
At time: 576.330867767334 and batch: 100, loss is 4.071648712158203 and perplexity is 58.65358579810531
At time: 577.5253255367279 and batch: 150, loss is 4.004071106910706 and perplexity is 54.82087800612821
At time: 578.7211332321167 and batch: 200, loss is 4.052933568954468 and perplexity is 57.56608366147202
At time: 579.9237201213837 and batch: 250, loss is 4.052457218170166 and perplexity is 57.5386685424958
At time: 581.1624443531036 and batch: 300, loss is 4.0592693328857425 and perplexity is 57.9319666281149
At time: 582.3609194755554 and batch: 350, loss is 4.022719602584839 and perplexity is 55.85279687436786
At time: 583.5615208148956 and batch: 400, loss is 4.025193743705749 and perplexity is 55.99115566484022
At time: 584.7609882354736 and batch: 450, loss is 3.964428324699402 and perplexity is 52.690139126980434
At time: 585.9604911804199 and batch: 500, loss is 3.975309762954712 and perplexity is 53.26661437430749
At time: 587.1597609519958 and batch: 550, loss is 3.9742326736450195 and perplexity is 53.20927236018026
At time: 588.3589544296265 and batch: 600, loss is 3.9681118297576905 and perplexity is 52.88458141570086
At time: 589.5585923194885 and batch: 650, loss is 3.992825722694397 and perplexity is 54.20784950063082
At time: 590.7588834762573 and batch: 700, loss is 3.9706906986236574 and perplexity is 53.02113982363427
At time: 591.9593482017517 and batch: 750, loss is 3.9603197145462037 and perplexity is 52.47410000057117
At time: 593.1582651138306 and batch: 800, loss is 3.9688656663894655 and perplexity is 52.92446278055636
At time: 594.3587119579315 and batch: 850, loss is 4.006311807632446 and perplexity is 54.94385291056949
At time: 595.5585174560547 and batch: 900, loss is 3.9443299436569212 and perplexity is 51.64172365208228
At time: 596.7580502033234 and batch: 950, loss is 3.9300065994262696 and perplexity is 50.90731362715573
At time: 597.9614906311035 and batch: 1000, loss is 3.9080150270462037 and perplexity is 49.80000214217675
At time: 599.1608307361603 and batch: 1050, loss is 3.896148610115051 and perplexity is 49.212546941129055
At time: 600.3599381446838 and batch: 1100, loss is 3.8311801242828367 and perplexity is 46.116929842229894
At time: 601.5591607093811 and batch: 1150, loss is 3.8309894466400145 and perplexity is 46.108137213058924
At time: 602.7598979473114 and batch: 1200, loss is 3.8423538112640383 and perplexity is 46.63511561228214
At time: 603.9792447090149 and batch: 1250, loss is 3.8825224113464354 and perplexity is 48.54651504145444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589228497804516 and perplexity of 98.41847079800915
Finished 19 epochs...
Completing Train Step...
At time: 607.0280203819275 and batch: 50, loss is 4.059420137405396 and perplexity is 57.94070368929242
At time: 608.2399418354034 and batch: 100, loss is 4.070356993675232 and perplexity is 58.57787078900349
At time: 609.4443109035492 and batch: 150, loss is 4.002675228118896 and perplexity is 54.74440808895553
At time: 610.643473148346 and batch: 200, loss is 4.051350078582764 and perplexity is 57.47500045597897
At time: 611.8438351154327 and batch: 250, loss is 4.050893063545227 and perplexity is 57.44873951776708
At time: 613.0923435688019 and batch: 300, loss is 4.057841391563415 and perplexity is 57.84930221309615
At time: 614.2933361530304 and batch: 350, loss is 4.021238174438476 and perplexity is 55.77011622686538
At time: 615.4960994720459 and batch: 400, loss is 4.023722767829895 and perplexity is 55.90885457189723
At time: 616.6932315826416 and batch: 450, loss is 3.963018202781677 and perplexity is 52.61589196802562
At time: 617.8881075382233 and batch: 500, loss is 3.974013967514038 and perplexity is 53.197636438560096
At time: 619.0820641517639 and batch: 550, loss is 3.973013095855713 and perplexity is 53.14441906828021
At time: 620.2761223316193 and batch: 600, loss is 3.9670290565490722 and perplexity is 52.827350397493014
At time: 621.4697859287262 and batch: 650, loss is 3.991916813850403 and perplexity is 54.15860189099012
At time: 622.6640305519104 and batch: 700, loss is 3.969848976135254 and perplexity is 52.976529515267664
At time: 623.8582305908203 and batch: 750, loss is 3.959589147567749 and perplexity is 52.4357781559282
At time: 625.0526385307312 and batch: 800, loss is 3.9682936477661133 and perplexity is 52.89419765914676
At time: 626.2483747005463 and batch: 850, loss is 4.006007037162781 and perplexity is 54.927110198184444
At time: 627.4507277011871 and batch: 900, loss is 3.944137282371521 and perplexity is 51.63177524958995
At time: 628.6499285697937 and batch: 950, loss is 3.929936089515686 and perplexity is 50.90372428356747
At time: 629.8519291877747 and batch: 1000, loss is 3.908076524734497 and perplexity is 49.803064821358376
At time: 631.0490627288818 and batch: 1050, loss is 3.8963375425338747 and perplexity is 49.22184566504665
At time: 632.2459650039673 and batch: 1100, loss is 3.831607294082642 and perplexity is 46.136633810088604
At time: 633.4482383728027 and batch: 1150, loss is 3.8315531730651857 and perplexity is 46.134136916092636
At time: 634.6442542076111 and batch: 1200, loss is 3.843031144142151 and perplexity is 46.66671380940178
At time: 635.8398368358612 and batch: 1250, loss is 3.8831336593627928 and perplexity is 48.5761980733976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589214686929744 and perplexity of 98.41711156221987
Finished 20 epochs...
Completing Train Step...
At time: 638.8328018188477 and batch: 50, loss is 4.058354511260986 and perplexity is 57.878993446479306
At time: 640.0684933662415 and batch: 100, loss is 4.069134197235107 and perplexity is 58.506285753015874
At time: 641.2651250362396 and batch: 150, loss is 4.001375641822815 and perplexity is 54.673309215983245
At time: 642.4895741939545 and batch: 200, loss is 4.049904279708862 and perplexity is 57.39196320708733
At time: 643.6847786903381 and batch: 250, loss is 4.04947856426239 and perplexity is 57.367535761786044
At time: 644.8826978206635 and batch: 300, loss is 4.056529822349549 and perplexity is 57.77347858410793
At time: 646.0886597633362 and batch: 350, loss is 4.019901471138001 and perplexity is 55.69561793058892
At time: 647.2936112880707 and batch: 400, loss is 4.022401809692383 and perplexity is 55.83505007255047
At time: 648.4907314777374 and batch: 450, loss is 3.961763062477112 and perplexity is 52.54989306895947
At time: 649.6933536529541 and batch: 500, loss is 3.97283899307251 and perplexity is 53.13516728241291
At time: 650.8966233730316 and batch: 550, loss is 3.971909747123718 and perplexity is 53.08581457742927
At time: 652.111225605011 and batch: 600, loss is 3.966034746170044 and perplexity is 52.77484972000591
At time: 653.3141584396362 and batch: 650, loss is 3.9910447359085084 and perplexity is 54.11139195728006
At time: 654.5162932872772 and batch: 700, loss is 3.969059467315674 and perplexity is 52.934720584416155
At time: 655.7135984897614 and batch: 750, loss is 3.9588910293579103 and perplexity is 52.39918455916483
At time: 656.9091684818268 and batch: 800, loss is 3.9677385473251343 and perplexity is 52.86484421450971
At time: 658.1050319671631 and batch: 850, loss is 4.005648536682129 and perplexity is 54.907422332043176
At time: 659.3014912605286 and batch: 900, loss is 3.943917999267578 and perplexity is 51.620454514919416
At time: 660.4980726242065 and batch: 950, loss is 3.929823217391968 and perplexity is 50.8979789963499
At time: 661.695716381073 and batch: 1000, loss is 3.90804470539093 and perplexity is 49.80148014573994
At time: 662.89280128479 and batch: 1050, loss is 3.896461582183838 and perplexity is 49.22795150422874
At time: 664.1171612739563 and batch: 1100, loss is 3.8319230842590333 and perplexity is 46.15120560651159
At time: 665.3366618156433 and batch: 1150, loss is 3.8319947624206545 and perplexity is 46.154513758645784
At time: 666.5363700389862 and batch: 1200, loss is 3.8435625219345093 and perplexity is 46.69151805439126
At time: 667.7319345474243 and batch: 1250, loss is 3.8836186838150026 and perplexity is 48.59976443192666
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589209786296761 and perplexity of 98.41662925725866
Finished 21 epochs...
Completing Train Step...
At time: 670.8990030288696 and batch: 50, loss is 4.05730890750885 and perplexity is 57.81850658191888
At time: 672.0995607376099 and batch: 100, loss is 4.067968544960022 and perplexity is 58.438127500049845
At time: 673.3639273643494 and batch: 150, loss is 4.000153770446778 and perplexity is 54.60654626059734
At time: 674.5860588550568 and batch: 200, loss is 4.048541069030762 and perplexity is 57.31377917276924
At time: 675.7932090759277 and batch: 250, loss is 4.048162317276001 and perplexity is 57.292075588730285
At time: 676.9883432388306 and batch: 300, loss is 4.0553249168395995 and perplexity is 57.703908922284185
At time: 678.2043840885162 and batch: 350, loss is 4.01866461277008 and perplexity is 55.62677292403289
At time: 679.4041154384613 and batch: 400, loss is 4.021205396652221 and perplexity is 55.7682882358752
At time: 680.6076822280884 and batch: 450, loss is 3.9606120347976685 and perplexity is 52.48944148488276
At time: 681.8037021160126 and batch: 500, loss is 3.9717536783218383 and perplexity is 53.077530184435815
At time: 682.9995450973511 and batch: 550, loss is 3.9708782386779786 and perplexity is 53.03108434354572
At time: 684.1946108341217 and batch: 600, loss is 3.965099740028381 and perplexity is 52.725527973053765
At time: 685.3905048370361 and batch: 650, loss is 3.990154423713684 and perplexity is 54.06323736463295
At time: 686.5863468647003 and batch: 700, loss is 3.968286623954773 and perplexity is 52.89382614158615
At time: 687.7807350158691 and batch: 750, loss is 3.9582107400894166 and perplexity is 52.363550078483094
At time: 688.9770219326019 and batch: 800, loss is 3.9673116302490232 and perplexity is 52.8422801266285
At time: 690.1716747283936 and batch: 850, loss is 4.005287609100342 and perplexity is 54.887608304808275
At time: 691.3667469024658 and batch: 900, loss is 3.943674659729004 and perplexity is 51.607894745543
At time: 692.561690568924 and batch: 950, loss is 3.929682102203369 and perplexity is 50.890797025199056
At time: 693.7570447921753 and batch: 1000, loss is 3.907978048324585 and perplexity is 49.79816063580942
At time: 694.9525237083435 and batch: 1050, loss is 3.8965272665023805 and perplexity is 49.23118511487413
At time: 696.148848772049 and batch: 1100, loss is 3.8321569967269897 and perplexity is 46.162002211595215
At time: 697.3438365459442 and batch: 1150, loss is 3.8323499298095705 and perplexity is 46.170909248183385
At time: 698.5400795936584 and batch: 1200, loss is 3.84399329662323 and perplexity is 46.71163591136773
At time: 699.735463142395 and batch: 1250, loss is 3.88401309967041 and perplexity is 48.618936730268366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589209786296761 and perplexity of 98.41662925725866
Finished 22 epochs...
Completing Train Step...
At time: 702.7110183238983 and batch: 50, loss is 4.056282629966736 and perplexity is 57.75919918522538
At time: 703.9317798614502 and batch: 100, loss is 4.066846289634705 and perplexity is 58.372581786653654
At time: 705.1258993148804 and batch: 150, loss is 3.9989927196502686 and perplexity is 54.54318207820221
At time: 706.3199684619904 and batch: 200, loss is 4.047237162590027 and perplexity is 57.239096067435206
At time: 707.5142886638641 and batch: 250, loss is 4.046913328170777 and perplexity is 57.220563078973555
At time: 708.7086684703827 and batch: 300, loss is 4.054168996810913 and perplexity is 57.63724635394895
At time: 709.9025619029999 and batch: 350, loss is 4.017475447654724 and perplexity is 55.56066282189601
At time: 711.0961010456085 and batch: 400, loss is 4.020073494911194 and perplexity is 55.705199725060254
At time: 712.2905309200287 and batch: 450, loss is 3.9595334196090697 and perplexity is 52.432856098470715
At time: 713.4844200611115 and batch: 500, loss is 3.970739312171936 and perplexity is 53.0237174320277
At time: 714.6777656078339 and batch: 550, loss is 3.9698920249938965 and perplexity is 52.97881014348702
At time: 715.8711459636688 and batch: 600, loss is 3.964216318130493 and perplexity is 52.6789696554113
At time: 717.0645186901093 and batch: 650, loss is 3.9892985105514525 and perplexity is 54.016983725552684
At time: 718.2567074298859 and batch: 700, loss is 3.9675410747528077 and perplexity is 52.8544058884125
At time: 719.4488523006439 and batch: 750, loss is 3.9575575160980225 and perplexity is 52.32935612066995
At time: 720.6417849063873 and batch: 800, loss is 3.966697487831116 and perplexity is 52.8098374041895
At time: 721.8367898464203 and batch: 850, loss is 4.0048660564422605 and perplexity is 54.864475163892756
At time: 723.030190706253 and batch: 900, loss is 3.9434160375595093 and perplexity is 51.59454952560011
At time: 724.2259912490845 and batch: 950, loss is 3.9295154428482055 and perplexity is 50.88231630449843
At time: 725.420716047287 and batch: 1000, loss is 3.907891969680786 and perplexity is 49.79387426216346
At time: 726.6148149967194 and batch: 1050, loss is 3.8965535879135134 and perplexity is 49.23248096619234
At time: 727.8088459968567 and batch: 1100, loss is 3.8323335742950437 and perplexity is 46.170154105381854
At time: 729.0028796195984 and batch: 1150, loss is 3.8326438283920288 and perplexity is 46.18448080719553
At time: 730.1970448493958 and batch: 1200, loss is 3.8443576192855833 and perplexity is 46.72865711934346
At time: 731.3909547328949 and batch: 1250, loss is 3.8843382167816163 and perplexity is 48.63474614834494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.58921780551437 and perplexity of 98.41741848478952
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 734.3622388839722 and batch: 50, loss is 4.056187181472779 and perplexity is 57.75368641974772
At time: 735.5591459274292 and batch: 100, loss is 4.067961449623108 and perplexity is 58.43771286331763
At time: 736.7572958469391 and batch: 150, loss is 4.001045217514038 and perplexity is 54.6552468098704
At time: 737.9555954933167 and batch: 200, loss is 4.049710025787354 and perplexity is 57.38081567593216
At time: 739.1531360149384 and batch: 250, loss is 4.048882083892822 and perplexity is 57.33332735618911
At time: 740.3521063327789 and batch: 300, loss is 4.056885566711426 and perplexity is 57.79403482954766
At time: 741.5506184101105 and batch: 350, loss is 4.020181703567505 and perplexity is 55.711227836013045
At time: 742.7484788894653 and batch: 400, loss is 4.022915368080139 and perplexity is 55.8637319951366
At time: 743.9453890323639 and batch: 450, loss is 3.9622546195983888 and perplexity is 52.575730692933625
At time: 745.1420519351959 and batch: 500, loss is 3.973060245513916 and perplexity is 53.14692486854803
At time: 746.3388617038727 and batch: 550, loss is 3.9703273582458496 and perplexity is 53.001878602060614
At time: 747.5365009307861 and batch: 600, loss is 3.96399854183197 and perplexity is 52.66749867348907
At time: 748.7415287494659 and batch: 650, loss is 3.9885947418212893 and perplexity is 53.97898163541877
At time: 749.9428248405457 and batch: 700, loss is 3.9663165283203123 and perplexity is 52.78972282602958
At time: 751.1435208320618 and batch: 750, loss is 3.954685077667236 and perplexity is 52.17925894274483
At time: 752.3532948493958 and batch: 800, loss is 3.9634107637405394 and perplexity is 52.63655096772104
At time: 753.5550198554993 and batch: 850, loss is 4.0017844581604 and perplexity is 54.69566512744015
At time: 754.7529044151306 and batch: 900, loss is 3.940004744529724 and perplexity is 51.41884525818789
At time: 755.9502618312836 and batch: 950, loss is 3.9264301300048827 and perplexity is 50.72557036996934
At time: 757.1480286121368 and batch: 1000, loss is 3.905198564529419 and perplexity is 49.659939635793826
At time: 758.3454277515411 and batch: 1050, loss is 3.8932265186309816 and perplexity is 49.06895327589047
At time: 759.5436255931854 and batch: 1100, loss is 3.827690353393555 and perplexity is 45.95627281412014
At time: 760.7411236763 and batch: 1150, loss is 3.8275288105010987 and perplexity is 45.94884950449084
At time: 761.9389865398407 and batch: 1200, loss is 3.8398499965667723 and perplexity is 46.51849598230007
At time: 763.1362597942352 and batch: 1250, loss is 3.88082218170166 and perplexity is 48.464044946347606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589084597399635 and perplexity of 98.4043093591564
Finished 24 epochs...
Completing Train Step...
At time: 766.1034331321716 and batch: 50, loss is 4.055954623222351 and perplexity is 57.74025688511326
At time: 767.325395822525 and batch: 100, loss is 4.067216062545777 and perplexity is 58.39417037733972
At time: 768.5200870037079 and batch: 150, loss is 4.000166945457458 and perplexity is 54.60726570716691
At time: 769.7145285606384 and batch: 200, loss is 4.048752965927124 and perplexity is 57.325925071488435
At time: 770.9093132019043 and batch: 250, loss is 4.047939038276672 and perplexity is 57.27928489942266
At time: 772.1038286685944 and batch: 300, loss is 4.056063270568847 and perplexity is 57.74653055161229
At time: 773.2983791828156 and batch: 350, loss is 4.019419865608215 and perplexity is 55.66880107110099
At time: 774.4912509918213 and batch: 400, loss is 4.022196674346924 and perplexity is 55.823597504968504
At time: 775.6857659816742 and batch: 450, loss is 3.9615148210525515 and perplexity is 52.53684962767174
At time: 776.8801505565643 and batch: 500, loss is 3.972368698120117 and perplexity is 53.11018395667192
At time: 778.0747561454773 and batch: 550, loss is 3.969678363800049 and perplexity is 52.96749183684764
At time: 779.2697939872742 and batch: 600, loss is 3.9635320901870728 and perplexity is 52.64293756083122
At time: 780.4642705917358 and batch: 650, loss is 3.9882282972335816 and perplexity is 53.95920495349883
At time: 781.6570436954498 and batch: 700, loss is 3.966026120185852 and perplexity is 52.774394486949916
At time: 782.8511464595795 and batch: 750, loss is 3.9546117877960203 and perplexity is 52.175434871711325
At time: 784.0459270477295 and batch: 800, loss is 3.963502869606018 and perplexity is 52.64139932608143
At time: 785.2414283752441 and batch: 850, loss is 4.0018852186203 and perplexity is 54.70117656547579
At time: 786.4359431266785 and batch: 900, loss is 3.940244183540344 and perplexity is 51.43115840968932
At time: 787.630214214325 and batch: 950, loss is 3.9267010927200316 and perplexity is 50.73931697056827
At time: 788.8263401985168 and batch: 1000, loss is 3.9054386949539186 and perplexity is 49.67186593005493
At time: 790.0221936702728 and batch: 1050, loss is 3.8936203289031983 and perplexity is 49.08828093920383
At time: 791.217206954956 and batch: 1100, loss is 3.8281154346466066 and perplexity is 45.97581211675496
At time: 792.41255235672 and batch: 1150, loss is 3.8279511070251466 and perplexity is 45.968257641626586
At time: 793.6081736087799 and batch: 1200, loss is 3.840290460586548 and perplexity is 46.53899021919
At time: 794.8305773735046 and batch: 1250, loss is 3.8811652946472166 and perplexity is 48.48067644064006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589076132669936 and perplexity of 98.40347639680182
Finished 25 epochs...
Completing Train Step...
At time: 797.7786612510681 and batch: 50, loss is 4.055784955024719 and perplexity is 57.730461030842726
At time: 798.9752156734467 and batch: 100, loss is 4.0667030811309814 and perplexity is 58.364222935101125
At time: 800.174124956131 and batch: 150, loss is 3.9995793676376343 and perplexity is 54.575189113706436
At time: 801.3716773986816 and batch: 200, loss is 4.0480598449707035 and perplexity is 57.286205038458995
At time: 802.5691137313843 and batch: 250, loss is 4.047287230491638 and perplexity is 57.241961980609965
At time: 803.7675611972809 and batch: 300, loss is 4.055425715446472 and perplexity is 57.70972568907074
At time: 804.9644684791565 and batch: 350, loss is 4.018856949806214 and perplexity is 55.637473041648235
At time: 806.1630072593689 and batch: 400, loss is 4.021582598686218 and perplexity is 55.78932811552353
At time: 807.3610706329346 and batch: 450, loss is 3.960951771736145 and perplexity is 52.50727711657507
At time: 808.5597937107086 and batch: 500, loss is 3.971858949661255 and perplexity is 53.08311802124564
At time: 809.7715971469879 and batch: 550, loss is 3.969202618598938 and perplexity is 52.94229879999974
At time: 810.9747772216797 and batch: 600, loss is 3.963181743621826 and perplexity is 52.62449751886358
At time: 812.1716628074646 and batch: 650, loss is 3.987969756126404 and perplexity is 53.94525608416348
At time: 813.3676383495331 and batch: 700, loss is 3.9658152866363525 and perplexity is 52.763269046886755
At time: 814.5648112297058 and batch: 750, loss is 3.95458402633667 and perplexity is 52.17398642560262
At time: 815.7629737854004 and batch: 800, loss is 3.9635582876205446 and perplexity is 52.64431668875046
At time: 816.961578130722 and batch: 850, loss is 4.001962013244629 and perplexity is 54.705377483082366
At time: 818.1607937812805 and batch: 900, loss is 3.9404392528533934 and perplexity is 51.44119202902342
At time: 819.3591845035553 and batch: 950, loss is 3.92691743850708 and perplexity is 50.750295395557735
At time: 820.5577309131622 and batch: 1000, loss is 3.905629849433899 and perplexity is 49.68136183732013
At time: 821.7558796405792 and batch: 1050, loss is 3.8939356565475465 and perplexity is 49.1037622719153
At time: 822.9550848007202 and batch: 1100, loss is 3.8284521293640137 and perplexity is 45.99129453610131
At time: 824.1544184684753 and batch: 1150, loss is 3.828313593864441 and perplexity is 45.98492355045077
At time: 825.4001512527466 and batch: 1200, loss is 3.8406469011306763 and perplexity is 46.555581558924324
At time: 826.5967166423798 and batch: 1250, loss is 3.8814410018920897 and perplexity is 48.49404475715727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5891077640282845 and perplexity of 98.40658908165544
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 829.5197539329529 and batch: 50, loss is 4.055761952400207 and perplexity is 57.72913309399781
At time: 830.7389059066772 and batch: 100, loss is 4.0666019105911255 and perplexity is 58.3583184938413
At time: 831.9314427375793 and batch: 150, loss is 3.9995628786087036 and perplexity is 54.574289229253374
At time: 833.1255812644958 and batch: 200, loss is 4.048043870925904 and perplexity is 57.285289953362124
At time: 834.3189487457275 and batch: 250, loss is 4.047257738113403 and perplexity is 57.24027380391063
At time: 835.5152132511139 and batch: 300, loss is 4.055494537353516 and perplexity is 57.71369751912051
At time: 836.7104933261871 and batch: 350, loss is 4.019006280899048 and perplexity is 55.64578206668255
At time: 837.9044768810272 and batch: 400, loss is 4.021725769042969 and perplexity is 55.79731606533787
At time: 839.0989100933075 and batch: 450, loss is 3.961109509468079 and perplexity is 52.51556014863363
At time: 840.2928812503815 and batch: 500, loss is 3.972019672393799 and perplexity is 53.09165037067884
At time: 841.4874837398529 and batch: 550, loss is 3.9691954374313356 and perplexity is 52.941918613843896
At time: 842.6830232143402 and batch: 600, loss is 3.9631152153015137 and perplexity is 52.62099661589226
At time: 843.8757667541504 and batch: 650, loss is 3.9878921031951906 and perplexity is 53.941067239543635
At time: 845.0688235759735 and batch: 700, loss is 3.9657118988037108 and perplexity is 52.75781424884164
At time: 846.2627527713776 and batch: 750, loss is 3.9542505741119385 and perplexity is 52.156591794056375
At time: 847.456621170044 and batch: 800, loss is 3.963047947883606 and perplexity is 52.61745705637202
At time: 848.6513819694519 and batch: 850, loss is 4.0013947105407714 and perplexity is 54.674351775836556
At time: 849.8453993797302 and batch: 900, loss is 3.9398121881484984 and perplexity is 51.408945184609934
At time: 851.040016412735 and batch: 950, loss is 3.9263615083694456 and perplexity is 50.72208961780089
At time: 852.2336533069611 and batch: 1000, loss is 3.905110058784485 and perplexity is 49.65554464033701
At time: 853.4279119968414 and batch: 1050, loss is 3.8934343242645264 and perplexity is 49.07915114036349
At time: 854.6225492954254 and batch: 1100, loss is 3.8277418184280396 and perplexity is 45.95863801614739
At time: 855.8439729213715 and batch: 1150, loss is 3.8275679206848143 and perplexity is 45.95064660757877
At time: 857.0382568836212 and batch: 1200, loss is 3.8399968671798708 and perplexity is 46.52532868407467
At time: 858.2331626415253 and batch: 1250, loss is 3.880937418937683 and perplexity is 48.46963013073814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589110882612911 and perplexity of 98.4068959714098
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 861.2015006542206 and batch: 50, loss is 4.055755529403687 and perplexity is 57.72876230116763
At time: 862.3970720767975 and batch: 100, loss is 4.066575808525085 and perplexity is 58.35679524103809
At time: 863.5923826694489 and batch: 150, loss is 3.9995341730117797 and perplexity is 54.57272266418905
At time: 864.7879018783569 and batch: 200, loss is 4.048016614913941 and perplexity is 57.283728606092
At time: 865.9837167263031 and batch: 250, loss is 4.047236022949218 and perplexity is 57.239030835462664
At time: 867.1800577640533 and batch: 300, loss is 4.055498037338257 and perplexity is 57.713899516534674
At time: 868.3761141300201 and batch: 350, loss is 4.019009647369384 and perplexity is 55.64596939687253
At time: 869.5849442481995 and batch: 400, loss is 4.021735677719116 and perplexity is 55.79786894561179
At time: 870.7892637252808 and batch: 450, loss is 3.9611207962036135 and perplexity is 52.51615288121749
At time: 871.9857099056244 and batch: 500, loss is 3.9720369386672973 and perplexity is 53.09256707354863
At time: 873.1824004650116 and batch: 550, loss is 3.9691902208328247 and perplexity is 52.94164243783044
At time: 874.3787977695465 and batch: 600, loss is 3.9631054067611693 and perplexity is 52.62048048325525
At time: 875.5755727291107 and batch: 650, loss is 3.987866287231445 and perplexity is 53.939674716882124
At time: 876.7744765281677 and batch: 700, loss is 3.965682277679443 and perplexity is 52.7562515262146
At time: 877.9708158969879 and batch: 750, loss is 3.954194917678833 and perplexity is 52.15368902497382
At time: 879.1679129600525 and batch: 800, loss is 3.9629803228378297 and perplexity is 52.61389891874093
At time: 880.3701868057251 and batch: 850, loss is 4.001319694519043 and perplexity is 54.67025047730921
At time: 881.5674955844879 and batch: 900, loss is 3.9397273778915407 and perplexity is 51.40458536364028
At time: 882.763601064682 and batch: 950, loss is 3.9262867832183836 and perplexity is 50.71829954360072
At time: 883.9594943523407 and batch: 1000, loss is 3.9050391340255737 and perplexity is 49.65202295769351
At time: 885.1560733318329 and batch: 1050, loss is 3.893365378379822 and perplexity is 49.07576745151463
At time: 886.3804686069489 and batch: 1100, loss is 3.8276450967788698 and perplexity is 45.95419303585124
At time: 887.575747013092 and batch: 1150, loss is 3.827464809417725 and perplexity is 45.94590882244719
At time: 888.7726924419403 and batch: 1200, loss is 3.8399061346054078 and perplexity is 46.5211075127272
At time: 889.9685785770416 and batch: 1250, loss is 3.880867395401001 and perplexity is 48.46623623464241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.589114001197537 and perplexity of 98.4072028621212
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 892.9860708713531 and batch: 50, loss is 4.055755543708801 and perplexity is 57.728763126984184
At time: 894.2215418815613 and batch: 100, loss is 4.066573061943054 and perplexity is 58.356634959533
At time: 895.4170999526978 and batch: 150, loss is 3.9995312213897707 and perplexity is 54.57256158637746
At time: 896.6125907897949 and batch: 200, loss is 4.048013896942138 and perplexity is 57.2835729107445
At time: 897.8079469203949 and batch: 250, loss is 4.047234015464783 and perplexity is 57.23891592911449
At time: 899.0135350227356 and batch: 300, loss is 4.055499520301819 and perplexity is 57.71398510420812
At time: 900.2118899822235 and batch: 350, loss is 4.019011082649231 and perplexity is 55.64604926446829
At time: 901.4227950572968 and batch: 400, loss is 4.021738138198852 and perplexity is 55.798006235306566
At time: 902.6271374225616 and batch: 450, loss is 3.9611232900619506 and perplexity is 52.5162838492265
At time: 903.8298573493958 and batch: 500, loss is 3.972040286064148 and perplexity is 53.09274479573791
At time: 905.0242903232574 and batch: 550, loss is 3.969190545082092 and perplexity is 52.941659604122
At time: 906.2170705795288 and batch: 600, loss is 3.9631049633026123 and perplexity is 52.62045714825808
At time: 907.4099242687225 and batch: 650, loss is 3.987863440513611 and perplexity is 53.93952116606669
At time: 908.6038134098053 and batch: 700, loss is 3.965679044723511 and perplexity is 52.75608096785397
At time: 909.7971978187561 and batch: 750, loss is 3.95418794631958 and perplexity is 52.15332544413859
At time: 910.9910454750061 and batch: 800, loss is 3.9629716968536375 and perplexity is 52.61344507403799
At time: 912.18749833107 and batch: 850, loss is 4.001310095787049 and perplexity is 54.669725714745354
At time: 913.382084608078 and batch: 900, loss is 3.9397165060043333 and perplexity is 51.40402650182421
At time: 914.5789234638214 and batch: 950, loss is 3.92627733707428 and perplexity is 50.71782045349731
At time: 915.7729389667511 and batch: 1000, loss is 3.9050301122665405 and perplexity is 49.65157501112751
At time: 917.0193057060242 and batch: 1050, loss is 3.893356485366821 and perplexity is 49.07533102201725
At time: 918.2138140201569 and batch: 1100, loss is 3.82763258934021 and perplexity is 45.953618270195115
At time: 919.4086050987244 and batch: 1150, loss is 3.8274514436721803 and perplexity is 45.945294725225
At time: 920.6041946411133 and batch: 1200, loss is 3.8398944997787474 and perplexity is 46.52056625085399
At time: 921.7985861301422 and batch: 1250, loss is 3.880858540534973 and perplexity is 48.46580707451375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5891148922217155 and perplexity of 98.40729054535738
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 924.7526211738586 and batch: 50, loss is 4.055755724906922 and perplexity is 57.728773587328504
At time: 925.9464781284332 and batch: 100, loss is 4.066572799682617 and perplexity is 58.356619654898395
At time: 927.1398344039917 and batch: 150, loss is 3.99953104019165 and perplexity is 54.57255169793277
At time: 928.3351740837097 and batch: 200, loss is 4.048013792037964 and perplexity is 57.28356690145891
At time: 929.5283370018005 and batch: 250, loss is 4.047233986854553 and perplexity is 57.23891429149598
At time: 930.7222383022308 and batch: 300, loss is 4.055499849319458 and perplexity is 57.714004093130356
At time: 931.9151978492737 and batch: 350, loss is 4.019011449813843 and perplexity is 55.64606969573211
At time: 933.1088387966156 and batch: 400, loss is 4.021738467216491 and perplexity is 55.79802459383785
At time: 934.3022811412811 and batch: 450, loss is 3.9611238765716554 and perplexity is 52.51631465054567
At time: 935.495813369751 and batch: 500, loss is 3.9720408010482786 and perplexity is 53.09277213766596
At time: 936.6889071464539 and batch: 550, loss is 3.9691908597946166 and perplexity is 52.94167626552797
At time: 937.8824768066406 and batch: 600, loss is 3.9631051588058472 and perplexity is 52.620467435728685
At time: 939.0752058029175 and batch: 650, loss is 3.9878632974624635 and perplexity is 53.93951344995685
At time: 940.268634557724 and batch: 700, loss is 3.9656789922714233 and perplexity is 52.75607820068746
At time: 941.4623250961304 and batch: 750, loss is 3.954187297821045 and perplexity is 52.15329162279439
At time: 942.6569457054138 and batch: 800, loss is 3.9629709196090697 and perplexity is 52.613404180539504
At time: 943.8511009216309 and batch: 850, loss is 4.001309251785278 and perplexity is 54.66967957341952
At time: 945.045526266098 and batch: 900, loss is 3.939715552330017 and perplexity is 51.403977479147756
At time: 946.2403309345245 and batch: 950, loss is 3.9262764501571654 and perplexity is 50.71777547101429
At time: 947.461181640625 and batch: 1000, loss is 3.9050293445587156 and perplexity is 49.65153689323949
At time: 948.6550996303558 and batch: 1050, loss is 3.8933558750152586 and perplexity is 49.07530106882142
At time: 949.8500158786774 and batch: 1100, loss is 3.827631421089172 and perplexity is 45.95356458486424
At time: 951.0447885990143 and batch: 1150, loss is 3.8274501371383667 and perplexity is 45.94523469618308
At time: 952.2384693622589 and batch: 1200, loss is 3.8398935699462893 and perplexity is 46.52052299454163
At time: 953.4310717582703 and batch: 1250, loss is 3.880857787132263 and perplexity is 48.46577056025711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5891148922217155 and perplexity of 98.40729054535738
Annealing...
Model not improving. Stopping early with 98.40347639680182loss at 29 epochs.
Finished Training.
Improved accuracyfrom -114.15170787862657 to -98.40347639680182
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fd9237c4668>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -114.15170787862657, 'params': {'lr': 11.358133663123011, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 3.2445999806025085, 'wordvec_source': '', 'dropout': 0.2746691213793858, 'tune_wordvecs': True, 'seq_len': 35}}, {'best_accuracy': -140.42951812288277, 'params': {'lr': 16.217378339003858, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 3.178296882833527, 'wordvec_source': '', 'dropout': 0.38907510530416645, 'tune_wordvecs': True, 'seq_len': 35}}, {'best_accuracy': -164.83637125909323, 'params': {'lr': 25.329360970367127, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 2.081824762075887, 'wordvec_source': '', 'dropout': 0.3259448805725369, 'tune_wordvecs': True, 'seq_len': 35}}, {'best_accuracy': -170.95131467380358, 'params': {'lr': 27.230779683856515, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 7.81897938379199, 'wordvec_source': '', 'dropout': 0.36584214465228326, 'tune_wordvecs': True, 'seq_len': 35}}, {'best_accuracy': -122.87843589535802, 'params': {'lr': 14.633979970439194, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 5.346345943505483, 'wordvec_source': '', 'dropout': 0.17378847536166508, 'tune_wordvecs': True, 'seq_len': 35}}, {'best_accuracy': -98.40347639680182, 'params': {'lr': 9.314818014280576, 'data': 'wikitext', 'batch_size': 50, 'wordvec_dim': 200, 'num_layers': 1, 'anneal': 7.261082075760348, 'wordvec_source': '', 'dropout': 0.0, 'tune_wordvecs': True, 'seq_len': 35}}]
