Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'anneal': 5.453002407156198, 'dropout': 0.8275739045832949, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 6.0869734814206735}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.0323522090911865 and batch: 50, loss is 6.888987827301025 and perplexity is 981.4075603865423
At time: 1.5119006633758545 and batch: 100, loss is 6.2856910610198975 and perplexity is 536.8351483252121
At time: 1.9671788215637207 and batch: 150, loss is 6.078572607040405 and perplexity is 436.4058273236682
At time: 2.4250433444976807 and batch: 200, loss is 6.034347848892212 and perplexity is 417.52643050762106
At time: 2.8829853534698486 and batch: 250, loss is 6.0834668922424315 and perplexity is 438.54695728269814
At time: 3.339951753616333 and batch: 300, loss is 5.976382694244385 and perplexity is 394.0125234679295
At time: 3.7937309741973877 and batch: 350, loss is 5.951320114135743 and perplexity is 384.2602717995624
At time: 4.247509479522705 and batch: 400, loss is 5.928994636535645 and perplexity is 375.77653185605567
At time: 4.701023101806641 and batch: 450, loss is 5.966745529174805 and perplexity is 390.2335980510487
At time: 5.149687051773071 and batch: 500, loss is 5.952640962600708 and perplexity is 384.76815673539534
At time: 5.596832990646362 and batch: 550, loss is 5.900773649215698 and perplexity is 365.31998808772596
At time: 6.046693563461304 and batch: 600, loss is 5.80751859664917 and perplexity is 332.792308316151
At time: 6.494971752166748 and batch: 650, loss is 5.77156925201416 and perplexity is 321.0411320068531
At time: 6.942500829696655 and batch: 700, loss is 5.867729158401489 and perplexity is 353.4454496378319
At time: 7.390568971633911 and batch: 750, loss is 5.741738300323487 and perplexity is 311.6056045898629
At time: 7.839146375656128 and batch: 800, loss is 5.844406661987304 and perplexity is 345.297602794617
At time: 8.288572072982788 and batch: 850, loss is 5.805298757553101 and perplexity is 332.0543822808335
At time: 8.737003087997437 and batch: 900, loss is 5.831085329055786 and perplexity is 340.72828076898406
At time: 9.18583059310913 and batch: 950, loss is 5.792083234786987 and perplexity is 327.6949793737033
At time: 9.63433837890625 and batch: 1000, loss is 5.7188661003112795 and perplexity is 304.5593872980885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.650653746069931 and perplexity of 284.4773810164601
Finished 1 epochs...
Completing Train Step...
At time: 11.046980381011963 and batch: 50, loss is 5.409639892578125 and perplexity is 223.55107078429253
At time: 11.50406002998352 and batch: 100, loss is 5.212760772705078 and perplexity is 183.60023767378493
At time: 11.948609590530396 and batch: 150, loss is 5.111989831924438 and perplexity is 166.00033923261716
At time: 12.39047646522522 and batch: 200, loss is 5.0521774578094485 and perplexity is 156.36256696516156
At time: 12.832166194915771 and batch: 250, loss is 5.060801477432251 and perplexity is 157.7168721946626
At time: 13.275404691696167 and batch: 300, loss is 4.922069673538208 and perplexity is 137.28645750085786
At time: 13.718350887298584 and batch: 350, loss is 4.8785974979400635 and perplexity is 131.4461810076279
At time: 14.160439252853394 and batch: 400, loss is 4.866764125823974 and perplexity is 129.89989635899124
At time: 14.603397130966187 and batch: 450, loss is 4.8688961791992185 and perplexity is 130.1771453212371
At time: 15.046210527420044 and batch: 500, loss is 4.866145839691162 and perplexity is 129.81960587822198
At time: 15.491055011749268 and batch: 550, loss is 4.753143768310547 and perplexity is 115.94822651297744
At time: 15.933801412582397 and batch: 600, loss is 4.666356725692749 and perplexity is 106.30972055307579
At time: 16.40504550933838 and batch: 650, loss is 4.637771883010864 and perplexity is 103.31389549574752
At time: 16.846648693084717 and batch: 700, loss is 4.676341457366943 and perplexity is 107.37651153522746
At time: 17.29003381729126 and batch: 750, loss is 4.64022539138794 and perplexity is 103.56768821778637
At time: 17.733036041259766 and batch: 800, loss is 4.719874906539917 and perplexity is 112.15422203056028
At time: 18.176483154296875 and batch: 850, loss is 4.597272109985352 and perplexity is 99.21330318482681
At time: 18.619918823242188 and batch: 900, loss is 4.58551833152771 and perplexity is 98.05399845123463
At time: 19.06412696838379 and batch: 950, loss is 4.577074193954468 and perplexity is 97.22950297443246
At time: 19.50724744796753 and batch: 1000, loss is 4.459662837982178 and perplexity is 86.45835370856692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.657218374857089 and perplexity of 105.34265047221027
Finished 2 epochs...
Completing Train Step...
At time: 20.91960382461548 and batch: 50, loss is 4.571586751937867 and perplexity is 96.69742292892265
At time: 21.363230228424072 and batch: 100, loss is 4.425217504501343 and perplexity is 83.53097360909564
At time: 21.80784296989441 and batch: 150, loss is 4.497006063461304 and perplexity is 89.74802876098728
At time: 22.25096082687378 and batch: 200, loss is 4.518353099822998 and perplexity is 91.68447838918695
At time: 22.694759130477905 and batch: 250, loss is 4.523273429870605 and perplexity is 92.13670793004415
At time: 23.13849449157715 and batch: 300, loss is 4.400817975997925 and perplexity is 81.51752077674371
At time: 23.584376573562622 and batch: 350, loss is 4.394860801696777 and perplexity is 81.03335027277072
At time: 24.029098749160767 and batch: 400, loss is 4.408386993408203 and perplexity is 82.13686928369017
At time: 24.474105834960938 and batch: 450, loss is 4.448012585639954 and perplexity is 85.45693677744173
At time: 24.91844081878662 and batch: 500, loss is 4.461037349700928 and perplexity is 86.57727343848164
At time: 25.36189317703247 and batch: 550, loss is 4.353948721885681 and perplexity is 77.78500866109546
At time: 25.805410623550415 and batch: 600, loss is 4.295509114265442 and perplexity is 73.36955842688475
At time: 26.25003480911255 and batch: 650, loss is 4.2662803649902346 and perplexity is 71.2560954019195
At time: 26.693254947662354 and batch: 700, loss is 4.335832796096802 and perplexity is 76.38854849253094
At time: 27.13675856590271 and batch: 750, loss is 4.303988742828369 and perplexity is 73.99435028787231
At time: 27.610656261444092 and batch: 800, loss is 4.4093670082092284 and perplexity is 82.21740408750229
At time: 28.05370545387268 and batch: 850, loss is 4.293403325080871 and perplexity is 73.21522016322835
At time: 28.497296571731567 and batch: 900, loss is 4.250891304016113 and perplexity is 70.16792543720955
At time: 28.941474199295044 and batch: 950, loss is 4.279847021102905 and perplexity is 72.2293895897493
At time: 29.385241985321045 and batch: 1000, loss is 4.172749743461609 and perplexity is 64.89364788275503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.516306435189596 and perplexity of 91.49702290452626
Finished 3 epochs...
Completing Train Step...
At time: 30.797558307647705 and batch: 50, loss is 4.303278341293335 and perplexity is 73.94180325480032
At time: 31.240543365478516 and batch: 100, loss is 4.1703219318389895 and perplexity is 64.73628942582917
At time: 31.684884548187256 and batch: 150, loss is 4.267742881774902 and perplexity is 71.36038488142259
At time: 32.12923550605774 and batch: 200, loss is 4.3020047855377195 and perplexity is 73.84769418497835
At time: 32.57377338409424 and batch: 250, loss is 4.293018455505371 and perplexity is 73.18704727431566
At time: 33.018118143081665 and batch: 300, loss is 4.170069561004639 and perplexity is 64.71995393584133
At time: 33.46177816390991 and batch: 350, loss is 4.173844094276428 and perplexity is 64.96470317180751
At time: 33.904876708984375 and batch: 400, loss is 4.190649676322937 and perplexity is 66.06569834091651
At time: 34.348949670791626 and batch: 450, loss is 4.240135426521301 and perplexity is 69.41725213919133
At time: 34.79379868507385 and batch: 500, loss is 4.262574038505554 and perplexity is 70.99248586161066
At time: 35.237868547439575 and batch: 550, loss is 4.150257143974304 and perplexity is 63.45031406646973
At time: 35.6817147731781 and batch: 600, loss is 4.1078795099258425 and perplexity is 60.81761758595492
At time: 36.127548694610596 and batch: 650, loss is 4.075390124320984 and perplexity is 58.87344407114184
At time: 36.57098889350891 and batch: 700, loss is 4.158350706100464 and perplexity is 63.965936923418894
At time: 37.015541315078735 and batch: 750, loss is 4.1261853504180905 and perplexity is 61.94118776496389
At time: 37.46023654937744 and batch: 800, loss is 4.235789890289307 and perplexity is 69.11625143329836
At time: 37.90331745147705 and batch: 850, loss is 4.121388936042786 and perplexity is 61.64480352057745
At time: 38.34646916389465 and batch: 900, loss is 4.069521656036377 and perplexity is 58.52895892053599
At time: 38.80503296852112 and batch: 950, loss is 4.104724297523498 and perplexity is 60.626027497324344
At time: 39.248730421066284 and batch: 1000, loss is 4.00622998714447 and perplexity is 54.93935756162139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.463780100752667 and perplexity of 86.81505929089455
Finished 4 epochs...
Completing Train Step...
At time: 40.64501881599426 and batch: 50, loss is 4.142028017044067 and perplexity is 62.93031587825659
At time: 41.102118253707886 and batch: 100, loss is 4.01478844165802 and perplexity is 55.41157138221231
At time: 41.544899463653564 and batch: 150, loss is 4.123789262771607 and perplexity is 61.792948917709886
At time: 41.98839831352234 and batch: 200, loss is 4.160540518760681 and perplexity is 64.10616382119886
At time: 42.43098568916321 and batch: 250, loss is 4.149244780540466 and perplexity is 63.38611179214405
At time: 42.87308406829834 and batch: 300, loss is 4.0215846538543705 and perplexity is 55.78944277209172
At time: 43.318151235580444 and batch: 350, loss is 4.033686933517456 and perplexity is 56.468724347661805
At time: 43.76232933998108 and batch: 400, loss is 4.051923003196716 and perplexity is 57.50793873310101
At time: 44.20656633377075 and batch: 450, loss is 4.102664647102356 and perplexity is 60.50128757866749
At time: 44.6518611907959 and batch: 500, loss is 4.130297684669495 and perplexity is 62.19643510445737
At time: 45.0964617729187 and batch: 550, loss is 4.016744747161865 and perplexity is 55.52007944714014
At time: 45.540494441986084 and batch: 600, loss is 3.9790259790420532 and perplexity is 53.46493289256163
At time: 45.98542833328247 and batch: 650, loss is 3.9529306411743166 and perplexity is 52.08779400485572
At time: 46.43038606643677 and batch: 700, loss is 4.037346081733704 and perplexity is 56.67573028135574
At time: 46.87377166748047 and batch: 750, loss is 4.004098358154297 and perplexity is 54.822371963584665
At time: 47.31855773925781 and batch: 800, loss is 4.119019980430603 and perplexity is 61.49894255460614
At time: 47.76304769515991 and batch: 850, loss is 4.007956509590149 and perplexity is 55.03429352655714
At time: 48.20757246017456 and batch: 900, loss is 3.944096131324768 and perplexity is 51.629650591708966
At time: 48.651201248168945 and batch: 950, loss is 3.990545516014099 and perplexity is 54.084385215613054
At time: 49.096028327941895 and batch: 1000, loss is 3.8907140398025515 and perplexity is 48.94582331513334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.439252899914253 and perplexity of 84.71162996067228
Finished 5 epochs...
Completing Train Step...
At time: 50.52224516868591 and batch: 50, loss is 4.030143685340882 and perplexity is 56.26899569613319
At time: 50.96725082397461 and batch: 100, loss is 3.906783127784729 and perplexity is 49.73869132844309
At time: 51.413002490997314 and batch: 150, loss is 4.016935729980469 and perplexity is 55.530683840998364
At time: 51.859060764312744 and batch: 200, loss is 4.059624767303466 and perplexity is 57.952561302752244
At time: 52.305413246154785 and batch: 250, loss is 4.044712419509888 and perplexity is 57.09476433266798
At time: 52.750364780426025 and batch: 300, loss is 3.9112982177734374 and perplexity is 49.963773747020895
At time: 53.19642210006714 and batch: 350, loss is 3.9309385919570925 and perplexity is 50.95478097939348
At time: 53.64126658439636 and batch: 400, loss is 3.9478252267837526 and perplexity is 51.8225419188342
At time: 54.08617830276489 and batch: 450, loss is 4.002946004867554 and perplexity is 54.75923360889752
At time: 54.5322527885437 and batch: 500, loss is 4.030062317848206 and perplexity is 56.264417415302134
At time: 54.97791385650635 and batch: 550, loss is 3.9181480407714844 and perplexity is 50.30719158644481
At time: 55.422598123550415 and batch: 600, loss is 3.883221273422241 and perplexity is 48.5804542177497
At time: 55.8681423664093 and batch: 650, loss is 3.8612617015838624 and perplexity is 47.52527627357831
At time: 56.313894271850586 and batch: 700, loss is 3.947648687362671 and perplexity is 51.81339400479234
At time: 56.758830547332764 and batch: 750, loss is 3.9130859327316285 and perplexity is 50.05317462054568
At time: 57.20467209815979 and batch: 800, loss is 4.031135354042053 and perplexity is 56.32482357481626
At time: 57.650721311569214 and batch: 850, loss is 3.9211524295806885 and perplexity is 50.4585612226305
At time: 58.09593486785889 and batch: 900, loss is 3.85325749874115 and perplexity is 47.14639267554894
At time: 58.54068350791931 and batch: 950, loss is 3.9023704528808594 and perplexity is 49.51969419040803
At time: 58.98689842224121 and batch: 1000, loss is 3.806995711326599 and perplexity is 45.014997468633716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4338862721512955 and perplexity of 84.25823187304341
Finished 6 epochs...
Completing Train Step...
At time: 60.386008501052856 and batch: 50, loss is 3.9430863952636717 and perplexity is 51.57754458276996
At time: 60.84437298774719 and batch: 100, loss is 3.8219748306274415 and perplexity is 45.6943578939517
At time: 61.288978576660156 and batch: 150, loss is 3.93559787273407 and perplexity is 51.19274755695689
At time: 61.7485933303833 and batch: 200, loss is 3.9802416276931765 and perplexity is 53.529966987410745
At time: 62.19243860244751 and batch: 250, loss is 3.9620556259155273 and perplexity is 52.56526949554453
At time: 62.63812470436096 and batch: 300, loss is 3.8270368242263793 and perplexity is 45.92624886125449
At time: 63.08148121833801 and batch: 350, loss is 3.8521942615509035 and perplexity is 47.09629151689774
At time: 63.52846837043762 and batch: 400, loss is 3.867563591003418 and perplexity is 47.82572099969213
At time: 63.973039388656616 and batch: 450, loss is 3.9245549392700196 and perplexity is 50.630539378854735
At time: 64.41829776763916 and batch: 500, loss is 3.9554132413864136 and perplexity is 52.21726782266475
At time: 64.86410689353943 and batch: 550, loss is 3.8413013362884523 and perplexity is 46.586059139999726
At time: 65.30845212936401 and batch: 600, loss is 3.809965715408325 and perplexity is 45.1488909284936
At time: 65.75285387039185 and batch: 650, loss is 3.7885455322265624 and perplexity is 44.19207755701395
At time: 66.19809699058533 and batch: 700, loss is 3.8734345054626464 and perplexity is 48.107327551762545
At time: 66.64294028282166 and batch: 750, loss is 3.8411992740631105 and perplexity is 46.581304705762186
At time: 67.08744192123413 and batch: 800, loss is 3.9592551374435425 and perplexity is 52.418266999768
At time: 67.53275179862976 and batch: 850, loss is 3.850936951637268 and perplexity is 47.037114092654946
At time: 67.97691202163696 and batch: 900, loss is 3.7792745065689086 and perplexity is 43.78426501341376
At time: 68.42089581489563 and batch: 950, loss is 3.831956958770752 and perplexity is 46.15276898254589
At time: 68.86501216888428 and batch: 1000, loss is 3.735474696159363 and perplexity is 41.907914368157044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.438303505502096 and perplexity of 84.63124337786718
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 70.27482080459595 and batch: 50, loss is 3.9063282108306883 and perplexity is 49.71606950040267
At time: 70.73472046852112 and batch: 100, loss is 3.7568501234054565 and perplexity is 42.8133565746541
At time: 71.17857718467712 and batch: 150, loss is 3.8569671010971067 and perplexity is 47.321611840672176
At time: 71.62288188934326 and batch: 200, loss is 3.8996558141708375 and perplexity is 49.38544840848947
At time: 72.06809544563293 and batch: 250, loss is 3.8628417253494263 and perplexity is 47.600426693646135
At time: 72.51247668266296 and batch: 300, loss is 3.7109002828598023 and perplexity is 40.89060306992732
At time: 72.95600438117981 and batch: 350, loss is 3.7350913524627685 and perplexity is 41.89185231218724
At time: 73.41552519798279 and batch: 400, loss is 3.739957308769226 and perplexity is 42.09619298834577
At time: 73.85874938964844 and batch: 450, loss is 3.7829385471343993 and perplexity is 43.94498660195335
At time: 74.30274796485901 and batch: 500, loss is 3.81657292842865 and perplexity is 45.4481869359084
At time: 74.74802207946777 and batch: 550, loss is 3.6943863344192507 and perplexity is 40.22088284169541
At time: 75.19176983833313 and batch: 600, loss is 3.645910310745239 and perplexity is 38.31763793766593
At time: 75.63555550575256 and batch: 650, loss is 3.608721284866333 and perplexity is 36.918813972604205
At time: 76.08082842826843 and batch: 700, loss is 3.6814063739776612 and perplexity is 39.70219095595011
At time: 76.52596759796143 and batch: 750, loss is 3.6414372968673705 and perplexity is 38.14662536733657
At time: 76.97114062309265 and batch: 800, loss is 3.7480485010147095 and perplexity is 42.438183066587484
At time: 77.41639304161072 and batch: 850, loss is 3.617251534461975 and perplexity is 37.235087699675184
At time: 77.86096096038818 and batch: 900, loss is 3.5365338850021364 and perplexity is 34.34765968829319
At time: 78.30485010147095 and batch: 950, loss is 3.568052124977112 and perplexity is 35.447478602035744
At time: 78.74957847595215 and batch: 1000, loss is 3.4600373983383177 and perplexity is 31.81816643897009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.327881231540587 and perplexity of 75.78354855358259
Finished 8 epochs...
Completing Train Step...
At time: 80.1585202217102 and batch: 50, loss is 3.7934494495391844 and perplexity is 44.409324095387625
At time: 80.60394072532654 and batch: 100, loss is 3.6587798261642455 and perplexity is 38.813954194629744
At time: 81.0490243434906 and batch: 150, loss is 3.7734107303619386 and perplexity is 43.5282751491193
At time: 81.49403667449951 and batch: 200, loss is 3.8264049816131593 and perplexity is 45.89723986568348
At time: 81.94014072418213 and batch: 250, loss is 3.788830738067627 and perplexity is 44.20468319317707
At time: 82.38463044166565 and batch: 300, loss is 3.6410563564300538 and perplexity is 38.132096542671086
At time: 82.8296639919281 and batch: 350, loss is 3.6720811986923216 and perplexity is 39.333681942072324
At time: 83.27542638778687 and batch: 400, loss is 3.6783306980133057 and perplexity is 39.58026747616689
At time: 83.719975233078 and batch: 450, loss is 3.7271379375457765 and perplexity is 41.559990495842094
At time: 84.16415524482727 and batch: 500, loss is 3.7618062019348146 and perplexity is 43.02606960779111
At time: 84.62416577339172 and batch: 550, loss is 3.645210304260254 and perplexity is 38.29082472842532
At time: 85.06874561309814 and batch: 600, loss is 3.6023883628845215 and perplexity is 36.68574877485201
At time: 85.5130021572113 and batch: 650, loss is 3.56850492477417 and perplexity is 35.46353284755823
At time: 85.95839309692383 and batch: 700, loss is 3.6462820243835448 and perplexity is 38.33188377379678
At time: 86.40387535095215 and batch: 750, loss is 3.611798348426819 and perplexity is 37.032590468758166
At time: 86.84778690338135 and batch: 800, loss is 3.723149547576904 and perplexity is 41.394563160296045
At time: 87.29281163215637 and batch: 850, loss is 3.596244788169861 and perplexity is 36.46105804515087
At time: 87.73829007148743 and batch: 900, loss is 3.5196791458129884 and perplexity is 33.77359032734485
At time: 88.18333053588867 and batch: 950, loss is 3.558174629211426 and perplexity is 35.09906981808571
At time: 88.62800621986389 and batch: 1000, loss is 3.458086829185486 and perplexity is 31.756163395285444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.323116023366044 and perplexity of 75.4232832198938
Finished 9 epochs...
Completing Train Step...
At time: 90.03092122077942 and batch: 50, loss is 3.754407706260681 and perplexity is 42.7089160940601
At time: 90.49138355255127 and batch: 100, loss is 3.620375785827637 and perplexity is 37.351601387521974
At time: 90.93550705909729 and batch: 150, loss is 3.7365535831451417 and perplexity is 41.9531526706898
At time: 91.38073444366455 and batch: 200, loss is 3.7919041395187376 and perplexity is 44.340750918916086
At time: 91.82675862312317 and batch: 250, loss is 3.7529614114761354 and perplexity is 42.64719105851769
At time: 92.27221512794495 and batch: 300, loss is 3.6057890892028808 and perplexity is 36.81071934112651
At time: 92.71744012832642 and batch: 350, loss is 3.6392252826690674 and perplexity is 38.062337747480925
At time: 93.16511964797974 and batch: 400, loss is 3.6461644697189333 and perplexity is 38.3273779469015
At time: 93.60943961143494 and batch: 450, loss is 3.6968288946151735 and perplexity is 40.31924484782677
At time: 94.05357384681702 and batch: 500, loss is 3.732345309257507 and perplexity is 41.77697327937282
At time: 94.49891495704651 and batch: 550, loss is 3.6172763776779173 and perplexity is 37.236012750490104
At time: 94.94516944885254 and batch: 600, loss is 3.5773107624053955 and perplexity is 35.77719797486482
At time: 95.39056372642517 and batch: 650, loss is 3.544440622329712 and perplexity is 34.620314094201504
At time: 95.83563232421875 and batch: 700, loss is 3.625031061172485 and perplexity is 37.52588873960156
At time: 96.29603028297424 and batch: 750, loss is 3.5928754234313964 and perplexity is 36.33841417384152
At time: 96.74072456359863 and batch: 800, loss is 3.7064371299743653 and perplexity is 40.70850871659026
At time: 97.18595552444458 and batch: 850, loss is 3.5803494358062746 and perplexity is 35.886078537163925
At time: 97.63141536712646 and batch: 900, loss is 3.5053141117095947 and perplexity is 33.29189958567817
At time: 98.07594990730286 and batch: 950, loss is 3.547226948738098 and perplexity is 34.716912103925736
At time: 98.52067804336548 and batch: 1000, loss is 3.450318765640259 and perplexity is 31.510435152042287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.322535817216083 and perplexity of 75.3795348598804
Finished 10 epochs...
Completing Train Step...
At time: 99.92311596870422 and batch: 50, loss is 3.725457992553711 and perplexity is 41.490230610729576
At time: 100.38325810432434 and batch: 100, loss is 3.5919716262817385 and perplexity is 36.30558645572176
At time: 100.82833623886108 and batch: 150, loss is 3.708878421783447 and perplexity is 40.808011473694044
At time: 101.27332615852356 and batch: 200, loss is 3.766057963371277 and perplexity is 43.20939564442592
At time: 101.71863269805908 and batch: 250, loss is 3.7258395624160765 and perplexity is 41.50606505309411
At time: 102.16423845291138 and batch: 300, loss is 3.5795592880249023 and perplexity is 35.85773443131649
At time: 102.60928678512573 and batch: 350, loss is 3.614348945617676 and perplexity is 37.12716625106803
At time: 103.05605340003967 and batch: 400, loss is 3.62199209690094 and perplexity is 37.41202201055738
At time: 103.50323486328125 and batch: 450, loss is 3.6737692308425904 and perplexity is 39.400134533054555
At time: 103.94771099090576 and batch: 500, loss is 3.7100477266311644 and perplexity is 40.855756388075825
At time: 104.39201831817627 and batch: 550, loss is 3.5957732963562012 and perplexity is 36.443871006857535
At time: 104.83733487129211 and batch: 600, loss is 3.557526879310608 and perplexity is 35.07634176093532
At time: 105.28188252449036 and batch: 650, loss is 3.5249419450759887 and perplexity is 33.95180248971771
At time: 105.72703623771667 and batch: 700, loss is 3.6078179264068604 and perplexity is 36.88547810907252
At time: 106.17299151420593 and batch: 750, loss is 3.577000460624695 and perplexity is 35.76609796888962
At time: 106.61760020256042 and batch: 800, loss is 3.69198956489563 and perplexity is 40.12459808776157
At time: 107.06199216842651 and batch: 850, loss is 3.5658663940429687 and perplexity is 35.370084563601836
At time: 107.53582954406738 and batch: 900, loss is 3.491433262825012 and perplexity is 32.832972277496964
At time: 107.98043632507324 and batch: 950, loss is 3.5355597877502443 and perplexity is 34.31421801774589
At time: 108.42544460296631 and batch: 1000, loss is 3.440579614639282 and perplexity is 31.205039825508887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.32329242985423 and perplexity of 75.43658955004105
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 109.8413610458374 and batch: 50, loss is 3.719246020317078 and perplexity is 41.233293319979545
At time: 110.28536105155945 and batch: 100, loss is 3.592331638336182 and perplexity is 36.31865925753138
At time: 110.72989988327026 and batch: 150, loss is 3.702095293998718 and perplexity is 40.53214220323086
At time: 111.1744327545166 and batch: 200, loss is 3.7592595863342284 and perplexity is 42.916638146769785
At time: 111.61979913711548 and batch: 250, loss is 3.715024781227112 and perplexity is 41.05960457917151
At time: 112.06456470489502 and batch: 300, loss is 3.562159152030945 and perplexity is 35.23920185697355
At time: 112.50891613960266 and batch: 350, loss is 3.5969023847579957 and perplexity is 36.48504259773681
At time: 112.95437908172607 and batch: 400, loss is 3.6030417919158935 and perplexity is 36.709728141693134
At time: 113.40010285377502 and batch: 450, loss is 3.651395111083984 and perplexity is 38.5283799418012
At time: 113.84461951255798 and batch: 500, loss is 3.6844340562820435 and perplexity is 39.82257873295889
At time: 114.28875994682312 and batch: 550, loss is 3.5689885425567627 and perplexity is 35.48068779056025
At time: 114.73658299446106 and batch: 600, loss is 3.5264893865585325 and perplexity is 34.00438158834455
At time: 115.18121075630188 and batch: 650, loss is 3.485280785560608 and perplexity is 32.631588302448975
At time: 115.62664556503296 and batch: 700, loss is 3.570796933174133 and perplexity is 35.54490878427809
At time: 116.07388210296631 and batch: 750, loss is 3.5373119497299195 and perplexity is 34.37439479024971
At time: 116.5190749168396 and batch: 800, loss is 3.6480338048934935 and perplexity is 38.39909167025597
At time: 116.96298956871033 and batch: 850, loss is 3.512977194786072 and perplexity is 33.5479981771328
At time: 117.40817332267761 and batch: 900, loss is 3.4362593412399294 and perplexity is 31.070516320422673
At time: 117.85257172584534 and batch: 950, loss is 3.4758287239074708 and perplexity is 32.324605613147604
At time: 118.29669690132141 and batch: 1000, loss is 3.377726001739502 and perplexity is 29.304057912467844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.305332462962081 and perplexity of 74.09384481756597
Finished 12 epochs...
Completing Train Step...
At time: 119.69468569755554 and batch: 50, loss is 3.6990185499191286 and perplexity is 40.407626823877656
At time: 120.1549904346466 and batch: 100, loss is 3.56674786567688 and perplexity is 35.40127603501269
At time: 120.6010410785675 and batch: 150, loss is 3.680658164024353 and perplexity is 39.67249649174085
At time: 121.04625511169434 and batch: 200, loss is 3.7411229276657103 and perplexity is 42.14528971484026
At time: 121.49157571792603 and batch: 250, loss is 3.6976684522628784 and perplexity is 40.35310939181776
At time: 121.93771195411682 and batch: 300, loss is 3.5457695150375366 and perplexity is 34.66635135966208
At time: 122.3830099105835 and batch: 350, loss is 3.5831449127197263 and perplexity is 35.9865375912908
At time: 122.8280177116394 and batch: 400, loss is 3.5901169061660765 and perplexity is 36.238312160990496
At time: 123.27407121658325 and batch: 450, loss is 3.6399969387054445 and perplexity is 38.09172011524513
At time: 123.71889352798462 and batch: 500, loss is 3.674251480102539 and perplexity is 39.41913980104488
At time: 124.16441106796265 and batch: 550, loss is 3.559142551422119 and perplexity is 35.1330594343359
At time: 124.61011815071106 and batch: 600, loss is 3.517970600128174 and perplexity is 33.71593587199089
At time: 125.056880235672 and batch: 650, loss is 3.4782672548294067 and perplexity is 32.4035263496948
At time: 125.50242328643799 and batch: 700, loss is 3.5654204273223877 and perplexity is 35.354314199772425
At time: 125.94848585128784 and batch: 750, loss is 3.5329131650924683 and perplexity is 34.2235213037866
At time: 126.393146276474 and batch: 800, loss is 3.6447528743743898 and perplexity is 38.27331336625569
At time: 126.83807420730591 and batch: 850, loss is 3.511139712333679 and perplexity is 33.48641091925054
At time: 127.28452110290527 and batch: 900, loss is 3.436203565597534 and perplexity is 31.06878339074343
At time: 127.73045039176941 and batch: 950, loss is 3.477100553512573 and perplexity is 32.36574315796818
At time: 128.1751685142517 and batch: 1000, loss is 3.3812806367874146 and perplexity is 29.40840849801771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3041765166492 and perplexity of 74.00824579431111
Finished 13 epochs...
Completing Train Step...
At time: 129.57405996322632 and batch: 50, loss is 3.6913715600967407 and perplexity is 40.09980855440411
At time: 130.03244352340698 and batch: 100, loss is 3.5579460763931277 and perplexity is 35.09104874341367
At time: 130.47687220573425 and batch: 150, loss is 3.672189769744873 and perplexity is 39.33795267315619
At time: 130.93655610084534 and batch: 200, loss is 3.7328428125381468 and perplexity is 41.79776263159116
At time: 131.37998127937317 and batch: 250, loss is 3.6893647146224975 and perplexity is 40.01941513065955
At time: 131.82543921470642 and batch: 300, loss is 3.5376659440994263 and perplexity is 34.386565286477136
At time: 132.27103781700134 and batch: 350, loss is 3.5758174228668214 and perplexity is 35.72381034340151
At time: 132.71532583236694 and batch: 400, loss is 3.583103365898132 and perplexity is 35.98504249609214
At time: 133.1591351032257 and batch: 450, loss is 3.633509860038757 and perplexity is 37.84541589149694
At time: 133.60296964645386 and batch: 500, loss is 3.6681916189193724 and perplexity is 39.18098759931553
At time: 134.04659914970398 and batch: 550, loss is 3.55327853679657 and perplexity is 34.92764153539063
At time: 134.49198603630066 and batch: 600, loss is 3.512785334587097 and perplexity is 33.54156226894438
At time: 134.93871545791626 and batch: 650, loss is 3.4737288761138916 and perplexity is 32.25680007694024
At time: 135.38771677017212 and batch: 700, loss is 3.5618577432632446 and perplexity is 35.22858205309849
At time: 135.83298063278198 and batch: 750, loss is 3.530026421546936 and perplexity is 34.124869234677384
At time: 136.27816700935364 and batch: 800, loss is 3.6423850631713868 and perplexity is 38.18279659169787
At time: 136.72413778305054 and batch: 850, loss is 3.5093674087524414 and perplexity is 33.42711539361267
At time: 137.1686635017395 and batch: 900, loss is 3.4353559970855714 and perplexity is 31.04246162456228
At time: 137.61264085769653 and batch: 950, loss is 3.476781659126282 and perplexity is 32.35542354968699
At time: 138.05669522285461 and batch: 1000, loss is 3.3819038534164427 and perplexity is 29.426742019511266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.303865014052972 and perplexity of 73.98519562388427
Finished 14 epochs...
Completing Train Step...
At time: 139.46828985214233 and batch: 50, loss is 3.6848108530044557 and perplexity is 39.8375865773792
At time: 139.9133496284485 and batch: 100, loss is 3.5509721279144286 and perplexity is 34.847176940518736
At time: 140.357919216156 and batch: 150, loss is 3.6655043935775757 and perplexity is 39.075840796372425
At time: 140.80250668525696 and batch: 200, loss is 3.72631112575531 and perplexity is 41.525642407347476
At time: 141.24826860427856 and batch: 250, loss is 3.6827881431579588 and perplexity is 39.757088138686385
At time: 141.6915979385376 and batch: 300, loss is 3.5312593936920167 and perplexity is 34.16697019722175
At time: 142.15066289901733 and batch: 350, loss is 3.5699044704437255 and perplexity is 35.51320042929952
At time: 142.59772443771362 and batch: 400, loss is 3.577505702972412 and perplexity is 35.78417308196789
At time: 143.0416214466095 and batch: 450, loss is 3.6282653188705445 and perplexity is 37.647453614231615
At time: 143.48412442207336 and batch: 500, loss is 3.6632294178009035 and perplexity is 38.98704524726611
At time: 143.92919421195984 and batch: 550, loss is 3.5484926509857178 and perplexity is 34.760881197626574
At time: 144.372620344162 and batch: 600, loss is 3.5084537649154663 and perplexity is 33.39658886295723
At time: 144.81633734703064 and batch: 650, loss is 3.4698236703872682 and perplexity is 32.13107628497397
At time: 145.259761095047 and batch: 700, loss is 3.558675684928894 and perplexity is 35.11666081436248
At time: 145.70339369773865 and batch: 750, loss is 3.527357702255249 and perplexity is 34.033920949524344
At time: 146.14673900604248 and batch: 800, loss is 3.640007357597351 and perplexity is 38.092116990827044
At time: 146.59488224983215 and batch: 850, loss is 3.507302746772766 and perplexity is 33.35817089740082
At time: 147.03941226005554 and batch: 900, loss is 3.433947858810425 and perplexity is 30.998780308078594
At time: 147.48287320137024 and batch: 950, loss is 3.4757325649261475 and perplexity is 32.32149746144091
At time: 147.93243217468262 and batch: 1000, loss is 3.381442666053772 and perplexity is 29.41317390692881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.303841195455411 and perplexity of 73.98343342127092
Finished 15 epochs...
Completing Train Step...
At time: 149.34282803535461 and batch: 50, loss is 3.678901262283325 and perplexity is 39.60285700636318
At time: 149.80395460128784 and batch: 100, loss is 3.5448939418792724 and perplexity is 34.636011717143184
At time: 150.2492606639862 and batch: 150, loss is 3.65967782497406 and perplexity is 38.84882473380875
At time: 150.6957983970642 and batch: 200, loss is 3.720636692047119 and perplexity is 41.29067518575752
At time: 151.14069533348083 and batch: 250, loss is 3.6770631217956544 and perplexity is 39.53012825478155
At time: 151.58499813079834 and batch: 300, loss is 3.5256597423553466 and perplexity is 33.97618174981066
At time: 152.02927565574646 and batch: 350, loss is 3.564688353538513 and perplexity is 35.3284417046431
At time: 152.474436044693 and batch: 400, loss is 3.5725792980194093 and perplexity is 35.608319273650274
At time: 152.91964316368103 and batch: 450, loss is 3.623617744445801 and perplexity is 37.472890234021285
At time: 153.363929271698 and batch: 500, loss is 3.658804221153259 and perplexity is 38.814901072165384
At time: 153.82473969459534 and batch: 550, loss is 3.5442125463485716 and perplexity is 34.612418932479684
At time: 154.26874613761902 and batch: 600, loss is 3.504515080451965 and perplexity is 33.265308942075784
At time: 154.71441006660461 and batch: 650, loss is 3.4662068271636963 and perplexity is 32.0150731285202
At time: 155.16008520126343 and batch: 700, loss is 3.5556405925750734 and perplexity is 35.01024008569676
At time: 155.60445380210876 and batch: 750, loss is 3.524738059043884 and perplexity is 33.94488089705729
At time: 156.048481464386 and batch: 800, loss is 3.6375756645202637 and perplexity is 37.99960118426019
At time: 156.49439644813538 and batch: 850, loss is 3.5050546741485595 and perplexity is 33.28326353675361
At time: 156.93907594680786 and batch: 900, loss is 3.4322125339508056 and perplexity is 30.94503400113081
At time: 157.3861494064331 and batch: 950, loss is 3.474294328689575 and perplexity is 32.27504492543921
At time: 157.83088064193726 and batch: 1000, loss is 3.3804438734054565 and perplexity is 29.38381091128005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.303966987423781 and perplexity of 73.99274052835511
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 159.2455825805664 and batch: 50, loss is 3.6785056924819948 and perplexity is 39.587194410114336
At time: 159.70631909370422 and batch: 100, loss is 3.5472221183776855 and perplexity is 34.71674440913288
At time: 160.15157985687256 and batch: 150, loss is 3.662243709564209 and perplexity is 38.94863432973046
At time: 160.5976266860962 and batch: 200, loss is 3.7236783123016357 and perplexity is 41.4164569329081
At time: 161.0432333946228 and batch: 250, loss is 3.6788829374313354 and perplexity is 39.60213129651945
At time: 161.4884078502655 and batch: 300, loss is 3.524370884895325 and perplexity is 33.93241950220699
At time: 161.93342804908752 and batch: 350, loss is 3.5640663719177246 and perplexity is 35.3064748953957
At time: 162.38037657737732 and batch: 400, loss is 3.570677981376648 and perplexity is 35.54068090494866
At time: 162.82575941085815 and batch: 450, loss is 3.621064567565918 and perplexity is 37.377337350666494
At time: 163.2718689441681 and batch: 500, loss is 3.6583934879302977 and perplexity is 38.79896177637233
At time: 163.71940064430237 and batch: 550, loss is 3.541084399223328 and perplexity is 34.504315363784315
At time: 164.1648919582367 and batch: 600, loss is 3.5006328344345095 and perplexity is 33.13641518943364
At time: 164.60994625091553 and batch: 650, loss is 3.458118028640747 and perplexity is 31.75715418574054
At time: 165.0702621936798 and batch: 700, loss is 3.5444561767578127 and perplexity is 34.62085259757595
At time: 165.51628303527832 and batch: 750, loss is 3.514053897857666 and perplexity is 33.58413886271808
At time: 165.9608838558197 and batch: 800, loss is 3.6249978733062744 and perplexity is 37.52464335609256
At time: 166.40561079978943 and batch: 850, loss is 3.4929613065719605 and perplexity is 32.8831808461546
At time: 166.8510730266571 and batch: 900, loss is 3.417305965423584 and perplexity is 30.48717079414102
At time: 167.29559087753296 and batch: 950, loss is 3.4597844076156616 and perplexity is 31.810117756213067
At time: 167.74122023582458 and batch: 1000, loss is 3.362046914100647 and perplexity is 28.848180232472664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2988366848084985 and perplexity of 73.6141074596901
Finished 17 epochs...
Completing Train Step...
At time: 169.15518403053284 and batch: 50, loss is 3.6747355794906618 and perplexity is 39.43822720222974
At time: 169.60095524787903 and batch: 100, loss is 3.5405781173706057 and perplexity is 34.486850876424576
At time: 170.0452995300293 and batch: 150, loss is 3.6564279079437254 and perplexity is 38.722774214507574
At time: 170.4898784160614 and batch: 200, loss is 3.7183108711242676 and perplexity is 41.194752062735965
At time: 170.93616247177124 and batch: 250, loss is 3.67357394695282 and perplexity is 39.39244107275434
At time: 171.38118314743042 and batch: 300, loss is 3.519383206367493 and perplexity is 33.76359686855324
At time: 171.8259518146515 and batch: 350, loss is 3.559691252708435 and perplexity is 35.15234227901677
At time: 172.27172374725342 and batch: 400, loss is 3.5665008926391604 and perplexity is 35.39253395390478
At time: 172.71715664863586 and batch: 450, loss is 3.6174069452285766 and perplexity is 37.24087488288271
At time: 173.16208839416504 and batch: 500, loss is 3.6549017906188963 and perplexity is 38.66372378831347
At time: 173.60759782791138 and batch: 550, loss is 3.5378002452850343 and perplexity is 34.391183753089926
At time: 174.05273628234863 and batch: 600, loss is 3.498043026924133 and perplexity is 33.05070928131455
At time: 174.49768996238708 and batch: 650, loss is 3.4560166931152345 and perplexity is 31.69049181399544
At time: 174.94302940368652 and batch: 700, loss is 3.543299798965454 and perplexity is 34.580840951227884
At time: 175.3879783153534 and batch: 750, loss is 3.513670778274536 and perplexity is 33.571274585872764
At time: 175.83362770080566 and batch: 800, loss is 3.6246830463409423 and perplexity is 37.512831445951065
At time: 176.28006672859192 and batch: 850, loss is 3.4933066940307618 and perplexity is 32.89454024600856
At time: 176.74278473854065 and batch: 900, loss is 3.4184393072128296 and perplexity is 30.52174276606761
At time: 177.1893937587738 and batch: 950, loss is 3.461604132652283 and perplexity is 31.868056123872936
At time: 177.63577270507812 and batch: 1000, loss is 3.36470251083374 and perplexity is 28.924891177239154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.298094586628239 and perplexity of 73.55949882949153
Finished 18 epochs...
Completing Train Step...
At time: 179.03004264831543 and batch: 50, loss is 3.6730891418457032 and perplexity is 39.37334804471318
At time: 179.4898841381073 and batch: 100, loss is 3.538108024597168 and perplexity is 34.401770277041884
At time: 179.93349623680115 and batch: 150, loss is 3.653942403793335 and perplexity is 38.62664810888679
At time: 180.37726473808289 and batch: 200, loss is 3.716048865318298 and perplexity is 41.101674604956756
At time: 180.82270193099976 and batch: 250, loss is 3.671282753944397 and perplexity is 39.30228870485962
At time: 181.2671604156494 and batch: 300, loss is 3.5173075866699217 and perplexity is 33.693589161653655
At time: 181.7113003730774 and batch: 350, loss is 3.557693076133728 and perplexity is 35.082171821958056
At time: 182.15658283233643 and batch: 400, loss is 3.564516377449036 and perplexity is 35.32236657979449
At time: 182.60220503807068 and batch: 450, loss is 3.6156581974029542 and perplexity is 37.17580689424095
At time: 183.0600254535675 and batch: 500, loss is 3.65322630405426 and perplexity is 38.59899747774115
At time: 183.50818634033203 and batch: 550, loss is 3.5362015771865845 and perplexity is 34.3362475888028
At time: 183.95551800727844 and batch: 600, loss is 3.496728067398071 and perplexity is 33.00727749807876
At time: 184.4033236503601 and batch: 650, loss is 3.455004105567932 and perplexity is 31.658418657796453
At time: 184.84838676452637 and batch: 700, loss is 3.5427665948867797 and perplexity is 34.5624072206956
At time: 185.2944736480713 and batch: 750, loss is 3.513401608467102 and perplexity is 33.562239428407516
At time: 185.73879289627075 and batch: 800, loss is 3.6245069169998168 and perplexity is 37.50622491748357
At time: 186.1837239265442 and batch: 850, loss is 3.493441023826599 and perplexity is 32.89895925967993
At time: 186.62920999526978 and batch: 900, loss is 3.4189296627044676 and perplexity is 30.536712940300927
At time: 187.07565450668335 and batch: 950, loss is 3.4623592376708983 and perplexity is 31.89212894059412
At time: 187.51888489723206 and batch: 1000, loss is 3.3657564973831176 and perplexity is 28.9553936952803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2978973388671875 and perplexity of 73.54499081392716
Finished 19 epochs...
Completing Train Step...
At time: 188.9482970237732 and batch: 50, loss is 3.671721143722534 and perplexity is 39.319522203704096
At time: 189.40917491912842 and batch: 100, loss is 3.536366677284241 and perplexity is 34.341916974628056
At time: 189.85739994049072 and batch: 150, loss is 3.6521945285797117 and perplexity is 38.55919251722371
At time: 190.3029956817627 and batch: 200, loss is 3.714444427490234 and perplexity is 41.03578239753396
At time: 190.747731924057 and batch: 250, loss is 3.6696755123138427 and perplexity is 39.23917116643204
At time: 191.19381380081177 and batch: 300, loss is 3.5158007192611693 and perplexity is 33.642855624218946
At time: 191.6380786895752 and batch: 350, loss is 3.556242346763611 and perplexity is 35.0313139843275
At time: 192.0842320919037 and batch: 400, loss is 3.5631122732162477 and perplexity is 35.27280509825329
At time: 192.52963018417358 and batch: 450, loss is 3.614396324157715 and perplexity is 37.128925323671616
At time: 192.97556519508362 and batch: 500, loss is 3.652066216468811 and perplexity is 38.55424522324285
At time: 193.41955757141113 and batch: 550, loss is 3.5350683450698854 and perplexity is 34.297358689504875
At time: 193.86565399169922 and batch: 600, loss is 3.4957644605636595 and perplexity is 32.97548677923317
At time: 194.3097846508026 and batch: 650, loss is 3.454235873222351 and perplexity is 31.63410697627934
At time: 194.7542655467987 and batch: 700, loss is 3.542270288467407 and perplexity is 34.54525793212601
At time: 195.20009446144104 and batch: 750, loss is 3.513074269294739 and perplexity is 33.55125499064683
At time: 195.64600610733032 and batch: 800, loss is 3.6242528200149535 and perplexity is 37.4966959095158
At time: 196.09029293060303 and batch: 850, loss is 3.4933665132522584 and perplexity is 32.896508030652654
At time: 196.53665828704834 and batch: 900, loss is 3.4190746402740477 and perplexity is 30.54114039965937
At time: 196.98149251937866 and batch: 950, loss is 3.4626431560516355 and perplexity is 31.901184987729483
At time: 197.4260380268097 and batch: 1000, loss is 3.3662018632888793 and perplexity is 28.968292312509053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29784076969798 and perplexity of 73.54083055256982
Finished 20 epochs...
Completing Train Step...
At time: 198.85895681381226 and batch: 50, loss is 3.670455799102783 and perplexity is 39.26980092174482
At time: 199.3024878501892 and batch: 100, loss is 3.534896879196167 and perplexity is 34.29147836708266
At time: 199.7612955570221 and batch: 150, loss is 3.650746202468872 and perplexity is 38.5033866541875
At time: 200.20529294013977 and batch: 200, loss is 3.713102116584778 and perplexity is 40.980736571878985
At time: 200.64929294586182 and batch: 250, loss is 3.6683328580856323 and perplexity is 39.1865218801567
At time: 201.0957088470459 and batch: 300, loss is 3.5145014333724975 and perplexity is 33.599172321346586
At time: 201.53936529159546 and batch: 350, loss is 3.5550071811676025 and perplexity is 34.988071221994964
At time: 201.9822292327881 and batch: 400, loss is 3.561944169998169 and perplexity is 35.23162687599651
At time: 202.42830157279968 and batch: 450, loss is 3.613334584236145 and perplexity is 37.089524981569845
At time: 202.87306332588196 and batch: 500, loss is 3.6511083745956423 and perplexity is 38.5173340331434
At time: 203.31802773475647 and batch: 550, loss is 3.5341156482696534 and perplexity is 34.264699265359646
At time: 203.7627513408661 and batch: 600, loss is 3.49493709564209 and perplexity is 32.94821530148806
At time: 204.20657324790955 and batch: 650, loss is 3.4535458993911745 and perplexity is 31.612287798489525
At time: 204.65004873275757 and batch: 700, loss is 3.541766104698181 and perplexity is 34.527845163759366
At time: 205.0938801765442 and batch: 750, loss is 3.512705407142639 and perplexity is 33.53888148472464
At time: 205.5379455089569 and batch: 800, loss is 3.6239373445510865 and perplexity is 37.48486848770905
At time: 205.9810779094696 and batch: 850, loss is 3.493174195289612 and perplexity is 32.890182049569404
At time: 206.4248354434967 and batch: 900, loss is 3.4190409755706788 and perplexity is 30.540112258533394
At time: 206.87026739120483 and batch: 950, loss is 3.4627011394500733 and perplexity is 31.90303478047738
At time: 207.31433701515198 and batch: 1000, loss is 3.3663724565505984 and perplexity is 28.973234529523552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2978336985518295 and perplexity of 73.5403105364475
Finished 21 epochs...
Completing Train Step...
At time: 208.7079062461853 and batch: 50, loss is 3.6692527866363527 and perplexity is 39.22258726668365
At time: 209.1680941581726 and batch: 100, loss is 3.5335672235488893 and perplexity is 34.24591280918233
At time: 209.61200547218323 and batch: 150, loss is 3.6494547843933107 and perplexity is 38.45369477809163
At time: 210.0552055835724 and batch: 200, loss is 3.711897120475769 and perplexity is 40.93138468415353
At time: 210.49838709831238 and batch: 250, loss is 3.667122845649719 and perplexity is 39.13913437687745
At time: 210.95762634277344 and batch: 300, loss is 3.513309860229492 and perplexity is 33.55916029334448
At time: 211.40172696113586 and batch: 350, loss is 3.5538825464248656 and perplexity is 34.94874453973898
At time: 211.84516191482544 and batch: 400, loss is 3.5608933782577514 and perplexity is 35.19462521739702
At time: 212.28887724876404 and batch: 450, loss is 3.6123725318908693 and perplexity is 37.05386007558019
At time: 212.73313856124878 and batch: 500, loss is 3.6502443075180055 and perplexity is 38.48406684749677
At time: 213.1767077445984 and batch: 550, loss is 3.53324716091156 and perplexity is 34.23495372590097
At time: 213.62088680267334 and batch: 600, loss is 3.4941734647750855 and perplexity is 32.923064631407314
At time: 214.06767320632935 and batch: 650, loss is 3.4528870153427125 and perplexity is 31.591465826709527
At time: 214.5117223262787 and batch: 700, loss is 3.5412528848648073 and perplexity is 34.51012933526379
At time: 214.95578122138977 and batch: 750, loss is 3.5123101663589478 and perplexity is 33.52562817022022
At time: 215.40154004096985 and batch: 800, loss is 3.6235817575454714 and perplexity is 37.471541725119856
At time: 215.84631943702698 and batch: 850, loss is 3.4929128646850587 and perplexity is 32.88158796140856
At time: 216.29195165634155 and batch: 900, loss is 3.4189069414138795 and perplexity is 30.53601911465494
At time: 216.7372488975525 and batch: 950, loss is 3.4626378297805784 and perplexity is 31.9010150738237
At time: 217.18034076690674 and batch: 1000, loss is 3.366399636268616 and perplexity is 28.97402202457002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.297848957340892 and perplexity of 73.54143268109483
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 218.5927698612213 and batch: 50, loss is 3.6692122983932496 and perplexity is 39.22099924518358
At time: 219.05144262313843 and batch: 100, loss is 3.5339522409439086 and perplexity is 34.25910061992754
At time: 219.4964475631714 and batch: 150, loss is 3.650030312538147 and perplexity is 38.47583233149083
At time: 219.9409589767456 and batch: 200, loss is 3.7122246408462525 and perplexity is 40.94479274201587
At time: 220.38547778129578 and batch: 250, loss is 3.6674990224838258 and perplexity is 39.153860382154384
At time: 220.82976365089417 and batch: 300, loss is 3.512602686882019 and perplexity is 33.53543653901351
At time: 221.27538537979126 and batch: 350, loss is 3.5533795881271364 and perplexity is 34.931171198376894
At time: 221.7207736968994 and batch: 400, loss is 3.5602026176452637 and perplexity is 35.170322551154214
At time: 222.16422510147095 and batch: 450, loss is 3.611523108482361 and perplexity is 37.022399023225205
At time: 222.62476062774658 and batch: 500, loss is 3.64946044921875 and perplexity is 38.45391261217704
At time: 223.06893920898438 and batch: 550, loss is 3.532212896347046 and perplexity is 34.199564030703044
At time: 223.51336407661438 and batch: 600, loss is 3.493814854621887 and perplexity is 32.91126020287655
At time: 223.95758652687073 and batch: 650, loss is 3.451018662452698 and perplexity is 31.53249692474537
At time: 224.40748023986816 and batch: 700, loss is 3.5380215358734133 and perplexity is 34.39879504049977
At time: 224.85157132148743 and batch: 750, loss is 3.50804753780365 and perplexity is 33.38302501830616
At time: 225.29641890525818 and batch: 800, loss is 3.619928479194641 and perplexity is 37.334897504625644
At time: 225.74067378044128 and batch: 850, loss is 3.488964433670044 and perplexity is 32.75201325641105
At time: 226.183513879776 and batch: 900, loss is 3.414227595329285 and perplexity is 30.39346430493526
At time: 226.62911415100098 and batch: 950, loss is 3.4579615926742555 and perplexity is 31.752186613196116
At time: 227.07422471046448 and batch: 1000, loss is 3.3612339162826537 and perplexity is 28.82473625613269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.297208460365853 and perplexity of 73.49434469738792
Finished 23 epochs...
Completing Train Step...
At time: 228.48483896255493 and batch: 50, loss is 3.668675813674927 and perplexity is 39.19996342165466
At time: 228.93022346496582 and batch: 100, loss is 3.533242015838623 and perplexity is 34.23477758502019
At time: 229.37591576576233 and batch: 150, loss is 3.649307017326355 and perplexity is 38.448013008200235
At time: 229.82307028770447 and batch: 200, loss is 3.7115261602401732 and perplexity is 40.91620358401417
At time: 230.26704740524292 and batch: 250, loss is 3.666782865524292 and perplexity is 39.125830110784065
At time: 230.71125030517578 and batch: 300, loss is 3.512003107070923 and perplexity is 33.51533539503066
At time: 231.15697026252747 and batch: 350, loss is 3.552823920249939 and perplexity is 34.91176646042314
At time: 231.60230493545532 and batch: 400, loss is 3.5595573377609253 and perplexity is 35.147635170128794
At time: 232.04613757133484 and batch: 450, loss is 3.6110144090652465 and perplexity is 37.0035705398472
At time: 232.49187874794006 and batch: 500, loss is 3.6490420722961425 and perplexity is 38.43782774755884
At time: 232.93757796287537 and batch: 550, loss is 3.531797752380371 and perplexity is 34.18536923467862
At time: 233.38171124458313 and batch: 600, loss is 3.4933909797668456 and perplexity is 32.897312903392425
At time: 233.85405135154724 and batch: 650, loss is 3.4508073043823244 and perplexity is 31.525832981305218
At time: 234.2985384464264 and batch: 700, loss is 3.5378764057159424 and perplexity is 34.39380310020804
At time: 234.74423170089722 and batch: 750, loss is 3.508212614059448 and perplexity is 33.38853621795498
At time: 235.18906617164612 and batch: 800, loss is 3.6199941062927246 and perplexity is 37.33734776600701
At time: 235.63328313827515 and batch: 850, loss is 3.489175057411194 and perplexity is 32.75891233450262
At time: 236.0797724723816 and batch: 900, loss is 3.414507565498352 and perplexity is 30.40197475955637
At time: 236.52450966835022 and batch: 950, loss is 3.458334708213806 and perplexity is 31.76403605790472
At time: 236.97010207176208 and batch: 1000, loss is 3.361679530143738 and perplexity is 28.8375838204643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296871650509718 and perplexity of 73.46959524588722
Finished 24 epochs...
Completing Train Step...
At time: 238.37392926216125 and batch: 50, loss is 3.668266477584839 and perplexity is 39.183920745543205
At time: 238.83350896835327 and batch: 100, loss is 3.532706866264343 and perplexity is 34.216461759660184
At time: 239.27732515335083 and batch: 150, loss is 3.648782958984375 and perplexity is 38.427869284954866
At time: 239.72342133522034 and batch: 200, loss is 3.7110040283203123 and perplexity is 40.8948455044362
At time: 240.16728711128235 and batch: 250, loss is 3.666255693435669 and perplexity is 39.10520950098798
At time: 240.61150193214417 and batch: 300, loss is 3.5115501403808596 and perplexity is 33.50015750228985
At time: 241.05760669708252 and batch: 350, loss is 3.5524085760116577 and perplexity is 34.89726907028773
At time: 241.50193428993225 and batch: 400, loss is 3.5591040420532227 and perplexity is 35.13170650844001
At time: 241.94661498069763 and batch: 450, loss is 3.6106347465515136 and perplexity is 36.98952433781586
At time: 242.39388728141785 and batch: 500, loss is 3.6487323951721193 and perplexity is 38.42592627451037
At time: 242.83964014053345 and batch: 550, loss is 3.531497778892517 and perplexity is 34.17511606815167
At time: 243.28303933143616 and batch: 600, loss is 3.4930886697769163 and perplexity is 32.88736922017302
At time: 243.72797226905823 and batch: 650, loss is 3.450652313232422 and perplexity is 31.520947134841077
At time: 244.17309093475342 and batch: 700, loss is 3.5377854347229003 and perplexity is 34.39067440409797
At time: 244.61717915534973 and batch: 750, loss is 3.508316955566406 and perplexity is 33.39202020989841
At time: 245.06229496002197 and batch: 800, loss is 3.6200464487075807 and perplexity is 37.339302144101396
At time: 245.52275037765503 and batch: 850, loss is 3.4893367671966553 and perplexity is 32.76421019953494
At time: 245.9666075706482 and batch: 900, loss is 3.414714913368225 and perplexity is 30.408279197846035
At time: 246.4110176563263 and batch: 950, loss is 3.458604211807251 and perplexity is 31.772597733413974
At time: 246.85690546035767 and batch: 1000, loss is 3.362009410858154 and perplexity is 28.847098352461057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296677380073361 and perplexity of 73.45532366187813
Finished 25 epochs...
Completing Train Step...
At time: 248.25304579734802 and batch: 50, loss is 3.667919883728027 and perplexity is 39.17034219258445
At time: 248.7113778591156 and batch: 100, loss is 3.53226309299469 and perplexity is 34.201280777255384
At time: 249.15580439567566 and batch: 150, loss is 3.6483514165878295 and perplexity is 38.41128960782359
At time: 249.60051488876343 and batch: 200, loss is 3.7105816793441773 and perplexity is 40.877577255178586
At time: 250.04392766952515 and batch: 250, loss is 3.6658268642425536 and perplexity is 39.088443640653175
At time: 250.48886251449585 and batch: 300, loss is 3.5111734104156493 and perplexity is 33.48753936608372
At time: 250.93339920043945 and batch: 350, loss is 3.552064690589905 and perplexity is 34.88527047138533
At time: 251.38239073753357 and batch: 400, loss is 3.558741512298584 and perplexity is 35.118972527862354
At time: 251.82554602622986 and batch: 450, loss is 3.610326342582703 and perplexity is 36.978118380617296
At time: 252.27005124092102 and batch: 500, loss is 3.648476071357727 and perplexity is 38.41607805673642
At time: 252.71559238433838 and batch: 550, loss is 3.5312509298324586 and perplexity is 34.16668101400828
At time: 253.15945887565613 and batch: 600, loss is 3.492848124504089 and perplexity is 32.879459270360044
At time: 253.6030147075653 and batch: 650, loss is 3.450516381263733 and perplexity is 31.51666272164305
At time: 254.04712986946106 and batch: 700, loss is 3.5377093076705934 and perplexity is 34.38805644307884
At time: 254.49112701416016 and batch: 750, loss is 3.508375072479248 and perplexity is 33.39396090741969
At time: 254.93537378311157 and batch: 800, loss is 3.6200787019729614 and perplexity is 37.340506477944324
At time: 255.38077855110168 and batch: 850, loss is 3.4894515800476076 and perplexity is 32.76797216787418
At time: 255.82600569725037 and batch: 900, loss is 3.414867959022522 and perplexity is 30.412933408976222
At time: 256.2704236507416 and batch: 950, loss is 3.4588017988204958 and perplexity is 31.77887620635508
At time: 256.73041129112244 and batch: 1000, loss is 3.3622563886642456 and perplexity is 28.85422382540487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296557542754383 and perplexity of 73.44652150025006
Finished 26 epochs...
Completing Train Step...
At time: 258.138498544693 and batch: 50, loss is 3.667610774040222 and perplexity is 39.158236131484855
At time: 258.5829048156738 and batch: 100, loss is 3.531877489089966 and perplexity is 34.18809517221489
At time: 259.0272784233093 and batch: 150, loss is 3.64797598361969 and perplexity is 38.39687145005129
At time: 259.4725351333618 and batch: 200, loss is 3.710220007896423 and perplexity is 40.86279567583075
At time: 259.9179289340973 and batch: 250, loss is 3.665459032058716 and perplexity is 39.07406829708511
At time: 260.36243963241577 and batch: 300, loss is 3.510843892097473 and perplexity is 33.47650642630966
At time: 260.8109130859375 and batch: 350, loss is 3.5517634773254394 and perplexity is 34.87476414758636
At time: 261.25620460510254 and batch: 400, loss is 3.558432173728943 and perplexity is 35.10811055523349
At time: 261.7004599571228 and batch: 450, loss is 3.61006103515625 and perplexity is 36.968309112488036
At time: 262.14434027671814 and batch: 500, loss is 3.64825083732605 and perplexity is 38.40742642295223
At time: 262.5896439552307 and batch: 550, loss is 3.531032962799072 and perplexity is 34.15923461547118
At time: 263.03416633605957 and batch: 600, loss is 3.4926428604125976 and perplexity is 32.87271099063763
At time: 263.4789409637451 and batch: 650, loss is 3.4503884983062743 and perplexity is 31.512632535306548
At time: 263.9250626564026 and batch: 700, loss is 3.537635660171509 and perplexity is 34.38552394198076
At time: 264.3696117401123 and batch: 750, loss is 3.5083999252319336 and perplexity is 33.39479084958444
At time: 264.81401801109314 and batch: 800, loss is 3.6200924348831176 and perplexity is 37.34101927528606
At time: 265.2592496871948 and batch: 850, loss is 3.489529609680176 and perplexity is 32.77052914046096
At time: 265.703973531723 and batch: 900, loss is 3.414980764389038 and perplexity is 30.41636434458659
At time: 266.1478545665741 and batch: 950, loss is 3.458948030471802 and perplexity is 31.78352362369086
At time: 266.59439873695374 and batch: 1000, loss is 3.362443280220032 and perplexity is 28.85961694013471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296481993140244 and perplexity of 73.44097285349257
Finished 27 epochs...
Completing Train Step...
At time: 268.0089445114136 and batch: 50, loss is 3.6673264265060426 and perplexity is 39.14710316648864
At time: 268.46835589408875 and batch: 100, loss is 3.5315318870544434 and perplexity is 34.1762817384237
At time: 268.9135582447052 and batch: 150, loss is 3.6476382398605347 and perplexity is 38.383905336083245
At time: 269.3593943119049 and batch: 200, loss is 3.709898200035095 and perplexity is 40.849647822601156
At time: 269.8033332824707 and batch: 250, loss is 3.665131916999817 and perplexity is 39.06128867125068
At time: 270.24644947052 and batch: 300, loss is 3.5105463600158693 and perplexity is 33.46654757328042
At time: 270.6904294490814 and batch: 350, loss is 3.551490378379822 and perplexity is 34.865241186683484
At time: 271.13424801826477 and batch: 400, loss is 3.558156943321228 and perplexity is 35.098449065280164
At time: 271.5777544975281 and batch: 450, loss is 3.6098236656188964 and perplexity is 36.95953500345157
At time: 272.02154088020325 and batch: 500, loss is 3.648045234680176 and perplexity is 38.3995305661908
At time: 272.4665186405182 and batch: 550, loss is 3.530832962989807 and perplexity is 34.15240345820129
At time: 272.9103932380676 and batch: 600, loss is 3.492459354400635 and perplexity is 32.86667920399281
At time: 273.3532121181488 and batch: 650, loss is 3.4502651691436768 and perplexity is 31.50874634836974
At time: 273.79745864868164 and batch: 700, loss is 3.5375603580474855 and perplexity is 34.382934736479825
At time: 274.24245858192444 and batch: 750, loss is 3.508401117324829 and perplexity is 33.39483065930109
At time: 274.686163187027 and batch: 800, loss is 3.6200907850265502 and perplexity is 37.340957668011
At time: 275.13075280189514 and batch: 850, loss is 3.4895794439315795 and perplexity is 32.772162275941476
At time: 275.5770447254181 and batch: 900, loss is 3.4150630235671997 and perplexity is 30.418866472630324
At time: 276.0202178955078 and batch: 950, loss is 3.459056577682495 and perplexity is 31.786973823777622
At time: 276.46399450302124 and batch: 1000, loss is 3.362585883140564 and perplexity is 28.86373269924847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296432867282775 and perplexity of 73.4373650913457
Finished 28 epochs...
Completing Train Step...
At time: 277.8579070568085 and batch: 50, loss is 3.667059397697449 and perplexity is 39.13665115772603
At time: 278.3120527267456 and batch: 100, loss is 3.5312149953842162 and perplexity is 34.165453275236956
At time: 278.756019115448 and batch: 150, loss is 3.647327766418457 and perplexity is 38.371990002666166
At time: 279.2004222869873 and batch: 200, loss is 3.7096041440963745 and perplexity is 40.83763750700314
At time: 279.6449112892151 and batch: 250, loss is 3.6648339080810546 and perplexity is 39.04964979317916
At time: 280.1032283306122 and batch: 300, loss is 3.5102711868286134 and perplexity is 33.457339743650245
At time: 280.54765701293945 and batch: 350, loss is 3.5512367820739748 and perplexity is 34.85640061133207
At time: 280.99160075187683 and batch: 400, loss is 3.557905578613281 and perplexity is 35.08962766262258
At time: 281.4356813430786 and batch: 450, loss is 3.6096051502227784 and perplexity is 36.9514596583457
At time: 281.879953622818 and batch: 500, loss is 3.6478531169891357 and perplexity is 38.39215404564413
At time: 282.32528471946716 and batch: 550, loss is 3.530644907951355 and perplexity is 34.145981530512586
At time: 282.77146220207214 and batch: 600, loss is 3.492290458679199 and perplexity is 32.86112863124465
At time: 283.21448707580566 and batch: 650, loss is 3.4501446056365968 and perplexity is 31.50494777239571
At time: 283.6583340167999 and batch: 700, loss is 3.537482295036316 and perplexity is 34.38025080582067
At time: 284.102783203125 and batch: 750, loss is 3.508385410308838 and perplexity is 33.394306130281315
At time: 284.5455002784729 and batch: 800, loss is 3.6200764179229736 and perplexity is 37.34042119045836
At time: 284.98860478401184 and batch: 850, loss is 3.489607319831848 and perplexity is 32.77307584220185
At time: 285.4330201148987 and batch: 900, loss is 3.4151218557357788 and perplexity is 30.420656133154914
At time: 285.8757059574127 and batch: 950, loss is 3.4591367197036744 and perplexity is 31.789521398189553
At time: 286.3190097808838 and batch: 1000, loss is 3.362695083618164 and perplexity is 28.866884804747126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296400861042302 and perplexity of 73.43501467499307
Finished 29 epochs...
Completing Train Step...
At time: 287.7245659828186 and batch: 50, loss is 3.6668051624298097 and perplexity is 39.12670250544716
At time: 288.1690664291382 and batch: 100, loss is 3.530919270515442 and perplexity is 34.155351194842346
At time: 288.6130418777466 and batch: 150, loss is 3.6470376777648927 and perplexity is 38.36086033812446
At time: 289.0565552711487 and batch: 200, loss is 3.7093302631378173 and perplexity is 40.8264543871892
At time: 289.50132179260254 and batch: 250, loss is 3.664556851387024 and perplexity is 39.03883232489961
At time: 289.94557452201843 and batch: 300, loss is 3.5100125169754026 and perplexity is 33.44868645771028
At time: 290.38971614837646 and batch: 350, loss is 3.55099750995636 and perplexity is 34.848061444250234
At time: 290.8337514400482 and batch: 400, loss is 3.5576712131500243 and perplexity is 35.081404829391616
At time: 291.29320096969604 and batch: 450, loss is 3.609399881362915 and perplexity is 36.943875452778585
At time: 291.73700976371765 and batch: 500, loss is 3.647670946121216 and perplexity is 38.38516075062697
At time: 292.18093752861023 and batch: 550, loss is 3.530465831756592 and perplexity is 34.13986734554248
At time: 292.62605476379395 and batch: 600, loss is 3.492131471633911 and perplexity is 32.855904552790136
At time: 293.0699303150177 and batch: 650, loss is 3.450025787353516 and perplexity is 31.501204630974062
At time: 293.51356649398804 and batch: 700, loss is 3.537401628494263 and perplexity is 34.37747758172795
At time: 293.95843839645386 and batch: 750, loss is 3.508357219696045 and perplexity is 33.39336473759698
At time: 294.4027695655823 and batch: 800, loss is 3.6200517654418944 and perplexity is 37.3395006677781
At time: 294.84721302986145 and batch: 850, loss is 3.4896184158325196 and perplexity is 32.77343949429094
At time: 295.2929229736328 and batch: 900, loss is 3.415162353515625 and perplexity is 30.42188812713611
At time: 295.73817896842957 and batch: 950, loss is 3.459194931983948 and perplexity is 31.791371992582082
At time: 296.1836154460907 and batch: 1000, loss is 3.362778692245483 and perplexity is 28.869298426259018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296380391935023 and perplexity of 73.43351154118365
Finished 30 epochs...
Completing Train Step...
At time: 297.583265542984 and batch: 50, loss is 3.666560659408569 and perplexity is 39.11713707790902
At time: 298.0421371459961 and batch: 100, loss is 3.5306400871276855 and perplexity is 34.14581691915338
At time: 298.48702335357666 and batch: 150, loss is 3.6467635154724123 and perplexity is 38.35034467827722
At time: 298.93260741233826 and batch: 200, loss is 3.70907160282135 and perplexity is 40.81589556920964
At time: 299.3761866092682 and batch: 250, loss is 3.6642958116531372 and perplexity is 39.02864296846957
At time: 299.8220238685608 and batch: 300, loss is 3.5097662258148192 and perplexity is 33.44044935630689
At time: 300.2664771080017 and batch: 350, loss is 3.550769019126892 and perplexity is 34.840099891391034
At time: 300.71046328544617 and batch: 400, loss is 3.5574499797821044 and perplexity is 35.07364451050224
At time: 301.15622115135193 and batch: 450, loss is 3.609204316139221 and perplexity is 36.93665122193873
At time: 301.6004252433777 and batch: 500, loss is 3.647496156692505 and perplexity is 38.37845201663337
At time: 302.044278383255 and batch: 550, loss is 3.5302932024002076 and perplexity is 34.13397431088559
At time: 302.4902856349945 and batch: 600, loss is 3.4919796657562254 and perplexity is 32.850917211925754
At time: 302.9496703147888 and batch: 650, loss is 3.449908413887024 and perplexity is 31.49750744236802
At time: 303.3929080963135 and batch: 700, loss is 3.537318239212036 and perplexity is 34.37461098807089
At time: 303.83841133117676 and batch: 750, loss is 3.508319659233093 and perplexity is 33.392110490913105
At time: 304.2829427719116 and batch: 800, loss is 3.6200187397003174 and perplexity is 37.338267523441296
At time: 304.72749948501587 and batch: 850, loss is 3.489616150856018 and perplexity is 32.77336526330468
At time: 305.1725277900696 and batch: 900, loss is 3.4151881074905397 and perplexity is 30.422671621768806
At time: 305.61775612831116 and batch: 950, loss is 3.459236102104187 and perplexity is 31.792680874132806
At time: 306.0613408088684 and batch: 1000, loss is 3.3628423690795897 and perplexity is 28.871136790315656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2963673661394814 and perplexity of 73.43255501750616
Finished 31 epochs...
Completing Train Step...
At time: 307.4581573009491 and batch: 50, loss is 3.6663235235214233 and perplexity is 39.107862100663894
At time: 307.9176516532898 and batch: 100, loss is 3.530373463630676 and perplexity is 34.13671405561073
At time: 308.36239361763 and batch: 150, loss is 3.646501908302307 and perplexity is 38.340313265335354
At time: 308.80911135673523 and batch: 200, loss is 3.7088246965408325 and perplexity is 40.80581911227009
At time: 309.25474786758423 and batch: 250, loss is 3.6640468978881837 and perplexity is 39.0189294109765
At time: 309.70049691200256 and batch: 300, loss is 3.509529333114624 and perplexity is 33.43252849619715
At time: 310.14409923553467 and batch: 350, loss is 3.5505491733551025 and perplexity is 34.83244128462819
At time: 310.58859062194824 and batch: 400, loss is 3.5572387027740477 and perplexity is 35.06623503858151
At time: 311.0360333919525 and batch: 450, loss is 3.609016122817993 and perplexity is 36.929700644916835
At time: 311.4819481372833 and batch: 500, loss is 3.647327337265015 and perplexity is 38.37197353519811
At time: 311.925993680954 and batch: 550, loss is 3.5301257753372193 and perplexity is 34.12825983821064
At time: 312.3705897331238 and batch: 600, loss is 3.4918333435058595 and perplexity is 32.84611074344794
At time: 312.81604766845703 and batch: 650, loss is 3.449792156219482 and perplexity is 31.49384582846886
At time: 313.26088976860046 and batch: 700, loss is 3.5372327852249144 and perplexity is 34.371673666010935
At time: 313.70531010627747 and batch: 750, loss is 3.5082750272750856 and perplexity is 33.39062016889813
At time: 314.17645478248596 and batch: 800, loss is 3.619979000091553 and perplexity is 37.336783744780554
At time: 314.6207573413849 and batch: 850, loss is 3.489603319168091 and perplexity is 32.77294472840739
At time: 315.06541657447815 and batch: 900, loss is 3.415202035903931 and perplexity is 30.42309536426664
At time: 315.51137256622314 and batch: 950, loss is 3.4592636251449584 and perplexity is 31.793555917426605
At time: 315.95940494537354 and batch: 1000, loss is 3.3628902435302734 and perplexity is 28.872519013216426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296360294993331 and perplexity of 73.43203576701329
Finished 32 epochs...
Completing Train Step...
At time: 317.37555480003357 and batch: 50, loss is 3.6660925340652466 and perplexity is 39.098829640106764
At time: 317.820839881897 and batch: 100, loss is 3.530116934776306 and perplexity is 34.12795812658433
At time: 318.2645194530487 and batch: 150, loss is 3.6462504863739014 and perplexity is 38.3306748815398
At time: 318.7081594467163 and batch: 200, loss is 3.7085871744155883 and perplexity is 40.796127978367146
At time: 319.15312576293945 and batch: 250, loss is 3.6638077688217163 and perplexity is 39.009599966327045
At time: 319.5963842868805 and batch: 300, loss is 3.5093000030517576 and perplexity is 33.42486229141454
At time: 320.0394058227539 and batch: 350, loss is 3.5503359699249266 and perplexity is 34.8250156802752
At time: 320.48421454429626 and batch: 400, loss is 3.557035303115845 and perplexity is 35.059103303681006
At time: 320.9285943508148 and batch: 450, loss is 3.608833599090576 and perplexity is 36.922960713420146
At time: 321.37463188171387 and batch: 500, loss is 3.6471630430221555 and perplexity is 38.3656697587104
At time: 321.8198082447052 and batch: 550, loss is 3.529962568283081 and perplexity is 34.12269031996427
At time: 322.26582860946655 and batch: 600, loss is 3.4916912078857423 and perplexity is 32.8414424729006
At time: 322.70958971977234 and batch: 650, loss is 3.449676742553711 and perplexity is 31.490211218018462
At time: 323.1539967060089 and batch: 700, loss is 3.5371454524993897 and perplexity is 34.368672025141485
At time: 323.5990002155304 and batch: 750, loss is 3.508224892616272 and perplexity is 33.38894618351108
At time: 324.0429594516754 and batch: 800, loss is 3.6199338674545287 and perplexity is 37.335098675298255
At time: 324.4860098361969 and batch: 850, loss is 3.489582166671753 and perplexity is 32.77225150614575
At time: 324.93052768707275 and batch: 900, loss is 3.4152064228057863 and perplexity is 30.423228827692892
At time: 325.37359046936035 and batch: 950, loss is 3.459280333518982 and perplexity is 31.794087140488333
At time: 325.8324227333069 and batch: 1000, loss is 3.3629255723953246 and perplexity is 28.873539064562856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296358806330983 and perplexity of 73.43192645158787
Finished 33 epochs...
Completing Train Step...
At time: 327.22841930389404 and batch: 50, loss is 3.6658661937713624 and perplexity is 39.089981000955056
At time: 327.6884994506836 and batch: 100, loss is 3.5298688125610354 and perplexity is 34.119491272462035
At time: 328.13300466537476 and batch: 150, loss is 3.6460074090957644 and perplexity is 38.32135869774262
At time: 328.57722997665405 and batch: 200, loss is 3.7083573007583617 and perplexity is 40.786751101017906
At time: 329.02112197875977 and batch: 250, loss is 3.66357656955719 and perplexity is 39.000582018017035
At time: 329.46674633026123 and batch: 300, loss is 3.5090768051147463 and perplexity is 33.41740276361279
At time: 329.9108965396881 and batch: 350, loss is 3.5501282930374147 and perplexity is 34.81778408035524
At time: 330.3558099269867 and batch: 400, loss is 3.556838245391846 and perplexity is 35.05219531723694
At time: 330.8014817237854 and batch: 450, loss is 3.608655643463135 and perplexity is 36.916390649386685
At time: 331.2454752922058 and batch: 500, loss is 3.647002501487732 and perplexity is 38.3595109696021
At time: 331.69005942344666 and batch: 550, loss is 3.529802770614624 and perplexity is 34.117238029252945
At time: 332.137193441391 and batch: 600, loss is 3.491552176475525 and perplexity is 32.83687679823347
At time: 332.5817904472351 and batch: 650, loss is 3.449561901092529 and perplexity is 31.486595043796594
At time: 333.0264136791229 and batch: 700, loss is 3.5370565366744997 and perplexity is 34.36561624217374
At time: 333.4726724624634 and batch: 750, loss is 3.508170232772827 and perplexity is 33.38712119881705
At time: 333.91626811027527 and batch: 800, loss is 3.619884214401245 and perplexity is 37.33324491967707
At time: 334.36101150512695 and batch: 850, loss is 3.4895540285110473 and perplexity is 32.77132936823988
At time: 334.8064923286438 and batch: 900, loss is 3.415202808380127 and perplexity is 30.423118865392706
At time: 335.2539451122284 and batch: 950, loss is 3.4592881393432617 and perplexity is 31.794335320514314
At time: 335.6984853744507 and batch: 1000, loss is 3.3629506158828737 and perplexity is 28.87426216773339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296358061999809 and perplexity of 73.43187179393621
Finished 34 epochs...
Completing Train Step...
At time: 337.0932469367981 and batch: 50, loss is 3.665643744468689 and perplexity is 39.08128642902647
At time: 337.5513982772827 and batch: 100, loss is 3.5296274995803834 and perplexity is 34.111258789667176
At time: 337.99571466445923 and batch: 150, loss is 3.645771279335022 and perplexity is 38.312310952744966
At time: 338.4394896030426 and batch: 200, loss is 3.708133783340454 and perplexity is 40.777635570504835
At time: 338.8826234340668 and batch: 250, loss is 3.6633517217636107 and perplexity is 38.99181380899515
At time: 339.3270335197449 and batch: 300, loss is 3.508858489990234 and perplexity is 33.410108035471296
At time: 339.77050852775574 and batch: 350, loss is 3.549925274848938 and perplexity is 34.81071615438751
At time: 340.21395325660706 and batch: 400, loss is 3.5566463232040406 and perplexity is 35.04546866874147
At time: 340.6582899093628 and batch: 450, loss is 3.6084814262390137 and perplexity is 36.90995973848716
At time: 341.10125827789307 and batch: 500, loss is 3.6468451738357546 and perplexity is 38.35347643252248
At time: 341.5442259311676 and batch: 550, loss is 3.529645867347717 and perplexity is 34.111885343086264
At time: 341.9882843494415 and batch: 600, loss is 3.4914158391952514 and perplexity is 32.83240021292779
At time: 342.43182921409607 and batch: 650, loss is 3.4494476318359375 and perplexity is 31.482997299548032
At time: 342.87614011764526 and batch: 700, loss is 3.5369664478302 and perplexity is 34.36252042297432
At time: 343.3201241493225 and batch: 750, loss is 3.508112254142761 and perplexity is 33.38518551538277
At time: 343.7640891075134 and batch: 800, loss is 3.6198307991027834 and perplexity is 37.3312508065157
At time: 344.2079005241394 and batch: 850, loss is 3.4895202255249025 and perplexity is 32.770221618170034
At time: 344.652001619339 and batch: 900, loss is 3.415192642211914 and perplexity is 30.42280958042088
At time: 345.09576654434204 and batch: 950, loss is 3.4592886400222778 and perplexity is 31.79435123927482
At time: 345.5393135547638 and batch: 1000, loss is 3.362967166900635 and perplexity is 28.874740070114246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296359550662157 and perplexity of 73.4319811092802
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 346.94998598098755 and batch: 50, loss is 3.6656347370147704 and perplexity is 39.08093440772529
At time: 347.3937211036682 and batch: 100, loss is 3.5297168064117432 and perplexity is 34.114305294138184
At time: 347.84024310112 and batch: 150, loss is 3.645910215377808 and perplexity is 38.3176342834114
At time: 348.28519201278687 and batch: 200, loss is 3.708119568824768 and perplexity is 40.77705594028398
At time: 348.7446036338806 and batch: 250, loss is 3.663407621383667 and perplexity is 38.99399349749369
At time: 349.1890802383423 and batch: 300, loss is 3.5086123752593994 and perplexity is 33.4018863275082
At time: 349.6328053474426 and batch: 350, loss is 3.5497557020187376 and perplexity is 34.80481370318973
At time: 350.07606744766235 and batch: 400, loss is 3.556381707191467 and perplexity is 35.03619630342586
At time: 350.52154445648193 and batch: 450, loss is 3.6082476711273195 and perplexity is 36.90133285505419
At time: 350.9649872779846 and batch: 500, loss is 3.6465580701828 and perplexity is 38.342466589893846
At time: 351.40875148773193 and batch: 550, loss is 3.5292811965942383 and perplexity is 34.09944800406034
At time: 351.8547124862671 and batch: 600, loss is 3.4912586975097657 and perplexity is 32.82724127957214
At time: 352.2990052700043 and batch: 650, loss is 3.4489982748031616 and perplexity is 31.46885337137341
At time: 352.7424781322479 and batch: 700, loss is 3.536315951347351 and perplexity is 34.34017499289714
At time: 353.1874783039093 and batch: 750, loss is 3.507172393798828 and perplexity is 33.353822844016214
At time: 353.63174271583557 and batch: 800, loss is 3.619044680595398 and perplexity is 37.30191555135707
At time: 354.0754199028015 and batch: 850, loss is 3.4886230850219726 and perplexity is 32.740835308861534
At time: 354.5199418067932 and batch: 900, loss is 3.4141874361038207 and perplexity is 30.392243751458007
At time: 354.9649579524994 and batch: 950, loss is 3.4582399702072144 and perplexity is 31.7610269389885
At time: 355.40820264816284 and batch: 1000, loss is 3.3618981409072877 and perplexity is 28.843888715816107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296334615567836 and perplexity of 73.43015009873338
Finished 36 epochs...
Completing Train Step...
At time: 356.8025748729706 and batch: 50, loss is 3.665569558143616 and perplexity is 39.078387239548604
At time: 357.2609851360321 and batch: 100, loss is 3.5296288394927977 and perplexity is 34.111304495796915
At time: 357.70553255081177 and batch: 150, loss is 3.645835018157959 and perplexity is 38.31475301217526
At time: 358.149347782135 and batch: 200, loss is 3.708059368133545 and perplexity is 40.77460120721938
At time: 358.5935308933258 and batch: 250, loss is 3.663336091041565 and perplexity is 38.99120434355465
At time: 359.03886795043945 and batch: 300, loss is 3.508563370704651 and perplexity is 33.40024952304673
At time: 359.4825282096863 and batch: 350, loss is 3.54970468044281 and perplexity is 34.80303795204592
At time: 359.92630219459534 and batch: 400, loss is 3.5563272285461425 and perplexity is 35.03428763090533
At time: 360.3862955570221 and batch: 450, loss is 3.6081975173950194 and perplexity is 36.89948216189464
At time: 360.82982087135315 and batch: 500, loss is 3.6465199518203737 and perplexity is 38.34100506571168
At time: 361.27469182014465 and batch: 550, loss is 3.5292488813400267 and perplexity is 34.09834608953404
At time: 361.721626996994 and batch: 600, loss is 3.491227321624756 and perplexity is 32.82621131198273
At time: 362.16641116142273 and batch: 650, loss is 3.4489844369888307 and perplexity is 31.468417914236145
At time: 362.6099741458893 and batch: 700, loss is 3.5362973642349242 and perplexity is 34.33953671413568
At time: 363.0549681186676 and batch: 750, loss is 3.5071931409835817 and perplexity is 33.35451484911955
At time: 363.4987726211548 and batch: 800, loss is 3.619048957824707 and perplexity is 37.302075100544755
At time: 363.9431071281433 and batch: 850, loss is 3.4886320304870604 and perplexity is 32.74112819217072
At time: 364.3877727985382 and batch: 900, loss is 3.414201340675354 and perplexity is 30.392666345523292
At time: 364.8330328464508 and batch: 950, loss is 3.4582567501068113 and perplexity is 31.761559890303054
At time: 365.2769138813019 and batch: 1000, loss is 3.361924366950989 and perplexity is 28.844645186821655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296314146460556 and perplexity of 73.4286470644964
Finished 37 epochs...
Completing Train Step...
At time: 366.6748876571655 and batch: 50, loss is 3.6655097436904907 and perplexity is 39.07604985709218
At time: 367.13418459892273 and batch: 100, loss is 3.529554090499878 and perplexity is 34.10875480543326
At time: 367.58016777038574 and batch: 150, loss is 3.645767412185669 and perplexity is 38.31216279360292
At time: 368.02423071861267 and batch: 200, loss is 3.708000807762146 and perplexity is 40.772213501342186
At time: 368.46823263168335 and batch: 250, loss is 3.6632721614837647 and perplexity is 38.988711732779464
At time: 368.91448497772217 and batch: 300, loss is 3.5085140323638915 and perplexity is 33.398601650806285
At time: 369.35874700546265 and batch: 350, loss is 3.5496553325653077 and perplexity is 34.80132053836807
At time: 369.8028907775879 and batch: 400, loss is 3.556276116371155 and perplexity is 35.03249699802734
At time: 370.2488498687744 and batch: 450, loss is 3.6081526470184326 and perplexity is 36.89782650537942
At time: 370.69389033317566 and batch: 500, loss is 3.6464845037460325 and perplexity is 38.33964597500251
At time: 371.1383628845215 and batch: 550, loss is 3.5292165327072142 and perplexity is 34.09724307249752
At time: 371.5973153114319 and batch: 600, loss is 3.491195225715637 and perplexity is 32.82515774179548
At time: 372.04263067245483 and batch: 650, loss is 3.448967442512512 and perplexity is 31.467883129497324
At time: 372.48662304878235 and batch: 700, loss is 3.5362805223464964 and perplexity is 34.33895837635984
At time: 372.93113684654236 and batch: 750, loss is 3.507205677032471 and perplexity is 33.35493298556926
At time: 373.37657380104065 and batch: 800, loss is 3.6190517473220827 and perplexity is 37.30217915473049
At time: 373.8204519748688 and batch: 850, loss is 3.488640532493591 and perplexity is 32.741406558639774
At time: 374.26550030708313 and batch: 900, loss is 3.4142138528823853 and perplexity is 30.393046627235922
At time: 374.7111487388611 and batch: 950, loss is 3.4582729625701902 and perplexity is 31.762074827603822
At time: 375.15657567977905 and batch: 1000, loss is 3.361948084831238 and perplexity is 28.845329328775193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296292560856517 and perplexity of 73.42706207990221
Finished 38 epochs...
Completing Train Step...
At time: 376.57373547554016 and batch: 50, loss is 3.665453243255615 and perplexity is 39.073842105652076
At time: 377.01701641082764 and batch: 100, loss is 3.5294867753982544 and perplexity is 34.106458848414434
At time: 377.4623701572418 and batch: 150, loss is 3.645704312324524 and perplexity is 38.30974537772059
At time: 377.90648341178894 and batch: 200, loss is 3.707943596839905 and perplexity is 40.76988095213025
At time: 378.3505756855011 and batch: 250, loss is 3.663213014602661 and perplexity is 38.986405740279025
At time: 378.7962441444397 and batch: 300, loss is 3.5084647130966187 and perplexity is 33.39695449686348
At time: 379.24090099334717 and batch: 350, loss is 3.5496072149276734 and perplexity is 34.79964602132443
At time: 379.68570041656494 and batch: 400, loss is 3.5562275409698487 and perplexity is 35.03079532175704
At time: 380.13127088546753 and batch: 450, loss is 3.608110604286194 and perplexity is 36.896275252549145
At time: 380.5762550830841 and batch: 500, loss is 3.6464505577087403 and perplexity is 38.33834451804025
At time: 381.0204231739044 and batch: 550, loss is 3.5291839694976805 and perplexity is 34.096132774904355
At time: 381.4645736217499 and batch: 600, loss is 3.4911628770828247 and perplexity is 32.82409590999517
At time: 381.9095220565796 and batch: 650, loss is 3.4489490604400634 and perplexity is 31.467304689906307
At time: 382.3537039756775 and batch: 700, loss is 3.5362646102905275 and perplexity is 34.33841197727942
At time: 382.8126666545868 and batch: 750, loss is 3.5072130870819094 and perplexity is 33.35518014818744
At time: 383.25825548171997 and batch: 800, loss is 3.6190530395507814 and perplexity is 37.30222735770806
At time: 383.7027471065521 and batch: 850, loss is 3.4886478996276855 and perplexity is 32.74164776986084
At time: 384.14676785469055 and batch: 900, loss is 3.414224910736084 and perplexity is 30.393382710957155
At time: 384.59214901924133 and batch: 950, loss is 3.4582880401611327 and perplexity is 31.762553726785868
At time: 385.0367271900177 and batch: 1000, loss is 3.361969394683838 and perplexity is 28.845944025040904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.2962773020674545 and perplexity of 73.42594168039845
Finished 39 epochs...
Completing Train Step...
At time: 386.44792199134827 and batch: 50, loss is 3.665398750305176 and perplexity is 39.07171291472422
At time: 386.9064841270447 and batch: 100, loss is 3.529424204826355 and perplexity is 34.104324854542135
At time: 387.35170888900757 and batch: 150, loss is 3.6456440591812136 and perplexity is 38.30743716468141
At time: 387.79652190208435 and batch: 200, loss is 3.70788761138916 and perplexity is 40.76759849586111
At time: 388.2417161464691 and batch: 250, loss is 3.663156576156616 and perplexity is 38.98420547021266
At time: 388.6863169670105 and batch: 300, loss is 3.5084156084060667 and perplexity is 33.395314590011424
At time: 389.1298987865448 and batch: 350, loss is 3.5495603036880494 and perplexity is 34.79801356508167
At time: 389.5735716819763 and batch: 400, loss is 3.55618070602417 and perplexity is 35.02915469478071
At time: 390.01753067970276 and batch: 450, loss is 3.608070521354675 and perplexity is 36.894796371314044
At time: 390.4626908302307 and batch: 500, loss is 3.6464175605773925 and perplexity is 38.337079483521904
At time: 390.9062592983246 and batch: 550, loss is 3.5291516828536986 and perplexity is 34.09503194297547
At time: 391.350478887558 and batch: 600, loss is 3.491130681037903 and perplexity is 32.82303912094105
At time: 391.7966413497925 and batch: 650, loss is 3.448929672241211 and perplexity is 31.466694601459903
At time: 392.24013924598694 and batch: 700, loss is 3.536249208450317 and perplexity is 34.33788310661787
At time: 392.68482089042664 and batch: 750, loss is 3.5072172403335573 and perplexity is 33.355318680932044
At time: 393.1301612854004 and batch: 800, loss is 3.6190531969070436 and perplexity is 37.302233227447594
At time: 393.5737679004669 and batch: 850, loss is 3.4886544513702393 and perplexity is 32.74186228541054
At time: 394.0167405605316 and batch: 900, loss is 3.4142350721359254 and perplexity is 30.393691551840543
At time: 394.4762830734253 and batch: 950, loss is 3.4583019065856933 and perplexity is 31.762994162894604
At time: 394.9206771850586 and batch: 1000, loss is 3.3619890356063844 and perplexity is 28.84651059155721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296261671112805 and perplexity of 73.42479397180384
Finished 40 epochs...
Completing Train Step...
At time: 396.314875125885 and batch: 50, loss is 3.6653458786010744 and perplexity is 39.06964718129016
At time: 396.7734191417694 and batch: 100, loss is 3.5293646240234375 and perplexity is 34.102292952016136
At time: 397.21763348579407 and batch: 150, loss is 3.6455858898162843 and perplexity is 38.30520891019817
At time: 397.66344571113586 and batch: 200, loss is 3.707832899093628 and perplexity is 40.76536806798053
At time: 398.1073384284973 and batch: 250, loss is 3.6631022453308106 and perplexity is 38.98208748367259
At time: 398.5520715713501 and batch: 300, loss is 3.5083669996261597 and perplexity is 33.3936913239674
At time: 398.99817514419556 and batch: 350, loss is 3.549514346122742 and perplexity is 34.79641436984851
At time: 399.442533493042 and batch: 400, loss is 3.5561351585388183 and perplexity is 35.02755924120508
At time: 399.8867597579956 and batch: 450, loss is 3.608031554222107 and perplexity is 36.89335871490362
At time: 400.33263659477234 and batch: 500, loss is 3.646385178565979 and perplexity is 38.33583807187631
At time: 400.7776424884796 and batch: 550, loss is 3.5291195583343504 and perplexity is 34.093936674054724
At time: 401.2231001853943 and batch: 600, loss is 3.4910988569259644 and perplexity is 32.82199457349088
At time: 401.6698853969574 and batch: 650, loss is 3.4489097023010253 and perplexity is 31.466066219725267
At time: 402.1142330169678 and batch: 700, loss is 3.5362340021133423 and perplexity is 34.33736095716635
At time: 402.55890130996704 and batch: 750, loss is 3.507219214439392 and perplexity is 33.35538452792627
At time: 403.00466561317444 and batch: 800, loss is 3.6190525007247927 and perplexity is 37.302207258303945
At time: 403.45089077949524 and batch: 850, loss is 3.488659906387329 and perplexity is 32.74204089331601
At time: 403.8958058357239 and batch: 900, loss is 3.414243931770325 and perplexity is 30.393960830028593
At time: 404.33976435661316 and batch: 950, loss is 3.4583144903182985 and perplexity is 31.763393862434743
At time: 404.78519654273987 and batch: 1000, loss is 3.3620071172714234 and perplexity is 28.847032189214932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29624790098609 and perplexity of 73.4237829100481
Finished 41 epochs...
Completing Train Step...
At time: 406.19279766082764 and batch: 50, loss is 3.6652942895889282 and perplexity is 39.06763166877678
At time: 406.63592004776 and batch: 100, loss is 3.5293071174621584 and perplexity is 34.100331902803866
At time: 407.07850551605225 and batch: 150, loss is 3.645529561042786 and perplexity is 38.3030512855304
At time: 407.52265644073486 and batch: 200, loss is 3.70777925491333 and perplexity is 40.763181301880145
At time: 407.96670866012573 and batch: 250, loss is 3.663049535751343 and perplexity is 38.980032808385566
At time: 408.40958762168884 and batch: 300, loss is 3.508318791389465 and perplexity is 33.39208151179537
At time: 408.8543133735657 and batch: 350, loss is 3.5494692802429197 and perplexity is 34.79484627415436
At time: 409.2976233959198 and batch: 400, loss is 3.556090831756592 and perplexity is 35.026006616626354
At time: 409.74151611328125 and batch: 450, loss is 3.60799334526062 and perplexity is 36.891949084911786
At time: 410.1912603378296 and batch: 500, loss is 3.646353254318237 and perplexity is 38.33461424861924
At time: 410.63591957092285 and batch: 550, loss is 3.5290876770019532 and perplexity is 34.09284973125358
At time: 411.07905411720276 and batch: 600, loss is 3.4910675954818724 and perplexity is 32.82096852658047
At time: 411.5230519771576 and batch: 650, loss is 3.4488897371292113 and perplexity is 31.465438000578153
At time: 411.9867479801178 and batch: 700, loss is 3.536218910217285 and perplexity is 34.33684274519431
At time: 412.43036460876465 and batch: 750, loss is 3.507219519615173 and perplexity is 33.35539470718335
At time: 412.87413573265076 and batch: 800, loss is 3.619050831794739 and perplexity is 37.302145003581124
At time: 413.3185935020447 and batch: 850, loss is 3.4886645603179933 and perplexity is 32.74219327285872
At time: 413.7620177268982 and batch: 900, loss is 3.41425199508667 and perplexity is 30.394205907137817
At time: 414.2066206932068 and batch: 950, loss is 3.458325881958008 and perplexity is 31.763755701634537
At time: 414.65376901626587 and batch: 1000, loss is 3.362023825645447 and perplexity is 28.847514180244847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296235619521722 and perplexity of 73.42288116401191
Finished 42 epochs...
Completing Train Step...
At time: 416.0714383125305 and batch: 50, loss is 3.6652436876296997 and perplexity is 39.06565482008855
At time: 416.53092336654663 and batch: 100, loss is 3.5292511796951294 and perplexity is 34.098424459731824
At time: 416.97473669052124 and batch: 150, loss is 3.645474672317505 and perplexity is 38.300948937569096
At time: 417.4477949142456 and batch: 200, loss is 3.707726922035217 and perplexity is 40.761048103100286
At time: 417.89328718185425 and batch: 250, loss is 3.662997913360596 and perplexity is 38.97802061783809
At time: 418.33696269989014 and batch: 300, loss is 3.5082712411880492 and perplexity is 33.39049374934329
At time: 418.78073716163635 and batch: 350, loss is 3.549424958229065 and perplexity is 34.79330413067143
At time: 419.2266938686371 and batch: 400, loss is 3.556047410964966 and perplexity is 35.0244857927095
At time: 419.6756248474121 and batch: 450, loss is 3.6079559755325317 and perplexity is 36.89057046856525
At time: 420.1260964870453 and batch: 500, loss is 3.6463218545913696 and perplexity is 38.33341057109993
At time: 420.5738182067871 and batch: 550, loss is 3.529056077003479 and perplexity is 34.09177241427578
At time: 421.0229194164276 and batch: 600, loss is 3.491036891937256 and perplexity is 32.819960821979095
At time: 421.4670526981354 and batch: 650, loss is 3.4488695287704467 and perplexity is 31.464802142143203
At time: 421.91248965263367 and batch: 700, loss is 3.536203999519348 and perplexity is 34.33633076272105
At time: 422.35654520988464 and batch: 750, loss is 3.5072186183929444 and perplexity is 33.35536464657373
At time: 422.80035972595215 and batch: 800, loss is 3.619048447608948 and perplexity is 37.30205606844306
At time: 423.24733424186707 and batch: 850, loss is 3.48866810798645 and perplexity is 32.742309431511046
At time: 423.69659185409546 and batch: 900, loss is 3.414259214401245 and perplexity is 30.394425333263573
At time: 424.15301418304443 and batch: 950, loss is 3.458336143493652 and perplexity is 31.76408164821822
At time: 424.61013984680176 and batch: 1000, loss is 3.362039341926575 and perplexity is 28.84796178985732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296225198885289 and perplexity of 73.42211605484792
Finished 43 epochs...
Completing Train Step...
At time: 426.02148056030273 and batch: 50, loss is 3.6651939868927004 and perplexity is 39.0637132765011
At time: 426.48307967185974 and batch: 100, loss is 3.5291965293884275 and perplexity is 34.09656102129626
At time: 426.9270169734955 and batch: 150, loss is 3.6454207849502565 and perplexity is 38.29888505587681
At time: 427.37200117111206 and batch: 200, loss is 3.7076753616333007 and perplexity is 40.75894650125774
At time: 427.81670475006104 and batch: 250, loss is 3.6629473781585693 and perplexity is 38.976050905461896
At time: 428.2629520893097 and batch: 300, loss is 3.508224115371704 and perplexity is 33.388920232144116
At time: 428.7065007686615 and batch: 350, loss is 3.549381275177002 and perplexity is 34.791784286151604
At time: 429.1670010089874 and batch: 400, loss is 3.5560048723220827 and perplexity is 35.02299593030478
At time: 429.6104769706726 and batch: 450, loss is 3.6079190492630007 and perplexity is 36.88920826256772
At time: 430.05415749549866 and batch: 500, loss is 3.6462905979156495 and perplexity is 38.332212414841756
At time: 430.5011830329895 and batch: 550, loss is 3.5290247869491576 and perplexity is 34.09070569755393
At time: 430.94549775123596 and batch: 600, loss is 3.4910066509246827 and perplexity is 32.818968328138304
At time: 431.3891658782959 and batch: 650, loss is 3.4488491725921633 and perplexity is 31.464161645540198
At time: 431.8370018005371 and batch: 700, loss is 3.5361889600753784 and perplexity is 34.33581436728158
At time: 432.28190302848816 and batch: 750, loss is 3.507216763496399 and perplexity is 33.35530277588047
At time: 432.7265045642853 and batch: 800, loss is 3.6190454149246216 and perplexity is 37.30194294325381
At time: 433.1703112125397 and batch: 850, loss is 3.488670916557312 and perplexity is 32.74240139073641
At time: 433.6162803173065 and batch: 900, loss is 3.4142656040191652 and perplexity is 30.394619542648815
At time: 434.0596537590027 and batch: 950, loss is 3.458345580101013 and perplexity is 31.764381394799205
At time: 434.50556564331055 and batch: 1000, loss is 3.3620537757873534 and perplexity is 28.848378180326595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296215522580031 and perplexity of 73.42140560347754
Finished 44 epochs...
Completing Train Step...
At time: 435.93590331077576 and batch: 50, loss is 3.6651451063156126 and perplexity is 39.06180386631987
At time: 436.3831949234009 and batch: 100, loss is 3.5291430044174192 and perplexity is 34.09473605269724
At time: 436.8277792930603 and batch: 150, loss is 3.645367994308472 and perplexity is 38.2968632865208
At time: 437.2717571258545 and batch: 200, loss is 3.7076248359680175 and perplexity is 40.75688718039424
At time: 437.7175018787384 and batch: 250, loss is 3.662897758483887 and perplexity is 38.97411697447647
At time: 438.1614181995392 and batch: 300, loss is 3.5081776428222655 and perplexity is 33.38736859995237
At time: 438.60600543022156 and batch: 350, loss is 3.5493383312225344 and perplexity is 34.79029022143213
At time: 439.05319380760193 and batch: 400, loss is 3.555963010787964 and perplexity is 35.021529844652214
At time: 439.49797892570496 and batch: 450, loss is 3.6078826141357423 and perplexity is 36.88786422405547
At time: 439.94154691696167 and batch: 500, loss is 3.646259651184082 and perplexity is 38.331026176508956
At time: 440.4017629623413 and batch: 550, loss is 3.528993730545044 and perplexity is 34.08964697926134
At time: 440.8472592830658 and batch: 600, loss is 3.490976948738098 and perplexity is 32.81799354749415
At time: 441.2950222492218 and batch: 650, loss is 3.448828892707825 and perplexity is 31.463523562451364
At time: 441.74176263809204 and batch: 700, loss is 3.536173982620239 and perplexity is 34.33530010801338
At time: 442.18621850013733 and batch: 750, loss is 3.5072140645980836 and perplexity is 33.35521275343147
At time: 442.63008284568787 and batch: 800, loss is 3.619041781425476 and perplexity is 37.30180740692224
At time: 443.07458090782166 and batch: 850, loss is 3.488673062324524 and perplexity is 32.74247164838313
At time: 443.5209038257599 and batch: 900, loss is 3.4142713165283203 and perplexity is 30.394793172687148
At time: 443.96601009368896 and batch: 950, loss is 3.4583540058135984 and perplexity is 31.76464903347481
At time: 444.41111755371094 and batch: 1000, loss is 3.362067174911499 and perplexity is 28.84876472591692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296206590605945 and perplexity of 73.42074980831413
Finished 45 epochs...
Completing Train Step...
At time: 445.8231945037842 and batch: 50, loss is 3.6650969266891478 and perplexity is 39.05992192853644
At time: 446.28293919563293 and batch: 100, loss is 3.5290902853012085 and perplexity is 34.09293865572411
At time: 446.7284336090088 and batch: 150, loss is 3.645316095352173 and perplexity is 38.29487577086215
At time: 447.17236852645874 and batch: 200, loss is 3.7075752592086793 and perplexity is 40.75486663609355
At time: 447.6192247867584 and batch: 250, loss is 3.6628489971160887 and perplexity is 38.97221658955713
At time: 448.0633223056793 and batch: 300, loss is 3.5081315565109255 and perplexity is 33.385829934744216
At time: 448.5077531337738 and batch: 350, loss is 3.549295816421509 and perplexity is 34.78881115060717
At time: 448.95362615585327 and batch: 400, loss is 3.555921778678894 and perplexity is 35.02008586288327
At time: 449.39870262145996 and batch: 450, loss is 3.6078465366363526 and perplexity is 36.886533426162515
At time: 449.84169483184814 and batch: 500, loss is 3.6462289905548095 and perplexity is 38.329850941142546
At time: 450.28676104545593 and batch: 550, loss is 3.52896288394928 and perplexity is 34.08859544591943
At time: 450.7316243648529 and batch: 600, loss is 3.4909477996826173 and perplexity is 32.81703694792151
At time: 451.1766126155853 and batch: 650, loss is 3.4488085508346558 and perplexity is 31.462883541955236
At time: 451.6219434738159 and batch: 700, loss is 3.536158924102783 and perplexity is 34.334783073190245
At time: 452.0817503929138 and batch: 750, loss is 3.5072107315063477 and perplexity is 33.35510157763277
At time: 452.52641224861145 and batch: 800, loss is 3.619037628173828 and perplexity is 37.30165248345087
At time: 452.9712097644806 and batch: 850, loss is 3.4886743593215943 and perplexity is 32.742514115300466
At time: 453.41658639907837 and batch: 900, loss is 3.414276261329651 and perplexity is 30.394943469272462
At time: 453.86155915260315 and batch: 950, loss is 3.458361668586731 and perplexity is 31.76489243970657
At time: 454.30520486831665 and batch: 1000, loss is 3.3620798063278197 and perplexity is 28.84912912897596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296199891625381 and perplexity of 73.42025796578558
Finished 46 epochs...
Completing Train Step...
At time: 455.69853162765503 and batch: 50, loss is 3.665049409866333 and perplexity is 39.058065969242
At time: 456.1582202911377 and batch: 100, loss is 3.529038276672363 and perplexity is 34.09116557483947
At time: 456.6020259857178 and batch: 150, loss is 3.6452650451660156 and perplexity is 38.29292086022487
At time: 457.0452244281769 and batch: 200, loss is 3.7075262832641602 and perplexity is 40.752870676883695
At time: 457.48973321914673 and batch: 250, loss is 3.662800793647766 and perplexity is 38.970338038825986
At time: 457.9339368343353 and batch: 300, loss is 3.508086042404175 and perplexity is 33.38431044309603
At time: 458.3776137828827 and batch: 350, loss is 3.5492538976669312 and perplexity is 34.787352877535206
At time: 458.82246565818787 and batch: 400, loss is 3.55588107585907 and perplexity is 35.01866047564701
At time: 459.2678108215332 and batch: 450, loss is 3.6078109073638918 and perplexity is 36.885219209225376
At time: 459.7120637893677 and batch: 500, loss is 3.646198396682739 and perplexity is 38.32867830052428
At time: 460.15676259994507 and batch: 550, loss is 3.5289321899414063 and perplexity is 34.08754914636006
At time: 460.6011862754822 and batch: 600, loss is 3.4909190225601194 and perplexity is 32.81609258161738
At time: 461.04395174980164 and batch: 650, loss is 3.448788199424744 and perplexity is 31.462243234430865
At time: 461.48770213127136 and batch: 700, loss is 3.53614378452301 and perplexity is 34.33426326293778
At time: 461.932092666626 and batch: 750, loss is 3.507206554412842 and perplexity is 33.35496225054558
At time: 462.376083612442 and batch: 800, loss is 3.619032940864563 and perplexity is 37.30147763947935
At time: 462.82046008110046 and batch: 850, loss is 3.4886749792099 and perplexity is 32.742534412008354
At time: 463.2804481983185 and batch: 900, loss is 3.4142806100845338 and perplexity is 30.3950756497187
At time: 463.72630548477173 and batch: 950, loss is 3.4583684682846068 and perplexity is 31.76510843211256
At time: 464.170405626297 and batch: 1000, loss is 3.362091474533081 and perplexity is 28.849465748500123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296193192644817 and perplexity of 73.41976612655188
Finished 47 epochs...
Completing Train Step...
At time: 465.5932297706604 and batch: 50, loss is 3.665002455711365 and perplexity is 39.05623207381457
At time: 466.0386004447937 and batch: 100, loss is 3.528987159729004 and perplexity is 34.0894229831981
At time: 466.48338890075684 and batch: 150, loss is 3.6452148723602296 and perplexity is 38.290999645140346
At time: 466.92849373817444 and batch: 200, loss is 3.70747820854187 and perplexity is 40.75091154103621
At time: 467.37513184547424 and batch: 250, loss is 3.6627533054351806 and perplexity is 38.96848745106957
At time: 467.821985244751 and batch: 300, loss is 3.5080409717559813 and perplexity is 33.38280582449218
At time: 468.2663354873657 and batch: 350, loss is 3.549212441444397 and perplexity is 34.78591075518561
At time: 468.7107563018799 and batch: 400, loss is 3.555840916633606 and perplexity is 35.01725418160354
At time: 469.15689873695374 and batch: 450, loss is 3.607775583267212 and perplexity is 36.88391629518824
At time: 469.60102581977844 and batch: 500, loss is 3.6461681222915647 and perplexity is 38.32751794068889
At time: 470.046950340271 and batch: 550, loss is 3.528901777267456 and perplexity is 34.08651246860626
At time: 470.49354577064514 and batch: 600, loss is 3.4908905982971192 and perplexity is 32.81515982162777
At time: 470.9384551048279 and batch: 650, loss is 3.448767886161804 and perplexity is 31.461604140102438
At time: 471.38423895835876 and batch: 700, loss is 3.53612859249115 and perplexity is 34.333741659678516
At time: 471.8299617767334 and batch: 750, loss is 3.5072018098831177 and perplexity is 33.35480399731115
At time: 472.2750606536865 and batch: 800, loss is 3.6190278339385986 and perplexity is 37.30128714408111
At time: 472.7192585468292 and batch: 850, loss is 3.48867506980896 and perplexity is 32.74253737845134
At time: 473.16491413116455 and batch: 900, loss is 3.414284334182739 and perplexity is 30.395188844176154
At time: 473.6090030670166 and batch: 950, loss is 3.4583746147155763 and perplexity is 31.7653036747588
At time: 474.053471326828 and batch: 1000, loss is 3.3621023511886596 and perplexity is 28.849779535909168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296186493664253 and perplexity of 73.41927429061298
Finished 48 epochs...
Completing Train Step...
At time: 475.45513439178467 and batch: 50, loss is 3.664956045150757 and perplexity is 39.05441949425053
At time: 475.9172360897064 and batch: 100, loss is 3.5289367198944093 and perplexity is 34.087703561705425
At time: 476.36106419563293 and batch: 150, loss is 3.645165190696716 and perplexity is 38.289097331835826
At time: 476.805686712265 and batch: 200, loss is 3.7074308300018313 and perplexity is 40.74898086807874
At time: 477.2496008872986 and batch: 250, loss is 3.662706427574158 and perplexity is 38.96666073454719
At time: 477.69404673576355 and batch: 300, loss is 3.507996191978455 and perplexity is 33.381310983343724
At time: 478.13697934150696 and batch: 350, loss is 3.5491713809967043 and perplexity is 34.78448245944004
At time: 478.5802330970764 and batch: 400, loss is 3.5558013248443605 and perplexity is 35.015867813300616
At time: 479.0258436203003 and batch: 450, loss is 3.607740683555603 and perplexity is 36.882629079608385
At time: 479.46919417381287 and batch: 500, loss is 3.6461378860473634 and perplexity is 38.32635907801672
At time: 479.91372084617615 and batch: 550, loss is 3.5288715600967406 and perplexity is 34.085482486201556
At time: 480.358952999115 and batch: 600, loss is 3.4908623838424684 and perplexity is 32.814233972850346
At time: 480.80465054512024 and batch: 650, loss is 3.4487474822998045 and perplexity is 31.460962208422245
At time: 481.24915862083435 and batch: 700, loss is 3.536113305091858 and perplexity is 34.33321679007253
At time: 481.6941022872925 and batch: 750, loss is 3.507196669578552 and perplexity is 33.35463254390054
At time: 482.13761854171753 and batch: 800, loss is 3.6190223264694215 and perplexity is 37.30108170895761
At time: 482.5816402435303 and batch: 850, loss is 3.4886744928359987 and perplexity is 32.742518486898035
At time: 483.0252118110657 and batch: 900, loss is 3.4142874813079835 and perplexity is 30.395284501792798
At time: 483.47070503234863 and batch: 950, loss is 3.45838011264801 and perplexity is 31.76547831873223
At time: 483.9150376319885 and batch: 1000, loss is 3.362112593650818 and perplexity is 28.850075030197633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.296180539014863 and perplexity of 73.41883710587771
Finished 49 epochs...
Completing Train Step...
At time: 485.30946493148804 and batch: 50, loss is 3.6649101781845093 and perplexity is 39.05262822759006
At time: 485.76817321777344 and batch: 100, loss is 3.5288869094848634 and perplexity is 34.08600568151694
At time: 486.2225389480591 and batch: 150, loss is 3.6451163053512574 and perplexity is 38.28722560183591
At time: 486.6997706890106 and batch: 200, loss is 3.707384009361267 and perplexity is 40.74707301935586
At time: 487.143851518631 and batch: 250, loss is 3.6626600503921507 and perplexity is 38.96485361253503
At time: 487.58788108825684 and batch: 300, loss is 3.507951855659485 and perplexity is 33.37983101170082
At time: 488.03205609321594 and batch: 350, loss is 3.5491306924819948 and perplexity is 34.78306715930725
At time: 488.47640562057495 and batch: 400, loss is 3.5557620763778686 and perplexity is 35.014493521155664
At time: 488.92156648635864 and batch: 450, loss is 3.607706084251404 and perplexity is 36.88135298838126
At time: 489.3662095069885 and batch: 500, loss is 3.6461079216003416 and perplexity is 38.32521066706642
At time: 489.80929136276245 and batch: 550, loss is 3.5288415336608887 and perplexity is 34.084459036013556
At time: 490.25557041168213 and batch: 600, loss is 3.4908345556259155 and perplexity is 32.813320823947045
At time: 490.69935369491577 and batch: 650, loss is 3.448727011680603 and perplexity is 31.460318189636915
At time: 491.142698764801 and batch: 700, loss is 3.5360979890823363 and perplexity is 34.33269094622419
At time: 491.58620142936707 and batch: 750, loss is 3.5071909999847413 and perplexity is 33.354443437218386
At time: 492.0322163105011 and batch: 800, loss is 3.6190163230895998 and perplexity is 37.30085777706853
At time: 492.47544503211975 and batch: 850, loss is 3.4886733627319337 and perplexity is 32.7424814844657
At time: 492.9196779727936 and batch: 900, loss is 3.4142901563644408 and perplexity is 30.395365811003625
At time: 493.3644654750824 and batch: 950, loss is 3.4583849191665648 and perplexity is 31.765631000460104
At time: 493.80972838401794 and batch: 1000, loss is 3.362121934890747 and perplexity is 28.850344526929177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.29617607302782 and perplexity of 73.4185092190347
Finished Training.
Improved accuracyfrom -10000000 to -73.4185092190347
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
SETTINGS FOR THIS RUN
{'anneal': 5.878162736267521, 'dropout': 0.5378033310392732, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 29.53510895111857}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7328670024871826 and batch: 50, loss is 6.6876768875122075 and perplexity is 802.4558898000239
At time: 1.1994810104370117 and batch: 100, loss is 6.082900056838989 and perplexity is 438.2984437810178
At time: 1.64967942237854 and batch: 150, loss is 6.046014080047607 and perplexity is 422.4259140445815
At time: 2.1000924110412598 and batch: 200, loss is 6.068408670425415 and perplexity is 431.9926915641454
At time: 2.5495851039886475 and batch: 250, loss is 6.234233961105347 and perplexity is 509.90985801454883
At time: 2.999711513519287 and batch: 300, loss is 6.294470949172974 and perplexity is 541.5692529273715
At time: 3.447690010070801 and batch: 350, loss is 6.446226797103882 and perplexity is 630.3194769395333
At time: 3.895669937133789 and batch: 400, loss is 6.77135931968689 and perplexity is 872.4970907498436
At time: 4.341727256774902 and batch: 450, loss is 6.549997673034668 and perplexity is 699.2425467008276
At time: 4.804074048995972 and batch: 500, loss is 6.703297986984253 and perplexity is 815.0895520204805
At time: 5.253166913986206 and batch: 550, loss is 6.568921575546264 and perplexity is 712.6009423247123
At time: 5.701128005981445 and batch: 600, loss is 6.567108535766602 and perplexity is 711.3101389617597
At time: 6.148284196853638 and batch: 650, loss is 6.45832929611206 and perplexity is 637.9942661712842
At time: 6.596432209014893 and batch: 700, loss is 6.5317349529266355 and perplexity is 686.5883772670055
At time: 7.043179512023926 and batch: 750, loss is 6.823281831741333 and perplexity is 918.996056933868
At time: 7.490253686904907 and batch: 800, loss is 7.120473127365113 and perplexity is 1237.0355704125475
At time: 7.937450408935547 and batch: 850, loss is 6.845925893783569 and perplexity is 940.0432584036846
At time: 8.384528160095215 and batch: 900, loss is 6.772404279708862 and perplexity is 873.4092918528008
At time: 8.831322431564331 and batch: 950, loss is 6.71210428237915 and perplexity is 822.2991698082308
At time: 9.279550075531006 and batch: 1000, loss is 6.896624145507812 and perplexity is 988.9305883682588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 7.092032827982089 and perplexity of 1202.3494876879229
Finished 1 epochs...
Completing Train Step...
At time: 10.68865418434143 and batch: 50, loss is 6.9222931385040285 and perplexity is 1014.6440481627945
At time: 11.130803108215332 and batch: 100, loss is 6.64785400390625 and perplexity is 771.1277115705554
At time: 11.572187662124634 and batch: 150, loss is 6.465450353622437 and perplexity is 642.5536746699676
At time: 12.014730453491211 and batch: 200, loss is 6.9810997486114506 and perplexity is 1076.1011572440545
At time: 12.45607328414917 and batch: 250, loss is 6.583040933609009 and perplexity is 722.7337764050868
At time: 12.897459983825684 and batch: 300, loss is 6.557204055786133 and perplexity is 704.2997563367293
At time: 13.33972454071045 and batch: 350, loss is 6.60460732460022 and perplexity is 738.4898254814474
At time: 13.782501459121704 and batch: 400, loss is 6.601360673904419 and perplexity is 736.0960948819051
At time: 14.2250235080719 and batch: 450, loss is 6.839445123672485 and perplexity is 933.9707526622982
At time: 14.666718244552612 and batch: 500, loss is 6.680775079727173 and perplexity is 796.9365620715888
At time: 15.10978627204895 and batch: 550, loss is 6.550523204803467 and perplexity is 699.6101174498076
At time: 15.5518958568573 and batch: 600, loss is 6.696354722976684 and perplexity is 809.4497719692002
At time: 15.99298357963562 and batch: 650, loss is 6.720267686843872 and perplexity is 829.03940472289
At time: 16.451013565063477 and batch: 700, loss is 6.9861931896209715 and perplexity is 1081.5961974604695
At time: 16.892580032348633 and batch: 750, loss is 6.675142183303833 and perplexity is 792.460120463701
At time: 17.334800243377686 and batch: 800, loss is 6.886131410598755 and perplexity is 978.6082513392023
At time: 17.77869415283203 and batch: 850, loss is 6.792434663772583 and perplexity is 891.0804041725132
At time: 18.2220356464386 and batch: 900, loss is 6.595465068817139 and perplexity is 731.7691305697001
At time: 18.664507389068604 and batch: 950, loss is 6.39717999458313 and perplexity is 600.1502224226988
At time: 19.106773138046265 and batch: 1000, loss is 6.387742738723755 and perplexity is 594.513092575656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 7.739570431592988 and perplexity of 2297.485244039213
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 20.494052171707153 and batch: 50, loss is 6.060978002548218 and perplexity is 428.7945940631261
At time: 20.953186988830566 and batch: 100, loss is 5.837537174224853 and perplexity is 342.93371378707934
At time: 21.39736032485962 and batch: 150, loss is 5.777216711044312 and perplexity is 322.8593279078573
At time: 21.84115171432495 and batch: 200, loss is 5.792859916687012 and perplexity is 327.9495929970088
At time: 22.2855544090271 and batch: 250, loss is 5.825596551895142 and perplexity is 338.86322229103416
At time: 22.72940993309021 and batch: 300, loss is 5.748754167556763 and perplexity is 313.79947509394634
At time: 23.172083377838135 and batch: 350, loss is 5.759073524475098 and perplexity is 317.0544496152512
At time: 23.615939378738403 and batch: 400, loss is 5.738443679809571 and perplexity is 310.5806716823176
At time: 24.05936598777771 and batch: 450, loss is 5.784738502502441 and perplexity is 325.2969646473874
At time: 24.502469778060913 and batch: 500, loss is 5.798478326797485 and perplexity is 329.7973341291958
At time: 24.94667363166809 and batch: 550, loss is 5.731863422393799 and perplexity is 308.54368022844756
At time: 25.390548706054688 and batch: 600, loss is 5.651903657913208 and perplexity is 284.8331749733564
At time: 25.833791732788086 and batch: 650, loss is 5.60525598526001 and perplexity is 271.85150650664195
At time: 26.277883768081665 and batch: 700, loss is 5.694769945144653 and perplexity is 297.3083884796412
At time: 26.722065210342407 and batch: 750, loss is 5.58492805480957 and perplexity is 266.38111720983505
At time: 27.16560435295105 and batch: 800, loss is 5.6652553749084475 and perplexity is 288.66168865699956
At time: 27.623875856399536 and batch: 850, loss is 5.621021461486817 and perplexity is 276.1713375926215
At time: 28.068011045455933 and batch: 900, loss is 5.650510215759278 and perplexity is 284.4365528197008
At time: 28.511488914489746 and batch: 950, loss is 5.602191915512085 and perplexity is 271.0198093693466
At time: 28.955358505249023 and batch: 1000, loss is 5.536491899490357 and perplexity is 253.78612873959142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.656942227991616 and perplexity of 286.2719485229265
Finished 3 epochs...
Completing Train Step...
At time: 30.343908309936523 and batch: 50, loss is 5.664797315597534 and perplexity is 288.52949476143993
At time: 30.804415225982666 and batch: 100, loss is 5.6339540195465085 and perplexity is 279.76613430641135
At time: 31.24810290336609 and batch: 150, loss is 5.593113737106323 and perplexity is 268.57057729203007
At time: 31.692808151245117 and batch: 200, loss is 5.608847045898438 and perplexity is 272.8294967112293
At time: 32.137431144714355 and batch: 250, loss is 5.657980117797852 and perplexity is 286.56922150172136
At time: 32.579777002334595 and batch: 300, loss is 5.5936720752716065 and perplexity is 268.7205723654948
At time: 33.02321648597717 and batch: 350, loss is 5.594234771728516 and perplexity is 268.87182302957905
At time: 33.465909481048584 and batch: 400, loss is 5.580069522857666 and perplexity is 265.0900349623644
At time: 33.90902638435364 and batch: 450, loss is 5.632598104476929 and perplexity is 279.3870522485634
At time: 34.35184907913208 and batch: 500, loss is 5.652005052566528 and perplexity is 284.86205699860346
At time: 34.79564690589905 and batch: 550, loss is 5.587737522125244 and perplexity is 267.13055852463674
At time: 35.238834381103516 and batch: 600, loss is 5.506171360015869 and perplexity is 246.2066835012856
At time: 35.6804563999176 and batch: 650, loss is 5.481331977844238 and perplexity is 240.16639073280243
At time: 36.12364172935486 and batch: 700, loss is 5.580449848175049 and perplexity is 265.1908745887624
At time: 36.56825876235962 and batch: 750, loss is 5.482428922653198 and perplexity is 240.42998455590967
At time: 37.0104763507843 and batch: 800, loss is 5.565505037307739 and perplexity is 261.2571149868871
At time: 37.453949213027954 and batch: 850, loss is 5.515640087127686 and perplexity is 248.5490193695344
At time: 37.89876842498779 and batch: 900, loss is 5.5514803314208985 and perplexity is 257.61863473430833
At time: 38.34186100959778 and batch: 950, loss is 5.510612411499023 and perplexity is 247.30253161604776
At time: 38.799259185791016 and batch: 1000, loss is 5.441384296417237 and perplexity is 230.7614046469828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.568177013862424 and perplexity of 261.9561213212679
Finished 4 epochs...
Completing Train Step...
At time: 40.20347881317139 and batch: 50, loss is 5.57089617729187 and perplexity is 262.6693921369584
At time: 40.64907693862915 and batch: 100, loss is 5.53423547744751 and perplexity is 253.21412570735313
At time: 41.09325098991394 and batch: 150, loss is 5.498642768859863 and perplexity is 244.36005402269862
At time: 41.53583312034607 and batch: 200, loss is 5.522838191986084 and perplexity is 250.3445557503538
At time: 41.980847120285034 and batch: 250, loss is 5.566941661834717 and perplexity is 261.6327130982548
At time: 42.42367076873779 and batch: 300, loss is 5.508850803375244 and perplexity is 246.86726496451874
At time: 42.868595361709595 and batch: 350, loss is 5.504709854125976 and perplexity is 245.84711380383627
At time: 43.313056230545044 and batch: 400, loss is 5.495636882781983 and perplexity is 243.62663837249008
At time: 43.7578809261322 and batch: 450, loss is 5.540241775512695 and perplexity is 254.73958180669777
At time: 44.201717138290405 and batch: 500, loss is 5.562739276885987 and perplexity is 260.5355387118771
At time: 44.647310972213745 and batch: 550, loss is 5.494708795547485 and perplexity is 243.40063649038595
At time: 45.09322786331177 and batch: 600, loss is 5.4208825588226315 and perplexity is 226.0785621100931
At time: 45.53586196899414 and batch: 650, loss is 5.4106591701507565 and perplexity is 223.7790475431419
At time: 45.98099493980408 and batch: 700, loss is 5.50998776435852 and perplexity is 247.14810303354216
At time: 46.42642116546631 and batch: 750, loss is 5.415634202957153 and perplexity is 224.89512961651124
At time: 46.87037944793701 and batch: 800, loss is 5.503554639816284 and perplexity is 245.5632716807807
At time: 47.31417798995972 and batch: 850, loss is 5.450792083740234 and perplexity is 232.94260290141324
At time: 47.75904893875122 and batch: 900, loss is 5.490436487197876 and perplexity is 242.36297210825037
At time: 48.202472448349 and batch: 950, loss is 5.453580417633057 and perplexity is 233.59303104016496
At time: 48.64679265022278 and batch: 1000, loss is 5.3845922470092775 and perplexity is 218.02118718951468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.52498608100705 and perplexity of 250.88284595997743
Finished 5 epochs...
Completing Train Step...
At time: 50.040303230285645 and batch: 50, loss is 5.517107038497925 and perplexity is 248.91389625696218
At time: 50.49974822998047 and batch: 100, loss is 5.484667129516602 and perplexity is 240.9687192724785
At time: 50.94247364997864 and batch: 150, loss is 5.458131875991821 and perplexity is 234.65864319894445
At time: 51.384830713272095 and batch: 200, loss is 5.480973873138428 and perplexity is 240.080401415612
At time: 51.8290650844574 and batch: 250, loss is 5.520846633911133 and perplexity is 249.8464761706615
At time: 52.27249312400818 and batch: 300, loss is 5.465091781616211 and perplexity is 236.29754188392272
At time: 52.715660572052 and batch: 350, loss is 5.464401750564575 and perplexity is 236.1345454853533
At time: 53.15836787223816 and batch: 400, loss is 5.456331634521485 and perplexity is 234.2365809990099
At time: 53.602320432662964 and batch: 450, loss is 5.5040271186828615 and perplexity is 245.6793225506904
At time: 54.04540395736694 and batch: 500, loss is 5.529621973037719 and perplexity is 252.04861184073616
At time: 54.488879680633545 and batch: 550, loss is 5.463125963211059 and perplexity is 235.83348010703122
At time: 54.93238353729248 and batch: 600, loss is 5.386424331665039 and perplexity is 218.42098658253386
At time: 55.374207973480225 and batch: 650, loss is 5.380163993835449 and perplexity is 217.05786865872798
At time: 55.816144704818726 and batch: 700, loss is 5.477467823028564 and perplexity is 239.240141355524
At time: 56.2611358165741 and batch: 750, loss is 5.3856307220458985 and perplexity is 218.24771435090875
At time: 56.70556592941284 and batch: 800, loss is 5.472156677246094 and perplexity is 237.97287039712762
At time: 57.15008544921875 and batch: 850, loss is 5.41658052444458 and perplexity is 225.10805344139044
At time: 57.59605669975281 and batch: 900, loss is 5.4544091892242434 and perplexity is 233.78670655348623
At time: 58.04028630256653 and batch: 950, loss is 5.417117052078247 and perplexity is 225.22886253843416
At time: 58.48284602165222 and batch: 1000, loss is 5.352781820297241 and perplexity is 211.19498795930838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.500627843345084 and perplexity of 244.84560870280478
Finished 6 epochs...
Completing Train Step...
At time: 59.88948369026184 and batch: 50, loss is 5.48732400894165 and perplexity is 241.60979535861193
At time: 60.34806776046753 and batch: 100, loss is 5.446091766357422 and perplexity is 231.8502679074557
At time: 60.79194641113281 and batch: 150, loss is 5.427299585342407 and perplexity is 227.53397896870848
At time: 61.236377000808716 and batch: 200, loss is 5.449932031631469 and perplexity is 232.74234625248732
At time: 61.68038296699524 and batch: 250, loss is 5.490125570297241 and perplexity is 242.2876290774517
At time: 62.13840889930725 and batch: 300, loss is 5.428756256103515 and perplexity is 227.86566258129133
At time: 62.581451177597046 and batch: 350, loss is 5.426204872131348 and perplexity is 227.28503080425037
At time: 63.025304555892944 and batch: 400, loss is 5.42678991317749 and perplexity is 227.41804078079875
At time: 63.4690158367157 and batch: 450, loss is 5.471193504333496 and perplexity is 237.74377172295792
At time: 63.911564111709595 and batch: 500, loss is 5.496060342788696 and perplexity is 243.72982635686114
At time: 64.3537266254425 and batch: 550, loss is 5.4271135330200195 and perplexity is 227.4916496813525
At time: 64.79804420471191 and batch: 600, loss is 5.355515832901001 and perplexity is 211.77318776085073
At time: 65.2397677898407 and batch: 650, loss is 5.344355001449585 and perplexity is 209.4227636481694
At time: 65.68200397491455 and batch: 700, loss is 5.443947763442993 and perplexity is 231.3537127554091
At time: 66.12667322158813 and batch: 750, loss is 5.349531011581421 and perplexity is 210.50954817215987
At time: 66.5697021484375 and batch: 800, loss is 5.4348234272003175 and perplexity is 229.2523649698117
At time: 67.01471400260925 and batch: 850, loss is 5.3792853355407715 and perplexity is 216.86723272619253
At time: 67.45798325538635 and batch: 900, loss is 5.420673122406006 and perplexity is 226.03121798413403
At time: 67.90058422088623 and batch: 950, loss is 5.3838707542419435 and perplexity is 217.8639432118493
At time: 68.34186005592346 and batch: 1000, loss is 5.320498189926147 and perplexity is 204.48572935160138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.479048938286014 and perplexity of 239.61870679214408
Finished 7 epochs...
Completing Train Step...
At time: 69.74627208709717 and batch: 50, loss is 5.45806056022644 and perplexity is 234.64190893491696
At time: 70.19135761260986 and batch: 100, loss is 5.411049070358277 and perplexity is 223.86631605211699
At time: 70.63726758956909 and batch: 150, loss is 5.394982261657715 and perplexity is 220.29823933626238
At time: 71.08140540122986 and batch: 200, loss is 5.413196306228638 and perplexity is 224.34752628715881
At time: 71.52665853500366 and batch: 250, loss is 5.458469734191895 and perplexity is 234.7379379402024
At time: 71.97123742103577 and batch: 300, loss is 5.39952054977417 and perplexity is 221.30028829149663
At time: 72.414217710495 and batch: 350, loss is 5.3993666553497315 and perplexity is 221.2662340314495
At time: 72.85723090171814 and batch: 400, loss is 5.395843029022217 and perplexity is 220.48794650629753
At time: 73.31762218475342 and batch: 450, loss is 5.439758281707764 and perplexity is 230.38648810116163
At time: 73.7610387802124 and batch: 500, loss is 5.466939115524292 and perplexity is 236.73446579336232
At time: 74.20422530174255 and batch: 550, loss is 5.400715532302857 and perplexity is 221.56489633910726
At time: 74.64806747436523 and batch: 600, loss is 5.3310075283050535 and perplexity is 206.64607107267184
At time: 75.09023714065552 and batch: 650, loss is 5.319758262634277 and perplexity is 204.33448074304343
At time: 75.53287768363953 and batch: 700, loss is 5.419329452514648 and perplexity is 225.72771059455633
At time: 75.97745108604431 and batch: 750, loss is 5.328270759582519 and perplexity is 206.08130174274996
At time: 76.42003107070923 and batch: 800, loss is 5.416020307540894 and perplexity is 224.98197942239224
At time: 76.86468291282654 and batch: 850, loss is 5.3592000007629395 and perplexity is 212.55483470838755
At time: 77.308673620224 and batch: 900, loss is 5.3998720169067385 and perplexity is 221.37808173937822
At time: 77.75209355354309 and batch: 950, loss is 5.364314279556274 and perplexity is 213.6446839128014
At time: 78.19457173347473 and batch: 1000, loss is 5.298448638916016 and perplexity is 200.02625619691466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.46368408203125 and perplexity of 235.96513994821004
Finished 8 epochs...
Completing Train Step...
At time: 79.58709764480591 and batch: 50, loss is 5.441300659179688 and perplexity is 230.74210520765237
At time: 80.04629635810852 and batch: 100, loss is 5.394658641815186 and perplexity is 220.22695798939017
At time: 80.49035954475403 and batch: 150, loss is 5.374970178604126 and perplexity is 215.93343277821543
At time: 80.93396472930908 and batch: 200, loss is 5.393004398345948 and perplexity is 219.86295014419602
At time: 81.37870740890503 and batch: 250, loss is 5.4329807090759275 and perplexity is 228.83030646867618
At time: 81.82414174079895 and batch: 300, loss is 5.37971983909607 and perplexity is 216.9614827843496
At time: 82.26788067817688 and batch: 350, loss is 5.377954425811768 and perplexity is 216.5787940017839
At time: 82.7117006778717 and batch: 400, loss is 5.368829946517945 and perplexity is 214.6116136768029
At time: 83.15789198875427 and batch: 450, loss is 5.416552314758301 and perplexity is 225.10170330339201
At time: 83.6008129119873 and batch: 500, loss is 5.446904039382934 and perplexity is 232.03867013269868
At time: 84.0460033416748 and batch: 550, loss is 5.3817160987854 and perplexity is 217.39502683585152
At time: 84.49094152450562 and batch: 600, loss is 5.312656955718994 and perplexity is 202.8885788546755
At time: 84.94859337806702 and batch: 650, loss is 5.2977411079406735 and perplexity is 199.88478147954785
At time: 85.3926522731781 and batch: 700, loss is 5.397405347824097 and perplexity is 220.8326881988046
At time: 85.83729219436646 and batch: 750, loss is 5.308445863723755 and perplexity is 202.03599280341257
At time: 86.28187417984009 and batch: 800, loss is 5.397032871246338 and perplexity is 220.75044851197634
At time: 86.72648477554321 and batch: 850, loss is 5.339646091461182 and perplexity is 208.43892891717175
At time: 87.17058658599854 and batch: 900, loss is 5.371336793899536 and perplexity is 215.15028714259319
At time: 87.61440539360046 and batch: 950, loss is 5.3337365245819095 and perplexity is 207.21077762182392
At time: 88.05678582191467 and batch: 1000, loss is 5.2683898448944095 and perplexity is 194.10317447700302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.436943984613186 and perplexity of 229.7390235831654
Finished 9 epochs...
Completing Train Step...
At time: 89.44906258583069 and batch: 50, loss is 5.4123023319244385 and perplexity is 224.14705498489505
At time: 89.9078369140625 and batch: 100, loss is 5.364391450881958 and perplexity is 213.66117179247178
At time: 90.35268473625183 and batch: 150, loss is 5.349655504226685 and perplexity is 210.53575669401533
At time: 90.79618501663208 and batch: 200, loss is 5.36665078163147 and perplexity is 214.14444878358552
At time: 91.24026989936829 and batch: 250, loss is 5.413203830718994 and perplexity is 224.34921439430786
At time: 91.68498110771179 and batch: 300, loss is 5.351082954406738 and perplexity is 210.83650059521727
At time: 92.12858176231384 and batch: 350, loss is 5.355208806991577 and perplexity is 211.7081778856555
At time: 92.57175731658936 and batch: 400, loss is 5.342989826202393 and perplexity is 209.13705993722778
At time: 93.0165696144104 and batch: 450, loss is 5.396093788146973 and perplexity is 220.54324280351855
At time: 93.45983529090881 and batch: 500, loss is 5.423444471359253 and perplexity is 226.65849816815728
At time: 93.90270066261292 and batch: 550, loss is 5.352963514328003 and perplexity is 211.23336431421922
At time: 94.3478422164917 and batch: 600, loss is 5.28814356803894 and perplexity is 197.9755559031898
At time: 94.79057574272156 and batch: 650, loss is 5.266943387985229 and perplexity is 193.8226155562979
At time: 95.23369812965393 and batch: 700, loss is 5.3693413734436035 and perplexity is 214.72139990602017
At time: 95.67847657203674 and batch: 750, loss is 5.283518342971802 and perplexity is 197.06198875482224
At time: 96.13732171058655 and batch: 800, loss is 5.368191900253296 and perplexity is 214.47472521357653
At time: 96.58168911933899 and batch: 850, loss is 5.31035888671875 and perplexity is 202.42286223053316
At time: 97.02840948104858 and batch: 900, loss is 5.353483009338379 and perplexity is 211.34312750125073
At time: 97.47411751747131 and batch: 950, loss is 5.319887619018555 and perplexity is 204.36091442230136
At time: 97.91722702980042 and batch: 1000, loss is 5.246416330337524 and perplexity is 189.88456413479187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.423803934236852 and perplexity of 226.7399881295716
Finished 10 epochs...
Completing Train Step...
At time: 99.3188374042511 and batch: 50, loss is 5.395703868865967 and perplexity is 220.45726550404655
At time: 99.76195812225342 and batch: 100, loss is 5.344096574783325 and perplexity is 209.36865021399933
At time: 100.20686292648315 and batch: 150, loss is 5.3298491191864015 and perplexity is 206.40682897647403
At time: 100.64931988716125 and batch: 200, loss is 5.345588121414185 and perplexity is 209.68116632707319
At time: 101.0929548740387 and batch: 250, loss is 5.3933172702789305 and perplexity is 219.9317498525866
At time: 101.53617715835571 and batch: 300, loss is 5.333529529571533 and perplexity is 207.16789046362695
At time: 101.97868490219116 and batch: 350, loss is 5.3384812068939205 and perplexity is 208.19626299199584
At time: 102.42098617553711 and batch: 400, loss is 5.3236854553222654 and perplexity is 205.1385193957003
At time: 102.86590838432312 and batch: 450, loss is 5.3746769237518315 and perplexity is 215.87011853533875
At time: 103.30860209465027 and batch: 500, loss is 5.407354516983032 and perplexity is 223.04075597548228
At time: 103.75181937217712 and batch: 550, loss is 5.3332510089874265 and perplexity is 207.11019797641254
At time: 104.19569063186646 and batch: 600, loss is 5.273619422912597 and perplexity is 195.1209110183692
At time: 104.6388578414917 and batch: 650, loss is 5.250580568313598 and perplexity is 190.67693731804576
At time: 105.08175325393677 and batch: 700, loss is 5.350389804840088 and perplexity is 210.69041000335994
At time: 105.52590918540955 and batch: 750, loss is 5.264740800857544 and perplexity is 193.39617416777855
At time: 105.96902251243591 and batch: 800, loss is 5.351519718170166 and perplexity is 210.92860645147107
At time: 106.41212105751038 and batch: 850, loss is 5.289537324905395 and perplexity is 198.25167807250622
At time: 106.85526418685913 and batch: 900, loss is 5.333813915252685 and perplexity is 207.22681442343747
At time: 107.29867315292358 and batch: 950, loss is 5.296868133544922 and perplexity is 199.7103633255838
At time: 107.75586342811584 and batch: 1000, loss is 5.225747222900391 and perplexity is 186.0001021488453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.409598745950839 and perplexity of 223.54187260094204
Finished 11 epochs...
Completing Train Step...
At time: 109.14528346061707 and batch: 50, loss is 5.37970006942749 and perplexity is 216.95719357013874
At time: 109.60543584823608 and batch: 100, loss is 5.324153079986572 and perplexity is 205.23446965957555
At time: 110.05271792411804 and batch: 150, loss is 5.31257966041565 and perplexity is 202.87289712649755
At time: 110.49806499481201 and batch: 200, loss is 5.330064115524292 and perplexity is 206.45121045957669
At time: 110.94223928451538 and batch: 250, loss is 5.378691520690918 and perplexity is 216.73849197082222
At time: 111.38704705238342 and batch: 300, loss is 5.314924125671387 and perplexity is 203.34908356831147
At time: 111.83097696304321 and batch: 350, loss is 5.32166166305542 and perplexity is 204.72378145978365
At time: 112.27404165267944 and batch: 400, loss is 5.305865907669068 and perplexity is 201.51542063592396
At time: 112.71928906440735 and batch: 450, loss is 5.357958335876464 and perplexity is 212.29107661714687
At time: 113.16347455978394 and batch: 500, loss is 5.389802932739258 and perplexity is 219.16019199937554
At time: 113.60722517967224 and batch: 550, loss is 5.3193436622619625 and perplexity is 204.24978115070516
At time: 114.05199027061462 and batch: 600, loss is 5.256350393295288 and perplexity is 191.78028988906632
At time: 114.49704813957214 and batch: 650, loss is 5.237809133529663 and perplexity is 188.25720387228242
At time: 114.94024157524109 and batch: 700, loss is 5.335490169525147 and perplexity is 207.5744705551169
At time: 115.38435101509094 and batch: 750, loss is 5.254179048538208 and perplexity is 191.36432053197788
At time: 115.82930636405945 and batch: 800, loss is 5.338301753997802 and perplexity is 208.15890492174816
At time: 116.27416920661926 and batch: 850, loss is 5.276828193664551 and perplexity is 195.7480148689206
At time: 116.71725177764893 and batch: 900, loss is 5.319553985595703 and perplexity is 204.29274416349617
At time: 117.16354274749756 and batch: 950, loss is 5.284285697937012 and perplexity is 197.2132632835548
At time: 117.60656833648682 and batch: 1000, loss is 5.210456581115722 and perplexity is 183.1776745704671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.4047300757431405 and perplexity of 222.4561660637995
Finished 12 epochs...
Completing Train Step...
At time: 118.99999761581421 and batch: 50, loss is 5.363340225219726 and perplexity is 213.43668370015584
At time: 119.45773148536682 and batch: 100, loss is 5.307076711654663 and perplexity is 201.75956408549254
At time: 119.9022536277771 and batch: 150, loss is 5.296972303390503 and perplexity is 199.7311682068943
At time: 120.34533619880676 and batch: 200, loss is 5.310742769241333 and perplexity is 202.5005837465261
At time: 120.78809309005737 and batch: 250, loss is 5.362359447479248 and perplexity is 213.22745237326728
At time: 121.23231911659241 and batch: 300, loss is 5.297275581359863 and perplexity is 199.79175145632408
At time: 121.67520809173584 and batch: 350, loss is 5.301643362045288 and perplexity is 200.66630655150823
At time: 122.1177282333374 and batch: 400, loss is 5.286735620498657 and perplexity is 197.69701283924394
At time: 122.5618634223938 and batch: 450, loss is 5.331952991485596 and perplexity is 206.84153971391567
At time: 123.0063579082489 and batch: 500, loss is 5.36229474067688 and perplexity is 213.21365555302606
At time: 123.45003724098206 and batch: 550, loss is 5.286674556732177 and perplexity is 197.6849410835954
At time: 123.89248156547546 and batch: 600, loss is 5.229197225570679 and perplexity is 186.6429112068288
At time: 124.33564233779907 and batch: 650, loss is 5.199893264770508 and perplexity is 181.25289477334115
At time: 124.77825975418091 and batch: 700, loss is 5.2933252334594725 and perplexity is 199.0040613797047
At time: 125.22102785110474 and batch: 750, loss is 5.2119538116455075 and perplexity is 183.45213919434389
At time: 125.6653208732605 and batch: 800, loss is 5.295196037292481 and perplexity is 199.37670740563942
At time: 126.10832452774048 and batch: 850, loss is 5.23244873046875 and perplexity is 187.25076923795368
At time: 126.55079460144043 and batch: 900, loss is 5.277846803665161 and perplexity is 195.9475073397297
At time: 126.99394345283508 and batch: 950, loss is 5.232456207275391 and perplexity is 187.25216928098246
At time: 127.4361162185669 and batch: 1000, loss is 5.161523904800415 and perplexity is 174.43006798765913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.362369444312119 and perplexity of 213.2295839831268
Finished 13 epochs...
Completing Train Step...
At time: 128.84128785133362 and batch: 50, loss is 5.315227785110474 and perplexity is 203.41084181322913
At time: 129.28506183624268 and batch: 100, loss is 5.253085565567017 and perplexity is 191.1551812721313
At time: 129.72950053215027 and batch: 150, loss is 5.246436309814453 and perplexity is 189.88835796695943
At time: 130.17274165153503 and batch: 200, loss is 5.2592604446411135 and perplexity is 192.33919320401475
At time: 130.6303369998932 and batch: 250, loss is 5.3120039939880375 and perplexity is 202.75614361931085
At time: 131.07434797286987 and batch: 300, loss is 5.252323913574219 and perplexity is 191.00964297918284
At time: 131.51885724067688 and batch: 350, loss is 5.2507648944854735 and perplexity is 190.71208730739846
At time: 131.96096897125244 and batch: 400, loss is 5.242193393707275 and perplexity is 189.08438439669786
At time: 132.404217004776 and batch: 450, loss is 5.28561861038208 and perplexity is 197.47630656440455
At time: 132.8479974269867 and batch: 500, loss is 5.3222620391845705 and perplexity is 204.84672963512324
At time: 133.29083728790283 and batch: 550, loss is 5.243071670532227 and perplexity is 189.25052577785638
At time: 133.73374462127686 and batch: 600, loss is 5.188851594924927 and perplexity is 179.2625686341741
At time: 134.17769265174866 and batch: 650, loss is 5.163176717758179 and perplexity is 174.7186066487862
At time: 134.62025618553162 and batch: 700, loss is 5.262139739990235 and perplexity is 192.89379259303004
At time: 135.06349539756775 and batch: 750, loss is 5.181618728637695 and perplexity is 177.9706641631756
At time: 135.50883197784424 and batch: 800, loss is 5.266604280471801 and perplexity is 193.75689999404074
At time: 135.95343685150146 and batch: 850, loss is 5.208404273986816 and perplexity is 182.8021232282555
At time: 136.39639806747437 and batch: 900, loss is 5.251765642166138 and perplexity is 190.90303751699938
At time: 136.841463804245 and batch: 950, loss is 5.205977411270141 and perplexity is 182.3590254570689
At time: 137.28341603279114 and batch: 1000, loss is 5.137112503051758 and perplexity is 170.22353805997008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.348855367520961 and perplexity of 210.367366683715
Finished 14 epochs...
Completing Train Step...
At time: 138.69207763671875 and batch: 50, loss is 5.2935013103485105 and perplexity is 199.03910448078796
At time: 139.15039801597595 and batch: 100, loss is 5.234101619720459 and perplexity is 187.56052995135488
At time: 139.59415888786316 and batch: 150, loss is 5.22769684791565 and perplexity is 186.36308632737632
At time: 140.03845357894897 and batch: 200, loss is 5.241467790603638 and perplexity is 188.94723394494446
At time: 140.48143219947815 and batch: 250, loss is 5.291343612670898 and perplexity is 198.61010126335978
At time: 140.9256682395935 and batch: 300, loss is 5.228762798309326 and perplexity is 186.56184604779477
At time: 141.3699245452881 and batch: 350, loss is 5.228524312973023 and perplexity is 186.51735908815334
At time: 141.81270575523376 and batch: 400, loss is 5.219426870346069 and perplexity is 184.82822317333375
At time: 142.2710087299347 and batch: 450, loss is 5.263622665405274 and perplexity is 193.18005189861063
At time: 142.71698880195618 and batch: 500, loss is 5.298354301452637 and perplexity is 200.00738711734044
At time: 143.1606740951538 and batch: 550, loss is 5.222756309509277 and perplexity is 185.44462306158948
At time: 143.6034595966339 and batch: 600, loss is 5.167087478637695 and perplexity is 175.40322716159974
At time: 144.04837226867676 and batch: 650, loss is 5.141642036437989 and perplexity is 170.99632010853713
At time: 144.4908127784729 and batch: 700, loss is 5.239651470184326 and perplexity is 188.60435670748495
At time: 144.93425488471985 and batch: 750, loss is 5.160979948043823 and perplexity is 174.33521137491113
At time: 145.37816262245178 and batch: 800, loss is 5.243977031707764 and perplexity is 189.42194344208565
At time: 145.82241415977478 and batch: 850, loss is 5.189237279891968 and perplexity is 179.33172084667663
At time: 146.26513648033142 and batch: 900, loss is 5.2266828727722165 and perplexity is 186.17421456200688
At time: 146.70891666412354 and batch: 950, loss is 5.186298055648804 and perplexity is 178.80539857369783
At time: 147.15190172195435 and batch: 1000, loss is 5.112653322219849 and perplexity is 166.11051539309773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.325829854825648 and perplexity of 205.5788903316734
Finished 15 epochs...
Completing Train Step...
At time: 148.54606175422668 and batch: 50, loss is 5.267463331222534 and perplexity is 193.92341851813137
At time: 149.00633645057678 and batch: 100, loss is 5.206094217300415 and perplexity is 182.38032733498684
At time: 149.4514765739441 and batch: 150, loss is 5.201972408294678 and perplexity is 181.63013759083003
At time: 149.89568305015564 and batch: 200, loss is 5.214947881698609 and perplexity is 184.00223084601413
At time: 150.34004998207092 and batch: 250, loss is 5.26961046218872 and perplexity is 194.34024482529944
At time: 150.7846064567566 and batch: 300, loss is 5.205892782211304 and perplexity is 182.34359323739034
At time: 151.2294569015503 and batch: 350, loss is 5.202297115325928 and perplexity is 181.68912374968477
At time: 151.67361450195312 and batch: 400, loss is 5.194224443435669 and perplexity is 180.2283113304821
At time: 152.11739110946655 and batch: 450, loss is 5.2326552677154545 and perplexity is 187.28944749038774
At time: 152.56494998931885 and batch: 500, loss is 5.2731577587127685 and perplexity is 195.03085146934842
At time: 153.00865197181702 and batch: 550, loss is 5.200271892547607 and perplexity is 181.32153514773816
At time: 153.46747207641602 and batch: 600, loss is 5.145813617706299 and perplexity is 171.71113507220235
At time: 153.91320157051086 and batch: 650, loss is 5.117493133544922 and perplexity is 166.91640755663528
At time: 154.35709691047668 and batch: 700, loss is 5.2211002922058105 and perplexity is 185.13777769770488
At time: 154.80111575126648 and batch: 750, loss is 5.140478410720825 and perplexity is 170.79746011465846
At time: 155.24612641334534 and batch: 800, loss is 5.229720373153686 and perplexity is 186.74057853969921
At time: 155.6899116039276 and batch: 850, loss is 5.168078346252441 and perplexity is 175.5771146744415
At time: 156.13363790512085 and batch: 900, loss is 5.206866846084595 and perplexity is 182.5212940760223
At time: 156.57824778556824 and batch: 950, loss is 5.1674986171722415 and perplexity is 175.47535701403453
At time: 157.02239346504211 and batch: 1000, loss is 5.092707405090332 and perplexity is 162.8301128923908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.309906005859375 and perplexity of 202.3312095461688
Finished 16 epochs...
Completing Train Step...
At time: 158.43062615394592 and batch: 50, loss is 5.244935503005982 and perplexity is 189.6035859737984
At time: 158.87426662445068 and batch: 100, loss is 5.17647123336792 and perplexity is 177.05691478913877
At time: 159.31796216964722 and batch: 150, loss is 5.1828956794738765 and perplexity is 178.1980691131644
At time: 159.76219630241394 and batch: 200, loss is 5.195100183486939 and perplexity is 180.38621361167128
At time: 160.20644640922546 and batch: 250, loss is 5.244749393463135 and perplexity is 189.5683022205141
At time: 160.6496615409851 and batch: 300, loss is 5.1804606723785405 and perplexity is 177.7646834132638
At time: 161.0942792892456 and batch: 350, loss is 5.175442428588867 and perplexity is 176.87485145891003
At time: 161.53749132156372 and batch: 400, loss is 5.175984878540039 and perplexity is 176.97082324103837
At time: 161.9827139377594 and batch: 450, loss is 5.212024812698364 and perplexity is 183.46516495179128
At time: 162.4298484325409 and batch: 500, loss is 5.247782154083252 and perplexity is 190.1440901745032
At time: 162.874418258667 and batch: 550, loss is 5.17223237991333 and perplexity is 176.30798489764453
At time: 163.31753396987915 and batch: 600, loss is 5.120362348556519 and perplexity is 167.39601433733998
At time: 163.76144170761108 and batch: 650, loss is 5.098176164627075 and perplexity is 163.7230309768865
At time: 164.2049491405487 and batch: 700, loss is 5.196769351959229 and perplexity is 180.68756002126503
At time: 164.66481399536133 and batch: 750, loss is 5.121648368835449 and perplexity is 167.6114274895425
At time: 165.10904145240784 and batch: 800, loss is 5.20434063911438 and perplexity is 182.06078942065093
At time: 165.55309414863586 and batch: 850, loss is 5.14604076385498 and perplexity is 171.75014302530514
At time: 165.99615597724915 and batch: 900, loss is 5.184348354339599 and perplexity is 178.45712108287242
At time: 166.4396903514862 and batch: 950, loss is 5.142547960281372 and perplexity is 171.1512999414179
At time: 166.88397693634033 and batch: 1000, loss is 5.067940940856934 and perplexity is 158.84691518541618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.294973233850991 and perplexity of 199.33229053737574
Finished 17 epochs...
Completing Train Step...
At time: 168.27464604377747 and batch: 50, loss is 5.222286224365234 and perplexity is 185.35746878581568
At time: 168.73345971107483 and batch: 100, loss is 5.160148944854736 and perplexity is 174.19039843662875
At time: 169.17692351341248 and batch: 150, loss is 5.163669929504395 and perplexity is 174.80480117219625
At time: 169.62071633338928 and batch: 200, loss is 5.176069707870483 and perplexity is 176.9858361942425
At time: 170.06483840942383 and batch: 250, loss is 5.231469755172729 and perplexity is 187.06754506131352
At time: 170.5082426071167 and batch: 300, loss is 5.163592729568482 and perplexity is 174.79130677363847
At time: 170.95255064964294 and batch: 350, loss is 5.159624948501587 and perplexity is 174.09914721283647
At time: 171.39684414863586 and batch: 400, loss is 5.158758583068848 and perplexity is 173.948379049415
At time: 171.8405773639679 and batch: 450, loss is 5.19569803237915 and perplexity is 180.49408955319223
At time: 172.28342652320862 and batch: 500, loss is 5.235137271881103 and perplexity is 187.7548780405888
At time: 172.72739481925964 and batch: 550, loss is 5.153734865188599 and perplexity is 173.07670282831378
At time: 173.17049527168274 and batch: 600, loss is 5.105334148406983 and perplexity is 164.89916211908044
At time: 173.61390352249146 and batch: 650, loss is 5.080395135879517 and perplexity is 160.83759607927004
At time: 174.05963039398193 and batch: 700, loss is 5.179026432037354 and perplexity is 177.50990888070405
At time: 174.50638961791992 and batch: 750, loss is 5.104554586410522 and perplexity is 164.77066309203082
At time: 174.95321249961853 and batch: 800, loss is 5.187628231048584 and perplexity is 179.0433993725398
At time: 175.39685082435608 and batch: 850, loss is 5.130578784942627 and perplexity is 169.11497092301238
At time: 175.84212517738342 and batch: 900, loss is 5.162328720092773 and perplexity is 174.57050848057034
At time: 176.30121326446533 and batch: 950, loss is 5.1226887702941895 and perplexity is 167.7859014089263
At time: 176.74508953094482 and batch: 1000, loss is 5.047113637924195 and perplexity is 155.57277645536624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.277717776414825 and perplexity of 195.92222640264734
Finished 18 epochs...
Completing Train Step...
At time: 178.14264464378357 and batch: 50, loss is 5.200587539672852 and perplexity is 181.37877780281696
At time: 178.6019868850708 and batch: 100, loss is 5.132760772705078 and perplexity is 169.4843805971423
At time: 179.04763674736023 and batch: 150, loss is 5.14162389755249 and perplexity is 170.99321845399638
At time: 179.49189448356628 and batch: 200, loss is 5.146615200042724 and perplexity is 171.84883086492854
At time: 179.9364674091339 and batch: 250, loss is 5.196648283004761 and perplexity is 180.66568569146574
At time: 180.3815221786499 and batch: 300, loss is 5.130519065856934 and perplexity is 169.10487183312898
At time: 180.82775712013245 and batch: 350, loss is 5.124206399917602 and perplexity is 168.04073158329587
At time: 181.27240109443665 and batch: 400, loss is 5.124260969161988 and perplexity is 168.04990168924496
At time: 181.71766352653503 and batch: 450, loss is 5.16555115699768 and perplexity is 175.1339582827483
At time: 182.1628930568695 and batch: 500, loss is 5.203124370574951 and perplexity is 181.8394892177332
At time: 182.6080605983734 and batch: 550, loss is 5.12276520729065 and perplexity is 167.79872694944504
At time: 183.05296683311462 and batch: 600, loss is 5.069722385406494 and perplexity is 159.13014436031926
At time: 183.49801111221313 and batch: 650, loss is 5.045335111618042 and perplexity is 155.29633208458162
At time: 183.94151711463928 and batch: 700, loss is 5.142034597396851 and perplexity is 171.06345976526265
At time: 184.38518905639648 and batch: 750, loss is 5.0634397029876705 and perplexity is 158.13351423358424
At time: 184.83057022094727 and batch: 800, loss is 5.153533687591553 and perplexity is 173.04188717531625
At time: 185.27488374710083 and batch: 850, loss is 5.0917258548736575 and perplexity is 162.67036537273205
At time: 185.7187535762787 and batch: 900, loss is 5.1285988712310795 and perplexity is 168.78046912487446
At time: 186.16382932662964 and batch: 950, loss is 5.0881877136230464 and perplexity is 162.0958316328728
At time: 186.6068239212036 and batch: 1000, loss is 5.013397750854492 and perplexity is 150.4149413786694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.25665283203125 and perplexity of 191.83830044940086
Finished 19 epochs...
Completing Train Step...
At time: 188.01963257789612 and batch: 50, loss is 5.1708142185211186 and perplexity is 176.05812893024282
At time: 188.46307158470154 and batch: 100, loss is 5.106610593795776 and perplexity is 165.10978128751933
At time: 188.9091489315033 and batch: 150, loss is 5.110652103424072 and perplexity is 165.77842431180574
At time: 189.35339832305908 and batch: 200, loss is 5.1198955917358395 and perplexity is 167.31789933766615
At time: 189.7969205379486 and batch: 250, loss is 5.166881542205811 and perplexity is 175.3671089660109
At time: 190.24267196655273 and batch: 300, loss is 5.105126523971558 and perplexity is 164.86492857762627
At time: 190.68734908103943 and batch: 350, loss is 5.094296789169311 and perplexity is 163.08911825637765
At time: 191.13087224960327 and batch: 400, loss is 5.097199363708496 and perplexity is 163.56318425175937
At time: 191.57526993751526 and batch: 450, loss is 5.137118692398071 and perplexity is 170.22459163565833
At time: 192.01940989494324 and batch: 500, loss is 5.1747079372406 and perplexity is 176.74498610911377
At time: 192.46277904510498 and batch: 550, loss is 5.100094423294068 and perplexity is 164.03739551990114
At time: 192.9064393043518 and batch: 600, loss is 5.043861036300659 and perplexity is 155.0675822332514
At time: 193.3508026599884 and batch: 650, loss is 5.020927267074585 and perplexity is 151.55176762445902
At time: 193.7941644191742 and batch: 700, loss is 5.115424308776856 and perplexity is 166.5714437163919
At time: 194.23861289024353 and batch: 750, loss is 5.038112030029297 and perplexity is 154.1786554018027
At time: 194.68289065361023 and batch: 800, loss is 5.1244189643859865 and perplexity is 168.07645486868788
At time: 195.1266803741455 and batch: 850, loss is 5.064992218017578 and perplexity is 158.37920956466115
At time: 195.56997990608215 and batch: 900, loss is 5.1009768295288085 and perplexity is 164.1822070223285
At time: 196.01289582252502 and batch: 950, loss is 5.064489068984986 and perplexity is 158.29954126278113
At time: 196.45638275146484 and batch: 1000, loss is 4.985265951156617 and perplexity is 146.24246320698597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.237637031369093 and perplexity of 188.22480718860277
Finished 20 epochs...
Completing Train Step...
At time: 197.8489990234375 and batch: 50, loss is 5.14657338142395 and perplexity is 171.8416445344461
At time: 198.30761551856995 and batch: 100, loss is 5.077873220443726 and perplexity is 160.43248830155017
At time: 198.7516965866089 and batch: 150, loss is 5.086818389892578 and perplexity is 161.87402186335703
At time: 199.21152925491333 and batch: 200, loss is 5.096549758911133 and perplexity is 163.4569673258449
At time: 199.65521788597107 and batch: 250, loss is 5.143780994415283 and perplexity is 171.36246549672347
At time: 200.09853386878967 and batch: 300, loss is 5.078416786193848 and perplexity is 160.5197176126829
At time: 200.54303550720215 and batch: 350, loss is 5.072952461242676 and perplexity is 159.64497782236697
At time: 200.98808693885803 and batch: 400, loss is 5.069715557098388 and perplexity is 159.12905777437447
At time: 201.4343445301056 and batch: 450, loss is 5.1088535022735595 and perplexity is 165.48052303021066
At time: 201.87841415405273 and batch: 500, loss is 5.148133888244629 and perplexity is 172.1100139344976
At time: 202.3222963809967 and batch: 550, loss is 5.078712415695191 and perplexity is 160.56717899190275
At time: 202.76591181755066 and batch: 600, loss is 5.022589845657349 and perplexity is 151.80394392085802
At time: 203.2094213962555 and batch: 650, loss is 4.995186595916748 and perplexity is 147.70050312309638
At time: 203.65441250801086 and batch: 700, loss is 5.087025079727173 and perplexity is 161.90748303608422
At time: 204.09776401519775 and batch: 750, loss is 5.010854797363281 and perplexity is 150.03292910394458
At time: 204.54092717170715 and batch: 800, loss is 5.0940822410583495 and perplexity is 163.0541315474377
At time: 204.98557305335999 and batch: 850, loss is 5.035736045837402 and perplexity is 153.81276420182442
At time: 205.428386926651 and batch: 900, loss is 5.073668127059936 and perplexity is 159.75927116893416
At time: 205.87092757225037 and batch: 950, loss is 5.0334062576293945 and perplexity is 153.45483015470992
At time: 206.31409525871277 and batch: 1000, loss is 4.958742980957031 and perplexity is 142.4146653889542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.223240177805831 and perplexity of 185.53437554789554
Finished 21 epochs...
Completing Train Step...
At time: 207.7072262763977 and batch: 50, loss is 5.121463298797607 and perplexity is 167.58041050656
At time: 208.1657783985138 and batch: 100, loss is 5.050872068405152 and perplexity is 156.15858609321705
At time: 208.60922932624817 and batch: 150, loss is 5.06299017906189 and perplexity is 158.06244541023344
At time: 209.05657291412354 and batch: 200, loss is 5.068508729934693 and perplexity is 158.93713233862755
At time: 209.49993252754211 and batch: 250, loss is 5.116797857284546 and perplexity is 166.8003948760986
At time: 209.9434711933136 and batch: 300, loss is 5.054928703308105 and perplexity is 156.79335109840696
At time: 210.3879520893097 and batch: 350, loss is 5.046796522140503 and perplexity is 155.52344969400013
At time: 210.86013174057007 and batch: 400, loss is 5.040775718688965 and perplexity is 154.58988679080437
At time: 211.3040156364441 and batch: 450, loss is 5.088296527862549 and perplexity is 162.11347092720425
At time: 211.7489471435547 and batch: 500, loss is 5.1302605152130125 and perplexity is 169.06115531133727
At time: 212.19219756126404 and batch: 550, loss is 5.056278505325317 and perplexity is 157.00513398033053
At time: 212.63580560684204 and batch: 600, loss is 5.004066123962402 and perplexity is 149.01785395411582
At time: 213.07949113845825 and batch: 650, loss is 4.9749526691436765 and perplexity is 144.74197422446616
At time: 213.5244369506836 and batch: 700, loss is 5.069169492721557 and perplexity is 159.0421867853401
At time: 213.96870040893555 and batch: 750, loss is 4.988679218292236 and perplexity is 146.7424806616051
At time: 214.4122986793518 and batch: 800, loss is 5.081644287109375 and perplexity is 161.03863209627286
At time: 214.86102676391602 and batch: 850, loss is 5.015593996047974 and perplexity is 150.74565249912266
At time: 215.3056137561798 and batch: 900, loss is 5.0426067352294925 and perplexity is 154.87320272944723
At time: 215.74903416633606 and batch: 950, loss is 5.01481336593628 and perplexity is 150.62802182257246
At time: 216.1933970451355 and batch: 1000, loss is 4.938006496429443 and perplexity is 139.49189461142257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.210723504787538 and perplexity of 183.22657555408276
Finished 22 epochs...
Completing Train Step...
At time: 217.6037015914917 and batch: 50, loss is 5.10209023475647 and perplexity is 164.36511015367245
At time: 218.04810500144958 and batch: 100, loss is 5.029317197799682 and perplexity is 152.82862534005858
At time: 218.49446177482605 and batch: 150, loss is 5.043653430938721 and perplexity is 155.0353927131911
At time: 218.93972754478455 and batch: 200, loss is 5.052186136245727 and perplexity is 156.36392395362364
At time: 219.38553619384766 and batch: 250, loss is 5.097850570678711 and perplexity is 163.6697324261062
At time: 219.8298056125641 and batch: 300, loss is 5.039278163909912 and perplexity is 154.35855322762276
At time: 220.27534365653992 and batch: 350, loss is 5.032502784729004 and perplexity is 153.3162504851455
At time: 220.7200083732605 and batch: 400, loss is 5.0220007228851316 and perplexity is 151.7145390983761
At time: 221.16501379013062 and batch: 450, loss is 5.067921857833863 and perplexity is 158.84388393499174
At time: 221.60988473892212 and batch: 500, loss is 5.110407819747925 and perplexity is 165.73793229485975
At time: 222.0701858997345 and batch: 550, loss is 5.038015193939209 and perplexity is 154.16372606649946
At time: 222.51329231262207 and batch: 600, loss is 4.984536895751953 and perplexity is 146.13588320487767
At time: 222.95718097686768 and batch: 650, loss is 4.955986452102661 and perplexity is 142.02263582316382
At time: 223.4027121067047 and batch: 700, loss is 5.049453639984131 and perplexity is 155.9372433330576
At time: 223.84689688682556 and batch: 750, loss is 4.977854051589966 and perplexity is 145.16253585776786
At time: 224.2906038761139 and batch: 800, loss is 5.069874658584594 and perplexity is 159.15437745811118
At time: 224.73665070533752 and batch: 850, loss is 4.993770427703858 and perplexity is 147.49148240472726
At time: 225.18152618408203 and batch: 900, loss is 5.034783735275268 and perplexity is 153.66635640579398
At time: 225.62633323669434 and batch: 950, loss is 4.997656288146973 and perplexity is 148.0657287198316
At time: 226.07002592086792 and batch: 1000, loss is 4.922075967788697 and perplexity is 137.28732161892958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.205007041372904 and perplexity of 182.18215557675833
Finished 23 epochs...
Completing Train Step...
At time: 227.46288347244263 and batch: 50, loss is 5.089032707214355 and perplexity is 162.2328594574439
At time: 227.92630410194397 and batch: 100, loss is 5.01670018196106 and perplexity is 150.9124974801467
At time: 228.36969780921936 and batch: 150, loss is 5.030576868057251 and perplexity is 153.0212603167076
At time: 228.8148148059845 and batch: 200, loss is 5.038576316833496 and perplexity is 154.25025513711486
At time: 229.26053166389465 and batch: 250, loss is 5.087872772216797 and perplexity is 162.04478898185522
At time: 229.70519876480103 and batch: 300, loss is 5.023557319641113 and perplexity is 151.9508813548454
At time: 230.14997243881226 and batch: 350, loss is 5.020384168624878 and perplexity is 151.4694824409121
At time: 230.5942268371582 and batch: 400, loss is 5.009768238067627 and perplexity is 149.869997963375
At time: 231.03906655311584 and batch: 450, loss is 5.058673124313355 and perplexity is 157.38155196433385
At time: 231.48382878303528 and batch: 500, loss is 5.103827676773071 and perplexity is 164.65093323072716
At time: 231.92877840995789 and batch: 550, loss is 5.025350923538208 and perplexity is 152.2236656081311
At time: 232.37260603904724 and batch: 600, loss is 4.973722019195557 and perplexity is 144.5639570822567
At time: 232.81701612472534 and batch: 650, loss is 4.944254474639893 and perplexity is 140.36616529749696
At time: 233.2619822025299 and batch: 700, loss is 5.036777715682984 and perplexity is 153.9730697984167
At time: 233.72079634666443 and batch: 750, loss is 4.967993812561035 and perplexity is 143.73823208372323
At time: 234.16469407081604 and batch: 800, loss is 5.056041374206543 and perplexity is 156.9679075911985
At time: 234.61007356643677 and batch: 850, loss is 4.9843740558624265 and perplexity is 146.11208839122577
At time: 235.05437016487122 and batch: 900, loss is 5.022795486450195 and perplexity is 151.83516421421604
At time: 235.49800944328308 and batch: 950, loss is 4.987608222961426 and perplexity is 146.5854042790355
At time: 235.9410684108734 and batch: 1000, loss is 4.910942945480347 and perplexity is 135.76737530183027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.200504582102706 and perplexity of 181.36373168423776
Finished 24 epochs...
Completing Train Step...
At time: 237.33459568023682 and batch: 50, loss is 5.078671436309815 and perplexity is 160.56059918241533
At time: 237.79417896270752 and batch: 100, loss is 5.0041392803192135 and perplexity is 149.02875595618147
At time: 238.23793053627014 and batch: 150, loss is 5.021477708816528 and perplexity is 151.63521100667825
At time: 238.6818356513977 and batch: 200, loss is 5.035192279815674 and perplexity is 153.7291487826156
At time: 239.1334512233734 and batch: 250, loss is 5.076331357955933 and perplexity is 160.18531406932115
At time: 239.58088636398315 and batch: 300, loss is 5.018162927627563 and perplexity is 151.1334056086725
At time: 240.02548336982727 and batch: 350, loss is 5.009057979583741 and perplexity is 149.76358931913592
At time: 240.47917366027832 and batch: 400, loss is 4.999612302780151 and perplexity is 148.35563088588896
At time: 240.93441128730774 and batch: 450, loss is 5.0488436794281 and perplexity is 155.8421567678834
At time: 241.38941621780396 and batch: 500, loss is 5.091090316772461 and perplexity is 162.56701500262977
At time: 241.83487820625305 and batch: 550, loss is 5.0117862510681155 and perplexity is 150.17274293655313
At time: 242.28145623207092 and batch: 600, loss is 4.963940505981445 and perplexity is 143.1567961270224
At time: 242.7290050983429 and batch: 650, loss is 4.928915519714355 and perplexity is 138.22952383138121
At time: 243.1727135181427 and batch: 700, loss is 5.024825496673584 and perplexity is 152.14370421356105
At time: 243.616765499115 and batch: 750, loss is 4.955517892837524 and perplexity is 141.95610538923253
At time: 244.06142401695251 and batch: 800, loss is 5.044216032028198 and perplexity is 155.1226403345412
At time: 244.50500226020813 and batch: 850, loss is 4.977205486297607 and perplexity is 145.06841899902284
At time: 244.9645004272461 and batch: 900, loss is 5.011052713394165 and perplexity is 150.06262596442016
At time: 245.40826082229614 and batch: 950, loss is 4.97957064628601 and perplexity is 145.41193509434703
At time: 245.8511471748352 and batch: 1000, loss is 4.90111081123352 and perplexity is 134.43903316162877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.200602089486471 and perplexity of 181.38141684942715
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 247.2581763267517 and batch: 50, loss is 5.0418681716918945 and perplexity is 154.75886125837846
At time: 247.70375728607178 and batch: 100, loss is 4.935630178451538 and perplexity is 139.160811050194
At time: 248.1480004787445 and batch: 150, loss is 4.932765254974365 and perplexity is 138.76269653163254
At time: 248.591698884964 and batch: 200, loss is 4.933302707672119 and perplexity is 138.8372949619993
At time: 249.0380711555481 and batch: 250, loss is 4.981073026657104 and perplexity is 145.6305633216112
At time: 249.48228216171265 and batch: 300, loss is 4.910417385101319 and perplexity is 135.69604009572794
At time: 249.9261119365692 and batch: 350, loss is 4.889201307296753 and perplexity is 132.84742739673933
At time: 250.37143325805664 and batch: 400, loss is 4.887343330383301 and perplexity is 132.60082910153403
At time: 250.81564688682556 and batch: 450, loss is 4.928969688415528 and perplexity is 138.2370117479544
At time: 251.26077795028687 and batch: 500, loss is 4.967856340408325 and perplexity is 143.71847343769483
At time: 251.70944476127625 and batch: 550, loss is 4.881211652755737 and perplexity is 131.79025120515038
At time: 252.1564211845398 and batch: 600, loss is 4.826563777923584 and perplexity is 124.78144636342756
At time: 252.60140585899353 and batch: 650, loss is 4.793329858779908 and perplexity is 120.70262292114293
At time: 253.04723262786865 and batch: 700, loss is 4.882069826126099 and perplexity is 131.90339863231833
At time: 253.49483728408813 and batch: 750, loss is 4.80651777267456 and perplexity is 122.30498137817436
At time: 253.94287872314453 and batch: 800, loss is 4.892020664215088 and perplexity is 133.22250019384865
At time: 254.3910346031189 and batch: 850, loss is 4.826952667236328 and perplexity is 124.82998197122487
At time: 254.8387427330017 and batch: 900, loss is 4.8476137447357175 and perplexity is 127.4359320697536
At time: 255.28374528884888 and batch: 950, loss is 4.799180736541748 and perplexity is 121.41090924117843
At time: 255.7284014225006 and batch: 1000, loss is 4.729085502624511 and perplexity is 113.19200121464753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0778071705887955 and perplexity of 160.42189210891408
Finished 26 epochs...
Completing Train Step...
At time: 257.143390417099 and batch: 50, loss is 4.955691232681274 and perplexity is 141.98071417113962
At time: 257.61824202537537 and batch: 100, loss is 4.882872047424317 and perplexity is 132.00925680317098
At time: 258.0616707801819 and batch: 150, loss is 4.892379150390625 and perplexity is 133.27026717982918
At time: 258.50718784332275 and batch: 200, loss is 4.902584714889526 and perplexity is 134.63732944302453
At time: 258.9522383213043 and batch: 250, loss is 4.952896385192871 and perplexity is 141.58445373042372
At time: 259.39572739601135 and batch: 300, loss is 4.88335807800293 and perplexity is 132.07343293315475
At time: 259.8391156196594 and batch: 350, loss is 4.864061155319214 and perplexity is 129.54925487084523
At time: 260.2846839427948 and batch: 400, loss is 4.863792209625244 and perplexity is 129.51441784143526
At time: 260.72822976112366 and batch: 450, loss is 4.908305826187134 and perplexity is 135.4098122125133
At time: 261.17306685447693 and batch: 500, loss is 4.947443227767945 and perplexity is 140.81447273716606
At time: 261.61754274368286 and batch: 550, loss is 4.863116111755371 and perplexity is 129.42688301380443
At time: 262.0630314350128 and batch: 600, loss is 4.812942485809327 and perplexity is 123.09328540052248
At time: 262.5068030357361 and batch: 650, loss is 4.782486371994018 and perplexity is 119.40085622471068
At time: 262.9502463340759 and batch: 700, loss is 4.872199077606201 and perplexity is 130.6078180497563
At time: 263.3945028781891 and batch: 750, loss is 4.796947336196899 and perplexity is 121.14005265279746
At time: 263.83775448799133 and batch: 800, loss is 4.888475732803345 and perplexity is 132.75107165276768
At time: 264.2829613685608 and batch: 850, loss is 4.8240383529663085 and perplexity is 124.46671776357901
At time: 264.72755002975464 and batch: 900, loss is 4.848196620941162 and perplexity is 127.51023309432175
At time: 265.1715679168701 and batch: 950, loss is 4.802683038711548 and perplexity is 121.83687242252874
At time: 265.616046667099 and batch: 1000, loss is 4.7338111877441404 and perplexity is 113.72817687128267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.072079914372142 and perplexity of 159.50574085079737
Finished 27 epochs...
Completing Train Step...
At time: 267.0280239582062 and batch: 50, loss is 4.941522912979126 and perplexity is 139.98326965151384
At time: 267.4903655052185 and batch: 100, loss is 4.870542507171631 and perplexity is 130.3916361095944
At time: 267.9352343082428 and batch: 150, loss is 4.880788507461548 and perplexity is 131.73449657751905
At time: 268.39447474479675 and batch: 200, loss is 4.8935387992858885 and perplexity is 133.4249035425499
At time: 268.83908557891846 and batch: 250, loss is 4.943061103820801 and perplexity is 140.19875632220385
At time: 269.28219842910767 and batch: 300, loss is 4.8726284790039065 and perplexity is 130.6639132721997
At time: 269.7258720397949 and batch: 350, loss is 4.853573884963989 and perplexity is 128.1977360691952
At time: 270.17005681991577 and batch: 400, loss is 4.854025211334228 and perplexity is 128.2556081467032
At time: 270.61400413513184 and batch: 450, loss is 4.89908950805664 and perplexity is 134.16756556838396
At time: 271.0572922229767 and batch: 500, loss is 4.938225612640381 and perplexity is 139.5224628957074
At time: 271.5011086463928 and batch: 550, loss is 4.854977178573608 and perplexity is 128.37776141765661
At time: 271.9452295303345 and batch: 600, loss is 4.806786088943482 and perplexity is 122.33780219743123
At time: 272.3883738517761 and batch: 650, loss is 4.777887296676636 and perplexity is 118.85298351359818
At time: 272.8322591781616 and batch: 700, loss is 4.8675987434387205 and perplexity is 130.0083583564969
At time: 273.2761902809143 and batch: 750, loss is 4.792928295135498 and perplexity is 120.6541628665422
At time: 273.72028398513794 and batch: 800, loss is 4.886589412689209 and perplexity is 132.50089666537247
At time: 274.1641652584076 and batch: 850, loss is 4.822434711456299 and perplexity is 124.26727772626107
At time: 274.60925483703613 and batch: 900, loss is 4.849506549835205 and perplexity is 127.67737187900013
At time: 275.0540020465851 and batch: 950, loss is 4.803027153015137 and perplexity is 121.87880544749693
At time: 275.4980068206787 and batch: 1000, loss is 4.7337899589538575 and perplexity is 113.72576258529288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0701904296875 and perplexity of 159.2046417470007
Finished 28 epochs...
Completing Train Step...
At time: 276.9184911251068 and batch: 50, loss is 4.933294086456299 and perplexity is 138.83609802087506
At time: 277.36561250686646 and batch: 100, loss is 4.863000354766846 and perplexity is 129.41190181469852
At time: 277.80930352211 and batch: 150, loss is 4.872643756866455 and perplexity is 130.66590955275612
At time: 278.2532515525818 and batch: 200, loss is 4.887110242843628 and perplexity is 132.56992510232865
At time: 278.6975619792938 and batch: 250, loss is 4.937573261260987 and perplexity is 139.43147490587796
At time: 279.14070224761963 and batch: 300, loss is 4.867608165740966 and perplexity is 130.00958334031492
At time: 279.60080552101135 and batch: 350, loss is 4.847034454345703 and perplexity is 127.36213103714138
At time: 280.0440242290497 and batch: 400, loss is 4.84815821647644 and perplexity is 127.50533622610456
At time: 280.49027490615845 and batch: 450, loss is 4.893332166671753 and perplexity is 133.39733645416797
At time: 280.93386578559875 and batch: 500, loss is 4.932739486694336 and perplexity is 138.75912090167986
At time: 281.37839698791504 and batch: 550, loss is 4.850154275894165 and perplexity is 127.76009862914934
At time: 281.8216733932495 and batch: 600, loss is 4.803259420394897 and perplexity is 121.90711720610828
At time: 282.265545129776 and batch: 650, loss is 4.774321603775024 and perplexity is 118.42994493499192
At time: 282.7081067562103 and batch: 700, loss is 4.864417514801025 and perplexity is 129.59542920302195
At time: 283.15268063545227 and batch: 750, loss is 4.79001127243042 and perplexity is 120.30272475966642
At time: 283.5970160961151 and batch: 800, loss is 4.885115089416504 and perplexity is 132.30569144292434
At time: 284.042227268219 and batch: 850, loss is 4.821047830581665 and perplexity is 124.09505327044693
At time: 284.48665738105774 and batch: 900, loss is 4.848414678573608 and perplexity is 127.53804070558365
At time: 284.93122482299805 and batch: 950, loss is 4.801967926025391 and perplexity is 121.7497764748379
At time: 285.37489581108093 and batch: 1000, loss is 4.7324314785003665 and perplexity is 113.57137325095228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.067653749047256 and perplexity of 158.80130220253344
Finished 29 epochs...
Completing Train Step...
At time: 286.76989483833313 and batch: 50, loss is 4.926434755325317 and perplexity is 137.88703394514906
At time: 287.2299029827118 and batch: 100, loss is 4.857083826065064 and perplexity is 128.6484931747695
At time: 287.6751432418823 and batch: 150, loss is 4.86646222114563 and perplexity is 129.86068489192456
At time: 288.1196792125702 and batch: 200, loss is 4.881890153884887 and perplexity is 131.87970138199546
At time: 288.56408190727234 and batch: 250, loss is 4.932931823730469 and perplexity is 138.78581198649024
At time: 289.0092635154724 and batch: 300, loss is 4.862137413024902 and perplexity is 129.30027505334132
At time: 289.4529438018799 and batch: 350, loss is 4.8422150039672855 and perplexity is 126.74979231978726
At time: 289.8975872993469 and batch: 400, loss is 4.843523597717285 and perplexity is 126.91576487747193
At time: 290.343736410141 and batch: 450, loss is 4.889026193618775 and perplexity is 132.82416603186894
At time: 290.7880811691284 and batch: 500, loss is 4.929069709777832 and perplexity is 138.25083909369377
At time: 291.26030802726746 and batch: 550, loss is 4.846662807464599 and perplexity is 127.31480609299936
At time: 291.70595240592957 and batch: 600, loss is 4.799934635162353 and perplexity is 121.50247526959613
At time: 292.15048027038574 and batch: 650, loss is 4.771077632904053 and perplexity is 118.04638410980604
At time: 292.5950152873993 and batch: 700, loss is 4.861435985565185 and perplexity is 129.20961209033308
At time: 293.041051864624 and batch: 750, loss is 4.787370624542237 and perplexity is 119.98546669136503
At time: 293.48708963394165 and batch: 800, loss is 4.8841096210479735 and perplexity is 132.172729111113
At time: 293.93142008781433 and batch: 850, loss is 4.819756860733032 and perplexity is 123.93495366242891
At time: 294.37661576271057 and batch: 900, loss is 4.846222705841065 and perplexity is 127.25878696809593
At time: 294.8212945461273 and batch: 950, loss is 4.800350503921509 and perplexity is 121.5530148614111
At time: 295.2651045322418 and batch: 1000, loss is 4.730514583587646 and perplexity is 113.35387738829198
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.065416289538872 and perplexity of 158.44638792020945
Finished 30 epochs...
Completing Train Step...
At time: 296.6595392227173 and batch: 50, loss is 4.920625562667847 and perplexity is 137.0883437187904
At time: 297.11969470977783 and batch: 100, loss is 4.852233276367188 and perplexity is 128.025988231167
At time: 297.5655708312988 and batch: 150, loss is 4.861621007919312 and perplexity is 129.2335209687081
At time: 298.0096478462219 and batch: 200, loss is 4.877498884201049 and perplexity is 131.30185172281614
At time: 298.45427203178406 and batch: 250, loss is 4.929227600097656 and perplexity is 138.2726692862369
At time: 298.8993821144104 and batch: 300, loss is 4.857457809448242 and perplexity is 128.69661457122822
At time: 299.3431124687195 and batch: 350, loss is 4.8375550365448 and perplexity is 126.16051648622178
At time: 299.78743720054626 and batch: 400, loss is 4.83923749923706 and perplexity is 126.37295550869874
At time: 300.23202085494995 and batch: 450, loss is 4.885217027664185 and perplexity is 132.31917914071263
At time: 300.67601776123047 and batch: 500, loss is 4.925202941894531 and perplexity is 137.71728741429632
At time: 301.1221203804016 and batch: 550, loss is 4.843348026275635 and perplexity is 126.89348404966005
At time: 301.5681552886963 and batch: 600, loss is 4.797046661376953 and perplexity is 121.15208550791182
At time: 302.0119459629059 and batch: 650, loss is 4.76790189743042 and perplexity is 117.6720946569691
At time: 302.471638917923 and batch: 700, loss is 4.858401803970337 and perplexity is 128.81816083087122
At time: 302.91788482666016 and batch: 750, loss is 4.784507427215576 and perplexity is 119.64241596920941
At time: 303.36243295669556 and batch: 800, loss is 4.882195596694946 and perplexity is 131.91998924108051
At time: 303.8064239025116 and batch: 850, loss is 4.817916011810302 and perplexity is 123.7070179984192
At time: 304.2517318725586 and batch: 900, loss is 4.8432595157623295 and perplexity is 126.88225313928587
At time: 304.69542813301086 and batch: 950, loss is 4.797938747406006 and perplexity is 121.26021181259618
At time: 305.1415596008301 and batch: 1000, loss is 4.727268981933594 and perplexity is 112.98657224187315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.062495906178544 and perplexity of 157.984338734188
Finished 31 epochs...
Completing Train Step...
At time: 306.52666091918945 and batch: 50, loss is 4.915467433929443 and perplexity is 136.38304496801348
At time: 306.96923995018005 and batch: 100, loss is 4.847651510238648 and perplexity is 127.44074484269741
At time: 307.41331577301025 and batch: 150, loss is 4.857376499176025 and perplexity is 128.68615063988238
At time: 307.85673427581787 and batch: 200, loss is 4.873105516433716 and perplexity is 130.726259719168
At time: 308.29917335510254 and batch: 250, loss is 4.925595903396607 and perplexity is 137.77141564087853
At time: 308.74282479286194 and batch: 300, loss is 4.853146829605103 and perplexity is 128.14300022745888
At time: 309.18602299690247 and batch: 350, loss is 4.833460035324097 and perplexity is 125.6449453698506
At time: 309.6280736923218 and batch: 400, loss is 4.836108446121216 and perplexity is 125.97814583086293
At time: 310.07052850723267 and batch: 450, loss is 4.881830577850342 and perplexity is 131.87184474638596
At time: 310.516548871994 and batch: 500, loss is 4.9217843914031985 and perplexity is 137.24729771321364
At time: 310.9600443840027 and batch: 550, loss is 4.840803756713867 and perplexity is 126.57104318278557
At time: 311.40313601493835 and batch: 600, loss is 4.795158033370972 and perplexity is 120.92349021991832
At time: 311.8535542488098 and batch: 650, loss is 4.765323095321655 and perplexity is 117.36903254768065
At time: 312.29609179496765 and batch: 700, loss is 4.855689897537231 and perplexity is 128.46929129639207
At time: 312.74783539772034 and batch: 750, loss is 4.782244253158569 and perplexity is 119.37195052789797
At time: 313.2006928920746 and batch: 800, loss is 4.880366163253784 and perplexity is 131.67887102331898
At time: 313.6466362476349 and batch: 850, loss is 4.815842771530152 and perplexity is 123.45080930862781
At time: 314.10654950141907 and batch: 900, loss is 4.840508117675781 and perplexity is 126.53362937209349
At time: 314.5513563156128 and batch: 950, loss is 4.796006927490234 and perplexity is 121.02618504199498
At time: 314.9956924915314 and batch: 1000, loss is 4.724693241119385 and perplexity is 112.6959225960614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.060604560665968 and perplexity of 157.68581815583266
Finished 32 epochs...
Completing Train Step...
At time: 316.3920531272888 and batch: 50, loss is 4.911040439605713 and perplexity is 135.78061246860102
At time: 316.8511691093445 and batch: 100, loss is 4.842789640426636 and perplexity is 126.82264830251648
At time: 317.2967381477356 and batch: 150, loss is 4.853423833847046 and perplexity is 128.1785012988439
At time: 317.74267649650574 and batch: 200, loss is 4.869567985534668 and perplexity is 130.26462853479293
At time: 318.1866490840912 and batch: 250, loss is 4.922227764129639 and perplexity is 137.30816291378522
At time: 318.6319272518158 and batch: 300, loss is 4.849144363403321 and perplexity is 127.63113724053173
At time: 319.077351808548 and batch: 350, loss is 4.82939829826355 and perplexity is 125.13564366377115
At time: 319.5222098827362 and batch: 400, loss is 4.832450428009033 and perplexity is 125.51815732774145
At time: 319.9665832519531 and batch: 450, loss is 4.878487749099731 and perplexity is 131.43175573328926
At time: 320.41241431236267 and batch: 500, loss is 4.918398303985596 and perplexity is 136.78335228849332
At time: 320.8571865558624 and batch: 550, loss is 4.8374837875366214 and perplexity is 126.15152799476519
At time: 321.3012149333954 and batch: 600, loss is 4.791584329605103 and perplexity is 120.49211674714147
At time: 321.74658131599426 and batch: 650, loss is 4.762750358581543 and perplexity is 117.06746102390446
At time: 322.19082903862 and batch: 700, loss is 4.852654094696045 and perplexity is 128.07987525111233
At time: 322.63558864593506 and batch: 750, loss is 4.779719591140747 and perplexity is 119.07095681198369
At time: 323.08119463920593 and batch: 800, loss is 4.879111213684082 and perplexity is 131.5137243278507
At time: 323.5270562171936 and batch: 850, loss is 4.813762531280518 and perplexity is 123.19426889161743
At time: 323.9709665775299 and batch: 900, loss is 4.837224159240723 and perplexity is 126.11877973989765
At time: 324.41670656204224 and batch: 950, loss is 4.792727355957031 and perplexity is 120.62992115381365
At time: 324.86244678497314 and batch: 1000, loss is 4.720938377380371 and perplexity is 112.27355821935494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057112530964177 and perplexity of 157.13613490897558
Finished 33 epochs...
Completing Train Step...
At time: 326.2852952480316 and batch: 50, loss is 4.907083988189697 and perplexity is 135.24446439342083
At time: 326.7450177669525 and batch: 100, loss is 4.839144344329834 and perplexity is 126.36118379605732
At time: 327.1899878978729 and batch: 150, loss is 4.848539047241211 and perplexity is 127.55390342816725
At time: 327.63776779174805 and batch: 200, loss is 4.86381911277771 and perplexity is 129.51790223443533
At time: 328.0814690589905 and batch: 250, loss is 4.918196353912354 and perplexity is 136.75573166956724
At time: 328.5252068042755 and batch: 300, loss is 4.844393911361695 and perplexity is 127.02626947919174
At time: 328.9693853855133 and batch: 350, loss is 4.825270013809204 and perplexity is 124.62011299214886
At time: 329.4139573574066 and batch: 400, loss is 4.82935959815979 and perplexity is 125.1308009950838
At time: 329.8574845790863 and batch: 450, loss is 4.875059785842896 and perplexity is 130.98198384504408
At time: 330.30179047584534 and batch: 500, loss is 4.915248641967773 and perplexity is 136.35320871815128
At time: 330.7465064525604 and batch: 550, loss is 4.833869619369507 and perplexity is 125.69641807534035
At time: 331.1921627521515 and batch: 600, loss is 4.788037986755371 and perplexity is 120.06556718300793
At time: 331.63584756851196 and batch: 650, loss is 4.759020404815674 and perplexity is 116.63161814899117
At time: 332.08070969581604 and batch: 700, loss is 4.848437948226929 and perplexity is 127.54100850610578
At time: 332.5236027240753 and batch: 750, loss is 4.776900691986084 and perplexity is 118.73578042864126
At time: 332.96756196022034 and batch: 800, loss is 4.876301603317261 and perplexity is 131.1447405975838
At time: 333.41161155700684 and batch: 850, loss is 4.811118011474609 and perplexity is 122.86890960671185
At time: 333.85566997528076 and batch: 900, loss is 4.8355270195007325 and perplexity is 125.90492007304306
At time: 334.30135464668274 and batch: 950, loss is 4.790414419174194 and perplexity is 120.35123418897264
At time: 334.74617981910706 and batch: 1000, loss is 4.718279380798339 and perplexity is 111.97541976202643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.057147886694931 and perplexity of 157.14169067006668
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 336.15070700645447 and batch: 50, loss is 4.90372052192688 and perplexity is 134.79033834714232
At time: 336.59452414512634 and batch: 100, loss is 4.828408975601196 and perplexity is 125.01190535427568
At time: 337.0527925491333 and batch: 150, loss is 4.83865065574646 and perplexity is 126.29881611860749
At time: 337.4974846839905 and batch: 200, loss is 4.854358930587768 and perplexity is 128.29841665513064
At time: 337.9418272972107 and batch: 250, loss is 4.9059108638763425 and perplexity is 135.08589885072604
At time: 338.3847954273224 and batch: 300, loss is 4.826665668487549 and perplexity is 124.79416106311282
At time: 338.8288083076477 and batch: 350, loss is 4.806595573425293 and perplexity is 122.31449716770689
At time: 339.27396512031555 and batch: 400, loss is 4.810370798110962 and perplexity is 122.77713460749992
At time: 339.7186324596405 and batch: 450, loss is 4.856954965591431 and perplexity is 128.63191653706645
At time: 340.1628921031952 and batch: 500, loss is 4.892710084915161 and perplexity is 133.3143782108621
At time: 340.6098618507385 and batch: 550, loss is 4.813177757263183 and perplexity is 123.12224914377605
At time: 341.0533585548401 and batch: 600, loss is 4.761078338623047 and perplexity is 116.87188544128428
At time: 341.49684405326843 and batch: 650, loss is 4.729921741485596 and perplexity is 113.2866963531862
At time: 341.9412980079651 and batch: 700, loss is 4.817022323608398 and perplexity is 123.59651188230231
At time: 342.3859794139862 and batch: 750, loss is 4.747298631668091 and perplexity is 115.27247015365586
At time: 342.8292293548584 and batch: 800, loss is 4.841536235809326 and perplexity is 126.66378778860329
At time: 343.27456307411194 and batch: 850, loss is 4.780989608764648 and perplexity is 119.22227509373302
At time: 343.71863985061646 and batch: 900, loss is 4.795003976821899 and perplexity is 120.90486259920358
At time: 344.16224670410156 and batch: 950, loss is 4.7499986743927005 and perplexity is 115.58413130791794
At time: 344.6059138774872 and batch: 1000, loss is 4.676234846115112 and perplexity is 107.36506460111227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.039829161108994 and perplexity of 154.44362779387703
Finished 35 epochs...
Completing Train Step...
At time: 345.9981858730316 and batch: 50, loss is 4.890053548812866 and perplexity is 132.96069374796352
At time: 346.4576618671417 and batch: 100, loss is 4.818981876373291 and perplexity is 123.83894321981631
At time: 346.9012007713318 and batch: 150, loss is 4.830129079818725 and perplexity is 125.22712390602126
At time: 347.34658646583557 and batch: 200, loss is 4.847004556655884 and perplexity is 127.35832326057509
At time: 347.79107189178467 and batch: 250, loss is 4.899702625274658 and perplexity is 134.24985123573552
At time: 348.2489161491394 and batch: 300, loss is 4.819589262008667 and perplexity is 123.9141840628186
At time: 348.6925914287567 and batch: 350, loss is 4.800559329986572 and perplexity is 121.57840094974739
At time: 349.13807821273804 and batch: 400, loss is 4.806013317108154 and perplexity is 122.24329950870818
At time: 349.58223009109497 and batch: 450, loss is 4.853191623687744 and perplexity is 128.148740404163
At time: 350.02618885040283 and batch: 500, loss is 4.889314708709716 and perplexity is 132.86249333694795
At time: 350.4713866710663 and batch: 550, loss is 4.809402770996094 and perplexity is 122.65834051933987
At time: 350.91503167152405 and batch: 600, loss is 4.758125114440918 and perplexity is 116.52724571266056
At time: 351.35930371284485 and batch: 650, loss is 4.72758716583252 and perplexity is 113.02252846999846
At time: 351.80357933044434 and batch: 700, loss is 4.815435609817505 and perplexity is 123.40055509717173
At time: 352.2477009296417 and batch: 750, loss is 4.747159700393677 and perplexity is 115.25645631490985
At time: 352.6921727657318 and batch: 800, loss is 4.841680860519409 and perplexity is 126.68210782692336
At time: 353.1418311595917 and batch: 850, loss is 4.781281118392944 and perplexity is 119.25703460094954
At time: 353.5863513946533 and batch: 900, loss is 4.796025047302246 and perplexity is 121.02837803358467
At time: 354.03201818466187 and batch: 950, loss is 4.7526226997375485 and perplexity is 115.88782527400691
At time: 354.476628780365 and batch: 1000, loss is 4.678784818649292 and perplexity is 107.63919192701194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03856938059737 and perplexity of 154.24918522463332
Finished 36 epochs...
Completing Train Step...
At time: 355.87939405441284 and batch: 50, loss is 4.886211519241333 and perplexity is 132.45083490428644
At time: 356.3392856121063 and batch: 100, loss is 4.816026554107666 and perplexity is 123.47349950152602
At time: 356.7838981151581 and batch: 150, loss is 4.827578983306885 and perplexity is 124.90818948377519
At time: 357.2303388118744 and batch: 200, loss is 4.844372892379761 and perplexity is 127.02359954438822
At time: 357.6748540401459 and batch: 250, loss is 4.897234020233154 and perplexity is 133.91885009978395
At time: 358.11894822120667 and batch: 300, loss is 4.816649932861328 and perplexity is 123.55049425370846
At time: 358.5635461807251 and batch: 350, loss is 4.7978032684326175 and perplexity is 121.24378471637516
At time: 359.00799322128296 and batch: 400, loss is 4.804321765899658 and perplexity is 122.03669349928043
At time: 359.4519953727722 and batch: 450, loss is 4.8518186283111575 and perplexity is 127.97291350845133
At time: 359.9116973876953 and batch: 500, loss is 4.887955112457275 and perplexity is 132.68197673157425
At time: 360.35685873031616 and batch: 550, loss is 4.807577362060547 and perplexity is 122.43464312026758
At time: 360.80202984809875 and batch: 600, loss is 4.756788320541382 and perplexity is 116.37157687322156
At time: 361.247855424881 and batch: 650, loss is 4.726434984207153 and perplexity is 112.89238098061361
At time: 361.6931662559509 and batch: 700, loss is 4.814742937088012 and perplexity is 123.31510849453383
At time: 362.1378479003906 and batch: 750, loss is 4.747395782470703 and perplexity is 115.28366951065485
At time: 362.5831105709076 and batch: 800, loss is 4.841988582611084 and perplexity is 126.72109670868407
At time: 363.02922773361206 and batch: 850, loss is 4.781625452041626 and perplexity is 119.29810588151084
At time: 363.4742126464844 and batch: 900, loss is 4.796788682937622 and perplexity is 121.12083491313396
At time: 363.91803312301636 and batch: 950, loss is 4.754093866348267 and perplexity is 116.0584410444788
At time: 364.3639850616455 and batch: 1000, loss is 4.679656744003296 and perplexity is 107.73308619598586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0379087866806405 and perplexity of 154.14732279976675
Finished 37 epochs...
Completing Train Step...
At time: 365.7710690498352 and batch: 50, loss is 4.883452157974244 and perplexity is 132.08585898244843
At time: 366.21749329566956 and batch: 100, loss is 4.81394471168518 and perplexity is 123.21671451789076
At time: 366.66384077072144 and batch: 150, loss is 4.825963525772095 and perplexity is 124.70656850682744
At time: 367.10924100875854 and batch: 200, loss is 4.8426156425476075 and perplexity is 126.80058335038217
At time: 367.555202960968 and batch: 250, loss is 4.895559101104737 and perplexity is 133.69473459616177
At time: 367.99958539009094 and batch: 300, loss is 4.814653358459473 and perplexity is 123.3040625909825
At time: 368.4442615509033 and batch: 350, loss is 4.795852947235107 and perplexity is 121.00755083383206
At time: 368.88806915283203 and batch: 400, loss is 4.803288860321045 and perplexity is 121.91070619546522
At time: 369.33247995376587 and batch: 450, loss is 4.850951929092407 and perplexity is 127.8620475349585
At time: 369.77696466445923 and batch: 500, loss is 4.887015647888184 and perplexity is 132.5573852492832
At time: 370.2229232788086 and batch: 550, loss is 4.806309633255005 and perplexity is 122.27952753940704
At time: 370.6670820713043 and batch: 600, loss is 4.755828256607056 and perplexity is 116.25990633329128
At time: 371.1255943775177 and batch: 650, loss is 4.7255426692962645 and perplexity is 112.79169035629263
At time: 371.57112550735474 and batch: 700, loss is 4.814171333312988 and perplexity is 123.24464125458964
At time: 372.01474809646606 and batch: 750, loss is 4.747571392059326 and perplexity is 115.30391620613743
At time: 372.4585566520691 and batch: 800, loss is 4.8421080112457275 and perplexity is 126.73623174000465
At time: 372.90351939201355 and batch: 850, loss is 4.78176589012146 and perplexity is 119.3148610549333
At time: 373.3498446941376 and batch: 900, loss is 4.797154159545898 and perplexity is 121.16510983530092
At time: 373.79400420188904 and batch: 950, loss is 4.754944925308227 and perplexity is 116.15725566317775
At time: 374.24017095565796 and batch: 1000, loss is 4.679879140853882 and perplexity is 107.75704835951585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03738701052782 and perplexity of 154.06691338239128
Finished 38 epochs...
Completing Train Step...
At time: 375.6489951610565 and batch: 50, loss is 4.881243257522583 and perplexity is 131.79441647113313
At time: 376.1066093444824 and batch: 100, loss is 4.812256326675415 and perplexity is 123.00885278883828
At time: 376.55012464523315 and batch: 150, loss is 4.824745054244995 and perplexity is 124.55470964050033
At time: 376.99509382247925 and batch: 200, loss is 4.841237144470215 and perplexity is 126.62590941152631
At time: 377.439457654953 and batch: 250, loss is 4.894248332977295 and perplexity is 133.519606600418
At time: 377.8828475475311 and batch: 300, loss is 4.813071537017822 and perplexity is 123.10917176281617
At time: 378.3269236087799 and batch: 350, loss is 4.794293775558471 and perplexity is 120.81902629716402
At time: 378.77181673049927 and batch: 400, loss is 4.802556848526001 and perplexity is 121.82149877501338
At time: 379.2174291610718 and batch: 450, loss is 4.850308971405029 and perplexity is 127.77986407164803
At time: 379.66291332244873 and batch: 500, loss is 4.886283206939697 and perplexity is 132.46033034013604
At time: 380.10706543922424 and batch: 550, loss is 4.8052976703643795 and perplexity is 122.15584778546936
At time: 380.54936265945435 and batch: 600, loss is 4.755019931793213 and perplexity is 116.16596853738758
At time: 380.99328804016113 and batch: 650, loss is 4.724715452194214 and perplexity is 112.69842572142957
At time: 381.4376621246338 and batch: 700, loss is 4.813617467880249 and perplexity is 123.17639920822823
At time: 381.8818190097809 and batch: 750, loss is 4.747683601379395 and perplexity is 115.31685510609516
At time: 382.32584500312805 and batch: 800, loss is 4.842133932113647 and perplexity is 126.73951689570508
At time: 382.7858166694641 and batch: 850, loss is 4.781803798675537 and perplexity is 119.31938419452811
At time: 383.2293372154236 and batch: 900, loss is 4.797196025848389 and perplexity is 121.17018267663042
At time: 383.67314100265503 and batch: 950, loss is 4.755438928604126 and perplexity is 116.21465190611723
At time: 384.117600440979 and batch: 1000, loss is 4.6798013496398925 and perplexity is 107.74866613394401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036987676853087 and perplexity of 154.00540155840432
Finished 39 epochs...
Completing Train Step...
At time: 385.5103497505188 and batch: 50, loss is 4.879327344894409 and perplexity is 131.54215162016618
At time: 385.9698688983917 and batch: 100, loss is 4.810811281204224 and perplexity is 122.83122777222127
At time: 386.41473484039307 and batch: 150, loss is 4.823686304092408 and perplexity is 124.42290710795069
At time: 386.86018538475037 and batch: 200, loss is 4.84006251335144 and perplexity is 126.47725800026704
At time: 387.3049018383026 and batch: 250, loss is 4.893116445541382 and perplexity is 133.36856293359932
At time: 387.7498197555542 and batch: 300, loss is 4.811702356338501 and perplexity is 122.94072840440172
At time: 388.19466638565063 and batch: 350, loss is 4.79295093536377 and perplexity is 120.65689453525411
At time: 388.640718460083 and batch: 400, loss is 4.801921606063843 and perplexity is 121.74413716048052
At time: 389.08787536621094 and batch: 450, loss is 4.849724082946778 and perplexity is 127.70514895608989
At time: 389.53299713134766 and batch: 500, loss is 4.8856252002716065 and perplexity is 132.37319922908458
At time: 389.9785723686218 and batch: 550, loss is 4.804380693435669 and perplexity is 122.04388503281884
At time: 390.4232966899872 and batch: 600, loss is 4.754233560562134 and perplexity is 116.07465486962552
At time: 390.86785674095154 and batch: 650, loss is 4.723901195526123 and perplexity is 112.60669762697687
At time: 391.3137857913971 and batch: 700, loss is 4.81302695274353 and perplexity is 123.10368315208821
At time: 391.7592628002167 and batch: 750, loss is 4.747704219818115 and perplexity is 115.31923278411757
At time: 392.20509243011475 and batch: 800, loss is 4.842079648971557 and perplexity is 126.73263726322676
At time: 392.6518371105194 and batch: 850, loss is 4.78175256729126 and perplexity is 119.31327145388815
At time: 393.0972876548767 and batch: 900, loss is 4.797086086273193 and perplexity is 121.15686201046819
At time: 393.5419828891754 and batch: 950, loss is 4.755721626281738 and perplexity is 116.24751016257314
At time: 394.00205874443054 and batch: 1000, loss is 4.679570598602295 and perplexity is 107.72380588580818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.036573828720465 and perplexity of 153.94167989701043
Finished 40 epochs...
Completing Train Step...
At time: 395.55279088020325 and batch: 50, loss is 4.877642230987549 and perplexity is 131.3206747704015
At time: 395.9976975917816 and batch: 100, loss is 4.8095239162445065 and perplexity is 122.6732008945857
At time: 396.44199872016907 and batch: 150, loss is 4.82274167060852 and perplexity is 124.305428559555
At time: 396.88865542411804 and batch: 200, loss is 4.839009199142456 and perplexity is 126.34410784408827
At time: 397.33495807647705 and batch: 250, loss is 4.892076759338379 and perplexity is 133.22997353602926
At time: 397.78088188171387 and batch: 300, loss is 4.810503511428833 and perplexity is 122.79342984966632
At time: 398.2274765968323 and batch: 350, loss is 4.791766090393066 and perplexity is 120.51401947969352
At time: 398.6727843284607 and batch: 400, loss is 4.801333980560303 and perplexity is 121.67261821581447
At time: 399.1175467967987 and batch: 450, loss is 4.849206218719482 and perplexity is 127.63903214904632
At time: 399.56298542022705 and batch: 500, loss is 4.885026950836181 and perplexity is 132.2940307209992
At time: 400.0081219673157 and batch: 550, loss is 4.803559494018555 and perplexity is 121.94370380558017
At time: 400.4535160064697 and batch: 600, loss is 4.753524360656738 and perplexity is 115.99236391918937
At time: 400.89801263809204 and batch: 650, loss is 4.723136262893677 and perplexity is 112.52059402525578
At time: 401.3441345691681 and batch: 700, loss is 4.812480812072754 and perplexity is 123.0364695796999
At time: 401.7882933616638 and batch: 750, loss is 4.747665348052979 and perplexity is 115.31475020910842
At time: 402.2328577041626 and batch: 800, loss is 4.841940793991089 and perplexity is 126.71504102704566
At time: 402.6787281036377 and batch: 850, loss is 4.78165901184082 and perplexity is 119.30210956916953
At time: 403.123441696167 and batch: 900, loss is 4.796992273330688 and perplexity is 121.14549646186443
At time: 403.56795835494995 and batch: 950, loss is 4.755889139175415 and perplexity is 116.26698475045998
At time: 404.0143361091614 and batch: 1000, loss is 4.6792370796203615 and perplexity is 107.68788394239714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0362091064453125 and perplexity of 153.88554417487396
Finished 41 epochs...
Completing Train Step...
At time: 405.42530059814453 and batch: 50, loss is 4.876112031936645 and perplexity is 131.11988166439383
At time: 405.886376619339 and batch: 100, loss is 4.8083744144439695 and perplexity is 122.5322688456202
At time: 406.33139300346375 and batch: 150, loss is 4.821922750473022 and perplexity is 124.20367401126671
At time: 406.7763226032257 and batch: 200, loss is 4.838088445663452 and perplexity is 126.2278296072496
At time: 407.22228240966797 and batch: 250, loss is 4.891150531768798 and perplexity is 133.10662939263682
At time: 407.6673700809479 and batch: 300, loss is 4.8094094944000245 and perplexity is 122.65916520368097
At time: 408.11154103279114 and batch: 350, loss is 4.790689878463745 and perplexity is 120.38439062085281
At time: 408.55713725090027 and batch: 400, loss is 4.800803709030151 and perplexity is 121.60811579378607
At time: 409.0011339187622 and batch: 450, loss is 4.848752107620239 and perplexity is 127.58108300652103
At time: 409.44522285461426 and batch: 500, loss is 4.884498710632324 and perplexity is 132.22416614950438
At time: 409.89263892173767 and batch: 550, loss is 4.802804203033447 and perplexity is 121.85163559892521
At time: 410.3377332687378 and batch: 600, loss is 4.752883815765381 and perplexity is 115.91808939366804
At time: 410.7818765640259 and batch: 650, loss is 4.722427015304565 and perplexity is 112.44081735926618
At time: 411.22760105133057 and batch: 700, loss is 4.811965017318726 and perplexity is 122.9730243778943
At time: 411.67299008369446 and batch: 750, loss is 4.7475808906555175 and perplexity is 115.30501143667833
At time: 412.11780071258545 and batch: 800, loss is 4.841763801574707 and perplexity is 126.69261541038296
At time: 412.56240463256836 and batch: 850, loss is 4.781553468704224 and perplexity is 119.28951871477383
At time: 413.0074677467346 and batch: 900, loss is 4.796859722137452 and perplexity is 121.12943954595835
At time: 413.4518768787384 and batch: 950, loss is 4.755987396240235 and perplexity is 116.2784093643823
At time: 413.8957540988922 and batch: 1000, loss is 4.678863506317139 and perplexity is 107.64766213723982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035886811047066 and perplexity of 153.83595556364955
Finished 42 epochs...
Completing Train Step...
At time: 415.28919649124146 and batch: 50, loss is 4.874749937057495 and perplexity is 130.9414055233368
At time: 415.7504210472107 and batch: 100, loss is 4.807323961257935 and perplexity is 122.40362201398597
At time: 416.1945562362671 and batch: 150, loss is 4.821206426620483 and perplexity is 124.1147358150781
At time: 416.6393368244171 and batch: 200, loss is 4.837262363433838 and perplexity is 126.1235980981544
At time: 417.0856657028198 and batch: 250, loss is 4.890304412841797 and perplexity is 132.99405298742818
At time: 417.53133749961853 and batch: 300, loss is 4.808414402008057 and perplexity is 122.53716871053962
At time: 417.97693705558777 and batch: 350, loss is 4.789698209762573 and perplexity is 120.26506836241752
At time: 418.42658042907715 and batch: 400, loss is 4.800330400466919 and perplexity is 121.55057125045916
At time: 418.87149596214294 and batch: 450, loss is 4.848350296020508 and perplexity is 127.52982974522973
At time: 419.3162772655487 and batch: 500, loss is 4.883997793197632 and perplexity is 132.15794934535285
At time: 419.7624614238739 and batch: 550, loss is 4.802100343704224 and perplexity is 121.7658993650914
At time: 420.20758628845215 and batch: 600, loss is 4.752297639846802 and perplexity is 115.85016091210281
At time: 420.6520347595215 and batch: 650, loss is 4.72175106048584 and perplexity is 112.3648381291071
At time: 421.09777188301086 and batch: 700, loss is 4.811477289199829 and perplexity is 122.91306159999455
At time: 421.5436282157898 and batch: 750, loss is 4.747464799880982 and perplexity is 115.29162636554946
At time: 421.98990392684937 and batch: 800, loss is 4.841586027145386 and perplexity is 126.67009470484123
At time: 422.4356245994568 and batch: 850, loss is 4.781433115005493 and perplexity is 119.27516264389715
At time: 422.8830215930939 and batch: 900, loss is 4.7967056465148925 and perplexity is 121.11077788984014
At time: 423.3278467655182 and batch: 950, loss is 4.7560254669189455 and perplexity is 116.2828362466129
At time: 423.7736690044403 and batch: 1000, loss is 4.678468236923218 and perplexity is 107.6051207192839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.035569725967035 and perplexity of 153.7871842101109
Finished 43 epochs...
Completing Train Step...
At time: 425.1830620765686 and batch: 50, loss is 4.873519630432129 and perplexity is 130.78040650397483
At time: 425.6294639110565 and batch: 100, loss is 4.806351652145386 and perplexity is 122.2846656974197
At time: 426.08963799476624 and batch: 150, loss is 4.820556144714356 and perplexity is 124.03405248439203
At time: 426.5348753929138 and batch: 200, loss is 4.8364932823181155 and perplexity is 126.02663611120603
At time: 426.9824392795563 and batch: 250, loss is 4.889512338638306 and perplexity is 132.88875353683358
At time: 427.42795062065125 and batch: 300, loss is 4.807490339279175 and perplexity is 122.4239889806701
At time: 427.87370324134827 and batch: 350, loss is 4.78876148223877 and perplexity is 120.15246551005959
At time: 428.3196122646332 and batch: 400, loss is 4.799906539916992 and perplexity is 121.49906167569452
At time: 428.7651467323303 and batch: 450, loss is 4.84797004699707 and perplexity is 127.48134587056718
At time: 429.2104723453522 and batch: 500, loss is 4.883505373001099 and perplexity is 132.09288812200765
At time: 429.65656661987305 and batch: 550, loss is 4.801424236297607 and perplexity is 121.68360036327475
At time: 430.1025621891022 and batch: 600, loss is 4.7517370510101316 and perplexity is 115.78523480529468
At time: 430.54830026626587 and batch: 650, loss is 4.721087617874145 and perplexity is 112.29031523100268
At time: 430.9972765445709 and batch: 700, loss is 4.811010255813598 and perplexity is 122.85567049944667
At time: 431.4450206756592 and batch: 750, loss is 4.747321968078613 and perplexity is 115.27516023072951
At time: 431.89129090309143 and batch: 800, loss is 4.841389923095703 and perplexity is 126.64525662179989
At time: 432.33600330352783 and batch: 850, loss is 4.781284408569336 and perplexity is 119.25742697827484
At time: 432.78313660621643 and batch: 900, loss is 4.796530981063842 and perplexity is 121.08962586851042
At time: 433.2287812232971 and batch: 950, loss is 4.75600661277771 and perplexity is 116.2806438542629
At time: 433.6748468875885 and batch: 1000, loss is 4.678046503067017 and perplexity is 107.55974956472444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.03525078005907 and perplexity of 153.73814223829356
Finished 44 epochs...
Completing Train Step...
At time: 435.073970079422 and batch: 50, loss is 4.872384729385376 and perplexity is 130.6320678744934
At time: 435.5357882976532 and batch: 100, loss is 4.805430965423584 and perplexity is 122.17213164168679
At time: 435.98124599456787 and batch: 150, loss is 4.819941902160645 and perplexity is 123.95788888510343
At time: 436.4274353981018 and batch: 200, loss is 4.835751848220825 and perplexity is 125.93323029743297
At time: 436.8741202354431 and batch: 250, loss is 4.888757915496826 and perplexity is 132.78853699351592
At time: 437.33428263664246 and batch: 300, loss is 4.8066090965271 and perplexity is 122.31615125028873
At time: 437.77964901924133 and batch: 350, loss is 4.787872114181519 and perplexity is 120.04565324998157
At time: 438.2254400253296 and batch: 400, loss is 4.799507246017456 and perplexity is 121.45055752591811
At time: 438.6715774536133 and batch: 450, loss is 4.84759861946106 and perplexity is 127.43400458085672
At time: 439.11643719673157 and batch: 500, loss is 4.883000535964966 and perplexity is 132.026219569667
At time: 439.56300139427185 and batch: 550, loss is 4.800757942199707 and perplexity is 121.60255030312827
At time: 440.00884795188904 and batch: 600, loss is 4.751190233230591 and perplexity is 115.72193868759328
At time: 440.45340728759766 and batch: 650, loss is 4.720422191619873 and perplexity is 112.2156191622663
At time: 440.89914655685425 and batch: 700, loss is 4.810545330047607 and perplexity is 122.79856500866904
At time: 441.34601640701294 and batch: 750, loss is 4.747157802581787 and perplexity is 115.25623758004423
At time: 441.7910008430481 and batch: 800, loss is 4.841180009841919 and perplexity is 126.61867489393214
At time: 442.23659348487854 and batch: 850, loss is 4.781099395751953 and perplexity is 119.2353648666654
At time: 442.6829445362091 and batch: 900, loss is 4.796333351135254 and perplexity is 121.06569729897585
At time: 443.128249168396 and batch: 950, loss is 4.755923147201538 and perplexity is 116.27093882834944
At time: 443.57305216789246 and batch: 1000, loss is 4.677601795196534 and perplexity is 107.51192753175118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.034932206316692 and perplexity of 153.68917310352887
Finished 45 epochs...
Completing Train Step...
At time: 444.9686198234558 and batch: 50, loss is 4.871315011978149 and perplexity is 130.49240319173526
At time: 445.4288709163666 and batch: 100, loss is 4.804533290863037 and perplexity is 122.06251003672854
At time: 445.8731098175049 and batch: 150, loss is 4.819345874786377 and perplexity is 123.88402860363281
At time: 446.3186836242676 and batch: 200, loss is 4.835033349990844 and perplexity is 125.84277999245168
At time: 446.76478600502014 and batch: 250, loss is 4.888031759262085 and perplexity is 132.6921467708921
At time: 447.2100450992584 and batch: 300, loss is 4.805763063430786 and perplexity is 122.21271150101282
At time: 447.65495467185974 and batch: 350, loss is 4.787028093338012 and perplexity is 119.94437496297118
At time: 448.09951400756836 and batch: 400, loss is 4.799112710952759 and perplexity is 121.40265047347452
At time: 448.54462695121765 and batch: 450, loss is 4.847235088348389 and perplexity is 127.38768677487116
At time: 449.01770639419556 and batch: 500, loss is 4.882501802444458 and perplexity is 131.9603900854314
At time: 449.463663816452 and batch: 550, loss is 4.80009973526001 and perplexity is 121.52253699617957
At time: 449.91022205352783 and batch: 600, loss is 4.750651044845581 and perplexity is 115.65955958091828
At time: 450.3548331260681 and batch: 650, loss is 4.719766864776611 and perplexity is 112.14210534521573
At time: 450.7997717857361 and batch: 700, loss is 4.810091161727906 and perplexity is 122.74280645358034
At time: 451.2462103366852 and batch: 750, loss is 4.746979761123657 and perplexity is 115.23571901807841
At time: 451.6908061504364 and batch: 800, loss is 4.8409482192993165 and perplexity is 126.58932928372371
At time: 452.1343767642975 and batch: 850, loss is 4.780893468856812 and perplexity is 119.21081362615449
At time: 452.58007550239563 and batch: 900, loss is 4.796097717285156 and perplexity is 121.03717348332675
At time: 453.024160861969 and batch: 950, loss is 4.75579792022705 and perplexity is 116.25637948208959
At time: 453.4678189754486 and batch: 1000, loss is 4.6771648979187015 and perplexity is 107.46496612268096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.034625914038681 and perplexity of 153.6421065050292
Finished 46 epochs...
Completing Train Step...
At time: 454.88236117362976 and batch: 50, loss is 4.870267190933228 and perplexity is 130.35574211614775
At time: 455.33138608932495 and batch: 100, loss is 4.803649244308471 and perplexity is 121.95464877950015
At time: 455.77767419815063 and batch: 150, loss is 4.8187840461730955 and perplexity is 123.81444656005655
At time: 456.22243475914 and batch: 200, loss is 4.834366397857666 and perplexity is 125.75887686468582
At time: 456.6691734790802 and batch: 250, loss is 4.887317123413086 and perplexity is 132.59735408109046
At time: 457.1170029640198 and batch: 300, loss is 4.8049494171142575 and perplexity is 122.11331402114737
At time: 457.5645225048065 and batch: 350, loss is 4.786221561431884 and perplexity is 119.84767499864815
At time: 458.0116639137268 and batch: 400, loss is 4.798705091476441 and perplexity is 121.35317447304062
At time: 458.45947885513306 and batch: 450, loss is 4.846886014938354 and perplexity is 127.3432268809871
At time: 458.90522623062134 and batch: 500, loss is 4.882034330368042 and perplexity is 131.89871670428843
At time: 459.35123109817505 and batch: 550, loss is 4.799451580047608 and perplexity is 121.4437970510107
At time: 459.79804277420044 and batch: 600, loss is 4.750111608505249 and perplexity is 115.59718543632624
At time: 460.2572066783905 and batch: 650, loss is 4.719133501052856 and perplexity is 112.07110109191733
At time: 460.70287704467773 and batch: 700, loss is 4.809646081924439 and perplexity is 122.68818826503495
At time: 461.1501021385193 and batch: 750, loss is 4.746784858703613 and perplexity is 115.21326148615103
At time: 461.5959360599518 and batch: 800, loss is 4.840692377090454 and perplexity is 126.55694653271523
At time: 462.0409080982208 and batch: 850, loss is 4.780673446655274 and perplexity is 119.1845874857657
At time: 462.48823642730713 and batch: 900, loss is 4.795828800201416 and perplexity is 121.00462889570345
At time: 462.9337110519409 and batch: 950, loss is 4.755648136138916 and perplexity is 116.23896743035576
At time: 463.3783152103424 and batch: 1000, loss is 4.676725149154663 and perplexity is 107.41771892586067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.034330414562691 and perplexity of 153.59671205041596
Finished 47 epochs...
Completing Train Step...
At time: 464.77644395828247 and batch: 50, loss is 4.869255514144897 and perplexity is 130.22393092402731
At time: 465.2378671169281 and batch: 100, loss is 4.802781343460083 and perplexity is 121.84885015435887
At time: 465.6826982498169 and batch: 150, loss is 4.818243579864502 and perplexity is 123.7475471032534
At time: 466.1274564266205 and batch: 200, loss is 4.833718776702881 and perplexity is 125.677459122398
At time: 466.5730686187744 and batch: 250, loss is 4.8866001224517825 and perplexity is 132.50231572611546
At time: 467.01768374443054 and batch: 300, loss is 4.804158020019531 and perplexity is 122.01671212947372
At time: 467.46271300315857 and batch: 350, loss is 4.785431747436523 and perplexity is 119.75305499864156
At time: 467.9069540500641 and batch: 400, loss is 4.798304586410523 and perplexity is 121.30458164338572
At time: 468.352952003479 and batch: 450, loss is 4.846534738540649 and perplexity is 127.29850206682194
At time: 468.79752802848816 and batch: 500, loss is 4.881572208404541 and perplexity is 131.83777749210043
At time: 469.2429687976837 and batch: 550, loss is 4.7988080978393555 and perplexity is 121.36567526599082
At time: 469.689040184021 and batch: 600, loss is 4.749574766159058 and perplexity is 115.53514462664042
At time: 470.1347849369049 and batch: 650, loss is 4.718501749038697 and perplexity is 112.0003223077472
At time: 470.5809667110443 and batch: 700, loss is 4.809195985794068 and perplexity is 122.63297921187727
At time: 471.02712297439575 and batch: 750, loss is 4.746574106216431 and perplexity is 115.18898256324805
At time: 471.47200059890747 and batch: 800, loss is 4.8404203796386716 and perplexity is 126.52252804683542
At time: 471.93150329589844 and batch: 850, loss is 4.7804293537139895 and perplexity is 119.1554989195518
At time: 472.37638783454895 and batch: 900, loss is 4.795527296066284 and perplexity is 120.96815099911592
At time: 472.8221261501312 and batch: 950, loss is 4.755467023849487 and perplexity is 116.21791703114414
At time: 473.2662823200226 and batch: 1000, loss is 4.676268129348755 and perplexity is 107.36863811711123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.034048685213414 and perplexity of 153.55344544370166
Finished 48 epochs...
Completing Train Step...
At time: 474.662070274353 and batch: 50, loss is 4.868278217315674 and perplexity is 130.09672565802242
At time: 475.1217887401581 and batch: 100, loss is 4.801922578811645 and perplexity is 121.74425558687999
At time: 475.56764912605286 and batch: 150, loss is 4.817713794708252 and perplexity is 123.68200485286435
At time: 476.0125186443329 and batch: 200, loss is 4.833068103790283 and perplexity is 125.59571080261578
At time: 476.4585540294647 and batch: 250, loss is 4.885869407653809 and perplexity is 132.4055296890499
At time: 476.904265165329 and batch: 300, loss is 4.803383617401123 and perplexity is 121.92225864534417
At time: 477.3487854003906 and batch: 350, loss is 4.784653568267823 and perplexity is 119.6599019154486
At time: 477.79331731796265 and batch: 400, loss is 4.797912607192993 and perplexity is 121.25704208625827
At time: 478.2391531467438 and batch: 450, loss is 4.846168279647827 and perplexity is 127.25186094524547
At time: 478.684743642807 and batch: 500, loss is 4.881104230880737 and perplexity is 131.77609480962613
At time: 479.12904691696167 and batch: 550, loss is 4.798166227340698 and perplexity is 121.28779921533145
At time: 479.57477855682373 and batch: 600, loss is 4.749044284820557 and perplexity is 115.47387164199934
At time: 480.0192732810974 and batch: 650, loss is 4.717864818572998 and perplexity is 111.92900860364762
At time: 480.4641945362091 and batch: 700, loss is 4.808738565444946 and perplexity is 122.57689721921253
At time: 480.9091203212738 and batch: 750, loss is 4.746348333358765 and perplexity is 115.16297895304822
At time: 481.35490703582764 and batch: 800, loss is 4.840130233764649 and perplexity is 126.48582338246266
At time: 481.7998719215393 and batch: 850, loss is 4.780156764984131 and perplexity is 119.12302289994524
At time: 482.24511551856995 and batch: 900, loss is 4.7951874160766605 and perplexity is 120.92704333144209
At time: 482.69085788726807 and batch: 950, loss is 4.755253772735596 and perplexity is 116.19313607326049
At time: 483.15023469924927 and batch: 1000, loss is 4.675788612365722 and perplexity is 107.31716537370258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.033770677520008 and perplexity of 153.51076233791002
Finished 49 epochs...
Completing Train Step...
At time: 484.5631866455078 and batch: 50, loss is 4.867327108383178 and perplexity is 129.97304832479213
At time: 485.0081617832184 and batch: 100, loss is 4.801068439483642 and perplexity is 121.64031342708735
At time: 485.45587491989136 and batch: 150, loss is 4.8171874523162845 and perplexity is 123.61692289978475
At time: 485.90014123916626 and batch: 200, loss is 4.832402305603027 and perplexity is 125.51211723734643
At time: 486.34490275382996 and batch: 250, loss is 4.885125427246094 and perplexity is 132.30705920368604
At time: 486.79144859313965 and batch: 300, loss is 4.802623109817505 and perplexity is 121.82957109229334
At time: 487.2373013496399 and batch: 350, loss is 4.783888502120972 and perplexity is 119.56838918646679
At time: 487.6819429397583 and batch: 400, loss is 4.797531719207764 and perplexity is 121.2108655304091
At time: 488.1280446052551 and batch: 450, loss is 4.845783786773682 and perplexity is 127.20294291640968
At time: 488.5738482475281 and batch: 500, loss is 4.880626773834228 and perplexity is 131.71319240239157
At time: 489.0205807685852 and batch: 550, loss is 4.797531089782715 and perplexity is 121.21078923727814
At time: 489.46520161628723 and batch: 600, loss is 4.748512716293335 and perplexity is 115.41250567767058
At time: 489.910413980484 and batch: 650, loss is 4.717225637435913 and perplexity is 111.85748855222437
At time: 490.35479521751404 and batch: 700, loss is 4.808273458480835 and perplexity is 122.51989910681382
At time: 490.7998402118683 and batch: 750, loss is 4.746106357574463 and perplexity is 115.13511567214906
At time: 491.2457582950592 and batch: 800, loss is 4.839820117950439 and perplexity is 126.44660420991556
At time: 491.6903932094574 and batch: 850, loss is 4.779854984283447 and perplexity is 119.0870792944429
At time: 492.13548707962036 and batch: 900, loss is 4.79480134010315 and perplexity is 120.88036531667392
At time: 492.5818386077881 and batch: 950, loss is 4.755013942718506 and perplexity is 116.16527281280695
At time: 493.0274426937103 and batch: 1000, loss is 4.67529257774353 and perplexity is 107.26394554465182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.033481877024581 and perplexity of 153.46643475491558
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
SETTINGS FOR THIS RUN
{'anneal': 6.949597665762493, 'dropout': 0.6744407721634755, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 12.605229715772824}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7280550003051758 and batch: 50, loss is 6.6900187110900875 and perplexity is 804.3373020309004
At time: 1.192922830581665 and batch: 100, loss is 6.0767457389831545 and perplexity is 435.609299255106
At time: 1.6448380947113037 and batch: 150, loss is 5.874568862915039 and perplexity is 355.8711983204356
At time: 2.09539794921875 and batch: 200, loss is 5.839987287521362 and perplexity is 343.77497040522064
At time: 2.5455400943756104 and batch: 250, loss is 5.902767820358276 and perplexity is 366.0492255362512
At time: 3.0117344856262207 and batch: 300, loss is 5.814419803619384 and perplexity is 335.09692006854175
At time: 3.4621291160583496 and batch: 350, loss is 5.806373252868652 and perplexity is 332.41136491283953
At time: 3.91225004196167 and batch: 400, loss is 5.793658971786499 and perplexity is 328.2117475156718
At time: 4.36416220664978 and batch: 450, loss is 5.835951204299927 and perplexity is 342.39026229354386
At time: 4.814923286437988 and batch: 500, loss is 5.850901823043824 and perplexity is 347.54766568733584
At time: 5.265476703643799 and batch: 550, loss is 5.819367694854736 and perplexity is 336.75905181491936
At time: 5.716835260391235 and batch: 600, loss is 5.751088829040527 and perplexity is 314.5329465126513
At time: 6.168260812759399 and batch: 650, loss is 5.742240657806397 and perplexity is 311.7621813224991
At time: 6.618648529052734 and batch: 700, loss is 5.839145708084106 and perplexity is 343.48577816527416
At time: 7.0695648193359375 and batch: 750, loss is 5.740602245330811 and perplexity is 311.2518044932576
At time: 7.5210041999816895 and batch: 800, loss is 5.8455400466918945 and perplexity is 345.6891796778641
At time: 7.971221208572388 and batch: 850, loss is 5.831959543228149 and perplexity is 341.0262804997723
At time: 8.424155950546265 and batch: 900, loss is 5.849578914642334 and perplexity is 347.08819594567984
At time: 8.876410245895386 and batch: 950, loss is 5.82922215461731 and perplexity is 340.09403558397514
At time: 9.327869176864624 and batch: 1000, loss is 5.766833429336548 and perplexity is 319.52433261482685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.777879668445122 and perplexity of 323.0734408547537
Finished 1 epochs...
Completing Train Step...
At time: 10.745793342590332 and batch: 50, loss is 5.6159604549407955 and perplexity is 274.7771635829748
At time: 11.190110445022583 and batch: 100, loss is 5.504641084671021 and perplexity is 245.830207613139
At time: 11.636709690093994 and batch: 150, loss is 5.434716243743896 and perplexity is 229.22779422575363
At time: 12.08238410949707 and batch: 200, loss is 5.4276664924621585 and perplexity is 227.61747812285782
At time: 12.526880741119385 and batch: 250, loss is 5.47167028427124 and perplexity is 237.85715020978512
At time: 12.97542119026184 and batch: 300, loss is 5.384922714233398 and perplexity is 218.093247952249
At time: 13.421125411987305 and batch: 350, loss is 5.3248687267303465 and perplexity is 205.3813976074807
At time: 13.86486005783081 and batch: 400, loss is 5.359416580200195 and perplexity is 212.60087470035245
At time: 14.325008153915405 and batch: 450, loss is 5.3904065418243405 and perplexity is 219.2925190152316
At time: 14.769546747207642 and batch: 500, loss is 5.4144497489929195 and perplexity is 224.62890938262987
At time: 15.213565587997437 and batch: 550, loss is 5.327917423248291 and perplexity is 206.00849859379545
At time: 15.658385515213013 and batch: 600, loss is 5.251490621566773 and perplexity is 190.8505424681416
At time: 16.104081869125366 and batch: 650, loss is 5.217545671463013 and perplexity is 184.48085136651332
At time: 16.549360990524292 and batch: 700, loss is 5.2889713859558105 and perplexity is 198.1395114687832
At time: 16.993936777114868 and batch: 750, loss is 5.228256797790527 and perplexity is 186.46746953620206
At time: 17.438287019729614 and batch: 800, loss is 5.325252208709717 and perplexity is 205.46017277582445
At time: 17.88228440284729 and batch: 850, loss is 5.221172847747803 and perplexity is 185.1512109568318
At time: 18.327604055404663 and batch: 900, loss is 5.256368970870971 and perplexity is 191.7838527350107
At time: 18.772566556930542 and batch: 950, loss is 5.230364799499512 and perplexity is 186.86095787198553
At time: 19.2173752784729 and batch: 1000, loss is 5.10624529838562 and perplexity is 165.04947845708725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.358408485970846 and perplexity of 212.38666097736268
Finished 2 epochs...
Completing Train Step...
At time: 20.627718448638916 and batch: 50, loss is 5.29648678779602 and perplexity is 199.6342191470702
At time: 21.08495044708252 and batch: 100, loss is 5.2024619674682615 and perplexity is 181.71907805992186
At time: 21.5296573638916 and batch: 150, loss is 5.19334566116333 and perplexity is 180.06999945653362
At time: 21.974623203277588 and batch: 200, loss is 5.210018301010132 and perplexity is 183.09740903059406
At time: 22.419143438339233 and batch: 250, loss is 5.256975889205933 and perplexity is 191.90028520051015
At time: 22.863973140716553 and batch: 300, loss is 5.196013193130494 and perplexity is 180.55098317091557
At time: 23.308777570724487 and batch: 350, loss is 5.169252672195435 and perplexity is 175.78342054665987
At time: 23.751532554626465 and batch: 400, loss is 5.186822528839111 and perplexity is 178.89920180802258
At time: 24.195087432861328 and batch: 450, loss is 5.218022203445434 and perplexity is 184.5687833418691
At time: 24.639668226242065 and batch: 500, loss is 5.248676366806031 and perplexity is 190.314195482929
At time: 25.083287239074707 and batch: 550, loss is 5.163271503448486 and perplexity is 174.7351682574163
At time: 25.526540756225586 and batch: 600, loss is 5.113257150650025 and perplexity is 166.21084793361499
At time: 25.986042499542236 and batch: 650, loss is 5.076397733688355 and perplexity is 160.19594683974074
At time: 26.429630517959595 and batch: 700, loss is 5.161554136276245 and perplexity is 174.43534134575387
At time: 26.873266458511353 and batch: 750, loss is 5.105555438995362 and perplexity is 164.93565678950358
At time: 27.318828582763672 and batch: 800, loss is 5.211750936508179 and perplexity is 183.41492509144737
At time: 27.762858390808105 and batch: 850, loss is 5.104674625396728 and perplexity is 164.79044318255217
At time: 28.206833600997925 and batch: 900, loss is 5.136405553817749 and perplexity is 170.10324118702954
At time: 28.650824546813965 and batch: 950, loss is 5.117521152496338 and perplexity is 166.92108444486956
At time: 29.094930410385132 and batch: 1000, loss is 5.03302885055542 and perplexity is 153.39692614365293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.297943115234375 and perplexity of 199.9251637419251
Finished 3 epochs...
Completing Train Step...
At time: 30.493452787399292 and batch: 50, loss is 5.210202445983887 and perplexity is 183.13112860272415
At time: 30.953105211257935 and batch: 100, loss is 5.107400064468384 and perplexity is 165.24018208463616
At time: 31.397616624832153 and batch: 150, loss is 5.1105351161956785 and perplexity is 165.75903148779656
At time: 31.843623638153076 and batch: 200, loss is 5.136361303329468 and perplexity is 170.0957142020865
At time: 32.28792142868042 and batch: 250, loss is 5.176595640182495 and perplexity is 177.07894324612292
At time: 32.73293876647949 and batch: 300, loss is 5.107588233947754 and perplexity is 165.27127816924542
At time: 33.1784131526947 and batch: 350, loss is 5.096970176696777 and perplexity is 163.52570198972134
At time: 33.62354493141174 and batch: 400, loss is 5.110216445922852 and perplexity is 165.7062174275913
At time: 34.0676052570343 and batch: 450, loss is 5.150663166046143 and perplexity is 172.5458789516553
At time: 34.51354622840881 and batch: 500, loss is 5.174577054977417 and perplexity is 176.7218548390949
At time: 34.96176290512085 and batch: 550, loss is 5.092736043930054 and perplexity is 162.8347762246717
At time: 35.410146951675415 and batch: 600, loss is 5.04712287902832 and perplexity is 155.57421412623538
At time: 35.85796070098877 and batch: 650, loss is 5.019836702346802 and perplexity is 151.38658070214188
At time: 36.303476333618164 and batch: 700, loss is 5.1081274890899655 and perplexity is 165.36042559030028
At time: 36.75079369544983 and batch: 750, loss is 5.04371901512146 and perplexity is 155.04556091615078
At time: 37.21120309829712 and batch: 800, loss is 5.155349702835083 and perplexity is 173.35641939140368
At time: 37.65592408180237 and batch: 850, loss is 5.032068176269531 and perplexity is 153.24963242314533
At time: 38.100830078125 and batch: 900, loss is 5.082852611541748 and perplexity is 161.23333661941703
At time: 38.54665017127991 and batch: 950, loss is 5.046514873504639 and perplexity is 155.47965289449286
At time: 38.991437673568726 and batch: 1000, loss is 4.9680941200256346 and perplexity is 143.7526508244911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.266818535037157 and perplexity of 193.79841774195822
Finished 4 epochs...
Completing Train Step...
At time: 40.39839220046997 and batch: 50, loss is 5.153201045989991 and perplexity is 172.9843358173475
At time: 40.84050703048706 and batch: 100, loss is 5.058171215057373 and perplexity is 157.30258052658783
At time: 41.28656339645386 and batch: 150, loss is 5.06845763206482 and perplexity is 158.9290111972096
At time: 41.73040461540222 and batch: 200, loss is 5.095572395324707 and perplexity is 163.29728848288732
At time: 42.174460649490356 and batch: 250, loss is 5.133405618667602 and perplexity is 169.59370716128964
At time: 42.61847472190857 and batch: 300, loss is 5.056773519515991 and perplexity is 157.0828729890282
At time: 43.06261110305786 and batch: 350, loss is 5.047472009658813 and perplexity is 155.62853933244665
At time: 43.50473690032959 and batch: 400, loss is 5.069408111572265 and perplexity is 159.0801417773731
At time: 43.94905471801758 and batch: 450, loss is 5.0972652244567875 and perplexity is 163.57395700021388
At time: 44.392868518829346 and batch: 500, loss is 5.12354546546936 and perplexity is 167.92970436998135
At time: 44.83612275123596 and batch: 550, loss is 5.03821361541748 and perplexity is 154.1943184959185
At time: 45.27993726730347 and batch: 600, loss is 5.009042539596558 and perplexity is 149.76127698908758
At time: 45.72564697265625 and batch: 650, loss is 4.971779613494873 and perplexity is 144.28342776697906
At time: 46.17041015625 and batch: 700, loss is 5.064741802215576 and perplexity is 158.33955387329067
At time: 46.61449861526489 and batch: 750, loss is 4.997332286834717 and perplexity is 148.01776300032387
At time: 47.05958867073059 and batch: 800, loss is 5.118866033554077 and perplexity is 167.1457244726769
At time: 47.502668142318726 and batch: 850, loss is 5.001722450256348 and perplexity is 148.66901367164573
At time: 47.94714975357056 and batch: 900, loss is 5.050290851593018 and perplexity is 156.0678504687101
At time: 48.39412307739258 and batch: 950, loss is 5.0096197509765625 and perplexity is 149.84774585545583
At time: 48.85260200500488 and batch: 1000, loss is 4.94328706741333 and perplexity is 140.23043971636244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.256605567001715 and perplexity of 191.8292334207426
Finished 5 epochs...
Completing Train Step...
At time: 50.24765920639038 and batch: 50, loss is 5.120397329330444 and perplexity is 167.40187008189218
At time: 50.705873012542725 and batch: 100, loss is 5.021534976959228 and perplexity is 151.64389512223974
At time: 51.149669885635376 and batch: 150, loss is 5.037239751815796 and perplexity is 154.0442273575594
At time: 51.59371066093445 and batch: 200, loss is 5.060426559448242 and perplexity is 157.65775238612744
At time: 52.037160873413086 and batch: 250, loss is 5.0894872760772705 and perplexity is 162.30662222775413
At time: 52.481099367141724 and batch: 300, loss is 5.010112295150757 and perplexity is 149.9215706691915
At time: 52.92531418800354 and batch: 350, loss is 5.009616250991821 and perplexity is 149.84722139154962
At time: 53.370007038116455 and batch: 400, loss is 5.042975149154663 and perplexity is 154.9302706856379
At time: 53.81284189224243 and batch: 450, loss is 5.065343780517578 and perplexity is 158.4348995442043
At time: 54.256552934646606 and batch: 500, loss is 5.08690598487854 and perplexity is 161.88820183706807
At time: 54.7001531124115 and batch: 550, loss is 4.988875226974487 and perplexity is 146.77124628093333
At time: 55.14433217048645 and batch: 600, loss is 4.9627919769287105 and perplexity is 142.99247077186007
At time: 55.589110136032104 and batch: 650, loss is 4.929478740692138 and perplexity is 138.30739952750625
At time: 56.03379678726196 and batch: 700, loss is 5.031924896240234 and perplexity is 153.22767638429048
At time: 56.47758221626282 and batch: 750, loss is 4.957711896896362 and perplexity is 142.26789957441483
At time: 56.92157459259033 and batch: 800, loss is 5.068402547836303 and perplexity is 158.92025695635147
At time: 57.36591839790344 and batch: 850, loss is 4.9606266212463375 and perplexity is 142.68317619996785
At time: 57.80948495864868 and batch: 900, loss is 4.990587539672852 and perplexity is 147.0227798402905
At time: 58.254138469696045 and batch: 950, loss is 4.95911901473999 and perplexity is 142.4682281844084
At time: 58.698200702667236 and batch: 1000, loss is 4.876139974594116 and perplexity is 131.1235455535239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2503662109375 and perplexity of 190.6360686904822
Finished 6 epochs...
Completing Train Step...
At time: 60.09693455696106 and batch: 50, loss is 5.0765170478820805 and perplexity is 160.21506163028633
At time: 60.5557644367218 and batch: 100, loss is 4.968366155624389 and perplexity is 143.7917619825032
At time: 61.00067591667175 and batch: 150, loss is 4.993081541061401 and perplexity is 147.38991248172093
At time: 61.4477961063385 and batch: 200, loss is 5.034736490249633 and perplexity is 153.65909660634242
At time: 61.892499685287476 and batch: 250, loss is 5.073788270950318 and perplexity is 159.77846642237
At time: 62.33716154098511 and batch: 300, loss is 4.987427587509155 and perplexity is 146.55892814957306
At time: 62.7815887928009 and batch: 350, loss is 4.990755958557129 and perplexity is 147.0475433380963
At time: 63.225456953048706 and batch: 400, loss is 5.000087890625 and perplexity is 148.42620380113334
At time: 63.668973207473755 and batch: 450, loss is 5.043210792541504 and perplexity is 154.9667832811533
At time: 64.1144208908081 and batch: 500, loss is 5.077905750274658 and perplexity is 160.43770722815557
At time: 64.55749607086182 and batch: 550, loss is 4.982336091995239 and perplexity is 145.81462045193715
At time: 65.00147938728333 and batch: 600, loss is 4.941680498123169 and perplexity is 140.0053306734245
At time: 65.44698309898376 and batch: 650, loss is 4.9152168273925785 and perplexity is 136.34887076774484
At time: 65.89269089698792 and batch: 700, loss is 5.012705583572387 and perplexity is 150.31086510075983
At time: 66.3366277217865 and batch: 750, loss is 4.9319265174865725 and perplexity is 138.646359850932
At time: 66.78205108642578 and batch: 800, loss is 5.049824924468994 and perplexity is 155.99515116159336
At time: 67.2258026599884 and batch: 850, loss is 4.9308553886413575 and perplexity is 138.4979312428846
At time: 67.67012763023376 and batch: 900, loss is 4.977322902679443 and perplexity is 145.08545340794015
At time: 68.11504530906677 and batch: 950, loss is 4.946041440963745 and perplexity is 140.61721915347846
At time: 68.56006836891174 and batch: 1000, loss is 4.8690011692047115 and perplexity is 130.19081333792946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.2517268483231705 and perplexity of 190.89563179818902
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 69.96563911437988 and batch: 50, loss is 5.004332361221313 and perplexity is 149.05753334091258
At time: 70.41027235984802 and batch: 100, loss is 4.821133298873901 and perplexity is 124.10565991598443
At time: 70.85455989837646 and batch: 150, loss is 4.7841447162628175 and perplexity is 119.59902822360378
At time: 71.30028176307678 and batch: 200, loss is 4.784069652557373 and perplexity is 119.5900510143133
At time: 71.76050615310669 and batch: 250, loss is 4.806248426437378 and perplexity is 122.27204342770713
At time: 72.20543694496155 and batch: 300, loss is 4.712357044219971 and perplexity is 111.31422348371314
At time: 72.65061092376709 and batch: 350, loss is 4.705383090972901 and perplexity is 110.54062395228378
At time: 73.09514880180359 and batch: 400, loss is 4.700588188171387 and perplexity is 110.01186110112133
At time: 73.54015302658081 and batch: 450, loss is 4.729554510116577 and perplexity is 113.24510156251621
At time: 73.98704433441162 and batch: 500, loss is 4.758986387252808 and perplexity is 116.62765069307058
At time: 74.43330883979797 and batch: 550, loss is 4.673431329727173 and perplexity is 107.06448641781348
At time: 74.8797538280487 and batch: 600, loss is 4.607839689254761 and perplexity is 100.26730695632224
At time: 75.32458782196045 and batch: 650, loss is 4.582070465087891 and perplexity is 97.71650351375764
At time: 75.76963233947754 and batch: 700, loss is 4.657452116012573 and perplexity is 105.36727626297169
At time: 76.21383905410767 and batch: 750, loss is 4.603717517852783 and perplexity is 99.8548386476319
At time: 76.65882563591003 and batch: 800, loss is 4.678677520751953 and perplexity is 107.62764308764112
At time: 77.10373878479004 and batch: 850, loss is 4.582677249908447 and perplexity is 97.77581439745981
At time: 77.54844117164612 and batch: 900, loss is 4.582575025558472 and perplexity is 97.76581983924206
At time: 77.99325466156006 and batch: 950, loss is 4.527091474533081 and perplexity is 92.48916241129233
At time: 78.44104385375977 and batch: 1000, loss is 4.438094453811646 and perplexity is 84.61355292254837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.92895023997237 and perplexity of 138.23432327943252
Finished 8 epochs...
Completing Train Step...
At time: 79.83667182922363 and batch: 50, loss is 4.75226279258728 and perplexity is 115.84612392181933
At time: 80.29605674743652 and batch: 100, loss is 4.660091209411621 and perplexity is 105.64571760097387
At time: 80.74041271209717 and batch: 150, loss is 4.666804904937744 and perplexity is 106.35737704189582
At time: 81.1851532459259 and batch: 200, loss is 4.694879894256592 and perplexity is 109.38567000560151
At time: 81.62975978851318 and batch: 250, loss is 4.726048946380615 and perplexity is 112.84880866204983
At time: 82.07346701622009 and batch: 300, loss is 4.641579122543335 and perplexity is 103.70798596530926
At time: 82.51770257949829 and batch: 350, loss is 4.636690979003906 and perplexity is 103.20228342396638
At time: 82.9626030921936 and batch: 400, loss is 4.638097915649414 and perplexity is 103.34758468928477
At time: 83.42194414138794 and batch: 450, loss is 4.677612905502319 and perplexity is 107.51312202877726
At time: 83.86697149276733 and batch: 500, loss is 4.708109502792358 and perplexity is 110.84241443158028
At time: 84.31397843360901 and batch: 550, loss is 4.632662725448609 and perplexity is 102.78739465844852
At time: 84.7576277256012 and batch: 600, loss is 4.5659253215789795 and perplexity is 96.15152394575082
At time: 85.20114469528198 and batch: 650, loss is 4.544616489410401 and perplexity is 94.12432261214101
At time: 85.64624118804932 and batch: 700, loss is 4.625371971130371 and perplexity is 102.04072222705975
At time: 86.0901358127594 and batch: 750, loss is 4.567327260971069 and perplexity is 96.28641708869162
At time: 86.53440976142883 and batch: 800, loss is 4.655154762268066 and perplexity is 105.12548819900061
At time: 86.9804539680481 and batch: 850, loss is 4.558823957443237 and perplexity is 95.47113566377128
At time: 87.42563605308533 and batch: 900, loss is 4.5666096401214595 and perplexity is 96.21734473509933
At time: 87.8700783252716 and batch: 950, loss is 4.527069206237793 and perplexity is 92.48710285824428
At time: 88.31600713729858 and batch: 1000, loss is 4.444498405456543 and perplexity is 85.15715275972488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.91544360649295 and perplexity of 136.37979534838598
Finished 9 epochs...
Completing Train Step...
At time: 89.71479368209839 and batch: 50, loss is 4.717705698013305 and perplexity is 111.9111998140626
At time: 90.17412185668945 and batch: 100, loss is 4.627654438018799 and perplexity is 102.27389279763942
At time: 90.61895632743835 and batch: 150, loss is 4.639267406463623 and perplexity is 103.46851944250744
At time: 91.06437134742737 and batch: 200, loss is 4.668829650878906 and perplexity is 106.57294186773304
At time: 91.50875115394592 and batch: 250, loss is 4.699771251678467 and perplexity is 109.922025097285
At time: 91.95214033126831 and batch: 300, loss is 4.6179589176177975 and perplexity is 101.28708571784999
At time: 92.39670872688293 and batch: 350, loss is 4.611598358154297 and perplexity is 100.64488772063167
At time: 92.84131479263306 and batch: 400, loss is 4.611704769134522 and perplexity is 100.6555980116248
At time: 93.28507590293884 and batch: 450, loss is 4.6547527027130124 and perplexity is 105.08322998771861
At time: 93.72952342033386 and batch: 500, loss is 4.684571409225464 and perplexity is 108.26386147102382
At time: 94.17411923408508 and batch: 550, loss is 4.6141792678833005 and perplexity is 100.90497858166698
At time: 94.64405608177185 and batch: 600, loss is 4.550046691894531 and perplexity is 94.63682698450955
At time: 95.08774423599243 and batch: 650, loss is 4.529481725692749 and perplexity is 92.71049915883114
At time: 95.53242206573486 and batch: 700, loss is 4.612191791534424 and perplexity is 100.70463148176196
At time: 95.97601771354675 and batch: 750, loss is 4.552257165908814 and perplexity is 94.84625060881903
At time: 96.42005491256714 and batch: 800, loss is 4.641399154663086 and perplexity is 103.68932353827928
At time: 96.86525917053223 and batch: 850, loss is 4.546995105743409 and perplexity is 94.34847474351568
At time: 97.30878901481628 and batch: 900, loss is 4.561666011810303 and perplexity is 95.74285576056066
At time: 97.75289726257324 and batch: 950, loss is 4.526929864883423 and perplexity is 92.47421647789392
At time: 98.19706964492798 and batch: 1000, loss is 4.4423895263671875 and perplexity is 84.97775585064299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.908883443692836 and perplexity of 135.48804988404007
Finished 10 epochs...
Completing Train Step...
At time: 99.6099123954773 and batch: 50, loss is 4.697348041534424 and perplexity is 109.65598339866882
At time: 100.0563747882843 and batch: 100, loss is 4.606724901199341 and perplexity is 100.15559244075142
At time: 100.50239729881287 and batch: 150, loss is 4.620330152511596 and perplexity is 101.52754617123331
At time: 100.94836974143982 and batch: 200, loss is 4.650393514633179 and perplexity is 104.62614939810136
At time: 101.3948221206665 and batch: 250, loss is 4.6823271369934085 and perplexity is 108.0211603385643
At time: 101.83850526809692 and batch: 300, loss is 4.601576957702637 and perplexity is 99.64132196340402
At time: 102.28450226783752 and batch: 350, loss is 4.594885358810425 and perplexity is 98.97678808051086
At time: 102.72949266433716 and batch: 400, loss is 4.5941962718963625 and perplexity is 98.90860796475936
At time: 103.17540693283081 and batch: 450, loss is 4.640197582244873 and perplexity is 103.56480812917422
At time: 103.62030076980591 and batch: 500, loss is 4.670559062957763 and perplexity is 106.75740986529344
At time: 104.06702208518982 and batch: 550, loss is 4.60137674331665 and perplexity is 99.62137433427601
At time: 104.51189732551575 and batch: 600, loss is 4.53810977935791 and perplexity is 93.51387110523132
At time: 104.95679426193237 and batch: 650, loss is 4.516805286407471 and perplexity is 91.54267769236061
At time: 105.40225958824158 and batch: 700, loss is 4.602854795455933 and perplexity is 99.76872879168825
At time: 105.86108493804932 and batch: 750, loss is 4.540235557556152 and perplexity is 93.71287229491587
At time: 106.30529069900513 and batch: 800, loss is 4.633082084655761 and perplexity is 102.83050853824717
At time: 106.75087451934814 and batch: 850, loss is 4.538922472000122 and perplexity is 93.58990003011264
At time: 107.1954414844513 and batch: 900, loss is 4.555535106658936 and perplexity is 95.15766111243647
At time: 107.63888645172119 and batch: 950, loss is 4.5228810882568355 and perplexity is 92.1005659558313
At time: 108.08369207382202 and batch: 1000, loss is 4.436467533111572 and perplexity is 84.47600530166697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.90541113876715 and perplexity of 135.01840989953868
Finished 11 epochs...
Completing Train Step...
At time: 109.48000121116638 and batch: 50, loss is 4.681977949142456 and perplexity is 107.98344724658826
At time: 109.93939900398254 and batch: 100, loss is 4.589820652008057 and perplexity is 98.47676696765576
At time: 110.38330674171448 and batch: 150, loss is 4.606482334136963 and perplexity is 100.13130093919057
At time: 110.82749509811401 and batch: 200, loss is 4.637767486572265 and perplexity is 103.313441283548
At time: 111.27226996421814 and batch: 250, loss is 4.669656343460083 and perplexity is 106.66108135523984
At time: 111.71573662757874 and batch: 300, loss is 4.588211288452149 and perplexity is 98.31840950933496
At time: 112.15911078453064 and batch: 350, loss is 4.580431108474731 and perplexity is 97.55644255185261
At time: 112.60620450973511 and batch: 400, loss is 4.579856042861938 and perplexity is 97.50035732432312
At time: 113.05018782615662 and batch: 450, loss is 4.626321706771851 and perplexity is 102.13767997266649
At time: 113.49510312080383 and batch: 500, loss is 4.656695823669434 and perplexity is 105.28761792500903
At time: 113.9457893371582 and batch: 550, loss is 4.589988689422608 and perplexity is 98.49331613937142
At time: 114.39040994644165 and batch: 600, loss is 4.52792519569397 and perplexity is 92.56630473627351
At time: 114.83408498764038 and batch: 650, loss is 4.50719614982605 and perplexity is 90.66724441447413
At time: 115.27846097946167 and batch: 700, loss is 4.594066143035889 and perplexity is 98.895737937713
At time: 115.7222912311554 and batch: 750, loss is 4.531013994216919 and perplexity is 92.85266542918755
At time: 116.16618776321411 and batch: 800, loss is 4.625295858383179 and perplexity is 102.03295592292672
At time: 116.61113715171814 and batch: 850, loss is 4.529948310852051 and perplexity is 92.75376659503696
At time: 117.055588722229 and batch: 900, loss is 4.548660440444946 and perplexity is 94.50572743538261
At time: 117.51488399505615 and batch: 950, loss is 4.515796566009522 and perplexity is 91.45038328354178
At time: 117.95833396911621 and batch: 1000, loss is 4.429645462036133 and perplexity is 83.90166531096033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.901169195407775 and perplexity of 134.44688250270386
Finished 12 epochs...
Completing Train Step...
At time: 119.35382461547852 and batch: 50, loss is 4.667381963729858 and perplexity is 106.41876921316651
At time: 119.81351804733276 and batch: 100, loss is 4.575982294082642 and perplexity is 97.12339603221736
At time: 120.25684213638306 and batch: 150, loss is 4.593345251083374 and perplexity is 98.82447048724782
At time: 120.70127081871033 and batch: 200, loss is 4.627414789199829 and perplexity is 102.24938591665885
At time: 121.1462562084198 and batch: 250, loss is 4.660491237640381 and perplexity is 105.68798732423943
At time: 121.5900502204895 and batch: 300, loss is 4.5777912521362305 and perplexity is 97.29924718738987
At time: 122.034175157547 and batch: 350, loss is 4.567940196990967 and perplexity is 96.3454525925973
At time: 122.47955679893494 and batch: 400, loss is 4.568446416854858 and perplexity is 96.39423692125158
At time: 122.92365741729736 and batch: 450, loss is 4.615027132034302 and perplexity is 100.99056857488007
At time: 123.3668315410614 and batch: 500, loss is 4.645979795455933 and perplexity is 104.16537656502923
At time: 123.8112735748291 and batch: 550, loss is 4.580533390045166 and perplexity is 97.56642128831454
At time: 124.25544881820679 and batch: 600, loss is 4.517513179779053 and perplexity is 91.60750308914321
At time: 124.69869947433472 and batch: 650, loss is 4.498223237991333 and perplexity is 89.85733428417008
At time: 125.14161682128906 and batch: 700, loss is 4.586749067306519 and perplexity is 98.17475130757207
At time: 125.58487319946289 and batch: 750, loss is 4.525126981735229 and perplexity is 92.3076464696121
At time: 126.02874159812927 and batch: 800, loss is 4.61814866065979 and perplexity is 101.30630606101413
At time: 126.47388768196106 and batch: 850, loss is 4.521969528198242 and perplexity is 92.01664901201005
At time: 126.9196105003357 and batch: 900, loss is 4.543653516769409 and perplexity is 94.03372709211818
At time: 127.36414361000061 and batch: 950, loss is 4.510015201568604 and perplexity is 90.92320067529673
At time: 127.80744218826294 and batch: 1000, loss is 4.423140773773193 and perplexity is 83.35768227145333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.899178853849086 and perplexity of 134.17955341137383
Finished 13 epochs...
Completing Train Step...
At time: 129.21216225624084 and batch: 50, loss is 4.656181287765503 and perplexity is 105.23345760025798
At time: 129.65676760673523 and batch: 100, loss is 4.565162162780762 and perplexity is 96.0781730570377
At time: 130.10058784484863 and batch: 150, loss is 4.582297105789184 and perplexity is 97.73865256048488
At time: 130.544349193573 and batch: 200, loss is 4.615374307632447 and perplexity is 101.02563612287827
At time: 130.9896376132965 and batch: 250, loss is 4.649355192184448 and perplexity is 104.51757009837426
At time: 131.43351292610168 and batch: 300, loss is 4.566374464035034 and perplexity is 96.19471937709413
At time: 131.87696981430054 and batch: 350, loss is 4.559111013412475 and perplexity is 95.49854515699464
At time: 132.32138347625732 and batch: 400, loss is 4.560356588363647 and perplexity is 95.61756986442414
At time: 132.76661729812622 and batch: 450, loss is 4.606910943984985 and perplexity is 100.17422739956305
At time: 133.20987153053284 and batch: 500, loss is 4.636117992401123 and perplexity is 103.14316683631154
At time: 133.65518450737 and batch: 550, loss is 4.572030172348023 and perplexity is 96.74031004766802
At time: 134.09925389289856 and batch: 600, loss is 4.510007333755493 and perplexity is 90.92248531136059
At time: 134.5439772605896 and batch: 650, loss is 4.49091139793396 and perplexity is 89.20270800574636
At time: 134.98768782615662 and batch: 700, loss is 4.580287761688233 and perplexity is 97.54245915157226
At time: 135.4324128627777 and batch: 750, loss is 4.519870882034302 and perplexity is 91.82374111805045
At time: 135.8759458065033 and batch: 800, loss is 4.611810207366943 and perplexity is 100.6662115194874
At time: 136.31942868232727 and batch: 850, loss is 4.51455096244812 and perplexity is 91.33654327491118
At time: 136.76494073867798 and batch: 900, loss is 4.53766788482666 and perplexity is 93.47255696591228
At time: 137.20871758460999 and batch: 950, loss is 4.504463729858398 and perplexity is 90.41984158356989
At time: 137.65200352668762 and batch: 1000, loss is 4.416425609588623 and perplexity is 82.79979699099437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.896865100395389 and perplexity of 133.86945389151228
Finished 14 epochs...
Completing Train Step...
At time: 139.04650115966797 and batch: 50, loss is 4.645771551132202 and perplexity is 104.14368697507567
At time: 139.50710678100586 and batch: 100, loss is 4.555280618667602 and perplexity is 95.13344771154053
At time: 139.95369696617126 and batch: 150, loss is 4.572894983291626 and perplexity is 96.82400831285615
At time: 140.41398811340332 and batch: 200, loss is 4.605815315246582 and perplexity is 100.06453373991279
At time: 140.8594172000885 and batch: 250, loss is 4.641069822311401 and perplexity is 103.65518091195813
At time: 141.3049521446228 and batch: 300, loss is 4.557708940505981 and perplexity is 95.36474305628965
At time: 141.7496201992035 and batch: 350, loss is 4.549600477218628 and perplexity is 94.59460806347977
At time: 142.19531965255737 and batch: 400, loss is 4.549734363555908 and perplexity is 94.60727383694791
At time: 142.64021587371826 and batch: 450, loss is 4.596557292938233 and perplexity is 99.14240916555474
At time: 143.0850269794464 and batch: 500, loss is 4.626969842910767 and perplexity is 102.20390055185892
At time: 143.53060483932495 and batch: 550, loss is 4.5635990524292 and perplexity is 95.92810958361173
At time: 143.97501683235168 and batch: 600, loss is 4.501692428588867 and perplexity is 90.16960785847043
At time: 144.4199516773224 and batch: 650, loss is 4.482710380554199 and perplexity is 88.47414660281419
At time: 144.86557126045227 and batch: 700, loss is 4.574330139160156 and perplexity is 96.96306561718646
At time: 145.3105595111847 and batch: 750, loss is 4.513996887207031 and perplexity is 91.28594997521759
At time: 145.75463223457336 and batch: 800, loss is 4.605856304168701 and perplexity is 100.06863536135305
At time: 146.19883036613464 and batch: 850, loss is 4.507640714645386 and perplexity is 90.70756084257573
At time: 146.64537453651428 and batch: 900, loss is 4.532286729812622 and perplexity is 92.97091755751904
At time: 147.08930850028992 and batch: 950, loss is 4.49862678527832 and perplexity is 89.8936032852727
At time: 147.53277611732483 and batch: 1000, loss is 4.41030948638916 and perplexity is 82.29492872376588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.895118155130526 and perplexity of 133.63579543661604
Finished 15 epochs...
Completing Train Step...
At time: 148.92807817459106 and batch: 50, loss is 4.635354719161987 and perplexity is 103.06447045450744
At time: 149.38842368125916 and batch: 100, loss is 4.544928550720215 and perplexity is 94.15369975503718
At time: 149.83252501487732 and batch: 150, loss is 4.5639730739593505 and perplexity is 95.96399547257187
At time: 150.2764720916748 and batch: 200, loss is 4.5968978977203365 and perplexity is 99.17618329571444
At time: 150.72273683547974 and batch: 250, loss is 4.632560167312622 and perplexity is 102.77685351539868
At time: 151.167010307312 and batch: 300, loss is 4.548887729644775 and perplexity is 94.52721000783654
At time: 151.61042642593384 and batch: 350, loss is 4.539313926696777 and perplexity is 93.6265434076821
At time: 152.07043480873108 and batch: 400, loss is 4.540735054016113 and perplexity is 93.75969323535345
At time: 152.5160882472992 and batch: 450, loss is 4.587351636886597 and perplexity is 98.23392625296225
At time: 152.96239137649536 and batch: 500, loss is 4.618503713607788 and perplexity is 101.34228154985571
At time: 153.4079670906067 and batch: 550, loss is 4.555368909835815 and perplexity is 95.14184752558435
At time: 153.85323643684387 and batch: 600, loss is 4.493788051605224 and perplexity is 89.4596827396653
At time: 154.29720640182495 and batch: 650, loss is 4.475663070678711 and perplexity is 87.85283373930874
At time: 154.7424635887146 and batch: 700, loss is 4.568283348083496 and perplexity is 96.37851931303085
At time: 155.18794465065002 and batch: 750, loss is 4.5082275390625 and perplexity is 90.76080587530971
At time: 155.63263964653015 and batch: 800, loss is 4.599399824142456 and perplexity is 99.42462547157254
At time: 156.07756733894348 and batch: 850, loss is 4.501286764144897 and perplexity is 90.13303667295189
At time: 156.52424311637878 and batch: 900, loss is 4.526424102783203 and perplexity is 92.42745834919327
At time: 156.9695384502411 and batch: 950, loss is 4.493147888183594 and perplexity is 89.40243224985456
At time: 157.4135992527008 and batch: 1000, loss is 4.40411605834961 and perplexity is 81.78681610835935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.893098040324886 and perplexity of 133.3661082789141
Finished 16 epochs...
Completing Train Step...
At time: 158.82211685180664 and batch: 50, loss is 4.625069990158081 and perplexity is 102.00991252275477
At time: 159.266126871109 and batch: 100, loss is 4.53592432975769 and perplexity is 93.30972441041925
At time: 159.71094155311584 and batch: 150, loss is 4.55461483001709 and perplexity is 95.07013002220845
At time: 160.15604329109192 and batch: 200, loss is 4.588718519210816 and perplexity is 98.36829228074981
At time: 160.60104179382324 and batch: 250, loss is 4.623986682891846 and perplexity is 101.8994642787849
At time: 161.0450897216797 and batch: 300, loss is 4.5407681655883785 and perplexity is 93.76279781761012
At time: 161.48852801322937 and batch: 350, loss is 4.530154666900635 and perplexity is 92.77290887079705
At time: 161.9323456287384 and batch: 400, loss is 4.531787242889404 and perplexity is 92.92449139555512
At time: 162.37679266929626 and batch: 450, loss is 4.578678188323974 and perplexity is 97.38558369258621
At time: 162.818621635437 and batch: 500, loss is 4.60993613243103 and perplexity is 100.47773216296845
At time: 163.2780101299286 and batch: 550, loss is 4.547064409255982 and perplexity is 94.35501365080333
At time: 163.72294974327087 and batch: 600, loss is 4.486327629089356 and perplexity is 88.79475909796427
At time: 164.16743421554565 and batch: 650, loss is 4.468910865783691 and perplexity is 87.26163161043269
At time: 164.61076045036316 and batch: 700, loss is 4.561744832992554 and perplexity is 95.75040262306622
At time: 165.05592250823975 and batch: 750, loss is 4.50205283164978 and perplexity is 90.20211111792925
At time: 165.50029754638672 and batch: 800, loss is 4.593876342773438 and perplexity is 98.8769692819014
At time: 165.94416403770447 and batch: 850, loss is 4.494531230926514 and perplexity is 89.52619203707259
At time: 166.38885974884033 and batch: 900, loss is 4.520724458694458 and perplexity is 91.90215318090954
At time: 166.83332443237305 and batch: 950, loss is 4.487102479934692 and perplexity is 88.86358845491175
At time: 167.27607107162476 and batch: 1000, loss is 4.397326564788818 and perplexity is 81.23340586057886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.892626134360709 and perplexity of 133.30318686466126
Finished 17 epochs...
Completing Train Step...
At time: 168.67116928100586 and batch: 50, loss is 4.615768251419067 and perplexity is 101.0654423847182
At time: 169.1318700313568 and batch: 100, loss is 4.528744506835937 and perplexity is 92.64217641812762
At time: 169.57750391960144 and batch: 150, loss is 4.547376480102539 and perplexity is 94.38446369480131
At time: 170.02144122123718 and batch: 200, loss is 4.580503225326538 and perplexity is 97.56347826905676
At time: 170.4659104347229 and batch: 250, loss is 4.616538381576538 and perplexity is 101.14330590845228
At time: 170.9101688861847 and batch: 300, loss is 4.533393487930298 and perplexity is 93.07387083691341
At time: 171.35353565216064 and batch: 350, loss is 4.522276477813721 and perplexity is 92.04489782230087
At time: 171.79828071594238 and batch: 400, loss is 4.5237500095367436 and perplexity is 92.18062887663224
At time: 172.24402165412903 and batch: 450, loss is 4.571090259552002 and perplexity is 96.64942531091762
At time: 172.68807530403137 and batch: 500, loss is 4.602537136077881 and perplexity is 99.73704135252385
At time: 173.13314509391785 and batch: 550, loss is 4.539319696426392 and perplexity is 93.62708360908076
At time: 173.57773637771606 and batch: 600, loss is 4.479131460189819 and perplexity is 88.15807062057708
At time: 174.02213191986084 and batch: 650, loss is 4.461898860931396 and perplexity is 86.65189286997611
At time: 174.46587681770325 and batch: 700, loss is 4.555329990386963 and perplexity is 95.13814472937175
At time: 174.93754148483276 and batch: 750, loss is 4.496440448760986 and perplexity is 89.69728030998348
At time: 175.38204312324524 and batch: 800, loss is 4.587682723999023 and perplexity is 98.2664556246784
At time: 175.8265736103058 and batch: 850, loss is 4.488719053268433 and perplexity is 89.00735913896571
At time: 176.27095103263855 and batch: 900, loss is 4.514750394821167 and perplexity is 91.35476055497972
At time: 176.71665287017822 and batch: 950, loss is 4.480602293014527 and perplexity is 88.28783180972363
At time: 177.16088676452637 and batch: 1000, loss is 4.390965051651001 and perplexity is 80.71827871324065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.890789125023819 and perplexity of 133.0585324507759
Finished 18 epochs...
Completing Train Step...
At time: 178.55348134040833 and batch: 50, loss is 4.607801952362061 and perplexity is 100.26352325111135
At time: 179.01276779174805 and batch: 100, loss is 4.520795660018921 and perplexity is 91.90869696889744
At time: 179.45841312408447 and batch: 150, loss is 4.53935866355896 and perplexity is 93.63073205914405
At time: 179.9013078212738 and batch: 200, loss is 4.573232002258301 and perplexity is 96.85664533942669
At time: 180.34472680091858 and batch: 250, loss is 4.609467601776123 and perplexity is 100.43066629207719
At time: 180.78956413269043 and batch: 300, loss is 4.525639457702637 and perplexity is 92.35496404355985
At time: 181.23398876190186 and batch: 350, loss is 4.513905258178711 and perplexity is 91.27758591552325
At time: 181.6783094406128 and batch: 400, loss is 4.515077648162841 and perplexity is 91.38466159799046
At time: 182.1238076686859 and batch: 450, loss is 4.56335129737854 and perplexity is 95.90434585387578
At time: 182.56897115707397 and batch: 500, loss is 4.59483941078186 and perplexity is 98.97224039670431
At time: 183.01377940177917 and batch: 550, loss is 4.531047821044922 and perplexity is 92.85580639345476
At time: 183.45829582214355 and batch: 600, loss is 4.471986026763916 and perplexity is 87.53038819812372
At time: 183.90225791931152 and batch: 650, loss is 4.454482135772705 and perplexity is 86.01159698049901
At time: 184.34656834602356 and batch: 700, loss is 4.549506025314331 and perplexity is 94.5856738445456
At time: 184.7900106906891 and batch: 750, loss is 4.490520868301392 and perplexity is 89.16787850638323
At time: 185.234605550766 and batch: 800, loss is 4.581575794219971 and perplexity is 97.66817795976168
At time: 185.67724800109863 and batch: 850, loss is 4.482509260177612 and perplexity is 88.45635443837463
At time: 186.1350326538086 and batch: 900, loss is 4.50882703781128 and perplexity is 90.81523317778995
At time: 186.58035969734192 and batch: 950, loss is 4.474425315856934 and perplexity is 87.74416073987989
At time: 187.0237922668457 and batch: 1000, loss is 4.384166975021362 and perplexity is 80.17141060087414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.888437782845846 and perplexity of 132.74603385083327
Finished 19 epochs...
Completing Train Step...
At time: 188.43725204467773 and batch: 50, loss is 4.598510875701904 and perplexity is 99.3362813782968
At time: 188.88060116767883 and batch: 100, loss is 4.512609195709229 and perplexity is 91.15936109204084
At time: 189.32611846923828 and batch: 150, loss is 4.5306729984283445 and perplexity is 92.82100845907176
At time: 189.7693235874176 and batch: 200, loss is 4.564557723999023 and perplexity is 96.02011723052746
At time: 190.21321058273315 and batch: 250, loss is 4.601475429534912 and perplexity is 99.63120607608788
At time: 190.6559600830078 and batch: 300, loss is 4.515575857162475 and perplexity is 91.43020160210506
At time: 191.0999791622162 and batch: 350, loss is 4.504282398223877 and perplexity is 90.40344709236727
At time: 191.54302382469177 and batch: 400, loss is 4.506157474517822 and perplexity is 90.57311947752194
At time: 191.98675918579102 and batch: 450, loss is 4.554702215194702 and perplexity is 95.07843810540243
At time: 192.4349329471588 and batch: 500, loss is 4.586439085006714 and perplexity is 98.14432358864978
At time: 192.87910652160645 and batch: 550, loss is 4.522053709030152 and perplexity is 92.02439537611653
At time: 193.32253456115723 and batch: 600, loss is 4.463428106307983 and perplexity is 86.78450624987504
At time: 193.7683687210083 and batch: 650, loss is 4.444635534286499 and perplexity is 85.16883106114282
At time: 194.21079635620117 and batch: 700, loss is 4.541338863372803 and perplexity is 93.81632331057345
At time: 194.65451502799988 and batch: 750, loss is 4.483537454605102 and perplexity is 88.54735154242516
At time: 195.09859442710876 and batch: 800, loss is 4.574252758026123 and perplexity is 96.95556279550186
At time: 195.54217624664307 and batch: 850, loss is 4.475109357833862 and perplexity is 87.8042019620733
At time: 195.98681497573853 and batch: 900, loss is 4.5017047691345216 and perplexity is 90.17072060749882
At time: 196.431081533432 and batch: 950, loss is 4.466005544662476 and perplexity is 87.00847647559452
At time: 196.87547874450684 and batch: 1000, loss is 4.376666431427002 and perplexity is 79.57233096070998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.885571363495617 and perplexity of 132.36607287464238
Finished 20 epochs...
Completing Train Step...
At time: 198.27292585372925 and batch: 50, loss is 4.589267950057984 and perplexity is 98.4223537050592
At time: 198.73150825500488 and batch: 100, loss is 4.50372730255127 and perplexity is 90.35327845558234
At time: 199.17581176757812 and batch: 150, loss is 4.522833547592163 and perplexity is 92.09618753778632
At time: 199.6200816631317 and batch: 200, loss is 4.556554813385009 and perplexity is 95.25474350888437
At time: 200.06382656097412 and batch: 250, loss is 4.5929834842681885 and perplexity is 98.7887255393551
At time: 200.5075969696045 and batch: 300, loss is 4.507365264892578 and perplexity is 90.68257890815563
At time: 200.9524004459381 and batch: 350, loss is 4.49558349609375 and perplexity is 89.6204469123725
At time: 201.39570498466492 and batch: 400, loss is 4.497479619979859 and perplexity is 89.79053958988015
At time: 201.8397433757782 and batch: 450, loss is 4.546241149902344 and perplexity is 94.27736696931656
At time: 202.28524613380432 and batch: 500, loss is 4.5785739040374756 and perplexity is 97.37542843600161
At time: 202.72977328300476 and batch: 550, loss is 4.512432308197021 and perplexity is 91.14323756551013
At time: 203.17344546318054 and batch: 600, loss is 4.45579665184021 and perplexity is 86.12473495127043
At time: 203.61935186386108 and batch: 650, loss is 4.4359048557281495 and perplexity is 84.42848593433222
At time: 204.0642490386963 and batch: 700, loss is 4.5329332447052 and perplexity is 93.03104407454666
At time: 204.50796175003052 and batch: 750, loss is 4.475991363525391 and perplexity is 87.88167993092601
At time: 204.95279574394226 and batch: 800, loss is 4.567423076629638 and perplexity is 96.29564327715583
At time: 205.3986611366272 and batch: 850, loss is 4.467728204727173 and perplexity is 87.15849167881078
At time: 205.84327578544617 and batch: 900, loss is 4.49395565032959 and perplexity is 89.47467732487618
At time: 206.2874138355255 and batch: 950, loss is 4.458467454910278 and perplexity is 86.35506460344278
At time: 206.73110246658325 and batch: 1000, loss is 4.368403005599975 and perplexity is 78.91750020483506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.882829991782584 and perplexity of 132.0037051859007
Finished 21 epochs...
Completing Train Step...
At time: 208.12341856956482 and batch: 50, loss is 4.578812322616577 and perplexity is 97.39864731508483
At time: 208.5831117630005 and batch: 100, loss is 4.492984857559204 and perplexity is 89.38785810355125
At time: 209.02806043624878 and batch: 150, loss is 4.514511327743531 and perplexity is 91.33292324974005
At time: 209.48873782157898 and batch: 200, loss is 4.547644844055176 and perplexity is 94.40979648159734
At time: 209.9340751171112 and batch: 250, loss is 4.584107885360718 and perplexity is 97.91579605139458
At time: 210.37971830368042 and batch: 300, loss is 4.4983860301971434 and perplexity is 89.87196354855928
At time: 210.82505631446838 and batch: 350, loss is 4.486823816299438 and perplexity is 88.83882885427155
At time: 211.2690818309784 and batch: 400, loss is 4.48893440246582 and perplexity is 89.02652886633646
At time: 211.71300387382507 and batch: 450, loss is 4.5380551052093505 and perplexity is 93.50875845371634
At time: 212.15803575515747 and batch: 500, loss is 4.570379571914673 and perplexity is 96.58076216111006
At time: 212.60285758972168 and batch: 550, loss is 4.504805421829223 and perplexity is 90.45074259645564
At time: 213.0481264591217 and batch: 600, loss is 4.448051671981812 and perplexity is 85.46027704176569
At time: 213.4921658039093 and batch: 650, loss is 4.428583364486695 and perplexity is 83.81260086377777
At time: 213.93816614151 and batch: 700, loss is 4.526068191528321 and perplexity is 92.39456822983567
At time: 214.38209342956543 and batch: 750, loss is 4.470221548080445 and perplexity is 87.37607887178602
At time: 214.8267695903778 and batch: 800, loss is 4.5623025226593015 and perplexity is 95.80381652600249
At time: 215.27155780792236 and batch: 850, loss is 4.461960124969482 and perplexity is 86.65720167745896
At time: 215.71921014785767 and batch: 900, loss is 4.486938276290894 and perplexity is 88.84899792782808
At time: 216.1649992465973 and batch: 950, loss is 4.451092290878296 and perplexity is 85.7205246314001
At time: 216.61249017715454 and batch: 1000, loss is 4.359682369232178 and perplexity is 78.23228149662587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.880099785037157 and perplexity of 131.6437993118656
Finished 22 epochs...
Completing Train Step...
At time: 218.02568793296814 and batch: 50, loss is 4.569407558441162 and perplexity is 96.48692996947254
At time: 218.4711456298828 and batch: 100, loss is 4.4842444896698 and perplexity is 88.60997976241661
At time: 218.9165518283844 and batch: 150, loss is 4.505939025878906 and perplexity is 90.55333606375822
At time: 219.36152696609497 and batch: 200, loss is 4.539999132156372 and perplexity is 93.69071881054536
At time: 219.80597114562988 and batch: 250, loss is 4.575421733856201 and perplexity is 97.06896777592794
At time: 220.25085496902466 and batch: 300, loss is 4.489306344985962 and perplexity is 89.05964777662612
At time: 220.7113275527954 and batch: 350, loss is 4.479179954528808 and perplexity is 88.16234589160074
At time: 221.15528297424316 and batch: 400, loss is 4.480602788925171 and perplexity is 88.28787559261004
At time: 221.59964036941528 and batch: 450, loss is 4.5299700832366945 and perplexity is 92.75578608770492
At time: 222.04473423957825 and batch: 500, loss is 4.562832174301147 and perplexity is 95.85457261505695
At time: 222.489333152771 and batch: 550, loss is 4.496258687973023 and perplexity is 89.68097834321019
At time: 222.93275785446167 and batch: 600, loss is 4.439952545166015 and perplexity is 84.77091878849825
At time: 223.37858843803406 and batch: 650, loss is 4.419310712814331 and perplexity is 83.03902788935643
At time: 223.82375407218933 and batch: 700, loss is 4.518460359573364 and perplexity is 91.69431297086936
At time: 224.26846027374268 and batch: 750, loss is 4.462750720977783 and perplexity is 86.72573960453153
At time: 224.71309876441956 and batch: 800, loss is 4.554557867050171 and perplexity is 95.06471469977494
At time: 225.15698146820068 and batch: 850, loss is 4.453637008666992 and perplexity is 85.93893695628786
At time: 225.60120558738708 and batch: 900, loss is 4.479927110671997 and perplexity is 88.22824154403685
At time: 226.04583477973938 and batch: 950, loss is 4.4434935665130615 and perplexity is 85.07162651356363
At time: 226.49045372009277 and batch: 1000, loss is 4.352314500808716 and perplexity is 77.6579945732871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8777406273818595 and perplexity of 131.33359688703177
Finished 23 epochs...
Completing Train Step...
At time: 227.8890335559845 and batch: 50, loss is 4.560344095230103 and perplexity is 95.61637530881652
At time: 228.34688305854797 and batch: 100, loss is 4.475535125732422 and perplexity is 87.8415941322554
At time: 228.79068303108215 and batch: 150, loss is 4.49748477935791 and perplexity is 89.79100285441439
At time: 229.23658108711243 and batch: 200, loss is 4.53196346282959 and perplexity is 92.94086798676923
At time: 229.67988801002502 and batch: 250, loss is 4.567299680709839 and perplexity is 96.28376152077618
At time: 230.12402272224426 and batch: 300, loss is 4.480772647857666 and perplexity is 88.302873350626
At time: 230.56897449493408 and batch: 350, loss is 4.470973243713379 and perplexity is 87.44178378064956
At time: 231.01339888572693 and batch: 400, loss is 4.472161312103271 and perplexity is 87.5457323366849
At time: 231.45805525779724 and batch: 450, loss is 4.5209833526611325 and perplexity is 91.9259491740791
At time: 231.90384244918823 and batch: 500, loss is 4.555253324508667 and perplexity is 95.1308511595341
At time: 232.36346578598022 and batch: 550, loss is 4.48800304889679 and perplexity is 88.94365229062642
At time: 232.80708956718445 and batch: 600, loss is 4.43141547203064 and perplexity is 84.05030360394241
At time: 233.2522473335266 and batch: 650, loss is 4.41054123878479 and perplexity is 82.31400298081287
At time: 233.69569730758667 and batch: 700, loss is 4.510259389877319 and perplexity is 90.94540576889442
At time: 234.13944005966187 and batch: 750, loss is 4.454474382400512 and perplexity is 86.01093010316004
At time: 234.58322715759277 and batch: 800, loss is 4.547691850662232 and perplexity is 94.41423447010934
At time: 235.0272102355957 and batch: 850, loss is 4.446683330535889 and perplexity is 85.34341817235999
At time: 235.4709930419922 and batch: 900, loss is 4.47205415725708 and perplexity is 87.53635188979085
At time: 235.91520738601685 and batch: 950, loss is 4.435909051895141 and perplexity is 84.42884021110135
At time: 236.36198329925537 and batch: 1000, loss is 4.343813400268555 and perplexity is 77.00061434039581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.87486229873285 and perplexity of 130.95611914564293
Finished 24 epochs...
Completing Train Step...
At time: 237.76153445243835 and batch: 50, loss is 4.5525490665435795 and perplexity is 94.87394033070397
At time: 238.21993160247803 and batch: 100, loss is 4.466887922286987 and perplexity is 87.08528469032089
At time: 238.66549491882324 and batch: 150, loss is 4.489365768432617 and perplexity is 89.06494016509939
At time: 239.11020970344543 and batch: 200, loss is 4.52466347694397 and perplexity is 92.26487134720645
At time: 239.55460929870605 and batch: 250, loss is 4.560300245285034 and perplexity is 95.61218262793663
At time: 239.99913954734802 and batch: 300, loss is 4.471835441589356 and perplexity is 87.51720841170273
At time: 240.44350790977478 and batch: 350, loss is 4.461329298019409 and perplexity is 86.60255321789569
At time: 240.88831329345703 and batch: 400, loss is 4.462526016235351 and perplexity is 86.70625410887511
At time: 241.33248233795166 and batch: 450, loss is 4.511395349502563 and perplexity is 91.04877477834427
At time: 241.7783088684082 and batch: 500, loss is 4.5465657424926755 and perplexity is 94.30797367115635
At time: 242.2233374118805 and batch: 550, loss is 4.478911628723145 and perplexity is 88.13869283261516
At time: 242.66862440109253 and batch: 600, loss is 4.421844568252563 and perplexity is 83.24970357991256
At time: 243.11229920387268 and batch: 650, loss is 4.401909246444702 and perplexity is 81.60652699415628
At time: 243.572172164917 and batch: 700, loss is 4.501380796432495 and perplexity is 90.14151248707228
At time: 244.01856637001038 and batch: 750, loss is 4.4479309177398685 and perplexity is 85.44995797384338
At time: 244.46498847007751 and batch: 800, loss is 4.5382595634460445 and perplexity is 93.52787904419972
At time: 244.9119951725006 and batch: 850, loss is 4.437842998504639 and perplexity is 84.59227907044489
At time: 245.35582756996155 and batch: 900, loss is 4.461901998519897 and perplexity is 86.65216474838529
At time: 245.8013710975647 and batch: 950, loss is 4.423635206222534 and perplexity is 83.39890720510282
At time: 246.2468616962433 and batch: 1000, loss is 4.3352259349823 and perplexity is 76.34220531621567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.871393343297447 and perplexity of 130.5026252341834
Finished 25 epochs...
Completing Train Step...
At time: 247.6680839061737 and batch: 50, loss is 4.542093772888183 and perplexity is 93.88717288488279
At time: 248.11182522773743 and batch: 100, loss is 4.457081880569458 and perplexity is 86.23549609638519
At time: 248.55689406394958 and batch: 150, loss is 4.4799462509155275 and perplexity is 88.22993027022754
At time: 249.00157284736633 and batch: 200, loss is 4.5153624534606935 and perplexity is 91.41069214039823
At time: 249.4452931880951 and batch: 250, loss is 4.5511251640319825 and perplexity is 94.73894522152801
At time: 249.88894271850586 and batch: 300, loss is 4.4593491363525395 and perplexity is 86.43123583579553
At time: 250.33370208740234 and batch: 350, loss is 4.452026128768921 and perplexity is 85.80061109337156
At time: 250.77775168418884 and batch: 400, loss is 4.45314242362976 and perplexity is 85.89644335317003
At time: 251.2217502593994 and batch: 450, loss is 4.501804838180542 and perplexity is 90.17974435698045
At time: 251.66613674163818 and batch: 500, loss is 4.538453502655029 and perplexity is 93.54601952609806
At time: 252.11150097846985 and batch: 550, loss is 4.468679189682007 and perplexity is 87.24141751744612
At time: 252.555095911026 and batch: 600, loss is 4.411812019348145 and perplexity is 82.41867250780726
At time: 252.99896836280823 and batch: 650, loss is 4.392735757827759 and perplexity is 80.86133368471835
At time: 253.44404673576355 and batch: 700, loss is 4.492112474441528 and perplexity is 89.30991164974517
At time: 253.88810014724731 and batch: 750, loss is 4.44039870262146 and perplexity is 84.80874840427565
At time: 254.3324956893921 and batch: 800, loss is 4.530605955123901 and perplexity is 92.8147856405444
At time: 254.77751445770264 and batch: 850, loss is 4.427837476730347 and perplexity is 83.75010937967951
At time: 255.23758912086487 and batch: 900, loss is 4.453258085250854 and perplexity is 85.9063788496212
At time: 255.68088841438293 and batch: 950, loss is 4.416675567626953 and perplexity is 82.82049605266499
At time: 256.1263554096222 and batch: 1000, loss is 4.3266870880126955 and perplexity is 75.69310613092091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.864960461128049 and perplexity of 129.66581167052883
Finished 26 epochs...
Completing Train Step...
At time: 257.52539467811584 and batch: 50, loss is 4.531084861755371 and perplexity is 92.85924590219342
At time: 257.98588728904724 and batch: 100, loss is 4.445893516540528 and perplexity is 85.27603935814335
At time: 258.4300982952118 and batch: 150, loss is 4.469335136413574 and perplexity is 87.29866201274122
At time: 258.87433218955994 and batch: 200, loss is 4.504449663162231 and perplexity is 90.4185696840766
At time: 259.31919956207275 and batch: 250, loss is 4.540801935195923 and perplexity is 93.76596420395816
At time: 259.762423992157 and batch: 300, loss is 4.447660522460938 and perplexity is 85.42685583211819
At time: 260.20638847351074 and batch: 350, loss is 4.441540050506592 and perplexity is 84.90559995004718
At time: 260.6516454219818 and batch: 400, loss is 4.442048540115357 and perplexity is 84.94878454387586
At time: 261.0953640937805 and batch: 450, loss is 4.49109824180603 and perplexity is 89.21937654226777
At time: 261.5393760204315 and batch: 500, loss is 4.528083305358887 and perplexity is 92.58094152077602
At time: 261.9839608669281 and batch: 550, loss is 4.458434796333313 and perplexity is 86.35224441597099
At time: 262.42771792411804 and batch: 600, loss is 4.400853767395019 and perplexity is 81.52043845491353
At time: 262.8710117340088 and batch: 650, loss is 4.382922773361206 and perplexity is 80.0717232271654
At time: 263.31543040275574 and batch: 700, loss is 4.481475782394409 and perplexity is 88.36498398407927
At time: 263.7587881088257 and batch: 750, loss is 4.431084642410278 and perplexity is 84.02250187298145
At time: 264.20347571372986 and batch: 800, loss is 4.520734767913819 and perplexity is 91.90310062525012
At time: 264.6494686603546 and batch: 850, loss is 4.415157966613769 and perplexity is 82.69490290818932
At time: 265.0934810638428 and batch: 900, loss is 4.442441968917847 and perplexity is 84.98221241776497
At time: 265.53737449645996 and batch: 950, loss is 4.406048974990845 and perplexity is 81.9450560893275
At time: 265.9812545776367 and batch: 1000, loss is 4.313897018432617 and perplexity is 74.73115088751105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.858554747046494 and perplexity of 128.8378641833608
Finished 27 epochs...
Completing Train Step...
At time: 267.38957166671753 and batch: 50, loss is 4.518775520324707 and perplexity is 91.72321597374591
At time: 267.8498594760895 and batch: 100, loss is 4.433928461074829 and perplexity is 84.26178671213185
At time: 268.2934947013855 and batch: 150, loss is 4.457535104751587 and perplexity is 86.27458896682191
At time: 268.73738193511963 and batch: 200, loss is 4.492981567382812 and perplexity is 89.38756400221463
At time: 269.1825258731842 and batch: 250, loss is 4.530245027542114 and perplexity is 92.78129226911345
At time: 269.6260380744934 and batch: 300, loss is 4.437264661788941 and perplexity is 84.54337039379882
At time: 270.06915616989136 and batch: 350, loss is 4.430380325317383 and perplexity is 83.96334422404168
At time: 270.5141258239746 and batch: 400, loss is 4.430896511077881 and perplexity is 84.00669609458045
At time: 270.95978713035583 and batch: 450, loss is 4.480872726440429 and perplexity is 88.31171101926815
At time: 271.4043939113617 and batch: 500, loss is 4.518198356628418 and perplexity is 91.67029193776433
At time: 271.84979486465454 and batch: 550, loss is 4.449536004066467 and perplexity is 85.58722266436497
At time: 272.2951383590698 and batch: 600, loss is 4.391360607147217 and perplexity is 80.75021358762187
At time: 272.7398467063904 and batch: 650, loss is 4.374632043838501 and perplexity is 79.41061455091896
At time: 273.1847698688507 and batch: 700, loss is 4.471597690582275 and perplexity is 87.49640358054872
At time: 273.6299855709076 and batch: 750, loss is 4.42070198059082 and perplexity is 83.15463781658492
At time: 274.0744318962097 and batch: 800, loss is 4.511649007797241 and perplexity is 91.07187298468905
At time: 274.51859426498413 and batch: 850, loss is 4.403756670951843 and perplexity is 81.75742823847789
At time: 274.96355152130127 and batch: 900, loss is 4.432721204757691 and perplexity is 84.16012251754854
At time: 275.4071397781372 and batch: 950, loss is 4.395456266403198 and perplexity is 81.08161714208363
At time: 275.8513584136963 and batch: 1000, loss is 4.303206453323364 and perplexity is 73.93648791972498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.8558130031678735 and perplexity of 128.48510756303105
Finished 28 epochs...
Completing Train Step...
At time: 277.26031017303467 and batch: 50, loss is 4.509672288894653 and perplexity is 90.89202730260678
At time: 277.7055492401123 and batch: 100, loss is 4.422698488235474 and perplexity is 83.3208225259939
At time: 278.1645119190216 and batch: 150, loss is 4.447333564758301 and perplexity is 85.3989294292169
At time: 278.6088571548462 and batch: 200, loss is 4.483882150650024 and perplexity is 88.57787872528752
At time: 279.05553460121155 and batch: 250, loss is 4.52076358795166 and perplexity is 91.90574931425536
At time: 279.49913334846497 and batch: 300, loss is 4.427538022994995 and perplexity is 83.72503385125738
At time: 279.943984746933 and batch: 350, loss is 4.421077823638916 and perplexity is 83.18589678298981
At time: 280.3895926475525 and batch: 400, loss is 4.42070122718811 and perplexity is 83.15457516767904
At time: 280.8336749076843 and batch: 450, loss is 4.471004495620727 and perplexity is 87.44451654587647
At time: 281.2786376476288 and batch: 500, loss is 4.507808809280395 and perplexity is 90.72280957848761
At time: 281.7247724533081 and batch: 550, loss is 4.439368495941162 and perplexity is 84.72142285452762
At time: 282.1691997051239 and batch: 600, loss is 4.3816382026672365 and perplexity is 79.968931473864
At time: 282.6142454147339 and batch: 650, loss is 4.365298843383789 and perplexity is 78.67290730692379
At time: 283.0590033531189 and batch: 700, loss is 4.461160717010498 and perplexity is 86.58795490263383
At time: 283.5059320926666 and batch: 750, loss is 4.412704133987427 and perplexity is 82.49223221907067
At time: 283.95232105255127 and batch: 800, loss is 4.504161739349366 and perplexity is 90.39253977223488
At time: 284.3982925415039 and batch: 850, loss is 4.395868334770203 and perplexity is 81.11503519644212
At time: 284.8448724746704 and batch: 900, loss is 4.424081468582154 and perplexity is 83.43613330390741
At time: 285.2885272502899 and batch: 950, loss is 4.387047262191772 and perplexity is 80.40266015895764
At time: 285.7328083515167 and batch: 1000, loss is 4.293936700820923 and perplexity is 73.25428180180509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.852590421350991 and perplexity of 128.07172023665225
Finished 29 epochs...
Completing Train Step...
At time: 287.12764739990234 and batch: 50, loss is 4.500270729064941 and perplexity is 90.04150485347323
At time: 287.5875720977783 and batch: 100, loss is 4.412725639343262 and perplexity is 82.4940062629538
At time: 288.0319130420685 and batch: 150, loss is 4.438865041732788 and perplexity is 84.67878023285644
At time: 288.4764931201935 and batch: 200, loss is 4.475022706985474 and perplexity is 87.79659398310491
At time: 288.9205768108368 and batch: 250, loss is 4.511566495895385 and perplexity is 91.06435878125338
At time: 289.37971448898315 and batch: 300, loss is 4.417165117263794 and perplexity is 82.86105072238253
At time: 289.8245499134064 and batch: 350, loss is 4.410221090316773 and perplexity is 82.2876544968006
At time: 290.269695520401 and batch: 400, loss is 4.411244592666626 and perplexity is 82.37191921975696
At time: 290.7145857810974 and batch: 450, loss is 4.460777978897095 and perplexity is 86.5548207333902
At time: 291.15912652015686 and batch: 500, loss is 4.4975522518157955 and perplexity is 89.79706147846582
At time: 291.60454630851746 and batch: 550, loss is 4.4298924493789675 and perplexity is 83.92239051966166
At time: 292.05021262168884 and batch: 600, loss is 4.368653154373169 and perplexity is 78.93724379000864
At time: 292.49458956718445 and batch: 650, loss is 4.355082659721375 and perplexity is 77.87326205293967
At time: 292.9399003982544 and batch: 700, loss is 4.449446096420288 and perplexity is 85.57952806453905
At time: 293.3855092525482 and batch: 750, loss is 4.402472143173218 and perplexity is 81.65247597227831
At time: 293.83057260513306 and batch: 800, loss is 4.49437593460083 and perplexity is 89.51229002789022
At time: 294.27554178237915 and batch: 850, loss is 4.3851389312744145 and perplexity is 80.24937158591969
At time: 294.7218396663666 and batch: 900, loss is 4.413106260299682 and perplexity is 82.52541118682349
At time: 295.16857624053955 and batch: 950, loss is 4.375174522399902 and perplexity is 79.45370479357025
At time: 295.6122479438782 and batch: 1000, loss is 4.282439804077148 and perplexity is 72.41690771316686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.847721006812119 and perplexity of 127.4496018455467
Finished 30 epochs...
Completing Train Step...
At time: 297.0078282356262 and batch: 50, loss is 4.4893004417419435 and perplexity is 89.05912203734492
At time: 297.47058272361755 and batch: 100, loss is 4.401574602127075 and perplexity is 81.5792224025345
At time: 297.91261196136475 and batch: 150, loss is 4.427475318908692 and perplexity is 83.71978411410069
At time: 298.35808086395264 and batch: 200, loss is 4.46459659576416 and perplexity is 86.88597229984742
At time: 298.80240273475647 and batch: 250, loss is 4.499949207305908 and perplexity is 90.0125592040242
At time: 299.24683141708374 and batch: 300, loss is 4.404249048233032 and perplexity is 81.79769365078458
At time: 299.69398498535156 and batch: 350, loss is 4.398497686386109 and perplexity is 81.32859578521234
At time: 300.13840675354004 and batch: 400, loss is 4.399619102478027 and perplexity is 81.41985013874601
At time: 300.5822739601135 and batch: 450, loss is 4.451042394638062 and perplexity is 85.71624760621475
At time: 301.0414664745331 and batch: 500, loss is 4.486410369873047 and perplexity is 88.80210634987422
At time: 301.4858136177063 and batch: 550, loss is 4.418006882667542 and perplexity is 82.93082965282333
At time: 301.9297459125519 and batch: 600, loss is 4.356300287246704 and perplexity is 77.96814043184298
At time: 302.37511229515076 and batch: 650, loss is 4.3420664501190185 and perplexity is 76.8662155338473
At time: 302.81989002227783 and batch: 700, loss is 4.4377114200592045 and perplexity is 84.58114928210524
At time: 303.2633442878723 and batch: 750, loss is 4.390416421890259 and perplexity is 80.67400640896498
At time: 303.7072470188141 and batch: 800, loss is 4.483755521774292 and perplexity is 88.56666291822727
At time: 304.1501274108887 and batch: 850, loss is 4.373677225112915 and perplexity is 79.33482799610404
At time: 304.5944709777832 and batch: 900, loss is 4.400207872390747 and perplexity is 81.4678018116718
At time: 305.03779673576355 and batch: 950, loss is 4.362584714889526 and perplexity is 78.45966843722366
At time: 305.4821753501892 and batch: 1000, loss is 4.270808610916138 and perplexity is 71.57949218309305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.841127721274772 and perplexity of 126.61205435793853
Finished 31 epochs...
Completing Train Step...
At time: 306.8931317329407 and batch: 50, loss is 4.478496170043945 and perplexity is 88.10208245328104
At time: 307.33758902549744 and batch: 100, loss is 4.38898699760437 and perplexity is 80.55877140444227
At time: 307.78282499313354 and batch: 150, loss is 4.414692163467407 and perplexity is 82.65639233209666
At time: 308.22835898399353 and batch: 200, loss is 4.451126480102539 and perplexity is 85.72345539973907
At time: 308.67310214042664 and batch: 250, loss is 4.488018636703491 and perplexity is 88.94503873789145
At time: 309.1175801753998 and batch: 300, loss is 4.392650289535522 and perplexity is 80.85442289995105
At time: 309.5635278224945 and batch: 350, loss is 4.384721050262451 and perplexity is 80.21584390309084
At time: 310.0095191001892 and batch: 400, loss is 4.389009571075439 and perplexity is 80.56058991606292
At time: 310.45562386512756 and batch: 450, loss is 4.440813856124878 and perplexity is 84.84396436280468
At time: 310.8995728492737 and batch: 500, loss is 4.472908039093017 and perplexity is 87.61112951173683
At time: 311.3452453613281 and batch: 550, loss is 4.406495561599732 and perplexity is 81.98165982680263
At time: 311.7894010543823 and batch: 600, loss is 4.346808185577393 and perplexity is 77.23156029315933
At time: 312.2498872280121 and batch: 650, loss is 4.331039342880249 and perplexity is 76.02325975572907
At time: 312.6956329345703 and batch: 700, loss is 4.426842889785767 and perplexity is 83.66685402349151
At time: 313.14025807380676 and batch: 750, loss is 4.378797073364257 and perplexity is 79.74205184911537
At time: 313.5854289531708 and batch: 800, loss is 4.475150508880615 and perplexity is 87.80781527123858
At time: 314.03153014183044 and batch: 850, loss is 4.363486337661743 and perplexity is 78.53044136143288
At time: 314.47671818733215 and batch: 900, loss is 4.386698398590088 and perplexity is 80.37461548951623
At time: 314.9210240840912 and batch: 950, loss is 4.350805282592773 and perplexity is 77.54088011113303
At time: 315.367901802063 and batch: 1000, loss is 4.259920001029968 and perplexity is 70.80431895511234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.83549946110423 and perplexity of 125.90145038314382
Finished 32 epochs...
Completing Train Step...
At time: 316.7630021572113 and batch: 50, loss is 4.46675124168396 and perplexity is 87.07338263450328
At time: 317.2205922603607 and batch: 100, loss is 4.375999670028687 and perplexity is 79.51929288588684
At time: 317.6639792919159 and batch: 150, loss is 4.401816024780273 and perplexity is 81.59891985246236
At time: 318.1089744567871 and batch: 200, loss is 4.438004865646362 and perplexity is 84.60597288912962
At time: 318.55361342430115 and batch: 250, loss is 4.475638856887818 and perplexity is 87.85070651491738
At time: 318.99694991111755 and batch: 300, loss is 4.37912088394165 and perplexity is 79.76787735002664
At time: 319.44051122665405 and batch: 350, loss is 4.3711826038360595 and perplexity is 79.13716429673168
At time: 319.8851878643036 and batch: 400, loss is 4.376821718215942 and perplexity is 79.58468845192611
At time: 320.3293604850769 and batch: 450, loss is 4.42961935043335 and perplexity is 83.89947453260538
At time: 320.7734320163727 and batch: 500, loss is 4.46105299949646 and perplexity is 86.57862836571084
At time: 321.2182524204254 and batch: 550, loss is 4.393295793533325 and perplexity is 80.90663160182308
At time: 321.66188645362854 and batch: 600, loss is 4.3330738067626955 and perplexity is 76.17808377045905
At time: 322.10540986061096 and batch: 650, loss is 4.3166285991668705 and perplexity is 74.93556411804214
At time: 322.5507926940918 and batch: 700, loss is 4.413469696044922 and perplexity is 82.55540932200653
At time: 322.9958233833313 and batch: 750, loss is 4.365155391693115 and perplexity is 78.66162235480247
At time: 323.4410545825958 and batch: 800, loss is 4.461502227783203 and perplexity is 86.61753067195409
At time: 323.9129285812378 and batch: 850, loss is 4.351069779396057 and perplexity is 77.56139213860948
At time: 324.35774850845337 and batch: 900, loss is 4.370052843093872 and perplexity is 79.04780871997673
At time: 324.80211114883423 and batch: 950, loss is 4.336393265724182 and perplexity is 76.43137395390484
At time: 325.24668860435486 and batch: 1000, loss is 4.2452538871765135 and perplexity is 69.77347248526468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.827378435832698 and perplexity of 124.88314197356043
Finished 33 epochs...
Completing Train Step...
At time: 326.6421887874603 and batch: 50, loss is 4.4541832637786865 and perplexity is 85.98589436408818
At time: 327.1007812023163 and batch: 100, loss is 4.363014478683471 and perplexity is 78.49339480867047
At time: 327.5447099208832 and batch: 150, loss is 4.389815292358398 and perplexity is 80.62552545438574
At time: 327.9894485473633 and batch: 200, loss is 4.426627883911133 and perplexity is 83.64886709208156
At time: 328.43242859840393 and batch: 250, loss is 4.463589563369751 and perplexity is 86.79851935248783
At time: 328.87588477134705 and batch: 300, loss is 4.366101303100586 and perplexity is 78.73606448299661
At time: 329.3191282749176 and batch: 350, loss is 4.3580690860748295 and perplexity is 78.10617242669753
At time: 329.76388788223267 and batch: 400, loss is 4.362637395858765 and perplexity is 78.46380187747891
At time: 330.207537651062 and batch: 450, loss is 4.415235319137573 and perplexity is 82.70129981504026
At time: 330.6513891220093 and batch: 500, loss is 4.447637119293213 and perplexity is 85.42485659647728
At time: 331.0975513458252 and batch: 550, loss is 4.374841103553772 and perplexity is 79.42721784686616
At time: 331.5415451526642 and batch: 600, loss is 4.31928286075592 and perplexity is 75.13472690567615
At time: 331.984806060791 and batch: 650, loss is 4.300173850059509 and perplexity is 73.71260752692632
At time: 332.43006563186646 and batch: 700, loss is 4.396240606307983 and perplexity is 81.14523763673745
At time: 332.873916387558 and batch: 750, loss is 4.351668634414673 and perplexity is 77.60785407813681
At time: 333.3187663555145 and batch: 800, loss is 4.448030433654785 and perplexity is 85.45846202772807
At time: 333.76279878616333 and batch: 850, loss is 4.334188318252563 and perplexity is 76.26303244944356
At time: 334.20660066604614 and batch: 900, loss is 4.353629388809204 and perplexity is 77.76017330056303
At time: 334.65071153640747 and batch: 950, loss is 4.31974449634552 and perplexity is 75.16941977674658
At time: 335.1094071865082 and batch: 1000, loss is 4.2277755498886105 and perplexity is 68.56454400565003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.819709963914825 and perplexity of 123.92914164372111
Finished 34 epochs...
Completing Train Step...
At time: 336.5213131904602 and batch: 50, loss is 4.4403876686096195 and perplexity is 84.80781262870428
At time: 336.96623849868774 and batch: 100, loss is 4.346684112548828 and perplexity is 77.2219785340042
At time: 337.4103343486786 and batch: 150, loss is 4.373346824645996 and perplexity is 79.30862006168648
At time: 337.8537266254425 and batch: 200, loss is 4.4095994567871095 and perplexity is 82.23651762753104
At time: 338.2990942001343 and batch: 250, loss is 4.448351678848266 and perplexity is 85.48591955796044
At time: 338.7425379753113 and batch: 300, loss is 4.349829683303833 and perplexity is 77.46526817310651
At time: 339.1865026950836 and batch: 350, loss is 4.3423545455932615 and perplexity is 76.88836353288073
At time: 339.6328353881836 and batch: 400, loss is 4.345326948165893 and perplexity is 77.11724670039071
At time: 340.07752084732056 and batch: 450, loss is 4.402568264007568 and perplexity is 81.66032485361004
At time: 340.5233519077301 and batch: 500, loss is 4.433818073272705 and perplexity is 84.25248575205937
At time: 340.9689795970917 and batch: 550, loss is 4.35867449760437 and perplexity is 78.15347312076095
At time: 341.41336607933044 and batch: 600, loss is 4.303256039619446 and perplexity is 73.94015424720526
At time: 341.85814213752747 and batch: 650, loss is 4.285439224243164 and perplexity is 72.6344425225813
At time: 342.30304861068726 and batch: 700, loss is 4.3806427955627445 and perplexity is 79.88936943621445
At time: 342.7475724220276 and batch: 750, loss is 4.339799060821533 and perplexity is 76.69212733684803
At time: 343.1902596950531 and batch: 800, loss is 4.435177392959595 and perplexity is 84.36708968865857
At time: 343.633606672287 and batch: 850, loss is 4.320759272575378 and perplexity is 75.24573863388004
At time: 344.07906341552734 and batch: 900, loss is 4.337786283493042 and perplexity is 76.53791840785061
At time: 344.5233201980591 and batch: 950, loss is 4.30515573501587 and perplexity is 74.08075152155418
At time: 344.9680881500244 and batch: 1000, loss is 4.214691729545593 and perplexity is 67.67330097583871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.816375360256288 and perplexity of 123.51657532946405
Finished 35 epochs...
Completing Train Step...
At time: 346.36378622055054 and batch: 50, loss is 4.427817306518555 and perplexity is 83.74842013927191
At time: 346.82502341270447 and batch: 100, loss is 4.333978357315064 and perplexity is 76.24702187251063
At time: 347.269250869751 and batch: 150, loss is 4.358593502044678 and perplexity is 78.1471432928111
At time: 347.71349143981934 and batch: 200, loss is 4.3970489406585695 and perplexity is 81.2108566371788
At time: 348.1587960720062 and batch: 250, loss is 4.436805963516235 and perplexity is 84.50459938860683
At time: 348.6046850681305 and batch: 300, loss is 4.336100988388061 and perplexity is 76.40903805982633
At time: 349.04992866516113 and batch: 350, loss is 4.331384611129761 and perplexity is 76.0495127054413
At time: 349.49565029144287 and batch: 400, loss is 4.333453369140625 and perplexity is 76.20700359317223
At time: 349.94227623939514 and batch: 450, loss is 4.391065311431885 and perplexity is 80.72637191588232
At time: 350.3873107433319 and batch: 500, loss is 4.4229340076446535 and perplexity is 83.3404485079467
At time: 350.83174300193787 and batch: 550, loss is 4.347730894088745 and perplexity is 77.30285539842872
At time: 351.27683329582214 and batch: 600, loss is 4.290920686721802 and perplexity is 73.03367869333601
At time: 351.72210812568665 and batch: 650, loss is 4.275722937583923 and perplexity is 71.93212295210402
At time: 352.1668510437012 and batch: 700, loss is 4.371026563644409 and perplexity is 79.1248166818353
At time: 352.6131274700165 and batch: 750, loss is 4.329167795181275 and perplexity is 75.88111165882826
At time: 353.0569574832916 and batch: 800, loss is 4.424113693237305 and perplexity is 83.43882204785199
At time: 353.50204253196716 and batch: 850, loss is 4.311501631736755 and perplexity is 74.55235511091328
At time: 353.94785356521606 and batch: 900, loss is 4.326860485076904 and perplexity is 75.70623223128557
At time: 354.39224791526794 and batch: 950, loss is 4.295550360679626 and perplexity is 73.37258472049162
At time: 354.83630633354187 and batch: 1000, loss is 4.2024071455001835 and perplexity is 66.84704809968221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.813789925924161 and perplexity of 123.19764380093963
Finished 36 epochs...
Completing Train Step...
At time: 356.23542737960815 and batch: 50, loss is 4.4164487075805665 and perplexity is 82.80170952212589
At time: 356.69467401504517 and batch: 100, loss is 4.319977350234986 and perplexity is 75.1869253065467
At time: 357.13933277130127 and batch: 150, loss is 4.344373931884766 and perplexity is 77.04378771809243
At time: 357.5839674472809 and batch: 200, loss is 4.3843991088867185 and perplexity is 80.19002326053874
At time: 358.0285110473633 and batch: 250, loss is 4.426839923858642 and perplexity is 83.66660587406774
At time: 358.4882438182831 and batch: 300, loss is 4.322782163619995 and perplexity is 75.39810662467153
At time: 358.93245100975037 and batch: 350, loss is 4.317693796157837 and perplexity is 75.01542778317354
At time: 359.377854347229 and batch: 400, loss is 4.320213775634766 and perplexity is 75.20470350694681
At time: 359.8292498588562 and batch: 450, loss is 4.377057199478149 and perplexity is 79.60343136153051
At time: 360.285719871521 and batch: 500, loss is 4.409197692871094 and perplexity is 82.20348459835341
At time: 360.73931789398193 and batch: 550, loss is 4.334833045005798 and perplexity is 76.3122171203794
At time: 361.18512630462646 and batch: 600, loss is 4.275512924194336 and perplexity is 71.91701782933718
At time: 361.63031458854675 and batch: 650, loss is 4.261882972717285 and perplexity is 70.94344233151226
At time: 362.0754020214081 and batch: 700, loss is 4.353674821853637 and perplexity is 77.76370626222773
At time: 362.5231008529663 and batch: 750, loss is 4.317215309143067 and perplexity is 74.97954246108684
At time: 362.96815633773804 and batch: 800, loss is 4.41255937576294 and perplexity is 82.48029165426898
At time: 363.4116687774658 and batch: 850, loss is 4.300036196708679 and perplexity is 73.70246143783943
At time: 363.86000061035156 and batch: 900, loss is 4.313471374511718 and perplexity is 74.6993487960976
At time: 364.3088023662567 and batch: 950, loss is 4.284120860099793 and perplexity is 72.53874697263723
At time: 364.75623083114624 and batch: 1000, loss is 4.191947302818298 and perplexity is 66.15148258743662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.80475337330888 and perplexity of 122.08937680882713
Finished 37 epochs...
Completing Train Step...
At time: 366.16690397262573 and batch: 50, loss is 4.405594902038574 and perplexity is 81.90785550231442
At time: 366.6118965148926 and batch: 100, loss is 4.309298810958862 and perplexity is 74.3883103809384
At time: 367.05625677108765 and batch: 150, loss is 4.331521530151367 and perplexity is 76.05992604319027
At time: 367.4991774559021 and batch: 200, loss is 4.374574384689331 and perplexity is 79.40603593444975
At time: 367.94417810440063 and batch: 250, loss is 4.417032241821289 and perplexity is 82.85004125506148
At time: 368.38987827301025 and batch: 300, loss is 4.3111203670501705 and perplexity is 74.5239363484873
At time: 368.835168838501 and batch: 350, loss is 4.307508487701416 and perplexity is 74.25525040450745
At time: 369.2784504890442 and batch: 400, loss is 4.3109898853302 and perplexity is 74.51421297146888
At time: 369.7377784252167 and batch: 450, loss is 4.367718238830566 and perplexity is 78.86347862137345
At time: 370.1816644668579 and batch: 500, loss is 4.3992616558074955 and perplexity is 81.39075208520971
At time: 370.62574434280396 and batch: 550, loss is 4.326161327362061 and perplexity is 75.65332013406524
At time: 371.07077074050903 and batch: 600, loss is 4.265553469657898 and perplexity is 71.20431849924556
At time: 371.51484084129333 and batch: 650, loss is 4.255254144668579 and perplexity is 70.47472568846456
At time: 371.9586019515991 and batch: 700, loss is 4.344421977996826 and perplexity is 77.04748946147718
At time: 372.4035964012146 and batch: 750, loss is 4.308005471229553 and perplexity is 74.29216321260523
At time: 372.8470513820648 and batch: 800, loss is 4.403194274902344 and perplexity is 81.7114611108964
At time: 373.29092359542847 and batch: 850, loss is 4.292381601333618 and perplexity is 73.14045263651029
At time: 373.7359616756439 and batch: 900, loss is 4.3054358100891115 and perplexity is 74.10150259925655
At time: 374.180997133255 and batch: 950, loss is 4.274633693695068 and perplexity is 71.85381398329142
At time: 374.62514758110046 and batch: 1000, loss is 4.181397533416748 and perplexity is 65.45726804094991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.804868372475228 and perplexity of 122.10341779271543
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 376.02296590805054 and batch: 50, loss is 4.39720871925354 and perplexity is 81.22383343042794
At time: 376.4827299118042 and batch: 100, loss is 4.291728343963623 and perplexity is 73.0926886995512
At time: 376.92796778678894 and batch: 150, loss is 4.309475450515747 and perplexity is 74.40145145970455
At time: 377.372403383255 and batch: 200, loss is 4.34770565032959 and perplexity is 77.30090400839535
At time: 377.81740856170654 and batch: 250, loss is 4.388394708633423 and perplexity is 80.51107146009632
At time: 378.26483273506165 and batch: 300, loss is 4.278320088386535 and perplexity is 72.11918433112092
At time: 378.7093036174774 and batch: 350, loss is 4.270744247436523 and perplexity is 71.57488522616859
At time: 379.15366411209106 and batch: 400, loss is 4.27102029800415 and perplexity is 71.59464624125734
At time: 379.59950971603394 and batch: 450, loss is 4.323887910842895 and perplexity is 75.48152398244018
At time: 380.04397988319397 and batch: 500, loss is 4.352462711334229 and perplexity is 77.66950515844695
At time: 380.488646030426 and batch: 550, loss is 4.276785173416138 and perplexity is 72.0085724270884
At time: 380.93468046188354 and batch: 600, loss is 4.215056247711182 and perplexity is 67.69797361991006
At time: 381.3933777809143 and batch: 650, loss is 4.193891334533691 and perplexity is 66.28020825045196
At time: 381.8376920223236 and batch: 700, loss is 4.281897268295288 and perplexity is 72.37762960537371
At time: 382.2827081680298 and batch: 750, loss is 4.244773750305176 and perplexity is 69.7399797096849
At time: 382.72759079933167 and batch: 800, loss is 4.3317769289016725 and perplexity is 76.07935413409952
At time: 383.17199540138245 and batch: 850, loss is 4.218799471855164 and perplexity is 67.95185718436132
At time: 383.61711716651917 and batch: 900, loss is 4.226755175590515 and perplexity is 68.49461818850637
At time: 384.06315326690674 and batch: 950, loss is 4.182200813293457 and perplexity is 65.50986967123868
At time: 384.5084939002991 and batch: 1000, loss is 4.0905532550811765 and perplexity is 59.77295224739293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.763836744354992 and perplexity of 117.19471055662034
Finished 39 epochs...
Completing Train Step...
At time: 385.90254831314087 and batch: 50, loss is 4.368014898300171 and perplexity is 78.88687768971845
At time: 386.36184668540955 and batch: 100, loss is 4.267756204605103 and perplexity is 71.36133561004661
At time: 386.8065724372864 and batch: 150, loss is 4.291193056106567 and perplexity is 73.05357354071954
At time: 387.25144481658936 and batch: 200, loss is 4.330499181747436 and perplexity is 75.9822060344298
At time: 387.6971354484558 and batch: 250, loss is 4.371632919311524 and perplexity is 79.17280901157987
At time: 388.14290380477905 and batch: 300, loss is 4.262246360778809 and perplexity is 70.96922701613845
At time: 388.58807826042175 and batch: 350, loss is 4.256024622917176 and perplexity is 70.52904585524959
At time: 389.03261256217957 and batch: 400, loss is 4.258232450485229 and perplexity is 70.68493385062115
At time: 389.4777066707611 and batch: 450, loss is 4.312096519470215 and perplexity is 74.59671858680477
At time: 389.9219899177551 and batch: 500, loss is 4.341148481369019 and perplexity is 76.79568712644614
At time: 390.36597299575806 and batch: 550, loss is 4.267064023017883 and perplexity is 71.31195769870992
At time: 390.8104405403137 and batch: 600, loss is 4.206255922317505 and perplexity is 67.10482320994527
At time: 391.25405955314636 and batch: 650, loss is 4.187873363494873 and perplexity is 65.8825336739763
At time: 391.698189496994 and batch: 700, loss is 4.276623992919922 and perplexity is 71.99696698496341
At time: 392.1427686214447 and batch: 750, loss is 4.240096755027771 and perplexity is 69.4145677222799
At time: 392.60219740867615 and batch: 800, loss is 4.327848420143128 and perplexity is 75.78106203024349
At time: 393.04586362838745 and batch: 850, loss is 4.217481622695923 and perplexity is 67.86236586748223
At time: 393.49017333984375 and batch: 900, loss is 4.225581941604614 and perplexity is 68.41430509683303
At time: 393.93611884117126 and batch: 950, loss is 4.184250073432922 and perplexity is 65.64425408317831
At time: 394.3809812068939 and batch: 1000, loss is 4.0952241277694705 and perplexity is 60.05279714871488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7605702469988564 and perplexity of 116.81251889828695
Finished 40 epochs...
Completing Train Step...
At time: 395.7933123111725 and batch: 50, loss is 4.360846290588379 and perplexity is 78.32339073151067
At time: 396.23846983909607 and batch: 100, loss is 4.260284962654114 and perplexity is 70.83016453038942
At time: 396.68335604667664 and batch: 150, loss is 4.283729472160339 and perplexity is 72.51036173711084
At time: 397.1316440105438 and batch: 200, loss is 4.3235765552520755 and perplexity is 75.45802604624137
At time: 397.5781216621399 and batch: 250, loss is 4.365660309791565 and perplexity is 78.70135006035761
At time: 398.0243935585022 and batch: 300, loss is 4.2565470838546755 and perplexity is 70.56590415434043
At time: 398.46873784065247 and batch: 350, loss is 4.249884533882141 and perplexity is 70.09731801420284
At time: 398.91307520866394 and batch: 400, loss is 4.252833213806152 and perplexity is 70.30431760634569
At time: 399.357054233551 and batch: 450, loss is 4.306728253364563 and perplexity is 74.1973365045949
At time: 399.8005311489105 and batch: 500, loss is 4.33621922492981 and perplexity is 76.41807293436048
At time: 400.2448921203613 and batch: 550, loss is 4.262277202606201 and perplexity is 70.97141587054222
At time: 400.6893901824951 and batch: 600, loss is 4.202441325187683 and perplexity is 66.84933294994407
At time: 401.1342463493347 and batch: 650, loss is 4.185193605422974 and perplexity is 65.70622076604549
At time: 401.57986640930176 and batch: 700, loss is 4.274927120208741 and perplexity is 71.87490089100021
At time: 402.02508020401 and batch: 750, loss is 4.2383716106414795 and perplexity is 69.29492080422511
At time: 402.4691832065582 and batch: 800, loss is 4.326042423248291 and perplexity is 75.64432517786018
At time: 402.91205048561096 and batch: 850, loss is 4.217240619659424 and perplexity is 67.84601280189254
At time: 403.3567724227905 and batch: 900, loss is 4.225253381729126 and perplexity is 68.391830593581
At time: 403.8010287284851 and batch: 950, loss is 4.185225830078125 and perplexity is 65.70833816046694
At time: 404.2720470428467 and batch: 1000, loss is 4.096632380485534 and perplexity is 60.13742623899371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.7588865698837655 and perplexity of 116.6160098088117
Finished 41 epochs...
Completing Train Step...
At time: 405.6710274219513 and batch: 50, loss is 4.355977325439453 and perplexity is 77.94296376607073
At time: 406.13177943229675 and batch: 100, loss is 4.255144414901733 and perplexity is 70.46699293751045
At time: 406.5757188796997 and batch: 150, loss is 4.278176879882812 and perplexity is 72.1088569901423
At time: 407.0210642814636 and batch: 200, loss is 4.318794937133789 and perplexity is 75.09807583976402
At time: 407.46504616737366 and batch: 250, loss is 4.3617931079864505 and perplexity is 78.39758379863201
At time: 407.90987253189087 and batch: 300, loss is 4.252738614082336 and perplexity is 70.29766715188758
At time: 408.3531973361969 and batch: 350, loss is 4.24569333076477 and perplexity is 69.80414072836662
At time: 408.79722142219543 and batch: 400, loss is 4.249295635223389 and perplexity is 70.05604995018857
At time: 409.2424113750458 and batch: 450, loss is 4.302914619445801 and perplexity is 73.91491389593058
At time: 409.68631958961487 and batch: 500, loss is 4.332537250518799 and perplexity is 76.1372209075564
At time: 410.13031029701233 and batch: 550, loss is 4.258810629844666 and perplexity is 70.7258142373558
At time: 410.5755863189697 and batch: 600, loss is 4.199675054550171 and perplexity is 66.66466514139596
At time: 411.0194582939148 and batch: 650, loss is 4.183025288581848 and perplexity is 65.56390321152685
At time: 411.4628665447235 and batch: 700, loss is 4.273530673980713 and perplexity is 71.77460150441033
At time: 411.90857577323914 and batch: 750, loss is 4.237183437347412 and perplexity is 69.21263532429649
At time: 412.3530330657959 and batch: 800, loss is 4.324263110160827 and perplexity is 75.50984991235735
At time: 412.79722476005554 and batch: 850, loss is 4.216932725906372 and perplexity is 67.82512665390364
At time: 413.2420337200165 and batch: 900, loss is 4.223853693008423 and perplexity is 68.29617028265763
At time: 413.68658661842346 and batch: 950, loss is 4.185457129478454 and perplexity is 65.72353821749434
At time: 414.1314902305603 and batch: 1000, loss is 4.096729965209961 and perplexity is 60.14329501950797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.75770717713891 and perplexity of 116.47855480555889
Finished 42 epochs...
Completing Train Step...
At time: 415.5308973789215 and batch: 50, loss is 4.352176780700684 and perplexity is 77.647300242314
At time: 415.991482257843 and batch: 100, loss is 4.250877828598022 and perplexity is 70.16697990144849
At time: 416.43921160697937 and batch: 150, loss is 4.2736313533782955 and perplexity is 71.78182809182964
At time: 416.8835780620575 and batch: 200, loss is 4.315033340454102 and perplexity is 74.81611780570293
At time: 417.3286259174347 and batch: 250, loss is 4.3585664081573485 and perplexity is 78.14502601159843
At time: 417.77450227737427 and batch: 300, loss is 4.249598116874695 and perplexity is 70.07724382507902
At time: 418.2200186252594 and batch: 350, loss is 4.242234177589417 and perplexity is 69.56309466130524
At time: 418.66538310050964 and batch: 400, loss is 4.24642991065979 and perplexity is 69.85557599578269
At time: 419.11110663414 and batch: 450, loss is 4.2993394947052 and perplexity is 73.65113066849558
At time: 419.55618166923523 and batch: 500, loss is 4.329598455429077 and perplexity is 75.91379767495752
At time: 420.0014154911041 and batch: 550, loss is 4.25616759300232 and perplexity is 70.53913011979756
At time: 420.4476613998413 and batch: 600, loss is 4.19757604598999 and perplexity is 66.52488219272725
At time: 420.8920214176178 and batch: 650, loss is 4.180985822677612 and perplexity is 65.43032412767279
At time: 421.33660912513733 and batch: 700, loss is 4.271849055290222 and perplexity is 71.6540054197518
At time: 421.7826499938965 and batch: 750, loss is 4.236049184799194 and perplexity is 69.13417522150853
At time: 422.22733664512634 and batch: 800, loss is 4.322899942398071 and perplexity is 75.40698744451483
At time: 422.6723327636719 and batch: 850, loss is 4.2164649438858035 and perplexity is 67.79340669870292
At time: 423.11833691596985 and batch: 900, loss is 4.2228464508056645 and perplexity is 68.22741413052417
At time: 423.5630805492401 and batch: 950, loss is 4.185219349861145 and perplexity is 65.70791235755792
At time: 424.0078363418579 and batch: 1000, loss is 4.0962442207336425 and perplexity is 60.114087840351914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.756702330054306 and perplexity of 116.361570454879
Finished 43 epochs...
Completing Train Step...
At time: 425.4169623851776 and batch: 50, loss is 4.348970394134522 and perplexity is 77.39873169829777
At time: 425.86321806907654 and batch: 100, loss is 4.247606768608093 and perplexity is 69.93783447940775
At time: 426.3080344200134 and batch: 150, loss is 4.270056238174439 and perplexity is 71.52565797854334
At time: 426.7522237300873 and batch: 200, loss is 4.311881246566773 and perplexity is 74.58066166297967
At time: 427.2126350402832 and batch: 250, loss is 4.3558305692672725 and perplexity is 77.93152599436232
At time: 427.6604766845703 and batch: 300, loss is 4.247155728340149 and perplexity is 69.90629681271825
At time: 428.1069304943085 and batch: 350, loss is 4.239365181922913 and perplexity is 69.3638044622308
At time: 428.5516257286072 and batch: 400, loss is 4.243855257034301 and perplexity is 69.67595341597182
At time: 428.9972541332245 and batch: 450, loss is 4.296572279930115 and perplexity is 73.44760390252007
At time: 429.44173789024353 and batch: 500, loss is 4.327101945877075 and perplexity is 75.724514525838
At time: 429.88633131980896 and batch: 550, loss is 4.253872556686401 and perplexity is 70.37742588400337
At time: 430.33140444755554 and batch: 600, loss is 4.195699172019959 and perplexity is 66.40014047184326
At time: 430.7776367664337 and batch: 650, loss is 4.179139657020569 and perplexity is 65.30964034574335
At time: 431.2218699455261 and batch: 700, loss is 4.2701270341873165 and perplexity is 71.5307218891969
At time: 431.66711711883545 and batch: 750, loss is 4.234879450798035 and perplexity is 69.05335390504627
At time: 432.1117432117462 and batch: 800, loss is 4.321608991622925 and perplexity is 75.30970354349631
At time: 432.5557758808136 and batch: 850, loss is 4.21569128036499 and perplexity is 67.74097769680672
At time: 433.0000584125519 and batch: 900, loss is 4.221637001037598 and perplexity is 68.14494638068129
At time: 433.444931268692 and batch: 950, loss is 4.184673361778259 and perplexity is 65.67204641253961
At time: 433.88994789123535 and batch: 1000, loss is 4.095558285713196 and perplexity is 60.07286762109218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.755772660418255 and perplexity of 116.25344290526142
Finished 44 epochs...
Completing Train Step...
At time: 435.28539848327637 and batch: 50, loss is 4.346191577911377 and perplexity is 77.18395339992021
At time: 435.74438095092773 and batch: 100, loss is 4.244657769203186 and perplexity is 69.731891659025
At time: 436.1890068054199 and batch: 150, loss is 4.266962232589722 and perplexity is 71.30469919343221
At time: 436.6331250667572 and batch: 200, loss is 4.308995485305786 and perplexity is 74.36574991986717
At time: 437.0765597820282 and batch: 250, loss is 4.3533524227142335 and perplexity is 77.73863935124432
At time: 437.52166390419006 and batch: 300, loss is 4.245040874481202 and perplexity is 69.75861143268195
At time: 437.9658682346344 and batch: 350, loss is 4.2366945886611935 and perplexity is 69.17880908707791
At time: 438.4100294113159 and batch: 400, loss is 4.24157380104065 and perplexity is 69.51717198972783
At time: 438.86967396736145 and batch: 450, loss is 4.29409761428833 and perplexity is 73.26607035073498
At time: 439.3131437301636 and batch: 500, loss is 4.324854917526245 and perplexity is 75.5545504234383
At time: 439.75673484802246 and batch: 550, loss is 4.251736583709717 and perplexity is 70.22726203420123
At time: 440.20108795166016 and batch: 600, loss is 4.194082927703858 and perplexity is 66.2929083022527
At time: 440.64818024635315 and batch: 650, loss is 4.177383146286011 and perplexity is 65.19502395333684
At time: 441.0922043323517 and batch: 700, loss is 4.26845199584961 and perplexity is 71.41100548048699
At time: 441.5377697944641 and batch: 750, loss is 4.233818941116333 and perplexity is 68.98016097244269
At time: 441.9816448688507 and batch: 800, loss is 4.320394582748413 and perplexity is 75.21830228166117
At time: 442.425265789032 and batch: 850, loss is 4.214945406913757 and perplexity is 67.69047033837252
At time: 442.8702929019928 and batch: 900, loss is 4.220590486526489 and perplexity is 68.0736690083426
At time: 443.3148863315582 and batch: 950, loss is 4.184011034965515 and perplexity is 65.628564456575
At time: 443.75849962234497 and batch: 1000, loss is 4.094643020629883 and perplexity is 60.01791017705546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.755059591153773 and perplexity of 116.17057569677651
Finished 45 epochs...
Completing Train Step...
At time: 445.15303349494934 and batch: 50, loss is 4.343739719390869 and perplexity is 76.99494107655693
At time: 445.61240816116333 and batch: 100, loss is 4.2419751310348515 and perplexity is 69.54507691512663
At time: 446.0582616329193 and batch: 150, loss is 4.264152107238769 and perplexity is 71.10460532668559
At time: 446.5030953884125 and batch: 200, loss is 4.306438570022583 and perplexity is 74.17584588507776
At time: 446.9471890926361 and batch: 250, loss is 4.351167259216308 and perplexity is 77.56895317769197
At time: 447.3923192024231 and batch: 300, loss is 4.243026685714722 and perplexity is 69.6182458300338
At time: 447.8367884159088 and batch: 350, loss is 4.234297180175782 and perplexity is 69.01315786932034
At time: 448.28225207328796 and batch: 400, loss is 4.239459552764893 and perplexity is 69.37035069174259
At time: 448.7274053096771 and batch: 450, loss is 4.291817708015442 and perplexity is 73.09922085023712
At time: 449.1732704639435 and batch: 500, loss is 4.322859363555908 and perplexity is 75.40392757835667
At time: 449.6179666519165 and batch: 550, loss is 4.2496741676330565 and perplexity is 70.08257345527447
At time: 450.07849168777466 and batch: 600, loss is 4.192602353096008 and perplexity is 66.19482933007438
At time: 450.5237293243408 and batch: 650, loss is 4.175661339759826 and perplexity is 65.08286731933988
At time: 450.9682266712189 and batch: 700, loss is 4.266839370727539 and perplexity is 71.29593910345844
At time: 451.4128384590149 and batch: 750, loss is 4.232714757919312 and perplexity is 68.90403627331035
At time: 451.85809421539307 and batch: 800, loss is 4.319134697914124 and perplexity is 75.1235955556668
At time: 452.30296874046326 and batch: 850, loss is 4.214222865104675 and perplexity is 67.64157880869084
At time: 452.74780654907227 and batch: 900, loss is 4.21950526714325 and perplexity is 67.99983421396782
At time: 453.19343638420105 and batch: 950, loss is 4.183225240707397 and perplexity is 65.57701416407103
At time: 453.6408734321594 and batch: 1000, loss is 4.09356876373291 and perplexity is 59.9534701418791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.754400113733803 and perplexity of 116.09398908158695
Finished 46 epochs...
Completing Train Step...
At time: 455.0536699295044 and batch: 50, loss is 4.341470727920532 and perplexity is 76.82043825956318
At time: 455.49741411209106 and batch: 100, loss is 4.2395042085647585 and perplexity is 69.37344854940784
At time: 455.94317507743835 and batch: 150, loss is 4.261553697586059 and perplexity is 70.92008626572576
At time: 456.3871862888336 and batch: 200, loss is 4.304060277938842 and perplexity is 73.99964367122368
At time: 456.83079171180725 and batch: 250, loss is 4.34910936832428 and perplexity is 77.40948887179125
At time: 457.2761387825012 and batch: 300, loss is 4.241060156822204 and perplexity is 69.48147406505379
At time: 457.7198793888092 and batch: 350, loss is 4.232022967338562 and perplexity is 68.8563855940867
At time: 458.1638083457947 and batch: 400, loss is 4.237515678405762 and perplexity is 69.23563442391884
At time: 458.608300447464 and batch: 450, loss is 4.289704136848449 and perplexity is 72.94488360355012
At time: 459.0523817539215 and batch: 500, loss is 4.320963487625122 and perplexity is 75.26110651526112
At time: 459.4957664012909 and batch: 550, loss is 4.247726831436157 and perplexity is 69.94623191770407
At time: 459.9401640892029 and batch: 600, loss is 4.191162314414978 and perplexity is 66.09957481690748
At time: 460.385014295578 and batch: 650, loss is 4.1739438533782955 and perplexity is 64.97118431552016
At time: 460.8290972709656 and batch: 700, loss is 4.26527696609497 and perplexity is 71.18463297316914
At time: 461.28746819496155 and batch: 750, loss is 4.231530175209046 and perplexity is 68.82246206850873
At time: 461.7325277328491 and batch: 800, loss is 4.31781729221344 and perplexity is 75.02469246467817
At time: 462.17626881599426 and batch: 850, loss is 4.213449053764343 and perplexity is 67.58925723405537
At time: 462.6200706958771 and batch: 900, loss is 4.218349342346191 and perplexity is 67.92127693130665
At time: 463.0652205944061 and batch: 950, loss is 4.182370839118957 and perplexity is 65.52100898786684
At time: 463.5091543197632 and batch: 1000, loss is 4.092456436157226 and perplexity is 59.88681931942833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.753716445550686 and perplexity of 116.0146464401109
Finished 47 epochs...
Completing Train Step...
At time: 464.90838837623596 and batch: 50, loss is 4.339368028640747 and perplexity is 76.65907768519632
At time: 465.36695170402527 and batch: 100, loss is 4.237107400894165 and perplexity is 69.20737284105934
At time: 465.8102214336395 and batch: 150, loss is 4.259070410728454 and perplexity is 70.74418983859667
At time: 466.2550992965698 and batch: 200, loss is 4.301846952438354 and perplexity is 73.83603949429633
At time: 466.7004942893982 and batch: 250, loss is 4.347169413566589 and perplexity is 77.25946353379886
At time: 467.1431405544281 and batch: 300, loss is 4.23909776687622 and perplexity is 69.34525801713156
At time: 467.58620834350586 and batch: 350, loss is 4.22989324092865 and perplexity is 68.70989637754221
At time: 468.02888798713684 and batch: 400, loss is 4.235581121444702 and perplexity is 69.10182361943465
At time: 468.47289657592773 and batch: 450, loss is 4.287658710479736 and perplexity is 72.79583270345157
At time: 468.9173505306244 and batch: 500, loss is 4.319163599014282 and perplexity is 75.1257667416009
At time: 469.36076164245605 and batch: 550, loss is 4.245913243293762 and perplexity is 69.8194932215292
At time: 469.8031029701233 and batch: 600, loss is 4.189720630645752 and perplexity is 66.00434879214193
At time: 470.24632239341736 and batch: 650, loss is 4.172271914482117 and perplexity is 64.86264722430803
At time: 470.6893699169159 and batch: 700, loss is 4.263652529716492 and perplexity is 71.0690919357185
At time: 471.13295817375183 and batch: 750, loss is 4.230333218574524 and perplexity is 68.74013384741625
At time: 471.5760838985443 and batch: 800, loss is 4.316473865509034 and perplexity is 74.92396996113037
At time: 472.0203444957733 and batch: 850, loss is 4.212631640434265 and perplexity is 67.53403144844381
At time: 472.4675269126892 and batch: 900, loss is 4.21714572429657 and perplexity is 67.83957483536093
At time: 472.9286346435547 and batch: 950, loss is 4.181479759216309 and perplexity is 65.46265053843872
At time: 473.3870565891266 and batch: 1000, loss is 4.091290473937988 and perplexity is 59.81703424195932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.753085997046494 and perplexity of 115.94152823085301
Finished 48 epochs...
Completing Train Step...
At time: 474.7972927093506 and batch: 50, loss is 4.337339096069336 and perplexity is 76.50369926505738
At time: 475.256178855896 and batch: 100, loss is 4.234814286231995 and perplexity is 69.04885421981703
At time: 475.7004191875458 and batch: 150, loss is 4.2566636228561405 and perplexity is 70.5741283135564
At time: 476.1461396217346 and batch: 200, loss is 4.299703798294067 and perplexity is 73.67796692769821
At time: 476.58962225914 and batch: 250, loss is 4.345366077423096 and perplexity is 77.12026430000942
At time: 477.03327083587646 and batch: 300, loss is 4.237120618820191 and perplexity is 69.20828762503974
At time: 477.47846031188965 and batch: 350, loss is 4.22784197807312 and perplexity is 68.5690987751109
At time: 477.92244958877563 and batch: 400, loss is 4.233793296813965 and perplexity is 68.97839204701884
At time: 478.3659975528717 and batch: 450, loss is 4.285690541267395 and perplexity is 72.6526990885296
At time: 478.81168246269226 and batch: 500, loss is 4.317496280670166 and perplexity is 75.00061253754053
At time: 479.25537753105164 and batch: 550, loss is 4.244205565452575 and perplexity is 69.70036576468068
At time: 479.699462890625 and batch: 600, loss is 4.188289365768433 and perplexity is 65.90994665930931
At time: 480.1441330909729 and batch: 650, loss is 4.170584197044373 and perplexity is 64.75326972864515
At time: 480.58998250961304 and batch: 700, loss is 4.2620506095886235 and perplexity is 70.95533606511296
At time: 481.034542798996 and batch: 750, loss is 4.229180703163147 and perplexity is 68.66095541972041
At time: 481.4793722629547 and batch: 800, loss is 4.315146203041077 and perplexity is 74.8245622228263
At time: 481.9242630004883 and batch: 850, loss is 4.211739420890808 and perplexity is 67.47380313817027
At time: 482.3685393333435 and batch: 900, loss is 4.215856094360351 and perplexity is 67.75214327808733
At time: 482.81301760673523 and batch: 950, loss is 4.180591068267822 and perplexity is 65.40450031605653
At time: 483.258017539978 and batch: 1000, loss is 4.090065937042237 and perplexity is 59.743830905775944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.752314869950458 and perplexity of 115.85215703958056
Finished 49 epochs...
Completing Train Step...
At time: 484.66933822631836 and batch: 50, loss is 4.335416641235351 and perplexity is 76.35676564046953
At time: 485.1134192943573 and batch: 100, loss is 4.232644515037537 and perplexity is 68.89919642522099
At time: 485.5576648712158 and batch: 150, loss is 4.254363579750061 and perplexity is 70.41199130878933
At time: 486.0035617351532 and batch: 200, loss is 4.297776117324829 and perplexity is 73.53607611704551
At time: 486.4484040737152 and batch: 250, loss is 4.343654608726501 and perplexity is 76.98838826482998
At time: 486.8945622444153 and batch: 300, loss is 4.235224304199218 and perplexity is 69.07717129552157
At time: 487.34241819381714 and batch: 350, loss is 4.225935025215149 and perplexity is 68.43846533172935
At time: 487.78794527053833 and batch: 400, loss is 4.2319333410263065 and perplexity is 68.85021452671981
At time: 488.23402428627014 and batch: 450, loss is 4.28378077507019 and perplexity is 72.51408182508716
At time: 488.6788547039032 and batch: 500, loss is 4.315921125411987 and perplexity is 74.8825679220406
At time: 489.12506461143494 and batch: 550, loss is 4.242539887428284 and perplexity is 69.58436403472794
At time: 489.5699625015259 and batch: 600, loss is 4.186866974830627 and perplexity is 65.81626359123833
At time: 490.01525020599365 and batch: 650, loss is 4.168893642425537 and perplexity is 64.6438932689099
At time: 490.4614956378937 and batch: 700, loss is 4.260473728179932 and perplexity is 70.84353608564537
At time: 490.905841588974 and batch: 750, loss is 4.228226857185364 and perplexity is 68.59549466826296
At time: 491.3503460884094 and batch: 800, loss is 4.31365629196167 and perplexity is 74.71316328642081
At time: 491.7964334487915 and batch: 850, loss is 4.210801253318786 and perplexity is 67.4105310886384
At time: 492.2433211803436 and batch: 900, loss is 4.214470105171204 and perplexity is 67.65830458469136
At time: 492.688036441803 and batch: 950, loss is 4.179587163925171 and perplexity is 65.33887340124438
At time: 493.1349630355835 and batch: 1000, loss is 4.088743195533753 and perplexity is 59.66485750297462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.751711961699695 and perplexity of 115.78232987003628
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
SETTINGS FOR THIS RUN
{'anneal': 5.917012875567329, 'dropout': 0.44532173091606186, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 0.44540625695229985}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7221782207489014 and batch: 50, loss is 8.205210399627685 and perplexity is 3659.9706221480237
At time: 1.1877126693725586 and batch: 100, loss is 6.819025115966797 and perplexity is 915.0924660543179
At time: 1.6373004913330078 and batch: 150, loss is 6.298810930252075 and perplexity is 543.9247609710194
At time: 2.0878183841705322 and batch: 200, loss is 6.157183446884155 and perplexity is 472.09651559945536
At time: 2.538325071334839 and batch: 250, loss is 6.163247709274292 and perplexity is 474.96813105796554
At time: 2.988334894180298 and batch: 300, loss is 6.027842578887939 and perplexity is 414.81912377133244
At time: 3.4392900466918945 and batch: 350, loss is 5.970163269042969 and perplexity is 391.5695967244927
At time: 3.9040753841400146 and batch: 400, loss is 5.921509294509888 and perplexity is 372.97421721617445
At time: 4.354646444320679 and batch: 450, loss is 5.934880743026733 and perplexity is 377.9949149541046
At time: 4.805250883102417 and batch: 500, loss is 5.915942630767822 and perplexity is 370.90376327147914
At time: 5.256540775299072 and batch: 550, loss is 5.848726682662964 and perplexity is 346.79252229459917
At time: 5.707326173782349 and batch: 600, loss is 5.743608493804931 and perplexity is 312.1889126397887
At time: 6.157410144805908 and batch: 650, loss is 5.704861440658569 and perplexity is 300.32386448867743
At time: 6.608021974563599 and batch: 700, loss is 5.782112560272217 and perplexity is 324.44387418390534
At time: 7.057900667190552 and batch: 750, loss is 5.662054071426391 and perplexity is 287.7390725629634
At time: 7.509567499160767 and batch: 800, loss is 5.7508350849151615 and perplexity is 314.4531457501553
At time: 7.9610817432403564 and batch: 850, loss is 5.705980596542358 and perplexity is 300.6601618581507
At time: 8.41081190109253 and batch: 900, loss is 5.7288187217712405 and perplexity is 307.60568577393576
At time: 8.86025357246399 and batch: 950, loss is 5.6800181007385255 and perplexity is 292.9547325715735
At time: 9.311663150787354 and batch: 1000, loss is 5.591990203857422 and perplexity is 268.26899876723985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.600388689738948 and perplexity of 270.5315398264788
Finished 1 epochs...
Completing Train Step...
At time: 10.720640659332275 and batch: 50, loss is 5.572297925949097 and perplexity is 263.03784678475336
At time: 11.164472579956055 and batch: 100, loss is 5.539811592102051 and perplexity is 254.63002063195972
At time: 11.607915163040161 and batch: 150, loss is 5.483558645248413 and perplexity is 240.70175722698875
At time: 12.052961349487305 and batch: 200, loss is 5.472299871444702 and perplexity is 238.00694917147771
At time: 12.498387575149536 and batch: 250, loss is 5.536585102081299 and perplexity is 253.80978336665422
At time: 12.943280935287476 and batch: 300, loss is 5.438816938400269 and perplexity is 230.1697173662977
At time: 13.388453722000122 and batch: 350, loss is 5.435194940567016 and perplexity is 229.3375511106605
At time: 13.832653760910034 and batch: 400, loss is 5.418524112701416 and perplexity is 225.5459962629789
At time: 14.275891542434692 and batch: 450, loss is 5.44353196144104 and perplexity is 231.2575354152371
At time: 14.719112396240234 and batch: 500, loss is 5.455203790664672 and perplexity is 233.972547632283
At time: 15.179045915603638 and batch: 550, loss is 5.384593782424926 and perplexity is 218.02152194291426
At time: 15.622681856155396 and batch: 600, loss is 5.2873608684539795 and perplexity is 197.82066114369158
At time: 16.06647300720215 and batch: 650, loss is 5.257296562194824 and perplexity is 191.9618323062543
At time: 16.51210355758667 and batch: 700, loss is 5.331641778945923 and perplexity is 206.77717804862957
At time: 16.95616102218628 and batch: 750, loss is 5.248719644546509 and perplexity is 190.32243202951872
At time: 17.39970898628235 and batch: 800, loss is 5.327819423675537 and perplexity is 205.9883108381615
At time: 17.843767166137695 and batch: 850, loss is 5.269710569381714 and perplexity is 194.35970065551265
At time: 18.287418603897095 and batch: 900, loss is 5.310493850708008 and perplexity is 202.45018387121422
At time: 18.730894565582275 and batch: 950, loss is 5.255043172836304 and perplexity is 191.52975455862503
At time: 19.17620325088501 and batch: 1000, loss is 5.168427886962891 and perplexity is 175.63849675098587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.318146496284299 and perplexity of 204.0054065694201
Finished 2 epochs...
Completing Train Step...
At time: 20.563672065734863 and batch: 50, loss is 5.297623481750488 and perplexity is 199.86127117696716
At time: 21.023452043533325 and batch: 100, loss is 5.257037725448608 and perplexity is 191.91215196000945
At time: 21.468608856201172 and batch: 150, loss is 5.239241323471069 and perplexity is 188.52701711185006
At time: 21.9136803150177 and batch: 200, loss is 5.241715078353882 and perplexity is 188.9939640589958
At time: 22.358655214309692 and batch: 250, loss is 5.312753353118897 and perplexity is 202.90813772884414
At time: 22.80382537841797 and batch: 300, loss is 5.2174609088897705 and perplexity is 184.46521495753808
At time: 23.24801754951477 and batch: 350, loss is 5.221700563430786 and perplexity is 185.24894393992287
At time: 23.693833351135254 and batch: 400, loss is 5.224326848983765 and perplexity is 185.7360999905376
At time: 24.13860845565796 and batch: 450, loss is 5.249595556259155 and perplexity is 190.48921070794867
At time: 24.582062005996704 and batch: 500, loss is 5.272388820648193 and perplexity is 194.88094246664264
At time: 25.029295682907104 and batch: 550, loss is 5.19928674697876 and perplexity is 181.14299499930837
At time: 25.47525429725647 and batch: 600, loss is 5.105628137588501 and perplexity is 164.94764781557075
At time: 25.921531677246094 and batch: 650, loss is 5.082563982009888 and perplexity is 161.1868066322356
At time: 26.36896300315857 and batch: 700, loss is 5.158325252532959 and perplexity is 173.87301823435482
At time: 26.828579664230347 and batch: 750, loss is 5.091204195022583 and perplexity is 162.5855289039712
At time: 27.273717641830444 and batch: 800, loss is 5.175405893325806 and perplexity is 176.86838940773018
At time: 27.718887090682983 and batch: 850, loss is 5.107789545059204 and perplexity is 165.3045524630746
At time: 28.16366481781006 and batch: 900, loss is 5.151977033615112 and perplexity is 172.77273037990298
At time: 28.60917615890503 and batch: 950, loss is 5.1009005928039555 and perplexity is 164.16969078569048
At time: 29.053934812545776 and batch: 1000, loss is 5.0164120388031 and perplexity is 150.8690193408122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.181213006740663 and perplexity of 177.89847221361282
Finished 3 epochs...
Completing Train Step...
At time: 30.440627813339233 and batch: 50, loss is 5.159636535644531 and perplexity is 174.10116453622925
At time: 30.899688482284546 and batch: 100, loss is 5.110341424942017 and perplexity is 165.7269285223141
At time: 31.343210697174072 and batch: 150, loss is 5.1097392559051515 and perplexity is 165.6271629382707
At time: 31.787516832351685 and batch: 200, loss is 5.116363487243652 and perplexity is 166.72795751520388
At time: 32.23212647438049 and batch: 250, loss is 5.185750284194946 and perplexity is 178.70748090126483
At time: 32.67604064941406 and batch: 300, loss is 5.087961778640747 and perplexity is 162.05921265093235
At time: 33.119664669036865 and batch: 350, loss is 5.0941555786132815 and perplexity is 163.06608997726258
At time: 33.564608335494995 and batch: 400, loss is 5.103155717849732 and perplexity is 164.54033173090062
At time: 34.00743269920349 and batch: 450, loss is 5.129622011184693 and perplexity is 168.95324353735597
At time: 34.44993805885315 and batch: 500, loss is 5.156346559524536 and perplexity is 173.529317060515
At time: 34.89406371116638 and batch: 550, loss is 5.080413408279419 and perplexity is 160.8405349949954
At time: 35.33824634552002 and batch: 600, loss is 4.990320024490356 and perplexity is 146.98345427483792
At time: 35.78311085700989 and batch: 650, loss is 4.969839391708374 and perplexity is 144.00375731607969
At time: 36.228039026260376 and batch: 700, loss is 5.044302005767822 and perplexity is 155.1359773813407
At time: 36.67176365852356 and batch: 750, loss is 4.986538181304931 and perplexity is 146.4286356796469
At time: 37.115450382232666 and batch: 800, loss is 5.0730992031097415 and perplexity is 159.668406143396
At time: 37.559176206588745 and batch: 850, loss is 4.997919378280639 and perplexity is 148.10468847693102
At time: 38.01954174041748 and batch: 900, loss is 5.040484304428101 and perplexity is 154.54484365663268
At time: 38.46322178840637 and batch: 950, loss is 4.994643774032593 and perplexity is 147.6203498141536
At time: 38.90786004066467 and batch: 1000, loss is 4.909565572738647 and perplexity is 135.58050174669194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.0860141661109 and perplexity of 161.74389125941562
Finished 4 epochs...
Completing Train Step...
At time: 40.3089017868042 and batch: 50, loss is 5.061020212173462 and perplexity is 157.75137412713482
At time: 40.75422143936157 and batch: 100, loss is 5.003368721008301 and perplexity is 148.91396469310394
At time: 41.1973774433136 and batch: 150, loss is 5.014598865509033 and perplexity is 150.5957155125187
At time: 41.64057540893555 and batch: 200, loss is 5.024246988296508 and perplexity is 152.05571326036
At time: 42.08456563949585 and batch: 250, loss is 5.0900617122650145 and perplexity is 162.39988380892132
At time: 42.527289390563965 and batch: 300, loss is 4.990118227005005 and perplexity is 146.95379637592237
At time: 42.97047567367554 and batch: 350, loss is 4.997365226745606 and perplexity is 148.02263877255035
At time: 43.414923906326294 and batch: 400, loss is 5.00794472694397 and perplexity is 149.596957376919
At time: 43.85811924934387 and batch: 450, loss is 5.035968198776245 and perplexity is 153.84847643225459
At time: 44.300782918930054 and batch: 500, loss is 5.065560293197632 and perplexity is 158.46920642272178
At time: 44.74565029144287 and batch: 550, loss is 4.986932287216186 and perplexity is 146.48635544364825
At time: 45.18941330909729 and batch: 600, loss is 4.899884986877441 and perplexity is 134.27433548620598
At time: 45.63232946395874 and batch: 650, loss is 4.882537622451782 and perplexity is 131.96511699222913
At time: 46.07587504386902 and batch: 700, loss is 4.955560073852539 and perplexity is 141.96209336813408
At time: 46.520514249801636 and batch: 750, loss is 4.90298469543457 and perplexity is 134.69119252681315
At time: 46.96412253379822 and batch: 800, loss is 4.99207932472229 and perplexity is 147.24226990047342
At time: 47.40815234184265 and batch: 850, loss is 4.911251640319824 and perplexity is 135.80929245942752
At time: 47.85289192199707 and batch: 900, loss is 4.951531343460083 and perplexity is 141.39131689227966
At time: 48.29580545425415 and batch: 950, loss is 4.9100696849823 and perplexity is 135.6488667679974
At time: 48.73998427391052 and batch: 1000, loss is 4.82395583152771 and perplexity is 124.4564470147558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.013120325600228 and perplexity of 150.37321826310298
Finished 5 epochs...
Completing Train Step...
At time: 50.14214873313904 and batch: 50, loss is 4.980146236419678 and perplexity is 145.4956568619051
At time: 50.601592779159546 and batch: 100, loss is 4.916112442016601 and perplexity is 136.47104151115929
At time: 51.04668140411377 and batch: 150, loss is 4.936985206604004 and perplexity is 139.3495056812839
At time: 51.49182367324829 and batch: 200, loss is 4.949767036437988 and perplexity is 141.14207912964267
At time: 51.937599658966064 and batch: 250, loss is 5.0112802696228025 and perplexity is 150.09677753519796
At time: 52.381803035736084 and batch: 300, loss is 4.909655570983887 and perplexity is 135.5927043030339
At time: 52.82781100273132 and batch: 350, loss is 4.918221426010132 and perplexity is 136.7591604656268
At time: 53.272526264190674 and batch: 400, loss is 4.929649095535279 and perplexity is 138.3309628698657
At time: 53.71622443199158 and batch: 450, loss is 4.959974403381348 and perplexity is 142.59014602468574
At time: 54.15954613685608 and batch: 500, loss is 4.990899906158448 and perplexity is 147.06871200279232
At time: 54.603882789611816 and batch: 550, loss is 4.911333608627319 and perplexity is 135.820424973523
At time: 55.04815435409546 and batch: 600, loss is 4.82657036781311 and perplexity is 124.78226866208337
At time: 55.49237108230591 and batch: 650, loss is 4.811387157440185 and perplexity is 122.9019837287108
At time: 55.93684244155884 and batch: 700, loss is 4.882655000686645 and perplexity is 131.98060773384537
At time: 56.38181018829346 and batch: 750, loss is 4.834150609970092 and perplexity is 125.73174255003705
At time: 56.82441830635071 and batch: 800, loss is 4.925888719558716 and perplexity is 137.81176324498188
At time: 57.26749849319458 and batch: 850, loss is 4.840528964996338 and perplexity is 126.53626728672288
At time: 57.71226763725281 and batch: 900, loss is 4.8777525520324705 and perplexity is 131.3351630036258
At time: 58.15640139579773 and batch: 950, loss is 4.841398363113403 and perplexity is 126.64632551451811
At time: 58.59962248802185 and batch: 1000, loss is 4.75274842262268 and perplexity is 115.90239594166603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.953282426043255 and perplexity of 141.63912166469186
Finished 6 epochs...
Completing Train Step...
At time: 59.988505125045776 and batch: 50, loss is 4.912555494308472 and perplexity is 135.98648343757543
At time: 60.448219299316406 and batch: 100, loss is 4.844194450378418 and perplexity is 127.00093522126144
At time: 60.89188098907471 and batch: 150, loss is 4.87238320350647 and perplexity is 130.63186854592854
At time: 61.350849628448486 and batch: 200, loss is 4.887833995819092 and perplexity is 132.66590770973758
At time: 61.795780658721924 and batch: 250, loss is 4.945576162338257 and perplexity is 140.55180818537346
At time: 62.23986458778381 and batch: 300, loss is 4.842253332138061 and perplexity is 126.7546505005751
At time: 62.683791160583496 and batch: 350, loss is 4.852454271316528 and perplexity is 128.05428445449644
At time: 63.12946820259094 and batch: 400, loss is 4.8642926597595215 and perplexity is 129.57924957040547
At time: 63.574403524398804 and batch: 450, loss is 4.896765813827515 and perplexity is 133.85616311270104
At time: 64.02045845985413 and batch: 500, loss is 4.928991746902466 and perplexity is 138.24006108090413
At time: 64.46742510795593 and batch: 550, loss is 4.848364200592041 and perplexity is 127.53160300519824
At time: 64.9161159992218 and batch: 600, loss is 4.765455465316773 and perplexity is 117.38456971425062
At time: 65.36041617393494 and batch: 650, loss is 4.751500749588013 and perplexity is 115.7578778220242
At time: 65.80452632904053 and batch: 700, loss is 4.821272497177124 and perplexity is 124.12293641566689
At time: 66.24899315834045 and batch: 750, loss is 4.77619402885437 and perplexity is 118.65190386992334
At time: 66.69305372238159 and batch: 800, loss is 4.86974494934082 and perplexity is 130.28768269908159
At time: 67.13701605796814 and batch: 850, loss is 4.781435976028442 and perplexity is 119.27550389336291
At time: 67.582688331604 and batch: 900, loss is 4.815114984512329 and perplexity is 123.3609960986817
At time: 68.02635526657104 and batch: 950, loss is 4.78340051651001 and perplexity is 119.51005576711019
At time: 68.47109699249268 and batch: 1000, loss is 4.693084440231323 and perplexity is 109.18944926945088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.905693240282012 and perplexity of 135.05650417047883
Finished 7 epochs...
Completing Train Step...
At time: 69.87577629089355 and batch: 50, loss is 4.854274663925171 and perplexity is 128.28760583124503
At time: 70.32055020332336 and batch: 100, loss is 4.7832831859588625 and perplexity is 119.49603440898225
At time: 70.76542973518372 and batch: 150, loss is 4.817710704803467 and perplexity is 123.68162268783624
At time: 71.20898413658142 and batch: 200, loss is 4.835087661743164 and perplexity is 125.8496149199566
At time: 71.65416264533997 and batch: 250, loss is 4.889386663436889 and perplexity is 132.87205376536244
At time: 72.09909796714783 and batch: 300, loss is 4.784794387817382 and perplexity is 119.67675355542904
At time: 72.55814409255981 and batch: 350, loss is 4.796299486160279 and perplexity is 121.06159748158694
At time: 73.00260901451111 and batch: 400, loss is 4.808519258499145 and perplexity is 122.550018201743
At time: 73.44857954978943 and batch: 450, loss is 4.8429686546325685 and perplexity is 126.84535339040262
At time: 73.89185404777527 and batch: 500, loss is 4.876452655792236 and perplexity is 131.16455183146132
At time: 74.3360767364502 and batch: 550, loss is 4.794897527694702 and perplexity is 120.89199306709337
At time: 74.78087711334229 and batch: 600, loss is 4.7132988452911375 and perplexity is 111.41910872138813
At time: 75.22497773170471 and batch: 650, loss is 4.6996885013580325 and perplexity is 109.91292939082693
At time: 75.66896939277649 and batch: 700, loss is 4.768423881530762 and perplexity is 117.73353365312849
At time: 76.11471199989319 and batch: 750, loss is 4.7265819072723385 and perplexity is 112.90896869379232
At time: 76.56283235549927 and batch: 800, loss is 4.821359720230102 and perplexity is 124.13376326929446
At time: 77.0090000629425 and batch: 850, loss is 4.730834875106812 and perplexity is 113.39018948880246
At time: 77.4661374092102 and batch: 900, loss is 4.760942392349243 and perplexity is 116.85599822387452
At time: 77.91135597229004 and batch: 950, loss is 4.733385925292969 and perplexity is 113.67982283032681
At time: 78.35632944107056 and batch: 1000, loss is 4.642082929611206 and perplexity is 103.76024794550536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.86629951290968 and perplexity of 129.8395572078372
Finished 8 epochs...
Completing Train Step...
At time: 79.74723768234253 and batch: 50, loss is 4.803312683105469 and perplexity is 121.9136104825319
At time: 80.20622444152832 and batch: 100, loss is 4.730404682159424 and perplexity is 113.34142031980986
At time: 80.65059733390808 and batch: 150, loss is 4.7702317237854 and perplexity is 117.94656981992455
At time: 81.09479308128357 and batch: 200, loss is 4.789336814880371 and perplexity is 120.22161303494308
At time: 81.539311170578 and batch: 250, loss is 4.840475339889526 and perplexity is 126.52948194780792
At time: 81.98178696632385 and batch: 300, loss is 4.7348052024841305 and perplexity is 113.84128055949873
At time: 82.42484045028687 and batch: 350, loss is 4.74772575378418 and perplexity is 115.32171609130056
At time: 82.86998009681702 and batch: 400, loss is 4.760167236328125 and perplexity is 116.76545169162625
At time: 83.31364846229553 and batch: 450, loss is 4.796107873916626 and perplexity is 121.03840281953495
At time: 83.75659656524658 and batch: 500, loss is 4.830575294494629 and perplexity is 125.28301455521597
At time: 84.21577501296997 and batch: 550, loss is 4.748175420761108 and perplexity is 115.3735841195453
At time: 84.65930652618408 and batch: 600, loss is 4.667688474655152 and perplexity is 106.45139272806308
At time: 85.10238194465637 and batch: 650, loss is 4.654368762969971 and perplexity is 105.04289210355307
At time: 85.54600286483765 and batch: 700, loss is 4.722603092193603 and perplexity is 112.4606173316953
At time: 85.99006414413452 and batch: 750, loss is 4.683131322860718 and perplexity is 108.10806436789154
At time: 86.43351459503174 and batch: 800, loss is 4.779190645217896 and perplexity is 119.00799136897324
At time: 86.87757873535156 and batch: 850, loss is 4.686308107376099 and perplexity is 108.45204648200229
At time: 87.32267880439758 and batch: 900, loss is 4.713187046051026 and perplexity is 111.4066528459908
At time: 87.76636075973511 and batch: 950, loss is 4.689331970214844 and perplexity is 108.78048692430454
At time: 88.20901012420654 and batch: 1000, loss is 4.597596998214722 and perplexity is 99.24554165589481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.832049858279344 and perplexity of 125.46788862212267
Finished 9 epochs...
Completing Train Step...
At time: 89.59748220443726 and batch: 50, loss is 4.758164539337158 and perplexity is 116.53183987779357
At time: 90.0595030784607 and batch: 100, loss is 4.683714246749878 and perplexity is 108.17110151236608
At time: 90.50434064865112 and batch: 150, loss is 4.728037385940552 and perplexity is 113.07342494142401
At time: 90.95023584365845 and batch: 200, loss is 4.748885087966919 and perplexity is 115.45549002814286
At time: 91.39596843719482 and batch: 250, loss is 4.7972047233581545 and perplexity is 121.17123656006063
At time: 91.84031558036804 and batch: 300, loss is 4.690462989807129 and perplexity is 108.90358938880108
At time: 92.28452658653259 and batch: 350, loss is 4.704764347076416 and perplexity is 110.47224877144409
At time: 92.7304573059082 and batch: 400, loss is 4.717016820907593 and perplexity is 111.83413329836465
At time: 93.175452709198 and batch: 450, loss is 4.754950075149536 and perplexity is 116.15785385615159
At time: 93.61970949172974 and batch: 500, loss is 4.789889087677002 and perplexity is 120.2880264988761
At time: 94.06493854522705 and batch: 550, loss is 4.706571025848389 and perplexity is 110.67201704240027
At time: 94.5102870464325 and batch: 600, loss is 4.627267694473266 and perplexity is 102.23434667731976
At time: 94.9563844203949 and batch: 650, loss is 4.614151830673218 and perplexity is 100.90221006855157
At time: 95.41580963134766 and batch: 700, loss is 4.681972246170044 and perplexity is 107.9828314217237
At time: 95.8609721660614 and batch: 750, loss is 4.644481658935547 and perplexity is 104.00943944694878
At time: 96.3057689666748 and batch: 800, loss is 4.7418498516082765 and perplexity is 114.6460838870783
At time: 96.75161695480347 and batch: 850, loss is 4.6466906452178955 and perplexity is 104.23944882216932
At time: 97.19767451286316 and batch: 900, loss is 4.6706778049469 and perplexity is 106.77008720514728
At time: 97.64161610603333 and batch: 950, loss is 4.65010820388794 and perplexity is 104.5963026914406
At time: 98.08680176734924 and batch: 1000, loss is 4.557820816040039 and perplexity is 95.37541263467057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.801672586580602 and perplexity of 121.71382427276366
Finished 10 epochs...
Completing Train Step...
At time: 99.49080491065979 and batch: 50, loss is 4.717671222686768 and perplexity is 111.90734170541096
At time: 99.93541812896729 and batch: 100, loss is 4.641881685256958 and perplexity is 103.73936888237824
At time: 100.37882256507874 and batch: 150, loss is 4.690059204101562 and perplexity is 108.85962455290445
At time: 100.82237911224365 and batch: 200, loss is 4.7126584815979005 and perplexity is 111.34778280912977
At time: 101.26750040054321 and batch: 250, loss is 4.758418807983398 and perplexity is 116.56147403832247
At time: 101.71206378936768 and batch: 300, loss is 4.650446891784668 and perplexity is 104.63173419297638
At time: 102.15604305267334 and batch: 350, loss is 4.666134853363037 and perplexity is 106.286135984184
At time: 102.59980392456055 and batch: 400, loss is 4.679064483642578 and perplexity is 107.66929905065757
At time: 103.04547309875488 and batch: 450, loss is 4.718007478713989 and perplexity is 111.94497755083505
At time: 103.49024105072021 and batch: 500, loss is 4.753077955245971 and perplexity is 115.9405958559597
At time: 103.93555641174316 and batch: 550, loss is 4.668774700164795 and perplexity is 106.56708576937231
At time: 104.3794059753418 and batch: 600, loss is 4.590752124786377 and perplexity is 98.56853812991747
At time: 104.82271361351013 and batch: 650, loss is 4.57788348197937 and perplexity is 97.30822149553872
At time: 105.26630473136902 and batch: 700, loss is 4.645514793395996 and perplexity is 104.11695071028613
At time: 105.71140885353088 and batch: 750, loss is 4.609746236801147 and perplexity is 100.4586536922466
At time: 106.15497207641602 and batch: 800, loss is 4.708118448257446 and perplexity is 110.84340597296371
At time: 106.59791612625122 and batch: 850, loss is 4.610988836288453 and perplexity is 100.58356115272117
At time: 107.05762958526611 and batch: 900, loss is 4.632348566055298 and perplexity is 102.75510810473027
At time: 107.50068855285645 and batch: 950, loss is 4.614775228500366 and perplexity is 100.9651318976908
At time: 107.9446930885315 and batch: 1000, loss is 4.521827735900879 and perplexity is 92.0036026849072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.774831260122904 and perplexity of 118.49031889188548
Finished 11 epochs...
Completing Train Step...
At time: 109.32766771316528 and batch: 50, loss is 4.6808743572235105 and perplexity is 107.8643433199807
At time: 109.78730416297913 and batch: 100, loss is 4.603995666503907 and perplexity is 99.8826169993863
At time: 110.23063611984253 and batch: 150, loss is 4.655549964904785 and perplexity is 105.16704227972353
At time: 110.67461705207825 and batch: 200, loss is 4.679902248382568 and perplexity is 107.75953838737102
At time: 111.11981654167175 and batch: 250, loss is 4.723272581100463 and perplexity is 112.53593367637298
At time: 111.56410193443298 and batch: 300, loss is 4.613882474899292 and perplexity is 100.87503513569453
At time: 112.00717639923096 and batch: 350, loss is 4.630754938125611 and perplexity is 102.59148510625542
At time: 112.45201873779297 and batch: 400, loss is 4.644909954071045 and perplexity is 104.05399572484717
At time: 112.8970856666565 and batch: 450, loss is 4.684333419799804 and perplexity is 108.23809888254601
At time: 113.34107542037964 and batch: 500, loss is 4.719440679550171 and perplexity is 112.10553221232236
At time: 113.78461170196533 and batch: 550, loss is 4.634374666213989 and perplexity is 102.9635112971455
At time: 114.22824239730835 and batch: 600, loss is 4.557373924255371 and perplexity is 95.33279966870624
At time: 114.67146754264832 and batch: 650, loss is 4.544847898483276 and perplexity is 94.14610635475235
At time: 115.11474943161011 and batch: 700, loss is 4.612408695220947 and perplexity is 100.72647705668751
At time: 115.55972695350647 and batch: 750, loss is 4.577995748519897 and perplexity is 97.31914656617933
At time: 116.00432467460632 and batch: 800, loss is 4.6772076034545895 and perplexity is 107.46955556964512
At time: 116.44863271713257 and batch: 850, loss is 4.57847186088562 and perplexity is 97.36549244732916
At time: 116.8938193321228 and batch: 900, loss is 4.597006044387817 and perplexity is 99.18690944941977
At time: 117.3382031917572 and batch: 950, loss is 4.582560024261475 and perplexity is 97.76435323614295
At time: 117.78072953224182 and batch: 1000, loss is 4.489122085571289 and perplexity is 89.04323920981871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.750887614924733 and perplexity of 115.68692440880515
Finished 12 epochs...
Completing Train Step...
At time: 119.18670845031738 and batch: 50, loss is 4.647222623825074 and perplexity is 104.29491673152921
At time: 119.64614224433899 and batch: 100, loss is 4.569319982528686 and perplexity is 96.4784804085329
At time: 120.09007859230042 and batch: 150, loss is 4.623962049484253 and perplexity is 101.89695417866412
At time: 120.53381276130676 and batch: 200, loss is 4.649955816268921 and perplexity is 104.58036472432059
At time: 120.97948884963989 and batch: 250, loss is 4.691151962280274 and perplexity is 108.9786468173987
At time: 121.42392230033875 and batch: 300, loss is 4.580109348297119 and perplexity is 97.52505782301867
At time: 121.86711740493774 and batch: 350, loss is 4.5986181831359865 and perplexity is 99.34694147170622
At time: 122.3109347820282 and batch: 400, loss is 4.612722749710083 and perplexity is 100.7581156268395
At time: 122.75587773323059 and batch: 450, loss is 4.653300552368164 and perplexity is 104.93074408208052
At time: 123.19951391220093 and batch: 500, loss is 4.688434581756592 and perplexity is 108.68291235854838
At time: 123.64350914955139 and batch: 550, loss is 4.6027656841278075 and perplexity is 99.75983866387166
At time: 124.08875274658203 and batch: 600, loss is 4.526803493499756 and perplexity is 92.46253112156698
At time: 124.53244733810425 and batch: 650, loss is 4.5143108081817624 and perplexity is 91.31461104803368
At time: 124.97660684585571 and batch: 700, loss is 4.582070093154908 and perplexity is 97.71646716977376
At time: 125.42084383964539 and batch: 750, loss is 4.548773574829101 and perplexity is 94.51641988748574
At time: 125.86476612091064 and batch: 800, loss is 4.648684511184692 and perplexity is 104.44749565139543
At time: 126.30834341049194 and batch: 850, loss is 4.5485982418060305 and perplexity is 94.49984949056837
At time: 126.75264263153076 and batch: 900, loss is 4.5641387176513675 and perplexity is 95.97989261967737
At time: 127.19677305221558 and batch: 950, loss is 4.55287612915039 and perplexity is 94.90497512383047
At time: 127.6402518749237 and batch: 1000, loss is 4.459580278396606 and perplexity is 86.45121603736119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.729264049995236 and perplexity of 113.21221315319202
Finished 13 epochs...
Completing Train Step...
At time: 129.05987930297852 and batch: 50, loss is 4.6163747215271 and perplexity is 101.12675414447519
At time: 129.50633335113525 and batch: 100, loss is 4.53743987083435 and perplexity is 93.45124634467935
At time: 129.96732831001282 and batch: 150, loss is 4.594894199371338 and perplexity is 98.97766309470272
At time: 130.4127848148346 and batch: 200, loss is 4.622306270599365 and perplexity is 101.72837495697335
At time: 130.85811400413513 and batch: 250, loss is 4.661463794708252 and perplexity is 105.79082492291148
At time: 131.3041479587555 and batch: 300, loss is 4.548911170959473 and perplexity is 94.52942587588512
At time: 131.74816417694092 and batch: 350, loss is 4.568601083755493 and perplexity is 96.40914707213899
At time: 132.19227290153503 and batch: 400, loss is 4.582964582443237 and perplexity is 97.80391260662331
At time: 132.63866996765137 and batch: 450, loss is 4.624641933441162 and perplexity is 101.9662558389421
At time: 133.0826177597046 and batch: 500, loss is 4.659869918823242 and perplexity is 105.62234178448571
At time: 133.52771496772766 and batch: 550, loss is 4.573335032463074 and perplexity is 96.86662501352475
At time: 133.9724006652832 and batch: 600, loss is 4.498491125106812 and perplexity is 89.88140913078273
At time: 134.41648054122925 and batch: 650, loss is 4.48610553741455 and perplexity is 88.77504071092811
At time: 134.86161303520203 and batch: 700, loss is 4.554133739471435 and perplexity is 95.02440368161766
At time: 135.30749917030334 and batch: 750, loss is 4.521673974990844 and perplexity is 91.98945721476996
At time: 135.75175309181213 and batch: 800, loss is 4.622216472625732 and perplexity is 101.7192403651813
At time: 136.19622802734375 and batch: 850, loss is 4.520943622589112 and perplexity is 91.92229702204845
At time: 136.64201426506042 and batch: 900, loss is 4.533530511856079 and perplexity is 93.08662505787989
At time: 137.0881006717682 and batch: 950, loss is 4.525271329879761 and perplexity is 92.32097186883125
At time: 137.53247666358948 and batch: 1000, loss is 4.4321439075469975 and perplexity is 84.11155113497837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.709401572622904 and perplexity of 110.98572313370028
Finished 14 epochs...
Completing Train Step...
At time: 138.92523097991943 and batch: 50, loss is 4.587633953094483 and perplexity is 98.26166319761802
At time: 139.383606672287 and batch: 100, loss is 4.507918519973755 and perplexity is 90.73276338683961
At time: 139.82932543754578 and batch: 150, loss is 4.5679699802398686 and perplexity is 96.34832211592406
At time: 140.27353930473328 and batch: 200, loss is 4.596621170043945 and perplexity is 99.14874229797452
At time: 140.71786880493164 and batch: 250, loss is 4.633904829025268 and perplexity is 102.91514657311951
At time: 141.17701292037964 and batch: 300, loss is 4.519785604476929 and perplexity is 91.8159109475727
At time: 141.62042331695557 and batch: 350, loss is 4.540743503570557 and perplexity is 93.76048546633307
At time: 142.06430983543396 and batch: 400, loss is 4.555057735443115 and perplexity is 95.11224642475135
At time: 142.51109766960144 and batch: 450, loss is 4.597972116470337 and perplexity is 99.2827774538355
At time: 142.95644521713257 and batch: 500, loss is 4.633272705078125 and perplexity is 102.85011200156742
At time: 143.4018268585205 and batch: 550, loss is 4.545707850456238 and perplexity is 94.22710230598135
At time: 143.84649682044983 and batch: 600, loss is 4.472079439163208 and perplexity is 87.53856500359784
At time: 144.29072284698486 and batch: 650, loss is 4.459709644317627 and perplexity is 86.46240060198238
At time: 144.73483276367188 and batch: 700, loss is 4.528278713226318 and perplexity is 92.59903433280469
At time: 145.17998385429382 and batch: 750, loss is 4.496310596466064 and perplexity is 89.68563366847489
At time: 145.62343192100525 and batch: 800, loss is 4.597554521560669 and perplexity is 99.24132612688703
At time: 146.0674753189087 and batch: 850, loss is 4.495181140899658 and perplexity is 89.58439491340134
At time: 146.51210379600525 and batch: 900, loss is 4.504852809906006 and perplexity is 90.45502898475189
At time: 146.9568750858307 and batch: 950, loss is 4.499366874694824 and perplexity is 89.96015721456708
At time: 147.40022921562195 and batch: 1000, loss is 4.406432609558106 and perplexity is 81.97649907638225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.691190021794017 and perplexity of 108.9827945706352
Finished 15 epochs...
Completing Train Step...
At time: 148.78734493255615 and batch: 50, loss is 4.560804224014282 and perplexity is 95.6603812787653
At time: 149.24599647521973 and batch: 100, loss is 4.4805617523193355 and perplexity is 88.28425263219681
At time: 149.69139671325684 and batch: 150, loss is 4.542822093963623 and perplexity is 93.95557779895479
At time: 150.13439440727234 and batch: 200, loss is 4.572596273422241 and perplexity is 96.79509034523592
At time: 150.57802271842957 and batch: 250, loss is 4.608139886856079 and perplexity is 100.2974114797879
At time: 151.02341842651367 and batch: 300, loss is 4.492528371810913 and perplexity is 89.34706313212665
At time: 151.46690273284912 and batch: 350, loss is 4.514517192840576 and perplexity is 91.33345892776924
At time: 151.91046738624573 and batch: 400, loss is 4.529037275314331 and perplexity is 92.66930309787725
At time: 152.35478806495667 and batch: 450, loss is 4.572944059371948 and perplexity is 96.8287601722656
At time: 152.81352138519287 and batch: 500, loss is 4.608279247283935 and perplexity is 100.31138994396436
At time: 153.2566363811493 and batch: 550, loss is 4.519808745384216 and perplexity is 91.81803567563941
At time: 153.70265865325928 and batch: 600, loss is 4.447260341644287 and perplexity is 85.39267648260355
At time: 154.14651727676392 and batch: 650, loss is 4.434846105575562 and perplexity is 84.3391445654061
At time: 154.59005975723267 and batch: 700, loss is 4.504131965637207 and perplexity is 90.38984849083931
At time: 155.0360758304596 and batch: 750, loss is 4.472546319961548 and perplexity is 87.57944462092391
At time: 155.4818229675293 and batch: 800, loss is 4.574461498260498 and perplexity is 96.97580343484816
At time: 155.92677211761475 and batch: 850, loss is 4.471062068939209 and perplexity is 87.44955116180542
At time: 156.37102150917053 and batch: 900, loss is 4.47777307510376 and perplexity is 88.03839931049326
At time: 156.8155596256256 and batch: 950, loss is 4.475078134536743 and perplexity is 87.80146046818665
At time: 157.25912284851074 and batch: 1000, loss is 4.3824207305908205 and perplexity is 80.03153388663496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.674262814405488 and perplexity of 107.15354591853567
Finished 16 epochs...
Completing Train Step...
At time: 158.65713953971863 and batch: 50, loss is 4.535789556503296 and perplexity is 93.29714960258679
At time: 159.1030056476593 and batch: 100, loss is 4.454761667251587 and perplexity is 86.03564329009764
At time: 159.54845023155212 and batch: 150, loss is 4.519413003921509 and perplexity is 91.78170666082107
At time: 159.99332785606384 and batch: 200, loss is 4.55007682800293 and perplexity is 94.6396790131604
At time: 160.4380397796631 and batch: 250, loss is 4.583865661621093 and perplexity is 97.89208139334926
At time: 160.88320899009705 and batch: 300, loss is 4.466974353790283 and perplexity is 87.09281192768215
At time: 161.3292191028595 and batch: 350, loss is 4.489835414886475 and perplexity is 89.1067790223774
At time: 161.77324438095093 and batch: 400, loss is 4.504727096557617 and perplexity is 90.44365829491832
At time: 162.22002983093262 and batch: 450, loss is 4.54934796333313 and perplexity is 94.57072462702705
At time: 162.66476583480835 and batch: 500, loss is 4.584669284820556 and perplexity is 97.97078135932446
At time: 163.1086721420288 and batch: 550, loss is 4.495363755226135 and perplexity is 89.60075580116211
At time: 163.55394434928894 and batch: 600, loss is 4.423975338935852 and perplexity is 83.427278726466
At time: 164.0139446258545 and batch: 650, loss is 4.41136248588562 and perplexity is 82.38163088292687
At time: 164.45919466018677 and batch: 700, loss is 4.481500053405762 and perplexity is 88.36712871763606
At time: 164.9043571949005 and batch: 750, loss is 4.450158309936524 and perplexity is 85.64050067131967
At time: 165.34906435012817 and batch: 800, loss is 4.552718381881714 and perplexity is 94.89000530397605
At time: 165.79354524612427 and batch: 850, loss is 4.448396072387696 and perplexity is 85.48971466473948
At time: 166.23872232437134 and batch: 900, loss is 4.452109642028809 and perplexity is 85.80777688131933
At time: 166.68396520614624 and batch: 950, loss is 4.4522249794006346 and perplexity is 85.81767429554692
At time: 167.1287772655487 and batch: 1000, loss is 4.359933180809021 and perplexity is 78.25190551937126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.658568963771913 and perplexity of 105.48502120872405
Finished 17 epochs...
Completing Train Step...
At time: 168.52217745780945 and batch: 50, loss is 4.512230672836304 and perplexity is 91.12486171859918
At time: 168.98143196105957 and batch: 100, loss is 4.430530300140381 and perplexity is 83.97593755604775
At time: 169.426087141037 and batch: 150, loss is 4.4973909950256346 and perplexity is 89.78258226003344
At time: 169.87016367912292 and batch: 200, loss is 4.528895244598389 and perplexity is 92.65614214506431
At time: 170.31340718269348 and batch: 250, loss is 4.561021327972412 and perplexity is 95.68115178077468
At time: 170.7587468624115 and batch: 300, loss is 4.4429926013946535 and perplexity is 85.02901926937675
At time: 171.2024736404419 and batch: 350, loss is 4.4669021892547605 and perplexity is 87.08652714213409
At time: 171.64644813537598 and batch: 400, loss is 4.481931247711182 and perplexity is 88.40524033649139
At time: 172.09115958213806 and batch: 450, loss is 4.527012042999267 and perplexity is 92.48181614702725
At time: 172.5350260734558 and batch: 500, loss is 4.562458543777466 and perplexity is 95.81876511069844
At time: 172.9791407585144 and batch: 550, loss is 4.472392334938049 and perplexity is 87.56595973634768
At time: 173.4249289035797 and batch: 600, loss is 4.401826238632202 and perplexity is 81.5997532960036
At time: 173.86955547332764 and batch: 650, loss is 4.389118981361389 and perplexity is 80.56940455543928
At time: 174.31267666816711 and batch: 700, loss is 4.460257930755615 and perplexity is 86.50981976208163
At time: 174.755952835083 and batch: 750, loss is 4.429020299911499 and perplexity is 83.8492295597448
At time: 175.20026898384094 and batch: 800, loss is 4.532169427871704 and perplexity is 92.96001252804369
At time: 175.6584541797638 and batch: 850, loss is 4.4270164585113525 and perplexity is 83.68137723306917
At time: 176.10201358795166 and batch: 900, loss is 4.4279446697235105 and perplexity is 83.7590872857574
At time: 176.54724025726318 and batch: 950, loss is 4.430617914199829 and perplexity is 83.98329535115145
At time: 176.99094414710999 and batch: 1000, loss is 4.338753032684326 and perplexity is 76.61194715642475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.64399868104516 and perplexity of 103.95921731636993
Finished 18 epochs...
Completing Train Step...
At time: 178.38051652908325 and batch: 50, loss is 4.490023469924926 and perplexity is 89.12353757684866
At time: 178.8386995792389 and batch: 100, loss is 4.4076622867584225 and perplexity is 82.07736571224937
At time: 179.2833800315857 and batch: 150, loss is 4.476631498336792 and perplexity is 87.93795406312806
At time: 179.72893500328064 and batch: 200, loss is 4.508940391540527 and perplexity is 90.82552800661074
At time: 180.1726574897766 and batch: 250, loss is 4.539452152252197 and perplexity is 93.63948588311654
At time: 180.61725902557373 and batch: 300, loss is 4.420347814559936 and perplexity is 83.12519248313606
At time: 181.06266069412231 and batch: 350, loss is 4.4451089859008786 and perplexity is 85.20916392878074
At time: 181.50676703453064 and batch: 400, loss is 4.460415945053101 and perplexity is 86.52349063054484
At time: 181.95129799842834 and batch: 450, loss is 4.505832529067993 and perplexity is 90.54369293574021
At time: 182.39539885520935 and batch: 500, loss is 4.541369199752808 and perplexity is 93.81916940137785
At time: 182.83900618553162 and batch: 550, loss is 4.450626678466797 and perplexity is 85.68062138165662
At time: 183.28286480903625 and batch: 600, loss is 4.38088752746582 and perplexity is 79.90892330626238
At time: 183.72750997543335 and batch: 650, loss is 4.368021950721741 and perplexity is 78.88743403519807
At time: 184.17088842391968 and batch: 700, loss is 4.440165190696717 and perplexity is 84.78894686223877
At time: 184.61450290679932 and batch: 750, loss is 4.408993396759033 and perplexity is 82.18669246139471
At time: 185.05882477760315 and batch: 800, loss is 4.512744541168213 and perplexity is 91.17169993259274
At time: 185.50329446792603 and batch: 850, loss is 4.40674524307251 and perplexity is 82.00213168398433
At time: 185.94713306427002 and batch: 900, loss is 4.405246953964234 and perplexity is 81.8793607793485
At time: 186.3909478187561 and batch: 950, loss is 4.41009861946106 and perplexity is 82.27757727443539
At time: 186.8493194580078 and batch: 1000, loss is 4.318616976737976 and perplexity is 75.08471254556665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.630391935022866 and perplexity of 102.55425083733309
Finished 19 epochs...
Completing Train Step...
At time: 188.2562460899353 and batch: 50, loss is 4.468929185867309 and perplexity is 87.26323026546414
At time: 188.70013976097107 and batch: 100, loss is 4.386052236557007 and perplexity is 80.32269724017063
At time: 189.14434242248535 and batch: 150, loss is 4.456940326690674 and perplexity is 86.22328999135503
At time: 189.58944296836853 and batch: 200, loss is 4.490070333480835 and perplexity is 89.12771432060245
At time: 190.03346610069275 and batch: 250, loss is 4.5190427875518795 and perplexity is 91.74773385961433
At time: 190.4768090248108 and batch: 300, loss is 4.398877153396606 and perplexity is 81.35946316052784
At time: 190.92187023162842 and batch: 350, loss is 4.42435242652893 and perplexity is 83.4587440504168
At time: 191.36598992347717 and batch: 400, loss is 4.4400187110900875 and perplexity is 84.77652792023898
At time: 191.81010699272156 and batch: 450, loss is 4.485703115463257 and perplexity is 88.73932287312206
At time: 192.25457072257996 and batch: 500, loss is 4.521264019012452 and perplexity is 91.9517533158331
At time: 192.698721408844 and batch: 550, loss is 4.429905848503113 and perplexity is 83.9235150137244
At time: 193.14261078834534 and batch: 600, loss is 4.361046023368836 and perplexity is 78.33903604250503
At time: 193.58797025680542 and batch: 650, loss is 4.34797167301178 and perplexity is 77.32147053767575
At time: 194.03206419944763 and batch: 700, loss is 4.421061511039734 and perplexity is 83.18453981586589
At time: 194.47670197486877 and batch: 750, loss is 4.389971694946289 and perplexity is 80.63813648138976
At time: 194.9224636554718 and batch: 800, loss is 4.494336032867432 and perplexity is 89.50871840361513
At time: 195.36597990989685 and batch: 850, loss is 4.387433929443359 and perplexity is 80.43375524592008
At time: 195.81032276153564 and batch: 900, loss is 4.383637237548828 and perplexity is 80.12895204738925
At time: 196.25442934036255 and batch: 950, loss is 4.3905424213409425 and perplexity is 80.68417192986855
At time: 196.69953227043152 and batch: 1000, loss is 4.2994682359695435 and perplexity is 73.66061321856282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.617723046279535 and perplexity of 101.26319781473956
Finished 20 epochs...
Completing Train Step...
At time: 198.10696363449097 and batch: 50, loss is 4.448782548904419 and perplexity is 85.52276081724844
At time: 198.56515049934387 and batch: 100, loss is 4.3655323314666745 and perplexity is 78.6912786378859
At time: 199.00992345809937 and batch: 150, loss is 4.438161964416504 and perplexity is 84.61926542751056
At time: 199.455650806427 and batch: 200, loss is 4.472103319168091 and perplexity is 87.5406554499174
At time: 199.89949703216553 and batch: 250, loss is 4.499717426300049 and perplexity is 89.99169842017203
At time: 200.34441137313843 and batch: 300, loss is 4.3785026741027835 and perplexity is 79.7185793032622
At time: 200.789865732193 and batch: 350, loss is 4.404562454223633 and perplexity is 81.82333355563213
At time: 201.23464393615723 and batch: 400, loss is 4.420617351531982 and perplexity is 83.14760081562007
At time: 201.67885160446167 and batch: 450, loss is 4.466476783752442 and perplexity is 87.04948793321022
At time: 202.1237473487854 and batch: 500, loss is 4.502047481536866 and perplexity is 90.20162852774061
At time: 202.56787753105164 and batch: 550, loss is 4.410145225524903 and perplexity is 82.28141199781467
At time: 203.01170778274536 and batch: 600, loss is 4.34225100517273 and perplexity is 76.88040289151775
At time: 203.45715975761414 and batch: 650, loss is 4.32880331993103 and perplexity is 75.85345991116249
At time: 203.90048623085022 and batch: 700, loss is 4.402844948768616 and perplexity is 81.68292214709841
At time: 204.34473037719727 and batch: 750, loss is 4.371817417144776 and perplexity is 79.1874175708773
At time: 204.7888605594635 and batch: 800, loss is 4.476830625534058 and perplexity is 87.9554666450118
At time: 205.23445558547974 and batch: 850, loss is 4.3690148544311525 and perplexity is 78.96580055984589
At time: 205.67847561836243 and batch: 900, loss is 4.363027830123901 and perplexity is 78.49444281555158
At time: 206.12336039543152 and batch: 950, loss is 4.3719394540786745 and perplexity is 79.19708195021464
At time: 206.56997847557068 and batch: 1000, loss is 4.2812191677093505 and perplexity is 72.32856692892437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.605946610613567 and perplexity of 100.07767261210998
Finished 21 epochs...
Completing Train Step...
At time: 207.96390581130981 and batch: 50, loss is 4.429717674255371 and perplexity is 83.90772425517247
At time: 208.42246890068054 and batch: 100, loss is 4.346009120941162 and perplexity is 77.16987193430356
At time: 208.8667175769806 and batch: 150, loss is 4.420253524780273 and perplexity is 83.1173549965556
At time: 209.31226515769958 and batch: 200, loss is 4.454979066848755 and perplexity is 86.05434943757179
At time: 209.75769090652466 and batch: 250, loss is 4.48129487991333 and perplexity is 88.34899998505215
At time: 210.2161102294922 and batch: 300, loss is 4.358979225158691 and perplexity is 78.17729226647747
At time: 210.66072940826416 and batch: 350, loss is 4.385560235977173 and perplexity is 80.28318814660007
At time: 211.1060917377472 and batch: 400, loss is 4.402155256271362 and perplexity is 81.62660547136653
At time: 211.55013632774353 and batch: 450, loss is 4.4481095027923585 and perplexity is 85.4652194217661
At time: 211.9958312511444 and batch: 500, loss is 4.48370246887207 and perplexity is 88.56196432435749
At time: 212.4408941268921 and batch: 550, loss is 4.391281471252442 and perplexity is 80.74382360005838
At time: 212.88507676124573 and batch: 600, loss is 4.324372291564941 and perplexity is 75.51809463387217
At time: 213.32925748825073 and batch: 650, loss is 4.310428667068481 and perplexity is 74.47240596691618
At time: 213.77455163002014 and batch: 700, loss is 4.38543734550476 and perplexity is 80.27332271387822
At time: 214.21864581108093 and batch: 750, loss is 4.354467325210571 and perplexity is 77.82535868713819
At time: 214.6624472141266 and batch: 800, loss is 4.460125904083252 and perplexity is 86.49839891239671
At time: 215.10785174369812 and batch: 850, loss is 4.351486854553222 and perplexity is 77.59374781533111
At time: 215.5519676208496 and batch: 900, loss is 4.343335013389588 and perplexity is 76.96378706636763
At time: 215.9964518547058 and batch: 950, loss is 4.354213528633117 and perplexity is 77.80560938372312
At time: 216.4419755935669 and batch: 1000, loss is 4.263890047073364 and perplexity is 71.08597408341258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5950402980897485 and perplexity of 98.99212466081451
Finished 22 epochs...
Completing Train Step...
At time: 217.84489154815674 and batch: 50, loss is 4.4115063571929936 and perplexity is 82.39348408851319
At time: 218.28856253623962 and batch: 100, loss is 4.327370309829712 and perplexity is 75.74483898292152
At time: 218.73204565048218 and batch: 150, loss is 4.403165512084961 and perplexity is 81.70911089286197
At time: 219.17637872695923 and batch: 200, loss is 4.438635921478271 and perplexity is 84.65938083166006
At time: 219.62062454223633 and batch: 250, loss is 4.463684492111206 and perplexity is 86.80675941779346
At time: 220.0653522014618 and batch: 300, loss is 4.340331907272339 and perplexity is 76.73300335405744
At time: 220.51125049591064 and batch: 350, loss is 4.367385969161988 and perplexity is 78.83727903237441
At time: 220.957359790802 and batch: 400, loss is 4.384519491195679 and perplexity is 80.19967730177066
At time: 221.41546440124512 and batch: 450, loss is 4.430522270202637 and perplexity is 83.97526323720454
At time: 221.85949182510376 and batch: 500, loss is 4.466190805435181 and perplexity is 87.02459722640353
At time: 222.30450534820557 and batch: 550, loss is 4.3732429790496825 and perplexity is 79.30038463835703
At time: 222.7473030090332 and batch: 600, loss is 4.3072483396530155 and perplexity is 74.23593555849993
At time: 223.1907434463501 and batch: 650, loss is 4.292877802848816 and perplexity is 73.17675404559301
At time: 223.63508439064026 and batch: 700, loss is 4.368787484169006 and perplexity is 78.94784812607415
At time: 224.07858729362488 and batch: 750, loss is 4.3378915596008305 and perplexity is 76.54597644615085
At time: 224.52152705192566 and batch: 800, loss is 4.444155025482178 and perplexity is 85.12791651865031
At time: 224.96626210212708 and batch: 850, loss is 4.334722919464111 and perplexity is 76.30381365885809
At time: 225.41010665893555 and batch: 900, loss is 4.324369311332703 and perplexity is 75.5178695727473
At time: 225.85307383537292 and batch: 950, loss is 4.337257814407349 and perplexity is 76.49748116994495
At time: 226.29739713668823 and batch: 1000, loss is 4.247356986999511 and perplexity is 69.92036747616967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.584797928972942 and perplexity of 97.98338553816058
Finished 23 epochs...
Completing Train Step...
At time: 227.68558740615845 and batch: 50, loss is 4.394093208312988 and perplexity is 80.97117347553706
At time: 228.1439986228943 and batch: 100, loss is 4.309597759246826 and perplexity is 74.4105519633472
At time: 228.58811140060425 and batch: 150, loss is 4.38684497833252 and perplexity is 80.38639764344418
At time: 229.0328688621521 and batch: 200, loss is 4.423026914596558 and perplexity is 83.34819177468398
At time: 229.47704935073853 and batch: 250, loss is 4.446739463806153 and perplexity is 85.3482089119762
At time: 229.92128825187683 and batch: 300, loss is 4.322398109436035 and perplexity is 75.36915522616839
At time: 230.36516857147217 and batch: 350, loss is 4.34994176864624 and perplexity is 77.47395138083544
At time: 230.81061625480652 and batch: 400, loss is 4.367635641098023 and perplexity is 78.85696494586995
At time: 231.2550528049469 and batch: 450, loss is 4.413680181503296 and perplexity is 82.57278786407996
At time: 231.69904041290283 and batch: 500, loss is 4.449449214935303 and perplexity is 85.57979494599843
At time: 232.144104719162 and batch: 550, loss is 4.356041221618653 and perplexity is 77.94794418276395
At time: 232.58762311935425 and batch: 600, loss is 4.29083963394165 and perplexity is 73.02775935052605
At time: 233.0465669631958 and batch: 650, loss is 4.27609956741333 and perplexity is 71.95921983772773
At time: 233.49340510368347 and batch: 700, loss is 4.352840132713318 and perplexity is 77.69882482278307
At time: 233.9393651485443 and batch: 750, loss is 4.321998653411865 and perplexity is 75.33905457542005
At time: 234.3832721710205 and batch: 800, loss is 4.4288514137268065 and perplexity is 83.8350697790044
At time: 234.82852935791016 and batch: 850, loss is 4.318616585731506 and perplexity is 75.084683186964
At time: 235.27323412895203 and batch: 900, loss is 4.306027226448059 and perplexity is 74.1453404020354
At time: 235.71763849258423 and batch: 950, loss is 4.32099778175354 and perplexity is 75.26368757357015
At time: 236.16285967826843 and batch: 1000, loss is 4.2315124940872195 and perplexity is 68.82124522093017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.575390327267531 and perplexity of 97.065919218657
Finished 24 epochs...
Completing Train Step...
At time: 237.54863333702087 and batch: 50, loss is 4.3774165916442875 and perplexity is 79.63204535267371
At time: 238.0081729888916 and batch: 100, loss is 4.292617254257202 and perplexity is 73.15769042899464
At time: 238.45163321495056 and batch: 150, loss is 4.371217126846314 and perplexity is 79.13989639702608
At time: 238.89530062675476 and batch: 200, loss is 4.4080766010284425 and perplexity is 82.11137858163163
At time: 239.34020614624023 and batch: 250, loss is 4.430462121963501 and perplexity is 83.9702124248901
At time: 239.78397274017334 and batch: 300, loss is 4.305145854949951 and perplexity is 74.08001960246158
At time: 240.22824788093567 and batch: 350, loss is 4.333313636779785 and perplexity is 76.19635575258887
At time: 240.67304754257202 and batch: 400, loss is 4.35150182723999 and perplexity is 77.59490961090987
At time: 241.11680388450623 and batch: 450, loss is 4.397511720657349 and perplexity is 81.2484480949307
At time: 241.56047368049622 and batch: 500, loss is 4.433406076431274 and perplexity is 84.2177811436323
At time: 242.00500559806824 and batch: 550, loss is 4.3395213651657105 and perplexity is 76.6708332230291
At time: 242.448885679245 and batch: 600, loss is 4.275036425590515 and perplexity is 71.8827576338664
At time: 242.89312410354614 and batch: 650, loss is 4.259981307983399 and perplexity is 70.80865988526043
At time: 243.3376772403717 and batch: 700, loss is 4.337562713623047 and perplexity is 76.52080874805597
At time: 243.76973915100098 and batch: 750, loss is 4.306749715805053 and perplexity is 74.1989289776033
At time: 244.22969794273376 and batch: 800, loss is 4.414184217453003 and perplexity is 82.61441800829424
At time: 244.67358565330505 and batch: 850, loss is 4.303185186386108 and perplexity is 73.93491553379546
At time: 245.1185781955719 and batch: 900, loss is 4.288458466529846 and perplexity is 72.85407489774825
At time: 245.56208205223083 and batch: 950, loss is 4.305346164703369 and perplexity is 74.09486003921384
At time: 246.00590538978577 and batch: 1000, loss is 4.216170568466186 and perplexity is 67.7734529232533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.566166575362042 and perplexity of 96.17472366304726
Finished 25 epochs...
Completing Train Step...
At time: 247.4171495437622 and batch: 50, loss is 4.36153826713562 and perplexity is 78.37760743717828
At time: 247.86096954345703 and batch: 100, loss is 4.276206588745117 and perplexity is 71.96692142137863
At time: 248.30567359924316 and batch: 150, loss is 4.356213026046753 and perplexity is 77.96133713518716
At time: 248.74978804588318 and batch: 200, loss is 4.393718147277832 and perplexity is 80.94081003782269
At time: 249.1936593055725 and batch: 250, loss is 4.414788722991943 and perplexity is 82.66437397938594
At time: 249.63824248313904 and batch: 300, loss is 4.288555274009704 and perplexity is 72.86112805853034
At time: 250.08254265785217 and batch: 350, loss is 4.317369422912598 and perplexity is 74.99109873148055
At time: 250.5256850719452 and batch: 400, loss is 4.336030082702637 and perplexity is 76.4036204166832
At time: 250.9709107875824 and batch: 450, loss is 4.381952314376831 and perplexity is 79.99405459717076
At time: 251.41502285003662 and batch: 500, loss is 4.417955989837647 and perplexity is 82.92660917561366
At time: 251.8582673072815 and batch: 550, loss is 4.323615097999573 and perplexity is 75.46093446193474
At time: 252.30218386650085 and batch: 600, loss is 4.259837436676025 and perplexity is 70.79847328358679
At time: 252.74528408050537 and batch: 650, loss is 4.244593086242676 and perplexity is 69.72738133970253
At time: 253.1895785331726 and batch: 700, loss is 4.322898435592651 and perplexity is 75.40687382094305
At time: 253.6344347000122 and batch: 750, loss is 4.292110676765442 and perplexity is 73.1206397749802
At time: 254.07801127433777 and batch: 800, loss is 4.4000637912750244 and perplexity is 81.45606468546073
At time: 254.52125024795532 and batch: 850, loss is 4.288268423080444 and perplexity is 72.84023077358589
At time: 254.96620845794678 and batch: 900, loss is 4.271570692062378 and perplexity is 71.63406235535187
At time: 255.4111213684082 and batch: 950, loss is 4.29032289981842 and perplexity is 72.99003316337019
At time: 255.86895442008972 and batch: 1000, loss is 4.201387915611267 and perplexity is 66.77895029982486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5571348608993905 and perplexity of 95.3100118136615
Finished 26 epochs...
Completing Train Step...
At time: 257.26585817337036 and batch: 50, loss is 4.346288833618164 and perplexity is 77.19146034490149
At time: 257.7256805896759 and batch: 100, loss is 4.260501074790954 and perplexity is 70.84547344276012
At time: 258.1703474521637 and batch: 150, loss is 4.341841096878052 and perplexity is 76.84889543469944
At time: 258.61433815956116 and batch: 200, loss is 4.379751062393188 and perplexity is 79.81816118968958
At time: 259.05822920799255 and batch: 250, loss is 4.399700560569763 and perplexity is 81.42648271450255
At time: 259.50557112693787 and batch: 300, loss is 4.272571430206299 and perplexity is 71.70578517585032
At time: 259.95125460624695 and batch: 350, loss is 4.302021427154541 and perplexity is 73.84892314023399
At time: 260.39511346817017 and batch: 400, loss is 4.321106166839599 and perplexity is 75.27184547691448
At time: 260.83954095840454 and batch: 450, loss is 4.366940450668335 and perplexity is 78.80216338949123
At time: 261.28275513648987 and batch: 500, loss is 4.403028783798217 and perplexity is 81.69793970984404
At time: 261.72629022598267 and batch: 550, loss is 4.308360199928284 and perplexity is 74.31852144972275
At time: 262.1706702709198 and batch: 600, loss is 4.245119748115539 and perplexity is 69.7641137648836
At time: 262.61570048332214 and batch: 650, loss is 4.229742150306702 and perplexity is 68.69951574079273
At time: 263.0593283176422 and batch: 700, loss is 4.308888955116272 and perplexity is 74.35782814439646
At time: 263.50471472740173 and batch: 750, loss is 4.27801281452179 and perplexity is 72.09702739492725
At time: 263.9489996433258 and batch: 800, loss is 4.386415033340454 and perplexity is 80.35184334310368
At time: 264.3927490711212 and batch: 850, loss is 4.273865928649903 and perplexity is 71.79866830872274
At time: 264.83707904815674 and batch: 900, loss is 4.255149135589599 and perplexity is 70.46732559097417
At time: 265.28160285949707 and batch: 950, loss is 4.275790257453918 and perplexity is 71.93696557627021
At time: 265.72631430625916 and batch: 1000, loss is 4.187171363830567 and perplexity is 65.83630038722806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.548817704363567 and perplexity of 94.52059094512748
Finished 27 epochs...
Completing Train Step...
At time: 267.11390018463135 and batch: 50, loss is 4.331504440307617 and perplexity is 76.05862620204563
At time: 267.5753583908081 and batch: 100, loss is 4.24531542301178 and perplexity is 69.77776618628039
At time: 268.0207483768463 and batch: 150, loss is 4.327949657440185 and perplexity is 75.78873428848436
At time: 268.4652667045593 and batch: 200, loss is 4.366280136108398 and perplexity is 78.75014634934512
At time: 268.9098587036133 and batch: 250, loss is 4.385130844116211 and perplexity is 80.24872259918017
At time: 269.3557138442993 and batch: 300, loss is 4.257221570014954 and perplexity is 70.61351593501861
At time: 269.80180978775024 and batch: 350, loss is 4.287250776290893 and perplexity is 72.76614285065128
At time: 270.2464973926544 and batch: 400, loss is 4.306732997894287 and perplexity is 74.19768853689855
At time: 270.69268798828125 and batch: 450, loss is 4.35245792388916 and perplexity is 77.66913332084756
At time: 271.13697695732117 and batch: 500, loss is 4.388627433776856 and perplexity is 80.52981059119871
At time: 271.58145403862 and batch: 550, loss is 4.293715319633484 and perplexity is 73.238066476865
At time: 272.02734875679016 and batch: 600, loss is 4.231015033721924 and perplexity is 68.7870178932178
At time: 272.4728536605835 and batch: 650, loss is 4.2152783203125 and perplexity is 67.71300915444412
At time: 272.9198830127716 and batch: 700, loss is 4.295409116744995 and perplexity is 73.36222201978319
At time: 273.3641698360443 and batch: 750, loss is 4.2643357944488525 and perplexity is 71.11766753290551
At time: 273.80919218063354 and batch: 800, loss is 4.37324089050293 and perplexity is 79.30021901596918
At time: 274.25357699394226 and batch: 850, loss is 4.259964017868042 and perplexity is 70.80743560594678
At time: 274.69904613494873 and batch: 900, loss is 4.239255685806274 and perplexity is 69.35620981080692
At time: 275.14484310150146 and batch: 950, loss is 4.261796574592591 and perplexity is 70.9373132159113
At time: 275.5891320705414 and batch: 1000, loss is 4.173477115631104 and perplexity is 64.94086688701488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5410215796493905 and perplexity of 93.78656163989311
Finished 28 epochs...
Completing Train Step...
At time: 277.00606751441956 and batch: 50, loss is 4.317233028411866 and perplexity is 74.98087105552496
At time: 277.450156211853 and batch: 100, loss is 4.23074544429779 and perplexity is 68.76847614011865
At time: 277.89511585235596 and batch: 150, loss is 4.314510040283203 and perplexity is 74.77697676062019
At time: 278.3380959033966 and batch: 200, loss is 4.353277015686035 and perplexity is 77.73277753248858
At time: 278.7968509197235 and batch: 250, loss is 4.3710629796981815 and perplexity is 79.12769814787981
At time: 279.2421123981476 and batch: 300, loss is 4.242430868148804 and perplexity is 69.57677841099496
At time: 279.686261177063 and batch: 350, loss is 4.27302435874939 and perplexity is 71.73827012878705
At time: 280.1308629512787 and batch: 400, loss is 4.292861652374268 and perplexity is 73.17557221583289
At time: 280.5762252807617 and batch: 450, loss is 4.338496508598328 and perplexity is 76.59229686719793
At time: 281.01983308792114 and batch: 500, loss is 4.374691686630249 and perplexity is 79.41535096291022
At time: 281.4639973640442 and batch: 550, loss is 4.279625868797302 and perplexity is 72.21341765988886
At time: 281.9092538356781 and batch: 600, loss is 4.217374591827393 and perplexity is 67.85510288821136
At time: 282.3536751270294 and batch: 650, loss is 4.201266989707947 and perplexity is 66.77087548317417
At time: 282.79747796058655 and batch: 700, loss is 4.282427520751953 and perplexity is 72.4160181982029
At time: 283.24259066581726 and batch: 750, loss is 4.2510907840728756 and perplexity is 70.18192393512284
At time: 283.6867823600769 and batch: 800, loss is 4.360549020767212 and perplexity is 78.30011101150527
At time: 284.1300256252289 and batch: 850, loss is 4.246532192230225 and perplexity is 69.86272129920934
At time: 284.5753667354584 and batch: 900, loss is 4.223912644386291 and perplexity is 68.30019655467488
At time: 285.0215744972229 and batch: 950, loss is 4.248296866416931 and perplexity is 69.98611508299017
At time: 285.4670765399933 and batch: 1000, loss is 4.160262613296509 and perplexity is 64.08835084326246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.533660516506288 and perplexity of 93.09872753868198
Finished 29 epochs...
Completing Train Step...
At time: 286.8569939136505 and batch: 50, loss is 4.303434400558472 and perplexity is 73.95334345873364
At time: 287.3155071735382 and batch: 100, loss is 4.216733689308167 and perplexity is 67.81162831479789
At time: 287.760614156723 and batch: 150, loss is 4.3015021800994875 and perplexity is 73.81058725813284
At time: 288.2045531272888 and batch: 200, loss is 4.340712814331055 and perplexity is 76.76223706398126
At time: 288.64825654029846 and batch: 250, loss is 4.35747196674347 and perplexity is 78.05954764290823
At time: 289.0931169986725 and batch: 300, loss is 4.228187189102173 and perplexity is 68.59277367044268
At time: 289.53669118881226 and batch: 350, loss is 4.259283928871155 and perplexity is 70.75929661934636
At time: 289.9797761440277 and batch: 400, loss is 4.279424543380737 and perplexity is 72.19888072687309
At time: 290.4387366771698 and batch: 450, loss is 4.325011291503906 and perplexity is 75.5663661128275
At time: 290.8821470737457 and batch: 500, loss is 4.361197938919068 and perplexity is 78.35093786428307
At time: 291.3248369693756 and batch: 550, loss is 4.266049304008484 and perplexity is 71.23963280056587
At time: 291.7713267803192 and batch: 600, loss is 4.2042215013504025 and perplexity is 66.96844252551371
At time: 292.2154381275177 and batch: 650, loss is 4.187718071937561 and perplexity is 65.87230346707534
At time: 292.6591191291809 and batch: 700, loss is 4.269865045547485 and perplexity is 71.51198410731317
At time: 293.1040415763855 and batch: 750, loss is 4.238269715309143 and perplexity is 69.2878603349611
At time: 293.5478811264038 and batch: 800, loss is 4.348290071487427 and perplexity is 77.3460934957779
At time: 293.9912269115448 and batch: 850, loss is 4.233566780090332 and perplexity is 68.9627690571519
At time: 294.4352583885193 and batch: 900, loss is 4.209143285751343 and perplexity is 67.29885921414315
At time: 294.8798761367798 and batch: 950, loss is 4.235262174606323 and perplexity is 69.0797873256549
At time: 295.32327580451965 and batch: 1000, loss is 4.147495789527893 and perplexity is 63.27534694389519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.526618771436738 and perplexity of 92.44545282948356
Finished 30 epochs...
Completing Train Step...
At time: 296.7092823982239 and batch: 50, loss is 4.29011239528656 and perplexity is 72.9746700476681
At time: 297.16828083992004 and batch: 100, loss is 4.203200302124023 and perplexity is 66.90008931087995
At time: 297.61427569389343 and batch: 150, loss is 4.288907227516174 and perplexity is 72.8867763012656
At time: 298.05998039245605 and batch: 200, loss is 4.328539590835572 and perplexity is 75.83345778448007
At time: 298.50631880760193 and batch: 250, loss is 4.344346427917481 and perplexity is 77.04166873741585
At time: 298.9536907672882 and batch: 300, loss is 4.214423713684082 and perplexity is 67.6551658881305
At time: 299.3990263938904 and batch: 350, loss is 4.2459588670730595 and perplexity is 69.82267872334536
At time: 299.8442029953003 and batch: 400, loss is 4.26638587474823 and perplexity is 71.26361401193743
At time: 300.28926515579224 and batch: 450, loss is 4.311965770721436 and perplexity is 74.58696579678298
At time: 300.7340724468231 and batch: 500, loss is 4.348133130073547 and perplexity is 77.33395564299738
At time: 301.1784691810608 and batch: 550, loss is 4.252933106422424 and perplexity is 70.31134083934532
At time: 301.64131689071655 and batch: 600, loss is 4.191548595428467 and perplexity is 66.1251127597526
At time: 302.0904700756073 and batch: 650, loss is 4.1746058893203735 and perplexity is 65.01421181604078
At time: 302.53490829467773 and batch: 700, loss is 4.257706580162048 and perplexity is 70.64777251349165
At time: 302.9796290397644 and batch: 750, loss is 4.225855593681335 and perplexity is 68.43302937535223
At time: 303.42533016204834 and batch: 800, loss is 4.336443109512329 and perplexity is 76.43518367806071
At time: 303.8691291809082 and batch: 850, loss is 4.221041588783264 and perplexity is 68.10438412136679
At time: 304.31396651268005 and batch: 900, loss is 4.194911484718323 and perplexity is 66.34785851798553
At time: 304.75869393348694 and batch: 950, loss is 4.222657551765442 and perplexity is 68.21452725467562
At time: 305.2026298046112 and batch: 1000, loss is 4.135148630142212 and perplexity is 62.49887959753356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5199086259051064 and perplexity of 91.82720696687889
Finished 31 epochs...
Completing Train Step...
At time: 306.6012783050537 and batch: 50, loss is 4.277210869789124 and perplexity is 72.03923274072363
At time: 307.04504919052124 and batch: 100, loss is 4.19011579990387 and perplexity is 66.03043683594363
At time: 307.49041962623596 and batch: 150, loss is 4.276702313423157 and perplexity is 72.00260604447315
At time: 307.93311285972595 and batch: 200, loss is 4.316732578277588 and perplexity is 74.94335625646302
At time: 308.37618350982666 and batch: 250, loss is 4.331655464172363 and perplexity is 76.07011373714612
At time: 308.82022428512573 and batch: 300, loss is 4.201100878715515 and perplexity is 66.75978502793086
At time: 309.2633545398712 and batch: 350, loss is 4.233041806221008 and perplexity is 68.92657490676356
At time: 309.7071409225464 and batch: 400, loss is 4.2537181949615475 and perplexity is 70.36656314157055
At time: 310.1522591114044 and batch: 450, loss is 4.299337854385376 and perplexity is 73.65100985718497
At time: 310.5962030887604 and batch: 500, loss is 4.335474557876587 and perplexity is 76.3611880959367
At time: 311.0409412384033 and batch: 550, loss is 4.240241560935974 and perplexity is 69.42462008960497
At time: 311.48724341392517 and batch: 600, loss is 4.179327683448792 and perplexity is 65.32192143869491
At time: 311.9314954280853 and batch: 650, loss is 4.161921911239624 and perplexity is 64.19478078706854
At time: 312.3746292591095 and batch: 700, loss is 4.245927481651306 and perplexity is 69.82048734351457
At time: 312.83440923690796 and batch: 750, loss is 4.213842024803162 and perplexity is 67.61582307414464
At time: 313.2787582874298 and batch: 800, loss is 4.324967255592346 and perplexity is 75.56303855227907
At time: 313.72254061698914 and batch: 850, loss is 4.208929138183594 and perplexity is 67.2844488701553
At time: 314.340779542923 and batch: 900, loss is 4.181190762519837 and perplexity is 65.44373478211799
At time: 314.78586292266846 and batch: 950, loss is 4.210439796447754 and perplexity is 67.38616949209494
At time: 315.23079776763916 and batch: 1000, loss is 4.123176074028015 and perplexity is 61.755069791713915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.513657360542111 and perplexity of 91.25496122212014
Finished 32 epochs...
Completing Train Step...
At time: 316.64066457748413 and batch: 50, loss is 4.264737334251404 and perplexity is 71.14622984115262
At time: 317.08697962760925 and batch: 100, loss is 4.177447562217712 and perplexity is 65.19922368681053
At time: 317.5330665111542 and batch: 150, loss is 4.264849653244019 and perplexity is 71.15422136280816
At time: 317.977712392807 and batch: 200, loss is 4.305277509689331 and perplexity is 74.08977323017716
At time: 318.4369535446167 and batch: 250, loss is 4.3193582248687745 and perplexity is 75.14038958109252
At time: 318.88293719291687 and batch: 300, loss is 4.188210535049438 and perplexity is 65.9047511356114
At time: 319.3278946876526 and batch: 350, loss is 4.220508193969726 and perplexity is 68.06806728256501
At time: 319.7728612422943 and batch: 400, loss is 4.241409835815429 and perplexity is 69.50577452538529
At time: 320.2193429470062 and batch: 450, loss is 4.287106156349182 and perplexity is 72.75562017622632
At time: 320.6641528606415 and batch: 500, loss is 4.323195686340332 and perplexity is 75.42929190229727
At time: 321.10865545272827 and batch: 550, loss is 4.227954568862915 and perplexity is 68.57681945872834
At time: 321.5553765296936 and batch: 600, loss is 4.167537245750427 and perplexity is 64.55626994643961
At time: 321.9993724822998 and batch: 650, loss is 4.149654779434204 and perplexity is 63.412105356159294
At time: 322.4437026977539 and batch: 700, loss is 4.234497666358948 and perplexity is 69.02699544100442
At time: 322.88989090919495 and batch: 750, loss is 4.202208118438721 and perplexity is 66.83374505200864
At time: 323.3351254463196 and batch: 800, loss is 4.313822484016418 and perplexity is 74.72558105238257
At time: 323.7809133529663 and batch: 850, loss is 4.197209968566894 and perplexity is 66.50053339232734
At time: 324.2290561199188 and batch: 900, loss is 4.167961535453796 and perplexity is 64.5836663186541
At time: 324.67541551589966 and batch: 950, loss is 4.198588109016418 and perplexity is 66.59224364760122
At time: 325.11963510513306 and batch: 1000, loss is 4.111560287475586 and perplexity is 61.04188619562449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.507882467130336 and perplexity of 90.7294922716951
Finished 33 epochs...
Completing Train Step...
At time: 326.5294978618622 and batch: 50, loss is 4.252684769630432 and perplexity is 70.29388211443218
At time: 326.9753694534302 and batch: 100, loss is 4.165165920257568 and perplexity is 64.40336738031813
At time: 327.4211323261261 and batch: 150, loss is 4.253329076766968 and perplexity is 70.33918755807636
At time: 327.8658266067505 and batch: 200, loss is 4.294171476364136 and perplexity is 73.27148213463755
At time: 328.3111300468445 and batch: 250, loss is 4.307428212165832 and perplexity is 74.24928976376137
At time: 328.7578582763672 and batch: 300, loss is 4.17572199344635 and perplexity is 65.08681495489047
At time: 329.2040822505951 and batch: 350, loss is 4.208344774246216 and perplexity is 67.24514175063935
At time: 329.6500401496887 and batch: 400, loss is 4.229446568489075 and perplexity is 68.67921241385491
At time: 330.0967307090759 and batch: 450, loss is 4.275249071121216 and perplexity is 71.89804480632857
At time: 330.54062700271606 and batch: 500, loss is 4.3112846899032595 and perplexity is 74.53618334053428
At time: 330.98576974868774 and batch: 550, loss is 4.216047005653381 and perplexity is 67.76507916213002
At time: 331.432501077652 and batch: 600, loss is 4.156142706871033 and perplexity is 63.82485599459041
At time: 331.8765721321106 and batch: 650, loss is 4.137775869369507 and perplexity is 62.663294990387456
At time: 332.32122015953064 and batch: 700, loss is 4.223396153450012 and perplexity is 68.2649292306189
At time: 332.767374753952 and batch: 750, loss is 4.190924363136292 and perplexity is 66.08384820971652
At time: 333.22764015197754 and batch: 800, loss is 4.302994198799134 and perplexity is 73.92079623103318
At time: 333.67187762260437 and batch: 850, loss is 4.18586612701416 and perplexity is 65.75042448049602
At time: 334.1176788806915 and batch: 900, loss is 4.155196371078492 and perplexity is 63.76448481911649
At time: 334.56278586387634 and batch: 950, loss is 4.187073383331299 and perplexity is 65.82985002965597
At time: 335.00866413116455 and batch: 1000, loss is 4.100283493995667 and perplexity is 60.3573961315614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.502897309093941 and perplexity of 90.27831693870358
Finished 34 epochs...
Completing Train Step...
At time: 336.42052364349365 and batch: 50, loss is 4.2409811162948605 and perplexity is 69.4759824297368
At time: 336.8663651943207 and batch: 100, loss is 4.153257431983948 and perplexity is 63.640969150033335
At time: 337.3147852420807 and batch: 150, loss is 4.242108588218689 and perplexity is 69.55435882459645
At time: 337.76127457618713 and batch: 200, loss is 4.283415946960449 and perplexity is 72.48763147489431
At time: 338.2072718143463 and batch: 250, loss is 4.295847702026367 and perplexity is 73.39440466747172
At time: 338.65407133102417 and batch: 300, loss is 4.163609118461609 and perplexity is 64.3031821068558
At time: 339.10112142562866 and batch: 350, loss is 4.196536178588867 and perplexity is 66.4557410913905
At time: 339.5471782684326 and batch: 400, loss is 4.217804789543152 and perplexity is 67.88430027835145
At time: 339.99433398246765 and batch: 450, loss is 4.263745284080505 and perplexity is 71.07568420986838
At time: 340.44019079208374 and batch: 500, loss is 4.299729104042053 and perplexity is 73.67983142735258
At time: 340.88598799705505 and batch: 550, loss is 4.204490365982056 and perplexity is 66.98645039187643
At time: 341.33369064331055 and batch: 600, loss is 4.145093402862549 and perplexity is 63.12351754363856
At time: 341.7802519798279 and batch: 650, loss is 4.126253094673157 and perplexity is 61.945384066723
At time: 342.2253067493439 and batch: 700, loss is 4.212600917816162 and perplexity is 67.53195665805833
At time: 342.67268896102905 and batch: 750, loss is 4.179954614639282 and perplexity is 65.36288662850345
At time: 343.11856150627136 and batch: 800, loss is 4.2924685430526734 and perplexity is 73.14681186963232
At time: 343.56521105766296 and batch: 850, loss is 4.17487606048584 and perplexity is 65.03177915440628
At time: 344.01220893859863 and batch: 900, loss is 4.142830004692078 and perplexity is 62.98080545757966
At time: 344.47384333610535 and batch: 950, loss is 4.175883569717407 and perplexity is 65.09733228939882
At time: 344.91978311538696 and batch: 1000, loss is 4.089325075149536 and perplexity is 59.699585370073336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.497409727515244 and perplexity of 89.78426412707545
Finished 35 epochs...
Completing Train Step...
At time: 346.3163962364197 and batch: 50, loss is 4.229587349891663 and perplexity is 68.68888185032961
At time: 346.776859998703 and batch: 100, loss is 4.14164888381958 and perplexity is 62.90646142697293
At time: 347.22321915626526 and batch: 150, loss is 4.231223292350769 and perplexity is 68.8013448750536
At time: 347.668847322464 and batch: 200, loss is 4.272973051071167 and perplexity is 71.73458949913012
At time: 348.1143546104431 and batch: 250, loss is 4.284598803520202 and perplexity is 72.57342467580746
At time: 348.5613098144531 and batch: 300, loss is 4.151889653205871 and perplexity is 63.55398188624182
At time: 349.0069308280945 and batch: 350, loss is 4.185057692527771 and perplexity is 65.69729105019383
At time: 349.4530439376831 and batch: 400, loss is 4.206449456214905 and perplexity is 67.1178115247145
At time: 349.90093755722046 and batch: 450, loss is 4.2525852680206295 and perplexity is 70.28688810796577
At time: 350.34841227531433 and batch: 500, loss is 4.288508939743042 and perplexity is 72.85775216980382
At time: 350.7946889400482 and batch: 550, loss is 4.193246974945068 and perplexity is 66.23751371952227
At time: 351.2421748638153 and batch: 600, loss is 4.134399957656861 and perplexity is 62.45210591728207
At time: 351.68797278404236 and batch: 650, loss is 4.115111355781555 and perplexity is 61.259035430724374
At time: 352.1332457065582 and batch: 700, loss is 4.2021120738983155 and perplexity is 66.82732634392742
At time: 352.5798327922821 and batch: 750, loss is 4.169282503128052 and perplexity is 64.66903562678038
At time: 353.0258324146271 and batch: 800, loss is 4.282242531776428 and perplexity is 72.40262327217987
At time: 353.4723606109619 and batch: 850, loss is 4.164213666915893 and perplexity is 64.3420682492984
At time: 353.91866993904114 and batch: 900, loss is 4.130797328948975 and perplexity is 62.22751896225057
At time: 354.3640260696411 and batch: 950, loss is 4.165024242401123 and perplexity is 64.39424349562132
At time: 354.80922532081604 and batch: 1000, loss is 4.078674697875977 and perplexity is 59.067136152437286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.492075850323933 and perplexity of 89.30664081293459
Finished 36 epochs...
Completing Train Step...
At time: 356.23202300071716 and batch: 50, loss is 4.218499321937561 and perplexity is 67.93146450061073
At time: 356.67728209495544 and batch: 100, loss is 4.130393161773681 and perplexity is 62.20237372346842
At time: 357.1236743927002 and batch: 150, loss is 4.220614285469055 and perplexity is 68.07528910895985
At time: 357.56795287132263 and batch: 200, loss is 4.262846736907959 and perplexity is 71.01184803898532
At time: 358.0152790546417 and batch: 250, loss is 4.273677563667297 and perplexity is 71.78514522749312
At time: 358.46083784103394 and batch: 300, loss is 4.140518870353699 and perplexity is 62.83541642693282
At time: 358.9058380126953 and batch: 350, loss is 4.173886547088623 and perplexity is 64.9674611646924
At time: 359.35041975975037 and batch: 400, loss is 4.195390968322754 and perplexity is 66.37967885638969
At time: 359.79625129699707 and batch: 450, loss is 4.241754117012024 and perplexity is 69.52970817632574
At time: 360.24114871025085 and batch: 500, loss is 4.27760576248169 and perplexity is 72.06768612495802
At time: 360.68586826324463 and batch: 550, loss is 4.182306971549988 and perplexity is 65.51682445393577
At time: 361.13223242759705 and batch: 600, loss is 4.124018483161926 and perplexity is 61.807114745065995
At time: 361.5769054889679 and batch: 650, loss is 4.104306712150573 and perplexity is 60.6007162401961
At time: 362.02133774757385 and batch: 700, loss is 4.191900768280029 and perplexity is 66.1484043303618
At time: 362.4671108722687 and batch: 750, loss is 4.1588917112350465 and perplexity is 64.00055218640607
At time: 362.9130036830902 and batch: 800, loss is 4.272296633720398 and perplexity is 71.68608338518061
At time: 363.35949087142944 and batch: 850, loss is 4.1538472032547 and perplexity is 63.67851383558102
At time: 363.8177261352539 and batch: 900, loss is 4.119128208160401 and perplexity is 61.50559880573276
At time: 364.29244327545166 and batch: 950, loss is 4.154468097686768 and perplexity is 63.718063747153245
At time: 364.73874497413635 and batch: 1000, loss is 4.06831422328949 and perplexity is 58.45833178623208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.487251839986661 and perplexity of 88.87686211635423
Finished 37 epochs...
Completing Train Step...
At time: 366.137597322464 and batch: 50, loss is 4.207744750976563 and perplexity is 67.20480520346149
At time: 366.60797357559204 and batch: 100, loss is 4.119475474357605 and perplexity is 61.52696133016343
At time: 367.0664060115814 and batch: 150, loss is 4.210261416435242 and perplexity is 67.37415021836875
At time: 367.53118419647217 and batch: 200, loss is 4.252991981506348 and perplexity is 70.31548054729966
At time: 367.9780201911926 and batch: 250, loss is 4.263066582679748 and perplexity is 71.02746140972751
At time: 368.42516589164734 and batch: 300, loss is 4.129458327293396 and perplexity is 62.14425197109798
At time: 368.86984062194824 and batch: 350, loss is 4.16301130771637 and perplexity is 64.2647524616112
At time: 369.31820726394653 and batch: 400, loss is 4.184637403488159 and perplexity is 65.66968500049965
At time: 369.7654666900635 and batch: 450, loss is 4.2312392902374265 and perplexity is 68.80244555997507
At time: 370.2103281021118 and batch: 500, loss is 4.267002463340759 and perplexity is 71.30756789273782
At time: 370.6580514907837 and batch: 550, loss is 4.171645793914795 and perplexity is 64.82204809813024
At time: 371.1046402454376 and batch: 600, loss is 4.1139148283004765 and perplexity is 61.18578114548955
At time: 371.5497477054596 and batch: 650, loss is 4.093804249763489 and perplexity is 59.9675900090327
At time: 371.99772214889526 and batch: 700, loss is 4.181949028968811 and perplexity is 65.49337738928199
At time: 372.4447774887085 and batch: 750, loss is 4.148770537376404 and perplexity is 63.356058488780214
At time: 372.89043045043945 and batch: 800, loss is 4.262616300582886 and perplexity is 70.99548621493827
At time: 373.3351905345917 and batch: 850, loss is 4.143759574890137 and perplexity is 63.039377756696915
At time: 373.78440380096436 and batch: 900, loss is 4.10784836769104 and perplexity is 60.815723618919264
At time: 374.22925782203674 and batch: 950, loss is 4.1441971921920775 and perplexity is 63.0669709162885
At time: 374.67411065101624 and batch: 1000, loss is 4.058228344917297 and perplexity is 57.87169152613775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.482641824861852 and perplexity of 88.46808140434334
Finished 38 epochs...
Completing Train Step...
At time: 376.10772037506104 and batch: 50, loss is 4.1972810173034665 and perplexity is 66.50525833905502
At time: 376.5702929496765 and batch: 100, loss is 4.108863587379456 and perplexity is 60.87749629001278
At time: 377.0172941684723 and batch: 150, loss is 4.200169043540955 and perplexity is 66.69760488731619
At time: 377.4669437408447 and batch: 200, loss is 4.24336950302124 and perplexity is 69.64211626091841
At time: 377.91204380989075 and batch: 250, loss is 4.252755661010742 and perplexity is 70.29886552140084
At time: 378.3581736087799 and batch: 300, loss is 4.118692665100098 and perplexity is 61.478816301893374
At time: 378.8017547130585 and batch: 350, loss is 4.152426643371582 and perplexity is 63.58811891431036
At time: 379.2612478733063 and batch: 400, loss is 4.174173302650452 and perplexity is 64.98609361687126
At time: 379.7067549228668 and batch: 450, loss is 4.221018099784851 and perplexity is 68.10278443638383
At time: 380.151394367218 and batch: 500, loss is 4.256681275367737 and perplexity is 70.57537413517078
At time: 380.599641084671 and batch: 550, loss is 4.161257472038269 and perplexity is 64.15214142539163
At time: 381.0666251182556 and batch: 600, loss is 4.104072504043579 and perplexity is 60.586524723112014
At time: 381.5283513069153 and batch: 650, loss is 4.08359037399292 and perplexity is 59.35820587905575
At time: 381.9777455329895 and batch: 700, loss is 4.17223620891571 and perplexity is 64.86033130809605
At time: 382.4303352832794 and batch: 750, loss is 4.138915243148804 and perplexity is 62.734732594962445
At time: 382.8780233860016 and batch: 800, loss is 4.253188424110412 and perplexity is 70.32929486021979
At time: 383.3268983364105 and batch: 850, loss is 4.133941111564636 and perplexity is 62.42345658585025
At time: 383.77593302726746 and batch: 900, loss is 4.096917066574097 and perplexity is 60.154548964831186
At time: 384.2274932861328 and batch: 950, loss is 4.134197168350219 and perplexity is 62.4394425820627
At time: 384.67689394950867 and batch: 1000, loss is 4.048408470153809 and perplexity is 57.30617993385339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.478329170040968 and perplexity of 88.08737063372025
Finished 39 epochs...
Completing Train Step...
At time: 386.09815073013306 and batch: 50, loss is 4.1871007585525515 and perplexity is 65.83165216103231
At time: 386.5479805469513 and batch: 100, loss is 4.0985379934310915 and perplexity is 60.25213415668153
At time: 386.99873757362366 and batch: 150, loss is 4.190318827629089 and perplexity is 66.04384420631759
At time: 387.4483439922333 and batch: 200, loss is 4.234027366638184 and perplexity is 68.99453969688527
At time: 387.89735198020935 and batch: 250, loss is 4.242741923332215 and perplexity is 69.59842399487573
At time: 388.3478157520294 and batch: 300, loss is 4.10822169303894 and perplexity is 60.83843190862344
At time: 388.7964782714844 and batch: 350, loss is 4.142120718955994 and perplexity is 62.93614990926537
At time: 389.2452886104584 and batch: 400, loss is 4.1639774131774905 and perplexity is 64.32686899064947
At time: 389.69739389419556 and batch: 450, loss is 4.211078748703003 and perplexity is 67.42923979553295
At time: 390.14452028274536 and batch: 500, loss is 4.246613392829895 and perplexity is 69.86839442440089
At time: 390.60801243782043 and batch: 550, loss is 4.151138973236084 and perplexity is 63.50629108754283
At time: 391.05741024017334 and batch: 600, loss is 4.094477992057801 and perplexity is 60.00800632427139
At time: 391.5056884288788 and batch: 650, loss is 4.073645458221436 and perplexity is 58.770819118276705
At time: 391.95326828956604 and batch: 700, loss is 4.16274407863617 and perplexity is 64.24758134533458
At time: 392.4056041240692 and batch: 750, loss is 4.129311361312866 and perplexity is 62.13511955126674
At time: 392.85315918922424 and batch: 800, loss is 4.243990831375122 and perplexity is 69.68540032779217
At time: 393.3014862537384 and batch: 850, loss is 4.124381513595581 and perplexity is 61.829556682041435
At time: 393.7523555755615 and batch: 900, loss is 4.086310963630677 and perplexity is 59.519915071320206
At time: 394.200163602829 and batch: 950, loss is 4.124451851844787 and perplexity is 61.83390581776116
At time: 394.6478931903839 and batch: 1000, loss is 4.038840708732605 and perplexity is 56.76050269380372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4742677269912345 and perplexity of 87.73033432659018
Finished 40 epochs...
Completing Train Step...
At time: 396.0584065914154 and batch: 50, loss is 4.177192301750183 and perplexity is 65.1825830264314
At time: 396.5235240459442 and batch: 100, loss is 4.088482203483582 and perplexity is 59.64928748140608
At time: 396.9863398075104 and batch: 150, loss is 4.180729608535767 and perplexity is 65.41356210075165
At time: 397.43153834342957 and batch: 200, loss is 4.224869003295899 and perplexity is 68.36554730058099
At time: 397.87717509269714 and batch: 250, loss is 4.2330012607574465 and perplexity is 68.92378030348686
At time: 398.32600808143616 and batch: 300, loss is 4.098012566566467 and perplexity is 60.22048438229367
At time: 398.7724370956421 and batch: 350, loss is 4.132098436355591 and perplexity is 62.308536342795236
At time: 399.2355763912201 and batch: 400, loss is 4.154041085243225 and perplexity is 63.69086114939199
At time: 399.7016668319702 and batch: 450, loss is 4.20138952255249 and perplexity is 66.77905760975914
At time: 400.1717355251312 and batch: 500, loss is 4.236803240776062 and perplexity is 69.18632591934178
At time: 400.6661698818207 and batch: 550, loss is 4.141279664039612 and perplexity is 62.88323940440511
At time: 401.1419086456299 and batch: 600, loss is 4.085120658874512 and perplexity is 59.44911038126379
At time: 401.59830236434937 and batch: 650, loss is 4.063954067230225 and perplexity is 58.20399920447894
At time: 402.05192470550537 and batch: 700, loss is 4.153467288017273 and perplexity is 63.65432599283413
At time: 402.5241892337799 and batch: 750, loss is 4.1199507141113285 and perplexity is 61.55620833723308
At time: 402.9970073699951 and batch: 800, loss is 4.23501519203186 and perplexity is 69.06272792871151
At time: 403.45541071891785 and batch: 850, loss is 4.115073666572571 and perplexity is 61.25672666964383
At time: 403.9037024974823 and batch: 900, loss is 4.075994195938111 and perplexity is 58.90901859140492
At time: 404.3770844936371 and batch: 950, loss is 4.1149486589431765 and perplexity is 61.24906959006504
At time: 404.8453495502472 and batch: 1000, loss is 4.029525799751282 and perplexity is 56.23423863360113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.47044223692359 and perplexity of 87.39536392547068
Finished 41 epochs...
Completing Train Step...
At time: 406.2426338195801 and batch: 50, loss is 4.167534713745117 and perplexity is 64.55610648982824
At time: 406.70251297950745 and batch: 100, loss is 4.078672952651978 and perplexity is 59.06703306714367
At time: 407.14682936668396 and batch: 150, loss is 4.171349897384643 and perplexity is 64.80287031648011
At time: 407.6009621620178 and batch: 200, loss is 4.215868945121765 and perplexity is 67.75301395031029
At time: 408.052668094635 and batch: 250, loss is 4.223501415252685 and perplexity is 68.27211529833167
At time: 408.50556921958923 and batch: 300, loss is 4.088057723045349 and perplexity is 59.62397289886834
At time: 408.9518356323242 and batch: 350, loss is 4.122353587150574 and perplexity is 61.704297939648015
At time: 409.3993384838104 and batch: 400, loss is 4.144368052482605 and perplexity is 63.07774747787895
At time: 409.8483986854553 and batch: 450, loss is 4.1919462823867795 and perplexity is 66.15141508441322
At time: 410.2945032119751 and batch: 500, loss is 4.2272497129440305 and perplexity is 68.5284997128645
At time: 410.7410640716553 and batch: 550, loss is 4.1316598558425905 and perplexity is 62.28121502469851
At time: 411.1983063220978 and batch: 600, loss is 4.075980887413025 and perplexity is 58.9082346044701
At time: 411.6470992565155 and batch: 650, loss is 4.05449490070343 and perplexity is 57.65603361814467
At time: 412.09423875808716 and batch: 700, loss is 4.144406476020813 and perplexity is 63.08017119468283
At time: 412.5387234687805 and batch: 750, loss is 4.11081072807312 and perplexity is 60.99614881946386
At time: 412.98547410964966 and batch: 800, loss is 4.226253461837769 and perplexity is 68.46026211574983
At time: 413.4315137863159 and batch: 850, loss is 4.106002655029297 and perplexity is 60.70357879320169
At time: 413.89233660697937 and batch: 900, loss is 4.065915861129761 and perplexity is 58.31829553130817
At time: 414.33692240715027 and batch: 950, loss is 4.105676255226135 and perplexity is 60.683768390264085
At time: 414.799387216568 and batch: 1000, loss is 4.0204520463943485 and perplexity is 55.72629100285098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.466955789705602 and perplexity of 87.09119514435352
Finished 42 epochs...
Completing Train Step...
At time: 416.23997163772583 and batch: 50, loss is 4.158121781349182 and perplexity is 63.951295213207075
At time: 416.6852514743805 and batch: 100, loss is 4.069081988334656 and perplexity is 58.503231283903126
At time: 417.1341814994812 and batch: 150, loss is 4.16215473651886 and perplexity is 64.2097286948887
At time: 417.5800087451935 and batch: 200, loss is 4.2070825481414795 and perplexity is 67.1603167227493
At time: 418.0417459011078 and batch: 250, loss is 4.214221181869507 and perplexity is 67.64146495210238
At time: 418.4943177700043 and batch: 300, loss is 4.078342967033386 and perplexity is 59.047545011264546
At time: 418.9478118419647 and batch: 350, loss is 4.112860617637634 and perplexity is 61.121312430365734
At time: 419.3999054431915 and batch: 400, loss is 4.134913840293884 and perplexity is 62.484207217602524
At time: 419.84977984428406 and batch: 450, loss is 4.182744617462158 and perplexity is 65.54550389959977
At time: 420.3029136657715 and batch: 500, loss is 4.217918133735656 and perplexity is 67.89199500561831
At time: 420.75319719314575 and batch: 550, loss is 4.122278146743774 and perplexity is 61.69964311789319
At time: 421.1974391937256 and batch: 600, loss is 4.067043609619141 and perplexity is 58.38410100003143
At time: 421.64748525619507 and batch: 650, loss is 4.045252494812011 and perplexity is 57.1256081330023
At time: 422.100150346756 and batch: 700, loss is 4.135538063049316 and perplexity is 62.52322345774844
At time: 422.54511404037476 and batch: 750, loss is 4.101873812675476 and perplexity is 60.45345999192348
At time: 422.9984860420227 and batch: 800, loss is 4.217680931091309 and perplexity is 67.87589275469668
At time: 423.4544701576233 and batch: 850, loss is 4.097157101631165 and perplexity is 60.16898989852071
At time: 423.9007911682129 and batch: 900, loss is 4.056071557998657 and perplexity is 57.74700912391407
At time: 424.3629460334778 and batch: 950, loss is 4.096619291305542 and perplexity is 60.13663909454894
At time: 424.8121347427368 and batch: 1000, loss is 4.011607332229614 and perplexity is 55.235581180500986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4635255394912345 and perplexity of 86.79296235252167
Finished 43 epochs...
Completing Train Step...
At time: 426.2414321899414 and batch: 50, loss is 4.148900079727173 and perplexity is 63.36426631315126
At time: 426.71801257133484 and batch: 100, loss is 4.0596736764907835 and perplexity is 57.95539578474404
At time: 427.1693522930145 and batch: 150, loss is 4.153164896965027 and perplexity is 63.63508040421068
At time: 427.62140583992004 and batch: 200, loss is 4.198526840209961 and perplexity is 66.5881637453003
At time: 428.07973551750183 and batch: 250, loss is 4.205158648490905 and perplexity is 67.03123122646087
At time: 428.52546787261963 and batch: 300, loss is 4.068857250213623 and perplexity is 58.49008485493723
At time: 428.9716649055481 and batch: 350, loss is 4.103614296913147 and perplexity is 60.55876990468925
At time: 429.41653060913086 and batch: 400, loss is 4.125714836120605 and perplexity is 61.912050405826825
At time: 429.8646924495697 and batch: 450, loss is 4.1737827825546265 and perplexity is 64.9607201961025
At time: 430.3192689418793 and batch: 500, loss is 4.208806467056275 and perplexity is 67.27619551719576
At time: 430.7711434364319 and batch: 550, loss is 4.113153047561646 and perplexity is 61.1391887447725
At time: 431.2175316810608 and batch: 600, loss is 4.058326449394226 and perplexity is 57.877369276665725
At time: 431.6647958755493 and batch: 650, loss is 4.036230463981628 and perplexity is 56.61253708682103
At time: 432.131098985672 and batch: 700, loss is 4.1268650245666505 and perplexity is 61.983301899341804
At time: 432.5769693851471 and batch: 750, loss is 4.0931398391723635 and perplexity is 59.927760140264986
At time: 433.03500390052795 and batch: 800, loss is 4.209294652938842 and perplexity is 67.30904682419983
At time: 433.5155599117279 and batch: 850, loss is 4.088529629707336 and perplexity is 59.652116488945005
At time: 433.96517992019653 and batch: 900, loss is 4.046451125144959 and perplexity is 57.194121672699154
At time: 434.408077955246 and batch: 950, loss is 4.087738194465637 and perplexity is 59.60492437892652
At time: 434.8517303466797 and batch: 1000, loss is 4.002990145683288 and perplexity is 54.761650779485535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.460387067096989 and perplexity of 86.52099204505697
Finished 44 epochs...
Completing Train Step...
At time: 436.2505280971527 and batch: 50, loss is 4.13990201473236 and perplexity is 62.79666799941209
At time: 436.72280406951904 and batch: 100, loss is 4.050476064682007 and perplexity is 57.42478845282524
At time: 437.1725752353668 and batch: 150, loss is 4.14439371585846 and perplexity is 63.079366286592496
At time: 437.6336488723755 and batch: 200, loss is 4.190167260169983 and perplexity is 66.03383486722583
At time: 438.07952404022217 and batch: 250, loss is 4.196298322677612 and perplexity is 66.43993608026726
At time: 438.52974104881287 and batch: 300, loss is 4.059590201377869 and perplexity is 57.95055815345063
At time: 438.97575426101685 and batch: 350, loss is 4.09458167552948 and perplexity is 60.0142284852577
At time: 439.4222776889801 and batch: 400, loss is 4.116728200912475 and perplexity is 61.358161918380254
At time: 439.8702609539032 and batch: 450, loss is 4.165022044181824 and perplexity is 64.39410194310811
At time: 440.3207964897156 and batch: 500, loss is 4.1999173879623415 and perplexity is 66.68082217478634
At time: 440.78401827812195 and batch: 550, loss is 4.104242906570435 and perplexity is 60.596849699694324
At time: 441.2302029132843 and batch: 600, loss is 4.0498045587539675 and perplexity is 57.386240311064114
At time: 441.689599275589 and batch: 650, loss is 4.027424654960632 and perplexity is 56.116206400869615
At time: 442.134916305542 and batch: 700, loss is 4.118377146720886 and perplexity is 61.459421665260976
At time: 442.6068186759949 and batch: 750, loss is 4.084597897529602 and perplexity is 59.4180408060612
At time: 443.0533535480499 and batch: 800, loss is 4.201082882881164 and perplexity is 66.75858364070822
At time: 443.50197434425354 and batch: 850, loss is 4.080110220909119 and perplexity is 59.15198927662135
At time: 443.94732546806335 and batch: 900, loss is 4.037073225975036 and perplexity is 56.6602680915415
At time: 444.42101669311523 and batch: 950, loss is 4.079055318832397 and perplexity is 59.08962262143183
At time: 444.8723180294037 and batch: 1000, loss is 3.9945890140533447 and perplexity is 54.30351805422166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4575001786394814 and perplexity of 86.27157578347548
Finished 45 epochs...
Completing Train Step...
At time: 446.29380893707275 and batch: 50, loss is 4.131079592704773 and perplexity is 62.2450860146213
At time: 446.73899507522583 and batch: 100, loss is 4.0414817333221436 and perplexity is 56.91060670407563
At time: 447.18878650665283 and batch: 150, loss is 4.135819816589356 and perplexity is 62.54084207923027
At time: 447.6350874900818 and batch: 200, loss is 4.182006101608277 and perplexity is 65.4971153758645
At time: 448.0938515663147 and batch: 250, loss is 4.1876318073272705 and perplexity is 65.86662126357787
At time: 448.55721855163574 and batch: 300, loss is 4.0505494165420535 and perplexity is 57.42900082236171
At time: 449.01589918136597 and batch: 350, loss is 4.0857599210739135 and perplexity is 59.487126100029954
At time: 449.4686014652252 and batch: 400, loss is 4.107939944267273 and perplexity is 60.82129316968574
At time: 449.92893147468567 and batch: 450, loss is 4.156448769569397 and perplexity is 63.84439339192056
At time: 450.3948769569397 and batch: 500, loss is 4.191236228942871 and perplexity is 66.10446071634125
At time: 450.84317779541016 and batch: 550, loss is 4.095538339614868 and perplexity is 60.07166941371761
At time: 451.2937812805176 and batch: 600, loss is 4.041473913192749 and perplexity is 56.91016165750744
At time: 451.76072120666504 and batch: 650, loss is 4.018809685707092 and perplexity is 55.63484344875068
At time: 452.20366954803467 and batch: 700, loss is 4.11006944656372 and perplexity is 60.95095025669855
At time: 452.65048456192017 and batch: 750, loss is 4.076248531341553 and perplexity is 58.924003145885315
At time: 453.11772561073303 and batch: 800, loss is 4.1930356788635255 and perplexity is 66.22351947093922
At time: 453.5673382282257 and batch: 850, loss is 4.0718778705596925 and perplexity is 58.667028300240744
At time: 454.03399085998535 and batch: 900, loss is 4.027907900810241 and perplexity is 56.143330878076306
At time: 454.4828987121582 and batch: 950, loss is 4.070542984008789 and perplexity is 58.588766719968085
At time: 454.946795463562 and batch: 1000, loss is 3.986377863883972 and perplexity is 53.859449365132676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.454482287895389 and perplexity of 86.011610064815
Finished 46 epochs...
Completing Train Step...
At time: 456.34261870384216 and batch: 50, loss is 4.122447400093079 and perplexity is 61.710086872937126
At time: 456.8048930168152 and batch: 100, loss is 4.032677998542786 and perplexity is 56.41177980819176
At time: 457.25419211387634 and batch: 150, loss is 4.1274458694458005 and perplexity is 62.01931504085475
At time: 457.72227478027344 and batch: 200, loss is 4.174013652801514 and perplexity is 64.97571942498337
At time: 458.17687463760376 and batch: 250, loss is 4.179150400161743 and perplexity is 65.31034198019847
At time: 458.6209788322449 and batch: 300, loss is 4.041718301773071 and perplexity is 56.92407155076111
At time: 459.0759038925171 and batch: 350, loss is 4.0771119499206545 and perplexity is 58.97490119495512
At time: 459.524950504303 and batch: 400, loss is 4.09930525302887 and perplexity is 60.298380924269296
At time: 459.9760539531708 and batch: 450, loss is 4.148042240142822 and perplexity is 63.30993324513469
At time: 460.4256339073181 and batch: 500, loss is 4.1827395725250245 and perplexity is 65.5451732274873
At time: 460.88889837265015 and batch: 550, loss is 4.087009072303772 and perplexity is 59.56148094731454
At time: 461.3353633880615 and batch: 600, loss is 4.033318486213684 and perplexity is 56.447922430868886
At time: 461.7887177467346 and batch: 650, loss is 4.010388994216919 and perplexity is 55.168326550051
At time: 462.234041929245 and batch: 700, loss is 4.101927905082703 and perplexity is 60.45673015354432
At time: 462.69434666633606 and batch: 750, loss is 4.068079848289489 and perplexity is 58.44463222020025
At time: 463.1382007598877 and batch: 800, loss is 4.185132799148559 and perplexity is 65.70222553702355
At time: 463.59394454956055 and batch: 850, loss is 4.063811078071594 and perplexity is 58.19567725859194
At time: 464.05684208869934 and batch: 900, loss is 4.018954749107361 and perplexity is 55.642914613715945
At time: 464.5148060321808 and batch: 950, loss is 4.062205271720886 and perplexity is 58.102301262415246
At time: 464.9607720375061 and batch: 1000, loss is 3.9783706426620484 and perplexity is 53.429906855154435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452151414824695 and perplexity of 85.81136138693535
Finished 47 epochs...
Completing Train Step...
At time: 466.3573715686798 and batch: 50, loss is 4.114023127555847 and perplexity is 61.19240787885555
At time: 466.82835936546326 and batch: 100, loss is 4.024097142219543 and perplexity is 55.92978933367833
At time: 467.2720699310303 and batch: 150, loss is 4.119262251853943 and perplexity is 61.51384379595233
At time: 467.7173092365265 and batch: 200, loss is 4.166231966018676 and perplexity is 64.47206092584095
At time: 468.1790018081665 and batch: 250, loss is 4.170839762687683 and perplexity is 64.76982055450092
At time: 468.65087819099426 and batch: 300, loss is 4.033104944229126 and perplexity is 56.4358697164149
At time: 469.0967216491699 and batch: 350, loss is 4.068694233894348 and perplexity is 58.48055079371502
At time: 469.54180812835693 and batch: 400, loss is 4.090820002555847 and perplexity is 59.78889865819637
At time: 469.9869701862335 and batch: 450, loss is 4.139834346771241 and perplexity is 62.7924188206917
At time: 470.4308731555939 and batch: 500, loss is 4.1744269227981565 and perplexity is 65.0025774897659
At time: 470.87994503974915 and batch: 550, loss is 4.078653950691223 and perplexity is 59.06591068836314
At time: 471.3637592792511 and batch: 600, loss is 4.025349478721619 and perplexity is 55.999876127379316
At time: 471.81892108917236 and batch: 650, loss is 4.00215708732605 and perplexity is 54.71605012528775
At time: 472.28303241729736 and batch: 700, loss is 4.093964524269104 and perplexity is 59.97720205513676
At time: 472.73167872428894 and batch: 750, loss is 4.060106220245362 and perplexity is 57.98046945157291
At time: 473.1987166404724 and batch: 800, loss is 4.177395377159119 and perplexity is 65.19582135027852
At time: 473.65965509414673 and batch: 850, loss is 4.055923042297363 and perplexity is 57.73843342318524
At time: 474.1043984889984 and batch: 900, loss is 4.010250024795532 and perplexity is 55.16066037232591
At time: 474.5504357814789 and batch: 950, loss is 4.054070777893067 and perplexity is 57.63158556398733
At time: 475.0022120475769 and batch: 1000, loss is 3.9705197191238404 and perplexity is 53.012075070633045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.449388085342035 and perplexity of 85.5745636478366
Finished 48 epochs...
Completing Train Step...
At time: 476.41204500198364 and batch: 50, loss is 4.10572193145752 and perplexity is 60.68654025941416
At time: 476.8741099834442 and batch: 100, loss is 4.0156520652770995 and perplexity is 55.45944679422096
At time: 477.3278980255127 and batch: 150, loss is 4.111255249977112 and perplexity is 61.023268970977384
At time: 477.77308344841003 and batch: 200, loss is 4.15857843875885 and perplexity is 63.98050571511249
At time: 478.22053360939026 and batch: 250, loss is 4.16271577835083 and perplexity is 64.24576314617805
At time: 478.67652797698975 and batch: 300, loss is 4.024659314155579 and perplexity is 55.96124033123589
At time: 479.12329292297363 and batch: 350, loss is 4.0604110431671145 and perplexity is 57.99814592163549
At time: 479.5806713104248 and batch: 400, loss is 4.0824819898605345 and perplexity is 59.29245063329981
At time: 480.0278949737549 and batch: 450, loss is 4.131772780418396 and perplexity is 62.28824850160457
At time: 480.4839687347412 and batch: 500, loss is 4.16628809928894 and perplexity is 64.47568005503722
At time: 480.92795276641846 and batch: 550, loss is 4.070461506843567 and perplexity is 58.58399326780821
At time: 481.3768005371094 and batch: 600, loss is 4.017495255470276 and perplexity is 55.56176336815682
At time: 481.8466796875 and batch: 650, loss is 3.994141297340393 and perplexity is 54.27921090338159
At time: 482.2920095920563 and batch: 700, loss is 4.086149640083313 and perplexity is 59.510313881951916
At time: 482.7374658584595 and batch: 750, loss is 4.052298746109009 and perplexity is 57.529550993553094
At time: 483.18283462524414 and batch: 800, loss is 4.169805684089661 and perplexity is 64.70287808711943
At time: 483.63011717796326 and batch: 850, loss is 4.048214015960693 and perplexity is 57.29503759024664
At time: 484.0933334827423 and batch: 900, loss is 4.001690735816956 and perplexity is 54.69053916174067
At time: 484.55700516700745 and batch: 950, loss is 4.0461271858215335 and perplexity is 57.17559724817737
At time: 485.0179650783539 and batch: 1000, loss is 3.962896509170532 and perplexity is 52.6094893397158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.447219476467225 and perplexity of 85.38918696705963
Finished 49 epochs...
Completing Train Step...
At time: 486.4158847332001 and batch: 50, loss is 4.0976402473449705 and perplexity is 60.19806731184236
At time: 486.8768033981323 and batch: 100, loss is 4.0074286460876465 and perplexity is 55.005250597643936
At time: 487.3249936103821 and batch: 150, loss is 4.103397250175476 and perplexity is 60.5456272475811
At time: 487.77471899986267 and batch: 200, loss is 4.151116895675659 and perplexity is 63.504889039040926
At time: 488.2436273097992 and batch: 250, loss is 4.154738836288452 and perplexity is 63.73531702209255
At time: 488.6995713710785 and batch: 300, loss is 4.01639681816101 and perplexity is 55.500765761461246
At time: 489.1536195278168 and batch: 350, loss is 4.052372651100159 and perplexity is 57.53380287162571
At time: 489.615758895874 and batch: 400, loss is 4.074295773506164 and perplexity is 58.80905111032233
At time: 490.06200098991394 and batch: 450, loss is 4.123898458480835 and perplexity is 61.799696811006065
At time: 490.507696390152 and batch: 500, loss is 4.158299708366394 and perplexity is 63.962674888757064
At time: 490.95331358909607 and batch: 550, loss is 4.062435274124145 and perplexity is 58.115666468296304
At time: 491.4072358608246 and batch: 600, loss is 4.009769320487976 and perplexity is 55.13415077742601
At time: 491.8584430217743 and batch: 650, loss is 3.986263828277588 and perplexity is 53.85330782034886
At time: 492.3050036430359 and batch: 700, loss is 4.078509783744812 and perplexity is 59.05739595016886
At time: 492.7733404636383 and batch: 750, loss is 4.0446563768386845 and perplexity is 57.09156467922246
At time: 493.217191696167 and batch: 800, loss is 4.162365126609802 and perplexity is 64.22323920673723
At time: 493.66318011283875 and batch: 850, loss is 4.04067367553711 and perplexity is 56.86463822037425
At time: 494.1277241706848 and batch: 900, loss is 3.993418188095093 and perplexity is 54.23997529167619
At time: 494.5885980129242 and batch: 950, loss is 4.0383983325958255 and perplexity is 56.735398755001526
At time: 495.0370662212372 and batch: 1000, loss is 3.955306329727173 and perplexity is 52.21168548633455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.444827940405869 and perplexity of 85.18521964199958
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
SETTINGS FOR THIS RUN
{'anneal': 4.977959423534917, 'dropout': 0.886815526576557, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 7.939448573385901}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7543039321899414 and batch: 50, loss is 7.066538705825805 and perplexity is 1172.0840772429626
At time: 1.220677137374878 and batch: 100, loss is 6.478400602340698 and perplexity is 650.9290189159701
At time: 1.6797447204589844 and batch: 150, loss is 6.244374561309814 and perplexity is 515.1069563376569
At time: 2.14638352394104 and batch: 200, loss is 6.198587808609009 and perplexity is 492.05367624919137
At time: 2.5964455604553223 and batch: 250, loss is 6.250332546234131 and perplexity is 518.1851165283339
At time: 3.063096284866333 and batch: 300, loss is 6.149280519485473 and perplexity is 468.3802750430131
At time: 3.52051043510437 and batch: 350, loss is 6.125909900665283 and perplexity is 457.56085884449965
At time: 3.9816315174102783 and batch: 400, loss is 6.1029274368286135 and perplexity is 447.1649028798272
At time: 4.449469804763794 and batch: 450, loss is 6.140903520584106 and perplexity is 464.473042284673
At time: 4.901182413101196 and batch: 500, loss is 6.123773860931396 and perplexity is 456.5845337751276
At time: 5.356479644775391 and batch: 550, loss is 6.084816970825195 and perplexity is 439.13942998962574
At time: 5.81444787979126 and batch: 600, loss is 6.006738147735596 and perplexity is 406.15633527403014
At time: 6.272203683853149 and batch: 650, loss is 5.978884000778198 and perplexity is 394.9993031720596
At time: 6.723478317260742 and batch: 700, loss is 6.058977622985839 and perplexity is 427.93769946366564
At time: 7.195140838623047 and batch: 750, loss is 5.9249780559539795 and perplexity is 374.27022226717645
At time: 7.646854877471924 and batch: 800, loss is 6.037743558883667 and perplexity is 418.9466391230013
At time: 8.10136866569519 and batch: 850, loss is 6.01382737159729 and perplexity is 409.0458987370136
At time: 8.55453872680664 and batch: 900, loss is 6.037490110397339 and perplexity is 418.8404711860832
At time: 9.003452062606812 and batch: 950, loss is 6.0090134239196775 and perplexity is 407.08150522014085
At time: 9.465022563934326 and batch: 1000, loss is 5.956994657516479 and perplexity is 386.44697177559766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 5.954522481778773 and perplexity of 385.49278689039573
Finished 1 epochs...
Completing Train Step...
At time: 10.901829481124878 and batch: 50, loss is 5.670480461120605 and perplexity is 290.17391819070957
At time: 11.34717607498169 and batch: 100, loss is 5.4292073249816895 and perplexity is 227.96846887470292
At time: 11.807785987854004 and batch: 150, loss is 5.302289819717407 and perplexity is 200.79607076393037
At time: 12.273313045501709 and batch: 200, loss is 5.228010892868042 and perplexity is 186.42162190486994
At time: 12.725931406021118 and batch: 250, loss is 5.243096542358399 and perplexity is 189.25523284257287
At time: 13.175923347473145 and batch: 300, loss is 5.117200794219971 and perplexity is 166.86761845856037
At time: 13.641060829162598 and batch: 350, loss is 5.07740963935852 and perplexity is 160.35813207092377
At time: 14.103940486907959 and batch: 400, loss is 5.065093078613281 and perplexity is 158.39518459169662
At time: 14.553579092025757 and batch: 450, loss is 5.070858907699585 and perplexity is 159.3111021286782
At time: 14.998535633087158 and batch: 500, loss is 5.074114866256714 and perplexity is 159.83065784183918
At time: 15.444849729537964 and batch: 550, loss is 4.974384803771972 and perplexity is 144.65980360260548
At time: 15.903385877609253 and batch: 600, loss is 4.873379602432251 and perplexity is 130.7620948673297
At time: 16.349977016448975 and batch: 650, loss is 4.844246196746826 and perplexity is 127.00750722848085
At time: 16.799876928329468 and batch: 700, loss is 4.89085636138916 and perplexity is 133.06747912364304
At time: 17.25718402862549 and batch: 750, loss is 4.851051282882691 and perplexity is 127.87475174510982
At time: 17.718151092529297 and batch: 800, loss is 4.9419491481781 and perplexity is 140.042948165945
At time: 18.166627168655396 and batch: 850, loss is 4.829308643341064 and perplexity is 125.12442514024369
At time: 18.627942085266113 and batch: 900, loss is 4.823548002243042 and perplexity is 124.4057003796828
At time: 19.074886560440063 and batch: 950, loss is 4.808471479415894 and perplexity is 122.54416301409971
At time: 19.53073811531067 and batch: 1000, loss is 4.692896203994751 and perplexity is 109.168897792774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.897649625452553 and perplexity of 133.97451904021116
Finished 2 epochs...
Completing Train Step...
At time: 20.967573404312134 and batch: 50, loss is 4.8341888904571535 and perplexity is 125.73655571450547
At time: 21.437575817108154 and batch: 100, loss is 4.702310171127319 and perplexity is 110.20146284952646
At time: 21.882131099700928 and batch: 150, loss is 4.738055448532105 and perplexity is 114.21189469903777
At time: 22.338627338409424 and batch: 200, loss is 4.7695546722412105 and perplexity is 117.86674093987878
At time: 22.78652572631836 and batch: 250, loss is 4.786506547927856 and perplexity is 119.88183483491119
At time: 23.247210025787354 and batch: 300, loss is 4.662778539657593 and perplexity is 105.93000434834057
At time: 23.704207181930542 and batch: 350, loss is 4.654917917251587 and perplexity is 105.10059269931965
At time: 24.149869441986084 and batch: 400, loss is 4.665253534317016 and perplexity is 106.19250525354676
At time: 24.604636192321777 and batch: 450, loss is 4.704249773025513 and perplexity is 110.41541724215047
At time: 25.057286977767944 and batch: 500, loss is 4.717512006759644 and perplexity is 111.88952569258346
At time: 25.528486013412476 and batch: 550, loss is 4.611957359313965 and perplexity is 100.68102583846296
At time: 25.975229740142822 and batch: 600, loss is 4.545347213745117 and perplexity is 94.19312668050442
At time: 26.421204090118408 and batch: 650, loss is 4.527924270629883 and perplexity is 92.56621910654889
At time: 26.868656873703003 and batch: 700, loss is 4.590180311203003 and perplexity is 98.51219141236602
At time: 27.328218936920166 and batch: 750, loss is 4.5660153675079345 and perplexity is 96.1601823888679
At time: 27.77563762664795 and batch: 800, loss is 4.669595127105713 and perplexity is 106.65455215253509
At time: 28.219762325286865 and batch: 850, loss is 4.554944305419922 and perplexity is 95.101458452285
At time: 28.66687846183777 and batch: 900, loss is 4.530031003952026 and perplexity is 92.76143700867188
At time: 29.125945806503296 and batch: 950, loss is 4.533613014221191 and perplexity is 93.09430524141976
At time: 29.571370363235474 and batch: 1000, loss is 4.431303853988648 and perplexity is 84.04092259717983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.737810088366997 and perplexity of 114.18387508728597
Finished 3 epochs...
Completing Train Step...
At time: 31.008446216583252 and batch: 50, loss is 4.584899950027466 and perplexity is 97.99338241641632
At time: 31.469666481018066 and batch: 100, loss is 4.456771039962769 and perplexity is 86.20869476814683
At time: 31.920737981796265 and batch: 150, loss is 4.526901416778564 and perplexity is 92.47158579910594
At time: 32.366872787475586 and batch: 200, loss is 4.578080968856812 and perplexity is 97.32744049003838
At time: 32.81213736534119 and batch: 250, loss is 4.587901086807251 and perplexity is 98.28791570683978
At time: 33.256022214889526 and batch: 300, loss is 4.434526901245118 and perplexity is 84.31222744149467
At time: 33.701709032058716 and batch: 350, loss is 4.448156957626343 and perplexity is 85.46927525579888
At time: 34.14739274978638 and batch: 400, loss is 4.464817094802856 and perplexity is 86.90513268556143
At time: 34.59233903884888 and batch: 450, loss is 4.508417825698853 and perplexity is 90.77807808705582
At time: 35.03860116004944 and batch: 500, loss is 4.530385236740113 and perplexity is 92.79430197171044
At time: 35.48969841003418 and batch: 550, loss is 4.427501120567322 and perplexity is 83.72194425125842
At time: 35.93666410446167 and batch: 600, loss is 4.377790379524231 and perplexity is 79.66181640976517
At time: 36.38440489768982 and batch: 650, loss is 4.348841643333435 and perplexity is 77.3887671910714
At time: 36.847033977508545 and batch: 700, loss is 4.424070959091186 and perplexity is 83.43525643722582
At time: 37.291123151779175 and batch: 750, loss is 4.399016733169556 and perplexity is 81.37082008850335
At time: 37.74435043334961 and batch: 800, loss is 4.508431673049927 and perplexity is 90.77933513167625
At time: 38.18939709663391 and batch: 850, loss is 4.3929439163208 and perplexity is 80.87816741006415
At time: 38.63872838020325 and batch: 900, loss is 4.3613990211486815 and perplexity is 78.36669442969072
At time: 39.08622431755066 and batch: 950, loss is 4.374963536262512 and perplexity is 79.43694293161693
At time: 39.533324003219604 and batch: 1000, loss is 4.2713224315643314 and perplexity is 71.61628065469272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.663723084984756 and perplexity of 106.03010730740294
Finished 4 epochs...
Completing Train Step...
At time: 40.96275329589844 and batch: 50, loss is 4.436793127059937 and perplexity is 84.50351465597181
At time: 41.43119239807129 and batch: 100, loss is 4.310460457801819 and perplexity is 74.47477353694846
At time: 41.88027095794678 and batch: 150, loss is 4.389756097793579 and perplexity is 80.62075300274611
At time: 42.32370066642761 and batch: 200, loss is 4.45691632270813 and perplexity is 86.22122031384755
At time: 42.783488512039185 and batch: 250, loss is 4.452614588737488 and perplexity is 85.85111617693109
At time: 43.23018407821655 and batch: 300, loss is 4.293057856559753 and perplexity is 73.18993097795547
At time: 43.674344301223755 and batch: 350, loss is 4.314370908737183 and perplexity is 74.76657364795413
At time: 44.11863732337952 and batch: 400, loss is 4.341142959594727 and perplexity is 76.79526307916598
At time: 44.56342101097107 and batch: 450, loss is 4.386481938362121 and perplexity is 80.35721946476629
At time: 45.00736117362976 and batch: 500, loss is 4.40088116645813 and perplexity is 81.5226720691509
At time: 45.45991516113281 and batch: 550, loss is 4.291835484504699 and perplexity is 73.10052030930116
At time: 45.91104745864868 and batch: 600, loss is 4.25372501373291 and perplexity is 70.36704295671206
At time: 46.35760045051575 and batch: 650, loss is 4.230846977233886 and perplexity is 68.77545875988895
At time: 46.80930995941162 and batch: 700, loss is 4.310440740585327 and perplexity is 74.47330511619211
At time: 47.27138113975525 and batch: 750, loss is 4.280329484939575 and perplexity is 72.26424606598957
At time: 47.71816349029541 and batch: 800, loss is 4.397797183990479 and perplexity is 81.2716448584907
At time: 48.166491746902466 and batch: 850, loss is 4.285592288970947 and perplexity is 72.64556114466643
At time: 48.63908505439758 and batch: 900, loss is 4.2397647190094 and perplexity is 69.3915234115787
At time: 49.08450388908386 and batch: 950, loss is 4.257396006584168 and perplexity is 70.62583458885908
At time: 49.544469594955444 and batch: 1000, loss is 4.168586792945862 and perplexity is 64.6240603669096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.643885170541158 and perplexity of 103.94741752292967
Finished 5 epochs...
Completing Train Step...
At time: 50.955559730529785 and batch: 50, loss is 4.327702116966248 and perplexity is 75.76997583111397
At time: 51.417500019073486 and batch: 100, loss is 4.206096425056457 and perplexity is 67.09412102794725
At time: 51.87152910232544 and batch: 150, loss is 4.295360841751099 and perplexity is 73.35868054444599
At time: 52.333927631378174 and batch: 200, loss is 4.36571063041687 and perplexity is 78.70531046114907
At time: 52.78482413291931 and batch: 250, loss is 4.358238153457641 and perplexity is 78.11937874919889
At time: 53.23172974586487 and batch: 300, loss is 4.194820833206177 and perplexity is 66.3418442568883
At time: 53.67638325691223 and batch: 350, loss is 4.222538528442382 and perplexity is 68.20640861812396
At time: 54.14148664474487 and batch: 400, loss is 4.2433929443359375 and perplexity is 69.64374878281603
At time: 54.58713936805725 and batch: 450, loss is 4.29145314693451 and perplexity is 73.0725765762964
At time: 55.03845953941345 and batch: 500, loss is 4.313280682563782 and perplexity is 74.68510558984164
At time: 55.4977502822876 and batch: 550, loss is 4.201055655479431 and perplexity is 66.75676600267718
At time: 55.94243288040161 and batch: 600, loss is 4.164933185577393 and perplexity is 64.38838022729146
At time: 56.4056077003479 and batch: 650, loss is 4.141007556915283 and perplexity is 62.86613075476059
At time: 56.853304862976074 and batch: 700, loss is 4.22538763999939 and perplexity is 68.40101337887532
At time: 57.29728603363037 and batch: 750, loss is 4.195727672576904 and perplexity is 66.40203293979604
At time: 57.74943232536316 and batch: 800, loss is 4.324745130538941 and perplexity is 75.54625597228987
At time: 58.194905281066895 and batch: 850, loss is 4.200864572525024 and perplexity is 66.74401114125986
At time: 58.65238904953003 and batch: 900, loss is 4.153531441688537 and perplexity is 63.65840978253207
At time: 59.10628128051758 and batch: 950, loss is 4.174769759178162 and perplexity is 65.02486655865702
At time: 59.55955934524536 and batch: 1000, loss is 4.093849172592163 and perplexity is 59.97028398331473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6325132788681405 and perplexity of 102.77203458158566
Finished 6 epochs...
Completing Train Step...
At time: 60.98742198944092 and batch: 50, loss is 4.249723725318908 and perplexity is 70.08604667149503
At time: 61.44622492790222 and batch: 100, loss is 4.135854167938232 and perplexity is 62.54299047841556
At time: 61.893702030181885 and batch: 150, loss is 4.213166584968567 and perplexity is 67.57016807413001
At time: 62.33929896354675 and batch: 200, loss is 4.292274484634399 and perplexity is 73.1326184922358
At time: 62.785346269607544 and batch: 250, loss is 4.282291078567505 and perplexity is 72.40613827252558
At time: 63.22933006286621 and batch: 300, loss is 4.115245108604431 and perplexity is 61.26722954762081
At time: 63.67743897438049 and batch: 350, loss is 4.144144177436829 and perplexity is 63.06362752488641
At time: 64.13303709030151 and batch: 400, loss is 4.157899117469787 and perplexity is 63.93705715493204
At time: 64.58969378471375 and batch: 450, loss is 4.217897825241089 and perplexity is 67.89061623540697
At time: 65.04062986373901 and batch: 500, loss is 4.239261507987976 and perplexity is 69.35661361643812
At time: 65.48445916175842 and batch: 550, loss is 4.132205004692078 and perplexity is 62.315176813687906
At time: 65.9286150932312 and batch: 600, loss is 4.091642870903015 and perplexity is 59.838117297867356
At time: 66.37199115753174 and batch: 650, loss is 4.071350998878479 and perplexity is 58.63612644577927
At time: 66.82195210456848 and batch: 700, loss is 4.151168098449707 and perplexity is 63.50814074877292
At time: 67.28899765014648 and batch: 750, loss is 4.125402030944824 and perplexity is 61.89268702465746
At time: 67.73259258270264 and batch: 800, loss is 4.260552220344543 and perplexity is 70.84909696638138
At time: 68.1764919757843 and batch: 850, loss is 4.1428227138519285 and perplexity is 62.980346276268506
At time: 68.63716769218445 and batch: 900, loss is 4.091054992675781 and perplexity is 59.80295010957427
At time: 69.0818977355957 and batch: 950, loss is 4.107844381332398 and perplexity is 60.81548118611704
At time: 69.54687094688416 and batch: 1000, loss is 4.0299060249328615 and perplexity is 56.2556243726372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.6140903379859 and perplexity of 100.89600551126752
Finished 7 epochs...
Completing Train Step...
At time: 70.98299717903137 and batch: 50, loss is 4.179678373336792 and perplexity is 65.3448331932337
At time: 71.4269700050354 and batch: 100, loss is 4.076130146980286 and perplexity is 58.917027878299045
At time: 71.88503885269165 and batch: 150, loss is 4.149075770378113 and perplexity is 63.37539980034235
At time: 72.3307843208313 and batch: 200, loss is 4.233766670227051 and perplexity is 68.97655541231958
At time: 72.77426862716675 and batch: 250, loss is 4.217515406608581 and perplexity is 67.86465856245138
At time: 73.23274946212769 and batch: 300, loss is 4.050365934371948 and perplexity is 57.418464591298665
At time: 73.67567706108093 and batch: 350, loss is 4.087182183265686 and perplexity is 59.57179258507735
At time: 74.1197190284729 and batch: 400, loss is 4.093949112892151 and perplexity is 59.97627773098986
At time: 74.57534289360046 and batch: 450, loss is 4.152439846992492 and perplexity is 63.588958513269766
At time: 75.02325916290283 and batch: 500, loss is 4.181021018028259 and perplexity is 65.4326270113986
At time: 75.46898341178894 and batch: 550, loss is 4.073809418678284 and perplexity is 58.78045599864073
At time: 75.92067074775696 and batch: 600, loss is 4.046793851852417 and perplexity is 57.2137269851462
At time: 76.36565089225769 and batch: 650, loss is 4.009691419601441 and perplexity is 55.12985594548986
At time: 76.80900740623474 and batch: 700, loss is 4.10070074558258 and perplexity is 60.382585605687844
At time: 77.26191520690918 and batch: 750, loss is 4.0691228723526 and perplexity is 58.50562317995557
At time: 77.71099281311035 and batch: 800, loss is 4.198212885856629 and perplexity is 66.56726138277759
At time: 78.15503716468811 and batch: 850, loss is 4.083272566795349 and perplexity is 59.33934441130635
At time: 78.60137557983398 and batch: 900, loss is 4.034157028198242 and perplexity is 56.49527623507
At time: 79.04786372184753 and batch: 950, loss is 4.056371426582336 and perplexity is 57.76432823435052
At time: 79.49144864082336 and batch: 1000, loss is 3.975985984802246 and perplexity is 53.302646604209045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.615322206078506 and perplexity of 101.02037266736673
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 80.90469074249268 and batch: 50, loss is 4.120781335830689 and perplexity is 61.60735950153256
At time: 81.36379027366638 and batch: 100, loss is 3.960612940788269 and perplexity is 52.489489039844905
At time: 81.80698680877686 and batch: 150, loss is 4.043519330024719 and perplexity is 57.02668578965702
At time: 82.25325751304626 and batch: 200, loss is 4.110735740661621 and perplexity is 60.991575047642286
At time: 82.70086288452148 and batch: 250, loss is 4.0799099683761595 and perplexity is 59.140145126889195
At time: 83.16868138313293 and batch: 300, loss is 3.901495614051819 and perplexity is 49.47639138337834
At time: 83.63831949234009 and batch: 350, loss is 3.9428820276260375 and perplexity is 51.56700487885249
At time: 84.08335733413696 and batch: 400, loss is 3.9299406957626344 and perplexity is 50.90395875923213
At time: 84.52988958358765 and batch: 450, loss is 3.978380584716797 and perplexity is 53.43043806085422
At time: 84.97809219360352 and batch: 500, loss is 4.001912069320679 and perplexity is 54.70264535009697
At time: 85.42315196990967 and batch: 550, loss is 3.8802468729019166 and perplexity is 48.43617117360098
At time: 85.87508201599121 and batch: 600, loss is 3.846652488708496 and perplexity is 46.836016426419306
At time: 86.33565282821655 and batch: 650, loss is 3.8035661506652834 and perplexity is 44.860880232410445
At time: 86.77992534637451 and batch: 700, loss is 3.8812505531311037 and perplexity is 48.484810005819455
At time: 87.23856449127197 and batch: 750, loss is 3.852216486930847 and perplexity is 47.09733826150275
At time: 87.69492173194885 and batch: 800, loss is 3.949500584602356 and perplexity is 51.90943598862272
At time: 88.15100026130676 and batch: 850, loss is 3.8181627702713015 and perplexity is 45.52049983297504
At time: 88.5966739654541 and batch: 900, loss is 3.7583245420455933 and perplexity is 42.87652794471329
At time: 89.05136060714722 and batch: 950, loss is 3.765769052505493 and perplexity is 43.1969137836797
At time: 89.50532627105713 and batch: 1000, loss is 3.666828336715698 and perplexity is 39.12760924934343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.471238299113948 and perplexity of 87.46496376953094
Finished 9 epochs...
Completing Train Step...
At time: 90.90941071510315 and batch: 50, loss is 4.01654019355774 and perplexity is 55.508723776249205
At time: 91.36881852149963 and batch: 100, loss is 3.863843936920166 and perplexity is 47.64815630564028
At time: 91.82486414909363 and batch: 150, loss is 3.9542202520370484 and perplexity is 52.15501032195086
At time: 92.26898193359375 and batch: 200, loss is 4.033226594924927 and perplexity is 56.44273559684515
At time: 92.7283821105957 and batch: 250, loss is 4.002229471206665 and perplexity is 54.72001082867153
At time: 93.19049978256226 and batch: 300, loss is 3.8257634592056275 and perplexity is 45.86780520037395
At time: 93.63439440727234 and batch: 350, loss is 3.8740590715408327 and perplexity is 48.13738314153748
At time: 94.08966779708862 and batch: 400, loss is 3.8627288150787353 and perplexity is 47.595052419994204
At time: 94.53393507003784 and batch: 450, loss is 3.9200488471984865 and perplexity is 50.40290675872531
At time: 94.99387550354004 and batch: 500, loss is 3.945787019729614 and perplexity is 51.71702441811816
At time: 95.43827605247498 and batch: 550, loss is 3.829965047836304 and perplexity is 46.06092827697376
At time: 95.88707900047302 and batch: 600, loss is 3.7992211627960204 and perplexity is 44.666383101823875
At time: 96.33423066139221 and batch: 650, loss is 3.7622277688980104 and perplexity is 43.0442118010999
At time: 96.7793664932251 and batch: 700, loss is 3.845269546508789 and perplexity is 46.77128968982076
At time: 97.24705195426941 and batch: 750, loss is 3.817996301651001 and perplexity is 45.512922728864986
At time: 97.69111204147339 and batch: 800, loss is 3.921242747306824 and perplexity is 50.463118730952985
At time: 98.13585495948792 and batch: 850, loss is 3.794319763183594 and perplexity is 44.44799095979601
At time: 98.58739447593689 and batch: 900, loss is 3.7371777868270875 and perplexity is 41.9793481578653
At time: 99.03464341163635 and batch: 950, loss is 3.7520488834381105 and perplexity is 42.6082920518502
At time: 99.47807312011719 and batch: 1000, loss is 3.660574517250061 and perplexity is 38.88367579788407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4625422780106705 and perplexity of 86.7076641179628
Finished 10 epochs...
Completing Train Step...
At time: 100.90510821342468 and batch: 50, loss is 3.9793514966964723 and perplexity is 53.48233950503661
At time: 101.36382675170898 and batch: 100, loss is 3.8284801197052003 and perplexity is 45.99258186614331
At time: 101.81658935546875 and batch: 150, loss is 3.9158301877975465 and perplexity is 50.19072194471231
At time: 102.26092195510864 and batch: 200, loss is 3.9980656480789185 and perplexity is 54.49264007633299
At time: 102.7117531299591 and batch: 250, loss is 3.963606986999512 and perplexity is 52.64688049670678
At time: 103.15426659584045 and batch: 300, loss is 3.7894221210479735 and perplexity is 44.2308328219297
At time: 103.61466097831726 and batch: 350, loss is 3.8390437078475954 and perplexity is 46.48100376056533
At time: 104.07532525062561 and batch: 400, loss is 3.8290944862365723 and perplexity is 46.0208468507745
At time: 104.51787304878235 and batch: 450, loss is 3.8892636346817016 and perplexity is 48.87488350052424
At time: 104.96125936508179 and batch: 500, loss is 3.916774506568909 and perplexity is 50.238140371124274
At time: 105.4128897190094 and batch: 550, loss is 3.801877512931824 and perplexity is 44.78519038164859
At time: 105.85770893096924 and batch: 600, loss is 3.7723988246917726 and perplexity is 43.484250918624156
At time: 106.31610298156738 and batch: 650, loss is 3.7382176971435546 and perplexity is 42.02302562147962
At time: 106.77711629867554 and batch: 700, loss is 3.823285837173462 and perplexity is 45.75430278175123
At time: 107.22059965133667 and batch: 750, loss is 3.7968443918228147 and perplexity is 44.560347400194914
At time: 107.66669917106628 and batch: 800, loss is 3.9022008848190306 and perplexity is 49.511297943729566
At time: 108.1210355758667 and batch: 850, loss is 3.7769291830062866 and perplexity is 43.68169706952843
At time: 108.56793856620789 and batch: 900, loss is 3.7202053117752074 and perplexity is 41.27286704438507
At time: 109.01361513137817 and batch: 950, loss is 3.7376214265823364 and perplexity is 41.99797599732712
At time: 109.46721148490906 and batch: 1000, loss is 3.6495521020889283 and perplexity is 38.45743718515368
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.459041316334794 and perplexity of 86.40463466564317
Finished 11 epochs...
Completing Train Step...
At time: 110.86122536659241 and batch: 50, loss is 3.947648329734802 and perplexity is 51.813375474881994
At time: 111.3194637298584 and batch: 100, loss is 3.8026973819732666 and perplexity is 44.8219234288441
At time: 111.76470971107483 and batch: 150, loss is 3.88788046836853 and perplexity is 48.80732813902657
At time: 112.2096655368805 and batch: 200, loss is 3.9716924476623534 and perplexity is 53.07428031175573
At time: 112.65666484832764 and batch: 250, loss is 3.9347182750701903 and perplexity is 51.14773833370397
At time: 113.11008810997009 and batch: 300, loss is 3.7630346632003784 and perplexity is 43.07895794670038
At time: 113.55493378639221 and batch: 350, loss is 3.812810459136963 and perplexity is 45.27751081138126
At time: 114.02466344833374 and batch: 400, loss is 3.8033214950561525 and perplexity is 44.849906108926206
At time: 114.47050762176514 and batch: 450, loss is 3.8657533073425294 and perplexity is 47.739221196608845
At time: 114.92629551887512 and batch: 500, loss is 3.894956865310669 and perplexity is 49.153933077322606
At time: 115.36849522590637 and batch: 550, loss is 3.7798022413253785 and perplexity is 43.80737758996639
At time: 115.81310772895813 and batch: 600, loss is 3.751242880821228 and perplexity is 42.57396349326797
At time: 116.26450085639954 and batch: 650, loss is 3.7189649534225464 and perplexity is 41.22170563480842
At time: 116.71235251426697 and batch: 700, loss is 3.805113186836243 and perplexity is 44.930335347726526
At time: 117.16620755195618 and batch: 750, loss is 3.7795237636566164 and perplexity is 43.79517991204984
At time: 117.62360167503357 and batch: 800, loss is 3.885782299041748 and perplexity is 48.70502945770998
At time: 118.07513761520386 and batch: 850, loss is 3.761644334793091 and perplexity is 43.0191056645161
At time: 118.5242190361023 and batch: 900, loss is 3.7050665283203124 and perplexity is 40.652751786208285
At time: 118.99953722953796 and batch: 950, loss is 3.7227266454696655 and perplexity is 41.377061013415585
At time: 119.4586923122406 and batch: 1000, loss is 3.636836633682251 and perplexity is 37.97152868165308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.457714546017531 and perplexity of 86.29007157735298
Finished 12 epochs...
Completing Train Step...
At time: 120.8203296661377 and batch: 50, loss is 3.9204308557510377 and perplexity is 50.42216477831048
At time: 121.29561114311218 and batch: 100, loss is 3.7825232410430907 and perplexity is 43.92673977060746
At time: 121.75305533409119 and batch: 150, loss is 3.8645401191711426 and perplexity is 47.6813396558415
At time: 122.21855998039246 and batch: 200, loss is 3.94927001953125 and perplexity is 51.89746886547624
At time: 122.67575669288635 and batch: 250, loss is 3.910317540168762 and perplexity is 49.914799411004395
At time: 123.11510634422302 and batch: 300, loss is 3.74044385433197 and perplexity is 42.116679687704845
At time: 123.57725954055786 and batch: 350, loss is 3.7906237125396727 and perplexity is 44.284012157839626
At time: 124.03901767730713 and batch: 400, loss is 3.781534514427185 and perplexity is 43.88332969772993
At time: 124.50426030158997 and batch: 450, loss is 3.845879092216492 and perplexity is 46.7998076193027
At time: 124.9692645072937 and batch: 500, loss is 3.8762721967697145 and perplexity is 48.24403517223917
At time: 125.4320285320282 and batch: 550, loss is 3.761049222946167 and perplexity is 42.99351210136274
At time: 125.87792754173279 and batch: 600, loss is 3.7327440071105955 and perplexity is 41.79363298980254
At time: 126.33821439743042 and batch: 650, loss is 3.7021275758743286 and perplexity is 40.53345067792359
At time: 126.82179474830627 and batch: 700, loss is 3.7887403678894045 and perplexity is 44.20068858857792
At time: 127.28191256523132 and batch: 750, loss is 3.7641352939605714 and perplexity is 43.12639807517688
At time: 127.72987675666809 and batch: 800, loss is 3.8707574558258058 and perplexity is 47.978714077148524
At time: 128.18020629882812 and batch: 850, loss is 3.747111186981201 and perplexity is 42.39842379841348
At time: 128.65502977371216 and batch: 900, loss is 3.6903875970840456 and perplexity is 40.06037123158579
At time: 129.1156783103943 and batch: 950, loss is 3.708358268737793 and perplexity is 40.78679058177315
At time: 129.5686056613922 and batch: 1000, loss is 3.623879327774048 and perplexity is 37.48269379953649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.45729102157965 and perplexity of 86.25353336125333
Finished 13 epochs...
Completing Train Step...
At time: 131.0366611480713 and batch: 50, loss is 3.896299295425415 and perplexity is 49.21996310777834
At time: 131.4844663143158 and batch: 100, loss is 3.766047201156616 and perplexity is 43.20893061813699
At time: 131.93884801864624 and batch: 150, loss is 3.844691209793091 and perplexity is 46.74424795612005
At time: 132.3675034046173 and batch: 200, loss is 3.929988956451416 and perplexity is 50.906415478624574
At time: 132.7994041442871 and batch: 250, loss is 3.888922080993652 and perplexity is 48.858192954332715
At time: 133.24334502220154 and batch: 300, loss is 3.7207614946365357 and perplexity is 41.29582869051815
At time: 133.69776940345764 and batch: 350, loss is 3.770973148345947 and perplexity is 43.42230062170311
At time: 134.15566897392273 and batch: 400, loss is 3.762384419441223 and perplexity is 43.05095522842769
At time: 134.609938621521 and batch: 450, loss is 3.8283355045318603 and perplexity is 45.985931121855245
At time: 135.08018708229065 and batch: 500, loss is 3.859730658531189 and perplexity is 47.4525687029265
At time: 135.55026173591614 and batch: 550, loss is 3.744374380111694 and perplexity is 42.282546140618706
At time: 136.01293754577637 and batch: 600, loss is 3.7158682584762572 and perplexity is 41.0942520316077
At time: 136.4764919281006 and batch: 650, loss is 3.686715478897095 and perplexity is 39.913534579581395
At time: 136.9395031929016 and batch: 700, loss is 3.773541874885559 and perplexity is 43.533984018363036
At time: 137.4069492816925 and batch: 750, loss is 3.750119857788086 and perplexity is 42.52617878833712
At time: 137.87077808380127 and batch: 800, loss is 3.856746497154236 and perplexity is 47.31117365791159
At time: 138.35929203033447 and batch: 850, loss is 3.7332523679733276 and perplexity is 41.81488463842148
At time: 138.80554962158203 and batch: 900, loss is 3.6764294862747193 and perplexity is 39.50508849525052
At time: 139.25043034553528 and batch: 950, loss is 3.694648265838623 and perplexity is 40.231419334485565
At time: 139.715158700943 and batch: 1000, loss is 3.6111166143417357 and perplexity is 37.00735269328004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.45781986887862 and perplexity of 86.2991603731957
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 141.14837384223938 and batch: 50, loss is 3.8863797044754027 and perplexity is 48.73413479993349
At time: 141.63434219360352 and batch: 100, loss is 3.7661335372924807 and perplexity is 43.21266127128417
At time: 142.0794641971588 and batch: 150, loss is 3.84288938999176 and perplexity is 46.660099077878094
At time: 142.5316300392151 and batch: 200, loss is 3.9176689338684083 and perplexity is 50.28309483660191
At time: 142.98347330093384 and batch: 250, loss is 3.872205080986023 and perplexity is 48.048219567611774
At time: 143.43532872200012 and batch: 300, loss is 3.703662190437317 and perplexity is 40.595701655033146
At time: 143.8695306777954 and batch: 350, loss is 3.750520935058594 and perplexity is 42.54323849295187
At time: 144.32424759864807 and batch: 400, loss is 3.738334641456604 and perplexity is 42.02794026270725
At time: 144.7827763557434 and batch: 450, loss is 3.797823886871338 and perplexity is 44.60401542264846
At time: 145.24315571784973 and batch: 500, loss is 3.827374219894409 and perplexity is 45.94174679298964
At time: 145.6938920021057 and batch: 550, loss is 3.7103100967407228 and perplexity is 40.86647712369424
At time: 146.1410207748413 and batch: 600, loss is 3.6735639142990113 and perplexity is 39.39204586401287
At time: 146.59350848197937 and batch: 650, loss is 3.638317484855652 and perplexity is 38.02780051928223
At time: 147.0580596923828 and batch: 700, loss is 3.7230597352981567 and perplexity is 41.39084558719522
At time: 147.5249800682068 and batch: 750, loss is 3.6938750743865967 and perplexity is 40.20032476752903
At time: 147.95947813987732 and batch: 800, loss is 3.7971013307571413 and perplexity is 44.57179815937907
At time: 148.41421246528625 and batch: 850, loss is 3.6675951290130615 and perplexity is 39.15762350460932
At time: 148.87261867523193 and batch: 900, loss is 3.6042022132873535 and perplexity is 36.75235162058417
At time: 149.3352165222168 and batch: 950, loss is 3.622454271316528 and perplexity is 37.42931688628277
At time: 149.79056549072266 and batch: 1000, loss is 3.5422897148132324 and perplexity is 34.54592902677167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.432788011504383 and perplexity of 84.1657451693486
Finished 15 epochs...
Completing Train Step...
At time: 151.20562839508057 and batch: 50, loss is 3.859411401748657 and perplexity is 47.43742156656119
At time: 151.68426084518433 and batch: 100, loss is 3.735915975570679 and perplexity is 41.92641154885124
At time: 152.13327383995056 and batch: 150, loss is 3.816680030822754 and perplexity is 45.45305480621256
At time: 152.5777304172516 and batch: 200, loss is 3.896277847290039 and perplexity is 49.21890744266746
At time: 153.05467319488525 and batch: 250, loss is 3.8504241609573366 and perplexity is 47.013000082185684
At time: 153.5274052619934 and batch: 300, loss is 3.683681311607361 and perplexity is 39.79261377828046
At time: 153.98769617080688 and batch: 350, loss is 3.731892580986023 and perplexity is 41.75806394319073
At time: 154.45818877220154 and batch: 400, loss is 3.7210004901885987 and perplexity is 41.305699389373515
At time: 154.93557810783386 and batch: 450, loss is 3.7819185733795164 and perplexity is 43.900186720196444
At time: 155.3895354270935 and batch: 500, loss is 3.8137784242630004 and perplexity is 45.32135908121482
At time: 155.85984778404236 and batch: 550, loss is 3.6979375219345094 and perplexity is 40.36396865059419
At time: 156.3201653957367 and batch: 600, loss is 3.662805423736572 and perplexity is 38.97051847536963
At time: 156.7898817062378 and batch: 650, loss is 3.6299308824539183 and perplexity is 37.71021008992076
At time: 157.2511579990387 and batch: 700, loss is 3.71614164352417 and perplexity is 41.1054881214878
At time: 157.686537027359 and batch: 750, loss is 3.688322606086731 and perplexity is 39.97773227935534
At time: 158.1383306980133 and batch: 800, loss is 3.7938091707229615 and perplexity is 44.425301943634906
At time: 158.59994101524353 and batch: 850, loss is 3.665606150627136 and perplexity is 39.079817240953155
At time: 159.05389499664307 and batch: 900, loss is 3.6047808742523193 and perplexity is 36.773624926259885
At time: 159.51516103744507 and batch: 950, loss is 3.6259514760971068 and perplexity is 37.56044402781919
At time: 159.97594666481018 and batch: 1000, loss is 3.549553437232971 and perplexity is 34.79777462690504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.431189560308689 and perplexity of 84.03131779976337
Finished 16 epochs...
Completing Train Step...
At time: 161.3966841697693 and batch: 50, loss is 3.848313579559326 and perplexity is 46.91387995608936
At time: 161.85205745697021 and batch: 100, loss is 3.7248370265960693 and perplexity is 41.46447458758003
At time: 162.30372834205627 and batch: 150, loss is 3.8062505960464477 and perplexity is 44.9814685991716
At time: 162.74830961227417 and batch: 200, loss is 3.886484451293945 and perplexity is 48.73923981287047
At time: 163.19314408302307 and batch: 250, loss is 3.8405016803741456 and perplexity is 46.54882121303277
At time: 163.65555667877197 and batch: 300, loss is 3.6749236011505126 and perplexity is 39.44564314032653
At time: 164.13267421722412 and batch: 350, loss is 3.7233276557922363 and perplexity is 41.401936528674234
At time: 164.6004102230072 and batch: 400, loss is 3.713016653060913 and perplexity is 40.97723436337864
At time: 165.08884191513062 and batch: 450, loss is 3.7745439767837525 and perplexity is 43.577631372279285
At time: 165.54065322875977 and batch: 500, loss is 3.8071472358703615 and perplexity is 45.02181886237886
At time: 165.99583745002747 and batch: 550, loss is 3.6918268823623657 and perplexity is 40.11807104743057
At time: 166.45690178871155 and batch: 600, loss is 3.6573847055435182 and perplexity is 38.75984180219497
At time: 166.9015245437622 and batch: 650, loss is 3.625301661491394 and perplexity is 37.5360446310936
At time: 167.3632025718689 and batch: 700, loss is 3.712255072593689 and perplexity is 40.94603878256694
At time: 167.82056093215942 and batch: 750, loss is 3.685209650993347 and perplexity is 39.853476895091326
At time: 168.29433274269104 and batch: 800, loss is 3.791884226799011 and perplexity is 44.33986798276143
At time: 168.7429482936859 and batch: 850, loss is 3.6640107583999635 and perplexity is 39.017519312316956
At time: 169.18917870521545 and batch: 900, loss is 3.604366211891174 and perplexity is 36.7583794492013
At time: 169.6490499973297 and batch: 950, loss is 3.6260573959350584 and perplexity is 37.564422634666975
At time: 170.11077737808228 and batch: 1000, loss is 3.550981707572937 and perplexity is 34.84751076617277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.430687881097561 and perplexity of 83.98917160735776
Finished 17 epochs...
Completing Train Step...
At time: 171.50160145759583 and batch: 50, loss is 3.8401680278778074 and perplexity is 46.533292673345514
At time: 171.97475242614746 and batch: 100, loss is 3.716928300857544 and perplexity is 41.1378367771459
At time: 172.43504452705383 and batch: 150, loss is 3.7992660427093505 and perplexity is 44.66838777021059
At time: 172.9081552028656 and batch: 200, loss is 3.879282865524292 and perplexity is 48.38950084613001
At time: 173.38876271247864 and batch: 250, loss is 3.833350644111633 and perplexity is 46.21713626356724
At time: 173.84793782234192 and batch: 300, loss is 3.6685564279556275 and perplexity is 39.1952837851715
At time: 174.30245232582092 and batch: 350, loss is 3.7168903017044066 and perplexity is 41.13627360388628
At time: 174.75043439865112 and batch: 400, loss is 3.707080807685852 and perplexity is 40.73472031132364
At time: 175.22950720787048 and batch: 450, loss is 3.768875951766968 and perplexity is 43.331330945377005
At time: 175.700115442276 and batch: 500, loss is 3.8021775007247927 and perplexity is 44.79862740743995
At time: 176.14536499977112 and batch: 550, loss is 3.687103581428528 and perplexity is 39.92902812973872
At time: 176.61183547973633 and batch: 600, loss is 3.6531659030914305 and perplexity is 38.596666131537724
At time: 177.07526445388794 and batch: 650, loss is 3.62164267539978 and perplexity is 37.39895172931666
At time: 177.5364751815796 and batch: 700, loss is 3.709035577774048 and perplexity is 40.81442520112628
At time: 177.9753074645996 and batch: 750, loss is 3.682406439781189 and perplexity is 39.74191561977839
At time: 178.4404067993164 and batch: 800, loss is 3.789890809059143 and perplexity is 44.25156814181573
At time: 178.90985250473022 and batch: 850, loss is 3.66200984954834 and perplexity is 38.93952686646529
At time: 179.37814903259277 and batch: 900, loss is 3.603015503883362 and perplexity is 36.70876312784973
At time: 179.82042622566223 and batch: 950, loss is 3.6249438524246216 and perplexity is 37.52261629652702
At time: 180.27905750274658 and batch: 1000, loss is 3.5504414653778076 and perplexity is 34.82868975487165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.430457882764863 and perplexity of 83.96985645923444
Finished 18 epochs...
Completing Train Step...
At time: 181.71139335632324 and batch: 50, loss is 3.833368668556213 and perplexity is 46.21796930928604
At time: 182.17643475532532 and batch: 100, loss is 3.710626654624939 and perplexity is 40.87941577703621
At time: 182.6320025920868 and batch: 150, loss is 3.793595566749573 and perplexity is 44.41581353603762
At time: 183.08853816986084 and batch: 200, loss is 3.8732908582687378 and perplexity is 48.10041756546524
At time: 183.52875399589539 and batch: 250, loss is 3.8273002767562865 and perplexity is 45.93834984165309
At time: 183.9905867576599 and batch: 300, loss is 3.663166379928589 and perplexity is 38.9845876643471
At time: 184.44461917877197 and batch: 350, loss is 3.711426701545715 and perplexity is 40.91213431418901
At time: 184.89434242248535 and batch: 400, loss is 3.702023220062256 and perplexity is 40.5292209974617
At time: 185.34871125221252 and batch: 450, loss is 3.7639968013763427 and perplexity is 43.12042580242626
At time: 185.79636526107788 and batch: 500, loss is 3.7978998136520388 and perplexity is 44.6074021905174
At time: 186.2777454853058 and batch: 550, loss is 3.682986812591553 and perplexity is 39.76498744151728
At time: 186.7240445613861 and batch: 600, loss is 3.6494458866119386 and perplexity is 38.45335262704474
At time: 187.19073295593262 and batch: 650, loss is 3.6183391332626345 and perplexity is 37.27560656654125
At time: 187.63647890090942 and batch: 700, loss is 3.706001486778259 and perplexity is 40.6907781941154
At time: 188.07809805870056 and batch: 750, loss is 3.679626030921936 and perplexity is 39.63157031911818
At time: 188.56575846672058 and batch: 800, loss is 3.787727189064026 and perplexity is 44.15592806586874
At time: 189.01012897491455 and batch: 850, loss is 3.6597255945205687 and perplexity is 38.85068056887452
At time: 189.46611738204956 and batch: 900, loss is 3.60117862701416 and perplexity is 36.64139554188555
At time: 189.92380666732788 and batch: 950, loss is 3.623311972618103 and perplexity is 37.461433831497146
At time: 190.39605855941772 and batch: 1000, loss is 3.549210729598999 and perplexity is 34.785851207135345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.430361864043445 and perplexity of 83.96179416805126
Finished 19 epochs...
Completing Train Step...
At time: 191.7791826725006 and batch: 50, loss is 3.8272993087768556 and perplexity is 45.93830537429688
At time: 192.22943425178528 and batch: 100, loss is 3.705096821784973 and perplexity is 40.65398331756146
At time: 192.69692611694336 and batch: 150, loss is 3.788561701774597 and perplexity is 44.1927921287119
At time: 193.1753363609314 and batch: 200, loss is 3.8679732131958007 and perplexity is 47.84531548927523
At time: 193.64768171310425 and batch: 250, loss is 3.821846489906311 and perplexity is 45.68849382341556
At time: 194.1203486919403 and batch: 300, loss is 3.658302083015442 and perplexity is 38.795415522649655
At time: 194.60108470916748 and batch: 350, loss is 3.706508102416992 and perplexity is 40.71139800141816
At time: 195.06528520584106 and batch: 400, loss is 3.6974479818344115 and perplexity is 40.34421370515413
At time: 195.52640461921692 and batch: 450, loss is 3.759550700187683 and perplexity is 42.92913359338856
At time: 195.98265385627747 and batch: 500, loss is 3.7939753484725953 and perplexity is 44.43268505377603
At time: 196.45230555534363 and batch: 550, loss is 3.67917640209198 and perplexity is 39.61375482800564
At time: 196.90346121788025 and batch: 600, loss is 3.645948076248169 and perplexity is 38.31908504985902
At time: 197.37353491783142 and batch: 650, loss is 3.615191502571106 and perplexity is 37.158461185184834
At time: 197.8264582157135 and batch: 700, loss is 3.7030354833602903 and perplexity is 40.57026801206354
At time: 198.27506709098816 and batch: 750, loss is 3.676839246749878 and perplexity is 39.52127943606077
At time: 198.74948143959045 and batch: 800, loss is 3.7854447078704836 and perplexity is 44.05525792303414
At time: 199.1938600540161 and batch: 850, loss is 3.6572780561447145 and perplexity is 38.75570830879025
At time: 199.6543836593628 and batch: 900, loss is 3.5990750217437744 and perplexity is 36.56439752422124
At time: 200.11582374572754 and batch: 950, loss is 3.6214166402816774 and perplexity is 37.390499208164904
At time: 200.60607171058655 and batch: 1000, loss is 3.547655701637268 and perplexity is 34.73180027208508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.430353676400533 and perplexity of 83.96110672167663
Finished 20 epochs...
Completing Train Step...
At time: 202.06734037399292 and batch: 50, loss is 3.8216959333419798 and perplexity is 45.68161563854698
At time: 202.5466959476471 and batch: 100, loss is 3.700010046958923 and perplexity is 40.44771073451426
At time: 202.97950720787048 and batch: 150, loss is 3.783917546272278 and perplexity is 43.98802977212614
At time: 203.40772223472595 and batch: 200, loss is 3.8630898571014405 and perplexity is 47.61223933640352
At time: 203.86920046806335 and batch: 250, loss is 3.816775197982788 and perplexity is 45.45738065018923
At time: 204.33176517486572 and batch: 300, loss is 3.6537851095199585 and perplexity is 38.620572836154864
At time: 204.7783215045929 and batch: 350, loss is 3.7019324922561645 and perplexity is 40.525544036961804
At time: 205.22403860092163 and batch: 400, loss is 3.6931745290756224 and perplexity is 40.17217248064113
At time: 205.67097449302673 and batch: 450, loss is 3.755380401611328 and perplexity is 42.75047906895516
At time: 206.13258266448975 and batch: 500, loss is 3.7902674102783203 and perplexity is 44.2682364747887
At time: 206.58474016189575 and batch: 550, loss is 3.675547885894775 and perplexity is 39.47027614177014
At time: 207.0170292854309 and batch: 600, loss is 3.642565841674805 and perplexity is 38.18969984448309
At time: 207.47269010543823 and batch: 650, loss is 3.6121329355239866 and perplexity is 37.04498316880687
At time: 207.9416148662567 and batch: 700, loss is 3.7001080179214476 and perplexity is 40.45167362978799
At time: 208.41514587402344 and batch: 750, loss is 3.674053888320923 and perplexity is 39.41135167244225
At time: 208.89281249046326 and batch: 800, loss is 3.783081922531128 and perplexity is 43.9512876835409
At time: 209.35185527801514 and batch: 850, loss is 3.6547435998916624 and perplexity is 38.657608029470744
At time: 209.80442333221436 and batch: 900, loss is 3.5968190002441407 and perplexity is 36.48200043703313
At time: 210.2574098110199 and batch: 950, loss is 3.6193635940551756 and perplexity is 37.313813531414034
At time: 210.71495127677917 and batch: 1000, loss is 3.5459250307083128 and perplexity is 34.67174293977387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.43040429092035 and perplexity of 83.96535648032551
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 212.1959400177002 and batch: 50, loss is 3.820179853439331 and perplexity is 45.612411132193024
At time: 212.6718351840973 and batch: 100, loss is 3.7010496616363526 and perplexity is 40.48978263375499
At time: 213.14411044120789 and batch: 150, loss is 3.7870858192443846 and perplexity is 44.127616866194046
At time: 213.61731028556824 and batch: 200, loss is 3.865084686279297 and perplexity is 47.7073124164045
At time: 214.078200340271 and batch: 250, loss is 3.820231294631958 and perplexity is 45.614757549370985
At time: 214.52403450012207 and batch: 300, loss is 3.65430034160614 and perplexity is 38.64047652153511
At time: 214.95366787910461 and batch: 350, loss is 3.7027851963043212 and perplexity is 40.56011506975095
At time: 215.39802861213684 and batch: 400, loss is 3.6901814985275267 and perplexity is 40.052115697657385
At time: 215.85245990753174 and batch: 450, loss is 3.7516592597961425 and perplexity is 42.59169408761253
At time: 216.30317521095276 and batch: 500, loss is 3.7879057121276856 and perplexity is 44.16381162110307
At time: 216.71983456611633 and batch: 550, loss is 3.6718004322052002 and perplexity is 39.322639912556355
At time: 217.18658328056335 and batch: 600, loss is 3.6346885347366333 and perplexity is 37.890049624797534
At time: 217.657696723938 and batch: 650, loss is 3.6016366386413576 and perplexity is 36.65818157088526
At time: 218.11935138702393 and batch: 700, loss is 3.6908726119995117 and perplexity is 40.0798058218043
At time: 218.57957530021667 and batch: 750, loss is 3.6621911239624025 and perplexity is 38.94658624620502
At time: 219.0346863269806 and batch: 800, loss is 3.7683197355270384 and perplexity is 43.30723605701273
At time: 219.48073506355286 and batch: 850, loss is 3.6399661350250243 and perplexity is 38.09054676814384
At time: 219.93215203285217 and batch: 900, loss is 3.578423094749451 and perplexity is 35.817016250824516
At time: 220.3637444972992 and batch: 950, loss is 3.5973467350006105 and perplexity is 36.501258337734114
At time: 220.82245111465454 and batch: 1000, loss is 3.524500513076782 and perplexity is 33.93681838514294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.421931569169208 and perplexity of 83.25694669550866
Finished 22 epochs...
Completing Train Step...
At time: 222.2276427745819 and batch: 50, loss is 3.8153262090682984 and perplexity is 45.39156110695567
At time: 222.66569924354553 and batch: 100, loss is 3.6923016500473023 and perplexity is 40.137122333255455
At time: 223.10639357566833 and batch: 150, loss is 3.77909441947937 and perplexity is 43.77638074250892
At time: 223.56722950935364 and batch: 200, loss is 3.85775475025177 and perplexity is 47.35889935104606
At time: 224.0229229927063 and batch: 250, loss is 3.813783369064331 and perplexity is 45.32158318688559
At time: 224.46293783187866 and batch: 300, loss is 3.647771592140198 and perplexity is 38.3890242586646
At time: 224.915842294693 and batch: 350, loss is 3.6960932397842408 and perplexity is 40.28959470804639
At time: 225.36992168426514 and batch: 400, loss is 3.68381329536438 and perplexity is 39.79786610355171
At time: 225.8122100830078 and batch: 450, loss is 3.746251497268677 and perplexity is 42.361989972783206
At time: 226.2515046596527 and batch: 500, loss is 3.7825319385528564 and perplexity is 43.927121825517055
At time: 226.68567728996277 and batch: 550, loss is 3.6679026556015013 and perplexity is 39.16966736678611
At time: 227.12346172332764 and batch: 600, loss is 3.630802435874939 and perplexity is 37.74309087913283
At time: 227.58496141433716 and batch: 650, loss is 3.5990095043182375 and perplexity is 36.56200199750438
At time: 228.01340293884277 and batch: 700, loss is 3.688427505493164 and perplexity is 39.981926139704875
At time: 228.47357511520386 and batch: 750, loss is 3.6603839874267576 and perplexity is 38.876268003730175
At time: 228.91332340240479 and batch: 800, loss is 3.7670227527618407 and perplexity is 43.2511037274391
At time: 229.35598468780518 and batch: 850, loss is 3.639763207435608 and perplexity is 38.08281792953245
At time: 229.8116443157196 and batch: 900, loss is 3.578886170387268 and perplexity is 35.83360607934583
At time: 230.25042986869812 and batch: 950, loss is 3.5994612646102904 and perplexity is 36.57852298968796
At time: 230.6954846382141 and batch: 1000, loss is 3.528478112220764 and perplexity is 34.072074263407075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.421163047232279 and perplexity of 83.19298648613155
Finished 23 epochs...
Completing Train Step...
At time: 232.07269978523254 and batch: 50, loss is 3.813323016166687 and perplexity is 45.30072406638636
At time: 232.54227805137634 and batch: 100, loss is 3.689419388771057 and perplexity is 40.02160321792303
At time: 232.99713587760925 and batch: 150, loss is 3.7764353370666504 and perplexity is 43.660130366546895
At time: 233.4442195892334 and batch: 200, loss is 3.855136594772339 and perplexity is 47.23506856407264
At time: 233.89716386795044 and batch: 250, loss is 3.8113516855239866 and perplexity is 45.21150932566564
At time: 234.34033155441284 and batch: 300, loss is 3.645274667739868 and perplexity is 38.293289338456724
At time: 234.7734980583191 and batch: 350, loss is 3.693567681312561 and perplexity is 40.18796936520135
At time: 235.20016503334045 and batch: 400, loss is 3.6815190744400024 and perplexity is 39.70666566337286
At time: 235.68194031715393 and batch: 450, loss is 3.7442954063415526 and perplexity is 42.279207060390405
At time: 236.10988879203796 and batch: 500, loss is 3.7806867218017577 and perplexity is 43.84614150060006
At time: 236.56200575828552 and batch: 550, loss is 3.6663656520843504 and perplexity is 39.10950969339846
At time: 236.98708200454712 and batch: 600, loss is 3.6292231798172 and perplexity is 37.68353191602939
At time: 237.4185824394226 and batch: 650, loss is 3.597964577674866 and perplexity is 36.52381734103903
At time: 237.86765551567078 and batch: 700, loss is 3.6875440883636474 and perplexity is 39.94662101815264
At time: 238.30040001869202 and batch: 750, loss is 3.6598565435409545 and perplexity is 38.85576836054974
At time: 238.7421658039093 and batch: 800, loss is 3.766677722930908 and perplexity is 43.23618338056277
At time: 239.2055139541626 and batch: 850, loss is 3.63980890750885 and perplexity is 38.08455835686962
At time: 239.65620040893555 and batch: 900, loss is 3.579280385971069 and perplexity is 35.84773503002938
At time: 240.11239624023438 and batch: 950, loss is 3.6002528858184815 and perplexity is 36.607490788500094
At time: 240.5747036933899 and batch: 1000, loss is 3.529717960357666 and perplexity is 34.114344660224404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420932304568407 and perplexity of 83.17379252933172
Finished 24 epochs...
Completing Train Step...
At time: 241.97025680541992 and batch: 50, loss is 3.8114471864700317 and perplexity is 45.215827273759146
At time: 242.428542137146 and batch: 100, loss is 3.6874255895614625 and perplexity is 39.94188767186412
At time: 242.87608408927917 and batch: 150, loss is 3.774664936065674 and perplexity is 43.582902810086146
At time: 243.31017875671387 and batch: 200, loss is 3.853393964767456 and perplexity is 47.1528269954351
At time: 243.7617268562317 and batch: 250, loss is 3.8096936178207397 and perplexity is 45.13660769538463
At time: 244.22439646720886 and batch: 300, loss is 3.6435977458953857 and perplexity is 38.22912829662971
At time: 244.6894555091858 and batch: 350, loss is 3.6918720245361327 and perplexity is 40.119882105242226
At time: 245.15235662460327 and batch: 400, loss is 3.6799993658065797 and perplexity is 39.64636892909831
At time: 245.5827398300171 and batch: 450, loss is 3.7429891777038575 and perplexity is 42.2240168027439
At time: 246.04318976402283 and batch: 500, loss is 3.7795043706893923 and perplexity is 43.79433060179658
At time: 246.51320910453796 and batch: 550, loss is 3.6652701187133787 and perplexity is 39.06668738132587
At time: 246.99602460861206 and batch: 600, loss is 3.628129940032959 and perplexity is 37.64235729069823
At time: 247.45342540740967 and batch: 650, loss is 3.5972005414962767 and perplexity is 36.49592248090843
At time: 247.89004111289978 and batch: 700, loss is 3.6869019746780394 and perplexity is 39.92097897953609
At time: 248.32845091819763 and batch: 750, loss is 3.659465026855469 and perplexity is 38.84055865652996
At time: 248.77644777297974 and batch: 800, loss is 3.7663833904266357 and perplexity is 43.22345943905983
At time: 249.22345352172852 and batch: 850, loss is 3.6397270584106445 and perplexity is 38.08144129767853
At time: 249.70395040512085 and batch: 900, loss is 3.579420862197876 and perplexity is 35.85277113830444
At time: 250.1678910255432 and batch: 950, loss is 3.600534873008728 and perplexity is 36.61781508756115
At time: 250.60858535766602 and batch: 1000, loss is 3.53011869430542 and perplexity is 34.128018175773086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420815816739711 and perplexity of 83.16410435912348
Finished 25 epochs...
Completing Train Step...
At time: 251.95393681526184 and batch: 50, loss is 3.8097237586975097 and perplexity is 45.137968172817885
At time: 252.41205215454102 and batch: 100, loss is 3.6857749319076536 and perplexity is 39.8760116735889
At time: 252.87044024467468 and batch: 150, loss is 3.7732299518585206 and perplexity is 43.52040688390951
At time: 253.31717133522034 and batch: 200, loss is 3.8519571304321287 and perplexity is 47.085124844635274
At time: 253.77280044555664 and batch: 250, loss is 3.8083107471466064 and perplexity is 45.074232742459564
At time: 254.2363612651825 and batch: 300, loss is 3.642209253311157 and perplexity is 38.176084269629406
At time: 254.7048375606537 and batch: 350, loss is 3.6904726028442383 and perplexity is 40.063776738637735
At time: 255.1598722934723 and batch: 400, loss is 3.678738398551941 and perplexity is 39.59640766249401
At time: 255.60843634605408 and batch: 450, loss is 3.7419036197662354 and perplexity is 42.17820505629674
At time: 256.05615758895874 and batch: 500, loss is 3.7785415172576906 and perplexity is 43.75218337434504
At time: 256.4885370731354 and batch: 550, loss is 3.664320845603943 and perplexity is 39.029620021827355
At time: 256.9489197731018 and batch: 600, loss is 3.627207374572754 and perplexity is 37.607645766309865
At time: 257.4019863605499 and batch: 650, loss is 3.5965222454071046 and perplexity is 36.471175833144905
At time: 257.84104442596436 and batch: 700, loss is 3.6863191080093385 and perplexity is 39.8977171514377
At time: 258.2877278327942 and batch: 750, loss is 3.659077730178833 and perplexity is 38.82551874988452
At time: 258.7172861099243 and batch: 800, loss is 3.7660606479644776 and perplexity is 43.20951164423137
At time: 259.1676630973816 and batch: 850, loss is 3.6395432853698733 and perplexity is 38.07444359842824
At time: 259.60492181777954 and batch: 900, loss is 3.579395775794983 and perplexity is 35.8518717325243
At time: 260.0586121082306 and batch: 950, loss is 3.6005745697021485 and perplexity is 36.619268722592466
At time: 260.5344684123993 and batch: 1000, loss is 3.5301742553710938 and perplexity is 34.12991441751038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420748082602897 and perplexity of 83.15847150107135
Finished 26 epochs...
Completing Train Step...
At time: 261.8599591255188 and batch: 50, loss is 3.808132653236389 and perplexity is 45.066206010877636
At time: 262.3299126625061 and batch: 100, loss is 3.6843186950683595 and perplexity is 39.81798501691765
At time: 262.7848689556122 and batch: 150, loss is 3.7719745349884035 and perplexity is 43.465804912202685
At time: 263.25146293640137 and batch: 200, loss is 3.8506730031967162 and perplexity is 47.02470035810714
At time: 263.71011185646057 and batch: 250, loss is 3.807072229385376 and perplexity is 45.01844206064092
At time: 264.15757870674133 and batch: 300, loss is 3.6409676122665404 and perplexity is 38.128712691811124
At time: 264.60939288139343 and batch: 350, loss is 3.6892202997207644 and perplexity is 40.01363614805169
At time: 265.04688262939453 and batch: 400, loss is 3.6776016426086424 and perplexity is 39.551421784576725
At time: 265.500118970871 and batch: 450, loss is 3.7409229707717895 and perplexity is 42.136863316101824
At time: 265.933536529541 and batch: 500, loss is 3.7776780891418458 and perplexity is 43.71442281320101
At time: 266.36999011039734 and batch: 550, loss is 3.663440113067627 and perplexity is 38.99526049859013
At time: 266.82135128974915 and batch: 600, loss is 3.626366124153137 and perplexity is 37.5760216223054
At time: 267.2501196861267 and batch: 650, loss is 3.595880184173584 and perplexity is 36.44776662087957
At time: 267.68859124183655 and batch: 700, loss is 3.68575439453125 and perplexity is 39.87519273333716
At time: 268.11944460868835 and batch: 750, loss is 3.658673577308655 and perplexity is 38.80983047548972
At time: 268.55039620399475 and batch: 800, loss is 3.7657045841217043 and perplexity is 43.19412903822839
At time: 268.9936571121216 and batch: 850, loss is 3.639287037849426 and perplexity is 38.0646883665941
At time: 269.4388201236725 and batch: 900, loss is 3.5792609024047852 and perplexity is 35.847036595111824
At time: 269.91544485092163 and batch: 950, loss is 3.600477442741394 and perplexity is 36.615712177037416
At time: 270.3680293560028 and batch: 1000, loss is 3.5300550174713137 and perplexity is 34.12584508080996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420709749547447 and perplexity of 83.15528384386889
Finished 27 epochs...
Completing Train Step...
At time: 271.6907172203064 and batch: 50, loss is 3.8066471862792968 and perplexity is 44.999311348173364
At time: 272.1571741104126 and batch: 100, loss is 3.6829896354675293 and perplexity is 39.76509969330347
At time: 272.60671877861023 and batch: 150, loss is 3.7708320379257203 and perplexity is 43.41617371491053
At time: 273.0681393146515 and batch: 200, loss is 3.849482159614563 and perplexity is 46.96873462531366
At time: 273.5116629600525 and batch: 250, loss is 3.805924820899963 and perplexity is 44.966817141319616
At time: 273.97572588920593 and batch: 300, loss is 3.6398166608810425 and perplexity is 38.08485364177007
At time: 274.4245796203613 and batch: 350, loss is 3.688056640625 and perplexity is 39.96710099717032
At time: 274.8760669231415 and batch: 400, loss is 3.6765383291244507 and perplexity is 39.50938857567349
At time: 275.32053446769714 and batch: 450, loss is 3.740002808570862 and perplexity is 42.09810840035146
At time: 275.77989506721497 and batch: 500, loss is 3.776868920326233 and perplexity is 43.67906477270916
At time: 276.2458472251892 and batch: 550, loss is 3.6625991344451903 and perplexity is 38.962480103872046
At time: 276.7011239528656 and batch: 600, loss is 3.6255711030960085 and perplexity is 37.54615976584776
At time: 277.16941928863525 and batch: 650, loss is 3.5952567529678343 and perplexity is 36.4250510273289
At time: 277.6151671409607 and batch: 700, loss is 3.6851953125 and perplexity is 39.852905460374764
At time: 278.0475594997406 and batch: 750, loss is 3.6582507419586183 and perplexity is 38.793423776146426
At time: 278.4980556964874 and batch: 800, loss is 3.7653202056884765 and perplexity is 43.177529337071974
At time: 278.9429774284363 and batch: 850, loss is 3.638978009223938 and perplexity is 38.05292710564528
At time: 279.38570952415466 and batch: 900, loss is 3.5790495014190675 and perplexity is 35.8394592971929
At time: 279.8242025375366 and batch: 950, loss is 3.6002937173843383 and perplexity is 36.608985560187804
At time: 280.264652967453 and batch: 1000, loss is 3.529835433959961 and perplexity is 34.11835243058057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420688163943407 and perplexity of 83.15348890621046
Finished 28 epochs...
Completing Train Step...
At time: 281.6394872665405 and batch: 50, loss is 3.805244975090027 and perplexity is 44.93625702835934
At time: 282.1029598712921 and batch: 100, loss is 3.681750330924988 and perplexity is 39.71584914913406
At time: 282.5721242427826 and batch: 150, loss is 3.7697664737701415 and perplexity is 43.36993563562249
At time: 283.03486347198486 and batch: 200, loss is 3.848355669975281 and perplexity is 46.91585462236773
At time: 283.4812927246094 and batch: 250, loss is 3.8048405981063844 and perplexity is 44.91808951379596
At time: 283.95529437065125 and batch: 300, loss is 3.6387281036376953 and perplexity is 38.043418654745686
At time: 284.4110312461853 and batch: 350, loss is 3.6869530963897703 and perplexity is 39.923019860481716
At time: 284.8642191886902 and batch: 400, loss is 3.6755240678787233 and perplexity is 39.46933604929504
At time: 285.31671118736267 and batch: 450, loss is 3.7391212797164917 and perplexity is 42.06101405535202
At time: 285.7719376087189 and batch: 500, loss is 3.776093587875366 and perplexity is 43.64521210160045
At time: 286.20604133605957 and batch: 550, loss is 3.661784625053406 and perplexity is 38.930757718744324
At time: 286.6625258922577 and batch: 600, loss is 3.624805269241333 and perplexity is 37.517416653215214
At time: 287.1142222881317 and batch: 650, loss is 3.594643568992615 and perplexity is 36.402722616153966
At time: 287.5483043193817 and batch: 700, loss is 3.6846370601654055 and perplexity is 39.83066369169841
At time: 287.9981863498688 and batch: 750, loss is 3.65781108379364 and perplexity is 38.776371679457235
At time: 288.4648516178131 and batch: 800, loss is 3.764913306236267 and perplexity is 43.159963997942754
At time: 288.9299774169922 and batch: 850, loss is 3.638629603385925 and perplexity is 38.039671552978255
At time: 289.36732363700867 and batch: 900, loss is 3.5787825918197633 and perplexity is 35.82989467797373
At time: 289.809711933136 and batch: 950, loss is 3.60005099773407 and perplexity is 36.60010091829816
At time: 290.25242853164673 and batch: 1000, loss is 3.5295534420013426 and perplexity is 34.1087326859629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420679604134908 and perplexity of 83.15277713131577
Finished 29 epochs...
Completing Train Step...
At time: 291.66592931747437 and batch: 50, loss is 3.803909406661987 and perplexity is 44.87628164172946
At time: 292.1214497089386 and batch: 100, loss is 3.6805776262283327 and perplexity is 39.66930148497192
At time: 292.59989523887634 and batch: 150, loss is 3.7687567377090456 and perplexity is 43.32616554947986
At time: 293.0832381248474 and batch: 200, loss is 3.847277045249939 and perplexity is 46.86527730343848
At time: 293.5443181991577 and batch: 250, loss is 3.8038026523590087 and perplexity is 44.87149116126928
At time: 293.99021768569946 and batch: 300, loss is 3.637685708999634 and perplexity is 38.003783060680774
At time: 294.454806804657 and batch: 350, loss is 3.6858934688568117 and perplexity is 39.880738734517436
At time: 294.91468691825867 and batch: 400, loss is 3.6745455074310303 and perplexity is 39.43073180951213
At time: 295.353542804718 and batch: 450, loss is 3.738266849517822 and perplexity is 42.02509120372655
At time: 295.81467843055725 and batch: 500, loss is 3.7753413677215577 and perplexity is 43.61239363834261
At time: 296.24980425834656 and batch: 550, loss is 3.660989217758179 and perplexity is 38.89980422199664
At time: 296.6928961277008 and batch: 600, loss is 3.6240592288970945 and perplexity is 37.489437584834924
At time: 297.12141370773315 and batch: 650, loss is 3.594037051200867 and perplexity is 36.38065041148825
At time: 297.5638077259064 and batch: 700, loss is 3.6840775918960573 and perplexity is 39.808385931646846
At time: 298.03404808044434 and batch: 750, loss is 3.6573569774627686 and perplexity is 38.75876708107168
At time: 298.49232721328735 and batch: 800, loss is 3.7644885301589968 and perplexity is 43.141634570967234
At time: 298.9581677913666 and batch: 850, loss is 3.6382510662078857 and perplexity is 38.02527484807098
At time: 299.4190049171448 and batch: 900, loss is 3.5784741020202637 and perplexity is 35.818843225666235
At time: 299.88256549835205 and batch: 950, loss is 3.5997662353515625 and perplexity is 36.589680070163844
At time: 300.32946133613586 and batch: 1000, loss is 3.5292305994033812 and perplexity is 34.09772271142941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.420681092797256 and perplexity of 83.15290091781637
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 301.69874024391174 and batch: 50, loss is 3.803485646247864 and perplexity is 44.857268878749316
At time: 302.1615250110626 and batch: 100, loss is 3.6810464572906496 and perplexity is 39.687904046117
At time: 302.6012673377991 and batch: 150, loss is 3.7693476390838625 and perplexity is 43.351774605736715
At time: 303.0475332736969 and batch: 200, loss is 3.847955708503723 and perplexity is 46.89709384015871
At time: 303.5000319480896 and batch: 250, loss is 3.805092649459839 and perplexity is 44.929412605992916
At time: 303.9356825351715 and batch: 300, loss is 3.6378375720977782 and perplexity is 38.009554871168994
At time: 304.3904745578766 and batch: 350, loss is 3.6860002183914187 and perplexity is 39.88499621205496
At time: 304.8749511241913 and batch: 400, loss is 3.6730875158309937 and perplexity is 39.37328402312215
At time: 305.32218623161316 and batch: 450, loss is 3.7369344425201416 and perplexity is 41.969133965314995
At time: 305.79351711273193 and batch: 500, loss is 3.773678379058838 and perplexity is 43.539926994472296
At time: 306.24616837501526 and batch: 550, loss is 3.6590126371383667 and perplexity is 38.822991561073515
At time: 306.69349694252014 and batch: 600, loss is 3.621849818229675 and perplexity is 37.406699456428335
At time: 307.1476163864136 and batch: 650, loss is 3.5903905916213987 and perplexity is 36.24823141727056
At time: 307.5806460380554 and batch: 700, loss is 3.6809126329421997 and perplexity is 39.6825931935853
At time: 308.0194683074951 and batch: 750, loss is 3.6530388736724855 and perplexity is 38.59176353085967
At time: 308.45086336135864 and batch: 800, loss is 3.7598319101333617 and perplexity is 42.94120739027071
At time: 308.89916825294495 and batch: 850, loss is 3.633118677139282 and perplexity is 37.830614307228736
At time: 309.338853597641 and batch: 900, loss is 3.5733038330078126 and perplexity is 35.634128095410425
At time: 309.79633140563965 and batch: 950, loss is 3.5933551502227785 and perplexity is 36.35585086676703
At time: 310.243843793869 and batch: 1000, loss is 3.5220928478240965 and perplexity is 33.855208171192054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.419256070765053 and perplexity of 83.03449059066726
Finished 31 epochs...
Completing Train Step...
At time: 311.63218665122986 and batch: 50, loss is 3.8025716495513917 and perplexity is 44.816288214130324
At time: 312.08494210243225 and batch: 100, loss is 3.6798033905029297 and perplexity is 39.63859998119464
At time: 312.53607511520386 and batch: 150, loss is 3.7679738569259644 and perplexity is 43.29225960095609
At time: 312.9949584007263 and batch: 200, loss is 3.8467770528793337 and perplexity is 46.84185087934515
At time: 313.4511663913727 and batch: 250, loss is 3.8038451957702635 and perplexity is 44.87340018817934
At time: 313.9148840904236 and batch: 300, loss is 3.6368103933334353 and perplexity is 37.97053230856806
At time: 314.3726062774658 and batch: 350, loss is 3.684865517616272 and perplexity is 39.839764343107966
At time: 314.811874628067 and batch: 400, loss is 3.672204222679138 and perplexity is 39.33852122610891
At time: 315.26510787010193 and batch: 450, loss is 3.736136918067932 and perplexity is 41.935675898326274
At time: 315.74369740486145 and batch: 500, loss is 3.7729948806762694 and perplexity is 43.51017769275239
At time: 316.2158145904541 and batch: 550, loss is 3.6583972930908204 and perplexity is 38.799109412930896
At time: 316.6888909339905 and batch: 600, loss is 3.621350531578064 and perplexity is 37.38802745243797
At time: 317.10980582237244 and batch: 650, loss is 3.5901632595062254 and perplexity is 36.239991966732525
At time: 317.5457708835602 and batch: 700, loss is 3.6806219387054444 and perplexity is 39.67105936893381
At time: 318.00035858154297 and batch: 750, loss is 3.6531048822402954 and perplexity is 38.59431100197614
At time: 318.4681451320648 and batch: 800, loss is 3.7598825216293337 and perplexity is 42.943380764013966
At time: 318.917809009552 and batch: 850, loss is 3.633469181060791 and perplexity is 37.84387640997026
At time: 319.36471462249756 and batch: 900, loss is 3.5737878274917603 and perplexity is 35.65137899118133
At time: 319.81296253204346 and batch: 950, loss is 3.5937900924682618 and perplexity is 36.37166700148271
At time: 320.2554702758789 and batch: 1000, loss is 3.5228759241104126 and perplexity is 33.88172976473943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418748064738948 and perplexity of 82.99231928161915
Finished 32 epochs...
Completing Train Step...
At time: 321.6087324619293 and batch: 50, loss is 3.8020327281951904 and perplexity is 44.79214226627394
At time: 322.0690486431122 and batch: 100, loss is 3.679131932258606 and perplexity is 39.61199325009793
At time: 322.5234944820404 and batch: 150, loss is 3.767245593070984 and perplexity is 43.26074289072168
At time: 322.98018646240234 and batch: 200, loss is 3.8460809993743896 and perplexity is 46.80925778944567
At time: 323.4264500141144 and batch: 250, loss is 3.8031043577194215 and perplexity is 44.840168576990216
At time: 323.8691291809082 and batch: 300, loss is 3.63616473197937 and perplexity is 37.94602411611091
At time: 324.31656312942505 and batch: 350, loss is 3.6842093086242675 and perplexity is 39.813629707336
At time: 324.7706882953644 and batch: 400, loss is 3.6716704416275023 and perplexity is 39.317528672091264
At time: 325.2023329734802 and batch: 450, loss is 3.735657148361206 and perplexity is 41.915561256982805
At time: 325.6602592468262 and batch: 500, loss is 3.772569360733032 and perplexity is 43.49166718298516
At time: 326.12992882728577 and batch: 550, loss is 3.6580838775634765 and perplexity is 38.786951074999244
At time: 326.559157371521 and batch: 600, loss is 3.6210283422470093 and perplexity is 37.37598336922537
At time: 327.0137460231781 and batch: 650, loss is 3.5900240087509157 and perplexity is 36.23494587182316
At time: 327.47565054893494 and batch: 700, loss is 3.680500044822693 and perplexity is 39.66622400418115
At time: 327.9415159225464 and batch: 750, loss is 3.6531831312179563 and perplexity is 38.597331085513254
At time: 328.41302847862244 and batch: 800, loss is 3.759944787025452 and perplexity is 42.946054733874924
At time: 328.87435698509216 and batch: 850, loss is 3.633698401451111 and perplexity is 37.85255199236437
At time: 329.33499550819397 and batch: 900, loss is 3.574097023010254 and perplexity is 35.662403942138404
At time: 329.77149391174316 and batch: 950, loss is 3.5940541315078733 and perplexity is 36.38127180947319
At time: 330.21953082084656 and batch: 1000, loss is 3.5233311223983765 and perplexity is 33.897156180894335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418497969464558 and perplexity of 82.97156589002687
Finished 33 epochs...
Completing Train Step...
At time: 331.62149143218994 and batch: 50, loss is 3.8016053819656372 and perplexity is 44.77300460265828
At time: 332.1021296977997 and batch: 100, loss is 3.6786465549468996 and perplexity is 39.59277115266524
At time: 332.55850863456726 and batch: 150, loss is 3.7667173767089843 and perplexity is 43.237897892576626
At time: 333.0085551738739 and batch: 200, loss is 3.845571508407593 and perplexity is 46.78541496980587
At time: 333.45900440216064 and batch: 250, loss is 3.8025661849975587 and perplexity is 44.816043313779915
At time: 333.9209506511688 and batch: 300, loss is 3.6356791400909425 and perplexity is 37.927602307704404
At time: 334.34764790534973 and batch: 350, loss is 3.6837330436706544 and perplexity is 39.7946723855426
At time: 334.78421330451965 and batch: 400, loss is 3.6712694931030274 and perplexity is 39.30176752689929
At time: 335.21761655807495 and batch: 450, loss is 3.735305585861206 and perplexity is 41.90082790747674
At time: 335.6724977493286 and batch: 500, loss is 3.7722445154190063 and perplexity is 43.47754141317088
At time: 336.130539894104 and batch: 550, loss is 3.6578530979156496 and perplexity is 38.778000868892306
At time: 336.5659170150757 and batch: 600, loss is 3.620776162147522 and perplexity is 37.36655907838012
At time: 337.0217094421387 and batch: 650, loss is 3.5899034929275513 and perplexity is 36.230579250615605
At time: 337.45966720581055 and batch: 700, loss is 3.6804084825515746 and perplexity is 39.662592240893424
At time: 337.9242043495178 and batch: 750, loss is 3.6532279920578 and perplexity is 38.599062633040525
At time: 338.3842327594757 and batch: 800, loss is 3.7599770164489748 and perplexity is 42.94743888276662
At time: 338.8475818634033 and batch: 850, loss is 3.633840641975403 and perplexity is 37.85793654214706
At time: 339.3343737125397 and batch: 900, loss is 3.5742973041534425 and perplexity is 35.66954716447128
At time: 339.79986929893494 and batch: 950, loss is 3.5942205429077148 and perplexity is 36.38732657161977
At time: 340.265647649765 and batch: 1000, loss is 3.5236137342453 and perplexity is 33.90673727260932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4183528248856705 and perplexity of 82.95952389097272
Finished 34 epochs...
Completing Train Step...
At time: 341.6231520175934 and batch: 50, loss is 3.801230115890503 and perplexity is 44.756205965125595
At time: 342.07780027389526 and batch: 100, loss is 3.678248257637024 and perplexity is 39.57700459852111
At time: 342.536173582077 and batch: 150, loss is 3.766291117668152 and perplexity is 43.21947127522845
At time: 342.9832103252411 and batch: 200, loss is 3.8451615381240845 and perplexity is 46.76623827117351
At time: 343.43153142929077 and batch: 250, loss is 3.802134509086609 and perplexity is 44.79670148245896
At time: 343.88568568229675 and batch: 300, loss is 3.6352823305130006 and perplexity is 37.91255525744461
At time: 344.3402810096741 and batch: 350, loss is 3.6833468675613403 and perplexity is 39.77930760073668
At time: 344.78130292892456 and batch: 400, loss is 3.6709373807907104 and perplexity is 39.288717093232584
At time: 345.2252595424652 and batch: 450, loss is 3.7350197315216063 and perplexity is 41.888852085738414
At time: 345.66786789894104 and batch: 500, loss is 3.7719732666015626 and perplexity is 43.46574978078267
At time: 346.09964537620544 and batch: 550, loss is 3.6576541233062745 and perplexity is 38.770285798894044
At time: 346.53650546073914 and batch: 600, loss is 3.620560784339905 and perplexity is 37.3585120174179
At time: 346.9742069244385 and batch: 650, loss is 3.5897880220413207 and perplexity is 36.22639591505227
At time: 347.42325019836426 and batch: 700, loss is 3.680319266319275 and perplexity is 39.65905385169368
At time: 347.85568022727966 and batch: 750, loss is 3.653242392539978 and perplexity is 38.599618482156295
At time: 348.3215494155884 and batch: 800, loss is 3.7599806118011476 and perplexity is 42.947593294211906
At time: 348.7591824531555 and batch: 850, loss is 3.63392457485199 and perplexity is 37.86111420101586
At time: 349.21524238586426 and batch: 900, loss is 3.5744283962249757 and perplexity is 35.67422346580604
At time: 349.684574842453 and batch: 950, loss is 3.5943278169631956 and perplexity is 36.39123019708433
At time: 350.1412162780762 and batch: 1000, loss is 3.5237941455841066 and perplexity is 33.91285498431092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4182653659727515 and perplexity of 82.95226865846894
Finished 35 epochs...
Completing Train Step...
At time: 351.5318250656128 and batch: 50, loss is 3.8008844327926634 and perplexity is 44.740737175003716
At time: 352.0101444721222 and batch: 100, loss is 3.677900252342224 and perplexity is 39.56323398763023
At time: 352.472017288208 and batch: 150, loss is 3.7659258365631105 and perplexity is 43.20368690204388
At time: 352.9318046569824 and batch: 200, loss is 3.844811339378357 and perplexity is 46.74986366053996
At time: 353.3832674026489 and batch: 250, loss is 3.801766152381897 and perplexity is 44.78020335590323
At time: 353.8435730934143 and batch: 300, loss is 3.6349402713775634 and perplexity is 37.89958913928704
At time: 354.29650473594666 and batch: 350, loss is 3.6830128622055054 and perplexity is 39.76602331758099
At time: 354.7356586456299 and batch: 400, loss is 3.670645866394043 and perplexity is 39.27726553580135
At time: 355.1770260334015 and batch: 450, loss is 3.734771180152893 and perplexity is 41.87844184801168
At time: 355.64091968536377 and batch: 500, loss is 3.771734189987183 and perplexity is 43.45535937858418
At time: 356.08017659187317 and batch: 550, loss is 3.6574706268310546 and perplexity is 38.76317224078306
At time: 356.5183720588684 and batch: 600, loss is 3.6203672218322756 and perplexity is 37.35128150995067
At time: 356.9741349220276 and batch: 650, loss is 3.5896737241744994 and perplexity is 36.22225555189841
At time: 357.40784907341003 and batch: 700, loss is 3.6802264404296876 and perplexity is 39.655372635598425
At time: 357.84762954711914 and batch: 750, loss is 3.653234391212463 and perplexity is 38.59930963520247
At time: 358.28856658935547 and batch: 800, loss is 3.7599618864059448 and perplexity is 42.94678909108399
At time: 358.73577547073364 and batch: 850, loss is 3.6339682865142824 and perplexity is 37.862769209425146
At time: 359.20530796051025 and batch: 900, loss is 3.574513759613037 and perplexity is 35.67726886836863
At time: 359.65961360931396 and batch: 950, loss is 3.594395914077759 and perplexity is 36.3937084192351
At time: 360.107453584671 and batch: 1000, loss is 3.5239085245132444 and perplexity is 33.91673412218962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418208796803544 and perplexity of 82.94757625027111
Finished 36 epochs...
Completing Train Step...
At time: 361.4340362548828 and batch: 50, loss is 3.800558190345764 and perplexity is 44.72614322814358
At time: 361.8994472026825 and batch: 100, loss is 3.677584900856018 and perplexity is 39.5507596300001
At time: 362.37239170074463 and batch: 150, loss is 3.765600380897522 and perplexity is 43.18962830521638
At time: 362.8460156917572 and batch: 200, loss is 3.8444999647140503 and perplexity is 46.73530920349845
At time: 363.3038709163666 and batch: 250, loss is 3.801438670158386 and perplexity is 44.76554103629452
At time: 363.7810881137848 and batch: 300, loss is 3.634634099006653 and perplexity is 37.88798710842501
At time: 364.24267745018005 and batch: 350, loss is 3.6827123403549193 and perplexity is 39.75407455418531
At time: 364.6865584850311 and batch: 400, loss is 3.6703803539276123 and perplexity is 39.266838316493896
At time: 365.14471554756165 and batch: 450, loss is 3.734546070098877 and perplexity is 41.869015650710864
At time: 365.5862309932709 and batch: 500, loss is 3.771516327857971 and perplexity is 43.44589313266999
At time: 366.0335485935211 and batch: 550, loss is 3.6572957706451414 and perplexity is 38.75639485288251
At time: 366.49997305870056 and batch: 600, loss is 3.620187382698059 and perplexity is 37.34456489179554
At time: 366.9461672306061 and batch: 650, loss is 3.5895591592788696 and perplexity is 36.21810599067319
At time: 367.383184671402 and batch: 700, loss is 3.6801291799545286 and perplexity is 39.65151592276918
At time: 367.8435456752777 and batch: 750, loss is 3.6532099056243896 and perplexity is 38.598364519977714
At time: 368.29501008987427 and batch: 800, loss is 3.7599263620376586 and perplexity is 42.9452634606303
At time: 368.731529712677 and batch: 850, loss is 3.6339837980270384 and perplexity is 37.86335652280776
At time: 369.187796831131 and batch: 900, loss is 3.574567437171936 and perplexity is 35.679183988468694
At time: 369.6310179233551 and batch: 950, loss is 3.594436240196228 and perplexity is 36.395176065824394
At time: 370.08948969841003 and batch: 1000, loss is 3.5239783763885497 and perplexity is 33.91910335241891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418170463748094 and perplexity of 82.94439667717303
Finished 37 epochs...
Completing Train Step...
At time: 371.48527574539185 and batch: 50, loss is 3.8002456188201905 and perplexity is 44.71216529398764
At time: 371.9335036277771 and batch: 100, loss is 3.6772924947738646 and perplexity is 39.53919643798672
At time: 372.3832063674927 and batch: 150, loss is 3.7653021669387816 and perplexity is 43.176750475452685
At time: 372.84383273124695 and batch: 200, loss is 3.84421471118927 and perplexity is 46.72197969305177
At time: 373.28098130226135 and batch: 250, loss is 3.8011389541625977 and perplexity is 44.75212609802203
At time: 373.7255005836487 and batch: 300, loss is 3.634352684020996 and perplexity is 37.8773263611937
At time: 374.1893537044525 and batch: 350, loss is 3.68243465423584 and perplexity is 39.74303693207291
At time: 374.6364629268646 and batch: 400, loss is 3.670132384300232 and perplexity is 39.25710254036642
At time: 375.0801954269409 and batch: 450, loss is 3.734336142539978 and perplexity is 41.860227112972225
At time: 375.5286707878113 and batch: 500, loss is 3.771312942504883 and perplexity is 43.43705777287682
At time: 375.9706618785858 and batch: 550, loss is 3.657126636505127 and perplexity is 38.7498403776774
At time: 376.4286673069 and batch: 600, loss is 3.6200167083740236 and perplexity is 37.338191677313745
At time: 376.87473368644714 and batch: 650, loss is 3.5894439315795896 and perplexity is 36.21393290207992
At time: 377.3357584476471 and batch: 700, loss is 3.680028042793274 and perplexity is 39.6475058837947
At time: 377.7737467288971 and batch: 750, loss is 3.6531737422943116 and perplexity is 38.59696869982001
At time: 378.2528131008148 and batch: 800, loss is 3.759878463745117 and perplexity is 42.94320650510051
At time: 378.7051839828491 and batch: 850, loss is 3.633978943824768 and perplexity is 37.86317272686266
At time: 379.15755319595337 and batch: 900, loss is 3.5745985746383666 and perplexity is 35.680294965158815
At time: 379.6176676750183 and batch: 950, loss is 3.5944562339782715 and perplexity is 36.39590375031664
At time: 380.0599467754364 and batch: 1000, loss is 3.52401659488678 and perplexity is 33.9203997143827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418144784322599 and perplexity of 82.9422667400662
Finished 38 epochs...
Completing Train Step...
At time: 381.47388434410095 and batch: 50, loss is 3.7999438142776487 and perplexity is 44.69867299551593
At time: 381.9328637123108 and batch: 100, loss is 3.677016701698303 and perplexity is 39.52829330496926
At time: 382.38457131385803 and batch: 150, loss is 3.765023641586304 and perplexity is 43.16472633040002
At time: 382.84204864501953 and batch: 200, loss is 3.8439481592178346 and perplexity is 46.70952751690499
At time: 383.29913425445557 and batch: 250, loss is 3.8008590221405028 and perplexity is 44.739600298138406
At time: 383.7613070011139 and batch: 300, loss is 3.6340890598297118 and perplexity is 37.867342297741864
At time: 384.20453095436096 and batch: 350, loss is 3.6821730852127077 and perplexity is 39.73264274418433
At time: 384.6614422798157 and batch: 400, loss is 3.6698966932296755 and perplexity is 39.24785108212762
At time: 385.10839915275574 and batch: 450, loss is 3.734136881828308 and perplexity is 41.85188684529846
At time: 385.5511999130249 and batch: 500, loss is 3.771119713783264 and perplexity is 43.42866529659232
At time: 386.0355064868927 and batch: 550, loss is 3.6569611406326294 and perplexity is 38.74342796966311
At time: 386.4879322052002 and batch: 600, loss is 3.6198524761199953 and perplexity is 37.33206004545288
At time: 386.9287405014038 and batch: 650, loss is 3.589328212738037 and perplexity is 36.209742510174756
At time: 387.39086270332336 and batch: 700, loss is 3.679923791885376 and perplexity is 39.64337281075248
At time: 387.84119725227356 and batch: 750, loss is 3.6531289863586425 and perplexity is 38.595241295027954
At time: 388.2949221134186 and batch: 800, loss is 3.7598212909698487 and perplexity is 42.94075139298915
At time: 388.7552773952484 and batch: 850, loss is 3.6339589595794677 and perplexity is 37.862416067491694
At time: 389.217737197876 and batch: 900, loss is 3.57461275100708 and perplexity is 35.68080078576139
At time: 389.66106843948364 and batch: 950, loss is 3.594460573196411 and perplexity is 36.39606168042504
At time: 390.1355788707733 and batch: 1000, loss is 3.524031639099121 and perplexity is 33.920910023917294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418125059546494 and perplexity of 82.94063073856006
Finished 39 epochs...
Completing Train Step...
At time: 391.531462430954 and batch: 50, loss is 3.799650301933289 and perplexity is 44.685555308410954
At time: 392.02220726013184 and batch: 100, loss is 3.676753730773926 and perplexity is 39.51789987978401
At time: 392.4631230831146 and batch: 150, loss is 3.7647597360610963 and perplexity is 43.153336423623124
At time: 392.91948795318604 and batch: 200, loss is 3.8436953258514404 and perplexity is 46.697719282640925
At time: 393.3640630245209 and batch: 250, loss is 3.8005936574935912 and perplexity is 44.72772956500844
At time: 393.8267729282379 and batch: 300, loss is 3.6338387537002563 and perplexity is 37.857865056013885
At time: 394.27768993377686 and batch: 350, loss is 3.6819236278533936 and perplexity is 39.7227323802048
At time: 394.74099373817444 and batch: 400, loss is 3.6696700429916382 and perplexity is 39.238956555348786
At time: 395.2063293457031 and batch: 450, loss is 3.7339452266693116 and perplexity is 41.843866483867195
At time: 395.6747522354126 and batch: 500, loss is 3.770934405326843 and perplexity is 43.42061834326907
At time: 396.127112865448 and batch: 550, loss is 3.6567983627319336 and perplexity is 38.737121909050025
At time: 396.5975935459137 and batch: 600, loss is 3.6196928644180297 and perplexity is 37.32610188731967
At time: 397.0393531322479 and batch: 650, loss is 3.5892118120193484 and perplexity is 36.20552791541873
At time: 397.4950659275055 and batch: 700, loss is 3.679816999435425 and perplexity is 39.639139423896594
At time: 397.94762110710144 and batch: 750, loss is 3.653077540397644 and perplexity is 38.593255776823455
At time: 398.40146589279175 and batch: 800, loss is 3.759757170677185 and perplexity is 42.9379981077143
At time: 398.8478591442108 and batch: 850, loss is 3.6339277124404905 and perplexity is 37.86123299379875
At time: 399.296870470047 and batch: 900, loss is 3.5746140909194946 and perplexity is 35.68084859494135
At time: 399.7353446483612 and batch: 950, loss is 3.5944527530670167 and perplexity is 36.39577705962615
At time: 400.1864700317383 and batch: 1000, loss is 3.5240293836593626 and perplexity is 33.92083351743445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4181127780821265 and perplexity of 82.93961211241417
Finished 40 epochs...
Completing Train Step...
At time: 401.57321214675903 and batch: 50, loss is 3.799363989830017 and perplexity is 44.672763124450675
At time: 402.03339672088623 and batch: 100, loss is 3.6765005826950072 and perplexity is 39.50789726547126
At time: 402.4929463863373 and batch: 150, loss is 3.7645068407058715 and perplexity is 43.14242452512152
At time: 402.94634675979614 and batch: 200, loss is 3.8434527826309206 and perplexity is 46.6863944408525
At time: 403.4018907546997 and batch: 250, loss is 3.800339379310608 and perplexity is 44.71635772507235
At time: 403.83429861068726 and batch: 300, loss is 3.6335983085632324 and perplexity is 37.84876341073008
At time: 404.29928374290466 and batch: 350, loss is 3.681683268547058 and perplexity is 39.7131857991549
At time: 404.75868344306946 and batch: 400, loss is 3.6694501638412476 and perplexity is 39.23032967538945
At time: 405.19862031936646 and batch: 450, loss is 3.733759355545044 and perplexity is 41.83608964012788
At time: 405.652871131897 and batch: 500, loss is 3.770754852294922 and perplexity is 43.412822739480724
At time: 406.09034037590027 and batch: 550, loss is 3.6566375494003296 and perplexity is 38.73089296428119
At time: 406.555114030838 and batch: 600, loss is 3.6195367765426636 and perplexity is 37.32027619005257
At time: 407.0168013572693 and batch: 650, loss is 3.5890947675704954 and perplexity is 36.20129050734582
At time: 407.48721289634705 and batch: 700, loss is 3.679708285331726 and perplexity is 39.63483032461691
At time: 407.9257321357727 and batch: 750, loss is 3.653020987510681 and perplexity is 38.59107327850584
At time: 408.3673310279846 and batch: 800, loss is 3.7596877717971804 and perplexity is 42.93501836213269
At time: 408.8138122558594 and batch: 850, loss is 3.6338878297805786 and perplexity is 37.85972301723056
At time: 409.26958203315735 and batch: 900, loss is 3.574605417251587 and perplexity is 35.68053911245216
At time: 409.7225172519684 and batch: 950, loss is 3.5944352102279664 and perplexity is 36.395138579967465
At time: 410.16742181777954 and batch: 1000, loss is 3.5240139150619507 and perplexity is 33.92030881377512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418103101776867 and perplexity of 82.93880956729213
Finished 41 epochs...
Completing Train Step...
At time: 411.56125926971436 and batch: 50, loss is 3.799083490371704 and perplexity is 44.66023419585392
At time: 412.02088689804077 and batch: 100, loss is 3.6762558746337892 and perplexity is 39.498230547338785
At time: 412.48745799064636 and batch: 150, loss is 3.7642629098892213 and perplexity is 43.131902041706
At time: 412.95005345344543 and batch: 200, loss is 3.8432183504104613 and perplexity is 46.67545092854456
At time: 413.3991596698761 and batch: 250, loss is 3.800093998908997 and perplexity is 44.70538655336549
At time: 413.8589618206024 and batch: 300, loss is 3.6333659505844116 and perplexity is 37.83996997021569
At time: 414.31471729278564 and batch: 350, loss is 3.681450071334839 and perplexity is 39.70392587467445
At time: 414.788094997406 and batch: 400, loss is 3.669235544204712 and perplexity is 39.22191097973436
At time: 415.2385604381561 and batch: 450, loss is 3.7335777187347414 and perplexity is 41.828491356335014
At time: 415.68948435783386 and batch: 500, loss is 3.770579843521118 and perplexity is 43.405225779392445
At time: 416.11431765556335 and batch: 550, loss is 3.656478290557861 and perplexity is 38.724725218247016
At time: 416.570919752121 and batch: 600, loss is 3.6193832111358644 and perplexity is 37.31454552668467
At time: 417.0214304924011 and batch: 650, loss is 3.5889772844314574 and perplexity is 36.197037715920324
At time: 417.44826078414917 and batch: 700, loss is 3.679598035812378 and perplexity is 39.63046084449514
At time: 417.9115045070648 and batch: 750, loss is 3.652960515022278 and perplexity is 38.588739650835386
At time: 418.3683247566223 and batch: 800, loss is 3.7596141719818115 and perplexity is 42.931858468993575
At time: 418.8154788017273 and batch: 850, loss is 3.6338409614562988 and perplexity is 37.85794863703648
At time: 419.2796688079834 and batch: 900, loss is 3.574588780403137 and perplexity is 35.679945505668215
At time: 419.73682713508606 and batch: 950, loss is 3.5944098329544065 and perplexity is 36.39421498229872
At time: 420.1737844944 and batch: 1000, loss is 3.5239880752563475 and perplexity is 33.9194323309135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4180967749618905 and perplexity of 82.93828483044956
Finished 42 epochs...
Completing Train Step...
At time: 421.53443479537964 and batch: 50, loss is 3.79880820274353 and perplexity is 44.64794147800264
At time: 421.99451994895935 and batch: 100, loss is 3.676017861366272 and perplexity is 39.48883056312992
At time: 422.4546604156494 and batch: 150, loss is 3.764025855064392 and perplexity is 43.12167862802559
At time: 422.9140956401825 and batch: 200, loss is 3.8429903411865234 and perplexity is 46.664809708395666
At time: 423.3754460811615 and batch: 250, loss is 3.799855489730835 and perplexity is 44.69472517982751
At time: 423.83815836906433 and batch: 300, loss is 3.633139715194702 and perplexity is 37.83141019816107
At time: 424.298043012619 and batch: 350, loss is 3.6812226963043213 and perplexity is 39.694899219573905
At time: 424.7499485015869 and batch: 400, loss is 3.669025139808655 and perplexity is 39.213659385358774
At time: 425.2082631587982 and batch: 450, loss is 3.7333995723724365 and perplexity is 41.82104042645691
At time: 425.66068744659424 and batch: 500, loss is 3.770408525466919 and perplexity is 43.397790317502476
At time: 426.1235852241516 and batch: 550, loss is 3.6563201665878298 and perplexity is 38.71860239505244
At time: 426.5896806716919 and batch: 600, loss is 3.619231843948364 and perplexity is 37.308897756329785
At time: 427.0523729324341 and batch: 650, loss is 3.58885941028595 and perplexity is 36.19277127248628
At time: 427.50667667388916 and batch: 700, loss is 3.679486346244812 and perplexity is 39.62603478263806
At time: 427.9617552757263 and batch: 750, loss is 3.6528966569900514 and perplexity is 38.58627552853302
At time: 428.40499329566956 and batch: 800, loss is 3.759537034034729 and perplexity is 42.928546921291485
At time: 428.8623447418213 and batch: 850, loss is 3.633788642883301 and perplexity is 37.85596801499927
At time: 429.3039720058441 and batch: 900, loss is 3.5745653915405273 and perplexity is 35.67911100208394
At time: 429.76041865348816 and batch: 950, loss is 3.594377751350403 and perplexity is 36.39304741623443
At time: 430.22081422805786 and batch: 1000, loss is 3.5239539432525633 and perplexity is 33.918274612478555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418091564643674 and perplexity of 82.937852696719
Finished 43 epochs...
Completing Train Step...
At time: 431.64504075050354 and batch: 50, loss is 3.798537530899048 and perplexity is 44.6358581727095
At time: 432.0810148715973 and batch: 100, loss is 3.675785460472107 and perplexity is 39.47965438991425
At time: 432.5381712913513 and batch: 150, loss is 3.7637947273254393 and perplexity is 43.111713163636445
At time: 432.97640109062195 and batch: 200, loss is 3.8427676868438723 and perplexity is 46.65442074248221
At time: 433.4294309616089 and batch: 250, loss is 3.7996227169036865 and perplexity is 44.684322673046196
At time: 433.8844075202942 and batch: 300, loss is 3.632918677330017 and perplexity is 37.82304894814332
At time: 434.3437759876251 and batch: 350, loss is 3.6809999704360963 and perplexity is 39.68605912317674
At time: 434.8067626953125 and batch: 400, loss is 3.668818225860596 and perplexity is 39.2055463716543
At time: 435.2641456127167 and batch: 450, loss is 3.7332240915298462 and perplexity is 41.81370227891586
At time: 435.70550441741943 and batch: 500, loss is 3.7702400207519533 and perplexity is 43.39047820129521
At time: 436.1361837387085 and batch: 550, loss is 3.6561630725860597 and perplexity is 38.71252041259324
At time: 436.5793676376343 and batch: 600, loss is 3.619082088470459 and perplexity is 37.30331096285308
At time: 437.0364775657654 and batch: 650, loss is 3.588741211891174 and perplexity is 36.18849359783151
At time: 437.51446437835693 and batch: 700, loss is 3.6793736743927004 and perplexity is 39.621570295423
At time: 437.9785940647125 and batch: 750, loss is 3.6528300762176515 and perplexity is 38.58370651002887
At time: 438.44485878944397 and batch: 800, loss is 3.7594571352005004 and perplexity is 42.92511711745783
At time: 438.8905997276306 and batch: 850, loss is 3.6337318563461305 and perplexity is 37.85381836670059
At time: 439.3524055480957 and batch: 900, loss is 3.574536681175232 and perplexity is 35.67808665647839
At time: 439.8126108646393 and batch: 950, loss is 3.594340386390686 and perplexity is 36.391687616888326
At time: 440.27647137641907 and batch: 1000, loss is 3.523913035392761 and perplexity is 33.91688711683591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418090448146913 and perplexity of 82.93776009692681
Finished 44 epochs...
Completing Train Step...
At time: 441.69143295288086 and batch: 50, loss is 3.7982709312438967 and perplexity is 44.623959854427525
At time: 442.1623475551605 and batch: 100, loss is 3.6755579614639284 and perplexity is 39.47067382927037
At time: 442.61867904663086 and batch: 150, loss is 3.7635684967041017 and perplexity is 43.10196107713239
At time: 443.0510742664337 and batch: 200, loss is 3.8425494718551634 and perplexity is 46.64424115929588
At time: 443.4795639514923 and batch: 250, loss is 3.799394817352295 and perplexity is 44.6741402962784
At time: 443.92949533462524 and batch: 300, loss is 3.632702031135559 and perplexity is 37.81485561608506
At time: 444.38006830215454 and batch: 350, loss is 3.6807812356948855 and perplexity is 39.67737935262304
At time: 444.8280670642853 and batch: 400, loss is 3.6686141872406006 and perplexity is 39.197547742118886
At time: 445.26002502441406 and batch: 450, loss is 3.7330509519577024 and perplexity is 41.80646329908856
At time: 445.70675134658813 and batch: 500, loss is 3.77007408618927 and perplexity is 43.38327881859999
At time: 446.1502709388733 and batch: 550, loss is 3.65600679397583 and perplexity is 38.70647094641807
At time: 446.5804224014282 and batch: 600, loss is 3.618933620452881 and perplexity is 37.29777302533879
At time: 447.0179326534271 and batch: 650, loss is 3.588622741699219 and perplexity is 36.1842065939945
At time: 447.4596333503723 and batch: 700, loss is 3.6792600393295287 and perplexity is 39.61706815158506
At time: 447.91615653038025 and batch: 750, loss is 3.652761154174805 and perplexity is 38.58104733379458
At time: 448.3676805496216 and batch: 800, loss is 3.7593750524520875 and perplexity is 42.92159385047058
At time: 448.8270916938782 and batch: 850, loss is 3.633671383857727 and perplexity is 37.851529321321216
At time: 449.2891778945923 and batch: 900, loss is 3.574503307342529 and perplexity is 35.6768959618523
At time: 449.72897577285767 and batch: 950, loss is 3.594298520088196 and perplexity is 36.39016406337943
At time: 450.1863760948181 and batch: 1000, loss is 3.5238666486740113 and perplexity is 33.915313860221744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418088587318978 and perplexity of 82.9376057641696
Finished 45 epochs...
Completing Train Step...
At time: 451.57081484794617 and batch: 50, loss is 3.798007960319519 and perplexity is 44.6122265932761
At time: 452.0300359725952 and batch: 100, loss is 3.6753346014022825 and perplexity is 39.46185864164773
At time: 452.4614017009735 and batch: 150, loss is 3.7633466482162476 and perplexity is 43.09240003283471
At time: 452.8984491825104 and batch: 200, loss is 3.842334957122803 and perplexity is 46.6342363555147
At time: 453.34437799453735 and batch: 250, loss is 3.799170870780945 and perplexity is 44.66413679589765
At time: 453.7926664352417 and batch: 300, loss is 3.632488842010498 and perplexity is 37.80679475937598
At time: 454.2419352531433 and batch: 350, loss is 3.680565605163574 and perplexity is 39.66882462059605
At time: 454.7030839920044 and batch: 400, loss is 3.668412389755249 and perplexity is 39.189638573604576
At time: 455.1470296382904 and batch: 450, loss is 3.732879672050476 and perplexity is 41.79930330513229
At time: 455.62442922592163 and batch: 500, loss is 3.769910078048706 and perplexity is 43.37616419115374
At time: 456.0917491912842 and batch: 550, loss is 3.655851373672485 and perplexity is 38.700455642424465
At time: 456.5443720817566 and batch: 600, loss is 3.6187864685058595 and perplexity is 37.29228498921606
At time: 456.9913294315338 and batch: 650, loss is 3.5885040664672854 and perplexity is 36.17991267968031
At time: 457.44426798820496 and batch: 700, loss is 3.679145755767822 and perplexity is 39.61254083063644
At time: 457.90517115592957 and batch: 750, loss is 3.6526902532577514 and perplexity is 38.57831199912775
At time: 458.33516216278076 and batch: 800, loss is 3.7592909288406373 and perplexity is 42.91798328285581
At time: 458.77928924560547 and batch: 850, loss is 3.6336078310012816 and perplexity is 37.84912382495091
At time: 459.2364649772644 and batch: 900, loss is 3.5744658517837524 and perplexity is 35.67555968880421
At time: 459.69209575653076 and batch: 950, loss is 3.5942526340484617 and perplexity is 36.38849430117497
At time: 460.14622950553894 and batch: 1000, loss is 3.5238158082962037 and perplexity is 33.91358963668208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418088215153392 and perplexity of 82.93757489765267
Finished 46 epochs...
Completing Train Step...
At time: 461.4911186695099 and batch: 50, loss is 3.7977482986450197 and perplexity is 44.60064401165709
At time: 461.94674801826477 and batch: 100, loss is 3.6751149797439577 and perplexity is 39.45319291443779
At time: 462.394654750824 and batch: 150, loss is 3.7631283473968504 and perplexity is 43.08299395331254
At time: 462.85187911987305 and batch: 200, loss is 3.8421236848831177 and perplexity is 46.62438487666251
At time: 463.2981858253479 and batch: 250, loss is 3.798950572013855 and perplexity is 44.65429842535862
At time: 463.74350905418396 and batch: 300, loss is 3.6322789144515992 and perplexity is 37.798858904248846
At time: 464.19349098205566 and batch: 350, loss is 3.680352907180786 and perplexity is 39.66038803887344
At time: 464.63071846961975 and batch: 400, loss is 3.668212876319885 and perplexity is 39.18182049411397
At time: 465.0827088356018 and batch: 450, loss is 3.732710056304932 and perplexity is 41.79221408637751
At time: 465.5349450111389 and batch: 500, loss is 3.769747743606567 and perplexity is 43.369123317241275
At time: 465.97415804862976 and batch: 550, loss is 3.65569655418396 and perplexity is 38.694464521459324
At time: 466.4124767780304 and batch: 600, loss is 3.618640284538269 and perplexity is 37.286833853479855
At time: 466.88593077659607 and batch: 650, loss is 3.5883852577209474 and perplexity is 36.17561444495121
At time: 467.33633041381836 and batch: 700, loss is 3.6790309000015258 and perplexity is 39.60799136317567
At time: 467.79644560813904 and batch: 750, loss is 3.6526176261901857 and perplexity is 38.57551027119749
At time: 468.2376244068146 and batch: 800, loss is 3.7592052841186523 and perplexity is 42.91430774150699
At time: 468.6668984889984 and batch: 850, loss is 3.6335415697097777 and perplexity is 37.84661597621154
At time: 469.09271264076233 and batch: 900, loss is 3.574424843788147 and perplexity is 35.674096735605865
At time: 469.5552935600281 and batch: 950, loss is 3.5942035579681395 and perplexity is 36.38670854032527
At time: 470.0002155303955 and batch: 1000, loss is 3.5237609195709227 and perplexity is 33.91172821406325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418087842987805 and perplexity of 82.93754403114714
Finished 47 epochs...
Completing Train Step...
At time: 471.35091495513916 and batch: 50, loss is 3.797491683959961 and perplexity is 44.58920029981568
At time: 471.80438446998596 and batch: 100, loss is 3.6748986196517945 and perplexity is 39.444657741351406
At time: 472.2611584663391 and batch: 150, loss is 3.7629133367538454 and perplexity is 43.07373164686297
At time: 472.70980954170227 and batch: 200, loss is 3.841915307044983 and perplexity is 46.614670400313486
At time: 473.155442237854 and batch: 250, loss is 3.798733229637146 and perplexity is 44.644594208615544
At time: 473.6007390022278 and batch: 300, loss is 3.6320717191696166 and perplexity is 37.791027970313856
At time: 474.055846452713 and batch: 350, loss is 3.680142765045166 and perplexity is 39.65205459586585
At time: 474.50192880630493 and batch: 400, loss is 3.66801513671875 and perplexity is 39.17407346253043
At time: 474.95900225639343 and batch: 450, loss is 3.7325417947769166 and perplexity is 41.78518265615239
At time: 475.4173221588135 and batch: 500, loss is 3.7695868682861327 and perplexity is 43.36214685681585
At time: 475.864942073822 and batch: 550, loss is 3.655542311668396 and perplexity is 38.688496650174706
At time: 476.3143401145935 and batch: 600, loss is 3.6184948015213014 and perplexity is 37.28140964697221
At time: 476.7462811470032 and batch: 650, loss is 3.588266372680664 and perplexity is 36.171313961207304
At time: 477.19734477996826 and batch: 700, loss is 3.6789153957366945 and perplexity is 39.60341673545142
At time: 477.64033794403076 and batch: 750, loss is 3.652543420791626 and perplexity is 38.57264786628743
At time: 478.0987157821655 and batch: 800, loss is 3.7591180562973023 and perplexity is 42.9105645831941
At time: 478.5719335079193 and batch: 850, loss is 3.6334729194641113 and perplexity is 37.844017885907924
At time: 479.02310705184937 and batch: 900, loss is 3.5743809843063357 and perplexity is 35.672532122520764
At time: 479.4670958518982 and batch: 950, loss is 3.594151487350464 and perplexity is 36.38481391126408
At time: 479.927321434021 and batch: 1000, loss is 3.52370285987854 and perplexity is 33.909759366710865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418088959484566 and perplexity of 82.9376366306981
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 481.28438448905945 and batch: 50, loss is 3.797423686981201 and perplexity is 44.5861684719887
At time: 481.749614238739 and batch: 100, loss is 3.6750979900360106 and perplexity is 39.452522621906645
At time: 482.21938967704773 and batch: 150, loss is 3.762896146774292 and perplexity is 43.07299121666067
At time: 482.67265915870667 and batch: 200, loss is 3.8419592332839967 and perplexity is 46.61671805243954
At time: 483.13009572029114 and batch: 250, loss is 3.7989122247695923 and perplexity is 44.65258608890142
At time: 483.596054315567 and batch: 300, loss is 3.631943507194519 and perplexity is 37.786183018573865
At time: 484.0582904815674 and batch: 350, loss is 3.6799964332580566 and perplexity is 39.646252664368134
At time: 484.51608300209045 and batch: 400, loss is 3.667567687034607 and perplexity is 39.1565489566927
At time: 484.98281812667847 and batch: 450, loss is 3.7320999574661253 and perplexity is 41.76672448147209
At time: 485.4479937553406 and batch: 500, loss is 3.7689805459976196 and perplexity is 43.33586338962998
At time: 485.9165949821472 and batch: 550, loss is 3.6548419904708864 and perplexity is 38.661411761038785
At time: 486.3786609172821 and batch: 600, loss is 3.617905812263489 and perplexity is 37.259457762520704
At time: 486.8342227935791 and batch: 650, loss is 3.587452425956726 and perplexity is 36.14188441737323
At time: 487.2806918621063 and batch: 700, loss is 3.6781281423568726 and perplexity is 39.57225108101695
At time: 487.71253180503845 and batch: 750, loss is 3.6515335559844972 and perplexity is 38.53371436878276
At time: 488.16854643821716 and batch: 800, loss is 3.7578728294372556 and perplexity is 42.85716445013549
At time: 488.62554001808167 and batch: 850, loss is 3.632052154541016 and perplexity is 37.79028861011985
At time: 489.0653259754181 and batch: 900, loss is 3.572959237098694 and perplexity is 35.621850836111896
At time: 489.51278591156006 and batch: 950, loss is 3.5926191854476928 and perplexity is 36.3291040847128
At time: 489.99053382873535 and batch: 1000, loss is 3.52195924282074 and perplexity is 33.85068524814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.418029040825076 and perplexity of 82.93266726757027
Finished 49 epochs...
Completing Train Step...
At time: 491.3662042617798 and batch: 50, loss is 3.7973673009872435 and perplexity is 44.58365450743951
At time: 491.8176944255829 and batch: 100, loss is 3.6750465059280395 and perplexity is 39.450491496258046
At time: 492.27828335762024 and batch: 150, loss is 3.76276882648468 and perplexity is 43.067507500046126
At time: 492.7401843070984 and batch: 200, loss is 3.841865258216858 and perplexity is 46.612337449067745
At time: 493.1989622116089 and batch: 250, loss is 3.7988032150268554 and perplexity is 44.6477187872758
At time: 493.6564099788666 and batch: 300, loss is 3.6318373394012453 and perplexity is 37.78217155585439
At time: 494.0961368083954 and batch: 350, loss is 3.6799018239974974 and perplexity is 39.642501939149085
At time: 494.52451825141907 and batch: 400, loss is 3.6674788284301756 and perplexity is 39.15306971498063
At time: 494.96620416641235 and batch: 450, loss is 3.73205050945282 and perplexity is 41.7646592509854
At time: 495.40470242500305 and batch: 500, loss is 3.7689316654205323 and perplexity is 43.3337451593895
At time: 495.85134840011597 and batch: 550, loss is 3.654779624938965 and perplexity is 38.65900069671391
At time: 496.3068130016327 and batch: 600, loss is 3.617861795425415 and perplexity is 37.257817755095886
At time: 496.7562515735626 and batch: 650, loss is 3.5874105501174927 and perplexity is 36.14037097732027
At time: 497.2104094028473 and batch: 700, loss is 3.678092432022095 and perplexity is 39.570837967914464
At time: 497.65692138671875 and batch: 750, loss is 3.651549801826477 and perplexity is 38.534340386502365
At time: 498.0942232608795 and batch: 800, loss is 3.7578818082809446 and perplexity is 42.85754925964361
At time: 498.54755687713623 and batch: 850, loss is 3.6320869588851927 and perplexity is 37.79160389921994
At time: 498.9773669242859 and batch: 900, loss is 3.5730038928985595 and perplexity is 35.62344159387168
At time: 499.4376931190491 and batch: 950, loss is 3.592660975456238 and perplexity is 36.33062231000605
At time: 499.86908769607544 and batch: 1000, loss is 3.5220152950286865 and perplexity is 33.85258270696655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.417984753120236 and perplexity of 82.9289944514116
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
SETTINGS FOR THIS RUN
{'anneal': 2.0, 'dropout': 1.0, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 4.042242204677684}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 9600 tokens
Getting Batches...
Created Iterator with 1043 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 0.7058773040771484 and batch: 50, loss is 7.475697679519653 and perplexity is 1764.6324067030368
At time: 1.1850149631500244 and batch: 100, loss is 6.594802083969117 and perplexity is 731.2841395125082
At time: 1.6431422233581543 and batch: 150, loss is 6.440677070617676 and perplexity is 626.8310650629139
At time: 2.107954978942871 and batch: 200, loss is 6.42119348526001 and perplexity is 614.7363552610681
At time: 2.581953525543213 and batch: 250, loss is 6.457240753173828 and perplexity is 637.3001598690062
At time: 3.0316197872161865 and batch: 300, loss is 6.43157452583313 and perplexity is 621.1511970649384
At time: 3.4793546199798584 and batch: 350, loss is 6.449535150527954 and perplexity is 632.4082498336337
At time: 3.9165539741516113 and batch: 400, loss is 6.411483097076416 and perplexity is 608.7959152877216
At time: 4.358181953430176 and batch: 450, loss is 6.454843616485595 and perplexity is 635.7742938597329
At time: 4.804131031036377 and batch: 500, loss is 6.4574151134490965 and perplexity is 637.4112893883149
At time: 5.267204761505127 and batch: 550, loss is 6.434388570785522 and perplexity is 622.9016061655564
At time: 5.730810880661011 and batch: 600, loss is 6.382274856567383 and perplexity is 591.2712361664483
At time: 6.185606956481934 and batch: 650, loss is 6.360873899459839 and perplexity is 578.7519064400149
At time: 6.641664743423462 and batch: 700, loss is 6.423031930923462 and perplexity is 615.8675541528028
At time: 7.105672121047974 and batch: 750, loss is 6.358128604888916 and perplexity is 577.165240902008
At time: 7.545611381530762 and batch: 800, loss is 6.423551445007324 and perplexity is 616.1875891451239
At time: 8.001213788986206 and batch: 850, loss is 6.397011775970459 and perplexity is 600.0492744757881
At time: 8.443080425262451 and batch: 900, loss is 6.419405488967896 and perplexity is 613.638190986622
At time: 8.883550643920898 and batch: 950, loss is 6.420373430252075 and perplexity is 614.2324442799784
At time: 9.352425575256348 and batch: 1000, loss is 6.3907364463806156 and perplexity is 596.2955577315494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 6.466006395293445 and perplexity of 642.9110606404817
Finished 1 epochs...
Completing Train Step...
At time: 10.813262939453125 and batch: 50, loss is 5.845110340118408 and perplexity is 345.5406666757653
At time: 11.254300355911255 and batch: 100, loss is 5.46380539894104 and perplexity is 235.99376824631995
At time: 11.716856002807617 and batch: 150, loss is 5.28885437965393 and perplexity is 198.11632925354897
At time: 12.1647207736969 and batch: 200, loss is 5.181532516479492 and perplexity is 177.9553215894889
At time: 12.613431692123413 and batch: 250, loss is 5.180048246383667 and perplexity is 177.69138375323524
At time: 13.06804084777832 and batch: 300, loss is 5.031692695617676 and perplexity is 153.1921009529194
At time: 13.52342939376831 and batch: 350, loss is 4.97715353012085 and perplexity is 145.06088199440208
At time: 13.971373081207275 and batch: 400, loss is 4.95691291809082 and perplexity is 142.15427593543004
At time: 14.43735146522522 and batch: 450, loss is 4.942973241806031 and perplexity is 140.18643871813777
At time: 14.896698474884033 and batch: 500, loss is 4.940719995498657 and perplexity is 139.8709197472243
At time: 15.334184646606445 and batch: 550, loss is 4.827371301651001 and perplexity is 124.88225103771275
At time: 15.792223453521729 and batch: 600, loss is 4.7337225151062015 and perplexity is 113.71809274093134
At time: 16.24478006362915 and batch: 650, loss is 4.695493564605713 and perplexity is 109.4528173489623
At time: 16.68208622932434 and batch: 700, loss is 4.730105590820313 and perplexity is 113.30752595163825
At time: 17.113973140716553 and batch: 750, loss is 4.691431999206543 and perplexity is 109.00916913617112
At time: 17.56364893913269 and batch: 800, loss is 4.758936376571655 and perplexity is 116.6218182106626
At time: 18.02991771697998 and batch: 850, loss is 4.629097566604615 and perplexity is 102.42159372600055
At time: 18.480123281478882 and batch: 900, loss is 4.624694976806641 and perplexity is 101.97166461576566
At time: 18.930753469467163 and batch: 950, loss is 4.604645566940308 and perplexity is 99.94755185407274
At time: 19.385055541992188 and batch: 1000, loss is 4.494484624862671 and perplexity is 89.52201967088044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.690578181569169 and perplexity of 108.91613490771161
Finished 2 epochs...
Completing Train Step...
At time: 20.756707429885864 and batch: 50, loss is 4.615959854125976 and perplexity is 101.08480865230949
At time: 21.200865745544434 and batch: 100, loss is 4.482666397094727 and perplexity is 88.4702552893501
At time: 21.630422353744507 and batch: 150, loss is 4.541114892959595 and perplexity is 93.79531358274241
At time: 22.086272716522217 and batch: 200, loss is 4.554453783035278 and perplexity is 95.0548204975174
At time: 22.545583724975586 and batch: 250, loss is 4.56829047203064 and perplexity is 96.37920591095383
At time: 23.001782417297363 and batch: 300, loss is 4.435389499664307 and perplexity is 84.38498641197899
At time: 23.45586109161377 and batch: 350, loss is 4.435009241104126 and perplexity is 84.35290439866175
At time: 23.896033763885498 and batch: 400, loss is 4.454597473144531 and perplexity is 86.02151790415698
At time: 24.337011098861694 and batch: 450, loss is 4.488234128952026 and perplexity is 88.96420776959982
At time: 24.78487753868103 and batch: 500, loss is 4.500135145187378 and perplexity is 90.02929750468209
At time: 25.237868070602417 and batch: 550, loss is 4.3956497955322265 and perplexity is 81.09731031532341
At time: 25.683528423309326 and batch: 600, loss is 4.331498928070069 and perplexity is 76.05820694998593
At time: 26.155909061431885 and batch: 650, loss is 4.300651869773865 and perplexity is 73.7478520296332
At time: 26.61917209625244 and batch: 700, loss is 4.370162959098816 and perplexity is 79.05651362813859
At time: 27.078968286514282 and batch: 750, loss is 4.3399067735672 and perplexity is 76.70038850136288
At time: 27.541499614715576 and batch: 800, loss is 4.438978137969971 and perplexity is 84.6883576258434
At time: 27.98803424835205 and batch: 850, loss is 4.3163430118560795 and perplexity is 74.91416652738869
At time: 28.44983673095703 and batch: 900, loss is 4.281293411254882 and perplexity is 72.33393705752263
At time: 28.908183336257935 and batch: 950, loss is 4.299279069900512 and perplexity is 73.64668044776312
At time: 29.34882640838623 and batch: 1000, loss is 4.19798469543457 and perplexity is 66.55207310428312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.5239384348799545 and perplexity of 92.1979996797641
Finished 3 epochs...
Completing Train Step...
At time: 30.74434733390808 and batch: 50, loss is 4.33116099357605 and perplexity is 76.03250860072818
At time: 31.1990168094635 and batch: 100, loss is 4.210985388755798 and perplexity is 67.42294489911573
At time: 31.653401374816895 and batch: 150, loss is 4.301896095275879 and perplexity is 73.83966809594781
At time: 32.08179044723511 and batch: 200, loss is 4.332912521362305 and perplexity is 76.16579834847336
At time: 32.54907774925232 and batch: 250, loss is 4.334850635528564 and perplexity is 76.3135595039786
At time: 33.00763201713562 and batch: 300, loss is 4.1954670238494876 and perplexity is 66.38472758981911
At time: 33.44445323944092 and batch: 350, loss is 4.2090685939788814 and perplexity is 67.29383273078464
At time: 33.89573621749878 and batch: 400, loss is 4.238147392272949 and perplexity is 69.2793853518669
At time: 34.35207557678223 and batch: 450, loss is 4.28142629146576 and perplexity is 72.34354944496631
At time: 34.79556727409363 and batch: 500, loss is 4.2921686935424805 and perplexity is 73.1248821218974
At time: 35.24202084541321 and batch: 550, loss is 4.190879626274109 and perplexity is 66.08089189183535
At time: 35.70488357543945 and batch: 600, loss is 4.137590022087097 and perplexity is 62.65165026941005
At time: 36.1603741645813 and batch: 650, loss is 4.105950469970703 and perplexity is 60.70041105604053
At time: 36.62020969390869 and batch: 700, loss is 4.1893442296981815 and perplexity is 65.97950936779749
At time: 37.07072186470032 and batch: 750, loss is 4.157429342269897 and perplexity is 63.907028165116316
At time: 37.50925016403198 and batch: 800, loss is 4.266261067390442 and perplexity is 71.25472034357534
At time: 37.94416332244873 and batch: 850, loss is 4.144786605834961 and perplexity is 63.10415440651361
At time: 38.38393568992615 and batch: 900, loss is 4.093439474105835 and perplexity is 59.94571928114632
At time: 38.84919762611389 and batch: 950, loss is 4.124704966545105 and perplexity is 61.84955886922683
At time: 39.28568625450134 and batch: 1000, loss is 4.031258111000061 and perplexity is 56.33173826322299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.452810892244664 and perplexity of 85.86797070637914
Finished 4 epochs...
Completing Train Step...
At time: 40.70974850654602 and batch: 50, loss is 4.164028635025025 and perplexity is 64.33016401611319
At time: 41.177523374557495 and batch: 100, loss is 4.052103390693665 and perplexity is 57.51831338192408
At time: 41.60887694358826 and batch: 150, loss is 4.155296506881714 and perplexity is 63.770870246720555
At time: 42.05887198448181 and batch: 200, loss is 4.193325238227844 and perplexity is 66.24269788765126
At time: 42.48868989944458 and batch: 250, loss is 4.190757975578308 and perplexity is 66.07285359429974
At time: 42.948739528656006 and batch: 300, loss is 4.046784286499023 and perplexity is 57.21317971824602
At time: 43.41244554519653 and batch: 350, loss is 4.068805885314942 and perplexity is 58.48708059481223
At time: 43.8522675037384 and batch: 400, loss is 4.09597499370575 and perplexity is 60.09790568158317
At time: 44.31095886230469 and batch: 450, loss is 4.144277772903442 and perplexity is 63.072053102428896
At time: 44.76568365097046 and batch: 500, loss is 4.154760255813598 and perplexity is 63.73668221693909
At time: 45.233272552490234 and batch: 550, loss is 4.058721461296082 and perplexity is 57.90023604240308
At time: 45.687905073165894 and batch: 600, loss is 4.010007286071778 and perplexity is 55.147272368984844
At time: 46.1294960975647 and batch: 650, loss is 3.9776997232437132 and perplexity is 53.39407171571514
At time: 46.56883692741394 and batch: 700, loss is 4.069492335319519 and perplexity is 58.52724283466205
At time: 47.03983163833618 and batch: 750, loss is 4.0349215364456175 and perplexity is 56.538483853875434
At time: 47.48580503463745 and batch: 800, loss is 4.1475091075897215 and perplexity is 63.27618965448963
At time: 47.93007016181946 and batch: 850, loss is 4.027095513343811 and perplexity is 56.09773926128377
At time: 48.3837411403656 and batch: 900, loss is 3.966159129142761 and perplexity is 52.781414420958825
At time: 48.834893465042114 and batch: 950, loss is 4.00240713596344 and perplexity is 54.72973350974949
At time: 49.28230333328247 and batch: 1000, loss is 3.9134941864013673 and perplexity is 50.0736131845421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.4245385891053735 and perplexity of 83.47428239151986
Finished 5 epochs...
Completing Train Step...
At time: 50.632896900177 and batch: 50, loss is 4.052126245498657 and perplexity is 57.51962796678215
At time: 51.08059859275818 and batch: 100, loss is 3.9416961526870726 and perplexity is 51.505889105083675
At time: 51.54419207572937 and batch: 150, loss is 4.049885311126709 and perplexity is 57.390874573243266
At time: 51.97315788269043 and batch: 200, loss is 4.0918059825897215 and perplexity is 59.84787839016163
At time: 52.42881941795349 and batch: 250, loss is 4.085258884429932 and perplexity is 59.45732833551799
At time: 52.90564203262329 and batch: 300, loss is 3.9387285137176513 and perplexity is 51.35326480034993
At time: 53.39162611961365 and batch: 350, loss is 3.9666861963272093 and perplexity is 52.80924110507071
At time: 53.8562273979187 and batch: 400, loss is 3.991323628425598 and perplexity is 54.12648532420197
At time: 54.29437327384949 and batch: 450, loss is 4.042315754890442 and perplexity is 56.9580911763163
At time: 54.74525213241577 and batch: 500, loss is 4.057397050857544 and perplexity is 57.823603123315294
At time: 55.184133768081665 and batch: 550, loss is 3.960844135284424 and perplexity is 52.5016257237301
At time: 55.63084173202515 and batch: 600, loss is 3.9155108165740966 and perplexity is 50.1746950318426
At time: 56.097164154052734 and batch: 650, loss is 3.881613006591797 and perplexity is 48.50238667816756
At time: 56.55696892738342 and batch: 700, loss is 3.978295741081238 and perplexity is 53.42590502054216
At time: 57.02247667312622 and batch: 750, loss is 3.944106273651123 and perplexity is 51.63017423913037
At time: 57.46248984336853 and batch: 800, loss is 4.058149204254151 and perplexity is 57.86711170332047
At time: 57.90205907821655 and batch: 850, loss is 3.938522233963013 and perplexity is 51.34267275398695
At time: 58.338106870651245 and batch: 900, loss is 3.872718014717102 and perplexity is 48.07287144199476
At time: 58.791099309921265 and batch: 950, loss is 3.9102339363098144 and perplexity is 49.91062651559255
At time: 59.24562335014343 and batch: 1000, loss is 3.8255222749710085 and perplexity is 45.85674394283713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.414068082483803 and perplexity of 82.60482414250585
Finished 6 epochs...
Completing Train Step...
At time: 60.686243534088135 and batch: 50, loss is 3.968455891609192 and perplexity is 52.90278011325788
At time: 61.13771891593933 and batch: 100, loss is 3.859555802345276 and perplexity is 47.44427205313273
At time: 61.568321228027344 and batch: 150, loss is 3.966738805770874 and perplexity is 52.81201944294836
At time: 62.00119376182556 and batch: 200, loss is 4.011380114555359 and perplexity is 55.223032105948775
At time: 62.434221506118774 and batch: 250, loss is 4.00289074420929 and perplexity is 54.75620766121094
At time: 62.87019395828247 and batch: 300, loss is 3.8562595653533935 and perplexity is 47.28814195081276
At time: 63.33564901351929 and batch: 350, loss is 3.886842851638794 and perplexity is 48.75671110389842
At time: 63.7952196598053 and batch: 400, loss is 3.9082121562957766 and perplexity is 49.809820146903924
At time: 64.22871851921082 and batch: 450, loss is 3.965951261520386 and perplexity is 52.770444014073234
At time: 64.67797255516052 and batch: 500, loss is 3.9795096111297608 and perplexity is 53.49079650340753
At time: 65.11141562461853 and batch: 550, loss is 3.8842320346832278 and perplexity is 48.629582283104185
At time: 65.54259872436523 and batch: 600, loss is 3.8410400199890136 and perplexity is 46.57388703387396
At time: 65.99112343788147 and batch: 650, loss is 3.806408152580261 and perplexity is 44.988556281790615
At time: 66.45151972770691 and batch: 700, loss is 3.906314253807068 and perplexity is 49.71537561688863
At time: 66.9272301197052 and batch: 750, loss is 3.870711898803711 and perplexity is 47.97652835959901
At time: 67.38913536071777 and batch: 800, loss is 3.9874823188781736 and perplexity is 53.91896756450509
At time: 67.84285616874695 and batch: 850, loss is 3.8652302551269533 and perplexity is 47.71425762038814
At time: 68.28431749343872 and batch: 900, loss is 3.795931167602539 and perplexity is 44.51967238721549
At time: 68.74870681762695 and batch: 950, loss is 3.8364980936050417 and perplexity is 46.36283152965578
At time: 69.21219944953918 and batch: 1000, loss is 3.7541553926467897 and perplexity is 42.69814141245161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.411802710556403 and perplexity of 82.41790529312017
Finished 7 epochs...
Completing Train Step...
At time: 70.62641882896423 and batch: 50, loss is 3.9018107557296755 and perplexity is 49.49198591348732
At time: 71.05883550643921 and batch: 100, loss is 3.794048466682434 and perplexity is 44.43593401094301
At time: 71.51836276054382 and batch: 150, loss is 3.9006400394439695 and perplexity is 49.43407874261432
At time: 71.96255612373352 and batch: 200, loss is 3.9450144243240355 and perplexity is 51.677083513728874
At time: 72.42564702033997 and batch: 250, loss is 3.9339321994781495 and perplexity is 51.107548143345106
At time: 72.88097977638245 and batch: 300, loss is 3.7870078563690184 and perplexity is 44.12417668440508
At time: 73.33676671981812 and batch: 350, loss is 3.8232900667190552 and perplexity is 45.75449630207019
At time: 73.78010201454163 and batch: 400, loss is 3.843023672103882 and perplexity is 46.666365115233035
At time: 74.2149076461792 and batch: 450, loss is 3.9030542516708375 and perplexity is 49.553567277243545
At time: 74.67768049240112 and batch: 500, loss is 3.9181359386444092 and perplexity is 50.30658276610346
At time: 75.13453769683838 and batch: 550, loss is 3.8242981147766115 and perplexity is 45.80064228797003
At time: 75.58737325668335 and batch: 600, loss is 3.7805998516082764 and perplexity is 43.84233274324063
At time: 76.02291750907898 and batch: 650, loss is 3.745874743461609 and perplexity is 42.34603293791155
At time: 76.47312474250793 and batch: 700, loss is 3.8464473390579226 and perplexity is 46.826409019526956
At time: 76.90523743629456 and batch: 750, loss is 3.8105738067626955 and perplexity is 45.17635392788662
At time: 77.35807466506958 and batch: 800, loss is 3.9289005994796753 and perplexity is 50.85104126535361
At time: 77.81493163108826 and batch: 850, loss is 3.8040117692947386 and perplexity is 44.8808755311837
At time: 78.2748384475708 and batch: 900, loss is 3.73493004322052 and perplexity is 41.88509531423213
At time: 78.72455525398254 and batch: 950, loss is 3.776040062904358 and perplexity is 43.642876055407015
At time: 79.18272686004639 and batch: 1000, loss is 3.6948592376708986 and perplexity is 40.23990792613299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.409691415181974 and perplexity of 82.2440803134122
Finished 8 epochs...
Completing Train Step...
At time: 80.56592226028442 and batch: 50, loss is 3.8447481203079223 and perplexity is 46.74690827103586
At time: 81.03607249259949 and batch: 100, loss is 3.7396138191223143 and perplexity is 42.08173586495811
At time: 81.47647976875305 and batch: 150, loss is 3.8448838901519777 and perplexity is 46.753255522354806
At time: 81.9407286643982 and batch: 200, loss is 3.8909780263900755 and perplexity is 48.95874606164468
At time: 82.37952136993408 and batch: 250, loss is 3.879271626472473 and perplexity is 48.38895699707869
At time: 82.82312226295471 and batch: 300, loss is 3.7276119422912597 and perplexity is 41.57969479815683
At time: 83.2857141494751 and batch: 350, loss is 3.7689489269256593 and perplexity is 43.33449317150963
At time: 83.73586177825928 and batch: 400, loss is 3.787616295814514 and perplexity is 44.151031743009895
At time: 84.1966118812561 and batch: 450, loss is 3.8505923748016357 and perplexity is 47.02090898483635
At time: 84.66265630722046 and batch: 500, loss is 3.8641668796539306 and perplexity is 47.663546416424936
At time: 85.11731624603271 and batch: 550, loss is 3.771985068321228 and perplexity is 43.466262754403616
At time: 85.57156419754028 and batch: 600, loss is 3.73014790058136 and perplexity is 41.68527298423632
At time: 86.02747130393982 and batch: 650, loss is 3.696345582008362 and perplexity is 40.299762756844025
At time: 86.47939085960388 and batch: 700, loss is 3.795316767692566 and perplexity is 44.492327905592475
At time: 86.9400098323822 and batch: 750, loss is 3.7607146501541138 and perplexity is 42.97913004803483
At time: 87.3758864402771 and batch: 800, loss is 3.878182349205017 and perplexity is 48.33627670315804
At time: 87.83513450622559 and batch: 850, loss is 3.7532473373413087 and perplexity is 42.659386736965175
At time: 88.28999614715576 and batch: 900, loss is 3.681107063293457 and perplexity is 39.69030944423108
At time: 88.7276017665863 and batch: 950, loss is 3.723052396774292 and perplexity is 41.390541840601635
At time: 89.16855454444885 and batch: 1000, loss is 3.6464617919921873 and perplexity is 38.33877522428879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.414851863209794 and perplexity of 82.66959359074946
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 90.50441217422485 and batch: 50, loss is 3.805516858100891 and perplexity is 44.94847609422011
At time: 90.96467280387878 and batch: 100, loss is 3.6879555940628053 and perplexity is 39.96306266304698
At time: 91.40938663482666 and batch: 150, loss is 3.7869267511367797 and perplexity is 44.12059812792953
At time: 91.8445897102356 and batch: 200, loss is 3.8304922103881838 and perplexity is 46.08521627476519
At time: 92.2985053062439 and batch: 250, loss is 3.8062637758255007 and perplexity is 44.982061448896026
At time: 92.75434637069702 and batch: 300, loss is 3.646617603302002 and perplexity is 38.34474930447573
At time: 93.22588896751404 and batch: 350, loss is 3.6900668144226074 and perplexity is 40.04752262000001
At time: 93.69908499717712 and batch: 400, loss is 3.704002523422241 and perplexity is 40.60952006264907
At time: 94.15886783599854 and batch: 450, loss is 3.7610736179351805 and perplexity is 42.994560940411255
At time: 94.60425162315369 and batch: 500, loss is 3.7745761108398437 and perplexity is 43.57903172082944
At time: 95.02773571014404 and batch: 550, loss is 3.6707968282699586 and perplexity is 39.283195353064364
At time: 95.49214220046997 and batch: 600, loss is 3.6268851470947268 and perplexity is 37.59552950166158
At time: 95.91943025588989 and batch: 650, loss is 3.5860647773742675 and perplexity is 36.09176696344308
At time: 96.38353061676025 and batch: 700, loss is 3.6763340187072755 and perplexity is 39.50131722057029
At time: 96.84729981422424 and batch: 750, loss is 3.6418194580078125 and perplexity is 38.16120631114856
At time: 97.2952241897583 and batch: 800, loss is 3.756805305480957 and perplexity is 42.81143781186937
At time: 97.74316048622131 and batch: 850, loss is 3.624911551475525 and perplexity is 37.52140429998247
At time: 98.20210218429565 and batch: 900, loss is 3.5511468553543093 and perplexity is 34.853266230500154
At time: 98.65291023254395 and batch: 950, loss is 3.5806997060775756 and perplexity is 35.89865056530438
At time: 99.09319496154785 and batch: 1000, loss is 3.50536536693573 and perplexity is 33.2936060132512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3736114501953125 and perplexity of 79.32960992594056
Finished 10 epochs...
Completing Train Step...
At time: 100.52066540718079 and batch: 50, loss is 3.7350932121276856 and perplexity is 41.89193021706774
At time: 100.95478630065918 and batch: 100, loss is 3.619803366661072 and perplexity is 37.330226733200405
At time: 101.41897487640381 and batch: 150, loss is 3.7229482889175416 and perplexity is 41.386232984297656
At time: 101.90593361854553 and batch: 200, loss is 3.772971806526184 and perplexity is 43.509173743964745
At time: 102.36994743347168 and batch: 250, loss is 3.745986828804016 and perplexity is 42.350779573522146
At time: 102.83643674850464 and batch: 300, loss is 3.5897148752212527 and perplexity is 36.22374616630009
At time: 103.28857207298279 and batch: 350, loss is 3.6389347982406615 and perplexity is 38.05128283677399
At time: 103.7275447845459 and batch: 400, loss is 3.653030695915222 and perplexity is 38.591447938075575
At time: 104.19046211242676 and batch: 450, loss is 3.714904909133911 and perplexity is 41.054682973412085
At time: 104.64755058288574 and batch: 500, loss is 3.731721019744873 and perplexity is 41.75090049241544
At time: 105.10923552513123 and batch: 550, loss is 3.6290957498550416 and perplexity is 37.67873021093038
At time: 105.56943726539612 and batch: 600, loss is 3.5893128728866577 and perplexity is 36.20918706236642
At time: 106.03860425949097 and batch: 650, loss is 3.550428395271301 and perplexity is 34.82823454316191
At time: 106.5017101764679 and batch: 700, loss is 3.643437008857727 and perplexity is 38.222983953620016
At time: 106.95243811607361 and batch: 750, loss is 3.611433057785034 and perplexity is 37.019065280481634
At time: 107.42112708091736 and batch: 800, loss is 3.7297237300872803 and perplexity is 41.66759507088843
At time: 107.88187074661255 and batch: 850, loss is 3.5977795791625975 and perplexity is 36.51706111413381
At time: 108.35278105735779 and batch: 900, loss is 3.5258734750747682 and perplexity is 33.983444347631355
At time: 108.81959891319275 and batch: 950, loss is 3.557860608100891 and perplexity is 35.08804969956819
At time: 109.28004503250122 and batch: 1000, loss is 3.4849760580062865 and perplexity is 32.621646073264465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.376903999142531 and perplexity of 79.59123702324176
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 110.64191317558289 and batch: 50, loss is 3.715421371459961 and perplexity is 41.07589164674474
At time: 111.10799956321716 and batch: 100, loss is 3.598834795951843 and perplexity is 36.55561486782203
At time: 111.55789804458618 and batch: 150, loss is 3.6982478523254394 and perplexity is 40.3764967605911
At time: 112.00528764724731 and batch: 200, loss is 3.7506816482543943 and perplexity is 42.55007630221814
At time: 112.46905589103699 and batch: 250, loss is 3.715433430671692 and perplexity is 41.07638699260588
At time: 112.91826295852661 and batch: 300, loss is 3.5534813928604128 and perplexity is 34.9347275379668
At time: 113.38169193267822 and batch: 350, loss is 3.6049802780151365 and perplexity is 36.78095845658508
At time: 113.85773348808289 and batch: 400, loss is 3.6146593713760375 and perplexity is 37.13869326885639
At time: 114.32831287384033 and batch: 450, loss is 3.677027201652527 and perplexity is 39.5287083524185
At time: 114.78568506240845 and batch: 500, loss is 3.691916742324829 and perplexity is 40.121676217766804
At time: 115.2388710975647 and batch: 550, loss is 3.583295950889587 and perplexity is 35.99197334256084
At time: 115.69749212265015 and batch: 600, loss is 3.5445021343231202 and perplexity is 34.62244372423208
At time: 116.13873600959778 and batch: 650, loss is 3.499113712310791 and perplexity is 33.0861151436481
At time: 116.59579515457153 and batch: 700, loss is 3.590918445587158 and perplexity is 36.267370240783485
At time: 117.06672286987305 and batch: 750, loss is 3.557312865257263 and perplexity is 35.068835734084814
At time: 117.53492879867554 and batch: 800, loss is 3.6727558183670044 and perplexity is 39.36022617041078
At time: 117.99434423446655 and batch: 850, loss is 3.53870991230011 and perplexity is 34.42248251212558
At time: 118.45115113258362 and batch: 900, loss is 3.465003409385681 and perplexity is 31.976568793400467
At time: 118.9047544002533 and batch: 950, loss is 3.490062212944031 and perplexity is 32.78798748001104
At time: 119.35862517356873 and batch: 1000, loss is 3.420722403526306 and perplexity is 30.591506452761887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.354824531369093 and perplexity of 77.85316335025327
Finished 12 epochs...
Completing Train Step...
At time: 120.72580981254578 and batch: 50, loss is 3.6791344547271727 and perplexity is 39.6120931702318
At time: 121.18421530723572 and batch: 100, loss is 3.5598745107650758 and perplexity is 35.15878481925487
At time: 121.6489007472992 and batch: 150, loss is 3.6613601303100585 and perplexity is 38.91423532382119
At time: 122.10322690010071 and batch: 200, loss is 3.7161057996749878 and perplexity is 41.10401476897649
At time: 122.56031012535095 and batch: 250, loss is 3.6819575119018553 and perplexity is 39.724078369997464
At time: 123.03676629066467 and batch: 300, loss is 3.5204740858078 and perplexity is 33.80044897915831
At time: 123.4937150478363 and batch: 350, loss is 3.574142184257507 and perplexity is 35.66401453714845
At time: 123.9502489566803 and batch: 400, loss is 3.5850180339813233 and perplexity is 36.054007910301735
At time: 124.41675734519958 and batch: 450, loss is 3.6498504447937012 and perplexity is 38.46891239266924
At time: 124.8729600906372 and batch: 500, loss is 3.6668427562713624 and perplexity is 39.128173456150805
At time: 125.30328392982483 and batch: 550, loss is 3.5598517084121704 and perplexity is 35.157983125375985
At time: 125.75127816200256 and batch: 600, loss is 3.5241633367538454 and perplexity is 33.92537762239322
At time: 126.19407629966736 and batch: 650, loss is 3.480250849723816 and perplexity is 32.46786560954056
At time: 126.65082216262817 and batch: 700, loss is 3.5744204902648926 and perplexity is 35.67394142793422
At time: 127.08418035507202 and batch: 750, loss is 3.541205129623413 and perplexity is 34.508481335057475
At time: 127.53653120994568 and batch: 800, loss is 3.658714756965637 and perplexity is 38.81142868390285
At time: 128.0035858154297 and batch: 850, loss is 3.526369423866272 and perplexity is 34.000302575850114
At time: 128.48012828826904 and batch: 900, loss is 3.4538048696517945 and perplexity is 31.620475501039465
At time: 128.93112754821777 and batch: 950, loss is 3.480431079864502 and perplexity is 32.47371782488409
At time: 129.37702751159668 and batch: 1000, loss is 3.4124478101730347 and perplexity is 30.33941857747649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.357316179973323 and perplexity of 78.04738794528646
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 130.73042559623718 and batch: 50, loss is 3.672460522651672 and perplexity is 39.34860498019645
At time: 131.20189428329468 and batch: 100, loss is 3.555885090827942 and perplexity is 35.01880107476101
At time: 131.65132880210876 and batch: 150, loss is 3.6544442319869996 and perplexity is 38.64603691445233
At time: 132.12131667137146 and batch: 200, loss is 3.710576958656311 and perplexity is 40.877384285351106
At time: 132.55822467803955 and batch: 250, loss is 3.6734722900390624 and perplexity is 39.38843676230587
At time: 132.9919240474701 and batch: 300, loss is 3.507812304496765 and perplexity is 33.37517314248484
At time: 133.44554734230042 and batch: 350, loss is 3.5629355621337893 and perplexity is 35.26657255337913
At time: 133.88838815689087 and batch: 400, loss is 3.571025919914246 and perplexity is 35.553049029036686
At time: 134.3207700252533 and batch: 450, loss is 3.6359060525894167 and perplexity is 37.936209531211546
At time: 134.76623916625977 and batch: 500, loss is 3.651816897392273 and perplexity is 38.54463411259358
At time: 135.20843195915222 and batch: 550, loss is 3.5408819913864136 and perplexity is 34.49733212670212
At time: 135.66934609413147 and batch: 600, loss is 3.505152087211609 and perplexity is 33.28650591932565
At time: 136.10706639289856 and batch: 650, loss is 3.4568073463439943 and perplexity is 31.715557911658884
At time: 136.55879259109497 and batch: 700, loss is 3.5516800689697265 and perplexity is 34.87185542216063
At time: 137.0203046798706 and batch: 750, loss is 3.518448357582092 and perplexity is 33.73204776015043
At time: 137.45938324928284 and batch: 800, loss is 3.6330334758758545 and perplexity is 37.827391228400664
At time: 137.90338134765625 and batch: 850, loss is 3.499306869506836 and perplexity is 33.09250658213356
At time: 138.3684582710266 and batch: 900, loss is 3.4251194286346434 and perplexity is 30.726314234140478
At time: 138.81334280967712 and batch: 950, loss is 3.448723382949829 and perplexity is 31.46020402881927
At time: 139.25077104568481 and batch: 1000, loss is 3.3827596712112427 and perplexity is 29.451936728476916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.345527462842988 and perplexity of 77.13271139060811
Finished 14 epochs...
Completing Train Step...
At time: 140.60887908935547 and batch: 50, loss is 3.6525720596313476 and perplexity is 38.57375255798578
At time: 141.0611536502838 and batch: 100, loss is 3.531236562728882 and perplexity is 34.166190141289505
At time: 141.51633048057556 and batch: 150, loss is 3.63165789604187 and perplexity is 37.775392404320975
At time: 141.95939826965332 and batch: 200, loss is 3.6887574100494387 and perplexity is 39.99511853530305
At time: 142.3919496536255 and batch: 250, loss is 3.6529820728302003 and perplexity is 38.58957154843966
At time: 142.82706093788147 and batch: 300, loss is 3.4880011463165284 and perplexity is 32.72047884702631
At time: 143.25314092636108 and batch: 350, loss is 3.54464759349823 and perplexity is 34.62748024263151
At time: 143.71720552444458 and batch: 400, loss is 3.5533553409576415 and perplexity is 34.93032422661656
At time: 144.1553933620453 and batch: 450, loss is 3.620404348373413 and perplexity is 37.352668259582636
At time: 144.5974154472351 and batch: 500, loss is 3.637569465637207 and perplexity is 37.999365629906336
At time: 145.03418493270874 and batch: 550, loss is 3.527207431793213 and perplexity is 34.028807040742706
At time: 145.46951818466187 and batch: 600, loss is 3.4936372089385985 and perplexity is 32.90541417884577
At time: 145.93559312820435 and batch: 650, loss is 3.44615327835083 and perplexity is 31.379451829009714
At time: 146.37922930717468 and batch: 700, loss is 3.54322895526886 and perplexity is 34.5783912033992
At time: 146.82831048965454 and batch: 750, loss is 3.510769944190979 and perplexity is 33.47403100027011
At time: 147.29222202301025 and batch: 800, loss is 3.626008529663086 and perplexity is 37.56258704622358
At time: 147.74914741516113 and batch: 850, loss is 3.493041443824768 and perplexity is 32.88581611952156
At time: 148.2064139842987 and batch: 900, loss is 3.4201129627227784 and perplexity is 30.57286842045376
At time: 148.6560788154602 and batch: 950, loss is 3.4446862602233885 and perplexity is 31.333451354362424
At time: 149.09413361549377 and batch: 1000, loss is 3.3794012689590454 and perplexity is 29.353191184276906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3465397532393295 and perplexity of 77.21083162710418
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 150.50218558311462 and batch: 50, loss is 3.6521121788024904 and perplexity is 38.5560173070508
At time: 150.9722397327423 and batch: 100, loss is 3.5326860666275026 and perplexity is 34.215750077083015
At time: 151.40449976921082 and batch: 150, loss is 3.6333882236480712 and perplexity is 37.84081279166179
At time: 151.8741171360016 and batch: 200, loss is 3.68841591835022 and perplexity is 39.98146286609552
At time: 152.29937744140625 and batch: 250, loss is 3.6528767919540406 and perplexity is 38.58550901839352
At time: 152.75381588935852 and batch: 300, loss is 3.484751524925232 and perplexity is 32.61432225681287
At time: 153.2109079360962 and batch: 350, loss is 3.5421789932250975 and perplexity is 34.54210425839235
At time: 153.66724729537964 and batch: 400, loss is 3.55151451587677 and perplexity is 34.866082756492865
At time: 154.10940599441528 and batch: 450, loss is 3.6159772729873656 and perplexity is 37.187670679173095
At time: 154.5420048236847 and batch: 500, loss is 3.6336776638031005 and perplexity is 37.85176702760405
At time: 154.97231125831604 and batch: 550, loss is 3.5199826383590698 and perplexity is 33.78384191582526
At time: 155.40336990356445 and batch: 600, loss is 3.4859361934661863 and perplexity is 32.65298231353014
At time: 155.84721326828003 and batch: 650, loss is 3.436624484062195 and perplexity is 31.081863567998084
At time: 156.2944552898407 and batch: 700, loss is 3.532722201347351 and perplexity is 34.21698647596478
At time: 156.7418553829193 and batch: 750, loss is 3.5017605352401735 and perplexity is 33.1738042294025
At time: 157.21060943603516 and batch: 800, loss is 3.6134184789657593 and perplexity is 37.092636727767434
At time: 157.6527087688446 and batch: 850, loss is 3.4795908451080324 and perplexity is 32.446443738418154
At time: 158.11841940879822 and batch: 900, loss is 3.405340967178345 and perplexity is 30.124565461080753
At time: 158.5571916103363 and batch: 950, loss is 3.4292281246185303 and perplexity is 30.852819024932277
At time: 159.00019073486328 and batch: 1000, loss is 3.366536326408386 and perplexity is 28.977982758380723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339383381169017 and perplexity of 76.66025460488797
Finished 16 epochs...
Completing Train Step...
At time: 160.38066697120667 and batch: 50, loss is 3.640539164543152 and perplexity is 38.112380030758146
At time: 160.84745693206787 and batch: 100, loss is 3.5175783014297486 and perplexity is 33.70271174830497
At time: 161.29788374900818 and batch: 150, loss is 3.6189805173873903 and perplexity is 37.29952221757327
At time: 161.7451720237732 and batch: 200, loss is 3.6754282760620116 and perplexity is 39.465555390971424
At time: 162.19516730308533 and batch: 250, loss is 3.6402698707580567 and perplexity is 38.1021179854952
At time: 162.6259250640869 and batch: 300, loss is 3.472284965515137 and perplexity is 32.21025775099235
At time: 163.0885078907013 and batch: 350, loss is 3.530964422225952 and perplexity is 34.15689340218825
At time: 163.53655433654785 and batch: 400, loss is 3.5408583641052247 and perplexity is 34.49651705816466
At time: 163.97553038597107 and batch: 450, loss is 3.6066427993774415 and perplexity is 36.84215844479281
At time: 164.42523407936096 and batch: 500, loss is 3.625139675140381 and perplexity is 37.52996479663074
At time: 164.88823294639587 and batch: 550, loss is 3.5118477535247803 and perplexity is 33.51012907324764
At time: 165.34040904045105 and batch: 600, loss is 3.479133810997009 and perplexity is 32.431617995041584
At time: 165.7810926437378 and batch: 650, loss is 3.4308895587921144 and perplexity is 30.904121558902133
At time: 166.22335958480835 and batch: 700, loss is 3.5281353521347047 and perplexity is 34.06039771754158
At time: 166.6945071220398 and batch: 750, loss is 3.4981227588653563 and perplexity is 33.05334458358184
At time: 167.1582887172699 and batch: 800, loss is 3.610042104721069 and perplexity is 36.96760929293261
At time: 167.60335755348206 and batch: 850, loss is 3.4766459369659426 and perplexity is 32.35103249969227
At time: 168.06806874275208 and batch: 900, loss is 3.403350510597229 and perplexity is 30.064663457451395
At time: 168.53330326080322 and batch: 950, loss is 3.4280548334121703 and perplexity is 30.816640911560317
At time: 168.99303007125854 and batch: 1000, loss is 3.3655832386016846 and perplexity is 28.950377353627914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.339695255930831 and perplexity of 76.68416673213414
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 170.42516708374023 and batch: 50, loss is 3.6416464281082153 and perplexity is 38.15460385267987
At time: 170.89089727401733 and batch: 100, loss is 3.5197291135787965 and perplexity is 33.7752779603601
At time: 171.32405877113342 and batch: 150, loss is 3.623002552986145 and perplexity is 37.4498443215318
At time: 171.75272965431213 and batch: 200, loss is 3.678307375907898 and perplexity is 39.57934439176092
At time: 172.2094383239746 and batch: 250, loss is 3.6426584672927858 and perplexity is 38.19323735286111
At time: 172.66266584396362 and batch: 300, loss is 3.471943678855896 and perplexity is 32.19926669538823
At time: 173.11761498451233 and batch: 350, loss is 3.530514664649963 and perplexity is 34.141534534750775
At time: 173.5540268421173 and batch: 400, loss is 3.542364101409912 and perplexity is 34.5484988764411
At time: 174.0064868927002 and batch: 450, loss is 3.6039686489105223 and perplexity is 36.743768582865776
At time: 174.45543479919434 and batch: 500, loss is 3.6235428285598754 and perplexity is 37.47008302440483
At time: 174.93430352210999 and batch: 550, loss is 3.5084374523162842 and perplexity is 33.396044082232464
At time: 175.38583755493164 and batch: 600, loss is 3.4756007099151613 and perplexity is 32.31723599099234
At time: 175.82813262939453 and batch: 650, loss is 3.427039608955383 and perplexity is 30.785370979716433
At time: 176.26914739608765 and batch: 700, loss is 3.523859615325928 and perplexity is 33.915075322852864
At time: 176.7172601222992 and batch: 750, loss is 3.49468843460083 and perplexity is 32.9400233825113
At time: 177.16296577453613 and batch: 800, loss is 3.603847904205322 and perplexity is 36.73933223519843
At time: 177.6147563457489 and batch: 850, loss is 3.4681457567214964 and perplexity is 32.077208318516284
At time: 178.06135082244873 and batch: 900, loss is 3.394659471511841 and perplexity is 29.804502464517476
At time: 178.5402901172638 and batch: 950, loss is 3.420002965927124 and perplexity is 30.569505687841836
At time: 179.02016878128052 and batch: 1000, loss is 3.359764142036438 and perplexity is 28.782401520002995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.335291048375572 and perplexity of 76.34717637809327
Finished 18 epochs...
Completing Train Step...
At time: 180.42951846122742 and batch: 50, loss is 3.6336685180664063 and perplexity is 37.85142084689244
At time: 180.8928143978119 and batch: 100, loss is 3.5100513315200805 and perplexity is 33.449984778441916
At time: 181.34750699996948 and batch: 150, loss is 3.6136268043518065 and perplexity is 37.10036487058947
At time: 181.80410981178284 and batch: 200, loss is 3.6702079725265504 and perplexity is 39.260070027269975
At time: 182.2641236782074 and batch: 250, loss is 3.634755973815918 and perplexity is 37.89260498102274
At time: 182.71857929229736 and batch: 300, loss is 3.4641597509384154 and perplexity is 31.94960286763831
At time: 183.16915202140808 and batch: 350, loss is 3.5237961196899414 and perplexity is 33.912921931941895
At time: 183.63134860992432 and batch: 400, loss is 3.5359984636306763 and perplexity is 34.32927413968349
At time: 184.13563585281372 and batch: 450, loss is 3.598648552894592 and perplexity is 36.54880727230274
At time: 184.59653544425964 and batch: 500, loss is 3.61863582611084 and perplexity is 37.28666761320765
At time: 185.05720925331116 and batch: 550, loss is 3.5038037204742434 and perplexity is 33.24165374732477
At time: 185.4832637310028 and batch: 600, loss is 3.4715123319625856 and perplexity is 32.185380636801725
At time: 185.92687344551086 and batch: 650, loss is 3.423951621055603 and perplexity is 30.69045275523089
At time: 186.4074137210846 and batch: 700, loss is 3.521395735740662 and perplexity is 33.83161552080499
At time: 186.8771378993988 and batch: 750, loss is 3.4930324602127074 and perplexity is 32.88552068743427
At time: 187.32484984397888 and batch: 800, loss is 3.6025060319900515 and perplexity is 36.69006580808178
At time: 187.7934627532959 and batch: 850, loss is 3.467116241455078 and perplexity is 32.04420133635965
At time: 188.25878858566284 and batch: 900, loss is 3.394145841598511 and perplexity is 29.78919791127442
At time: 188.70961666107178 and batch: 950, loss is 3.4200268745422364 and perplexity is 30.570236571124678
At time: 189.15465569496155 and batch: 1000, loss is 3.3598939752578736 and perplexity is 28.786138674511147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.335349478372714 and perplexity of 76.35163747372047
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 190.56256699562073 and batch: 50, loss is 3.634344387054443 and perplexity is 37.877012095587496
At time: 191.02219820022583 and batch: 100, loss is 3.5113837814331053 and perplexity is 33.49458491487591
At time: 191.48897314071655 and batch: 150, loss is 3.6169714450836183 and perplexity is 37.2246600075194
At time: 191.92820763587952 and batch: 200, loss is 3.6739007902145384 and perplexity is 39.405318330989516
At time: 192.36626195907593 and batch: 250, loss is 3.6370808410644533 and perplexity is 37.980802741621524
At time: 192.81194400787354 and batch: 300, loss is 3.464821858406067 and perplexity is 31.970763942965405
At time: 193.27254557609558 and batch: 350, loss is 3.5245717906951906 and perplexity is 33.93923740694383
At time: 193.75397491455078 and batch: 400, loss is 3.537278609275818 and perplexity is 34.37324875142277
At time: 194.21087884902954 and batch: 450, loss is 3.596585040092468 and perplexity is 36.47346610106376
At time: 194.6516342163086 and batch: 500, loss is 3.6176646184921264 and perplexity is 37.250472097070876
At time: 195.11109161376953 and batch: 550, loss is 3.502913317680359 and perplexity is 33.212068459321
At time: 195.56962275505066 and batch: 600, loss is 3.4701620531082153 and perplexity is 32.141950725751904
At time: 196.0247974395752 and batch: 650, loss is 3.42104914188385 and perplexity is 30.60150350445624
At time: 196.48727869987488 and batch: 700, loss is 3.5197597789764403 and perplexity is 33.77631370857002
At time: 196.93592643737793 and batch: 750, loss is 3.490878949165344 and perplexity is 32.81477755573332
At time: 197.3889262676239 and batch: 800, loss is 3.599410710334778 and perplexity is 36.57667383570061
At time: 197.82813143730164 and batch: 850, loss is 3.4614126920700072 and perplexity is 31.86195586858943
At time: 198.30088353157043 and batch: 900, loss is 3.3891275930404663 and perplexity is 29.640082773152454
At time: 198.76612401008606 and batch: 950, loss is 3.416531801223755 and perplexity is 30.46357785154221
At time: 199.21978545188904 and batch: 1000, loss is 3.3557165670394897 and perplexity is 28.666138042558035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3326870057641 and perplexity of 76.14862370954336
Finished 20 epochs...
Completing Train Step...
At time: 200.6139407157898 and batch: 50, loss is 3.629094066619873 and perplexity is 37.67866678881996
At time: 201.08169412612915 and batch: 100, loss is 3.5044823932647704 and perplexity is 33.26422161046631
At time: 201.53191423416138 and batch: 150, loss is 3.6106811809539794 and perplexity is 36.99124196415415
At time: 201.9790062904358 and batch: 200, loss is 3.6681434154510497 and perplexity is 39.17909898534017
At time: 202.43264412879944 and batch: 250, loss is 3.6319876432418825 and perplexity is 37.78785078814137
At time: 202.8771529197693 and batch: 300, loss is 3.4598562574386595 and perplexity is 31.81240338965359
At time: 203.3204219341278 and batch: 350, loss is 3.520388045310974 and perplexity is 33.79754089684342
At time: 203.7749216556549 and batch: 400, loss is 3.5333652114868164 and perplexity is 34.238995420439686
At time: 204.22679376602173 and batch: 450, loss is 3.5935246086120607 and perplexity is 36.36201219272533
At time: 204.6713490486145 and batch: 500, loss is 3.6149101305007934 and perplexity is 37.14800730281576
At time: 205.12652254104614 and batch: 550, loss is 3.500334548950195 and perplexity is 33.12653255177737
At time: 205.57621216773987 and batch: 600, loss is 3.4677564811706545 and perplexity is 32.06472387568046
At time: 206.02425336837769 and batch: 650, loss is 3.4192959594726564 and perplexity is 30.547900488418335
At time: 206.5177125930786 and batch: 700, loss is 3.5184423971176146 and perplexity is 33.731846702077206
At time: 206.97404837608337 and batch: 750, loss is 3.4902232265472413 and perplexity is 32.79326721706055
At time: 207.43164205551147 and batch: 800, loss is 3.5989880847930906 and perplexity is 36.56121886517037
At time: 207.9045751094818 and batch: 850, loss is 3.461236810684204 and perplexity is 31.85635243642102
At time: 208.36097049713135 and batch: 900, loss is 3.389272413253784 and perplexity is 29.644375567096585
At time: 208.84064292907715 and batch: 950, loss is 3.4170627403259277 and perplexity is 30.479756450761098
At time: 209.29420065879822 and batch: 1000, loss is 3.3562621021270753 and perplexity is 28.681780693105242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.33267509646532 and perplexity of 76.14771683823203
Finished 21 epochs...
Completing Train Step...
At time: 210.6917061805725 and batch: 50, loss is 3.6264016008377076 and perplexity is 37.577354718618686
At time: 211.14288067817688 and batch: 100, loss is 3.501595721244812 and perplexity is 33.16833717272221
At time: 211.6079032421112 and batch: 150, loss is 3.6078221702575686 and perplexity is 36.88563464586708
At time: 212.0530068874359 and batch: 200, loss is 3.6654089641571046 and perplexity is 39.07211198945257
At time: 212.5073549747467 and batch: 250, loss is 3.62924346446991 and perplexity is 37.68429632114001
At time: 212.975270986557 and batch: 300, loss is 3.4570899534225465 and perplexity is 31.72452221945371
At time: 213.42772340774536 and batch: 350, loss is 3.517859334945679 and perplexity is 33.71218467092601
At time: 213.8672387599945 and batch: 400, loss is 3.53097975730896 and perplexity is 34.15741720500014
At time: 214.30935835838318 and batch: 450, loss is 3.591407313346863 and perplexity is 36.28510452332978
At time: 214.7621738910675 and batch: 500, loss is 3.6129674911499023 and perplexity is 37.07591217211408
At time: 215.20961046218872 and batch: 550, loss is 3.4984751749038696 and perplexity is 33.06499516514492
At time: 215.66123390197754 and batch: 600, loss is 3.466066446304321 and perplexity is 32.01057914048379
At time: 216.1300048828125 and batch: 650, loss is 3.4178892755508423 and perplexity is 30.504959457265986
At time: 216.59454822540283 and batch: 700, loss is 3.517297873497009 and perplexity is 33.69326189158549
At time: 217.04106783866882 and batch: 750, loss is 3.489369330406189 and perplexity is 32.76527712474804
At time: 217.49149107933044 and batch: 800, loss is 3.598227844238281 and perplexity is 36.533434106744096
At time: 217.95293807983398 and batch: 850, loss is 3.460587949752808 and perplexity is 31.835688798546876
At time: 218.4037470817566 and batch: 900, loss is 3.388806071281433 and perplexity is 29.63055437347733
At time: 218.86712741851807 and batch: 950, loss is 3.416833996772766 and perplexity is 30.472785200315744
At time: 219.31877517700195 and batch: 1000, loss is 3.3560270977020266 and perplexity is 28.67504113966754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.332799399771342 and perplexity of 76.15718283949646
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 220.7233645915985 and batch: 50, loss is 3.6269069480895997 and perplexity is 37.596349130541824
At time: 221.16137719154358 and batch: 100, loss is 3.5023742866516114 and perplexity is 33.19417094798287
At time: 221.60572838783264 and batch: 150, loss is 3.6098952436447145 and perplexity is 36.96218058868402
At time: 222.0757191181183 and batch: 200, loss is 3.6678652334213258 and perplexity is 39.16820157986314
At time: 222.5384430885315 and batch: 250, loss is 3.630913705825806 and perplexity is 37.74729078465783
At time: 222.9865300655365 and batch: 300, loss is 3.4580133390426635 and perplexity is 31.753829716054298
At time: 223.4231641292572 and batch: 350, loss is 3.5193308925628664 and perplexity is 33.76183061254336
At time: 223.86199688911438 and batch: 400, loss is 3.5322227478027344 and perplexity is 34.19990094785222
At time: 224.30358028411865 and batch: 450, loss is 3.590616979598999 and perplexity is 36.25643851003122
At time: 224.7443265914917 and batch: 500, loss is 3.612283968925476 and perplexity is 37.05057862116244
At time: 225.19259405136108 and batch: 550, loss is 3.4986038637161254 and perplexity is 33.06925053390327
At time: 225.64937686920166 and batch: 600, loss is 3.4660170602798464 and perplexity is 32.00899830427483
At time: 226.11126351356506 and batch: 650, loss is 3.415706391334534 and perplexity is 30.438443287708285
At time: 226.5669014453888 and batch: 700, loss is 3.5162846565246584 and perplexity is 33.65914059584017
At time: 227.04182291030884 and batch: 750, loss is 3.487416844367981 and perplexity is 32.70136579192003
At time: 227.49833989143372 and batch: 800, loss is 3.5962465715408327 and perplexity is 36.46112306880137
At time: 227.96968913078308 and batch: 850, loss is 3.456759281158447 and perplexity is 31.714033534118165
At time: 228.42036724090576 and batch: 900, loss is 3.3850895404815673 and perplexity is 29.520635889767952
At time: 228.89502549171448 and batch: 950, loss is 3.414929995536804 and perplexity is 30.414820179877747
At time: 229.33729100227356 and batch: 1000, loss is 3.35197892665863 and perplexity is 28.559194310552392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.33188163943407 and perplexity of 76.08732086086819
Finished 23 epochs...
Completing Train Step...
At time: 230.740088224411 and batch: 50, loss is 3.624475111961365 and perplexity is 37.50503204952759
At time: 231.2224941253662 and batch: 100, loss is 3.4985365295410156 and perplexity is 33.06702391816158
At time: 231.69390273094177 and batch: 150, loss is 3.606287636756897 and perplexity is 36.82907581062188
At time: 232.1505537033081 and batch: 200, loss is 3.6641233777999878 and perplexity is 39.021913689373754
At time: 232.58535814285278 and batch: 250, loss is 3.6278705835342406 and perplexity is 37.63259576661988
At time: 233.0316083431244 and batch: 300, loss is 3.4551686954498293 and perplexity is 31.6636297420185
At time: 233.46560287475586 and batch: 350, loss is 3.516765470504761 and perplexity is 33.67532827251565
At time: 233.93041920661926 and batch: 400, loss is 3.529765944480896 and perplexity is 34.11598164641684
At time: 234.385910987854 and batch: 450, loss is 3.588707504272461 and perplexity is 36.18727379044603
At time: 234.81937956809998 and batch: 500, loss is 3.6107299375534057 and perplexity is 36.99304557528949
At time: 235.27521514892578 and batch: 550, loss is 3.4970310640335085 and perplexity is 33.017280107407586
At time: 235.73507404327393 and batch: 600, loss is 3.4645597887039186 and perplexity is 31.962386472170007
At time: 236.20867133140564 and batch: 650, loss is 3.414725937843323 and perplexity is 30.40861443501072
At time: 236.6938977241516 and batch: 700, loss is 3.5156716966629027 and perplexity is 33.638515215584626
At time: 237.15831017494202 and batch: 750, loss is 3.4872617721557617 and perplexity is 32.69629511195503
At time: 237.63023495674133 and batch: 800, loss is 3.5962065267562866 and perplexity is 36.45966302021763
At time: 238.0738935470581 and batch: 850, loss is 3.456922559738159 and perplexity is 31.719212179239957
At time: 238.51540160179138 and batch: 900, loss is 3.385447301864624 and perplexity is 29.53119912273834
At time: 238.96634244918823 and batch: 950, loss is 3.415567693710327 and perplexity is 30.434221840698836
At time: 239.4390106201172 and batch: 1000, loss is 3.352611236572266 and perplexity is 28.577258282652863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331903597203697 and perplexity of 76.08899158707379
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 240.84275197982788 and batch: 50, loss is 3.6246041822433472 and perplexity is 37.509873147004164
At time: 241.33794617652893 and batch: 100, loss is 3.498668599128723 and perplexity is 33.07139135477359
At time: 241.80595350265503 and batch: 150, loss is 3.6070062255859376 and perplexity is 36.8555502840745
At time: 242.2782437801361 and batch: 200, loss is 3.664839587211609 and perplexity is 39.04987156186816
At time: 242.75353336334229 and batch: 250, loss is 3.6284250354766847 and perplexity is 37.65346701796014
At time: 243.227135181427 and batch: 300, loss is 3.4554813051223756 and perplexity is 31.673529646266072
At time: 243.69073486328125 and batch: 350, loss is 3.517675271034241 and perplexity is 33.7059800453937
At time: 244.15005207061768 and batch: 400, loss is 3.5303802728652953 and perplexity is 34.13694650129725
At time: 244.59313440322876 and batch: 450, loss is 3.588505663871765 and perplexity is 36.17997047368106
At time: 245.05901670455933 and batch: 500, loss is 3.6101530313491823 and perplexity is 36.97171021262727
At time: 245.5379180908203 and batch: 550, loss is 3.496800870895386 and perplexity is 33.009680630794634
At time: 246.19516396522522 and batch: 600, loss is 3.4641995811462403 and perplexity is 31.950875452303936
At time: 246.68784141540527 and batch: 650, loss is 3.413580479621887 and perplexity is 30.37380257917322
At time: 247.14689087867737 and batch: 700, loss is 3.5147791862487794 and perplexity is 33.60850588425149
At time: 247.61095452308655 and batch: 750, loss is 3.485777540206909 and perplexity is 32.64780222239101
At time: 248.07615184783936 and batch: 800, loss is 3.5948469066619873 and perplexity is 36.41012541353782
At time: 248.54802298545837 and batch: 850, loss is 3.454530806541443 and perplexity is 31.64343830443852
At time: 249.01790952682495 and batch: 900, loss is 3.3828112840652467 and perplexity is 29.453456866216403
At time: 249.48571968078613 and batch: 950, loss is 3.4137860774993896 and perplexity is 30.380048010516887
At time: 249.93609809875488 and batch: 1000, loss is 3.3496381521224974 and perplexity is 28.492421855857057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331676576195694 and perplexity of 76.07171974811509
Finished 25 epochs...
Completing Train Step...
At time: 251.3682153224945 and batch: 50, loss is 3.6232838869094848 and perplexity is 37.46038171535691
At time: 251.82878732681274 and batch: 100, loss is 3.496805648803711 and perplexity is 33.0098383483993
At time: 252.30415844917297 and batch: 150, loss is 3.605133981704712 and perplexity is 36.786612260100085
At time: 252.77909231185913 and batch: 200, loss is 3.6627917957305907 and perplexity is 38.96998738852958
At time: 253.2549910545349 and batch: 250, loss is 3.6266956520080567 and perplexity is 37.58840600849505
At time: 253.7577657699585 and batch: 300, loss is 3.453836827278137 and perplexity is 31.62148603252731
At time: 254.2336823940277 and batch: 350, loss is 3.5160486984252928 and perplexity is 33.65119938593214
At time: 254.70218682289124 and batch: 400, loss is 3.5288349533081056 and perplexity is 34.084234748986624
At time: 255.15273761749268 and batch: 450, loss is 3.5872713708877564 and perplexity is 36.13534133834403
At time: 255.5949501991272 and batch: 500, loss is 3.6092909526824952 and perplexity is 36.93985142434596
At time: 256.04813838005066 and batch: 550, loss is 3.495892286300659 and perplexity is 32.97970216454528
At time: 256.5136172771454 and batch: 600, loss is 3.4633923053741453 and perplexity is 31.925092692952756
At time: 257.0001997947693 and batch: 650, loss is 3.41298819065094 and perplexity is 30.355817837507676
At time: 257.45450234413147 and batch: 700, loss is 3.514559187889099 and perplexity is 33.60111288133973
At time: 257.9172320365906 and batch: 750, loss is 3.4858304357528684 and perplexity is 32.64952919138802
At time: 258.4057273864746 and batch: 800, loss is 3.5949902629852293 and perplexity is 36.41534540939666
At time: 258.8711566925049 and batch: 850, loss is 3.4548099422454834 and perplexity is 31.652272350759397
At time: 259.32835054397583 and batch: 900, loss is 3.383224449157715 and perplexity is 29.465628520724273
At time: 259.8183240890503 and batch: 950, loss is 3.4144323587417604 and perplexity is 30.399688411616207
At time: 260.31073546409607 and batch: 1000, loss is 3.3503324365615845 and perplexity is 28.512210569686626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331672110208651 and perplexity of 76.07138001355902
Finished 26 epochs...
Completing Train Step...
At time: 261.76509642601013 and batch: 50, loss is 3.622470345497131 and perplexity is 37.42991853671776
At time: 262.2301824092865 and batch: 100, loss is 3.4958001947402955 and perplexity is 32.976665152156365
At time: 262.71220207214355 and batch: 150, loss is 3.6041182327270507 and perplexity is 36.74926526710129
At time: 263.1915216445923 and batch: 200, loss is 3.661695566177368 and perplexity is 38.927290743603336
At time: 263.66451954841614 and batch: 250, loss is 3.625714159011841 and perplexity is 37.55153135032836
At time: 264.1163237094879 and batch: 300, loss is 3.4528856801986696 and perplexity is 31.591423647580278
At time: 264.5853531360626 and batch: 350, loss is 3.5151660299301146 and perplexity is 33.621509637439644
At time: 265.0277359485626 and batch: 400, loss is 3.5280189037323 and perplexity is 34.056431669566564
At time: 265.47433257102966 and batch: 450, loss is 3.5865615701675413 and perplexity is 36.10970154768427
At time: 265.93081307411194 and batch: 500, loss is 3.60867160320282 and perplexity is 36.916979830073124
At time: 266.3814263343811 and batch: 550, loss is 3.495292649269104 and perplexity is 32.95993224181902
At time: 266.87197709083557 and batch: 600, loss is 3.4628535461425782 and perplexity is 31.907897387026622
At time: 267.34734296798706 and batch: 650, loss is 3.412594690322876 and perplexity is 30.34387516310722
At time: 267.81480050086975 and batch: 700, loss is 3.5143182611465456 and perplexity is 33.59301844978872
At time: 268.28243589401245 and batch: 750, loss is 3.485753631591797 and perplexity is 32.64702166798444
At time: 268.7440884113312 and batch: 800, loss is 3.5949444484710695 and perplexity is 36.41367709625553
At time: 269.23355436325073 and batch: 850, loss is 3.4548380374908447 and perplexity is 31.6531616416097
At time: 269.7142686843872 and batch: 900, loss is 3.3833235454559327 and perplexity is 29.46854860011748
At time: 270.1807794570923 and batch: 950, loss is 3.414642186164856 and perplexity is 30.4060677691572
At time: 270.6495249271393 and batch: 1000, loss is 3.3505438375473022 and perplexity is 28.518238716261664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331707093773819 and perplexity of 76.07404130818962
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 272.07849764823914 and batch: 50, loss is 3.6225485467910765 and perplexity is 37.43284571923283
At time: 272.56272625923157 and batch: 100, loss is 3.4959141778945924 and perplexity is 32.98042415069579
At time: 273.0436820983887 and batch: 150, loss is 3.6045160150527953 and perplexity is 36.76388638312662
At time: 273.51824617385864 and batch: 200, loss is 3.66198709487915 and perplexity is 38.93864082049392
At time: 273.98163199424744 and batch: 250, loss is 3.6259978818893432 and perplexity is 37.562187090424835
At time: 274.46654438972473 and batch: 300, loss is 3.453032865524292 and perplexity is 31.596073783764236
At time: 274.92034006118774 and batch: 350, loss is 3.515621929168701 and perplexity is 33.636841152630986
At time: 275.38259506225586 and batch: 400, loss is 3.5282298707962036 and perplexity is 34.063617212892765
At time: 275.8441367149353 and batch: 450, loss is 3.586572904586792 and perplexity is 36.110110832500126
At time: 276.3096008300781 and batch: 500, loss is 3.608363084793091 and perplexity is 36.90559201892898
At time: 276.7669007778168 and batch: 550, loss is 3.494999575614929 and perplexity is 32.95027396939728
At time: 277.2286331653595 and batch: 600, loss is 3.46225745677948 and perplexity is 31.888883096466582
At time: 277.69198274612427 and batch: 650, loss is 3.412123718261719 and perplexity is 30.329587410508402
At time: 278.18872714042664 and batch: 700, loss is 3.513564038276672 and perplexity is 33.567691379334306
At time: 278.6558105945587 and batch: 750, loss is 3.4846675539016725 and perplexity is 32.61158371377101
At time: 279.12390661239624 and batch: 800, loss is 3.593797707557678 and perplexity is 36.37194397603374
At time: 279.606915473938 and batch: 850, loss is 3.453299388885498 and perplexity is 31.604495997858542
At time: 280.0930905342102 and batch: 900, loss is 3.3816150856018066 and perplexity is 29.418245750310014
At time: 280.56790709495544 and batch: 950, loss is 3.412981324195862 and perplexity is 30.35560940136375
At time: 281.0325119495392 and batch: 1000, loss is 3.3486232471466066 and perplexity is 28.463519424232075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331691090653583 and perplexity of 76.07282389590094
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 282.5024712085724 and batch: 50, loss is 3.6222551250457764 and perplexity is 37.42186371956789
At time: 282.96488666534424 and batch: 100, loss is 3.49559130191803 and perplexity is 32.96977728294209
At time: 283.4341380596161 and batch: 150, loss is 3.604315090179443 and perplexity is 36.756500345955295
At time: 283.9251811504364 and batch: 200, loss is 3.6616875886917115 and perplexity is 38.92698020293845
At time: 284.3838872909546 and batch: 250, loss is 3.625802001953125 and perplexity is 37.55483013217712
At time: 284.8391463756561 and batch: 300, loss is 3.4527055501937864 and perplexity is 31.5857335967742
At time: 285.30588936805725 and batch: 350, loss is 3.5153989219665527 and perplexity is 33.629340731151515
At time: 285.77038764953613 and batch: 400, loss is 3.5278723001480103 and perplexity is 34.051439240578624
At time: 286.2442138195038 and batch: 450, loss is 3.5863375997543336 and perplexity is 36.10161494852111
At time: 286.7059314250946 and batch: 500, loss is 3.608166756629944 and perplexity is 36.89834712304987
At time: 287.15372371673584 and batch: 550, loss is 3.494722018241882 and perplexity is 32.941129647008914
At time: 287.59780383110046 and batch: 600, loss is 3.461781678199768 and perplexity is 31.873714657653615
At time: 288.053480386734 and batch: 650, loss is 3.411813645362854 and perplexity is 30.320184485290124
At time: 288.5326015949249 and batch: 700, loss is 3.5131564569473266 and perplexity is 33.55401260285517
At time: 289.00122594833374 and batch: 750, loss is 3.4841097688674925 and perplexity is 32.593398532616995
At time: 289.46721720695496 and batch: 800, loss is 3.593192663192749 and perplexity is 36.34994399244371
At time: 289.92320585250854 and batch: 850, loss is 3.45256694316864 and perplexity is 31.581355895600364
At time: 290.4125146865845 and batch: 900, loss is 3.3808060979843138 and perplexity is 29.394456377721013
At time: 290.8770275115967 and batch: 950, loss is 3.4121006774902343 and perplexity is 30.328888601466236
At time: 291.3404631614685 and batch: 1000, loss is 3.347707257270813 and perplexity is 28.437459065944328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331686624666539 and perplexity of 76.07248415641371
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 292.6949632167816 and batch: 50, loss is 3.62206326007843 and perplexity is 37.414684463653174
At time: 293.1592683792114 and batch: 100, loss is 3.4953802919387815 and perplexity is 32.96282106486332
At time: 293.631117105484 and batch: 150, loss is 3.604164161682129 and perplexity is 36.75095316121621
At time: 294.0813887119293 and batch: 200, loss is 3.661478638648987 and perplexity is 38.918847258481044
At time: 294.535236120224 and batch: 250, loss is 3.6256581354141235 and perplexity is 37.54942763737166
At time: 294.99611020088196 and batch: 300, loss is 3.4524794816970825 and perplexity is 31.578593864527363
At time: 295.4540765285492 and batch: 350, loss is 3.5152119827270507 and perplexity is 33.62305467534384
At time: 295.91037678718567 and batch: 400, loss is 3.5276077365875245 and perplexity is 34.04243166216452
At time: 296.3737065792084 and batch: 450, loss is 3.5861741733551025 and perplexity is 36.09571547366174
At time: 296.8299446105957 and batch: 500, loss is 3.6080504274368286 and perplexity is 36.894055017755164
At time: 297.2877035140991 and batch: 550, loss is 3.4945530462265015 and perplexity is 32.93556398817709
At time: 297.73427629470825 and batch: 600, loss is 3.4615137815475463 and perplexity is 31.865176939867023
At time: 298.1887946128845 and batch: 650, loss is 3.4116291999816895 and perplexity is 30.314592583021888
At time: 298.6555743217468 and batch: 700, loss is 3.512936143875122 and perplexity is 33.54662102951385
At time: 299.11459469795227 and batch: 750, loss is 3.483816089630127 and perplexity is 32.583827933604674
At time: 299.57717776298523 and batch: 800, loss is 3.5928753137588503 and perplexity is 36.33841018851534
At time: 300.02754259109497 and batch: 850, loss is 3.452205009460449 and perplexity is 31.56992760661763
At time: 300.47215270996094 and batch: 900, loss is 3.3804068517684938 and perplexity is 29.38272309463641
At time: 300.9280061721802 and batch: 950, loss is 3.411644220352173 and perplexity is 30.315047922853214
At time: 301.3808944225311 and batch: 1000, loss is 3.3472498655319214 and perplexity is 28.424454981299657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.331686624666539 and perplexity of 76.07248415641371
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 302.8297381401062 and batch: 50, loss is 3.621958842277527 and perplexity is 37.410777908540496
At time: 303.3044738769531 and batch: 100, loss is 3.495264000892639 and perplexity is 32.95898800679736
At time: 303.7522990703583 and batch: 150, loss is 3.6040774726867677 and perplexity is 36.74776739609548
At time: 304.2161612510681 and batch: 200, loss is 3.661362714767456 and perplexity is 38.914335896134496
At time: 304.67137813568115 and batch: 250, loss is 3.625576572418213 and perplexity is 37.54636511845463
At time: 305.1282081604004 and batch: 300, loss is 3.4523541736602783 and perplexity is 31.57463706083999
At time: 305.5860278606415 and batch: 350, loss is 3.515103373527527 and perplexity is 33.6194031005909
At time: 306.05089831352234 and batch: 400, loss is 3.527456884384155 and perplexity is 34.037296673662134
At time: 306.50324511528015 and batch: 450, loss is 3.586081790924072 and perplexity is 36.092381017741445
At time: 306.974547624588 and batch: 500, loss is 3.6079863929748535 and perplexity is 36.89169260243083
At time: 307.4520034790039 and batch: 550, loss is 3.494461107254028 and perplexity is 32.93253606546036
At time: 307.922655582428 and batch: 600, loss is 3.461374020576477 and perplexity is 31.860723742993425
At time: 308.38894605636597 and batch: 650, loss is 3.4115288066864013 and perplexity is 30.311549353939615
At time: 308.8637444972992 and batch: 700, loss is 3.5128210401535034 and perplexity is 33.542759910804456
At time: 309.3155460357666 and batch: 750, loss is 3.4836644315719605 and perplexity is 32.57888670823043
At time: 309.79660630226135 and batch: 800, loss is 3.5927124881744383 and perplexity is 36.332493847318716
At time: 310.2654905319214 and batch: 850, loss is 3.452024369239807 and perplexity is 31.564225322975506
At time: 310.75019907951355 and batch: 900, loss is 3.3802076148986817 and perplexity is 29.3768695560002
At time: 311.23013186454773 and batch: 950, loss is 3.4114114236831665 and perplexity is 30.3079915020635
At time: 311.68515372276306 and batch: 1000, loss is 3.3470201778411863 and perplexity is 28.417926983603657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 83 batches
Done Evaluating: Achieved loss of 4.3316877411633 and perplexity of 76.07256909114328
Annealing...
Model not improving. Stopping early with 76.07138001355902loss at 30 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fe0d8ae48d0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'anneal': 5.453002407156198, 'dropout': 0.8275739045832949, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 6.0869734814206735}, 'best_accuracy': -73.4185092190347}, {'params': {'anneal': 5.878162736267521, 'dropout': 0.5378033310392732, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 29.53510895111857}, 'best_accuracy': -153.46643475491558}, {'params': {'anneal': 6.949597665762493, 'dropout': 0.6744407721634755, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 12.605229715772824}, 'best_accuracy': -115.78232987003628}, {'params': {'anneal': 5.917012875567329, 'dropout': 0.44532173091606186, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 0.44540625695229985}, 'best_accuracy': -85.18521964199958}, {'params': {'anneal': 4.977959423534917, 'dropout': 0.886815526576557, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 7.939448573385901}, 'best_accuracy': -82.9289944514116}, {'params': {'anneal': 2.0, 'dropout': 1.0, 'seq_len': 50, 'data': 'ptb', 'wordvec_dim': 200, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'batch_size': 20, 'num_layers': 1, 'lr': 4.042242204677684}, 'best_accuracy': -76.07138001355902}]
