FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2822227478027344 and batch: 50, loss is 7.07012170791626 and perplexity is 1176.2911894870213
At time: 2.1005499362945557 and batch: 100, loss is 6.264649152755737 and perplexity is 525.6571282481052
At time: 2.9113011360168457 and batch: 150, loss is 6.1326438999176025 and perplexity is 460.6524711002879
At time: 3.721235513687134 and batch: 200, loss is 5.9902542781829835 and perplexity is 399.5161852620277
At time: 4.530707597732544 and batch: 250, loss is 6.03658164024353 and perplexity is 418.46013990476996
At time: 5.340040683746338 and batch: 300, loss is 5.942310228347778 and perplexity is 380.81368064970036
At time: 6.153632402420044 and batch: 350, loss is 5.9331325340270995 and perplexity is 377.3346781261016
At time: 6.966862916946411 and batch: 400, loss is 5.803270807266236 and perplexity is 331.3816748402029
At time: 7.779453754425049 and batch: 450, loss is 5.802938203811646 and perplexity is 331.27147447788985
At time: 8.590714693069458 and batch: 500, loss is 5.7581814193725585 and perplexity is 316.77172984958963
At time: 9.400657892227173 and batch: 550, loss is 5.78850209236145 and perplexity is 326.5235557497695
At time: 10.210590839385986 and batch: 600, loss is 5.7270934104919435 and perplexity is 307.0754277762811
At time: 11.01876950263977 and batch: 650, loss is 5.629495153427124 and perplexity is 278.521471523303
At time: 11.827691793441772 and batch: 700, loss is 5.726477375030518 and perplexity is 306.8863166789901
At time: 12.639050483703613 and batch: 750, loss is 5.683851709365845 and perplexity is 294.07996182798405
At time: 13.447240829467773 and batch: 800, loss is 5.6791565895080565 and perplexity is 292.7024574639665
At time: 14.256616830825806 and batch: 850, loss is 5.717560548782348 and perplexity is 304.1620287668104
At time: 15.070472240447998 and batch: 900, loss is 5.595670976638794 and perplexity is 269.2582554937268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.493931182443279 and perplexity of 243.2114385368167
finished 1 epochs...
Completing Train Step...
At time: 16.814141988754272 and batch: 50, loss is 5.350025930404663 and perplexity is 210.61375909586917
At time: 17.46124291419983 and batch: 100, loss is 5.160442094802857 and perplexity is 174.24146982831576
At time: 18.099161386489868 and batch: 150, loss is 5.085729398727417 and perplexity is 161.69783843219014
At time: 18.738348484039307 and batch: 200, loss is 4.932891750335694 and perplexity is 138.78025047929233
At time: 19.37687611579895 and batch: 250, loss is 4.99862512588501 and perplexity is 148.2092498986661
At time: 20.019404888153076 and batch: 300, loss is 4.919127950668335 and perplexity is 136.88319222725517
At time: 20.657727241516113 and batch: 350, loss is 4.888690357208252 and perplexity is 132.77956633023913
At time: 21.32313108444214 and batch: 400, loss is 4.732454233169555 and perplexity is 113.57395755938235
At time: 21.98028802871704 and batch: 450, loss is 4.739736051559448 and perplexity is 114.40400093697426
At time: 22.62104892730713 and batch: 500, loss is 4.638090562820435 and perplexity is 103.3468247949628
At time: 23.265540838241577 and batch: 550, loss is 4.6978623485565185 and perplexity is 109.71239474607806
At time: 23.914019346237183 and batch: 600, loss is 4.635106887817383 and perplexity is 103.03893101308152
At time: 24.558850526809692 and batch: 650, loss is 4.497664737701416 and perplexity is 89.80716294857808
At time: 25.204721212387085 and batch: 700, loss is 4.550484113693237 and perplexity is 94.67823225071577
At time: 25.84984827041626 and batch: 750, loss is 4.582556667327881 and perplexity is 97.7640250482522
At time: 26.518550872802734 and batch: 800, loss is 4.517972822189331 and perplexity is 91.64961946115653
At time: 27.181052684783936 and batch: 850, loss is 4.576357412338257 and perplexity is 97.15983562526736
At time: 27.830087661743164 and batch: 900, loss is 4.501967391967773 and perplexity is 90.19440460746459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.60245252635381 and perplexity of 99.72860298596427
finished 2 epochs...
Completing Train Step...
At time: 29.532214641571045 and batch: 50, loss is 4.5549114799499515 and perplexity is 95.09833675345232
At time: 30.197466373443604 and batch: 100, loss is 4.41971755027771 and perplexity is 83.07281814993023
At time: 30.842484712600708 and batch: 150, loss is 4.4177963829040525 and perplexity is 82.9133745700053
At time: 31.49103283882141 and batch: 200, loss is 4.308548264503479 and perplexity is 74.33249944522998
At time: 32.13809537887573 and batch: 250, loss is 4.447965841293335 and perplexity is 85.45294224212975
At time: 32.78344750404358 and batch: 300, loss is 4.409804816246033 and perplexity is 82.25340740847355
At time: 33.42936372756958 and batch: 350, loss is 4.399001226425171 and perplexity is 81.36955830177901
At time: 34.07608461380005 and batch: 400, loss is 4.296464881896973 and perplexity is 73.4397161978915
At time: 34.72288990020752 and batch: 450, loss is 4.325567502975463 and perplexity is 75.60840868371955
At time: 35.365161657333374 and batch: 500, loss is 4.204417119026184 and perplexity is 66.98154401799121
At time: 36.0104022026062 and batch: 550, loss is 4.28750027179718 and perplexity is 72.78429994126442
At time: 36.65927720069885 and batch: 600, loss is 4.274302463531495 and perplexity is 71.83001777396106
At time: 37.30370211601257 and batch: 650, loss is 4.1276939535140995 and perplexity is 62.03470295351227
At time: 37.94835186004639 and batch: 700, loss is 4.155476961135864 and perplexity is 63.78237900991884
At time: 38.592787981033325 and batch: 750, loss is 4.238061227798462 and perplexity is 69.27341618720395
At time: 39.236082315444946 and batch: 800, loss is 4.187553358078003 and perplexity is 65.86145427926364
At time: 39.880776166915894 and batch: 850, loss is 4.265545148849487 and perplexity is 71.20372602421823
At time: 40.52936029434204 and batch: 900, loss is 4.202099614143371 and perplexity is 66.8264936970049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423457733572346 and perplexity of 83.38410749332921
finished 3 epochs...
Completing Train Step...
At time: 42.195242166519165 and batch: 50, loss is 4.280655612945557 and perplexity is 72.28781730387435
At time: 42.842838525772095 and batch: 100, loss is 4.152605042457581 and perplexity is 63.599463988550404
At time: 43.48948931694031 and batch: 150, loss is 4.154800925254822 and perplexity is 63.73927440490135
At time: 44.13566780090332 and batch: 200, loss is 4.046199517250061 and perplexity is 57.179732990373516
At time: 44.782169818878174 and batch: 250, loss is 4.201270489692688 and perplexity is 66.77110918062849
At time: 45.429391384124756 and batch: 300, loss is 4.1696285104751585 and perplexity is 64.69141545980759
At time: 46.07772970199585 and batch: 350, loss is 4.160626516342163 and perplexity is 64.11167703330368
At time: 46.74012064933777 and batch: 400, loss is 4.077022490501403 and perplexity is 58.9696255705245
At time: 47.41086006164551 and batch: 450, loss is 4.1171167373657225 and perplexity is 61.38200643292138
At time: 48.058881759643555 and batch: 500, loss is 3.98496298789978 and perplexity is 53.78329880820489
At time: 48.71890211105347 and batch: 550, loss is 4.070519366264343 and perplexity is 58.58738300178853
At time: 49.393338441848755 and batch: 600, loss is 4.077411718368531 and perplexity is 58.99258265959018
At time: 50.0814425945282 and batch: 650, loss is 3.9254928493499754 and perplexity is 50.67804854827349
At time: 50.77645778656006 and batch: 700, loss is 3.938777174949646 and perplexity is 51.35576377428314
At time: 51.475786209106445 and batch: 750, loss is 4.039509735107422 and perplexity is 56.79848967288231
At time: 52.17330551147461 and batch: 800, loss is 3.9932115268707276 and perplexity is 54.22876715015688
At time: 52.871878147125244 and batch: 850, loss is 4.076433992385864 and perplexity is 58.934932266475485
At time: 53.56999754905701 and batch: 900, loss is 4.024391059875488 and perplexity is 55.9462305023133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356305266079837 and perplexity of 77.96852862317027
finished 4 epochs...
Completing Train Step...
At time: 55.33742380142212 and batch: 50, loss is 4.107576251029968 and perplexity is 60.79917689868866
At time: 56.0279324054718 and batch: 100, loss is 3.982128162384033 and perplexity is 53.63104844399095
At time: 56.717461824417114 and batch: 150, loss is 3.9872682762145994 and perplexity is 53.90742784011114
At time: 57.40147590637207 and batch: 200, loss is 3.8836380863189697 and perplexity is 48.600707398196775
At time: 58.08877348899841 and batch: 250, loss is 4.037377681732178 and perplexity is 56.677521262643495
At time: 58.777350425720215 and batch: 300, loss is 4.010437407493591 and perplexity is 55.17099749416185
At time: 59.47030067443848 and batch: 350, loss is 4.003136062622071 and perplexity is 54.76964201494408
At time: 60.15717887878418 and batch: 400, loss is 3.9268059492111207 and perplexity is 50.744637596252254
At time: 60.844945430755615 and batch: 450, loss is 3.968331022262573 and perplexity is 52.89617459009309
At time: 61.53130602836609 and batch: 500, loss is 3.8396104764938355 and perplexity is 46.50735520302287
At time: 62.21582841873169 and batch: 550, loss is 3.9217691135406496 and perplexity is 50.48968780462468
At time: 62.89970660209656 and batch: 600, loss is 3.936441798210144 and perplexity is 51.2359686559385
At time: 63.583715200424194 and batch: 650, loss is 3.7856078720092774 and perplexity is 44.06244674771574
At time: 64.2683002948761 and batch: 700, loss is 3.7933194255828857 and perplexity is 44.403550194753024
At time: 64.95122623443604 and batch: 750, loss is 3.9042941999435423 and perplexity is 49.61504924674904
At time: 65.63313364982605 and batch: 800, loss is 3.8517657995223997 and perplexity is 47.07611686664384
At time: 66.32251262664795 and batch: 850, loss is 3.938885178565979 and perplexity is 51.36131068202796
At time: 67.00721287727356 and batch: 900, loss is 3.894262204170227 and perplexity is 49.11979960708418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327815643728596 and perplexity of 75.77857823944582
finished 5 epochs...
Completing Train Step...
At time: 68.71156597137451 and batch: 50, loss is 3.978847327232361 and perplexity is 53.45538213870042
At time: 69.38453841209412 and batch: 100, loss is 3.8534056520462037 and perplexity is 47.153378086888296
At time: 70.04827189445496 and batch: 150, loss is 3.8639674043655394 and perplexity is 47.65403966497029
At time: 70.71146178245544 and batch: 200, loss is 3.764910659790039 and perplexity is 43.15984977756997
At time: 71.37076091766357 and batch: 250, loss is 3.9133790826797483 and perplexity is 50.06784985700624
At time: 72.03346753120422 and batch: 300, loss is 3.892744083404541 and perplexity is 49.045286393633496
At time: 72.69307446479797 and batch: 350, loss is 3.8835204887390136 and perplexity is 48.59499240866364
At time: 73.35548615455627 and batch: 400, loss is 3.8123340129852297 and perplexity is 45.255943653797715
At time: 74.02022576332092 and batch: 450, loss is 3.856013674736023 and perplexity is 47.27651566984945
At time: 74.6842725276947 and batch: 500, loss is 3.7293222188949584 and perplexity is 41.65086842330315
At time: 75.38979244232178 and batch: 550, loss is 3.805936403274536 and perplexity is 44.96733796685528
At time: 76.09446668624878 and batch: 600, loss is 3.8288368463516234 and perplexity is 46.00899157234847
At time: 76.75827312469482 and batch: 650, loss is 3.6756922006607056 and perplexity is 39.47597269647134
At time: 77.42210412025452 and batch: 700, loss is 3.683721671104431 and perplexity is 39.79421982056897
At time: 78.08452868461609 and batch: 750, loss is 3.797864766120911 and perplexity is 44.60583883859656
At time: 78.78014969825745 and batch: 800, loss is 3.7483778381347657 and perplexity is 42.45216183731675
At time: 79.44174933433533 and batch: 850, loss is 3.829954957962036 and perplexity is 46.06046353034342
At time: 80.10541844367981 and batch: 900, loss is 3.7901079273223877 and perplexity is 44.26117700852901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3189634558272685 and perplexity of 75.11073233578551
finished 6 epochs...
Completing Train Step...
At time: 81.91480922698975 and batch: 50, loss is 3.8760468912124635 and perplexity is 48.233166747414884
At time: 82.5780713558197 and batch: 100, loss is 3.753053517341614 and perplexity is 42.65111929586444
At time: 83.25946092605591 and batch: 150, loss is 3.7658037185668944 and perplexity is 43.19841127650122
At time: 83.93850588798523 and batch: 200, loss is 3.670508508682251 and perplexity is 39.27187087098978
At time: 84.62052941322327 and batch: 250, loss is 3.81675181388855 and perplexity is 45.45631768294459
At time: 85.28420114517212 and batch: 300, loss is 3.7957598257064817 and perplexity is 44.512044955604786
At time: 85.96072220802307 and batch: 350, loss is 3.7900300788879395 and perplexity is 44.25773147930833
At time: 86.64600467681885 and batch: 400, loss is 3.722107663154602 and perplexity is 41.351457269348856
At time: 87.30723834037781 and batch: 450, loss is 3.761601004600525 and perplexity is 43.017241678767355
At time: 87.97202396392822 and batch: 500, loss is 3.640073046684265 and perplexity is 38.0946193093974
At time: 88.63638734817505 and batch: 550, loss is 3.7156217288970947 and perplexity is 41.084122331634994
At time: 89.31154823303223 and batch: 600, loss is 3.7395437240600584 and perplexity is 42.07878624644088
At time: 89.98363184928894 and batch: 650, loss is 3.591576919555664 and perplexity is 36.29125922426692
At time: 90.65673232078552 and batch: 700, loss is 3.59422975063324 and perplexity is 36.387661617677935
At time: 91.33156728744507 and batch: 750, loss is 3.707901520729065 and perplexity is 40.76816555018935
At time: 92.00865268707275 and batch: 800, loss is 3.6652392387390136 and perplexity is 39.06548102164727
At time: 92.68383193016052 and batch: 850, loss is 3.7449200963974 and perplexity is 42.305626711797736
At time: 93.36745357513428 and batch: 900, loss is 3.704650573730469 and perplexity is 40.635845603858876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322271059637201 and perplexity of 75.35958019842414
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 95.08081817626953 and batch: 50, loss is 3.8170038747787474 and perplexity is 45.467776886992766
At time: 95.75540018081665 and batch: 100, loss is 3.692708854675293 and perplexity is 40.15346968335621
At time: 96.43268394470215 and batch: 150, loss is 3.7052361488342287 and perplexity is 40.65964791170401
At time: 97.10166358947754 and batch: 200, loss is 3.5978667306900025 and perplexity is 36.52024377047091
At time: 97.77579712867737 and batch: 250, loss is 3.7306798315048217 and perplexity is 41.707452568469975
At time: 98.45508408546448 and batch: 300, loss is 3.7073756408691407 and perplexity is 40.74673202922291
At time: 99.12869882583618 and batch: 350, loss is 3.6799936723709106 and perplexity is 39.64614320568987
At time: 99.8051130771637 and batch: 400, loss is 3.609487795829773 and perplexity is 36.94712349666561
At time: 100.48048281669617 and batch: 450, loss is 3.6322994470596313 and perplexity is 37.79963502137061
At time: 101.15941905975342 and batch: 500, loss is 3.504552664756775 and perplexity is 33.2665592190819
At time: 101.83491206169128 and batch: 550, loss is 3.5633263111114504 and perplexity is 35.280355623234826
At time: 102.50987815856934 and batch: 600, loss is 3.5780089569091795 and perplexity is 35.8021861401364
At time: 103.1837112903595 and batch: 650, loss is 3.417923011779785 and perplexity is 30.50598859692168
At time: 103.86165452003479 and batch: 700, loss is 3.408629884719849 and perplexity is 30.223805780012082
At time: 104.53934097290039 and batch: 750, loss is 3.5049904441833495 and perplexity is 33.281125822547956
At time: 105.2117714881897 and batch: 800, loss is 3.4508050012588503 and perplexity is 31.525760373502848
At time: 105.88159394264221 and batch: 850, loss is 3.5135947704315185 and perplexity is 33.56872300267553
At time: 106.55295419692993 and batch: 900, loss is 3.468312029838562 and perplexity is 32.082542339370015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279927710964255 and perplexity of 72.23521800432451
finished 8 epochs...
Completing Train Step...
At time: 108.2698860168457 and batch: 50, loss is 3.725965166091919 and perplexity is 41.51127869485396
At time: 108.93754124641418 and batch: 100, loss is 3.5969536304473877 and perplexity is 36.486912346805106
At time: 109.59597253799438 and batch: 150, loss is 3.6075038003921507 and perplexity is 36.87389324048302
At time: 110.26545405387878 and batch: 200, loss is 3.506173086166382 and perplexity is 33.3205087625572
At time: 110.92880153656006 and batch: 250, loss is 3.6405132102966307 and perplexity is 38.11139086548789
At time: 111.5962884426117 and batch: 300, loss is 3.6230990648269654 and perplexity is 37.45345884936527
At time: 112.26507925987244 and batch: 350, loss is 3.5982925701141357 and perplexity is 36.53579884179358
At time: 112.93239974975586 and batch: 400, loss is 3.5326403522491456 and perplexity is 34.21418596108979
At time: 113.60146450996399 and batch: 450, loss is 3.561292643547058 and perplexity is 35.208680015226236
At time: 114.27214670181274 and batch: 500, loss is 3.437267680168152 and perplexity is 31.101861732292168
At time: 114.9428653717041 and batch: 550, loss is 3.4989897918701174 and perplexity is 33.08201535170857
At time: 115.61140894889832 and batch: 600, loss is 3.5198327255249025 and perplexity is 33.778777663942265
At time: 116.28208518028259 and batch: 650, loss is 3.3649042797088624 and perplexity is 28.930727908810603
At time: 116.95439195632935 and batch: 700, loss is 3.3600078678131102 and perplexity is 28.789417388107175
At time: 117.62464618682861 and batch: 750, loss is 3.462480163574219 and perplexity is 31.89598575828496
At time: 118.29344010353088 and batch: 800, loss is 3.4129998445510865 and perplexity is 30.356171603238995
At time: 118.96591782569885 and batch: 850, loss is 3.4830791664123537 and perplexity is 32.559824999522434
At time: 119.63894701004028 and batch: 900, loss is 3.445908951759338 and perplexity is 31.371785931030942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.288488100652826 and perplexity of 72.85623389635317
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 121.35089945793152 and batch: 50, loss is 3.6975693607330324 and perplexity is 40.34911093858378
At time: 122.02241206169128 and batch: 100, loss is 3.5785034990310667 and perplexity is 35.81989620806468
At time: 122.68065404891968 and batch: 150, loss is 3.5948882055282594 and perplexity is 36.41162914148914
At time: 123.34239912033081 and batch: 200, loss is 3.4881517601013186 and perplexity is 32.72540737332829
At time: 124.00355958938599 and batch: 250, loss is 3.6176412534713744 and perplexity is 37.249601749185196
At time: 124.68347573280334 and batch: 300, loss is 3.598967123031616 and perplexity is 36.560452485653656
At time: 125.34668493270874 and batch: 350, loss is 3.573376841545105 and perplexity is 35.636729785952014
At time: 126.00861144065857 and batch: 400, loss is 3.5087966775894164 and perplexity is 33.40804294030694
At time: 126.66989088058472 and batch: 450, loss is 3.527633352279663 and perplexity is 34.04330369378242
At time: 127.34094452857971 and batch: 500, loss is 3.397195129394531 and perplexity is 29.880172382099094
At time: 128.00742745399475 and batch: 550, loss is 3.453414645195007 and perplexity is 31.608138825356548
At time: 128.70611310005188 and batch: 600, loss is 3.4732519578933716 and perplexity is 32.24141988908531
At time: 129.386944770813 and batch: 650, loss is 3.312149567604065 and perplexity is 27.444054964223678
At time: 130.051415681839 and batch: 700, loss is 3.304166145324707 and perplexity is 27.225829735510867
At time: 130.74886059761047 and batch: 750, loss is 3.401281352043152 and perplexity is 30.00251921719823
At time: 131.41721510887146 and batch: 800, loss is 3.347658085823059 and perplexity is 28.436060789289538
At time: 132.08966040611267 and batch: 850, loss is 3.416782956123352 and perplexity is 30.471229889262048
At time: 132.76295065879822 and batch: 900, loss is 3.3778513717651366 and perplexity is 29.30773199326452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274365986863228 and perplexity of 71.83458080093634
finished 10 epochs...
Completing Train Step...
At time: 134.4967405796051 and batch: 50, loss is 3.6695707750320437 and perplexity is 39.235061577521364
At time: 135.1640145778656 and batch: 100, loss is 3.543113446235657 and perplexity is 34.57439731753096
At time: 135.8161838054657 and batch: 150, loss is 3.5573130512237547 and perplexity is 35.06884225571377
At time: 136.4733498096466 and batch: 200, loss is 3.4531676054000853 and perplexity is 31.600331321645033
At time: 137.15774607658386 and batch: 250, loss is 3.5836285734176636 and perplexity is 36.00394707498069
At time: 137.84690046310425 and batch: 300, loss is 3.5662946319580078 and perplexity is 35.38523461855255
At time: 138.50849676132202 and batch: 350, loss is 3.5421272230148317 and perplexity is 34.54031605268016
At time: 139.1783754825592 and batch: 400, loss is 3.4796564626693725 and perplexity is 32.4485728647837
At time: 139.83784770965576 and batch: 450, loss is 3.501698126792908 and perplexity is 33.17173396839221
At time: 140.49843621253967 and batch: 500, loss is 3.3730522441864013 and perplexity is 29.167417411481
At time: 141.15601873397827 and batch: 550, loss is 3.4325519037246703 and perplexity is 30.95553759252192
At time: 141.81204390525818 and batch: 600, loss is 3.455203819274902 and perplexity is 31.664741909342403
At time: 142.47156310081482 and batch: 650, loss is 3.2969604253768923 and perplexity is 27.030353151650104
At time: 143.13190460205078 and batch: 700, loss is 3.2925892305374145 and perplexity is 26.91245607471643
At time: 143.8161735534668 and batch: 750, loss is 3.3925211668014525 and perplexity is 29.740839446230204
At time: 144.48374700546265 and batch: 800, loss is 3.3421950912475586 and perplexity is 28.28113829910684
At time: 145.15716695785522 and batch: 850, loss is 3.414585633277893 and perplexity is 30.404348266865526
At time: 145.827791929245 and batch: 900, loss is 3.3780296516418455 and perplexity is 29.31295743789287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.276462084626498 and perplexity of 71.98531102253112
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 147.52190518379211 and batch: 50, loss is 3.661523685455322 and perplexity is 38.92060046774423
At time: 148.23728013038635 and batch: 100, loss is 3.539789056777954 and perplexity is 34.45964939465335
At time: 148.91240453720093 and batch: 150, loss is 3.5576762771606445 and perplexity is 35.08158248244806
At time: 149.58251452445984 and batch: 200, loss is 3.452647271156311 and perplexity is 31.583892864260804
At time: 150.2537717819214 and batch: 250, loss is 3.585135746002197 and perplexity is 36.05825215022758
At time: 150.9231779575348 and batch: 300, loss is 3.5626343631744386 and perplexity is 35.255951897971656
At time: 151.590891122818 and batch: 350, loss is 3.5369190549850464 and perplexity is 34.36089192395473
At time: 152.2616217136383 and batch: 400, loss is 3.4759875297546388 and perplexity is 32.32973935714986
At time: 152.92914724349976 and batch: 450, loss is 3.4978278398513796 and perplexity is 33.043597961091415
At time: 153.60442399978638 and batch: 500, loss is 3.364995675086975 and perplexity is 28.933372164461435
At time: 154.27931547164917 and batch: 550, loss is 3.4189184141159057 and perplexity is 30.536369447312936
At time: 154.95044875144958 and batch: 600, loss is 3.4428951358795166 and perplexity is 31.277379477706212
At time: 155.61886548995972 and batch: 650, loss is 3.281698703765869 and perplexity is 26.620955430212287
At time: 156.28824830055237 and batch: 700, loss is 3.276181526184082 and perplexity is 26.474487308557023
At time: 156.9584538936615 and batch: 750, loss is 3.371945605278015 and perplexity is 29.135157465864076
At time: 157.6283872127533 and batch: 800, loss is 3.3199969863891603 and perplexity is 27.66026720111008
At time: 158.29846858978271 and batch: 850, loss is 3.3901042890548707 and perplexity is 29.669046265825095
At time: 158.9678955078125 and batch: 900, loss is 3.354166603088379 and perplexity is 28.62174097778772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272103923640839 and perplexity of 71.67227008537198
finished 12 epochs...
Completing Train Step...
At time: 160.66286754608154 and batch: 50, loss is 3.6509527158737183 and perplexity is 38.511338940762194
At time: 161.33045387268066 and batch: 100, loss is 3.527541241645813 and perplexity is 34.040168087914374
At time: 161.99108695983887 and batch: 150, loss is 3.543571515083313 and perplexity is 34.59023839974475
At time: 162.64868927001953 and batch: 200, loss is 3.439493350982666 and perplexity is 31.17116132866441
At time: 163.31262016296387 and batch: 250, loss is 3.5715059518814085 and perplexity is 35.570119726013054
At time: 163.9777090549469 and batch: 300, loss is 3.550500693321228 and perplexity is 34.830752647627705
At time: 164.6404356956482 and batch: 350, loss is 3.525820574760437 and perplexity is 33.98164666029284
At time: 165.3016767501831 and batch: 400, loss is 3.465630488395691 and perplexity is 31.99662691685976
At time: 165.96413612365723 and batch: 450, loss is 3.4879253911972046 and perplexity is 32.718000197133314
At time: 166.62811136245728 and batch: 500, loss is 3.356585364341736 and perplexity is 28.691053927814945
At time: 167.29237294197083 and batch: 550, loss is 3.412452178001404 and perplexity is 30.339551095139065
At time: 167.95448184013367 and batch: 600, loss is 3.4374283075332643 and perplexity is 31.10685794364571
At time: 168.61406326293945 and batch: 650, loss is 3.277530846595764 and perplexity is 26.51023398610856
At time: 169.28044152259827 and batch: 700, loss is 3.273778166770935 and perplexity is 26.410935999184645
At time: 169.9470603466034 and batch: 750, loss is 3.370587611198425 and perplexity is 29.095618947130934
At time: 170.61265683174133 and batch: 800, loss is 3.3204536867141723 and perplexity is 27.672902539192634
At time: 171.2787585258484 and batch: 850, loss is 3.392378649711609 and perplexity is 29.73660117036336
At time: 171.95102977752686 and batch: 900, loss is 3.35779851436615 and perplexity is 28.72588160191336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271880685466609 and perplexity of 71.65627188442625
finished 13 epochs...
Completing Train Step...
At time: 173.66254305839539 and batch: 50, loss is 3.6450230932235717 and perplexity is 38.2836569343985
At time: 174.35850286483765 and batch: 100, loss is 3.5210906362533567 and perplexity is 33.821295086714514
At time: 175.02657890319824 and batch: 150, loss is 3.5363557529449463 and perplexity is 34.3415418139242
At time: 175.69374775886536 and batch: 200, loss is 3.4326292181015017 and perplexity is 30.95793099314131
At time: 176.38044929504395 and batch: 250, loss is 3.5645122289657594 and perplexity is 35.3222200458514
At time: 177.05751872062683 and batch: 300, loss is 3.543773651123047 and perplexity is 34.59723104025685
At time: 177.7312788963318 and batch: 350, loss is 3.519379563331604 and perplexity is 33.763473866782164
At time: 178.3949098587036 and batch: 400, loss is 3.459618802070618 and perplexity is 31.80485026049943
At time: 179.05971717834473 and batch: 450, loss is 3.482286901473999 and perplexity is 32.53403920771451
At time: 179.72300696372986 and batch: 500, loss is 3.3515217971801756 and perplexity is 28.546142044467214
At time: 180.43400073051453 and batch: 550, loss is 3.4081753587722776 and perplexity is 30.210071397603755
At time: 181.1067476272583 and batch: 600, loss is 3.4336996126174926 and perplexity is 30.991085933970624
At time: 181.80552649497986 and batch: 650, loss is 3.2744578981399535 and perplexity is 26.428894443635997
At time: 182.46801924705505 and batch: 700, loss is 3.271691517829895 and perplexity is 26.35588310555667
At time: 183.13203597068787 and batch: 750, loss is 3.369025239944458 and perplexity is 29.050196281245018
At time: 183.79423713684082 and batch: 800, loss is 3.3196749114990234 and perplexity is 27.651359958066784
At time: 184.45604252815247 and batch: 850, loss is 3.392438154220581 and perplexity is 29.73837068486103
At time: 185.12152576446533 and batch: 900, loss is 3.358317050933838 and perplexity is 28.740780884540627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27237440135381 and perplexity of 71.69165845901139
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 186.8036847114563 and batch: 50, loss is 3.6427495956420897 and perplexity is 38.19671799812597
At time: 187.47110390663147 and batch: 100, loss is 3.5208140897750853 and perplexity is 33.81194321984023
At time: 188.12802386283875 and batch: 150, loss is 3.537364501953125 and perplexity is 34.37620128858462
At time: 188.7849292755127 and batch: 200, loss is 3.432770390510559 and perplexity is 30.962301707343915
At time: 189.44194912910461 and batch: 250, loss is 3.565921206474304 and perplexity is 35.37202333706727
At time: 190.10048174858093 and batch: 300, loss is 3.543728775978088 and perplexity is 34.59567851933381
At time: 190.75720143318176 and batch: 350, loss is 3.518231086730957 and perplexity is 33.72471956555342
At time: 191.41600036621094 and batch: 400, loss is 3.4581355333328245 and perplexity is 31.757710089811276
At time: 192.077534198761 and batch: 450, loss is 3.481050214767456 and perplexity is 32.49382966234597
At time: 192.74185752868652 and batch: 500, loss is 3.3478760623931887 and perplexity is 28.442259859889916
At time: 193.411518573761 and batch: 550, loss is 3.4026850271224975 and perplexity is 30.044662576613085
At time: 194.08048605918884 and batch: 600, loss is 3.429439787864685 and perplexity is 30.85935012393254
At time: 194.76222610473633 and batch: 650, loss is 3.2691362571716307 and perplexity is 26.288622924587518
At time: 195.43017673492432 and batch: 700, loss is 3.2657234859466553 and perplexity is 26.19905878661219
At time: 196.0990927219391 and batch: 750, loss is 3.361370129585266 and perplexity is 28.82866283607521
At time: 196.76723217964172 and batch: 800, loss is 3.3108416748046876 and perplexity is 27.408184544795176
At time: 197.43469166755676 and batch: 850, loss is 3.3819678020477295 and perplexity is 29.428623879557186
At time: 198.10228991508484 and batch: 900, loss is 3.3472649478912353 and perplexity is 28.424883692375968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270801282908819 and perplexity of 71.57896764997233
finished 15 epochs...
Completing Train Step...
At time: 199.7885456085205 and batch: 50, loss is 3.6395825576782226 and perplexity is 38.075938899079226
At time: 200.450532913208 and batch: 100, loss is 3.5179869413375853 and perplexity is 33.716486835661165
At time: 201.10624051094055 and batch: 150, loss is 3.5340856313705444 and perplexity is 34.263670760775135
At time: 201.76289796829224 and batch: 200, loss is 3.429686632156372 and perplexity is 30.866968518595733
At time: 202.4267613887787 and batch: 250, loss is 3.56252827167511 and perplexity is 35.252211739577554
At time: 203.08967518806458 and batch: 300, loss is 3.5404913330078127 and perplexity is 34.48385808691208
At time: 203.75273060798645 and batch: 350, loss is 3.5152200651168823 and perplexity is 33.62332643107727
At time: 204.41312050819397 and batch: 400, loss is 3.455414934158325 and perplexity is 31.671427513329633
At time: 205.0764398574829 and batch: 450, loss is 3.4784176111221314 and perplexity is 32.40839879007981
At time: 205.73870706558228 and batch: 500, loss is 3.3458792924880982 and perplexity is 28.38552387457389
At time: 206.4016876220703 and batch: 550, loss is 3.4010361862182616 and perplexity is 29.995164526421814
At time: 207.0650177001953 and batch: 600, loss is 3.4281672477722167 and perplexity is 30.82010533924923
At time: 207.72965693473816 and batch: 650, loss is 3.268262572288513 and perplexity is 26.265664982604118
At time: 208.3950161933899 and batch: 700, loss is 3.265227060317993 and perplexity is 26.186056130076693
At time: 209.0581657886505 and batch: 750, loss is 3.3614394521713256 and perplexity is 28.830661382807058
At time: 209.72157406806946 and batch: 800, loss is 3.311346688270569 and perplexity is 27.4220295427266
At time: 210.401300907135 and batch: 850, loss is 3.383145089149475 and perplexity is 29.463290220987055
At time: 211.0727071762085 and batch: 900, loss is 3.348853669166565 and perplexity is 28.47007880155639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270448031490797 and perplexity of 71.55368674367031
finished 16 epochs...
Completing Train Step...
At time: 212.75094509124756 and batch: 50, loss is 3.6377179527282717 and perplexity is 38.00500846410468
At time: 213.41088438034058 and batch: 100, loss is 3.516046543121338 and perplexity is 33.651126857447174
At time: 214.07075667381287 and batch: 150, loss is 3.5318123388290403 and perplexity is 34.18586788144894
At time: 214.728342294693 and batch: 200, loss is 3.4275112390518188 and perplexity is 30.79989371160993
At time: 215.3794400691986 and batch: 250, loss is 3.5602367973327635 and perplexity is 35.171524682332404
At time: 216.03130316734314 and batch: 300, loss is 3.5383291339874265 and perplexity is 34.40937767249394
At time: 216.6862826347351 and batch: 350, loss is 3.513264284133911 and perplexity is 33.557630832701015
At time: 217.34337329864502 and batch: 400, loss is 3.453568525314331 and perplexity is 31.613003063775793
At time: 217.99480676651 and batch: 450, loss is 3.4766312074661254 and perplexity is 32.35055598867437
At time: 218.65444922447205 and batch: 500, loss is 3.344453773498535 and perplexity is 28.345088598736158
At time: 219.305819272995 and batch: 550, loss is 3.3998995351791383 and perplexity is 29.961089860664938
At time: 219.95813965797424 and batch: 600, loss is 3.4272329711914065 and perplexity is 30.79132428343939
At time: 220.6097173690796 and batch: 650, loss is 3.267616367340088 and perplexity is 26.248697462756773
At time: 221.26413893699646 and batch: 700, loss is 3.2648989486694338 and perplexity is 26.17746558943588
At time: 221.92066621780396 and batch: 750, loss is 3.361333999633789 and perplexity is 28.82762127670166
At time: 222.5758740901947 and batch: 800, loss is 3.3115524768829347 and perplexity is 27.427673264821408
At time: 223.22918486595154 and batch: 850, loss is 3.3837213277816773 and perplexity is 29.48027299964081
At time: 223.88873672485352 and batch: 900, loss is 3.3496039295196534 and perplexity is 28.4914467877046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270374454864084 and perplexity of 71.54842225844475
finished 17 epochs...
Completing Train Step...
At time: 225.5553102493286 and batch: 50, loss is 3.6361513471603395 and perplexity is 37.945516218844254
At time: 226.22321772575378 and batch: 100, loss is 3.5143786811828615 and perplexity is 33.595048202501665
At time: 226.88310360908508 and batch: 150, loss is 3.5299237394332885 and perplexity is 34.12136540087003
At time: 227.55216670036316 and batch: 200, loss is 3.425705304145813 and perplexity is 30.744321303634216
At time: 228.21124124526978 and batch: 250, loss is 3.558381452560425 and perplexity is 35.106329876001105
At time: 228.8679416179657 and batch: 300, loss is 3.5365679454803467 and perplexity is 34.34882960593144
At time: 229.5233030319214 and batch: 350, loss is 3.5116483402252197 and perplexity is 33.50344737407242
At time: 230.1791274547577 and batch: 400, loss is 3.452046580314636 and perplexity is 31.564926406131956
At time: 230.83776354789734 and batch: 450, loss is 3.4751709508895874 and perplexity is 32.30335035109685
At time: 231.4947452545166 and batch: 500, loss is 3.343225836753845 and perplexity is 28.31030398393838
At time: 232.15216326713562 and batch: 550, loss is 3.3988980770111086 and perplexity is 29.931100101751852
At time: 232.80688190460205 and batch: 600, loss is 3.4263776969909667 and perplexity is 30.76500051680626
At time: 233.4608268737793 and batch: 650, loss is 3.2669805335998534 and perplexity is 26.232012960119707
At time: 234.11471724510193 and batch: 700, loss is 3.264522018432617 and perplexity is 26.16760037049845
At time: 234.79753589630127 and batch: 750, loss is 3.361095371246338 and perplexity is 28.82074300862999
At time: 235.4608564376831 and batch: 800, loss is 3.311543083190918 and perplexity is 27.42741561891615
At time: 236.1108913421631 and batch: 850, loss is 3.3839805221557615 and perplexity is 29.487915110902122
At time: 236.76238584518433 and batch: 900, loss is 3.349960279464722 and perplexity is 28.50160152241444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270422112451841 and perplexity of 71.5518321649107
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 238.39770007133484 and batch: 50, loss is 3.635424742698669 and perplexity is 37.91795485177799
At time: 239.05534410476685 and batch: 100, loss is 3.5142460250854493 and perplexity is 33.59059191009846
At time: 239.70516538619995 and batch: 150, loss is 3.5302570724487303 and perplexity is 34.132741074328536
At time: 240.35537695884705 and batch: 200, loss is 3.425790853500366 and perplexity is 30.746951572985225
At time: 241.00187802314758 and batch: 250, loss is 3.558703260421753 and perplexity is 35.11762918694361
At time: 241.64820885658264 and batch: 300, loss is 3.536569871902466 and perplexity is 34.3488957763403
At time: 242.3011817932129 and batch: 350, loss is 3.511178159713745 and perplexity is 33.48769840876833
At time: 242.95637202262878 and batch: 400, loss is 3.451386041641235 and perplexity is 31.544083436069386
At time: 243.6127679347992 and batch: 450, loss is 3.474528832435608 and perplexity is 32.28261443188172
At time: 244.3090455532074 and batch: 500, loss is 3.341866960525513 and perplexity is 28.2718599111218
At time: 244.95867824554443 and batch: 550, loss is 3.396810941696167 and perplexity is 29.868694992322048
At time: 245.60742449760437 and batch: 600, loss is 3.424720244407654 and perplexity is 30.714051221908058
At time: 246.258220911026 and batch: 650, loss is 3.2651083707809447 and perplexity is 26.18294830363438
At time: 246.9084734916687 and batch: 700, loss is 3.2621939277648924 and perplexity is 26.106750683557095
At time: 247.55772256851196 and batch: 750, loss is 3.358385081291199 and perplexity is 28.742736196644564
At time: 248.20694637298584 and batch: 800, loss is 3.308804144859314 and perplexity is 27.352396402342762
At time: 248.8541808128357 and batch: 850, loss is 3.380636682510376 and perplexity is 29.38947692377283
At time: 249.50444912910461 and batch: 900, loss is 3.3464525747299194 and perplexity is 28.40180145671456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2697206262039815 and perplexity of 71.50165713924068
finished 19 epochs...
Completing Train Step...
At time: 251.1678774356842 and batch: 50, loss is 3.6346403551101685 and perplexity is 37.88822414033581
At time: 251.81458568572998 and batch: 100, loss is 3.5135431814193727 and perplexity is 33.5669912700864
At time: 252.48287653923035 and batch: 150, loss is 3.529414691925049 and perplexity is 34.10400042500881
At time: 253.1541793346405 and batch: 200, loss is 3.425120038986206 and perplexity is 30.72633298800011
At time: 253.80549573898315 and batch: 250, loss is 3.558005075454712 and perplexity is 35.09311914343477
At time: 254.46349835395813 and batch: 300, loss is 3.5358556365966796 and perplexity is 34.32437134141326
At time: 255.14268851280212 and batch: 350, loss is 3.510594954490662 and perplexity is 33.46817390209784
At time: 255.79963207244873 and batch: 400, loss is 3.450885033607483 and perplexity is 31.5282835551147
At time: 256.4545018672943 and batch: 450, loss is 3.474035611152649 and perplexity is 32.26669588538004
At time: 257.111704826355 and batch: 500, loss is 3.34148699760437 and perplexity is 28.261119693215328
At time: 257.76651525497437 and batch: 550, loss is 3.396533794403076 and perplexity is 29.860418111367316
At time: 258.42143297195435 and batch: 600, loss is 3.424505562782288 and perplexity is 30.707458187197187
At time: 259.0754642486572 and batch: 650, loss is 3.264939785003662 and perplexity is 26.17853460299709
At time: 259.72909355163574 and batch: 700, loss is 3.2621912240982054 and perplexity is 26.106680099700387
At time: 260.4013795852661 and batch: 750, loss is 3.358535966873169 and perplexity is 28.747073388324626
At time: 261.0600161552429 and batch: 800, loss is 3.308926830291748 and perplexity is 27.35575234878214
At time: 261.71899700164795 and batch: 850, loss is 3.3809215354919435 and perplexity is 29.397849796363516
At time: 262.37616634368896 and batch: 900, loss is 3.346723532676697 and perplexity is 28.409498193220923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269405835295377 and perplexity of 71.47915260991952
Finished Training.
Improved accuracyfrom -10000000 to -71.47915260991952
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
274.4769277572632


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0428979396820068 and batch: 50, loss is 6.984161996841431 and perplexity is 1079.4014967584833
At time: 1.858947515487671 and batch: 100, loss is 6.092958211898804 and perplexity is 442.72916256603565
At time: 2.678260326385498 and batch: 150, loss is 5.863878631591797 and perplexity is 352.087115287938
At time: 3.4968152046203613 and batch: 200, loss is 5.6409461975097654 and perplexity is 281.72916383368175
At time: 4.314478158950806 and batch: 250, loss is 5.643592233657837 and perplexity is 282.47561652002264
At time: 5.130663633346558 and batch: 300, loss is 5.510912246704102 and perplexity is 247.3766927388334
At time: 5.94746994972229 and batch: 350, loss is 5.455002870559692 and perplexity is 233.92554256573985
At time: 6.762368202209473 and batch: 400, loss is 5.274422111511231 and perplexity is 195.27759522489896
At time: 7.576231479644775 and batch: 450, loss is 5.252600011825561 and perplexity is 191.0623876886249
At time: 8.392114162445068 and batch: 500, loss is 5.167997436523438 and perplexity is 175.56290935235606
At time: 9.204002380371094 and batch: 550, loss is 5.208954782485962 and perplexity is 182.9027850558081
At time: 10.018479585647583 and batch: 600, loss is 5.111455135345459 and perplexity is 165.91160314467552
At time: 10.832985639572144 and batch: 650, loss is 4.99127667427063 and perplexity is 147.12413324359719
At time: 11.648948192596436 and batch: 700, loss is 5.067020225524902 and perplexity is 158.7007297030935
At time: 12.464721202850342 and batch: 750, loss is 5.047775239944458 and perplexity is 155.67573777451975
At time: 13.284725904464722 and batch: 800, loss is 5.012637367248535 and perplexity is 150.30061179583302
At time: 14.103756427764893 and batch: 850, loss is 5.040517301559448 and perplexity is 154.54994327727383
At time: 14.924064636230469 and batch: 900, loss is 4.945815830230713 and perplexity is 140.58549797804173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.912537039142766 and perplexity of 135.98397380764771
finished 1 epochs...
Completing Train Step...
At time: 16.695233821868896 and batch: 50, loss is 4.8703797340393065 and perplexity is 130.3704135818314
At time: 17.343483686447144 and batch: 100, loss is 4.747714214324951 and perplexity is 115.32038534873755
At time: 17.992515802383423 and batch: 150, loss is 4.723394994735718 and perplexity is 112.54971045232644
At time: 18.640182733535767 and batch: 200, loss is 4.610585174560547 and perplexity is 100.54296761220792
At time: 19.302321434020996 and batch: 250, loss is 4.723156967163086 and perplexity is 112.52292370606564
At time: 19.96689748764038 and batch: 300, loss is 4.666141414642334 and perplexity is 106.28683335949549
At time: 20.64254379272461 and batch: 350, loss is 4.653509016036987 and perplexity is 104.95262061011563
At time: 21.289057731628418 and batch: 400, loss is 4.523848905563354 and perplexity is 92.1897456253563
At time: 21.94181752204895 and batch: 450, loss is 4.547652921676636 and perplexity is 94.41055909127545
At time: 22.607980251312256 and batch: 500, loss is 4.4415939426422115 and perplexity is 84.9101758174551
At time: 23.256911277770996 and batch: 550, loss is 4.510432395935059 and perplexity is 90.96114123614234
At time: 23.905193090438843 and batch: 600, loss is 4.471234827041626 and perplexity is 87.46466008537828
At time: 24.57723641395569 and batch: 650, loss is 4.332185344696045 and perplexity is 76.11043248996724
At time: 25.246708869934082 and batch: 700, loss is 4.379870910644531 and perplexity is 79.82772782999466
At time: 25.896581888198853 and batch: 750, loss is 4.42707106590271 and perplexity is 83.68594697955488
At time: 26.545367002487183 and batch: 800, loss is 4.376839723587036 and perplexity is 79.58612141667555
At time: 27.18627381324768 and batch: 850, loss is 4.4457499790191655 and perplexity is 85.26379992525224
At time: 27.831162214279175 and batch: 900, loss is 4.380299501419067 and perplexity is 79.86194859052209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.525036537483947 and perplexity of 92.29929815117393
finished 2 epochs...
Completing Train Step...
At time: 29.461179971694946 and batch: 50, loss is 4.431974010467529 and perplexity is 84.09726204196295
At time: 30.11467480659485 and batch: 100, loss is 4.310826587677002 and perplexity is 74.50204596880921
At time: 30.761438846588135 and batch: 150, loss is 4.308537073135376 and perplexity is 74.33166756752165
At time: 31.441458463668823 and batch: 200, loss is 4.201906085014343 and perplexity is 66.81356207524684
At time: 32.11006307601929 and batch: 250, loss is 4.346002531051636 and perplexity is 77.16936339504834
At time: 32.73769187927246 and batch: 300, loss is 4.311580748558044 and perplexity is 74.55825368960458
At time: 33.36356163024902 and batch: 350, loss is 4.304233093261718 and perplexity is 74.01243304860579
At time: 33.98994517326355 and batch: 400, loss is 4.209029369354248 and perplexity is 67.29119320722312
At time: 34.61709403991699 and batch: 450, loss is 4.243883862495422 and perplexity is 69.67794655725552
At time: 35.24388289451599 and batch: 500, loss is 4.120745167732239 and perplexity is 61.60513132078366
At time: 35.87995648384094 and batch: 550, loss is 4.200147566795349 and perplexity is 66.69617245520554
At time: 36.50720143318176 and batch: 600, loss is 4.200183157920837 and perplexity is 66.69854628929247
At time: 37.134560108184814 and batch: 650, loss is 4.045704760551453 and perplexity is 57.15144993164521
At time: 37.76157522201538 and batch: 700, loss is 4.0754469871521 and perplexity is 58.876791877031216
At time: 38.389347314834595 and batch: 750, loss is 4.154534640312195 and perplexity is 63.72230385547455
At time: 39.01601433753967 and batch: 800, loss is 4.109393544197083 and perplexity is 60.90976728457324
At time: 39.643195152282715 and batch: 850, loss is 4.193170475959778 and perplexity is 66.23244681074372
At time: 40.26994824409485 and batch: 900, loss is 4.13586320400238 and perplexity is 62.54355562344285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393152524347174 and perplexity of 80.89504100486677
finished 3 epochs...
Completing Train Step...
At time: 41.82473826408386 and batch: 50, loss is 4.213118224143982 and perplexity is 67.56690040409885
At time: 42.45746612548828 and batch: 100, loss is 4.088308081626892 and perplexity is 59.638902140903234
At time: 43.096895694732666 and batch: 150, loss is 4.088828539848327 and perplexity is 59.66994977663761
At time: 43.74307703971863 and batch: 200, loss is 3.9852138233184813 and perplexity is 53.79679125660181
At time: 44.422935009002686 and batch: 250, loss is 4.137962384223938 and perplexity is 62.67498371575734
At time: 45.10337233543396 and batch: 300, loss is 4.111594324111938 and perplexity is 61.04396389146586
At time: 45.75559949874878 and batch: 350, loss is 4.106598324775696 and perplexity is 60.73974885022182
At time: 46.406718015670776 and batch: 400, loss is 4.022203683853149 and perplexity is 55.82398880219413
At time: 47.058727502822876 and batch: 450, loss is 4.062840919494629 and perplexity is 58.139245601411524
At time: 47.714882135391235 and batch: 500, loss is 3.9331753206253053 and perplexity is 51.06888055612016
At time: 48.408910036087036 and batch: 550, loss is 4.014166345596314 and perplexity is 55.37711078189572
At time: 49.07199430465698 and batch: 600, loss is 4.024990782737732 and perplexity is 55.97979279883424
At time: 49.73133039474487 and batch: 650, loss is 3.871482262611389 and perplexity is 48.013501980406396
At time: 50.3921263217926 and batch: 700, loss is 3.8932386112213133 and perplexity is 49.069546650228155
At time: 51.085814237594604 and batch: 750, loss is 3.9830619716644287 and perplexity is 53.68115300515646
At time: 51.77171206474304 and batch: 800, loss is 3.941982083320618 and perplexity is 51.520618322253654
At time: 52.43359303474426 and batch: 850, loss is 4.024512476921082 and perplexity is 55.953023740732114
At time: 53.092923402786255 and batch: 900, loss is 3.974598412513733 and perplexity is 53.22873661845926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334812948148545 and perplexity of 76.3106835000558
finished 4 epochs...
Completing Train Step...
At time: 54.77456331253052 and batch: 50, loss is 4.059888243675232 and perplexity is 57.967832445043044
At time: 55.4209725856781 and batch: 100, loss is 3.9351664876937864 and perplexity is 51.17066853411229
At time: 56.06787729263306 and batch: 150, loss is 3.939840168952942 and perplexity is 51.41038366837667
At time: 56.716986894607544 and batch: 200, loss is 3.838491621017456 and perplexity is 46.45534929293594
At time: 57.36475205421448 and batch: 250, loss is 3.995807580947876 and perplexity is 54.369730857751
At time: 58.035804748535156 and batch: 300, loss is 3.970313005447388 and perplexity is 53.001117882238184
At time: 58.69307899475098 and batch: 350, loss is 3.9678240156173707 and perplexity is 52.869362675553894
At time: 59.34246873855591 and batch: 400, loss is 3.88689145565033 and perplexity is 48.7590809332385
At time: 59.99181604385376 and batch: 450, loss is 3.9318450021743776 and perplexity is 51.00098785151968
At time: 60.63520526885986 and batch: 500, loss is 3.8008143091201783 and perplexity is 44.737599900203215
At time: 61.2895724773407 and batch: 550, loss is 3.8772764778137208 and perplexity is 48.29251007938997
At time: 61.936174154281616 and batch: 600, loss is 3.89994330406189 and perplexity is 49.39964826673204
At time: 62.58689785003662 and batch: 650, loss is 3.745155167579651 and perplexity is 42.31557271444825
At time: 63.23576760292053 and batch: 700, loss is 3.7615495252609255 and perplexity is 43.015027236573836
At time: 63.893479108810425 and batch: 750, loss is 3.8567049646377565 and perplexity is 47.30920874661612
At time: 64.55331015586853 and batch: 800, loss is 3.817204976081848 and perplexity is 45.476921435633336
At time: 65.24855041503906 and batch: 850, loss is 3.9024831819534302 and perplexity is 49.5252768142642
At time: 65.91075825691223 and batch: 900, loss is 3.856168761253357 and perplexity is 47.283848188589076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318839713318707 and perplexity of 75.10143852037825
finished 5 epochs...
Completing Train Step...
At time: 67.59983468055725 and batch: 50, loss is 3.9452248287200926 and perplexity is 51.6879577432283
At time: 68.2660858631134 and batch: 100, loss is 3.822760591506958 and perplexity is 45.73027684280371
At time: 68.9249336719513 and batch: 150, loss is 3.8274531841278074 and perplexity is 45.94537469104133
At time: 69.58498978614807 and batch: 200, loss is 3.727773652076721 and perplexity is 41.58641918536714
At time: 70.24642443656921 and batch: 250, loss is 3.8834569311141967 and perplexity is 48.59190392451755
At time: 70.90719389915466 and batch: 300, loss is 3.860886058807373 and perplexity is 47.507427099512014
At time: 71.56871128082275 and batch: 350, loss is 3.86084415435791 and perplexity is 47.505436368644546
At time: 72.23129081726074 and batch: 400, loss is 3.7817391395568847 and perplexity is 43.89231024855283
At time: 72.89317846298218 and batch: 450, loss is 3.8264167308807373 and perplexity is 45.897779127803716
At time: 73.55360174179077 and batch: 500, loss is 3.6998927879333494 and perplexity is 40.44296815342749
At time: 74.20852494239807 and batch: 550, loss is 3.7734953117370607 and perplexity is 43.53195698619333
At time: 74.86500096321106 and batch: 600, loss is 3.798041806221008 and perplexity is 44.613736559855525
At time: 75.51969456672668 and batch: 650, loss is 3.646642928123474 and perplexity is 38.34572039070248
At time: 76.17450022697449 and batch: 700, loss is 3.659713177680969 and perplexity is 38.85019816920051
At time: 76.82822489738464 and batch: 750, loss is 3.7570502614974974 and perplexity is 42.821926015660004
At time: 77.48118591308594 and batch: 800, loss is 3.719732880592346 and perplexity is 41.25337306013281
At time: 78.13745093345642 and batch: 850, loss is 3.8042154932022094 and perplexity is 44.890019769936025
At time: 78.7938973903656 and batch: 900, loss is 3.7557576656341554 and perplexity is 42.76661032934728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314283397099743 and perplexity of 74.76003098895201
finished 6 epochs...
Completing Train Step...
At time: 80.44330286979675 and batch: 50, loss is 3.8518462419509887 and perplexity is 47.079903936131615
At time: 81.10956740379333 and batch: 100, loss is 3.733562879562378 and perplexity is 41.827870660747394
At time: 81.76727557182312 and batch: 150, loss is 3.7356655645370482 and perplexity is 41.915914027201346
At time: 82.42379260063171 and batch: 200, loss is 3.640073561668396 and perplexity is 38.094638927526866
At time: 83.08367967605591 and batch: 250, loss is 3.795053467750549 and perplexity is 44.48061462035607
At time: 83.74039340019226 and batch: 300, loss is 3.771796441078186 and perplexity is 43.458064606316256
At time: 84.39649486541748 and batch: 350, loss is 3.772289972305298 and perplexity is 43.47951781174726
At time: 85.1027421951294 and batch: 400, loss is 3.6960297298431395 and perplexity is 40.28703599951206
At time: 85.75928163528442 and batch: 450, loss is 3.7415933179855347 and perplexity is 42.16511911456153
At time: 86.41554403305054 and batch: 500, loss is 3.6156922721862794 and perplexity is 37.1770736733883
At time: 87.07208895683289 and batch: 550, loss is 3.6906286478042603 and perplexity is 40.0700289768797
At time: 87.72952198982239 and batch: 600, loss is 3.716136584281921 and perplexity is 41.10528015939171
At time: 88.3898549079895 and batch: 650, loss is 3.566308631896973 and perplexity is 35.38573001314521
At time: 89.04353547096252 and batch: 700, loss is 3.575991110801697 and perplexity is 35.73001567712614
At time: 89.71576595306396 and batch: 750, loss is 3.677467818260193 and perplexity is 39.546129195472695
At time: 90.39146065711975 and batch: 800, loss is 3.6415255212783815 and perplexity is 38.149990979354044
At time: 91.07225131988525 and batch: 850, loss is 3.7235139560699464 and perplexity is 41.40965043947681
At time: 91.73156118392944 and batch: 900, loss is 3.6786203289031985 and perplexity is 39.591732804534686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311901771858947 and perplexity of 74.58219246856092
finished 7 epochs...
Completing Train Step...
At time: 93.4019570350647 and batch: 50, loss is 3.7768097925186157 and perplexity is 43.67648220172202
At time: 94.05860042572021 and batch: 100, loss is 3.6569801473617556 and perplexity is 38.74416436250214
At time: 94.71481442451477 and batch: 150, loss is 3.661580171585083 and perplexity is 38.922799003925434
At time: 95.38236570358276 and batch: 200, loss is 3.5638571405410766 and perplexity is 35.299088445814206
At time: 96.07425498962402 and batch: 250, loss is 3.719958333969116 and perplexity is 41.26267482090965
At time: 96.73102355003357 and batch: 300, loss is 3.7012276792526246 and perplexity is 40.496991169947
At time: 97.38974571228027 and batch: 350, loss is 3.6988491344451906 and perplexity is 40.40078172647766
At time: 98.05674600601196 and batch: 400, loss is 3.6204077625274658 and perplexity is 37.352795787564055
At time: 98.71129703521729 and batch: 450, loss is 3.672071056365967 and perplexity is 39.33328300905639
At time: 99.36874771118164 and batch: 500, loss is 3.545605092048645 and perplexity is 34.66065188313288
At time: 100.02344226837158 and batch: 550, loss is 3.6182507514953612 and perplexity is 37.27231222813859
At time: 100.67360758781433 and batch: 600, loss is 3.649552550315857 and perplexity is 38.45745442281649
At time: 101.34828329086304 and batch: 650, loss is 3.4963449954986574 and perplexity is 32.99463575909517
At time: 102.0058925151825 and batch: 700, loss is 3.507785663604736 and perplexity is 33.374284009944404
At time: 102.66423892974854 and batch: 750, loss is 3.611500310897827 and perplexity is 37.02155501157458
At time: 103.32174468040466 and batch: 800, loss is 3.578291664123535 and perplexity is 35.81230910729841
At time: 103.97944188117981 and batch: 850, loss is 3.656903138160706 and perplexity is 38.741180820240814
At time: 104.63525366783142 and batch: 900, loss is 3.610692205429077 and perplexity is 36.991649775427966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325803991866438 and perplexity of 75.62629134687978
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 106.2943000793457 and batch: 50, loss is 3.736877889633179 and perplexity is 41.96676055673812
At time: 106.95524907112122 and batch: 100, loss is 3.6208108377456667 and perplexity is 37.367854808631755
At time: 107.60588479042053 and batch: 150, loss is 3.6264793634414674 and perplexity is 37.58027694518251
At time: 108.25721049308777 and batch: 200, loss is 3.5135768461227417 and perplexity is 33.56812131191165
At time: 108.94506645202637 and batch: 250, loss is 3.6615341186523436 and perplexity is 38.921006536155396
At time: 109.61798787117004 and batch: 300, loss is 3.628965125083923 and perplexity is 37.67380875685945
At time: 110.30652213096619 and batch: 350, loss is 3.6130935859680178 and perplexity is 37.08058754727998
At time: 110.96910429000854 and batch: 400, loss is 3.531075162887573 and perplexity is 34.1606761686117
At time: 111.62430334091187 and batch: 450, loss is 3.5661334657669066 and perplexity is 35.37953217460078
At time: 112.27673959732056 and batch: 500, loss is 3.4302138805389406 and perplexity is 30.883247368943607
At time: 112.92838788032532 and batch: 550, loss is 3.474032645225525 and perplexity is 32.26660018485342
At time: 113.5789246559143 and batch: 600, loss is 3.5079370498657227 and perplexity is 33.37933680046565
At time: 114.22865748405457 and batch: 650, loss is 3.339310040473938 and perplexity is 28.19966336535163
At time: 114.87784242630005 and batch: 700, loss is 3.3341255807876586 and perplexity is 28.05384167742223
At time: 115.53329253196716 and batch: 750, loss is 3.4308773469924927 and perplexity is 30.903744166266495
At time: 116.18659090995789 and batch: 800, loss is 3.3792945766448974 and perplexity is 29.35005959144324
At time: 116.84199905395508 and batch: 850, loss is 3.4322732210159304 and perplexity is 30.94691202140972
At time: 117.49626898765564 and batch: 900, loss is 3.381276001930237 and perplexity is 29.408272194560375
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2870240930008565 and perplexity of 72.74964985140092
finished 9 epochs...
Completing Train Step...
At time: 119.15345931053162 and batch: 50, loss is 3.6487293004989625 and perplexity is 38.4258073590118
At time: 119.83250975608826 and batch: 100, loss is 3.52402907371521 and perplexity is 33.92082300387209
At time: 120.52075290679932 and batch: 150, loss is 3.529265570640564 and perplexity is 34.09891517182852
At time: 121.1926782131195 and batch: 200, loss is 3.4218962717056276 and perplexity is 30.627437934029505
At time: 121.8513388633728 and batch: 250, loss is 3.5718950510025023 and perplexity is 35.583962721309646
At time: 122.50201153755188 and batch: 300, loss is 3.544900321960449 and perplexity is 34.63623269841452
At time: 123.15166425704956 and batch: 350, loss is 3.5304880046844485 and perplexity is 34.14062433475046
At time: 123.8007493019104 and batch: 400, loss is 3.4545123100280763 and perplexity is 31.642853016571863
At time: 124.47339105606079 and batch: 450, loss is 3.4946037769317626 and perplexity is 32.93723487494865
At time: 125.12211894989014 and batch: 500, loss is 3.362151417732239 and perplexity is 28.85119512960288
At time: 125.77334356307983 and batch: 550, loss is 3.410155749320984 and perplexity is 30.26995841774685
At time: 126.42667555809021 and batch: 600, loss is 3.450456027984619 and perplexity is 31.51476064509993
At time: 127.08219981193542 and batch: 650, loss is 3.28658145904541 and perplexity is 26.75125689709367
At time: 127.73621106147766 and batch: 700, loss is 3.285933427810669 and perplexity is 26.733926862863985
At time: 128.38919043540955 and batch: 750, loss is 3.3888944959640503 and perplexity is 29.63317456168653
At time: 129.0417284965515 and batch: 800, loss is 3.3424745321273805 and perplexity is 28.289042309575894
At time: 129.6946632862091 and batch: 850, loss is 3.402925581932068 and perplexity is 30.051890834059
At time: 130.34882831573486 and batch: 900, loss is 3.3590263509750367 and perplexity is 28.761173953153325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.297075245478382 and perplexity of 73.48455480862322
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 131.98174285888672 and batch: 50, loss is 3.621614818572998 and perplexity is 37.39790992770721
At time: 132.62986207008362 and batch: 100, loss is 3.508738775253296 and perplexity is 33.40610859257765
At time: 133.28066515922546 and batch: 150, loss is 3.5228990507125855 and perplexity is 33.88251334308534
At time: 133.92946529388428 and batch: 200, loss is 3.411856279373169 and perplexity is 30.32147718390449
At time: 134.5875141620636 and batch: 250, loss is 3.5609190607070924 and perplexity is 35.19552911318332
At time: 135.23572492599487 and batch: 300, loss is 3.532627592086792 and perplexity is 34.213749385307516
At time: 135.9201159477234 and batch: 350, loss is 3.5110958003997803 and perplexity is 33.48494049847249
At time: 136.57751512527466 and batch: 400, loss is 3.4370957088470457 and perplexity is 31.096513563920176
At time: 137.27665901184082 and batch: 450, loss is 3.467243957519531 and perplexity is 32.04829415699678
At time: 137.97625136375427 and batch: 500, loss is 3.329220747947693 and perplexity is 27.916579173475437
At time: 138.63087725639343 and batch: 550, loss is 3.3759205389022826 and perplexity is 29.251198257357473
At time: 139.28173661231995 and batch: 600, loss is 3.4121421194076538 and perplexity is 30.330145514807352
At time: 139.93294286727905 and batch: 650, loss is 3.24009464263916 and perplexity is 25.53613844052458
At time: 140.61042976379395 and batch: 700, loss is 3.236515884399414 and perplexity is 25.444914106950005
At time: 141.27824640274048 and batch: 750, loss is 3.3301542377471924 and perplexity is 27.942651182454746
At time: 141.93400859832764 and batch: 800, loss is 3.2842495155334475 and perplexity is 26.688947156769476
At time: 142.592280626297 and batch: 850, loss is 3.338762903213501 and perplexity is 28.18423849893703
At time: 143.25974225997925 and batch: 900, loss is 3.295648307800293 and perplexity is 26.994909408442513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.283130802520334 and perplexity of 72.46696497641068
finished 11 epochs...
Completing Train Step...
At time: 144.9356667995453 and batch: 50, loss is 3.599429335594177 and perplexity is 36.57735509208303
At time: 145.6128842830658 and batch: 100, loss is 3.4753491830825807 and perplexity is 32.3091083611871
At time: 146.26275825500488 and batch: 150, loss is 3.4851411247253417 and perplexity is 32.62703126579776
At time: 146.91026711463928 and batch: 200, loss is 3.3749851274490354 and perplexity is 29.223849144817084
At time: 147.55977606773376 and batch: 250, loss is 3.5244337558746337 and perplexity is 33.934552933716176
At time: 148.20917391777039 and batch: 300, loss is 3.497117338180542 and perplexity is 33.02012876796733
At time: 148.86034440994263 and batch: 350, loss is 3.4778067541122435 and perplexity is 32.388607937805475
At time: 149.5155041217804 and batch: 400, loss is 3.405822014808655 and perplexity is 30.13906029798388
At time: 150.16991519927979 and batch: 450, loss is 3.4391256093978884 and perplexity is 31.159700503838142
At time: 150.82067608833313 and batch: 500, loss is 3.303721866607666 and perplexity is 27.213736565373573
At time: 151.48697781562805 and batch: 550, loss is 3.3535318994522094 and perplexity is 28.60358041861175
At time: 152.14333772659302 and batch: 600, loss is 3.3935249614715577 and perplexity is 29.770708130856015
At time: 152.8014395236969 and batch: 650, loss is 3.2246205520629885 and perplexity is 25.144031494582528
At time: 153.45991253852844 and batch: 700, loss is 3.2238625192642214 and perplexity is 25.124978716244176
At time: 154.11880660057068 and batch: 750, loss is 3.321366672515869 and perplexity is 27.69817904309149
At time: 154.81671047210693 and batch: 800, loss is 3.278573637008667 and perplexity is 26.537893022735794
At time: 155.4759783744812 and batch: 850, loss is 3.3365026998519896 and perplexity is 28.12060832399574
At time: 156.1347713470459 and batch: 900, loss is 3.2960832786560057 and perplexity is 27.006653961371732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285057590432363 and perplexity of 72.60672805221184
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 157.77872467041016 and batch: 50, loss is 3.5913217544555662 and perplexity is 36.28200014282175
At time: 158.4378321170807 and batch: 100, loss is 3.4751182126998903 and perplexity is 32.30164677580032
At time: 159.09387493133545 and batch: 150, loss is 3.490151491165161 and perplexity is 32.79091486388156
At time: 159.7489936351776 and batch: 200, loss is 3.3774975109100343 and perplexity is 29.297362968864395
At time: 160.40398740768433 and batch: 250, loss is 3.526070923805237 and perplexity is 33.990154998057655
At time: 161.0602285861969 and batch: 300, loss is 3.4980762720108034 and perplexity is 33.05180807327374
At time: 161.7146773338318 and batch: 350, loss is 3.476068458557129 and perplexity is 32.33235587011506
At time: 162.3694462776184 and batch: 400, loss is 3.4033055877685547 and perplexity is 30.063312898061447
At time: 163.02383828163147 and batch: 450, loss is 3.434749526977539 and perplexity is 31.02364100715504
At time: 163.67800498008728 and batch: 500, loss is 3.2954219722747804 and perplexity is 26.98880019282773
At time: 164.33230257034302 and batch: 550, loss is 3.3437603092193604 and perplexity is 28.32543910620092
At time: 164.98708581924438 and batch: 600, loss is 3.382743091583252 and perplexity is 29.45144843037027
At time: 165.65497469902039 and batch: 650, loss is 3.210451216697693 and perplexity is 24.79026948489296
At time: 166.306813955307 and batch: 700, loss is 3.2094309377670287 and perplexity is 24.76498939381716
At time: 166.9596061706543 and batch: 750, loss is 3.3021581888198854 and perplexity is 27.171216302642783
At time: 167.62867093086243 and batch: 800, loss is 3.256724462509155 and perplexity is 25.96435049917584
At time: 168.28130197525024 and batch: 850, loss is 3.312877445220947 and perplexity is 27.46403814932293
At time: 168.93312811851501 and batch: 900, loss is 3.273523736000061 and perplexity is 26.40421709916257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279577385889341 and perplexity of 72.20991662827767
finished 13 epochs...
Completing Train Step...
At time: 170.55631065368652 and batch: 50, loss is 3.583042483329773 and perplexity is 35.982851700973754
At time: 171.20392274856567 and batch: 100, loss is 3.462378225326538 and perplexity is 31.892734503105153
At time: 171.8514585494995 and batch: 150, loss is 3.4739680337905883 and perplexity is 32.26451546086418
At time: 172.52442479133606 and batch: 200, loss is 3.3623516035079954 and perplexity is 28.85697130661635
At time: 173.1892602443695 and batch: 250, loss is 3.5117965221405028 and perplexity is 33.50841234692341
At time: 173.8363745212555 and batch: 300, loss is 3.484464392662048 and perplexity is 32.604958976965264
At time: 174.4842734336853 and batch: 350, loss is 3.464066824913025 and perplexity is 31.94663405597308
At time: 175.13392329216003 and batch: 400, loss is 3.3919727182388306 and perplexity is 29.72453259772731
At time: 175.7862856388092 and batch: 450, loss is 3.4244902324676514 and perplexity is 30.706987435809886
At time: 176.4378092288971 and batch: 500, loss is 3.286832318305969 and perplexity is 26.75796853942152
At time: 177.08871412277222 and batch: 550, loss is 3.336329231262207 and perplexity is 28.115730704795038
At time: 177.7448103427887 and batch: 600, loss is 3.376775164604187 and perplexity is 29.276207768563033
At time: 178.43811631202698 and batch: 650, loss is 3.2060375308990476 and perplexity is 24.68109413460305
At time: 179.09675765037537 and batch: 700, loss is 3.2063662147521974 and perplexity is 24.689207745056493
At time: 179.7550768852234 and batch: 750, loss is 3.3007163429260253 and perplexity is 27.132067825803137
At time: 180.41567969322205 and batch: 800, loss is 3.257144989967346 and perplexity is 25.975271517627963
At time: 181.0733003616333 and batch: 850, loss is 3.3151630687713625 and perplexity is 27.526882393490183
At time: 181.7581923007965 and batch: 900, loss is 3.2771774196624754 and perplexity is 26.500866210919618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.279380902852098 and perplexity of 72.1957299983012
finished 14 epochs...
Completing Train Step...
At time: 183.46959948539734 and batch: 50, loss is 3.577577648162842 and perplexity is 35.78674767372828
At time: 184.127179145813 and batch: 100, loss is 3.4560025787353514 and perplexity is 31.690044525511905
At time: 184.80112719535828 and batch: 150, loss is 3.466781187057495 and perplexity is 32.03346658424824
At time: 185.46475529670715 and batch: 200, loss is 3.3551313781738283 and perplexity is 28.649367845099007
At time: 186.11530184745789 and batch: 250, loss is 3.504494652748108 and perplexity is 33.2646294151366
At time: 186.7658941745758 and batch: 300, loss is 3.4773942947387697 and perplexity is 32.3752517075083
At time: 187.41367530822754 and batch: 350, loss is 3.457453737258911 and perplexity is 31.73606518729879
At time: 188.0614731311798 and batch: 400, loss is 3.3855592155456544 and perplexity is 29.534504252878595
At time: 188.71189093589783 and batch: 450, loss is 3.4185724210739137 and perplexity is 30.525805903518172
At time: 189.3615016937256 and batch: 500, loss is 3.28161865234375 and perplexity is 26.61882447016626
At time: 190.01077914237976 and batch: 550, loss is 3.3317348098754884 and perplexity is 27.98685147978164
At time: 190.66087937355042 and batch: 600, loss is 3.372952547073364 and perplexity is 29.164509649120493
At time: 191.3158881664276 and batch: 650, loss is 3.202918782234192 and perplexity is 24.604239912019754
At time: 192.0021150112152 and batch: 700, loss is 3.20392698764801 and perplexity is 24.629058548941686
At time: 192.66254687309265 and batch: 750, loss is 3.299099178314209 and perplexity is 27.08822626493485
At time: 193.32733488082886 and batch: 800, loss is 3.2562893724441526 and perplexity is 25.95305612544248
At time: 193.9877803325653 and batch: 850, loss is 3.3151101160049437 and perplexity is 27.525424807508507
At time: 194.64171075820923 and batch: 900, loss is 3.2775575304031372 and perplexity is 26.510941389523722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27986165921982 and perplexity of 72.23044689973732
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 196.28459072113037 and batch: 50, loss is 3.575110273361206 and perplexity is 35.69855719850529
At time: 196.9402813911438 and batch: 100, loss is 3.4560794830322266 and perplexity is 31.692481719818236
At time: 197.59049797058105 and batch: 150, loss is 3.468722729682922 and perplexity is 32.09572134063711
At time: 198.24064683914185 and batch: 200, loss is 3.3564714336395265 and perplexity is 28.68778532209463
At time: 198.89064955711365 and batch: 250, loss is 3.5064631128311157 and perplexity is 33.330174000101394
At time: 199.5407156944275 and batch: 300, loss is 3.478406939506531 and perplexity is 32.40805294195108
At time: 200.19152116775513 and batch: 350, loss is 3.4575828409194944 and perplexity is 31.740162693982853
At time: 200.85409426689148 and batch: 400, loss is 3.3855206680297854 and perplexity is 29.533365793049764
At time: 201.50425958633423 and batch: 450, loss is 3.4184533548355103 and perplexity is 30.522171527005074
At time: 202.15554094314575 and batch: 500, loss is 3.279411826133728 and perplexity is 26.560146120919093
At time: 202.80657649040222 and batch: 550, loss is 3.328512635231018 and perplexity is 27.896818086120984
At time: 203.45795917510986 and batch: 600, loss is 3.3684270763397217 and perplexity is 29.032824707158785
At time: 204.1048548221588 and batch: 650, loss is 3.196444454193115 and perplexity is 24.445458547583208
At time: 204.7844340801239 and batch: 700, loss is 3.1969819355010984 and perplexity is 24.45860105622759
At time: 205.44058656692505 and batch: 750, loss is 3.291045641899109 and perplexity is 26.870946358495754
At time: 206.0921413898468 and batch: 800, loss is 3.2467964363098143 and perplexity is 25.707851120281408
At time: 206.7404613494873 and batch: 850, loss is 3.304809765815735 and perplexity is 27.24335847773762
At time: 207.3907651901245 and batch: 900, loss is 3.266759924888611 and perplexity is 26.226226587832816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277564479880137 and perplexity of 72.06471104479687
finished 16 epochs...
Completing Train Step...
At time: 209.01666927337646 and batch: 50, loss is 3.572158541679382 and perplexity is 35.59333999909153
At time: 209.66049122810364 and batch: 100, loss is 3.4523588609695435 and perplexity is 31.57478506127569
At time: 210.31915402412415 and batch: 150, loss is 3.4641309547424317 and perplexity is 31.94868285385904
At time: 211.00764751434326 and batch: 200, loss is 3.3521635389328 and perplexity is 28.564467175065303
At time: 211.69899010658264 and batch: 250, loss is 3.5020577192306517 and perplexity is 33.18366441799526
At time: 212.3471713066101 and batch: 300, loss is 3.4742422533035278 and perplexity is 32.27336423377674
At time: 212.99909377098083 and batch: 350, loss is 3.4541498708724974 and perplexity is 31.631386485729827
At time: 213.6530783176422 and batch: 400, loss is 3.3821480226516725 and perplexity is 29.433928001867677
At time: 214.32392978668213 and batch: 450, loss is 3.4153130197525026 and perplexity is 30.426472023849506
At time: 214.9935278892517 and batch: 500, loss is 3.277067575454712 and perplexity is 26.497955404136196
At time: 215.64963173866272 and batch: 550, loss is 3.326385474205017 and perplexity is 27.8375401311683
At time: 216.30597949028015 and batch: 600, loss is 3.367035355567932 and perplexity is 28.99244722555725
At time: 216.9720983505249 and batch: 650, loss is 3.1956120586395262 and perplexity is 24.425118723146895
At time: 217.62907481193542 and batch: 700, loss is 3.1964311265945433 and perplexity is 24.445132750495834
At time: 218.30076456069946 and batch: 750, loss is 3.2913225412368776 and perplexity is 26.87838793598403
At time: 218.98058485984802 and batch: 800, loss is 3.247745351791382 and perplexity is 25.732257276065454
At time: 219.66137766838074 and batch: 850, loss is 3.306376280784607 and perplexity is 27.286069051237497
At time: 220.34375071525574 and batch: 900, loss is 3.268875231742859 and perplexity is 26.281761821017017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277116331335616 and perplexity of 72.03242258498433
finished 17 epochs...
Completing Train Step...
At time: 222.02347135543823 and batch: 50, loss is 3.5704243326187135 and perplexity is 35.531667198587016
At time: 222.69740962982178 and batch: 100, loss is 3.4501666021347046 and perplexity is 31.505640778541597
At time: 223.36720299720764 and batch: 150, loss is 3.461557126045227 and perplexity is 31.866558149888704
At time: 224.0272011756897 and batch: 200, loss is 3.34963481426239 and perplexity is 28.4923267522975
At time: 224.68937301635742 and batch: 250, loss is 3.499446082115173 and perplexity is 33.09711379697503
At time: 225.4113574028015 and batch: 300, loss is 3.4716907024383543 and perplexity is 32.191122070494615
At time: 226.09153819084167 and batch: 350, loss is 3.451939902305603 and perplexity is 31.561559302228837
At time: 226.7766089439392 and batch: 400, loss is 3.3800365114212036 and perplexity is 29.37184350146172
At time: 227.49541640281677 and batch: 450, loss is 3.4133585500717163 and perplexity is 30.36706248277079
At time: 228.2342653274536 and batch: 500, loss is 3.2754881620407104 and perplexity is 26.456137210728215
At time: 228.9399118423462 and batch: 550, loss is 3.3249987840652464 and perplexity is 27.79896484092165
At time: 229.67216563224792 and batch: 600, loss is 3.3660431146621703 and perplexity is 28.963694000883827
At time: 230.38330459594727 and batch: 650, loss is 3.1949895572662355 and perplexity is 24.40991878468107
At time: 231.0588502883911 and batch: 700, loss is 3.1960069847106936 and perplexity is 24.434766744324172
At time: 231.71211171150208 and batch: 750, loss is 3.2913024950027467 and perplexity is 26.87784913092693
At time: 232.36574935913086 and batch: 800, loss is 3.2481199407577517 and perplexity is 25.741898101281503
At time: 233.01541876792908 and batch: 850, loss is 3.3071284437179567 and perplexity is 27.30660034144443
At time: 233.7077190876007 and batch: 900, loss is 3.269819641113281 and perplexity is 26.306594287313228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277033557630565 and perplexity of 72.02646044124094
finished 18 epochs...
Completing Train Step...
At time: 235.35874366760254 and batch: 50, loss is 3.5689117383956908 and perplexity is 35.47796283074571
At time: 236.0248372554779 and batch: 100, loss is 3.4483959579467776 and perplexity is 31.44990485761423
At time: 236.68887972831726 and batch: 150, loss is 3.4595617866516113 and perplexity is 31.80303694532934
At time: 237.34676885604858 and batch: 200, loss is 3.3476611089706423 and perplexity is 28.436146755827938
At time: 238.00676155090332 and batch: 250, loss is 3.4974386930465697 and perplexity is 33.03074165218344
At time: 238.6719033718109 and batch: 300, loss is 3.4697477436065673 and perplexity is 32.12863676840469
At time: 239.33013725280762 and batch: 350, loss is 3.4501828002929686 and perplexity is 31.50615111603039
At time: 239.98073053359985 and batch: 400, loss is 3.3783631134033203 and perplexity is 29.32273381824806
At time: 240.63007998466492 and batch: 450, loss is 3.4118102836608886 and perplexity is 30.320082558037676
At time: 241.27568554878235 and batch: 500, loss is 3.274179859161377 and perplexity is 26.421547202277054
At time: 241.92008686065674 and batch: 550, loss is 3.3238573026657106 and perplexity is 27.767250943484946
At time: 242.5659646987915 and batch: 600, loss is 3.3651604223251343 and perplexity is 28.93813925028749
At time: 243.21241641044617 and batch: 650, loss is 3.194356532096863 and perplexity is 24.39447158145808
At time: 243.85312962532043 and batch: 700, loss is 3.1955397844314577 and perplexity is 24.42335348082571
At time: 244.49556851387024 and batch: 750, loss is 3.291097888946533 and perplexity is 26.87235032278065
At time: 245.14774441719055 and batch: 800, loss is 3.2481723499298094 and perplexity is 25.743247248201715
At time: 245.83997321128845 and batch: 850, loss is 3.3074417400360105 and perplexity is 27.31515673906392
At time: 246.51346826553345 and batch: 900, loss is 3.2702383375167847 and perplexity is 26.317611069912434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.277091248394692 and perplexity of 72.03061582264355
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 248.1930227279663 and batch: 50, loss is 3.568136463165283 and perplexity is 35.45046830422753
At time: 248.84889149665833 and batch: 100, loss is 3.448168869018555 and perplexity is 31.442763743292094
At time: 249.52562022209167 and batch: 150, loss is 3.4596962451934816 and perplexity is 31.80731342280208
At time: 250.21664667129517 and batch: 200, loss is 3.347730112075806 and perplexity is 28.438109005952874
At time: 250.88159656524658 and batch: 250, loss is 3.497674789428711 and perplexity is 33.03854101145098
At time: 251.53427410125732 and batch: 300, loss is 3.469595170021057 and perplexity is 32.1237351610328
At time: 252.22600054740906 and batch: 350, loss is 3.449847254753113 and perplexity is 31.49558114099855
At time: 252.876366853714 and batch: 400, loss is 3.378034734725952 and perplexity is 29.31310643849963
At time: 253.5255868434906 and batch: 450, loss is 3.411681652069092 and perplexity is 30.316182688383424
At time: 254.1754856109619 and batch: 500, loss is 3.2729391384124757 and perplexity is 26.38878576854313
At time: 254.8253951072693 and batch: 550, loss is 3.322553343772888 and perplexity is 27.73106718588117
At time: 255.47470426559448 and batch: 600, loss is 3.3636498737335203 and perplexity is 28.894459783083366
At time: 256.12164330482483 and batch: 650, loss is 3.1920784759521483 and perplexity is 24.338962855577307
At time: 256.7714478969574 and batch: 700, loss is 3.1928810119628905 and perplexity is 24.35850358975566
At time: 257.4215862751007 and batch: 750, loss is 3.2880434656143187 and perplexity is 26.790396014255442
At time: 258.071302652359 and batch: 800, loss is 3.2448450231552126 and perplexity is 25.657733397524563
At time: 258.72191095352173 and batch: 850, loss is 3.30403386592865 and perplexity is 27.222228557382394
At time: 259.409859418869 and batch: 900, loss is 3.2665065813064573 and perplexity is 26.219583183210172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27662679593857 and perplexity of 71.99716879408732
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
541.1911106109619


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0307433605194092 and batch: 50, loss is 7.073186140060425 and perplexity is 1179.9013827899857
At time: 1.8411614894866943 and batch: 100, loss is 6.095119762420654 and perplexity is 443.68717904591796
At time: 2.6517014503479004 and batch: 150, loss is 5.857973794937134 and perplexity is 350.01422444782827
At time: 3.4613420963287354 and batch: 200, loss is 5.6337982940673825 and perplexity is 279.72257098315094
At time: 4.27573823928833 and batch: 250, loss is 5.625457305908203 and perplexity is 277.3991117742426
At time: 5.085075855255127 and batch: 300, loss is 5.493290891647339 and perplexity is 243.05576233559222
At time: 5.89502215385437 and batch: 350, loss is 5.430414495468139 and perplexity is 228.24383185376067
At time: 6.705488920211792 and batch: 400, loss is 5.239712953567505 and perplexity is 188.61595309790653
At time: 7.517247915267944 and batch: 450, loss is 5.220680561065674 and perplexity is 185.0600859131607
At time: 8.329174280166626 and batch: 500, loss is 5.134794483184814 and perplexity is 169.8294134876963
At time: 9.140993356704712 and batch: 550, loss is 5.175090951919556 and perplexity is 176.8126949991469
At time: 9.952547073364258 and batch: 600, loss is 5.0742591285705565 and perplexity is 159.85371704560944
At time: 10.764755487442017 and batch: 650, loss is 4.948834867477417 and perplexity is 141.0105721673243
At time: 11.584086418151855 and batch: 700, loss is 5.026090250015259 and perplexity is 152.33625020780843
At time: 12.39391565322876 and batch: 750, loss is 5.008009605407715 and perplexity is 149.60666331254416
At time: 13.206576585769653 and batch: 800, loss is 4.965220165252686 and perplexity is 143.34010531006072
At time: 14.018756628036499 and batch: 850, loss is 4.996355905532837 and perplexity is 147.87331175540717
At time: 14.82969331741333 and batch: 900, loss is 4.910600833892822 and perplexity is 135.72093565375548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.868316232341609 and perplexity of 130.10167138242772
finished 1 epochs...
Completing Train Step...
At time: 16.566784143447876 and batch: 50, loss is 4.834975233078003 and perplexity is 125.83546661109278
At time: 17.206241369247437 and batch: 100, loss is 4.7067247581481935 and perplexity is 110.68903221396145
At time: 17.883310317993164 and batch: 150, loss is 4.6906890869140625 and perplexity is 108.92821495907677
At time: 18.541218757629395 and batch: 200, loss is 4.57786919593811 and perplexity is 97.30683135620129
At time: 19.178699016571045 and batch: 250, loss is 4.698157529830933 and perplexity is 109.74478457077797
At time: 19.817097187042236 and batch: 300, loss is 4.638885898590088 and perplexity is 103.42905291655097
At time: 20.45502495765686 and batch: 350, loss is 4.6234642696380615 and perplexity is 101.84624455064821
At time: 21.09316086769104 and batch: 400, loss is 4.4976284790039065 and perplexity is 89.80390671685628
At time: 21.730531692504883 and batch: 450, loss is 4.521294841766357 and perplexity is 91.95458756577621
At time: 22.3685359954834 and batch: 500, loss is 4.413863353729248 and perplexity is 82.58791429076437
At time: 23.006762504577637 and batch: 550, loss is 4.4849467468261714 and perplexity is 88.67222860962165
At time: 23.64487051963806 and batch: 600, loss is 4.4548870468139645 and perplexity is 86.04643107767215
At time: 24.32841968536377 and batch: 650, loss is 4.310848054885864 and perplexity is 74.5036453369576
At time: 24.99070429801941 and batch: 700, loss is 4.355143423080444 and perplexity is 77.8779940376879
At time: 25.728891134262085 and batch: 750, loss is 4.40849006652832 and perplexity is 82.14533582341275
At time: 26.426507711410522 and batch: 800, loss is 4.364523935317993 and perplexity is 78.61196665124436
At time: 27.084676504135132 and batch: 850, loss is 4.4281714725494385 and perplexity is 83.77808623787755
At time: 27.72646188735962 and batch: 900, loss is 4.3646949100494385 and perplexity is 78.62540846020305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.500230345007491 and perplexity of 90.03786868559044
finished 2 epochs...
Completing Train Step...
At time: 29.348508834838867 and batch: 50, loss is 4.41699444770813 and perplexity is 82.84691007039761
At time: 29.9990234375 and batch: 100, loss is 4.294021883010864 and perplexity is 73.26052202772586
At time: 30.647003889083862 and batch: 150, loss is 4.2906049585342405 and perplexity is 73.01062354210329
At time: 31.287524700164795 and batch: 200, loss is 4.1928914308547975 and perplexity is 66.21396754906401
At time: 31.95602774620056 and batch: 250, loss is 4.335868730545044 and perplexity is 76.39129352219328
At time: 32.61783289909363 and batch: 300, loss is 4.297754402160645 and perplexity is 73.53447928641695
At time: 33.305174350738525 and batch: 350, loss is 4.2870353317260745 and perplexity is 72.75046746931979
At time: 34.01802158355713 and batch: 400, loss is 4.193890323638916 and perplexity is 66.28014124816963
At time: 34.728293657302856 and batch: 450, loss is 4.2287997627258305 and perplexity is 68.63480466659217
At time: 35.41250205039978 and batch: 500, loss is 4.104369711875916 and perplexity is 60.60453418893838
At time: 36.13074564933777 and batch: 550, loss is 4.188300080299378 and perplexity is 65.91065285725567
At time: 36.86203122138977 and batch: 600, loss is 4.185283842086792 and perplexity is 65.71215014371914
At time: 37.54772996902466 and batch: 650, loss is 4.033960409164429 and perplexity is 56.48416928039677
At time: 38.2227509021759 and batch: 700, loss is 4.059528231620789 and perplexity is 57.94696708270922
At time: 38.873947620391846 and batch: 750, loss is 4.145858573913574 and perplexity is 63.1718363156071
At time: 39.54850220680237 and batch: 800, loss is 4.106634597778321 and perplexity is 60.74195210325035
At time: 40.19670915603638 and batch: 850, loss is 4.180553922653198 and perplexity is 65.402070870815
At time: 40.836684465408325 and batch: 900, loss is 4.129192433357239 and perplexity is 62.12773038792359
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376503304259418 and perplexity of 79.55935161041343
finished 3 epochs...
Completing Train Step...
At time: 42.5041720867157 and batch: 50, loss is 4.19695408821106 and perplexity is 66.48351938904716
At time: 43.165974140167236 and batch: 100, loss is 4.072007145881653 and perplexity is 58.674612989458716
At time: 43.84190344810486 and batch: 150, loss is 4.074979348182678 and perplexity is 58.849265231522686
At time: 44.485485553741455 and batch: 200, loss is 3.9798197460174563 and perplexity is 53.5073884383098
At time: 45.17299962043762 and batch: 250, loss is 4.130567789077759 and perplexity is 62.2132369047754
At time: 45.85302400588989 and batch: 300, loss is 4.100907073020935 and perplexity is 60.3950454752594
At time: 46.52581596374512 and batch: 350, loss is 4.088096899986267 and perplexity is 59.626308829488906
At time: 47.19391345977783 and batch: 400, loss is 4.007774481773376 and perplexity is 55.02427666596017
At time: 47.87541127204895 and batch: 450, loss is 4.046853032112121 and perplexity is 57.21711300855972
At time: 48.55262207984924 and batch: 500, loss is 3.9205461883544923 and perplexity is 50.42798043320705
At time: 49.230462312698364 and batch: 550, loss is 4.003726487159729 and perplexity is 54.801988903767224
At time: 49.89120888710022 and batch: 600, loss is 4.016012105941773 and perplexity is 55.47941804532268
At time: 50.55204677581787 and batch: 650, loss is 3.8603619956970214 and perplexity is 47.48253673213278
At time: 51.24089598655701 and batch: 700, loss is 3.8766240310668945 and perplexity is 48.26101206479853
At time: 51.91198396682739 and batch: 750, loss is 3.9775472402572634 and perplexity is 53.38593064890407
At time: 52.580450773239136 and batch: 800, loss is 3.940425405502319 and perplexity is 51.44047970970959
At time: 53.28597068786621 and batch: 850, loss is 4.0162149429321286 and perplexity is 55.4906724648734
At time: 53.99272656440735 and batch: 900, loss is 3.9702790117263795 and perplexity is 52.99931620764675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327930607207834 and perplexity of 75.78729050923874
finished 4 epochs...
Completing Train Step...
At time: 55.822571992874146 and batch: 50, loss is 4.044979701042175 and perplexity is 57.11002674835446
At time: 56.54359483718872 and batch: 100, loss is 3.9186086368560793 and perplexity is 50.33036821903987
At time: 57.21879029273987 and batch: 150, loss is 3.929848675727844 and perplexity is 50.89927479068893
At time: 57.88261437416077 and batch: 200, loss is 3.8380074596405027 and perplexity is 46.43286285102803
At time: 58.54537081718445 and batch: 250, loss is 3.9884491109848024 and perplexity is 53.971121203545025
At time: 59.21000695228577 and batch: 300, loss is 3.9597671747207643 and perplexity is 52.44511397921981
At time: 59.87285399436951 and batch: 350, loss is 3.9528667640686037 and perplexity is 52.08446689359597
At time: 60.535544633865356 and batch: 400, loss is 3.8735561180114746 and perplexity is 48.1131783622422
At time: 61.19499969482422 and batch: 450, loss is 3.9114677810668947 and perplexity is 49.97224648736357
At time: 61.85612082481384 and batch: 500, loss is 3.789765362739563 and perplexity is 44.24601729363071
At time: 62.52497482299805 and batch: 550, loss is 3.8705912446975708 and perplexity is 47.9707401436471
At time: 63.18501949310303 and batch: 600, loss is 3.891197853088379 and perplexity is 48.96950968416695
At time: 63.847815990448 and batch: 650, loss is 3.734849977493286 and perplexity is 41.881741887864585
At time: 64.50572562217712 and batch: 700, loss is 3.7498646783828735 and perplexity is 42.51532836778876
At time: 65.16454553604126 and batch: 750, loss is 3.850927748680115 and perplexity is 47.03668121410122
At time: 65.82346558570862 and batch: 800, loss is 3.8202392721176146 and perplexity is 45.61512144189653
At time: 66.48207259178162 and batch: 850, loss is 3.893798351287842 and perplexity is 49.09702052993298
At time: 67.14348340034485 and batch: 900, loss is 3.8526266527175905 and perplexity is 47.11665994057927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313436011745505 and perplexity of 74.69670726719602
finished 5 epochs...
Completing Train Step...
At time: 68.82983064651489 and batch: 50, loss is 3.9295160961151123 and perplexity is 50.88234954424267
At time: 69.49309420585632 and batch: 100, loss is 3.805911116600037 and perplexity is 44.96620090679331
At time: 70.14574360847473 and batch: 150, loss is 3.815718035697937 and perplexity is 45.40935021425362
At time: 70.80011105537415 and batch: 200, loss is 3.725758991241455 and perplexity is 41.502720995398
At time: 71.45306634902954 and batch: 250, loss is 3.873080053329468 and perplexity is 48.09027882854695
At time: 72.10601091384888 and batch: 300, loss is 3.8487000131607054 and perplexity is 46.93201255897334
At time: 72.76019525527954 and batch: 350, loss is 3.846544551849365 and perplexity is 46.83096136673057
At time: 73.41334390640259 and batch: 400, loss is 3.7683894491195677 and perplexity is 43.310255265259485
At time: 74.06758880615234 and batch: 450, loss is 3.8072650623321533 and perplexity is 45.02712393653176
At time: 74.74729919433594 and batch: 500, loss is 3.690483136177063 and perplexity is 40.064198745954904
At time: 75.4017481803894 and batch: 550, loss is 3.7670609617233275 and perplexity is 43.25275633876777
At time: 76.05650401115417 and batch: 600, loss is 3.7913100719451904 and perplexity is 44.314417339345745
At time: 76.71036124229431 and batch: 650, loss is 3.637463397979736 and perplexity is 37.99533533995408
At time: 77.36623167991638 and batch: 700, loss is 3.648518371582031 and perplexity is 38.4177030998247
At time: 78.03723859786987 and batch: 750, loss is 3.7529740524291992 and perplexity is 42.64773016306556
At time: 78.71154546737671 and batch: 800, loss is 3.724271936416626 and perplexity is 41.44105003930924
At time: 79.43781399726868 and batch: 850, loss is 3.795925006866455 and perplexity is 44.519398114108235
At time: 80.10345768928528 and batch: 900, loss is 3.758120393753052 and perplexity is 42.86777566815483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306178262788955 and perplexity of 74.15653988868769
finished 6 epochs...
Completing Train Step...
At time: 81.75461268424988 and batch: 50, loss is 3.8391620826721193 and perplexity is 46.48650626690187
At time: 82.41802477836609 and batch: 100, loss is 3.716504592895508 and perplexity is 41.12041004034676
At time: 83.07286930084229 and batch: 150, loss is 3.7258705139160155 and perplexity is 41.50734974794558
At time: 83.73006296157837 and batch: 200, loss is 3.6382085275650025 and perplexity is 38.02365733888725
At time: 84.38620162010193 and batch: 250, loss is 3.781975493431091 and perplexity is 43.902685592205984
At time: 85.04174470901489 and batch: 300, loss is 3.7609837293624877 and perplexity is 42.99069639438663
At time: 85.69518232345581 and batch: 350, loss is 3.7587507915496827 and perplexity is 42.89480793912931
At time: 86.39344048500061 and batch: 400, loss is 3.683999924659729 and perplexity is 39.805294244391796
At time: 87.0548324584961 and batch: 450, loss is 3.7212068319320677 and perplexity is 41.31422335879584
At time: 87.70941138267517 and batch: 500, loss is 3.609787130355835 and perplexity is 36.958184701784546
At time: 88.36411118507385 and batch: 550, loss is 3.6817431116104125 and perplexity is 39.71556242896038
At time: 89.05474972724915 and batch: 600, loss is 3.707895154953003 and perplexity is 40.76790603000303
At time: 89.75402522087097 and batch: 650, loss is 3.5586317729949952 and perplexity is 35.115118807730546
At time: 90.44101238250732 and batch: 700, loss is 3.5654569387435915 and perplexity is 35.355605059594964
At time: 91.11373138427734 and batch: 750, loss is 3.6719705486297607 and perplexity is 39.32932990848552
At time: 91.81438088417053 and batch: 800, loss is 3.6440015411376954 and perplexity is 38.244568153814186
At time: 92.48696422576904 and batch: 850, loss is 3.7156452798843382 and perplexity is 41.08508991466966
At time: 93.14889001846313 and batch: 900, loss is 3.6821437740325926 and perplexity is 39.731478150604296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30717530969071 and perplexity of 74.23051430888366
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 94.91360592842102 and batch: 50, loss is 3.7826671028137206 and perplexity is 43.93305960374916
At time: 95.61060380935669 and batch: 100, loss is 3.6651944732666015 and perplexity is 39.06373227607634
At time: 96.3487446308136 and batch: 150, loss is 3.6734807682037354 and perplexity is 39.38877070537456
At time: 97.071120262146 and batch: 200, loss is 3.568692617416382 and perplexity is 35.47018971644366
At time: 97.77929520606995 and batch: 250, loss is 3.7051315975189207 and perplexity is 40.655397114252
At time: 98.4691731929779 and batch: 300, loss is 3.672626075744629 and perplexity is 39.35511980271408
At time: 99.175208568573 and batch: 350, loss is 3.657690443992615 and perplexity is 38.77169398785691
At time: 99.8887197971344 and batch: 400, loss is 3.5792801094055178 and perplexity is 35.847725115782154
At time: 100.58695411682129 and batch: 450, loss is 3.6034541177749633 and perplexity is 36.72486763287274
At time: 101.25577521324158 and batch: 500, loss is 3.4777098512649536 and perplexity is 32.38546954153877
At time: 101.93271613121033 and batch: 550, loss is 3.5315030574798585 and perplexity is 34.175296464962855
At time: 102.61148643493652 and batch: 600, loss is 3.560048861503601 and perplexity is 35.16491531376635
At time: 103.28894376754761 and batch: 650, loss is 3.390732002258301 and perplexity is 29.68767576427871
At time: 103.97492146492004 and batch: 700, loss is 3.3781970739364624 and perplexity is 29.31786549133645
At time: 104.65650463104248 and batch: 750, loss is 3.4778523111343382 and perplexity is 32.39008349994377
At time: 105.36823177337646 and batch: 800, loss is 3.434030828475952 and perplexity is 31.00135237321273
At time: 106.02556824684143 and batch: 850, loss is 3.49097110748291 and perplexity is 32.81780184977895
At time: 106.71135878562927 and batch: 900, loss is 3.446920199394226 and perplexity is 31.40352662150009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2628499906357025 and perplexity of 71.01207909258127
finished 8 epochs...
Completing Train Step...
At time: 108.41492080688477 and batch: 50, loss is 3.6930550956726074 and perplexity is 40.16737486787857
At time: 109.07670140266418 and batch: 100, loss is 3.567860369682312 and perplexity is 35.44068201198838
At time: 109.73134899139404 and batch: 150, loss is 3.5753103399276736 and perplexity is 35.70570000076597
At time: 110.38898181915283 and batch: 200, loss is 3.4765047597885133 and perplexity is 32.34646559461601
At time: 111.04514455795288 and batch: 250, loss is 3.6145966386795045 and perplexity is 37.13636353155804
At time: 111.71608591079712 and batch: 300, loss is 3.5862469053268433 and perplexity is 36.09834088169393
At time: 112.37337636947632 and batch: 350, loss is 3.5761784887313843 and perplexity is 35.736711320779754
At time: 113.04397678375244 and batch: 400, loss is 3.501262192726135 and perplexity is 33.15727643099696
At time: 113.74355578422546 and batch: 450, loss is 3.5300439453125 and perplexity is 34.12546723612535
At time: 114.42649459838867 and batch: 500, loss is 3.4097927045822143 and perplexity is 30.258971063171824
At time: 115.08306074142456 and batch: 550, loss is 3.4663565587997436 and perplexity is 32.01986715669779
At time: 115.73617315292358 and batch: 600, loss is 3.5009379816055297 and perplexity is 33.146528215688654
At time: 116.39051723480225 and batch: 650, loss is 3.3363191080093384 and perplexity is 28.11544608358418
At time: 117.04569554328918 and batch: 700, loss is 3.3295178508758543 and perplexity is 27.92487450311447
At time: 117.70374083518982 and batch: 750, loss is 3.435763373374939 and perplexity is 31.055110163574252
At time: 118.36417031288147 and batch: 800, loss is 3.39693706035614 and perplexity is 29.872462229664087
At time: 119.02171516418457 and batch: 850, loss is 3.4603660106658936 and perplexity is 31.828623998850237
At time: 119.72158145904541 and batch: 900, loss is 3.4220487785339357 and perplexity is 30.632109183637706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271237308031892 and perplexity of 71.61018468336218
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 121.38521838188171 and batch: 50, loss is 3.662762451171875 and perplexity is 38.96884384822493
At time: 122.08992910385132 and batch: 100, loss is 3.5529527139663695 and perplexity is 34.91626316614023
At time: 122.74172186851501 and batch: 150, loss is 3.563327684402466 and perplexity is 35.2804040734635
At time: 123.39485096931458 and batch: 200, loss is 3.4607127618789675 and perplexity is 31.839662526533143
At time: 124.06510996818542 and batch: 250, loss is 3.599199752807617 and perplexity is 36.56895852486653
At time: 124.72842025756836 and batch: 300, loss is 3.565720410346985 and perplexity is 35.36492148480182
At time: 125.40521693229675 and batch: 350, loss is 3.5493909549713134 and perplexity is 34.79212106509719
At time: 126.06208753585815 and batch: 400, loss is 3.4748490858078003 and perplexity is 32.2929547036818
At time: 126.7168800830841 and batch: 450, loss is 3.498489818572998 and perplexity is 33.065479361539055
At time: 127.37264037132263 and batch: 500, loss is 3.3716097497940063 and perplexity is 29.12537390647471
At time: 128.0291063785553 and batch: 550, loss is 3.4242093420028685 and perplexity is 30.698363347105804
At time: 128.68517541885376 and batch: 600, loss is 3.455167679786682 and perplexity is 31.663597582452994
At time: 129.34199905395508 and batch: 650, loss is 3.285540189743042 and perplexity is 26.72341613186956
At time: 130.0081148147583 and batch: 700, loss is 3.2697374629974365 and perplexity is 26.304432549785382
At time: 130.66521978378296 and batch: 750, loss is 3.3754211711883544 and perplexity is 29.236594799905088
At time: 131.32015371322632 and batch: 800, loss is 3.3309264087677004 and perplexity is 27.9642360204547
At time: 131.9729781150818 and batch: 850, loss is 3.3901855182647704 and perplexity is 29.671456356895316
At time: 132.62895250320435 and batch: 900, loss is 3.3551790285110474 and perplexity is 28.65073302966343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.261095438918022 and perplexity of 70.88759396696393
finished 10 epochs...
Completing Train Step...
At time: 134.2837438583374 and batch: 50, loss is 3.6390357398986817 and perplexity is 38.05512399021627
At time: 134.9353005886078 and batch: 100, loss is 3.516346845626831 and perplexity is 33.661233892663184
At time: 135.58751511573792 and batch: 150, loss is 3.5269609832763673 and perplexity is 34.02042172503424
At time: 136.2392017841339 and batch: 200, loss is 3.4244874477386475 and perplexity is 30.706901925290413
At time: 136.8948998451233 and batch: 250, loss is 3.5632217025756834 and perplexity is 35.27666518992052
At time: 137.55015587806702 and batch: 300, loss is 3.5321627330780028 and perplexity is 34.1978485117998
At time: 138.20364022254944 and batch: 350, loss is 3.517571601867676 and perplexity is 33.70248595565195
At time: 138.87308025360107 and batch: 400, loss is 3.4451086807250975 and perplexity is 31.346690042553604
At time: 139.53435444831848 and batch: 450, loss is 3.471816272735596 and perplexity is 32.19516457306536
At time: 140.19650673866272 and batch: 500, loss is 3.3473379945755006 and perplexity is 28.42696011171717
At time: 140.85822415351868 and batch: 550, loss is 3.4019560289382933 and perplexity is 30.022768053653078
At time: 141.51643633842468 and batch: 600, loss is 3.4368924427032472 and perplexity is 31.09019333788821
At time: 142.20476531982422 and batch: 650, loss is 3.269852123260498 and perplexity is 26.307448795859703
At time: 142.86528706550598 and batch: 700, loss is 3.2574282598495485 and perplexity is 25.982630571980955
At time: 143.52771139144897 and batch: 750, loss is 3.3665862894058227 and perplexity is 28.979430621428488
At time: 144.19119143486023 and batch: 800, loss is 3.3254353904724123 and perplexity is 27.811104697060415
At time: 144.86203861236572 and batch: 850, loss is 3.387720308303833 and perplexity is 29.59840007366584
At time: 145.52315497398376 and batch: 900, loss is 3.354942774772644 and perplexity is 28.643964986396423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.263343706522902 and perplexity of 71.04714754042496
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 147.17649126052856 and batch: 50, loss is 3.630625615119934 and perplexity is 37.73641770730239
At time: 147.83804035186768 and batch: 100, loss is 3.51390163898468 and perplexity is 33.579025768851466
At time: 148.492036819458 and batch: 150, loss is 3.5284762525558473 and perplexity is 34.07201090082485
At time: 149.1450514793396 and batch: 200, loss is 3.4254529666900635 and perplexity is 30.736564338545282
At time: 149.79396414756775 and batch: 250, loss is 3.5652279901504516 and perplexity is 35.347511370111725
At time: 150.44580841064453 and batch: 300, loss is 3.5321497964859008 and perplexity is 34.19740611104442
At time: 151.1162829399109 and batch: 350, loss is 3.5143793344497682 and perplexity is 33.59507014904206
At time: 151.80154275894165 and batch: 400, loss is 3.4402966690063477 and perplexity is 31.196211744757058
At time: 152.4558367729187 and batch: 450, loss is 3.4646238899230957 and perplexity is 31.96443536577827
At time: 153.10788536071777 and batch: 500, loss is 3.33862051486969 and perplexity is 28.18022567759187
At time: 153.76101875305176 and batch: 550, loss is 3.3896926546096804 and perplexity is 29.65683597768614
At time: 154.41302037239075 and batch: 600, loss is 3.4239630556106566 and perplexity is 30.690803688908918
At time: 155.06556820869446 and batch: 650, loss is 3.2537510776519776 and perplexity is 25.887263154540953
At time: 155.73010087013245 and batch: 700, loss is 3.2412088203430174 and perplexity is 25.564606092684322
At time: 156.3817331790924 and batch: 750, loss is 3.3481305551528933 and perplexity is 28.449499130225767
At time: 157.04681873321533 and batch: 800, loss is 3.304736351966858 and perplexity is 27.24135851134895
At time: 157.7001519203186 and batch: 850, loss is 3.361953573226929 and perplexity is 28.845487643790825
At time: 158.35154366493225 and batch: 900, loss is 3.332767825126648 and perplexity is 28.015777262007013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259847144558005 and perplexity of 70.79916059014595
finished 12 epochs...
Completing Train Step...
At time: 159.9928708076477 and batch: 50, loss is 3.622091507911682 and perplexity is 37.41574136234856
At time: 160.66812992095947 and batch: 100, loss is 3.4999714612960817 and perplexity is 33.11450690009918
At time: 161.31291675567627 and batch: 150, loss is 3.5141558265686035 and perplexity is 33.5875622251668
At time: 161.95966911315918 and batch: 200, loss is 3.4117118406295774 and perplexity is 30.31709790411266
At time: 162.6071891784668 and batch: 250, loss is 3.5515213775634766 and perplexity is 34.86632199745023
At time: 163.29427409172058 and batch: 300, loss is 3.519084496498108 and perplexity is 33.75351285511482
At time: 163.95315313339233 and batch: 350, loss is 3.5023289346694946 and perplexity is 33.19266556067206
At time: 164.63309407234192 and batch: 400, loss is 3.4295278358459473 and perplexity is 30.862067347035268
At time: 165.2922224998474 and batch: 450, loss is 3.4547878742218017 and perplexity is 31.651573855370813
At time: 165.94619750976562 and batch: 500, loss is 3.3302793073654176 and perplexity is 27.94614617772464
At time: 166.59789276123047 and batch: 550, loss is 3.382361536026001 and perplexity is 29.44021321011923
At time: 167.2507381439209 and batch: 600, loss is 3.418625206947327 and perplexity is 30.527417277372948
At time: 167.9063138961792 and batch: 650, loss is 3.249621047973633 and perplexity is 25.78056846719307
At time: 168.56262850761414 and batch: 700, loss is 3.2386822080612183 and perplexity is 25.500095775549227
At time: 169.21683478355408 and batch: 750, loss is 3.346966257095337 and perplexity is 28.416394709096277
At time: 169.87216114997864 and batch: 800, loss is 3.304959659576416 and perplexity is 27.24744239326251
At time: 170.52839159965515 and batch: 850, loss is 3.36411600112915 and perplexity is 28.90793142187622
At time: 171.18465733528137 and batch: 900, loss is 3.3360364055633545 and perplexity is 28.107498901603243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.259735107421875 and perplexity of 70.79122889928325
finished 13 epochs...
Completing Train Step...
At time: 172.83074378967285 and batch: 50, loss is 3.6163069438934325 and perplexity is 37.19993239331457
At time: 173.47841215133667 and batch: 100, loss is 3.49322021484375 and perplexity is 32.89169567591078
At time: 174.1262686252594 and batch: 150, loss is 3.507090711593628 and perplexity is 33.351098541479494
At time: 174.77399611473083 and batch: 200, loss is 3.4044811391830443 and perplexity is 30.098674648768775
At time: 175.42224669456482 and batch: 250, loss is 3.544148063659668 and perplexity is 34.610187102595425
At time: 176.06975650787354 and batch: 300, loss is 3.512156882286072 and perplexity is 33.52048961922764
At time: 176.73515963554382 and batch: 350, loss is 3.495699806213379 and perplexity is 32.973354839480606
At time: 177.38357305526733 and batch: 400, loss is 3.4233934020996095 and perplexity is 30.673325543546756
At time: 178.0592978000641 and batch: 450, loss is 3.4490077304840088 and perplexity is 31.469150932214333
At time: 178.74886965751648 and batch: 500, loss is 3.3251928806304933 and perplexity is 27.804361048190398
At time: 179.43024849891663 and batch: 550, loss is 3.3776883459091187 and perplexity is 29.302954464609307
At time: 180.11911964416504 and batch: 600, loss is 3.4149932289123535 and perplexity is 30.41674347243195
At time: 180.79186010360718 and batch: 650, loss is 3.2465464735031127 and perplexity is 25.701425916725526
At time: 181.47149419784546 and batch: 700, loss is 3.236505641937256 and perplexity is 25.444653489714835
At time: 182.12616419792175 and batch: 750, loss is 3.3454560041427612 and perplexity is 28.3735111557428
At time: 182.78245043754578 and batch: 800, loss is 3.3041218519210815 and perplexity is 27.224623837552215
At time: 183.43801403045654 and batch: 850, loss is 3.364002342224121 and perplexity is 28.904645964758423
At time: 184.1456274986267 and batch: 900, loss is 3.336306219100952 and perplexity is 28.11508370851068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260254324299016 and perplexity of 70.82799444390098
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 185.80281496047974 and batch: 50, loss is 3.6136798286437988 and perplexity is 37.102332143325555
At time: 186.46876668930054 and batch: 100, loss is 3.4923282861709595 and perplexity is 32.862371708828285
At time: 187.13474321365356 and batch: 150, loss is 3.5083503341674804 and perplexity is 33.39313480742181
At time: 187.78185391426086 and batch: 200, loss is 3.4065954303741455 and perplexity is 30.162379332837467
At time: 188.43063497543335 and batch: 250, loss is 3.546351418495178 and perplexity is 34.68652969973563
At time: 189.11564326286316 and batch: 300, loss is 3.5123260402679444 and perplexity is 33.52616035721478
At time: 189.7807421684265 and batch: 350, loss is 3.495096116065979 and perplexity is 32.95345515726269
At time: 190.45896196365356 and batch: 400, loss is 3.4225638341903686 and perplexity is 30.647890488517504
At time: 191.10888147354126 and batch: 450, loss is 3.4467193412780763 and perplexity is 31.3972196017317
At time: 191.76475501060486 and batch: 500, loss is 3.3218224954605104 and perplexity is 27.71080738654981
At time: 192.41349124908447 and batch: 550, loss is 3.373033576011658 and perplexity is 29.16687291411835
At time: 193.0630111694336 and batch: 600, loss is 3.410357856750488 and perplexity is 30.276076819500258
At time: 193.7148721218109 and batch: 650, loss is 3.2395291423797605 and perplexity is 25.521701829950636
At time: 194.40358996391296 and batch: 700, loss is 3.230798907279968 and perplexity is 25.299861142507343
At time: 195.05703830718994 and batch: 750, loss is 3.3380893421173097 and perplexity is 28.165261084299956
At time: 195.71403551101685 and batch: 800, loss is 3.2961146068572997 and perplexity is 27.007500044516377
At time: 196.40112209320068 and batch: 850, loss is 3.353872504234314 and perplexity is 28.613324594249843
At time: 197.06922578811646 and batch: 900, loss is 3.3252625703811645 and perplexity is 27.806298794699167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2591009270654965 and perplexity of 70.74634872507805
finished 15 epochs...
Completing Train Step...
At time: 198.75973844528198 and batch: 50, loss is 3.610788540840149 and perplexity is 36.99521355287153
At time: 199.43583750724792 and batch: 100, loss is 3.489086904525757 and perplexity is 32.75602466913644
At time: 200.08582949638367 and batch: 150, loss is 3.5042904567718507 and perplexity is 33.25783760511209
At time: 200.7354724407196 and batch: 200, loss is 3.4027440977096557 and perplexity is 30.04643738489142
At time: 201.38011813163757 and batch: 250, loss is 3.542595176696777 and perplexity is 34.55648310317295
At time: 202.03321051597595 and batch: 300, loss is 3.5086898183822632 and perplexity is 33.40447317406039
At time: 202.68494248390198 and batch: 350, loss is 3.4916738986968996 and perplexity is 32.84087401909072
At time: 203.3362419605255 and batch: 400, loss is 3.419467792510986 and perplexity is 30.553150077987404
At time: 204.00402975082397 and batch: 450, loss is 3.4441552686691286 and perplexity is 31.316817972810824
At time: 204.65996503829956 and batch: 500, loss is 3.319746470451355 and perplexity is 27.653338731214554
At time: 205.31685423851013 and batch: 550, loss is 3.371231780052185 and perplexity is 29.11436747659859
At time: 205.97313404083252 and batch: 600, loss is 3.4090374946594237 and perplexity is 30.236127814781117
At time: 206.62741589546204 and batch: 650, loss is 3.238830633163452 and perplexity is 25.503880910769272
At time: 207.2847397327423 and batch: 700, loss is 3.2304321336746216 and perplexity is 25.290583522718375
At time: 207.9410080909729 and batch: 750, loss is 3.338306441307068 and perplexity is 28.171376403452086
At time: 208.5984878540039 and batch: 800, loss is 3.296731505393982 and perplexity is 27.024166071868688
At time: 209.25522637367249 and batch: 850, loss is 3.3550844192504883 and perplexity is 28.64802253321807
At time: 209.91305589675903 and batch: 900, loss is 3.32690710067749 and perplexity is 27.85206471690412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258876434744221 and perplexity of 70.73046849559246
finished 16 epochs...
Completing Train Step...
At time: 211.58214259147644 and batch: 50, loss is 3.6089508819580076 and perplexity is 36.92729139807928
At time: 212.23753094673157 and batch: 100, loss is 3.4869372415542603 and perplexity is 32.68568588523796
At time: 212.89533591270447 and batch: 150, loss is 3.5019242572784424 and perplexity is 33.179235956882685
At time: 213.54764223098755 and batch: 200, loss is 3.400401096343994 and perplexity is 29.97612094898578
At time: 214.2001826763153 and batch: 250, loss is 3.540163474082947 and perplexity is 34.47255409943232
At time: 214.87005281448364 and batch: 300, loss is 3.5063027000427245 and perplexity is 33.32482784275994
At time: 215.53896045684814 and batch: 350, loss is 3.4895134115219117 and perplexity is 32.76999832254269
At time: 216.18959259986877 and batch: 400, loss is 3.417533392906189 and perplexity is 30.494105203154184
At time: 216.84391808509827 and batch: 450, loss is 3.442401294708252 and perplexity is 31.261937233312718
At time: 217.49675226211548 and batch: 500, loss is 3.3182835245132445 and perplexity is 27.61291296920466
At time: 218.1525161266327 and batch: 550, loss is 3.369945011138916 and perplexity is 29.076928106674817
At time: 218.8330430984497 and batch: 600, loss is 3.408102765083313 and perplexity is 30.207878416676373
At time: 219.4857795238495 and batch: 650, loss is 3.238234133720398 and perplexity is 25.48867239639641
At time: 220.13956689834595 and batch: 700, loss is 3.2301297283172605 and perplexity is 25.282936671053047
At time: 220.79076313972473 and batch: 750, loss is 3.3383121728897094 and perplexity is 28.171537870486794
At time: 221.4435429573059 and batch: 800, loss is 3.296971778869629 and perplexity is 27.030660042310416
At time: 222.0963740348816 and batch: 850, loss is 3.355642113685608 and perplexity is 28.66400383188849
At time: 222.75043535232544 and batch: 900, loss is 3.3276347589492796 and perplexity is 27.87233887761759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258877688891267 and perplexity of 70.7305572020562
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 224.4299759864807 and batch: 50, loss is 3.6080007314682008 and perplexity is 36.892221577512124
At time: 225.08616042137146 and batch: 100, loss is 3.4863338041305543 and perplexity is 32.66596806898776
At time: 225.7359974384308 and batch: 150, loss is 3.501904249191284 and perplexity is 33.17857211047899
At time: 226.38704419136047 and batch: 200, loss is 3.400846214294434 and perplexity is 29.989466828529647
At time: 227.0391674041748 and batch: 250, loss is 3.5407540130615236 and perplexity is 34.49291749841776
At time: 227.70642042160034 and batch: 300, loss is 3.506032280921936 and perplexity is 33.31581739046848
At time: 228.36446118354797 and batch: 350, loss is 3.488569178581238 and perplexity is 32.739070414533366
At time: 229.02853322029114 and batch: 400, loss is 3.416894063949585 and perplexity is 30.47461566946754
At time: 229.6811385154724 and batch: 450, loss is 3.4417479038238525 and perplexity is 31.241517640210265
At time: 230.33396124839783 and batch: 500, loss is 3.3168642711639404 and perplexity is 27.57375104690736
At time: 230.9871747493744 and batch: 550, loss is 3.3685730123519897 and perplexity is 29.03706195099714
At time: 231.63999152183533 and batch: 600, loss is 3.4066218757629394 and perplexity is 30.163176999233126
At time: 232.29197216033936 and batch: 650, loss is 3.235829682350159 and perplexity is 25.427459744041396
At time: 232.9424967765808 and batch: 700, loss is 3.2279188299179076 and perplexity is 25.227100413726347
At time: 233.5980520248413 and batch: 750, loss is 3.33551824092865 and perplexity is 28.092938362406457
At time: 234.24842071533203 and batch: 800, loss is 3.2941736650466917 and perplexity is 26.955130897654
At time: 234.9052538871765 and batch: 850, loss is 3.352602367401123 and perplexity is 28.577004827182332
At time: 235.5568413734436 and batch: 900, loss is 3.323993306159973 and perplexity is 27.771027643455703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2582945105147685 and perplexity of 70.68932069582637
finished 18 epochs...
Completing Train Step...
At time: 237.23205709457397 and batch: 50, loss is 3.607276511192322 and perplexity is 36.86551315518134
At time: 237.8977632522583 and batch: 100, loss is 3.485657181739807 and perplexity is 32.643873019422436
At time: 238.57592487335205 and batch: 150, loss is 3.500929789543152 and perplexity is 33.14625667837413
At time: 239.22571206092834 and batch: 200, loss is 3.399863543510437 and perplexity is 29.96001153045031
At time: 239.9099678993225 and batch: 250, loss is 3.5398522329330446 and perplexity is 34.46182649157745
At time: 240.56684851646423 and batch: 300, loss is 3.5052504968643188 and perplexity is 33.289781793999296
At time: 241.21683716773987 and batch: 350, loss is 3.4879610919952393 and perplexity is 32.71916827670101
At time: 241.866375207901 and batch: 400, loss is 3.416255898475647 and perplexity is 30.45517402606835
At time: 242.5157823562622 and batch: 450, loss is 3.4411450958251955 and perplexity is 31.2226906785782
At time: 243.1633951663971 and batch: 500, loss is 3.3164806032180785 and perplexity is 27.563173911667107
At time: 243.81949758529663 and batch: 550, loss is 3.3681722354888914 and perplexity is 29.02542690008017
At time: 244.48884630203247 and batch: 600, loss is 3.4063443613052367 and perplexity is 30.154807442915747
At time: 245.14069294929504 and batch: 650, loss is 3.2357123374938963 and perplexity is 25.424476137491475
At time: 245.80301690101624 and batch: 700, loss is 3.2279243993759157 and perplexity is 25.227240915394027
At time: 246.4683268070221 and batch: 750, loss is 3.3357057905197145 and perplexity is 28.098207675621452
At time: 247.1330361366272 and batch: 800, loss is 3.294428687095642 and perplexity is 26.96200592697002
At time: 247.7909653186798 and batch: 850, loss is 3.35295334815979 and perplexity is 28.587036566387752
At time: 248.46924424171448 and batch: 900, loss is 3.3243249464035034 and perplexity is 27.78023916119703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.258029467438998 and perplexity of 70.67058746351145
finished 19 epochs...
Completing Train Step...
At time: 250.13959527015686 and batch: 50, loss is 3.6066990518569946 and perplexity is 36.84423096584909
At time: 250.8454623222351 and batch: 100, loss is 3.485083746910095 and perplexity is 32.6251592517323
At time: 251.5398452281952 and batch: 150, loss is 3.5001828384399416 and perplexity is 33.12150728982318
At time: 252.23137855529785 and batch: 200, loss is 3.399109983444214 and perplexity is 29.937443366478895
At time: 252.8734028339386 and batch: 250, loss is 3.539126319885254 and perplexity is 34.43681927970533
At time: 253.5133056640625 and batch: 300, loss is 3.5046042299270628 and perplexity is 33.268274659101024
At time: 254.15470910072327 and batch: 350, loss is 3.487417678833008 and perplexity is 32.70139308007751
At time: 254.79643440246582 and batch: 400, loss is 3.4157279396057127 and perplexity is 30.439099190605276
At time: 255.44096422195435 and batch: 450, loss is 3.440655131340027 and perplexity is 31.20739641614297
At time: 256.0881266593933 and batch: 500, loss is 3.316131386756897 and perplexity is 27.553550078113098
At time: 256.73270893096924 and batch: 550, loss is 3.367834701538086 and perplexity is 29.015631486300126
At time: 257.3794057369232 and batch: 600, loss is 3.4061150312423707 and perplexity is 30.147892831923546
At time: 258.0303523540497 and batch: 650, loss is 3.235601878166199 and perplexity is 25.421667922049917
At time: 258.67885637283325 and batch: 700, loss is 3.227917642593384 and perplexity is 25.22707046098915
At time: 259.3371481895447 and batch: 750, loss is 3.3358185625076295 and perplexity is 28.101376545034377
At time: 259.99430203437805 and batch: 800, loss is 3.2945975732803343 and perplexity is 26.966559821816812
At time: 260.6500053405762 and batch: 850, loss is 3.3532081747055056 and perplexity is 28.594322230419078
At time: 261.3047933578491 and batch: 900, loss is 3.32458881855011 and perplexity is 27.787570559771133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.257904470783391 and perplexity of 70.66175442849013
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
809.5650353431702


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}, {'params': {'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.66175442849013}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.6196663492825968, 'rnn_dropout': 0.0427385332517114, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.035151481628418 and batch: 50, loss is 6.996465349197388 and perplexity is 1092.763785624275
At time: 1.8453853130340576 and batch: 100, loss is 6.104412307739258 and perplexity is 447.8293782443559
At time: 2.6560654640197754 and batch: 150, loss is 5.94006010055542 and perplexity is 379.9577645246213
At time: 3.468353509902954 and batch: 200, loss is 5.759650955200195 and perplexity is 317.23757946329994
At time: 4.280198574066162 and batch: 250, loss is 5.79374529838562 and perplexity is 328.2400821426247
At time: 5.0968873500823975 and batch: 300, loss is 5.677547483444214 and perplexity is 292.23184689746074
At time: 5.911186218261719 and batch: 350, loss is 5.634434089660645 and perplexity is 279.90047391007363
At time: 6.727035760879517 and batch: 400, loss is 5.469143486022949 and perplexity is 237.25689186453207
At time: 7.541090726852417 and batch: 450, loss is 5.449993734359741 and perplexity is 232.75670753329598
At time: 8.36884331703186 and batch: 500, loss is 5.381726264953613 and perplexity is 217.39723692149704
At time: 9.182569742202759 and batch: 550, loss is 5.419333333969116 and perplexity is 225.72858674808754
At time: 9.994778394699097 and batch: 600, loss is 5.323394889831543 and perplexity is 205.07892188005604
At time: 10.807948112487793 and batch: 650, loss is 5.210492305755615 and perplexity is 183.18421864381912
At time: 11.623542070388794 and batch: 700, loss is 5.303364353179932 and perplexity is 201.01194882443818
At time: 12.436017751693726 and batch: 750, loss is 5.265510940551758 and perplexity is 193.54517360604083
At time: 13.248421669006348 and batch: 800, loss is 5.234191074371338 and perplexity is 187.57730886354497
At time: 14.063295364379883 and batch: 850, loss is 5.263452768325806 and perplexity is 193.14723395989648
At time: 14.876801490783691 and batch: 900, loss is 5.16361234664917 and perplexity is 174.79473570243982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.052608124197346 and perplexity of 156.4299215697199
finished 1 epochs...
Completing Train Step...
At time: 16.615488290786743 and batch: 50, loss is 4.997023506164551 and perplexity is 147.97206503195585
At time: 17.264397382736206 and batch: 100, loss is 4.851410160064697 and perplexity is 127.92065131132675
At time: 17.901824474334717 and batch: 150, loss is 4.817875833511352 and perplexity is 123.70204776071635
At time: 18.537824869155884 and batch: 200, loss is 4.705782270431518 and perplexity is 110.58475830688614
At time: 19.18467354774475 and batch: 250, loss is 4.805138502120972 and perplexity is 122.13640600105376
At time: 19.850291967391968 and batch: 300, loss is 4.735926771163941 and perplexity is 113.96903300242131
At time: 20.489758491516113 and batch: 350, loss is 4.723121824264527 and perplexity is 112.51896939385566
At time: 21.128854036331177 and batch: 400, loss is 4.582350988388061 and perplexity is 97.74391911498212
At time: 21.766572952270508 and batch: 450, loss is 4.601517915725708 and perplexity is 99.63543911644068
At time: 22.404208421707153 and batch: 500, loss is 4.497792062759399 and perplexity is 89.81859837880077
At time: 23.064578771591187 and batch: 550, loss is 4.5653533840179445 and perplexity is 96.09654700084404
At time: 23.720593452453613 and batch: 600, loss is 4.52377571105957 and perplexity is 92.18299808961554
At time: 24.357255935668945 and batch: 650, loss is 4.3820052909851075 and perplexity is 79.99829252312006
At time: 24.991312980651855 and batch: 700, loss is 4.4322246646881105 and perplexity is 84.1183440176658
At time: 25.628013849258423 and batch: 750, loss is 4.47780626296997 and perplexity is 88.04132116559579
At time: 26.26478886604309 and batch: 800, loss is 4.427682685852051 and perplexity is 83.7371466299748
At time: 26.903299570083618 and batch: 850, loss is 4.489135074615478 and perplexity is 89.0443958038991
At time: 27.542248010635376 and batch: 900, loss is 4.418489942550659 and perplexity is 82.97089988708439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.541434510113442 and perplexity of 93.82529696526039
finished 2 epochs...
Completing Train Step...
At time: 29.177690267562866 and batch: 50, loss is 4.462464036941529 and perplexity is 86.70088028301014
At time: 29.84571361541748 and batch: 100, loss is 4.336881976127625 and perplexity is 76.46873589034541
At time: 30.510541915893555 and batch: 150, loss is 4.332974700927735 and perplexity is 76.17053445195819
At time: 31.189521074295044 and batch: 200, loss is 4.238244104385376 and perplexity is 69.28608583157548
At time: 31.847108840942383 and batch: 250, loss is 4.369583559036255 and perplexity is 79.01072154644514
At time: 32.51267910003662 and batch: 300, loss is 4.333001823425293 and perplexity is 76.17260041510978
At time: 33.176828384399414 and batch: 350, loss is 4.326482019424438 and perplexity is 75.67758544396293
At time: 33.82522177696228 and batch: 400, loss is 4.225284457206726 and perplexity is 68.39395593540347
At time: 34.46323871612549 and batch: 450, loss is 4.260694427490234 and perplexity is 70.85917293065584
At time: 35.10375142097473 and batch: 500, loss is 4.1379296016693115 and perplexity is 62.67292910335787
At time: 35.78059267997742 and batch: 550, loss is 4.223726539611817 and perplexity is 68.28748674471333
At time: 36.41521191596985 and batch: 600, loss is 4.215111989974975 and perplexity is 67.70174736339138
At time: 37.09149384498596 and batch: 650, loss is 4.064649658203125 and perplexity is 58.24449946508712
At time: 37.73429799079895 and batch: 700, loss is 4.09168770313263 and perplexity is 59.84080003421786
At time: 38.37289786338806 and batch: 750, loss is 4.178922328948975 and perplexity is 65.29544826977462
At time: 39.01182508468628 and batch: 800, loss is 4.127128963470459 and perplexity is 61.99966386328397
At time: 39.64923453330994 and batch: 850, loss is 4.207372641563415 and perplexity is 67.17980231503012
At time: 40.28652906417847 and batch: 900, loss is 4.151344647407532 and perplexity is 63.51935403465334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3937093656357025 and perplexity of 80.94009924771619
finished 3 epochs...
Completing Train Step...
At time: 41.92467784881592 and batch: 50, loss is 4.224471554756165 and perplexity is 68.3383809126165
At time: 42.56253409385681 and batch: 100, loss is 4.096528635025025 and perplexity is 60.13118757763668
At time: 43.20136523246765 and batch: 150, loss is 4.097127485275268 and perplexity is 60.167207938689636
At time: 43.8404757976532 and batch: 200, loss is 3.9979850816726685 and perplexity is 54.48824997700461
At time: 44.48181080818176 and batch: 250, loss is 4.143968405723572 and perplexity is 63.05254369717242
At time: 45.135737895965576 and batch: 300, loss is 4.116852960586548 and perplexity is 61.36581742020191
At time: 45.79474115371704 and batch: 350, loss is 4.115163249969482 and perplexity is 61.26221450110818
At time: 46.45312166213989 and batch: 400, loss is 4.025852837562561 and perplexity is 56.02807125565791
At time: 47.10981488227844 and batch: 450, loss is 4.065962495803833 and perplexity is 58.32101524942883
At time: 47.76939249038696 and batch: 500, loss is 3.9360111808776854 and perplexity is 51.21391030948396
At time: 48.433048248291016 and batch: 550, loss is 4.022743535041809 and perplexity is 55.85413358502103
At time: 49.096290826797485 and batch: 600, loss is 4.028602433204651 and perplexity is 56.182337784313184
At time: 49.78191924095154 and batch: 650, loss is 3.8801578760147093 and perplexity is 48.431860696950636
At time: 50.45484471321106 and batch: 700, loss is 3.894130992889404 and perplexity is 49.11335495807855
At time: 51.11803460121155 and batch: 750, loss is 3.9929969120025635 and perplexity is 54.217130099230985
At time: 51.7819447517395 and batch: 800, loss is 3.9490664625167846 and perplexity is 51.88690584678044
At time: 52.44458723068237 and batch: 850, loss is 4.03046404838562 and perplexity is 56.287025090740464
At time: 53.106322050094604 and batch: 900, loss is 3.9850009441375733 and perplexity is 53.785340258626405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339764320687072 and perplexity of 76.68946308831072
finished 4 epochs...
Completing Train Step...
At time: 54.74874711036682 and batch: 50, loss is 4.064028263092041 and perplexity is 58.20831786057229
At time: 55.434882402420044 and batch: 100, loss is 3.9381394481658933 and perplexity is 51.32302326908237
At time: 56.133169174194336 and batch: 150, loss is 3.942513651847839 and perplexity is 51.54801234171099
At time: 56.801636695861816 and batch: 200, loss is 3.8406147003173827 and perplexity is 46.554082455471075
At time: 57.497984647750854 and batch: 250, loss is 3.9918927145004273 and perplexity is 54.157296719615914
At time: 58.16173958778381 and batch: 300, loss is 3.969276814460754 and perplexity is 52.94622704521371
At time: 58.82693099975586 and batch: 350, loss is 3.9675960397720336 and perplexity is 52.85731111169042
At time: 59.481714487075806 and batch: 400, loss is 3.886299605369568 and perplexity is 48.730231395644076
At time: 60.15641212463379 and batch: 450, loss is 3.9252903032302857 and perplexity is 50.667784945648116
At time: 60.83726119995117 and batch: 500, loss is 3.798565034866333 and perplexity is 44.63708585277538
At time: 61.49991750717163 and batch: 550, loss is 3.8834244632720947 and perplexity is 48.59032627586506
At time: 62.16255450248718 and batch: 600, loss is 3.895333938598633 and perplexity is 49.172471207381605
At time: 62.85053086280823 and batch: 650, loss is 3.7461048221588134 and perplexity is 42.35577697890683
At time: 63.53693246841431 and batch: 700, loss is 3.7563536262512205 and perplexity is 42.79210514102621
At time: 64.24083638191223 and batch: 750, loss is 3.8591997623443604 and perplexity is 47.42738300123493
At time: 64.97572541236877 and batch: 800, loss is 3.819363918304443 and perplexity is 45.57520954246929
At time: 65.68286633491516 and batch: 850, loss is 3.902393689155579 and perplexity is 49.52084485699481
At time: 66.41430974006653 and batch: 900, loss is 3.8591574001312257 and perplexity is 47.42537391488278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3163514855789815 and perplexity of 74.91480133196684
finished 5 epochs...
Completing Train Step...
At time: 68.25041770935059 and batch: 50, loss is 3.94043728351593 and perplexity is 51.44109072405655
At time: 68.89731001853943 and batch: 100, loss is 3.8196042823791503 and perplexity is 45.58616550219736
At time: 69.54175901412964 and batch: 150, loss is 3.8215524482727052 and perplexity is 45.67506147898577
At time: 70.20782732963562 and batch: 200, loss is 3.7222807216644287 and perplexity is 41.358614110181286
At time: 70.88040280342102 and batch: 250, loss is 3.8756164264678956 and perplexity is 48.21240853777051
At time: 71.5366096496582 and batch: 300, loss is 3.8567963457107544 and perplexity is 47.31353211040787
At time: 72.1877965927124 and batch: 350, loss is 3.852944712638855 and perplexity is 47.13164824519335
At time: 72.84005212783813 and batch: 400, loss is 3.7765175008773806 and perplexity is 43.66371779661123
At time: 73.52783584594727 and batch: 450, loss is 3.81793399810791 and perplexity is 45.510087200855196
At time: 74.2273383140564 and batch: 500, loss is 3.6950960254669187 and perplexity is 40.24943737342676
At time: 74.88511967658997 and batch: 550, loss is 3.7731020736694334 and perplexity is 43.51484192891462
At time: 75.54506826400757 and batch: 600, loss is 3.789495153427124 and perplexity is 44.23406322284299
At time: 76.20397710800171 and batch: 650, loss is 3.642360444068909 and perplexity is 38.18185657708689
At time: 76.8723816871643 and batch: 700, loss is 3.651056671142578 and perplexity is 38.51534260545334
At time: 77.57190656661987 and batch: 750, loss is 3.7564399337768553 and perplexity is 42.79579858112118
At time: 78.31184720993042 and batch: 800, loss is 3.7168380498886107 and perplexity is 41.134124215050626
At time: 79.05327653884888 and batch: 850, loss is 3.8062585020065307 and perplexity is 44.98182422227259
At time: 79.74386620521545 and batch: 900, loss is 3.7589423942565916 and perplexity is 42.90302748786152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314890822319136 and perplexity of 74.80545591190248
finished 6 epochs...
Completing Train Step...
At time: 81.40369772911072 and batch: 50, loss is 3.842651853561401 and perplexity is 46.64901692076307
At time: 82.04425692558289 and batch: 100, loss is 3.725704698562622 and perplexity is 41.500467762663874
At time: 82.68890571594238 and batch: 150, loss is 3.733089985847473 and perplexity is 41.80809519981819
At time: 83.33459663391113 and batch: 200, loss is 3.633590292930603 and perplexity is 37.848460030163004
At time: 84.00509595870972 and batch: 250, loss is 3.7843804025650023 and perplexity is 44.00839462114641
At time: 84.67510437965393 and batch: 300, loss is 3.7663748836517335 and perplexity is 43.22309174838382
At time: 85.32088541984558 and batch: 350, loss is 3.763421335220337 and perplexity is 43.09561859528308
At time: 85.97060823440552 and batch: 400, loss is 3.685678062438965 and perplexity is 39.872149092610755
At time: 86.6365270614624 and batch: 450, loss is 3.72862229347229 and perplexity is 41.62172612152614
At time: 87.36718416213989 and batch: 500, loss is 3.6074533081054687 and perplexity is 36.872031440298116
At time: 88.10097026824951 and batch: 550, loss is 3.68601713180542 and perplexity is 39.88567080921319
At time: 88.81722450256348 and batch: 600, loss is 3.704767231941223 and perplexity is 40.640586385419674
At time: 89.54224705696106 and batch: 650, loss is 3.5587159252166747 and perplexity is 35.118073947331794
At time: 90.27625226974487 and batch: 700, loss is 3.566014862060547 and perplexity is 35.37533627978487
At time: 91.01095485687256 and batch: 750, loss is 3.673978090286255 and perplexity is 39.40836448265486
At time: 91.71695590019226 and batch: 800, loss is 3.634105305671692 and perplexity is 37.86795748959818
At time: 92.37935709953308 and batch: 850, loss is 3.7228613328933715 and perplexity is 41.382634358485326
At time: 93.06602191925049 and batch: 900, loss is 3.674532814025879 and perplexity is 39.43023130243443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319327576519692 and perplexity of 75.13808668749256
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 94.72522449493408 and batch: 50, loss is 3.7909623527526857 and perplexity is 44.2990110446196
At time: 95.38302230834961 and batch: 100, loss is 3.6758814811706544 and perplexity is 39.48344543591371
At time: 96.02941966056824 and batch: 150, loss is 3.681480288505554 and perplexity is 39.70512563310724
At time: 96.67687392234802 and batch: 200, loss is 3.5621598148345948 and perplexity is 35.239225213652894
At time: 97.32732105255127 and batch: 250, loss is 3.711356177330017 and perplexity is 40.90924911974321
At time: 97.98316597938538 and batch: 300, loss is 3.674079566001892 and perplexity is 39.412363677549976
At time: 98.66795039176941 and batch: 350, loss is 3.6611612272262573 and perplexity is 38.90649593213147
At time: 99.33515787124634 and batch: 400, loss is 3.578317446708679 and perplexity is 35.81323245311025
At time: 99.98800230026245 and batch: 450, loss is 3.607811598777771 and perplexity is 36.88524471218668
At time: 100.6508719921112 and batch: 500, loss is 3.4752786684036256 and perplexity is 32.306830175107415
At time: 101.30401849746704 and batch: 550, loss is 3.536617660522461 and perplexity is 34.35053730189061
At time: 101.95860147476196 and batch: 600, loss is 3.5559102725982665 and perplexity is 35.0196829212699
At time: 102.61060667037964 and batch: 650, loss is 3.3887751626968385 and perplexity is 29.629638549134388
At time: 103.26486992835999 and batch: 700, loss is 3.3790531301498414 and perplexity is 29.342973977857906
At time: 103.93541812896729 and batch: 750, loss is 3.4826708602905274 and perplexity is 32.546533337367705
At time: 104.61187934875488 and batch: 800, loss is 3.422789607048035 and perplexity is 30.65481073150666
At time: 105.26458597183228 and batch: 850, loss is 3.492994632720947 and perplexity is 32.884276734199396
At time: 105.92032790184021 and batch: 900, loss is 3.436209659576416 and perplexity is 31.068972723830196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27659669640946 and perplexity of 71.99500174582309
finished 8 epochs...
Completing Train Step...
At time: 107.55668520927429 and batch: 50, loss is 3.6950849008560183 and perplexity is 40.24898961658758
At time: 108.21385955810547 and batch: 100, loss is 3.5772504949569703 and perplexity is 35.77504183940412
At time: 108.8608627319336 and batch: 150, loss is 3.5850366878509523 and perplexity is 36.05468046333773
At time: 109.51074647903442 and batch: 200, loss is 3.469526572227478 and perplexity is 32.12153161925914
At time: 110.16621017456055 and batch: 250, loss is 3.6200545120239256 and perplexity is 37.33960322392053
At time: 110.81572699546814 and batch: 300, loss is 3.5883804512023927 and perplexity is 36.17544056660703
At time: 111.46701216697693 and batch: 350, loss is 3.5793595552444457 and perplexity is 35.850573181509624
At time: 112.11802339553833 and batch: 400, loss is 3.49923969745636 and perplexity is 33.090283765267465
At time: 112.77191281318665 and batch: 450, loss is 3.5340436553955077 and perplexity is 34.26223253997218
At time: 113.42638397216797 and batch: 500, loss is 3.4064653730392456 and perplexity is 30.158456749253112
At time: 114.11465644836426 and batch: 550, loss is 3.469864258766174 and perplexity is 32.13238045973987
At time: 114.78052401542664 and batch: 600, loss is 3.4963094663619994 and perplexity is 32.99346350899694
At time: 115.44737505912781 and batch: 650, loss is 3.3337222480773927 and perplexity is 28.04252892697961
At time: 116.1482765674591 and batch: 700, loss is 3.329835796356201 and perplexity is 27.93375450235499
At time: 116.84619784355164 and batch: 750, loss is 3.4405144834518433 and perplexity is 31.203007470396543
At time: 117.5408730506897 and batch: 800, loss is 3.3860676956176756 and perplexity is 29.549525778477765
At time: 118.23041129112244 and batch: 850, loss is 3.461763710975647 and perplexity is 31.873141980623515
At time: 118.90800762176514 and batch: 900, loss is 3.4123654794692992 and perplexity is 30.336920814616782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.285237351508989 and perplexity of 72.61978108899567
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 120.5710985660553 and batch: 50, loss is 3.663475818634033 and perplexity is 38.99665287131097
At time: 121.2569625377655 and batch: 100, loss is 3.558929696083069 and perplexity is 35.125581970895375
At time: 121.91183066368103 and batch: 150, loss is 3.576236548423767 and perplexity is 35.738786243479915
At time: 122.58717393875122 and batch: 200, loss is 3.453229923248291 and perplexity is 31.60230064765703
At time: 123.2370011806488 and batch: 250, loss is 3.6067425918579104 and perplexity is 36.845835198622986
At time: 123.88688683509827 and batch: 300, loss is 3.568760380744934 and perplexity is 35.47259337600229
At time: 124.53746891021729 and batch: 350, loss is 3.5527174043655396 and perplexity is 34.90804800078369
At time: 125.18796300888062 and batch: 400, loss is 3.4714734745025635 and perplexity is 32.184130018958406
At time: 125.85384035110474 and batch: 450, loss is 3.5022248029708862 and perplexity is 33.18920933198049
At time: 126.54804277420044 and batch: 500, loss is 3.3702915000915525 and perplexity is 29.087004686651397
At time: 127.21998691558838 and batch: 550, loss is 3.425415759086609 and perplexity is 30.735420725923486
At time: 127.87904787063599 and batch: 600, loss is 3.4554879570007326 and perplexity is 31.673740335433155
At time: 128.57345294952393 and batch: 650, loss is 3.2858236837387085 and perplexity is 26.730993133853183
At time: 129.24731063842773 and batch: 700, loss is 3.2746142101287843 and perplexity is 26.43302591958028
At time: 129.9020414352417 and batch: 750, loss is 3.384137759208679 and perplexity is 29.492552068312047
At time: 130.59482955932617 and batch: 800, loss is 3.3216868114471434 and perplexity is 27.70704772805851
At time: 131.25860810279846 and batch: 850, loss is 3.3880059576034545 and perplexity is 29.606856043580244
At time: 131.91309070587158 and batch: 900, loss is 3.345980920791626 and perplexity is 28.38840879380225
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271675423400043 and perplexity of 71.64156507940307
finished 10 epochs...
Completing Train Step...
At time: 133.55581712722778 and batch: 50, loss is 3.640180826187134 and perplexity is 38.098725349798066
At time: 134.21630954742432 and batch: 100, loss is 3.522085738182068 and perplexity is 33.8549674736368
At time: 134.8668782711029 and batch: 150, loss is 3.536285328865051 and perplexity is 34.33912342759709
At time: 135.5266044139862 and batch: 200, loss is 3.416923470497131 and perplexity is 30.475511835878695
At time: 136.2082760334015 and batch: 250, loss is 3.5705544900894166 and perplexity is 35.53629221150291
At time: 136.86705207824707 and batch: 300, loss is 3.534935779571533 and perplexity is 34.292812344408944
At time: 137.56242561340332 and batch: 350, loss is 3.5203617906570432 and perplexity is 33.7966535657518
At time: 138.21505069732666 and batch: 400, loss is 3.4410727262496947 and perplexity is 31.220431187467995
At time: 138.86557745933533 and batch: 450, loss is 3.4752667713165284 and perplexity is 32.30644582022134
At time: 139.51205444335938 and batch: 500, loss is 3.3457614088058474 and perplexity is 28.38217788171979
At time: 140.15378546714783 and batch: 550, loss is 3.404163522720337 and perplexity is 30.08911633221435
At time: 140.80378437042236 and batch: 600, loss is 3.4367139196395873 and perplexity is 31.08464351672403
At time: 141.45302724838257 and batch: 650, loss is 3.2699087381362917 and perplexity is 26.30893823096742
At time: 142.1001899242401 and batch: 700, loss is 3.2625547742843626 and perplexity is 26.116172913563148
At time: 142.7477855682373 and batch: 750, loss is 3.374916362762451 and perplexity is 29.221839645081953
At time: 143.39618492126465 and batch: 800, loss is 3.3154318571090697 and perplexity is 27.534282292909772
At time: 144.05111980438232 and batch: 850, loss is 3.3851278734207155 and perplexity is 29.521767524196427
At time: 144.70611953735352 and batch: 900, loss is 3.346395506858826 and perplexity is 28.40018067261799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2734571483037245 and perplexity of 71.76932442224702
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 146.35611128807068 and batch: 50, loss is 3.632716751098633 and perplexity is 37.8154122534602
At time: 147.01653838157654 and batch: 100, loss is 3.520475468635559 and perplexity is 33.800495719389744
At time: 147.66809511184692 and batch: 150, loss is 3.5382852220535277 and perplexity is 34.40786672335068
At time: 148.31989216804504 and batch: 200, loss is 3.416107635498047 and perplexity is 30.45065898599883
At time: 148.9690661430359 and batch: 250, loss is 3.5707497882843016 and perplexity is 35.543233062970536
At time: 149.61717867851257 and batch: 300, loss is 3.5364019584655764 and perplexity is 34.34312861940226
At time: 150.2925717830658 and batch: 350, loss is 3.5187880992889404 and perplexity is 33.74350989060358
At time: 150.98508548736572 and batch: 400, loss is 3.436164321899414 and perplexity is 31.06756416071079
At time: 151.63959550857544 and batch: 450, loss is 3.467588801383972 and perplexity is 32.05934772036984
At time: 152.2925145626068 and batch: 500, loss is 3.337719955444336 and perplexity is 28.154859133503866
At time: 152.94483304023743 and batch: 550, loss is 3.3929251194000245 and perplexity is 29.7528557624559
At time: 153.59906935691833 and batch: 600, loss is 3.422677779197693 and perplexity is 30.65138286158915
At time: 154.28437209129333 and batch: 650, loss is 3.255208520889282 and perplexity is 25.92501987861556
At time: 154.9492666721344 and batch: 700, loss is 3.2449653005599974 and perplexity is 25.660819628708495
At time: 155.64252066612244 and batch: 750, loss is 3.353637108802795 and perplexity is 28.606589941044195
At time: 156.29629158973694 and batch: 800, loss is 3.29374972820282 and perplexity is 26.943706046413894
At time: 156.97979140281677 and batch: 850, loss is 3.359906916618347 and perplexity is 28.786511208718917
At time: 157.64414644241333 and batch: 900, loss is 3.323640060424805 and perplexity is 27.761219378845226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268114899935788 and perplexity of 71.38693717922854
finished 12 epochs...
Completing Train Step...
At time: 159.29150772094727 and batch: 50, loss is 3.6232207298278807 and perplexity is 37.458015901681726
At time: 159.94220638275146 and batch: 100, loss is 3.505667905807495 and perplexity is 33.30368014708314
At time: 160.5920159816742 and batch: 150, loss is 3.523202748298645 and perplexity is 33.89280494328524
At time: 161.24287223815918 and batch: 200, loss is 3.4024474000930787 and perplexity is 30.037524000887974
At time: 161.89414167404175 and batch: 250, loss is 3.556797938346863 and perplexity is 35.050782495297064
At time: 162.54484248161316 and batch: 300, loss is 3.522566866874695 and perplexity is 33.871259988960325
At time: 163.19679856300354 and batch: 350, loss is 3.5058388233184816 and perplexity is 33.30937281567509
At time: 163.84802794456482 and batch: 400, loss is 3.4250040864944458 and perplexity is 30.722770399676982
At time: 164.4991021156311 and batch: 450, loss is 3.457797198295593 and perplexity is 31.7469671612427
At time: 165.15008211135864 and batch: 500, loss is 3.329404926300049 and perplexity is 27.921721276548524
At time: 165.79954266548157 and batch: 550, loss is 3.3861478662490843 and perplexity is 29.55189487758207
At time: 166.45098900794983 and batch: 600, loss is 3.417167458534241 and perplexity is 30.482948403371307
At time: 167.11712837219238 and batch: 650, loss is 3.251205039024353 and perplexity is 25.821437016090414
At time: 167.76551985740662 and batch: 700, loss is 3.242860631942749 and perplexity is 25.606868901059908
At time: 168.41181063652039 and batch: 750, loss is 3.3529377079010008 and perplexity is 28.58658946123426
At time: 169.06086087226868 and batch: 800, loss is 3.29412802696228 and perplexity is 26.953900745185884
At time: 169.71080994606018 and batch: 850, loss is 3.362139344215393 and perplexity is 28.850846796315267
At time: 170.38003540039062 and batch: 900, loss is 3.327310299873352 and perplexity is 27.863296911254768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.268134966288527 and perplexity of 71.38836966906334
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 172.0225694179535 and batch: 50, loss is 3.6203840017318725 and perplexity is 37.351908265962656
At time: 172.6789903640747 and batch: 100, loss is 3.504183235168457 and perplexity is 33.25427183760573
At time: 173.32660126686096 and batch: 150, loss is 3.5233092880249024 and perplexity is 33.89641606580658
At time: 173.97446537017822 and batch: 200, loss is 3.401784143447876 and perplexity is 30.017608018922743
At time: 174.62201571464539 and batch: 250, loss is 3.5561149597167967 and perplexity is 35.02685173291556
At time: 175.26949954032898 and batch: 300, loss is 3.5217677545547486 and perplexity is 33.84420385969394
At time: 175.9161729812622 and batch: 350, loss is 3.5055054998397828 and perplexity is 33.298271869860606
At time: 176.56501960754395 and batch: 400, loss is 3.4243532180786134 and perplexity is 30.702780424904265
At time: 177.2253074645996 and batch: 450, loss is 3.454600410461426 and perplexity is 31.645640888439594
At time: 177.90087127685547 and batch: 500, loss is 3.3247832822799683 and perplexity is 27.792974759829313
At time: 178.54643058776855 and batch: 550, loss is 3.3822086381912233 and perplexity is 29.435712209369417
At time: 179.19308972358704 and batch: 600, loss is 3.412543592453003 and perplexity is 30.34232469533583
At time: 179.84617352485657 and batch: 650, loss is 3.245065965652466 and perplexity is 25.66340290751005
At time: 180.49988627433777 and batch: 700, loss is 3.236485743522644 and perplexity is 25.44414718648737
At time: 181.1550521850586 and batch: 750, loss is 3.343953857421875 and perplexity is 28.330921974607904
At time: 181.8099021911621 and batch: 800, loss is 3.286037015914917 and perplexity is 26.736696323105896
At time: 182.46384286880493 and batch: 850, loss is 3.35228835105896 and perplexity is 28.56803258944489
At time: 183.1190540790558 and batch: 900, loss is 3.3170717096328737 and perplexity is 27.57947149690746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.267157985739512 and perplexity of 71.31865867915624
finished 14 epochs...
Completing Train Step...
At time: 184.78186798095703 and batch: 50, loss is 3.617431616783142 and perplexity is 37.24179368449353
At time: 185.44277334213257 and batch: 100, loss is 3.500301175117493 and perplexity is 33.125427010870155
At time: 186.09307956695557 and batch: 150, loss is 3.5182774257659912 and perplexity is 33.72628237272409
At time: 186.7432050704956 and batch: 200, loss is 3.397562689781189 and perplexity is 29.89115716847575
At time: 187.3926064968109 and batch: 250, loss is 3.5518911838531495 and perplexity is 34.87921816701799
At time: 188.04274988174438 and batch: 300, loss is 3.517776107788086 and perplexity is 33.70937901837446
At time: 188.69251918792725 and batch: 350, loss is 3.501391544342041 and perplexity is 33.16156565568544
At time: 189.36960911750793 and batch: 400, loss is 3.4207716941833497 and perplexity is 30.59301436537759
At time: 190.0281617641449 and batch: 450, loss is 3.4521043825149538 and perplexity is 31.56675098106281
At time: 190.67793703079224 and batch: 500, loss is 3.322710280418396 and perplexity is 27.7354195480553
At time: 191.32574224472046 and batch: 550, loss is 3.380283842086792 and perplexity is 29.37910895751249
At time: 191.97483229637146 and batch: 600, loss is 3.4108731079101564 and perplexity is 30.291680622785364
At time: 192.6445553302765 and batch: 650, loss is 3.244009699821472 and perplexity is 25.636309843169855
At time: 193.3131673336029 and batch: 700, loss is 3.2361665391921997 and perplexity is 25.436026600653015
At time: 193.96397495269775 and batch: 750, loss is 3.3444247484207152 and perplexity is 28.344265892273384
At time: 194.614191532135 and batch: 800, loss is 3.286728343963623 and perplexity is 26.755186541870803
At time: 195.264799118042 and batch: 850, loss is 3.3538201236724854 and perplexity is 28.611825851484653
At time: 195.9150800704956 and batch: 900, loss is 3.319369525909424 and perplexity is 27.642916920459392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266870368016909 and perplexity of 71.29814911856579
finished 15 epochs...
Completing Train Step...
At time: 197.5698537826538 and batch: 50, loss is 3.615542016029358 and perplexity is 37.1714880088233
At time: 198.23292326927185 and batch: 100, loss is 3.4978601121902466 and perplexity is 33.0446643724899
At time: 198.88420629501343 and batch: 150, loss is 3.5155461502075194 and perplexity is 33.63429228432752
At time: 199.5312991142273 and batch: 200, loss is 3.3949858808517455 and perplexity is 29.81423252039727
At time: 200.18924975395203 and batch: 250, loss is 3.549205150604248 and perplexity is 34.7856571375954
At time: 200.83692407608032 and batch: 300, loss is 3.515162606239319 and perplexity is 33.62139452798361
At time: 201.48444890975952 and batch: 350, loss is 3.498848533630371 and perplexity is 33.077342574494644
At time: 202.1309299468994 and batch: 400, loss is 3.4184762620925904 and perplexity is 30.522870714243087
At time: 202.77817940711975 and batch: 450, loss is 3.4501956844329835 and perplexity is 31.506557048307737
At time: 203.4248652458191 and batch: 500, loss is 3.3211838674545286 and perplexity is 27.693116138548884
At time: 204.0772669315338 and batch: 550, loss is 3.3789188051223755 and perplexity is 29.339032746781314
At time: 204.73146772384644 and batch: 600, loss is 3.4097630167007447 and perplexity is 30.258072751760054
At time: 205.38531374931335 and batch: 650, loss is 3.2432862663269044 and perplexity is 25.61777038479333
At time: 206.0413806438446 and batch: 700, loss is 3.2359452295303344 and perplexity is 25.43039798506331
At time: 206.69578075408936 and batch: 750, loss is 3.3445570945739744 and perplexity is 28.34801739507419
At time: 207.35223531723022 and batch: 800, loss is 3.287059121131897 and perplexity is 26.76403801056294
At time: 208.0114231109619 and batch: 850, loss is 3.354614315032959 and perplexity is 28.63455814208283
At time: 208.67019939422607 and batch: 900, loss is 3.320507831573486 and perplexity is 27.67440092517201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2668394323897685 and perplexity of 71.2959434997251
finished 16 epochs...
Completing Train Step...
At time: 210.3259792327881 and batch: 50, loss is 3.6138809585571288 and perplexity is 37.10979528267901
At time: 210.98967027664185 and batch: 100, loss is 3.4958848094940187 and perplexity is 32.97945558261096
At time: 211.64171481132507 and batch: 150, loss is 3.5133983993530276 and perplexity is 33.56213172352541
At time: 212.32225584983826 and batch: 200, loss is 3.3929081916809083 and perplexity is 29.75235211873343
At time: 212.97804880142212 and batch: 250, loss is 3.5470226764678956 and perplexity is 34.70982112574561
At time: 213.63330149650574 and batch: 300, loss is 3.513063735961914 and perplexity is 33.55090158597258
At time: 214.28754687309265 and batch: 350, loss is 3.4968204879760743 and perplexity is 33.01032819071466
At time: 214.93949842453003 and batch: 400, loss is 3.416630811691284 and perplexity is 30.466594213951165
At time: 215.60668110847473 and batch: 450, loss is 3.448571524620056 and perplexity is 31.45542689751391
At time: 216.26097321510315 and batch: 500, loss is 3.319847764968872 and perplexity is 27.65614000469349
At time: 216.9251527786255 and batch: 550, loss is 3.37773503780365 and perplexity is 29.304322707011288
At time: 217.59997463226318 and batch: 600, loss is 3.408804316520691 and perplexity is 30.229078232710933
At time: 218.25639629364014 and batch: 650, loss is 3.2426114511489867 and perplexity is 25.600488956054264
At time: 218.9156255722046 and batch: 700, loss is 3.2356407022476197 and perplexity is 25.42265491411458
At time: 219.57017135620117 and batch: 750, loss is 3.344481544494629 and perplexity is 28.34587578101128
At time: 220.22487807273865 and batch: 800, loss is 3.2871608352661132 and perplexity is 26.766760429969228
At time: 220.89239478111267 and batch: 850, loss is 3.3550089693069456 and perplexity is 28.645861123075495
At time: 221.55848908424377 and batch: 900, loss is 3.3211244440078733 and perplexity is 27.691470567032557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266920951947774 and perplexity of 71.3017557504296
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 223.25156331062317 and batch: 50, loss is 3.6130785369873046 and perplexity is 37.080029526431986
At time: 223.95034766197205 and batch: 100, loss is 3.495505290031433 and perplexity is 32.96694161214976
At time: 224.61662435531616 and batch: 150, loss is 3.513241901397705 and perplexity is 33.5568797295083
At time: 225.26685762405396 and batch: 200, loss is 3.392515993118286 and perplexity is 29.740685576947843
At time: 225.91388940811157 and batch: 250, loss is 3.546559047698975 and perplexity is 34.69373238399758
At time: 226.56294751167297 and batch: 300, loss is 3.512687659263611 and perplexity is 33.53828624599544
At time: 227.2128849029541 and batch: 350, loss is 3.496538553237915 and perplexity is 33.0010227443056
At time: 227.86292815208435 and batch: 400, loss is 3.41654718875885 and perplexity is 30.46404661452207
At time: 228.51287579536438 and batch: 450, loss is 3.447641887664795 and perplexity is 31.42619835829657
At time: 229.16287183761597 and batch: 500, loss is 3.318182168006897 and perplexity is 27.610114362646915
At time: 229.81295800209045 and batch: 550, loss is 3.376216855049133 and perplexity is 29.25986714401797
At time: 230.49501299858093 and batch: 600, loss is 3.407199659347534 and perplexity is 30.18060982347738
At time: 231.18998217582703 and batch: 650, loss is 3.240677709579468 and perplexity is 25.551032060199052
At time: 231.86326265335083 and batch: 700, loss is 3.2336277961730957 and perplexity is 25.371532966707957
At time: 232.51407170295715 and batch: 750, loss is 3.341292352676392 and perplexity is 28.255619344932875
At time: 233.17695426940918 and batch: 800, loss is 3.2840152645111083 and perplexity is 26.682695975814198
At time: 233.82651829719543 and batch: 850, loss is 3.3517155933380125 and perplexity is 28.55167471320415
At time: 234.4772231578827 and batch: 900, loss is 3.317314567565918 and perplexity is 27.586170203733772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266665942048373 and perplexity of 71.28357541504987
finished 18 epochs...
Completing Train Step...
At time: 236.1336076259613 and batch: 50, loss is 3.6124895000457764 and perplexity is 37.058194450712406
At time: 236.7816572189331 and batch: 100, loss is 3.4948329162597656 and perplexity is 32.944782955562204
At time: 237.42727637290955 and batch: 150, loss is 3.5123852109909057 and perplexity is 33.52814418305286
At time: 238.07307124137878 and batch: 200, loss is 3.391830716133118 and perplexity is 29.720311951184545
At time: 238.72106456756592 and batch: 250, loss is 3.545809121131897 and perplexity is 34.66772438563509
At time: 239.36628532409668 and batch: 300, loss is 3.51196234703064 and perplexity is 33.51396933645036
At time: 240.01075863838196 and batch: 350, loss is 3.495820889472961 and perplexity is 32.977347602487434
At time: 240.65892028808594 and batch: 400, loss is 3.4158614826202394 and perplexity is 30.443164391104187
At time: 241.31469345092773 and batch: 450, loss is 3.447169280052185 and perplexity is 31.41134960680993
At time: 241.99087119102478 and batch: 500, loss is 3.317819581031799 and perplexity is 27.600105109520122
At time: 242.63904452323914 and batch: 550, loss is 3.3758681631088256 and perplexity is 29.24966624275977
At time: 243.2952470779419 and batch: 600, loss is 3.4068968200683596 and perplexity is 30.17147133317014
At time: 243.95406675338745 and batch: 650, loss is 3.2404793739318847 and perplexity is 25.5459648822266
At time: 244.6425940990448 and batch: 700, loss is 3.2335589838027956 and perplexity is 25.369787151453902
At time: 245.3045699596405 and batch: 750, loss is 3.3414230966567993 and perplexity is 28.25931383858605
At time: 245.95748662948608 and batch: 800, loss is 3.284231576919556 and perplexity is 26.6884683983454
At time: 246.61017441749573 and batch: 850, loss is 3.3520023441314697 and perplexity is 28.559863102540156
At time: 247.27463507652283 and batch: 900, loss is 3.3177645015716553 and perplexity is 27.598584952495873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266534674657534 and perplexity of 71.2742188202168
finished 19 epochs...
Completing Train Step...
At time: 248.94560766220093 and batch: 50, loss is 3.611978220939636 and perplexity is 37.039252212976294
At time: 249.65071940422058 and batch: 100, loss is 3.494229307174683 and perplexity is 32.9249031856726
At time: 250.3613474369049 and batch: 150, loss is 3.5116725969314575 and perplexity is 33.50426006720992
At time: 251.03472709655762 and batch: 200, loss is 3.3912237215042116 and perplexity is 29.702277355465853
At time: 251.68166732788086 and batch: 250, loss is 3.545159912109375 and perplexity is 34.645225090336616
At time: 252.32636833190918 and batch: 300, loss is 3.511338610649109 and perplexity is 33.493071972386325
At time: 252.9699308872223 and batch: 350, loss is 3.4952053594589234 and perplexity is 32.9570553011557
At time: 253.61272549629211 and batch: 400, loss is 3.415280890464783 and perplexity is 30.425494458679875
At time: 254.25844597816467 and batch: 450, loss is 3.4467442178726198 and perplexity is 31.398000667348608
At time: 254.906480550766 and batch: 500, loss is 3.317494478225708 and perplexity is 27.591133696295426
At time: 255.55405068397522 and batch: 550, loss is 3.3755586433410643 and perplexity is 29.240614293808047
At time: 256.204820394516 and batch: 600, loss is 3.406638331413269 and perplexity is 30.163673358010595
At time: 256.85224533081055 and batch: 650, loss is 3.240311040878296 and perplexity is 25.541665013866243
At time: 257.5010681152344 and batch: 700, loss is 3.233505115509033 and perplexity is 25.36842056111521
At time: 258.1630070209503 and batch: 750, loss is 3.3415144872665405 and perplexity is 28.261896592526558
At time: 258.8218629360199 and batch: 800, loss is 3.2843802118301393 and perplexity is 26.69243553127932
At time: 259.48687624931335 and batch: 850, loss is 3.352230372428894 and perplexity is 28.566376302066775
At time: 260.1429841518402 and batch: 900, loss is 3.318108434677124 and perplexity is 27.60807865203033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.266468204864084 and perplexity of 71.26948139506304
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
1076.8995537757874


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}, {'params': {'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.66175442849013}, {'params': {'wordvec_dim': 300, 'dropout': 0.6196663492825968, 'rnn_dropout': 0.0427385332517114, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.26948139506304}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.5317050526091995, 'rnn_dropout': 0.4783827073446225, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0183699131011963 and batch: 50, loss is 7.0667946338653564 and perplexity is 1172.3840848115774
At time: 1.8349428176879883 and batch: 100, loss is 6.106932067871094 and perplexity is 448.9592237295249
At time: 2.648158311843872 and batch: 150, loss is 5.933145923614502 and perplexity is 377.33973051557894
At time: 3.4615933895111084 and batch: 200, loss is 5.747424221038818 and perplexity is 313.38241596935046
At time: 4.274317979812622 and batch: 250, loss is 5.787359428405762 and perplexity is 326.1506621385488
At time: 5.086352348327637 and batch: 300, loss is 5.679379653930664 and perplexity is 292.76775625128397
At time: 5.898623466491699 and batch: 350, loss is 5.639174118041992 and perplexity is 281.23035945800683
At time: 6.709104537963867 and batch: 400, loss is 5.476009340286255 and perplexity is 238.89146806688368
At time: 7.5201637744903564 and batch: 450, loss is 5.468281707763672 and perplexity is 237.05251710876865
At time: 8.336872577667236 and batch: 500, loss is 5.4063614463806156 and perplexity is 222.81937070138
At time: 9.151407241821289 and batch: 550, loss is 5.44075385093689 and perplexity is 230.61596801213628
At time: 9.967119216918945 and batch: 600, loss is 5.352693653106689 and perplexity is 211.17636831139444
At time: 10.781934022903442 and batch: 650, loss is 5.23512222290039 and perplexity is 187.75205254231088
At time: 11.600859880447388 and batch: 700, loss is 5.327301368713379 and perplexity is 205.8816252084812
At time: 12.412903070449829 and batch: 750, loss is 5.294463367462158 and perplexity is 199.2306836074021
At time: 13.224902391433716 and batch: 800, loss is 5.271107225418091 and perplexity is 194.63134395663303
At time: 14.035727500915527 and batch: 850, loss is 5.303005952835083 and perplexity is 200.93991898119208
At time: 14.84749174118042 and batch: 900, loss is 5.201582841873169 and perplexity is 181.55939436857724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.101970933888056 and perplexity of 164.34550242292715
finished 1 epochs...
Completing Train Step...
At time: 16.643293857574463 and batch: 50, loss is 5.039567956924438 and perplexity is 154.40329174022165
At time: 17.281323671340942 and batch: 100, loss is 4.8913860034942624 and perplexity is 133.13797593079747
At time: 17.917993783950806 and batch: 150, loss is 4.858503370285034 and perplexity is 128.83124508118118
At time: 18.55583643913269 and batch: 200, loss is 4.735328798294067 and perplexity is 113.90090298466137
At time: 19.189252614974976 and batch: 250, loss is 4.832214221954346 and perplexity is 125.4885126802679
At time: 19.82780933380127 and batch: 300, loss is 4.761521415710449 and perplexity is 116.92368016956611
At time: 20.466164588928223 and batch: 350, loss is 4.74455958366394 and perplexity is 114.95716533898772
At time: 21.103339195251465 and batch: 400, loss is 4.605037660598755 and perplexity is 99.98674833917647
At time: 21.743781805038452 and batch: 450, loss is 4.6205157279968265 and perplexity is 101.54638894320247
At time: 22.381768465042114 and batch: 500, loss is 4.525053815841675 and perplexity is 92.30089294524308
At time: 23.022228240966797 and batch: 550, loss is 4.588372421264649 and perplexity is 98.33425310760721
At time: 23.68728756904602 and batch: 600, loss is 4.54088698387146 and perplexity is 93.77393921415144
At time: 24.325090408325195 and batch: 650, loss is 4.401654996871948 and perplexity is 81.58578120694965
At time: 24.96200942993164 and batch: 700, loss is 4.454464883804321 and perplexity is 86.01011312394704
At time: 25.599679946899414 and batch: 750, loss is 4.496890630722046 and perplexity is 89.73766949809496
At time: 26.237646102905273 and batch: 800, loss is 4.445170488357544 and perplexity is 85.21440466285011
At time: 26.885937452316284 and batch: 850, loss is 4.501360397338868 and perplexity is 90.13967370067417
At time: 27.51943850517273 and batch: 900, loss is 4.431051483154297 and perplexity is 84.01971579552607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.560630902852098 and perplexity of 95.64380274705378
finished 2 epochs...
Completing Train Step...
At time: 29.165897130966187 and batch: 50, loss is 4.479237194061279 and perplexity is 88.16739240748986
At time: 29.812955141067505 and batch: 100, loss is 4.352149977684021 and perplexity is 77.6452190883225
At time: 30.48699712753296 and batch: 150, loss is 4.34989146232605 and perplexity is 77.47005404946188
At time: 31.1287043094635 and batch: 200, loss is 4.252949314117432 and perplexity is 70.31248043334828
At time: 31.766308546066284 and batch: 250, loss is 4.3882185745239255 and perplexity is 80.49689196300321
At time: 32.40569806098938 and batch: 300, loss is 4.346170015335083 and perplexity is 77.18228913297942
At time: 33.043132066726685 and batch: 350, loss is 4.34074221611023 and perplexity is 76.76449404350383
At time: 33.68081450462341 and batch: 400, loss is 4.2468190145492555 and perplexity is 69.88276236092068
At time: 34.319169998168945 and batch: 450, loss is 4.277730593681335 and perplexity is 72.07668298220523
At time: 34.95735430717468 and batch: 500, loss is 4.160312752723694 and perplexity is 64.09156427702219
At time: 35.59440064430237 and batch: 550, loss is 4.234704756736756 and perplexity is 69.04129174783195
At time: 36.23511815071106 and batch: 600, loss is 4.229483585357666 and perplexity is 68.6817547502902
At time: 36.8726270198822 and batch: 650, loss is 4.078007616996765 and perplexity is 59.02774673474457
At time: 37.51006484031677 and batch: 700, loss is 4.111719737052917 and perplexity is 61.05162007458868
At time: 38.144978523254395 and batch: 750, loss is 4.19090401172638 and perplexity is 66.08250332391826
At time: 38.781867027282715 and batch: 800, loss is 4.145098190307618 and perplexity is 63.12381974473474
At time: 39.41588091850281 and batch: 850, loss is 4.2220610952377315 and perplexity is 68.17385238621117
At time: 40.05532431602478 and batch: 900, loss is 4.159235248565674 and perplexity is 64.02254254230066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.395875695633562 and perplexity of 81.11563227533006
finished 3 epochs...
Completing Train Step...
At time: 41.6688756942749 and batch: 50, loss is 4.2386369705200195 and perplexity is 69.31331133593987
At time: 42.32517910003662 and batch: 100, loss is 4.107960619926453 and perplexity is 60.82255070301429
At time: 42.97792649269104 and batch: 150, loss is 4.111337022781372 and perplexity is 61.02825921883677
At time: 43.64521670341492 and batch: 200, loss is 4.012224326133728 and perplexity is 55.269671713123465
At time: 44.30377125740051 and batch: 250, loss is 4.158886103630066 and perplexity is 64.00019329759714
At time: 44.9984130859375 and batch: 300, loss is 4.127575507164002 and perplexity is 62.02735560450995
At time: 45.65998601913452 and batch: 350, loss is 4.122703804969787 and perplexity is 61.72591166884417
At time: 46.319650411605835 and batch: 400, loss is 4.038920936584472 and perplexity is 56.76505664968038
At time: 46.98213076591492 and batch: 450, loss is 4.080106105804443 and perplexity is 59.15174586049455
At time: 47.642948150634766 and batch: 500, loss is 3.954748206138611 and perplexity is 52.18255304357404
At time: 48.3422315120697 and batch: 550, loss is 4.034353370666504 and perplexity is 56.506369746079145
At time: 49.00690841674805 and batch: 600, loss is 4.0476580286026005 and perplexity is 57.26319112760072
At time: 49.67083501815796 and batch: 650, loss is 3.8925679826736452 and perplexity is 49.03665024329109
At time: 50.3636839389801 and batch: 700, loss is 3.9119690656661987 and perplexity is 49.99730308463877
At time: 51.04894804954529 and batch: 750, loss is 4.009380040168762 and perplexity is 55.112692314561386
At time: 51.737356424331665 and batch: 800, loss is 3.9629827117919922 and perplexity is 52.61402461108389
At time: 52.42816925048828 and batch: 850, loss is 4.0413405799865725 and perplexity is 56.90257414903428
At time: 53.09158945083618 and batch: 900, loss is 3.9888251399993897 and perplexity is 53.991419727245116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334456770387415 and perplexity of 76.28350817156756
finished 4 epochs...
Completing Train Step...
At time: 54.75461530685425 and batch: 50, loss is 4.0770110464096065 and perplexity is 58.96895072057777
At time: 55.4053316116333 and batch: 100, loss is 3.948052568435669 and perplexity is 51.834324680426015
At time: 56.05871915817261 and batch: 150, loss is 3.9527366304397584 and perplexity is 52.077689393912515
At time: 56.70474410057068 and batch: 200, loss is 3.856496877670288 and perplexity is 47.29936534101243
At time: 57.3545880317688 and batch: 250, loss is 4.00358163356781 and perplexity is 54.79405121374556
At time: 58.00498676300049 and batch: 300, loss is 3.97713285446167 and perplexity is 53.3638128605238
At time: 58.666990995407104 and batch: 350, loss is 3.9769527196884153 and perplexity is 53.354201047931
At time: 59.3507661819458 and batch: 400, loss is 3.895094532966614 and perplexity is 49.1607004498833
At time: 60.056917905807495 and batch: 450, loss is 3.9380789279937742 and perplexity is 51.31991728486875
At time: 60.74622583389282 and batch: 500, loss is 3.814906816482544 and perplexity is 45.37252821417762
At time: 61.439547061920166 and batch: 550, loss is 3.893316249847412 and perplexity is 49.07335649030683
At time: 62.125298500061035 and batch: 600, loss is 3.9123237657547 and perplexity is 50.015040277973775
At time: 62.793150901794434 and batch: 650, loss is 3.7608526229858397 and perplexity is 42.98506040941764
At time: 63.446921825408936 and batch: 700, loss is 3.775407524108887 and perplexity is 43.61527897218908
At time: 64.13288307189941 and batch: 750, loss is 3.8765370845794678 and perplexity is 48.256816121733735
At time: 64.82166528701782 and batch: 800, loss is 3.830721459388733 and perplexity is 46.09578247563548
At time: 65.49602484703064 and batch: 850, loss is 3.9132814836502074 and perplexity is 50.062963521903654
At time: 66.14692211151123 and batch: 900, loss is 3.8612025785446167 and perplexity is 47.52246651786549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31165554098887 and perplexity of 74.56383029117733
finished 5 epochs...
Completing Train Step...
At time: 67.79119777679443 and batch: 50, loss is 3.954388952255249 and perplexity is 52.1638096257738
At time: 68.48586821556091 and batch: 100, loss is 3.829053964614868 and perplexity is 46.01898204921054
At time: 69.14535546302795 and batch: 150, loss is 3.8373050594329836 and perplexity is 46.40025985004352
At time: 69.81538510322571 and batch: 200, loss is 3.7366980266571046 and perplexity is 41.95921296907432
At time: 70.48604393005371 and batch: 250, loss is 3.883907618522644 and perplexity is 48.613808619483144
At time: 71.14004588127136 and batch: 300, loss is 3.8638168811798095 and perplexity is 47.646867166934186
At time: 71.79450225830078 and batch: 350, loss is 3.862454471588135 and perplexity is 47.58199681811957
At time: 72.44813251495361 and batch: 400, loss is 3.785493378639221 and perplexity is 44.05740217848504
At time: 73.10029292106628 and batch: 450, loss is 3.828305654525757 and perplexity is 45.984558462017645
At time: 73.7566328048706 and batch: 500, loss is 3.7094912004470824 and perplexity is 40.83302541565274
At time: 74.41574025154114 and batch: 550, loss is 3.785695586204529 and perplexity is 44.066311819281374
At time: 75.07212591171265 and batch: 600, loss is 3.8105440378189086 and perplexity is 45.17500909556326
At time: 75.73116397857666 and batch: 650, loss is 3.657259578704834 and perplexity is 38.754992209135814
At time: 76.39388847351074 and batch: 700, loss is 3.669833335876465 and perplexity is 39.24536452093565
At time: 77.06102538108826 and batch: 750, loss is 3.7745243883132935 and perplexity is 43.57677776149497
At time: 77.73290920257568 and batch: 800, loss is 3.7314702320098876 and perplexity is 41.74043119148834
At time: 78.41912698745728 and batch: 850, loss is 3.8130823802947997 and perplexity is 45.28982439863164
At time: 79.07791757583618 and batch: 900, loss is 3.7622360801696777 and perplexity is 43.044569554724575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30736844833583 and perplexity of 74.24485247442617
finished 6 epochs...
Completing Train Step...
At time: 80.72683024406433 and batch: 50, loss is 3.8553039264678954 and perplexity is 47.2429731495106
At time: 81.41001892089844 and batch: 100, loss is 3.73530300617218 and perplexity is 41.900719816510225
At time: 82.08948922157288 and batch: 150, loss is 3.7448901176452636 and perplexity is 42.304358460910954
At time: 82.7444486618042 and batch: 200, loss is 3.643694272041321 and perplexity is 38.23281858514845
At time: 83.41397047042847 and batch: 250, loss is 3.788659677505493 and perplexity is 44.19712216193666
At time: 84.06869792938232 and batch: 300, loss is 3.774480757713318 and perplexity is 43.57487652201264
At time: 84.71731948852539 and batch: 350, loss is 3.7729102087020876 and perplexity is 43.506493756075436
At time: 85.37029576301575 and batch: 400, loss is 3.692470555305481 and perplexity is 40.14390227683364
At time: 86.02911281585693 and batch: 450, loss is 3.7375737380981446 and perplexity is 41.99597322526765
At time: 86.70261240005493 and batch: 500, loss is 3.6248922157287597 and perplexity is 37.5206788026247
At time: 87.38189458847046 and batch: 550, loss is 3.698438816070557 and perplexity is 40.38420794388202
At time: 88.06355929374695 and batch: 600, loss is 3.725466027259827 and perplexity is 41.49056397387845
At time: 88.74267101287842 and batch: 650, loss is 3.574405870437622 and perplexity is 35.67341988488492
At time: 89.41150236129761 and batch: 700, loss is 3.584195318222046 and perplexity is 36.0243579082432
At time: 90.08890199661255 and batch: 750, loss is 3.6935869789123537 and perplexity is 40.18874490403363
At time: 90.74729943275452 and batch: 800, loss is 3.6458172082901 and perplexity is 38.314070637563574
At time: 91.41296339035034 and batch: 850, loss is 3.7284815073013307 and perplexity is 41.61586677054424
At time: 92.08388471603394 and batch: 900, loss is 3.682107377052307 and perplexity is 39.73003207109395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3138841602900255 and perplexity of 74.73018998989461
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 93.7983467578888 and batch: 50, loss is 3.7999616956710813 and perplexity is 44.69947227721979
At time: 94.50796437263489 and batch: 100, loss is 3.6793856620788574 and perplexity is 39.62204526921966
At time: 95.21199297904968 and batch: 150, loss is 3.685596742630005 and perplexity is 39.86890682889564
At time: 95.89202666282654 and batch: 200, loss is 3.568301405906677 and perplexity is 35.45631608391448
At time: 96.56089878082275 and batch: 250, loss is 3.710846185684204 and perplexity is 40.888391063624915
At time: 97.23112416267395 and batch: 300, loss is 3.6825157165527345 and perplexity is 39.7462587253083
At time: 97.88076615333557 and batch: 350, loss is 3.666381049156189 and perplexity is 39.11011186996463
At time: 98.53021025657654 and batch: 400, loss is 3.5876587438583374 and perplexity is 36.149341904406576
At time: 99.17998504638672 and batch: 450, loss is 3.615682716369629 and perplexity is 37.17671841778606
At time: 99.8692262172699 and batch: 500, loss is 3.4956210327148436 and perplexity is 32.97075751526295
At time: 100.52854919433594 and batch: 550, loss is 3.549120225906372 and perplexity is 34.78270310160966
At time: 101.22918272018433 and batch: 600, loss is 3.5666427612304688 and perplexity is 35.39755539902391
At time: 101.9062659740448 and batch: 650, loss is 3.4053858423233034 and perplexity is 30.125917335655192
At time: 102.57521367073059 and batch: 700, loss is 3.397227611541748 and perplexity is 29.881142970020576
At time: 103.2326922416687 and batch: 750, loss is 3.4964416217803955 and perplexity is 32.99782406210032
At time: 103.89083194732666 and batch: 800, loss is 3.438193554878235 and perplexity is 31.13067149456366
At time: 104.54655337333679 and batch: 850, loss is 3.4992990255355836 and perplexity is 33.09224700648131
At time: 105.20017075538635 and batch: 900, loss is 3.445826916694641 and perplexity is 31.3692124501017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.26607774708369 and perplexity of 71.24165910361629
finished 8 epochs...
Completing Train Step...
At time: 106.82285785675049 and batch: 50, loss is 3.711084804534912 and perplexity is 40.89814896867156
At time: 107.47456741333008 and batch: 100, loss is 3.5820855331420898 and perplexity is 35.94843437474501
At time: 108.11637020111084 and batch: 150, loss is 3.5892078924179076 and perplexity is 36.20538600445746
At time: 108.7647225856781 and batch: 200, loss is 3.4753271102905274 and perplexity is 32.30839521682739
At time: 109.41339063644409 and batch: 250, loss is 3.6215131282806396 and perplexity is 37.39410711667083
At time: 110.09693765640259 and batch: 300, loss is 3.5983202600479127 and perplexity is 36.53681052965072
At time: 110.75402474403381 and batch: 350, loss is 3.5846428728103636 and perplexity is 36.04048438338603
At time: 111.4307816028595 and batch: 400, loss is 3.5111201000213623 and perplexity is 33.48575417974133
At time: 112.11067485809326 and batch: 450, loss is 3.5442343378067016 and perplexity is 34.61317319577584
At time: 112.76702570915222 and batch: 500, loss is 3.42824857711792 and perplexity is 30.82261202018296
At time: 113.42395806312561 and batch: 550, loss is 3.4839787435531617 and perplexity is 32.58912825209268
At time: 114.08191204071045 and batch: 600, loss is 3.5103595781326296 and perplexity is 33.46029721224443
At time: 114.77624106407166 and batch: 650, loss is 3.3530569648742676 and perplexity is 28.589998814659975
At time: 115.4351155757904 and batch: 700, loss is 3.3481586742401124 and perplexity is 28.450299115420524
At time: 116.09393072128296 and batch: 750, loss is 3.454090337753296 and perplexity is 31.629503426680408
At time: 116.7624261379242 and batch: 800, loss is 3.401244158744812 and perplexity is 30.001403345301647
At time: 117.43444633483887 and batch: 850, loss is 3.468720941543579 and perplexity is 32.09566394906634
At time: 118.09332942962646 and batch: 900, loss is 3.421229338645935 and perplexity is 30.607018293161293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274210472629495 and perplexity of 71.82341036975056
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 119.73193120956421 and batch: 50, loss is 3.6827341413497923 and perplexity is 39.75494124200814
At time: 120.39000582695007 and batch: 100, loss is 3.566197757720947 and perplexity is 35.381806866978756
At time: 121.0384087562561 and batch: 150, loss is 3.5748253011703492 and perplexity is 35.68838555183705
At time: 121.69015908241272 and batch: 200, loss is 3.4580676221847533 and perplexity is 31.755553460489377
At time: 122.33998227119446 and batch: 250, loss is 3.605689024925232 and perplexity is 36.807036087370044
At time: 122.99214386940002 and batch: 300, loss is 3.5779291582107544 and perplexity is 35.79932928626977
At time: 123.6417555809021 and batch: 350, loss is 3.5575043249130247 and perplexity is 35.07555064409909
At time: 124.2911684513092 and batch: 400, loss is 3.4872253942489624 and perplexity is 32.69510571081286
At time: 124.94038987159729 and batch: 450, loss is 3.5108031463623046 and perplexity is 33.475142429233166
At time: 125.59606337547302 and batch: 500, loss is 3.390018081665039 and perplexity is 29.666488685030494
At time: 126.24819707870483 and batch: 550, loss is 3.438469705581665 and perplexity is 31.13926943850463
At time: 126.90891551971436 and batch: 600, loss is 3.4641415071487427 and perplexity is 31.94901999112042
At time: 127.58162426948547 and batch: 650, loss is 3.3013943147659304 and perplexity is 27.150468840727566
At time: 128.24294567108154 and batch: 700, loss is 3.288348088264465 and perplexity is 26.798558218820855
At time: 128.89217400550842 and batch: 750, loss is 3.394043173789978 and perplexity is 29.786139676600726
At time: 129.5483455657959 and batch: 800, loss is 3.3351650047302246 and perplexity is 28.08301667210997
At time: 130.19822072982788 and batch: 850, loss is 3.402481589317322 and perplexity is 30.03855097808743
At time: 130.84831595420837 and batch: 900, loss is 3.356384291648865 and perplexity is 28.685285520294425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.262645564667166 and perplexity of 70.99756386323047
finished 10 epochs...
Completing Train Step...
At time: 132.50805854797363 and batch: 50, loss is 3.6581148433685304 and perplexity is 38.788152162761094
At time: 133.18392086029053 and batch: 100, loss is 3.5307558393478393 and perplexity is 34.14976960203224
At time: 133.8449010848999 and batch: 150, loss is 3.537204818725586 and perplexity is 34.37071242406379
At time: 134.49473357200623 and batch: 200, loss is 3.4232176828384397 and perplexity is 30.667936122970993
At time: 135.14402270317078 and batch: 250, loss is 3.5711731243133547 and perplexity is 35.55828297947631
At time: 135.7940375804901 and batch: 300, loss is 3.5444146680831907 and perplexity is 34.61941556169528
At time: 136.44523763656616 and batch: 350, loss is 3.5258832693099977 and perplexity is 33.983777191109176
At time: 137.09518933296204 and batch: 400, loss is 3.4581579542160035 and perplexity is 31.758422133701526
At time: 137.74165964126587 and batch: 450, loss is 3.484600529670715 and perplexity is 32.60939802070031
At time: 138.3913950920105 and batch: 500, loss is 3.3661403608322145 and perplexity is 28.966510746152366
At time: 139.04569149017334 and batch: 550, loss is 3.4170253705978393 and perplexity is 30.478617451832505
At time: 139.73476767539978 and batch: 600, loss is 3.4457427978515627 and perplexity is 31.366573819222968
At time: 140.4040927886963 and batch: 650, loss is 3.285984206199646 and perplexity is 26.735284403067663
At time: 141.09052729606628 and batch: 700, loss is 3.276271758079529 and perplexity is 26.476876259506028
At time: 141.74486017227173 and batch: 750, loss is 3.3853559780120848 and perplexity is 29.528502343006377
At time: 142.41107320785522 and batch: 800, loss is 3.3299423456192017 and perplexity is 27.93673098187809
At time: 143.08125686645508 and batch: 850, loss is 3.3995659732818604 and perplexity is 29.95109764928964
At time: 143.73497533798218 and batch: 900, loss is 3.3557266187667847 and perplexity is 28.666426188208415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.264636314078553 and perplexity of 71.13904299973046
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 145.3783118724823 and batch: 50, loss is 3.6490476608276365 and perplexity is 38.438042559170015
At time: 146.05289793014526 and batch: 100, loss is 3.5287616729736326 and perplexity is 34.081737136378024
At time: 146.70321393013 and batch: 150, loss is 3.5368373584747315 and perplexity is 34.3580848736579
At time: 147.349995136261 and batch: 200, loss is 3.424862194061279 and perplexity is 30.7184113802946
At time: 148.0019805431366 and batch: 250, loss is 3.571072468757629 and perplexity is 35.554704020866424
At time: 148.65212297439575 and batch: 300, loss is 3.5454419088363647 and perplexity is 34.654996308078864
At time: 149.30026030540466 and batch: 350, loss is 3.5240870475769044 and perplexity is 33.92278958197799
At time: 149.94900107383728 and batch: 400, loss is 3.4544556522369385 and perplexity is 31.64106025320213
At time: 150.59864854812622 and batch: 450, loss is 3.4784772872924803 and perplexity is 32.41033285691502
At time: 151.25002717971802 and batch: 500, loss is 3.356276478767395 and perplexity is 28.682193043714094
At time: 151.9120213985443 and batch: 550, loss is 3.4049860000610352 and perplexity is 30.11387412856683
At time: 152.56235337257385 and batch: 600, loss is 3.4342055988311766 and perplexity is 31.006770964070164
At time: 153.21631789207458 and batch: 650, loss is 3.272449245452881 and perplexity is 26.375861254255632
At time: 153.86937069892883 and batch: 700, loss is 3.2569974565505984 and perplexity is 25.97143957974641
At time: 154.5464427471161 and batch: 750, loss is 3.3655800867080687 and perplexity is 28.95028610526216
At time: 155.21505427360535 and batch: 800, loss is 3.3078755283355714 and perplexity is 27.32700830480743
At time: 155.87108659744263 and batch: 850, loss is 3.37619668006897 and perplexity is 29.259276832733537
At time: 156.52495217323303 and batch: 900, loss is 3.3337740993499754 and perplexity is 28.043983005488496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260388518032962 and perplexity of 70.83749975470565
finished 12 epochs...
Completing Train Step...
At time: 158.1571671962738 and batch: 50, loss is 3.640395512580872 and perplexity is 38.106905505802104
At time: 158.8203318119049 and batch: 100, loss is 3.5147914361953734 and perplexity is 33.608917589175356
At time: 159.47019147872925 and batch: 150, loss is 3.522546238899231 and perplexity is 33.87056130064662
At time: 160.12816762924194 and batch: 200, loss is 3.409531555175781 and perplexity is 30.25106998256556
At time: 160.77500867843628 and batch: 250, loss is 3.556955490112305 and perplexity is 35.05630524300722
At time: 161.42497062683105 and batch: 300, loss is 3.5320526456832884 and perplexity is 34.19408396697057
At time: 162.0754053592682 and batch: 350, loss is 3.5114907932281496 and perplexity is 33.498169422320785
At time: 162.728506565094 and batch: 400, loss is 3.4429078340530395 and perplexity is 31.277776645819813
At time: 163.38273215293884 and batch: 450, loss is 3.4683488988876343 and perplexity is 32.083725214003486
At time: 164.036803483963 and batch: 500, loss is 3.3475212335586546 and perplexity is 28.43216951625062
At time: 164.69073843955994 and batch: 550, loss is 3.397274341583252 and perplexity is 29.882539349697936
At time: 165.34387493133545 and batch: 600, loss is 3.4282097673416136 and perplexity is 30.821415824717466
At time: 165.99501729011536 and batch: 650, loss is 3.2677803230285645 and perplexity is 26.253001438841927
At time: 166.6494493484497 and batch: 700, loss is 3.2542244005203247 and perplexity is 25.899519088469006
At time: 167.3044412136078 and batch: 750, loss is 3.364437208175659 and perplexity is 28.91721834458158
At time: 167.9591691493988 and batch: 800, loss is 3.3085125923156737 and perplexity is 27.34442290400034
At time: 168.61314845085144 and batch: 850, loss is 3.379048705101013 and perplexity is 29.342844134052573
At time: 169.26704502105713 and batch: 900, loss is 3.337655920982361 and perplexity is 28.153056309969298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260182419868364 and perplexity of 70.82290178038103
finished 13 epochs...
Completing Train Step...
At time: 170.90296864509583 and batch: 50, loss is 3.634720220565796 and perplexity is 37.89125022145776
At time: 171.54528427124023 and batch: 100, loss is 3.50811897277832 and perplexity is 33.385409819030826
At time: 172.18534445762634 and batch: 150, loss is 3.515545253753662 and perplexity is 33.634262132749974
At time: 172.82905840873718 and batch: 200, loss is 3.4025663423538206 and perplexity is 30.041096944382506
At time: 173.47279739379883 and batch: 250, loss is 3.54981276512146 and perplexity is 34.80679983051607
At time: 174.1168131828308 and batch: 300, loss is 3.5250247383117674 and perplexity is 33.954613585674615
At time: 174.76321148872375 and batch: 350, loss is 3.504769024848938 and perplexity is 33.273757553588574
At time: 175.41317796707153 and batch: 400, loss is 3.436666193008423 and perplexity is 31.083159986810248
At time: 176.07379531860352 and batch: 450, loss is 3.462656645774841 and perplexity is 31.901615328787482
At time: 176.72657871246338 and batch: 500, loss is 3.342449440956116 and perplexity is 28.288332513275236
At time: 177.3811161518097 and batch: 550, loss is 3.392707872390747 and perplexity is 29.746392745585002
At time: 178.03605580329895 and batch: 600, loss is 3.4244163036346436 and perplexity is 30.704717387975595
At time: 178.689936876297 and batch: 650, loss is 3.264730896949768 and perplexity is 26.17306679095026
At time: 179.34373569488525 and batch: 700, loss is 3.2520181703567506 and perplexity is 25.84244177423188
At time: 179.99830555915833 and batch: 750, loss is 3.362949991226196 and perplexity is 28.874244131238353
At time: 180.65294647216797 and batch: 800, loss is 3.3078577518463135 and perplexity is 27.326522530855538
At time: 181.30739569664001 and batch: 850, loss is 3.379121060371399 and perplexity is 29.344967320284713
At time: 181.96181559562683 and batch: 900, loss is 3.33803334236145 and perplexity is 28.163683880717432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260637257197132 and perplexity of 70.85512200676777
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 183.58374309539795 and batch: 50, loss is 3.6319396781921385 and perplexity is 37.78603833546614
At time: 184.23526668548584 and batch: 100, loss is 3.5072343301773072 and perplexity is 33.35588872298748
At time: 184.9084198474884 and batch: 150, loss is 3.5159153366088867 and perplexity is 33.64671190009407
At time: 185.56727266311646 and batch: 200, loss is 3.402663435935974 and perplexity is 30.044013883702405
At time: 186.2206780910492 and batch: 250, loss is 3.5501857471466063 and perplexity is 34.81978456259101
At time: 186.89441633224487 and batch: 300, loss is 3.5254300928115843 and perplexity is 33.968380031037896
At time: 187.56625628471375 and batch: 350, loss is 3.504863724708557 and perplexity is 33.276908722963185
At time: 188.24830079078674 and batch: 400, loss is 3.4354601812362673 and perplexity is 31.04569592554067
At time: 188.91824388504028 and batch: 450, loss is 3.4605394983291626 and perplexity is 31.83414635146904
At time: 189.57286024093628 and batch: 500, loss is 3.3386002492904665 and perplexity is 28.179654594782544
At time: 190.2263662815094 and batch: 550, loss is 3.388902201652527 and perplexity is 29.633402906578052
At time: 190.88099431991577 and batch: 600, loss is 3.4202513790130613 and perplexity is 30.57710049637119
At time: 191.53648400306702 and batch: 650, loss is 3.260525064468384 and perplexity is 26.06321842035184
At time: 192.1912350654602 and batch: 700, loss is 3.245720624923706 and perplexity is 25.680209192750016
At time: 192.87152075767517 and batch: 750, loss is 3.354630947113037 and perplexity is 28.635034398307404
At time: 193.54423832893372 and batch: 800, loss is 3.2978070831298827 and perplexity is 27.053248300522007
At time: 194.20976448059082 and batch: 850, loss is 3.3687121295928955 and perplexity is 29.04110178793874
At time: 194.87938690185547 and batch: 900, loss is 3.3275437211990355 and perplexity is 27.86980155808981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260452061483305 and perplexity of 70.84200115686949
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 196.50894451141357 and batch: 50, loss is 3.630036973953247 and perplexity is 37.71421103487857
At time: 197.16604614257812 and batch: 100, loss is 3.505300536155701 and perplexity is 33.29144763276888
At time: 197.81179690361023 and batch: 150, loss is 3.5141619062423706 and perplexity is 33.5877664272085
At time: 198.45741367340088 and batch: 200, loss is 3.4007818412780764 and perplexity is 29.987536378226064
At time: 199.10310649871826 and batch: 250, loss is 3.5481778526306154 and perplexity is 34.74994025158991
At time: 199.74786114692688 and batch: 300, loss is 3.523767647743225 and perplexity is 33.91195637878131
At time: 200.40643954277039 and batch: 350, loss is 3.503294725418091 and perplexity is 33.224738215235355
At time: 201.06103014945984 and batch: 400, loss is 3.433778657913208 and perplexity is 30.993535730343986
At time: 201.71712565422058 and batch: 450, loss is 3.4590632629394533 and perplexity is 31.787186328575885
At time: 202.37812995910645 and batch: 500, loss is 3.336509847640991 and perplexity is 28.12080932488899
At time: 203.04285621643066 and batch: 550, loss is 3.3869238805770876 and perplexity is 29.57483647177834
At time: 203.6908278465271 and batch: 600, loss is 3.418489513397217 and perplexity is 30.52327518478087
At time: 204.33960819244385 and batch: 650, loss is 3.2589177179336546 and perplexity is 26.021359446490393
At time: 204.98869347572327 and batch: 700, loss is 3.2438262128829956 and perplexity is 25.631606346691175
At time: 205.6391201019287 and batch: 750, loss is 3.3526595783233644 and perplexity is 28.578639790751836
At time: 206.29704189300537 and batch: 800, loss is 3.295137586593628 and perplexity is 26.98112605576047
At time: 206.9501416683197 and batch: 850, loss is 3.3659307861328127 and perplexity is 28.96044073445268
At time: 207.59991812705994 and batch: 900, loss is 3.324484152793884 and perplexity is 27.784662304884826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260244291122645 and perplexity of 70.82728381770568
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 209.26524472236633 and batch: 50, loss is 3.6295523118972777 and perplexity is 37.695936816586965
At time: 209.92019701004028 and batch: 100, loss is 3.504777216911316 and perplexity is 33.2740301354025
At time: 210.56916427612305 and batch: 150, loss is 3.5136842823028562 and perplexity is 33.57172793637653
At time: 211.21828246116638 and batch: 200, loss is 3.40027370929718 and perplexity is 29.972302622670963
At time: 211.86635994911194 and batch: 250, loss is 3.5476206016540526 and perplexity is 34.73058120787318
At time: 212.5121672153473 and batch: 300, loss is 3.523276662826538 and perplexity is 33.89531020654813
At time: 213.17513966560364 and batch: 350, loss is 3.5028273105621337 and perplexity is 33.20921210785756
At time: 213.82408547401428 and batch: 400, loss is 3.4332861709594726 and perplexity is 30.978275576368468
At time: 214.4714047908783 and batch: 450, loss is 3.458650465011597 and perplexity is 31.77406735185655
At time: 215.133944272995 and batch: 500, loss is 3.3359687995910643 and perplexity is 28.105598731043518
At time: 215.78637313842773 and batch: 550, loss is 3.3863918209075927 and perplexity is 29.559105079450617
At time: 216.48644018173218 and batch: 600, loss is 3.4179989385604856 and perplexity is 30.508304906361772
At time: 217.1565318107605 and batch: 650, loss is 3.258474645614624 and perplexity is 26.009832656205536
At time: 217.8450322151184 and batch: 700, loss is 3.2433252429962156 and perplexity is 25.618768899617375
At time: 218.49513125419617 and batch: 750, loss is 3.352138361930847 and perplexity is 28.563748016472616
At time: 219.17063164710999 and batch: 800, loss is 3.2944692182540893 and perplexity is 26.96309875045085
At time: 219.83984422683716 and batch: 850, loss is 3.3652355766296385 and perplexity is 28.940314157742296
At time: 220.48918461799622 and batch: 900, loss is 3.323704037666321 and perplexity is 27.762995521897967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260180747672303 and perplexity of 70.82278335070262
finished 17 epochs...
Completing Train Step...
At time: 222.13941407203674 and batch: 50, loss is 3.629389452934265 and perplexity is 37.68979819528554
At time: 222.8089702129364 and batch: 100, loss is 3.504561710357666 and perplexity is 33.2668601364606
At time: 223.45184469223022 and batch: 150, loss is 3.5134515810012816 and perplexity is 33.56391666047191
At time: 224.0973711013794 and batch: 200, loss is 3.400032138824463 and perplexity is 29.965063073823814
At time: 224.74488830566406 and batch: 250, loss is 3.547349648475647 and perplexity is 34.721172121275565
At time: 225.44338202476501 and batch: 300, loss is 3.523023958206177 and perplexity is 33.8867457872301
At time: 226.1107485294342 and batch: 350, loss is 3.502582392692566 and perplexity is 33.201079574320126
At time: 226.78095936775208 and batch: 400, loss is 3.433064742088318 and perplexity is 30.97141685116518
At time: 227.4311294555664 and batch: 450, loss is 3.45845739364624 and perplexity is 31.767933281465723
At time: 228.07982921600342 and batch: 500, loss is 3.3358176612854002 and perplexity is 28.10135121946057
At time: 228.72953414916992 and batch: 550, loss is 3.386239700317383 and perplexity is 29.554608872931322
At time: 229.37432861328125 and batch: 600, loss is 3.417905340194702 and perplexity is 30.50544951251189
At time: 230.02356004714966 and batch: 650, loss is 3.2583843946456907 and perplexity is 26.007485349531464
At time: 230.6900610923767 and batch: 700, loss is 3.2433094024658202 and perplexity is 25.618363087944072
At time: 231.36311316490173 and batch: 750, loss is 3.352149443626404 and perplexity is 28.564064552985972
At time: 232.01292729377747 and batch: 800, loss is 3.2945429277420044 and perplexity is 26.965086259900602
At time: 232.66938281059265 and batch: 850, loss is 3.365383381843567 and perplexity is 28.944592003203685
At time: 233.34063625335693 and batch: 900, loss is 3.323913493156433 and perplexity is 27.76881124277806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.2601251471532535 and perplexity of 70.81884567665719
finished 18 epochs...
Completing Train Step...
At time: 234.99784803390503 and batch: 50, loss is 3.6292328357696535 and perplexity is 37.68389578817862
At time: 235.68844866752625 and batch: 100, loss is 3.5043545055389402 and perplexity is 33.25996779682411
At time: 236.35514092445374 and batch: 150, loss is 3.513228211402893 and perplexity is 33.55642033914343
At time: 237.00344014167786 and batch: 200, loss is 3.39980001449585 and perplexity is 29.95810826089782
At time: 237.68364095687866 and batch: 250, loss is 3.547091593742371 and perplexity is 34.712213314445734
At time: 238.32986569404602 and batch: 300, loss is 3.522781643867493 and perplexity is 33.87853553760529
At time: 238.97753739356995 and batch: 350, loss is 3.5023497343063354 and perplexity is 33.19335596324154
At time: 239.67622876167297 and batch: 400, loss is 3.432853503227234 and perplexity is 30.964875175295788
At time: 240.37575960159302 and batch: 450, loss is 3.458273324966431 and perplexity is 31.762086338062417
At time: 241.03813195228577 and batch: 500, loss is 3.335673279762268 and perplexity is 28.09729419645606
At time: 241.69498467445374 and batch: 550, loss is 3.3860948514938354 and perplexity is 29.55032823263601
At time: 242.365953207016 and batch: 600, loss is 3.4178150367736815 and perplexity is 30.502694890438914
At time: 243.0553572177887 and batch: 650, loss is 3.2582994365692137 and perplexity is 26.00527589745889
At time: 243.74781847000122 and batch: 700, loss is 3.2432933187484743 and perplexity is 25.61795105274684
At time: 244.43313264846802 and batch: 750, loss is 3.352161822319031 and perplexity is 28.564418140949723
At time: 245.09412288665771 and batch: 800, loss is 3.294614429473877 and perplexity is 26.967014379199394
At time: 245.78707766532898 and batch: 850, loss is 3.365527834892273 and perplexity is 28.94877343976545
At time: 246.4482078552246 and batch: 900, loss is 3.3241117429733276 and perplexity is 27.774316950257035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260072472977312 and perplexity of 70.81511545056416
finished 19 epochs...
Completing Train Step...
At time: 248.08965945243835 and batch: 50, loss is 3.629081673622131 and perplexity is 37.678199840080985
At time: 248.74384212493896 and batch: 100, loss is 3.5041546058654784 and perplexity is 33.253319804610044
At time: 249.40232062339783 and batch: 150, loss is 3.5130132961273195 and perplexity is 33.54920932672509
At time: 250.0684938430786 and batch: 200, loss is 3.3995763444900513 and perplexity is 29.951408279969712
At time: 250.7103705406189 and batch: 250, loss is 3.5468447971343995 and perplexity is 34.70364751499316
At time: 251.3909661769867 and batch: 300, loss is 3.5225487327575684 and perplexity is 33.870645769133645
At time: 252.038409948349 and batch: 350, loss is 3.50212749004364 and perplexity is 33.18597975001
At time: 252.6905071735382 and batch: 400, loss is 3.432651333808899 and perplexity is 30.958615657255674
At time: 253.38243699073792 and batch: 450, loss is 3.4580971717834474 and perplexity is 31.75649183821472
At time: 254.06474709510803 and batch: 500, loss is 3.335534973144531 and perplexity is 28.093408423448448
At time: 254.76167058944702 and batch: 550, loss is 3.3859561777114866 and perplexity is 29.54623066096979
At time: 255.40712189674377 and batch: 600, loss is 3.4177276277542115 and perplexity is 30.500028796309383
At time: 256.05289244651794 and batch: 650, loss is 3.258219013214111 and perplexity is 26.003184550018563
At time: 256.70121216773987 and batch: 700, loss is 3.2432772016525266 and perplexity is 25.617538169098992
At time: 257.39441299438477 and batch: 750, loss is 3.3521754121780396 and perplexity is 28.56480633000264
At time: 258.08588433265686 and batch: 800, loss is 3.294683599472046 and perplexity is 26.968879752047794
At time: 258.78471064567566 and batch: 850, loss is 3.3656686115264893 and perplexity is 28.952849037522736
At time: 259.4282257556915 and batch: 900, loss is 3.324299569129944 and perplexity is 27.779534183413535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.260026487585616 and perplexity of 70.81185906461589
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
1343.4865069389343


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}, {'params': {'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.66175442849013}, {'params': {'wordvec_dim': 300, 'dropout': 0.6196663492825968, 'rnn_dropout': 0.0427385332517114, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.26948139506304}, {'params': {'wordvec_dim': 300, 'dropout': 0.5317050526091995, 'rnn_dropout': 0.4783827073446225, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.81185906461589}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 300, 'dropout': 0.31036864324501723, 'rnn_dropout': 0.09018996615541643, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: glove
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 200 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0305521488189697 and batch: 50, loss is 7.0666835975646975 and perplexity is 1172.253914846778
At time: 1.837986707687378 and batch: 100, loss is 6.147181243896484 and perplexity is 467.3980471095328
At time: 2.6486592292785645 and batch: 150, loss is 5.8849288177490235 and perplexity is 359.57717157313766
At time: 3.4602108001708984 and batch: 200, loss is 5.6422913551330565 and perplexity is 282.1083889677236
At time: 4.273164987564087 and batch: 250, loss is 5.628729677200317 and perplexity is 278.3083515377444
At time: 5.089313983917236 and batch: 300, loss is 5.48529746055603 and perplexity is 241.12065721633755
At time: 5.902956247329712 and batch: 350, loss is 5.425499153137207 and perplexity is 227.1246880260548
At time: 6.71467924118042 and batch: 400, loss is 5.228271808624267 and perplexity is 186.47026858939327
At time: 7.527232885360718 and batch: 450, loss is 5.207163724899292 and perplexity is 182.57548882570302
At time: 8.34255599975586 and batch: 500, loss is 5.127092914581299 and perplexity is 168.52648434807568
At time: 9.15473484992981 and batch: 550, loss is 5.15967490196228 and perplexity is 174.1078442849661
At time: 9.966607332229614 and batch: 600, loss is 5.056396503448486 and perplexity is 157.023661384546
At time: 10.777643203735352 and batch: 650, loss is 4.939491415023804 and perplexity is 139.69918258428748
At time: 11.587830066680908 and batch: 700, loss is 5.011380662918091 and perplexity is 150.11184700173098
At time: 12.400023937225342 and batch: 750, loss is 4.991997385025025 and perplexity is 147.23020540774027
At time: 13.2158682346344 and batch: 800, loss is 4.9518624210357665 and perplexity is 141.4381361366724
At time: 14.029358863830566 and batch: 850, loss is 4.984182872772217 and perplexity is 146.08415690074952
At time: 14.846619606018066 and batch: 900, loss is 4.890594778060913 and perplexity is 133.03267544180233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8707647454248715 and perplexity of 130.42061733929017
finished 1 epochs...
Completing Train Step...
At time: 16.585829734802246 and batch: 50, loss is 4.823651704788208 and perplexity is 124.41860223641149
At time: 17.222766876220703 and batch: 100, loss is 4.697441711425781 and perplexity is 109.66625534379892
At time: 17.86925196647644 and batch: 150, loss is 4.683364095687867 and perplexity is 108.13323191671911
At time: 18.506356477737427 and batch: 200, loss is 4.570362148284912 and perplexity is 96.57907938832815
At time: 19.143406629562378 and batch: 250, loss is 4.688012552261353 and perplexity is 108.63705464123983
At time: 19.776336669921875 and batch: 300, loss is 4.62392409324646 and perplexity is 101.89308662704028
At time: 20.409013271331787 and batch: 350, loss is 4.614201650619507 and perplexity is 100.90723713646075
At time: 21.044791221618652 and batch: 400, loss is 4.493668212890625 and perplexity is 89.44896264863107
At time: 21.6812584400177 and batch: 450, loss is 4.512890148162842 and perplexity is 91.18497613634571
At time: 22.31790781021118 and batch: 500, loss is 4.411120882034302 and perplexity is 82.36172956784168
At time: 22.954493761062622 and batch: 550, loss is 4.4779108619689945 and perplexity is 88.05053068130714
At time: 23.592509746551514 and batch: 600, loss is 4.4483434772491455 and perplexity is 85.48521843959291
At time: 24.234344244003296 and batch: 650, loss is 4.30105092048645 and perplexity is 73.7772870351766
At time: 24.870485544204712 and batch: 700, loss is 4.346056351661682 and perplexity is 77.17351680903175
At time: 25.507082223892212 and batch: 750, loss is 4.4019816207885745 and perplexity is 81.61243342673772
At time: 26.14329242706299 and batch: 800, loss is 4.358367023468017 and perplexity is 78.12944664305972
At time: 26.778985261917114 and batch: 850, loss is 4.419991188049316 and perplexity is 83.0955531212014
At time: 27.416316747665405 and batch: 900, loss is 4.352715649604797 and perplexity is 77.68915323352795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5024656530928935 and perplexity of 90.23935617086214
finished 2 epochs...
Completing Train Step...
At time: 29.155625581741333 and batch: 50, loss is 4.411928644180298 and perplexity is 82.42828513217924
At time: 29.79713201522827 and batch: 100, loss is 4.291995253562927 and perplexity is 73.11220044362413
At time: 30.42827343940735 and batch: 150, loss is 4.289662117958069 and perplexity is 72.94181860487654
At time: 31.05694556236267 and batch: 200, loss is 4.191669020652771 and perplexity is 66.13307637078906
At time: 31.68752121925354 and batch: 250, loss is 4.332201499938964 and perplexity is 76.11166208242497
At time: 32.33462929725647 and batch: 300, loss is 4.289643683433533 and perplexity is 72.94047396952566
At time: 33.00326728820801 and batch: 350, loss is 4.2872860622406 and perplexity is 72.76871051840932
At time: 33.67994046211243 and batch: 400, loss is 4.194449167251587 and perplexity is 66.31719183353033
At time: 34.31135034561157 and batch: 450, loss is 4.226618695259094 and perplexity is 68.48527065820694
At time: 34.944113969802856 and batch: 500, loss is 4.105343680381775 and perplexity is 60.663589851059776
At time: 35.60265517234802 and batch: 550, loss is 4.184486765861511 and perplexity is 65.65979342004927
At time: 36.25098943710327 and batch: 600, loss is 4.184025974273681 and perplexity is 65.62954490924757
At time: 36.88707447052002 and batch: 650, loss is 4.027639751434326 and perplexity is 56.128278097226634
At time: 37.52406358718872 and batch: 700, loss is 4.056121616363526 and perplexity is 57.74989991712044
At time: 38.162583351135254 and batch: 750, loss is 4.140607399940491 and perplexity is 62.84097946662817
At time: 38.80001711845398 and batch: 800, loss is 4.10334361076355 and perplexity is 60.5423797028398
At time: 39.436246395111084 and batch: 850, loss is 4.173782920837402 and perplexity is 64.96072917905181
At time: 40.07398176193237 and batch: 900, loss is 4.118434414863587 and perplexity is 61.46294143297553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37965413968857 and perplexity of 79.81042537252368
finished 3 epochs...
Completing Train Step...
At time: 41.682894229888916 and batch: 50, loss is 4.195611476898193 and perplexity is 66.39431775875485
At time: 42.32736611366272 and batch: 100, loss is 4.076767683029175 and perplexity is 58.95460158348798
At time: 42.96208453178406 and batch: 150, loss is 4.076258959770203 and perplexity is 58.92461763385198
At time: 43.5985004901886 and batch: 200, loss is 3.9813085126876833 and perplexity is 53.58710778185413
At time: 44.243271350860596 and batch: 250, loss is 4.13037600517273 and perplexity is 62.201306551318766
At time: 44.897926807403564 and batch: 300, loss is 4.096320085525512 and perplexity is 60.11864855611106
At time: 45.557098388671875 and batch: 350, loss is 4.093978023529052 and perplexity is 59.978011708443105
At time: 46.22031927108765 and batch: 400, loss is 4.014565567970276 and perplexity is 55.39922297707311
At time: 46.88684701919556 and batch: 450, loss is 4.0498804140090945 and perplexity is 57.39059352406864
At time: 47.55274987220764 and batch: 500, loss is 3.9236099195480345 and perplexity is 50.582715121599286
At time: 48.21975374221802 and batch: 550, loss is 4.002296085357666 and perplexity is 54.7236560771469
At time: 48.88614463806152 and batch: 600, loss is 4.015200271606445 and perplexity is 55.43439622645142
At time: 49.55265283584595 and batch: 650, loss is 3.857060360908508 and perplexity is 47.32602525106136
At time: 50.22730255126953 and batch: 700, loss is 3.8765084314346314 and perplexity is 48.255433432001354
At time: 50.894259452819824 and batch: 750, loss is 3.9780966901779173 and perplexity is 53.41527160421778
At time: 51.560970067977905 and batch: 800, loss is 3.938254280090332 and perplexity is 51.32891712900753
At time: 52.22779560089111 and batch: 850, loss is 4.012630648612976 and perplexity is 55.29213358623415
At time: 52.89421033859253 and batch: 900, loss is 3.9625884008407595 and perplexity is 52.593282414697654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330791316620291 and perplexity of 76.00440632911115
finished 4 epochs...
Completing Train Step...
At time: 54.570199966430664 and batch: 50, loss is 4.045672407150269 and perplexity is 57.14960091776843
At time: 55.228299617767334 and batch: 100, loss is 3.926807084083557 and perplexity is 50.744695184975434
At time: 55.90092182159424 and batch: 150, loss is 3.9299682188034057 and perplexity is 50.90535981024499
At time: 56.567768812179565 and batch: 200, loss is 3.8361467599868773 and perplexity is 46.34654556937631
At time: 57.22653770446777 and batch: 250, loss is 3.988550834655762 and perplexity is 53.976611623367745
At time: 57.903414249420166 and batch: 300, loss is 3.955764741897583 and perplexity is 52.23562544516414
At time: 58.56213736534119 and batch: 350, loss is 3.954958691596985 and perplexity is 52.19353786820291
At time: 59.222350120544434 and batch: 400, loss is 3.8824569845199584 and perplexity is 48.54333890094229
At time: 59.881747007369995 and batch: 450, loss is 3.921082019805908 and perplexity is 50.45500857177122
At time: 60.5412859916687 and batch: 500, loss is 3.794434356689453 and perplexity is 44.45308470275784
At time: 61.20115852355957 and batch: 550, loss is 3.86734543800354 and perplexity is 47.8152888131325
At time: 61.8844108581543 and batch: 600, loss is 3.8921173810958862 and perplexity is 49.01455922882022
At time: 62.55730628967285 and batch: 650, loss is 3.734684524536133 and perplexity is 41.874813003036564
At time: 63.21593642234802 and batch: 700, loss is 3.75154616355896 and perplexity is 42.58687739965577
At time: 63.87376117706299 and batch: 750, loss is 3.8563439559936525 and perplexity is 47.292132795781264
At time: 64.52988815307617 and batch: 800, loss is 3.8173056411743165 and perplexity is 45.48149960456187
At time: 65.2205708026886 and batch: 850, loss is 3.893474369049072 and perplexity is 49.08111654374841
At time: 65.8900077342987 and batch: 900, loss is 3.8452771520614624 and perplexity is 46.77164541268083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308948255565069 and perplexity of 74.36223772772081
finished 5 epochs...
Completing Train Step...
At time: 67.53271842002869 and batch: 50, loss is 3.928272256851196 and perplexity is 50.81909942468502
At time: 68.19448709487915 and batch: 100, loss is 3.8130441761016844 and perplexity is 45.28809417048535
At time: 68.84824085235596 and batch: 150, loss is 3.817543234825134 and perplexity is 45.49230700393147
At time: 69.50002765655518 and batch: 200, loss is 3.7257383012771608 and perplexity is 41.50186231446556
At time: 70.15438866615295 and batch: 250, loss is 3.8792838573455812 and perplexity is 48.38954883989092
At time: 70.80871748924255 and batch: 300, loss is 3.851672625541687 and perplexity is 47.071730801775615
At time: 71.46278047561646 and batch: 350, loss is 3.8440767621994016 and perplexity is 46.71553488768587
At time: 72.11592197418213 and batch: 400, loss is 3.776652383804321 and perplexity is 43.66960768388243
At time: 72.76919555664062 and batch: 450, loss is 3.818534507751465 and perplexity is 45.53742465447908
At time: 73.42065286636353 and batch: 500, loss is 3.693480896949768 and perplexity is 40.18448182922202
At time: 74.07327961921692 and batch: 550, loss is 3.7647447109222414 and perplexity is 43.15268804362232
At time: 74.72694492340088 and batch: 600, loss is 3.7932686233520507 and perplexity is 44.40129445264499
At time: 75.37959480285645 and batch: 650, loss is 3.6369789600372315 and perplexity is 37.9769334155323
At time: 76.03730034828186 and batch: 700, loss is 3.6491199684143067 and perplexity is 38.4408220217507
At time: 76.7210738658905 and batch: 750, loss is 3.761193242073059 and perplexity is 42.99970443533415
At time: 77.36756110191345 and batch: 800, loss is 3.719843239784241 and perplexity is 41.25792600027144
At time: 78.03806519508362 and batch: 850, loss is 3.7968425130844117 and perplexity is 44.56026368303764
At time: 78.71624541282654 and batch: 900, loss is 3.752069730758667 and perplexity is 42.60918032983204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308649768568065 and perplexity of 74.34004487899416
finished 6 epochs...
Completing Train Step...
At time: 80.32345771789551 and batch: 50, loss is 3.8382417821884154 and perplexity is 46.44374439260364
At time: 81.02528786659241 and batch: 100, loss is 3.718613586425781 and perplexity is 41.20722423219044
At time: 81.68576145172119 and batch: 150, loss is 3.7308547735214233 and perplexity is 41.71474959258906
At time: 82.33177971839905 and batch: 200, loss is 3.6384460163116454 and perplexity is 38.03268860198067
At time: 82.9731674194336 and batch: 250, loss is 3.7898578023910523 and perplexity is 44.250107569098034
At time: 83.63060164451599 and batch: 300, loss is 3.7616586303710937 and perplexity is 43.01972065189252
At time: 84.28014898300171 and batch: 350, loss is 3.756910662651062 and perplexity is 42.815948541420035
At time: 84.92733001708984 and batch: 400, loss is 3.690832805633545 and perplexity is 40.078210422139634
At time: 85.57479047775269 and batch: 450, loss is 3.735287046432495 and perplexity is 41.900051097265646
At time: 86.22497630119324 and batch: 500, loss is 3.61275625705719 and perplexity is 37.06808130254735
At time: 86.87680840492249 and batch: 550, loss is 3.682177348136902 and perplexity is 39.7328121217894
At time: 87.52874684333801 and batch: 600, loss is 3.7115655899047852 and perplexity is 40.91781692800528
At time: 88.1822144985199 and batch: 650, loss is 3.5577464389801023 and perplexity is 35.08404395645423
At time: 88.86741733551025 and batch: 700, loss is 3.5668817234039305 and perplexity is 35.40601508652969
At time: 89.52081489562988 and batch: 750, loss is 3.6790022706985472 and perplexity is 39.6068574302225
At time: 90.16959643363953 and batch: 800, loss is 3.6402265310287474 and perplexity is 38.1004666857993
At time: 90.8173201084137 and batch: 850, loss is 3.720126075744629 and perplexity is 41.269596875789176
At time: 91.46985411643982 and batch: 900, loss is 3.6756593132019044 and perplexity is 39.474674453393725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324266825636772 and perplexity of 75.51013046790948
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 93.10400938987732 and batch: 50, loss is 3.7876845312118532 and perplexity is 44.15404450899129
At time: 93.75217866897583 and batch: 100, loss is 3.6657574701309206 and perplexity is 39.08573122694547
At time: 94.40614938735962 and batch: 150, loss is 3.678637676239014 and perplexity is 39.59241962157636
At time: 95.12894773483276 and batch: 200, loss is 3.5706755352020263 and perplexity is 35.54059396634333
At time: 95.80936288833618 and batch: 250, loss is 3.7096089553833007 and perplexity is 40.837833989067235
At time: 96.45587134361267 and batch: 300, loss is 3.6725616979599 and perplexity is 39.35258628883532
At time: 97.10261154174805 and batch: 350, loss is 3.6567456197738646 and perplexity is 38.73507885253236
At time: 97.74955248832703 and batch: 400, loss is 3.5865920305252077 and perplexity is 36.1108014788607
At time: 98.39758682250977 and batch: 450, loss is 3.6137631225585936 and perplexity is 37.105422670527055
At time: 99.04459977149963 and batch: 500, loss is 3.4848192310333252 and perplexity is 32.6165305203966
At time: 99.70010876655579 and batch: 550, loss is 3.5347699880599976 and perplexity is 34.28712735848979
At time: 100.34934973716736 and batch: 600, loss is 3.554932060241699 and perplexity is 34.98544298440825
At time: 100.99758100509644 and batch: 650, loss is 3.3973464012145995 and perplexity is 29.884692752052967
At time: 101.67881846427917 and batch: 700, loss is 3.3840466356277465 and perplexity is 29.489864723798835
At time: 102.33261013031006 and batch: 750, loss is 3.4846887016296386 and perplexity is 32.612273381964364
At time: 102.9847137928009 and batch: 800, loss is 3.427595934867859 and perplexity is 30.80250244421459
At time: 103.63171553611755 and batch: 850, loss is 3.494364595413208 and perplexity is 32.929357839152516
At time: 104.28532147407532 and batch: 900, loss is 3.4390200853347777 and perplexity is 31.156412579116303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.274825004682149 and perplexity of 71.8675617223749
finished 8 epochs...
Completing Train Step...
At time: 105.89822340011597 and batch: 50, loss is 3.6924774503707884 and perplexity is 40.144179072615806
At time: 106.55232238769531 and batch: 100, loss is 3.5716357421875 and perplexity is 35.57473668235161
At time: 107.24108457565308 and batch: 150, loss is 3.580810842514038 and perplexity is 35.90264043510788
At time: 107.92521214485168 and batch: 200, loss is 3.4775627422332764 and perplexity is 32.38070569688498
At time: 108.5740180015564 and batch: 250, loss is 3.6204298734664917 and perplexity is 37.353621702085
At time: 109.23825860023499 and batch: 300, loss is 3.589262537956238 and perplexity is 36.2073645213242
At time: 109.90914130210876 and batch: 350, loss is 3.5745693111419676 and perplexity is 35.67925085025289
At time: 110.5487310886383 and batch: 400, loss is 3.508813738822937 and perplexity is 33.40861292759134
At time: 111.20060443878174 and batch: 450, loss is 3.542688708305359 and perplexity is 34.559715377782084
At time: 111.85300016403198 and batch: 500, loss is 3.4168055486679076 and perplexity is 30.47191831965765
At time: 112.50934934616089 and batch: 550, loss is 3.4690184354782105 and perplexity is 32.10521363483553
At time: 113.16523742675781 and batch: 600, loss is 3.4966969013214113 and perplexity is 33.00624880676814
At time: 113.82057166099548 and batch: 650, loss is 3.3436988687515257 and perplexity is 28.323698831432807
At time: 114.47511076927185 and batch: 700, loss is 3.334661102294922 and perplexity is 28.068869136406455
At time: 115.13000345230103 and batch: 750, loss is 3.4424305391311645 and perplexity is 31.262851483994538
At time: 115.78532218933105 and batch: 800, loss is 3.3902544736862184 and perplexity is 29.673502435216665
At time: 116.46871662139893 and batch: 850, loss is 3.462511487007141 and perplexity is 31.896984865703
At time: 117.11134147644043 and batch: 900, loss is 3.4157443237304688 and perplexity is 30.43959791268943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.284027935707406 and perplexity of 72.53200666682454
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 118.71230983734131 and batch: 50, loss is 3.6617513275146485 and perplexity is 38.92946144191186
At time: 119.36702156066895 and batch: 100, loss is 3.558005676269531 and perplexity is 35.093140227907135
At time: 120.00179433822632 and batch: 150, loss is 3.5670294427871703 and perplexity is 35.41124562755788
At time: 120.67240595817566 and batch: 200, loss is 3.4577125978469847 and perplexity is 31.744281467185967
At time: 121.31042003631592 and batch: 250, loss is 3.6029368019104004 and perplexity is 36.70587418945066
At time: 121.95512437820435 and batch: 300, loss is 3.5680385875701903 and perplexity is 35.44699873834162
At time: 122.59842038154602 and batch: 350, loss is 3.5477759504318236 and perplexity is 34.73597698031758
At time: 123.24122452735901 and batch: 400, loss is 3.480776376724243 and perplexity is 32.48493283381539
At time: 123.8839476108551 and batch: 450, loss is 3.5126805877685547 and perplexity is 33.53804908100861
At time: 124.52844262123108 and batch: 500, loss is 3.3784493684768675 and perplexity is 29.325263161892952
At time: 125.17361783981323 and batch: 550, loss is 3.4263282680511473 and perplexity is 30.763479873029386
At time: 125.81793022155762 and batch: 600, loss is 3.452301540374756 and perplexity is 31.572975227686552
At time: 126.46098780632019 and batch: 650, loss is 3.291765446662903 and perplexity is 26.890295156535338
At time: 127.10519671440125 and batch: 700, loss is 3.2812799263000487 and perplexity is 26.609809507952935
At time: 127.74993538856506 and batch: 750, loss is 3.3791302633285523 and perplexity is 29.345237382004303
At time: 128.39646244049072 and batch: 800, loss is 3.324250588417053 and perplexity is 27.77817355534799
At time: 129.04471015930176 and batch: 850, loss is 3.394397540092468 and perplexity is 29.796696751208653
At time: 129.6881902217865 and batch: 900, loss is 3.3474991655349733 and perplexity is 28.431542081383576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.272440871147261 and perplexity of 71.69642394712005
finished 10 epochs...
Completing Train Step...
At time: 131.31793069839478 and batch: 50, loss is 3.637378997802734 and perplexity is 37.992128662250124
At time: 131.9567551612854 and batch: 100, loss is 3.523637642860413 and perplexity is 33.90754794543149
At time: 132.6057484149933 and batch: 150, loss is 3.5300149297714234 and perplexity is 34.124477081594016
At time: 133.2489252090454 and batch: 200, loss is 3.4229745960235594 and perplexity is 30.660482058089055
At time: 133.89201545715332 and batch: 250, loss is 3.5683501768112182 and perplexity is 35.45804536269049
At time: 134.53432726860046 and batch: 300, loss is 3.535450134277344 and perplexity is 34.31045555085793
At time: 135.17885565757751 and batch: 350, loss is 3.516337103843689 and perplexity is 33.66090597381955
At time: 135.82442545890808 and batch: 400, loss is 3.4512726879119873 and perplexity is 31.540507999224577
At time: 136.46955013275146 and batch: 450, loss is 3.4862751865386965 and perplexity is 32.66405332472325
At time: 137.1178412437439 and batch: 500, loss is 3.3540666675567627 and perplexity is 28.618880791806806
At time: 137.76479172706604 and batch: 550, loss is 3.4043201446533202 and perplexity is 30.09382931684481
At time: 138.41039085388184 and batch: 600, loss is 3.433925681114197 and perplexity is 30.99809283416881
At time: 139.055095911026 and batch: 650, loss is 3.2761154127120973 and perplexity is 26.472737046140644
At time: 139.70117616653442 and batch: 700, loss is 3.268739924430847 and perplexity is 26.278205947043375
At time: 140.38860368728638 and batch: 750, loss is 3.36996994972229 and perplexity is 29.07765325311269
At time: 141.0360462665558 and batch: 800, loss is 3.3180669593811034 and perplexity is 27.606933622541057
At time: 141.68240666389465 and batch: 850, loss is 3.3920676469802857 and perplexity is 29.7273544441322
At time: 142.32749938964844 and batch: 900, loss is 3.3476520204544067 and perplexity is 28.435888314620897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.275063710669949 and perplexity of 71.8847189873761
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 143.96703720092773 and batch: 50, loss is 3.6288680696487425 and perplexity is 37.67015248638897
At time: 144.62691593170166 and batch: 100, loss is 3.5224167013168337 and perplexity is 33.86617407418279
At time: 145.27214407920837 and batch: 150, loss is 3.531192502975464 and perplexity is 34.164684820538916
At time: 145.9139380455017 and batch: 200, loss is 3.4248585224151613 and perplexity is 30.718298593365766
At time: 146.5581021308899 and batch: 250, loss is 3.568204221725464 and perplexity is 35.4528704582999
At time: 147.2017116546631 and batch: 300, loss is 3.5342759609222414 and perplexity is 34.27019277051643
At time: 147.8517565727234 and batch: 350, loss is 3.5121917581558226 and perplexity is 33.52165869584375
At time: 148.5040738582611 and batch: 400, loss is 3.4466418981552125 and perplexity is 31.3947881971455
At time: 149.14926505088806 and batch: 450, loss is 3.4807035303115845 and perplexity is 32.48256650918317
At time: 149.79456686973572 and batch: 500, loss is 3.345286374092102 and perplexity is 28.368698563799715
At time: 150.44076323509216 and batch: 550, loss is 3.3941770696640017 and perplexity is 29.790128184822958
At time: 151.08594751358032 and batch: 600, loss is 3.422101616859436 and perplexity is 30.633727775754984
At time: 151.73388743400574 and batch: 650, loss is 3.259497194290161 and perplexity is 26.036442578790997
At time: 152.3796181678772 and batch: 700, loss is 3.2487927627563478 and perplexity is 25.759223644459368
At time: 153.02789306640625 and batch: 750, loss is 3.3475130701065066 and perplexity is 28.431937412542695
At time: 153.6765742301941 and batch: 800, loss is 3.2949173164367678 and perplexity is 26.975183573391696
At time: 154.3227219581604 and batch: 850, loss is 3.368155484199524 and perplexity is 29.024940690827474
At time: 154.96843242645264 and batch: 900, loss is 3.325700531005859 and perplexity is 27.81847952583535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271436717412243 and perplexity of 71.62446584976868
finished 12 epochs...
Completing Train Step...
At time: 156.5935022830963 and batch: 50, loss is 3.620570950508118 and perplexity is 37.35889181226579
At time: 157.24313688278198 and batch: 100, loss is 3.508082404136658 and perplexity is 33.38418898226471
At time: 157.88552069664001 and batch: 150, loss is 3.5162565994262693 and perplexity is 33.65819623126891
At time: 158.52901792526245 and batch: 200, loss is 3.4093859338760377 and perplexity is 30.24666510316598
At time: 159.17336177825928 and batch: 250, loss is 3.554732279777527 and perplexity is 34.978454274496656
At time: 159.81613516807556 and batch: 300, loss is 3.5219716882705687 and perplexity is 33.8511065377665
At time: 160.45972442626953 and batch: 350, loss is 3.500328040122986 and perplexity is 33.12631693760265
At time: 161.12816762924194 and batch: 400, loss is 3.4360282754898073 and perplexity is 31.06333781764742
At time: 161.79043769836426 and batch: 450, loss is 3.471256957054138 and perplexity is 32.177162347585345
At time: 162.47313332557678 and batch: 500, loss is 3.33659592628479 and perplexity is 28.12323003020223
At time: 163.11710047721863 and batch: 550, loss is 3.386701030731201 and perplexity is 29.5682464583475
At time: 163.76153135299683 and batch: 600, loss is 3.4161794328689576 and perplexity is 30.452845341742886
At time: 164.4082863330841 and batch: 650, loss is 3.2553117179870608 and perplexity is 25.927695403477717
At time: 165.06429743766785 and batch: 700, loss is 3.2462111282348634 and perplexity is 25.692808510141113
At time: 165.71075344085693 and batch: 750, loss is 3.3464809465408325 and perplexity is 28.40260727868634
At time: 166.35545754432678 and batch: 800, loss is 3.295460662841797 and perplexity is 26.989844425011118
At time: 166.99859619140625 and batch: 850, loss is 3.3703628969192505 and perplexity is 29.08908148065063
At time: 167.64460706710815 and batch: 900, loss is 3.3289611101150514 and perplexity is 27.909331914236123
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.271636544841609 and perplexity of 71.63877981277275
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 169.30427312850952 and batch: 50, loss is 3.617547187805176 and perplexity is 37.24609800537461
At time: 169.9683723449707 and batch: 100, loss is 3.5064951181411743 and perplexity is 33.33124075972547
At time: 170.6278579235077 and batch: 150, loss is 3.5162148523330687 and perplexity is 33.656791128743556
At time: 171.26556587219238 and batch: 200, loss is 3.4090140008926393 and perplexity is 30.235417462590227
At time: 171.90251660346985 and batch: 250, loss is 3.5546423482894896 and perplexity is 34.9753087514974
At time: 172.54250621795654 and batch: 300, loss is 3.5216719007492063 and perplexity is 33.84095991943294
At time: 173.19901371002197 and batch: 350, loss is 3.498439631462097 and perplexity is 33.06381994230042
At time: 173.86596751213074 and batch: 400, loss is 3.434020199775696 and perplexity is 31.001022870881904
At time: 174.52672672271729 and batch: 450, loss is 3.4691958665847777 and perplexity is 32.110910603812144
At time: 175.18876266479492 and batch: 500, loss is 3.333349962234497 and perplexity is 28.032091033521642
At time: 175.82908034324646 and batch: 550, loss is 3.383323268890381 and perplexity is 29.468540450133204
At time: 176.48007678985596 and batch: 600, loss is 3.4115030860900877 and perplexity is 30.310769732841244
At time: 177.1196048259735 and batch: 650, loss is 3.2494472312927245 and perplexity is 25.77608776377195
At time: 177.76303577423096 and batch: 700, loss is 3.239534068107605 and perplexity is 25.521827543217587
At time: 178.41097235679626 and batch: 750, loss is 3.3387829494476318 and perplexity is 28.18480349244376
At time: 179.08280634880066 and batch: 800, loss is 3.2862123250961304 and perplexity is 26.74138392232424
At time: 179.75325894355774 and batch: 850, loss is 3.360117130279541 and perplexity is 28.792563162712526
At time: 180.40575575828552 and batch: 900, loss is 3.318920583724976 and perplexity is 27.63050963423808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270826783898759 and perplexity of 71.58079300778041
finished 14 epochs...
Completing Train Step...
At time: 182.05316829681396 and batch: 50, loss is 3.615313696861267 and perplexity is 37.163002014398884
At time: 182.71946668624878 and batch: 100, loss is 3.5030487108230592 and perplexity is 33.216565450069275
At time: 183.36788702011108 and batch: 150, loss is 3.5117795038223267 and perplexity is 33.50784209495289
At time: 184.01407194137573 and batch: 200, loss is 3.40477876663208 and perplexity is 30.107634173757987
At time: 184.66461563110352 and batch: 250, loss is 3.5507019996643066 and perplexity is 34.837765004862185
At time: 185.32302975654602 and batch: 300, loss is 3.5178050708770754 and perplexity is 33.71035536025762
At time: 185.9995834827423 and batch: 350, loss is 3.494742250442505 and perplexity is 32.941796125294815
At time: 186.68072128295898 and batch: 400, loss is 3.4309646701812744 and perplexity is 30.906442897581584
At time: 187.34539103507996 and batch: 450, loss is 3.4664700651168823 and perplexity is 32.02350182016875
At time: 188.0268087387085 and batch: 500, loss is 3.330836715698242 and perplexity is 27.961727934771602
At time: 188.68025398254395 and batch: 550, loss is 3.3811639404296874 and perplexity is 29.404976844094087
At time: 189.32918047904968 and batch: 600, loss is 3.40990394115448 and perplexity is 30.262337154605945
At time: 190.00459384918213 and batch: 650, loss is 3.248496789932251 and perplexity is 25.751600742432345
At time: 190.65343308448792 and batch: 700, loss is 3.2387978982925416 and perplexity is 25.503046058184417
At time: 191.3235821723938 and batch: 750, loss is 3.338692002296448 and perplexity is 28.18224028141958
At time: 192.00570702552795 and batch: 800, loss is 3.2868979835510252 and perplexity is 26.759725665673294
At time: 192.6516296863556 and batch: 850, loss is 3.361630582809448 and perplexity is 28.836172332153506
At time: 193.30003118515015 and batch: 900, loss is 3.32107451915741 and perplexity is 27.690088109015175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270506140303938 and perplexity of 71.55784476429055
finished 15 epochs...
Completing Train Step...
At time: 194.91832327842712 and batch: 50, loss is 3.613555698394775 and perplexity is 37.0977269074277
At time: 195.5950140953064 and batch: 100, loss is 3.5007180404663085 and perplexity is 33.13923873216971
At time: 196.24562001228333 and batch: 150, loss is 3.509138569831848 and perplexity is 33.419466843783766
At time: 196.89406991004944 and batch: 200, loss is 3.40214035987854 and perplexity is 30.028302688803794
At time: 197.54321718215942 and batch: 250, loss is 3.548129482269287 and perplexity is 34.748259425075226
At time: 198.2025876045227 and batch: 300, loss is 3.515360097885132 and perplexity is 33.62803512823342
At time: 198.84878015518188 and batch: 350, loss is 3.492414608001709 and perplexity is 32.86520857135679
At time: 199.49230647087097 and batch: 400, loss is 3.428828196525574 and perplexity is 30.840482582866603
At time: 200.13732528686523 and batch: 450, loss is 3.4645833110809328 and perplexity is 31.963138312317376
At time: 200.78405785560608 and batch: 500, loss is 3.329129433631897 and perplexity is 27.914030106533772
At time: 201.4303925037384 and batch: 550, loss is 3.379696526527405 and perplexity is 29.361859215717537
At time: 202.07729721069336 and batch: 600, loss is 3.4088081169128417 and perplexity is 30.22919311528087
At time: 202.72428107261658 and batch: 650, loss is 3.247818112373352 and perplexity is 25.73412963819652
At time: 203.37026000022888 and batch: 700, loss is 3.238314304351807 and perplexity is 25.490715921270443
At time: 204.02477979660034 and batch: 750, loss is 3.3385579633712767 and perplexity is 28.178463017379205
At time: 204.6901023387909 and batch: 800, loss is 3.2872392654418947 and perplexity is 26.768859834022088
At time: 205.33572554588318 and batch: 850, loss is 3.362408103942871 and perplexity is 28.858601784105275
At time: 205.98157620429993 and batch: 900, loss is 3.322114601135254 and perplexity is 27.718903052976636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270450539784889 and perplexity of 71.55386622158515
finished 16 epochs...
Completing Train Step...
At time: 207.632821559906 and batch: 50, loss is 3.6119576835632325 and perplexity is 37.03849153172312
At time: 208.28223967552185 and batch: 100, loss is 3.4988026571273805 and perplexity is 33.07582513649674
At time: 208.93099975585938 and batch: 150, loss is 3.5070434379577637 and perplexity is 33.34952195105725
At time: 209.58250308036804 and batch: 200, loss is 3.4000569105148317 and perplexity is 29.965805368282066
At time: 210.2597734928131 and batch: 250, loss is 3.5460590887069703 and perplexity is 34.6763912758087
At time: 210.90323209762573 and batch: 300, loss is 3.5134023141860964 and perplexity is 33.56226311392573
At time: 211.54712772369385 and batch: 350, loss is 3.49051287651062 and perplexity is 32.8027671614719
At time: 212.1913299560547 and batch: 400, loss is 3.4270503425598147 and perplexity is 30.78570141948421
At time: 212.83708214759827 and batch: 450, loss is 3.4630053806304932 and perplexity is 31.912742474103176
At time: 213.47714495658875 and batch: 500, loss is 3.327713580131531 and perplexity is 27.874535894904646
At time: 214.12861919403076 and batch: 550, loss is 3.3784697389602663 and perplexity is 29.325860537763752
At time: 214.80600118637085 and batch: 600, loss is 3.407844467163086 and perplexity is 30.200076792121944
At time: 215.45728421211243 and batch: 650, loss is 3.247157120704651 and perplexity is 25.717125213415066
At time: 216.10304141044617 and batch: 700, loss is 3.2378480577468873 and perplexity is 25.4788337317457
At time: 216.75057101249695 and batch: 750, loss is 3.3383305263519287 and perplexity is 28.172054920487575
At time: 217.3982493877411 and batch: 800, loss is 3.2873414516448975 and perplexity is 26.77159538193249
At time: 218.04412007331848 and batch: 850, loss is 3.3627900743484496 and perplexity is 28.86962702145632
At time: 218.6900668144226 and batch: 900, loss is 3.322647924423218 and perplexity is 27.733690132287986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.27052244421554 and perplexity of 71.55901144657668
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 220.3051881790161 and batch: 50, loss is 3.6111557054519654 and perplexity is 37.0087993800596
At time: 220.96106839179993 and batch: 100, loss is 3.49843035697937 and perplexity is 33.063513293895475
At time: 221.64193153381348 and batch: 150, loss is 3.507018389701843 and perplexity is 33.3486866141585
At time: 222.31755328178406 and batch: 200, loss is 3.399983868598938 and perplexity is 29.963616688380323
At time: 222.97112607955933 and batch: 250, loss is 3.5462983560562136 and perplexity is 34.68468919670177
At time: 223.61830806732178 and batch: 300, loss is 3.513244380950928 and perplexity is 33.55696293568074
At time: 224.28533959388733 and batch: 350, loss is 3.48963499546051 and perplexity is 32.7739828702302
At time: 224.93181562423706 and batch: 400, loss is 3.4262776041030882 and perplexity is 30.761921313164713
At time: 225.5759105682373 and batch: 450, loss is 3.4623474073410034 and perplexity is 31.891751648419454
At time: 226.22766137123108 and batch: 500, loss is 3.3263632345199583 and perplexity is 27.83692103992719
At time: 226.90346455574036 and batch: 550, loss is 3.376951985359192 and perplexity is 29.28138486742013
At time: 227.5461916923523 and batch: 600, loss is 3.406204538345337 and perplexity is 30.150591403240316
At time: 228.2018527984619 and batch: 650, loss is 3.2449075508117677 and perplexity is 25.659337765624592
At time: 228.88023257255554 and batch: 700, loss is 3.2357205867767336 and perplexity is 25.4246858720512
At time: 229.5256130695343 and batch: 750, loss is 3.335933485031128 and perplexity is 28.10460621171788
At time: 230.1713604927063 and batch: 800, loss is 3.284247751235962 and perplexity is 26.688900069568657
At time: 230.8335177898407 and batch: 850, loss is 3.3593652725219725 and perplexity is 28.770923386774456
At time: 231.4854428768158 and batch: 900, loss is 3.3190347480773927 and perplexity is 27.633664233545392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270193021591395 and perplexity of 71.53544217158412
finished 18 epochs...
Completing Train Step...
At time: 233.1317102909088 and batch: 50, loss is 3.6105819272994997 and perplexity is 36.98757063040512
At time: 233.77929711341858 and batch: 100, loss is 3.497740979194641 and perplexity is 33.04072789712114
At time: 234.41612696647644 and batch: 150, loss is 3.5061775970458986 and perplexity is 33.32065906769667
At time: 235.05342411994934 and batch: 200, loss is 3.3992303657531737 and perplexity is 29.941047521969637
At time: 235.6986072063446 and batch: 250, loss is 3.545502390861511 and perplexity is 34.657092375823595
At time: 236.33788871765137 and batch: 300, loss is 3.512503914833069 and perplexity is 33.53212433881321
At time: 236.98036074638367 and batch: 350, loss is 3.489042625427246 and perplexity is 32.75457429400418
At time: 237.66772866249084 and batch: 400, loss is 3.425731143951416 and perplexity is 30.7451157411841
At time: 238.34276819229126 and batch: 450, loss is 3.461801586151123 and perplexity is 31.874349204330763
At time: 238.98803067207336 and batch: 500, loss is 3.325973181724548 and perplexity is 27.826065288355714
At time: 239.6323823928833 and batch: 550, loss is 3.3766310882568358 and perplexity is 29.27199006332652
At time: 240.27696633338928 and batch: 600, loss is 3.4059542417526245 and perplexity is 30.143045757307945
At time: 240.9312870502472 and batch: 650, loss is 3.244825348854065 and perplexity is 25.65722860451667
At time: 241.5894377231598 and batch: 700, loss is 3.235640687942505 and perplexity is 25.422654550440583
At time: 242.2682716846466 and batch: 750, loss is 3.3359302568435667 and perplexity is 28.104515484924132
At time: 242.9281301498413 and batch: 800, loss is 3.284408941268921 and perplexity is 26.693202400987666
At time: 243.57233834266663 and batch: 850, loss is 3.359639186859131 and perplexity is 28.77880523460966
At time: 244.2164487838745 and batch: 900, loss is 3.3194358110427857 and perplexity is 27.64474929562292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.270033744916524 and perplexity of 71.52404915156484
finished 19 epochs...
Completing Train Step...
At time: 245.9118139743805 and batch: 50, loss is 3.6100848293304444 and perplexity is 36.969188753339864
At time: 246.58415627479553 and batch: 100, loss is 3.497149291038513 and perplexity is 33.021183872308754
At time: 247.26550459861755 and batch: 150, loss is 3.50548734664917 and perplexity is 33.29766740547077
At time: 247.91081404685974 and batch: 200, loss is 3.3985910654067992 and perplexity is 29.921912317141285
At time: 248.55244970321655 and batch: 250, loss is 3.544834671020508 and perplexity is 34.63395887182209
At time: 249.21892547607422 and batch: 300, loss is 3.5118791675567627 and perplexity is 33.511181778048844
At time: 249.87609910964966 and batch: 350, loss is 3.488513879776001 and perplexity is 32.737260033111305
At time: 250.5264699459076 and batch: 400, loss is 3.4252389430999757 and perplexity is 30.729986692609387
At time: 251.1744785308838 and batch: 450, loss is 3.4613340997695925 and perplexity is 31.859451862581093
At time: 251.8514084815979 and batch: 500, loss is 3.3256083345413208 and perplexity is 27.815914878601756
At time: 252.49959564208984 and batch: 550, loss is 3.3763303709030152 and perplexity is 29.263188791347552
At time: 253.16337037086487 and batch: 600, loss is 3.4057238054275514 and perplexity is 30.13610050486708
At time: 253.80619645118713 and batch: 650, loss is 3.2447261905670164 and perplexity is 25.654684603809297
At time: 254.47455501556396 and batch: 700, loss is 3.2355569410324097 and perplexity is 25.420525570824545
At time: 255.13210701942444 and batch: 750, loss is 3.335917067527771 and perplexity is 28.104144808038615
At time: 255.77924370765686 and batch: 800, loss is 3.284523468017578 and perplexity is 26.69625966173562
At time: 256.4248843193054 and batch: 850, loss is 3.359858751296997 and perplexity is 28.785124730546357
At time: 257.0710618495941 and batch: 900, loss is 3.3197414255142212 and perplexity is 27.653199222211022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.269955151701627 and perplexity of 71.51842806749198
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f41055bbb70>
ELAPSED
1608.154682636261


RESULTS SO FAR:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}, {'params': {'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.66175442849013}, {'params': {'wordvec_dim': 300, 'dropout': 0.6196663492825968, 'rnn_dropout': 0.0427385332517114, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.26948139506304}, {'params': {'wordvec_dim': 300, 'dropout': 0.5317050526091995, 'rnn_dropout': 0.4783827073446225, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.81185906461589}, {'params': {'wordvec_dim': 300, 'dropout': 0.31036864324501723, 'rnn_dropout': 0.09018996615541643, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.51842806749198}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'wordvec_dim': 300, 'dropout': 0.8413993489584263, 'rnn_dropout': 0.20669165520677413, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.47915260991952}, {'params': {'wordvec_dim': 300, 'dropout': 0.2429380477296963, 'rnn_dropout': 0.39746195688567276, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.99716879408732}, {'params': {'wordvec_dim': 300, 'dropout': 0.3275360299937369, 'rnn_dropout': 0.09858518316094289, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.66175442849013}, {'params': {'wordvec_dim': 300, 'dropout': 0.6196663492825968, 'rnn_dropout': 0.0427385332517114, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.26948139506304}, {'params': {'wordvec_dim': 300, 'dropout': 0.5317050526091995, 'rnn_dropout': 0.4783827073446225, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -70.81185906461589}, {'params': {'wordvec_dim': 300, 'dropout': 0.31036864324501723, 'rnn_dropout': 0.09018996615541643, 'data': 'ptb', 'seq_len': 35, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_source': 'glove', 'num_layers': 3, 'tie_weights': 'TRUE'}, 'best_accuracy': -71.51842806749198}]
