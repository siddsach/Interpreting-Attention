FALSE
TRUE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5432519912719727 and batch: 50, loss is 7.071847515106201 and perplexity is 1178.322994026448
At time: 2.661794662475586 and batch: 100, loss is 6.474772834777832 and perplexity is 648.571877900412
At time: 3.7751376628875732 and batch: 150, loss is 6.010754537582398 and perplexity is 407.79089777804876
At time: 4.888793230056763 and batch: 200, loss is 5.809228506088257 and perplexity is 333.36183981026915
At time: 5.997877359390259 and batch: 250, loss is 5.788742303848267 and perplexity is 326.60199987979007
At time: 7.112060070037842 and batch: 300, loss is 5.641827688217163 and perplexity is 281.9776149611964
At time: 8.224716663360596 and batch: 350, loss is 5.586814517974854 and perplexity is 266.8841096645793
At time: 9.339264631271362 and batch: 400, loss is 5.402506837844848 and perplexity is 221.96214245419122
At time: 10.450882196426392 and batch: 450, loss is 5.380715799331665 and perplexity is 217.17767543562982
At time: 11.568048238754272 and batch: 500, loss is 5.3016499996185305 and perplexity is 200.66763849323564
At time: 12.680426359176636 and batch: 550, loss is 5.334219627380371 and perplexity is 207.3109059125457
At time: 13.794856786727905 and batch: 600, loss is 5.225511655807495 and perplexity is 185.95629180584402
At time: 14.905382871627808 and batch: 650, loss is 5.106168279647827 and perplexity is 165.0367670440979
At time: 16.018609285354614 and batch: 700, loss is 5.182162160873413 and perplexity is 178.06740544287817
At time: 17.131986379623413 and batch: 750, loss is 5.149784440994263 and perplexity is 172.39432516202962
At time: 18.243534803390503 and batch: 800, loss is 5.11013801574707 and perplexity is 165.69322156947075
At time: 19.354954719543457 and batch: 850, loss is 5.1286782550811765 and perplexity is 168.79386810015885
At time: 20.470364332199097 and batch: 900, loss is 5.035940523147583 and perplexity is 153.84421863786952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.972740695901113 and perplexity of 144.42216268808485
finished 1 epochs...
Completing Train Step...
At time: 22.761062622070312 and batch: 50, loss is 4.92921085357666 and perplexity is 138.27035371946644
At time: 23.66076683998108 and batch: 100, loss is 4.801676158905029 and perplexity is 121.71425907480788
At time: 24.5465886592865 and batch: 150, loss is 4.790592422485352 and perplexity is 120.37265901394849
At time: 25.428726196289062 and batch: 200, loss is 4.670149507522583 and perplexity is 106.71369574012718
At time: 26.312925338745117 and batch: 250, loss is 4.7763026905059816 and perplexity is 118.66479748227265
At time: 27.196402549743652 and batch: 300, loss is 4.704966707229614 and perplexity is 110.4946062146811
At time: 28.080660343170166 and batch: 350, loss is 4.696481056213379 and perplexity is 109.56095447099264
At time: 28.966376543045044 and batch: 400, loss is 4.568975076675415 and perplexity is 96.44521015380795
At time: 29.848406553268433 and batch: 450, loss is 4.594106416702271 and perplexity is 98.89972091187327
At time: 30.732646703720093 and batch: 500, loss is 4.491430473327637 and perplexity is 89.24902295595706
At time: 31.616381645202637 and batch: 550, loss is 4.556895942687988 and perplexity is 95.28724323613245
At time: 32.50234937667847 and batch: 600, loss is 4.50712742805481 and perplexity is 90.6610138149359
At time: 33.39117455482483 and batch: 650, loss is 4.36762954711914 and perplexity is 78.85648439465508
At time: 34.28042435646057 and batch: 700, loss is 4.412819738388062 and perplexity is 82.50176923538328
At time: 35.170084714889526 and batch: 750, loss is 4.458348121643066 and perplexity is 86.34476018628548
At time: 36.078981161117554 and batch: 800, loss is 4.416129455566407 and perplexity is 82.7752791287877
At time: 36.96835923194885 and batch: 850, loss is 4.465718688964844 and perplexity is 86.98352117783102
At time: 37.858059883117676 and batch: 900, loss is 4.400768432617188 and perplexity is 81.51348222321772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.529723284995719 and perplexity of 92.73289694747415
finished 2 epochs...
Completing Train Step...
At time: 39.973100662231445 and batch: 50, loss is 4.447556715011597 and perplexity is 85.41798834837249
At time: 40.865970611572266 and batch: 100, loss is 4.325552368164063 and perplexity is 75.6072643733733
At time: 41.7546329498291 and batch: 150, loss is 4.327058281898498 and perplexity is 75.72120816444297
At time: 42.64484763145447 and batch: 200, loss is 4.2176182746887205 and perplexity is 67.87164002866587
At time: 43.535686016082764 and batch: 250, loss is 4.356813859939575 and perplexity is 78.00819302376115
At time: 44.42856454849243 and batch: 300, loss is 4.308656930923462 and perplexity is 74.3405773307226
At time: 45.31949186325073 and batch: 350, loss is 4.315789728164673 and perplexity is 74.87272920514816
At time: 46.21123647689819 and batch: 400, loss is 4.220521445274353 and perplexity is 68.06896927923624
At time: 47.10048460960388 and batch: 450, loss is 4.257168560028076 and perplexity is 70.60977281267785
At time: 47.99053454399109 and batch: 500, loss is 4.131834030151367 and perplexity is 62.292063757033006
At time: 48.88028526306152 and batch: 550, loss is 4.20577175617218 and perplexity is 67.07234119034068
At time: 49.76988410949707 and batch: 600, loss is 4.192488255500794 and perplexity is 66.18727709008658
At time: 50.660624265670776 and batch: 650, loss is 4.041716928482056 and perplexity is 56.92399337749879
At time: 51.554067850112915 and batch: 700, loss is 4.065160918235779 and perplexity is 58.274285163255236
At time: 52.45271325111389 and batch: 750, loss is 4.146939001083374 and perplexity is 63.24012576817564
At time: 53.342748403549194 and batch: 800, loss is 4.117995977401733 and perplexity is 61.435999683503326
At time: 54.231568336486816 and batch: 850, loss is 4.1786856985092165 and perplexity is 65.27999920706766
At time: 55.119935512542725 and batch: 900, loss is 4.125378065109253 and perplexity is 61.89120373247141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387447409433861 and perplexity of 80.43483949948465
finished 3 epochs...
Completing Train Step...
At time: 57.27059078216553 and batch: 50, loss is 4.1939921998977665 and perplexity is 66.28689396496098
At time: 58.163132667541504 and batch: 100, loss is 4.069976716041565 and perplexity is 58.5555991698823
At time: 59.0580940246582 and batch: 150, loss is 4.073007230758667 and perplexity is 58.733321934645005
At time: 59.9512677192688 and batch: 200, loss is 3.967545371055603 and perplexity is 52.85463296743206
At time: 60.84402060508728 and batch: 250, loss is 4.1131603813171385 and perplexity is 61.139637126277925
At time: 61.73665118217468 and batch: 300, loss is 4.075027322769165 and perplexity is 58.85208856841088
At time: 62.62972116470337 and batch: 350, loss is 4.082344484329224 and perplexity is 59.284298153891115
At time: 63.52299666404724 and batch: 400, loss is 3.999599051475525 and perplexity is 54.57626337345455
At time: 64.41490864753723 and batch: 450, loss is 4.040877118110656 and perplexity is 56.87620808557928
At time: 65.30622243881226 and batch: 500, loss is 3.9117389249801637 and perplexity is 49.98579799495237
At time: 66.19485592842102 and batch: 550, loss is 3.987163920402527 and perplexity is 53.90180258022153
At time: 67.08935856819153 and batch: 600, loss is 3.9836368274688723 and perplexity is 53.712020798965874
At time: 67.98257851600647 and batch: 650, loss is 3.831056294441223 and perplexity is 46.111219543671815
At time: 68.87326264381409 and batch: 700, loss is 3.8522134256362914 and perplexity is 47.09719408289823
At time: 69.76334023475647 and batch: 750, loss is 3.94475088596344 and perplexity is 51.663466414252156
At time: 70.65281820297241 and batch: 800, loss is 3.9167245960235597 and perplexity is 50.23563302071316
At time: 71.5491988658905 and batch: 850, loss is 3.9797308111190794 and perplexity is 53.502629975756456
At time: 72.44670486450195 and batch: 900, loss is 3.9333984422683717 and perplexity is 51.08027639994182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336673266267123 and perplexity of 76.45277777651079
finished 4 epochs...
Completing Train Step...
At time: 74.58354353904724 and batch: 50, loss is 4.004042115211487 and perplexity is 54.81928867876089
At time: 75.47416567802429 and batch: 100, loss is 3.887741847038269 and perplexity is 48.8005628711895
At time: 76.3668212890625 and batch: 150, loss is 3.8913752126693724 and perplexity is 48.978195666134276
At time: 77.2586362361908 and batch: 200, loss is 3.7853388452529906 and perplexity is 44.05059436496867
At time: 78.15141487121582 and batch: 250, loss is 3.9359428453445435 and perplexity is 51.21041069919391
At time: 79.04323768615723 and batch: 300, loss is 3.902081260681152 and perplexity is 49.50537555163539
At time: 79.94922876358032 and batch: 350, loss is 3.9080276441574098 and perplexity is 49.80063047830573
At time: 80.8393349647522 and batch: 400, loss is 3.8335698509216307 and perplexity is 46.2272684850602
At time: 81.73038172721863 and batch: 450, loss is 3.873368730545044 and perplexity is 48.104163400318804
At time: 82.62156414985657 and batch: 500, loss is 3.748592720031738 and perplexity is 42.461285018551074
At time: 83.51459169387817 and batch: 550, loss is 3.820424737930298 and perplexity is 45.62358227203851
At time: 84.40806245803833 and batch: 600, loss is 3.824403123855591 and perplexity is 45.805452023761895
At time: 85.2965989112854 and batch: 650, loss is 3.6725321340560915 and perplexity is 39.35142288995705
At time: 86.18658542633057 and batch: 700, loss is 3.689838213920593 and perplexity is 40.03836878255043
At time: 87.0772602558136 and batch: 750, loss is 3.78727011680603 and perplexity is 44.135750227840056
At time: 87.96566414833069 and batch: 800, loss is 3.7618916702270506 and perplexity is 43.02974712963563
At time: 88.84941244125366 and batch: 850, loss is 3.825425524711609 and perplexity is 45.852307505582125
At time: 89.73160481452942 and batch: 900, loss is 3.781598916053772 and perplexity is 43.88615594654905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322722134524828 and perplexity of 75.3935806803991
finished 5 epochs...
Completing Train Step...
At time: 91.82048320770264 and batch: 50, loss is 3.8538414430618286 and perplexity is 47.17393158360425
At time: 92.71212673187256 and batch: 100, loss is 3.7442584943771364 and perplexity is 42.27764648060606
At time: 93.5955958366394 and batch: 150, loss is 3.7472432327270506 and perplexity is 42.4040226995542
At time: 94.48394179344177 and batch: 200, loss is 3.640912342071533 and perplexity is 38.12660536866188
At time: 95.37440800666809 and batch: 250, loss is 3.791169104576111 and perplexity is 44.308170892804036
At time: 96.26463150978088 and batch: 300, loss is 3.759721808433533 and perplexity is 42.9364797506097
At time: 97.15384864807129 and batch: 350, loss is 3.7636115026473997 and perplexity is 43.10381475748586
At time: 98.04360318183899 and batch: 400, loss is 3.694979486465454 and perplexity is 40.244747017495754
At time: 98.93543004989624 and batch: 450, loss is 3.7337467002868654 and perplexity is 41.83556019696242
At time: 99.82868814468384 and batch: 500, loss is 3.6140363597869873 and perplexity is 37.11556263862123
At time: 100.71911382675171 and batch: 550, loss is 3.6819032192230225 and perplexity is 39.72192170191477
At time: 101.62368988990784 and batch: 600, loss is 3.6917684173583982 and perplexity is 40.115725612811204
At time: 102.51272654533386 and batch: 650, loss is 3.542136344909668 and perplexity is 34.54063112724784
At time: 103.40154075622559 and batch: 700, loss is 3.55460205078125 and perplexity is 34.973899362102436
At time: 104.29117369651794 and batch: 750, loss is 3.6551605081558227 and perplexity is 38.67372806578559
At time: 105.18144154548645 and batch: 800, loss is 3.633508906364441 and perplexity is 37.84537979931302
At time: 106.07121324539185 and batch: 850, loss is 3.695733485221863 and perplexity is 40.275102949428245
At time: 106.95643210411072 and batch: 900, loss is 3.6529900360107423 and perplexity is 38.58987884538847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33661348525792 and perplexity of 76.44820748890847
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.06728625297546 and batch: 50, loss is 3.761943497657776 and perplexity is 43.031977308665844
At time: 109.95669889450073 and batch: 100, loss is 3.658854937553406 and perplexity is 38.816869674139596
At time: 110.84576106071472 and batch: 150, loss is 3.6658266162872315 and perplexity is 39.088433948466744
At time: 111.73931765556335 and batch: 200, loss is 3.546326894760132 and perplexity is 34.68567906690199
At time: 112.62967205047607 and batch: 250, loss is 3.6939504194259642 and perplexity is 40.20335377669021
At time: 113.5224232673645 and batch: 300, loss is 3.6475535821914673 and perplexity is 38.38065598167111
At time: 114.41470670700073 and batch: 350, loss is 3.64521342754364 and perplexity is 38.2909443217088
At time: 115.3053994178772 and batch: 400, loss is 3.568691067695618 and perplexity is 35.470134747596745
At time: 116.19648575782776 and batch: 450, loss is 3.5957364940643313 and perplexity is 36.44252981355952
At time: 117.09038424491882 and batch: 500, loss is 3.4643257904052733 and perplexity is 31.95490820310012
At time: 117.98228025436401 and batch: 550, loss is 3.5116168689727782 and perplexity is 33.50239299521385
At time: 118.87566041946411 and batch: 600, loss is 3.5213911819458006 and perplexity is 33.83146145891887
At time: 119.76734781265259 and batch: 650, loss is 3.3490590476989746 and perplexity is 28.475926545038174
At time: 120.65855622291565 and batch: 700, loss is 3.347468514442444 and perplexity is 28.430670636911916
At time: 121.54802751541138 and batch: 750, loss is 3.4367333364486696 and perplexity is 31.08524708717227
At time: 122.43781018257141 and batch: 800, loss is 3.398649744987488 and perplexity is 29.923668173925424
At time: 123.32931852340698 and batch: 850, loss is 3.443666753768921 and perplexity is 31.30152297682491
At time: 124.22482967376709 and batch: 900, loss is 3.3922343158721926 and perplexity is 29.732309482270615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302245675700984 and perplexity of 73.86548551087895
finished 7 epochs...
Completing Train Step...
At time: 126.34929704666138 and batch: 50, loss is 3.669363889694214 and perplexity is 39.226945258154814
At time: 127.24280047416687 and batch: 100, loss is 3.556350588798523 and perplexity is 35.03510605026557
At time: 128.13656163215637 and batch: 150, loss is 3.558760976791382 and perplexity is 35.11965610750294
At time: 129.02918243408203 and batch: 200, loss is 3.442403893470764 and perplexity is 31.262018475768826
At time: 129.921480178833 and batch: 250, loss is 3.589520797729492 and perplexity is 36.21671663466096
At time: 130.8163161277771 and batch: 300, loss is 3.5466619443893435 and perplexity is 34.69730243790664
At time: 131.70953345298767 and batch: 350, loss is 3.5492891025543214 and perplexity is 34.78857758393356
At time: 132.6015875339508 and batch: 400, loss is 3.4767893981933593 and perplexity is 32.3556739514491
At time: 133.49366068840027 and batch: 450, loss is 3.508846001625061 and perplexity is 33.409690800446974
At time: 134.38783597946167 and batch: 500, loss is 3.384286279678345 and perplexity is 29.496932641291203
At time: 135.27968311309814 and batch: 550, loss is 3.4333054304122923 and perplexity is 30.978872206750733
At time: 136.17113828659058 and batch: 600, loss is 3.4515867185592652 and perplexity is 31.550414240717355
At time: 137.0631787776947 and batch: 650, loss is 3.284092946052551 and perplexity is 26.684768809276857
At time: 137.95659375190735 and batch: 700, loss is 3.2869469499588013 and perplexity is 26.761036025393757
At time: 138.85030579566956 and batch: 750, loss is 3.3828202199935915 and perplexity is 29.45372006137241
At time: 139.7439832687378 and batch: 800, loss is 3.3501500415802004 and perplexity is 28.507010559812816
At time: 140.63706302642822 and batch: 850, loss is 3.402645387649536 and perplexity is 30.04347164562734
At time: 141.53166699409485 and batch: 900, loss is 3.3609102964401245 and perplexity is 28.815409508766194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315119077081549 and perplexity of 74.82253256230815
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.64516520500183 and batch: 50, loss is 3.630434365272522 and perplexity is 37.729201313263125
At time: 144.54184436798096 and batch: 100, loss is 3.5309613227844237 and perplexity is 34.15678753505843
At time: 145.434579372406 and batch: 150, loss is 3.539825882911682 and perplexity is 34.46091843367692
At time: 146.3347716331482 and batch: 200, loss is 3.41999183177948 and perplexity is 30.56916532434694
At time: 147.227801322937 and batch: 250, loss is 3.5707666158676146 and perplexity is 35.54383117471849
At time: 148.12086987495422 and batch: 300, loss is 3.520797872543335 and perplexity is 33.81139488816733
At time: 149.01368165016174 and batch: 350, loss is 3.518300385475159 and perplexity is 33.72705672724812
At time: 149.90402936935425 and batch: 400, loss is 3.448279256820679 and perplexity is 31.446234832453847
At time: 150.79380702972412 and batch: 450, loss is 3.4732341718673707 and perplexity is 32.24084644745249
At time: 151.68320608139038 and batch: 500, loss is 3.3437376403808594 and perplexity is 28.32479700867418
At time: 152.57326412200928 and batch: 550, loss is 3.384547357559204 and perplexity is 29.504634643324472
At time: 153.46538376808167 and batch: 600, loss is 3.404644889831543 and perplexity is 30.103603729820556
At time: 154.35674285888672 and batch: 650, loss is 3.2238983869552613 and perplexity is 25.125879907379883
At time: 155.24890041351318 and batch: 700, loss is 3.2214331579208375 and perplexity is 25.064015145451588
At time: 156.14187741279602 and batch: 750, loss is 3.312295184135437 and perplexity is 27.448051563292736
At time: 157.0328917503357 and batch: 800, loss is 3.2761158037185667 and perplexity is 26.472747397154116
At time: 157.92528820037842 and batch: 850, loss is 3.324659352302551 and perplexity is 27.78953059051729
At time: 158.8179006576538 and batch: 900, loss is 3.2890922260284423 and perplexity is 26.818507459582765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309804837997645 and perplexity of 74.42596240302775
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.94312524795532 and batch: 50, loss is 3.622559180259705 and perplexity is 37.43324376233974
At time: 161.83883810043335 and batch: 100, loss is 3.5134198093414306 and perplexity is 33.56285029606868
At time: 162.72906064987183 and batch: 150, loss is 3.524732060432434 and perplexity is 33.94467727551679
At time: 163.61874151229858 and batch: 200, loss is 3.402435884475708 and perplexity is 30.037178102246436
At time: 164.50986099243164 and batch: 250, loss is 3.55303373336792 and perplexity is 34.9190921754871
At time: 165.40038681030273 and batch: 300, loss is 3.504063129425049 and perplexity is 33.25027804840849
At time: 166.29116296768188 and batch: 350, loss is 3.502935791015625 and perplexity is 33.21281485365732
At time: 167.18084263801575 and batch: 400, loss is 3.4329768371582032 and perplexity is 30.96869443059018
At time: 168.07085466384888 and batch: 450, loss is 3.456107497215271 and perplexity is 31.693369571238424
At time: 168.96675610542297 and batch: 500, loss is 3.326058769226074 and perplexity is 27.828446953679983
At time: 169.85791492462158 and batch: 550, loss is 3.365288424491882 and perplexity is 28.941843631892556
At time: 170.74765610694885 and batch: 600, loss is 3.3889280557632446 and perplexity is 29.634169061761828
At time: 171.6384584903717 and batch: 650, loss is 3.202701268196106 and perplexity is 24.598888726442542
At time: 172.53225708007812 and batch: 700, loss is 3.200307173728943 and perplexity is 24.54006710340094
At time: 173.423344373703 and batch: 750, loss is 3.2883458518981934 and perplexity is 26.798498287496137
At time: 174.31597518920898 and batch: 800, loss is 3.2512590789794924 and perplexity is 25.82283244309246
At time: 175.20859456062317 and batch: 850, loss is 3.300083031654358 and perplexity is 27.1148902213848
At time: 176.10053992271423 and batch: 900, loss is 3.2691685962677 and perplexity is 26.289473088636505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308301115689212 and perplexity of 74.31413052615241
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.23837208747864 and batch: 50, loss is 3.613637437820435 and perplexity is 37.10075937825291
At time: 179.1294629573822 and batch: 100, loss is 3.50488440990448 and perplexity is 33.27759706945909
At time: 180.02070784568787 and batch: 150, loss is 3.5148948764801027 and perplexity is 33.61239428499233
At time: 180.91832399368286 and batch: 200, loss is 3.393374800682068 and perplexity is 29.76623807344149
At time: 181.8113558292389 and batch: 250, loss is 3.544463062286377 and perplexity is 34.62109098126612
At time: 182.70427298545837 and batch: 300, loss is 3.496885395050049 and perplexity is 33.01247086406504
At time: 183.5968735218048 and batch: 350, loss is 3.4954513692855835 and perplexity is 32.965164057993604
At time: 184.487309217453 and batch: 400, loss is 3.4268874883651734 and perplexity is 30.78068824709223
At time: 185.37896919250488 and batch: 450, loss is 3.4491195821762086 and perplexity is 31.472671006857677
At time: 186.27142930030823 and batch: 500, loss is 3.3201989364624023 and perplexity is 27.665853758181058
At time: 187.16494917869568 and batch: 550, loss is 3.3581305599212645 and perplexity is 28.735421486966587
At time: 188.05631351470947 and batch: 600, loss is 3.382483568191528 and perplexity is 29.443806082309376
At time: 188.94666695594788 and batch: 650, loss is 3.1964573097229003 and perplexity is 24.44577280892367
At time: 189.8373692035675 and batch: 700, loss is 3.193895263671875 and perplexity is 24.3832217767626
At time: 190.73515629768372 and batch: 750, loss is 3.2812492179870607 and perplexity is 26.608992378140417
At time: 191.62857460975647 and batch: 800, loss is 3.242783603668213 and perplexity is 25.60489652409756
At time: 192.5185842514038 and batch: 850, loss is 3.2913351964950563 and perplexity is 26.87872809107516
At time: 193.40949010849 and batch: 900, loss is 3.2610795450210572 and perplexity is 26.07767397539772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305911547517123 and perplexity of 74.13676384439692
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.51996326446533 and batch: 50, loss is 3.6096273851394653 and perplexity is 36.95228128010697
At time: 196.41579174995422 and batch: 100, loss is 3.50163920879364 and perplexity is 33.16977961376845
At time: 197.3067409992218 and batch: 150, loss is 3.5112366580963137 and perplexity is 33.48965744226078
At time: 198.1991548538208 and batch: 200, loss is 3.3900552558898926 and perplexity is 29.66759153425014
At time: 199.09001398086548 and batch: 250, loss is 3.541643667221069 and perplexity is 34.523617920299195
At time: 199.98189234733582 and batch: 300, loss is 3.4945928049087525 and perplexity is 32.93687348883228
At time: 200.8743085861206 and batch: 350, loss is 3.4922978591918947 and perplexity is 32.861371821344136
At time: 201.7651584148407 and batch: 400, loss is 3.4239674520492556 and perplexity is 30.690938619439496
At time: 202.65584349632263 and batch: 450, loss is 3.4469646263122558 and perplexity is 31.40492181439498
At time: 203.55000162124634 and batch: 500, loss is 3.318309016227722 and perplexity is 27.613616878669855
At time: 204.4419560432434 and batch: 550, loss is 3.355401511192322 and perplexity is 28.657108030705718
At time: 205.334459066391 and batch: 600, loss is 3.3800206708908083 and perplexity is 29.371378239566972
At time: 206.2284791469574 and batch: 650, loss is 3.1943091344833374 and perplexity is 24.393315369122973
At time: 207.12138390541077 and batch: 700, loss is 3.1918613052368165 and perplexity is 24.33367771951269
At time: 208.01339173316956 and batch: 750, loss is 3.2790707635879515 and perplexity is 26.551088994477986
At time: 208.90423226356506 and batch: 800, loss is 3.240410885810852 and perplexity is 25.54421534700372
At time: 209.79553747177124 and batch: 850, loss is 3.288932728767395 and perplexity is 26.814230322202498
At time: 210.68675923347473 and batch: 900, loss is 3.258076558113098 and perplexity is 25.999480527571535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3047506254013275 and perplexity of 74.05074677487632
Annealing...
Model not improving. Stopping early with 73.86548551087895 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -73.86548551087895
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
223.28014612197876


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.354247808456421 and batch: 50, loss is 7.114007883071899 and perplexity is 1229.0636312859003
At time: 2.4714713096618652 and batch: 100, loss is 6.559087896347046 and perplexity is 705.6277952988381
At time: 3.590402841567993 and batch: 150, loss is 6.283924970626831 and perplexity is 535.8878856490659
At time: 4.724040985107422 and batch: 200, loss is 6.074523391723633 and perplexity is 434.6422990258264
At time: 5.843187093734741 and batch: 250, loss is 6.111324396133423 and perplexity is 450.9355371478441
At time: 6.966166973114014 and batch: 300, loss is 6.02173942565918 and perplexity is 412.29512909444276
At time: 8.08913540840149 and batch: 350, loss is 6.024473514556885 and perplexity is 413.42392303780446
At time: 9.210174322128296 and batch: 400, loss is 5.890374307632446 and perplexity is 361.5405864493533
At time: 10.332558631896973 and batch: 450, loss is 5.8877379512786865 and perplexity is 360.5886919450231
At time: 11.453516960144043 and batch: 500, loss is 5.839851560592652 and perplexity is 343.7283140506535
At time: 12.57528829574585 and batch: 550, loss is 5.880107383728028 and perplexity is 357.84766666222055
At time: 13.696513652801514 and batch: 600, loss is 5.811607608795166 and perplexity is 334.15588604998743
At time: 14.81992483139038 and batch: 650, loss is 5.726758966445923 and perplexity is 306.9727453994939
At time: 15.943891286849976 and batch: 700, loss is 5.819375257492066 and perplexity is 336.761598611126
At time: 17.065682649612427 and batch: 750, loss is 5.769191484451294 and perplexity is 320.2786776456746
At time: 18.187209606170654 and batch: 800, loss is 5.774145555496216 and perplexity is 321.86929773818616
At time: 19.30715274810791 and batch: 850, loss is 5.800712804794312 and perplexity is 330.53508295051546
At time: 20.428406476974487 and batch: 900, loss is 5.67713080406189 and perplexity is 292.110105277373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.630290828339041 and perplexity of 278.7431722597849
finished 1 epochs...
Completing Train Step...
At time: 22.78873372077942 and batch: 50, loss is 5.454697160720825 and perplexity is 233.8540401558672
At time: 23.680474519729614 and batch: 100, loss is 5.253868169784546 and perplexity is 191.30483867681713
At time: 24.572461128234863 and batch: 150, loss is 5.187679624557495 and perplexity is 179.05260127753792
At time: 25.46460247039795 and batch: 200, loss is 5.029925117492676 and perplexity is 152.92156111693822
At time: 26.356364250183105 and batch: 250, loss is 5.073786096572876 and perplexity is 159.77811900405467
At time: 27.24836492538452 and batch: 300, loss is 4.985616407394409 and perplexity is 146.29372377221478
At time: 28.143099069595337 and batch: 350, loss is 4.952634325027466 and perplexity is 141.54735494632936
At time: 29.035953760147095 and batch: 400, loss is 4.796467332839966 and perplexity is 121.08191897416171
At time: 29.94352126121521 and batch: 450, loss is 4.802271165847778 and perplexity is 121.78670145371092
At time: 30.83374261856079 and batch: 500, loss is 4.715710744857788 and perplexity is 111.6881647590695
At time: 31.724432945251465 and batch: 550, loss is 4.767967662811279 and perplexity is 117.67983366156723
At time: 32.615973472595215 and batch: 600, loss is 4.6908847618103025 and perplexity is 108.94953156173048
At time: 33.51115942001343 and batch: 650, loss is 4.562155904769898 and perplexity is 95.78977100231431
At time: 34.40329313278198 and batch: 700, loss is 4.612004880905151 and perplexity is 100.68581047469897
At time: 35.29529285430908 and batch: 750, loss is 4.636126260757447 and perplexity is 103.14401966429307
At time: 36.18777632713318 and batch: 800, loss is 4.581420888900757 and perplexity is 97.65304981122341
At time: 37.079142808914185 and batch: 850, loss is 4.628535928726197 and perplexity is 102.36408603015792
At time: 37.97097730636597 and batch: 900, loss is 4.552868146896362 and perplexity is 94.90421757123396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.649184135541524 and perplexity of 104.49969320273112
finished 2 epochs...
Completing Train Step...
At time: 40.11104726791382 and batch: 50, loss is 4.597635049819946 and perplexity is 99.2493181799171
At time: 41.009236574172974 and batch: 100, loss is 4.474682822227478 and perplexity is 87.76675832963524
At time: 41.90018177032471 and batch: 150, loss is 4.477079467773438 and perplexity is 87.97735640373274
At time: 42.791475772857666 and batch: 200, loss is 4.362533588409423 and perplexity is 78.4556571730882
At time: 43.67807984352112 and batch: 250, loss is 4.496807136535645 and perplexity is 89.73017723717516
At time: 44.569982051849365 and batch: 300, loss is 4.442401571273804 and perplexity is 84.97877940594097
At time: 45.46202111244202 and batch: 350, loss is 4.441166353225708 and perplexity is 84.87387688599463
At time: 46.3559787273407 and batch: 400, loss is 4.339065594673157 and perplexity is 76.63589688168805
At time: 47.24728202819824 and batch: 450, loss is 4.370253524780273 and perplexity is 79.06367375939516
At time: 48.139050245285034 and batch: 500, loss is 4.259489302635193 and perplexity is 70.77383021479226
At time: 49.030367612838745 and batch: 550, loss is 4.32680823802948 and perplexity is 75.70227690750782
At time: 49.921963930130005 and batch: 600, loss is 4.3108086681365965 and perplexity is 74.50071093834781
At time: 50.81350326538086 and batch: 650, loss is 4.162794184684754 and perplexity is 64.25080061841872
At time: 51.70496129989624 and batch: 700, loss is 4.1865362501144405 and perplexity is 65.79450012519808
At time: 52.62607264518738 and batch: 750, loss is 4.270586538314819 and perplexity is 71.56359810394795
At time: 53.517699241638184 and batch: 800, loss is 4.225440864562988 and perplexity is 68.40465408984885
At time: 54.411686182022095 and batch: 850, loss is 4.29244469165802 and perplexity is 73.14506723696084
At time: 55.307114601135254 and batch: 900, loss is 4.227755274772644 and perplexity is 68.56315386566179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4453321483037245 and perplexity of 85.22818153247573
finished 3 epochs...
Completing Train Step...
At time: 57.45345997810364 and batch: 50, loss is 4.303573079109192 and perplexity is 73.96359991238404
At time: 58.3531494140625 and batch: 100, loss is 4.180602879524231 and perplexity is 65.40527282994219
At time: 59.24384331703186 and batch: 150, loss is 4.182628393173218 and perplexity is 65.53788636269576
At time: 60.133261919021606 and batch: 200, loss is 4.06809901714325 and perplexity is 58.445752547546014
At time: 61.02379894256592 and batch: 250, loss is 4.218288745880127 and perplexity is 67.91716126665266
At time: 61.91732859611511 and batch: 300, loss is 4.175673971176147 and perplexity is 65.08368941332445
At time: 62.81183075904846 and batch: 350, loss is 4.173706588745117 and perplexity is 64.9557707799221
At time: 63.705957889556885 and batch: 400, loss is 4.08916925907135 and perplexity is 59.69028393948669
At time: 64.60048222541809 and batch: 450, loss is 4.1288776159286495 and perplexity is 62.10817457401582
At time: 65.49603343009949 and batch: 500, loss is 4.009353866577149 and perplexity is 55.11124983633754
At time: 66.3921537399292 and batch: 550, loss is 4.0750549793243405 and perplexity is 58.853716236953325
At time: 67.28463125228882 and batch: 600, loss is 4.081665649414062 and perplexity is 59.244067558894585
At time: 68.17841649055481 and batch: 650, loss is 3.9286046552658083 and perplexity is 50.83599442054516
At time: 69.07734441757202 and batch: 700, loss is 3.9404193782806396 and perplexity is 51.44016966746943
At time: 69.97102046012878 and batch: 750, loss is 4.04245575428009 and perplexity is 56.96606583252699
At time: 70.86712193489075 and batch: 800, loss is 4.00577260017395 and perplexity is 54.914234761162724
At time: 71.7606201171875 and batch: 850, loss is 4.075629506111145 and perplexity is 58.8875389885425
At time: 72.65679168701172 and batch: 900, loss is 4.0166769695281985 and perplexity is 55.516316555055646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36503580171768 and perplexity of 78.65221577579484
finished 4 epochs...
Completing Train Step...
At time: 74.81949853897095 and batch: 50, loss is 4.098120675086975 and perplexity is 60.22699508168922
At time: 75.71349954605103 and batch: 100, loss is 3.980030975341797 and perplexity is 53.518691961592616
At time: 76.6070191860199 and batch: 150, loss is 3.988859429359436 and perplexity is 53.99327109021641
At time: 77.50003576278687 and batch: 200, loss is 3.872571277618408 and perplexity is 48.06581788583531
At time: 78.39329528808594 and batch: 250, loss is 4.022078785896301 and perplexity is 55.81701693544465
At time: 79.28800654411316 and batch: 300, loss is 3.985742521286011 and perplexity is 53.82524103080499
At time: 80.18261361122131 and batch: 350, loss is 3.9829490900039675 and perplexity is 53.67509372946675
At time: 81.07722163200378 and batch: 400, loss is 3.9083578968048096 and perplexity is 49.817079984460335
At time: 81.97346115112305 and batch: 450, loss is 3.9496302032470703 and perplexity is 51.916164855447335
At time: 82.86902141571045 and batch: 500, loss is 3.8344798707962036 and perplexity is 46.269355365173034
At time: 83.76300549507141 and batch: 550, loss is 3.896664509773254 and perplexity is 49.2379422274218
At time: 84.65606808662415 and batch: 600, loss is 3.911450824737549 and perplexity is 49.971399148677875
At time: 85.54821825027466 and batch: 650, loss is 3.754861927032471 and perplexity is 42.72831977733503
At time: 86.44158506393433 and batch: 700, loss is 3.767822003364563 and perplexity is 43.28568601627833
At time: 87.33267116546631 and batch: 750, loss is 3.871676926612854 and perplexity is 48.022849390598864
At time: 88.2227418422699 and batch: 800, loss is 3.8383046102523806 and perplexity is 46.446662454814266
At time: 89.10992431640625 and batch: 850, loss is 3.9079147243499754 and perplexity is 49.79500731819106
At time: 89.99443531036377 and batch: 900, loss is 3.850724024772644 and perplexity is 47.02709969363536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342889655126284 and perplexity of 76.92951823934156
finished 5 epochs...
Completing Train Step...
At time: 92.1002209186554 and batch: 50, loss is 3.9384414529800416 and perplexity is 51.338525409929034
At time: 92.99278593063354 and batch: 100, loss is 3.825848350524902 and perplexity is 45.871699144148316
At time: 93.8904161453247 and batch: 150, loss is 3.83415913105011 and perplexity is 46.25451732358334
At time: 94.78334307670593 and batch: 200, loss is 3.7194411993026733 and perplexity is 41.24134197778138
At time: 95.6748697757721 and batch: 250, loss is 3.8665017318725585 and perplexity is 47.77496377443948
At time: 96.57151913642883 and batch: 300, loss is 3.8339667415618894 and perplexity is 46.24561929663844
At time: 97.46249914169312 and batch: 350, loss is 3.8298458003997804 and perplexity is 46.0554359568319
At time: 98.35444688796997 and batch: 400, loss is 3.7603548669815066 and perplexity is 42.96366966163046
At time: 99.24463772773743 and batch: 450, loss is 3.806304497718811 and perplexity is 44.98389324089999
At time: 100.13504147529602 and batch: 500, loss is 3.6925878047943117 and perplexity is 40.14860940480406
At time: 101.02522349357605 and batch: 550, loss is 3.750962414741516 and perplexity is 42.56202461493268
At time: 101.91594982147217 and batch: 600, loss is 3.769730095863342 and perplexity is 43.368357956842544
At time: 102.8084785938263 and batch: 650, loss is 3.616951274871826 and perplexity is 37.22390918581529
At time: 103.69924879074097 and batch: 700, loss is 3.6289196825027465 and perplexity is 37.67209680064494
At time: 104.58773756027222 and batch: 750, loss is 3.732007236480713 and perplexity is 41.76285200915322
At time: 105.48030686378479 and batch: 800, loss is 3.6992487382888792 and perplexity is 40.41692926023699
At time: 106.37225151062012 and batch: 850, loss is 3.768720655441284 and perplexity is 43.3246022713769
At time: 107.26277160644531 and batch: 900, loss is 3.713451499938965 and perplexity is 40.99505706060404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344374565229024 and perplexity of 77.04383651334845
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.38440132141113 and batch: 50, loss is 3.835830879211426 and perplexity is 46.33190789862348
At time: 110.28111934661865 and batch: 100, loss is 3.729469003677368 and perplexity is 41.656982585683856
At time: 111.17404389381409 and batch: 150, loss is 3.7375474119186403 and perplexity is 41.994867646290956
At time: 112.06466770172119 and batch: 200, loss is 3.609190945625305 and perplexity is 36.93615736323114
At time: 112.95609545707703 and batch: 250, loss is 3.7474049520492554 and perplexity is 42.410880803892965
At time: 113.84831190109253 and batch: 300, loss is 3.707728524208069 and perplexity is 40.76111340939728
At time: 114.73884463310242 and batch: 350, loss is 3.6918279504776 and perplexity is 40.11811389817632
At time: 115.62741732597351 and batch: 400, loss is 3.6226101160049438 and perplexity is 37.43515050106765
At time: 116.51998996734619 and batch: 450, loss is 3.6465595674514772 and perplexity is 38.34252399891105
At time: 117.41082453727722 and batch: 500, loss is 3.5276930999755858 and perplexity is 34.045337763504605
At time: 118.30124592781067 and batch: 550, loss is 3.5733903551101687 and perplexity is 35.63721136847258
At time: 119.1954517364502 and batch: 600, loss is 3.586254110336304 and perplexity is 36.09860097151847
At time: 120.08548998832703 and batch: 650, loss is 3.421205883026123 and perplexity is 30.60630039499603
At time: 120.97706317901611 and batch: 700, loss is 3.4132840394973756 and perplexity is 30.364799899798385
At time: 121.86797547340393 and batch: 750, loss is 3.50793004989624 and perplexity is 33.37910314694448
At time: 122.75811743736267 and batch: 800, loss is 3.457524037361145 and perplexity is 31.73829631434928
At time: 123.64990758895874 and batch: 850, loss is 3.511524729728699 and perplexity is 33.499306252255685
At time: 124.53907060623169 and batch: 900, loss is 3.4506923723220826 and perplexity is 31.52220986058019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305915309958262 and perplexity of 74.13704278013186
finished 7 epochs...
Completing Train Step...
At time: 126.6589617729187 and batch: 50, loss is 3.7378744506835937 and perplexity is 42.00860384195218
At time: 127.55188727378845 and batch: 100, loss is 3.6246329736709595 and perplexity is 37.51095312534861
At time: 128.4437494277954 and batch: 150, loss is 3.6317142152786257 and perplexity is 37.77751994549951
At time: 129.33602738380432 and batch: 200, loss is 3.5073134660720826 and perplexity is 33.35852847553581
At time: 130.2289412021637 and batch: 250, loss is 3.647402453422546 and perplexity is 38.3748559986654
At time: 131.12041878700256 and batch: 300, loss is 3.6113316011428833 and perplexity is 37.01530964094323
At time: 132.01239466667175 and batch: 350, loss is 3.5971009969711303 and perplexity is 36.49228969245039
At time: 132.9030110836029 and batch: 400, loss is 3.5342379140853883 and perplexity is 34.268888922886966
At time: 133.79628729820251 and batch: 450, loss is 3.562492804527283 and perplexity is 35.2509614663445
At time: 134.69153475761414 and batch: 500, loss is 3.44943253993988 and perplexity is 31.482522165010565
At time: 135.58641719818115 and batch: 550, loss is 3.496755814552307 and perplexity is 33.008193368804754
At time: 136.48052334785461 and batch: 600, loss is 3.5157834005355837 and perplexity is 33.642272977879834
At time: 137.37404608726501 and batch: 650, loss is 3.3556552505493165 and perplexity is 28.664380389473553
At time: 138.26856398582458 and batch: 700, loss is 3.3534464550018313 and perplexity is 28.601136505814914
At time: 139.1631736755371 and batch: 750, loss is 3.454744510650635 and perplexity is 31.650201359853764
At time: 140.05639839172363 and batch: 800, loss is 3.4092891216278076 and perplexity is 30.243736997256427
At time: 140.96669006347656 and batch: 850, loss is 3.4700666189193727 and perplexity is 32.138883431121286
At time: 141.85991787910461 and batch: 900, loss is 3.4176889991760255 and perplexity is 30.498850646317628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3142415921982025 and perplexity of 74.75690571854362
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.97545886039734 and batch: 50, loss is 3.697972936630249 and perplexity is 40.36539815357535
At time: 144.87447714805603 and batch: 100, loss is 3.599405345916748 and perplexity is 36.5764776236583
At time: 145.76520490646362 and batch: 150, loss is 3.604940643310547 and perplexity is 36.779500683051545
At time: 146.65701246261597 and batch: 200, loss is 3.4791448974609374 and perplexity is 32.43197754899771
At time: 147.54992723464966 and batch: 250, loss is 3.619846472740173 and perplexity is 37.331835927589616
At time: 148.4447865486145 and batch: 300, loss is 3.5774978303909304 and perplexity is 35.78389136925845
At time: 149.3351812362671 and batch: 350, loss is 3.560672569274902 and perplexity is 35.18685478592311
At time: 150.22568464279175 and batch: 400, loss is 3.496098575592041 and perplexity is 32.986506225713136
At time: 151.1180112361908 and batch: 450, loss is 3.519356074333191 and perplexity is 33.762680805912225
At time: 152.01046538352966 and batch: 500, loss is 3.404052901268005 and perplexity is 30.085788014561366
At time: 152.90214157104492 and batch: 550, loss is 3.4434708166122436 and perplexity is 31.295390446228083
At time: 153.7964732646942 and batch: 600, loss is 3.4619231700897215 and perplexity is 31.878224848850575
At time: 154.68949913978577 and batch: 650, loss is 3.2960372257232664 and perplexity is 27.005410254391734
At time: 155.5832827091217 and batch: 700, loss is 3.287034106254578 and perplexity is 26.763368519809184
At time: 156.47554278373718 and batch: 750, loss is 3.3843950605392457 and perplexity is 29.500141517546854
At time: 157.3685700893402 and batch: 800, loss is 3.3338815641403197 and perplexity is 28.04699690818391
At time: 158.26266956329346 and batch: 850, loss is 3.3897342252731324 and perplexity is 29.658068857659263
At time: 159.1556544303894 and batch: 900, loss is 3.33878303527832 and perplexity is 28.184805911564947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.309984599074272 and perplexity of 74.4393424967323
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.270446062088 and batch: 50, loss is 3.6810466146469114 and perplexity is 39.68791029125771
At time: 162.17928099632263 and batch: 100, loss is 3.5845026540756226 and perplexity is 36.03543118655105
At time: 163.07249569892883 and batch: 150, loss is 3.590806074142456 and perplexity is 36.26329505296564
At time: 163.96706318855286 and batch: 200, loss is 3.4608195495605467 and perplexity is 31.843062791826608
At time: 164.85779452323914 and batch: 250, loss is 3.604145712852478 and perplexity is 36.750275155396054
At time: 165.7486310005188 and batch: 300, loss is 3.559252152442932 and perplexity is 35.13691026453778
At time: 166.64117813110352 and batch: 350, loss is 3.544310040473938 and perplexity is 34.615793604492985
At time: 167.53045201301575 and batch: 400, loss is 3.476528491973877 and perplexity is 32.347233256042465
At time: 168.42014026641846 and batch: 450, loss is 3.5004886198043823 and perplexity is 33.13163677814005
At time: 169.31329584121704 and batch: 500, loss is 3.3871479558944704 and perplexity is 29.581464205175298
At time: 170.20446825027466 and batch: 550, loss is 3.4222283697128297 and perplexity is 30.63761093425616
At time: 171.09738731384277 and batch: 600, loss is 3.444075012207031 and perplexity is 31.31430469665085
At time: 171.98988246917725 and batch: 650, loss is 3.274767713546753 and perplexity is 26.437083790846778
At time: 172.88090443611145 and batch: 700, loss is 3.26656213760376 and perplexity is 26.2210398866328
At time: 173.77227234840393 and batch: 750, loss is 3.3640741443634035 and perplexity is 28.906721454685307
At time: 174.66121459007263 and batch: 800, loss is 3.3095096731185913 and perplexity is 27.371701100167716
At time: 175.55222535133362 and batch: 850, loss is 3.3627208709716796 and perplexity is 28.867629214908625
At time: 176.44306921958923 and batch: 900, loss is 3.312904944419861 and perplexity is 27.464793398755308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307201646778681 and perplexity of 74.23246935021407
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.57486963272095 and batch: 50, loss is 3.6750016975402833 and perplexity is 39.448723822941496
At time: 179.46658158302307 and batch: 100, loss is 3.575641384124756 and perplexity is 35.71752212226839
At time: 180.35976195335388 and batch: 150, loss is 3.583149890899658 and perplexity is 35.98671673919596
At time: 181.25198221206665 and batch: 200, loss is 3.451823925971985 and perplexity is 31.557899120549024
At time: 182.14381384849548 and batch: 250, loss is 3.5955192708969115 and perplexity is 36.43461451152923
At time: 183.03566002845764 and batch: 300, loss is 3.55247145652771 and perplexity is 34.89946349756832
At time: 183.92833876609802 and batch: 350, loss is 3.5367904901504517 and perplexity is 34.35647460552999
At time: 184.81825852394104 and batch: 400, loss is 3.4709711408615114 and perplexity is 32.167966907719034
At time: 185.7169349193573 and batch: 450, loss is 3.4942233610153197 and perplexity is 32.9247074095333
At time: 186.60795187950134 and batch: 500, loss is 3.3808366537094114 and perplexity is 29.39535456037173
At time: 187.49884414672852 and batch: 550, loss is 3.4139294815063477 and perplexity is 30.384404943526537
At time: 188.39046239852905 and batch: 600, loss is 3.4366737937927248 and perplexity is 31.0833962441026
At time: 189.2833878993988 and batch: 650, loss is 3.266989288330078 and perplexity is 26.23224261532171
At time: 190.1755130290985 and batch: 700, loss is 3.2594039249420166 and perplexity is 26.034014290007892
At time: 191.0681517124176 and batch: 750, loss is 3.3572835397720335 and perplexity is 28.711092311076143
At time: 191.9687774181366 and batch: 800, loss is 3.301956419944763 and perplexity is 27.16573454993583
At time: 192.86109924316406 and batch: 850, loss is 3.3541773843765257 and perplexity is 28.62204955868791
At time: 193.75119590759277 and batch: 900, loss is 3.3038729572296144 and perplexity is 27.217848616395127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30564943078446 and perplexity of 74.11733390465523
finished 11 epochs...
Completing Train Step...
At time: 195.86613178253174 and batch: 50, loss is 3.671356635093689 and perplexity is 39.30519251038587
At time: 196.7683606147766 and batch: 100, loss is 3.5700751113891602 and perplexity is 35.519260952468194
At time: 197.6628201007843 and batch: 150, loss is 3.577581944465637 and perplexity is 35.78690142476262
At time: 198.55599641799927 and batch: 200, loss is 3.4465359163284304 and perplexity is 31.391461096450147
At time: 199.44967365264893 and batch: 250, loss is 3.59050865650177 and perplexity is 36.25251131302423
At time: 200.34322571754456 and batch: 300, loss is 3.5476343917846678 and perplexity is 34.731060150426714
At time: 201.23635005950928 and batch: 350, loss is 3.5320542097091674 and perplexity is 34.19413744744462
At time: 202.12753677368164 and batch: 400, loss is 3.4669067239761353 and perplexity is 32.037488219363155
At time: 203.0217945575714 and batch: 450, loss is 3.4903204011917115 and perplexity is 32.79645404598036
At time: 203.91280817985535 and batch: 500, loss is 3.377348403930664 and perplexity is 29.292994853234948
At time: 204.80547881126404 and batch: 550, loss is 3.411313142776489 and perplexity is 30.30501295154883
At time: 205.69805359840393 and batch: 600, loss is 3.4345744752883913 and perplexity is 31.01821074169566
At time: 206.59124302864075 and batch: 650, loss is 3.2655946111679075 and perplexity is 26.195682606264587
At time: 207.48301196098328 and batch: 700, loss is 3.258592267036438 and perplexity is 26.012892149631156
At time: 208.38231253623962 and batch: 750, loss is 3.357218384742737 and perplexity is 28.709221699956
At time: 209.27603769302368 and batch: 800, loss is 3.302606711387634 and perplexity is 27.183405939807027
At time: 210.1722128391266 and batch: 850, loss is 3.3557781457901 and perplexity is 28.667903321874803
At time: 211.06601238250732 and batch: 900, loss is 3.306515736579895 and perplexity is 27.289874517038285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305120598779966 and perplexity of 74.07814864852062
finished 12 epochs...
Completing Train Step...
At time: 213.19032216072083 and batch: 50, loss is 3.6689597845077513 and perplexity is 39.21109664859524
At time: 214.09314441680908 and batch: 100, loss is 3.5667757606506347 and perplexity is 35.4022635664521
At time: 214.98621082305908 and batch: 150, loss is 3.574064998626709 and perplexity is 35.66126189392322
At time: 215.88031387329102 and batch: 200, loss is 3.4430051040649414 and perplexity is 31.280819183497858
At time: 216.7729344367981 and batch: 250, loss is 3.5868077754974363 and perplexity is 36.118593043188106
At time: 217.66490602493286 and batch: 300, loss is 3.544148874282837 and perplexity is 34.61021515842635
At time: 218.56410574913025 and batch: 350, loss is 3.528608078956604 and perplexity is 34.07650278745657
At time: 219.45540356636047 and batch: 400, loss is 3.4639181756973265 and perplexity is 31.941885566814797
At time: 220.34596467018127 and batch: 450, loss is 3.487407560348511 and perplexity is 32.701062193212636
At time: 221.23862624168396 and batch: 500, loss is 3.374757504463196 and perplexity is 29.217197882035883
At time: 222.13313150405884 and batch: 550, loss is 3.4091546630859373 and perplexity is 30.239670741856578
At time: 223.02705597877502 and batch: 600, loss is 3.4328661251068113 and perplexity is 30.96526601268793
At time: 223.91811347007751 and batch: 650, loss is 3.264427433013916 and perplexity is 26.165125414110236
At time: 224.80878114700317 and batch: 700, loss is 3.2578638887405393 and perplexity is 25.99395182227487
At time: 225.69628810882568 and batch: 750, loss is 3.357055745124817 and perplexity is 28.70455282279043
At time: 226.58565139770508 and batch: 800, loss is 3.3030097770690916 and perplexity is 27.19436484627768
At time: 227.4743309020996 and batch: 850, loss is 3.3568680000305178 and perplexity is 28.699164189674445
At time: 228.3616054058075 and batch: 900, loss is 3.3081804180145262 and perplexity is 27.335341297853514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304964666497217 and perplexity of 74.06659837425381
finished 13 epochs...
Completing Train Step...
At time: 230.47831273078918 and batch: 50, loss is 3.6668319797515867 and perplexity is 39.12775179288781
At time: 231.37082433700562 and batch: 100, loss is 3.5640925121307374 and perplexity is 35.30739782623295
At time: 232.2626736164093 and batch: 150, loss is 3.5711875057220457 and perplexity is 35.55879436135338
At time: 233.15417075157166 and batch: 200, loss is 3.4400821113586426 and perplexity is 31.189519076954998
At time: 234.0455083847046 and batch: 250, loss is 3.5837367582321167 and perplexity is 36.00784236601644
At time: 234.9485445022583 and batch: 300, loss is 3.5412513875961302 and perplexity is 34.51007766436678
At time: 235.8419313430786 and batch: 350, loss is 3.525715560913086 and perplexity is 33.9780783042044
At time: 236.72965240478516 and batch: 400, loss is 3.4613719463348387 and perplexity is 31.860657656222145
At time: 237.62149858474731 and batch: 450, loss is 3.484933500289917 and perplexity is 32.62025780004445
At time: 238.51221251487732 and batch: 500, loss is 3.3725423431396484 and perplexity is 29.152548705918228
At time: 239.40257120132446 and batch: 550, loss is 3.4072559452056885 and perplexity is 30.182308612809372
At time: 240.299560546875 and batch: 600, loss is 3.4313473987579344 and perplexity is 30.918273940376064
At time: 241.1953444480896 and batch: 650, loss is 3.2633307790756225 and perplexity is 26.136447054271212
At time: 242.08641648292542 and batch: 700, loss is 3.2571358346939085 and perplexity is 25.97503370800321
At time: 242.97843170166016 and batch: 750, loss is 3.3567755365371705 and perplexity is 28.69651068737526
At time: 243.86989402770996 and batch: 800, loss is 3.3031999492645263 and perplexity is 27.19953695012353
At time: 244.76140642166138 and batch: 850, loss is 3.3575989961624146 and perplexity is 28.720150837330372
At time: 245.65114331245422 and batch: 900, loss is 3.309294104576111 and perplexity is 27.365801258389332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304973863575556 and perplexity of 74.06727957369388
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 247.79190683364868 and batch: 50, loss is 3.6657823085784913 and perplexity is 39.08670206788836
At time: 248.69044637680054 and batch: 100, loss is 3.5632208395004272 and perplexity is 35.276634743516816
At time: 249.5840449333191 and batch: 150, loss is 3.571239085197449 and perplexity is 35.56062851261438
At time: 250.47918915748596 and batch: 200, loss is 3.439862389564514 and perplexity is 31.182666812688872
At time: 251.37254977226257 and batch: 250, loss is 3.5829368209838868 and perplexity is 35.979049869310124
At time: 252.27213335037231 and batch: 300, loss is 3.5412414503097533 and perplexity is 34.50973472954607
At time: 253.16371607780457 and batch: 350, loss is 3.524476580619812 and perplexity is 33.936006203416035
At time: 254.057443857193 and batch: 400, loss is 3.460842604637146 and perplexity is 31.843796944541374
At time: 254.95003962516785 and batch: 450, loss is 3.4846091079711914 and perplexity is 32.6096777551147
At time: 255.8440408706665 and batch: 500, loss is 3.371663827896118 and perplexity is 29.12694899400736
At time: 256.7368972301483 and batch: 550, loss is 3.4050595331192017 and perplexity is 30.116088575241267
At time: 257.6316993236542 and batch: 600, loss is 3.4289234018325807 and perplexity is 30.84341890025343
At time: 258.52518248558044 and batch: 650, loss is 3.260417823791504 and perplexity is 26.060423533032253
At time: 259.4142978191376 and batch: 700, loss is 3.254411368370056 and perplexity is 25.904361918574708
At time: 260.30490899086 and batch: 750, loss is 3.3542064380645753 and perplexity is 28.62288114686742
At time: 261.1962492465973 and batch: 800, loss is 3.300292134284973 and perplexity is 27.120560609084762
At time: 262.0876009464264 and batch: 850, loss is 3.354806923866272 and perplexity is 28.640073942097857
At time: 262.9788393974304 and batch: 900, loss is 3.3059645795822146 and perplexity is 27.274837655948655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.304004825957834 and perplexity of 73.99554035815764
finished 15 epochs...
Completing Train Step...
At time: 265.09005069732666 and batch: 50, loss is 3.664755654335022 and perplexity is 39.04659413136164
At time: 265.9872133731842 and batch: 100, loss is 3.562019715309143 and perplexity is 35.23428856074261
At time: 266.87760496139526 and batch: 150, loss is 3.5699095726013184 and perplexity is 35.513381623706984
At time: 267.76951122283936 and batch: 200, loss is 3.4386657762527464 and perplexity is 31.145375534554034
At time: 268.66099667549133 and batch: 250, loss is 3.5820102071762085 and perplexity is 35.94572662618699
At time: 269.55417680740356 and batch: 300, loss is 3.540073051452637 and perplexity is 34.46943714134105
At time: 270.44599628448486 and batch: 350, loss is 3.523593339920044 and perplexity is 33.906045774632354
At time: 271.3369936943054 and batch: 400, loss is 3.4598848819732666 and perplexity is 31.813314017928445
At time: 272.2286636829376 and batch: 450, loss is 3.483653016090393 and perplexity is 32.578514806672445
At time: 273.124484539032 and batch: 500, loss is 3.3708738231658937 and perplexity is 29.10394765330534
At time: 274.01849579811096 and batch: 550, loss is 3.404622182846069 and perplexity is 30.102920175488713
At time: 274.91383171081543 and batch: 600, loss is 3.4286038160324095 and perplexity is 30.83356335647398
At time: 275.80513095855713 and batch: 650, loss is 3.260296082496643 and perplexity is 26.05725109643891
At time: 276.69795417785645 and batch: 700, loss is 3.254395146369934 and perplexity is 25.9039417014209
At time: 277.590811252594 and batch: 750, loss is 3.354242792129517 and perplexity is 28.623921723861933
At time: 278.4842336177826 and batch: 800, loss is 3.300582585334778 and perplexity is 27.12843894846555
At time: 279.37888073921204 and batch: 850, loss is 3.355154023170471 and perplexity is 28.65001661728337
At time: 280.2728374004364 and batch: 900, loss is 3.306545910835266 and perplexity is 27.290697981104653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303607261344178 and perplexity of 73.96612819674759
finished 16 epochs...
Completing Train Step...
At time: 282.38987946510315 and batch: 50, loss is 3.664032096862793 and perplexity is 39.018351895085495
At time: 283.28404355049133 and batch: 100, loss is 3.5611509466171265 and perplexity is 35.20369140680473
At time: 284.1779019832611 and batch: 150, loss is 3.5689760303497313 and perplexity is 35.480243851626334
At time: 285.07184171676636 and batch: 200, loss is 3.437783727645874 and perplexity is 31.117915911594668
At time: 285.96522545814514 and batch: 250, loss is 3.5812035369873048 and perplexity is 35.91674197219904
At time: 286.85554456710815 and batch: 300, loss is 3.539209189414978 and perplexity is 34.439673160972504
At time: 287.74770522117615 and batch: 350, loss is 3.5228284072875975 and perplexity is 33.88011985083883
At time: 288.6366403102875 and batch: 400, loss is 3.459146194458008 and perplexity is 31.789822597522075
At time: 289.5274350643158 and batch: 450, loss is 3.4829173946380614 and perplexity is 32.55455816488569
At time: 290.4184260368347 and batch: 500, loss is 3.3702602005004882 and perplexity is 29.086094289547024
At time: 291.31075406074524 and batch: 550, loss is 3.4042012548446654 and perplexity is 30.09025167991214
At time: 292.2020604610443 and batch: 600, loss is 3.4283053207397463 and perplexity is 30.824361056445984
At time: 293.09139466285706 and batch: 650, loss is 3.2601132822036742 and perplexity is 26.05248825864185
At time: 293.98181557655334 and batch: 700, loss is 3.2543224096298218 and perplexity is 25.902057601667945
At time: 294.8734202384949 and batch: 750, loss is 3.354264326095581 and perplexity is 28.62453811705763
At time: 295.76602149009705 and batch: 800, loss is 3.300760931968689 and perplexity is 27.133277645705125
At time: 296.6634039878845 and batch: 850, loss is 3.35542631149292 and perplexity is 28.65781874441207
At time: 297.55433416366577 and batch: 900, loss is 3.306969223022461 and perplexity is 27.302252911656026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30342122953232 and perplexity of 73.95236942372749
finished 17 epochs...
Completing Train Step...
At time: 299.6580607891083 and batch: 50, loss is 3.6634195613861085 and perplexity is 38.99445908865083
At time: 300.5522940158844 and batch: 100, loss is 3.56041015625 and perplexity is 35.177622508309874
At time: 301.44144797325134 and batch: 150, loss is 3.5681923484802245 and perplexity is 35.45244952017346
At time: 302.3307316303253 and batch: 200, loss is 3.4370219469070435 and perplexity is 31.094219909345643
At time: 303.22314286231995 and batch: 250, loss is 3.5804509258270265 and perplexity is 35.88972080084295
At time: 304.11423683166504 and batch: 300, loss is 3.5384599018096923 and perplexity is 34.41387760609454
At time: 305.00361013412476 and batch: 350, loss is 3.522120237350464 and perplexity is 33.8561354620079
At time: 305.89254570007324 and batch: 400, loss is 3.458495545387268 and perplexity is 31.76914530654955
At time: 306.7782869338989 and batch: 450, loss is 3.482271752357483 and perplexity is 32.533546349496994
At time: 307.6623635292053 and batch: 500, loss is 3.3697149658203127 and perplexity is 29.070239864813374
At time: 308.54792284965515 and batch: 550, loss is 3.4037820434570314 and perplexity is 30.077640147384866
At time: 309.4333324432373 and batch: 600, loss is 3.428002190589905 and perplexity is 30.815018679309812
At time: 310.3202819824219 and batch: 650, loss is 3.259900608062744 and perplexity is 26.046948157221394
At time: 311.20630955696106 and batch: 700, loss is 3.254213032722473 and perplexity is 25.899224669645008
At time: 312.09192085266113 and batch: 750, loss is 3.354261083602905 and perplexity is 28.624445302352914
At time: 312.9786467552185 and batch: 800, loss is 3.3008751821517945 and perplexity is 27.1363778047379
At time: 313.86649441719055 and batch: 850, loss is 3.3556473302841185 and perplexity is 28.6641533608782
At time: 314.7532334327698 and batch: 900, loss is 3.3073029375076293 and perplexity is 27.311365589362076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303328004601884 and perplexity of 73.94547554057823
finished 18 epochs...
Completing Train Step...
At time: 316.8611137866974 and batch: 50, loss is 3.6628597497940065 and perplexity is 38.972635647502855
At time: 317.75506138801575 and batch: 100, loss is 3.559735670089722 and perplexity is 35.15390368868351
At time: 318.6458168029785 and batch: 150, loss is 3.5674836349487307 and perplexity is 35.42733279080819
At time: 319.5410330295563 and batch: 200, loss is 3.436321802139282 and perplexity is 31.07245707342536
At time: 320.4310245513916 and batch: 250, loss is 3.5797372961044314 and perplexity is 35.86411796590238
At time: 321.3242094516754 and batch: 300, loss is 3.5377691316604616 and perplexity is 34.390113735356174
At time: 322.21654510498047 and batch: 350, loss is 3.5214498376846315 and perplexity is 33.8334459264861
At time: 323.1094937324524 and batch: 400, loss is 3.457890396118164 and perplexity is 31.749926047334085
At time: 324.0019054412842 and batch: 450, loss is 3.481673855781555 and perplexity is 32.51410046742429
At time: 324.8948950767517 and batch: 500, loss is 3.369204144477844 and perplexity is 29.0553939579856
At time: 325.7861258983612 and batch: 550, loss is 3.4033660888671875 and perplexity is 30.065131816543047
At time: 326.6776156425476 and batch: 600, loss is 3.4276937198638917 and perplexity is 30.80551461406401
At time: 327.5688555240631 and batch: 650, loss is 3.2596740770339965 and perplexity is 26.04104838352573
At time: 328.46058988571167 and batch: 700, loss is 3.2540822505950926 and perplexity is 25.89583773542526
At time: 329.35402250289917 and batch: 750, loss is 3.3542366647720336 and perplexity is 28.623746335398295
At time: 330.24565744400024 and batch: 800, loss is 3.300950775146484 and perplexity is 27.138429202335857
At time: 331.13823533058167 and batch: 850, loss is 3.355831847190857 and perplexity is 28.669442869777523
At time: 332.03148078918457 and batch: 900, loss is 3.307579975128174 and perplexity is 27.31893291326708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303287035798373 and perplexity of 73.94244614497609
finished 19 epochs...
Completing Train Step...
At time: 334.16145491600037 and batch: 50, loss is 3.6623301839828493 and perplexity is 38.9520025358707
At time: 335.05497193336487 and batch: 100, loss is 3.559103026390076 and perplexity is 35.13167082647854
At time: 335.950133562088 and batch: 150, loss is 3.5668204641342163 and perplexity is 35.403846206334684
At time: 336.84332370758057 and batch: 200, loss is 3.4356604957580568 and perplexity is 31.05191545218357
At time: 337.73620891571045 and batch: 250, loss is 3.5790549898147583 and perplexity is 35.83965599886666
At time: 338.62928462028503 and batch: 300, loss is 3.5371158123016357 and perplexity is 34.36765334600312
At time: 339.52111768722534 and batch: 350, loss is 3.5208073234558106 and perplexity is 33.81171443821112
At time: 340.4140315055847 and batch: 400, loss is 3.4573137617111205 and perplexity is 31.731623225079787
At time: 341.3099753856659 and batch: 450, loss is 3.481106610298157 and perplexity is 32.49566222078786
At time: 342.20282793045044 and batch: 500, loss is 3.3687146377563475 and perplexity is 29.041174627860197
At time: 343.09500432014465 and batch: 550, loss is 3.402955241203308 and perplexity is 30.05278216445856
At time: 343.98721385002136 and batch: 600, loss is 3.42738214969635 and perplexity is 30.795918029796816
At time: 344.8810408115387 and batch: 650, loss is 3.2594406604766846 and perplexity is 26.03497068100905
At time: 345.7742199897766 and batch: 700, loss is 3.253937931060791 and perplexity is 25.892100729850878
At time: 346.66882371902466 and batch: 750, loss is 3.3541951751708985 and perplexity is 28.62255877221578
At time: 347.56143736839294 and batch: 800, loss is 3.3010005235671995 and perplexity is 27.13977932991245
At time: 348.4557681083679 and batch: 850, loss is 3.3559882402420045 and perplexity is 28.6739269220508
At time: 349.34728932380676 and batch: 900, loss is 3.307816915512085 and perplexity is 27.3254066386328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303274494327911 and perplexity of 73.94151880378699
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
579.7171065807343


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3145723342895508 and batch: 50, loss is 7.086821250915527 and perplexity is 1196.0996505715264
At time: 2.433955430984497 and batch: 100, loss is 6.505772724151611 and perplexity is 668.9924162746274
At time: 3.5542898178100586 and batch: 150, loss is 6.1293094444274905 and perplexity is 459.1190039987246
At time: 4.67435097694397 and batch: 200, loss is 5.944326810836792 and perplexity is 381.5823976798207
At time: 5.793566465377808 and batch: 250, loss is 5.994690084457398 and perplexity is 401.2922979970624
At time: 6.915130853652954 and batch: 300, loss is 5.89213285446167 and perplexity is 362.1769318588997
At time: 8.033324718475342 and batch: 350, loss is 5.864052906036377 and perplexity is 352.1484804214303
At time: 9.15409803390503 and batch: 400, loss is 5.717168569564819 and perplexity is 304.0428269366187
At time: 10.275645732879639 and batch: 450, loss is 5.712421188354492 and perplexity is 302.6028405196035
At time: 11.39122724533081 and batch: 500, loss is 5.653421964645386 and perplexity is 285.2659675732835
At time: 12.512055158615112 and batch: 550, loss is 5.69021448135376 and perplexity is 295.95709111118066
At time: 13.630871534347534 and batch: 600, loss is 5.604503574371338 and perplexity is 271.6470394043277
At time: 14.749502897262573 and batch: 650, loss is 5.505351676940918 and perplexity is 246.00495473800103
At time: 15.871259212493896 and batch: 700, loss is 5.590169448852539 and perplexity is 267.78099105114336
At time: 16.992129802703857 and batch: 750, loss is 5.554248733520508 and perplexity is 258.3328148162986
At time: 18.116117477416992 and batch: 800, loss is 5.5337111759185795 and perplexity is 253.08139995129687
At time: 19.237606048583984 and batch: 850, loss is 5.557608051300049 and perplexity is 259.20209611271946
At time: 20.36102604866028 and batch: 900, loss is 5.448487930297851 and perplexity is 232.4064852868767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.348925864859803 and perplexity of 210.38219754600746
finished 1 epochs...
Completing Train Step...
At time: 22.648037672042847 and batch: 50, loss is 5.235127344131469 and perplexity is 187.7530140664196
At time: 23.540615558624268 and batch: 100, loss is 5.077932386398316 and perplexity is 160.4419807236791
At time: 24.432253122329712 and batch: 150, loss is 5.0369970703125 and perplexity is 154.00684820868773
At time: 25.325915575027466 and batch: 200, loss is 4.8938930892944335 and perplexity is 133.47218302759615
At time: 26.219354152679443 and batch: 250, loss is 4.970946416854859 and perplexity is 144.16326136782564
At time: 27.11199402809143 and batch: 300, loss is 4.88535517692566 and perplexity is 132.33746020031825
At time: 28.006004571914673 and batch: 350, loss is 4.8645243644714355 and perplexity is 129.60927717173124
At time: 28.894514560699463 and batch: 400, loss is 4.721487379074096 and perplexity is 112.33521351586047
At time: 29.784608364105225 and batch: 450, loss is 4.730758762359619 and perplexity is 113.38155937841063
At time: 30.68252468109131 and batch: 500, loss is 4.633962078094482 and perplexity is 102.921038538122
At time: 31.58599615097046 and batch: 550, loss is 4.69169828414917 and perplexity is 109.03820050165653
At time: 32.478898763656616 and batch: 600, loss is 4.632566041946411 and perplexity is 102.77745729354858
At time: 33.37157917022705 and batch: 650, loss is 4.496397686004639 and perplexity is 89.693444689052
At time: 34.26338315010071 and batch: 700, loss is 4.542485570907592 and perplexity is 93.92396490031575
At time: 35.16371536254883 and batch: 750, loss is 4.570154314041138 and perplexity is 96.55900903412477
At time: 36.05642533302307 and batch: 800, loss is 4.5216706848144534 and perplexity is 91.98915455372752
At time: 36.94714641571045 and batch: 850, loss is 4.576152124404907 and perplexity is 97.13989193057726
At time: 37.84053564071655 and batch: 900, loss is 4.500834169387818 and perplexity is 90.09225216323792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6055540319991435 and perplexity of 100.0383919689359
finished 2 epochs...
Completing Train Step...
At time: 39.93838930130005 and batch: 50, loss is 4.543397245407104 and perplexity is 94.0096320283429
At time: 40.83068370819092 and batch: 100, loss is 4.419810652732849 and perplexity is 83.08055279330688
At time: 41.719117641448975 and batch: 150, loss is 4.417956094741822 and perplexity is 82.92661787496165
At time: 42.61619162559509 and batch: 200, loss is 4.29865526676178 and perplexity is 73.60075374344302
At time: 43.507126569747925 and batch: 250, loss is 4.436997919082642 and perplexity is 84.52082207381397
At time: 44.40202832221985 and batch: 300, loss is 4.388754978179931 and perplexity is 80.54008237286055
At time: 45.295493602752686 and batch: 350, loss is 4.391219110488891 and perplexity is 80.73878851056435
At time: 46.184494972229004 and batch: 400, loss is 4.29253746509552 and perplexity is 73.1518534710708
At time: 47.07528471946716 and batch: 450, loss is 4.32718186378479 and perplexity is 75.73056651242945
At time: 47.96788835525513 and batch: 500, loss is 4.201930103302002 and perplexity is 66.81516684187203
At time: 48.85960674285889 and batch: 550, loss is 4.273000869750977 and perplexity is 71.73658508846387
At time: 49.751906871795654 and batch: 600, loss is 4.264940061569214 and perplexity is 71.1606545875963
At time: 50.64177370071411 and batch: 650, loss is 4.11178671836853 and perplexity is 61.055709529378596
At time: 51.53024435043335 and batch: 700, loss is 4.135453329086304 and perplexity is 62.517925841691735
At time: 52.41840314865112 and batch: 750, loss is 4.21722439289093 and perplexity is 67.84491188928176
At time: 53.31111001968384 and batch: 800, loss is 4.176820130348205 and perplexity is 65.15832844683234
At time: 54.21785569190979 and batch: 850, loss is 4.2528067350387575 and perplexity is 70.30245605931879
At time: 55.111767292022705 and batch: 900, loss is 4.183481750488281 and perplexity is 65.5938374671839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.414573355896832 and perplexity of 82.64657271026542
finished 3 epochs...
Completing Train Step...
At time: 57.22417235374451 and batch: 50, loss is 4.252326078414917 and perplexity is 70.26867283785664
At time: 58.123111724853516 and batch: 100, loss is 4.135047459602356 and perplexity is 62.49255687198646
At time: 59.01508021354675 and batch: 150, loss is 4.133066306114197 and perplexity is 62.36887208468431
At time: 59.90886688232422 and batch: 200, loss is 4.0191672945022585 and perplexity is 55.654742515914315
At time: 60.80343198776245 and batch: 250, loss is 4.166564416885376 and perplexity is 64.49349828160997
At time: 61.696130990982056 and batch: 300, loss is 4.1260216474533085 and perplexity is 61.93104863880979
At time: 62.59098219871521 and batch: 350, loss is 4.133853192329407 and perplexity is 62.417968604539965
At time: 63.4810471534729 and batch: 400, loss is 4.046243476867676 and perplexity is 57.18224664482031
At time: 64.37376832962036 and batch: 450, loss is 4.090951290130615 and perplexity is 59.79674871299532
At time: 65.27246356010437 and batch: 500, loss is 3.9586388731002806 and perplexity is 52.385973442586405
At time: 66.16617608070374 and batch: 550, loss is 4.028861298561096 and perplexity is 56.196883327797046
At time: 67.05902338027954 and batch: 600, loss is 4.0392120885849 and perplexity is 56.781586315684244
At time: 67.95106887817383 and batch: 650, loss is 3.8824965858459475 and perplexity is 48.545261319595625
At time: 68.84452676773071 and batch: 700, loss is 3.897817401885986 and perplexity is 49.29474099779891
At time: 69.73874521255493 and batch: 750, loss is 3.99529390335083 and perplexity is 54.341809516949525
At time: 70.6319785118103 and batch: 800, loss is 3.9564684677124022 and perplexity is 52.2723979406027
At time: 71.52498435974121 and batch: 850, loss is 4.035474128723145 and perplexity is 56.56973521727199
At time: 72.41847896575928 and batch: 900, loss is 3.976142892837524 and perplexity is 53.31101087395624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34931883093429 and perplexity of 77.42570496364303
finished 4 epochs...
Completing Train Step...
At time: 74.53069305419922 and batch: 50, loss is 4.051715278625489 and perplexity is 57.49599416181892
At time: 75.42250657081604 and batch: 100, loss is 3.936080093383789 and perplexity is 51.217439709999276
At time: 76.31404185295105 and batch: 150, loss is 3.9365458631515504 and perplexity is 51.24130080145447
At time: 77.22128367424011 and batch: 200, loss is 3.8277273607254028 and perplexity is 45.95797356462859
At time: 78.12348198890686 and batch: 250, loss is 3.973168411254883 and perplexity is 53.15267385597267
At time: 79.01784372329712 and batch: 300, loss is 3.9386112689971924 and perplexity is 51.34724425411929
At time: 79.91118383407593 and batch: 350, loss is 3.943870129585266 and perplexity is 51.61798351930435
At time: 80.80515098571777 and batch: 400, loss is 3.8641381597518922 and perplexity is 47.66217754369809
At time: 81.69656443595886 and batch: 450, loss is 3.9095462274551394 and perplexity is 49.87631433554272
At time: 82.58722019195557 and batch: 500, loss is 3.7833189010620116 and perplexity is 43.96170442935039
At time: 83.47917675971985 and batch: 550, loss is 3.848449635505676 and perplexity is 46.920263302660025
At time: 84.37135171890259 and batch: 600, loss is 3.867462544441223 and perplexity is 47.820888619152456
At time: 85.28427386283875 and batch: 650, loss is 3.713345994949341 and perplexity is 40.9907321056904
At time: 86.18053555488586 and batch: 700, loss is 3.725800929069519 and perplexity is 41.504461565872916
At time: 87.08081388473511 and batch: 750, loss is 3.826961374282837 and perplexity is 45.922783859089556
At time: 87.97324919700623 and batch: 800, loss is 3.7911420488357543 and perplexity is 44.30697211865362
At time: 88.86236691474915 and batch: 850, loss is 3.8662517833709718 and perplexity is 47.76302398605897
At time: 89.75399899482727 and batch: 900, loss is 3.8161824703216554 and perplexity is 45.430444786879455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.327844489110659 and perplexity of 75.78076413301368
finished 5 epochs...
Completing Train Step...
At time: 91.91390347480774 and batch: 50, loss is 3.89559467792511 and perplexity is 49.18529407604584
At time: 92.81269645690918 and batch: 100, loss is 3.778647575378418 and perplexity is 43.75682389476953
At time: 93.70467829704285 and batch: 150, loss is 3.78319064617157 and perplexity is 43.95606648731972
At time: 94.59808826446533 and batch: 200, loss is 3.6765534925460814 and perplexity is 39.509987677733044
At time: 95.50272870063782 and batch: 250, loss is 3.824720845222473 and perplexity is 45.82000770679285
At time: 96.39506554603577 and batch: 300, loss is 3.7894450330734255 and perplexity is 44.2318462515069
At time: 97.28954577445984 and batch: 350, loss is 3.7929847621917725 and perplexity is 44.38869243837962
At time: 98.17976832389832 and batch: 400, loss is 3.7158062267303467 and perplexity is 41.09170296246972
At time: 99.07318162918091 and batch: 450, loss is 3.7657202005386354 and perplexity is 43.194803581023386
At time: 99.96531939506531 and batch: 500, loss is 3.644208846092224 and perplexity is 38.25249726411982
At time: 100.85512447357178 and batch: 550, loss is 3.7047039222717286 and perplexity is 40.63801352477189
At time: 101.7484655380249 and batch: 600, loss is 3.7262491464614866 and perplexity is 41.52306875711284
At time: 102.63932061195374 and batch: 650, loss is 3.573753848075867 and perplexity is 35.65016759872874
At time: 103.53129863739014 and batch: 700, loss is 3.5878441095352174 and perplexity is 36.15604337272937
At time: 104.43047976493835 and batch: 750, loss is 3.690931396484375 and perplexity is 40.08216196179452
At time: 105.32284832000732 and batch: 800, loss is 3.653133707046509 and perplexity is 38.59542349154528
At time: 106.21526145935059 and batch: 850, loss is 3.729078335762024 and perplexity is 41.640711717597576
At time: 107.10725712776184 and batch: 900, loss is 3.6803143310546873 and perplexity is 39.6588581242526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344135441192209 and perplexity of 77.02541568266912
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.23235034942627 and batch: 50, loss is 3.795804977416992 and perplexity is 44.51405479594637
At time: 110.12929177284241 and batch: 100, loss is 3.684293279647827 and perplexity is 39.816973038943665
At time: 111.0189733505249 and batch: 150, loss is 3.6984563636779786 and perplexity is 40.38491659632661
At time: 111.91099405288696 and batch: 200, loss is 3.570929250717163 and perplexity is 35.549612310448296
At time: 112.8100745677948 and batch: 250, loss is 3.7171732711791994 and perplexity is 41.147915560704426
At time: 113.70431160926819 and batch: 300, loss is 3.66518177986145 and perplexity is 39.063236427442845
At time: 114.59606003761292 and batch: 350, loss is 3.6607550621032714 and perplexity is 38.8906966791962
At time: 115.49087047576904 and batch: 400, loss is 3.581934299468994 and perplexity is 35.94299817205129
At time: 116.38191151618958 and batch: 450, loss is 3.6195066785812378 and perplexity is 37.31915294272321
At time: 117.27518010139465 and batch: 500, loss is 3.490692501068115 and perplexity is 32.80865987322992
At time: 118.16716885566711 and batch: 550, loss is 3.532947635650635 and perplexity is 34.22470102800113
At time: 119.0576593875885 and batch: 600, loss is 3.5496299695968627 and perplexity is 34.800437884766815
At time: 119.95183253288269 and batch: 650, loss is 3.382943072319031 and perplexity is 29.45733874165188
At time: 120.84389567375183 and batch: 700, loss is 3.381228542327881 and perplexity is 29.406876522775317
At time: 121.73710584640503 and batch: 750, loss is 3.468748517036438 and perplexity is 32.09654901502138
At time: 122.6299238204956 and batch: 800, loss is 3.4136320304870607 and perplexity is 30.375368415334542
At time: 123.52274656295776 and batch: 850, loss is 3.4711965894699097 and perplexity is 32.175219948656654
At time: 124.42565584182739 and batch: 900, loss is 3.4181926441192627 and perplexity is 30.514215107011882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.303973054232663 and perplexity of 73.99318942953225
finished 7 epochs...
Completing Train Step...
At time: 126.56054830551147 and batch: 50, loss is 3.6984798192977903 and perplexity is 40.38586386068572
At time: 127.45305252075195 and batch: 100, loss is 3.581303448677063 and perplexity is 35.920330653852744
At time: 128.34362196922302 and batch: 150, loss is 3.591057152748108 and perplexity is 36.27240113364708
At time: 129.23361158370972 and batch: 200, loss is 3.4679944467544557 and perplexity is 32.07235508436346
At time: 130.12424540519714 and batch: 250, loss is 3.6159061002731323 and perplexity is 37.18502402590071
At time: 131.01369667053223 and batch: 300, loss is 3.569377689361572 and perplexity is 35.49449767370806
At time: 131.90878033638 and batch: 350, loss is 3.5677929592132567 and perplexity is 35.43829301951277
At time: 132.79731011390686 and batch: 400, loss is 3.492926125526428 and perplexity is 32.88202400182162
At time: 133.68135929107666 and batch: 450, loss is 3.534583773612976 and perplexity is 34.280743194459
At time: 134.57929062843323 and batch: 500, loss is 3.4103634691238405 and perplexity is 30.27624674062384
At time: 135.49303889274597 and batch: 550, loss is 3.4550098323822023 and perplexity is 31.658599960199343
At time: 136.388192653656 and batch: 600, loss is 3.4795162296295166 and perplexity is 32.444022821812524
At time: 137.2823281288147 and batch: 650, loss is 3.3168386220932007 and perplexity is 27.57304381488616
At time: 138.17490124702454 and batch: 700, loss is 3.3215754079818725 and perplexity is 27.70396123885516
At time: 139.06636476516724 and batch: 750, loss is 3.4165489959716795 and perplexity is 30.4641016695877
At time: 139.96110081672668 and batch: 800, loss is 3.3656365299224853 and perplexity is 28.951920198584524
At time: 140.85727739334106 and batch: 850, loss is 3.4307442569732665 and perplexity is 30.89963146004739
At time: 141.75324153900146 and batch: 900, loss is 3.3849862670898436 and perplexity is 29.517587350992954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315961445847603 and perplexity of 74.88558728066978
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.8734896183014 and batch: 50, loss is 3.6567926836013793 and perplexity is 38.73690191650209
At time: 144.77220487594604 and batch: 100, loss is 3.552428374290466 and perplexity is 34.897959982989846
At time: 145.66807508468628 and batch: 150, loss is 3.5681429147720336 and perplexity is 35.45069701744594
At time: 146.56200647354126 and batch: 200, loss is 3.4429942417144774 and perplexity is 31.280479402122502
At time: 147.45508289337158 and batch: 250, loss is 3.5926298809051516 and perplexity is 36.32949264317796
At time: 148.3470540046692 and batch: 300, loss is 3.538545536994934 and perplexity is 34.41682477106693
At time: 149.24093079566956 and batch: 350, loss is 3.5330563259124754 and perplexity is 34.2284211218826
At time: 150.13260698318481 and batch: 400, loss is 3.459928479194641 and perplexity is 31.81470102025684
At time: 151.02539658546448 and batch: 450, loss is 3.4941538619995116 and perplexity is 32.92241925428576
At time: 151.9191756248474 and batch: 500, loss is 3.3654091358184814 and perplexity is 28.945337451099125
At time: 152.8126516342163 and batch: 550, loss is 3.4038373374938966 and perplexity is 30.07930330750899
At time: 153.70528268814087 and batch: 600, loss is 3.4294722414016725 and perplexity is 30.860351635244403
At time: 154.60521817207336 and batch: 650, loss is 3.260065131187439 and perplexity is 26.05123383505787
At time: 155.49969911575317 and batch: 700, loss is 3.255695457458496 and perplexity is 25.93764679285572
At time: 156.3944754600525 and batch: 750, loss is 3.348165555000305 and perplexity is 28.450494875779636
At time: 157.28932309150696 and batch: 800, loss is 3.291785020828247 and perplexity is 26.8908215167704
At time: 158.18410897254944 and batch: 850, loss is 3.3547301197052004 and perplexity is 28.637874349715712
At time: 159.08857321739197 and batch: 900, loss is 3.3071391487121584 and perplexity is 27.306892660007385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311991234348245 and perplexity of 74.58886507562599
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.21912670135498 and batch: 50, loss is 3.6429457521438597 and perplexity is 38.20421126761032
At time: 162.11680245399475 and batch: 100, loss is 3.5364254760742186 and perplexity is 34.34393629715797
At time: 163.00746178627014 and batch: 150, loss is 3.5496482706069945 and perplexity is 34.801074773760966
At time: 163.9011902809143 and batch: 200, loss is 3.4245200777053832 and perplexity is 30.707903906826015
At time: 164.79448246955872 and batch: 250, loss is 3.574270691871643 and perplexity is 35.66859792906119
At time: 165.686363697052 and batch: 300, loss is 3.523680372238159 and perplexity is 33.908996824810735
At time: 166.57939171791077 and batch: 350, loss is 3.514840021133423 and perplexity is 33.610550516021846
At time: 167.47369599342346 and batch: 400, loss is 3.442384262084961 and perplexity is 31.261404765047157
At time: 168.36586809158325 and batch: 450, loss is 3.476561598777771 and perplexity is 32.348304187277854
At time: 169.25953602790833 and batch: 500, loss is 3.347279734611511 and perplexity is 28.425304006287522
At time: 170.15290474891663 and batch: 550, loss is 3.382780599594116 and perplexity is 29.45255311633511
At time: 171.04523468017578 and batch: 600, loss is 3.411117572784424 and perplexity is 30.299086779915914
At time: 171.9363842010498 and batch: 650, loss is 3.242901520729065 and perplexity is 25.607915956256885
At time: 172.84535026550293 and batch: 700, loss is 3.233479828834534 and perplexity is 25.36777908623234
At time: 173.7588312625885 and batch: 750, loss is 3.325872230529785 and perplexity is 27.82325635560424
At time: 174.6609878540039 and batch: 800, loss is 3.2686107301712037 and perplexity is 26.274811172978676
At time: 175.56141567230225 and batch: 850, loss is 3.3288518524169923 and perplexity is 27.906282771451004
At time: 176.48110365867615 and batch: 900, loss is 3.282429828643799 and perplexity is 26.64042578976639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308077877514983 and perplexity of 74.2975426269294
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.61178493499756 and batch: 50, loss is 3.637127194404602 and perplexity is 37.98256331949415
At time: 179.51080703735352 and batch: 100, loss is 3.5285210704803465 and perplexity is 34.07353797185676
At time: 180.41298127174377 and batch: 150, loss is 3.539544048309326 and perplexity is 34.45120752293297
At time: 181.30708575248718 and batch: 200, loss is 3.415901551246643 and perplexity is 30.444384231323212
At time: 182.19789814949036 and batch: 250, loss is 3.5663678216934205 and perplexity is 35.387824549288815
At time: 183.10347867012024 and batch: 300, loss is 3.5178552055358887 and perplexity is 33.71204545978801
At time: 183.99513173103333 and batch: 350, loss is 3.5074724674224855 and perplexity is 33.36383294830887
At time: 184.88998746871948 and batch: 400, loss is 3.4360025262832643 and perplexity is 31.06253797164378
At time: 185.80077052116394 and batch: 450, loss is 3.4704852724075317 and perplexity is 32.15234130367134
At time: 186.69531774520874 and batch: 500, loss is 3.3403869819641114 and perplexity is 28.2300491117374
At time: 187.58883428573608 and batch: 550, loss is 3.375599250793457 and perplexity is 29.241801704769617
At time: 188.48109889030457 and batch: 600, loss is 3.403267068862915 and perplexity is 30.062154914450982
At time: 189.38103222846985 and batch: 650, loss is 3.2356532430648803 and perplexity is 25.422973736983284
At time: 190.2778103351593 and batch: 700, loss is 3.2266854858398437 and perplexity is 25.19600589788443
At time: 191.17240571975708 and batch: 750, loss is 3.318101849555969 and perplexity is 27.607896850086142
At time: 192.0669026374817 and batch: 800, loss is 3.2622238874435423 and perplexity is 26.107532845134767
At time: 192.96966862678528 and batch: 850, loss is 3.3197351694107056 and perplexity is 27.653026221475304
At time: 193.88302636146545 and batch: 900, loss is 3.2736628007888795 and perplexity is 26.407889251365003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306580425941781 and perplexity of 74.18636891423742
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 196.03083992004395 and batch: 50, loss is 3.634402241706848 and perplexity is 37.879203520347865
At time: 196.9329912662506 and batch: 100, loss is 3.5255138874053955 and perplexity is 33.97122651690342
At time: 197.82875728607178 and batch: 150, loss is 3.5365495824813844 and perplexity is 34.34819886420021
At time: 198.72779393196106 and batch: 200, loss is 3.4133053064346313 and perplexity is 30.365445672959417
At time: 199.6222219467163 and batch: 250, loss is 3.5641183137893675 and perplexity is 35.3083088274114
At time: 200.5174310207367 and batch: 300, loss is 3.515388889312744 and perplexity is 33.62900334131061
At time: 201.41423559188843 and batch: 350, loss is 3.5050581312179565 and perplexity is 33.2833785995043
At time: 202.316171169281 and batch: 400, loss is 3.433756504058838 and perplexity is 30.992849111672655
At time: 203.20928525924683 and batch: 450, loss is 3.4683508825302125 and perplexity is 32.08378885671001
At time: 204.1112551689148 and batch: 500, loss is 3.3382431745529173 and perplexity is 28.169594148286425
At time: 205.03053545951843 and batch: 550, loss is 3.373610091209412 and perplexity is 29.183692907649377
At time: 205.9262194633484 and batch: 600, loss is 3.4014209651947023 and perplexity is 30.006708255876234
At time: 206.82150793075562 and batch: 650, loss is 3.2337187194824217 and perplexity is 25.373839935324984
At time: 207.72450971603394 and batch: 700, loss is 3.224308657646179 and perplexity is 25.136190434398294
At time: 208.61937832832336 and batch: 750, loss is 3.315230522155762 and perplexity is 27.52873923749409
At time: 209.52470993995667 and batch: 800, loss is 3.2597967100143435 and perplexity is 26.04424207072205
At time: 210.4269723892212 and batch: 850, loss is 3.317300782203674 and perplexity is 27.58578992100577
At time: 211.32102394104004 and batch: 900, loss is 3.2708957767486573 and perplexity is 26.33491898874798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.305481793129281 and perplexity of 74.10491008996998
Annealing...
Model not improving. Stopping early with 73.99318942953225 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
798.1195321083069


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.99318942953225, 'params': {'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.1787797272182029, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.29506562024038585, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.343942403793335 and batch: 50, loss is 7.069192419052124 and perplexity is 1175.1985829358289
At time: 2.4643731117248535 and batch: 100, loss is 6.381497468948364 and perplexity is 590.8117678436024
At time: 3.5852744579315186 and batch: 150, loss is 5.99325590133667 and perplexity is 400.7171838648657
At time: 4.71306586265564 and batch: 200, loss is 5.761677017211914 and perplexity is 317.880974030295
At time: 5.834720611572266 and batch: 250, loss is 5.718892402648926 and perplexity is 304.5673980272766
At time: 6.956317663192749 and batch: 300, loss is 5.55990273475647 and perplexity is 259.79756582049197
At time: 8.079989194869995 and batch: 350, loss is 5.492871303558349 and perplexity is 242.9538004252533
At time: 9.203544616699219 and batch: 400, loss is 5.305636692047119 and perplexity is 201.46923544683625
At time: 10.32715654373169 and batch: 450, loss is 5.2741873073577885 and perplexity is 195.23174861716353
At time: 11.447025060653687 and batch: 500, loss is 5.193020076751709 and perplexity is 180.01138101485316
At time: 12.569366455078125 and batch: 550, loss is 5.232549934387207 and perplexity is 187.26972070850005
At time: 13.69247031211853 and batch: 600, loss is 5.115271091461182 and perplexity is 166.54592404199568
At time: 14.815264701843262 and batch: 650, loss is 4.993233652114868 and perplexity is 147.4123338218025
At time: 15.938135862350464 and batch: 700, loss is 5.065796308517456 and perplexity is 158.50661199712852
At time: 17.06124472618103 and batch: 750, loss is 5.038566484451294 and perplexity is 154.2487384971076
At time: 18.18142580986023 and batch: 800, loss is 4.99625521659851 and perplexity is 147.85842329879523
At time: 19.302433252334595 and batch: 850, loss is 5.020920848846435 and perplexity is 151.55079493375936
At time: 20.42435050010681 and batch: 900, loss is 4.927611303329468 and perplexity is 138.0493601332866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.909457690095248 and perplexity of 135.56587575272977
finished 1 epochs...
Completing Train Step...
At time: 22.69189977645874 and batch: 50, loss is 4.865849695205688 and perplexity is 129.78116620996357
At time: 23.59185791015625 and batch: 100, loss is 4.746556520462036 and perplexity is 115.18695689590318
At time: 24.485249042510986 and batch: 150, loss is 4.737623224258423 and perplexity is 114.16254021267747
At time: 25.37843370437622 and batch: 200, loss is 4.618697509765625 and perplexity is 101.36192319782297
At time: 26.278811931610107 and batch: 250, loss is 4.731111288070679 and perplexity is 113.42153633929085
At time: 27.175864219665527 and batch: 300, loss is 4.666045894622803 and perplexity is 106.2766813239661
At time: 28.067087173461914 and batch: 350, loss is 4.657101860046387 and perplexity is 105.33037720825406
At time: 28.95656132698059 and batch: 400, loss is 4.533029718399048 and perplexity is 93.04001955595301
At time: 29.84977436065674 and batch: 450, loss is 4.562586908340454 and perplexity is 95.83106563406668
At time: 30.738364934921265 and batch: 500, loss is 4.454460391998291 and perplexity is 86.00972678406993
At time: 31.63045883178711 and batch: 550, loss is 4.523515892028809 and perplexity is 92.15905030357922
At time: 32.52190089225769 and batch: 600, loss is 4.479816703796387 and perplexity is 88.21850107726456
At time: 33.413325786590576 and batch: 650, loss is 4.335229082107544 and perplexity is 76.34244557507526
At time: 34.32147550582886 and batch: 700, loss is 4.382063846588135 and perplexity is 80.00297700853004
At time: 35.215582609176636 and batch: 750, loss is 4.426519985198975 and perplexity is 83.63984197395693
At time: 36.10904026031494 and batch: 800, loss is 4.384859161376953 and perplexity is 80.22692336777402
At time: 37.00258731842041 and batch: 850, loss is 4.4452611923217775 and perplexity is 85.22213429771182
At time: 37.89518976211548 and batch: 900, loss is 4.377725648880005 and perplexity is 79.65666001595888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.525114712649828 and perplexity of 92.30651394616193
finished 2 epochs...
Completing Train Step...
At time: 40.012285470962524 and batch: 50, loss is 4.433020811080933 and perplexity is 84.18534120006939
At time: 40.90571212768555 and batch: 100, loss is 4.311121683120728 and perplexity is 74.52403442731028
At time: 41.799739360809326 and batch: 150, loss is 4.318163843154907 and perplexity is 75.05069684814661
At time: 42.69101166725159 and batch: 200, loss is 4.2028963041305545 and perplexity is 66.87975490893184
At time: 43.584477186203 and batch: 250, loss is 4.343035936355591 and perplexity is 76.94077240695623
At time: 44.477147340774536 and batch: 300, loss is 4.302165694236756 and perplexity is 73.85957787744557
At time: 45.37103223800659 and batch: 350, loss is 4.2994896411895756 and perplexity is 73.6621899570716
At time: 46.262428998947144 and batch: 400, loss is 4.204658288955688 and perplexity is 66.99769990031831
At time: 47.16491675376892 and batch: 450, loss is 4.241653175354004 and perplexity is 69.5226900865155
At time: 48.06734871864319 and batch: 500, loss is 4.118530011177063 and perplexity is 61.46881734444518
At time: 48.9616756439209 and batch: 550, loss is 4.192531294822693 and perplexity is 66.19012580691388
At time: 49.85384202003479 and batch: 600, loss is 4.182410926818847 and perplexity is 65.52363562705925
At time: 50.74554181098938 and batch: 650, loss is 4.031428999900818 and perplexity is 56.34136555462773
At time: 51.639302492141724 and batch: 700, loss is 4.055661883354187 and perplexity is 57.723356483756284
At time: 52.532697916030884 and batch: 750, loss is 4.131951079368592 and perplexity is 62.299355421068526
At time: 53.42578721046448 and batch: 800, loss is 4.100069990158081 and perplexity is 60.344510971425166
At time: 54.31840658187866 and batch: 850, loss is 4.171719493865967 and perplexity is 64.82682565596068
At time: 55.217180490493774 and batch: 900, loss is 4.112556095123291 and perplexity is 61.102702448347586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394602736381636 and perplexity of 81.01244107378118
finished 3 epochs...
Completing Train Step...
At time: 57.34295868873596 and batch: 50, loss is 4.188546624183655 and perplexity is 65.92690472894378
At time: 58.242146730422974 and batch: 100, loss is 4.06759991645813 and perplexity is 58.41658951065844
At time: 59.136736154556274 and batch: 150, loss is 4.079288096427917 and perplexity is 59.10337896272774
At time: 60.0303738117218 and batch: 200, loss is 3.9634271240234376 and perplexity is 52.63741212363002
At time: 60.92134976387024 and batch: 250, loss is 4.110412001609802 and perplexity is 60.971832888793756
At time: 61.81255054473877 and batch: 300, loss is 4.076537580490112 and perplexity is 58.94103754059439
At time: 62.705424070358276 and batch: 350, loss is 4.072080659866333 and perplexity is 58.6789265526108
At time: 63.59770655632019 and batch: 400, loss is 3.9947040367126463 and perplexity is 54.309764548515
At time: 64.49381184577942 and batch: 450, loss is 4.032704033851624 and perplexity is 56.41324852542032
At time: 65.3878653049469 and batch: 500, loss is 3.902107849121094 and perplexity is 49.506691839838986
At time: 66.28445863723755 and batch: 550, loss is 3.977314395904541 and perplexity is 53.37350148352429
At time: 67.17503428459167 and batch: 600, loss is 3.9833836126327515 and perplexity is 53.69842184022327
At time: 68.06800961494446 and batch: 650, loss is 3.8270154094696043 and perplexity is 45.92526537233616
At time: 68.96233201026917 and batch: 700, loss is 3.847890954017639 and perplexity is 46.89405714126926
At time: 69.8562581539154 and batch: 750, loss is 3.9341373205184937 and perplexity is 51.1180324520291
At time: 70.75064945220947 and batch: 800, loss is 3.911711459159851 and perplexity is 49.98442511286021
At time: 71.64420485496521 and batch: 850, loss is 3.9787547302246096 and perplexity is 53.45043255942783
At time: 72.53632259368896 and batch: 900, loss is 3.925602841377258 and perplexity is 50.68362303614103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.349333044600813 and perplexity of 77.42680547461488
finished 4 epochs...
Completing Train Step...
At time: 74.6543219089508 and batch: 50, loss is 4.009047980308533 and perplexity is 55.09439463978437
At time: 75.55384159088135 and batch: 100, loss is 3.889589762687683 and perplexity is 48.89082556825893
At time: 76.44849824905396 and batch: 150, loss is 3.903353142738342 and perplexity is 49.56838060954178
At time: 77.34889578819275 and batch: 200, loss is 3.7866043186187746 and perplexity is 44.10637450558166
At time: 78.24493288993835 and batch: 250, loss is 3.937517857551575 and perplexity is 51.29113127242961
At time: 79.13778424263 and batch: 300, loss is 3.9046063375473024 and perplexity is 49.63053838657709
At time: 80.02979159355164 and batch: 350, loss is 3.9018263053894042 and perplexity is 49.49275550301098
At time: 80.92267942428589 and batch: 400, loss is 3.8302743434906006 and perplexity is 46.07517692533162
At time: 81.81699013710022 and batch: 450, loss is 3.8712255859375 and perplexity is 48.00117961591573
At time: 82.70898818969727 and batch: 500, loss is 3.7433169412612917 and perplexity is 42.237858564989025
At time: 83.61353516578674 and batch: 550, loss is 3.8132232284545897 and perplexity is 45.29620383631088
At time: 84.50823760032654 and batch: 600, loss is 3.827987322807312 and perplexity is 45.9699224481778
At time: 85.4011549949646 and batch: 650, loss is 3.6755404329299926 and perplexity is 39.469981972288316
At time: 86.29525351524353 and batch: 700, loss is 3.6922253561019898 and perplexity is 40.13406023065059
At time: 87.18768215179443 and batch: 750, loss is 3.7803230047225953 and perplexity is 43.830196809934854
At time: 88.08095955848694 and batch: 800, loss is 3.7622142744064333 and perplexity is 43.04363094526549
At time: 88.9743983745575 and batch: 850, loss is 3.823872013092041 and perplexity is 45.781130714389114
At time: 89.86972236633301 and batch: 900, loss is 3.7752311754226686 and perplexity is 43.60758815319613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341966602900257 and perplexity of 76.85854103915437
finished 5 epochs...
Completing Train Step...
At time: 91.98866558074951 and batch: 50, loss is 3.8667511987686156 and perplexity is 47.78688353309231
At time: 92.88303518295288 and batch: 100, loss is 3.7455501794815063 and perplexity is 42.332291171081565
At time: 93.77410936355591 and batch: 150, loss is 3.758437442779541 and perplexity is 42.881369009462105
At time: 94.66605544090271 and batch: 200, loss is 3.645519003868103 and perplexity is 38.3026469156616
At time: 95.55912518501282 and batch: 250, loss is 3.795986199378967 and perplexity is 44.52212245128804
At time: 96.45268154144287 and batch: 300, loss is 3.7641276597976683 and perplexity is 43.12606884248526
At time: 97.3428635597229 and batch: 350, loss is 3.7642478561401367 and perplexity is 43.131252749762275
At time: 98.23481488227844 and batch: 400, loss is 3.6922617149353028 and perplexity is 40.13551948478492
At time: 99.13701915740967 and batch: 450, loss is 3.735089297294617 and perplexity is 41.89176621747502
At time: 100.03599071502686 and batch: 500, loss is 3.6119090700149536 and perplexity is 37.03669100299226
At time: 100.92809057235718 and batch: 550, loss is 3.6775333499908447 and perplexity is 39.54872080667491
At time: 101.8191487789154 and batch: 600, loss is 3.6976633644104004 and perplexity is 40.35290408167244
At time: 102.70918726921082 and batch: 650, loss is 3.5471919918060304 and perplexity is 34.71569852839929
At time: 103.60088181495667 and batch: 700, loss is 3.562821264266968 and perplexity is 35.26254188971869
At time: 104.49064660072327 and batch: 750, loss is 3.651498279571533 and perplexity is 38.53235506153753
At time: 105.38157558441162 and batch: 800, loss is 3.635484004020691 and perplexity is 37.9202019864943
At time: 106.27390146255493 and batch: 850, loss is 3.696369252204895 and perplexity is 40.30071667143834
At time: 107.16537380218506 and batch: 900, loss is 3.646836462020874 and perplexity is 38.3531423055912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353499739137415 and perplexity of 77.7500923731249
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.29762506484985 and batch: 50, loss is 3.776558728218079 and perplexity is 43.66551797268897
At time: 110.18965339660645 and batch: 100, loss is 3.6616390466690065 and perplexity is 38.92509065444323
At time: 111.08236265182495 and batch: 150, loss is 3.679425058364868 and perplexity is 39.623606261395864
At time: 111.97705960273743 and batch: 200, loss is 3.55802360534668 and perplexity is 35.09376942116608
At time: 112.87093472480774 and batch: 250, loss is 3.696618127822876 and perplexity is 40.31074778540308
At time: 113.76279258728027 and batch: 300, loss is 3.655206255912781 and perplexity is 38.67549734256772
At time: 114.65388536453247 and batch: 350, loss is 3.643895764350891 and perplexity is 38.24052298022866
At time: 115.5412163734436 and batch: 400, loss is 3.565033235549927 and perplexity is 35.34062794996592
At time: 116.43340492248535 and batch: 450, loss is 3.597117052078247 and perplexity is 36.49287558477361
At time: 117.32732725143433 and batch: 500, loss is 3.4717227602005005 and perplexity is 32.192154062370754
At time: 118.22234582901001 and batch: 550, loss is 3.5144393253326416 and perplexity is 33.59708560741445
At time: 119.11927676200867 and batch: 600, loss is 3.5292464971542357 and perplexity is 34.098264792838705
At time: 120.01253890991211 and batch: 650, loss is 3.364508571624756 and perplexity is 28.91928205065665
At time: 120.90484523773193 and batch: 700, loss is 3.36179648399353 and perplexity is 28.84095668414166
At time: 121.80311894416809 and batch: 750, loss is 3.4378142404556273 and perplexity is 31.11886542112883
At time: 122.69489455223083 and batch: 800, loss is 3.406995859146118 and perplexity is 30.174459635842517
At time: 123.58703374862671 and batch: 850, loss is 3.4415366888046264 and perplexity is 31.2349196592821
At time: 124.4800009727478 and batch: 900, loss is 3.384248013496399 and perplexity is 29.49580392789582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310207001150471 and perplexity of 74.45589980217576
finished 7 epochs...
Completing Train Step...
At time: 126.59143042564392 and batch: 50, loss is 3.68656662940979 and perplexity is 39.90759391256629
At time: 127.49025845527649 and batch: 100, loss is 3.558698058128357 and perplexity is 35.11744649520841
At time: 128.38627767562866 and batch: 150, loss is 3.5744509506225586 and perplexity is 35.67502808549937
At time: 129.2809498310089 and batch: 200, loss is 3.4553851461410523 and perplexity is 31.670484098351114
At time: 130.1755564212799 and batch: 250, loss is 3.5955801105499265 and perplexity is 36.43683124826592
At time: 131.07027006149292 and batch: 300, loss is 3.5585432147979734 and perplexity is 35.112009213812776
At time: 131.96283841133118 and batch: 350, loss is 3.5503122854232787 and perplexity is 34.8241908769015
At time: 132.8569004535675 and batch: 400, loss is 3.4763603019714355 and perplexity is 32.34179323229379
At time: 133.75280594825745 and batch: 450, loss is 3.5119470357894897 and perplexity is 33.51345619991234
At time: 134.64778208732605 and batch: 500, loss is 3.3894882202148438 and perplexity is 29.65077372005837
At time: 135.54384088516235 and batch: 550, loss is 3.4377462005615236 and perplexity is 31.116748168850577
At time: 136.43750190734863 and batch: 600, loss is 3.459451971054077 and perplexity is 31.799544667580584
At time: 137.32812309265137 and batch: 650, loss is 3.299560546875 and perplexity is 27.10072680435728
At time: 138.220844745636 and batch: 700, loss is 3.302008004188538 and perplexity is 27.16713590995291
At time: 139.1135699748993 and batch: 750, loss is 3.3850302267074586 and perplexity is 29.51888496136684
At time: 140.01304388046265 and batch: 800, loss is 3.3616029834747314 and perplexity is 28.835376483963852
At time: 140.90331387519836 and batch: 850, loss is 3.400385928153992 and perplexity is 29.975666268936042
At time: 141.79451537132263 and batch: 900, loss is 3.35188880443573 and perplexity is 28.5566206084525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322892280474101 and perplexity of 75.4064096841236
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.90344977378845 and batch: 50, loss is 3.649856061935425 and perplexity is 38.469128478609
At time: 144.80682635307312 and batch: 100, loss is 3.5350136947631836 and perplexity is 34.295484379549706
At time: 145.70301795005798 and batch: 150, loss is 3.558615679740906 and perplexity is 35.114553695748455
At time: 146.59765481948853 and batch: 200, loss is 3.4350143337249754 and perplexity is 31.03185736444919
At time: 147.49035739898682 and batch: 250, loss is 3.5760959148406983 and perplexity is 35.73376052331677
At time: 148.38630509376526 and batch: 300, loss is 3.5352635765075684 and perplexity is 34.3040552658204
At time: 149.28040170669556 and batch: 350, loss is 3.517247185707092 and perplexity is 33.69155409788763
At time: 150.17260456085205 and batch: 400, loss is 3.4460483026504516 and perplexity is 31.37615792196959
At time: 151.0656020641327 and batch: 450, loss is 3.477445116043091 and perplexity is 32.376897101838
At time: 151.96076273918152 and batch: 500, loss is 3.350074028968811 and perplexity is 28.50484374985075
At time: 152.85660862922668 and batch: 550, loss is 3.3892817592620847 and perplexity is 29.64465262497139
At time: 153.7497329711914 and batch: 600, loss is 3.4135153532028197 and perplexity is 30.37182450659087
At time: 154.64232516288757 and batch: 650, loss is 3.2467320442199705 and perplexity is 25.706195791318
At time: 155.53473496437073 and batch: 700, loss is 3.238692216873169 and perplexity is 25.50035100248983
At time: 156.4289243221283 and batch: 750, loss is 3.318843083381653 and perplexity is 27.628368343231706
At time: 157.32221794128418 and batch: 800, loss is 3.2878395700454712 and perplexity is 26.784934128066585
At time: 158.21700596809387 and batch: 850, loss is 3.325575280189514 and perplexity is 27.81499545676106
At time: 159.11218237876892 and batch: 900, loss is 3.277895698547363 and perplexity is 26.519908061411044
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318284544226241 and perplexity of 75.05975609437992
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.28089332580566 and batch: 50, loss is 3.636945548057556 and perplexity is 37.97566455220262
At time: 162.1750512123108 and batch: 100, loss is 3.5169305181503296 and perplexity is 33.680886764856204
At time: 163.06857109069824 and batch: 150, loss is 3.540718879699707 and perplexity is 34.491705667555145
At time: 163.96102046966553 and batch: 200, loss is 3.4174823999404906 and perplexity is 30.492550257939527
At time: 164.85465168952942 and batch: 250, loss is 3.5594668197631836 and perplexity is 35.1444538205547
At time: 165.7484154701233 and batch: 300, loss is 3.519666838645935 and perplexity is 33.77317467268447
At time: 166.65370297431946 and batch: 350, loss is 3.5005470180511473 and perplexity is 33.133571664136774
At time: 167.5451476573944 and batch: 400, loss is 3.431649045944214 and perplexity is 30.927601757498966
At time: 168.4374725818634 and batch: 450, loss is 3.4633776235580442 and perplexity is 31.924623978053628
At time: 169.33215641975403 and batch: 500, loss is 3.3301080465316772 and perplexity is 27.94136050724107
At time: 170.22634863853455 and batch: 550, loss is 3.3705231618881224 and perplexity is 29.093743814983092
At time: 171.12044477462769 and batch: 600, loss is 3.3959591579437256 and perplexity is 29.843264155545327
At time: 172.01446795463562 and batch: 650, loss is 3.226591968536377 and perplexity is 25.19364974552695
At time: 172.90776872634888 and batch: 700, loss is 3.2160667753219605 and perplexity is 24.92987230354434
At time: 173.8009307384491 and batch: 750, loss is 3.2959609508514403 and perplexity is 27.003350498740765
At time: 174.69394850730896 and batch: 800, loss is 3.265919885635376 and perplexity is 26.204204778921756
At time: 175.58414673805237 and batch: 850, loss is 3.3007815074920654 and perplexity is 27.133835932837115
At time: 176.4749629497528 and batch: 900, loss is 3.2531866455078124 and perplexity is 25.872655673945676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316261605040668 and perplexity of 74.90806825188645
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.59715461730957 and batch: 50, loss is 3.6291061544418337 and perplexity is 37.67912224458855
At time: 179.50043320655823 and batch: 100, loss is 3.5064455318450927 and perplexity is 33.32958802792915
At time: 180.3944594860077 and batch: 150, loss is 3.5299574375152587 and perplexity is 34.12251524481191
At time: 181.28725719451904 and batch: 200, loss is 3.409216275215149 and perplexity is 30.241533929754546
At time: 182.18100881576538 and batch: 250, loss is 3.5532545375823976 and perplexity is 34.926803309499306
At time: 183.07221245765686 and batch: 300, loss is 3.512220220565796 and perplexity is 33.52261281661475
At time: 183.96266341209412 and batch: 350, loss is 3.493530011177063 and perplexity is 32.90188698116062
At time: 184.85506391525269 and batch: 400, loss is 3.426515007019043 and perplexity is 30.769225149926594
At time: 185.75961565971375 and batch: 450, loss is 3.4579147243499757 and perplexity is 31.75069847629085
At time: 186.65355896949768 and batch: 500, loss is 3.3226223039627074 and perplexity is 27.73297959147737
At time: 187.54437065124512 and batch: 550, loss is 3.3639301919937132 and perplexity is 28.902560563124215
At time: 188.43790364265442 and batch: 600, loss is 3.390004096031189 and perplexity is 29.666073783283462
At time: 189.3481411933899 and batch: 650, loss is 3.2195029640197754 and perplexity is 25.015683396111204
At time: 190.24242782592773 and batch: 700, loss is 3.2091300678253174 and perplexity is 24.757539473686478
At time: 191.136732339859 and batch: 750, loss is 3.2875121355056764 and perplexity is 26.776165251181858
At time: 192.03097581863403 and batch: 800, loss is 3.257818727493286 and perplexity is 25.99277792949696
At time: 192.92551040649414 and batch: 850, loss is 3.29291024684906 and perplexity is 26.921096798935363
At time: 193.81941723823547 and batch: 900, loss is 3.2450975608825683 and perplexity is 25.664213761439616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.312238719365368 and perplexity of 74.60732698660539
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.94158720970154 and batch: 50, loss is 3.624236922264099 and perplexity is 37.49609980112477
At time: 196.8398883342743 and batch: 100, loss is 3.502547335624695 and perplexity is 33.19991566222189
At time: 197.7329671382904 and batch: 150, loss is 3.5263124895095825 and perplexity is 33.998366845601005
At time: 198.624737739563 and batch: 200, loss is 3.406691994667053 and perplexity is 30.165292082301686
At time: 199.5181906223297 and batch: 250, loss is 3.5509417343139646 and perplexity is 34.84611782544075
At time: 200.41343545913696 and batch: 300, loss is 3.5092108964920046 and perplexity is 33.42188404961795
At time: 201.30864787101746 and batch: 350, loss is 3.4909328269958495 and perplexity is 32.81654559238511
At time: 202.20248556137085 and batch: 400, loss is 3.4244343185424806 and perplexity is 30.70527053561194
At time: 203.09743928909302 and batch: 450, loss is 3.4563507604599 and perplexity is 31.70108034098896
At time: 203.99294328689575 and batch: 500, loss is 3.3205885219573976 and perplexity is 27.676634073302758
At time: 204.88921666145325 and batch: 550, loss is 3.36134135723114 and perplexity is 28.82783337951188
At time: 205.7852325439453 and batch: 600, loss is 3.3881490325927732 and perplexity is 29.611092347239765
At time: 206.68189072608948 and batch: 650, loss is 3.217373905181885 and perplexity is 24.96248019076514
At time: 207.57787919044495 and batch: 700, loss is 3.2071675491333007 and perplexity is 24.708999985106914
At time: 208.4757912158966 and batch: 750, loss is 3.2850626564025878 and perplexity is 26.710657856189215
At time: 209.3715465068817 and batch: 800, loss is 3.2551693725585937 and perplexity is 25.92400497723023
At time: 210.26851606369019 and batch: 850, loss is 3.29053475856781 and perplexity is 26.85722194599027
At time: 211.1687457561493 and batch: 900, loss is 3.242516198158264 and perplexity is 25.598050549051862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310783490742723 and perplexity of 74.49883522821749
Annealing...
Model not improving. Stopping early with 74.45589980217576 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
1016.7040357589722


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.99318942953225, 'params': {'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -74.45589980217576, 'params': {'rnn_dropout': 0.1787797272182029, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.29506562024038585, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.8453812474725898, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.79552321001791, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3167102336883545 and batch: 50, loss is 7.107424478530884 and perplexity is 1220.9987843976444
At time: 2.4367218017578125 and batch: 100, loss is 6.524860410690308 and perplexity is 681.884583208806
At time: 3.5572760105133057 and batch: 150, loss is 6.230053462982178 and perplexity is 507.78263034382775
At time: 4.678830862045288 and batch: 200, loss is 6.024157199859619 and perplexity is 413.2931716551287
At time: 5.797977924346924 and batch: 250, loss is 6.084629974365234 and perplexity is 439.0573201481525
At time: 6.920448303222656 and batch: 300, loss is 5.998645668029785 and perplexity is 402.88278680062
At time: 8.041388273239136 and batch: 350, loss is 6.007228031158447 and perplexity is 406.3553532735962
At time: 9.163780450820923 and batch: 400, loss is 5.891761732101441 and perplexity is 362.04254483968407
At time: 10.28632926940918 and batch: 450, loss is 5.898835344314575 and perplexity is 364.61257237948615
At time: 11.408779382705688 and batch: 500, loss is 5.861116733551025 and perplexity is 351.116028213082
At time: 12.52824330329895 and batch: 550, loss is 5.8982515716552735 and perplexity is 364.3997836446592
At time: 13.652517795562744 and batch: 600, loss is 5.832359800338745 and perplexity is 341.16280601427377
At time: 14.778480768203735 and batch: 650, loss is 5.739699630737305 and perplexity is 310.97099082458993
At time: 15.901063203811646 and batch: 700, loss is 5.8437441539764405 and perplexity is 345.0689161283722
At time: 17.023521423339844 and batch: 750, loss is 5.798927001953125 and perplexity is 329.94533920003187
At time: 18.1400249004364 and batch: 800, loss is 5.796603336334228 and perplexity is 329.17954662672423
At time: 19.26413059234619 and batch: 850, loss is 5.833053379058838 and perplexity is 341.3995113540169
At time: 20.387441158294678 and batch: 900, loss is 5.7060543823242185 and perplexity is 300.6823471217361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.675732756314212 and perplexity of 291.7020067397071
finished 1 epochs...
Completing Train Step...
At time: 22.655357360839844 and batch: 50, loss is 5.542616348266602 and perplexity is 255.3451982325609
At time: 23.543842315673828 and batch: 100, loss is 5.358547792434693 and perplexity is 212.4162498729861
At time: 24.43471622467041 and batch: 150, loss is 5.282339000701905 and perplexity is 196.82972220949168
At time: 25.3250253200531 and batch: 200, loss is 5.111373805999756 and perplexity is 165.8981102112405
At time: 26.216824769973755 and batch: 250, loss is 5.147497720718384 and perplexity is 172.00055795255312
At time: 27.110396146774292 and batch: 300, loss is 5.055744962692261 and perplexity is 156.92138739091783
At time: 28.00847101211548 and batch: 350, loss is 5.020911483764649 and perplexity is 151.54937565481583
At time: 28.899784564971924 and batch: 400, loss is 4.85639799118042 and perplexity is 128.56029179954064
At time: 29.788692951202393 and batch: 450, loss is 4.8580522537231445 and perplexity is 128.77314027984067
At time: 30.67704677581787 and batch: 500, loss is 4.764651975631714 and perplexity is 117.29029030463889
At time: 31.565654516220093 and batch: 550, loss is 4.820879220962524 and perplexity is 124.07413141464042
At time: 32.4538311958313 and batch: 600, loss is 4.741404848098755 and perplexity is 114.59507732728643
At time: 33.34284996986389 and batch: 650, loss is 4.606245250701904 and perplexity is 100.10756428030247
At time: 34.23144054412842 and batch: 700, loss is 4.666185216903687 and perplexity is 106.29148906511308
At time: 35.11940336227417 and batch: 750, loss is 4.677931776046753 and perplexity is 107.5474102629958
At time: 36.010011196136475 and batch: 800, loss is 4.6269469070434575 and perplexity is 102.20155644363955
At time: 36.900150775909424 and batch: 850, loss is 4.672132225036621 and perplexity is 106.92548874712813
At time: 37.789984703063965 and batch: 900, loss is 4.592393140792847 and perplexity is 98.7304234706118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.687009210455908 and perplexity of 108.52810920659195
finished 2 epochs...
Completing Train Step...
At time: 39.90406370162964 and batch: 50, loss is 4.640991420745849 and perplexity is 103.64705450206287
At time: 40.802682399749756 and batch: 100, loss is 4.512465815544129 and perplexity is 91.14629158477351
At time: 41.695762395858765 and batch: 150, loss is 4.518133115768433 and perplexity is 91.66431148417028
At time: 42.58792161941528 and batch: 200, loss is 4.403155202865601 and perplexity is 81.70826854005608
At time: 43.48080229759216 and batch: 250, loss is 4.535509624481201 and perplexity is 93.27103639797444
At time: 44.37271165847778 and batch: 300, loss is 4.4904820919036865 and perplexity is 89.16442096429968
At time: 45.26412606239319 and batch: 350, loss is 4.490985155105591 and perplexity is 89.20928758783337
At time: 46.15617322921753 and batch: 400, loss is 4.3793358755111695 and perplexity is 79.78502861479797
At time: 47.047457218170166 and batch: 450, loss is 4.413882951736451 and perplexity is 82.58953286516389
At time: 47.93983817100525 and batch: 500, loss is 4.2964080047607425 and perplexity is 73.43553927593541
At time: 48.8331618309021 and batch: 550, loss is 4.368266210556031 and perplexity is 78.90670542027988
At time: 49.72611165046692 and batch: 600, loss is 4.34971583366394 and perplexity is 77.45644928224414
At time: 50.621222257614136 and batch: 650, loss is 4.2023896312713624 and perplexity is 66.84587733543833
At time: 51.513447999954224 and batch: 700, loss is 4.229070806503296 and perplexity is 68.6534102246611
At time: 52.40372443199158 and batch: 750, loss is 4.307610764503479 and perplexity is 74.26284538244363
At time: 53.29465675354004 and batch: 800, loss is 4.262320504188538 and perplexity is 70.97448911168772
At time: 54.20693564414978 and batch: 850, loss is 4.327422571182251 and perplexity is 75.74879761409541
At time: 55.09902095794678 and batch: 900, loss is 4.263327469825745 and perplexity is 71.04599397877148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.464745090432363 and perplexity of 86.89887536146978
finished 3 epochs...
Completing Train Step...
At time: 57.212000608444214 and batch: 50, loss is 4.3421937942504885 and perplexity is 76.87600461858185
At time: 58.11242055892944 and batch: 100, loss is 4.213635530471802 and perplexity is 67.60186223143663
At time: 59.00429391860962 and batch: 150, loss is 4.2188333225250245 and perplexity is 67.95415743917766
At time: 59.89462375640869 and batch: 200, loss is 4.107225394248962 and perplexity is 60.77784883696049
At time: 60.79016137123108 and batch: 250, loss is 4.2548588228225706 and perplexity is 70.44687099595542
At time: 61.68234968185425 and batch: 300, loss is 4.221379685401916 and perplexity is 68.12741387627536
At time: 62.5734224319458 and batch: 350, loss is 4.224192605018616 and perplexity is 68.31932059775492
At time: 63.46388554573059 and batch: 400, loss is 4.1315223979949955 and perplexity is 62.272654571299846
At time: 64.3565285205841 and batch: 450, loss is 4.173294477462768 and perplexity is 64.92900728907709
At time: 65.24721765518188 and batch: 500, loss is 4.04261393070221 and perplexity is 56.97507723367941
At time: 66.1369936466217 and batch: 550, loss is 4.114027757644653 and perplexity is 61.192691205794176
At time: 67.0347626209259 and batch: 600, loss is 4.122950572967529 and perplexity is 61.74114552801262
At time: 67.92707586288452 and batch: 650, loss is 3.96698278427124 and perplexity is 52.82490601221237
At time: 68.81747436523438 and batch: 700, loss is 3.9796147298812867 and perplexity is 53.496419684699816
At time: 69.70774507522583 and batch: 750, loss is 4.0798859167099 and perplexity is 59.1387227249617
At time: 70.59845161437988 and batch: 800, loss is 4.042194085121155 and perplexity is 56.95116152007723
At time: 71.4888858795166 and batch: 850, loss is 4.108941903114319 and perplexity is 60.88226414256788
At time: 72.38741278648376 and batch: 900, loss is 4.053306007385254 and perplexity is 57.58752747633992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381366886504709 and perplexity of 79.94723755334377
finished 4 epochs...
Completing Train Step...
At time: 74.50686550140381 and batch: 50, loss is 4.140694694519043 and perplexity is 62.84646538288842
At time: 75.39685583114624 and batch: 100, loss is 4.012641429901123 and perplexity is 55.29272970987207
At time: 76.28954911231995 and batch: 150, loss is 4.021908845901489 and perplexity is 55.80753219781716
At time: 77.17816019058228 and batch: 200, loss is 3.911270170211792 and perplexity is 49.96237240464888
At time: 78.06615495681763 and batch: 250, loss is 4.059793658256531 and perplexity is 57.962349792633624
At time: 78.95486879348755 and batch: 300, loss is 4.030457262992859 and perplexity is 56.28664316246361
At time: 79.85035228729248 and batch: 350, loss is 4.033116993904113 and perplexity is 56.436549754399735
At time: 80.73890328407288 and batch: 400, loss is 3.9499686765670776 and perplexity is 51.9337400663302
At time: 81.62776207923889 and batch: 450, loss is 3.9938255643844602 and perplexity is 54.262075872866625
At time: 82.51624011993408 and batch: 500, loss is 3.8633686828613283 and perplexity is 47.625516706166785
At time: 83.40500831604004 and batch: 550, loss is 3.933493356704712 and perplexity is 51.085124885676436
At time: 84.29594683647156 and batch: 600, loss is 3.9567000532150267 and perplexity is 52.28450487199395
At time: 85.21069550514221 and batch: 650, loss is 3.794414439201355 and perplexity is 44.45219931778971
At time: 86.10187768936157 and batch: 700, loss is 3.8044573211669923 and perplexity is 44.9008767447632
At time: 87.00995469093323 and batch: 750, loss is 3.9118407487869264 and perplexity is 49.990887998325654
At time: 87.90576171875 and batch: 800, loss is 3.874601240158081 and perplexity is 48.16348879618715
At time: 88.79693222045898 and batch: 850, loss is 3.9430191850662233 and perplexity is 51.574078162305355
At time: 89.68900656700134 and batch: 900, loss is 3.893348455429077 and perplexity is 49.07493695174656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.351453807255993 and perplexity of 77.59118359405444
finished 5 epochs...
Completing Train Step...
At time: 91.86387252807617 and batch: 50, loss is 3.9827492570877077 and perplexity is 53.664368750593866
At time: 92.76003336906433 and batch: 100, loss is 3.8584987926483154 and perplexity is 47.394149492190515
At time: 93.65179109573364 and batch: 150, loss is 3.868617420196533 and perplexity is 47.876147706564495
At time: 94.54722094535828 and batch: 200, loss is 3.7597452497482298 and perplexity is 42.9374862499403
At time: 95.43804907798767 and batch: 250, loss is 3.907560234069824 and perplexity is 49.777358600430965
At time: 96.32950711250305 and batch: 300, loss is 3.8783816957473753 and perplexity is 48.345913333271824
At time: 97.22074127197266 and batch: 350, loss is 3.8802749490737916 and perplexity is 48.437531094958416
At time: 98.11101841926575 and batch: 400, loss is 3.803763017654419 and perplexity is 44.86971272821666
At time: 99.00678277015686 and batch: 450, loss is 3.8480286169052125 and perplexity is 46.900513156952044
At time: 99.89384174346924 and batch: 500, loss is 3.722263240814209 and perplexity is 41.35789113276188
At time: 100.78377985954285 and batch: 550, loss is 3.787812376022339 and perplexity is 44.15968973529247
At time: 101.67351198196411 and batch: 600, loss is 3.8171347856521605 and perplexity is 45.473729502999774
At time: 102.56186485290527 and batch: 650, loss is 3.6598198509216306 and perplexity is 38.85434266678913
At time: 103.45944452285767 and batch: 700, loss is 3.6638368129730225 and perplexity is 39.01073298350454
At time: 104.35350131988525 and batch: 750, loss is 3.776726803779602 and perplexity is 43.672857695938205
At time: 105.24344658851624 and batch: 800, loss is 3.736651549339294 and perplexity is 41.9572628627163
At time: 106.13250088691711 and batch: 850, loss is 3.8096029567718506 and perplexity is 45.13251574868057
At time: 107.0211718082428 and batch: 900, loss is 3.7597188091278078 and perplexity is 42.93635097117329
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346662129441353 and perplexity of 77.22028097360956
finished 6 epochs...
Completing Train Step...
At time: 109.14426755905151 and batch: 50, loss is 3.856322417259216 and perplexity is 47.29111419406178
At time: 110.03761219978333 and batch: 100, loss is 3.730151391029358 and perplexity is 41.685418484767894
At time: 110.93642067909241 and batch: 150, loss is 3.740876040458679 and perplexity is 42.13488586631444
At time: 111.8269305229187 and batch: 200, loss is 3.635096297264099 and perplexity is 37.90550291762109
At time: 112.71785616874695 and batch: 250, loss is 3.78215989112854 and perplexity is 43.910781892787654
At time: 113.60937786102295 and batch: 300, loss is 3.755290913581848 and perplexity is 42.74665358399406
At time: 114.5013542175293 and batch: 350, loss is 3.75491868019104 and perplexity is 42.73074481325632
At time: 115.39289546012878 and batch: 400, loss is 3.6796152544021608 and perplexity is 39.63114323101818
At time: 116.28362607955933 and batch: 450, loss is 3.7261651134490967 and perplexity is 41.51957959516596
At time: 117.18968343734741 and batch: 500, loss is 3.6052253437042237 and perplexity is 36.7899733120853
At time: 118.08011960983276 and batch: 550, loss is 3.667183141708374 and perplexity is 39.14149438356838
At time: 118.98082327842712 and batch: 600, loss is 3.698227953910828 and perplexity is 40.37569334031142
At time: 119.87091588973999 and batch: 650, loss is 3.543504390716553 and perplexity is 34.587916629820626
At time: 120.76793146133423 and batch: 700, loss is 3.547970485687256 and perplexity is 34.74273500978832
At time: 121.65372896194458 and batch: 750, loss is 3.661978311538696 and perplexity is 38.938298810656775
At time: 122.53967046737671 and batch: 800, loss is 3.6208560943603514 and perplexity is 37.3695459895067
At time: 123.42360210418701 and batch: 850, loss is 3.69732075214386 and perplexity is 40.33908104984888
At time: 124.30919075012207 and batch: 900, loss is 3.6476524257659912 and perplexity is 38.3844498503976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358371264313998 and perplexity of 78.12977797871208
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 126.41479086875916 and batch: 50, loss is 3.7792864418029786 and perplexity is 43.78478759198382
At time: 127.3047227859497 and batch: 100, loss is 3.6595787143707277 and perplexity is 38.84497459414862
At time: 128.1957082748413 and batch: 150, loss is 3.668426752090454 and perplexity is 39.190201432372305
At time: 129.09460926055908 and batch: 200, loss is 3.549098501205444 and perplexity is 34.78194746599532
At time: 129.9842767715454 and batch: 250, loss is 3.6932921981811524 and perplexity is 40.17689978236735
At time: 130.87664437294006 and batch: 300, loss is 3.6548611640930178 and perplexity is 38.66215304744551
At time: 131.76878571510315 and batch: 350, loss is 3.643462724685669 and perplexity is 38.2239669019374
At time: 132.6603229045868 and batch: 400, loss is 3.569494786262512 and perplexity is 35.49865421274025
At time: 133.55231165885925 and batch: 450, loss is 3.594312243461609 and perplexity is 36.39066346261615
At time: 134.44556212425232 and batch: 500, loss is 3.464240622520447 and perplexity is 31.95218678704891
At time: 135.34480810165405 and batch: 550, loss is 3.509063820838928 and perplexity is 33.416968865655114
At time: 136.23660516738892 and batch: 600, loss is 3.535198645591736 and perplexity is 34.301827944407016
At time: 137.1281018257141 and batch: 650, loss is 3.3652418422698975 and perplexity is 28.940495487907867
At time: 138.01902222633362 and batch: 700, loss is 3.3565740728378297 and perplexity is 28.690729964496466
At time: 138.9237072467804 and batch: 750, loss is 3.4556101512908937 and perplexity is 31.67761092212717
At time: 139.82175874710083 and batch: 800, loss is 3.398935194015503 and perplexity is 29.932211075143734
At time: 140.71448278427124 and batch: 850, loss is 3.456093502044678 and perplexity is 31.6929260202284
At time: 141.60636639595032 and batch: 900, loss is 3.398031964302063 and perplexity is 29.905187618743298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325192804205908 and perplexity of 75.58008361303365
finished 8 epochs...
Completing Train Step...
At time: 143.71518754959106 and batch: 50, loss is 3.6900695037841795 and perplexity is 40.04763032241323
At time: 144.6111125946045 and batch: 100, loss is 3.5606691312789915 and perplexity is 35.1867338138682
At time: 145.50241923332214 and batch: 150, loss is 3.567519826889038 and perplexity is 35.42861499792444
At time: 146.40081357955933 and batch: 200, loss is 3.449069275856018 and perplexity is 31.471087772416443
At time: 147.2909290790558 and batch: 250, loss is 3.5957565546035766 and perplexity is 36.443260877691785
At time: 148.17945265769958 and batch: 300, loss is 3.55971643447876 and perplexity is 35.153227488371954
At time: 149.0708029270172 and batch: 350, loss is 3.551025619506836 and perplexity is 34.84904102136007
At time: 149.95858097076416 and batch: 400, loss is 3.482442183494568 and perplexity is 32.53909155131885
At time: 150.84919095039368 and batch: 450, loss is 3.512192378044128 and perplexity is 33.52167947553439
At time: 151.74004745483398 and batch: 500, loss is 3.3881697130203245 and perplexity is 29.61170472362185
At time: 152.6305320262909 and batch: 550, loss is 3.435668225288391 and perplexity is 31.052155469833615
At time: 153.52135682106018 and batch: 600, loss is 3.467817897796631 and perplexity is 32.06669324330911
At time: 154.41331720352173 and batch: 650, loss is 3.3044786834716797 and perplexity is 27.234340175735536
At time: 155.31966376304626 and batch: 700, loss is 3.2980963373184204 and perplexity is 27.061074697761008
At time: 156.22062015533447 and batch: 750, loss is 3.4047547817230224 and perplexity is 30.10691205355042
At time: 157.11057949066162 and batch: 800, loss is 3.353359479904175 and perplexity is 28.598649027350145
At time: 158.00214052200317 and batch: 850, loss is 3.4188872385025024 and perplexity is 30.53541747210359
At time: 158.89268469810486 and batch: 900, loss is 3.365993409156799 and perplexity is 28.96225438161489
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337867632304152 and perplexity of 76.5441449297746
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 160.99973726272583 and batch: 50, loss is 3.65540274143219 and perplexity is 38.68309726436432
At time: 161.89498782157898 and batch: 100, loss is 3.544934334754944 and perplexity is 34.6374107935144
At time: 162.78546595573425 and batch: 150, loss is 3.557675085067749 and perplexity is 35.08154066196775
At time: 163.68473029136658 and batch: 200, loss is 3.434429850578308 and perplexity is 31.013725066337408
At time: 164.57656002044678 and batch: 250, loss is 3.577963700294495 and perplexity is 35.800565891057154
At time: 165.46770763397217 and batch: 300, loss is 3.538382873535156 and perplexity is 34.4112268665747
At time: 166.35650515556335 and batch: 350, loss is 3.5248137760162352 and perplexity is 33.94745119797211
At time: 167.24656534194946 and batch: 400, loss is 3.4561626434326174 and perplexity is 31.695117388877563
At time: 168.1387894153595 and batch: 450, loss is 3.4799113178253176 and perplexity is 32.45684360475695
At time: 169.02976632118225 and batch: 500, loss is 3.351359658241272 and perplexity is 28.541513978491327
At time: 169.9177849292755 and batch: 550, loss is 3.3885135316848753 and perplexity is 29.621887530814274
At time: 170.8070616722107 and batch: 600, loss is 3.4232371759414675 and perplexity is 30.66853394203614
At time: 171.69204092025757 and batch: 650, loss is 3.2505797147750854 and perplexity is 25.805295292804804
At time: 172.58008790016174 and batch: 700, loss is 3.23879946231842 and perplexity is 25.503085945639626
At time: 173.4689826965332 and batch: 750, loss is 3.344972376823425 and perplexity is 28.359792268284178
At time: 174.35906052589417 and batch: 800, loss is 3.2851686906814574 and perplexity is 26.713490251695983
At time: 175.24871158599854 and batch: 850, loss is 3.3469912576675416 and perplexity is 28.417105144104596
At time: 176.13960313796997 and batch: 900, loss is 3.2970464038848877 and perplexity is 27.032677280995827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32954971104452 and perplexity of 75.91009739379516
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.25316166877747 and batch: 50, loss is 3.645529489517212 and perplexity is 38.303048545882774
At time: 179.14344668388367 and batch: 100, loss is 3.528767943382263 and perplexity is 34.08195084346672
At time: 180.0339994430542 and batch: 150, loss is 3.5456296491622923 and perplexity is 34.661503059151435
At time: 180.9347779750824 and batch: 200, loss is 3.418514680862427 and perplexity is 30.524043387914002
At time: 181.8245780467987 and batch: 250, loss is 3.5629235315322876 and perplexity is 35.26614827785056
At time: 182.71198654174805 and batch: 300, loss is 3.5223738670349123 and perplexity is 33.86472347200287
At time: 183.60599374771118 and batch: 350, loss is 3.5069253063201904 and perplexity is 33.345582550105405
At time: 184.49515461921692 and batch: 400, loss is 3.4402508211135863 and perplexity is 31.194781496973594
At time: 185.3937373161316 and batch: 450, loss is 3.4630357933044436 and perplexity is 31.91371304069359
At time: 186.28616547584534 and batch: 500, loss is 3.334141845703125 and perplexity is 28.05429797449643
At time: 187.177237033844 and batch: 550, loss is 3.3708969831466673 and perplexity is 29.104621707978943
At time: 188.06960916519165 and batch: 600, loss is 3.4095732593536376 and perplexity is 30.25233160487574
At time: 188.9610629081726 and batch: 650, loss is 3.233659977912903 and perplexity is 25.37234947991874
At time: 189.8516263961792 and batch: 700, loss is 3.218330392837524 and perplexity is 24.98636791725833
At time: 190.74112010002136 and batch: 750, loss is 3.323032603263855 and perplexity is 27.744360748303173
At time: 191.63084983825684 and batch: 800, loss is 3.261332006454468 and perplexity is 26.08425841347293
At time: 192.52016854286194 and batch: 850, loss is 3.3218323230743407 and perplexity is 27.71107971900192
At time: 193.40779972076416 and batch: 900, loss is 3.275168423652649 and perplexity is 26.447679520258333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324278112960188 and perplexity of 75.51098277998344
finished 11 epochs...
Completing Train Step...
At time: 195.51299595832825 and batch: 50, loss is 3.6352431631088256 and perplexity is 37.91107035014971
At time: 196.4163203239441 and batch: 100, loss is 3.5133730602264404 and perplexity is 33.561281299195684
At time: 197.30770587921143 and batch: 150, loss is 3.5273241329193117 and perplexity is 34.03277847257493
At time: 198.20027494430542 and batch: 200, loss is 3.4013762950897215 and perplexity is 30.00536788300584
At time: 199.09278893470764 and batch: 250, loss is 3.5458342361450197 and perplexity is 34.668595076921626
At time: 199.9920470714569 and batch: 300, loss is 3.506790838241577 and perplexity is 33.341098935147954
At time: 200.88350462913513 and batch: 350, loss is 3.4923106145858767 and perplexity is 32.86179098376179
At time: 201.7754464149475 and batch: 400, loss is 3.4257836818695067 and perplexity is 30.746731067989185
At time: 202.66654348373413 and batch: 450, loss is 3.450390830039978 and perplexity is 31.512706014459592
At time: 203.55714321136475 and batch: 500, loss is 3.3225120973587035 and perplexity is 27.729923402386888
At time: 204.4467704296112 and batch: 550, loss is 3.360360369682312 and perplexity is 28.799567500411385
At time: 205.33843636512756 and batch: 600, loss is 3.4016431856155394 and perplexity is 30.01337710016204
At time: 206.23487496376038 and batch: 650, loss is 3.2271806478500364 and perplexity is 25.208485092170143
At time: 207.12555575370789 and batch: 700, loss is 3.2143232440948486 and perplexity is 24.886444162856346
At time: 208.01479744911194 and batch: 750, loss is 3.3216632270812987 and perplexity is 27.70639428261399
At time: 208.9047405719757 and batch: 800, loss is 3.262003011703491 and perplexity is 26.101766961292057
At time: 209.79783844947815 and batch: 850, loss is 3.325482702255249 and perplexity is 27.81242052113304
At time: 210.6885175704956 and batch: 900, loss is 3.2805938291549683 and perplexity is 26.591558855194698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324049022099743 and perplexity of 75.49368588532118
finished 12 epochs...
Completing Train Step...
At time: 212.79678750038147 and batch: 50, loss is 3.628257827758789 and perplexity is 37.64717159400561
At time: 213.6931734085083 and batch: 100, loss is 3.504988975524902 and perplexity is 33.281076943977276
At time: 214.5827991962433 and batch: 150, loss is 3.5180326080322266 and perplexity is 33.71802659132738
At time: 215.4732322692871 and batch: 200, loss is 3.3918937253952026 and perplexity is 29.722184665108063
At time: 216.3631625175476 and batch: 250, loss is 3.536230640411377 and perplexity is 34.33724552538656
At time: 217.25492811203003 and batch: 300, loss is 3.497576870918274 and perplexity is 33.0353060851103
At time: 218.14602398872375 and batch: 350, loss is 3.483247365951538 and perplexity is 32.56530200768548
At time: 219.03699374198914 and batch: 400, loss is 3.4170524644851685 and perplexity is 30.47944324724665
At time: 219.92451977729797 and batch: 450, loss is 3.4423603439331054 and perplexity is 31.26065705896268
At time: 220.81663060188293 and batch: 500, loss is 3.315325589179993 and perplexity is 27.531356437216953
At time: 221.70690202713013 and batch: 550, loss is 3.353835310935974 and perplexity is 28.61226039012247
At time: 222.59854793548584 and batch: 600, loss is 3.3963406610488893 and perplexity is 29.85465162552835
At time: 223.48959136009216 and batch: 650, loss is 3.222739191055298 and perplexity is 25.096770965151077
At time: 224.38026094436646 and batch: 700, loss is 3.2111655569076536 and perplexity is 24.807984497721463
At time: 225.27025771141052 and batch: 750, loss is 3.3198623943328855 and perplexity is 27.656544599392305
At time: 226.16152143478394 and batch: 800, loss is 3.261252174377441 and perplexity is 26.08217613606345
At time: 227.05787920951843 and batch: 850, loss is 3.3261144161224365 and perplexity is 27.829995563470803
At time: 227.9611611366272 and batch: 900, loss is 3.2818562889099123 and perplexity is 26.625150827865813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324680276113014 and perplexity of 75.54135662210189
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 230.07431268692017 and batch: 50, loss is 3.625397834777832 and perplexity is 37.53965476946232
At time: 230.96346807479858 and batch: 100, loss is 3.5049898862838744 and perplexity is 33.281107255030506
At time: 231.84998965263367 and batch: 150, loss is 3.519356565475464 and perplexity is 33.762697388196095
At time: 232.73826026916504 and batch: 200, loss is 3.39162663936615 and perplexity is 29.714247344851945
At time: 233.62680435180664 and batch: 250, loss is 3.536753854751587 and perplexity is 34.35521596543476
At time: 234.51686143875122 and batch: 300, loss is 3.4957276678085325 and perplexity is 32.97427354254221
At time: 235.40625977516174 and batch: 350, loss is 3.4816863250732424 and perplexity is 32.514505897754674
At time: 236.29220843315125 and batch: 400, loss is 3.4155474424362184 and perplexity is 30.433605515170765
At time: 237.18066263198853 and batch: 450, loss is 3.4401009464263916 and perplexity is 31.1901065391923
At time: 238.07067465782166 and batch: 500, loss is 3.311858444213867 and perplexity is 27.436066520769508
At time: 238.960875749588 and batch: 550, loss is 3.348561315536499 and perplexity is 28.46175668722993
At time: 239.85022735595703 and batch: 600, loss is 3.391401138305664 and perplexity is 29.70754750600291
At time: 240.75294589996338 and batch: 650, loss is 3.2171998167037965 and perplexity is 24.9581348888239
At time: 241.6420066356659 and batch: 700, loss is 3.2042079973220825 and perplexity is 24.635980525182827
At time: 242.53079628944397 and batch: 750, loss is 3.3109735107421874 and perplexity is 27.41179816669719
At time: 243.42091393470764 and batch: 800, loss is 3.250070581436157 and perplexity is 25.79216030066528
At time: 244.30877804756165 and batch: 850, loss is 3.315421905517578 and perplexity is 27.534008284343788
At time: 245.19625759124756 and batch: 900, loss is 3.2718799018859865 and perplexity is 26.36084860141341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323403554419949 and perplexity of 75.44497287408213
finished 14 epochs...
Completing Train Step...
At time: 247.29481410980225 and batch: 50, loss is 3.6218383216857912 and perplexity is 37.4062694111385
At time: 248.19102478027344 and batch: 100, loss is 3.500642623901367 and perplexity is 33.13673957885972
At time: 249.08093571662903 and batch: 150, loss is 3.5145816707611086 and perplexity is 33.601868339352414
At time: 249.97673416137695 and batch: 200, loss is 3.3876284408569335 and perplexity is 29.59568106911468
At time: 250.8663125038147 and batch: 250, loss is 3.5325452041625978 and perplexity is 34.21093070163111
At time: 251.75695323944092 and batch: 300, loss is 3.492038221359253 and perplexity is 32.85284087351314
At time: 252.6461787223816 and batch: 350, loss is 3.477992000579834 and perplexity is 32.394608368778286
At time: 253.53540110588074 and batch: 400, loss is 3.412237024307251 and perplexity is 30.333024130817428
At time: 254.42529344558716 and batch: 450, loss is 3.4371829891204833 and perplexity is 31.099227794574663
At time: 255.31670236587524 and batch: 500, loss is 3.3090306520462036 and perplexity is 27.358592618423785
At time: 256.2051920890808 and batch: 550, loss is 3.346477599143982 and perplexity is 28.40251220404732
At time: 257.0940451622009 and batch: 600, loss is 3.3899632692337036 and perplexity is 29.6648626372207
At time: 257.98122930526733 and batch: 650, loss is 3.216124691963196 and perplexity is 24.931316199826995
At time: 258.8719563484192 and batch: 700, loss is 3.20359489440918 and perplexity is 24.620880763087683
At time: 259.7674422264099 and batch: 750, loss is 3.311042461395264 and perplexity is 27.413688293244764
At time: 260.6640770435333 and batch: 800, loss is 3.2506674480438233 and perplexity is 25.8075593750278
At time: 261.5540294647217 and batch: 850, loss is 3.3167415142059324 and perplexity is 27.570366384857632
At time: 262.4441592693329 and batch: 900, loss is 3.2738876390457152 and perplexity is 26.41382742269002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3230306546982025 and perplexity of 75.41684470950852
finished 15 epochs...
Completing Train Step...
At time: 264.5613543987274 and batch: 50, loss is 3.6195882081985475 and perplexity is 37.32219568301599
At time: 265.4583044052124 and batch: 100, loss is 3.4980226373672485 and perplexity is 33.05003539886769
At time: 266.35004234313965 and batch: 150, loss is 3.5116028594970703 and perplexity is 33.50192364754069
At time: 267.2422697544098 and batch: 200, loss is 3.384890065193176 and perplexity is 29.514747839690116
At time: 268.13155579566956 and batch: 250, loss is 3.529658889770508 and perplexity is 34.11232956537181
At time: 269.0195138454437 and batch: 300, loss is 3.4894373273849486 and perplexity is 32.767505140349044
At time: 269.91864562034607 and batch: 350, loss is 3.475446147918701 and perplexity is 32.312241360477465
At time: 270.81272625923157 and batch: 400, loss is 3.4099186992645265 and perplexity is 30.262783772803548
At time: 271.70447039604187 and batch: 450, loss is 3.4350702381134033 and perplexity is 31.033592229949775
At time: 272.60228729248047 and batch: 500, loss is 3.3070745706558227 and perplexity is 27.30512929089292
At time: 273.494104385376 and batch: 550, loss is 3.344828906059265 and perplexity is 28.355723759079066
At time: 274.38403725624084 and batch: 600, loss is 3.3887658309936524 and perplexity is 29.629362055432015
At time: 275.27518129348755 and batch: 650, loss is 3.2151824617385865 and perplexity is 24.90783622367295
At time: 276.16729974746704 and batch: 700, loss is 3.2030294227600096 and perplexity is 24.606962288660775
At time: 277.06023049354553 and batch: 750, loss is 3.3109715938568116 and perplexity is 27.411745621472523
At time: 277.9516634941101 and batch: 800, loss is 3.250888981819153 and perplexity is 25.813277254416512
At time: 278.8423275947571 and batch: 850, loss is 3.3174428224563597 and perplexity is 27.589708491867487
At time: 279.7304861545563 and batch: 900, loss is 3.2748642587661743 and perplexity is 26.439636288115945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323005989806293 and perplexity of 75.4149845841256
finished 16 epochs...
Completing Train Step...
At time: 281.83422017097473 and batch: 50, loss is 3.6176709270477296 and perplexity is 37.25070709448659
At time: 282.7250304222107 and batch: 100, loss is 3.495849781036377 and perplexity is 32.97830038338058
At time: 283.61314368247986 and batch: 150, loss is 3.5092214822769163 and perplexity is 33.42223784836646
At time: 284.5009865760803 and batch: 200, loss is 3.3825824213027955 and perplexity is 29.44671683801442
At time: 285.3892698287964 and batch: 250, loss is 3.527284235954285 and perplexity is 34.03142069508823
At time: 286.28330659866333 and batch: 300, loss is 3.4872324085235595 and perplexity is 32.6953350440666
At time: 287.1740572452545 and batch: 350, loss is 3.47328236579895 and perplexity is 32.24240029804302
At time: 288.06368494033813 and batch: 400, loss is 3.407934250831604 and perplexity is 30.202788387532536
At time: 288.9549069404602 and batch: 450, loss is 3.433246569633484 and perplexity is 30.97704881986955
At time: 289.8445363044739 and batch: 500, loss is 3.30541937828064 and perplexity is 27.259971431868994
At time: 290.73486828804016 and batch: 550, loss is 3.343371648788452 and perplexity is 28.314432267932048
At time: 291.6258533000946 and batch: 600, loss is 3.3876504945755004 and perplexity is 29.596333771133
At time: 292.51629161834717 and batch: 650, loss is 3.2142665147781373 and perplexity is 24.885032411927828
At time: 293.4061768054962 and batch: 700, loss is 3.202424168586731 and perplexity is 24.592073328301076
At time: 294.3030400276184 and batch: 750, loss is 3.31073823928833 and perplexity is 27.40534971168811
At time: 295.1926198005676 and batch: 800, loss is 3.2508765935897825 and perplexity is 25.812957475597834
At time: 296.08284425735474 and batch: 850, loss is 3.3177845954895018 and perplexity is 27.599139521766304
At time: 296.9727864265442 and batch: 900, loss is 3.275360040664673 and perplexity is 26.452747831154767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323123879628639 and perplexity of 75.42387576734016
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 299.0729308128357 and batch: 50, loss is 3.61675621509552 and perplexity is 37.21664900652377
At time: 299.96720933914185 and batch: 100, loss is 3.495686078071594 and perplexity is 32.97290217969735
At time: 300.85997581481934 and batch: 150, loss is 3.5092800951004026 and perplexity is 33.42419687750554
At time: 301.74408411979675 and batch: 200, loss is 3.3824387264251707 and perplexity is 29.442485799638483
At time: 302.6330831050873 and batch: 250, loss is 3.5275088119506837 and perplexity is 34.03906419354071
At time: 303.5230894088745 and batch: 300, loss is 3.486489806175232 and perplexity is 32.67106442430849
At time: 304.41421937942505 and batch: 350, loss is 3.4728894567489625 and perplexity is 32.22973445559761
At time: 305.30908489227295 and batch: 400, loss is 3.407789421081543 and perplexity is 30.198414441985598
At time: 306.20305728912354 and batch: 450, loss is 3.433083653450012 and perplexity is 30.972002568369778
At time: 307.092139005661 and batch: 500, loss is 3.304579095840454 and perplexity is 27.237074977646262
At time: 307.9820828437805 and batch: 550, loss is 3.341398711204529 and perplexity is 28.25862473083938
At time: 308.8778076171875 and batch: 600, loss is 3.3855335330963134 and perplexity is 29.533745744209533
At time: 309.768750667572 and batch: 650, loss is 3.212291579246521 and perplexity is 24.835934575701042
At time: 310.65893959999084 and batch: 700, loss is 3.1997967338562012 and perplexity is 24.52754407107103
At time: 311.54891896247864 and batch: 750, loss is 3.307942442893982 and perplexity is 27.328836940681327
At time: 312.43699073791504 and batch: 800, loss is 3.2473167753219605 and perplexity is 25.72123139897634
At time: 313.32482409477234 and batch: 850, loss is 3.3144336843490603 and perplexity is 27.506812034671153
At time: 314.21443152427673 and batch: 900, loss is 3.2717397165298463 and perplexity is 26.357153455472787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322435352900257 and perplexity of 75.3719622868765
finished 18 epochs...
Completing Train Step...
At time: 316.32018518447876 and batch: 50, loss is 3.615918312072754 and perplexity is 37.185478124735724
At time: 317.21588492393494 and batch: 100, loss is 3.4946174669265746 and perplexity is 32.937685788609706
At time: 318.1067008972168 and batch: 150, loss is 3.5083398389816285 and perplexity is 33.392784342104925
At time: 318.99652004241943 and batch: 200, loss is 3.3815963554382322 and perplexity is 29.41769474691525
At time: 319.88698291778564 and batch: 250, loss is 3.526682262420654 and perplexity is 34.01094084530015
At time: 320.7813711166382 and batch: 300, loss is 3.4857309436798096 and perplexity is 32.646280983632515
At time: 321.6706008911133 and batch: 350, loss is 3.472029709815979 and perplexity is 32.20203694837806
At time: 322.5591011047363 and batch: 400, loss is 3.4069416999816893 and perplexity is 30.17282545657484
At time: 323.44765305519104 and batch: 450, loss is 3.432339367866516 and perplexity is 30.948959129879434
At time: 324.33799958229065 and batch: 500, loss is 3.3039831733703613 and perplexity is 27.22084862795087
At time: 325.2268407344818 and batch: 550, loss is 3.3410497283935547 and perplexity is 28.248764677141246
At time: 326.11580181121826 and batch: 600, loss is 3.3853222513198853 and perplexity is 29.527506461090738
At time: 327.00580072402954 and batch: 650, loss is 3.2121203327178955 and perplexity is 24.83168187226053
At time: 327.8955731391907 and batch: 700, loss is 3.199761357307434 and perplexity is 24.526676386559988
At time: 328.786465883255 and batch: 750, loss is 3.3079393243789674 and perplexity is 27.328751715425884
At time: 329.67604970932007 and batch: 800, loss is 3.247452583312988 and perplexity is 25.72472478494889
At time: 330.56622552871704 and batch: 850, loss is 3.314737434387207 and perplexity is 27.515168498949905
At time: 331.45651292800903 and batch: 900, loss is 3.2722797775268555 and perplexity is 26.371391770479505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322115545403467 and perplexity of 75.34786162228143
finished 19 epochs...
Completing Train Step...
At time: 333.5784864425659 and batch: 50, loss is 3.6152803707122803 and perplexity is 37.16176353529394
At time: 334.4691526889801 and batch: 100, loss is 3.493824310302734 and perplexity is 32.91157140272061
At time: 335.35982394218445 and batch: 150, loss is 3.507562313079834 and perplexity is 33.36683067847628
At time: 336.25155544281006 and batch: 200, loss is 3.3809053611755373 and perplexity is 29.39737431008459
At time: 337.14158749580383 and batch: 250, loss is 3.525971794128418 and perplexity is 33.98678573197755
At time: 338.0321776866913 and batch: 300, loss is 3.4850891637802124 and perplexity is 32.62533597846118
At time: 338.92904448509216 and batch: 350, loss is 3.471337423324585 and perplexity is 32.179751628006485
At time: 339.8203706741333 and batch: 400, loss is 3.4063054752349853 and perplexity is 30.15363486375375
At time: 340.7113480567932 and batch: 450, loss is 3.4317541456222536 and perplexity is 30.930852409304453
At time: 341.602313041687 and batch: 500, loss is 3.303485951423645 and perplexity is 27.207317188947812
At time: 342.4934821128845 and batch: 550, loss is 3.340699453353882 and perplexity is 28.238871572728172
At time: 343.38473105430603 and batch: 600, loss is 3.3850958442687986 and perplexity is 29.520821982162076
At time: 344.2731091976166 and batch: 650, loss is 3.211945958137512 and perplexity is 24.82735223565409
At time: 345.15849208831787 and batch: 700, loss is 3.199687261581421 and perplexity is 24.52485913199267
At time: 346.0453827381134 and batch: 750, loss is 3.3079376077651976 and perplexity is 27.328704802554643
At time: 346.9319055080414 and batch: 800, loss is 3.2475413131713866 and perplexity is 25.727007437404616
At time: 347.8183207511902 and batch: 850, loss is 3.314965672492981 and perplexity is 27.52144922561146
At time: 348.70465540885925 and batch: 900, loss is 3.272643389701843 and perplexity is 26.380982473141078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321963375561858 and perplexity of 75.33639682243329
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
1372.6639063358307


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.99318942953225, 'params': {'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -74.45589980217576, 'params': {'rnn_dropout': 0.1787797272182029, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.29506562024038585, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -75.33639682243329, 'params': {'rnn_dropout': 0.8453812474725898, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.79552321001791, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.5539506876158059, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8281496471240387, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.3159542083740234 and batch: 50, loss is 7.0693318557739255 and perplexity is 1175.3624601986883
At time: 2.4308228492736816 and batch: 100, loss is 6.533851909637451 and perplexity is 688.0433947009003
At time: 3.5474815368652344 and batch: 150, loss is 6.220878295898437 and perplexity is 503.14494815739414
At time: 4.666845798492432 and batch: 200, loss is 6.024125366210938 and perplexity is 413.2800152349093
At time: 5.7878265380859375 and batch: 250, loss is 6.074632425308227 and perplexity is 434.68969221738314
At time: 6.904699087142944 and batch: 300, loss is 5.984558115005493 and perplexity is 397.24694499584535
At time: 8.026502847671509 and batch: 350, loss is 5.975986013412475 and perplexity is 393.8562572482936
At time: 9.146430969238281 and batch: 400, loss is 5.831107568740845 and perplexity is 340.7358585429024
At time: 10.26644253730774 and batch: 450, loss is 5.825985708236694 and perplexity is 338.99511872540995
At time: 11.387325048446655 and batch: 500, loss is 5.778361129760742 and perplexity is 323.22902566965894
At time: 12.50758981704712 and batch: 550, loss is 5.824583129882813 and perplexity is 338.51998479398907
At time: 13.631240129470825 and batch: 600, loss is 5.743671770095825 and perplexity is 312.2086674212369
At time: 14.74762511253357 and batch: 650, loss is 5.6540765380859375 and perplexity is 285.4527562259264
At time: 15.866046905517578 and batch: 700, loss is 5.753560218811035 and perplexity is 315.3112413579731
At time: 16.98381471633911 and batch: 750, loss is 5.694279165267944 and perplexity is 297.16251130511756
At time: 18.096168756484985 and batch: 800, loss is 5.691066255569458 and perplexity is 296.2092871221243
At time: 19.213521718978882 and batch: 850, loss is 5.720637035369873 and perplexity is 305.09922005777264
At time: 20.336305618286133 and batch: 900, loss is 5.600150241851806 and perplexity is 270.46703984264997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.530290159460616 and perplexity of 252.21708358007436
finished 1 epochs...
Completing Train Step...
At time: 22.61106514930725 and batch: 50, loss is 5.361839933395386 and perplexity is 213.1167064782102
At time: 23.506669282913208 and batch: 100, loss is 5.184442558288574 and perplexity is 178.4739332402745
At time: 24.39792799949646 and batch: 150, loss is 5.1331500053405765 and perplexity is 169.55036228955896
At time: 25.28954029083252 and batch: 200, loss is 4.977813949584961 and perplexity is 145.15671466574992
At time: 26.180202960968018 and batch: 250, loss is 5.033732271194458 and perplexity is 153.5048666668125
At time: 27.07021403312683 and batch: 300, loss is 4.947677669525146 and perplexity is 140.84748939968492
At time: 27.973068475723267 and batch: 350, loss is 4.920939874649048 and perplexity is 137.1314390000311
At time: 28.863413333892822 and batch: 400, loss is 4.772466354370117 and perplexity is 118.21043153913944
At time: 29.753833293914795 and batch: 450, loss is 4.778060131072998 and perplexity is 118.87352717253387
At time: 30.643354415893555 and batch: 500, loss is 4.680814762115478 and perplexity is 107.85791532432815
At time: 31.530275344848633 and batch: 550, loss is 4.735157690048218 and perplexity is 113.88141526825302
At time: 32.418946743011475 and batch: 600, loss is 4.664919309616089 and perplexity is 106.15701902575489
At time: 33.309751749038696 and batch: 650, loss is 4.53328688621521 and perplexity is 93.0639495314758
At time: 34.19950985908508 and batch: 700, loss is 4.582794198989868 and perplexity is 97.78724985780902
At time: 35.09033679962158 and batch: 750, loss is 4.609206647872925 and perplexity is 100.40446193692057
At time: 35.980313777923584 and batch: 800, loss is 4.556592702865601 and perplexity is 95.2583527300151
At time: 36.9045193195343 and batch: 850, loss is 4.604910097122192 and perplexity is 99.97399449542779
At time: 37.79887366294861 and batch: 900, loss is 4.5307433223724365 and perplexity is 92.82753622800774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.627119508508134 and perplexity of 102.21919810441837
finished 2 epochs...
Completing Train Step...
At time: 39.9079213142395 and batch: 50, loss is 4.579030885696411 and perplexity is 97.41993738994107
At time: 40.81510281562805 and batch: 100, loss is 4.451251063346863 and perplexity is 85.73413577121124
At time: 41.71603059768677 and batch: 150, loss is 4.445610542297363 and perplexity is 85.25191184933867
At time: 42.60714530944824 and batch: 200, loss is 4.333256301879882 and perplexity is 76.19198716739625
At time: 43.49830770492554 and batch: 250, loss is 4.467068490982055 and perplexity is 87.10101098633271
At time: 44.38947510719299 and batch: 300, loss is 4.423880066871643 and perplexity is 83.41933081602329
At time: 45.289429903030396 and batch: 350, loss is 4.421674013137817 and perplexity is 83.23550612792437
At time: 46.192447662353516 and batch: 400, loss is 4.320561294555664 and perplexity is 75.23084310609558
At time: 47.094093799591064 and batch: 450, loss is 4.354782400131225 and perplexity is 77.84988336920468
At time: 47.988670110702515 and batch: 500, loss is 4.225860991477966 and perplexity is 68.43339876392459
At time: 48.88993310928345 and batch: 550, loss is 4.299892325401306 and perplexity is 73.69185853110105
At time: 49.78198528289795 and batch: 600, loss is 4.286071310043335 and perplexity is 72.68036823526951
At time: 50.67246127128601 and batch: 650, loss is 4.137233324050904 and perplexity is 62.62930653401485
At time: 51.57630491256714 and batch: 700, loss is 4.161306414604187 and perplexity is 64.15528127263761
At time: 52.47580146789551 and batch: 750, loss is 4.242329077720642 and perplexity is 69.56969652137079
At time: 53.3850576877594 and batch: 800, loss is 4.200969252586365 and perplexity is 66.75099827414063
At time: 54.27351355552673 and batch: 850, loss is 4.271292386054992 and perplexity is 71.61412893938832
At time: 55.16134715080261 and batch: 900, loss is 4.2031253767013546 and perplexity is 66.89507698118956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429820857635916 and perplexity of 83.91638258450517
finished 3 epochs...
Completing Train Step...
At time: 57.26723027229309 and batch: 50, loss is 4.28633665561676 and perplexity is 72.69965620813163
At time: 58.16169285774231 and batch: 100, loss is 4.15904366016388 and perplexity is 64.01027774063058
At time: 59.049604415893555 and batch: 150, loss is 4.155287351608276 and perplexity is 63.770286409638665
At time: 59.94379639625549 and batch: 200, loss is 4.041154432296753 and perplexity is 56.89198285211495
At time: 60.831934690475464 and batch: 250, loss is 4.18983099937439 and perplexity is 66.01163401022544
At time: 61.71972990036011 and batch: 300, loss is 4.154638962745667 and perplexity is 63.72895186804155
At time: 62.6096510887146 and batch: 350, loss is 4.155006055831909 and perplexity is 63.752350620163085
At time: 63.49930500984192 and batch: 400, loss is 4.067915897369385 and perplexity is 58.435050954422735
At time: 64.39549493789673 and batch: 450, loss is 4.111800489425659 and perplexity is 61.05655033683193
At time: 65.29232978820801 and batch: 500, loss is 3.9719918823242186 and perplexity is 53.090174970521744
At time: 66.18323421478271 and batch: 550, loss is 4.050437173843384 and perplexity is 57.4225551980714
At time: 67.07477807998657 and batch: 600, loss is 4.057155151367187 and perplexity is 57.80961731483734
At time: 67.96878385543823 and batch: 650, loss is 3.9027710342407227 and perplexity is 49.53953483047682
At time: 68.87092995643616 and batch: 700, loss is 3.9169134664535523 and perplexity is 50.24512194238288
At time: 69.7766764163971 and batch: 750, loss is 4.0145669221878055 and perplexity is 55.39929799972276
At time: 70.66999650001526 and batch: 800, loss is 3.9809066677093505 and perplexity is 53.56557839771578
At time: 71.57119560241699 and batch: 850, loss is 4.053055062294006 and perplexity is 57.57307798209327
At time: 72.47369313240051 and batch: 900, loss is 3.9926029729843138 and perplexity is 54.19577606259765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.35164903614619 and perplexity of 77.6063331134804
finished 4 epochs...
Completing Train Step...
At time: 74.59667801856995 and batch: 50, loss is 4.083760886192322 and perplexity is 59.368328040244194
At time: 75.48804903030396 and batch: 100, loss is 3.9583144998550415 and perplexity is 52.36898359005159
At time: 76.37905597686768 and batch: 150, loss is 3.959912271499634 and perplexity is 52.452724148417815
At time: 77.27207064628601 and batch: 200, loss is 3.840213351249695 and perplexity is 46.535401766869754
At time: 78.1646077632904 and batch: 250, loss is 3.994980397224426 and perplexity is 54.32477569698907
At time: 79.05617785453796 and batch: 300, loss is 3.9646025848388673 and perplexity is 52.699321718029715
At time: 79.9484269618988 and batch: 350, loss is 3.9633574056625367 and perplexity is 52.633742457457735
At time: 80.84215474128723 and batch: 400, loss is 3.8853501796722414 and perplexity is 48.68398761770965
At time: 81.74058699607849 and batch: 450, loss is 3.9291455078125 and perplexity is 50.863496634241876
At time: 82.63147401809692 and batch: 500, loss is 3.7947750949859618 and perplexity is 44.46823415196621
At time: 83.5258355140686 and batch: 550, loss is 3.8679071617126466 and perplexity is 47.84215533959262
At time: 84.42178559303284 and batch: 600, loss is 3.887245411872864 and perplexity is 48.77634256809118
At time: 85.31562519073486 and batch: 650, loss is 3.7280306959152223 and perplexity is 41.59711009214106
At time: 86.21951937675476 and batch: 700, loss is 3.7413690996170046 and perplexity is 42.15566598016757
At time: 87.11427903175354 and batch: 750, loss is 3.846445293426514 and perplexity is 46.82631323005188
At time: 88.00872421264648 and batch: 800, loss is 3.8116859340667726 and perplexity is 45.22662373261842
At time: 88.90399408340454 and batch: 850, loss is 3.8847243785858154 and perplexity is 48.65353065636067
At time: 89.79859113693237 and batch: 900, loss is 3.8292386627197263 and perplexity is 46.02748245296261
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332455151701627 and perplexity of 76.13097038836074
finished 5 epochs...
Completing Train Step...
At time: 91.91581988334656 and batch: 50, loss is 3.9247667837142943 and perplexity is 50.64126631351341
At time: 92.81218767166138 and batch: 100, loss is 3.799345626831055 and perplexity is 44.67194280607956
At time: 93.7015130519867 and batch: 150, loss is 3.803342957496643 and perplexity is 44.850868707696904
At time: 94.60153985023499 and batch: 200, loss is 3.687582573890686 and perplexity is 39.9481584144995
At time: 95.49377989768982 and batch: 250, loss is 3.83875216960907 and perplexity is 46.467454745728006
At time: 96.38489103317261 and batch: 300, loss is 3.815830454826355 and perplexity is 45.41445538078054
At time: 97.2754898071289 and batch: 350, loss is 3.809229364395142 and perplexity is 45.11565773406412
At time: 98.16573071479797 and batch: 400, loss is 3.7367071533203124 and perplexity is 41.959595918427084
At time: 99.05609822273254 and batch: 450, loss is 3.7817711448669433 and perplexity is 43.893715058032086
At time: 99.94667983055115 and batch: 500, loss is 3.6526916360855104 and perplexity is 38.578365346325356
At time: 100.8377685546875 and batch: 550, loss is 3.717231092453003 and perplexity is 41.15029485438274
At time: 101.72807812690735 and batch: 600, loss is 3.7461367082595824 and perplexity is 42.357127561012014
At time: 102.61792206764221 and batch: 650, loss is 3.5843501043319703 and perplexity is 36.029934410037725
At time: 103.50722980499268 and batch: 700, loss is 3.5988214588165284 and perplexity is 36.55512732389125
At time: 104.41125130653381 and batch: 750, loss is 3.707436890602112 and perplexity is 40.749227832112005
At time: 105.30163264274597 and batch: 800, loss is 3.67238694190979 and perplexity is 39.345709787166506
At time: 106.19344329833984 and batch: 850, loss is 3.7471984148025514 and perplexity is 42.40212228185309
At time: 107.08682870864868 and batch: 900, loss is 3.696363677978516 and perplexity is 40.30049202674648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.340842887146832 and perplexity of 76.7722223936974
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 109.20933032035828 and batch: 50, loss is 3.819771523475647 and perplexity is 45.593790020049624
At time: 110.1099500656128 and batch: 100, loss is 3.705583486557007 and perplexity is 40.67377299416361
At time: 111.00245499610901 and batch: 150, loss is 3.712757120132446 and perplexity is 40.96660080168323
At time: 111.90472769737244 and batch: 200, loss is 3.5870434617996216 and perplexity is 36.127106704059806
At time: 112.79664063453674 and batch: 250, loss is 3.7264321184158327 and perplexity is 41.5306670092661
At time: 113.69144177436829 and batch: 300, loss is 3.693665943145752 and perplexity is 40.19191850276497
At time: 114.58739113807678 and batch: 350, loss is 3.6756795930862425 and perplexity is 39.475475003343426
At time: 115.47553491592407 and batch: 400, loss is 3.597929573059082 and perplexity is 36.52253886122253
At time: 116.3674578666687 and batch: 450, loss is 3.626132106781006 and perplexity is 37.56722920929902
At time: 117.25625586509705 and batch: 500, loss is 3.49715763092041 and perplexity is 33.02145926623072
At time: 118.14583969116211 and batch: 550, loss is 3.543940896987915 and perplexity is 34.603017767973874
At time: 119.03842759132385 and batch: 600, loss is 3.565435905456543 and perplexity is 35.354861422825564
At time: 119.93114638328552 and batch: 650, loss is 3.389286823272705 and perplexity is 29.64480274618722
At time: 120.82347226142883 and batch: 700, loss is 3.3870589923858643 and perplexity is 29.578832651387767
At time: 121.71476697921753 and batch: 750, loss is 3.4874697303771973 and perplexity is 32.70309528238522
At time: 122.6064612865448 and batch: 800, loss is 3.4265513801574707 and perplexity is 30.77034434356646
At time: 123.49952030181885 and batch: 850, loss is 3.4886545515060425 and perplexity is 32.74186556404339
At time: 124.39067602157593 and batch: 900, loss is 3.4329656028747557 and perplexity is 30.968346521453203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.302009059958262 and perplexity of 73.84800984175403
finished 7 epochs...
Completing Train Step...
At time: 126.50350308418274 and batch: 50, loss is 3.7266871118545533 and perplexity is 41.54125840717025
At time: 127.39430236816406 and batch: 100, loss is 3.5982624912261962 and perplexity is 36.534699902121964
At time: 128.2885673046112 and batch: 150, loss is 3.604930305480957 and perplexity is 36.77912046480639
At time: 129.18152594566345 and batch: 200, loss is 3.4823159742355347 and perplexity is 32.53498507582761
At time: 130.07449316978455 and batch: 250, loss is 3.62517849445343 and perplexity is 37.53142171236097
At time: 130.96647548675537 and batch: 300, loss is 3.5966673040390016 and perplexity is 36.47646667574685
At time: 131.86807322502136 and batch: 350, loss is 3.5805810260772706 and perplexity is 35.89439036624955
At time: 132.76405882835388 and batch: 400, loss is 3.5078690242767334 and perplexity is 33.3770662286491
At time: 133.66367197036743 and batch: 450, loss is 3.5424067211151122 and perplexity is 34.5499713546564
At time: 134.55648016929626 and batch: 500, loss is 3.416110053062439 and perplexity is 30.450732602516705
At time: 135.44837498664856 and batch: 550, loss is 3.4661496686935425 and perplexity is 32.01324324821539
At time: 136.3424620628357 and batch: 600, loss is 3.4941903734207154 and perplexity is 32.92362132054664
At time: 137.24151968955994 and batch: 650, loss is 3.324009575843811 and perplexity is 27.771479472970867
At time: 138.1440703868866 and batch: 700, loss is 3.3266077184677125 and perplexity is 27.84372755228396
At time: 139.0469515323639 and batch: 750, loss is 3.434549083709717 and perplexity is 31.017423150356414
At time: 139.949401140213 and batch: 800, loss is 3.377302632331848 and perplexity is 29.291654096710925
At time: 140.85750699043274 and batch: 850, loss is 3.446443109512329 and perplexity is 31.388547890077806
At time: 141.7496497631073 and batch: 900, loss is 3.4004235601425172 and perplexity is 29.97679433409065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313267537992295 and perplexity of 74.6841238925842
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 143.88620352745056 and batch: 50, loss is 3.6883375597000123 and perplexity is 39.97833009537345
At time: 144.78490352630615 and batch: 100, loss is 3.570072584152222 and perplexity is 35.51917118699332
At time: 145.67827343940735 and batch: 150, loss is 3.5825126504898073 and perplexity is 35.96379185417922
At time: 146.5793538093567 and batch: 200, loss is 3.4573914194107056 and perplexity is 31.734087525628265
At time: 147.47193360328674 and batch: 250, loss is 3.601697173118591 and perplexity is 36.66040072190988
At time: 148.37528681755066 and batch: 300, loss is 3.566632857322693 and perplexity is 35.39720482663577
At time: 149.26991200447083 and batch: 350, loss is 3.547183384895325 and perplexity is 34.71539973476782
At time: 150.17564344406128 and batch: 400, loss is 3.4708567523956297 and perplexity is 32.16428747378034
At time: 151.08035016059875 and batch: 450, loss is 3.5035434627532958 and perplexity is 33.23300347597874
At time: 151.97929859161377 and batch: 500, loss is 3.372629804611206 and perplexity is 29.155098542232295
At time: 152.87475490570068 and batch: 550, loss is 3.414811773300171 and perplexity is 30.411224684347236
At time: 153.76750254631042 and batch: 600, loss is 3.441520748138428 and perplexity is 31.23442175782252
At time: 154.66607308387756 and batch: 650, loss is 3.2630888986587525 and perplexity is 26.130125924071848
At time: 155.56441068649292 and batch: 700, loss is 3.256099271774292 and perplexity is 25.94812290100759
At time: 156.46448874473572 and batch: 750, loss is 3.3660037088394166 and perplexity is 28.962552685179123
At time: 157.37345051765442 and batch: 800, loss is 3.3006536102294923 and perplexity is 27.130365811412442
At time: 158.27025175094604 and batch: 850, loss is 3.367142524719238 and perplexity is 28.995554488019067
At time: 159.16965675354004 and batch: 900, loss is 3.321278204917908 and perplexity is 27.695728760110573
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315560536841824 and perplexity of 74.85557099162824
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 161.2740342617035 and batch: 50, loss is 3.67565158367157 and perplexity is 39.47436933387931
At time: 162.16845154762268 and batch: 100, loss is 3.553024621009827 and perplexity is 34.91877398166467
At time: 163.07005786895752 and batch: 150, loss is 3.5647488117218016 and perplexity is 35.330577662614395
At time: 163.96831035614014 and batch: 200, loss is 3.4399332427978515 and perplexity is 31.184876283729793
At time: 164.86240243911743 and batch: 250, loss is 3.583267207145691 and perplexity is 35.990938813364956
At time: 165.75543975830078 and batch: 300, loss is 3.5497824001312255 and perplexity is 34.80574293842545
At time: 166.64693927764893 and batch: 350, loss is 3.530665650367737 and perplexity is 34.146689808024924
At time: 167.53870129585266 and batch: 400, loss is 3.455004949569702 and perplexity is 31.65844537756912
At time: 168.42946100234985 and batch: 450, loss is 3.485052399635315 and perplexity is 32.624136557929916
At time: 169.31891584396362 and batch: 500, loss is 3.3555619764328 and perplexity is 28.661706869404288
At time: 170.20996022224426 and batch: 550, loss is 3.3945968198776244 and perplexity is 29.802635222224563
At time: 171.1146047115326 and batch: 600, loss is 3.4247476768493654 and perplexity is 30.71489379488479
At time: 172.01940441131592 and batch: 650, loss is 3.241383295059204 and perplexity is 25.569066859210942
At time: 172.91195845603943 and batch: 700, loss is 3.2340215396881105 and perplexity is 25.38152481026229
At time: 173.80325293540955 and batch: 750, loss is 3.341163558959961 and perplexity is 28.251980433046906
At time: 174.70629978179932 and batch: 800, loss is 3.27655339717865 and perplexity is 26.484334233262498
At time: 175.59595322608948 and batch: 850, loss is 3.339415011405945 and perplexity is 28.20262366566739
At time: 176.4942193031311 and batch: 900, loss is 3.2957411432266235 and perplexity is 26.99741560869648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.311327372511772 and perplexity of 74.53936480716398
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 178.61834597587585 and batch: 50, loss is 3.668800449371338 and perplexity is 39.20484944087489
At time: 179.51189422607422 and batch: 100, loss is 3.545106973648071 and perplexity is 34.643391073974236
At time: 180.40365624427795 and batch: 150, loss is 3.5574402093887327 and perplexity is 35.07330182887247
At time: 181.30323338508606 and batch: 200, loss is 3.432854971885681 and perplexity is 30.964920652154678
At time: 182.20447611808777 and batch: 250, loss is 3.5770634317398073 and perplexity is 35.768350270876184
At time: 183.09476327896118 and batch: 300, loss is 3.543271737098694 and perplexity is 34.57987056189219
At time: 183.9952700138092 and batch: 350, loss is 3.5230787134170534 and perplexity is 33.88860131394101
At time: 184.89798855781555 and batch: 400, loss is 3.448104019165039 and perplexity is 31.44072475078411
At time: 185.80416631698608 and batch: 450, loss is 3.477959632873535 and perplexity is 32.393559846578164
At time: 186.70530819892883 and batch: 500, loss is 3.349288640022278 and perplexity is 28.482465149749867
At time: 187.59627199172974 and batch: 550, loss is 3.387591395378113 and perplexity is 29.594584703246277
At time: 188.49640250205994 and batch: 600, loss is 3.4173051214218138 and perplexity is 30.487145062925755
At time: 189.38740944862366 and batch: 650, loss is 3.2335922574996947 and perplexity is 25.37063131210607
At time: 190.2774658203125 and batch: 700, loss is 3.2274563312530518 and perplexity is 25.215435611152554
At time: 191.17778825759888 and batch: 750, loss is 3.332855362892151 and perplexity is 28.018229807890986
At time: 192.07198810577393 and batch: 800, loss is 3.2693092393875123 and perplexity is 26.293170782171387
At time: 192.97095274925232 and batch: 850, loss is 3.3300460720062257 and perplexity is 27.939628908341227
At time: 193.8575792312622 and batch: 900, loss is 3.287235059738159 and perplexity is 26.76874725236503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.308153126337757 and perplexity of 74.303133639903
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 195.94995307922363 and batch: 50, loss is 3.6649669408798218 and perplexity is 39.05484502294234
At time: 196.84982776641846 and batch: 100, loss is 3.542097039222717 and perplexity is 34.53927351069487
At time: 197.7475197315216 and batch: 150, loss is 3.5548699951171874 and perplexity is 34.98327167591528
At time: 198.64126920700073 and batch: 200, loss is 3.430269660949707 and perplexity is 30.884970097214445
At time: 199.53248000144958 and batch: 250, loss is 3.575362753868103 and perplexity is 35.707571526245395
At time: 200.4300787448883 and batch: 300, loss is 3.5412807178497316 and perplexity is 34.51108986854051
At time: 201.33586049079895 and batch: 350, loss is 3.520108599662781 and perplexity is 33.788097640620045
At time: 202.25254583358765 and batch: 400, loss is 3.4457762098312377 and perplexity is 31.367621856058282
At time: 203.15534043312073 and batch: 450, loss is 3.475980181694031 and perplexity is 32.329501797138434
At time: 204.05535364151 and batch: 500, loss is 3.3466023302078245 and perplexity is 28.40605510056036
At time: 204.9535415172577 and batch: 550, loss is 3.3855118942260742 and perplexity is 29.533106674232098
At time: 205.84466123580933 and batch: 600, loss is 3.415090675354004 and perplexity is 30.41970762027042
At time: 206.7379231452942 and batch: 650, loss is 3.2316598415374758 and perplexity is 25.321652038593623
At time: 207.63128542900085 and batch: 700, loss is 3.2255280447006225 and perplexity is 25.166859874771095
At time: 208.5237522125244 and batch: 750, loss is 3.3304132556915285 and perplexity is 27.94988976794462
At time: 209.4140341281891 and batch: 800, loss is 3.2668789339065554 and perplexity is 26.22934793103373
At time: 210.30795621871948 and batch: 850, loss is 3.327547197341919 and perplexity is 27.869898437670543
At time: 211.20164012908936 and batch: 900, loss is 3.284720525741577 and perplexity is 26.70152088426909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306922808085402 and perplexity of 74.21177335101837
Annealing...
Model not improving. Stopping early with 73.84800984175403 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f112a5ceb70>
ELAPSED
1591.101794242859


RESULTS SO FAR:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.99318942953225, 'params': {'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -74.45589980217576, 'params': {'rnn_dropout': 0.1787797272182029, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.29506562024038585, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -75.33639682243329, 'params': {'rnn_dropout': 0.8453812474725898, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.79552321001791, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.84800984175403, 'params': {'rnn_dropout': 0.5539506876158059, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8281496471240387, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -73.86548551087895, 'params': {'rnn_dropout': 0.012279170854736288, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.4899017729269186, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.94151880378699, 'params': {'rnn_dropout': 0.6274824676074374, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8541924201991064, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.99318942953225, 'params': {'rnn_dropout': 0.5528376327954642, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.7369485087726723, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -74.45589980217576, 'params': {'rnn_dropout': 0.1787797272182029, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.29506562024038585, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -75.33639682243329, 'params': {'rnn_dropout': 0.8453812474725898, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.79552321001791, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}, {'best_accuracy': -73.84800984175403, 'params': {'rnn_dropout': 0.5539506876158059, 'tie_weights': 'TRUE', 'data': 'ptb', 'batch_size': 32, 'dropout': 0.8281496471240387, 'tune_wordvecs': 'FALSE', 'wordvec_source': 'gigavec', 'wordvec_dim': 300, 'num_layers': 3, 'seq_len': 35}}]
