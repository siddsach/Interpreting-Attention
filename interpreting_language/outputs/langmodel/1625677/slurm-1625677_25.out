FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'rnn_dropout', 'domain': [0, 1], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0708227157592773 and batch: 50, loss is 6.855498285293579 and perplexity is 949.0849266626308
At time: 1.7081358432769775 and batch: 100, loss is 6.045483264923096 and perplexity is 422.20174348224714
At time: 2.3560383319854736 and batch: 150, loss is 5.96407603263855 and perplexity is 389.1932600199122
At time: 2.9941563606262207 and batch: 200, loss is 5.812004041671753 and perplexity is 334.28838269039966
At time: 3.6323695182800293 and batch: 250, loss is 5.871857452392578 and perplexity is 354.9075923646816
At time: 4.271556615829468 and batch: 300, loss is 5.791064786911011 and perplexity is 327.36140900883936
At time: 4.910627365112305 and batch: 350, loss is 5.787364845275879 and perplexity is 326.1524288591092
At time: 5.54929518699646 and batch: 400, loss is 5.652043647766114 and perplexity is 284.8730515187142
At time: 6.186013698577881 and batch: 450, loss is 5.658282251358032 and perplexity is 286.65581676186014
At time: 6.821913957595825 and batch: 500, loss is 5.61507869720459 and perplexity is 274.53498348103324
At time: 7.45891261100769 and batch: 550, loss is 5.665064144134521 and perplexity is 288.60649293660316
At time: 8.095557689666748 and batch: 600, loss is 5.604305410385132 and perplexity is 271.59321407745506
At time: 8.732977867126465 and batch: 650, loss is 5.505068330764771 and perplexity is 245.9352600490909
At time: 9.371669054031372 and batch: 700, loss is 5.614026765823365 and perplexity is 274.24634335808355
At time: 10.009887933731079 and batch: 750, loss is 5.581969032287597 and perplexity is 265.59405452696654
At time: 10.64990234375 and batch: 800, loss is 5.56168384552002 and perplexity is 260.2607063912094
At time: 11.289185762405396 and batch: 850, loss is 5.603206396102905 and perplexity is 271.2948932156987
At time: 11.928471088409424 and batch: 900, loss is 5.512262029647827 and perplexity is 247.71082303032196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.307211679955051 and perplexity of 201.78679706869764
finished 1 epochs...
Completing Train Step...
At time: 13.515772342681885 and batch: 50, loss is 5.218430843353271 and perplexity is 184.64422092483724
At time: 14.145453929901123 and batch: 100, loss is 5.041610174179077 and perplexity is 154.71893900722952
At time: 14.781339406967163 and batch: 150, loss is 4.988737754821777 and perplexity is 146.75107070857257
At time: 15.412319421768188 and batch: 200, loss is 4.855359315872192 and perplexity is 128.42682872322973
At time: 16.04228162765503 and batch: 250, loss is 4.93863733291626 and perplexity is 139.57991894972167
At time: 16.67367911338806 and batch: 300, loss is 4.880268573760986 and perplexity is 131.66602117609884
At time: 17.303203105926514 and batch: 350, loss is 4.8479928779602055 and perplexity is 127.48425642570044
At time: 17.932361364364624 and batch: 400, loss is 4.732272882461547 and perplexity is 113.55336270926946
At time: 18.563175916671753 and batch: 450, loss is 4.735772190093994 and perplexity is 113.95141690895133
At time: 19.194114923477173 and batch: 500, loss is 4.643203449249268 and perplexity is 103.87657850414446
At time: 19.823827981948853 and batch: 550, loss is 4.718823509216309 and perplexity is 112.03636534964237
At time: 20.45595908164978 and batch: 600, loss is 4.674916610717774 and perplexity is 107.22362541806848
At time: 21.086195945739746 and batch: 650, loss is 4.533159666061401 and perplexity is 93.0521106745889
At time: 21.716110467910767 and batch: 700, loss is 4.576672554016113 and perplexity is 97.19045956407287
At time: 22.347105741500854 and batch: 750, loss is 4.627216567993164 and perplexity is 102.22911992864239
At time: 22.977283239364624 and batch: 800, loss is 4.561772880554199 and perplexity is 95.75308822604843
At time: 23.630624294281006 and batch: 850, loss is 4.623488702774048 and perplexity is 101.84873300419126
At time: 24.262526273727417 and batch: 900, loss is 4.567237243652344 and perplexity is 96.27775003369402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.671386300700984 and perplexity of 106.84576016241436
finished 2 epochs...
Completing Train Step...
At time: 25.827927112579346 and batch: 50, loss is 4.565742521286011 and perplexity is 96.13394902540341
At time: 26.472665786743164 and batch: 100, loss is 4.420410485267639 and perplexity is 83.13040216102227
At time: 27.104612112045288 and batch: 150, loss is 4.418076210021972 and perplexity is 82.93657922714299
At time: 27.737547159194946 and batch: 200, loss is 4.3118290615081785 and perplexity is 74.57676976833103
At time: 28.369344234466553 and batch: 250, loss is 4.454699068069458 and perplexity is 86.03025769776238
At time: 29.001792669296265 and batch: 300, loss is 4.4259909868240355 and perplexity is 83.59560833426582
At time: 29.634035348892212 and batch: 350, loss is 4.402802867889404 and perplexity is 81.67948493023897
At time: 30.26646614074707 and batch: 400, loss is 4.3329763507843015 and perplexity is 76.17066012251834
At time: 30.897803783416748 and batch: 450, loss is 4.345886821746826 and perplexity is 77.16043469823391
At time: 31.5301456451416 and batch: 500, loss is 4.243772597312927 and perplexity is 69.67019425910439
At time: 32.16260886192322 and batch: 550, loss is 4.32788848400116 and perplexity is 75.78409817277357
At time: 32.79696846008301 and batch: 600, loss is 4.321262903213501 and perplexity is 75.28364423765477
At time: 33.430126428604126 and batch: 650, loss is 4.175409436225891 and perplexity is 65.06647477981952
At time: 34.06219029426575 and batch: 700, loss is 4.194447364807129 and perplexity is 66.31707230058313
At time: 34.693933725357056 and batch: 750, loss is 4.291775941848755 and perplexity is 73.09616783975112
At time: 35.32636570930481 and batch: 800, loss is 4.2408282995224 and perplexity is 69.46536614553219
At time: 35.95857119560242 and batch: 850, loss is 4.307847018241882 and perplexity is 74.28039232997409
At time: 36.591684103012085 and batch: 900, loss is 4.268174810409546 and perplexity is 71.39121413257095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5450196984696065 and perplexity of 94.16228204398897
finished 3 epochs...
Completing Train Step...
At time: 38.17927122116089 and batch: 50, loss is 4.302307739257812 and perplexity is 73.87007000790021
At time: 38.814969539642334 and batch: 100, loss is 4.159939427375793 and perplexity is 64.06764173721402
At time: 39.47442126274109 and batch: 150, loss is 4.153342962265015 and perplexity is 63.64641261279922
At time: 40.11025857925415 and batch: 200, loss is 4.061403493881226 and perplexity is 58.05573479530507
At time: 40.74536681175232 and batch: 250, loss is 4.2092088603973385 and perplexity is 67.30327245770891
At time: 41.39887714385986 and batch: 300, loss is 4.186299843788147 and perplexity is 65.77894772754819
At time: 42.04010272026062 and batch: 350, loss is 4.162941904067993 and perplexity is 64.2602924081021
At time: 42.68963265419006 and batch: 400, loss is 4.107925992012024 and perplexity is 60.82044458139874
At time: 43.32621717453003 and batch: 450, loss is 4.1242678928375245 and perplexity is 61.822531960025614
At time: 43.96176624298096 and batch: 500, loss is 4.0171691417694095 and perplexity is 55.54364687005783
At time: 44.59789252281189 and batch: 550, loss is 4.101108851432801 and perplexity is 60.4072331211805
At time: 45.233622789382935 and batch: 600, loss is 4.109635577201844 and perplexity is 60.92451124275868
At time: 45.869123220443726 and batch: 650, loss is 3.963497185707092 and perplexity is 52.64110011853868
At time: 46.505046129226685 and batch: 700, loss is 3.9748870849609377 and perplexity is 53.24410450615669
At time: 47.140082359313965 and batch: 750, loss is 4.082115221023559 and perplexity is 59.27070799764396
At time: 47.776172161102295 and batch: 800, loss is 4.0400418853759765 and perplexity is 56.82872304805113
At time: 48.41224026679993 and batch: 850, loss is 4.104769897460938 and perplexity is 60.62879210341787
At time: 49.047908544540405 and batch: 900, loss is 4.071799168586731 and perplexity is 58.66241127105275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.497967027638056 and perplexity of 89.83431485384277
finished 4 epochs...
Completing Train Step...
At time: 50.62656545639038 and batch: 50, loss is 4.120173535346985 and perplexity is 61.56992589586203
At time: 51.2628276348114 and batch: 100, loss is 3.981584916114807 and perplexity is 53.601921489280116
At time: 51.899413108825684 and batch: 150, loss is 3.977553973197937 and perplexity is 53.386290094417994
At time: 52.53572750091553 and batch: 200, loss is 3.890552077293396 and perplexity is 48.93789656871174
At time: 53.17329430580139 and batch: 250, loss is 4.034235253334045 and perplexity is 56.499695758582355
At time: 53.81010866165161 and batch: 300, loss is 4.018429083824158 and perplexity is 55.61367275163648
At time: 54.446627378463745 and batch: 350, loss is 3.9883782148361204 and perplexity is 53.967294994544986
At time: 55.08331918716431 and batch: 400, loss is 3.943929057121277 and perplexity is 51.6210253295093
At time: 55.7407603263855 and batch: 450, loss is 3.9629956817626955 and perplexity is 52.61470701786706
At time: 56.38284373283386 and batch: 500, loss is 3.854133324623108 and perplexity is 47.187702794089994
At time: 57.027990102767944 and batch: 550, loss is 3.9340981435775757 and perplexity is 51.11602984312018
At time: 57.6652147769928 and batch: 600, loss is 3.950537405014038 and perplexity is 51.96328466229271
At time: 58.30215764045715 and batch: 650, loss is 3.805290322303772 and perplexity is 44.93829480861518
At time: 58.939189195632935 and batch: 700, loss is 3.8134914207458497 and perplexity is 45.308353558161166
At time: 59.57573437690735 and batch: 750, loss is 3.925733733177185 and perplexity is 50.690257540978706
At time: 60.2120885848999 and batch: 800, loss is 3.885768847465515 and perplexity is 48.70437430269975
At time: 60.84900665283203 and batch: 850, loss is 3.9512192249298095 and perplexity is 51.998726345714154
At time: 61.4971444606781 and batch: 900, loss is 3.9261948251724244 and perplexity is 50.71363580231787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.485522544547303 and perplexity of 88.72330057893343
finished 5 epochs...
Completing Train Step...
At time: 63.090904235839844 and batch: 50, loss is 3.9762378787994384 and perplexity is 53.31607491210718
At time: 63.73873019218445 and batch: 100, loss is 3.84343966960907 and perplexity is 46.68578224515595
At time: 64.37254548072815 and batch: 150, loss is 3.836745948791504 and perplexity is 46.374324222112364
At time: 65.00904417037964 and batch: 200, loss is 3.7510816717147826 and perplexity is 42.567100735839745
At time: 65.64160585403442 and batch: 250, loss is 3.8947646951675416 and perplexity is 49.14448806652147
At time: 66.27220630645752 and batch: 300, loss is 3.884193663597107 and perplexity is 48.62771634901357
At time: 66.90242171287537 and batch: 350, loss is 3.8520675134658813 and perplexity is 47.09032253042312
At time: 67.53337121009827 and batch: 400, loss is 3.8097393798828123 and perplexity is 45.138673286890246
At time: 68.16362524032593 and batch: 450, loss is 3.8337290573120115 and perplexity is 46.23462874749774
At time: 68.79282689094543 and batch: 500, loss is 3.725667314529419 and perplexity is 41.49891633679856
At time: 69.42339706420898 and batch: 550, loss is 3.8001193475723265 and perplexity is 44.70651978952121
At time: 70.05297255516052 and batch: 600, loss is 3.8235715055465698 and perplexity is 45.767375206089774
At time: 70.6827380657196 and batch: 650, loss is 3.6782858180999756 and perplexity is 39.5784911570538
At time: 71.3139009475708 and batch: 700, loss is 3.685310516357422 and perplexity is 39.857496933285894
At time: 71.98293924331665 and batch: 750, loss is 3.798100118637085 and perplexity is 44.61633817047691
At time: 72.6133291721344 and batch: 800, loss is 3.7581071710586547 and perplexity is 42.867208844405155
At time: 73.24390649795532 and batch: 850, loss is 3.826564793586731 and perplexity is 45.904575380303825
At time: 73.87471723556519 and batch: 900, loss is 3.804057927131653 and perplexity is 44.88294718312817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.484920972014127 and perplexity of 88.6699431290497
finished 6 epochs...
Completing Train Step...
At time: 75.47863101959229 and batch: 50, loss is 3.8568049335479735 and perplexity is 47.313938433064614
At time: 76.13303232192993 and batch: 100, loss is 3.7276640701293946 and perplexity is 41.581862314250436
At time: 76.76794123649597 and batch: 150, loss is 3.719314317703247 and perplexity is 41.23610954230567
At time: 77.40242743492126 and batch: 200, loss is 3.63615122795105 and perplexity is 37.94551169538649
At time: 78.03454995155334 and batch: 250, loss is 3.7764752388000487 and perplexity is 43.66187251618608
At time: 78.66690587997437 and batch: 300, loss is 3.77119788646698 and perplexity is 43.43206036460782
At time: 79.3006284236908 and batch: 350, loss is 3.737487483024597 and perplexity is 41.992351015727614
At time: 79.93354439735413 and batch: 400, loss is 3.6966316175460814 and perplexity is 40.31129156990066
At time: 80.56624269485474 and batch: 450, loss is 3.7248765420913696 and perplexity is 41.46611310920401
At time: 81.19871020317078 and batch: 500, loss is 3.6137118577957152 and perplexity is 37.10352051858945
At time: 81.8307557106018 and batch: 550, loss is 3.688761854171753 and perplexity is 39.995296278896745
At time: 82.46406054496765 and batch: 600, loss is 3.715344467163086 and perplexity is 41.07273285564299
At time: 83.09568190574646 and batch: 650, loss is 3.5703107643127443 and perplexity is 35.52763215646579
At time: 83.72761416435242 and batch: 700, loss is 3.5757537364959715 and perplexity is 35.721535296013336
At time: 84.35967421531677 and batch: 750, loss is 3.688577857017517 and perplexity is 39.98793793517652
At time: 84.99194526672363 and batch: 800, loss is 3.648750514984131 and perplexity is 38.42662255137902
At time: 85.62251853942871 and batch: 850, loss is 3.721858859062195 and perplexity is 41.34117013734657
At time: 86.25339555740356 and batch: 900, loss is 3.694455828666687 and perplexity is 40.223678058804346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.494066212275257 and perplexity of 89.48457036617603
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 87.84593415260315 and batch: 50, loss is 3.7658895874023437 and perplexity is 43.20212083303616
At time: 88.48105931282043 and batch: 100, loss is 3.635913486480713 and perplexity is 37.936491545917626
At time: 89.11440086364746 and batch: 150, loss is 3.626615080833435 and perplexity is 37.58537758847412
At time: 89.74642825126648 and batch: 200, loss is 3.521041302680969 and perplexity is 33.81962660256159
At time: 90.38026762008667 and batch: 250, loss is 3.6538181257247926 and perplexity is 38.62184796194822
At time: 91.01324009895325 and batch: 300, loss is 3.641647849082947 and perplexity is 38.15465806944636
At time: 91.64651012420654 and batch: 350, loss is 3.589404330253601 and perplexity is 36.21249881071388
At time: 92.27971482276917 and batch: 400, loss is 3.5415427446365357 and perplexity is 34.5201338833632
At time: 92.91368055343628 and batch: 450, loss is 3.548017883300781 and perplexity is 34.7443817715411
At time: 93.54753112792969 and batch: 500, loss is 3.425307297706604 and perplexity is 30.73208730055375
At time: 94.18161797523499 and batch: 550, loss is 3.4790916299819945 and perplexity is 32.43025002532737
At time: 94.81566619873047 and batch: 600, loss is 3.501845917701721 and perplexity is 33.1766368113912
At time: 95.44926404953003 and batch: 650, loss is 3.336494188308716 and perplexity is 28.12036897523972
At time: 96.08251070976257 and batch: 700, loss is 3.3246866273880005 and perplexity is 27.790288562675578
At time: 96.71531271934509 and batch: 750, loss is 3.4159882974624636 and perplexity is 30.44702528099715
At time: 97.34823298454285 and batch: 800, loss is 3.3575329303741457 and perplexity is 28.718253480601977
At time: 97.98164391517639 and batch: 850, loss is 3.4086078214645386 and perplexity is 30.223138951824936
At time: 98.61910271644592 and batch: 900, loss is 3.36934121131897 and perplexity is 29.059376762002028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.44251491598887 and perplexity of 84.98841184736224
finished 8 epochs...
Completing Train Step...
At time: 100.24068999290466 and batch: 50, loss is 3.65880126953125 and perplexity is 38.81478650541818
At time: 100.88111519813538 and batch: 100, loss is 3.5288170623779296 and perplexity is 34.08362495577753
At time: 101.5127682685852 and batch: 150, loss is 3.521334743499756 and perplexity is 33.829552117687335
At time: 102.14395427703857 and batch: 200, loss is 3.420476107597351 and perplexity is 30.583972817051947
At time: 102.77493953704834 and batch: 250, loss is 3.5556057405471804 and perplexity is 35.00901992909534
At time: 103.40620493888855 and batch: 300, loss is 3.5500648260116576 and perplexity is 34.81557436927884
At time: 104.05092096328735 and batch: 350, loss is 3.502499847412109 and perplexity is 33.19833909500357
At time: 104.68211841583252 and batch: 400, loss is 3.4606982421875 and perplexity is 31.839200227813055
At time: 105.31367993354797 and batch: 450, loss is 3.4739174461364746 and perplexity is 32.26288331599944
At time: 105.94568943977356 and batch: 500, loss is 3.3557408475875854 and perplexity is 28.666834080551553
At time: 106.58582282066345 and batch: 550, loss is 3.413121223449707 and perplexity is 30.359856425539842
At time: 107.22086429595947 and batch: 600, loss is 3.4450576543807983 and perplexity is 31.345090576362654
At time: 107.85280442237854 and batch: 650, loss is 3.2870711135864257 and perplexity is 26.764358978996373
At time: 108.48339700698853 and batch: 700, loss is 3.280090374946594 and perplexity is 26.57817459244623
At time: 109.11441612243652 and batch: 750, loss is 3.3797731447219848 and perplexity is 29.364108954544516
At time: 109.74434423446655 and batch: 800, loss is 3.327653822898865 and perplexity is 27.872870239545687
At time: 110.37511658668518 and batch: 850, loss is 3.388753867149353 and perplexity is 29.629007576478166
At time: 111.0054931640625 and batch: 900, loss is 3.358396134376526 and perplexity is 28.743053894316045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.451345783390411 and perplexity of 85.74225689689567
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 112.56583428382874 and batch: 50, loss is 3.6158047103881836 and perplexity is 37.18125403171578
At time: 113.21130061149597 and batch: 100, loss is 3.4933805370330813 and perplexity is 32.89696936730594
At time: 113.8438549041748 and batch: 150, loss is 3.4900046730041505 and perplexity is 32.78610091545957
At time: 114.47556209564209 and batch: 200, loss is 3.3841385889053344 and perplexity is 29.492576538194008
At time: 115.10713934898376 and batch: 250, loss is 3.5205722045898438 and perplexity is 33.80376560075345
At time: 115.74005651473999 and batch: 300, loss is 3.5118471431732177 and perplexity is 33.51010862029424
At time: 116.37284421920776 and batch: 350, loss is 3.4556351041793825 and perplexity is 31.67840137988216
At time: 117.00560450553894 and batch: 400, loss is 3.4132430744171143 and perplexity is 30.363556028811182
At time: 117.63794660568237 and batch: 450, loss is 3.420564064979553 and perplexity is 30.586663021548215
At time: 118.27129125595093 and batch: 500, loss is 3.2956230354309084 and perplexity is 26.99422719174109
At time: 118.90369009971619 and batch: 550, loss is 3.34713095664978 and perplexity is 28.421075262075657
At time: 119.53552341461182 and batch: 600, loss is 3.3754051542282104 and perplexity is 29.236126522281634
At time: 120.17940497398376 and batch: 650, loss is 3.2104587030410765 and perplexity is 24.790455074057583
At time: 120.81122350692749 and batch: 700, loss is 3.1956576824188234 and perplexity is 24.426233114794012
At time: 121.44386053085327 and batch: 750, loss is 3.288525323867798 and perplexity is 26.803308298384387
At time: 122.07721400260925 and batch: 800, loss is 3.231800298690796 and perplexity is 25.32520889554348
At time: 122.70948100090027 and batch: 850, loss is 3.284779348373413 and perplexity is 26.70309158419743
At time: 123.34198832511902 and batch: 900, loss is 3.2529507875442505 and perplexity is 25.866554121644615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4410467278467465 and perplexity of 84.86372442356638
finished 10 epochs...
Completing Train Step...
At time: 124.91472840309143 and batch: 50, loss is 3.5863499641418457 and perplexity is 36.102061325637735
At time: 125.56929659843445 and batch: 100, loss is 3.4616575479507445 and perplexity is 31.86975841106587
At time: 126.20467352867126 and batch: 150, loss is 3.455219922065735 and perplexity is 31.665251804163496
At time: 126.84140849113464 and batch: 200, loss is 3.3510747861862185 and perplexity is 28.53338445674169
At time: 127.47813534736633 and batch: 250, loss is 3.4867780780792237 and perplexity is 32.680483931879984
At time: 128.11357140541077 and batch: 300, loss is 3.479798698425293 and perplexity is 32.45318854032293
At time: 128.7505111694336 and batch: 350, loss is 3.426288185119629 and perplexity is 30.762246807284978
At time: 129.38739371299744 and batch: 400, loss is 3.385345249176025 and perplexity is 29.528185538245133
At time: 130.0230791568756 and batch: 450, loss is 3.3958399152755736 and perplexity is 29.839705777260534
At time: 130.66054677963257 and batch: 500, loss is 3.2736077880859376 and perplexity is 26.406436521957946
At time: 131.29693579673767 and batch: 550, loss is 3.32715398311615 and perplexity is 27.858941751437733
At time: 131.93290066719055 and batch: 600, loss is 3.359449472427368 and perplexity is 28.77334599779214
At time: 132.56870865821838 and batch: 650, loss is 3.19821364402771 and perplexity is 24.488745484461152
At time: 133.20432877540588 and batch: 700, loss is 3.1867240333557127 and perplexity is 24.208989553930042
At time: 133.84388256072998 and batch: 750, loss is 3.2830109357833863 and perplexity is 26.65591123030873
At time: 134.48572206497192 and batch: 800, loss is 3.229736633300781 and perplexity is 25.273000027796115
At time: 135.12228298187256 and batch: 850, loss is 3.286984477043152 and perplexity is 26.762040307893773
At time: 135.7812910079956 and batch: 900, loss is 3.2581074953079225 and perplexity is 26.000284891008267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.443755685466609 and perplexity of 85.09392832201762
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.36856245994568 and batch: 50, loss is 3.572158079147339 and perplexity is 35.593323536035065
At time: 138.00484991073608 and batch: 100, loss is 3.4497000408172607 and perplexity is 31.490944893805302
At time: 138.64210724830627 and batch: 150, loss is 3.4449798822402955 and perplexity is 31.3426528963673
At time: 139.27887105941772 and batch: 200, loss is 3.339650731086731 and perplexity is 28.209272362698773
At time: 139.91531252861023 and batch: 250, loss is 3.4755660772323607 and perplexity is 32.31611677779006
At time: 140.55339288711548 and batch: 300, loss is 3.4675327205657958 and perplexity is 32.05754985633279
At time: 141.1899721622467 and batch: 350, loss is 3.4121453952789307 and perplexity is 30.33024487262261
At time: 141.82606101036072 and batch: 400, loss is 3.370707859992981 and perplexity is 29.099117870602633
At time: 142.46328139305115 and batch: 450, loss is 3.37965970993042 and perplexity is 29.36077823187926
At time: 143.1002688407898 and batch: 500, loss is 3.2542365741729737 and perplexity is 25.8998343821373
At time: 143.73772192001343 and batch: 550, loss is 3.3065414476394652 and perplexity is 27.290576177647836
At time: 144.3747992515564 and batch: 600, loss is 3.3378177070617676 and perplexity is 28.15761145104228
At time: 145.01197290420532 and batch: 650, loss is 3.174916214942932 and perplexity is 23.92481524238703
At time: 145.65105986595154 and batch: 700, loss is 3.16009464263916 and perplexity is 23.572826818040372
At time: 146.2880153656006 and batch: 750, loss is 3.253134560585022 and perplexity is 25.871308133766085
At time: 146.925213098526 and batch: 800, loss is 3.198247375488281 and perplexity is 24.489571539545846
At time: 147.56235456466675 and batch: 850, loss is 3.25349130153656 and perplexity is 25.880539135287492
At time: 148.19914841651917 and batch: 900, loss is 3.224884204864502 and perplexity is 25.150661662920243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442731465378853 and perplexity of 85.00681802895521
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.77434492111206 and batch: 50, loss is 3.5650928449630737 and perplexity is 35.34273464684712
At time: 150.40504240989685 and batch: 100, loss is 3.442652425765991 and perplexity is 31.269789062554924
At time: 151.03655409812927 and batch: 150, loss is 3.438284397125244 and perplexity is 31.133499603167056
At time: 151.66764783859253 and batch: 200, loss is 3.3332963609695434 and perplexity is 28.03058851825167
At time: 152.32593488693237 and batch: 250, loss is 3.4693155670166016 and perplexity is 32.11475452373253
At time: 152.95793652534485 and batch: 300, loss is 3.460654821395874 and perplexity is 31.837817774548245
At time: 153.58977890014648 and batch: 350, loss is 3.405722489356995 and perplexity is 30.136060843658615
At time: 154.22065258026123 and batch: 400, loss is 3.364325928688049 and perplexity is 28.914000630377206
At time: 154.85151433944702 and batch: 450, loss is 3.373733162879944 and perplexity is 29.18728481451427
At time: 155.4817008972168 and batch: 500, loss is 3.247697229385376 and perplexity is 25.731019007727834
At time: 156.113516330719 and batch: 550, loss is 3.2999641036987306 and perplexity is 27.111665694670695
At time: 156.7438678741455 and batch: 600, loss is 3.3312898969650266 and perplexity is 27.974402537787974
At time: 157.37460374832153 and batch: 650, loss is 3.1679198360443115 and perplexity is 23.758012358367516
At time: 158.00464034080505 and batch: 700, loss is 3.1528729438781737 and perplexity is 23.403204182795495
At time: 158.63525462150574 and batch: 750, loss is 3.2454388332366944 and perplexity is 25.672973742771482
At time: 159.26877617835999 and batch: 800, loss is 3.189965081214905 and perplexity is 24.287579335523905
At time: 159.9058997631073 and batch: 850, loss is 3.2445645332336426 and perplexity is 25.65053767110777
At time: 160.5360598564148 and batch: 900, loss is 3.215790638923645 and perplexity is 24.922989208776205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442455553028681 and perplexity of 84.98336683339825
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 162.11264491081238 and batch: 50, loss is 3.563163900375366 and perplexity is 35.27462617998289
At time: 162.75969529151917 and batch: 100, loss is 3.4407462739944457 and perplexity is 31.210240870713438
At time: 163.39412927627563 and batch: 150, loss is 3.4362180852890014 and perplexity is 31.06923450316753
At time: 164.02761387825012 and batch: 200, loss is 3.331482334136963 and perplexity is 27.979786370707124
At time: 164.66114854812622 and batch: 250, loss is 3.467555446624756 and perplexity is 32.05827840637945
At time: 165.29481410980225 and batch: 300, loss is 3.4588025331497194 and perplexity is 31.77889954252114
At time: 165.92807006835938 and batch: 350, loss is 3.403909068107605 and perplexity is 30.08146099178024
At time: 166.56211280822754 and batch: 400, loss is 3.362436099052429 and perplexity is 28.85940969513264
At time: 167.19635438919067 and batch: 450, loss is 3.3720043468475343 and perplexity is 29.136868961009274
At time: 167.83022260665894 and batch: 500, loss is 3.2459351062774657 and perplexity is 25.685717709498455
At time: 168.47754192352295 and batch: 550, loss is 3.2981904315948487 and perplexity is 27.063621109803584
At time: 169.11071109771729 and batch: 600, loss is 3.3295920705795288 and perplexity is 27.926947155940127
At time: 169.74485731124878 and batch: 650, loss is 3.1660258197784423 and perplexity is 23.713056883155748
At time: 170.39018988609314 and batch: 700, loss is 3.1509458351135256 and perplexity is 23.35814709179569
At time: 171.02268815040588 and batch: 750, loss is 3.2435349941253664 and perplexity is 25.624143028916663
At time: 171.6559636592865 and batch: 800, loss is 3.1878573226928713 and perplexity is 24.23644089587337
At time: 172.28992199897766 and batch: 850, loss is 3.242294225692749 and perplexity is 25.592369117243152
At time: 172.9240472316742 and batch: 900, loss is 3.213447971343994 and perplexity is 24.86467126641644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442387829088185 and perplexity of 84.9776116198047
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 174.49805569648743 and batch: 50, loss is 3.5626854419708254 and perplexity is 35.257752775553016
At time: 175.14218735694885 and batch: 100, loss is 3.4402869033813475 and perplexity is 31.195907095739276
At time: 175.7765510082245 and batch: 150, loss is 3.43569598197937 and perplexity is 31.053017386879148
At time: 176.41068124771118 and batch: 200, loss is 3.3310178756713866 and perplexity is 27.966793939518126
At time: 177.0447473526001 and batch: 250, loss is 3.4670883417129517 and perplexity is 32.04330732387714
At time: 177.67892479896545 and batch: 300, loss is 3.458331046104431 and perplexity is 31.76391973474348
At time: 178.31281661987305 and batch: 350, loss is 3.403427062034607 and perplexity is 30.0669650387469
At time: 178.94792461395264 and batch: 400, loss is 3.3619617795944214 and perplexity is 28.845724361434232
At time: 179.58125185966492 and batch: 450, loss is 3.37155695438385 and perplexity is 29.123836261004033
At time: 180.21566033363342 and batch: 500, loss is 3.2454874610900877 and perplexity is 25.674222194729342
At time: 180.84963369369507 and batch: 550, loss is 3.2977410459518435 and perplexity is 27.05146183933452
At time: 181.48322176933289 and batch: 600, loss is 3.329160647392273 and perplexity is 27.914901421979092
At time: 182.11693692207336 and batch: 650, loss is 3.1655389928817748 and perplexity is 23.701515538809186
At time: 182.7503707408905 and batch: 700, loss is 3.150449733734131 and perplexity is 23.346561956741695
At time: 183.38413000106812 and batch: 750, loss is 3.243045172691345 and perplexity is 25.61159484786758
At time: 184.0405411720276 and batch: 800, loss is 3.187329025268555 and perplexity is 24.223640228151098
At time: 184.6757709980011 and batch: 850, loss is 3.241726198196411 and perplexity is 25.577836075862447
At time: 185.3084421157837 and batch: 900, loss is 3.212855405807495 and perplexity is 24.849941683700127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.442372361274614 and perplexity of 84.97629721211604
Annealing...
Model not improving. Stopping early with 84.86372442356638 lossat 14 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -84.86372442356638
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
196.650865316391


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8510608673095703 and batch: 50, loss is 6.489763441085816 and perplexity is 658.3676020675239
At time: 1.5116207599639893 and batch: 100, loss is 5.615148754119873 and perplexity is 274.5542172288339
At time: 2.1692397594451904 and batch: 150, loss is 5.440549173355103 and perplexity is 230.56877092373978
At time: 2.8156471252441406 and batch: 200, loss is 5.2369139289855955 and perplexity is 188.0887505792489
At time: 3.455815553665161 and batch: 250, loss is 5.269006004333496 and perplexity is 194.22280983355586
At time: 4.093751668930054 and batch: 300, loss is 5.193388147354126 and perplexity is 180.07765010740948
At time: 4.73063588142395 and batch: 350, loss is 5.147187023162842 and perplexity is 171.9471261006481
At time: 5.3675925731658936 and batch: 400, loss is 5.000248460769654 and perplexity is 148.4500385316699
At time: 6.0051047801971436 and batch: 450, loss is 4.999024562835693 and perplexity is 148.26846197446693
At time: 6.642436742782593 and batch: 500, loss is 4.913726310729981 and perplexity is 136.14579188776372
At time: 7.278798341751099 and batch: 550, loss is 4.9796138191223145 and perplexity is 145.41821307553568
At time: 7.916738271713257 and batch: 600, loss is 4.914941711425781 and perplexity is 136.31136417590957
At time: 8.554927349090576 and batch: 650, loss is 4.792143650054932 and perplexity is 120.55952930293444
At time: 9.192530632019043 and batch: 700, loss is 4.853905658721924 and perplexity is 128.24027577023767
At time: 9.830293655395508 and batch: 750, loss is 4.876627807617187 and perplexity is 131.18752755414468
At time: 10.469103574752808 and batch: 800, loss is 4.808168630599976 and perplexity is 122.50705627858235
At time: 11.106426000595093 and batch: 850, loss is 4.864962329864502 and perplexity is 129.6660539819843
At time: 11.743458986282349 and batch: 900, loss is 4.803787202835083 and perplexity is 121.97147462376684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.798254143701841 and perplexity of 121.29846286603723
finished 1 epochs...
Completing Train Step...
At time: 13.324568271636963 and batch: 50, loss is 4.7692350578308105 and perplexity is 117.82907505056865
At time: 13.956387042999268 and batch: 100, loss is 4.6050506019592286 and perplexity is 99.98804231210221
At time: 14.588471412658691 and batch: 150, loss is 4.5912777614593505 and perplexity is 98.62036298768962
At time: 15.220483303070068 and batch: 200, loss is 4.484361209869385 and perplexity is 88.62032294055683
At time: 15.877655506134033 and batch: 250, loss is 4.613111925125122 and perplexity is 100.7973358395834
At time: 16.512267112731934 and batch: 300, loss is 4.582610845565796 and perplexity is 97.7693218743458
At time: 17.1519033908844 and batch: 350, loss is 4.560951232910156 and perplexity is 95.67444523953435
At time: 17.78422522544861 and batch: 400, loss is 4.466331462860108 and perplexity is 87.03683874306462
At time: 18.41638422012329 and batch: 450, loss is 4.484236326217651 and perplexity is 88.60925640203946
At time: 19.04837679862976 and batch: 500, loss is 4.377609014511108 and perplexity is 79.64736985347619
At time: 19.680983066558838 and batch: 550, loss is 4.464690999984741 and perplexity is 86.89417508952474
At time: 20.31424641609192 and batch: 600, loss is 4.4479850482940675 and perplexity is 85.45458355261623
At time: 20.94646191596985 and batch: 650, loss is 4.300816307067871 and perplexity is 73.75997992397144
At time: 21.577736377716064 and batch: 700, loss is 4.321913285255432 and perplexity is 75.3326232937408
At time: 22.209585189819336 and batch: 750, loss is 4.414469809532165 and perplexity is 82.63801540115588
At time: 22.841721057891846 and batch: 800, loss is 4.358480997085572 and perplexity is 78.13835184620062
At time: 23.473923444747925 and batch: 850, loss is 4.423487749099731 and perplexity is 83.38661034885325
At time: 24.105695009231567 and batch: 900, loss is 4.3817526054382325 and perplexity is 79.97808066455461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.587326258829195 and perplexity of 98.23143329837634
finished 2 epochs...
Completing Train Step...
At time: 25.681467056274414 and batch: 50, loss is 4.42099479675293 and perplexity is 83.17899040373352
At time: 26.316709518432617 and batch: 100, loss is 4.260092134475708 and perplexity is 70.8165077955308
At time: 26.951719045639038 and batch: 150, loss is 4.256203851699829 and perplexity is 70.54168782314837
At time: 27.586780786514282 and batch: 200, loss is 4.157497010231018 and perplexity is 63.91135276973047
At time: 28.226043701171875 and batch: 250, loss is 4.304722089767456 and perplexity is 74.04863372002724
At time: 28.87861728668213 and batch: 300, loss is 4.284096837043762 and perplexity is 72.53700439118546
At time: 29.519187927246094 and batch: 350, loss is 4.264002828598023 and perplexity is 71.09399172006371
At time: 30.155194520950317 and batch: 400, loss is 4.190619921684265 and perplexity is 66.06373260917873
At time: 30.79014778137207 and batch: 450, loss is 4.21203565120697 and perplexity is 67.493793884993
At time: 31.425685167312622 and batch: 500, loss is 4.102005620002746 and perplexity is 60.46142872606651
At time: 32.0737624168396 and batch: 550, loss is 4.187862029075623 and perplexity is 65.88178693795028
At time: 32.708598375320435 and batch: 600, loss is 4.192079257965088 and perplexity is 66.16021219197727
At time: 33.34408450126648 and batch: 650, loss is 4.0413523769378665 and perplexity is 56.90324542988955
At time: 33.97841477394104 and batch: 700, loss is 4.051206936836243 and perplexity is 57.46677397282305
At time: 34.61320471763611 and batch: 750, loss is 4.166431722640991 and perplexity is 64.48494093335572
At time: 35.2490017414093 and batch: 800, loss is 4.1150262880325315 and perplexity is 61.253824484117814
At time: 35.88479423522949 and batch: 850, loss is 4.181344041824341 and perplexity is 65.45376672109421
At time: 36.520023584365845 and batch: 900, loss is 4.150435500144958 and perplexity is 63.46163183077996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.521090154778467 and perplexity of 91.93576758439309
finished 3 epochs...
Completing Train Step...
At time: 38.09723734855652 and batch: 50, loss is 4.206495504379273 and perplexity is 67.12090224789212
At time: 38.75166654586792 and batch: 100, loss is 4.049445204734802 and perplexity is 57.36562203981473
At time: 39.388532400131226 and batch: 150, loss is 4.051326394081116 and perplexity is 57.47363920535629
At time: 40.0248236656189 and batch: 200, loss is 3.958848032951355 and perplexity is 52.39693163095662
At time: 40.66098737716675 and batch: 250, loss is 4.10263162612915 and perplexity is 60.49928980025592
At time: 41.296375036239624 and batch: 300, loss is 4.092689867019653 and perplexity is 59.90080038305247
At time: 41.932093143463135 and batch: 350, loss is 4.068522458076477 and perplexity is 58.47050611201428
At time: 42.56864547729492 and batch: 400, loss is 4.00895233631134 and perplexity is 55.08912544364563
At time: 43.20447587966919 and batch: 450, loss is 4.030540633201599 and perplexity is 56.29133598727152
At time: 43.84047484397888 and batch: 500, loss is 3.918722004890442 and perplexity is 50.336074397396615
At time: 44.476940631866455 and batch: 550, loss is 4.00309148311615 and perplexity is 54.76720046578555
At time: 45.11373949050903 and batch: 600, loss is 4.016752672195435 and perplexity is 55.52051944737706
At time: 45.74934244155884 and batch: 650, loss is 3.865931658744812 and perplexity is 47.74773631297203
At time: 46.38438606262207 and batch: 700, loss is 3.8690991258621215 and perplexity is 47.89921547365229
At time: 47.02064323425293 and batch: 750, loss is 3.9911664724349976 and perplexity is 54.11797969115622
At time: 47.65724778175354 and batch: 800, loss is 3.9428713274002076 and perplexity is 51.56645310320698
At time: 48.316579818725586 and batch: 850, loss is 4.010856280326843 and perplexity is 55.19411196686993
At time: 48.95261526107788 and batch: 900, loss is 3.9843243741989136 and perplexity is 53.748963021527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.500657591101241 and perplexity of 90.07634523216775
finished 4 epochs...
Completing Train Step...
At time: 50.52063202857971 and batch: 50, loss is 4.042797355651856 and perplexity is 56.98552884286571
At time: 51.1621572971344 and batch: 100, loss is 3.8947144460678103 and perplexity is 49.14201866228255
At time: 51.79180407524109 and batch: 150, loss is 3.8965210819244387 and perplexity is 49.230880641714144
At time: 52.42247533798218 and batch: 200, loss is 3.8055601072311402 and perplexity is 44.950420118755595
At time: 53.05813145637512 and batch: 250, loss is 3.9481279134750364 and perplexity is 51.83823028679184
At time: 53.70344924926758 and batch: 300, loss is 3.9487499523162843 and perplexity is 51.87048571051492
At time: 54.33408832550049 and batch: 350, loss is 3.92102068901062 and perplexity is 50.45191422085971
At time: 54.9661660194397 and batch: 400, loss is 3.869306626319885 and perplexity is 47.90915561404577
At time: 55.59727072715759 and batch: 450, loss is 3.887907934188843 and perplexity is 48.80866869074078
At time: 56.22811150550842 and batch: 500, loss is 3.7784038734436036 and perplexity is 43.74616157139223
At time: 56.85959577560425 and batch: 550, loss is 3.860777816772461 and perplexity is 47.50228507722582
At time: 57.4907968044281 and batch: 600, loss is 3.87782998085022 and perplexity is 48.31924752930632
At time: 58.12292122840881 and batch: 650, loss is 3.7298353099823 and perplexity is 41.672244596164106
At time: 58.75213599205017 and batch: 700, loss is 3.7295245504379273 and perplexity is 41.65929656038747
At time: 59.38363814353943 and batch: 750, loss is 3.853676962852478 and perplexity is 47.166173043542116
At time: 60.01497149467468 and batch: 800, loss is 3.80590256690979 and perplexity is 44.96581646134743
At time: 60.64569878578186 and batch: 850, loss is 3.877722072601318 and perplexity is 48.31403376522631
At time: 61.276240825653076 and batch: 900, loss is 3.8528348875045775 and perplexity is 47.1264722898263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498370444937928 and perplexity of 89.87056288162606
finished 5 epochs...
Completing Train Step...
At time: 62.85261416435242 and batch: 50, loss is 3.9122334909439087 and perplexity is 50.010525383469655
At time: 63.49706745147705 and batch: 100, loss is 3.7705199575424193 and perplexity is 43.4026264927972
At time: 64.14699482917786 and batch: 150, loss is 3.772511405944824 and perplexity is 43.48914670566233
At time: 64.78848600387573 and batch: 200, loss is 3.6826538181304933 and perplexity is 39.75174812538696
At time: 65.42213439941406 and batch: 250, loss is 3.8219007635116578 and perplexity is 45.69097356999001
At time: 66.053302526474 and batch: 300, loss is 3.8268250322341917 and perplexity is 45.916523079472164
At time: 66.68623161315918 and batch: 350, loss is 3.799179792404175 and perplexity is 44.664535274275565
At time: 67.31769156455994 and batch: 400, loss is 3.7514766883850097 and perplexity is 42.583918771717215
At time: 67.95083928108215 and batch: 450, loss is 3.7694307661056516 and perplexity is 43.3553784594357
At time: 68.58142948150635 and batch: 500, loss is 3.6633713388442994 and perplexity is 38.9925787220555
At time: 69.22463536262512 and batch: 550, loss is 3.7427266693115233 and perplexity is 42.21293409869018
At time: 69.85632920265198 and batch: 600, loss is 3.765870008468628 and perplexity is 43.20127498985637
At time: 70.48781323432922 and batch: 650, loss is 3.6143085956573486 and perplexity is 37.12566820160606
At time: 71.11864066123962 and batch: 700, loss is 3.613761191368103 and perplexity is 37.10535101295683
At time: 71.74871516227722 and batch: 750, loss is 3.7375200986862183 and perplexity is 41.99372064637461
At time: 72.38048791885376 and batch: 800, loss is 3.6908548974990847 and perplexity is 40.07909583435551
At time: 73.01170039176941 and batch: 850, loss is 3.7630239725112915 and perplexity is 43.07849740541654
At time: 73.64281177520752 and batch: 900, loss is 3.7396961879730224 and perplexity is 42.08520223193548
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5089307811162245 and perplexity of 90.8246551389434
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.21604061126709 and batch: 50, loss is 3.8178579711914065 and perplexity is 45.506627340778415
At time: 75.84686231613159 and batch: 100, loss is 3.6716282844543455 and perplexity is 39.31587119116453
At time: 76.47748947143555 and batch: 150, loss is 3.6719181680679323 and perplexity is 39.32726987004203
At time: 77.10854458808899 and batch: 200, loss is 3.5633666133880615 and perplexity is 35.28177753053895
At time: 77.7404773235321 and batch: 250, loss is 3.698289866447449 and perplexity is 40.37819317928884
At time: 78.37150263786316 and batch: 300, loss is 3.687519659996033 and perplexity is 39.94564519932833
At time: 79.00431728363037 and batch: 350, loss is 3.640472903251648 and perplexity is 38.1098547389005
At time: 79.63502311706543 and batch: 400, loss is 3.5844048738479612 and perplexity is 36.03190780614703
At time: 80.27943110466003 and batch: 450, loss is 3.5880130624771116 and perplexity is 36.162152558692384
At time: 80.91026616096497 and batch: 500, loss is 3.468648829460144 and perplexity is 32.09334954731892
At time: 81.5403504371643 and batch: 550, loss is 3.530668683052063 and perplexity is 34.14679336431291
At time: 82.17097306251526 and batch: 600, loss is 3.5393200874328614 and perplexity is 34.443492664246364
At time: 82.80259299278259 and batch: 650, loss is 3.3749669551849366 and perplexity is 29.223318086137724
At time: 83.43396353721619 and batch: 700, loss is 3.3531752252578735 and perplexity is 28.593380078817944
At time: 84.06638836860657 and batch: 750, loss is 3.4571770668029784 and perplexity is 31.72728597020492
At time: 84.69796776771545 and batch: 800, loss is 3.3888645458221434 and perplexity is 29.632287057193764
At time: 85.33013343811035 and batch: 850, loss is 3.4432945346832273 and perplexity is 31.289874120659444
At time: 85.9604697227478 and batch: 900, loss is 3.4057542276382446 and perplexity is 30.137017325611897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.444871040239726 and perplexity of 85.18889118993425
finished 7 epochs...
Completing Train Step...
At time: 87.51391220092773 and batch: 50, loss is 3.70256055355072 and perplexity is 40.551004557158926
At time: 88.1574776172638 and batch: 100, loss is 3.5578212118148804 and perplexity is 35.08666738795581
At time: 88.7855920791626 and batch: 150, loss is 3.560852437019348 and perplexity is 35.19318433535142
At time: 89.4140248298645 and batch: 200, loss is 3.4586209535598753 and perplexity is 31.773129666838194
At time: 90.0508816242218 and batch: 250, loss is 3.5971047401428224 and perplexity is 36.49242628961181
At time: 90.69296407699585 and batch: 300, loss is 3.5917036819458006 and perplexity is 36.29585988261632
At time: 91.32391047477722 and batch: 350, loss is 3.5497471475601197 and perplexity is 34.80451596812468
At time: 91.95313835144043 and batch: 400, loss is 3.501013331413269 and perplexity is 33.14902589431556
At time: 92.58213114738464 and batch: 450, loss is 3.5106910037994385 and perplexity is 33.47138865145191
At time: 93.22644782066345 and batch: 500, loss is 3.3955625247955323 and perplexity is 29.831429674859763
At time: 93.86041402816772 and batch: 550, loss is 3.4618739461898804 and perplexity is 31.876655716923217
At time: 94.4917426109314 and batch: 600, loss is 3.478743705749512 and perplexity is 32.41896871811196
At time: 95.1233696937561 and batch: 650, loss is 3.3215836095809936 and perplexity is 27.704188456571085
At time: 95.75489664077759 and batch: 700, loss is 3.305789761543274 and perplexity is 27.270069939070822
At time: 96.40069246292114 and batch: 750, loss is 3.417102155685425 and perplexity is 30.480957844995526
At time: 97.03257989883423 and batch: 800, loss is 3.3565892028808593 and perplexity is 28.691164059759313
At time: 97.66376185417175 and batch: 850, loss is 3.420487155914307 and perplexity is 30.584310720344032
At time: 98.29619455337524 and batch: 900, loss is 3.3925445604324342 and perplexity is 29.741535200591372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45236707713506 and perplexity of 85.82986965908422
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 99.87534666061401 and batch: 50, loss is 3.657831139564514 and perplexity is 38.777149377281596
At time: 100.5198323726654 and batch: 100, loss is 3.5224346828460695 and perplexity is 33.86678304525711
At time: 101.15303158760071 and batch: 150, loss is 3.527236385345459 and perplexity is 34.02979230984868
At time: 101.78685975074768 and batch: 200, loss is 3.419734101295471 and perplexity is 30.561287733763375
At time: 102.42045068740845 and batch: 250, loss is 3.5581874513626097 and perplexity is 35.09951986655062
At time: 103.05405616760254 and batch: 300, loss is 3.551658854484558 and perplexity is 34.87111564154804
At time: 103.68663954734802 and batch: 350, loss is 3.5016172885894776 and perplexity is 33.16905253339619
At time: 104.32051706314087 and batch: 400, loss is 3.451062741279602 and perplexity is 31.53388687085254
At time: 104.95424699783325 and batch: 450, loss is 3.4570066690444947 and perplexity is 31.721880172373822
At time: 105.58614754676819 and batch: 500, loss is 3.332987837791443 and perplexity is 28.021941765928336
At time: 106.21832609176636 and batch: 550, loss is 3.3907912158966065 and perplexity is 29.689433731620852
At time: 106.85128092765808 and batch: 600, loss is 3.4090067863464357 and perplexity is 30.235199328560828
At time: 107.48394322395325 and batch: 650, loss is 3.243438787460327 and perplexity is 25.621677934152533
At time: 108.1168942451477 and batch: 700, loss is 3.2203810882568358 and perplexity is 25.037659921655642
At time: 108.75049567222595 and batch: 750, loss is 3.323408255577087 and perplexity is 27.754784939410595
At time: 109.38329315185547 and batch: 800, loss is 3.2580902004241943 and perplexity is 25.999835222992665
At time: 110.01462435722351 and batch: 850, loss is 3.3130013370513915 and perplexity is 27.46744093006466
At time: 110.64759588241577 and batch: 900, loss is 3.285164499282837 and perplexity is 26.71337828504444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.439313914677868 and perplexity of 84.71679877843526
finished 9 epochs...
Completing Train Step...
At time: 112.23523139953613 and batch: 50, loss is 3.626547861099243 and perplexity is 37.58285119429583
At time: 112.87046122550964 and batch: 100, loss is 3.486764659881592 and perplexity is 32.6800454216299
At time: 113.52021026611328 and batch: 150, loss is 3.4908882713317873 and perplexity is 32.81508346197735
At time: 114.16750526428223 and batch: 200, loss is 3.3849770641326904 and perplexity is 29.51731570315128
At time: 114.81269216537476 and batch: 250, loss is 3.52398220539093 and perplexity is 33.91923322899503
At time: 115.45801854133606 and batch: 300, loss is 3.5178466558456423 and perplexity is 33.711757233473875
At time: 116.09415435791016 and batch: 350, loss is 3.4714055919647215 and perplexity is 32.181945352685666
At time: 116.73437452316284 and batch: 400, loss is 3.422781295776367 and perplexity is 30.654555952105518
At time: 117.37807202339172 and batch: 450, loss is 3.431474390029907 and perplexity is 30.92220054062766
At time: 118.01375317573547 and batch: 500, loss is 3.309964127540588 and perplexity is 27.384143117721074
At time: 118.64996838569641 and batch: 550, loss is 3.3700786304473875 and perplexity is 29.080813605283947
At time: 119.28648114204407 and batch: 600, loss is 3.3920831727981566 and perplexity is 29.727815989206007
At time: 119.92331838607788 and batch: 650, loss is 3.229985909461975 and perplexity is 25.27930076950463
At time: 120.55950570106506 and batch: 700, loss is 3.210564498901367 and perplexity is 24.793077940320888
At time: 121.19561815261841 and batch: 750, loss is 3.3169374465942383 and perplexity is 27.575768841830747
At time: 121.83104109764099 and batch: 800, loss is 3.25502893447876 and perplexity is 25.920364515385344
At time: 122.4664511680603 and batch: 850, loss is 3.314185085296631 and perplexity is 27.499974717174513
At time: 123.10835027694702 and batch: 900, loss is 3.289877438545227 and perplexity is 26.839573957077878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.441570125214041 and perplexity of 84.90815349952496
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.70280766487122 and batch: 50, loss is 3.6117871475219725 and perplexity is 37.03217567255917
At time: 125.34870553016663 and batch: 100, loss is 3.474240002632141 and perplexity is 32.273291597121045
At time: 125.98652458190918 and batch: 150, loss is 3.4798804235458376 and perplexity is 32.455840889448744
At time: 126.62295389175415 and batch: 200, loss is 3.372810750007629 and perplexity is 29.160374500411553
At time: 127.25894665718079 and batch: 250, loss is 3.511654076576233 and perplexity is 33.50363956215794
At time: 127.89535975456238 and batch: 300, loss is 3.5043876791000366 and perplexity is 33.261071166699125
At time: 128.545072555542 and batch: 350, loss is 3.457088327407837 and perplexity is 31.724470634955864
At time: 129.18317699432373 and batch: 400, loss is 3.4095066261291502 and perplexity is 30.250315861631123
At time: 129.81958532333374 and batch: 450, loss is 3.415991702079773 and perplexity is 30.4471289416429
At time: 130.45735812187195 and batch: 500, loss is 3.291928601264954 and perplexity is 26.894682789862184
At time: 131.09461617469788 and batch: 550, loss is 3.3476599264144897 and perplexity is 28.43611312850752
At time: 131.73253226280212 and batch: 600, loss is 3.369480938911438 and perplexity is 29.063437442443558
At time: 132.36943101882935 and batch: 650, loss is 3.2049798154830933 and perplexity is 24.65500236212179
At time: 133.00661087036133 and batch: 700, loss is 3.182316460609436 and perplexity is 24.102521476615408
At time: 133.64365220069885 and batch: 750, loss is 3.286743221282959 and perplexity is 26.75558459028582
At time: 134.28095769882202 and batch: 800, loss is 3.2241108274459838 and perplexity is 25.13121822865496
At time: 134.9173879623413 and batch: 850, loss is 3.2790553092956545 and perplexity is 26.550678669358515
At time: 135.55348873138428 and batch: 900, loss is 3.2555908870697023 and perplexity is 25.934934624854893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.440918386799016 and perplexity of 84.85283362314286
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.11957669258118 and batch: 50, loss is 3.604004726409912 and perplexity is 36.745094230067274
At time: 137.76382112503052 and batch: 100, loss is 3.4663943243026734 and perplexity is 32.02107642591889
At time: 138.3951461315155 and batch: 150, loss is 3.4724284696578978 and perplexity is 32.21488038809487
At time: 139.0247163772583 and batch: 200, loss is 3.3655890607833863 and perplexity is 28.950545908475878
At time: 139.6549732685089 and batch: 250, loss is 3.50448308467865 and perplexity is 33.26424460981877
At time: 140.28634572029114 and batch: 300, loss is 3.4969554281234743 and perplexity is 33.0147829098199
At time: 140.91801738739014 and batch: 350, loss is 3.450293116569519 and perplexity is 31.509626949026902
At time: 141.54875898361206 and batch: 400, loss is 3.4030657148361207 and perplexity is 30.056102387875594
At time: 142.1790783405304 and batch: 450, loss is 3.409529948234558 and perplexity is 30.251021370913215
At time: 142.81061840057373 and batch: 500, loss is 3.285349841117859 and perplexity is 26.718329850447365
At time: 143.4423213005066 and batch: 550, loss is 3.340259337425232 and perplexity is 28.226445930104106
At time: 144.07216548919678 and batch: 600, loss is 3.3625049114227297 and perplexity is 28.861395647847512
At time: 144.71592259407043 and batch: 650, loss is 3.197876558303833 and perplexity is 24.48049206909508
At time: 145.34734344482422 and batch: 700, loss is 3.174478516578674 and perplexity is 23.91434568131229
At time: 145.97848534584045 and batch: 750, loss is 3.278670654296875 and perplexity is 26.540467782047678
At time: 146.60968852043152 and batch: 800, loss is 3.2158346652984617 and perplexity is 24.924086501795408
At time: 147.2412085533142 and batch: 850, loss is 3.26986545085907 and perplexity is 26.30779941331314
At time: 147.87383913993835 and batch: 900, loss is 3.246028995513916 and perplexity is 25.688129435137647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4410726468857025 and perplexity of 84.86592403825149
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.44074249267578 and batch: 50, loss is 3.602065963745117 and perplexity is 36.673923227394184
At time: 150.08350276947021 and batch: 100, loss is 3.464361991882324 and perplexity is 31.95606503891555
At time: 150.71560192108154 and batch: 150, loss is 3.470461297035217 and perplexity is 32.15157044855861
At time: 151.34723019599915 and batch: 200, loss is 3.3636118268966673 and perplexity is 28.893360461199038
At time: 151.97900128364563 and batch: 250, loss is 3.502521629333496 and perplexity is 33.199062226491485
At time: 152.61050939559937 and batch: 300, loss is 3.4949728298187255 and perplexity is 32.949392699870025
At time: 153.24260878562927 and batch: 350, loss is 3.4484049558639525 and perplexity is 31.450187842526432
At time: 153.8735752105713 and batch: 400, loss is 3.4010934400558472 and perplexity is 29.996881913863003
At time: 154.50481510162354 and batch: 450, loss is 3.4077499437332155 and perplexity is 30.197222312190945
At time: 155.13655757904053 and batch: 500, loss is 3.283408603668213 and perplexity is 26.66651353810475
At time: 155.76756238937378 and batch: 550, loss is 3.338204960823059 and perplexity is 28.168517703593043
At time: 156.39971733093262 and batch: 600, loss is 3.3606131172180174 and perplexity is 28.80684744008115
At time: 157.0317997932434 and batch: 650, loss is 3.19596275806427 and perplexity is 24.43368610043174
At time: 157.6646590232849 and batch: 700, loss is 3.1723923015594484 and perplexity is 23.86450721912712
At time: 158.2963001728058 and batch: 750, loss is 3.2766252613067626 and perplexity is 26.486237575241006
At time: 158.9343819618225 and batch: 800, loss is 3.2137068939208984 and perplexity is 24.87111012472154
At time: 159.56561732292175 and batch: 850, loss is 3.267546057701111 and perplexity is 26.246851991192525
At time: 160.20929741859436 and batch: 900, loss is 3.2435550355911253 and perplexity is 25.624656579447915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.441043801503639 and perplexity of 84.86347608355472
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 161.78316712379456 and batch: 50, loss is 3.601563882827759 and perplexity is 36.65551457208104
At time: 162.4139518737793 and batch: 100, loss is 3.46385374546051 and perplexity is 31.939827609861403
At time: 163.04579830169678 and batch: 150, loss is 3.4699522399902345 and perplexity is 32.13520763027183
At time: 163.68909573554993 and batch: 200, loss is 3.3631157445907593 and perplexity is 28.8790305310223
At time: 164.32567024230957 and batch: 250, loss is 3.5020136880874633 and perplexity is 33.18220333548256
At time: 164.97126460075378 and batch: 300, loss is 3.4944676256179807 and perplexity is 32.93275073241596
At time: 165.6102809906006 and batch: 350, loss is 3.447912559509277 and perplexity is 31.43470569666782
At time: 166.24279403686523 and batch: 400, loss is 3.4006093835830686 and perplexity is 29.982365242737185
At time: 166.87465715408325 and batch: 450, loss is 3.407298731803894 and perplexity is 30.183600038748562
At time: 167.50765085220337 and batch: 500, loss is 3.282914776802063 and perplexity is 26.653348148272233
At time: 168.13980650901794 and batch: 550, loss is 3.3376842451095583 and perplexity is 28.153853732010308
At time: 168.7706458568573 and batch: 600, loss is 3.3601266956329345 and perplexity is 28.79283857507148
At time: 169.40274024009705 and batch: 650, loss is 3.1954635047912596 and perplexity is 24.421490547262618
At time: 170.03477001190186 and batch: 700, loss is 3.171856942176819 and perplexity is 23.85173455056457
At time: 170.66624093055725 and batch: 750, loss is 3.2760955810546877 and perplexity is 26.47221205309461
At time: 171.2986936569214 and batch: 800, loss is 3.2131721639633177 and perplexity is 24.85781435220031
At time: 171.93150734901428 and batch: 850, loss is 3.2669656467437744 and perplexity is 26.231622450824844
At time: 172.56374979019165 and batch: 900, loss is 3.242929916381836 and perplexity is 25.60864312007066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.441030423935145 and perplexity of 84.8623408241843
Annealing...
Model not improving. Stopping early with 84.71679877843526 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
375.4130883216858


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8462727069854736 and batch: 50, loss is 6.754616918563843 and perplexity is 858.0109988376031
At time: 1.4938795566558838 and batch: 100, loss is 5.975621452331543 and perplexity is 393.712698754929
At time: 2.129981517791748 and batch: 150, loss is 5.883260192871094 and perplexity is 358.9776724677752
At time: 2.7658324241638184 and batch: 200, loss is 5.7323672580719 and perplexity is 308.69917471134397
At time: 3.401707410812378 and batch: 250, loss is 5.78939193725586 and perplexity is 326.8142403816948
At time: 4.037511348724365 and batch: 300, loss is 5.705590934753418 and perplexity is 300.5430289042182
At time: 4.681981563568115 and batch: 350, loss is 5.695476913452149 and perplexity is 297.5186504033496
At time: 5.331644773483276 and batch: 400, loss is 5.564766426086425 and perplexity is 261.0642187965246
At time: 5.9688591957092285 and batch: 450, loss is 5.565616903305053 and perplexity is 261.2863424093575
At time: 6.608285188674927 and batch: 500, loss is 5.517491235733032 and perplexity is 249.00954666081756
At time: 7.248039722442627 and batch: 550, loss is 5.579408836364746 and perplexity is 264.91495140089796
At time: 7.887980222702026 and batch: 600, loss is 5.504261445999146 and perplexity is 245.73689867257582
At time: 8.52769684791565 and batch: 650, loss is 5.413817329406738 and perplexity is 224.48689457192094
At time: 9.167705297470093 and batch: 700, loss is 5.510583543777466 and perplexity is 247.2953926584679
At time: 9.807422161102295 and batch: 750, loss is 5.483516941070556 and perplexity is 240.69171916741084
At time: 10.44465446472168 and batch: 800, loss is 5.457709341049195 and perplexity is 234.5595126671352
At time: 11.082308530807495 and batch: 850, loss is 5.500689687728882 and perplexity is 244.86075149684422
At time: 11.720598459243774 and batch: 900, loss is 5.412480125427246 and perplexity is 224.18691041785831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.223363588934076 and perplexity of 185.55727396744092
finished 1 epochs...
Completing Train Step...
At time: 13.30410885810852 and batch: 50, loss is 5.14933298110962 and perplexity is 172.31651360559857
At time: 13.936723947525024 and batch: 100, loss is 4.9836178970336915 and perplexity is 146.0016462067826
At time: 14.569328308105469 and batch: 150, loss is 4.937630529403687 and perplexity is 139.43946011612664
At time: 15.202879190444946 and batch: 200, loss is 4.8130846691131595 and perplexity is 123.11078845481192
At time: 15.843697786331177 and batch: 250, loss is 4.903257284164429 and perplexity is 134.7279128324505
At time: 16.483036994934082 and batch: 300, loss is 4.850502767562866 and perplexity is 127.80462971802008
At time: 17.115755558013916 and batch: 350, loss is 4.823202352523804 and perplexity is 124.36270701502525
At time: 17.749011039733887 and batch: 400, loss is 4.706207809448242 and perplexity is 110.63182645015611
At time: 18.380969762802124 and batch: 450, loss is 4.709671945571899 and perplexity is 111.01573472793905
At time: 19.013479232788086 and batch: 500, loss is 4.612644681930542 and perplexity is 100.75024997151286
At time: 19.646608352661133 and batch: 550, loss is 4.691360664367676 and perplexity is 109.00139326200443
At time: 20.279022455215454 and batch: 600, loss is 4.6558710479736325 and perplexity is 105.20081505804382
At time: 20.911622762680054 and batch: 650, loss is 4.5161789035797115 and perplexity is 91.48535488593205
At time: 21.557366371154785 and batch: 700, loss is 4.550292415618896 and perplexity is 94.6600843554252
At time: 22.18972873687744 and batch: 750, loss is 4.609217729568481 and perplexity is 100.40557459476535
At time: 22.822840929031372 and batch: 800, loss is 4.542167148590088 and perplexity is 93.89406217484321
At time: 23.455547094345093 and batch: 850, loss is 4.608820276260376 and perplexity is 100.36567599643062
At time: 24.09036159515381 and batch: 900, loss is 4.549931116104126 and perplexity is 94.62588989047325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.665976746441567 and perplexity of 106.26933273881832
finished 2 epochs...
Completing Train Step...
At time: 25.68117332458496 and batch: 50, loss is 4.551136903762817 and perplexity is 94.74005743777302
At time: 26.3198082447052 and batch: 100, loss is 4.405559496879578 and perplexity is 81.90495559300346
At time: 26.957496881484985 and batch: 150, loss is 4.3986600971221925 and perplexity is 81.34180549498879
At time: 27.594842195510864 and batch: 200, loss is 4.295495028495789 and perplexity is 73.36852496746377
At time: 28.233208417892456 and batch: 250, loss is 4.444564399719238 and perplexity is 85.16277282867854
At time: 28.877940893173218 and batch: 300, loss is 4.41464515209198 and perplexity is 82.65250663274203
At time: 29.524306297302246 and batch: 350, loss is 4.394072027206421 and perplexity is 80.96945843464609
At time: 30.162608861923218 and batch: 400, loss is 4.319564361572265 and perplexity is 75.15588036985498
At time: 30.79911732673645 and batch: 450, loss is 4.332476625442505 and perplexity is 76.13260522264459
At time: 31.434977054595947 and batch: 500, loss is 4.221576509475708 and perplexity is 68.14082431111626
At time: 32.07120156288147 and batch: 550, loss is 4.315906181335449 and perplexity is 74.88144887957574
At time: 32.706517934799194 and batch: 600, loss is 4.310857234001159 and perplexity is 74.50432921764678
At time: 33.34273815155029 and batch: 650, loss is 4.16821858882904 and perplexity is 64.6002699019791
At time: 33.979384660720825 and batch: 700, loss is 4.183345317840576 and perplexity is 65.58488893671488
At time: 34.6226167678833 and batch: 750, loss is 4.282050395011902 and perplexity is 72.3887134027426
At time: 35.26607656478882 and batch: 800, loss is 4.225212845802307 and perplexity is 68.38905832352971
At time: 35.90303039550781 and batch: 850, loss is 4.302618503570557 and perplexity is 73.89302975678912
At time: 36.53892993927002 and batch: 900, loss is 4.255405726432801 and perplexity is 70.48540918140833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.553659517471105 and perplexity of 94.97935170198778
finished 3 epochs...
Completing Train Step...
At time: 38.111326694488525 and batch: 50, loss is 4.294388809204102 and perplexity is 73.28740816450032
At time: 38.75851058959961 and batch: 100, loss is 4.152611308097839 and perplexity is 63.59986248116078
At time: 39.393975496292114 and batch: 150, loss is 4.149202523231506 and perplexity is 63.38343332222711
At time: 40.03207612037659 and batch: 200, loss is 4.04862913608551 and perplexity is 57.31882685076128
At time: 40.66925287246704 and batch: 250, loss is 4.207723569869995 and perplexity is 67.20338174639589
At time: 41.3060188293457 and batch: 300, loss is 4.180630493164062 and perplexity is 65.40707893252564
At time: 41.942375898361206 and batch: 350, loss is 4.162096109390259 and perplexity is 64.20596437321379
At time: 42.57913780212402 and batch: 400, loss is 4.099187922477722 and perplexity is 60.291306497033474
At time: 43.21614360809326 and batch: 450, loss is 4.119317150115966 and perplexity is 61.51722089176461
At time: 43.85309171676636 and batch: 500, loss is 4.005499529838562 and perplexity is 54.89924135987864
At time: 44.49060034751892 and batch: 550, loss is 4.0973015403747555 and perplexity is 60.17768125949427
At time: 45.12735986709595 and batch: 600, loss is 4.105247468948364 and perplexity is 60.657753600885826
At time: 45.76469826698303 and batch: 650, loss is 3.9592819690704344 and perplexity is 52.419673486019526
At time: 46.402082204818726 and batch: 700, loss is 3.970337209701538 and perplexity is 53.002400750291024
At time: 47.03923940658569 and batch: 750, loss is 4.077513675689698 and perplexity is 58.99859769191996
At time: 47.67694926261902 and batch: 800, loss is 4.026803317070008 and perplexity is 56.08135010544457
At time: 48.31385111808777 and batch: 850, loss is 4.106195621490478 and perplexity is 60.71529367823607
At time: 48.95120978355408 and batch: 900, loss is 4.06396101474762 and perplexity is 58.20440357918058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.503800901648116 and perplexity of 90.35992861999382
finished 4 epochs...
Completing Train Step...
At time: 50.5150945186615 and batch: 50, loss is 4.114748229980469 and perplexity is 61.23679473273985
At time: 51.158803939819336 and batch: 100, loss is 3.9750949859619142 and perplexity is 53.255175159539704
At time: 51.79160928726196 and batch: 150, loss is 3.9726889324188233 and perplexity is 53.12719438270099
At time: 52.424071073532104 and batch: 200, loss is 3.875909676551819 and perplexity is 48.2265489038508
At time: 53.05573344230652 and batch: 250, loss is 4.035864233970642 and perplexity is 56.59180767284023
At time: 53.71493983268738 and batch: 300, loss is 4.013073949813843 and perplexity is 55.31665008914751
At time: 54.354135274887085 and batch: 350, loss is 3.992808837890625 and perplexity is 54.20693421945629
At time: 54.98626255989075 and batch: 400, loss is 3.937635507583618 and perplexity is 51.297166030655134
At time: 55.61785554885864 and batch: 450, loss is 3.9590655946731568 and perplexity is 52.40833243776399
At time: 56.248878955841064 and batch: 500, loss is 3.8494678497314454 and perplexity is 46.96806251301992
At time: 56.88005518913269 and batch: 550, loss is 3.938586935997009 and perplexity is 51.345994836816544
At time: 57.511563777923584 and batch: 600, loss is 3.9525915145874024 and perplexity is 52.070132643942756
At time: 58.1436812877655 and batch: 650, loss is 3.8053129434585573 and perplexity is 44.939311376235764
At time: 58.775983572006226 and batch: 700, loss is 3.8129982471466066 and perplexity is 45.28601418340882
At time: 59.40798044204712 and batch: 750, loss is 3.924354395866394 and perplexity is 50.62038677621307
At time: 60.040329933166504 and batch: 800, loss is 3.877954683303833 and perplexity is 48.325273433744684
At time: 60.67164874076843 and batch: 850, loss is 3.9567877578735353 and perplexity is 52.28909066773392
At time: 61.302695989608765 and batch: 900, loss is 3.9169351768493654 and perplexity is 50.24621279570932
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.490555854692851 and perplexity of 89.17099822325977
finished 5 epochs...
Completing Train Step...
At time: 62.881208419799805 and batch: 50, loss is 3.9719754219055177 and perplexity is 53.08930109120505
At time: 63.51409578323364 and batch: 100, loss is 3.8370047092437742 and perplexity is 46.386325615897874
At time: 64.15410137176514 and batch: 150, loss is 3.8392395687103273 and perplexity is 46.490108461660725
At time: 64.80752325057983 and batch: 200, loss is 3.7382203722000122 and perplexity is 42.02313803559604
At time: 65.44487619400024 and batch: 250, loss is 3.897623825073242 and perplexity is 49.28519960247775
At time: 66.08709931373596 and batch: 300, loss is 3.8843407344818117 and perplexity is 48.63486859620896
At time: 66.72289299964905 and batch: 350, loss is 3.857715492248535 and perplexity is 47.35704017171621
At time: 67.35521650314331 and batch: 400, loss is 3.8083333015441894 and perplexity is 45.07524937609032
At time: 67.98662972450256 and batch: 450, loss is 3.833621220588684 and perplexity is 46.22964322544536
At time: 68.61864447593689 and batch: 500, loss is 3.7206637907028197 and perplexity is 41.29179412270881
At time: 69.24984335899353 and batch: 550, loss is 3.8055285310745237 and perplexity is 44.949000779658704
At time: 69.89746761322021 and batch: 600, loss is 3.820938992500305 and perplexity is 45.64705044151151
At time: 70.54057621955872 and batch: 650, loss is 3.6790185165405274 and perplexity is 39.607500882196334
At time: 71.17150855064392 and batch: 700, loss is 3.6857203388214113 and perplexity is 39.87383477846686
At time: 71.80327367782593 and batch: 750, loss is 3.8005782842636107 and perplexity is 44.72704196062069
At time: 72.43555426597595 and batch: 800, loss is 3.7554075717926025 and perplexity is 42.75164062300078
At time: 73.06808924674988 and batch: 850, loss is 3.8316915321350096 and perplexity is 46.14052043396209
At time: 73.69957399368286 and batch: 900, loss is 3.7964284086227416 and perplexity is 44.54181489915879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493310797704409 and perplexity of 89.41699794366501
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.2749559879303 and batch: 50, loss is 3.863360896110535 and perplexity is 47.625145859580634
At time: 75.90797591209412 and batch: 100, loss is 3.7240933704376222 and perplexity is 41.43365073828941
At time: 76.53997826576233 and batch: 150, loss is 3.7239360427856445 and perplexity is 41.42713259206165
At time: 77.17927694320679 and batch: 200, loss is 3.607805151939392 and perplexity is 36.88500691974197
At time: 77.81852984428406 and batch: 250, loss is 3.756366033554077 and perplexity is 42.79263607892832
At time: 78.45411658287048 and batch: 300, loss is 3.735716209411621 and perplexity is 41.9180369071659
At time: 79.08664178848267 and batch: 350, loss is 3.6917076826095583 and perplexity is 40.11328926827771
At time: 79.71923542022705 and batch: 400, loss is 3.635710620880127 and perplexity is 37.92879631735101
At time: 80.35232520103455 and batch: 450, loss is 3.642924680709839 and perplexity is 38.20340625857465
At time: 80.98437929153442 and batch: 500, loss is 3.5219544696807863 and perplexity is 33.8505236744674
At time: 81.61597800254822 and batch: 550, loss is 3.584271674156189 and perplexity is 36.02710868676098
At time: 82.24942898750305 and batch: 600, loss is 3.590755276679993 and perplexity is 36.26145301638219
At time: 82.8827075958252 and batch: 650, loss is 3.4305661487579346 and perplexity is 30.894128471911877
At time: 83.5146152973175 and batch: 700, loss is 3.4280023050308226 and perplexity is 30.815022205809026
At time: 84.14739751815796 and batch: 750, loss is 3.521117367744446 and perplexity is 33.822199192446746
At time: 84.77938580513 and batch: 800, loss is 3.455066318511963 and perplexity is 31.66038828249196
At time: 85.42425203323364 and batch: 850, loss is 3.5160103368759157 and perplexity is 33.64990849854568
At time: 86.055979013443 and batch: 900, loss is 3.466891260147095 and perplexity is 32.03699280095299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4300645802119005 and perplexity of 83.93683739398412
finished 7 epochs...
Completing Train Step...
At time: 87.61634111404419 and batch: 50, loss is 3.7535423755645754 and perplexity is 42.67197474351393
At time: 88.25957655906677 and batch: 100, loss is 3.614905109405518 and perplexity is 37.147820779600075
At time: 88.89028859138489 and batch: 150, loss is 3.615868263244629 and perplexity is 37.18361708170421
At time: 89.52004051208496 and batch: 200, loss is 3.5049127626419065 and perplexity is 33.27854059380668
At time: 90.15078711509705 and batch: 250, loss is 3.6561165285110473 and perplexity is 38.71071861607105
At time: 90.781653881073 and batch: 300, loss is 3.6439045095443725 and perplexity is 38.24085740246325
At time: 91.41269612312317 and batch: 350, loss is 3.6040473318099977 and perplexity is 36.74665980285882
At time: 92.04270792007446 and batch: 400, loss is 3.5553570318222047 and perplexity is 35.00031396305586
At time: 92.67365741729736 and batch: 450, loss is 3.5667908430099486 and perplexity is 35.402797520138364
At time: 93.30476260185242 and batch: 500, loss is 3.4504702949523924 and perplexity is 31.51521026838181
At time: 93.93464779853821 and batch: 550, loss is 3.5180497980117797 and perplexity is 33.718606208496844
At time: 94.56486511230469 and batch: 600, loss is 3.5308746337890624 and perplexity is 34.15382664580236
At time: 95.19559621810913 and batch: 650, loss is 3.3769304847717283 and perplexity is 29.28075530721171
At time: 95.82553672790527 and batch: 700, loss is 3.3807350540161134 and perplexity is 29.39236815307561
At time: 96.45654201507568 and batch: 750, loss is 3.4816073179244995 and perplexity is 32.511937120828115
At time: 97.08685159683228 and batch: 800, loss is 3.4226079750061036 and perplexity is 30.649243341262018
At time: 97.7176718711853 and batch: 850, loss is 3.4928780126571657 and perplexity is 32.88044199135757
At time: 98.34764170646667 and batch: 900, loss is 3.4529197549819948 and perplexity is 31.59250013683647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.436492501872859 and perplexity of 84.47811458921088
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 99.90775966644287 and batch: 50, loss is 3.708916983604431 and perplexity is 40.809585135268634
At time: 100.55141949653625 and batch: 100, loss is 3.576825098991394 and perplexity is 35.75982651743506
At time: 101.18383741378784 and batch: 150, loss is 3.580913486480713 and perplexity is 35.90632581367393
At time: 101.82886290550232 and batch: 200, loss is 3.4622509241104127 and perplexity is 31.88867477762682
At time: 102.46053862571716 and batch: 250, loss is 3.614513273239136 and perplexity is 37.13326777130004
At time: 103.09225296974182 and batch: 300, loss is 3.6000232458114625 and perplexity is 36.59908520922406
At time: 103.72383999824524 and batch: 350, loss is 3.5554422998428343 and perplexity is 35.003298497789785
At time: 104.35609698295593 and batch: 400, loss is 3.5029920387268065 and perplexity is 33.21468305101516
At time: 104.99025559425354 and batch: 450, loss is 3.5082795286178587 and perplexity is 33.390770471863206
At time: 105.62322998046875 and batch: 500, loss is 3.387918167114258 and perplexity is 29.604256957293806
At time: 106.26008033752441 and batch: 550, loss is 3.447492127418518 and perplexity is 31.42149231548874
At time: 106.89231300354004 and batch: 600, loss is 3.4596703243255615 and perplexity is 31.80648896031739
At time: 107.52534651756287 and batch: 650, loss is 3.2976732635498047 and perplexity is 27.049628288414336
At time: 108.1581346988678 and batch: 700, loss is 3.294265556335449 and perplexity is 26.95760795317898
At time: 108.791677236557 and batch: 750, loss is 3.390409536361694 and perplexity is 29.678104044654724
At time: 109.4248640537262 and batch: 800, loss is 3.3254885244369508 and perplexity is 27.812582450570265
At time: 110.05784964561462 and batch: 850, loss is 3.3899517965316774 and perplexity is 29.6645223030433
At time: 110.69103789329529 and batch: 900, loss is 3.3458881855010985 and perplexity is 28.385776308529174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.427390320660317 and perplexity of 83.71266838151335
finished 9 epochs...
Completing Train Step...
At time: 112.2737443447113 and batch: 50, loss is 3.678201265335083 and perplexity is 39.57514482766888
At time: 112.90920925140381 and batch: 100, loss is 3.5429188060760497 and perplexity is 34.56766840619818
At time: 113.54471254348755 and batch: 150, loss is 3.545905156135559 and perplexity is 34.67105386054392
At time: 114.18149137496948 and batch: 200, loss is 3.4292422103881837 and perplexity is 30.853253613694967
At time: 114.8198471069336 and batch: 250, loss is 3.5816135931015016 and perplexity is 35.931472871888104
At time: 115.45666098594666 and batch: 300, loss is 3.5686140203475953 and perplexity is 35.46740197305831
At time: 116.09333944320679 and batch: 350, loss is 3.5262283420562746 and perplexity is 33.995506089978655
At time: 116.73113512992859 and batch: 400, loss is 3.476398448944092 and perplexity is 32.343026997327925
At time: 117.36795783042908 and batch: 450, loss is 3.4842747020721436 and perplexity is 32.59877470963031
At time: 118.02636337280273 and batch: 500, loss is 3.366175608634949 and perplexity is 28.967531770003365
At time: 118.66244649887085 and batch: 550, loss is 3.4280647945404055 and perplexity is 30.816947881601095
At time: 119.30327939987183 and batch: 600, loss is 3.4436620616912843 and perplexity is 31.301376107993516
At time: 119.94860887527466 and batch: 650, loss is 3.284634385108948 and perplexity is 26.699220897430543
At time: 120.5865364074707 and batch: 700, loss is 3.2843860960006714 and perplexity is 26.692592594584
At time: 121.22260594367981 and batch: 750, loss is 3.3837015867233275 and perplexity is 29.47969103359569
At time: 121.85919523239136 and batch: 800, loss is 3.3224974203109743 and perplexity is 27.7295164119643
At time: 122.49665117263794 and batch: 850, loss is 3.3907538557052614 and perplexity is 29.688324549415473
At time: 123.13926529884338 and batch: 900, loss is 3.3498685503005983 and perplexity is 28.49898721423659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4295850779912245 and perplexity of 83.89659914199375
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.75459504127502 and batch: 50, loss is 3.663897252082825 and perplexity is 39.013090828731116
At time: 125.39204597473145 and batch: 100, loss is 3.5305655574798585 and perplexity is 34.14327213827562
At time: 126.02916932106018 and batch: 150, loss is 3.535211458206177 and perplexity is 34.30226744331865
At time: 126.66571426391602 and batch: 200, loss is 3.4164259243011474 and perplexity is 30.46035263240883
At time: 127.3026955127716 and batch: 250, loss is 3.568444666862488 and perplexity is 35.46139595351099
At time: 127.94081544876099 and batch: 300, loss is 3.553956332206726 and perplexity is 34.951323355318685
At time: 128.58505654335022 and batch: 350, loss is 3.511798005104065 and perplexity is 33.50846203871479
At time: 129.22835421562195 and batch: 400, loss is 3.46226496219635 and perplexity is 31.889122436725906
At time: 129.86493253707886 and batch: 450, loss is 3.467254123687744 and perplexity is 32.04861996700224
At time: 130.50273513793945 and batch: 500, loss is 3.345698847770691 and perplexity is 28.380402318832623
At time: 131.1399221420288 and batch: 550, loss is 3.4052343273162844 and perplexity is 30.12135315285845
At time: 131.7776584625244 and batch: 600, loss is 3.4215324926376343 and perplexity is 30.616298339501345
At time: 132.41421103477478 and batch: 650, loss is 3.261800556182861 and perplexity is 26.096483049369166
At time: 133.05134224891663 and batch: 700, loss is 3.257034912109375 and perplexity is 25.972412372746245
At time: 133.70159077644348 and batch: 750, loss is 3.354602165222168 and perplexity is 28.63421023973279
At time: 134.35113787651062 and batch: 800, loss is 3.2927567911148072 and perplexity is 26.916965919220893
At time: 134.99703192710876 and batch: 850, loss is 3.357631883621216 and perplexity is 28.72109538563961
At time: 135.6404631137848 and batch: 900, loss is 3.3156942033767702 and perplexity is 27.54150675671725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42902071182042 and perplexity of 83.8492640979976
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.2437241077423 and batch: 50, loss is 3.656453914642334 and perplexity is 38.723781279121006
At time: 137.88784193992615 and batch: 100, loss is 3.5232543897628785 and perplexity is 33.894555262553624
At time: 138.51943278312683 and batch: 150, loss is 3.529082040786743 and perplexity is 34.09265757715688
At time: 139.15103840827942 and batch: 200, loss is 3.409942660331726 and perplexity is 30.263508910086674
At time: 139.7822825908661 and batch: 250, loss is 3.5615417432785033 and perplexity is 35.217451580414476
At time: 140.41405129432678 and batch: 300, loss is 3.5470932006835936 and perplexity is 34.71226909497706
At time: 141.04565405845642 and batch: 350, loss is 3.5050296354293824 and perplexity is 33.28243017689779
At time: 141.67587399482727 and batch: 400, loss is 3.455995354652405 and perplexity is 31.68981559482858
At time: 142.30739855766296 and batch: 450, loss is 3.46116379737854 and perplexity is 31.854026583733365
At time: 142.93957352638245 and batch: 500, loss is 3.339253225326538 and perplexity is 28.198061242833738
At time: 143.57185554504395 and batch: 550, loss is 3.3979881811141968 and perplexity is 29.903878302958827
At time: 144.20419478416443 and batch: 600, loss is 3.4148516130447386 and perplexity is 30.4124362839054
At time: 144.83653593063354 and batch: 650, loss is 3.25527352809906 and perplexity is 25.926705246601248
At time: 145.46867084503174 and batch: 700, loss is 3.249645118713379 and perplexity is 25.781189032015842
At time: 146.10022902488708 and batch: 750, loss is 3.3468845224380495 and perplexity is 28.414072199729425
At time: 146.73082518577576 and batch: 800, loss is 3.284943208694458 and perplexity is 26.707467519868626
At time: 147.36105585098267 and batch: 850, loss is 3.348939800262451 and perplexity is 28.47253106625014
At time: 147.9925549030304 and batch: 900, loss is 3.3063351774215697 and perplexity is 27.28494752508483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428803326332406 and perplexity of 83.83103846586778
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.5657992362976 and batch: 50, loss is 3.6544919395446778 and perplexity is 38.647880666467564
At time: 150.20930528640747 and batch: 100, loss is 3.5213090229034423 and perplexity is 33.8286820126237
At time: 150.84096956253052 and batch: 150, loss is 3.5272500467300416 and perplexity is 34.030257207104256
At time: 151.4739329814911 and batch: 200, loss is 3.408200659751892 and perplexity is 30.210835751673752
At time: 152.10979795455933 and batch: 250, loss is 3.559786710739136 and perplexity is 35.15569801254856
At time: 152.74204659461975 and batch: 300, loss is 3.545232696533203 and perplexity is 34.64774681485323
At time: 153.3800003528595 and batch: 350, loss is 3.503227653503418 and perplexity is 33.222509843160175
At time: 154.02186942100525 and batch: 400, loss is 3.454140205383301 and perplexity is 31.631080754383
At time: 154.65513348579407 and batch: 450, loss is 3.459477577209473 and perplexity is 31.800358942088025
At time: 155.29002904891968 and batch: 500, loss is 3.3375672912597656 and perplexity is 28.150561222969404
At time: 155.92507910728455 and batch: 550, loss is 3.396029443740845 and perplexity is 29.845361786871123
At time: 156.5600917339325 and batch: 600, loss is 3.4130540466308594 and perplexity is 30.357817015465812
At time: 157.1945412158966 and batch: 650, loss is 3.2534841108322143 and perplexity is 25.880353036651353
At time: 157.8408694267273 and batch: 700, loss is 3.2476942920684815 and perplexity is 25.73094342768199
At time: 158.47594356536865 and batch: 750, loss is 3.3449510955810546 and perplexity is 28.35918874309324
At time: 159.11123323440552 and batch: 800, loss is 3.282906312942505 and perplexity is 26.653122559031434
At time: 159.74480032920837 and batch: 850, loss is 3.346732540130615 and perplexity is 28.409754091619234
At time: 160.3798611164093 and batch: 900, loss is 3.3039343404769896 and perplexity is 27.219519387607924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428721388725386 and perplexity of 83.82416983258538
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 161.96909928321838 and batch: 50, loss is 3.6539821100234984 and perplexity is 38.62818185791653
At time: 162.60360455513 and batch: 100, loss is 3.5208229780197144 and perplexity is 33.81224374999855
At time: 163.23585867881775 and batch: 150, loss is 3.5267784643173217 and perplexity is 34.01421291970427
At time: 163.86955857276917 and batch: 200, loss is 3.407760891914368 and perplexity is 30.197552918660882
At time: 164.50319719314575 and batch: 250, loss is 3.5593359088897705 and perplexity is 35.13985333054308
At time: 165.137788772583 and batch: 300, loss is 3.5447603273391723 and perplexity is 34.63138415152861
At time: 165.77333879470825 and batch: 350, loss is 3.5027583026885987 and perplexity is 33.20692048981898
At time: 166.4298779964447 and batch: 400, loss is 3.4536790657043457 and perplexity is 31.61649777061357
At time: 167.06497430801392 and batch: 450, loss is 3.4590386819839476 and perplexity is 31.786404978766292
At time: 167.70666337013245 and batch: 500, loss is 3.3371381664276125 and perplexity is 28.138483709675352
At time: 168.34088349342346 and batch: 550, loss is 3.3955313396453857 and perplexity is 29.83049939175185
At time: 168.9733738899231 and batch: 600, loss is 3.412597918510437 and perplexity is 30.34397311898568
At time: 169.61454010009766 and batch: 650, loss is 3.2530207204818726 and perplexity is 25.868363109013664
At time: 170.2547664642334 and batch: 700, loss is 3.2471944999694826 and perplexity is 25.71808651861546
At time: 170.89386916160583 and batch: 750, loss is 3.344452295303345 and perplexity is 28.345046699193492
At time: 171.53352618217468 and batch: 800, loss is 3.282394137382507 and perplexity is 26.63947497633657
At time: 172.17262196540833 and batch: 850, loss is 3.3461810731887818 and perplexity is 28.394091370545475
At time: 172.80698657035828 and batch: 900, loss is 3.3033282279968263 and perplexity is 27.203026296042292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428698396029538 and perplexity of 83.82224251110098
Annealing...
Model not improving. Stopping early with 83.71266838151335 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
554.4561202526093


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.71266838151335, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.2283769027621978, 'rnn_dropout': 0.6017017728864293, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8699207305908203 and batch: 50, loss is 6.527928009033203 and perplexity is 683.9795428308103
At time: 1.507298469543457 and batch: 100, loss is 5.684654865264893 and perplexity is 294.3162487590376
At time: 2.144798517227173 and batch: 150, loss is 5.495515909194946 and perplexity is 243.59716776676686
At time: 2.783918857574463 and batch: 200, loss is 5.294820489883423 and perplexity is 199.30184605761926
At time: 3.4221272468566895 and batch: 250, loss is 5.333368759155274 and perplexity is 207.1345866728455
At time: 4.0728254318237305 and batch: 300, loss is 5.255904111862183 and perplexity is 191.69472100177632
At time: 4.716636657714844 and batch: 350, loss is 5.220366945266724 and perplexity is 185.00205724629228
At time: 5.357216835021973 and batch: 400, loss is 5.069450273513794 and perplexity is 159.08684904640378
At time: 5.996049642562866 and batch: 450, loss is 5.0676953411102295 and perplexity is 158.80790721365196
At time: 6.634625196456909 and batch: 500, loss is 4.991140871047974 and perplexity is 147.1041546687804
At time: 7.273693323135376 and batch: 550, loss is 5.060702524185181 and perplexity is 157.70126637017745
At time: 7.912137746810913 and batch: 600, loss is 4.992685594558716 and perplexity is 147.33156551324404
At time: 8.552120923995972 and batch: 650, loss is 4.868679141998291 and perplexity is 130.14889510377677
At time: 9.19118595123291 and batch: 700, loss is 4.93657169342041 and perplexity is 139.291894735821
At time: 9.829444169998169 and batch: 750, loss is 4.950599842071533 and perplexity is 141.25967200748107
At time: 10.468461751937866 and batch: 800, loss is 4.891591548919678 and perplexity is 133.16534464535977
At time: 11.107284307479858 and batch: 850, loss is 4.945978908538819 and perplexity is 140.60842629270073
At time: 11.745710134506226 and batch: 900, loss is 4.87844485282898 and perplexity is 131.42611792203138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.839815740715967 and perplexity of 126.44605072469214
finished 1 epochs...
Completing Train Step...
At time: 13.33055305480957 and batch: 50, loss is 4.817839212417603 and perplexity is 123.69751773937598
At time: 13.976002931594849 and batch: 100, loss is 4.658508100509644 and perplexity is 105.47860124158481
At time: 14.608514785766602 and batch: 150, loss is 4.6393964481353756 and perplexity is 103.48187205473323
At time: 15.240718126296997 and batch: 200, loss is 4.526358814239502 and perplexity is 92.42142409202573
At time: 15.873955249786377 and batch: 250, loss is 4.65951455116272 and perplexity is 105.58481368851764
At time: 16.507009983062744 and batch: 300, loss is 4.622666625976563 and perplexity is 101.76503992971595
At time: 17.13990879058838 and batch: 350, loss is 4.5980838012695315 and perplexity is 99.29386645012355
At time: 17.772916078567505 and batch: 400, loss is 4.507487573623657 and perplexity is 90.69367085762018
At time: 18.40526270866394 and batch: 450, loss is 4.521357622146606 and perplexity is 91.96036069096684
At time: 19.037708520889282 and batch: 500, loss is 4.416553010940552 and perplexity is 82.81034646906505
At time: 19.67009663581848 and batch: 550, loss is 4.5022126483917235 and perplexity is 90.2165280774496
At time: 20.303805828094482 and batch: 600, loss is 4.485596199035644 and perplexity is 88.72983568890866
At time: 20.937644243240356 and batch: 650, loss is 4.339641466140747 and perplexity is 76.6800420178376
At time: 21.57168698310852 and batch: 700, loss is 4.36144268989563 and perplexity is 78.37011667976108
At time: 22.204920768737793 and batch: 750, loss is 4.451253519058228 and perplexity is 85.73434630976134
At time: 22.837870359420776 and batch: 800, loss is 4.388864974975586 and perplexity is 80.54894201110034
At time: 23.483773231506348 and batch: 850, loss is 4.454311628341674 and perplexity is 85.99693261428628
At time: 24.11640238761902 and batch: 900, loss is 4.408733940124511 and perplexity is 82.16537134483836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.587399417406892 and perplexity of 98.23862003320397
finished 2 epochs...
Completing Train Step...
At time: 25.710546731948853 and batch: 50, loss is 4.4398845672607425 and perplexity is 84.76515643486954
At time: 26.346047401428223 and batch: 100, loss is 4.284544477462768 and perplexity is 72.56948215486211
At time: 26.983060598373413 and batch: 150, loss is 4.282240839004516 and perplexity is 72.40250071115658
At time: 27.619029760360718 and batch: 200, loss is 4.1741596031188966 and perplexity is 64.98520334392927
At time: 28.255246877670288 and batch: 250, loss is 4.328538684844971 and perplexity is 75.83338908011122
At time: 28.891368865966797 and batch: 300, loss is 4.300305075645447 and perplexity is 73.72228114173907
At time: 29.529050827026367 and batch: 350, loss is 4.283842582702636 and perplexity is 72.51856388731515
At time: 30.16637897491455 and batch: 400, loss is 4.212680072784424 and perplexity is 67.53730235951087
At time: 30.8133065700531 and batch: 450, loss is 4.231605505943298 and perplexity is 68.8276467103884
At time: 31.467571258544922 and batch: 500, loss is 4.126449790000915 and perplexity is 61.95756963272781
At time: 32.109450817108154 and batch: 550, loss is 4.212874078750611 and perplexity is 67.55040627018613
At time: 32.75651407241821 and batch: 600, loss is 4.213247742652893 and perplexity is 67.57565213503425
At time: 33.39696502685547 and batch: 650, loss is 4.068559579849243 and perplexity is 58.47267668114311
At time: 34.036784410476685 and batch: 700, loss is 4.079199919700622 and perplexity is 59.09816764996037
At time: 34.679760456085205 and batch: 750, loss is 4.188089284896851 and perplexity is 65.89676065891301
At time: 35.31481313705444 and batch: 800, loss is 4.1329749488830565 and perplexity is 62.363174497483186
At time: 35.95012164115906 and batch: 850, loss is 4.20658748626709 and perplexity is 67.12707643914537
At time: 36.58592891693115 and batch: 900, loss is 4.1669207954406735 and perplexity is 64.51648647736035
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.509604676129067 and perplexity of 90.88588204902224
finished 3 epochs...
Completing Train Step...
At time: 38.15776801109314 and batch: 50, loss is 4.2196284198760985 and perplexity is 68.0082090950687
At time: 38.80596876144409 and batch: 100, loss is 4.068731169700623 and perplexity is 58.48271085990163
At time: 39.44310402870178 and batch: 150, loss is 4.072470669746399 and perplexity is 58.70181637705669
At time: 40.07958769798279 and batch: 200, loss is 3.964305195808411 and perplexity is 52.683651847977494
At time: 40.716070890426636 and batch: 250, loss is 4.1214764070510865 and perplexity is 61.65019588953242
At time: 41.353745222091675 and batch: 300, loss is 4.105550799369812 and perplexity is 60.67615573367193
At time: 42.00895023345947 and batch: 350, loss is 4.0872856760025025 and perplexity is 59.577958151969256
At time: 42.65529775619507 and batch: 400, loss is 4.021996874809265 and perplexity is 55.81244509015728
At time: 43.29893469810486 and batch: 450, loss is 4.04347249507904 and perplexity is 57.0240150104692
At time: 43.93547487258911 and batch: 500, loss is 3.9395469665527343 and perplexity is 51.39531223008841
At time: 44.572516441345215 and batch: 550, loss is 4.024350347518921 and perplexity is 55.94395284579321
At time: 45.20978760719299 and batch: 600, loss is 4.034155778884887 and perplexity is 56.495205654810995
At time: 45.84680676460266 and batch: 650, loss is 3.886409206390381 and perplexity is 48.7355725714433
At time: 46.484065532684326 and batch: 700, loss is 3.8930469942092896 and perplexity is 49.06014499110536
At time: 47.13338351249695 and batch: 750, loss is 4.010571188926697 and perplexity is 55.1783788430052
At time: 47.77080583572388 and batch: 800, loss is 3.956783037185669 and perplexity is 52.288843827840694
At time: 48.40831184387207 and batch: 850, loss is 4.035701532363891 and perplexity is 56.58260084380617
At time: 49.04620838165283 and batch: 900, loss is 3.996449337005615 and perplexity is 54.404634160391645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.482414350117723 and perplexity of 88.44795943886854
finished 4 epochs...
Completing Train Step...
At time: 50.63735485076904 and batch: 50, loss is 4.0542955493927 and perplexity is 57.6445409578479
At time: 51.268754959106445 and batch: 100, loss is 3.9073833417892456 and perplexity is 49.768554148689695
At time: 51.90119743347168 and batch: 150, loss is 3.9156157541275025 and perplexity is 50.17996051785091
At time: 52.53240990638733 and batch: 200, loss is 3.811080174446106 and perplexity is 45.199235566342225
At time: 53.16334843635559 and batch: 250, loss is 3.9629744720458984 and perplexity is 52.61359108666619
At time: 53.795334815979004 and batch: 300, loss is 3.9570228624343873 and perplexity is 52.30138551666361
At time: 54.42675757408142 and batch: 350, loss is 3.9357400846481325 and perplexity is 51.200028293264545
At time: 55.080029249191284 and batch: 400, loss is 3.8733469915390013 and perplexity is 48.103117674986514
At time: 55.71272253990173 and batch: 450, loss is 3.897071008682251 and perplexity is 49.25796146584211
At time: 56.34450435638428 and batch: 500, loss is 3.793703637123108 and perplexity is 44.420613828977366
At time: 56.9752471446991 and batch: 550, loss is 3.8796584033966064 and perplexity is 48.40767634890133
At time: 57.60686159133911 and batch: 600, loss is 3.8930103969573975 and perplexity is 49.058349557475424
At time: 58.2390193939209 and batch: 650, loss is 3.74800332069397 and perplexity is 42.43626573917798
At time: 58.87145972251892 and batch: 700, loss is 3.7500240182876587 and perplexity is 42.52210329590666
At time: 59.50366926193237 and batch: 750, loss is 3.866936206817627 and perplexity is 47.79572530905774
At time: 60.13590693473816 and batch: 800, loss is 3.8154433012008666 and perplexity is 45.39687641283046
At time: 60.76728057861328 and batch: 850, loss is 3.897643485069275 and perplexity is 49.28616855883123
At time: 61.399744749069214 and batch: 900, loss is 3.859701099395752 and perplexity is 47.451166066751824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.47889186911387 and perplexity of 88.1369512636892
finished 5 epochs...
Completing Train Step...
At time: 62.973238468170166 and batch: 50, loss is 3.9236342334747314 and perplexity is 50.583945000978424
At time: 63.60621905326843 and batch: 100, loss is 3.7798540210723877 and perplexity is 43.80964598362306
At time: 64.23896431922913 and batch: 150, loss is 3.7900135135650634 and perplexity is 44.25699834176896
At time: 64.87617373466492 and batch: 200, loss is 3.68469069480896 and perplexity is 39.83280005243902
At time: 65.51120710372925 and batch: 250, loss is 3.833738536834717 and perplexity is 46.235067031788084
At time: 66.14333200454712 and batch: 300, loss is 3.8361415672302246 and perplexity is 46.34630490366834
At time: 66.78310871124268 and batch: 350, loss is 3.8141615438461303 and perplexity is 45.33872590798619
At time: 67.42967486381531 and batch: 400, loss is 3.7515466928482057 and perplexity is 42.58689994043795
At time: 68.06841015815735 and batch: 450, loss is 3.7786604738235474 and perplexity is 43.7573882934015
At time: 68.71085047721863 and batch: 500, loss is 3.6744122982025145 and perplexity is 39.42547962197566
At time: 69.34905433654785 and batch: 550, loss is 3.7585130405426024 and perplexity is 42.88461086757331
At time: 69.98088645935059 and batch: 600, loss is 3.775134258270264 and perplexity is 43.60336203472408
At time: 70.62651872634888 and batch: 650, loss is 3.632445216178894 and perplexity is 37.805145442490996
At time: 71.25853300094604 and batch: 700, loss is 3.6292744159698485 and perplexity is 37.685462724686154
At time: 71.89089894294739 and batch: 750, loss is 3.7496117639541624 and perplexity is 42.50457698745007
At time: 72.52438879013062 and batch: 800, loss is 3.698315658569336 and perplexity is 40.37923463199948
At time: 73.15745091438293 and batch: 850, loss is 3.784198217391968 and perplexity is 44.00037767446398
At time: 73.78980255126953 and batch: 900, loss is 3.745486135482788 and perplexity is 42.329580128694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.489195523196703 and perplexity of 89.04977857398025
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.35406970977783 and batch: 50, loss is 3.8244722652435303 and perplexity is 45.80861918577974
At time: 75.9993085861206 and batch: 100, loss is 3.680060262680054 and perplexity is 39.648783342525064
At time: 76.63167905807495 and batch: 150, loss is 3.68759729385376 and perplexity is 39.94874645424418
At time: 77.26456904411316 and batch: 200, loss is 3.566649904251099 and perplexity is 35.39780824539542
At time: 77.89748573303223 and batch: 250, loss is 3.706391444206238 and perplexity is 40.706648959582765
At time: 78.53020334243774 and batch: 300, loss is 3.6971601963043215 and perplexity is 40.33260489473088
At time: 79.16195106506348 and batch: 350, loss is 3.656944317817688 and perplexity is 38.742776201626484
At time: 79.79479169845581 and batch: 400, loss is 3.585749502182007 and perplexity is 36.08038991821895
At time: 80.43608951568604 and batch: 450, loss is 3.59525794506073 and perplexity is 36.42509444939933
At time: 81.07491779327393 and batch: 500, loss is 3.484089455604553 and perplexity is 32.592736461066956
At time: 81.70649790763855 and batch: 550, loss is 3.541505899429321 and perplexity is 34.51886200530864
At time: 82.33893465995789 and batch: 600, loss is 3.542555294036865 and perplexity is 34.555104926192456
At time: 82.97232675552368 and batch: 650, loss is 3.389417061805725 and perplexity is 29.648663893238158
At time: 83.60525727272034 and batch: 700, loss is 3.3691925859451293 and perplexity is 29.05505812220488
At time: 84.23859763145447 and batch: 750, loss is 3.4686498260498047 and perplexity is 32.09338153123519
At time: 84.87191414833069 and batch: 800, loss is 3.399917106628418 and perplexity is 29.961616325061144
At time: 85.50652885437012 and batch: 850, loss is 3.46692822933197 and perplexity is 32.03817720435577
At time: 86.14051270484924 and batch: 900, loss is 3.4136890983581543 and perplexity is 30.377101922406997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.432343783443922 and perplexity of 84.12836468695906
finished 7 epochs...
Completing Train Step...
At time: 87.74818801879883 and batch: 50, loss is 3.714114255905151 and perplexity is 41.02223578469103
At time: 88.39185667037964 and batch: 100, loss is 3.5670045518875124 and perplexity is 35.41036422076576
At time: 89.02466058731079 and batch: 150, loss is 3.577968912124634 and perplexity is 35.80075247801169
At time: 89.66600203514099 and batch: 200, loss is 3.463382906913757 and perplexity is 31.924792647643674
At time: 90.29952645301819 and batch: 250, loss is 3.6050172328948973 and perplexity is 36.78231771759784
At time: 90.93079447746277 and batch: 300, loss is 3.6035573434829713 and perplexity is 36.72865877900415
At time: 91.56227946281433 and batch: 350, loss is 3.5679294157028196 and perplexity is 35.44312913452645
At time: 92.19422054290771 and batch: 400, loss is 3.5012762212753294 and perplexity is 33.15774158274321
At time: 92.82502126693726 and batch: 450, loss is 3.518052830696106 and perplexity is 33.718708466540456
At time: 93.45571947097778 and batch: 500, loss is 3.4117715835571287 and perplexity is 30.318909190401545
At time: 94.08540940284729 and batch: 550, loss is 3.473802719116211 and perplexity is 32.259182103850065
At time: 94.71584057807922 and batch: 600, loss is 3.4829015922546387 and perplexity is 32.55404372934007
At time: 95.34655022621155 and batch: 650, loss is 3.335768461227417 and perplexity is 28.099968665362315
At time: 95.97803926467896 and batch: 700, loss is 3.3218425989151 and perplexity is 27.711364475107427
At time: 96.60881018638611 and batch: 750, loss is 3.429219226837158 and perplexity is 30.85254450451519
At time: 97.24589729309082 and batch: 800, loss is 3.3669567632675172 and perplexity is 28.99016873197234
At time: 97.87669134140015 and batch: 850, loss is 3.444410619735718 and perplexity is 31.32481577676279
At time: 98.50796604156494 and batch: 900, loss is 3.399813675880432 and perplexity is 29.958517532931733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.439329382491438 and perplexity of 84.71810917221949
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.08092594146729 and batch: 50, loss is 3.669966850280762 and perplexity is 39.25060469221247
At time: 100.71319603919983 and batch: 100, loss is 3.5283362865448 and perplexity is 34.06724231109872
At time: 101.34646105766296 and batch: 150, loss is 3.543093481063843 and perplexity is 34.573707040638915
At time: 101.97886204719543 and batch: 200, loss is 3.426291127204895 and perplexity is 30.762337312571198
At time: 102.62445116043091 and batch: 250, loss is 3.566683292388916 and perplexity is 35.39899013202593
At time: 103.26774621009827 and batch: 300, loss is 3.5610320138931275 and perplexity is 35.19950478485901
At time: 103.90303468704224 and batch: 350, loss is 3.519389443397522 and perplexity is 33.763807453777524
At time: 104.53402376174927 and batch: 400, loss is 3.4518558168411255 and perplexity is 31.558905545428026
At time: 105.16626000404358 and batch: 450, loss is 3.4622192859649656 and perplexity is 31.887665895055765
At time: 105.7989194393158 and batch: 500, loss is 3.3509829950332644 and perplexity is 28.530765464686713
At time: 106.43144059181213 and batch: 550, loss is 3.406167459487915 and perplexity is 30.14947347448638
At time: 107.06590723991394 and batch: 600, loss is 3.4108156108856202 and perplexity is 30.289938991351146
At time: 107.69929385185242 and batch: 650, loss is 3.2569797801971436 and perplexity is 25.97098050345808
At time: 108.33279418945312 and batch: 700, loss is 3.2352806663513185 and perplexity is 25.413503493285077
At time: 108.9660370349884 and batch: 750, loss is 3.335465931892395 and perplexity is 28.091468886308924
At time: 109.59890866279602 and batch: 800, loss is 3.2685852432250977 and perplexity is 26.274141516816144
At time: 110.2318024635315 and batch: 850, loss is 3.337815690040588 and perplexity is 28.157554656600897
At time: 110.86526870727539 and batch: 900, loss is 3.289770231246948 and perplexity is 26.836696713100515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42748563583583 and perplexity of 83.72064784946897
finished 9 epochs...
Completing Train Step...
At time: 112.44867324829102 and batch: 50, loss is 3.63976243019104 and perplexity is 38.08278832988058
At time: 113.08393812179565 and batch: 100, loss is 3.493839616775513 and perplexity is 32.9120751666478
At time: 113.72100257873535 and batch: 150, loss is 3.5071144914627075 and perplexity is 33.35189163566627
At time: 114.35766696929932 and batch: 200, loss is 3.3913994789123536 and perplexity is 29.707498209538215
At time: 114.9958643913269 and batch: 250, loss is 3.5322509765625 and perplexity is 34.20086638226653
At time: 115.6316146850586 and batch: 300, loss is 3.528627161979675 and perplexity is 34.077153076350164
At time: 116.28708839416504 and batch: 350, loss is 3.4889739894866945 and perplexity is 32.752326230140014
At time: 116.93768906593323 and batch: 400, loss is 3.4227143144607544 and perplexity is 30.65250273838257
At time: 117.57896041870117 and batch: 450, loss is 3.4365864849090575 and perplexity is 31.08068250594439
At time: 118.22034668922424 and batch: 500, loss is 3.3281408405303954 and perplexity is 27.88644812486011
At time: 118.88193821907043 and batch: 550, loss is 3.385224461555481 and perplexity is 29.52461911436924
At time: 119.52296090126038 and batch: 600, loss is 3.3939557456970215 and perplexity is 29.783535645046683
At time: 120.16497540473938 and batch: 650, loss is 3.243759660720825 and perplexity is 25.62990056463127
At time: 120.8082263469696 and batch: 700, loss is 3.2253251314163207 and perplexity is 25.161753702650937
At time: 121.4495210647583 and batch: 750, loss is 3.32924720287323 and perplexity is 27.917317714267696
At time: 122.09095191955566 and batch: 800, loss is 3.265565891265869 and perplexity is 26.19493027962973
At time: 122.73467946052551 and batch: 850, loss is 3.3393092203140258 and perplexity is 28.199640237127678
At time: 123.37806844711304 and batch: 900, loss is 3.2945841693878175 and perplexity is 26.966198367369866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429795356645976 and perplexity of 83.91424266096227
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.95939016342163 and batch: 50, loss is 3.625476655960083 and perplexity is 37.54261380604839
At time: 125.62056040763855 and batch: 100, loss is 3.481078758239746 and perplexity is 32.49475716230952
At time: 126.25787496566772 and batch: 150, loss is 3.495216736793518 and perplexity is 32.95743026673416
At time: 126.90367150306702 and batch: 200, loss is 3.379354228973389 and perplexity is 29.351810443059904
At time: 127.54429936408997 and batch: 250, loss is 3.5198807859420778 and perplexity is 33.78040112510026
At time: 128.1821916103363 and batch: 300, loss is 3.5157123041152953 and perplexity is 33.63988121772454
At time: 128.81909251213074 and batch: 350, loss is 3.4745911502838136 and perplexity is 32.28462627763387
At time: 129.4565131664276 and batch: 400, loss is 3.4087882661819457 and perplexity is 30.22859304965903
At time: 130.09529638290405 and batch: 450, loss is 3.4188979339599608 and perplexity is 30.535744064108655
At time: 130.7333881855011 and batch: 500, loss is 3.3076109981536863 and perplexity is 27.319780442366213
At time: 131.3713412284851 and batch: 550, loss is 3.36502947807312 and perplexity is 28.934350215370262
At time: 132.00954675674438 and batch: 600, loss is 3.3716375637054443 and perplexity is 29.126184008311142
At time: 132.6473536491394 and batch: 650, loss is 3.2188029956817625 and perplexity is 24.998179336638565
At time: 133.28386187553406 and batch: 700, loss is 3.1983884620666503 and perplexity is 24.493026933149185
At time: 133.92145538330078 and batch: 750, loss is 3.3001147890090943 and perplexity is 27.115751332245424
At time: 134.5594446659088 and batch: 800, loss is 3.2339439249038695 and perplexity is 25.379554905138306
At time: 135.2209277153015 and batch: 850, loss is 3.3047629594802856 and perplexity is 27.242083345804303
At time: 135.85864210128784 and batch: 900, loss is 3.259270944595337 and perplexity is 26.030552507941735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429741428322988 and perplexity of 83.9097174286009
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.43325972557068 and batch: 50, loss is 3.6182275915145876 and perplexity is 37.271449012100064
At time: 138.07632565498352 and batch: 100, loss is 3.4735318279266356 and perplexity is 32.250444559150594
At time: 138.70693469047546 and batch: 150, loss is 3.4882956171035766 and perplexity is 32.73011549097041
At time: 139.3392083644867 and batch: 200, loss is 3.372239055633545 and perplexity is 29.143708442763096
At time: 139.97095584869385 and batch: 250, loss is 3.51288170337677 and perplexity is 33.544794784458276
At time: 140.60300087928772 and batch: 300, loss is 3.508610906600952 and perplexity is 33.40183727158172
At time: 141.23529529571533 and batch: 350, loss is 3.467945351600647 and perplexity is 32.070780525809504
At time: 141.86778855323792 and batch: 400, loss is 3.4021111631393435 and perplexity is 30.02742597308036
At time: 142.499449968338 and batch: 450, loss is 3.4122459840774537 and perplexity is 30.333295908960725
At time: 143.13196206092834 and batch: 500, loss is 3.300707845687866 and perplexity is 27.131837279140573
At time: 143.76400208473206 and batch: 550, loss is 3.358122344017029 and perplexity is 28.735185400465312
At time: 144.39714097976685 and batch: 600, loss is 3.3652488136291505 and perplexity is 28.940697243202127
At time: 145.03082489967346 and batch: 650, loss is 3.2115177726745605 and perplexity is 24.81672379997642
At time: 145.66259789466858 and batch: 700, loss is 3.1907901525497437 and perplexity is 24.30762659009656
At time: 146.29491758346558 and batch: 750, loss is 3.2922006750106814 and perplexity is 26.902001122466913
At time: 146.92718887329102 and batch: 800, loss is 3.2256111526489257 and perplexity is 25.16895152777581
At time: 147.55912470817566 and batch: 850, loss is 3.295883116722107 and perplexity is 27.00124879825869
At time: 148.19075965881348 and batch: 900, loss is 3.2497985267639162 and perplexity is 25.78514437734893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429822529831978 and perplexity of 83.91652290926696
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.77905178070068 and batch: 50, loss is 3.6162533712387086 and perplexity is 37.19793954756221
At time: 150.41055536270142 and batch: 100, loss is 3.471602787971497 and perplexity is 32.18829212955844
At time: 151.05732321739197 and batch: 150, loss is 3.486365122795105 and perplexity is 32.666991139504454
At time: 151.68942618370056 and batch: 200, loss is 3.370308837890625 and perplexity is 29.087508995666067
At time: 152.32196068763733 and batch: 250, loss is 3.510983958244324 and perplexity is 33.48119567997
At time: 152.95421361923218 and batch: 300, loss is 3.506612000465393 and perplexity is 33.335136820300434
At time: 153.58705163002014 and batch: 350, loss is 3.465982336997986 and perplexity is 32.007886866101146
At time: 154.22116327285767 and batch: 400, loss is 3.400120873451233 and perplexity is 29.967722130485264
At time: 154.85417437553406 and batch: 450, loss is 3.410447974205017 and perplexity is 30.278805345418412
At time: 155.48642110824585 and batch: 500, loss is 3.2987588453292847 and perplexity is 27.07900881662361
At time: 156.11674809455872 and batch: 550, loss is 3.35615647315979 and perplexity is 28.67875122623327
At time: 156.7489378452301 and batch: 600, loss is 3.3634997034072875 and perplexity is 28.89012101841643
At time: 157.3807303905487 and batch: 650, loss is 3.209605679512024 and perplexity is 24.769317249396124
At time: 158.01433992385864 and batch: 700, loss is 3.1887521886825563 and perplexity is 24.258138969528215
At time: 158.6463577747345 and batch: 750, loss is 3.290178256034851 and perplexity is 26.847648984832123
At time: 159.27929282188416 and batch: 800, loss is 3.2234527397155763 and perplexity is 25.114685123002154
At time: 159.91216826438904 and batch: 850, loss is 3.2936450958251955 and perplexity is 26.940887009872064
At time: 160.54598593711853 and batch: 900, loss is 3.247342195510864 and perplexity is 25.72188524584776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429773618097174 and perplexity of 83.91241850693031
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 162.13230657577515 and batch: 50, loss is 3.615750470161438 and perplexity is 37.17923736675909
At time: 162.76489925384521 and batch: 100, loss is 3.4711254978179933 and perplexity is 32.17293264042495
At time: 163.39648485183716 and batch: 150, loss is 3.4858654069900514 and perplexity is 32.6506710057825
At time: 164.02887892723083 and batch: 200, loss is 3.3698239517211914 and perplexity is 29.07340828374716
At time: 164.66158390045166 and batch: 250, loss is 3.5104872512817384 and perplexity is 33.4645694664799
At time: 165.29411506652832 and batch: 300, loss is 3.506103949546814 and perplexity is 33.31820517485112
At time: 165.92731428146362 and batch: 350, loss is 3.4654642868041994 and perplexity is 31.99130946844025
At time: 166.56071209907532 and batch: 400, loss is 3.3996251106262205 and perplexity is 29.95286893003919
At time: 167.21975135803223 and batch: 450, loss is 3.4099788284301757 and perplexity is 30.264603503450925
At time: 167.85223507881165 and batch: 500, loss is 3.2982624101638796 and perplexity is 27.065569180632696
At time: 168.4845359325409 and batch: 550, loss is 3.3556539678573607 and perplexity is 28.664343621926992
At time: 169.1170494556427 and batch: 600, loss is 3.3630528974533083 and perplexity is 28.877215623652525
At time: 169.74909567832947 and batch: 650, loss is 3.2091162109375 and perplexity is 24.75719641361624
At time: 170.38146114349365 and batch: 700, loss is 3.188225755691528 and perplexity is 24.24537204563352
At time: 171.01428294181824 and batch: 750, loss is 3.2896570444107054 and perplexity is 26.83365932420382
At time: 171.64552688598633 and batch: 800, loss is 3.2229126119613647 and perplexity is 25.101123647323337
At time: 172.27860021591187 and batch: 850, loss is 3.2930859613418577 and perplexity is 26.92582764143157
At time: 172.918869972229 and batch: 900, loss is 3.246718363761902 and perplexity is 25.70584412118988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42975856833262 and perplexity of 83.91115565429142
Annealing...
Model not improving. Stopping early with 83.72064784946897 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
733.9972496032715


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.71266838151335, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.72064784946897, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.2283769027621978, 'rnn_dropout': 0.6017017728864293, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.9626269818184358, 'rnn_dropout': 0.9013488056646575, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8683896064758301 and batch: 50, loss is 10.015320129394532 and perplexity is 22366.512227667343
At time: 1.5193932056427002 and batch: 100, loss is 9.269173603057862 and perplexity is 10605.983511509317
At time: 2.158320665359497 and batch: 150, loss is 8.714334182739258 and perplexity is 6089.578720766855
At time: 2.796654462814331 and batch: 200, loss is 8.256058616638184 and perplexity is 3850.8863310300326
At time: 3.435547351837158 and batch: 250, loss is 8.031230974197388 and perplexity is 3075.525236269382
At time: 4.073582649230957 and batch: 300, loss is 7.755381097793579 and perplexity is 2334.098695153125
At time: 4.712468385696411 and batch: 350, loss is 7.604297800064087 and perplexity is 2006.8022224398956
At time: 5.371875762939453 and batch: 400, loss is 7.428220310211182 and perplexity is 1682.810026984949
At time: 6.021512031555176 and batch: 450, loss is 7.35563886642456 and perplexity is 1564.9964998036444
At time: 6.6610119342803955 and batch: 500, loss is 7.259041652679444 and perplexity is 1420.8941743722623
At time: 7.299601793289185 and batch: 550, loss is 7.221794309616089 and perplexity is 1368.9431662242941
At time: 7.938563823699951 and batch: 600, loss is 7.126140279769897 and perplexity is 1244.0659417972968
At time: 8.577730894088745 and batch: 650, loss is 7.0145550727844235 and perplexity is 1112.7114604470482
At time: 9.21711254119873 and batch: 700, loss is 7.087891397476196 and perplexity is 1197.3803376380106
At time: 9.856018781661987 and batch: 750, loss is 6.9932544136047365 and perplexity is 1089.2606187221352
At time: 10.495091915130615 and batch: 800, loss is 6.992786674499512 and perplexity is 1088.7512480705507
At time: 11.134020328521729 and batch: 850, loss is 7.002753324508667 and perplexity is 1099.656705874145
At time: 11.782753229141235 and batch: 900, loss is 6.853740310668945 and perplexity is 947.4179251474357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.34521944228917 and perplexity of 569.7624059646479
finished 1 epochs...
Completing Train Step...
At time: 13.359198093414307 and batch: 50, loss is 5.77561469078064 and perplexity is 322.34251480529696
At time: 13.992053270339966 and batch: 100, loss is 5.394079675674439 and perplexity is 220.0994909405479
At time: 14.624016523361206 and batch: 150, loss is 5.267390327453613 and perplexity is 193.9092618944473
At time: 15.257301092147827 and batch: 200, loss is 5.079381484985351 and perplexity is 160.67464550763765
At time: 15.888901472091675 and batch: 250, loss is 5.1269769287109375 and perplexity is 168.50693879063576
At time: 16.52100682258606 and batch: 300, loss is 5.050230693817139 and perplexity is 156.05846205633554
At time: 17.154290676116943 and batch: 350, loss is 5.0077718734741214 and perplexity is 149.57110125848186
At time: 17.786020755767822 and batch: 400, loss is 4.872347192764282 and perplexity is 130.62716448008786
At time: 18.419116020202637 and batch: 450, loss is 4.857719593048095 and perplexity is 128.730309644496
At time: 19.05300784111023 and batch: 500, loss is 4.768614645004273 and perplexity is 117.75599505329613
At time: 19.68600034713745 and batch: 550, loss is 4.83748945236206 and perplexity is 126.15224262317426
At time: 20.3197762966156 and batch: 600, loss is 4.777945499420166 and perplexity is 118.85990128463014
At time: 20.95247745513916 and batch: 650, loss is 4.641883630752563 and perplexity is 103.73957070706075
At time: 21.58374810218811 and batch: 700, loss is 4.688840312957764 and perplexity is 108.72701735390775
At time: 22.214568853378296 and batch: 750, loss is 4.729201564788818 and perplexity is 113.20513928569245
At time: 22.846667051315308 and batch: 800, loss is 4.655032815933228 and perplexity is 105.11266931264068
At time: 23.478071451187134 and batch: 850, loss is 4.712874212265015 and perplexity is 111.37180653183057
At time: 24.109333038330078 and batch: 900, loss is 4.6544596958160405 and perplexity is 105.05244438699305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.741532417192851 and perplexity of 114.60969704998153
finished 2 epochs...
Completing Train Step...
At time: 25.695388555526733 and batch: 50, loss is 4.688081064224243 and perplexity is 108.6444978340669
At time: 26.331188440322876 and batch: 100, loss is 4.523002958297729 and perplexity is 92.11179093955138
At time: 26.96703863143921 and batch: 150, loss is 4.518674898147583 and perplexity is 91.71398704838691
At time: 27.602503061294556 and batch: 200, loss is 4.398279294967652 and perplexity is 81.31083625715253
At time: 28.25053310394287 and batch: 250, loss is 4.540850372314453 and perplexity is 93.77050606707691
At time: 28.885820865631104 and batch: 300, loss is 4.505422830581665 and perplexity is 90.50660491976869
At time: 29.522204875946045 and batch: 350, loss is 4.486257085800171 and perplexity is 88.78849544452706
At time: 30.159140586853027 and batch: 400, loss is 4.40793025970459 and perplexity is 82.09936317298347
At time: 30.79494857788086 and batch: 450, loss is 4.410110635757446 and perplexity is 82.27856595212998
At time: 31.431186199188232 and batch: 500, loss is 4.313364486694336 and perplexity is 74.69136477244996
At time: 32.06734657287598 and batch: 550, loss is 4.398226776123047 and perplexity is 81.30656601811346
At time: 32.702815532684326 and batch: 600, loss is 4.384790115356445 and perplexity is 80.2213842092085
At time: 33.33928942680359 and batch: 650, loss is 4.242333946228027 and perplexity is 69.57003522277654
At time: 33.975276708602905 and batch: 700, loss is 4.259507946968078 and perplexity is 70.77514975794334
At time: 34.611857175827026 and batch: 750, loss is 4.3588765478134155 and perplexity is 78.16926564173001
At time: 35.24824023246765 and batch: 800, loss is 4.297756757736206 and perplexity is 73.53465250264328
At time: 35.88426947593689 and batch: 850, loss is 4.365968580245972 and perplexity is 78.72561510120858
At time: 36.52092456817627 and batch: 900, loss is 4.319849891662598 and perplexity is 75.17734269909059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.587739291256422 and perplexity of 98.27201444578948
finished 3 epochs...
Completing Train Step...
At time: 38.09263563156128 and batch: 50, loss is 4.390612516403198 and perplexity is 80.68982769014116
At time: 38.74209117889404 and batch: 100, loss is 4.232721962928772 and perplexity is 68.90453272933202
At time: 39.3788959980011 and batch: 150, loss is 4.23542573928833 and perplexity is 69.09108726321213
At time: 40.016233921051025 and batch: 200, loss is 4.118213920593262 and perplexity is 61.44939070053659
At time: 40.65366315841675 and batch: 250, loss is 4.272939438819885 and perplexity is 71.7321783786041
At time: 41.29103326797485 and batch: 300, loss is 4.246872158050537 and perplexity is 69.88647627427606
At time: 41.9281485080719 and batch: 350, loss is 4.229501023292541 and perplexity is 68.6829524286991
At time: 42.56523156166077 and batch: 400, loss is 4.168300380706787 and perplexity is 64.60555389544828
At time: 43.20351433753967 and batch: 450, loss is 4.174104433059693 and perplexity is 64.98161820531048
At time: 43.840431690216064 and batch: 500, loss is 4.0743869066238405 and perplexity is 58.814410806716815
At time: 44.490684270858765 and batch: 550, loss is 4.1596438550949095 and perplexity is 64.04870791650906
At time: 45.12777280807495 and batch: 600, loss is 4.163670139312744 and perplexity is 64.307106061479
At time: 45.765302658081055 and batch: 650, loss is 4.01499219417572 and perplexity is 55.422862779677445
At time: 46.4025514125824 and batch: 700, loss is 4.027982020378113 and perplexity is 56.147492351721276
At time: 47.04077649116516 and batch: 750, loss is 4.137962079048156 and perplexity is 62.674964588873124
At time: 47.67833065986633 and batch: 800, loss is 4.083751873970032 and perplexity is 59.36779300208584
At time: 48.31580305099487 and batch: 850, loss is 4.152820062637329 and perplexity is 63.61314062705307
At time: 48.953540086746216 and batch: 900, loss is 4.1119792270660405 and perplexity is 61.067464415918295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.534802580532962 and perplexity of 93.20511298426494
finished 4 epochs...
Completing Train Step...
At time: 50.52319264411926 and batch: 50, loss is 4.1912057590484615 and perplexity is 66.10244655108909
At time: 51.16854190826416 and batch: 100, loss is 4.0419886112213135 and perplexity is 56.93946074496231
At time: 51.80013990402222 and batch: 150, loss is 4.0438050317764285 and perplexity is 57.04298074131857
At time: 52.433117389678955 and batch: 200, loss is 3.9314944410324095 and perplexity is 50.9831120204459
At time: 53.0650053024292 and batch: 250, loss is 4.0876670789718625 and perplexity is 59.600685696017706
At time: 53.69694924354553 and batch: 300, loss is 4.068706030845642 and perplexity is 58.48124068999371
At time: 54.32803964614868 and batch: 350, loss is 4.04977780342102 and perplexity is 57.38470494363772
At time: 54.959513902664185 and batch: 400, loss is 3.9916686105728147 and perplexity is 54.1451612165699
At time: 55.59176468849182 and batch: 450, loss is 4.004407572746277 and perplexity is 54.83932646211681
At time: 56.223726987838745 and batch: 500, loss is 3.9025293350219727 and perplexity is 49.527562610507445
At time: 56.85400176048279 and batch: 550, loss is 3.985969514846802 and perplexity is 53.83746040073369
At time: 57.48507022857666 and batch: 600, loss is 4.00297070980072 and perplexity is 54.76058644881488
At time: 58.11765432357788 and batch: 650, loss is 3.8521871042251585 and perplexity is 47.09595443460429
At time: 58.75091242790222 and batch: 700, loss is 3.8604952001571657 and perplexity is 47.48886203907466
At time: 59.38315749168396 and batch: 750, loss is 3.9762849044799804 and perplexity is 53.31858219576665
At time: 60.03763794898987 and batch: 800, loss is 3.9230341482162476 and perplexity is 50.553599427143766
At time: 60.66928243637085 and batch: 850, loss is 3.9979895114898683 and perplexity is 54.488491350526154
At time: 61.30250930786133 and batch: 900, loss is 3.9570470905303954 and perplexity is 52.30265269500386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5220010835830475 and perplexity of 92.01955267859107
finished 5 epochs...
Completing Train Step...
At time: 62.87742829322815 and batch: 50, loss is 4.038321738243103 and perplexity is 56.73105331027782
At time: 63.51005482673645 and batch: 100, loss is 3.8970706272125244 and perplexity is 49.2579426754246
At time: 64.14252018928528 and batch: 150, loss is 3.8984520483016967 and perplexity is 49.326035657958094
At time: 64.77484726905823 and batch: 200, loss is 3.787760829925537 and perplexity is 44.157413534315786
At time: 65.41120886802673 and batch: 250, loss is 3.9429505586624147 and perplexity is 51.57053894023477
At time: 66.04312825202942 and batch: 300, loss is 3.928558626174927 and perplexity is 50.833654539789634
At time: 66.67522549629211 and batch: 350, loss is 3.9051805543899536 and perplexity is 49.659045261409055
At time: 67.31065940856934 and batch: 400, loss is 3.853090286254883 and perplexity is 47.13850986908134
At time: 67.94908118247986 and batch: 450, loss is 3.8716261625289916 and perplexity is 48.0204116165213
At time: 68.58541083335876 and batch: 500, loss is 3.766670546531677 and perplexity is 43.23587310156296
At time: 69.22625184059143 and batch: 550, loss is 3.8450213432312013 and perplexity is 46.7596823429731
At time: 69.85926461219788 and batch: 600, loss is 3.8703115510940553 and perplexity is 47.9573249106379
At time: 70.49123072624207 and batch: 650, loss is 3.7177053594589236 and perplexity is 41.169815710199096
At time: 71.12405681610107 and batch: 700, loss is 3.7294618034362794 and perplexity is 41.65668264644603
At time: 71.75627446174622 and batch: 750, loss is 3.8424942874908448 and perplexity is 46.641667197520256
At time: 72.38989853858948 and batch: 800, loss is 3.7929481220245362 and perplexity is 44.38706605906087
At time: 73.02175331115723 and batch: 850, loss is 3.868068675994873 and perplexity is 47.84988315503195
At time: 73.65499591827393 and batch: 900, loss is 3.8299511241912843 and perplexity is 46.060286945424004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.522996040239726 and perplexity of 92.11115370704312
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.23440623283386 and batch: 50, loss is 3.925153923034668 and perplexity is 50.66087533440349
At time: 75.87527346611023 and batch: 100, loss is 3.7801308059692382 and perplexity is 43.82177351024837
At time: 76.52113103866577 and batch: 150, loss is 3.7824013090133666 and perplexity is 43.921384020593436
At time: 77.15388774871826 and batch: 200, loss is 3.651464009284973 and perplexity is 38.531034569314684
At time: 77.78630542755127 and batch: 250, loss is 3.8007590866088865 and perplexity is 44.73512944580054
At time: 78.42167830467224 and batch: 300, loss is 3.7765962171554563 and perplexity is 43.66715497724239
At time: 79.0689971446991 and batch: 350, loss is 3.730448913574219 and perplexity is 41.697822681731694
At time: 79.70586323738098 and batch: 400, loss is 3.6764566802978518 and perplexity is 39.50616281214834
At time: 80.3379909992218 and batch: 450, loss is 3.679424271583557 and perplexity is 39.62357508629524
At time: 80.96974205970764 and batch: 500, loss is 3.5607082796096803 and perplexity is 35.188111342723204
At time: 81.60244297981262 and batch: 550, loss is 3.6213862991333006 and perplexity is 37.38936475469094
At time: 82.24828624725342 and batch: 600, loss is 3.636860523223877 and perplexity is 37.97243581490359
At time: 82.89111304283142 and batch: 650, loss is 3.4695832347869873 and perplexity is 32.12335175902245
At time: 83.53506064414978 and batch: 700, loss is 3.4658188724517824 and perplexity is 32.00265513901225
At time: 84.17167258262634 and batch: 750, loss is 3.5571137857437134 and perplexity is 35.0618549422155
At time: 84.80511593818665 and batch: 800, loss is 3.4932310676574705 and perplexity is 32.892052645293965
At time: 85.43767356872559 and batch: 850, loss is 3.5497921228408815 and perplexity is 34.80608134620353
At time: 86.06863522529602 and batch: 900, loss is 3.4989148139953614 and perplexity is 33.07953502549081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.464752615314641 and perplexity of 86.89952926773726
finished 7 epochs...
Completing Train Step...
At time: 87.62893438339233 and batch: 50, loss is 3.8105334568023683 and perplexity is 45.17453110057365
At time: 88.27200603485107 and batch: 100, loss is 3.6693208265304564 and perplexity is 39.22525605815886
At time: 88.90232801437378 and batch: 150, loss is 3.673762707710266 and perplexity is 39.39987752160165
At time: 89.53275227546692 and batch: 200, loss is 3.5496094751358034 and perplexity is 34.799724675856176
At time: 90.16380834579468 and batch: 250, loss is 3.7003464317321777 and perplexity is 40.46131901719958
At time: 90.79432034492493 and batch: 300, loss is 3.6838234186172487 and perplexity is 39.79826898945317
At time: 91.42489814758301 and batch: 350, loss is 3.6423694896698 and perplexity is 38.182201956484846
At time: 92.05502891540527 and batch: 400, loss is 3.5934193897247315 and perplexity is 36.35818642353648
At time: 92.69758796691895 and batch: 450, loss is 3.6029378175735474 and perplexity is 36.705911470273286
At time: 93.3282561302185 and batch: 500, loss is 3.4875506448745726 and perplexity is 32.70574154396162
At time: 93.95900416374207 and batch: 550, loss is 3.5516945266723634 and perplexity is 34.87235959272128
At time: 94.58991885185242 and batch: 600, loss is 3.576272449493408 and perplexity is 35.74006932716563
At time: 95.22557735443115 and batch: 650, loss is 3.4135811281204225 and perplexity is 30.37382227654609
At time: 95.86562371253967 and batch: 700, loss is 3.4163704776763915 and perplexity is 30.458663755488182
At time: 96.49628353118896 and batch: 750, loss is 3.517254853248596 and perplexity is 33.69181243026739
At time: 97.12720584869385 and batch: 800, loss is 3.4581190967559814 and perplexity is 31.757188106058845
At time: 97.75889205932617 and batch: 850, loss is 3.524569296836853 and perplexity is 33.939152767399186
At time: 98.38993883132935 and batch: 900, loss is 3.483068809509277 and perplexity is 32.55948778231699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.470365759444563 and perplexity of 87.3886804039317
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 99.95276999473572 and batch: 50, loss is 3.764023809432983 and perplexity is 43.121590417055614
At time: 100.61025834083557 and batch: 100, loss is 3.6285116195678713 and perplexity is 37.656727350326335
At time: 101.24305152893066 and batch: 150, loss is 3.63707049369812 and perplexity is 37.98040974237519
At time: 101.87706136703491 and batch: 200, loss is 3.507141880989075 and perplexity is 33.35280514069181
At time: 102.51030254364014 and batch: 250, loss is 3.6569505548477172 and perplexity is 38.74301784223863
At time: 103.14331126213074 and batch: 300, loss is 3.6375871753692626 and perplexity is 38.00003859444892
At time: 103.77662968635559 and batch: 350, loss is 3.590864396095276 and perplexity is 36.26541006082392
At time: 104.40945839881897 and batch: 400, loss is 3.541587209701538 and perplexity is 34.521668857486326
At time: 105.04272222518921 and batch: 450, loss is 3.5449990701675413 and perplexity is 34.63965313317203
At time: 105.67770838737488 and batch: 500, loss is 3.422794508934021 and perplexity is 30.654960998262084
At time: 106.32134866714478 and batch: 550, loss is 3.4803249645233154 and perplexity is 32.47027204806563
At time: 106.95394492149353 and batch: 600, loss is 3.5014170742034914 and perplexity is 33.162412276669116
At time: 107.58747506141663 and batch: 650, loss is 3.3334598445892336 and perplexity is 28.035171434930376
At time: 108.23384666442871 and batch: 700, loss is 3.3298038482666015 and perplexity is 27.932862086518863
At time: 108.86687207221985 and batch: 750, loss is 3.4261228752136232 and perplexity is 30.757161923459154
At time: 109.49918413162231 and batch: 800, loss is 3.3596730518341062 and perplexity is 28.77977984463123
At time: 110.13257479667664 and batch: 850, loss is 3.4179529666900637 and perplexity is 30.50690241475965
At time: 110.76832294464111 and batch: 900, loss is 3.376188888549805 and perplexity is 29.25904885940547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.460090114645762 and perplexity of 86.49530323875238
finished 9 epochs...
Completing Train Step...
At time: 112.34851408004761 and batch: 50, loss is 3.7339614057540893 and perplexity is 41.84454348480708
At time: 112.984450340271 and batch: 100, loss is 3.5949762773513796 and perplexity is 36.41483612127062
At time: 113.62071251869202 and batch: 150, loss is 3.6021705532073973 and perplexity is 36.67775913389844
At time: 114.25642919540405 and batch: 200, loss is 3.4750904560089113 and perplexity is 32.300750201415674
At time: 114.89340615272522 and batch: 250, loss is 3.6239302825927733 and perplexity is 37.48460377206512
At time: 115.52848863601685 and batch: 300, loss is 3.606611795425415 and perplexity is 36.84101620998682
At time: 116.164386510849 and batch: 350, loss is 3.5617322540283203 and perplexity is 35.22416152265947
At time: 116.8007128238678 and batch: 400, loss is 3.514743838310242 and perplexity is 33.60731791384743
At time: 117.43680739402771 and batch: 450, loss is 3.521003370285034 and perplexity is 33.81834376742559
At time: 118.0734634399414 and batch: 500, loss is 3.4006786823272703 and perplexity is 29.984443054990763
At time: 118.70980405807495 and batch: 550, loss is 3.460631070137024 and perplexity is 31.837061595277206
At time: 119.34601163864136 and batch: 600, loss is 3.485054054260254 and perplexity is 32.62419053868453
At time: 119.98213791847229 and batch: 650, loss is 3.3204420232772827 and perplexity is 27.67257977992256
At time: 120.61786651611328 and batch: 700, loss is 3.3192544412612914 and perplexity is 27.6397358281432
At time: 121.25441312789917 and batch: 750, loss is 3.419249081611633 and perplexity is 30.54646850174919
At time: 121.89070916175842 and batch: 800, loss is 3.3557779359817506 and perplexity is 28.66789730710996
At time: 122.52734899520874 and batch: 850, loss is 3.4180808734893797 and perplexity is 30.510804704563952
At time: 123.163321018219 and batch: 900, loss is 3.3797564029693605 and perplexity is 29.363617352011524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.462102184556935 and perplexity of 86.66951303823177
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 124.75100255012512 and batch: 50, loss is 3.7190898942947386 and perplexity is 41.22685623241706
At time: 125.39502239227295 and batch: 100, loss is 3.581972608566284 and perplexity is 35.94437514224023
At time: 126.04133677482605 and batch: 150, loss is 3.590224571228027 and perplexity is 36.24221397115485
At time: 126.68343544006348 and batch: 200, loss is 3.4622214651107788 and perplexity is 31.887735383005104
At time: 127.3326301574707 and batch: 250, loss is 3.6107786512374878 and perplexity is 36.994847686718266
At time: 127.97779870033264 and batch: 300, loss is 3.5929205417633057 and perplexity is 36.34005373946023
At time: 128.61555767059326 and batch: 350, loss is 3.5464512872695924 and perplexity is 34.68999397392896
At time: 129.25201439857483 and batch: 400, loss is 3.5002143001556396 and perplexity is 33.12254936566168
At time: 129.88921189308167 and batch: 450, loss is 3.503957805633545 and perplexity is 33.24677618747374
At time: 130.52690076828003 and batch: 500, loss is 3.380723071098328 and perplexity is 29.392015948854727
At time: 131.18139576911926 and batch: 550, loss is 3.438364014625549 and perplexity is 31.13597847326062
At time: 131.8343062400818 and batch: 600, loss is 3.462672080993652 and perplexity is 31.902107741000744
At time: 132.47617864608765 and batch: 650, loss is 3.296072311401367 and perplexity is 27.006357774144984
At time: 133.11675024032593 and batch: 700, loss is 3.2924227619171145 and perplexity is 26.90797636816042
At time: 133.75730347633362 and batch: 750, loss is 3.389877643585205 and perplexity is 29.662322672864367
At time: 134.39613270759583 and batch: 800, loss is 3.3253653717041014 and perplexity is 27.809157465936316
At time: 135.0367910861969 and batch: 850, loss is 3.385135350227356 and perplexity is 29.521988253569067
At time: 135.67583847045898 and batch: 900, loss is 3.3456019878387453 and perplexity is 28.377653528121417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.461560393032962 and perplexity of 86.62256894878402
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.23940014839172 and batch: 50, loss is 3.7118402242660524 and perplexity is 40.9290559097564
At time: 137.88397812843323 and batch: 100, loss is 3.5744159030914306 and perplexity is 35.673777785752144
At time: 138.51440143585205 and batch: 150, loss is 3.583339505195618 and perplexity is 35.99354098212093
At time: 139.1455307006836 and batch: 200, loss is 3.455541043281555 and perplexity is 31.675421821138812
At time: 139.77655029296875 and batch: 250, loss is 3.6041993284225464 and perplexity is 36.75224559517134
At time: 140.42004537582397 and batch: 300, loss is 3.58606764793396 and perplexity is 36.091870567163255
At time: 141.0529146194458 and batch: 350, loss is 3.539570698738098 and perplexity is 34.45212567461967
At time: 141.68491506576538 and batch: 400, loss is 3.493885974884033 and perplexity is 32.9136009435658
At time: 142.32827854156494 and batch: 450, loss is 3.497698450088501 and perplexity is 33.03932273437782
At time: 142.9656310081482 and batch: 500, loss is 3.374083480834961 and perplexity is 29.1975114356229
At time: 143.60161066055298 and batch: 550, loss is 3.4315144538879396 and perplexity is 30.92343942809731
At time: 144.23614168167114 and batch: 600, loss is 3.456402840614319 and perplexity is 31.702731381141202
At time: 144.87137603759766 and batch: 650, loss is 3.2899010705947878 and perplexity is 26.84020823871448
At time: 145.5023889541626 and batch: 700, loss is 3.2855106019973754 and perplexity is 26.7226254579269
At time: 146.1330292224884 and batch: 750, loss is 3.3814336490631103 and perplexity is 29.41290868981009
At time: 146.7620882987976 and batch: 800, loss is 3.3169697523117065 and perplexity is 27.576659711217932
At time: 147.39167428016663 and batch: 850, loss is 3.376257300376892 and perplexity is 29.26105059286712
At time: 148.02461576461792 and batch: 900, loss is 3.3364801025390625 and perplexity is 28.119972880989422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.461392337328767 and perplexity of 86.60801275512006
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 149.59892773628235 and batch: 50, loss is 3.709976620674133 and perplexity is 40.85285140369968
At time: 150.24964714050293 and batch: 100, loss is 3.572515344619751 and perplexity is 35.60604207339473
At time: 150.88640975952148 and batch: 150, loss is 3.58140558719635 and perplexity is 35.92399969060982
At time: 151.53561186790466 and batch: 200, loss is 3.453744258880615 and perplexity is 31.61855901771464
At time: 152.17474746704102 and batch: 250, loss is 3.6023949718475343 and perplexity is 36.68599123040995
At time: 152.80979180335999 and batch: 300, loss is 3.5841645908355715 and perplexity is 36.02325099088169
At time: 153.44987988471985 and batch: 350, loss is 3.537612419128418 and perplexity is 34.384724795823644
At time: 154.0921769142151 and batch: 400, loss is 3.4919493579864502 and perplexity is 32.84992158897762
At time: 154.72358059883118 and batch: 450, loss is 3.495901246070862 and perplexity is 32.979997656421794
At time: 155.35573863983154 and batch: 500, loss is 3.372246866226196 and perplexity is 29.143936073287055
At time: 155.9872546195984 and batch: 550, loss is 3.4295786952972414 and perplexity is 30.86363701476222
At time: 156.6323320865631 and batch: 600, loss is 3.454587769508362 and perplexity is 31.645240859906824
At time: 157.26390051841736 and batch: 650, loss is 3.2881456232070922 and perplexity is 26.793132996421082
At time: 157.8950972557068 and batch: 700, loss is 3.283651933670044 and perplexity is 26.67300309041217
At time: 158.5257568359375 and batch: 750, loss is 3.3793901920318605 and perplexity is 29.3528660429163
At time: 159.1579520702362 and batch: 800, loss is 3.314830975532532 and perplexity is 27.517742419706966
At time: 159.78976678848267 and batch: 850, loss is 3.3739735317230224 and perplexity is 29.194301371644816
At time: 160.42742443084717 and batch: 900, loss is 3.3341269588470457 and perplexity is 28.053880337308733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.461363909995719 and perplexity of 86.60555075529109
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 162.01595449447632 and batch: 50, loss is 3.709528250694275 and perplexity is 40.834538317364526
At time: 162.65075731277466 and batch: 100, loss is 3.572058572769165 and perplexity is 35.58978194953097
At time: 163.28683185577393 and batch: 150, loss is 3.5809226179122926 and perplexity is 35.90665369132837
At time: 163.92027354240417 and batch: 200, loss is 3.453294515609741 and perplexity is 31.604341980809675
At time: 164.55200505256653 and batch: 250, loss is 3.6019214010238647 and perplexity is 36.66862192844776
At time: 165.18365454673767 and batch: 300, loss is 3.58368896484375 and perplexity is 36.006121470346045
At time: 165.8171148300171 and batch: 350, loss is 3.537104058265686 and perplexity is 34.36724938974424
At time: 166.46064972877502 and batch: 400, loss is 3.4914704895019533 and perplexity is 32.83419456269962
At time: 167.0921812057495 and batch: 450, loss is 3.495440468788147 and perplexity is 32.96480472326576
At time: 167.7238245010376 and batch: 500, loss is 3.3717774152755737 and perplexity is 29.130257635721538
At time: 168.3550090789795 and batch: 550, loss is 3.4290827465057374 and perplexity is 30.848334026346304
At time: 168.9895462989807 and batch: 600, loss is 3.454118332862854 and perplexity is 31.630388910488655
At time: 169.62583017349243 and batch: 650, loss is 3.287691159248352 and perplexity is 26.78095924960659
At time: 170.27704977989197 and batch: 700, loss is 3.283176231384277 and perplexity is 26.66031769934778
At time: 170.9071443080902 and batch: 750, loss is 3.378873195648193 and perplexity is 29.337694639440134
At time: 171.54262447357178 and batch: 800, loss is 3.314296746253967 and perplexity is 27.503045562121713
At time: 172.18033576011658 and batch: 850, loss is 3.373400635719299 and perplexity is 29.1775808630711
At time: 172.83991718292236 and batch: 900, loss is 3.3335310411453247 and perplexity is 28.03716751364209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.461355549015411 and perplexity of 86.60482665101377
Annealing...
Model not improving. Stopping early with 86.49530323875238 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
913.2509119510651


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.71266838151335, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.72064784946897, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.2283769027621978, 'rnn_dropout': 0.6017017728864293, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -86.49530323875238, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.9626269818184358, 'rnn_dropout': 0.9013488056646575, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6447005347113727, 'rnn_dropout': 0.9039291818368151, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8497471809387207 and batch: 50, loss is 6.754981737136841 and perplexity is 858.3240742902133
At time: 1.4995198249816895 and batch: 100, loss is 5.970581283569336 and perplexity is 391.73331271945557
At time: 2.138645648956299 and batch: 150, loss is 5.8791357612609865 and perplexity is 357.50014268790386
At time: 2.777752637863159 and batch: 200, loss is 5.72467755317688 and perplexity is 306.33447273900873
At time: 3.417506456375122 and batch: 250, loss is 5.779188327789306 and perplexity is 323.49651069875966
At time: 4.057236433029175 and batch: 300, loss is 5.704651098251343 and perplexity is 300.2607002873614
At time: 4.696603536605835 and batch: 350, loss is 5.692243747711181 and perplexity is 296.55827665591033
At time: 5.3359904289245605 and batch: 400, loss is 5.55729458808899 and perplexity is 259.12085852452157
At time: 5.9811742305755615 and batch: 450, loss is 5.564540996551513 and perplexity is 261.00537384404345
At time: 6.621734142303467 and batch: 500, loss is 5.512417287826538 and perplexity is 247.74928514725443
At time: 7.261402130126953 and batch: 550, loss is 5.579260416030884 and perplexity is 264.8756355530737
At time: 7.9014623165130615 and batch: 600, loss is 5.501307859420776 and perplexity is 245.01216427660086
At time: 8.54049277305603 and batch: 650, loss is 5.407683563232422 and perplexity is 223.11415877549285
At time: 9.179600954055786 and batch: 700, loss is 5.510916023254395 and perplexity is 247.37762697111893
At time: 9.819618225097656 and batch: 750, loss is 5.473211364746094 and perplexity is 238.22398981185037
At time: 10.458734273910522 and batch: 800, loss is 5.459486474990845 and perplexity is 234.97672695146173
At time: 11.097800254821777 and batch: 850, loss is 5.4927741813659665 and perplexity is 242.93020536532882
At time: 11.73695421218872 and batch: 900, loss is 5.4089638614654545 and perplexity is 223.3999943771012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.225608930195848 and perplexity of 185.97438147020594
finished 1 epochs...
Completing Train Step...
At time: 13.345196962356567 and batch: 50, loss is 5.146770191192627 and perplexity is 171.87546797703874
At time: 13.989134073257446 and batch: 100, loss is 4.980681943893432 and perplexity is 145.57362085376687
At time: 14.624627351760864 and batch: 150, loss is 4.9324747085571286 and perplexity is 138.72238538372258
At time: 15.259604692459106 and batch: 200, loss is 4.809440307617187 and perplexity is 122.66294478540563
At time: 15.907350540161133 and batch: 250, loss is 4.89602557182312 and perplexity is 133.75711382296322
At time: 16.541602849960327 and batch: 300, loss is 4.84833215713501 and perplexity is 127.52751651723021
At time: 17.176055192947388 and batch: 350, loss is 4.816546039581299 and perplexity is 123.53765885437868
At time: 17.809831619262695 and batch: 400, loss is 4.706828622817993 and perplexity is 110.70052949081293
At time: 18.444744110107422 and batch: 450, loss is 4.706906461715699 and perplexity is 110.70914663337396
At time: 19.079075574874878 and batch: 500, loss is 4.613716125488281 and perplexity is 100.85825602865003
At time: 19.7136390209198 and batch: 550, loss is 4.691528177261352 and perplexity is 109.01965393021048
At time: 20.348636865615845 and batch: 600, loss is 4.6474377727508545 and perplexity is 104.31735808485837
At time: 20.982797384262085 and batch: 650, loss is 4.510235109329224 and perplexity is 90.94319759140356
At time: 21.622498989105225 and batch: 700, loss is 4.5441530418396 and perplexity is 94.08071103009462
At time: 22.271756172180176 and batch: 750, loss is 4.605904197692871 and perplexity is 100.07342811572939
At time: 22.911935567855835 and batch: 800, loss is 4.540859746932983 and perplexity is 93.77138513392114
At time: 23.555734395980835 and batch: 850, loss is 4.600950345993042 and perplexity is 99.57890510191271
At time: 24.199837684631348 and batch: 900, loss is 4.548325128555298 and perplexity is 94.47404385357139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.658178460108091 and perplexity of 105.44383696328643
finished 2 epochs...
Completing Train Step...
At time: 25.802636861801147 and batch: 50, loss is 4.5471734619140625 and perplexity is 94.36530387692325
At time: 26.439924478530884 and batch: 100, loss is 4.402422060966492 and perplexity is 81.64838673849678
At time: 27.0775625705719 and batch: 150, loss is 4.398479671478271 and perplexity is 81.32713067125171
At time: 27.71491503715515 and batch: 200, loss is 4.292789831161499 and perplexity is 73.17031684621705
At time: 28.351579427719116 and batch: 250, loss is 4.434240598678588 and perplexity is 84.28809208955984
At time: 28.99008321762085 and batch: 300, loss is 4.40724015712738 and perplexity is 82.04272573593893
At time: 29.627607583999634 and batch: 350, loss is 4.391960868835449 and perplexity is 80.7986993977864
At time: 30.265297174453735 and batch: 400, loss is 4.315600876808166 and perplexity is 74.85859072375013
At time: 30.90499496459961 and batch: 450, loss is 4.32863000869751 and perplexity is 75.8403147935897
At time: 31.542898416519165 and batch: 500, loss is 4.2259193754196165 and perplexity is 68.43739429212116
At time: 32.20924258232117 and batch: 550, loss is 4.316793618202209 and perplexity is 74.94793093289732
At time: 32.853498458862305 and batch: 600, loss is 4.305356888771057 and perplexity is 74.09565464176892
At time: 33.50522589683533 and batch: 650, loss is 4.165232849121094 and perplexity is 64.40767796875393
At time: 34.14852237701416 and batch: 700, loss is 4.177944264411926 and perplexity is 65.23161632835891
At time: 34.79687571525574 and batch: 750, loss is 4.2805040740966795 and perplexity is 72.27686372121988
At time: 35.43802881240845 and batch: 800, loss is 4.224356746673584 and perplexity is 68.33053556450051
At time: 36.082926750183105 and batch: 850, loss is 4.292759065628052 and perplexity is 73.16806575701494
At time: 36.72346258163452 and batch: 900, loss is 4.255761747360229 and perplexity is 70.51050792973001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5480150196650255 and perplexity of 94.44475115487029
finished 3 epochs...
Completing Train Step...
At time: 38.3052191734314 and batch: 50, loss is 4.2923847198486325 and perplexity is 73.14068072646563
At time: 38.95582437515259 and batch: 100, loss is 4.151722121238708 and perplexity is 63.54333545446911
At time: 39.59337568283081 and batch: 150, loss is 4.146348395347595 and perplexity is 63.20278681455104
At time: 40.23091506958008 and batch: 200, loss is 4.048713827133179 and perplexity is 57.32368144782598
At time: 40.87311792373657 and batch: 250, loss is 4.196578888893128 and perplexity is 66.45857949692652
At time: 41.51164388656616 and batch: 300, loss is 4.177817296981812 and perplexity is 65.2233345634398
At time: 42.17390155792236 and batch: 350, loss is 4.163890686035156 and perplexity is 64.32129034704
At time: 42.81150436401367 and batch: 400, loss is 4.097087111473083 and perplexity is 60.16477880877523
At time: 43.449352502822876 and batch: 450, loss is 4.111556482315064 and perplexity is 61.04165392189083
At time: 44.08786725997925 and batch: 500, loss is 4.012829265594482 and perplexity is 55.30311663358182
At time: 44.72637486457825 and batch: 550, loss is 4.099109902381897 and perplexity is 60.28660274701902
At time: 45.36602330207825 and batch: 600, loss is 4.10010027885437 and perplexity is 60.346338755671134
At time: 46.01512289047241 and batch: 650, loss is 3.9571580505371093 and perplexity is 52.30845651968831
At time: 46.65730619430542 and batch: 700, loss is 3.9672863578796385 and perplexity is 52.84094469388086
At time: 47.29694128036499 and batch: 750, loss is 4.079194016456604 and perplexity is 59.09781878008544
At time: 47.93619179725647 and batch: 800, loss is 4.025188016891479 and perplexity is 55.99083501480912
At time: 48.61126661300659 and batch: 850, loss is 4.0971200656890865 and perplexity is 60.16676152456112
At time: 49.2587513923645 and batch: 900, loss is 4.065209679603576 and perplexity is 58.2771267663869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.502653357100813 and perplexity of 90.25629604947933
finished 4 epochs...
Completing Train Step...
At time: 50.884639739990234 and batch: 50, loss is 4.111853466033936 and perplexity is 61.05978499146163
At time: 51.53030180931091 and batch: 100, loss is 3.975814905166626 and perplexity is 53.2935283868433
At time: 52.16256356239319 and batch: 150, loss is 3.9745488214492797 and perplexity is 53.226097014201784
At time: 52.79437446594238 and batch: 200, loss is 3.8755266761779783 and perplexity is 48.20808165429906
At time: 53.42790389060974 and batch: 250, loss is 4.025680527687073 and perplexity is 56.01841789738439
At time: 54.0608868598938 and batch: 300, loss is 4.013606042861938 and perplexity is 55.34609152620122
At time: 54.6956353187561 and batch: 350, loss is 3.995858612060547 and perplexity is 54.37250547640762
At time: 55.33535552024841 and batch: 400, loss is 3.9386623668670655 and perplexity is 51.3498680559593
At time: 55.97569799423218 and batch: 450, loss is 3.9536659812927244 and perplexity is 52.12611033550787
At time: 56.609564781188965 and batch: 500, loss is 3.8546022272109983 and perplexity is 47.2098344184292
At time: 57.24352765083313 and batch: 550, loss is 3.933863043785095 and perplexity is 51.1040138876413
At time: 57.876341819763184 and batch: 600, loss is 3.9448834085464477 and perplexity is 51.67031344395152
At time: 58.51417779922485 and batch: 650, loss is 3.802654061317444 and perplexity is 44.81998175578351
At time: 59.155385971069336 and batch: 700, loss is 3.810037384033203 and perplexity is 45.152126803375175
At time: 59.78887629508972 and batch: 750, loss is 3.926246151924133 and perplexity is 50.716238835312986
At time: 60.42253804206848 and batch: 800, loss is 3.874250888824463 and perplexity is 48.146617609249695
At time: 61.055806398391724 and batch: 850, loss is 3.9473832416534425 and perplexity is 51.799642186934484
At time: 61.69036602973938 and batch: 900, loss is 3.9160645818710327 and perplexity is 50.2024877313416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4916009772313785 and perplexity of 89.26424156015503
finished 5 epochs...
Completing Train Step...
At time: 63.28251600265503 and batch: 50, loss is 3.9705095863342286 and perplexity is 53.011537913150924
At time: 63.9222309589386 and batch: 100, loss is 3.839563670158386 and perplexity is 46.50517841509851
At time: 64.57533740997314 and batch: 150, loss is 3.838482322692871 and perplexity is 46.45491733802774
At time: 65.21734738349915 and batch: 200, loss is 3.743545341491699 and perplexity is 42.24750680340518
At time: 65.8545925617218 and batch: 250, loss is 3.890878973007202 and perplexity is 48.95389677239926
At time: 66.4973464012146 and batch: 300, loss is 3.882426347732544 and perplexity is 48.54185171176945
At time: 67.1312108039856 and batch: 350, loss is 3.858493294715881 and perplexity is 47.393888923075124
At time: 67.77218866348267 and batch: 400, loss is 3.8064659214019776 and perplexity is 44.99115529274792
At time: 68.41063332557678 and batch: 450, loss is 3.8269759225845337 and perplexity is 45.92345196246376
At time: 69.04776430130005 and batch: 500, loss is 3.726992673873901 and perplexity is 41.55395377748812
At time: 69.68504214286804 and batch: 550, loss is 3.807713341712952 and perplexity is 45.04731319264469
At time: 70.3186342716217 and batch: 600, loss is 3.820162463188171 and perplexity is 45.61161792780445
At time: 70.95271325111389 and batch: 650, loss is 3.6773424530029297 and perplexity is 39.54117179556169
At time: 71.58685302734375 and batch: 700, loss is 3.6856886625289915 and perplexity is 39.87257174322076
At time: 72.21994233131409 and batch: 750, loss is 3.800572843551636 and perplexity is 44.726798614329894
At time: 72.85356879234314 and batch: 800, loss is 3.7478668546676634 and perplexity is 42.430475025748095
At time: 73.48638319969177 and batch: 850, loss is 3.825975103378296 and perplexity is 45.87751388141434
At time: 74.11979174613953 and batch: 900, loss is 3.795949215888977 and perplexity is 44.52047589826585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.493759364297945 and perplexity of 89.45711641906806
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.71810603141785 and batch: 50, loss is 3.863778691291809 and perplexity is 47.64504757315877
At time: 76.35140585899353 and batch: 100, loss is 3.727674336433411 and perplexity is 41.582289208481825
At time: 76.98520731925964 and batch: 150, loss is 3.72469473361969 and perplexity is 41.45857490382868
At time: 77.61921453475952 and batch: 200, loss is 3.6105023431777954 and perplexity is 36.98462712421226
At time: 78.2635931968689 and batch: 250, loss is 3.7557007312774657 and perplexity is 42.764175509213494
At time: 78.89678955078125 and batch: 300, loss is 3.735928945541382 and perplexity is 41.92695533670717
At time: 79.5324034690857 and batch: 350, loss is 3.694789309501648 and perplexity is 40.23709412142416
At time: 80.16583609580994 and batch: 400, loss is 3.6361830615997315 and perplexity is 37.94671965870168
At time: 80.81443953514099 and batch: 450, loss is 3.6375524473190306 and perplexity is 37.99871895011426
At time: 81.44765377044678 and batch: 500, loss is 3.524689884185791 and perplexity is 33.94324564662639
At time: 82.08109879493713 and batch: 550, loss is 3.5849062490463255 and perplexity is 36.04997784062581
At time: 82.71519017219543 and batch: 600, loss is 3.5914249420166016 and perplexity is 36.28574418709205
At time: 83.34842348098755 and batch: 650, loss is 3.431935930252075 and perplexity is 30.9364756739558
At time: 83.98161745071411 and batch: 700, loss is 3.42267382144928 and perplexity is 30.65126155136733
At time: 84.62148594856262 and batch: 750, loss is 3.518698239326477 and perplexity is 33.74047783630361
At time: 85.26032423973083 and batch: 800, loss is 3.4481277894973754 and perplexity is 31.441472116142855
At time: 85.89420294761658 and batch: 850, loss is 3.5056560468673705 and perplexity is 33.303285203076165
At time: 86.52771353721619 and batch: 900, loss is 3.4681652307510378 and perplexity is 32.07783299710117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.429271541229666 and perplexity of 83.87029859730228
finished 7 epochs...
Completing Train Step...
At time: 88.10554671287537 and batch: 50, loss is 3.7515614652633666 and perplexity is 42.58752905645106
At time: 88.75036382675171 and batch: 100, loss is 3.6169472789764403 and perplexity is 37.22376044326552
At time: 89.38256216049194 and batch: 150, loss is 3.6156991720199585 and perplexity is 37.177330189898285
At time: 90.01388120651245 and batch: 200, loss is 3.5082274293899536 and perplexity is 33.38903088381845
At time: 90.65613555908203 and batch: 250, loss is 3.6556546688079834 and perplexity is 38.692843823206644
At time: 91.28800249099731 and batch: 300, loss is 3.6429611015319825 and perplexity is 38.204797683377535
At time: 91.92112040519714 and batch: 350, loss is 3.6055688428878785 and perplexity is 36.80261280858877
At time: 92.55242204666138 and batch: 400, loss is 3.553537311553955 and perplexity is 34.9366810969097
At time: 93.19062447547913 and batch: 450, loss is 3.5611790657043456 and perplexity is 35.204681316391444
At time: 93.83090806007385 and batch: 500, loss is 3.453632173538208 and perplexity is 31.615015239307244
At time: 94.46286082267761 and batch: 550, loss is 3.517336187362671 and perplexity is 33.69455283542569
At time: 95.09420108795166 and batch: 600, loss is 3.5320332193374635 and perplexity is 34.19341970732235
At time: 95.7265100479126 and batch: 650, loss is 3.3788459157943724 and perplexity is 29.336894322335247
At time: 96.35816025733948 and batch: 700, loss is 3.3749642086029055 and perplexity is 29.223237822007604
At time: 97.00433373451233 and batch: 750, loss is 3.4782189893722535 and perplexity is 32.40196241642444
At time: 97.63605427742004 and batch: 800, loss is 3.4163688135147097 and perplexity is 30.45861306738926
At time: 98.26835346221924 and batch: 850, loss is 3.481701292991638 and perplexity is 32.51499257586791
At time: 98.89928150177002 and batch: 900, loss is 3.4539139747619627 and perplexity is 31.62392564471331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.434745475037457 and perplexity of 84.33065789892125
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.47048997879028 and batch: 50, loss is 3.7068848705291746 and perplexity is 40.72673964792757
At time: 101.11674642562866 and batch: 100, loss is 3.5775062942504885 and perplexity is 35.784194240371164
At time: 101.75107955932617 and batch: 150, loss is 3.576477746963501 and perplexity is 35.74740742620025
At time: 102.3844108581543 and batch: 200, loss is 3.4672834777832033 and perplexity is 32.04956073905977
At time: 103.01879549026489 and batch: 250, loss is 3.6140785598754883 and perplexity is 37.11712895169839
At time: 103.65371489524841 and batch: 300, loss is 3.598417196273804 and perplexity is 36.54035244183659
At time: 104.28848838806152 and batch: 350, loss is 3.5532902336120604 and perplexity is 34.92805007995851
At time: 104.92327785491943 and batch: 400, loss is 3.501226053237915 and perplexity is 33.156078165648445
At time: 105.55674242973328 and batch: 450, loss is 3.5034568548202514 and perplexity is 33.23012535887452
At time: 106.1915180683136 and batch: 500, loss is 3.3888158226013183 and perplexity is 29.630843311900172
At time: 106.8257110118866 and batch: 550, loss is 3.445667853355408 and perplexity is 31.36422315523775
At time: 107.46042037010193 and batch: 600, loss is 3.458559303283691 and perplexity is 31.771170904998726
At time: 108.1046130657196 and batch: 650, loss is 3.301571207046509 and perplexity is 27.155271973885643
At time: 108.739741563797 and batch: 700, loss is 3.2887676906585694 and perplexity is 26.809805317497762
At time: 109.37424278259277 and batch: 750, loss is 3.386284532546997 and perplexity is 29.55593390164424
At time: 110.00888323783875 and batch: 800, loss is 3.3196624755859374 and perplexity is 27.651016090295798
At time: 110.64325976371765 and batch: 850, loss is 3.375697584152222 and perplexity is 29.244677290728283
At time: 111.27829051017761 and batch: 900, loss is 3.347678747177124 and perplexity is 28.436648322879318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421512133454623 and perplexity of 83.22203308109756
finished 9 epochs...
Completing Train Step...
At time: 112.86991596221924 and batch: 50, loss is 3.675570025444031 and perplexity is 39.47115000556636
At time: 113.50768876075745 and batch: 100, loss is 3.544012203216553 and perplexity is 34.60548526664336
At time: 114.14461469650269 and batch: 150, loss is 3.542383937835693 and perplexity is 34.549184201972096
At time: 114.78120374679565 and batch: 200, loss is 3.434451422691345 and perplexity is 31.014394105136507
At time: 115.41822361946106 and batch: 250, loss is 3.58158616065979 and perplexity is 35.93048719737273
At time: 116.0616192817688 and batch: 300, loss is 3.567791962623596 and perplexity is 35.438257702093956
At time: 116.70564866065979 and batch: 350, loss is 3.524912962913513 and perplexity is 33.95081850732065
At time: 117.34304141998291 and batch: 400, loss is 3.4746861934661863 and perplexity is 32.28769485707848
At time: 117.98013544082642 and batch: 450, loss is 3.4794201278686523 and perplexity is 32.440905043904195
At time: 118.61818885803223 and batch: 500, loss is 3.3674376821517944 and perplexity is 29.00411400457762
At time: 119.25534558296204 and batch: 550, loss is 3.426623225212097 and perplexity is 30.772555120052424
At time: 119.89343476295471 and batch: 600, loss is 3.4428252935409547 and perplexity is 31.27519506866241
At time: 120.53137993812561 and batch: 650, loss is 3.2884835147857667 and perplexity is 26.80218770009479
At time: 121.16874861717224 and batch: 700, loss is 3.2785450267791747 and perplexity is 26.537133778387304
At time: 121.80626773834229 and batch: 750, loss is 3.3797911643981933 and perplexity is 29.36463809104745
At time: 122.4437882900238 and batch: 800, loss is 3.3167497539520263 and perplexity is 27.57059355861228
At time: 123.08537340164185 and batch: 850, loss is 3.3763058233261107 and perplexity is 29.26247045978691
At time: 123.72388815879822 and batch: 900, loss is 3.3516414070129397 and perplexity is 28.549556647949213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.423441847709761 and perplexity of 83.38278287537715
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.33949375152588 and batch: 50, loss is 3.66109317779541 and perplexity is 38.903848457307625
At time: 125.99030923843384 and batch: 100, loss is 3.5311565589904785 and perplexity is 34.163456827690304
At time: 126.62850379943848 and batch: 150, loss is 3.5296655893325806 and perplexity is 34.11255810380673
At time: 127.2661988735199 and batch: 200, loss is 3.4213815116882325 and perplexity is 30.611676210646362
At time: 127.90361166000366 and batch: 250, loss is 3.5683446455001833 and perplexity is 35.457849233755326
At time: 128.53957867622375 and batch: 300, loss is 3.5534733819961546 and perplexity is 34.93444768172755
At time: 129.18950986862183 and batch: 350, loss is 3.509082098007202 and perplexity is 33.417579638799864
At time: 129.8260633945465 and batch: 400, loss is 3.459704432487488 and perplexity is 31.807573839694673
At time: 130.4622790813446 and batch: 450, loss is 3.4617100763320923 and perplexity is 31.871432521857926
At time: 131.09949207305908 and batch: 500, loss is 3.3475324392318724 and perplexity is 28.432488119636172
At time: 131.73838567733765 and batch: 550, loss is 3.404276480674744 and perplexity is 30.092515329213413
At time: 132.38550734519958 and batch: 600, loss is 3.4198843669891357 and perplexity is 30.565880391914806
At time: 133.03050923347473 and batch: 650, loss is 3.264268970489502 and perplexity is 26.160979550776172
At time: 133.66903448104858 and batch: 700, loss is 3.251649079322815 and perplexity is 25.832905320696018
At time: 134.30669116973877 and batch: 750, loss is 3.3512344408035277 and perplexity is 28.537940306989256
At time: 134.94415950775146 and batch: 800, loss is 3.286209754943848 and perplexity is 26.741315192983638
At time: 135.58290600776672 and batch: 850, loss is 3.342764263153076 and perplexity is 28.297239710283502
At time: 136.22152256965637 and batch: 900, loss is 3.317611174583435 and perplexity is 27.594353668979238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.422183520173373 and perplexity of 83.277926009601
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.87607264518738 and batch: 50, loss is 3.6536933994293213 and perplexity is 38.61703110232844
At time: 138.520277261734 and batch: 100, loss is 3.523765892982483 and perplexity is 33.91189687146378
At time: 139.15387058258057 and batch: 150, loss is 3.523469271659851 and perplexity is 33.90183937146902
At time: 139.78717803955078 and batch: 200, loss is 3.41474335193634 and perplexity is 30.409143978061763
At time: 140.4211609363556 and batch: 250, loss is 3.5617051887512208 and perplexity is 35.223208183868515
At time: 141.05410766601562 and batch: 300, loss is 3.5467632484436034 and perplexity is 34.70081759336223
At time: 141.68806886672974 and batch: 350, loss is 3.502472634315491 and perplexity is 33.197435677686656
At time: 142.3212161064148 and batch: 400, loss is 3.453530716896057 and perplexity is 31.611807848727796
At time: 142.954115152359 and batch: 450, loss is 3.4554099321365355 and perplexity is 31.671269092555317
At time: 143.58624005317688 and batch: 500, loss is 3.3410255241394045 and perplexity is 28.248080945136216
At time: 144.21918177604675 and batch: 550, loss is 3.397272891998291 and perplexity is 29.882496032449694
At time: 144.85174226760864 and batch: 600, loss is 3.4132357263565063 and perplexity is 30.36333291638093
At time: 145.49709343910217 and batch: 650, loss is 3.2573230123519896 and perplexity is 25.979896109033472
At time: 146.12933015823364 and batch: 700, loss is 3.2443823862075805 and perplexity is 25.645865927439107
At time: 146.76204752922058 and batch: 750, loss is 3.3434714031219483 and perplexity is 28.317256896133365
At time: 147.39485883712769 and batch: 800, loss is 3.2783592987060546 and perplexity is 26.532205545334293
At time: 148.02759742736816 and batch: 850, loss is 3.33432062625885 and perplexity is 28.05931398584807
At time: 148.66106605529785 and batch: 900, loss is 3.3083557987213137 and perplexity is 27.34013580975076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4219949800674225 and perplexity of 83.26222626067047
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 150.23531675338745 and batch: 50, loss is 3.6518082141876222 and perplexity is 38.544299423100476
At time: 150.89086866378784 and batch: 100, loss is 3.5218165063858033 and perplexity is 33.8458538668228
At time: 151.5229651927948 and batch: 150, loss is 3.5217107105255128 and perplexity is 33.84227330500334
At time: 152.1566927433014 and batch: 200, loss is 3.4129734086990355 and perplexity is 30.355369122584833
At time: 152.7908103466034 and batch: 250, loss is 3.5599612617492675 and perplexity is 35.16183501074257
At time: 153.4246072769165 and batch: 300, loss is 3.544909634590149 and perplexity is 34.636555254325756
At time: 154.05750012397766 and batch: 350, loss is 3.5006555938720703 and perplexity is 33.1371693641884
At time: 154.70754957199097 and batch: 400, loss is 3.451707820892334 and perplexity is 31.55423530085617
At time: 155.3487606048584 and batch: 450, loss is 3.453763651847839 and perplexity is 31.61917220133905
At time: 155.99539303779602 and batch: 500, loss is 3.3393189573287962 and perplexity is 28.19991481877799
At time: 156.63051342964172 and batch: 550, loss is 3.3953873443603517 and perplexity is 29.826204249736747
At time: 157.26472520828247 and batch: 600, loss is 3.4114608430862425 and perplexity is 30.309489341922838
At time: 157.89879441261292 and batch: 650, loss is 3.255496940612793 and perplexity is 25.932498244083174
At time: 158.53377747535706 and batch: 700, loss is 3.2425046300888063 and perplexity is 25.597754430737886
At time: 159.16765093803406 and batch: 750, loss is 3.341452856063843 and perplexity is 28.260154831523003
At time: 159.80131268501282 and batch: 800, loss is 3.2762444734573366 and perplexity is 26.47615385779573
At time: 160.43547987937927 and batch: 850, loss is 3.3321773290634153 and perplexity is 27.999238939213207
At time: 161.08532691001892 and batch: 900, loss is 3.305990309715271 and perplexity is 27.27553945017923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4219381254013275 and perplexity of 83.25749254916616
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 162.67260718345642 and batch: 50, loss is 3.651322546005249 and perplexity is 38.525584228313996
At time: 163.30635380744934 and batch: 100, loss is 3.521337194442749 and perplexity is 33.82963503209267
At time: 163.94136214256287 and batch: 150, loss is 3.5212518072128294 and perplexity is 33.82674653659016
At time: 164.57634830474854 and batch: 200, loss is 3.4125350952148437 and perplexity is 30.342066870471985
At time: 165.21001029014587 and batch: 250, loss is 3.559500255584717 and perplexity is 35.145628923885695
At time: 165.85120844841003 and batch: 300, loss is 3.5444316625595094 and perplexity is 34.620003905532485
At time: 166.48887372016907 and batch: 350, loss is 3.5001792907714844 and perplexity is 33.121389785904945
At time: 167.128981590271 and batch: 400, loss is 3.45125205039978 and perplexity is 31.539857088322336
At time: 167.77186632156372 and batch: 450, loss is 3.4533371257781984 and perplexity is 31.605688675836703
At time: 168.4120376110077 and batch: 500, loss is 3.3388845252990724 and perplexity is 28.187666533261634
At time: 169.04397463798523 and batch: 550, loss is 3.394906301498413 and perplexity is 29.811860017455572
At time: 169.67787766456604 and batch: 600, loss is 3.411011772155762 and perplexity is 30.295881287061285
At time: 170.31139850616455 and batch: 650, loss is 3.2550281047821046 and perplexity is 25.92034300935452
At time: 170.94734239578247 and batch: 700, loss is 3.242023572921753 and perplexity is 25.585443408898563
At time: 171.5893590450287 and batch: 750, loss is 3.3409410190582274 and perplexity is 28.24569393962132
At time: 172.22285962104797 and batch: 800, loss is 3.275711088180542 and perplexity is 26.462035632703245
At time: 172.85734272003174 and batch: 850, loss is 3.3316397619247438 and perplexity is 27.98419151331488
At time: 173.49135851860046 and batch: 900, loss is 3.3053898286819456 and perplexity is 27.259165922554065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421924747832834 and perplexity of 83.25637877380677
Annealing...
Model not improving. Stopping early with 83.22203308109756 lossat 13 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fcd10b4eda0>
ELAPSED
1093.4913523197174


RESULTS SO FAR:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.71266838151335, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.72064784946897, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.2283769027621978, 'rnn_dropout': 0.6017017728864293, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -86.49530323875238, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.9626269818184358, 'rnn_dropout': 0.9013488056646575, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.22203308109756, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6447005347113727, 'rnn_dropout': 0.9039291818368151, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -84.86372442356638, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.7009713395388745, 'rnn_dropout': 0.6331878518737708, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -84.71679877843526, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.15755079747865564, 'rnn_dropout': 0.5201807953434683, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.71266838151335, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6496406766207276, 'rnn_dropout': 0.8955071474877299, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.72064784946897, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.2283769027621978, 'rnn_dropout': 0.6017017728864293, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -86.49530323875238, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.9626269818184358, 'rnn_dropout': 0.9013488056646575, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}, {'best_accuracy': -83.22203308109756, 'params': {'tie_weights': 'FALSE', 'tune_wordvecs': 'FALSE', 'data': 'ptb', 'dropout': 0.6447005347113727, 'rnn_dropout': 0.9039291818368151, 'batch_size': 32, 'seq_len': 35, 'num_layers': 1, 'wordvec_source': 'None', 'wordvec_dim': 300}}]
