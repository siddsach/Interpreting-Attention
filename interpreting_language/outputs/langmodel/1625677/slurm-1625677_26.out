FALSE
FALSE
Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.331275224685669 and batch: 50, loss is 7.954783315658569 and perplexity is 2849.1708914518063
At time: 2.1562647819519043 and batch: 100, loss is 7.059906997680664 and perplexity is 1164.3368747073866
At time: 3.0199711322784424 and batch: 150, loss is 6.839802513122558 and perplexity is 934.3046036098265
At time: 3.8466427326202393 and batch: 200, loss is 6.61837986946106 and perplexity is 748.7310718627635
At time: 4.673556566238403 and batch: 250, loss is 6.620213851928711 and perplexity is 750.1054914670372
At time: 5.500772476196289 and batch: 300, loss is 6.480626001358032 and perplexity is 652.3792087420335
At time: 6.324247360229492 and batch: 350, loss is 6.45329607963562 and perplexity is 634.7911706176448
At time: 7.1490318775177 and batch: 400, loss is 6.327411413192749 and perplexity is 559.705869770034
At time: 7.973096609115601 and batch: 450, loss is 6.297649841308594 and perplexity is 543.2935824430847
At time: 8.798381567001343 and batch: 500, loss is 6.2758364009857175 and perplexity is 531.5708022182109
At time: 9.62455439567566 and batch: 550, loss is 6.278719005584716 and perplexity is 533.1053213006821
At time: 10.450656175613403 and batch: 600, loss is 6.223501167297363 and perplexity is 504.4663648468987
At time: 11.275944471359253 and batch: 650, loss is 6.1542980670928955 and perplexity is 470.73630116513664
At time: 12.103855848312378 and batch: 700, loss is 6.2656884765625 and perplexity is 526.2037402199015
At time: 12.928618431091309 and batch: 750, loss is 6.1991449737548825 and perplexity is 492.3279077965322
At time: 13.75318717956543 and batch: 800, loss is 6.217531032562256 and perplexity is 501.4636050385686
At time: 14.578117847442627 and batch: 850, loss is 6.248783950805664 and perplexity is 517.3832784474155
At time: 15.405031442642212 and batch: 900, loss is 6.113808889389038 and perplexity is 452.0572763483303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.052424182630565 and perplexity of 425.1424046775405
finished 1 epochs...
Completing Train Step...
At time: 17.331907987594604 and batch: 50, loss is 5.816519575119019 and perplexity is 335.80128627616267
At time: 18.10179114341736 and batch: 100, loss is 5.471092472076416 and perplexity is 237.71975314643984
At time: 18.854018926620483 and batch: 150, loss is 5.3344778633117675 and perplexity is 207.3644479503642
At time: 19.608772039413452 and batch: 200, loss is 5.14057918548584 and perplexity is 170.81467305586708
At time: 20.36316752433777 and batch: 250, loss is 5.170758295059204 and perplexity is 176.04828342547475
At time: 21.116908073425293 and batch: 300, loss is 5.0846888542175295 and perplexity is 161.52967214157732
At time: 21.870359897613525 and batch: 350, loss is 5.0273600101470945 and perplexity is 152.5298035621166
At time: 22.623586177825928 and batch: 400, loss is 4.866037454605102 and perplexity is 129.80553613155573
At time: 23.37733221054077 and batch: 450, loss is 4.858042907714844 and perplexity is 128.77193677062672
At time: 24.132436513900757 and batch: 500, loss is 4.7665016460418705 and perplexity is 117.50743944927711
At time: 24.886937856674194 and batch: 550, loss is 4.8242740821838375 and perplexity is 124.4960616640373
At time: 25.641026735305786 and batch: 600, loss is 4.751446399688721 and perplexity is 115.75158656398845
At time: 26.395238637924194 and batch: 650, loss is 4.612149076461792 and perplexity is 100.70032996798425
At time: 27.15025281906128 and batch: 700, loss is 4.659690599441529 and perplexity is 105.60340334952679
At time: 27.90332794189453 and batch: 750, loss is 4.68017915725708 and perplexity is 107.7893820916664
At time: 28.68162703514099 and batch: 800, loss is 4.608255281448364 and perplexity is 100.3089859264943
At time: 29.43465805053711 and batch: 850, loss is 4.666678419113159 and perplexity is 106.34392519211208
At time: 30.18775963783264 and batch: 900, loss is 4.595587110519409 and perplexity is 99.0462695872278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.668138895949272 and perplexity of 106.4993515021523
finished 2 epochs...
Completing Train Step...
At time: 32.05532884597778 and batch: 50, loss is 4.649856214523315 and perplexity is 104.56994885616588
At time: 32.824106216430664 and batch: 100, loss is 4.502858228683472 and perplexity is 90.27478889396036
At time: 33.580201625823975 and batch: 150, loss is 4.489443855285645 and perplexity is 89.07189523753826
At time: 34.33565092086792 and batch: 200, loss is 4.3738271999359135 and perplexity is 79.34672711515223
At time: 35.09061622619629 and batch: 250, loss is 4.511934127807617 and perplexity is 91.09784310018324
At time: 35.84574890136719 and batch: 300, loss is 4.46864483833313 and perplexity is 87.238420708549
At time: 36.60035157203674 and batch: 350, loss is 4.449363965988159 and perplexity is 85.57249966954421
At time: 37.35532093048096 and batch: 400, loss is 4.35122727394104 and perplexity is 77.57360859676012
At time: 38.11066222190857 and batch: 450, loss is 4.371136360168457 and perplexity is 79.13350478862611
At time: 38.86507296562195 and batch: 500, loss is 4.254282364845276 and perplexity is 70.40627303782699
At time: 39.619194746017456 and batch: 550, loss is 4.334047198295593 and perplexity is 76.25227097293332
At time: 40.37527394294739 and batch: 600, loss is 4.320382280349731 and perplexity is 75.2173769218104
At time: 41.133015871047974 and batch: 650, loss is 4.172944588661194 and perplexity is 64.9062933304409
At time: 41.8894784450531 and batch: 700, loss is 4.192769746780396 and perplexity is 66.20591085390602
At time: 42.64517688751221 and batch: 750, loss is 4.2819836235046385 and perplexity is 72.38388006060637
At time: 43.399876832962036 and batch: 800, loss is 4.216120414733886 and perplexity is 67.77005391687551
At time: 44.15587520599365 and batch: 850, loss is 4.293994088172912 and perplexity is 73.25848579168635
At time: 44.91369271278381 and batch: 900, loss is 4.2399114942550655 and perplexity is 69.40170911696099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.466026828713613 and perplexity of 87.01032838816518
finished 3 epochs...
Completing Train Step...
At time: 46.80070233345032 and batch: 50, loss is 4.3216823291778566 and perplexity is 75.31522677554453
At time: 47.57860040664673 and batch: 100, loss is 4.181910810470581 and perplexity is 65.4908743785851
At time: 48.34617757797241 and batch: 150, loss is 4.173026237487793 and perplexity is 64.91159306948605
At time: 49.10366868972778 and batch: 200, loss is 4.0643947267532345 and perplexity is 58.22965300288581
At time: 49.86105537414551 and batch: 250, loss is 4.219333553314209 and perplexity is 67.98815870451274
At time: 50.62007117271423 and batch: 300, loss is 4.179941177368164 and perplexity is 65.36200833557571
At time: 51.37703800201416 and batch: 350, loss is 4.1667223119735715 and perplexity is 64.50368229219065
At time: 52.13605046272278 and batch: 400, loss is 4.085037746429443 and perplexity is 59.44418151449994
At time: 52.88962435722351 and batch: 450, loss is 4.111277718544006 and perplexity is 61.02464009178186
At time: 53.642985582351685 and batch: 500, loss is 3.9859260177612303 and perplexity is 53.83511867904108
At time: 54.40142631530762 and batch: 550, loss is 4.0731963491439815 and perplexity is 58.74443053604072
At time: 55.16139507293701 and batch: 600, loss is 4.076510910987854 and perplexity is 58.93946563342168
At time: 55.92214918136597 and batch: 650, loss is 3.9259206008911134 and perplexity is 50.69973079861927
At time: 56.681440114974976 and batch: 700, loss is 3.928619613647461 and perplexity is 50.836754850438794
At time: 57.44173312187195 and batch: 750, loss is 4.041318302154541 and perplexity is 56.90130649716554
At time: 58.201499462127686 and batch: 800, loss is 3.981422438621521 and perplexity is 53.59321309091976
At time: 58.96161913871765 and batch: 850, loss is 4.065428471565246 and perplexity is 58.2898787282351
At time: 59.7219512462616 and batch: 900, loss is 4.017197318077088 and perplexity is 55.545211906989984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.402354201225386 and perplexity of 81.64284628809985
finished 4 epochs...
Completing Train Step...
At time: 61.61506414413452 and batch: 50, loss is 4.1034610414505 and perplexity is 60.549489653532945
At time: 62.36923170089722 and batch: 100, loss is 3.9698866748809816 and perplexity is 52.97852670162887
At time: 63.125314235687256 and batch: 150, loss is 3.964871644973755 and perplexity is 52.713502912351125
At time: 63.88097643852234 and batch: 200, loss is 3.8542575788497926 and perplexity is 47.19356642989292
At time: 64.6365385055542 and batch: 250, loss is 4.014665288925171 and perplexity is 55.404747715950336
At time: 65.39087867736816 and batch: 300, loss is 3.9788687229156494 and perplexity is 53.456525865362096
At time: 66.16945123672485 and batch: 350, loss is 3.963973379135132 and perplexity is 52.666173433860095
At time: 66.9297788143158 and batch: 400, loss is 3.8949298810958863 and perplexity is 49.15260671493054
At time: 67.69039154052734 and batch: 450, loss is 3.9199892902374267 and perplexity is 50.399905004158754
At time: 68.45065355300903 and batch: 500, loss is 3.7963975715637206 and perplexity is 44.540441381761575
At time: 69.21174311637878 and batch: 550, loss is 3.8793284559249877 and perplexity is 48.391706993152226
At time: 69.97195529937744 and batch: 600, loss is 3.8941336154937742 and perplexity is 49.113483763146796
At time: 70.73046731948853 and batch: 650, loss is 3.740999264717102 and perplexity is 42.140078226284835
At time: 71.50060200691223 and batch: 700, loss is 3.744270372390747 and perplexity is 42.2781486580488
At time: 72.26131844520569 and batch: 750, loss is 3.8611567878723143 and perplexity is 47.520290481995616
At time: 73.03275609016418 and batch: 800, loss is 3.8063321113586426 and perplexity is 44.98513542707673
At time: 73.7943069934845 and batch: 850, loss is 3.888761811256409 and perplexity is 48.85036309204748
At time: 74.55318236351013 and batch: 900, loss is 3.8432631969451903 and perplexity is 46.677544207713424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387997979987158 and perplexity of 80.47913674683322
finished 5 epochs...
Completing Train Step...
At time: 76.46009731292725 and batch: 50, loss is 3.9341967582702635 and perplexity is 51.12107088325081
At time: 77.24724292755127 and batch: 100, loss is 3.8076971626281737 and perplexity is 45.04658437424134
At time: 78.01854658126831 and batch: 150, loss is 3.8043249654769897 and perplexity is 44.89493425151
At time: 78.78237891197205 and batch: 200, loss is 3.688717498779297 and perplexity is 39.99352231117672
At time: 79.54306364059448 and batch: 250, loss is 3.8529572677612305 and perplexity is 47.13223999251955
At time: 80.30673122406006 and batch: 300, loss is 3.819618515968323 and perplexity is 45.58681436156686
At time: 81.07124900817871 and batch: 350, loss is 3.80350350856781 and perplexity is 44.858070140794126
At time: 81.83545923233032 and batch: 400, loss is 3.7406374979019166 and perplexity is 42.12483610160693
At time: 82.59867024421692 and batch: 450, loss is 3.7693842029571534 and perplexity is 43.35335974350954
At time: 83.36164140701294 and batch: 500, loss is 3.6452380132675173 and perplexity is 38.29188574386561
At time: 84.12410569190979 and batch: 550, loss is 3.72508816242218 and perplexity is 41.474889110334985
At time: 84.88807892799377 and batch: 600, loss is 3.745718421936035 and perplexity is 42.33941385880645
At time: 85.67172527313232 and batch: 650, loss is 3.593476929664612 and perplexity is 36.36027853158676
At time: 86.43419981002808 and batch: 700, loss is 3.593886036872864 and perplexity is 36.37515682682948
At time: 87.20971584320068 and batch: 750, loss is 3.7173397445678713 and perplexity is 41.15476616385034
At time: 87.97310018539429 and batch: 800, loss is 3.663687229156494 and perplexity is 39.00489804559632
At time: 88.73687505722046 and batch: 850, loss is 3.7464830923080443 and perplexity is 42.37180193567575
At time: 89.50748634338379 and batch: 900, loss is 3.700888981819153 and perplexity is 40.48327726553763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.402190326011344 and perplexity of 81.62946814539238
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.39864540100098 and batch: 50, loss is 3.8114250183105467 and perplexity is 45.214824933198955
At time: 92.17975044250488 and batch: 100, loss is 3.6848482847213746 and perplexity is 39.83907779455193
At time: 92.94577503204346 and batch: 150, loss is 3.6919098711013794 and perplexity is 40.12140053371148
At time: 93.70876812934875 and batch: 200, loss is 3.554569582939148 and perplexity is 34.972763853494115
At time: 94.47248005867004 and batch: 250, loss is 3.709462966918945 and perplexity is 40.831872571555245
At time: 95.23654651641846 and batch: 300, loss is 3.669393763542175 and perplexity is 39.228117135457595
At time: 96.00008010864258 and batch: 350, loss is 3.634219694137573 and perplexity is 37.87228939491675
At time: 96.76505422592163 and batch: 400, loss is 3.563089942932129 and perplexity is 35.272017455287966
At time: 97.52758264541626 and batch: 450, loss is 3.573560748100281 and perplexity is 35.64328421684762
At time: 98.29253506660461 and batch: 500, loss is 3.4422696733474734 and perplexity is 31.257822765375373
At time: 99.05535697937012 and batch: 550, loss is 3.4984391784667968 and perplexity is 33.063804964548765
At time: 99.81932711601257 and batch: 600, loss is 3.5088757467269898 and perplexity is 33.41068458988535
At time: 100.58346819877625 and batch: 650, loss is 3.3413688230514524 and perplexity is 28.25778014535931
At time: 101.3478376865387 and batch: 700, loss is 3.320606379508972 and perplexity is 27.677128314636082
At time: 102.1116931438446 and batch: 750, loss is 3.422772846221924 and perplexity is 30.654296935860355
At time: 102.87433362007141 and batch: 800, loss is 3.352524757385254 and perplexity is 28.574787051446126
At time: 103.6384687423706 and batch: 850, loss is 3.411147532463074 and perplexity is 30.299994544417327
At time: 104.42320990562439 and batch: 900, loss is 3.35133207321167 and perplexity is 28.540726670842336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387551085589683 and perplexity of 80.44317910673988
finished 7 epochs...
Completing Train Step...
At time: 106.32505750656128 and batch: 50, loss is 3.69331148147583 and perplexity is 40.17767453283493
At time: 107.08898115158081 and batch: 100, loss is 3.562291646003723 and perplexity is 35.24387114814475
At time: 107.85294318199158 and batch: 150, loss is 3.5670945549011233 and perplexity is 35.41355140368457
At time: 108.61502313613892 and batch: 200, loss is 3.4344997119903566 and perplexity is 31.01589180464825
At time: 109.37884426116943 and batch: 250, loss is 3.5917147541046144 and perplexity is 36.296261758366036
At time: 110.1421890258789 and batch: 300, loss is 3.5552464628219607 and perplexity is 34.99644422727309
At time: 110.90674567222595 and batch: 350, loss is 3.5241388273239136 and perplexity is 33.92454614091708
At time: 111.66988229751587 and batch: 400, loss is 3.459839072227478 and perplexity is 31.811856691480717
At time: 112.43468928337097 and batch: 450, loss is 3.4751079082489014 and perplexity is 32.30131392677917
At time: 113.19999885559082 and batch: 500, loss is 3.3501004934310914 and perplexity is 28.505598125194997
At time: 113.96375250816345 and batch: 550, loss is 3.409705181121826 and perplexity is 30.256322809210456
At time: 114.72900533676147 and batch: 600, loss is 3.4276948595046997 and perplexity is 30.805549721305578
At time: 115.50417232513428 and batch: 650, loss is 3.2672896242141722 and perplexity is 26.240122282313497
At time: 116.26903510093689 and batch: 700, loss is 3.252587342262268 and perplexity is 25.857154752769556
At time: 117.03403425216675 and batch: 750, loss is 3.3646444606781007 and perplexity is 28.92321213153937
At time: 117.7979257106781 and batch: 800, loss is 3.300515179634094 and perplexity is 27.126610398657807
At time: 118.5630853176117 and batch: 850, loss is 3.36880476474762 and perplexity is 29.043792139504873
At time: 119.32809352874756 and batch: 900, loss is 3.3168800354003904 and perplexity is 27.574185729464887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.406547650898973 and perplexity of 81.98593030523577
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.19696545600891 and batch: 50, loss is 3.638972291946411 and perplexity is 38.05270954712223
At time: 121.98016858100891 and batch: 100, loss is 3.5184122467041017 and perplexity is 33.73082968828235
At time: 122.741379737854 and batch: 150, loss is 3.5289469957351685 and perplexity is 34.08805384331878
At time: 123.5137631893158 and batch: 200, loss is 3.3913894653320313 and perplexity is 29.70720073260813
At time: 124.27438998222351 and batch: 250, loss is 3.5438758993148802 and perplexity is 34.60076872543103
At time: 125.03556752204895 and batch: 300, loss is 3.504702296257019 and perplexity is 33.27153731667663
At time: 125.798348903656 and batch: 350, loss is 3.4686761236190797 and perplexity is 32.09422552025666
At time: 126.55970168113708 and batch: 400, loss is 3.401588134765625 and perplexity is 30.01172488372226
At time: 127.32039642333984 and batch: 450, loss is 3.4091720390319824 and perplexity is 30.24019618930887
At time: 128.08745527267456 and batch: 500, loss is 3.283353304862976 and perplexity is 26.66503895253805
At time: 128.85032415390015 and batch: 550, loss is 3.33265576839447 and perplexity is 28.012638081444056
At time: 129.61208176612854 and batch: 600, loss is 3.349625549316406 and perplexity is 28.492062773652066
At time: 130.37425303459167 and batch: 650, loss is 3.182010464668274 and perplexity is 24.095147331158625
At time: 131.1343388557434 and batch: 700, loss is 3.1601172399520876 and perplexity is 23.573359506603207
At time: 131.89502787590027 and batch: 750, loss is 3.2650688076019287 and perplexity is 26.18191244345457
At time: 132.65690326690674 and batch: 800, loss is 3.196814603805542 and perplexity is 24.45450869944196
At time: 133.41837215423584 and batch: 850, loss is 3.2573557710647583 and perplexity is 25.980747190927968
At time: 134.18044900894165 and batch: 900, loss is 3.205675859451294 and perplexity is 24.67216930158117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.403735853221319 and perplexity of 81.75572625208426
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.05685186386108 and batch: 50, loss is 3.6139560651779177 and perplexity is 37.112582578671734
At time: 136.8332166671753 and batch: 100, loss is 3.493920397758484 and perplexity is 32.91473394381926
At time: 137.59549021720886 and batch: 150, loss is 3.5044087886810305 and perplexity is 33.26177330138572
At time: 138.35807418823242 and batch: 200, loss is 3.368715467453003 and perplexity is 29.041198723235652
At time: 139.12135887145996 and batch: 250, loss is 3.5206502532958983 and perplexity is 33.806404043880555
At time: 139.8851079940796 and batch: 300, loss is 3.481933116912842 and perplexity is 32.522531202728004
At time: 140.64771461486816 and batch: 350, loss is 3.4459560537338256 and perplexity is 31.373263638892695
At time: 141.41047620773315 and batch: 400, loss is 3.3782120513916016 and perplexity is 29.318304601639994
At time: 142.17147421836853 and batch: 450, loss is 3.3830004739761352 and perplexity is 29.4590296902407
At time: 142.94496369361877 and batch: 500, loss is 3.257698450088501 and perplexity is 25.989651773631177
At time: 143.70535707473755 and batch: 550, loss is 3.305448541641235 and perplexity is 27.260766435838118
At time: 144.46760821342468 and batch: 600, loss is 3.322619366645813 and perplexity is 27.732898131047516
At time: 145.22944283485413 and batch: 650, loss is 3.1536636781692504 and perplexity is 23.421717217344902
At time: 145.9914436340332 and batch: 700, loss is 3.1316886949539184 and perplexity is 22.912639349393473
At time: 146.75090622901917 and batch: 750, loss is 3.2352376079559324 and perplexity is 25.41240925216182
At time: 147.50983476638794 and batch: 800, loss is 3.165108332633972 and perplexity is 23.691310435877746
At time: 148.26975345611572 and batch: 850, loss is 3.2254606676101685 and perplexity is 25.165164262100234
At time: 149.038391828537 and batch: 900, loss is 3.1741915702819825 and perplexity is 23.907484532818888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.401345448951199 and perplexity of 81.56053040638236
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.94467616081238 and batch: 50, loss is 3.6034391593933104 and perplexity is 36.72431829239517
At time: 151.7085998058319 and batch: 100, loss is 3.483180270195007 and perplexity is 32.56311708741088
At time: 152.47998356819153 and batch: 150, loss is 3.493891921043396 and perplexity is 32.91379665366407
At time: 153.24823999404907 and batch: 200, loss is 3.359034194946289 and perplexity is 28.76139955585981
At time: 154.01217675209045 and batch: 250, loss is 3.511444215774536 and perplexity is 33.49660919922427
At time: 154.77600932121277 and batch: 300, loss is 3.472792615890503 and perplexity is 32.2266134515678
At time: 155.54042315483093 and batch: 350, loss is 3.437379379272461 and perplexity is 31.105335976421376
At time: 156.30485367774963 and batch: 400, loss is 3.3699299478530884 and perplexity is 29.076490115894558
At time: 157.06906700134277 and batch: 450, loss is 3.375456485748291 and perplexity is 29.237627295615454
At time: 157.83484363555908 and batch: 500, loss is 3.2491228199005127 and perplexity is 25.767727063481267
At time: 158.602534532547 and batch: 550, loss is 3.297212100028992 and perplexity is 27.037156862500538
At time: 159.3726100921631 and batch: 600, loss is 3.314649152755737 and perplexity is 27.512739522203237
At time: 160.1360821723938 and batch: 650, loss is 3.145555601119995 and perplexity is 23.232579935518107
At time: 160.89889407157898 and batch: 700, loss is 3.123834114074707 and perplexity is 22.73337511481503
At time: 161.67526030540466 and batch: 750, loss is 3.226929097175598 and perplexity is 25.202144678244768
At time: 162.44009590148926 and batch: 800, loss is 3.1555000352859497 and perplexity is 23.464767370071623
At time: 163.20376443862915 and batch: 850, loss is 3.2160601949691774 and perplexity is 24.929708256729487
At time: 163.97669649124146 and batch: 900, loss is 3.1642528200149536 and perplexity is 23.671050888223775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.400478415293236 and perplexity of 81.48984532895743
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 165.90139174461365 and batch: 50, loss is 3.6002772521972655 and perplexity is 36.60838279135437
At time: 166.70655131340027 and batch: 100, loss is 3.479858841896057 and perplexity is 32.45514044641572
At time: 167.48604083061218 and batch: 150, loss is 3.490689935684204 and perplexity is 32.8085757065297
At time: 168.27469873428345 and batch: 200, loss is 3.356179213523865 and perplexity is 28.67940339889266
At time: 169.04902029037476 and batch: 250, loss is 3.50875018119812 and perplexity is 33.40648962298205
At time: 169.82379055023193 and batch: 300, loss is 3.470269589424133 and perplexity is 32.14540733857174
At time: 170.5872311592102 and batch: 350, loss is 3.434658284187317 and perplexity is 31.020810452722504
At time: 171.35514998435974 and batch: 400, loss is 3.3673444509506227 and perplexity is 29.00141004223884
At time: 172.12655234336853 and batch: 450, loss is 3.373473892211914 and perplexity is 29.179718388600964
At time: 172.8910529613495 and batch: 500, loss is 3.2465987491607664 and perplexity is 25.70276951078629
At time: 173.652761220932 and batch: 550, loss is 3.2947081470489503 and perplexity is 26.969541780823103
At time: 174.42626690864563 and batch: 600, loss is 3.31254638671875 and perplexity is 27.454947450848472
At time: 175.2001268863678 and batch: 650, loss is 3.1432293319702147 and perplexity is 23.178597514708333
At time: 175.97409915924072 and batch: 700, loss is 3.1215174341201783 and perplexity is 22.68077011838573
At time: 176.73574495315552 and batch: 750, loss is 3.224695110321045 and perplexity is 25.145906259660016
At time: 177.51602959632874 and batch: 800, loss is 3.153024883270264 and perplexity is 23.40676032156393
At time: 178.2819368839264 and batch: 850, loss is 3.2135974311828615 and perplexity is 24.868387813907784
At time: 179.04414892196655 and batch: 900, loss is 3.1614783334732057 and perplexity is 23.605466899123822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.399981355013913 and perplexity of 81.44935002881259
Annealing...
Model not improving. Stopping early with 80.44317910673988 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -80.44317910673988
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
190.9781482219696


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0562121868133545 and batch: 50, loss is 6.763676681518555 and perplexity is 865.8196941529278
At time: 1.9089879989624023 and batch: 100, loss is 5.885798854827881 and perplexity is 359.8901531781746
At time: 2.738438367843628 and batch: 150, loss is 5.634991884231567 and perplexity is 280.0566444263032
At time: 3.581413507461548 and batch: 200, loss is 5.423366212844849 and perplexity is 226.6407609048686
At time: 4.4146411418914795 and batch: 250, loss is 5.44109769821167 and perplexity is 230.69527831879302
At time: 5.244780778884888 and batch: 300, loss is 5.355503187179566 and perplexity is 211.77050975304354
At time: 6.074356555938721 and batch: 350, loss is 5.3020046424865725 and perplexity is 200.73881646070737
At time: 6.90657377243042 and batch: 400, loss is 5.143419198989868 and perplexity is 171.3004785545911
At time: 7.738585710525513 and batch: 450, loss is 5.143868989944458 and perplexity is 171.37754529102315
At time: 8.570936918258667 and batch: 500, loss is 5.082122116088867 and perplexity is 161.115599408648
At time: 9.402161121368408 and batch: 550, loss is 5.13581955909729 and perplexity is 170.00359078591703
At time: 10.232842445373535 and batch: 600, loss is 5.051809701919556 and perplexity is 156.30507428248566
At time: 11.065199613571167 and batch: 650, loss is 4.9373751068115235 and perplexity is 139.40384867595074
At time: 11.895956993103027 and batch: 700, loss is 5.019714069366455 and perplexity is 151.36801685285926
At time: 12.730439186096191 and batch: 750, loss is 5.015167369842529 and perplexity is 150.68135417006098
At time: 13.566418886184692 and batch: 800, loss is 4.960615396499634 and perplexity is 142.68157462644476
At time: 14.399415731430054 and batch: 850, loss is 5.011827125549316 and perplexity is 150.1788812949614
At time: 15.232393741607666 and batch: 900, loss is 4.92806191444397 and perplexity is 138.11158072690523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.914829619943279 and perplexity of 136.2960856893273
finished 1 epochs...
Completing Train Step...
At time: 17.148630619049072 and batch: 50, loss is 4.877007808685303 and perplexity is 131.23738842773125
At time: 17.91147541999817 and batch: 100, loss is 4.729449081420898 and perplexity is 113.23316290851591
At time: 18.673358917236328 and batch: 150, loss is 4.698180599212646 and perplexity is 109.7473163443074
At time: 19.43809223175049 and batch: 200, loss is 4.586745481491089 and perplexity is 98.17439927166521
At time: 20.20107364654541 and batch: 250, loss is 4.697514925003052 and perplexity is 109.67428469658378
At time: 20.964412212371826 and batch: 300, loss is 4.652135620117187 and perplexity is 104.80857804557387
At time: 21.728298664093018 and batch: 350, loss is 4.632027463912964 and perplexity is 102.7221185161793
At time: 22.492143630981445 and batch: 400, loss is 4.51953088760376 and perplexity is 91.79252686411573
At time: 23.269989728927612 and batch: 450, loss is 4.5330444431304935 and perplexity is 93.04138955534108
At time: 24.03454089164734 and batch: 500, loss is 4.421939897537231 and perplexity is 83.25764009288869
At time: 24.804296016693115 and batch: 550, loss is 4.501121034622193 and perplexity is 90.11810020554518
At time: 25.56926441192627 and batch: 600, loss is 4.475308046340943 and perplexity is 87.82164938111998
At time: 26.333067655563354 and batch: 650, loss is 4.330877933502197 and perplexity is 76.01098987891766
At time: 27.096888065338135 and batch: 700, loss is 4.364596042633057 and perplexity is 78.61763535346645
At time: 27.86194396018982 and batch: 750, loss is 4.434259557723999 and perplexity is 84.28969012647394
At time: 28.626129150390625 and batch: 800, loss is 4.365402460098267 and perplexity is 78.68105955744477
At time: 29.390405416488647 and batch: 850, loss is 4.4402075862884525 and perplexity is 84.79254161601398
At time: 30.154858589172363 and batch: 900, loss is 4.3744268798828125 and perplexity is 79.39432402628545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.514560229157748 and perplexity of 91.33738966805777
finished 2 epochs...
Completing Train Step...
At time: 32.02792954444885 and batch: 50, loss is 4.4202047538757325 and perplexity is 83.1133013868193
At time: 32.80457091331482 and batch: 100, loss is 4.27121533870697 and perplexity is 71.6086114732284
At time: 33.56938672065735 and batch: 150, loss is 4.262687482833862 and perplexity is 71.00054001332444
At time: 34.33313703536987 and batch: 200, loss is 4.156924862861633 and perplexity is 63.87479651614881
At time: 35.097084760665894 and batch: 250, loss is 4.306811714172364 and perplexity is 74.20352933266186
At time: 35.861414432525635 and batch: 300, loss is 4.274170951843262 and perplexity is 71.8205719081923
At time: 36.626548290252686 and batch: 350, loss is 4.261199998855591 and perplexity is 70.89500635687628
At time: 37.392356395721436 and batch: 400, loss is 4.176976809501648 and perplexity is 65.16853819837985
At time: 38.16196084022522 and batch: 450, loss is 4.201617469787598 and perplexity is 66.79428144635419
At time: 38.93887186050415 and batch: 500, loss is 4.074324049949646 and perplexity is 58.8107140446431
At time: 39.70525813102722 and batch: 550, loss is 4.160591979026794 and perplexity is 64.10946282633172
At time: 40.47041606903076 and batch: 600, loss is 4.170372891426086 and perplexity is 64.73958844446608
At time: 41.23617696762085 and batch: 650, loss is 4.014330606460572 and perplexity is 55.38620782109618
At time: 42.00154900550842 and batch: 700, loss is 4.029052877426148 and perplexity is 56.20765049427228
At time: 42.78942155838013 and batch: 750, loss is 4.131466174125672 and perplexity is 62.26915346013071
At time: 43.55935287475586 and batch: 800, loss is 4.073290281295776 and perplexity is 58.7499487859734
At time: 44.33527421951294 and batch: 850, loss is 4.150383129119873 and perplexity is 63.45830836709474
At time: 45.099167346954346 and batch: 900, loss is 4.0990889406204225 and perplexity is 60.285339046876885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.40679931640625 and perplexity of 82.00656593250453
finished 3 epochs...
Completing Train Step...
At time: 47.05326819419861 and batch: 50, loss is 4.165297431945801 and perplexity is 64.41183773285317
At time: 47.83425784111023 and batch: 100, loss is 4.020201139450073 and perplexity is 55.712310643417624
At time: 48.59420347213745 and batch: 150, loss is 4.016669397354126 and perplexity is 55.515896177434406
At time: 49.362897634506226 and batch: 200, loss is 3.9094008731842043 and perplexity is 49.869065127100015
At time: 50.14116668701172 and batch: 250, loss is 4.067158341407776 and perplexity is 58.39079989664785
At time: 50.90588927268982 and batch: 300, loss is 4.041565799713135 and perplexity is 56.91539117449415
At time: 51.67566537857056 and batch: 350, loss is 4.031423482894898 and perplexity is 56.34105471983783
At time: 52.4441773891449 and batch: 400, loss is 3.957753300666809 and perplexity is 52.339602404092275
At time: 53.21007585525513 and batch: 450, loss is 3.9848502683639526 and perplexity is 53.777236721392185
At time: 54.00884485244751 and batch: 500, loss is 3.857232537269592 and perplexity is 47.33417437539681
At time: 54.781553745269775 and batch: 550, loss is 3.944686007499695 and perplexity is 51.66011467664843
At time: 55.56356716156006 and batch: 600, loss is 3.9640957164764403 and perplexity is 52.67261686762312
At time: 56.37545943260193 and batch: 650, loss is 3.802420997619629 and perplexity is 44.809537062286225
At time: 57.155351638793945 and batch: 700, loss is 3.812593250274658 and perplexity is 45.26767720278205
At time: 57.934361934661865 and batch: 750, loss is 3.922743248939514 and perplexity is 50.53889556040992
At time: 58.70378565788269 and batch: 800, loss is 3.8743819570541382 and perplexity is 48.15292851475517
At time: 59.47411632537842 and batch: 850, loss is 3.949813652038574 and perplexity is 51.92568968678226
At time: 60.244170904159546 and batch: 900, loss is 3.9048182344436646 and perplexity is 49.6410560579176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374753351080908 and perplexity of 79.42024821789293
finished 4 epochs...
Completing Train Step...
At time: 62.12021327018738 and batch: 50, loss is 3.9778212785720823 and perplexity is 53.400562444118826
At time: 62.90541124343872 and batch: 100, loss is 3.8391579818725585 and perplexity is 46.48631563544826
At time: 63.67794060707092 and batch: 150, loss is 3.83518349647522 and perplexity is 46.30192312817301
At time: 64.4413230419159 and batch: 200, loss is 3.7308922147750856 and perplexity is 41.71631147434924
At time: 65.20340275764465 and batch: 250, loss is 3.8899730920791624 and perplexity is 48.909570451175675
At time: 65.96506094932556 and batch: 300, loss is 3.8655194091796874 and perplexity is 47.72805638624067
At time: 66.72614526748657 and batch: 350, loss is 3.858272490501404 and perplexity is 47.38342530790811
At time: 67.49000430107117 and batch: 400, loss is 3.789852919578552 and perplexity is 44.24989150464717
At time: 68.25269198417664 and batch: 450, loss is 3.818931837081909 and perplexity is 45.55552160391352
At time: 69.0203115940094 and batch: 500, loss is 3.6930550479888917 and perplexity is 40.16737295254894
At time: 69.78600430488586 and batch: 550, loss is 3.777141432762146 and perplexity is 43.69096948306591
At time: 70.54824304580688 and batch: 600, loss is 3.8013119506835937 and perplexity is 44.75986872984904
At time: 71.31354832649231 and batch: 650, loss is 3.6409386920928957 and perplexity is 38.127610018764045
At time: 72.07566523551941 and batch: 700, loss is 3.6485181713104247 and perplexity is 38.41769540585036
At time: 72.84425163269043 and batch: 750, loss is 3.765555906295776 and perplexity is 43.187707506411655
At time: 73.62868547439575 and batch: 800, loss is 3.714333944320679 and perplexity is 41.03124888467254
At time: 74.41141724586487 and batch: 850, loss is 3.7941724061965942 and perplexity is 44.441441720319936
At time: 75.19366264343262 and batch: 900, loss is 3.7481480169296266 and perplexity is 42.44240655135127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3739122364619005 and perplexity of 79.35347477206912
finished 5 epochs...
Completing Train Step...
At time: 77.1139280796051 and batch: 50, loss is 3.826364426612854 and perplexity is 45.895378540849975
At time: 77.89527082443237 and batch: 100, loss is 3.6910360431671143 and perplexity is 40.086356646556936
At time: 78.66018414497375 and batch: 150, loss is 3.6906780290603636 and perplexity is 40.072007734099024
At time: 79.43312573432922 and batch: 200, loss is 3.589190435409546 and perplexity is 36.204753972247964
At time: 80.20818901062012 and batch: 250, loss is 3.7420610046386718 and perplexity is 42.18484379012355
At time: 80.9872899055481 and batch: 300, loss is 3.7230800771713257 and perplexity is 41.391687563090166
At time: 81.75066614151001 and batch: 350, loss is 3.7151482009887697 and perplexity is 41.06467245851423
At time: 82.51410222053528 and batch: 400, loss is 3.6528854465484617 and perplexity is 38.585842961769686
At time: 83.27903246879578 and batch: 450, loss is 3.6797952461242676 and perplexity is 39.63827715074138
At time: 84.05159735679626 and batch: 500, loss is 3.5549118947982787 and perplexity is 34.984737494550494
At time: 84.81924891471863 and batch: 550, loss is 3.638308072090149 and perplexity is 38.02744257419798
At time: 85.58258533477783 and batch: 600, loss is 3.666224865913391 and perplexity is 39.10400400285226
At time: 86.34580373764038 and batch: 650, loss is 3.509572997093201 and perplexity is 33.43398832527591
At time: 87.11003994941711 and batch: 700, loss is 3.5147075700759887 and perplexity is 33.606099057871724
At time: 87.8785126209259 and batch: 750, loss is 3.6317361879348753 and perplexity is 37.77835002707875
At time: 88.64499807357788 and batch: 800, loss is 3.5817750787734983 and perplexity is 35.93727575845956
At time: 89.40785956382751 and batch: 850, loss is 3.661377372741699 and perplexity is 38.91490630564827
At time: 90.17240595817566 and batch: 900, loss is 3.619115719795227 and perplexity is 37.304565543715476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39143392484482 and perplexity of 80.7561342244042
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 92.04860591888428 and batch: 50, loss is 3.7172634553909303 and perplexity is 41.15162662037062
At time: 92.82447242736816 and batch: 100, loss is 3.594644284248352 and perplexity is 36.40274865341948
At time: 93.5877857208252 and batch: 150, loss is 3.600652804374695 and perplexity is 36.62213373115991
At time: 94.36585545539856 and batch: 200, loss is 3.4762093687057494 and perplexity is 32.33691214819134
At time: 95.12827968597412 and batch: 250, loss is 3.619193334579468 and perplexity is 37.30746104188663
At time: 95.88989543914795 and batch: 300, loss is 3.5880003690719606 and perplexity is 36.16169354075208
At time: 96.6518497467041 and batch: 350, loss is 3.5626024532318117 and perplexity is 35.25482690051873
At time: 97.42008662223816 and batch: 400, loss is 3.493599324226379 and perplexity is 32.90416759031262
At time: 98.189777135849 and batch: 450, loss is 3.50394437789917 and perplexity is 33.246329761591525
At time: 98.95176196098328 and batch: 500, loss is 3.370392198562622 and perplexity is 29.08993385103005
At time: 99.71438455581665 and batch: 550, loss is 3.429316339492798 and perplexity is 30.85554082253311
At time: 100.48827862739563 and batch: 600, loss is 3.444141960144043 and perplexity is 31.316401194926588
At time: 101.24976778030396 and batch: 650, loss is 3.2674096488952635 and perplexity is 26.243271933636393
At time: 102.01371669769287 and batch: 700, loss is 3.2531542015075683 and perplexity is 25.871816275115474
At time: 102.77754068374634 and batch: 750, loss is 3.3506832122802734 and perplexity is 28.522213715168323
At time: 103.54046034812927 and batch: 800, loss is 3.280084114074707 and perplexity is 26.578008190421023
At time: 104.30262422561646 and batch: 850, loss is 3.330959930419922 and perplexity is 27.965173443561117
At time: 105.0642991065979 and batch: 900, loss is 3.272006320953369 and perplexity is 26.364181325964534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363872789356806 and perplexity of 78.56079544842748
finished 7 epochs...
Completing Train Step...
At time: 106.94881105422974 and batch: 50, loss is 3.604338321685791 and perplexity is 36.75735426474529
At time: 107.71263885498047 and batch: 100, loss is 3.470210299491882 and perplexity is 32.14350149604765
At time: 108.47623229026794 and batch: 150, loss is 3.4728372955322264 and perplexity is 32.22805335727777
At time: 109.23955488204956 and batch: 200, loss is 3.3538817024230956 and perplexity is 28.613587786221576
At time: 110.0026786327362 and batch: 250, loss is 3.498761229515076 and perplexity is 33.074454912419085
At time: 110.76488924026489 and batch: 300, loss is 3.472859206199646 and perplexity is 32.2287595031725
At time: 111.53422808647156 and batch: 350, loss is 3.4516484928131104 and perplexity is 31.55236330421593
At time: 112.29918336868286 and batch: 400, loss is 3.389451198577881 and perplexity is 29.64967602019749
At time: 113.0721435546875 and batch: 450, loss is 3.405986776351929 and perplexity is 30.144026465176857
At time: 113.83354043960571 and batch: 500, loss is 3.2775980472564696 and perplexity is 26.512015551208385
At time: 114.61935424804688 and batch: 550, loss is 3.339498257637024 and perplexity is 28.204971525517983
At time: 115.41650366783142 and batch: 600, loss is 3.3637015771865846 and perplexity is 28.895953765050262
At time: 116.18914866447449 and batch: 650, loss is 3.194698386192322 and perplexity is 24.40281235705768
At time: 116.9909017086029 and batch: 700, loss is 3.186238989830017 and perplexity is 24.197249987613752
At time: 117.78353548049927 and batch: 750, loss is 3.291435132026672 and perplexity is 26.881414365280982
At time: 118.54867053031921 and batch: 800, loss is 3.2288776588439942 and perplexity is 25.251300487335584
At time: 119.36847519874573 and batch: 850, loss is 3.2891536855697634 and perplexity is 26.82015576340163
At time: 120.14689993858337 and batch: 900, loss is 3.2382252740859987 and perplexity is 25.48844657707799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.383973004066781 and perplexity of 80.15586118394614
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 122.0340883731842 and batch: 50, loss is 3.551892123222351 and perplexity is 34.87925093149669
At time: 122.8312635421753 and batch: 100, loss is 3.4332044172286986 and perplexity is 30.975743090288656
At time: 123.59426879882812 and batch: 150, loss is 3.4417729139328004 and perplexity is 31.242299003741095
At time: 124.35679268836975 and batch: 200, loss is 3.318969988822937 and perplexity is 27.631874755994986
At time: 125.11948418617249 and batch: 250, loss is 3.46024178981781 and perplexity is 31.824670465744475
At time: 125.8832037448883 and batch: 300, loss is 3.428256187438965 and perplexity is 30.822846591048453
At time: 126.64648985862732 and batch: 350, loss is 3.399515061378479 and perplexity is 29.949572820716238
At time: 127.41057705879211 and batch: 400, loss is 3.3400597190856933 and perplexity is 28.220811976175437
At time: 128.1785717010498 and batch: 450, loss is 3.3494760942459108 and perplexity is 28.487804808596827
At time: 128.9442412853241 and batch: 500, loss is 3.218676347732544 and perplexity is 24.995013568964605
At time: 129.70818710327148 and batch: 550, loss is 3.269325647354126 and perplexity is 26.293602203179116
At time: 130.47243905067444 and batch: 600, loss is 3.291205701828003 and perplexity is 26.875247664483393
At time: 131.2489926815033 and batch: 650, loss is 3.1172743320465086 and perplexity is 22.584737178527806
At time: 132.0120666027069 and batch: 700, loss is 3.096469745635986 and perplexity is 22.119725027587666
At time: 132.78368616104126 and batch: 750, loss is 3.193816328048706 and perplexity is 24.381297147918673
At time: 133.56045699119568 and batch: 800, loss is 3.1238081741333006 and perplexity is 22.732785420044937
At time: 134.33537936210632 and batch: 850, loss is 3.1767920589447023 and perplexity is 23.969736583094285
At time: 135.09903407096863 and batch: 900, loss is 3.126556816101074 and perplexity is 22.795355660295662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375439787564212 and perplexity of 79.47478388927863
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.98979687690735 and batch: 50, loss is 3.5285744094848632 and perplexity is 34.07535546892385
At time: 137.76952362060547 and batch: 100, loss is 3.406235570907593 and perplexity is 30.15152706786305
At time: 138.5343132019043 and batch: 150, loss is 3.4133572959899903 and perplexity is 30.367024400016536
At time: 139.31322407722473 and batch: 200, loss is 3.291781759262085 and perplexity is 26.8907338107199
At time: 140.07908630371094 and batch: 250, loss is 3.4360191869735717 and perplexity is 31.06305549928026
At time: 140.8426914215088 and batch: 300, loss is 3.4043988370895386 and perplexity is 30.09619756676935
At time: 141.61552619934082 and batch: 350, loss is 3.3757292509078978 and perplexity is 29.245603389442064
At time: 142.37940168380737 and batch: 400, loss is 3.317238450050354 and perplexity is 27.58407049290734
At time: 143.14585661888123 and batch: 450, loss is 3.326025619506836 and perplexity is 27.82752446376686
At time: 143.91093158721924 and batch: 500, loss is 3.193354425430298 and perplexity is 24.37003796344944
At time: 144.68147468566895 and batch: 550, loss is 3.240085310935974 and perplexity is 25.53590014597198
At time: 145.44688844680786 and batch: 600, loss is 3.2639411449432374 and perplexity is 26.15240471896538
At time: 146.21014022827148 and batch: 650, loss is 3.0906146287918093 and perplexity is 21.99058987262222
At time: 146.97356605529785 and batch: 700, loss is 3.066660232543945 and perplexity is 21.47007773556612
At time: 147.73680877685547 and batch: 750, loss is 3.161901149749756 and perplexity is 23.61544978505895
At time: 148.50224375724792 and batch: 800, loss is 3.0910072469711305 and perplexity is 21.999225473116148
At time: 149.26677918434143 and batch: 850, loss is 3.142012348175049 and perplexity is 23.15040669450328
At time: 150.03122854232788 and batch: 900, loss is 3.0924694442749026 and perplexity is 22.031416210157094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3731275584599745 and perplexity of 79.29123226939407
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.92291808128357 and batch: 50, loss is 3.5196097564697264 and perplexity is 33.77124688139847
At time: 152.68484449386597 and batch: 100, loss is 3.3948713159561157 and perplexity is 29.810817051610435
At time: 153.4498987197876 and batch: 150, loss is 3.4020559930801393 and perplexity is 30.025769403908605
At time: 154.21462035179138 and batch: 200, loss is 3.280817217826843 and perplexity is 26.597499771752467
At time: 154.9765658378601 and batch: 250, loss is 3.426452217102051 and perplexity is 30.76729321348722
At time: 155.74533939361572 and batch: 300, loss is 3.3951469898223876 and perplexity is 29.819036247660502
At time: 156.5141201019287 and batch: 350, loss is 3.3676298236846924 and perplexity is 29.00968743492939
At time: 157.27544951438904 and batch: 400, loss is 3.3096103620529176 and perplexity is 27.374457266337576
At time: 158.0473234653473 and batch: 450, loss is 3.317663898468018 and perplexity is 27.59580858885139
At time: 158.81256651878357 and batch: 500, loss is 3.1840469455718994 and perplexity is 24.144266636856738
At time: 159.57344961166382 and batch: 550, loss is 3.2303012084960936 and perplexity is 25.28727256530416
At time: 160.3351514339447 and batch: 600, loss is 3.2552306699752807 and perplexity is 25.92559410046959
At time: 161.0967047214508 and batch: 650, loss is 3.0819980764389037 and perplexity is 21.80192080945295
At time: 161.8573203086853 and batch: 700, loss is 3.0584692478179933 and perplexity is 21.294934932009188
At time: 162.62057328224182 and batch: 750, loss is 3.1523604869842528 and perplexity is 23.391214121929252
At time: 163.3828251361847 and batch: 800, loss is 3.0811985540390014 and perplexity is 21.784496651835234
At time: 164.1451654434204 and batch: 850, loss is 3.131794319152832 and perplexity is 22.91505960638635
At time: 164.9072642326355 and batch: 900, loss is 3.082243571281433 and perplexity is 21.80727372559867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372852064158819 and perplexity of 79.26939099548362
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 166.82643270492554 and batch: 50, loss is 3.5164243364334107 and perplexity is 33.66384242989551
At time: 167.6025447845459 and batch: 100, loss is 3.391851239204407 and perplexity is 29.72092190952454
At time: 168.3650062084198 and batch: 150, loss is 3.39930202960968 and perplexity is 29.94319328978898
At time: 169.12632369995117 and batch: 200, loss is 3.2780031824111937 and perplexity is 26.52275867679275
At time: 169.8880934715271 and batch: 250, loss is 3.4238730382919313 and perplexity is 30.688041109393186
At time: 170.65513348579407 and batch: 300, loss is 3.3921338510513306 and perplexity is 29.729322581166418
At time: 171.426607131958 and batch: 350, loss is 3.364739236831665 and perplexity is 28.925953492239678
At time: 172.18899607658386 and batch: 400, loss is 3.3070497608184812 and perplexity is 27.304451863480097
At time: 172.9570333957672 and batch: 450, loss is 3.315419101715088 and perplexity is 27.533931084531023
At time: 173.7216715812683 and batch: 500, loss is 3.1813624000549314 and perplexity is 24.07953717756044
At time: 174.48340106010437 and batch: 550, loss is 3.227602915763855 and perplexity is 25.219132074361813
At time: 175.2505874633789 and batch: 600, loss is 3.2528922891616823 and perplexity is 25.86504101432348
At time: 176.01228713989258 and batch: 650, loss is 3.079521450996399 and perplexity is 21.74799242544966
At time: 176.78280639648438 and batch: 700, loss is 3.056374640464783 and perplexity is 21.250377086600796
At time: 177.57298183441162 and batch: 750, loss is 3.1499915981292723 and perplexity is 23.33586851516847
At time: 178.3350567817688 and batch: 800, loss is 3.0785059261322023 and perplexity is 21.725918008851806
At time: 179.09670853614807 and batch: 850, loss is 3.129072208404541 and perplexity is 22.85276709835194
At time: 179.86223244667053 and batch: 900, loss is 3.0793876314163207 and perplexity is 21.745082312955095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.372578660102739 and perplexity of 79.24772138487722
Annealing...
Model not improving. Stopping early with 78.56079544842748 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
377.7754728794098


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.060208797454834 and batch: 50, loss is 6.746551284790039 and perplexity is 851.1184301803385
At time: 1.90256667137146 and batch: 100, loss is 5.951685695648194 and perplexity is 384.40077593219354
At time: 2.7305431365966797 and batch: 150, loss is 5.756187238693237 and perplexity is 316.1406592297217
At time: 3.562685012817383 and batch: 200, loss is 5.571490125656128 and perplexity is 262.82545053348554
At time: 4.401081800460815 and batch: 250, loss is 5.6165498352050784 and perplexity is 274.93915955419106
At time: 5.237586498260498 and batch: 300, loss is 5.528953199386597 and perplexity is 251.8801047231756
At time: 6.064455509185791 and batch: 350, loss is 5.49594485282898 and perplexity is 243.7016796343967
At time: 6.891274452209473 and batch: 400, loss is 5.345629205703736 and perplexity is 209.68978110578877
At time: 7.717312812805176 and batch: 450, loss is 5.344764852523804 and perplexity is 209.50861338441817
At time: 8.544264316558838 and batch: 500, loss is 5.292569599151611 and perplexity is 198.8537438832036
At time: 9.37388563156128 and batch: 550, loss is 5.346136684417725 and perplexity is 209.79622121200077
At time: 10.209812879562378 and batch: 600, loss is 5.271960277557373 and perplexity is 194.7974454775227
At time: 11.045580625534058 and batch: 650, loss is 5.161619691848755 and perplexity is 174.44677692925077
At time: 11.882134437561035 and batch: 700, loss is 5.254748992919922 and perplexity is 191.47341863829865
At time: 12.710606813430786 and batch: 750, loss is 5.242185449600219 and perplexity is 189.08288229607209
At time: 13.539741516113281 and batch: 800, loss is 5.199974851608276 and perplexity is 181.26768322712547
At time: 14.367284297943115 and batch: 850, loss is 5.245106773376465 and perplexity is 189.63606223124404
At time: 15.1961088180542 and batch: 900, loss is 5.158003664016723 and perplexity is 173.8171116583457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0556661527450775 and perplexity of 156.9090209119728
finished 1 epochs...
Completing Train Step...
At time: 17.13040518760681 and batch: 50, loss is 5.008084535598755 and perplexity is 149.61787378840336
At time: 17.895230531692505 and batch: 100, loss is 4.849443998336792 and perplexity is 127.66938571785145
At time: 18.661840200424194 and batch: 150, loss is 4.8182166481018065 and perplexity is 123.74421440855853
At time: 19.426533222198486 and batch: 200, loss is 4.688832273483277 and perplexity is 108.72614324933936
At time: 20.201225519180298 and batch: 250, loss is 4.791270341873169 and perplexity is 120.45428963962276
At time: 20.96388030052185 and batch: 300, loss is 4.742314100265503 and perplexity is 114.69932053414394
At time: 21.72524642944336 and batch: 350, loss is 4.715424861907959 and perplexity is 111.65623958071663
At time: 22.487812995910645 and batch: 400, loss is 4.593364162445068 and perplexity is 98.82633941022536
At time: 23.249483346939087 and batch: 450, loss is 4.606587734222412 and perplexity is 100.14185534307343
At time: 24.013400316238403 and batch: 500, loss is 4.499086952209472 and perplexity is 89.934978867934
At time: 24.787989377975464 and batch: 550, loss is 4.57590313911438 and perplexity is 97.11570853714268
At time: 25.552372455596924 and batch: 600, loss is 4.539616661071777 and perplexity is 93.65489167156454
At time: 26.31540322303772 and batch: 650, loss is 4.389621267318725 and perplexity is 80.60988360111547
At time: 27.077319145202637 and batch: 700, loss is 4.4265828132629395 and perplexity is 83.64509706838044
At time: 27.84071397781372 and batch: 750, loss is 4.490617074966431 and perplexity is 89.17645746327273
At time: 28.602973699569702 and batch: 800, loss is 4.424829874038696 and perplexity is 83.49860073391189
At time: 29.366254568099976 and batch: 850, loss is 4.493355770111084 and perplexity is 89.42101933168519
At time: 30.132134675979614 and batch: 900, loss is 4.431547508239746 and perplexity is 84.06140202008481
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.549864468509203 and perplexity of 94.6195835126494
finished 2 epochs...
Completing Train Step...
At time: 32.003992795944214 and batch: 50, loss is 4.453449716567993 and perplexity is 85.92284277960272
At time: 32.78057336807251 and batch: 100, loss is 4.307961387634277 and perplexity is 74.28888821913729
At time: 33.544699907302856 and batch: 150, loss is 4.303090496063232 and perplexity is 73.92791494422103
At time: 34.30688738822937 and batch: 200, loss is 4.19500412940979 and perplexity is 66.35400557960939
At time: 35.069831132888794 and batch: 250, loss is 4.345061273574829 and perplexity is 77.09676132875143
At time: 35.83196258544922 and batch: 300, loss is 4.309009208679199 and perplexity is 74.36677047582516
At time: 36.60691475868225 and batch: 350, loss is 4.29642243385315 and perplexity is 73.43659889176222
At time: 37.37200665473938 and batch: 400, loss is 4.21615957736969 and perplexity is 67.77270802278599
At time: 38.15362763404846 and batch: 450, loss is 4.235233449935913 and perplexity is 69.07780306003086
At time: 38.94066905975342 and batch: 500, loss is 4.112208194732666 and perplexity is 61.081448491641865
At time: 39.710047483444214 and batch: 550, loss is 4.197264170646667 and perplexity is 66.50413795722979
At time: 40.47666621208191 and batch: 600, loss is 4.196260437965393 and perplexity is 66.43741907008742
At time: 41.241490602493286 and batch: 650, loss is 4.042195944786072 and perplexity is 56.95126743025278
At time: 42.00509715080261 and batch: 700, loss is 4.056532144546509 and perplexity is 57.773612745660046
At time: 42.76784348487854 and batch: 750, loss is 4.1589611148834225 and perplexity is 64.0049942123705
At time: 43.52973008155823 and batch: 800, loss is 4.100594434738159 and perplexity is 60.376166623233544
At time: 44.29386782646179 and batch: 850, loss is 4.180404124259948 and perplexity is 65.3922744794435
At time: 45.05776119232178 and batch: 900, loss is 4.131958112716675 and perplexity is 62.29979359566149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.418700753825984 and perplexity of 82.98839293210524
finished 3 epochs...
Completing Train Step...
At time: 46.94635248184204 and batch: 50, loss is 4.18203224658966 and perplexity is 65.49882781911248
At time: 47.73206305503845 and batch: 100, loss is 4.043051323890686 and perplexity is 57.00000319519983
At time: 48.49496936798096 and batch: 150, loss is 4.037156715393066 and perplexity is 56.664998821830075
At time: 49.25798439979553 and batch: 200, loss is 3.936484341621399 and perplexity is 51.238148455191784
At time: 50.024001359939575 and batch: 250, loss is 4.0924446439743045 and perplexity is 59.886113127264096
At time: 50.8006751537323 and batch: 300, loss is 4.06181800365448 and perplexity is 58.0798044529807
At time: 51.569560527801514 and batch: 350, loss is 4.054178581237793 and perplexity is 57.637798776569554
At time: 52.334317445755005 and batch: 400, loss is 3.988947901725769 and perplexity is 53.99804821399443
At time: 53.09824228286743 and batch: 450, loss is 4.005673050880432 and perplexity is 54.908768359980876
At time: 53.86359763145447 and batch: 500, loss is 3.8821698999404908 and perplexity is 48.52940485712822
At time: 54.63087201118469 and batch: 550, loss is 3.9657566213607787 and perplexity is 52.76017376596159
At time: 55.40191435813904 and batch: 600, loss is 3.9811924266815186 and perplexity is 53.580887429584735
At time: 56.16643023490906 and batch: 650, loss is 3.8182482147216796 and perplexity is 45.52438947323593
At time: 56.927563190460205 and batch: 700, loss is 3.8297040557861326 and perplexity is 46.04890830949617
At time: 57.69041562080383 and batch: 750, loss is 3.9425244998931883 and perplexity is 51.54857153991962
At time: 58.46480584144592 and batch: 800, loss is 3.8909188175201415 and perplexity is 48.95584735543239
At time: 59.22873878479004 and batch: 850, loss is 3.971648678779602 and perplexity is 53.0719573606405
At time: 60.00381350517273 and batch: 900, loss is 3.9276282739639283 and perplexity is 50.786383329742456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.378677995237585 and perplexity of 79.73255688029697
finished 4 epochs...
Completing Train Step...
At time: 61.890066385269165 and batch: 50, loss is 3.988748197555542 and perplexity is 53.98726565527876
At time: 62.65481901168823 and batch: 100, loss is 3.853467655181885 and perplexity is 47.15630183482748
At time: 63.418747663497925 and batch: 150, loss is 3.8517215919494627 and perplexity is 47.07403579177384
At time: 64.18189692497253 and batch: 200, loss is 3.7533854341506956 and perplexity is 42.665278268955475
At time: 64.9457802772522 and batch: 250, loss is 3.9084564304351805 and perplexity is 49.82198888404756
At time: 65.72275042533875 and batch: 300, loss is 3.881756844520569 and perplexity is 48.50936366277331
At time: 66.49409580230713 and batch: 350, loss is 3.8741353702545167 and perplexity is 48.14105610207059
At time: 67.26075482368469 and batch: 400, loss is 3.814775676727295 and perplexity is 45.36657846206564
At time: 68.03141355514526 and batch: 450, loss is 3.8295480585098267 and perplexity is 46.04172536549744
At time: 68.79469418525696 and batch: 500, loss is 3.712942624092102 and perplexity is 40.974200973254746
At time: 69.55849242210388 and batch: 550, loss is 3.792411389350891 and perplexity is 44.36324846283256
At time: 70.32269096374512 and batch: 600, loss is 3.8189054107666016 and perplexity is 45.55431775524232
At time: 71.08569169044495 and batch: 650, loss is 3.6510877323150637 and perplexity is 38.516538955733274
At time: 71.8496630191803 and batch: 700, loss is 3.6629269790649412 and perplexity is 38.975255837459585
At time: 72.61281967163086 and batch: 750, loss is 3.777069926261902 and perplexity is 43.68784540644313
At time: 73.37720727920532 and batch: 800, loss is 3.7314362716674805 and perplexity is 41.73901369622231
At time: 74.15021467208862 and batch: 850, loss is 3.8103244304656982 and perplexity is 45.16508942064117
At time: 74.91811442375183 and batch: 900, loss is 3.7673671436309815 and perplexity is 43.26600157783767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374973244863014 and perplexity of 79.43771415690472
finished 5 epochs...
Completing Train Step...
At time: 76.78872466087341 and batch: 50, loss is 3.835586724281311 and perplexity is 46.32059711573629
At time: 77.56285858154297 and batch: 100, loss is 3.707223401069641 and perplexity is 40.74052922707717
At time: 78.32447719573975 and batch: 150, loss is 3.7039103937149047 and perplexity is 40.60577889178976
At time: 79.08866667747498 and batch: 200, loss is 3.605451645851135 and perplexity is 36.79829990415794
At time: 79.84997272491455 and batch: 250, loss is 3.7601152658462524 and perplexity is 42.95337675075066
At time: 80.61769342422485 and batch: 300, loss is 3.735611753463745 and perplexity is 41.9136585475645
At time: 81.37918281555176 and batch: 350, loss is 3.73041277885437 and perplexity is 41.696315969813256
At time: 82.13930988311768 and batch: 400, loss is 3.6716784715652464 and perplexity is 39.317844390666345
At time: 82.90682411193848 and batch: 450, loss is 3.688873977661133 and perplexity is 39.999780942487696
At time: 83.67126560211182 and batch: 500, loss is 3.5778201961517335 and perplexity is 35.79542873014934
At time: 84.43012642860413 and batch: 550, loss is 3.649643397331238 and perplexity is 38.460948326472916
At time: 85.1894793510437 and batch: 600, loss is 3.682467918395996 and perplexity is 39.74435897280668
At time: 85.94997382164001 and batch: 650, loss is 3.5151429176330566 and perplexity is 33.62073257610124
At time: 86.7095456123352 and batch: 700, loss is 3.5253634881973266 and perplexity is 33.96611765553213
At time: 87.46929454803467 and batch: 750, loss is 3.6425219583511352 and perplexity is 38.188023990295264
At time: 88.22964453697205 and batch: 800, loss is 3.5981473779678343 and perplexity is 36.53049451582546
At time: 88.98875594139099 and batch: 850, loss is 3.675789589881897 and perplexity is 39.4798174179222
At time: 89.74954867362976 and batch: 900, loss is 3.636126537322998 and perplexity is 37.944574808437196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.388042711231806 and perplexity of 80.48273675930409
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.62586569786072 and batch: 50, loss is 3.724631767272949 and perplexity is 41.45596449101086
At time: 92.41100311279297 and batch: 100, loss is 3.606752161979675 and perplexity is 36.84618781943967
At time: 93.17483949661255 and batch: 150, loss is 3.607371115684509 and perplexity is 36.869000963311166
At time: 93.93687534332275 and batch: 200, loss is 3.4898542881011965 and perplexity is 32.78117075157556
At time: 94.69863200187683 and batch: 250, loss is 3.635501546859741 and perplexity is 37.92086722032953
At time: 95.46488332748413 and batch: 300, loss is 3.5960410594940186 and perplexity is 36.453630638689056
At time: 96.22385668754578 and batch: 350, loss is 3.5748826026916505 and perplexity is 35.69043060921385
At time: 97.00511980056763 and batch: 400, loss is 3.5062905025482176 and perplexity is 33.32442136583439
At time: 97.76467943191528 and batch: 450, loss is 3.5075580310821532 and perplexity is 33.366687802090595
At time: 98.52317976951599 and batch: 500, loss is 3.3854991722106935 and perplexity is 29.5327309559847
At time: 99.28039002418518 and batch: 550, loss is 3.437874140739441 and perplexity is 31.120729505828557
At time: 100.04071545600891 and batch: 600, loss is 3.460420136451721 and perplexity is 31.830346794759404
At time: 100.80494451522827 and batch: 650, loss is 3.270391039848328 and perplexity is 26.32163013733613
At time: 101.56616568565369 and batch: 700, loss is 3.2614901447296143 and perplexity is 26.088383659277937
At time: 102.32860589027405 and batch: 750, loss is 3.3563348293304442 and perplexity is 28.68386671465703
At time: 103.09083557128906 and batch: 800, loss is 3.2884371089935303 and perplexity is 26.800943952199674
At time: 103.85514688491821 and batch: 850, loss is 3.341058373451233 and perplexity is 28.249008890396833
At time: 104.61954140663147 and batch: 900, loss is 3.2915071487426757 and perplexity is 26.883350346175757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361828529671447 and perplexity of 78.40036082233297
finished 7 epochs...
Completing Train Step...
At time: 106.52040386199951 and batch: 50, loss is 3.6076089668273927 and perplexity is 36.87777134030801
At time: 107.28527593612671 and batch: 100, loss is 3.4788032579421997 and perplexity is 32.420899396271295
At time: 108.05031204223633 and batch: 150, loss is 3.4830418395996094 and perplexity is 32.55860966771406
At time: 108.81667804718018 and batch: 200, loss is 3.3673380470275878 and perplexity is 29.0012243200357
At time: 109.58346390724182 and batch: 250, loss is 3.514455499649048 and perplexity is 33.597629021702055
At time: 110.34790754318237 and batch: 300, loss is 3.4816974353790284 and perplexity is 32.514867145864464
At time: 111.11456966400146 and batch: 350, loss is 3.465432643890381 and perplexity is 31.990297186207663
At time: 111.88170647621155 and batch: 400, loss is 3.401919140815735 and perplexity is 30.021660590531816
At time: 112.64810919761658 and batch: 450, loss is 3.4079550218582155 and perplexity is 30.203415736969202
At time: 113.41322326660156 and batch: 500, loss is 3.2923921298980714 and perplexity is 26.907152135139924
At time: 114.17956185340881 and batch: 550, loss is 3.349094657897949 and perplexity is 28.476940596503606
At time: 114.94437003135681 and batch: 600, loss is 3.3796074104309084 and perplexity is 29.359242718026117
At time: 115.7314293384552 and batch: 650, loss is 3.1963531732559205 and perplexity is 24.44322724505607
At time: 116.49855923652649 and batch: 700, loss is 3.1937901401519775 and perplexity is 24.3806586613872
At time: 117.26487565040588 and batch: 750, loss is 3.2976407337188722 and perplexity is 27.048748382890995
At time: 118.03606462478638 and batch: 800, loss is 3.2361897468566894 and perplexity is 25.436616918274236
At time: 118.81262159347534 and batch: 850, loss is 3.2974284934997558 and perplexity is 27.043008159782037
At time: 119.57899594306946 and batch: 900, loss is 3.2588817501068115 and perplexity is 26.020423531571115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382321292406892 and perplexity of 80.02357609190912
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.46246385574341 and batch: 50, loss is 3.5534431076049806 and perplexity is 34.93339007860221
At time: 122.24085140228271 and batch: 100, loss is 3.4346202754974366 and perplexity is 31.019631414765158
At time: 123.01030778884888 and batch: 150, loss is 3.446820020675659 and perplexity is 31.40038081401878
At time: 123.78356695175171 and batch: 200, loss is 3.3306327152252195 and perplexity is 27.95602431083715
At time: 124.54771041870117 and batch: 250, loss is 3.4741991186141967 and perplexity is 32.27197216226035
At time: 125.31301259994507 and batch: 300, loss is 3.4370474147796632 and perplexity is 31.09501182306164
At time: 126.07669830322266 and batch: 350, loss is 3.415454888343811 and perplexity is 30.430788890780686
At time: 126.85057854652405 and batch: 400, loss is 3.350288987159729 and perplexity is 28.5109717581048
At time: 127.61514186859131 and batch: 450, loss is 3.347213177680969 and perplexity is 28.423412168261304
At time: 128.38003826141357 and batch: 500, loss is 3.229580125808716 and perplexity is 25.269044923454604
At time: 129.14422345161438 and batch: 550, loss is 3.2771471834182737 and perplexity is 26.500064936371135
At time: 129.90645337104797 and batch: 600, loss is 3.303183436393738 and perplexity is 27.199087811392662
At time: 130.67177724838257 and batch: 650, loss is 3.1161759519577026 and perplexity is 22.55994417146525
At time: 131.43757247924805 and batch: 700, loss is 3.1022282457351684 and perplexity is 22.247468919959157
At time: 132.20456504821777 and batch: 750, loss is 3.1999887084960936 and perplexity is 24.532253189512716
At time: 132.97790431976318 and batch: 800, loss is 3.130053129196167 and perplexity is 22.875194850870933
At time: 133.74351048469543 and batch: 850, loss is 3.187013339996338 and perplexity is 24.21599438859456
At time: 134.5075650215149 and batch: 900, loss is 3.1470854330062865 and perplexity is 23.268149077570992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3776918176102315 and perplexity of 79.65396517558985
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.40539717674255 and batch: 50, loss is 3.5294684410095214 and perplexity is 34.10583353307201
At time: 137.19083404541016 and batch: 100, loss is 3.4068681955337525 and perplexity is 30.170607701205405
At time: 137.9555003643036 and batch: 150, loss is 3.419530234336853 and perplexity is 30.55505793202858
At time: 138.71919775009155 and batch: 200, loss is 3.3055777740478516 and perplexity is 27.264289637941868
At time: 139.48578310012817 and batch: 250, loss is 3.448232264518738 and perplexity is 31.444757136212143
At time: 140.25190234184265 and batch: 300, loss is 3.4134434413909913 and perplexity is 30.369640492191223
At time: 141.0155508518219 and batch: 350, loss is 3.3911201667785646 and perplexity is 29.699201703535113
At time: 141.78112721443176 and batch: 400, loss is 3.3278432178497312 and perplexity is 27.878149720372935
At time: 142.54474472999573 and batch: 450, loss is 3.3224777936935426 and perplexity is 27.728972180694843
At time: 143.31247234344482 and batch: 500, loss is 3.2044714450836183 and perplexity is 24.64247167410721
At time: 144.08470106124878 and batch: 550, loss is 3.2488411235809327 and perplexity is 25.760469411878407
At time: 144.85035943984985 and batch: 600, loss is 3.2760884666442873 and perplexity is 26.4720237195838
At time: 145.61606001853943 and batch: 650, loss is 3.0877339029312134 and perplexity is 21.927332169482817
At time: 146.38126754760742 and batch: 700, loss is 3.0737079858779905 and perplexity is 21.62192802064205
At time: 147.14706897735596 and batch: 750, loss is 3.169217710494995 and perplexity is 23.7888672941917
At time: 147.91006922721863 and batch: 800, loss is 3.0980873823165895 and perplexity is 22.155535662646614
At time: 148.67497324943542 and batch: 850, loss is 3.152003331184387 and perplexity is 23.38286130585685
At time: 149.4392020702362 and batch: 900, loss is 3.1131041193008424 and perplexity is 22.49075012889269
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376799282962328 and perplexity of 79.58290296928585
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.3438732624054 and batch: 50, loss is 3.5210264825820925 and perplexity is 33.819125396065346
At time: 152.10943984985352 and batch: 100, loss is 3.395971488952637 and perplexity is 29.84363215537047
At time: 152.8759627342224 and batch: 150, loss is 3.4090647411346438 and perplexity is 30.236951653911674
At time: 153.6419894695282 and batch: 200, loss is 3.2947334909439085 and perplexity is 26.970225302718585
At time: 154.4286060333252 and batch: 250, loss is 3.4382276725769043 and perplexity is 31.131733619551614
At time: 155.19222402572632 and batch: 300, loss is 3.4034258031845095 and perplexity is 30.066927188968855
At time: 155.95527577400208 and batch: 350, loss is 3.38159405708313 and perplexity is 29.41762713468413
At time: 156.72026777267456 and batch: 400, loss is 3.3189404153823854 and perplexity is 27.6310575984727
At time: 157.48238158226013 and batch: 450, loss is 3.313255271911621 and perplexity is 27.47441675650337
At time: 158.24615240097046 and batch: 500, loss is 3.195200901031494 and perplexity is 24.415078214014088
At time: 159.00974822044373 and batch: 550, loss is 3.2391770458221436 and perplexity is 25.51271730839416
At time: 159.77184176445007 and batch: 600, loss is 3.2672007083892822 and perplexity is 26.237789223919993
At time: 160.53453922271729 and batch: 650, loss is 3.079056420326233 and perplexity is 21.737881293132816
At time: 161.29793095588684 and batch: 700, loss is 3.065258288383484 and perplexity is 21.439998974758147
At time: 162.0609884262085 and batch: 750, loss is 3.160104818344116 and perplexity is 23.573066689391478
At time: 162.82384419441223 and batch: 800, loss is 3.0895786046981812 and perplexity is 21.9678188893615
At time: 163.58684253692627 and batch: 850, loss is 3.141821222305298 and perplexity is 23.1459824756935
At time: 164.35618567466736 and batch: 900, loss is 3.101937479972839 and perplexity is 22.241001058060714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377406708181721 and perplexity of 79.6312583162237
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 166.34575200080872 and batch: 50, loss is 3.5188517427444457 and perplexity is 33.745657512514214
At time: 167.14890837669373 and batch: 100, loss is 3.3934858512878416 and perplexity is 29.769543815760095
At time: 167.9257926940918 and batch: 150, loss is 3.406454758644104 and perplexity is 30.158136637175218
At time: 168.70907974243164 and batch: 200, loss is 3.292169814109802 and perplexity is 26.90117091528736
At time: 169.4733681678772 and batch: 250, loss is 3.4355444145202636 and perplexity is 31.048311116603948
At time: 170.24860668182373 and batch: 300, loss is 3.401004810333252 and perplexity is 29.994223416352945
At time: 171.02022171020508 and batch: 350, loss is 3.378586869239807 and perplexity is 29.329295685182277
At time: 171.78557515144348 and batch: 400, loss is 3.315898241996765 and perplexity is 27.54712686108814
At time: 172.5610740184784 and batch: 450, loss is 3.3108230447769165 and perplexity is 27.40767393431231
At time: 173.33858036994934 and batch: 500, loss is 3.19242356300354 and perplexity is 24.34736336587337
At time: 174.11352467536926 and batch: 550, loss is 3.236488060951233 and perplexity is 25.444206151549807
At time: 174.88625645637512 and batch: 600, loss is 3.2648427581787107 and perplexity is 26.17599470612374
At time: 175.6587564945221 and batch: 650, loss is 3.0766341161727904 and perplexity is 21.685289255660386
At time: 176.42626690864563 and batch: 700, loss is 3.062769641876221 and perplexity is 21.38670873398349
At time: 177.19722247123718 and batch: 750, loss is 3.157643265724182 and perplexity is 23.515111704208735
At time: 177.97261905670166 and batch: 800, loss is 3.0872695589065553 and perplexity is 21.91715270738233
At time: 178.76049637794495 and batch: 850, loss is 3.139259309768677 and perplexity is 23.086760386314722
At time: 179.55378031730652 and batch: 900, loss is 3.098920297622681 and perplexity is 22.173997034727076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377241160771618 and perplexity of 79.61807665877093
Annealing...
Model not improving. Stopping early with 78.40036082233297 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
564.3882801532745


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.40036082233297, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.5304780587349254, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10882121312718696, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0842969417572021 and batch: 50, loss is 6.772669267654419 and perplexity is 873.6407654541819
At time: 1.9214274883270264 and batch: 100, loss is 5.895729866027832 and perplexity is 363.4820322953067
At time: 2.7629036903381348 and batch: 150, loss is 5.593228960037232 and perplexity is 268.60152456398487
At time: 3.6023848056793213 and batch: 200, loss is 5.366760520935059 and perplexity is 214.16795013574995
At time: 4.438459873199463 and batch: 250, loss is 5.388707046508789 and perplexity is 218.92014891667466
At time: 5.280116319656372 and batch: 300, loss is 5.2977432346344 and perplexity is 199.88520657371058
At time: 6.1091227531433105 and batch: 350, loss is 5.240854845046997 and perplexity is 188.83145506415855
At time: 6.952181816101074 and batch: 400, loss is 5.0842122459411625 and perplexity is 161.45270410621413
At time: 7.778803110122681 and batch: 450, loss is 5.077506647109986 and perplexity is 160.373688807295
At time: 8.608482599258423 and batch: 500, loss is 5.005055522918701 and perplexity is 149.1653650249043
At time: 9.4575777053833 and batch: 550, loss is 5.057335395812988 and perplexity is 157.17115893259174
At time: 10.291871309280396 and batch: 600, loss is 4.9816978168487545 and perplexity is 145.72158029947573
At time: 11.122804164886475 and batch: 650, loss is 4.861910934448242 and perplexity is 129.27099462690532
At time: 11.953704833984375 and batch: 700, loss is 4.939227104187012 and perplexity is 139.66226345571724
At time: 12.785081624984741 and batch: 750, loss is 4.939464721679688 and perplexity is 139.6954535957038
At time: 13.622121810913086 and batch: 800, loss is 4.883273324966431 and perplexity is 132.06223978300622
At time: 14.462050914764404 and batch: 850, loss is 4.928891744613647 and perplexity is 138.22623744959853
At time: 15.293075561523438 and batch: 900, loss is 4.850216865539551 and perplexity is 127.76809533866997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.838027745077055 and perplexity of 126.22016773651517
finished 1 epochs...
Completing Train Step...
At time: 17.237240076065063 and batch: 50, loss is 4.8198674774169925 and perplexity is 123.94866369429495
At time: 18.001256942749023 and batch: 100, loss is 4.673232498168946 and perplexity is 107.0432007353511
At time: 18.764384269714355 and batch: 150, loss is 4.648872232437133 and perplexity is 104.4671045065353
At time: 19.527198791503906 and batch: 200, loss is 4.532207050323486 and perplexity is 92.96350997742361
At time: 20.290411710739136 and batch: 250, loss is 4.649926385879517 and perplexity is 104.57728692875325
At time: 21.054370403289795 and batch: 300, loss is 4.609415092468262 and perplexity is 100.42539288575485
At time: 21.81819725036621 and batch: 350, loss is 4.591585149765015 and perplexity is 98.65068239364908
At time: 22.58291721343994 and batch: 400, loss is 4.4804401779174805 and perplexity is 88.27352017939883
At time: 23.347151517868042 and batch: 450, loss is 4.495128374099732 and perplexity is 89.57966795627276
At time: 24.113797903060913 and batch: 500, loss is 4.388309984207154 and perplexity is 80.5042504947138
At time: 24.880725622177124 and batch: 550, loss is 4.4668223094940185 and perplexity is 87.07957096901455
At time: 25.644664525985718 and batch: 600, loss is 4.444841041564941 and perplexity is 85.18633567442336
At time: 26.410222053527832 and batch: 650, loss is 4.297425627708435 and perplexity is 73.51030700210354
At time: 27.183278560638428 and batch: 700, loss is 4.329476327896118 and perplexity is 75.90452707623909
At time: 27.956252813339233 and batch: 750, loss is 4.4050757026672365 and perplexity is 81.86534003318748
At time: 28.720421075820923 and batch: 800, loss is 4.34028564453125 and perplexity is 76.72945355710263
At time: 29.485247373580933 and batch: 850, loss is 4.410643739700317 and perplexity is 82.32244067390732
At time: 30.245093822479248 and batch: 900, loss is 4.351665735244751 and perplexity is 77.60762908010672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5057611334813785 and perplexity of 90.53722874642888
finished 2 epochs...
Completing Train Step...
At time: 32.13249397277832 and batch: 50, loss is 4.395473670959473 and perplexity is 81.08302834393264
At time: 32.919649839401245 and batch: 100, loss is 4.250650763511658 and perplexity is 70.15104923881431
At time: 33.6842155456543 and batch: 150, loss is 4.248342452049255 and perplexity is 69.98930551701855
At time: 34.44948625564575 and batch: 200, loss is 4.140552840232849 and perplexity is 62.83755097469017
At time: 35.21567487716675 and batch: 250, loss is 4.285023159980774 and perplexity is 72.60422821281217
At time: 35.98165965080261 and batch: 300, loss is 4.253684477806091 and perplexity is 70.36419062121973
At time: 36.74842429161072 and batch: 350, loss is 4.2399622297286985 and perplexity is 69.40523033486855
At time: 37.5162889957428 and batch: 400, loss is 4.155164012908935 and perplexity is 63.76242155048702
At time: 38.28169798851013 and batch: 450, loss is 4.176557569503784 and perplexity is 65.14122266684659
At time: 39.04738736152649 and batch: 500, loss is 4.059947748184204 and perplexity is 57.97128189507674
At time: 39.81459426879883 and batch: 550, loss is 4.143951125144959 and perplexity is 63.051454122148556
At time: 40.58189153671265 and batch: 600, loss is 4.15146192073822 and perplexity is 63.526803597673144
At time: 41.34776425361633 and batch: 650, loss is 3.993651452064514 and perplexity is 54.252628999383695
At time: 42.11398792266846 and batch: 700, loss is 4.009586381912231 and perplexity is 55.124065536925784
At time: 42.879777908325195 and batch: 750, loss is 4.112887516021728 and perplexity is 61.122956517015375
At time: 43.64640021324158 and batch: 800, loss is 4.055556921958924 and perplexity is 57.71729807767541
At time: 44.41230273246765 and batch: 850, loss is 4.134518823623657 and perplexity is 62.45952978844448
At time: 45.17539644241333 and batch: 900, loss is 4.087909059524536 and perplexity is 59.61510964796971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404563172222817 and perplexity of 81.82339230473991
finished 3 epochs...
Completing Train Step...
At time: 47.06540060043335 and batch: 50, loss is 4.1487677526474 and perplexity is 63.355882059572224
At time: 47.84371829032898 and batch: 100, loss is 4.009425473213196 and perplexity is 55.11519630884181
At time: 48.60895252227783 and batch: 150, loss is 4.011869816780091 and perplexity is 55.25008157017833
At time: 49.373581409454346 and batch: 200, loss is 3.9059194421768186 and perplexity is 49.695751282605485
At time: 50.13782715797424 and batch: 250, loss is 4.052020015716553 and perplexity is 57.51351799377283
At time: 50.91469430923462 and batch: 300, loss is 4.030657286643982 and perplexity is 56.29790294841265
At time: 51.67903232574463 and batch: 350, loss is 4.016673526763916 and perplexity is 55.516125425792914
At time: 52.44368052482605 and batch: 400, loss is 3.9426928329467774 and perplexity is 51.55724959875667
At time: 53.20762872695923 and batch: 450, loss is 3.9660186624526976 and perplexity is 52.77400091106603
At time: 53.97046756744385 and batch: 500, loss is 3.8510952472686766 and perplexity is 47.04456045167738
At time: 54.731196641922 and batch: 550, loss is 3.9294898891448975 and perplexity is 50.88101608949668
At time: 55.49274230003357 and batch: 600, loss is 3.950710639953613 and perplexity is 51.97228729853436
At time: 56.25177335739136 and batch: 650, loss is 3.7896972560882567 and perplexity is 44.2430039481748
At time: 57.01173162460327 and batch: 700, loss is 3.80297767162323 and perplexity is 44.834488310893605
At time: 57.78102350234985 and batch: 750, loss is 3.9126856756210326 and perplexity is 50.03314449036434
At time: 58.54644012451172 and batch: 800, loss is 3.8613143730163575 and perplexity is 47.52777956388472
At time: 59.30889272689819 and batch: 850, loss is 3.940024719238281 and perplexity is 51.41987234489411
At time: 60.071433544158936 and batch: 900, loss is 3.8997107553482055 and perplexity is 49.38816177770704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3707823034835185 and perplexity of 79.105492001603
finished 4 epochs...
Completing Train Step...
At time: 61.963725090026855 and batch: 50, loss is 3.966201729774475 and perplexity is 52.78366299045081
At time: 62.729798793792725 and batch: 100, loss is 3.83286226272583 and perplexity is 46.194570185385786
At time: 63.49407696723938 and batch: 150, loss is 3.836359977722168 and perplexity is 46.3564285284348
At time: 64.26027011871338 and batch: 200, loss is 3.7319231700897215 and perplexity is 41.7593413044755
At time: 65.02577805519104 and batch: 250, loss is 3.8744391298294065 and perplexity is 48.15568163001652
At time: 65.79103779792786 and batch: 300, loss is 3.8597806787490843 and perplexity is 47.454942350117385
At time: 66.55592656135559 and batch: 350, loss is 3.8458508014678956 and perplexity is 46.79848363643931
At time: 67.32168936729431 and batch: 400, loss is 3.7808211135864256 and perplexity is 43.852034457780775
At time: 68.08741450309753 and batch: 450, loss is 3.8025800514221193 and perplexity is 44.81666475637221
At time: 68.8531744480133 and batch: 500, loss is 3.6903087091445923 and perplexity is 40.05721107609631
At time: 69.63099646568298 and batch: 550, loss is 3.765359945297241 and perplexity is 43.17924522928947
At time: 70.39622950553894 and batch: 600, loss is 3.791656951904297 and perplexity is 44.329791789010315
At time: 71.16210126876831 and batch: 650, loss is 3.6330310678482056 and perplexity is 37.82730013910638
At time: 71.9257972240448 and batch: 700, loss is 3.6441836833953856 and perplexity is 38.25153474023774
At time: 72.69095087051392 and batch: 750, loss is 3.754713439941406 and perplexity is 42.721975644447795
At time: 73.45537185668945 and batch: 800, loss is 3.7098389625549317 and perplexity is 40.8472280640695
At time: 74.22022342681885 and batch: 850, loss is 3.7862567138671874 and perplexity is 44.09104558458548
At time: 74.98588752746582 and batch: 900, loss is 3.745376830101013 and perplexity is 42.32495353063829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3736287992294525 and perplexity of 79.33098622999022
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.88127493858337 and batch: 50, loss is 3.8356096029281614 and perplexity is 46.32165688044254
At time: 77.66212248802185 and batch: 100, loss is 3.7043740797042846 and perplexity is 40.624611588441105
At time: 78.42776465415955 and batch: 150, loss is 3.707485728263855 and perplexity is 40.75121797771379
At time: 79.19317579269409 and batch: 200, loss is 3.587781391143799 and perplexity is 36.15377579495913
At time: 79.95822191238403 and batch: 250, loss is 3.7258273267745974 and perplexity is 41.50555720286985
At time: 80.72335815429688 and batch: 300, loss is 3.6954447507858275 and perplexity is 40.26347581894913
At time: 81.48875427246094 and batch: 350, loss is 3.6650114393234254 and perplexity is 39.05658294142809
At time: 82.25326037406921 and batch: 400, loss is 3.59851327419281 and perplexity is 36.54386333151573
At time: 83.01737856864929 and batch: 450, loss is 3.603679075241089 and perplexity is 36.7331300953557
At time: 83.78333306312561 and batch: 500, loss is 3.4819803857803344 and perplexity is 32.524068542279814
At time: 84.554616689682 and batch: 550, loss is 3.5337983989715576 and perplexity is 34.25383053770751
At time: 85.32165765762329 and batch: 600, loss is 3.55143967628479 and perplexity is 34.86347349072485
At time: 86.08816719055176 and batch: 650, loss is 3.3778510236740114 and perplexity is 29.30772179150489
At time: 86.85369062423706 and batch: 700, loss is 3.3656536102294923 and perplexity is 28.952414710493155
At time: 87.61920356750488 and batch: 750, loss is 3.4582278156280517 and perplexity is 31.76064089941836
At time: 88.38378310203552 and batch: 800, loss is 3.3934357261657713 and perplexity is 29.76805165114013
At time: 89.16068482398987 and batch: 850, loss is 3.449976673126221 and perplexity is 31.499657511642848
At time: 89.92590689659119 and batch: 900, loss is 3.392933950424194 and perplexity is 29.75311851180443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332278316968108 and perplexity of 76.11750897875707
finished 6 epochs...
Completing Train Step...
At time: 91.8058249950409 and batch: 50, loss is 3.7136783361434937 and perplexity is 41.0043572785253
At time: 92.58357524871826 and batch: 100, loss is 3.577473978996277 and perplexity is 35.78303788372161
At time: 93.34808945655823 and batch: 150, loss is 3.5828834199905395 and perplexity is 35.977128603606616
At time: 94.11172842979431 and batch: 200, loss is 3.4675566291809083 and perplexity is 32.05831631711623
At time: 94.8752748966217 and batch: 250, loss is 3.605211606025696 and perplexity is 36.78946790673046
At time: 95.63921594619751 and batch: 300, loss is 3.582858781814575 and perplexity is 35.97624220370107
At time: 96.40346622467041 and batch: 350, loss is 3.555240445137024 and perplexity is 34.99623363033148
At time: 97.16898584365845 and batch: 400, loss is 3.495177993774414 and perplexity is 32.9561534211183
At time: 97.94550061225891 and batch: 450, loss is 3.5051427841186524 and perplexity is 33.28619625330731
At time: 98.71135759353638 and batch: 500, loss is 3.388568024635315 and perplexity is 29.62350175884504
At time: 99.4761209487915 and batch: 550, loss is 3.4443295001983643 and perplexity is 31.32227482526109
At time: 100.24282479286194 and batch: 600, loss is 3.46938994884491 and perplexity is 32.117143366731824
At time: 101.00851631164551 and batch: 650, loss is 3.3022866916656493 and perplexity is 27.174708105609042
At time: 101.77283358573914 and batch: 700, loss is 3.2956417322158815 and perplexity is 26.99473190172062
At time: 102.53799176216125 and batch: 750, loss is 3.3960268354415892 and perplexity is 29.845283941337712
At time: 103.30358076095581 and batch: 800, loss is 3.3390196466445925 and perplexity is 28.19147554602435
At time: 104.0691032409668 and batch: 850, loss is 3.4054815196990966 and perplexity is 30.128799842262364
At time: 104.83521842956543 and batch: 900, loss is 3.357395248413086 and perplexity is 28.714299767328303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3470743257705475 and perplexity of 77.25211745095504
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.72233510017395 and batch: 50, loss is 3.656059494018555 and perplexity is 38.708510832841256
At time: 107.48436689376831 and batch: 100, loss is 3.531075510978699 and perplexity is 34.16068805964199
At time: 108.25984597206116 and batch: 150, loss is 3.5405044317245484 and perplexity is 34.48430978415943
At time: 109.02253293991089 and batch: 200, loss is 3.4175124025344847 and perplexity is 30.49346512726892
At time: 109.78663778305054 and batch: 250, loss is 3.5565672492980958 and perplexity is 35.04269759620938
At time: 110.55106019973755 and batch: 300, loss is 3.5302874851226806 and perplexity is 34.13377915803923
At time: 111.31379389762878 and batch: 350, loss is 3.4963512134552004 and perplexity is 32.994840918944305
At time: 112.07810831069946 and batch: 400, loss is 3.4390751266479493 and perplexity is 31.158127516174137
At time: 112.84189653396606 and batch: 450, loss is 3.4398798418045042 and perplexity is 31.183211024822466
At time: 113.60641026496887 and batch: 500, loss is 3.3206949186325074 and perplexity is 27.679578931805185
At time: 114.37139821052551 and batch: 550, loss is 3.3671824407577513 and perplexity is 28.996711898788195
At time: 115.13453102111816 and batch: 600, loss is 3.3914278650283816 and perplexity is 29.708341501998135
At time: 115.89773726463318 and batch: 650, loss is 3.2180015945434572 and perplexity is 24.9781537925813
At time: 116.66034293174744 and batch: 700, loss is 3.2021630096435545 and perplexity is 24.585651726985944
At time: 117.42230415344238 and batch: 750, loss is 3.2954749345779417 and perplexity is 26.990229619698038
At time: 118.18607139587402 and batch: 800, loss is 3.229457712173462 and perplexity is 25.265951837127965
At time: 118.95004749298096 and batch: 850, loss is 3.2915213537216186 and perplexity is 26.883732226313633
At time: 119.71403479576111 and batch: 900, loss is 3.245091824531555 and perplexity is 25.66406654292325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.34095241598887 and perplexity of 76.78063162683523
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.60797810554504 and batch: 50, loss is 3.630190258026123 and perplexity is 37.719992465840626
At time: 122.38551139831543 and batch: 100, loss is 3.504624423980713 and perplexity is 33.26894648720778
At time: 123.14978861808777 and batch: 150, loss is 3.5147302675247194 and perplexity is 33.6068618392387
At time: 123.91229057312012 and batch: 200, loss is 3.391136350631714 and perplexity is 29.699682354943516
At time: 124.67643284797668 and batch: 250, loss is 3.5309536027908326 and perplexity is 34.15652384589541
At time: 125.44082140922546 and batch: 300, loss is 3.5058034801483156 and perplexity is 33.308195577647325
At time: 126.20465683937073 and batch: 350, loss is 3.470374193191528 and perplexity is 32.148770045156624
At time: 126.98048162460327 and batch: 400, loss is 3.4132743549346922 and perplexity is 30.364505831414352
At time: 127.74465847015381 and batch: 450, loss is 3.4131759071350096 and perplexity is 30.361516659767897
At time: 128.5090799331665 and batch: 500, loss is 3.2942672395706176 and perplexity is 26.957653329210935
At time: 129.27171516418457 and batch: 550, loss is 3.3396620035171507 and perplexity is 28.209590351550926
At time: 130.03468441963196 and batch: 600, loss is 3.3642697143554687 and perplexity is 28.912375294813547
At time: 130.80100965499878 and batch: 650, loss is 3.18987982749939 and perplexity is 24.285508817405617
At time: 131.56468653678894 and batch: 700, loss is 3.1733429336547854 and perplexity is 23.887204372240845
At time: 132.3280291557312 and batch: 750, loss is 3.2658486413955687 and perplexity is 26.202337946773838
At time: 133.0924813747406 and batch: 800, loss is 3.197940502166748 and perplexity is 24.482057496373248
At time: 133.8563826084137 and batch: 850, loss is 3.2557119703292847 and perplexity is 25.93807510140206
At time: 134.62006330490112 and batch: 900, loss is 3.2120416831970213 and perplexity is 24.829728949178012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338470458984375 and perplexity of 76.59030169341962
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.49680948257446 and batch: 50, loss is 3.620760931968689 and perplexity is 37.36598998333671
At time: 137.2761127948761 and batch: 100, loss is 3.494363269805908 and perplexity is 32.929314187784314
At time: 138.04219388961792 and batch: 150, loss is 3.504534273147583 and perplexity is 33.26594739915177
At time: 138.8085105419159 and batch: 200, loss is 3.3823015213012697 and perplexity is 29.438446416844734
At time: 139.5750970840454 and batch: 250, loss is 3.522585978507996 and perplexity is 33.87190733024652
At time: 140.34290146827698 and batch: 300, loss is 3.496483573913574 and perplexity is 32.999208420248166
At time: 141.10964918136597 and batch: 350, loss is 3.461642622947693 and perplexity is 31.869282758373895
At time: 141.8763976097107 and batch: 400, loss is 3.4046871280670166 and perplexity is 30.104875279777325
At time: 142.6437783241272 and batch: 450, loss is 3.404441065788269 and perplexity is 30.09746851686442
At time: 143.41352319717407 and batch: 500, loss is 3.2846612548828125 and perplexity is 26.69993830909672
At time: 144.17937803268433 and batch: 550, loss is 3.3306806325912475 and perplexity is 27.957363921981802
At time: 144.94797921180725 and batch: 600, loss is 3.3553835725784302 and perplexity is 28.656593966520315
At time: 145.7132248878479 and batch: 650, loss is 3.1813288164138793 and perplexity is 24.0787285126062
At time: 146.50309038162231 and batch: 700, loss is 3.1647320318222047 and perplexity is 23.682397053690554
At time: 147.26967883110046 and batch: 750, loss is 3.2566789627075194 and perplexity is 25.963169153254235
At time: 148.03566527366638 and batch: 800, loss is 3.1891142082214357 and perplexity is 24.266922479617243
At time: 148.80222272872925 and batch: 850, loss is 3.2452995777130127 and perplexity is 25.66939888828336
At time: 149.56926774978638 and batch: 900, loss is 3.2021073293685913 and perplexity is 24.58428282924829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337980505538313 and perplexity of 76.55278520258751
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.47415113449097 and batch: 50, loss is 3.618001503944397 and perplexity is 37.26302335325969
At time: 152.23829460144043 and batch: 100, loss is 3.4913595724105835 and perplexity is 32.83055289130679
At time: 153.0049023628235 and batch: 150, loss is 3.5018590545654296 and perplexity is 33.17707265121007
At time: 153.7684268951416 and batch: 200, loss is 3.37949182510376 and perplexity is 29.355849416463432
At time: 154.53222012519836 and batch: 250, loss is 3.5199344158172607 and perplexity is 33.78221281237608
At time: 155.29740476608276 and batch: 300, loss is 3.4941216325759887 and perplexity is 32.92135820079088
At time: 156.06238460540771 and batch: 350, loss is 3.45894082069397 and perplexity is 31.783294472372877
At time: 156.82723760604858 and batch: 400, loss is 3.402407283782959 and perplexity is 30.036319030429574
At time: 157.59163522720337 and batch: 450, loss is 3.402287721633911 and perplexity is 30.032728038254454
At time: 158.3551425933838 and batch: 500, loss is 3.2821578121185304 and perplexity is 26.6331801392244
At time: 159.11829805374146 and batch: 550, loss is 3.3280651473999026 and perplexity is 27.8843373921882
At time: 159.88159275054932 and batch: 600, loss is 3.353060131072998 and perplexity is 28.590089336421226
At time: 160.6449785232544 and batch: 650, loss is 3.1789690971374513 and perplexity is 24.021976458575317
At time: 161.40770363807678 and batch: 700, loss is 3.162371344566345 and perplexity is 23.626556258037695
At time: 162.17080855369568 and batch: 750, loss is 3.2542533922195434 and perplexity is 25.900269970420947
At time: 162.9332013130188 and batch: 800, loss is 3.1866436576843262 and perplexity is 24.207043818337006
At time: 163.69849801063538 and batch: 850, loss is 3.2425880289077758 and perplexity is 25.599889342248993
At time: 164.47307348251343 and batch: 900, loss is 3.199414758682251 and perplexity is 24.518176947276167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337751832726884 and perplexity of 76.53528166334051
Annealing...
Model not improving. Stopping early with 76.11750897875707 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
736.1853699684143


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.40036082233297, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.11750897875707, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5304780587349254, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10882121312718696, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.9235553054801141, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.46698454815308676, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0683300495147705 and batch: 50, loss is 6.826652164459229 and perplexity is 922.0986047845777
At time: 1.9143602848052979 and batch: 100, loss is 6.058542728424072 and perplexity is 427.7516321481582
At time: 2.744166851043701 and batch: 150, loss is 5.946746482849121 and perplexity is 382.5068198761154
At time: 3.5870823860168457 and batch: 200, loss is 5.794031639099121 and perplexity is 328.33408409959645
At time: 4.413957834243774 and batch: 250, loss is 5.846538705825806 and perplexity is 346.0345777734191
At time: 5.242234468460083 and batch: 300, loss is 5.758619260787964 and perplexity is 316.91045599995823
At time: 6.068958520889282 and batch: 350, loss is 5.7444594287872315 and perplexity is 312.454678165195
At time: 6.895462274551392 and batch: 400, loss is 5.609836130142212 and perplexity is 273.09948156453527
At time: 7.722388982772827 and batch: 450, loss is 5.62161865234375 and perplexity is 276.3363138466312
At time: 8.550521850585938 and batch: 500, loss is 5.570745525360107 and perplexity is 262.629823466241
At time: 9.38315749168396 and batch: 550, loss is 5.628200693130493 and perplexity is 278.1611697851153
At time: 10.212563276290894 and batch: 600, loss is 5.557618560791016 and perplexity is 259.20482020912164
At time: 11.043402194976807 and batch: 650, loss is 5.466461420059204 and perplexity is 236.62140581889125
At time: 11.873990297317505 and batch: 700, loss is 5.568528614044189 and perplexity is 262.0482413348969
At time: 12.705646753311157 and batch: 750, loss is 5.535363245010376 and perplexity is 253.49985347175382
At time: 13.536833763122559 and batch: 800, loss is 5.528501796722412 and perplexity is 251.76643103108242
At time: 14.37034273147583 and batch: 850, loss is 5.567967205047608 and perplexity is 261.9011663831365
At time: 15.20771050453186 and batch: 900, loss is 5.461434583663941 and perplexity is 235.43493332420334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.360739930035317 and perplexity of 212.88240627412867
finished 1 epochs...
Completing Train Step...
At time: 17.1483051776886 and batch: 50, loss is 5.293369445800781 and perplexity is 199.01286000969088
At time: 17.913570642471313 and batch: 100, loss is 5.117512741088867 and perplexity is 166.91968040951778
At time: 18.677311897277832 and batch: 150, loss is 5.053181734085083 and perplexity is 156.51967705932734
At time: 19.441818952560425 and batch: 200, loss is 4.894522552490234 and perplexity is 133.5562253024597
At time: 20.20463252067566 and batch: 250, loss is 4.960149040222168 and perplexity is 142.61504969180518
At time: 20.967974185943604 and batch: 300, loss is 4.891657991409302 and perplexity is 133.17419277633232
At time: 21.731619834899902 and batch: 350, loss is 4.853050746917725 and perplexity is 128.13068849510998
At time: 22.496028184890747 and batch: 400, loss is 4.717388687133789 and perplexity is 111.87572836889584
At time: 23.27117919921875 and batch: 450, loss is 4.713477869033813 and perplexity is 111.43905717280694
At time: 24.03600239753723 and batch: 500, loss is 4.620994520187378 and perplexity is 101.59502020241173
At time: 24.80030345916748 and batch: 550, loss is 4.686723041534424 and perplexity is 108.49705627803307
At time: 25.565716981887817 and batch: 600, loss is 4.629242677688598 and perplexity is 102.43645731289898
At time: 26.329070568084717 and batch: 650, loss is 4.485194416046142 and perplexity is 88.69419271111815
At time: 27.094775676727295 and batch: 700, loss is 4.530986042022705 and perplexity is 92.85007002972388
At time: 27.85839319229126 and batch: 750, loss is 4.569558744430542 and perplexity is 96.5015185442085
At time: 28.623244285583496 and batch: 800, loss is 4.500921335220337 and perplexity is 90.10010547166634
At time: 29.38635492324829 and batch: 850, loss is 4.563835201263427 and perplexity is 95.95076556984665
At time: 30.15299081802368 and batch: 900, loss is 4.496524353027343 and perplexity is 89.70480661022205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.60322048239512 and perplexity of 99.80521958444314
finished 2 epochs...
Completing Train Step...
At time: 32.044416427612305 and batch: 50, loss is 4.534344301223755 and perplexity is 93.16240879544243
At time: 32.82486581802368 and batch: 100, loss is 4.386906747817993 and perplexity is 80.39136322322474
At time: 33.605891942977905 and batch: 150, loss is 4.377721214294434 and perplexity is 79.656306772467
At time: 34.37128233909607 and batch: 200, loss is 4.265008792877198 and perplexity is 71.16554572054814
At time: 35.13582944869995 and batch: 250, loss is 4.406380205154419 and perplexity is 81.97220325939267
At time: 35.900776624679565 and batch: 300, loss is 4.368433012962341 and perplexity is 78.91986834639137
At time: 36.66629505157471 and batch: 350, loss is 4.360077629089355 and perplexity is 78.2632096889707
At time: 37.429856300354004 and batch: 400, loss is 4.26682421207428 and perplexity is 71.29485836123006
At time: 38.19364833831787 and batch: 450, loss is 4.280329060554505 and perplexity is 72.26421539812894
At time: 38.957383155822754 and batch: 500, loss is 4.169378266334534 and perplexity is 64.67522883752822
At time: 39.72327017784119 and batch: 550, loss is 4.250244760513306 and perplexity is 70.12257348349802
At time: 40.48841571807861 and batch: 600, loss is 4.2450075387954715 and perplexity is 69.75628602029408
At time: 41.25445604324341 and batch: 650, loss is 4.088097462654114 and perplexity is 59.62634237930513
At time: 42.01979875564575 and batch: 700, loss is 4.112642641067505 and perplexity is 61.10799086826734
At time: 42.79694747924805 and batch: 750, loss is 4.205315246582031 and perplexity is 67.04172901126181
At time: 43.561944246292114 and batch: 800, loss is 4.1476297950744625 and perplexity is 63.28382675950525
At time: 44.328089237213135 and batch: 850, loss is 4.219530873298645 and perplexity is 68.00157545058264
At time: 45.092682123184204 and batch: 900, loss is 4.169958534240723 and perplexity is 64.71276868767985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.421992053724315 and perplexity of 83.26198260718505
finished 3 epochs...
Completing Train Step...
At time: 46.97023034095764 and batch: 50, loss is 4.239260115623474 and perplexity is 69.35651704681858
At time: 47.74533939361572 and batch: 100, loss is 4.092728123664856 and perplexity is 59.90309203055518
At time: 48.50762319564819 and batch: 150, loss is 4.095168595314026 and perplexity is 60.049462362028194
At time: 49.26929712295532 and batch: 200, loss is 3.9806571674346922 and perplexity is 53.552215438293665
At time: 50.03194832801819 and batch: 250, loss is 4.131656589508057 and perplexity is 62.28101159374732
At time: 50.79441976547241 and batch: 300, loss is 4.100265202522277 and perplexity is 60.35629211595328
At time: 51.557626724243164 and batch: 350, loss is 4.097289204597473 and perplexity is 60.17693892559953
At time: 52.323286294937134 and batch: 400, loss is 4.017487845420837 and perplexity is 55.56135165426876
At time: 53.09075140953064 and batch: 450, loss is 4.037767238616944 and perplexity is 56.69960468235765
At time: 53.85533261299133 and batch: 500, loss is 3.922600245475769 and perplexity is 50.53166884002628
At time: 54.61767601966858 and batch: 550, loss is 4.003072185516357 and perplexity is 54.7661436004667
At time: 55.380391359329224 and batch: 600, loss is 4.017365937232971 and perplexity is 55.55457868342187
At time: 56.14367866516113 and batch: 650, loss is 3.856663775444031 and perplexity is 47.307260158582714
At time: 56.90564203262329 and batch: 700, loss is 3.871933813095093 and perplexity is 48.035187396111475
At time: 57.6673538684845 and batch: 750, loss is 3.9776225423812868 and perplexity is 53.38995087423876
At time: 58.43019104003906 and batch: 800, loss is 3.926441969871521 and perplexity is 50.7261709575131
At time: 59.19339418411255 and batch: 850, loss is 3.9986747169494627 and perplexity is 54.5258399565577
At time: 59.956984519958496 and batch: 900, loss is 3.9632179975509643 and perplexity is 52.62640539825142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3700887601669525 and perplexity of 79.05064793688723
finished 4 epochs...
Completing Train Step...
At time: 61.84196400642395 and batch: 50, loss is 4.03653254032135 and perplexity is 56.62964097801694
At time: 62.6054949760437 and batch: 100, loss is 3.893238248825073 and perplexity is 49.06952886761216
At time: 63.36898112297058 and batch: 150, loss is 3.901766929626465 and perplexity is 49.48981692013419
At time: 64.13151574134827 and batch: 200, loss is 3.784824752807617 and perplexity is 44.02795410728278
At time: 64.89518547058105 and batch: 250, loss is 3.938022437095642 and perplexity is 51.31701825853433
At time: 65.66545128822327 and batch: 300, loss is 3.9115707731246947 and perplexity is 49.97739349690819
At time: 66.4306468963623 and batch: 350, loss is 3.907235794067383 and perplexity is 49.761211453616944
At time: 67.19361996650696 and batch: 400, loss is 3.835226716995239 and perplexity is 46.30392436461544
At time: 67.95717000961304 and batch: 450, loss is 3.860256385803223 and perplexity is 47.47752237125893
At time: 68.7307608127594 and batch: 500, loss is 3.747345623970032 and perplexity is 42.40836472243457
At time: 69.49397110939026 and batch: 550, loss is 3.8225059604644773 and perplexity is 45.718633977118735
At time: 70.2567183971405 and batch: 600, loss is 3.847194972038269 and perplexity is 46.8614310774526
At time: 71.01895451545715 and batch: 650, loss is 3.683757095336914 and perplexity is 39.79562952523207
At time: 71.78105473518372 and batch: 700, loss is 3.696861424446106 and perplexity is 40.32055644737794
At time: 72.54467606544495 and batch: 750, loss is 3.8046654748916624 and perplexity is 44.910224002298065
At time: 73.3087170124054 and batch: 800, loss is 3.7592320823669434 and perplexity is 42.9154577851906
At time: 74.07254433631897 and batch: 850, loss is 3.8318183708190916 and perplexity is 46.146373208028024
At time: 74.83557844161987 and batch: 900, loss is 3.80064971446991 and perplexity is 44.730236936562626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.358817740662457 and perplexity of 78.16466886508807
finished 5 epochs...
Completing Train Step...
At time: 76.72286343574524 and batch: 50, loss is 3.875312671661377 and perplexity is 48.19776601092485
At time: 77.50141167640686 and batch: 100, loss is 3.7367213439941405 and perplexity is 41.96019135759155
At time: 78.2679660320282 and batch: 150, loss is 3.7465838766098023 and perplexity is 42.37607256335056
At time: 79.03454566001892 and batch: 200, loss is 3.6326677322387697 and perplexity is 37.813558630497894
At time: 79.80094456672668 and batch: 250, loss is 3.782761926651001 and perplexity is 43.93722570256387
At time: 80.57979130744934 and batch: 300, loss is 3.7583369636535644 and perplexity is 42.87706054344244
At time: 81.34641146659851 and batch: 350, loss is 3.75576931476593 and perplexity is 42.76710852612834
At time: 82.1123697757721 and batch: 400, loss is 3.686975288391113 and perplexity is 39.92390584202288
At time: 82.88377809524536 and batch: 450, loss is 3.7176960039138796 and perplexity is 41.169430545935484
At time: 83.6538074016571 and batch: 500, loss is 3.6058452892303468 and perplexity is 36.81278816269789
At time: 84.41937589645386 and batch: 550, loss is 3.6773336791992186 and perplexity is 39.54082487060378
At time: 85.18368077278137 and batch: 600, loss is 3.7060303783416746 and perplexity is 40.69195383129694
At time: 85.94987463951111 and batch: 650, loss is 3.5475053215026855 and perplexity is 34.726577691981866
At time: 86.71591806411743 and batch: 700, loss is 3.5567933559417724 and perplexity is 35.05062187878094
At time: 87.48170685768127 and batch: 750, loss is 3.6679368352890016 and perplexity is 39.17100619665646
At time: 88.24804186820984 and batch: 800, loss is 3.6204776430130003 and perplexity is 37.35540611027401
At time: 89.01469731330872 and batch: 850, loss is 3.6924550676345826 and perplexity is 40.1432805461012
At time: 89.78034472465515 and batch: 900, loss is 3.666265034675598 and perplexity is 39.10557479383855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.368309125508348 and perplexity of 78.9100917704425
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.65825176239014 and batch: 50, loss is 3.759678406715393 and perplexity is 42.93461627405698
At time: 92.43669509887695 and batch: 100, loss is 3.627327971458435 and perplexity is 37.61218140475352
At time: 93.2028501033783 and batch: 150, loss is 3.640670881271362 and perplexity is 38.11740039938611
At time: 93.96833419799805 and batch: 200, loss is 3.5025624513626097 and perplexity is 33.200417507238704
At time: 94.73017311096191 and batch: 250, loss is 3.653559112548828 and perplexity is 38.61184568986203
At time: 95.49398183822632 and batch: 300, loss is 3.616653060913086 and perplexity is 37.212810151522945
At time: 96.26097750663757 and batch: 350, loss is 3.5962021255493166 and perplexity is 36.459502554047745
At time: 97.02641344070435 and batch: 400, loss is 3.520988917350769 and perplexity is 33.81785499665826
At time: 97.79112553596497 and batch: 450, loss is 3.5323739433288575 and perplexity is 34.20507221079545
At time: 98.56281208992004 and batch: 500, loss is 3.408812394142151 and perplexity is 30.229322412748182
At time: 99.32542514801025 and batch: 550, loss is 3.4619112253189086 and perplexity is 31.87784407303498
At time: 100.10698223114014 and batch: 600, loss is 3.482100887298584 and perplexity is 32.52798797806305
At time: 100.87050795555115 and batch: 650, loss is 3.3006515216827395 and perplexity is 27.130309148434193
At time: 101.6346697807312 and batch: 700, loss is 3.289177474975586 and perplexity is 26.820793806560612
At time: 102.3998646736145 and batch: 750, loss is 3.3853031635284423 and perplexity is 29.526942851584625
At time: 103.16250443458557 and batch: 800, loss is 3.316164255142212 and perplexity is 27.554455733697495
At time: 103.9265706539154 and batch: 850, loss is 3.3658772230148317 and perplexity is 28.958889564491958
At time: 104.70436787605286 and batch: 900, loss is 3.3237542152404784 and perplexity is 27.764388636615866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.348690085215111 and perplexity of 77.37703918389631
finished 7 epochs...
Completing Train Step...
At time: 106.68624830245972 and batch: 50, loss is 3.6425968742370607 and perplexity is 38.19088498710995
At time: 107.45069289207458 and batch: 100, loss is 3.5068590307235716 and perplexity is 33.34337262496016
At time: 108.21647906303406 and batch: 150, loss is 3.5174144077301026 and perplexity is 33.697188538810416
At time: 108.98107671737671 and batch: 200, loss is 3.383846082687378 and perplexity is 29.483951037740788
At time: 109.7541344165802 and batch: 250, loss is 3.5357622718811035 and perplexity is 34.32116680584325
At time: 110.5186665058136 and batch: 300, loss is 3.5038700103759766 and perplexity is 33.243857406324494
At time: 111.28236722946167 and batch: 350, loss is 3.4876826190948487 and perplexity is 32.710058143534106
At time: 112.0459656715393 and batch: 400, loss is 3.416954975128174 and perplexity is 30.47647197075919
At time: 112.81132364273071 and batch: 450, loss is 3.4375708293914795 and perplexity is 31.111291666786798
At time: 113.57452964782715 and batch: 500, loss is 3.31811900138855 and perplexity is 27.608370380171767
At time: 114.34649085998535 and batch: 550, loss is 3.3751643657684327 and perplexity is 29.229087647880338
At time: 115.11068868637085 and batch: 600, loss is 3.4027424335479735 and perplexity is 30.046387382803243
At time: 115.87263989448547 and batch: 650, loss is 3.228305501937866 and perplexity is 25.23685691376225
At time: 116.63346362113953 and batch: 700, loss is 3.2229207515716554 and perplexity is 25.1013279615192
At time: 117.40407061576843 and batch: 750, loss is 3.3267575168609618 and perplexity is 27.847898810349438
At time: 118.17516946792603 and batch: 800, loss is 3.265554804801941 and perplexity is 26.194639872089883
At time: 118.95402956008911 and batch: 850, loss is 3.3243704319000242 and perplexity is 27.78150278790687
At time: 119.71967148780823 and batch: 900, loss is 3.290373568534851 and perplexity is 26.852893178386076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367021116491866 and perplexity of 78.80852028728583
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.59369254112244 and batch: 50, loss is 3.590025863647461 and perplexity is 36.235013083961405
At time: 122.36780261993408 and batch: 100, loss is 3.464349594116211 and perplexity is 31.95566885755118
At time: 123.12939667701721 and batch: 150, loss is 3.4779390144348143 and perplexity is 32.39289194883504
At time: 123.89097833633423 and batch: 200, loss is 3.3390529632568358 and perplexity is 28.192414806130078
At time: 124.65467715263367 and batch: 250, loss is 3.4923873233795164 and perplexity is 32.864311868790786
At time: 125.42028975486755 and batch: 300, loss is 3.455596103668213 and perplexity is 31.677165930127057
At time: 126.18546748161316 and batch: 350, loss is 3.4347408628463745 and perplexity is 31.023372215424576
At time: 126.94967269897461 and batch: 400, loss is 3.3641587400436403 and perplexity is 28.909166941887552
At time: 127.71540117263794 and batch: 450, loss is 3.377021760940552 and perplexity is 29.283428064354077
At time: 128.4811191558838 and batch: 500, loss is 3.2529190587997436 and perplexity is 25.86573342137758
At time: 129.2437288761139 and batch: 550, loss is 3.301337561607361 and perplexity is 27.148928009588143
At time: 130.00874662399292 and batch: 600, loss is 3.3275590133666992 and perplexity is 27.870227751026693
At time: 130.77512621879578 and batch: 650, loss is 3.1453640604019166 and perplexity is 23.2281303766239
At time: 131.54012298583984 and batch: 700, loss is 3.1304851293563845 and perplexity is 22.88507907355069
At time: 132.30582523345947 and batch: 750, loss is 3.2301286172866823 and perplexity is 25.282908580952903
At time: 133.07129883766174 and batch: 800, loss is 3.160935254096985 and perplexity is 23.59265073729643
At time: 133.8364815711975 and batch: 850, loss is 3.2139030504226684 and perplexity is 24.875989233197522
At time: 134.60080814361572 and batch: 900, loss is 3.178334593772888 and perplexity is 24.006739268224365
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362418396832192 and perplexity of 78.44662026269367
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.489102602005 and batch: 50, loss is 3.5659376049041747 and perplexity is 35.37260338746728
At time: 137.26817297935486 and batch: 100, loss is 3.4406372690200806 and perplexity is 31.20683898462202
At time: 138.03432607650757 and batch: 150, loss is 3.4517317056655883 and perplexity is 31.554988975612183
At time: 138.81080389022827 and batch: 200, loss is 3.312478804588318 and perplexity is 27.453092049705525
At time: 139.57555222511292 and batch: 250, loss is 3.466015267372131 and perplexity is 32.00894091514627
At time: 140.34142208099365 and batch: 300, loss is 3.4303273916244508 and perplexity is 30.886753158845778
At time: 141.10652494430542 and batch: 350, loss is 3.4088625860214234 and perplexity is 30.230839717327072
At time: 141.872132062912 and batch: 400, loss is 3.339555163383484 and perplexity is 28.206576596144963
At time: 142.6368317604065 and batch: 450, loss is 3.3518935871124267 and perplexity is 28.556757185863027
At time: 143.40288639068604 and batch: 500, loss is 3.2267683124542237 and perplexity is 25.19809288417666
At time: 144.16833591461182 and batch: 550, loss is 3.2742803287506104 and perplexity is 26.42420189762722
At time: 144.93412470817566 and batch: 600, loss is 3.301052408218384 and perplexity is 27.141187504427208
At time: 145.70002222061157 and batch: 650, loss is 3.117716450691223 and perplexity is 22.594724519552567
At time: 146.46607756614685 and batch: 700, loss is 3.10126513004303 and perplexity is 22.226052348505483
At time: 147.23003029823303 and batch: 750, loss is 3.200744209289551 and perplexity is 24.550794329303685
At time: 147.99472904205322 and batch: 800, loss is 3.1298391389846802 and perplexity is 22.870300306797933
At time: 148.76023244857788 and batch: 850, loss is 3.180439591407776 and perplexity is 24.057326622055985
At time: 149.5254828929901 and batch: 900, loss is 3.144167947769165 and perplexity is 23.200363525896147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.362461873929795 and perplexity of 78.45003096820271
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.42593574523926 and batch: 50, loss is 3.557210536003113 and perplexity is 35.0652473498817
At time: 152.18855929374695 and batch: 100, loss is 3.430937204360962 and perplexity is 30.905594038431826
At time: 152.9519760608673 and batch: 150, loss is 3.4432602071762086 and perplexity is 31.28880003572139
At time: 153.7138476371765 and batch: 200, loss is 3.3041937875747682 and perplexity is 27.226582329106186
At time: 154.47540187835693 and batch: 250, loss is 3.457205390930176 and perplexity is 31.728184630615196
At time: 155.2374460697174 and batch: 300, loss is 3.4214937496185303 and perplexity is 30.61511219464745
At time: 155.9999852180481 and batch: 350, loss is 3.3998599290847777 and perplexity is 29.95990324241158
At time: 156.76381421089172 and batch: 400, loss is 3.3307682609558107 and perplexity is 27.959813887401445
At time: 157.53779315948486 and batch: 450, loss is 3.3437410497665407 and perplexity is 28.324893578996154
At time: 158.30129885673523 and batch: 500, loss is 3.218451433181763 and perplexity is 24.989392458874395
At time: 159.06329035758972 and batch: 550, loss is 3.26534122467041 and perplexity is 26.1890458148716
At time: 159.82623553276062 and batch: 600, loss is 3.292737989425659 and perplexity is 26.916459839552456
At time: 160.58990716934204 and batch: 650, loss is 3.108906936645508 and perplexity is 22.39655016805531
At time: 161.3532359600067 and batch: 700, loss is 3.092763342857361 and perplexity is 22.037892163740985
At time: 162.1173779964447 and batch: 750, loss is 3.1925058126449586 and perplexity is 24.349366010136958
At time: 162.88127183914185 and batch: 800, loss is 3.120474033355713 and perplexity is 22.657117327321558
At time: 163.6441206932068 and batch: 850, loss is 3.1709815216064454 and perplexity is 23.830863388355187
At time: 164.40736413002014 and batch: 900, loss is 3.133906373977661 and perplexity is 22.963508614059457
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36171523838827 and perplexity of 78.39147924796586
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 166.28096103668213 and batch: 50, loss is 3.5544116926193237 and perplexity is 34.967242428525395
At time: 167.06882619857788 and batch: 100, loss is 3.4280563735961915 and perplexity is 30.816688374894788
At time: 167.83203959465027 and batch: 150, loss is 3.440744009017944 and perplexity is 31.210170180331318
At time: 168.5958104133606 and batch: 200, loss is 3.302075996398926 and perplexity is 27.168983126370794
At time: 169.3587167263031 and batch: 250, loss is 3.455043759346008 and perplexity is 31.659674058594682
At time: 170.12247490882874 and batch: 300, loss is 3.419303021430969 and perplexity is 30.54811621717986
At time: 170.88469243049622 and batch: 350, loss is 3.397199201583862 and perplexity is 29.880294060066024
At time: 171.64938759803772 and batch: 400, loss is 3.3282242441177368 and perplexity is 27.8887740516659
At time: 172.4136185646057 and batch: 450, loss is 3.3416346740722656 and perplexity is 28.26529350372961
At time: 173.17818593978882 and batch: 500, loss is 3.216090998649597 and perplexity is 24.93047619532319
At time: 173.9433557987213 and batch: 550, loss is 3.2628491735458374 and perplexity is 26.123862627448954
At time: 174.70744466781616 and batch: 600, loss is 3.2905996465682983 and perplexity is 26.858964713962767
At time: 175.471018075943 and batch: 650, loss is 3.106397943496704 and perplexity is 22.340427811871464
At time: 176.2329921722412 and batch: 700, loss is 3.090489821434021 and perplexity is 21.98784545646924
At time: 177.02032279968262 and batch: 750, loss is 3.1903270721435546 and perplexity is 24.296372810405266
At time: 177.78473806381226 and batch: 800, loss is 3.1179738903045653 and perplexity is 22.60054204549514
At time: 178.54854249954224 and batch: 850, loss is 3.1684850740432737 and perplexity is 23.771445085718604
At time: 179.3131217956543 and batch: 900, loss is 3.1311927747726442 and perplexity is 22.901279326199052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361172192717252 and perplexity of 78.34892065119226
Annealing...
Model not improving. Stopping early with 77.37703918389631 lossat 11 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
922.5259418487549


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.40036082233297, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.11750897875707, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5304780587349254, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10882121312718696, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -77.37703918389631, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.9235553054801141, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.46698454815308676, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'None', 'rnn_dropout': 0.5231190382360152, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10266485833677604, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0735201835632324 and batch: 50, loss is 6.7328917503356935 and perplexity is 839.5715904226442
At time: 1.904064416885376 and batch: 100, loss is 5.86380747795105 and perplexity is 352.0620638990843
At time: 2.7312963008880615 and batch: 150, loss is 5.5811417770385745 and perplexity is 265.37443130600593
At time: 3.561708450317383 and batch: 200, loss is 5.355952348709106 and perplexity is 211.86565028424954
At time: 4.39210844039917 and batch: 250, loss is 5.367298135757446 and perplexity is 214.28312095621934
At time: 5.222776412963867 and batch: 300, loss is 5.28085786819458 and perplexity is 196.5384071009143
At time: 6.051846265792847 and batch: 350, loss is 5.229550638198853 and perplexity is 186.70888482587682
At time: 6.8793745040893555 and batch: 400, loss is 5.072373523712158 and perplexity is 159.55258010198165
At time: 7.708322525024414 and batch: 450, loss is 5.07120231628418 and perplexity is 159.3658203235816
At time: 8.537102222442627 and batch: 500, loss is 4.992340612411499 and perplexity is 147.2807475195747
At time: 9.366413354873657 and batch: 550, loss is 5.050995960235595 and perplexity is 156.17793406479066
At time: 10.19578504562378 and batch: 600, loss is 4.974853734970093 and perplexity is 144.7276550052052
At time: 11.026118516921997 and batch: 650, loss is 4.850701751708985 and perplexity is 127.83006334349471
At time: 11.859980583190918 and batch: 700, loss is 4.925699739456177 and perplexity is 137.7857220245465
At time: 12.690582990646362 and batch: 750, loss is 4.9291633605957035 and perplexity is 138.26378700412803
At time: 13.519810676574707 and batch: 800, loss is 4.865201063156128 and perplexity is 129.69701328122423
At time: 14.353927373886108 and batch: 850, loss is 4.920083923339844 and perplexity is 137.01411138581182
At time: 15.214627265930176 and batch: 900, loss is 4.841716012954712 and perplexity is 126.68656108979386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8313577730361725 and perplexity of 125.38108418941368
finished 1 epochs...
Completing Train Step...
At time: 17.1522479057312 and batch: 50, loss is 4.817579965591431 and perplexity is 123.66545370692565
At time: 17.91485834121704 and batch: 100, loss is 4.663549604415894 and perplexity is 106.01171473949343
At time: 18.678470611572266 and batch: 150, loss is 4.638929290771484 and perplexity is 103.43354102615052
At time: 19.44152808189392 and batch: 200, loss is 4.524325895309448 and perplexity is 92.23372967785167
At time: 20.206235885620117 and batch: 250, loss is 4.643247127532959 and perplexity is 103.88111575389816
At time: 20.9703209400177 and batch: 300, loss is 4.604959173202515 and perplexity is 99.97890094760552
At time: 21.73580503463745 and batch: 350, loss is 4.585687866210938 and perplexity is 98.07062341401539
At time: 22.50000500679016 and batch: 400, loss is 4.477703714370728 and perplexity is 88.03229311434971
At time: 23.263248682022095 and batch: 450, loss is 4.4996324634552005 and perplexity is 89.98405279426316
At time: 24.027158975601196 and batch: 500, loss is 4.3828135681152345 and perplexity is 80.06297945237675
At time: 24.79182267189026 and batch: 550, loss is 4.467811136245728 and perplexity is 87.16572016461113
At time: 25.55610966682434 and batch: 600, loss is 4.443578100204467 and perplexity is 85.07881823615388
At time: 26.32150650024414 and batch: 650, loss is 4.291467590332031 and perplexity is 73.07363200018975
At time: 27.08557415008545 and batch: 700, loss is 4.319377164840699 and perplexity is 75.14181275143892
At time: 27.850824117660522 and batch: 750, loss is 4.399651188850402 and perplexity is 81.42246264828904
At time: 28.61599898338318 and batch: 800, loss is 4.335430641174316 and perplexity is 76.35783463801096
At time: 29.380536794662476 and batch: 850, loss is 4.4070017051696775 and perplexity is 82.023164819634
At time: 30.14414882659912 and batch: 900, loss is 4.3508161926269535 and perplexity is 77.54172608940021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.505308804446703 and perplexity of 90.49628538978043
finished 2 epochs...
Completing Train Step...
At time: 32.04241061210632 and batch: 50, loss is 4.402696695327759 and perplexity is 81.67081327044434
At time: 32.821967124938965 and batch: 100, loss is 4.248345260620117 and perplexity is 69.98950208721872
At time: 33.586267709732056 and batch: 150, loss is 4.241702942848206 and perplexity is 69.52615014268974
At time: 34.36147594451904 and batch: 200, loss is 4.134791250228882 and perplexity is 62.47654774407537
At time: 35.12431335449219 and batch: 250, loss is 4.281970548629761 and perplexity is 72.38293365661848
At time: 35.88812589645386 and batch: 300, loss is 4.254106907844544 and perplexity is 70.39392084799785
At time: 36.65165185928345 and batch: 350, loss is 4.241284112930298 and perplexity is 69.49703660816829
At time: 37.41667175292969 and batch: 400, loss is 4.157610564231873 and perplexity is 63.91861057160593
At time: 38.18284010887146 and batch: 450, loss is 4.182204966545105 and perplexity is 65.51014175077782
At time: 38.946531772613525 and batch: 500, loss is 4.057438230514526 and perplexity is 57.82598432848565
At time: 39.71122407913208 and batch: 550, loss is 4.149251279830932 and perplexity is 63.38652375823483
At time: 40.47483706474304 and batch: 600, loss is 4.151569528579712 and perplexity is 63.533639947701
At time: 41.23764085769653 and batch: 650, loss is 3.9927159547805786 and perplexity is 54.201899544641655
At time: 42.00112056732178 and batch: 700, loss is 4.00734402179718 and perplexity is 55.00059601428836
At time: 42.766486406326294 and batch: 750, loss is 4.112167553901672 and perplexity is 61.078966141259585
At time: 43.53197360038757 and batch: 800, loss is 4.0552898073196415 and perplexity is 57.70188300130699
At time: 44.2961003780365 and batch: 850, loss is 4.13254819393158 and perplexity is 62.33656638195294
At time: 45.05885648727417 and batch: 900, loss is 4.086861896514892 and perplexity is 59.55271558440971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.404592435653895 and perplexity of 81.82578677297623
finished 3 epochs...
Completing Train Step...
At time: 46.93339705467224 and batch: 50, loss is 4.1577459859848025 and perplexity is 63.92726712802416
At time: 47.71173119544983 and batch: 100, loss is 4.005807104110718 and perplexity is 54.91612955113526
At time: 48.47663068771362 and batch: 150, loss is 4.01043128490448 and perplexity is 55.170659705847406
At time: 49.24181151390076 and batch: 200, loss is 3.904987487792969 and perplexity is 49.64945868398465
At time: 50.0070915222168 and batch: 250, loss is 4.0518800020217896 and perplexity is 57.50546587733611
At time: 50.77202105522156 and batch: 300, loss is 4.031170525550842 and perplexity is 56.326804638681956
At time: 51.53668928146362 and batch: 350, loss is 4.021453566551209 and perplexity is 55.782129963812466
At time: 52.30180311203003 and batch: 400, loss is 3.946625270843506 and perplexity is 51.76039444639152
At time: 53.066012144088745 and batch: 450, loss is 3.9725992488861084 and perplexity is 53.12242996187376
At time: 53.84253478050232 and batch: 500, loss is 3.8463140773773192 and perplexity is 46.82016926933343
At time: 54.60614085197449 and batch: 550, loss is 3.941176953315735 and perplexity is 51.47915422080825
At time: 55.37212872505188 and batch: 600, loss is 3.9506464767456055 and perplexity is 51.968952696834315
At time: 56.13744258880615 and batch: 650, loss is 3.792504458427429 and perplexity is 44.367377501539146
At time: 56.90200090408325 and batch: 700, loss is 3.8005714321136477 and perplexity is 44.72673548527178
At time: 57.66551637649536 and batch: 750, loss is 3.9119656610488893 and perplexity is 49.997132863245035
At time: 58.4305260181427 and batch: 800, loss is 3.8594224977493288 and perplexity is 47.43794793514304
At time: 59.19551968574524 and batch: 850, loss is 3.9395668649673463 and perplexity is 51.39633492549525
At time: 59.959763526916504 and batch: 900, loss is 3.898174343109131 and perplexity is 49.31233946356737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37234998729131 and perplexity of 79.22960165745222
finished 4 epochs...
Completing Train Step...
At time: 61.84980368614197 and batch: 50, loss is 3.975371789932251 and perplexity is 53.26991844387053
At time: 62.621166944503784 and batch: 100, loss is 3.8293600273132324 and perplexity is 46.03306889865212
At time: 63.390848875045776 and batch: 150, loss is 3.83497935295105 and perplexity is 46.292471855150694
At time: 64.15339231491089 and batch: 200, loss is 3.7324630069732665 and perplexity is 41.78189062307352
At time: 64.9250020980835 and batch: 250, loss is 3.879404606819153 and perplexity is 48.39539220522425
At time: 65.70159220695496 and batch: 300, loss is 3.8634650325775146 and perplexity is 47.63010563225197
At time: 66.46695065498352 and batch: 350, loss is 3.852780890464783 and perplexity is 47.12392766852832
At time: 67.23107647895813 and batch: 400, loss is 3.781679463386536 and perplexity is 43.88969100172352
At time: 67.99623107910156 and batch: 450, loss is 3.8079678583145142 and perplexity is 45.058779940884996
At time: 68.76157641410828 and batch: 500, loss is 3.687314248085022 and perplexity is 39.93744073068802
At time: 69.5268063545227 and batch: 550, loss is 3.7759459590911866 and perplexity is 43.63876928758673
At time: 70.30306577682495 and batch: 600, loss is 3.793898539543152 and perplexity is 44.42927235786919
At time: 71.06981492042542 and batch: 650, loss is 3.6362542724609375 and perplexity is 37.949421973504464
At time: 71.84911394119263 and batch: 700, loss is 3.6409771156311037 and perplexity is 38.129075044589946
At time: 72.63988995552063 and batch: 750, loss is 3.7583120393753053 and perplexity is 42.87599187697245
At time: 73.40830945968628 and batch: 800, loss is 3.7064221239089967 and perplexity is 40.70789784663079
At time: 74.1786561012268 and batch: 850, loss is 3.7854966735839843 and perplexity is 44.05754734543079
At time: 74.9448471069336 and batch: 900, loss is 3.745580930709839 and perplexity is 42.33359296104893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376666343375428 and perplexity of 79.5723239542413
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 76.82407116889954 and batch: 50, loss is 3.8464675045013426 and perplexity is 46.82735330434953
At time: 77.60903143882751 and batch: 100, loss is 3.7021997213363647 and perplexity is 40.53637508794085
At time: 78.37679505348206 and batch: 150, loss is 3.7115791988372804 and perplexity is 40.918373779602774
At time: 79.14036130905151 and batch: 200, loss is 3.5888654708862306 and perplexity is 36.19299062307071
At time: 79.90330123901367 and batch: 250, loss is 3.729500560760498 and perplexity is 41.65829717928851
At time: 80.6655900478363 and batch: 300, loss is 3.7015242099761965 and perplexity is 40.50900155267684
At time: 81.42699360847473 and batch: 350, loss is 3.6765279388427734 and perplexity is 39.508978064129955
At time: 82.18825030326843 and batch: 400, loss is 3.6007832431793214 and perplexity is 36.626910990069874
At time: 82.94875907897949 and batch: 450, loss is 3.605952181816101 and perplexity is 36.81672338713284
At time: 83.70914220809937 and batch: 500, loss is 3.480888247489929 and perplexity is 32.488567151407935
At time: 84.46985077857971 and batch: 550, loss is 3.540981330871582 and perplexity is 34.500759244136496
At time: 85.23004794120789 and batch: 600, loss is 3.550068802833557 and perplexity is 34.815712824892735
At time: 85.99668741226196 and batch: 650, loss is 3.381857409477234 and perplexity is 29.42537535743063
At time: 86.7606258392334 and batch: 700, loss is 3.3648319625854493 and perplexity is 28.928635797438638
At time: 87.52182078361511 and batch: 750, loss is 3.4646038675308226 and perplexity is 31.963795367721758
At time: 88.28340673446655 and batch: 800, loss is 3.3895029163360597 and perplexity is 29.651209474625052
At time: 89.04411888122559 and batch: 850, loss is 3.4459062433242797 and perplexity is 31.371700962701144
At time: 89.80651116371155 and batch: 900, loss is 3.3946399307250976 and perplexity is 29.803920066781096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.342385069964683 and perplexity of 76.89071053766746
finished 6 epochs...
Completing Train Step...
At time: 91.68777418136597 and batch: 50, loss is 3.721119513511658 and perplexity is 41.31061602356739
At time: 92.47495913505554 and batch: 100, loss is 3.5766241121292115 and perplexity is 35.752639984334685
At time: 93.23732113838196 and batch: 150, loss is 3.5858337259292603 and perplexity is 36.08342887183446
At time: 94.00011801719666 and batch: 200, loss is 3.4684008979797363 and perplexity is 32.0853935819623
At time: 94.76309442520142 and batch: 250, loss is 3.6095940542221068 and perplexity is 36.95104964719926
At time: 95.53559184074402 and batch: 300, loss is 3.58781973361969 and perplexity is 36.15516204681192
At time: 96.29941582679749 and batch: 350, loss is 3.5670354700088502 and perplexity is 35.41145905962845
At time: 97.06283640861511 and batch: 400, loss is 3.4963549041748045 and perplexity is 32.99496269387524
At time: 97.82776665687561 and batch: 450, loss is 3.50777382850647 and perplexity is 33.37388902435093
At time: 98.59019470214844 and batch: 500, loss is 3.3883238458633422 and perplexity is 29.61626921161722
At time: 99.3542377948761 and batch: 550, loss is 3.4517691230773924 and perplexity is 31.556169703718915
At time: 100.11812543869019 and batch: 600, loss is 3.468756470680237 and perplexity is 32.09680430055465
At time: 100.88127279281616 and batch: 650, loss is 3.306631202697754 and perplexity is 27.293025754832723
At time: 101.64270615577698 and batch: 700, loss is 3.2955496549606322 and perplexity is 26.992246415331056
At time: 102.40661978721619 and batch: 750, loss is 3.4034724473953246 and perplexity is 30.06832966976777
At time: 103.17098808288574 and batch: 800, loss is 3.336293783187866 and perplexity is 28.114734073947297
At time: 103.934987783432 and batch: 850, loss is 3.4004369258880613 and perplexity is 29.97719499897353
At time: 104.70079684257507 and batch: 900, loss is 3.3592768001556395 and perplexity is 28.7683780676977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.356518889126712 and perplexity of 77.98518627698115
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 106.6062126159668 and batch: 50, loss is 3.664369125366211 and perplexity is 39.03150440809191
At time: 107.37351894378662 and batch: 100, loss is 3.5301363182067873 and perplexity is 34.12861964989942
At time: 108.1401424407959 and batch: 150, loss is 3.5400885009765624 and perplexity is 34.469969681848596
At time: 108.90548157691956 and batch: 200, loss is 3.4204173564910887 and perplexity is 30.582176027597253
At time: 109.67090964317322 and batch: 250, loss is 3.5618525075912477 and perplexity is 35.228397608280794
At time: 110.44022607803345 and batch: 300, loss is 3.535065097808838 and perplexity is 34.2972473172088
At time: 111.21716403961182 and batch: 350, loss is 3.509260196685791 and perplexity is 33.42353179559507
At time: 111.98515820503235 and batch: 400, loss is 3.440373697280884 and perplexity is 31.198614827671296
At time: 112.75282025337219 and batch: 450, loss is 3.4430378437042237 and perplexity is 31.28184332299927
At time: 113.52041053771973 and batch: 500, loss is 3.3207630062103273 and perplexity is 27.681463631451315
At time: 114.28770589828491 and batch: 550, loss is 3.3751119995117187 and perplexity is 29.227557070048718
At time: 115.05310201644897 and batch: 600, loss is 3.3895194911956787 and perplexity is 29.651700943332628
At time: 115.81947755813599 and batch: 650, loss is 3.221369242668152 and perplexity is 25.06241322378441
At time: 116.58601117134094 and batch: 700, loss is 3.201380577087402 and perplexity is 24.566422636374796
At time: 117.35329604148865 and batch: 750, loss is 3.3038110065460207 and perplexity is 27.216162504295827
At time: 118.12031054496765 and batch: 800, loss is 3.2289121294021608 and perplexity is 25.252170928760027
At time: 118.88576459884644 and batch: 850, loss is 3.2861847639083863 and perplexity is 26.74064690817796
At time: 119.65101099014282 and batch: 900, loss is 3.245484108924866 and perplexity is 25.674136130638853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.347007019879067 and perplexity of 77.24691810329657
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 121.52753949165344 and batch: 50, loss is 3.636388177871704 and perplexity is 37.954503946686444
At time: 122.30269813537598 and batch: 100, loss is 3.5009388160705566 and perplexity is 33.146555875318754
At time: 123.06415390968323 and batch: 150, loss is 3.512807927131653 and perplexity is 33.542320066744665
At time: 123.8256607055664 and batch: 200, loss is 3.3943747520446776 and perplexity is 29.79601775039567
At time: 124.58853220939636 and batch: 250, loss is 3.534297728538513 and perplexity is 34.27093875904138
At time: 125.35135817527771 and batch: 300, loss is 3.5098685359954835 and perplexity is 33.44387082974483
At time: 126.11520457267761 and batch: 350, loss is 3.485017442703247 and perplexity is 32.62299613813738
At time: 126.87944674491882 and batch: 400, loss is 3.4148409032821654 and perplexity is 30.41211057567766
At time: 127.64351797103882 and batch: 450, loss is 3.4155937004089356 and perplexity is 30.435013344625787
At time: 128.40882921218872 and batch: 500, loss is 3.293540234565735 and perplexity is 26.93806210264348
At time: 129.17407703399658 and batch: 550, loss is 3.348124885559082 and perplexity is 28.449337833578806
At time: 129.93912148475647 and batch: 600, loss is 3.3633731698989866 and perplexity is 28.88646568131492
At time: 130.71680736541748 and batch: 650, loss is 3.193029727935791 and perplexity is 24.36212635769046
At time: 131.48191475868225 and batch: 700, loss is 3.1717389106750487 and perplexity is 23.848919460653658
At time: 132.24796080589294 and batch: 750, loss is 3.2722918128967287 and perplexity is 26.371709161843487
At time: 133.0113832950592 and batch: 800, loss is 3.196433129310608 and perplexity is 24.445181707204913
At time: 133.77633666992188 and batch: 850, loss is 3.249995484352112 and perplexity is 25.79022345736203
At time: 134.5411365032196 and batch: 900, loss is 3.211319899559021 and perplexity is 24.81181372332274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344776310332834 and perplexity of 77.07479471568314
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 136.4235143661499 and batch: 50, loss is 3.626151671409607 and perplexity is 37.56796420537602
At time: 137.2016532421112 and batch: 100, loss is 3.489505772590637 and perplexity is 32.76974799573375
At time: 137.96724700927734 and batch: 150, loss is 3.5026137685775756 and perplexity is 33.20212130391755
At time: 138.732670545578 and batch: 200, loss is 3.384866461753845 and perplexity is 29.51405119835171
At time: 139.49832201004028 and batch: 250, loss is 3.5249450540542604 and perplexity is 33.95190804529802
At time: 140.26513290405273 and batch: 300, loss is 3.5013804769515993 and perplexity is 33.16119864572159
At time: 141.03086352348328 and batch: 350, loss is 3.4760376262664794 and perplexity is 32.331359004889386
At time: 141.79642605781555 and batch: 400, loss is 3.4059119749069215 and perplexity is 30.141771732768593
At time: 142.5606598854065 and batch: 450, loss is 3.4066721200942993 and perplexity is 30.164692565967144
At time: 143.32584309577942 and batch: 500, loss is 3.284571886062622 and perplexity is 26.69755227373105
At time: 144.0888912677765 and batch: 550, loss is 3.3394240713119507 and perplexity is 28.202879179944382
At time: 144.85299611091614 and batch: 600, loss is 3.355590810775757 and perplexity is 28.662533322804954
At time: 145.6164848804474 and batch: 650, loss is 3.184362740516663 and perplexity is 24.151892478243383
At time: 146.3834502696991 and batch: 700, loss is 3.163216166496277 and perplexity is 23.646524924689327
At time: 147.1485276222229 and batch: 750, loss is 3.2630936765670775 and perplexity is 26.130250771716288
At time: 147.91380095481873 and batch: 800, loss is 3.1878151321411132 and perplexity is 24.235418368629965
At time: 148.67941522598267 and batch: 850, loss is 3.239900107383728 and perplexity is 25.531171244473345
At time: 149.45748090744019 and batch: 900, loss is 3.2005110454559325 and perplexity is 24.545070639284173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344701897608091 and perplexity of 77.06905958358539
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 151.34705471992493 and batch: 50, loss is 3.62342574596405 and perplexity is 37.46569618663474
At time: 152.1126000881195 and batch: 100, loss is 3.4864978885650633 and perplexity is 32.67132848565449
At time: 152.87778043746948 and batch: 150, loss is 3.5000431823730467 and perplexity is 33.116881993368324
At time: 153.6429054737091 and batch: 200, loss is 3.382194218635559 and perplexity is 29.435287762538874
At time: 154.4072937965393 and batch: 250, loss is 3.5223815774917604 and perplexity is 33.864984585498526
At time: 155.17220664024353 and batch: 300, loss is 3.4988005638122557 and perplexity is 33.075755898444186
At time: 155.93558359146118 and batch: 350, loss is 3.4733825445175173 and perplexity is 32.24563046218256
At time: 156.69988989830017 and batch: 400, loss is 3.4032595205307006 and perplexity is 30.061927996175037
At time: 157.46351838111877 and batch: 450, loss is 3.404368968009949 and perplexity is 30.095298634474087
At time: 158.228093624115 and batch: 500, loss is 3.282088222503662 and perplexity is 26.63132681096274
At time: 158.99752116203308 and batch: 550, loss is 3.336983404159546 and perplexity is 28.134129271076574
At time: 159.77332091331482 and batch: 600, loss is 3.35356183052063 and perplexity is 28.60443656714699
At time: 160.53919744491577 and batch: 650, loss is 3.1819929313659667 and perplexity is 24.09472486735993
At time: 161.30555415153503 and batch: 700, loss is 3.1609154891967775 and perplexity is 23.592184435517193
At time: 162.07212686538696 and batch: 750, loss is 3.2606709384918213 and perplexity is 26.06702064420265
At time: 162.8379545211792 and batch: 800, loss is 3.1853850984573366 and perplexity is 24.17659698357262
At time: 163.60321497917175 and batch: 850, loss is 3.237251868247986 and perplexity is 25.463648045853613
At time: 164.3678638935089 and batch: 900, loss is 3.1976695537567137 and perplexity is 24.47542502039022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344543875080266 and perplexity of 77.05688189817256
Annealing...
Model not improving. Stopping early with 76.89071053766746 lossat 10 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fa5c0a2eb70>
ELAPSED
1094.273776769638


RESULTS SO FAR:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.40036082233297, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.11750897875707, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5304780587349254, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10882121312718696, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -77.37703918389631, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.9235553054801141, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.46698454815308676, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.89071053766746, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5231190382360152, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10266485833677604, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'best_accuracy': -80.44317910673988, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.8640677829243316, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.9393660013930725, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.56079544842748, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.6976354442409246, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10733810844703018, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -78.40036082233297, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.32991339207981796, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.5347829683435903, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.11750897875707, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5304780587349254, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10882121312718696, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -77.37703918389631, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.9235553054801141, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.46698454815308676, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}, {'best_accuracy': -76.89071053766746, 'params': {'wordvec_source': 'None', 'rnn_dropout': 0.5231190382360152, 'seq_len': 35, 'data': 'ptb', 'tie_weights': 'FALSE', 'dropout': 0.10266485833677604, 'num_layers': 2, 'tune_wordvecs': 'FALSE', 'batch_size': 32, 'wordvec_dim': 300}}]
