Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0657262802124023 and batch: 50, loss is 6.846747255325317 and perplexity is 940.81569096349
At time: 1.6814393997192383 and batch: 100, loss is 5.9868693351745605 and perplexity is 398.16613195977396
At time: 2.2994768619537354 and batch: 150, loss is 5.679884729385376 and perplexity is 292.9156634078903
At time: 2.913555145263672 and batch: 200, loss is 5.390166521072388 and perplexity is 219.23989057613136
At time: 3.524662494659424 and batch: 250, loss is 5.357633895874024 and perplexity is 212.2222120715418
At time: 4.130787134170532 and batch: 300, loss is 5.237349433898926 and perplexity is 188.17068199374015
At time: 4.738790512084961 and batch: 350, loss is 5.1824272346496585 and perplexity is 178.1146126988915
At time: 5.363989591598511 and batch: 400, loss is 5.021184644699097 and perplexity is 151.5907786784716
At time: 5.97930121421814 and batch: 450, loss is 5.003248682022095 and perplexity is 148.89609028458463
At time: 6.594731092453003 and batch: 500, loss is 4.916430864334107 and perplexity is 136.51450385579267
At time: 7.209418296813965 and batch: 550, loss is 4.970521688461304 and perplexity is 144.1020441386832
At time: 7.824764966964722 and batch: 600, loss is 4.8819710159301755 and perplexity is 131.89036587555177
At time: 8.456553936004639 and batch: 650, loss is 4.750796728134155 and perplexity is 115.67641047332995
At time: 9.083701133728027 and batch: 700, loss is 4.813200149536133 and perplexity is 123.12500616165288
At time: 9.705991744995117 and batch: 750, loss is 4.816655778884888 and perplexity is 123.55121653492
At time: 10.325912714004517 and batch: 800, loss is 4.769687871932984 and perplexity is 117.88244179909388
At time: 10.945095300674438 and batch: 850, loss is 4.7970506286621095 and perplexity is 121.15256615373575
At time: 11.56460165977478 and batch: 900, loss is 4.7228256034851075 and perplexity is 112.4856438731419
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.765778842037672 and perplexity of 117.42253528974973
finished 1 epochs...
Completing Train Step...
At time: 13.208748579025269 and batch: 50, loss is 4.743395977020263 and perplexity is 114.82347821235955
At time: 13.832438945770264 and batch: 100, loss is 4.622475910186767 and perplexity is 101.74563358035965
At time: 14.448369026184082 and batch: 150, loss is 4.614822244644165 and perplexity is 100.9698790004451
At time: 15.066679954528809 and batch: 200, loss is 4.502066621780395 and perplexity is 90.20335502540014
At time: 15.696995258331299 and batch: 250, loss is 4.616285371780395 and perplexity is 101.11771889826183
At time: 16.319081783294678 and batch: 300, loss is 4.561690855026245 and perplexity is 95.74523435054702
At time: 16.93997287750244 and batch: 350, loss is 4.5583259391784665 and perplexity is 95.42360113196733
At time: 17.557610988616943 and batch: 400, loss is 4.451109309196472 and perplexity is 85.72198346297589
At time: 18.167815685272217 and batch: 450, loss is 4.482493753433228 and perplexity is 88.45498277893218
At time: 18.779298782348633 and batch: 500, loss is 4.37141604423523 and perplexity is 79.15564026438904
At time: 19.388455867767334 and batch: 550, loss is 4.447856607437134 and perplexity is 85.44360839752011
At time: 19.99753212928772 and batch: 600, loss is 4.420160331726074 and perplexity is 83.10960939731017
At time: 20.60711908340454 and batch: 650, loss is 4.271464042663574 and perplexity is 71.62642303303961
At time: 21.216626405715942 and batch: 700, loss is 4.30538423538208 and perplexity is 74.09768093452088
At time: 21.825294494628906 and batch: 750, loss is 4.372985429763794 and perplexity is 79.27996351075905
At time: 22.434552431106567 and batch: 800, loss is 4.33221999168396 and perplexity is 76.11306953288447
At time: 23.069620609283447 and batch: 850, loss is 4.388777370452881 and perplexity is 80.54188586856054
At time: 23.68002200126648 and batch: 900, loss is 4.3290418529510495 and perplexity is 75.87155562416078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.521410798373288 and perplexity of 91.96525092597363
finished 2 epochs...
Completing Train Step...
At time: 25.23260760307312 and batch: 50, loss is 4.407373132705689 and perplexity is 82.05363614023227
At time: 25.85856056213379 and batch: 100, loss is 4.2927193307876585 and perplexity is 73.16515849336042
At time: 26.47205686569214 and batch: 150, loss is 4.2900782442092895 and perplexity is 72.9721779266271
At time: 27.085741758346558 and batch: 200, loss is 4.180165510177613 and perplexity is 65.37667282333821
At time: 27.69927453994751 and batch: 250, loss is 4.321023073196411 and perplexity is 75.26559112489633
At time: 28.312223196029663 and batch: 300, loss is 4.2822048425674435 and perplexity is 72.39989452600278
At time: 28.922741651535034 and batch: 350, loss is 4.28010929107666 and perplexity is 72.24833567424727
At time: 29.53401279449463 and batch: 400, loss is 4.194745059013367 and perplexity is 66.33681744764381
At time: 30.146692991256714 and batch: 450, loss is 4.231588644981384 and perplexity is 68.82648621984211
At time: 30.76116633415222 and batch: 500, loss is 4.114261283874511 and perplexity is 61.206982972952666
At time: 31.390705347061157 and batch: 550, loss is 4.19832688331604 and perplexity is 66.57485031400638
At time: 32.0049102306366 and batch: 600, loss is 4.183642945289612 and perplexity is 65.60441170501615
At time: 32.62354278564453 and batch: 650, loss is 4.03681972026825 and perplexity is 56.64590621071852
At time: 33.25180006027222 and batch: 700, loss is 4.050706396102905 and perplexity is 57.43801670932742
At time: 33.88470387458801 and batch: 750, loss is 4.1422071504592894 and perplexity is 62.941589810399705
At time: 34.508437395095825 and batch: 800, loss is 4.109418663978577 and perplexity is 60.91129734383558
At time: 35.12716889381409 and batch: 850, loss is 4.1729348850250245 and perplexity is 64.90566350644113
At time: 35.746105670928955 and batch: 900, loss is 4.122208752632141 and perplexity is 61.69536167452615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.428098077643408 and perplexity of 83.77193757875355
finished 3 epochs...
Completing Train Step...
At time: 37.32618045806885 and batch: 50, loss is 4.2047138500213626 and perplexity is 67.00142246733645
At time: 37.947699308395386 and batch: 100, loss is 4.093065509796142 and perplexity is 59.92330591277746
At time: 38.58171081542969 and batch: 150, loss is 4.096211290359497 and perplexity is 60.11210829354169
At time: 39.20246744155884 and batch: 200, loss is 3.9832486295700074 and perplexity is 53.69117395196083
At time: 39.82432532310486 and batch: 250, loss is 4.135550699234009 and perplexity is 62.52401351773928
At time: 40.445542335510254 and batch: 300, loss is 4.0994868659973145 and perplexity is 60.309332886700446
At time: 41.06410479545593 and batch: 350, loss is 4.099689331054687 and perplexity is 60.32154465542998
At time: 41.70666551589966 and batch: 400, loss is 4.021530876159668 and perplexity is 55.78644262514191
At time: 42.33753418922424 and batch: 450, loss is 4.063176546096802 and perplexity is 58.15876195378592
At time: 42.967859745025635 and batch: 500, loss is 3.9476340055465697 and perplexity is 51.8126332956543
At time: 43.59918427467346 and batch: 550, loss is 4.029754228591919 and perplexity is 56.2470856227919
At time: 44.231547832489014 and batch: 600, loss is 4.01956687450409 and perplexity is 55.676985481656935
At time: 44.86314415931702 and batch: 650, loss is 3.876760206222534 and perplexity is 48.26758446311631
At time: 45.48530197143555 and batch: 700, loss is 3.8861724281311036 and perplexity is 48.72403441345134
At time: 46.10466265678406 and batch: 750, loss is 3.984200482368469 and perplexity is 53.74230437659814
At time: 46.72470688819885 and batch: 800, loss is 3.9577705335617064 and perplexity is 52.34050437473123
At time: 47.345157861709595 and batch: 850, loss is 4.016834812164307 and perplexity is 55.52508008841912
At time: 47.96518421173096 and batch: 900, loss is 3.975205969810486 and perplexity is 53.261085951830445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38918105870077 and perplexity of 80.57440624495376
finished 4 epochs...
Completing Train Step...
At time: 49.57239294052124 and batch: 50, loss is 4.057787594795227 and perplexity is 57.84619019130423
At time: 50.19813776016235 and batch: 100, loss is 3.950300416946411 and perplexity is 51.950971442973696
At time: 50.82513165473938 and batch: 150, loss is 3.954095387458801 and perplexity is 52.14849841514524
At time: 51.45377159118652 and batch: 200, loss is 3.8426292896270753 and perplexity is 46.647964347284045
At time: 52.0885546207428 and batch: 250, loss is 3.997813591957092 and perplexity is 54.4789066036829
At time: 52.71372747421265 and batch: 300, loss is 3.963191261291504 and perplexity is 52.624998383831475
At time: 53.336848735809326 and batch: 350, loss is 3.967568988800049 and perplexity is 52.85588128938748
At time: 53.96069145202637 and batch: 400, loss is 3.892638683319092 and perplexity is 49.040117288673564
At time: 54.598838090896606 and batch: 450, loss is 3.935124063491821 and perplexity is 51.168497705383864
At time: 55.22311449050903 and batch: 500, loss is 3.820350112915039 and perplexity is 45.62017773854856
At time: 55.848154067993164 and batch: 550, loss is 3.8996067476272582 and perplexity is 49.383025294680344
At time: 56.472999572753906 and batch: 600, loss is 3.8943389749526975 and perplexity is 49.12357071728851
At time: 57.10275483131409 and batch: 650, loss is 3.756584644317627 and perplexity is 42.801992032394644
At time: 57.73201656341553 and batch: 700, loss is 3.7616701698303223 and perplexity is 43.020217079069255
At time: 58.355238914489746 and batch: 750, loss is 3.866124863624573 and perplexity is 47.756962299849384
At time: 58.98089098930359 and batch: 800, loss is 3.8409934568405153 and perplexity is 46.57171845754412
At time: 59.60596060752869 and batch: 850, loss is 3.8945932817459106 and perplexity is 49.13606476362189
At time: 60.22995615005493 and batch: 900, loss is 3.8578455543518064 and perplexity is 47.36319992853243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38215658109482 and perplexity of 80.01039638914726
finished 5 epochs...
Completing Train Step...
At time: 61.81569743156433 and batch: 50, loss is 3.943312282562256 and perplexity is 51.589196610956414
At time: 62.459881067276 and batch: 100, loss is 3.8409829330444336 and perplexity is 46.5712283488548
At time: 63.09654998779297 and batch: 150, loss is 3.845642237663269 and perplexity is 46.78872418441093
At time: 63.72161793708801 and batch: 200, loss is 3.7363461780548097 and perplexity is 41.94445227555473
At time: 64.34350204467773 and batch: 250, loss is 3.88694944858551 and perplexity is 48.761908697452895
At time: 64.9650182723999 and batch: 300, loss is 3.8566656923294067 and perplexity is 47.3073508412648
At time: 65.59074568748474 and batch: 350, loss is 3.861578845977783 and perplexity is 47.540351038830295
At time: 66.20889496803284 and batch: 400, loss is 3.790369153022766 and perplexity is 44.27274067579042
At time: 66.82658505439758 and batch: 450, loss is 3.829647841453552 and perplexity is 46.046319773606584
At time: 67.44344329833984 and batch: 500, loss is 3.721578149795532 and perplexity is 41.32956691643634
At time: 68.06148362159729 and batch: 550, loss is 3.797117300033569 and perplexity is 44.5725099444281
At time: 68.67856502532959 and batch: 600, loss is 3.7949453353881837 and perplexity is 44.475805086455516
At time: 69.29604005813599 and batch: 650, loss is 3.6611111259460447 and perplexity is 38.90454671570621
At time: 69.9132752418518 and batch: 700, loss is 3.663787498474121 and perplexity is 39.008809236190395
At time: 70.54474830627441 and batch: 750, loss is 3.7673076820373534 and perplexity is 43.26342898891981
At time: 71.16222977638245 and batch: 800, loss is 3.7466395139694213 and perplexity is 42.378430321728125
At time: 71.77922797203064 and batch: 850, loss is 3.7980484533309937 and perplexity is 44.61403311325492
At time: 72.3949384689331 and batch: 900, loss is 3.7653201627731323 and perplexity is 43.17752748409348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386513905982449 and perplexity of 80.35978833490884
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 73.96112179756165 and batch: 50, loss is 3.8703666496276856 and perplexity is 47.959967361714234
At time: 74.60064172744751 and batch: 100, loss is 3.7696787023544314 and perplexity is 43.36612916202472
At time: 75.21699619293213 and batch: 150, loss is 3.778186764717102 and perplexity is 43.73666492890341
At time: 75.8325617313385 and batch: 200, loss is 3.6554010820388796 and perplexity is 38.683033073944756
At time: 76.4477789402008 and batch: 250, loss is 3.7949912309646607 and perplexity is 44.47784637601196
At time: 77.0745952129364 and batch: 300, loss is 3.757558889389038 and perplexity is 42.84371198160634
At time: 77.69309997558594 and batch: 350, loss is 3.7420260667800904 and perplexity is 42.183369967763184
At time: 78.30942893028259 and batch: 400, loss is 3.6653648090362547 and perplexity is 39.07038679371421
At time: 78.92439031600952 and batch: 450, loss is 3.69261372089386 and perplexity is 40.14964991364502
At time: 79.53961277008057 and batch: 500, loss is 3.582652530670166 and perplexity is 35.96882282772871
At time: 80.15597462654114 and batch: 550, loss is 3.632241678237915 and perplexity is 37.797451444066084
At time: 80.77163600921631 and batch: 600, loss is 3.6278951501846315 and perplexity is 37.63352028479949
At time: 81.38600277900696 and batch: 650, loss is 3.48174898147583 and perplexity is 32.516543203550604
At time: 82.0020899772644 and batch: 700, loss is 3.471415286064148 and perplexity is 32.18225732917581
At time: 82.6188714504242 and batch: 750, loss is 3.55953161239624 and perplexity is 35.14673099602637
At time: 83.23453760147095 and batch: 800, loss is 3.522745943069458 and perplexity is 33.877326068439984
At time: 83.85058093070984 and batch: 850, loss is 3.562850360870361 and perplexity is 35.263567924841695
At time: 84.46616911888123 and batch: 900, loss is 3.522448344230652 and perplexity is 33.867245715565204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335600552493578 and perplexity of 76.3708098007126
finished 7 epochs...
Completing Train Step...
At time: 86.03897953033447 and batch: 50, loss is 3.779798436164856 and perplexity is 43.80721089617973
At time: 86.65359544754028 and batch: 100, loss is 3.6730551290512086 and perplexity is 39.37200886989224
At time: 87.26637196540833 and batch: 150, loss is 3.6814323091506957 and perplexity is 39.70322065249502
At time: 87.88115429878235 and batch: 200, loss is 3.564502029418945 and perplexity is 35.32185977705175
At time: 88.49484491348267 and batch: 250, loss is 3.708911418914795 and perplexity is 40.809358043225025
At time: 89.11071419715881 and batch: 300, loss is 3.6740618181228637 and perplexity is 39.41166419789437
At time: 89.72720670700073 and batch: 350, loss is 3.663979959487915 and perplexity is 39.01631763367661
At time: 90.34388160705566 and batch: 400, loss is 3.5934371995925902 and perplexity is 36.35883396379855
At time: 90.96030139923096 and batch: 450, loss is 3.6242487955093385 and perplexity is 37.49654500415623
At time: 91.57589197158813 and batch: 500, loss is 3.518150444030762 and perplexity is 33.722000022761435
At time: 92.19135236740112 and batch: 550, loss is 3.5717946243286134 and perplexity is 35.58038932172508
At time: 92.80467939376831 and batch: 600, loss is 3.5739549922943117 and perplexity is 35.65733914506117
At time: 93.41797161102295 and batch: 650, loss is 3.4324816942214964 and perplexity is 30.953364295900997
At time: 94.03232312202454 and batch: 700, loss is 3.428859872817993 and perplexity is 30.84145951048389
At time: 94.64689564704895 and batch: 750, loss is 3.5218040847778322 and perplexity is 33.845433449505755
At time: 95.25964283943176 and batch: 800, loss is 3.4900864219665526 and perplexity is 32.7887812547466
At time: 95.8734724521637 and batch: 850, loss is 3.5378662586212157 and perplexity is 34.39345410480062
At time: 96.48634672164917 and batch: 900, loss is 3.503608713150024 and perplexity is 33.23517201338822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338740936697346 and perplexity of 76.61102046491406
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 98.04174852371216 and batch: 50, loss is 3.75643310546875 and perplexity is 42.79550635922055
At time: 98.65583539009094 and batch: 100, loss is 3.6566671657562257 and perplexity is 38.73204004917755
At time: 99.26955008506775 and batch: 150, loss is 3.6699213361740113 and perplexity is 39.24881827665434
At time: 99.88264441490173 and batch: 200, loss is 3.5492604255676268 and perplexity is 34.78757996666146
At time: 100.4963538646698 and batch: 250, loss is 3.6935131788253783 and perplexity is 40.18577908060465
At time: 101.10815834999084 and batch: 300, loss is 3.6506254148483275 and perplexity is 38.49873620259478
At time: 101.73444533348083 and batch: 350, loss is 3.640283617973328 and perplexity is 38.10264178711383
At time: 102.34842085838318 and batch: 400, loss is 3.570112552642822 and perplexity is 35.520590863024
At time: 102.96129870414734 and batch: 450, loss is 3.591640772819519 and perplexity is 36.29357661360345
At time: 103.5741446018219 and batch: 500, loss is 3.4835564374923704 and perplexity is 32.575368571318705
At time: 104.18697309494019 and batch: 550, loss is 3.5295989990234373 and perplexity is 34.11028661364738
At time: 104.8015923500061 and batch: 600, loss is 3.5348057746887207 and perplexity is 34.28835440114228
At time: 105.41895914077759 and batch: 650, loss is 3.387246322631836 and perplexity is 29.5843741804157
At time: 106.03236436843872 and batch: 700, loss is 3.377239103317261 and perplexity is 29.2897932858989
At time: 106.64530372619629 and batch: 750, loss is 3.4657069635391236 and perplexity is 31.99907395706027
At time: 107.26040244102478 and batch: 800, loss is 3.430776424407959 and perplexity is 30.900625437911174
At time: 107.87404179573059 and batch: 850, loss is 3.4753656578063965 and perplexity is 32.309640649208724
At time: 108.48762893676758 and batch: 900, loss is 3.43987184047699 and perplexity is 31.1829615187363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.323224629441353 and perplexity of 75.4314750915067
finished 9 epochs...
Completing Train Step...
At time: 110.03470277786255 and batch: 50, loss is 3.7303856801986695 and perplexity is 41.695186071012245
At time: 110.66284489631653 and batch: 100, loss is 3.624206943511963 and perplexity is 37.49497573169194
At time: 111.27800369262695 and batch: 150, loss is 3.635757746696472 and perplexity is 37.93058378495806
At time: 111.894366979599 and batch: 200, loss is 3.515705509185791 and perplexity is 33.63965263787973
At time: 112.50870084762573 and batch: 250, loss is 3.6621602249145506 and perplexity is 38.94538285236489
At time: 113.1256103515625 and batch: 300, loss is 3.6199692821502687 and perplexity is 37.33642090987139
At time: 113.73965692520142 and batch: 350, loss is 3.6111857795715334 and perplexity is 37.00991240385374
At time: 114.3535509109497 and batch: 400, loss is 3.5444016885757446 and perplexity is 34.61896622164932
At time: 114.97078394889832 and batch: 450, loss is 3.568041625022888 and perplexity is 35.44710640708709
At time: 115.58976364135742 and batch: 500, loss is 3.4616886806488036 and perplexity is 31.87075061807666
At time: 116.22414374351501 and batch: 550, loss is 3.5099709033966064 and perplexity is 33.44729456712172
At time: 116.84713745117188 and batch: 600, loss is 3.517871923446655 and perplexity is 33.71260905946685
At time: 117.48687672615051 and batch: 650, loss is 3.3735003519058226 and perplexity is 29.180490485232546
At time: 118.10139346122742 and batch: 700, loss is 3.3670408296585084 and perplexity is 28.992605933273783
At time: 118.71716713905334 and batch: 750, loss is 3.4583657026290893 and perplexity is 31.765020580886645
At time: 119.33192849159241 and batch: 800, loss is 3.4260360765457154 and perplexity is 30.754492358634582
At time: 119.94690370559692 and batch: 850, loss is 3.4740510892868044 and perplexity is 32.26719531749283
At time: 120.5632643699646 and batch: 900, loss is 3.441076650619507 and perplexity is 31.220553708226078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324027283550942 and perplexity of 75.49204477998408
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 122.11710357666016 and batch: 50, loss is 3.7245162391662596 and perplexity is 41.45117543856263
At time: 122.75913763046265 and batch: 100, loss is 3.6276936769485473 and perplexity is 37.625938901431034
At time: 123.3772463798523 and batch: 150, loss is 3.6392692947387695 and perplexity is 38.064012986607985
At time: 123.99730038642883 and batch: 200, loss is 3.5146804428100586 and perplexity is 33.60518742865077
At time: 124.61990475654602 and batch: 250, loss is 3.662068123817444 and perplexity is 38.94179610505116
At time: 125.24297189712524 and batch: 300, loss is 3.6173874855041506 and perplexity is 37.24015019277125
At time: 125.88153338432312 and batch: 350, loss is 3.6104194116592407 and perplexity is 36.98156006010157
At time: 126.50797510147095 and batch: 400, loss is 3.543637418746948 and perplexity is 34.59251809830073
At time: 127.13800358772278 and batch: 450, loss is 3.5601981592178347 and perplexity is 35.170165747173016
At time: 127.75694370269775 and batch: 500, loss is 3.452214865684509 and perplexity is 31.570238768431114
At time: 128.3766062259674 and batch: 550, loss is 3.498227949142456 and perplexity is 33.05682165693162
At time: 128.99888348579407 and batch: 600, loss is 3.507851939201355 and perplexity is 33.37649598382802
At time: 129.62066435813904 and batch: 650, loss is 3.3618028450012205 and perplexity is 28.841140142272415
At time: 130.24283719062805 and batch: 700, loss is 3.3494556379318237 and perplexity is 28.487222059074487
At time: 130.86602115631104 and batch: 750, loss is 3.439050078392029 and perplexity is 31.157347069196568
At time: 131.48910784721375 and batch: 800, loss is 3.4062313270568847 and perplexity is 30.15139910955507
At time: 132.11074209213257 and batch: 850, loss is 3.4521334886550905 and perplexity is 31.567669780711817
At time: 132.74562764167786 and batch: 900, loss is 3.4216552448272703 and perplexity is 30.62005678783523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.320219693118578 and perplexity of 75.20514853087914
finished 11 epochs...
Completing Train Step...
At time: 134.34173107147217 and batch: 50, loss is 3.7136126565933227 and perplexity is 41.00166421922462
At time: 134.96793222427368 and batch: 100, loss is 3.612481050491333 and perplexity is 37.0578813268037
At time: 135.59104895591736 and batch: 150, loss is 3.6255254983901977 and perplexity is 37.54444752332077
At time: 136.22343707084656 and batch: 200, loss is 3.5027054738998413 and perplexity is 33.2051662547686
At time: 136.85074996948242 and batch: 250, loss is 3.6506556844711304 and perplexity is 38.499901562455435
At time: 137.48529314994812 and batch: 300, loss is 3.6059409856796263 and perplexity is 36.81631118438079
At time: 138.1064727306366 and batch: 350, loss is 3.598889083862305 and perplexity is 36.55759944963776
At time: 138.72937107086182 and batch: 400, loss is 3.534886751174927 and perplexity is 34.291131064020156
At time: 139.35342526435852 and batch: 450, loss is 3.5523030614852904 and perplexity is 34.89358709572458
At time: 139.97489500045776 and batch: 500, loss is 3.4451472091674806 and perplexity is 31.347897804961264
At time: 140.5991725921631 and batch: 550, loss is 3.492198462486267 and perplexity is 32.85810567156765
At time: 141.21914196014404 and batch: 600, loss is 3.503131380081177 and perplexity is 33.21931155239981
At time: 141.83919382095337 and batch: 650, loss is 3.3583346223831176 and perplexity is 28.741285906151163
At time: 142.45790934562683 and batch: 700, loss is 3.3475395488739013 and perplexity is 28.432690265167285
At time: 143.07654476165771 and batch: 750, loss is 3.4381715440750122 and perplexity is 31.12998629102017
At time: 143.69600820541382 and batch: 800, loss is 3.406626191139221 and perplexity is 30.163307164972583
At time: 144.32837963104248 and batch: 850, loss is 3.4543683528900146 and perplexity is 31.638298129873583
At time: 144.94815182685852 and batch: 900, loss is 3.425470314025879 and perplexity is 30.737097540673478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319662433781036 and perplexity of 75.16325143448736
finished 12 epochs...
Completing Train Step...
At time: 146.50716280937195 and batch: 50, loss is 3.7077866840362548 and perplexity is 40.763484137689744
At time: 147.12148451805115 and batch: 100, loss is 3.606349949836731 and perplexity is 36.83137081526674
At time: 147.73289465904236 and batch: 150, loss is 3.6189729499816896 and perplexity is 37.299239958024195
At time: 148.3450071811676 and batch: 200, loss is 3.4960847091674805 and perplexity is 32.98604882398431
At time: 148.96984958648682 and batch: 250, loss is 3.6443006706237795 and perplexity is 38.25600994303449
At time: 149.58237195014954 and batch: 300, loss is 3.599767150878906 and perplexity is 36.58971356903178
At time: 150.19918513298035 and batch: 350, loss is 3.5928858280181886 and perplexity is 36.33879226199261
At time: 150.81778144836426 and batch: 400, loss is 3.529665780067444 and perplexity is 34.11256461026146
At time: 151.43731927871704 and batch: 450, loss is 3.547347140312195 and perplexity is 34.72108503500985
At time: 152.05457258224487 and batch: 500, loss is 3.440651326179504 and perplexity is 31.207277667216044
At time: 152.67075943946838 and batch: 550, loss is 3.4882285451889037 and perplexity is 32.727920293075854
At time: 153.2938997745514 and batch: 600, loss is 3.499791965484619 and perplexity is 33.10856351823399
At time: 153.90926694869995 and batch: 650, loss is 3.355755105018616 and perplexity is 28.66724279887495
At time: 154.5256748199463 and batch: 700, loss is 3.3457249641418456 and perplexity is 28.38114352163182
At time: 155.14734745025635 and batch: 750, loss is 3.4369108390808107 and perplexity is 31.090765290084285
At time: 155.76507353782654 and batch: 800, loss is 3.4060758543014527 and perplexity is 30.146711752842947
At time: 156.38176846504211 and batch: 850, loss is 3.454596734046936 and perplexity is 31.645524546160754
At time: 156.995596408844 and batch: 900, loss is 3.4262616872787475 and perplexity is 30.76143168496346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319832161681293 and perplexity of 75.17600981802603
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 158.55218863487244 and batch: 50, loss is 3.7061480188369753 and perplexity is 40.696741134485244
At time: 159.1821150779724 and batch: 100, loss is 3.6075903940200806 and perplexity is 36.87708642292725
At time: 159.7973828315735 and batch: 150, loss is 3.6205922603607177 and perplexity is 37.35968793322618
At time: 160.41246724128723 and batch: 200, loss is 3.496372971534729 and perplexity is 32.99555883112723
At time: 161.03206038475037 and batch: 250, loss is 3.6434655666351317 and perplexity is 38.224075532673965
At time: 161.65588212013245 and batch: 300, loss is 3.5990252542495726 and perplexity is 36.562577851060134
At time: 162.27174067497253 and batch: 350, loss is 3.5935793352127074 and perplexity is 36.36400221649844
At time: 162.887629032135 and batch: 400, loss is 3.5311747074127195 and perplexity is 34.16407684615619
At time: 163.50364899635315 and batch: 450, loss is 3.545000100135803 and perplexity is 34.639688810933734
At time: 164.1209638118744 and batch: 500, loss is 3.4379033756256105 and perplexity is 31.121639330112416
At time: 164.75149297714233 and batch: 550, loss is 3.484681444168091 and perplexity is 32.61203670050317
At time: 165.37083649635315 and batch: 600, loss is 3.495860366821289 and perplexity is 32.97864948642295
At time: 165.99112391471863 and batch: 650, loss is 3.3508493518829345 and perplexity is 28.52695277808409
At time: 166.61360669136047 and batch: 700, loss is 3.3393740892410277 and perplexity is 28.201469576864714
At time: 167.235693693161 and batch: 750, loss is 3.42973051071167 and perplexity is 30.86832294629571
At time: 167.85247898101807 and batch: 800, loss is 3.3980346632003786 and perplexity is 29.9052683299127
At time: 168.4694299697876 and batch: 850, loss is 3.445184063911438 and perplexity is 31.349053144998223
At time: 169.08404970169067 and batch: 900, loss is 3.417527227401733 and perplexity is 30.493917192192267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.319372307764341 and perplexity of 75.14144778280304
finished 14 epochs...
Completing Train Step...
At time: 170.6404435634613 and batch: 50, loss is 3.7033416175842286 and perplexity is 40.58268986085589
At time: 171.2686173915863 and batch: 100, loss is 3.6033483123779297 and perplexity is 36.720982149228035
At time: 171.88307762145996 and batch: 150, loss is 3.6169961738586425 and perplexity is 37.225580539143834
At time: 172.49867725372314 and batch: 200, loss is 3.493562660217285 and perplexity is 32.90296121372829
At time: 173.11827683448792 and batch: 250, loss is 3.6404220628738404 and perplexity is 38.10791726873865
At time: 173.7426860332489 and batch: 300, loss is 3.5960751581192016 and perplexity is 36.45487367856962
At time: 174.35781025886536 and batch: 350, loss is 3.5902801942825318 and perplexity is 36.244229929864304
At time: 174.97329783439636 and batch: 400, loss is 3.5280273246765135 and perplexity is 34.05671845808528
At time: 175.58924198150635 and batch: 450, loss is 3.542692823410034 and perplexity is 34.55985759492103
At time: 176.20637607574463 and batch: 500, loss is 3.4360367345809935 and perplexity is 31.063600586365954
At time: 176.8242905139923 and batch: 550, loss is 3.4828629875183106 and perplexity is 32.55278701332324
At time: 177.45086121559143 and batch: 600, loss is 3.4948176908493043 and perplexity is 32.94428136153765
At time: 178.06631350517273 and batch: 650, loss is 3.3501550006866454 and perplexity is 28.507151929463145
At time: 178.6835765838623 and batch: 700, loss is 3.3388106393814088 and perplexity is 28.185583938591183
At time: 179.30030608177185 and batch: 750, loss is 3.4295819282531737 and perplexity is 30.863736795701893
At time: 179.9320192337036 and batch: 800, loss is 3.3983403205871583 and perplexity is 29.91441049319506
At time: 180.54915356636047 and batch: 850, loss is 3.446274871826172 and perplexity is 31.383267597593502
At time: 181.1648609638214 and batch: 900, loss is 3.4195296478271486 and perplexity is 30.555040011195842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318991047062286 and perplexity of 75.11280476224401
finished 15 epochs...
Completing Train Step...
At time: 182.7775273323059 and batch: 50, loss is 3.7014851140975953 and perplexity is 40.507417848628236
At time: 183.41553354263306 and batch: 100, loss is 3.601222915649414 and perplexity is 36.643018375224166
At time: 184.0527627468109 and batch: 150, loss is 3.6148344707489013 and perplexity is 37.14519680012226
At time: 184.68352031707764 and batch: 200, loss is 3.491634340286255 and perplexity is 32.83957491200582
At time: 185.29973340034485 and batch: 250, loss is 3.638425250053406 and perplexity is 38.03189881354812
At time: 185.9226324558258 and batch: 300, loss is 3.594129376411438 and perplexity is 36.3840094177563
At time: 186.53888773918152 and batch: 350, loss is 3.5883048295974733 and perplexity is 36.17270502516703
At time: 187.15379357337952 and batch: 400, loss is 3.526238865852356 and perplexity is 33.99586385363494
At time: 187.76701402664185 and batch: 450, loss is 3.54112841129303 and perplexity is 34.50583400353612
At time: 188.38171124458313 and batch: 500, loss is 3.43471230506897 and perplexity is 31.02248626951689
At time: 188.99472403526306 and batch: 550, loss is 3.4817176818847657 and perplexity is 32.51552546497299
At time: 189.60765099525452 and batch: 600, loss is 3.4940123319625855 and perplexity is 32.91776007278784
At time: 190.2207248210907 and batch: 650, loss is 3.349621458053589 and perplexity is 28.491946205373505
At time: 190.83478569984436 and batch: 700, loss is 3.3384752321243285 and perplexity is 28.176131874427202
At time: 191.47076749801636 and batch: 750, loss is 3.4294707012176513 and perplexity is 30.86030410466053
At time: 192.09698820114136 and batch: 800, loss is 3.398484649658203 and perplexity is 29.918728323859145
At time: 192.71906566619873 and batch: 850, loss is 3.4468306589126585 and perplexity is 31.40071486048858
At time: 193.3423297405243 and batch: 900, loss is 3.420474200248718 and perplexity is 30.58391448280884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.31884305771083 and perplexity of 75.10168968945769
finished 16 epochs...
Completing Train Step...
At time: 195.00038409233093 and batch: 50, loss is 3.699913821220398 and perplexity is 40.44381881093179
At time: 195.631694316864 and batch: 100, loss is 3.599564480781555 and perplexity is 36.58229867963425
At time: 196.26954889297485 and batch: 150, loss is 3.613096580505371 and perplexity is 37.080698586650726
At time: 196.88704633712769 and batch: 200, loss is 3.489985942840576 and perplexity is 32.78548683217742
At time: 197.5045928955078 and batch: 250, loss is 3.636766972541809 and perplexity is 37.9688836337904
At time: 198.12187433242798 and batch: 300, loss is 3.5925513744354247 and perplexity is 36.326640654914804
At time: 198.7396879196167 and batch: 350, loss is 3.5867324018478395 and perplexity is 36.1158707556078
At time: 199.35409927368164 and batch: 400, loss is 3.524846477508545 and perplexity is 33.948561348438076
At time: 199.97113347053528 and batch: 450, loss is 3.539844174385071 and perplexity is 34.461548780414375
At time: 200.58870649337769 and batch: 500, loss is 3.4335859966278077 and perplexity is 30.98756505108891
At time: 201.20614433288574 and batch: 550, loss is 3.4807571840286253 and perplexity is 32.48430936637038
At time: 201.82337069511414 and batch: 600, loss is 3.4932605409622193 and perplexity is 32.8930220970718
At time: 202.44135355949402 and batch: 650, loss is 3.349091567993164 and perplexity is 28.476852605604538
At time: 203.0589394569397 and batch: 700, loss is 3.338122386932373 and perplexity is 28.166191815521533
At time: 203.67702841758728 and batch: 750, loss is 3.4292800331115725 and perplexity is 30.854420589841013
At time: 204.294739484787 and batch: 800, loss is 3.398488073348999 and perplexity is 29.918830756509276
At time: 204.91159009933472 and batch: 850, loss is 3.447099556922913 and perplexity is 31.40915958556913
At time: 205.52902150154114 and batch: 900, loss is 3.4209543895721435 and perplexity is 30.598604078623318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318811704034674 and perplexity of 75.0993350123145
finished 17 epochs...
Completing Train Step...
At time: 207.11083030700684 and batch: 50, loss is 3.6985000801086425 and perplexity is 40.386682119323766
At time: 207.74430751800537 and batch: 100, loss is 3.5981081104278565 and perplexity is 36.52906008133519
At time: 208.36488366127014 and batch: 150, loss is 3.611566300392151 and perplexity is 37.02399812587793
At time: 208.98602080345154 and batch: 200, loss is 3.4884923315048217 and perplexity is 32.7365546093548
At time: 209.6066288948059 and batch: 250, loss is 3.6352881383895874 and perplexity is 37.91277544952606
At time: 210.22881698608398 and batch: 300, loss is 3.591154432296753 and perplexity is 36.27592986809169
At time: 210.85269951820374 and batch: 350, loss is 3.585356454849243 and perplexity is 36.066211403792686
At time: 211.4738907814026 and batch: 400, loss is 3.523627882003784 and perplexity is 33.90721698033262
At time: 212.10795521736145 and batch: 450, loss is 3.5386957502365113 and perplexity is 34.421995022190956
At time: 212.74276185035706 and batch: 500, loss is 3.432559356689453 and perplexity is 30.955768303913167
At time: 213.37069940567017 and batch: 550, loss is 3.4798681640625 and perplexity is 32.455443000047126
At time: 213.9914321899414 and batch: 600, loss is 3.4925302362442014 and perplexity is 32.86900893737363
At time: 214.6136758327484 and batch: 650, loss is 3.348549518585205 and perplexity is 28.461420927253037
At time: 215.23499393463135 and batch: 700, loss is 3.3377287197113037 and perplexity is 28.15510589128707
At time: 215.85488152503967 and batch: 750, loss is 3.4290211629867553 and perplexity is 30.84643433587764
At time: 216.4762225151062 and batch: 800, loss is 3.3983916568756105 and perplexity is 29.91594622742013
At time: 217.09743785858154 and batch: 850, loss is 3.447196807861328 and perplexity is 31.412214304348385
At time: 217.71839332580566 and batch: 900, loss is 3.421191062927246 and perplexity is 30.60584680995901
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318849328446062 and perplexity of 75.10216063374575
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 219.29299068450928 and batch: 50, loss is 3.6980166912078856 and perplexity is 40.36716436316228
At time: 219.93809270858765 and batch: 100, loss is 3.5983046197891237 and perplexity is 36.53623908894746
At time: 220.5609188079834 and batch: 150, loss is 3.6119773769378662 and perplexity is 37.039220951795066
At time: 221.18386721611023 and batch: 200, loss is 3.4888136100769045 and perplexity is 32.74707385258784
At time: 221.80617761611938 and batch: 250, loss is 3.634849228858948 and perplexity is 37.89613882230246
At time: 222.42885375022888 and batch: 300, loss is 3.590782308578491 and perplexity is 36.262433245548195
At time: 223.05113530158997 and batch: 350, loss is 3.58548508644104 and perplexity is 36.07085095636575
At time: 223.67338156700134 and batch: 400, loss is 3.523756308555603 and perplexity is 33.91157184692544
At time: 224.2961835861206 and batch: 450, loss is 3.5378388929367066 and perplexity is 34.39251291726459
At time: 224.93044757843018 and batch: 500, loss is 3.431422333717346 and perplexity is 30.920590886787927
At time: 225.56202387809753 and batch: 550, loss is 3.478534336090088 and perplexity is 32.41218188017635
At time: 226.18565034866333 and batch: 600, loss is 3.4909452629089355 and perplexity is 32.81695369863146
At time: 226.80923295021057 and batch: 650, loss is 3.3467683887481687 and perplexity is 28.410772560283693
At time: 227.4464888572693 and batch: 700, loss is 3.335868616104126 and perplexity is 28.10278315519961
At time: 228.07351756095886 and batch: 750, loss is 3.4267836570739747 and perplexity is 30.777492414404136
At time: 228.70785570144653 and batch: 800, loss is 3.395719361305237 and perplexity is 29.836108699080793
At time: 229.33275270462036 and batch: 850, loss is 3.4438953971862794 and perplexity is 31.308680682256785
At time: 229.9613242149353 and batch: 900, loss is 3.4176414346694948 and perplexity is 30.497400018036362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.318611458556293 and perplexity of 75.08429821562437
finished 19 epochs...
Completing Train Step...
At time: 231.53364396095276 and batch: 50, loss is 3.6973040819168093 and perplexity is 40.338408593813774
At time: 232.16282415390015 and batch: 100, loss is 3.597474665641785 and perplexity is 36.50592826582401
At time: 232.77914309501648 and batch: 150, loss is 3.6113099431991578 and perplexity is 37.0145079741313
At time: 233.3939929008484 and batch: 200, loss is 3.4881676626205445 and perplexity is 32.72592779388621
At time: 234.00914573669434 and batch: 250, loss is 3.6343412446975707 and perplexity is 37.876893072685235
At time: 234.6237006187439 and batch: 300, loss is 3.59026300907135 and perplexity is 36.243607070470844
At time: 235.23832893371582 and batch: 350, loss is 3.5847621536254883 and perplexity is 36.0447835781414
At time: 235.87264132499695 and batch: 400, loss is 3.5231651401519777 and perplexity is 33.89153032167419
At time: 236.49391865730286 and batch: 450, loss is 3.53744713306427 and perplexity is 34.379041949655914
At time: 237.11274361610413 and batch: 500, loss is 3.4311371755599978 and perplexity is 30.911774885103664
At time: 237.72932958602905 and batch: 550, loss is 3.4782002449035643 and perplexity is 32.40135506454672
At time: 238.34698152542114 and batch: 600, loss is 3.4907472038269045 and perplexity is 32.810454646526246
At time: 238.96546030044556 and batch: 650, loss is 3.3466301488876344 and perplexity is 28.40684533050339
At time: 239.58396220207214 and batch: 700, loss is 3.3357313680648804 and perplexity is 28.09892636798851
At time: 240.20085215568542 and batch: 750, loss is 3.426762590408325 and perplexity is 30.77684404209145
At time: 240.81602787971497 and batch: 800, loss is 3.3958851432800294 and perplexity is 29.841055398126485
At time: 241.4313349723816 and batch: 850, loss is 3.444173550605774 and perplexity is 31.31739051012647
At time: 242.04681968688965 and batch: 900, loss is 3.4180896043777467 and perplexity is 30.511071092156712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3184446569991435 and perplexity of 75.07177508223263
Finished Training.
Improved accuracyfrom -10000000 to -75.07177508223263
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
253.84554409980774


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8588967323303223 and batch: 50, loss is 6.931471357345581 and perplexity is 1023.9995409881377
At time: 1.5018644332885742 and batch: 100, loss is 6.14966926574707 and perplexity is 468.562391520198
At time: 2.124302387237549 and batch: 150, loss is 6.053754615783691 and perplexity is 425.7084046566042
At time: 2.7653887271881104 and batch: 200, loss is 5.923343696594238 and perplexity is 373.6590298164408
At time: 3.3881452083587646 and batch: 250, loss is 5.972703142166138 and perplexity is 392.5653978880467
At time: 4.012021064758301 and batch: 300, loss is 5.892273178100586 and perplexity is 362.2277574098395
At time: 4.63539981842041 and batch: 350, loss is 5.8824114322662355 and perplexity is 358.6731156275681
At time: 5.260408639907837 and batch: 400, loss is 5.761054468154907 and perplexity is 317.6831391170309
At time: 5.88261079788208 and batch: 450, loss is 5.765294342041016 and perplexity is 319.032935022783
At time: 6.5059285163879395 and batch: 500, loss is 5.7201580238342284 and perplexity is 304.95310900908146
At time: 7.129378080368042 and batch: 550, loss is 5.772134132385254 and perplexity is 321.22253307076414
At time: 7.753134250640869 and batch: 600, loss is 5.701664657592773 and perplexity is 299.36532717865384
At time: 8.374765872955322 and batch: 650, loss is 5.620885190963745 and perplexity is 276.1337061440771
At time: 9.000478744506836 and batch: 700, loss is 5.720947885513306 and perplexity is 305.1940749361513
At time: 9.635786056518555 and batch: 750, loss is 5.6762220287323 and perplexity is 291.8447634065077
At time: 10.26796841621399 and batch: 800, loss is 5.681470041275024 and perplexity is 293.38039436618135
At time: 10.903721809387207 and batch: 850, loss is 5.709551773071289 and perplexity is 301.7357918605196
At time: 11.529785633087158 and batch: 900, loss is 5.6010995197296145 and perplexity is 270.7239101215961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.480043176102312 and perplexity of 239.8570632437934
finished 1 epochs...
Completing Train Step...
At time: 13.167700052261353 and batch: 50, loss is 5.274298906326294 and perplexity is 195.25353749471424
At time: 13.782970666885376 and batch: 100, loss is 5.072928628921509 and perplexity is 159.64117315731997
At time: 14.399254083633423 and batch: 150, loss is 5.021332578659058 and perplexity is 151.6132057614778
At time: 15.017172813415527 and batch: 200, loss is 4.88303126335144 and perplexity is 132.03027645266886
At time: 15.635087490081787 and batch: 250, loss is 4.949208335876465 and perplexity is 141.06324499516663
At time: 16.251648664474487 and batch: 300, loss is 4.865408306121826 and perplexity is 129.72389486031167
At time: 16.865031480789185 and batch: 350, loss is 4.849208669662476 and perplexity is 127.63934498542122
At time: 17.489835500717163 and batch: 400, loss is 4.712538604736328 and perplexity is 111.33443558641113
At time: 18.133222818374634 and batch: 450, loss is 4.724642105102539 and perplexity is 112.6901599228064
At time: 18.74976944923401 and batch: 500, loss is 4.628243818283081 and perplexity is 102.3341887784908
At time: 19.36522603034973 and batch: 550, loss is 4.690935544967651 and perplexity is 108.95506450342377
At time: 19.991406679153442 and batch: 600, loss is 4.633925056457519 and perplexity is 102.91722830332837
At time: 20.607731342315674 and batch: 650, loss is 4.491373043060303 and perplexity is 89.24389750788876
At time: 21.223471641540527 and batch: 700, loss is 4.539083862304688 and perplexity is 93.6050057515085
At time: 21.83989119529724 and batch: 750, loss is 4.576915426254272 and perplexity is 97.21406729523011
At time: 22.45703673362732 and batch: 800, loss is 4.528867740631103 and perplexity is 92.6535937686073
At time: 23.077086448669434 and batch: 850, loss is 4.578709182739257 and perplexity is 97.38860214858717
At time: 23.700371265411377 and batch: 900, loss is 4.517576065063476 and perplexity is 91.61326403416771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.609884601749786 and perplexity of 100.47255461038282
finished 2 epochs...
Completing Train Step...
At time: 25.28771662712097 and batch: 50, loss is 4.549945220947266 and perplexity is 94.6272245832199
At time: 25.903838396072388 and batch: 100, loss is 4.413438506126404 and perplexity is 82.55283446567154
At time: 26.519413471221924 and batch: 150, loss is 4.413952255249024 and perplexity is 82.5952568082356
At time: 27.13566303253174 and batch: 200, loss is 4.302505259513855 and perplexity is 73.88466228412969
At time: 27.751821756362915 and batch: 250, loss is 4.436530265808106 and perplexity is 84.48130487549847
At time: 28.36792802810669 and batch: 300, loss is 4.389004850387574 and perplexity is 80.56020961556132
At time: 28.984559535980225 and batch: 350, loss is 4.39318021774292 and perplexity is 80.89728129427164
At time: 29.598437547683716 and batch: 400, loss is 4.300020680427552 and perplexity is 73.70131785860005
At time: 30.214285373687744 and batch: 450, loss is 4.33318564414978 and perplexity is 76.18660380472134
At time: 30.830031871795654 and batch: 500, loss is 4.22038314819336 and perplexity is 68.05955619039506
At time: 31.446080684661865 and batch: 550, loss is 4.293173289299011 and perplexity is 73.19837997981068
At time: 32.061556577682495 and batch: 600, loss is 4.277508850097656 and perplexity is 72.06070221210317
At time: 32.677531480789185 and batch: 650, loss is 4.125459394454956 and perplexity is 61.89623750826984
At time: 33.29279708862305 and batch: 700, loss is 4.15381263256073 and perplexity is 63.67631246321847
At time: 33.94639348983765 and batch: 750, loss is 4.236881775856018 and perplexity is 69.19175968634757
At time: 34.56800293922424 and batch: 800, loss is 4.199299311637878 and perplexity is 66.63962107132927
At time: 35.18498253822327 and batch: 850, loss is 4.2628990125656125 and perplexity is 71.01556032707305
At time: 35.81655955314636 and batch: 900, loss is 4.212386970520019 and perplexity is 67.5175099240031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.459962191647047 and perplexity of 86.48423920787437
finished 3 epochs...
Completing Train Step...
At time: 37.4032039642334 and batch: 50, loss is 4.279745349884033 and perplexity is 72.22204631297758
At time: 38.03178095817566 and batch: 100, loss is 4.150887832641602 and perplexity is 63.49034408239973
At time: 38.64656853675842 and batch: 150, loss is 4.1532277011871335 and perplexity is 63.639077081436945
At time: 39.260464906692505 and batch: 200, loss is 4.042057065963745 and perplexity is 56.9433586544955
At time: 39.87540888786316 and batch: 250, loss is 4.194366173744202 and perplexity is 66.31168816556762
At time: 40.4908173084259 and batch: 300, loss is 4.152294998168945 and perplexity is 63.57974839449207
At time: 41.10469198226929 and batch: 350, loss is 4.159900331497193 and perplexity is 64.06513700543312
At time: 41.71942758560181 and batch: 400, loss is 4.082226266860962 and perplexity is 59.27729012849937
At time: 42.33338284492493 and batch: 450, loss is 4.1159672975540165 and perplexity is 61.311492044791194
At time: 42.94785952568054 and batch: 500, loss is 4.002530345916748 and perplexity is 54.73647717309444
At time: 43.56187319755554 and batch: 550, loss is 4.073715572357178 and perplexity is 58.77493992792783
At time: 44.17896580696106 and batch: 600, loss is 4.071012053489685 and perplexity is 58.616255368898905
At time: 44.79258394241333 and batch: 650, loss is 3.9203043127059938 and perplexity is 50.41578460773335
At time: 45.40844535827637 and batch: 700, loss is 3.9402330446243288 and perplexity is 51.43058552552588
At time: 46.02476525306702 and batch: 750, loss is 4.031055212020874 and perplexity is 56.32030977048752
At time: 46.64064621925354 and batch: 800, loss is 4.004865312576294 and perplexity is 54.864434352092104
At time: 47.25566029548645 and batch: 850, loss is 4.0686842918396 and perplexity is 58.479969379767546
At time: 47.870039224624634 and batch: 900, loss is 4.0206758451461795 and perplexity is 55.7387638728759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.398081740287886 and perplexity of 81.29477450752339
finished 4 epochs...
Completing Train Step...
At time: 49.4303023815155 and batch: 50, loss is 4.097852940559387 and perplexity is 60.210872394009364
At time: 50.06166362762451 and batch: 100, loss is 3.979741106033325 and perplexity is 53.503180783579225
At time: 50.68123507499695 and batch: 150, loss is 3.9803211879730225 and perplexity is 53.53422601598692
At time: 51.29935956001282 and batch: 200, loss is 3.8750278615951537 and perplexity is 48.18404075663354
At time: 51.92042684555054 and batch: 250, loss is 4.028044490814209 and perplexity is 56.15100001961582
At time: 52.54066205024719 and batch: 300, loss is 3.992879843711853 and perplexity is 54.21078336399102
At time: 53.16127562522888 and batch: 350, loss is 3.9980291318893433 and perplexity is 54.49065024908818
At time: 53.78154110908508 and batch: 400, loss is 3.9283708906173707 and perplexity is 50.82411215106285
At time: 54.40380263328552 and batch: 450, loss is 3.96056547164917 and perplexity is 52.48699746812528
At time: 55.03299522399902 and batch: 500, loss is 3.8499795627593993 and perplexity is 46.99210283285344
At time: 55.660871267318726 and batch: 550, loss is 3.919616584777832 and perplexity is 50.38112418447507
At time: 56.283021450042725 and batch: 600, loss is 3.9207555150985716 and perplexity is 50.438537463057415
At time: 56.90537738800049 and batch: 650, loss is 3.7753116846084596 and perplexity is 43.61109910594273
At time: 57.545183181762695 and batch: 700, loss is 3.7934354639053343 and perplexity is 44.40870300718446
At time: 58.16973853111267 and batch: 750, loss is 3.883342456817627 and perplexity is 48.586341718867686
At time: 58.79522109031677 and batch: 800, loss is 3.863675284385681 and perplexity is 47.64012100092209
At time: 59.424190282821655 and batch: 850, loss is 3.9248849821090697 and perplexity is 50.647252383666036
At time: 60.05326962471008 and batch: 900, loss is 3.8826271295547485 and perplexity is 48.55159901171692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382082586419092 and perplexity of 80.0044762648431
finished 5 epochs...
Completing Train Step...
At time: 61.64673066139221 and batch: 50, loss is 3.9625365257263185 and perplexity is 52.59055420291733
At time: 62.26967525482178 and batch: 100, loss is 3.8515279626846315 and perplexity is 47.064921763230636
At time: 62.89286541938782 and batch: 150, loss is 3.853004102706909 and perplexity is 47.134447480112684
At time: 63.51541519165039 and batch: 200, loss is 3.751298017501831 and perplexity is 42.57631094500985
At time: 64.14584255218506 and batch: 250, loss is 3.9026249980926515 and perplexity is 49.53230079586107
At time: 64.77844977378845 and batch: 300, loss is 3.8738055658340453 and perplexity is 48.125181586848505
At time: 65.40534019470215 and batch: 350, loss is 3.875429801940918 and perplexity is 48.20341175936857
At time: 66.04952716827393 and batch: 400, loss is 3.8096412658691405 and perplexity is 45.134244767735694
At time: 66.67238068580627 and batch: 450, loss is 3.8407133197784424 and perplexity is 46.558673820388215
At time: 67.29593324661255 and batch: 500, loss is 3.7365112113952637 and perplexity is 41.95137507985832
At time: 67.91734409332275 and batch: 550, loss is 3.8038096904754637 and perplexity is 44.87180697316094
At time: 68.54051399230957 and batch: 600, loss is 3.8075237178802492 and perplexity is 45.038771958300536
At time: 69.16395592689514 and batch: 650, loss is 3.6672499275207517 and perplexity is 39.144108567362686
At time: 69.78766942024231 and batch: 700, loss is 3.677310042381287 and perplexity is 39.53989026237106
At time: 70.41068077087402 and batch: 750, loss is 3.7706320762634276 and perplexity is 43.40749301257686
At time: 71.03538918495178 and batch: 800, loss is 3.756481170654297 and perplexity is 42.79756338260949
At time: 71.67454171180725 and batch: 850, loss is 3.8159653997421263 and perplexity is 45.420584244156885
At time: 72.30696773529053 and batch: 900, loss is 3.772263250350952 and perplexity is 43.47835596958073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376513337435788 and perplexity of 79.56014984742443
finished 6 epochs...
Completing Train Step...
At time: 73.87867879867554 and batch: 50, loss is 3.8572482681274414 and perplexity is 47.33491898842201
At time: 74.49156165122986 and batch: 100, loss is 3.7504323959350585 and perplexity is 42.5394719186504
At time: 75.10520339012146 and batch: 150, loss is 3.7533889055252074 and perplexity is 42.66542637637207
At time: 75.71804547309875 and batch: 200, loss is 3.654399175643921 and perplexity is 38.644295704581054
At time: 76.33118200302124 and batch: 250, loss is 3.804757022857666 and perplexity is 44.91433563016095
At time: 76.95184898376465 and batch: 300, loss is 3.7786726140975953 and perplexity is 43.75791952331163
At time: 77.57004642486572 and batch: 350, loss is 3.7792630052566527 and perplexity is 43.78376143980583
At time: 78.19284868240356 and batch: 400, loss is 3.7145838499069215 and perplexity is 41.041504104344305
At time: 78.80944657325745 and batch: 450, loss is 3.7464004039764403 and perplexity is 42.36829842691827
At time: 79.42571973800659 and batch: 500, loss is 3.643996276855469 and perplexity is 38.24436682414372
At time: 80.04228162765503 and batch: 550, loss is 3.7148912572860717 and perplexity is 41.054122504952765
At time: 80.65803170204163 and batch: 600, loss is 3.717973303794861 and perplexity is 41.180848407138555
At time: 81.28679990768433 and batch: 650, loss is 3.578679928779602 and perplexity is 35.826216460869134
At time: 81.90082812309265 and batch: 700, loss is 3.5891437005996703 and perplexity is 36.20306198949202
At time: 82.5151138305664 and batch: 750, loss is 3.6825997161865236 and perplexity is 39.74959753671322
At time: 83.12985134124756 and batch: 800, loss is 3.6675770139694213 and perplexity is 39.156914168975526
At time: 83.7451581954956 and batch: 850, loss is 3.729909648895264 and perplexity is 41.675342580677125
At time: 84.36265325546265 and batch: 900, loss is 3.686183867454529 and perplexity is 39.892321726882834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38490859776327 and perplexity of 80.23088959498783
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 85.91914916038513 and batch: 50, loss is 3.80870361328125 and perplexity is 45.09194436096887
At time: 86.55050897598267 and batch: 100, loss is 3.704302053451538 and perplexity is 40.62168565527237
At time: 87.17939019203186 and batch: 150, loss is 3.7040709495544433 and perplexity is 40.61229891010935
At time: 87.80182337760925 and batch: 200, loss is 3.59959219455719 and perplexity is 36.58331252730077
At time: 88.42106890678406 and batch: 250, loss is 3.7371806144714355 and perplexity is 41.97946686069968
At time: 89.0374391078949 and batch: 300, loss is 3.6946710300445558 and perplexity is 40.232335181224485
At time: 89.6584689617157 and batch: 350, loss is 3.6817023181915283 and perplexity is 39.713942328430946
At time: 90.28107714653015 and batch: 400, loss is 3.6070257091522215 and perplexity is 36.85626836862679
At time: 90.90356135368347 and batch: 450, loss is 3.6258673763275144 and perplexity is 37.55728533595515
At time: 91.52587628364563 and batch: 500, loss is 3.5174123573303224 and perplexity is 33.69711944617328
At time: 92.14935851097107 and batch: 550, loss is 3.5734992933273317 and perplexity is 35.64109383421433
At time: 92.77117848396301 and batch: 600, loss is 3.571132788658142 and perplexity is 35.556848741759744
At time: 93.39112973213196 and batch: 650, loss is 3.4237577295303345 and perplexity is 30.68450271338498
At time: 94.02497863769531 and batch: 700, loss is 3.408019027709961 and perplexity is 30.205348994188295
At time: 94.66203308105469 and batch: 750, loss is 3.489081916809082 and perplexity is 32.755861291773435
At time: 95.28843140602112 and batch: 800, loss is 3.4535985374450684 and perplexity is 31.613951851594248
At time: 95.91715240478516 and batch: 850, loss is 3.5009525489807127 and perplexity is 33.14701107711819
At time: 96.54127717018127 and batch: 900, loss is 3.4576887559890745 and perplexity is 31.74352463355996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.345273370612158 and perplexity of 77.11311505764431
finished 8 epochs...
Completing Train Step...
At time: 98.1124062538147 and batch: 50, loss is 3.7177095556259157 and perplexity is 41.16998846598331
At time: 98.74282193183899 and batch: 100, loss is 3.6107846689224243 and perplexity is 36.995070310725765
At time: 99.35995197296143 and batch: 150, loss is 3.6085437297821046 and perplexity is 36.912259431392975
At time: 99.97672271728516 and batch: 200, loss is 3.506302218437195 and perplexity is 33.32481179334244
At time: 100.59195446968079 and batch: 250, loss is 3.6481462335586547 and perplexity is 38.40340907157115
At time: 101.20482587814331 and batch: 300, loss is 3.612456283569336 and perplexity is 37.056963528513066
At time: 101.8194363117218 and batch: 350, loss is 3.6021511602401732 and perplexity is 36.67704785021468
At time: 102.43585920333862 and batch: 400, loss is 3.5332043409347533 and perplexity is 34.23348781736176
At time: 103.05058550834656 and batch: 450, loss is 3.555074725151062 and perplexity is 34.990434535511774
At time: 103.66628074645996 and batch: 500, loss is 3.452569646835327 and perplexity is 31.581441281175653
At time: 104.2815375328064 and batch: 550, loss is 3.5115045356750487 and perplexity is 33.49862977229845
At time: 104.89641737937927 and batch: 600, loss is 3.516811604499817 and perplexity is 33.67688188578061
At time: 105.51223039627075 and batch: 650, loss is 3.3752326726913453 and perplexity is 29.23108426510772
At time: 106.12774014472961 and batch: 700, loss is 3.364233202934265 and perplexity is 28.91131968217223
At time: 106.74301218986511 and batch: 750, loss is 3.4500893783569335 and perplexity is 31.50320788787929
At time: 107.36087560653687 and batch: 800, loss is 3.421925911903381 and perplexity is 30.628345750800424
At time: 107.97910404205322 and batch: 850, loss is 3.4766813945770263 and perplexity is 32.35217961035758
At time: 108.59487557411194 and batch: 900, loss is 3.441377100944519 and perplexity is 31.22993534302181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353419891775471 and perplexity of 77.74388448120322
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 110.2302131652832 and batch: 50, loss is 3.6972292375564577 and perplexity is 40.335389604403545
At time: 110.84521627426147 and batch: 100, loss is 3.604020652770996 and perplexity is 36.745679450366254
At time: 111.46146655082703 and batch: 150, loss is 3.60681321144104 and perplexity is 36.84843732802434
At time: 112.07637572288513 and batch: 200, loss is 3.498199758529663 and perplexity is 33.0558897780073
At time: 112.70355272293091 and batch: 250, loss is 3.6375815868377686 and perplexity is 37.99982623062986
At time: 113.31852006912231 and batch: 300, loss is 3.597921600341797 and perplexity is 36.52224767850642
At time: 113.9433012008667 and batch: 350, loss is 3.5798168897628786 and perplexity is 35.866972635863576
At time: 114.56351709365845 and batch: 400, loss is 3.5139867067337036 and perplexity is 33.581882382489155
At time: 115.17953109741211 and batch: 450, loss is 3.525194354057312 and perplexity is 33.960373311228885
At time: 115.8002257347107 and batch: 500, loss is 3.4212378215789796 and perplexity is 30.607277931549408
At time: 116.41673135757446 and batch: 550, loss is 3.4711236715316773 and perplexity is 32.17287388349198
At time: 117.03138661384583 and batch: 600, loss is 3.4854722785949708 and perplexity is 32.63783762264063
At time: 117.64649033546448 and batch: 650, loss is 3.3317674875259398 and perplexity is 27.98776603927428
At time: 118.26364517211914 and batch: 700, loss is 3.3161667871475218 and perplexity is 27.554525501814048
At time: 118.87858176231384 and batch: 750, loss is 3.40005512714386 and perplexity is 29.965751928182275
At time: 119.49459719657898 and batch: 800, loss is 3.3627985239028932 and perplexity is 28.869870957972186
At time: 120.11073732376099 and batch: 850, loss is 3.4139093208312987 and perplexity is 30.383792379586787
At time: 120.7265076637268 and batch: 900, loss is 3.383994812965393 and perplexity is 29.48833652009454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.339569091796875 and perplexity of 76.67449255093042
finished 10 epochs...
Completing Train Step...
At time: 122.29506087303162 and batch: 50, loss is 3.6719213104248047 and perplexity is 39.32739345055295
At time: 122.90962553024292 and batch: 100, loss is 3.569099678993225 and perplexity is 35.48463120688909
At time: 123.52586221694946 and batch: 150, loss is 3.5705268144607545 and perplexity is 35.53530873588485
At time: 124.14326357841492 and batch: 200, loss is 3.4636003732681275 and perplexity is 31.93173597085702
At time: 124.76093101501465 and batch: 250, loss is 3.604245414733887 and perplexity is 36.75393940963483
At time: 125.37779426574707 and batch: 300, loss is 3.567451014518738 and perplexity is 35.426177154827826
At time: 125.99516129493713 and batch: 350, loss is 3.5509551477432253 and perplexity is 34.846585234511984
At time: 126.6121780872345 and batch: 400, loss is 3.4866809034347535 and perplexity is 32.677308371767246
At time: 127.2287073135376 and batch: 450, loss is 3.5002809047698973 and perplexity is 33.124755553755755
At time: 127.84631681442261 and batch: 500, loss is 3.398753547668457 and perplexity is 29.926774492125592
At time: 128.4754283428192 and batch: 550, loss is 3.451474595069885 and perplexity is 31.54687689649469
At time: 129.0948417186737 and batch: 600, loss is 3.4680709886550902 and perplexity is 32.07481005733239
At time: 129.71246695518494 and batch: 650, loss is 3.3173866415023805 and perplexity is 27.588158519264287
At time: 130.33456993103027 and batch: 700, loss is 3.3052203369140627 and perplexity is 27.254546109852008
At time: 130.95983910560608 and batch: 750, loss is 3.3916860723495486 and perplexity is 29.71601340370165
At time: 131.5864715576172 and batch: 800, loss is 3.357794237136841 and perplexity is 28.725758734993413
At time: 132.2186131477356 and batch: 850, loss is 3.412543716430664 and perplexity is 30.342328457106507
At time: 132.84211683273315 and batch: 900, loss is 3.385570502281189 and perplexity is 29.53483760289846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.341125070232234 and perplexity of 76.79388927315532
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 134.47890377044678 and batch: 50, loss is 3.667743444442749 and perplexity is 39.1634316150705
At time: 135.11697626113892 and batch: 100, loss is 3.5738961696624756 and perplexity is 35.6552417482162
At time: 135.73725295066833 and batch: 150, loss is 3.5744240140914916 and perplexity is 35.6740671369394
At time: 136.35758447647095 and batch: 200, loss is 3.4643265867233275 and perplexity is 31.95493364938057
At time: 136.97785258293152 and batch: 250, loss is 3.6051060485839845 and perplexity is 36.78558470957008
At time: 137.59844756126404 and batch: 300, loss is 3.5688466739654543 and perplexity is 35.475654552402396
At time: 138.2192509174347 and batch: 350, loss is 3.5486579656600954 and perplexity is 34.766628156398056
At time: 138.8401358127594 and batch: 400, loss is 3.4882601928710937 and perplexity is 32.728956072285946
At time: 139.462988615036 and batch: 450, loss is 3.495879120826721 and perplexity is 32.97926797399411
At time: 140.08399295806885 and batch: 500, loss is 3.392465829849243 and perplexity is 29.739193724354138
At time: 140.70492553710938 and batch: 550, loss is 3.438267903327942 and perplexity is 31.132986097770136
At time: 141.32615327835083 and batch: 600, loss is 3.460649456977844 and perplexity is 31.837646983642642
At time: 141.94784235954285 and batch: 650, loss is 3.3021378755569457 and perplexity is 27.17066437218741
At time: 142.56926703453064 and batch: 700, loss is 3.2907640027999876 and perplexity is 26.863379514980323
At time: 143.19102239608765 and batch: 750, loss is 3.3754248905181883 and perplexity is 29.23670354064659
At time: 143.8181734085083 and batch: 800, loss is 3.3382925176620484 and perplexity is 28.17098415793808
At time: 144.4550426006317 and batch: 850, loss is 3.390830187797546 and perplexity is 29.69059080783778
At time: 145.07449626922607 and batch: 900, loss is 3.367201023101807 and perplexity is 28.99725073067154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334796226187928 and perplexity of 76.30940744648075
finished 12 epochs...
Completing Train Step...
At time: 146.65000748634338 and batch: 50, loss is 3.6560802602767946 and perplexity is 38.709314672119625
At time: 147.28499794006348 and batch: 100, loss is 3.557546844482422 and perplexity is 35.07704207311609
At time: 147.90667080879211 and batch: 150, loss is 3.558779559135437 and perplexity is 35.120308719099334
At time: 148.52848410606384 and batch: 200, loss is 3.4509835338592527 and perplexity is 31.531389251936197
At time: 149.15058708190918 and batch: 250, loss is 3.5921675252914427 and perplexity is 36.31269938083991
At time: 149.7729995250702 and batch: 300, loss is 3.5571661520004274 and perplexity is 35.06369104838686
At time: 150.3957257270813 and batch: 350, loss is 3.5373008155822756 and perplexity is 34.37401206279362
At time: 151.0201277732849 and batch: 400, loss is 3.47702627658844 and perplexity is 32.36333921939882
At time: 151.6428017616272 and batch: 450, loss is 3.4866791200637817 and perplexity is 32.67725009605602
At time: 152.2658007144928 and batch: 500, loss is 3.3846579217910766 and perplexity is 29.507896980936096
At time: 152.88845920562744 and batch: 550, loss is 3.4316291189193726 and perplexity is 30.926985468550885
At time: 153.51074314117432 and batch: 600, loss is 3.4558581829071047 and perplexity is 31.68546894564074
At time: 154.13671827316284 and batch: 650, loss is 3.298128056526184 and perplexity is 27.061933067224974
At time: 154.76006650924683 and batch: 700, loss is 3.2880722951889036 and perplexity is 26.791168381108946
At time: 155.38234543800354 and batch: 750, loss is 3.3740917921066282 and perplexity is 29.197754105080897
At time: 156.01079964637756 and batch: 800, loss is 3.338801131248474 and perplexity is 28.1853159475863
At time: 156.6347246170044 and batch: 850, loss is 3.393285002708435 and perplexity is 29.763565245588875
At time: 157.2592523097992 and batch: 900, loss is 3.3711382818222044 and perplexity is 29.11164546202628
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334321322506422 and perplexity of 76.27317643175293
finished 13 epochs...
Completing Train Step...
At time: 158.84055399894714 and batch: 50, loss is 3.6503204250335695 and perplexity is 38.48699627054278
At time: 159.4540843963623 and batch: 100, loss is 3.551058359146118 and perplexity is 34.850181985069696
At time: 160.08508396148682 and batch: 150, loss is 3.551862053871155 and perplexity is 34.878202150819135
At time: 160.71580457687378 and batch: 200, loss is 3.4442385482788085 and perplexity is 31.319426133789822
At time: 161.33307361602783 and batch: 250, loss is 3.585450267791748 and perplexity is 36.06959503992143
At time: 161.95604586601257 and batch: 300, loss is 3.5508161544799806 and perplexity is 34.841742130504514
At time: 162.57253503799438 and batch: 350, loss is 3.531139268875122 and perplexity is 34.16286614268731
At time: 163.19783401489258 and batch: 400, loss is 3.471261281967163 and perplexity is 32.17730151131482
At time: 163.81578016281128 and batch: 450, loss is 3.4813896083831786 and perplexity is 32.504859732345544
At time: 164.43320226669312 and batch: 500, loss is 3.379923839569092 and perplexity is 29.368534307884456
At time: 165.06915807724 and batch: 550, loss is 3.427486228942871 and perplexity is 30.799123412545292
At time: 165.69569540023804 and batch: 600, loss is 3.4525623655319215 and perplexity is 31.581211327956883
At time: 166.3170940876007 and batch: 650, loss is 3.295329756736755 and perplexity is 26.98631152084617
At time: 166.9299659729004 and batch: 700, loss is 3.286123309135437 and perplexity is 26.739003618288226
At time: 167.541987657547 and batch: 750, loss is 3.3727664709091187 and perplexity is 29.159083333902505
At time: 168.15378308296204 and batch: 800, loss is 3.3383073902130125 and perplexity is 28.171403135451303
At time: 168.76514267921448 and batch: 850, loss is 3.393474817276001 and perplexity is 29.769215340073348
At time: 169.37744545936584 and batch: 900, loss is 3.3718176221847536 and perplexity is 29.13142889689092
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.334682098806721 and perplexity of 76.30069895059579
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 170.94803762435913 and batch: 50, loss is 3.648574638366699 and perplexity is 38.419864801267885
At time: 171.56556749343872 and batch: 100, loss is 3.552647337913513 and perplexity is 34.905602203398274
At time: 172.18041157722473 and batch: 150, loss is 3.5542711877822875 and perplexity is 34.96232970696502
At time: 172.79613280296326 and batch: 200, loss is 3.4429767751693725 and perplexity is 31.27993304498962
At time: 173.41148400306702 and batch: 250, loss is 3.5850673484802247 and perplexity is 36.055785939476166
At time: 174.02651381492615 and batch: 300, loss is 3.551307649612427 and perplexity is 34.85887088617282
At time: 174.6408019065857 and batch: 350, loss is 3.5322673177719115 and perplexity is 34.20142527035258
At time: 175.255131483078 and batch: 400, loss is 3.4738628244400025 and perplexity is 32.2611211107074
At time: 175.8844621181488 and batch: 450, loss is 3.482245798110962 and perplexity is 32.532701976772415
At time: 176.50153589248657 and batch: 500, loss is 3.3786038827896117 and perplexity is 29.329794684860016
At time: 177.1155560016632 and batch: 550, loss is 3.4232754945755004 and perplexity is 30.669709140880457
At time: 177.7308874130249 and batch: 600, loss is 3.449013562202454 and perplexity is 31.469334451977392
At time: 178.34681701660156 and batch: 650, loss is 3.2897555208206177 and perplexity is 26.83630193675424
At time: 178.96135067939758 and batch: 700, loss is 3.2796807622909547 and perplexity is 26.567290065141624
At time: 179.5733494758606 and batch: 750, loss is 3.3670172119140624 and perplexity is 28.991921201401972
At time: 180.18553376197815 and batch: 800, loss is 3.3299698400497437 and perplexity is 27.93749909694704
At time: 180.79872012138367 and batch: 850, loss is 3.384084711074829 and perplexity is 29.490987584959175
At time: 181.41182780265808 and batch: 900, loss is 3.3612076234817505 and perplexity is 28.823978383044572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3341001745772685 and perplexity of 76.25631064172113
finished 15 epochs...
Completing Train Step...
At time: 182.95969009399414 and batch: 50, loss is 3.645182976722717 and perplexity is 38.28977834877279
At time: 183.58929204940796 and batch: 100, loss is 3.5482399559020994 and perplexity is 34.752098403576866
At time: 184.2138385772705 and batch: 150, loss is 3.550702838897705 and perplexity is 34.83779424189037
At time: 184.84705924987793 and batch: 200, loss is 3.4408777809143065 and perplexity is 31.21434550324634
At time: 185.46965909004211 and batch: 250, loss is 3.581751503944397 and perplexity is 35.936428553311586
At time: 186.09784197807312 and batch: 300, loss is 3.547787094116211 and perplexity is 34.736364069238725
At time: 186.7220642566681 and batch: 350, loss is 3.528618197441101 and perplexity is 34.07684759176618
At time: 187.3374445438385 and batch: 400, loss is 3.4700307035446167 and perplexity is 32.137729171806576
At time: 187.95400953292847 and batch: 450, loss is 3.478957929611206 and perplexity is 32.42591437871946
At time: 188.56986474990845 and batch: 500, loss is 3.37602490901947 and perplexity is 29.254251367671436
At time: 189.18385410308838 and batch: 550, loss is 3.421376085281372 and perplexity is 30.61151009968726
At time: 189.7997362613678 and batch: 600, loss is 3.4478790378570556 and perplexity is 31.43365197105718
At time: 190.41691493988037 and batch: 650, loss is 3.2888437509536743 and perplexity is 26.8118445567535
At time: 191.03327322006226 and batch: 700, loss is 3.2787585926055907 and perplexity is 26.542801808520622
At time: 191.6624116897583 and batch: 750, loss is 3.366811389923096 and perplexity is 28.985954640504136
At time: 192.27641248703003 and batch: 800, loss is 3.330456647872925 and perplexity is 27.95110260094499
At time: 192.89231824874878 and batch: 850, loss is 3.3856008052825928 and perplexity is 29.53573261068444
At time: 193.50643706321716 and batch: 900, loss is 3.363570852279663 and perplexity is 28.89217659107465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33370470020869 and perplexity of 76.22615918787653
finished 16 epochs...
Completing Train Step...
At time: 195.0670189857483 and batch: 50, loss is 3.6432445192337037 and perplexity is 38.215627133888184
At time: 195.69622373580933 and batch: 100, loss is 3.545941514968872 and perplexity is 34.67231448252925
At time: 196.31497478485107 and batch: 150, loss is 3.548524136543274 and perplexity is 34.76197568058234
At time: 196.9305911064148 and batch: 200, loss is 3.439071774482727 and perplexity is 31.15802306915775
At time: 197.5490424633026 and batch: 250, loss is 3.579638090133667 and perplexity is 35.8605602077423
At time: 198.17240977287292 and batch: 300, loss is 3.5457410430908203 and perplexity is 34.66536435520435
At time: 198.78861260414124 and batch: 350, loss is 3.5265851640701293 and perplexity is 34.007638599368185
At time: 199.4047303199768 and batch: 400, loss is 3.4679432106018067 and perplexity is 32.07071186237909
At time: 200.02105236053467 and batch: 450, loss is 3.4770679759979246 and perplexity is 32.36468877967095
At time: 200.63720846176147 and batch: 500, loss is 3.374473686218262 and perplexity is 29.208906684863283
At time: 201.25269222259521 and batch: 550, loss is 3.420142569541931 and perplexity is 30.573773599239733
At time: 201.86542439460754 and batch: 600, loss is 3.4470713758468627 and perplexity is 31.408274454126218
At time: 202.4804060459137 and batch: 650, loss is 3.288203125 and perplexity is 26.794673693902002
At time: 203.09541416168213 and batch: 700, loss is 3.2782899951934814 and perplexity is 26.530366834009
At time: 203.7103955745697 and batch: 750, loss is 3.3666850328445435 and perplexity is 28.982292291343455
At time: 204.32658219337463 and batch: 800, loss is 3.330645227432251 and perplexity is 27.956374104589443
At time: 204.94235253334045 and batch: 850, loss is 3.3863275623321534 and perplexity is 29.557205714492845
At time: 205.55795812606812 and batch: 900, loss is 3.364606623649597 and perplexity is 28.92211778384109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333612729425299 and perplexity of 76.21914893067552
finished 17 epochs...
Completing Train Step...
At time: 207.13138246536255 and batch: 50, loss is 3.641672534942627 and perplexity is 38.15559996160726
At time: 207.75582933425903 and batch: 100, loss is 3.5441851568222047 and perplexity is 34.61147092770147
At time: 208.37687706947327 and batch: 150, loss is 3.546764192581177 and perplexity is 34.70085035572342
At time: 208.99285554885864 and batch: 200, loss is 3.437434868812561 and perplexity is 31.107062045098306
At time: 209.61801958084106 and batch: 250, loss is 3.577926697731018 and perplexity is 35.79924120285385
At time: 210.23455810546875 and batch: 300, loss is 3.5441016817092894 and perplexity is 34.60858185184226
At time: 210.85078716278076 and batch: 350, loss is 3.5249946975708006 and perplexity is 33.953593579244206
At time: 211.4679660797119 and batch: 400, loss is 3.4663683938980103 and perplexity is 32.02024611721459
At time: 212.0858862400055 and batch: 450, loss is 3.4756165075302126 and perplexity is 32.31774653027868
At time: 212.70369267463684 and batch: 500, loss is 3.3732266092300414 and perplexity is 29.172503632907546
At time: 213.3216700553894 and batch: 550, loss is 3.4190879392623903 and perplexity is 30.541546568630324
At time: 213.9402072429657 and batch: 600, loss is 3.4463307762146 and perplexity is 31.3850221090174
At time: 214.5548870563507 and batch: 650, loss is 3.2875860595703124 and perplexity is 26.778144727317148
At time: 215.178236246109 and batch: 700, loss is 3.2778767108917237 and perplexity is 26.51940451530978
At time: 215.8112223148346 and batch: 750, loss is 3.366489539146423 and perplexity is 28.97662698962685
At time: 216.43503069877625 and batch: 800, loss is 3.33065682888031 and perplexity is 27.956698440892918
At time: 217.0510733127594 and batch: 850, loss is 3.386662368774414 and perplexity is 29.56710331417889
At time: 217.6780230998993 and batch: 900, loss is 3.365089192390442 and perplexity is 28.936078061928605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333655788473887 and perplexity of 76.22243092537194
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 219.2626359462738 and batch: 50, loss is 3.6410328769683837 and perplexity is 38.13120123208266
At time: 219.88371682167053 and batch: 100, loss is 3.544306826591492 and perplexity is 34.61568235358034
At time: 220.5046968460083 and batch: 150, loss is 3.5475554418563844 and perplexity is 34.728318243956714
At time: 221.12480068206787 and batch: 200, loss is 3.4373709869384768 and perplexity is 31.105074931148565
At time: 221.7449655532837 and batch: 250, loss is 3.5777229261398316 and perplexity is 35.79194707770366
At time: 222.3693127632141 and batch: 300, loss is 3.543720393180847 and perplexity is 34.5953885119909
At time: 223.01125717163086 and batch: 350, loss is 3.525067720413208 and perplexity is 33.956073057685494
At time: 223.6450366973877 and batch: 400, loss is 3.4664530038833616 and perplexity is 32.02295546438681
At time: 224.27301049232483 and batch: 450, loss is 3.4755419874191285 and perplexity is 32.31533829794924
At time: 224.90128350257874 and batch: 500, loss is 3.372663154602051 and perplexity is 29.15607088071541
At time: 225.52402997016907 and batch: 550, loss is 3.417365508079529 and perplexity is 30.488986135306927
At time: 226.14434695243835 and batch: 600, loss is 3.444580874443054 and perplexity is 31.330149428131968
At time: 226.7652542591095 and batch: 650, loss is 3.285845332145691 and perplexity is 26.731571823535425
At time: 227.38901591300964 and batch: 700, loss is 3.2760335922241213 and perplexity is 26.47057112248714
At time: 228.0092113018036 and batch: 750, loss is 3.3645195055007933 and perplexity is 28.919598252230337
At time: 228.64405012130737 and batch: 800, loss is 3.3276094150543214 and perplexity is 27.871632492940144
At time: 229.27151083946228 and batch: 850, loss is 3.3833306407928467 and perplexity is 29.468757690139945
At time: 229.9044463634491 and batch: 900, loss is 3.3610924339294432 and perplexity is 28.820658353098967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.333208057978382 and perplexity of 76.18831145733824
finished 19 epochs...
Completing Train Step...
At time: 231.49095153808594 and batch: 50, loss is 3.640351719856262 and perplexity is 38.10523673712379
At time: 232.13382959365845 and batch: 100, loss is 3.543513283729553 and perplexity is 34.588224221980575
At time: 232.75552082061768 and batch: 150, loss is 3.5467982339859008 and perplexity is 34.70203164152084
At time: 233.3777368068695 and batch: 200, loss is 3.4368146228790284 and perplexity is 31.087773998645222
At time: 234.00004887580872 and batch: 250, loss is 3.57704626083374 and perplexity is 35.76773610116644
At time: 234.64249539375305 and batch: 300, loss is 3.543072838783264 and perplexity is 34.57299336784348
At time: 235.27789950370789 and batch: 350, loss is 3.5243976402282713 and perplexity is 33.93332738753379
At time: 235.91237378120422 and batch: 400, loss is 3.465772099494934 and perplexity is 32.0011583152101
At time: 236.53785705566406 and batch: 450, loss is 3.4749794244766234 and perplexity is 32.29716399872124
At time: 237.1598138809204 and batch: 500, loss is 3.3721805238723754 and perplexity is 29.14200266010265
At time: 237.78333234786987 and batch: 550, loss is 3.4170725536346436 and perplexity is 30.480055559488353
At time: 238.40628504753113 and batch: 600, loss is 3.4444331312179566 and perplexity is 31.325520952734028
At time: 239.0434947013855 and batch: 650, loss is 3.285708255767822 and perplexity is 26.727907807625865
At time: 239.6668016910553 and batch: 700, loss is 3.2758672857284545 and perplexity is 26.46616926060472
At time: 240.29038047790527 and batch: 750, loss is 3.3645366287231444 and perplexity is 28.92009345318122
At time: 240.9142472743988 and batch: 800, loss is 3.327782139778137 and perplexity is 27.876447028747492
At time: 241.5428295135498 and batch: 850, loss is 3.383699507713318 and perplexity is 29.479629745086665
At time: 242.17180609703064 and batch: 900, loss is 3.361526927947998 and perplexity is 28.833183477612994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.332977712970891 and perplexity of 76.17076388124072
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
503.3101351261139


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8412322998046875 and batch: 50, loss is 7.263144702911377 and perplexity is 1426.7361513105736
At time: 1.496415376663208 and batch: 100, loss is 6.481162691116333 and perplexity is 652.7294279530196
At time: 2.1246755123138428 and batch: 150, loss is 6.357550992965698 and perplexity is 576.8319596400778
At time: 2.759855031967163 and batch: 200, loss is 6.207811136245727 and perplexity is 496.61304246231424
At time: 3.385732889175415 and batch: 250, loss is 6.259527521133423 and perplexity is 522.971788607735
At time: 4.00604772567749 and batch: 300, loss is 6.1544198799133305 and perplexity is 470.7936463742826
At time: 4.627718925476074 and batch: 350, loss is 6.159644899368286 and perplexity is 473.25999007165865
At time: 5.250017166137695 and batch: 400, loss is 6.050960988998413 and perplexity is 424.52079389762827
At time: 5.872889280319214 and batch: 450, loss is 6.052774229049683 and perplexity is 425.29125030383227
At time: 6.495317220687866 and batch: 500, loss is 6.024402418136597 and perplexity is 413.39453112165745
At time: 7.11796498298645 and batch: 550, loss is 6.064549503326416 and perplexity is 430.328772318143
At time: 7.740725994110107 and batch: 600, loss is 6.00036060333252 and perplexity is 403.57429749313997
At time: 8.362732887268066 and batch: 650, loss is 5.934297695159912 and perplexity is 377.7745900614921
At time: 8.984827518463135 and batch: 700, loss is 6.031627330780029 and perplexity is 416.3920859931363
At time: 9.606693506240845 and batch: 750, loss is 5.983361902236939 and perplexity is 396.7720372299687
At time: 10.228688716888428 and batch: 800, loss is 5.990785942077637 and perplexity is 399.72865006798963
At time: 10.850979089736938 and batch: 850, loss is 6.034349737167358 and perplexity is 417.52721891314695
At time: 11.473585605621338 and batch: 900, loss is 5.9120907211303715 and perplexity is 369.4778235497252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.839436988308005 and perplexity of 343.5858433524418
finished 1 epochs...
Completing Train Step...
At time: 13.05417275428772 and batch: 50, loss is 5.581682138442993 and perplexity is 265.5178681567821
At time: 13.670082330703735 and batch: 100, loss is 5.291611442565918 and perplexity is 198.6633021100015
At time: 14.30123782157898 and batch: 150, loss is 5.199021787643432 and perplexity is 181.09500582962417
At time: 14.918297052383423 and batch: 200, loss is 5.024978036880493 and perplexity is 152.16691401585075
At time: 15.545651197433472 and batch: 250, loss is 5.072159233093262 and perplexity is 159.5183931439482
At time: 16.16430926322937 and batch: 300, loss is 4.979501533508301 and perplexity is 145.4018856188781
At time: 16.782068252563477 and batch: 350, loss is 4.949659042358398 and perplexity is 141.12683744373635
At time: 17.41501259803772 and batch: 400, loss is 4.799737548828125 and perplexity is 121.47853115175225
At time: 18.04257583618164 and batch: 450, loss is 4.807161321640015 and perplexity is 122.38371595447043
At time: 18.667261123657227 and batch: 500, loss is 4.711481370925903 and perplexity is 111.21679125658419
At time: 19.292976140975952 and batch: 550, loss is 4.769784832000733 and perplexity is 117.89387224277648
At time: 19.916166305541992 and batch: 600, loss is 4.702989892959595 and perplexity is 110.27639465328046
At time: 20.53143835067749 and batch: 650, loss is 4.564392852783203 and perplexity is 96.00428758201902
At time: 21.148197650909424 and batch: 700, loss is 4.614447813034058 and perplexity is 100.93207976313495
At time: 21.762788772583008 and batch: 750, loss is 4.645293951034546 and perplexity is 104.09395981579941
At time: 22.37812328338623 and batch: 800, loss is 4.591493978500366 and perplexity is 98.64168869616638
At time: 22.99282956123352 and batch: 850, loss is 4.640795516967773 and perplexity is 103.62675164126796
At time: 23.60966420173645 and batch: 900, loss is 4.571275339126587 and perplexity is 96.66731480087661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6607904303563785 and perplexity of 105.71961313108491
finished 2 epochs...
Completing Train Step...
At time: 25.183553457260132 and batch: 50, loss is 4.619185972213745 and perplexity is 101.41144678519599
At time: 25.8051438331604 and batch: 100, loss is 4.477828922271729 and perplexity is 88.04331614306162
At time: 26.428922176361084 and batch: 150, loss is 4.479098978042603 and perplexity is 88.1552071036557
At time: 27.04247546195984 and batch: 200, loss is 4.360589456558228 and perplexity is 78.30327720244394
At time: 27.65593719482422 and batch: 250, loss is 4.497153816223144 and perplexity is 89.76129025979388
At time: 28.269776582717896 and batch: 300, loss is 4.455047869682312 and perplexity is 86.06027042434104
At time: 28.883411169052124 and batch: 350, loss is 4.442653875350953 and perplexity is 85.00022260344555
At time: 29.498443126678467 and batch: 400, loss is 4.346370692253113 and perplexity is 77.19777939110574
At time: 30.126737117767334 and batch: 450, loss is 4.383918075561524 and perplexity is 80.15145846322226
At time: 30.742417812347412 and batch: 500, loss is 4.264462690353394 and perplexity is 71.12669264626884
At time: 31.354931831359863 and batch: 550, loss is 4.341074695587158 and perplexity is 76.79002090567384
At time: 31.967020750045776 and batch: 600, loss is 4.322837905883789 and perplexity is 75.40230960296125
At time: 32.57970952987671 and batch: 650, loss is 4.174794335365295 and perplexity is 65.0264646415832
At time: 33.19196534156799 and batch: 700, loss is 4.192838115692139 and perplexity is 66.21043743471898
At time: 33.81710433959961 and batch: 750, loss is 4.282557320594788 and perplexity is 72.42541839604266
At time: 34.449801445007324 and batch: 800, loss is 4.244891490936279 and perplexity is 69.74819142232569
At time: 35.08046007156372 and batch: 850, loss is 4.307279376983643 and perplexity is 74.238239679533
At time: 35.70448327064514 and batch: 900, loss is 4.250812087059021 and perplexity is 70.16236716782846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.468392986140839 and perplexity of 87.21645228755513
finished 3 epochs...
Completing Train Step...
At time: 37.256765842437744 and batch: 50, loss is 4.330017929077148 and perplexity is 75.94564819236649
At time: 37.901593923568726 and batch: 100, loss is 4.196136131286621 and perplexity is 66.42916096845582
At time: 38.52773928642273 and batch: 150, loss is 4.204550986289978 and perplexity is 66.9905112542099
At time: 39.1552619934082 and batch: 200, loss is 4.087098569869995 and perplexity is 59.566811793445275
At time: 39.78141450881958 and batch: 250, loss is 4.2416253185272215 and perplexity is 69.52075343195497
At time: 40.406405448913574 and batch: 300, loss is 4.207251124382019 and perplexity is 67.1716393107887
At time: 41.03033089637756 and batch: 350, loss is 4.1960579204559325 and perplexity is 66.42396569176066
At time: 41.661261320114136 and batch: 400, loss is 4.118800930976867 and perplexity is 61.485472720168026
At time: 42.27475309371948 and batch: 450, loss is 4.162328987121582 and perplexity is 64.22091825367976
At time: 42.888020277023315 and batch: 500, loss is 4.035138878822327 and perplexity is 56.55077339780274
At time: 43.50088405609131 and batch: 550, loss is 4.1153146839141845 and perplexity is 61.271492382386626
At time: 44.11960172653198 and batch: 600, loss is 4.112436962127686 and perplexity is 61.09542353395337
At time: 44.74185395240784 and batch: 650, loss is 3.9640379333496094 and perplexity is 52.66957336705448
At time: 45.35706448554993 and batch: 700, loss is 3.9705289936065675 and perplexity is 53.01256673248757
At time: 46.00411319732666 and batch: 750, loss is 4.078067321777343 and perplexity is 59.031271078620435
At time: 46.626569747924805 and batch: 800, loss is 4.044329409599304 and perplexity is 57.072900659355184
At time: 47.23770761489868 and batch: 850, loss is 4.107665457725525 and perplexity is 60.80460083427444
At time: 47.873223066329956 and batch: 900, loss is 4.05670063495636 and perplexity is 57.78334786546426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.394917527290239 and perplexity of 81.03794706803194
finished 4 epochs...
Completing Train Step...
At time: 49.52253746986389 and batch: 50, loss is 4.1413883161544796 and perplexity is 62.89007217254677
At time: 50.1678147315979 and batch: 100, loss is 4.014991211891174 and perplexity is 55.42280833868257
At time: 50.803529024124146 and batch: 150, loss is 4.024027752876282 and perplexity is 55.92590853697224
At time: 51.421146631240845 and batch: 200, loss is 3.9120157527923585 and perplexity is 49.99963736952564
At time: 52.04651379585266 and batch: 250, loss is 4.071463413238526 and perplexity is 58.642718358915175
At time: 52.6749210357666 and batch: 300, loss is 4.037040514945984 and perplexity is 56.658414706179066
At time: 53.29549837112427 and batch: 350, loss is 4.025697979927063 and perplexity is 56.01939555278851
At time: 53.91186738014221 and batch: 400, loss is 3.961258282661438 and perplexity is 52.52337363742243
At time: 54.525264263153076 and batch: 450, loss is 4.005417666435242 and perplexity is 54.89474730509304
At time: 55.13794994354248 and batch: 500, loss is 3.8794664430618284 and perplexity is 48.398384886968174
At time: 55.74970364570618 and batch: 550, loss is 3.95559449672699 and perplexity is 52.2267333391397
At time: 56.361823320388794 and batch: 600, loss is 3.962056517601013 and perplexity is 52.565316367253295
At time: 56.97863507270813 and batch: 650, loss is 3.8178647470474245 and perplexity is 45.5069356881778
At time: 57.59529519081116 and batch: 700, loss is 3.8178972148895265 and perplexity is 45.50841322416634
At time: 58.21164608001709 and batch: 750, loss is 3.9335608863830567 and perplexity is 51.08857476421145
At time: 58.8315212726593 and batch: 800, loss is 3.897225942611694 and perplexity is 49.265593786605834
At time: 59.44837212562561 and batch: 850, loss is 3.9619453287124635 and perplexity is 52.5594720130698
At time: 60.06551790237427 and batch: 900, loss is 3.9113436555862426 and perplexity is 49.966044043197705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.376195620184076 and perplexity of 79.53487623041376
finished 5 epochs...
Completing Train Step...
At time: 61.644551277160645 and batch: 50, loss is 4.0006830739974975 and perplexity is 54.63545735012181
At time: 62.26380729675293 and batch: 100, loss is 3.8827801275253297 and perplexity is 48.559027876120275
At time: 62.883790493011475 and batch: 150, loss is 3.891533088684082 and perplexity is 48.9859287588926
At time: 63.505635499954224 and batch: 200, loss is 3.782260956764221 and perplexity is 43.915219988138595
At time: 64.12785005569458 and batch: 250, loss is 3.9382534217834473 and perplexity is 51.328873073063484
At time: 64.75044226646423 and batch: 300, loss is 3.9075115537643432 and perplexity is 49.774935482387804
At time: 65.37138628959656 and batch: 350, loss is 3.897324161529541 and perplexity is 49.27043283755392
At time: 65.99344873428345 and batch: 400, loss is 3.8399263286590575 and perplexity is 46.52204697195387
At time: 66.61485481262207 and batch: 450, loss is 3.881063723564148 and perplexity is 48.47575245589754
At time: 67.23736023902893 and batch: 500, loss is 3.7623213481903077 and perplexity is 43.04824003645451
At time: 67.86024761199951 and batch: 550, loss is 3.835006985664368 and perplexity is 46.29375105942811
At time: 68.48127484321594 and batch: 600, loss is 3.843237600326538 and perplexity is 46.676349435705845
At time: 69.09934258460999 and batch: 650, loss is 3.705586895942688 and perplexity is 40.673911666979244
At time: 69.7181944847107 and batch: 700, loss is 3.701999936103821 and perplexity is 40.528277327750814
At time: 70.33869552612305 and batch: 750, loss is 3.818703780174255 and perplexity is 45.54513353711018
At time: 70.95850920677185 and batch: 800, loss is 3.783912377357483 and perplexity is 43.98780240233589
At time: 71.57887864112854 and batch: 850, loss is 3.8454864692687987 and perplexity is 46.7814365475719
At time: 72.19787073135376 and batch: 900, loss is 3.7971302556991575 and perplexity is 44.57308741470212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377585633160317 and perplexity of 79.64550761215297
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 73.7787938117981 and batch: 50, loss is 3.9120553588867186 and perplexity is 50.001617699097565
At time: 74.40014743804932 and batch: 100, loss is 3.7982340383529665 and perplexity is 44.622313577911875
At time: 75.02071166038513 and batch: 150, loss is 3.804219479560852 and perplexity is 44.89019871801098
At time: 75.64323163032532 and batch: 200, loss is 3.67793306350708 and perplexity is 39.5645321247194
At time: 76.26719617843628 and batch: 250, loss is 3.829311418533325 and perplexity is 46.03083134172046
At time: 76.88970637321472 and batch: 300, loss is 3.783843755722046 and perplexity is 43.9847839909611
At time: 77.52550959587097 and batch: 350, loss is 3.7608289194107054 and perplexity is 42.984041521884265
At time: 78.14887857437134 and batch: 400, loss is 3.6958077335357666 and perplexity is 40.278093418931824
At time: 78.77215886116028 and batch: 450, loss is 3.7242314767837525 and perplexity is 41.43937338355743
At time: 79.39539933204651 and batch: 500, loss is 3.608429231643677 and perplexity is 36.90803328835029
At time: 80.01845741271973 and batch: 550, loss is 3.663574833869934 and perplexity is 39.00051432526261
At time: 80.63906836509705 and batch: 600, loss is 3.6654817342758177 and perplexity is 39.07495537513592
At time: 81.26079845428467 and batch: 650, loss is 3.513317222595215 and perplexity is 33.55940736906544
At time: 81.88178873062134 and batch: 700, loss is 3.4950471687316895 and perplexity is 32.9518422129521
At time: 82.50275564193726 and batch: 750, loss is 3.59726309299469 and perplexity is 36.498205426945695
At time: 83.12418580055237 and batch: 800, loss is 3.5520778465270997 and perplexity is 34.88572942283164
At time: 83.74478936195374 and batch: 850, loss is 3.598980360031128 and perplexity is 36.560936439548406
At time: 84.36502885818481 and batch: 900, loss is 3.538076539039612 and perplexity is 34.40068713517554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325686938142123 and perplexity of 75.6174395259009
finished 7 epochs...
Completing Train Step...
At time: 85.92553114891052 and batch: 50, loss is 3.8154414796829226 and perplexity is 45.39679372168078
At time: 86.57415819168091 and batch: 100, loss is 3.703271036148071 and perplexity is 40.57982557740619
At time: 87.18995547294617 and batch: 150, loss is 3.7108125925064086 and perplexity is 40.887017515705196
At time: 87.80568099021912 and batch: 200, loss is 3.588971652984619 and perplexity is 36.19683387480069
At time: 88.42181634902954 and batch: 250, loss is 3.742675223350525 and perplexity is 42.21076246959016
At time: 89.036869764328 and batch: 300, loss is 3.7011028337478638 and perplexity is 40.49193561823111
At time: 89.65419483184814 and batch: 350, loss is 3.682704362869263 and perplexity is 39.75375741789074
At time: 90.27022385597229 and batch: 400, loss is 3.6233935832977293 and perplexity is 37.46449120932755
At time: 90.88624000549316 and batch: 450, loss is 3.657559952735901 and perplexity is 38.76663495087073
At time: 91.49884343147278 and batch: 500, loss is 3.546430993080139 and perplexity is 34.68928997576267
At time: 92.11421084403992 and batch: 550, loss is 3.601303429603577 and perplexity is 36.64596876829833
At time: 92.72776341438293 and batch: 600, loss is 3.6117408084869385 and perplexity is 37.03045967703238
At time: 93.35517454147339 and batch: 650, loss is 3.4652223443984984 and perplexity is 31.98357035031411
At time: 93.97379851341248 and batch: 700, loss is 3.4504894018173218 and perplexity is 31.51581243100033
At time: 94.59463334083557 and batch: 750, loss is 3.559575915336609 and perplexity is 35.14828813404649
At time: 95.20898962020874 and batch: 800, loss is 3.519278631210327 and perplexity is 33.76006621971653
At time: 95.8217544555664 and batch: 850, loss is 3.5741816425323485 and perplexity is 35.66542180540001
At time: 96.43542432785034 and batch: 900, loss is 3.5203728246688843 and perplexity is 33.7970264804848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.328513367535317 and perplexity of 75.83146920702087
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 97.9944658279419 and batch: 50, loss is 3.7882657289505004 and perplexity is 44.17971419867226
At time: 98.62382769584656 and batch: 100, loss is 3.6861207723617553 and perplexity is 39.88980479654633
At time: 99.2412588596344 and batch: 150, loss is 3.6927307415008546 and perplexity is 40.154348524960355
At time: 99.86013722419739 and batch: 200, loss is 3.5643217420578 and perplexity is 35.315492266169926
At time: 100.47776937484741 and batch: 250, loss is 3.7202351903915405 and perplexity is 41.27410023896744
At time: 101.09365820884705 and batch: 300, loss is 3.671215786933899 and perplexity is 39.299656836204505
At time: 101.70848393440247 and batch: 350, loss is 3.6476882457733155 and perplexity is 38.3858248062977
At time: 102.3262369632721 and batch: 400, loss is 3.587968444824219 and perplexity is 36.16053912431576
At time: 102.93875288963318 and batch: 450, loss is 3.620127444267273 and perplexity is 37.34232658425852
At time: 103.55276203155518 and batch: 500, loss is 3.5089665126800536 and perplexity is 33.41371728014509
At time: 104.16667222976685 and batch: 550, loss is 3.552469825744629 and perplexity is 34.89940658416012
At time: 104.78110980987549 and batch: 600, loss is 3.565302095413208 and perplexity is 35.35013090378901
At time: 105.39613008499146 and batch: 650, loss is 3.4166445541381836 and perplexity is 30.467012902381256
At time: 106.01024389266968 and batch: 700, loss is 3.3958061122894287 and perplexity is 29.838697123147433
At time: 106.63743305206299 and batch: 750, loss is 3.4969398593902588 and perplexity is 33.01426891547374
At time: 107.26222538948059 and batch: 800, loss is 3.455168113708496 and perplexity is 31.663611321981676
At time: 107.88842272758484 and batch: 850, loss is 3.5102379274368287 and perplexity is 33.45622699138447
At time: 108.51951503753662 and batch: 900, loss is 3.4551106834411622 and perplexity is 31.661792924534726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3151312005029965 and perplexity of 74.8234396729028
finished 9 epochs...
Completing Train Step...
At time: 110.079106092453 and batch: 50, loss is 3.766128993034363 and perplexity is 43.21246490224357
At time: 110.70381188392639 and batch: 100, loss is 3.6527078914642335 and perplexity is 38.57899245736153
At time: 111.32580542564392 and batch: 150, loss is 3.6592903661727907 and perplexity is 38.833775330446834
At time: 111.94189929962158 and batch: 200, loss is 3.5339901304244994 and perplexity is 34.26039870404725
At time: 112.55828332901001 and batch: 250, loss is 3.6890323162078857 and perplexity is 40.006114951118185
At time: 113.17556095123291 and batch: 300, loss is 3.6435556697845457 and perplexity is 38.22751979743012
At time: 113.79108595848083 and batch: 350, loss is 3.620784559249878 and perplexity is 37.36687285051878
At time: 114.40464329719543 and batch: 400, loss is 3.5645844888687135 and perplexity is 35.32477251826363
At time: 115.01817655563354 and batch: 450, loss is 3.5989106130599975 and perplexity is 36.55838651389588
At time: 115.63237047195435 and batch: 500, loss is 3.4893342208862306 and perplexity is 32.76412677179117
At time: 116.24730086326599 and batch: 550, loss is 3.5335687017440796 and perplexity is 34.24596343136334
At time: 116.86072087287903 and batch: 600, loss is 3.550122284889221 and perplexity is 34.817574890577134
At time: 117.47356986999512 and batch: 650, loss is 3.4036199760437014 and perplexity is 30.07276593703311
At time: 118.08763551712036 and batch: 700, loss is 3.385182147026062 and perplexity is 29.52336982043697
At time: 118.70315217971802 and batch: 750, loss is 3.489071497917175 and perplexity is 32.75552001377319
At time: 119.31817412376404 and batch: 800, loss is 3.4502126216888427 and perplexity is 31.507090687444887
At time: 119.93292713165283 and batch: 850, loss is 3.5087478065490725 and perplexity is 33.40641029438747
At time: 120.54799437522888 and batch: 900, loss is 3.4559038162231444 and perplexity is 31.686914891650407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315362799657534 and perplexity of 74.8407707251217
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 122.12271046638489 and batch: 50, loss is 3.76256094455719 and perplexity is 43.05855547408926
At time: 122.75262951850891 and batch: 100, loss is 3.6530534410476685 and perplexity is 38.59232571565277
At time: 123.37431740760803 and batch: 150, loss is 3.658293161392212 and perplexity is 38.79506940611327
At time: 123.98994755744934 and batch: 200, loss is 3.531177182197571 and perplexity is 34.16416139500065
At time: 124.6203875541687 and batch: 250, loss is 3.6869653463363647 and perplexity is 39.92350891833836
At time: 125.23738622665405 and batch: 300, loss is 3.638566017150879 and perplexity is 38.037252830381355
At time: 125.85443162918091 and batch: 350, loss is 3.613061022758484 and perplexity is 37.07938010399727
At time: 126.46784830093384 and batch: 400, loss is 3.5582018232345582 and perplexity is 35.100024315980534
At time: 127.08392357826233 and batch: 450, loss is 3.588029046058655 and perplexity is 36.16273056402587
At time: 127.69924116134644 and batch: 500, loss is 3.47828134059906 and perplexity is 32.40398278151751
At time: 128.3156397342682 and batch: 550, loss is 3.518769679069519 and perplexity is 33.74288833347198
At time: 128.93230748176575 and batch: 600, loss is 3.539021830558777 and perplexity is 34.433221187637166
At time: 129.54744577407837 and batch: 650, loss is 3.389200196266174 and perplexity is 29.642234816893748
At time: 130.16228699684143 and batch: 700, loss is 3.368224720954895 and perplexity is 29.026950353116195
At time: 130.7797498703003 and batch: 750, loss is 3.4696124601364136 and perplexity is 32.1242905889211
At time: 131.40375018119812 and batch: 800, loss is 3.429891529083252 and perplexity is 30.873293713571673
At time: 132.01937532424927 and batch: 850, loss is 3.4872351837158204 and perplexity is 32.69542578003329
At time: 132.63544869422913 and batch: 900, loss is 3.4379065799713135 and perplexity is 31.121739054763452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310696954596533 and perplexity of 74.49238866505557
finished 11 epochs...
Completing Train Step...
At time: 134.22571682929993 and batch: 50, loss is 3.7535952949523925 and perplexity is 42.67423297804597
At time: 134.8696939945221 and batch: 100, loss is 3.639620885848999 and perplexity is 38.07739830813588
At time: 135.49062752723694 and batch: 150, loss is 3.644909267425537 and perplexity is 38.279299514594356
At time: 136.1090853214264 and batch: 200, loss is 3.520371856689453 and perplexity is 33.796993765674166
At time: 136.72589921951294 and batch: 250, loss is 3.6756123161315917 and perplexity is 39.47281930293653
At time: 137.3444676399231 and batch: 300, loss is 3.629253582954407 and perplexity is 37.684677631037246
At time: 137.96259808540344 and batch: 350, loss is 3.6038435459136964 and perplexity is 36.73917211482334
At time: 138.5805048942566 and batch: 400, loss is 3.5500947713851927 and perplexity is 34.816616950268326
At time: 139.19726610183716 and batch: 450, loss is 3.5811506366729735 and perplexity is 35.91484201551356
At time: 139.81471586227417 and batch: 500, loss is 3.4720584392547607 and perplexity is 32.20296210811681
At time: 140.4554681777954 and batch: 550, loss is 3.5130105066299437 and perplexity is 33.549115741424245
At time: 141.07607531547546 and batch: 600, loss is 3.535124101638794 and perplexity is 34.29927104586075
At time: 141.6966052055359 and batch: 650, loss is 3.3862056350708007 and perplexity is 29.553602105040422
At time: 142.32191228866577 and batch: 700, loss is 3.3659774255752564 and perplexity is 28.961791464759866
At time: 142.94869089126587 and batch: 750, loss is 3.4680039501190185 and perplexity is 32.07265988109451
At time: 143.5689148902893 and batch: 800, loss is 3.4300303745269773 and perplexity is 30.877580627338936
At time: 144.18637418746948 and batch: 850, loss is 3.488801136016846 and perplexity is 32.74666536616961
At time: 144.821683883667 and batch: 900, loss is 3.4409820365905763 and perplexity is 31.217599945590194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310107505484803 and perplexity of 74.4484921313837
finished 12 epochs...
Completing Train Step...
At time: 146.4088180065155 and batch: 50, loss is 3.748209738731384 and perplexity is 42.4450262540001
At time: 147.05983638763428 and batch: 100, loss is 3.6336299419403075 and perplexity is 37.84996071387215
At time: 147.68286275863647 and batch: 150, loss is 3.638571743965149 and perplexity is 38.037470663287394
At time: 148.30369925498962 and batch: 200, loss is 3.514386954307556 and perplexity is 33.59532613967427
At time: 148.92520928382874 and batch: 250, loss is 3.669476671218872 and perplexity is 39.23136958233503
At time: 149.54523706436157 and batch: 300, loss is 3.6235619831085204 and perplexity is 37.47080075380672
At time: 150.16627311706543 and batch: 350, loss is 3.598237566947937 and perplexity is 36.53378931244343
At time: 150.78835463523865 and batch: 400, loss is 3.5450096321105957 and perplexity is 34.64001899714796
At time: 151.41162490844727 and batch: 450, loss is 3.5765751791000366 and perplexity is 35.75089054216235
At time: 152.0521650314331 and batch: 500, loss is 3.467856273651123 and perplexity is 32.067923853675765
At time: 152.6835868358612 and batch: 550, loss is 3.5090664625167847 and perplexity is 33.4170571426384
At time: 153.30851984024048 and batch: 600, loss is 3.53216423034668 and perplexity is 34.19789971520553
At time: 153.92928409576416 and batch: 650, loss is 3.3837507724761964 and perplexity is 29.481141050053303
At time: 154.55354261398315 and batch: 700, loss is 3.363984851837158 and perplexity is 28.904140415736602
At time: 155.1791753768921 and batch: 750, loss is 3.466579384803772 and perplexity is 32.027002810720944
At time: 155.81373810768127 and batch: 800, loss is 3.4294024658203126 and perplexity is 30.858198411390177
At time: 156.43420934677124 and batch: 850, loss is 3.4887480449676516 and perplexity is 32.74492685749774
At time: 157.05618929862976 and batch: 900, loss is 3.441554403305054 and perplexity is 31.235472975180596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.310195713827055 and perplexity of 74.4550593990974
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 158.63446259498596 and batch: 50, loss is 3.747651391029358 and perplexity is 42.421333786061766
At time: 159.25827288627625 and batch: 100, loss is 3.635145945549011 and perplexity is 37.907384907548085
At time: 159.88150453567505 and batch: 150, loss is 3.639428734779358 and perplexity is 38.070082398224294
At time: 160.5036940574646 and batch: 200, loss is 3.515433225631714 and perplexity is 33.6304943605823
At time: 161.12617182731628 and batch: 250, loss is 3.6698021125793456 and perplexity is 39.2441391703885
At time: 161.7560338973999 and batch: 300, loss is 3.6241773319244386 and perplexity is 37.49386546237484
At time: 162.38354110717773 and batch: 350, loss is 3.5973334074020387 and perplexity is 36.50077186685737
At time: 163.00814485549927 and batch: 400, loss is 3.5445317220687866 and perplexity is 34.62346813944633
At time: 163.6301019191742 and batch: 450, loss is 3.573855562210083 and perplexity is 35.65379390908109
At time: 164.25348353385925 and batch: 500, loss is 3.4640613794326782 and perplexity is 31.946460091678848
At time: 164.87621068954468 and batch: 550, loss is 3.5041429471969603 and perplexity is 33.252932117437275
At time: 165.49912667274475 and batch: 600, loss is 3.527771496772766 and perplexity is 34.048006913569225
At time: 166.12311124801636 and batch: 650, loss is 3.3786584663391115 and perplexity is 29.331395652852876
At time: 166.75896501541138 and batch: 700, loss is 3.3583407402038574 and perplexity is 28.741461740724027
At time: 167.3827245235443 and batch: 750, loss is 3.4605181074142455 and perplexity is 31.83346539723612
At time: 168.0056438446045 and batch: 800, loss is 3.4231783390045165 and perplexity is 30.66672955252111
At time: 168.62810444831848 and batch: 850, loss is 3.47919105052948 and perplexity is 32.4334744188228
At time: 169.2597017288208 and batch: 900, loss is 3.4331078243255617 and perplexity is 30.972751197836857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.30867568760702 and perplexity of 74.34197172648422
finished 14 epochs...
Completing Train Step...
At time: 170.84042596817017 and batch: 50, loss is 3.743468008041382 and perplexity is 42.24423978426333
At time: 171.4579038619995 and batch: 100, loss is 3.6314129066467284 and perplexity is 37.76613896732785
At time: 172.08651638031006 and batch: 150, loss is 3.6361041688919067 and perplexity is 37.943726057322955
At time: 172.70316576957703 and batch: 200, loss is 3.5123732900619506 and perplexity is 33.52774449881037
At time: 173.31843304634094 and batch: 250, loss is 3.6663657283782958 and perplexity is 39.10951267721737
At time: 173.9342439174652 and batch: 300, loss is 3.6210832834243774 and perplexity is 37.37803690616831
At time: 174.55047273635864 and batch: 350, loss is 3.594477205276489 and perplexity is 36.39666702767151
At time: 175.17397809028625 and batch: 400, loss is 3.5421295070648195 and perplexity is 34.540394944578715
At time: 175.79535460472107 and batch: 450, loss is 3.571687240600586 and perplexity is 35.57656877201115
At time: 176.4110724925995 and batch: 500, loss is 3.4623488330841066 and perplexity is 31.89179711789683
At time: 177.02674436569214 and batch: 550, loss is 3.502756481170654 and perplexity is 33.20686000287251
At time: 177.6513180732727 and batch: 600, loss is 3.526876196861267 and perplexity is 34.01753737771443
At time: 178.28211283683777 and batch: 650, loss is 3.3779713487625123 and perplexity is 29.31124845789218
At time: 178.9099636077881 and batch: 700, loss is 3.358177423477173 and perplexity is 28.736768162552746
At time: 179.5294156074524 and batch: 750, loss is 3.460045881271362 and perplexity is 31.818436351490412
At time: 180.14392375946045 and batch: 800, loss is 3.4234437847137453 and perplexity is 30.67487098480324
At time: 180.7593696117401 and batch: 850, loss is 3.4802481889724732 and perplexity is 32.46777922073847
At time: 181.37537455558777 and batch: 900, loss is 3.43514075756073 and perplexity is 31.035780778889112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3081121575342465 and perplexity of 74.30008959177661
finished 15 epochs...
Completing Train Step...
At time: 182.95600581169128 and batch: 50, loss is 3.741345534324646 and perplexity is 42.15467258127908
At time: 183.5864109992981 and batch: 100, loss is 3.629275708198547 and perplexity is 37.68551142295408
At time: 184.20261478424072 and batch: 150, loss is 3.634039645195007 and perplexity is 37.86547114308659
At time: 184.817551612854 and batch: 200, loss is 3.510461368560791 and perplexity is 33.46370332357589
At time: 185.43206357955933 and batch: 250, loss is 3.664306154251099 and perplexity is 39.02904662812022
At time: 186.047865152359 and batch: 300, loss is 3.619198884963989 and perplexity is 37.30766811321559
At time: 186.6627197265625 and batch: 350, loss is 3.5927311754226685 and perplexity is 36.333172807993996
At time: 187.27897834777832 and batch: 400, loss is 3.5405889749526978 and perplexity is 34.487225322271854
At time: 187.91130876541138 and batch: 450, loss is 3.570287094116211 and perplexity is 35.52679122038288
At time: 188.53290581703186 and batch: 500, loss is 3.461203007698059 and perplexity is 31.855275614780968
At time: 189.15455150604248 and batch: 550, loss is 3.5017011404037475 and perplexity is 33.171833935239896
At time: 189.7762644290924 and batch: 600, loss is 3.5261906385421753 and perplexity is 33.994224364098415
At time: 190.39508652687073 and batch: 650, loss is 3.377476463317871 and perplexity is 29.2967463364065
At time: 191.0124797821045 and batch: 700, loss is 3.357905764579773 and perplexity is 28.728962624068863
At time: 191.62974452972412 and batch: 750, loss is 3.45970534324646 and perplexity is 31.807602808741127
At time: 192.24739742279053 and batch: 800, loss is 3.423491287231445 and perplexity is 30.67632815301444
At time: 192.86511707305908 and batch: 850, loss is 3.480696063041687 and perplexity is 32.4823239539977
At time: 193.48389410972595 and batch: 900, loss is 3.4360629653930665 and perplexity is 31.064415420522085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307944937928082 and perplexity of 74.28766619880348
finished 16 epochs...
Completing Train Step...
At time: 195.0509672164917 and batch: 50, loss is 3.7397636556625367 and perplexity is 42.088041719078554
At time: 195.68352484703064 and batch: 100, loss is 3.627626824378967 and perplexity is 37.6234235948109
At time: 196.31834840774536 and batch: 150, loss is 3.6323868465423583 and perplexity is 37.80293883429242
At time: 196.9521062374115 and batch: 200, loss is 3.508917460441589 and perplexity is 33.412078302715166
At time: 197.57273030281067 and batch: 250, loss is 3.662688136100769 and perplexity is 38.96594798342797
At time: 198.18984627723694 and batch: 300, loss is 3.6176978492736818 and perplexity is 37.25170997993976
At time: 198.80821204185486 and batch: 350, loss is 3.591301794052124 and perplexity is 36.28127594668886
At time: 199.42697644233704 and batch: 400, loss is 3.5392981719970704 and perplexity is 34.44273782836577
At time: 200.04564714431763 and batch: 450, loss is 3.569120078086853 and perplexity is 35.48535506858646
At time: 200.66480159759521 and batch: 500, loss is 3.4602079439163207 and perplexity is 31.823593349311047
At time: 201.29463458061218 and batch: 550, loss is 3.5007516050338747 and perplexity is 33.14035105505444
At time: 201.91826748847961 and batch: 600, loss is 3.5255321645736695 and perplexity is 33.971847420401105
At time: 202.55119514465332 and batch: 650, loss is 3.3769749450683593 and perplexity is 29.282057167218575
At time: 203.19013476371765 and batch: 700, loss is 3.357543611526489 and perplexity is 28.718560226280637
At time: 203.80791091918945 and batch: 750, loss is 3.4593717193603517 and perplexity is 31.796992802658423
At time: 204.42807269096375 and batch: 800, loss is 3.423415923118591 and perplexity is 30.67401634587231
At time: 205.05382800102234 and batch: 850, loss is 3.4808494806289674 and perplexity is 32.48730769605505
At time: 205.6708583831787 and batch: 900, loss is 3.436504240036011 and perplexity is 31.078126384272878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307920691085188 and perplexity of 74.28586497926926
finished 17 epochs...
Completing Train Step...
At time: 207.26756048202515 and batch: 50, loss is 3.738391513824463 and perplexity is 42.030330559156496
At time: 207.887512922287 and batch: 100, loss is 3.626196756362915 and perplexity is 37.56965799346999
At time: 208.51069808006287 and batch: 150, loss is 3.6309285736083985 and perplexity is 37.74785200734273
At time: 209.13750457763672 and batch: 200, loss is 3.507546763420105 and perplexity is 33.36631183964688
At time: 209.7589795589447 and batch: 250, loss is 3.66127646446228 and perplexity is 38.9109796675277
At time: 210.37637853622437 and batch: 300, loss is 3.616373119354248 and perplexity is 37.20239419743762
At time: 210.99344992637634 and batch: 350, loss is 3.590020189285278 and perplexity is 36.23480747395682
At time: 211.62176990509033 and batch: 400, loss is 3.5381322479248047 and perplexity is 34.40260361248762
At time: 212.23993396759033 and batch: 450, loss is 3.5680590581893923 and perplexity is 35.44772436778166
At time: 212.8590931892395 and batch: 500, loss is 3.4592778062820435 and perplexity is 31.794006789398424
At time: 213.48563647270203 and batch: 550, loss is 3.499854340553284 and perplexity is 33.11062873156508
At time: 214.11528038978577 and batch: 600, loss is 3.524876494407654 and perplexity is 33.94958039427319
At time: 214.7402629852295 and batch: 650, loss is 3.3764516925811767 and perplexity is 29.266739265890482
At time: 215.3585705757141 and batch: 700, loss is 3.357127141952515 and perplexity is 28.706602309965035
At time: 215.9796702861786 and batch: 750, loss is 3.4590220069885254 and perplexity is 31.78587494502795
At time: 216.5982644557953 and batch: 800, loss is 3.4232621240615844 and perplexity is 30.669299073848997
At time: 217.21526527404785 and batch: 850, loss is 3.480843858718872 and perplexity is 32.48712505584534
At time: 217.83295369148254 and batch: 900, loss is 3.4367033576965333 and perplexity is 31.084315204223163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307966676476884 and perplexity of 74.2892811224135
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 219.42038941383362 and batch: 50, loss is 3.7381160926818846 and perplexity is 42.01875611148791
At time: 220.04121041297913 and batch: 100, loss is 3.6266249942779543 and perplexity is 37.585750190876446
At time: 220.81046724319458 and batch: 150, loss is 3.631472454071045 and perplexity is 37.76838791058845
At time: 221.43348574638367 and batch: 200, loss is 3.5083196210861205 and perplexity is 33.39210921710521
At time: 222.05955028533936 and batch: 250, loss is 3.66136824131012 and perplexity is 38.91455095846634
At time: 222.68646669387817 and batch: 300, loss is 3.6169171476364137 and perplexity is 37.222638858380044
At time: 223.3205463886261 and batch: 350, loss is 3.5897848653793334 and perplexity is 36.22628156074584
At time: 223.9473717212677 and batch: 400, loss is 3.538117232322693 and perplexity is 34.40208704055849
At time: 224.56698322296143 and batch: 450, loss is 3.5678391599655153 and perplexity is 35.4399303331313
At time: 225.18753743171692 and batch: 500, loss is 3.4584771680831907 and perplexity is 31.76856148067057
At time: 225.8066487312317 and batch: 550, loss is 3.4981137037277223 and perplexity is 33.053045282352485
At time: 226.42931056022644 and batch: 600, loss is 3.5230324983596804 and perplexity is 33.88703518647661
At time: 227.05265641212463 and batch: 650, loss is 3.374803385734558 and perplexity is 29.21853843497325
At time: 227.66956162452698 and batch: 700, loss is 3.3549954223632814 and perplexity is 28.645473061837176
At time: 228.28756761550903 and batch: 750, loss is 3.457176184654236 and perplexity is 31.727257982031844
At time: 228.90464997291565 and batch: 800, loss is 3.4205690574645997 and perplexity is 30.586815725387165
At time: 229.52176690101624 and batch: 850, loss is 3.477672357559204 and perplexity is 32.38425531303633
At time: 230.14080691337585 and batch: 900, loss is 3.4335314321517942 and perplexity is 30.985874276967486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.307116782828553 and perplexity of 74.2261699569367
finished 19 epochs...
Completing Train Step...
At time: 231.73249197006226 and batch: 50, loss is 3.7371390056610108 and perplexity is 41.97772018136022
At time: 232.3545172214508 and batch: 100, loss is 3.6257482624053954 and perplexity is 37.55281200681782
At time: 232.97803258895874 and batch: 150, loss is 3.630698642730713 and perplexity is 37.73917360835412
At time: 233.5992841720581 and batch: 200, loss is 3.50757972240448 and perplexity is 33.367411577520485
At time: 234.22117257118225 and batch: 250, loss is 3.660659031867981 and perplexity is 38.88696217575898
At time: 234.86145186424255 and batch: 300, loss is 3.6161955070495604 and perplexity is 37.19578718122537
At time: 235.4832718372345 and batch: 350, loss is 3.589217014312744 and perplexity is 36.205716267687414
At time: 236.11518812179565 and batch: 400, loss is 3.5375625133514403 and perplexity is 34.38300884223491
At time: 236.74150204658508 and batch: 450, loss is 3.5672836780548094 and perplexity is 35.42024955957748
At time: 237.36228775978088 and batch: 500, loss is 3.4580658721923827 and perplexity is 31.755497888561724
At time: 237.9838387966156 and batch: 550, loss is 3.4978636837005617 and perplexity is 33.04478239206032
At time: 238.60413098335266 and batch: 600, loss is 3.5228631782531736 and perplexity is 33.881297915800985
At time: 239.22537922859192 and batch: 650, loss is 3.374687213897705 and perplexity is 29.21514426085064
At time: 239.84633493423462 and batch: 700, loss is 3.3550560092926025 and perplexity is 28.647208655665562
At time: 240.4683964252472 and batch: 750, loss is 3.4571309089660645 and perplexity is 31.725821541111085
At time: 241.09149169921875 and batch: 800, loss is 3.4207875442504885 and perplexity is 30.59349927055214
At time: 241.71389079093933 and batch: 850, loss is 3.477926354408264 and perplexity is 32.39248185655883
At time: 242.33540797233582 and batch: 900, loss is 3.433987231254578 and perplexity is 31.00000082985239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.306698733813142 and perplexity of 74.19514626483225
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
752.6501519680023


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}, {'params': {'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.19514626483225}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 0.5844906825889364, 'batch_size': 32, 'dropout': 0.5939710512473378, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.865349531173706 and batch: 50, loss is 6.847737712860107 and perplexity is 941.7479905788272
At time: 1.4926791191101074 and batch: 100, loss is 6.003805103302002 and perplexity is 404.96680601953364
At time: 2.1250898838043213 and batch: 150, loss is 5.830616121292114 and perplexity is 340.56844591515426
At time: 2.7590653896331787 and batch: 200, loss is 5.647387475967407 and perplexity is 283.5497168775561
At time: 3.387319564819336 and batch: 250, loss is 5.676787872314453 and perplexity is 292.00994862295596
At time: 4.016992092132568 and batch: 300, loss is 5.570383615493775 and perplexity is 262.53479233932393
At time: 4.657810211181641 and batch: 350, loss is 5.545727071762085 and perplexity is 256.14074325895
At time: 5.285987615585327 and batch: 400, loss is 5.4017727661132815 and perplexity is 221.79926610871203
At time: 5.914694309234619 and batch: 450, loss is 5.400244092941284 and perplexity is 221.4604665439223
At time: 6.554410457611084 and batch: 500, loss is 5.340035638809204 and perplexity is 208.52014156621507
At time: 7.207011938095093 and batch: 550, loss is 5.3930525302886965 and perplexity is 219.8735328298055
At time: 7.845220565795898 and batch: 600, loss is 5.306185131072998 and perplexity is 201.57975934310798
At time: 8.475635528564453 and batch: 650, loss is 5.203154888153076 and perplexity is 181.8450386032281
At time: 9.104724884033203 and batch: 700, loss is 5.286392440795899 and perplexity is 197.6291788774267
At time: 9.745863676071167 and batch: 750, loss is 5.263314332962036 and perplexity is 193.12049740298733
At time: 10.37533187866211 and batch: 800, loss is 5.2396050071716305 and perplexity is 188.59559378444223
At time: 11.003470420837402 and batch: 850, loss is 5.261716213226318 and perplexity is 192.81211420698568
At time: 11.632155656814575 and batch: 900, loss is 5.1797832679748534 and perplexity is 177.64430561072962
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.043224177948416 and perplexity of 154.96885758848816
finished 1 epochs...
Completing Train Step...
At time: 13.209421396255493 and batch: 50, loss is 4.966485376358032 and perplexity is 143.5215755780322
At time: 13.835484504699707 and batch: 100, loss is 4.833402881622314 and perplexity is 125.63776450132097
At time: 14.451488733291626 and batch: 150, loss is 4.81381630897522 and perplexity is 123.2008941735435
At time: 15.076374292373657 and batch: 200, loss is 4.693715658187866 and perplexity is 109.25839336758199
At time: 15.693131685256958 and batch: 250, loss is 4.79182204246521 and perplexity is 120.52076267745193
At time: 16.311720609664917 and batch: 300, loss is 4.722837734222412 and perplexity is 112.48700841521469
At time: 16.92665958404541 and batch: 350, loss is 4.715624351501464 and perplexity is 111.67851606045201
At time: 17.54486918449402 and batch: 400, loss is 4.594753036499023 and perplexity is 98.96369210960064
At time: 18.161534786224365 and batch: 450, loss is 4.614338340759278 and perplexity is 100.92103110353696
At time: 18.77805185317993 and batch: 500, loss is 4.517700119018555 and perplexity is 91.62462972687376
At time: 19.39472007751465 and batch: 550, loss is 4.585007295608521 and perplexity is 98.00390213760406
At time: 20.01137375831604 and batch: 600, loss is 4.5456397914886475 and perplexity is 94.22068952490545
At time: 20.62729048728943 and batch: 650, loss is 4.40372932434082 and perplexity is 81.75519248045997
At time: 21.243050813674927 and batch: 700, loss is 4.440918073654175 and perplexity is 84.85280705191994
At time: 21.85647749900818 and batch: 750, loss is 4.494647750854492 and perplexity is 89.53662423028833
At time: 22.494706630706787 and batch: 800, loss is 4.450349321365357 and perplexity is 85.65686054813087
At time: 23.10980463027954 and batch: 850, loss is 4.5036522388458256 and perplexity is 90.34649645824653
At time: 23.722553730010986 and batch: 900, loss is 4.438575553894043 and perplexity is 84.65427030361366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.565044664356806 and perplexity of 96.06688468632281
finished 2 epochs...
Completing Train Step...
At time: 25.29570746421814 and batch: 50, loss is 4.4739428234100345 and perplexity is 87.7018350567957
At time: 25.909008502960205 and batch: 100, loss is 4.348082475662231 and perplexity is 77.33003843621228
At time: 26.52263593673706 and batch: 150, loss is 4.349122986793518 and perplexity is 77.41054307771255
At time: 27.13724946975708 and batch: 200, loss is 4.241333684921265 and perplexity is 69.50048180003101
At time: 27.750744581222534 and batch: 250, loss is 4.374499616622924 and perplexity is 79.40009912062672
At time: 28.36330008506775 and batch: 300, loss is 4.330113506317138 and perplexity is 75.9529072147031
At time: 28.976226568222046 and batch: 350, loss is 4.336804180145264 and perplexity is 76.46278716131353
At time: 29.58947777748108 and batch: 400, loss is 4.245778193473816 and perplexity is 69.81006474821207
At time: 30.20179772377014 and batch: 450, loss is 4.279744839668274 and perplexity is 72.22200946416078
At time: 30.81450605392456 and batch: 500, loss is 4.169352517127991 and perplexity is 64.67356352314303
At time: 31.427579641342163 and batch: 550, loss is 4.240743813514709 and perplexity is 69.45949754198467
At time: 32.04244875907898 and batch: 600, loss is 4.237334237098694 and perplexity is 69.22307335949573
At time: 32.66093707084656 and batch: 650, loss is 4.086818618774414 and perplexity is 59.55013833320907
At time: 33.27745032310486 and batch: 700, loss is 4.10566554069519 and perplexity is 60.68311819563307
At time: 33.89433550834656 and batch: 750, loss is 4.192915134429931 and perplexity is 66.21553707542033
At time: 34.51217746734619 and batch: 800, loss is 4.162323360443115 and perplexity is 64.22055690423848
At time: 35.12766671180725 and batch: 850, loss is 4.226323790550232 and perplexity is 68.46507700714992
At time: 35.74332618713379 and batch: 900, loss is 4.166244015693665 and perplexity is 64.47283779790146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.430220512494649 and perplexity of 83.9499268771556
finished 3 epochs...
Completing Train Step...
At time: 37.3112735748291 and batch: 50, loss is 4.230823664665222 and perplexity is 68.77385544597297
At time: 37.944891691207886 and batch: 100, loss is 4.111941628456115 and perplexity is 61.06516840730825
At time: 38.57462120056152 and batch: 150, loss is 4.1156982421875 and perplexity is 61.2949980778221
At time: 39.18980145454407 and batch: 200, loss is 4.002057938575745 and perplexity is 54.710625366229685
At time: 39.806103467941284 and batch: 250, loss is 4.151867971420288 and perplexity is 63.55260393737187
At time: 40.42472052574158 and batch: 300, loss is 4.113988575935363 and perplexity is 61.1902936185278
At time: 41.03966927528381 and batch: 350, loss is 4.11857385635376 and perplexity is 61.471512514687696
At time: 41.65352439880371 and batch: 400, loss is 4.045421357154846 and perplexity is 57.13525531152717
At time: 42.26787972450256 and batch: 450, loss is 4.081519856452942 and perplexity is 59.23543082045948
At time: 42.88373398780823 and batch: 500, loss is 3.972455768585205 and perplexity is 53.114808486416976
At time: 43.49922728538513 and batch: 550, loss is 4.036748428344726 and perplexity is 56.641867959054096
At time: 44.11453342437744 and batch: 600, loss is 4.0482566356658936 and perplexity is 57.29747953989541
At time: 44.7320876121521 and batch: 650, loss is 3.8989190244674683 and perplexity is 49.3490751199835
At time: 45.350239992141724 and batch: 700, loss is 3.910898652076721 and perplexity is 49.94381392484876
At time: 45.97619009017944 and batch: 750, loss is 4.0075077724456785 and perplexity is 55.00960313499431
At time: 46.594289779663086 and batch: 800, loss is 3.9841654109954834 and perplexity is 53.740419593247395
At time: 47.20958375930786 and batch: 850, loss is 4.047472615242004 and perplexity is 57.2525747511347
At time: 47.82846784591675 and batch: 900, loss is 3.989993453025818 and perplexity is 54.05453546851998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.387388882571703 and perplexity of 80.4301320384784
finished 4 epochs...
Completing Train Step...
At time: 49.42236375808716 and batch: 50, loss is 4.061128668785095 and perplexity is 58.039781814649125
At time: 50.05587315559387 and batch: 100, loss is 3.9500161170959474 and perplexity is 51.9362038888672
At time: 50.67317509651184 and batch: 150, loss is 3.95711051940918 and perplexity is 52.30597029883659
At time: 51.29145860671997 and batch: 200, loss is 3.8430687379837036 and perplexity is 46.668468223423886
At time: 51.906864404678345 and batch: 250, loss is 3.999558835029602 and perplexity is 54.574068554244114
At time: 52.52049708366394 and batch: 300, loss is 3.9626256704330443 and perplexity is 52.59524258141725
At time: 53.146164417266846 and batch: 350, loss is 3.964970202445984 and perplexity is 52.71869847797722
At time: 53.76113986968994 and batch: 400, loss is 3.9023551082611085 and perplexity is 49.518934335360356
At time: 54.398390769958496 and batch: 450, loss is 3.9374404096603395 and perplexity is 51.28715903629613
At time: 55.01406741142273 and batch: 500, loss is 3.834003210067749 and perplexity is 46.24730583602934
At time: 55.629995584487915 and batch: 550, loss is 3.8905261945724487 and perplexity is 48.936629939183085
At time: 56.24546933174133 and batch: 600, loss is 3.9095948553085327 and perplexity is 49.878739772615454
At time: 56.86142063140869 and batch: 650, loss is 3.763168635368347 and perplexity is 43.08472971470984
At time: 57.47659230232239 and batch: 700, loss is 3.7711565828323366 and perplexity is 43.43026649970153
At time: 58.09262537956238 and batch: 750, loss is 3.87272385597229 and perplexity is 48.0731522487246
At time: 58.70664024353027 and batch: 800, loss is 3.849529209136963 and perplexity is 46.970944533832665
At time: 59.321452617645264 and batch: 850, loss is 3.9129200839996336 and perplexity is 50.04487405334085
At time: 59.936927795410156 and batch: 900, loss is 3.858484973907471 and perplexity is 47.39349456924625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374862043824915 and perplexity of 79.42888109175833
finished 5 epochs...
Completing Train Step...
At time: 61.50073742866516 and batch: 50, loss is 3.9331481075286865 and perplexity is 51.0674908326488
At time: 62.13972091674805 and batch: 100, loss is 3.8302093172073364 and perplexity is 46.072180925235855
At time: 62.76215934753418 and batch: 150, loss is 3.8396743154525756 and perplexity is 46.51032427892314
At time: 63.38857555389404 and batch: 200, loss is 3.7244773864746095 and perplexity is 41.44956498011031
At time: 64.01124501228333 and batch: 250, loss is 3.879195113182068 and perplexity is 48.38525474039729
At time: 64.6372742652893 and batch: 300, loss is 3.848495445251465 and perplexity is 46.922412757226866
At time: 65.26290988922119 and batch: 350, loss is 3.8494044446945193 and perplexity is 46.96508459569041
At time: 65.89001941680908 and batch: 400, loss is 3.790791563987732 and perplexity is 44.29144591727239
At time: 66.5167441368103 and batch: 450, loss is 3.8240631771087648 and perplexity is 45.78988325578546
At time: 67.14362716674805 and batch: 500, loss is 3.7270570039749145 and perplexity is 41.55662703351661
At time: 67.77841067314148 and batch: 550, loss is 3.7803550863265993 and perplexity is 43.83160297550823
At time: 68.40353536605835 and batch: 600, loss is 3.800129814147949 and perplexity is 44.706987716140205
At time: 69.02798366546631 and batch: 650, loss is 3.658408498764038 and perplexity is 38.799544185508076
At time: 69.67730855941772 and batch: 700, loss is 3.6668412637710572 and perplexity is 39.12811505738356
At time: 70.30280685424805 and batch: 750, loss is 3.7667065811157228 and perplexity is 43.237431116337056
At time: 70.92760229110718 and batch: 800, loss is 3.7438357496261596 and perplexity is 42.259777604725585
At time: 71.55453181266785 and batch: 850, loss is 3.808695020675659 and perplexity is 45.09155690534027
At time: 72.18250370025635 and batch: 900, loss is 3.7572772455215455 and perplexity is 42.83164701195822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.371390982849957 and perplexity of 79.15365653924941
finished 6 epochs...
Completing Train Step...
At time: 73.8435389995575 and batch: 50, loss is 3.8317680311203004 and perplexity is 46.14405027196887
At time: 74.47274661064148 and batch: 100, loss is 3.735180993080139 and perplexity is 41.895607692006074
At time: 75.10174298286438 and batch: 150, loss is 3.747351384162903 and perplexity is 42.40860900349827
At time: 75.72936391830444 and batch: 200, loss is 3.6329287672042847 and perplexity is 37.823430579876934
At time: 76.35661578178406 and batch: 250, loss is 3.7824784183502196 and perplexity is 43.924770899967264
At time: 76.98633027076721 and batch: 300, loss is 3.759346923828125 and perplexity is 42.92038654207733
At time: 77.62233972549438 and batch: 350, loss is 3.7547581243515014 and perplexity is 42.72388469337961
At time: 78.24451899528503 and batch: 400, loss is 3.696162614822388 and perplexity is 40.29238989717324
At time: 78.86401128768921 and batch: 450, loss is 3.734078345298767 and perplexity is 41.849437052789675
At time: 79.48551082611084 and batch: 500, loss is 3.642803502082825 and perplexity is 38.19877710274002
At time: 80.10881471633911 and batch: 550, loss is 3.695900459289551 and perplexity is 40.28182840866728
At time: 80.7355272769928 and batch: 600, loss is 3.7112737607955935 and perplexity is 40.90587766013879
At time: 81.37049317359924 and batch: 650, loss is 3.5751614427566527 and perplexity is 35.70038391883114
At time: 81.99702978134155 and batch: 700, loss is 3.5817528533935548 and perplexity is 35.93647704772755
At time: 82.62660264968872 and batch: 750, loss is 3.680039701461792 and perplexity is 39.647968123617915
At time: 83.25422477722168 and batch: 800, loss is 3.660831503868103 and perplexity is 38.893669666314636
At time: 83.8778326511383 and batch: 850, loss is 3.723091096878052 and perplexity is 41.39214368986121
At time: 84.50022435188293 and batch: 900, loss is 3.6757374715805056 and perplexity is 39.477759850518055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3793966214950775 and perplexity of 79.78987538207161
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 86.1384871006012 and batch: 50, loss is 3.7916693353652953 and perplexity is 44.33034074865701
At time: 86.76227855682373 and batch: 100, loss is 3.69408287525177 and perplexity is 40.208679297805226
At time: 87.38726377487183 and batch: 150, loss is 3.7034119033813475 and perplexity is 40.58554234780548
At time: 88.02847170829773 and batch: 200, loss is 3.573929100036621 and perplexity is 35.656415907999865
At time: 88.654869556427 and batch: 250, loss is 3.715759916305542 and perplexity is 41.08980003231267
At time: 89.28051948547363 and batch: 300, loss is 3.6758235120773315 and perplexity is 39.48115668271964
At time: 89.91051411628723 and batch: 350, loss is 3.6576733112335207 and perplexity is 38.77102972745447
At time: 90.53329110145569 and batch: 400, loss is 3.595133457183838 and perplexity is 36.42056024895809
At time: 91.15588283538818 and batch: 450, loss is 3.6227093505859376 and perplexity is 37.43886554686952
At time: 91.77853298187256 and batch: 500, loss is 3.520303978919983 and perplexity is 33.79469977897879
At time: 92.40146517753601 and batch: 550, loss is 3.556714949607849 and perplexity is 35.04787379575266
At time: 93.02423286437988 and batch: 600, loss is 3.5660310220718383 and perplexity is 35.37590795023767
At time: 93.65638661384583 and batch: 650, loss is 3.4145182371139526 and perplexity is 30.40229919947565
At time: 94.2870135307312 and batch: 700, loss is 3.4001304864883424 and perplexity is 29.96801021269486
At time: 94.92046618461609 and batch: 750, loss is 3.486547989845276 and perplexity is 32.672965402043324
At time: 95.55262684822083 and batch: 800, loss is 3.4481013011932373 and perplexity is 31.44063929589694
At time: 96.18474340438843 and batch: 850, loss is 3.499366054534912 and perplexity is 33.0944652210252
At time: 96.82063484191895 and batch: 900, loss is 3.4434184503555296 and perplexity is 31.293751666686763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337582104826627 and perplexity of 76.52229259302321
finished 8 epochs...
Completing Train Step...
At time: 98.4558458328247 and batch: 50, loss is 3.703384938240051 and perplexity is 40.5844479676766
At time: 99.10423183441162 and batch: 100, loss is 3.5996644067764283 and perplexity is 36.58595438487151
At time: 99.7348062992096 and batch: 150, loss is 3.6077511978149412 and perplexity is 36.88301687517429
At time: 100.36274862289429 and batch: 200, loss is 3.4834142827987673 and perplexity is 32.57073815890567
At time: 100.9908754825592 and batch: 250, loss is 3.627044825553894 and perplexity is 37.601533177200054
At time: 101.64129638671875 and batch: 300, loss is 3.5938997983932497 and perplexity is 36.375657407736064
At time: 102.26808524131775 and batch: 350, loss is 3.579171872138977 and perplexity is 35.84384526597988
At time: 102.88396716117859 and batch: 400, loss is 3.52210343837738 and perplexity is 33.85556671847672
At time: 103.50023818016052 and batch: 450, loss is 3.5531725549697875 and perplexity is 34.92394003628478
At time: 104.12174344062805 and batch: 500, loss is 3.4560299158096313 and perplexity is 31.69091085045437
At time: 104.74119400978088 and batch: 550, loss is 3.4973569440841676 and perplexity is 33.02804153369345
At time: 105.3594617843628 and batch: 600, loss is 3.511927270889282 and perplexity is 33.51279381634093
At time: 105.97850847244263 and batch: 650, loss is 3.366335391998291 and perplexity is 28.972160669458013
At time: 106.59979033470154 and batch: 700, loss is 3.3564283514022826 and perplexity is 28.6865494147444
At time: 107.2238335609436 and batch: 750, loss is 3.4504505300521853 and perplexity is 31.514587379551536
At time: 107.85084199905396 and batch: 800, loss is 3.4153407192230225 and perplexity is 30.42731483268698
At time: 108.48266839981079 and batch: 850, loss is 3.474499878883362 and perplexity is 32.28167974904939
At time: 109.10747456550598 and batch: 900, loss is 3.426448645591736 and perplexity is 30.76718332797837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344199402691567 and perplexity of 77.03034250130656
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 110.69734215736389 and batch: 50, loss is 3.6877366495132446 and perplexity is 39.954313926072246
At time: 111.32911205291748 and batch: 100, loss is 3.5982875347137453 and perplexity is 36.535614869881016
At time: 111.95271563529968 and batch: 150, loss is 3.604361138343811 and perplexity is 36.75819295429527
At time: 112.57720756530762 and batch: 200, loss is 3.4774308252334594 and perplexity is 32.37643441307115
At time: 113.19589352607727 and batch: 250, loss is 3.620087161064148 and perplexity is 37.34082234602955
At time: 113.81480503082275 and batch: 300, loss is 3.5758444833755494 and perplexity is 35.724777060962985
At time: 114.43339610099792 and batch: 350, loss is 3.5608389282226565 and perplexity is 35.192708920990455
At time: 115.05250406265259 and batch: 400, loss is 3.500299906730652 and perplexity is 33.12538499504108
At time: 115.67209219932556 and batch: 450, loss is 3.5237159538269043 and perplexity is 33.91020338225601
At time: 116.29022455215454 and batch: 500, loss is 3.4262804126739503 and perplexity is 30.7620077103219
At time: 116.90796089172363 and batch: 550, loss is 3.463136034011841 and perplexity is 31.916912254207496
At time: 117.54918384552002 and batch: 600, loss is 3.481563005447388 and perplexity is 32.51049646827827
At time: 118.18026518821716 and batch: 650, loss is 3.3286704444885253 and perplexity is 27.901220809656103
At time: 118.80797266960144 and batch: 700, loss is 3.308630962371826 and perplexity is 27.347659856450285
At time: 119.43556928634644 and batch: 750, loss is 3.3992529630661013 and perplexity is 29.94172411683445
At time: 120.06331133842468 and batch: 800, loss is 3.3579548454284667 and perplexity is 28.730372700540137
At time: 120.6814239025116 and batch: 850, loss is 3.4092894983291626 and perplexity is 30.24374839011528
At time: 121.30633878707886 and batch: 900, loss is 3.37100821018219 and perplexity is 29.107859108811443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.330213990930009 and perplexity of 75.96053969664803
finished 10 epochs...
Completing Train Step...
At time: 122.90917110443115 and batch: 50, loss is 3.66045045375824 and perplexity is 38.878852052521275
At time: 123.53546380996704 and batch: 100, loss is 3.559870581626892 and perplexity is 35.15864667580234
At time: 124.159348487854 and batch: 150, loss is 3.567819437980652 and perplexity is 35.439231394253966
At time: 124.77669954299927 and batch: 200, loss is 3.442798709869385 and perplexity is 31.274363670199282
At time: 125.39639949798584 and batch: 250, loss is 3.5858331727981567 and perplexity is 36.08340891297315
At time: 126.01444268226624 and batch: 300, loss is 3.5447344827651976 and perplexity is 34.630489129724836
At time: 126.63331460952759 and batch: 350, loss is 3.531575794219971 and perplexity is 34.177782355026764
At time: 127.25923943519592 and batch: 400, loss is 3.4739342641830446 and perplexity is 32.26342591923628
At time: 127.87677693367004 and batch: 450, loss is 3.4992287826538084 and perplexity is 33.08992259332486
At time: 128.50600790977478 and batch: 500, loss is 3.404064350128174 and perplexity is 30.086132464513188
At time: 129.13247203826904 and batch: 550, loss is 3.4425069808959963 and perplexity is 31.265241362877767
At time: 129.75046277046204 and batch: 600, loss is 3.464317898750305 and perplexity is 31.954656026985084
At time: 130.3687925338745 and batch: 650, loss is 3.314602088928223 and perplexity is 27.511444697845853
At time: 130.9869396686554 and batch: 700, loss is 3.2971646976470947 and perplexity is 27.035875267241106
At time: 131.60513925552368 and batch: 750, loss is 3.39147340297699 and perplexity is 29.709694389730327
At time: 132.22269105911255 and batch: 800, loss is 3.352832388877869 and perplexity is 28.58357890809457
At time: 132.8399784564972 and batch: 850, loss is 3.407785973548889 and perplexity is 30.19831033214517
At time: 133.48204708099365 and batch: 900, loss is 3.371922836303711 and perplexity is 29.134494095764545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.33158937219071 and perplexity of 76.06508627872384
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 135.09280467033386 and batch: 50, loss is 3.6574197959899903 and perplexity is 38.761201926212536
At time: 135.7080078125 and batch: 100, loss is 3.562258415222168 and perplexity is 35.24269998622089
At time: 136.32510209083557 and batch: 150, loss is 3.570154790878296 and perplexity is 35.522091221791065
At time: 136.94018959999084 and batch: 200, loss is 3.444718999862671 and perplexity is 31.33447721705033
At time: 137.56034779548645 and batch: 250, loss is 3.5877031230926515 and perplexity is 36.15094622012014
At time: 138.17572379112244 and batch: 300, loss is 3.540411729812622 and perplexity is 34.481113170879354
At time: 138.78999781608582 and batch: 350, loss is 3.5267683029174806 and perplexity is 34.01386728944256
At time: 139.4039430618286 and batch: 400, loss is 3.4720384550094603 and perplexity is 32.20231856265305
At time: 140.0203788280487 and batch: 450, loss is 3.492811288833618 and perplexity is 32.878248155739335
At time: 140.6366319656372 and batch: 500, loss is 3.396339020729065 and perplexity is 29.854602654391606
At time: 141.25286102294922 and batch: 550, loss is 3.4341413593292236 and perplexity is 31.004779168522923
At time: 141.87506008148193 and batch: 600, loss is 3.459735879898071 and perplexity is 31.808574121256914
At time: 142.49350214004517 and batch: 650, loss is 3.305135407447815 and perplexity is 27.252231494089006
At time: 143.11284756660461 and batch: 700, loss is 3.280007743835449 and perplexity is 26.57597849908152
At time: 143.73052215576172 and batch: 750, loss is 3.373234362602234 and perplexity is 29.17272981906286
At time: 144.34689497947693 and batch: 800, loss is 3.334517002105713 and perplexity is 28.06482469846215
At time: 144.9607663154602 and batch: 850, loss is 3.383804144859314 and perplexity is 29.482714570799068
At time: 145.57850337028503 and batch: 900, loss is 3.350831637382507 and perplexity is 28.526447441842812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3253173828125 and perplexity of 75.58949986105564
finished 12 epochs...
Completing Train Step...
At time: 147.16332268714905 and batch: 50, loss is 3.6436952781677245 and perplexity is 38.23285705221606
At time: 147.8024387359619 and batch: 100, loss is 3.545947427749634 and perplexity is 34.67251949292939
At time: 148.4207799434662 and batch: 150, loss is 3.556501808166504 and perplexity is 35.04040443745892
At time: 149.0522017478943 and batch: 200, loss is 3.431587300300598 and perplexity is 30.92569217177786
At time: 149.68348145484924 and batch: 250, loss is 3.5750523090362547 and perplexity is 35.69648801570551
At time: 150.30862975120544 and batch: 300, loss is 3.5294867610931395 and perplexity is 34.10645836051763
At time: 150.9320616722107 and batch: 350, loss is 3.5170653104782104 and perplexity is 33.68542699597561
At time: 151.56185960769653 and batch: 400, loss is 3.4627756881713867 and perplexity is 31.905413199579232
At time: 152.18520593643188 and batch: 450, loss is 3.484505615234375 and perplexity is 32.606303064948094
At time: 152.81411361694336 and batch: 500, loss is 3.3891125297546387 and perplexity is 29.639636299476376
At time: 153.43523716926575 and batch: 550, loss is 3.4270921325683594 and perplexity is 30.78698798109211
At time: 154.05922436714172 and batch: 600, loss is 3.4542583274841308 and perplexity is 31.634817304773474
At time: 154.68832349777222 and batch: 650, loss is 3.3012096786499026 and perplexity is 27.145456346370466
At time: 155.31865048408508 and batch: 700, loss is 3.277545442581177 and perplexity is 26.51062093192103
At time: 155.94046664237976 and batch: 750, loss is 3.3717640113830565 and perplexity is 29.12986717949601
At time: 156.55971240997314 and batch: 800, loss is 3.334744162559509 and perplexity is 28.071200640929938
At time: 157.19148898124695 and batch: 850, loss is 3.3860253953933714 and perplexity is 29.548275853345498
At time: 157.8074266910553 and batch: 900, loss is 3.354782943725586 and perplexity is 28.6393871573301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32505233973673 and perplexity of 75.56946804228124
finished 13 epochs...
Completing Train Step...
At time: 159.38873052597046 and batch: 50, loss is 3.6379778814315795 and perplexity is 38.014888340650074
At time: 160.02574038505554 and batch: 100, loss is 3.5392002868652344 and perplexity is 34.43936656143379
At time: 160.64742803573608 and batch: 150, loss is 3.5497124671936033 and perplexity is 34.80330895568442
At time: 161.26947784423828 and batch: 200, loss is 3.4246370649337767 and perplexity is 30.711496549536406
At time: 161.8898069858551 and batch: 250, loss is 3.568351263999939 and perplexity is 35.45808391229843
At time: 162.51468324661255 and batch: 300, loss is 3.5232833576202394 and perplexity is 33.895537129417
At time: 163.15270519256592 and batch: 350, loss is 3.511174273490906 and perplexity is 33.48756826836282
At time: 163.7754933834076 and batch: 400, loss is 3.4572550201416017 and perplexity is 31.72975931447322
At time: 164.39693117141724 and batch: 450, loss is 3.4793207931518553 and perplexity is 32.43768269583719
At time: 165.0350217819214 and batch: 500, loss is 3.384553327560425 and perplexity is 29.504810786555105
At time: 165.65623450279236 and batch: 550, loss is 3.4228250217437743 and perplexity is 30.65589638152554
At time: 166.27832198143005 and batch: 600, loss is 3.4508244609832763 and perplexity is 31.52637386208118
At time: 166.90572118759155 and batch: 650, loss is 3.2985341930389405 and perplexity is 27.07292613854207
At time: 167.5365891456604 and batch: 700, loss is 3.2755230712890624 and perplexity is 26.457060790713186
At time: 168.15818691253662 and batch: 750, loss is 3.3703812885284425 and perplexity is 29.089616480588717
At time: 168.77932572364807 and batch: 800, loss is 3.334090986251831 and perplexity is 28.05287118457819
At time: 169.39895606040955 and batch: 850, loss is 3.3861608362197875 and perplexity is 29.552278167278477
At time: 170.02022290229797 and batch: 900, loss is 3.3555061483383177 and perplexity is 28.660106785590386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3254983980361725 and perplexity of 75.60318394975711
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 171.62228345870972 and batch: 50, loss is 3.636537818908691 and perplexity is 37.960183922983525
At time: 172.24560546875 and batch: 100, loss is 3.5388139343261718 and perplexity is 34.42606339474067
At time: 172.86862134933472 and batch: 150, loss is 3.550792932510376 and perplexity is 34.840933046022094
At time: 173.49268460273743 and batch: 200, loss is 3.425382776260376 and perplexity is 30.734407001600328
At time: 174.114737033844 and batch: 250, loss is 3.5693228340148924 and perplexity is 35.49255066413538
At time: 174.73722982406616 and batch: 300, loss is 3.5227015304565428 and perplexity is 33.87582152128138
At time: 175.36649537086487 and batch: 350, loss is 3.510739245414734 and perplexity is 33.473003404255465
At time: 175.99427700042725 and batch: 400, loss is 3.457608208656311 and perplexity is 31.74096788028942
At time: 176.61872577667236 and batch: 450, loss is 3.4786789226531982 and perplexity is 32.41686858496641
At time: 177.24344539642334 and batch: 500, loss is 3.381160559654236 and perplexity is 29.404877432638262
At time: 177.86884903907776 and batch: 550, loss is 3.418437113761902 and perplexity is 30.521675818196055
At time: 178.49392819404602 and batch: 600, loss is 3.4477168226242068 and perplexity is 31.428553367430172
At time: 179.11812376976013 and batch: 650, loss is 3.293057765960693 and perplexity is 26.925068468160692
At time: 179.75049901008606 and batch: 700, loss is 3.2698641633987426 and perplexity is 26.307765543086905
At time: 180.3828101158142 and batch: 750, loss is 3.3633519506454466 and perplexity is 28.88585273857887
At time: 181.02585744857788 and batch: 800, loss is 3.3256044292449953 and perplexity is 27.81580624942371
At time: 181.65942120552063 and batch: 850, loss is 3.37681836605072 and perplexity is 29.277472570408076
At time: 182.2824990749359 and batch: 900, loss is 3.344807729721069 and perplexity is 28.35512329504079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325504250722389 and perplexity of 75.6036264327646
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 183.86081743240356 and batch: 50, loss is 3.6340082931518554 and perplexity is 37.86428400181109
At time: 184.4758186340332 and batch: 100, loss is 3.536835355758667 and perplexity is 34.358016064238285
At time: 185.09109473228455 and batch: 150, loss is 3.548887376785278 and perplexity is 34.7746049226267
At time: 185.70750164985657 and batch: 200, loss is 3.424459991455078 and perplexity is 30.706058839457672
At time: 186.3240249156952 and batch: 250, loss is 3.5682638549804686 and perplexity is 35.454984691403254
At time: 186.94014358520508 and batch: 300, loss is 3.5210407638549803 and perplexity is 33.81960837967276
At time: 187.55814337730408 and batch: 350, loss is 3.5088123989105227 and perplexity is 33.40856816300612
At time: 188.17448830604553 and batch: 400, loss is 3.456406216621399 and perplexity is 31.702838409967466
At time: 188.79083013534546 and batch: 450, loss is 3.4779431009292603 and perplexity is 32.39302432247855
At time: 189.40647077560425 and batch: 500, loss is 3.379909176826477 and perplexity is 29.36810368778197
At time: 190.02243375778198 and batch: 550, loss is 3.4159532165527344 and perplexity is 30.44595719038665
At time: 190.6378149986267 and batch: 600, loss is 3.445721788406372 and perplexity is 31.365914831831997
At time: 191.2536654472351 and batch: 650, loss is 3.2910411834716795 and perplexity is 26.87082655659852
At time: 191.8691611289978 and batch: 700, loss is 3.2681486940383913 and perplexity is 26.26267406494108
At time: 192.48528361320496 and batch: 750, loss is 3.36159255027771 and perplexity is 28.83507564036919
At time: 193.10013842582703 and batch: 800, loss is 3.3231802654266356 and perplexity is 27.748457843101733
At time: 193.71645188331604 and batch: 850, loss is 3.3744374990463255 and perplexity is 29.207849716259474
At time: 194.3314950466156 and batch: 900, loss is 3.341666202545166 and perplexity is 28.266184679318492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.325017223619435 and perplexity of 75.56681438257094
finished 16 epochs...
Completing Train Step...
At time: 195.90744423866272 and batch: 50, loss is 3.6330277729034424 and perplexity is 37.82717550044721
At time: 196.53742218017578 and batch: 100, loss is 3.5358505630493164 and perplexity is 34.32419719553132
At time: 197.1633951663971 and batch: 150, loss is 3.5477493524551393 and perplexity is 34.73505308589866
At time: 197.78578519821167 and batch: 200, loss is 3.4234793376922608 and perplexity is 30.67596158721929
At time: 198.41049337387085 and batch: 250, loss is 3.5673469734191894 and perplexity is 35.42249156813341
At time: 199.0341022014618 and batch: 300, loss is 3.5199144744873045 and perplexity is 33.78153915684055
At time: 199.64993381500244 and batch: 350, loss is 3.5078817129135134 and perplexity is 33.37748974080614
At time: 200.27548694610596 and batch: 400, loss is 3.4555270624160768 and perplexity is 31.674978974423063
At time: 200.89734625816345 and batch: 450, loss is 3.4771398639678956 and perplexity is 32.36701549507649
At time: 201.524395942688 and batch: 500, loss is 3.3794317197799684 and perplexity is 29.354085026654225
At time: 202.14266657829285 and batch: 550, loss is 3.415532445907593 and perplexity is 30.433149120156656
At time: 202.7578945159912 and batch: 600, loss is 3.445362672805786 and perplexity is 31.354652864784295
At time: 203.37478399276733 and batch: 650, loss is 3.2908747959136964 and perplexity is 26.86635595732314
At time: 203.99503350257874 and batch: 700, loss is 3.2680115127563476 and perplexity is 26.259071564746407
At time: 204.62692379951477 and batch: 750, loss is 3.361502170562744 and perplexity is 28.832469652217803
At time: 205.25008702278137 and batch: 800, loss is 3.3233240270614623 and perplexity is 27.752447293523264
At time: 205.86627745628357 and batch: 850, loss is 3.374881591796875 and perplexity is 29.220823591166273
At time: 206.48265600204468 and batch: 900, loss is 3.342499294281006 and perplexity is 28.289742815860464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.32478269812179 and perplexity of 75.54909411583081
finished 17 epochs...
Completing Train Step...
At time: 208.04435849189758 and batch: 50, loss is 3.6322841882705688 and perplexity is 37.79905824911362
At time: 208.68289518356323 and batch: 100, loss is 3.53505606174469 and perplexity is 34.29693740648213
At time: 209.31076383590698 and batch: 150, loss is 3.546869144439697 and perplexity is 34.70449246558024
At time: 209.93517780303955 and batch: 200, loss is 3.4227040433883666 and perplexity is 30.652187905924915
At time: 210.55143332481384 and batch: 250, loss is 3.566582188606262 and perplexity is 35.395411341139145
At time: 211.1650514602661 and batch: 300, loss is 3.5190579652786256 and perplexity is 33.75261734513651
At time: 211.7801969051361 and batch: 350, loss is 3.5071442031860354 and perplexity is 33.35288259256446
At time: 212.40824437141418 and batch: 400, loss is 3.454818992614746 and perplexity is 31.652558816808476
At time: 213.02404069900513 and batch: 450, loss is 3.4764885807037356 and perplexity is 32.34594226264042
At time: 213.63921284675598 and batch: 500, loss is 3.3790256690979006 and perplexity is 29.342168199989207
At time: 214.2541172504425 and batch: 550, loss is 3.41514741897583 and perplexity is 30.421433793629888
At time: 214.8692605495453 and batch: 600, loss is 3.4450721311569215 and perplexity is 31.345544355506114
At time: 215.48400902748108 and batch: 650, loss is 3.2907355785369874 and perplexity is 26.862615954067792
At time: 216.09971165657043 and batch: 700, loss is 3.267937755584717 and perplexity is 26.25713484132264
At time: 216.7203710079193 and batch: 750, loss is 3.3614457035064698 and perplexity is 28.83084161349713
At time: 217.3500337600708 and batch: 800, loss is 3.3234587860107423 and perplexity is 27.75618743616367
At time: 217.97114634513855 and batch: 850, loss is 3.375236349105835 and perplexity is 29.23119173088701
At time: 218.58692836761475 and batch: 900, loss is 3.34316668510437 and perplexity is 28.308629432287322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324657701466181 and perplexity of 75.53965132190322
finished 18 epochs...
Completing Train Step...
At time: 220.17116594314575 and batch: 50, loss is 3.6316638469696043 and perplexity is 37.77561720362018
At time: 220.78721380233765 and batch: 100, loss is 3.5343666553497313 and perplexity is 34.27330102697836
At time: 221.40339374542236 and batch: 150, loss is 3.546127724647522 and perplexity is 34.67877140421909
At time: 222.01949071884155 and batch: 200, loss is 3.422034726142883 and perplexity is 30.631678732285135
At time: 222.634624004364 and batch: 250, loss is 3.565911636352539 and perplexity is 35.371684824116656
At time: 223.24967503547668 and batch: 300, loss is 3.5183495616912843 and perplexity is 33.728715337060756
At time: 223.8659691810608 and batch: 350, loss is 3.5065186977386475 and perplexity is 33.33202670623198
At time: 224.48623895645142 and batch: 400, loss is 3.454213705062866 and perplexity is 31.6334057141236
At time: 225.1100616455078 and batch: 450, loss is 3.475929789543152 and perplexity is 32.32787268505361
At time: 225.73670625686646 and batch: 500, loss is 3.3786619186401365 and perplexity is 29.331496913834947
At time: 226.37311506271362 and batch: 550, loss is 3.4147925043106078 and perplexity is 30.41063869642191
At time: 227.00737810134888 and batch: 600, loss is 3.444818367958069 and perplexity is 31.337591019075415
At time: 227.6371066570282 and batch: 650, loss is 3.290608959197998 and perplexity is 26.859214842719947
At time: 228.26701521873474 and batch: 700, loss is 3.267886924743652 and perplexity is 26.25580020299539
At time: 228.88329482078552 and batch: 750, loss is 3.3614028453826905 and perplexity is 28.82960600419674
At time: 229.50474381446838 and batch: 800, loss is 3.323576545715332 and perplexity is 27.759456189056603
At time: 230.12248611450195 and batch: 850, loss is 3.375525851249695 and perplexity is 29.23965544863378
At time: 230.74051690101624 and batch: 900, loss is 3.343707857131958 and perplexity is 28.32395341675731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324585797035531 and perplexity of 75.53421988155803
finished 19 epochs...
Completing Train Step...
At time: 232.336332321167 and batch: 50, loss is 3.631117148399353 and perplexity is 37.75497097185152
At time: 232.95377349853516 and batch: 100, loss is 3.533745541572571 and perplexity is 34.25202001717008
At time: 233.57205319404602 and batch: 150, loss is 3.5454725074768065 and perplexity is 34.65605672007391
At time: 234.19041442871094 and batch: 200, loss is 3.4214306449890137 and perplexity is 30.61318030029118
At time: 234.8080382347107 and batch: 250, loss is 3.565306906700134 and perplexity is 35.350300983820816
At time: 235.42344689369202 and batch: 300, loss is 3.517732458114624 and perplexity is 33.70790764710038
At time: 236.04122161865234 and batch: 350, loss is 3.505965647697449 and perplexity is 33.31359752408857
At time: 236.67093563079834 and batch: 400, loss is 3.4536777782440184 and perplexity is 31.616457065653208
At time: 237.2907383441925 and batch: 450, loss is 3.475434408187866 and perplexity is 32.311862025687866
At time: 237.92349648475647 and batch: 500, loss is 3.3783263206481933 and perplexity is 29.32165497392998
At time: 238.55740785598755 and batch: 550, loss is 3.414462456703186 and perplexity is 30.40060339403485
At time: 239.18356776237488 and batch: 600, loss is 3.444586548805237 and perplexity is 31.330327207251457
At time: 239.80681657791138 and batch: 650, loss is 3.2904882860183715 and perplexity is 26.855973851416895
At time: 240.42373704910278 and batch: 700, loss is 3.267842378616333 and perplexity is 26.254630634826736
At time: 241.04150295257568 and batch: 750, loss is 3.3613648557662965 and perplexity is 28.828510799327187
At time: 241.65848326683044 and batch: 800, loss is 3.3236761808395388 and perplexity is 27.76222214371274
At time: 242.2782337665558 and batch: 850, loss is 3.375765404701233 and perplexity is 29.246660748056613
At time: 242.89545392990112 and batch: 900, loss is 3.3441515827178954 and perplexity is 28.336524268381215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.324546082379067 and perplexity of 75.5312201255317
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
1002.4855494499207


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}, {'params': {'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.19514626483225}, {'params': {'num_layers': 1, 'rnn_dropout': 0.5844906825889364, 'batch_size': 32, 'dropout': 0.5939710512473378, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.5312201255317}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 0.16540051295511726, 'batch_size': 32, 'dropout': 0.5337025234107475, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8528499603271484 and batch: 50, loss is 6.88689395904541 and perplexity is 979.3547721341072
At time: 1.4876933097839355 and batch: 100, loss is 6.003159093856811 and perplexity is 404.7052781216975
At time: 2.113926410675049 and batch: 150, loss is 5.79635947227478 and perplexity is 329.099281353502
At time: 2.749162435531616 and batch: 200, loss is 5.596445693969726 and perplexity is 269.46693535426937
At time: 3.3737010955810547 and batch: 250, loss is 5.620855588912963 and perplexity is 276.12553214106964
At time: 3.9979076385498047 and batch: 300, loss is 5.517805643081665 and perplexity is 249.08784940100105
At time: 4.6218883991241455 and batch: 350, loss is 5.488834552764892 and perplexity is 241.97503332715613
At time: 5.249842166900635 and batch: 400, loss is 5.342018203735352 and perplexity is 208.93395635708094
At time: 5.877346754074097 and batch: 450, loss is 5.335690727233887 and perplexity is 207.61610539027836
At time: 6.51584792137146 and batch: 500, loss is 5.273449211120606 and perplexity is 195.08770196481618
At time: 7.148246765136719 and batch: 550, loss is 5.32292140007019 and perplexity is 204.98184209523237
At time: 7.790382623672485 and batch: 600, loss is 5.232768325805664 and perplexity is 187.31062327466176
At time: 8.422849893569946 and batch: 650, loss is 5.128994741439819 and perplexity is 168.84729751122902
At time: 9.048896312713623 and batch: 700, loss is 5.205922536849975 and perplexity is 182.34901888583983
At time: 9.675855875015259 and batch: 750, loss is 5.18961353302002 and perplexity is 179.39920766287085
At time: 10.301817178726196 and batch: 800, loss is 5.162822856903076 and perplexity is 174.65679151085882
At time: 10.92832088470459 and batch: 850, loss is 5.1869305229187015 and perplexity is 178.91852290592456
At time: 11.556328535079956 and batch: 900, loss is 5.104084501266479 and perplexity is 164.69322505379887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.971787962194991 and perplexity of 144.28463235107782
finished 1 epochs...
Completing Train Step...
At time: 13.161153078079224 and batch: 50, loss is 4.921764936447143 and perplexity is 137.2446275990415
At time: 13.798159122467041 and batch: 100, loss is 4.788109788894653 and perplexity is 120.07418845709506
At time: 14.42617678642273 and batch: 150, loss is 4.771732358932495 and perplexity is 118.12369745681478
At time: 15.050161838531494 and batch: 200, loss is 4.653043251037598 and perplexity is 104.90374873512917
At time: 15.674601793289185 and batch: 250, loss is 4.756399908065796 and perplexity is 116.32638547797936
At time: 16.304590702056885 and batch: 300, loss is 4.69130389213562 and perplexity is 108.99520518526887
At time: 16.93260407447815 and batch: 350, loss is 4.682335357666016 and perplexity is 108.02204834880814
At time: 17.55810236930847 and batch: 400, loss is 4.5654868888854985 and perplexity is 96.10937721405267
At time: 18.19158411026001 and batch: 450, loss is 4.588992547988892 and perplexity is 98.39525171734697
At time: 18.814406871795654 and batch: 500, loss is 4.48741307258606 and perplexity is 88.89119311914102
At time: 19.437414407730103 and batch: 550, loss is 4.560146894454956 and perplexity is 95.59752154453845
At time: 20.0751051902771 and batch: 600, loss is 4.519240808486939 and perplexity is 91.76590363060092
At time: 20.708754301071167 and batch: 650, loss is 4.37658462524414 and perplexity is 79.56582171830445
At time: 21.339832067489624 and batch: 700, loss is 4.419012413024903 and perplexity is 83.01426105897646
At time: 21.96466064453125 and batch: 750, loss is 4.46902533531189 and perplexity is 87.27162097996127
At time: 22.588539600372314 and batch: 800, loss is 4.424775123596191 and perplexity is 83.49402927371904
At time: 23.211754083633423 and batch: 850, loss is 4.487742862701416 and perplexity is 88.92051339047602
At time: 23.834218978881836 and batch: 900, loss is 4.420661659240722 and perplexity is 83.15128497691722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.554016949379281 and perplexity of 95.01330642079242
finished 2 epochs...
Completing Train Step...
At time: 25.428653955459595 and batch: 50, loss is 4.460486259460449 and perplexity is 86.52957469240633
At time: 26.045916080474854 and batch: 100, loss is 4.332243733406067 and perplexity is 76.11487660968146
At time: 26.66168785095215 and batch: 150, loss is 4.335457792282105 and perplexity is 76.35990786595482
At time: 27.278352737426758 and batch: 200, loss is 4.223246741294861 and perplexity is 68.25473038235623
At time: 27.907642364501953 and batch: 250, loss is 4.365046920776368 and perplexity is 78.65309031926004
At time: 28.54002594947815 and batch: 300, loss is 4.320064444541931 and perplexity is 75.19347394486803
At time: 29.161815643310547 and batch: 350, loss is 4.31692346572876 and perplexity is 74.9576633682017
At time: 29.77571964263916 and batch: 400, loss is 4.238518991470337 and perplexity is 69.30513429970708
At time: 30.391695022583008 and batch: 450, loss is 4.270905117988587 and perplexity is 71.58640044367367
At time: 31.00707483291626 and batch: 500, loss is 4.152692351341248 and perplexity is 63.605017029164415
At time: 31.621723651885986 and batch: 550, loss is 4.22988377571106 and perplexity is 68.70924602650027
At time: 32.244404792785645 and batch: 600, loss is 4.228419470787048 and perplexity is 68.60870836607843
At time: 32.865777015686035 and batch: 650, loss is 4.073384056091308 and perplexity is 58.755458308731484
At time: 33.48534178733826 and batch: 700, loss is 4.0958333015441895 and perplexity is 60.08939088267628
At time: 34.13322687149048 and batch: 750, loss is 4.176291351318359 and perplexity is 65.12388319689536
At time: 34.748404026031494 and batch: 800, loss is 4.148463129997253 and perplexity is 63.33658536213165
At time: 35.36897945404053 and batch: 850, loss is 4.215945315361023 and perplexity is 67.75818846178294
At time: 36.00390815734863 and batch: 900, loss is 4.156845445632935 and perplexity is 63.86972395825275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431097161279966 and perplexity of 84.02355374631735
finished 3 epochs...
Completing Train Step...
At time: 37.59003758430481 and batch: 50, loss is 4.225580525398255 and perplexity is 68.41420820812769
At time: 38.21943759918213 and batch: 100, loss is 4.101099853515625 and perplexity is 60.40668958434541
At time: 38.834991216659546 and batch: 150, loss is 4.107890362739563 and perplexity is 60.81827763181118
At time: 39.45214128494263 and batch: 200, loss is 3.9990366744995116 and perplexity is 54.54557956823796
At time: 40.069106101989746 and batch: 250, loss is 4.147941179275513 and perplexity is 63.30353541166238
At time: 40.685182332992554 and batch: 300, loss is 4.109735841751099 and perplexity is 60.93062011766371
At time: 41.30033850669861 and batch: 350, loss is 4.110348291397095 and perplexity is 60.9679484840907
At time: 41.9159836769104 and batch: 400, loss is 4.042841801643371 and perplexity is 56.988061677483934
At time: 42.53218626976013 and batch: 450, loss is 4.075676379203796 and perplexity is 58.89029929430502
At time: 43.14867401123047 and batch: 500, loss is 3.960483684539795 and perplexity is 52.482704883864024
At time: 43.765292167663574 and batch: 550, loss is 4.033804211616516 and perplexity is 56.475347280664586
At time: 44.380133628845215 and batch: 600, loss is 4.044872612953186 and perplexity is 57.10391127218102
At time: 44.997291564941406 and batch: 650, loss is 3.893158302307129 and perplexity is 49.065606086450465
At time: 45.613659620285034 and batch: 700, loss is 3.9076931619644166 and perplexity is 49.783975839706194
At time: 46.23043465614319 and batch: 750, loss is 3.993151903152466 and perplexity is 54.22553392581202
At time: 46.84664440155029 and batch: 800, loss is 3.9735888862609863 and perplexity is 53.175027926171225
At time: 47.463550090789795 and batch: 850, loss is 4.03722204208374 and perplexity is 56.66870067959388
At time: 48.07934093475342 and batch: 900, loss is 3.987336792945862 and perplexity is 53.91112152739576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385371796072346 and perplexity of 80.26806101558884
finished 4 epochs...
Completing Train Step...
At time: 49.633241176605225 and batch: 50, loss is 4.061467757225037 and perplexity is 58.059465770831615
At time: 50.26356530189514 and batch: 100, loss is 3.9476663970947268 and perplexity is 51.814311614242364
At time: 50.87962985038757 and batch: 150, loss is 3.951504421234131 and perplexity is 52.013558305206864
At time: 51.49475836753845 and batch: 200, loss is 3.8476657247543335 and perplexity is 46.88349641666268
At time: 52.12273287773132 and batch: 250, loss is 3.9968245220184326 and perplexity is 54.42504979333671
At time: 52.744903564453125 and batch: 300, loss is 3.963996787071228 and perplexity is 52.6674062547111
At time: 53.35910415649414 and batch: 350, loss is 3.9664041996002197 and perplexity is 52.79435117147922
At time: 53.974687814712524 and batch: 400, loss is 3.901751914024353 and perplexity is 49.48907380631389
At time: 54.58987593650818 and batch: 450, loss is 3.9347676372528078 and perplexity is 51.15026316001902
At time: 55.20764946937561 and batch: 500, loss is 3.824196434020996 and perplexity is 45.79598548081237
At time: 55.8245632648468 and batch: 550, loss is 3.8965824174880983 and perplexity is 49.233900338134205
At time: 56.44755005836487 and batch: 600, loss is 3.9123098134994505 and perplexity is 50.014342460233586
At time: 57.06499242782593 and batch: 650, loss is 3.762353415489197 and perplexity is 43.04962049936815
At time: 57.694056034088135 and batch: 700, loss is 3.773350229263306 and perplexity is 43.52564172031466
At time: 58.322144985198975 and batch: 750, loss is 3.859722809791565 and perplexity is 47.45219626153185
At time: 58.94381785392761 and batch: 800, loss is 3.8459474849700928 and perplexity is 46.80300849647093
At time: 59.55921292304993 and batch: 850, loss is 3.9040379905700684 and perplexity is 49.602339034373955
At time: 60.17132902145386 and batch: 900, loss is 3.8602168130874634 and perplexity is 47.47564359393558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37646024521083 and perplexity of 79.55592593418052
finished 5 epochs...
Completing Train Step...
At time: 61.75105929374695 and batch: 50, loss is 3.937852110862732 and perplexity is 51.308278368467576
At time: 62.365745544433594 and batch: 100, loss is 3.829461917877197 and perplexity is 46.0377594729618
At time: 62.98073935508728 and batch: 150, loss is 3.833805551528931 and perplexity is 46.23816556448963
At time: 63.597015142440796 and batch: 200, loss is 3.7310986328125 and perplexity is 41.724923362285864
At time: 64.21401596069336 and batch: 250, loss is 3.8789956378936767 and perplexity is 48.37560404032416
At time: 64.82935905456543 and batch: 300, loss is 3.8511676263809203 and perplexity is 47.047965618428755
At time: 65.44511556625366 and batch: 350, loss is 3.854210572242737 and perplexity is 47.191348072599325
At time: 66.07480120658875 and batch: 400, loss is 3.7953871726989745 and perplexity is 44.495460498497614
At time: 66.70297574996948 and batch: 450, loss is 3.8264622354507445 and perplexity is 45.89986773402742
At time: 67.33828401565552 and batch: 500, loss is 3.7192701387405394 and perplexity is 41.23428781400132
At time: 67.95705080032349 and batch: 550, loss is 3.7865033864974977 and perplexity is 44.10192298029571
At time: 68.57336115837097 and batch: 600, loss is 3.8089680242538453 and perplexity is 45.10386874223271
At time: 69.18978595733643 and batch: 650, loss is 3.6592761278152466 and perplexity is 38.83322240520528
At time: 69.80682396888733 and batch: 700, loss is 3.668673424720764 and perplexity is 39.19986977485072
At time: 70.42638039588928 and batch: 750, loss is 3.757598624229431 and perplexity is 42.84541440348628
At time: 71.04718685150146 and batch: 800, loss is 3.7451925706863403 and perplexity is 42.317155477929056
At time: 71.66293334960938 and batch: 850, loss is 3.8003354501724242 and perplexity is 44.716182028668946
At time: 72.27987265586853 and batch: 900, loss is 3.759343032836914 and perplexity is 42.920219539555426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.374703185199058 and perplexity of 79.41626413103783
finished 6 epochs...
Completing Train Step...
At time: 73.83709836006165 and batch: 50, loss is 3.841914610862732 and perplexity is 46.61463794801862
At time: 74.46561169624329 and batch: 100, loss is 3.7369346046447753 and perplexity is 41.96914076954602
At time: 75.09143042564392 and batch: 150, loss is 3.7402837085723877 and perplexity is 42.10993542009251
At time: 75.7230064868927 and batch: 200, loss is 3.640923790931702 and perplexity is 38.127041877334214
At time: 76.34775233268738 and batch: 250, loss is 3.7850790882110594 and perplexity is 44.0391533988812
At time: 76.9721360206604 and batch: 300, loss is 3.7616521739959716 and perplexity is 43.019442901334976
At time: 77.60263395309448 and batch: 350, loss is 3.7667878341674803 and perplexity is 43.240944432297304
At time: 78.22589683532715 and batch: 400, loss is 3.7056549310684206 and perplexity is 40.67667901581094
At time: 78.85560536384583 and batch: 450, loss is 3.7359770822525022 and perplexity is 41.92897361102052
At time: 79.47705888748169 and batch: 500, loss is 3.6362181329727172 and perplexity is 37.94805052559795
At time: 80.09455513954163 and batch: 550, loss is 3.701315155029297 and perplexity is 40.50053383064873
At time: 80.71937227249146 and batch: 600, loss is 3.721947693824768 and perplexity is 41.34484283350935
At time: 81.3545548915863 and batch: 650, loss is 3.5765250539779663 and perplexity is 35.7490985693216
At time: 81.9705445766449 and batch: 700, loss is 3.5855136585235594 and perplexity is 36.07188159041944
At time: 82.58751058578491 and batch: 750, loss is 3.675212745666504 and perplexity is 39.45705028079688
At time: 83.20434165000916 and batch: 800, loss is 3.662852864265442 and perplexity is 38.97236730123072
At time: 83.82277297973633 and batch: 850, loss is 3.7162836742401124 and perplexity is 41.11132677801933
At time: 84.44644284248352 and batch: 900, loss is 3.679422826766968 and perplexity is 39.623517837537996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385021889046447 and perplexity of 80.23997957031823
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 86.13203978538513 and batch: 50, loss is 3.7968595695495604 and perplexity is 44.561023730104004
At time: 86.77856230735779 and batch: 100, loss is 3.692169680595398 and perplexity is 40.13182580871754
At time: 87.40530753135681 and batch: 150, loss is 3.6987240648269655 and perplexity is 40.39572913210074
At time: 88.04049968719482 and batch: 200, loss is 3.580579786300659 and perplexity is 35.89434586525148
At time: 88.66536688804626 and batch: 250, loss is 3.7205912399291994 and perplexity is 41.288798479770456
At time: 89.2898383140564 and batch: 300, loss is 3.680892491340637 and perplexity is 39.68179393065346
At time: 89.92393493652344 and batch: 350, loss is 3.6770809268951417 and perplexity is 39.53083209891386
At time: 90.54576373100281 and batch: 400, loss is 3.6049273347854616 and perplexity is 36.77901120540119
At time: 91.16764450073242 and batch: 450, loss is 3.629099931716919 and perplexity is 37.6788877785053
At time: 91.78938364982605 and batch: 500, loss is 3.514574198722839 and perplexity is 33.601617265844276
At time: 92.40992188453674 and batch: 550, loss is 3.56168044090271 and perplexity is 35.22233649603455
At time: 93.02999114990234 and batch: 600, loss is 3.573660650253296 and perplexity is 35.64684523555516
At time: 93.6516501903534 and batch: 650, loss is 3.4170444297790525 and perplexity is 30.4791983548614
At time: 94.27660727500916 and batch: 700, loss is 3.408295121192932 and perplexity is 30.213689645543216
At time: 94.89797377586365 and batch: 750, loss is 3.4852334022521974 and perplexity is 32.63004214646779
At time: 95.53618383407593 and batch: 800, loss is 3.4501504135131835 and perplexity is 31.5051307497756
At time: 96.16805529594421 and batch: 850, loss is 3.4955677318573 and perplexity is 32.96900019244735
At time: 96.80292749404907 and batch: 900, loss is 3.4512783098220825 and perplexity is 31.54068531762334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.346425931747645 and perplexity of 77.20204387520121
finished 8 epochs...
Completing Train Step...
At time: 98.38868975639343 and batch: 50, loss is 3.7077246141433715 and perplexity is 40.7609540311183
At time: 99.03890132904053 and batch: 100, loss is 3.5975200748443603 and perplexity is 36.507586008553936
At time: 99.66128945350647 and batch: 150, loss is 3.6020644903182983 and perplexity is 36.67386919109196
At time: 100.28441095352173 and batch: 200, loss is 3.489238519668579 and perplexity is 32.76099135499782
At time: 100.906982421875 and batch: 250, loss is 3.6330661487579348 and perplexity is 37.828627178484595
At time: 101.53032350540161 and batch: 300, loss is 3.5988055944442747 and perplexity is 36.554547404343644
At time: 102.15345048904419 and batch: 350, loss is 3.598143367767334 and perplexity is 36.5303480215118
At time: 102.7774007320404 and batch: 400, loss is 3.5312457370758055 and perplexity is 34.16650359520877
At time: 103.40075254440308 and batch: 450, loss is 3.559041028022766 and perplexity is 35.129492787764555
At time: 104.02713298797607 and batch: 500, loss is 3.450272560119629 and perplexity is 31.50897922961689
At time: 104.65960359573364 and batch: 550, loss is 3.501397852897644 and perplexity is 33.16177485792614
At time: 105.28444361686707 and batch: 600, loss is 3.51874858379364 and perplexity is 33.74217652544155
At time: 105.90833377838135 and batch: 650, loss is 3.36962975025177 and perplexity is 29.0677627333422
At time: 106.53332543373108 and batch: 700, loss is 3.364990916252136 and perplexity is 28.933234475649588
At time: 107.162348985672 and batch: 750, loss is 3.449138674736023 and perplexity is 31.473271906447735
At time: 107.79388761520386 and batch: 800, loss is 3.4190396165847776 and perplexity is 30.540070754979617
At time: 108.41837048530579 and batch: 850, loss is 3.4726116132736204 and perplexity is 32.22078087807126
At time: 109.0423150062561 and batch: 900, loss is 3.4347304916381836 and perplexity is 31.023050467241006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.353497230843322 and perplexity of 77.7498973532721
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 110.61350202560425 and batch: 50, loss is 3.6904189586639404 and perplexity is 40.06162760781965
At time: 111.22913193702698 and batch: 100, loss is 3.5918225240707398 and perplexity is 36.30017361605282
At time: 111.85258293151855 and batch: 150, loss is 3.5967475271224973 and perplexity is 36.479393047758336
At time: 112.4676685333252 and batch: 200, loss is 3.485667834281921 and perplexity is 32.64422076150671
At time: 113.09935593605042 and batch: 250, loss is 3.6239347648620606 and perplexity is 37.4847717885299
At time: 113.71615624427795 and batch: 300, loss is 3.5866830015182494 and perplexity is 36.114086663756765
At time: 114.33612728118896 and batch: 350, loss is 3.583136034011841 and perplexity is 35.98621807875415
At time: 114.97335195541382 and batch: 400, loss is 3.511161651611328 and perplexity is 33.487145594976255
At time: 115.59693455696106 and batch: 450, loss is 3.5305500268936156 and perplexity is 34.142741877360706
At time: 116.22470378875732 and batch: 500, loss is 3.4221845293045043 and perplexity is 30.636267798324717
At time: 116.84212899208069 and batch: 550, loss is 3.465011281967163 and perplexity is 31.97682053253472
At time: 117.45891427993774 and batch: 600, loss is 3.4830421876907347 and perplexity is 32.55862100107911
At time: 118.07522892951965 and batch: 650, loss is 3.3267397356033324 and perplexity is 27.847403644088605
At time: 118.68991756439209 and batch: 700, loss is 3.315391278266907 and perplexity is 27.53316500628379
At time: 119.30680799484253 and batch: 750, loss is 3.3995652866363524 and perplexity is 29.95107708351004
At time: 119.92341375350952 and batch: 800, loss is 3.3588829469680785 and perplexity is 28.757049781282074
At time: 120.54019212722778 and batch: 850, loss is 3.413711371421814 and perplexity is 30.377778521066848
At time: 121.15806555747986 and batch: 900, loss is 3.37725989818573 and perplexity is 29.290402369630648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3427103120986725 and perplexity of 76.91572270373133
finished 10 epochs...
Completing Train Step...
At time: 122.72933769226074 and batch: 50, loss is 3.663202862739563 and perplexity is 38.98600995763423
At time: 123.34536790847778 and batch: 100, loss is 3.5557414722442626 and perplexity is 35.01377208528532
At time: 123.9611120223999 and batch: 150, loss is 3.5607076358795164 and perplexity is 35.188088691081816
At time: 124.57576251029968 and batch: 200, loss is 3.4511325407028197 and perplexity is 31.536087994785635
At time: 125.19069457054138 and batch: 250, loss is 3.5907323932647706 and perplexity is 36.260623239990345
At time: 125.80670118331909 and batch: 300, loss is 3.5546952867507935 and perplexity is 34.97716033953601
At time: 126.42292785644531 and batch: 350, loss is 3.5522560977935793 and perplexity is 34.89194840253736
At time: 127.03862237930298 and batch: 400, loss is 3.483538856506348 and perplexity is 32.57479586925352
At time: 127.67016696929932 and batch: 450, loss is 3.5056778144836427 and perplexity is 33.30401014409915
At time: 128.28597617149353 and batch: 500, loss is 3.399011244773865 and perplexity is 29.934487529057375
At time: 128.91744923591614 and batch: 550, loss is 3.4452280950546266 and perplexity is 31.350433510035373
At time: 129.53857374191284 and batch: 600, loss is 3.4657012462615966 and perplexity is 31.99889100999683
At time: 130.15493488311768 and batch: 650, loss is 3.3123679637908934 and perplexity is 27.450049295724725
At time: 130.7721073627472 and batch: 700, loss is 3.3050421047210694 and perplexity is 27.24968890519779
At time: 131.38965368270874 and batch: 750, loss is 3.3926701021194456 and perplexity is 29.745269237478503
At time: 132.006117105484 and batch: 800, loss is 3.3543647718429566 and perplexity is 28.62741347458838
At time: 132.6301350593567 and batch: 850, loss is 3.4128771877288817 and perplexity is 30.352448440036188
At time: 133.24959230422974 and batch: 900, loss is 3.378688626289368 and perplexity is 29.3322802996271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.344464863816353 and perplexity of 77.05079377705884
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 134.82581758499146 and batch: 50, loss is 3.6610199069976805 and perplexity is 38.900998045723654
At time: 135.46593713760376 and batch: 100, loss is 3.559826259613037 and perplexity is 35.15708840831028
At time: 136.08150434494019 and batch: 150, loss is 3.5700672817230226 and perplexity is 35.5189828496022
At time: 136.6968972682953 and batch: 200, loss is 3.454947061538696 and perplexity is 31.65661278554458
At time: 137.31241250038147 and batch: 250, loss is 3.590499210357666 and perplexity is 36.252168868195625
At time: 137.93278861045837 and batch: 300, loss is 3.5548148059844973 and perplexity is 34.98134103276858
At time: 138.5501847267151 and batch: 350, loss is 3.552236433029175 and perplexity is 34.89126226733878
At time: 139.16548657417297 and batch: 400, loss is 3.4811579847335814 and perplexity is 32.49733170997236
At time: 139.78082704544067 and batch: 450, loss is 3.4984226703643797 and perplexity is 33.06325914837532
At time: 140.39880418777466 and batch: 500, loss is 3.3919777202606203 and perplexity is 29.72468128085891
At time: 141.0175380706787 and batch: 550, loss is 3.4340227317810057 and perplexity is 31.0011013657358
At time: 141.6318006515503 and batch: 600, loss is 3.457721314430237 and perplexity is 31.744558170064103
At time: 142.24889659881592 and batch: 650, loss is 3.300356526374817 and perplexity is 27.12230701488687
At time: 142.86508464813232 and batch: 700, loss is 3.291452479362488 and perplexity is 26.881880690247918
At time: 143.48310899734497 and batch: 750, loss is 3.37286612033844 and perplexity is 29.16198916469604
At time: 144.09915256500244 and batch: 800, loss is 3.3346989059448244 and perplexity is 28.069930262165535
At time: 144.73010396957397 and batch: 850, loss is 3.3906929206848146 and perplexity is 29.686515545868417
At time: 145.3468883037567 and batch: 900, loss is 3.360181226730347 and perplexity is 28.79440872296715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337621819483091 and perplexity of 76.52533170993372
finished 12 epochs...
Completing Train Step...
At time: 146.8970320224762 and batch: 50, loss is 3.6467329025268556 and perplexity is 38.349170679233374
At time: 147.53684258460999 and batch: 100, loss is 3.5438825368881224 and perplexity is 34.60099839132989
At time: 148.15209364891052 and batch: 150, loss is 3.553795347213745 and perplexity is 34.945697169651524
At time: 148.76621079444885 and batch: 200, loss is 3.440272498130798 and perplexity is 31.195457714118184
At time: 149.39917159080505 and batch: 250, loss is 3.5792169046401976 and perplexity is 35.845459440330444
At time: 150.02179193496704 and batch: 300, loss is 3.543453311920166 and perplexity is 34.586149965789566
At time: 150.63698649406433 and batch: 350, loss is 3.541312928199768 and perplexity is 34.51220150072825
At time: 151.2529275417328 and batch: 400, loss is 3.4707962608337404 and perplexity is 32.16234186464106
At time: 151.86835741996765 and batch: 450, loss is 3.489828314781189 and perplexity is 32.78031932679461
At time: 152.4837248325348 and batch: 500, loss is 3.3841804456710816 and perplexity is 29.49381102789714
At time: 153.0993082523346 and batch: 550, loss is 3.4279290723800657 and perplexity is 30.812765622678555
At time: 153.7141444683075 and batch: 600, loss is 3.45294641494751 and perplexity is 31.593342403028
At time: 154.3285355567932 and batch: 650, loss is 3.296491665840149 and perplexity is 27.01768538512887
At time: 154.94449925422668 and batch: 700, loss is 3.2895141410827637 and perplexity is 26.82982497896241
At time: 155.56027626991272 and batch: 750, loss is 3.372133584022522 and perplexity is 29.14063477097787
At time: 156.1775369644165 and batch: 800, loss is 3.335297660827637 and perplexity is 28.086742302613594
At time: 156.79345774650574 and batch: 850, loss is 3.3932295274734496 and perplexity is 29.761914150610743
At time: 157.40909242630005 and batch: 900, loss is 3.3641222095489502 and perplexity is 28.908110895007162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337308282721533 and perplexity of 76.50134196628203
finished 13 epochs...
Completing Train Step...
At time: 158.98138880729675 and batch: 50, loss is 3.64072491645813 and perplexity is 38.11946013588442
At time: 159.59870600700378 and batch: 100, loss is 3.537504668235779 and perplexity is 34.38102001063429
At time: 160.2301218509674 and batch: 150, loss is 3.547011947631836 and perplexity is 34.70944873176278
At time: 160.84827041625977 and batch: 200, loss is 3.4332173585891725 and perplexity is 30.976143961139837
At time: 161.4683964252472 and batch: 250, loss is 3.5724559926986696 and perplexity is 35.60392884910815
At time: 162.10558414459229 and batch: 300, loss is 3.536966552734375 and perplexity is 34.36252402774637
At time: 162.7384877204895 and batch: 350, loss is 3.534939227104187 and perplexity is 34.29293057020309
At time: 163.36962032318115 and batch: 400, loss is 3.4649227905273436 and perplexity is 31.97399098284226
At time: 163.99944019317627 and batch: 450, loss is 3.4845475816726683 and perplexity is 32.60767146406686
At time: 164.61346578598022 and batch: 500, loss is 3.3794301080703737 and perplexity is 29.35403771643187
At time: 165.2434117794037 and batch: 550, loss is 3.42389714717865 and perplexity is 30.688780972818513
At time: 165.87139630317688 and batch: 600, loss is 3.449665060043335 and perplexity is 31.489843335448064
At time: 166.4861695766449 and batch: 650, loss is 3.293722267150879 and perplexity is 26.942966154061832
At time: 167.1009759902954 and batch: 700, loss is 3.287762608528137 and perplexity is 26.78287279821695
At time: 167.71555519104004 and batch: 750, loss is 3.3710403442382812 and perplexity is 29.108794477417252
At time: 168.3305902481079 and batch: 800, loss is 3.334818730354309 and perplexity is 28.07329392650399
At time: 168.94510173797607 and batch: 850, loss is 3.39353675365448 and perplexity is 29.771059194561914
At time: 169.5592007637024 and batch: 900, loss is 3.364866714477539 and perplexity is 28.929641139736848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.337790711285317 and perplexity of 76.53825730260476
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 171.13202333450317 and batch: 50, loss is 3.639541058540344 and perplexity is 38.07435881322732
At time: 171.75119161605835 and batch: 100, loss is 3.538905062675476 and perplexity is 34.429200728018714
At time: 172.37118816375732 and batch: 150, loss is 3.5518453025817873 and perplexity is 34.87761790085577
At time: 172.99077439308167 and batch: 200, loss is 3.434795546531677 and perplexity is 31.025068734133356
At time: 173.61098980903625 and batch: 250, loss is 3.5728245306015016 and perplexity is 35.61705266454195
At time: 174.23080801963806 and batch: 300, loss is 3.538489022254944 and perplexity is 34.41487976812489
At time: 174.8503839969635 and batch: 350, loss is 3.5363194274902345 and perplexity is 34.340294364459524
At time: 175.46640348434448 and batch: 400, loss is 3.4645578384399416 and perplexity is 31.962324137139838
At time: 176.10398173332214 and batch: 450, loss is 3.4828679609298705 and perplexity is 32.552948912133076
At time: 176.74175572395325 and batch: 500, loss is 3.3777215385437014 and perplexity is 29.3039271230115
At time: 177.36868906021118 and batch: 550, loss is 3.4201669931411742 and perplexity is 30.574520329952357
At time: 177.98719024658203 and batch: 600, loss is 3.44702956199646 and perplexity is 31.40696118069349
At time: 178.60567212104797 and batch: 650, loss is 3.2918540382385255 and perplexity is 26.89267751567913
At time: 179.2247006893158 and batch: 700, loss is 3.2831165075302122 and perplexity is 26.658725489971093
At time: 179.84349942207336 and batch: 750, loss is 3.3621525382995605 and perplexity is 28.851227459327443
At time: 180.46271204948425 and batch: 800, loss is 3.3253824853897096 and perplexity is 27.80963338718659
At time: 181.08188247680664 and batch: 850, loss is 3.3835486555099488 and perplexity is 29.475183013393178
At time: 181.7171790599823 and batch: 900, loss is 3.3561831378936766 and perplexity is 28.67951594769842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336610140865797 and perplexity of 76.44795181655304
finished 15 epochs...
Completing Train Step...
At time: 183.28135919570923 and batch: 50, loss is 3.6348398876190187 and perplexity is 37.89578482703072
At time: 183.93684315681458 and batch: 100, loss is 3.533959059715271 and perplexity is 34.25933422569819
At time: 184.56236910820007 and batch: 150, loss is 3.5465316820144652 and perplexity is 34.692782979253316
At time: 185.1819326877594 and batch: 200, loss is 3.431133246421814 and perplexity is 30.911653428707247
At time: 185.7995903491974 and batch: 250, loss is 3.5699273443222044 and perplexity is 35.51401276322112
At time: 186.41730523109436 and batch: 300, loss is 3.534600729942322 and perplexity is 34.28132447495901
At time: 187.0361430644989 and batch: 350, loss is 3.532994689941406 and perplexity is 34.22631148492402
At time: 187.65987467765808 and batch: 400, loss is 3.4613514709472657 and perplexity is 31.860005303586913
At time: 188.2841718196869 and batch: 450, loss is 3.4803764057159423 and perplexity is 32.4719424005468
At time: 188.91084098815918 and batch: 500, loss is 3.375330677032471 and perplexity is 29.2339491786462
At time: 189.53561878204346 and batch: 550, loss is 3.4186302328109743 and perplexity is 30.527570704395245
At time: 190.1604564189911 and batch: 600, loss is 3.445798468589783 and perplexity is 31.368320068149952
At time: 190.78397178649902 and batch: 650, loss is 3.290858302116394 and perplexity is 26.865912832748144
At time: 191.4077820777893 and batch: 700, loss is 3.2824300718307495 and perplexity is 26.640432268371086
At time: 192.05689597129822 and batch: 750, loss is 3.361997375488281 and perplexity is 28.846751169051863
At time: 192.67959642410278 and batch: 800, loss is 3.3259327888488768 and perplexity is 27.82494133626008
At time: 193.3029179573059 and batch: 850, loss is 3.384844341278076 and perplexity is 29.513398340718123
At time: 193.9272334575653 and batch: 900, loss is 3.3585195302963258 and perplexity is 28.746600888727013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336237659193065 and perplexity of 76.41948165821879
finished 16 epochs...
Completing Train Step...
At time: 195.5351424217224 and batch: 50, loss is 3.6326444292068483 and perplexity is 37.81267747020095
At time: 196.16534447669983 and batch: 100, loss is 3.5317105770111086 and perplexity is 34.18238924238507
At time: 196.7832214832306 and batch: 150, loss is 3.5440175771713256 and perplexity is 34.605671235455766
At time: 197.40112686157227 and batch: 200, loss is 3.428951897621155 and perplexity is 30.844297820320012
At time: 198.02003288269043 and batch: 250, loss is 3.5679686737060545 and perplexity is 35.44452058831734
At time: 198.65185165405273 and batch: 300, loss is 3.5325282192230225 and perplexity is 34.21034963597503
At time: 199.2693476676941 and batch: 350, loss is 3.53099072933197 and perplexity is 34.15779198302371
At time: 199.88562059402466 and batch: 400, loss is 3.4595482206344603 and perplexity is 31.80260550771114
At time: 200.51264095306396 and batch: 450, loss is 3.4787544631958007 and perplexity is 32.4193174653025
At time: 201.139386177063 and batch: 500, loss is 3.37394157409668 and perplexity is 29.19336840597801
At time: 201.76473951339722 and batch: 550, loss is 3.4175471019744874 and perplexity is 30.494523251790632
At time: 202.38015294075012 and batch: 600, loss is 3.444922776222229 and perplexity is 31.340863093369503
At time: 202.99598360061646 and batch: 650, loss is 3.2901656436920166 and perplexity is 26.84731037521416
At time: 203.61640620231628 and batch: 700, loss is 3.2820580768585206 and perplexity is 26.630524004535673
At time: 204.2367570400238 and batch: 750, loss is 3.3619329261779787 and perplexity is 28.84489207574384
At time: 204.85210013389587 and batch: 800, loss is 3.326177883148193 and perplexity is 27.831761906567717
At time: 205.4677164554596 and batch: 850, loss is 3.38545343875885 and perplexity is 29.531380353139824
At time: 206.0831172466278 and batch: 900, loss is 3.359549069404602 and perplexity is 28.7762118787926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336167845007491 and perplexity of 76.41414668057556
finished 17 epochs...
Completing Train Step...
At time: 207.6698932647705 and batch: 50, loss is 3.630970039367676 and perplexity is 37.749417283139756
At time: 208.28546285629272 and batch: 100, loss is 3.5299913120269775 and perplexity is 34.12367114793216
At time: 208.9004786014557 and batch: 150, loss is 3.5421607208251955 and perplexity is 34.541473097016315
At time: 209.51635694503784 and batch: 200, loss is 3.427162837982178 and perplexity is 30.789164864775362
At time: 210.13656425476074 and batch: 250, loss is 3.5662835693359374 and perplexity is 35.384843167240334
At time: 210.7601978778839 and batch: 300, loss is 3.5308887243270872 and perplexity is 34.15430789498593
At time: 211.3841528892517 and batch: 350, loss is 3.5293568849563597 and perplexity is 34.102029033104714
At time: 212.01051568984985 and batch: 400, loss is 3.45809045791626 and perplexity is 31.756278630061903
At time: 212.62926578521729 and batch: 450, loss is 3.4774176263809204 and perplexity is 32.376007084107734
At time: 213.2457935810089 and batch: 500, loss is 3.3727953910827635 and perplexity is 29.159926631849945
At time: 213.8625841140747 and batch: 550, loss is 3.416585931777954 and perplexity is 30.465226906525928
At time: 214.47946906089783 and batch: 600, loss is 3.4441485691070555 and perplexity is 31.316608164547695
At time: 215.09614896774292 and batch: 650, loss is 3.2895474004745484 and perplexity is 26.830717337462467
At time: 215.7123668193817 and batch: 700, loss is 3.2816964101791384 and perplexity is 26.620894372812174
At time: 216.32819938659668 and batch: 750, loss is 3.361774444580078 and perplexity is 28.840321053377437
At time: 216.94441533088684 and batch: 800, loss is 3.326216254234314 and perplexity is 27.832829861989904
At time: 217.55958890914917 and batch: 850, loss is 3.3857285833358763 and perplexity is 29.539506870228372
At time: 218.17598462104797 and batch: 900, loss is 3.3600506258010863 and perplexity is 28.79064839198713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.336223445526541 and perplexity of 76.41839546490998
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 219.74240827560425 and batch: 50, loss is 3.6303683280944825 and perplexity is 37.726709865543704
At time: 220.35583305358887 and batch: 100, loss is 3.5299276399612425 and perplexity is 34.12149849246917
At time: 220.9690682888031 and batch: 150, loss is 3.542987790107727 and perplexity is 34.570053105582666
At time: 221.5815908908844 and batch: 200, loss is 3.427539758682251 and perplexity is 30.800772125721902
At time: 222.19508171081543 and batch: 250, loss is 3.5667411518096923 and perplexity is 35.401038356345
At time: 222.80859804153442 and batch: 300, loss is 3.5314853620529174 and perplexity is 34.174691723851666
At time: 223.4349648952484 and batch: 350, loss is 3.5296265983581545 and perplexity is 34.11122804785635
At time: 224.0498206615448 and batch: 400, loss is 3.45839035987854 and perplexity is 31.76580382857924
At time: 224.66475319862366 and batch: 450, loss is 3.477529182434082 and perplexity is 32.37961902513837
At time: 225.27813148498535 and batch: 500, loss is 3.3718939590454102 and perplexity is 29.133652783600535
At time: 225.89216256141663 and batch: 550, loss is 3.4150596857070923 and perplexity is 30.418764938878866
At time: 226.5105881690979 and batch: 600, loss is 3.44208101272583 and perplexity is 31.251926201343448
At time: 227.1259732246399 and batch: 650, loss is 3.2879552173614504 and perplexity is 26.788031912928975
At time: 227.74062633514404 and batch: 700, loss is 3.2802484035491943 and perplexity is 26.582375036123118
At time: 228.35746502876282 and batch: 750, loss is 3.359391994476318 and perplexity is 28.771692212347894
At time: 228.9758801460266 and batch: 800, loss is 3.323106260299683 and perplexity is 27.74640439094024
At time: 229.592946767807 and batch: 850, loss is 3.3823143911361693 and perplexity is 29.438825287227814
At time: 230.20973873138428 and batch: 900, loss is 3.3562694501876833 and perplexity is 28.681991449342444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335713425727739 and perplexity of 76.3794305075247
finished 19 epochs...
Completing Train Step...
At time: 231.76534152030945 and batch: 50, loss is 3.629601526260376 and perplexity is 37.697792043760316
At time: 232.39421129226685 and batch: 100, loss is 3.52901762008667 and perplexity is 34.090461375029605
At time: 233.007004737854 and batch: 150, loss is 3.541933145523071 and perplexity is 34.53361320523299
At time: 233.6213207244873 and batch: 200, loss is 3.4267612075805665 and perplexity is 30.776801483046615
At time: 234.23558640480042 and batch: 250, loss is 3.565899715423584 and perplexity is 35.37126316328814
At time: 234.84975910186768 and batch: 300, loss is 3.5305203437805175 and perplexity is 34.141728429533295
At time: 235.46415996551514 and batch: 350, loss is 3.5288665199279787 and perplexity is 34.085310690050434
At time: 236.078058719635 and batch: 400, loss is 3.457603635787964 and perplexity is 31.740822733353966
At time: 236.69771933555603 and batch: 450, loss is 3.47688835144043 and perplexity is 32.35887580886223
At time: 237.3139808177948 and batch: 500, loss is 3.3714014863967896 and perplexity is 29.119308788752672
At time: 237.93075799942017 and batch: 550, loss is 3.414775538444519 and perplexity is 30.41012275797479
At time: 238.54653978347778 and batch: 600, loss is 3.441999354362488 and perplexity is 31.24937432439102
At time: 239.17620754241943 and batch: 650, loss is 3.2880470180511474 and perplexity is 26.790491185613945
At time: 239.79257225990295 and batch: 700, loss is 3.2801776695251466 and perplexity is 26.580494824266328
At time: 240.41003489494324 and batch: 750, loss is 3.3592344617843626 and perplexity is 28.767160087209874
At time: 241.0223331451416 and batch: 800, loss is 3.323175621032715 and perplexity is 27.74832896863209
At time: 241.63661670684814 and batch: 850, loss is 3.3826438665390013 and perplexity is 29.448526254075446
At time: 242.25153517723083 and batch: 900, loss is 3.3568765687942506 and perplexity is 28.699410107085317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.335483916818279 and perplexity of 76.36190275918786
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
1251.7261154651642


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}, {'params': {'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.19514626483225}, {'params': {'num_layers': 1, 'rnn_dropout': 0.5844906825889364, 'batch_size': 32, 'dropout': 0.5939710512473378, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.5312201255317}, {'params': {'num_layers': 1, 'rnn_dropout': 0.16540051295511726, 'batch_size': 32, 'dropout': 0.5337025234107475, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.36190275918786}]
SETTINGS FOR THIS RUN
{'num_layers': 1, 'rnn_dropout': 1.0, 'batch_size': 32, 'dropout': 0.7554479327164237, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/train.txt...
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Using these vectors: gigavec
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8550577163696289 and batch: 50, loss is 6.903668994903565 and perplexity is 995.9220534198844
At time: 1.489173412322998 and batch: 100, loss is 6.070324983596802 and perplexity is 432.82131854975813
At time: 2.1107397079467773 and batch: 150, loss is 5.957899312973023 and perplexity is 386.7967313193654
At time: 2.7389073371887207 and batch: 200, loss is 5.8149029159545895 and perplexity is 335.25884863581075
At time: 3.359097957611084 and batch: 250, loss is 5.862150106430054 and perplexity is 351.4790495299167
At time: 3.9793386459350586 and batch: 300, loss is 5.783288927078247 and perplexity is 324.82576376498895
At time: 4.600489377975464 and batch: 350, loss is 5.76205436706543 and perplexity is 318.0009490041264
At time: 5.235871315002441 and batch: 400, loss is 5.623686323165893 and perplexity is 276.90827749236666
At time: 5.856794834136963 and batch: 450, loss is 5.631244726181031 and perplexity is 279.0091916277518
At time: 6.481815814971924 and batch: 500, loss is 5.584907417297363 and perplexity is 266.3756198230034
At time: 7.105484247207642 and batch: 550, loss is 5.630086317062378 and perplexity is 278.68617196656476
At time: 7.736898183822632 and batch: 600, loss is 5.559583969116211 and perplexity is 259.71476448089766
At time: 8.369634628295898 and batch: 650, loss is 5.471477060317993 and perplexity is 237.811194950885
At time: 8.99459981918335 and batch: 700, loss is 5.561177558898926 and perplexity is 260.1289732277501
At time: 9.619585990905762 and batch: 750, loss is 5.534340963363648 and perplexity is 253.24083764022404
At time: 10.245235919952393 and batch: 800, loss is 5.522197380065918 and perplexity is 250.18418336463637
At time: 10.870723247528076 and batch: 850, loss is 5.551863050460815 and perplexity is 257.717249160464
At time: 11.5169358253479 and batch: 900, loss is 5.442704391479492 and perplexity is 231.0662327946153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.28496854599208 and perplexity of 197.34797596573364
finished 1 epochs...
Completing Train Step...
At time: 13.098853826522827 and batch: 50, loss is 5.145745325088501 and perplexity is 171.69940886969425
At time: 13.719608783721924 and batch: 100, loss is 4.9740760612487795 and perplexity is 144.61514786375412
At time: 14.340831995010376 and batch: 150, loss is 4.938599929809571 and perplexity is 139.57469832475593
At time: 14.961406469345093 and batch: 200, loss is 4.809355955123902 and perplexity is 122.65259829656144
At time: 15.587875366210938 and batch: 250, loss is 4.888647565841675 and perplexity is 132.773884632707
At time: 16.2160542011261 and batch: 300, loss is 4.814973649978637 and perplexity is 123.34356216186005
At time: 16.836544275283813 and batch: 350, loss is 4.799464750289917 and perplexity is 121.4453965057774
At time: 17.456782817840576 and batch: 400, loss is 4.670022354125977 and perplexity is 106.70012759388561
At time: 18.084841012954712 and batch: 450, loss is 4.683486652374268 and perplexity is 108.14648517943407
At time: 18.71506905555725 and batch: 500, loss is 4.587316904067993 and perplexity is 98.23051437107348
At time: 19.354331016540527 and batch: 550, loss is 4.653080673217773 and perplexity is 104.90767453557093
At time: 19.981632709503174 and batch: 600, loss is 4.599734535217285 and perplexity is 99.45790956479524
At time: 20.60391640663147 and batch: 650, loss is 4.458807172775269 and perplexity is 86.38440594522783
At time: 21.229838848114014 and batch: 700, loss is 4.505241260528565 and perplexity is 90.49017312251377
At time: 21.861542463302612 and batch: 750, loss is 4.5457639980316165 and perplexity is 94.23239307784138
At time: 22.484716415405273 and batch: 800, loss is 4.495668163299561 and perplexity is 89.62803514642884
At time: 23.11491322517395 and batch: 850, loss is 4.55518611907959 and perplexity is 95.12445806469123
At time: 23.741125345230103 and batch: 900, loss is 4.486527194976807 and perplexity is 88.81248127117645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.592294353328339 and perplexity of 98.72067062414473
finished 2 epochs...
Completing Train Step...
At time: 25.316238164901733 and batch: 50, loss is 4.513457841873169 and perplexity is 91.236755969927
At time: 25.93782687187195 and batch: 100, loss is 4.383923921585083 and perplexity is 80.1519270319064
At time: 26.55912160873413 and batch: 150, loss is 4.3823309421539305 and perplexity is 80.02434830290136
At time: 27.17980670928955 and batch: 200, loss is 4.2722336864471435 and perplexity is 71.68157108372125
At time: 27.81416416168213 and batch: 250, loss is 4.411562185287476 and perplexity is 82.39808408813104
At time: 28.44076418876648 and batch: 300, loss is 4.3674325323104854 and perplexity is 78.8409500297711
At time: 29.069348573684692 and batch: 350, loss is 4.366934556961059 and perplexity is 78.80169895397613
At time: 29.690428495407104 and batch: 400, loss is 4.274956078529358 and perplexity is 71.8769822975614
At time: 30.310811042785645 and batch: 450, loss is 4.310963706970215 and perplexity is 74.51226233710999
At time: 30.93145442008972 and batch: 500, loss is 4.192335777282715 and perplexity is 66.17718574139136
At time: 31.552080631256104 and batch: 550, loss is 4.265953388214111 and perplexity is 71.2328001222811
At time: 32.1729691028595 and batch: 600, loss is 4.254905724525452 and perplexity is 70.45017515165247
At time: 32.79379677772522 and batch: 650, loss is 4.107292885780335 and perplexity is 60.78195096547993
At time: 33.41497874259949 and batch: 700, loss is 4.128832411766052 and perplexity is 62.10536708944924
At time: 34.04233503341675 and batch: 750, loss is 4.2142040109634396 and perplexity is 67.64030349683306
At time: 34.67005395889282 and batch: 800, loss is 4.1764666223526 and perplexity is 65.13529852761624
At time: 35.29355978965759 and batch: 850, loss is 4.248618416786194 and perplexity is 70.00862276262077
At time: 35.919281244277954 and batch: 900, loss is 4.190039939880371 and perplexity is 66.02542795544295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.439805122270976 and perplexity of 84.75842253537486
finished 3 epochs...
Completing Train Step...
At time: 37.48573589324951 and batch: 50, loss is 4.249651379585266 and perplexity is 70.08097642843795
At time: 38.11505341529846 and batch: 100, loss is 4.127812252044678 and perplexity is 62.0420420018073
At time: 38.72960567474365 and batch: 150, loss is 4.133984222412109 and perplexity is 62.42614777197512
At time: 39.343146324157715 and batch: 200, loss is 4.02122389793396 and perplexity is 55.76932003023266
At time: 39.95659327507019 and batch: 250, loss is 4.171968698501587 and perplexity is 64.84298281456243
At time: 40.57173299789429 and batch: 300, loss is 4.133376927375793 and perplexity is 62.38824819157521
At time: 41.18062925338745 and batch: 350, loss is 4.133584456443787 and perplexity is 62.401196910147426
At time: 41.79110860824585 and batch: 400, loss is 4.059035339355469 and perplexity is 57.91841250857787
At time: 42.40300679206848 and batch: 450, loss is 4.101801099777222 and perplexity is 60.449064405447864
At time: 43.01333260536194 and batch: 500, loss is 3.97513671875 and perplexity is 53.25739769285504
At time: 43.63727164268494 and batch: 550, loss is 4.051673002243042 and perplexity is 57.49356349056096
At time: 44.24831938743591 and batch: 600, loss is 4.050104179382324 and perplexity is 57.40343698855616
At time: 44.865397453308105 and batch: 650, loss is 3.906948280334473 and perplexity is 49.74690647849472
At time: 45.496021032333374 and batch: 700, loss is 3.921126914024353 and perplexity is 50.45727376079425
At time: 46.113935232162476 and batch: 750, loss is 4.0145956230163575 and perplexity is 55.40088802829401
At time: 46.733097314834595 and batch: 800, loss is 3.987750964164734 and perplexity is 53.9334545868455
At time: 47.35533618927002 and batch: 850, loss is 4.0558464097976685 and perplexity is 57.73400895223489
At time: 47.98418474197388 and batch: 900, loss is 4.003056740760803 and perplexity is 54.765297757298065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391332338934076 and perplexity of 80.74793095563612
finished 4 epochs...
Completing Train Step...
At time: 49.6069061756134 and batch: 50, loss is 4.0735302877426145 and perplexity is 58.76405084465827
At time: 50.26184368133545 and batch: 100, loss is 3.957699909210205 and perplexity is 52.33680799108141
At time: 50.88598895072937 and batch: 150, loss is 3.9701061487197875 and perplexity is 52.99015537830684
At time: 51.51057505607605 and batch: 200, loss is 3.8571417760849 and perplexity is 47.329878464608
At time: 52.136024475097656 and batch: 250, loss is 4.0077634620666505 and perplexity is 55.02367031790941
At time: 52.7614643573761 and batch: 300, loss is 3.9753783178329467 and perplexity is 53.2702661857432
At time: 53.38383340835571 and batch: 350, loss is 3.9748773765563965 and perplexity is 53.24358759335991
At time: 54.006924867630005 and batch: 400, loss is 3.909594802856445 and perplexity is 49.87873715637149
At time: 54.62897515296936 and batch: 450, loss is 3.9517878532409667 and perplexity is 52.028302701838236
At time: 55.25092601776123 and batch: 500, loss is 3.826375331878662 and perplexity is 45.89587904488139
At time: 55.87640714645386 and batch: 550, loss is 3.9013969802856447 and perplexity is 49.47151158122008
At time: 56.50486469268799 and batch: 600, loss is 3.904918785095215 and perplexity is 49.64604774940258
At time: 57.12594175338745 and batch: 650, loss is 3.7651810169219972 and perplexity is 43.17151992825473
At time: 57.76113748550415 and batch: 700, loss is 3.77560350894928 and perplexity is 43.62382774336455
At time: 58.38587474822998 and batch: 750, loss is 3.8716911602020265 and perplexity is 48.02353293297261
At time: 59.03261423110962 and batch: 800, loss is 3.851573567390442 and perplexity is 47.06706819408494
At time: 59.64974880218506 and batch: 850, loss is 3.9139943361282348 and perplexity is 50.09866375249477
At time: 60.267173767089844 and batch: 900, loss is 3.8643176651000974 and perplexity is 47.670733927409636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.37862657520869 and perplexity of 79.7284571353237
finished 5 epochs...
Completing Train Step...
At time: 61.84973645210266 and batch: 50, loss is 3.943359794616699 and perplexity is 51.591647777904
At time: 62.46685171127319 and batch: 100, loss is 3.831512088775635 and perplexity is 46.13224156678821
At time: 63.085591554641724 and batch: 150, loss is 3.8487830352783203 and perplexity is 46.93590911578788
At time: 63.7015917301178 and batch: 200, loss is 3.737396116256714 and perplexity is 41.988514485607844
At time: 64.31717348098755 and batch: 250, loss is 3.884980101585388 and perplexity is 48.66597407412617
At time: 64.95183968544006 and batch: 300, loss is 3.8572200536727905 and perplexity is 47.333583478337246
At time: 65.57403659820557 and batch: 350, loss is 3.8562567329406736 and perplexity is 47.28800801146768
At time: 66.18904280662537 and batch: 400, loss is 3.7942999601364136 and perplexity is 44.447110762849285
At time: 66.80640840530396 and batch: 450, loss is 3.8359785079956055 and perplexity is 46.338748326764886
At time: 67.42449855804443 and batch: 500, loss is 3.7146933937072752 and perplexity is 41.04600019293095
At time: 68.04238295555115 and batch: 550, loss is 3.7873258113861086 and perplexity is 44.13820841836875
At time: 68.66245651245117 and batch: 600, loss is 3.792562985420227 and perplexity is 44.36997426671233
At time: 69.27832555770874 and batch: 650, loss is 3.656860647201538 and perplexity is 38.73953470528123
At time: 69.89448094367981 and batch: 700, loss is 3.6659926128387452 and perplexity is 39.094923032274124
At time: 70.51141786575317 and batch: 750, loss is 3.761897826194763 and perplexity is 43.03001202018496
At time: 71.12803721427917 and batch: 800, loss is 3.746618781089783 and perplexity is 42.377551703941165
At time: 71.74283933639526 and batch: 850, loss is 3.8012557935714724 and perplexity is 44.75735521545875
At time: 72.35830879211426 and batch: 900, loss is 3.757142171859741 and perplexity is 42.82586197526704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382690847736516 and perplexity of 80.05315469607825
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 73.97137570381165 and batch: 50, loss is 3.873571066856384 and perplexity is 48.113897604059545
At time: 74.58586406707764 and batch: 100, loss is 3.760228657722473 and perplexity is 42.95824759088207
At time: 75.21519255638123 and batch: 150, loss is 3.777414331436157 and perplexity is 43.70289431776588
At time: 75.83169555664062 and batch: 200, loss is 3.648155484199524 and perplexity is 38.4037643293598
At time: 76.44703984260559 and batch: 250, loss is 3.784190783500671 and perplexity is 44.00005058165512
At time: 77.06224083900452 and batch: 300, loss is 3.745175824165344 and perplexity is 42.31644681873016
At time: 77.67758297920227 and batch: 350, loss is 3.7331785917282105 and perplexity is 41.81179980703789
At time: 78.29918217658997 and batch: 400, loss is 3.67202260017395 and perplexity is 39.33137711411884
At time: 78.91362309455872 and batch: 450, loss is 3.6920804166793824 and perplexity is 40.12824364467036
At time: 79.5296049118042 and batch: 500, loss is 3.5724434423446656 and perplexity is 35.60348201000116
At time: 80.15034699440002 and batch: 550, loss is 3.6257754945755005 and perplexity is 37.55383466530686
At time: 80.77269911766052 and batch: 600, loss is 3.6252562618255615 and perplexity is 37.53434054589342
At time: 81.39560437202454 and batch: 650, loss is 3.4748370122909544 and perplexity is 32.29256481650284
At time: 82.02590537071228 and batch: 700, loss is 3.463055381774902 and perplexity is 31.914338187641526
At time: 82.64784932136536 and batch: 750, loss is 3.5509123754501344 and perplexity is 34.84509479803003
At time: 83.2770607471466 and batch: 800, loss is 3.515457425117493 and perplexity is 33.63130821109966
At time: 83.89173603057861 and batch: 850, loss is 3.5552328062057494 and perplexity is 34.99596629752898
At time: 84.50797295570374 and batch: 900, loss is 3.5051013708114622 and perplexity is 33.28481779038023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.329770440924658 and perplexity of 75.9268548698639
finished 7 epochs...
Completing Train Step...
At time: 86.06879687309265 and batch: 50, loss is 3.7815398931503297 and perplexity is 43.88356573464583
At time: 86.69829392433167 and batch: 100, loss is 3.6633233785629273 and perplexity is 38.990708671852985
At time: 87.3149037361145 and batch: 150, loss is 3.6815991020202636 and perplexity is 39.709843418898494
At time: 87.93223905563354 and batch: 200, loss is 3.5555522441864014 and perplexity is 35.00714712402927
At time: 88.54886960983276 and batch: 250, loss is 3.695210084915161 and perplexity is 40.254028463869275
At time: 89.16617608070374 and batch: 300, loss is 3.66340491771698 and perplexity is 38.993888070874995
At time: 89.787180185318 and batch: 350, loss is 3.6535032033920287 and perplexity is 38.60968699447302
At time: 90.41202282905579 and batch: 400, loss is 3.5978092908859254 and perplexity is 36.51814611506892
At time: 91.05241370201111 and batch: 450, loss is 3.6226385927200315 and perplexity is 37.4362165463614
At time: 91.66935276985168 and batch: 500, loss is 3.5072655153274535 and perplexity is 33.35692894760526
At time: 92.28546571731567 and batch: 550, loss is 3.5632995843887327 and perplexity is 35.279412707553284
At time: 92.90251851081848 and batch: 600, loss is 3.5693244981765746 and perplexity is 35.492609729527345
At time: 93.51925039291382 and batch: 650, loss is 3.4247556114196778 and perplexity is 30.71513750533611
At time: 94.1375081539154 and batch: 700, loss is 3.416556549072266 and perplexity is 30.46433176888086
At time: 94.7556083202362 and batch: 750, loss is 3.5127703285217287 and perplexity is 33.541058945844874
At time: 95.39176774024963 and batch: 800, loss is 3.4821154642105103 and perplexity is 32.528462139134845
At time: 96.0132646560669 and batch: 850, loss is 3.529141511917114 and perplexity is 34.09468516633125
At time: 96.62972712516785 and batch: 900, loss is 3.487370467185974 and perplexity is 32.69984922989404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3338075402664815 and perplexity of 76.23399869359334
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 98.18301177024841 and batch: 50, loss is 3.7579859828948976 and perplexity is 42.86201416085335
At time: 98.81359720230103 and batch: 100, loss is 3.6456839179992677 and perplexity is 38.30896408427988
At time: 99.4326491355896 and batch: 150, loss is 3.669840621948242 and perplexity is 39.245650466520175
At time: 100.0525848865509 and batch: 200, loss is 3.537300524711609 and perplexity is 34.37400206440327
At time: 100.67205786705017 and batch: 250, loss is 3.6808992719650266 and perplexity is 39.68206299890544
At time: 101.29193830490112 and batch: 300, loss is 3.639307622909546 and perplexity is 38.06547193855747
At time: 101.9124276638031 and batch: 350, loss is 3.6268932104110716 and perplexity is 37.59583264753128
At time: 102.53374433517456 and batch: 400, loss is 3.574600191116333 and perplexity is 35.68035264161608
At time: 103.15376281738281 and batch: 450, loss is 3.5861525678634645 and perplexity is 36.09493561640754
At time: 103.77007389068604 and batch: 500, loss is 3.4676426887512206 and perplexity is 32.06107536076277
At time: 104.3893129825592 and batch: 550, loss is 3.5201538753509523 and perplexity is 33.78962745462416
At time: 105.00930118560791 and batch: 600, loss is 3.5288667345046996 and perplexity is 34.08531800396542
At time: 105.62789702415466 and batch: 650, loss is 3.373513388633728 and perplexity is 29.180870905826858
At time: 106.26796293258667 and batch: 700, loss is 3.362480206489563 and perplexity is 28.860682637804402
At time: 106.8872218132019 and batch: 750, loss is 3.457957706451416 and perplexity is 31.752063217363066
At time: 107.5072283744812 and batch: 800, loss is 3.4166242980957033 and perplexity is 30.466395767524038
At time: 108.12770438194275 and batch: 850, loss is 3.460816674232483 and perplexity is 31.842971232706162
At time: 108.75441789627075 and batch: 900, loss is 3.418292980194092 and perplexity is 30.51727693718565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.321689135407748 and perplexity of 75.3157393900368
finished 9 epochs...
Completing Train Step...
At time: 110.34129548072815 and batch: 50, loss is 3.7300938415527343 and perplexity is 41.68301957977978
At time: 110.9637815952301 and batch: 100, loss is 3.611574339866638 and perplexity is 37.02429578056276
At time: 111.5902910232544 and batch: 150, loss is 3.635051097869873 and perplexity is 37.903789650570985
At time: 112.21709513664246 and batch: 200, loss is 3.503613872528076 and perplexity is 33.23534348664761
At time: 112.84044885635376 and batch: 250, loss is 3.6488019943237306 and perplexity is 38.42860077944951
At time: 113.46408271789551 and batch: 300, loss is 3.6102763700485228 and perplexity is 36.97627053650371
At time: 114.08575487136841 and batch: 350, loss is 3.598514199256897 and perplexity is 36.54389713694694
At time: 114.70837116241455 and batch: 400, loss is 3.5481554555892942 and perplexity is 34.74916196445788
At time: 115.33054614067078 and batch: 450, loss is 3.5623676490783693 and perplexity is 35.246549892509556
At time: 115.95285749435425 and batch: 500, loss is 3.4456189680099487 and perplexity is 31.36268994182988
At time: 116.57642006874084 and batch: 550, loss is 3.5008140277862547 and perplexity is 33.142419831550825
At time: 117.19909858703613 and batch: 600, loss is 3.512393889427185 and perplexity is 33.52843515617831
At time: 117.83643627166748 and batch: 650, loss is 3.3595380544662476 and perplexity is 28.775894912338373
At time: 118.46341347694397 and batch: 700, loss is 3.3512370014190673 and perplexity is 28.538013381776235
At time: 119.08710384368896 and batch: 750, loss is 3.449869532585144 and perplexity is 31.496282802080646
At time: 119.71659183502197 and batch: 800, loss is 3.4110737800598145 and perplexity is 30.29775992940606
At time: 120.35225296020508 and batch: 850, loss is 3.459099349975586 and perplexity is 31.788333454615543
At time: 120.98242950439453 and batch: 900, loss is 3.4195668935775756 and perplexity is 30.556178077784324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.322395220194777 and perplexity of 75.36893746681012
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 122.62230372428894 and batch: 50, loss is 3.725153980255127 and perplexity is 41.47761898749425
At time: 123.23785448074341 and batch: 100, loss is 3.6121649026870726 and perplexity is 37.046167410753675
At time: 123.85398030281067 and batch: 150, loss is 3.6372256469726563 and perplexity is 37.98630298448102
At time: 124.46818256378174 and batch: 200, loss is 3.5023359394073488 and perplexity is 33.19289806740732
At time: 125.09499335289001 and batch: 250, loss is 3.6482112073898314 and perplexity is 38.40590436925242
At time: 125.70925426483154 and batch: 300, loss is 3.6082564115524294 and perplexity is 36.90165538980001
At time: 126.3259449005127 and batch: 350, loss is 3.594010772705078 and perplexity is 36.37969439528101
At time: 126.95182704925537 and batch: 400, loss is 3.54475679397583 and perplexity is 34.63126178648153
At time: 127.56847834587097 and batch: 450, loss is 3.5544530963897705 and perplexity is 34.968690234176165
At time: 128.18483543395996 and batch: 500, loss is 3.436128406524658 and perplexity is 31.066448377538208
At time: 128.79982924461365 and batch: 550, loss is 3.486781539916992 and perplexity is 32.68059706660937
At time: 129.41706776618958 and batch: 600, loss is 3.501067724227905 and perplexity is 33.15082901217427
At time: 130.03508162498474 and batch: 650, loss is 3.344891676902771 and perplexity is 28.35750372764212
At time: 130.65246534347534 and batch: 700, loss is 3.3328566455841067 and perplexity is 28.018265746672025
At time: 131.26844453811646 and batch: 750, loss is 3.430823392868042 and perplexity is 30.902076826788075
At time: 131.8836076259613 and batch: 800, loss is 3.3906920528411866 and perplexity is 29.68648978262624
At time: 132.49920296669006 and batch: 850, loss is 3.437529411315918 and perplexity is 31.110003123642397
At time: 133.1126208305359 and batch: 900, loss is 3.39448046207428 and perplexity is 29.7991676547994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.316174650845462 and perplexity of 74.90155496428012
finished 11 epochs...
Completing Train Step...
At time: 134.70376181602478 and batch: 50, loss is 3.713556661605835 and perplexity is 40.999368395827595
At time: 135.33321404457092 and batch: 100, loss is 3.597766613960266 and perplexity is 36.5165876661171
At time: 135.94882655143738 and batch: 150, loss is 3.6245026350021363 and perplexity is 37.50606431625932
At time: 136.5644142627716 and batch: 200, loss is 3.4904995346069336 and perplexity is 32.802329513031324
At time: 137.1826195716858 and batch: 250, loss is 3.636977162361145 and perplexity is 37.97686514536862
At time: 137.81932091712952 and batch: 300, loss is 3.5979307460784913 and perplexity is 36.52258170289461
At time: 138.4465308189392 and batch: 350, loss is 3.583936820030212 and perplexity is 36.01504688035574
At time: 139.06372570991516 and batch: 400, loss is 3.535730595588684 and perplexity is 34.320079655745864
At time: 139.68060159683228 and batch: 450, loss is 3.546402597427368 and perplexity is 34.68830496471473
At time: 140.29728317260742 and batch: 500, loss is 3.4287738895416258 and perplexity is 30.838807774751327
At time: 140.91303324699402 and batch: 550, loss is 3.4807766675949097 and perplexity is 32.484942282730835
At time: 141.5300326347351 and batch: 600, loss is 3.4966167402267456 and perplexity is 33.003603095775944
At time: 142.14615321159363 and batch: 650, loss is 3.3415099287033083 and perplexity is 28.261767759177527
At time: 142.7613763809204 and batch: 700, loss is 3.330635027885437 and perplexity is 27.956088963697173
At time: 143.37784600257874 and batch: 750, loss is 3.4297352409362794 and perplexity is 30.8684689607419
At time: 143.99492502212524 and batch: 800, loss is 3.3909842729568482 and perplexity is 29.695166039730793
At time: 144.61056923866272 and batch: 850, loss is 3.439276247024536 and perplexity is 31.164394680720047
At time: 145.22547936439514 and batch: 900, loss is 3.397756986618042 and perplexity is 29.89696549001445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3156846973994005 and perplexity of 74.86486567806506
finished 12 epochs...
Completing Train Step...
At time: 146.78001070022583 and batch: 50, loss is 3.7079265785217284 and perplexity is 40.76918712322811
At time: 147.40841007232666 and batch: 100, loss is 3.5915691375732424 and perplexity is 36.290976807424464
At time: 148.02384638786316 and batch: 150, loss is 3.618046803474426 and perplexity is 37.26471138893839
At time: 148.63908886909485 and batch: 200, loss is 3.484035358428955 and perplexity is 32.590973333769924
At time: 149.25481963157654 and batch: 250, loss is 3.63051383972168 and perplexity is 37.73219993991018
At time: 149.86882328987122 and batch: 300, loss is 3.5920006799697877 and perplexity is 36.30664128222831
At time: 150.48842787742615 and batch: 350, loss is 3.5779861402511597 and perplexity is 35.80136926321811
At time: 151.12290978431702 and batch: 400, loss is 3.530206708908081 and perplexity is 34.131022071923844
At time: 151.7444715499878 and batch: 450, loss is 3.541375675201416 and perplexity is 34.514367105834594
At time: 152.35757446289062 and batch: 500, loss is 3.4241819620132445 and perplexity is 30.697522837742508
At time: 152.9718475341797 and batch: 550, loss is 3.476751322746277 and perplexity is 32.3544420181511
At time: 153.59817242622375 and batch: 600, loss is 3.493418254852295 and perplexity is 32.898210192649685
At time: 154.21308422088623 and batch: 650, loss is 3.3388881921768188 and perplexity is 28.18776989417827
At time: 154.8265402317047 and batch: 700, loss is 3.3286332178115843 and perplexity is 27.900182159255632
At time: 155.4407308101654 and batch: 750, loss is 3.4283751249313354 and perplexity is 30.826512801150308
At time: 156.05563402175903 and batch: 800, loss is 3.390262317657471 and perplexity is 29.673735194229607
At time: 156.66944479942322 and batch: 850, loss is 3.4392798852920534 and perplexity is 31.164508065331177
At time: 157.28222036361694 and batch: 900, loss is 3.3983993530273438 and perplexity is 29.916176465967514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3159593556025255 and perplexity of 74.88543075160318
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 158.8418848514557 and batch: 50, loss is 3.7064083766937257 and perplexity is 40.707338230242456
At time: 159.45557808876038 and batch: 100, loss is 3.5921275568008424 and perplexity is 36.31124804606006
At time: 160.07022857666016 and batch: 150, loss is 3.6201033449172972 and perplexity is 37.341426669304994
At time: 160.68428993225098 and batch: 200, loss is 3.48446008682251 and perplexity is 32.60481858554601
At time: 161.29707551002502 and batch: 250, loss is 3.6315916919708253 and perplexity is 37.77289160234104
At time: 161.91068768501282 and batch: 300, loss is 3.592965774536133 and perplexity is 36.341697538032065
At time: 162.52452516555786 and batch: 350, loss is 3.577940664291382 and perplexity is 35.799741198608686
At time: 163.13911414146423 and batch: 400, loss is 3.529924988746643 and perplexity is 34.121408029174134
At time: 163.75314927101135 and batch: 450, loss is 3.5398776006698607 and perplexity is 34.46270072121065
At time: 164.36814618110657 and batch: 500, loss is 3.42173038482666 and perplexity is 30.62235766532647
At time: 164.9821639060974 and batch: 550, loss is 3.4718937349319456 and perplexity is 32.197658577819325
At time: 165.59537506103516 and batch: 600, loss is 3.488296990394592 and perplexity is 32.73016043897482
At time: 166.21060395240784 and batch: 650, loss is 3.334273295402527 and perplexity is 28.057985945918162
At time: 166.82309913635254 and batch: 700, loss is 3.3237914848327637 and perplexity is 27.76542342334332
At time: 167.44345378875732 and batch: 750, loss is 3.420485453605652 and perplexity is 30.584258656451503
At time: 168.06456661224365 and batch: 800, loss is 3.3825650644302367 and perplexity is 29.446205739538538
At time: 168.67887234687805 and batch: 850, loss is 3.4290929508209227 and perplexity is 30.848648814075744
At time: 169.31088399887085 and batch: 900, loss is 3.3882058238983155 and perplexity is 29.612774047585226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.315227769825556 and perplexity of 74.83066567068283
finished 14 epochs...
Completing Train Step...
At time: 170.89801692962646 and batch: 50, loss is 3.7028867387771607 and perplexity is 40.56423385324597
At time: 171.51313614845276 and batch: 100, loss is 3.588409910202026 and perplexity is 36.176506274594686
At time: 172.14108109474182 and batch: 150, loss is 3.616235098838806 and perplexity is 37.197259858145046
At time: 172.75783967971802 and batch: 200, loss is 3.481182451248169 and perplexity is 32.4981268161394
At time: 173.37311029434204 and batch: 250, loss is 3.6281214141845703 and perplexity is 37.64203635903509
At time: 173.98912644386292 and batch: 300, loss is 3.5899629306793215 and perplexity is 36.2327327787914
At time: 174.6048903465271 and batch: 350, loss is 3.574713315963745 and perplexity is 35.68438920437773
At time: 175.21952295303345 and batch: 400, loss is 3.5272836446762086 and perplexity is 34.03140057306121
At time: 175.851580619812 and batch: 450, loss is 3.537487964630127 and perplexity is 34.38044572843042
At time: 176.47182869911194 and batch: 500, loss is 3.4196528720855714 and perplexity is 30.55880536532902
At time: 177.08656692504883 and batch: 550, loss is 3.470214219093323 and perplexity is 32.143627486009336
At time: 177.71601819992065 and batch: 600, loss is 3.487421770095825 and perplexity is 32.70152687034478
At time: 178.34151554107666 and batch: 650, loss is 3.3334746980667114 and perplexity is 28.035587857810526
At time: 178.95648002624512 and batch: 700, loss is 3.322996735572815 and perplexity is 27.743365639989978
At time: 179.57151341438293 and batch: 750, loss is 3.420360918045044 and perplexity is 30.580450065811352
At time: 180.19169855117798 and batch: 800, loss is 3.38325457572937 and perplexity is 29.466516232465054
At time: 180.81560134887695 and batch: 850, loss is 3.430321760177612 and perplexity is 30.886579222227105
At time: 181.43251538276672 and batch: 900, loss is 3.389814968109131 and perplexity is 29.66046363092816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314805958369007 and perplexity of 74.79910789478535
finished 15 epochs...
Completing Train Step...
At time: 182.98726892471313 and batch: 50, loss is 3.701087760925293 and perplexity is 40.49132529506964
At time: 183.63441038131714 and batch: 100, loss is 3.5862807416915894 and perplexity is 36.099562338987454
At time: 184.2701268196106 and batch: 150, loss is 3.6139830684661867 and perplexity is 37.11358475396847
At time: 184.92373371124268 and batch: 200, loss is 3.4790730476379395 and perplexity is 32.42964740086271
At time: 185.55081343650818 and batch: 250, loss is 3.6260160112380984 and perplexity is 37.562868074587485
At time: 186.17058968544006 and batch: 300, loss is 3.58811288356781 and perplexity is 36.16576248437317
At time: 186.79172492027283 and batch: 350, loss is 3.572805848121643 and perplexity is 35.61638725588868
At time: 187.41038155555725 and batch: 400, loss is 3.5255827856063844 and perplexity is 33.97356715392773
At time: 188.02923226356506 and batch: 450, loss is 3.535938367843628 and perplexity is 34.32721115692417
At time: 188.64925408363342 and batch: 500, loss is 3.4183180046081545 and perplexity is 30.51804062371516
At time: 189.26952385902405 and batch: 550, loss is 3.4690880393981933 and perplexity is 32.10744836132836
At time: 189.8967845439911 and batch: 600, loss is 3.48673846244812 and perplexity is 32.67918929952826
At time: 190.5155165195465 and batch: 650, loss is 3.332936964035034 and perplexity is 28.020516220750558
At time: 191.13417744636536 and batch: 700, loss is 3.3225756597518923 and perplexity is 27.731686038699383
At time: 191.75414609909058 and batch: 750, loss is 3.4202492332458494 and perplexity is 30.577034885101906
At time: 192.37268829345703 and batch: 800, loss is 3.3835333347320558 and perplexity is 29.474731434120155
At time: 192.99276041984558 and batch: 850, loss is 3.4308548259735105 and perplexity is 30.90304819029458
At time: 193.61157417297363 and batch: 900, loss is 3.390571393966675 and perplexity is 29.682908060268502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314683051958476 and perplexity of 74.78991516985708
finished 16 epochs...
Completing Train Step...
At time: 195.17119455337524 and batch: 50, loss is 3.6995902347564695 and perplexity is 40.43073385578636
At time: 195.8041889667511 and batch: 100, loss is 3.584599847793579 and perplexity is 36.03893377429824
At time: 196.4241464138031 and batch: 150, loss is 3.6122171545028685 and perplexity is 37.04810319084274
At time: 197.04549074172974 and batch: 200, loss is 3.4773739433288573 and perplexity is 32.374592832194324
At time: 197.6823480129242 and batch: 250, loss is 3.624332056045532 and perplexity is 37.49966711657125
At time: 198.31264853477478 and batch: 300, loss is 3.5865860748291016 and perplexity is 36.11058641454138
At time: 198.93525958061218 and batch: 350, loss is 3.5712632751464843 and perplexity is 35.56148873281006
At time: 199.57167053222656 and batch: 400, loss is 3.5241693592071535 and perplexity is 33.925581937011145
At time: 200.20651817321777 and batch: 450, loss is 3.534651966094971 and perplexity is 34.28308096313028
At time: 200.85876441001892 and batch: 500, loss is 3.4171917963027956 and perplexity is 30.48369029934242
At time: 201.48914504051208 and batch: 550, loss is 3.4681044816970825 and perplexity is 32.07588435828324
At time: 202.1121745109558 and batch: 600, loss is 3.486059284210205 and perplexity is 32.65700184079577
At time: 202.73752856254578 and batch: 650, loss is 3.332399778366089 and perplexity is 28.00546804319486
At time: 203.36376643180847 and batch: 700, loss is 3.3221735048294065 and perplexity is 27.720535846852634
At time: 203.9957799911499 and batch: 750, loss is 3.420040264129639 and perplexity is 30.5706458967195
At time: 204.62490892410278 and batch: 800, loss is 3.3835704565048217 and perplexity is 29.475825608711514
At time: 205.25185441970825 and batch: 850, loss is 3.4310638093948365 and perplexity is 30.90950708991275
At time: 205.87263655662537 and batch: 900, loss is 3.3909534502029417 and perplexity is 29.69425076704142
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314691412938784 and perplexity of 74.7905404894792
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 207.43217587471008 and batch: 50, loss is 3.699008183479309 and perplexity is 40.407207942817095
At time: 208.04965734481812 and batch: 100, loss is 3.584497127532959 and perplexity is 36.03523203575351
At time: 208.67252802848816 and batch: 150, loss is 3.612566547393799 and perplexity is 37.06104979631432
At time: 209.28914141654968 and batch: 200, loss is 3.4774367141723634 and perplexity is 32.37662507647675
At time: 209.91304516792297 and batch: 250, loss is 3.6244465494155884 and perplexity is 37.50396082563134
At time: 210.53534603118896 and batch: 300, loss is 3.5869918298721313 and perplexity is 36.12524144006003
At time: 211.14915204048157 and batch: 350, loss is 3.571425585746765 and perplexity is 35.56726120784743
At time: 211.7624475955963 and batch: 400, loss is 3.5241076374053955 and perplexity is 33.923488053588095
At time: 212.38152837753296 and batch: 450, loss is 3.5347321796417237 and perplexity is 34.28583104094322
At time: 212.9946916103363 and batch: 500, loss is 3.4163192462921144 and perplexity is 30.457103355951812
At time: 213.60515809059143 and batch: 550, loss is 3.4660482215881347 and perplexity is 32.00999576207996
At time: 214.217116355896 and batch: 600, loss is 3.4836398649215696 and perplexity is 32.57808636394147
At time: 214.82856488227844 and batch: 650, loss is 3.3307558822631838 and perplexity is 27.95946778360159
At time: 215.44277143478394 and batch: 700, loss is 3.3206936502456665 and perplexity is 27.679543823413773
At time: 216.05490040779114 and batch: 750, loss is 3.4176737785339357 and perplexity is 30.498386437760573
At time: 216.69015383720398 and batch: 800, loss is 3.3809063959121706 and perplexity is 29.39740472864045
At time: 217.30511164665222 and batch: 850, loss is 3.427606883049011 and perplexity is 30.802839677437337
At time: 218.07816290855408 and batch: 900, loss is 3.387423343658447 and perplexity is 29.589611700266655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.314129555062072 and perplexity of 74.74853063808973
finished 18 epochs...
Completing Train Step...
At time: 219.63843059539795 and batch: 50, loss is 3.6983040952682495 and perplexity is 40.37876771745133
At time: 220.2519724369049 and batch: 100, loss is 3.5837934160232545 and perplexity is 36.00988254862411
At time: 220.86318802833557 and batch: 150, loss is 3.611816201210022 and perplexity is 37.03325160946885
At time: 221.47597193717957 and batch: 200, loss is 3.476763391494751 and perplexity is 32.35483249813013
At time: 222.0888864994049 and batch: 250, loss is 3.623738031387329 and perplexity is 37.47739800448529
At time: 222.70175075531006 and batch: 300, loss is 3.586187343597412 and perplexity is 36.09619086611138
At time: 223.31462287902832 and batch: 350, loss is 3.570725750923157 and perplexity is 35.54237870770942
At time: 223.95172119140625 and batch: 400, loss is 3.523510913848877 and perplexity is 33.90325114766698
At time: 224.57078337669373 and batch: 450, loss is 3.5341649723052977 and perplexity is 34.26638938028885
At time: 225.18356895446777 and batch: 500, loss is 3.415927529335022 and perplexity is 30.445175128500377
At time: 225.79837036132812 and batch: 550, loss is 3.465804114341736 and perplexity is 32.002182843791026
At time: 226.41206789016724 and batch: 600, loss is 3.483545732498169 and perplexity is 32.57501985405356
At time: 227.0264699459076 and batch: 650, loss is 3.330642523765564 and perplexity is 27.956298519974265
At time: 227.6392662525177 and batch: 700, loss is 3.3206056356430054 and perplexity is 27.677107726569933
At time: 228.2517795562744 and batch: 750, loss is 3.4176028633117674 and perplexity is 30.496223714596468
At time: 228.86962962150574 and batch: 800, loss is 3.381119132041931 and perplexity is 29.403659284008786
At time: 229.48458433151245 and batch: 850, loss is 3.427847633361816 and perplexity is 30.81025636347388
At time: 230.10150861740112 and batch: 900, loss is 3.3877644157409668 and perplexity is 29.599705612028114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313859495398116 and perplexity of 74.72834680056727
finished 19 epochs...
Completing Train Step...
At time: 231.6739501953125 and batch: 50, loss is 3.697774019241333 and perplexity is 40.35736957251074
At time: 232.30639624595642 and batch: 100, loss is 3.5832181215286254 and perplexity is 35.98917221928207
At time: 232.92120575904846 and batch: 150, loss is 3.611199893951416 and perplexity is 37.01043477950333
At time: 233.5386197566986 and batch: 200, loss is 3.476212658882141 and perplexity is 32.3370185425113
At time: 234.15476322174072 and batch: 250, loss is 3.6231505060195923 and perplexity is 37.45538554951194
At time: 234.77122926712036 and batch: 300, loss is 3.5856074285507202 and perplexity is 36.07526421032762
At time: 235.38728547096252 and batch: 350, loss is 3.570179891586304 and perplexity is 35.52298286262102
At time: 236.00403547286987 and batch: 400, loss is 3.5230474853515625 and perplexity is 33.88754305500356
At time: 236.61677861213684 and batch: 450, loss is 3.5337255573272706 and perplexity is 34.2513355232396
At time: 237.2368643283844 and batch: 500, loss is 3.4155886697769167 and perplexity is 30.43486023765827
At time: 237.8744773864746 and batch: 550, loss is 3.4655658340454103 and perplexity is 31.994558262609747
At time: 238.49992394447327 and batch: 600, loss is 3.4834346866607664 and perplexity is 32.5714027345322
At time: 239.12588667869568 and batch: 650, loss is 3.330539135932922 and perplexity is 27.9534083282695
At time: 239.74304008483887 and batch: 700, loss is 3.320524101257324 and perplexity is 27.674851182588228
At time: 240.3598084449768 and batch: 750, loss is 3.4175669384002685 and perplexity is 30.495128160137437
At time: 240.9763035774231 and batch: 800, loss is 3.381266169548035 and perplexity is 29.407983042609782
At time: 241.5925648212433 and batch: 850, loss is 3.4280261421203613 and perplexity is 30.815756755007207
At time: 242.20810174942017 and batch: 900, loss is 3.388015570640564 and perplexity is 29.607140656754083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.313719867026969 and perplexity of 74.71791333164609
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f34428f2b70>
ELAPSED
1501.3523766994476


RESULTS SO FAR:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}, {'params': {'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.19514626483225}, {'params': {'num_layers': 1, 'rnn_dropout': 0.5844906825889364, 'batch_size': 32, 'dropout': 0.5939710512473378, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.5312201255317}, {'params': {'num_layers': 1, 'rnn_dropout': 0.16540051295511726, 'batch_size': 32, 'dropout': 0.5337025234107475, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.36190275918786}, {'params': {'num_layers': 1, 'rnn_dropout': 1.0, 'batch_size': 32, 'dropout': 0.7554479327164237, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.71791333164609}]
here
Saving Model Parameters and Results...
/home-nfs/siddsach/ml/Interpreting-Attention/interpreting_language/trained_models/langmodel/



FINAL RESULTS:
[{'params': {'num_layers': 1, 'rnn_dropout': 0.39574039908140535, 'batch_size': 32, 'dropout': 0.08488702009850746, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.07177508223263}, {'params': {'num_layers': 1, 'rnn_dropout': 0.21681538762305053, 'batch_size': 32, 'dropout': 0.8204969826418383, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.17076388124072}, {'params': {'num_layers': 1, 'rnn_dropout': 0.9492725295489616, 'batch_size': 32, 'dropout': 0.9137138357868608, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.19514626483225}, {'params': {'num_layers': 1, 'rnn_dropout': 0.5844906825889364, 'batch_size': 32, 'dropout': 0.5939710512473378, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -75.5312201255317}, {'params': {'num_layers': 1, 'rnn_dropout': 0.16540051295511726, 'batch_size': 32, 'dropout': 0.5337025234107475, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -76.36190275918786}, {'params': {'num_layers': 1, 'rnn_dropout': 1.0, 'batch_size': 32, 'dropout': 0.7554479327164237, 'tune_wordvecs': True, 'wordvec_dim': 300, 'data': 'ptb', 'tie_weights': True, 'wordvec_source': 'gigavec', 'seq_len': 35}, 'best_accuracy': -74.71791333164609}]
