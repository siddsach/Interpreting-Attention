Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'type': 'continuous', 'domain': [0, 30]}, {'name': 'dropout', 'type': 'continuous', 'domain': [0, 1]}, {'name': 'anneal', 'type': 'continuous', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.7178254696833063, 'tune_wordvecs': True, 'lr': 3.893393447066984, 'num_layers': 1, 'anneal': 4.251717208023265, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.4471402168273926 and batch: 50, loss is 7.600318927764892 and perplexity is 1998.8332768887315
At time: 4.094165563583374 and batch: 100, loss is 6.626797571182251 and perplexity is 755.0602679649063
At time: 5.702362537384033 and batch: 150, loss is 6.427311315536499 and perplexity is 618.508735572586
At time: 7.302923917770386 and batch: 200, loss is 6.385221223831177 and perplexity is 593.0159073395918
At time: 8.908974647521973 and batch: 250, loss is 6.365425968170166 and perplexity is 581.3924302476693
At time: 10.511977195739746 and batch: 300, loss is 6.270107936859131 and perplexity is 528.5344231445655
At time: 12.111884355545044 and batch: 350, loss is 6.2191783046722415 and perplexity is 502.2903327851569
At time: 13.711775541305542 and batch: 400, loss is 6.226862249374389 and perplexity is 506.1647703454996
At time: 15.311395168304443 and batch: 450, loss is 6.205255870819092 and perplexity is 495.3456842324168
At time: 16.908897161483765 and batch: 500, loss is 6.193661012649536 and perplexity is 489.63539027763005
At time: 18.51345920562744 and batch: 550, loss is 6.125489292144775 and perplexity is 457.36844531684005
At time: 20.112071752548218 and batch: 600, loss is 6.134834222793579 and perplexity is 461.66255454622893
At time: 21.710511684417725 and batch: 650, loss is 6.140848922729492 and perplexity is 464.44768374530537
At time: 23.30970573425293 and batch: 700, loss is 6.092527055740357 and perplexity is 442.53831830586677
At time: 24.909353256225586 and batch: 750, loss is 6.071478185653686 and perplexity is 433.3207368943573
At time: 26.514808893203735 and batch: 800, loss is 6.084925651550293 and perplexity is 439.1871585748376
At time: 28.117545127868652 and batch: 850, loss is 6.079824275970459 and perplexity is 436.9524049343713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.412192662556966 and perplexity of 224.1224742670545
Finished 1 epochs...
Completing Train Step...
At time: 32.4094181060791 and batch: 50, loss is 5.70708251953125 and perplexity is 300.991648805348
At time: 33.99603271484375 and batch: 100, loss is 5.533545274734497 and perplexity is 253.0394169299884
At time: 35.595959424972534 and batch: 150, loss is 5.451011257171631 and perplexity is 232.9936633263495
At time: 37.1917724609375 and batch: 200, loss is 5.426013412475586 and perplexity is 227.24151905599794
At time: 38.787774324417114 and batch: 250, loss is 5.443097867965698 and perplexity is 231.15716981359225
At time: 40.38518238067627 and batch: 300, loss is 5.360739097595215 and perplexity is 212.8822290623505
At time: 41.98184061050415 and batch: 350, loss is 5.304046421051026 and perplexity is 201.14909938411537
At time: 43.57529950141907 and batch: 400, loss is 5.306686515808106 and perplexity is 201.68085369881484
At time: 45.187243938446045 and batch: 450, loss is 5.285002870559692 and perplexity is 197.35474996593445
At time: 46.803279399871826 and batch: 500, loss is 5.263045768737793 and perplexity is 193.0686391103697
At time: 48.5016348361969 and batch: 550, loss is 5.223836174011231 and perplexity is 185.64498629021045
At time: 50.111536741256714 and batch: 600, loss is 5.250469818115234 and perplexity is 190.6558209787553
At time: 51.723785161972046 and batch: 650, loss is 5.240388593673706 and perplexity is 188.7434326607921
At time: 53.339102268218994 and batch: 700, loss is 5.175353393554688 and perplexity is 176.85910410150862
At time: 54.95138692855835 and batch: 750, loss is 5.165900363922119 and perplexity is 175.195126953325
At time: 56.56866788864136 and batch: 800, loss is 5.1523302459716795 and perplexity is 172.83376662189394
At time: 58.184398889541626 and batch: 850, loss is 5.13952054977417 and perplexity is 170.63393822594492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.879568099975586 and perplexity of 131.5738248741531
Finished 2 epochs...
Completing Train Step...
At time: 62.43009948730469 and batch: 50, loss is 5.080437183380127 and perplexity is 160.84435904037116
At time: 64.03025317192078 and batch: 100, loss is 4.997407884597778 and perplexity is 148.02895323507303
At time: 65.63176774978638 and batch: 150, loss is 4.966794891357422 and perplexity is 143.56600453376905
At time: 67.22960877418518 and batch: 200, loss is 4.988457469940186 and perplexity is 146.70994436591093
At time: 68.82596015930176 and batch: 250, loss is 5.004643602371216 and perplexity is 149.10393339942047
At time: 70.42400598526001 and batch: 300, loss is 4.961733388900757 and perplexity is 142.841180745247
At time: 72.02332282066345 and batch: 350, loss is 4.9236544990539555 and perplexity is 137.50420508205727
At time: 73.63011574745178 and batch: 400, loss is 4.938111381530762 and perplexity is 139.50652600021596
At time: 75.23146104812622 and batch: 450, loss is 4.935918016433716 and perplexity is 139.20087258258368
At time: 76.83099603652954 and batch: 500, loss is 4.925029153823853 and perplexity is 137.69335587218606
At time: 78.42800211906433 and batch: 550, loss is 4.91724084854126 and perplexity is 136.62512324189322
At time: 80.03091311454773 and batch: 600, loss is 4.950930242538452 and perplexity is 141.30635198018692
At time: 81.62779307365417 and batch: 650, loss is 4.941748857498169 and perplexity is 140.01490167745618
At time: 83.28633880615234 and batch: 700, loss is 4.897692651748657 and perplexity is 133.98028359155052
At time: 84.9072527885437 and batch: 750, loss is 4.884933166503906 and perplexity is 132.28162419544196
At time: 86.5267162322998 and batch: 800, loss is 4.866338653564453 and perplexity is 129.8446393125796
At time: 88.14306855201721 and batch: 850, loss is 4.8709398460388185 and perplexity is 130.44345606893708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.739983558654785 and perplexity of 114.43232024341118
Finished 3 epochs...
Completing Train Step...
At time: 92.328045129776 and batch: 50, loss is 4.8381813621521 and perplexity is 126.23955879885558
At time: 93.98553562164307 and batch: 100, loss is 4.765971202850341 and perplexity is 117.4451249566775
At time: 95.60037112236023 and batch: 150, loss is 4.751271457672119 and perplexity is 115.7313385191786
At time: 97.21033835411072 and batch: 200, loss is 4.776902313232422 and perplexity is 118.73597292874652
At time: 98.82491374015808 and batch: 250, loss is 4.790313110351563 and perplexity is 120.33904216472496
At time: 100.4337682723999 and batch: 300, loss is 4.759252967834473 and perplexity is 116.6587455044829
At time: 102.04335856437683 and batch: 350, loss is 4.723844194412232 and perplexity is 112.60027910271377
At time: 103.65330410003662 and batch: 400, loss is 4.742188329696655 and perplexity is 114.68489564248637
At time: 105.2652268409729 and batch: 450, loss is 4.749651794433594 and perplexity is 115.54404444224356
At time: 106.87775373458862 and batch: 500, loss is 4.739609699249268 and perplexity is 114.38954664034816
At time: 108.48477792739868 and batch: 550, loss is 4.745181331634521 and perplexity is 115.02866194737338
At time: 110.09499335289001 and batch: 600, loss is 4.778068313598633 and perplexity is 118.87449986219679
At time: 111.70842456817627 and batch: 650, loss is 4.763206958770752 and perplexity is 117.12092625394641
At time: 113.32093095779419 and batch: 700, loss is 4.730610523223877 and perplexity is 113.36475303974866
At time: 114.93032097816467 and batch: 750, loss is 4.714983053207398 and perplexity is 111.60691977835909
At time: 116.54106378555298 and batch: 800, loss is 4.690088233947754 and perplexity is 108.8627847769319
At time: 118.15199112892151 and batch: 850, loss is 4.7055277824401855 and perplexity is 110.55661939453212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.678872426350911 and perplexity of 107.64862236230418
Finished 4 epochs...
Completing Train Step...
At time: 122.32868814468384 and batch: 50, loss is 4.6810465335845945 and perplexity is 107.88291660899903
At time: 123.98825693130493 and batch: 100, loss is 4.613767671585083 and perplexity is 100.86345501207103
At time: 125.59996914863586 and batch: 150, loss is 4.606052703857422 and perplexity is 100.08829074028073
At time: 127.21441102027893 and batch: 200, loss is 4.634906663894653 and perplexity is 103.01830221937813
At time: 128.8277668952942 and batch: 250, loss is 4.645152320861817 and perplexity is 104.07921801425722
At time: 130.43809843063354 and batch: 300, loss is 4.6167227268219 and perplexity is 101.16195291468736
At time: 132.04978609085083 and batch: 350, loss is 4.584006452560425 and perplexity is 97.90586468170007
At time: 133.66835689544678 and batch: 400, loss is 4.605772123336792 and perplexity is 100.06021185493421
At time: 135.27864503860474 and batch: 450, loss is 4.617717952728271 and perplexity is 101.26268202676398
At time: 136.8889091014862 and batch: 500, loss is 4.6101790809631344 and perplexity is 100.50214604604508
At time: 138.50190329551697 and batch: 550, loss is 4.61912088394165 and perplexity is 101.4048463041634
At time: 140.11664843559265 and batch: 600, loss is 4.652320489883423 and perplexity is 104.82795577401954
At time: 141.73037433624268 and batch: 650, loss is 4.6387034511566165 and perplexity is 103.41018427262016
At time: 143.34861731529236 and batch: 700, loss is 4.6095351886749265 and perplexity is 100.43745431874493
At time: 144.9610254764557 and batch: 750, loss is 4.5949624824523925 and perplexity is 98.98442182524522
At time: 146.57805132865906 and batch: 800, loss is 4.564158115386963 and perplexity is 95.9817544303143
At time: 148.19258880615234 and batch: 850, loss is 4.584202642440796 and perplexity is 97.92507470592413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.644473711649577 and perplexity of 104.0086128574745
Finished 5 epochs...
Completing Train Step...
At time: 152.43471431732178 and batch: 50, loss is 4.5611174774169925 and perplexity is 95.69035191266191
At time: 154.05221104621887 and batch: 100, loss is 4.496766796112061 and perplexity is 89.72655755682736
At time: 155.67421174049377 and batch: 150, loss is 4.498910388946533 and perplexity is 89.91910105637308
At time: 157.29480385780334 and batch: 200, loss is 4.525683298110962 and perplexity is 92.35901301164792
At time: 158.91419196128845 and batch: 250, loss is 4.533586292266846 and perplexity is 93.09181761288258
At time: 160.53063988685608 and batch: 300, loss is 4.507309675216675 and perplexity is 90.67753803309566
At time: 162.1496126651764 and batch: 350, loss is 4.476444282531738 and perplexity is 87.9214922292692
At time: 163.7669641971588 and batch: 400, loss is 4.497504949569702 and perplexity is 89.7928139762243
At time: 165.43682026863098 and batch: 450, loss is 4.516112184524536 and perplexity is 91.47925127310755
At time: 167.05614757537842 and batch: 500, loss is 4.507527256011963 and perplexity is 90.69726987049229
At time: 168.67277812957764 and batch: 550, loss is 4.521868286132812 and perplexity is 92.00733352797752
At time: 170.2905831336975 and batch: 600, loss is 4.5545594596862795 and perplexity is 95.06486610339276
At time: 171.90773153305054 and batch: 650, loss is 4.535111665725708 and perplexity is 93.23392575714793
At time: 173.53429698944092 and batch: 700, loss is 4.5117738151550295 and perplexity is 91.08324013386226
At time: 175.14826893806458 and batch: 750, loss is 4.495255737304688 and perplexity is 89.59107783646658
At time: 176.7641954421997 and batch: 800, loss is 4.4609302806854245 and perplexity is 86.56800419128305
At time: 178.3791491985321 and batch: 850, loss is 4.486124763488769 and perplexity is 88.7767475228572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.629202524820964 and perplexity of 102.4323442779632
Finished 6 epochs...
Completing Train Step...
At time: 182.57538199424744 and batch: 50, loss is 4.46552267074585 and perplexity is 86.96647249390881
At time: 184.18661522865295 and batch: 100, loss is 4.406044845581055 and perplexity is 81.9447177053093
At time: 185.79665803909302 and batch: 150, loss is 4.4077003288269045 and perplexity is 82.08048816440851
At time: 187.40586829185486 and batch: 200, loss is 4.436260299682617 and perplexity is 84.45850086323894
At time: 189.01191973686218 and batch: 250, loss is 4.4434450912475585 and perplexity is 85.06750274383283
At time: 190.6243975162506 and batch: 300, loss is 4.418676023483276 and perplexity is 82.98634062608447
At time: 192.24080181121826 and batch: 350, loss is 4.3864382457733155 and perplexity is 80.3537085265202
At time: 193.8534927368164 and batch: 400, loss is 4.410116567611694 and perplexity is 82.2790540180385
At time: 195.4651744365692 and batch: 450, loss is 4.431901760101319 and perplexity is 84.09118620347652
At time: 197.0765061378479 and batch: 500, loss is 4.423054485321045 and perplexity is 83.35048977639458
At time: 198.68922066688538 and batch: 550, loss is 4.4408598804473876 and perplexity is 84.84786933864474
At time: 200.30619525909424 and batch: 600, loss is 4.470468683242798 and perplexity is 87.39767524172507
At time: 201.91826152801514 and batch: 650, loss is 4.450420083999634 and perplexity is 85.66292206768921
At time: 203.52917623519897 and batch: 700, loss is 4.430243940353393 and perplexity is 83.95189366722285
At time: 205.13988637924194 and batch: 750, loss is 4.414325370788574 and perplexity is 82.62608013201684
At time: 206.80398392677307 and batch: 800, loss is 4.377398386001587 and perplexity is 79.63059561330523
At time: 208.41760635375977 and batch: 850, loss is 4.404844217300415 and perplexity is 81.84639159814724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.615392367045085 and perplexity of 101.02746060300252
Finished 7 epochs...
Completing Train Step...
At time: 212.6023473739624 and batch: 50, loss is 4.386116418838501 and perplexity is 80.32785269957806
At time: 214.21622467041016 and batch: 100, loss is 4.328011732101441 and perplexity is 75.79343899451231
At time: 215.8292384147644 and batch: 150, loss is 4.330836420059204 and perplexity is 76.00783446651891
At time: 217.4415943622589 and batch: 200, loss is 4.3578116321563725 and perplexity is 78.08606627486543
At time: 219.0545883178711 and batch: 250, loss is 4.363887157440185 and perplexity is 78.56192422459597
At time: 220.66947078704834 and batch: 300, loss is 4.3413026237487795 and perplexity is 76.80752550878994
At time: 222.28553533554077 and batch: 350, loss is 4.309420843124389 and perplexity is 74.3973887014564
At time: 223.89834642410278 and batch: 400, loss is 4.335786380767822 and perplexity is 76.3850029752062
At time: 225.50940084457397 and batch: 450, loss is 4.359652242660522 and perplexity is 78.22992466169347
At time: 227.12219500541687 and batch: 500, loss is 4.34864688873291 and perplexity is 77.37369684018971
At time: 228.73352599143982 and batch: 550, loss is 4.370469045639038 and perplexity is 79.08071546661661
At time: 230.34426069259644 and batch: 600, loss is 4.399967231750488 and perplexity is 81.44819970630955
At time: 231.95630931854248 and batch: 650, loss is 4.378093461990357 and perplexity is 79.68596416873504
At time: 233.57394576072693 and batch: 700, loss is 4.359571590423584 and perplexity is 78.22361549770147
At time: 235.18607473373413 and batch: 750, loss is 4.342511682510376 and perplexity is 76.90044648260216
At time: 236.79836988449097 and batch: 800, loss is 4.3062241172790525 and perplexity is 74.1599403769749
At time: 238.40883255004883 and batch: 850, loss is 4.336220026016235 and perplexity is 76.4181341518659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.612077077229817 and perplexity of 100.69307988257039
Finished 8 epochs...
Completing Train Step...
At time: 242.57446599006653 and batch: 50, loss is 4.316487636566162 and perplexity is 74.92500175050539
At time: 244.21577382087708 and batch: 100, loss is 4.260376625061035 and perplexity is 70.83665729131937
At time: 245.83046007156372 and batch: 150, loss is 4.2666195154190065 and perplexity is 71.2802660357364
At time: 247.47381353378296 and batch: 200, loss is 4.2912256622314455 and perplexity is 73.05595557349544
At time: 249.0879943370819 and batch: 250, loss is 4.2967222213745115 and perplexity is 73.45861756801851
At time: 250.7022581100464 and batch: 300, loss is 4.273975915908814 and perplexity is 71.80656568174074
At time: 252.31233048439026 and batch: 350, loss is 4.241344947814941 and perplexity is 69.50126458097617
At time: 253.9217541217804 and batch: 400, loss is 4.270025415420532 and perplexity is 71.5234533947653
At time: 255.529070854187 and batch: 450, loss is 4.29723482131958 and perplexity is 73.49628210394347
At time: 257.1345007419586 and batch: 500, loss is 4.283605651855469 and perplexity is 72.50138403783647
At time: 258.74031162261963 and batch: 550, loss is 4.308291940689087 and perplexity is 74.31344869712345
At time: 260.3562731742859 and batch: 600, loss is 4.334793558120728 and perplexity is 76.30920384812528
At time: 261.96644926071167 and batch: 650, loss is 4.315856790542602 and perplexity is 74.87775051677936
At time: 263.577490568161 and batch: 700, loss is 4.297500829696656 and perplexity is 73.51583533121288
At time: 265.18815326690674 and batch: 750, loss is 4.278315534591675 and perplexity is 72.11885591589775
At time: 266.8098871707916 and batch: 800, loss is 4.242863645553589 and perplexity is 69.60689618526499
At time: 268.42024421691895 and batch: 850, loss is 4.2741115760803225 and perplexity is 71.81630763353905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.611204465230306 and perplexity of 100.60525221810705
Finished 9 epochs...
Completing Train Step...
At time: 272.6067373752594 and batch: 50, loss is 4.254704465866089 and perplexity is 70.43599787054771
At time: 274.2478151321411 and batch: 100, loss is 4.201017074584961 and perplexity is 66.75419051661545
At time: 275.8611822128296 and batch: 150, loss is 4.207142205238342 and perplexity is 67.16432343178248
At time: 277.4760901927948 and batch: 200, loss is 4.230553617477417 and perplexity is 68.75528576717288
At time: 279.09581232070923 and batch: 250, loss is 4.23460880279541 and perplexity is 69.03466728160006
At time: 280.71222710609436 and batch: 300, loss is 4.213882207870483 and perplexity is 67.6185401399049
At time: 282.329039812088 and batch: 350, loss is 4.181599526405335 and perplexity is 65.47049128560265
At time: 283.94359374046326 and batch: 400, loss is 4.21115894317627 and perplexity is 67.4346474647308
At time: 285.55998945236206 and batch: 450, loss is 4.240520877838135 and perplexity is 69.44401426785686
At time: 287.2252860069275 and batch: 500, loss is 4.225564441680908 and perplexity is 68.41310786218925
At time: 288.83726501464844 and batch: 550, loss is 4.251885843276978 and perplexity is 70.23774490725744
At time: 290.4509243965149 and batch: 600, loss is 4.277297945022583 and perplexity is 72.04550584684488
At time: 292.06697058677673 and batch: 650, loss is 4.2568878555297855 and perplexity is 70.58995511341482
At time: 293.6837477684021 and batch: 700, loss is 4.240564370155335 and perplexity is 69.44703461463351
At time: 295.29844093322754 and batch: 750, loss is 4.224490690231323 and perplexity is 68.33968861251849
At time: 296.9162106513977 and batch: 800, loss is 4.188367137908935 and perplexity is 65.91507281627494
At time: 298.53605914115906 and batch: 850, loss is 4.219000453948975 and perplexity is 67.96551566340547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.609658241271973 and perplexity of 100.44981416877978
Finished 10 epochs...
Completing Train Step...
At time: 302.7070372104645 and batch: 50, loss is 4.197395648956299 and perplexity is 66.51288238371052
At time: 304.3467798233032 and batch: 100, loss is 4.146886944770813 and perplexity is 63.236833806106695
At time: 305.9548020362854 and batch: 150, loss is 4.152739806175232 and perplexity is 63.60803546630724
At time: 307.560338973999 and batch: 200, loss is 4.17724910736084 and perplexity is 65.18628586803453
At time: 309.1718864440918 and batch: 250, loss is 4.180687184333801 and perplexity is 65.41078704144705
At time: 310.78730273246765 and batch: 300, loss is 4.161336545944214 and perplexity is 64.15721438635563
At time: 312.3999457359314 and batch: 350, loss is 4.127979946136475 and perplexity is 62.05244695809447
At time: 314.01365756988525 and batch: 400, loss is 4.159694967269897 and perplexity is 64.0519816689394
At time: 315.6273002624512 and batch: 450, loss is 4.1909780597686765 and perplexity is 66.08739678509285
At time: 317.2371778488159 and batch: 500, loss is 4.1742102146148685 and perplexity is 64.98849242551852
At time: 318.84321808815 and batch: 550, loss is 4.2025937604904176 and perplexity is 66.85952392496232
At time: 320.4466326236725 and batch: 600, loss is 4.228249711990356 and perplexity is 68.59706242283225
At time: 322.05267453193665 and batch: 650, loss is 4.203255119323731 and perplexity is 66.90375668695287
At time: 323.6522936820984 and batch: 700, loss is 4.190193829536438 and perplexity is 66.0355893676907
At time: 325.26080894470215 and batch: 750, loss is 4.172640709877014 and perplexity is 64.88657268143324
At time: 326.87505555152893 and batch: 800, loss is 4.140983409881592 and perplexity is 62.86461274251106
At time: 328.5315148830414 and batch: 850, loss is 4.170043721199035 and perplexity is 64.71828160641938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.613832791646321 and perplexity of 100.8700234603047
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 332.73619651794434 and batch: 50, loss is 4.172723340988159 and perplexity is 64.8919345525579
At time: 334.34546971321106 and batch: 100, loss is 4.132171168327331 and perplexity is 62.3130683303079
At time: 335.9550836086273 and batch: 150, loss is 4.132130012512207 and perplexity is 62.31050383796014
At time: 337.5695104598999 and batch: 200, loss is 4.153130259513855 and perplexity is 63.632876285393095
At time: 339.1822874546051 and batch: 250, loss is 4.14099308013916 and perplexity is 62.86522066244756
At time: 340.7911078929901 and batch: 300, loss is 4.114274878501892 and perplexity is 61.20781506473523
At time: 342.40098237991333 and batch: 350, loss is 4.071184391975403 and perplexity is 58.62635807610525
At time: 344.01466512680054 and batch: 400, loss is 4.085701684951783 and perplexity is 59.48366190136091
At time: 345.6256413459778 and batch: 450, loss is 4.111903471946716 and perplexity is 61.06283841808836
At time: 347.2361583709717 and batch: 500, loss is 4.0805666589736935 and perplexity is 59.17899465879585
At time: 348.85078167915344 and batch: 550, loss is 4.092779664993286 and perplexity is 59.90617959506341
At time: 350.45979714393616 and batch: 600, loss is 4.105591645240784 and perplexity is 60.67863415471638
At time: 352.068510055542 and batch: 650, loss is 4.069553442001343 and perplexity is 58.53081934954134
At time: 353.6790533065796 and batch: 700, loss is 4.0413792610168455 and perplexity is 56.90477524179755
At time: 355.29426169395447 and batch: 750, loss is 4.0072370052337645 and perplexity is 54.99471035445428
At time: 356.90551137924194 and batch: 800, loss is 3.9566282415390015 and perplexity is 52.28075036887914
At time: 358.5163080692291 and batch: 850, loss is 3.9814303064346315 and perplexity is 53.59363475396313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562687238057454 and perplexity of 95.84068082009794
Finished 12 epochs...
Completing Train Step...
At time: 362.74149227142334 and batch: 50, loss is 4.110304594039917 and perplexity is 60.96528440407643
At time: 364.3583254814148 and batch: 100, loss is 4.061780843734741 and perplexity is 58.07764625220833
At time: 365.9739508628845 and batch: 150, loss is 4.0601848602294925 and perplexity is 57.98502921405795
At time: 367.5906414985657 and batch: 200, loss is 4.086876401901245 and perplexity is 59.55357942582276
At time: 369.235111951828 and batch: 250, loss is 4.078801288604736 and perplexity is 59.074613977550214
At time: 370.8504514694214 and batch: 300, loss is 4.05612606048584 and perplexity is 57.75015656530961
At time: 372.4657714366913 and batch: 350, loss is 4.017011499404907 and perplexity is 55.534891528356646
At time: 374.0860376358032 and batch: 400, loss is 4.0353846979141235 and perplexity is 56.56467636629733
At time: 375.702933549881 and batch: 450, loss is 4.069148540496826 and perplexity is 58.50712493000288
At time: 377.3175890445709 and batch: 500, loss is 4.04114405632019 and perplexity is 56.8913925452968
At time: 378.93301582336426 and batch: 550, loss is 4.0616725730895995 and perplexity is 58.071358488376546
At time: 380.5551357269287 and batch: 600, loss is 4.078380165100097 and perplexity is 59.04974150663545
At time: 382.1705904006958 and batch: 650, loss is 4.047071604728699 and perplexity is 57.22962046950745
At time: 383.7856638431549 and batch: 700, loss is 4.025305876731872 and perplexity is 55.99743447458544
At time: 385.39894676208496 and batch: 750, loss is 3.9990232038497924 and perplexity is 54.54484480879072
At time: 387.0147409439087 and batch: 800, loss is 3.954357933998108 and perplexity is 52.162191620407356
At time: 388.62978649139404 and batch: 850, loss is 3.985764570236206 and perplexity is 53.82642783394755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.560506502787272 and perplexity of 95.63190539182312
Finished 13 epochs...
Completing Train Step...
At time: 392.8442416191101 and batch: 50, loss is 4.080684690475464 and perplexity is 59.18598005664909
At time: 394.4562888145447 and batch: 100, loss is 4.031677756309509 and perplexity is 56.35538257372324
At time: 396.0703568458557 and batch: 150, loss is 4.029614849090576 and perplexity is 56.239246478366496
At time: 397.6922998428345 and batch: 200, loss is 4.05787679195404 and perplexity is 57.85135013724033
At time: 399.30525183677673 and batch: 250, loss is 4.050023937225342 and perplexity is 57.3988309977538
At time: 400.91705870628357 and batch: 300, loss is 4.029048523902893 and perplexity is 56.20740579349141
At time: 402.53051018714905 and batch: 350, loss is 3.990788125991821 and perplexity is 54.097508218926734
At time: 404.14790630340576 and batch: 400, loss is 4.010748081207275 and perplexity is 55.18814033561823
At time: 405.76105523109436 and batch: 450, loss is 4.047151446342468 and perplexity is 57.23418995717635
At time: 407.37233567237854 and batch: 500, loss is 4.020614848136902 and perplexity is 55.73536407866854
At time: 408.98423433303833 and batch: 550, loss is 4.043921971321106 and perplexity is 57.04965171155604
At time: 410.65255641937256 and batch: 600, loss is 4.062408938407898 and perplexity is 58.11413597074811
At time: 412.26460242271423 and batch: 650, loss is 4.0326355361938475 and perplexity is 56.409384482369255
At time: 413.875773191452 and batch: 700, loss is 4.013354201316833 and perplexity is 55.33215483598819
At time: 415.4894759654999 and batch: 750, loss is 3.990217981338501 and perplexity is 54.066673604788235
At time: 417.10621786117554 and batch: 800, loss is 3.9476269483566284 and perplexity is 51.812267645350005
At time: 418.71817898750305 and batch: 850, loss is 3.9813548851013185 and perplexity is 53.589592802999476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5612789789835615 and perplexity of 95.70580730240258
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 422.93502593040466 and batch: 50, loss is 4.0781104326248165 and perplexity is 59.033816021600984
At time: 424.58534693717957 and batch: 100, loss is 4.045824017524719 and perplexity is 57.15826604700841
At time: 426.19631361961365 and batch: 150, loss is 4.0444712066650395 and perplexity is 57.080994002994316
At time: 427.8115084171295 and batch: 200, loss is 4.077380905151367 and perplexity is 58.99076493633474
At time: 429.4210605621338 and batch: 250, loss is 4.064792437553406 and perplexity is 58.25281617059094
At time: 431.0335907936096 and batch: 300, loss is 4.0371041584014895 and perplexity is 56.66202075822403
At time: 432.64881205558777 and batch: 350, loss is 3.995195999145508 and perplexity is 54.33648948570397
At time: 434.2626760005951 and batch: 400, loss is 4.007503576278687 and perplexity is 55.00937230599771
At time: 435.8739035129547 and batch: 450, loss is 4.040797181129456 and perplexity is 56.87166175491517
At time: 437.4857943058014 and batch: 500, loss is 4.010544948577881 and perplexity is 55.17693096209376
At time: 439.0982177257538 and batch: 550, loss is 4.027290720939636 and perplexity is 56.1086910349961
At time: 440.70987486839294 and batch: 600, loss is 4.038496203422547 and perplexity is 56.74095176711655
At time: 442.31819915771484 and batch: 650, loss is 4.001846199035644 and perplexity is 54.69904218992752
At time: 443.9289391040802 and batch: 700, loss is 3.975936393737793 and perplexity is 53.30000333476759
At time: 445.5409026145935 and batch: 750, loss is 3.946875238418579 and perplexity is 51.77333448390363
At time: 447.15498995780945 and batch: 800, loss is 3.898804187774658 and perplexity is 49.34340836078566
At time: 448.76540517807007 and batch: 850, loss is 3.9318450403213503 and perplexity is 51.00098979705301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.541153271993001 and perplexity of 93.79891342529457
Finished 15 epochs...
Completing Train Step...
At time: 452.92426466941833 and batch: 50, loss is 4.062388029098511 and perplexity is 58.112920857002976
At time: 454.55606508255005 and batch: 100, loss is 4.02329912185669 and perplexity is 55.885174027229205
At time: 456.1691288948059 and batch: 150, loss is 4.0189706897735595 and perplexity is 55.64380160591372
At time: 457.7813649177551 and batch: 200, loss is 4.050165934562683 and perplexity is 57.40698205762266
At time: 459.3923499584198 and batch: 250, loss is 4.038862843513488 and perplexity is 56.761759088998666
At time: 461.0031554698944 and batch: 300, loss is 4.013571081161499 and perplexity is 55.344156566553714
At time: 462.6178619861603 and batch: 350, loss is 3.974079895019531 and perplexity is 53.20114374164122
At time: 464.23025465011597 and batch: 400, loss is 3.9880226755142214 and perplexity is 53.94811090962826
At time: 465.83952832221985 and batch: 450, loss is 4.024323663711548 and perplexity is 55.942460068048355
At time: 467.4473090171814 and batch: 500, loss is 3.996374011039734 and perplexity is 54.4005362331172
At time: 469.0560095310211 and batch: 550, loss is 4.01640221118927 and perplexity is 55.501065079466535
At time: 470.66939401626587 and batch: 600, loss is 4.031324005126953 and perplexity is 56.33545031623205
At time: 472.2808780670166 and batch: 650, loss is 3.9979462146759035 and perplexity is 54.48613222352464
At time: 473.8899738788605 and batch: 700, loss is 3.9753097820281984 and perplexity is 53.26661539028754
At time: 475.5005443096161 and batch: 750, loss is 3.9499475240707396 and perplexity is 51.932641549701856
At time: 477.11581468582153 and batch: 800, loss is 3.9044082069396975 and perplexity is 49.620706031928165
At time: 478.72686791419983 and batch: 850, loss is 3.939416379928589 and perplexity is 51.388601127967014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.540445963541667 and perplexity of 93.73259211867821
Finished 16 epochs...
Completing Train Step...
At time: 482.91695642471313 and batch: 50, loss is 4.053489599227905 and perplexity is 57.59810104720401
At time: 484.559250831604 and batch: 100, loss is 4.013024997711182 and perplexity is 55.313942289090264
At time: 486.1754548549652 and batch: 150, loss is 4.007735891342163 and perplexity is 55.02215329636758
At time: 487.7959785461426 and batch: 200, loss is 4.038969111442566 and perplexity is 56.76779136410088
At time: 489.41152811050415 and batch: 250, loss is 4.0280460309982296 and perplexity is 56.15108650255538
At time: 491.0540864467621 and batch: 300, loss is 4.003589239120483 and perplexity is 54.794467954373005
At time: 492.6766378879547 and batch: 350, loss is 3.9647451496124266 and perplexity is 52.70683532047197
At time: 494.291170835495 and batch: 400, loss is 3.9793404293060304 and perplexity is 53.481747598379
At time: 495.90600419044495 and batch: 450, loss is 4.016969542503357 and perplexity is 55.532561505260844
At time: 497.5204391479492 and batch: 500, loss is 3.989909052848816 and perplexity is 54.049973448679026
At time: 499.14102244377136 and batch: 550, loss is 4.01124641418457 and perplexity is 55.21564925963577
At time: 500.7580828666687 and batch: 600, loss is 4.0273622798919675 and perplexity is 56.11270625780412
At time: 502.37189841270447 and batch: 650, loss is 3.995278253555298 and perplexity is 54.34095908539631
At time: 503.9864525794983 and batch: 700, loss is 3.973957347869873 and perplexity is 53.19462449258298
At time: 505.60068583488464 and batch: 750, loss is 3.9501276922225954 and perplexity is 51.94199900068287
At time: 507.22218561172485 and batch: 800, loss is 3.905627236366272 and perplexity is 49.68123201673146
At time: 508.84186363220215 and batch: 850, loss is 3.941501455307007 and perplexity is 51.49586201957228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.540500322977702 and perplexity of 93.73768750801388
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 513.0467264652252 and batch: 50, loss is 4.052807326316834 and perplexity is 57.558816825934215
At time: 514.6554942131042 and batch: 100, loss is 4.021339974403381 and perplexity is 55.77579391172918
At time: 516.2691807746887 and batch: 150, loss is 4.018086099624634 and perplexity is 55.594601411376345
At time: 517.8792562484741 and batch: 200, loss is 4.051321387290955 and perplexity is 57.47335144762537
At time: 519.4883270263672 and batch: 250, loss is 4.039847745895385 and perplexity is 56.81769142014783
At time: 521.0965042114258 and batch: 300, loss is 4.015127429962158 and perplexity is 55.43035844094148
At time: 522.7066466808319 and batch: 350, loss is 3.975363941192627 and perplexity is 53.269500343791655
At time: 524.3145933151245 and batch: 400, loss is 3.986819748878479 and perplexity is 53.883254306755425
At time: 525.9212815761566 and batch: 450, loss is 4.022760796546936 and perplexity is 55.855097719755435
At time: 527.524879693985 and batch: 500, loss is 3.9913239669799805 and perplexity is 54.12650364896388
At time: 529.1305894851685 and batch: 550, loss is 4.007018065452575 and perplexity is 54.98267114258201
At time: 530.7430453300476 and batch: 600, loss is 4.018841180801392 and perplexity is 55.63659570098498
At time: 532.3845338821411 and batch: 650, loss is 3.982605528831482 and perplexity is 53.656656218719554
At time: 533.9937970638275 and batch: 700, loss is 3.9589537715911867 and perplexity is 52.40247230416509
At time: 535.6011703014374 and batch: 750, loss is 3.933150234222412 and perplexity is 51.06759943767662
At time: 537.2127313613892 and batch: 800, loss is 3.8838559579849243 and perplexity is 48.611297268858685
At time: 538.8274054527283 and batch: 850, loss is 3.9232712173461914 and perplexity is 50.5655855456887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.536442756652832 and perplexity of 93.35811122256057
Finished 18 epochs...
Completing Train Step...
At time: 543.006795167923 and batch: 50, loss is 4.048335094451904 and perplexity is 57.30197520694155
At time: 544.6177914142609 and batch: 100, loss is 4.012911295890808 and perplexity is 55.30765335069857
At time: 546.2324655056 and batch: 150, loss is 4.007996401786804 and perplexity is 55.03648900920832
At time: 547.8451685905457 and batch: 200, loss is 4.040074939727783 and perplexity is 56.83060151570108
At time: 549.4546737670898 and batch: 250, loss is 4.028423280715942 and perplexity is 56.17227348022712
At time: 551.0633380413055 and batch: 300, loss is 4.004556655883789 and perplexity is 54.84750269041859
At time: 552.6748945713043 and batch: 350, loss is 3.9659052658081055 and perplexity is 52.76801685573333
At time: 554.289834022522 and batch: 400, loss is 3.9783208751678467 and perplexity is 53.427247848741416
At time: 555.9061710834503 and batch: 450, loss is 4.0166093492507935 and perplexity is 55.51256265325105
At time: 557.5154454708099 and batch: 500, loss is 3.9861873245239257 and perplexity is 53.84918799774643
At time: 559.1275501251221 and batch: 550, loss is 4.003454070091248 and perplexity is 54.78706193987532
At time: 560.7424988746643 and batch: 600, loss is 4.017275719642639 and perplexity is 55.54956690927966
At time: 562.3551788330078 and batch: 650, loss is 3.9830408573150633 and perplexity is 53.68001957450344
At time: 563.9646213054657 and batch: 700, loss is 3.961052641868591 and perplexity is 52.51257379970629
At time: 565.5751531124115 and batch: 750, loss is 3.9367150020599366 and perplexity is 51.24996843213245
At time: 567.1962161064148 and batch: 800, loss is 3.888542447090149 and perplexity is 48.83964824814568
At time: 568.806485414505 and batch: 850, loss is 3.9282430934906007 and perplexity is 50.81761739057402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.53608767191569 and perplexity of 93.32496706701718
Finished 19 epochs...
Completing Train Step...
At time: 573.0182976722717 and batch: 50, loss is 4.045942478179931 and perplexity is 57.165037453719854
At time: 574.6325533390045 and batch: 100, loss is 4.0092177581787105 and perplexity is 55.10374924284486
At time: 576.2499716281891 and batch: 150, loss is 4.003295660018921 and perplexity is 54.77838380480099
At time: 577.8681418895721 and batch: 200, loss is 4.034999008178711 and perplexity is 56.54286415787875
At time: 579.4865002632141 and batch: 250, loss is 4.023384051322937 and perplexity is 55.889920526786455
At time: 581.103013753891 and batch: 300, loss is 3.9998475551605224 and perplexity is 54.58982746130934
At time: 582.7204647064209 and batch: 350, loss is 3.9618101358413695 and perplexity is 52.55236682744121
At time: 584.3388450145721 and batch: 400, loss is 3.974634509086609 and perplexity is 53.23065802810765
At time: 585.9548962116241 and batch: 450, loss is 4.013777008056641 and perplexity is 55.35555459041928
At time: 587.5691046714783 and batch: 500, loss is 3.983846116065979 and perplexity is 53.72326328886808
At time: 589.1831862926483 and batch: 550, loss is 4.001940450668335 and perplexity is 54.70419790692408
At time: 590.8008010387421 and batch: 600, loss is 4.01665153503418 and perplexity is 55.51490454359127
At time: 592.4204778671265 and batch: 650, loss is 3.9832934236526487 and perplexity is 53.69357905271068
At time: 594.0364317893982 and batch: 700, loss is 3.9621126556396487 and perplexity is 52.56826736384521
At time: 595.6496460437775 and batch: 750, loss is 3.9385699892044066 and perplexity is 51.34512469426416
At time: 597.2667667865753 and batch: 800, loss is 3.890766181945801 and perplexity is 48.94837552180228
At time: 598.8810465335846 and batch: 850, loss is 3.9305402565002443 and perplexity is 50.93448792542325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.53604793548584 and perplexity of 93.32125873968836
Finished 20 epochs...
Completing Train Step...
At time: 603.0138258934021 and batch: 50, loss is 4.043646364212036 and perplexity is 57.03393058850052
At time: 604.6466784477234 and batch: 100, loss is 4.0063338947296145 and perplexity is 54.94506647418952
At time: 606.2589092254639 and batch: 150, loss is 3.9999111700057983 and perplexity is 54.59330029519766
At time: 607.8752343654633 and batch: 200, loss is 4.031561169624329 and perplexity is 56.34881266946601
At time: 609.4901127815247 and batch: 250, loss is 4.020038604736328 and perplexity is 55.70325619480617
At time: 611.105090379715 and batch: 300, loss is 3.9967404270172118 and perplexity is 54.42047311114864
At time: 612.7187731266022 and batch: 350, loss is 3.959095435142517 and perplexity is 52.409896350336155
At time: 614.3581955432892 and batch: 400, loss is 3.9721754884719847 and perplexity is 53.0999235479543
At time: 615.9708731174469 and batch: 450, loss is 4.011828303337097 and perplexity is 55.247787996674084
At time: 617.5880219936371 and batch: 500, loss is 3.982203049659729 and perplexity is 53.63506487748963
At time: 619.2005977630615 and batch: 550, loss is 4.000807180404663 and perplexity is 54.64223838121337
At time: 620.8117513656616 and batch: 600, loss is 4.016046419143676 and perplexity is 55.481321754456566
At time: 622.4223754405975 and batch: 650, loss is 3.9832138204574585 and perplexity is 53.689305042371586
At time: 624.0343298912048 and batch: 700, loss is 3.9625454807281493 and perplexity is 52.59102515353518
At time: 625.6473388671875 and batch: 750, loss is 3.9395328330993653 and perplexity is 51.39458584197282
At time: 627.2664709091187 and batch: 800, loss is 3.8919290256500245 and perplexity is 49.0053279390721
At time: 628.8793981075287 and batch: 850, loss is 3.931778326034546 and perplexity is 50.99758741588737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.536074638366699 and perplexity of 93.3237507194135
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 633.0551829338074 and batch: 50, loss is 4.043480172157287 and perplexity is 57.024452789974816
At time: 634.692307472229 and batch: 100, loss is 4.00849054813385 and perplexity is 55.06369180973519
At time: 636.305988073349 and batch: 150, loss is 4.002360939979553 and perplexity is 54.72720527425974
At time: 637.9194192886353 and batch: 200, loss is 4.034567337036133 and perplexity is 56.51846150244311
At time: 639.5294647216797 and batch: 250, loss is 4.021768741607666 and perplexity is 55.799713870631955
At time: 641.1399874687195 and batch: 300, loss is 3.9987863969802855 and perplexity is 54.53192974409218
At time: 642.7548666000366 and batch: 350, loss is 3.9606551361083984 and perplexity is 52.49170389736636
At time: 644.3663558959961 and batch: 400, loss is 3.972764587402344 and perplexity is 53.13121387176178
At time: 645.9780418872833 and batch: 450, loss is 4.012953147888184 and perplexity is 55.30996813490032
At time: 647.5876455307007 and batch: 500, loss is 3.983469672203064 and perplexity is 53.70304330219095
At time: 649.2011411190033 and batch: 550, loss is 3.9999802017211916 and perplexity is 54.59706909444786
At time: 650.8100628852844 and batch: 600, loss is 4.012602906227112 and perplexity is 55.29059967180627
At time: 652.4183104038239 and batch: 650, loss is 3.977352204322815 and perplexity is 53.375519489341684
At time: 654.0549702644348 and batch: 700, loss is 3.955949034690857 and perplexity is 52.24525298160136
At time: 655.666913986206 and batch: 750, loss is 3.9330661916732788 and perplexity is 51.06330776678576
At time: 657.2831408977509 and batch: 800, loss is 3.8838075733184816 and perplexity is 48.608945284355435
At time: 658.8919584751129 and batch: 850, loss is 3.9236882448196413 and perplexity is 50.58667718166249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.53573735555013 and perplexity of 93.29227952956217
Finished 22 epochs...
Completing Train Step...
At time: 663.0486390590668 and batch: 50, loss is 4.0424515962600704 and perplexity is 56.965828966977256
At time: 664.6866748332977 and batch: 100, loss is 4.006562829017639 and perplexity is 54.95764672383355
At time: 666.3013277053833 and batch: 150, loss is 4.000452399253845 and perplexity is 54.62285578349071
At time: 667.9159119129181 and batch: 200, loss is 4.032686805725097 and perplexity is 56.41227663920888
At time: 669.5265057086945 and batch: 250, loss is 4.019875597953797 and perplexity is 55.69417692624861
At time: 671.1405787467957 and batch: 300, loss is 3.9970566034317017 and perplexity is 54.437682301638475
At time: 672.7521243095398 and batch: 350, loss is 3.9589879608154295 and perplexity is 52.40426393466863
At time: 674.3682079315186 and batch: 400, loss is 3.971074023246765 and perplexity is 53.041468027975625
At time: 675.97913646698 and batch: 450, loss is 4.0116791343688964 and perplexity is 55.23954735578248
At time: 677.5867385864258 and batch: 500, loss is 3.9823742628097536 and perplexity is 53.64424869207156
At time: 679.1970036029816 and batch: 550, loss is 3.9993485736846925 and perplexity is 54.56259494346341
At time: 680.8106603622437 and batch: 600, loss is 4.0125533294677735 and perplexity is 55.28785861099967
At time: 682.4209733009338 and batch: 650, loss is 3.977591633796692 and perplexity is 53.38830069192817
At time: 684.0287647247314 and batch: 700, loss is 3.9565181875228883 and perplexity is 52.274996978933274
At time: 685.636602640152 and batch: 750, loss is 3.933787798881531 and perplexity is 51.100168715711185
At time: 687.248297214508 and batch: 800, loss is 3.884777979850769 and perplexity is 48.656138617042686
At time: 688.8486261367798 and batch: 850, loss is 3.9247230768203734 and perplexity is 50.63905298942774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535709381103516 and perplexity of 93.28966976617245
Finished 23 epochs...
Completing Train Step...
At time: 693.0255842208862 and batch: 50, loss is 4.041678986549377 and perplexity is 56.92183361214737
At time: 694.6355817317963 and batch: 100, loss is 4.005405821800232 and perplexity is 54.894097100697984
At time: 696.2745792865753 and batch: 150, loss is 3.999143624305725 and perplexity is 54.55141351936699
At time: 697.89035820961 and batch: 200, loss is 4.031342830657959 and perplexity is 56.336510870981435
At time: 699.5077743530273 and batch: 250, loss is 4.018512959480286 and perplexity is 55.618337580558276
At time: 701.1206040382385 and batch: 300, loss is 3.9957920026779177 and perplexity is 54.36888387800339
At time: 702.7321655750275 and batch: 350, loss is 3.957820243835449 and perplexity is 52.34310630020221
At time: 704.3432619571686 and batch: 400, loss is 3.9699934577941893 and perplexity is 52.984184205104505
At time: 705.9554824829102 and batch: 450, loss is 4.010889210700989 and perplexity is 55.19592955955448
At time: 707.5733616352081 and batch: 500, loss is 3.981728367805481 and perplexity is 53.60961132708807
At time: 709.1880054473877 and batch: 550, loss is 3.998990058898926 and perplexity is 54.543036952550324
At time: 710.797993183136 and batch: 600, loss is 4.01255316734314 and perplexity is 55.28784964747657
At time: 712.408269405365 and batch: 650, loss is 3.9778187084198 and perplexity is 53.40042519671775
At time: 714.0179800987244 and batch: 700, loss is 3.95697247505188 and perplexity is 52.29875025313849
At time: 715.6282775402069 and batch: 750, loss is 3.93438108921051 and perplexity is 51.130494946859706
At time: 717.2459292411804 and batch: 800, loss is 3.8855269861221315 and perplexity is 48.69259602171508
At time: 718.864654302597 and batch: 850, loss is 3.9254679250717164 and perplexity is 50.67678545023081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535714785257976 and perplexity of 93.29017391931964
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 723.0450637340546 and batch: 50, loss is 4.0414797306060795 and perplexity is 56.9104927284035
At time: 724.6558499336243 and batch: 100, loss is 4.005598254203797 and perplexity is 54.904661520180305
At time: 726.2687857151031 and batch: 150, loss is 3.999445676803589 and perplexity is 54.56789339885161
At time: 727.8770852088928 and batch: 200, loss is 4.031597032546997 and perplexity is 56.35083353881413
At time: 729.4825057983398 and batch: 250, loss is 4.018490371704101 and perplexity is 55.61708130018559
At time: 731.0902631282806 and batch: 300, loss is 3.995860676765442 and perplexity is 54.372617739701724
At time: 732.6984393596649 and batch: 350, loss is 3.9577686977386475 and perplexity is 52.34040828691459
At time: 734.3107676506042 and batch: 400, loss is 3.9693881940841673 and perplexity is 52.95212450446545
At time: 735.9663639068604 and batch: 450, loss is 4.01055874824524 and perplexity is 55.17769239064065
At time: 737.5744845867157 and batch: 500, loss is 3.9818995141983033 and perplexity is 53.618787203874014
At time: 739.1835894584656 and batch: 550, loss is 3.998672685623169 and perplexity is 54.5257291968978
At time: 740.7927997112274 and batch: 600, loss is 4.011722664833069 and perplexity is 55.24195201125708
At time: 742.4029974937439 and batch: 650, loss is 3.9762560081481935 and perplexity is 53.317041506585355
At time: 744.0151915550232 and batch: 700, loss is 3.9552388334274293 and perplexity is 52.20816150968988
At time: 745.6204118728638 and batch: 750, loss is 3.9326118850708007 and perplexity is 51.04011463771769
At time: 747.2287640571594 and batch: 800, loss is 3.883606948852539 and perplexity is 48.599194118861625
At time: 748.834317445755 and batch: 850, loss is 3.923467617034912 and perplexity is 50.57551758624276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535597483317058 and perplexity of 93.27923144264976
Finished 25 epochs...
Completing Train Step...
At time: 753.0356130599976 and batch: 50, loss is 4.041245651245117 and perplexity is 56.897172715664446
At time: 754.6455929279327 and batch: 100, loss is 4.005230231285095 and perplexity is 54.88445906410877
At time: 756.2557139396667 and batch: 150, loss is 3.9990410709381106 and perplexity is 54.545819375056524
At time: 757.873131275177 and batch: 200, loss is 4.0312518119812015 and perplexity is 56.331383429658835
At time: 759.4835872650146 and batch: 250, loss is 4.018141436576843 and perplexity is 55.59767793229958
At time: 761.0934774875641 and batch: 300, loss is 3.9955389881134034 and perplexity is 54.35512949862839
At time: 762.7034900188446 and batch: 350, loss is 3.957484230995178 and perplexity is 52.32552129894423
At time: 764.3123669624329 and batch: 400, loss is 3.9691120195388794 and perplexity is 52.93750249476478
At time: 765.9211285114288 and batch: 450, loss is 4.01034273147583 and perplexity is 55.16577437107932
At time: 767.5357947349548 and batch: 500, loss is 3.9817084741592406 and perplexity is 53.60854484705336
At time: 769.148017168045 and batch: 550, loss is 3.998585638999939 and perplexity is 54.52098312285992
At time: 770.7614741325378 and batch: 600, loss is 4.011732773780823 and perplexity is 55.242510452086385
At time: 772.3773305416107 and batch: 650, loss is 3.9763199520111083 and perplexity is 53.32045091318263
At time: 773.996550321579 and batch: 700, loss is 3.9553744697570803 and perplexity is 52.215243313358954
At time: 775.6088917255402 and batch: 750, loss is 3.9327874994277954 and perplexity is 51.049078801725635
At time: 777.2516498565674 and batch: 800, loss is 3.883821716308594 and perplexity is 48.60963276504947
At time: 778.8656516075134 and batch: 850, loss is 3.923681263923645 and perplexity is 50.58632404256289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535553614298503 and perplexity of 93.27513946407103
Finished 26 epochs...
Completing Train Step...
At time: 783.0471374988556 and batch: 50, loss is 4.041041631698608 and perplexity is 56.885565764352066
At time: 784.6883924007416 and batch: 100, loss is 4.004951086044311 and perplexity is 54.86914046672428
At time: 786.3020865917206 and batch: 150, loss is 3.99871636390686 and perplexity is 54.52811083917877
At time: 787.9174554347992 and batch: 200, loss is 4.030922985076904 and perplexity is 56.312863200372576
At time: 789.5340926647186 and batch: 250, loss is 4.017829103469849 and perplexity is 55.58031564835849
At time: 791.1521532535553 and batch: 300, loss is 3.9952437257766724 and perplexity is 54.33908284518209
At time: 792.7677845954895 and batch: 350, loss is 3.957224197387695 and perplexity is 52.31191667378398
At time: 794.3817820549011 and batch: 400, loss is 3.9688688468933107 and perplexity is 52.92463110728142
At time: 796.0006213188171 and batch: 450, loss is 4.010159502029419 and perplexity is 55.15566730276502
At time: 797.6136944293976 and batch: 500, loss is 3.9815573358535765 and perplexity is 53.60044315466951
At time: 799.228675365448 and batch: 550, loss is 3.998510546684265 and perplexity is 54.51688916969853
At time: 800.8467857837677 and batch: 600, loss is 4.011741065979004 and perplexity is 55.24296853583035
At time: 802.4605734348297 and batch: 650, loss is 3.97638108253479 and perplexity is 53.32371051989961
At time: 804.075701713562 and batch: 700, loss is 3.9554977560043336 and perplexity is 52.221681131595545
At time: 805.6954157352448 and batch: 750, loss is 3.932946925163269 and perplexity is 51.057217987439444
At time: 807.3120470046997 and batch: 800, loss is 3.884013433456421 and perplexity is 48.61895295859201
At time: 808.9252293109894 and batch: 850, loss is 3.9238714361190796 and perplexity is 50.595945069661965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535535494486491 and perplexity of 93.27344935139088
Finished 27 epochs...
Completing Train Step...
At time: 813.0938229560852 and batch: 50, loss is 4.040849227905273 and perplexity is 56.8746218185751
At time: 814.7468318939209 and batch: 100, loss is 4.004694900512695 and perplexity is 54.85508558720976
At time: 816.3607122898102 and batch: 150, loss is 3.998417272567749 and perplexity is 54.511804392168784
At time: 818.0025181770325 and batch: 200, loss is 4.030615072250367 and perplexity is 56.29552641674014
At time: 819.6136422157288 and batch: 250, loss is 4.017535085678101 and perplexity is 55.563976448812305
At time: 821.2260267734528 and batch: 300, loss is 3.9949667167663576 and perplexity is 54.324032514256615
At time: 822.8393788337708 and batch: 350, loss is 3.9569805240631104 and perplexity is 52.29917120806075
At time: 824.4452173709869 and batch: 400, loss is 3.9686448764801026 and perplexity is 52.91277888310676
At time: 826.0546636581421 and batch: 450, loss is 4.00999345779419 and perplexity is 55.14650978246695
At time: 827.670252084732 and batch: 500, loss is 3.98142391204834 and perplexity is 53.59329205665541
At time: 829.2819261550903 and batch: 550, loss is 3.9984436798095704 and perplexity is 54.51324391757635
At time: 830.89284324646 and batch: 600, loss is 4.01175021648407 and perplexity is 55.24347403920658
At time: 832.5080542564392 and batch: 650, loss is 3.976437878608704 and perplexity is 53.32673918331094
At time: 834.1236350536346 and batch: 700, loss is 3.9556115674972534 and perplexity is 52.22762489731592
At time: 835.7340557575226 and batch: 750, loss is 3.9330944442749023 and perplexity is 51.06475045845748
At time: 837.3555116653442 and batch: 800, loss is 3.8841912651062014 and perplexity is 48.62759971601802
At time: 838.9723105430603 and batch: 850, loss is 3.9240447092056274 and perplexity is 50.60471274481012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535529454549153 and perplexity of 93.27288598730286
Finished 28 epochs...
Completing Train Step...
At time: 843.1587660312653 and batch: 50, loss is 4.040664987564087 and perplexity is 56.86414418407754
At time: 844.7691583633423 and batch: 100, loss is 4.00445294380188 and perplexity is 54.84181463669209
At time: 846.377623796463 and batch: 150, loss is 3.9981352949142455 and perplexity is 54.49643544842921
At time: 847.9882764816284 and batch: 200, loss is 4.0303244972229 and perplexity is 56.27917071899911
At time: 849.5997371673584 and batch: 250, loss is 4.017256364822388 and perplexity is 55.54849176780171
At time: 851.2127642631531 and batch: 300, loss is 3.9947046184539796 and perplexity is 54.309796142759026
At time: 852.8280465602875 and batch: 350, loss is 3.956750817298889 and perplexity is 52.28715911435349
At time: 854.4399461746216 and batch: 400, loss is 3.968435754776001 and perplexity is 52.901714829525176
At time: 856.0517165660858 and batch: 450, loss is 4.0098401641845705 and perplexity is 55.13805682283339
At time: 857.667290687561 and batch: 500, loss is 3.9813032817840575 and perplexity is 53.58682747359087
At time: 859.3243780136108 and batch: 550, loss is 3.998382773399353 and perplexity is 54.509923812688875
At time: 860.9328110218048 and batch: 600, loss is 4.011759123802185 and perplexity is 55.24396611259515
At time: 862.5465130805969 and batch: 650, loss is 3.9764901494979856 and perplexity is 53.3295266922427
At time: 864.1586785316467 and batch: 700, loss is 3.9557168674468994 and perplexity is 52.23312475314993
At time: 865.7707011699677 and batch: 750, loss is 3.9332322311401366 and perplexity is 51.07178699510715
At time: 867.3839223384857 and batch: 800, loss is 3.8843577003479 and perplexity is 48.63569373587638
At time: 869.0001246929169 and batch: 850, loss is 3.924204559326172 and perplexity is 50.61280256080431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535530090332031 and perplexity of 93.27294528862559
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 873.2295434474945 and batch: 50, loss is 4.040602660179138 and perplexity is 56.86060010112103
At time: 874.8436098098755 and batch: 100, loss is 4.004475722312927 and perplexity is 54.843063865800374
At time: 876.4538190364838 and batch: 150, loss is 3.9981947422027586 and perplexity is 54.49967521004682
At time: 878.0666301250458 and batch: 200, loss is 4.030360221862793 and perplexity is 56.28118130802007
At time: 879.6820933818817 and batch: 250, loss is 4.017259421348572 and perplexity is 55.54866155348076
At time: 881.2953760623932 and batch: 300, loss is 3.9947219133377074 and perplexity is 54.31073543249103
At time: 882.9088637828827 and batch: 350, loss is 3.9567752647399903 and perplexity is 52.28843741722185
At time: 884.5197870731354 and batch: 400, loss is 3.968273015022278 and perplexity is 52.893106317974805
At time: 886.137930393219 and batch: 450, loss is 4.009692182540894 and perplexity is 55.12989800624804
At time: 887.7505996227264 and batch: 500, loss is 3.9813404417037965 and perplexity is 53.58881879279726
At time: 889.3628041744232 and batch: 550, loss is 3.99826256275177 and perplexity is 54.50337153328243
At time: 890.9716801643372 and batch: 600, loss is 4.011504092216492 and perplexity is 55.22987895272919
At time: 892.5802476406097 and batch: 650, loss is 3.9760992622375486 and perplexity is 53.30868493330797
At time: 894.184324502945 and batch: 700, loss is 3.955264673233032 and perplexity is 52.209510575863895
At time: 895.7962870597839 and batch: 750, loss is 3.932796277999878 and perplexity is 51.04952694171066
At time: 897.4170482158661 and batch: 800, loss is 3.883847451210022 and perplexity is 48.61088374525399
At time: 899.0273034572601 and batch: 850, loss is 3.9237156772613524 and perplexity is 50.58806491677002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535531044006348 and perplexity of 93.27303424068035
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 903.267049074173 and batch: 50, loss is 4.040588369369507 and perplexity is 56.85978752291571
At time: 904.8805000782013 and batch: 100, loss is 4.004456186294556 and perplexity is 54.84199246116269
At time: 906.4932351112366 and batch: 150, loss is 3.9981808948516844 and perplexity is 54.49892053913587
At time: 908.110301733017 and batch: 200, loss is 4.030369362831116 and perplexity is 56.28169577486693
At time: 909.7282590866089 and batch: 250, loss is 4.017244706153869 and perplexity is 55.54784415012466
At time: 911.343076467514 and batch: 300, loss is 3.9947197532653806 and perplexity is 54.31061811750108
At time: 912.9587581157684 and batch: 350, loss is 3.9567754650115967 and perplexity is 52.288447889112255
At time: 914.5773742198944 and batch: 400, loss is 3.968227996826172 and perplexity is 52.89072521933871
At time: 916.1926507949829 and batch: 450, loss is 4.009646868705749 and perplexity is 55.127399915737705
At time: 917.8043127059937 and batch: 500, loss is 3.9813291883468627 and perplexity is 53.588215742084905
At time: 919.4182994365692 and batch: 550, loss is 3.998229842185974 and perplexity is 54.50158818130438
At time: 921.0364665985107 and batch: 600, loss is 4.011444072723389 and perplexity is 55.226564182866724
At time: 922.6519541740417 and batch: 650, loss is 3.976008996963501 and perplexity is 53.30387322742158
At time: 924.2633771896362 and batch: 700, loss is 3.9551603317260744 and perplexity is 52.20406324104939
At time: 925.8749713897705 and batch: 750, loss is 3.9326942920684815 and perplexity is 51.04432087363553
At time: 927.4933590888977 and batch: 800, loss is 3.883726396560669 and perplexity is 48.60499952793058
At time: 929.1132640838623 and batch: 850, loss is 3.9235999822616576 and perplexity is 50.58221246917091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535532315572103 and perplexity of 93.27315284355196
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 933.292397737503 and batch: 50, loss is 4.040585732460022 and perplexity is 56.85963758900033
At time: 934.9277517795563 and batch: 100, loss is 4.004452481269836 and perplexity is 54.841789270601346
At time: 936.5371825695038 and batch: 150, loss is 3.9981782102584837 and perplexity is 54.49877423190073
At time: 938.1507987976074 and batch: 200, loss is 4.03037202835083 and perplexity is 56.28184579503651
At time: 939.7589309215546 and batch: 250, loss is 4.017241706848145 and perplexity is 55.54767754540756
At time: 941.391649723053 and batch: 300, loss is 3.9947197151184084 and perplexity is 54.31061604571548
At time: 942.9961400032043 and batch: 350, loss is 3.9567756414413453 and perplexity is 52.288457114350784
At time: 944.6018607616425 and batch: 400, loss is 3.968217477798462 and perplexity is 52.89016886326068
At time: 946.2068009376526 and batch: 450, loss is 4.009636344909668 and perplexity is 55.12681976927521
At time: 947.8153448104858 and batch: 500, loss is 3.9813263511657713 and perplexity is 53.58806370282816
At time: 949.4276342391968 and batch: 550, loss is 3.9982222843170168 and perplexity is 54.501176266999536
At time: 951.03737616539 and batch: 600, loss is 4.011429891586304 and perplexity is 55.22578101294244
At time: 952.6446409225464 and batch: 650, loss is 3.975987205505371 and perplexity is 53.30271167095603
At time: 954.2525022029877 and batch: 700, loss is 3.955135216712952 and perplexity is 52.202752151780125
At time: 955.8619801998138 and batch: 750, loss is 3.9326696252822875 and perplexity is 51.043061789814956
At time: 957.4764883518219 and batch: 800, loss is 3.8836970949172973 and perplexity is 48.60357534243393
At time: 959.0876371860504 and batch: 850, loss is 3.9235719537734983 and perplexity is 50.58079474609605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535532633463542 and perplexity of 93.27318249429348
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 963.2550134658813 and batch: 50, loss is 4.040585660934449 and perplexity is 56.85963352208231
At time: 964.9118885993958 and batch: 100, loss is 4.004452056884766 and perplexity is 54.8417659965697
At time: 966.5237646102905 and batch: 150, loss is 3.998178277015686 and perplexity is 54.498777870086556
At time: 968.1367814540863 and batch: 200, loss is 4.03037323474884 and perplexity is 56.28191369338423
At time: 969.7463536262512 and batch: 250, loss is 4.017241535186767 and perplexity is 55.54766801001753
At time: 971.3552796840668 and batch: 300, loss is 3.9947203683853147 and perplexity is 54.31065152505519
At time: 972.9598679542542 and batch: 350, loss is 3.956776432991028 and perplexity is 52.2884985032788
At time: 974.5685529708862 and batch: 400, loss is 3.9682157135009763 and perplexity is 52.890075549351074
At time: 976.183367729187 and batch: 450, loss is 4.009634132385254 and perplexity is 55.12669779997551
At time: 977.7991049289703 and batch: 500, loss is 3.981326003074646 and perplexity is 53.58804504930201
At time: 979.4124994277954 and batch: 550, loss is 3.9982207202911377 and perplexity is 54.50109102581607
At time: 981.0582160949707 and batch: 600, loss is 4.011426792144776 and perplexity is 55.2256098441286
At time: 982.6683781147003 and batch: 650, loss is 3.9759824466705322 and perplexity is 53.302458012758294
At time: 984.2871086597443 and batch: 700, loss is 3.955129747390747 and perplexity is 52.202466638889405
At time: 985.8992431163788 and batch: 750, loss is 3.932664031982422 and perplexity is 51.04277629146275
At time: 987.5138854980469 and batch: 800, loss is 3.8836904907226564 and perplexity is 48.60325435602205
At time: 989.1269116401672 and batch: 850, loss is 3.9235657167434694 and perplexity is 50.58047927314415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5355329513549805 and perplexity of 93.27321214504435
Annealing...
Model not improving. Stopping early with 93.27288598730286loss at 32 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -93.27288598730286
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.9182670318718621, 'tune_wordvecs': True, 'lr': 3.1841405083311223, 'num_layers': 1, 'anneal': 2.566900712595255, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.122598171234131 and batch: 50, loss is 9.214089088439941 and perplexity is 10037.557516895966
At time: 3.7315902709960938 and batch: 100, loss is 7.771890497207641 and perplexity is 2372.9531118149043
At time: 5.335220098495483 and batch: 150, loss is 7.363614616394043 and perplexity is 1577.5284299275397
At time: 6.946839094161987 and batch: 200, loss is 7.243323860168457 and perplexity is 1398.7354538342086
At time: 8.553932666778564 and batch: 250, loss is 7.161683053970337 and perplexity is 1289.078697181399
At time: 10.162430047988892 and batch: 300, loss is 7.041156311035156 and perplexity is 1142.7081690572186
At time: 11.768044471740723 and batch: 350, loss is 6.972185764312744 and perplexity is 1066.5514346972732
At time: 13.37314486503601 and batch: 400, loss is 6.957573280334473 and perplexity is 1051.0797838679923
At time: 14.982146739959717 and batch: 450, loss is 6.92218939781189 and perplexity is 1014.538793746641
At time: 16.58845090866089 and batch: 500, loss is 6.882434644699097 and perplexity is 974.9972443626124
At time: 18.1923189163208 and batch: 550, loss is 6.810457944869995 and perplexity is 907.2861988874066
At time: 19.84502077102661 and batch: 600, loss is 6.808790435791016 and perplexity is 905.7745516065839
At time: 21.454140424728394 and batch: 650, loss is 6.804339218139648 and perplexity is 901.7517118441477
At time: 23.063263177871704 and batch: 700, loss is 6.760697317123413 and perplexity is 863.2439407428866
At time: 24.666727542877197 and batch: 750, loss is 6.727110357284546 and perplexity is 834.7317011957073
At time: 26.27069115638733 and batch: 800, loss is 6.744945602416992 and perplexity is 849.7529009161433
At time: 27.874717235565186 and batch: 850, loss is 6.740720767974853 and perplexity is 846.1704086401479
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.981595993041992 and perplexity of 396.0719921310855
Finished 1 epochs...
Completing Train Step...
At time: 32.02884125709534 and batch: 50, loss is 6.183551206588745 and perplexity is 484.71020970105167
At time: 33.62534523010254 and batch: 100, loss is 5.878899488449097 and perplexity is 357.4156851018486
At time: 35.216726541519165 and batch: 150, loss is 5.749185581207275 and perplexity is 313.93488167704936
At time: 36.81012988090515 and batch: 200, loss is 5.692544593811035 and perplexity is 296.6475084786714
At time: 38.42127227783203 and batch: 250, loss is 5.689266901016236 and perplexity is 295.6767808201275
At time: 40.03281855583191 and batch: 300, loss is 5.59344220161438 and perplexity is 268.6588076840616
At time: 41.6450731754303 and batch: 350, loss is 5.530243473052979 and perplexity is 252.2053087453617
At time: 43.30156993865967 and batch: 400, loss is 5.5189985752105715 and perplexity is 249.385171606818
At time: 44.91978716850281 and batch: 450, loss is 5.496089820861816 and perplexity is 243.73701114840046
At time: 46.53094530105591 and batch: 500, loss is 5.466834392547607 and perplexity is 236.70967555349702
At time: 48.14417815208435 and batch: 550, loss is 5.409998092651367 and perplexity is 223.6311611375498
At time: 49.757455348968506 and batch: 600, loss is 5.427750730514527 and perplexity is 227.63665298351498
At time: 51.3685667514801 and batch: 650, loss is 5.420897073745728 and perplexity is 226.0818436468514
At time: 52.98266363143921 and batch: 700, loss is 5.352864427566528 and perplexity is 211.21243492116386
At time: 54.598743200302124 and batch: 750, loss is 5.339113359451294 and perplexity is 208.3279164002169
At time: 56.21055293083191 and batch: 800, loss is 5.328861293792724 and perplexity is 206.20303574202046
At time: 57.82040095329285 and batch: 850, loss is 5.316252279281616 and perplexity is 203.6193418202983
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.994531949361165 and perplexity of 147.60384313998352
Finished 2 epochs...
Completing Train Step...
At time: 62.015904903411865 and batch: 50, loss is 5.278834791183471 and perplexity is 196.1411966968036
At time: 63.62960505485535 and batch: 100, loss is 5.188721837997437 and perplexity is 179.23930958309833
At time: 65.24459266662598 and batch: 150, loss is 5.151719465255737 and perplexity is 172.72823532169983
At time: 66.85802602767944 and batch: 200, loss is 5.164304513931274 and perplexity is 174.91576478092912
At time: 68.46935701370239 and batch: 250, loss is 5.188110208511352 and perplexity is 179.1297150553307
At time: 70.07966685295105 and batch: 300, loss is 5.14213454246521 and perplexity is 171.08055756884897
At time: 71.6950330734253 and batch: 350, loss is 5.101896324157715 and perplexity is 164.33324110672098
At time: 73.3066816329956 and batch: 400, loss is 5.112113313674927 and perplexity is 166.02083851070466
At time: 74.91925811767578 and batch: 450, loss is 5.1046444702148435 and perplexity is 164.7854739716892
At time: 76.57368469238281 and batch: 500, loss is 5.09146821975708 and perplexity is 162.6284611724141
At time: 78.18591022491455 and batch: 550, loss is 5.066267786026001 and perplexity is 158.58136191970976
At time: 79.79787707328796 and batch: 600, loss is 5.099610624313354 and perplexity is 163.9580535894698
At time: 81.40964388847351 and batch: 650, loss is 5.093788194656372 and perplexity is 163.00619311513574
At time: 83.02279472351074 and batch: 700, loss is 5.041741571426392 and perplexity is 154.73926998561055
At time: 84.63649678230286 and batch: 750, loss is 5.0310384559631345 and perplexity is 153.09190938393596
At time: 86.24716520309448 and batch: 800, loss is 5.017340593338012 and perplexity is 151.00917451368926
At time: 87.86046743392944 and batch: 850, loss is 5.016623373031616 and perplexity is 150.90090649792677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.829504013061523 and perplexity of 125.14887305231908
Finished 3 epochs...
Completing Train Step...
At time: 92.01276206970215 and batch: 50, loss is 5.0072283267974855 and perplexity is 149.48982447433605
At time: 93.66710305213928 and batch: 100, loss is 4.927988767623901 and perplexity is 138.10147867343147
At time: 95.27162981033325 and batch: 150, loss is 4.913443431854248 and perplexity is 136.10728456592955
At time: 96.87661981582642 and batch: 200, loss is 4.932904005050659 and perplexity is 138.7819512021257
At time: 98.48437929153442 and batch: 250, loss is 4.9525128364562985 and perplexity is 141.53015960496427
At time: 100.09418272972107 and batch: 300, loss is 4.921787395477295 and perplexity is 137.24771001488475
At time: 101.71072244644165 and batch: 350, loss is 4.884687652587891 and perplexity is 132.24915120231788
At time: 103.32001376152039 and batch: 400, loss is 4.893796062469482 and perplexity is 133.45923327370454
At time: 104.93146276473999 and batch: 450, loss is 4.896144046783447 and perplexity is 133.77296163048246
At time: 106.53995370864868 and batch: 500, loss is 4.889171934127807 and perplexity is 132.84352530411908
At time: 108.15332508087158 and batch: 550, loss is 4.876475133895874 and perplexity is 131.16750019498772
At time: 109.76351857185364 and batch: 600, loss is 4.916480216979981 and perplexity is 136.52124137401393
At time: 111.37254047393799 and batch: 650, loss is 4.9067025566101075 and perplexity is 135.19288772084764
At time: 112.98426008224487 and batch: 700, loss is 4.864581432342529 and perplexity is 129.6166739083094
At time: 114.59921431541443 and batch: 750, loss is 4.848409652709961 and perplexity is 127.53739971839201
At time: 116.21099162101746 and batch: 800, loss is 4.836687469482422 and perplexity is 126.05111124260085
At time: 117.86859273910522 and batch: 850, loss is 4.840450429916382 and perplexity is 126.52633014106658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.749080975850423 and perplexity of 115.47810857500073
Finished 4 epochs...
Completing Train Step...
At time: 122.08910655975342 and batch: 50, loss is 4.835353727340698 and perplexity is 125.88310362784873
At time: 123.70554184913635 and batch: 100, loss is 4.766781311035157 and perplexity is 117.54030676224598
At time: 125.32223272323608 and batch: 150, loss is 4.7573237705230715 and perplexity is 116.43390471715978
At time: 126.93856811523438 and batch: 200, loss is 4.782813282012939 and perplexity is 119.43989594176792
At time: 128.55399417877197 and batch: 250, loss is 4.795097541809082 and perplexity is 120.91617559036472
At time: 130.17396879196167 and batch: 300, loss is 4.7717280673980715 and perplexity is 118.12319052598863
At time: 131.7919886112213 and batch: 350, loss is 4.737138185501099 and perplexity is 114.10718038295663
At time: 133.40997624397278 and batch: 400, loss is 4.748973340988159 and perplexity is 115.46567977358792
At time: 135.0290400981903 and batch: 450, loss is 4.757146511077881 and perplexity is 116.41326753692985
At time: 136.64978408813477 and batch: 500, loss is 4.7506881523132325 and perplexity is 115.66385149391454
At time: 138.26958107948303 and batch: 550, loss is 4.74476128578186 and perplexity is 114.98035478130785
At time: 139.88590717315674 and batch: 600, loss is 4.785714159011841 and perplexity is 119.78687942353254
At time: 141.5039279460907 and batch: 650, loss is 4.776491651535034 and perplexity is 118.68722262318931
At time: 143.11814284324646 and batch: 700, loss is 4.741384830474853 and perplexity is 114.5927834290868
At time: 144.7326991558075 and batch: 750, loss is 4.719735279083252 and perplexity is 112.13856331500264
At time: 146.35402059555054 and batch: 800, loss is 4.704398021697998 and perplexity is 110.43178739457525
At time: 147.97081232070923 and batch: 850, loss is 4.712851390838623 and perplexity is 111.3692648973477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.704762776692708 and perplexity of 110.4720752877606
Finished 5 epochs...
Completing Train Step...
At time: 152.16839337348938 and batch: 50, loss is 4.710940284729004 and perplexity is 111.15662966374335
At time: 153.78301644325256 and batch: 100, loss is 4.644928226470947 and perplexity is 104.05589705843943
At time: 155.39695763587952 and batch: 150, loss is 4.640748310089111 and perplexity is 103.62185986124082
At time: 157.01085567474365 and batch: 200, loss is 4.668365364074707 and perplexity is 106.52347294191254
At time: 158.67342019081116 and batch: 250, loss is 4.67633360862732 and perplexity is 107.37566876825407
At time: 160.28641057014465 and batch: 300, loss is 4.655649938583374 and perplexity is 105.1775567413825
At time: 161.897070646286 and batch: 350, loss is 4.622968158721924 and perplexity is 101.79573004839307
At time: 163.5009765625 and batch: 400, loss is 4.638875350952149 and perplexity is 103.42796199010179
At time: 165.10870003700256 and batch: 450, loss is 4.6496477127075195 and perplexity is 104.5481481047788
At time: 166.71928811073303 and batch: 500, loss is 4.643603172302246 and perplexity is 103.91810866696584
At time: 168.3307433128357 and batch: 550, loss is 4.642212924957275 and perplexity is 103.77373717159453
At time: 169.94216561317444 and batch: 600, loss is 4.685500631332397 and perplexity is 108.36450939938257
At time: 171.55191612243652 and batch: 650, loss is 4.6712377166748045 and perplexity is 106.82988576855908
At time: 173.16687726974487 and batch: 700, loss is 4.63897442817688 and perplexity is 103.4382098531919
At time: 174.7779836654663 and batch: 750, loss is 4.6176003646850585 and perplexity is 101.25077544618347
At time: 176.3886215686798 and batch: 800, loss is 4.5945995426177975 and perplexity is 98.94850295414935
At time: 178.00297570228577 and batch: 850, loss is 4.610245428085327 and perplexity is 100.50881429541656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.672077178955078 and perplexity of 106.9196030799485
Finished 6 epochs...
Completing Train Step...
At time: 182.1976339817047 and batch: 50, loss is 4.608831787109375 and perplexity is 100.36683129722097
At time: 183.81108260154724 and batch: 100, loss is 4.544194059371948 and perplexity is 94.08457006784623
At time: 185.42325615882874 and batch: 150, loss is 4.5450319576263425 and perplexity is 94.16343640123888
At time: 187.03266859054565 and batch: 200, loss is 4.572149028778076 and perplexity is 96.75180893890762
At time: 188.64300417900085 and batch: 250, loss is 4.575353927612305 and perplexity is 97.06238611696548
At time: 190.25909304618835 and batch: 300, loss is 4.557873163223267 and perplexity is 95.38040539954866
At time: 191.86854124069214 and batch: 350, loss is 4.526290845870972 and perplexity is 92.41514257208775
At time: 193.47956681251526 and batch: 400, loss is 4.54378155708313 and perplexity is 94.04576797087786
At time: 195.0906105041504 and batch: 450, loss is 4.555635118484497 and perplexity is 95.16717847975721
At time: 196.70696449279785 and batch: 500, loss is 4.552399797439575 and perplexity is 94.85977963954058
At time: 198.3174991607666 and batch: 550, loss is 4.55665602684021 and perplexity is 95.26438505851823
At time: 199.97832012176514 and batch: 600, loss is 4.599074726104736 and perplexity is 99.39230797439284
At time: 201.588285446167 and batch: 650, loss is 4.5811445426940915 and perplexity is 97.62606748974157
At time: 203.20199394226074 and batch: 700, loss is 4.552569246292114 and perplexity is 94.87585488227984
At time: 204.8166844844818 and batch: 750, loss is 4.530590200424195 and perplexity is 92.81332338298705
At time: 206.42755889892578 and batch: 800, loss is 4.5050718784332275 and perplexity is 90.47484700540441
At time: 208.0385434627533 and batch: 850, loss is 4.523282823562622 and perplexity is 92.13757343796699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6473642985026045 and perplexity of 104.30969372696357
Finished 7 epochs...
Completing Train Step...
At time: 212.20244097709656 and batch: 50, loss is 4.519264392852783 and perplexity is 91.7680678967655
At time: 213.84386920928955 and batch: 100, loss is 4.462441034317017 and perplexity is 86.69888595815358
At time: 215.45869421958923 and batch: 150, loss is 4.4579771900177 and perplexity is 86.31273812341279
At time: 217.07643294334412 and batch: 200, loss is 4.4919690418243405 and perplexity is 89.29710261401516
At time: 218.69746613502502 and batch: 250, loss is 4.4898851776123045 and perplexity is 89.11121332892213
At time: 220.31552147865295 and batch: 300, loss is 4.474588079452515 and perplexity is 87.75844345729483
At time: 221.93202590942383 and batch: 350, loss is 4.441082963943481 and perplexity is 84.8667996094099
At time: 223.54706716537476 and batch: 400, loss is 4.462064352035522 and perplexity is 86.66623417404817
At time: 225.15796184539795 and batch: 450, loss is 4.478731126785278 and perplexity is 88.1227850634926
At time: 226.77074217796326 and batch: 500, loss is 4.470103845596314 and perplexity is 87.36579509547408
At time: 228.39030861854553 and batch: 550, loss is 4.479862775802612 and perplexity is 88.22256557422442
At time: 230.0097336769104 and batch: 600, loss is 4.5253237438201905 and perplexity is 92.32581090156647
At time: 231.624520778656 and batch: 650, loss is 4.504945182800293 and perplexity is 90.46338496350909
At time: 233.23710179328918 and batch: 700, loss is 4.477171850204468 and perplexity is 87.98548434122648
At time: 234.84991455078125 and batch: 750, loss is 4.455214242935181 and perplexity is 86.07458974261705
At time: 236.46064043045044 and batch: 800, loss is 4.429505214691162 and perplexity is 83.88989915026757
At time: 238.07871294021606 and batch: 850, loss is 4.448315191268921 and perplexity is 85.48280044059251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.634073257446289 and perplexity of 102.93248186859319
Finished 8 epochs...
Completing Train Step...
At time: 242.2860984802246 and batch: 50, loss is 4.442839097976685 and perplexity is 85.01596802602323
At time: 243.9258201122284 and batch: 100, loss is 4.388712978363037 and perplexity is 80.53669977518297
At time: 245.5367193222046 and batch: 150, loss is 4.3835454940795895 and perplexity is 80.12160107654893
At time: 247.14690732955933 and batch: 200, loss is 4.419489240646362 and perplexity is 83.0538539903759
At time: 248.76054048538208 and batch: 250, loss is 4.416496000289917 and perplexity is 82.80562553190666
At time: 250.3696744441986 and batch: 300, loss is 4.401774921417236 and perplexity is 81.59556593136544
At time: 251.98034286499023 and batch: 350, loss is 4.368002710342407 and perplexity is 78.8859162256442
At time: 253.59421706199646 and batch: 400, loss is 4.389136543273926 and perplexity is 80.57081952069993
At time: 255.2016818523407 and batch: 450, loss is 4.410414400100708 and perplexity is 82.30356304310124
At time: 256.8106586933136 and batch: 500, loss is 4.40219425201416 and perplexity is 81.62978862354329
At time: 258.42101788520813 and batch: 550, loss is 4.412887878417969 and perplexity is 82.50739109994122
At time: 260.03316378593445 and batch: 600, loss is 4.458128871917725 and perplexity is 86.32583119649371
At time: 261.64103150367737 and batch: 650, loss is 4.438919286727906 and perplexity is 84.6833737574635
At time: 263.250675201416 and batch: 700, loss is 4.412942867279053 and perplexity is 82.511928212153
At time: 264.86527585983276 and batch: 750, loss is 4.392245960235596 and perplexity is 80.82173769598205
At time: 266.47929310798645 and batch: 800, loss is 4.364014549255371 and perplexity is 78.57193300823243
At time: 268.09164476394653 and batch: 850, loss is 4.3847306442260745 and perplexity is 80.21661349467098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.624354362487793 and perplexity of 101.93693752128934
Finished 9 epochs...
Completing Train Step...
At time: 272.2564661502838 and batch: 50, loss is 4.376205472946167 and perplexity is 79.53565987248776
At time: 273.897070646286 and batch: 100, loss is 4.3256400680541995 and perplexity is 75.61389541291885
At time: 275.5044376850128 and batch: 150, loss is 4.319847736358643 and perplexity is 75.17718066924117
At time: 277.11518454551697 and batch: 200, loss is 4.357491989135742 and perplexity is 78.06111059743871
At time: 278.723224401474 and batch: 250, loss is 4.354308080673218 and perplexity is 77.81296641062708
At time: 280.3802673816681 and batch: 300, loss is 4.339643993377686 and perplexity is 76.68023580671712
At time: 281.9908654689789 and batch: 350, loss is 4.305665483474732 and perplexity is 74.11852369680979
At time: 283.60358357429504 and batch: 400, loss is 4.328327045440674 and perplexity is 75.81734144503636
At time: 285.21762204170227 and batch: 450, loss is 4.348969173431397 and perplexity is 77.39863721748176
At time: 286.8316888809204 and batch: 500, loss is 4.340925607681275 and perplexity is 76.77857329563554
At time: 288.4403257369995 and batch: 550, loss is 4.353644323348999 and perplexity is 77.76133462163753
At time: 290.04957151412964 and batch: 600, loss is 4.4003086757659915 and perplexity is 81.4760144549923
At time: 291.6625499725342 and batch: 650, loss is 4.380326337814331 and perplexity is 79.86409182609927
At time: 293.2748267650604 and batch: 700, loss is 4.355736036300659 and perplexity is 77.92415924422319
At time: 294.8835551738739 and batch: 750, loss is 4.335374479293823 and perplexity is 76.35354635884737
At time: 296.49657702445984 and batch: 800, loss is 4.3056211757659915 and perplexity is 74.11523974760225
At time: 298.1050977706909 and batch: 850, loss is 4.328382663726806 and perplexity is 75.82155839289526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.617373466491699 and perplexity of 101.2278044342976
Finished 10 epochs...
Completing Train Step...
At time: 302.3176667690277 and batch: 50, loss is 4.318193302154541 and perplexity is 75.05290779916366
At time: 303.931040763855 and batch: 100, loss is 4.268875198364258 and perplexity is 71.4412331933508
At time: 305.5448639392853 and batch: 150, loss is 4.263938794136047 and perplexity is 71.08943940030845
At time: 307.1594252586365 and batch: 200, loss is 4.301585474014282 and perplexity is 73.81673548695034
At time: 308.7645184993744 and batch: 250, loss is 4.296097888946533 and perplexity is 73.4127692847306
At time: 310.3686559200287 and batch: 300, loss is 4.281422023773193 and perplexity is 72.34324070559693
At time: 311.98254799842834 and batch: 350, loss is 4.248152256011963 and perplexity is 69.97599509429135
At time: 313.59646916389465 and batch: 400, loss is 4.273190889358521 and perplexity is 71.75021774140359
At time: 315.20735025405884 and batch: 450, loss is 4.295412425994873 and perplexity is 73.36246479410914
At time: 316.81561946868896 and batch: 500, loss is 4.28738184928894 and perplexity is 72.77568115224429
At time: 318.42579555511475 and batch: 550, loss is 4.30185173034668 and perplexity is 73.83639227696696
At time: 320.0368790626526 and batch: 600, loss is 4.347207689285279 and perplexity is 77.26242075188566
At time: 321.69166016578674 and batch: 650, loss is 4.327228813171387 and perplexity is 75.73412209953952
At time: 323.3039228916168 and batch: 700, loss is 4.302032747268677 and perplexity is 73.84975912320445
At time: 324.91554522514343 and batch: 750, loss is 4.2825320434570315 and perplexity is 72.42358771190214
At time: 326.5297944545746 and batch: 800, loss is 4.250981092453003 and perplexity is 70.17422598840847
At time: 328.1379568576813 and batch: 850, loss is 4.276079416275024 and perplexity is 71.95776979214651
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.61436112721761 and perplexity of 100.92333076260753
Finished 11 epochs...
Completing Train Step...
At time: 332.3305344581604 and batch: 50, loss is 4.264877052307129 and perplexity is 71.15617094851811
At time: 333.94888973236084 and batch: 100, loss is 4.219599189758301 and perplexity is 68.00622123615848
At time: 335.5657000541687 and batch: 150, loss is 4.213837809562683 and perplexity is 67.61553805779094
At time: 337.1816282272339 and batch: 200, loss is 4.251917247772217 and perplexity is 70.23995072281906
At time: 338.7976679801941 and batch: 250, loss is 4.245549001693726 and perplexity is 69.7940666885861
At time: 340.41600489616394 and batch: 300, loss is 4.230914134979248 and perplexity is 68.78007771973317
At time: 342.036235332489 and batch: 350, loss is 4.196824469566345 and perplexity is 66.47490244383856
At time: 343.6516571044922 and batch: 400, loss is 4.22318205833435 and perplexity is 68.25031560710816
At time: 345.26655316352844 and batch: 450, loss is 4.246494598388672 and perplexity is 69.86009494050215
At time: 346.8803005218506 and batch: 500, loss is 4.24185154914856 and perplexity is 69.53648293438027
At time: 348.4962639808655 and batch: 550, loss is 4.255984544754028 and perplexity is 70.5262192372864
At time: 350.1146409511566 and batch: 600, loss is 4.301911735534668 and perplexity is 73.84082297649705
At time: 351.73086309432983 and batch: 650, loss is 4.279646663665772 and perplexity is 72.21491934402447
At time: 353.34651803970337 and batch: 700, loss is 4.255035133361816 and perplexity is 70.45929261676804
At time: 354.96113085746765 and batch: 750, loss is 4.235657844543457 and perplexity is 69.10712552885921
At time: 356.581285238266 and batch: 800, loss is 4.203506698608399 and perplexity is 66.92059040362018
At time: 358.19857835769653 and batch: 850, loss is 4.228179550170898 and perplexity is 68.59224969695995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6149593989054365 and perplexity of 100.98372839934044
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 362.41063809394836 and batch: 50, loss is 4.229104747772217 and perplexity is 68.655740448065
At time: 364.0201678276062 and batch: 100, loss is 4.188445358276367 and perplexity is 65.92022891914344
At time: 365.6331663131714 and batch: 150, loss is 4.179462285041809 and perplexity is 65.33071446514391
At time: 367.24073910713196 and batch: 200, loss is 4.206990971565246 and perplexity is 67.15416669248876
At time: 368.84737062454224 and batch: 250, loss is 4.195595579147339 and perplexity is 66.39326224682313
At time: 370.45646023750305 and batch: 300, loss is 4.174655952453613 and perplexity is 65.01746671266389
At time: 372.0668592453003 and batch: 350, loss is 4.1357705020904545 and perplexity is 62.537757984988176
At time: 373.67472100257874 and batch: 400, loss is 4.151387414932251 and perplexity is 63.52207065828837
At time: 375.28232884407043 and batch: 450, loss is 4.169605526924133 and perplexity is 64.68992863844576
At time: 376.89293384552 and batch: 500, loss is 4.156319198608398 and perplexity is 63.83612154842132
At time: 378.5002477169037 and batch: 550, loss is 4.167554025650024 and perplexity is 64.55735320325611
At time: 380.1046288013458 and batch: 600, loss is 4.207186765670777 and perplexity is 67.1673163697616
At time: 381.70871925354004 and batch: 650, loss is 4.177180399894715 and perplexity is 65.18180723736585
At time: 383.30866956710815 and batch: 700, loss is 4.1420904779434204 and perplexity is 62.93424668514251
At time: 384.9166798591614 and batch: 750, loss is 4.11784791469574 and perplexity is 61.426903976528884
At time: 386.52631068229675 and batch: 800, loss is 4.07332456111908 and perplexity is 58.75196275835599
At time: 388.12942838668823 and batch: 850, loss is 4.097435808181762 and perplexity is 60.185761727248945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5843610763549805 and perplexity of 97.94059058789509
Finished 13 epochs...
Completing Train Step...
At time: 392.3616211414337 and batch: 50, loss is 4.173975677490234 and perplexity is 64.97325199866353
At time: 394.00751209259033 and batch: 100, loss is 4.129812216758728 and perplexity is 62.16624805906384
At time: 395.6131420135498 and batch: 150, loss is 4.123269400596619 and perplexity is 61.760833449418485
At time: 397.2184386253357 and batch: 200, loss is 4.1560492420196535 and perplexity is 63.81889089267859
At time: 398.8250517845154 and batch: 250, loss is 4.145805897712708 and perplexity is 63.16850875091076
At time: 400.4303708076477 and batch: 300, loss is 4.130659747123718 and perplexity is 62.21895817552837
At time: 402.0401248931885 and batch: 350, loss is 4.093654842376709 and perplexity is 59.958631077399914
At time: 403.68334579467773 and batch: 400, loss is 4.1123642730712895 and perplexity is 61.09098272666756
At time: 405.2925593852997 and batch: 450, loss is 4.135688643455506 and perplexity is 62.53263893900869
At time: 406.90244913101196 and batch: 500, loss is 4.124551258087158 and perplexity is 61.84005279950875
At time: 408.51657795906067 and batch: 550, loss is 4.141189556121827 and perplexity is 62.877573381919376
At time: 410.1281292438507 and batch: 600, loss is 4.183887100219726 and perplexity is 65.62043130112329
At time: 411.73924374580383 and batch: 650, loss is 4.156458930969238 and perplexity is 63.84504214362557
At time: 413.34860134124756 and batch: 700, loss is 4.124490189552307 and perplexity is 61.8362764333989
At time: 414.9600307941437 and batch: 750, loss is 4.106058893203735 and perplexity is 60.70699274765128
At time: 416.5724675655365 and batch: 800, loss is 4.063055686950683 and perplexity is 58.151733360220334
At time: 418.1860771179199 and batch: 850, loss is 4.090458545684815 and perplexity is 59.76729145523615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.582511901855469 and perplexity of 97.75964869344149
Finished 14 epochs...
Completing Train Step...
At time: 422.37678503990173 and batch: 50, loss is 4.144592609405517 and perplexity is 63.09191361324919
At time: 424.01513147354126 and batch: 100, loss is 4.100346760749817 and perplexity is 60.36121486890189
At time: 425.6318531036377 and batch: 150, loss is 4.095007481575013 and perplexity is 60.039788347950534
At time: 427.2489900588989 and batch: 200, loss is 4.128210611343384 and perplexity is 62.06676194953305
At time: 428.8668763637543 and batch: 250, loss is 4.117738356590271 and perplexity is 61.42017452994385
At time: 430.4821560382843 and batch: 300, loss is 4.105339713096619 and perplexity is 60.663349181777626
At time: 432.0957281589508 and batch: 350, loss is 4.068992409706116 and perplexity is 58.49799087939719
At time: 433.71041083335876 and batch: 400, loss is 4.088485746383667 and perplexity is 59.64949881324615
At time: 435.33000230789185 and batch: 450, loss is 4.113658146858215 and perplexity is 61.1700779063905
At time: 436.94470596313477 and batch: 500, loss is 4.103319969177246 and perplexity is 60.5409484018642
At time: 438.55828404426575 and batch: 550, loss is 4.12193428516388 and perplexity is 61.67843062842083
At time: 440.1794273853302 and batch: 600, loss is 4.1656934356689455 and perplexity is 64.43735011157504
At time: 441.797682762146 and batch: 650, loss is 4.139006767272949 and perplexity is 62.74047459917863
At time: 443.46076345443726 and batch: 700, loss is 4.108117380142212 and perplexity is 60.83208600654411
At time: 445.07448983192444 and batch: 750, loss is 4.091978740692139 and perplexity is 59.858218489208255
At time: 446.6918535232544 and batch: 800, loss is 4.048747758865357 and perplexity is 57.32562657263286
At time: 448.3111011981964 and batch: 850, loss is 4.077577576637268 and perplexity is 59.00236787867579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.583103179931641 and perplexity of 97.81746892268059
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 452.49517798423767 and batch: 50, loss is 4.1344642210006715 and perplexity is 62.45611942739575
At time: 454.1312518119812 and batch: 100, loss is 4.098504734039307 and perplexity is 60.25013024067045
At time: 455.741491317749 and batch: 150, loss is 4.094254746437072 and perplexity is 59.99461129489732
At time: 457.3542685508728 and batch: 200, loss is 4.125692830085755 and perplexity is 61.91068798207876
At time: 458.9622497558594 and batch: 250, loss is 4.108884654045105 and perplexity is 60.87877878938163
At time: 460.56822085380554 and batch: 300, loss is 4.093068532943725 and perplexity is 59.92348707004872
At time: 462.17739152908325 and batch: 350, loss is 4.056067271232605 and perplexity is 57.74676157652632
At time: 463.7799382209778 and batch: 400, loss is 4.067293591499329 and perplexity is 58.398697791763205
At time: 465.38868522644043 and batch: 450, loss is 4.092524037361145 and perplexity is 59.89086787735536
At time: 467.0030951499939 and batch: 500, loss is 4.0763397121429445 and perplexity is 58.92937612866609
At time: 468.61280393600464 and batch: 550, loss is 4.090522627830506 and perplexity is 59.771121594235204
At time: 470.2211594581604 and batch: 600, loss is 4.1305212926864625 and perplexity is 62.21034428101727
At time: 471.8312888145447 and batch: 650, loss is 4.099883995056152 and perplexity is 60.333288231675844
At time: 473.44423747062683 and batch: 700, loss is 4.065061945915222 and perplexity is 58.26851790742789
At time: 475.0533459186554 and batch: 750, loss is 4.043959584236145 and perplexity is 57.051797555614314
At time: 476.66335225105286 and batch: 800, loss is 3.9949717807769773 and perplexity is 54.32430761243072
At time: 478.27521204948425 and batch: 850, loss is 4.024283652305603 and perplexity is 55.94022177634791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564034461975098 and perplexity of 95.96988669266041
Finished 16 epochs...
Completing Train Step...
At time: 482.48195934295654 and batch: 50, loss is 4.113317699432373 and perplexity is 61.149256255368535
At time: 484.092246055603 and batch: 100, loss is 4.071358623504639 and perplexity is 58.636573526027284
At time: 485.7332344055176 and batch: 150, loss is 4.067378888130188 and perplexity is 58.40367921637774
At time: 487.3429374694824 and batch: 200, loss is 4.099895758628845 and perplexity is 60.33399797087228
At time: 488.9542770385742 and batch: 250, loss is 4.084616842269898 and perplexity is 59.41916647607587
At time: 490.5699713230133 and batch: 300, loss is 4.07205436706543 and perplexity is 58.67738373956029
At time: 492.1785387992859 and batch: 350, loss is 4.036846284866333 and perplexity is 56.64741100643712
At time: 493.7870011329651 and batch: 400, loss is 4.049428019523621 and perplexity is 57.36463620795632
At time: 495.3998279571533 and batch: 450, loss is 4.077589721679687 and perplexity is 59.00308446928799
At time: 497.01298928260803 and batch: 500, loss is 4.0634552145004275 and perplexity is 58.17497122153691
At time: 498.62092781066895 and batch: 550, loss is 4.079596643447876 and perplexity is 59.12161794782373
At time: 500.2287452220917 and batch: 600, loss is 4.1219472980499265 and perplexity is 61.67923324803234
At time: 501.8391377925873 and batch: 650, loss is 4.093113236427307 and perplexity is 59.92616591854559
At time: 503.45617413520813 and batch: 700, loss is 4.0608648014068605 and perplexity is 58.024469029949415
At time: 505.06523418426514 and batch: 750, loss is 4.042140960693359 and perplexity is 56.94813610257168
At time: 506.673930644989 and batch: 800, loss is 3.994742512702942 and perplexity is 54.311854210689376
At time: 508.28056263923645 and batch: 850, loss is 4.025286965370178 and perplexity is 55.99637549686152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.563739776611328 and perplexity of 95.94160993826692
Finished 17 epochs...
Completing Train Step...
At time: 512.5066239833832 and batch: 50, loss is 4.099654607772827 and perplexity is 60.31945012979727
At time: 514.1175696849823 and batch: 100, loss is 4.057342257499695 and perplexity is 57.8204348607379
At time: 515.7271656990051 and batch: 150, loss is 4.053650641441346 and perplexity is 57.60737751981839
At time: 517.3398492336273 and batch: 200, loss is 4.086850290298462 and perplexity is 59.552024406714594
At time: 518.9539339542389 and batch: 250, loss is 4.071742763519287 and perplexity is 58.65910250710502
At time: 520.5645558834076 and batch: 300, loss is 4.060227680206299 and perplexity is 57.987512184824
At time: 522.1774227619171 and batch: 350, loss is 4.02564236164093 and perplexity is 56.01627993666107
At time: 523.7904515266418 and batch: 400, loss is 4.038961110115051 and perplexity is 56.76733714822703
At time: 525.4241273403168 and batch: 450, loss is 4.068301372528076 and perplexity is 58.457580556984574
At time: 527.0297315120697 and batch: 500, loss is 4.0549289894104 and perplexity is 57.68106688416294
At time: 528.6369531154633 and batch: 550, loss is 4.071778874397278 and perplexity is 58.66122077704477
At time: 530.2376608848572 and batch: 600, loss is 4.115019884109497 and perplexity is 61.25343222059626
At time: 531.8487868309021 and batch: 650, loss is 4.08668625831604 and perplexity is 59.54225677121813
At time: 533.4647188186646 and batch: 700, loss is 4.0555982446670535 and perplexity is 57.71968316201665
At time: 535.0721406936646 and batch: 750, loss is 4.0381005048751835 and perplexity is 56.71850389651409
At time: 536.6811044216156 and batch: 800, loss is 3.9913057041168214 and perplexity is 54.125515153060874
At time: 538.2935836315155 and batch: 850, loss is 4.022381987571716 and perplexity is 55.83394331441969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.564072608947754 and perplexity of 95.97354772313207
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 542.4885280132294 and batch: 50, loss is 4.097600960731507 and perplexity is 60.195702380095994
At time: 544.108499288559 and batch: 100, loss is 4.064055657386779 and perplexity is 58.20991245822933
At time: 545.7248373031616 and batch: 150, loss is 4.062688474655151 and perplexity is 58.130383248975555
At time: 547.3393623828888 and batch: 200, loss is 4.098111958503723 and perplexity is 60.22647011036052
At time: 548.9548227787018 and batch: 250, loss is 4.077260918617249 and perplexity is 58.98368726352626
At time: 550.572420835495 and batch: 300, loss is 4.061810321807862 and perplexity is 58.07935829454494
At time: 552.1887106895447 and batch: 350, loss is 4.026391172409058 and perplexity is 56.058241238844865
At time: 553.8024971485138 and batch: 400, loss is 4.040306310653687 and perplexity is 56.843751985853
At time: 555.416289806366 and batch: 450, loss is 4.069555587768555 and perplexity is 58.530944943189134
At time: 557.0348052978516 and batch: 500, loss is 4.051441659927368 and perplexity is 57.48026433483476
At time: 558.6476521492004 and batch: 550, loss is 4.0639111328125 and perplexity is 58.20150030330864
At time: 560.2593965530396 and batch: 600, loss is 4.102762579917908 and perplexity is 60.50721293024304
At time: 561.8743472099304 and batch: 650, loss is 4.071549749374389 and perplexity is 58.64778156318042
At time: 563.4927115440369 and batch: 700, loss is 4.0391508436203 and perplexity is 56.778108835930546
At time: 565.1063687801361 and batch: 750, loss is 4.018471136093139 and perplexity is 55.616011481936205
At time: 566.7666420936584 and batch: 800, loss is 3.9688578414916993 and perplexity is 52.92404865366603
At time: 568.3817970752716 and batch: 850, loss is 4.001769561767578 and perplexity is 54.694850365395254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.553895314534505 and perplexity of 95.0017501948484
Finished 19 epochs...
Completing Train Step...
At time: 572.5536322593689 and batch: 50, loss is 4.091732597351074 and perplexity is 59.84348660047166
At time: 574.1869304180145 and batch: 100, loss is 4.051003913879395 and perplexity is 57.4551080827124
At time: 575.795126914978 and batch: 150, loss is 4.047578468322754 and perplexity is 57.25863543331824
At time: 577.4081430435181 and batch: 200, loss is 4.083353219032287 and perplexity is 59.34413045517153
At time: 579.0175406932831 and batch: 250, loss is 4.06369312286377 and perplexity is 58.18881318022128
At time: 580.6282775402069 and batch: 300, loss is 4.050032329559326 and perplexity is 57.399312709935174
At time: 582.2364602088928 and batch: 350, loss is 4.016035742759705 and perplexity is 55.48072941772428
At time: 583.8497538566589 and batch: 400, loss is 4.030879721641541 and perplexity is 56.31042696515573
At time: 585.4575598239899 and batch: 450, loss is 4.060680589675903 and perplexity is 58.01378122651115
At time: 587.064611196518 and batch: 500, loss is 4.044175586700439 and perplexity is 57.06412221550929
At time: 588.6827380657196 and batch: 550, loss is 4.0587094163894655 and perplexity is 57.899538643666965
At time: 590.2959756851196 and batch: 600, loss is 4.0996791362762455 and perplexity is 60.32092969378166
At time: 591.9020805358887 and batch: 650, loss is 4.069580955505371 and perplexity is 58.53242975962918
At time: 593.5089128017426 and batch: 700, loss is 4.038759303092957 and perplexity is 56.75588225684257
At time: 595.1137442588806 and batch: 750, loss is 4.019748239517212 and perplexity is 55.687084254613794
At time: 596.7211816310883 and batch: 800, loss is 3.9706833457946775 and perplexity is 53.020749969694094
At time: 598.3218655586243 and batch: 850, loss is 4.004205026626587 and perplexity is 54.828220094149344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.553786277770996 and perplexity of 94.99139207619761
Finished 20 epochs...
Completing Train Step...
At time: 602.4666593074799 and batch: 50, loss is 4.085794634819031 and perplexity is 59.48919115680589
At time: 604.1077864170074 and batch: 100, loss is 4.044379611015319 and perplexity is 57.07576587170264
At time: 605.7208931446075 and batch: 150, loss is 4.040744047164917 and perplexity is 56.868640018335384
At time: 607.3699834346771 and batch: 200, loss is 4.076867470741272 and perplexity is 58.96048482183014
At time: 608.9822294712067 and batch: 250, loss is 4.057424936294556 and perplexity is 57.82521558223995
At time: 610.5931169986725 and batch: 300, loss is 4.044084095954895 and perplexity is 57.05890161524593
At time: 612.2059743404388 and batch: 350, loss is 4.010452470779419 and perplexity is 55.17182855692575
At time: 613.8160190582275 and batch: 400, loss is 4.0259702682495115 and perplexity is 56.03465105688171
At time: 615.4252305030823 and batch: 450, loss is 4.056331462860108 and perplexity is 57.76201980290932
At time: 617.0375862121582 and batch: 500, loss is 4.040308685302734 and perplexity is 56.843886969974754
At time: 618.6481413841248 and batch: 550, loss is 4.055495090484619 and perplexity is 57.713729442370536
At time: 620.2597370147705 and batch: 600, loss is 4.097142524719239 and perplexity is 60.168112826846766
At time: 621.8717720508575 and batch: 650, loss is 4.067387657165527 and perplexity is 58.404191362550264
At time: 623.4854352474213 and batch: 700, loss is 4.0372383546829225 and perplexity is 56.669625100934105
At time: 625.0927991867065 and batch: 750, loss is 4.019094357490539 and perplexity is 55.65068337333987
At time: 626.701007604599 and batch: 800, loss is 3.970187864303589 and perplexity is 52.994485676713325
At time: 628.3113038539886 and batch: 850, loss is 4.0040371799468994 and perplexity is 54.81901813173439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5539595286051435 and perplexity of 95.00785083981773
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 632.496337890625 and batch: 50, loss is 4.086182022094727 and perplexity is 59.512240976816706
At time: 634.1365230083466 and batch: 100, loss is 4.051447319984436 and perplexity is 57.48058967733188
At time: 635.7535855770111 and batch: 150, loss is 4.047557406425476 and perplexity is 57.25742947052046
At time: 637.3689756393433 and batch: 200, loss is 4.086904020309448 and perplexity is 59.55522422360254
At time: 638.9810616970062 and batch: 250, loss is 4.067414951324463 and perplexity is 58.40578547758673
At time: 640.5955414772034 and batch: 300, loss is 4.049784655570984 and perplexity is 57.38509815358879
At time: 642.211314201355 and batch: 350, loss is 4.0133489942550655 and perplexity is 55.33186671879035
At time: 643.827644109726 and batch: 400, loss is 4.029854655265808 and perplexity is 56.2527346141669
At time: 645.4409575462341 and batch: 450, loss is 4.060759086608886 and perplexity is 58.01833530914659
At time: 647.0536816120148 and batch: 500, loss is 4.042919840812683 and perplexity is 56.99250915201427
At time: 648.6957213878632 and batch: 550, loss is 4.056390347480774 and perplexity is 57.765421197678286
At time: 650.3112490177155 and batch: 600, loss is 4.094045391082764 and perplexity is 59.98205241647316
At time: 651.9240803718567 and batch: 650, loss is 4.061577067375183 and perplexity is 58.06581260663292
At time: 653.5365786552429 and batch: 700, loss is 4.030361018180847 and perplexity is 56.2812261257587
At time: 655.1519515514374 and batch: 750, loss is 4.008399724960327 and perplexity is 55.05869097759828
At time: 656.7677428722382 and batch: 800, loss is 3.95866804599762 and perplexity is 52.387501715503625
At time: 658.380161523819 and batch: 850, loss is 3.9932754755020143 and perplexity is 54.23223511647717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.55103874206543 and perplexity of 94.73075804935368
Finished 22 epochs...
Completing Train Step...
At time: 662.568977355957 and batch: 50, loss is 4.0834225034713745 and perplexity is 59.348242222402334
At time: 664.1787359714508 and batch: 100, loss is 4.046136717796326 and perplexity is 57.17614224712655
At time: 665.7943019866943 and batch: 150, loss is 4.040416541099549 and perplexity is 56.85001824333869
At time: 667.4062633514404 and batch: 200, loss is 4.078754110336304 and perplexity is 59.07182700529722
At time: 669.009289264679 and batch: 250, loss is 4.05941267490387 and perplexity is 57.94027130831604
At time: 670.6198427677155 and batch: 300, loss is 4.042580018043518 and perplexity is 56.97314509009349
At time: 672.2384080886841 and batch: 350, loss is 4.0071765899658205 and perplexity is 54.99138793465616
At time: 673.848961353302 and batch: 400, loss is 4.024513773918152 and perplexity is 55.95309631168701
At time: 675.4571352005005 and batch: 450, loss is 4.056643071174622 and perplexity is 57.78002173317295
At time: 677.0668776035309 and batch: 500, loss is 4.03910683631897 and perplexity is 56.77561023956469
At time: 678.683046579361 and batch: 550, loss is 4.053251495361328 and perplexity is 57.58438834922516
At time: 680.2989466190338 and batch: 600, loss is 4.092446804046631 and perplexity is 59.88624248573951
At time: 681.9139831066132 and batch: 650, loss is 4.061185755729675 and perplexity is 58.04309522303232
At time: 683.5253667831421 and batch: 700, loss is 4.031164350509644 and perplexity is 56.326456819416634
At time: 685.1388247013092 and batch: 750, loss is 4.010372638702393 and perplexity is 55.167424251063494
At time: 686.7505211830139 and batch: 800, loss is 3.961006383895874 and perplexity is 52.510144730682484
At time: 688.4114713668823 and batch: 850, loss is 3.9954807233810423 and perplexity is 54.35196260381574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.550893147786458 and perplexity of 94.71696679692714
Finished 23 epochs...
Completing Train Step...
At time: 692.596508026123 and batch: 50, loss is 4.080883984565735 and perplexity is 59.1977766481577
At time: 694.2028877735138 and batch: 100, loss is 4.043076596260071 and perplexity is 57.00144373853841
At time: 695.8167080879211 and batch: 150, loss is 4.0369542121887205 and perplexity is 56.65352513976232
At time: 697.4255814552307 and batch: 200, loss is 4.075280232429504 and perplexity is 58.86697471248647
At time: 699.0342662334442 and batch: 250, loss is 4.05603298664093 and perplexity is 57.74478178632346
At time: 700.6447803974152 and batch: 300, loss is 4.039325156211853 and perplexity is 56.78800683787333
At time: 702.2581610679626 and batch: 350, loss is 4.004251785278321 and perplexity is 54.830783847736264
At time: 703.8655035495758 and batch: 400, loss is 4.021930370330811 and perplexity is 55.8087334360275
At time: 705.4758048057556 and batch: 450, loss is 4.0545602130889895 and perplexity is 57.65979939421673
At time: 707.0904748439789 and batch: 500, loss is 4.037184681892395 and perplexity is 56.66658356564136
At time: 708.7020103931427 and batch: 550, loss is 4.051881651878357 and perplexity is 57.50556075318494
At time: 710.3192293643951 and batch: 600, loss is 4.0914963388442995 and perplexity is 59.829349737730055
At time: 711.9386925697327 and batch: 650, loss is 4.060601963996887 and perplexity is 58.00922003288573
At time: 713.5471544265747 and batch: 700, loss is 4.031095581054688 and perplexity is 56.32258341286909
At time: 715.1519110202789 and batch: 750, loss is 4.010754947662353 and perplexity is 55.18851928380571
At time: 716.7622368335724 and batch: 800, loss is 3.961618604660034 and perplexity is 52.54230237439984
At time: 718.3764173984528 and batch: 850, loss is 3.996030082702637 and perplexity is 54.38182956421452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5509382883707685 and perplexity of 94.72124247265499
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 722.5870616436005 and batch: 50, loss is 4.081201667785645 and perplexity is 59.216585775968746
At time: 724.2009425163269 and batch: 100, loss is 4.04739803314209 and perplexity is 57.24830489311377
At time: 725.8138093948364 and batch: 150, loss is 4.042252626419067 and perplexity is 56.95449561258033
At time: 727.4253308773041 and batch: 200, loss is 4.082148518562317 and perplexity is 59.27268159919854
At time: 729.0338726043701 and batch: 250, loss is 4.062557029724121 and perplexity is 58.12274280691832
At time: 730.6722388267517 and batch: 300, loss is 4.043501162528992 and perplexity is 57.025649766997596
At time: 732.2758495807648 and batch: 350, loss is 4.006876788139343 and perplexity is 54.97490388721006
At time: 733.8847146034241 and batch: 400, loss is 4.023742880821228 and perplexity is 55.90997907751321
At time: 735.4990744590759 and batch: 450, loss is 4.057229037284851 and perplexity is 57.81388878926142
At time: 737.1077935695648 and batch: 500, loss is 4.038102121353149 and perplexity is 56.71859558079999
At time: 738.720358133316 and batch: 550, loss is 4.053160719871521 and perplexity is 57.579161335413445
At time: 740.3318798542023 and batch: 600, loss is 4.090687227249146 and perplexity is 59.780960695834054
At time: 741.9440257549286 and batch: 650, loss is 4.056232790946961 and perplexity is 57.75632059508863
At time: 743.5530760288239 and batch: 700, loss is 4.026458644866944 and perplexity is 56.06202375377236
At time: 745.1616990566254 and batch: 750, loss is 4.004667439460754 and perplexity is 54.853579229538404
At time: 746.77365899086 and batch: 800, loss is 3.9546294021606445 and perplexity is 52.17635391693976
At time: 748.3852784633636 and batch: 850, loss is 3.9900452661514283 and perplexity is 54.05733627551468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.550114631652832 and perplexity of 94.64325680607905
Finished 25 epochs...
Completing Train Step...
At time: 752.5661640167236 and batch: 50, loss is 4.079213752746582 and perplexity is 59.09898516328392
At time: 754.2064940929413 and batch: 100, loss is 4.044166688919067 and perplexity is 57.06361447368452
At time: 755.8268821239471 and batch: 150, loss is 4.038259515762329 and perplexity is 56.72752347322268
At time: 757.4390952587128 and batch: 200, loss is 4.0773798274993895 and perplexity is 58.990701364854516
At time: 759.0537121295929 and batch: 250, loss is 4.057926354408264 and perplexity is 57.85421746318853
At time: 760.6682295799255 and batch: 300, loss is 4.039702882766724 and perplexity is 56.80946122774548
At time: 762.2826340198517 and batch: 350, loss is 4.003641877174378 and perplexity is 54.79735230444292
At time: 763.9004149436951 and batch: 400, loss is 4.020853848457336 and perplexity is 55.7486864405038
At time: 765.5184137821198 and batch: 450, loss is 4.0551127910614015 and perplexity is 57.69166973386856
At time: 767.1315960884094 and batch: 500, loss is 4.036763896942139 and perplexity is 56.642744136082854
At time: 768.74711561203 and batch: 550, loss is 4.052397093772888 and perplexity is 57.53520916872694
At time: 770.3909344673157 and batch: 600, loss is 4.0907046461105345 and perplexity is 59.78200202117143
At time: 772.009886264801 and batch: 650, loss is 4.056935706138611 and perplexity is 57.79693266199596
At time: 773.6233065128326 and batch: 700, loss is 4.027667989730835 and perplexity is 56.12986308656468
At time: 775.2370994091034 and batch: 750, loss is 4.006112833023071 and perplexity is 54.93292156646505
At time: 776.8567895889282 and batch: 800, loss is 3.9561994886398315 and perplexity is 52.2583396502613
At time: 778.4695382118225 and batch: 850, loss is 3.991480212211609 and perplexity is 54.13496131778175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.550005912780762 and perplexity of 94.63296785726162
Finished 26 epochs...
Completing Train Step...
At time: 782.662960767746 and batch: 50, loss is 4.07791983127594 and perplexity is 59.02256516888586
At time: 784.3024051189423 and batch: 100, loss is 4.042468862533569 and perplexity is 56.96681256305179
At time: 785.9178631305695 and batch: 150, loss is 4.036192355155944 and perplexity is 56.61037969062186
At time: 787.5271685123444 and batch: 200, loss is 4.075215182304382 and perplexity is 58.86314553296152
At time: 789.1369516849518 and batch: 250, loss is 4.055815467834472 and perplexity is 57.73222257629185
At time: 790.7466676235199 and batch: 300, loss is 4.0378355169296265 and perplexity is 56.703476167862895
At time: 792.3580002784729 and batch: 350, loss is 4.001938085556031 and perplexity is 54.70406852550552
At time: 793.9660222530365 and batch: 400, loss is 4.019360809326172 and perplexity is 55.6655135757585
At time: 795.5771236419678 and batch: 450, loss is 4.054010620117188 and perplexity is 57.62811868026047
At time: 797.1844863891602 and batch: 500, loss is 4.036001505851746 and perplexity is 56.59957666995478
At time: 798.7886900901794 and batch: 550, loss is 4.052052907943725 and perplexity is 57.51540977258439
At time: 800.3952202796936 and batch: 600, loss is 4.090739736557007 and perplexity is 59.78409983511975
At time: 801.9981763362885 and batch: 650, loss is 4.057101082801819 and perplexity is 57.80649171626373
At time: 803.6022651195526 and batch: 700, loss is 4.028066468238831 and perplexity is 56.152234087546944
At time: 805.2130453586578 and batch: 750, loss is 4.006669483184814 and perplexity is 54.963508498460584
At time: 806.8174154758453 and batch: 800, loss is 3.956837077140808 and perplexity is 52.2916695909668
At time: 808.4224858283997 and batch: 850, loss is 3.9920848035812377 and perplexity is 54.16770074417973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549996058146159 and perplexity of 94.6320352885371
Finished 27 epochs...
Completing Train Step...
At time: 812.582982301712 and batch: 50, loss is 4.0767746257781985 and perplexity is 58.95501089191142
At time: 814.2274329662323 and batch: 100, loss is 4.041160478591919 and perplexity is 56.89232683887577
At time: 815.8386707305908 and batch: 150, loss is 4.034738650321961 and perplexity is 56.52814469519911
At time: 817.4489185810089 and batch: 200, loss is 4.073781442642212 and perplexity is 58.7788115774857
At time: 819.0665395259857 and batch: 250, loss is 4.054410953521728 and perplexity is 57.65119375976339
At time: 820.6783654689789 and batch: 300, loss is 4.036525573730469 and perplexity is 56.62924646385071
At time: 822.2896769046783 and batch: 350, loss is 4.000702271461487 and perplexity is 54.63650622241464
At time: 823.8985602855682 and batch: 400, loss is 4.018246583938598 and perplexity is 55.60352418880772
At time: 825.5077753067017 and batch: 450, loss is 4.053147048950195 and perplexity is 57.57837418060941
At time: 827.1217229366302 and batch: 500, loss is 4.035353164672852 and perplexity is 56.56289272683214
At time: 828.734970331192 and batch: 550, loss is 4.051719031333923 and perplexity is 57.496209927926024
At time: 830.3417253494263 and batch: 600, loss is 4.090654401779175 and perplexity is 59.7789983899109
At time: 831.952526807785 and batch: 650, loss is 4.057045788764953 and perplexity is 57.80329545034773
At time: 833.5589866638184 and batch: 700, loss is 4.028184871673584 and perplexity is 56.158883098557105
At time: 835.1670024394989 and batch: 750, loss is 4.006907072067261 and perplexity is 54.97656876844609
At time: 836.776379108429 and batch: 800, loss is 3.9571362018585203 and perplexity is 52.307313661519345
At time: 838.3864705562592 and batch: 850, loss is 3.992381100654602 and perplexity is 54.18375285336015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.550012270609538 and perplexity of 94.63356951938049
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 842.6366238594055 and batch: 50, loss is 4.076870856285095 and perplexity is 58.96068443547324
At time: 844.2548515796661 and batch: 100, loss is 4.0430350017547605 and perplexity is 56.99907284099263
At time: 845.8772659301758 and batch: 150, loss is 4.037081198692322 and perplexity is 56.660719829641096
At time: 847.4939773082733 and batch: 200, loss is 4.077005300521851 and perplexity is 58.96861189257929
At time: 849.1107647418976 and batch: 250, loss is 4.057396650314331 and perplexity is 57.82357996246816
At time: 850.7271177768707 and batch: 300, loss is 4.038998436927796 and perplexity is 56.76945613153798
At time: 852.3949294090271 and batch: 350, loss is 4.001589212417603 and perplexity is 54.684987074130696
At time: 854.0164680480957 and batch: 400, loss is 4.017876224517822 and perplexity is 55.5829347127846
At time: 855.6347620487213 and batch: 450, loss is 4.0533513164520265 and perplexity is 57.59013677257954
At time: 857.2491090297699 and batch: 500, loss is 4.035097155570984 and perplexity is 56.54841396489259
At time: 858.8679480552673 and batch: 550, loss is 4.051186428070069 and perplexity is 57.46559541227831
At time: 860.4814131259918 and batch: 600, loss is 4.089290728569031 and perplexity is 59.69753492867134
At time: 862.0937435626984 and batch: 650, loss is 4.053957896232605 and perplexity is 57.6250803820786
At time: 863.7052381038666 and batch: 700, loss is 4.024718618392944 and perplexity is 55.96455916832533
At time: 865.3196811676025 and batch: 750, loss is 4.003636040687561 and perplexity is 54.79703248135194
At time: 866.9366767406464 and batch: 800, loss is 3.953673357963562 and perplexity is 52.12649485408409
At time: 868.554532289505 and batch: 850, loss is 3.989033570289612 and perplexity is 54.00267434738951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549902598063151 and perplexity of 94.6231913839463
Finished 29 epochs...
Completing Train Step...
At time: 872.765317440033 and batch: 50, loss is 4.076066722869873 and perplexity is 58.91329123671555
At time: 874.3737111091614 and batch: 100, loss is 4.04187463760376 and perplexity is 56.932971518447
At time: 875.9856975078583 and batch: 150, loss is 4.03541316986084 and perplexity is 56.56628689567622
At time: 877.5995469093323 and batch: 200, loss is 4.075081043243408 and perplexity is 58.855250215440805
At time: 879.2103173732758 and batch: 250, loss is 4.05567494392395 and perplexity is 57.72411038860382
At time: 880.8183090686798 and batch: 300, loss is 4.037457389831543 and perplexity is 56.68203910019181
At time: 882.4338500499725 and batch: 350, loss is 4.000485558509826 and perplexity is 54.62466706677828
At time: 884.036360502243 and batch: 400, loss is 4.017029423713684 and perplexity is 55.53588696182148
At time: 885.6438634395599 and batch: 450, loss is 4.052753057479858 and perplexity is 57.55569326064429
At time: 887.2571048736572 and batch: 500, loss is 4.034762115478515 and perplexity is 56.52947115252683
At time: 888.8767066001892 and batch: 550, loss is 4.051114301681519 and perplexity is 57.461450775885986
At time: 890.4873714447021 and batch: 600, loss is 4.0896306896209715 and perplexity is 59.71783321556193
At time: 892.0977210998535 and batch: 650, loss is 4.054455695152282 and perplexity is 57.653773225879924
At time: 893.7368836402893 and batch: 700, loss is 4.025359144210816 and perplexity is 56.000417396192844
At time: 895.3486816883087 and batch: 750, loss is 4.004306426048279 and perplexity is 54.833779925836396
At time: 896.9604368209839 and batch: 800, loss is 3.9543465423583983 and perplexity is 52.161597410898466
At time: 898.5726518630981 and batch: 850, loss is 3.9896160221099852 and perplexity is 54.034137465353844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549893379211426 and perplexity of 94.62231907079601
Finished 30 epochs...
Completing Train Step...
At time: 902.7716114521027 and batch: 50, loss is 4.0754529476165775 and perplexity is 58.87714281110362
At time: 904.3809213638306 and batch: 100, loss is 4.041084218025207 and perplexity is 56.88798836321892
At time: 905.9944114685059 and batch: 150, loss is 4.034364213943482 and perplexity is 56.50698246361923
At time: 907.6044325828552 and batch: 200, loss is 4.073922772407531 and perplexity is 58.78711936018635
At time: 909.212553024292 and batch: 250, loss is 4.054591417312622 and perplexity is 57.66159865156371
At time: 910.8235297203064 and batch: 300, loss is 4.036543235778809 and perplexity is 56.630246661171974
At time: 912.4396929740906 and batch: 350, loss is 3.999754114151001 and perplexity is 54.584726771032294
At time: 914.0503480434418 and batch: 400, loss is 4.016435618400574 and perplexity is 55.502919246246336
At time: 915.6588215827942 and batch: 450, loss is 4.052318601608277 and perplexity is 57.530693282851054
At time: 917.2686142921448 and batch: 500, loss is 4.0344993686676025 and perplexity is 56.51462016536911
At time: 918.8817586898804 and batch: 550, loss is 4.051028118133545 and perplexity is 57.456498757580725
At time: 920.4948780536652 and batch: 600, loss is 4.089794445037842 and perplexity is 59.727613134970255
At time: 922.1023921966553 and batch: 650, loss is 4.05470769405365 and perplexity is 57.66830374415296
At time: 923.7160379886627 and batch: 700, loss is 4.025696997642517 and perplexity is 56.019340525829
At time: 925.3275101184845 and batch: 750, loss is 4.004681344032288 and perplexity is 54.854341950357295
At time: 926.9379336833954 and batch: 800, loss is 3.9547484159469604 and perplexity is 52.182563991910506
At time: 928.5476043224335 and batch: 850, loss is 3.989959454536438 and perplexity is 54.052697727210315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549899737040202 and perplexity of 94.6229206652115
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 932.7292478084564 and batch: 50, loss is 4.07538830280304 and perplexity is 58.87333683220468
At time: 934.3651933670044 and batch: 100, loss is 4.041688060760498 and perplexity is 56.92235013522644
At time: 935.9774343967438 and batch: 150, loss is 4.035022349357605 and perplexity is 56.54418395038893
At time: 937.5864305496216 and batch: 200, loss is 4.074738392829895 and perplexity is 58.83508689429951
At time: 939.2008943557739 and batch: 250, loss is 4.055448908805847 and perplexity is 57.71106418700011
At time: 940.8125264644623 and batch: 300, loss is 4.037472233772278 and perplexity is 56.68288049126573
At time: 942.423894405365 and batch: 350, loss is 4.0000603723526 and perplexity is 54.60144635141158
At time: 944.0328695774078 and batch: 400, loss is 4.015866117477417 and perplexity is 55.47131928145634
At time: 945.6390650272369 and batch: 450, loss is 4.052016530036926 and perplexity is 57.51331752042429
At time: 947.2473819255829 and batch: 500, loss is 4.034195652008057 and perplexity is 56.49745834001505
At time: 948.8482897281647 and batch: 550, loss is 4.050474953651428 and perplexity is 57.424724652164734
At time: 950.4541208744049 and batch: 600, loss is 4.088644227981567 and perplexity is 59.65895291026242
At time: 952.0662577152252 and batch: 650, loss is 4.052953014373779 and perplexity is 57.56720306899056
At time: 953.6767456531525 and batch: 700, loss is 4.023941407203674 and perplexity is 55.921079785257334
At time: 955.284259557724 and batch: 750, loss is 4.003140392303467 and perplexity is 54.76987915055757
At time: 956.8940176963806 and batch: 800, loss is 3.95335663318634 and perplexity is 52.1099877158619
At time: 958.5052325725555 and batch: 850, loss is 3.988540835380554 and perplexity is 53.97607189907176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5498701731363935 and perplexity of 94.62012328363782
Finished 32 epochs...
Completing Train Step...
At time: 962.6797151565552 and batch: 50, loss is 4.075089473724365 and perplexity is 58.85574639559847
At time: 964.322761297226 and batch: 100, loss is 4.0412501621246335 and perplexity is 56.897429372534255
At time: 965.9413347244263 and batch: 150, loss is 4.03441545009613 and perplexity is 56.50987773816917
At time: 967.5555975437164 and batch: 200, loss is 4.074134149551392 and perplexity is 58.799546926977996
At time: 969.1669309139252 and batch: 250, loss is 4.054912157058716 and perplexity is 57.68009598433092
At time: 970.7817249298096 and batch: 300, loss is 4.036922554969788 and perplexity is 56.65173167510187
At time: 972.3996374607086 and batch: 350, loss is 3.999674735069275 and perplexity is 54.58039405751063
At time: 974.058970451355 and batch: 400, loss is 4.015579395294189 and perplexity is 55.455416703605295
At time: 975.6738433837891 and batch: 450, loss is 4.051811046600342 and perplexity is 57.5015007004128
At time: 977.289811372757 and batch: 500, loss is 4.034107565879822 and perplexity is 56.4924819168349
At time: 978.904369354248 and batch: 550, loss is 4.050488810539246 and perplexity is 57.425520385645385
At time: 980.5152494907379 and batch: 600, loss is 4.08880316734314 and perplexity is 59.66843581973389
At time: 982.1272480487823 and batch: 650, loss is 4.053176703453064 and perplexity is 57.58008166398892
At time: 983.7416875362396 and batch: 700, loss is 4.024240298271179 and perplexity is 55.93779659461843
At time: 985.3571782112122 and batch: 750, loss is 4.003445763587951 and perplexity is 54.78660685285479
At time: 986.9739339351654 and batch: 800, loss is 3.9536719703674317 and perplexity is 52.12642252361172
At time: 988.5885787010193 and batch: 850, loss is 3.9887687826156615 and perplexity is 53.98837699782646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.54987907409668 and perplexity of 94.6209654973457
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 992.7646043300629 and batch: 50, loss is 4.075011048316956 and perplexity is 58.851130790701745
At time: 994.3994636535645 and batch: 100, loss is 4.041399064064026 and perplexity is 56.90590214090445
At time: 996.0051279067993 and batch: 150, loss is 4.034535713195801 and perplexity is 56.51667419990157
At time: 997.6108121871948 and batch: 200, loss is 4.074267444610595 and perplexity is 58.8073851384526
At time: 999.2163178920746 and batch: 250, loss is 4.055129680633545 and perplexity is 57.69264412971515
At time: 1000.826251745224 and batch: 300, loss is 4.037147026062012 and perplexity is 56.664449778557255
At time: 1002.4384210109711 and batch: 350, loss is 3.999776911735535 and perplexity is 54.585971185139876
At time: 1004.0445878505707 and batch: 400, loss is 4.015292077064514 and perplexity is 55.4394856402042
At time: 1005.6509666442871 and batch: 450, loss is 4.051604390144348 and perplexity is 57.489618871834296
At time: 1007.257735490799 and batch: 500, loss is 4.033983664512634 and perplexity is 56.48548285469523
At time: 1008.8729078769684 and batch: 550, loss is 4.050263438224793 and perplexity is 57.41257972149587
At time: 1010.4810671806335 and batch: 600, loss is 4.088323798179626 and perplexity is 59.63983946621946
At time: 1012.0892577171326 and batch: 650, loss is 4.05246335029602 and perplexity is 57.539021377934596
At time: 1013.6996908187866 and batch: 700, loss is 4.023488969802856 and perplexity is 55.89578471991666
At time: 1015.3354504108429 and batch: 750, loss is 4.0028194189071655 and perplexity is 54.75230229743361
At time: 1016.9402697086334 and batch: 800, loss is 3.9530735301971434 and perplexity is 52.09523731061296
At time: 1018.5424015522003 and batch: 850, loss is 3.988213291168213 and perplexity is 53.95839524421736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.54987112681071 and perplexity of 94.62021352046224
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1022.6857297420502 and batch: 50, loss is 4.074971570968628 and perplexity is 58.84880754997001
At time: 1024.2981660366058 and batch: 100, loss is 4.041397666931152 and perplexity is 56.90582263585337
At time: 1025.9086515903473 and batch: 150, loss is 4.034511857032776 and perplexity is 56.51532594499036
At time: 1027.5170516967773 and batch: 200, loss is 4.074299149513244 and perplexity is 58.809249650430395
At time: 1029.1256973743439 and batch: 250, loss is 4.055181374549866 and perplexity is 57.695626565519355
At time: 1030.7386405467987 and batch: 300, loss is 4.037198286056519 and perplexity is 56.6673544723883
At time: 1032.3483152389526 and batch: 350, loss is 3.9998109579086303 and perplexity is 54.587829660200235
At time: 1033.9611763954163 and batch: 400, loss is 4.015168571472168 and perplexity is 55.43263897650017
At time: 1035.5723114013672 and batch: 450, loss is 4.0514969682693485 and perplexity is 57.483443560869745
At time: 1037.184522151947 and batch: 500, loss is 4.033909826278687 and perplexity is 56.48131222037559
At time: 1038.792368888855 and batch: 550, loss is 4.050165152549743 and perplexity is 57.40693716463741
At time: 1040.400592803955 and batch: 600, loss is 4.088135757446289 and perplexity is 59.62862580141604
At time: 1042.0154016017914 and batch: 650, loss is 4.052196974754334 and perplexity is 57.523696431133054
At time: 1043.6263434886932 and batch: 700, loss is 4.023204984664917 and perplexity is 55.87991340149872
At time: 1045.2338712215424 and batch: 750, loss is 4.002583618164063 and perplexity is 54.73939318591414
At time: 1046.84512925148 and batch: 800, loss is 3.9528321695327757 and perplexity is 52.08266508680646
At time: 1048.45609664917 and batch: 850, loss is 3.9879949426651002 and perplexity is 53.946614795553884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549872080485026 and perplexity of 94.62030375737271
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1052.6362237930298 and batch: 50, loss is 4.074958872795105 and perplexity is 58.84806028234463
At time: 1054.2472667694092 and batch: 100, loss is 4.041395149230957 and perplexity is 56.90567936423298
At time: 1055.885398387909 and batch: 150, loss is 4.034496421813965 and perplexity is 56.51445362530048
At time: 1057.4989304542542 and batch: 200, loss is 4.074305515289307 and perplexity is 58.80962401813565
At time: 1059.1111340522766 and batch: 250, loss is 4.055195298194885 and perplexity is 57.6964299045355
At time: 1060.7270920276642 and batch: 300, loss is 4.037212672233582 and perplexity is 56.66816970484745
At time: 1062.344714641571 and batch: 350, loss is 3.9998200130462647 and perplexity is 54.58832396274895
At time: 1063.9633495807648 and batch: 400, loss is 4.015119419097901 and perplexity is 55.42991439764287
At time: 1065.5793640613556 and batch: 450, loss is 4.051452317237854 and perplexity is 57.48087692312284
At time: 1067.1915559768677 and batch: 500, loss is 4.033875026702881 and perplexity is 56.4793467288687
At time: 1068.8030273914337 and batch: 550, loss is 4.050124468803406 and perplexity is 57.40460168287621
At time: 1070.4184746742249 and batch: 600, loss is 4.088062071800232 and perplexity is 59.624232189475435
At time: 1072.0337913036346 and batch: 650, loss is 4.052093472480774 and perplexity is 57.517742905875885
At time: 1073.6488251686096 and batch: 700, loss is 4.023095507621765 and perplexity is 55.87379616866229
At time: 1075.2619197368622 and batch: 750, loss is 4.002492246627807 and perplexity is 54.73439179196106
At time: 1076.8809881210327 and batch: 800, loss is 3.9527371549606323 and perplexity is 52.07771670975483
At time: 1078.5010917186737 and batch: 850, loss is 3.987909212112427 and perplexity is 53.9419901206934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549873987833659 and perplexity of 94.62048423145184
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1082.6963729858398 and batch: 50, loss is 4.074954023361206 and perplexity is 58.84777490325818
At time: 1084.3134169578552 and batch: 100, loss is 4.0413942670822145 and perplexity is 56.90562916498163
At time: 1085.92396402359 and batch: 150, loss is 4.034489846229553 and perplexity is 56.514082010961964
At time: 1087.5348207950592 and batch: 200, loss is 4.074307088851929 and perplexity is 58.8097165588346
At time: 1089.1532351970673 and batch: 250, loss is 4.055200095176697 and perplexity is 57.69670667392418
At time: 1090.7651312351227 and batch: 300, loss is 4.037217612266541 and perplexity is 56.668449648164966
At time: 1092.375275850296 and batch: 350, loss is 3.999822902679443 and perplexity is 54.58848170320894
At time: 1093.988474369049 and batch: 400, loss is 4.015100522041321 and perplexity is 55.428866945311185
At time: 1095.5994079113007 and batch: 450, loss is 4.051434679031372 and perplexity is 57.47986307248818
At time: 1097.2595422267914 and batch: 500, loss is 4.033860812187195 and perplexity is 56.47854390801454
At time: 1098.870941877365 and batch: 550, loss is 4.050108542442322 and perplexity is 57.403687443742186
At time: 1100.4827213287354 and batch: 600, loss is 4.088033418655396 and perplexity is 59.62252379219035
At time: 1102.0939915180206 and batch: 650, loss is 4.052053174972534 and perplexity is 57.5154251308578
At time: 1103.7018144130707 and batch: 700, loss is 4.023052845001221 and perplexity is 55.8714124969452
At time: 1105.3076331615448 and batch: 750, loss is 4.002456650733948 and perplexity is 54.732443507036066
At time: 1106.9133105278015 and batch: 800, loss is 3.9526999616622924 and perplexity is 52.07577980372057
At time: 1108.518138885498 and batch: 850, loss is 3.9878757190704346 and perplexity is 53.94018346960843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.549874941507976 and perplexity of 94.62057446862048
Annealing...
Model not improving. Stopping early with 94.62012328363782loss at 36 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.4086023050581945, 'tune_wordvecs': True, 'lr': 26.194614181991568, 'num_layers': 1, 'anneal': 3.9173863786566594, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.13213849067688 and batch: 50, loss is 6.9346394348144536 and perplexity is 1027.248795088779
At time: 3.737222909927368 and batch: 100, loss is 6.210831251144409 and perplexity is 498.115138019591
At time: 5.3381452560424805 and batch: 150, loss is 6.101107625961304 and perplexity is 446.3518873217338
At time: 6.936952590942383 and batch: 200, loss is 6.091351737976074 and perplexity is 442.0185006942612
At time: 8.53687047958374 and batch: 250, loss is 6.124511976242065 and perplexity is 456.92167021760014
At time: 10.141275644302368 and batch: 300, loss is 6.05579026222229 and perplexity is 426.57587909048857
At time: 11.74551272392273 and batch: 350, loss is 6.034966688156128 and perplexity is 417.78489222142196
At time: 13.346320629119873 and batch: 400, loss is 6.0475426769256595 and perplexity is 423.0721267515328
At time: 14.952419996261597 and batch: 450, loss is 6.041901988983154 and perplexity is 420.6924267894411
At time: 16.55725860595703 and batch: 500, loss is 6.050451850891113 and perplexity is 424.3047091972593
At time: 18.211188554763794 and batch: 550, loss is 6.019091501235962 and perplexity is 411.20484688203186
At time: 19.82066583633423 and batch: 600, loss is 6.044932765960693 and perplexity is 421.96938582272884
At time: 21.42727017402649 and batch: 650, loss is 6.07099720954895 and perplexity is 433.1123700879688
At time: 23.03110909461975 and batch: 700, loss is 6.029305047988892 and perplexity is 415.42622774952997
At time: 24.634504795074463 and batch: 750, loss is 6.021010646820068 and perplexity is 411.99476659110206
At time: 26.243639707565308 and batch: 800, loss is 6.043948478698731 and perplexity is 421.5542510707655
At time: 27.8485004901886 and batch: 850, loss is 6.02844557762146 and perplexity is 415.0693346084341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.476519266764323 and perplexity of 239.01331621599414
Finished 1 epochs...
Completing Train Step...
At time: 32.02249598503113 and batch: 50, loss is 5.8407946300506595 and perplexity is 344.0526266260931
At time: 33.61716938018799 and batch: 100, loss is 5.812598714828491 and perplexity is 334.487234138144
At time: 35.212016582489014 and batch: 150, loss is 5.819388208389282 and perplexity is 336.7659600042178
At time: 36.80852007865906 and batch: 200, loss is 5.813399229049683 and perplexity is 334.75510312804187
At time: 38.4019820690155 and batch: 250, loss is 5.857126636505127 and perplexity is 349.7178325094558
At time: 39.99968957901001 and batch: 300, loss is 5.801030588150025 and perplexity is 330.6401381898754
At time: 41.621294260025024 and batch: 350, loss is 5.782276830673218 and perplexity is 324.49717508698035
At time: 43.231428146362305 and batch: 400, loss is 5.8221571826934815 and perplexity is 337.6997485155654
At time: 44.83758306503296 and batch: 450, loss is 5.806571283340454 and perplexity is 332.4771990106292
At time: 46.45153856277466 and batch: 500, loss is 5.810076894760132 and perplexity is 333.6447802235496
At time: 48.06094694137573 and batch: 550, loss is 5.752711162567139 and perplexity is 315.04363800095416
At time: 49.67178726196289 and batch: 600, loss is 5.842103281021118 and perplexity is 344.5031661650204
At time: 51.2855122089386 and batch: 650, loss is 5.8072937393188475 and perplexity is 332.7174859386464
At time: 52.898000717163086 and batch: 700, loss is 5.783221988677979 and perplexity is 324.80402117571185
At time: 54.50646781921387 and batch: 750, loss is 5.808442277908325 and perplexity is 333.0998443453014
At time: 56.11889934539795 and batch: 800, loss is 5.784934511184693 and perplexity is 325.36073192603425
At time: 57.73427104949951 and batch: 850, loss is 5.767791500091553 and perplexity is 319.8306062258852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.52334467569987 and perplexity of 250.47138330601084
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 61.946842193603516 and batch: 50, loss is 5.719862623214722 and perplexity is 304.8630389757864
At time: 63.571004152297974 and batch: 100, loss is 5.610955400466919 and perplexity is 273.40532483866446
At time: 65.19203281402588 and batch: 150, loss is 5.598940496444702 and perplexity is 270.1400414157295
At time: 66.80860614776611 and batch: 200, loss is 5.615426034927368 and perplexity is 274.63035639936584
At time: 68.42858934402466 and batch: 250, loss is 5.648830842971802 and perplexity is 283.9592786858993
At time: 70.04922389984131 and batch: 300, loss is 5.613712024688721 and perplexity is 274.16004033507124
At time: 71.6651885509491 and batch: 350, loss is 5.588456497192383 and perplexity is 267.3226877956883
At time: 73.33004236221313 and batch: 400, loss is 5.569628067016602 and perplexity is 262.3365094923511
At time: 74.9476580619812 and batch: 450, loss is 5.571762619018554 and perplexity is 262.8970784828598
At time: 76.56480407714844 and batch: 500, loss is 5.607400722503662 and perplexity is 272.435182249204
At time: 78.17959022521973 and batch: 550, loss is 5.570241165161133 and perplexity is 262.49739683438946
At time: 79.79228973388672 and batch: 600, loss is 5.577034368515014 and perplexity is 264.286665583315
At time: 81.40407204627991 and batch: 650, loss is 5.561854724884033 and perplexity is 260.30518337518635
At time: 83.0198712348938 and batch: 700, loss is 5.531288928985596 and perplexity is 252.469116157111
At time: 84.63331031799316 and batch: 750, loss is 5.538134517669678 and perplexity is 254.20334501802964
At time: 86.24589419364929 and batch: 800, loss is 5.557305316925049 and perplexity is 259.12363860464575
At time: 87.86009526252747 and batch: 850, loss is 5.516215152740479 and perplexity is 248.6919924691837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.31104850769043 and perplexity of 202.56250542637872
Finished 3 epochs...
Completing Train Step...
At time: 92.03874969482422 and batch: 50, loss is 5.562547826766968 and perplexity is 260.4856639264013
At time: 93.68819737434387 and batch: 100, loss is 5.487957458496094 and perplexity is 241.76289146004052
At time: 95.29701828956604 and batch: 150, loss is 5.489119415283203 and perplexity is 242.04397276318252
At time: 96.91316986083984 and batch: 200, loss is 5.519426250457764 and perplexity is 249.49185028201947
At time: 98.52514982223511 and batch: 250, loss is 5.553710689544678 and perplexity is 258.1938577873759
At time: 100.13558149337769 and batch: 300, loss is 5.528637409210205 and perplexity is 251.80057601832834
At time: 101.74596905708313 and batch: 350, loss is 5.492522306442261 and perplexity is 242.86902504360455
At time: 103.35693383216858 and batch: 400, loss is 5.497557430267334 and perplexity is 244.09498449670858
At time: 104.96763920783997 and batch: 450, loss is 5.505797185897827 and perplexity is 246.11457656572088
At time: 106.57787275314331 and batch: 500, loss is 5.543581075668335 and perplexity is 255.5916556051724
At time: 108.19338917732239 and batch: 550, loss is 5.5068063545227055 and perplexity is 246.36307304084974
At time: 109.8076982498169 and batch: 600, loss is 5.519676704406738 and perplexity is 249.55434432677288
At time: 111.41706204414368 and batch: 650, loss is 5.5124164485931395 and perplexity is 247.7490772278672
At time: 113.02818179130554 and batch: 700, loss is 5.4789792346954345 and perplexity is 239.60200509000174
At time: 114.66544651985168 and batch: 750, loss is 5.492806978225708 and perplexity is 242.93817284385466
At time: 116.27377963066101 and batch: 800, loss is 5.510328207015991 and perplexity is 247.23225711453452
At time: 117.88219952583313 and batch: 850, loss is 5.48528263092041 and perplexity is 241.11708151136386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.302400271097819 and perplexity of 200.81825019197933
Finished 4 epochs...
Completing Train Step...
At time: 122.03261828422546 and batch: 50, loss is 5.522325658798218 and perplexity is 250.2162787330526
At time: 123.64475870132446 and batch: 100, loss is 5.442265424728394 and perplexity is 230.96482466014487
At time: 125.25999975204468 and batch: 150, loss is 5.432311687469483 and perplexity is 228.67726524908883
At time: 126.87148261070251 and batch: 200, loss is 5.459947032928467 and perplexity is 235.08497227292327
At time: 128.48100638389587 and batch: 250, loss is 5.492215890884399 and perplexity is 242.79461759619034
At time: 130.09251165390015 and batch: 300, loss is 5.470833129882813 and perplexity is 237.658110377827
At time: 131.70716857910156 and batch: 350, loss is 5.435030889511109 and perplexity is 229.29993112912473
At time: 133.32145285606384 and batch: 400, loss is 5.436438655853271 and perplexity is 229.62295917508558
At time: 134.936354637146 and batch: 450, loss is 5.452312202453613 and perplexity is 233.29697258498194
At time: 136.54850506782532 and batch: 500, loss is 5.48988037109375 and perplexity is 242.22822762667786
At time: 138.16189980506897 and batch: 550, loss is 5.462407445907592 and perplexity is 235.66409053279511
At time: 139.77851605415344 and batch: 600, loss is 5.475166320800781 and perplexity is 238.6901627684565
At time: 141.3909296989441 and batch: 650, loss is 5.471678829193115 and perplexity is 237.85918268923473
At time: 143.0018515586853 and batch: 700, loss is 5.445701560974121 and perplexity is 231.7598163332917
At time: 144.61176705360413 and batch: 750, loss is 5.4626351261138915 and perplexity is 235.71775269021862
At time: 146.22662663459778 and batch: 800, loss is 5.471300497055053 and perplexity is 237.7692099369554
At time: 147.83832669258118 and batch: 850, loss is 5.436862468719482 and perplexity is 229.72029696460834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.251766204833984 and perplexity of 190.9031449320306
Finished 5 epochs...
Completing Train Step...
At time: 152.05485320091248 and batch: 50, loss is 5.445500297546387 and perplexity is 231.71317625187413
At time: 153.67575478553772 and batch: 100, loss is 5.3934533977508545 and perplexity is 219.9616906435257
At time: 155.31835865974426 and batch: 150, loss is 5.402157468795776 and perplexity is 221.88460929618347
At time: 156.93619990348816 and batch: 200, loss is 5.422859468460083 and perplexity is 226.5259410665322
At time: 158.5532009601593 and batch: 250, loss is 5.451084098815918 and perplexity is 233.0106355860308
At time: 160.17199754714966 and batch: 300, loss is 5.431903562545776 and perplexity is 228.5839553999884
At time: 161.7893569469452 and batch: 350, loss is 5.39490198135376 and perplexity is 220.28055443653184
At time: 163.40234208106995 and batch: 400, loss is 5.400467720031738 and perplexity is 221.50999664163558
At time: 165.0156660079956 and batch: 450, loss is 5.416822052001953 and perplexity is 225.16242980611403
At time: 166.62936997413635 and batch: 500, loss is 5.451714582443238 and perplexity is 233.15759129847643
At time: 168.24463820457458 and batch: 550, loss is 5.417144565582276 and perplexity is 225.23505945890014
At time: 169.86091899871826 and batch: 600, loss is 5.431266622543335 and perplexity is 228.43840749244083
At time: 171.47330904006958 and batch: 650, loss is 5.433708829879761 and perplexity is 228.9969832484152
At time: 173.08746552467346 and batch: 700, loss is 5.395732717514038 and perplexity is 220.46362548985186
At time: 174.70134496688843 and batch: 750, loss is 5.395744199752808 and perplexity is 220.46615692037298
At time: 176.3173108100891 and batch: 800, loss is 5.41641674041748 and perplexity is 225.0711873569855
At time: 177.93138527870178 and batch: 850, loss is 5.40456784248352 and perplexity is 222.42007920218012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.2772674560546875 and perplexity of 195.83401849749322
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 182.12374448776245 and batch: 50, loss is 5.4086830520629885 and perplexity is 223.33727036532756
At time: 183.74047803878784 and batch: 100, loss is 5.330745620727539 and perplexity is 206.59195598767906
At time: 185.35194373130798 and batch: 150, loss is 5.3077003765106205 and perplexity is 201.88543368110783
At time: 186.96375823020935 and batch: 200, loss is 5.3249655437469485 and perplexity is 205.4012829842685
At time: 188.57655215263367 and batch: 250, loss is 5.335671129226685 and perplexity is 207.61203656822022
At time: 190.1897325515747 and batch: 300, loss is 5.311814384460449 and perplexity is 202.71770276719468
At time: 191.80639719963074 and batch: 350, loss is 5.268484468460083 and perplexity is 194.12154208047096
At time: 193.41943168640137 and batch: 400, loss is 5.277684297561645 and perplexity is 195.91566726099188
At time: 195.03143572807312 and batch: 450, loss is 5.294841213226318 and perplexity is 199.3059763009108
At time: 196.68808913230896 and batch: 500, loss is 5.3111633205413815 and perplexity is 202.58576354026232
At time: 198.29528713226318 and batch: 550, loss is 5.270985269546509 and perplexity is 194.6076089687836
At time: 199.9012553691864 and batch: 600, loss is 5.291797647476196 and perplexity is 198.70029763661395
At time: 201.51551747322083 and batch: 650, loss is 5.29607759475708 and perplexity is 199.55254692525295
At time: 203.12998342514038 and batch: 700, loss is 5.273322610855103 and perplexity is 195.06300537328156
At time: 204.7415874004364 and batch: 750, loss is 5.258112945556641 and perplexity is 192.11861073919752
At time: 206.35248160362244 and batch: 800, loss is 5.251032133102417 and perplexity is 190.76305975244432
At time: 207.96768975257874 and batch: 850, loss is 5.25794529914856 and perplexity is 192.08640544380805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.163469314575195 and perplexity of 174.7697362367777
Finished 7 epochs...
Completing Train Step...
At time: 212.12186884880066 and batch: 50, loss is 5.316636791229248 and perplexity is 203.6976509444293
At time: 213.76210641860962 and batch: 100, loss is 5.261225824356079 and perplexity is 192.7175844721906
At time: 215.3694851398468 and batch: 150, loss is 5.247931156158447 and perplexity is 190.17242414938346
At time: 216.97898149490356 and batch: 200, loss is 5.276131210327148 and perplexity is 195.61162929898327
At time: 218.59159588813782 and batch: 250, loss is 5.29153790473938 and perplexity is 198.6486933797053
At time: 220.20143103599548 and batch: 300, loss is 5.271346836090088 and perplexity is 194.6779852914078
At time: 221.8110692501068 and batch: 350, loss is 5.231387386322021 and perplexity is 187.0521371571964
At time: 223.41894245147705 and batch: 400, loss is 5.246804552078247 and perplexity is 189.95829576200094
At time: 225.03254961967468 and batch: 450, loss is 5.270469045639038 and perplexity is 194.5071737942076
At time: 226.64265489578247 and batch: 500, loss is 5.290558233261108 and perplexity is 198.45417821664108
At time: 228.24616932868958 and batch: 550, loss is 5.2561271858215335 and perplexity is 191.737487872087
At time: 229.85327982902527 and batch: 600, loss is 5.280586671829224 and perplexity is 196.4851138260534
At time: 231.46222043037415 and batch: 650, loss is 5.284214611053467 and perplexity is 197.19924450555575
At time: 233.07820391654968 and batch: 700, loss is 5.26252366065979 and perplexity is 192.96786272465283
At time: 234.69258546829224 and batch: 750, loss is 5.2523359680175785 and perplexity is 191.01194550798317
At time: 236.33215427398682 and batch: 800, loss is 5.255295343399048 and perplexity is 191.57805881481406
At time: 237.9473078250885 and batch: 850, loss is 5.262789764404297 and perplexity is 193.01921902824267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.160601298014323 and perplexity of 174.26921183813297
Finished 8 epochs...
Completing Train Step...
At time: 242.10439085960388 and batch: 50, loss is 5.2948086547851565 and perplexity is 199.29948731464444
At time: 243.74295496940613 and batch: 100, loss is 5.240220994949341 and perplexity is 188.71180215293623
At time: 245.35654187202454 and batch: 150, loss is 5.230018148422241 and perplexity is 186.79619354559966
At time: 246.97239708900452 and batch: 200, loss is 5.259810905456543 and perplexity is 192.445097538564
At time: 248.5805299282074 and batch: 250, loss is 5.274944639205932 and perplexity is 195.37965984003043
At time: 250.18956923484802 and batch: 300, loss is 5.254488439559936 and perplexity is 191.42353609453897
At time: 251.80132961273193 and batch: 350, loss is 5.215866565704346 and perplexity is 184.17134842342466
At time: 253.41166424751282 and batch: 400, loss is 5.234601640701294 and perplexity is 187.65433760244954
At time: 255.02361249923706 and batch: 450, loss is 5.2598802185058595 and perplexity is 192.45843695739288
At time: 256.64094138145447 and batch: 500, loss is 5.280294914245605 and perplexity is 196.42779616586407
At time: 258.25531911849976 and batch: 550, loss is 5.247437295913696 and perplexity is 190.07852873696214
At time: 259.86601424217224 and batch: 600, loss is 5.272337455749511 and perplexity is 194.87093268385576
At time: 261.4758651256561 and batch: 650, loss is 5.273967180252075 and perplexity is 195.18877754711926
At time: 263.0921857357025 and batch: 700, loss is 5.25108383178711 and perplexity is 190.77292220665734
At time: 264.7047529220581 and batch: 750, loss is 5.245985813140869 and perplexity is 189.80283315911015
At time: 266.32050037384033 and batch: 800, loss is 5.2526857376098635 and perplexity is 191.0787673637304
At time: 267.9305124282837 and batch: 850, loss is 5.258069477081299 and perplexity is 192.1102598176061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.159283955891927 and perplexity of 174.03979081091114
Finished 9 epochs...
Completing Train Step...
At time: 272.12853169441223 and batch: 50, loss is 5.279440078735352 and perplexity is 196.25995445924693
At time: 273.77233147621155 and batch: 100, loss is 5.227337961196899 and perplexity is 186.29621509114514
At time: 275.3821494579315 and batch: 150, loss is 5.217983436584473 and perplexity is 184.5616283281971
At time: 277.0208396911621 and batch: 200, loss is 5.2478171062469485 and perplexity is 190.15073623801538
At time: 278.6276090145111 and batch: 250, loss is 5.263308143615722 and perplexity is 193.11930211704765
At time: 280.24112820625305 and batch: 300, loss is 5.243007669448852 and perplexity is 189.23841392676735
At time: 281.86096024513245 and batch: 350, loss is 5.204873104095459 and perplexity is 182.15775622886878
At time: 283.4737763404846 and batch: 400, loss is 5.225670328140259 and perplexity is 185.98580026548308
At time: 285.08570551872253 and batch: 450, loss is 5.252227354049682 and perplexity is 190.99120006930858
At time: 286.69923162460327 and batch: 500, loss is 5.272763605117798 and perplexity is 194.95399450583085
At time: 288.31952357292175 and batch: 550, loss is 5.240300121307373 and perplexity is 188.72673482133445
At time: 289.93405199050903 and batch: 600, loss is 5.265695371627808 and perplexity is 193.58087264257784
At time: 291.55347084999084 and batch: 650, loss is 5.265701189041137 and perplexity is 193.58199878580234
At time: 293.17367482185364 and batch: 700, loss is 5.241306390762329 and perplexity is 188.91674035226671
At time: 294.7904751300812 and batch: 750, loss is 5.239078493118286 and perplexity is 188.4963216902855
At time: 296.4035687446594 and batch: 800, loss is 5.243560838699341 and perplexity is 189.34312375682185
At time: 298.0186188220978 and batch: 850, loss is 5.249563245773316 and perplexity is 190.48305600843466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.159166971842448 and perplexity of 174.01943212225584
Finished 10 epochs...
Completing Train Step...
At time: 302.21232414245605 and batch: 50, loss is 5.266636114120484 and perplexity is 193.7630680813008
At time: 303.8222768306732 and batch: 100, loss is 5.215800676345825 and perplexity is 184.15921389119188
At time: 305.4319977760315 and batch: 150, loss is 5.207147941589356 and perplexity is 182.57260720291694
At time: 307.0418772697449 and batch: 200, loss is 5.237057304382324 and perplexity is 188.11571981179935
At time: 308.6546185016632 and batch: 250, loss is 5.252150936126709 and perplexity is 190.9766054761444
At time: 310.2743227481842 and batch: 300, loss is 5.232515134811401 and perplexity is 187.2632039150496
At time: 311.8888211250305 and batch: 350, loss is 5.194191627502441 and perplexity is 180.2223970672933
At time: 313.50177025794983 and batch: 400, loss is 5.215844793319702 and perplexity is 184.16733861763814
At time: 315.11554527282715 and batch: 450, loss is 5.24308837890625 and perplexity is 189.2536878728418
At time: 316.72873425483704 and batch: 500, loss is 5.264250440597534 and perplexity is 193.30136361709174
At time: 318.3894383907318 and batch: 550, loss is 5.2325594997406 and perplexity is 187.27151201812563
At time: 319.9993190765381 and batch: 600, loss is 5.259128932952881 and perplexity is 192.31390001521459
At time: 321.61217069625854 and batch: 650, loss is 5.258693323135376 and perplexity is 192.2301444360261
At time: 323.2265100479126 and batch: 700, loss is 5.236077327728271 and perplexity is 187.93146109748875
At time: 324.8376462459564 and batch: 750, loss is 5.234593591690063 and perplexity is 187.65282717665738
At time: 326.45190167427063 and batch: 800, loss is 5.240290021896362 and perplexity is 188.7248288020955
At time: 328.06558895111084 and batch: 850, loss is 5.244278059005738 and perplexity is 189.47897320122348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1583404541015625 and perplexity of 173.87566139705996
Finished 11 epochs...
Completing Train Step...
At time: 332.27001547813416 and batch: 50, loss is 5.256165571212769 and perplexity is 191.74484793183194
At time: 333.88548040390015 and batch: 100, loss is 5.205755109786987 and perplexity is 182.31849128081396
At time: 335.499436378479 and batch: 150, loss is 5.1980125427246096 and perplexity is 180.91232881359738
At time: 337.11103796958923 and batch: 200, loss is 5.228556346893311 and perplexity is 186.52333406606715
At time: 338.72396445274353 and batch: 250, loss is 5.243250684738159 and perplexity is 189.28440734300108
At time: 340.33799934387207 and batch: 300, loss is 5.224149208068848 and perplexity is 185.7031085902003
At time: 341.94802021980286 and batch: 350, loss is 5.185113077163696 and perplexity is 178.5936435107422
At time: 343.55686831474304 and batch: 400, loss is 5.208028326034546 and perplexity is 182.73341206107773
At time: 345.16669058799744 and batch: 450, loss is 5.2360317993164065 and perplexity is 187.92290507129823
At time: 346.77070474624634 and batch: 500, loss is 5.25721809387207 and perplexity is 191.94676997420612
At time: 348.375629901886 and batch: 550, loss is 5.225970306396484 and perplexity is 186.04160033051406
At time: 349.993625164032 and batch: 600, loss is 5.252779407501221 and perplexity is 191.0966665294034
At time: 351.6048390865326 and batch: 650, loss is 5.250828475952148 and perplexity is 190.72421344711685
At time: 353.2148230075836 and batch: 700, loss is 5.226769456863403 and perplexity is 186.19033498505314
At time: 354.8273391723633 and batch: 750, loss is 5.227137956619263 and perplexity is 186.258958721177
At time: 356.44259548187256 and batch: 800, loss is 5.234858713150024 and perplexity is 187.70258456374836
At time: 358.0582983493805 and batch: 850, loss is 5.2383309173583985 and perplexity is 188.35545906866295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.158483187357585 and perplexity of 173.900481007604
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 362.2962055206299 and batch: 50, loss is 5.252450704574585 and perplexity is 191.03386281829216
At time: 363.91262769699097 and batch: 100, loss is 5.20308214187622 and perplexity is 181.8318105348572
At time: 365.5245940685272 and batch: 150, loss is 5.186750659942627 and perplexity is 178.88634498181514
At time: 367.14026498794556 and batch: 200, loss is 5.215317897796631 and perplexity is 184.0703272311015
At time: 368.7570083141327 and batch: 250, loss is 5.223065557479859 and perplexity is 185.5019803032747
At time: 370.37099409103394 and batch: 300, loss is 5.191858997344971 and perplexity is 179.80249479758768
At time: 371.9854156970978 and batch: 350, loss is 5.153838615417481 and perplexity is 173.0946605073869
At time: 373.60331630706787 and batch: 400, loss is 5.174991912841797 and perplexity is 176.7951845000244
At time: 375.223210811615 and batch: 450, loss is 5.206655397415161 and perplexity is 182.4827042712717
At time: 376.84274458885193 and batch: 500, loss is 5.221369457244873 and perplexity is 185.1876170220718
At time: 378.4634413719177 and batch: 550, loss is 5.180805387496949 and perplexity is 177.82597215012558
At time: 380.08476161956787 and batch: 600, loss is 5.197765159606933 and perplexity is 180.86757969298614
At time: 381.7004334926605 and batch: 650, loss is 5.190946455001831 and perplexity is 179.63849224869873
At time: 383.3219816684723 and batch: 700, loss is 5.166040363311768 and perplexity is 175.21965588114537
At time: 384.93716406822205 and batch: 750, loss is 5.15892876625061 and perplexity is 173.97798465714075
At time: 386.5553023815155 and batch: 800, loss is 5.162898120880127 and perplexity is 174.66993737030546
At time: 388.1742408275604 and batch: 850, loss is 5.1831761074066165 and perplexity is 178.2480478366916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.132932345072429 and perplexity of 169.5134619282554
Finished 13 epochs...
Completing Train Step...
At time: 392.3659813404083 and batch: 50, loss is 5.226766223907471 and perplexity is 186.18973304087814
At time: 394.0114607810974 and batch: 100, loss is 5.17917841911316 and perplexity is 177.536890143031
At time: 395.62400460243225 and batch: 150, loss is 5.163651542663574 and perplexity is 174.8015870936909
At time: 397.2375087738037 and batch: 200, loss is 5.195986194610596 and perplexity is 180.54610862740685
At time: 398.8530728816986 and batch: 250, loss is 5.2060391330719 and perplexity is 182.3702813320499
At time: 400.492577791214 and batch: 300, loss is 5.178499898910522 and perplexity is 177.4164686351886
At time: 402.1036913394928 and batch: 350, loss is 5.140775461196899 and perplexity is 170.8482031177412
At time: 403.7180001735687 and batch: 400, loss is 5.163471755981445 and perplexity is 174.77016292122485
At time: 405.3323221206665 and batch: 450, loss is 5.197015800476074 and perplexity is 180.73209569009407
At time: 406.9437325000763 and batch: 500, loss is 5.213100681304931 and perplexity is 183.66265558109615
At time: 408.55244874954224 and batch: 550, loss is 5.175766725540161 and perplexity is 176.93222073583672
At time: 410.1649851799011 and batch: 600, loss is 5.195959224700927 and perplexity is 180.5412393808281
At time: 411.77444887161255 and batch: 650, loss is 5.190336580276489 and perplexity is 179.52896867380255
At time: 413.3836278915405 and batch: 700, loss is 5.167380914688111 and perplexity is 175.45470434406934
At time: 414.9984953403473 and batch: 750, loss is 5.164085302352905 and perplexity is 174.87742542241912
At time: 416.61083340644836 and batch: 800, loss is 5.169445819854737 and perplexity is 175.81737598198342
At time: 418.2225956916809 and batch: 850, loss is 5.187448120117187 and perplexity is 179.01115460302324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.132073084513347 and perplexity of 169.36786825659485
Finished 14 epochs...
Completing Train Step...
At time: 422.3946306705475 and batch: 50, loss is 5.21879301071167 and perplexity is 184.71110514548278
At time: 424.03705739974976 and batch: 100, loss is 5.17091968536377 and perplexity is 176.0766982044283
At time: 425.64790558815 and batch: 150, loss is 5.154926271438598 and perplexity is 173.2830303793537
At time: 427.25879883766174 and batch: 200, loss is 5.188578491210937 and perplexity is 179.21361804549886
At time: 428.8689455986023 and batch: 250, loss is 5.199042778015137 and perplexity is 181.09880712100554
At time: 430.4799449443817 and batch: 300, loss is 5.172459077835083 and perplexity is 176.34795808215702
At time: 432.09309124946594 and batch: 350, loss is 5.1353437614440915 and perplexity is 169.92272271633024
At time: 433.7073097229004 and batch: 400, loss is 5.159338445663452 and perplexity is 174.04927445772725
At time: 435.31788778305054 and batch: 450, loss is 5.19385817527771 and perplexity is 180.16231152643024
At time: 436.92920780181885 and batch: 500, loss is 5.210336818695068 and perplexity is 183.1557380823604
At time: 438.5421919822693 and batch: 550, loss is 5.174329833984375 and perplexity is 176.67817088666698
At time: 440.2035849094391 and batch: 600, loss is 5.1953920650482175 and perplexity is 180.43887270606893
At time: 441.8139855861664 and batch: 650, loss is 5.190430717468262 and perplexity is 179.54586982225615
At time: 443.4252781867981 and batch: 700, loss is 5.168115978240967 and perplexity is 175.58372211473102
At time: 445.03952646255493 and batch: 750, loss is 5.166097211837768 and perplexity is 175.22961714344737
At time: 446.65030431747437 and batch: 800, loss is 5.171316890716553 and perplexity is 176.14665070328928
At time: 448.2620882987976 and batch: 850, loss is 5.187889709472656 and perplexity is 179.09022147966684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131786664326985 and perplexity of 169.31936482671748
Finished 15 epochs...
Completing Train Step...
At time: 452.44197273254395 and batch: 50, loss is 5.213329420089722 and perplexity is 183.7046711588581
At time: 454.0768098831177 and batch: 100, loss is 5.1658978843688965 and perplexity is 175.194692548222
At time: 455.6881673336029 and batch: 150, loss is 5.149632091522217 and perplexity is 172.36806297817319
At time: 457.29735946655273 and batch: 200, loss is 5.183954772949218 and perplexity is 178.38689750128717
At time: 458.9109115600586 and batch: 250, loss is 5.19427975654602 and perplexity is 180.23828059466814
At time: 460.52468037605286 and batch: 300, loss is 5.16831976890564 and perplexity is 175.61950808446602
At time: 462.1412808895111 and batch: 350, loss is 5.131890478134156 and perplexity is 169.33694342704274
At time: 463.7554953098297 and batch: 400, loss is 5.157066669464111 and perplexity is 173.65432224987575
At time: 465.37108635902405 and batch: 450, loss is 5.191927843093872 and perplexity is 179.81487386111434
At time: 466.9846119880676 and batch: 500, loss is 5.208641738891601 and perplexity is 182.84553747151799
At time: 468.5971930027008 and batch: 550, loss is 5.173044595718384 and perplexity is 176.45124319998868
At time: 470.2061598300934 and batch: 600, loss is 5.194798564910888 and perplexity is 180.3318139831643
At time: 471.81482672691345 and batch: 650, loss is 5.190047092437744 and perplexity is 179.47700474249504
At time: 473.4226768016815 and batch: 700, loss is 5.167733478546142 and perplexity is 175.51657423744598
At time: 475.03198409080505 and batch: 750, loss is 5.16652403831482 and perplexity is 175.30442574761852
At time: 476.64593601226807 and batch: 800, loss is 5.17161374092102 and perplexity is 176.1989476343601
At time: 478.2602593898773 and batch: 850, loss is 5.1875129985809325 and perplexity is 179.02276894848356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131731669108073 and perplexity of 169.3100533272291
Finished 16 epochs...
Completing Train Step...
At time: 482.46017503738403 and batch: 50, loss is 5.209061899185181 and perplexity is 182.92237804777082
At time: 484.0809910297394 and batch: 100, loss is 5.161936063766479 and perplexity is 174.50197572182216
At time: 485.6955316066742 and batch: 150, loss is 5.145826053619385 and perplexity is 171.71327047023178
At time: 487.31031799316406 and batch: 200, loss is 5.180504913330078 and perplexity is 177.7725480659764
At time: 488.92415595054626 and batch: 250, loss is 5.1906609535217285 and perplexity is 179.58721251384674
At time: 490.53833413124084 and batch: 300, loss is 5.164974727630615 and perplexity is 175.03303501638553
At time: 492.1535575389862 and batch: 350, loss is 5.129055328369141 and perplexity is 168.85752776041605
At time: 493.7660639286041 and batch: 400, loss is 5.1551675128936765 and perplexity is 173.3248384724626
At time: 495.3858127593994 and batch: 450, loss is 5.19026665687561 and perplexity is 179.5164158366302
At time: 497.0004732608795 and batch: 500, loss is 5.207064504623413 and perplexity is 182.5573745340004
At time: 498.61796379089355 and batch: 550, loss is 5.171602754592896 and perplexity is 176.1970118655397
At time: 500.2319781780243 and batch: 600, loss is 5.193884792327881 and perplexity is 180.1671069795352
At time: 501.84861850738525 and batch: 650, loss is 5.1892584037780765 and perplexity is 179.33550906953434
At time: 503.4690284729004 and batch: 700, loss is 5.166472635269165 and perplexity is 175.29541479781548
At time: 505.08626103401184 and batch: 750, loss is 5.166026191711426 and perplexity is 175.2171727558051
At time: 506.7057149410248 and batch: 800, loss is 5.171306133270264 and perplexity is 176.14475582534735
At time: 508.3237326145172 and batch: 850, loss is 5.1866139221191405 and perplexity is 178.86188612461447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131429990132649 and perplexity of 169.25898374750895
Finished 17 epochs...
Completing Train Step...
At time: 512.5435082912445 and batch: 50, loss is 5.205430488586426 and perplexity is 182.2593164385137
At time: 514.1536185741425 and batch: 100, loss is 5.158589010238647 and perplexity is 173.91888463126742
At time: 515.7647426128387 and batch: 150, loss is 5.1424111557006835 and perplexity is 171.12788726111256
At time: 517.375009059906 and batch: 200, loss is 5.177457218170166 and perplexity is 177.23157630890546
At time: 518.9862372875214 and batch: 250, loss is 5.187439966201782 and perplexity is 179.00969496716294
At time: 520.5989601612091 and batch: 300, loss is 5.16195704460144 and perplexity is 174.5056369573829
At time: 522.2339701652527 and batch: 350, loss is 5.126539268493652 and perplexity is 168.43320614329076
At time: 523.8418855667114 and batch: 400, loss is 5.153016138076782 and perplexity is 172.9523526018597
At time: 525.4524734020233 and batch: 450, loss is 5.188588085174561 and perplexity is 179.21533742267906
At time: 527.0686237812042 and batch: 500, loss is 5.205602340698242 and perplexity is 182.29064077844166
At time: 528.6816267967224 and batch: 550, loss is 5.170334787368774 and perplexity is 175.97374140922983
At time: 530.2970552444458 and batch: 600, loss is 5.192875003814697 and perplexity is 179.98526812929606
At time: 531.9097664356232 and batch: 650, loss is 5.1882147789001465 and perplexity is 179.1484476987015
At time: 533.522442817688 and batch: 700, loss is 5.165326166152954 and perplexity is 175.0945591779191
At time: 535.1353206634521 and batch: 750, loss is 5.1652432060241695 and perplexity is 175.08003391325752
At time: 536.7462341785431 and batch: 800, loss is 5.170497741699219 and perplexity is 176.00241942897725
At time: 538.3577971458435 and batch: 850, loss is 5.185490322113037 and perplexity is 178.66102977050497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131359100341797 and perplexity of 169.246985438835
Finished 18 epochs...
Completing Train Step...
At time: 542.5459232330322 and batch: 50, loss is 5.202326135635376 and perplexity is 181.69439650078738
At time: 544.1537780761719 and batch: 100, loss is 5.155677947998047 and perplexity is 173.41333213769354
At time: 545.7634892463684 and batch: 150, loss is 5.139491834640503 and perplexity is 170.62903851994878
At time: 547.3788759708405 and batch: 200, loss is 5.174837760925293 and perplexity is 176.7679332839725
At time: 548.9929685592651 and batch: 250, loss is 5.184494361877442 and perplexity is 178.48317907001714
At time: 550.602000951767 and batch: 300, loss is 5.159182567596435 and perplexity is 174.0221461076714
At time: 552.2133803367615 and batch: 350, loss is 5.124183931350708 and perplexity is 168.03695599129358
At time: 553.8236110210419 and batch: 400, loss is 5.151118993759155 and perplexity is 172.62454807346836
At time: 555.4375920295715 and batch: 450, loss is 5.187080612182617 and perplexity is 178.94537867065898
At time: 557.0512664318085 and batch: 500, loss is 5.204049167633056 and perplexity is 182.0077316254602
At time: 558.6601769924164 and batch: 550, loss is 5.168964319229126 and perplexity is 175.73274018318506
At time: 560.268482208252 and batch: 600, loss is 5.191626052856446 and perplexity is 179.76061567534626
At time: 561.8772237300873 and batch: 650, loss is 5.187034864425659 and perplexity is 178.9371925082174
At time: 563.5313844680786 and batch: 700, loss is 5.163947057723999 and perplexity is 174.85325122865225
At time: 565.1388819217682 and batch: 750, loss is 5.164272403717041 and perplexity is 174.9101482884231
At time: 566.7482352256775 and batch: 800, loss is 5.169525899887085 and perplexity is 175.83145600689636
At time: 568.3510642051697 and batch: 850, loss is 5.184201345443726 and perplexity is 178.43088822682012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131349563598633 and perplexity of 169.2453713815
Finished 19 epochs...
Completing Train Step...
At time: 572.521162033081 and batch: 50, loss is 5.199452114105225 and perplexity is 181.17295257280134
At time: 574.1614735126495 and batch: 100, loss is 5.1529865550994876 and perplexity is 172.94723623201875
At time: 575.7744793891907 and batch: 150, loss is 5.137039880752564 and perplexity is 170.21117648412772
At time: 577.3909330368042 and batch: 200, loss is 5.172403745651245 and perplexity is 176.33820063447382
At time: 579.0057709217072 and batch: 250, loss is 5.1819216251373295 and perplexity is 178.0245790192866
At time: 580.6181678771973 and batch: 300, loss is 5.156718921661377 and perplexity is 173.59394483954242
At time: 582.2324335575104 and batch: 350, loss is 5.122112970352173 and perplexity is 167.68931810562262
At time: 583.8496029376984 and batch: 400, loss is 5.1495169830322265 and perplexity is 172.34822309261276
At time: 585.4654664993286 and batch: 450, loss is 5.1855463027954105 and perplexity is 178.6710316168175
At time: 587.0812740325928 and batch: 500, loss is 5.202612705230713 and perplexity is 181.74647205174603
At time: 588.6984367370605 and batch: 550, loss is 5.167568531036377 and perplexity is 175.48762560317113
At time: 590.3123440742493 and batch: 600, loss is 5.190542163848877 and perplexity is 179.56588067465015
At time: 591.9255380630493 and batch: 650, loss is 5.185777750015259 and perplexity is 178.71238931623026
At time: 593.5379545688629 and batch: 700, loss is 5.162450742721558 and perplexity is 174.5918113326119
At time: 595.1621515750885 and batch: 750, loss is 5.1631234264373775 and perplexity is 174.70929591156224
At time: 596.7780005931854 and batch: 800, loss is 5.168486261367798 and perplexity is 175.64874984296867
At time: 598.39026927948 and batch: 850, loss is 5.18298602104187 and perplexity is 178.214168533354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.131394068400065 and perplexity of 169.25290378075945
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 602.5617866516113 and batch: 50, loss is 5.19895770072937 and perplexity is 181.08340038143103
At time: 604.2053654193878 and batch: 100, loss is 5.153374614715577 and perplexity is 173.01436309388433
At time: 605.825243473053 and batch: 150, loss is 5.135987634658814 and perplexity is 170.0321666362832
At time: 607.4351646900177 and batch: 200, loss is 5.171075220108032 and perplexity is 176.10408637850526
At time: 609.0446364879608 and batch: 250, loss is 5.176978321075439 and perplexity is 177.14672094203337
At time: 610.6551365852356 and batch: 300, loss is 5.149006433486939 and perplexity is 172.2602532440756
At time: 612.2663950920105 and batch: 350, loss is 5.112756710052491 and perplexity is 166.12769008707514
At time: 613.8796145915985 and batch: 400, loss is 5.1398231887817385 and perplexity is 170.6855865266681
At time: 615.4910905361176 and batch: 450, loss is 5.17448733329773 and perplexity is 176.7059997687238
At time: 617.0982112884521 and batch: 500, loss is 5.190123147964478 and perplexity is 179.4906554797277
At time: 618.7121875286102 and batch: 550, loss is 5.152193040847778 and perplexity is 172.8100545702749
At time: 620.3238587379456 and batch: 600, loss is 5.170159511566162 and perplexity is 175.94290017340575
At time: 621.935418844223 and batch: 650, loss is 5.16372465133667 and perplexity is 174.81436707293634
At time: 623.5511105060577 and batch: 700, loss is 5.141302642822265 and perplexity is 170.93829489643034
At time: 625.1630201339722 and batch: 750, loss is 5.1394906425476075 and perplexity is 170.6288351144054
At time: 626.7801947593689 and batch: 800, loss is 5.143515634536743 and perplexity is 171.31699880647744
At time: 628.3954999446869 and batch: 850, loss is 5.164934024810791 and perplexity is 175.02591082328638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.12701956431071 and perplexity of 168.5141233381946
Finished 21 epochs...
Completing Train Step...
At time: 632.5819985866547 and batch: 50, loss is 5.1927579975128175 and perplexity is 179.9642099506733
At time: 634.2156066894531 and batch: 100, loss is 5.147471694946289 and perplexity is 171.99608156348265
At time: 635.8193473815918 and batch: 150, loss is 5.129696578979492 and perplexity is 168.96584247788252
At time: 637.4235157966614 and batch: 200, loss is 5.16556336402893 and perplexity is 175.13609616149853
At time: 639.0347480773926 and batch: 250, loss is 5.172704982757568 and perplexity is 176.39132824536935
At time: 640.650710105896 and batch: 300, loss is 5.145076999664306 and perplexity is 171.5846961264187
At time: 642.2583334445953 and batch: 350, loss is 5.1098060131073 and perplexity is 165.63822011333716
At time: 643.8673431873322 and batch: 400, loss is 5.136614007949829 and perplexity is 170.13870360655073
At time: 645.5025765895844 and batch: 450, loss is 5.172179288864136 and perplexity is 176.2986247702167
At time: 647.1163976192474 and batch: 500, loss is 5.187912044525146 and perplexity is 179.09422151383438
At time: 648.7269582748413 and batch: 550, loss is 5.151298131942749 and perplexity is 172.65547449142238
At time: 650.333331823349 and batch: 600, loss is 5.170615797042847 and perplexity is 176.02319868160907
At time: 651.942019701004 and batch: 650, loss is 5.165243968963623 and perplexity is 175.08016748877392
At time: 653.5526251792908 and batch: 700, loss is 5.1433994102478025 and perplexity is 171.29708876714523
At time: 655.1657872200012 and batch: 750, loss is 5.141615142822266 and perplexity is 170.9917214610515
At time: 656.777783870697 and batch: 800, loss is 5.145683345794677 and perplexity is 171.68876739136167
At time: 658.3968677520752 and batch: 850, loss is 5.166589088439942 and perplexity is 175.31582969335787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.12690544128418 and perplexity of 168.49489309375386
Finished 22 epochs...
Completing Train Step...
At time: 662.5868389606476 and batch: 50, loss is 5.190228204727173 and perplexity is 179.50951317747348
At time: 664.1997859477997 and batch: 100, loss is 5.144747676849366 and perplexity is 171.5281986747501
At time: 665.8095927238464 and batch: 150, loss is 5.1267511177062985 and perplexity is 168.46889236531126
At time: 667.4188888072968 and batch: 200, loss is 5.162844104766846 and perplexity is 174.66050263399788
At time: 669.0281076431274 and batch: 250, loss is 5.170251712799073 and perplexity is 175.95912307359785
At time: 670.6383123397827 and batch: 300, loss is 5.142953386306763 and perplexity is 171.22070320068673
At time: 672.2521214485168 and batch: 350, loss is 5.10809066772461 and perplexity is 165.35433690575175
At time: 673.8641266822815 and batch: 400, loss is 5.135083684921264 and perplexity is 169.878535551728
At time: 675.4730887413025 and batch: 450, loss is 5.170987520217896 and perplexity is 176.08864274668954
At time: 677.0843334197998 and batch: 500, loss is 5.186975288391113 and perplexity is 178.92653245739996
At time: 678.6964616775513 and batch: 550, loss is 5.150962266921997 and perplexity is 172.59749529403663
At time: 680.3080794811249 and batch: 600, loss is 5.17089656829834 and perplexity is 176.07262787492246
At time: 681.9211754798889 and batch: 650, loss is 5.1663358116149904 and perplexity is 175.27143187935548
At time: 683.5355927944183 and batch: 700, loss is 5.144786434173584 and perplexity is 171.53484677758925
At time: 685.1937313079834 and batch: 750, loss is 5.142805681228638 and perplexity is 171.195414900995
At time: 686.8077945709229 and batch: 800, loss is 5.14682752609253 and perplexity is 171.8853227222986
At time: 688.4198613166809 and batch: 850, loss is 5.167380361557007 and perplexity is 175.45460729464196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126873970031738 and perplexity of 168.48959043187935
Finished 23 epochs...
Completing Train Step...
At time: 692.6208388805389 and batch: 50, loss is 5.188445539474487 and perplexity is 179.18979286761865
At time: 694.2360513210297 and batch: 100, loss is 5.142830438613892 and perplexity is 171.19965330430108
At time: 695.8541181087494 and batch: 150, loss is 5.124656896591187 and perplexity is 168.1164504281642
At time: 697.4688489437103 and batch: 200, loss is 5.161042919158936 and perplexity is 174.3461898032331
At time: 699.0819211006165 and batch: 250, loss is 5.168542556762695 and perplexity is 175.65863833703992
At time: 700.6974883079529 and batch: 300, loss is 5.141429214477539 and perplexity is 170.95993220867143
At time: 702.3137624263763 and batch: 350, loss is 5.106875019073486 and perplexity is 165.15344626009582
At time: 703.9262361526489 and batch: 400, loss is 5.134108047485352 and perplexity is 169.71287651760386
At time: 705.537543296814 and batch: 450, loss is 5.170243282318115 and perplexity is 175.9576396598144
At time: 707.1486415863037 and batch: 500, loss is 5.186378221511841 and perplexity is 178.81973323735792
At time: 708.7537434101105 and batch: 550, loss is 5.150748348236084 and perplexity is 172.56057741350438
At time: 710.3636569976807 and batch: 600, loss is 5.171118259429932 and perplexity is 176.11166594207515
At time: 711.9815747737885 and batch: 650, loss is 5.167146425247193 and perplexity is 175.41356689187887
At time: 713.5956361293793 and batch: 700, loss is 5.145716829299927 and perplexity is 171.69451622935097
At time: 715.2085182666779 and batch: 750, loss is 5.143517971038818 and perplexity is 171.31739908946835
At time: 716.8251972198486 and batch: 800, loss is 5.147520122528076 and perplexity is 172.00441111947802
At time: 718.4416060447693 and batch: 850, loss is 5.167885036468506 and perplexity is 175.5431771806702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126873016357422 and perplexity of 168.489429747761
Finished 24 epochs...
Completing Train Step...
At time: 722.6322107315063 and batch: 50, loss is 5.187063846588135 and perplexity is 178.942378570155
At time: 724.2440557479858 and batch: 100, loss is 5.141295509338379 and perplexity is 170.93707551520737
At time: 725.8538813591003 and batch: 150, loss is 5.123062152862548 and perplexity is 167.84856143708194
At time: 727.5011763572693 and batch: 200, loss is 5.159621553421021 and perplexity is 174.09855613320858
At time: 729.1135997772217 and batch: 250, loss is 5.167128572463989 and perplexity is 175.41043529945208
At time: 730.7274513244629 and batch: 300, loss is 5.140196123123169 and perplexity is 170.7492529144244
At time: 732.3398404121399 and batch: 350, loss is 5.10590503692627 and perplexity is 164.9933280341367
At time: 733.9519381523132 and batch: 400, loss is 5.133377437591553 and perplexity is 169.58892789547335
At time: 735.5670201778412 and batch: 450, loss is 5.169589424133301 and perplexity is 175.84262592237678
At time: 737.17795753479 and batch: 500, loss is 5.185887174606323 and perplexity is 178.73194591631633
At time: 738.7858753204346 and batch: 550, loss is 5.1504853820800784 and perplexity is 172.5152057876464
At time: 740.3959650993347 and batch: 600, loss is 5.171155080795288 and perplexity is 176.1181507334591
At time: 742.0145788192749 and batch: 650, loss is 5.167652263641357 and perplexity is 175.5023202544068
At time: 743.6277594566345 and batch: 700, loss is 5.146337900161743 and perplexity is 171.80118381114946
At time: 745.2368030548096 and batch: 750, loss is 5.1438812732696535 and perplexity is 171.3796503900719
At time: 746.8535749912262 and batch: 800, loss is 5.147889261245727 and perplexity is 172.06791632762352
At time: 748.4681768417358 and batch: 850, loss is 5.16814923286438 and perplexity is 175.5895611823749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126802126566569 and perplexity of 168.47748599067555
Finished 25 epochs...
Completing Train Step...
At time: 752.651585817337 and batch: 50, loss is 5.185822525024414 and perplexity is 178.7203913442422
At time: 754.2872714996338 and batch: 100, loss is 5.139966325759888 and perplexity is 170.71001969434033
At time: 755.896125793457 and batch: 150, loss is 5.1216388702392575 and perplexity is 167.60983542383696
At time: 757.5046594142914 and batch: 200, loss is 5.158367557525635 and perplexity is 173.88037408671136
At time: 759.116133928299 and batch: 250, loss is 5.165893154144287 and perplexity is 175.1938638399359
At time: 760.7294685840607 and batch: 300, loss is 5.138916625976562 and perplexity is 170.53091944084974
At time: 762.3395547866821 and batch: 350, loss is 5.104993314743042 and perplexity is 164.84296851035535
At time: 763.9513676166534 and batch: 400, loss is 5.132702741622925 and perplexity is 169.47454552050104
At time: 765.5647602081299 and batch: 450, loss is 5.168992223739624 and perplexity is 175.73764398769728
At time: 767.2005667686462 and batch: 500, loss is 5.185314197540283 and perplexity is 178.6295659438266
At time: 768.808426618576 and batch: 550, loss is 5.150175399780274 and perplexity is 172.46173741495778
At time: 770.4162876605988 and batch: 600, loss is 5.1711087417602535 and perplexity is 176.10998977738885
At time: 772.0195560455322 and batch: 650, loss is 5.167990484237671 and perplexity is 175.56168879308305
At time: 773.623651266098 and batch: 700, loss is 5.146700439453125 and perplexity is 171.86347978227312
At time: 775.233510017395 and batch: 750, loss is 5.144023866653442 and perplexity is 171.4040897367367
At time: 776.8493120670319 and batch: 800, loss is 5.1480919551849365 and perplexity is 172.1027969863251
At time: 778.4613215923309 and batch: 850, loss is 5.168255500793457 and perplexity is 175.60822171289968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126740137736003 and perplexity of 168.46704259203258
Finished 26 epochs...
Completing Train Step...
At time: 782.6546349525452 and batch: 50, loss is 5.184758453369141 and perplexity is 178.53032118366252
At time: 784.2987885475159 and batch: 100, loss is 5.138824672698974 and perplexity is 170.515239284809
At time: 785.9184267520905 and batch: 150, loss is 5.120342130661011 and perplexity is 167.39262997642612
At time: 787.5309188365936 and batch: 200, loss is 5.157344846725464 and perplexity is 173.70263565319487
At time: 789.1434438228607 and batch: 250, loss is 5.164747228622437 and perplexity is 174.99321970366464
At time: 790.7605881690979 and batch: 300, loss is 5.137963056564331 and perplexity is 170.36838387901744
At time: 792.3753023147583 and batch: 350, loss is 5.104239807128907 and perplexity is 164.71880486344756
At time: 793.9911315441132 and batch: 400, loss is 5.132154397964477 and perplexity is 169.3816407024084
At time: 795.6083471775055 and batch: 450, loss is 5.168507604598999 and perplexity is 175.65249879485395
At time: 797.2222857475281 and batch: 500, loss is 5.184873609542847 and perplexity is 178.55088123612632
At time: 798.8403630256653 and batch: 550, loss is 5.149892377853393 and perplexity is 172.41293386828414
At time: 800.455274105072 and batch: 600, loss is 5.170971660614014 and perplexity is 176.08585007271293
At time: 802.0767729282379 and batch: 650, loss is 5.168137035369873 and perplexity is 175.58741944272887
At time: 803.6900362968445 and batch: 700, loss is 5.146965818405151 and perplexity is 171.9090947847925
At time: 805.3037786483765 and batch: 750, loss is 5.144104290008545 and perplexity is 171.41787518304014
At time: 806.9174716472626 and batch: 800, loss is 5.148198623657226 and perplexity is 172.12115590789807
At time: 808.5839214324951 and batch: 850, loss is 5.168309869766236 and perplexity is 175.6177696110781
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.126757621765137 and perplexity of 168.46998810046296
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 812.7594304084778 and batch: 50, loss is 5.18480993270874 and perplexity is 178.53951204326313
At time: 814.3975365161896 and batch: 100, loss is 5.139063987731934 and perplexity is 170.55605102815562
At time: 816.0113110542297 and batch: 150, loss is 5.119958686828613 and perplexity is 167.32845660910004
At time: 817.6265013217926 and batch: 200, loss is 5.157056560516358 and perplexity is 173.65256679627785
At time: 819.2371687889099 and batch: 250, loss is 5.162545499801635 and perplexity is 174.6083559267056
At time: 820.8490114212036 and batch: 300, loss is 5.13544921875 and perplexity is 169.94064325378972
At time: 822.4633700847626 and batch: 350, loss is 5.101738176345825 and perplexity is 164.30725421915807
At time: 824.0753808021545 and batch: 400, loss is 5.128028411865234 and perplexity is 168.68421418286727
At time: 825.6866867542267 and batch: 450, loss is 5.164336433410645 and perplexity is 174.92134809018236
At time: 827.2982907295227 and batch: 500, loss is 5.180780515670777 and perplexity is 177.82154934845923
At time: 828.9142053127289 and batch: 550, loss is 5.145136222839356 and perplexity is 171.59485821782587
At time: 830.5256955623627 and batch: 600, loss is 5.164858207702637 and perplexity is 175.01264136790792
At time: 832.1383619308472 and batch: 650, loss is 5.161111078262329 and perplexity is 174.3580734881962
At time: 833.750236749649 and batch: 700, loss is 5.140033464431763 and perplexity is 170.7214813230934
At time: 835.3656010627747 and batch: 750, loss is 5.136770391464234 and perplexity is 170.1653125755048
At time: 836.9777886867523 and batch: 800, loss is 5.140690288543701 and perplexity is 170.83365214266894
At time: 838.5937111377716 and batch: 850, loss is 5.162989320755005 and perplexity is 174.68586797316217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.128263155619304 and perplexity of 168.72381639656115
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 842.7861249446869 and batch: 50, loss is 5.1840951538085935 and perplexity is 178.4119413650581
At time: 844.3977167606354 and batch: 100, loss is 5.137832622528077 and perplexity is 170.34616349223683
At time: 846.0074474811554 and batch: 150, loss is 5.119182014465332 and perplexity is 167.19854767613063
At time: 847.6119616031647 and batch: 200, loss is 5.1560330581665035 and perplexity is 173.47492391056673
At time: 849.240416765213 and batch: 250, loss is 5.161694192886353 and perplexity is 174.45977387927474
At time: 850.853592634201 and batch: 300, loss is 5.13434178352356 and perplexity is 169.75254916927756
At time: 852.4692597389221 and batch: 350, loss is 5.1010693359375 and perplexity is 164.19739563118216
At time: 854.0772795677185 and batch: 400, loss is 5.126614255905151 and perplexity is 168.44583698700114
At time: 855.6917440891266 and batch: 450, loss is 5.163410940170288 and perplexity is 174.75953445519346
At time: 857.3017361164093 and batch: 500, loss is 5.1801826286315915 and perplexity is 177.71526392532056
At time: 858.9230711460114 and batch: 550, loss is 5.144341964721679 and perplexity is 171.45862171936923
At time: 860.5340061187744 and batch: 600, loss is 5.163350458145142 and perplexity is 174.7489649642713
At time: 862.1483583450317 and batch: 650, loss is 5.1593019390106205 and perplexity is 174.0429206172685
At time: 863.7587275505066 and batch: 700, loss is 5.138039417266846 and perplexity is 170.3813938252148
At time: 865.3751935958862 and batch: 750, loss is 5.1351891613006595 and perplexity is 169.89645466960016
At time: 866.9890727996826 and batch: 800, loss is 5.139057970046997 and perplexity is 170.55502467866455
At time: 868.5987002849579 and batch: 850, loss is 5.161576404571533 and perplexity is 174.43922576669314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.127594947814941 and perplexity of 168.6111114850075
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 872.7927765846252 and batch: 50, loss is 5.18400839805603 and perplexity is 178.39646377421346
At time: 874.4065062999725 and batch: 100, loss is 5.137506151199341 and perplexity is 170.2905594309563
At time: 876.0165600776672 and batch: 150, loss is 5.118965139389038 and perplexity is 167.1622904101379
At time: 877.6263418197632 and batch: 200, loss is 5.155455780029297 and perplexity is 173.37480952934163
At time: 879.2427153587341 and batch: 250, loss is 5.161312627792358 and perplexity is 174.39321881760785
At time: 880.8539569377899 and batch: 300, loss is 5.134060373306275 and perplexity is 169.704785788398
At time: 882.4628708362579 and batch: 350, loss is 5.100920600891113 and perplexity is 164.1729755400331
At time: 884.0712609291077 and batch: 400, loss is 5.126373329162598 and perplexity is 168.40525876858442
At time: 885.680980682373 and batch: 450, loss is 5.163242807388306 and perplexity is 174.7301541184557
At time: 887.2957019805908 and batch: 500, loss is 5.1802559757232665 and perplexity is 177.72829930112334
At time: 888.9397900104523 and batch: 550, loss is 5.14418667793274 and perplexity is 171.4319985277349
At time: 890.5483255386353 and batch: 600, loss is 5.162907505035401 and perplexity is 174.67157650781044
At time: 892.1580748558044 and batch: 650, loss is 5.158711977005005 and perplexity is 173.94027218907152
At time: 893.765647649765 and batch: 700, loss is 5.137555360794067 and perplexity is 170.29893956656144
At time: 895.3729813098907 and batch: 750, loss is 5.134749536514282 and perplexity is 169.821780392544
At time: 896.9831829071045 and batch: 800, loss is 5.138702421188355 and perplexity is 170.4943948133828
At time: 898.5971307754517 and batch: 850, loss is 5.161117753982544 and perplexity is 174.35923745779715
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.12730598449707 and perplexity of 168.56239609762184
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 902.8163120746613 and batch: 50, loss is 5.183853712081909 and perplexity is 178.36887047763776
At time: 904.4326469898224 and batch: 100, loss is 5.137361221313476 and perplexity is 170.26588102797635
At time: 906.0483162403107 and batch: 150, loss is 5.118783369064331 and perplexity is 167.1319080277226
At time: 907.6623549461365 and batch: 200, loss is 5.1553629112243655 and perplexity is 173.35870916559776
At time: 909.2766070365906 and batch: 250, loss is 5.161202020645142 and perplexity is 174.3739307478994
At time: 910.8927853107452 and batch: 300, loss is 5.134007358551026 and perplexity is 169.69578916919374
At time: 912.5129508972168 and batch: 350, loss is 5.100906867980957 and perplexity is 164.1707209827907
At time: 914.1247720718384 and batch: 400, loss is 5.126293478012085 and perplexity is 168.39181195179833
At time: 915.7359983921051 and batch: 450, loss is 5.163179473876953 and perplexity is 174.71908819468177
At time: 917.3506140708923 and batch: 500, loss is 5.180235719680786 and perplexity is 177.72469926560404
At time: 918.9660704135895 and batch: 550, loss is 5.144123373031616 and perplexity is 171.42114638551917
At time: 920.5782489776611 and batch: 600, loss is 5.162781639099121 and perplexity is 174.64959268982776
At time: 922.1890859603882 and batch: 650, loss is 5.15854588508606 and perplexity is 173.91138451455282
At time: 923.8025348186493 and batch: 700, loss is 5.137390394210815 and perplexity is 170.27084824949765
At time: 925.4120268821716 and batch: 750, loss is 5.134670391082763 and perplexity is 169.80834030632127
At time: 927.0201547145844 and batch: 800, loss is 5.138559818267822 and perplexity is 170.4700835482179
At time: 928.6353421211243 and batch: 850, loss is 5.160985679626465 and perplexity is 174.336210594446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.12725289662679 and perplexity of 168.55344771653102
Annealing...
Model not improving. Stopping early with 168.46704259203258loss at 30 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.8297006600589905, 'tune_wordvecs': True, 'lr': 15.687241825130098, 'num_layers': 1, 'anneal': 7.971058623808862, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1186296939849854 and batch: 50, loss is 7.540293703079223 and perplexity is 1882.3828056083412
At time: 3.719460964202881 and batch: 100, loss is 6.812943897247314 and perplexity is 909.5444749908594
At time: 5.343080043792725 and batch: 150, loss is 6.691012287139893 and perplexity is 805.13686945991
At time: 6.942558526992798 and batch: 200, loss is 6.684911746978759 and perplexity is 800.2400514569014
At time: 8.543049812316895 and batch: 250, loss is 6.6732680606842045 and perplexity is 790.9763438507365
At time: 10.138349294662476 and batch: 300, loss is 6.597870149612427 and perplexity is 733.5312125776323
At time: 11.736151218414307 and batch: 350, loss is 6.562771806716919 and perplexity is 708.2320588427004
At time: 13.332056999206543 and batch: 400, loss is 6.571355972290039 and perplexity is 714.3378089918335
At time: 14.931818723678589 and batch: 450, loss is 6.560152444839478 and perplexity is 706.3793702775688
At time: 16.53371000289917 and batch: 500, loss is 6.549784431457519 and perplexity is 699.0934550141976
At time: 18.13029980659485 and batch: 550, loss is 6.483349943161011 and perplexity is 654.1586742200399
At time: 19.726571559906006 and batch: 600, loss is 6.495591306686402 and perplexity is 662.2156821234979
At time: 21.322858095169067 and batch: 650, loss is 6.504701948165893 and perplexity is 668.2764586441795
At time: 22.919740200042725 and batch: 700, loss is 6.475625238418579 and perplexity is 649.1249586209415
At time: 24.516681432724 and batch: 750, loss is 6.449382095336914 and perplexity is 632.3114638751262
At time: 26.114542961120605 and batch: 800, loss is 6.474462852478028 and perplexity is 648.3708632552124
At time: 27.71729826927185 and batch: 850, loss is 6.468685436248779 and perplexity is 644.6357549343509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.742324193318685 and perplexity of 311.7882256238247
Finished 1 epochs...
Completing Train Step...
At time: 31.88849925994873 and batch: 50, loss is 6.05964282989502 and perplexity is 428.2224612803109
At time: 33.484145402908325 and batch: 100, loss is 5.860131311416626 and perplexity is 350.77020112795543
At time: 35.09047603607178 and batch: 150, loss is 5.784344663619995 and perplexity is 325.16887527931004
At time: 36.69879364967346 and batch: 200, loss is 5.7525572681427 and perplexity is 314.99515827208666
At time: 38.30744028091431 and batch: 250, loss is 5.768700790405274 and perplexity is 320.121557357634
At time: 39.91301226615906 and batch: 300, loss is 5.6905282497406 and perplexity is 296.04996766033315
At time: 41.52036666870117 and batch: 350, loss is 5.680215311050415 and perplexity is 293.0125119629122
At time: 43.129903078079224 and batch: 400, loss is 5.656825342178345 and perplexity is 286.2384893488962
At time: 44.73984694480896 and batch: 450, loss is 5.678818340301514 and perplexity is 292.6034678325022
At time: 46.35583209991455 and batch: 500, loss is 5.681109762191772 and perplexity is 293.2747145849054
At time: 47.96545386314392 and batch: 550, loss is 5.652208766937256 and perplexity is 284.9200934045134
At time: 49.576780796051025 and batch: 600, loss is 5.627614049911499 and perplexity is 277.99803627633537
At time: 51.23282170295715 and batch: 650, loss is 5.665896215438843 and perplexity is 288.8467340526252
At time: 52.84364986419678 and batch: 700, loss is 5.636821575164795 and perplexity is 280.56953059790385
At time: 54.457579612731934 and batch: 750, loss is 5.643580083847046 and perplexity is 282.4721845155779
At time: 56.075719594955444 and batch: 800, loss is 5.656028137207032 and perplexity is 286.0103895354392
At time: 57.68697237968445 and batch: 850, loss is 5.631435832977295 and perplexity is 279.0625172757767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.351027806599935 and perplexity of 210.82487374521583
Finished 2 epochs...
Completing Train Step...
At time: 61.86842346191406 and batch: 50, loss is 5.626454029083252 and perplexity is 277.6757397355346
At time: 63.48256850242615 and batch: 100, loss is 5.552989358901978 and perplexity is 258.00768180129126
At time: 65.09805297851562 and batch: 150, loss is 5.537143201828003 and perplexity is 253.9514740775593
At time: 66.7199399471283 and batch: 200, loss is 5.560065631866455 and perplexity is 259.83988954025745
At time: 68.33605885505676 and batch: 250, loss is 5.597452716827393 and perplexity is 269.7384313959728
At time: 69.95234155654907 and batch: 300, loss is 5.5759315681457515 and perplexity is 263.99537080043757
At time: 71.57081317901611 and batch: 350, loss is 5.527716197967529 and perplexity is 251.56872130676808
At time: 73.19493389129639 and batch: 400, loss is 5.528663587570191 and perplexity is 251.80716783073296
At time: 74.81463575363159 and batch: 450, loss is 5.524955377578736 and perplexity is 250.87514311475363
At time: 76.43720483779907 and batch: 500, loss is 5.5413589191436765 and perplexity is 255.02432152604084
At time: 78.0580153465271 and batch: 550, loss is 5.548941383361816 and perplexity is 256.96538403737924
At time: 79.67871928215027 and batch: 600, loss is 5.554956417083741 and perplexity is 258.51569740705696
At time: 81.29802298545837 and batch: 650, loss is 5.566841125488281 and perplexity is 261.60641082336133
At time: 82.92002463340759 and batch: 700, loss is 5.536727743148804 and perplexity is 253.84598964728676
At time: 84.54187798500061 and batch: 750, loss is 5.543375253677368 and perplexity is 255.53905463514505
At time: 86.16356182098389 and batch: 800, loss is 5.544406309127807 and perplexity is 255.80266544555494
At time: 87.82722902297974 and batch: 850, loss is 5.527796907424927 and perplexity is 251.58902610114635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.248191197713216 and perplexity of 190.2218833126735
Finished 3 epochs...
Completing Train Step...
At time: 92.02349019050598 and batch: 50, loss is 5.494422245025635 and perplexity is 243.33089990298586
At time: 93.66366648674011 and batch: 100, loss is 5.442031698226929 and perplexity is 230.91084836780632
At time: 95.28564262390137 and batch: 150, loss is 5.45508602142334 and perplexity is 233.9449944853442
At time: 96.90476274490356 and batch: 200, loss is 5.472895088195801 and perplexity is 238.14865706377455
At time: 98.52137637138367 and batch: 250, loss is 5.525285873413086 and perplexity is 250.95807000723656
At time: 100.14044499397278 and batch: 300, loss is 5.475683517456055 and perplexity is 238.81364445169135
At time: 101.75533843040466 and batch: 350, loss is 5.460860166549683 and perplexity is 235.2997343032409
At time: 103.37435698509216 and batch: 400, loss is 5.45135687828064 and perplexity is 233.07420477227058
At time: 104.98973894119263 and batch: 450, loss is 5.479066162109375 and perplexity is 239.6228339779668
At time: 106.60473370552063 and batch: 500, loss is 5.495895910263061 and perplexity is 243.68975254075045
At time: 108.22060799598694 and batch: 550, loss is 5.4760675048828125 and perplexity is 238.90536349685203
At time: 109.84262084960938 and batch: 600, loss is 5.5409245204925535 and perplexity is 254.91356336310633
At time: 111.45988059043884 and batch: 650, loss is 5.514574995040894 and perplexity is 248.28443270541783
At time: 113.07847690582275 and batch: 700, loss is 5.4708749961853025 and perplexity is 237.66806045245016
At time: 114.69259428977966 and batch: 750, loss is 5.462437343597412 and perplexity is 235.67113645000336
At time: 116.30200266838074 and batch: 800, loss is 5.5201136589050295 and perplexity is 249.66341204717486
At time: 117.90713787078857 and batch: 850, loss is 5.515241413116455 and perplexity is 248.44994908468107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.345130920410156 and perplexity of 209.58532179907937
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 122.11900663375854 and batch: 50, loss is 5.508234558105468 and perplexity is 246.71518104553257
At time: 123.73112392425537 and batch: 100, loss is 5.390986433029175 and perplexity is 219.41972169658803
At time: 125.34647870063782 and batch: 150, loss is 5.358831157684326 and perplexity is 212.4764497855364
At time: 126.96430039405823 and batch: 200, loss is 5.357113361358643 and perplexity is 212.11177183170008
At time: 128.62332606315613 and batch: 250, loss is 5.374463796615601 and perplexity is 215.8241156575824
At time: 130.2296006679535 and batch: 300, loss is 5.310423927307129 and perplexity is 202.4360283607558
At time: 131.83784580230713 and batch: 350, loss is 5.277355337142945 and perplexity is 195.85122936040017
At time: 133.45507526397705 and batch: 400, loss is 5.274274044036865 and perplexity is 195.24868310509888
At time: 135.06755542755127 and batch: 450, loss is 5.289625606536865 and perplexity is 198.2691808266626
At time: 136.67938780784607 and batch: 500, loss is 5.2801430225372314 and perplexity is 196.39796267811946
At time: 138.29736471176147 and batch: 550, loss is 5.244751815795898 and perplexity is 189.5687614185797
At time: 139.91178631782532 and batch: 600, loss is 5.24038272857666 and perplexity is 188.74232566548903
At time: 141.52253699302673 and batch: 650, loss is 5.241811361312866 and perplexity is 189.0121618331393
At time: 143.1331648826599 and batch: 700, loss is 5.250642280578614 and perplexity is 190.6887047868287
At time: 144.74549055099487 and batch: 750, loss is 5.15282603263855 and perplexity is 172.9194765441327
At time: 146.36325597763062 and batch: 800, loss is 5.145656623840332 and perplexity is 171.68417959325558
At time: 147.97836589813232 and batch: 850, loss is 5.163173732757568 and perplexity is 174.71808511441697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.100406964619954 and perplexity of 164.08867199758365
Finished 5 epochs...
Completing Train Step...
At time: 152.24428129196167 and batch: 50, loss is 5.319708776473999 and perplexity is 204.32436926437023
At time: 153.8607850074768 and batch: 100, loss is 5.243497409820557 and perplexity is 189.33111431565322
At time: 155.47724866867065 and batch: 150, loss is 5.227323656082153 and perplexity is 186.2935501214729
At time: 157.0908076763153 and batch: 200, loss is 5.245319604873657 and perplexity is 189.67642705359077
At time: 158.70682454109192 and batch: 250, loss is 5.265489101409912 and perplexity is 193.540946791696
At time: 160.32464981079102 and batch: 300, loss is 5.217894048690796 and perplexity is 184.54513149030697
At time: 161.93634271621704 and batch: 350, loss is 5.186142683029175 and perplexity is 178.77761926864522
At time: 163.54606795310974 and batch: 400, loss is 5.189685935974121 and perplexity is 179.41219716570262
At time: 165.15633416175842 and batch: 450, loss is 5.21292444229126 and perplexity is 183.63028990795914
At time: 166.76809787750244 and batch: 500, loss is 5.205254211425781 and perplexity is 182.22719111527434
At time: 168.43058514595032 and batch: 550, loss is 5.1796315479278565 and perplexity is 177.61735545282494
At time: 170.0421919822693 and batch: 600, loss is 5.188777847290039 and perplexity is 179.24934893118035
At time: 171.65409636497498 and batch: 650, loss is 5.188117141723633 and perplexity is 179.13095700397633
At time: 173.26918125152588 and batch: 700, loss is 5.1960453605651855 and perplexity is 180.5567911262883
At time: 174.88602209091187 and batch: 750, loss is 5.120629100799561 and perplexity is 167.4406735558493
At time: 176.49678206443787 and batch: 800, loss is 5.125454740524292 and perplexity is 168.2506346401052
At time: 178.10930848121643 and batch: 850, loss is 5.154474229812622 and perplexity is 173.20471693834804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0850474039713545 and perplexity of 161.58759894995998
Finished 6 epochs...
Completing Train Step...
At time: 182.3061363697052 and batch: 50, loss is 5.262687921524048 and perplexity is 192.99956239599445
At time: 183.92546272277832 and batch: 100, loss is 5.18739182472229 and perplexity is 179.00107738303714
At time: 185.5406563282013 and batch: 150, loss is 5.175313911437988 and perplexity is 176.8521214675666
At time: 187.15388369560242 and batch: 200, loss is 5.197347249984741 and perplexity is 180.7920091830113
At time: 188.76712083816528 and batch: 250, loss is 5.21634840965271 and perplexity is 184.26011165642055
At time: 190.37754917144775 and batch: 300, loss is 5.172154674530029 and perplexity is 176.2942853503702
At time: 191.99435448646545 and batch: 350, loss is 5.140299644470215 and perplexity is 170.76693002205636
At time: 193.60167574882507 and batch: 400, loss is 5.147439117431641 and perplexity is 171.99047844988428
At time: 195.21499824523926 and batch: 450, loss is 5.174355516433716 and perplexity is 176.68270847310828
At time: 196.8371341228485 and batch: 500, loss is 5.167088260650635 and perplexity is 175.4033643292467
At time: 198.45326566696167 and batch: 550, loss is 5.146479406356812 and perplexity is 171.82549646313367
At time: 200.06630778312683 and batch: 600, loss is 5.161561422348022 and perplexity is 174.4366122988015
At time: 201.6817479133606 and batch: 650, loss is 5.159045476913452 and perplexity is 173.99829092800607
At time: 203.3045573234558 and batch: 700, loss is 5.163595695495605 and perplexity is 174.79182519268497
At time: 204.91965055465698 and batch: 750, loss is 5.100278997421265 and perplexity is 164.06767537335975
At time: 206.541424036026 and batch: 800, loss is 5.107060546875 and perplexity is 165.18408965840612
At time: 208.16127800941467 and batch: 850, loss is 5.139200839996338 and perplexity is 170.57939360715264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.076329231262207 and perplexity of 160.18497340458097
Finished 7 epochs...
Completing Train Step...
At time: 212.37220811843872 and batch: 50, loss is 5.22330493927002 and perplexity is 185.5463914147917
At time: 214.01105332374573 and batch: 100, loss is 5.1513416194915775 and perplexity is 172.6629830180624
At time: 215.63243651390076 and batch: 150, loss is 5.141640071868896 and perplexity is 170.9959841747817
At time: 217.24527788162231 and batch: 200, loss is 5.163496074676513 and perplexity is 174.77441315520386
At time: 218.85463619232178 and batch: 250, loss is 5.18108359336853 and perplexity is 177.87545126206808
At time: 220.47531867027283 and batch: 300, loss is 5.139370613098144 and perplexity is 170.6083558583456
At time: 222.0914704799652 and batch: 350, loss is 5.105583572387696 and perplexity is 164.9402970543188
At time: 223.7025671005249 and batch: 400, loss is 5.116523809432984 and perplexity is 166.7546898492114
At time: 225.31494450569153 and batch: 450, loss is 5.1463275814056395 and perplexity is 171.79941104578174
At time: 226.92934346199036 and batch: 500, loss is 5.140478544235229 and perplexity is 170.79748291858107
At time: 228.5406756401062 and batch: 550, loss is 5.121485471725464 and perplexity is 167.58412629610714
At time: 230.15144729614258 and batch: 600, loss is 5.140266304016113 and perplexity is 170.7612366699739
At time: 231.76460123062134 and batch: 650, loss is 5.134910078048706 and perplexity is 169.8490460303219
At time: 233.37722849845886 and batch: 700, loss is 5.137534646987915 and perplexity is 170.29541206387347
At time: 234.99832129478455 and batch: 750, loss is 5.082056303024292 and perplexity is 161.10499624621704
At time: 236.60927271842957 and batch: 800, loss is 5.090014305114746 and perplexity is 162.39218507571493
At time: 238.21935296058655 and batch: 850, loss is 5.123063755035401 and perplexity is 167.84883035970577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.071448961893718 and perplexity of 159.40513205130807
Finished 8 epochs...
Completing Train Step...
At time: 242.4153184890747 and batch: 50, loss is 5.193329839706421 and perplexity is 180.06715050933394
At time: 244.05773162841797 and batch: 100, loss is 5.122505445480346 and perplexity is 167.75514490908594
At time: 245.6760959625244 and batch: 150, loss is 5.1159443855285645 and perplexity is 166.65809618278087
At time: 247.28856563568115 and batch: 200, loss is 5.13793288230896 and perplexity is 170.36324321745337
At time: 248.89899921417236 and batch: 250, loss is 5.154095449447632 and perplexity is 173.1391228161152
At time: 250.53905963897705 and batch: 300, loss is 5.113679065704345 and perplexity is 166.2809895884191
At time: 252.1562955379486 and batch: 350, loss is 5.0785853290557865 and perplexity is 160.54677434533252
At time: 253.7680218219757 and batch: 400, loss is 5.092838144302368 and perplexity is 162.85140256471325
At time: 255.3809404373169 and batch: 450, loss is 5.125242929458619 and perplexity is 168.21500106780755
At time: 256.99667859077454 and batch: 500, loss is 5.119549674987793 and perplexity is 167.26003128337888
At time: 258.6105649471283 and batch: 550, loss is 5.101844081878662 and perplexity is 164.32465618793123
At time: 260.217337846756 and batch: 600, loss is 5.122356748580932 and perplexity is 167.73020209368283
At time: 261.82650685310364 and batch: 650, loss is 5.1141884136199955 and perplexity is 166.36570603711993
At time: 263.4366874694824 and batch: 700, loss is 5.115137548446655 and perplexity is 166.52368448224397
At time: 265.04441499710083 and batch: 750, loss is 5.065749578475952 and perplexity is 158.49920514963375
At time: 266.6521782875061 and batch: 800, loss is 5.07361439704895 and perplexity is 159.75068753214063
At time: 268.26571464538574 and batch: 850, loss is 5.107124652862549 and perplexity is 165.19467928702667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0687611897786455 and perplexity of 158.97726264768872
Finished 9 epochs...
Completing Train Step...
At time: 272.4277672767639 and batch: 50, loss is 5.166592292785644 and perplexity is 175.31639146678341
At time: 274.07266664505005 and batch: 100, loss is 5.098194313049317 and perplexity is 163.72600231854582
At time: 275.6970326900482 and batch: 150, loss is 5.0932832527160645 and perplexity is 162.92390522875297
At time: 277.31268668174744 and batch: 200, loss is 5.114878978729248 and perplexity is 166.48063206647117
At time: 278.92846298217773 and batch: 250, loss is 5.130071992874146 and perplexity is 169.02928651101124
At time: 280.54895520210266 and batch: 300, loss is 5.091357307434082 and perplexity is 162.61042467225343
At time: 282.16557121276855 and batch: 350, loss is 5.054761619567871 and perplexity is 156.76715566733776
At time: 283.7802987098694 and batch: 400, loss is 5.071252431869507 and perplexity is 159.37380723508093
At time: 285.3962495326996 and batch: 450, loss is 5.104175872802735 and perplexity is 164.70827401429574
At time: 287.0177674293518 and batch: 500, loss is 5.100141544342041 and perplexity is 164.0451253160014
At time: 288.63296127319336 and batch: 550, loss is 5.083775730133056 and perplexity is 161.3822428284599
At time: 290.2473449707031 and batch: 600, loss is 5.106330995559692 and perplexity is 165.06362333705263
At time: 291.89206767082214 and batch: 650, loss is 5.095788555145264 and perplexity is 163.33259061077655
At time: 293.51439809799194 and batch: 700, loss is 5.094381675720215 and perplexity is 163.1029629167208
At time: 295.1343946456909 and batch: 750, loss is 5.050266418457031 and perplexity is 156.06403728828067
At time: 296.74774265289307 and batch: 800, loss is 5.057891311645508 and perplexity is 157.25855715903603
At time: 298.3589415550232 and batch: 850, loss is 5.091587371826172 and perplexity is 162.64783984453757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.066523551940918 and perplexity of 158.62192681416752
Finished 10 epochs...
Completing Train Step...
At time: 302.5458388328552 and batch: 50, loss is 5.143614921569824 and perplexity is 171.3340092074475
At time: 304.15764474868774 and batch: 100, loss is 5.076810092926025 and perplexity is 160.26201874000046
At time: 305.775591135025 and batch: 150, loss is 5.073218660354614 and perplexity is 159.68748083056923
At time: 307.3946352005005 and batch: 200, loss is 5.09492094039917 and perplexity is 163.19094230361054
At time: 309.00714588165283 and batch: 250, loss is 5.109140701293946 and perplexity is 165.52805569965926
At time: 310.619252204895 and batch: 300, loss is 5.071953935623169 and perplexity is 159.4856477827873
At time: 312.23507618904114 and batch: 350, loss is 5.034343538284301 and perplexity is 153.59872782413242
At time: 313.84873270988464 and batch: 400, loss is 5.051694602966308 and perplexity is 156.28708476735554
At time: 315.4607734680176 and batch: 450, loss is 5.086334953308105 and perplexity is 161.79578495192894
At time: 317.07344341278076 and batch: 500, loss is 5.0839476394653325 and perplexity is 161.4099883268525
At time: 318.6893970966339 and batch: 550, loss is 5.067396841049194 and perplexity is 158.7605101180257
At time: 320.30995178222656 and batch: 600, loss is 5.091669864654541 and perplexity is 162.6612576783044
At time: 321.92912912368774 and batch: 650, loss is 5.079256019592285 and perplexity is 160.6544876646656
At time: 323.54048466682434 and batch: 700, loss is 5.076641321182251 and perplexity is 160.23497332195234
At time: 325.1546804904938 and batch: 750, loss is 5.036921510696411 and perplexity is 153.99521194998312
At time: 326.77171778678894 and batch: 800, loss is 5.044795227050781 and perplexity is 155.21251261999066
At time: 328.3906669616699 and batch: 850, loss is 5.076805200576782 and perplexity is 160.26123468415233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.064870516459147 and perplexity of 158.35993574088505
Finished 11 epochs...
Completing Train Step...
At time: 332.60749077796936 and batch: 50, loss is 5.124232139587402 and perplexity is 168.04505695190633
At time: 334.2173135280609 and batch: 100, loss is 5.058158512115479 and perplexity is 157.300582333739
At time: 335.8283133506775 and batch: 150, loss is 5.054155530929566 and perplexity is 156.67216966330503
At time: 337.43384194374084 and batch: 200, loss is 5.077288064956665 and perplexity is 160.33863781196013
At time: 339.0507571697235 and batch: 250, loss is 5.090769863128662 and perplexity is 162.51492815646617
At time: 340.66782665252686 and batch: 300, loss is 5.053183822631836 and perplexity is 156.52000395833207
At time: 342.2866666316986 and batch: 350, loss is 5.014734420776367 and perplexity is 150.61613093867246
At time: 343.89796018600464 and batch: 400, loss is 5.034185438156128 and perplexity is 153.5744457651251
At time: 345.5075058937073 and batch: 450, loss is 5.070001726150513 and perplexity is 159.17460210238096
At time: 347.1176164150238 and batch: 500, loss is 5.06841742515564 and perplexity is 158.9226212813506
At time: 348.73324823379517 and batch: 550, loss is 5.051776037216187 and perplexity is 156.29981240709512
At time: 350.34687972068787 and batch: 600, loss is 5.078085041046142 and perplexity is 160.46647480726335
At time: 351.9596543312073 and batch: 650, loss is 5.06379846572876 and perplexity is 158.19025682456387
At time: 353.5716767311096 and batch: 700, loss is 5.05972297668457 and perplexity is 157.5468661223956
At time: 355.1835913658142 and batch: 750, loss is 5.023771476745606 and perplexity is 151.983426200352
At time: 356.8008027076721 and batch: 800, loss is 5.029689807891845 and perplexity is 152.88558143878132
At time: 358.4133219718933 and batch: 850, loss is 5.0635618972778325 and perplexity is 158.1528384267358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.063916206359863 and perplexity of 158.20888334176425
Finished 12 epochs...
Completing Train Step...
At time: 362.62676548957825 and batch: 50, loss is 5.104137859344482 and perplexity is 164.7020130021998
At time: 364.24007749557495 and batch: 100, loss is 5.041018018722534 and perplexity is 154.6273484638622
At time: 365.85500049591064 and batch: 150, loss is 5.038096265792847 and perplexity is 154.17622491218077
At time: 367.4649395942688 and batch: 200, loss is 5.061431045532227 and perplexity is 157.81619696879835
At time: 369.07641649246216 and batch: 250, loss is 5.073730297088623 and perplexity is 159.7692037161559
At time: 370.69158959388733 and batch: 300, loss is 5.03698763847351 and perplexity is 154.0053956477423
At time: 372.3043932914734 and batch: 350, loss is 4.997778635025025 and perplexity is 148.08384520771222
At time: 373.9414064884186 and batch: 400, loss is 5.018227167129517 and perplexity is 151.14311465522616
At time: 375.550416469574 and batch: 450, loss is 5.055168972015381 and perplexity is 156.83102816031302
At time: 377.1627221107483 and batch: 500, loss is 5.053824987411499 and perplexity is 156.62039125114202
At time: 378.77948689460754 and batch: 550, loss is 5.038398561477661 and perplexity is 154.22283876490093
At time: 380.39196825027466 and batch: 600, loss is 5.065927686691285 and perplexity is 158.52743767434225
At time: 382.00206780433655 and batch: 650, loss is 5.050054721832275 and perplexity is 156.03100255513502
At time: 383.61607360839844 and batch: 700, loss is 5.0447338199615475 and perplexity is 155.20298176401204
At time: 385.23270320892334 and batch: 750, loss is 5.010827560424804 and perplexity is 150.02884272193555
At time: 386.84619760513306 and batch: 800, loss is 5.017157106399536 and perplexity is 150.98146884446788
At time: 388.4579565525055 and batch: 850, loss is 5.050579538345337 and perplexity is 156.11291169357975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.062282244364421 and perplexity of 157.950587119615
Finished 13 epochs...
Completing Train Step...
At time: 392.62970900535583 and batch: 50, loss is 5.087267789840698 and perplexity is 161.9467843888975
At time: 394.27085638046265 and batch: 100, loss is 5.024883308410645 and perplexity is 152.1525001596365
At time: 395.8963360786438 and batch: 150, loss is 5.023029737472534 and perplexity is 151.8707359228521
At time: 397.5114347934723 and batch: 200, loss is 5.046748752593994 and perplexity is 155.51602058678077
At time: 399.12609004974365 and batch: 250, loss is 5.057865419387817 and perplexity is 157.25448543266333
At time: 400.74661898612976 and batch: 300, loss is 5.0216794681549075 and perplexity is 151.66580791302798
At time: 402.36328625679016 and batch: 350, loss is 4.982180948257446 and perplexity is 145.79199998145293
At time: 403.97753977775574 and batch: 400, loss is 5.003228740692139 and perplexity is 148.89312112812365
At time: 405.5921061038971 and batch: 450, loss is 5.04141583442688 and perplexity is 154.68887388848123
At time: 407.2019612789154 and batch: 500, loss is 5.041691617965698 and perplexity is 154.7315404166306
At time: 408.80781507492065 and batch: 550, loss is 5.025544166564941 and perplexity is 152.25308461243083
At time: 410.4236397743225 and batch: 600, loss is 5.054441871643067 and perplexity is 156.7170377076195
At time: 412.04184126853943 and batch: 650, loss is 5.0367624378204345 and perplexity is 153.97071743698962
At time: 413.68348479270935 and batch: 700, loss is 5.030687074661255 and perplexity is 153.03812519944125
At time: 415.2990393638611 and batch: 750, loss is 4.9992968940734865 and perplexity is 148.30884560685487
At time: 416.9207532405853 and batch: 800, loss is 5.005265083312988 and perplexity is 149.19662745318192
At time: 418.5367946624756 and batch: 850, loss is 5.038870191574096 and perplexity is 154.29559205219158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061861673990886 and perplexity of 157.8841717493375
Finished 14 epochs...
Completing Train Step...
At time: 422.70691990852356 and batch: 50, loss is 5.071752958297729 and perplexity is 159.45359800459715
At time: 424.3456747531891 and batch: 100, loss is 5.0098823547363285 and perplexity is 149.88710160416753
At time: 425.9572637081146 and batch: 150, loss is 5.00852933883667 and perplexity is 149.6844391062514
At time: 427.5718688964844 and batch: 200, loss is 5.03261589050293 and perplexity is 153.33359241903005
At time: 429.1859588623047 and batch: 250, loss is 5.042399892807007 and perplexity is 154.84117169381923
At time: 430.79836082458496 and batch: 300, loss is 5.006936893463135 and perplexity is 149.44626450406003
At time: 432.41108870506287 and batch: 350, loss is 4.966873874664307 and perplexity is 143.57734429938358
At time: 434.02524518966675 and batch: 400, loss is 4.9893207931518555 and perplexity is 146.83665715544078
At time: 435.6460032463074 and batch: 450, loss is 5.028591794967651 and perplexity is 152.71780322262006
At time: 437.25882172584534 and batch: 500, loss is 5.028886861801148 and perplexity is 152.76287183003413
At time: 438.8726580142975 and batch: 550, loss is 5.012630281448364 and perplexity is 150.29954679950535
At time: 440.48390078544617 and batch: 600, loss is 5.04237229347229 and perplexity is 154.8368982394663
At time: 442.0950164794922 and batch: 650, loss is 5.023680372238159 and perplexity is 151.96958045588244
At time: 443.7141447067261 and batch: 700, loss is 5.017550678253174 and perplexity is 151.04090259598456
At time: 445.3254656791687 and batch: 750, loss is 4.98778018951416 and perplexity is 146.61061423326277
At time: 446.9373347759247 and batch: 800, loss is 4.993719882965088 and perplexity is 147.48402767467866
At time: 448.554566860199 and batch: 850, loss is 5.027010984420777 and perplexity is 152.47657602607325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.061282157897949 and perplexity of 157.79270183769165
Finished 15 epochs...
Completing Train Step...
At time: 452.7110118865967 and batch: 50, loss is 5.0579609680175786 and perplexity is 157.26951160112367
At time: 454.3495740890503 and batch: 100, loss is 4.996535911560058 and perplexity is 147.89993223864022
At time: 455.9630913734436 and batch: 150, loss is 4.99660361289978 and perplexity is 147.90994560115274
At time: 457.5838534832001 and batch: 200, loss is 5.019807424545288 and perplexity is 151.38214850076312
At time: 459.1946265697479 and batch: 250, loss is 5.02984317779541 and perplexity is 152.90903128386728
At time: 460.8055331707001 and batch: 300, loss is 4.995117483139038 and perplexity is 147.6902954837995
At time: 462.4159903526306 and batch: 350, loss is 4.954019374847412 and perplexity is 141.74354091712044
At time: 464.0286235809326 and batch: 400, loss is 4.976710300445557 and perplexity is 144.99660095346283
At time: 465.64876914024353 and batch: 450, loss is 5.0168867111206055 and perplexity is 150.94064968698873
At time: 467.2608218193054 and batch: 500, loss is 5.018217210769653 and perplexity is 151.14160982747705
At time: 468.8705155849457 and batch: 550, loss is 5.001532821655274 and perplexity is 148.64082444738617
At time: 470.4828860759735 and batch: 600, loss is 5.0322076606750485 and perplexity is 153.27100984789365
At time: 472.09950065612793 and batch: 650, loss is 5.012651748657227 and perplexity is 150.30277334590087
At time: 473.7143259048462 and batch: 700, loss is 5.005476026535034 and perplexity is 149.22810279013314
At time: 475.32836389541626 and batch: 750, loss is 4.977726135253906 and perplexity is 145.14396838561467
At time: 476.9407343864441 and batch: 800, loss is 4.983095293045044 and perplexity is 145.9253650982677
At time: 478.55253744125366 and batch: 850, loss is 5.01680627822876 and perplexity is 150.92850958227578
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.06078306833903 and perplexity of 157.71396879678448
Finished 16 epochs...
Completing Train Step...
At time: 482.7362058162689 and batch: 50, loss is 5.04444993019104 and perplexity is 155.1589274787133
At time: 484.3441185951233 and batch: 100, loss is 4.983992719650269 and perplexity is 146.05638118315113
At time: 485.9611039161682 and batch: 150, loss is 4.984521942138672 and perplexity is 146.13369796173245
At time: 487.5823926925659 and batch: 200, loss is 5.007678995132446 and perplexity is 149.5572099877435
At time: 489.1978847980499 and batch: 250, loss is 5.016601238250733 and perplexity is 150.89756637639286
At time: 490.8102743625641 and batch: 300, loss is 4.98317554473877 and perplexity is 145.93707632589104
At time: 492.424729347229 and batch: 350, loss is 4.940982904434204 and perplexity is 139.90769789628914
At time: 494.04116582870483 and batch: 400, loss is 4.964971218109131 and perplexity is 143.304425641647
At time: 495.6860775947571 and batch: 450, loss is 5.005274438858033 and perplexity is 149.19802327547987
At time: 497.2995729446411 and batch: 500, loss is 5.00679497718811 and perplexity is 149.42505715175506
At time: 498.91278886795044 and batch: 550, loss is 4.990370359420776 and perplexity is 146.9908528629839
At time: 500.5261595249176 and batch: 600, loss is 5.0215271472930905 and perplexity is 151.64270780581728
At time: 502.1483910083771 and batch: 650, loss is 5.000605230331421 and perplexity is 148.50301043568078
At time: 503.7636902332306 and batch: 700, loss is 4.9929010200500485 and perplexity is 147.36330790707117
At time: 505.37934851646423 and batch: 750, loss is 4.966728000640869 and perplexity is 143.55640162202963
At time: 506.99996614456177 and batch: 800, loss is 4.971926469802856 and perplexity is 144.30461825442433
At time: 508.61518931388855 and batch: 850, loss is 5.0069029331207275 and perplexity is 149.44118934392353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0616458257039385 and perplexity of 157.85009639901463
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 512.7811510562897 and batch: 50, loss is 5.03914888381958 and perplexity is 154.33859902978608
At time: 514.3925988674164 and batch: 100, loss is 4.9840848255157475 and perplexity is 146.06983445210165
At time: 516.0074701309204 and batch: 150, loss is 4.977392454147338 and perplexity is 145.0955446651217
At time: 517.628125667572 and batch: 200, loss is 4.996605529785156 and perplexity is 147.91022912783615
At time: 519.2427158355713 and batch: 250, loss is 4.997452421188354 and perplexity is 148.03554608676717
At time: 520.8548278808594 and batch: 300, loss is 4.951071510314941 and perplexity is 141.32631542439745
At time: 522.4678909778595 and batch: 350, loss is 4.906503686904907 and perplexity is 135.16600462432672
At time: 524.0881896018982 and batch: 400, loss is 4.926558647155762 and perplexity is 137.90411808044962
At time: 525.7009029388428 and batch: 450, loss is 4.969094152450562 and perplexity is 143.89648004143856
At time: 527.3120906352997 and batch: 500, loss is 4.962905216217041 and perplexity is 143.00866405432745
At time: 528.9262421131134 and batch: 550, loss is 4.934196310043335 and perplexity is 138.96141574704495
At time: 530.5464205741882 and batch: 600, loss is 4.950038070678711 and perplexity is 141.18033865047948
At time: 532.1660614013672 and batch: 650, loss is 4.92292350769043 and perplexity is 137.40372742431805
At time: 533.7807433605194 and batch: 700, loss is 4.907088241577148 and perplexity is 135.24503964175187
At time: 535.391636133194 and batch: 750, loss is 4.867908658981324 and perplexity is 130.04865621156318
At time: 537.0307023525238 and batch: 800, loss is 4.86052981376648 and perplexity is 129.09257901777852
At time: 538.6445932388306 and batch: 850, loss is 4.917480249404907 and perplexity is 136.65783532988803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.023871421813965 and perplexity of 151.99861695338086
Finished 18 epochs...
Completing Train Step...
At time: 542.8136205673218 and batch: 50, loss is 5.008149194717407 and perplexity is 149.6275482610252
At time: 544.4243996143341 and batch: 100, loss is 4.954289112091065 and perplexity is 141.78177958611766
At time: 546.042950630188 and batch: 150, loss is 4.94895079612732 and perplexity is 141.0269202801667
At time: 547.6595559120178 and batch: 200, loss is 4.972002153396606 and perplexity is 144.3155401598279
At time: 549.2749750614166 and batch: 250, loss is 4.974574794769287 and perplexity is 144.68729027399073
At time: 550.8852047920227 and batch: 300, loss is 4.931759052276611 and perplexity is 138.62314335320127
At time: 552.4975991249084 and batch: 350, loss is 4.889055910110474 and perplexity is 132.82811315874338
At time: 554.1102459430695 and batch: 400, loss is 4.909779872894287 and perplexity is 135.60955978277758
At time: 555.7191345691681 and batch: 450, loss is 4.9542797088623045 and perplexity is 141.7804463858784
At time: 557.3260643482208 and batch: 500, loss is 4.949037380218506 and perplexity is 141.03913149653292
At time: 558.9318075180054 and batch: 550, loss is 4.922823181152344 and perplexity is 137.38994287551503
At time: 560.54119181633 and batch: 600, loss is 4.943047866821289 and perplexity is 140.1969005236175
At time: 562.1517741680145 and batch: 650, loss is 4.917673816680908 and perplexity is 136.68429037515003
At time: 563.775919675827 and batch: 700, loss is 4.905062532424926 and perplexity is 134.97134982872697
At time: 565.3962526321411 and batch: 750, loss is 4.8710102844238286 and perplexity is 130.45264461892722
At time: 567.006795167923 and batch: 800, loss is 4.868927783966065 and perplexity is 130.18125960457883
At time: 568.6160361766815 and batch: 850, loss is 4.925473861694336 and perplexity is 137.7546028087436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.021271387736003 and perplexity of 151.603928693309
Finished 19 epochs...
Completing Train Step...
At time: 572.7563056945801 and batch: 50, loss is 4.998174076080322 and perplexity is 148.14241521946403
At time: 574.3982450962067 and batch: 100, loss is 4.944105987548828 and perplexity is 140.34532428127727
At time: 576.0206100940704 and batch: 150, loss is 4.937631855010986 and perplexity is 139.43964495821533
At time: 577.6574892997742 and batch: 200, loss is 4.961629266738892 and perplexity is 142.8263085869785
At time: 579.2702860832214 and batch: 250, loss is 4.964108276367187 and perplexity is 143.1808156127531
At time: 580.8887207508087 and batch: 300, loss is 4.922846803665161 and perplexity is 137.3931884095352
At time: 582.5003650188446 and batch: 350, loss is 4.880672225952148 and perplexity is 131.71917918199915
At time: 584.1122422218323 and batch: 400, loss is 4.902442417144775 and perplexity is 134.61817221773282
At time: 585.7262988090515 and batch: 450, loss is 4.947399482727051 and perplexity is 140.80831293702911
At time: 587.3416566848755 and batch: 500, loss is 4.942751541137695 and perplexity is 140.15536273589265
At time: 588.9526476860046 and batch: 550, loss is 4.918230924606323 and perplexity is 136.760459491837
At time: 590.5658404827118 and batch: 600, loss is 4.940623378753662 and perplexity is 139.85740652705095
At time: 592.1784524917603 and batch: 650, loss is 4.916513681411743 and perplexity is 136.52581005622378
At time: 593.7967088222504 and batch: 700, loss is 4.905904607772827 and perplexity is 135.08505374200294
At time: 595.4140424728394 and batch: 750, loss is 4.874198608398437 and perplexity is 130.86923367084793
At time: 597.0288305282593 and batch: 800, loss is 4.873481531143188 and perplexity is 130.77542395839598
At time: 598.6409711837769 and batch: 850, loss is 4.928791618347168 and perplexity is 138.21239806536767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.020430564880371 and perplexity of 151.47651022074896
Finished 20 epochs...
Completing Train Step...
At time: 602.7903680801392 and batch: 50, loss is 4.991344776153564 and perplexity is 147.13415301527502
At time: 604.438719034195 and batch: 100, loss is 4.937592239379883 and perplexity is 139.43412107809618
At time: 606.0548377037048 and batch: 150, loss is 4.930232772827148 and perplexity is 138.41172707947996
At time: 607.6687123775482 and batch: 200, loss is 4.954819965362549 and perplexity is 141.85706488860436
At time: 609.2860414981842 and batch: 250, loss is 4.957209606170654 and perplexity is 142.1964576716972
At time: 610.9069902896881 and batch: 300, loss is 4.917232913970947 and perplexity is 136.62403918454714
At time: 612.52108335495 and batch: 350, loss is 4.875325593948364 and perplexity is 131.01680454562765
At time: 614.1420221328735 and batch: 400, loss is 4.897798986434936 and perplexity is 133.9945311004626
At time: 615.7612953186035 and batch: 450, loss is 4.943260450363159 and perplexity is 140.2267072453869
At time: 617.4015972614288 and batch: 500, loss is 4.939049634933472 and perplexity is 139.63747989729893
At time: 619.0155806541443 and batch: 550, loss is 4.915307779312133 and perplexity is 136.3612725232435
At time: 620.6384234428406 and batch: 600, loss is 4.939220275878906 and perplexity is 139.66130980200757
At time: 622.2540993690491 and batch: 650, loss is 4.9160618019104 and perplexity is 136.4641307781054
At time: 623.8666145801544 and batch: 700, loss is 4.90655686378479 and perplexity is 135.17319252183214
At time: 625.4788320064545 and batch: 750, loss is 4.876165800094604 and perplexity is 131.12693192844088
At time: 627.1007475852966 and batch: 800, loss is 4.876011352539063 and perplexity is 131.1066812582124
At time: 628.7178530693054 and batch: 850, loss is 4.930387229919433 and perplexity is 138.43310740351166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.020044326782227 and perplexity of 151.41801551869617
Finished 21 epochs...
Completing Train Step...
At time: 632.8617031574249 and batch: 50, loss is 4.985929956436157 and perplexity is 146.33960122115656
At time: 634.4974327087402 and batch: 100, loss is 4.932645902633667 and perplexity is 138.74613586729666
At time: 636.101466178894 and batch: 150, loss is 4.924711036682129 and perplexity is 137.64956022182452
At time: 637.7158203125 and batch: 200, loss is 4.949584045410156 and perplexity is 141.11625375849007
At time: 639.336837053299 and batch: 250, loss is 4.952047176361084 and perplexity is 141.4642699996375
At time: 640.94713306427 and batch: 300, loss is 4.912884855270386 and perplexity is 136.0312794531922
At time: 642.5568408966064 and batch: 350, loss is 4.87129921913147 and perplexity is 130.49034236149208
At time: 644.1690924167633 and batch: 400, loss is 4.8944719123840335 and perplexity is 133.54946217227086
At time: 645.7843072414398 and batch: 450, loss is 4.940391788482666 and perplexity is 139.82502066264288
At time: 647.3936548233032 and batch: 500, loss is 4.936420307159424 and perplexity is 139.27080945274267
At time: 649.0022854804993 and batch: 550, loss is 4.913250942230224 and perplexity is 136.08108784727563
At time: 650.6146004199982 and batch: 600, loss is 4.9379496669769285 and perplexity is 139.4839675886679
At time: 652.2288587093353 and batch: 650, loss is 4.915800762176514 and perplexity is 136.42851286676935
At time: 653.8383965492249 and batch: 700, loss is 4.906877326965332 and perplexity is 135.21651749469024
At time: 655.4473261833191 and batch: 750, loss is 4.877134981155396 and perplexity is 131.25407927187166
At time: 657.0601830482483 and batch: 800, loss is 4.87724925994873 and perplexity is 131.26907968676935
At time: 658.704737663269 and batch: 850, loss is 4.930899000167846 and perplexity is 138.50397148079267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019845326741536 and perplexity of 151.38788632540147
Finished 22 epochs...
Completing Train Step...
At time: 662.8894202709198 and batch: 50, loss is 4.981573524475098 and perplexity is 145.70346934391748
At time: 664.5073750019073 and batch: 100, loss is 4.928675546646118 and perplexity is 138.19635644822495
At time: 666.1204888820648 and batch: 150, loss is 4.92027060508728 and perplexity is 137.03969180717337
At time: 667.727301120758 and batch: 200, loss is 4.945535688400269 and perplexity is 140.5461196153251
At time: 669.340003490448 and batch: 250, loss is 4.947985467910766 and perplexity is 140.89084870216013
At time: 670.9582362174988 and batch: 300, loss is 4.909160852432251 and perplexity is 135.525640666929
At time: 672.5717384815216 and batch: 350, loss is 4.868010063171386 and perplexity is 130.06184435887045
At time: 674.1876394748688 and batch: 400, loss is 4.891802854537964 and perplexity is 133.1934862039744
At time: 675.8024282455444 and batch: 450, loss is 4.938039293289185 and perplexity is 139.49646958254718
At time: 677.4234855175018 and batch: 500, loss is 4.934307460784912 and perplexity is 138.97686226988614
At time: 679.0372877120972 and batch: 550, loss is 4.911538534164428 and perplexity is 135.8482608991097
At time: 680.6489136219025 and batch: 600, loss is 4.936642866134644 and perplexity is 139.30180887084393
At time: 682.2646522521973 and batch: 650, loss is 4.915337181091308 and perplexity is 136.36528184620656
At time: 683.8744413852692 and batch: 700, loss is 4.906729078292846 and perplexity is 135.19647331127203
At time: 685.4845514297485 and batch: 750, loss is 4.877672061920166 and perplexity is 131.32459224703254
At time: 687.0965943336487 and batch: 800, loss is 4.877816953659058 and perplexity is 131.34362147411804
At time: 688.7103037834167 and batch: 850, loss is 4.930883798599243 and perplexity is 138.50186601917162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019741694132487 and perplexity of 151.37219841666666
Finished 23 epochs...
Completing Train Step...
At time: 692.892302274704 and batch: 50, loss is 4.978125019073486 and perplexity is 145.20187551445386
At time: 694.5024464130402 and batch: 100, loss is 4.925318746566773 and perplexity is 137.73323664310715
At time: 696.1157705783844 and batch: 150, loss is 4.916538915634155 and perplexity is 136.5292552223476
At time: 697.7268414497375 and batch: 200, loss is 4.942269678115845 and perplexity is 140.08784331813803
At time: 699.3689813613892 and batch: 250, loss is 4.944517059326172 and perplexity is 140.4030281425733
At time: 700.9866132736206 and batch: 300, loss is 4.905990085601807 and perplexity is 135.09660101263515
At time: 702.6097886562347 and batch: 350, loss is 4.865366373062134 and perplexity is 129.71845525453546
At time: 704.2251200675964 and batch: 400, loss is 4.889651651382446 and perplexity is 132.9072679233693
At time: 705.8381490707397 and batch: 450, loss is 4.936180477142334 and perplexity is 139.23741213713092
At time: 707.4506611824036 and batch: 500, loss is 4.932628612518311 and perplexity is 138.7437369513411
At time: 709.0628175735474 and batch: 550, loss is 4.910093669891357 and perplexity is 135.6521203327486
At time: 710.6747093200684 and batch: 600, loss is 4.935692520141601 and perplexity is 139.16948684077443
At time: 712.2857191562653 and batch: 650, loss is 4.914792957305909 and perplexity is 136.29108880696268
At time: 713.8925902843475 and batch: 700, loss is 4.906453714370728 and perplexity is 135.1592502053099
At time: 715.503749370575 and batch: 750, loss is 4.877806062698364 and perplexity is 131.34219102368874
At time: 717.1191854476929 and batch: 800, loss is 4.877921895980835 and perplexity is 131.35740570196796
At time: 718.7397639751434 and batch: 850, loss is 4.930593109130859 and perplexity is 138.46161083653055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019606272379558 and perplexity of 151.35170071616085
Finished 24 epochs...
Completing Train Step...
At time: 722.9275188446045 and batch: 50, loss is 4.9747092914581295 and perplexity is 144.70675154415898
At time: 724.537712097168 and batch: 100, loss is 4.922340183258057 and perplexity is 137.3235998454716
At time: 726.1551852226257 and batch: 150, loss is 4.913366317749023 and perplexity is 136.096789179142
At time: 727.7716348171234 and batch: 200, loss is 4.939224052429199 and perplexity is 139.66183724096396
At time: 729.3878605365753 and batch: 250, loss is 4.9414448547363286 and perplexity is 139.97234322991798
At time: 730.9991171360016 and batch: 300, loss is 4.903304891586304 and perplexity is 134.73432703371563
At time: 732.6070864200592 and batch: 350, loss is 4.862753915786743 and perplexity is 129.38001360701222
At time: 734.2163138389587 and batch: 400, loss is 4.88749589920044 and perplexity is 132.62106139655114
At time: 735.8342475891113 and batch: 450, loss is 4.934309921264648 and perplexity is 138.9772042200603
At time: 737.4480741024017 and batch: 500, loss is 4.930911178588867 and perplexity is 138.50565825074153
At time: 739.0602290630341 and batch: 550, loss is 4.908605175018311 and perplexity is 135.45035304915223
At time: 740.6998331546783 and batch: 600, loss is 4.934664983749389 and perplexity is 139.02655857293405
At time: 742.3195736408234 and batch: 650, loss is 4.914299507141113 and perplexity is 136.22385253696925
At time: 743.9323117733002 and batch: 700, loss is 4.905938930511475 and perplexity is 135.08969031056714
At time: 745.5443613529205 and batch: 750, loss is 4.877505369186402 and perplexity is 131.30270321616223
At time: 747.1590101718903 and batch: 800, loss is 4.8776215648651124 and perplexity is 131.31796090930084
At time: 748.7739071846008 and batch: 850, loss is 4.9300520133972165 and perplexity is 138.3867101156914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019423802693685 and perplexity of 151.3240861383631
Finished 25 epochs...
Completing Train Step...
At time: 752.9212145805359 and batch: 50, loss is 4.971827001571655 and perplexity is 144.29026524313858
At time: 754.5565550327301 and batch: 100, loss is 4.919626665115357 and perplexity is 136.95147487813037
At time: 756.1700458526611 and batch: 150, loss is 4.910532627105713 and perplexity is 135.71167888048208
At time: 757.7824356555939 and batch: 200, loss is 4.9363600730896 and perplexity is 139.26242085772375
At time: 759.3936326503754 and batch: 250, loss is 4.93870062828064 and perplexity is 139.58875399115738
At time: 761.010758638382 and batch: 300, loss is 4.900694541931152 and perplexity is 134.38308196527416
At time: 762.6243081092834 and batch: 350, loss is 4.860541105270386 and perplexity is 129.0940366753683
At time: 764.2352283000946 and batch: 400, loss is 4.885690774917602 and perplexity is 132.38187983937422
At time: 765.8467400074005 and batch: 450, loss is 4.932674865722657 and perplexity is 138.7501544421716
At time: 767.459566116333 and batch: 500, loss is 4.929324846267701 and perplexity is 138.28611642757772
At time: 769.0703537464142 and batch: 550, loss is 4.9071805286407475 and perplexity is 135.25752158527885
At time: 770.6839759349823 and batch: 600, loss is 4.933695697784424 and perplexity is 138.89186736864426
At time: 772.2959682941437 and batch: 650, loss is 4.913382835388184 and perplexity is 136.09903719536248
At time: 773.9072892665863 and batch: 700, loss is 4.905293073654175 and perplexity is 135.002469876719
At time: 775.5160644054413 and batch: 750, loss is 4.877019233703614 and perplexity is 131.23888782586243
At time: 777.1313076019287 and batch: 800, loss is 4.877275171279908 and perplexity is 131.2724810874337
At time: 778.7431311607361 and batch: 850, loss is 4.9293318939208985 and perplexity is 138.2870910236027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019300778706868 and perplexity of 151.3054707910756
Finished 26 epochs...
Completing Train Step...
At time: 782.9041006565094 and batch: 50, loss is 4.96891902923584 and perplexity is 143.8712826336565
At time: 784.5427460670471 and batch: 100, loss is 4.916993980407715 and perplexity is 136.59139901561278
At time: 786.1573786735535 and batch: 150, loss is 4.9079366874694825 and perplexity is 135.35983643264086
At time: 787.7663505077362 and batch: 200, loss is 4.933887910842896 and perplexity is 138.91856676517213
At time: 789.3742325305939 and batch: 250, loss is 4.935971336364746 and perplexity is 139.20829496138776
At time: 790.983705997467 and batch: 300, loss is 4.8984317874908445 and perplexity is 134.0793498150846
At time: 792.5879790782928 and batch: 350, loss is 4.858443899154663 and perplexity is 128.8235835692307
At time: 794.2009422779083 and batch: 400, loss is 4.883781833648682 and perplexity is 132.1294116558262
At time: 795.8165838718414 and batch: 450, loss is 4.930986595153809 and perplexity is 138.51610426560808
At time: 797.4279046058655 and batch: 500, loss is 4.9278326416015625 and perplexity is 138.07991912193438
At time: 799.0375361442566 and batch: 550, loss is 4.9057762336730955 and perplexity is 135.06771343288867
At time: 800.6502683162689 and batch: 600, loss is 4.932618150711059 and perplexity is 138.74228544870041
At time: 802.265367269516 and batch: 650, loss is 4.9125580215454105 and perplexity is 135.9868271080738
At time: 803.875735282898 and batch: 700, loss is 4.90447811126709 and perplexity is 134.89249276129908
At time: 805.4879324436188 and batch: 750, loss is 4.876340436935425 and perplexity is 131.14983352125205
At time: 807.1000618934631 and batch: 800, loss is 4.8767224884033205 and perplexity is 131.19994908041502
At time: 808.7174942493439 and batch: 850, loss is 4.92846113204956 and perplexity is 138.16672830867765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019178708394368 and perplexity of 151.28700201224092
Finished 27 epochs...
Completing Train Step...
At time: 812.9061059951782 and batch: 50, loss is 4.966413707733154 and perplexity is 143.5112899526529
At time: 814.51828789711 and batch: 100, loss is 4.914686498641967 and perplexity is 136.27658021203771
At time: 816.1349172592163 and batch: 150, loss is 4.905396490097046 and perplexity is 135.01643207388045
At time: 817.7503201961517 and batch: 200, loss is 4.931393003463745 and perplexity is 138.57240980218558
At time: 819.3703656196594 and batch: 250, loss is 4.933541431427002 and perplexity is 138.8704426787866
At time: 820.9838180541992 and batch: 300, loss is 4.896224031448364 and perplexity is 133.78366184391422
At time: 822.6241748332977 and batch: 350, loss is 4.856621475219726 and perplexity is 128.58902618356493
At time: 824.2375495433807 and batch: 400, loss is 4.882184143066406 and perplexity is 131.91847828717974
At time: 825.8520588874817 and batch: 450, loss is 4.929474687576294 and perplexity is 138.30683895272983
At time: 827.4702486991882 and batch: 500, loss is 4.926522493362427 and perplexity is 137.89913241359042
At time: 829.083517074585 and batch: 550, loss is 4.90441556930542 and perplexity is 134.88405658399753
At time: 830.6982412338257 and batch: 600, loss is 4.931643838882446 and perplexity is 138.60717303036097
At time: 832.3124804496765 and batch: 650, loss is 4.9116316986083985 and perplexity is 135.8609177163742
At time: 833.9303195476532 and batch: 700, loss is 4.9036781787872314 and perplexity is 134.78463102185626
At time: 835.5447161197662 and batch: 750, loss is 4.875645294189453 and perplexity is 131.05869734583987
At time: 837.1571123600006 and batch: 800, loss is 4.876085681915283 and perplexity is 131.11642669823038
At time: 838.771497964859 and batch: 850, loss is 4.927654676437378 and perplexity is 138.05534789293347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.019020080566406 and perplexity of 151.26300558701357
Finished 28 epochs...
Completing Train Step...
At time: 842.9710395336151 and batch: 50, loss is 4.963872470855713 and perplexity is 143.1470567677115
At time: 844.5837433338165 and batch: 100, loss is 4.91232668876648 and perplexity is 135.955372535838
At time: 846.1986610889435 and batch: 150, loss is 4.903212556838989 and perplexity is 134.72188694800928
At time: 847.8137540817261 and batch: 200, loss is 4.928967208862304 and perplexity is 138.2366689823513
At time: 849.4275548458099 and batch: 250, loss is 4.931122093200684 and perplexity is 138.53487419881273
At time: 851.0415709018707 and batch: 300, loss is 4.8942492580413814 and perplexity is 133.51973011466805
At time: 852.6547079086304 and batch: 350, loss is 4.854652271270752 and perplexity is 128.3360573207974
At time: 854.2639467716217 and batch: 400, loss is 4.880548133850097 and perplexity is 131.70283488629403
At time: 855.8761641979218 and batch: 450, loss is 4.927927179336548 and perplexity is 138.09297350179114
At time: 857.490371465683 and batch: 500, loss is 4.925047645568847 and perplexity is 137.69590208615222
At time: 859.101188659668 and batch: 550, loss is 4.9030631732940675 and perplexity is 134.70176321807295
At time: 860.7167870998383 and batch: 600, loss is 4.93048996925354 and perplexity is 138.44733065941602
At time: 862.3535676002502 and batch: 650, loss is 4.910700740814209 and perplexity is 135.734495791969
At time: 863.9639027118683 and batch: 700, loss is 4.902663965225219 and perplexity is 134.64799991939353
At time: 865.569840669632 and batch: 750, loss is 4.874816684722901 and perplexity is 130.9501458481553
At time: 867.1764698028564 and batch: 800, loss is 4.875058031082153 and perplexity is 130.98175400320247
At time: 868.7907793521881 and batch: 850, loss is 4.926440076828003 and perplexity is 137.8877677133229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018815358479817 and perplexity of 151.23204187847176
Finished 29 epochs...
Completing Train Step...
At time: 872.9939465522766 and batch: 50, loss is 4.961430568695068 and perplexity is 142.79793209812547
At time: 874.6070251464844 and batch: 100, loss is 4.909964094161987 and perplexity is 135.6345442490574
At time: 876.2238063812256 and batch: 150, loss is 4.901064538955689 and perplexity is 134.43281250525797
At time: 877.8391149044037 and batch: 200, loss is 4.926760158538818 and perplexity is 137.93191013012634
At time: 879.4515171051025 and batch: 250, loss is 4.928839073181153 and perplexity is 138.21895706739946
At time: 881.0631401538849 and batch: 300, loss is 4.89215054512024 and perplexity is 133.23980437647754
At time: 882.669483423233 and batch: 350, loss is 4.852696046829224 and perplexity is 128.0852485878071
At time: 884.2814347743988 and batch: 400, loss is 4.878931245803833 and perplexity is 131.49005821129404
At time: 885.8952238559723 and batch: 450, loss is 4.926588163375855 and perplexity is 137.90818854882275
At time: 887.5106737613678 and batch: 500, loss is 4.923545045852661 and perplexity is 137.48915563023996
At time: 889.121568441391 and batch: 550, loss is 4.901639652252197 and perplexity is 134.5101488396623
At time: 890.7333035469055 and batch: 600, loss is 4.929394025802612 and perplexity is 138.2956833277099
At time: 892.3462455272675 and batch: 650, loss is 4.90967363357544 and perplexity is 135.5951534807885
At time: 893.9676132202148 and batch: 700, loss is 4.901558713912964 and perplexity is 134.49926225218226
At time: 895.5796525478363 and batch: 750, loss is 4.873893671035766 and perplexity is 130.82933283581448
At time: 897.189600944519 and batch: 800, loss is 4.87422456741333 and perplexity is 130.8726309513287
At time: 898.7986359596252 and batch: 850, loss is 4.925424690246582 and perplexity is 137.74782938201977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018707275390625 and perplexity of 151.21569713551173
Finished 30 epochs...
Completing Train Step...
At time: 902.9482936859131 and batch: 50, loss is 4.959032850265503 and perplexity is 142.45595301324522
At time: 904.5961980819702 and batch: 100, loss is 4.907695379257202 and perplexity is 135.32717693315792
At time: 906.2129213809967 and batch: 150, loss is 4.899023351669311 and perplexity is 134.15868982054616
At time: 907.8260776996613 and batch: 200, loss is 4.924611120223999 and perplexity is 137.6358074523795
At time: 909.4401326179504 and batch: 250, loss is 4.9266667079925535 and perplexity is 137.91902092003858
At time: 911.0581674575806 and batch: 300, loss is 4.890085315704345 and perplexity is 132.96491756298116
At time: 912.6747307777405 and batch: 350, loss is 4.850982608795166 and perplexity is 127.86597036474643
At time: 914.2882332801819 and batch: 400, loss is 4.877437152862549 and perplexity is 131.29374653393634
At time: 915.9032135009766 and batch: 450, loss is 4.9253028392791744 and perplexity is 137.73104569832606
At time: 917.5211043357849 and batch: 500, loss is 4.922253751754761 and perplexity is 137.31173127321563
At time: 919.1353750228882 and batch: 550, loss is 4.9003370571136475 and perplexity is 134.3350506394885
At time: 920.7514803409576 and batch: 600, loss is 4.9283068656921385 and perplexity is 138.1454154747535
At time: 922.3766629695892 and batch: 650, loss is 4.908926029205322 and perplexity is 135.4938198349374
At time: 923.9935495853424 and batch: 700, loss is 4.900488109588623 and perplexity is 134.35534381399205
At time: 925.6078143119812 and batch: 750, loss is 4.8729427051544185 and perplexity is 130.70497774210534
At time: 927.22585105896 and batch: 800, loss is 4.873301076889038 and perplexity is 130.75182710594464
At time: 928.8469219207764 and batch: 850, loss is 4.924398717880249 and perplexity is 137.60657638877572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018636067708333 and perplexity of 151.2049297995556
Finished 31 epochs...
Completing Train Step...
At time: 932.9843804836273 and batch: 50, loss is 4.956952419281006 and perplexity is 142.1598913094257
At time: 934.619482755661 and batch: 100, loss is 4.905677995681763 and perplexity is 135.05444530375496
At time: 936.2293193340302 and batch: 150, loss is 4.897117881774903 and perplexity is 133.9032978741258
At time: 937.8399693965912 and batch: 200, loss is 4.922765340805054 and perplexity is 137.38199642331992
At time: 939.4533967971802 and batch: 250, loss is 4.924684019088745 and perplexity is 137.6458413122152
At time: 941.0714957714081 and batch: 300, loss is 4.8881552791595455 and perplexity is 132.70853790354974
At time: 942.6827321052551 and batch: 350, loss is 4.849353008270263 and perplexity is 127.657769600435
At time: 944.3202908039093 and batch: 400, loss is 4.87589129447937 and perplexity is 131.09094178929172
At time: 945.931295633316 and batch: 450, loss is 4.923838624954223 and perplexity is 137.52952549860936
At time: 947.5446882247925 and batch: 500, loss is 4.920773096084595 and perplexity is 137.1085703225489
At time: 949.1611328125 and batch: 550, loss is 4.898902139663696 and perplexity is 134.14242916219575
At time: 950.7733290195465 and batch: 600, loss is 4.9271515846252445 and perplexity is 137.9859108458568
At time: 952.3875677585602 and batch: 650, loss is 4.907875766754151 and perplexity is 135.35159046575583
At time: 954.0083193778992 and batch: 700, loss is 4.89936559677124 and perplexity is 134.2046128330332
At time: 955.6248824596405 and batch: 750, loss is 4.872006778717041 and perplexity is 130.58270472613927
At time: 957.2387731075287 and batch: 800, loss is 4.872343492507935 and perplexity is 130.6266811269876
At time: 958.8556714057922 and batch: 850, loss is 4.923373384475708 and perplexity is 137.4655560780921
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018528938293457 and perplexity of 151.18873217153651
Finished 32 epochs...
Completing Train Step...
At time: 963.0007584095001 and batch: 50, loss is 4.954926176071167 and perplexity is 141.87213242814155
At time: 964.6420383453369 and batch: 100, loss is 4.903749313354492 and perplexity is 134.7942192092791
At time: 966.2574343681335 and batch: 150, loss is 4.895211095809937 and perplexity is 133.64821621542632
At time: 967.8684089183807 and batch: 200, loss is 4.920709476470948 and perplexity is 137.09984780574175
At time: 969.4797723293304 and batch: 250, loss is 4.922668199539185 and perplexity is 137.36865161045628
At time: 971.0947551727295 and batch: 300, loss is 4.886402187347412 and perplexity is 132.47609146185968
At time: 972.7076699733734 and batch: 350, loss is 4.847966470718384 and perplexity is 127.48088996256224
At time: 974.3179273605347 and batch: 400, loss is 4.874638805389404 and perplexity is 130.92685459506825
At time: 975.9291033744812 and batch: 450, loss is 4.922386894226074 and perplexity is 137.3300145135685
At time: 977.5417232513428 and batch: 500, loss is 4.919461793899536 and perplexity is 136.9288973831995
At time: 979.1546411514282 and batch: 550, loss is 4.897562627792358 and perplexity is 133.96286407751353
At time: 980.7636475563049 and batch: 600, loss is 4.92604250907898 and perplexity is 137.83295887972946
At time: 982.3730864524841 and batch: 650, loss is 4.906920957565307 and perplexity is 135.22241720117808
At time: 983.9868671894073 and batch: 700, loss is 4.898438339233398 and perplexity is 134.08022827134408
At time: 985.630090713501 and batch: 750, loss is 4.871043138504028 and perplexity is 130.45693059098122
At time: 987.2435338497162 and batch: 800, loss is 4.871288404464722 and perplexity is 130.48893115955647
At time: 988.8573865890503 and batch: 850, loss is 4.92219838142395 and perplexity is 137.3041284877171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.018369992574056 and perplexity of 151.16470327943148
Finished 33 epochs...
Completing Train Step...
At time: 993.045859336853 and batch: 50, loss is 4.953072290420533 and perplexity is 141.60936136661795
At time: 994.6566300392151 and batch: 100, loss is 4.901931715011597 and perplexity is 134.5494399823601
At time: 996.2766697406769 and batch: 150, loss is 4.893409872055054 and perplexity is 133.40770254807393
At time: 997.8929679393768 and batch: 200, loss is 4.9188962745666505 and perplexity is 136.85148333613185
At time: 999.5010924339294 and batch: 250, loss is 4.92075930595398 and perplexity is 137.10667959049243
At time: 1001.1105220317841 and batch: 300, loss is 4.884490051269531 and perplexity is 132.22302117743706
At time: 1002.7227375507355 and batch: 350, loss is 4.84625168800354 and perplexity is 127.26247525638328
At time: 1004.3252515792847 and batch: 400, loss is 4.873223085403442 and perplexity is 130.74162997435417
At time: 1005.9320628643036 and batch: 450, loss is 4.920910291671753 and perplexity is 137.12738230378955
At time: 1007.5410141944885 and batch: 500, loss is 4.918225078582764 and perplexity is 136.75965998930576
At time: 1009.1433563232422 and batch: 550, loss is 4.896236095428467 and perplexity is 133.78527581708425
At time: 1010.7437679767609 and batch: 600, loss is 4.9247811794281 and perplexity is 137.6592156785864
At time: 1012.3445150852203 and batch: 650, loss is 4.905770387649536 and perplexity is 135.06692382616185
At time: 1013.945562839508 and batch: 700, loss is 4.897341136932373 and perplexity is 133.93319581328888
At time: 1015.5475208759308 and batch: 750, loss is 4.870077953338623 and perplexity is 130.33107624299572
At time: 1017.1510636806488 and batch: 800, loss is 4.870247077941895 and perplexity is 130.35312029860276
At time: 1018.7579307556152 and batch: 850, loss is 4.9210296058654786 and perplexity is 137.14374452294996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.01847489674886 and perplexity of 151.18056191969285
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1022.8993515968323 and batch: 50, loss is 4.952757558822632 and perplexity is 141.56479943892853
At time: 1024.5072960853577 and batch: 100, loss is 4.902721080780029 and perplexity is 134.65569063424064
At time: 1026.1524729728699 and batch: 150, loss is 4.892770395278931 and perplexity is 133.32241869196744
At time: 1027.7612147331238 and batch: 200, loss is 4.918662853240967 and perplexity is 136.8195430093915
At time: 1029.3705563545227 and batch: 250, loss is 4.917118768692017 and perplexity is 136.6084450854992
At time: 1030.9774758815765 and batch: 300, loss is 4.879963045120239 and perplexity is 131.62579958035394
At time: 1032.588482618332 and batch: 350, loss is 4.8408412170410156 and perplexity is 126.57578466427891
At time: 1034.1967494487762 and batch: 400, loss is 4.865231294631958 and perplexity is 129.70093427261477
At time: 1035.8062300682068 and batch: 450, loss is 4.911886491775513 and perplexity is 135.89553856027612
At time: 1037.4125938415527 and batch: 500, loss is 4.908907051086426 and perplexity is 135.4912484415151
At time: 1039.0244238376617 and batch: 550, loss is 4.885534448623657 and perplexity is 132.3611866881974
At time: 1040.6329431533813 and batch: 600, loss is 4.909993267059326 and perplexity is 135.63850115940934
At time: 1042.2415671348572 and batch: 650, loss is 4.888933897018433 and perplexity is 132.81190737862562
At time: 1043.849266052246 and batch: 700, loss is 4.880138082504272 and perplexity is 131.6488410324827
At time: 1045.4572110176086 and batch: 750, loss is 4.850850496292114 and perplexity is 127.84907878716524
At time: 1047.0666460990906 and batch: 800, loss is 4.849394006729126 and perplexity is 127.66300347954021
At time: 1048.6747558116913 and batch: 850, loss is 4.904196004867554 and perplexity is 134.85444409298054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.016805648803711 and perplexity of 150.92841458412113
Finished 35 epochs...
Completing Train Step...
At time: 1052.8238847255707 and batch: 50, loss is 4.950628442764282 and perplexity is 141.26371218973375
At time: 1054.4274260997772 and batch: 100, loss is 4.899771213531494 and perplexity is 134.25905951479857
At time: 1056.0335686206818 and batch: 150, loss is 4.88994875907898 and perplexity is 132.94676156223605
At time: 1057.6401937007904 and batch: 200, loss is 4.916144609451294 and perplexity is 136.4754315050816
At time: 1059.2430651187897 and batch: 250, loss is 4.9150121307373045 and perplexity is 136.32096346631138
At time: 1060.8467953205109 and batch: 300, loss is 4.87792441368103 and perplexity is 131.35773642095023
At time: 1062.4517602920532 and batch: 350, loss is 4.839281606674194 and perplexity is 126.37852961881819
At time: 1064.0569252967834 and batch: 400, loss is 4.863886489868164 and perplexity is 129.52662906784056
At time: 1065.6636941432953 and batch: 450, loss is 4.910805788040161 and perplexity is 135.74875507315375
At time: 1067.2947888374329 and batch: 500, loss is 4.908049039840698 and perplexity is 135.37504528559074
At time: 1068.8995296955109 and batch: 550, loss is 4.884943933486938 and perplexity is 132.28304847711274
At time: 1070.507210969925 and batch: 600, loss is 4.90953932762146 and perplexity is 135.57694346722926
At time: 1072.10972738266 and batch: 650, loss is 4.888527326583862 and perplexity is 132.75792095911123
At time: 1073.7125344276428 and batch: 700, loss is 4.880119342803955 and perplexity is 131.6463739957704
At time: 1075.3179142475128 and batch: 750, loss is 4.851505289077759 and perplexity is 127.93282085546537
At time: 1076.9270570278168 and batch: 800, loss is 4.850424194335938 and perplexity is 127.79458809035346
At time: 1078.5341091156006 and batch: 850, loss is 4.905341892242432 and perplexity is 135.00906066758486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.016617139180501 and perplexity of 150.89996580707458
Finished 36 epochs...
Completing Train Step...
At time: 1082.6756007671356 and batch: 50, loss is 4.9494825458526615 and perplexity is 141.10193124805477
At time: 1084.3041682243347 and batch: 100, loss is 4.898545150756836 and perplexity is 134.0945503496565
At time: 1085.916648864746 and batch: 150, loss is 4.888512334823608 and perplexity is 132.75593069910724
At time: 1087.533338546753 and batch: 200, loss is 4.914907350540161 and perplexity is 136.30668047718484
At time: 1089.1446621418 and batch: 250, loss is 4.9137688159942625 and perplexity is 136.15157892361762
At time: 1090.7549221515656 and batch: 300, loss is 4.876801786422729 and perplexity is 131.2103533890387
At time: 1092.3676900863647 and batch: 350, loss is 4.838401784896851 and perplexity is 126.26738793587688
At time: 1093.9818539619446 and batch: 400, loss is 4.8631807231903075 and perplexity is 129.4352457405963
At time: 1095.5961265563965 and batch: 450, loss is 4.910202760696411 and perplexity is 135.66691953897688
At time: 1097.2048738002777 and batch: 500, loss is 4.907574605941773 and perplexity is 135.31083400824647
At time: 1098.8120539188385 and batch: 550, loss is 4.884770298004151 and perplexity is 132.2600814401298
At time: 1100.4170308113098 and batch: 600, loss is 4.909352474212646 and perplexity is 135.55161281981842
At time: 1102.0211734771729 and batch: 650, loss is 4.8884005451202395 and perplexity is 132.74109078248432
At time: 1103.6315548419952 and batch: 700, loss is 4.880213842391968 and perplexity is 131.65881511170738
At time: 1105.2368738651276 and batch: 750, loss is 4.852139043807983 and perplexity is 128.01392458305443
At time: 1106.869255065918 and batch: 800, loss is 4.851194610595703 and perplexity is 127.89308105434714
At time: 1108.4811346530914 and batch: 850, loss is 4.906075086593628 and perplexity is 135.10808484577458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.01655642191569 and perplexity of 150.890803852038
Finished 37 epochs...
Completing Train Step...
At time: 1112.6416018009186 and batch: 50, loss is 4.948610429763794 and perplexity is 140.97892762815727
At time: 1114.2856776714325 and batch: 100, loss is 4.89760269165039 and perplexity is 133.96823125419567
At time: 1115.9050867557526 and batch: 150, loss is 4.887425346374512 and perplexity is 132.61170493595827
At time: 1117.527194738388 and batch: 200, loss is 4.913974161148071 and perplexity is 136.17953986125627
At time: 1119.1448922157288 and batch: 250, loss is 4.912809982299804 and perplexity is 136.02109476849108
At time: 1120.761664390564 and batch: 300, loss is 4.875924692153931 and perplexity is 131.09531999501405
At time: 1122.3789947032928 and batch: 350, loss is 4.837717437744141 and perplexity is 126.18100676918684
At time: 1123.9970989227295 and batch: 400, loss is 4.862621774673462 and perplexity is 129.36291831749807
At time: 1125.6190407276154 and batch: 450, loss is 4.909710617065429 and perplexity is 135.60016835552312
At time: 1127.2348623275757 and batch: 500, loss is 4.907166395187378 and perplexity is 135.25560994291376
At time: 1128.8506953716278 and batch: 550, loss is 4.884717674255371 and perplexity is 132.25312160195847
At time: 1130.4690744876862 and batch: 600, loss is 4.90916488647461 and perplexity is 135.5261873842069
At time: 1132.0829367637634 and batch: 650, loss is 4.888271007537842 and perplexity is 132.7238969361482
At time: 1133.699922800064 and batch: 700, loss is 4.88023045539856 and perplexity is 131.66100237863918
At time: 1135.3166136741638 and batch: 750, loss is 4.852662601470947 and perplexity is 128.08096480241488
At time: 1136.9339661598206 and batch: 800, loss is 4.8517147827148435 and perplexity is 127.95962477493616
At time: 1138.5514450073242 and batch: 850, loss is 4.906567125320435 and perplexity is 135.1745796134823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.01649284362793 and perplexity of 150.88121077804908
Finished 38 epochs...
Completing Train Step...
At time: 1142.7138421535492 and batch: 50, loss is 4.947826595306396 and perplexity is 140.8684667840788
At time: 1144.3536047935486 and batch: 100, loss is 4.896744804382324 and perplexity is 133.85335089852035
At time: 1145.9645218849182 and batch: 150, loss is 4.886487398147583 and perplexity is 132.4873803365769
At time: 1147.6045987606049 and batch: 200, loss is 4.91317081451416 and perplexity is 136.07018441734624
At time: 1149.2101783752441 and batch: 250, loss is 4.911982164382935 and perplexity is 135.90854066274983
At time: 1150.817535161972 and batch: 300, loss is 4.875142822265625 and perplexity is 130.99286057200052
At time: 1152.4324162006378 and batch: 350, loss is 4.837107162475586 and perplexity is 126.10402511378372
At time: 1154.0522918701172 and batch: 400, loss is 4.8621413612365725 and perplexity is 129.30078555920406
At time: 1155.6695108413696 and batch: 450, loss is 4.9092823505401615 and perplexity is 135.54210777618445
At time: 1157.2851648330688 and batch: 500, loss is 4.906836986541748 and perplexity is 135.21106291311978
At time: 1158.9012310504913 and batch: 550, loss is 4.884673452377319 and perplexity is 132.24727324985653
At time: 1160.5186257362366 and batch: 600, loss is 4.909055147171021 and perplexity is 135.51131565080703
At time: 1162.1370663642883 and batch: 650, loss is 4.888186082839966 and perplexity is 132.7126258779024
At time: 1163.7505939006805 and batch: 700, loss is 4.880311365127564 and perplexity is 131.67165546562538
At time: 1165.3667027950287 and batch: 750, loss is 4.8530603122711184 and perplexity is 128.13191411628776
At time: 1166.9840731620789 and batch: 800, loss is 4.85213885307312 and perplexity is 128.01390016633832
At time: 1168.6024692058563 and batch: 850, loss is 4.906945695877075 and perplexity is 135.22576241687014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.016490300496419 and perplexity of 150.8808270677755
Finished 39 epochs...
Completing Train Step...
At time: 1172.7697546482086 and batch: 50, loss is 4.9471923446655275 and perplexity is 140.77914919660552
At time: 1174.3850696086884 and batch: 100, loss is 4.89604474067688 and perplexity is 133.75967781809172
At time: 1176.002275943756 and batch: 150, loss is 4.885720157623291 and perplexity is 132.38576963433417
At time: 1177.6159331798553 and batch: 200, loss is 4.91255744934082 and perplexity is 135.98674929580937
At time: 1179.2300388813019 and batch: 250, loss is 4.91126690864563 and perplexity is 135.81136605578246
At time: 1180.8489685058594 and batch: 300, loss is 4.874504079818726 and perplexity is 130.9092165880378
At time: 1182.4634704589844 and batch: 350, loss is 4.836635723114013 and perplexity is 126.04458872411993
At time: 1184.075294494629 and batch: 400, loss is 4.8617589473724365 and perplexity is 129.25134859945445
At time: 1185.6852548122406 and batch: 450, loss is 4.908987827301026 and perplexity is 135.50219335371395
At time: 1187.2947437763214 and batch: 500, loss is 4.906642122268677 and perplexity is 135.18471767459025
At time: 1188.9328305721283 and batch: 550, loss is 4.884680242538452 and perplexity is 132.2481712332
At time: 1190.5468327999115 and batch: 600, loss is 4.908983068466187 and perplexity is 135.50154852268975
At time: 1192.1645605564117 and batch: 650, loss is 4.888154249191285 and perplexity is 132.70840121803812
At time: 1193.7753326892853 and batch: 700, loss is 4.880384302139282 and perplexity is 131.6812595529455
At time: 1195.3851308822632 and batch: 750, loss is 4.853409757614136 and perplexity is 128.17669704110418
At time: 1196.995893239975 and batch: 800, loss is 4.852486572265625 and perplexity is 128.05842079622377
At time: 1198.609665632248 and batch: 850, loss is 4.907251052856445 and perplexity is 135.26706085227656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.016498565673828 and perplexity of 150.88207412973244
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1202.7827050685883 and batch: 50, loss is 4.947028942108155 and perplexity is 140.75614740292914
At time: 1204.3940043449402 and batch: 100, loss is 4.895774803161621 and perplexity is 133.7235759358651
At time: 1206.0125179290771 and batch: 150, loss is 4.885880155563354 and perplexity is 132.4069527793539
At time: 1207.6254167556763 and batch: 200, loss is 4.912185115814209 and perplexity is 135.93612629477676
At time: 1209.2357578277588 and batch: 250, loss is 4.910396471023559 and perplexity is 135.69320216787014
At time: 1210.848680973053 and batch: 300, loss is 4.873478469848632 and perplexity is 130.7750236169154
At time: 1212.465856552124 and batch: 350, loss is 4.836141033172607 and perplexity is 125.98225115406515
At time: 1214.085652589798 and batch: 400, loss is 4.860136814117432 and perplexity is 129.04185564731608
At time: 1215.7013301849365 and batch: 450, loss is 4.907684192657471 and perplexity is 135.32566309066422
At time: 1217.3145616054535 and batch: 500, loss is 4.905804119110107 and perplexity is 135.07147990761862
At time: 1218.9290537834167 and batch: 550, loss is 4.883430366516113 and perplexity is 132.08298067034423
At time: 1220.5439584255219 and batch: 600, loss is 4.906827917098999 and perplexity is 135.20983662968655
At time: 1222.1522665023804 and batch: 650, loss is 4.885835313796997 and perplexity is 132.40101555083226
At time: 1223.7630715370178 and batch: 700, loss is 4.877883548736572 and perplexity is 131.35236860402583
At time: 1225.366203069687 and batch: 750, loss is 4.85089033126831 and perplexity is 127.85417175361381
At time: 1226.9745728969574 and batch: 800, loss is 4.849826030731201 and perplexity is 127.71816887675016
At time: 1228.5878558158875 and batch: 850, loss is 4.904872617721558 and perplexity is 134.94571921875283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015661557515462 and perplexity of 150.75583744067387
Finished 41 epochs...
Completing Train Step...
At time: 1232.784677028656 and batch: 50, loss is 4.946828050613403 and perplexity is 140.7278735301673
At time: 1234.404626607895 and batch: 100, loss is 4.895389862060547 and perplexity is 133.67211014159008
At time: 1236.0238618850708 and batch: 150, loss is 4.8854619407653805 and perplexity is 132.35158980996042
At time: 1237.6419606208801 and batch: 200, loss is 4.911807374954224 and perplexity is 135.88478736254376
At time: 1239.2597200870514 and batch: 250, loss is 4.910170049667358 and perplexity is 135.66248180701214
At time: 1240.8809778690338 and batch: 300, loss is 4.873361835479736 and perplexity is 130.75977164403784
At time: 1242.4945695400238 and batch: 350, loss is 4.835928945541382 and perplexity is 125.95553471005526
At time: 1244.107382774353 and batch: 400, loss is 4.860053043365479 and perplexity is 129.0310461668006
At time: 1245.7229902744293 and batch: 450, loss is 4.907659749984742 and perplexity is 135.32235541019386
At time: 1247.3392124176025 and batch: 500, loss is 4.905788698196411 and perplexity is 135.06939699804437
At time: 1248.9534275531769 and batch: 550, loss is 4.883388547897339 and perplexity is 132.07745725802056
At time: 1250.5696241855621 and batch: 600, loss is 4.906829242706299 and perplexity is 135.21001586495174
At time: 1252.1836805343628 and batch: 650, loss is 4.885732727050781 and perplexity is 132.38743365812417
At time: 1253.8035514354706 and batch: 700, loss is 4.87786153793335 and perplexity is 131.3494774647059
At time: 1255.417899608612 and batch: 750, loss is 4.8509368896484375 and perplexity is 127.86012457531896
At time: 1257.0394649505615 and batch: 800, loss is 4.849959669113159 and perplexity is 127.73523806670954
At time: 1258.652132987976 and batch: 850, loss is 4.904991207122802 and perplexity is 134.96172329973396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015570958455403 and perplexity of 150.74217972220097
Finished 42 epochs...
Completing Train Step...
At time: 1262.811543226242 and batch: 50, loss is 4.9466746807098385 and perplexity is 140.70629176481404
At time: 1264.4452636241913 and batch: 100, loss is 4.895209255218506 and perplexity is 133.64797022389124
At time: 1266.0594546794891 and batch: 150, loss is 4.885266551971435 and perplexity is 132.32573231866502
At time: 1267.6739978790283 and batch: 200, loss is 4.911601476669311 and perplexity is 135.85681179803933
At time: 1269.2878468036652 and batch: 250, loss is 4.910011796951294 and perplexity is 135.64101454946893
At time: 1270.9283974170685 and batch: 300, loss is 4.873243885040283 and perplexity is 130.7443493810589
At time: 1272.5434460639954 and batch: 350, loss is 4.835818281173706 and perplexity is 125.94159669168653
At time: 1274.1531145572662 and batch: 400, loss is 4.8599992275238035 and perplexity is 129.02410243929182
At time: 1275.7628486156464 and batch: 450, loss is 4.9076964473724365 and perplexity is 135.32732147825442
At time: 1277.376630306244 and batch: 500, loss is 4.9058333778381344 and perplexity is 135.07543198512954
At time: 1278.9912784099579 and batch: 550, loss is 4.883401031494141 and perplexity is 132.0791060700351
At time: 1280.600562095642 and batch: 600, loss is 4.906825265884399 and perplexity is 135.20947815986884
At time: 1282.2109253406525 and batch: 650, loss is 4.8857053565979 and perplexity is 132.3838102036972
At time: 1283.825255870819 and batch: 700, loss is 4.877867383956909 and perplexity is 131.35024533909018
At time: 1285.4352254867554 and batch: 750, loss is 4.851002912521363 and perplexity is 127.86856654675455
At time: 1287.0473849773407 and batch: 800, loss is 4.850062417984009 and perplexity is 127.74836339248328
At time: 1288.6549997329712 and batch: 850, loss is 4.90505521774292 and perplexity is 134.97036255983372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015538851420085 and perplexity of 150.737339915409
Finished 43 epochs...
Completing Train Step...
At time: 1292.7763543128967 and batch: 50, loss is 4.946530532836914 and perplexity is 140.6860107139194
At time: 1294.4134583473206 and batch: 100, loss is 4.895067024230957 and perplexity is 133.62896269286432
At time: 1296.0303981304169 and batch: 150, loss is 4.885104370117188 and perplexity is 132.30427322621762
At time: 1297.6403939723969 and batch: 200, loss is 4.91143832206726 and perplexity is 135.83464794209124
At time: 1299.2507934570312 and batch: 250, loss is 4.909870243072509 and perplexity is 135.6218153966265
At time: 1300.8639507293701 and batch: 300, loss is 4.873135251998901 and perplexity is 130.73014699618022
At time: 1302.4774434566498 and batch: 350, loss is 4.835729160308838 and perplexity is 125.93037316779898
At time: 1304.0871903896332 and batch: 400, loss is 4.859955797195434 and perplexity is 129.01849900183578
At time: 1305.7025508880615 and batch: 450, loss is 4.90773946762085 and perplexity is 135.3331434184713
At time: 1307.3139960765839 and batch: 500, loss is 4.905893507003785 and perplexity is 135.08355420234327
At time: 1308.9244933128357 and batch: 550, loss is 4.883425178527832 and perplexity is 132.08229542716595
At time: 1310.566425561905 and batch: 600, loss is 4.906823091506958 and perplexity is 135.20918416374926
At time: 1312.1788065433502 and batch: 650, loss is 4.88570203781128 and perplexity is 132.38337085080818
At time: 1313.7867419719696 and batch: 700, loss is 4.8778777599334715 and perplexity is 131.35160823322795
At time: 1315.396814584732 and batch: 750, loss is 4.851065540313721 and perplexity is 127.87657492356026
At time: 1317.0154919624329 and batch: 800, loss is 4.850152149200439 and perplexity is 127.75982692283806
At time: 1318.6293182373047 and batch: 850, loss is 4.905103750228882 and perplexity is 134.97691316601734
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015523274739583 and perplexity of 150.73499194631228
Finished 44 epochs...
Completing Train Step...
At time: 1322.7836918830872 and batch: 50, loss is 4.946400308609009 and perplexity is 140.6676911796466
At time: 1324.4272952079773 and batch: 100, loss is 4.894943656921387 and perplexity is 133.61247826409704
At time: 1326.0437049865723 and batch: 150, loss is 4.884955959320068 and perplexity is 132.28463930054517
At time: 1327.6589167118073 and batch: 200, loss is 4.911294298171997 and perplexity is 135.81508591571634
At time: 1329.2763924598694 and batch: 250, loss is 4.909737167358398 and perplexity is 135.60376862751346
At time: 1330.8946816921234 and batch: 300, loss is 4.873034648895263 and perplexity is 130.71699579919007
At time: 1332.5088684558868 and batch: 350, loss is 4.835649633407593 and perplexity is 125.92035871366286
At time: 1334.1287891864777 and batch: 400, loss is 4.859917001724243 and perplexity is 129.0134937654655
At time: 1335.749033689499 and batch: 450, loss is 4.907780876159668 and perplexity is 135.33874748222118
At time: 1337.3637413978577 and batch: 500, loss is 4.905957584381103 and perplexity is 135.09221027954186
At time: 1338.9790868759155 and batch: 550, loss is 4.883450365066528 and perplexity is 132.08562216490503
At time: 1340.5985753536224 and batch: 600, loss is 4.906822681427002 and perplexity is 135.20912871718434
At time: 1342.2142629623413 and batch: 650, loss is 4.885703773498535 and perplexity is 132.38360062713718
At time: 1343.8267962932587 and batch: 700, loss is 4.877889108657837 and perplexity is 131.35309891488342
At time: 1345.441498041153 and batch: 750, loss is 4.851121234893799 and perplexity is 127.88369715403536
At time: 1347.0622849464417 and batch: 800, loss is 4.850232973098755 and perplexity is 127.77015338740502
At time: 1348.6769571304321 and batch: 850, loss is 4.905142860412598 and perplexity is 134.98219224112074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015516599019368 and perplexity of 150.73398568503822
Finished 45 epochs...
Completing Train Step...
At time: 1352.8557543754578 and batch: 50, loss is 4.946279058456421 and perplexity is 140.65063623460506
At time: 1354.4608063697815 and batch: 100, loss is 4.894831438064575 and perplexity is 133.59748526579466
At time: 1356.0741975307465 and batch: 150, loss is 4.884816761016846 and perplexity is 132.2662267847373
At time: 1357.6928269863129 and batch: 200, loss is 4.911161289215088 and perplexity is 135.79702249413126
At time: 1359.3050470352173 and batch: 250, loss is 4.909609308242798 and perplexity is 135.58643155795954
At time: 1360.9157450199127 and batch: 300, loss is 4.87294002532959 and perplexity is 130.70462747613007
At time: 1362.5287764072418 and batch: 350, loss is 4.835576696395874 and perplexity is 125.91117479391158
At time: 1364.1438133716583 and batch: 400, loss is 4.859880752563477 and perplexity is 129.0088172193499
At time: 1365.760871887207 and batch: 450, loss is 4.907820310592651 and perplexity is 135.3440845942211
At time: 1367.3762130737305 and batch: 500, loss is 4.906022043228149 and perplexity is 135.10091844831757
At time: 1368.987515449524 and batch: 550, loss is 4.883474998474121 and perplexity is 132.0888759239484
At time: 1370.5983307361603 and batch: 600, loss is 4.906822681427002 and perplexity is 135.20912871718434
At time: 1372.2138679027557 and batch: 650, loss is 4.88570613861084 and perplexity is 132.3839137295903
At time: 1373.8314776420593 and batch: 700, loss is 4.87789981842041 and perplexity is 131.35450568291907
At time: 1375.4504747390747 and batch: 750, loss is 4.851171150207519 and perplexity is 127.89008066821478
At time: 1377.0628390312195 and batch: 800, loss is 4.850307102203369 and perplexity is 127.7796252255372
At time: 1378.6793546676636 and batch: 850, loss is 4.905176734924316 and perplexity is 134.9867647744193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015513102213542 and perplexity of 150.7334585984804
Finished 46 epochs...
Completing Train Step...
At time: 1382.8739829063416 and batch: 50, loss is 4.946164579391479 and perplexity is 140.6345356028957
At time: 1384.4860157966614 and batch: 100, loss is 4.894726810455322 and perplexity is 133.58350801152523
At time: 1386.1050753593445 and batch: 150, loss is 4.884684448242187 and perplexity is 132.24872743099738
At time: 1387.7161378860474 and batch: 200, loss is 4.911035442352295 and perplexity is 135.77993394016676
At time: 1389.3256876468658 and batch: 250, loss is 4.90948655128479 and perplexity is 135.5697884016268
At time: 1390.9371736049652 and batch: 300, loss is 4.872849798202514 and perplexity is 130.69283490511032
At time: 1392.5810680389404 and batch: 350, loss is 4.8355081844329835 and perplexity is 125.9025486676764
At time: 1394.1961376667023 and batch: 400, loss is 4.859845714569092 and perplexity is 129.00429708832522
At time: 1395.807171344757 and batch: 450, loss is 4.907857618331909 and perplexity is 135.34913407023092
At time: 1397.417320728302 and batch: 500, loss is 4.906085634231568 and perplexity is 135.10950992445188
At time: 1399.0318355560303 and batch: 550, loss is 4.883498849868775 and perplexity is 132.0920264654297
At time: 1400.6425445079803 and batch: 600, loss is 4.906822786331177 and perplexity is 135.20914290118722
At time: 1402.2524394989014 and batch: 650, loss is 4.885708141326904 and perplexity is 132.38417885724644
At time: 1403.8659925460815 and batch: 700, loss is 4.877909107208252 and perplexity is 131.3557258127212
At time: 1405.4828362464905 and batch: 750, loss is 4.851217222213745 and perplexity is 127.89597295654131
At time: 1407.0950429439545 and batch: 800, loss is 4.850376253128052 and perplexity is 127.78846161029568
At time: 1408.7010340690613 and batch: 850, loss is 4.905207118988037 and perplexity is 134.99086628319165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015511194864909 and perplexity of 150.73317109749843
Finished 47 epochs...
Completing Train Step...
At time: 1412.8296530246735 and batch: 50, loss is 4.946055221557617 and perplexity is 140.61915695561905
At time: 1414.4346697330475 and batch: 100, loss is 4.894627628326416 and perplexity is 133.57025957182896
At time: 1416.0514707565308 and batch: 150, loss is 4.884558057785034 and perplexity is 132.23201351014168
At time: 1417.6685676574707 and batch: 200, loss is 4.910915117263794 and perplexity is 135.7635971904789
At time: 1419.2851407527924 and batch: 250, loss is 4.9093680000305175 and perplexity is 135.55371738580874
At time: 1420.9044935703278 and batch: 300, loss is 4.8727631092071535 and perplexity is 130.6815057656145
At time: 1422.5183863639832 and batch: 350, loss is 4.835443449020386 and perplexity is 125.89439857804388
At time: 1424.1314108371735 and batch: 400, loss is 4.859811735153198 and perplexity is 128.9999136721358
At time: 1425.7423713207245 and batch: 450, loss is 4.907893047332764 and perplexity is 135.35392943976467
At time: 1427.3541417121887 and batch: 500, loss is 4.906148281097412 and perplexity is 135.11797437692735
At time: 1428.957153081894 and batch: 550, loss is 4.883521995544434 and perplexity is 132.095083860014
At time: 1430.567451953888 and batch: 600, loss is 4.906822252273559 and perplexity is 135.20907069173376
At time: 1432.1834590435028 and batch: 650, loss is 4.8857090950012205 and perplexity is 132.3843051086979
At time: 1433.8237552642822 and batch: 700, loss is 4.877917194366455 and perplexity is 131.35678811155228
At time: 1435.4342811107635 and batch: 750, loss is 4.851259746551514 and perplexity is 127.90141176373456
At time: 1437.0468380451202 and batch: 800, loss is 4.8504414558410645 and perplexity is 127.79679403632986
At time: 1438.6598737239838 and batch: 850, loss is 4.905234651565552 and perplexity is 134.99458298084627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015511512756348 and perplexity of 150.73321901429063
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1442.8276536464691 and batch: 50, loss is 4.946011371612549 and perplexity is 140.6129909485016
At time: 1444.4655814170837 and batch: 100, loss is 4.894573936462402 and perplexity is 133.5630881281416
At time: 1446.082914352417 and batch: 150, loss is 4.88457465171814 and perplexity is 132.23420777753404
At time: 1447.7005009651184 and batch: 200, loss is 4.9107347583770755 and perplexity is 135.73911322724922
At time: 1449.3139712810516 and batch: 250, loss is 4.9092255878448485 and perplexity is 135.53441425917313
At time: 1450.9294159412384 and batch: 300, loss is 4.872642221450806 and perplexity is 130.66570892642784
At time: 1452.5537555217743 and batch: 350, loss is 4.835442028045654 and perplexity is 125.89421968541174
At time: 1454.1721382141113 and batch: 400, loss is 4.859631624221802 and perplexity is 128.97668146978378
At time: 1455.7849621772766 and batch: 450, loss is 4.907728776931763 and perplexity is 135.3316966216455
At time: 1457.399406671524 and batch: 500, loss is 4.906211624145508 and perplexity is 135.12653343235235
At time: 1459.017653465271 and batch: 550, loss is 4.883378667831421 and perplexity is 132.07615233048293
At time: 1460.6343836784363 and batch: 600, loss is 4.906487197875976 and perplexity is 135.16377588654095
At time: 1462.2474863529205 and batch: 650, loss is 4.8853092288970945 and perplexity is 132.33137969461143
At time: 1463.8631744384766 and batch: 700, loss is 4.877520809173584 and perplexity is 131.3047305438678
At time: 1465.4821572303772 and batch: 750, loss is 4.850948553085328 and perplexity is 127.86161587250952
At time: 1467.0971171855927 and batch: 800, loss is 4.850002861022949 and perplexity is 127.74075531473851
At time: 1468.7108719348907 and batch: 850, loss is 4.904884376525879 and perplexity is 134.9473060283886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015511830647786 and perplexity of 150.73326693109806
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1472.8919241428375 and batch: 50, loss is 4.9459925365448 and perplexity is 140.61034251823244
At time: 1474.5271797180176 and batch: 100, loss is 4.894537525177002 and perplexity is 133.5582250129575
At time: 1476.1418330669403 and batch: 150, loss is 4.884530820846558 and perplexity is 132.22841196397292
At time: 1477.756575345993 and batch: 200, loss is 4.910727300643921 and perplexity is 135.73810092493886
At time: 1479.3719232082367 and batch: 250, loss is 4.909187994003296 and perplexity is 135.5293190956525
At time: 1480.9839627742767 and batch: 300, loss is 4.872624998092651 and perplexity is 130.66345844350496
At time: 1482.5951874256134 and batch: 350, loss is 4.835441980361939 and perplexity is 125.89421368230771
At time: 1484.2112967967987 and batch: 400, loss is 4.85959834098816 and perplexity is 128.97238878019775
At time: 1485.8218936920166 and batch: 450, loss is 4.907684917449951 and perplexity is 135.32576117372275
At time: 1487.4330959320068 and batch: 500, loss is 4.906158838272095 and perplexity is 135.11940084851537
At time: 1489.0471692085266 and batch: 550, loss is 4.883334465026856 and perplexity is 132.07031432316288
At time: 1490.6611924171448 and batch: 600, loss is 4.906446342468262 and perplexity is 135.15825382817258
At time: 1492.2726159095764 and batch: 650, loss is 4.885261669158935 and perplexity is 132.3250861985026
At time: 1493.887112379074 and batch: 700, loss is 4.877476291656494 and perplexity is 131.2988853133904
At time: 1495.4993653297424 and batch: 750, loss is 4.850906276702881 and perplexity is 127.85621046019813
At time: 1497.1144967079163 and batch: 800, loss is 4.849946212768555 and perplexity is 127.7335192288927
At time: 1498.7213521003723 and batch: 850, loss is 4.904839496612549 and perplexity is 134.94124974089382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.015511194864909 and perplexity of 150.73317109749843
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.9441594106889342, 'tune_wordvecs': True, 'lr': 14.573061955702206, 'num_layers': 1, 'anneal': 6.21341231060793, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1006200313568115 and batch: 50, loss is 8.801921424865723 and perplexity is 6647.003461918969
At time: 3.694277763366699 and batch: 100, loss is 7.737969665527344 and perplexity is 2293.810449652022
At time: 5.292214393615723 and batch: 150, loss is 7.462688970565796 and perplexity is 1741.8254832554187
At time: 6.8897788524627686 and batch: 200, loss is 7.384127893447876 and perplexity is 1610.2228965337185
At time: 8.488548278808594 and batch: 250, loss is 7.333867063522339 and perplexity is 1531.2919403317228
At time: 10.087181091308594 and batch: 300, loss is 7.23818187713623 and perplexity is 1391.561639486138
At time: 11.72158169746399 and batch: 350, loss is 7.181496467590332 and perplexity is 1314.8744537101722
At time: 13.322320938110352 and batch: 400, loss is 7.179082412719726 and perplexity is 1311.7041028710446
At time: 14.918941736221313 and batch: 450, loss is 7.1480762386322025 and perplexity is 1271.657235489771
At time: 16.517191410064697 and batch: 500, loss is 7.113977909088135 and perplexity is 1229.026791904685
At time: 18.115642070770264 and batch: 550, loss is 7.044627370834351 and perplexity is 1146.6814692370297
At time: 19.70919680595398 and batch: 600, loss is 7.043290138244629 and perplexity is 1145.1491141922334
At time: 21.30576181411743 and batch: 650, loss is 7.040412693023682 and perplexity is 1141.8587465429316
At time: 22.909038543701172 and batch: 700, loss is 7.001613998413086 and perplexity is 1098.4045517343536
At time: 24.511701822280884 and batch: 750, loss is 6.962939319610595 and perplexity is 1056.7350789683273
At time: 26.110525131225586 and batch: 800, loss is 6.974372997283935 and perplexity is 1068.8867842064133
At time: 27.70619225502014 and batch: 850, loss is 6.968212442398071 and perplexity is 1062.322090346117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 6.18147087097168 and perplexity of 483.7028979244253
Finished 1 epochs...
Completing Train Step...
At time: 31.893012523651123 and batch: 50, loss is 6.2947790145874025 and perplexity is 541.7361173849811
At time: 33.49055290222168 and batch: 100, loss is 5.907200202941895 and perplexity is 367.67529677102465
At time: 35.0830774307251 and batch: 150, loss is 5.788619928359985 and perplexity is 326.5620342460379
At time: 36.67598247528076 and batch: 200, loss is 5.742465181350708 and perplexity is 311.83218713111387
At time: 38.26738619804382 and batch: 250, loss is 5.752651510238647 and perplexity is 315.0248454748854
At time: 39.87397217750549 and batch: 300, loss is 5.67798942565918 and perplexity is 292.36102502969464
At time: 41.48322010040283 and batch: 350, loss is 5.6512026119232175 and perplexity is 284.6335637947083
At time: 43.091333866119385 and batch: 400, loss is 5.6387575912475585 and perplexity is 281.11324387046784
At time: 44.70979404449463 and batch: 450, loss is 5.637675619125366 and perplexity is 280.80925166262404
At time: 46.32646107673645 and batch: 500, loss is 5.648671445846557 and perplexity is 283.9140200003382
At time: 47.93837547302246 and batch: 550, loss is 5.616907320022583 and perplexity is 275.037463699542
At time: 49.55216836929321 and batch: 600, loss is 5.637746677398682 and perplexity is 280.8292061921372
At time: 51.17708921432495 and batch: 650, loss is 5.653521747589111 and perplexity is 285.29443367146456
At time: 52.79145789146423 and batch: 700, loss is 5.6257562255859375 and perplexity is 277.48204422179083
At time: 54.399381160736084 and batch: 750, loss is 5.608922929763794 and perplexity is 272.8502008535681
At time: 56.013627767562866 and batch: 800, loss is 5.613694849014283 and perplexity is 274.15533149191333
At time: 57.63133430480957 and batch: 850, loss is 5.59170636177063 and perplexity is 268.19286354051013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.269920349121094 and perplexity of 194.40047765980722
Finished 2 epochs...
Completing Train Step...
At time: 61.859925270080566 and batch: 50, loss is 5.580670366287231 and perplexity is 265.24936042815114
At time: 63.47384548187256 and batch: 100, loss is 5.519359617233277 and perplexity is 249.47522638940978
At time: 65.0866425037384 and batch: 150, loss is 5.490440807342529 and perplexity is 242.36401915361023
At time: 66.70056414604187 and batch: 200, loss is 5.514392852783203 and perplexity is 248.23921373656296
At time: 68.31624674797058 and batch: 250, loss is 5.566134023666382 and perplexity is 261.4214938389206
At time: 69.93156242370605 and batch: 300, loss is 5.512555742263794 and perplexity is 247.783589509851
At time: 71.54184460639954 and batch: 350, loss is 5.489612445831299 and perplexity is 242.16333725849069
At time: 73.14543962478638 and batch: 400, loss is 5.500306701660156 and perplexity is 244.76699119583367
At time: 74.75022029876709 and batch: 450, loss is 5.503396129608154 and perplexity is 245.52435048026283
At time: 76.35292625427246 and batch: 500, loss is 5.5200278949737545 and perplexity is 249.6420008496295
At time: 77.95706796646118 and batch: 550, loss is 5.4875992488861085 and perplexity is 241.67630517792665
At time: 79.56278443336487 and batch: 600, loss is 5.498891353607178 and perplexity is 244.4208057556459
At time: 81.1684410572052 and batch: 650, loss is 5.510280075073243 and perplexity is 247.22035763206435
At time: 82.77290868759155 and batch: 700, loss is 5.476975002288818 and perplexity is 239.1222678997118
At time: 84.38410782814026 and batch: 750, loss is 5.497107343673706 and perplexity is 243.9851453370356
At time: 85.99564599990845 and batch: 800, loss is 5.506488161087036 and perplexity is 246.28469439866285
At time: 87.60691452026367 and batch: 850, loss is 5.485419836044311 and perplexity is 241.15016628005537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.272653261820476 and perplexity of 194.93248382605242
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 91.76694893836975 and batch: 50, loss is 5.479051456451416 and perplexity is 239.61931019244102
At time: 93.40435218811035 and batch: 100, loss is 5.385937843322754 and perplexity is 218.31475316160663
At time: 95.01650476455688 and batch: 150, loss is 5.345307607650756 and perplexity is 209.62235612290823
At time: 96.63177633285522 and batch: 200, loss is 5.342510709762573 and perplexity is 209.0368829337772
At time: 98.24379396438599 and batch: 250, loss is 5.352490358352661 and perplexity is 211.13344162707472
At time: 99.90373539924622 and batch: 300, loss is 5.3046144676208495 and perplexity is 201.26339389927398
At time: 101.51697945594788 and batch: 350, loss is 5.254104137420654 and perplexity is 191.34998575379078
At time: 103.12757301330566 and batch: 400, loss is 5.253533926010132 and perplexity is 191.2409069104667
At time: 104.73819160461426 and batch: 450, loss is 5.261194620132446 and perplexity is 192.71157096341074
At time: 106.3465883731842 and batch: 500, loss is 5.255546293258667 and perplexity is 191.62614133467815
At time: 107.95685505867004 and batch: 550, loss is 5.226875724792481 and perplexity is 186.2101220977154
At time: 109.55971097946167 and batch: 600, loss is 5.2416667652130124 and perplexity is 188.98483338755423
At time: 111.1637020111084 and batch: 650, loss is 5.217583322525025 and perplexity is 184.48779739725006
At time: 112.77530455589294 and batch: 700, loss is 5.161285018920898 and perplexity is 174.388404084111
At time: 114.38950061798096 and batch: 750, loss is 5.138066968917847 and perplexity is 170.38608817858275
At time: 116.00203108787537 and batch: 800, loss is 5.183163728713989 and perplexity is 178.2458413725526
At time: 117.61136507987976 and batch: 850, loss is 5.135034046173096 and perplexity is 169.8701032031698
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0584306716918945 and perplexity of 157.3433990198182
Finished 4 epochs...
Completing Train Step...
At time: 121.80230927467346 and batch: 50, loss is 5.296600198745727 and perplexity is 199.65686113735822
At time: 123.41478419303894 and batch: 100, loss is 5.229505567550659 and perplexity is 186.70046992504822
At time: 125.03092908859253 and batch: 150, loss is 5.205176210403442 and perplexity is 182.21297776240465
At time: 126.64107775688171 and batch: 200, loss is 5.211986322402954 and perplexity is 183.45810345929507
At time: 128.2534327507019 and batch: 250, loss is 5.2266872787475585 and perplexity is 186.1750348428126
At time: 129.86857843399048 and batch: 300, loss is 5.190339641571045 and perplexity is 179.52951826569816
At time: 131.4761438369751 and batch: 350, loss is 5.1439780998229985 and perplexity is 171.39624529433337
At time: 133.08701014518738 and batch: 400, loss is 5.151761121749878 and perplexity is 172.73543072428913
At time: 134.70365476608276 and batch: 450, loss is 5.172284774780273 and perplexity is 176.31722277306073
At time: 136.31566500663757 and batch: 500, loss is 5.170221977233886 and perplexity is 175.9538909074147
At time: 137.92451119422913 and batch: 550, loss is 5.144792137145996 and perplexity is 171.53582503887756
At time: 139.5319664478302 and batch: 600, loss is 5.16841757774353 and perplexity is 175.63668606452805
At time: 141.18394947052002 and batch: 650, loss is 5.147031679153442 and perplexity is 171.92041721926088
At time: 142.80469608306885 and batch: 700, loss is 5.103273134231568 and perplexity is 164.5596525955894
At time: 144.41320276260376 and batch: 750, loss is 5.096422491073608 and perplexity is 163.43616583479346
At time: 146.04448676109314 and batch: 800, loss is 5.130412931442261 and perplexity is 169.08692493894705
At time: 147.6549472808838 and batch: 850, loss is 5.105401449203491 and perplexity is 164.91026033748986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0366560618082685 and perplexity of 153.95433951720267
Finished 5 epochs...
Completing Train Step...
At time: 151.86801195144653 and batch: 50, loss is 5.220686550140381 and perplexity is 185.06119425515953
At time: 153.48184537887573 and batch: 100, loss is 5.159954414367676 and perplexity is 174.15651638923214
At time: 155.09961032867432 and batch: 150, loss is 5.140403203964233 and perplexity is 170.784615474657
At time: 156.7147192955017 and batch: 200, loss is 5.14788125038147 and perplexity is 172.0665379204239
At time: 158.32808017730713 and batch: 250, loss is 5.161838979721069 and perplexity is 174.48503518642823
At time: 159.94673228263855 and batch: 300, loss is 5.129556865692138 and perplexity is 168.94223735359262
At time: 161.56800293922424 and batch: 350, loss is 5.083348255157471 and perplexity is 161.31327070112644
At time: 163.18045902252197 and batch: 400, loss is 5.096403274536133 and perplexity is 163.4330251877642
At time: 164.78937005996704 and batch: 450, loss is 5.120024671554566 and perplexity is 167.33949809573417
At time: 166.4018988609314 and batch: 500, loss is 5.121271867752075 and perplexity is 167.54833348373293
At time: 168.0195779800415 and batch: 550, loss is 5.094755983352661 and perplexity is 163.16402502790925
At time: 169.6308090686798 and batch: 600, loss is 5.123537015914917 and perplexity is 167.92828544479636
At time: 171.24172687530518 and batch: 650, loss is 5.10341851234436 and perplexity is 164.5835777063771
At time: 172.85579586029053 and batch: 700, loss is 5.06544095993042 and perplexity is 158.45029690285673
At time: 174.4734513759613 and batch: 750, loss is 5.061538772583008 and perplexity is 157.83319895803393
At time: 176.08653616905212 and batch: 800, loss is 5.091408796310425 and perplexity is 162.61879751585377
At time: 177.697856426239 and batch: 850, loss is 5.074765272140503 and perplexity is 159.93464645584675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.023231188456218 and perplexity of 151.90133351385046
Finished 6 epochs...
Completing Train Step...
At time: 181.88347625732422 and batch: 50, loss is 5.1651247692108155 and perplexity is 175.05929921985867
At time: 183.49876952171326 and batch: 100, loss is 5.1073807334899906 and perplexity is 165.23698786112044
At time: 185.1065855026245 and batch: 150, loss is 5.090848503112793 and perplexity is 162.52770883036683
At time: 186.7132592201233 and batch: 200, loss is 5.09728609085083 and perplexity is 163.57737023446663
At time: 188.32458090782166 and batch: 250, loss is 5.110084829330444 and perplexity is 165.6844091751081
At time: 189.93355131149292 and batch: 300, loss is 5.080810756683349 and perplexity is 160.90445742375942
At time: 191.53803491592407 and batch: 350, loss is 5.034039115905761 and perplexity is 153.5519760505716
At time: 193.15583109855652 and batch: 400, loss is 5.05120846748352 and perplexity is 156.21112653444064
At time: 194.76971983909607 and batch: 450, loss is 5.078171768188477 and perplexity is 160.4803922095634
At time: 196.39008927345276 and batch: 500, loss is 5.081288642883301 and perplexity is 160.98136981969552
At time: 197.99819326400757 and batch: 550, loss is 5.0561344432830815 and perplexity is 156.98251712924178
At time: 199.6067271232605 and batch: 600, loss is 5.088401641845703 and perplexity is 162.13051221547948
At time: 201.21423888206482 and batch: 650, loss is 5.0683439826965335 and perplexity is 158.91095004182358
At time: 202.8240246772766 and batch: 700, loss is 5.034500989913941 and perplexity is 153.622914098173
At time: 204.43550157546997 and batch: 750, loss is 5.0298864364624025 and perplexity is 152.9156460678041
At time: 206.05329155921936 and batch: 800, loss is 5.055094757080078 and perplexity is 156.81938938759532
At time: 207.6705243587494 and batch: 850, loss is 5.045350904464722 and perplexity is 155.29878467511088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.014264742533366 and perplexity of 150.54540642910737
Finished 7 epochs...
Completing Train Step...
At time: 211.85518980026245 and batch: 50, loss is 5.120485906600952 and perplexity is 167.41669873935325
At time: 213.49892377853394 and batch: 100, loss is 5.065046510696411 and perplexity is 158.38780862965083
At time: 215.11433839797974 and batch: 150, loss is 5.05167329788208 and perplexity is 156.28375509332037
At time: 216.7252049446106 and batch: 200, loss is 5.058808526992798 and perplexity is 157.40286329093752
At time: 218.33408617973328 and batch: 250, loss is 5.070988826751709 and perplexity is 159.33180102062062
At time: 219.945969581604 and batch: 300, loss is 5.044011135101318 and perplexity is 155.09085943826224
At time: 221.56315279006958 and batch: 350, loss is 4.996311225891113 and perplexity is 147.86670497641285
At time: 223.22051167488098 and batch: 400, loss is 5.014835176467895 and perplexity is 150.6313071356317
At time: 224.83026123046875 and batch: 450, loss is 5.0445935249328615 and perplexity is 155.18120908456837
At time: 226.4407205581665 and batch: 500, loss is 5.048752489089966 and perplexity is 155.8279461168591
At time: 228.05507731437683 and batch: 550, loss is 5.023073072433472 and perplexity is 151.87731737786345
At time: 229.66788506507874 and batch: 600, loss is 5.056575546264648 and perplexity is 157.05177786003577
At time: 231.27609372138977 and batch: 650, loss is 5.037006063461304 and perplexity is 154.00823322141824
At time: 232.8888738155365 and batch: 700, loss is 5.0054697418212895 and perplexity is 149.22716493717147
At time: 234.50110697746277 and batch: 750, loss is 5.00379641532898 and perplexity is 148.97766797186708
At time: 236.11850452423096 and batch: 800, loss is 5.025293807983399 and perplexity is 152.21497151730108
At time: 237.7341239452362 and batch: 850, loss is 5.017740201950073 and perplexity is 151.0695311390356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.008848190307617 and perplexity of 149.73217381957568
Finished 8 epochs...
Completing Train Step...
At time: 241.8769404888153 and batch: 50, loss is 5.081701946258545 and perplexity is 161.04791771448086
At time: 243.5135838985443 and batch: 100, loss is 5.027820730209351 and perplexity is 152.60009329341105
At time: 245.12538623809814 and batch: 150, loss is 5.016962537765503 and perplexity is 150.9520954439743
At time: 246.74403977394104 and batch: 200, loss is 5.025272121429444 and perplexity is 152.21167053490205
At time: 248.36314010620117 and batch: 250, loss is 5.0371222496032715 and perplexity is 154.02612788340326
At time: 249.98148798942566 and batch: 300, loss is 5.012622003555298 and perplexity is 150.29830264107858
At time: 251.59524130821228 and batch: 350, loss is 4.964174165725708 and perplexity is 143.1902500156562
At time: 253.2128348350525 and batch: 400, loss is 4.983610706329346 and perplexity is 146.0005963558853
At time: 254.828622341156 and batch: 450, loss is 5.0154786300659175 and perplexity is 150.7282625820031
At time: 256.4409112930298 and batch: 500, loss is 5.01946008682251 and perplexity is 151.32957690061204
At time: 258.0551266670227 and batch: 550, loss is 4.992784557342529 and perplexity is 147.34614657659017
At time: 259.6755847930908 and batch: 600, loss is 5.030636596679687 and perplexity is 153.03040033874754
At time: 261.28812408447266 and batch: 650, loss is 5.009180898666382 and perplexity is 149.78199925359104
At time: 262.9289298057556 and batch: 700, loss is 4.9805022144317626 and perplexity is 145.54745933632563
At time: 264.5469717979431 and batch: 750, loss is 4.978640375137329 and perplexity is 145.2767254670134
At time: 266.1619565486908 and batch: 800, loss is 4.997649717330932 and perplexity is 148.06475581036267
At time: 267.7749855518341 and batch: 850, loss is 4.994135427474975 and perplexity is 147.5453265880063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.004894574483235 and perplexity of 149.14135902469383
Finished 9 epochs...
Completing Train Step...
At time: 271.90771555900574 and batch: 50, loss is 5.050691719055176 and perplexity is 156.13042553316765
At time: 273.53985953330994 and batch: 100, loss is 4.998443117141724 and perplexity is 148.1822769740772
At time: 275.15081691741943 and batch: 150, loss is 4.987693777084351 and perplexity is 146.59794580121397
At time: 276.7754952907562 and batch: 200, loss is 4.994393939971924 and perplexity is 147.5834738293525
At time: 278.38886976242065 and batch: 250, loss is 5.005708122253418 and perplexity is 149.2627420135055
At time: 280.00443410873413 and batch: 300, loss is 4.983715543746948 and perplexity is 146.0159034837415
At time: 281.61638474464417 and batch: 350, loss is 4.935537433624267 and perplexity is 139.14790520329487
At time: 283.22829031944275 and batch: 400, loss is 4.957963762283325 and perplexity is 142.30373644683792
At time: 284.84309911727905 and batch: 450, loss is 4.992426147460938 and perplexity is 147.29334572438154
At time: 286.4565706253052 and batch: 500, loss is 4.995789318084717 and perplexity is 147.7895523238167
At time: 288.07218408584595 and batch: 550, loss is 4.969266786575317 and perplexity is 143.9213236286897
At time: 289.6839249134064 and batch: 600, loss is 5.005582065582275 and perplexity is 149.24392763498543
At time: 291.30314564704895 and batch: 650, loss is 4.984267139434815 and perplexity is 146.0964674437869
At time: 292.92241072654724 and batch: 700, loss is 4.957434673309326 and perplexity is 142.22846502332106
At time: 294.53165555000305 and batch: 750, loss is 4.956825590133667 and perplexity is 142.14186243494265
At time: 296.141282081604 and batch: 800, loss is 4.973626728057861 and perplexity is 144.5501820746451
At time: 297.75062441825867 and batch: 850, loss is 4.970883960723877 and perplexity is 144.1542577694588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.00233777364095 and perplexity of 148.76052134284143
Finished 10 epochs...
Completing Train Step...
At time: 301.9220869541168 and batch: 50, loss is 5.022338104248047 and perplexity is 151.76573339186584
At time: 303.53658294677734 and batch: 100, loss is 4.970986919403076 and perplexity is 144.16910046551877
At time: 305.1725912094116 and batch: 150, loss is 4.962224111557007 and perplexity is 142.91129335039918
At time: 306.7840025424957 and batch: 200, loss is 4.969333868026734 and perplexity is 143.93097840379306
At time: 308.4021441936493 and batch: 250, loss is 4.980574026107788 and perplexity is 145.55791171861884
At time: 310.01783537864685 and batch: 300, loss is 4.958953399658203 and perplexity is 142.44463525087227
At time: 311.6277675628662 and batch: 350, loss is 4.908906679153443 and perplexity is 135.49119804786025
At time: 313.2393639087677 and batch: 400, loss is 4.932700071334839 and perplexity is 138.75365176883065
At time: 314.85813665390015 and batch: 450, loss is 4.9678264999389645 and perplexity is 143.71418487497837
At time: 316.46732091903687 and batch: 500, loss is 4.972170343399048 and perplexity is 144.3398146321843
At time: 318.07541823387146 and batch: 550, loss is 4.94613353729248 and perplexity is 140.63017007947673
At time: 319.6870412826538 and batch: 600, loss is 4.984213247299194 and perplexity is 146.08859420530442
At time: 321.300044298172 and batch: 650, loss is 4.962121801376343 and perplexity is 142.89667281808494
At time: 322.9136197566986 and batch: 700, loss is 4.9366085243225095 and perplexity is 139.29702507643623
At time: 324.5252606868744 and batch: 750, loss is 4.937929468154907 and perplexity is 139.4811502052858
At time: 326.1383168697357 and batch: 800, loss is 4.950825109481811 and perplexity is 141.29149679237983
At time: 327.75219559669495 and batch: 850, loss is 4.951674213409424 and perplexity is 141.41151890565732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9999847412109375 and perplexity of 148.41089451476523
Finished 11 epochs...
Completing Train Step...
At time: 331.9568290710449 and batch: 50, loss is 4.996150522232056 and perplexity is 147.84294416514618
At time: 333.5740430355072 and batch: 100, loss is 4.946759185791016 and perplexity is 140.7181826638354
At time: 335.1862573623657 and batch: 150, loss is 4.9386028385162355 and perplexity is 139.57510430720166
At time: 336.79707288742065 and batch: 200, loss is 4.945586194992066 and perplexity is 140.5532183000808
At time: 338.41253304481506 and batch: 250, loss is 4.95578218460083 and perplexity is 141.9936281769013
At time: 340.02847027778625 and batch: 300, loss is 4.936195945739746 and perplexity is 139.2395659612622
At time: 341.6465837955475 and batch: 350, loss is 4.884553470611572 and perplexity is 132.23140694034979
At time: 343.26575326919556 and batch: 400, loss is 4.910703449249268 and perplexity is 135.7348634205339
At time: 344.91062903404236 and batch: 450, loss is 4.947062540054321 and perplexity is 140.76087659983742
At time: 346.52297806739807 and batch: 500, loss is 4.951836500167847 and perplexity is 141.43446998494193
At time: 348.13442754745483 and batch: 550, loss is 4.925232439041138 and perplexity is 137.72134974122673
At time: 349.74060463905334 and batch: 600, loss is 4.964122610092163 and perplexity is 143.1828679418946
At time: 351.3567304611206 and batch: 650, loss is 4.942337102890015 and perplexity is 140.09728902777155
At time: 352.9596538543701 and batch: 700, loss is 4.918143281936645 and perplexity is 136.74847396529017
At time: 354.5664885044098 and batch: 750, loss is 4.917915153503418 and perplexity is 136.71728130827947
At time: 356.1822910308838 and batch: 800, loss is 4.931322870254516 and perplexity is 138.56269161516343
At time: 357.79533672332764 and batch: 850, loss is 4.933905982971192 and perplexity is 138.9210773420191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.999292691548665 and perplexity of 148.30822233655962
Finished 12 epochs...
Completing Train Step...
At time: 361.9676764011383 and batch: 50, loss is 4.973382759094238 and perplexity is 144.5149206180582
At time: 363.58420586586 and batch: 100, loss is 4.924418964385986 and perplexity is 137.60936246931823
At time: 365.1980586051941 and batch: 150, loss is 4.918890419006348 and perplexity is 136.85068199636484
At time: 366.8163890838623 and batch: 200, loss is 4.92534836769104 and perplexity is 137.7373165168504
At time: 368.4341244697571 and batch: 250, loss is 4.935389919281006 and perplexity is 139.12738040533085
At time: 370.0542402267456 and batch: 300, loss is 4.915815858840943 and perplexity is 136.43057249779338
At time: 371.6700015068054 and batch: 350, loss is 4.863826522827148 and perplexity is 129.51886197204985
At time: 373.29002928733826 and batch: 400, loss is 4.890745801925659 and perplexity is 133.05276806777994
At time: 374.90370774269104 and batch: 450, loss is 4.927403450012207 and perplexity is 138.0206690977039
At time: 376.517507314682 and batch: 500, loss is 4.934458837509156 and perplexity is 138.9979017244439
At time: 378.1350440979004 and batch: 550, loss is 4.906346549987793 and perplexity is 135.14476672373428
At time: 379.75053000450134 and batch: 600, loss is 4.945519981384277 and perplexity is 140.5439120725137
At time: 381.3633511066437 and batch: 650, loss is 4.92417727470398 and perplexity is 137.57610772508806
At time: 382.97759532928467 and batch: 700, loss is 4.900197277069092 and perplexity is 134.3162745924136
At time: 384.5964903831482 and batch: 750, loss is 4.900701036453247 and perplexity is 134.38395472200327
At time: 386.2426929473877 and batch: 800, loss is 4.911544160842896 and perplexity is 135.84902527574454
At time: 387.854718208313 and batch: 850, loss is 4.916108083724976 and perplexity is 136.47044673185806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.999962488810222 and perplexity of 148.4075920528139
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 392.0420660972595 and batch: 50, loss is 4.959127559661865 and perplexity is 142.4694455694891
At time: 393.6828854084015 and batch: 100, loss is 4.91523476600647 and perplexity is 136.3513166994303
At time: 395.2940020561218 and batch: 150, loss is 4.904479351043701 and perplexity is 134.8926599979603
At time: 396.9052746295929 and batch: 200, loss is 4.905061540603637 and perplexity is 134.97121596133516
At time: 398.5281939506531 and batch: 250, loss is 4.908372011184692 and perplexity is 135.41877460718868
At time: 400.1451439857483 and batch: 300, loss is 4.873166399002075 and perplexity is 130.7342189118973
At time: 401.754563331604 and batch: 350, loss is 4.819076747894287 and perplexity is 123.85069256604842
At time: 403.3647072315216 and batch: 400, loss is 4.840418710708618 and perplexity is 126.5223168897621
At time: 404.9776406288147 and batch: 450, loss is 4.878189067840577 and perplexity is 131.39250539295625
At time: 406.59164023399353 and batch: 500, loss is 4.874146156311035 and perplexity is 130.86236948638714
At time: 408.2000343799591 and batch: 550, loss is 4.835435247421264 and perplexity is 125.89336604688928
At time: 409.82098507881165 and batch: 600, loss is 4.8598510360717775 and perplexity is 129.00498358686522
At time: 411.4357671737671 and batch: 650, loss is 4.834133138656616 and perplexity is 125.72954587053859
At time: 413.05274844169617 and batch: 700, loss is 4.801605243682861 and perplexity is 121.70562798712592
At time: 414.6613233089447 and batch: 750, loss is 4.785941257476806 and perplexity is 119.81408592913355
At time: 416.26911902427673 and batch: 800, loss is 4.785608758926392 and perplexity is 119.77425454154915
At time: 417.8746154308319 and batch: 850, loss is 4.811036500930786 and perplexity is 122.85889490322838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.965965270996094 and perplexity of 143.44694864563633
Finished 14 epochs...
Completing Train Step...
At time: 422.0021233558655 and batch: 50, loss is 4.922816982269287 and perplexity is 137.38909121396568
At time: 423.6421856880188 and batch: 100, loss is 4.8800507926940915 and perplexity is 131.63734993167373
At time: 425.255788564682 and batch: 150, loss is 4.8694886302948 and perplexity is 130.2542917640931
At time: 426.8996546268463 and batch: 200, loss is 4.874785966873169 and perplexity is 130.9461234030336
At time: 428.51022815704346 and batch: 250, loss is 4.880271377563477 and perplexity is 131.6663903421345
At time: 430.1230540275574 and batch: 300, loss is 4.849525089263916 and perplexity is 127.6797389664763
At time: 431.7295763492584 and batch: 350, loss is 4.796106300354004 and perplexity is 121.03821235817826
At time: 433.33697056770325 and batch: 400, loss is 4.818963918685913 and perplexity is 123.83671937875633
At time: 434.95163440704346 and batch: 450, loss is 4.859235734939575 and perplexity is 128.92563108974863
At time: 436.5675208568573 and batch: 500, loss is 4.857004556655884 and perplexity is 128.63829568890333
At time: 438.1774001121521 and batch: 550, loss is 4.821745052337646 and perplexity is 124.18160521083847
At time: 439.788348197937 and batch: 600, loss is 4.850949363708496 and perplexity is 127.86171952013973
At time: 441.3968114852905 and batch: 650, loss is 4.826236572265625 and perplexity is 124.74062384721243
At time: 443.0139400959015 and batch: 700, loss is 4.797937173843383 and perplexity is 121.26002100220941
At time: 444.62700033187866 and batch: 750, loss is 4.788070344924927 and perplexity is 120.06945234784678
At time: 446.23512172698975 and batch: 800, loss is 4.792791872024536 and perplexity is 120.63770397300571
At time: 447.8425304889679 and batch: 850, loss is 4.818309316635132 and perplexity is 123.75568213475572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.963503519694011 and perplexity of 143.09425223656834
Finished 15 epochs...
Completing Train Step...
At time: 451.9945375919342 and batch: 50, loss is 4.909787712097168 and perplexity is 135.6106228577961
At time: 453.64289450645447 and batch: 100, loss is 4.866494312286377 and perplexity is 129.86485233630953
At time: 455.2571430206299 and batch: 150, loss is 4.854713134765625 and perplexity is 128.34386853947072
At time: 456.87453532218933 and batch: 200, loss is 4.86180230140686 and perplexity is 129.25695228834132
At time: 458.5006995201111 and batch: 250, loss is 4.867385501861572 and perplexity is 129.98063812477653
At time: 460.123482465744 and batch: 300, loss is 4.83786449432373 and perplexity is 126.19956388089068
At time: 461.74245166778564 and batch: 350, loss is 4.784922370910644 and perplexity is 119.69207113671541
At time: 463.35809779167175 and batch: 400, loss is 4.80889401435852 and perplexity is 122.59595314578772
At time: 464.9789414405823 and batch: 450, loss is 4.849964370727539 and perplexity is 127.73583862995353
At time: 466.5955629348755 and batch: 500, loss is 4.849018878936768 and perplexity is 127.61512252017991
At time: 468.2700426578522 and batch: 550, loss is 4.815317583084107 and perplexity is 123.38599139222426
At time: 469.8854033946991 and batch: 600, loss is 4.846718015670777 and perplexity is 127.32183510909148
At time: 471.50650215148926 and batch: 650, loss is 4.822765207290649 and perplexity is 124.3083543313452
At time: 473.1297996044159 and batch: 700, loss is 4.796357870101929 and perplexity is 121.0686657411648
At time: 474.7479178905487 and batch: 750, loss is 4.789167394638062 and perplexity is 120.20124678540809
At time: 476.36692786216736 and batch: 800, loss is 4.7950720119476316 and perplexity is 120.91308865655941
At time: 477.9861617088318 and batch: 850, loss is 4.82023138999939 and perplexity is 123.99377838098782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.962628364562988 and perplexity of 142.9690773492154
Finished 16 epochs...
Completing Train Step...
At time: 482.19226479530334 and batch: 50, loss is 4.900435857772827 and perplexity is 134.34832368672468
At time: 483.81273317337036 and batch: 100, loss is 4.857128276824951 and perplexity is 128.65421182514768
At time: 485.42469334602356 and batch: 150, loss is 4.844826545715332 and perplexity is 127.08123729680857
At time: 487.0371196269989 and batch: 200, loss is 4.852844858169556 and perplexity is 128.10431054360862
At time: 488.65366983413696 and batch: 250, loss is 4.858329944610595 and perplexity is 128.80890437289636
At time: 490.2701504230499 and batch: 300, loss is 4.829847583770752 and perplexity is 125.19187792657733
At time: 491.87598419189453 and batch: 350, loss is 4.7773486328125 and perplexity is 118.78897894625608
At time: 493.4843952655792 and batch: 400, loss is 4.801871604919434 and perplexity is 121.73804996648202
At time: 495.089364528656 and batch: 450, loss is 4.843633184432983 and perplexity is 126.92967392142451
At time: 496.6986310482025 and batch: 500, loss is 4.843402643203735 and perplexity is 126.9004147712204
At time: 498.3005483150482 and batch: 550, loss is 4.81076078414917 and perplexity is 122.82502531355439
At time: 499.9100160598755 and batch: 600, loss is 4.843430795669556 and perplexity is 126.90398738109855
At time: 501.5240728855133 and batch: 650, loss is 4.8198466491699214 and perplexity is 123.94608208778867
At time: 503.1340847015381 and batch: 700, loss is 4.794657144546509 and perplexity is 120.86293616174369
At time: 504.7459988594055 and batch: 750, loss is 4.788670454025269 and perplexity is 120.14152874356117
At time: 506.36203384399414 and batch: 800, loss is 4.794942684173584 and perplexity is 120.8974522470831
At time: 508.0048859119415 and batch: 850, loss is 4.819798545837402 and perplexity is 123.94012001158659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.961926778157552 and perplexity of 142.86880736629078
Finished 17 epochs...
Completing Train Step...
At time: 512.2089059352875 and batch: 50, loss is 4.893008909225464 and perplexity is 133.35422174079554
At time: 513.8259892463684 and batch: 100, loss is 4.849535284042358 and perplexity is 127.6810406397617
At time: 515.4359514713287 and batch: 150, loss is 4.8372493171691895 and perplexity is 126.12195266704862
At time: 517.0477995872498 and batch: 200, loss is 4.845947217941284 and perplexity is 127.22373354076473
At time: 518.6673686504364 and batch: 250, loss is 4.851123838424683 and perplexity is 127.88403010362391
At time: 520.2856209278107 and batch: 300, loss is 4.823261165618897 and perplexity is 124.37002138582719
At time: 521.8974404335022 and batch: 350, loss is 4.771237821578979 and perplexity is 118.06529531829673
At time: 523.508467912674 and batch: 400, loss is 4.796586904525757 and perplexity is 121.09639780894435
At time: 525.1254034042358 and batch: 450, loss is 4.8387633800506595 and perplexity is 126.31305386723075
At time: 526.7351703643799 and batch: 500, loss is 4.838850574493408 and perplexity is 126.32406814375946
At time: 528.3504812717438 and batch: 550, loss is 4.806749467849731 and perplexity is 122.33332213534064
At time: 529.9658815860748 and batch: 600, loss is 4.840272216796875 and perplexity is 126.50378349818583
At time: 531.5796275138855 and batch: 650, loss is 4.817428150177002 and perplexity is 123.6466808098642
At time: 533.1942851543427 and batch: 700, loss is 4.792467260360718 and perplexity is 120.59854992247438
At time: 534.8076977729797 and batch: 750, loss is 4.787494506835937 and perplexity is 120.00033168692536
At time: 536.4250621795654 and batch: 800, loss is 4.793816919326782 and perplexity is 120.76142672601631
At time: 538.0443499088287 and batch: 850, loss is 4.818563909530639 and perplexity is 123.78719346331593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.961872418721517 and perplexity of 142.86104130957648
Finished 18 epochs...
Completing Train Step...
At time: 542.242184638977 and batch: 50, loss is 4.88628041267395 and perplexity is 132.45996021128929
At time: 543.8524796962738 and batch: 100, loss is 4.843248987197876 and perplexity is 126.8809172583382
At time: 545.4698729515076 and batch: 150, loss is 4.830794115066528 and perplexity is 125.31043205575732
At time: 547.0845329761505 and batch: 200, loss is 4.839892272949219 and perplexity is 126.45572829365815
At time: 548.6941492557526 and batch: 250, loss is 4.844871263504029 and perplexity is 127.08692021578817
At time: 550.3351535797119 and batch: 300, loss is 4.817457866668701 and perplexity is 123.650355210023
At time: 551.9578387737274 and batch: 350, loss is 4.7661006641387935 and perplexity is 117.46033053812266
At time: 553.5669803619385 and batch: 400, loss is 4.792170581817627 and perplexity is 120.56277622729071
At time: 555.1757998466492 and batch: 450, loss is 4.834604349136352 and perplexity is 125.78880491080913
At time: 556.7916512489319 and batch: 500, loss is 4.834583377838134 and perplexity is 125.78616698392935
At time: 558.4063177108765 and batch: 550, loss is 4.802960901260376 and perplexity is 121.87073103024302
At time: 560.0186133384705 and batch: 600, loss is 4.83718692779541 and perplexity is 126.1140842428575
At time: 561.6277468204498 and batch: 650, loss is 4.814720087051391 and perplexity is 123.31229077198147
At time: 563.2374169826508 and batch: 700, loss is 4.790114774703979 and perplexity is 120.31517700960136
At time: 564.8390121459961 and batch: 750, loss is 4.785947170257568 and perplexity is 119.81479436565026
At time: 566.4438519477844 and batch: 800, loss is 4.791964445114136 and perplexity is 120.5379263753568
At time: 568.0545661449432 and batch: 850, loss is 4.816817722320557 and perplexity is 123.57122646360126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.961464246114095 and perplexity of 142.8027412449003
Finished 19 epochs...
Completing Train Step...
At time: 572.2241039276123 and batch: 50, loss is 4.880204906463623 and perplexity is 131.6576386232261
At time: 573.8612303733826 and batch: 100, loss is 4.837618083953857 and perplexity is 126.16847083065932
At time: 575.4733259677887 and batch: 150, loss is 4.825107069015503 and perplexity is 124.59980844785112
At time: 577.0871994495392 and batch: 200, loss is 4.834301338195801 and perplexity is 125.75069530082826
At time: 578.7043750286102 and batch: 250, loss is 4.839345064163208 and perplexity is 126.38654953743232
At time: 580.3257179260254 and batch: 300, loss is 4.812360372543335 and perplexity is 123.02165201753036
At time: 581.9388241767883 and batch: 350, loss is 4.761613359451294 and perplexity is 116.93443106434542
At time: 583.5570955276489 and batch: 400, loss is 4.788103818893433 and perplexity is 120.0734716161832
At time: 585.1771342754364 and batch: 450, loss is 4.830503463745117 and perplexity is 125.27401570558028
At time: 586.79052734375 and batch: 500, loss is 4.830815849304199 and perplexity is 125.31315561206736
At time: 588.4054069519043 and batch: 550, loss is 4.7994017410278325 and perplexity is 121.43774456203421
At time: 590.0517251491547 and batch: 600, loss is 4.834198913574219 and perplexity is 125.73781599303872
At time: 591.6671328544617 and batch: 650, loss is 4.812050123214721 and perplexity is 122.98349055267752
At time: 593.2806479930878 and batch: 700, loss is 4.787477636337281 and perplexity is 119.9983072385676
At time: 594.8977375030518 and batch: 750, loss is 4.784006271362305 and perplexity is 119.58247149416378
At time: 596.5207982063293 and batch: 800, loss is 4.790056219100952 and perplexity is 120.3081320881201
At time: 598.1330070495605 and batch: 850, loss is 4.814531450271606 and perplexity is 123.28903173236647
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.96127192179362 and perplexity of 142.77527944559904
Finished 20 epochs...
Completing Train Step...
At time: 602.2775843143463 and batch: 50, loss is 4.874628057479859 and perplexity is 130.92544741264015
At time: 603.9326322078705 and batch: 100, loss is 4.832419719696045 and perplexity is 125.51430293606174
At time: 605.550400018692 and batch: 150, loss is 4.819772624969483 and perplexity is 123.9369074177426
At time: 607.1671566963196 and batch: 200, loss is 4.829386043548584 and perplexity is 125.1341101715222
At time: 608.7768332958221 and batch: 250, loss is 4.834303703308105 and perplexity is 125.7509927156967
At time: 610.3955774307251 and batch: 300, loss is 4.8075879859924315 and perplexity is 122.43594386448588
At time: 612.0133829116821 and batch: 350, loss is 4.757274875640869 and perplexity is 116.4282118342818
At time: 613.6239628791809 and batch: 400, loss is 4.784138040542603 and perplexity is 119.59822981661837
At time: 615.2315189838409 and batch: 450, loss is 4.826930198669434 and perplexity is 124.82717725193368
At time: 616.8419723510742 and batch: 500, loss is 4.827141933441162 and perplexity is 124.85361030411396
At time: 618.4568107128143 and batch: 550, loss is 4.796187543869019 and perplexity is 121.04804632746908
At time: 620.0653331279755 and batch: 600, loss is 4.831162042617798 and perplexity is 125.3565456988919
At time: 621.6763617992401 and batch: 650, loss is 4.809175481796265 and perplexity is 122.6304647713191
At time: 623.2983109951019 and batch: 700, loss is 4.784882774353028 and perplexity is 119.68733183655499
At time: 624.9176003932953 and batch: 750, loss is 4.782012939453125 and perplexity is 119.34434135300863
At time: 626.5320024490356 and batch: 800, loss is 4.7879149532318115 and perplexity is 120.05079600191311
At time: 628.1421508789062 and batch: 850, loss is 4.8122703742980955 and perplexity is 123.01058078292566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.961157480875651 and perplexity of 142.75894104646457
Finished 21 epochs...
Completing Train Step...
At time: 632.2397618293762 and batch: 50, loss is 4.869606094360352 and perplexity is 130.26959286140624
At time: 633.8752789497375 and batch: 100, loss is 4.8276760959625244 and perplexity is 124.9203202387825
At time: 635.4988839626312 and batch: 150, loss is 4.815040111541748 and perplexity is 123.35176004021986
At time: 637.1090123653412 and batch: 200, loss is 4.825141258239746 and perplexity is 124.60406849146624
At time: 638.7201266288757 and batch: 250, loss is 4.829727029800415 and perplexity is 125.17678645832581
At time: 640.333313703537 and batch: 300, loss is 4.803313064575195 and perplexity is 121.91365698889236
At time: 641.9515891075134 and batch: 350, loss is 4.753602695465088 and perplexity is 116.00145051468077
At time: 643.5607783794403 and batch: 400, loss is 4.780424604415893 and perplexity is 119.15493301591147
At time: 645.1754548549652 and batch: 450, loss is 4.823538150787353 and perplexity is 124.40447480847494
At time: 646.7854073047638 and batch: 500, loss is 4.823827543258667 and perplexity is 124.44048173669793
At time: 648.4086201190948 and batch: 550, loss is 4.7929355239868165 and perplexity is 120.65503506069898
At time: 650.020813703537 and batch: 600, loss is 4.828585624694824 and perplexity is 125.03399054465628
At time: 651.6328618526459 and batch: 650, loss is 4.806442823410034 and perplexity is 122.29581505328112
At time: 653.2463102340698 and batch: 700, loss is 4.782141094207764 and perplexity is 119.35963687786841
At time: 654.860029220581 and batch: 750, loss is 4.7798048114776615 and perplexity is 119.08110451142896
At time: 656.4813106060028 and batch: 800, loss is 4.785465927124023 and perplexity is 119.75714819059019
At time: 658.103816986084 and batch: 850, loss is 4.8098437786102295 and perplexity is 122.71244571096996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960740089416504 and perplexity of 142.6993671174443
Finished 22 epochs...
Completing Train Step...
At time: 662.3024117946625 and batch: 50, loss is 4.8649728488922115 and perplexity is 129.6674179499729
At time: 663.9242839813232 and batch: 100, loss is 4.823099184036255 and perplexity is 124.34987736445416
At time: 665.5390532016754 and batch: 150, loss is 4.810696115493775 and perplexity is 122.81708264114224
At time: 667.1522259712219 and batch: 200, loss is 4.8209327030181885 and perplexity is 124.08076733169288
At time: 668.7757427692413 and batch: 250, loss is 4.825489196777344 and perplexity is 124.64743059207134
At time: 670.397207736969 and batch: 300, loss is 4.799263257980346 and perplexity is 121.42092865747314
At time: 672.0402991771698 and batch: 350, loss is 4.749941377639771 and perplexity is 115.57750890222677
At time: 673.6607038974762 and batch: 400, loss is 4.776961898803711 and perplexity is 118.74304809031267
At time: 675.2788803577423 and batch: 450, loss is 4.820178918838501 and perplexity is 123.98727245418141
At time: 676.8988265991211 and batch: 500, loss is 4.820427770614624 and perplexity is 124.0181307465595
At time: 678.5123059749603 and batch: 550, loss is 4.789738788604736 and perplexity is 120.26994867866243
At time: 680.1259038448334 and batch: 600, loss is 4.825778961181641 and perplexity is 124.6835542139611
At time: 681.7381579875946 and batch: 650, loss is 4.803736124038696 and perplexity is 121.96524462676132
At time: 683.3561193943024 and batch: 700, loss is 4.779499044418335 and perplexity is 119.04469899836816
At time: 684.9739542007446 and batch: 750, loss is 4.777448854446411 and perplexity is 118.8008847684172
At time: 686.5881531238556 and batch: 800, loss is 4.783168935775757 and perplexity is 119.48238274503808
At time: 688.2060670852661 and batch: 850, loss is 4.807552967071533 and perplexity is 122.43165636492483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960597991943359 and perplexity of 142.6790913385602
Finished 23 epochs...
Completing Train Step...
At time: 692.4049215316772 and batch: 50, loss is 4.8603747463226314 and perplexity is 129.07256251352464
At time: 694.0234026908875 and batch: 100, loss is 4.818760843276977 and perplexity is 123.81157373964409
At time: 695.6422855854034 and batch: 150, loss is 4.806394605636597 and perplexity is 122.28991836354234
At time: 697.2579996585846 and batch: 200, loss is 4.816935768127442 and perplexity is 123.58581438974174
At time: 698.8783233165741 and batch: 250, loss is 4.821272239685059 and perplexity is 124.12290445499978
At time: 700.4976880550385 and batch: 300, loss is 4.795352849960327 and perplexity is 120.94705041674003
At time: 702.1123056411743 and batch: 350, loss is 4.7464636707305905 and perplexity is 115.17626231439155
At time: 703.7229344844818 and batch: 400, loss is 4.773575201034546 and perplexity is 118.34158148100221
At time: 705.3325154781342 and batch: 450, loss is 4.816873836517334 and perplexity is 123.57816075827341
At time: 706.9388236999512 and batch: 500, loss is 4.817289457321167 and perplexity is 123.62953308774895
At time: 708.551064491272 and batch: 550, loss is 4.7865597438812255 and perplexity is 119.88821223303128
At time: 710.1545038223267 and batch: 600, loss is 4.8229491329193115 and perplexity is 124.3312199262836
At time: 711.7629342079163 and batch: 650, loss is 4.801114816665649 and perplexity is 121.64595489285914
At time: 713.4070756435394 and batch: 700, loss is 4.776898555755615 and perplexity is 118.73552678192029
At time: 715.0216891765594 and batch: 750, loss is 4.775093202590942 and perplexity is 118.52136060384845
At time: 716.6412737369537 and batch: 800, loss is 4.780388622283936 and perplexity is 119.15064564452314
At time: 718.2591233253479 and batch: 850, loss is 4.805114841461181 and perplexity is 122.13351620729068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960375467936198 and perplexity of 142.64734534767098
Finished 24 epochs...
Completing Train Step...
At time: 722.4437508583069 and batch: 50, loss is 4.856083421707154 and perplexity is 128.51985701637395
At time: 724.0592615604401 and batch: 100, loss is 4.814684591293335 and perplexity is 123.30791378642552
At time: 725.6737148761749 and batch: 150, loss is 4.802563409805298 and perplexity is 121.82229808251354
At time: 727.2840571403503 and batch: 200, loss is 4.813388481140136 and perplexity is 123.14819667524101
At time: 728.8959143161774 and batch: 250, loss is 4.817349109649658 and perplexity is 123.63690809723396
At time: 730.5181441307068 and batch: 300, loss is 4.791585426330567 and perplexity is 120.49224889398661
At time: 732.1319303512573 and batch: 350, loss is 4.742899580001831 and perplexity is 114.766494324615
At time: 733.7390882968903 and batch: 400, loss is 4.770172023773194 and perplexity is 117.93952861844856
At time: 735.3486466407776 and batch: 450, loss is 4.813783178329468 and perplexity is 123.19681251597663
At time: 736.9612367153168 and batch: 500, loss is 4.814024591445923 and perplexity is 123.22655743268557
At time: 738.5763981342316 and batch: 550, loss is 4.783468780517578 and perplexity is 119.51821428092472
At time: 740.1969015598297 and batch: 600, loss is 4.820343084335327 and perplexity is 124.0076285572032
At time: 741.8096549510956 and batch: 650, loss is 4.79889331817627 and perplexity is 121.3760185304488
At time: 743.4209976196289 and batch: 700, loss is 4.7743745708465575 and perplexity is 118.43621798848814
At time: 745.0370543003082 and batch: 750, loss is 4.772859964370728 and perplexity is 118.25696950547339
At time: 746.6534135341644 and batch: 800, loss is 4.777934875488281 and perplexity is 118.8586385318427
At time: 748.2707619667053 and batch: 850, loss is 4.802559223175049 and perplexity is 121.82178805866295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960182825724284 and perplexity of 142.61986809426392
Finished 25 epochs...
Completing Train Step...
At time: 752.444783449173 and batch: 50, loss is 4.851989364624023 and perplexity is 127.99476499721865
At time: 754.0768489837646 and batch: 100, loss is 4.810777683258056 and perplexity is 122.82710096456942
At time: 755.6900424957275 and batch: 150, loss is 4.798762674331665 and perplexity is 121.36016253651174
At time: 757.3028373718262 and batch: 200, loss is 4.80970139503479 and perplexity is 122.69497471801947
At time: 758.9179022312164 and batch: 250, loss is 4.813541517257691 and perplexity is 123.16704423928623
At time: 760.5375170707703 and batch: 300, loss is 4.788064479827881 and perplexity is 120.06874813092162
At time: 762.1556158065796 and batch: 350, loss is 4.739558191299438 and perplexity is 114.38365482105782
At time: 763.7684738636017 and batch: 400, loss is 4.766993236541748 and perplexity is 117.5652191909978
At time: 765.3886888027191 and batch: 450, loss is 4.810761852264404 and perplexity is 122.82515650490517
At time: 767.0042114257812 and batch: 500, loss is 4.811040458679199 and perplexity is 122.85938114878687
At time: 768.6175153255463 and batch: 550, loss is 4.780463056564331 and perplexity is 119.15951486717334
At time: 770.2348446846008 and batch: 600, loss is 4.817470092773437 and perplexity is 123.65186698145793
At time: 771.8489062786102 and batch: 650, loss is 4.796157741546631 and perplexity is 121.04443886832362
At time: 773.4701581001282 and batch: 700, loss is 4.771635980606079 and perplexity is 118.11231344113877
At time: 775.0909597873688 and batch: 750, loss is 4.770290317535401 and perplexity is 117.95348095422264
At time: 776.7034599781036 and batch: 800, loss is 4.775375080108643 and perplexity is 118.5547738197659
At time: 778.3121049404144 and batch: 850, loss is 4.800153522491455 and perplexity is 121.52907353279237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.960156122843425 and perplexity of 142.61605978376468
Finished 26 epochs...
Completing Train Step...
At time: 782.4943447113037 and batch: 50, loss is 4.847979669570923 and perplexity is 127.48257257513464
At time: 784.143824338913 and batch: 100, loss is 4.80706865310669 and perplexity is 122.37237536047627
At time: 785.754798412323 and batch: 150, loss is 4.7950529289245605 and perplexity is 120.9107812913148
At time: 787.3680860996246 and batch: 200, loss is 4.806231117248535 and perplexity is 122.26992701613395
At time: 788.9743502140045 and batch: 250, loss is 4.809807462692261 and perplexity is 122.7079893767762
At time: 790.5954792499542 and batch: 300, loss is 4.78455885887146 and perplexity is 119.64856953501929
At time: 792.2163133621216 and batch: 350, loss is 4.736406116485596 and perplexity is 114.02367662073968
At time: 793.8868107795715 and batch: 400, loss is 4.763817634582519 and perplexity is 117.19247101376523
At time: 795.501261472702 and batch: 450, loss is 4.807831630706787 and perplexity is 122.46577836941776
At time: 797.1121790409088 and batch: 500, loss is 4.808044395446777 and perplexity is 122.49183754104833
At time: 798.7241053581238 and batch: 550, loss is 4.777920942306519 and perplexity is 118.8569824643652
At time: 800.3452162742615 and batch: 600, loss is 4.814800987243652 and perplexity is 123.32226716355359
At time: 801.9731886386871 and batch: 650, loss is 4.793518075942993 and perplexity is 120.72534336452773
At time: 803.5863904953003 and batch: 700, loss is 4.76900393486023 and perplexity is 117.80184519156636
At time: 805.1974880695343 and batch: 750, loss is 4.767876682281494 and perplexity is 117.66912757498574
At time: 806.8141157627106 and batch: 800, loss is 4.772631225585937 and perplexity is 118.2299226434268
At time: 808.4325275421143 and batch: 850, loss is 4.797656412124634 and perplexity is 121.22598060914048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959977467854817 and perplexity of 142.5905829890748
Finished 27 epochs...
Completing Train Step...
At time: 812.6071975231171 and batch: 50, loss is 4.843943529129028 and perplexity is 126.96907198567065
At time: 814.2576258182526 and batch: 100, loss is 4.803300666809082 and perplexity is 121.9121455412563
At time: 815.8712255954742 and batch: 150, loss is 4.791376543045044 and perplexity is 120.46708270564717
At time: 817.4804146289825 and batch: 200, loss is 4.802777194976807 and perplexity is 121.84834466749031
At time: 819.0906054973602 and batch: 250, loss is 4.806171207427979 and perplexity is 122.26260206616742
At time: 820.7137875556946 and batch: 300, loss is 4.780939197540283 and perplexity is 119.21626510436063
At time: 822.3240978717804 and batch: 350, loss is 4.733269243240357 and perplexity is 113.66655920908673
At time: 823.9386448860168 and batch: 400, loss is 4.760795297622681 and perplexity is 116.83881058690504
At time: 825.5564842224121 and batch: 450, loss is 4.804907960891724 and perplexity is 122.1082517693586
At time: 827.1674737930298 and batch: 500, loss is 4.805103712081909 and perplexity is 122.13215694463082
At time: 828.7822573184967 and batch: 550, loss is 4.774674606323242 and perplexity is 118.47175838703266
At time: 830.3994545936584 and batch: 600, loss is 4.8120738220214845 and perplexity is 122.98640514919128
At time: 832.0172591209412 and batch: 650, loss is 4.790817947387695 and perplexity is 120.39980910751345
At time: 833.628103017807 and batch: 700, loss is 4.766521358489991 and perplexity is 117.50975583141168
At time: 835.2842507362366 and batch: 750, loss is 4.765498542785645 and perplexity is 117.38962645331351
At time: 836.8952448368073 and batch: 800, loss is 4.77018027305603 and perplexity is 117.94050153899067
At time: 838.5067226886749 and batch: 850, loss is 4.79500033378601 and perplexity is 120.90442213925229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959921201070149 and perplexity of 142.58256010115923
Finished 28 epochs...
Completing Train Step...
At time: 842.7031028270721 and batch: 50, loss is 4.84012659072876 and perplexity is 126.48536259090797
At time: 844.3197000026703 and batch: 100, loss is 4.799829797744751 and perplexity is 121.48973793154306
At time: 845.9310779571533 and batch: 150, loss is 4.787924184799194 and perplexity is 120.05190426404121
At time: 847.5549774169922 and batch: 200, loss is 4.799532251358032 and perplexity is 121.45359447644202
At time: 849.1694085597992 and batch: 250, loss is 4.802896604537964 and perplexity is 121.8628953935854
At time: 850.7886896133423 and batch: 300, loss is 4.7775858116149905 and perplexity is 118.81715651546058
At time: 852.3993084430695 and batch: 350, loss is 4.730138988494873 and perplexity is 113.31131022270777
At time: 854.013215303421 and batch: 400, loss is 4.757518482208252 and perplexity is 116.45657796626105
At time: 855.6276469230652 and batch: 450, loss is 4.801867599487305 and perplexity is 121.7375623539619
At time: 857.240344285965 and batch: 500, loss is 4.802094745635986 and perplexity is 121.76521771318572
At time: 858.8462815284729 and batch: 550, loss is 4.771673250198364 and perplexity is 118.11671552093593
At time: 860.4568526744843 and batch: 600, loss is 4.809301080703736 and perplexity is 122.64586799101056
At time: 862.0631783008575 and batch: 650, loss is 4.788262233734131 and perplexity is 120.09249454277855
At time: 863.6650342941284 and batch: 700, loss is 4.7641182708740235 and perplexity is 117.22770862022927
At time: 865.2746081352234 and batch: 750, loss is 4.763659534454345 and perplexity is 117.17394433365075
At time: 866.8925621509552 and batch: 800, loss is 4.767608728408813 and perplexity is 117.637601900459
At time: 868.503155708313 and batch: 850, loss is 4.7926180934906 and perplexity is 120.61674155113408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959841092427571 and perplexity of 142.57113846330628
Finished 29 epochs...
Completing Train Step...
At time: 872.6896939277649 and batch: 50, loss is 4.836545610427857 and perplexity is 126.03323101939883
At time: 874.3031811714172 and batch: 100, loss is 4.796353302001953 and perplexity is 121.06811268865899
At time: 875.9425904750824 and batch: 150, loss is 4.784737510681152 and perplexity is 119.66994687798748
At time: 877.5589351654053 and batch: 200, loss is 4.796434164047241 and perplexity is 121.07790289969307
At time: 879.1841423511505 and batch: 250, loss is 4.799576444625854 and perplexity is 121.45896202627458
At time: 880.8072235584259 and batch: 300, loss is 4.774345588684082 and perplexity is 118.43278550051615
At time: 882.4288144111633 and batch: 350, loss is 4.727217969894409 and perplexity is 112.98080871342852
At time: 884.0496575832367 and batch: 400, loss is 4.75473650932312 and perplexity is 116.13304915691037
At time: 885.6665279865265 and batch: 450, loss is 4.799024677276611 and perplexity is 121.39196342227635
At time: 887.2805624008179 and batch: 500, loss is 4.799463863372803 and perplexity is 121.44528879382459
At time: 888.8968315124512 and batch: 550, loss is 4.768885345458984 and perplexity is 117.78787596959666
At time: 890.5165922641754 and batch: 600, loss is 4.8067788410186765 and perplexity is 122.3369155054534
At time: 892.1289472579956 and batch: 650, loss is 4.785729942321777 and perplexity is 119.78877007189709
At time: 893.7429692745209 and batch: 700, loss is 4.76175461769104 and perplexity is 116.95095018294663
At time: 895.3569076061249 and batch: 750, loss is 4.76120231628418 and perplexity is 116.88637584251795
At time: 896.9791722297668 and batch: 800, loss is 4.765081415176391 and perplexity is 117.34067021029004
At time: 898.6016676425934 and batch: 850, loss is 4.790457248687744 and perplexity is 120.35638888417384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959747950236003 and perplexity of 142.55785969343185
Finished 30 epochs...
Completing Train Step...
At time: 902.7937064170837 and batch: 50, loss is 4.833133802413941 and perplexity is 125.60396253901372
At time: 904.4185583591461 and batch: 100, loss is 4.793128118515015 and perplexity is 120.67827479810256
At time: 906.0353701114655 and batch: 150, loss is 4.78177038192749 and perplexity is 119.31539699534936
At time: 907.648122549057 and batch: 200, loss is 4.793337993621826 and perplexity is 120.70360482189332
At time: 909.2590885162354 and batch: 250, loss is 4.7963057899475094 and perplexity is 121.06236063054466
At time: 910.873767375946 and batch: 300, loss is 4.77127552986145 and perplexity is 118.06974744174302
At time: 912.4914782047272 and batch: 350, loss is 4.7243122959136965 and perplexity is 112.65299980078339
At time: 914.1046533584595 and batch: 400, loss is 4.7519259262084965 and perplexity is 115.8071058298683
At time: 915.7149338722229 and batch: 450, loss is 4.796188802719116 and perplexity is 121.04819870890985
At time: 917.3730247020721 and batch: 500, loss is 4.796676778793335 and perplexity is 121.1072817480895
At time: 918.9901964664459 and batch: 550, loss is 4.766100111007691 and perplexity is 117.46026556717847
At time: 920.6070377826691 and batch: 600, loss is 4.804157943725586 and perplexity is 122.0167028203377
At time: 922.2188503742218 and batch: 650, loss is 4.783223247528076 and perplexity is 119.48887221884192
At time: 923.8323357105255 and batch: 700, loss is 4.75915641784668 and perplexity is 116.64748264775156
At time: 925.4494409561157 and batch: 750, loss is 4.758776655197144 and perplexity is 116.60319270104557
At time: 927.0719866752625 and batch: 800, loss is 4.7626627159118655 and perplexity is 117.05720136868634
At time: 928.6848380565643 and batch: 850, loss is 4.788095235824585 and perplexity is 120.07244102173237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.959880510965983 and perplexity of 142.57675851997072
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 932.8368511199951 and batch: 50, loss is 4.832558965682983 and perplexity is 125.53178151593174
At time: 934.4722051620483 and batch: 100, loss is 4.793577976226807 and perplexity is 120.73257506344706
At time: 936.0726408958435 and batch: 150, loss is 4.781257839202881 and perplexity is 119.25425842608831
At time: 937.6753449440002 and batch: 200, loss is 4.792679347991943 and perplexity is 120.62413009577993
At time: 939.284304857254 and batch: 250, loss is 4.790994491577148 and perplexity is 120.42106687063472
At time: 940.9066359996796 and batch: 300, loss is 4.7648788261413575 and perplexity is 117.31690068494581
At time: 942.525821685791 and batch: 350, loss is 4.716339607238769 and perplexity is 111.758423333459
At time: 944.137282371521 and batch: 400, loss is 4.741839847564697 and perplexity is 114.64493696839583
At time: 945.746285200119 and batch: 450, loss is 4.783941745758057 and perplexity is 119.5747556118718
At time: 947.3661532402039 and batch: 500, loss is 4.783054580688477 and perplexity is 119.46872010794172
At time: 948.973991394043 and batch: 550, loss is 4.749892625808716 and perplexity is 115.57187442438571
At time: 950.5862579345703 and batch: 600, loss is 4.782887115478515 and perplexity is 119.44871492877753
At time: 952.2095894813538 and batch: 650, loss is 4.760072860717774 and perplexity is 116.7544324008392
At time: 953.8269321918488 and batch: 700, loss is 4.73451494216919 and perplexity is 113.80824174871003
At time: 955.4384799003601 and batch: 750, loss is 4.731031513214111 and perplexity is 113.412488513397
At time: 957.0760633945465 and batch: 800, loss is 4.731902227401734 and perplexity is 113.51128138013297
At time: 958.6973807811737 and batch: 850, loss is 4.7622313308715825 and perplexity is 117.00671553335846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.95600700378418 and perplexity of 142.02555465713704
Finished 32 epochs...
Completing Train Step...
At time: 962.8394870758057 and batch: 50, loss is 4.8273852348327635 and perplexity is 124.88399105693729
At time: 964.4809441566467 and batch: 100, loss is 4.78812463760376 and perplexity is 120.07597141702797
At time: 966.0963661670685 and batch: 150, loss is 4.775850687026978 and perplexity is 118.61117270118217
At time: 967.7058207988739 and batch: 200, loss is 4.7878795337677005 and perplexity is 120.0465439423559
At time: 969.3192245960236 and batch: 250, loss is 4.786967420578003 and perplexity is 119.9370978274588
At time: 970.9424250125885 and batch: 300, loss is 4.760921258926391 and perplexity is 116.85352868274632
At time: 972.5586822032928 and batch: 350, loss is 4.713227710723877 and perplexity is 111.41118325319538
At time: 974.1691553592682 and batch: 400, loss is 4.738748826980591 and perplexity is 114.29111422676556
At time: 975.7798705101013 and batch: 450, loss is 4.781591367721558 and perplexity is 119.29403975598235
At time: 977.3918771743774 and batch: 500, loss is 4.781222772598267 and perplexity is 119.25007665748022
At time: 979.0134024620056 and batch: 550, loss is 4.74848198890686 and perplexity is 115.40895940748872
At time: 980.6278488636017 and batch: 600, loss is 4.782128868103027 and perplexity is 119.35817758336738
At time: 982.2399380207062 and batch: 650, loss is 4.759519701004028 and perplexity is 116.68986641173224
At time: 983.8546240329742 and batch: 700, loss is 4.73477071762085 and perplexity is 113.83735482619252
At time: 985.4691023826599 and batch: 750, loss is 4.73201696395874 and perplexity is 113.52430602092686
At time: 987.077761888504 and batch: 800, loss is 4.733345851898194 and perplexity is 113.67526738518526
At time: 988.6916153430939 and batch: 850, loss is 4.763958549499511 and perplexity is 117.20898634469113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955710728963216 and perplexity of 141.98348229415737
Finished 33 epochs...
Completing Train Step...
At time: 992.8601441383362 and batch: 50, loss is 4.824938611984253 and perplexity is 124.57882050185889
At time: 994.5035400390625 and batch: 100, loss is 4.785490856170655 and perplexity is 119.76013365933416
At time: 996.1205434799194 and batch: 150, loss is 4.773169527053833 and perplexity is 118.29358311706092
At time: 997.7615482807159 and batch: 200, loss is 4.785567274093628 and perplexity is 119.76928582969391
At time: 999.3755087852478 and batch: 250, loss is 4.7847673702239994 and perplexity is 119.67352022124274
At time: 1000.9919106960297 and batch: 300, loss is 4.758747119903564 and perplexity is 116.59974884237475
At time: 1002.6159436702728 and batch: 350, loss is 4.711493968963623 and perplexity is 111.21819237874118
At time: 1004.2327632904053 and batch: 400, loss is 4.737111129760742 and perplexity is 114.10409317047497
At time: 1005.8441226482391 and batch: 450, loss is 4.7802326869964595 and perplexity is 119.13206730288874
At time: 1007.4559271335602 and batch: 500, loss is 4.7802404689788816 and perplexity is 119.13299439014968
At time: 1009.0663809776306 and batch: 550, loss is 4.747864894866943 and perplexity is 115.33776319612349
At time: 1010.6778571605682 and batch: 600, loss is 4.7818341732025145 and perplexity is 119.32300851942553
At time: 1012.2878515720367 and batch: 650, loss is 4.759381856918335 and perplexity is 116.6737825123477
At time: 1013.9008951187134 and batch: 700, loss is 4.735188055038452 and perplexity is 113.88487332881722
At time: 1015.5056705474854 and batch: 750, loss is 4.732767944335937 and perplexity is 113.60959256733663
At time: 1017.1151099205017 and batch: 800, loss is 4.734211597442627 and perplexity is 113.7737238544115
At time: 1018.7311551570892 and batch: 850, loss is 4.764895868301392 and perplexity is 117.31890003537858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955604553222656 and perplexity of 141.96840789305958
Finished 34 epochs...
Completing Train Step...
At time: 1022.9077527523041 and batch: 50, loss is 4.823177490234375 and perplexity is 124.35961511184532
At time: 1024.5147852897644 and batch: 100, loss is 4.783589286804199 and perplexity is 119.53261784495392
At time: 1026.1238946914673 and batch: 150, loss is 4.771253395080566 and perplexity is 118.06713402267823
At time: 1027.7382040023804 and batch: 200, loss is 4.783877325057984 and perplexity is 119.56705277051809
At time: 1029.3600928783417 and batch: 250, loss is 4.7831221961975094 and perplexity is 119.4767983193684
At time: 1030.972174167633 and batch: 300, loss is 4.7571630859375 and perplexity is 116.41519708648802
At time: 1032.5831387043 and batch: 350, loss is 4.7102463436126705 and perplexity is 111.07952026585183
At time: 1034.1949038505554 and batch: 400, loss is 4.735891208648682 and perplexity is 113.96498004901316
At time: 1035.8111279010773 and batch: 450, loss is 4.7792411804199215 and perplexity is 119.014005613824
At time: 1037.4246826171875 and batch: 500, loss is 4.779559984207153 and perplexity is 119.05195377823513
At time: 1039.0768105983734 and batch: 550, loss is 4.7474368381500245 and perplexity is 115.28840265718185
At time: 1040.6882088184357 and batch: 600, loss is 4.781592311859131 and perplexity is 119.29415238602076
At time: 1042.2988214492798 and batch: 650, loss is 4.759335212707519 and perplexity is 116.66834048276012
At time: 1043.9138324260712 and batch: 700, loss is 4.735502653121948 and perplexity is 113.92070692800093
At time: 1045.5332746505737 and batch: 750, loss is 4.733293190002441 and perplexity is 113.66928118772822
At time: 1047.1488976478577 and batch: 800, loss is 4.734758615493774 and perplexity is 113.83597716039483
At time: 1048.7604072093964 and batch: 850, loss is 4.765451707839966 and perplexity is 117.38412864528107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955542882283528 and perplexity of 141.9596528379873
Finished 35 epochs...
Completing Train Step...
At time: 1052.9343461990356 and batch: 50, loss is 4.8217801761627195 and perplexity is 124.18596702041849
At time: 1054.5431492328644 and batch: 100, loss is 4.782080211639404 and perplexity is 119.35237017782678
At time: 1056.1520516872406 and batch: 150, loss is 4.76974232673645 and perplexity is 117.88886123907729
At time: 1057.7626807689667 and batch: 200, loss is 4.7825400924682615 and perplexity is 119.40727066762403
At time: 1059.3849098682404 and batch: 250, loss is 4.781772527694702 and perplexity is 119.31565301869078
At time: 1061.0058398246765 and batch: 300, loss is 4.755929698944092 and perplexity is 116.27170060810263
At time: 1062.6215047836304 and batch: 350, loss is 4.709241399765014 and perplexity is 110.96794765685344
At time: 1064.2341759204865 and batch: 400, loss is 4.734959144592285 and perplexity is 113.8588068752074
At time: 1065.8576173782349 and batch: 450, loss is 4.778471784591675 and perplexity is 118.92247195172611
At time: 1067.4660398960114 and batch: 500, loss is 4.778986577987671 and perplexity is 118.98370821558211
At time: 1069.0803000926971 and batch: 550, loss is 4.747114934921265 and perplexity is 115.25129692067655
At time: 1070.6888127326965 and batch: 600, loss is 4.781343965530396 and perplexity is 119.2645297997186
At time: 1072.3067784309387 and batch: 650, loss is 4.759266376495361 and perplexity is 116.66030975252824
At time: 1073.9200012683868 and batch: 700, loss is 4.73569396018982 and perplexity is 113.94250284920359
At time: 1075.5279123783112 and batch: 750, loss is 4.73363468170166 and perplexity is 113.70810493232597
At time: 1077.1435632705688 and batch: 800, loss is 4.7350924205780025 and perplexity is 113.87398253117915
At time: 1078.756273984909 and batch: 850, loss is 4.765776624679566 and perplexity is 117.42227492222794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955512364705403 and perplexity of 141.95532063929568
Finished 36 epochs...
Completing Train Step...
At time: 1082.9508657455444 and batch: 50, loss is 4.820572013854981 and perplexity is 124.03602081383096
At time: 1084.5617773532867 and batch: 100, loss is 4.780802402496338 and perplexity is 119.19995802552596
At time: 1086.1790153980255 and batch: 150, loss is 4.768462677001953 and perplexity is 117.73810126964241
At time: 1087.7994976043701 and batch: 200, loss is 4.78140193939209 and perplexity is 119.27144422550029
At time: 1089.414086818695 and batch: 250, loss is 4.780597867965699 and perplexity is 119.17558001121681
At time: 1091.0310292243958 and batch: 300, loss is 4.754901876449585 and perplexity is 116.15225533352844
At time: 1092.640129327774 and batch: 350, loss is 4.70837212562561 and perplexity is 110.87152800327979
At time: 1094.2558407783508 and batch: 400, loss is 4.734175014495849 and perplexity is 113.76956175245847
At time: 1095.8623118400574 and batch: 450, loss is 4.777807569503784 and perplexity is 118.84350807896743
At time: 1097.4657940864563 and batch: 500, loss is 4.778467130661011 and perplexity is 118.92191849607514
At time: 1099.0773739814758 and batch: 550, loss is 4.746846723556518 and perplexity is 115.22038935810572
At time: 1100.7028560638428 and batch: 600, loss is 4.781101894378662 and perplexity is 119.23566279170493
At time: 1102.3173513412476 and batch: 650, loss is 4.7591633605957036 and perplexity is 116.64829250475914
At time: 1103.9292969703674 and batch: 700, loss is 4.7358009624481205 and perplexity is 113.95469560663959
At time: 1105.544879436493 and batch: 750, loss is 4.733834953308105 and perplexity is 113.73087971766192
At time: 1107.165061712265 and batch: 800, loss is 4.735257568359375 and perplexity is 113.89279011972329
At time: 1108.7819476127625 and batch: 850, loss is 4.765944528579712 and perplexity is 117.44199223541199
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9554901123046875 and perplexity of 141.95216182776275
Finished 37 epochs...
Completing Train Step...
At time: 1112.95236992836 and batch: 50, loss is 4.819497146606445 and perplexity is 123.90277018361782
At time: 1114.5911161899567 and batch: 100, loss is 4.779664945602417 and perplexity is 119.06445029322697
At time: 1116.2014825344086 and batch: 150, loss is 4.767341690063477 and perplexity is 117.60619234386279
At time: 1117.8106248378754 and batch: 200, loss is 4.78040919303894 and perplexity is 119.15309668847313
At time: 1119.4240469932556 and batch: 250, loss is 4.779542512893677 and perplexity is 119.04987380240075
At time: 1121.0772109031677 and batch: 300, loss is 4.753970851898194 and perplexity is 116.04416505727168
At time: 1122.6889169216156 and batch: 350, loss is 4.707573118209839 and perplexity is 110.78297621168745
At time: 1124.3028283119202 and batch: 400, loss is 4.733483171463012 and perplexity is 113.6908782952504
At time: 1125.9180102348328 and batch: 450, loss is 4.777212152481079 and perplexity is 118.77276769332406
At time: 1127.5418224334717 and batch: 500, loss is 4.778014459609985 and perplexity is 118.86809816861067
At time: 1129.1532754898071 and batch: 550, loss is 4.746597118377686 and perplexity is 115.1916333411895
At time: 1130.7675681114197 and batch: 600, loss is 4.780864553451538 and perplexity is 119.20736664700112
At time: 1132.3771607875824 and batch: 650, loss is 4.759029970169068 and perplexity is 116.63273377697132
At time: 1133.9952964782715 and batch: 700, loss is 4.735827436447144 and perplexity is 113.95771248307398
At time: 1135.6137628555298 and batch: 750, loss is 4.7339362621307375 and perplexity is 113.74240224283992
At time: 1137.2283065319061 and batch: 800, loss is 4.735294342041016 and perplexity is 113.89697845393802
At time: 1138.8392584323883 and batch: 850, loss is 4.766028261184692 and perplexity is 117.45182637106909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955466906229655 and perplexity of 141.94886771346629
Finished 38 epochs...
Completing Train Step...
At time: 1143.0158395767212 and batch: 50, loss is 4.818501138687134 and perplexity is 123.77942348063382
At time: 1144.6517825126648 and batch: 100, loss is 4.778621816635132 and perplexity is 118.94031547172243
At time: 1146.2685589790344 and batch: 150, loss is 4.766321907043457 and perplexity is 117.48632067779461
At time: 1147.8862538337708 and batch: 200, loss is 4.779498138427734 and perplexity is 119.04459114503865
At time: 1149.4973711967468 and batch: 250, loss is 4.778536205291748 and perplexity is 118.93013326739467
At time: 1151.1090009212494 and batch: 300, loss is 4.753096694946289 and perplexity is 115.94276856833864
At time: 1152.7209022045135 and batch: 350, loss is 4.706831474304199 and perplexity is 110.70084515229576
At time: 1154.34241938591 and batch: 400, loss is 4.732837705612183 and perplexity is 113.61751839396253
At time: 1155.955801486969 and batch: 450, loss is 4.776635408401489 and perplexity is 118.70428595285793
At time: 1157.5659275054932 and batch: 500, loss is 4.777593517303467 and perplexity is 118.81807208698193
At time: 1159.1754007339478 and batch: 550, loss is 4.746350269317627 and perplexity is 115.16320190405378
At time: 1160.8217492103577 and batch: 600, loss is 4.780599479675293 and perplexity is 119.1757720877973
At time: 1162.4260303974152 and batch: 650, loss is 4.75885048866272 and perplexity is 116.61180223669194
At time: 1164.0300815105438 and batch: 700, loss is 4.735789833068847 and perplexity is 113.95342736866957
At time: 1165.6329958438873 and batch: 750, loss is 4.733974781036377 and perplexity is 113.74678356008033
At time: 1167.2417838573456 and batch: 800, loss is 4.735292987823486 and perplexity is 113.8968242127577
At time: 1168.8521964550018 and batch: 850, loss is 4.766046772003174 and perplexity is 117.45400052062995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9554487864176435 and perplexity of 141.94629564997075
Finished 39 epochs...
Completing Train Step...
At time: 1172.9564654827118 and batch: 50, loss is 4.817587232589721 and perplexity is 123.66635238683163
At time: 1174.5850901603699 and batch: 100, loss is 4.777678909301758 and perplexity is 118.82821863280127
At time: 1176.1871588230133 and batch: 150, loss is 4.765367336273194 and perplexity is 117.37422518022439
At time: 1177.7899596691132 and batch: 200, loss is 4.778657245635986 and perplexity is 118.94452948290952
At time: 1179.3925111293793 and batch: 250, loss is 4.7776237487792965 and perplexity is 118.82166418695329
At time: 1181.0061905384064 and batch: 300, loss is 4.752280120849609 and perplexity is 115.84813135123703
At time: 1182.6117532253265 and batch: 350, loss is 4.706132383346557 and perplexity is 110.62348223745441
At time: 1184.218581199646 and batch: 400, loss is 4.732237873077392 and perplexity is 113.54938734556018
At time: 1185.826825618744 and batch: 450, loss is 4.7760977458953855 and perplexity is 118.64048026348776
At time: 1187.4329323768616 and batch: 500, loss is 4.777186555862427 and perplexity is 118.769727550992
At time: 1189.0371458530426 and batch: 550, loss is 4.7460925579071045 and perplexity is 115.13352685681413
At time: 1190.6497225761414 and batch: 600, loss is 4.7803347873687745 and perplexity is 119.14423135228144
At time: 1192.2543995380402 and batch: 650, loss is 4.7586571216583256 and perplexity is 116.5892555417801
At time: 1193.861977338791 and batch: 700, loss is 4.735704364776612 and perplexity is 113.94368838003128
At time: 1195.4650917053223 and batch: 750, loss is 4.733972473144531 and perplexity is 113.74652104510902
At time: 1197.0782589912415 and batch: 800, loss is 4.7352673625946045 and perplexity is 113.89390561796333
At time: 1198.6879532337189 and batch: 850, loss is 4.766011934280396 and perplexity is 117.44990876199488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955436706542969 and perplexity of 141.94458096686535
Finished 40 epochs...
Completing Train Step...
At time: 1202.852213859558 and batch: 50, loss is 4.8167541980743405 and perplexity is 123.5633769439062
At time: 1204.4657270908356 and batch: 100, loss is 4.776812725067138 and perplexity is 118.72533606725425
At time: 1206.080206155777 and batch: 150, loss is 4.7644956684112545 and perplexity is 117.27195841811493
At time: 1207.6930577754974 and batch: 200, loss is 4.777880973815918 and perplexity is 118.85223202511332
At time: 1209.30375623703 and batch: 250, loss is 4.77677206993103 and perplexity is 118.72050937067259
At time: 1210.9133067131042 and batch: 300, loss is 4.751503553390503 and perplexity is 115.75820238470524
At time: 1212.5206265449524 and batch: 350, loss is 4.705480079650879 and perplexity is 110.55134566119764
At time: 1214.128356218338 and batch: 400, loss is 4.731664266586304 and perplexity is 113.48427335660833
At time: 1215.7389783859253 and batch: 450, loss is 4.775589771270752 and perplexity is 118.58022921435723
At time: 1217.3452048301697 and batch: 500, loss is 4.776811513900757 and perplexity is 118.72519227120566
At time: 1218.9517903327942 and batch: 550, loss is 4.745818204879761 and perplexity is 115.10194395780232
At time: 1220.558531999588 and batch: 600, loss is 4.780045757293701 and perplexity is 119.10980006222799
At time: 1222.1655638217926 and batch: 650, loss is 4.758439168930054 and perplexity is 116.56384736443898
At time: 1223.774988412857 and batch: 700, loss is 4.735588159561157 and perplexity is 113.93044829847155
At time: 1225.3837051391602 and batch: 750, loss is 4.733933343887329 and perplexity is 113.74207031530861
At time: 1226.994386434555 and batch: 800, loss is 4.735180902481079 and perplexity is 113.88405876363994
At time: 1228.6020460128784 and batch: 850, loss is 4.765929832458496 and perplexity is 117.44026630634052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.95542844136556 and perplexity of 141.94340777456972
Finished 41 epochs...
Completing Train Step...
At time: 1232.7734444141388 and batch: 50, loss is 4.8159701538085935 and perplexity is 123.46653575560708
At time: 1234.3771409988403 and batch: 100, loss is 4.77599609375 and perplexity is 118.62842081708409
At time: 1235.9813780784607 and batch: 150, loss is 4.763672723770141 and perplexity is 117.17548978799735
At time: 1237.5935583114624 and batch: 200, loss is 4.777155380249024 and perplexity is 118.76602488959858
At time: 1239.197885274887 and batch: 250, loss is 4.7759710311889645 and perplexity is 118.62544772230366
At time: 1240.8050873279572 and batch: 300, loss is 4.750756750106811 and perplexity is 115.67178605106729
At time: 1242.434776544571 and batch: 350, loss is 4.704867315292359 and perplexity is 110.48362448746973
At time: 1244.04270029068 and batch: 400, loss is 4.731131200790405 and perplexity is 113.42379489304183
At time: 1245.6507902145386 and batch: 450, loss is 4.775098714828491 and perplexity is 118.52201392354337
At time: 1247.2523777484894 and batch: 500, loss is 4.7764452362060545 and perplexity is 118.68171384455249
At time: 1248.855480670929 and batch: 550, loss is 4.745543403625488 and perplexity is 115.07031814483902
At time: 1250.4635119438171 and batch: 600, loss is 4.77974380493164 and perplexity is 119.07384000613978
At time: 1252.0715777873993 and batch: 650, loss is 4.758212194442749 and perplexity is 116.53739334725225
At time: 1253.68252825737 and batch: 700, loss is 4.735437936782837 and perplexity is 113.9133346354561
At time: 1255.2921829223633 and batch: 750, loss is 4.733854990005494 and perplexity is 113.7331585317124
At time: 1256.9029395580292 and batch: 800, loss is 4.735061273574829 and perplexity is 113.87043575311996
At time: 1258.5114686489105 and batch: 850, loss is 4.765830926895141 and perplexity is 117.42865138504062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955423037211101 and perplexity of 141.9426406925424
Finished 42 epochs...
Completing Train Step...
At time: 1262.669843673706 and batch: 50, loss is 4.815210905075073 and perplexity is 123.37282952237342
At time: 1264.2715876102448 and batch: 100, loss is 4.775220289230346 and perplexity is 118.5364240424258
At time: 1265.8785095214844 and batch: 150, loss is 4.762900238037109 and perplexity is 117.08500834618599
At time: 1267.484753370285 and batch: 200, loss is 4.77647385597229 and perplexity is 118.68511053606512
At time: 1269.0896167755127 and batch: 250, loss is 4.77521276473999 and perplexity is 118.5355321196019
At time: 1270.6981449127197 and batch: 300, loss is 4.750063762664795 and perplexity is 115.59165472414676
At time: 1272.3031468391418 and batch: 350, loss is 4.704296474456787 and perplexity is 110.4205739205816
At time: 1273.9061558246613 and batch: 400, loss is 4.730623235702515 and perplexity is 113.36619419591027
At time: 1275.5091726779938 and batch: 450, loss is 4.774620552062988 and perplexity is 118.465354656848
At time: 1277.1156718730927 and batch: 500, loss is 4.77608063697815 and perplexity is 118.63845047069394
At time: 1278.7188379764557 and batch: 550, loss is 4.745254888534546 and perplexity is 115.03712341035585
At time: 1280.3219125270844 and batch: 600, loss is 4.779424848556519 and perplexity is 119.03586670199529
At time: 1281.9298734664917 and batch: 650, loss is 4.7579625225067135 and perplexity is 116.50830086257427
At time: 1283.5769295692444 and batch: 700, loss is 4.735263233184814 and perplexity is 113.8934353043255
At time: 1285.1818940639496 and batch: 750, loss is 4.7337426090240475 and perplexity is 113.72037780590236
At time: 1286.7883007526398 and batch: 800, loss is 4.734910974502563 and perplexity is 113.8533224183591
At time: 1288.3968567848206 and batch: 850, loss is 4.765706720352173 and perplexity is 117.41406688397066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955423037211101 and perplexity of 141.9426406925424
Finished 43 epochs...
Completing Train Step...
At time: 1292.5035755634308 and batch: 50, loss is 4.814477519989014 and perplexity is 123.2823828993329
At time: 1294.1316022872925 and batch: 100, loss is 4.774477901458741 and perplexity is 118.44845670770384
At time: 1295.7383978366852 and batch: 150, loss is 4.762151784896851 and perplexity is 116.99740849029503
At time: 1297.3457462787628 and batch: 200, loss is 4.7758175563812255 and perplexity is 118.6072431015326
At time: 1298.9520041942596 and batch: 250, loss is 4.774479084014892 and perplexity is 118.4485967797378
At time: 1300.5605866909027 and batch: 300, loss is 4.7493870830535885 and perplexity is 115.51346266663847
At time: 1302.167557477951 and batch: 350, loss is 4.703732328414917 and perplexity is 110.35829815882721
At time: 1303.7748045921326 and batch: 400, loss is 4.730137300491333 and perplexity is 113.31111895297647
At time: 1305.3826830387115 and batch: 450, loss is 4.7741590595245365 and perplexity is 118.41069639276947
At time: 1306.994677066803 and batch: 500, loss is 4.7757101821899415 and perplexity is 118.59450842842399
At time: 1308.6108152866364 and batch: 550, loss is 4.744961538314819 and perplexity is 115.00338219416531
At time: 1310.2181639671326 and batch: 600, loss is 4.779098815917969 and perplexity is 118.99706345019938
At time: 1311.8252921104431 and batch: 650, loss is 4.757707204818725 and perplexity is 116.47855802966387
At time: 1313.432326555252 and batch: 700, loss is 4.735063562393188 and perplexity is 113.87069638216211
At time: 1315.0421118736267 and batch: 750, loss is 4.733611068725586 and perplexity is 113.7054199772648
At time: 1316.6566562652588 and batch: 800, loss is 4.734743509292603 and perplexity is 113.83425754421174
At time: 1318.2651619911194 and batch: 850, loss is 4.7655477046966555 and perplexity is 117.39539769354421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955419540405273 and perplexity of 141.9421443475571
Finished 44 epochs...
Completing Train Step...
At time: 1322.430498123169 and batch: 50, loss is 4.813753480911255 and perplexity is 123.19315394303824
At time: 1324.0623309612274 and batch: 100, loss is 4.773754205703735 and perplexity is 118.36276707274995
At time: 1325.667048215866 and batch: 150, loss is 4.761428241729736 and perplexity is 116.9127864323594
At time: 1327.2827258110046 and batch: 200, loss is 4.775179471969604 and perplexity is 118.53158580904064
At time: 1328.8886530399323 and batch: 250, loss is 4.773758373260498 and perplexity is 118.36326035732823
At time: 1330.5012352466583 and batch: 300, loss is 4.748730316162109 and perplexity is 115.43762215633308
At time: 1332.1066453456879 and batch: 350, loss is 4.703194074630737 and perplexity is 110.29891337070475
At time: 1333.7135384082794 and batch: 400, loss is 4.729648962020874 and perplexity is 113.25579828316594
At time: 1335.324807882309 and batch: 450, loss is 4.77371132850647 and perplexity is 118.35769211783797
At time: 1336.9339814186096 and batch: 500, loss is 4.7753386878967286 and perplexity is 118.5504594278192
At time: 1338.5506601333618 and batch: 550, loss is 4.744643526077271 and perplexity is 114.96681552590023
At time: 1340.1625137329102 and batch: 600, loss is 4.77875864982605 and perplexity is 118.95659156814688
At time: 1341.7689044475555 and batch: 650, loss is 4.7574347877502445 and perplexity is 116.4468316039507
At time: 1343.3723359107971 and batch: 700, loss is 4.734852561950683 and perplexity is 113.84667214948813
At time: 1344.9847962856293 and batch: 750, loss is 4.733497266769409 and perplexity is 113.69248081430847
At time: 1346.603166103363 and batch: 800, loss is 4.734565544128418 and perplexity is 113.8140008144276
At time: 1348.2148427963257 and batch: 850, loss is 4.7653697681427 and perplexity is 117.37451061937054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955419858296712 and perplexity of 141.94218946975673
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1352.3986115455627 and batch: 50, loss is 4.813665285110473 and perplexity is 123.18228930329026
At time: 1354.0137250423431 and batch: 100, loss is 4.773737392425537 and perplexity is 118.36077702334853
At time: 1355.626404285431 and batch: 150, loss is 4.761455135345459 and perplexity is 116.91593068219073
At time: 1357.2365610599518 and batch: 200, loss is 4.775200595855713 and perplexity is 118.53408968320524
At time: 1358.8447732925415 and batch: 250, loss is 4.772610607147217 and perplexity is 118.22748495214269
At time: 1360.4582002162933 and batch: 300, loss is 4.747238264083863 and perplexity is 115.265511643141
At time: 1362.0760769844055 and batch: 350, loss is 4.701929492950439 and perplexity is 110.15951954151696
At time: 1363.6872222423553 and batch: 400, loss is 4.727013158798218 and perplexity is 112.95767135962163
At time: 1365.3234162330627 and batch: 450, loss is 4.771142225265503 and perplexity is 118.05400925077667
At time: 1366.9338626861572 and batch: 500, loss is 4.772946310043335 and perplexity is 118.26718092389541
At time: 1368.547554731369 and batch: 550, loss is 4.741400947570801 and perplexity is 114.59463034685567
At time: 1370.1636707782745 and batch: 600, loss is 4.774743547439575 and perplexity is 118.47992624385715
At time: 1371.771980524063 and batch: 650, loss is 4.752984409332275 and perplexity is 115.92975059425875
At time: 1373.3797068595886 and batch: 700, loss is 4.729862184524536 and perplexity is 113.27994954273466
At time: 1374.9956560134888 and batch: 750, loss is 4.728272876739502 and perplexity is 113.10005582814479
At time: 1376.6094679832458 and batch: 800, loss is 4.728827095031738 and perplexity is 113.16275532093569
At time: 1378.2199349403381 and batch: 850, loss is 4.760324754714966 and perplexity is 116.78384584588616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955428759256999 and perplexity of 141.9434528971711
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1382.3836498260498 and batch: 50, loss is 4.813435792922974 and perplexity is 123.15402317380882
At time: 1383.9935276508331 and batch: 100, loss is 4.773127603530884 and perplexity is 118.28862393726824
At time: 1385.6046149730682 and batch: 150, loss is 4.761185188293457 and perplexity is 116.88437383090208
At time: 1387.2216839790344 and batch: 200, loss is 4.774511289596558 and perplexity is 118.45241154712262
At time: 1388.8329911231995 and batch: 250, loss is 4.7721131229400635 and perplexity is 118.16868327319357
At time: 1390.4462394714355 and batch: 300, loss is 4.746758642196656 and perplexity is 115.2102410364727
At time: 1392.0567998886108 and batch: 350, loss is 4.701620979309082 and perplexity is 110.1255390690022
At time: 1393.6709463596344 and batch: 400, loss is 4.726451988220215 and perplexity is 112.8943006204551
At time: 1395.2818644046783 and batch: 450, loss is 4.770758476257324 and perplexity is 118.00871483321487
At time: 1396.8905971050262 and batch: 500, loss is 4.772878255844116 and perplexity is 118.25913261946731
At time: 1398.5026242733002 and batch: 550, loss is 4.740979471206665 and perplexity is 114.54634159570605
At time: 1400.1219158172607 and batch: 600, loss is 4.773984375 and perplexity is 118.39001368309737
At time: 1401.7368276119232 and batch: 650, loss is 4.7521007633209225 and perplexity is 115.82735497994989
At time: 1403.3505589962006 and batch: 700, loss is 4.729008979797364 and perplexity is 113.18333977410725
At time: 1404.9634006023407 and batch: 750, loss is 4.7274253463745115 and perplexity is 113.0042407053967
At time: 1406.621503829956 and batch: 800, loss is 4.7279414939880375 and perplexity is 113.0625826297824
At time: 1408.2317793369293 and batch: 850, loss is 4.759489421844482 and perplexity is 116.68633319414147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955275853474935 and perplexity of 141.921750581744
Finished 47 epochs...
Completing Train Step...
At time: 1412.3580362796783 and batch: 50, loss is 4.81330602645874 and perplexity is 123.13804294853651
At time: 1413.978837966919 and batch: 100, loss is 4.772958507537842 and perplexity is 118.26862349598294
At time: 1415.5977675914764 and batch: 150, loss is 4.760977783203125 and perplexity is 116.86013393061558
At time: 1417.2127239704132 and batch: 200, loss is 4.774439039230347 and perplexity is 118.44385362616998
At time: 1418.8314678668976 and batch: 250, loss is 4.772001495361328 and perplexity is 118.15549312540328
At time: 1420.4510445594788 and batch: 300, loss is 4.746686687469483 and perplexity is 115.20195141325374
At time: 1422.0731084346771 and batch: 350, loss is 4.701554613113403 and perplexity is 110.1182306984442
At time: 1423.6877479553223 and batch: 400, loss is 4.726386022567749 and perplexity is 112.88685371987754
At time: 1425.3010759353638 and batch: 450, loss is 4.770722789764404 and perplexity is 118.00450359119117
At time: 1426.9152166843414 and batch: 500, loss is 4.772811975479126 and perplexity is 118.25129462074943
At time: 1428.5293583869934 and batch: 550, loss is 4.740939035415649 and perplexity is 114.54170991741906
At time: 1430.141681432724 and batch: 600, loss is 4.773960399627685 and perplexity is 118.38717527246703
At time: 1431.76283121109 and batch: 650, loss is 4.752068004608154 and perplexity is 115.82356068704581
At time: 1433.3871660232544 and batch: 700, loss is 4.7290044784545895 and perplexity is 113.18283029824528
At time: 1435.0009651184082 and batch: 750, loss is 4.727458410263061 and perplexity is 113.007977126787
At time: 1436.6148972511292 and batch: 800, loss is 4.728042278289795 and perplexity is 113.07397813746213
At time: 1438.2301228046417 and batch: 850, loss is 4.7595456600189205 and perplexity is 116.69289560502948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955186208089192 and perplexity of 141.90902852191337
Finished 48 epochs...
Completing Train Step...
At time: 1442.382853269577 and batch: 50, loss is 4.813221111297607 and perplexity is 123.12758710571401
At time: 1444.0453548431396 and batch: 100, loss is 4.7728551197052 and perplexity is 118.25639659139765
At time: 1445.6606407165527 and batch: 150, loss is 4.760858144760132 and perplexity is 116.84615380244118
At time: 1447.3073091506958 and batch: 200, loss is 4.774352064132691 and perplexity is 118.43355240841524
At time: 1448.9176964759827 and batch: 250, loss is 4.771926183700561 and perplexity is 118.14659497405871
At time: 1450.535718202591 and batch: 300, loss is 4.746630840301513 and perplexity is 115.19551789017147
At time: 1452.1505830287933 and batch: 350, loss is 4.701497726440429 and perplexity is 110.11196661683906
At time: 1453.7644300460815 and batch: 400, loss is 4.726342334747314 and perplexity is 112.88192204701055
At time: 1455.3787021636963 and batch: 450, loss is 4.770717010498047 and perplexity is 118.00382161370425
At time: 1456.994325876236 and batch: 500, loss is 4.772812976837158 and perplexity is 118.25141303269238
At time: 1458.6078379154205 and batch: 550, loss is 4.740930051803589 and perplexity is 114.54068092375446
At time: 1460.218631029129 and batch: 600, loss is 4.773941278457642 and perplexity is 118.38491159279982
At time: 1461.8337914943695 and batch: 650, loss is 4.752041330337525 and perplexity is 115.8204712192476
At time: 1463.4456269741058 and batch: 700, loss is 4.728995628356934 and perplexity is 113.18182862357666
At time: 1465.0649192333221 and batch: 750, loss is 4.727492084503174 and perplexity is 113.0117826486171
At time: 1466.6765649318695 and batch: 800, loss is 4.728114261627197 and perplexity is 113.08211787274092
At time: 1468.2900037765503 and batch: 850, loss is 4.75958758354187 and perplexity is 116.69778788486646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9551388422648115 and perplexity of 141.9023070429758
Finished 49 epochs...
Completing Train Step...
At time: 1472.4434216022491 and batch: 50, loss is 4.813146915435791 and perplexity is 123.11845188717724
At time: 1474.088930606842 and batch: 100, loss is 4.7727699470520015 and perplexity is 118.24632480926851
At time: 1475.7032163143158 and batch: 150, loss is 4.760763683319092 and perplexity is 116.83511686766363
At time: 1477.3139898777008 and batch: 200, loss is 4.774269371032715 and perplexity is 118.42375917574743
At time: 1478.9204587936401 and batch: 250, loss is 4.7718596935272215 and perplexity is 118.1387396476333
At time: 1480.5301849842072 and batch: 300, loss is 4.746579570770264 and perplexity is 115.18961202136403
At time: 1482.1439151763916 and batch: 350, loss is 4.701446990966797 and perplexity is 110.10638017577675
At time: 1483.752756357193 and batch: 400, loss is 4.726305694580078 and perplexity is 112.87778611027996
At time: 1485.359216928482 and batch: 450, loss is 4.77071662902832 and perplexity is 118.00377659882723
At time: 1487.0121004581451 and batch: 500, loss is 4.7728253269195555 and perplexity is 118.25287345640514
At time: 1488.617348909378 and batch: 550, loss is 4.740927438735962 and perplexity is 114.5403816216003
At time: 1490.2188382148743 and batch: 600, loss is 4.773925228118896 and perplexity is 118.38301149011512
At time: 1491.8239438533783 and batch: 650, loss is 4.752019510269165 and perplexity is 115.81794403621994
At time: 1493.4355063438416 and batch: 700, loss is 4.728987064361572 and perplexity is 113.18085933907179
At time: 1495.0453612804413 and batch: 750, loss is 4.727521762847901 and perplexity is 113.01513670103184
At time: 1496.6538772583008 and batch: 800, loss is 4.728174915313721 and perplexity is 113.08897692808124
At time: 1498.2653119564056 and batch: 850, loss is 4.759623250961304 and perplexity is 116.70195026804426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.955112139383952 and perplexity of 141.898517893168
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.0, 'tune_wordvecs': True, 'lr': 0.0, 'num_layers': 1, 'anneal': 8.0, 'batch_size': 50, 'wordvec_source': ''}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.112987756729126 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 3.704953908920288 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 5.295003175735474 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 6.891752004623413 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 8.4926278591156 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 10.088813543319702 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 11.680100440979004 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 13.272297143936157 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 14.893041849136353 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 16.493108987808228 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 18.097604751586914 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 19.691871643066406 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 21.28455424308777 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 22.88086438179016 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 24.47410798072815 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 26.083003759384155 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 27.679551362991333 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 1 epochs...
Completing Train Step...
At time: 31.8319833278656 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 33.42043375968933 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 35.009432792663574 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 36.63343834877014 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 38.23929691314697 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 39.84421491622925 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 41.44852423667908 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 43.058828830718994 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 44.66130304336548 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 46.265674114227295 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 47.87356185913086 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 49.4878511428833 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 51.09383201599121 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 52.69895148277283 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 54.307448863983154 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 55.92221665382385 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 57.537039279937744 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 2 epochs...
Completing Train Step...
At time: 61.70044183731079 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 63.31642699241638 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 64.9213662147522 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 66.5285062789917 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 68.18798542022705 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 69.8002860546112 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 71.41589021682739 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 73.02345037460327 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 74.63656616210938 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 76.24988603591919 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 77.85974383354187 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 79.46689581871033 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 81.07428812980652 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 82.68495988845825 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 84.29500603675842 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 85.90978813171387 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 87.52608275413513 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 3 epochs...
Completing Train Step...
At time: 91.72125697135925 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 93.33001780509949 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 94.93845677375793 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 96.54528284072876 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 98.15869402885437 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 99.77829217910767 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 101.39436793327332 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 103.00092911720276 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 104.61221146583557 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 106.22369766235352 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 107.85578441619873 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 109.47373867034912 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 111.0869390964508 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 112.6976261138916 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 114.30225205421448 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 115.908775806427 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 117.515784740448 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 4 epochs...
Completing Train Step...
At time: 121.68755888938904 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 123.30014181137085 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 124.91571426391602 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 126.5273048877716 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 128.13155484199524 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 129.73619055747986 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 131.3510172367096 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 132.96763372421265 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 134.5827317237854 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 136.19765663146973 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 137.82053208351135 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 139.43503880500793 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 141.050879240036 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 142.66530513763428 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 144.2857482433319 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 145.90932846069336 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 147.52193427085876 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 5 epochs...
Completing Train Step...
At time: 151.7425639629364 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 153.34859085083008 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 154.95821857452393 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 156.5773422718048 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 158.187748670578 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 159.7994499206543 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 161.40618324279785 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 163.01412320137024 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 164.6293661594391 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 166.23412156105042 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 167.83768796920776 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 169.4407639503479 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 171.0470368862152 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 172.65753316879272 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 174.2636206150055 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 175.86938738822937 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 177.4797224998474 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 6 epochs...
Completing Train Step...
At time: 181.65398240089417 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 183.31135964393616 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 184.92035818099976 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 186.53431105613708 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 188.14228558540344 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 189.78641176223755 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 191.39734292030334 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 193.00875902175903 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 194.62177681922913 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 196.2319986820221 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 197.83877825737 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 199.44906091690063 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 201.06903982162476 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 202.68664598464966 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 204.29481720924377 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 205.9064917564392 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 207.51849150657654 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 7 epochs...
Completing Train Step...
At time: 211.70776510238647 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 213.34956860542297 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 214.96110844612122 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 216.57572865486145 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 218.18034434318542 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 219.78611588478088 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 221.4004249572754 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 223.01654314994812 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 224.63272190093994 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 226.24935340881348 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 227.86557030677795 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 229.50589847564697 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 231.11856961250305 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 232.72939586639404 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 234.34219312667847 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 235.96297097206116 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 237.58200407028198 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 8 epochs...
Completing Train Step...
At time: 241.80526161193848 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 243.46880173683167 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 245.08136796951294 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 246.70292568206787 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 248.31327390670776 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 249.9241237640381 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 251.5347764492035 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 253.1488790512085 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 254.75971603393555 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 256.3731095790863 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 257.9826340675354 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 259.593945980072 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 261.20844435691833 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 262.81942224502563 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 264.43443274497986 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 266.0532126426697 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 267.67193579673767 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 9 epochs...
Completing Train Step...
At time: 271.88632678985596 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 273.49509739875793 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 275.10450410842896 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 276.7213497161865 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 278.32816457748413 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 279.93798756599426 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 281.5450541973114 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 283.15275287628174 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 284.75351309776306 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 286.355411529541 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 287.9638931751251 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 289.58143186569214 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 291.19498085975647 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 292.800500869751 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 294.41109561920166 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 296.0218241214752 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 297.6307044029236 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 10 epochs...
Completing Train Step...
At time: 301.8218984603882 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 303.4274790287018 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 305.0328121185303 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 306.643630027771 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 308.2499966621399 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 309.85578751564026 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 311.51086044311523 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 313.1190826892853 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 314.72498083114624 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 316.33160734176636 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 317.9434115886688 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 319.5484538078308 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 321.15714502334595 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 322.76607751846313 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 324.3831479549408 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 325.99327874183655 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 327.60107493400574 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 11 epochs...
Completing Train Step...
At time: 331.7935905456543 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 333.4036645889282 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 335.0217354297638 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 336.63693737983704 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 338.258602142334 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 339.86962819099426 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 341.47936272621155 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 343.0920259952545 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 344.7104027271271 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 346.3312997817993 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 347.9431109428406 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 349.5564155578613 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 351.16613483428955 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 352.8193337917328 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 354.4318473339081 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 356.0421724319458 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 357.65101385116577 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 12 epochs...
Completing Train Step...
At time: 361.813752412796 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 363.4462869167328 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 365.05145263671875 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 366.6596796512604 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 368.26939249038696 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 369.8761010169983 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 371.4818158149719 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 373.09377217292786 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 374.71375703811646 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 376.3187475204468 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 377.93076372146606 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 379.54886984825134 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 381.1540195941925 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 382.7656033039093 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 384.3798522949219 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 385.9910569190979 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 387.5964720249176 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 13 epochs...
Completing Train Step...
At time: 391.7887227535248 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 393.4254581928253 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 395.03170228004456 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 396.6410949230194 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 398.2527437210083 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 399.8615412712097 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 401.4751901626587 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 403.0843975543976 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 404.69577503204346 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 406.3020861148834 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 407.90833806991577 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 409.52347350120544 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 411.13619470596313 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 412.746839761734 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 414.35318660736084 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 415.96849155426025 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 417.58048820495605 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 14 epochs...
Completing Train Step...
At time: 421.77081537246704 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 423.4112899303436 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 425.023823261261 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 426.6292781829834 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 428.24032759666443 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 429.8650949001312 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 431.4799311161041 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 433.0940639972687 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 434.73667454719543 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 436.3521959781647 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 437.9637439250946 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 439.57649278640747 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 441.19680619239807 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 442.8188328742981 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 444.43145060539246 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 446.04458570480347 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 447.6646661758423 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 15 epochs...
Completing Train Step...
At time: 451.88497018814087 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 453.4983928203583 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 455.1136164665222 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 456.72593235969543 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 458.3345241546631 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 459.9423031806946 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 461.55566453933716 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 463.1720085144043 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 464.78755283355713 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 466.39369440078735 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 467.99957394599915 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 469.6078395843506 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 471.2213990688324 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 472.82864809036255 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 474.46727991104126 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 476.08334827423096 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 477.70537781715393 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 16 epochs...
Completing Train Step...
At time: 481.9442503452301 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 483.5586431026459 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 485.16894936561584 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 486.77979588508606 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 488.3902530670166 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 489.99951362609863 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 491.6147644519806 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 493.22279381752014 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 494.8335506916046 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 496.44029927253723 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 498.0518651008606 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 499.66366624832153 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 501.2706937789917 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 502.87764072418213 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 504.4826328754425 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 506.0903651714325 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 507.69105648994446 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 17 epochs...
Completing Train Step...
At time: 511.8673782348633 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 513.4746246337891 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 515.1064200401306 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 516.7122735977173 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 518.3249897956848 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 519.9375088214874 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 521.5416843891144 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 523.146892786026 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 524.7595493793488 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 526.3719036579132 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 527.9782471656799 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 529.5916755199432 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 531.1966626644135 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 532.8178873062134 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 534.4293315410614 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 536.0361065864563 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 537.6453721523285 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 18 epochs...
Completing Train Step...
At time: 541.841703414917 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 543.4994592666626 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 545.1194121837616 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 546.7361588478088 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 548.3533861637115 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 549.9747166633606 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 551.5923340320587 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 553.213990688324 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 554.8376383781433 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 556.4858350753784 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 558.1027388572693 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 559.7162570953369 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 561.3308665752411 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 562.9516413211823 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 564.5653860569 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 566.1775016784668 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 567.7952153682709 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 19 epochs...
Completing Train Step...
At time: 571.9686055183411 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 573.6043794155121 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 575.2178964614868 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 576.8367154598236 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 578.4424357414246 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 580.0491709709167 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 581.6599895954132 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 583.2700664997101 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 584.8730156421661 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 586.4867408275604 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 588.0967817306519 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 589.7059600353241 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 591.3105311393738 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 592.9148736000061 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 594.5213561058044 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 596.1312963962555 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 597.7822968959808 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 20 epochs...
Completing Train Step...
At time: 601.9278163909912 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 603.5615880489349 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 605.1730687618256 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 606.789041519165 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 608.404717206955 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 610.018871307373 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 611.6284310817719 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 613.2424252033234 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 614.851197719574 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 616.4579205513 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 618.0665571689606 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 619.6779158115387 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 621.2854855060577 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 622.9042060375214 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 624.5149931907654 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 626.1243436336517 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 627.7310934066772 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 21 epochs...
Completing Train Step...
At time: 631.9567201137543 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 633.568187713623 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 635.186422586441 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 636.8009812831879 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 638.4472877979279 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 640.0601501464844 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 641.6785433292389 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 643.2946968078613 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 644.9142031669617 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 646.5267691612244 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 648.1347188949585 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 649.7470400333405 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 651.3587169647217 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 652.9641852378845 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 654.5846221446991 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 656.2023992538452 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 657.8153328895569 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 22 epochs...
Completing Train Step...
At time: 662.0298218727112 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 663.6386318206787 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 665.2455596923828 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 666.8620691299438 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 668.4751160144806 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 670.0895824432373 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 671.6993305683136 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 673.3153734207153 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 674.926611661911 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 676.5344262123108 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 678.1742198467255 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 679.787362575531 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 681.3959107398987 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 683.0049664974213 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 684.6219115257263 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 686.2330877780914 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 687.8404462337494 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 23 epochs...
Completing Train Step...
At time: 692.0373589992523 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 693.6481149196625 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 695.2573759555817 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 696.8681955337524 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 698.4867520332336 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 700.0946300029755 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 701.7002093791962 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 703.3147187232971 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 704.9300661087036 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 706.5374290943146 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 708.1404366493225 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 709.7404608726501 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 711.3539049625397 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 712.9627709388733 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 714.5771081447601 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 716.1855652332306 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 717.7939488887787 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 24 epochs...
Completing Train Step...
At time: 721.9825065135956 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 723.6036324501038 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 725.2038650512695 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 726.812343120575 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 728.424468755722 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 730.0306830406189 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 731.640692949295 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 733.2495682239532 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 734.868043422699 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 736.4748637676239 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 738.0863862037659 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 739.6965384483337 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 741.3132653236389 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 742.9217648506165 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 744.5266783237457 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 746.1353003978729 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 747.7477955818176 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 25 epochs...
Completing Train Step...
At time: 751.9062416553497 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 753.5700254440308 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 755.184296131134 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 756.7958509922028 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 758.4078719615936 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 760.0473699569702 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 761.663437128067 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 763.2831826210022 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 764.8993520736694 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 766.5100028514862 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 768.1224231719971 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 769.7377781867981 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 771.3546378612518 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 772.9672539234161 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 774.5802161693573 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 776.1928894519806 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 777.8063714504242 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 26 epochs...
Completing Train Step...
At time: 782.0274868011475 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 783.6311767101288 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 785.2376191616058 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 786.8426434993744 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 788.4558939933777 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 790.0672354698181 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 791.6736972332001 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 793.27858710289 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 794.8957850933075 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 796.5060839653015 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 798.1116893291473 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 799.7165968418121 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 801.348973274231 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 802.9561536312103 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 804.5567712783813 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 806.1566019058228 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 807.7652101516724 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 27 epochs...
Completing Train Step...
At time: 811.9596951007843 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 813.5638751983643 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 815.1715412139893 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 816.7814655303955 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 818.3945679664612 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 820.0007607936859 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 821.6070959568024 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 823.2151873111725 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 824.8323726654053 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 826.4375121593475 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 828.0413074493408 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 829.6489462852478 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 831.263471364975 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 832.8716027736664 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 834.4777677059174 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 836.0887475013733 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 837.7012629508972 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 28 epochs...
Completing Train Step...
At time: 841.8827543258667 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 843.532220363617 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 845.1462967395782 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 846.760790348053 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 848.3792378902435 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 849.9952194690704 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 851.6086263656616 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 853.2208135128021 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 854.8328609466553 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 856.4471995830536 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 858.0647239685059 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 859.6835532188416 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 861.2984795570374 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 862.9128150939941 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 864.5269944667816 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 866.1383755207062 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 867.7497429847717 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 29 epochs...
Completing Train Step...
At time: 871.9288578033447 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 873.5841467380524 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 875.1983094215393 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 876.8069264888763 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 878.4145619869232 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 880.0206205844879 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 881.6290996074677 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 883.2296705245972 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 884.8613173961639 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 886.4707407951355 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 888.0826139450073 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 889.6901345252991 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 891.2996876239777 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 892.9117610454559 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 894.5220868587494 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 896.132741689682 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 897.7444670200348 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 30 epochs...
Completing Train Step...
At time: 901.9589931964874 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 903.5681610107422 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 905.1820955276489 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 906.7932250499725 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 908.4086751937866 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 910.0210030078888 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 911.6327137947083 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 913.2447440624237 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 914.8533861637115 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 916.4659237861633 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 918.0717952251434 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 919.6820108890533 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 921.2908127307892 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 922.9071667194366 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 924.5425560474396 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 926.1480095386505 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 927.756484746933 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 31 epochs...
Completing Train Step...
At time: 931.9453611373901 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 933.5595054626465 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 935.167286157608 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 936.7747571468353 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 938.3810031414032 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 939.9872937202454 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 941.5926859378815 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 943.2008719444275 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 944.8136005401611 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 946.4232070446014 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 948.0295507907867 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 949.6351370811462 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 951.24241065979 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 952.8523354530334 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 954.461620092392 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 956.0732338428497 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 957.6807005405426 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 32 epochs...
Completing Train Step...
At time: 961.8370039463043 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 963.4782979488373 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 965.0878067016602 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 966.7269411087036 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 968.330676317215 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 969.935848236084 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 971.5512945652008 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 973.1657569408417 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 974.7785670757294 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 976.395117521286 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 978.0132021903992 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 979.6266763210297 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 981.2393953800201 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 982.8558573722839 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 984.4723401069641 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 986.0862076282501 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 987.7005496025085 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 33 epochs...
Completing Train Step...
At time: 991.867044210434 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 993.4707963466644 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 995.0739481449127 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 996.6772999763489 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 998.2808573246002 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 999.8894917964935 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1001.4991962909698 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1003.1104764938354 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1004.71950507164 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1006.3221955299377 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1007.9713730812073 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1009.5742182731628 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1011.1807217597961 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1012.7902607917786 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1014.3986911773682 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1016.0089628696442 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1017.6164050102234 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 34 epochs...
Completing Train Step...
At time: 1021.7901713848114 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1023.3956804275513 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1025.005301952362 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1026.6159718036652 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1028.222220659256 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1029.8289113044739 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1031.4388828277588 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1033.0507235527039 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1034.6596250534058 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1036.270653963089 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1037.8776676654816 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1039.4923272132874 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1041.104258298874 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1042.7163605690002 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1044.3314023017883 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1045.9473779201508 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1047.5594120025635 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 35 epochs...
Completing Train Step...
At time: 1051.7146229743958 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1053.352622270584 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1054.9701869487762 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1056.5823714733124 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1058.1932277679443 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1059.8052661418915 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1061.422269821167 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1063.0354645252228 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1064.6493213176727 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1066.2622735500336 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1067.8778674602509 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1069.4899768829346 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1071.1014931201935 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1072.7226350307465 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1074.3353064060211 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1075.9453508853912 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1077.55784034729 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 36 epochs...
Completing Train Step...
At time: 1081.7127628326416 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1083.3674929141998 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1084.976068019867 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1086.584712266922 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1088.1932120323181 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1089.8318192958832 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1091.4460000991821 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1093.05566573143 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1094.665370464325 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1096.2777342796326 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1097.8906197547913 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1099.4985194206238 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1101.1049563884735 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1102.7162578105927 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1104.3276388645172 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1105.942317724228 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1107.5507080554962 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 37 epochs...
Completing Train Step...
At time: 1111.7322010993958 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1113.3413772583008 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1114.9534583091736 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1116.5640616416931 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1118.1739301681519 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1119.7862677574158 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1121.3984327316284 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1123.007176399231 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1124.6204552650452 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1126.227736234665 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1127.8311092853546 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1129.4302515983582 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1131.0622045993805 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1132.6686327457428 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1134.2808611392975 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1135.8957314491272 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1137.5024621486664 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 38 epochs...
Completing Train Step...
At time: 1141.6972916126251 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1143.3019077777863 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1144.9096765518188 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1146.523086309433 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1148.1297824382782 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1149.7360637187958 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1151.3447415828705 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1152.95361661911 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1154.5615139007568 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1156.1703577041626 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1157.7749042510986 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1159.3864414691925 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1160.9964315891266 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1162.601798772812 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1164.2069873809814 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1165.814049243927 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1167.4221894741058 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 39 epochs...
Completing Train Step...
At time: 1171.574586391449 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1173.2117793560028 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1174.8201808929443 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1176.4307351112366 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1178.0454783439636 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1179.6563966274261 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1181.2667253017426 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1182.8828592300415 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1184.4961528778076 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1186.108339548111 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1187.7221946716309 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1189.33984708786 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1190.954312801361 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1192.5646531581879 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1194.1742165088654 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1195.7853083610535 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1197.3988497257233 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 40 epochs...
Completing Train Step...
At time: 1201.6079556941986 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1203.214501619339 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1204.8240275382996 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1206.4291589260101 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1208.032862663269 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1209.6389439105988 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1211.2420179843903 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1212.8405718803406 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1214.4731674194336 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1216.0816695690155 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1217.689861536026 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1219.3029789924622 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1220.9096031188965 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1222.5194070339203 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1224.124656200409 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1225.7314522266388 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1227.3429968357086 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 41 epochs...
Completing Train Step...
At time: 1231.5190916061401 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1233.1254448890686 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1234.7358400821686 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1236.3492782115936 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1237.9564633369446 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1239.5674924850464 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1241.1791241168976 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1242.7875282764435 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1244.3958702087402 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1246.0060045719147 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1247.6196036338806 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1249.226129770279 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1250.837848186493 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1252.4488623142242 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1254.0637865066528 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1255.7183434963226 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1257.3262948989868 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 42 epochs...
Completing Train Step...
At time: 1261.4846198558807 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1263.1240375041962 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1264.7371366024017 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1266.3528208732605 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1267.9633796215057 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1269.5733721256256 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1271.1876134872437 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1272.803759098053 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1274.4155943393707 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1276.0278508663177 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1277.6436614990234 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1279.2564344406128 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1280.86674451828 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1282.474033355713 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1284.0841507911682 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1285.6931989192963 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1287.2961502075195 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 43 epochs...
Completing Train Step...
At time: 1291.4449689388275 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1293.0792875289917 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1294.6853816509247 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1296.3215355873108 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1297.9343435764313 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1299.5447964668274 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1301.1527812480927 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1302.761292219162 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1304.3740901947021 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1305.9876670837402 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1307.5963888168335 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1309.202679157257 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1310.8118772506714 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1312.4235458374023 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1314.0359873771667 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1315.6435577869415 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1317.2518582344055 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 44 epochs...
Completing Train Step...
At time: 1321.455567598343 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1323.0674407482147 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1324.6752135753632 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1326.283077955246 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1327.8945760726929 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1329.5017766952515 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1331.1067185401917 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1332.7144396305084 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1334.3339114189148 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1335.9411334991455 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1337.5760786533356 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1339.1868917942047 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1340.7977132797241 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1342.4060049057007 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1344.0144028663635 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1345.6249063014984 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1347.2330434322357 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 45 epochs...
Completing Train Step...
At time: 1351.4137926101685 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1353.022037744522 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1354.6292552947998 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1356.234700679779 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1357.8409893512726 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1359.4438378810883 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1361.0411176681519 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1362.6437361240387 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1364.2535071372986 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1365.8607015609741 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1367.4719099998474 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1369.078637123108 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1370.6880640983582 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1372.2943332195282 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1373.8994536399841 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1375.508516550064 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1377.1127591133118 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 46 epochs...
Completing Train Step...
At time: 1381.2578411102295 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1382.8918704986572 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1384.5005366802216 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1386.1178319454193 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1387.723641872406 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1389.3316798210144 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1390.9364166259766 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1392.5401937961578 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1394.1522583961487 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1395.770301103592 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1397.3803279399872 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1399.0098531246185 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1400.6518743038177 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1402.2651102542877 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1403.87344956398 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1405.4823093414307 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1407.0911815166473 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 47 epochs...
Completing Train Step...
At time: 1411.3799171447754 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1412.989770412445 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1414.6028134822845 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1416.2371978759766 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1417.8658330440521 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1419.4743378162384 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1421.1435823440552 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1422.7476451396942 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1424.3561573028564 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1425.9539914131165 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1427.5582168102264 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1429.1644213199615 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1430.7712514400482 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1432.3763637542725 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1433.981815814972 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1435.592743396759 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1437.1996862888336 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 48 epochs...
Completing Train Step...
At time: 1441.399397611618 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1443.0062334537506 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1444.612508058548 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1446.2223823070526 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1447.8343615531921 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1449.4403824806213 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1451.0466015338898 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1452.6556377410889 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1454.2664971351624 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1455.8731434345245 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1457.4789109230042 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1459.0845296382904 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1460.6970384120941 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1462.3519349098206 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1463.9570257663727 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1465.562989473343 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1467.168437719345 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished 49 epochs...
Completing Train Step...
At time: 1471.352028131485 and batch: 50, loss is 9.934866104125977 and perplexity is 20637.52096559723
At time: 1472.9901869297028 and batch: 100, loss is 9.934596347808839 and perplexity is 20631.954614759597
At time: 1474.5989644527435 and batch: 150, loss is 9.934887504577636 and perplexity is 20637.962622592844
At time: 1476.2121315002441 and batch: 200, loss is 9.934183235168456 and perplexity is 20623.43305381591
At time: 1477.8286664485931 and batch: 250, loss is 9.934328212738038 and perplexity is 20626.423205763716
At time: 1479.438682794571 and batch: 300, loss is 9.934316902160644 and perplexity is 20626.189910327048
At time: 1481.0489451885223 and batch: 350, loss is 9.935177478790283 and perplexity is 20643.947967310836
At time: 1482.6633458137512 and batch: 400, loss is 9.934774074554444 and perplexity is 20635.62179077672
At time: 1484.2788183689117 and batch: 450, loss is 9.934608249664306 and perplexity is 20632.200174762736
At time: 1485.891633272171 and batch: 500, loss is 9.934975128173829 and perplexity is 20639.77107432628
At time: 1487.5034022331238 and batch: 550, loss is 9.93474063873291 and perplexity is 20634.931833344
At time: 1489.1142563819885 and batch: 600, loss is 9.934957695007324 and perplexity is 20639.411260896868
At time: 1490.7300543785095 and batch: 650, loss is 9.934219818115235 and perplexity is 20624.187533570166
At time: 1492.3435952663422 and batch: 700, loss is 9.93416093826294 and perplexity is 20622.97322020413
At time: 1493.952966451645 and batch: 750, loss is 9.934718379974365 and perplexity is 20634.472530490508
At time: 1495.5635023117065 and batch: 800, loss is 9.934435138702392 and perplexity is 20628.628823873
At time: 1497.172861814499 and batch: 850, loss is 9.934909057617187 and perplexity is 20638.407438211034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 10.039813995361328 and perplexity of 22921.119085607585
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7febc4031860>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -93.27288598730286, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.7178254696833063, 'tune_wordvecs': True, 'lr': 3.893393447066984, 'num_layers': 1, 'anneal': 4.251717208023265, 'batch_size': 50, 'wordvec_source': ''}}, {'best_accuracy': -94.62012328363782, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.9182670318718621, 'tune_wordvecs': True, 'lr': 3.1841405083311223, 'num_layers': 1, 'anneal': 2.566900712595255, 'batch_size': 50, 'wordvec_source': ''}}, {'best_accuracy': -168.46704259203258, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.4086023050581945, 'tune_wordvecs': True, 'lr': 26.194614181991568, 'num_layers': 1, 'anneal': 3.9173863786566594, 'batch_size': 50, 'wordvec_source': ''}}, {'best_accuracy': -150.73317109749843, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.8297006600589905, 'tune_wordvecs': True, 'lr': 15.687241825130098, 'num_layers': 1, 'anneal': 7.971058623808862, 'batch_size': 50, 'wordvec_source': ''}}, {'best_accuracy': -141.898517893168, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.9441594106889342, 'tune_wordvecs': True, 'lr': 14.573061955702206, 'num_layers': 1, 'anneal': 6.21341231060793, 'batch_size': 50, 'wordvec_source': ''}}, {'best_accuracy': -22921.119085607585, 'params': {'seq_len': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'dropout': 0.0, 'tune_wordvecs': True, 'lr': 0.0, 'num_layers': 1, 'anneal': 8.0, 'batch_size': 50, 'wordvec_source': ''}}]
